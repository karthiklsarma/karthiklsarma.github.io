"V1","V2","V3","V4"
"0.0826898230594723","0.105880887471907","  1432","<p>In answering <a href=""http://stats.stackexchange.com/questions/1412/consequences-of-an-improper-link-function-in-n-alternative-forced-choice-procedur"">this</a> question John Christie suggested that the fit of logistic regression models should be assessed by evaluating the residuals.  I'm familiar with how to interpret residuals in OLS, they are in the same scale as the DV and very clearly the difference between y and the y predicted by the model.  However for logistic regression, in the past I've typically just examined estimates of model fit, e.g. AIC, because I wasn't sure what a residual would mean for a logistic regression.  After looking into R's help files a little bit I see that in R there are five types of glm residuals available, c(""deviance"", ""pearson"", ""working"",""response"", ""partial"").  The help file refers to Davison, A. C. and Snell, E. J. (1991) Residuals and diagnostics. In: Statistical Theory and Modelling. In Honour of Sir David Cox, FRS, eds. Hinkley, D. V., Reid, N. and Snell, E. J., Chapman &amp; Hall, of which I do not have a copy.  Is there a short way to describe how to interpret each of these types?  In a logistic context will sum of squared residuals provide a meaningful measure of model fit or is one better off with an Information Criterion?</p>
"
"0.10336227882434","0.105880887471907","  5354","<p>I've got some data about airline flights (in a data frame called <code>flights</code>) and I would like to see if the flight time has any effect on the probability of a significantly delayed arrival (meaning 10 or more minutes). I figured I'd use logistic regression, with the flight time as the predictor and whether or not each flight was significantly delayed (a bunch of Bernoullis) as the response. I used the following code...</p>

<pre><code>flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
summary(delay.model)
</code></pre>

<p>...but got the following output.</p>

<pre><code>&gt; flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
&gt; delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  fitted probabilities numerically 0 or 1 occurred
&gt; summary(delay.model)

Call:
glm(formula = BigDelay ~ ArrDelay, family = binomial(link = ""logit""),
    data = flights)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-3.843e-04  -2.107e-08  -2.107e-08   2.107e-08   3.814e-04

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -312.14     170.26  -1.833   0.0668 .
ArrDelay       32.86      17.92   1.833   0.0668 .
---
Signif. codes:  0 Ã¢***Ã¢ 0.001 Ã¢**Ã¢ 0.01 Ã¢*Ã¢ 0.05 Ã¢.Ã¢ 0.1 Ã¢ Ã¢ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.8375e+06  on 2291292  degrees of freedom
Residual deviance: 9.1675e-03  on 2291291  degrees of freedom
AIC: 4.0092

Number of Fisher Scoring iterations: 25
</code></pre>

<p>What does it mean that the algorithm did not converge? I thought it be because the <code>BigDelay</code> values were <code>TRUE</code> and <code>FALSE</code> instead of <code>0</code> and <code>1</code>, but I got the same error after I converted everything. Any ideas?</p>
"
"0.0800640769025436","0.0820149827720712","  6562","<p>I have read an <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">article</a> from Christopher Manning, and saw an interesting code for collapsing categorical variable in an logistic regression model:</p>

<pre><code>glm(ced.del ~ cat + follows + I(class == 1), family=binomial(""logit""))
</code></pre>

<p>Does the <code>I(class == 1)</code> means that the <code>class</code> variable has been recoded into either it is <code>1</code> or it is not <code>1</code>?</p>

<p>After that, I am thinking of modifying it a bit:</p>

<pre><code>glm(ced.del ~ cat + follows + I(class %in% C(1,2)), family=binomial(""logit""))
</code></pre>

<p>I am planning to merge the variable <code>class</code> from <code>c(1,2,3,4)</code> into two groups, one group contains <code>c(1,2)</code>, another group contains <code>c(3,4)</code>, can the code above give me the result I want?</p>

<p>Thanks.</p>
"
"0.0462250163521024","0.0473513723810378","  9027","<p>I have two logistic regression models in R made with <code>glm()</code>.  They both use the same variables, but were made using different subsets of a matrix.  Is there an easy way to get an average model which gives the means of the coefficients and then use this with the predict() function?</p>

<p>[ sorry if this type of question should be posted on a programming site let me know and I'll post it there ]</p>

<p>Thanks</p>
"
"0.10336227882434","0.105880887471907"," 10316","<p>I'm working on a multiple logistic regression in R using <code>glm</code>. The predictor variables are continuous and categorical. An extract of the summary of the model shows the following:</p>

<pre><code>Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   2.451e+00  2.439e+00   1.005   0.3150
Age           5.747e-02  3.466e-02   1.658   0.0973 .
BMI          -7.750e-02  7.090e-02  -1.093   0.2743
...
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Confidence intervals:</p>

<pre><code>                  2.5 %       97.5 %
(Intercept)  0.10969506 1.863217e+03
Age          0.99565783 1.142627e+00
BMI          0.80089276 1.064256e+00
...
</code></pre>

<p>Odd ratios:</p>

<pre><code>                 Estimate Std. Error   z value Pr(&gt;|z|)
(Intercept)  1.159642e+01  11.464683 2.7310435 1.370327
Age          1.059155e+00   1.035269 5.2491658 1.102195
B            9.254228e-01   1.073477 0.3351730 1.315670
...
</code></pre>

<p>The first output shows that $Age$ is significant. However, the confidence interval for $Age$ includes the value 1 and the odds ratio for $Age$ is very close to 1. What does the significant p-value from the first output mean? Is $Age$ a predictor of the outcome or not?</p>
"
"0.11322770341446","0.0966555841283824"," 12319","<p>Background: Iâ€™m analyzing data with mixed-models (lmer in lme4) from an experiment that had RTs and Error Rates as dependent variables. This is a repeated-measures design with approximately 300 measurements for each of the 190 human subjects. The fixed-effects are 1 between-subjects experimental manipulation (dichotomous), 2 within-subjects experimental manipulations (both dichotomous), and 1 subject variable (continuous, centered). My uncorrelated random effects are the participants, and 2 stimulus characteristics.  For the mixed-models, Iâ€™ve coded the experimental manipulations as a -.5/+.5 contrasts so that the parameters are estimates of the experimental effects and the intercept should be the grand mean.</p>

<p>The grand mean produced by the RT model (740 ms) does not match the mean I get if I average all of the individual trials (730 ms). Why does this happen?</p>

<p>A related question: the GLMM (binomial distribution, logistic link function) for error rates produces a parameter estimate  with an associated Z-score that has an absolute value over 2, but when I look at the means (determined the same way as above) to examine this difference they are tiny and almost identical (0.01353835 vs. 0.01354846). What are the values that I can provide that support the reliable parameter estimate? </p>

<p>I have a feeling the discrepancy between my calculated means and the model estimates has something to do with the random factors (perhaps the grouping by subjects), but Iâ€™m not sure exactly what.</p>

<p>If I want to display descriptive statistics along with the table of mixed model estimates, how should these descriptive be determined? Any points to references, examples, etc. will be greatly appreciated.</p>

<p>If this is all just a brain fart on my part, please let me know that too.</p>

<p>Edit: It is probably also important to mention that the amount of trials and types of trials contributed are not the same for every person. The between-subject manipulation changes the proportions of the different trial types presented, and for RTs only correct trials were analyzed. There were, however, very few errors made.</p>
"
"0.130744090092123","0.117188667781924"," 18248","<p>This question arises from my actual confusion about how to decide if a logistic model is good enough. I have models that use the state of pairs individual-project two years after they are formed as a dependent variable. The outcome is successful (1) or not (0). I have independent variables measured at the time of formation of the pairs.  My aim is to test whether a variable, which I hypothesized would influence the success of the pairs has an effect on that success, controlling for other potential influences. In the models, the variable of interest is significant.</p>

<p>The models were estimated using the <code>glm()</code> function in <code>R</code>. To assess the quality of the models, I have done a few things: <code>glm()</code> gives you the <code>residual deviance</code>, the <code>AIC</code> and the <code>BIC</code> by default. In addition, I have calculated the error rate of the model and plotted the binned residuals.  </p>

<ul>
<li>The complete model has a smaller residual deviance, AIC and BIC than the other models that I have estimated (and that are nested in the complete model), which leads me to think that this model is ""better"" than the others.  </li>
<li>The error-rate of the model is fairly low, IMHO (as in <a href=""http://www.stat.columbia.edu/~gelman/arm/"">Gelman and Hill, 2007, pp.99</a>):<br>
<code>error.rate &lt;- mean((predicted&gt;0.5 &amp; y==0) | (predicted&lt;0.5 &amp; y==1)</code>, at around 20%.  </li>
</ul>

<p>So far so good. But when I plot the binned residual (again following Gelman and Hill's advice), a large portion of the bins fall outside of the 95% CI:
<img src=""https://lh5.googleusercontent.com/-DhQ3a9hTVoE/Tr1H-Csj_JI/AAAAAAAAAC0/eYXlUlkc6ic/s550/binned.res.jpeg"" alt=""Binned Residuals plot""></p>

<p>That plot leads me to think there is something utterly wrong about the model. Should that lead me to throw the model away? Should I acknowledge that the model is imperfect but keep it and interpret the effect of the variable of interest? I have toyed around with excluding variables in turn, and also some transformation, without really improving the binned residuals plot.</p>

<p><strong>Edit:</strong>  </p>

<ul>
<li>At the moment, the model has a dozen predictors and 5 interaction effects.  </li>
<li>The pairs are ""relatively"" independent of each other in the sense that they are all formed during a short period of time (but not stricly speaking, all simultaneously) and there are a lot of projects (13k) and a lot of individuals (19k), so a fair proportion of projects are only joined by one individual (there are about 20000 pairs).</li>
</ul>
"
"0.0924500327042048","0.0947027447620757"," 20645","<p>Possible warning: basic question ahead.</p>

<p>Let's say that I model whether I wear red shoes depending on the weather. Red shoes, which is my dependent variable, is a dichotomous variable as I either wear them or don't. Weather is a variable with five 'levels' and I'm trying to find the probability that I will wear read shoes.</p>

<p>Let's say I model out this relationship with a logistic regression model in R:</p>

<pre><code>mod = glm(shoes ~ weather, data=mydat, family=binomial(link=""logit""))
</code></pre>

<p>Now, what I am interested in is finding what are the probabilities for wearing red shoes for each of the five 'levels' in weather. So perhaps I might have a 20% probability of wearing red shoes when cold, 30% when mild, 40% when warm, and so forth.</p>

<p>I'm wondering if modeling is a requirement for finding this information?
If so, how does one go from somewhat meaningful regression coefficients to meaningful probabilities in R?</p>
"
"0.179028718509858","0.171165004875037"," 25988","<p>An assumption of the ordinal logistic regression is the proportional odds assumption. Using R and the 2 packages mentioned I have 2 ways to check that but I have questions in each one.</p>

<p>1) Using the rms package</p>

<p>Given the next commands</p>

<pre><code>library(rms)
ddist &lt;- datadist(Ki67,Cyclin_E)
options(datadist='ddist')
f &lt;- lrm(grade ~Ki67+Cyclin_E);f
sf &lt;- function(y)
c('Y&gt;=1'=qlogis(mean(y &gt;= 1)),'Y&gt;=2'=qlogis(mean(y &gt;= 2)),'Y&gt;=3'=qlogis(mean(y &gt;= 3)))
s &lt;- summary(grade ~Ki67+Cyclin_E, fun=sf)
plot(s,which=1:3,pch=1:3,xlab='logit',main='',xlim=c(-2.5,2.5))
</code></pre>

<p>I have</p>

<pre><code>lrm(formula = grade ~ Ki67 + Cyclin_E)

Frequencies of Missing Values Due to Each Variable
   grade     Ki67 Cyclin_E 
       0        0        3 


                     Model Likelihood     Discrimination    Rank Discrim.    
                        Ratio Test            Indexes          Indexes       

Obs            42    LR chi2     11.38    R2       0.268    C       0.728    
 1             11    d.f.            2    g        1.279    Dxy     0.456    
 2             15    Pr(&gt; chi2) 0.0034    gr       3.592    gamma   0.458    
 3             16                         gp       0.192    tau-a   0.308    
max |deriv| 1e-07                         Brier    0.166                     


         Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=2     -0.1895 0.8427 -0.22  0.8221  
y&gt;=3     -2.0690 0.9109 -2.27  0.0231  
Ki67      0.0971 0.0330  2.94  0.0033  
Cyclin_E -0.0076 0.0227 -0.33  0.7387 
</code></pre>

<p>The <code>s</code> table gives: (unfortunately I don't know how to upload a graph made in R)</p>

<pre><code>grade    N=45

+--------+-------+--+----+---------+----------+
|        |       |N |Y&gt;=1|Y&gt;=2     |Y&gt;=3      |
+--------+-------+--+----+---------+----------+
|Ki67    |[ 2, 9)|12|Inf |0.6931472|-1.0986123|
|        |[ 9,16)|12|Inf |0.3364722|-2.3978953|
|        |[16,24)|10|Inf |2.1972246| 0.0000000|
|        |[24,44]|11|Inf |2.3025851| 1.5040774|
+--------+-------+--+----+---------+----------+
|Cyclin_E|[ 3,16)|15|Inf |1.0116009|-0.1335314|
|        |[16,22)| 7|Inf |1.7917595|-0.9162907|
|        |[22,33)|10|Inf |1.3862944|-0.8472979|
|        |[33,80]|10|Inf |0.4054651|-0.4054651|
|        |Missing| 3|Inf |      Inf| 0.6931472|
+--------+-------+--+----+---------+----------+
|Overall |       |45|Inf |1.1284653|-0.4054651|
+--------+-------+--+----+---------+----------+
</code></pre>

<p>Where for the Ki67 I see that 3 out of the 4 differences  <code>logit(P[Y&gt; = 2])-logit(P[Y&gt; = 3])</code> are close to 2. Only the last one is quite lower (around 0.8). But here Ki67 is continuous and not categorical so I don't know if the results of the table are correct and there isn't any p-value to decide. By the way I run the above in SPSS and I didn't reject the assumption.</p>

<p>2) Using the VGAM package</p>

<p>Here using the next commands I have the model under the assumption of proportional odds</p>

<pre><code>library(VGAM)
fit1 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=T))
summary(fit1)
</code></pre>

<p>And the results</p>

<pre><code>Coefficients:
                Estimate Std. Error  z value
(Intercept):1  0.1894723   0.820442  0.23094
(Intercept):2  2.0690395   0.886732  2.33333
Ki67          -0.0970972   0.032423 -2.99467
Cyclin_E       0.0075887   0.021521  0.35261

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 79.86801 on 80 degrees of freedom

Log-likelihood: -39.93401 on 80 degrees of freedom

Number of iterations: 5 
</code></pre>

<p>While using the next commands I have the model without the assumption of proportional odds</p>

<pre><code>fit2 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=F))
</code></pre>

<p>where unfortunately i receice the next message </p>

<blockquote>
  <p>Warning message: In vglm.fitter(x = x, y = y, w = w, offset = offset,
  Xm2 = Xm2,  :   convergence not obtained in 30 iterations</p>
</blockquote>

<p>However if I type <code>summary(fit2)</code> I get results but again I don't know if they are correct. My intention was to use the next commands and get the answer but know I doubt if this is correct (by the way if I do it I get <code>p-value=0.6</code>. </p>

<pre><code>pchisq(deviance(fit1)-deviance(fit2),
df=df.residual(fit1)-df.residual(fit2),lower.tail=FALSE)
</code></pre>

<p>So, regarding the methods mentioned above does anyone knows whether the results I get are valid or in the case of the VGAM package is there any way to increase the number of itterations?Is there any other way to check it? </p>
"
"0.226636788841842","0.213586467331736"," 27830","<p>In a previous post Iâ€™ve wondered how to <a href=""http://stats.stackexchange.com/questions/22494/is-using-a-questionnaire-score-euroqols-eq-5d-with-a-bimodal-distribution-as"">deal with EQ-5D scores</a>. Recently I stumbled upon logistic quantile regression suggested by <a href=""http://www.ncbi.nlm.nih.gov.proxy.kib.ki.se/pubmed/19941281"">Bottai and McKeown</a> that introduces an elegant way to deal with bounded outcomes.
The formula is simple:</p>

<p>$logit(y)=log(\frac{y-y_{min}}{y_{max}-y})$</p>

<p>To avoid log(0) and division by 0 you extend the range by a small value, $\epsilon$. This gives an environment that respects the boundaries of the score. </p>

<p>The problem is that any $\beta$ will be in the logit scale and that makes doesnâ€™t make any sense unless transformed back into the regular scale but that means that the $\beta$ will be non-linear. For graphing purposes this doesnâ€™t matter but not with more $\beta$:s this will be very inconvenient. </p>

<p>My question:</p>

<p><strong>How do you suggest to report a logit $\beta$ without reporting the full span?</strong></p>

<hr>

<h2>Implementation example</h2>

<p>For testing the implementation Iâ€™ve written a simulation based on this basic function:</p>

<p>$outcome=\beta_0+\beta_1* xtest^3+\beta_2*sex$</p>

<p>Where $\beta_0 = 0$, $\beta_1 = 0.5$ and $\beta_2 = 1$. Since there is a ceiling in scores Iâ€™ve set any outcome value above 4 and any below -1 to the max value.</p>

<h3>Simulate the data</h3>

<pre><code>set.seed(10)
intercept &lt;- 0
beta1 &lt;- 0.5
beta2 &lt;- 1
n = 1000
xtest &lt;- rnorm(n,1,1)
gender &lt;- factor(rbinom(n, 1, .4), labels=c(""Male"", ""Female""))
random_noise  &lt;- runif(n, -1,1)

# Add a ceiling and a floor to simulate a bound score
fake_ceiling &lt;- 4
fake_floor &lt;- -1

# Just to give the graphs the same look
my_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, 
             fake_ceiling + abs(fake_ceiling)*.25)
my_xlim &lt;- c(-1.5, 3.5)

# Simulate the predictor
linpred &lt;- intercept + beta1*xtest^3 + beta2*(gender == ""Female"") + random_noise
# Remove some extremes
linpred[linpred &gt; fake_ceiling + abs(diff(range(linpred)))/2 |
    linpred &lt; fake_floor - abs(diff(range(linpred)))/2 ] &lt;- NA
#limit the interval and give a ceiling and a floor effect similar to scores
linpred[linpred &gt; fake_ceiling] &lt;- fake_ceiling
linpred[linpred &lt; fake_floor] &lt;- fake_floor
</code></pre>

<p>To plot the above:</p>

<pre><code>library(ggplot2)
# Just to give all the graphs the same look
my_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, 
             fake_ceiling + abs(fake_ceiling)*.25)
my_xlim &lt;- c(-1.5, 3.5)
qplot(y=linpred, x=xtest, col=gender, ylab=""Outcome"")
</code></pre>

<p>Gives this image:</p>

<p><img src=""http://i.stack.imgur.com/luZGu.png"" alt=""Scatterplot from simulation""></p>

<h3>The regressions</h3>

<p>In this section I create the regular linear regression, quantile regression (using the median) and logistic quantile regression. All estimates are based on bootstrapped values using the bootcov() function.</p>

<pre><code>library(rms)

# Regular linear regression
fit_lm &lt;- Glm(linpred~rcs(xtest, 5)+gender, x=T, y=T)
boot_fit_lm &lt;- bootcov(fit_lm, B=500)
p &lt;- Predict(boot_fit_lm, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))
lm_plot &lt;- plot.Predict(p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)

# Quantile regression regular
fit_rq &lt;- Rq(formula(fit_lm), x=T, y=T)
boot_rq &lt;- bootcov(fit_rq, B=500)
# A little disturbing warning:
# In rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique

p &lt;- Predict(boot_rq, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))
rq_plot &lt;- plot.Predict(p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)

# The logit transformations
logit_fn &lt;- function(y, y_min, y_max, epsilon)
    log((y-(y_min-epsilon))/(y_max+epsilon-y))


antilogit_fn &lt;- function(antiy, y_min, y_max, epsilon)
    (exp(antiy)*(y_max+epsilon)+y_min-epsilon)/
        (1+exp(antiy))


epsilon &lt;- .0001
y_min &lt;- min(linpred, na.rm=T)
y_max &lt;- max(linpred, na.rm=T)
logit_linpred &lt;- logit_fn(linpred, 
                          y_min=y_min,
                          y_max=y_max,
                          epsilon=epsilon)

fit_rq_logit &lt;- update(fit_rq, logit_linpred ~ .)
boot_rq_logit &lt;- bootcov(fit_rq_logit, B=500)


p &lt;- Predict(boot_rq_logit, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))

# Change back to org. scale
transformed_p &lt;- p
transformed_p$yhat &lt;- antilogit_fn(p$yhat,
                                    y_min=y_min,
                                    y_max=y_max,
                                    epsilon=epsilon)
transformed_p$lower &lt;- antilogit_fn(p$lower, 
                                     y_min=y_min,
                                     y_max=y_max,
                                     epsilon=epsilon)
transformed_p$upper &lt;- antilogit_fn(p$upper, 
                                     y_min=y_min,
                                     y_max=y_max,
                                     epsilon=epsilon)

logit_rq_plot &lt;- plot.Predict(transformed_p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)
</code></pre>

<h3>The plots</h3>

<p>To compare with the base function Iâ€™ve added this code:</p>

<pre><code>library(lattice)
# Calculate the true lines
x &lt;- seq(min(xtest), max(xtest), by=.1)
y &lt;- beta1*x^3+intercept
y_female &lt;- y + beta2
y[y &gt; fake_ceiling] &lt;- fake_ceiling
y[y &lt; fake_floor] &lt;- fake_floor
y_female[y_female &gt; fake_ceiling] &lt;- fake_ceiling
y_female[y_female &lt; fake_floor] &lt;- fake_floor

tr_df &lt;- data.frame(x=x, y=y, y_female=y_female)
true_line_plot &lt;- xyplot(y  + y_female ~ x, 
                         data=tr_df,
                         type=""l"", 
                         xlim=my_xlim, 
                         ylim=my_ylim, 
                         ylab=""Outcome"", 
                         auto.key = list(
                           text = c(""Male"","" Female""),
                           columns=2))


# Just for making pretty graphs with the comparison plot
compareplot &lt;- function(regr_plot, regr_title, true_plot){
  print(regr_plot, position=c(0,0.5,1,1), more=T)
  trellis.focus(""toplevel"")
  panel.text(0.3, .8, regr_title, cex = 1.2, font = 2)
  trellis.unfocus()
  print(true_plot, position=c(0,0,1,.5), more=F)
  trellis.focus(""toplevel"")
  panel.text(0.3, .65, ""True line"", cex = 1.2, font = 2)
  trellis.unfocus()
}

compareplot(lm_plot, ""Linear regression"", true_line_plot)
compareplot(rq_plot, ""Quantile regression"", true_line_plot)
compareplot(logit_rq_plot, ""Logit - Quantile regression"", true_line_plot)
</code></pre>

<p><img src=""http://i.stack.imgur.com/74Uid.png"" alt=""Linear regression for bounded outcome""></p>

<p><img src=""http://i.stack.imgur.com/xHRtF.png"" alt=""Quantile regression for bounded outcome""></p>

<p><img src=""http://i.stack.imgur.com/XfLy8.png"" alt=""Logistic quantile regression for bounded outcome""></p>

<h3>The contrast output</h3>

<p>Now I've tried to get the contrast and it's almost ""right"" but it varies along the span as expected:</p>

<pre><code>&gt; contrast(boot_rq_logit, list(gender=levels(gender), 
+                              xtest=c(-1:1)), 
+          FUN=function(x)antilogit_fn(x, epsilon))
   gender xtest Contrast   S.E.       Lower      Upper       Z      Pr(&gt;|z|)
   Male   -1    -2.5001505 0.33677523 -3.1602179 -1.84008320  -7.42 0.0000  
   Female -1    -1.3020162 0.29623080 -1.8826179 -0.72141450  -4.40 0.0000  
   Male    0    -1.3384751 0.09748767 -1.5295474 -1.14740279 -13.73 0.0000  
*  Female  0    -0.1403408 0.09887240 -0.3341271  0.05344555  -1.42 0.1558  
   Male    1    -1.3308691 0.10810012 -1.5427414 -1.11899674 -12.31 0.0000  
*  Female  1    -0.1327348 0.07605115 -0.2817923  0.01632277  -1.75 0.0809  

Redundant contrasts are denoted by *

Confidence intervals are 0.95 individual intervals
</code></pre>
"
"0.0533760512683624","0.0546766551813808"," 31724","<p>Let's say I have a logistic regression model which predicts whether a consumer will buy an item based on about 10 consumer characteristics. </p>

<p>$$\begin{array}{rcl}Buy &amp;=&amp; B_0 + B_1\times Gender + B_2\times CreditType + B_3\times Education + B_4\times OwnsHome \\\phantom{Buy} &amp;&amp; + B_5\times CarMake + B_6\times CarYear + B_7\times State + B_8\times Income + B_9\times Insurance \\ \phantom{Buy} &amp;&amp;+ B_{10}\times CarAccidents\end{array} $$</p>

<ol>
<li><p>Is there ever an issue with including too many predictors in a logistic regression model? I'm not talking about insignificant variables or ones that may be related, but just the sheer number of variables included in a model. </p></li>
<li><p>With a larger number of predictors, how should one present the regression results in a meaningful manner? Is it just a matter of plotting the probability curve for $Y=1$, or are there ""better"" ways of doing this. I'd be doing this in R, so any help on that end would be appreciated.</p></li>
</ol>
"
"0.0800640769025436","0.0820149827720712"," 31735","<p>Related my earlier question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>. Having ""mastered"" linear regression, I'm trying to learn everything I can about logistic regression and am having issues turning largely ""useless"" coefficients into meaningful information.</p>

<p>I asked in my previous question about graphing the probability curve for every permutation of a logit model. However, I was working on just plotting the main curve and was having some issues. </p>

<p>If I was running a logit with just one predictor, I'd run the following:</p>

<pre><code>mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:250,predict(mod1,newdata=data.frame(bid&lt;-c(000:250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>However, what about with multiple predictors? I tried to use the mtcars data set to mess around and couldn't get it.</p>

<p>Any suggestion on how to plot the main probability curve for the logit model.</p>

<pre><code>head(mtcars)

m1 = glm(vs ~ disp + wt, data=mtcars, family=binomial(link=""logit""))
summary(m1)

all.x &lt;- expand.grid(vs=unique(mtcars$vs), disp=unique(mtcars$disp), wt=unique(mtcars$wt))

y.hat.new &lt;- predict(m1, newdata=all.x, type=""response"")
plot(disp&lt;-000:250,predict(m1,newdata=data.frame(disp&lt;-c(000:250), wt&lt;-c(0,250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>EDIT = OR is my previous question what I need to do. Since Y = B0 + B1X1 + B2X2, a one unit change in X1 is associated with a exp(B1) change in Y, regardless of the value of B2. Then would it be possible that my original question is really all that should be done. </p>

<p>My appologies on my stupidity if that is indeed the case.</p>
"
"0.0653720450460613","0.0669649530182425"," 34549","<p>I understand that there is a function in R called <code>poly()</code> that can generate orthogonal polynomials--useful for applying on input variables before running a predictive model.</p>

<p>My question is that what is the role of categorical variables when we generate polynomials? Are they to be excluded?</p>

<h2>Update:</h2>

<p>Dan, Thank you for your kind response. I'm not sure I understand it completely - let me explain the query in more detail. I'm trying to run logistic regression using glmnet on <a href=""http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls"" rel=""nofollow"">Titanic dataset</a>. <BR/> Let us assume shortened set of columns:<ul>* class(factor with three levels 1, 2 ,3), <br/>* sex(factor: male, female), <br/>* Age (integer), <br/>*survived(factor &amp; target variable 0 or 1).</ul> The questions is it meaningful to create polynomial features based on these factors? e.g. class. If yes could you pls explain what it means? <BR> I've seen examples with numeric input variables, where one can pass the entire input set to the poly() function and get polynomial features as output. <br/> Your response is highly appreciated.</p>
"
"0.11322770341446","0.0966555841283824"," 38500","<p>I am currently working with a logistic semi-parametric model in R using the mgcv package.  The output from the model gives the standard log-odds coefficients; however, reviewers have requested marginal effects (like the ones in Stata using the margins command).  I would like to do average marginal effects (though, marginal effects at the means--modes for categorical covariates--would at least be a start).</p>

<p>I was wondering if anyone has an implementation for this in R for models that use the gam() function.  I have a rather large data set (1.2 million observations, a large number of discrete and continuous covariates, and fixed effects for 2,000 individuals).  Are these estimates even tractable, given the non-parametric treatment of a couple of the continuous covariates?  Any information would be helpful.</p>

<p>Here is what I am working with, though I get errors when attempting to do the bootstrapped SEs for the effects (comes from this helpful site probitlogit-marginal-effects-in-r-2/).  I am not sure how this treats the non-parametrically estimated smooths (but maybe these don't matter, since it is using the ""predict"" function?):</p>

<pre><code>mfxboot &lt;- function(modform,dist,data,boot=1000,digits=3){ #dist is the distribution choice of logit or probit
      require(mgcv)

x &lt;- gam(modform, family=binomial(link=dist),method=""GCV.Cp"",data)
      # get marginal effects

pdf &lt;- ifelse(dist==""probit"",               
 mean(dnorm(predict(x, type = ""link"")))            
 mean(dlogis(predict(x, type = ""link"")))
 marginal.effects &lt;- pdf*coef(x)


bootvals &lt;- matrix(rep(NA,boot*length(coef(x))), nrow=boot)  
set.seed(1111)  
for(i in 1:boot){    
samp1 &lt;- data[sample(1:dim(data)[1],replace=T,dim(data)[1]),]    
x1 &lt;- gam(modform, family=binomial(link=dist),method=""GCV.Cp"",samp1)    
pdf1 &lt;- ifelse(dist==""probit"",                   
mean(dnorm(predict(x1, type = ""link""))),                   
mean(dlogis(predict(x1, type = ""link""))))       
bootvals[i,] &lt;- pdf1*coef(x1)     
}

res &lt;- cbind(marginal.effects,apply(bootvals,2,sd),marginal.effects/apply(bootvals,2,sd))     
if(names(x$coefficients[1])==""(Intercept)""){        
res1 &lt;- res[2:nrow(res),]    
res2 &lt;- matrix(as.numeric(sprintf(paste(""%."",paste(digits,""f"",sep=""""),sep=""""),res1)),nrow=dim(res1)[1])         
rownames(res2) &lt;- rownames(res1)        
} else {    
res2 &lt;- matrix(as.numeric(sprintf(paste(""%."",paste(digits,""f"",sep=""""),sep="""")),nrow=dim(res)[1]))       
rownames(res2) &lt;- rownames(res)    
}     
colnames(res2) &lt;- c(""marginal.effect"",""standard.error"",""z.ratio"") 
      return(res2)
}
</code></pre>
"
"0.166666666666667","0.170727801083421"," 40499","<p>When using the <code>step.plr()</code> function in the <a href=""http://cran.r-project.org/web/packages/stepPlr/index.html"" rel=""nofollow"">stepPlr</a> package, if my predictors are factors, do I need to encode my predictors as dummy variables manually before passing it to the function? I do know that I can specify ""level"", but how  the ""level"" parameter works is confusing to me. 
My understanding is that I need to tell <code>step.plr()</code> explicitly which factors should be encoded as dummy variables and thus leaving one factor out intentionally. </p>

<p>Let's consider a simple example. Suppose I have 1 categorical predicator with 4 levels and binary response. Normally, if I use <code>glm()</code> to fit a logistic regression model, <code>glm()</code> would automatically convert the categorical predicator into 3 dummy variables. Now in <code>stepPlr()</code>, do I specify the ""level"" parameter for that predictor with 4 levels or 3 levels? The ""Help"" section is vague, and says: </p>

<blockquote>
  <p>If the j-th column of x is discrete, level[[ j ]] is the set of levels for the categorical factor.</p>
</blockquote>

<p>Does it mean I should tell <code>step.plr()</code> about all 4 levels, or I should make an intelligent decision myself and tell <code>step.plr()</code> to use only 3 levels? </p>

<p>==============UPDATE (16 Oct 2012)=============</p>

<p>The following example will demonstrate what is the problem with <code>step.plr()</code>'s automatic dummy variable encoding. It is a slight modification of the code in the function's help section. 
     set.seed(100)</p>

<pre><code>n &lt;- 100
p &lt;- 3
z &lt;- matrix(sample(seq(3),n*p,replace=TRUE),nrow=n)
x &lt;- data.frame(x1=factor(z[ ,1]),x2=factor(z[ ,2]),
                x3=factor(sample(seq(3), n, replace=TRUE, prob=c(0.2, 0.5, 0.3))),
                x4=factor(sample(seq(3), n, replace=TRUE, prob=c(0.1, 0.3, 0.6))))
y &lt;- sample(c(0,1),n,replace=TRUE)
fit &lt;- step.plr(x,y, cp=""aic"")
summary(fit)
</code></pre>

<p>And here's an excerpt of the result:</p>

<pre><code>Call:
plr(x = ix0, y = y, weights = weights, offset.subset = offset.subset, 
    offset.coefficients = offset.coefficients, lambda = lambda, 
    cp = cp)

Coefficients:
      Estimate Std.Error z value Pr(&gt;|z|)
Intercept  0.91386   5.04780   0.181    0.856
x4.1       1.33787   4.61089   0.290    0.772
x4.2      -1.70462   4.91240  -0.347    0.729
x4.3       0.36675   3.18857   0.115    0.908
x3.1:x4.1  7.04901  14.35112   0.491    0.623
x3.1:x4.2 -5.50973  15.53674  -0.355    0.723
x3.1:x4.3 -0.50012   7.95651  -0.063    0.950
</code></pre>

<p>You can see that all levels, that is, (1,2,3), are used to fit the model. But normally you only need two dummy variables to encode a predictor with 3 levels.
On the other hand, if you use <code>glm()</code>: </p>

<pre><code>glm(y~.^2, data=x, family=binomial)
</code></pre>

<p>you will get the correct dummy variable encoding.</p>
"
"0.11322770341446","0.115986700954059"," 41561","<p>I am fitting a logistic model to data using the glm function in R.   I have attempted to specify interaction terms in two ways:</p>

<pre><code>fit1 &lt;- glm(y ~ x*z, family = ""binomial"", data = myData) 
fit2 &lt;- glm(y ~ x/z, family = ""binomial"", data = myData) 
</code></pre>

<p>I have 3 questions:</p>

<p>1) What is the difference between specifying my interaction terms as x*z compared to x/d?</p>

<p>When I call summary(fit1) the report includes results for the intercept, x, z, and x:z while summary(fit2) only includes results for intercept, x, and x:z.</p>

<p>I did look at Section 11.1 in ""An Introduction to R"" but couldn't understand the meaning. </p>

<p>2) How do I interpret the fit equation mathematically?  Specifically, how do I interpret the interaction terms formulaically?</p>

<p>Moving to math instead of R, do I interpret the equation as:
logit(y) = (intercept) + (coeff_x)*x + (coeff_z)*x + (coeff_xz)*x*z
?</p>

<p>This interpration may differ in the two specifications fit1 and fit2.  What is the interpretation of each?</p>

<p>3) Assuming the above interpretation is correct, how to I fit the model of x*(1/z) in R?  Do I need to just create another column with these values?</p>
"
"0.235864088262432","0.250559911115677"," 43040","<p>I need some guidance related to regression model verification using validation data. 
I am new to R-tool &amp; statistics and trying my best to learn. I did search on internet too but I couldn't get a final answer to my questions. 
Actually I have a lot of questions, I may try my best to explain the problems:
I am experimenting with network packets and R-tool.
I have captured some packets from a network using a custom made packet sniffer in java. The sniffer will capture some packets and save the information of packet header like: tcp window size, tcp sequence numbers, date-time, ip header length, ip time to live etc... in a csv file.</p>

<p>Also the sniffer will add category number to each csv file so that we can know which packet belongs to which category. I created 9 different categories saved in 9 different csv files.
Now I extracted 1000 observation from each of the csv files and created a data set named ""alldata"".</p>

<p>Then I created training data set and validation data set from ""alldata"" data set.</p>

<p>Now I want to perform linear regression, logistic regression, decision tree analysis, cluster analysis etc on this ""alldata"" data set.</p>

<p>So my plan was to use training data set to create models and then later use validation data set to verify my models. </p>

<p>Category will be my target variable in any case. I want to predict the category from other independent variables.</p>

<ol>
<li><p>My first confusion is that after I created scatter plot of category with other independent variables and I don't see any linear relationship between them. Moreover I even don't know what relation exists between category and independent variables. From scatter plots it seems to me that there is no specific relation between category and other independent variables(except date_time it is bit linear to category). Am I doing the correct interpretation ?
Here are some of the plots:
<a href=""http://imageshack.us/photo/my-images/211/tcpdport.png/"" rel=""nofollow"">plot 1</a>
<a href=""http://imageshack.us/photo/my-images/547/tcpchksum.png/"" rel=""nofollow"">plot 2</a></p></li>
<li><p>I think doing linear regression won't make any sense now after having a look at scatter plots. Is this correct assumption?</p></li>
<li><p>Although I tried to do make some regression models with training data set, but the R-square values for all the models is quite low (for example like 0.00019, 0.0035, 0.018 etc. ) 
So can I assume that these models are not good due to very low r-square vales?</p></li>
<li><p>As logistic regression is used when we have target variables having only two values 1 or 0, or some probabilities between 0.0 - 1.0.
This means performing logistic regression is not possible for this type of data set.
Is my assumption true?</p></li>
<li><p>My main question was how to verify a model created with training data set by using validation data set?
Please let me know the commands and the procedure.
Please let me know if I am doing this in wrong way or if you can suggest me a better way to do this whole work. I think if someone could please clear my doubts then I may ask further more questions.</p></li>
</ol>

<p>If you don't understand my problem we can discuss in more detail
I look forward for your replies.
Thank you!</p>

<hr>

<p>@Wayne</p>

<p>Hello thanks for the reply, but the thing is for each category I have almost same range of values of independent variables like(tcpheader, ipttl, iplen). For example iptype is only having two values 6 and 17. So most of the categories are having iptype value of 6 &amp; 17.
So it is also same is for tcpheader, tcp sequence number, tcp acknowledgement number etc. I don't think there is any way to distinguish a particular packet based on these independent variables. Only the independent variable that can be helpful is time.
But when I created a model with time it had good r-squared value but the regression line equation doesn't predict category with any value of date_time.
I don't understand this behaviour.</p>

<p>Thanks.</p>
"
"0.0924500327042048","0.0947027447620757"," 44359","<p>So I have data from a randomized blind trial of 1mg of nicotine gum on dual n-back working memory scores; I analyzed them as usual with a t-test and found a small increase in means but a large increase in standard deviations on a f-test! Strange. I also have data for each day on mood/productivity that day on a 1-5 scale.</p>

<p>I wondered: is nicotine following an inverse U-curve, where it causes higher scores on the worser days (1-3) and lower scores on the better days (3-5)? I look around and it seems I want a multinomial logistic regression comparing the placebo &amp; active days.</p>

<p>I enter the data &amp; load <code>mlogit</code>:</p>

<pre><code>nicotine &lt;- read.table(stdin(),header=TRUE)
day      active mp score
20120824 1      3  35.2
20120827 0      5  37.2
20120828 0      3  37.6
20120830 1      3  37.75
20120831 1      2  37.75
20120902 0      2  36.0
20120905 0      5  36.0
20120906 1      5  37.25
20120910 0      5  49.2
20120911 1      3  36.8
20120912 0      3  44.6
20120913 0      5  38.4
20120915 0      5  43.8
20120916 0      2  39.6
20120918 0      3  49.6
20120919 0      4  38.4
20120923 0      5  36.2
20120924 0      5  45.4
20120925 1      3  43.8
20120926 0      4  36.4
20120929 1      3  43.8
20120930 1      3  36.0
20121001 1      3  46.0
20121002 0      4  45.0
20121008 0      2  34.6
20121009 1      3  45.2
20121012 0      5  37.8
20121013 0      4  37.2
20121016 0      4  40.2
20121020 1      3  39.0
20121021 0      3  41.2
20121022 0      3  42.2
20121024 0      5  40.4
20121029 1      2  41.4
20121031 1      3  38.4
20121101 1      5  43.8
20121102 0      3  48.2
20121103 1      5  40.6

library(mlogit)
Nicotine &lt;- mlogit.data(nicotine,shape=""wide"", choice=""mp"")
mlogit(score ~ (active + mp)^2, Nicotine)
Error in solve.default(H, g[!fixed]) : 
  Lapack routine dgesv: system is exactly singular
Calls: mlogit ... mlogit.optim -&gt; as.vector -&gt; solve -&gt; solve.default
</code></pre>

<p>The error also happens even with the simplest call I can think of:</p>

<pre><code>mlogit(score ~ active, Nicotine)
Error in solve.default(H, g[!fixed]) : 
  Lapack routine dgesv: system is exactly singular
Calls: mlogit ... mlogit.optim -&gt; as.vector -&gt; solve -&gt; solve.default
</code></pre>

<p>Reading the documentation for <code>mlogit</code> didn't much help, and look at the other questions having the same error, they're different enough I can't tell whether they apply or not.</p>

<p>Thank you for your assistance.</p>
"
"0.0924500327042048","0.0710270585715568"," 46205","<p>I have a large amount of vegetation data that has been broken down into 13 habitat classes. I am trying to determine which vegetation tends to fall into or is absent from which habitat with any sort of significance. I have been put onto running a multinomial logistic regression, specifically using glmnet (as I have approximately 200 variables, and only about 260 observations).</p>

<p>Running cv.glmnet using the code:</p>

<pre><code>cv&lt;-cv.glmnet(data,Class,family=""multinomial"",nfolds=50,standardize=FALSE)
</code></pre>

<p>I get a list of numbers that I am struggling to understand, however I found the code:</p>

<pre><code>coef(cv, s=cv$lambda.1se)
</code></pre>

<p>Which returns the coefficients for each variable for each habitat class for the lambda that is 1 SE larger than the minimum Lambda value (which as far as I can tell the generally accepted lambda value).</p>

<pre><code>(Intercept)                                              0.7914263664   
Salix                                                    0.0000000000  
Mash                                                     0.0000000000   
Pin                                                      0.0000000000   
Choke                                                    .          
Betula                                                   0.0025260258   
Ideae                                                    0.0000000000   
Leather                                                  0.0000000000
</code></pre>

<p>What I'm wondering, using these coefficients, is it possible to state that those values with the largest magnitude (either closest to -1 and +1) are the most important in defining that class, which those close to 0 are unimportant, and those with periods were removed during the cv.glmnet. So in this case the plant ""Betula"" would be more influential than all others, and ""Choke"" was so uninfluential that it was removed? Also, no idea what intercept means, but I imagine I can find that one on my own.</p>
"
"0.173343811320384","0.189405489524151"," 48381","<p>I'm trying to estimate power in a logistic regression with a continuous exposure in a cohort study (ie, the ratio of the sampling probabilities is 1). I have population cumulative incidence (probability) and population exposure variability and exposure mean and an expected odds ratio. I also have a total sample size.</p>

<p>I'm using R and it seems like <code>Hmisc::bpower</code> is only for logistic regression with binary exposure and I can't seem to find any packages that estimate binomial power with continuous exposure.</p>

<p>I've attempted the following simulation but it's quite slow given my total sample size and I'm not sure if it's right:</p>

<pre><code>p &lt;- vector()
betahat &lt;- vector()
for(i in 1:1000){
n &lt;- 40000  #total sample size
intercept = log(0.008662265)  #where exp(intercept) = P(D=1)
beta &lt;- log(1.4) #where exp(beta)=OR corresponding to a one unit change in xtest
xtest &lt;- rnorm(n,1.2,.31)  #xtest is vector length 40,000 with mean 1.2 and sd .31
linpred &lt;- intercept + xtest*beta #linear predictor
prob &lt;- exp(linpred)/(1 + exp(linpred)) #link function
runis &lt;- runif(n,0,1) #generate a vector length n from a uniform distribution 0,1
ytest &lt;- ifelse(runis &lt; prob,1,0)  #if a random value from a uniform distribution 0,1 is less than prob, then the outcome is 1.  otherwise the outcome is 0
coefs &lt;- coef(summary(glm(ytest~xtest, family=""binomial"")))  #run a logistic regression
p[i] &lt;- coefs[2,4] #store the p value
betahat[i] &lt;- coefs[2,1] #store the unexponentiated betahat
}

mean(p &lt; .05)
#power

exp(mean(betahat))
#sanity check, should equal 1.4--it does
</code></pre>

<p>Is there anything wrong with this approach?</p>

<p>One concern of mine is that the cumulative incidence (ie, probability of event over the given time period) comes from a population that did not have 0 exposure.  In fact, it's reasonable to assume that the value i'm using for an intercept is actually from a population that has an exposure variability similar to mine. In that case, how would I estimate the unexposed probability given an odds ratio (and other information that I would find in say, a published paper) to use in my power calculation?</p>
"
"0.146784140987997","0.164029965544142"," 48766","<p>My logistic model <a href=""http://stats.stackexchange.com/q/48739/5509"">has been suspicious</a> due to enormous coefficients, so I tried to do a crossvalidation, and also do a crossvalidation of simplified model, to confirm the fact that the original model is overspecified, as <a href=""http://stats.stackexchange.com/a/48741/5509"">James suggested</a>. However, I don't know how to interpret the result (this is the model from the linked question):</p>

<pre><code>&gt; summary(m5)

Call:
glm(formula = cbind(ml, ad) ~ rok + obdobi + kraj + resid_usili2 + 
    rok:obdobi + rok:kraj + obdobi:kraj + kraj:resid_usili2 + 
    rok:obdobi:kraj, family = ""quasibinomial"")
[... see http://stats.stackexchange.com/q/48739/5509 for complete summary output ]

&gt; cv.glm(na.omit(data.frame(orel, resid_usili2)), m5, K = 10)
$call
cv.glm(data = na.omit(data.frame(orel, resid_usili2)), glmfit = m5, 
    K = 10)

$K
[1] 10

$delta
[1] 0.2415355 0.2151626

$seed
  [1]         403         271  1234892862 -1124595763  -489713400  1566924080   147612843
  [8]  1879282918  -694084381  1171051622  2063023839 -1307030905  -477709428  1248673977
 [15]  -746898494   420363755  -890078828   460552896  -758793089  -913500073  -882355605
[....]
Warning message:
glm.fit: algorithm did not converge
</code></pre>

<p>I guess the delta is the mean fitting error, but how to interpret it? Is it a good or bad fit? BTW, the algorithm did not converge, maybe due to the enormous coefficients (?)</p>

<p>I tried a simplified model:</p>

<pre><code>&gt; summary(m)

Call:
glm(formula = cbind(ml, ad) ~ rok + obdobi + kraj, family = ""quasibinomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.7335  -1.2324  -0.1666   1.0866   3.1788  

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -107.60761   48.06535  -2.239 0.025335 *  
rok            0.05381    0.02393   2.249 0.024683 *  
obdobinehn    -0.26962    0.10372  -2.599 0.009441 ** 
krajJHC        0.68869    0.27617   2.494 0.012761 *  
krajJHM       -0.26607    0.28647  -0.929 0.353169    
krajLBK       -1.11305    0.55165  -2.018 0.043828 *  
krajMSK       -0.61390    0.37252  -1.648 0.099593 .  
krajOLK       -0.49704    0.32935  -1.509 0.131501    
krajPAK       -1.18444    0.35090  -3.375 0.000758 ***
krajPLK       -1.28668    0.44238  -2.909 0.003691 ** 
krajSTC        0.01872    0.27806   0.067 0.946322    
krajULKV      -0.41950    0.61647  -0.680 0.496315    
krajVYS       -1.17290    0.39733  -2.952 0.003213 ** 
krajZLK       -0.38170    0.36487  -1.046 0.295698    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for quasibinomial family taken to be 1.304775)

    Null deviance: 2396.8  on 1343  degrees of freedom
Residual deviance: 2198.6  on 1330  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 4
</code></pre>

<p>and it's crossvalidation:</p>

<pre><code>&gt; cv.glm(orel, m, K = 10)
$call
cv.glm(data = orel, glmfit = m, K = 10)

$K
[1] 10

$delta
[1] 0.2156313 0.2154078

$seed
  [1]         403         526   300751243  -244464717  1066448079  1971573706 -1154513152
  [8]   634841816 -1521293072 -1040655077   505710009  -323431793 -1218609191  1060964279
 [15]  1349082996   -32847357 -1387496845   821178952  -971482876  1295018851  1380491861
</code></pre>

<p>Now it converged. But the delta seems more or less the same, despite of the fact that this model looks much more sane! I'm confused by the crossvalidation now... please give me a hint on how interpret it.</p>
"
"0.138675049056307","0.142054117143114"," 49497","<p>I have a dataset I'm working on that has some co-variate shift between the training set and the test set.  I'm trying to build a predictive model to predict an outcome, using the training set.  So far my best model is a random forest.</p>

<p>How can I deal with the shifted distributions in the training vs. test set?  I've come across 2 possible solutions that I've been able to implement myself:</p>

<ol>
<li>Remove the shifted variables.  This is sub-optimal, but helps prevent my model from over fitting the training set.</li>
<li>Use a logistic regression to predict whether a given observation is from the test set (after balancing the classes), predict ""test set probabilities"" for the training set, and then boostrap sample the training set, using the probabilities for sampling.  Then fit the final model on the new training set.</li>
</ol>

<p>Both 1 and 2 are pretty easy to implement, but neither one satisfies me, as #1 omits variables that might be relevant, and #2 uses a logistic regression, when my final model is tree-based.  Furthermore, #2 takes a few paragraphs of custom code, and I worry that my implementation may not be correct.</p>

<p>What are the standard methods for dealing with covariate shift?  Are there any packages in R (or another language) that implement these methods?</p>

<p>/edit: It seems like ""kernel mean matching"" is another approach I could take.  I've found lots of academic papers on the subject, but no one seems to have published any code.  I'm going to try to implement this on my own, and will post the code as an answer to this question when I do.</p>
"
"0.0924500327042048","0.0947027447620757"," 50726","<p>I'm using the <code>lme4</code> package in R to do some logistic mixed-effects modeling.<br>
My understanding was that sum of each random effects should be zero.</p>

<p>When I make toy linear mixed-models using <code>lmer</code>, the random effects are usually &lt; $10^{-10}$ confirming my belief that the <code>colSums(ranef(model)$groups) ~ 0</code>
But in toy binomial models (and in models of my real binomial data) some of the random effect sum to ~0.9. </p>

<p>Should I be concerned?  How do I interpret this?  </p>

<p>Here is a linear toy example
<code><pre>
toylin&lt;-function(n=30,gn=10,doplot=FALSE){
 require(lme4)
 x=runif(n,0,1000)
 y1=matrix(0,gn,n)
 y2=y1
 for (gx in 1:gn)
 {
   y1[gx,]=2*x*(1+(gx-5.5)/10) + gx-5.5  + rnorm(n,sd=10)
   y2[gx,]=3*x*(1+(gx-5.5)/10) * runif(1,1,10)  + rnorm(n,sd=20)
 }
 c1=y1*0;
 c2=y2*0+1;
 y=c(t(y1[c(1:gn),]),t(y2[c(1:gn),]))
 g=rep(1:gn,each=n,times=2)
 x=rep(x,times=gn*2)
 c=c(c1,c2)
 df=data.frame(list(x=x,y=y,c=factor(c),g=factor(g)))
 (m=lmer(y~x*c + (x*c|g),data=df))
 if (doplot==TRUE)
  {require(lattice)
   df$fit=fitted(m)
   plot1=xyplot(fit ~ x|g,data=df,group=c,pch=19,cex=.1)
   plot2=xyplot(y ~ x|g,data=df,group=c)
   print(plot1+plot2)
  }
 print(colMeans(ranef(m)$g))
 m
}
</code></pre></p>

<p>In this case the colMeans always come out $&lt;10^{-6}$ </p>

<p>Here is a binomial toy example (I would share my actual data, but it is being submitted for publication and I am not sure what the journal policy is on posting beforehand):</p>

<p><pre><code>
toybin&lt;-function(n=100,gn=4,doplot=FALSE){
  require(lme4)<br>
  x=runif(n,-16,16)
  y1=matrix(0,gn,n)
  y2=y1
  for (gx in 1:gn)
  { com=runif(1,1,5)
    ucom=runif(1,1,5)
    y1[gx,]=tanh(x/(com+ucom) + rnorm(1)) > runif(x,-1,1)
    y2[gx,]=tanh(2*(x+2)/com + rnorm(1)) > runif(x,-1,1)
  }
  c1=y1*0;
  c2=y2*0+1;
  y=c(t(y1[c(1:gn),]),t(y2[c(1:gn),]))
  g=rep(1:gn,each=n,times=2)
  x=rep(x,times=gn*2)
  c=c(c1,c2)
  df=data.frame(list(x=x,y=y,c=factor(c),g=factor(g)))
  (m=lmer(y~x*c + (x*c|g),data=df,family=binomial))
  if (doplot==TRUE)
   {require(lattice)
    df$fit=fitted(m)
    print(xyplot(fit ~ x|g,data=df,group=c,pch=19,cex=.1))
   }
  print(colMeans(ranef(m)$g))
  m
}
</pre></code></p>

<p>Now the colMeans sometimes come out above 0.3, and definitely higher, on average than the linear example.</p>
"
"0.0800640769025436","0.0820149827720712"," 51786","<p>Does anyone know what exact data cleaning steps one need to undertake in order to clean data for a logit regression (not a logistic regression)?</p>

<p>I have only time variables, meaning year and month, as my independent variables, and I am using R.</p>

<p>A logit regression is simply a normal linear regression where the DV have been transformed with the following formula:</p>

<blockquote>
  <p><code>logit(y) = ln(y/(1-y)</code> for </p>
</blockquote>

<p>An example:</p>

<blockquote>
  <p>3 of 12 people gets cured from taking a pill in period 3 ->
  <code>ln(0.25/(1-0.25)</code></p>
  
  <p>5 of 25 people gets cured taking a pill in period 5 ->
  <code>ln(0.20/(1-0.20)</code></p>
</blockquote>

<p>One can use the logit transformation if you have ratios and in many papers and books it is closely related to the logistic regression.</p>
"
"0.0462250163521024","0.0473513723810378"," 52785","<p>I'm only a linguist, so my knowledge of statistics is very basic.</p>

<p>I fitted a logistic regression model with R (with <code>lrm(formula, y=T, x=T)</code>), and when I use the option <code>validate(lrm)</code>, I get some statistics I don't really understand.</p>

<pre><code>index.orig   training   test optimism index.corrected     n
Dxy           0.5984   0.6112  0.5461   0.0651          0.5333 40
R2            0.3258   0.3676  0.2929   0.0747          0.2511 40
Intercept     0.0000   0.0000 -0.0105   0.0105         -0.0105 40
Slope         1.0000   1.0000  0.8427   0.1573          0.8427 40
Emax          0.0000   0.0000  0.0399   0.0399          0.0399 40
D             0.2713   0.3176  0.2394   0.0782          0.1931 40
U            -0.0177  -0.0177  0.0092  -0.0269          0.0092 40
Q             0.2890   0.3353  0.2302   0.1051          0.1839 40
B             0.1864   0.1772  0.1972  -0.0201          0.2064 40
g             1.4632   1.6642  1.3460   0.3182          1.1449 40
gp            0.2840   0.3011  0.2703   0.0308          0.2532 40
</code></pre>

<p>I don't really understand most of that. I think <code>R2</code> and <code>Dxy</code> are supposed to be statistics of how good the predictors are, but I'm not sure how I should interpret the values, does the corrected <code>Dyx = 0.651</code> mean that there is a strong correlation, while the corrected <code>R2 = 0.0747</code> means that the correlation is very weak? I think the model is overfitted, but I'm not sure if I'm right.</p>

<p>Also, the other statistics are totally strange to me. What are <code>Emax, D, U, Q, B, g</code>, and <code>gp</code>?</p>
"
"0.138675049056307","0.126270326349434"," 56055","<p>I have data from a randomized survey experiment in which each respondent was assigned to one of 4 groups, one of which can be considered a ""control"" or ""no treatment"" group. The key question asked in the survey was a binary one: i.e. each respondent was faced with a choice between two products given some stimulus based on the assigned group. Of course, there are several other questions to be controlled for (demographics, pre-existing preferences, etc.).</p>

<p>I want to know what effect, if any, being in a particular group had on the respondent's choice for that key question, controlling for the other factors. Since my response variable is categorical I can't use ANOVA (at least R doesn't appear willing to let me have a non-numeric response variable). I have tried to do a logistic regression but it seems like the structure of my data means that this would result in the respondents in each group being compared to the rest of the respondents which seems like it would be incorrect.</p>

<p>My data resembles the following in structure:</p>

<pre><code>| Id | Group | Product Chosen | ... (other variables)
| 1  |     1 | A              | ...
| 2  |     4 | B              | ...
| 3  |     3 | B              | ...
| 4  |     2 | B              | ...
| 5  |     1 | A              | ...
| 5  |     2 | B              | ...
| 5  |     4 | A              | ...
| 5  |     3 | B              | ...
</code></pre>

<p>etc.</p>

<p>In case it is relevant, I have been using R for my analysis.</p>

<p><strong>Update:</strong> Just so it's clear, my working hypothesis is that respondents in non-control groups were more likely to choose product A than B (and less importantly, but similarly, that respondents in group 2 were more likely than those in group 3, and those in group 3 were more likely than those in group 4).</p>
"
"0.0653720450460613","0.0669649530182425"," 58151","<p>I'm using the well-known USC Burns Survival dataset to explore logistic regression in R.</p>

<p>The independent variable is burn area, and the outcome is binary survival (yes/no).</p>

<p>In the documentation, burn areas are grouped as the 'midpoint of set intervals', and taken as a log ie.</p>

<pre><code>log(area + 1)
</code></pre>

<p>for sample datapoints like (highest/lowest):</p>

<pre><code>1.35    yes
2.35    no
</code></pre>

<p>Medically, burns are usually specifed in terms of surface area % of total body eg. 40%, or alternatively, as an estimate of surface area in square meters. My question is: how does this relate to the dataset independent variable ie. what does '1.35' actually mean in terms of % body surface area burnt? What would eg. 30% burns become in the dataset, using the 'midpoint of set interval'?</p>

<p>Thanks guys</p>

<p>Ref: <a href=""http://statmaster.sdu.dk/courses/st111/data/index.html#burns"" rel=""nofollow"">http://statmaster.sdu.dk/courses/st111/data/index.html#burns</a></p>
"
"0.0653720450460613","0.0669649530182425"," 60109","<p>I would like to understand what the following code is doing. The person who wrote the code no longer works here and it is almost completely undocumented. I was asked to investigate it by someone who thinks ""<em>it's a bayesian logistic regression model</em>""</p>

<pre><code>bglm &lt;- function(Y,X) {
    # Y is a vector of binary responses
    # X is a design matrix

    fit &lt;- glm.fit(X,Y, family = binomial(link = logit))
    beta &lt;- coef(fit)
    fs &lt;- summary.glm(fit)
    M &lt;- t(chol(fs$cov.unscaled))
    betastar &lt;- beta + M %*% rnorm(ncol(M))
    p &lt;- 1/(1 + exp(-(X %*% betastar)))
    return(runif(length(p)) &lt;= p)
}
</code></pre>

<p>I can see that it fits a logistic model, takes the transpose of the Cholseky factorisation of the estimated covariance matrix, post-multiplies this by a vector of draws from $N(0,1)$ and is then added to the model estimates. This is then premultiplied by the design matrix, the inverse logit of this is taken, compared with a vector of draws from $U(0,1)$ and the resulting binary vector returned. But what does all this <strong><em>mean</em></strong> statistically ?</p>
"
"0.0800640769025436","0.0820149827720712"," 60817","<p>I am having trouble interpreting the z values for categorical variables in logistic regression. In the example below I have a categorical variable with 3 classes and according to the z value, CLASS2 might be relevant while the others are not. </p>

<p>But now what does this mean?</p>

<p>That I could merge the other classes to one? <br>
That the whole variable might not be a good predictor?</p>

<p>This is just an example and the actual z values here are not from a real problem, I just have difficulties about their interpretation.  </p>

<pre><code>           Estimate    Std. Error  z value Pr(&gt;|z|)    
CLASS0     6.069e-02  1.564e-01   0.388   0.6979    
CLASS1     1.734e-01  2.630e-01   0.659   0.5098    
CLASS2     1.597e+00  6.354e-01   2.514   0.0119 *  
</code></pre>
"
"0.153846153846154","0.144461985532126"," 61138","<p>I am trying to calculate the marginal effects of a multinomial logistic regression. To do this I use the <code>mlogit</code> package and the <code>effects()</code> function.</p>

<p>Here is how the procedure works (source : <code>effects()</code> function of <code>mlogit</code> package) :</p>

<pre><code>data(""Fishing"", package = ""mlogit"")
Fish &lt;- mlogit.data(Fishing, varying = c(2:9), shape = ""wide"", choice = ""mode"")
m &lt;- mlogit(mode ~ price | income | catch, data = Fish)
# compute a data.frame containing the mean value of the covariates in the sample
z &lt;- with(Fish, data.frame(price = tapply(price, index(m)$alt, mean), 
	catch = tapply(catch, index(m)$alt, mean), 
income = mean(income)))
# compute the marginal effects (the second one is an elasticity
effects(m, covariate = ""income"", data = z)
effects(m, covariate = ""price"", type = ""rr"", data = z)
effects(m, covariate = ""catch"", type = ""ar"", data = z)
</code></pre>

<p>I have no problem with first step (<code>mlogit.data()</code> function). I think my problem is in the specification of the multinomial regression.</p>

<p>My regression (for example with three variables) is on the form: <code>Y ~ 0 | X1 + X2 + X3</code>. When I try to estimate the marginal effects for a model with 2 variables, there is no problem, however for 3 variables R console returns me the following error: ""Error in if (rhs% in% c (1, 3)) {: argument is of length zero "" (translation from error in R console in french).</p>

<p>To understand what is my problem I tried to perform a multinomial regression of similar shape on the dataset ""Fishing"", i.e.,: <code>mode ~ 0 | income + price + catch</code> (even if this form has no ""economic"" sense.) Again the R console returns me the same error for 3 variables but manages to estimate these effects for a model with two variables.</p>

<p>This leads me to think that my problem really comes from the specification of my multinomial regression.  Do you know how I could find a solution to my problem? Or could you suggest another logit multinomial regression form ?</p>

<p>Thank you for your help :)</p>
"
"0.0462250163521024","0.0473513723810378"," 61144","<p>I am a biology student. We do many Enzyme Linked Immunosorbent Assay (ELISA) experiments and Bradford detection. A 4-parametric logistic regression (<a href=""http://www.miraibio.com/blog/2010/08/the-4-parameter-logistic-4pl-nonlinear-regression-model/"" rel=""nofollow"">reference</a>) is often used for regression these data following this function:
$$
F(x) = \left(\frac{A-D}{1+(x/C)^B}\right) + D 
$$
How can I do this in <code>R</code>? I want to get the $A$, $B$, $C$ and $D$ values and plot the curve.</p>

<p>PS. If I have some data, how can I use the calculated function $F(x)$ to get the value? I mean how do I go from ""data -> F(x) -> value""?</p>
"
"0.0462250163521024","0.0473513723810378"," 61845","<p>I am trying to run a Cox regression on a sample 2,000,000 row dataset as follows using only R. This is a direct translation of a PHREG in SAS. The sample is representative of the structure of the original dataset.</p>

<pre><code>##
library(survival)

### Replace 100000 by 2,000,000

test &lt;- data.frame(start=runif(100000,1,100), stop=runif(100000,101,300), censor=round(runif(100000,0,1)), testfactor=round(runif(100000,1,11)))

test$testfactorf &lt;- as.factor(test$testfactor)
summ &lt;- coxph(Surv(start,stop,censor) ~ relevel(testfactorf, 2), test)

# summary(summ)
##

user  system elapsed 
9.400   0.090   9.481 
</code></pre>

<p>The main challenge is in the compute time for the original dataset (2m rows). As far as I understand, in SAS this could take up to 1 day, ... but at least it finishes.</p>

<ul>
<li><p>Running the example with only 100,000 observations take only 9 seconds. Thereafter the time increases almost quadratically for every 100,000 increment in the number of observations.</p></li>
<li><p>I have not found any means to parallelize the operation (e.g., we can leverage a 48-core machine if this was possible)</p></li>
<li><p>Neither <code>biglm</code> nor any package from Revolution Analytics is available for Cox regression, and so I cannot leverage those.</p></li>
</ul>

<p><strong>Is there a means to represent this in terms of a logistic regression (for which there are packages in Revolution) or if there are any other alternatives to this problem?</strong> I know that they are fundamentally different, but it's the closest I can assume as a possibility given the circumstances. </p>
"
"0.0653720450460613","0.0669649530182425"," 62180","<p>I want to do some regression analysis that constrains the coefficients to vary smoothly as a function of their sequence.
It is similar to the ""Phoneme Recognition"" example in the part 5 ""Basis Expansions and Regularization"" of <em><a href=""http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf"" rel=""nofollow"">The Elements of Statistical Learning</a></em> (Hastie, Tibshirani &amp; Friedman, 2008).</p>

<p>in the book it says:</p>

<blockquote>
  <p>The smooth red curve was obtained through a very simple use of natural cubic splines. We can represent the coefficient function as an expansion of splines $\beta(f)=\sum_{m=1}^M h_m(f)\theta_m$. In practice this means that $\beta=\mathbf{H}\theta$ where, $\mathbf{H}$ is a $p Ã— M$ basis matrix of natural cubic splines, defined on the set of frequencies. Here we used $M = 12$ basis functions, with knots uniformly placed over the integers 1, 2, . . . , 256 representing the frequencies. Since $x^T\beta=x^T\mathbf{H}\theta$, we can simply replace the input features $x$ by their filtered versions $x^* = \mathbf{H}^T x$, and fit $\theta$ by linear logistic regression on the $x^*$. The red curve is thus $\hat\beta(f) = h(f)^T\hat \theta$.</p>
</blockquote>

<p>But I am not sure about how to create the basis matrix $\mathbf{H}$. When using ns() function in R, how to set the knots?</p>

<p>Any hint will be appreciated.</p>
"
"0.179028718509858","0.183391076651825"," 62208","<p>I've tried to simulate data for a power analysis of a logistic regression. The results of the power analysis look reasonable: power=90% for a sample of 6000 persons. But I feel that the analysis lacks something. So, my question is: when generating the data should I include something about how the variables are correlated, or their covariance, other than just defining their linear relationship as I have done in the example below, and if so where do I write that into the code?</p>

<p>I know other questions look like this but I'm not confident that they answer this question.</p>

<pre><code>library(plyr) # functions
## Define Function
simfunktion &lt;- function() {
   # Number in each sample
  antal &lt;- 6000
  beta0 &lt;- log(0.16) # logit in reference group
  beta1 &lt;- log(1.1)  # logit given smoking
  beta2 &lt;- log(1.1)  # logit given SNP(genevariation)
  beta3 &lt;- log(1.2)  # logit for interactioncoefficient for SNP*rygning
   ## Smoking variable, with probabilities defined according to empirical studies.
  smoking  &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,25,40))
   ## SNP variables with probabilities defined according to empirical studies
  SNP      &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,40,20))
   ## calculated probabilites given the model:
  pi.x     &lt;- exp( beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) / 
              ( 1 + exp(beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) )
   ## binoial events given the probabilities:
  sim.y    &lt;- rbinom( n = antal, size = 1, prob = pi.x)  
  sim.data &lt;- data.frame(sim.y, smoking, SNP)
   #################### p-value of the interaction is extracted:
   ## the model is run:
  glm1     &lt;- glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial )
   ## p-value of the interactionterm is extracted:
  summary(    glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial ))$coef[4,4]
}
pvalue     &lt;- as.vector(replicate( 100 , simfunktion()))
mean(pvalue &lt; 0.05)
</code></pre>
"
"0.0800640769025436","0.0820149827720712"," 65095","<p>In documentation to <code>glm</code> I read: ""<em>For binomial and quasibinomial families the response can also be specified as a factor (when the first level denotes failure and all others success)</em>"" Does it mean that probability of failure or success is being modeled? </p>

<p>I'm trying to apply simple logistic model to ""german credit scoring"" dataset where there are levels ""good"" and ""bad"". To get correct results (higher probability means higher likelihood of being good) I have to assume that <code>Failure=Good</code> and <code>Success=Bad</code>. This works, but it is really counterintuitive. I interpret this as - this will model probability of Failure (failed to be bad).</p>

<pre><code>require(ggplot2)

german_data &lt;- read.csv(file=""http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data"",
              sep="" "", header=FALSE)

names(german_data) &lt;- c('ca_status','mob','credit_history','purpose','credit_amount','savings',
'present_employment_since','status_sex','installment_rate_income','other_debtors',
'present_residence_since','property','age','other_installment','housing','existing_credits',
'job','liable_maintenance_people','telephone','foreign_worker','gb')

str(german_data)

german_data$gb &lt;- factor(german_data$gb,levels=c(2,1),labels=c(""bad"",""good""))

levels(german_data$gb)[1] 

table(german_data$gb)

model &lt;- glm(data=german_data,formula=gb~.,family=binomial(link=""logit""))

german_data$prob &lt;- predict(model,newdata=german_data, type=""response"")

ggplot(data=german_data) + geom_boxplot(aes(y=prob,x=gb))  + coord_flip()
</code></pre>

<p><img src=""http://i.stack.imgur.com/kcbao.png"" alt=""enter image description here""></p>
"
"0.10336227882434","0.105880887471907"," 65244","<p>I'm curious about how to understand the accuracy of my model which I computed with <code>glm( family = binomial(logit) )</code>.</p>

<p>In some articles it is mentioned that we should perform chisq test with residual deviance with it's DoF. 
When I call summary() of my glm module.
<strong>""Residual deviance: 9109.9 on 99993 degrees of freedom""</strong> 
Therefore when I perform pchisq test with these inputs: <strong>1-pchisq(9110, 99993)</strong> it returns 1.</p>

<p>Hence it is much more greater than our significance level. So we are curious about why does it return 1, is it a perfect model ?</p>

<p>In addition to these, here's the output of my Logistic Regression Model</p>

<pre><code>Logistic Regression Model

lrm(formula = bool.revenue.all.time ~ level + building.count + 
    gold.spent + npc + friends + post.count, data = sn, x = TRUE, 
    y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs         1e+05    LR chi2    1488.63    R2       0.147    C       0.774    
 0          99065    d.f.             6    g        1.141    Dxy     0.547    
 1            935    Pr(&gt; chi2) &lt;0.0001    gr       3.130    gamma   0.586    
max |deriv| 8e-09                          gp       0.011    tau-a   0.010    
                                           Brier    0.009                     

               Coef    S.E.   Wald Z Pr(&gt;|Z|)
Intercept      -6.7910 0.0938 -72.36 &lt;0.0001 
level           0.0756 0.0193   3.92 &lt;0.0001 
building.count  0.0698 0.0091   7.64 &lt;0.0001 
gold.spent      0.0020 0.0002  11.05 &lt;0.0001 
npc             0.0172 0.0057   3.03 0.0024  
friends         0.0304 0.0045   6.82 &lt;0.0001 
post.count     -0.0132 0.0042  -3.17 0.0015 
</code></pre>

<p>This is validation with bootstrap's output</p>

<pre><code>  index.orig training   test optimism index.corrected    n
Dxy           0.5511   0.5500 0.5506  -0.0006          0.5518 1000
R2            0.1469   0.1469 0.1465   0.0005          0.1465 1000
Intercept     0.0000   0.0000 0.0002  -0.0002          0.0002 1000
Slope         1.0000   1.0000 0.9997   0.0003          0.9997 1000
Emax          0.0000   0.0000 0.0001   0.0001          0.0001 1000
D             0.0149   0.0149 0.0148   0.0000          0.0148 1000
U             0.0000   0.0000 0.0000   0.0000          0.0000 1000
Q             0.0149   0.0149 0.0148   0.0001          0.0148 1000
B             0.0086   0.0086 0.0086   0.0000          0.0086 1000
g             1.1410   1.1381 1.1365   0.0016          1.1394 1000
gp            0.0111   0.0111 0.0111   0.0000          0.0111 1000
</code></pre>

<p>And this is the output of my calibration curve:</p>

<pre><code>n=100000   Mean absolute error=0.002   Mean squared error=5e-05
0.9 Quantile of absolute error=0.002
</code></pre>

<p><img src=""http://i.stack.imgur.com/2AUEX.png"" alt=""Calibration Curve""></p>

<p>Thanks.</p>
"
"0.11322770341446","0.115986700954059"," 65463","<p>This is a question regarding using logistic regression, and relating it to gaussian distribution or a binomial distribution.  </p>

<pre><code>model&lt;-glm(target~ x1, data=data, type='response', family='binomial')
model&lt;-glm(target~ x1, data=data, type='response')  #defaults to gaussian
</code></pre>

<p>My understanding of binomial is that it is </p>

<pre><code>theta=chance of success
z=trails ending in success
k=trials ending in failure
(theta^z)*(1-theta)^k
</code></pre>

<p>And something Gaussian is </p>

<pre><code>theta = standard deviation
x = success
u = mean
Y = [ 1/Ïƒ * sqrt(2Ï€) ] * e -(x - Î¼)2/2Ïƒ2 
</code></pre>

<p>So I understand how to do GLM with R, I kind of understand what binomial and gaussian means, but I have no understanding of how you relate binomial or gaussian to logistic regression, and how binomial and gaussian are different in this context.</p>

<p>Question 1- Can someone explain the intuition behind how ""family='binomial'"" is used when building a model with GLM?</p>

<p>Question 2- Given that the shapes of a binomial distribution and a gaussian distribution look very much the same (they both peak in the middle and gradually go down towards the ends), how does choosing either binomial or guassian lead to different models built from GLM?</p>

<p>thanks!!</p>
"
"0.138675049056307","0.142054117143114"," 65690","<p>I fit a logistic on three numeric continuous variables, followed by a categorical factor [Y, N].</p>

<pre><code>logit2A &lt;- glm(DisclosedDriver ~ VehDrvr_Dif+POL_SEQ_NUM+PRMTOTAL+SAFE_DRVR_PLEDGE_FLG, data = DF, family = ""binomial"") 
</code></pre>

<p>Fit looks wonderful.</p>

<pre><code>Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -2.204e+00  2.253e-01  -9.782  &lt; 2e-16 ***
VehDrvr_Dif            2.918e-01  1.026e-01   2.845 0.004440 ** 
POL_SEQ_NUM           -1.893e-01  5.617e-02  -3.370 0.000751 ***
PRMTOTAL               1.109e-04  5.526e-05   2.006 0.044804 *  
SAFE_DRVR_PLEDGE_FLGY -7.220e-01  1.633e-01  -4.422 9.76e-06 ***
</code></pre>

<p>So obviously R took the Safe_Drvr_Pledge_Flg categorical factor variable and placed all 'N' values in reference or intercept as opposed to the listed 'Y'.</p>

<p>Now I want to take my fit and calculate the probabilities that my model determines. And here comes the error:</p>

<pre><code>&gt; DF$P_GLM&lt;- predict.glm(logit2A, DF, type=""response"", se.fit=FALSE)
    Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
factor SAFE_DRVR_PLEDGE_FLG has new levels 
</code></pre>

<p>Umm... no it doesn't, because I just fit the model with the exact same data I'm trying to use for the prediction. What's the problem?</p>

<p>Trying to respond to first comment:
Don't know what you mean. I've got 3500 rows of data... It's a logistic regression on 4 continuous variables and one categorical. The categorical has two values, Y or N. My glm fit give the numbers given. I just want to plug it all back in with the predict function and it gives me that error. Here's the categorical variable:</p>

<pre><code> &gt; DF$SAFE_DRVR_PLEDGE_FLG
 [1] Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y Y N Y Y Y Y Y N Y Y Y Y Y Y
 [60] Y Y Y Y N Y Y Y Y Y Y Y Y N Y Y Y N N Y N Y Y Y Y Y N Y Y N Y N N Y Y Y N Y Y Y Y N Y Y Y Y Y N Y N Y N Y Y Y Y Y N Y
 [119] N Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y N Y Y Y N Y Y Y N N Y N N N Y N Y Y Y N N Y Y N Y Y Y Y N N Y Y Y Y N N Y N N
 Levels:  N Y
</code></pre>

<p>What do you mean by a working example? The fit works. The probability output of the predict function doesn't...</p>
"
"NaN","NaN"," 66946","<p>When you predict a fitted value from a logistic regression model, how are standard errors computed?  I mean for the <em>fitted values</em>, not for the coefficients (which involves Fishers information matrix).</p>

<p>I only found out how to get the numbers with <code>R</code> (e.g., <a href=""https://stat.ethz.ch/pipermail/r-help/2010-August/248241.html"">here</a> on r-help, or <a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">here</a> on Stack Overflow), but I cannot find the formula.</p>

<pre><code>pred &lt;- predict(y.glm, newdata= something, se.fit=TRUE)
</code></pre>

<p>If you could provide online source (preferably on a university website), that would be fantastic.</p>
"
"0.10336227882434","0.105880887471907"," 67460","<p>I fitted the following multinomial regression:</p>

<pre><code>library(car)
p1&lt;-c(1,2,3,4,3,4,3,4,3,2,1,2,1,2,1,2,3,4,3,2,3,4,3,2,2,2,3,4,3,3,4,3,4)

d1&lt;-c(1,2,3,4,3,4,3,4,3,2,1,2,1,2,1,2,3,4,3,2,3,4,3,2,1,2,3,4,3,2,2,2,1)

d1&lt;-as.ordered(d1)

library(nnet)
test&lt;-multinom(p1~d1)
predi&lt;-expand.grid(d1=c(""1"",""2"",""3"",""4""))

pre&lt;-predict(test,predi,type=""probs"")
</code></pre>

<p>The output is a table of the predicted probabilities for every coefficient. I can also order the results for the confidence interval of the coefficents with:</p>

<pre><code>confint(test)
</code></pre>

<p>My question is: is it possible to get the results for the confidence interval for the predicted probabilities? It means for every amount in the ""pre"" output! 
PS: I found a similar question here in 
[""plotting confidence intervals""][1]<a href=""http://stats.stackexchange.com/questions/29044/plotting-confidence-intervals-for-the-predicted-probabilities-from-a-logistic-re"">Plotting confidence intervals for the predicted probabilities from a logistic regression</a></p>

<p>The main answer is perfect for my question, but I do not know how to combine with multinomial regression. 
I hope you understand my bad english :) Thank you for your help</p>
"
"0.288765387882807","0.295801678213397"," 67873","<p><strong>TLDR</strong>: How can I perform inference for the between group differences in a possibly logistic growth with time in the presence of outliers, unequal measurement times and frequency, bounded measurements and possible random effects on individual and per study level?</p>

<p>I am attempting to analyse a dataset where measurements for individuals were made at different time points. Measurements start low at time 0 and follow (very roughly) a logistic growth pattern with time. I am trying to establish if there are differences between two groups of individuals. The analysis is complicated by the following factors:</p>

<ul>
<li>The effect of time is non-linear, so either a non-linear logistic regression (biologically plausible, but not particularly well fitting) or a non-parametric regression seem appropriate</li>
<li>There are massive outliers, so regression using the sum of squared residuals seems off the table. Quantile regression seems appropriate.</li>
<li>Random effects may be appropriate on a per individual and per study level. Mixed effects models seems appropriate.</li>
<li>Measurement times, number of available measurements and end of monitoring differ between individuals. Survival analysis techniques seem appropriate. Possibly also applying weights equal to 1 / number of observations for individual.</li>
<li>Measurements are bounded below at 0 and while there is no obvious boundary above, arbitrarily high measurements seem biologically implausible. However, quite a few individuals have some measurements of zero (partly due to the measurement accuracy of the device).</li>
<li>A few models I tried so far failed to fit, usually with an unhelpful error related to the numerical procedure. This leads me to believe that I will need a reasonably robust method able to deal with this somewhat ugly dataset.</li>
<li>Finally, I want to produce inference of the form ""group 1 has faster growth than group 2"" or ""group 1 has a higher asymptotic level than group 2"".</li>
</ul>

<p>What I have tried so far (all in R) - I was aware that most of the below are not particularly appropriate for the dataset, but I wanted to see which models could actually be fitted without numerical errors:</p>

<ul>
<li>Non-parametric regression using crs in the crs package. Nicely produces a curve reasonably close to logistic growth for most of the time period with some strange behavious toward the end of the monitoring period (where there are fewer observations). Using individuals as fixed effects reveals some outliers. Using the variable of interest as fixed effects shows some difference. However, I am not sure if there is any way to assess fits and do inference on a model this complex.</li>
<li>Non-linear mixed effects regression using nlme in package nlme and SSlogis. Gradually building up the model with update() works reasonably well. Getting too complex with the fixed effects or the random effects leads to convergence failure. Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further. Edit: I have recently become aware that it is possible to specify autocorrelated residuals in nlme. However, at the moment it seems I cannot even get fixed weights to work. Advice on the correct syntax is welcome.</li>
<li>Non-linear mixed effects regression using nlmer in package LME4 and a custom likelihood for the logistic growth model. Works fairly well, but standard errors on the fixed effects get massive, probably due to the outliers. I also have the slight suspicion that some of the models fail to fit without error, as I sometimes get tiny random effects (about 10^10 smaller than with slightly simpler models). Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further.</li>
<li>Non-linear quantile regression using nlrq in package quantreg and SSlogis. Fits reliably and quickly, but percentile lines intersect. This means that an area containing 90% of the data is not fully contained in an area containing 95% of the data.</li>
<li>Non-parametric quantile regression using the LMS method with package VGAM. Even trivial models failed with obscure errors using this dataset. I believe the number of zeros in the dataset and / or the large range of the data while also getting close to zero may be an issue.</li>
<li>To complete this list, I should probably also mention the lqmm package for Linear Quantile Mixed Models, which I have not used yet. While the package cannot use non-linear models as far as I know, transforming the time variable may produce something reasonably close.</li>
</ul>

<p>I would appreciate feedback if these or any other method might be used to produce reasonably robust inference in this scenario. Maybe regression is not needed at all and another, possibly simpler method is sufficient. I'd be happy to provide an example dataset, if required, but think this question might also be of interest beyond the current dataset.</p>
"
"0.122299897617557","0.125279955557839"," 68553","<p>I am trying to run a logistic regression analysis in R using the speedglm package. 
The data is CNVs (a type of genetic variant), whether that CNV occurred in a case or control and wether genes in a pathway is hit by the CNV or not (Pathway.hit), and how many genes were hit by the CNV that were not in the pathway (Pathway.out).
I run two models with and without the Pathway.hit covariate and compare to see if a pathway is preferentially hit by cases vs controls.  </p>

<p>the models and comparison of said are as follows:</p>

<pre><code>fit1 = speedglm(status~size+Pathway.out, data=cnvs, family=binomial('logit'))
fit2 = speedglm(status~size+Pathway.out+Pathway.hit, data=cnvs,family=binomial('logit'))
P.anova = 1-pchisq(abs(fit1$deviance - fit2$deviance), abs(fit1$df - fit2$df))
</code></pre>

<p>It seems to work okay for most data I throw at it, but in a few cases I get the error:</p>

<pre><code>Error in solve.default(XTX, XTz, tol = tol.solve) : 
  system is computationally singular: reciprocal condition number = 1.87978e-16
</code></pre>

<p>After some googling around I think I found what's causing the problem:</p>

<pre><code>by(cnvs$Pathway.hit, cnvs$status, summary)
cnvs$status: 1 (controls)
        0     1 
    13333     0 
    ------------------------------------ 
    cnvs$status: 2 (cases)
    0     1 
10258     2 
</code></pre>

<p>So here there no observations in controls and only 2 in cases. </p>

<p>If I use with normal glm method however, then it does not throw an error (but that of course doesn't necessarily mean the results will be meaningful). The reason I am using the speedglm package is that I have approximately 16,000 of these analyses to run, and using the base glm function for all 16,000 takes about 20 hours, where as I think speedglm can reduce it down to 8 or so.</p>

<p>So my question is, should I ignore those analyses which throw an error and list the results as NA as there were too few observations, or when speed glm fails should I retry with normal glm? In the above example there are 2 observations of the covariate in cases, but 0 in controls. Might this not be interesting? Would the analysis also fail if there were 0 in controls and 20 in cases - that would certainly be interesting would it not?</p>

<p>Thanks for the help in advance,
Cheers,
Davy</p>
"
"0.197104131996361","0.191811583729112"," 69957","<p>Here's my issue of the day:</p>

<p>At the moment I'm teaching myself Econometrics and making use of logistic regression. I have some SAS code and I want to be sure I understand it well first before trying to convert it to R. (I don't have and I don't know SAS). In this code, I want to model the probability for one person to be an 'unemployed employee'. By this I mean ""age"" between 15 and 64, and ""tact"" = ""jobless"". I want to try to predict this outcome with the following variables: sex, age and idnat (nationality number). (Other things being equal).</p>

<p>SAS code :</p>

<pre><code>/* Unemployment rate : number of unemployment amongst the workforce */
proc logistic data=census;
class sex(ref=""Man"") age idnat(ref=""spanish"") / param=glm;
class tact (ref=first);
model tact = sex age idnat / link=logit;
where 15&lt;=age&lt;=64 and tact in (""Employee"" ""Jobless"");
weight weight;
format age ageC. tact $activity. idnat $nat_dom. inat $nationalty. sex $M_W.;

lsmeans sex / obsmargins ilink;
lsmeans idnat / obsmargins ilink;
lsmeans age / obsmargins ilink;
run;
</code></pre>

<p>This is a sample of what the database should looks like :</p>

<pre><code>      idnat     sex     age  tact      
 [1,] ""english"" ""Woman"" ""42"" ""Employee""
 [2,] ""french""  ""Woman"" ""31"" ""Jobless"" 
 [3,] ""spanish"" ""Woman"" ""19"" ""Employee""
 [4,] ""english"" ""Man""   ""45"" ""Jobless"" 
 [5,] ""english"" ""Man""   ""34"" ""Employee""
 [6,] ""spanish"" ""Woman"" ""25"" ""Employee""
 [7,] ""spanish"" ""Man""   ""39"" ""Jobless"" 
 [8,] ""spanish"" ""Woman"" ""44"" ""Jobless"" 
 [9,] ""spanish"" ""Man""   ""29"" ""Employee""
[10,] ""spanish"" ""Man""   ""62"" ""Retired"" 
[11,] ""spanish"" ""Man""   ""64"" ""Retired"" 
[12,] ""english"" ""Woman"" ""53"" ""Jobless"" 
[13,] ""english"" ""Man""   ""43"" ""Jobless"" 
[14,] ""french""  ""Man""   ""61"" ""Retired"" 
[15,] ""french""  ""Man""   ""50"" ""Employee""
</code></pre>

<p>This is the kind of result I wish to get :</p>

<pre><code>Variable    Modality    Value   ChiSq   Indicator
Sex         Women       56.6%   0.00001 -8.9%
            Men         65.5%       
Nationality 
            1:Spanish   62.6%       
            2:French    51.2%   0.00001 -11.4%
            3:English   48.0%   0.00001 -14.6%
Age 
            &lt;25yo       33.1%   0.00001 -44.9%
        Ref:26&lt;x&lt;54yo   78.0%       
            55yo=&lt;      48.7%   0.00001 -29.3%
</code></pre>

<p>Indicator is P(category)-P(ref)
(I interpret the above as follows: other things being equal, women have -8.9% chance of being employed vs men and those aged less than 25 have a -44.9% chance of being employed than those aged between 26 and 54).</p>

<p>So if I understand well, the best approach would be to use a binary logistic regression (link=logit). This uses references ""male vs female""(sex), ""employee vs jobless""(from 'tact' variable)... I presume 'tact' is automatically converted to a binary (0-1) variable by SAS.</p>

<p>Here is my 1st attempt in R. I haven't check it yet (need my own PC) :</p>

<pre><code>### before using glm function 
### change all predictors to factors and relevel reference
recens$sex &lt;- relevel(factor(recens$sex), ref = ""Man"")
recens$idnat &lt;- relevel(factor(recens$idnat), ref = ""spanish"")  
recens$tact &lt;- relevel(factor(recens$tact), ref = ""Employee"")
recens$ageC &lt;- relevel(factor(recens$ageC), ref = ""Ref : De 26 a 54 ans"")

### Calculations of the probabilities with function glm, 
### formatted variables, and conditions with subset restriction to ""from 15yo to 64""
### and ""employee"" and ""jobless"" only.
glm1 &lt;- glm(activite ~ sex + ageC + idnat, data=recens, weights = weight, 
            subset= recens$age[(15&lt;= recens$age | recens$age &lt;= 64)] 
            &amp; recens$tact %in% c(""Employee"",""Jobless""), 
            family=quasibinomial(""logit""))
</code></pre>

<p>My questions :</p>

<p>For the moment, it seems there are many functions to carry out a logistic regression in R like glm which seems to fit.</p>

<p>However after visiting many forums it seems a lot of people recommend not trying to exactly reproduce SAS PROC LOGISTIC, particularly the function LSMEANS. Dr Franck Harrel, (author of package:rms) for one.</p>

<p>That said, I guess my big issue is LSMEANS and its options Obsmargins and ILINK. Even after reading over its description repeatedly I can hardly understand how it works.</p>

<p>So far, what I understand of Obsmargin is that it respects the structure of the total population of the database (i.e. calculations are done with proportions of the total population). ILINK appears to be used to obtain the predicted probability value (jobless rate, employment rate) for each of the predictors (e.g. female then male) rather than the value found by the (exponential) model?</p>

<p>In short, how could this be done through R, with lrm from rms or lsmeans?</p>

<p>I'm really lost in all of this. If someone could explain it to me better and tell me if I'm on the right track it would make my day.</p>

<p>Thank you for your help and sorry for all the mistakes my English is a bit rusty.</p>

<p>Binh</p>
"
"0.221880078490092","0.236756861905189"," 71414","<p>I'm trying to use a zero-inflated gamma model (or a gamma 'hurdle' model). The model is a mixture of logistic regression and generalized linear modeling. I can do this analysis in two steps: 1) do a logistic regression against presence/absence data and then 2) Use a generalized linear model with a gamma distribution on the positive values. I'm trying to set the model up where, if y = 0, then E(y) = p, but if y > 0, then E(y) is gamma distributed.</p>

<p>I'm trying to set this up in BUGS/JAGS, because I've seen these models <a href=""http://stats.stackexchange.com/questions/15967/how-can-i-set-up-a-zero-inflated-poisson-in-jags"">worked before for poisson-distributions</a>. I tried to adapt the code from that link into a gamma distribution, but I can't quite seem to get it to work. I know the code won't work because my data have zeroes, and the likelihood function can't be evaluated with zeroes. However, even when restricted to positive values, I get errors for invalid parent values. Even so, I'm not even sure the model is specified correctly.</p>

<p>Here is the model:</p>

<pre><code># For the ones trick
C &lt;- 10000

# for every observation
for(i in 1:N){
    # log-likelihood of the observation from the gamma likelihood
    LogPos[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - N*shape[i]*log(scale[i]) - N*loggam(scale[i])
    #likelihood
    Lpos[i] &lt;- exp(LogPos[i])

    # redefine the shape and rate parameters as a function of the mean and sd
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # mu is a function of MTD: use the inverse link
    #mu[i] &lt;- 1/eta[i]
    mu[i] &lt;- beta0 + beta1*MTD[i]


    # zero-inflated part, where w[i] is the probability of being zero
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # ones trick
    p[i] &lt;- Lpos[i] / C
    ones[i] ~ dbern(p[i])

    # Full likelihood
    Lik[i] &lt;- Lpos[i]*(1 - w[i]) + equals(y[i], 0)*w[i]
  } 

# PRIORS
beta0 ~ dnorm(0, 0.001)
beta1 ~ dnorm(0, 0.001)

gamma0 ~ dnorm(0, 0.001)
gamma1 ~ dnorm(0, 0.001)

sd ~ dunif(0, 100)
</code></pre>

<p>Has anyone set a model up like this or have any advice on how to set it up correctly?</p>

<p><strong>UPDATE</strong></p>

<p>I've tried a new set of code that's similar, but slight different. I still have not gotten it to work</p>

<pre><code>model{

  # For the ones trick
  C &lt;- 10000

  # for every observation
  for(i in 1:N){

    # make a dummy variable that is 0 if y is &lt; 0.0001 and 1 if y &gt; 0.0001. This is essentially a presence
    # absence dummy variable
    z[i] &lt;- step(y[i] - 0.0001)

    # define the logistic regression model, where w is the probability of occurance.
    # use the logistic transformation exp(z)/(1 + exp(z)), where z is a linear function
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # define the gamma regression model for the mean. use the log link the ensure positive, non-zero mu
    mu[i] &lt;- exp(eta[i])
    eta[i] &lt;- beta0 + beta1*MTD[i]

    # redefine the mu and sd of the continuous part into the shape and scale parameters
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # for readability, define the log-likelihood of the gamma here
    logGamma[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i])

    # define the total likelihood, where the likelihood is (1 - w) if y &lt; 0.0001 (z = 0) or
    # the likelihood is w * gammalik if y &gt;= 0.0001 (z = 1). So if z = 1, then the first bit must be
    # 0 and the second bit 1. Use 1 - z, which is 0 if y &gt; 0.0001 and 1 if y &lt; 0.0001
    logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + logGamma[i] )

    # Use the ones trick
    p[i] &lt;- logLik[i] / C
    ones[i] ~ dbern(p[i])
  } 

  # PRIORS
  beta0 ~ dnorm(0, 0.001)
  beta1 ~ dnorm(0, 0.001)

  gamma0 ~ dnorm(0, 0.001)
  gamma1 ~ dnorm(0, 0.001)

  sd ~ dgamma(1, 2)

}
</code></pre>

<p><strong>UPDATE 2</strong></p>

<p>I've gotten it to run by deleting the definition of logGamma[i] and putting it directly into the likelihood function, which now reads:</p>

<pre><code>logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i]) )
</code></pre>

<p>The problem was that JAGS was trying to evaluate the log likelihood at all observations first, resulting in NA's for the 0's. In the new way, it only evaluates it if z = 1 (I think). I'm still having trouble getting the parameter estimates to line up. For example, the gamma's are almost identical to a logistic regression of the same form (hooray!). But the betas are pretty far off from a gamma GLM of the positive values. I don't know if this is normal or not, but I suspect there are still problems with my model specification.</p>
"
"0.160604018609905","0.177172612243394"," 73165","<p>I have a logistic regression model (fit via glmnet in R with elastic net regularization), and I would like to maximize the difference between true positives and false positives.  In order to do this, the following procedure came to mind:</p>

<ol>
<li>Fit standard logistic regression model</li>
<li>Using prediction threshold as 0.5, identify all positive predictions</li>
<li>Assign weight 1 for positively predicted observations, 0 for all others</li>
<li>Fit weighted logistic regression model</li>
</ol>

<p>What would be the flaws with this approach?  What would be the correct way to proceed with this problem?</p>

<p>The reason for wanting to maximize the difference between the number of true positives and false negatives is due to the design of my application.  As part of a class project, I am building a autonomous participant in an online marketplace - if my model predicts it can buy something and sell it later at a higher price, it places a bid.  I would like to stick to logistic regression and output binary outcomes (win, lose) based on fixed costs and unit price increments (I gain or lose the same amount on every transaction).  A false positive hurts me because it means that I buy something and am unable to sell it for a higher price.  However, a false negative doesn't hurt me (only in terms of opportunity cost) because it just means if I didn't buy, but if I had, I would have made money.  Similarly, a true positive benefits me because I buy and then sell for a higher price, but a true negative doesn't benefit me because I didn't take any action.</p>

<p>I agree that the 0.5 cut-off is completely arbitrary, and when I optimized the model from step 1 on the prediction threshold which yields the highest difference between true/false positives, it turns out to be closer to 0.4.  I think this is due to the skewed nature of my data - the ratio between negatives and positives is about 1:3.</p>

<p>Right now, I am following the following steps:</p>

<ol>
<li>Split data intto training/test</li>
<li>Fit model on training, make predictions in test set and compute difference between true/false positives</li>
<li>Fit model on full, make predictions in test set and compute difference between true/false positives</li>
</ol>

<p>The difference between true/false positives is smaller in step #3 than in step #2, despite the training set being a subset of the full set.  Since I don't care whether the model in #3 has more true negatives and less false negatives, is there anything I can do without altering the likelihood function itself?</p>
"
"0.122299897617557","0.125279955557839"," 73567","<p>I have a logistic regression model with several variables and one of those variables (called x3 in my example below) is not significant. However, x3 should remain in the model because it is scientifically important.</p>

<p>Now, x3 is continuous and I want to create a plot of the predicted probability vs x3. Even though x3 is not statistically significant, it has an effect on my outcome and therefore it has an effect on the predicted probability. This means that I can see from the graph, that the probability changes with increasing x3. However, how should I interpret the graph and the change in the predicted probability, given that x3 is indeed not statistically significant?</p>

<p>Below is a simulated data in R set to illustrate my question. The graph also contains a 95% confidence interval for the predicted probability (dashed lines):</p>

<pre><code>&gt; set.seed(314)
&gt; n &lt;- 300
&gt; x1 &lt;- rbinom(n,1,0.5)
&gt; x2 &lt;- rbinom(n,1,0.5)
&gt; x3 &lt;- rexp(n)
&gt; logit &lt;- 0.5+0.9*x1-0.5*x2
&gt; prob &lt;- exp(logit)/(1+exp(logit))
&gt; y &lt;- rbinom(n,1,prob)
&gt; 
&gt; model &lt;- glm(y~x1+x2+x3, family=""binomial"")
&gt; summary(model)

Call:
glm(formula = y ~ x1 + x2 + x3, family = ""binomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0394  -1.1254   0.5604   0.8554   1.4457  

Coefficients:
            Estimate Std. Error z value Pr(    &gt;|z|)    
(Intercept)   1.1402     0.2638   4.323 1.54e-05 ***
x1            0.8256     0.2653   3.112  0.00186 ** 
x2           -1.1338     0.2658  -4.266 1.99e-05 ***
x3           -0.1478     0.1249  -1.183  0.23681    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 373.05  on 299  degrees of freedom
Residual deviance: 341.21  on 296  degrees of freedom
AIC: 349.21

Number of Fisher Scoring iterations: 3

&gt; 
&gt; dat &lt;- data.frame(x1=1, x2=1, x3=seq(0,5,0.1))
&gt; preds &lt;- predict(model, dat,type = ""link"", se.fit = TRUE )
&gt; critval &lt;- 1.96
&gt; upr &lt;- preds$fit + (critval * preds$se.fit)
&gt; lwr &lt;- preds$fit - (critval * preds$se.fit)
&gt; fit &lt;- preds$fit
    &gt; 
    &gt; fit2 &lt;- mod$family$linkinv(fit)
    &gt; upr2 &lt;- mod$family$linkinv(upr)
    &gt; lwr2 &lt;- mod$family$linkinv(lwr)
    &gt; 
    &gt; plot(dat$x3, fit2, lwd=2, type=""l"", main=""Predicted Probability"", ylab=""Probability"", xlab=""x3"", ylim=c(0,1.00))
&gt; lines(dat$x3, upr2, lty=2)
    &gt; lines(dat$x3, lwr2, lty=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ljW7W.png"" alt=""enter image description here""></p>

<p>Thanks!</p>

<p>Emilia</p>
"
"0.0800640769025436","0.0820149827720712"," 74549","<p>This is from the book <em>The statistical sleuth--A course in methods of Data analysis</em> Chapter 20, Exercise 12(c)-(e). I am using logistic regression to predict carrier with possible predictors <code>CK</code> and <code>H</code>. Here is my solution:</p>

<pre><code>Carrier &lt;- c(0,0,0,0,0,1,1,1,1,1)  
CK      &lt;- c(52,20,28,30,40,167,104,30,65,440)  
H       &lt;- c(83.5,77,86.5,104,83,89,81,108,87,107)  
logCK   &lt;- log(CK)  
fit4    &lt;- glm(Carrier~logCK+H, family=""binomial"", control=list(maxit=100))  
Warning message:  
glm.fit: fitted probabilities numerically 0 or 1 occurred   
summary(fit4)
## 
## Call:
## glm(formula = Carrier ~ logCK + H, family = ""binomial"", control = list(maxit = 100))
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -1.480e-05  -2.110e-08   0.000e+00   2.110e-08   1.376e-05  
##
## Coefficients:  
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   -2292.8  4130902.8  -0.001        1  
## logCK           315.6   589675.2   0.001        1  
## H                11.5    21279.6   0.001        1
</code></pre>

<p>This results appear to be weird, because it seems that all coefficients are not significant.  Also the next question is to do a drop-in-deviance test for this full model and the reduced model that neither of <code>logCK</code> and <code>H</code> is useful predictor. I get:  </p>

<pre><code>fit5 &lt;- glm(Carrier~1, family=""binomial"")  
1-pchisq(deviance(fit5)-deviance(fit4), df.residual(fit5)-df.residual(fit4))  
## [1] 0.0009765625
</code></pre>

<p>So the p-value indicates that at least one of <code>logCK</code> and <code>H</code> is useful. Then I'm stuck at the next question, it asks me to calculate odds ratio for a woman with (CK, H)=(300,100) over one with (CK, H)=(80, 85).  </p>

<p>But how can I get a meaningful result with all coefficients in this model ranging so wildly? Is there anything wrong with the way I did this logistic regression? Are there any remedial measures?  </p>
"
"0.10336227882434","0.105880887471907"," 77094","<p>I have a logistic model fitted with the following R function:</p>

<pre><code>glmfit&lt;-glm(formula, data, family=binomial)
</code></pre>

<p>A  reasonable cutoff value in order to get a good data classification (or confusion matrix) with the fitted model is 0.2 instead of the mostly used 0.5.</p>

<p>And I want to use the <code>cv.glm</code> function with the fitted model:</p>

<pre><code>cv.glm(data, glmfit, cost, K)
</code></pre>

<p>Since the response in the fitted model is a binary variable an appropriate cost function is (obtained from ""Examples"" section of ?cv.glm):</p>

<pre><code>cost &lt;- function(r, pi = 0) mean(abs(r-pi) &gt; 0.5)
</code></pre>

<p>As I have a cutoff value of 0.2, can I apply this standard cost function or should I define a different one and how?</p>

<p>Thank you very much in advance.</p>
"
"0.0462250163521024","0.0473513723810378"," 80463","<p>I have the following set of model-averaged fixed effects from a set of binomial GLMMs: </p>

<p><img src=""http://i.stack.imgur.com/yN4wR.png"" alt=""model parameters image""></p>

<p>I would like to plot the predicted effect of ""NBT"", along with confidence bands, while holding all the other variables at their baseline levels. My attempt to do this in ggplot:</p>

<pre><code>Xvars &lt;- seq(from=0, to=100, by=0.1)  #NBT range is 0-100
  binomIntercept &lt;- 1.317
  binomSlope &lt;- -0.0076     
  binomSE &lt;- 0.009    
Means &lt;- logistic(binomIntercept + binomSlope*Xvars)              
loCI &lt;- logistic(binomIntercept + (binomSlope - 1.96*binomSE)*Xvars)
upCI &lt;- logistic(binomIntercept + (binomSlope + 1.96*binomSE)*Xvars)
df &lt;- data.frame(Xvars,Means,loCI,upCI)
p &lt;- ggplot(data=df, aes(x = Xvars, y = Means)) + 
geom_line() +          
geom_line(data=df, aes(x = Xvars, y = upCI),col='grey') +
geom_line(data=df, aes(x = Xvars, y = loCI), col='grey')
p                                            
</code></pre>

<p><img src=""http://i.imgur.com/eMJBxQQ.png"" alt=""graph image""></p>

<p>I'm assuming that the confidence bands are cone shaped because I'm not accounting for uncertainty in the estimate for the intercept. Maybe this is okay (?), but it does look different from every regression line I've ever seen with confidence intervals plotted.</p>

<p>Can someone please tell me how I should be writing my equations to get the correct confidence intervals, given the intercept, slope, and standard errors from my model output?</p>

<p>(I know I can use the predict function to do this in R, but would like to know how to do it by hand.)  </p>
"
"0.10336227882434","0.105880887471907"," 80880","<p>I have seen several articles and CrossValidated questions on bootstrapping ( <a href=""http://stats.stackexchange.com/questions/41625/can-i-use-boostrapping-why-or-why-not"">this</a>, <a href=""http://stats.stackexchange.com/questions/59829/which-bootstrapped-regression-model-should-i-choose"">this</a> or <a href=""http://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients"">this</a> for example); there are a lot of theoretical and statistical explanations, however since they are so theory based, I am afraid I might be understanding the use wrongly. Hence my questions:</p>

<p>1) When I make a non-parametric bootstrapping (changing the sample for every run) with logistic regression on my data, I basically will end up with several different coefficients for each predictor for each run. Eventually I'll have the confidence interval for each predictor as well. I understand until that point. My question is; assuming that the distribution is normal, when I want to come up with a final model on practice, can I just take the mean of the confidence intervals for each predictor and consider this as my final model coefficient?</p>

<p>2) If the answer to question #1 is yes, is this the only way of choosing coefficients while bootstrapping? If not, what else? I encountered in a few more articles a method called ""bagging"". This seems to be my main purpose. </p>

<p>3) This one is more of a curiosity question: Can above methodology be applied to the categorical predictors when they are assigned with Weight Of Evidences? I know we can split the  categorical predictors into dummy variables; but how would I treat each coefficient if I want to use WOE methodology?</p>
"
"0.146176336551172","0.134764368352983"," 81612","<p>Recently I was trying to do logistic regression using the <code>rms::lrm()</code> function. But I had some trouble understanding the model objects from the function. Here is the example from the package:</p>

<pre><code>#dataset
n            &lt;- 1000    # define sample size
set.seed(17)            # so can reproduce the results
treat        &lt;- factor(sample(c('a','b','c'), n,TRUE))
num.diseases &lt;- sample(0:4, n,TRUE)
age          &lt;- rnorm(n, 50, 10)
cholesterol  &lt;- rnorm(n, 200, 25)
weight       &lt;- rnorm(n, 150, 20)
sex          &lt;- factor(sample(c('female','male'), n,TRUE))
L            &lt;- .1*(num.diseases-2) + .045*(age-50) +
                (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
                3.5*(treat=='b')+2*(treat=='c'))
y            &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)
#fit model
g            &lt;- lrm(y ~ treat*rcs(age))

&gt; g

Logistic Regression Model

lrm(formula = y ~ treat * rcs(age))

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          1000    LR chi2      76.77    R2       0.099    C       0.656    
 0            478    d.f.            14    g        0.665    Dxy     0.312    
 1            522    Pr(&gt; chi2) &lt;0.0001    gr       1.945    gamma   0.314    
max |deriv| 3e-06                          gp       0.156    tau-a   0.156    
                                           Brier    0.231    
&gt; anova(g)
                Wald Statistics          Response: y 

 Factor                                     Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)        5.62      10   0.8462
  All Interactions                           1.30       8   0.9956
 age  (Factor+Higher Order Factors)         65.99      12   &lt;.0001
  All Interactions                           1.30       8   0.9956
  Nonlinear (Factor+Higher Order Factors)    2.23       9   0.9872
 treat * age  (Factor+Higher Order Factors)  1.30       8   0.9956
  Nonlinear                                  0.99       6   0.9858
  Nonlinear Interaction : f(A,B) vs. AB      0.99       6   0.9858
 TOTAL NONLINEAR                             2.23       9   0.9872
 TOTAL NONLINEAR + INTERACTION               2.57      11   0.9953
 TOTAL                                      69.06      14   &lt;.0001
</code></pre>

<p><strong>Here are my questions:</strong><br>
For the object <code>g</code>,  </p>

<ul>
<li>What does the <code>max |deriv| 3e-06</code> mean?  </li>
<li>What do the Discrimination and Rand Discrim. Indexes suggest?  </li>
</ul>

<p>For the <code>anova(g)</code> object,  </p>

<ul>
<li>What's the <code>Factor +Higher Order Factors</code> for the treat?  </li>
<li>Why there are two <code>all interactions</code>? How to explain the nonlinear parts?</li>
</ul>
"
"0.122299897617557","0.125279955557839"," 87872","<p>When I do a (logistic) regression in R, I run something like this:</p>

<pre><code>mydata &lt;- read.csv(""data.csv"")
mylogit &lt;- glm(a ~ c+d, data = mydata, family=""binomial"")
summary(mylogit)
</code></pre>

<p>As of a few months ago, the output for the coefficients might look like this:</p>

<pre><code>Call:
glm(formula = a ~ c + d, family = ""binomial"", data = mydata)
...
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.6476     0.1898  -8.680  &lt; 2e-16 ***
c             2.4558     0.3414   7.194 6.29e-13 ***
d             2.3783     0.4466   5.326 1.01e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Trying it today (with a newer version of R), the output looks like the following:</p>

<pre><code>Call:
glm(formula = a ~ c + d, family = ""binomial"", data = mydata)
...
Coefficients: (1 not defined because of singularities)
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.6709     0.1924  -8.683  &lt; 2e-16 ***
c1            2.4961     0.3476   7.181 6.94e-13 ***
cc           18.2370   979.6100   0.019    0.985    
d1            2.4524     0.4630   5.296 1.18e-07 ***
dd                NA         NA      NA       NA    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the ""c1"", ""cc"", etc fields mean?  I can't seem to find this any documentation, but perhaps I am looking in the wrong places?</p>
"
"0.0924500327042048","0.0947027447620757"," 87956","<p>I have a repeated-measures experiment where the dependent variable is a percentage, and I have multiple factors as independent variables. I'd like to use <code>glmer</code> from the R package <code>lme4</code> to treat it as a logistic regression problem (by specifying <code>family=binomial</code>) since it seems to accommodate this setup directly.</p>

<p>My data looks like this:</p>

<pre><code> &gt; head(data.xvsy)
   foldnum      featureset noisered pooldur dpoolmode       auc
 1       0         mfcc-ms      nr0       1      mean 0.6760438
 2       1         mfcc-ms      nr0       1      mean 0.6739482
 3       0    melspec-maxp    nr075       1       max 0.8141421
 4       1    melspec-maxp    nr075       1       max 0.7822994
 5       0 chrmpeak-tpor1d    nr075       1       max 0.6547476
 6       1 chrmpeak-tpor1d    nr075       1       max 0.6699825
</code></pre>

<p>and here's the R command that I was hoping would be appropriate:</p>

<pre><code> glmer(auc~1+featureset*noisered*pooldur*dpoolmode+(1|foldnum), data.xvsy, family=binomial)
</code></pre>

<p>The problem with this is that the command complains about my dependent variable not being integers:</p>

<pre><code>In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>and the analysis of this (pilot) data gives weird answers as a result.</p>

<p>I understand why the <code>binomial</code> family expects integers (yes-no counts), but it seems it should be OK to regress percentage data directly. How to do this?</p>
"
"0.0800640769025436","0.0820149827720712"," 88036","<p>I am new to R, and don't see these questions answered anywhere in documentation (though I could be wrong).</p>

<ol>
<li><p>I am using the following nomenclature to run my mixed-effects logistic regression, based on instructions from another site:  </p>

<p><code>output &lt;- glmer(DV ~ IV1 + IV2 + (1 | RE), family = binomial, nAGQ = 10)</code><br>
RE is a factor with several levels.</p>

<p>This works. But I'm wondering why it's necessary to use the <code>(1 | RE)</code> syntax instead of just <code>DV~IV1+IV2 | RE</code>.</p></li>
<li><p>I am running two mixed effects logistic regressions. On one of them I can view the random effects intercepts using <code>ranef()</code>. But I get all 0s when I run ranef on the output of the other one. Both regressions/data are ostensibly the same. What do all 0s for the random effects intercepts mean?</p></li>
</ol>
"
"0.0653720450460613","0.0669649530182425"," 90000","<p>I want to find the sample size for logistic regression where I have a covariate with 15 levels and the covariates interacts with time, which means that the effect of the covariates is different for different periods of times. Can anyone help me find the sample size and effect size? I wish I could do this using simulations. Is there a code in SAS or R?</p>
"
"0.17295817388759","0.177172612243394"," 94468","<p>I am completely out of my depth on this, and all the reading I try to do just confuses me. I'm hoping you can explain things to me in a way that makes sense. (As always seems to be the case, ""It shouldn't be this hard!"")</p>

<p>I'm trying to help a student who is looking at the effect of social systems on prevalence of diseases in various canid host species. We want to consider social system (e.g., group-living vs. solitary) as a fixed effect, and host species as a random effect nested within social system (i.e., each species only ever has one social system type).</p>

<p>My understanding is that the best way to do this would be to do a mixed-effects logistic regression. We've done this, and it works, and we were happy. Unfortunately, her advisor is insisting that she calculate the amount of variation due to social system vs. host species vs. residual. I can't figure out how to do this via mixed-effects logistic regression, and <a href=""http://stats.stackexchange.com/questions/93450/partitioning-variance-from-logistic-regression"">my previous question on this topic</a> went unanswered.</p>

<p>Her advisor suggested doing ANOVA instead, logit-transforming disease prevalence values (the fraction of each population that is infected). This presented a problem because some of the prevalence values are 0 or 1, which would result in $-\infty$ or $\infty$ once logit-transformed. Her advisor's ""solution"" was to just substitute $-5$ and $5$ for $-\infty$ or $\infty$, respectively. This feels really kludgey and makes me cringe pretty hard. But he's the one grading her, and at this point I just want to be done with this, so if he's fine with it then whatever.</p>

<p>We are using R for this analysis. The code can be downloaded <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_code.R"">here</a>, and the input data <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_data.csv"">here</a>. The data file includes data on two different pathogens (A and B), which we are analyzing separately (as shown in the code).</p>

<p>Here's the ANOVA setup we made for Pathogen B:</p>

<pre><code>mod1.lm &lt;- lm(Seroprevalence_logit ~ Social.System + Social.System/Host.Species,
              data = prev_B)
print(mod1.anova &lt;- anova(mod1.lm))
</code></pre>

<p>This leads to my first question: <strong>Is this correct and appropriate?</strong> Factors to consider:</p>

<ul>
<li>We want to have a Model II (random effect) variable nested within a Model I (fixed effect) variable.</li>
<li>Not every social system has the same number of host species nested within it.</li>
<li>Not every host species has the same number of populations examined.</li>
<li>Not every population examined had the same number of individuals (column N_indiv in mydata.csv). This is more of a weighting problem than something more fundamental, I think.</li>
</ul>

<p>My next question, and the main one of this post, is: <strong>How do I partition the variance?</strong> Here's what we were thinking:</p>

<pre><code>MS_A &lt;- mod1.anova$""Mean Sq""[1]
MS_BinA &lt;- mod1.anova$""Mean Sq""[2]
MS_resid &lt;- mod1.anova$""Mean Sq""[3]
n &lt;- length(unique(prev_A$Social.System))
r &lt;- length(unique(prev_A$Host.Species))
VC_A &lt;- (MS_A - MS_BinA)/(n*r)
VC_BinA &lt;- (MS_BinA - MS_resid)/n
VC_resid &lt;- MS_resid
</code></pre>

<p>Unfortunately, this results in sadness using the ANOVA specification I detailed above. Here are the results for Pathogen B:</p>

<ul>
<li><code>VC_A</code> (i.e., Social.System): $-1.48$</li>
<li><code>VC_BinA</code> (i.e., Host.Species): $13.8$</li>
<li><code>VC_resid</code>: $5.57$</li>
</ul>

<p>Research leads me to believe that this should result in variance component percentages of 0%, 71.3%, and 28.7%, respectively. However, this is unsatisfying for two reasons:</p>

<ul>
<li>The p-value for Social.System from the ANOVA was ~$0.025$, suggesting that it should account for at least <em>some</em> of the observed variance. (Host.Species had a p-value of ~$3*10^{-5}$.)</li>
<li>I'm concerned that a negative variance component might be a red flag for something.</li>
</ul>

<p>Please, any assistance you can render on either of these questions would be greatly appreciated. I TA'd an undergraduate course on biostatistics, so I've got some background, but I just can't seem to figure out these specific issues. Thanks in advance.</p>
"
"0.166666666666667","0.170727801083421"," 94581","<p>I have a ordinal dependendent variable, easiness, that ranges from 1 (not easy) to 5 (very easy).  Increases in the values of the independent factors are associated with an increased easiness rating.</p>

<p>Two of my independent variables (<code>condA</code> and <code>condB</code>) are categorical, each with 2 levels, and 2 (<code>abilityA</code>, <code>abilityB</code>) are continuous.</p>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/ordinal/index.html"">ordinal</a> package in R, where it uses what I believe to be</p>

<p>$$\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y &gt; g)} = \beta_{0_g} - (\beta_{1} X_{1} + \dots + \beta_{p} X_{p}) \quad(g = 1, \ldots, k-1)$$<br>
(from @caracal's answer <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">here</a>)</p>

<p>I've been learning this independently and would appreciate any help possible as I'm still struggling with it.  In addition to the tutorials accompanying the ordinal package, I've also found the following to be helpful: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">Interpretation of ordinal logistic regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">Negative coefficient in ordered logistic regression</a></li>
</ul>

<p>But I'm trying to interpret the results, and put the different resources together and am getting stuck. </p>

<ol>
<li><p>I've read many different explanations, both abstract and applied, but am still having a hard time wrapping my mind around what it means to say: </p>

<blockquote>
  <p>With a 1 unit increase in condB (i.e., changing from one level to the next of the categorical predictor), the predicted odds of observing Y = 5 versus Y = 1 to 4 (as well as the predicted odds of observed Y = 4 versus Y = 1 to 3) change by a factor of exp(beta) which, for diagram, is exp(0.457) = 1.58. </p>
</blockquote>

<p>a. Is this different for the categorical vs. continuous independent variables?<br>
b. Part of my difficulty may be with the cumulative odds idea and those comparisons. ... Is it fair to say that going from condA = absent (reference level) to condA = present is 1.58 times more likely to be rated at a higher level of easiness?  I'm pretty sure that is NOT correct, but I'm not sure how to correctly state it.</p></li>
</ol>

<p>Graphically,<br>
1. Implementing the code in <a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">this post</a>, I'm confused as to why the resulting 'probability' values are so large.<br>
2. The graph of p (Y = g) in <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">this post</a> makes the most sense to me ... with an interpretation of the probability of observing a particular category of Y at a particular value of X.  The reason I am trying to get the graph in the first place is to get a better understanding of the results overall.</p>

<p>Here's the output from my model:</p>

<pre><code>m1c2 &lt;- clmm (easiness ~ condA + condB + abilityA + abilityB + (1|content) + (1|ID), 
              data = d, na.action = na.omit)
summary(m1c2)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: 
easiness ~ illus2 + dx2 + abilEM_obli + valueEM_obli + (1 | content) +  (1 | ID)
data:    d

link  threshold nobs logLik  AIC    niter     max.grad
logit flexible  366  -468.44 956.88 729(3615) 4.36e-04
cond.H 
4.5e+01

Random effects:
 Groups  Name        Variance Std.Dev.
 ID      (Intercept) 2.90     1.70    
 content  (Intercept) 0.24     0.49    
Number of groups:  ID 92,  content 4 

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
condA              0.681      0.213    3.20   0.0014 ** 
condB              0.457      0.211    2.17   0.0303 *  
abilityA           1.148      0.255    4.51  6.5e-06 ***
abilityB           0.577      0.247    2.34   0.0195 *  

Threshold coefficients:
    Estimate Std. Error z value
1|2   -3.500      0.438   -7.99
2|3   -1.545      0.378   -4.08
3|4    0.193      0.366    0.53
4|5    2.121      0.385    5.50
</code></pre>
"
"0.196116135138184","0.189734033551687"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.114401078830607","0.117188667781924"," 96236","<p>I am following an example <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">here</a> on using Logistic Regression in R. However, I need some help interpreting the results. They do go over some of the interpretations in the above link, but I need more help with understanding a goodness of fit for Logistic Regression and the output that I am given.</p>

<p>For convenience, here is the summary given in the example:</p>

<pre><code>## Call:
## glm(formula = admit ~ gre + gpa + rank, family = ""binomial"", 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.627  -0.866  -0.639   1.149   2.079  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.98998    1.13995   -3.50  0.00047 ***
## gre          0.00226    0.00109    2.07  0.03847 *  
## gpa          0.80404    0.33182    2.42  0.01539 *  
## rank2       -0.67544    0.31649   -2.13  0.03283 *  
## rank3       -1.34020    0.34531   -3.88  0.00010 ***
## rank4       -1.55146    0.41783   -3.71  0.00020 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 458.52  on 394  degrees of freedom
## AIC: 470.5
## 
## Number of Fisher Scoring iterations: 4
</code></pre>

<ol>
<li>How well did Logistic Regression fit here?</li>
<li>What exactly are the Deviance Residuals? I believe they are the average residuals per quartile. How do I determine if they are bad/good/statistically significant?</li>
<li>What exactly is the <code>z-value</code> here? Is it the normalized standard deviation from the mean of the Estimate assuming a mean of 0? </li>
<li>What exactly are Signif. codes?</li>
</ol>

<p>Any help is greatly appreciated! You do not have to answer them all!</p>
"
"0.0924500327042048","0.0947027447620757"," 97347","<p>How can I improve the accuracy of my logistic regression code, which tests the accuracy using the 10-fold cross-validation technique? I have implemented this code using <code>glmfit</code> and <code>glmval</code>. The desired accuracy is somewhat higher and it requires the parameters to be found using maximum likelihood estimator. Also, when I run this code in MATLAB, I get the following error</p>

<blockquote>
  <p>Warning: X is ill conditioned, or the model is overparameterized, and some coefficients are not identifiable. You should use caution in making predictions. In glmfit at 245 In LR at 8</p>
</blockquote>

<p>The code is:</p>

<pre><code>function LR( X,y)
y(y==-1)=0;
X=[ones(size(X,1),1) X];
disp(size(X,2));
indices = crossvalind('Kfold',y,10);
for i = 1:10
    test = (indices == i); train = ~test;
    b = glmfit(X(train,:),y(train),'binomial','logit');
    y_hat= glmval(b,X(test,:),'logit');
    y_true=y(test,:);
    error(i)=mean(abs(y_true-y_hat));
end
accuracy=(1-error)*100;
fprintf('accuracy= %f +- %f\n',mean(accuracy),std(accuracy));
end
</code></pre>
"
"0.11322770341446","0.115986700954059"," 99254","<p>I have a large set of time series data, consisting of series from two different conditions, the averages of which are shown below.</p>

<p>I would like to fit a model to this data, to test that a) the peak value is greater in the 'conflict' condition, and b) the peak occurs earlier in this condition.</p>

<pre><code>ggplot(data, aes(x=Time, y=Variable, colour=Condition)) 
    + stat_summary(fun.data=mean_se, geom=""pointrange"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/3NQ8T.png"" alt=""enter image description here""></p>

<p>From my own research on this, I know that:</p>

<ul>
<li>Growth curve analysis is the usual way of modelling time series data like this, but I can't figure out how I would fit a polynomial for this shape of curve.
<ul>
<li>I have some experience fitting GCA models using <code>lme4</code> in R, mostly following <a href=""http://www.danmirman.org/gca"" rel=""nofollow"">Dan Mirman's tutorials</a>, but I'm still learning.</li>
</ul></li>
<li>Curves of this kind are referred to as Hubbert curves, and typically used to model oil production, as a symmetric logistic curve up and down.</li>
<li>R package <code>grofit</code> is maybe useful for analyses of this kind, although I would rather know I'm barking up the right tree before investing time in learning how to use this.</li>
</ul>

<p>Can anyone point me in the right direction here?</p>
"
"0.0653720450460613","0.0669649530182425","103735","<p>I am fitting a few time series using <code>fitdistr</code> in R. To see how different distributions fit the data, I compare the log likelihood from the <code>fitdistr</code> function. Also, I am fitting both the original data, and the standardized data (ie. (x-mean)/sd).</p>

<p>What I am confused about is that, the original and standardized data generate log likelihood of different signs.</p>

<p>For example,</p>

<p>original:</p>

<pre><code>           loglik           m          s          df
t        1890.340 0.007371982 0.05494671 2.697321186
cauchy   1758.588 0.006721215 0.04089592 0.006721215
logistic 1787.952 0.007758433 0.04641496 0.007758433
</code></pre>

<p>standardized:</p>

<pre><code>            loglik           m         s          df
t        -2108.163 -0.02705098 0.5469259  2.69758567
cauchy   -2239.915 -0.03361670 0.4069660 -0.03361670
logistic -2210.552 -0.02328445 0.4619152 -0.02328445
</code></pre>

<p>How can I interprete this? Is larger loglik better or smaller better?</p>

<p>Thank you!</p>
"
"0.131558702896054","0.14973818705887","104595","<p>I've been reading <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a>, <a href=""http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html"" rel=""nofollow"">http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html</a>, and <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and I'm still a little lost on how to do a power analysis for my data. I want to be able to determine what N I should have if I have an interaction between a categorical variable (with 3 levels) and a continuous variable.</p>

<p><a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a> provides some information, but I can't figure out how to simulate the relationship between the categorical and continuous variables and outcome.</p>

<blockquote>
  <p>set.seed(1)<br></p>
  
  <p>repetitions = 1000<br>
  N = 10000<br>
  n = N/8<br>
  var1  = c(   .03,    .03,    .03,    .03,    .06,    .06,    .09,   .09)<br>
  var2  = c(     0,      0,      0,      1,      0,      1,      0,     1)<br>
  rates = c(0.0025, 0.0025, 0.0025, 0.00395, 0.003, 0.0042, 0.0035, 0.002)<br></p>
  
  <p>var1    = rep(var1, times=n)<br>
  var2    = rep(var2, times=n)<br>
  var12   = var1**2<br>
  var1x2  = var1 *var2<br>
  var12x2 = var12*var2<br></p>
  
  <p>significant = matrix(nrow=repetitions, ncol=7)<br></p>
  
  <p>startT = proc.time()[3]<br>
  for(i in 1:repetitions){<br>
   responses          = rbinom(n=N, size=1, prob=rates)<br>
   model              = glm(responses~var1+var2+var12+var1x2+var12x2, <br>
                            family=binomial(link=""logit""))<br>
   significant[i,1:5] = (summary(model)$coefficients[2:6,4]&lt;.05)&lt;br&gt;
&gt;      significant[i,6]   = sum(significant[i,1:5])&lt;br&gt;
&gt;      modelDev           = model$null.deviance-model$deviance<br>
   significant[i,7]   = (1-pchisq(modelDev, 5))&lt;.05<br>
  }<br>
  endT = proc.time()[3]<br>
  endT-startT<br></p>
  
  <p>sum(significant[,1])/repetitions      # pre-specified effect power for var1<br>
  [1] 0.042<br>
  sum(significant[,2])/repetitions      # pre-specified effect power for var2<br>
  [1] 0.017<br>
  sum(significant[,3])/repetitions      # pre-specified effect power for var12<br>
  [1] 0.035<br>
  sum(significant[,4])/repetitions      # pre-specified effect power for var1X2<br>
  [1] 0.019<br>
  sum(significant[,5])/repetitions      # pre-specified effect power for var12X2<br>
  [1] 0.022<br>
  sum(significant[,7])/repetitions      # power for likelihood ratio test of model<br>
  [1] 0.168<br>
  sum(significant[,6]==5)/repetitions   # all effects power<br>
  [1] 0.001<br>
  sum(significant[,6]>0)/repetitions    # any effect power<br>
  [1] 0.065<br>
  sum(significant[,4]&amp;significant[,5])/repetitions   # power for interaction terms<br>
  [1] 0.017<br></p>
</blockquote>

<p>I feel like I should be able to adapt the code from <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and that this would be a better, more succinct option</p>

<blockquote>
  <p>library(rms)</p>
  
  <p>tmpfun &lt;- function(n, beta0, beta1, beta2) { <br>
     x &lt;- runif(n, 0, 10) <br>
     eta1 &lt;- beta0 + beta1*x <br>
     eta2 &lt;- eta1 + beta2 <br>
     p1 &lt;- exp(eta1)/(1+exp(eta1)) <br>
     p2 &lt;- exp(eta2)/(1+exp(eta2)) <br>
     tmp &lt;- runif(n) <br>
     y &lt;- (tmp &lt; p1) + (tmp &lt; p2) <br>
     fit &lt;- lrm(y~x) <br>
     fit$stats[5] <br>
  } <br></p>
  
  <p>out &lt;- replicate(1000, tmpfun(100, -1/2, 1/4, 1/4)) <br>
  mean( out &lt; 0.05 ) <br></p>
</blockquote>

<p>but I'm not completely sure how to do so. I'm assuming tmpfun(100,-1/2, 1/4,1/4) is specifying the N and betas that you want, but how do I adjust tmpfun to another (categorical) variable and include an interaction term? Ultimately the equation should include 6 betas: the intercept, the beta for x, the beta for z1, the beta for z2, the interaction term between x and z1, and the interaction term between x and z2. </p>

<p>Finally, I can't find any reliable sources on what sorts of ""effect sizes"" I should be using as small or medium. </p>

<p>Let me know if I can provide more information!</p>
"
"0.122299897617557","0.125279955557839","105346","<p>I am interested in estimating an adjusted risk ratio, analogous to how one estimates an adjusted odds ratio using logistic regression. Some literature (e.g., <a href=""http://aje.oxfordjournals.org/content/159/7/702.abstract"">this</a>) indicates that using Poisson regression with Huber-White standard errors is a model-based way to do this</p>

<p>I have not found literature on how adjusting for continuous covariates affects this. The following simple simulation demonstrates that this issue is not so straightforward: </p>

<pre><code>arr &lt;- function(BLR,RR,p,n,nr,ce)
{
   B = rep(0,nr)
   for(i in 1:nr){
   b &lt;- runif(n)&lt;p 
   x &lt;- rnorm(n)
   pr &lt;- exp( log(BLR) + log(RR)*b + ce*x)
   y &lt;- runif(n)&lt;pr
   model &lt;- glm(y ~ b + x, family=poisson)
   B[i] &lt;- coef(model)[2]
   }
   return( mean( exp(B), na.rm=TRUE )  )
}

set.seed(1234)
arr(.3, 2, .5, 200, 100, 0)
[1] 1.992103
arr(.3, 2, .5, 200, 100, .1)
[1] 1.980366
arr(.3, 2, .5, 200, 100, 1)
[1] 1.566326 
</code></pre>

<p>In this case, the true risk ratio is 2, which is recovered reliably when the covariate effect is small. But, when the covariate effect is large, this gets distorted. I assume this arises because the covariate effect can push up against the upper bound (1) and this contaminates the estimation.</p>

<p>I have looked but have not found any literature on adjusting for continuous covariates in adjusted risk ratio estimation. I am aware of the following posts on this site: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/18595/poisson-regression-to-estimate-relative-risk-for-binary-outcomes"">Poisson regression to estimate relative risk for binary outcomes</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38004/poisson-regression-for-binary-data"">Poisson regression for binary data</a></li>
</ul>

<p>but they do not answer my question. Are there any papers on this? Are there any known cautions that should be exercised? </p>
"
"0.146176336551172","0.134764368352983","106360","<p>I am running a binomial mixed effects logistic regression in R using <code>glmer</code> for a sociolinguistics project. I was asked to used deviation (effect) coding. From what I gather, in deviation coding the last level in a factor is assigned -1, because this is the level that is never compared to the other levels within that variable. Is it possible to obtain the <code>Estimate</code> (<code>Exp(B)</code> value) for the last level as well by using function <code>relevel</code>? I need to report the estimates for all the levels.</p>

<p>For example, my model has the independent variable called <strong>Orthography</strong> with four levels (<code>s</code>, <code>sh</code>, <code>s1</code>, <code>sh1</code>). The dependent variable is <strong>produced sibilant</strong>. In deviation coding the fourth level (<code>sh1</code>) will not be compared to the other three levels, and estimates will be available for the first three (<code>s</code>, <code>sh</code>, <code>s1</code>). The intercept is the mean of the means of all four levels (<code>s + sh + s1 + sh1 / 4</code>). I am interested in obtaining the estimate for the last level (<code>sh1</code>) as well. Does anyone know how to get that? Do I have to rerun the model by changing levels? If so, does anyone know how to do that? I have been unsuccessful with using function <code>relevel</code> to do this.</p>

<p>I have other terms in my model as well:  </p>

<ul>
<li>following segment, which has two levels (<code>vowel</code>, <code>consonant</code>), </li>
<li>position of sibilant in word (<code>initial</code>, <code>medial</code>, <code>final</code>), </li>
<li>grammatical function (<code>noun</code>, <code>verb</code>, <code>adjectives</code>), and </li>
<li>language of instruction (<code>English</code>, <code>Gujarati</code>).</li>
</ul>

<p>This is the code for my model:</p>

<pre><code>model.final_si = glmer(prod_sib ~ orthography + foll_segment + word_position + 
                                  grammatical_func + language_instruction + 
                                  (1|participant) + (1|item), 
                       family=""binomial"",data=data)
</code></pre>
"
"0.11322770341446","0.0966555841283824","109796","<p>I am currently working on a project using a sales system and trying to come up with a way to use the current pipeline of potential sales to predict the amount of product that will be sold in the future. Iâ€™m looking for advice on how to approach this problem and hopefully some resources to teach me what approach to use and why.</p>

<p>The sales system Iâ€™m using has historical data for opportunities (potential sales). Around 50,000 of the opportunities are â€œclosedâ€ meaning that they are either won or lost. I have around 1,000 â€œopenâ€ opportunities that have not yet been won or lost. Some variables that I have on each sale include the product (which is generally homogenous except for the amount), the amount, the salesman, the date, the time it was input into the system, the customer, and other data about the customer.</p>

<p>I understand that if I want to predict a dichotomous variable like win / lose then I should look at a logistic regression. However, Iâ€™m looking for general advice on how to </p>

<ol>
<li>Predict the probability of each individual opportunity closing as won using the data I have (and how to tell if I've done it correctly).</li>
<li>Estimate the total amount of won opportunities for a period.</li>
</ol>

<p>I found a similar question here <a href=""http://stats.stackexchange.com/questions/66276/using-a-logistic-model-on-the-estimates-of-several-other-classification-models"">Using a logistic model on the estimates of several other classification models</a> but Iâ€™m hoping for a response that gives me a better idea of where to start. Iâ€™m comfortable using R or any other statistical software, but ideally I'd like some kind of book or other reference material that is as low-level as possible.</p>
"
"0.122299897617557","0.125279955557839","112247","<p>I'm trying to use the Match() function from the Matching package in R to do a propensity score analysis.</p>

<p>My outcome of interest is a binary variable (0/1).  My treatment is also a binary variable (0/1).  In addition, I have a number of other variables that I want to control for in this analysis.</p>

<p>First, I fit a logistic regression to define a propensity score for the treatment:</p>

<pre><code>glm1 = glm(Treatment ~ variable1 + variable2 + variable3 + ..., 
           data=dataset, family=""binomial"")
</code></pre>

<p>Then, I used the Match function to estimate the average treatment effect on the treated:</p>

<pre><code>rr1 = Match(Y = Outcome, Tr = Treatment, X = glm1$fitted)
</code></pre>

<p>Finally, I called for a summary:</p>

<pre><code>summary(rr1)
</code></pre>

<p>My question is how to interpret the output.  I get:</p>

<pre><code>Estimate... -0.349,
AI SE... 0.124,
T-stat... -2.827,
p.val... 0.005
</code></pre>

<p>What does this mean?  In particular, what is Estimate?  The documentation says it's ""The estimated average causal effect.""  But what are the units?  Can I interpret this to mean that the treatment reduced the outcome by a relative 35%?  Or by an absolute 0.35?  Or do I need to exponentiate?</p>

<p>Any help on the interpretation would be much appreciated!</p>
"
"0.130744090092123","0.117188667781924","112801","<p>I am seemingly blindly following this <a href=""http://www.cfc.umt.edu/grizzlybearrecovery/pdfs/Schwartz%20et%20al.%202006e.pdf"" rel=""nofollow"">publication</a> that has done work very similar to what I need to accomplish (page 18-21).  My analysis is a multinomial logistic regression where I have 3 possible outcomes 0, 1, or 2 offspring produced.  In the publication, they have recommended a Hosmer-Lemshow and a Persons test for goodness-of-fit.  I have only figured out how to do the Hosmer-Lemshow test and my results are not so good (i.e. P-val is 0.00002).  I have no idea how to do the Pearsons test (suggestions are appreciated).</p>

<p>The paper I am following, of course their tests are ""good"" for model fit (page 21).  But they then go onto suggest that Somers D, the Goodman-Kruskala gamma and the Kendall's tau-a all indicate that their models are a good fit.  But the paper does not report any of the values for these indices or how they calculated them. </p>

<p>I have just found a package <code>ryouready</code> that runs all of these tests.  However, I have been having difficulties finding any help explaining what the values mean, let alone knowing if I have input my variables correctly.</p>

<p>My response variable is number of offspring, most of my explanatory variables are continuous like age or risk.  Do I need to calculate the mean of each explanatory variable within each response variable (get the mean risk for 1 offspring, mean risk for 2 offspring etc...)and then compare those? It also seems that these tests are for 2x2 tables.  If I am just looking at risk, my table would be a 1x3.  However, my complete model will have 4 variables (age, risk, bp, and #offspring year before).  </p>

<p>As you can likely tell, I am in the dark here on where to start. I would appreciate suggested readings, pdf lectures or videos of lectures would even be better!    </p>

<p>EDIT/UPDATE:
I have run the tests over my counts - I have 2 time periods (before/after) and then the count of offspring in each class (0,1,2).  I do not know how to interpret the values - what is ""good"".  What should I be looking for?  Any source that explains these values would be nice to see.  </p>

<pre><code>Kendall's (and Stuart's) Tau statistics
    Tau-b: 0.143
    Tau-c: 0.130
Somers' d:
    Columns dependent: 0.151 
    Rows dependent: 0.136 
    Symmetric: 0.143 
Goodman-Kruskal Gamma: 0.312 
Warning message:
In formatC(x, digits, format = ""f"") : class of 'x' was discarded
</code></pre>
"
"0.139373668334515","0.142769759542091","115219","<p><strong><em>Imagine the situation:</em></strong> Mythical Seafolk use holes in the seabed as their burrows. Each hole has two parameters - diameter and depth. <strong>Majority of holes are unoccupied</strong> due to their surplus (n = 235). Occupied holes (n = 15) are (generally) expected to be much deeper and with larger diameter than random.</p>

<pre><code># generate data
set.seed(1234)
x &lt;- runif(250, min=0, max=10)
y &lt;- runif(250, min=0, max=10)
rbPal   &lt;- colorRampPalette(c(""deepskyblue"",""darkblue""))
my.data &lt;- data.frame(x_coor = x, y_coor = y,
                      diameter = c(abs(rnorm(15)+2.5), abs(rnorm(235))),
                      depth = c(abs(rnorm(15)+2.5), abs(rnorm(235))),
                      usage = rep(c(1,0), times = c(15, 235)))
my.data$col &lt;- rbPal(10)[as.numeric(cut(my.data$depth,breaks = 10))]

# look at the situation
# occupied holes are marked by red circles
plot(my.data$x, my.data$y, cex = my.data$diameter, col = my.data$col, pch = 20)
grid(5, 5, lwd = 0.75, lty = 2, col = ""grey"") 
points(my.data$x[1:25], my.data$y[1:25], pch = 1, cex = 2,
       col = ""red"", lwd = 1.75)
</code></pre>

<p><img src=""http://i.stack.imgur.com/8O1Xg.png"" alt=""enter image description here""></p>

<p>Although this analysis seem to be rather straightforward, <strong>I have some doubts about comparing unequal samples (15 vs 235)</strong>. Such problem do not occur in other <em>Seafolk burrow selection studies</em> because <strong>mapping of seabed is very expensive and time demanding</strong> and when researchers find 10 used holes, they continue to map the seabed only till they get find additonal 10 nonused holes (which are most probably located in close vicinity of used holes). Due to this
data collection approach the sample sizes of compared groups are usually almost equal.
<strong>However, we were able to find ALL holes in seabed - which can be now considered as a ""handicap"".</strong></p>

<p><strong><em>We do not want to do random sampling of 15 holes from 235 to obtain equal sizes (15 vs 15). Why to throw away such large amount of data!? Is there any solution/approach to this situation which is able to fully embrace such unique dataset?</em></strong></p>

<p><em>I have in mind two possible ways (please comment or add others):</em></p>

<p><strong>1.</strong> Take the sample of unoccupied holes (n = 235) and randomly resample them to (for example 50) smaller groups (all with n = 15). Subsequently, each of these reduced groups will be compared with sample of occupied holes by binomial logistic regression. By this I will obtain 50 logistic curves which will
be ""averaged"" into just one curve - final model.</p>

<p><strong>2.</strong> Take the sample of unoccupied holes (n = 235) and randomly resample them to (for example 50) smaller groups (all with n = 15). Subsequently, from each of these reduced groups (and from sample of used holes also) we gain (mean, sd, sample size). Thus, we will have six numbers for each comparison and we calculate effect size - standardized mean difference (SMD). Effect sizes will be inserted into meta-analytic model and tested if there is a heterogeneity present among effect sizes (<em>to see if the ""randomnes"" of creating unused holes sample has the significant impact on test result</em>).</p>
"
"0.0943564195120496","0.0966555841283824","116347","<p>I am trying to generate a data frame of fake data for exploratory purposes. Specifically, I am trying to produce data with a binary dependent variable (say, failure/success), and a categorical independent variable called 'picture' with 5 levels (pict1, pict2, etc.). I am following the answer provided <a href=""http://stats.stackexchange.com/questions/49916/simulating-data-for-logistic-regression-with-a-categorical-variable"">here</a>, which allows me to successfully generate the data. However, I need each level of 'picture' to occur the same number of times (i.e. 11 repetitions of each level = 55 total observations per subject). </p>

<p>Here is a reproducible example of what has worked so far (code from user: ocram):</p>

<pre><code>library(dummies)

#------ parameters ------
n &lt;- 1000 
beta0 &lt;- 0.07
betaB &lt;- 0.1
betaC &lt;- -0.15
betaD &lt;- -0.03
betaE &lt;- 0.9
#------------------------

#------ initialisation ------
beta0Hat &lt;- rep(NA, 1000)
betaBHat &lt;- rep(NA, 1000)
betaCHat &lt;- rep(NA, 1000)
betaDHat &lt;- rep(NA, 1000)
betaEHat &lt;- rep(NA, 1000)
#----------------------------

#------ simulations ------
for(i in 1:1000)
{
  #data generation
  x &lt;- sample(x=c(""pict1"",""pict2"", ""pict3"", ""pict4"", ""pict5""), 
              size=n, replace=TRUE, prob=rep(1/5, 5))  #(a)
  linpred &lt;- cbind(1, dummy(x)[, -1]) %*% c(beta0, betaB, betaC, betaD, betaE)  #(b)
  pi &lt;- exp(linpred) / (1 + exp(linpred))  #(c)
  y &lt;- rbinom(n=n, size=1, prob=pi)  #(d)
  data &lt;- data.frame(picture=x, choice=y)

  #fit the logistic model
  mod &lt;- glm(choice ~ picture, family=""binomial"", data=data)

  #save the estimates
  beta0Hat[i] &lt;- mod$coef[1]
      betaBHat[i] &lt;- mod$coef[2]
  betaCHat[i] &lt;- mod$coef[3]
      betaDHat[i] &lt;- mod$coef[4]
  betaEHat[i] &lt;- mod$coef[5]
}
</code></pre>

<p>However, as you can see from the output, each level of the factor 'picture' does not occur the same number of times (i.e. 200 times each). </p>

<pre><code>&gt; summary(data)
picture     choice     
pict1:200   Min.   :0.000  
pict2:207   1st Qu.:0.000  
pict3:217   Median :1.000  
pict4:163   Mean   :0.559  
pict5:213   3rd Qu.:1.000  
            Max.   :1.000 
</code></pre>

<p>Moreover, it is not entirely clear to me how to manipulate the initial beta values as to determine the probability of success/failure for each level of 'picture'. I cannot comment the original question because I do not yet have the necessary reputation points. </p>
"
"0.10336227882434","0.0847047099775253","122593","<p>I wanted to check whether the level of satisfaction relates to the level of support to the value of democracy.
Dependent variable (support) is binary variable (Good/Bad) and independent variable is ordinary variable (level of satisfaction).
After doing binary logistic regression, I got this â€œstrangeâ€ figure. Is this result correct? Or how can I fix it?</p>

<pre><code>satisfaction &lt;- data_american$V23c
    support &lt;- data_american$V130b
dat=as.data.frame(cbind(satisfaction,suport)) 
library(ggplot2)
ggplot(dat, aes(x=satisfaction, y=support)) + geom_point() + 
  stat_smooth(method=""glm"", family=""binomial"", se=FALSE)
</code></pre>

<p>Here is a part of data:</p>

<pre><code>   satisfaction support
1             7       1
2             8       1
3             8       1
4             8       1
5            10       1
6             6       1
7             7       1
8             7       1
9             7       1
10            8       0
11            8       1
12            7       1
13            7       0
14            1       1
15            7       1
16            8       1
17            6       1
18            7       1
19            8       1
20            8       1
</code></pre>

<p><img src=""http://i.stack.imgur.com/CgjzY.png"" alt=""results I got""></p>

<p>[Added 1]
Based on the first answer, I got this results:</p>

<pre><code>&gt; satisfaction_j &lt;- jitter(satisfaction)
&gt; chisq.test(table(satisfaction_j,support))

    Pearson's Chi-squared test

data:  table(satisfaction_j, support)
X-squared = 2158, df = 2157, p-value = 0.4899

&gt; t.test(satisfaction_j~support)

    Welch Two Sample t-test

data:  satisfaction_j by support
t = -2.7775, df = 459.931, p-value = 0.005703
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.57390716 -0.09829989
sample estimates:
mean in group 0 mean in group 1 
       7.164214        7.500317 
</code></pre>
"
"0.123266710272273","0.126270326349434","123210","<p>I am doing a logistic regression in R, where I am modeling how potholes and weather correlate to accidents. When I run a logistic regression, I get the message ""Algorithm does not converge""</p>

<p>The problem I think I have is that I have 24,000 accidents with only 350 potholes related to these accidents. Is this to small of a sample size?</p>

<p>The other possible issue I thought of, is that when I look at sample logistic regressions, the outcome is either zero or one, but the only outcome I have is the outcome of one, or accident in my case. I do not have any non accident data in my set, could this be what is causing the problem? </p>

<p>I will attach my current code and its output.</p>

<pre><code>require(ggplot2)
require(sandwich)
require(msm)
mydata &lt;- read.csv(""C:\\Users\\myname\\downloads\\logreg1.csv"")
## view the first few rows of the data
head(mydata)
summary(mydata)
mylogit &lt;- glm(Accident ~  Rain + Snow, data = mydata, family = ""binomial"")

&gt; head(mydata)
      Date Pothole Rain Snow Accident
1 1/1/2012       0    0    0        1
2 1/1/2012       0    0    0        1
3 1/1/2012       0    0    0        1
4 1/1/2012       0    0    0        1
5 1/1/2012       0    0    0        1
6 1/1/2012       0    0    0        1
&gt; summary(mydata)
         Date          Pothole            Rain              Snow            Accident
 1/8/2014  :   87   Min.   :0.0000   Min.   :0.00000   Min.   : 0.0000   Min.   :1  
 1/30/2013 :   82   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.: 0.0000   1st Qu.:1  
 3/21/2013 :   77   Median :0.0000   Median :0.00000   Median : 0.0000   Median :1  
 12/21/2012:   76   Mean   :0.0173   Mean   :0.08077   Mean   : 0.1129   Mean   :1  
 3/10/2013 :   66   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.: 0.0000   3rd Qu.:1  
 12/13/2013:   59   Max.   :8.0000   Max.   :3.32000   Max.   :11.1000   Max.   :1  
 (Other)   :23606                                                                   
&gt; mylogit &lt;- glm(Accident ~  Rain + Snow, data = mydata, family = ""binomial"")
Warning message:
glm.fit: algorithm did not converge
</code></pre>
"
"0.153311035167967","0.1570467354963","123498","<p>I would like to generate a confidence interval for predicted vs actual rates.</p>

<p>I am auditing my group of anaesthetists (aka anesthesiologists) to see how we compare on a number of potentially preventable complications (eg post-operative nausea, severe pain, hypothermia).</p>

<p>I have 20000 surgical operation records and I can make a GLM to make a ""case-mix adjusted risk"" (using age, gender, type of surgery, duration of surgery as risk factors) and thus <a href=""http://www.r-tutor.com/elementary-statistics/logistic-regression/estimated-logistic-regression-equation"" rel=""nofollow"">generate a risk</a> for each patient.</p>

<p>I can then aggregate the risk and actual per clinician I can generate an actual and predicted rate for each complication. I can make a confidence interval for my actual - but it seems a bit simplistic to just test to see if the confidence interval on the actual rate includes the rate generated from summing the glm-predicted risks.</p>

<p><a href=""http://stats.stackexchange.com/questions/7344/how-to-graphically-compare-predicted-and-actual-values-from-multivariate-regress"">This question</a> has some pointers to a package but I am hoping for some more specific suggestions.</p>

<p>To clarify what I have already (using ""requirement for pain protocol"" as an example):</p>

<pre><code># make model (dependent variable has values 1/0)
model.pp = glm(
pain_protocol1 ~
age + log_age + age2 + inv_age
+ op_time + log_op_time + op_time2
+ gender
+ category
+ thimble,
family = ""binomial"",
data=d4)
# calculate predicted PACU time and then difference between predicted and actual:
d4$pred_pp = predict(model.pp, newdata=d4, type=""response"", na.action=""na.pass"")

d4$extra_pp = d4$pain_protocol1 - d4$pred_pp
# aggregate deviation from predicted rate
ppr_pa &lt;- aggregate(extra_pp ~ adult_anaesthetist, data=d4, FUN=mean)
barplot(ppr_pa$extra_pp, name=ppr_pa$adult_anaesthetist,las=2) 
</code></pre>

<p>So I can make this plot for my colleagues, showing the variation we have in how much pain our patients experience in the ""post anaesthesia care unit"". These variations are great enough so that they definitely represent a material difference in patient experience, and most of the difference will also be unlikely to be variation due to chance (ie ""statistically significant""). However, as I examine smaller subgroups and other complications that are less frequent it would be good to be able to calculate confidence intervals.</p>

<p><img src=""http://i.stack.imgur.com/FFe8W.png"" alt=""bar plot illustrating difference between predicted and actual rates of &quot;needing pain protocol&quot;""></p>

<p>Note that each clinician has a different number of cases, and each clinician is given a bird code-name for anonymity.</p>
"
"0.122299897617557","0.125279955557839","124616","<p>I am testing the logistic regression classifier in R. I created some test data like this:</p>

<pre><code>x=runif(10000)
y=runif(10000)
df=data.frame(x,y,as.factor(x-y&gt;0))
</code></pre>

<p>basically I am sampling the 2D unit square [0,1] and classifying a point belonging to class A or B depending on which side of y=x it lies.</p>

<p>I generated a scatter plot of the data like below:</p>

<pre><code>names(df) = c(""feature1"", ""feature2"", ""class"")
levels=levels(df[[3]])
obs1=as.matrix(subset(df,class==levels[[1]])[,1:2])
obs2=as.matrix(subset(df,class==levels[[2]])[,1:2])
# make scatter plot
dev.new()
plot(obs1[,1],obs1[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=0,col=colors[[1]])
points(obs2[,1],obs2[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=1,col=colors[[2]])
</code></pre>

<p>it gives me below graph:</p>

<p><img src=""http://i.stack.imgur.com/5zN4y.png"" alt=""scatter plot""></p>

<p>Now I tried running LR (logistic regression) on this data using code below:</p>

<pre><code>model=glm(class~.,family=""binomial"",data=df)
summary(model) # prints summary
</code></pre>

<p>here are the results:</p>

<pre><code>Call:
glm(formula = class ~ ., family = ""binomial"", data = df)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.11832   0.00000   0.00000   0.00000   0.08847  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  5.765e-01  1.923e+01   0.030    0.976
feature1     9.761e+04  8.981e+04   1.087    0.277
feature2    -9.761e+04  8.981e+04  -1.087    0.277

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.3863e+04  on 9999  degrees of freedom
Residual deviance: 2.9418e-02  on 9997  degrees of freedom
AIC: 6.0294

Number of Fisher Scoring iterations: 25
</code></pre>

<p>I also get these warning messages:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>If I try plotting the ROC curve using a varying threshold, I get following graph (AUC=1 which is good):
<img src=""http://i.stack.imgur.com/xbyPX.png"" alt=""enter image description here""></p>

<p><strong>Could someone please explain why the algorithm does not converge and coefficient estimates are not statistically significant (high std. error in coeff estimates)?</strong></p>

<p>I also compared to LDA:</p>

<pre><code>lda_classifier=lda(class~., data=df)
</code></pre>

<p>gives:</p>

<pre><code>Call:
lda(class ~ ., data = df)

Prior probabilities of groups:
 FALSE   TRUE 
0.5007 0.4993 

Group means:
       feature1  feature2
FALSE 0.3346288 0.6676169
TRUE  0.6710111 0.3380432

Coefficients of linear discriminants:
               LD1
**feature1  4.280490
feature2 -4.196388**
</code></pre>
"
"0.221880078490092","0.236756861905189","125453","<p>I have used the â€˜polrâ€™ function in the MASS package to run an ordinal logistic regression for an ordinal categorical response variable with 15 continuous explanatory variables.</p>

<p>I have used the code (shown below) to check that my model meets the proportional odds assumption following advice provided in <a href=""http://www.ats.ucla.edu/stat/r/dae/ologit.htm"">UCLA's guide</a>. However, Iâ€™m a little worried about the output implying that not only are the coefficients across various cutpoints similar, but they are exactly the same (see graphic below). </p>

<pre><code>FGV1b &lt;- data.frame(FG1_val_cat=factor(FGV1b[,""FG1_val_cat""]), 
                    scale(FGV1[,c(""X"",""Y"",""Slope"",""Ele"",""Aspect"",""Prox_to_for_FG"", 
                          ""Prox_to_for_mL"", ""Prox_to_nat_border"", ""Prox_to_village"", 
                          ""Prox_to_roads"", ""Prox_to_rivers"", ""Prox_to_waterFG"", 
                          ""Prox_to_watermL"", ""Prox_to_core"", ""Prox_to_NR"", ""PCA1"", 
                          ""PCA2"", ""PCA3"")]))
b     &lt;- polr(FG1_val_cat ~ X + Y + Slope + Ele + Aspect + Prox_to_for_FG + 
                            Prox_to_for_mL + Prox_to_nat_border + Prox_to_village + 
                            Prox_to_roads + Prox_to_rivers + Prox_to_waterFG + 
                            Prox_to_watermL + Prox_to_core + Prox_to_NR, 
              data=FGV1b, Hess=TRUE)
</code></pre>

<p>View a summary of the model:</p>

<pre><code>summary(b)
(ctableb &lt;- coef(summary(b)))
q        &lt;- pnorm(abs(ctableb[, ""t value""]), lower.tail=FALSE) * 2
(ctableb &lt;- cbind(ctableb, ""p value""=q))
</code></pre>

<p>And now we can look at the confidence intervals for the parameter estimates:</p>

<pre><code>(cib &lt;- confint(b)) 
confint.default(b)
</code></pre>

<p>But these results are still quite hard to interpret, so let's convert the coefficients into odds ratios</p>

<pre><code>exp(cbind(OR=coef(b), cib))
</code></pre>

<p>Checking the assumption. So the following code will estimate the values to be graphed. First it shows us the logit transformations of the probabilities of being greater than or equal to each value of the target variable</p>

<pre><code>FG1_val_cat &lt;- as.numeric(FG1_val_cat)
sf &lt;- function(y) {
  c('VC&gt;=1' = qlogis(mean(FG1_val_cat &gt;= 1)),
    'VC&gt;=2' = qlogis(mean(FG1_val_cat &gt;= 2)),
    'VC&gt;=3' = qlogis(mean(FG1_val_cat &gt;= 3)),
    'VC&gt;=4' = qlogis(mean(FG1_val_cat &gt;= 4)),
    'VC&gt;=5' = qlogis(mean(FG1_val_cat &gt;= 5)),
    'VC&gt;=6' = qlogis(mean(FG1_val_cat &gt;= 6)),
    'VC&gt;=7' = qlogis(mean(FG1_val_cat &gt;= 7)),
    'VC&gt;=8' = qlogis(mean(FG1_val_cat &gt;= 8)))
}
(t &lt;- with(FGV1b, summary(as.numeric(FG1_val_cat) ~ X + Y + Slope + Ele + Aspect + 
                             Prox_to_for_FG + Prox_to_for_mL + Prox_to_nat_border + 
                             Prox_to_village + Prox_to_roads + Prox_to_rivers + 
                             Prox_to_waterFG + Prox_to_watermL + Prox_to_core + 
                             Prox_to_NR, fun=sf)))
</code></pre>

<p>The table above displays the (linear) predicted values we would get if we regressed our dependent variable on our predictor variables one at a time, without the parallel slopes assumption. So now, we can run a series of binary logistic regressions with varying cutpoints on the dependent variable to check the equality of coefficients across cutpoints</p>

<pre><code>par(mfrow=c(1,1))
plot(t, which=1:8, pch=1:8, xlab='logit', main=' ', xlim=range(s[,7:8]))
</code></pre>

<p><img src=""http://i.stack.imgur.com/4Uicq.jpg"" alt=""polr assumption check""></p>

<p>Apologies that I am no statistics expert and perhaps I am missing something obvious here. However, I have spent a long time trying to figure out if there is a problem in how I tested the model assumption and also trying to figure out other ways to run the same kind of model. </p>

<p>For example, I read in many help mailing lists that others use the vglm function (in the VGAM package) and the lrm function (in the rms package) (for example see here:  <a href=""http://stats.stackexchange.com/questions/25988/proportional-odds-assumption-in-ordinal-logistic-regression-in-r-with-the-packag"">Proportional odds assumption in ordinal logistic regression in R with the packages VGAM and rms</a>). I have tried to run the same models but am continuously coming up against warnings and errors.</p>

<p>For example, when I try to fit the vglm model with the â€˜parallel=FALSEâ€™ argument (as the previous link mentions is important for testing the proportional odds assumption), I encounter the following error:</p>

<blockquote>
  <p>Error in lm.fit(X.vlm, y = z.vlm, ...) : NA/NaN/Inf in 'y'<br>
  In addition: Warning message:<br>
  In Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals = residuals,  :
    fitted values close to 0 or 1</p>
</blockquote>

<p>I would like to ask please if there is anyone who might understand and be able to explain to me why the graph I produced above looks as it does. If indeed it means that something isnâ€™t right, could you please help me find a way to test the proportional odds assumption when just using the polr function. Or if that is just not possible, then I will resort to trying to use the vglm function, but would then need some help to explain why I keep getting the error given above.</p>

<p>NOTE: As a background, there are 1000 datapoints here, which are actually location points across a study area. I am looking to see if there are any relationships between the categorical response variable and these 15 explanatory variables. All of those 15 explanatory variables are spatial characteristics (for example, elevation, x-y coordinates, proximity to forest etc.). The 1000 datapoints were randomly allocated using a GIS, but I took a stratified sampling approach. I made sure that 125 points were randomly chosen within each of the 8 different categorical response levels. I hope this information is also helpful.</p>
"
"0.0800640769025436","0.0820149827720712","125603","<p>I am new to the world of <strong>Regression</strong> in statistics and I have been doing a research in which I am building an ordinal logistic regression model (ORM). In order to fit my ORM model, I am using the 'orm' function of 'rms' package from R (<a href=""http://cran.r-project.org/web/packages/rms/rms.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/rms/rms.pdf</a>).</p>

<p>Now I am trying to assess the goodness of fit of my model. By reading the R documentation, I can see the following statement in the 'stat' property of the 'orm' object (pg.98):</p>

<p>""(...)Nagelkerke R2 index, the g-index, gr (the g-index on the odds ratio scale),
and <strong>pdm (the mean absolute difference between 0.5 and the predicted probability
that $Y\geq q$  the marginal median)</strong>(...).""</p>

<p>I don't have enough background to understand the short description of the pdm measure. But when I try to do more research on this measure, I am not able to find related material (e.g. I've been finding ""prescription drug misuse""). In summary, my question is:</p>

<p>Would you know if the 'pdm' measure has some synonym which is more widely used? Or can you provide some references where I can study the pdm metric?</p>
"
"0.122299897617557","0.125279955557839","126338","<p>I'm running a binary prediction using a supervised topic modeling package in R (<code>lda</code> package, using <code>slda.predict</code> function). The result of the prediction returns results in linear space. From Googling around, people say that I need to take a sigmoid  to convert the result to a logical value. I'm not really sure what this means. </p>

<p>Basically I have list of documents, and their corresponding labels. What I am trying to do is set 80% of these documents and their labels, and train them using supervised LDA. The label of the document is 0 or 1. I manage to train the document just fine using this piece of code:</p>

<pre><code>example &lt;- c(""I am the role model"",""I have a major crazy   headache"",""i don't have money"", ""you are money crazy major"")
corpus = lexicalize(example, lower=TRUE)
label = c(1,1,0,0)
params &lt;- sample(c(1, 0), 2, replace=TRUE)
result &lt;- slda.em(documents=corpus$documents,
              K=2,
              vocab=poliblog.vocab,
              num.e.iterations=10,
              num.m.iterations=4,
              alpha=1.0, eta=0.1,
              label,
              params,
              variance=0.25,
              lambda=1.0,
              logistic=TRUE,
              method=""sLDA"")
</code></pre>

<p>for simplicity purpose, i'll try to predict the same document given the model above.</p>

<pre><code>predictions &lt;- slda.predict(corpus$documents,
                            result$topics, 
                        result$model,
                        alpha = 1.0,
                        eta=0.1)
</code></pre>

<p>Now, my problem is, the result of the prediction isn't binary. it's continuous value. I need to convert it back to binary using some sort of sigmoid(according to an <a href=""https://lists.cs.princeton.edu/pipermail/topic-models/2012-June/001912.html"" rel=""nofollow"">article here</a>) </p>

<p>The result i'm getting doesn't seem like a probability. For the 4 documents above, this is the output of the predictions variable</p>

<pre><code>           [,1]
[1,]  44.827420
[2,]  53.895682
[3,] -17.139034
[4,]   1.299764
</code></pre>

<p>How do I do this in R?</p>
"
"0.122299897617557","0.125279955557839","126715","<p>I am doing a logistic regression. The predictor variables are a mixture of categorical and continuous. I ran glm and out of the 80 predictors about 36 came out to be significant based on the p value. The accuracy of the model was also very good.</p>

<ol>
<li><p>I am still stuck with 36 variables but I want to narrow it down further to identify which of the predictors have the greatest impact. I understand that all the 36 predictors are statistically significant but all of these variables do not impact the DV equally. Is their a way to rank these variables based on their influence on the DV? Please feel free to suggest any methods/algorithms you know that does this efficiently.</p></li>
<li><p>Once I narrow down the predictors of my interest, I want to come up with rules based on the variables, much like a decision tree gives. I have tried running <code>rpart</code> and <code>ctree</code> on the 80 variable dataset but the output tree is very small meaning only a few variables appear in the tree, and thus there are very few rules which I can make based on that. I wonder if their is a way to increase the size of my tree to include more variables. Suppose I narrow down to 10-12 predictors, what all modeling techniques can I use to makes rules.</p>

<p>For example, I want something like: when x1 in range (a, b), x2 in range (c, d), ... and so on then the probability of $y(dv) &gt; 0.5$ or the event occurs i.e., $y = 1$ so that the range of values of the predictors can act as rules for determining when the event occurs.</p></li>
</ol>
"
"NaN","NaN","127226","<p>I'm trying to simulate a logistic regression. My goal is showing that if <code>Y=1</code> is rare, than the intercept is biased. In my R script I define the logistic regression model through the latent variable's approach (see for example pp. 140 <a href=""http://gking.harvard.edu/files/abs/0s-abs.shtml"" rel=""nofollow"">http://gking.harvard.edu/files/abs/0s-abs.shtml</a>):</p>

<pre><code>x   &lt;- rnorm(10000)

b0h &lt;- numeric(1000)
b1h &lt;- numeric(1000)

for(i in 1:1000){
  eps &lt;- rlogis(10000)
  eta &lt;- 1+2*x+eps
  y   &lt;-numeric(10000)
  y   &lt;- ifelse (eta&gt;0,1,0)

  m      &lt;- glm(y~x,family=binomial)
  b0h[i] &lt;- coef(m)[1]
  b1h[i] &lt;- coef(m)[2]
}

mean(b0h)
mean(b1h)
hist(b0h)
hist(b1h)
</code></pre>

<p>The problem here is that I don't know how to force the observations y to be balanced before (50:50), then unbalanced (90:10). As we can see with the function table(), in my script the proportion of ones is random.</p>

<pre><code>table(y)
</code></pre>

<p>How to solve this problem?</p>
"
"0.11322770341446","0.115986700954059","129657","<p>What is the fastest algorithm for fitting a simple logistic 'random effects' type model, with only one level of categorical predictors? </p>

<p>Another way of putting it might be a logistic regression with a Gaussian prior on the coefficients, or ""with shrinkage"".</p>

<p>I'm looking for a very fast and reliable implementation to use in a production environment. This means that the algorithm would need to have a low risk of 'hanging', and a not-drastically-variable time to converge.</p>

<p>There would be between 1 and 5000 data points per 'cell', and 5-100 groups/categories. It would need to exploit sufficient statistics (take counts of group data). Second-level nesting a bonus, but not essential.</p>

<p>This could be done via <code>lme4</code> in <code>R</code>. However, is there a library (e.g. stand-alone C++) which is more efficient for this narrowly-defined type of model?</p>

<p>EDIT: Goal is inference over prediction - specifically, comparison of group estimates (with standard errors), construction of confidence intervals etc.</p>

<p>EDIT: Just to make it clear, I wouldn't be fitting a 'mixed model' so to speak - there would be no fixed effect. The data would be a very long two-column ('successes', 'failures') contingency table, with highly variable n counts.</p>

<p>EDIT: I need the degree of 'shrinkage' in the individual estimates to be informed by the group level variance (as opposed to banging a Jeffery's prior on each individual estimate, or using an Agresti-Coull (1998) type interval).</p>
"
"0.211829636434081","0.196325415034609","130313","<p>In a logistic Generalized Linear Mixed Model (family = binomial), I don't know how to interpret the random effects variance:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev.
 HOSPITAL (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14
</code></pre>

<p>How do I interpret this numerical result?</p>

<p>I have a sample of renal trasplanted patients in a multicenter study. I was testing if the probability of a patient being treated with a specific antihypertensive treatment is the same among centers. The proportion of patients treated varies greatly between centers, but may be due to differences in basal characteristics of the patients. So I estimated a generalized linear mixed model (logistic), adjusting for the principal features of the patiens.
This are the results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: HTATTO ~ AGE + SEX + BMI + INMUNOTTO + log(SCR) + log(PROTEINUR) + (1 | CENTER) 
   Data: DATOS 

     AIC      BIC   logLik deviance 
1815.888 1867.456 -898.944 1797.888 

Random effects:
 Groups   Name        Variance Std.Dev.
 CENTER (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14

Fixed effects:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)               -1.804469   0.216661  -8.329  &lt; 2e-16 ***
AGE                       -0.007282   0.004773  -1.526  0.12712    
SEXFemale                 -0.127849   0.134732  -0.949  0.34267    
BMI                        0.015358   0.014521   1.058  0.29021    
INMUNOTTOB                 0.031134   0.142988   0.218  0.82763    
INMUNOTTOC                -0.152468   0.317454  -0.480  0.63102    
log(SCR)                   0.001744   0.195482   0.009  0.99288    
log(PROTEINUR)             0.253084   0.088111   2.872  0.00407 ** 
</code></pre>

<p>The quantitative variables are centered.
I know that the among-hospital standard deviation of the intercept is 0.6554, in log-odds scale.
Because the intercept is -1.804469, in log-odds scale, then probability of being treated with the antihypertensive of a man, of average age, with average value in all variables and inmuno treatment A, for an ""average"" center, is 14.1 %.
And now begins the interpretation:  under the assumption that the random effects follow a normal distribution, we would expect approximately 95% of centers to have a value within 2 standard deviations of the mean of zero, so the probability of being treated for the average man will vary between centers with coverage interval of:</p>

<pre><code>exp(-1.804469-2*0.6554)/(1+exp(-1.804469-2*0.6554))

exp(-1.804469+2*0.6554)/(1+exp(-1.804469+2*0.6554))
</code></pre>

<p>Is this correct?</p>

<p>Also, how can I test in glmer if the variability between centers is statistically significant?
I used to work with MIXNO, an excellent software of Donald Hedeker, and there I have an standard error of the estimate variance, that I don't have in glmer.
How can I have the probability of being treated for the ""average"" man in each center, with a confidene interval?</p>

<p>Thanks</p>
"
"0.0653720450460613","0.0334824765091213","132787","<p>How do you calculate marginal effects of parameters of logit model in R uging package {glm}?</p>

<p>Are following codes correct?</p>

<pre><code>#### preparation ####
# dependent variable
yseed &lt;- rnorm(100)
y &lt;- ifelse(yseed &gt; 0, 1, 0)

# independent variables
x1 &lt;- rnorm(100, mean=100, sd=20)
x2 &lt;- rnorm(100, mean=50, sd=20)
X &lt;- cbind(1, x1, x2)ã€€
Xmean &lt;- apply(X, 2, mean)

#### analysis ####
# logit model
res &lt;- glm(y ~ x1 + x2, family=binomial(link=""logit""))
summary(res)

# Marginal effects (ME) calculation
LAMBDA &lt;- function(x) { 1 / (1 + exp(-x))}ã€€# cdf of standard logistic distribution

# ME of (intercept)
ME_1  &lt;- coef[1] * LAMBDA(Xmean %*% coef(res)) * (1 - LAMBDA(Xmean %*% coef(res)))
# ME of x1   
ME_x1 &lt;- coef[2] * LAMBDA(Xmean %*% coef(res)) * (1 - LAMBDA(Xmean %*% coef(res))) 
# ME of x2
ME_x2 &lt;- coef[3] * LAMBDA(Xmean %*% coef(res)) * (1 - LAMBDA(Xmean %*% coef(res))) 
</code></pre>
"
"0.146784140987997","0.150360801748797","132971","<p>There is something I'm not quite understanding conceptually about the output from generalized linear mixed models. I have read that the target of inference in GLMMs is subject-specific. For example, the accepted answer to <a href=""http://stats.stackexchange.com/questions/17331/what-is-the-difference-between-generalized-estimating-equations-and-glmm"">this</a> question states that in a logistic GLMM the odds-ratios are conditioned on both the fixed and random effects. So, in a GLMM of pupils within classrooms, with random intercepts for classroom (i.e., the ""subject"" in this case), the odds-ratios will differ for each classroom as there will be many random intercepts. So far, this makes sense to me.</p>

<p>What I am confused about is that the typical output from the fixed effects part of such a model reports just one odds-ratio. For example, in the R example I provide below, the odds-ratio for the fixed effect of <code>sex</code> is .662. I have three questions:</p>

<ol>
<li><p><strong>How do I interpret this single fixed effect odds-ratio?</strong><br>
(Is it an odds-ratio ignoring the random effects? Is it an odds-ratio of the average random effect - in which case, isn't it a population average? Is it calculated assuming the random effect variance is zero?) </p></li>
<li><p><strong>Is it possible to calculate a population average odds-ratio using the output from a GLMM?</strong><br>
I know this can be done using a GEE, but what about a GLMM?</p></li>
<li><p><strong>How would I go about calculating the odds-ratio for a particular random effect (a particular classroom, lets say class 7 in the example below)?</strong><br>
Presumably this involves combining the fixed and random effect estimates somehow.</p></li>
</ol>

<p><strong>EDIT 1:</strong>
It seems after doing more reading (for example, this <a href=""http://stats.stackexchange.com/questions/32419/difference-between-generalized-linear-models-generalized-linear-mixed-models-i?lq=1"">post</a>), that since the fixed effect for <code>sex</code> in this example does not have its own random effect (e.g., a random slope), there will be no subject-level interpretation of this parameter. Does this mean that only the intercept term in the model below is subject-specific, while the <code>sex</code> term is a population average?</p>

<pre><code># dummy data:
set.seed(1)
dat &lt;- data.frame(Y         = factor(sample(rep(c(0, 1), 100))),
                  sex       = factor(sample(rep(c(""M"", ""F""), 100))),
                  classroom = factor(sample(rep(paste(""class"", 1:10), 20)))
) 

# model:
library(lme4)
fit &lt;- glmer(Y ~ sex + (1 | classroom), family=binomial, data=dat)

# summary(fit)
exp(fixef(fit))
# (Intercept)   sexM 
#  1.229       0.662 
</code></pre>
"
"0.20149017494233","0.195536697202333","133571","<p>I know there are already lots of questions around this topic (especially <a href=""http://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models/16415#16415"">this one</a> and <a href=""http://stats.stackexchange.com/questions/24689/interpreting-coefficients-of-ordinal-logistic-regression-when-there-is-clusterin/29701#29701"">this one</a>) but I haven't really seen anything that directly helps me (It will be obvious I'm not a great statistician, but I'll do my best to explain). </p>

<p>I am running an ordinal regression in R (<code>clm</code> and <code>clmm</code>). My response variable is a rating between 0 and 4. I have two types of explanatory variables: individual and scenario variables [let's say <code>IVs</code> and <code>SVs</code>]. </p>

<p>Six different scenario variables (all dummies with at most 4 different values) represent potential collaboration scenarios that get rated by the respondent (between 0 and 4) creating the response variable. (Research design is a conjoint analysis; there are a total of 192 different scenarios possible)</p>

<p>On top of that I have a variety of individual characteristics about the respondent (age, gender, work experience, networking skills, ...) all derived from a survey.</p>

<p>Every respondent rates between 3 and 16 different scenarios (average 8.1); every scenario is rated by at least 8 respondents. Every respondent and every scenario have a unique identifier (called <code>IVid</code> and <code>SVid</code>). So they are non nested within each other.</p>

<p>Thus the basic regression looks like this:</p>

<pre><code>clm.base &lt;- clm(rating ~ SVs + IVs, data = dt) 
</code></pre>

<p>The hypothesis I am trying to test is that there are specific individual characteristics, that will influence the rating of the scenarios, independent of the actual content of the scenarios. Basically, some people are more or less favourable to all types of collaboration scenarios. </p>

<p>Now a reviewer of my paper asks me to include individual fixed effects (which in management [my field] basically means dummies for each individual). My assumption originally was that this would result in all individual variables being dropped. This is exactly what happens when I use another model (package <code>lfe</code>)</p>

<pre><code>felm.complete &lt;- felm(rating ~ SVs + IVs | SVid + IVid | 0 | IVid, data = dt) 
</code></pre>

<p>In this regression basically all my variables are perfectly collinear as expected.
However, when I approximate this in the ordinal package, there is no perfect collinearity. I presume this is related that <code>clmm</code> adds so-called 'random effects'. The regression takes a couple of minutes to run but eventually returns results</p>

<pre><code>clmm.complete &lt;- clmm(rating ~ SVs + IVs + (1|SVid) + (1|IVid), data = dt)
</code></pre>

<p>Now, the results here are pretty useless:</p>

<ul>
<li>All but one of my IVs are insignificant</li>
</ul>

<p>I am trying to understand what exactly happens when adding the <code>(1|IVid)</code> term in the <code>clmm</code> model. If it basically adds something like an individual dummy than the fact almost everything is now insignificant is no surprise. The coefficients of the <code>IVid</code> dummies would capture the effect I am looking for (some people rate all scenarios higher or lower, regardless of scenario content) most accurately.</p>

<p>Now I wonder whether this interpretation is correct or whether the results I got from running the simple <code>clm</code> regression are just not reliable? </p>

<p>Concretely, I'd like to find out:</p>

<ul>
<li>What happens when adding a random effect to <code>clmm</code></li>
<li>A laymen explanation of how the Laplace approximation works</li>
<li>How to group errors around individuals when running <code>clm</code></li>
<li>Is it possible to extract the coefficients of these random effects <code>(1|id)</code> for as far as there is such a thing?</li>
</ul>
"
"0.11322770341446","0.115986700954059","134335","<p>I am using <a href=""http://cran.r-project.org/web/packages/twang/vignettes/mnps.pdf"" rel=""nofollow"">mnsp</a> function to estimate propensity scores for multiple treatments. Then, I generate weights using the survey package, but I cannot use svyglm to estimate my treatment effects because my outcome is not binary or numeric. </p>

<p>My outcome variable has 3 categories, therefore I want to estimate relative risk ratios by running a multinomial logistic model. However, survey package does not allow multinomial logit. It seems like mlogit function allows weights. Would it be fine to just plug in the weights I derived from the get.weights function? </p>

<p>I am a novice in R, any recommendations are welcome. Here is my R code:</p>

<pre><code>DAT &lt;-read.delim(""DAT.txt"", header=TRUE)
library(twang)
set.seed(1)
mnps.DAT &lt;- mnps(city_ber ~ age + unemplyd + student + moedum + faedum,
                data = TIES, estimand = ""ATE"", verbose = FALSE,
               stop.method = c(""es.mean"", ""ks.mean""),
                n.trees = 10000)
#Diagnostics
plot(mnps.DAT, plots = 1)
plot(mnps.DAT, plots = 2)
plot(mnps.DAT, plots = 3, pairwiseMax = FALSE, figureRows = 3)
means.table(mnps.DAT, stop.method = ""es.mean"", digits = 3)

#Generating Weights
require(survey)
DAT$w &lt;- get.weights(mnps.DAT, stop.method = ""es.mean"")
    design.mnps &lt;- svydesign(ids=~1, weights=~w, data=DAT)
    summary(DAT$w)

#Outcome analysis?
</code></pre>
"
"0.131558702896054","0.14973818705887","135967","<p>I am a beginner in R. I am doing logistic regression using around 80 independent variables using <code>glm</code> function in R. The dependent variable is <code>churn</code> which says whether a customer churned or not. I want to know how to identify the right combination of variables to get a good predictive logistic regression model in R.  I also want to know how to identify the same for making good decision tree in R ( I am using the <code>ctree</code> function from the <code>party</code> package).
So far, I had used <code>drop1</code> function  and  <code>anova(LogMdl, test=""Chisq"")</code> where <code>LogMdl</code> is my logistic regression model to drop unwanted variables in the predictive model.  But maximum accuracy I was able to achieve was only 60%. </p>

<p>Also I am not sure if I am using the <code>drop1</code> and <code>anova</code> functions correctly. I dropped the variables with lowest AIC using <code>drop1</code> function.  Using <code>anova</code> function, I dropped variables with p value > 0.05</p>

<p>Kindly help me how to identify the right set of variables for both logistic regression and decision tree models to increase my model's predictive accuracy to close to 90% or more than that if possible.   </p>

<pre><code>library(party)
setwd(""D:/CIS/Project work"")
CellData &lt;- read.csv(""Cell2Cell_SPSS_Data - Orig.csv"")
trainData &lt;- subset(CellData,calibrat==""1"")
testData &lt;- subset(CellData,calibrat==""0"") # validation or test data set
LogMdl = glm(formula=churn ~ revenue  + mou    + recchrge+ directas+ 
               overage + roam    + changem +
               changer  +dropvce + blckvce + unansvce+ 
               custcare+ threeway+ mourec  +
               outcalls +incalls + peakvce + opeakvce+ 
               dropblk + callfwdv+ callwait+
               months  + uniqsubs+ actvsubs+  phones  + models  +
               eqpdays  +customer+ age1    + age2    + 
               children+ credita + creditaa+
               creditb  +creditc + creditde+ creditgy+ creditz + 
               prizmrur+ prizmub +
               prizmtwn +refurb  + webcap  + truck   + 
               rv      + occprof + occcler +
               occcrft  +occstud + occhmkr + occret  + 
               occself + ownrent + marryun +
               marryyes +marryno + mailord + mailres + 
               mailflag+ travel  + pcown   +
               creditcd +retcalls+ retaccpt+ newcelly+ newcelln+ 
               refer   + incmiss +
               income   +mcycle  + creditad+ setprcm + setprc  + retcall, 
               data=trainData, family=binomial(link=""logit""),
               control = list(maxit = 50))
ProbMdl = predict(LogMdl, testData, type = ""response"")
testData$churndep = rep(0,31047)  # replacing all churndep with zero
testData$churndep[ProbMdl&gt;0.5] = 1   # converting records with prob &gt; 0.5 as churned
table(testData$churndep,testData$churn)  # comparing predicted and actual churn
mean(testData$churndep!=testData$churn)    # prints the error %
</code></pre>

<p>Link for documentation of variables: <a href=""https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/</a></p>

<p>Link for Dataset (.csv file) : 
<a href=""https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/</a></p>

<p>I could not produce the output of <code>dput</code> since the data size is more than 5 MB. So I have zipped the file and placed in the above link. </p>

<p>Description of important variables:
* <code>churn</code> is the variable that says whether a customer churned or not.....
* <code>churndep</code> is the variable that needs to be predicted in the test data (validation data) and has to be compared with the <code>churn</code> variable which is already populated with actual churn.
For both churn and churndep, value of 1 means churned and 0 means not churned.</p>
"
"0.0653720450460613","0.0669649530182425","137901","<p>I am using the package MatchIt in R to perform propensity score matching. I have chosen to use nearest neighbor matching with a caliper of 0.2 and since in my case i have more cases than controls i have to use the replacement=TRUE option, so that a control can be used more than once.</p>

<p>The graphical histogram check is satisfying and the stand.mean differences are all small with a max of 0.03 (btw any other suggestions for testing the matching?)
I want to use the matched dataset to check the treatment effect after all the matching(perform logistic regression with mortality as outcome and treatment as explanatory variable now) and i am wondering if i should take into consideration the weights that were resulted from the matching. Since i used the replacement option not all observations have a weight of 1 anymore. Shall i use this somehow or can i just perform an unweighted final logistic regression on the matched data to estimate the effect of treatment.</p>
"
"0.0653720450460613","0.0669649530182425","138176","<p>I used a logistic regression on a variable indicating whether a person of an address-dataset took part in a survey (1), or not (0). I extracted the probabilities of each person to participate and calculated the inverse-probability (hence the name of the weighting method - inverse propensity score weighting). </p>

<p>What irritates me, is, that my smallest survey-weight is 1.901. I expected the smallest survey weight to at least be below ""1"". </p>

<p>I hope somebody can help me and either find out where i made a mistake, or assure me, that iÂ´m on the right track. Any help is greatly appreciated! Thank you!</p>

<hr>

<hr>

<pre><code>#Calculate logistic regression 
glm2&lt;-glm(indicator ~ var1 + varx,family=binomial,data=sampleframe)

#extract inverse probability of every case  
sampleframe$weight&lt;-glm2$fitted^-1

#combine the survey-weight to the survey-data 
surveydata&lt;-left_join(surveydata,sampleframe, by=""ID"")

#diagnostics:
#summary of the weights for the complete sampleframe    
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.901   2.810   3.247   3.616   3.836  12.070

#summary of the survey-weights of the participants   
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.925   2.686   3.078   3.308   3.502  12.070 

#comparison of mean-weight for participants (1) / non-participants (0)   
indicator weight.mean 
0    3.755967 
1    3.295854
</code></pre>
"
"0.201742510889601","0.18599249845384","138230","<p>I want to perform propensity score matching of observational data of an Intensive Care Unit in order to find out wheather hydroxyethyl starch is better or worse than colloids in terms of renal replacement therapy (RRT), Akute Kidney Injury (AKI) and mortality. </p>

<p>I use the MatchIt package in R (King et al. 2007 - <a href=""http://gking.harvard.edu/matchit"" rel=""nofollow"">http://gking.harvard.edu/matchit</a>). This package is quite well documented. But there are some things that I dont understand.
First I matched on sociodemographic covariates (as this seems standard protocol with matching): Gender, weight, height and age.
Nearest neighbor matching seems to have worked:</p>

<p>NN matching</p>

<pre><code>m.out.nn

Call: 
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0

Treatment status treat1 is HES = yes , Colloids otherwise btw. I did a numerical balance check and balance actually WORSENED after matching. Overall as well as some of the covariates drastically:
Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9168        0.9145     0.0131    0.0022  0.0018   0.0023   0.0388
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056   1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362   1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390   1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462  30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604  30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916   6.2000
BMI                      28.4858       27.8005    15.1559    0.6853  0.7080   2.1550 347.8520


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9357        0.9145     0.0131    0.0212  0.0172   0.0212   0.0687
Geschlecht                0.0446        0.0000     0.0000    0.0446  0.0000   0.0446   1.0000
Geschlechtm               0.9053        0.6100     0.4884    0.2953  0.0000   0.2953   1.0000
Geschlechtw               0.0501        0.3900     0.4884   -0.3398  0.0000   0.3398   1.0000
Gewicht.kg               98.1744       77.8930    18.2092   20.2813 17.0000  20.2813  62.0000
Groesse.cm              164.7103      169.9861    11.9693   -5.2758  2.0000   5.4540  77.0000
Alter.bei.ITS.Aufn       72.5702       63.4808    14.4918    9.0894  7.0000   9.0894  26.5000
BMI                      44.1753       27.8005    15.1559   16.3748  6.7500  16.3748 258.9020

Percent Balance Improvement:
                   Mean Diff.   eQQ Med  eQQ Mean   eQQ Max
distance            -852.5593 -839.8420 -808.2029  -77.0723
Geschlecht          -998.6072    0.0000 -700.0000    0.0000
Geschlechtm         -714.0668    0.0000 -715.3846    0.0000
Geschlechtw         -742.6908    0.0000 -771.4286    0.0000
Gewicht.kg         -1533.1522 -750.0000 -998.5214 -106.6667
Groesse.cm         -7691.0845      -Inf -617.2161 -156.6667
Alter.bei.ITS.Aufn  -715.7611 -775.0000 -603.7093 -327.4194
BMI                -2289.4307 -853.3898 -659.8482   25.5712

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0
</code></pre>

<p>How can this be possible?</p>

<p>I also did genetic matching (Sekhon 2011 - <a href=""http://sekhon.berkeley.edu/matching/"" rel=""nofollow"">http://sekhon.berkeley.edu/matching/</a>). This is a fancy algorithm that automatically optimizes covariate balance. There covariate balance has indeed improved (as it should have):</p>

<pre><code>Genetic matching
load(file=""m.out.genetic.RData"")
Numerical Balance Check 
summary(m.out.genetic)

Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn, data = hes.vs.kristall.clean, method = ""genetic"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9147     0.0126    0.0021  0.0019   0.0022  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362  1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390  1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462 30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604 30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916  6.2000


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9164     0.0105    0.0003  0.0018   0.0021  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6481     0.4782   -0.0018  0.0000   0.0364  1.0000
Geschlechtw               0.3496        0.3519     0.4782   -0.0023  0.0000   0.0392  1.0000
Gewicht.kg               79.1349       79.0556    15.9832    0.0793  2.0000   1.7801 30.0000
Groesse.cm              169.9184      170.0479    10.6992   -0.1296  0.0000   0.7703 30.0000
Alter.bei.ITS.Aufn       64.5950       64.7378    13.3160   -0.1428  0.8000   1.2440  6.2000

Percent Balance Improvement:
                   Mean Diff. eQQ Med eQQ Mean eQQ Max
distance              83.7418  3.8423   2.7923       0
Geschlecht             0.0000  0.0000  -0.5602       0
Geschlechtm           95.1066  0.0000  -0.5602       0
Geschlechtw           94.3414  0.0000  -0.5602       0
Gewicht.kg            93.6115  0.0000   3.5817       0
Groesse.cm           -91.3359  0.0000  -1.2969       0
Alter.bei.ITS.Aufn    87.1817  0.0000   3.6903       0

Sample sizes:
          Control Treated
All           359    3944
Matched       357    3944
Unmatched       2       0
Discarded       0       0
</code></pre>

<p>I also checked balance graphically and it did improve (despite being good pre-matching).</p>

<p>Now my questions are:</p>

<ol>
<li><p>Can I use the nearest neighbor matched data? How could I change this so that balance does improve? What kind of distance metric does Nearest neighbor matching use (by default) (Euclidean ?). Because with Euclidean the non-Boolean covariates (Gender) could be made more important than they are.</p></li>
<li><p>How can I perform analysis after matching? - How can I get the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATET) in terms of HES for AKI, RRT and mortaility and does that make sense for these response variables (AKI, RRT and mortaility)? Or should I get the odds ratio for Akute Kidney Injury, renal replacement therapy and mortaility from the matched observational data? How do I get these values?
I know that MatchIt recommends using Zelig to get these values but that didn't seem to work with my data. 
Can I use logistic regression with the matched data to get the odds ratio of HES vs. Cristalloids of AKI, RRT and mortality ?</p></li>
</ol>
"
"0.197104131996361","0.191811583729112","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"NaN","NaN","140605","<p>I am trying to compare a linear model and other non linear models(Asymptotic, Logistic and Ricker) by means of an F test or a likelihood ratio test. I have tried anova(Linear, Logistic,Ricker, Asymptote) but this generates an error. Is there a way to do this in R?</p>

<p>I used the following models:</p>

<pre><code>Linear&lt;-lm(mean~age,Lmaxl)
Logistic&lt;- nlsLM(mean ~ k/(1+((k- Bo)/Bo)*exp(-r*age)), 
    data=Lmaxl, start=list(k=50,Bo=20,r=0.1), 
    control=liâ€Œst(maxiter=200),
    na.action=""na.exclude"")
Asymptote&lt;-nlsLM(mean ~k+(Bo-)*exp(-r*age), 
    data=Lmaxl,
    start=list(k=50,Bo=20,r=0.1),
    control=list(maxitâ€Œâ€‹er=200),
    na.action=""na.exclude"")
 Ricker&lt;- nlsLM( mean~ Bo+(a*age)*exp(-b*age), 
    data=Lmaxl, 
    start=list(Bo=10, a=5, b=0.01),
    control=list(maxiter=200),
    na.action=""na.exclude"")
</code></pre>
"
"0.153846153846154","0.157594893307774","140761","<p>I have a question regarding the use of propensity score in a survival analysis with use of mutliple imputation to handle missing data. The question is of theoretical nature and may well apply to other situations.</p>

<p>I have a data set of <em>n</em> individuals. The aim is to estimate the effect of a treatment on a binary outcome (death). The analysis is based on propensity score; the propensity score is derived by means of logistic regression, which includes 30 predictors variables. Effect estimation is carried out by means of Cox regression (which uses the propensity score in various ways [stratification, covariate adjustments etc]). There are a large number of patients, and on average 2â€“7% missing for each variable (of which there are 30 included in the prop. score).</p>

<p>Thus, I have a large data set with a substantial amount of missing data (at least in terms of complete cases) which is why I use multiple imputation - 5 complete data sets are imputed. Now the question is what to do with the muliply imputed data sets; which one of the strategies below should I prefer?</p>

<p><strong>1.</strong> Calculate one average propensity score for each individual using the 5 separate data sets. That way, each individual will have one propensity score, which is the average from the n complete data sets. Then do the Cox regression..</p>

<p><strong>2.</strong> Analyze each separate multiply imputed data set (with Cox regression), and then pool the 5 hazard ratio estimates to one hazard ratio.</p>

<p>The second method appears to be used more often, but is it better/worse?</p>

<p>Any thoughts about this?</p>
"
"0.17937941173235","0.183750315148211","141844","<p>I tried to plot the results of an ordered logistic regression analysis by calculating the probabilities of endorsing every answer category of the dependent variable (6-point Likert scale, ranging from ""1"" to ""6""). However, I've received strange probabilities when I calculated the probabilities based on this formula: $\rm{Pr}(y_i \le k|X_i) = \rm{logit}^{-1}(X_i\beta)$.</p>

<p>Below you see how exactly I tried to calculate the probabilities and plot the results of the ordered logistic regression model (<code>m2</code>) that I fitted using the <code>polr</code> function (<code>MASS</code> package). The probabilities (<code>probLALR</code>) that I calculated and used to plot an ""expected mean score"" are puzzling as the expcected mean score in the plot increases along the RIV.st continuum while the coefficient for <code>RIV.st</code> is negative (-0.1636). I would have expected that the expected mean score decreases due to the negative main effect of <code>RIV.st</code> and the irrelevance of the interaction terms for the low admiration and low rivalry condition (LALR) of the current 2 by 2 design (first factor = <code>f.adm</code>; second factor = <code>f.riv</code>; dummy coding 0 and 1).</p>

<p>Any idea of how to make sense of the found pattern? Is this the right way to calculate the probabilities? The way I used the intercepts in the formula to calculate the probabilities might be problematic (cf., <a href=""https://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression"">Negative coefficient in ordered logistic regression</a>).</p>

<pre><code>m2 &lt;- polr(short.f ~ 1 + f.adm*f.riv + f.adm*RIV.st + f.riv*RIV.st, data=sampleNS)

# f.adm  = dummy (first factor of 2 by 2 design);
# f.riv  = dummy (second factor of 2 by 2 design);
# RIV.st = continuous predictor (standardized)
summary(m2)
Coefficients:
                Value Std. Error t value
f.adm1         1.0203    0.14959  6.8203
f.riv1        -0.8611    0.14535 -5.9240
RIV.st        -0.1636    0.09398 -1.7403
f.adm1:f.riv1 -1.2793    0.20759 -6.1625
f.adm1:RIV.st  0.0390    0.10584  0.3685
f.riv1:RIV.st  0.6989    0.10759  6.4953

Intercepts:
    Value    Std. Error t value 
1|2  -2.6563   0.1389   -19.1278
2|3  -1.2139   0.1136   -10.6898
3|4  -0.3598   0.1069    -3.3660
4|5   0.9861   0.1121     8.7967
5|6   3.1997   0.1720    18.6008
</code></pre>

<p>Here you see how I tried to calculate the probabilities (<code>probLALR</code>) for 1 of the 4 conditions of the 2 by 2 design:</p>

<pre><code>inv.logit  &lt;- function(x){ return(exp(x)/(1+exp(x))) }
Pred       &lt;- seq(-3, 3, by=0.01)
b = c(-2.6563,-1.2139,-0.3598,0.9861,3.1997) # intercepts of model m2
a = c(1.0203,-0.8611,-0.1636,-1.2793,0.0390,0.6989) # coefficients of m2
probLALR   &lt;- data.frame(matrix(NA,601,5))
for (k in 1:5){ 
    probLALR[,k] &lt;- inv.logit(b[k] + a[1]*0 + a[2]*0 + 
                               a[3]*Pred  + a[4]*0*0 + 
                               a[5]*Pred*0 + a[6]*Pred*0)
}

plot(Pred,probLALR[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,probLALR[,2],col=""red"")             # p(1 or 2)
lines(Pred,probLALR[,3],col=""green"")           # P(1 or 2 or 3)
lines(Pred,probLALR[,4],col=""orange"")          # P(1 or 2 or 3 or 4)
lines(Pred,probLALR[,5],col=""orange"")          # P(1 or 2 or 3 or 4 or 5)

# option response functions:

orc = matrix(NA,601,6)
orc[,6] = 1-probLALR[,5]        # prob of 6
orc[,5]= probLALR[,5]-probLALR[,4]  # prob of 5
orc[,4]= probLALR[,4]-probLALR[,3]  # prob of 4
orc[,3]= probLALR[,3]-probLALR[,2]  # prob of 3
orc[,2]= probLALR[,2]-probLALR[,1]  # prob of 2
orc[,1]= probLALR[,1]           # prob of 1


plot(Pred,orc[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,orc[,2],col=""red"")             # p(2)
lines(Pred,orc[,3],col=""green"")           # P(3)
lines(Pred,orc[,4],col=""orange"")          # P(4)
lines(Pred,orc[,5],col=""purple"")          # P(5)
lines(Pred,orc[,6],col=""purple"")          # P(6)

# mean score

mean = orc[,1]*1+orc[,2]*2+orc[,3]*3+orc[,4]*4+orc[,5]*5+orc[,6]*6
plot(Pred,mean,type=""l"",xlab=""RIV.st"",ylab=""expected mean score"",ylim=c(1,6))  
</code></pre>
"
"0.130744090092123","0.133929906036485","149012","<p><a href=""http://en.wikipedia.org/wiki/Discrete_choice#F._Logit_with_variables_that_vary_over_alternatives_.28also_called_conditional_logit.29"" rel=""nofollow"">Conditional logistic regression</a> is a <a href=""http://en.wikipedia.org/wiki/Fixed_effects_model"" rel=""nofollow"">fixed effects model</a>. If you're modeling the dependent variable $y$, a glm fixed effect model doesn't actually model $y$. Instead, the glm fixed effect models measure $y-mean(y)$ for a particular group. I think that this is <em>not</em> the case for a conditional logistic regression. The coefficients of the regression can be interpreted in the space of $y$. Is that correct?</p>

<p>My particular situation:
I am running a conditional logit with <a href=""https://stat.ethz.ch/R-manual/R-devel/library/survival/html/clogit.html"" rel=""nofollow"">clogit</a> in R, from the <code>survival</code> package. Are the coefficients returned to be interpreted in the space of $y$, or in the space of something like $y-mean(y)$? </p>

<p>Normally the difference isn't very relevant; one would interpret the coefficient roughly the same either way. However, in my case one of the independent variables is fitted as a spline. Specifically, it is a restricted cubic spline, as calculated from <code>rcspline.eval</code> in the <a href=""http://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf"" rel=""nofollow"">Hmisc</a> package. <code>clogit</code> produces a coefficient for each knot of the spline, and in order to interpret the overall effect of the variable one needs to reconstruct the spline from the coefficients (using <code>rcspline.restate</code>). I want to make sure that I should be looking at the shape of this spline in the range of $y$ (which in my case is 0-100) or in the range of something like $y-mean(y)$ (in this case, $mean(y)$ is the same for all groups: 50). If it is the case that the space is shifted this will be particularly weird for a spline, because presumably the knots should also be shifted somehow.</p>
"
"0.10336227882434","0.105880887471907","153510","<p>I am trying to fit a regularized logistic regression to my data using glmnet. Using $\alpha=1$ I get a LASSO-regression, which is what I want. My problem is though that I don't know how the intercept is fitted. In glmnet one has the option to put <code>Intercept=TRUE</code> or <code>Intercept=FALSE</code>. As far as I understand <code>FALSE</code> sets my intercept to 0. When <code>TRUE</code>, I understood that the intercept was fitted as the mean of the $y$-values. Since my data is balanced binary data with values 0 and 1, $\bar{y}=0.5$, but my analysis gives me the value -2.6. </p>

<p>I read <a href=""http://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet"">How is the intercept computed in GLMnet? </a> but I don't understand it, so I hope someone will give some details. Also, in the link's article there is a likelihood function (13) and (14) on page 8 and I don't understand why it has $1/N$ in front.  </p>
"
"0.179028718509858","0.158938933098249","154112","<p>I have a dataset with more than 20 predictors and a single binary response variable. With only $n=181$ observations (64 deaths, 117 survivors), I decided to apply penalized logistic regression to modeling, with all predictors involved (so that I avoid problems associated with model selection). Nevertheless, I have to produce a ''simpler'' model too (i.e. one that is simple enough to be suitable for a nomogram-style hand calculation in clinical setting). For that end, I intend to use <code>rms</code>'s <code>fastbw</code>.</p>

<p>To exemplify my questions, I'll use the <code>support</code> dataset from <code>Hmisc</code>:</p>

<pre><code>library( rms )
getHdata( support )
fit &lt;- lrm( hospdead ~ rcs( age ) + sex + rcs( meanbp ) + rcs( crea ) + rcs( ph ) + rcs( sod ), data = support, x = TRUE, y = TRUE )
fit
</code></pre>

<p>First, I apply penalization:</p>

<pre><code>p &lt;- pentrace( fit, seq( 0, 10, by = 0.01 ) )
plot( p )
fitPen &lt;- update( fit, penalty = p$penalty )
fitPen
</code></pre>

<p>I hope I'm correct up to this point.</p>

<p>Next, I validate the model and calculate its calibration curve. If I understand it correctly, I shouldn't validate/calibrate the simpler model, rather, I have to run the necessary functions on the <em>original</em> model, but with <code>bw=T</code>. That is:</p>

<pre><code>validate( fitPen, B = 1000, bw = TRUE )
plot( calibrate( fitPen, B = 1000, bw = TRUE ) )
</code></pre>

<p><strong>Question #1</strong>: Am I correct in this? I.e. is it true that to get the simpler model's validation/calibration I have to run these not on the simpler model, but on the original one (with <code>bw=T</code>)? And the results will be those pertaining to the simpler model, despite the fact that I haven't run validation/calibration on the simpler model itself?</p>

<p>Next, I try to come up with the simpler model <em>explicitly</em>. Interestingly, <a href=""http://www.aliquote.org/cours/2011_health_measures/harrell98.pdf"" rel=""nofollow"">(Harrell, 1998)</a> uses a method which is based on calculating the logits for the observations, then modeling them with OLS, then narrowing this model with <code>fastbw</code>. Although it is surely my statistical shortcoming, I simply can't understand why this is necessary.</p>

<p><strong>Question #2</strong>: Why can't we <em>directly</em> use <code>fastbw</code> on the logistic regression model? Such as:</p>

<pre><code> fastbw( fitPen )
 fitApprox &lt;- lrm( as.formula( paste( ""hospdead ~"", paste( fastbw( fitPen )$names.kept, collapse = ""+"" ) ) ), data = support, x = TRUE, y = TRUE )
</code></pre>

<p>And finally, I am not completely sure on where should I apply penalizing in the whole process.</p>

<p><strong>Question #3</strong>: Should I penalize the original model, then run <code>fastbw</code> (see above), and then re-penalize the obtained model? I.e.</p>

<pre><code>p &lt;- pentrace( fitApprox, seq( 0, 10, by = 0.01 ) )
plot( p )
fitApproxPen &lt;- update( fitApprox, penalty = p$penalty )
fitApproxPen
</code></pre>

<p>Or I don't have to re-penalize the narrowed model? Or I don't have to penalize the original model and it is sufficient to penalize the simpler one? (I suspect that the very first option is the correct, but I'm not entirely sure.)</p>
"
"0.174325453456164","0.189734033551687","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"0.0943564195120496","0.115986700954059","154917","<p>I have written <code>R</code> codes for simulating data from Multilevel logistic regression model . </p>

<p>I focus on the following multilevel logistic model with
one explanatory variable at level 1 (individual level) and
one explanatory variable at level 2 (group level) : </p>

<p>$$\text{logit}(p_{ij})=\pi_{0j}+\pi_{1j}x_{ij}\ldots (1)$$
$$\pi_{0j}=\gamma_{00}+\gamma_{01}z_j+u_{0j}\ldots (2)$$
$$\pi_{1j}=\gamma_{10}+\gamma_{11}z_j+u_{1j}\ldots (3)$$</p>

<p>where , $u_{0j}\sim N(0,\sigma_0^2)$ , $u_{1j}\sim N(0,\sigma_1^2)$ , $\text{cov}(u_{0j},u_{1j})=\sigma_{01}$</p>

<p>In this <a href=""http://www.biomedcentral.com/1471-2288/7/34#sec2"" rel=""nofollow"">paper</a> in equation (2) , they assumed $\text{cov}(u_{0j},u_{1j})=\sigma_{01}$ , that is not independent . But also they mentioned in the methodology section that :</p>

<blockquote>
  <p>The group random components $u_{0j}$ and $u_{1j}$
  are ""independent"" normal variables with mean zero and
  standard deviations $Ïƒ_0$ and $Ïƒ_1$. </p>
</blockquote>

<p>So I assumed $\text{cov}(u_{0j},u_{1j})=0$ . </p>

<p>R code :</p>

<pre><code>## Simulating data from multilevel logistic regression 

set.seed(1234)
x &lt;- rnorm(1000) ### individual level variable
z &lt;- rnorm(1000) ### group level variable

##fixed effect parameter
g_00 &lt;- -1
g_01 &lt;- 0.3
g_10 &lt;- 0.3
g_11 &lt;- 0.3

g &lt;- matrix(c(g_00,g_01,g_10,g_11),ncol=1)

require(mvtnorm)

##need variance values as input 
s2_0 &lt;- 0.36
s2_1 &lt;- 1
s01 &lt;- 0

##generate bi-variate normal rv for u0, u1

avg &lt;- c(0,0) ##mean
sigma &lt;- matrix(c(s2_0,s01,s01,s2_1),ncol=2)

u &lt;- rmvnorm(1000,mean=avg,sigma=sigma,method=""chol"")

pi_0j &lt;- g_00 +g_01*z + as.vector(u[,1])
pi_1j &lt;- g_10 +g_11*z + as.vector(u[,2])
p &lt;- exp(pi_0j+pi_1j*x)/(1+exp(pi_0j+pi_1j*x))

y &lt;- rbinom(1000,1,p)
</code></pre>

<p>But i am not understanding where is to consider the group ? If i select number of groups to be $100$ $(j=1,2,\ldots, 100)$, then will I assign the groups randomly against each $y_{i,j}$ ?</p>

<ul>
<li>Have i correctly simulated data from <code>Multilevel Logistic Distribution</code> ?</li>
</ul>
"
"0.10336227882434","0.105880887471907","154986","<p>We want to do the logistic regression analysis to consider the effect of Age, CD4 on drug resistance mutations. The code that we wrote is:</p>

<pre><code>logist.summary(glm(DRM ~ Age, data = Database, family = binomial),""wald"")
</code></pre>

<p>The results are: </p>

<pre><code>            log.OR OR lower.CI upper.CI p.value
(Intercept)  -0.31 0.74     0.05     9.95  0.8169
Age          -0.07 0.93     0.86     1.00  0.0525
</code></pre>

<p>However, we want to do the test like, we will consider whether, 20 years old differences between the subjects, what the results is? Is it relative to DRMs? We wrote:</p>

<pre><code>logist.summary(glm(DRM ~ I(Age+20), data = Database, family = binomial),""wald"")
</code></pre>

<p>Results:</p>

<pre><code>            log.OR   OR lower.CI upper.CI p.value
(Intercept)   1.17 3.22     0.05   190.62  0.5742
I(Age + 20)  -0.07 0.93     0.86     1.00  0.0525
</code></pre>

<p>I want to ask:</p>

<ul>
<li>Is the code we wrote correct?</li>
<li>Can you help me explain what is meaning of these table?</li>
<li>Why it is the same results for the Age and Age+20? But differences in the Intercept? What does intercept meaning in this case?</li>
</ul>
"
"0.20149017494233","0.206399847046907","156465","<p>The following multilevel logistic model with
one explanatory variable at level 1 (individual level) and
one explanatory variable at level 2 (group level) : </p>

<p>$$\text{logit}(p_{ij})=\pi_{0j}+\pi_{1j}x_{ij}\ldots (1)$$
$$\pi_{0j}=\gamma_{00}+\gamma_{01}z_j+u_{0j}\ldots (2)$$
$$\pi_{1j}=\gamma_{10}+\gamma_{11}z_j+u_{1j}\ldots (3)$$</p>

<p>where , the group-level residuals $u_{0j}$ and $u_{1j}$ are assumed to have a multivariate normal distribution with expectation zero . The variance of the residual errors  $u_{0j}$ is specified as $\sigma^2_0$ , and the variance of the residual errors  $u_{1j}$ is specified as $\sigma^2_1$ .</p>

<p>I want to estimate the parameter of the model and I like to use  <code>R</code> command <code>glmmPQL</code> . </p>

<p>Substituting  equation (2) and (3) in equation (1) yields ,</p>

<p>$$\text{logit}(p_{ij})=\gamma_{00}+\gamma_{10}x_{ij}+\gamma_{01}z_j+\gamma_{11}x_{ij}z_j+u_{0j}+u_{1j}x_{ij}\ldots (4)$$</p>

<p>There are 30 groups$(j=1,...,30)$ and 5 individual in each group .</p>

<p>R code  :</p>

<pre><code>   #Simulating data from multilevel logistic distribution 
   library(mvtnorm)
   set.seed(1234)

   J &lt;- 30             ## number of groups
   n_j &lt;- rep(5,J)     ## number of individuals in jth group
   N &lt;- sum(n_j)

   g_00 &lt;- -1
   g_01 &lt;- 0.3
   g_10 &lt;- 0.3
   g_11 &lt;- 0.3

   s2_0 &lt;- 0.13  ##variance corresponding to specific ICC
   s2_1 &lt;- 1     ##variance standardized to 1
   s01  &lt;- 0     ##covariance assumed zero

   z &lt;- rnorm(J)
   x &lt;- rnorm(N)

   #Generate (u_0j,u_1j) from a bivariate normal .
   mu &lt;- c(0,0)
  sig &lt;- matrix(c(s2_0,s01,s01,s2_1),ncol=2)
  u &lt;- rmvnorm(J,mean=mu,sigma=sig,method=""chol"")

  pi_0 &lt;- g_00 +g_01*z + as.vector(u[,1])
  pi_1 &lt;- g_10 + g_11*z + as.vector(u[,2])
  eta &lt;- rep(pi_0,n_j)+rep(pi_1,n_j)*x
  p &lt;- exp(eta)/(1+exp(eta))

  y &lt;- rbinom(N,1,p)
</code></pre>

<p>Now the parameter estimation .</p>

<pre><code>  #### estimating parameters 
  library(MASS)
  library(nlme)

  sim_data_mat &lt;- matrix(c(y,x,rep(z,n_j),rep(1:30,n_j)),ncol=4)
  sim_data &lt;- data.frame(sim_data_mat)
  colnames(sim_data) &lt;- c(""Y"",""X"",""Z"",""cluster"")
  summary(glmmPQL(Y~X*Z,random=~1|cluster,family=binomial,data=sim_data,,niter=200))
</code></pre>

<h3>OUTPUT :</h3>

<pre><code>      iteration 1
      Linear mixed-effects model fit by maximum likelihood
      Data: sim_data 

      Random effects:
      Formula: ~1 | cluster
              (Intercept)  Residual
      StdDev: 0.0001541031 0.9982503

      Variance function:
      Structure: fixed weights
      Formula: ~invwt 
      Fixed effects: Y ~ X * Z 
                      Value Std.Error  DF   t-value p-value
      (Intercept) -0.8968692 0.2018882 118 -4.442404  0.0000
      X            0.5803201 0.2216070 118  2.618691  0.0100
      Z            0.2535626 0.2258860  28  1.122525  0.2712
      X:Z          0.3375088 0.2691334 118  1.254057  0.2123
      Correlation: 
           (Intr) X      Z     
      X   -0.072              
      Z    0.315  0.157       
      X:Z  0.095  0.489  0.269

      Number of Observations: 150
      Number of Groups: 30 
</code></pre>

<ul>
<li><p>Why does it take only $1$ iteration while I mentioned to take $200$ iterations inside the function <code>glmmPQL</code> by the argument <code>niter=200</code> ?</p></li>
<li><p>Also p-value of group-level variable $(Z)$ and cross-level interaction $(X:Z)$ shows they are not significant . Still why in this <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1955447/"" rel=""nofollow"">article</a>, they keep the group-level variable $(Z)$ and cross-level interaction $(X:Z)$ for further analysis ?</p></li>
<li><p>Also How are the degrees of freedom <code>DF</code> being calculated ?</p></li>
<li><p>It doesn't match with the relative bias of the various estimates of <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1955447/table/T1/"" rel=""nofollow"">the table</a> .  I tried to calculate the relative bias as :</p>

<pre><code> #Estimated Fixed Effect parameters :

 hat_g_00 &lt;- -0.8968692 #overall intercept
 hat_g_10 &lt;- 0.5803201  # X
 hat_g_01 &lt;-0.2535626   # Z
 hat_g_11 &lt;-0.3375088   #X*Z

fixed &lt;-c(g_00,g_10,g_01,g_11)
hat_fixed &lt;-c(hat_g_00,hat_g_10,hat_g_01,hat_g_11)


#Estimated Random Effect parameters :

hat_s_0 &lt;-0.0001541031  ##Estimated Standard deviation of random intercept 
hat_s_1 &lt;-  0.9982503 

std  &lt;- c(sqrt(0.13),1) 
hat_std  &lt;- c(0.0001541031,0.9982503) 

##Relative bias of Fixed Effect :
rel_bias_fixed &lt;- ((hat_fixed-fixed)/fixed)*100
[1] -10.31308  93.44003 -15.47913  12.50293

##Relative bias of Random Effect :
rel_bias_Random &lt;- ((hat_std-std)/std)*100
[1] -99.95726  -0.17497
</code></pre></li>
<li>Why doesn't the relative bias match with the table ?</li>
</ul>
"
"0.0462250163521024","0","156804","<p>I have estimated a mixed-effects logistic regression with glmer
and want to draw a bootstrapped confidence-region for the mean predicted probability for two subgroups of the sample.</p>

<p>I have a $1000 \times 2$ Matrix $X$ containing the bootstrapped mean predicted probabilities for the two groups.
One could now compute the empirical covariance matrix $S$ and draw a
circle around the means using the metric induced by $S^{-1}$,
i.e. drawing a confidence ellipsoid based on normality-assumption.</p>

<p>Are there any widely used alternatives to this approach that do not imply distributional assumptions? </p>

<p>skeletor</p>
"
"0.0653720450460613","0.0669649530182425","159301","<p>I have a classification problem I am attempting to model using logistic regression (via the <code>glm</code> package in R): </p>

<pre><code>cols &lt;- c(""x"", ""z"", ""a"", ""b"", ""c"") 
formula = paste0(""x ~ "", paste(cols, collapse = ""+""))
formula = as.formula(formula)
</code></pre>

<p>I have a bunch of explanatory variables at the moment. How advisable is it to model this relationship using <code>gbm</code>, see the relative inference strength of each variable, and then remove seemingly meaningless variables before <code>glm</code> regression?</p>

<p>I ask just because I have done this in the past, with <code>glm</code> and <code>gbm</code> giving seemingly contradictory signals on different explanatory variables. </p>
"
"0.166666666666667","0.170727801083421","160638","<h1>General question</h1>

<p>When I perform a logistic regression using lrm and specify weights for the observations, I get the following warning message:</p>

<blockquote>
  <p>Warning message:
  In lrm(Tag ~ DLL, weights = W, data = tagdata, x = TRUE, y = TRUE) :
    currently weights are ignored in model validation and bootstrapping lrm fits</p>
</blockquote>

<p>My interpretation is that everything that the rms package will tell me regarding goodness-of-fit, notably using the residuals.lrm tool, is wrong. Is this correct?</p>

<h1>Specific example</h1>

<p>To be more specific, I have working example. All the code and output can be found in this <a href=""https://github.com/jwimberley/crossvalidated-posts/tree/master/lrm_gof"" rel=""nofollow"">GitHub repository</a>. I have two CSV tables of data, <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toystudy.csv"" rel=""nofollow"">toystudy.csv</a> and <a href=""https://github.com/jwimberley/crossvalidated-posts/raw/master/lrm_gof/realstudy.csv"" rel=""nofollow"">realstudy.csv</a>. There are three columns in each:</p>

<ol>
<li>The binomial response $y$ (0 or 1) [called Tag in code]</li>
<li>The predictor $x$ [called DLL in code]</li>
<li>The weight for the observation [called W in code]</li>
</ol>

<p>The former is simulated data, where all the weights are unity and where a logistic regression $log(\pi) = \theta_0 + \theta_1 x$ should fit the data perfectly. The latter is real data from my analysis, where the validity of this simple model is in question. The real data has weighted observations. (Some of the weights are negative, but there is a well-defined reason for this). The analysis code in contained completely in <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/regressionTest.R"" rel=""nofollow"">regressionTest.R</a>; the meat of the code is</p>

<pre><code>library(rms)
fit &lt;- lrm(Tag ~ DLL, weights = W, data = tagdata, x=TRUE, y=TRUE)
residuals(fit,""gof"")
</code></pre>

<p>Here are the results for the two tables of data.</p>

<h3>Case 1: Toy data</h3>

<p>The goodness-of-fit claimed by lrm (which is something called the le Cessie-van Houwelingen-Copas-Hosmer test, I understand?) is very good:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toy/residuals.png"" alt=""enter image description here""></p>

<p>This is confirmed by grouping the data into 20 quantiles of the predictor and overlaying the predicted success rate over the average actual success rate:</p>

<p><img src=""http://i.stack.imgur.com/hOEFs.png"" alt=""enter image description here""></p>

<h3>Case 2: Real data</h3>

<p>In this case, the goodness-of-fit reported by lrm is horrendous:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/real/residuals.png"" alt=""enter image description here""></p>

<p>However, I don't think it should be that bad. Again grouping the data into quantiles, and taking into account the weights when computing the average values in each bin:</p>

<p><img src=""http://i.stack.imgur.com/mgzhc.png"" alt=""enter image description here""></p>

<p>Comparing the prediction to the observed values and their standard errors, I don't think this is that bad (the error bars here depend on how the standard error on a weighted mean is computed, so they might not be 100% right, but should at least be close). On the other hand, if I produce the same plot while ignoring the weights:</p>

<p><img src=""http://i.stack.imgur.com/dId9F.png"" alt=""enter image description here""></p>

<p>I can definitely imagine this fit being as poor as the goodness-of-fit test says.</p>

<h2>Conclusion</h2>

<p>So, is residuals.rm simply ignoring the weights when it calculates its goodness-of-fit statistic? And if so, is there any R package that will do this correctly?</p>
"
"0.130744090092123","0.133929906036485","161113","<p>I am working on example 7.3.1 from the Second Edition of the book <a href=""https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=an+introduction+to+generalized+linear+models+second+edition+pdf"" rel=""nofollow"">An Introduction to Generalized Linear Models</a> in section <em>7.3 Dose response models</em>. This example fits a simple logistic regression model on the following data: </p>

<p><img src=""http://i.stack.imgur.com/YkHCG.png"" alt=""enter image description here""></p>

<p>This seems easy enough. However, I am having an issue with the Deviance Statistic calculated for this example. The following is my R code that will reproduce a Deviance Statistic $D=11.23$ just like this example in the book has. </p>

<pre><code>#original data
#copied in by row
( df &lt;-  data.frame( 
  Trial = 1:8,
  Dose = c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839),
  Yes = c(6, 13, 18, 28, 52, 53, 61, 60),
  No = c(59, 60, 62, 56, 63, 59, 62, 60)- c(6, 13, 18, 28, 52, 53, 61, 60),
  Total = c(59, 60, 62, 56, 63, 59, 62, 60)
) )

#Logistic Regression Model
mle_beet &lt;- glm(cbind(Yes, No)~Dose, family=binomial(logit), data=df)
mle_beet$deviance
##
</code></pre>

<p>Section 5.6.1 of this same book derives the <em>Deviance Statistic</em> for the Binomial Model to be: </p>

<p>$D = 2\sum^{N}_{i=1}y_{i}[ log_{e}(\frac{y_i}{\hat{y_i}})+(n_i - y_i)log_{e}(\frac{n_i - y_i}{n_i - \hat{y_i}}) ]$</p>

<p>However, looking closely at the given data, it can be seen that for the last row, the number of beetles killed is the same as the total number of beetles ( $n_{8}=y_{8}$ ). This means that the very last part in the sum for <code>D</code> is: </p>

<p>$ y_{8}log_{e}(\frac{y_8}{\hat{y_8}})+(n_8 - y_8)log_{e}(\frac{n_8 - y_8}{n_8 - \hat{y_8}}) = 60log_{e}(\frac{60}{\hat{y_8}})+(0)log_{e}(\frac{0}{n_8 - \hat{y_8}})$</p>

<p>In particular, this value contains: </p>

<p>$0log_{e}(0)=0(-\infty)=$ <strong><em>undefined</em></strong></p>

<p>Here is the R code that agrees with this: </p>

<pre><code>sum( 2*(df$Yes*(log(df$Yes/(mle_beet$fitted.values*df$Total))) + (df$Total-df$Yes)*
log((df$Total-df$Yes)/(df$Total-mle_beet$fitted.values*df$Total) ) ) )
</code></pre>

<p>My question is: What is the mathematical reasoning for computing the Deviance Statistic when $n_i=y_i$? What do the book and R do in the background to obtain $D=11.23$?</p>

<p>(Note that the book likely didn't use R to get this value, but the two agree)</p>

<p>Thank you!</p>

<p>EDIT: See the accepted answer and its comments for a great explanation.</p>

<p>If you happen to be computing the Deviance through the formula in R (you likely shouldn't since <code>mle_beet$deviance</code> shows this for you), you can replace <code>-Inf</code> or <code>Nan</code> in each vector that results from an individual operation. The following works for this example: </p>

<pre><code>x &lt;- df$Yes*(log(df$Yes/(mle_beet$fitted.values*df$Total))) 
x[is.na(x) | x==-Inf ] &lt;- 0 #only in a case $n_i = y_i$ 
y &lt;- (df$Total-df$Yes)*
    log((df$Total-df$Yes)/(df$Total-mle_beet$fitted.values*df$Total) ) ) 
    y[is.na(y) | y==-Inf ] &lt;- 0 #only in a case $n_i = y_i$ 

sum(x+y)*2 #the deviance
</code></pre>
"
"0.160604018609905","0.177172612243394","163181","<p>I'm running a logistic regression to find a relationship between falls and drugs taken by someone. What happens is that every time I re-run the algorithm it gives a different result. </p>

<p>The table is this:</p>

<pre><code>caseID fallFlag hypSeds antiPsycho antiHypertensives NSAIDs centralMuscleRelax
     1     TRUE   FALSE      FALSE             FALSE  FALSE              TRUE
     2    FALSE    TRUE      FALSE             TRUE   FALSE              FALSE
     3     TRUE   FALSE      TRUE              FALSE  TRUE               TRUE
     4    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
     5     TRUE   FALSE      TRUE              FALSE  FALSE              FALSE
     6    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
</code></pre>

<p>The <code>TRUE</code> flags mean that the individual took that medicine, and <code>FALSE</code> otherwise. </p>

<p>The algorithm to perform the logistic regression is the following</p>

<pre><code># Match column labels
cols &lt;- c(""hypSeds"", ""antiPsycho"", ""antiHypertensives"", ""NSAIDs"", ""centralMuscleRelax"")

# Data frame to store the OR and CIs 
coefficients &lt;- data.frame(drugNames=cols)

# This loop run through the match labels
# - perform a logistic regression for each classifier
# - get the OR and CIs coefficients and store the coefficients into a data frame

for(i in 1:length(cols)){
  eqString  &lt;- as.formula(paste(""fallFlag"", cols[i], sep=""~""))
  model     &lt;- glm(eqString, observation, family=""binomial"")
  modelCoef &lt;- exp(cbind(coef(model), confint(model)))

  coefficients$OR[i]    &lt;- modelCoef[2] # odds ratios
  coefficients$CIMin[i] &lt;- modelCoef[4] # lower confidence limit
  coefficients$CIMax[i] &lt;- modelCoef[6] # upper confidence limit
}
</code></pre>

<p>In this algorithm I run a logistic regression on each of the drug categories against the <code>fallFlag</code>. Then, I exponentiate the coefficients to find the odds ratios.  </p>

<p>Every time I restart the R studio and run this algorithm it results differently. For example, here is an actual result:  </p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.4347210 1.2534578 1.643824
2     antiPsycho         2.1583970 1.8225014 2.564792
3     antiHypertensives  1.0327465 0.9041444 1.179742
4     NSAIDs             0.9857518 0.8824338 1.101139
5     centralMuscleRelax 0.9597043 0.7240041 1.271461
</code></pre>

<p>But the previous result was:</p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.2870853 1.1286756 1.468686
2     antiPsycho         1.9665091 1.6684292 2.324333
3     antiHypertensives  1.1718176 1.0218085 1.344455
4     NSAIDs             1.0263196 0.9178526 1.147658
5     centralMuscleRelax 1.2014783 0.8928298 1.621132
</code></pre>

<p>As you can see the results were very different, and this has been happening every time I load and build the observation table again. It's important to note that all the runs have been performed in the same machine. </p>
"
"0.122299897617557","0.125279955557839","164541","<p>I am attempting to do a logistic regression bootstrap with R. The problem is I get high SE's. I'm not sure what to do about this or what it means. Does it mean that bootstrap does not work well for my particular data? Here is my code:</p>

<pre><code>get.coeffic = function(data, indices){
  data    = data[indices,]
  mylogit = glm(F~B+D, data=data, family=""binomial"")
  return(mylogit$coefficients)
}

Call:
boot(data = Pres, statistic = logit.bootstrap, R = 1000)

Bootstrap Statistics :
       original      bias    std. error
t1* -10.8609610 -23.0604501  338.048398
t2*   0.2078474   0.4351766    6.387781
</code></pre>

<p>I also want to know that after bootstrapping, how would this help with my final regression model? That is, how do I find what regression coefficient do I use in my final model?</p>

<pre><code>&gt; fit &lt;- glm(F ~ B + D , data = President, family = ""binomial"")
&gt; summary(fit)
Call:
glm(formula = F ~ B + D, family = ""binomial"", data = President)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7699  -0.5073   0.1791   0.8147   1.2836  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -14.57829    8.98809  -1.622   0.1048  
B             0.15034    0.14433   1.042   0.2976  
D             0.13385    0.08052   1.662   0.0965 .
- --
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 23.508  on 16  degrees of freedom
Residual deviance: 14.893  on 14  degrees of freedom
AIC: 20.893

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.139373668334515","0.1570467354963","166779","<p>Iâ€™ve seen some papers that present the idea of training classifiers such as logistic regression that are really meant to optimize a custom cost model (such as by maximizing profit given expect revenues for predictions depending on whether they are false positives, true negatives, true positives, or true negatives) not by optimizing the typical log-loss function and then looking for the optimal decision cut-off threshold, but by using different loss functions that weight differently the costs of each classification type or of each misclassification type (although I've seen that different authors propose different functions), and these seem to provide better results when evaluating them based on the customly-defined cost function.</p>

<p>I was wondering if there are any implementations of such methods in R. Particularly, I'd like to try fitting a logistic regression treating the cost of misclassifying as false positive to be a multiple of the cost of misclassifying as false negative. I found a package that does just this for decision trees (although in that case it's based on the class proportions on the leaves rather than something like log-loss) and I see that there are some options for observation-specific weights in logistic regression, but not for error type weights.</p>
"
"0.185220794297174","0.178573208048647","168725","<p>This question relates to whether it is a good starting point for a cut point in binary classification with logistic regression to the use the mean of the binary response variable as the initial cut point rather than simply 0.5.</p>

<p>Traditionally when people use logistic regression, people with use 0.5 as the threshold to determine when the model predicts YES/positive versus NO/negative.</p>

<p>People may run into trouble when the model only predicts one ""answer"" when using an imbalanced training set.</p>

<p>One way of dealing with this is to balance the training set via oversampling or under-sampling and keeping the test holdout set with the original balance.</p>

<p>However, I suspect that a good starting point for a cut point appears to be the mean of the binary response variable.  Is this usually true?</p>

<p>I created two models, one on a balanced training set and another on the original imbalanced training set.
<code>print(table(actual=test$y, predicted=test$fit&gt;0.5))</code></p>

<pre><code>       predicted
 actual FALSE TRUE
      0  2359  500
      1    11  130
</code></pre>

<p>With the imbalanced training, I used the mean of the binary response variable:</p>

<pre><code>print(table(actual=test$y, predicted=test$fit&gt;0.0496))

       predicted
 actual FALSE TRUE
      0  2317  542
      1     7  134
</code></pre>

<p>If one just uses 0.5, it looks like the model is a complete failure:</p>

<pre><code>`print(table(actual=test$y, predicted=test$fit&gt;0.5))`

       predicted
 actual FALSE
      0  2848
      1   152
</code></pre>

<p>They both had a KS of 0.76, so it seems like sound advice.</p>

<p>Example R code:</p>

<pre><code>require(ROCR)
require(lattice)
#
x=1:10000/10000;
y=ifelse(runif(10000)-0.7&gt;jitter(x),1,0)
#y=ifelse(rnorm(10000)-0.99&gt;x,1,0)
mean(y)

s=sample(length(x),length(x)*0.7);

df=data.frame(x=x,y=y)


##undersample
train=df[s,]
train=rbind(train[train$y==1,],train[sample(which(train$y==0),sum(train$y==1)),])
    ##oversample
    train=df[s,]
    train=rbind(train[train$y==0,],train[sample(which(train$y==1),sum(train$y==0),replace = T),])
mean(train$y) #now balanced
    threshold=0.5
    test=df[-s,] #unbalanced
    mean(test$y)
#

ex=glm(y~x,train, family = ""binomial"")
summary(ex)
nrow(test)
test$fit=predict(ex,newdata = test,type=""response"")
    message(""threshold="",threshold)
    print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

#+results
pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 

#+ imbalanced approach
#############imbalance approach

train=df[s,]
threshold=mean(y)
message(""threshold="",threshold)
ex=glm(y~x,train, family = ""binomial"")
summary(ex)
test$fit=predict(ex,test,type = ""response"")
    summary(test$fit)
print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

print(table(actual=test$y, predicted=test$fit&gt;0.5)) 

pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 
</code></pre>

<p>I noticed a similar question asked <a href=""http://stats.stackexchange.com/questions/91305/how-to-choose-the-cutoff-probability-for-a-rare-event-logistic-regression"">How to choose the cutoff probability for a rare event Logistic Regression</a></p>

<p>I like the answer given here which states to maximize the specificity or sensitivity:
<a href=""http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit/25398#25398"">Obtaining predicted values (Y=1 or 0) from a logistic regression model fit</a></p>

<p>But I also suspect that the usual starting cut off of 0.5 is bad advice.</p>

<p>Comments?</p>
"
"0.166666666666667","0.170727801083421","171325","<p>I am trying to fit a logistic regression model in R to classify a y variable as either 0 or 1. I have a dataset of around 2000 observations and decided to split it in half (training and testing).</p>

<p>After having decided which variables to include in my model, I subset the data and fitted the logistic regression as follows:</p>

<pre><code>clf &lt;- glm(y~.,data=df,family='binomial')
summary(clf)
</code></pre>

<p>Then, I tested the classifier on the testing set (1000 observations) and got 0.75 accuracy score.</p>

<pre><code>results &lt;- ifelse(predict(model,testdf,type='response') &gt; 0.5,1,0)
error &lt;- mean(r_results != results)
print(1-error) #prints out 0.74984
</code></pre>

<p>After this step, I decided to crossvalidate using the boot package</p>

<pre><code>library(boot)

# K-fold CV
error_cv = NULL

# Cost function for binary variable (as suggested by the R documentation)
cost &lt;- function(r, pi = 0) mean(abs(r-pi) &gt; 0.5)


for(i in 1:10)
{
    error_cv[i] &lt;- cv.glm(df,clf,cost,K=10)$delta[1]
}

error_cv
</code></pre>

<p>now, here is where I encounter a problem:</p>

<p>K-fold cross validation as I understand it, does the following (quote from Wikipedia):
""In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data.""</p>

<p>However, how come that cv.glm() gets as argument my already fitted model? I don't understand what it is doing. Furthermore, if the data argument is equal to the training set, I get error rates of arount 0.2 whereas if I set data=testdf I get error rates of around 0.4. Since the two sets, df and testdf, have been splitted randomly, I cannot explain this large difference and I cannot explain why cv.glm() does not (apparently) do the fit and test process it is supposed to do.</p>

<p>What am I missing?</p>
"
"0.0800640769025436","0.0820149827720712","171879","<p>I have the R output for the logistic regression model. It seems that only the intercept and psa are statistically significant. Does that mean I should remove sorbets_psa and cinko from my model and create a new model as new.model = glm(status ~ psa,family = binomial(link =""probit""))</p>

<pre><code>Call:
glm(formula = status ~ psa + serbest_psa + cinko, family = binomial(link =""probit""), data = data)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.3285  -0.6773  -0.6261  -0.5604   1.9500  

Coefficients:
      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.9697009  0.2409856  -4.024 5.72e-05 ***
psa          0.0444376  0.0094368   4.709 2.49e-06 ***
serbest_psa -0.0440718  0.0250486  -1.759   0.0785 .  
cinko       -0.0006923  0.0016984  -0.408   0.6835    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 534.27  on 477  degrees of freedom
Residual deviance: 477.07  on 474  degrees of freedom
AIC: 485.07

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.0653720450460613","0.0669649530182425","172889","<p>Recently I'm using R survival package to try to predict the probability of people going to churn. I found some <a href=""http://stackoverflow.com/questions/27408862/how-to-predict-survival-probabilities-in-r"">examples</a> on stack overflow and also tried that to my own data. Here is my prediction model and output:</p>

<pre><code>&gt; Status_by_Time &lt;- Surv(time = Duration, event = Gift_Status_ind)
&gt; model.fit2 &lt;- survreg(Status_by_Time ~ Age
                + Gender_ind 
                + Fundraiser_ind
                + Monthly.Recurring.Amount
                + Frequency_ind
                + Monthly.first.gift.amount
                + Monthly.last.gift.amount
                #+ Duration
                #+ Saved.
                + Upgrade.first.time
                + Upgrade.second.time,
                dist = ""logistic""
)

&gt; summary(model.fit2)
                            Value Std. Error      z         p
(Intercept)                81.525    1.46725  55.56  0.00e+00
Age                         0.156    0.01889   8.27  1.33e-16
Gender_ind2                 2.278    0.55955   4.07  4.68e-05
Gender_ind3                -9.514    1.09689  -8.67  4.18e-18
Fundraiser_ind2            -8.798    0.69303 -12.70  6.25e-37
Fundraiser_ind3             4.028    0.90970   4.43  9.52e-06
Monthly.Recurring.Amount   -1.211    0.04856 -24.95 2.39e-137
Frequency_ind2            257.319    0.00000    Inf  0.00e+00
Frequency_ind3              8.562    2.71423   3.15  1.61e-03
Frequency_ind4            -89.067    1.39379 -63.90  0.00e+00
Monthly.first.gift.amount  -2.538    0.03721 -68.22  0.00e+00
Monthly.last.gift.amount    1.827    0.04981  36.67 2.38e-294
Upgrade.first.time          6.467    0.82381   7.85  4.15e-15
Upgrade.second.time        10.849    2.72927   3.98  7.04e-05
Log(scale)                  2.869    0.00841 341.02  0.00e+00

Scale= 17.6 

Logistic distribution
Loglik(model)= -51841.8   Loglik(intercept only)= -55404
Chisq= 7124.45 on 13 degrees of freedom, p= 0 
Number of Newton-Raphson Iterations: 8 
n= 18097 

&gt; predicted.values &lt;- predict(model.fit2, newdata = churn.df.trim, type = ""quantile"", p = (1:9)/10) # 10 times event???
&gt; head(predicted.values)
            [,1]      [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]     [,9]
 [1,]   2.219425 16.513993 26.01508 33.80343 40.95072 48.09800 55.88635 65.38744 79.68201
 [2,]  11.088257 25.382825 34.88392 42.67227 49.81955 56.96683 64.75518 74.25627 88.55084
 [3,] -11.996590  2.297977 11.79907 19.58742 26.73470 33.88198 41.67033 51.17143 65.46599
 [4,]   5.456971 19.751539 29.25263 37.04098 44.18826 51.33555 59.12390 68.62499 82.91955
 [5,]  19.690749 33.985316 43.48641 51.27476 58.42204 65.56932 73.35767 82.85876 97.15333
 [6,]  -8.187469  6.107099 15.60819 23.39654 30.54382 37.69111 45.47946 54.98055 69.27511
</code></pre>

<p>I reckon all these numbers are not probabilities. Is there some way to interpret these numbers or turn them into probabilities? Also if I use <code>p = (1:9)/10</code> does that mean I'm calculating the probability for the next 9 or 10 period?</p>

<p>Much appreciate if someone could give me a straight forward explanation (none academic one). </p>
"
"0.160604018609905","0.16451742565458","172943","<p>I'm trying to understand the output of <code>glm</code> when a categorical variable has more than 2 categories.</p>

<p>I'm analysing if age affects death. Age is a categorical variable with 4 categories</p>

<p>I use the following code in R:</p>

<pre><code>mydata &lt;- read.delim(""Data.txt"", header = TRUE)
mydata$Agecod &lt;- factor(mydata$Agecod)
mylogit &lt;- glm(Death ~ Agecod, data = mydata, family = ""binomial"")
summary(mylogit)
</code></pre>

<p>Obtaining the following output: </p>

<pre><code>Call:
glm(formula = Death ~ Agecod, family = ""binomial"", data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4006  -0.8047  -0.8047   1.2435   2.0963  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.5108     0.7303   0.699   0.4843  
Agecod2      -0.6650     0.7715  -0.862   0.3887  
Agecod3      -1.4722     0.7658  -1.922   0.0546 .
Agecod4      -2.5903     1.0468  -2.474   0.0133 *

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 237.32  on 184  degrees of freedom
Residual deviance: 223.73  on 181  degrees of freedom
  (1 observation deleted due to missingness)
AIC: 231.73

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Since I have p-values for <code>Agecod2</code>, <code>Agecod3</code> and <code>Agecod4</code> and only <code>Agecod4</code> has a significant p-value my questions are:</p>

<ol>
<li>Is really <code>Age</code> associated with death?</li>
<li>Is only the 4th age category associated with death?</li>
<li>What happens with the first category since I don't have its p-value?</li>
</ol>

<p>Update:</p>

<p>Since Antoni Parellada says â€œIt seems as though you have proven that old age is a good predictor of deathâ€ and Gung points â€œYou cannot tell from your output if Age is associated with deathâ€ Iâ€™m still confused.</p>

<p>I understand that â€œInterceptâ€ is representing Agecod1 and is the â€œreference levelâ€. According to Gung â€œThe Estimates for the rest are the differences between the indicated level and the reference level. The associated p-values are for the tests of the indicated level vs. the reference level in isolation.â€ </p>

<p>My question now is: </p>

<p>Since Agecod4 p-value (0.0133) is significantly different from Agecod1 (reference lelvel) it doesnâ€™t mean that age is associated with death?</p>

<p>I have also tried to perform a nested test with the following command:</p>

<pre><code>anova(mylogit, test=""LRT"")
</code></pre>

<p>Obtaining:</p>

<pre><code>       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   
NULL                     184     237.32            
Agecod  3   13.583       181     223.73 0.003531 *
</code></pre>

<p>Does it mean that Age is definitively associated with death?</p>

<p>Update2:</p>

<p>I have solved my problem using binary logistic regression in SPSS. The output is the same than â€œmylogitâ€ but with SPSS I obtain a global p-value for the overall variable Agecod which is 0.008.</p>

<p>I donâ€™t know if is possible to obtain this â€œglobal p-valueâ€ with R, but since I know that I can use SPSS is not a big problem for me.</p>
"
"0.0653720450460613","0.0669649530182425","172958","<p>I am working on a school enrollment admission project to see how high school students react to scholarship in admission. The purpose is to redesign the scholarship level.</p>

<p>The original policy is 3 levels(0,2000,4000,6000) and used as training data. 
The other attributes are like GPA, ACT/SAT, gender,etc.. Y={enrolled, not enrolled}</p>

<p>What I did is manually expand the levels to (0,1000,2000,...,6000) for this year as testing data. And I used logistic regression and regression tree(LOTUS). </p>

<p>Ideally the probability will increase as the scholarship increases and it will give a sigmoid or S-curve, but not all the plots shown this. I think the reason is there are no data in the training set has the new levels.</p>

<p>I tried conjoint analysis but I don't know what does it mean.</p>

<p>what methods should I use or do I miss something here? </p>
"
"0.166666666666667","0.170727801083421","175767","<p>Logistic regression models the relationship between a set of independent variables and the probability that a case is a member of one of the categories of the dependent variable. If the probability is greater than 0.5, the case is classified in the modeled category.  If the probability is less than 0.50, the case is classified in the other category. The problem is that when I run the model with my dataset, the probabilities are far from 0.5, in fact it never gets to that value.</p>

<p>Here is part of My dataset:</p>

<pre><code>  sum_profit   direction   profit_cl1
   10           up          0.00
   0            Not_up     -0.03
  -5            Not_up      0.04
  -5            Not_up     -0.04
</code></pre>

<p>I want to find a relationship between the price of oil and the stock price of a Colombian oil company. So the variable 'sum_profit' is the sum of the change in the stock price in the next ten minutes. The variable 'profit_cl1' shows me the net change in the oil price in the last 10 minutes. </p>

<p>So what I want to know is that if the oil price changes in the last 10 minutes how would I expect the stock price direction to be in the following 10 minutes (Up or Down).</p>

<p>The problem is that my probabilities once I run the logistic regression are far from 0.5 even though the model is significant </p>

<pre><code>    glm.fit=glm(formula = direction ~ profit_cl1, family = binomial, data = datos)

    Deviance Residuals: 
      Min       1Q   Median       3Q      Max  
    -0.6786  -0.6786  -0.6131  -0.6131   1.8783  

    Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)     -1.57612    0.01618 -97.394   &lt;2e-16 ***
    profit_cl1       0.22485    0.02288   9.829   &lt;2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 48530  on 50309  degrees of freedom
    Residual deviance: 48434  on 50308  degrees of freedom
    AIC: 48438

    Number of Fisher Scoring iterations: 4 
</code></pre>

<p>The code to get the probabilities:</p>

<pre><code>   log.probs=predict(glm.fit, type=""response"")
   mean(log.probs)=0.1873 
</code></pre>

<p>the 0.1873 is very far from 0.5.</p>

<p>Sorry but I did not know where else to look for help! I appreciate any suggestion!</p>
"
"0.18490006540841","0.177567646428892","176586","<p>This question stems from <a href=""http://stats.stackexchange.com/questions/175853/what-type-of-hypothesis-test-for-multivariate-testing-website"">another I asked last week</a>, where the person answering stated </p>

<blockquote>
  <p>""Finally, and this is very, very important: please don't just run the
  code I've provided, and consider your job complete. If you don't
  actually read up and understand some of how these analyses work, all
  of this information will be less than useless.""</p>
</blockquote>

<p>This is my intention, to really understand what is going on as well as how to interpret.</p>

<p>Context is website testing. Show people a different landing page, change the design and look of each page with a goal of getting more people to purchase online (""success"").</p>

<p>Here is my data:</p>

<pre><code>variant successes   failures
Original    757 49114
Date    553 41794
Cranberry   494 41495
Apple   546 41835
</code></pre>

<p>My script and output are below. I think I understand how to interpret it but just wanted to make sure. My questions:</p>

<ol>
<li>The first thing I want to do is check if there is a difference between the variance overall, or if it's just ebbs n flows. With a p-value of 8.55e-05 translates to 0.0000855 (right?) then yes, there is a meaningful variance between the groups. Is that a correct statement?</li>
<li>Since I'm comparing each group to the original (It's really a case of ""which test can beat the original), then it looks like only first Vs. 4th (Original Vs. Apple) is the only real difference statistically because the p-value is 0.0098. Is this a correct statement?</li>
<li>In my contrast function I have assumed data are read int he order they appear in test2. Is this correct?</li>
<li>Reading more about logistic regression it seems to be used to measure the impact of incrementing a predictor up or down a unit (resulting in the log unit increase or decrease). But in the context of measuring a web page variant performance in this way, why is logistic regression an appropriate method of determining whether or not the variants are different? Put another way, I'm hypothesis testing rather than predicting the impact of each variant, since an observation can only be one variant, not a combination of 1 or more predictors (they can only ever see one of the test pages, not 2 or more test pages).</li>
<li>I edited my data to include only visits from one state, just to experiment and play around. The output I got in this instance was a p-value of 0.001721 in the anova of m whereas the p-values for contrast where between 0.2 -0.3 (reject). If the script says overall there is a variance but at an individual test level there is not, how would I interpret that? I can provide the output if desired.</li>
</ol>

<p>Here is my script &amp; output:</p>

<pre><code>&gt; test2 &lt;- read.csv(""test2.csv"")
&gt; 
&gt; m &lt;- glm(cbind(successes, failures) ~ variant, family=binomial, data=test2)
&gt; anova(m, test='Chisq') # Tests if there's a difference between the variants
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(successes, failures)

Terms added sequentially (first to last)


        Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
NULL                        3     21.435             
variant  3   21.435         0      0.000 8.55e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; 
&gt; library(lsmeans)
&gt; #lsmeans(m, pairwise ~ variant) # Compares every variant to every other one
&gt; 
&gt; m.comparisons = lsmeans(m, specs = pairwise ~ variant)
&gt; contrast(m.comparisons,
+          list(
+            first.vs.second = c(1,-1,0,0),
+            first.vs.third =  c(1,0,-1,0),
+            first.vs.fourth = c(1,0,0,-1)
+            ), adjust=""tukey"")
 contrast           estimate         SE df    z.ratio p.value
 first.vs.second  0.09192309 0.06248035 NA  1.4712319  0.3667
 first.vs.third  -0.01371955 0.06072602 NA -0.2259254  0.9943
 first.vs.fourth -0.16633346 0.05653998 NA -2.9418735  0.0098

P value adjustment: sidak method for 3 tests 
</code></pre>
"
"0.0653720450460613","0.0669649530182425","177288","<p>First query, so apologize in advance for any stupidity or ""unawareness"".  I have a large sample, at roughly 88000 obs.  But, my events for this sample (the 1's) are about .00072% of the sample.  </p>

<p>Pretty sure that my sample suffers from rare event bias.  Therefore, I am using the logistf function to run a logistic model.  But not sure that this is the best method.  I've read the standard King and Zeng paper.  But I am just getting some unusual results.  Meaning, that variables that I thought would be significant, are just not coming out that way.  In addition, the df  for the lrtest and extractAIC are really small, between 5 to 7 for any model that I have run.  </p>

<p>Sorry, I can't provide screen shots or results.  Work data, so not sure that I can share. </p>
"
"0.146176336551172","0.119790549647096","177805","<p>R and statistics beginner here, trying to do a quantile regression on a non-linear dataset. </p>

<p>I want to identify datapoints that have a higher y axis value that expected given their value on the x axis. 
I should highlight that the y-data are means of discrete values (0.1-1, in steps of 0.1) taken in dependence on the x-data. x values are number of SNPs in a gene. Each SNP has a discrete value and the y value is a mean of these SNP values for each gene.</p>

<p>After initially investigating  funnel plots it seems that a quantile regression might be most appropriate for this dataset, though thoughts on this are welcome.  I'd appreciate any guidance in fitting a quantile regression to identify that don't fall within 95 percent of the data.</p>

<p>Sample of data (I actually have ~20,000 datapoints):</p>

<pre><code>GENE    mean  total
X1  0.1 3
X2  0.1466666667    30
X3  0.1375  8
X4  0.24    5
X5  0.2625  8
X6  0.2 1
X7  0.1466666667    15
X8  0.2 1
X9  0.1666666667    9
X10 0.1 1
X11 0.1928571429    14
X12 0.1 2
X13 0.1545454545    11
X14 0.1333333333    3
X15 0.1666666667    3
X16 0.2117647059    34
X17 0.1452380952    42
X18 0.16    5
X19 0.2 1
X20 0.25    2
X21 0.125   4
X22 0.2 13
X23 0.1714285714    7
X24 0.15    6
X25 0.2 3
X26 0.2894736842    19
X27 0.2352941176    17
X28 0.1333333333    6
X29 0.12    5
X30 0.2 3
X31 0.1 1
X32 0.1571428571    7
X33 0.2125  8
X34 0.18125 16
X35 0.26    10
X36 0.1368421053    19
X37 0.1333333333    6
X38 0.15    2
X39 0.14    5
X40 0.18    15
X41 0.14    5
X42 0.3 1
X43 0.1 2
X44 0.1 6
X45 0.1 4
X46 0.1 1
X47 0.1333333333    3
X48 0.1166666667    6
X49 0.225   4
X50 0.2 15
X51 0.125   12
X52 0.1 3
X53 0.1714285714    14
X54 0.175   4
X55 0.3404761905    42
X56 0.1 1
X57 0.25    2
X58 0.15    4
X59 0.1 1
X60 0.1666666667    3
X61 0.3 2
X62 0.225   4
X63 0.3076923077    13
X64 0.1 1
X65 0.1666666667    3
X66 0.1666666667    6
X67 0.1 3
X68 0.1 3
X69 0.1166666667    6
X70 0.125   8
X71 0.2 1
X72 0.2 2
X73 0.1333333333    42
X74 0.1 1
X75 0.2 8
X76 0.1444444444    9
X77 0.1666666667    15
X78 0.1 2
X79 0.176744186 43
X80 0.1275  40
X81 0.1666666667    3
X82 0.125   4
X83 0.2545454545    11
X84 0.1304347826    46
X85 0.21    10
X86 0.1571428571    7
X87 0.3 9
X88 0.275   16
X89 0.11    10
X90 0.1333333333    6
X91 0.2333333333    3
X92 0.2 2
X93 0.2866666667    15
X94 0.25    2
X95 0.1125  8
X96 0.4 11
X97 0.1 1
X98 0.2 2
X99 0.15    2
X100    0.1625  8
X101    0.24    5
X102    0.175   4
X103    0.15    4
X104    0.1333333333    3
X105    0.4 2
X106    0.2 3
X107    0.25    2
X108    0.32    5
X109    0.2333333333    3
X110    0.1714285714    7
X111    0.2 1
X112    0.225   4
X113    0.2 1
X114    0.1714285714    7
X115    0.15    2
X116    0.1166666667    6
X117    0.16875 16
X118    0.1555555556    9
X119    0.15    6
X120    0.12    5
X121    0.1 1
X122    0.1333333333    6
X123    0.2333333333    3
X124    0.1 1
X125    0.2333333333    3
X126    0.1333333333    3
X127    0.1 1
X128    0.1827586207    29
X129    0.25    8
X130    0.2 7
X131    0.25    6
X132    0.1 1
X133    0.125   4
X134    0.2 1
X135    0.1666666667    3
X136    0.1 3
X137    0.12    5
X138    0.1 1
X139    0.175   4
X140    0.1 1
X141    0.1666666667    3
X142    0.1666666667    3
X143    0.1 1
X144    0.1375  8
X145    0.1 9
X146    0.1 2
X147    0.125   4
X148    0.1333333333    3
X149    0.1769230769    13
X150    0.15    2
X151    0.1214285714    14
X152    0.1 1
X153    0.2555555556    18
X154    0.2 1
X155    0.1 1
X156    0.1 1
X157    0.1 1
X158    0.4 1
X159    0.14    5
X160    0.1 2
X161    0.1333333333    3
X162    0.375   8
X163    0.2263157895    19
X164    0.1636363636    11
X165    0.3 1
X166    0.1 3
X167    0.2 1
X168    0.3 1
X169    0.1428571429    7
X170    0.1 2
X171    0.1222222222    9
X172    0.1 8
X173    0.1 5
X174    0.1 8
X175    0.1666666667    3
X176    0.2 5
X177    0.1 4
X178    0.1166666667    6
X179    0.15    2
X180    0.3666666667    3
X181    0.25    4
X182    0.1 1
X183    0.1 2
X184    0.1 1
X185    0.1 1
X186    0.1 1
X187    0.184   25
X188    0.2333333333    3
X189    0.2333333333    3
X190    0.1 2
X191    0.32    5
X192    0.1 2
X193    0.12    5
X194    0.1 5
X195    0.2 1
X196    0.1 6
X197    0.1 2
X198    0.4 1
X199    0.2 2
X200    0.1 2
X201    0.2 1
X202    0.2333333333    6
X203    0.35    2
X204    0.1 1
X205    0.12    5
X206    0.14    5
X207    0.125   4
X208    0.3333333333    3
X209    0.1 2
X210    0.1 3
X211    0.1 1
X212    0.2 4
X213    0.15    8
X214    0.125   4
X215    0.1548387097    31
X216    0.2 7
X217    0.225   4
X218    0.125   4
X219    0.15    2
X220    0.4 1
X221    0.275   4
X222    0.325   4
X223    0.2 3
X224    0.175   4
X225    0.3 1
X226    0.1 1
X227    0.19    10
X228    0.25    4
X229    0.2666666667    9
X230    0.1 1
X231    0.2 1
X232    0.3 1
X233    0.2166666667    6
X234    0.26    5
X235    0.225   4
X236    0.1 1
X237    0.1857142857    7
X238    0.58    5
X239    0.25    10
X240    0.6066666667    15
X241    0.3 1
X242    0.5 2
X243    0.2333333333    3
X244    0.25    2
X245    0.1 4
X246    0.1 1
X247    0.1714285714    7
X248    0.16875 16
X249    0.2 1
X250    0.4 3
X251    0.1 1
X252    0.1666666667    6
X253    0.2 6
X254    0.3166666667    12
X255    0.1 1
X256    0.1 2
X257    0.4 1
X258    0.1333333333    3
X259    0.225   4
X260    0.2571428571    7
X261    0.4 5
X262    0.15    10
X263    0.1571428571    7
X264    0.2 11
X265    0.2285714286    7
X266    0.15    4
X267    0.3 1
X268    0.1384615385    13
X269    0.1 4
X270    0.1 1
X271    0.16    5
X272    0.1285714286    7
X273    0.1 1
X274    0.2222222222    9
X275    0.2083333333    12
X276    0.2153846154    13
X277    0.1888888889    9
X278    0.1 1
X279    0.1 2
X280    0.3 2
X281    0.17    10
X282    0.1 5
X283    0.2833333333    6
X284    0.1333333333    6
X285    0.1833333333    6
X286    0.1833333333    12
X287    0.1953488372    43
X288    0.2526315789    19
X289    0.1 1
X290    0.125   4
X291    0.26    5
X292    0.1 2
X293    0.2578947368    19
X294    0.2545454545    11
X295    0.1 1
X296    0.3666666667    3
X297    0.1714285714    7
X298    0.1833333333    6
X299    0.16    5
X300    0.2733333333    15
X301    0.275   4
X302    0.1 1
X303    0.2 7
X304    0.1583333333    12
X305    0.1666666667    3
X306    0.1 1
X307    0.1 6
X308    0.1642857143    14
X309    0.1 1
X310    0.1606060606    33
X311    0.1428571429    7
X312    0.1888888889    9
X313    0.2 2
X314    0.1388888889    18
X315    0.35    2
X316    0.3 2
X317    0.1 4
X318    0.15    16
X319    0.1166666667    12
X320    0.1888888889    9
X321    0.16    5
X322    0.2333333333    3
X323    0.1857142857    14
X324    0.31    20
X325    0.2 1
X326    0.1 1
X327    0.1952380952    21
X328    0.215625    32
X329    0.1 1
X330    0.1 1
X331    0.1307692308    13
X332    0.1 4
X333    0.1666666667    3
X334    0.2 14
X335    0.1583333333    12
X336    0.1961538462    26
X337    0.2222222222    9
X338    0.1 3
X339    0.1 2
X340    0.1285714286    14
X341    0.175   4
X342    0.125   4
X343    0.1 4
X344    0.1428571429    7
X345    0.1 4
X346    0.1 2
X347    0.15    2
X348    0.25    4
X349    0.22    5
X350    0.1 2
X351    0.1 3
X352    0.14    10
X353    0.1666666667    18
X354    0.1333333333    3
X355    0.2 3
X356    0.16    5
X357    0.3 1
X358    0.175   4
X359    0.5 1
X360    0.1111111111    9
X361    0.2333333333    6
X362    0.175   4
X363    0.227027027 37
X364    0.3857142857    7
X365    0.1 2
X366    0.2 3
X367    0.1916666667    12
X368    0.1428571429    14
X369    0.2666666667    3
X370    0.2 9
X371    0.25    2
X372    0.2 1
X373    0.1 2
X374    0.225   4
X375    0.1 1
X376    0.1 3
X377    0.3 2
X378    0.1 1
X379    0.1545454545    11
X380    0.1730769231    52
X381    0.1 3
X382    0.1333333333    3
X383    0.1814814815    27
X384    0.108   25
X385    0.2666666667    6
X386    0.1666666667    3
X387    0.25    8
X388    0.225   4
X389    0.24    25
X390    0.2666666667    6
X391    0.1 2
X392    0.15    4
X393    0.1666666667    6
X394    0.1 1
X395    0.2375  8
X396    0.125   4
X397    0.1 7
X398    0.1 7
X399    0.1 4
X400    0.1 2
X401    0.1625  8
X402    0.3 1
X403    0.3 2
X404    0.25    4
X405    0.2 1
X406    0.1285714286    7
X407    0.15    8
X408    0.5 1
X409    0.1 1
X410    0.1285714286    7
X411    0.1 1
X412    0.2166666667    30
X413    0.22    5
X414    0.2714285714    14
X415    0.1214285714    14
X416    0.2 8
X417    0.28    5
X418    0.24    35
X419    0.15    4
X420    0.1333333333    12
X421    0.125   4
X422    0.1 1
X423    0.1666666667    3
X424    0.2111111111    9
X425    0.3 4
X426    0.2 2
X427    0.2 3
X428    0.1 1
X429    0.1 1
X430    0.1617021277    47
X431    0.15    8
X432    0.1142857143    14
X433    0.15    4
X434    0.1384615385    13
X435    0.1 2
X436    0.1166666667    12
X437    0.1714285714    14
X438    0.2416666667    12
X439    0.1 1
X440    0.1428571429    7
X441    0.1 1
X442    0.1416666667    12
X443    0.3333333333    6
X444    0.2 1
X445    0.14    5
X446    0.2 3
X447    0.225   28
X448    0.1571428571    14
X449    0.1 1
X450    0.1583333333    12
X451    0.1518518519    27
X452    0.1363636364    11
X453    0.2 1
X454    0.1666666667    6
X455    0.1 1
X456    0.1333333333    3
X457    0.2368421053    19
X458    0.1222222222    9
X459    0.15    2
X460    0.2 1
X461    0.1625  24
X462    0.2 6
X463    0.1666666667    3
X464    0.1 3
X465    0.3 8
X466    0.1523809524    21
X467    0.1 3
X468    0.1 3
X469    0.15    4
X470    0.1 1
X471    0.1642857143    28
X472    0.1 5
X473    0.1 2
X474    0.12    15
X475    0.1 3
X476    0.1090909091    11
X477    0.1346153846    26
X478    0.125   4
X479    0.1444444444    9
X480    0.2 1
X481    0.1 1
X482    0.1 3
X483    0.2 3
X484    0.1375  8
X485    0.1 4
X486    0.12    5
X487    0.1739130435    23
X488    0.25    2
X489    0.1333333333    6
X490    0.3 1
X491    0.225   20
X492    0.175   4
X493    0.1 3
X494    0.1222222222    9
X495    0.1 1
X496    0.175   4
X497    0.2333333333    6
X498    0.1615384615    13
X499    0.15    8
X500    0.1666666667    6
X501    0.2 2
X502    0.1777777778    9
X503    0.15    4
X504    0.2666666667    3
X505    0.1 4
X506    0.1222222222    9
X507    0.15    2
X508    0.2 3
X509    0.1333333333    15
X510    0.14    5
X511    0.1 1
X512    0.4 1
X513    0.2125  8
X514    0.36    5
X515    0.34    5
X516    0.4 1
X517    0.1428571429    7
X518    0.3333333333    3
X519    0.1 3
X520    0.2277777778    18
X521    0.1916666667    12
X522    0.2 4
X523    0.1857142857    7
X524    0.1 2
X525    0.1 5
X526    0.2222222222    9
X527    0.1818181818    11
X528    0.2151515152    33
X529    0.1 3
X530    0.1214285714    14
X531    0.2 1
X532    0.1 2
X533    0.1 3
X534    0.1166666667    12
X535    0.1 2
X536    0.1 2
X537    0.1 1
X538    0.2379310345    29
X539    0.175   4
X540    0.1363636364    11
X541    0.1 1
X542    0.1479166667    48
X543    0.1928571429    28
X544    0.4 1
X545    0.1951219512    41
X546    0.1333333333    3
X547    0.15    4
X548    0.2833333333    6
X549    0.1547619048    42
X550    0.1555555556    9
X551    0.2363636364    11
X552    0.2142857143    7
X553    0.5 1
X554    0.15    4
X555    0.1709677419    31
X556    0.17    10
X557    0.1 2
X558    0.2866666667    15
X559    0.4 2
X560    0.15    2
X561    0.1424242424    66
X562    0.25    2
X563    0.1 3
X564    0.1285714286    7
X565    0.12    5
X566    0.25    4
X567    0.2263157895    19
X568    0.1 12
X569    0.1666666667    6
X570    0.5 1
X571    0.147826087 23
X572    0.1 1
X573    0.1818181818    11
X574    0.2 2
X575    0.15    2
X576    0.2 3
X577    0.16    15
X578    0.1621621622    37
X579    0.1333333333    3
X580    0.1333333333    12
X581    0.18    5
X582    0.1534482759    58
X583    0.1538461538    26
X584    0.1 9
X585    0.2142857143    7
X586    0.1 1
X587    0.1222222222    9
X588    0.1 1
X589    0.1 3
X590    0.1 6
X591    0.15    2
X592    0.1 2
X593    0.3 1
X594    0.1285714286    21
X595    0.2 2
X596    0.12    5
X597    0.1 1
X598    0.1 1
X599    0.1 2
X600    0.1153846154    13
X601    0.1 15
X602    0.1 1
X603    0.1 1
X604    0.1 4
X605    0.15    10
X606    0.15    4
X607    0.15    4
X608    0.2 1
X609    0.14    5
X610    0.2 1
X611    0.1 2
X612    0.1 3
X613    0.125   4
X614    0.172   25
X615    0.2 4
X616    0.1727272727    11
X617    0.2090909091    22
X618    0.1333333333    3
X619    0.1 7
X620    0.15    4
X621    0.1181818182    11
X622    0.1375  8
X623    0.1666666667    3
X624    0.1 3
X625    0.1090909091    11
X626    0.125   8
X627    0.1 2
X628    0.12    5
X629    0.1 8
X630    0.13    40
X631    0.1666666667    3
X632    0.34    5
X633    0.1714285714    7
X634    0.1636363636    11
X635    0.1 1
X636    0.1 1
X637    0.18125 16
X638    0.2 4
X639    0.2 8
X640    0.1 2
X641    0.1 1
X642    0.1166666667    6
X643    0.2 1
X644    0.6 1
X645    0.2666666667    9
X646    0.2666666667    3
X647    0.2 2
X648    0.1 2
X649    0.1 1
X650    0.1 2
X651    0.1 1
X652    0.125   4
X653    0.15    2
X654    0.1 1
X655    0.1 1
X656    0.35    4
X657    0.2666666667    3
X658    0.1 2
X659    0.1 1
X660    0.2 1
X661    0.1 2
X662    0.1 2
X663    0.1333333333    3
X664    0.1 2
X665    0.1 1
X666    0.225   4
X667    0.1666666667    6
X668    0.1 2
X669    0.1 3
X670    0.175   4
X671    0.1 3
X672    0.15    4
X673    0.1666666667    3
X674    0.1 3
X675    0.175   4
X676    0.25    8
X677    0.25    4
X678    0.2571428571    7
X679    0.1 1
X680    0.2571428571    7
X681    0.208   25
X682    0.325   12
X683    0.1 1
X684    0.25    2
X685    0.1 2
X686    0.3047619048    21
X687    0.24    5
X688    0.15    6
X689    0.1333333333    6
X690    0.3 1
X691    0.1 1
X692    0.15    2
X693    0.23    20
X694    0.2 2
X695    0.1666666667    6
X696    0.1342857143    35
X697    0.25    6
X698    0.2 8
X699    0.2 5
X700    0.5 1
X701    0.1333333333    6
X702    0.3 1
X703    0.15    2
X704    0.15    2
X705    0.1833333333    6
X706    0.15    6
X707    0.1493506494    77
X708    0.36    5
X709    0.3 2
X710    0.15    2
X711    0.38    5
X712    0.2666666667    3
X713    0.25    4
X714    0.225   4
X715    0.5 1
X716    0.1 2
X717    0.16    5
X718    0.3 2
X719    0.3538461538    13
X720    0.1 2
X721    0.175   4
X722    0.22    5
X723    0.175   4
X724    0.2333333333    6
X725    0.34    5
X726    0.2 7
X727    0.1 1
X728    0.3 3
X729    0.1 1
X730    0.1 3
X731    0.3 5
X732    0.35    6
X733    0.2875  8
X734    0.1 1
X735    0.1 2
X736    0.2 5
X737    0.1714285714    7
X738    0.375   4
X739    0.1 4
X740    0.3 1
X741    0.1 1
X742    0.1142857143    7
X743    0.1 1
X744    0.2285714286    7
X745    0.14    5
X746    0.15    6
X747    0.1 1
X748    0.125   4
X749    0.1666666667    6
X750    0.125   8
X751    0.1 1
X752    0.15    2
X753    0.2 1
X754    0.225   4
X755    0.3 1
X756    0.3 5
X757    0.175   4
X758    0.1 3
X759    0.1333333333    18
X760    0.1230769231    13
X761    0.2 1
X762    0.11    10
X763    0.1666666667    6
X764    0.1 1
X765    0.2090909091    11
X766    0.145   20
X767    0.14    5
X768    0.2375  8
X769    0.1571428571    7
X770    0.1 1
X771    0.1 2
X772    0.2 2
X773    0.16    5
X774    0.2 1
X775    0.1777777778    9
X776    0.1210526316    19
X777    0.2 1
X778    0.225   12
X779    0.1666666667    3
X780    0.1 6
X781    0.2333333333    6
X782    0.1692307692    13
X783    0.19    10
X784    0.2 3
X785    0.1489361702    47
X786    0.2 5
X787    0.45    2
X788    0.1666666667    6
X789    0.18    5
X790    0.3 1
X791    0.2 2
X792    0.11    10
X793    0.3333333333    3
X794    0.25    2
X795    0.2 1
X796    0.25    2
X797    0.2 2
X798    0.2 1
X799    0.1 3
X800    0.1333333333    18
X801    0.1473684211    19
X802    0.2 5
X803    0.14    5
X804    0.125   4
X805    0.1583333333    12
X806    0.1857142857    7
X807    0.1 1
X808    0.2 1
X809    0.1769230769    26
X810    0.1 1
X811    0.1 2
X812    0.1833333333    6
X813    0.1409090909    22
X814    0.1416666667    24
X815    0.1307692308    13
X816    0.1235294118    17
X817    0.1 1
X818    0.1 1
X819    0.18    30
X820    0.2514285714    35
X821    0.18    5
X822    0.2 4
X823    0.1 1
X824    0.2333333333    9
X825    0.1222222222    9
X826    0.15    2
X827    0.14    5
X828    0.1588235294    51
X829    0.15    2
X830    0.2 4
X831    0.1 2
X832    0.1391304348    23
X833    0.18    20
X834    0.15    2
X835    0.3 1
X836    0.1 8
X837    0.1666666667    9
X838    0.1954545455    22
X839    0.225   16
X840    0.1222222222    9
X841    0.1210526316    19
X842    0.1 2
X843    0.1 2
X844    0.125   4
X845    0.1 4
X846    0.1 1
X847    0.2 2
X848    0.275   4
X849    0.1 3
X850    0.2833333333    6
X851    0.175   4
X852    0.32    5
X853    0.1 1
X854    0.1428571429    7
X855    0.2277777778    18
X856    0.15    8
X857    0.12    5
X858    0.1 2
X859    0.175   4
X860    0.18    5
X861    0.16    5
X862    0.2333333333    6
X863    0.1 1
X864    0.3333333333    3
X865    0.1 2
X866    0.15    12
X867    0.1636363636    11
X868    0.4 1
X869    0.4 1
X870    0.1 3
X871    0.1555555556    9
X872    0.2 1
X873    0.3 1
X874    0.2 2
X875    0.15    12
X876    0.1 1
X877    0.1181818182    11
X878    0.1428571429    7
X879    0.1461538462    13
X880    0.3076923077    13
X881    0.2 2
X882    0.3 1
X883    0.205   20
X884    0.2 5
X885    0.1333333333    3
X886    0.15    2
X887    0.25    2
X888    0.15    4
X889    0.3 1
X890    0.125   4
X891    0.1875  8
X892    0.1428571429    7
X893    0.2333333333    3
X894    0.1 2
X895    0.1 1
X896    0.35    6
X897    0.1444444444    9
X898    0.2 2
X899    0.3 1
X900    0.1 2
X901    0.1 1
X902    0.25    2
X903    0.1 1
X904    0.1 1
X905    0.7 1
X906    0.2 1
X907    0.45    4
X908    0.25    2
X909    0.15    4
X910    0.1 2
X911    0.4 13
X912    0.1 2
X913    0.1842105263    19
X914    0.1 1
X915    0.1333333333    3
X916    0.2 2
X917    0.1 7
X918    0.1 1
X919    0.225   4
X920    0.2 1
X921    0.2 3
X922    0.18    5
X923    0.1 1
X924    0.1875  8
X925    0.2833333333    6
X926    0.5 3
X927    0.2 1
X928    0.1 1
X929    0.1 2
X930    0.2 3
X931    0.4 1
X932    0.2875  16
X933    0.1857142857    7
X934    0.1 1
X935    0.2 2
X936    0.1 1
X937    0.2 13
X938    0.2444444444    9
X939    0.1 1
X940    0.1714285714    7
X941    0.3 1
X942    0.1 1
X943    0.2857142857    7
X944    0.15    2
X945    0.1 1
X946    0.15625 16
X947    0.1666666667    3
X948    0.3 1
X949    0.2 2
X950    0.1 8
X951    0.1 1
X952    0.1 3
X953    0.3 1
X954    0.3 1
X955    0.1 3
X956    0.1125  8
X957    0.18    5
X958    0.2666666667    3
X959    0.2 1
X960    0.125   4
X961    0.1333333333    3
X962    0.2444444444    9
X963    0.25    10
X964    0.25    4
X965    0.2 1
X966    0.225   4
X967    0.1625  8
X968    0.1333333333    3
X969    0.1333333333    3
X970    0.1 1
X971    0.2 7
X972    0.3 10
X973    0.1 1
X974    0.3 2
X975    0.225   4
X976    0.1 1
X977    0.1 2
X978    0.4 1
X979    0.1333333333    3
X980    0.1333333333    9
X981    0.13125 16
X982    0.1 1
X983    0.2 1
X984    0.1782608696    23
X985    0.2225806452    31
X986    0.15    4
X987    0.1 3
X988    0.1 3
X989    0.15    4
X990    0.2285714286    14
X991    0.2384615385    26
X992    0.4 1
X993    0.4 2
X994    0.1 1
X995    0.1 1
X996    0.1666666667    3
X997    0.1 6
X998    0.13    20
X999    0.2666666667    3
</code></pre>

<p>Code I am using:</p>

<pre><code>Asianpig &lt;- NULL; Asianpig$x &lt;- (Asianpig_data$total)
Asianpig$y &lt;- (Asianpig_data$mean)
plot(Asianpig)

#increase maxiterations for nls
nlc &lt;- nls.control(maxiter = 21811)

# fit first a nonlinear least-square regression
Dat.nls &lt;- nls(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, control = nlc); Dat.nls
lines(1:8000, predict(Dat.nls, newdata=list(x=1:8000)), col=1)

# and finally ""external envelopes"" holding 95 percent of the data
Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, tau=0.025, trace=TRUE)
lines(1:8000, predict(Dat.nlrq, newdata=list(x=1:8000)), col=4)

Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, tau=0.975, trace=TRUE)
lines(1:8000, predict(Dat.nlrq, newdata=list(x=1:8000)), col=4)
</code></pre>

<p>How this looks: </p>

<p><a href=""http://i.stack.imgur.com/tF8Vu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tF8Vu.png"" alt=""enter image description here""></a></p>

<p>I was expecting the quantile regression line to more dynamically follow the slope of the datapoints. 
I adapted the code from an example that was using <code>SSlogis()</code> for the input data:</p>

<pre><code># build artificial data with multiplicative error
Dat &lt;- NULL; Dat$x &lt;- rep(1:25, 20)
    set.seed(1)
    Dat$y &lt;- SSlogis(Dat$x, 10, 12, 2)*rnorm(500, 1, 0.1)
plot(Dat)
</code></pre>

<p>I have a feeling I should not be using <code>SSlogis()</code> in my code, but instead should be modelling an exponential distribution. SSlogis is a selfStart model evaluates the logistic function and its gradient. It has an initial attribute that creates initial estimates of the parameters Asym, xmid, and scale.</p>

<p>But I am still trying to understand how to fit a quantile regression for this non-linear data.</p>

<p>Here is a hexbin plot that gives a feeling for how the data is clustered:<a href=""http://i.stack.imgur.com/NCrLX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NCrLX.png"" alt=""enter image description here""></a></p>
"
"0.146784140987997","0.150360801748797","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"NaN","NaN","180337","<p>I always report odds ratios when using logistic regression for predictions. 
I wanted know is it meaningful to report odds ratios when modeling with gradient boosting approach? 
I am using gbm package in R to make the predictions.</p>

<p>Thanks!</p>
"
"0.167093470609201","0.183391076651825","180447","<p>I have some data on patients presenting to emergency departments after sustaining self-inflicted gunshot injuries, stored in a data frame (""SIGSW,"" which is ~16,000 observations of 47 variables) in R. I want to create a model that helps a physician predict, using several objective covariates, the ""pretest probability"" of the self-shooting being a suicide attempt, or a negligent discharge. The covariates are largely categorical variables, but a few are continuous or binary. My outcome, suicide attempt or not, is coded as a binary/indicator variable, ""SI,"" so I believe a binary logistic regression to be the appropriate tool.  </p>

<p>In order to construct my model, I intended to individually regress SI on each covariate, and use the p-value from the likelihood ratio test for each model to inform which covariates should be considered for the backward model selection. </p>

<p>For each model, SI~SEX, SI~AGE, etc, I receive the following error:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: algorithm did not converge
</code></pre>

<p>A little Googling revealed that I perhaps need to increase the number of iterations to allow convergence. I did this with the following:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW, control = list(maxit = 50))

Call:  glm(formula = SI ~ SEX, family = binomial, data = SIGSW, control = list(maxit = 50))

Coefficients:
(Intercept)          SEX  
 -3.157e+01   -2.249e-13  

Degrees of Freedom: 15986 Total (i.e. Null);  15985 Residual
Null Deviance:      0 
Residual Deviance: 7.1e-12  AIC: 4
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>This warning message, after a little Googling, suggests a ""perfect separation,"" which, as I understand it, means that my predictor is ""too good."" Seeing as how this happens with all of the predictors, I'm somewhat skeptical that they're all ""too good."" Am I doing something wrong? </p>

<p>Edit: In light of the answers, here is a sample of the data (I only selected a few of the variables for space concerns):</p>

<pre><code>   SIGSW.AGENYR_C SIGSW.SEX SIGSW.RACE_C SIGSW.SI
1              19      Male        White        0
2              13      Male        Other        0
3              18      Male   Not Stated        0
4              15      Male        White        0
5              23      Male        White        0
6              11      Male        Black        0
7              16      Male   Not Stated        1
8              21      Male   Not Stated        0
9              14      Male        White        0
10             41      Male        White        0
</code></pre>

<p>And here is the crosstabulation of SEX and SI, showing that SI is coded as an indicator variable, and that there are both men and women with SI, so sex is not a perfect predictor. </p>

<pre><code>  &gt;table(SIGSW$SEX, SIGSW$SI)        
              0     1
  Unknown     1     3
  Male    11729  2121
  Female   1676   457
</code></pre>

<p>Does the small cell size represent a problem?</p>
"
"0.0800640769025436","0.0820149827720712","181463","<p>I made a logistic regression in R statistics, but I don't know how to interpret it with 2 categorical variables (the examples I found on the internet and / or stackoverflow were just with one and I have difficulties to imagine it with two). </p>

<p>So imagine I want to see which factors infuence the fact of having a special desease (1: yes, 0: no) and I have:</p>

<pre><code>City: Manhattan, New York
hospital: St. Mary, Avante, Copperfield
bloodshugar: 1, 28, 7 ... , 66 (numeric)
timetoreact: 113, 423, 334, ... (numeric),
</code></pre>

<p>I give it all in a glm-model glm (desease dependent on: <code>City</code>, <code>hospital</code>, <code>City:hospital</code>, ...)</p>

<p>In the output I have the problem that it's all comprised with the factor level of the first letter of the alphabet, so i.e. ""Manhattan"" and ""Avante"" doesn't appear anymore. </p>

<p>There is just written: </p>

<pre><code>NewYork:Bloodshugar: Coeff.: 0.034 
</code></pre>

<p>and I don't know now what it is... Manhattan:Bloodshugar doesn't appear. Is it the difference of the incline from the probability on bloodshugar in Newyork in comparison to Manhattan? Where can I see if the probability to get the desease sinks or inclines with more bloodshugar in New York? When there's written bloodshugar: Coef.: 0.021, is it the bloodshugar ""mean"" of Manhattan and New York or is it just from Manhattan?  </p>

<p>What is the intercept now? Is it the probability to show the desease when cured in the Avantehospital and raised in Manhattan (because it's always the first letter)?</p>

<p>I hope I explained it well, I still can add some more explanations if you'd like to. </p>
"
"0.0462250163521024","0.0473513723810378","183845","<p>I'm going crazy, because I can't find a simple description how the coefficients are calculated in R statistics in the multivariable logistic regression (and I'm not a mathematician). 
Are they standardised? So i.e. when I have x ~ y1 + y2 and the coefficient for y1 = 0.2, is this the coefficient in the model when the parameter y2 is 0, the mean of y2 or somehow all the parameters of y2? </p>

<p>Sorry, I'm stuck on this simple question... </p>

<p>p.s.: I also have an interaction y1:y2 if this changes anything...</p>
"
"0.0653720450460613","0.0669649530182425","183846","<p>I created a SEM model in R (lavaan package), but one of myÂ dependent variables is continuous, while the other is binary.</p>

<p>The model isÂ as follows:</p>

<pre><code>modelx &lt;- '
a =~ a1Â + a2Â + a3

bÂ =~ b1Â + b2 +Â b3

c =~Â c1 + c2 + c3

x ~ a + b + c + zÂ +Â w

y ~ a + b + cÂ + zÂ +Â w

'

sem(modelx, data=mydata, estimator=""DWLS"")
</code></pre>

<p>zÂ and wÂ are covariates. x is a scale (0-12), however y is a binary variable (0;1).Â </p>

<p>Thus, the question is, how can I implement a linear regression for predicting x and a logistic regression for y in a single SEM model? I assume that lavaan doesn't automatically account for the different dependent variables. If it does, I would like to see a reference for that. All ideas are welcome.</p>

<p>Edit: Using the latent variable factor scores from the measurement model for a, b, c in a glm (binomial reg for y and linear for x) and lavaan, the results are more closely aligned for x than for y. Does it mean that lavaan ignores/doesn't do good with the dichotomous variable in this particular case, or my question from the start is moot or unnecessary?</p>
"
"0.146176336551172","0.14973818705887","183908","<p>I have a binary logistic regression with just one binary fixed factor predictor. The reason I don't do it as a Chi square or Fisher's exact test is that I also have a number of random factors (there are multiple data points per individual and individuals are in groups, although I don't care about coefficients or significances for those random variables). I do this with R glmer.</p>

<p>I would like to be able to express the coefficient and associated confidence interval for the predictor as a risk ratio rather than an odds ratio. This is because (maybe not for you but for my audience) risk ratio is much easier to understand. Risk ratio here is the relative increase in chance of the outcome being 1 rather than 0 if the predictor is 1 rather than 0.</p>

<p>The odds ratio is trivial to get from the coefficient and associated CI using exp(). To convert an odds ratio to a risk ratio, you can use ""RR = OR / (1 â€“ p + (p x OR)), where p is the risk in the control group"" (source: <a href=""http://www.r-bloggers.com/how-to-convert-odds-ratios-to-relative-risks/"" rel=""nofollow"">http://www.r-bloggers.com/how-to-convert-odds-ratios-to-relative-risks/</a>). But, you need the risk in the control group, which in my case means the chance that the outcome is 1 if the predictor is 0. I believe the intercept coefficient from the model is in fact the odds for this chance, so I can use prob=odds/(odds+1) to get that. I'm pretty much so-far-so-good on this as far as the central estimate for the risk ratio goes. But what worries me is the associated confidence interval, because the intercept coefficient also has its own associated CI. Should I use the central estimate of the intercept, or to be conservative should I use whatever limits of the intercept CI make my relative risk CI widest? Or am I barking up the wrong tree entirely?</p>
"
"0.0462250163521024","0.0473513723810378","184391","<p>I am attempting to perform a piecewise/segmented logistic regression on survey data using  <a href=""http://www.asdfree.com/2015/11/statistically-significant-trends-with.html"" rel=""nofollow"">this tutorial</a> as my basis. I have data for the period 2006 to 2013, however 2012 is missing.</p>

<p>The analysis proceeds as expected until the point in step 8 where I add the segmented variable with one breakpoint (the final line of code in the example below).</p>

<pre><code>library(segmented)
df &lt;- data.frame(yr=c(2006:2011,2013),
             mean= c(0.11290830, 0.12814364, 0.11149552, 0.12071058, 0.11776731, 0.10363014, 0.09888132),
             wgt = c(602.2272, 546.2958, 594.1818, 756.0167, 579.1533, 481.9694, 654.3281))
o &lt;- lm( log( mean ) ~ yr , weights = wgt , data = df )
os &lt;- segmented( o , ~yr)
</code></pre>

<p>At this point I get the error message:</p>

<blockquote>
  <p>""Error in segmented.lm(o, ~yr) : only 1 datum in an interval: breakpoint(s) at the boundary or too close each other""</p>
</blockquote>

<p>From my reading, in particular <a href=""http://r.789695.n4.nabble.com/Estimating-and-predicting-using-quot-segmented-quot-Package-td4682541.html"" rel=""nofollow"">here</a>, this is because the breakpoint falls at 2007, thus leaving 2006 on it's own and unable to have a slope calculated for it. I understand that this is likely because I have so few data points.</p>

<p>Does anyone have any tips for getting around this or another package / technique that would be more appropriate? The second link suggests using additional dummy data but I'm a bit wary of this approach.</p>
"
"0.0462250163521024","0.0473513723810378","185166","<p>I thought I've understood the output of the logistic regression in R (also I learned a lot through stackexchange), but somehow my vizualization tells me something different.
The output of the glm in R is: </p>

<pre><code>Call:
glm(formula = Zustand ~ Temp + Tag + Gehege + Temp:Gehege + Tag:Gehege + 
    Individuum + Gehege:Individuum + Temp:Individuum + Tag:Individuum, 
    family = binomial)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9257  -0.5462  -0.4408  -0.3106   2.8510  

Coefficients:
                               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -3.124244   1.302784  -2.398 0.016479 *  
Temp                           0.083585   0.068098   1.227 0.219664    
Tag                           -0.005485   0.013584  -0.404 0.686364    
Gehegeneues                    3.646637   1.249182   2.919 0.003509 ** 
IndividuumJoachim             -0.769787   0.927953  -0.830 0.406791    
IndividuumMary                -0.420745   0.797966  -0.527 0.598005    
Temp:Gehegeneues              -0.169516   0.062047  -2.732 0.006294 ** 
Tag:Gehegeneues               -0.057955   0.017154  -3.379 0.000729 ***
Gehegeneues:IndividuumJoachim  0.728588   0.329791   2.209 0.027158 *  
Gehegeneues:IndividuumMary     0.951688   0.396865   2.398 0.016484 *  
Temp:IndividuumJoachim         0.015466   0.049143   0.315 0.752979    
Temp:IndividuumMary           -0.012944   0.044467  -0.291 0.770985    
Tag:IndividuumJoachim         -0.035718   0.019177  -1.863 0.062532 .  
Tag:IndividuumMary             0.016538   0.019597   0.844 0.398724  
</code></pre>

<p>But my vizualization with the visreg-package...</p>

<pre><code>visreg(Ergebnis3, ""Gehege"", by = ""Individuum"", type = ""contrast"", scale = 'response', rug = F, ylim = c(0.0,0.6), main = ""Preening / Moulting"", xlab = ""Enclosure"", ylab = ""Likelihood"")
</code></pre>

<p>...looks like this:</p>

<p><a href=""http://i.stack.imgur.com/YxQc4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YxQc4.jpg"" alt=""enter image description here""></a></p>

<p>""Individuum = Georg"" and ""Gehege = altes"" is my reference level and I thought the coefficient of ""Gehege = neues"" (3.64) means that the probability of ""Zustand"" in ""Gehege = neues"" is 3.64 times higher than in ""Gehege = altes"" or not? But in the graphic it's lower. Also for changing continuous variables ""Temp"" and ""Tag"" (here it's shown i.e. for 19.5 Â°C) ""Gehege = neues"" keeps to be lower for most of the times. The odds Ratio for ""Gehege = neues"" are also about 38, this high number is a bit weird... </p>

<p>I hope this is enough information to help, I have a real complex question I want to answer.</p>
"
"0.123266710272273","0.126270326349434","188112","<p>I am studying logistic regressions and I wonder why are estimators biased when the independent variables have low variance (maybe low variance compared to its mean, but anyway).</p>

<p>I simulate the underlying model as a linear function of a single variable <code>x</code> and I do not include an error term. <code>x</code> is generated from a normal distribution, with mean <code>mx</code> and sd <code>sx</code>.</p>

<p><code>f</code> is a helper to map the probabilities using a logistic function</p>

<p>I use <code>mx = 1.0</code>, and sample <code>sx</code> from a uniform distribution from 0 to 1, so I can estimate the model for different values of <code>sx</code>.</p>

<pre><code>SAMPLE_SIZE = 1000
set.seed(100)

f &lt;- function(v) exp(v) / (1 + exp(v));

sim = function(b0, b1, mx, sx) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean = mx, sd = sx)
  ps &lt;- f(b0 + b1 * xs)
  ys &lt;- rbinom(SAMPLE_SIZE, 1, ps)
  glm(ys ~ xs, family = binomial)
}  


sx &lt;- runif(n = 1000, min = 0.05, max = 1.0)
b0 = 1.5
b0s &lt;- sapply(sx, function(v) {
  sim(b0 = b0, b1 = 1.0, mx = 1.0, sx = v)$coefficients[[1]]
})
</code></pre>

<p>And then I plot the error between the estimated <code>b0</code> coefficient and the real one, for different values of <code>sx</code>:</p>

<pre><code>plot(sx, b0s - b0)
</code></pre>

<p>What I get is that the error gets smaller the greater <code>sx</code> is.</p>

<p>From common linear regressions, we know that the estimators get more precise the larger the variance in the independent variables. But that does not say anything about the biases. </p>

<p>How to interpret this result? Are the estimators really biased in logistic regressions? What's missing here? Is there any problem related to numerical estimates here?</p>

<p><a href=""http://i.stack.imgur.com/aj8md.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aj8md.png"" alt=""Estimation error vs. standard deviation in X""></a></p>
"
"NaN","NaN","188399","<p>I would like to train a model that has a probability (a success rate between 0 and 1) as outcome.</p>

<p>So the data looks like this:</p>

<pre><code>feature1  feature2   success_rate
0.1       0.3        0.55
0.3       0.6        0.45
</code></pre>

<p>I started using <em>xgboost</em> (gradient boosting machine) with:</p>

<pre><code>""objective"" = ""reg:logistic""
""eval_metric"" = ""auc""
</code></pre>

<p>which means I doing a logistic regression using the Area Under the Curve (AUC) as evaluation function to measure the improvement of the model.</p>

<p>But I understand a logistic regression is usually trained with a categorical target (success or failure), not a probability.
Does this matter? and is this the right approach?</p>
"
"0.0653720450460613","0.0669649530182425","191712","<p>I am using KFAS to fit a dynamic logistic model of the form;</p>

<p>$\hat{y} = \bf \beta_t x + \epsilon$ </p>

<p>$\beta_t = \beta_{t-1} + \eta$</p>

<p>So the regression parameters change over time, and act as latent variables to be estimated by the filter.</p>

<p>Can state space models of this form generally accept situations where we have multiple observations per time period? I believe they can, but I can't figure out how to specify this in KFAS (or any other R package for that matter).</p>

<p>I've tried the below code, but KFAS thinks that this means there are 22 time periods - there are actually only ten.</p>

<pre><code>library(KFAS)
y = c(1,0,0,0,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1)
i = seq.Date(from = as.Date(""2014-01-01""), as.Date(""2014-01-10""), length.out = 22)
x = rnorm(n = 22, mean = 1, sd = 2)

a =   model = SSModel(y ~ 
                    SSMregression(~x),
                  distribution = ""binomial"")

fit = fitSSM(a, inits = c(0,0))
</code></pre>
"
"0.17295817388759","0.177172612243394","191916","<p>I have taken plenty of time to try and help myself, but I keep reaching dead ends. </p>

<p>I have a dataset consisting of body measurements collected from a bird species, and the sex of each bird (known by molecular means). I built a logistic regression model (using the AIC information criterion) to assess which measurements explain better the sex of the birds. My ultimate goal is to have an equation which could be used by others under field conditions to predict reliably the sex of the birds by taking as few body measurements as possible. </p>

<p>My final model includes four independent variables, namely ""Culmen"", ""Head-bill"", ""Tarsus length"", and ""Wing length"" (all continuous). I wish my model was a little more parsimonious, but all the variables seem to be important according to AIC criterion. Because the model produced should be used as prediction tool, I decided validate it using a leave-one-out cross validation approach. In my learning process, I first tried to complete the analyses (cross-validation and plotting) by including only one explanatory variable, namely ""Culmen"". </p>

<p>The output of the cross validation (package ""boot"" in R) yields two values (deltas), which are the cross-validated prediction errors where the first number is the raw leave-one-out, or lieu cross-validation result, and the second one is a bias-corrected version of it. </p>

<pre><code>model.full &lt;- glm(Sex ~ Culmen, data = my.data, family = binomial)
summary(model.full.1)

cv.glm(my.data, model.full, K=114)

$call
cv.glm(data = my.data, glmfit = model.full, K = 114)

$K
[1] 114

$delta
[1] 0.05941851 0.05937288
</code></pre>

<p>Q1. Could anyone expalin what do these two values represent and how to interpret them?    </p>

<p>Following is the code as presented by Dr. Markus MÃ¼ller (Calimo) in a similar, albeit not identical, post (<a href=""http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r"">http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r</a>) which I tried to tweak to meet my data:</p>

<pre><code>library(pROC)
data(my.data)
k &lt;- 114    # Number of observations or rows in dataset
n &lt;- dim(my.data)[1]
indices &lt;- sample(rep(1:k, ceiling(n/k))[1:n])

all.response &lt;- all.predictor &lt;- aucs &lt;- c()
for (i in 1:k) {
test = my.data[indices==i,]
learn = my.data[indices!=i,]
model &lt;- glm(Sex ~ Culmen, data = learn, family=binomial)
model.pred &lt;- predict(model, newdata=test)
aucs &lt;- c(aucs, roc(test$Sex, model.pred)$auc)
all.response &lt;- c(all.response, test$outcome)
all.predictor &lt;- c(all.predictor, model.pred)
}

Error in roc.default(test$Sex, model.pred) : No case observation.

roc(all.response, all.predictor)

Error in roc.default(all.response, all.predictor) : No valid data provided.

mean(aucs)
</code></pre>

<p>Q2. What's the reason for the first error message? I guess the second error is associated with the first one, and that it will be solved once I find a solution to the first one.</p>

<p>I will appreciate very much any help!!</p>

<p>Luciano </p>
"
"0.0924500327042048","0.0947027447620757","194140","<p>I've been using stepAIC to narrow down my logistic regression model.  However, I get the following warning when I run my model:</p>

<p>glm.fit: fitted probabilities numerically 0 or 1 occurred</p>

<p>I know this means I have complete or quasi-complete separation in my data.  On examination of my data, I see the quasi-complete separation and think that it's meaningful.  Reading online, I see recommendations to use a Firth penalized regression (logistf) or exact logistic regression (elrm); but neither of these will work with stepAIC.  I've also tried bayesglm but I still get the same warning. </p>

<p>How should I select a model when my data has complete separation?  How would I do this in R?  Is my mistake in my stats or in my understanding of using the packages in R?  Any help would be much appreciated!</p>
"
"0.130744090092123","0.133929906036485","198268","<p>I'm am trying to predict disease states in a medical setting where I have three subject groups (1,2,3). I have cross-validated a multinomial logistic regression model using the following</p>

<pre><code>cvfit=cv.glmnet(Xtrain, ytrain, family=""multinomial"", type.multinomial = ""grouped"", parallel = TRUE, standardize=TRUE)
</code></pre>

<p>where Xtrain is a 42x20 matrix with 42 observations and 20 predictors.</p>

<p>If I run the following to get the coefficients of the model</p>

<pre><code>coef(cvfit)
</code></pre>

<p>I get the following output</p>

<pre><code>$`1`
21 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)  2.519025
V1           2.955347
V2           .       
V3           .       
V4          -3.508274
V5           .       
V6           .       
V7           .       
V8           .       
V9           .       
V10          .       
V11          .       
V12          .       
V13          .       
V14          .       
V15          .       
V16          .       
V17          .       
V18          .       
V19          .       
V20         -2.108070

$`2`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)  1.5460376
V1          -5.2882709
V2           .        
V3           .        
V4           0.4144632
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          1.4674672

$`3`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept) -4.0650622
V1           2.3329236
V2           .        
V3           .        
V4           3.0938106
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          0.6406032
</code></pre>

<p>I would like to be able to say something concerning the risk of being in one group compared to another based on increments in the predictors with non-zero coefficients, however, I cannot seem to find any information as to which class the cvglmnet() function uses as base in order to calculate the risks. </p>

<p>Does anyone know this, or have an idea on how to interpret the results for use in a model?</p>

<p><strong>EDIT:</strong></p>

<p>I realize now that I may have overlooked a crucial detail. In ""The Elements of Statistical Learning: Data Mining, Inference, and Prediction"" by Hastie, T et al (2009), it is stated on page 657 that a multiclass logistic model can be described as</p>

<p>$P(Y=k|X=x) = \frac{\exp{(\beta_{k0}+x^{T}\beta_{k})}}{\sum_{l=1}^{K}\exp{(\beta_{l0}+x^{T}\beta_{l})}}$</p>

<p>where I can see that the denominator is just a normalization factor. I guess this means that I can interpret the obtained coefficients above directly for each subject group. Or is this wrongly interpreted?</p>
"
"0.153311035167967","0.1570467354963","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"NaN","NaN","199970","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>There are a few things I'm confused by here:</p>

<p>1) What is going on with the X1:term + term:X5 terms? What do they mean in the context of glm()?</p>

<p>2) There does not seem to be an intercept term in the output under <code>Coefficients</code>. Could this be for any other reason than there simply not being an intercept term?</p>

<p>3) The AIC for the model is 50000. How should I interpret this? Can I interpret this without more models to compare to? If it is not useful, what else should I be looking for instead?</p>
"
"0.0462250163521024","0.0473513723810378","200794","<p>I have a problem with outputting the terms for a logistic regression model in R. For a given list of independent values, say list l of terms {w,y,z} to determine dependent variable {x}, I want to find out what the biggest regressor is when we pair two terms together. I want to be able to group multiple independent variables together and say ""when a record has this combination of values, then they have a very strong chance of predicting X"". I tried to just add the interactions when calling the glm function like glm(x~y + w + z + w:z + y:z + y:w, data = l). But the results come out very hard to explain, because of how they are measured between themselves and not just measured against the mean. Does anyone know a way to do this?</p>
"
"0.10336227882434","0.105880887471907","202028","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>I am not too familiar with logistic regression, so I have a few questions about how to properly predict on a new test set using this model:</p>

<p>1) Unlike a regular regression, I cannot simply 'plug-in' the variables and get a meaningful numeric output. Instead, I must first set a threshold probability above which values will be 1 and below which values will be 0. Is this correct?</p>

<p>2) I cannot make use of this sample model or get the same results as the person who provided it until I have the probability threshold that was used for prediction. Is this correct?</p>

<p>3) If I wanted to split the outputs into tiers, would I use the probabilities for that and map them to some other value? How would that process work (feel free to let me know if this is out of scope).</p>

<p>Thanks!</p>
"
"0.0924500327042048","0.0947027447620757","202447","<p>I often come across a classification problem - where we have 0/1 binary outcome and several features. And the main goal is build a classifier on training set.</p>

<p>Now given several choices of algorithms - Random forests, logistic regression, SVM, etc., is there a scientific approach one can apply to choose one among the above algorithms just based on the data attributes. By attributes I mean number of features in dataset, no. of categorical variables, how many levels in categorical variables, etc.</p>

<p>In other words, you have dataset and based on it you take a call which method suits best.</p>

<p>The reason I ask is that I currently apply different methods and choose one with the best accuracy on cross validation set. But I think there is a way to narrow down on methods just based on dataset features.</p>

<p>Would appreciate any thoughts on this.</p>

<p>Thanks in advance!</p>
"
"NaN","NaN","203298","<p>I am implementing some machine learning algorithms on a  large data set (90K rows) with 274 different variables. I have to carry out Logistic Regression and Random Forest for this data set. meanwhile, I want to carry out feature selection to reduce the number of those variables drastically.</p>

<p>What would be an effective feature selection algorithm (in R) for classification use case?
Thanks,
Aman</p>
"
"0.0653720450460613","0.0669649530182425","203863","<p>I'm using R,OpenBUGS and R2OpenBUGS package on Linux Mint and i'm fitting a bayesian multinomial logistic model. After dribbling a lot of usual errors on my way to get my model running, i came upon an error i can't get the grasp of how to solve it. </p>

<p>So after ""model is syntactically correct,data loaded, model compiled,initial values generated, model initialized"" messages,
 i get 
""inference can not be made when sampler is in adaptive phase"".</p>

<p>Does anyone know what exactly this message mean and how could i solve it?
Thanks</p>
"
"0.0462250163521024","0.0473513723810378","204192","<p>I have a categorical response which i want to predict, so i am in the process of developing a logistic model.  I am using k-fold cross-validation for model selection. The first model, which was just an intercept model is throwing negative fitted values.</p>

<p>So i tried adding just 2 predictors to understand what was causing this, but the model with the 2 predictors is also predicting negative probabilities.</p>

<p>Below is the code that i used:</p>

<pre><code>    logistic_null1 &lt;- glm(SeriousDlqin2yrs ~ 1, family=binomial(), data=trainingdata)

logistic_null1 &lt;- glm(SeriousDlqin2yrs ~ age + income, family=binomial(), data=trainingdata)
</code></pre>

<p>I checked if may be the response Y is not a factors but doesn't seem to be the case either</p>

<pre><code>&gt; class(trainingdata$SeriousDlqin2yrs)
[1] ""factor""
&gt; check3 &lt;- as.data.frame(predict(logistic_null1, testdata))
&gt; summary(check3)
 predict(logistic_null1, testdata)
 Min.   :-4.609                   
 1st Qu.:-3.047                   
 Median :-2.700                   
 Mean   :-2.703                   
 3rd Qu.:-2.346                   
 Max.   :-1.601 
</code></pre>

<p>What could cause this</p>
"
"0.0653720450460613","0.0669649530182425","206790","<p>I have a count variable that represents the number of new band foundings in a country-year. However, there is zero inflation as there are no foundings for most country-year. There is also overdispersion as the variance is greater than the mean number of foundings. </p>

<p>Under these circumstances, a zero-inflated negative binomial model would fit best however I don't really see how the logistic step fits with the nature of my data. In other words, I don't think the assumptions of ZINB are satisfied. Is there an alternative model that I could use to get around the issue? Any help would be appreciated.</p>

<p>UPDATE 1: </p>

<pre><code>summary(zinb1 &lt;- zeroinfl(foundings ~ disbandings + pop100k | disbandings + pop100k,
           data = casper, dist = ""negbin""))
</code></pre>

<p>However I get the following warning message: </p>

<pre><code>Warning messages:
1: glm.fit: fitted probabilities numerically 0 or 1 occurred 
2: In sqrt(diag(object$vcov)) : NaNs produced
</code></pre>
"
"0.11322770341446","0.0966555841283824","206870","<p>By converting and by trying to interpret the parameters of a logistic regression ran in R, I just find them to be overestimated. Therefore I tried to compute them myself but I can not obtain the same values reported by the regression.</p>

<p>I used this web-page for computations:
<a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm</a></p>

<p>Let say we only focus on the LagC parameter:</p>

<p><strong>Logistic Regression</strong></p>

<pre><code>&gt; model &lt;- glmer(RepT2 ~ DistractorC1 + DistractorC2 + LagC + DistractorC1:LagC + DistractorC2:LagC + (LagC | Subject) + (1 | Item),
                data = DF,
                family = binomial(link = ""logit""),
                control = glmerControl(optimizer = ""bobyqa""))
&gt; summary(model)

  Fixed effects:
                    Estimate Std. Error z value Pr(&gt;|z|)    
  (Intercept)       -0.81039    0.22040  -3.677 0.000236 ***
  DistractorC1       0.33129    0.06393   5.182  2.2e-07 ***
  DistractorC2       0.03436    0.10011   0.343 0.731467    
  LagC               2.09567    0.12725  16.469  &lt; 2e-16 ***
  DistractorC1:LagC -0.21654    0.12770  -1.696 0.089932 .  
  DistractorC2:LagC -0.84018    0.20055  -4.189  2.8e-05 ***
</code></pre>

<p>Odds of the parameters:</p>

<pre><code>&gt; show(Odds &lt;- exp(summary(model)$coefficients[,""Estimate""])

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.4446833         1.3927594         1.0349529         8.1308503         0.8052993         0.4316343 
</code></pre>

<p>Probabilities of the parameters:</p>

<pre><code>&gt; show(P &lt;- Odds / (1 + Odds))

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.3078068         0.5820725         0.5085881         0.8904812         0.4460752         0.3014976 
</code></pre>

<p><strong>My Estimations</strong></p>

<pre><code>&gt; Means &lt;- DF %&gt;%
    group_by(Subject, Lag) %&gt;%
    filter(RepT1 == 1) %&gt;%
    summarise(repok = sum(RepT2) / (n())) %&gt;%
    group_by(Lag) %&gt;%
    summarise(Means = mean(repok))

&gt; show(Means)

     Lag     Means
  (fctr)     (dbl)
1   Lag3 0.1972174
2   Lag8 0.5475624
</code></pre>

<p>Odds of the parameter:</p>

<pre><code>&gt; OddsLag3 &lt;- 0.1972174 / (1-0.1972174)
&gt; OddsLag8 &lt;- 0.5475624 / (1-0.5475624)
&gt; OddsLagC &lt;- OddsLag8 / OddsLag3
&gt; show(OddsLagC)

[1] 4.926377
</code></pre>

<p>Probabilities of the parameter:</p>

<pre><code>&gt; show(OddsLag / (1 + OddsLag))

[1] 0.8312628
</code></pre>

<p>We can see that it is close, but not accurate. Does anyone have an explanation?
Note that I compute a mean for each subject and then only a mean for each condition. I also did estimate the parameters without taking into account the subjects, but still, the mismatch was here.</p>

<p><a href=""http://i.stack.imgur.com/YRdPf.png"" rel=""nofollow"">Graphical representation</a></p>
"
"0.0800640769025436","0.0820149827720712","207427","<p>I am confused with the answer from
<a href=""http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r"">http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r</a></p>

<p>It said if you want to predict the probability of ""Yes"", you set as  <code>relevel(auth$class, ref = ""YES"")</code>. However, in my experiment, if we have a binary response variable with ""0"" and ""1"". We only get the estimation for probability of ""1"" when we set <code>relevel(factor(y),ref=""0"")</code>.</p>

<pre><code>n &lt;- 200
x &lt;- rnorm(n)
sumx &lt;- 5 + 3*x
exp1 &lt;- exp(sumx)/(1+exp(sumx))
y &lt;- rbinom(n,1,exp1) #probability here is for 1
model1 &lt;- glm(y~x,family = ""binomial"")
summary(model1)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
model2 &lt;- glm(relevel(factor(y),ref=""0"")~x,family = ""binomial"")

summary(model2)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
</code></pre>

<p>I think if we want to get probability of ""Yes"", we should set <code>relevel(auth$class, ref = ""No"")</code>, am I correct? And what is reference level here means? Actually, what is glm() to predict in default if we use response other than ""0"" and ""1""? </p>
"
"0.10336227882434","0.0847047099775253","212213","<p>I have a problem with this ""-inf"" value in my table and I don't understand where it's coming from. As long as it's there, I cannot run a proportional odds assumption check. Here is my model, I used the MASS package:</p>

<pre><code>    m27 &lt;- polr(typ ~ bek2 + vst2 + verw2 + nwoe2, data = typmed23, method=""logistic"", Hess=T)

    summary(m27)
    Call: polr(formula = typ ~ bek2 + vst2 + verw2 + nwoe2, data = typmed23, Hess = T, method = ""logistic"")

    Coefficients:
        Value Std. Error t value
bek2   0.4620     0.2705  1.7080
vst2   0.1169     0.3217  0.3635
verw2 -0.7230     0.2580 -2.8028
nwoe2  1.8877     0.2791  6.7626

Intercepts:
     Value   Std. Error t value
1|2  2.9883  1.1341     2.6349
2|3  5.2964  1.1764     4.5021

Residual Deviance: 373.3716 
AIC: 385.3716 
</code></pre>

<p>Here is my code for the table:</p>

<pre><code>    sf &lt;- function(y) {
      c('Y&gt;=1' = qlogis(mean(y &gt;= 1)),   #typ outcome
        'Y&gt;=2' = qlogis(mean(y &gt;= 2)),
        'Y&gt;=3' = qlogis(mean(y &gt;= 3)))
    }
    s &lt;- with(typmed23, summary(as.numeric(typ) ~ bek2 + vst2 + verw2 + nwoe2, fun=sf))  
</code></pre>

<p>which produces this table:</p>

<pre><code>    as.numeric(typ)    N=244

    +-------+-+---+----+----------+-----------+
    |       | |N  |Y&gt;=1|Y&gt;=2      |Y&gt;=3       |
    +-------+-+---+----+----------+-----------+
    |bek2   |1|104|Inf | 0.6359888|-0.63598877|
    |       |2|128|Inf | 0.7884574|-0.54430155|
    |       |3| 12|Inf | 0.0000000|-1.60943791|
    +-------+-+---+----+----------+-----------+
    |vst2   |1| 55|Inf | 1.7707061| 0.10919929|
    |       |2|148|Inf | 0.8602013|-0.55431074|
    |       |3| 41|Inf |-1.0033021|-2.97041447|
    +-------+-+---+----+----------+-----------+
    |verw2  |1| 77|Inf | 3.6243409| 0.73236789|
    |       |2| 76|Inf | 1.1700713|-0.89794159|
    |       |3| 91|Inf |-0.7598386|-1.98413136|
    +-------+-+---+----+----------+-----------+
    |nwoe2  |1| 58|Inf |-2.3608540|       -Inf|
    |       |2| 40|Inf |-0.1000835|-0.96940056|
    |       |3|146|Inf | 2.8478121| 0.02739897|
    +-------+-+---+----+----------+-----------+
    |Overall| |244|Inf | 0.6808771|-0.62625295|
    +-------+-+---+----+----------+-----------+
</code></pre>

<p>My dataframe is fine: No NA or huge values, no empty cells, only 5 columns containing values between 1 and 3. Can somebody tell me what to do and how to solve the problem? </p>

<p>Here is the variable (column in a df) </p>

<pre><code>nwoe2: typmed23$nwoe2
 [1] 3 3 3 3 3 3 3 1 1 1 3 3 3 2 3 3 2 2 1 2 1 3 3 3 2 2 1 2 3 3 3 1 3 3 3
 [36] 3 2 3 2 3 3 3 2 3 1 3 3 1 3 1 3 3 3 2 1 1 3 3 2 1 1 1 3 3 2 3 3 3 2 1
 [71] 1 3 3 3 3 1 2 3 1 3 1 1 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3
 [106] 3 3 3 1 3 3 3 3 3 1 2 2 1 3 3 2 3 3 3 3 1 1 1 1 3 3 3 2 3 2 1 3 3 3 3
 [141] 1 2 3 1 3 3 3 3 3 1 3 3 3 1 2 2 3 1 3 3 1 2 3 2 2 1 3 3 3 3 3 1 1 3 3
 [176] 1 2 3 3 3 2 1 1 2 3 3 3 1 3 3 1 1 3 3 3 2 2 2 3 2 3 3 1 1 3 1 2 3 3 2
 [211] 3 1 3 3 2 3 3 1 3 3 1 1 3 3 3 3 3 3 3 1 1 3 1 3 3 3 2 3 3 2 1 1 1 3
</code></pre>

<p>Just to avoid any confusion: I'm referring to the lonely ""-Inf"" in column ""Y>=3"", not to the column ""Y>=1"".</p>
"
"0.11322770341446","0.115986700954059","212301","<p>I have a huge doubt, which I believe is Basic. I have no difficulty in interpreting the results of our logistic regression model using the ODD ratio, but I do not know what to do when I work with Mixed effects model for longitudinal data.</p>

<p>Below they use the <code>glmer</code> function to estimate a mixed effects logistic regression model with Il6, CRP, and LengthofStay as patient level continuous predictors, CancerStage as a patient level categorical predictor (I, II, III, or IV), Experience as a doctor level continuous predictor, and a random intercept by DID, doctor ID.</p>

<p>The <code>glmer</code> function created 407 groups that refer to the number of doctors.</p>

<p>What would it mean for example the -0.0568 of IL6 and the -2.3370 of CancerStageIV's in the study presented?</p>

#################

<p>m &lt;â€ glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer =  ""bobyqa""),      nAGQ = 10) 
print(m, corr = FALSE) </p>

<h1>Generalized linear mixed model fit by maximum likelihood</h1>

<h2>Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]</h2>

<h2>Family:</h2>

<p>binomial ( logit )  </p>

<h2>Formula:</h2>

<p>remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +<br>
   (1 | DID)  </p>

<p>Data: hdp  </p>

<pre><code>  AIC        BIC    logLik     deviance  df.resid   
 7397        7461    -3690        7379     8516 
</code></pre>

<h2>Random effects:</h2>

<p>Groups Name         Std.Dev.<br>
     DID    (Intercept) 2.01 </p>

<p>Number of obs: 8525, groups: DID, 407  </p>

<h1>Fixed Effects:</h1>

<pre><code>  Intercept    IL6        CRP       CancerStageII  
 â€2.0527     â€0.0568    â€0.0215       â€0.4139 

CancerStageIII   CancerStageIV       LengthofStay      Experience  
 â€1.0035           â€2.3370              â€0.1212          0.1201 
</code></pre>
"
"0.166666666666667","0.170727801083421","213531","<p>I am trying to analyse my data using bam. And I would greatly appreciate your advice as to the appropriate analyses.</p>

<p>The experimental design is: 
There are two groups of participants, ""CAT"" and ""PA"" coded in the factor ""group.""
Within each group, there are two conditions, ""Label"" and ""Ideo"" coded in the factor ""cnd.""
The prediction is that performance of participants in the Label condition would be higher compared to the Ideo condition, and that this difference would be greater in the CAT group compared to the PA group. So, my hypothesis forces me to test for an interaction of group by condition. The dependent variable is accuracy (so I need a logistic model) which greatly depends on time (coded as ""ctrial""). ""Sbj"" codes participants that will be treated as random effects. </p>

<p>My understanding of gamms is that If I had two isotropic continuous variables I could use s(x1,x2) and If I had two non-isotropic variables I could use te(x1,x2) to model interaction. But this is not the case, because my variables of interest are factors.</p>

<p>I believe that the standard approach is that I should create a variable with four levels (group by condition)</p>

<pre><code>data$igc &lt;- as.factor(paste(as.character(data$group),as.character(data$cnd),sep=""."")) 
</code></pre>

<p>and use an additive model such as:</p>

<pre><code>bam(acc~ 1 + igc + s(ctrial, by=igc) + s(sbj, bs = ""re""), data=data, family=binomial)
</code></pre>

<p>I could then inspect the plots and find out if CAT. label - CAT.ideo is greater compared to PA.label- PA.ideo.</p>

<p>But do I really need the 4-level variable?
I mean, is my 4-level model the most parsimonious one?</p>

<p>If there is no differences between the two groups, wouldn't a model such as 
    bam(acc~ 1 + cnd + s(ctrial, by=cnd) + s(sbj, bs =""re""), data=data, family=binomial)
be more appropriate?</p>

<p>[I believe that this question is not the same as the one posted here: <a href=""http://stats.stackexchange.com/questions/32730/how-to-include-an-interaction-term-in-gam"">How to include an interaction term in GAM?</a>, as my question is related to model comparison] </p>

<p>My best guess is that I should do the following: </p>

<pre><code>Analysis of Deviance Table

Model 1: acc ~ 1 + igc + s(ctrial) + s(sbj, bs = ""re"")
Model 2: acc ~ 1 + group + s(ctrial, by = group) + s(sbj, bs = ""re"")
Model 3: acc ~ 1 + cnd + s(ctrial, by = cnd) + s(sbj, bs = ""re"")
Model 4: acc ~ 1 + igc + s(ctrial, by = igc) + s(sbj, bs = ""re"")    
  Resid. Df Resid. Dev     Df Deviance  Pr(&gt;Chi)    
1     18350     8495.0                              
2     18347     8497.3 2.1307   -2.262              
3     18346     8474.8 1.3922   22.419 4.861e-06 ***
4     18338     8456.5 7.7730   18.327   0.01667 *  
</code></pre>

<p>Is my understanding correct?
And is this the correct way to ""justify"" the use of a 4-level variable?</p>

<p>Thank you in advance for your time,</p>

<p>Fotis</p>
"
"0.0462250163521024","0.0473513723810378","213910","<p>I'm curious as to how BoxTidwell works in R. The page for the package itself seems to lack descriptions. I have a logistic regression with many numerical and categorical predictors. Every time I use BoxTidwell(y ~ x1+x2...) I get</p>

<blockquote>
  <p>Error in boxTidwell.default(y, X1, X2, max.iter = max.iter, tol = tol,  : 
    the variables to be transformed must have only positive values</p>
</blockquote>

<p>This occurs even when I removed all the negative predictors. Does this mean that I should not take any categorical variables in the test? and because I do have negative predictors how would I incorporate them?</p>

<p>Also, should I specify something like 'family= binomial' in the command as I do in glm?</p>
"
"0.196116135138184","0.189734033551687","213982","<p>I am trying to use ""propodds""  in the VGAM function in R, but am not sure if I am doing it right and don't really understand how to analyze the output I got so far to check to see if I am using it right. Any help on how to correctly use ""propodds"" or analyze the output would be appreciated. This is what I have so far:</p>

<pre><code>    &gt; fittest &lt;-vglm(rp ~ is.native + is.male + age2 + is.debt + oh + ms + cjs, propodds, data = dummydata2)
&gt; fittest
Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

    Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5     is.native 
  2.827674173  -0.463602645  -0.474290665  -0.614877500  -2.514394420  -0.063546621 
      is.male          age2       is.debt            oh            ms           cjs 
  0.114052675   0.067835161  -0.058563607  -0.089420626   0.109135966   0.003937505 

Degrees of Freedom: 52000 Total; 51988 Residual
Residual deviance: 24702.04 
Log-likelihood: -12351.02 
&gt; summary(fittest)

Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

Pearson residuals:
                    Min      1Q  Median      3Q     Max
logit(P[Y&gt;=2])  -5.1080  0.1300  0.2803  0.3016  0.3335
logit(P[Y&gt;=3])  -0.6976 -0.5876 -0.5490  0.5937 14.7717
logit(P[Y&gt;=4]) -13.1157 -0.5173 -0.4831  0.6080  3.0626
logit(P[Y&gt;=5])  -4.0174 -0.4072 -0.3746  1.0167  1.2176
logit(P[Y&gt;=6])  -0.6164 -0.5749 -0.1610 -0.1541  3.6060

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept):1  2.827674   0.079827  35.423  &lt; 2e-16 ***
(Intercept):2 -0.463603   0.068894  -6.729 1.71e-11 ***
(Intercept):3 -0.474291   0.068901  -6.884 5.83e-12 ***
(Intercept):4 -0.614878   0.069009  -8.910  &lt; 2e-16 ***
(Intercept):5 -2.514394   0.074892 -33.573  &lt; 2e-16 ***
is.native     -0.063547   0.062409  -1.018  0.30857    
is.male        0.114053   0.039694   2.873  0.00406 ** 
age2           0.067835   0.024789   2.737  0.00621 ** 
is.debt       -0.058564   0.052983  -1.105  0.26902    
oh            -0.089421   0.057526  -1.554  0.12008    
ms             0.109136   0.041587   2.624  0.00868 ** 
cjs            0.003938   0.043653   0.090  0.92813    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of linear predictors:  5 

Names of linear predictors: 
logit(P[Y&gt;=2]), logit(P[Y&gt;=3]), logit(P[Y&gt;=4]), logit(P[Y&gt;=5]), logit(P[Y&gt;=6])

Dispersion Parameter for cumulative family:   1

Residual deviance: 24702.04 on 51988 degrees of freedom

Log-likelihood: -12351.02 on 51988 degrees of freedom

Number of iterations: 4 

Exponentiated coefficients:
is.native   is.male      age2   is.debt        oh        ms       cjs 
0.9384304 1.1208112 1.0701889 0.9431182 0.9144608 1.1153140 1.0039453 
</code></pre>

<p>A little background on my data that may help: I am trying to determine if risk preferences (variable ""rp"" in the code) is determined by immigration status (variable ""is.native"" in the code, which is a dummy variable where 0 = native and 1 = immigrant). I have a few factors that I want to control for since they may affect risk preferences [age2, is.debt, oh(owns home), ms (marital status), and cjs (current job status)]. Based on similar research the best way to analyze this is the cumulative logistic regression and they seemed to look at the proportional odds. The data came from the 2014 Health and Retirement Study which is representative of the US population over age 50. There are about 20,000 participants. </p>

<p>I'm not sure if my model is formatted correctly. ""rp"" has 6 categories - a control group, low risk tolerance (rt), some rt, high rt, substantial rt and ""ignore"" which is answers of ""don't know"" or ""NA"".  All other variables are dummy variables with only options for ""0"" or ""1"" besides ""age2"" which has 6 categories (under 50, 50-60, 60-70, 70-80, 80-90, 90+). Are these dummy variables appropriate to use or should I just use the actual answers provided by the participants?</p>

<p>I know the significant codes in the ""summary"" section tell me gender, age, and marital status are significant at the 1% significance level, but I don't understand any of the other results. Such as, what does it mean that all the intercepts are significant? Is the model as a whole significant? What is the dispersion parameter? What are the exponentiated coefficients? </p>
"
"0.0462250163521024","0.0473513723810378","214790","<p>My model is logistic regression. Is there a way to tune the parameter lambda of lasso or ridge based on cross-validated log-loss and brier(eg. proper scores?) in any R packages? </p>

<p>I'm using glmnet right now and the only measure available seems to be deviance, mean absolute error, misclassification error(is this based on 0.5 cut off?) and auc which are not proper scores and are therefore less desirable. </p>

<p>On a related note, is there a score like squared loss but penalize the deviation from one outcome more severely?</p>
"
"0.0653720450460613","0.0669649530182425","215560","<p>I have three data sets that, when joined, have O(320) independent variables for a classification problem.  </p>

<p>Principal component analysis (PCA) seems out of the question because the data is mostly factors, not continuous.</p>

<p>I'm at a loss as to how to proceed.  </p>

<p>How do experienced analysts go about winnowing a large data set with hundreds of columns to something manageable?  How do you decide between variables?  What calculations can you go on to supplement your gut and experience?  How do you avoid throwing away significant variables?</p>

<p>A large number of columns might not be a problem for R, given enough CPU and RAM, but coming up with a cogent story should include identifying what is truly significant.  How to accomplish that?</p>

<p>Should I just toss all of it into a logistic regression and see what happens, without any forethought?</p>

<p>More detail in response to comments:</p>

<ol>
<li>Classification. </li>
<li>Many more observations than columns. </li>
<li>Yes, big oh notation meaning approximately. </li>
<li>Linear model at first. Also interested in boosted models in addition to logistic regression. </li>
</ol>
"
"0.0462250163521024","0.0473513723810378","216119","<p>I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket.</p>

<pre><code>  logitshots &lt;- glm(df$shot_made_flag ~ df$shot_distance, family = binomial(link=""logit""))
Call:  glm(formula = df$shot_made_flag ~ df$shot_distance, family = binomial(link = ""logit""))

Coefficients:
 (Intercept)  df$shot_distance  
      0.3681           -0.0441  

Degrees of Freedom: 25696 Total (i.e. Null);  25695 Residual
Null Deviance:      35330 
Residual Deviance: 34290    AIC: 34300
</code></pre>

<p>As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. </p>

<p>To do so I have done it this way, but I get a positive effect, so I am not sure about it. </p>

<pre><code>(exp(coef(logitshots))/(1+exp(coef(logitshots))))
(Intercept) df$shot_distance 
   0.5909933        0.4889768 
</code></pre>

<p>So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?)  </p>

<p>I'd really appreciate any interesting insight and help! :)</p>
"
"0.153311035167967","0.142769759542091","218738","<p>I want to build a linear regression model where I predict a mean of a group of participants (how they rate something on average). Predictors should be </p>

<ol>
<li>age (continuous)</li>
<li>origin (deviation coded, each level compared to grand mean, levels=1,2,3,4)</li>
<li>education (Helmert coded, each level compared to subsequent ones, haven't decided on number of levels yet)</li>
<li>gender/sex (dummy coded, 0/1)</li>
</ol>

<p>Following questions:</p>

<p><strong>1.</strong> In R, I use the following code for the coding, for example for 4) sex:</p>

<pre><code>    data$sex &lt;- factor(data$sex, labels=c(""1"",""2"")) 
    contr.treatment(2) 
    contrasts(data$sex) = contr.treatment(2)  
</code></pre>

<p>That gives me the right (dummy) coding, for the other 2 and 3 a little differently. Can I use run this kind of code for each predictor (except age) and then throw all predictors into a model like this:</p>

<pre><code>    model &lt;- lm(Mean ~ age +  sex + educ..., data)
</code></pre>

<p>It seems wrong because: what is the common intercept going to be with these different coding systems? It's different for each coding system.
But then, how am I going to enter these different predictors into a model?</p>

<p><strong>2.</strong> Can I leave age in there as it is, unchanged, continuous?</p>

<p><strong>3. Quite a different question:</strong> This is my <em>participant analysis</em>. For the <em>item analysis</em>, I used a logistic regression based on medians instead of means. That's because I did four rating surveys with Likert scales. 4 surveys - 4 participant groups - each group rated the items on <strong>one</strong> property only, such that each item was rated on 4 properties by different people.</p>

<p>Given this, is it okay if I use linear models and means in this analysis now? And can I even build my model as I suggested above?</p>

<p><strong>Many thanks</strong> for any input! I've been trying some things, but confusion isn't fading yet...</p>
"
"0.0924500327042048","0.0710270585715568","219390","<p><a href=""http://i.stack.imgur.com/fUmBg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fUmBg.png"" alt=""enter image description here""></a></p>

<p>This is a graph of revenues for different products with the Y-axis showing normalized revenues (mean of 3 and SD of 1) and X-axis is weeks. I need do a regression analysis of sorts on this data and am unsure how to find a curve/function in R that fits this data. </p>

<p>The data points can be interpreted as being: Week 0 of product release yield normalized revenues between 2.25 to 3.25, etc.</p>

<p>Any help regarding what kind of statistical analysis I can use to create a regression model (linear and logistic wouldn't work clearly) with the end goal being to do predictive analysis (ie. if a new product is released, what normalized revenues would it yield in the first 6 weeks)</p>

<p>Thanks</p>
"
"0.11322770341446","0.115986700954059","219684","<p>I am trying to create a logistic regression model to predict whether a customer given a loan will be a bad or a good customer: bad meaning missing a certain amount of payments and good meaning frequent enough and in time with payments. For the purpose of the model I have coded Bad as 1 and Good as 0 and tried different combinations with the variables. </p>

<p>One of the models I have built has an AIC of 5383.7 and Gini coefficient of 0.416733. This is the result after I play around with the threshold:</p>

<pre><code>     FALSE TRUE
  0  3327  638
  1   165   95
</code></pre>

<p>So the model guessed that 165 customers would be good, but they are bad, but also put 638 good customers into the bad customers group.</p>

<p>The second model I built has an AIC of 5734.6 (350.9 higher), but its Gini is 0.4190394 and is slightly better at predicting the bad customers:</p>

<pre><code>     FALSE TRUE
  0  3537  673
  1   177  105
</code></pre>

<p>[UPDATE] Okay. After checking a few things - It turns out that one of the variables has missing values and the model excludes the observations that have them by default. Hence the difference in observations in my models. I know about multiple imputation, but I don't really feel alright with it. My question is should I impute the missing data or should I exclude it from the data set so I can compare models with different number of variables?</p>
"
"0.122299897617557","0.125279955557839","219828","<p>I am doing logistic regression in R on a binary dependent variable with only one independent variable. I found the odd ratio as 0.99 for an outcomes. This can be shown in following. Odds ratio is defined as, $ratio_{odds}(H) = \frac{P(X=H)}{1-P(X=H)}$. As given earlier $ratio_{odds} (H) = 0.99$ which implies that $P(X=H) = 0.497$ which is close to 50% probability. This implies that the probability for having a H cases or non H cases 50% under the given condition of independent variable. This does not seem realistic from the data as only ~20% are found as H cases. Please give clarifications and proper explanations of this kind of cases in logistic regression.</p>

<p>I am hereby adding the results of my model output:</p>

<pre><code>M1 &lt;- glm(H~X, data=data, family=binomial())
summary(M1)

Call:
glm(formula = H ~ X, family = binomial(), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8563   0.6310   0.6790   0.7039   0.7608  

Coefficients:
                Estimate      Std. Error      z value     Pr(&gt;|z|)    
(Intercept)    1.6416666      0.2290133      7.168      7.59e-13 ***
   X          -0.0014039      0.0009466     -1.483      0.138    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1101.1  on 1070  degrees of freedom
Residual deviance: 1098.9  on 1069  degrees of freedom
  (667 observations deleted due to missingness)
AIC: 1102.9

Number of Fisher Scoring iterations: 4


exp(cbind(OR=coef(M1), confint(M1)))
Waiting for profiling to be done...
                                      OR           2.5 %       97.5 %
(Intercept)                    5.1637680       3.3204509     8.155564
     X                         0.9985971       0.9967357     1.000445
</code></pre>

<p>I have 1738 total dataset, of which H is a dependent binomial variable. There are 19.95% fall in (H=0) category and remaining are in (H=1) category. Further this binomial dependent variable compare with the covariate X whose minimum value is 82.23, mean value is 223.8 and maximum value is 391.6. The 667 missing values correspond to the covariate X i.e 667 data for X is missing in the dataset out of 1738 data.</p>
"
"NaN","NaN","220168","<p>I'm searching for an R package to raster time series smoothing. Currently, I'm using an approach like this one (using the equation suggested by Hamunyella et al., 2013)</p>

<pre><code>for (i in 2:(length(stacklist)-1)){
    r &lt;-  raster(stacklist[i])
    r1 &lt;- raster(stacklist[i-1])
    r3 &lt;- raster(stacklist[i+1])
    r2&lt;-mean(r1,r3)
    r[((r-r1)&lt;(-0.01*r1)) &amp; ((r-r3)&lt;(-0.01*r3))]&lt;-r2[((r-r1)&lt;(-0.01*r1)) &amp; ((r-r3)&lt;(-0.01*r3))]
    writeRaster(r,filename=paste(substr(stacklist[i], p1+1, (p1+7)),""_cropmLname.tif"",sep=""""),format=""GTiff"",overwrite=TRUE)
}
</code></pre>

<p>This is fast, but there ought to be an easier way. For example for implementing another kind of filter (e.g. Savitzky-Golay, Double-Logistic filtering). Does anybody know such?</p>
"
"0.139373668334515","0.142769759542091","221046","<p>I have a small data set of cases from an outbreak. For each case I have collected data for a few variables (age, HIV status, hospitalisation, country of infection etc. ). There are only 28 cases and so I imagine that it won't be possible to do any meaningful inferential statistical analysis. I would however like to present the data in as scientific manner as possible. </p>

<p>I would like to, for example, tabulate the data showing the difference between the cases that are HIV positive and HIV negative. I've included some R script and the results of what I've managed to do so far: </p>

<pre><code>data &lt;- read.csv(""Shigella.csv"")
library(dplyr)
library(epiR)
data$Hospitalised &lt;- factor(data$Hospitalised, levels = c(""Yes"", ""No""), labels = c(""Hospitalised"", ""Not Hospitalised""))
data$HIV &lt;- factor(data$HIV, levels = c(""Positive"", ""Negative""))
ttab &lt;- table(data$Hospitalised, data$HIV)
ttab


                   Positive Negative
  Hospitalised            2        1
  Not Hospitalised       13       11


Odds_Ratio &lt;- epi.2by2(ttab, method = ""case.control"", conf.level = 0.95)
Odds_Ratio

&gt; Odds_Ratio
             Outcome +    Outcome -      Total        Prevalence *        Odds
Exposed +            2            1          3                66.7        2.00
Exposed -           13           11         24                54.2        1.18
Total               15           12         27                55.6        1.25
Point estimates and 95 % CIs:
-------------------------------------------------------------------
Odds ratio (W)                               1.69 (0.13, 21.27)
Attrib prevalence *                          12.50 (-44.45, 69.45)
Attrib prevalence in population *            1.39 (-25.97, 28.75)
Attrib fraction (est) in exposed  (%)        39.79 (-1206.69, 99.08)
Attrib fraction (est) in population (%)      5.45 (-22.83, 27.23)
-------------------------------------------------------------------
 X2 test statistic: 0.169 p-value: 0.681
 Wald confidence limits
 * Outcomes per 100 population units
</code></pre>

<p>From the above, my interpreting is that the odds of a hospitalised case being HIV postive are 1.69 that of a non-hospitalised case being HIV positive. The 95% CI is (0.13, 21.27). </p>

<p>I am not sure that the p value is however. I see a p value of 0.681 but that seems to be for the X2 value. </p>

<p>Here is what I'm stuck with: 
1) am I using the right statistical test (or should I be doing logistic regression or something else)? 
2) how do I get a p value for the odds ratio (or is the p value provided above actually for the odds ratio)? </p>

<p>With appreciation! </p>

<p>Greg</p>
"
"0.174325453456164","0.189734033551687","221510","<p>I'm new to logistic regression analysis, and was unable to find an answer elsewhere in Cross Validated or Stack Overflow. </p>

<p>Consider a standard logistic regression analysis of a binary outcome (admission to college) based on continuous covariates gre score and high school gpa, and ordinal categorical rank prestige of the undergraduate institution (data from the nice UCLA stats dept. logistic regression in R tutorial: <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a>)</p>

<pre><code>&gt; admissions.data &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; admissions.data$rank &lt;- as.factor(admissions.data$rank)
&gt; summary(admissions.data)
     admit             gre             gpa        rank
 Min.   :0.0000   Min.   :220.0   Min.   :2.260   1: 61
 1st Qu.:0.0000   1st Qu.:520.0   1st Qu.:3.130   2:151
 Median :0.0000   Median :580.0   Median :3.395   3:121
 Mean   :0.3175   Mean   :587.7   Mean   :3.390   4: 67
 3rd Qu.:1.0000   3rd Qu.:660.0   3rd Qu.:3.670
 Max.   :1.0000   Max.   :800.0   Max.   :4.000

&gt; fit1 &lt;- glm(admit ~ gre + gpa + rank, data = admissions.data, family=""binomial"")
&gt; summary(fit1)

Call:
glm(formula = admit ~ gre + gpa + rank, family = ""binomial"",
    data = admissions.data)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6268  -0.8662  -0.6388   1.1490   2.0790

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *
gpa          0.804038   0.331819   2.423 0.015388 *
rank2       -0.675443   0.316490  -2.134 0.032829 *
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4

# Odds Ratios
&gt; exp(coef(fit1))
(Intercept)         gre         gpa       rank2       rank3       rank4
  0.0185001   1.0022670   2.2345448   0.5089310   0.2617923   0.2119375

# 95% confidence intervals
&gt; exp(confint(fit1))
Waiting for profiling to be done...
                  2.5 %    97.5 %
(Intercept) 0.001889165 0.1665354
gre         1.000137602 1.0044457
gpa         1.173858216 4.3238349
rank2       0.272289674 0.9448343
rank3       0.131641717 0.5115181
rank4       0.090715546 0.4706961
</code></pre>

<p>My questions are:</p>

<p>1) In R, is there a straight-forward way to determine ORs with 95% CIs for specific values of the covariates? E.g., based on this model, what are the odds of college acceptance for students applying to a rank 2 schools with a gpa of 3 and a gre score of 750, compared with a student applying to a rank 3 school with the same gpa and gre score? I could calculate ORs by hand given the model coefficient estimates and these specific covariate values, but am unsure how to correctly propagate SEs to calculate 95% CIs.</p>

<p>2) Would this particular example be considered a case-control study design, and therefore odds ratios could be estimated, but not predictions? (See: <a href=""http://stats.stackexchange.com/questions/69561/case-control-study-and-logistic-regression"">Case-control study and Logistic regression</a>)</p>
"
"0.160128153805087","0.164029965544142","222479","<p>I'm new in this area, hope my question is understandable.
I need to fit conditional logistic regression model in R and use it for predictions on unseen data (output should be probability).
My datasets are  quite large (over 150k rows) and contains many (~500) noisy features.
I found package called <strong>clogitboost</strong> and tried to use it with relatively small number of boosting iterations (max 30, because with larger values it takes too long to compute and raises an error in the end - perhaps, it's resources limitations) - results are mediocre. I tried to use unconditional approach with regularization - <strong>glmnet</strong> and got better results, however, due nature of data I guess it will be better to use conditional regression with regularization similar to what is used in <strong>glmnet</strong> (tried to remove some features and apply <strong>clogitboost</strong> again and got slightly better results). There is package called <strong>clogitL1</strong> , which seems to do that, I tried to use it and it fits model quickly, but it doesn't provide <strong>predict()</strong> function, Usage described in  paper with attached R code here:
<a href=""https://www.jstatsoft.org/article/view/v058i12"" rel=""nofollow"">https://www.jstatsoft.org/article/view/v058i12</a>, they made some predictions in some way, but I can't understand it. Can I somehow manually predict using  unseen data, just like it's possible with <strong>clogitboost</strong> <strong>predict()</strong> (parameters are Model, X and Strata column) using model that was fitted with <strong>clogitL1</strong>? Note: in description of package <strong>clogitL1</strong> - ""Tools for the fitting and cross validation of <strong><em>exact</em></strong> conditional logistic regression models"" - so I'm not sure about what ""exact"" means here and  if it makes sense to use that package for my purposes. If it's not possible to predict, then, should I manually select features by checking their ""importance"" that can be found in <strong>clogitL1</strong> model? </p>
"
"0.11322770341446","0.115986700954059","223441","<p>I am running a multivariate binary logistic regression trying to predict political party affiliation using the variables: <strong>Age, Sex, Race, Level of Education, Church Attendance</strong></p>

<p>I have ran the regression using glm() in R and have got some pretty decent results. All of them being pretty significant with p-values less than .05.</p>

<p>I know that doesn't really speak to the validity of the regression but I am now trying to graph it and can't figure out if it's possible given how many ""values"" are within each category.</p>

<p>Although I am only using 5 variables I end up with about 15 values and I know that means a lot of potential graphs.</p>

<p>Is this possible and would anybody know how to set it up using R?</p>

<p>I have attempted to graph just the first few values thus far but it doesn't look correct and I'm thinking it would be impossible to have all the variable values in a single graph. </p>
"
"0.167093470609201","0.183391076651825","225283","<p>Iâ€™m analyzing crowdsourced Twitter data, where workers labeled tweets. Within my dataset (N=2,400), I have one IV (call it â€˜dsâ€™) with 2 levels that differentiates which dataset the workers labeled. I have four factors of interest (what workers labeled) -- these are my DVs (let's call them f1, f2, f3, f4). Three of those factors are binomial &lt;0,1>, and one multinomial &lt;0, 1, 2>. Even though the latter can be treated as ordinal, I'm working under the assumption it is nominal. Finally, my datasets are of unequal lengths.</p>

<p>My goal is to analyze the relationship between each of the labeled factors for each level of the IV. More specifically, <strong>I want to tease out the different contributions of each of those factors on each dataset quantitatively, i.e., show amount of variance explained</strong> (e.g., ds1 influenced f1 more than f2, while the inverse for ds2). The end game is to model each factor into a scoring function, which allows me to compute a unified score. Hence, I need to back up the parameter weights for this function.</p>

<p>A snippet of my data frame looks like this:</p>

<pre><code>   f1 f2 f3 f4 ds
1   1  0  1  0  1
2   0  0  0  2  1
3   0  0  1  1  2
4   1  1  0  2  2
</code></pre>

<p>What I initially did was to compute correlations between each factor, and used the strengths of those correlations to back up my scoring function. However, given the many posts and tutorials I've been reading, it seems I need to make use of a mix of logistic and multinomial regression. What I have done so far is run binomial logit (using ?glm with class â€˜binomial') on the first 3 factors, and multinomial regression (using ?nnet) on f4. However, it seems I can only assess one outcome variable at a time.</p>

<p>For f1-f3, I have run the following R code:</p>

<pre><code>fit &lt;- glm(f1 ~ ds, data = xx, family = ""binomial"")
summary(fit)
confint.default(fit)
wald.test(b = coef(fit), Sigma = vcov(fit), Terms = 2)
</code></pre>

<p>For f4:</p>

<pre><code>fit &lt;- multinom(f4 ~ ds, data = xx)
summary(fit)
z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1))*2
</code></pre>

<p>My questions:</p>

<p><strong>1.</strong> Is running such logistic regression analyses appropriate for what I want to do, namely to tease out contributions of each factor? If so, is it meaningful to compare the coefficients of each factor with the other, when computed separately? Or is simply showing a correlation matrix sufficient in my case?</p>

<p><strong>2.</strong> Are there alternative techniques to assess all outcome variables/DVs at once, with respect to each level of my IV? If so, could you please provide me with some pointers (ideally for R)? I'm now looking into hierarchical multinomial marginal (HMM) models... </p>

<p>If something is unclear above, Iâ€™d be happy to clarify.</p>
"
"NaN","NaN","228878","<p>gling with the interpretation of the coefficients of a zero-inflation model and I find no clear answer in the net. Maybe someone can help me and other people in the same situation.</p>

<p>After fitting cancer incidences through a Poisson regression with zero-inflation (zeroinfl package in R), in the logistic component, the coefficient estimate for the age variable is -3.6.</p>

<p>Does that mean that for each additional year of age, the odds of having zero cancer incidences increases by 3.6, or vice versa?</p>

<p>Many thanks, Gion</p>
"
"0.122299897617557","0.125279955557839","229211","<p>I have a logistic mixed model, where I look at the effect of two conditions over  accuracy (% correct) for different subjects. </p>

<pre><code>glmres = glmer(accuracy ~ condition + (1|subject), data = myData, family = binomial) 
</code></pre>

<p>All subjects saw the same 100 stimuli (words), but the experimental condition these words belonged to depended on the subject. So for example, for some subjects word A belonged to condition C1 and word B belonged to condition C2. For some subjects it was the other way around. The words were drawn from a bigger pool so I think I should include them as a random effect. </p>

<p>The problem is, the correspondence between words and conditions could not be (precisely) controlled and it is the case that for <em>most</em> subjects (about 70:30, depending on the word), A belonged to condition C1.
This means that the fixed effect <code>condition</code> and the (potential) random effect <code>(1|word)</code> are not completely independent. </p>

<p>My question is whether it would be incorrect to include two non-independent effects (where one is fixed, one is random). As a result of this dependency (I think) my results change a lot according to whether I include the random effect or not. </p>
"
"0.18490006540841","0.189405489524151","229477","<p><br>I am struggling to interpret the results of a binomial logistic regression I did.<br> The experiment has 4 conditions, in each condition all participants receive different version of treatment. <br>DVs (1 per condition)=DE01,DE02,DE03,DE04, <br>all binary (1 - participants take a spec. decision, 0 - don't)
<br>Predictors: FTFinal (continuous, a freedom threat scale)
<br>SRFinal (continuous, situational reactance scale)
<br>TRFinal (continuous, trait reactance scale)
<br>SVO_Type(binary, egoists=1, altruists=0)
<br>After running these binomial (logit) models,<br><br> <code>model_soc_inf&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal+SVO_Type,
                    family=binomial(link='logit'),data=mydata)
model_soc_inf1&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal,
                     family=binomial(link='logit'),data=mydata)
summary(model_soc_inf)
model_pers_inf &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_inf1 &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
model_pers_inf2 &lt;- glm(mydata$DE02~SRFinal+TRFinal+SVO_Type,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_inf)
model_soc_uninf&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal+SVO_Type,
                     family=binomial(link='logit'),data=mydata)
model_soc_uninf1&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal,
                      family=binomial(link='logit'),data=mydata)
summary(model_soc_uninf)
model_pers_uninf&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_uninf1&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_uninf)</code><br><br>I ended up with the following<a href=""http://i.stack.imgur.com/4JKLa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4JKLa.png"" alt=""enter image description here""></a>. Initially I tested 2 models per condition, when condition 2 (DE02 as a DV) got my attention. In model(3)There are two variables, which are significant predictors of DE02 (taking a decision or not) - FTFinal and SVO Type. In context, the values for model (3) would mean that all else equal, being an Egoist (SVO_Type 1) decreases the (log)likelihood of taking a decision in comparison to being an altruist. Also, higher scores on FTFinal(freedom threat) increase the likelihood of taking the decision. So far so good. Removing SVO_Type from the regression (model 4) made the FTFinal coefficient non-significant. Removing FTFinal from the model does not change the significance of SVO_Type.</p>

<p>So I figured:ok, mediaiton, perhaps, or moderation. I tried first to look for mediation in both in R and SPSS. The moderation attempt was in vain: entering an interaction term SVO_Type:FTFinal makes all variables in model(3) non-significant.Here's the code for that:<code>model1&lt;-glm(DE02~FTFinal,family=binomial(link='logit'),data=mydata)
summary(model1)
model2&lt;-glm(DE02~SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model2)
model3&lt;-glm(DE02~FTFinal+SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model3)
interaction&lt;-glm(DE02~SVO_Type+FTFinal+SVO_Type:FTFinal, family =binomial(
  link = ""logit""),data = mydata)</code> <br>As for mediation, I followed  <a href=""http://www.nrhpsych.com/mediation/logmed.html"" rel=""nofollow"">this</a> mediation procedure for logistic regression, but found no mediation. </p>

<p>To sum up:
There is some relationship between SVO_Type and FTFinal, but I have no clue what.
Predicting DE02 from SVO_Type only is not significant.
Predicting DE02 from FTFinal is not significant
Putitng those two in the regression makes them both significant predictors.
Including an interaction between these both in any model, predicting DE02 model makes all variables in the model insignificant.<br>
So I am at a total loss: As far as I know, to test moderation, you need an interaction term. This term is between a categorical var (SVO_Type) and the continuous one(FTFinal), perhaps that goes wrong? And to test mediation outside SPSS, I tried the ""mediate"" package in R, only to discover that there is a ""treatment"" argument in the main funciton, which is to be the treatment variable (exp Vs cntrl). I don't have such, all ppns are subjected to different versions of the same treatment. 
I apologize for <a href=""http://www.filedropper.com/mydata"" rel=""nofollow"">this external way of uploading the dataset</a>, it is way too complicated to reproduce here (I am a noob).
Any help would be greatly appreciated. I have no clue what the relationship between SVO_Final and FTFinal is.</p>
"
"0.160128153805087","0.150360801748797","229598","<p>This isn't a problem with correlation between predictors - I have two models, each considers only one of the variables. That is the only difference between the models.  </p>

<p>I'm estimating the probability of an diagnosis given some confounders and a measure of monthly temperature. I have two possible temperature definitions I'm considering: monthly average temperature and monthly average <em>high</em> temperature. I don't expect the response to temperature to be linear, so I broke average temperature into 5 degree bins with bottom and top coding at &lt; 40 and > 90. I did the same with average high temperature but shifted the bins slightly with bottom and top coding &lt; 50 and > 100. </p>

<p>I estimate the first logistic model </p>

<pre><code>event ~ age + sex + ... + mean_temp_group
</code></pre>

<p>and get the response I'd expect from my theorized process. However, I'd prefer to report the results using mean high temperature since average temperature is misleadingly low (average temp of 70, for instance, is pretty warm with highs in the 80s but people think ""70 degrees? That's wonderful!""). So I estimate the same model but instead replace <code>mean_temp_group</code> with <code>mean_high_group</code>:</p>

<pre><code>event ~ age + sex + ... + mean_high_group
</code></pre>

<p>and the results don't match either my theory or what I saw with <code>mean_temp_group</code>. </p>

<p>That seems weird given how similar the two variables are. The average and average high variables have a correlation coefficient of 0.9939. In essence the average high is the average plus a constant (on average, 9.4 degrees with a standard deviation of 2.1). </p>

<p>At first I assumed this was a problem with the code, so I re-pulled the data (still have the same problem and the data extraction seems to be accurate). I also took the model with <code>mean_temp_group</code> and edited the formula in place to read <code>mean_high_group</code> lest I omitted/included a different variable between the models (I didn't). </p>

<p>I assume it has something to do with the binning or something along those lines - any ideas? I'm very confused by two variables that basically appear to be an additive shift of each other giving very different results. </p>
"
"0.146176336551172","0.14973818705887","229624","<p>I have been working to fit a normal distribution to data that is truncated to only be zero or greater. Given my data, which I have at the bottom, I previously used the following code: </p>

<pre><code>library(fitdistrplus)
library(truncnorm)
fitdist(testData, ""truncnorm"", start = list(a = 0, mean = 0.8, sd = 0.9))
</code></pre>

<p>Which, of course, won't work for a number of reasons, not least of which being that the mle estimator provides increasingly negative estimates as <code>a</code> tends towards zero. I previously got some very helpful information about fitting a normal distribution to this data <a href=""http://stackoverflow.com/questions/38838343/fitting-truncnorm-using-fitdistrplus/38839835#38839835"">here</a>, where two basic options were presented:</p>

<p>I might either use a low, negative value for <code>a</code>, and try out a number of different values</p>

<pre><code>fitdist(testData, ""truncnorm"", fix.arg=list(a=-.15),
        start = list(mean = mean(testData), sd = sd(testData)))
</code></pre>

<p>or I might set lower bounds for the parameters</p>

<pre><code>fitdist(testData, ""truncnorm"", fix.arg=list(a=0),
        start = list(mean = mean(testData), sd = sd(testData)),
        optim.method=""L-BFGS-B"", lower=c(0, 0))
</code></pre>

<p>Either way, though, it seems that some information is being lost - in the first case, because <code>a</code> isn't being truncated at zero, and in the second because the parameters have arbitrary lower bounds - I'm only concerned with achieving a good fit of the data, not with having positive a positive mean for the distribution. Given that mle estimators tend negative as <code>a</code> goes to zero, would it be better to use non-mle estimation? Does it make sense to have negative values of a if the data itself can't be negative?</p>

<p>This question applies more generally as well, as I have been using the <code>truncdist</code> package to try to fit Weibull, Log Normal, and Logistic distributions as well (for the Weibull, of course, there is no need to truncate at zero).</p>

<p>Finally, here's the data:</p>

<pre><code>testData &lt;- c(3.2725167726, 0.1501345235, 1.5784128343, 1.218953218, 1.1895520932, 
              2.659871271, 2.8200152609, 0.0497193249, 0.0430677458, 1.6035277181, 
              0.2003910167, 0.4982836845, 0.9867184303, 3.4082793339, 1.6083770189, 
              2.9140912221, 0.6486576911, 0.335227878, 0.5088426851, 2.0395797721, 
              1.5216239237, 2.6116576364, 0.1081283479, 0.4791143698, 0.6388625172, 
              0.261194346, 0.2300098384, 0.6421213993, 0.2671907741, 0.1388568942, 
              0.479645736, 0.0726750815, 0.2058983462, 1.0936704833, 0.2874115077, 
              0.1151566887, 0.0129750118, 0.152288794, 0.1508512023, 0.176000366, 
              0.2499423442, 0.8463027325, 0.0456045486, 0.7689214668, 0.9332181529, 
              0.0290242892, 0.0441181842, 0.0759601229, 0.0767983979, 0.1348839304
)
</code></pre>
"
"0.138675049056307","0.126270326349434","230567","<p>I've been looking at measures of variable importance for a random forest model - and was wondering if there are ways in which you can track how the <strong>variable importance shifts</strong> as the model is applied to datasets in the <strong>future</strong>.</p>

<p>The purpose of this question is to gain information as to whether a variable that was very important at model development is no longer that important when making predictions on future datasets either due to a change in population or a change in the variable itself (i.e. if this variable was somehow distorted and replaced with a column of missings it would no longer be important!).</p>

<p>Some example code  (sourced from : <a href=""http://stats.stackexchange.com/questions/197827/how-to-interpret-mean-decrease-in-accuracy-and-mean-decrease-gini-in-random-fore"">How to interpret Mean Decrease in Accuracy and Mean Decrease GINI in Random Forest models</a>) may help illustrate the problem:</p>

<pre><code>require(randomForest)
data(iris)
set.seed(1)
dat &lt;- iris
dat$Species &lt;- factor(ifelse(dat$Species=='virginica','virginica','other'))
model.rf &lt;- randomForest(Species~., dat, ntree=25,
importance=TRUE, nodesize=5)
model.rf
varImpPlot(model.rf)
</code></pre>

<p>It seems that the variable importance plot can only be created on the dataset in which the random forest model was trained on as the variable importance plot function can only be applied to a random forest object (which stays the same no matter what dataset it is trying to score in the future).</p>

<p>Is there a built-in way (in Python or R) to compute variable importance over time, or is it not possible for some reason? My understanding is that if a new dataset possessed an outcome flag it would be possible to compute the mean decrease in gini.</p>

<p>Edit: Would also like to slightly expand this question to discuss potential ways on measuring variable stability over time. For example, in a standard logistic scorecard one would compute a characteristic stability index using the pre-defined bins. However, as many random forests have continuous inputs the choice of bins isn't natural and potentially there should be an alternative method?</p>
"
"0.206724557648681","0.201173686196623","231066","<p>*EDIT: I ran test again with data set provided and realized that the cause of problem is definitely rank deficiency, because estimated values of parameters in nonlinear regression showed non existing p values and there was no way to create confidence intervals with this data. </p>

<h2>Thank you all for reading and help! This question is closed.</h2>

<p>I researched seed germination. I took 75 seed replicates and put them in  different ecological parameters (like temperature) and took data about sprouts in different time intervals. </p>

<p>Reading statistical science papers about this topic, I found that I should analyze my data in a time-to-event model (dose response curve), where I can use log-logistic regression or nonlinear regression (Ritz et al., 2013 -<a href=""http://dx.doi.org/10.1016/j.eja.2012.10.003"" rel=""nofollow"">http://dx.doi.org/10.1016/j.eja.2012.10.003</a>). </p>

<p>Two models (nonlinear and log-logistic) lead to quantitatively very similar fitted germination curves, i.e., similar parameter estimates, but qualitatively different statements about the precision of estimates. Nonlinear regression model yields an overly precise estimate of the proportion of seeds that germinated during the experiment, so the precision reported by the nonlinear regression is too high.</p>

<p>Similarly, the 95% confidence intervals of the fitted curves also demonstrate the dramatic difference in precision of the two models: Accurate prediction of germination percentages is not warranted by the data unless very low percentages are of interest.</p>

<p>Because of that I choose log-logistic regression as a model. First few data sets; treatments analyzed in R using analysis of Dose-Response Curves (drc package) went smooth, and I was able to plot and get final graph. Such data, which was successfully analyzed, contained treatments where max seeds germination was for example 50% of total seed number.</p>

<p>Example:</p>

<p><a href=""http://i.stack.imgur.com/yUqOu.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yUqOu.jpg"" alt=""Example of successful data analysis""></a></p>

<p>The problems arose when I entered the log-logistic model with treatment where all the seeds germinated in a short amount of time (meaning the treatment for this set of seeds is most adequate for their successful sprouting). For example, 100% of seeds germinated in only 5 days, so there are only two or three time intervals and a large number of sprouted seeds. The R program here reported  convergence error:</p>

<pre><code>Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
non-finite value supplied by optim
Error in drmOpt(opfct, opdfct1, startVecSc, optMethod, constrained, warnVal,  : 
Convergence failed 
</code></pre>

<p>Since I'm still a student in biology I have a very basic knowledge in statistics, so I tried to solve the problem with literature. </p>

<p>At first I thought that convergence failed because of perfect or complete separation, but through longer research it seems that the problem lies in rank deficiency. </p>

<p>When I analyzed the same data with nonlinear regression I've managed to fit curve and plot a graph without a problem.  </p>

<p>So, is there a way to make log-logistic model work even though I have obviously small data in cases of 100% germination? Should I switch to nonlinear regression  even though the reported precision would be too high. </p>
"
"0.206724557648681","0.190585597449432","232829","<p>I am using CT scans to classify lung cancer into one of two types (Adenocarcinoma vs. Squamous carcinoma; we can abbreviate them A &amp; B). I am applying LASSO penalized logistic regression to a data set containing 756 CT-derived texture features and 12 radiologist identified categorical (yes/no) features. The latter are based on literature for relevance whereas the former are computer generated with no prior proof of being useful. I have 107 cases so my final dataframe (df) is 107 x 768 dimensional:</p>

<p>i)Texture features (mathematical quantities n=756) are continuous variables scaled and centered. Their names are stored in list <code>â€˜texVarsâ€™</code></p>

<p>ii)Semantic features(Qualitative features subjective assessed by experienced radiologist, n=12). These are usually categorical binary inputs of yes / no type. Their names are stored in list <code>â€˜semVarsâ€™</code>.</p>

<p>Following comments from community on my original (very different) model <a href=""http://stats.stackexchange.com/questions/229884/is-my-high-dimensional-data-logistic-regression-workflow-correct"">Is my high dimensional data logistic regression workflow correct?</a>, I performed my LR development in three steps:</p>

<p>1)Feature selection: I used principle components analysis to reduce texture feature-space from 756 to 30. I kept 4 most relevant (from literature) semantic features. This gave me 34 final features. I used the following command:</p>

<pre><code>trans = preProcess(df[,texVars], method=c(""BoxCox"", ""center"",   ""scale"", ""pca""),thresh=.95)  # only column-names matching â€˜texVarsâ€™ are included.
neodf2 &lt;- predict(trans,df[,texVars]).
neodf.sem &lt;- neodf2[,c(""Tumour"",""AirBronchogram"", ""Cavity"", ""GroundglassComponent"",""Shape"")]  # this DF is 107 x 4 dimensional, containing only 4 semantic features (most relevant from prior knowledge).
neodf.tex &lt;- neodf2[,c(""Tumour"",setdiff(names(neodf2),names(neodf.sem)))] # this only has the 30 PCA vectors (labelled PC1 â€“ PC30).
</code></pre>

<p>2) Model development (LASSO) and penalty term tuning (10fold cross-validation) using cv.glmnet command  Deviance was used as determinant of model quality. Using this method, I developed a model incorporating only semantic features, a second model incorporating only texture features, and a third model incorporating both semantic and texture features. Here are the commands:</p>

<pre><code>#Converting to model.matrix for glmnet 
xall &lt;- model.matrix(Tumour~.,neodf2)[,-1]
xtex &lt;- model.matrix(Tumour~.,neodf.tex)[,-1]
xsem &lt;- model.matrix(Tumour~.,neodf.sem)[,-1]
y &lt;- neodf$Tumour
require(glmnet)
grid &lt;- 10^seq(10,-2,length=100)

lasso.all &lt;- cv.glmnet(xall,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"") 
lasso.tex &lt;- cv.glmnet(xtex,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
lasso.sem &lt;- cv.glmnet(xsem,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
</code></pre>

<p>3) Testing model classification accuracy on entire dataset. The following is the backbone of  bootstrap to generate 95% confidence intervals of predictive accuracy:</p>

<pre><code>pred &lt;- predict(lasso.all, newx = xall, s = ""lambda.min"", ""class"")
tabl &lt;- table(pred,y)
sum(diag(prop.table(tabl)))
</code></pre>

<p>4) As an alternative means to assess model performance than classification accuracy, I used ROC area under curve on entire dataset and compared AUROC curves from different models using DeLong's method (pROC package)</p>

<p>The results are interesting</p>

<pre><code> =================================================
</code></pre>

<p>LR MODEL BASED ON SEMANTIC FEATURES ALONE:
    lasso.sem$lambda.min
     0.01</p>

<p>Plot cv lambda vs. binomial devance <a href=""http://i.stack.imgur.com/6Modw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6Modw.png"" alt=""cv lambda vs binomia deviance""></a></p>

<pre><code>             Feature          Odds Ratio
1                (Intercept)  0.1292604
2      AirBronchogramPresent  0.1145378
3              CavityPresent 35.4350358
4 GroundglassComponentAbsent  4.3657928
5                 ShapeOvoid  2.4752881

AUC: .84


=================================================    
</code></pre>

<p>LR MODEL BASED ON TEXTURE FEATURES ALONE:</p>

<pre><code>lasso.tex$lambda.min
1e+10   
</code></pre>

<p>Plot  cv lambda vs binomial deviance (texture alone). Note how the 95% CI's are all overlapping! <a href=""http://i.stack.imgur.com/1Mk7M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1Mk7M.png"" alt="" cv lambda vs binomial deviance ""></a></p>

<pre><code>   Feature OddsRatio
1 (Intercept) 0.6461538



============================================================
</code></pre>

<p>LR MODEL BASED ON TEXTURE + SEMANTIC FEATURES:</p>

<pre><code>lasso.all$lambda.min
0.05 
</code></pre>

<p>Plot  cv lambda vs binomial deviance <a href=""http://i.stack.imgur.com/p1AHX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p1AHX.png"" alt=""cv lambda vs binomial deviance""></a></p>

<pre><code>                          Feature   OddsRatio
1                (Intercept)        0.3136489
2                       PC23        0.9404430
3                       PC27        0.8564001
4      AirBronchogramPresent        0.2691959
5              CavityPresent        6.7422427
6 GroundglassComponentAbsent        2.0514275
7                 ShapeOvoid        1.5974378

 AUC : .88
</code></pre>

<p>Plot showing loglambda vs coefficients. The dashed vertical line shows the cross-validated optimum lambda:<a href=""http://i.stack.imgur.com/d6FaO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d6FaO.jpg"" alt=""enter image description here""></a></p>

<p>Having rejected the texture only model, which only contains intercept, i was left with two models - semantic and combined texture+semantic. I created ROC curves for both and compared them using DeLong's method:</p>

<pre><code>pred.sem&lt;- predict(lasso.sem, newx = xsem, s = ""lambda.min"")
pred.all&lt;- predict(lasso.all, newx = xall, s = ""lambda.min"")

roc.sem&lt;- roc(y,as.numeric(pred.sem), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)    

roc.all&lt;- roc(y,as.numeric(pred.all), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)
</code></pre>

<p>Outputs of ROC analysis are:</p>

<pre><code>data:  roc.sem and roc.all
Z = -2.1212, p-value = 0.0339
alternative hypothesis: true difference in AUC is not equal to 0
sample estimates:
AUC of roc1 AUC of roc2 
  0.8369963   0.8809524 
</code></pre>

<p>Showing that combined model ROC curve is significantly better despite the modest improvement in AUC (83% vs 88%).
Questions are:</p>

<p>a) is my methodology airtight from a publication point of view now? Apologies in advance for any gross errors in my presentation of this problem.</p>

<p>b) what is the formal inference that texture model is intercept only.</p>

<p>c) if texture model is useless, how do its variables become useful once added to semantic features and yield a higher overall accuracy in the combined result? perhaps that means the effect of texture features alone is too small to be detected in this small dataset but becomes apparent when combined with a stronger predictor (i.e., semantic features). </p>

<p>Any further comments are welcome.</p>
"
"0.11322770341446","0.115986700954059","233366","<p>I am trying to use <code>lme4::glmer()</code> to fit a binomial GLMM with dependent variable that is not binary, but a continuous variable between zero and one. One can think of this variable as a probability; in fact it <em>is</em> probability as reported by human subjects (in an experiment that I help analyzing). The <code>glmer()</code> yields a model that is clearly off, and very far from the one I get with <code>glm()</code>, so something goes wrong. Why? What can I do? </p>

<hr>

<p><strong>More details</strong></p>

<p>Apparently it is possible to use logistic regression not only for binary DV but also for continuous DV between zero and one. Indeed, when I run </p>

<pre><code>glm(reportedProbability ~ a + b + c, myData, family=""binomial"")
</code></pre>

<p>I get a warning message</p>

<pre class=""lang-none prettyprint-override""><code>Warning message:
In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>but a very reasonable fit (all factors are categorical, so I can easily check whether model predictions are close to the across-subjects-means, and they are). </p>

<p>However, what I actually want to use is</p>

<pre><code>glmer(reportedProbability ~ a + b + c + (1 | subject), myData, family=""binomial"")
</code></pre>

<p>It gives me the identical warning, returns a model, but this model is clearly very much off; the estimates of the fixed effects are very far from the <code>glm()</code> ones and from the across-subject-means. (And I need to include <code>glmerControl(optimizer=""bobyqa"")</code> into the <code>glmer</code> call, otherwise it does not converge at all.)</p>
"
"0.0533760512683624","0.0820149827720712","234998","<p>For testing purposes I made up some correlated data in R like this:</p>

<pre><code>mydata = data.frame(
  outcome   = c(1, 0, 1, 0, 0, 1, 1, 0, 1, 1),
  predictor = c(0.1, -0.2, 0, 0.1, -0.3, 0.3, 0.2, -0.1, 0.1, 0.1)
)
</code></pre>

<p>Then I did this in order to create a logistic model that modeled this data:</p>

<pre><code>model1 = glm(family = binomial, formula = outcome ~ predictor, data = mydata)
</code></pre>

<p>Running <code>plot(model1)</code> yields the following plots:</p>

<p><a href=""http://i.stack.imgur.com/tbdpD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tbdpD.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/4O3jW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4O3jW.png"" alt=""enter image description here""></a></p>

<p>I need answers to some questions in order to understand how to perform diagnostics on such a logistic model. As someone with only an introductory course in statistics I'm having trouble gathering knowledge on how to interpret the plots.</p>

<ol>
<li>What do the ""Predicted Values"" in the first plot represent?</li>
<li>What does residual mean in the context of logistic regression?</li>
<li>Which of these plots can in any way be useful for model diagnostics based on real data? How?</li>
</ol>
"
"0.10336227882434","0.105880887471907","235402","<p>I am a newbie at R. I am trying to do some logistic regressions. My predictors are categorical, and most have more than two levels.</p>

<p>A couple questions:
1. It looks like R already creates the relevant contrasts for the categorical predictors (I am used to SAS where I need to specify all the contrasts). Is this correct for R? As in, I do <strong>not</strong> need to manually create the contrasts myself?</p>

<ol start=""2"">
<li><p>the ""family=binomial"" step in the glm syntax, will I always need to write this regardless of the number of predictors, and even if the categorical predictors have more than two levels (DV is always binary)?</p></li>
<li><p>For interpreting the results, if I use a categorical predictor (e.g., lettergroup) with different levels (a, b, c, d) as my predictor, with the binary DV as 1=yes and 0=no, if R created the lettergroupb contrast, and it is a significant, positive coefficient, this means b is more likely to say yes compared to a, c, and d? thank you!</p></li>
</ol>
"
