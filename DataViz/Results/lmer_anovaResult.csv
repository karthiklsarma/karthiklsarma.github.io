"V1","V2","V3","V4"
"0.0821994936526786","0.0823852554571635","  2290","<p>This is a follow-up to the <a href=""http://stats.stackexchange.com/questions/1818/how-to-determine-the-sample-size-needed-for-repeated-measurement-anova"">repeated measures sample size</a> question.</p>

<p>I am planning a repeated measures experiment. We record energy usage for 12 months, then give (a randomly assigned) half of the customers continuous information about their energy usage (perform the treatment), and record their energy usage for another 12 months. A similar study performed in the past showed a 5% reduction in energy usage.</p>

<p>I want to estimate the required sample size using $\alpha=0.05, \beta=0.1$. G*Power 3 has a tool for repeated measures power analysis. However, it requires two inputs that I am not entirely familiar with:</p>

<ul>
<li>$\lambda$ - the noncentrality parameter (How do I estimate this?)</li>
<li>$f$ - the effect size (I believe that this is the square root of Cohen's $f^2$)</li>
</ul>

<p>According to Wikipedia's effect size page:</p>

<blockquote>
  <p>Cohen's $f^2= {R^2_{AB} - R^2_A \over 1 - R^2_{AB}}$ where $R^2_A$ is the variance accounted for by a set of one or more independent variables $A$, and $R^2_{AB}$ is the combined variance accounted for by $A$ and another set of one or more independent variables $B$.</p>
</blockquote>

<p>However, my expected 5% change in energy consumption does not tell me how much variability will be explained. Is there any way to make this conversion?</p>

<p>If you know of a way to do this power analysis in R, I would love to hear it. I am planning to simulate some data and try using lmer from the lme4 package.</p>
"
"0.142373699362875","0.142695448246348","  3412","<p>I have an experiment that I'll try to abstract here.  Imagine I toss three white stones in front of you and ask you to make a judgment about their position.  I record a variety of properties of the stones and your response.   I do this over a number of subjects.  I generate two models.  One is that the nearest stone to you predicts your response, and the other is that the geometric center of the stones predicts your response.  So, using lmer in R I could write.</p>

<pre><code>mNear   &lt;- lmer(resp ~ nearest + (1|subject), REML = FALSE)
mCenter &lt;- lmer(resp ~ center  + (1|subject), REML = FALSE)
</code></pre>

<p><strong>UPDATE AND CHANGE - more direct version that incorporates several helpful comments</strong></p>

<p>I could try</p>

<pre><code>anova(mNear, mCenter)
</code></pre>

<p>Which is incorrect, of course, because they're not nested and I can't really compare them that way.  I was expecting anova.mer to throw an error but it didn't.  But the possible nesting that I could try here isn't natural and still leaves me with somewhat less analytical statements.  When models are nested naturally (e.g. quadratic on linear) the test is only one way.  But in this case what would it mean to have asymmetric findings?</p>

<p>For example, I could make a model three:</p>

<pre><code>mBoth &lt;- lmer(resp ~ center + nearest + (1|subject), REML = FALSE)
</code></pre>

<p>Then I can anova.</p>

<pre><code>anova(mCenter, mBoth)
anova(mNearest, mBoth)
</code></pre>

<p>This is fair to do and now I find that the center adds to the nearest effect (the second command) but BIC actually goes up when nearest is added to center (correction for the lower parsimony).  This confirms what was suspected.</p>

<p>But is finding this sufficient?  And is this fair when center and nearest are so highly correlated?</p>

<p>Is there a better way to analytically compare the models when it's not about adding and subtracting explanatory variables (degrees of freedom)?</p>
"
"0.0547996624351191","0.054923503638109","  3757","<p>In my data, the RT (gaze) of individuals (ID) is examined as a function of a visual conditions, the factor size (small, medium, large). 
Base model:</p>

<pre><code>print(Base &lt;- lmer(RT ~ Size + (1|ID), data=rt), cor=F)
</code></pre>

<p>Random effect:</p>

<pre><code>print(NoCor &lt;- lmer(RT ~ Size + (0+Size|ID) , data=rt))
print(WithCor &lt;- lmer(RT ~ Size + (1+Size|ID), data=rt))
</code></pre>

<p>Addition of ID slopes improves the Base model. My question is, how can a significant random effect (Size/ID) be interpreted when there is no relationship between the random and fixed effect, i.e., when the correlation between the random factor and the fixed facor does not improve the model [the anova(NoCr, WithCor) does not show a significant improvement]?</p>
"
"0.157400046933839","0.157755753708238","  3874","<p>I have data from patients treated with 2 different kinds of treatments during surgery.
I need to analyze its effect on heart rate. 
The heart rate measurement is taken every 15 minutes. </p>

<p>Given that the surgery length can be different for each patient, each patient can have between 7 and 10 heart rate measurements. 
So an unbalanced design should be used. 
I'm doing my analysis using R. And have been using the ez package to do repeated measure mixed effect ANOVA. But I do not know how to analyse unbalanced data. Can anyone help?</p>

<p>Suggestions on how to analyze the data are also welcomed.</p>

<p>Update:<br>
As suggested, I fitted the data using the <code>lmer</code> function and found that the best model is:</p>

<pre><code>heart.rate~ time + treatment + (1|id) + (0+time|id) + (0+treatment|time)
</code></pre>

<p>with the following result:</p>

<pre><code>Random effects:
 Groups   Name        Variance   Std.Dev. Corr   
 id       time        0.00037139 0.019271        
 id       (Intercept) 9.77814104 3.127002        
 time     treat0      0.09981062 0.315928        
          treat1      1.82667634 1.351546 -0.504 
 Residual             2.70163305 1.643665        
Number of obs: 378, groups: subj, 60; time, 9

Fixed effects:
             Estimate Std. Error t value
(Intercept) 72.786396   0.649285  112.10
time         0.040714   0.005378    7.57
treat1       2.209312   1.040471    2.12

Correlation of Fixed Effects:
       (Intr) time  
time   -0.302       
treat1 -0.575 -0.121
</code></pre>

<p>Now I'm lost at interpreting the result. 
Am I right in concluding that the two treatments differed in affecting heart rate? What does the correlation of -504 between treat0 and treat1 means?</p>
"
"0.0949157995752499","0.0951302988308988","  9324","<p>I'd love a check if anyone is willing!</p>

<p>I am trying to see if there is a statistical difference in female size between sites. Over the years females were repeatedly sampled within sites. I have sampled females opportunistically. Meaning that females were sampled a different number of times between and within sites.</p>

<p>My formula is:</p>

<pre><code>&gt; lmerfit1&lt;-lmer(size ~ (1|FEMALE), data=Data)
&gt; lmerfit2&lt;-lmer(size ~ SITE+(1|FEMALE), data=Data)
&gt; anova(lmerfit1, lmerfit2)
Data: Data
Models:
lmerfit1: size ~ (1 | FEMALE)
lmerfit2: size ~ SITE + (1 | FEMALE)
         Df    AIC    BIC  logLik Chisq Chi Df Pr(&gt;Chisq)
lmerfit1  3 2167.8 2179.6 -1080.9                        
lmerfit2  4 2169.8 2185.5 -1080.9     0      1          **1**
</code></pre>

<p>A p value of <strong>1</strong> leaves me concerned. The other female traits I ran thru this same formula made sense.</p>

<p>thanks! </p>
"
"0.0949157995752499","0.0951302988308988"," 10429","<p>I'm wondering how to fit multivariate linear mixed model and finding multivariate BLUP in R. I'd appreciate if someone come up with example and R code. Thanks</p>

<p><strong>Edit</strong></p>

<p>I wonder how to fit multivariate linear mixed model with <code>lme4</code>. I fitted univariate linear mixed models with the following code:</p>

<pre><code>library(lme4)
lmer.m1 &lt;- lmer(Y1~A*B+(1|Block)+(1|Block:A), data=Data)
summary(lmer.m1)
anova(lmer.m1)

lmer.m2 &lt;- lmer(Y2~A*B+(1|Block)+(1|Block:A), data=Data)
summary(lmer.m2)
anova(lmer.m2)
</code></pre>

<p>I'd like to know how to fit multivariate linear mixed model with <code>lme4</code>. The data is below:</p>

<pre><code>Block A B    Y1    Y2
 1 1 1 135.8 121.6
 1 1 2 149.4 142.5
 1 1 3 155.4 145.0
 1 2 1 105.9 106.6
 1 2 2 112.9 119.2
 1 2 3 121.6 126.7
 2 1 1 121.9 133.5
 2 1 2 136.5 146.1
 2 1 3 145.8 154.0
 2 2 1 102.1 116.0
 2 2 2 112.0 121.3
 2 2 3 114.6 137.3
 3 1 1 133.4 132.4
 3 1 2 139.1 141.8
 3 1 3 157.3 156.1
 3 2 1 101.2  89.0
 3 2 2 109.8 104.6
 3 2 3 111.0 107.7
 4 1 1 124.9 133.4
 4 1 2 140.3 147.7
 4 1 3 147.1 157.7
 4 2 1 110.5  99.1
 4 2 2 117.7 100.9
 4 2 3 129.5 116.2
</code></pre>

<p>Thank in advance for your time and cooperation.</p>
"
"0.143090951758036","0.157755753708238"," 14088","<p>I am trying to move from using the <code>ez</code> package to <code>lme</code> for repeated measures ANOVA (as I hope I will be able to use custom contrasts on with <code>lme</code>).</p>

<p>Following the advice from <a href=""http://blog.gribblelab.org/2009/03/09/repeated-measures-anova-using-r/"">this blog post</a> I was able to set up the same model using both <code>aov</code> (as does <code>ez</code>, when requested) and <code>lme</code>. However, whereas in the example given in <a href=""http://blog.gribblelab.org/2009/03/09/repeated-measures-anova-using-r/"">that post</a> the <em>F</em>-values do perfectly agree between <code>aov</code> and <code>lme</code> (I checked it, and they do), this is not the case for my data. Although the <em>F</em>-values are similar, they are not the same. </p>

<p><code>aov</code> returns a f-value of 1.3399, <code>lme</code> returns 1.36264. I am willing to accept the <code>aov</code> result as the ""correct"" one as this is also what SPSS returns (and this is what counts for my field/supervisor).</p>

<p>Questions:</p>

<ol>
<li><p>It would be great if someone could explain why this difference exists and how I can use <code>lme</code> to provide credible results. (I would also be willing to use <code>lmer</code> instead of <code>lme</code> for this type of stuff, if it gives the ""correct"" result. However, I haven't used it so far.)</p></li>
<li><p>After solving this problem I would like to run a contrast analysis. Especially I would be interested in the contrast of pooling the first two levels of factor (i.e., <code>c(""MP"", ""MT"")</code>) and compare this with the third level of factor (i.e., <code>""AC""</code>). Furthermore, testing the third versus the fourth level of factor (i.e., <code>""AC""</code> versus <code>""DA""</code>).</p></li>
</ol>

<p>Data:</p>

<pre><code>tau.base &lt;- structure(list(id = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 
22L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 1L, 2L, 3L, 4L, 
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 
19L, 20L, 21L, 22L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L), .Label = c(""A18K"", 
""D21C"", ""F25E"", ""G25D"", ""H05M"", ""H07A"", ""H08H"", ""H25C"", ""H28E"", 
""H30D"", ""J10G"", ""J22J"", ""K20U"", ""M09M"", ""P20E"", ""P26G"", ""P28G"", 
""R03C"", ""U21S"", ""W08A"", ""W15V"", ""W18R""), class = ""factor""), factor = structure(c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L), .Label = c(""MP"", ""MT"", ""AC"", ""DA""
), class = ""factor""), value = c(0.9648092876, 0.2128662077, 1, 
0.0607615485, 0.9912814024, 3.22e-08, 0.8073856412, 0.1465590332, 
0.9981672618, 1, 1, 1, 0.9794401938, 0.6102546108, 0.428651501, 
1, 0.1710644881, 1, 0.7639763913, 1, 0.5298989196, 1, 1, 0.7162733447, 
0.7871177434, 1, 1, 1, 0.8560509327, 0.3096989662, 1, 8.51e-08, 
0.3278862311, 0.0953598576, 1, 1.38e-08, 1.07e-08, 0.545290432, 
0.1305621416, 2.61e-08, 1, 0.9834051136, 0.8044114935, 0.7938839461, 
0.9910112678, 2.58e-08, 0.5762677121, 0.4750002288, 1e-08, 0.8584252623, 
1, 1, 0.6020385797, 8.51e-08, 0.7964935271, 0.2238374288, 0.263377904, 
1, 1.07e-08, 0.3160751898, 5.8e-08, 0.3460325565, 0.6842217296, 
1.01e-08, 0.9438301877, 0.5578367224, 2.18e-08, 1, 0.9161424562, 
0.2924856039, 1e-08, 0.8672987992, 0.9266688748, 0.8356425464, 
0.9988463913, 0.2960361777, 0.0285680426, 0.0969063841, 0.6947998266, 
0.0138254805, 1, 0.3494775301, 1, 2.61e-08, 1.52e-08, 0.5393467752, 
1, 0.9069223275)), .Names = c(""id"", ""factor"", ""value""), class = ""data.frame"", row.names = c(1L, 
6L, 10L, 13L, 16L, 17L, 18L, 22L, 23L, 24L, 27L, 29L, 31L, 33L, 
42L, 43L, 44L, 45L, 54L, 56L, 58L, 61L, 64L, 69L, 73L, 76L, 79L, 
80L, 81L, 85L, 86L, 87L, 90L, 92L, 94L, 96L, 105L, 106L, 107L, 
108L, 117L, 119L, 121L, 124L, 127L, 132L, 136L, 139L, 142L, 143L, 
144L, 148L, 149L, 150L, 153L, 155L, 157L, 159L, 168L, 169L, 170L, 
171L, 180L, 182L, 184L, 187L, 190L, 195L, 199L, 202L, 205L, 206L, 
207L, 211L, 212L, 213L, 216L, 218L, 220L, 222L, 231L, 232L, 233L, 
234L, 243L, 245L, 247L, 250L))
</code></pre>

<p>And the code:</p>

<pre><code>require(nlme)

summary(aov(value ~ factor+Error(id/factor), data = tau.base))

anova(lme(value ~ factor, data = tau.base, random = ~1|id))
</code></pre>
"
"0.157949437947797","0.158306386285393"," 24844","<p>I am running 3 models on 3 subsets of the same data.  The set up is as follows:</p>

<ol>
<li>Outcome (DV) is binary categorical</li>
<li>Time (IV) is repeated twice (pre and post)</li>
<li>Treatement (IV of interest) is binary categorical</li>
</ol>

<p>I am interested to know if at time 2 treatment has had an effect on outcome.  I used the lme4 package and used the following R code:</p>

<pre><code>tot.null&lt;-lmer(as.factor(outcome)~Time+(1|id), family=binomial(link='logit'),
             data=df.total)
tot.mod&lt;-lmer(as.factor(outcome)~trt*Time+(Time|id), 
             family=binomial(link='logit'), data=df.total)
anova(tot.null,tot.mod)
summary(tot.mod)
</code></pre>

<p><strong>Data head</strong></p>

<pre><code>   id             trt Time outcome
1   1 peer discussion   -1       1
2   2 peer discussion   -1       1
3   3 peer discussion   -1       0
4   4 peer discussion   -1       1
5   5 peer discussion   -1       1
</code></pre>

<p><strong>str of data</strong></p>

<pre><code>&gt; str(df.total)
'data.frame':   872 obs. of  4 variables:
 $ id     : int  1 2 3 4 5 6 7 8 9 10 ...
     $ trt    : Factor w/ 2 levels ""peer discussion"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Time   : num  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...
     $ outcome: num  1 1 1 1 1 1 1 0 1 0 ...
</code></pre>

<p>The problem is I get an error messoge on the <code>tot.mod</code>:</p>

<pre><code>&gt; tot.mod&lt;-glmer(as.factor(outcome)~trt*Time+(Time|id), family=binomial(link='logit'),
               data=df.total)
Warning message:
In mer_finalize(ans) : false convergence (8)
</code></pre>

<p>I think this is the reason the model is significant but none of the predictors are.  look at the inflated SEs.</p>

<p><strong>Comparison to the null model and the summary of full model</strong></p>

<pre><code>&gt; anova(tot.null,tot.mod)
Data: df.total
Models:
tot.null: as.factor(outcome) ~ Time + (1 | id)
tot.mod: as.factor(outcome) ~ trt * Time + (Time | id)
         Df    AIC    BIC  logLik  Chisq Chi Df            Pr(&gt;Chisq)    
tot.null  3 689.54 703.85 -341.77                                        
tot.mod   7 410.67 444.07 -198.34 286.86      4 &lt; 0.00000000000000022 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; summary(tot.mod)
Generalized linear mixed model fit by the Laplace approximation 
Formula: as.factor(outcome) ~ trt2 * Time + (Time | id) 
   Data: df.total 
   AIC   BIC logLik deviance
 410.7 444.1 -198.3    396.7
Random effects:
 Groups Name        Variance Std.Dev. Corr  
 id     (Intercept)  396.46  19.911         
        Time        1441.98  37.973   0.470 
Number of obs: 872, groups: id, 436

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) 10.09866    3.33921   3.024  0.00249 **
trt21        0.01792    5.10796   0.004  0.99720   
Time        -0.93753    5.79560  -0.162  0.87149   
trt21:Time  -0.84882   10.41073  -0.082  0.93502   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
           (Intr) trt21  Time  
trt21      -0.654              
Time        0.558 -0.365       
trt21:Time -0.311  0.473 -0.557
</code></pre>

<p>What's going on?  Why is the model significant but none of the betas?  In OLS I know this is an indicator of multi-colinearity among predictors.  I don't think that's the reason here.  Please help with understanding this problem as well as the error message (I think they may be connected).  What are some things I should check for?</p>

<p>The other two  models from the same data set (<code>split</code> on a different grouping variable) had no apparent problems.</p>

<p>Thank you in advance.</p>

<p><em>Using R 2.14.2, lme4 v. 0.999375-42 on a win 7 machine</em> </p>
"
"0.0821994936526786","0.0823852554571635"," 26810","<p>I ran a simple psychology experiment that included 4 conditions, each containing 8 blocks of training. There were different participants in each condition. Hence, condition and block and subject and block are crossed, but subject is nested in condition. I'm trying to do a basic repeated measures anova to test for effects of block and condition. I have an unbalanced design, and a mixed effects model. My approach was to use the lme4 package in conjunction with the car package. I am running R version 2.14 on mac os x lion.</p>

<p>Here is what I've done:</p>

<pre><code>library(nlme)
library(car)

rm( list = ls() )

data &lt;- read.table(""anova_data"", header = TRUE)

condition &lt;- factor(data$condition)
block &lt;- factor(data$block)
subject &lt;- factor(data$subject)
accuracy &lt;- data$accuracy

fm1 &lt;- lmer( accuracy ~ block*condition + (1|subject %in% condition) )

Anova(fm1)
</code></pre>

<p>My problem is that this returns a summary table without F values, like such:</p>

<pre><code>Analysis of Deviance Table (Type II tests)

Response: accuracy

                 Chisq Df Pr(&gt;Chisq)
     block           17.169  7    0.01634 *  

condition       68.294  3  9.897e-15 ***

block:condition 26.481 21    0.18869
</code></pre>

<p>Any help is greatly appreciated.</p>
"
"0.0671156055214024","0.0672672793996312"," 26855","<p>As the general consensus seems to be to use mixed-models via <code>lmer()</code> in R instead of classical ANOVA (for the often cited reasons, like unbalanced designs, crossed random effects etc.), I would like to give it a try with my data. However I am worried that I would be able to ""sell"" this approach to my supervisor (who is expecting classical analysis with a p-value in the end) or later to the reviewers.</p>

<p>Could you recommend some nice examples of published articles that used mixed-models or <code>lmer()</code> for different designs like repeated-measures or multiple within- and between-subject designs for the field biology, psychology, medicine?</p>
"
"0.0671156055214024","0.0672672793996312"," 26947","<p>Assume I have two factors A and B potentially predicting my outcome Y. Now I would like to test for fixed-effects using likelihood ratio test to find the best model.</p>

<pre><code>fm1 &lt;- lmer(Y~1+A*B+(1|subject))
fm2 &lt;- lmer(Y~1+A+B+(1|subject))
fm3 &lt;- lmer(Y~1+A+(1|subject))
fm4 &lt;- lmer(Y~1+B+(1|subject))
fm5 &lt;- lmer(Y~1+(1|subject))

anova(fm1, fm2, fm3, fm4, fm5)
</code></pre>

<p>However I would also like to test for different random-effect specifications.</p>

<pre><code># all possible random-effects specifications for fm1
fm6 &lt;- lmer(Y~1+A*B+(1+A*B|subject)) 
fm7 &lt;- lmer(Y~1+A*B+(1+A|subject)+(1+B|subject))
fm8 &lt;- lmer(Y~1+A*B+(1+A|subject))
fm8 &lt;- lmer(Y~1+A*B+(1+B|subject)) 
fm8 &lt;- lmer(Y~1+A*B+(1|subject)) 
</code></pre>

<p><strong>Is it a valid approach to first detect which fixed-effects specification is most predictive [<code>anova(fm1, fm2, fm3, fm4, fm5)</code>] and then to try different random-effects specification with this model? Or would only the most comprehensive approach be valid in which I specify all possible combinations of fixed- and random-effects to compare all these model to find the best fit?</strong></p>
"
"0.125561800583481","0.125845556426908"," 27945","<p>What is the meaning and effect of %in% in a model formula?</p>

<p>It is apparently used for nesting of one variable into another in a variety of analysis (manova, anova, regressions) in a few published articles.</p>

<p>From ?formula, b%in%a is a:b, so why use %in%?<br>
How is a:b nesting?</p>

<p>I am probably mistaken, but my understanding is that nesting b in a should not lead to the same mean square as the interaction of a and b denoted by a:b?</p>

<pre><code>library(lme4)  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>with(sleepstudy, Days%in%Subject)
  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ...  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fit&lt;-aov(data=sleepstudy, Reaction~Days + Days%in%Subject)
anova(fit)


               Df Sum Sq Mean Sq F value    Pr(&gt;F)    
 Days           1 162703  162703  193.23 &lt; 2.2e-16 ***
 Days:Subject  17 269685   15864   18.84 &lt; 2.2e-16 ***
 Residuals    161 135567     842
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
anova(fm1)


      Df Sum Sq Mean Sq F value
 Days  1  29986   29986  45.785
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction~Days + Days%in%Subject + (1|Subject), sleepstudy)
anova(fm1)

Analysis of Variance Table
             Df Sum Sq Mean Sq  F value
Days          1 162703  162703 248.4233
Days:Subject 17  73391    4317   6.5916
</code></pre>
"
"0.203433367801684","0.203893104812412"," 29617","<p><em>Note : this question is a repost, as my previous question had to be deleted for legal reasons.</em></p>

<hr>

<p>While comparing PROC MIXED from SAS with the function <code>lme</code> from the <code>nlme</code> package in R, I stumbled upon some rather confusing differences. More specifically, the degrees of freedom in the different tests differ between <code>PROC MIXED</code> and <code>lme</code>, and I wondered why.</p>

<p>Start from the following dataset (R code given below) :</p>

<ul>
<li>ind : factor indicating the individual where the measurement is taken</li>
<li>fac : organ where measurement is taken</li>
<li>trt : factor indicating the treatment</li>
<li>y : some continuous response variable</li>
</ul>

<p>The idea is to build the following simple models :</p>

<p><code>y ~ trt + (ind)</code> : <code>ind</code> as a random factor
<code>y ~ trt + (fac(ind))</code> : <code>fac</code> nested in <code>ind</code> as a random factor</p>

<p>Note that the last model should cause singularities, as there's only 1 value of <code>y</code> for every combination of <code>ind</code> and <code>fac</code>.</p>

<p><strong>First Model</strong></p>

<p>In SAS, I build the following model :</p>

<pre><code>PROC MIXED data=Data;
    CLASS ind fac trt;
    MODEL y = trt /s;
    RANDOM ind /s;
run;
</code></pre>

<p>According to tutorials, the same model in R using <code>nlme</code> should be : </p>

<pre><code>&gt; require(nlme)
&gt; options(contrasts=c(factor=""contr.SAS"",ordered=""contr.poly""))
&gt; m2&lt;-lme(y~trt,random=~1|ind,data=Data)
</code></pre>

<p>Both models give the same estimates for the coefficients and their SE, but when carrying out an F test for the effect of <code>trt</code>, they use a different amount of degrees of freedom :</p>

<pre><code>SAS : 
Type 3 Tests of Fixed Effects 
Effect Num DF Den DF     F  Value Pr &gt; F 
trt         1      8  0.89        0.3724 

R : 
&gt; anova(m2)
            numDF denDF  F-value p-value
(Intercept)     1     8 70.96836  &lt;.0001
trt             1     6  0.89272  0.3812
</code></pre>

<p><em>Question1:</em> What is the difference between both tests? Both are fitted using REML, and use the same contrasts.</p>

<p>NOTE: I tried different values for the DDFM= option (including BETWITHIN, which theoretically should give the same results as lme)</p>

<p><strong>Second Model</strong></p>

<p>In SAS : </p>

<pre><code>PROC MIXED data=Data;
    CLASS ind fac trt;
    MODEL y = trt /s;
    RANDOM fac(ind) /s;
run;
</code></pre>

<p>The equivalent model in R should be :</p>

<pre><code>&gt; m4&lt;-lme(y~trt,random=~1|ind/fac,data=Data)
</code></pre>

<p>In this case, there are some very odd differences :</p>

<ul>
<li>R fits without complaining, whereas SAS notes that the final hessian is not positive definite (which doesn't surprise me a bit, see above)</li>
<li>The SE on the coefficients differ (is smaller in SAS)</li>
<li>Again, the F test used a different amount of DF (in fact, in SAS that amount = 0)</li>
</ul>

<p>SAS output :</p>

<pre><code>Effect     trt Estimate Std Error  DF t Value Pr &gt; |t| 
Intercept        0.8863    0.1192  14    7.43 &lt;.0001 
trt       Cont  -0.1788    0.1686   0   -1.06 . 
</code></pre>

<p>R Output : </p>

<pre><code>&gt; summary(m4)
...
Fixed effects: y ~ trt 
               Value Std.Error DF   t-value p-value
(Intercept)  0.88625 0.1337743  8  6.624963  0.0002
trtCont     -0.17875 0.1891855  6 -0.944840  0.3812
...
</code></pre>

<p>(Note that in this case, the F and T test are equivalent and use the same DF.)</p>

<p>Interestingly, when using <code>lme4</code> in R the model doesn't even fit :</p>

<pre><code>&gt; require(lme4)
&gt; m4r &lt;- lmer(y~trt+(1|ind/fac),data=Data)
Error in function (fr, FL, start, REML, verbose)  : 
  Number of levels of a grouping factor for the random effects
must be less than the number of observations
</code></pre>

<p><em>Question 2</em>: What is the difference between these models with nested factors? Are they specified correctly and if so, how comes the results are so different?</p>

<hr>

<p>Simulated Data in R : </p>

<pre><code>Data &lt;- structure(list(y = c(1.05, 0.86, 1.02, 1.14, 0.68, 1.05, 0.22, 
1.07, 0.46, 0.65, 0.41, 0.82, 0.6, 0.49, 0.68, 1.55), ind = structure(c(1L, 
2L, 3L, 1L, 3L, 4L, 4L, 2L, 5L, 6L, 7L, 8L, 6L, 5L, 7L, 8L), .Label = c(""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8""), class = ""factor""), fac = structure(c(1L, 
1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L), .Label = c(""l"", 
""r""), class = ""factor""), trt = structure(c(2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c(""Cont"", 
""Treat""), class = ""factor"")), .Names = c(""y"", ""ind"", ""fac"", ""trt""
), row.names = c(NA, -16L), class = ""data.frame"")
</code></pre>

<p>Simulated Data :</p>

<pre><code>   y ind fac   trt
1.05   1   l Treat
0.86   2   l Treat
1.02   3   l Treat
1.14   1   r Treat
0.68   3   r Treat
1.05   4   l Treat
0.22   4   r Treat
1.07   2   r Treat
0.46   5   r  Cont
0.65   6   l  Cont
0.41   7   l  Cont
0.82   8   l  Cont
0.60   6   r  Cont
0.49   5   l  Cont
0.68   7   r  Cont
1.55   8   r  Cont
</code></pre>
"
"0.232495277487639","0.233020691214185"," 31118","<p>I performed an experiment where I raised different families coming from two different source populations, where each family was split up into a different treatments. After the experiment I measured several traits on each individual. 
To test for an effect of either treatment or source as well as their interaction, I used a linear mixed effect model with family as random factor, i.e.</p>

<pre><code>lme(fixed=Trait~Treatment*Source,random=~1|Family,method=""ML"")
</code></pre>

<p>so far so good,
Now I have to calculate the relative variance components, i.e. the percentage of variation that is explained by either treatment or source as well as the interaction.</p>

<p>Without a random effect, I could easily use the sums of squares (SS) to calculate the variance explained by each factor. But for a mixed model (with ML estimation), there are no SS, hence I thought I could use Treatment and Source as random effects too to estimate the variance, i.e.</p>

<pre><code>lme(fixed=Trait~1,random=~(Treatment*Source)|Family, method=""REML"")
</code></pre>

<p>However, in some cases, lme does not converge, hence I used lmer from the lme4 package:</p>

<pre><code>lmer(Trait~1+(Treatment*Source|Family),data=DATA)
</code></pre>

<p>Where I extract the variances from the model using the summary function:</p>

<pre><code>model&lt;-lmer(Trait~1+(Treatment*Source|Family),data=regrexpdat)
results&lt;-model@REmat
variances&lt;-results[,3]
</code></pre>

<p>I get the same values as with the VarCorr function. I use then these values to calculate the actual percentage of variation taking the sum as the total variation.</p>

<p>Where I am struggling is with the interpretation of the results from the initial lme model (with treatment and source as fixed effects) and the random model to estimate the variance components (with treatment and source as random effect). I find in most cases that the percentage of variance explained by each factor does not correspond to the significance of the fixed effect.</p>

<p>For example for the trait HD,
The initial lme suggests a tendency for the interaction as well as a significance for Treatment. Using a backward procedure, I find that Treatment has a close to significant tendency. However, estimating variance components, I find that Source has the highest variance, making up to 26.7% of the total variance.</p>

<p>The lme:</p>

<pre><code>anova(lme(fixed=HD~as.factor(Treatment)*as.factor(Source),random=~1|as.factor(Family),method=""ML"",data=test),type=""m"")
                                      numDF denDF  F-value p-value
(Intercept)                                1   426 0.044523  0.8330
as.factor(Treatment)                       1   426 5.935189  0.0153
as.factor(Source)                          1    11 0.042662  0.8401
as.factor(Treatment):as.factor(Source)     1   426 3.754112  0.0533
</code></pre>

<p>And the lmer:</p>

<pre><code>summary(lmer(HD~1+(as.factor(Treatment)*as.factor(Source)|Family),data=regrexpdat))
Linear mixed model fit by REML 
Formula: HD ~ 1 + (as.factor(Treatment) * as.factor(Source) | Family) 
   Data: regrexpdat 
    AIC    BIC logLik deviance REMLdev
 -103.5 -54.43  63.75   -132.5  -127.5
Random effects:
 Groups   Name                                      Variance  Std.Dev. Corr                 
 Family   (Intercept)                               0.0113276 0.106431                      
          as.factor(Treatment)                      0.0063710 0.079819  0.405               
          as.factor(Source)                         0.0235294 0.153393 -0.134 -0.157        
          as.factor(Treatment)L:as.factor(Source)   0.0076353 0.087380 -0.578 -0.589 -0.585 
 Residual                                           0.0394610 0.198648                      
Number of obs: 441, groups: Family, 13

Fixed effects:
            Estimate Std. Error t value
(Intercept) -0.02740    0.03237  -0.846
</code></pre>

<p>Hence my question is, is it correct what I am doing? Or should I use another way to estimate the amount of variance explained by each factor (i.e. Treatment, Source and their interaction). For example, would the effect sizes be a more appropriate way to go?</p>

<p>Thanks!</p>

<p>Kay Lucek</p>
"
"0.222807974258987","0.223311495746927"," 32040","<p>I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>

<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)
&gt; summary(fix1)

Call:
lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5835 -0.3543 -0.0024  0.3944  4.7294 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
Group1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***
Group2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***
Group3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***
Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** 
Group2:Year  0.0032369  0.0031098   1.041  0.29802    
Group3:Year  0.0006081  0.0032666   0.186  0.85235    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.7926 on 2981 degrees of freedom
Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 
F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>

<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>

<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)
&gt; summary(mix1a)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 4727 4775  -2356     4671    4711
Random effects:
 Groups     Name        Variance Std.Dev.
 Individual (Intercept) 0.39357  0.62735 
 Residual               0.24532  0.49530 
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.1010868   45.90
Group2       4.8094268  0.1158095   41.53
Group3       4.5607287  0.1216522   37.49
Group1:Year -0.0084165  0.0016963   -4.96
Group2:Year  0.0032369  0.0019433    1.67
Group3:Year  0.0006081  0.0020414    0.30

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.252  0.000  0.000              
Group2:Year  0.000 -0.252  0.000  0.000       
Group3:Year  0.000  0.000 -0.252  0.000  0.000
</code></pre>

<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>

<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 2941 3001  -1461     2885    2921
Random effects:
 Groups     Name        Variance  Std.Dev. Corr   
 Individual (Intercept) 0.1054790 0.324775        
            Year        0.0017447 0.041769 -0.246 
 Residual               0.1223920 0.349846        
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.0541746   85.64
Group2       4.8094268  0.0620648   77.49
Group3       4.5607287  0.0651960   69.95
Group1:Year -0.0084165  0.0065557   -1.28
Group2:Year  0.0032369  0.0075105    0.43
Group3:Year  0.0006081  0.0078894    0.08

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.285  0.000  0.000              
Group2:Year  0.000 -0.285  0.000  0.000       
Group3:Year  0.000  0.000 -0.285  0.000  0.000
</code></pre>

<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>

<p>How is this possible? I would expect that the random effect will ""eat"" the unexplained variability and increase ""sureness"" of the estimate!</p>

<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>

<h2>Edit</h2>

<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>

<pre><code>indivSlope = c()
for (indiv in 1:103) {
    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])
    indivSlope[indiv] = coef(mod1)['Year']
}

indivGroup = unique(mydata[,c(""Individual"", ""Group"")])[,""Group""]


anova1 = lm(indivSlope ~ 0 + indivGroup)
summary(anova1)

Call:
lm(formula = indivSlope ~ 0 + indivGroup)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.176288 -0.016502  0.004692  0.020316  0.153086 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
indivGroup1 -0.0084165  0.0065555  -1.284    0.202
indivGroup2  0.0032369  0.0075103   0.431    0.667
indivGroup3  0.0006081  0.0078892   0.077    0.939

Residual standard error: 0.04248 on 100 degrees of freedom
Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 
F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 
</code></pre>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>
"
"0.0474578997876249","0.0475651494154494"," 37577","<p>I have chemical compositions of individuals collected in 6 different sites in monthly intervals over a period of 6 months. </p>

<p>I would like to determine whether there is significant variation among sites as well as among months within sites. I was advised to use a repeated measures ANOVA to address this. However my design is unbalanced and has missing values. So, for example not all the sites are represented at all times and the number of individuals per site and time also varies. </p>

<p>Is it possible to do a repeated measures ANOVA with this dataset? Should I use the lmer function from the lme4 package?</p>
"
"0.116247638743819","0.116510345607093"," 37805","<p>I have a GLMM of the form: </p>

<pre><code>lmer(present? ~ factor1 + factor2 + continuous + factor1*continuous + 
                (1 | factor3), family=binomial)
</code></pre>

<p>When I use <code>drop1(model, test=""Chi"")</code>, I get different results than if I use <code>Anova(model, type=""III"")</code> from the car package or <code>summary(model)</code>. These latter two give the same answers. </p>

<p>Using a bunch of fabricated data, I have found that these two methods normally do not differ. They give the same answer for balanced linear models, unbalanced linear models (where unequal n in different groups), and for balanced generalised linear models, but not for balanced generalised linear mixed models. So it appears that only in cases where random factors are included does this discord manifest.</p>

<ul>
<li>Why is there a discrepancy between these two methods?  </li>
<li>When using GLMM should <code>Anova()</code> or <code>drop1()</code> be used?  </li>
<li>The difference between these two is rather slight, at least for my data. Does it even matter which is used?</li>
</ul>
"
"0.164398987305357","0.164770510914327"," 37944","<p>I am currently using the R package <a href=""http://cran.r-project.org/web/packages/lme4/lme4.pdf"">lme4</a>.</p>

<p>I am using a linear mixed effects models with random effects:</p>

<pre><code>library(lme4)
mod1 &lt;- lmer(r1 ~ (1 | site), data = sample_set) #Only random effects
mod2 &lt;- lmer(r1 ~ p1 + (1 | site), data = sample_set) #One fixed effect + 
            # random effects
mod3 &lt;- lmer(r1 ~ p1 + p2 + (1 | site), data = sample_set) #Two fixed effects + 
            # random effects
</code></pre>

<p>To compare models, I am using the <code>anova</code> function and looking at differences in AIC relative to the lowest AIC model:</p>

<pre><code>anova(mod1, mod2, mod3)
</code></pre>

<p>The above is fine for comparing models. </p>

<p>However, I also need some simple way to interpret goodness of fit measures for each model. Does anyone have experience with such measures? I have done some research, and there are journal papers on R squared for the fixed effects of mixed effects models:</p>

<ul>
<li>Cheng, J., Edwards, L. J., Maldonado-Molina, M. M., Komro, K. A., &amp; Muller, K. E. (2010). Real longitudinal data analysis for real people: Building a good enough mixed model. Statistics in Medicine, 29(4), 504-520. doi: 10.1002/sim.3775  </li>
<li>Edwards, L. J., Muller, K. E., Wolfinger, R. D., Qaqish, B. F., &amp; Schabenberger, O. (2008). An R2 statistic for fixed effects in the linear mixed model. Statistics in Medicine, 27(29), 6137-6157. doi: 10.1002/sim.3429  </li>
</ul>

<p>It seems however, that there is some criticism surrounding the use of measures such as those proposed in the above papers.</p>

<p>Could someone please suggest a few easy to interpret, goodness of fit measures that could apply to my models?  </p>
"
"0.142373699362875","0.142695448246348"," 41123","<p>I feel overwhelmed after attempting to dig into the literature on how to run my mixed model analysis following it up with using AIC to select the best model or models.  I do not think my data is that complicated, but I am looking for confirmation that what I have done is correct, and then advise on how to proceed.  I am unsure if I should be using lme or lmer and then with either of those, if I should be using REML or ML.</p>

<p>I have a value of selection and I want to know which covariates best influence that value and allow for predictions.  Here's some made up example data and my code for my test that I am working with:</p>

<pre><code>ID=as.character(rep(1:5,3))
season=c(""s"",""w"",""w"",""s"",""s"",""s"",""s"",""w"",""w"",""w"",""s"",""w"",""s"",""w"",""w"")
time=c(""n"",""d"",""d"",""n"",""d"",""d"",""n"",""n"",""n"",""n"",""n"",""n"",""d"",""d"",""d"")
repro=as.character(rep(1:3,5))
risk=runif(15, min=0, max=1.1)
comp1=rnorm(15, mean = 0, sd = 1)
mydata=data.frame(ID, season, time, repro, risk, comp1)
c1.mod1&lt;-lmer(comp1~1+(1|ID),REML=T,data=mydata)
c1.mod2&lt;-lmer(comp1~risk+(1|ID),REML=T,data=mydata)
c1.mod3&lt;-lmer(comp1~season+(1|ID),REML=T,data=mydata)
c1.mod4&lt;-lmer(comp1~repro+(1|ID),REML=T,data=mydata)
c1.mod5&lt;-lmer(comp1~time+(1|ID),REML=T,data=mydata)
c1.mod6&lt;-lmer(comp1~season+repro+time+(1|ID),REML=T,data=mydata)
c1.mod7&lt;-lmer(comp1~risk+season+season*time+(1|ID),REML=T,data=mydata)
</code></pre>

<p>I have ~19 models that explore this data with various combinations and up to a 2 way interaction terms, but always with ID as a random effect and comp1 as my dependent variable.  </p>

<ul>
<li>Q1. Which to use? lme or lmer? does it matter?</li>
</ul>

<p>In both of these, I have the option to use ML or REML - and I get drastically different answers - using ML followed by AIC I end up with 6 models all with similar AIC values and the model combinations simply do not make sense, whereas REML results in 2 of the most likely models being the best.  However, when running REML I cannot use anova any longer.  </p>

<ul>
<li>Q2. is the main reason to use ML over REML because of use with ANOVA?
This is not clear to me.</li>
</ul>

<p>I am still not able to run stepAIC or I do not know of another way to narrow down those 19 models.</p>

<ul>
<li>Q3. is there a way to use stepAIC at this point?</li>
</ul>
"
"0.100673408282104","0.100900919099447"," 41510","<p>How do you explain that ? There's only one operator but the mixed model returns an estimate for the <code>operator</code> random effect. Furthermore the <code>sample</code> effect is confounded with the interaction <code>sample:operator</code>. Below is the R code.</p>

<pre><code>&gt; dd
   sample operator         y
9      10      SCF 0.9153188
10     10      SCF 0.9884982
19    100      SCF 2.0798781
20    100      SCF 2.0464027
29   1000      SCF 3.0401590
30   1000      SCF 3.0114448
39  10000      SCF 4.1348324
40  10000      SCF 4.0840063
49  1e+05      SCF 5.1235795
50  1e+05      SCF 5.1106381
59  1e+06      SCF 6.0803404
60  1e+06      SCF 6.2353263
&gt; str(dd)
'data.frame':   12 obs. of  3 variables:
 $ sample  : Factor w/ 6 levels ""10"",""100"",""1000"",..: 1 1 2 2 3 3 4 4 5 5 ...
     $ operator: Factor w/ 1 level ""SCF"": 1 1 1 1 1 1 1 1 1 1 ...
 $ y       : num  0.915 0.988 2.08 2.046 3.04 ...
&gt; lmer(y ~ (1|sample)+(1|operator)+(1|sample:operator), data=dd) 
Linear mixed model fit by REML 
Formula: y ~ (1 | sample) + (1 | operator) + (1 | sample:operator) 
   Data: dd 
  AIC   BIC logLik deviance REMLdev
 18.6 21.03 -4.302    9.932   8.605
Random effects:
 Groups          Name        Variance   Std.Dev.
 sample:operator (Intercept) 1.87954740 1.370966
 sample          (Intercept) 1.87954925 1.370967
 operator        (Intercept) 0.00063096 0.025119
 Residual                    0.00283931 0.053285
Number of obs: 12, groups: sample:operator, 6; sample, 6; operator, 1

Fixed effects:
            Estimate Std. Error t value
(Intercept)   3.5709     0.7921   4.508
</code></pre>

<p>For those who are more familiar with SAS the corresponding code is:</p>

<pre><code>PROC MIXED DATA=dd;
CLASS sample operator;
MODEL y=;
RANDOM sample operator sample*operator;
RUN;
</code></pre>

<p>This is nothing but the crossed 2-way ANOVA with random effects.</p>
"
"0.0821994936526786","0.0823852554571635"," 45866","<p>So I have a data set which I am modeling using the following:</p>

<p><code>model1 &lt;- lm(subject ~ .^2, data=hre.train)</code></p>

<p>This model treats all of the variables as fixed.  I want to compare this model to a model which treats all variables as random. Would I use something like:</p>

<p><code>model2&lt;-lmer(subject~(1|town)+(1|district)+(1|street)+(1|family)+(1|gender)+(1|replicate),data=hre.train)</code></p>

<p>I can construct a fully nested model by using</p>

<p><code>model3&lt;-lme(subject~1,random=~1|town/district/street/family/gender/replicate,data=hre.train)</code></p>

<p>I just keep getting an error when I use</p>

<p><code>anova(model1,model2,model3)</code> </p>

<p>The goal is to build an optimal mixed model for prediction but I need something to compare to when I start combining fixed and random effects (i.e gender as fixed, others as random etc)  </p>
"
"0.177571201301144","0.177972492663322"," 47692","<p>I have a run an unbalanced 2x2x2x2 Type II ANOVA in R and am having trouble following up on the results. Here is the output:</p>

<pre><code>       Effect DFn DFd             F           p p&lt;.05             ges
2           cond   1 127  3.2359349424 0.074414031       0.0110769653158
3             sf   1 127  1.6981345415 0.194889782       0.0058436648717
5             ba   1 127  1.5404865586 0.216833055       0.0012264293759
9             tt   1 127  1.9253260611 0.167700584       0.0054448755666
4        cond:sf   1 127  0.1846599042 0.668127012       0.0006387833954
6        cond:ba   1 127  5.8799698820 0.016721105     * 0.0046651103251
7          sf:ba   1 127  1.4992638464 0.223051114       0.0011936498636
10       cond:tt   1 127  0.5266890712 0.469337439       0.0014954062256
11         sf:tt   1 127  0.0768302867 0.782090431       0.0002184199961
13         ba:tt   1 127 11.5004885802 0.000927851     * 0.0087996011237
8     cond:sf:ba   1 127  0.0042138896 0.948344162       0.0000033589171
12    cond:sf:tt   1 127  0.1197411309 0.729888009       0.0003403692520
14    cond:ba:tt   1 127  0.3878677814 0.534539033       0.0002993221940
15      sf:ba:tt   1 127  0.0001339682 0.990783282       0.0000001034158
16 cond:sf:ba:tt   1 127  0.4820119706 0.488780454       0.0003719473651
</code></pre>

<p><strong>cond</strong> and <strong>sf</strong> are between-subjects factors. <strong>ba</strong> and <strong>tt</strong> are within (or repeated). The unbalanced nature of the experiment has meant that I have used a type II anova.</p>

<p>You can see that we have a suggestively significant main effect of <strong>cond</strong> (2) and two significant interactions (6 &amp; 13). I have graphed the interactions and they seem logical, but I am sure that <strong>cond</strong> is contributing somewhat to the interactions.</p>

<p>I am at a loss at how to proceed. I suppose I wish to do some kind of post-hoc analysis concentrating on the interactions. I have investigated a number of different R packages (afex, phia, contrasts etc), but have yet to work out what I am actually doing with these interactions. </p>

<p>My data looks like this:</p>

<pre><code>str(xx)
'data.frame':   524 obs. of  6 variables:
$ ba  : Factor w/ 2 levels ""before"",""after"": 1 1 1 1 1 1 1 1 1 1 ...
    $ tt  : Factor w/ 2 levels ""targ"",""calm"": 1 1 1 1 1 1 1 1 1 1 ...
$ p   : Factor w/ 131 levels ""1"",""2"",""3"",""4"",..: 4 8 9 10 13 18 19 22 25 29 ...
    $ cond: Factor w/ 2 levels ""Control"",""Spider"": 1 1 1 1 1 1 1 1 1 1 ...
$ sf  : Factor w/ 2 levels ""Fear"",""No-Fear"": 1 1 1 1 1 1 1 1 1 1 ...
    $ eda : num  1.478 -0.56 -0.27 -0.902 -0.483 ...
</code></pre>

<p>Moreover consider <a href=""http://books.google.co.uk/books/about/Foundations_of_Behavioral_Statistics.html?id=8sLOa8vHl7YC"" rel=""nofollow"">Thompson (2006)</a> :</p>

<blockquote>
  <p>As noted by Rosnow and Rosenthal (1989a), the cell means â€œare the
  combined effects of the interaction, the row effects [a main effect],
  the column effects [a second main effect], and the grand meanâ€ (p.
  144). By the same token, simple post hoc tests of the cell means also
  do not yield insight about the origins of interaction effects,
  because the interaction effects are not uniquely a function of the
  cell means (Boik, 1979).</p>
</blockquote>

<p>So I guess post-hoc t-tests (with adjusted p-values) are out? </p>

<p><strong>Update:</strong> Following advice I have <em>found</em> from <a href=""http://stats.stackexchange.com/users/442/henrik"">@henrik</a> (<a href=""https://groups.google.com/forum/?fromgroups=#!topic/ez4r/RpwYT6pEva0"" rel=""nofollow"">here</a>) I have been investigating the phia package and the <strong>testInteractions</strong> function. I have been getting some results (for a type III anova - so not the type II I am after) but, again, I am way out of my depth here:</p>

<p>e.g., </p>

<pre><code>&gt; testInteractions(m2[[""lm""]], pairwise = ""ba"", ""cond"", idata = m2[[""idata""]],     adjustment = ""none"")              
Multivariate Test: Pillai test statistic
P-value adjustment method: none
                      Value Df test stat approx F num Df den Df  Pr(&gt;F)  
after-before : Control -0.31910  1  0.045305   6.0268      1    127 0.01544 *
after-before :  Spider  0.14243  1  0.008045   1.0300      1    127 0.31208  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p><strong>Latest Update:</strong></p>

<p>I seem to be getting further bogged down with this. Using lmer as referenced <a href=""http://www.uni-kiel.de/psychologie/rexrepos/Univariate/ANOVA/anovaMixed.html"" rel=""nofollow"">here</a> gives this:</p>

<pre><code>fit.1 &lt;-lmer(eda ~ 1 + cond * sf * ba *tt +(ba *tt | p), xx)
</code></pre>

<p>which leads to:</p>

<pre><code>&gt; anova(fit.1)
Analysis of Variance Table
          Df  Sum Sq Mean Sq F value
cond           1 0.00330 0.00330  0.0158
sf             1 0.04236 0.04236  0.2035
ba             1 0.07408 0.07408  0.3559
tt             1 0.34651 0.34651  1.6649
cond:sf        1 0.05633 0.05633  0.2707 
cond:ba        1 2.38017 2.38017 11.4359
sf:ba          1 0.45349 0.45349  2.1789
cond:tt        1 0.03832 0.03832  0.1841
sf:tt          1 0.03197 0.03197  0.1536
ba:tt          1 2.39403 2.39403 11.5025
cond:sf:ba     1 0.02595 0.02595  0.1247 
cond:sf:tt     1 0.00667 0.00667  0.0320
cond:ba:tt     1 0.08078 0.08078  0.3881
sf:ba:tt       1 0.00003 0.00003  0.0001
cond:sf:ba:tt  1 0.10034 0.10034  0.4821
</code></pre>

<p>Which seems to be confirming the earlier finding of a significant interaction between cond * ba and ba * tt (but is it type II?). Still not sure if any of this is correct.</p>

<p><strong>To clarify then: I am looking for advice about what to do next, in terms of understanding the significant interactions.</strong></p>
"
"0.106119089994502","0.106358907452879"," 49014","<p>I am trying to predict (binary) memory for pictures based on two continuous fixed effects: memorability and clutter. Using a mixed effects model, I found that both effects predict memory and including both in a model is better than either on their own.</p>

<p>However, memorability and clutter are highly correlated. </p>

<p>1) How do I interpret my result (that including both is better), given that they are correlated? Can they still be independent predictors, yet be correlated?</p>

<p>2) Is this a problem for the model in general? How do I deal with correlated fixed effects?</p>

<hr>

<p>In R:</p>

<pre><code>m_mem &lt;- lmer(memory ~ memorability + (1|subject), data=memDat, family='binomial')
m_clut &lt;- lmer(memory ~ clutter + (1|subject), data=memDat, family='binomial')
m_mem_clut &lt;- lmer(memory ~ memorability+clutter + (1|subject), data=memDat, family='binomial')
</code></pre>

<p><code>anova(m_mem_clut,m_mem)</code> and <code>anova(m_mem_clut,m_clut)</code> are significant.</p>

<p>[Edit: R <code>dput</code> can be found <a href=""http://bit.ly/14ATKAW"" rel=""nofollow"">here</a> ]</p>
"
"0.152203886829552","0.165260171758799"," 51489","<p>Let's say I have an experiment with three within-subject factors, A, B, &amp; C. The data looks like this.</p>

<pre><code> s  a  b  c
 1  1  1  1
 1  1  1  2
 1  1  2  1
 1  1  2  2
 1  2  1  1
 1  2  1  2
 1  2  2  1
 1  2  2  2
</code></pre>

<p>Simple enough. I have 49 subjects. Now, to do this ANOVA in R, I use</p>

<pre><code>m1 &lt;- aov(score ~ a*b*c + Error(subject/(a*b*c)), data)
summary(m1, type=3)
... (clipped) ...
Error: s:b:c
                  Df Sum Sq Mean Sq F value  Pr(&gt;F)   
b:c                1  4.608   4.608   8.121 0.00643 **
Residuals         48 27.236   0.567  
</code></pre>

<p>That looks fine, and matches SPSS's repeated measures GLM. All is well.</p>

<p>We can also do a mixed model in R using <code>lme4</code> and get the exact same results as this, as well as a mixed model done in JMP.</p>

<pre><code>m2 &lt;- lmer(score~a*b*c + (a*b*c|s), data)
library(car); Anova(m2, type=3, test.statistic""F"")
... (clipped) ...
                     F Df Df.res    Pr(&gt;F) 
b:c             8.1206  1 48.000  0.006430 ** 
</code></pre>

<p>I can do the same thing in SAS and get similar results. </p>

<pre><code>proc mixed data=mixedexample method=reml covtest;
    class a b c s;
    model score = a|b|c; 
    random intercept a|b|c/sub=s;
run;
</code></pre>

<p>I can do the ANOVA, by hand in Stata:</p>

<pre><code>anova score a / s|a ///
            b / s#b ///
            c / s#c ///
            a#b / s#a#b ///
            a#c / s#a#c /// 
            b#c / s#b#c /// 
            a#b#c 
</code></pre>

<p>I leave off the full interaction error term so it is the residual. Though, for some reason, the <code>df</code> of the main effect of <code>a</code> is twice the size of the others. But that's not the question I have. </p>

<p>My question is: how do I do the <code>lmer</code> and <code>proc mixed</code> version of the full LMM in Stata? The simple version is </p>

<pre><code>xtmixed score a##b##c || s:, reml
</code></pre>

<p>But how do I add the fully crossed error terms to <code>xtmixed</code> in the same way I do in <code>lmer</code> by adding <code>+ (a*b*c|s)</code>? The data is balanced with no missing values, so the LMM should be the same as the repeated-measures ANOVA, right? Why can't I do this in Stata? </p>

<p>I barely know the basics of LMMs, but this is a Stata question. I'm just trying to figure out all the different ways of performing these two models. Also, if anyone knows a simpler way of doing the univariate ANOVA in Stata without specifying every single error term by hand? This may not even be the ""right"" way of doing this procedure, but because I get the same output everywhere else, how do I get Stata to do the same thing as R's <code>lmer</code>, SAS's <code>proc mixed</code>, and JMP?</p>
"
"0.117452309662454","0.117717738949355"," 53427","<p>Lets take as an example a repeated measures design with 10 subjects that are all reading the same letter strings and pressing a button as soon as they determine whether the string is valid English word, producing reaction times (RT). I wish to determine whether word length has a significant effect on the produced RT (it should), using a linear mixed effect model. Using R and the lme4 package I construct the following model:</p>

<pre><code>m = lmer(RT ~ 1 + word.length + (1 + word.length|subject), data=rt.data)
</code></pre>

<p>As you can see, I allow both the intercept and the slope to vary randomly across subjects, as I suspect that the effect of word length might be larger for slow readers than fast readers. </p>

<p>Understanding that p-values are not trivial in these types of models, my approach is to construct a NULL model, containing only the random effects. But I'm not sure whether this should be:</p>

<pre><code>m.null = lmer(RT ~ (1 + word.length|subject), data=rt.data)
</code></pre>

<p>or:</p>

<pre><code>m.null = lmer(RT ~ (1 | subject), data=rt.data)
</code></pre>

<p>In the end, I wish to perform an anova between the model with word length and the NULL model like so:</p>

<pre><code>anova(m, m.null)
</code></pre>

<p>which should give me a p-value whether the addition of word length actually makes the model fit better and thus whether word length actually influences the RT.</p>
"
"0.116247638743819","0.116510345607093"," 56380","<p>The <code>lme4</code> package in R includes the <code>cake</code> dataset. </p>

<pre><code>library(lme4)
head(cake[,2:4], 20)
   recipe temperature angle
1       A         175    42
2       A         185    46
3       A         195    47
4       A         205    39
5       A         215    53
6       A         225    42
7       B         175    39
8       B         185    46
9       B         195    51
10      B         205    49
11      B         215    55
12      B         225    42
13      C         175    46
14      C         185    44
15      C         195    45
16      C         205    46
17      C         215    48
18      C         225    63
19      A         175    47
20      A         185    29
</code></pre>

<p>I've analysed the <code>cake</code> dataset using two different models below. The first model is a 2 factor ANOVA:</p>

<pre><code>summary(aov(angle ~ temperature + recipe, cake))
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
temperature   5   2100   420.1   6.918 4.37e-06 ***
recipe        2    135    67.5   1.112     0.33    
Residuals   262  15908    60.7                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...and the second is a mixed effects model, with <code>temperature</code> as a random effect:</p>

<pre><code>lmer(angle ~ recipe + (1| temperature), data=cake, REML=F)
Linear mixed model fit by maximum likelihood 
Formula: angle ~ recipe + (1 | temperature) 
   Data: cake 
  AIC  BIC logLik deviance REMLdev
 1893 1911 -941.7     1883    1877
Random effects:
 Groups      Name        Variance Std.Dev.
 temperature (Intercept)  6.4399  2.5377  
 Residual                60.2560  7.7625  
Number of obs: 270, groups: temperature, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)   33.122      1.320  25.093
recipeB       -1.478      1.157  -1.277
recipeC       -1.522      1.157  -1.315

Correlation of Fixed Effects:
        (Intr) recipB
recipeB -0.438       
recipeC -0.438  0.500
</code></pre>

<p>Is someone able to provide a summary of what the mixed effect model has done differently to the ANOVA?</p>
"
"0.171111891110113","0.171498585142509"," 57882","<p>My data is a series of repeated measures in time (14 measures). I am trying to model the variable <code>HbA1c</code> which is a blood test performed at each visit to measure the global blood glucose level. Other predictors are recorded at each visit are BMI, age, insulin dose, and insulin type, etc....</p>

<p>By looking at the data in facetted (per Patient ID) scatter plots ($Y$: parameters, i.e. BMI, age, insulin dose and insulin type; $X$: Visit NR), I see different intercepts and slopes for the included parameters in the model.</p>

<p>So is this the model I am looking for?</p>

<pre><code>HbA1cfit &lt;- lmer(HbA1c ~ VisitNR + BMI + Age + Insulindose + ... + 
    (1|PatientId) + (BMI|PatientId) + (Age|PatientId) + (...|PatientId),
    data=type2diabetes ,REML=F)
</code></pre>

<p>The plot of the fitted values vs. the residuals shows a normal distribution with no clear trend. The <code>anova(HbA1cfit1, HbA1cfit2 [or more, each time model + extra parameter])</code> is significantly better every time.</p>

<p>The scatter plots <code>(y=HbA1cRAW, x=visitNR)</code> and <code>(y=fitted(HbA1cfit), x=visitNR)</code> look very similar to me.</p>

<p>One more question: how should I order the parameters in the hierarchy of significance in their determining power towards <code>HbA1c</code>? </p>

<p>Let me rephrase the whole problem:</p>

<p>Data set:</p>

<p>PatientID Sex VisitNr Age Insulindose Insulintype C-peptide PO-drugs HbA1c<br>
1         f   1       35  50          2           1.5       1        65<br>
1         f   2       36  55          2           1.6       1        66<br>
...<br>
1         f   14      42  60          3           0.2       2        70  </p>

<p>2         m   1       60  50          4           2.5       2        80<br>
...<br>
2         m   14      67  40          4           1.3       3        75  </p>

<p>...  </p>

<p>485       m       1       50   20           3            2.5        2         50<br>
...<br>
485       m       14      57   30           3            2.5        3         55       </p>

<p>So data for 485 patients, most of them for 14 visits, if not completed: marked af missing data. 
Variables :
PatientId : 1 to 485 
Sex : categorical : f or m
Visit nr : 1 to 14
Age : continuous integer, visits are typical 6 months appart , so age goes up with about 7 years from visit 1 to visit 14
Insulinedose: continuous integer
Insulintype: factorial ordened : 1 = 1xlongacting , 2 = 2xmix , 3 = 3xmix, 4 = basal-bolus
C-peptide: continuous double<br>
PO-drugs : factorial ordened : 1 = none, 2 = 1 drug, 3 = 2 drugs, 4= 3 drugs
HbA1c : continuous integer</p>

<p>Questions to solve:</p>

<p>which factors determine the outcome : HbA1c ?
Time ? (as visit Nr ?) , Age, Others ???
In what order : most to least to not significant ?
Is HbA1c going up or down in time significantly ?</p>

<p>Since there are missing values all over the database and since I am dealing clearly with repeated measures, I tought it might be adressed by lmer in R.</p>

<p>Everything seems to be nested in PtientId , only F or m stays the same in all visits, the other values can all change and do clearly not group the data.</p>

<p>My effort to model this (if appropriate at all) is on top of this post.</p>

<p>Please help , the more I read about lmer the more I get confused.</p>

<p>Jan </p>
"
"0.171111891110113","0.171498585142509"," 58020","<p>I'm struggling with modeling some experimental data using the <code>lme4</code> package in R, and would appreciate input.</p>

<p>My experimental design is as follows:  subjects entering the experiment answer a screener question, which is used to randomly assign them to a between-subjects condition (variable=<code>rank</code>, which has 2-levels (0/1)). They then complete a distractor task, and make two choices (<code>choice</code> being the within-subjects dependent variable).  At each choice, the stimuli are RANDOMLY ASSIGNED and crossed by two factors (<code>msg</code> has 3-levels (""norm"", ""no norm"" and ""provincial""), and <code>cost</code> has 2-levels (0/1)).   <strong>Because participants were randomly assigned to both <code>cost</code> and <code>msg</code> at two points in time, theoretically they could have the same combination of <code>cost</code> and <code>msg</code> at both points in time.</strong></p>

<p>My life would be easiest if I could use a wonderful package such as <code>ezANOVA</code>, but my data won't allow me to do this because every individual doesn't have EVERY combination of the two within-subjects variables <a href=""http://stats.stackexchange.com/questions/57709/error-in-ezanova-with-balanced-dataset-with-no-missing-data"">see here</a>.  So, I'm in the less-familiar territory of mixed models.</p>

<p>My hypothesis argues that there should be a three-way interaction between <code>rank</code>, <code>msg</code>, and <code>cost</code>.  Thus, a simple version of my model might be:</p>

<pre><code>m1 &lt;- glmer(choice ~ msg*cost*rank + (1|id), data=df, family=""binomial"")
</code></pre>

<p>But, I've also seen it suggested <a href=""http://stats.stackexchange.com/questions/46321/how-to-deal-with-repeated-measurements-in-the-same-condition-of-a-factorial-expe"">on this site</a> that my random effects should be modeled as follows in order to evaluate the interaction between the factor and subjects:</p>

<pre><code>m1 = lmer(choice ~ msg*cost*rank + (rank|id) + (cost|id), data=df)
</code></pre>

<p>The latter model has me straying into unfamilar territory, so I'd appreciate any advice about how to model this data in a way that is (1) simple, but (2) appropriate.</p>

<p>Sample data below:</p>

<pre><code>&gt; dput(df[1:700,2:6])
structure(list(time = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), choice = c(1, 1, 
1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 
1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 
1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 
0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 
0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 
1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 
1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 
1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 
1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 
1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 
1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 
1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 
1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 
1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 
0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 
1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 
0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 
1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 
1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 
0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 
1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 
1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 
1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 
0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 
1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 
1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 
1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 
1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 
1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 
1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 
1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 
0, 1, 0, 1, 0), msg = structure(c(3L, 1L, 1L, 2L, 3L, 1L, 3L, 
3L, 3L, 2L, 2L, 3L, 2L, 3L, 2L, 3L, 1L, 3L, 2L, 3L, 1L, 1L, 3L, 
1L, 2L, 3L, 3L, 2L, 2L, 1L, 1L, 1L, 1L, 3L, 2L, 3L, 3L, 2L, 3L, 
3L, 1L, 3L, 1L, 2L, 2L, 3L, 2L, 3L, 3L, 2L, 3L, 3L, 1L, 2L, 3L, 
3L, 1L, 2L, 3L, 2L, 3L, 3L, 1L, 1L, 3L, 1L, 3L, 2L, 1L, 3L, 2L, 
3L, 3L, 2L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 2L, 2L, 2L, 2L, 2L, 1L, 
3L, 2L, 3L, 2L, 3L, 3L, 3L, 1L, 1L, 3L, 1L, 2L, 2L, 3L, 2L, 3L, 
3L, 2L, 2L, 3L, 2L, 1L, 2L, 1L, 3L, 2L, 2L, 1L, 3L, 3L, 2L, 3L, 
3L, 3L, 2L, 2L, 1L, 2L, 3L, 2L, 2L, 2L, 3L, 3L, 3L, 1L, 2L, 2L, 
1L, 1L, 3L, 1L, 3L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 
2L, 1L, 2L, 1L, 1L, 3L, 3L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 3L, 
3L, 1L, 2L, 3L, 1L, 1L, 3L, 2L, 2L, 3L, 3L, 1L, 1L, 1L, 2L, 1L, 
2L, 3L, 3L, 2L, 1L, 2L, 3L, 1L, 2L, 2L, 1L, 3L, 3L, 1L, 1L, 1L, 
3L, 2L, 3L, 1L, 2L, 2L, 3L, 2L, 1L, 3L, 1L, 2L, 2L, 3L, 3L, 2L, 
1L, 3L, 3L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 3L, 2L, 2L, 1L, 1L, 1L, 
1L, 1L, 3L, 2L, 2L, 1L, 2L, 2L, 3L, 1L, 1L, 2L, 3L, 2L, 3L, 3L, 
3L, 3L, 1L, 3L, 2L, 1L, 2L, 3L, 1L, 1L, 2L, 3L, 3L, 2L, 1L, 1L, 
2L, 1L, 2L, 3L, 3L, 3L, 1L, 2L, 2L, 3L, 1L, 3L, 1L, 3L, 3L, 1L, 
1L, 3L, 1L, 3L, 1L, 1L, 3L, 1L, 1L, 2L, 2L, 3L, 2L, 3L, 2L, 3L, 
1L, 2L, 2L, 1L, 2L, 3L, 3L, 3L, 2L, 3L, 1L, 3L, 1L, 2L, 3L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 3L, 2L, 3L, 2L, 1L, 3L, 3L, 
3L, 1L, 3L, 3L, 1L, 3L, 1L, 3L, 2L, 1L, 3L, 1L, 1L, 2L, 2L, 3L, 
1L, 3L, 3L, 3L, 1L, 3L, 1L, 3L, 2L, 1L, 2L, 1L, 1L, 2L, 3L, 3L, 
3L, 3L, 3L, 2L, 2L, 3L, 2L, 1L, 1L, 2L, 1L, 2L, 3L, 3L, 1L, 1L, 
1L, 3L, 3L, 1L, 1L, 3L, 1L, 2L, 3L, 1L, 1L, 3L, 2L, 1L, 3L, 3L, 
3L, 1L, 3L, 1L, 3L, 1L, 3L, 2L, 3L, 2L, 2L, 1L, 2L, 1L, 3L, 2L, 
2L, 3L, 1L, 2L, 1L, 3L, 3L, 3L, 3L, 2L, 2L, 3L, 3L, 3L, 1L, 1L, 
1L, 1L, 3L, 1L, 3L, 1L, 1L, 1L, 3L, 3L, 2L, 1L, 2L, 3L, 2L, 3L, 
2L, 3L, 1L, 3L, 1L, 3L, 1L, 1L, 3L, 1L, 3L, 2L, 3L, 2L, 1L, 1L, 
1L, 2L, 2L, 1L, 2L, 1L, 3L, 1L, 2L, 2L, 2L, 1L, 1L, 3L, 3L, 1L, 
1L, 3L, 1L, 3L, 3L, 3L, 3L, 2L, 1L, 1L, 3L, 2L, 1L, 3L, 2L, 2L, 
1L, 1L, 3L, 3L, 3L, 1L, 1L, 2L, 1L, 1L, 3L, 1L, 2L, 2L, 3L, 1L, 
2L, 2L, 3L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 3L, 1L, 3L, 
1L, 1L, 3L, 3L, 3L, 2L, 3L, 2L, 2L, 3L, 1L, 2L, 3L, 3L, 2L, 2L, 
1L, 2L, 3L, 2L, 1L, 2L, 3L, 3L, 2L, 2L, 2L, 3L, 1L, 2L, 3L, 2L, 
3L, 3L, 3L, 3L, 1L, 2L, 3L, 2L, 3L, 3L, 3L, 2L, 3L, 1L, 1L, 2L, 
2L, 3L, 3L, 1L, 1L, 3L, 1L, 1L, 1L, 2L, 3L, 2L, 3L, 1L, 2L, 3L, 
2L, 2L, 3L, 3L, 3L, 1L, 3L, 2L, 2L, 1L, 2L, 3L, 1L, 3L, 2L, 3L, 
1L, 2L, 3L, 2L, 1L, 2L, 3L, 3L, 3L, 3L, 2L, 3L, 2L, 3L, 3L, 2L, 
1L, 2L, 3L, 2L, 1L, 2L, 1L, 3L, 1L, 3L, 1L, 1L, 2L, 1L, 3L, 1L, 
2L, 2L, 3L, 2L, 2L, 3L, 3L, 1L, 3L, 1L, 3L, 2L, 1L, 3L, 1L, 2L, 
1L, 3L, 1L, 3L, 1L, 1L, 2L, 1L, 3L, 2L, 3L, 3L, 3L, 1L, 3L, 3L, 
2L, 2L, 1L, 2L, 2L, 2L, 2L, 3L, 1L, 1L, 1L, 2L, 3L, 2L, 3L, 1L, 
3L, 3L, 3L, 1L, 3L, 3L, 2L, 2L, 3L, 2L, 2L, 3L, 2L, 1L, 2L, 3L, 
2L, 2L, 1L, 3L, 2L), .Label = c(""No norm"", ""Norm"", ""Provincial""
), class = ""factor""), cost = structure(c(1L, 2L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 
2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 
2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 
1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 
2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 
1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 
2L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 
1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 
2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 
1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 
2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 
1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 
1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 
1L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 
2L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 
1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 
1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 
2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 
1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 
1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 
2L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 
1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 
1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 
2L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 
1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 
2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 
1L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 
2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 
2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 
1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 
1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 
2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 
1L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 
2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 
1L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 
1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 
2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 
1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L), .Label = c(""0"", ""1""), class = ""factor""), 
    rank = structure(c(1L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 
    2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 
    2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 
    1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
    1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 
    1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
    1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 
    2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 
    2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 
    2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 
    2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 
    2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
    2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 
    2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 
    2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 
    1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 
    1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 
    2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 
    2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 
    1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 
    2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 
    1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 
    1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 
    2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 
    1L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 
    2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 
    2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 
    2L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 
    2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 
    2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 
    2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 
    1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 
    1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 
    1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 
    2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 
    1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 
    1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 
    2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 
    2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 
    1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 
    2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 
    1L), .Label = c(""0"", ""1""), class = ""factor"")), .Names = c(""time"", 
""choice"", ""msg"", ""cost"", ""rank""), row.names = c(""1.1"", 
""2.1"", ""3.1"", ""4.1"", ""5.1"", ""6.1"", ""7.1"", ""8.1"", ""9.1"", ""10.1"", 
""11.1"", ""12.1"", ""13.1"", ""14.1"", ""15.1"", ""16.1"", ""17.1"", ""18.1"", 
""19.1"", ""20.1"", ""21.1"", ""22.1"", ""23.1"", ""24.1"", ""25.1"", ""26.1"", 
""27.1"", ""28.1"", ""29.1"", ""30.1"", ""31.1"", ""32.1"", ""33.1"", ""34.1"", 
""35.1"", ""36.1"", ""37.1"", ""38.1"", ""39.1"", ""40.1"", ""41.1"", ""42.1"", 
""43.1"", ""44.1"", ""45.1"", ""46.1"", ""47.1"", ""48.1"", ""49.1"", ""50.1"", 
""51.1"", ""52.1"", ""53.1"", ""54.1"", ""55.1"", ""56.1"", ""57.1"", ""58.1"", 
""59.1"", ""60.1"", ""61.1"", ""62.1"", ""63.1"", ""64.1"", ""65.1"", ""66.1"", 
""67.1"", ""68.1"", ""69.1"", ""70.1"", ""71.1"", ""72.1"", ""73.1"", ""74.1"", 
""75.1"", ""76.1"", ""77.1"", ""78.1"", ""79.1"", ""80.1"", ""81.1"", ""82.1"", 
""83.1"", ""84.1"", ""85.1"", ""86.1"", ""87.1"", ""88.1"", ""89.1"", ""90.1"", 
""91.1"", ""92.1"", ""93.1"", ""94.1"", ""95.1"", ""96.1"", ""97.1"", ""98.1"", 
""99.1"", ""100.1"", ""101.1"", ""102.1"", ""103.1"", ""104.1"", ""105.1"", 
""106.1"", ""107.1"", ""108.1"", ""109.1"", ""110.1"", ""111.1"", ""112.1"", 
""113.1"", ""114.1"", ""115.1"", ""116.1"", ""117.1"", ""118.1"", ""119.1"", 
""120.1"", ""121.1"", ""122.1"", ""123.1"", ""124.1"", ""125.1"", ""126.1"", 
""127.1"", ""128.1"", ""129.1"", ""130.1"", ""131.1"", ""132.1"", ""133.1"", 
""134.1"", ""135.1"", ""136.1"", ""137.1"", ""138.1"", ""139.1"", ""140.1"", 
""141.1"", ""142.1"", ""143.1"", ""144.1"", ""145.1"", ""146.1"", ""147.1"", 
""148.1"", ""149.1"", ""150.1"", ""151.1"", ""152.1"", ""153.1"", ""154.1"", 
""155.1"", ""156.1"", ""157.1"", ""158.1"", ""159.1"", ""160.1"", ""161.1"", 
""162.1"", ""163.1"", ""164.1"", ""165.1"", ""166.1"", ""167.1"", ""168.1"", 
""169.1"", ""170.1"", ""171.1"", ""172.1"", ""173.1"", ""174.1"", ""175.1"", 
""176.1"", ""177.1"", ""178.1"", ""179.1"", ""180.1"", ""181.1"", ""182.1"", 
""183.1"", ""184.1"", ""185.1"", ""186.1"", ""187.1"", ""188.1"", ""189.1"", 
""190.1"", ""191.1"", ""192.1"", ""193.1"", ""194.1"", ""195.1"", ""196.1"", 
""197.1"", ""198.1"", ""199.1"", ""200.1"", ""201.1"", ""202.1"", ""203.1"", 
""204.1"", ""205.1"", ""206.1"", ""207.1"", ""208.1"", ""209.1"", ""210.1"", 
""211.1"", ""212.1"", ""213.1"", ""214.1"", ""215.1"", ""216.1"", ""217.1"", 
""218.1"", ""219.1"", ""220.1"", ""221.1"", ""222.1"", ""223.1"", ""224.1"", 
""225.1"", ""226.1"", ""227.1"", ""228.1"", ""229.1"", ""230.1"", ""231.1"", 
""232.1"", ""233.1"", ""234.1"", ""235.1"", ""236.1"", ""237.1"", ""238.1"", 
""239.1"", ""240.1"", ""241.1"", ""242.1"", ""243.1"", ""244.1"", ""245.1"", 
""246.1"", ""247.1"", ""248.1"", ""249.1"", ""250.1"", ""251.1"", ""252.1"", 
""253.1"", ""254.1"", ""255.1"", ""256.1"", ""257.1"", ""258.1"", ""259.1"", 
""260.1"", ""261.1"", ""262.1"", ""263.1"", ""264.1"", ""265.1"", ""266.1"", 
""267.1"", ""268.1"", ""269.1"", ""270.1"", ""271.1"", ""272.1"", ""273.1"", 
""274.1"", ""275.1"", ""276.1"", ""277.1"", ""278.1"", ""279.1"", ""280.1"", 
""281.1"", ""282.1"", ""283.1"", ""284.1"", ""285.1"", ""286.1"", ""287.1"", 
""288.1"", ""289.1"", ""290.1"", ""291.1"", ""292.1"", ""293.1"", ""294.1"", 
""295.1"", ""296.1"", ""297.1"", ""298.1"", ""299.1"", ""300.1"", ""301.1"", 
""302.1"", ""303.1"", ""304.1"", ""305.1"", ""306.1"", ""307.1"", ""308.1"", 
""309.1"", ""310.1"", ""311.1"", ""312.1"", ""313.1"", ""314.1"", ""315.1"", 
""316.1"", ""317.1"", ""318.1"", ""319.1"", ""320.1"", ""321.1"", ""322.1"", 
""323.1"", ""324.1"", ""325.1"", ""326.1"", ""327.1"", ""328.1"", ""329.1"", 
""330.1"", ""331.1"", ""332.1"", ""333.1"", ""334.1"", ""335.1"", ""336.1"", 
""337.1"", ""338.1"", ""339.1"", ""340.1"", ""341.1"", ""342.1"", ""343.1"", 
""344.1"", ""345.1"", ""346.1"", ""347.1"", ""348.1"", ""349.1"", ""350.1"", 
""351.1"", ""352.1"", ""353.1"", ""354.1"", ""355.1"", ""356.1"", ""357.1"", 
""358.1"", ""359.1"", ""360.1"", ""361.1"", ""362.1"", ""363.1"", ""364.1"", 
""365.1"", ""366.1"", ""367.1"", ""368.1"", ""369.1"", ""370.1"", ""371.1"", 
""372.1"", ""373.1"", ""374.1"", ""375.1"", ""376.1"", ""377.1"", ""378.1"", 
""379.1"", ""380.1"", ""381.1"", ""382.1"", ""383.1"", ""384.1"", ""385.1"", 
""386.1"", ""387.1"", ""388.1"", ""389.1"", ""390.1"", ""391.1"", ""392.1"", 
""393.1"", ""394.1"", ""395.1"", ""396.1"", ""397.1"", ""398.1"", ""399.1"", 
""400.1"", ""401.1"", ""402.1"", ""403.1"", ""404.1"", ""405.1"", ""406.1"", 
""407.1"", ""408.1"", ""409.1"", ""410.1"", ""411.1"", ""412.1"", ""413.1"", 
""414.1"", ""415.1"", ""416.1"", ""417.1"", ""418.1"", ""419.1"", ""420.1"", 
""421.1"", ""422.1"", ""423.1"", ""424.1"", ""425.1"", ""426.1"", ""427.1"", 
""428.1"", ""429.1"", ""430.1"", ""431.1"", ""432.1"", ""433.1"", ""434.1"", 
""435.1"", ""436.1"", ""437.1"", ""438.1"", ""439.1"", ""440.1"", ""441.1"", 
""442.1"", ""443.1"", ""444.1"", ""445.1"", ""446.1"", ""447.1"", ""448.1"", 
""449.1"", ""450.1"", ""451.1"", ""452.1"", ""453.1"", ""454.1"", ""455.1"", 
""456.1"", ""457.1"", ""458.1"", ""459.1"", ""460.1"", ""461.1"", ""462.1"", 
""463.1"", ""464.1"", ""465.1"", ""466.1"", ""467.1"", ""468.1"", ""469.1"", 
""470.1"", ""471.1"", ""472.1"", ""473.1"", ""474.1"", ""475.1"", ""476.1"", 
""477.1"", ""478.1"", ""479.1"", ""480.1"", ""481.1"", ""482.1"", ""483.1"", 
""484.1"", ""485.1"", ""486.1"", ""487.1"", ""488.1"", ""489.1"", ""490.1"", 
""491.1"", ""492.1"", ""493.1"", ""494.1"", ""495.1"", ""496.1"", ""497.1"", 
""498.1"", ""499.1"", ""500.1"", ""501.1"", ""502.1"", ""503.1"", ""504.1"", 
""505.1"", ""506.1"", ""507.1"", ""508.1"", ""509.1"", ""510.1"", ""511.1"", 
""512.1"", ""513.1"", ""514.1"", ""515.1"", ""516.1"", ""517.1"", ""518.1"", 
""519.1"", ""520.1"", ""521.1"", ""522.1"", ""523.1"", ""524.1"", ""525.1"", 
""526.1"", ""527.1"", ""528.1"", ""529.1"", ""530.1"", ""531.1"", ""532.1"", 
""533.1"", ""534.1"", ""535.1"", ""536.1"", ""537.1"", ""538.1"", ""539.1"", 
""540.1"", ""541.1"", ""542.1"", ""543.1"", ""544.1"", ""545.1"", ""546.1"", 
""547.1"", ""548.1"", ""549.1"", ""550.1"", ""551.1"", ""552.1"", ""553.1"", 
""554.1"", ""555.1"", ""556.1"", ""557.1"", ""558.1"", ""559.1"", ""560.1"", 
""561.1"", ""562.1"", ""563.1"", ""564.1"", ""565.1"", ""566.1"", ""567.1"", 
""568.1"", ""569.1"", ""570.1"", ""571.1"", ""572.1"", ""573.1"", ""574.1"", 
""575.1"", ""576.1"", ""577.1"", ""578.1"", ""579.1"", ""580.1"", ""581.1"", 
""582.1"", ""583.1"", ""584.1"", ""585.1"", ""586.1"", ""587.1"", ""588.1"", 
""589.1"", ""590.1"", ""591.1"", ""592.1"", ""593.1"", ""594.1"", ""595.1"", 
""596.1"", ""597.1"", ""598.1"", ""599.1"", ""600.1"", ""601.1"", ""602.1"", 
""603.1"", ""604.1"", ""605.1"", ""606.1"", ""607.1"", ""608.1"", ""609.1"", 
""610.1"", ""611.1"", ""612.1"", ""613.1"", ""614.1"", ""615.1"", ""616.1"", 
""617.1"", ""618.1"", ""619.1"", ""620.1"", ""621.1"", ""622.1"", ""623.1"", 
""624.1"", ""625.1"", ""626.1"", ""627.1"", ""628.1"", ""629.1"", ""630.1"", 
""631.1"", ""632.1"", ""633.1"", ""634.1"", ""635.1"", ""636.1"", ""637.1"", 
""638.1"", ""639.1"", ""640.1"", ""641.1"", ""642.1"", ""643.1"", ""644.1"", 
""645.1"", ""646.1"", ""647.1"", ""648.1"", ""649.1"", ""650.1"", ""651.1"", 
""652.1"", ""653.1"", ""654.1"", ""655.1"", ""656.1"", ""657.1"", ""658.1"", 
""659.1"", ""660.1"", ""661.1"", ""662.1"", ""663.1"", ""664.1"", ""665.1"", 
""666.1"", ""667.1"", ""668.1"", ""669.1"", ""670.1"", ""671.1"", ""672.1"", 
""673.1"", ""674.1"", ""675.1"", ""676.1"", ""677.1"", ""678.1"", ""679.1"", 
""680.1"", ""681.1"", ""682.1"", ""683.1"", ""684.1"", ""685.1"", ""686.1"", 
""687.1"", ""688.1"", ""689.1"", ""690.1"", ""691.1"", ""692.1"", ""693.1"", 
""694.1"", ""695.1"", ""696.1"", ""697.1"", ""698.1"", ""699.1"", ""700.1""
), class = ""data.frame"")
</code></pre>
"
"0.201346816564207","0.190590624965622"," 58745","<p>EDIT 2: I originally thought I needed to run a two-factor ANOVA with repeated measures on one factor, but I now think a linear mixed-effect model will work better for my data. I think I nearly know what needs to happen, but am still confused by few points.</p>

<p>The experiments I need to analyze look like this: </p>

<ul>
<li>Subjects were assigned to one of several treatment groups</li>
<li>Measurements of each subject were taken on multiple days</li>
<li>So:
<ul>
<li>Subject is nested within treatment</li>
<li>Treatment is crossed with day</li>
</ul></li>
</ul>

<p>(each subject is assigned to only one treatment, and measurements are taken on each subject on each day)</p>

<p>My dataset contains the following information:</p>

<ul>
<li>Subject = blocking factor (random factor)</li>
<li>Day = within subject or repeated measures factor (fixed factor)</li>
<li>Treatment = between subject factor (fixed factor)</li>
<li>Obs = measured (dependent) variable</li>
</ul>

<p><strong>UPDATE</strong>
OK, so I went and talked to a statistician, but he's an SAS user.  He thinks that the model should be:</p>

<p><strong>Treatment + Day + Subject(Treatment) + Day*Subject(Treatment)</strong></p>

<p>Obviously his notation is different from the R syntax, but this model is supposed to account for:</p>

<ul>
<li>Treatment   (fixed)</li>
<li>Day   (fixed)</li>
<li>the Treatment*Day interaction</li>
<li>Subject nested within Treatment  (random)</li>
<li>Day crossed with ""Subject within Treatment""   (random)</li>
</ul>

<p>So, is this the correct syntax to use? </p>

<pre><code>m4 &lt;- lmer(Obs~Treatment*Day + (1+Treatment/Subject) + (1+Day*Treatment/Subject), mydata)
</code></pre>

<p>I'm particularly concerned about whether the Day crossed with ""Subject within Treatment"" part is right.  Is anyone familiar with SAS, or confident that they understand what's going on in his model, able to comment on whether my sad attempt at R syntax matches?</p>

<p>Here are my previous attempts at building a model and writing syntax (discussed in answers &amp; comments):</p>

<pre><code>m1 &lt;- lmer(Obs ~ Treatment * Day + (1 | Subject), mydata)
</code></pre>

<p>How do I deal with the fact that subject is nested within treatment?  How does <code>m1</code> differ from: </p>

<pre><code>m2 &lt;- lmer(Obs ~ Treatment * Day + (Treatment|Subject), mydata)
m3 &lt;- lmer(Obs ~ Treatment * Day + (Treatment:Subject), mydata)
</code></pre>

<p>and are <code>m2</code> and <code>m3</code> equivalent (and if not, why)?</p>

<p>Also, do I need to be using nlme instead of lme4 if I want to specify the correlation structure (like <code>correlation = corAR1</code>)?  According to <a href=""http://circ.ahajournals.org/content/117/9/1238.full"">Repeated Measures</a>, for a repeated-measures analysis with repeated measures on one factor, the covariance structure (the nature of the correlations between measurements of the same subject) is important. </p>

<p>When I was trying to do a repeated-measures ANOVA, I'd decided to use a Type II SS; is this still relevant, and if so, how do I go about specifying that?</p>

<p>Here's an example of what the data look like:</p>

<pre><code>mydata &lt;- data.frame(
  Subject  = c(13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 
               34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 
               19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 
               40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 
               29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65), 
  Day       = c(rep(c(""Day1"", ""Day3"", ""Day6""), each=28)), 
  Treatment = c(rep(c(""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", 
                      ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A""), each = 4)), 
  Obs       = c(6.472687, 7.017110, 6.200715, 6.613928, 6.829968, 7.387583, 7.367293, 
                8.018853, 7.527408, 6.746739, 7.296910, 6.983360, 6.816621, 6.571689, 
                5.911261, 6.954988, 7.624122, 7.669865, 7.676225, 7.263593, 7.704737, 
                7.328716, 7.295610, 5.964180, 6.880814, 6.926342, 6.926342, 7.562293, 
                6.677607, 7.023526, 6.441864, 7.020875, 7.478931, 7.495336, 7.427709, 
                7.633020, 7.382091, 7.359731, 7.285889, 7.496863, 6.632403, 6.171196, 
                6.306012, 7.253833, 7.594852, 6.915225, 7.220147, 7.298227, 7.573612, 
                7.366550, 7.560513, 7.289078, 7.287802, 7.155336, 7.394452, 7.465383, 
                6.976048, 7.222966, 6.584153, 7.013223, 7.569905, 7.459185, 7.504068, 
                7.801867, 7.598728, 7.475841, 7.511873, 7.518384, 6.618589, 5.854754, 
                6.125749, 6.962720, 7.540600, 7.379861, 7.344189, 7.362815, 7.805802, 
                7.764172, 7.789844, 7.616437, NA, NA, NA, NA))
</code></pre>
"
"0.106119089994502","0.0850871259623034"," 59184","<p>I have various datasets I need to analyse regarding soil properties, all in the same fashion, with one fixed effect (which is a position along a transect, indicating different land uses). Now my main level of replication is across different transects, which will obviously have some form of random variance associated with them, and so I want to account for this in my statistical analyses. </p>

<p>So, in <code>lme4</code> I specified a mixed model to this specification</p>

<pre><code>model &lt;- lmer(variable.of.interest ~ transect.position + (1|transect))
</code></pre>

<p>Now, when I analyse the above model against a model without the transect position term, I get exactly the results I was expecting, and then plugging the above model into <code>anova()</code>, I get the F values, d.f. etc. that I need.</p>

<p>However, I can't figure out how to say, for my overall report, that the random effect of transect does not make any difference to the overall analyses (i.e. I can't get a p-value, F value, d.f. etc.).</p>

<p>Help?</p>
"
"0.164398987305357","0.164770510914327"," 59367","<p>I have data from a two-factor within-subjects experiment design where the conditions are not orthogonal. Factor one (Location) has three levels; factor two (Stimulus) has three levels, one of which is ""no stimulus"".</p>

<p>When multiplying these factors I get nine conditions, three of which are equivalent. All three Location x ""no stimulus"" conditions are equivalent because of lack of stimulus. As a result I only have observations from 7 conditions (just a single Location x ""no stimulus"" condition, intended to be used as a control condition). I just chose an arbitrary level of Location for that condition for the sake of coding the data.</p>

<p>My question is - how can I analyse my data in R using ANOVA?</p>

<p>I've tried building a model with the lme() function in the nlme package (following a textbook example for orthogonal designs), but I get an error when trying to build the model (presumably because of missing conditions?):</p>

<pre><code>&gt; model &lt;- lme(Y ~ Location * Design, data, random = ~ 1 | Subject / Location / Design, method=""ML"")
Error in MEEM(object, conLin, control$niterEM) : 
  Singularity in backsolve at level 0, block 1
</code></pre>

<p>The lmer() function in the lme4 package also gives an error:</p>

<pre><code>model &lt;- lmer(Y ~ Location * Design + (1|Subject) + (1|Location) + (1|Design) + (1|Location:Design), data, REML=FALSE)
Error in mer_finalize(ans) : Downdated X'X is not positive definite, 8.
</code></pre>

<p>The ezANOVA() function in the ez package also gives an error.</p>

<p>Any advice on how I could approach this analysis would be greatly appreciated! Would it be bad to duplicate the ""no stimulus"" observations for each of the two Location x ""no stimulus"" conditions which don't have observations?</p>
"
"0.150075056296916","0.150414209399047"," 59861","<p><strong>Data structure:</strong>
I have two datasets from two protected areas that differ in protection status. Both areas contain 43 and 37 sites each. </p>

<p><strong>Question:</strong>
I would like to know which test would be the best for testing whether the PA status has had an effect on:  </p>

<ol>
<li>the first axis of a PCoA (principal coordinates analysis) - i.e. species composition turnover (derived by constructing a bray curtis dissimilarity matrix) and </li>
<li>species richness per site (a continuous variable). </li>
</ol>

<p><strong>Problem:</strong>
I understand that there is pseudoreplication present in this as I only have two areas. From what I have read, it seems that I either have to use an ANCOVA / GLM / mixed-effect model, where I define PA status as both a random effect and a fixed effect. I intended to nest sites within PA, but it seems that as there is only one datapoint per site it will not work as a nested object. </p>

<p>For those familiar with R, here are some codes I have tried:</p>

<pre><code>pcoaPAanovadata1 &lt;- read.csv(""PCoA\\data\\
                              combined data PCoA axis 1 with distance variables.csv"", 
                              header=T)

str(pcoaPAanovadata1)
'data.frame': 80 obs. of 7 variables:
PCOA:    num -0.2215 -0.3521 -0.0611 0.3434 -0.3624 ...
PA.stat: Factor w/ 2 levels ""N"",""P"": 1 1 1 1 1 1 1 1 1 1 ...
village: num 33.6 33.7 39.9 37.9 34 ...
road:    num 4.18 3.8 0.89 0.1 3.43 5.49 1.86 5.04 0.79 0.88 ...
track:   num 8.11 6.48 3.11 2.71 4.49 5.35 1.25 4.03 7.62 6.77 ...
site:    Factor w/ 80 levels ""M1_11"",""M1_17"",..: 1 2 3 4 5 6 7 8 9 10 ...
rich:    num 3.27 1.79 7.31 0.82 1.79 1.82 2.45 0.82 5.47 2.79 ...
</code></pre>

<p>compare community composition turnover at different PAs:
below specifies a null model where the slope deviates as a result of the random effect </p>

<pre><code>z0 &lt;- lmer(rich ~ 1, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(z0)
z1 &lt;- lme(rich ~ pastat, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(z1)
anova(z0,z1)
</code></pre>

<p>impacts of distance variables:</p>

<pre><code>zz &lt;- lme(pcoa ~ road, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(zz)
</code></pre>

<p>The errors I get from the lme(linear mixed effect model):</p>

<pre><code>Warning message:
In pt(-abs(tTable[, ""t-value""]), tTable[, ""DF""]) : NaNs produced
</code></pre>

<p>The error I get from the ANOVA:</p>

<pre><code>Warning message:
In anova.lme(z0, z1) :
fitted objects with different fixed effects. REML comparisons are not meaningful.
</code></pre>

<p>Firstly, I was hoping to just clarify whether the test I am running is correct. Secondly, it'd be great if someone could tell me what the errors mean. I apologise if my question is poorly phrased, I am relatively new to R and the statistics I am using. </p>
"
"0.0968730322865161","0.116510345607093"," 62756","<p>I have 3 within-subject factors, namely <code>offset</code> (1px, ..., 5px), <code>side</code> (left, right) and <code>color</code> (red, green), which define the characteristics of the stimulus in a reaction time experiment. The DV is reaction time <code>RT</code>. The design is fully balanced. </p>

<p>I ran a repeated measures ANOVA in R, like this: </p>

<pre><code>options(contrasts = c(""contr.helmert"", ""contr.poly""))

simon.aov &lt;- aov(median.RT ~ color*side*offset + Error(VP / (color*side*offset)), data=dfa)
</code></pre>

<p>The results revealed a significant main effect of the <code>color</code>, as well as a significant interaction <code>color x side</code> and a significant 3-way interaction <code>color x side x offset</code>.<br>
My primary focus lies on the interactions. <strong>Specifically, I want to know on which of the 5 offsets (i.e. on which levels of the third factor) the 2-way interaction <code>color x side</code> reaches significance.</strong> </p>

<p>I am by no means familiar with post-hoc contrasts and multiple comparisons, but this question is the gist of the thesis that I'm working on. So my progress depends on an adequate test to examine this question. </p>

<p>I highly appreciate any help on which test to run, and how to do this most efficiently in R. </p>

<h2>Edit:</h2>

<p>I'm sorry I didn't provide any plots earlier.  </p>

<p>@John: Here is the plot you requested.</p>

<p><img src=""http://i.stack.imgur.com/Z6alg.png"" alt=""Plot 1 of 3-way anova results""> </p>

<p>However, I believe, that this following plot rather clarifies my question: </p>

<p><img src=""http://i.stack.imgur.com/wCtKH.png"" alt=""Plot 2 of 3-way anova results""></p>

<p>It seems like there is no <code>color x side</code> interaction at the first 3 levels of <code>offset</code>, but this interaction emerges at <code>offset</code> 4 and 5. This is what the plot seems to imply, however I don't know how to prove it statistically. </p>
"
"0.217704435665112","0.218196423717792"," 63464","<p>I have an unbalanced repeated measures data set to analyse, and I've read that the way most statistical packages handle this with ANOVA (i.e. type III sum of squares) is wrong. Therefore, I would like to use a mixed effects model to analyse these data. I have read a lot about mixed models in <code>R</code>, but I am still very new to <code>R</code> and mixed effect models and not very confident I am doing things right. Note that I can't yet entirely divorce myself of ""traditional"" methods, and still need $p$-values and post hoc tests.</p>

<p>I would like to know if the following approach makes sense, or if I am doing something horribly wrong. Here's my code:</p>

<pre><code># load packages
library(lme4)
library(languageR)
library(LMERConvenienceFunctions)
library(coda)
library(pbkrtest)

# import data
my.data &lt;- read.csv(""data.csv"")

# create separate data frames for each DV &amp; remove NAs
region.data &lt;- na.omit(data.frame(time=my.data$time, subject=my.data$subject, dv=my.data$dv1))

# output summary of data
data.summary &lt;- summary(region.data)

# fit model
# ""time"" is a factor with three levels (""t1"", ""t2"", ""t3"")
region.lmer &lt;- lmer(dv ~ time + (1|subject), data=region.data)

# check model assumptions
mcp.fnc(region.lmer)

# remove outliers (over 2.5 standard deviations)
rm.outliers &lt;- romr.fnc(region.lmer, region.data, trim=2.5)
region.data &lt;- rm.outliers$data
region.lmer &lt;- update(region.lmer)

# re-check model assumptions
mcp.fnc(region.lmer)

# compare model to null model
region.lmer.null &lt;- lmer(dv ~ 1 + (1|subject), data=region.data)
region.krtest &lt;- KRmodcomp(region.lmer, region.lmer.null)

# output lmer summary
region.lmer.summary &lt;- summary(region.lmer)

# run post hoc tests
t1.pvals &lt;- pvals.fnc(region.lmer, ndigits=10, withMCMC=TRUE)

region.lmer &lt;- lmer(dv ~ relevel(time,ref=""t2"") + (1|subject), data=region.data)
t2.pvals &lt;- pvals.fnc(region.lmer, ndigits=10, withMCMC=TRUE)

region.lmer &lt;- lmer(dv ~ relevel(time,ref=""t3"") + (1|subject), data=region.data)
t3.pvals &lt;- pvals.fnc(region.lmer, ndigits=10, withMCMC=TRUE)

# Get mcmc mean and 50/95% HPD confidence intervals for graphs
# repeated three times and stored in a matrix (not shown here for brevity)
as.numeric(t1.pvals$fixed$MCMCmean)
as.numeric(t1.pvals$fixed$HPD95lower)
as.numeric(t1.pvals$fixed$HPD95upper)
HPDinterval(as.mcmc(t1.pvals$mcmc),prob=0.5)
    HPDinterval(as.mcmc(t1.pvals$mcmc),prob=0.5)
</code></pre>

<p>Some specific questions I have:</p>

<ol>
<li>Is this a valid way of analysing mixed effects models? If not, what
should I be doing instead.</li>
<li>Are the criticism plots output by mcp.fnc good enough for verifying
model assumptions, or should I be taking additional steps.</li>
<li>I get that for mixed models to be valid, the data need respect
assumptions of normality and homoscedasticity. How to I judge what
is ""approximately normal"" and what is not by looking at the
criticism plots generated by mcp.fnc? Do I just need to get a feel
for this, or is their a prescribed way of doing things? How robust
are mixed models in respect to these assumptions?</li>
<li>I need to assess differences between the three time points for ~20
characteristics (biomarkers) of the subjects in my sample. Is
fitting and testing separate models for each acceptable so long as I
report all undertaken tests (significant or not), or do I need any
form of correction for multiple comparisons.</li>
</ol>

<p>To be a little more precise in regards to the experiment, here are some more details. We followed a number of participants longitudinally as they underwent a treatment. We measured a number of biomarkers before the start of the treatment and at two time points after. What I'd like to see is if there are difference in these biomarkers between the three time points.</p>

<p>I am basing most of what I am doing here on this <a href=""http://www.bodo-winter.net/tutorial/bw_LME_tutorial.pdf"">tutorial</a>, but made some changes based on my needs and things I read. The changes I made are:</p>

<ol>
<li>relevel the ""time"" factor to obtain t1-t2, t2-t3, and t1-t3 comparisons with pvals.fnc (from the languageR package)</li>
<li>compare my mixed model to the null model using an approximate F-test based on a Kenward-Roger's approach (using the pbkrtest package) rather than a likelihood ratio test (because I read, that Kenward-Roger's is better regarded right now)</li>
<li>Use the LMERConvenienceFunctions package to check assumptions and remove outliers (because I read that mixed models are very sensitive to outliers)</li>
</ol>
"
"0.143090951758036","0.157755753708238"," 63872","<p>I am think that it is possible to analyse <strong>a model with just random effects</strong> but I am not sure as I have never done it. I am looking for guidance on whether it is appropriate, what assumptions I need to be aware of, and how to do it properly.</p>

<p>From my study of an insect; </p>

<ul>
<li>I have a response variable (age at death, ""age"")  </li>
<li>Two treatments
(""Treat1"" and ""Treat2"") both of which have two levels (Treat1 has
""A"" and ""B"", and Treat2 has ""P"" and ""Q"")  </li>
<li>There is also 40 genotypes
(1-40)  </li>
<li>With four replicates (w,x,y,z) of each combination of
Genotype/Treat1/Treat2 </li>
<li>Each replicate contains 50 individuals</li>
</ul>

<p>Put simply, my data looks like 32000 rows of this:</p>

<pre><code>Treat1  Treat2  Genotype  Block  Individual   Age   
A       P       1         w      1            23
A       P       1         w      2            35
A       P       1         w      3            44
.       .       .         .      .            .
.       .       .         .      .            .
.       .       .         .      .            .
B       Q       40        z      50           76     
</code></pre>

<p>I would like to know if each combination of Treat1 and Treat2 (AP,AQ,BP,BQ) have genetic genetic variation - i.e. is there variation between my 40 genotypes within each treatment combination?</p>

<p>I think I need a model for each of AP, AQ, BP, and BQ, along the lines of </p>

<pre><code>Age ~ Genotype [ Treat1 == ""A"" &amp; Treat2 == ""P""] * Block [ Treat1 == ""A"" &amp; Treat2 == ""P""]
</code></pre>

<p>Where  Genotype and Block are random effects. I hear Gamma distribtions are better to use in lifespan (time to death) models.</p>

<p><strong>My questions are:</strong></p>

<p>a. Is this an appropriate way to show whether or not my genotypes have variation?</p>

<p>b. Can I build the four models as defined above or is that a really poor way of doing it?</p>

<p>c. If possible, what functions should I be using in R (lm, glm, lmer... &amp; summary, summary.lm, aov, anova...)?</p>

<p>d. What should I expect, if gamma is more suitable than gaussian, to see when I compare <code>plot(model)</code> for gamma compared to gaussian?</p>

<hr>

<p>This is currently my model...</p>

<pre><code>AP= df$Treat1==""A"" &amp; df$Treat2==""P""
apmodel&lt;- lmer(df$Age[AP]~(1|df$Genotype[AP])+(1|df$Block[AP]))
summary(apmodel)
</code></pre>

<p>Which I think is right but I'm not sure what to do with the output..</p>

<pre><code>&gt; summary(apmodel)
Linear mixed model fit by REML 
Formula: df$Age[AP] ~ (1 | df$Genotype[AP]) + (1 | df$Block[AP]) 
       AIC   BIC logLik deviance REMLdev
     57343 57371 -28667    57336   57335
    Random effects:
     Groups           Name        Variance Std.Dev.
     df$Genotype[AP]  (Intercept) 17.23798 4.15186 
     df$Block[AP]     (Intercept)  0.15416 0.39263 
     Residual                     93.18777 9.65338 
    Number of obs: 7757, groups: df$line[AP], 40; df$Block[AP], 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)  49.9948     0.6939   72.05
</code></pre>

<p><strong>Is there genetic variance??</strong></p>
"
"0.0949157995752499","0.0951302988308988"," 65390","<p>I have models like this:</p>

<pre><code>require(nlme)

set.seed(123)
n &lt;- 100
k &lt;- 5
cat &lt;- as.factor(rep(1:k, n))
cat_i &lt;- 1:k # intercept per kategorie
x &lt;- rep(1:n, each = k)
sigma &lt;- 0.2
alpha &lt;- 0.001
y &lt;- cat_i[cat] + alpha * x + rnorm(n*k, 0, sigma)
plot(x, y)

m1 &lt;- lm(y ~ x)
summary(m1)

m2 &lt;- lm(y ~ cat + x)
summary(m2)

m3 &lt;- lme(y ~ x, random = ~ 1|cat, na.action = na.omit)
summary(m3)
</code></pre>

<p>Now I am trying to assess whether the random effect should be present in the model. So I compare the models using AIC or anova, and I get the following error:</p>

<pre><code>&gt; AIC(m1, m2, m3)
   df       AIC
m1  3 1771.4696
m2  7 -209.1825
m3  4 -154.0245
Warning message:
In AIC.default(m1, m2, m3) :
  models are not all fitted to the same number of observations  
&gt; anova(m2, m3)
Error in anova.lmlist(object, ...) : 
  models were not all fitted to the same size of dataset
</code></pre>

<p>As you can see, in both cases I use the same dataset. I have found two remedies, but I don't consider them satisfying:</p>

<ol>
<li><a href=""https://stat.ethz.ch/pipermail/r-help/2012-March/307348.html"">Adding <code>method = ""ML""</code> to the lme() call</a> - not sure if it is good idea to change the method.</li>
<li>Using <code>lmer()</code> instead. Surprisingly, this works, despite the fact that lmer() uses REML method. However I dont like this solution because the <code>lmer()</code> doesn't show p-values for coefficients - I like to use older <code>lme()</code> instead.</li>
</ol>

<p>Do you have any idea if this is a bug or not and how can we go around that?</p>
"
"0.1898315991505","0.178369310307935"," 68786","<p>I measured a binary response for each subject in 5 different conditions. For each subject and condition, I replicated the experiment 36 times. I thus have 36 binary values per condition per subject.</p>

<p>I am trying to build a model for those data. I suppose a logistic regression is what I'm looking for, and I am working with the <code>lmer</code> package. My aim is to check whether the conditions significantly influence the observed values, so I would have two models:</p>

<pre><code>lmH1&lt;-lmer(value~condition, (random effects), data=dataset, family=binomial)
</code></pre>

<p>and</p>

<pre><code>lmH0&lt;-lmer(value~1, (random effects), data=dataset, family=binomial) 
</code></pre>

<p>By looking at the output from <code>anova(lmH0, lmH1)</code>, I would be able to determine the significance of the effect of my condition.</p>

<p>I am just not sure what to specify as random effect; the models I defined so far are:</p>

<pre><code>lmH1 &lt;- lmer( value ~ condition + ( 1 | subject ), data = dataSet, family = binomial )
</code></pre>

<p>and </p>

<pre><code>lmH2 &lt;- lmer( value ~ condition + ( 1 | subject/condition ), data = dataSet, family = binomial )
</code></pre>

<p>However I am not sure about how lmer handles the replicates, so I don't know whether I should include those replicates in my random effects or not. I could modify the proposed models so that the grouping defined by the random effects refers to a specific binary values instead of a group of binaries values. My new models would then be</p>

<pre><code>lmH1a &lt;- lmer( value ~ condition + ( 1 | subject/(condition:replicate) ), data = dataSet, family = binomial )
</code></pre>

<p>and</p>

<pre><code>lmH2a &lt;- lmer( value ~ condition + ( 1 | subject/condition/replicate ), data = dataSet, family = binomial )
</code></pre>

<p>With those models R returns the warning message <code>Number of levels of a grouping factor for the random effects is equal to n, the number of observations</code>. But the model is still computed.</p>

<p>All 4 models return very similar values for the fixed effects and for the random effects that they have in common (e.g. the subject random effects are very similar for all 4 models and the condition within subject random effects are very similar for <code>lmH2</code> and <code>lmH2a</code>).</p>

<p>How can I check which random effect structure is the most appropriate for my design and collected data?</p>
"
"0.107624400500126","0.125845556426908"," 69664","<p>I want to compare two â€‹GLMs with binomial dependent variables. The results are: </p>

<pre><code> m1 &lt;- glm(symptoms ~ 1,         data=data2)
 m2 &lt;- glm(symptoms ~ phq_index, data=data2)
</code></pre>

<p>The model test gives the following results: </p>

<pre><code>â€‹ anova(m1, m2)â€‹
         no AIC    logLik   LR.stat df  Pr(&gt;Chisq)   
 m1      1  4473.9 -2236.0                        
 m2      9  4187.3 -2084.7  302.62  8   &lt; 2.2e-16 ***
</code></pre>

<p>â€‹I am used to comparing these kinds of models using chi-squared values, a chi-squared difference, and a chi-squared difference test. Since all other models in the paper are compared this way, and since I'd like to report them in a table together: why exactly is this model test different from my other model tests in which I get chi-squared values and difference tests? Can I obtain chi-squared values from this test? </p>

<p>Results from other model comparisons (e.g., GLMER), look like this: </p>

<pre><code>    #Df AIC     BIC     logLik  Chisq   Chi     DF diff Pr(&gt;Chisq)
m3  13  11288   11393   -5630.9 392.16          
m4  21  11212   11382   -5584.9 92.02   300.14  8       0.001
</code></pre>
"
"0.177967124203594","0.190260597661798"," 71914","<p>Hopefully this is a question that someone here can answer for me on the nature of decomposing sums of squares from a mixed-effects model fit with <code>lmer</code> (from the <a href=""http://cran.r-project.org/web/packages/lme4/index.html"">lme4</a> R package).</p>

<p>First off I should say that I am aware of the controversy with using this approach, and in practise I would be more likely to use a bootstrapped LRT to compare models (as suggested by Faraway, 2006). However, I am puzzled at how to replicate the results, and so for my own sanity I thought I would ask here.</p>

<p>Basically, I am getting to grips with using mixed-effects models fit by the <code>lme4</code> package. I know that you can use the <code>anova()</code> command to give a summary of sequentially testing the fixed-effects in the model. As far as I know this is what Faraway (2006) refers to as the 'Expected mean squares' approach. What I want to know is how are the sums of squares calculated?</p>

<p>I know that I could take the estimated values from a particular model (using <code>coef()</code>), assume that they are fixed, and then make tests using the sums of squares of model residuals with and without the factors of interest. This is fine for a model containing a single within-subject factor. However, when implementing a split-plot design the sums of squares value I get is equivalent to the value produced by R using <code>aov()</code> with an appropriate <code>Error()</code> designation. However, this is <em>not</em> the same as the sums of squares produced by the <code>anova()</code> command on the model object, despite the fact that the F-ratios are the same. </p>

<p>Of course this makes complete sense as there is no need for the <code>Error()</code> strata in a mixed-model. However, this must mean that the sums of squares are penalised somehow in a mixed-model in order to provide appropriate F-ratios. How is this achieved? And how does the model somehow correct the between-plot sum of squares but not correct the within-plot sum of squares. Evidently this is something that is necessary for a classical split-plot ANOVA that was achieved by designating different error values for the different effects, so how does a mixed-effect model allow for this?</p>

<p>Basically, I want to be able to replicate the results from the <code>anova()</code> command applied to a lmer model object myself to verify the results and my understanding, however, at present I can achieve this for a normal within-subject design but not for the split-plot design and I can't seem to find out why this is the case.   </p>

<p>As an example:</p>

<pre><code>library(faraway)
library(lme4)
data(irrigation)

anova(lmer(yield ~ irrigation + variety + (1|field), data = irrigation))

Analysis of Variance Table
           Df Sum Sq Mean Sq F value
irrigation  3 1.6605  0.5535  0.3882
variety     1 2.2500  2.2500  1.5782

summary(aov(yield ~ irrigation + variety + Error(field/irrigation), data = irrigation))

Error: field
           Df Sum Sq Mean Sq F value Pr(&gt;F)
irrigation  3  40.19   13.40   0.388  0.769
Residuals   4 138.03   34.51               

Error: Within
          Df Sum Sq Mean Sq F value Pr(&gt;F)
variety    1   2.25   2.250   1.578  0.249
Residuals  7   9.98   1.426               
</code></pre>

<p>As can be seen above all the F-ratios agree. The sums of squares for variety also agree. However, the sums of squares for irrigation do not agree, however it appears the lmer output is scaled. So what does the anova() command actually do?</p>
"
"0.201346816564207","0.201801838198894"," 76980","<p>I'm trying to analyse some data I've recently gotten my hands on, but I'm not entirely sure which model to use. One suggestion has been a Mixed Model, Repeated Measurements ANOVA, but I'm not sure if that such kind of model can answer the questions of interest.</p>

<p><strong>The data</strong>: 
Two individual persons (A and B) have had a lot of different values (V1, V2, V3, ..., Vn) measured four times (At T0, T1, T2 and T3) - The spacing between times differs.</p>

<p>The different values have been grouped into categories (C1, C2, C3, ..., Cn). One value may belong to none, one or multiple categories. Each of the categories have a continuos value (Response_C1,Response_C1, ..., Response_Cn), which is the sum of the measured values belonging to that category. </p>

<p>In addition to this, person B was given a drug at T1.</p>

<p>What I would like to investigate now, is:</p>

<ol>
<li>Is there any observable effect after administering the drug</li>
<li>On which categories did the drug have an effect</li>
<li>If there is an effct on a category, what is the effect size</li>
<li>How does the effect vary over time</li>
<li>If there is an effect, is the effect observed from the drug at T1 still persistant at T3</li>
</ol>

<p>I realise one of the major pitfalls is the lack of both time points and samples, but it would be appreciated if you could suggest any articles/methods for this type of analysis.</p>

<p><strong>What I have tried so far</strong> is just Repeated Measurements ANOVA, using R:</p>

<pre><code>test.aov &lt;- aov(Response_C ~ Category * Timepoint * Treatment + Error(Sample), data=df)
</code></pre>

<p>But I am not sure that the model is correct, neither am I sure that it actually answers my questions, even if I try to model it as a mixed model. </p>

<p>Any help is much appreciated. Please let me know if any additional information is needed</p>

<p><strong>Edit 1:</strong> After doing some more reading, it seems a Generalised Linear Model with a negative binomial distribution (since this kind of data is usually over-dispersed) might be better suited for this kind of data, but I'm still not sure if such a model would answer the questions. Potentially I could fit a model to each individual category, but that would inflate the Type-I error I guess, and so we would need to correct for multiple testing.</p>

<p><strong>Edit 2:</strong> Some more reading, and I thought the <code>lme4</code> R package would be a good way to fit a Linear mixed model to my data, and just do individual comparisons of each category. Here's the model I tried to fit:</p>

<pre><code>lm1 &lt;- lmer(Response ~ Treatment * Timepoint + (1|Subject), data=my_data)
</code></pre>

<p>First off, I'm not sure whether Timepoint should be a factorial or a numerical value. As I mentioned, timepoints are not evenly distributed (To be precise, I have for time 0, 2days, 14 days, 90days), however, the design is balanced. If I enter the Timepoints as a numerical value, I don't get any estimate of what the value is at any given Timepoint, but just some numbers for Correlation of fixed effects, which I can't really use for anything. On the other hand, if I enter the Timepoints as factors, I do get an estimated value for the effect at each timepoint, but I'm not too sure how certain or reliable this value is.</p>
"
"0.106119089994502","0.106358907452879"," 77891","<p>I ran a repeated design whereby I tested 30 males and 30 females across three different tasks. I want to understand how the behaviour of males and females is different and how that depends on the task. I used both the lmer and lme4 package to investigate this, however, I am stuck with trying to check assumptions for either method. The code I run is</p>

<pre><code>lm.full &lt;- lmer(behaviour ~ task*sex + (1|ID/task), REML=FALSE, data=dat)
lm.full2 &lt;-lme(behaviour ~ task*sex, random = ~ 1|ID/task, method=""ML"", data=dat)
</code></pre>

<p>I checked if the interaction was the best model by comparing it with the simpler model without the interaction and running an anova:</p>

<pre><code>lm.base1 &lt;- lmer(behaviour ~ task+sex+(1|ID/task), REML=FALSE, data=dat)
lm.base2 &lt;- lme(behaviour ~ task+sex, random= ~1|ID/task), method=""ML"", data=dat)
anova(lm.base1, lm.full)
anova(lm.base2, lm.full2)
</code></pre>

<p>Q1: Is it ok to use these categorical predictors in a linear mixed model?<br/>
Q2: Do I understand correctly it is fine the outcome variable (""behaviour"") does not need to be normally distributed itself (across sex/tasks)?<br/>
Q3: How can I check homogeneity of variance? For a simple linear model I use <code>plot(LM$fitted.values,rstandard(LM))</code>. Is using <code>plot(reside(lm.base1))</code> sufficient?<br/>
Q4: To check for normality is using the following code ok?</p>

<pre><code>hist((resid(lm.base1) - mean(resid(lm.base1))) / sd(resid(lm.base1)), freq = FALSE); curve(dnorm, add = TRUE)
</code></pre>
"
"0.0968730322865161","0.116510345607093"," 78042","<p>If I had a <code>glm</code> using on count data I may do the following:</p>

<pre><code>glm(response ~ exp1 * exp2, family = poisson, data =data)
</code></pre>

<p>The first thing I would do here is check for overdispersion with the <code>residual deviance/df</code>. If there was overdispersion I may choose to use <code>family =  quasipossion</code></p>

<pre><code>glm(response ~ exp1 * exp2, family = quaispoisson, data =data)
</code></pre>

<p>I would then simplify my model to find the optimal model using analysis of deviance based on log likelihoods (likelihood ratio tests)e.g. </p>

<pre><code>m1 &lt;- glm(response ~ exp1 * exp2, family = quaipoissn, data =data)
m2 &lt;- glm(response ~ exp1 + exp2, family = quaipoissn, data =data) #remove interaction
anova(m1, m2, test = ""chi"") #if it was still poisson
# or
anova(m1, m2, test = ""F"") #for quasipoisson
# using p-values to assess the significance of the removed interaction
</code></pre>

<p>Finally then I would then validate my model by plotting deviance residuals against fitted values, explanatory values e.g. <code>plot(m2)</code>. If all is ok, there is independence and no patterns, I don't have to add in extra explanatory variables etc.</p>

<p>My question is, what are the key differences to this process using <code>glmer</code> e.g.</p>

<pre><code>glmer(response ~ exp1 * exp2 + (1|random1/random2), family = poisson, data =data)
</code></pre>
"
"0.150699071696578","0.1510396350048"," 81057","<p>I conducted an experiment which measured a binary response for each subject. The subjects were in 1 of 3 groups. There were two other fixed factors, each of which were continuums (cont1, cont2) ranging from 0 to 10. In other words, for each step in cont1, there was a corresponding 0-10 step continuum (cont2). Cont1 refers to formant frequency of a vowel and cont2 refers to vowels duration. Therefore, there was a total of 121 stimuli repeated 7 times. Each subject provided 847 responses.</p>

<p>I would like to know how the three groups differ in there responses for the two continua. Groups 1 and 2 are control groups (I know more or less how they behave), but I am interested in group 3 and if they perform more like group 1 or group 2.</p>

<p>I have used the following model in R using lme4:</p>

<pre><code>full.mod &lt;- glmer(response ~ cont1+cont2+group+(random effects), data=df, family=binomial)
</code></pre>

<p>I think that I need the interaction between the three fixed effects to answer my research question; however I am not sure. </p>

<pre><code>full.mod.int &lt;- glmer(response ~ cont1*cont2*group+(random effects), data=df, family=binomial)
</code></pre>

<p>Therefore, my first question is whether full.mod or full.mod.int is the best choice. My experience with ANOVA leads me to believe that I need the interactions, however, being that there are 11 levels for each continuum, the output of coefficients is going to be enormous, and this makes me think that something is not right. </p>

<p>With regard to random effects, I know that I should include random intercepts for each subject (1|subject). However, it is not clear to me whether or not I need a random effect for items too. In this case, both cont1 and cont2 are items. I have a column in my data farm called stimuli which gives information like: cont1_0_cont2_0, cont1_0_cont2_1, etc. Assuming this is what I should do, my model now looks like this:</p>

<pre><code>full.mod.int &lt;- glmer(response ~ cont1*cont2*group+(1|subjects)+(1|stimuli), data=df, family=binomial)
</code></pre>

<p>Would there be any benefit to adding random slopes? If so what would they be (cont1,cont2?)?</p>
"
"0.150699071696578","0.137308759095272"," 81430","<p>I have a mixed model and the data looks like this:</p>

<pre><code>&gt; head(pce.ddply)
  subject Condition errorType     errors
1    j202         G         O 0.00000000
2    j202         G         P 0.00000000
3    j203         G         O 0.08333333
4    j203         G         P 0.00000000
5    j205         G         O 0.16666667
6    j205         G         P 0.00000000
</code></pre>

<p>Each subject provides two datapoints for errorType (O or P) and each subject is in either Condition G (N=30) or N (N=33).  errorType is a repeated variable and Condition is a between variable.  I'm interested in both main effects and the interactions.  So, first an anova:</p>

<pre><code>&gt; summary(aov(errors ~ Condition * errorType + Error(subject/(errorType)),
                 data = pce.ddply))

Error: subject
          Df  Sum Sq  Mean Sq F value Pr(&gt;F)
Condition  1 0.00507 0.005065   2.465  0.122
Residuals 61 0.12534 0.002055               

Error: subject:errorType
                    Df  Sum Sq Mean Sq F value   Pr(&gt;F)    
errorType            1 0.03199 0.03199   10.52 0.001919 ** 
Condition:errorType  1 0.04010 0.04010   13.19 0.000579 ***
Residuals           61 0.18552 0.00304                     
</code></pre>

<p>Condition is not significant, but errorType is, as well as the interaction.</p>

<p>However, when I use lmer, I get a totally different set of results:</p>

<pre><code>&gt; lmer(errors ~ Condition * errorType + (1 | subject),
                    data = pce.ddply)
Linear mixed model fit by REML 
Formula: errors ~ Condition * errorType + (1 | subject) 
   Data: pce.ddply 
    AIC    BIC logLik deviance REMLdev
 -356.6 -339.6  184.3     -399  -368.6
Random effects:
 Groups   Name        Variance Std.Dev.
 subject  (Intercept) 0.000000 0.000000
 Residual             0.002548 0.050477
Number of obs: 126, groups: subject, 63

Fixed effects:
                       Estimate Std. Error t value
(Intercept)            0.028030   0.009216   3.042
ConditionN             0.048416   0.012734   3.802
errorTypeP             0.005556   0.013033   0.426
ConditionN:errorTypeP -0.071442   0.018008  -3.967

Correlation of Fixed Effects:
            (Intr) CndtnN errrTP
ConditionN  -0.724              
errorTypeP  -0.707  0.512       
CndtnN:rrTP  0.512 -0.707 -0.724
</code></pre>

<p>So for lmer, Condition and the interaction are significant, but errorType is not.</p>

<p>Also, the lmer result is exactly the same as a glm result, leading me to believe something is wrong.</p>

<p>Can someone please help me understand why they are so different?  I suspect I am using lmer incorrectly (though I've tried many other versions like (errorType | subject) with similar results.</p>

<p>(I have seen researchers use both approaches in the literature with similar data.)</p>
"
"0.3455579827038","0.333511538816971"," 82102","<p>I hope this is an appropriate forum to post this question. I recently upgraded my R software from 2.15.0 to 3.0.2. I also upgraded the lme4 package from .999999-0 to 1.1-2. After doing so, the results from one of my linear mixed models analyses have changed a bit unexpectedly. In some respects, I was expecting some change, as the lme4 developers very clearly stated that they had made some significant changes to some fundamental components in the package. However, the changes that I am seeing (described below) make me think that something else is awry. I will start by explaining the experimental design, which is quite simple and then the issue at hand.</p>

<p>My experiment is a basic repeated measures design. I used 24 ""Items"" that each appeared in three different ""Conditions"" (SmallClause_Som, NoSmallClause, SmallClause_NoSom). Levels of Condition were rotated across three presentation lists such that each Subject (45 total, each assigned to a particular list) only saw one level of each item.</p>

<p>I used lmer() for the analysis. Condition was entered in as a Fixed effect and ""Subject"" and ""Item"" were entered as Random effects.</p>

<p>The problem:
Using the current version of R 3.0.2 and lme4 1.1-2 with NoSmallClause as the reference level (and no weighting on any of the contrasts), the ConditionSmallClause_Som/NoSmallClause contrast produces a t value of 1.680. </p>

<p>But, when I change reference level to SmallClause_Som (to observe the one remaining contrast) I get not only a change in the polarity of the effect (plus to minus, as expected), but the values change as well.</p>

<p>When I use R 2.15.0 and lme4 .999999-0 (on another computer), I do not experience this issue. I get slightly different values, but they do not change (apart from the polarity) when I change reference level.</p>

<p>My colleague also tried my analysis for me using R 3.0.2 and a version of lme4 (pre version 1.0) (I don't know exactly which version, but it was before the major changes) and he also does not experience the issue.</p>

<p>R 2.15.0 lme4 1.1-2 (older) output:</p>

<pre><code>&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""NoSmallClause"")

&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood 
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 
 AIC  BIC logLik deviance REMLdev
 3930 4010  -1949     3898    3902
Random effects:
 Groups   Name                       Variance   Std.Dev. Corr          
 Subject  (Intercept)                0.98998765 0.994981               
          ConditionSmallClause_Som   0.00203374 0.045097 -1.000        
          ConditionSmallClause_NoSom 0.00019873 0.014097  1.000 -1.000 
 Item     (Intercept)                0.96231875 0.980978               
          ConditionSmallClause_Som   0.89924400 0.948285 -0.020        
          ConditionSmallClause_NoSom 0.62128577 0.788217 -0.256  0.361 
 Residual                            1.68810777 1.299272               
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                       Estimate Std. Error t value
(Intercept)                  2.9583     0.2584  11.447
ConditionSmallClause_Som     0.3639     0.2165   1.680
ConditionSmallClause_NoSom   0.1472     0.1878   0.784

Correlation of Fixed Effects:
            (Intr) CnSC_S
CndtnSmlC_S -0.116       
CndtnSmC_NS -0.260  0.392

&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""SmallClause_Som"")
&gt; 
&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood 
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 
  AIC  BIC logLik deviance REMLdev
 3930 4010  -1949     3898    3902
Random effects:
 Groups   Name                       Variance  Std.Dev. Corr          
 Subject  (Intercept)                0.9023239 0.949907               
          ConditionNoSmallClause     0.0020340 0.045099 1.000         
          ConditionSmallClause_NoSom 0.0035039 0.059194 1.000  1.000  
 Item     (Intercept)                1.8238288 1.350492               
          ConditionNoSmallClause     0.8992237 0.948274 -0.687        
          ConditionSmallClause_NoSom 0.9804329 0.990168 -0.604  0.670 
 Residual                            1.6881050 1.299271               
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  3.3222     0.3174  10.468
ConditionNoSmallClause      -0.3639     0.2165  -1.680
ConditionSmallClause_NoSom  -0.2167     0.2243  -0.966

Correlation of Fixed Effects:
            (Intr) CndNSC
CndtnNSmllC -0.588       
CndtnSmC_NS -0.521  0.638
</code></pre>

<p>R 3.0.2 and lme4 1.1-2 (newer) output:</p>

<pre><code>&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""NoSmallClause"")

&gt; #Model 4: Random slopes by Subject and Item
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 

      AIC       BIC    logLik  deviance 
 3942.557  4022.312 -1955.278  3910.557 

Random effects:
 Groups   Name                       Variance Std.Dev. Corr       
 Subject  (Intercept)                0.9522   0.9758              
          ConditionSmallClause_NoSom 0.1767   0.4204    0.03      
          ConditionSmallClause_Som   0.1760   0.4196   -0.15  0.92
 Item     (Intercept)                1.2830   1.1327              
          ConditionSmallClause_NoSom 0.7782   0.8822   -0.41      
          ConditionSmallClause_Som   1.4901   1.2207    0.09  0.41
 Residual                            1.6466   1.2832              
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  2.9583     0.2814  10.512
ConditionSmallClause_NoSom   0.1472     0.2133   0.690
ConditionSmallClause_Som     0.3639     0.2741   1.327

Correlation of Fixed Effects:
            (Intr) CSC_NS
CndtnSmC_NS -0.357       
CndtnSmlC_S -0.007  0.451
&gt; #anova (test.lmer3, test.lmer4)
&gt; 
&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""SmallClause_Som"")
&gt; 
&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)
Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 

      AIC       BIC    logLik  deviance 
 3951.357  4031.113 -1959.679  3919.357 

Random effects:
 Groups   Name                       Variance Std.Dev. Corr       
 Subject  (Intercept)                0.88980  0.9433              
          ConditionNoSmallClause     0.04299  0.2073   0.83       
          ConditionSmallClause_NoSom 0.01562  0.1250   0.90  0.67 
 Item     (Intercept)                2.39736  1.5483              
          ConditionNoSmallClause     0.72053  0.8488   -0.04      
          ConditionSmallClause_NoSom 1.87804  1.3704   -0.16  0.53
 Residual                            1.65166  1.2852              
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  3.3222     0.3525   9.425
ConditionNoSmallClause      -0.3639     0.2004  -1.816
ConditionSmallClause_NoSom  -0.2167     0.2963  -0.731

Correlation of Fixed Effects:
            (Intr) CndNSC
CndtnNSmllC -0.045       
CndtnSmC_NS -0.160  0.514
</code></pre>

<p>My question:
What is going on here? Why is changing the reference level producing a shift from 1.327 to -1.816 in the t scores for the new version of lme4 whereas it produces the same (disregarding sign) value of 1.680/-1.680 in the old version's t scores? Only the older version seems to make sense to me.</p>

<p>1) Am I specifying my model incorrectly for the new version of lme4?</p>

<p>2) Am I missing some basic fundamental fact about how contrasts work? That is, is it possible to get different values just from changing the reference level? (the correlation values look a bit odd in the newer output).</p>

<p>3) Is this a bug in lme4?</p>

<p>4) Some other explanation...?</p>

<p>I have had some other odd issues as well with this same analysis using lme4 1.1-2. For example, if I don't clear the workspace and re-run an analysis, the values also will change between analyses (and also within the analysis as I change the reference level). This never happened to me on the earlier version (and it still does not happen when I run it on the earlier version now).</p>

<p>I hope someone can help with this. I found two other similar questions online (after much searching) but neither had any informative responses.</p>

<p>Thanks DT</p>
"
"0.206864189246938","0.207331679536357"," 83458","<p>My question is about the best way to estimate the effect of a predictor on a dependent variable, while accounting for several other predictors that may correlate with the predictor of interest. I'm using a linear mixed-effects model, using the <code>lmer</code> function from the R <code>lme4</code> package. (Warning: I'm fairly new at this, so their may be some misunderstandings woven through my question.)</p>

<h2>The problem</h2>

<p>To make things a bit more specific, I'll just explain the actual data that I'm working with. I have eye-movement data of participants freely viewing natural scenes. I want to determine whether pupil size predicts the 'visual saliency' (i.e. the conspicuity) of the locations in the image that participants are looking at. But there are many other things that correlate with pupil size, such as luminosity, and this makes the analysis tricky (or does it?).</p>

<h2>Option 1 (simple): Looking at fixed effects</h2>

<p>One option would be to simply create a linear mixed-effects model that has all predictors of saliency that I can think of, including the predictor of interest (<code>pupil_size</code>), as fixed effects and <code>subject</code> and <code>scene</code> as random effects. (To keep things manageable, I'm using a purely additive model, although I suppose that this is a whole topic in itself.)</p>

<pre><code>my_lmer = lmer(saliency ~ brightness + (.. lots of predictors ...)
    + pupil_size + (1|subject) + (1|scene))
</code></pre>

<p>This will give me a t-value for the fixed effect <code>pupil_size</code>. From what I understand, this fixed effect will already be partial, so it's the unique predictive power of pupil size, with any correlations between fixed effects already taken into account. Is my understanding correct?</p>

<h2>Option 2 (complex): Using model comparison</h2>

<p>An alternative approach, which I have from <a href=""http://www.sciencedirect.com/science/article/pii/S0749596X07001398"">Baayen et al. (2008)</a>, is to compare a model without pupil size as fixed effect (<code>simple_model</code>) to a model with pupil size as fixed effect (<code>complex_model</code>).</p>

<pre><code>simple_model = lmer(saliency ~ brightness + (.. lots of predictors ...)
    + (1|subject) + (1|scene))
complex_model = lmer(saliency ~ brightness + (.. lots of predictors ...)
    pupil_size + (1|subject) + (1|scene))
</code></pre>

<p>Now I can use the <code>anova</code> function to compare these two models (see Baayen's paper for an example). This will give me a <code>Chisq Chi</code> value, and I can use this to determine whether adding <code>pupil_size</code> as fixed effect is a justified addition to the model.</p>

<p>Clearly, this model comparison approach is more complex than simply looking at the t-values for fixed effects in a single model. And it seems to me that if <code>pupil_size</code> is a significant predictor (per Option 1), then it must also be a significant addition to the model (per Option 2).</p>

<p>In sum, my question is: <em>Is there any reason to do a model comparison (Option 2), or am I better off just creating a single linear mixed-effects model and seeing whether the t-value associated with <code>pupil_size</code> as fixed effect is sufficiently high (Option 1)?</em></p>
"
"0.0671156055214024","0.0672672793996312"," 83778","<p>I am currently reviewing some work and have come across the following, which seems wrong to me. Two mixed models are fitted (in R) using lmer. The models are non-nested and are compared by likelihood-ratio tests. In short, here is a reproducible example of what I have:</p>

<pre><code>set.seed(105)
Resp = rnorm(100)
A = factor(rep(1:5,each=20))
B = factor(rep(1:2,times=50))
C = rep(1:4, times=25)
m1 = lmer(Resp ~ A + (1|C), REML = TRUE)
m2 = lmer(Resp ~ B + (1|C), REML = TRUE)
anova(m1,m2)
</code></pre>

<p>As far as I can see, <code>lmer</code> is used to compute the log-likelihood and the <code>anova</code> statement tests the difference between the models using a chi-square with the usual degrees of freedom. This does not seem correct to me. If it is correct, does anyone know of any reference justifying this? I am aware of methods relying on simulations (Paper by Lewis et al., 2011) and the approach developed by Vuong (1989) but I do not think that this is what is produced here. I do not think that the use of the <code>anova</code> statement is correct.</p>
"
"0.24198875709123","0.242535625036333"," 86032","<p>I'm currently working with a data set that has numerous samples collected over time at different sites in a study area, and I'm interested in detecting a trend over time for that area.  I know that in an ideal experimental or balanced situation, using a random slope and intercept model is a great way to get at the overall trend within the study area.  With our data, however, many of the sites are missing samples and a handful of the sites only have one data point.</p>

<p>I'm curious if there's a way to intuitively understand how the sample imbalance will affect the estimate of the overall slope?  To put it differently, are there ways to know if sample imbalances are causing problems,  or are there things I can look for in my model output that would indicate I shouldn't trust what the model is estimating?</p>

<p>I created a contrived example with 20 data points to look at this. I put 10 data points with a slope of 1 into one site (a), and put the other 10 data points with a slope of -1 into unique sites (b through l).  I had assumed that when I looked at both a random intercept and random slope and intercept model that they would be somewhat similar, or that at least the latter would give more weight to the site with good data over time.</p>

<pre><code>&gt; library(lme4)
&gt; set.seed(9999)

&gt; x = c(0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9) + rnorm(20,mean=0,sd=0.1)
&gt; y = c(0,1,2,3,4,5,6,7,8,9,9,8,7,6,5,4,3,2,1,0) + rnorm(20,mean=0,sd=0.1)
&gt; z = c(rep('a',10),'b','c','d','e','f','h','i','j','k','l')
&gt; z = factor(z)

&gt; m0 = lm(y~x)
&gt; m1 = lmer(y~x+(1|z))
&gt; m2 = lmer(y~x+(1+x|z))

&gt; summary(m0)
&gt; summary(m1)
&gt; summary(m2)
&gt; anova(m1,m2)
</code></pre>

<p>As expected, the slope of the linear model was near zero, but the results for the two mixed effects models were nearly opposite.  Even though sites b through l only have one data point, it seems like they contribute more towards the slope because the trend is occurring over so many sites.  The random slope and intercept model was also preferred to using model selection criteria.</p>

<pre><code> &gt; summary(m0)$coefficients
                Estimate Std. Error    t value    Pr(&gt;|t|)
 (Intercept)  4.53784796  1.2586990  3.6051890 0.002023703
 x           -0.01178748  0.2335094 -0.0504797 0.960296079

 &gt; summary(m1)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 | z) 

 REML criterion at convergence: 62.0877 

 Random effects:
  Groups   Name        Variance Std.Dev.
  z        (Intercept) 33.30788 5.7713  
  Residual              0.01583 0.1258  
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept) -0.03597    1.74163   -0.02
 x            0.99332    0.01386   71.66

 Correlation of Fixed Effects:
   (Intr)
 x -0.036

 &gt; summary(m2)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 + x | z) 

 REML criterion at convergence: 31.0386 

 Random effects:
  Groups   Name        Variance Std.Dev. Corr 
  z        (Intercept) 7.78818  2.7907        
      x           0.37691  0.6139   -1.00
  Residual             0.01524  0.1234        
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept)   8.2121     0.8566   9.587
 x            -0.8201     0.1882  -4.358

 Correlation of Fixed Effects:
   (Intr)
 x -0.999

 &gt; anova(m1,m2)
 Data: 
 Models:
 m1: y ~ x + (1 | z)
 m2: y ~ x + (1 + x | z)
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
 m1  4 66.206 70.189 -29.103   58.206                             
 m2  6 36.745 42.719 -12.372   24.745 33.462      2  5.419e-08 ***
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I see that under this extreme example, the random slope and intercept have an almost perfect correlation.  Is what I can pull from this is that, in a sense, the model gives more value to the sites with only one data point because the overall trend is so strong but across multiple sites, but that I should view the slope estimate this model produces as suspect with such a high correlation?  Is there anything else that should look for?  For my specific study, I could also set some sort of criteria for what level of replication I thought was necessary to make proper inferences, e.g. eliminate all the sites that less than five samples.</p>

<p>Many thanks for your thoughts.</p>
"
"0.106119089994502","0.0850871259623034"," 86495","<p>I don't know if this belongs here or in StackExchange, it is a mixed but probably pretty simple question. How do I normally report a Likelihood Ratio Test? I would love a good reference in your answer, I have searched but could not find any good answers.</p>

<pre><code>&gt; glmm0 &lt;- glmer(yngel ~ (1|nest), data, family=poisson(link=""log""))
&gt; glmm &lt;- glmer(yngel ~ age.level + (1|nest) + 0, data, family=poisson(link=""log""))
&gt; anova(glmm0, glmm)
Data: data
Models:
glmm0: yngel ~ (1 | nest)
glmm: yngel ~ age.level + (1 | nest) + 0
      Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
glmm0  2 682.33 689.38 -339.16   678.33                             
glmm   3 672.37 682.95 -333.18   666.37 11.959      1   0.000544 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My best guess so far is: I used likelihood ratio test to compare the model with the fixed effect to a model without it. The model including the fixed effect (age-level) was a better fit ($\chi^2(df=?)=11.96$, $p=0.00054$).</p>

<p>And I cannot actually figure out how many df to report from that. The there is 2 for one model and 3 for the other, and 1 between them.</p>

<p>Thank you for your help.</p>
"
"0.116247638743819","0.0970919546725772"," 87834","<p>I am doing linear mixed models using lme4 and this is the results of model comparison:</p>

<pre><code>&gt; anova(lmer5,lmer6,lmer32)

       Df   AIC   BIC logLik   Chisq Chi Df Pr(&gt;Chisq)    
lmer32  9 43172 43226 -21577                              
lmer6  21 43190 43315 -21574  6.3081     12     0.8998    
lmer5  26 43162 43317 -21555 37.9971      5  3.778e-07 ***
</code></pre>

<p>As you can see, the results show that one model is significantly better than the others and normally I will choose model with smallest logLik. However in this result, the logLik is negative. Do you think it is a good idea to choose model from logLik in this case, or should I choose it from AIC or BIC instead.</p>

<p>As no conclusion whether AIC is better than BIC, I am confused which one I should choose. What do you think?</p>
"
"0.143090951758036","0.114731457242355"," 87920","<p>I am doing linear mixed model using lme4. According to Winter (2013, <a href=""http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf"" rel=""nofollow"">http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf</a>), as the new version of R does not give p-values due to inconclusion of degree of freedom, p-values of mixed models can be derived from model comparison. From the examples on page 12, he suggested to construct the null model:</p>

<pre><code>politeness.null=lmer (frequency ~ gender + (1|subject) + (1|scenario), data=politeness, REML=FALSE)
</code></pre>

<p>Then add the fixed effect that we are interested in:</p>

<pre><code>politeness.null=lmer (frequency ~ attitude + gender + (1|subject) + (1|scenario), data=politeness, REML=FALSE)
</code></pre>

<p>And then the p-values of attitude can be given from </p>

<pre><code>anova(politeness.null,politeness.model)
</code></pre>

<p>However, in my case, I have 3-way interaction: color*sex*food, and when I run the model I have 17 layers of fixed effects, such as white (compare to red), blue (compared to red), white:male(compared to female), etc.</p>

<p>Then my question is how I can get p-values for all these fixed effects? I am not sure if I should have some fixed effects that I am not interested in first:</p>

<pre><code>lmer1=lmer (duration ~ action + (1|subject) + (1|repetition), data=data.frame, REML=FALSE) 
</code></pre>

<p>Then add:</p>

<pre><code>lmer1=lmer (duration ~ action + color + (1|subject) + (1|repetition), data=data.frame, REML=FALSE)  
</code></pre>

<p>Then add:</p>

<pre><code>lmer1=lmer (duration ~ action + color + sex + (1|subject) + (1|repetition), data=data.frame, REML=FALSE) 
</code></pre>

<p>or </p>

<pre><code>lmer1=lmer (duration ~ action + color*sex + (1|subject) + (1|repetition), data=data.frame, REML=FALSE)
</code></pre>

<p>or</p>

<pre><code>lmer1=lmer (duration ~ action + sex + (1|subject) + (1|repetition), data=data.frame, REML=FALSE)
</code></pre>

<p>Anyone can help me about this? or is there any other ways to get p-values easier than model comparison?</p>
"
"0.0474578997876249","0.0475651494154494"," 87955","<p>I have installed the package afex with this formula:</p>

<pre><code>x &lt;- mixed(duration ~ (1|item) + (1+color|speaker) + group*color*sex, data=data1.frame,REML=FALSE,type=2,method=c(""KR""))
</code></pre>

<p>The formula gave me this:</p>

<pre><code>Fitting 10 (g)lmer() models:
[..........]
Obtaining 7 p-values:
[Error in list2env(data) : first argument must be a named list
In addition: Warning message:
In mixed(duration ~ (1 | item) + (1 + color | speaker) + group *  :
  Implementation of Type 2 method not unproblematic.
  Check documentation or use car::Anova (Wald tests).
</code></pre>

<blockquote>
  <p>x</p>
</blockquote>

<pre><code>               Effect stat ndf    ddf F.scaling p.value
1               group 1.61   2  35.74      0.98     .21
2               color 0.04   2  17.04      1.00     .96
3                 sex 9.71   1  66.83      1.00    .003
4         group:color 0.20   4  36.56      0.97     .93
5           group:sex 3.30   2 170.83      1.00     .04
6           color:sex 0.31   2  63.48      0.98     .73
7     group:color:sex 1.06   4 193.57      0.99     .38
</code></pre>

<p>Could I ask how I can get p values for all parameters?</p>
"
"0.142373699362875","0.142695448246348"," 90511","<p>My data has a binary response (correct/incorrect), one continuous predictor <code>score</code>, three categorical predictors (<code>race</code>, <code>sex</code>, <code>emotion</code>) and a random intercept for the random factor <code>subj</code>. All predictors are within-subject. One of the categorical factor has 3 levels, the other have two. </p>

<p>I need advice on obtaining ""global"" p-values for each categorical factor (in an ""ANOVA like"" way)</p>

<hr>

<p>Here is how I proceed :</p>

<p>I fitted a binomial GLMM using 'glmer' from the lme4 package (because 'glmmML' doesn't compute on my data and glmmPQL does not provide AIC) and did model selection using <code>drop1</code> repeatedly until no more terms can be dropped. Here is the final model (let's assume it has been validated):</p>

<pre><code>library(lme4)
M5 &lt;- glmer(acc ~ race + sex + emotion + sex:emotion + race:emotion + score +(1|subj), 
        family=binomial, data=subset)
# apparently using family with lmer is deprecated 
drop1(M5, test=""Chisq"")
summary(M5)
</code></pre>

<p><code>drop1</code> gives p-values for the higher level terms only (the two 2-way interactions + <code>score</code>). 
<code>summary</code>gives p-values for every term, but separates the different levels of each categorical factor.</p>

<p>How can I get ""global"" p-values for each factor? I need to report them even if they are not the most relevant or meaningful estimates of signifiance here. How should I proceed? I tried searching on the web and ended up reading about likelihood ratios or the ""Wald test"" but I am not sure if or how this would apply here.</p>

<p>(PS: This is a duplicate from my ""anonymous"" post here that needed editing: <a href=""http://stats.stackexchange.com/questions/90487/binomial-mixed-model-with-categorical-predictors-model-selection-and-getting-p"">Binomial mixed model with categorical predictors: model selection and getting p-values</a> Sorry about that.)</p>
"
"NaN","NaN"," 91805","<p>I am comparing models that I created with the lmer function from the lme4 package using ANOVA. Paricipants and verbs are my random factors, and RT are the Reaction Times that I measured. From the little statistics that I know, I would expect F-statictic values in the output, but instead I get chi-square.</p>

<p>Could you please help me understand why I get that, or whether I should (or could) change that?<br>
Here is my code and output:</p>

<pre><code>m0&lt;-lmer((sqrt(RT))~(1|Participant)+(1|Verbs), data=data, REML=FALSE)
m1&lt;-lmer((sqrt(RT))~(1|Participant)+(1|Verbs)+verbT, data=data, REML=FALSE)

anova(m0,m1)


Data: data
Models:
m0: (sqrt(RT)) ~ (1 | Participant) + (1 | Verbs)
m1: (sqrt(RT)) ~ (1 | Participant) + (1 | Verbs) + verbT
   Df     AIC     BIC logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
m0  4 -693.68 -672.44 350.84  -701.68                         
m1  5 -693.71 -667.16 351.85  -703.71 2.0238      1     0.1548
</code></pre>
"
"0.0474578997876249","0.0475651494154494"," 92616","<p>What's going on here?</p>

<pre><code>data.2
         subj phon f.amp
    1     1    V   100
    2     2    V    60
    3     3    V   124
    4     4    V    42
    5     5    V   210
    6     6    V   104
    7     7    V   150
    8     1    Ê”    92
    9     2    Ê”    33
    10    3    Ê”    92
    11    4    Ê”    32
    12    5    Ê”    90
    13    6    Ê”    65
    14    7    Ê”   105
    15    1    h   142
    16    2    h    72
    17    3    h   141
    18    4    h    60
    19    5    h   268
    20    6    h   134
    21    7    h   145
</code></pre>

<p>Pairwise comparison of levels PHON<sub>V</sub> and PHON<sub>h</sub> by running ANOVA on the pertinent subset of data:</p>

<pre><code>library(lme4)
anova(lmer(f.amp~phon+(1|subj),data.2[which(data.2[,2]!=""Ê”""),]))
   Analysis of Variance Table
        Df Sum Sq Mean Sq F value
   phon  1 2113.1  2113.1  9.8144
</code></pre>

<p>Pairwise comparison of the same levels by direct definition of contrast coefficients; different resulting <em>F</em>-ratio:</p>

<pre><code>contrasts(data.2[,2],1)=matrix(c(-1,0,1),nrow=3)
contrasts(data.2[,2])
    [,1]
  V   -1
  Ê”    0
  h    1
anova(lmer(f.amp~phon+(1|subj),data.2))
   Analysis of Variance Table
        Df Sum Sq Mean Sq F value
   phon  1 2113.1  2113.1  1.2566
</code></pre>

<p>Since <em>df</em><sub>PHON</sub>, <em>SS</em><sub>PHON</sub> and <em>MS</em><sub>PHON</sub> are the same for both analyses; and since <em>F</em><sub>PHON</sub> = <em>MS</em><sub>PHON</sub> / <em>MS</em><sub>PHON x <em>S</em></sub>, I deduce that the analyses differ regarding the handling of <em>S</em>.</p>

<p>Any ideas as to how and why precisely?</p>
"
"0.197311400916898","0.206746271272778"," 93601","<p>I am a complete novice and dummy when it comes to statistics so I apologise in advance...</p>

<p>I have been asked to report the results of my GLMMs (I ran two) in a table. This table must state: effect, standard error, test statistic, and P value, for all fixed effects. </p>

<p>Unfortunately I am struggling to read my output. </p>

<p>The out put is as follows, if anyone would be kind enough to help I would be very grateful and will know for future reference which bit equates to what (also I have been told my degrees of freedom are different for both the tests, could someone explain why this is?).</p>

<pre><code>GLMM 1-run for predictors of step length. 
Response variable = step length. 
fixed effects = depth and direction threshold. 
random factor = individual

Models:
m2: step ~ (1 | ind)
m1: step ~ Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m2 3 373235 373259 -186615 373229 
m1 8 373225 373290 -186605 373209 19.767 5 0.001382 **
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>.</p>

<pre><code>GLMM 2 -run to investigate potential predictors of PDBA.
response variables = depth and step length. 
fixed effect = direction threshold.
random factor = Individual

Models:
m3: PDBA ~ Depth + (1 | ind) + thresholdepth
m2: PDBA ~ step * threshold + Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m3 6 -48205 -48157 24109 -48217 
m2 11 -48430 -48341 24226 -48452 235.1 5 &lt; 2.2e-16 ***
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Models:
m4: PDBA ~ step + (1 | ind) + step:threshold
m2: PDBA ~ step * threshold + Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m4 6 -48206 -48158 24109 -48218 
m2 11 -48430 -48341 24226 -48452 233.81 5 &lt; 2.2e-16 ***
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Hi, I think the package I used was was lme4? </p>

<p>I have run a summary for the first GLMM and this is what I got, I have no idea which parts are relevant though, I assume it doesn't all go in a table?! </p>

<pre><code>&gt; summary(m1)
Linear mixed model fit by REML ['lmerMod']
Formula: step ~ Depth * threshold + (1 | ind) 

REML criterion at convergence: 373184 

Random effects:
 Groups   Name        Variance Std.Dev.
 ind      (Intercept) 196519   443.3   
 Residual             469370   685.1   
Number of obs: 23473, groups: ind, 11

Fixed effects:
                  Estimate Std. Error t value
(Intercept)      160.95895  134.80279   1.194
Depth              0.06438    0.44777   0.144
threshold2        51.18065   17.62222   2.904
threshold3         1.47733   21.43879   0.069
Depth:threshold2  -1.23654    0.60029  -2.060
Depth:threshold3  -0.09587    0.65088  -0.147

Correlation of Fixed Effects:
            (Intr) Depth  thrsh2 thrsh3 Dpth:2
Depth       -0.094                            
threshold2  -0.090  0.712                     
threshold3  -0.075  0.588  0.567              
Dpth:thrsh2  0.071 -0.737 -0.745 -0.435       
Dpth:thrsh3  0.064 -0.674 -0.490 -0.857  0.502
</code></pre>

<p>For GLMM 1 I ran this code -</p>

<pre><code>m1&lt;-lmer(step~Depth*threshold+(1|ind))
m2&lt;-lmer(step~(1|ind))
anova(m1,m2)
</code></pre>

<p>For GLMM 2 I ran this code -</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m3&lt;-update(m2,~.-step*threshold)
anova(m2,m3)
</code></pre>

<p>and this one:</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m4&lt;-update(m2,~.-Depth*threshold)
anova(m2,m4)
</code></pre>

<p>The output from the Anova only gives me one p value for each GLMM and I think I need a p value for each of the fixed effects within the models?</p>

<p>Does anyone know what code I can run to get this?
Thank you</p>
"
"0.139520229593756","0.12712320904523"," 93892","<p>I need to get p values for the fixed effects in the following GLMM's I ran. Does anyone know of code that I can run that will give me the p values I need? At the moment the output from the ANOVA only gives me one p value and I believe I need a separate p value for each of the fixed effects in the models. </p>

<p>Thanks in advance.
Code is as follows -</p>

<p>For GLMM 1 I ran this code -</p>

<pre><code>m1&lt;-lmer(step~Depth*threshold+(1|ind))
m2&lt;-lmer(step~(1|ind))
anova(m1,m2)
</code></pre>

<p>For GLMM 2 I ran this code -</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m3&lt;-update(m2,~.-step*threshold)
anova(m2,m3)
</code></pre>

<p>and this one:</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m4&lt;-update(m2,~.-Depth*threshold)
anova(m2,m4)
</code></pre>

<p>When I ran GLMM 1 code this is what I got:</p>

<pre><code>m2: step ~ (1 | ind)
m1: step ~ Depth * threshold + (1 | ind)
Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
m2  3 373235 373259 -186615   373229                            
m1  8 373225 373290 -186605   373209 19.767      5   0.001382 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>summary</p>

<pre><code>&gt; summary(m1)
Linear mixed model fit by REML ['lmerMod']
Formula: step ~ Depth * threshold + (1 | ind) 

REML criterion at convergence: 373184 

Random effects:
 Groups   Name        Variance Std.Dev.
 ind      (Intercept) 196519   443.3   
 Residual             469370   685.1   
Number of obs: 23473, groups: ind, 11

Fixed effects:
                 Estimate Std. Error t value
  (Intercept)      160.95895  134.80279   1.194
Depth              0.06438    0.44777   0.144
threshold2        51.18065   17.62222   2.904
threshold3         1.47733   21.43879   0.069
Depth:threshold2  -1.23654    0.60029  -2.060
Depth:threshold3  -0.09587    0.65088  -0.147

Correlation of Fixed Effects:
            (Intr) Depth  thrsh2 thrsh3 Dpth:2
Depth       -0.094                            
threshold2  -0.090  0.712                     
threshold3  -0.075  0.588  0.567              
Dpth:thrsh2  0.071 -0.737 -0.745 -0.435       
Dpth:thrsh3  0.064 -0.674 -0.490 -0.857  0.502
</code></pre>

<p>OUTPUT FROM SUGGESTED CODE BY SETH (IN COMMENTS)</p>

<pre><code>Models:
m6: step ~ Depth + threshold + (1 | ind)
m5: step ~ Depth + threshold + Depth:threshold + (1 | ind)
   Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)  
m6  6 373227 373275 -186607   373215                           
m5  8 373225 373290 -186605   373209 5.2901      2      0.071 .
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.259937622455018","0.251840884430805"," 94057","<p>I have an agricultural field experiment (testing a plant protection agent):</p>

<p><strong>Split plot design</strong> with: </p>

<pre><code>2 whole plot treatments ""infestation"": ""high"" &amp; ""low"" 
8 split-plot treatments (""treat""): 

1. Untreated Control (""Ctrl1"")
2. Reference Product (""Ctrl2"")
3. 1 x Test-Product 1
4. 2 x Test-Product 1
5. 3 x Test-Product 1
6. 1 x Test-Product 2
7. 2 x Test-Product 2
8. 3 x Test-Product 2

and 4 replicates (""block""):
</code></pre>

<p>The parameter of interest in this example is grain (<strong>yield</strong>):</p>

<p>First, I could model this:</p>

<pre><code>lme(yield ~ infestation * treat, random = ~ 1 | block/infestation, data)
</code></pre>

<p>or</p>

<pre><code> lmer(yield ~ infestation * treat + (1 | block/infestation), data)
</code></pre>

<p>But as can be seen treatments 3-8 can and have to be recoded as 2 products (""prod"") being tested 1-3 times (""times""), so I have a 2x3 subdesign.</p>

<p>One possibility would be subsetting the data: </p>

<pre><code>  data2 &lt;- subset(data, !data$treat == ""Ctrl1"" &amp;  !data$treat == ""Ctrl2"")
</code></pre>

<p>and recode the resting treatments to ""prod"" = 1,2 and ""times"" = 1:3
then run:</p>

<pre><code>lme(yield ~ infestation * form * times, random = ~ 1 | block/infestation, data)
</code></pre>

<p>Afterwards I could still do contrasts to compare the control treatments with the treated ones. </p>

<p>But (here my actual problem starts): I read an article of</p>

<p><strong>H.P. Piepho</strong>: ""<em>A Note on the Analysis of Designed Experiments with Complex Treatment Structure</em>"", 
HortScience 41(2):446--452. 2006</p>

<p>The author wants to show ""<em>how a meaningful analysis can be obtained based on a linear model by appropiate coidng of factors. (...) Our main objective is to demonstrate that the introduction of dummy variables can conveniently solve a wide variety of inferential problems that would otherwise either require ... multiple linear contrasts... or not make fully eficient use of the data, e.g when only data from orthogonal subdesigns are analysed.</em>""  </p>

<p>A very similar example (Example 1 in the article) is discussed within, and an alternative analysis in SAS is proposed - which I wanted to try to realise in R. </p>

<p>The author adds a dummy variable (<strong>ctrl_vs_trt</strong>) to the data and codes it: ""control"", ""trt"" (in my case <strong>trt</strong>, <strong>Ctrl1</strong>, <strong>Ctrl2</strong>"". </p>

<p>The he uses: 
(in his case <strong>prod</strong> is <strong>form</strong> ulation, and <strong>times</strong> is <strong>conc</strong> entration)</p>

<pre><code>PROC GLM;
CLASS block contr_vs_trt form conc;   ## 
MODEL set = block contr_vs_trt
        contr_vs_trt * form
    contr_vs_trt * conc
    contr_vs_trt * form * conc;
RUN.                    
</code></pre>

<p>I cite a further paragraph: 
""<em>Of course, a test for contr_vs_trt is not produced with this model, and one cannot compute simple means or marginal means. Also, the Type I SS for <strong>form</strong>, <strong>conc</strong>, and <strong>form x conc</strong> are not the same as with Type III SS. With Type III SS, the test for form is adjusted for <strong>conc</strong>, as fitting <strong>conc</strong> takes out the control when coding factors as in Table ""xy"" (as I did here). Similarly, the test for <strong>conc</strong> is adjusted for <strong>form</strong>, because fitting of <strong>form</strong> takes out the control. As a result, the Type III ANOVA for the model <strong>form x conc</strong> turns out to be that for the 3x2 factorial subdesign. (...)
It seems much more stringent and transparent to use the nested model <strong>contr_vs_trt/(form x conc)</strong>, as this properly reflects all nesting and crossing features of the design.</em>""</p>

<p>Now, how to do that in <code>lme</code> or <code>lmer</code>?</p>

<p>lme does not run at all, even if I simplify to: </p>

<pre><code> lme(yield ~ prod * times, random = ~1|block, data), 
 I get
 Error in MEEM(object, conLin, control$niterEM) : 
 Singularity in backsolve at level 0, block 1
</code></pre>

<p>The term <strong>prod * times</strong> cannot be run (<strong>prod + times</strong> logically can). Eliminating both controls from the data set resolves this problem. </p>

<p><code>lmer</code> runs with <strong>prod * times</strong>, but always given the message:</p>

<pre><code> fixed-effect model matrix is rank deficient so dropping ""x"" columns / coefficients
</code></pre>

<p>I understand that the subdesign is not orthogonal and therefor dropping is occuring, but I cannot say if the analysis after dropping can still be right. </p>

<p>Also, I do not know how to specify the full model (leaving out the ""infestation"" whole plot for a second):</p>

<pre><code>lmer(yield ~ prod * times + (1|block/ctr_vs_trt), data)
</code></pre>

<p><strong>prod * times</strong> is nested inside <strong>ctr_vs_trt</strong> but both are nested inside the same block (or whole plot).
Is nesting of fixed effects possible in <code>lme</code> or <code>lmer</code> - does it work as I proposed?
Does it even make sense to run the full model?</p>

<p>With <code>aov()</code> I get the model running, even the partitioning of Df's is right. But due to strong non-orthogonality it is not possible to assume that the results are right.</p>

<p>I can get meaningful results subsetting and using contrasts, but I found the authors approach interesting and it would help in the analysis of some of my other trials. 
Thanks in advance for any help; I hope this question is not too long...</p>
"
"0.212479222860543","0.212959402507645"," 94302","<p>I am performing a parametric bootstrap to test whether I need a specific fixed effect in my model or not. I have mainly done this for exercise and I am interested if my procedure so far is correct.</p>

<p>First, I fit the two models to be compared. One of them includes the effect to be tested for and the other one does not. As I am testing for fixed effects I set <code>REML=FALSE</code>:</p>

<pre><code>    mod8 &lt;- lmer(log(value) 
                 ~ matching 
                 + (sentence_type | subject) 
                 + (sentence_type | context), 
                 data = wort12_lmm2_melt,
                 REML = FALSE)
    mod_min &lt;- lmer(log(value) 
                    ~ 1 
                    + (sentence_type | subject)
                    + (sentence_type | context),
                    data = wort12_lmm2_melt,
                    REML = FALSE)
</code></pre>

<p>Both models are fit on a balanced data set which includes few missing values. There are slightly above 11000 observations for 70 subjects. Every subject saw every item only one time. The dependent variable are reading times; sentence_type and matching are two-level factors. Context and subject are treated as random effects. Context has 40 levels.</p>

<p>I call anova():</p>

<pre><code>    anova(mod_min, mod8)
</code></pre>

<p>and get the output:</p>

<pre><code>    Data: wort12_lmm2_melt
    Models:
    mod_min: log(value) ~ 1 + (sentence_type |  subject) + (sentence_type | context)
    mod8: log(value) ~ matching + (sentence_type |  subject) + (sentence_type |   
    mod8:     context)
            Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
    mod_min  8 3317.6 3375.8 -1650.8   3301.6                            
    mod8     9 3310.9 3376.4 -1646.4   3292.9 8.6849      1   0.003209 **
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Mistrusting the almighty p I set up a parametric bootstrap by hand:</p>

<pre><code>    mod &lt;- mod8
    modnull &lt;- mod_min
    lrt.obs &lt;- anova(mod, modnull)$Chisq[2] # save the observed likelihood ratio test statistic
        n.sim &lt;- 10000  
        lrt.sim &lt;- numeric(n.sim)
        dattemp &lt;- mod@frame
        # pb &lt;- txtProgressBar(min = 0, max = n.sim, style = 3) # set up progress bar to satisfy need for control
        for(i in 1:n.sim) {
        # Sys.sleep(0.1) # progress bar related stuff
        ysim       &lt;- unlist(simulate(modnull) # simulate new observations from the null-model  
        modnullsim &lt;- lmer(ysim 
                           ~ 1
                           + (sentence_type | subject)
                           + (sentence_type | context),
                           data = dattemp,
                           REML = FALSE) # fit the null-model
        modaltsim  &lt;- lmer(ysim
                           ~ matching
                           + (sentence_type | subject)
                           + (sentence_type | context),
                           data = dattemp,
                           REML = FALSE) # fit the alternative model
        lrt.sim[i] &lt;- anova(modnullsim, modaltsim)$Chisq[2] # save the likelihood ratio test statistic
    # setTxtProgressBar(pb, i)
    }

    # assuming chi-squared distribution for comparison

    pchisq((2*(logLik(mod8)-logLik(mod_min))),
           df    = 1,
           lower = FALSE)
</code></pre>

<p>with the output:</p>

<pre><code>    'log Lik.' 0.003208543 (df=9)
</code></pre>

<p>compare to parametric bootstrap p-value</p>

<pre><code>    p_mod8_mod_min &lt;- (sum(lrt.sim&gt;=lrt.obs)+1)/(n.sim+1)  # p-value. alternative: mean(lrt.sim&gt;lrt.obs)
</code></pre>

<p>with the output:</p>

<pre><code>    [1] 0.00319968
</code></pre>

<p>Plot the whole thing:</p>

<pre><code>    xx &lt;- seq(0, 20, 0.1)
    hist(lrt.sim,
         xlim     = c(0, max(c(lrt.sim, lrt.obs))),
         col      = ""blue"", 
         xlab     = ""likelihood ratio test statistic"",
         ylab     = ""density"", 
         cex.lab  = 1.5, 
         cex.axis = 1.2, 
         freq     = FALSE)
    abline(v   = lrt.obs,
           col = ""orange"",
           lwd = 3)
    lines(density(lrt.sim),
          col = ""blue"")
    lines(xx,
          dchisq(xx, df = 1),
          col = ""red"")
    box()
</code></pre>

<p>which yields:</p>

<p><img src=""http://i.stack.imgur.com/bh4YO.png"" alt=""enter image description here""></p>

<p>I do have some questions though:</p>

<p>(1) Is the procedure correct or did I make a mistake?</p>

<p>(2) How is the histogram to be interpreted?</p>

<p>(3) Is the form of the histogram normal or extreme?</p>

<p>Thanks for any help!</p>
"
"0.19016088231064","0.17937941173235"," 94888","<p>I'm analysing some behavioural data using <code>lme4</code> in <code>R</code>, mostly following <a href=""http://www.bodowinter.com/tutorials.html"" rel=""nofollow"">Bodo Winter's excellent tutorials</a>, but I don't understand if I'm handling interactions properly. Worse, no-one else involved in this research uses mixed models, so I'm a bit adrift when it comes to making sure things are right.</p>

<p>Rather than just post a cry for help, I thought I should make my best effort at interpreting the problem, and then beg your collective corrections. A few other asides are:</p>

<ul>
<li>While writing, I've found <a href=""http://stackoverflow.com/questions/17794729/test-for-significance-of-interaction-in-linear-mixed-models-in-nlme-in-r"">this question</a>, showing that <code>nlme</code> more directly give p values for interaction terms, but I think it's still valid to ask with relation to <code>lme4</code>.</li>
<li><code>Livius'</code> answer to <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">this question</a> provided links to a lot of additional reading, which I'll be trying to get through in the next few days, so I'll comment with any progress that brings.</li>
</ul>

<hr>

<p>In my data, I have a dependent variable <code>dv</code>, a <code>condition</code> manipulation (0 = control, 1 = experimental condition, which should result in a higher <code>dv</code>), and also a prerequisite, labelled <code>appropriate</code>: trials coded <code>1</code> for this should show the effect, but trials coded <code>0</code> might not, because a crucial factor is missing.</p>

<p>I have also included two random intercepts, for <code>subject</code>, and for <code>target</code>, reflecting correlated <code>dv</code> values within each subject, and within each of the 14 problems solved (each participant solved both a control and an experimental version of each problem).</p>

<pre><code>library(lme4)
data = read.csv(""data.csv"")

null_model        = lmer(dv ~ (1 | subject) + (1 | target), data = data)
mainfx_model      = lmer(dv ~ condition + appropriate + (1 | subject) + (1 | target),
                         data = data)
interaction_model = lmer(dv ~ condition + appropriate + condition*appropriate +
                              (1 | subject) + (1 | target), data = data)
summary(interaction_model)
</code></pre>

<p>Output:</p>

<pre><code>## Linear mixed model fit by REML ['lmerMod']
## ...excluded for brevity....
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  subject  (Intercept) 0.006594 0.0812  
##  target   (Intercept) 0.000557 0.0236  
##  Residual             0.210172 0.4584  
## Number of obs: 690, groups: subject, 38; target, 14
## 
## Fixed effects:
##                                Estimate Std. Error t value
## (Intercept)                    0.2518     0.0501    5.03
## conditioncontrol               0.0579     0.0588    0.98
## appropriate                   -0.0358     0.0595   -0.60
## conditioncontrol:appropriate  -0.1553     0.0740   -2.10
## 
## Correlation of Fixed Effects:
## ...excluded for brevity.
</code></pre>

<p>ANOVA then shows <code>interaction_model</code> to be a significantly better fit than <code>mainfx_model</code>, from which I conclude that there's a significant interaction present (p = .035).</p>

<pre><code>anova(mainfx_model, interaction_model)
</code></pre>

<p>Output:</p>

<pre><code>## ...excluded for brevity....
##                   Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
## mainfx_model       6 913 940   -450      901                          
## interaction_model  7 910 942   -448      896  4.44      1      0.035 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>From there, I isolate a subset of the data for which the <code>appropriate</code> requirement is met (i.e., <code>appropriate = 1</code>), and for it fit a null model, and a model including <code>condition</code> as an effect, compare the two models using ANOVA again, and lo, find that <code>condition</code> is a significant predictor.</p>

<pre><code>good_data = data[data$appropriate == 1, ]
good_null_model   = lmer(dv ~ (1 | subject) + (1 | target), data = good_data)
good_mainfx_model = lmer(dv ~ condition + (1 | subject) + (1 | target), data = good_data)

anova(good_null_model, good_mainfx_model)
</code></pre>

<p>Output:</p>

<pre><code>## Data: good_data
## models:
## good_null_model: dv ~ (1 | subject) + (1 | target)
## good_mainfx_model: dv ~ condition + (1 | subject) + (1 | target)
##                   Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
## good_null_model    4 491 507   -241      483                          
## good_mainfx_model  5 487 507   -238      477  5.55      1      0.018 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>
"
"0.142373699362875","0.126840398441198"," 97929","<p>My data is described here <a href=""http://stats.stackexchange.com/questions/97165/what-can-cause-a-error-model-is-singular-error-in-aov-when-fitting-a-repeate"">What can cause a &quot;Error() model is singular error&quot; in aov when fitting a repeated measures ANOVA?</a></p>

<p>I am trying to see the effect of an interaction using <code>lmer</code> so my base case is:</p>

<pre><code>my_null.model &lt;- lmer(value ~ Condition+Scenario+ 
                             (1|Player)+(1|Trial), data = my, REML=FALSE)

my.model &lt;- lmer(value ~ Condition*Scenario+ 
                             (1|Player)+(1|Trial), data = my, REML=FALSE)
</code></pre>

<p>Running the <code>anova</code> gives me significant results, but when I try to account for random slope (<code>(1+Scenario|Player)</code>) the model fails with this error: </p>

<pre><code>  Warning messages:
 1: In commonArgs(par, fn, control, environment()) :
   maxfun &lt; 10 * length(par)^2 is not recommended.
 2: In optwrap(optimizer, devfun, getStart(start, rho$lower, rho$pp),  :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
 3: In commonArgs(par, fn, control, environment()) :
  maxfun &lt; 10 * length(par)^2 is not recommended.
 4: In optwrap(optimizer, devfun, opt$par, lower = rho$lower, control = control,  :
   convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
 5: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
   Model failed to converge with max|grad| = 36.9306 (tol = 0.002)
 6: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
   Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
</code></pre>

<p>Alternatively if it fails to converge after a lot of iterations (I set it to <code>100 000</code>) and I am getting the same results after <code>50k</code> and <code>100k</code> it means that it is very close to the actual value, just it does not reach it. So can I report my results like this?</p>

<p>Note that when I set the iterations so high I get only these warnings:</p>

<pre><code> Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
 Model failed to converge with max|grad| = 43.4951 (tol = 0.002)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
 Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
</code></pre>
"
"0.126554399433667","0.142695448246348"," 99765","<p>I'm trying to run a tree-way repeated measures ANOVA on the following data: I have a completely balanced design with three within-subject factors (type, form and ch - channel) and one dependent variable amp (amplitude).</p>

<p>I'm inclined to believe the results I got using <code>aov</code> function:</p>

<pre><code>res &lt;- aov(amp ~ type*form*ch + Error(sbj/(type*form*ch)), data = p3vals)
</code></pre>

<p>Here is the anova table I have:</p>

<pre><code>Error: Within
               Df Sum Sq Mean Sq F value   Pr(&gt;F)    
type            1   25.0  24.950  12.315 0.000462 ***
form            1   12.9  12.910   6.372 0.011693 *  
ch              1    0.9   0.875   0.432 0.511113    
type:form       1    3.1   3.123   1.542 0.214581    
type:ch         1    0.9   0.938   0.463 0.496404    
form:ch         1    1.3   1.256   0.620 0.431239    
type:form:ch    1    3.0   2.960   1.461 0.226974    
Residuals    1514 3067.3   2.026                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, after reading several pages of similar examples(including <a href=""http://stats.stackexchange.com/questions/14088/why-do-lme-and-aov-return-different-results-for-repeated-measures-anova-in-r"">Why do lme and aov return different results for repeated measures ANOVA in R?</a> ) I decided to try <code>lme</code> and <code>lmer</code> functions from name and lme4 packages for further pairwise multiple comparisons using <code>glht</code> from multcomp package.
In the example mentioned above F values are at least closet those obtained using <code>aov</code>, but I cannot figure out how to get any meaningful results.</p>

<pre><code>lme_p3amp = lmer(amp ~ type*form*ch + (1|sbj) + (1|type:sbj) + (1|form:sbj) + (1|ch:sbj), data = p3vals)
&gt; anova(lme_p3amp)
Analysis of Variance Table
             Df  Sum Sq Mean Sq F value
type          1  0.0266  0.0266  0.0433
form          1  2.0782  2.0782  3.3863
ch            1 28.5513 28.5513 46.5227
type:form     1  2.1980  2.1980  3.5815
type:ch       1  2.9789  2.9789  4.8539
form:ch       1  0.9278  0.9278  1.5118
type:form:ch  1  6.6072  6.6072 10.7661
</code></pre>

<p>and lme produces the following result:</p>

<pre><code> anova(lme(amp ~ type*form*ch, random=list(sbj=pdBlocked(list(~1, pdIdent(~type-1), pdIdent(~form-1), pdIdent(~ch-1)))), data=p3vals))
             numDF denDF  F-value p-value
(Intercept)      1  1508 32.56485  &lt;.0001
type             1  1508  0.02920  0.8643
form             1  1508  3.54422  0.0599
ch               1  1508  8.05747  0.0046
type:form        1  1508  2.79623  0.0947
type:ch          1  1508  3.78969  0.0518
form:ch          1  1508  1.18037  0.2775
type:form:ch     1  1508  8.40553  0.0038
</code></pre>

<p>I'd appreciate if you tell me what is wrong with my code and how can I perform a valid post hoc analysis.</p>
"
"0.0821994936526786","0.0823852554571635","100060","<p>Maybe it's a basic question, but I'm learning about GLMM using the lme4 package. I'm confused about the way that I can know the significance the overall model using glmer.</p>

<p>First, the random model is:</p>

<pre><code>fit.random &lt;- glmer(VDEP ~ AGE +GENDER +EDUC +V1 +V2 +V3 +(1|STATE), family = binomial(""logit""), data = mydata, nAGQ = 0)
</code></pre>

<p>From Stackoverflow's ask (<a href=""http://stackoverflow.com/questions/23802033/significance-of-the-overall-model-glmer?noredirect=1#comment36684401_23802033"">link</a>) help me with the next null model.</p>

<pre><code>fit.null &lt;- update(fit.random,.~1+(1|STATE))
</code></pre>

<p>Then I can do anova and test the significance of the overall model.</p>

<p>Is correct this conjecture? 
How is the best way to test the significance of the overal model? 
It will be interesting make a pseudo r squared?
There are other important tests (overdispersion, VIF)? (I know that is a very general question, but I only want some ideas to continue learning).</p>

<p>Finally thanks for any help and sorry for the format mistakes.</p>
"
"0.0474578997876249","0.0475651494154494","101566","<p>I am fitting a <code>glmer</code> model in the <code>lme4</code> R package. I'm looking for an anova table with p-value shown therein, but I cannot find any package that fits it. Is it possible to do it in R?</p>

<p>The model I am fitting is of the form:</p>

<pre><code>model1&lt;-glmer(dmn~period*teethTreated+(1|fullName), 
   family=""poisson"", 
   data=subset(dataset, 
          group=='Four times a year'),
   control=glmerControl(optimizer=""bobyqa""))
</code></pre>
"
"0.107624400500126","0.107867619794493","103104","<p>For a simulation study, I contrast the power of different LMEMs for repeated measures. To get p-values, I use likelihood ratio tests where I compare a model including a fixed treatment effect with one that has the same random effects structure but without having the fixed treatment effect. I want to specify a model in which random intercept and slope are allowed to correlate and one in which it is not allowed. But, when I extract the p-values of both models, it appears that they are exactly the same. Why? I am not an expert for mixed models but that seems weird. Here's my code using lmer syntax:</p>

<pre><code># correlation allowed
  ml3 &lt;- lmer(rt ~ treatment + (1+treatment|subject),data=df)
  ml0 &lt;- lmer(rt ~ 1 + (1+treatment|subject),data=df)
  lrt &lt;- anova(ml3,ml0)
  pVal &lt;- lrt$""Pr(&gt;Chisq)""[2]

# no correlation
  ml2 &lt;- lmer(rt ~ treatment + (1|subject) + (0+treatment|subject) ,data=df)
  ml0 &lt;- lmer(rt ~ 1 + (1|subject) + (0 + treatment|subject),data=df)
  lrt &lt;- anova(ml2,ml0)
  pVal &lt;- lrt$""Pr(&gt;Chisq)""[2]
</code></pre>

<p>Data:</p>

<pre><code>dput(DF)
structure(list(subject = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 10L, 10L, 10L, 10L, 
10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 
10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 11L, 11L, 
11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 
11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 
11L, 11L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 
12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 
12L, 12L, 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 
13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 
13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 
14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 
14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 
14L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 
15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 
15L, 15L, 15L, 15L, 15L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 
16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 
16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 17L, 
17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 
17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 
18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 
18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 
18L, 18L, 18L, 18L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 
19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 
19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 20L, 20L, 20L, 20L, 20L, 
20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 
20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 21L, 
21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 
21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 
21L, 21L, 21L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 
22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 
22L, 22L, 22L, 22L, 22L, 22L, 22L, 23L, 23L, 23L, 23L, 23L, 23L, 
23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 
23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 24L, 24L, 
24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 
24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 
24L, 24L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 
25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 
25L, 25L, 25L, 25L, 25L, 25L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 
26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 
26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 27L, 27L, 27L, 
27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 
27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 
27L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 
28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 
28L, 28L, 28L, 28L, 28L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 
29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 
29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 30L, 30L, 30L, 30L, 
30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 
30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L
), .Label = c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", 
""11"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", 
""22"", ""23"", ""24"", ""25"", ""26"", ""27"", ""28"", ""29"", ""30""), class = ""factor""), 
    treatment = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L), .Label = c(""1"", ""2"", ""3""), class = ""factor""), 
    rt = c(551.798792586772, 693.014255128461, 715.599061613616, 
    670.119831344829, 777.748610260388, 736.018489208224, 636.791011800404, 
    864.593711496912, 604.529352905588, 596.673178487122, 860.858066937491, 
    717.975814131377, 531.672833100059, 571.150454430927, 644.315598150879, 
    601.914697283216, 583.92746647402, 702.714068138085, 660.346853172676, 
    541.292786332608, 608.233103066463, 740.593415976325, 686.059921551164, 
    706.522723402261, 567.648255604935, 596.111352599386, 625.779084220279, 
    752.776987343973, 922.314285125596, 720.736074757203, 768.585671134519, 
    539.657760625667, 431.193030969184, 739.341430343149, 581.505474510558, 
    485.905153431116, 524.085545405872, 876.566370460358, 631.259754679943, 
    587.887351105621, 624.365050240473, 642.528438460209, 440.661792577731, 
    517.142782023978, 705.840003729944, 557.122142924839, 645.711579229236, 
    477.292943229673, 578.522058679457, 623.879658296107, 480.855063147831, 
    622.295733392922, 611.623490658329, 594.974733982977, 546.239019853272, 
    551.638287622872, 567.791819285002, 539.239628365136, 541.328446070423, 
    609.931976806498, 549.492601324081, 479.862984098331, 592.411150981731, 
    466.224011597179, 489.388878789762, 565.187127159354, 806.196199699478, 
    565.001071713299, 449.529036961143, 446.824243314547, 357.993777663337, 
    370.054045045062, 546.443479822161, 473.894296409884, 335.821704077378, 
    370.498649523398, 486.052525038318, 436.53292033153, 359.637460079864, 
    333.146018287273, 597.894114487158, 551.993792800734, 518.563432886515, 
    513.629383189428, 572.062676720248, 366.611317255576, 353.934207291842, 
    492.273303938824, 414.632984933654, 456.987565377718, 585.524844348671, 
    453.577328112778, 665.072248688078, 459.204631254183, 452.028605442515, 
    422.731506299078, 522.84363619892, 771.586286956136, 478.422495080758, 
    530.925498291748, 457.030618882822, 446.313635696342, 537.708665959068, 
    815.929601138346, 460.420404065423, 603.027278932425, 538.526470664698, 
    571.491835856551, 567.234631106499, 450.878624452358, 650.340961680322, 
    598.829083718722, 620.85411026516, 573.498196791879, 519.953442801483, 
    1143.14393274202, 505.028670926264, 685.875665196364, 605.316954852204, 
    645.269429082978, 678.192056130499, 605.671978651269, 650.564580984954, 
    641.331733928499, 687.164180542278, 613.873849203194, 789.829709495785, 
    560.793473918547, 707.169378961089, 680.753196215641, 659.262985906231, 
    600.712008959484, 662.275074291484, 661.346206480403, 568.31000899618, 
    661.439508442242, 761.227769640367, 699.901658463283, 631.422448673388, 
    734.257735977184, 585.776345181453, 714.587957176744, 893.931699334816, 
    586.343993838929, 664.205207596859, 828.003782888565, 906.448165648461, 
    584.196768113385, 747.575564348236, 687.698668648395, 398.221092516595, 
    490.332613905338, 502.683377386602, 451.168200674477, 620.606534311108, 
    369.820458042713, 429.483129392912, 628.153937257066, 476.31856841443, 
    608.016880363378, 402.588700424079, 460.302916138576, 341.209753425326, 
    600.165531243784, 454.003777748405, 589.089266888531, 504.033320854066, 
    399.871492203846, 421.426563579218, 423.375093277487, 587.614013919312, 
    689.18637317583, 652.10069672704, 553.995320740249, 570.86533170596, 
    636.399559100471, 801.517490092303, 653.425223465164, 684.914139340214, 
    639.418654954543, 555.718100869331, 677.062768399616, 579.433200999322, 
    561.757369869387, 672.316124102021, 701.108131071079, 588.129426947175, 
    438.090900053591, 562.520435558598, 610.177372103278, 564.672192806652, 
    552.305226838045, 586.912866128373, 872.521433158083, 654.971253063189, 
    575.068762782096, 784.098108527601, 648.265348029739, 590.541840435637, 
    552.569131260877, 554.840084955354, 582.798864712891, 573.196470707737, 
    512.123960183202, 579.838037093289, 710.216216611067, 779.786949219207, 
    615.995650564573, 549.096392807351, 600.781394864656, 415.016144351654, 
    765.924387691343, 401.541419432177, 436.050367769487, 536.508634405116, 
    445.112952169149, 478.003493101049, 509.819044087032, 490.265270275681, 
    594.667876389766, 781.844411855516, 827.832351086729, 379.401116898897, 
    469.280230588986, 397.115839743604, 874.524377877882, 612.130504039819, 
    802.270319490186, 651.842161968928, 581.489774054855, 686.457677143518, 
    570.172663082147, 566.996565453736, 577.947675356248, 494.016772046721, 
    546.065861910691, 506.178677541412, 527.653822550596, 470.043764013502, 
    595.080116592997, 464.590366280242, 684.362069491853, 534.310814471562, 
    545.7046301149, 452.141529834992, 619.652055160652, 568.61376011316, 
    576.847350527713, 514.248803061826, 585.909312171032, 687.892034205561, 
    907.133281713537, 549.603068658537, 617.860688444804, 423.424246676122, 
    524.28348263976, 593.203848577403, 431.733188413523, 476.284708033659, 
    588.88583865225, 437.275988819986, 733.45270912684, 592.366412341047, 
    606.958434204909, 721.61902078205, 604.596941234802, 534.65440311647, 
    526.29928462654, 655.076689084035, 560.740728878595, 591.083376633783, 
    533.331301213643, 750.32841350028, 547.366173885661, 602.313382446308, 
    787.158938523176, 534.80106549099, 454.37886245909, 599.535859565986, 
    607.126697674517, 668.173760533712, 589.060272311024, 590.188448587092, 
    711.910534337354, 528.634489779135, 600.468858484032, 580.666817624455, 
    659.907090614686, 596.395917159692, 994.163737779338, 662.059444540888, 
    637.256716085147, 714.353436812361, 587.212427691626, 676.527668439672, 
    609.004414569998, 667.364031145608, 788.145350832559, 725.891539439069, 
    561.397498270981, 430.027498616446, 500.437956195847, 463.801763917305, 
    458.358780003907, 383.304386810731, 598.957692241571, 409.89510543858, 
    390.650415086637, 552.072907469115, 388.554580583084, 671.244783776164, 
    433.841093781351, 423.187562794827, 502.566122911232, 469.869008810394, 
    547.610270179268, 501.091740213331, 417.336826115574, 500.284514580019, 
    460.835882303962, 650.071068396249, 523.313503950421, 861.366681123829, 
    879.241985731583, 673.655630620254, 448.199583227711, 578.587129494665, 
    654.742597624172, 623.62363768736, 665.526175470944, 942.738238156293, 
    1006.15443845549, 667.679153732234, 711.686114156855, 642.929069350516, 
    685.862290196822, 1062.63097632175, 758.162511396556, 827.547233897549, 
    668.688764986398, 791.497544557741, 838.143090686178, 681.935257212825, 
    758.732997665222, 661.724656922782, 793.560801116029, 896.416624866383, 
    642.617709357462, 633.832129070135, 751.515360586321, 616.151652306802, 
    684.496510560379, 655.310039878885, 710.298048482024, 606.373767619465, 
    754.268924528687, 822.582103710613, 820.556840434073, 733.785411148237, 
    584.830824784288, 588.316573524589, 572.95505735157, 559.402915982595, 
    640.891735376065, 482.407652048448, 569.682285396545, 517.277707765673, 
    698.102946480301, 651.001615070688, 665.691471843539, 511.440973330271, 
    504.930464361447, 613.891964397534, 454.7073692139, 513.19138352863, 
    422.708112768038, 347.049510934991, 523.980248957572, 480.301125161823, 
    633.307276361827, 799.987010744151, 533.354042715484, 410.150445477125, 
    809.568249688128, 531.41976915349, 792.355614308461, 747.208014043674, 
    607.571115317023, 546.485408007754, 633.55875460818, 767.73368427773, 
    676.492693414302, 751.649529779836, 984.189814104173, 684.929427919003, 
    615.787024482925, 567.942282464503, 571.041675151281, 614.028930539252, 
    779.839834490734, 630.179209124113, 651.603035032816, 788.591687415382, 
    799.38918489533, 701.842888543902, 693.932887722425, 624.800556024233, 
    659.981040765196, 542.243217216484, 721.703181143723, 607.818766172507, 
    586.813797432694, 610.206652108693, 837.694469363876, 763.535995041537, 
    758.89830766469, 616.838182390385, 578.107924042397, 628.314074464124, 
    676.917384461922, 635.824489980127, 495.143374853889, 815.582155744321, 
    534.740299502999, 568.853739307473, 771.28519095763, 673.064072347686, 
    713.558399193608, 599.316767121742, 689.796480377465, 673.175516507747, 
    518.390229271871, 784.139459988779, 536.808895738866, 591.342581848355, 
    563.762291009613, 679.413099342014, 490.762928348403, 575.612328735691, 
    504.631070884374, 689.919729220693, 545.809277581445, 641.095314483731, 
    618.332256267043, 641.913937397485, 953.129874375365, 646.628853366556, 
    631.881991258933, 671.647395089865, 503.290799393907, 506.506064370266, 
    529.718096437596, 484.255438291713, 861.643688089666, 625.018895601203, 
    768.279996867868, 596.708155034627, 671.714642028838, 844.072568247028, 
    578.694918479722, 422.588349061727, 594.493346157147, 520.812331184257, 
    741.876339265066, 555.516494731537, 579.386393427601, 622.316950052304, 
    523.488853303438, 595.901305255518, 553.512680895547, 557.643582245011, 
    624.788623102115, 479.715363427417, 574.354660431126, 524.472350214463, 
    660.841590121958, 608.621321258764, 631.743182107793, 711.470012104646, 
    617.432792370567, 573.363544694191, 617.449976333406, 563.127159530709, 
    583.25391667852, 678.936105477067, 572.153554376884, 609.829503412847, 
    606.069768210344, 693.198276061625, 647.952198803514, 508.091779167254, 
    654.226813385831, 530.92016927824, 504.963210966908, 512.488303835862, 
    763.325818301401, 664.628862733417, 477.385861339593, 566.148674353306, 
    578.70068655976, 606.967024346421, 697.460752784057, 662.304772796768, 
    520.905460930742, 629.14344808993, 815.023764792173, 680.359748369552, 
    628.317980877129, 679.505810999772, 512.999611466799, 656.728389486035, 
    548.409794219861, 619.925877003775, 581.949057396067, 663.545400676099, 
    666.518874913722, 691.701483159255, 616.896649470106, 595.504424960074, 
    574.172251324537, 552.787259430621, 678.018976276998, 607.329759814185, 
    611.581207725935, 690.981992177989, 564.832150097104, 540.336710300887, 
    533.631062681699, 553.612294126468, 479.592612257575, 805.559491265258, 
    528.304765655223, 496.66528049325, 503.82305630743, 696.245302300331, 
    566.070769246181, 659.391688013324, 697.528902380277, 524.903347139913, 
    599.821891499886, 605.43297053286, 663.035359384042, 714.444647793395, 
    578.371129029246, 615.320894052349, 586.420779403222, 611.255799029828, 
    583.666658817928, 585.113768358993, 590.122958856932, 629.219469590256, 
    538.928053998428, 686.894125956954, 600.89266348967, 577.953187882193, 
    554.413905790583, 579.409330807908, 745.040084235899, 891.087132273406, 
    552.988614856682, 820.955690300634, 634.469321378978, 673.975047013567, 
    756.507601731563, 639.35059215201, 722.557986588015, 586.447409643988, 
    656.432427481585, 682.783381677787, 644.716232116734, 572.306442663379, 
    517.147784935371, 455.957276558869, 581.819706567048, 484.576715810217, 
    481.058650198264, 769.846887958231, 614.717393882487, 512.55467514312, 
    562.454770697369, 470.842224095898, 473.821865893767, 525.60888531351, 
    504.615217687803, 633.463711580414, 697.478798243637, 542.901502870182, 
    554.078075963646, 552.734146037028, 505.049122827182, 486.831379133217, 
    784.789844765716, 625.603196289942, 670.995369035953, 685.579926259636, 
    540.482850768361, 439.117039069522, 502.605387735171, 683.149979103402, 
    516.322278257158, 642.3671240847, 552.631029671279, 647.736853458454, 
    560.328629000192, 730.565596415312, 618.355157539931, 663.62525661894, 
    762.952777957374, 628.608584740535, 624.968635247218, 661.70575556195, 
    574.702252033339, 612.175432188694, 517.854558597715, 675.314287039473, 
    542.173699486536, 627.693153783529, 692.014742774091, 739.358457751625, 
    594.351746882543, 487.81864701434, 584.028504991851, 772.039898984639, 
    467.800944704621, 699.587386648698, 711.892383008835, 615.312970618784, 
    675.467367812567, 504.86811313757, 549.182867476271, 399.814725143066, 
    558.16544073586, 422.858340371991, 636.197179849367, 452.640724786824, 
    620.185492727861, 463.138266913543, 513.777642749675, 562.005709606924, 
    536.148107655772, 609.263894686695, 569.500530985324, 431.43349765191, 
    584.398797922899, 499.315449753743, 619.406287942484, 530.179301913325, 
    581.286972325074, 609.256211854971, 603.54936265609, 631.687639186526, 
    657.738657905408, 662.587176694764, 566.971357910094, 858.130855520899, 
    636.509916564228, 961.258701590037, 654.287721552112, 722.951283166332, 
    632.132002720104, 487.318833408862, 568.846556179602, 618.040023485574, 
    496.276276900436, 575.584711170303, 581.733646148308, 429.189732200854, 
    542.010860963286, 494.264804962282, 422.488093162063, 458.026827419797, 
    453.276318995818, 469.307056127784, 569.3897375524, 579.968164508765, 
    499.796867133562, 434.308570876294, 715.325075398682, 716.351002032214, 
    628.027210297141, 500.606473063414, 481.830926575354, 613.318935569666, 
    976.743117685661, 644.275338785824, 478.107026795071, 557.000656140104, 
    705.452526296914, 485.589962586432, 499.786618070234, 558.712600821937, 
    590.922333630969, 542.410481740128, 489.828649613243, 588.761989730902, 
    546.978243344109, 606.196264556647, 564.236942812287, 536.823507783435, 
    579.521762239388, 515.499050687035, 630.900495035976, 656.332012380511, 
    587.856990348358, 522.305185738772, 739.045222536055, 730.745657623434, 
    924.844562132056, 784.35778551794, 602.551997131679, 647.756594982111, 
    698.734409940058, 836.546703053691, 698.539340777214, 1026.87197547421, 
    629.904387631378, 1256.38776880042, 550.201102905894, 602.768212743634, 
    721.187968335008, 978.957227830475, 720.414204972345, 674.653879707098, 
    788.5848329187, 719.806764303146, 717.541641935441, 692.033638361742, 
    697.745018761048, 666.284578038868, 772.479906582772, 659.065480010219, 
    820.683359002167, 676.970146466469, 874.368371442289, 727.177048657942, 
    698.30875424695, 779.956903736863, 845.624757018358, 713.683982932567, 
    726.314529163246, 619.106987740244, 602.275574812157, 1012.31009024481, 
    900.357542354599, 644.181000629658, 696.237940779481, 723.968447714559, 
    728.433707295704, 738.277840656323, 624.899240530917, 602.62262026982, 
    741.637155120584, 771.780154658976, 655.858038042129, 746.047425940232, 
    620.210165076071, 727.335465168732, 575.696484591354, 664.112900876799, 
    635.568414156208, 813.389693290661, 782.803606862186, 611.54036820155, 
    626.182943686362, 629.796912199246, 850.589128120044, 751.317837909887, 
    698.370084520991, 684.731100139964, 675.610584876072, 746.655183232173, 
    700.916031879367, 672.999065959173, 754.165715908428, 1000.39209579409, 
    711.494061908177, 641.566680754116, 804.073919378063, 684.705472972499, 
    703.074833865838, 461.242335766073, 427.275150289403, 448.904300292751, 
    504.467553858542, 468.445048327652, 492.18239230431, 441.918536235364, 
    516.654435536123, 552.509951287832, 409.431285285276, 435.346043325067, 
    547.743500224329, 531.812309405229, 364.578903523756, 508.390034089605, 
    379.665384727272, 509.53781675453, 357.585528744983, 441.32777083335, 
    443.431829371385, 606.59281583625, 686.319960511486, 625.7347791126, 
    738.463790041674, 563.274182025531, 464.717827380926, 507.784383381725, 
    588.175980498022, 413.597103546393, 718.241261054521, 748.95432032953, 
    753.3423119569, 624.488936342165, 685.583685719862, 556.272453690569, 
    553.85940984926, 547.648759204925, 600.572261526898, 798.618024234413, 
    726.941645252511, 571.72160960877, 550.034219181198, 627.264280360843, 
    678.989621824996, 643.317665763982, 562.10080966811, 612.803383559254, 
    681.139896626253, 602.318525361381, 725.533189662524, 624.989203686985, 
    757.711095633453, 672.222798550503, 814.596048431427, 661.778896020412, 
    598.861275115565, 720.798422223033, 607.767472082705, 786.918370564599, 
    609.319159908796, 654.545951651791, 590.381966203786, 580.331158503126, 
    515.973182460188, 652.661059652561, 545.988014818838, 695.948607980764, 
    541.582595061958, 589.514921567389, 543.237248780138, 629.781187086802, 
    495.114814971547, 713.705169086826, 604.33689023605, 515.485770936165, 
    519.257552917307, 699.375113218082, 538.736700962025, 475.293688282428, 
    575.221421714611, 504.071893399439, 480.754098260713, 529.183372881182, 
    600.773688635732, 528.753221287108, 623.467259878089, 524.506017554373, 
    578.950778202312, 558.484848311201, 548.077100964434, 628.137735553388, 
    536.855310919075, 671.288977759216, 656.468309447081, 554.138278777839, 
    652.139385673766, 576.644938824018, 623.229206449598, 690.260009908557, 
    908.122945156817, 512.745373098672, 512.90318083329, 732.257651656802, 
    542.85407478119, 497.700590782599, 523.882208542902, 448.976579619777, 
    563.466660067041, 505.398939062326, 668.814284148356, 512.653186957994, 
    503.040433891059, 433.619712591384, 464.197386550985, 408.88198638402, 
    681.920233753602, 504.514813136438, 570.133904166935, 491.416987899975, 
    440.029552147731)), .Names = c(""subject"", ""treatment"", ""rt""
), class = ""data.frame"", row.names = c(NA, -900L))
</code></pre>

<p>Thanks in advance</p>
"
"0.125561800583481","0.125845556426908","105064","<p>I struggle performing a MANOVA for a mixed model. I have the following columns:</p>

<p>S - subject (1,...,N)
B - in between group (B1, B2)
W - repeated measurement - within (W1, W2)
M - measured dependent variable 1
N - measured dependent variable 2</p>

<p>My data is organized the following way:</p>

<pre><code>S; B; W;   M;   N
1;B1;W1;12.3;14.4
1;B1;W2;35.4;13.5
2;B1;W1;11.1;22.2
2;B1;W2;33.3;11.1
3;B2;W1;15.3;12.3
3;B2;W2;20.3;18.8
...
</code></pre>

<p>If I had only one dependent variable (M), I could obtain the results of an ANOVA in one of these ways:</p>

<pre><code>dat=read.csv(file=""data.csv"", sep="";"")
library(ez)
ezANOVA(dat, dv=.(M), wid=.(S), between=.(B), within=.(W), type=3, detailed=TRUE)

library(lmerTest)
results = lmerTest::lmer(M ~ B*W + (1 | S), data=dat)
anova(results, ddf=""Kenward-Roger"", type=3, method.grad=""Richardson"")

library(car)
options(contrasts=c(""contr.sum"", ""contr.poly""))
mod1 &lt;- lme4::lmer(M ~ B*W + (1 | S), data=dat)
car::Anova(mod1, test.statistic=""F"", type=3)
</code></pre>

<p>I wonder, how the code for the MANOVA has to look like? I tried</p>

<pre><code>dat=read.csv(file=""vor_actions_new.csv"", sep="";"")
results = manova(cbind(M, N) ~ B*W, data=dat)
summary.aov(results)
summary(results)
</code></pre>

<p>but I doubt, the within-factor is handled correctly. I also tried</p>

<pre><code>mod1 &lt;- lme4::lmer(cbind(M, N) ~ B*W + (1 | S), data=dat)
</code></pre>

<p>but, this gave an error, probably because multiple responses are not supported.  ezAnova does also not support multiple responses, I think. Is there a way to execute the planned MANOVA?</p>
"
"0.222597281091997","0.22310032643658","105906","<blockquote>
  <p>The bounty I placed on this question expires in the next 24 hours.</p>
</blockquote>

<p>I have a psychological data set which, traditionally, would be analysed using a paired samples t test.
The design of the experiment is $39 (subjects) \times 7 (targets) \times 2 (conditions)$, and I'm interested in the difference in a given variable between the conditions.</p>

<p>The traditional approach has been to average across targets so that I have 2 observations per participant, and then compare these averages using a paired t test.</p>

<p>I wanted to use a mixed models approach, as has become increasingly popular in this field (i.e. <a href=""http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenDavidsonBates.pdf"" rel=""nofollow"">Baayen, Davidson &amp; Bates, 2008</a>), and so the first model I fit, which I thought should approximate the results of the t test, was one with $condition$ as a fixed effect, and random intercepts for $subjects$ (i.e. $var = \alpha + \beta*condition + Intercept(subject) + \epsilon$. Obviously, the full model would also include random intercepts for $targets$.</p>

<p>However, I'm struggling to understand why I achieve pretty divergent results between the two approaches.
Can anyone explain what's going on here?
I've also seen (what I understand to be) a similar question asked <a href=""http://stats.stackexchange.com/questions/23276/paired-t-test-as-a-special-case-of-linear-mixed-effect-modeling-still-unresolve"">here</a>, with an answer about correlation structure which I'm not equipped to understand. If this is also what's at issue here, I would appreciate if anyone could suggest some resources to read up on this.</p>

<p><strong>Edit:</strong> I've posted <a href=""https://gist.github.com/EoinTravers/ce86c93fb42fba284464"" rel=""nofollow"">the example data, and R script, here</a>.</p>

<p><strong>Edit #2 - Bounty added</strong></p>

<p>Some additional points:</p>

<ul>
<li>I'm only analysing the correct responses (think of it as analogous to reaction time), so there are <strong>missing cases</strong> - not every participant provides 7 data points per condition.
<ul>
<li>When I analyse all responsees, rather than just the correct ones, the difference between the two results is reduced, but not eliminated. This suggests to me that the missing cases are a factor here.</li>
</ul></li>
<li>The variable isn't normally distributed. In my final model, I scale it using a Box-Cox transformation, but I omit that here for consistency with the t test.</li>
<li>As pointed out by @PeterFlom, the $df$s differ hugely between the two approaches, but I assume this to be because the t test is being applied to the aggregate data (2 observations per participant, 1 per condition), while the mixed model is applied to raw scores ($&lt;14$ observations per participant, $&lt;7$ per condition).</li>
<li>@BenBolker notes that the t values also differ pretty considerably.</li>
</ul>

<p>My analysis code is below.</p>

<pre><code>&gt;library(dplyr)
&gt;subject_means = group_by(data, subject, condition) %&gt;% summarise(var=mean(var))
&gt;t.test(var ~ condition, data=subject_means, paired=T)

    Paired t-test

data:  var by condition
t = -1.3394, df = 37, p-value = 0.1886
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.14596388  0.02978745
sample estimates:
mean of the differences 
            -0.05808822 

&gt;library(lme4)
&gt;lm.0 = lmer(var ~ (1|subject), data=data)
&gt;lm.1 = lmer(var ~ condition + (1|subject), data=data)
&gt;anova(lm.0, lm.1)

Data: data
Models:
object: var ~ (1 | subject)
..1: var ~ condition + (1 | subject)
       Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)  
object  3 489.09 501.23 -241.55   483.09                           
..1     4 485.81 502.00 -238.90   477.81 5.2859      1     0.0215 *

&gt;library(lmerTest)
&gt;summary(lm.1)$coef

              Estimate Std. Error        df  t value     Pr(&gt;|t|)
(Intercept) 0.11862462 0.02878027  98.60659 4.121734 7.842075e-05
condition   0.09580546 0.04161237 400.27441 2.302331 2.182890e-02
</code></pre>

<p>Notice, specifically, the jump in the p value from $p = .188$ in the t test, to $p = .021$ from either <code>lmer</code> method.</p>

<hr>

<p>I've tried, and failed to provide a reproducible example of this, using the <code>anorexia</code> dataset in the <code>MASS</code> package, so I would assume the problem is something idiosyncratic to my data, but I don't understand what.</p>

<pre><code># Borrowing from http://ww2.coastal.edu/kingw/statistics/R-tutorials/dependent-t.html
&gt;data(anorexia, package=""MASS"")
&gt;ft = subset(anorexia, subset=(Treat==""FT""))
&gt;wgt = c(ft$Prewt, ft$Postwt)
&gt;pre.post = rep(c(""pre"",""post""),c(17,17))
&gt;subject = rep(LETTERS[1:17],2)
&gt;t.test(wgt~pre.post, data=ft.new, paired=T)

    Paired t-test

data:  wgt by pre.post
t = 4.1849, df = 16, p-value = 0.0007003
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  3.58470 10.94471
sample estimates:
mean of the differences 
               7.264706 

&gt;m = lmer(wgt ~ pre.post + (1|subject), data=ft.new)
&gt;summary(m)$coef

             Estimate Std. Error       df   t value     Pr(&gt;|t|)
(Intercept) 90.494118   1.689013 26.17129 53.578096 0.0000000000
pre.postpre -7.264706   1.735930 15.99968 -4.184908 0.0007002806
</code></pre>
"
"0.142373699362875","0.142695448246348","106079","<p>I am estimating a random intercept and a random slope model using the following R code. My dependent and independent variable are both continuous.</p>

<pre><code>randominterceptfixedslope&lt;-lmer(y ~ x + (1|state),data=data,method=""ML"") # model with fixed slope but random intercept
randominterceptrandomslope&lt;-lmer(y ~ x + (1+x|state),data=data,method=""ML"") # model with random slope and random intercept

anova(randominterceptfixedslope,randominterceptrandomslope)
</code></pre>

<p>Anova tells me that my randominterceptrandomslope model is a better fit on the data. So far good, please correct me if I am wrong. </p>

<p>My question is: If I have another independent variable $x_1$, can I put two independent variables in the above model i.e. can I have a randominterceptfixedslope and randominterceptrandomslope model with two independent variables. If yes, how do I do that? As in what the code should look like?</p>

<p>Thanks for your response. I got a second query. lets say my full model is this: </p>

<pre><code>     randominterceptrandomslope&lt;-lmer(y ~ x1 + x2 + x3 + x4 (1+x1+x2+x3+x4|state),data=data,method=""ML"")
</code></pre>

<p>If some of my independent variables are correlated, what is the procedure of reducing the collinearity issue in a linear mixed effect model?  I could spot collinerity  using VIF and retain the most significant independent variables but I can do this for each factor level (levels of state) individually. But won't it result in retaining some independent variables in one factor level while deleting the same in other factor level? I guess the main question is how to spot collinearity in a mixed effect model and what to do with it when you have 5 or 6 independent variables?</p>
"
"NaN","NaN","108161","<p>The <code>lmerTest</code> package provides an ANOVA function for linear mixed effects models with optionally Satterthwaite's (default) or Kenward-Roger's approximation of the degrees of freedom. What is the difference between these two approaches? When to choose which?</p>
"
"0.227600091831708","0.218196423717792","108280","<p>I know that there are already a host of questions about nested designs but many of them haven't been answered or come from biological domains which I sometimes find hard to transfer to my domain.</p>

<p>I am currently trying to analyse the data from a psychological study. Participants read four statements and indicated how appropriate they found each on a 5-point Likert-scale. But I'll just call the outcome variable ""out"" and so the content will not distract from the statistical problem at hand.</p>

<p>There are two IVs, one that was varied on three levels between subjects (let's call it ""between"" and the levels 1, 2 and 3) and one on two levels within subjects (""within"" with its levels a and b). The within subjects factor is realised in such a manner that two of the four statements correspond to one of the levels and the other two to the other. </p>

<p>Correspondingly, for every participant, I now have four ""out"" data points, two for the a statements and two for the b statements, like this (in long format):</p>

<pre><code>subj    between within  item    out
123     1       a       a1      3
123     1       a       a2      4
123     1       b       b1      1
123     1       b       b2      2
124     2       a       a1      5
124     2       a       a2      4
124     2       b       b1      2
124     2       b       b2      3
125     3       a       a1      1
125     3       a       a2      1
125     3       b       b1      2
125     3       b       b2      3
</code></pre>

<p>and so on
What is the right way to analyse the effect and interaction of between and within? I assume that the two different a and the two different b statements do not differ systematically but are just two instances of the same thing. Or do I need an additional factor that tells R which of the four statements it is in order to allow for that error? Anyway, I have tried these options:</p>

<pre><code>m1 &lt;- aov(out~between*within+Error(subj/within))
summary(m1)

m2 &lt;- lm(out~between*within+within/subj)
anova(m2)

m3 &lt;- lmer(out~between*within+(between|subj))
Anova(m3)
</code></pre>

<p>They produce different results but I'm not entirely sure about which one is right or what the differences are. Could anyone enlighten me on this? I assume this is about fixed and random effects which always gets my head in a twist. I have read other posts about nested designs and I think <a href=""https://stats.stackexchange.com/questions/94882/repeated-measures-mixed-model-using-lmer-in-r"">this</a> one comes nearest. But unfortunately the question ID being nested groups wasn't answered. Any help would be very much appreciated!</p>

<p><strong>Edit:</strong><br>
I greatly appreciate the answers so far! The comments prompted me to read some more tutorials on mixed effects models as well as answers to similar questions. This enables me to clarify my question: I know that I will need to specify random intercepts at least for subjects and probably also the four different items representing the two levels of the within factors. However, I am unsure about the specific fixed/random effects structure I would have to model, and specifically what the maximal model in my case would look like.
Right now, I have tried some code which roughly looks like this:</p>

<pre><code>m1 &lt;- lmer(out ~ between * within + (between|subj) + (within|subj) + (between|within/item))
</code></pre>

<p>This is probably wrong on several levels so any feedback would be greatly appreciated! Just for clarity: The item factor has no intrinsic meaning, but both levels of the within factor is realised by two items each (which every participant sees) and I am unsure whether this is relevant to the model.</p>

<p>Apart from the model specification, I am still getting confused about the concepts nested and crossed and which one applies to my setup. Lastly, most of my models fail to converge but that is probably a matter for another question (and has been discussed here in length.)</p>
"
"0.0896870004167719","0.125845556426908","108677","<p>I have used lme4 for mixed effects models of reaction times and accuracy rates. I could not use lmerTest because the type of model I was using are not yet implemented there (problem with predictors that are factors). I was able to get p-values for the models ran on accuracy rates (based on Wald z-values) but not for the models built on reaction times.
In order to get p-values for all models, I used Anova in the car package which gives me Wald chisquare values and probability of significance based on those chisquares. 
My concern is that sometimes for the accuracy rates, the effects indicated as significant in the analysis of deviance table (with the Wald chisquare values) are not significant in the mixed effect models. That is even for main effects of a factor with only 2-levels.
Does anyone know why this could be the case?</p>
"
"0.1898315991505","0.190260597661798","111535","<p>This is my first post, so sorry if it not optimally written. I have a paired samples at two time points in two groups, undergoing the same intervention. I want to test the effect of my intervention on weight.</p>

<p>I'm using R. Here's some data to illustrate: my data frame called <code>df</code>:</p>

<pre><code>      patients   timepoint        group          Weight
        102            1            1            107.30
        104            1            1             94.10
        117            1            1            110.80
        121            1            1            108.90
        153            1            1             95.40
        155            1            1            105.10
        161            1            1             97.70
        162            1            1             83.60
        167            1            1             82.40
        173            1            1             86.40
        176            1            1             81.90
        177            1            1             90.90
        179            1            1             95.30
        101            1            2             81.30
        108            1            2             72.30
        113            1            2             68.50
        170            1            2             89.20
        171            1            2             77.50
        172            1            2             94.50
        175            1            2             78.30
        181            1            2             71.40
        182            1            2             72.80
        183            1            2             73.50
        186            1            2             87.90
        187            1            2             83.50
        188            1            2             70.10
        102            2            1            102.70
        104            2            1             90.40
        117            2            1            107.30
        121            2            1            107.50
        153            2            1             95.00
        155            2            1            102.80
        161            2            1             95.40
        162            2            1             78.30
        167            2            1             81.90
        173            2            1             85.30
        176            2            1             83.10
        177            2            1             90.50
        179            2            1             97.50
        101            2            2             78.40
        108            2            2             72.00
        113            2            2             66.80
        170            2            2             90.20
        171            2            2             77.60
        172            2            2             93.40
        175            2            2             80.30
        181            2            2             72.60
        182            2            2             71.40
        183            2            2             74.20
        186            2            2             88.70
        187            2            2             80.50
        188            2            2             71.20
</code></pre>

<p>Since this is paired data (between time points) and unpaired (between groups), I guess I must use a mixed linear model. Im going for the lme4 package in R.</p>

<p>""timepoints"" and ""group"" will be my fixed effects (I exhaust both). ""patients"" will be my random effect, which also picks up that I have multiple responses per patient. </p>

<p>I will use a random intercept model, but I actually expect that my patients differ with how they react to my experimental manipulation, so a random slopes model would be nice also. However, it seams I over-parametrize the model if doing so.</p>

<p>I will use the likelighood ratio using anova() for a full model and a reduced model.</p>

<pre><code>full = lmer(Weight ~ timepoint + group + (1|patients), data=df,
        REML=FALSE)

reduced = lmer(Weight ~ group + (1|patients), data=df,
                   REML=FALSE)
</code></pre>

<p>""timepoint"" is the main factor in question. I now test using anova():</p>

<pre><code>&gt; anova(reduced,full)
Data: df
Models:
reduced: Weight ~ group + (1 | patients)
full: Weight ~ timepoint + group + (1 | patients)
        Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
reduced  4 309.72 317.52 -150.86   301.72                          
full     5 306.02 315.78 -148.01   296.02 5.695      1    0.01701 *
</code></pre>

<p>Question is, Im I doing it correctly? And how do I interpret the result?</p>

<p>BUT, what I really want is to test if the effect of time is different on the two groups. How do I do this?</p>

<p>Thank you.</p>
"
"0.135067550667224","0.150414209399047","111836","<p>In R, using package lme4, I have used the following 2 mixed models to determine I have a signifacnt interaction between a covariate (continous, normally distributed) and a factor (three levels: herbivores, plants, predators):</p>

<pre><code>test1 &lt;- lmer( mode ~ sr * func.group + (1|community), data=nr.test, REML=""FALSE"")
test2 &lt; -lmer( mode ~ sr + func.group + (1|community), data=nr.test, REML=""FALSE"")
anova(test1, test2)

Data: nr.test
Models:
test2: mode ~ sr + func.group + (1 | community)
test1: mode ~ sr * func.group + (1 | community)
     Df    AIC    BIC  logLik  Chisq Chi Df Pr(&gt;Chisq)    
nr.test2  6 77.458 82.093 -32.729                             
nr.test1  8 62.570 68.751 -23.285 18.887      2  7.919e-05 ***
</code></pre>

<p>I have obviously plotted the interaction for each level of factor, and all three levels are positive relationships which cross over each other (i.e. different levels of slope steepness).</p>

<p>However, I would like to be able to identify which level of the factor are significantly different from one another, and which are also significantly different from a slope value of zero (i.e. which slopes are significant). </p>

<p>I have installed the R package phia, and used the command </p>

<pre><code>testInteractions(test1, pairwise=""func.group"", slope=""sr"")
</code></pre>

<p>and recieved the following output:</p>

<pre><code>Adjusted slope for sr 
Chisq Test: 
P-value adjustment method: holm
                     Value Df   Chisq Pr(&gt; Chisq)    
    herbivore-plant 0.11878  1  4.3061     0.03798 *  
 herbivore-predator 0.40567  1 65.3283   1.902e-15 ***
     plant-predator 0.28689  1 30.3474   7.224e-08 ***
</code></pre>

<p>It would appear for each level of factor the slopes are significantly different from one another, but how can I know which ones are significantly different from zero, and how would I report this?</p>
"
"0.143090951758036","0.114731457242355","113682","<p>According to this document: <a href=""http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf"" rel=""nofollow"">http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf</a> </p>

<p>I can get the overall p-value of fix effects by comparing models that I would like to know to null model.</p>

<p>In my case I would like to know the effect of three-way interaction. 
So first, I run the null model:</p>

<pre><code>lmer86 &lt;- lmer(IntensityMax ~ (1|item) + (1+vowel3|speaker) + sex + vowel3 + Language, data=data1.frame, REML=FALSE, na.action=na.omit)
</code></pre>

<p>Then I run the model that I would like to test:</p>

<pre><code>lmer81 &lt;- lmer(IntensityMax ~ (1|item) + (1+vowel3|speaker) + sex*vowel3*Language, data=data1.frame, REML=FALSE, na.action=na.omit)
</code></pre>

<p>And finally, I compare these models using ANOVA function:</p>

<pre><code>anova(lmer81,lmer86)
</code></pre>

<p>And this is result:</p>

<pre><code>Data: data1.frame
Models:
lmer86: IntensityMax ~ (1 | item) + (1 + vowel3 | speaker) + sex + vowel3 + 
lmer86:     Language
lmer81: IntensityMax ~ (1 | item) + (1 + vowel3 | speaker) + sex * vowel3 * 
lmer81:     Language
       Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
lmer86 14 13349 13432 -6660.5    13321                             
lmer81 26 13319 13474 -6633.6    13267 53.813     12  2.951e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Then I know that three-way interaction is an important effect. Am I on the right track?</p>
"
"0.0671156055214024","0.0672672793996312","113690","<p>I have used ANOVA function so that I can get the overall p value of significant factors:</p>

<pre><code>&gt; anova(lmer81)
Analysis of Variance Table
                    Df Sum Sq Mean Sq F value
sex                  1   0.12   0.118  0.0195
vowel3               2 399.96 199.982 33.0859
Language             2 120.41  60.204  9.9604
sex:vowel3           2  89.73  44.865  7.4227
sex:Language         2 166.93  83.463 13.8084
vowel3:Language      4  48.27  12.067  1.9964
sex:vowel3:Language  4  52.76  13.189  2.1821
</code></pre>

<p>However, this does not give me p value. Can I ask how I can get p value from this?</p>
"
"0.0821994936526786","0.0823852554571635","114689","<p>Tissue samples were taken from 4 differention locations and repeatedly measured. This was done identically for 3 animals. The research question was: Are there differences in measurement between the locations?</p>

<p>=> Repeated-measures design, with location as within-subjects factor</p>

<pre><code>m1 &lt;- lmer(meas ~ location + (1 + location | animal))  
</code></pre>

<p>I would like to test for a main effect of location by a likelihood ratio test via anova(model, nullmodel) (I am aware of the differing opinions regarding this).</p>

<p>But I am unclear, which, or if any, of the following two is the correct null model:</p>

<pre><code>null.1 &lt;- lmer(meas ~ 1 + (1 | animal), REML=FALSE)
null.2 &lt;- lmer(meas ~ 1 + (1 + location | animal), REML=FALSE)
</code></pre>

<p>I am inclinced to think itâ€™s null.1, but I am not entirely sure.</p>

<p>Formulated more generally:
When specifying a maximal random effects structure for a within-subjects factor, what is the null model for that factor for a maximum-likelihood ratio test?</p>
"
"0.272624878403135","0.264960950893567","115065","<p>I am working with data from a computer task which has 288 total trials, each of which can be categorically classified according to <strong>Trial Type</strong>, <strong>Number of Stimuli</strong>, and <strong>Probe Location</strong>.  Because I want to also examine a continuous variable, the total Cartesian <strong>Distance</strong> between stimuli per trial (divided by number of stimuli to control for varying numbers), I have opted to use a mixed linear model with repeated measures.  In addition to each of these task variables, I am also interested in whether folks in various diagnostic groups perform differently on the task, as well as whether or not there is a <strong>Dx</strong> interaction with any of the above variables.  Thus (if I'm not mistaken), I have the following effects in my model:</p>

<p><strong>Trial Type</strong>, a fixed effect
<strong>Number of Stimuli</strong>, a fixed effect
<strong>Probe Location</strong>, a fixed effect
<strong>Dist</strong>(ance), a fixed effect
<strong>Dx</strong>, a fixed effect
<strong>Dx*Trial Type</strong>, a fixed effect
<strong>Dx*Number of Stimuli</strong>, a fixed effect
<strong>Dx*Probe Location</strong>, a fixed effect
<strong>Dx*Dist</strong>, a fixed effect
<strong>Trial</strong>, a random effect, nested within
<strong>SubID</strong>, a random effect</p>

<p>Based on my examination of documentation, it seems that the nesting of random effects does not seem to be important to lme4, and so I specify my model as follows:</p>

<p><code>tab.lmer &lt;- lmer(Correct ~  Dx+No_of_Stim+Trial_Type+Probe_Loc+Dist+Dx*No_of_Stim+Dx*Trial_Type+Dx*Probe_Loc+Dx*Dist+(1|Trial)+(1|SubID),data=bigdf)</code></p>

<p>This would be my first question: <strong>1) Is the above model specification correct?</strong></p>

<p>Assuming so, I am a bit troubled by my results, but as I read and recall my instruction on such models, I am wondering if interpretation of particular coefficients is bad practice in this case:</p>

<pre><code>Linear mixed model fit by REML ['merModLmerTest']
Formula: Correct ~ Dx + No_of_Stim + Trial_Type + Probe_Loc + Dist + Dx *  
    No_of_Stim + Dx * Trial_Type + Dx * Probe_Loc + Dx * Dist +  
    (1 | Trial) + (1 | SubID)
   Data: bigdf

REML criterion at convergence: 13600.4

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.89810 -0.03306  0.27004  0.55363  2.81656 

Random effects:
 Groups   Name        Variance Std.Dev.
 Trial    (Intercept) 0.013256 0.11513 
 SubID    (Intercept) 0.006299 0.07937 
 Residual             0.131522 0.36266 
Number of obs: 15840, groups:  Trial, 288; SubID, 55

Fixed effects:
                         Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)             4.196e-01  4.229e-02  4.570e+02   9.922  &lt; 2e-16 ***
DxPROBAND               8.662e-02  4.330e-02  2.920e+02   2.000  0.04640 *  
DxRELATIVE              9.917e-02  4.009e-02  2.920e+02   2.474  0.01394 *  
No_of_Stim3            -9.281e-02  1.999e-02  4.520e+02  -4.642 4.53e-06 ***
Trial_Type1             3.656e-02  2.020e-02  4.520e+02   1.810  0.07097 .  
Probe_Loc1              3.502e-01  2.266e-02  4.520e+02  15.456  &lt; 2e-16 ***
Probe_Loc2              3.535e-01  3.110e-02  4.520e+02  11.369  &lt; 2e-16 ***
Dist                    1.817e-01  2.794e-02  4.520e+02   6.505 2.06e-10 ***
DxPROBAND:No_of_Stim3  -1.744e-02  1.759e-02  1.548e+04  -0.992  0.32144    
DxRELATIVE:No_of_Stim3 -2.886e-02  1.628e-02  1.548e+04  -1.773  0.07628 .  
DxPROBAND:Trial_Type1  -9.250e-03  1.777e-02  1.548e+04  -0.521  0.60267    
DxRELATIVE:Trial_Type1  1.336e-02  1.645e-02  1.548e+04   0.812  0.41682    
DxPROBAND:Probe_Loc1   -8.696e-02  1.993e-02  1.548e+04  -4.363 1.29e-05 ***
DxRELATIVE:Probe_Loc1  -4.287e-02  1.845e-02  1.548e+04  -2.323  0.02018 *  
DxPROBAND:Probe_Loc2   -1.389e-01  2.735e-02  1.548e+04  -5.079 3.83e-07 ***
DxRELATIVE:Probe_Loc2  -8.036e-02  2.532e-02  1.548e+04  -3.173  0.00151 ** 
DxPROBAND:Dist         -3.920e-02  2.457e-02  1.548e+04  -1.595  0.11066    
DxRELATIVE:Dist        -1.485e-02  2.275e-02  1.548e+04  -0.653  0.51390    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In general, these results make sense to me.  The troubling portion, however, comes in the positive, significant (yes, I am using LmerTest) p-value for DxProband, particularly in light of the fact that in terms of performance means, Probands are performing worse than Controls.  So, this mismatch concerns me.  Examining the corresponding ANOVA:</p>

<pre><code>&gt; anova(tab.lmer)
Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq Mean Sq NumDF   DenDF F.value    Pr(&gt;F)    
Dx             0.8615  0.4308     2   159.0   1.412   0.24662    
No_of_Stim     0.6984  0.6984     1   283.5  37.043 3.741e-09 ***
Trial_Type     8.3413  8.3413     1   283.5   4.456   0.03565 *  
Probe_Loc     25.7223 12.8612     2   283.5 116.405 &lt; 2.2e-16 ***
Dist           5.8596  5.8596     1   283.5  43.399 2.166e-10 ***
Dx:No_of_Stim  1.4103  0.7051     2 15483.7   1.590   0.20395    
Dx:Trial_Type  2.0323  1.0162     2 15483.7   0.841   0.43128    
Dx:Probe_Loc   3.5740  0.8935     4 15483.7   7.299 7.224e-06 ***
Dx:Dist        0.3360  0.1680     2 15483.7   1.277   0.27885    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...the results seem to more or less line up with the regression, with the exception of the <strong>Dx</strong> variable.  So, my second question is <strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong></p>

<p>Finally, as a basic (and somewhat embarrassing) afterthought, <strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p>In summation,
<strong>1) Is the above model specification correct?</strong>
<strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong>
<strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p><strong>ADDENDUM</strong></p>

<p>By request, I'll describe the task and data a little further.  The data come from a computer task in which participants are presented a number of stimuli, either two or three, in various locations about the screen.  These stimuli can either be ""targets"" or ""distractors"".  After these stimuli, a probe stimulus is presented; if it appears in a position where a previous target has appeared, participants should respond ""yes""; if it appears in the position of a previous distractor or elsewhere, the correct answer is ""no.""  There are 288 trials of this nature; some have two stimuli and some have three, and some lack distractors entirely.  The variables in my model, then, can be elaborated as follows:</p>

<p><strong>Number of Stimuli:</strong> 2 or 3 (2 levels)</p>

<p><strong>Trial Type:</strong> No Distractor (0) or Distractor (1) (2 levels)</p>

<p><strong>Probe Location:</strong> Probe at Target (1), Probe at Distractor (2), or Probe Elsewhere (0) (3 levels)</p>

<p><strong>Distance:</strong> Total Cartesian distance between stimuli, divided by number of stimuli per trial (Continuous)</p>

<p><strong>Dx:</strong> Participant's clinical categorization</p>

<p><strong>Sub ID:</strong> Unique subject identifier (random effect)</p>

<p><strong>Trial:</strong> Trial number (1:288) (random effect)</p>

<p><strong>Correct:</strong> Response classification, either incorrect (0) or correct (1) per trial</p>

<p>Note that the task design makes it inherently imbalanced, as trials without distractors cannot have Probe Location ""Probe at Distractor""; this makes R mad when I try to run RM ANOVAs, and it is another reason I opted for a regression.</p>

<p>Below is a sample of my data (with SubID altered, just in case anyone might get mad):</p>

<pre><code>     SubID      Dx Correct No_of_Stim Trial_Type Probe_Loc      Dist Trial
1 99999999 PROBAND       1          3          0         1 0.9217487     1
2 99999999 PROBAND       0          3          0         0 1.2808184     2
3 99999999 PROBAND       1          3          0         0 1.0645292     3
4 99999999 PROBAND       1          3          1         2 0.7838786     4
5 99999999 PROBAND       0          3          0         0 1.0968788     5
6 99999999 PROBAND       1          3          1         1 1.3076598     6
</code></pre>

<p>Hopefully, with the above variable descriptions these data should be self-explanatory.</p>

<p>Any assistance that people can provide in this matter is very much appreciated.</p>

<p>Sincerely,
peteralynn</p>
"
"0.134231211042805","0.134534558799262","115473","<p>I am using a mixed effects model as created here (using dummy data for now) in this R script</p>

<pre><code>#### Mixed effects model ####
require(lme4)
require(nlme)

# make data
data = data.frame(c(rep(1:10,each=100)), c(rep(1:4,each = 10)), c(rep(1:200, each=5)),c(rep(1:10)),rnorm(1000,65,5)); colnames(data) = c(""Line"",""Block"",""Vial"",""Ind"",""A"")
data$A = ifelse(data$Line==1, data$A + 10, data$A)
data$A = ifelse(data$Block==4, data$A + 5, data$A)

# plot data
par(mfrow=c(1,3))
boxplot(data$A~data$Line)
boxplot(data$A~data$Block)
boxplot(data$A~data$Vial)

# null and alternative models
nul.data = lmer(A ~ 1 + (1|Block) + (1|Vial), data = data, REML = F)
alt.data = lmer(A ~ 1 + (1|Line) + (1|Block) + (1|Vial), data = data, REML = F)

# does line improve model fit?
anova(nul.data,alt.data)
summary(alt.data)
</code></pre>

<p>I am testing whether there is variance in the variable <strong>A</strong> as explained by <strong>Line</strong>, which is a random effect (of ten groups) because Line is drawn from a random population of genotypes, while <strong>block</strong> and <strong>vial</strong> are also replication of the measures thus treated as random effects, with one measure (of <strong>A</strong>) per individual (<strong>Ind</strong>). </p>

<p>In this dummy data I have made it so that Line and Block are variable, and specifically <strong>Line 1 and Block 4 are different from the rest</strong> (as can be seen in the boxplots). How would I statistically show that Line 1 has a higher value of A? What would be the appropriate test?</p>

<p>(I am looking at 38 lines of Drosophila and we expect 4 specific lines to differ from the rest - and possibly from each other. e.g. 2 of those lines might be very high in variable A, 2 might be very low, and the remaining 34 might be middle).</p>
"
"0.0474578997876249","0.0475651494154494","116935","<p>say we have to glmms</p>

<pre><code>mod1 &lt;- glmer(y ~ x + A + (1|g), data = dat)
mod2 &lt;- glmer(y ~ x + B + (1|g), data = dat)
</code></pre>

<p>These models are not nested in the usual sense of:</p>

<pre><code>a &lt;- glmer(y ~ x + A + (1|g),     data = dat)
b &lt;- glmer(y ~ x + A + B + (1|g), data = dat)
</code></pre>

<p>so we can't do <code>anova(mod1, mod2)</code> as we would with <code>anova(a ,b)</code></p>

<p>Can we use an AIC approach to say which is the best model instead?</p>
"
"0.0711868496814374","0.0951302988308988","117660","<p>My question is based on <a href=""http://stats.stackexchange.com/a/13816/442"">this response</a> which showed which <code>lme4::lmer</code> model corresponds to a two-way repeated measures ANOVA:</p>

<pre><code>require(lme4)
set.seed(1234)
d &lt;- data.frame(
    y = rnorm(96),
    subject = factor(rep(1:12, 4)),
    a = factor(rep(1:2, each=24)),
    b = factor(rep(rep(1:2, each=12))),
    c = factor(rep(rep(1:2, each=48))))

# standard two-way repeated measures ANOVA:
summary(aov(y~a*b+Error(subject/(a*b)), d[d$c == ""1"",]))

# corresponding lmer call:
anova(lmer(y ~ a*b+(1|subject) + (1|a:subject) + (1|b:subject), d[d$c == ""1"",]))
</code></pre>

<p>My question now is on how to extend this to the case of a three-way ANOVA:</p>

<pre><code>summary(aov(y~a*b*c+Error(subject/(a*b*c)), d))
## [...]
## Error: subject:a:b:c
##           Df Sum Sq Mean Sq F value Pr(&gt;F)
## a:b:c      1  0.101  0.1014   0.115  0.741
## Residuals 11  9.705  0.8822 
</code></pre>

<p>The natural extension as well as versions thereof do not match the ANOVA results:</p>

<pre><code>anova(lmer(y ~ a*b*c +(1|subject) + (1|a:subject) + (1|b:subject) + (1|c:subject), d))
## [...]
## a:b:c  1 0.1014  0.1014  0.1500

anova(lmer(y ~ a*b*c +(1|subject) + (1|a:subject) + (1|b:subject) + (1|c:subject) + 
               (1|a:b:subject) + (1|a:c:subject) + (1|b:c:subject), d))
## [...]
## a:b:c  1 0.1014  0.1014  0.1539
</code></pre>

<p>Note that a very similar question has been asked <a href=""http://stats.stackexchange.com/q/99765/442"">before</a>. However, it was missing example data (which is provided here).</p>
"
"0.0821994936526786","0.0823852554571635","117964","<p>I am a bachelor student in biology and for a project work, I have a model with a design like this (A, B &amp; C are fixed factors, D is random and nested in C): </p>

<pre><code>lmer1 = lmer(y~A+B+C+A:B+A:C+B:C+A:B:C+(1+A+B|D:C))  
summary(lmer1)
</code></pre>

<p>If A:B:C is not significant, I can simplify the model by removing this term:</p>

<pre><code>lmer2 = lmer(y~A+B+C+A:B+A:C+B:C+(1+A+B|D:C))  
anova (lmer1,lmer2)  
summary(lmer2)
</code></pre>

<p>If now the p value from the ANOVA table >0.05, I can proceed with lmer2. But here is my question: how should I simplify further if there are still unsignificant fixed and random terms? Should the next step be removing A:B (or A:C or B:C) from the fixed part or removing from the random part of the model?</p>
"
"0.150075056296916","0.150414209399047","118475","<p>I have an unbalanced linear mixed effects model with three fixed factors of various levels and one random factor for my repeated measures data (<a href=""http://stats.stackexchange.com/questions/99742/how-to-analyze-interdependent-interaction-terms-of-lmer-model"">for details see here</a>).
Thanks to your help I managed to do post-hoc tests on the significant interaction terms using <code>lsm</code> from the <strong>lsmeans</strong> package. However, I need to report the F statistic (F value and degrees of freedom) for these post-hoc tests and wonder how???</p>

<p>Here is what I do:</p>

<ol>
<li><p>Model comparison using <code>anova()</code> resulting into the final model
<code>model_final</code>, which reads:</p>

<p><code>sc ~ time + cond + place + time:cond + cond:place + (1|ID), data)</code> . </p></li>
<li><p>I analyze the significant interaction time:cond using <code>lsmeans</code>:</p>

<p><code>posthoc_1 &lt;- glht(model_final, lsm(pairwise ~ cond|time)</code></p>

<p><code>summary(posthoc_1)</code></p></li>
</ol>

<p>and get sth like below for each level of <code>time</code>, here is the example for <code>time1</code>.</p>

<pre><code>&gt; Note: df set to 268 
&gt;
&gt; $`time = time1`
&gt; 
&gt;    Simultaneous Tests for General Linear Hypotheses
&gt; 
&gt; Fit: lme4::lmer(formula = sc ~ time + cond + place + time:cond + cond:place + (1|ID), data)
&gt; 
&gt; Linear Hypotheses:
&gt;                    Estimate Std. Error t value Pr(&gt;|t|) 
&gt; cond1 - cond2 == 0   3.1867     0.6797   4.688 4.39e-06 ***
</code></pre>

<p>This gives me t-values for the various levels of the interaction terms and their corresponding p-value, but no F stats!</p>

<p>My questions: </p>

<ol>
<li>Is there any way of obtaining the F stats? (F value and degree of
freedom) </li>
<li>Or am I stuck with the t-values? If so, is t(0.095;268) =
4.588, p &lt; 0.001 reporting the correct degrees of freedom?</li>
</ol>
"
"0.201626270989554","0.191446033415183","120650","<p>Field explains how to analyse repeated-measures data using linear mixed-effect models (LME). See Field et al., Discovering Statistics Using R, 2012, p. 573.</p>

<p>However, the way he specifies the model, there appears to be <strong>only one observation per level of the grouping factors</strong>. Is this a mistake in the textbook? If not, why not? It seems to me the random effects specify a full model and fit the data (almost) exactly.</p>

<p>The code is as follows:</p>

<pre><code>library(reshape2)
library(nlme)
# Load dataset:
dat.wide &lt;- read.delim(""http://www.sagepub.com/dsur/study/DSUR%20Data%20Files/Chapter%2013/Bushtucker.dat"")

dat.wide
#   participant stick_insect kangaroo_testicle fish_eye witchetty_grub
# 1          P1            8                 7        1              6
# 2          P2            9                 5        2              5
# 3          P3            6                 2        3              8
# 4          P4            5                 3        1              9
# 5          P5            8                 4        5              8
# 6          P6            7                 5        6              7
# 7          P7           10                 2        7              2
# 8          P8           12                 6        8              1


dat &lt;- melt(dat.wide, variable.name=""animal"", value.name=""retch"")
head(dat)
#   participant       animal retch
# 1          P1 stick_insect     8
# 2          P2 stick_insect     9
# ...

# set contrasts, not relevant to question but keeps example same as in book:
PartvsWhole &lt;- c(1, -1, -1, 1)
TesticlevsEye &lt;- c(0, -1, 1, 0)
StickvsGrub &lt;- c(-1, 0, 0, 1)
contrasts(dat$animal) &lt;- cbind(PartvsWhole, TesticlevsEye, StickvsGrub)

# Fit intercept term, then add ""animal"" term.
# NOTE: random effects are animal nested within participant, as in textbook.
# This would presumably give one observation per group?    
lme1 &lt;- lme(retch ~ 1, random=~1|participant/animal, data=dat, method=""ML"")
lme2 &lt;- lme(retch ~ 1 + animal, random=~1|participant/animal, data=dat, method=""ML"")

# I have checked these are the same results as in textbook
anova(lme1, lme2)    
summary(lme2)

# residuals are near-zero (e-05)
resid(lme2)

ran1 &lt;- random.effects(lme1)
ran2 &lt;- random.effects(lme2)
# same number of random effects as observations:
nrow(ran1$animal)
    nrow(ran2$animal)
nrow(dat)
</code></pre>

<p>The concern of one observation per level seems to be confirmed by using package lme4 instead:</p>

<pre><code>library(lme4)
# Using lme4 produces an error:
# ""Error in checkNlevels(reTrms$flist, n = n, control) : 
# number of levels of each grouping factor must be &lt; number of observations""    
lmer1 &lt;- lmer(retch ~ 1 + (1|participant/animal), data=dat, REML=F)
lmer2 &lt;- lmer(retch ~ 1 + animal + (1|participant/animal), data=dat, REML=F)
anova(lmer1, lmer2)

# see http://stackoverflow.com/questions/19713228/lmer-returning-error-grouping-factor-must-be-number-of-observations
# can force fit with lme4:
# control=lmerControl(check.nobs.vs.nlev = ""ignore"",
#                     check.nobs.vs.rankZ = ""ignore"",
#                     check.nobs.vs.nRE=""ignore""))

lmer1 &lt;- lmer(retch ~ 1 + (1|participant/animal), data=dat, REML=F,
              control=lmerControl(check.nobs.vs.nlev=""ignore"",
                                  check.nobs.vs.rankZ=""ignore"",
                                  check.nobs.vs.nRE=""ignore""))
lmer2 &lt;- lmer(retch ~ 1 + animal + (1|participant/animal), data=dat, REML=F,
              control=lmerControl(check.nobs.vs.nlev=""ignore"",
                                  check.nobs.vs.rankZ=""ignore"",
                                  check.nobs.vs.nRE=""ignore""))
# ignoring errors, we get the same results as with lme:
anova(lmer1, lmer2)
anova(lme1, lme2) # same
</code></pre>

<p>Should the random term should be 1|participant instead?</p>
"
"0.150075056296916","0.150414209399047","120768","<p>I'm using <code>glmer()</code> with a binomial response variable. My optimal model has two fixed effects (flow and DNA) which in summary() show a non-significant p value but when I remove each fixed effect in turn from the model the likelihood ratio test comparing the two models shows a significant p value. I'm struggling to understand (1) if this is normal, and (2) how to report the results if the explanatory variables ""flow"" and ""DNA"" are important but their p values in the model are well above 0.05?</p>

<p>Optimal model:</p>

<pre><code>a25 &lt;- glmer(Status_qpcr~(1|Root)+Flow+DNA,
             family=binomial, data=spore)
summary(a25)

Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['glmerMod']  
Family: binomial  ( logit ) 
Formula: Status_qpcr ~ (1 | Root) + Flow + DNA   
Data: spore
      AIC      BIC   logLik deviance df.resid 
     72.9     81.0    -32.4     64.9       52 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.9318 -0.8163  0.4435  0.6848  1.6133 

Random effects:  
  Groups Name        Variance Std.Dev.  
  Root   (Intercept) 0.3842   0.6199   
  Number of obs: 56, groups:  Root, 9

Fixed effects:
Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -0.97752    0.79252  -1.233    0.217   
Flow         3.82779    2.27165   1.685    0.092 . 
DNA          0.01616    0.01039   1.556    0.120  
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr) Flow   Flow -0.775        
     DNA    -0.576  0.227
</code></pre>

<p>Likelihood ratio test:</p>

<pre><code>a26 &lt;- update(a25,~.-DNA)
anova(a25,a26)

Data: spore 
Models: 
    a26: Status_qpcr ~ (1 | Root) + Flow 
    a25: Status_qpcr ~ (1 | Root) + Flow + DNA
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
a26  3 74.802 80.878 -34.401   68.802                            
a25  4 72.897 80.998 -32.448   64.897 3.9049      1    0.04815 *

a27 &lt;- update(a25,~.-Flow)
anova(a25,a27)

Data: spore 
Models: 
    a27: Status_qpcr ~ (1 | Root) + DNA 
    a25: Status_qpcr ~ (1 | Root) + Flow + DNA
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
a27  3 78.440 84.723 -36.220   72.440                             
a25  4 72.897 80.998 -32.448   64.897 7.5427      1   0.006025 **
</code></pre>
"
"0.177571201301144","0.165260171758799","121593","<p>The experiment this data comes from an experiment where two people collaborate to put objects in a specific order. The Direction has the target array on their screen, and the Matcher has a scrambled array on their screen that must be reorganized to match the Director's display. The Director is a confederate using a script. After a few rounds using the same objects in different orders, there is a critical round.</p>

<p>The measure of interest is the reaction time (RT) of the Matcher to look at a target object on a screen based on whether the object was referred to using a previously established label (Old Label) or a novel label (New Label) and whether the interaction partner was the same partner as before (Same Partner) or a new partner (Different Partner). Each participant experienced 8 critical rounds, such that there are 2 observations in each cell of the design per participant, with each critical round using a different target object. Thus, we have a 2 (Label) x 2 (Partner) within-subjects design. We predicted that RT would be slowest when the Same Partner used a New Label, but RT would not differ in the other three groups (i.e., an interaction effect).</p>

<p>This effect was borne out using ANOVA. However, I want to use a mixed effect model to capture both participant and item level variation in the same model. My current model in R is an intercepts only random model:</p>

<pre><code>rm1 &lt;- lmer(RT ~ Label*Partner + (1|Subject) + (1|Item))
</code></pre>

<p>However, I would like to include random slopes in the model that acknowledge the fact that participants may have had idiosyncratic differences to each cell in the design and that the RT to each item may also display idiosyncratic differences. Thus, I imagine this model  would look like:</p>

<pre><code>rm2 &lt;- lmer(RT~Label*Partner) + (Label*Partner|Subject) + (Label*Partner|Item))
</code></pre>

<p>Yet, I'm not sure if it's appropriate to include an effect as both a fixed and random effect in my model. If I only include the Label and Partner variables as random effects then I am unable to extract coefficients from the model and thus I cannot determine significance.</p>
"
"NaN","NaN","121661","<p>I've got two models (all variable are count variables):</p>

<pre><code>frm.ct &lt;- glmer(frm ~ age + education + socialrole +
              offset(log(words)) + (1|subkorpus), family=negative.binomial(1), 
data=daten.alle.kom)

frm.oage &lt;- glmer(frm ~ education + socialrole +
              offset(log(words)) + (1|subkorpus), family=negative.binomial(1), 
data=daten.alle.kom)
</code></pre>

<p>I used this to compare them:</p>

<pre><code>anova(frm.ct, frm.oage)
</code></pre>

<p><img src=""http://i.stack.imgur.com/xb6Jm.png"" alt=""enter image description here""></p>

<p>AIC values tell me that <code>frm.oage</code> is the better model, right? but what do 0.0452 and 0.8315 mean?</p>
"
"0.209736267254383","0.210210248123848","122717","<p>I have some trouble obtaining equivalent results between an <code>aov</code> between-within repeated measures model and an <code>lmer</code> mixed model.</p>

<p>My data and script look as follows</p>

<pre><code>data=read.csv(""https://www.dropbox.com/s/zgle45tpyv5t781/fitness.csv?dl=1"")
data$id=factor(data$id)
data
   id  FITNESS      TEST PULSE
1   1  pilates   CYCLING    91
2   2  pilates   CYCLING    82
3   3  pilates   CYCLING    65
4   4  pilates   CYCLING    90
5   5  pilates   CYCLING    79
6   6  pilates   CYCLING    84
7   7 aerobics   CYCLING    84
8   8 aerobics   CYCLING    77
9   9 aerobics   CYCLING    71
10 10 aerobics   CYCLING    91
11 11 aerobics   CYCLING    72
12 12 aerobics   CYCLING    93
13 13    zumba   CYCLING    63
14 14    zumba   CYCLING    87
15 15    zumba   CYCLING    67
16 16    zumba   CYCLING    98
17 17    zumba   CYCLING    63
18 18    zumba   CYCLING    72
19  1  pilates   JOGGING   136
20  2  pilates   JOGGING   119
21  3  pilates   JOGGING   126
22  4  pilates   JOGGING   108
23  5  pilates   JOGGING   122
24  6  pilates   JOGGING   101
25  7 aerobics   JOGGING   116
26  8 aerobics   JOGGING   142
27  9 aerobics   JOGGING   137
28 10 aerobics   JOGGING   134
29 11 aerobics   JOGGING   131
30 12 aerobics   JOGGING   120
31 13    zumba   JOGGING    99
32 14    zumba   JOGGING    99
33 15    zumba   JOGGING    98
34 16    zumba   JOGGING    99
35 17    zumba   JOGGING    87
36 18    zumba   JOGGING    89
37  1  pilates SPRINTING   179
38  2  pilates SPRINTING   195
39  3  pilates SPRINTING   188
40  4  pilates SPRINTING   189
41  5  pilates SPRINTING   173
42  6  pilates SPRINTING   193
43  7 aerobics SPRINTING   184
44  8 aerobics SPRINTING   179
45  9 aerobics SPRINTING   179
46 10 aerobics SPRINTING   174
47 11 aerobics SPRINTING   164
48 12 aerobics SPRINTING   182
49 13    zumba SPRINTING   111
50 14    zumba SPRINTING   103
51 15    zumba SPRINTING   113
52 16    zumba SPRINTING   118
53 17    zumba SPRINTING   127
54 18    zumba SPRINTING   113
</code></pre>

<p>Basically, 3 x 6 subjects (<code>id</code>) were subjected to three different <code>FITNESS</code> workout schemes each and their <code>PULSE</code> was measured after carrying out three different types of endurance <code>TEST</code>s.</p>

<p>I then fitted the following <code>aov</code> model :</p>

<pre><code>library(afex)
library(car)
set_sum_contrasts()
fit1 = aov(PULSE ~ FITNESS*TEST + Error(id/TEST),data=data)
summary(fit1)
Error: id
          Df Sum Sq Mean Sq F value   Pr(&gt;F)    
FITNESS    2  14194    7097   115.1 7.92e-10 ***
Residuals 15    925      62                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: id:TEST
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
TEST          2  57459   28729   253.7  &lt; 2e-16 ***
FITNESS:TEST  4   8200    2050    18.1 1.16e-07 ***
Residuals    30   3397     113                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The result I obtain using</p>

<pre><code>set_sum_contrasts()
fit2=aov.car(PULSE ~ FITNESS*TEST+Error(id/TEST),data=data,type=3,return=""Anova"")
summary(fit2)
</code></pre>

<p>is identical to this.</p>

<p>A mixed model run using <code>nlme</code> gives a directly equivalent result, e.g. using <code>lme</code> :</p>

<pre><code>library(lmerTest)    
lme1=lme(PULSE ~ FITNESS*TEST, random=~1|id, correlation=corCompSymm(form=~1|id),data=data)
anova(lme1)
             numDF denDF   F-value p-value
(Intercept)      1    30 12136.126  &lt;.0001
FITNESS          2    15   115.127  &lt;.0001
TEST             2    30   253.694  &lt;.0001
FITNESS:TEST     4    30    18.103  &lt;.0001


summary(lme1)
Linear mixed-effects model fit by REML
 Data: data 
       AIC      BIC    logLik
  371.5375 393.2175 -173.7688

Random effects:
 Formula: ~1 | id
        (Intercept) Residual
StdDev:    1.699959 9.651662

Correlation Structure: Compound symmetry
 Formula: ~1 | id 
 Parameter estimate(s):
       Rho 
-0.2156615 
Fixed effects: PULSE ~ FITNESS * TEST 
                                 Value Std.Error DF   t-value p-value
(Intercept)                   81.33333  4.000926 30 20.328628  0.0000
FITNESSpilates                 0.50000  5.658164 15  0.088368  0.9308
FITNESSzumba                  -6.33333  5.658164 15 -1.119327  0.2806
TESTJOGGING                   48.66667  6.143952 30  7.921069  0.0000
TESTSPRINTING                 95.66667  6.143952 30 15.570868  0.0000
FITNESSpilates:TESTJOGGING   -11.83333  8.688861 30 -1.361897  0.1834
FITNESSzumba:TESTJOGGING     -28.50000  8.688861 30 -3.280062  0.0026
FITNESSpilates:TESTSPRINTING   8.66667  8.688861 30  0.997446  0.3265
FITNESSzumba:TESTSPRINTING   -56.50000  8.688861 30 -6.502579  0.0000
</code></pre>

<p>Or using <code>gls</code> :</p>

<pre><code>library(lmerTest)    
gls1=gls(PULSE ~ FITNESS*TEST, correlation=corCompSymm(form=~1|id),data=data)
anova(gls1)
</code></pre>

<p>However, the result I obtain using <code>lme4</code>'s <code>lmer</code> is different :</p>

<pre><code>set_sum_contrasts()
fit3=lmer(PULSE ~ FITNESS*TEST+(1|id),data=data)
summary(fit3)
Linear mixed model fit by REML ['lmerMod']
Formula: PULSE ~ FITNESS * TEST + (1 | id)
   Data: data

REML criterion at convergence: 362.4

Random effects:
 Groups   Name        Variance Std.Dev.
 id       (Intercept)  0.00    0.0     
 Residual             96.04    9.8     
...

Anova(fit3,test.statistic=""F"",type=3)
Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)

Response: PULSE
                    F Df Df.res    Pr(&gt;F)    
(Intercept)  7789.360  1     15 &lt; 2.2e-16 ***
FITNESS        73.892  2     15 1.712e-08 ***
TEST          299.127  2     30 &lt; 2.2e-16 ***
FITNESS:TEST   21.345  4     30 2.030e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Anybody any thoughts what I am doing wrong with the <code>lmer</code> model? Or where the difference comes from? Could it have to do anything with <code>lmer</code> not allowing negative intraclass corellations or something like that? Given that <code>nlme</code>'s <code>gls</code> and <code>lme</code> do return the correct result, though, I am wondering how this is different in <code>gls</code> and <code>lme</code>? Is it that the option <code>correlation=corCompSymm(form=~1|id)</code> causes them to  directly estimate the intraclass correlation, which can be either positive or negative, whereas <code>lmer</code> estimates a variance component, which cannot be negative (and ends up being estimated as zero in this case)?</p>
"
"0.26846242208561","0.269069117598525","127479","<p>I'm using a mixed effects model with logistic link function (using lme4 version 1.1-7 in R).  However, I noticed that the estimates of significance for fixed effects change depending on the order of the rows in the dataset.  </p>

<p>That is, if I run a model on a dataset, I get certain estimate for my fixed effect and it has a certain p-value.  I run the model again, and I get the same estimate and p-value.  Now, I shuffle the order of rows (the data is not mixed, just the rows are in a different order).  Running the model a third time, the p-value is very different.</p>

<p>For the data I have, the estimated p-value for the fixed effect can be between p=0.001 and p=0.08.  Obviously, these are crucial differences given conventional significance levels. </p>

<p>I understand that the estimates are just estimated, and there will be differences between values for a number of reasons.  However, the magnitude of the differences for my data seem large to me, and I wouldn't expect the order of my dataframe to have this effect (we discovered this problem by chance when a colleague ran the same model but got different results.  It turned out they had ordered their data frame.).  </p>

<p>Here is the output of my script:
(X and Y are binary variables which are contrast-coded and centred, Group and SubGroup are categorical variables)</p>

<pre><code>&gt; # Fit model
&gt; m1 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; # Shuffle order of rows
&gt; d = d[sample(1:nrow(d)),]
&gt; # Fit model again
&gt; m2 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; summary(m1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5421        
              Y1          0.1847   0.4298   -0.79
 Group        (Intercept) 0.2829   0.5319        
              Y1          0.4640   0.6812   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1325  -8.214   &lt;2e-16 ***
Y1            0.3772     0.2123   1.777   0.0756 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.112 
&gt;
&gt; # -----------------
&gt; summary(m2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5422        
              Y1          0.1846   0.4296   -0.79
 Group        (Intercept) 0.2829   0.5318        
              Y1          0.4641   0.6813   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1166  -9.334  &lt; 2e-16 ***
Y1            0.3773     0.1130   3.339 0.000841 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.074 
</code></pre>

<p>I'm afraid that I can't attach the data due to privacy reasons. </p>

<p>Both models converge.  The difference appears to be in the standard errors, while the differences in coefficient estimates are smaller.  The model fit (AIC etc.) are the same, so maybe there are multiple optimal convergences, and the order of the data pushes the optimiser into different ones.  However, I get slightly different estimates every time I shuffle the data frame (not just two or three unique estimates).  In one case (not shown above), the model did not converge simply because of a shuffling of the rows.</p>

<p>I suspect that the problem lies with the structure of my particular data.  It's reasonably large (nearly 200,000 cases), and has nested random effects.  I have tried centering the data, using contrast coding and feeding starting values to lmer based on a previous fit.  This seems to help somewhat, but I still get reasonably large differences in p-values.  I also tried using different ways of calculating p-values, but I got the same problem.</p>

<p>Below, I've tried to replicate this problem with synthesised data.  The differences here aren't as big as with my real data, but it gives an idea of the problem.</p>

<pre><code>library(lme4)
set.seed(999)

# make a somewhat complex data frame
x = c(rnorm(10000),rnorm(10000,0.1))
x = sample(x)
y = jitter(x,amount=10)
a = rep(1:20,length.out=length(x))
y[a==1] = jitter(y[a==1],amount=3)
y[a==2] = jitter(x[a==2],amount=1)
y[a&gt;3 &amp; a&lt;6] = rnorm(sum(a&gt;3 &amp; a&lt;6))
# convert to binary variables
y = y &gt;0
x = x &gt;0
# make a data frame
d = data.frame(x1=x,y1=y,a1=a)

# run model 
m1 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# shuffle order of rows
d = d[sample(nrow(d)),]

# run model again
m2 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# show output
summary(m1)
summary(m2)
</code></pre>

<p>One solution to this is to run the model multiple times with different row orders, and report the range of p-values.  However, this seems inelegant and potentially quite confusing.</p>

<p>The problem does not affect model comparison estimates (using anova), since these are based on differences in model fit.  The fixed effect coefficient estimates are also reasonably robust.  Therefore, I could just report the effect size, confidence intervals and the p-value from a model comparison with a null model, rather than the p-values from within the main model.</p>

<p>Anyway, has anyone else had this problem?  Any advice on how to proceed?</p>
"
"0.0474578997876249","0.0475651494154494","130476","<p>This model is a simple linear regression:</p>

<pre><code>mtcars_lm &lt;- lm(mpg ~ wt, mtcars)
</code></pre>

<p>And this model adds <code>cyl</code> as a random effect:</p>

<pre><code>library(lme4)
mtcars_mixed_effects &lt;- lmer(mpg ~ wt + (1 | cyl), mtcars)
</code></pre>

<p>Is there a way to test whether adding <code>cyl</code> as random effect is worthwhile? I tried this but it threw an error:</p>

<pre><code>anova(mtcars_mixed_effects, mtcars_lm)
</code></pre>

<p>(please disregard the fact that <code>cyl</code> only has three groups, I'm just using one of R's built in datasets to make question reprodicible).</p>
"
"0.157400046933839","0.157755753708238","131152","<p>Let say I've ran this linear regression:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt + vs, mtcars)
</code></pre>

<p>I can use <code>anova()</code> to see the amount of variance in the dependent variable accounted for by the two predictors:</p>

<pre><code>anova(lm_mtcars)

Analysis of Variance Table

Response: mpg
          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
wt         1 847.73  847.73 109.7042 2.284e-11 ***
vs         1  54.23   54.23   7.0177   0.01293 *  
Residuals 29 224.09    7.73                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Lets say I now add a random intercept for <code>cyl</code>:</p>

<pre><code>library(lme4)
lmer_mtcars &lt;- lmer(mpg ~ wt + vs + (1 | cyl), mtcars)
summary(lmer_mtcars)

Linear mixed model fit by REML ['lmerMod']
Formula: mpg ~ wt + vs + (1 | cyl)
   Data: mtcars

REML criterion at convergence: 148.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.67088 -0.68589 -0.08363  0.48294  2.16959 

Random effects:
 Groups   Name        Variance Std.Dev.
 cyl      (Intercept) 3.624    1.904   
 Residual             6.784    2.605   
Number of obs: 32, groups:  cyl, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  31.4788     2.6007  12.104
wt           -3.8054     0.6989  -5.445
vs            1.9500     1.4315   1.362

Correlation of Fixed Effects:
   (Intr) wt    
wt -0.846       
vs -0.272  0.006
</code></pre>

<p>The variance accounted for by each fixed effect now drops because the random intercept for <code>cyl</code> is now accounting for some of the variance in <code>mpg</code>:</p>

<pre><code>anova(lmer_mtcars)

Analysis of Variance Table
   Df  Sum Sq Mean Sq F value
wt  1 201.707 201.707 29.7345
vs  1  12.587  12.587  1.8555
</code></pre>

<p>But in <code>lmer_mtcars</code>, how can I tell what proportion of the variance is being accounted for by <code>wt</code>, <code>vs</code> and the random intecept for <code>cyl</code>?</p>
"
"0.269454078160044","0.270063014706494","136495","<p><b>Background:</b><br>
I am using linear mixed-effects models (LMMs) in order to determine how the interaction between two fixed effects influences measures of a response variable.  Since I am working with a dataset in which there are multiple samples from multiple individuals that could violate the assumption of independence of data points, I am treating ""individual"" as a random effect.  Thus, the generic model I am working with is:  </p>

<pre><code>lmer(y ~ Factor1*ContinuousVariable1 + (1|Ind), dataset, REML=T)
</code></pre>

<p>Note: for my actual dataset, I used a likelihood ratio test to determine whether I needed to also nest the multiple trials within individual [i.e., lmer(y ~ Factor1*ContinuousVariable1 + (1|Ind/Trial), dataset)], and failed to reject the null hypothesis that this ""fuller"" model contributed significantly to accounting for additional variation in the data.   </p>

<p><b>Problem to solve:</b><br>
Determine whether the results from my Tukey's post-hoc comparisons are reliable, given the interactions included in my LMM model.</p>

<p><b>Loading data and libraries:</b><br>
library(car) # for Soils dataset<br>
data(Soils)<br>
library(lme4) # for lmer()<br>
library(lsmeans) # for remaining functions  </p>

<p><b>Example code:</b><br>
     ## Create the LMM<br>
     ## ""Na"" is a numeric continuous response variable<br>
     ## ""Contour"" is a factor, with character categories, and is treated as a fixed effect<br>
     ## ""P"" is an integer variable, is treated as a fixed effect, and differs across the Contour groups<br>
     ## ""Group"" is a a numerical factor and is treated as a random effect  </p>

<pre><code>Na.LMER &lt;- lmer(Na ~ Contour*P + (1|Group), Soils, REML=T)
Na.LMER  

Linear mixed model fit by REML ['lmerMod']
Formula: Na ~ Contour * P + (1 | Group)
   Data: Soils
REML criterion at convergence: 190.4919
Random effects:
 Groups   Name        Std.Dev.
 Group    (Intercept) 2.514   
 Residual             1.063   
Number of obs: 48, groups:  Group, 12
Fixed Effects:
   (Intercept)    ContourSlope      ContourTop               P  ContourSlope:P    ContourTop:P  
    7.104951        4.381251       -0.260527       -0.006811       -0.026952       -0.006258  

### Conduct Tukey's post-hoc comparisons
Na.Tukey &lt;- lsmeans(Na.LMER, pairwise~Contour, adjust=""tukey"")
</code></pre>

<blockquote>
  <p>NOTE: Results may be misleading due to involvement in interactions  </p>
</blockquote>

<pre><code>Na.Tukey  

$lsmeans
 Contour      lsmean       SE   df lower.CL upper.CL
 Depression 5.973118 1.289466 8.15 3.008857 8.937380
 Slope      5.875929 1.286895 8.08 2.913697 8.838160
 Top        4.672639 1.294933 8.24 1.701416 7.643863

Confidence level used: 0.95 

$contrasts
 contrast             estimate       SE   df t.ratio p.value
 Depression - Slope 0.09718976 1.821763 8.11   0.053  0.9984
 Depression - Top   1.30047917 1.827450 8.19   0.712  0.7635
 Slope - Top        1.20328941 1.825636 8.16   0.659  0.7925

P value adjustment: tukey method for a family of 3 means 
</code></pre>

<p><b>So this is where the question comes in.</b><br>
Since I received the warning message (""NOTE: Results may be misleading due to involvement in interactions""), I want to verify whether I can reliably use the p-values output from lsmeans() to determine which contrasts were different from each other.  So how can I tell whether the interactions from this particular dataset could be problematic for interpreting the results from the Tukey's post-hoc comparisons.  </p>

<p><b>Here is what I have tried to investigate this issue.</b><br>
Based on the recommendations by Professor Russell Lenth (developer of the lsmeans R package), I used additional functions from the lsmeans R package to investigate what's going on with the data.</p>

<pre><code>### First, here are the F-tests of the fixed effects of the LMM.
anova(Na.LMER)   

Analysis of Variance Table
          Df  Sum Sq Mean Sq F value
Contour    2  0.5696  0.2848  0.2520
P          1 10.4083 10.4083  9.2093
Contour:P  2  6.7070  3.3535  2.9672  
</code></pre>

<p>Does the Contour:P interaction seem relatively strong?  </p>

<p>Next, I'm going to evaluate whether this interaction is important by determining to what extent the values of P varies across the Contour groups, using lsmip().    </p>

<pre><code>Na.lsm &lt;- lsmeans(Na.LMER, ~Contour|P, at=list(P = c(75, 100, 200, 300, 400)))  
Na.lsm    

P =  75:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  6.594094 1.399580 10.70  3.5029413  9.685246
 Slope       8.953983 1.562754 13.53  5.5913341 12.316631
 Top         5.864180 1.511863 12.76  2.5917619  9.136598

P = 100:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  6.423808 1.355688  9.64  3.3876590  9.459957
 Slope       8.109909 1.429365 10.79  4.9562943 11.263524
 Top         5.537432 1.391548 10.16  2.4433848  8.631479

P = 200:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  5.742665 1.286244  8.08  2.7814923  8.703838
 Slope       4.733616 1.354120  9.32  1.6863856  7.780847
 Top         4.230440 1.384598 10.01  1.1459415  7.314939

P = 300:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  5.061522 1.396923 10.63  1.9738402  8.149204
 Slope       1.357323 1.960472 21.77 -2.7109112  5.425557
 Top         2.923449 2.025495 24.22 -1.2549312  7.101829

P = 400:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  4.380379 1.651907 17.57  0.9037052  7.857053
 Slope      -2.018970 2.841216 33.67 -7.7950921  3.757152
 Top         1.616457 2.914268 36.01 -4.2938885  7.526803

Confidence level used: 0.95  
</code></pre>

<blockquote>
  <h3>Plotting the interactions</h3>
  
  <p>Na.lsmip &lt;- lsmip(Na.lsm, Contour~P)</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/9uDul.jpg"" alt=""Interaction of Contour and P""></p>

<blockquote>
  <h3>It seems like the levels of Contour vary at different values of P (especially for Slope), but I'm going to use pairs() to verify this using pairwise comparison at each value of P.</h3>
</blockquote>

<pre><code>pairs(Na.lsm)  
P =  75:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope -2.3598888 2.097862 12.15  -1.125  0.5175
 Depression - Top    0.7299139 2.060232 11.74   0.354  0.9335
 Slope - Top         3.0898026 2.174381 13.15   1.421  0.3589

P = 100:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope -1.6861012 1.970019 10.22  -0.856  0.6784
 Depression - Top    0.8863760 1.942755  9.90   0.456  0.8928
 Slope - Top         2.5724773 1.994865 10.47   1.290  0.4308

P = 200:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  1.0090489 1.867637  8.70   0.540  0.8539
 Depression - Top    1.5122246 1.889851  9.04   0.800  0.7122
 Slope - Top         0.5031757 1.936686  9.67   0.260  0.9636

P = 300:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  3.7041990 2.407248 16.78   1.539  0.2988
 Depression - Top    2.1380732 2.460493 18.21   0.869  0.6660
 Slope - Top        -1.5661258 2.818879 23.01  -0.556  0.8447

P = 400:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  6.3993492 3.286534 28.79   1.947  0.1439
 Depression - Top    2.7639218 3.349889 30.81   0.825  0.6905
 Slope - Top        -3.6354273 4.070070 34.89  -0.893  0.6481

P value adjustment: tukey method for a family of 3 means  
</code></pre>

<blockquote>
  <h3>Based on the pairs() output, it doesn't seem like Contour groups vary at these incremental values of P.</h3>
  
  <p><b>Since the Contour groups do not seem to vary at different levels of P, does that mean that the interaction strength is not that strong?  and thus, I am okay to ignore the warning message that ""NOTE: Results may be misleading due to involvement in interactions""?</b>  </p>
</blockquote>

<p>I would appreciate any feedback about interpreting these results, and whether there are additional analyses that I should be conducting in order to address my concern.  If there is any additional information that would be helpful in tackling this problem, please let me know.  </p>

<p>Thank you for your time!</p>

<p>UPDATE (2/6/15): I had a minor typo at the beginning, in which the first line of code read ""Dens.LMER &lt;- lmer(...)"".  The lmer product should have been named ""Na.LMER"", which was used in the remaining code.  Thus, the Dens.LMER product that rvl mentions is equivalent to Na.LMER.  I apologize for the inconvenience.  </p>
"
"0.100673408282104","0.100900919099447","137766","<p>I did the model comparison using these three models:</p>

<pre><code>lmer1 &lt;- lmer(peak_Mid ~ (1|item) + (1+vowel|speaker) + gender*vowel*language, data=data.frame, REML=FALSE, na.action=na.omit)
lmer2 &lt;- lmer(peak_Mid ~ (1|item) + (1+vowel|speaker) + gender + vowel + Language + language:vowel + Language:gender + vowel:gender, data=data.frame, REML=FALSE, na.action=na.omit)
anova(lmer1,lmer2)
lmer3 &lt;- lmer(peak_Mid ~ (1|item) + (1+vowel|speaker) + gender + vowel + language, data=data.frame, REML=FALSE, na.action=na.omit)
anova(lmer2,lmer3)
</code></pre>

<p>Then I run Tukey's HSD Post-hoc test based on the optimal model without changing 'REML=FALSE' to 'REML=TRUE'. The question is whether I need to change this before running the Tukey's HSD Post-hoc test? In my opinion, I don't need to change it as if I change, it means I didn't get the Tukey's HSD Post-hoc test based on the optimal model from model comparison as I changed the formula. What do you think?</p>
"
"0.0821994936526786","0.0823852554571635","138132","<p>I have a model based on a dataset that respects all linear model assumptions except for homoscedasticity. When I just ignore the problem of heteroscedasticity, the p-value, for the interaction with group, in my model is &lt;.00001. I definitely know that there is something as per my previous studies and the literature in this field. However, I would like to be honest regarding my analyses and assumptions. Is this assumption really needed if the other 3 main ones are respected (independence, linearity, absence of collinearity) for the interpretation of the p-value in the mixed effects models? </p>

<p>When I run the following on my lmer model called mod:</p>

<pre><code>plot(fitted(mod),residuals(mod))
</code></pre>

<p><img src=""http://i.stack.imgur.com/yYkUm.png"" alt=""enter image description here""></p>

<p>I get a cone shape distribution. I then try to log transform it, and recheck the model, for the interaction with group in my model the p value goes to .40. Quite a jump! My data comes brain activity from patients and healthy individual, just to clarify.</p>

<p>This is my model: </p>

<blockquote>
  <p>lmer(value ~ dist*group + (1|patientnumber), dat1)</p>
</blockquote>

<p>This is how I obtained the p-value:</p>

<blockquote>
  <p>Anova(mod)</p>
</blockquote>

<p>Kindly advise.</p>
"
"0.135067550667224","0.150414209399047","138908","<p>I am looking at the effects role has an opportunities to collaborate between groups in a social network. At a basic level the data are modeled as:</p>

<pre><code>relRatio~role
</code></pre>

<p>With relative ratio being the percentage of teammates who are part of the subject's normal group. The data I have come from multiple time slices over the years, with some of the subjects being polled two or more times. Not every subject has multiple entries, nor does every subject with multiple entries have the same number of entries. From some advice I received it was suggested that I test the differences between groups using a random effect ANOVA model, which would be modeled (in R) as</p>

<pre><code>relRatio~role+Error(subjectId)
</code></pre>

<p>After trying to read up more on random effects ANOVA, I started to get the impression that linear mixed effects models (with the lmer) package are preferred over random effects ANOVA, although I have yet to see a clear distinction between the two. This leads to my first question: Which approach is best for modeling my data?</p>

<p>If it involves using the random effects ANOVA, I would greatly appreciate it if someone could recommend a resource for the process of interpreting the results.</p>

<p>My second question is, if the better approach is to use the mixed effects models, which I have tried, why do I get striping in my residuals?</p>

<p><img src=""http://i.stack.imgur.com/vqWjn.png"" alt=""enter image description here""></p>

<p>One guide suggested that I am dealing with categorical data, which requires the use of logistic regression. However, the dependent variable for my data is continuous, and the IV is categorical, which I thought LMM are supposed to handle. This leads to my second question - does my residual plot indicate something is wrong with the way I have modeled my data?</p>
"
"0.150699071696578","0.164770510914327","140183","<p>The age old question of comparing sums of squares (SS) between programs has reared its ugly head again. </p>

<p>I am trying to replicate output in SPSS, that was computed using Type 3 Sums of Squares, in R. </p>

<p>I understand that with multiple regressions, there are several ways to get Type 3 SS in R (to match Type 3 output from SPSS). </p>

<p>However, I am running a mixed model using aov (which uses Type 1 SS) and even when I try all the ""usual"" fixes,"" my estimates don't match the Type 3 SS output from SPSS. </p>

<p>First of all, when I run the SPSS syntax using ""/METHOD=SSTYPE(1)"" the results match those I get using this code:</p>

<pre><code>  mymodel&lt;-aov(data=longdat,  DV ~ 1 + Task + Cue + Compatibility + Cue:Task + Compatibility:Task + Cue:Compatibility + Cue:Compatibility:Task + Error(subject/Cue/Compatibility/Cue*Compatibility))
  summary(mymodel)
</code></pre>

<p>So I know the analyses are the same when they use Type 1 SS. </p>

<p>However, when I use:</p>

<pre><code> options(contrasts = c(""contr.sum"",""contr.poly""))
 tt&lt;-lm(DV ~ 1 + Task + Cue + Compatibility + Cue:Task  + Compatibility:Task + Cue:Compatibility + Cue:Compatibility:Task + 1/subject/Cue/Compatibility    /(Cue*Compatibility), data=longdat)
 drop1(tt, ~., test=""F"")
</code></pre>

<p>The results do not match the SPSS Type 3 output. </p>

<p>In attempts to get matching output, I have also tried the Anova function (which can give Type 3 SS)</p>

<pre><code>  Anova(mymodel, type=3, test.statistic=""F"") 
</code></pre>

<p>but I get this error <code>""Error in terms.formula(formula, data = data) : 'data' argument is of the wrong type.""</code></p>

<p>I have also tried using <code>lmer</code>.</p>

<p>Can someone help me get Type 3 Sums of Squares for a mixed model in R?</p>

<p>Thank you! </p>
"
"0.196767092567047","0.217970897638181","141746","<p>Given three variables, <code>y</code> and <code>x</code>, which are positive continuous, and <code>z</code>, which is categorical, I have two candidate models given by:</p>

<pre><code> fit.me &lt;- lmer( y ~ 1 + x + ( 1 + x | factor(z) ) )
</code></pre>

<p>and</p>

<pre><code> fit.fe &lt;- lm( y ~ 1 + x )
</code></pre>

<p>I hope to compare these models to determine which model is more appropriate. It seems to me that in some sense <code>fit.fe</code> is nested within <code>fit.me</code>. Typically, when this general scenario holds, a chi-squared test can be performed. In <code>R</code>, we can perform this test with the following command,</p>

<pre><code> anova(fit.fe,fit.me)
</code></pre>

<p>When both models contain random-effects (generated by <code>lmer</code> from the <code>lme4</code> package), the <code>anova()</code> command works fine. Owing to boundary parameters, it is normally advisable to test the resulting Chi-Square statistic via simulation, nonetheless, we can still <em>use</em> the statistic in the simulation procedure.</p>

<p>When both models contain <em>only</em> fixed-effects, this approach---and, the associated <code>anova()</code> command---work fine.</p>

<p>However, when one model contains random effects and the reduced model contains <em>only</em> fixed-effects, as in the above scenario, the <code>anova()</code> command doesn't work.</p>

<p>More specifically, I get the following error:</p>

<pre><code> &gt; anova(fit.fe, fit.me)
 Error: $ operator not defined for this S4 class
</code></pre>

<p>Is there anything wrong with using the Chi-Square approach from above (with simulation)? Or is this simply a problem of <code>anova()</code> not knowing how to deal with linear models generated by different functions?</p>

<p>In other words, would it be appropriate to manually generate the Chi-Square statistic derived from the models? If so, what are the appropriate degrees of freedom for comparing these models? By my reckoning:</p>

<p>$$ F = \frac{\left((SSE_{reduced}-SSE_{full})/(p-k)\right)}{\left((SSE_{full})/(n-p-1)\right)} \sim F_{p-k,n-p-1} $$</p>

<p>We are estimating two parameters in the fixed effects model (slope and intercept) and two more parameters (variance parameters for the random slope and random intercept) in the mixed-effects model. Typically, the intercept parameter isn't counted in the degrees of freedom computation, so that implies that $k=1$ and $p=k+2=3$; having said that I'm not sure if the variance parameters for the random-effects parameters should be included in the degrees of freedom computation; the variance estimates for fixed-effect parameters are <em>not considered</em>, but I believe that to be because the parameter estimates for fixed effects are assumed to be <em>unknown constants</em> whilst they are considered to be <em>unknowable random variables</em> for mixed effects. I would appreciate some assistance on this issue.</p>

<p>Finally, does anybody have a more appropriate (<code>R</code>-based) solution to comparing these models?</p>
"
"0.134231211042805","0.134534558799262","143556","<p>I would like to model a treatment effect in two different groups, controlled for some co-variates (like age and education), and I assume that a two-way repeated-measure Anova would be the right approach - if yes, I have some questions on how to model this design.</p>

<p>I'm a bit confused on how to do this with R (and the  <code>lme4</code> package), because I found different approaches for the same design. Let's say, I have following variables:</p>

<ul>
<li>subject</li>
<li>group (control vs treatment group)</li>
<li>time (t0 vs t1, i.e. two measures for each subject)</li>
<li>age (co-variate)</li>
<li>education (co-variate)</li>
</ul>

<p>Am I right, that, according to <a href=""http://stats.stackexchange.com/questions/58745/using-lmer-for-repeated-measures-linear-mixed-effect-model"">this posting on Cross Validated</a>, my model would look like this?</p>

<ol>
<li>model: <code>lmer(DV ~ group * time + age + education + (1+time|subject), mydata)</code> </li>
</ol>

<p>Then I found <a href=""http://www.uni-kiel.de/psychologie/rexrepos/posts/anovaMixed.html#mixed-effects-analysis-1"">this tutorial</a>. Following these instructions, my model would look like this?</p>

<ol start=""2"">
<li>model: <code>lmer(DV ~ group * time + age + education + (1|subject) + (1|group:subject) + (1|time:subject), data=mydata)</code></li>
</ol>

<p>Now I have two questions:</p>

<p>a) which of the two above models is correct? or do both work?</p>

<p>b) my data is in long format, how should my variable <code>subject</code> look like? the same value for each measured person, i.e. a value appears twice in this variable (for <em>person A in group X</em> at <strong>t0</strong> and <em>person A in group X</em> at <strong>t1</strong> the same value), or should each row/observation be indicated by a new, unique ID?</p>
"
"0.177571201301144","0.177972492663322","151200","<p>I have been trying to figure out how to do a fairly basic repeated measures analysis using linear mixed effects in R, and then analysing it using post-hoc tests. The problem is that I'm not sure whether the output I get is statistically sound?</p>

<p>The response variable: <code>weighted</code>- an index of habitat preference (prop. individuals on habitatA / prop. of total habitat that is A). A value above 1 indicates the habitat is being used more than what you would expect from its availability. this was repeatedly  measured on the same colony through time over several weeks</p>

<p>Fixed variables: <code>Type</code> - habitat type (live/dead), <code>weeks</code> - the time variable</p>

<p>Random variables: <code>colony</code> - because each measurement of colony violates independence assumption.</p>

<p>Here's what the data loss like plotted over time (orange=live habitat, blue=dead habitat):</p>

<p><img src=""http://i.stack.imgur.com/atUVm.jpg"" alt=""enter image description here""> </p>

<p>i run the analysis using the <code>lmer()</code> function from the <code>lme4</code> package:</p>

<pre><code>results_full=lmer(weighted~type*weeks+(weeks|colony), data=Pos, REML=F)
</code></pre>

<p>My reasoning is that i have no reason to expect a random intercept, they should all start on 1 at time 0, and then individuals will start avoiding the dead habitat and favouring the live habitat. The <code>(weeks|colony)</code> term allows the slope of each colony to be random across time?</p>

<p>So to my question:</p>

<p>I compare the likelihood of two models with each other, in a likelihood ratio test to get p-values of the fixed effects using a reduced model:</p>

<pre><code>results_null=lmer(weighted~type+weeks+ (weeks|colony), data=Pos, REML=F)
anova(results_null, results_full)
</code></pre>

<p>But what I'm really interested in is at what time point (week) do the individuals start avoiding the dead habitat. as you can see from the figure this happens at week 1 so comparing live-dead habitat week by week ""should"" generate a n/s result at week 0 and sig result from then on (I'm not trying to force a statistically significant result, but the fig is pretty clear...)</p>

<p>I tried converting the weeks into a factor, and then performing </p>

<pre><code>lsmeans(results_full, pairwise~type+weeks)
</code></pre>

<p>But it didn't generate anything that seemed meaningful, the output didn't make sense in relation to the data. </p>

<p>Does anyone have any thoughts on A) whether my model and test is appropriate to this data, and B) how I can perform a post hoc test to compare habitat type over time?</p>

<p>Would it be appropriate to use a Dunetts post hoc test to compare preferences to a reference value (=1) rather than to each other?</p>

<p>Grateful for any ideas or pointers!</p>
"
"0.135067550667224","0.150414209399047","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.142373699362875","0.142695448246348","153698","<p>I just encountered a problem while analyzing experimental data using lme4 and lmertest. In the experiment, 67 subjects gave 3 ratings for 50 stimuli shown for 3 different durations (a total of 10050 responses). I used the same nested model for each of the 3 ratings (the only difference being the response variable), but the denominator dfs are different for each of the 3 lmertest anovas (1808, 9848, 1807). How is this possible?</p>

<p>A similar question was asked fro mixed effects in SPSS, but remained unanswered. <a href=""http://stats.stackexchange.com/questions/82997/different-degrees-of-freedom-when-using-the-same-mixed-effect-model-spss"">Different degrees of freedom when using the same mixed effect model (SPSS)</a></p>

<p>Edit: Looking more closely at the model using summary(), I found that one group of the analysis that yielded much higher dfs (9848) has a variance of 0. Might this be the reason (the factor cd is nested within the duration factor named dur)?</p>

<p>Low df model:</p>

<pre><code>Random effects:
Groups        Name        Variance Std.Dev.
cd:(dur:subj) (Intercept) 0.13794  0.3714  
dur:subj      (Intercept) 0.04408  0.2099  
subj          (Intercept) 0.41425  0.6436  
Residual                  1.58692  1.2597  
Number of obs: 10050, groups:  cd:(dur:subj), 2010; dur:subj, 201; subj, 67
</code></pre>

<p>High df model:</p>

<pre><code>Random effects:
Groups        Name        Variance Std.Dev.
cd:(dur:subj) (Intercept) 0.00000  0.0000  
dur:subj      (Intercept) 0.06821  0.2612  
subj          (Intercept) 0.39013  0.6246  
Residual                  1.74595  1.3213  
Number of obs: 10050, groups:  cd:(dur:subj), 2010; dur:subj, 201; subj, 67
</code></pre>
"
"0.157949437947797","0.171498585142509","153802","<p>I have a large data set with repeated measurements of same blood value (co) (2 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement.</p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to right and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>At first I assumed random intercepts among patients. I constructed a null model and model with time as independent.</p>

<pre><code>fit0&lt;-(lmer(lgco~(1|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(1|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (1 | id)
fit1: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit0  3 200.44 213.16 -97.219   194.44                             
fit1  4 189.62 206.59 -90.811   181.62 12.815      1  0.0003438 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Ok, so I have an empty model and model with independent variable.
<img src=""http://i.stack.imgur.com/SFIFL.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/8RbgF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/phYJ1.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/G4HNH.png"" alt=""enter image description here""></p>

<p>Adding covariate time in my model improves it significantly and also the graphical explanation is clear.</p>

<p>Fixed slopes, however, are not reasonable in my data, so I should use random slopes.</p>

<pre><code>fit0&lt;-(lmer(lgco~(time|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(time|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (time| id)
fit1: lgco ~ time + (time | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
fit0  5 190.15 211.36 -90.076   180.15                            
fit1  6 182.06 207.51 -85.029   170.06 10.094      1   0.001487 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>At this point I dont understand my model equations. Graphical outputs for fit0 and fit1 are as follows:
<img src=""http://i.stack.imgur.com/E4w5D.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/XyeTJ.png"" alt=""enter image description here""></p>

<p>For the fit1 the model equation is:
<img src=""http://i.stack.imgur.com/Z6WLa.png"" alt=""enter image description here""></p>

<p>Why the lines in fit0 have non-zero slopes? What are they and what is the equation in that case? Also I dont understand how should I clarify the change in model fit? In the case of only random intercepts I can state that ""adding fixed factor <em>beta1</em> to model improves it significantly"". What would be the equal statement in the case of random slopes?</p>
"
"0.157400046933839","0.157755753708238","154488","<p>We measured temperatures of a pond repeatedly every day at each hour for a month at two different depths (i.e., top and bottom). We want to see if the temperatures at the top of the pond are significantly different from the bottom and if so at what hours. Initially I did a two-way anova in R:</p>

<pre><code>aov.result &lt;- aov(temp ~ depth * hour, data = pondtemp)
</code></pre>

<p>followed by post hoc tukey hsd test:</p>

<pre><code>tukey.result &lt;- TukeyHSD(aov.result)
</code></pre>

<p>The pairwise comparison of the depth*hour interaction term is what I need to see which hours have significantly different temperatures between top and bottom. This worked out well but someone pointed out that since it is a repeated measure it does not satisfy the assumption of independence. Therefore I tried using a linear mixed model. I took the depth as the fixed variable and figured the hour (since it has multiple observations in a month) should be the random variable:</p>

<pre><code>pondmdl &lt;- lmer(temp ~ depth + (1+variable|hour), data = pondtemp)
</code></pre>

<p>And used glht package for post-hoc:</p>

<pre><code>summary(glht(pondmdl, mcp(depth = ""Tukey"")))
</code></pre>

<p>However, this does not allow me to do the pair wise comparison I want to do (i.e., comparing Top Hour 1 to Bottom Hour 0 -23)</p>

<p>I found one way by introducing an interaction factor:</p>

<pre><code>pondtemp$depth.hour &lt;- interaction (pondtemp$depth, pondtemp$hour)
</code></pre>

<p>and then using this in my model and glht function:</p>

<pre><code>pondmdl &lt;- lmer(temp~depth.hour + (1+depth|hour), data = pondtemp)
summary(glht(pondmdl, mcp(depth.hour = ""Tukey"")))
</code></pre>

<p>However, I'm not sure if I can allow fixed and random variables to interact like that and still use the same random variable in the random variable error term.</p>

<p>Please advise what is my best option.</p>
"
"0.171111891110113","0.171498585142509","158319","<p>I have used a repeated-measures ANOVA in SPSS to analyse some of my data. It's the typical approach in my area, but I think it might be more appropriate to use a mixed effect model. However, I struggle with both building the model as well as interpreting it.</p>

<p><strong>Experimental design</strong></p>

<p>300+ participants from two different samples have rated on a continuous scale a stimulus at seven different manipulation levels. I want to test whether individual differences in the participants (recorded as ordinal or binary variables) interact with that rating score. In particular, I'm interested in whether the rating score changes as a function of stimulus level differently in people that, for example, feel mainly attracted to men or women.</p>

<p>Thus,  I have a within-subjects factor (stimulus level), a between-subjects factor (such as being attracted to men or women), and a random effect of participant nested in sample.</p>

<p>I've been using <code>lmer()</code> from the lme4 package and lmerTest and have come up with the following model</p>

<pre><code>model &lt;- lmer (rating.score ~ stim.level + factor + stim.level*factor +
                                                     (1|participant) + (1|sample), mydata)
</code></pre>

<p><strong>Analysis</strong></p>

<ol>
<li>Is lmer() the right package to work with?</li>
<li>Am I appropriately accounting for the random effects of participant and sample, or do I need something like <code>(1|sample/participant)</code>? I followed the <a href=""http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf"" rel=""nofollow"">Pastes data example</a>, but am not sure that's the right thing to do in this context.</li>
<li>Based on previous literature, I expect the relationship of <code>rating.score</code> and <code>stim.level</code> to be quadratic - should/could I enter <code>stim.level</code> as squared term?</li>
</ol>

<p><strong>Interpretation</strong></p>

<p>In SPSS, I find a significant interaction of <code>stim.level</code> x <code>factor</code>. By visualizing the interaction and running post-hoc tests I can then interpret the nature of that interaction. In R, I get estimates of the interaction at each level of <code>stim.level</code>, some of which are significant, some of which are not. Can I still make the conclusion that <code>factor</code> affects the relationship of <code>rating.score</code> and <code>stim.level</code> (even though not necessarily to the same extent at each level)?</p>

<p><strong>EDIT:</strong> I just realized I had entered <code>stim.level</code> as a factor. I think it is appropriate to enter it as a linear variable - the different levels correspond to the same manipulation applied with increasing extent (the steps between each level are the same). This also resolves one of my earlier questions regarding an error message when trying to model random slopes which I have thus now removed.</p>
"
"0.134231211042805","0.134534558799262","159673","<p>I'm new to using R, only started in the last 2 weeks so I apologize if I'm missing something very obvious. 
I am analyzing data from a repeated measures experiment with three factors, diversity (2 levels), temperature(2 levels) and par (3 levels), with ecosystem respiration as my variables. 
So far I've been successful in running a mixed model with the code  </p>

<pre><code>lmermod.e&lt;- lmer(er ~ div:par+div:tem+tem:par + (1 | week), data=mm)
</code></pre>

<p>I now need to do a pairwise comparison, 
I've tried ...</p>

<pre><code>summary(glht(ermod.e,linfct=mcp(div=""Tukey"")))
</code></pre>

<p>but get this error message...</p>

<pre><code>Error in summary(glht(ermod.e, linfct = mcp(div = ""Tukey""))) : 
  error in evaluating the argument 'object' in selecting a method for function 'summary': Error in mcp2matrix(model, linfct = linfct) : 
  Variable(s) â€˜divâ€™ of class â€˜integerâ€™ is/are not contained as a factor in â€˜modelâ€™.
</code></pre>

<p>I am aware that wouldn't be appropriate to test the interactions I have. I have also tried the aov, with</p>

<pre><code>anovaer&lt;-aov(er ~ div:tem + tem:par + par:div + (1 | week), data=mm)
summary(anovaer)
anova(anovaer)
TukeyHSD(anovaer)
</code></pre>

<p>but I just get this </p>

<pre><code>Error in TukeyHSD.aov(anovaer) : no factors in the fitted model
In addition: Warning messages:
1: In replications(paste(""~"", xx), data = mf) :
  non-factors ignored: div, tem
2: In replications(paste(""~"", xx), data = mf) :
  non-factors ignored: tem, par
3: In replications(paste(""~"", xx), data = mf) :
  non-factors ignored: div, par
</code></pre>

<p>when I try <code>lsmeans</code></p>

<pre><code>lsmeans(ermod.i, pairwise~tem*par*div, adjust=""tukey"")
</code></pre>

<p>I get...</p>

<pre><code>Error in solve.default(L %*% V0 %*% t(L), L) : 
  Lapack routine dgesv: system is exactly singular: U[1,1] = 
</code></pre>

<p>Any help would be much appreciated</p>
"
"0.142373699362875","0.126840398441198","160985","<p>I'm fitting multi-level models in R using both <code>lme</code> from <code>nlme</code> package and <code>lmer</code> from <code>lme4</code>. It is a very simple model. </p>

<p>Using <code>lme</code>: <code>lmeModel &lt;- lme(Flourish ~ Week * condition, random = ~ Week|id, na.action = na.exclude)</code></p>

<p>Using <code>lmer</code>: <code>lmerModel &lt;- lmer(Flourish ~ Week * condition + (Week | id), na.action = na.exclude)</code></p>

<p>For each fixed effect, I get the same t-values, regardless of whether I use <code>lme</code> or <code>lmer</code>. Here's one of the fixed effects:</p>

<pre><code>summary(lmeModel) #using nlme
Fixed effects: Flourish ~ Week * condition 
       Value Std.Error  DF  t-value p-value
Week 0.00570 0.1728345 569  0.03297  0.9737

summary(lmerModel) #using lme4
Fixed effects:
       Estimate Std. Error t value
Week 0.005698   0.172834    0.03
</code></pre>

<p>Someone has suggested that I use the <code>pamer.fnc</code> function from the <code>LMERConvenienceFunctions</code> package to run an ANOVA on my <code>lmerModel</code> to get the p values. Here is a section of the output for the same fixed effect:</p>

<pre><code>pamer.fnc(lmerModel)
       Df  Sum Sq Mean Sq F value upper.p.val lower.p.val expl.dev.(%)
Week    1 49.7262 49.7262  5.0849      0.0244      0.0246       0.1043
</code></pre>

<p>Can anyone explain why the t-value of the fixed effect 0.03 (<strong>p = 0.97</strong>), which is not significant at all, but the F-value of the same fixed effect is 5.08 (<strong>p â‰ˆ 0.02</strong>), which is significant? I really appreciate any help anyone can provide. Thanks a lot!</p>
"
"0.142373699362875","0.126840398441198","169543","<p>I'm doing a two-factor ANOVA using the <code>lmerTest</code> package. Each factor has multiple levels. When one (or more) of the effects are significant, I would like to do a post-hoc test to determine which of the levels differ from each other. Here, I set up the model as:</p>

<pre><code>library('lmerTest')
model = lmer('measure~factor*experiment+(1|subject_id)', data=data)
print(anova(model))
</code></pre>

<p>The output appears as follows:</p>

<pre><code>Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
                  Df  Sum Sq Mean Sq F value  Denom    Pr(&gt;F)    
factor             3 2388.82  796.27 16.3140  9.999 0.0003527 ***
experiment         3  254.11   84.70  2.7689 30.000 0.0588323 .  
factor:experiment  9 1301.40  144.60  2.9626 30.000 0.0121071 *  
</code></pre>

<p>At this point, I can inspect the factors and see which factors are significant. However, when I look at the summary of the model:</p>

<pre><code>summary(model)
</code></pre>

<p>I get a much more detailed output (truncated to the relevant portion for clarity):</p>

<pre><code>                                t value Pr(&gt;|t|)    
(Intercept)                      -5.600 8.99e-06 ***
factorLevel1                      0.289  0.77522    
factorLevel2                      2.855  0.00871 ** 
factorLevel3                     -6.535 9.00e-07 ***
experimentSession1               -0.747  0.46086    
experimentSession2               -0.825  0.41596    
experimentSession3                0.317  0.75354    
factorLevel1:experimentSession1  -1.297  0.20454    
factorLevel2:experimentSession1  -0.903  0.37376    
factorLevel3:experimentSession1   3.025  0.00506 ** 
factorLevel1:experimentSession2   0.591  0.55917    
factorLevel2:experimentSession2  -0.777  0.44341    
factorLevel3:experimentSession2   3.027  0.00504 ** 
factorLevel1:experimentSession3  -0.123  0.90269    
factorLevel2:experimentSession3  -1.060  0.29770    
</code></pre>

<p>How do I interpret these values? Is this telling me that coefficient for <code>factorLevel2</code> is significantly different from 0? If I then do a Multiple Comparison of Means:</p>

<pre><code>print(summary(glht(m, linfct=mcp(experiment=""Tukey"", factor=""Tukey""))))
</code></pre>

<p>I get the following output:</p>

<pre><code>experiment: Session1 - Session0 == 0   -0.747   0.9829    
experiment: Session2 - Session0 == 0   -0.825   0.9718    
experiment: Session3 - Session0 == 0    0.317   0.9999    
experiment: Session2 - Session1 == 0   -0.078   1.0000    
experiment: Session3 - Session1 == 0    1.064   0.9079    
experiment: Session3 - Session2 == 0    1.142   0.8759    
factor: Level2 - Level1 == 0            0.289   0.9999    
factor: Level3 - Level1 == 0            2.855   0.0425 *  
factor: Level4 - Level1 == 0           -6.535   &lt;0.001 ***
factor: Level3 - Level2 == 0            1.517   0.6542    
factor: Level4 - Level2 == 0           -5.395   &lt;0.001 ***
factor: Level4 - Level3 == 0           -8.341   &lt;0.001 ***
</code></pre>

<p>This is more understandable as it is telling me which pairwise factors are significantly different. But, I'm unsure how to interpret the summary table produced by <code>summary()</code> and how the numbers compare to the table produced by <code>glht()</code>.</p>
"
"0.142373699362875","0.142695448246348","172408","<p>I want to investigate whether Group1 affects Resp variable in my model.
Group1 is an ordinal variable that can assume 4 values (1 2 3 4).</p>

<p>Group2 is also an ordinal variable taht can assume 5 values (0 1 2 3 4)</p>

<p>I create the null model (without Group1).</p>

<pre><code>model.null =  lmer(Resp~Group2+Gender+Age+BMI+(1|Subject)+(1|Day_type),data=table_data,REML=FALSE)
</code></pre>

<p>I create the full model:</p>

<pre><code>model.full = lmer(Resp~Group1+Group2+Gender+Age+BMI+(1|Subject)+(1|Day_type),data=table_data,REML=FALSE)
</code></pre>

<p>I run anova to see whether the 2 models are significantly different:</p>

<pre><code>anova(model.null,model.full)
Data: table_data
Models:
model.null: Resp ~ Group2 + Gender + Age + BMI + (1 | Subject) + (1 | Day_type)
model.full: Resp ~ Group1 + Group2 + Gender + Age + BMI + (1 | Subject) + (1 | 
model.full:     Day_type)
           Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
model.null 11 18320 18391 -9148.8    18298                            
model.full 14 18314 18405 -9143.0    18286 11.588      3   0.008935 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>according to the output of anova I can conclude that are significantly different: i.e. Group1 affects the response.</p>

<p>Now I run</p>

<pre><code>summary(model.full)
</code></pre>

<p>and get</p>

<pre><code>Fixed effects:
             Estimate Std. Error t value
(Intercept)  2.428580   0.462824   5.247
Group1   2   0.119003   0.163572   0.728
Group1   3   0.210836   0.171478   1.230
Group1   4   0.562406   0.196697   2.859
Group2   1   0.069754   0.139780   0.499
Group2   2   0.139545   0.148745   0.938
Group2   3   0.094811   0.162958   0.582
Group2   4   0.600394   0.214628   2.797
GenderMale   0.451459   0.095112   4.747
Age          0.005298   0.005544   0.956
BMI         -0.003777   0.008470  -0.446
</code></pre>

<p>Are all the 4 values of Group1 significantly different? What can I say looking at this output?</p>
"
"0.177967124203594","0.178369310307935","173976","<p>I need to understand why, in this simple simulation, does repeated measures anova's false discovery rate is kept at 5% when that of mixed model is about 10 % ?</p>

<p>I simulated 1000 sampling and measurement of 10 individuals from a given infinite population with true mean = 0 and true standard deviation = 1.</p>

<p>Each individual was measured 10 times with error following N(0, 0.1).</p>

<p>the first five individuals were assigned to group ""A"", and the 5 others to ""B"". This grouping is completely artificial so that, in the long run, no more that 5 % of the experiments should show ""significant"" group effect at alpha = 5%...</p>

<p>I test for group effect using repeated measures anova AND mixed model. </p>

<p>repeated measures anova syntax used :
     summary(aov(myvar ~ group + Error(indiv), SIMULATED_DATASET))</p>

<p>While using lmer from R's lme4 package :
     FULL_model &lt;- lmer(
            myvar # Response
            ~ 1 + group +  (1 | indiv)
            , data=SIMULATED_DATASET
            ,REML = FALSE
     )</p>

<pre><code> NULL_model &lt;- lmer(
        myvar # Response
        ~ 1 +          (1 | indiv)
        , data=SIMULATED_DATASET
        ,REML = FALSE
 )
</code></pre>

<p>lmer test (method 1) using Anova() from car package:
     Anova(FULL_model) </p>

<p>lmer test (method 2) using anova():
     anova(NULL_model, FULL_model) </p>

<p>The first problem is that NONE of the above mentioned method for lmer test of group effect keeps FDR at 5% (closer to 10%), while repeated meaures anova keeps FDR at 5%.</p>

<p>The second problem is : why does each method for test with lmer gives different results ?</p>

<p>I provide complete code for reproducing these results :</p>

<p><a href=""http://i.stack.imgur.com/2uxbF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2uxbF.png"" alt=""simulated p-value under null hypothesis for (left to right) repeated measures anova, Anova(lmer) and anova(null,full)""></a></p>

<pre><code>require(lme4)
require(car)

pp1 &lt;- c()
pp2 &lt;- c()
pp3 &lt;- c()
for(i in 1:1000){

    print(i)

    gg &lt;- data.frame(
         group=as.factor(rep(c(""A"",""B""), each=500))
        ,indiv=as.factor(rep(letters[1:10], each=100))
        ,myvar=rep(rnorm(10,0,1),each=100) + rnorm(1000,0,0.1)
    )

    lmer_fit &lt;- lmer(
        myvar # Response
        ~ 1 + group +  (1 | indiv)
        , data=gg
        ,REML = FALSE   # FALSE nÃ©cessaire quand on veut comparer deux modÃ¨les mixtes
    )

    lmer_fit_null &lt;- lmer(
        myvar # Response
        ~ 1 +  (1 | indiv)
        , data=gg
        ,REML = FALSE   # FALSE nÃ©cessaire quand on veut comparer deux modÃ¨les mixtes
    )



    pp1 &lt;- c(pp1,unlist(summary(aov(myvar ~ group + Error(indiv), gg))[[1]])[[9]])

    pp2 &lt;- c(pp2,Anova(lmer_fit)[[3]])

    pp3 &lt;- c(pp3,anova(lmer_fit_null,lmer_fit)$'Pr(&gt;Chisq)'[2])

}


par(mfrow=c(1,3))

# repeated measures anova rate of false positive under H0
hist(pp1, main=""aov(myvar ~ group + Error(indiv), gg)"")
100*length(which(pp1&lt;0.05))/length(pp1  )

# Anova(lmer_fit) rate of false positive under H0
hist(pp2, main=""Anova(lmer_fit)"")
100*length(which(pp2&lt;0.05))/length(pp2  )

# anova(null_lmer,full_lmer) rate of false positive under H0
hist(pp3, main=""anova(lmer_fit_null,lmer_fit)"")
100*length(which(pp3&lt;0.05))/length(pp3  )
</code></pre>
"
"NaN","NaN","175418","<p>I ran the mixed effects model using the lme4 package as follows:
lmer9&lt;- lmer(y ~ station + (1|station:tow) + (1|tow), data=my.data)
 where station is the fixed factor and tow a random one...</p>

<p>However I am interested in obtaining the ANOVA for the regression model. 
I ran anova(lmer9) but it gives the ANOVA (F stat ) only for the fixed factor and doesn't give details about the random effects term and the interaction term.</p>

<p>Please suggest ways in which I could obtain a complete ANOVA...</p>

<p>Thanks,
Praniti</p>
"
"0.106119089994502","0.0850871259623034","175766","<p>Four different labs record different subjects' reaction times repeatedly (without them getting older or changing gender). I assume reaction time will depend on age, gender, subject, and lab. I want to test the hypothesis that the effect of age on reaction time varies according to lab. Does this make sense statistically? Would you please let me know if my code is doing it right?</p>

<pre><code>model                    &lt;- lmer( reactionTime ~ age + gender + ( 1 | subject ) + ( 1 | lab ),                     REML=F)
modWithRandomSlopeForAge &lt;- lmer( reactionTime ~ age + gender + ( 1 | subject ) + ( 1 | lab ) + ( 1 + age | lab ), REML=F)
anova( mod, modWithRandomSlopeForAge )
</code></pre>
"
"0.157949437947797","0.145114187428277","175773","<p>I have a hypothesis testing model in which I would like to know if environmental variables I collected influences the probability a bacteria on a given amphibian kills a pathogen. </p>

<p>Following Crawley's R intro to stats book on binomial models I do this:</p>

<pre><code>y &lt;- cbind(df$Antipathogen, df$Total_isolated - df$Antipathogen)
</code></pre>

<p>I still follow Crawley, but bring in a glmer model (using help from online) as I want to examine this at the Site level and not transect level. So, I make transect a random effect. I sampled three sites. One site has one transect, the second site has two transects, and the third site was sampled along an altitudinal gradient and has seven transects.</p>

<p>This is my model:</p>

<pre><code>model &lt;- glmer(y ~ Site + Species + sex + BodyCon + Leaf_litter + (1|Transect), 
               data = df, family = binomial)
</code></pre>

<p>I use the Anova function in car to see which terms are significant when they are introduced into the model</p>

<pre><code>Anova(model, type = ""III"", test.statistic = ""Chisq"")
</code></pre>

<p>I get this:</p>

<pre><code>Response: y
              Chisq Df Pr(&gt;Chisq)    
(Intercept) 21.3200  1  3.887e-06 ***
Site        12.0107  2   0.002466 **
Species      0.0617  2   0.969644  
sex          0.2313  2   0.890785    
BodyCon      0.7058  1   0.400851    
Leaf_litter  2.8763  1   0.089890 . 
</code></pre>

<p>I am starting to understand the function lsmeans in lsmeans package to look at pairwise comparisons to figure out how each of my sites differ from one another. </p>

<p>This is where my question comes in:</p>

<p>What is the appropriate approach -- use the full model and apply the lsmeans function to the full model </p>

<pre><code>lsmeans(model, pairwise~Site, adjust = ""tukey"")
</code></pre>

<p>OR -- remove non-significant terms in a step-wise manner following Crawleyâ€™s (2007, pg. 325) Principle of Parsimony to simplify the full model. And use </p>

<pre><code>model2 &lt;- update(model, ~.-Species)
anova(model, model2, test = ""Chisq"")
</code></pre>

<p>To make sure removing the terms is valid.</p>

<p>Then, I can do lsmeans on the final model that now only contains the only significant variable -- site. </p>

<pre><code>lsmeans(model5, pairwise~Site, adjust = ""tukey"")
</code></pre>

<p>Regardless of how I do it I still get a significant p-value for Site, and the pairwise comparisons give similar results. The p-value is only much lower when I do lsmeans on the reduced model. </p>

<p>I don't know what the better approach is, and doing some basic searching I could not find a similar question. </p>
"
"0.106119089994502","0.106358907452879","176294","<p>I have tried to read documentation with no luck. </p>

<p>Let suppose i have a hundred mice and 50 of those mice has a condition X. I am studying the deliveries of the animals and want to know if an incidence of the event Y is more common among the animals with the condition X during the delivery. Some individuals of the animals has only one delivery but some of the animals can have more than one delivery. I want also standardise other factors(nominal and linear). </p>

<p>I have the following variables: 
ID (Same ID can occur on several rows if there is many deliveries), 
Condition.X, 
Color, 
Age, 
Weight, 
Event.Y(0/1)</p>

<p>I am using the following R-code:</p>

<blockquote>
  <p>model &lt;- glmer(Event.Y ~ Condition.X + Color + Age + Weight + (1 | ID), family = binomial)
  nullmodel &lt;- glmer(Event.Y ~  Color + Age + Weight + (1 | ID), family = binomial)
  anova(nullmodel, model)</p>
</blockquote>

<p>If I got it right the method for approximation above is Laplace. </p>

<p>I have also analysed data with SAS and by using RSPL or MMPL methods the significance is better. </p>

<p><strong>My question:</strong> Is it possible to use similar methods to RSPL/MMPL with glmer-function in R? </p>
"
"0.138122776654839","0.173043648651339","180288","<p>I am trying to understand the effect of a covariate (COVAR) in a linear mixed effects model with 2 categorical IVs (IV1, IV2). In order to illustrate where I am struggling, I had to paste the rather long <code>dput()</code> here:</p>

<pre><code>df &lt;- structure(list(ID=c(1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L),
IV1=structure(c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L),.Label=c(""412A"",""415D"",""512A"",""515A"",""615A""),class=""factor""),
IV2=structure(c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L),.Label=c(""24"",""27"",""2403""),class=""factor""),
DV=c(NA,NA,NA,17,19,27,14,21,21,31,34,NA,22,29,32,16,18,NA,NA,NA,39,33,27.5,28,27,NA,18,NA,24,38,27,15,NA,NA,22,27,17,52,NA,19,35,37,38,30,29,44,74,60,31,54,66,61,60,35,49,NA,52,53,30,36.5,46,57,54,59,NA,41,45,53.5,39,48,43,58.5,50,31,46,23,46,44,25,51,49,32.5,51,37,53,34,52,56,50.5,10,33,31,35,39,27,22,36,21,39,26,35,24,NA,28,39,28,35,21,39,34,30,NA,25,13,NA,31,28,29,32,NA,21,18,32,34,33.5,55,46,26.5,57,29,37,NA,23,52,31,32,41,25,29.5,47,37.5,30,NA,NA,NA,NA,43,43,43,29,42,31,NA,36,16,55,11,30,50,49,38,33,42,45,43.5,35,28,NA,44,36.5,34,41,35,17,38.5,24,49,42,40.5,37.5,15,37.5,32,30,44,25,38,39.5,37.5,43,25,28.5,26,32,43,NA,19,35,19,40.5,33,13,39,39,32,39,44,7,39,40,16,35,52,33,NA,54,24,52,37,31,27,24,31,18,50,16,31,NA,43,NA,42,39,NA,NA,51,36,38,28,NA,30,27,30,31,31,19,NA,38,35,38,21,29,31,8,32,19,23,18,NA,22,30,31,44,31,14,NA,28,25,34,32,39,30,27,33,44,47,16,46.5,12,24,17,40,29,21,47,6,19.5,39,32,28,43,51,42,44,36,48,37,32,37,43,41,10,5,37,28,10,35,45.5,51,22,35,38,39,45,44,46,24,41,37.5,30,NA,33,21,24,NA,25,27,18,NA,22,42,19,30,31,36,19,18,42,25,12,30,32,36.5,27,36,39,37,36,43,35,30.5,11,36,15,43,37,38,23,34,NA,14,39,35,42,38,45,31,41,37,36,37,33,12,44,42,45,39.5,36,44.5,38,14,14,36.5,36,32,43,39,35,38,51,43,48,35,25,49,46,26,46,51.5,35,45.5,NA,53,38.5,45,53,34,51,31,13,36,NA,32,37,43,43,19,35.5,45,41,28,42,44,43,44,34,30,46,43,45,37,33.5,47,23,19,36,38.5,26,41,NA,34,35.5,25,11,38,34,47,9,47,16,20,31,9,9,35,32,NA,34.5,31,NA,32,39,NA,NA,NA,NA,32,26,10,11,NA,37,44,25,15,37,25,10,NA,15,32,NA,24,27,NA,25,31,23,41.5,27,40,31,32,11,NA,14,25,29,36,37,31.5,37,27,21,NA,27,38,NA,NA,25,23,25,40,NA,47,35,33,39,35,38,43,27,35.5,33,28,NA,40,30,48,39,11,35,42.5,42.5,42,42,38,48,46,41,NA,32.5,43.5,34,29,35,NA,38,NA,NA,31,36,31,28.5,15,25,34,30,36,26,35,39,19,NA,NA,31,22,NA,NA,35,35,15,23,38.5,38,NA,36,16,18,26,30,28,NA,25,27,26,25,5,41,29,37,28,34,43,38,29,45,NA,41,32,37,50,31,NA,35,40,41,36,25,34,38,32,38,42,33,34,39,34,39,31,46,8,NA,36,48,25,32,37,NA,40,32,17,37,29,NA,37.5,NA,38,39,NA,44,48,40,NA,20,NA,36.5,20,33,31,41,32.5,28,43,39,29,23,37,32,39,26,36,15,37,31,11,38,29,42,38.5,32,30,37,38,32,33),
COVAR=c(5.2,5.2,5.87,5.68,5.49,7.67,6.3,8.34,7.01,5.51,5.8,4.35,3.95,5.23,6.32,4.01,3.16,3.61,4.67,3.44,5.27,4.59,4.18,4.64,3.97,4.11,3.68,7.57,3.97,5.9,6.02,4.79,5.14,5.84,7.61,4.99,4.18,7.25,3.92,6.3,6.04,5.02,8.01,4.14,8.24,6.21,7.44,5.69,6.31,5.9,6.7,4.96,5.08,4.93,6.4,7.2,7.38,9.59,6.37,8.24,5.6,5.87,4.99,3.64,3.44,5.72,4.52,6.5,4.78,5.18,5.92,8.79,7.65,4.5,4.3,5.76,8.53,4.38,4.46,8.7,8.26,8.89,5.85,6.98,6.65,7.27,8.92,7.43,5.91,5.49,7.64,7.15,6.8,5.74,4.63,4.62,7.02,5.43,9.59,5.42,6.13,8.9,4.66,6.87,6.83,8.38,8.96,5.25,5.54,6.95,8.03,4.33,7.76,6.35,4.99,7.41,6.13,4.67,4.1,4.51,4.6,3.71,6.72,5.37,8.21,6.5,5.46,5.6,7.83,5.08,5.42,3.9,4.88,6.63,4.21,5.3,4.57,8.56,3.84,7.07,4.84,6.19,5.15,3.73,5.32,8.32,7.09,6.06,5.42,7,6.65,5.28,6.08,4.84,4.73,5.15,5.44,6.38,7.4,6.28,4.96,5.14,5.53,8.46,6.93,5.34,5.03,4.4,6.68,7.31,6.17,5.5,9.65,4.36,4.64,6.77,6.95,7.56,8.47,4.68,3.9,4.33,4.77,3.65,5.17,4.44,6.37,4.35,4.55,7.09,4.06,7.78,4.49,6.37,9.03,2.67,3.89,4.38,5.56,6.77,4.48,4.69,4.94,6.17,4.32,4.25,8.11,3.79,5.62,3.99,5.19,4.47,7.07,8.32,8.79,4.27,4.55,4.5,4.15,5.12,10.11,7.68,4.01,6.53,5.66,6.52,5.99,6.62,9.44,5.44,11.1,8.62,5.85,3.82,9.46,8.69,10.36,6.95,6.27,8.37,6.35,7.12,3.71,8.21,5.98,5.49,7.62,6.31,7.98,8.26,6.93,7.03,3.4,3.35,4.74,5.84,7.99,5.07,7.35,7.88,7.44,9.32,7.22,6.47,5.32,5.98,6.61,8.26,7.79,8.19,7.05,3.24,6.5,3.94,7.33,4.4,6.22,5.95,3.56,6.13,6.98,5.2,5.67,5.29,3.6,4.71,5.88,4.27,4.52,5.44,5.39,6.07,6.51,3.24,7.55,4.52,4.19,6.41,5.43,5.48,4.08,5.26,6.99,3.66,5.4,6.13,7.24,10.57,5.92,6.78,6.47,7.78,12.14,8.49,8.77,4.74,8.49,8.03,9.02,5.42,8.22,4.95,5.77,7.49,4.52,4.8,4.62,7,9.01,9.36,4.73,5.14,6.63,7.44,6.91,5.47,7.24,7.46,4.52,6.35,9.13,9.56,8.11,8.97,12.03,8.16,10.79,7.8,6.39,5.8,3.97,7.44,5.03,8.35,6.94,8.44,4.04,6.6,6.04,4.61,5.9,7.72,7.57,6.25,6.96,5.55,9.01,7.44,5.09,5.56,9.17,8.97,7.99,10.16,11.04,6.33,6.96,7,5.08,5.37,4.4,5.49,6.17,6.97,7.65,6.48,5.54,7.79,8.42,7,8.11,5.02,3.9,5.09,4.4,4.63,7.92,9.47,7.05,9.63,4.93,8.36,7.83,10.81,11.58,5.68,11.66,8.01,4.35,5.43,9.3,6.01,5.7,7.64,8.03,7.8,5.9,9.05,6.9,6.36,9.57,6.58,7.66,7.14,5.75,3.58,10.36,6.4,6.09,7.46,7.16,8.78,5.12,4.66,4.61,4.48,4.66,8.11,4.18,5.93,5.97,6.36,6.07,7.4,4.78,8.51,5.21,8.44,5.25,4.68,4.1,3.92,3.57,4.7,5.54,4.5,5.88,5.42,4.45,4.86,6.48,4.71,4.67,4.29,4.71,3.71,5.23,5.64,4.67,3.93,4.79,4.21,4.39,3.4,4.41,4.81,3.85,4.72,4.58,3.09,5.58,4.84,5.19,6.39,3.82,3.89,4.04,4.53,5.8,4.6,4.49,4.35,5.85,4.67,5.44,3.83,5.28,4.33,5.14,3.92,4.37,6.03,6.1,6.38,6.04,5.98,5.26,5.44,3.76,5.37,5.36,6.33,5.52,4.56,4.6,5.58,5.1,4.21,5.03,4.85,4.56,5.79,4.22,3.77,3.34,4.03,6.53,6.97,4.49,6.4,4.49,5.98,5.41,5.03,5.28,4.92,6.92,4.91,4.7,6.6,4.98,6.81,4.8,4.1,4.09,4.87,4.83,4.77,4.4,4.89,4.55,4.55,4.65,5.12,4.85,5.78,5.49,4.58,5.25,5.09,4.93,4.9,5.42,5.33,4.81,4.61,6.67,4.46,5.33,8.05,5.99,4.35,5.06,5.31,4.29,4.29,3.48,4.32,3.86,4.64,4.03,4.18,5.39,4.35,3.54,4.22,3.65,4.63,4.61,4.14,3.4,4.28,5.98,3.48,3.68,5.54,4.22,4.78,3.49,5.84,6.52,6.1,3.9,4.77,4.59,5.31,4.45,4.44,3.97,4.24,3.75,3.84,5.66,4.15,4.35,5.62,5.09,5.65,4.57,4.97,3.53,3.64,3.87,5.49,5.33,4.66,5.85,3.69,6.43,4.73,4.67,4.76,4.7,5.05,8.12,4.53,9.82,3.97,5.24,11.78,5.09,4.94,4.33,5,6.49,7.02,5.1,5.98,4.56,4.06,5.76,4.51,6.56,5.41,4.35,3.76,3.91,3.77,4.69,3.97,4.83,4.78,4.75,4.39,3.46,8.21,3.85,3.48,9.49,3.91,5.19,4.52,4.2,4.7,4.95)),.Names=c(""ID"",""IV1"",""IV2"",""DV"",""COVAR""),class=""data.frame"",row.names=c(NA,675L))
</code></pre>

<p>Model fit with the covariate:</p>

<pre><code>require(lmerTest)
require(car)

m1&lt;-lmer(DV ~ COVAR*IV1*IV2 + (1|IV1:ID), data=df)
</code></pre>

<p>Then I wanted to test whether COVAR is significant and whether an interaction between COVAR and the IVs exists. I used the <code>anova()</code> function provided by <code>lmerTest</code>. Here the covariate is significant as well as the interaction between COVAR and IV2:</p>

<pre><code>anova(m1)
Analysis of Variance Table of type III  with  Satterthwaite 
approximation for degrees of freedom
          Sum Sq Mean Sq NumDF  DenDF F.value    Pr(&gt;F)    
COVAR         4589.8  4589.8     1 567.74  56.506 2.197e-13
IV1            610.4   152.6     4 548.49   1.879  0.112731    
IV2           1223.2   611.6     2 562.64   7.529  0.000593
COVAR:IV1      208.7    52.2     4 560.52   0.642  0.632594    
COVAR:IV2      703.4   351.7     2 563.35   4.330  0.013613  
IV1:IV2        776.8    97.1     8 561.48   1.195  0.299305    
COVAR:IV1:IV2  680.6    85.1     8 561.47   1.047  0.399018
</code></pre>

<p>However when I use <code>Anova(m1, type=3)</code>, just to check and compare different outputs, it comes out like this:</p>

<pre><code>Analysis of Deviance Table (Type III Wald chisquare tests)

Response: DV
           Chisq Df Pr(&gt;Chisq)   
(Intercept)   7.7308  1   0.005429
COVAR         1.9850  1   0.158866   
IV1           6.5038  4   0.164549   
IV2           2.0069  2   0.366610   
COVAR:IV1     0.3739  4   0.984554   
COVAR:IV2     1.6527  2   0.437654   
IV1:IV2       9.5635  8   0.297007   
COVAR:IV1:IV2 8.3786  8   0.397383 
</code></pre>

<p>When I run the <code>Anova(m1)</code> it looks again closer to what <code>anova(m1)</code> produced, however, IV1 is now ""highly"" significant (which is what I would have expected a priori given the nature of IV1), plus there is also an interaction between IV1 and IV2. That being said and also given the discussions regarding type 2 and type 3 SS, I would opt for going ahead with type 2 SS:</p>

<pre><code>Analysis of Deviance Table (Type II Wald chisquare tests)

Response: DV
             Chisq Df Pr(&gt;Chisq)    
COVAR          97.1301  1  &lt; 2.2e-16 
IV1           104.2557  4  &lt; 2.2e-16 
IV2            20.0292  2  4.474e-05 
COVAR:IV1       0.2244  4  0.9941594    
COVAR:IV2       9.1881  2  0.0101119   
IV1:IV2        28.5092  8  0.0003865 
COVAR:IV1:IV2   8.3786  8  0.3973834 
</code></pre>

<p><strong>Question 1:</strong> What is the explanation for these substantial variations between these outputs (especially <code>anova(m1)</code> vs. <code>Anova(m1, type=3)</code> which are both <code>type=3</code> calculations)?</p>

<p>Given the fact that COVAR interacts with IV2 and also that <code>m2 &lt;- lmer(COVAR ~ IV1*IV2 + (1|IV1:ID), data=df); Anova(m2)</code> turns out to be significant (again for IV2), I cannot sell this anlysis as ANCOVA since both additional assumptions for ANCOVA are violated. </p>

<pre><code>Analysis of Deviance Table (Type II Wald chisquare tests)

Response: COVAR
      Chisq Df Pr(&gt;Chisq)    
IV1       3.093  4     0.5424    
IV2     160.317  2  &lt; 2.2e-16
IV1:IV2  34.734  8  2.989e-05
</code></pre>

<p>However, COVAR seems to play an important role and therefore should be kept in the model nonetheless.</p>

<p><strong>Question 2:</strong> Is this reasonable? And if yes, how do I go on and interpret the output of such a model, especially the interaction between COVAR and IV2?</p>

<p>What I would do is plot the interactions for IV1:IV2 and for COVAR:IV2 first:</p>

<pre><code>with(na.omit(df), interaction.plot(IV1,IV2,DV))
require(ggplot2)
ggplot(df,aes(x=COVAR, y=DV))+geom_point(aes(colour=IV2))+
geom_smooth(aes(colour=IV2), method=lm)
</code></pre>

<p>and then start discussing.</p>

<pre><code>&gt; sessionInfo()
R version 3.2.2 (2015-08-14)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 8 x64 (build 9200)

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods  
[7] base     

other attached packages:
[1] ggplot2_1.0.1   car_2.1-0       lmerTest_2.0-29
[4] lme4_1.1-10     Matrix_1.2-2
</code></pre>
"
"0.206864189246938","0.207331679536357","181687","<p>I am just dipping my toe into the ocean that is linear effects models and am working through Barr et al.'s 'Keeping it Maximal' paper, trying to figure out the best way to fit a lmem for my experiment. Say you have three groups given three different types of drug over three days: 100mg on first day, 50mg on second day, 10mg on last day. The outcome measure is how they feel that next day on some scale (e.g. mood), before they are given their daily dose (i.e. so we are measuring the effects of the previous day's dose). However participants don't come in at exactly the same time each day, thus as each time of measurement the drug will have had less time to take effect. </p>

<p>I would like to know how best to include that random effect of 'time elapsed since dose' into this model, and just how best to fit the model really.</p>

<p>This is a toy dataset. I have not built any trends into it. </p>

<pre><code>dose100mg &lt;- c(6,2,9,4,6,5,2,4,6,7,3,2)
dose50mg &lt;- c(1,2,4,3,6,1,3,3,2,1,4,1)
dose10mg &lt;- c(8,9,7,9,6,7,8,9,8,7,1,3)
timeD1 &lt;- c(24.2,20.5,26,30,22,26,19,23,29,30,24,16)
timeD2 &lt;- c(24,16,28,20,19,28,30,20,18,15,27,32)
timeD3 &lt;- c(21,28,29,30,29,17,23,18,24,16,28,21)
subject &lt;- c(1,2,3,4,5,6,7,8,9,10,11,12)
group &lt;- factor(c(0,1,0,2,1,2,0,2,1,2,1,0))

df &lt;- data.frame(subject, group, dose100mg, dose50mg, dose10mg, cov)
</code></pre>

<p>Turn it from wide to long</p>

<pre><code>require(tidyr)

df &lt;- gather(df, dose, score, dose100mg:dose50mg:dose10mg)
</code></pre>

<p>Now add the 'hours elapsed since last dose' variable to the dataframe (btw: if anyone knows how to build this into the gather function above I'd appreciate it) </p>

<pre><code>df$hrsElapsed &lt;- c(timeD1, timeD2, timeD3) 
</code></pre>

<p>Now fit a model. First group*dose plus with random intercepts for subject. </p>

<p>require(lme4)</p>

<pre><code># random intercepts

anDf_randomintercepts &lt;- lmer(score ~ group*dose + (1|subject), data = df)

anova(anDf_randomintercepts)
</code></pre>

<p>Next random slopes, and my first question. Is it better to include hrsElasped as a covariate, like this?</p>

<pre><code>anDf_randomSlopes &lt;- lmer(score ~ group*dose + hrsElapsed + (1|subject) + (1+hrsElapsed|subject), data = df)
</code></pre>

<p>Or to include it as a random effect? like this</p>

<pre><code>nDf_randomSlopes &lt;- lmer(score ~ group*dose + (1|subject) + (1+hrsElapsed|subject), data = df)
</code></pre>

<p>I know it's not the latter because I get an error message. </p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0450795 (tol = 0.002, component 1)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>But I don't know WHY this doesn't work. I would have thought time elapsed would be exactly the sort of variable you'd want to assign to random effects.</p>

<p>What am I doing wrong?</p>

<p>An ancillary question pertains to fitting random slopes for the group-by-subject effect </p>

<pre><code>anDf_randomSlopes &lt;- lmer(score ~ group*dose + (1|subject) + (1+group|subject), data = df)
</code></pre>

<p>When I run this i get the error message</p>

<pre><code>Error: number of observations (=36) &lt;= number of random effects (=36) for term (1 + group | subject); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable
</code></pre>

<p>Why doesn't this work? What does it mean?</p>
"
"0.125561800583481","0.125845556426908","182696","<p>As you may know, R has the problem that it uses the wrong MSE in calculating p values in 2-way ANOVA for split plot experiments. </p>

<p>The whole plot factor is Tree.Name, subplot factor is In.Out, the replicates are Tree.ID, and P is the dependent variable as shown below:</p>

<pre><code>structure(list(Tree.Name = structure(c(5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L), .Label = c(""Akun"", ""Jaror"", ""Ku'ch"", ""Petz-kin"", 
""Puuna"", ""Yax bache""), class = ""factor""), Tree.ID = structure(c(10L, 
10L, 10L, 10L, 10L, 10L, 11L, 11L, 11L, 11L), .Label = c(""AK-1"", 
""AK-2"", ""AK-3"", ""AK-4"", ""AK-5"", ""AK-6"", ""AK-7"", ""AK-8"", ""AK-9"", 
""C-1"", ""C-2"", ""C-3"", ""C-4"", ""C-5"", ""C-6"", ""C-6 "", ""C-7"", ""C-8"", 
""C-9"", ""Ce-1"", ""Ce-2"", ""Ce-3"", ""Ce-4"", ""Ce-5"", ""Ce-6"", ""Ce-7"", 
""Ce-8"", ""Ce-9"", ""J-1"", ""J-2"", ""J-3"", ""J-4"", ""J-5"", ""PK-1"", ""PK-3"", 
""PK-4"", ""PK-5"", ""PK-6"", ""PK-7"", ""PK-8"", ""PK-9"", ""Y-1"", ""Y-2"", 
""Y-3"", ""Y-4"", ""Y-5"", ""Y-6"", ""Y-7"", ""Y-8"", ""Y-9""), class = ""factor""), 
    Sample = c(1L, 2L, 3L, 5L, 6L, 7L, 1L, 2L, 3L, 4L), In.Out = structure(c(2L, 
    1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L), .Label = c(""In"", ""Out""
    ), class = ""factor""), P = c(9.18, 10.38, 6.77, 7.37, 7.97, 
    7.37, 7.07, 6.77, 6.47, 9.78), OM = c(14.71, 15.55, 11.19, 
    15.89, 14.21, 18.91, 12.19, 17.9, 9.84, 30.65), N = c(0.73, 
    0.79, 0.57, 0.8, 0.72, 0.96, 0.61, 0.91, 0.5, 1.55)), .Names = c(""Tree.Name"", 
""Tree.ID"", ""Sample"", ""In.Out"", ""P"", ""OM"", ""N""), row.names = c(NA, 
10L), class = ""data.frame"")
</code></pre>

<p>I found this solution while digging online: </p>

<pre><code>split.plot&lt;-aov(P~Tree.Name*In.Out+
                  Error(Tree.ID),
                data=tree.data)
summary(split.plot)
</code></pre>

<p>However, a friend recommended that I try using <code>lmer()</code></p>

<pre><code>library(""lme4"", lib.loc=""~/R/win-library/3.2"")
x &lt;- lmer(
  P~Tree.Name*In.Out+(1|Tree.ID),
  data=tree.data)
anova(x)
</code></pre>

<p>However the (F) values in the results of the analysis using <code>aov()</code></p>

<pre><code>##                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Tree.Name         5 2273.5   454.7  17.935 2.14e-09 ***
## In.Out            1    3.5     3.5   0.137    0.713    
## Tree.Name:In.Out  2  192.0    96.0   3.787    0.031 *  
## Residuals        41 1039.5    25.4   
</code></pre>

<p>are quite different than those from <code>lmer()</code></p>

<pre><code>## Analysis of Variance Table
##                  Df Sum Sq Mean Sq F value
## Tree.Name         5 322.78  64.556 15.7145
## In.Out            1  25.87  25.870  6.2974
## Tree.Name:In.Out  5  26.04   5.208  1.2677
</code></pre>

<p>My question is twofold:</p>

<p>Why aren't I getting the appropriate F values from lmer? and is one method preferred over the other for any particular reason?</p>
"
"0.0547996624351191","0.054923503638109","185435","<p>I have a dataset with 3 colonies (A, B and C, see below). Each colony is divided into 2 treatments: control and DWV. I use a GLM to test wether there is a difference of the life expectancy <code>last.scan</code>between the treatment groups for each of the colonies. I am using the following command with <code>treatment</code>as a fixed factor and <code>colony</code>as a random factor:</p>

<pre><code>    fit_life = lmer(last.scan~treatment + (1|colony), data = data)
    Anova(fit_life, type = 3) # Type of treatment has a significant effect on the on the life expectancy.
    Response: last.scan
          Chisq Df Pr(&gt;Chisq)    
    (Intercept) 106.976  1  &lt; 2.2e-16 ***
    treatment    25.373  1  4.724e-07 ***
</code></pre>

<p>However, I am not sure about my strategy. I do not know how to find the difference for each of the colonies. Can I do that with a posthoc Tukey test?</p>

<p>I tested for overdispersion, but the proposed model has a lower AIC without overdispersion.</p>

<p>Here is an example of the dataset, RFID is an ID code.</p>

<pre><code>    &gt; head(data, 30)
                      RFID colony treatment last.scan
1  A0 01 03 C0 00 C0 20 01      A   Control        24
2  A0 01 03 C0 00 C0 20 0C      A   Control        21
3  A0 01 03 C0 00 C0 20 1D      A   Control        19
4  A0 01 03 C0 00 C0 20 1E      A   Control        18
5  A0 01 03 C0 00 C0 20 1F      A   Control        31
6  A0 01 03 C0 00 C0 20 21      A   Control        19
7  A0 01 03 C0 00 C0 20 2F      A   Control        18
8  A0 01 03 C0 00 C0 20 37      A   Control        16
9  A0 01 03 C0 00 C0 20 5E      A   Control        23
10 A0 01 03 C0 00 C0 20 79      A   Control        19
11 A0 01 03 C0 00 C0 20 7F      A   Control        17
12 A0 01 03 C0 00 C0 20 8C      A   Control        24
13 A0 01 03 C0 00 C0 20 92      A   Control        26
14 A0 01 03 C0 00 C0 20 95      A   Control        19
15 A0 01 03 C0 00 C0 20 98      A   Control        21
16 A0 01 03 C0 00 C0 20 B8      A   Control        21
17 A0 01 03 C0 00 C0 20 B9      A   Control        20
18 A0 01 03 C0 00 C0 20 D5      A   Control        17
19 A0 01 03 C0 00 C0 20 D9      A   Control        27
20 A0 01 03 C0 00 C0 20 E4      A   Control        26
21 A0 01 03 C0 00 C0 20 FE      A   Control        31
22 A0 01 03 C0 00 C0 3A 02      A       DWV        11
23 A0 01 03 C0 00 C0 3A 0C      A       DWV        22
24 A0 01 03 C0 00 C0 3A 0D      A       DWV        12
25 A0 01 03 C0 00 C0 3A 11      A       DWV        19
26 A0 01 03 C0 00 C0 3A 14      A       DWV        21
27 A0 01 03 C0 00 C0 3A 1D      A       DWV        24
28 A0 01 03 C0 00 C0 3A 24      A       DWV         9
29 A0 01 03 C0 00 C0 3A 2A      A       DWV        16
30 A0 01 03 C0 00 C0 3A 2C      A       DWV        23
</code></pre>
"
"0.150075056296916","0.150414209399047","185936","<p><strong>When we use R code for a linear mixed model like this: <code>model=lmer(y~x1+x2+(1|x3),data)</code>, how can we calculate the variance explained by each fixed variable?</strong></p>

<p>To solve this problem, I tried my best to search on the internet.
My understanding is the following : </p>

<ol>
<li><p>After we run the code : <code>model=lmer(y~x1+x2+(1|x3),data)</code>, we run the function summary(model), then we get the variance explained by the random effect ï¼ˆnamely <code>rvariance</code>ï¼‰. </p></li>
<li><p>we run function  <code>anova(model)</code>, we get the sum of square for each fixed variable; Variance of each fixed variable can be obtained by dividing sum of square into n (the number of observation)  ï¼ˆ<code>fvariance1</code>, <code>fvariance2</code>ï¼‰.</p></li>
<li><p>We use function <code>resquredLR</code> in the MunMIn package to get the whole variance explained by the whole model (whole variance)</p></li>
<li><p>To calculate variance explained by each fixed vaiable,  we use this function</p>

<p><code>fixedvaiance1= whole variance*fvariance1/(rvariance+fvariance1+fvariance2)</code></p></li>
</ol>

<p>This is my current understanding. Can anyone give some comments about my understanding? If there are problems with this approach, I'd be happy if you could point them out.</p>
"
"0.128781856582232","0.157755753708238","186836","<p>Based on a <a href=""http://stats.stackexchange.com/questions/182988/plotting-to-check-homoskedasticity-assumption-for-repeated-measures-anova-in-r"">previous question</a> that I asked about checking assumptions of repeated-measures ANOVAs in R (which turns out to be not so trivial), I'm wondering about the relationship between a repeated-measures ANOVA and a linear mixed model on the same data.</p>

<p>In an excellent exploration of my data, it was suggested to me that I switch to linear mixed models for my full analysis, which I have already done. However, since for reasons of completeness I still need to run a respeated-measures ANOVA, I'm specifically wondering <strong>whether assumption checks on a linear mixed model can be used to infer assumption violations of assuptions for a repeated-measures ANOVA using the same data</strong>.</p>

<p>For example, using the example data below:</p>

<pre><code>set.seed(12)

#Generate variables and data frame
subj &lt;- sort(factor(rep(1:20,8)))
x1 &lt;- rep(c('A','B'),80)
x2 &lt;- rep(c('A','B'),20,each=2)
x3 &lt;- rep(c('A','B'),10, each=4)
outcome &lt;- rnorm(80,10,2)

d3 &lt;- data.frame(outcome,subj,x1,x2,x3)

#Repeated measures ANOVA
m.aov &lt;- aov(outcome ~ x1*x2*x3 + Error(subj/(x1*x2*x3)), d3)

#Linear mixed model assumption checks
require(lme4)
#`subj` as random term to account for the repeated measurements on subject.
m.lmer&lt;-lmer(outcome ~ x1*x2*x3 + (1|subj), data = d3)

# Check for heteroscedasticity
plot(m.lmer)
# or
boxplot(residuals(m.lmer) ~ d3$x1 + d3$x2 + d3$x3)
# Check for normality
qqnorm(residuals(m.lmer))
</code></pre>

<p><strong>Can the assumption plots on m.lmer be used to test assumption violations for m.aov?</strong> For example, if m.lmer displays heteroskedasticity, would that suggest that m.aov is afflicted with heteroskedasticity as well?</p>

<p>Thanks for any insight!</p>
"
"0.260446805357205","0.261035386458931","188361","<p>I'm pretty new in using <code>lmer</code> and be confused about different p-values in Tukey post hoc tests associated with exactly the same estimates. I built a linear mixed model with monetary contributions of human subjects as response variable and their wealth and number of children as explanatory variables. The experiment was designed in a way to contribute for future generations. I don't have repeated measurements of the same individual but some individuals played within the same group. There are several subsets and additional random factors but here I only want to consider the following model where <code>totalcontSubject</code> means contribution of a subject over the entire game, <code>poverty</code> is a factor with 2 levels (rich and poor), and <code>children</code> is a factor with 2 levels (child or noChild). Particularly I'm interested in understanding the fixed effects part of the model.</p>

<pre><code> &gt; summary(TC1)
Linear mixed model fit by REML ['lmerMod']
Formula: totalcontSubject ~ poverty * children + (1 | group_2)
   Data: data

REML criterion at convergence: 414.6

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.42955 -0.45554 -0.09361  0.45228  2.33159 

Random effects:
 Groups   Name        Variance  Std.Dev. 
 group_2  (Intercept) 8.611e-15 9.280e-08
 Residual             1.042e+02 1.021e+01
Number of obs: 58, groups:  group_2, 10

Fixed effects:
                             Estimate Std. Error t value
(Intercept)                    16.200      3.228   5.019
povertyrich                     8.600      3.953   2.175
childrennoChild                 2.800      4.565   0.613
povertyrich:childrennoChild    -4.489      5.642  -0.796

Correlation of Fixed Effects:
            (Intr) pvrty chldrC
povertyrch  -0.816              
chldrnnChld -0.707  0.577       
pvrtyrch:C   0.572 -0.701 -0.809
</code></pre>

<p>If I interpret fixed effects of the summary table in the right way, my intercept denotes poor people with children. The estimate also corresponds to the mean value of this combination in my data. According to my calculations the difference to rich people (shown as <code>povertyrich</code>) actually shows the difference of the intercept to rich people with children, even if not explicitly mentioned by <code>povertyrich</code>. This is the first issue I'm a bit confused. A reduced model only with fixed factor poverty is significant better by <code>anova()</code> but it seems data including children are used for this evaluation.</p>

<p>If I run a Tukey post hoc test by means of my TC1 model, I get a significant difference between rich and poor. But the estimates in the summary actually include children. Estimates of intercept and slope are the means of poor people with children and the difference to rich people with children. They don't correspond to the means of poor or rich data irrespective of parenthood. </p>

<pre><code>summary(glht(TC1, linfct=mcp(povertry=""Tukey"")))

         Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lmer(formula = totalcontSubject ~ poverty * children + (1 | 
    group_2), data = data)

Linear Hypotheses:
                 Estimate Std. Error z value Pr(&gt;|z|)  
rich - poor == 0    8.600      3.953   2.175   0.0296 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>I get even more confused when I run a Tukey post hoc test for a subset where I coded interactions in a column such as poor people with children and rich people with children. In this output I have exactly the same estimates and parameters for these categories (like in the summary shown before for rich poor people exclusively) but the p-values are different. A visual check indicates that there is a significant difference between <code>richChild</code> and <code>poorChild</code> but outputs of <code>glht</code> <code>Interak</code> shows me it is not. Also, a comparison between models <code>anova()</code> with fixed factor poverty vs. fixed factors poverty and children indicates that I can get rid of the variable children in my model. Before I do so, I would like to understand the outputs better. I also worry about the high value for Residual and the correlations in the summary table. </p>

<pre><code>&gt; summary(glht(TC1_2, linfct=mcp(Interak=""Tukey"")))

         Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lmer(formula = totalcontSubject ~ Interak + (1 | group_2), data = data)

Linear Hypotheses:
                               Estimate Std. Error z value Pr(&gt;|z|)
poorNoChild - poorChild == 0      2.800      4.565   0.613    0.927
richChild - poorChild == 0        8.600      3.953   2.175    0.129
richNoChild - poorChild == 0      6.911      4.026   1.717    0.312
richChild - poorNoChild == 0      5.800      3.953   1.467    0.454
richNoChild - poorNoChild == 0    4.111      4.026   1.021    0.735
richNoChild - richChild == 0     -1.689      3.316  -0.509    0.956
(Adjusted p values reported -- single-step method)
</code></pre>
"
"0.0821994936526786","0.0823852554571635","189835","<p>I would appreciate some assistance with setting the statistical analyses of my experiment.</p>

<p>During my experiment 14 participant's motor responses (<code>ptp</code>) were tested once before and twice (<code>time</code>: <code>pre</code>, <code>post15</code>, <code>post60</code>) after a certain intervention (<code>condition</code>). During each test 7 different intensities were applied, and the stimulus was replied 10 times at each <code>intensity</code>.</p>

<p>This gives me the following data <code>io</code>:</p>

<pre><code>&gt; io
       subject condition   time intensity        ptp
1         Sbj1        MI    pre       90%  33.006978
...
10        Sbj1        MI    pre       90%         NA
11        Sbj2        MI    pre       90%  44.005610
...
11760    Sbj14       ERS post60      150%   415.1405
</code></pre>

<p>My variable of interest was the motor responses (<code>ptp</code>) and I would like to know whether I have an effect of intervention (<code>condition</code>) on response amplitude over time and a difference across interventions.
It is also noteworthy, that I had to rejects some of the motor responses due to preactivation. Thus the number of stimuli per <code>intensity</code> varies from 0 to 10.</p>

<p>After a lot of reading by my own, I reached the conclusion that the best approach is a linear mixed model with:</p>

<ul>
<li>dependent variable: <code>ptp</code></li>
<li>fixed factors: <code>condition</code>, <code>time</code> and <code>intensity</code></li>
<li>random effect: <code>subject</code></li>
</ul>

<p>and worked with the following code:</p>

<pre><code>io.model = lmer(ptp ~ condition + time + intensity + (1 | subject), data=io, REML=FALSE)
io.null = lmer(ptp ~ time + intensity + (1|subject), data=io, REML = FALSE)
anova(io.null, io.model)
</code></pre>

<p>However, I am not fully convinced with my setup, also as I don't include any interaction which might happen due to some of the variables.</p>
"
"NaN","NaN","192266","<p>I'm trying to understand mixed effects models in R. I have 2 questions:</p>

<ol>
<li><p>What is the difference between the models<br>
<code>m=lmer(y ~ 1 + x + (1+x|g), data=r.data)</code>
and
<code>m=lmer(y ~ 1 + (1+x|g), data=r.data)</code> ?</p></li>
<li><p>If I would like to know if adding a random effect for x is significant using anova(m0,m1), what would be the proper null model, given <code>m1 = lmer(x + (x|g),,data=r.data)</code>?</p></li>
</ol>
"
"0.157400046933839","0.157755753708238","194472","<p>I'm analysing the results of an experiment using a mixed model. Reaction times for each subject (40 male and 40 female) to 8 stimuli have been registered. The process has been repeated on the same subjects with the same stimuli testing 3 different conditions. So at the end of the experiment I have a dataset composed by 1920 rows [80subject*3conditions*8stimuli] and 5 variables.<br>
The variables are:</p>

<ul>
<li>reaction time: dependent continuous variable;</li>
<li>sex: dummy variable (included as fixed effect);</li>
<li>condition: categorical variable with 3 levels (included as fixed effect);</li>
<li>items: categorical variable with 8 levels (included as random effect);</li>
<li>subject: categorical variable with 20 levels (included as random effect).</li>
</ul>

<p>I'm interested in the reaction time differences between male and female, subjects and conditions. 
I have tried several models and the best one (according to anova function) is:</p>

<pre><code>fm&lt;-lmer(RT~ sex + conditions+ (conditions|stimuli)+(conditions|subject:sex) ,data = exper, REML=FALSE)
</code></pre>

<p>But I don't know if I have chosen the right setting since I have clustered (40 males and 40 females) and repeated-measures (8 stimuli tested on the 80 subjects for 3 conditions)  data in the same model. I don't know how to handle this situation since during my researches I have found informations just on model for clustered or repeated-measures data.<br>
Thanks</p>
"
"0.117452309662454","0.0840840992495391","196629","<p>I have a question concerning a mixed model ANOVA conducted using the lmer function of the lme4 package. Despite a lengthy research across tutorials and forum entries I do not have a clear response to how exactly my lmer formula should be written.</p>

<h3>Experimental design</h3>

<p>32 participants take place in the study, half of which constitute the test group, the other half the control group.
Both groups do a pre and a posttest, and in both testing sessions they see 18 items of 3 conditions (= 54 items per testing session), and the stimuli of each condition are not repeated in the pre and the posttest.</p>

<p>My question is now if the following formula is correct/complete and if the fact that we deal here with repeated measures is automatically calculated: (dv: dependent variable)</p>

<h3>Formula</h3>

<pre><code>fit1 &lt;- lmer(dv ~ (condition * session * group) + (1|item_ID) + (1|subj_ID) + group, 
             myData, REML=FALSE)
</code></pre>

<p>Or is one of the following more accurate: </p>

<pre><code>fit2 &lt;- lmer(dv ~ (condition * session * group) + (1|item_ID) + (1+session|subj_ID) + group, 
             myData, REML=FALSE)

fit3 &lt;- lmer(dv ~ (condition * session * group) + (1|item_ID) + 
             (1+session+condition|subj_ID) + group, myData, REML=FALSE)
</code></pre>

<p>I have also conducted the ANOVA comparing the three models (<code>anova(fit1,fit2,fit3)</code>) but given that these three models are not equally complex this comparison is actually not valid, is this assumption right?</p>

<p>Moreover, I have included the <code>pbkrtest</code> package to obtain degrees of freedom and p values; are the obtained dfs usable for reporting the inferential statistics or do I need to apply an additional transformation?</p>
"
"0.0949157995752499","0.0951302988308988","203199","<p>I have been looking into segmented modelling, but I need to use a mixed effects framework, as I need to include some random effects.</p>

<p>I originally had the following model, which came out as the best fit through model selection with anova:</p>

<pre><code>m01 &lt;- lmer(specARC ~ year + (1|reference/study) + (1|country), data = manHunt, weights = SumOfharvestValue)
</code></pre>

<p>Where specARC is an arcsine transformed proportion.</p>

<p>I then tried to integrate a segmented modelling approach:</p>

<pre><code>bp = 2000

b1 &lt;- function(x, bp) ifelse(x &lt; bp, bp -x, 0)
b2 &lt;- function(x, bp) ifelse(x &lt; bp, 0, x - bp)

foo &lt;- function(bp)
{
  mod &lt;- lmer(specARC ~ b1(year, bp) + b2(year, bp) + (b1(year, bp) + 
                                  b2(year, bp) |reference), data = manHunt)
  deviance(mod)
}

search.range &lt;- c(1970,2015) 

foo.opt &lt;- optimize(foo, interval = search.range)

bp &lt;- foo.opt$minimum

bp


foo.root &lt;- function(bp, tgt)
{
  foo(bp) - tgt
}

tgt &lt;- foo.opt$objective + qchisq(0.95,1)
lb95 &lt;- uniroot(foo.root, lower=search.range[1], upper=bp, tgt=tgt)
ub95 &lt;- uniroot(foo.root, lower=bp, upper=search.range[2], tgt=tgt)

ub95$root


lb95$root 
</code></pre>

<p>But do I do model selection first, without the segmented model, or do I do the segmented model after I have found the break point?</p>

<p>Can I compare the following models with anova:</p>

<pre><code>mod &lt;- lmer(specARC ~ b1(year, bp) + b2(year, bp) + (b1(year, bp) + 
                b2(year, bp) |reference), data = manHunt)

mod1 &lt;- lmer(specARC ~ year+ (1|reference), 
           data = manHunt, weights = SumOfharvestValue)
</code></pre>

<p>anova(mod, mod1)</p>

<p>I also can't seem to add the nested component in the segmented model...</p>

<p>Hope someone can help!</p>
"
"0.246598480958036","0.24715576637149","203295","<p>I want to use linear mixed effects models (<code>lme4::lmer</code>) for the selection of features (dependent variables, >1000) on significant differences between specific groups (combinations of independent variables). Therefore I would make a model for each feature (gaining over 1000 models). I then look for significant differences by using contrasts in a posthoc test (<code>multcomp::glht</code>). My question is: given my data does it make sense to use LME models or is a repeated measure ANOVA more appropriate from statistical point of view? And if appropriate which of the models below (if any) should I use? Am I using the right grouping variables and reference levels? </p>

<p>My example data (included at the end of this post) with a representative feature (out of >1000) (<em>Mrkr</em>) looks as follows: The dependent variable (<em>Mrkr</em>) is a continuous variable.</p>

<p>As for the independent variables I have two different species (<em>Spcs</em>: C450, DX20) that undergo two different challenges (<em>Chal</em>: mck, inf). A sample is taken from each subject (<em>Subj</em>: B1 ... B10, D1 ... D10) at different time points (<em>Day</em>: day 0 ... day 30, but differs between species) after the challenge. This let me to believe that I have a random effect for the species since we are following the effect of the challenge trough time on each individual subject.</p>

<p>Because I am firstly interested in the differences between challenges for the same time points for each individual species I constructed a new group (Grp) which is a combination of <em>Chal</em>, <em>Spcs</em> and <em>Day</em> (<em>Grp</em>: I_C_00...M_D_05). </p>

<p>The model I initially used to select features is as follows:</p>

<pre><code>library(""lme4"")
library(""multcomp"")
library(""ggplot2"")

#mydata and K are defined at the end of this post for clarity
mydata$Grp &lt;- relevel(mydata$Grp, ref = ""M_C_03"")   #Set reference level.
mod01 &lt;- lmer(Mrkr ~ Grp + (1|Subj), data = mydata) #Make model with random intercept for Subj.
summary(glht(mod01, linfct = K))                    #Gives the p vals for my specified contrasts (yes, I am aware of the dangers of using p values).

#plots time courses for each individual subject for visualization
ggplot(mydata, aes(x = Day, y = Mrkr)) + 
  geom_point() + 
  geom_line(aes(group = 1)) + 
  facet_wrap(Spcs  + Chal ~  Subj , nrow = 4, ncol = 5) +
  theme(axis.text.x = element_text(angle = 90))
</code></pre>

<p>First I set the benchmark level at the C450/mck/day03 because this is the only time point that all <em>Spcs</em> and <em>Chal</em> combinations have in common. I don't know if this is necessary but intuitively it seemed the right thing to do. If I understand the literature correctly <code>mod01</code> allows for a random intercept for each individual subject. The contrasts were checked with <code>glht()</code> and contrast matrix <code>K</code> (included at the end of this post).</p>

<p>However, after some contemplation and wrestling trough the literature I figured that slopes of the effects might well be dependent on species and challenge and so I came up with the next model:</p>

<pre><code>mod02 &lt;-  lmer(Mrkr ~ Grp + (1 + Spcs|Subj) + (1 + Chal|Subj), data = mydata)
summary(glht(mod02, linfct = K))
</code></pre>

<p>Finally I realized that the subjects are nested in species so my final model would be:</p>

<pre><code>mod03 &lt;-  lmer(Mrkr ~ Grp + (1 + Spcs|Subj/Spcs) + (1 + Chal|Subj/Spcs), data = mydata)
summary(glht(mod03, linfct = K))
</code></pre>

<p>It turns out that the p values for the posthoc tests for all three models are very similar and by using <code>mod01</code> I found already a set of interesting features. However I want to know which model, if any, is the one that describes my random effects structure most accurate. </p>

<p>Kind regards</p>

<pre><code>#Data frame with exampel data
mydata &lt;- structure(list(Subj = structure(c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 
3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 
4L, 5L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 6L, 7L, 8L, 
9L, 10L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 11L, 12L, 
13L, 14L, 15L, 11L, 12L, 13L, 14L, 15L, 11L, 12L, 13L, 14L, 15L, 
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 16L, 17L, 18L, 
19L, 20L), .Label = c(""B1"", ""B2"", ""B3"", ""B4"", ""B5"", ""D1"", ""D2"", 
""D3"", ""D4"", ""D5"", ""B6"", ""B7"", ""B8"", ""B9"", ""B10"", ""D6"", ""D7"", 
""D8"", ""D9"", ""D10""), class = c(""ordered"", ""factor"")), Chal = structure(c(2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c(""mck"", 
""inf""), class = ""factor""), Day = structure(c(1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 
4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 2L, 2L, 3L, 
3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 
6L, 6L, 6L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L), .Label = c(""day00"", 
""day03"", ""day05"", ""day08"", ""day18"", ""day30""), class = ""factor""), 
Spcs = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""C450"", 
""DX20""), class = ""factor""), Grp = structure(c(1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 
4L, 4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 
7L, 7L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 
10L, 10L, 10L, 11L, 11L, 11L, 11L, 11L, 12L, 12L, 12L, 12L, 
12L, 13L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 14L, 15L, 
15L, 15L, 15L, 15L, 16L, 16L, 16L, 16L, 16L), .Label = c(""I_C_00"", 
""I_C_03"", ""I_C_05"", ""I_C_08"", ""I_C_18"", ""I_C_30"", ""I_D_00"", 
""I_D_03"", ""I_D_05"", ""M_C_03"", ""M_C_05"", ""M_C_08"", ""M_C_18"", 
""M_C_30"", ""M_D_03"", ""M_D_05""), class = ""factor""), Mrkr = c(2399.849218, 
1762.777866, 1774.939084, 1461.419699, 1368.804546, 1126.699114, 
1557.100579, 1369.699809, 2146.155143, 1006.337489, 856.6567507, 
856.775057, 683.6396713, 459.4223325, 651.9368177, 276.29906, 
559.5347751, 294.9815688, 304.0486838, 325.3924639, 814.1927642, 
1424.429248, 949.7589963, 1469.905312, 1319.214754, 1268.595709, 
1184.70564, 870.8718067, 682.4456494, 1177.223394, 512.4325239, 
360.1808537, 525.5669889, 488.9804713, 541.2128606, 1475.036591, 
1132.173062, 1256.048921, 1843.616592, 1892.594627, NA, NA, 
NA, NA, 1100.36921, 1566.125524, 720.1838491, 930.9203894, 
1069.445235, 866.415662, 1021.757551, 1310.491871, 1459.588906, 
1081.572941, 871.4666637, 511.329317, 1010.567794, 513.5011174, 
1005.356367, 734.6804492, 1144.873026, 1467.333437, 1496.635963, 
1519.662963, 1105.464233, 916.0012586, 1248.81632, 591.8699979, 
887.1439846, 610.6604304, 376.610192, 317.2069945, 479.5381028, 
279.0847122, 410.3471923, 491.626902, 331.8743751, 632.6303274, 
588.0827988, 513.1653612)), .Names = c(""Subj"", ""Chal"", ""Day"", 
""Spcs"", ""Grp"", ""Mrkr""), row.names = c(NA, -80L), class = ""data.frame"")

#Contrast matrix for glht()
K &lt;- structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 
0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 
0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1), .Dim = c(11L, 
16L), .Dimnames = list(c(""I_C_00 vs I_C_03"", ""M_C_03 vs I_C_03"", 
""M_C_05 vs I_C_05"", ""M_C_08 vs I_C_08"", ""M_C_18 vs I_C_18"", ""M_C_30 vs I_C_30"", 
""I_D_00 vs I_D_03"", ""M_D_03 vs I_D_03"", ""I_C_00 vs I_D_00"", ""M_C_03 vs M_D_03"", 
""M_C_05 vs M_D_05""), c(""M_C_03"", ""I_C_00"", ""I_C_03"", ""I_C_05"", 
""I_C_08"", ""I_C_18"", ""I_C_30"", ""I_D_00"", ""I_D_03"", ""I_D_05"", ""M_C_05"", 
""M_C_08"", ""M_C_18"", ""M_C_30"", ""M_D_03"", ""M_D_05"")))
</code></pre>
"
"0.0821994936526786","0.0823852554571635","203717","<p>I am trying to do model simplification looking at how different factors may affect distance. So I have snails kept in several habitats and I wanted to see if that affects how closely another snail may follow that snail. So I start off with this model: </p>

<pre><code>  model1 &lt;- lmer(sqrt(dist+6)~  (1|snail)+food+stress+food:stress+
       weight+OriginalL+FollowedL)
summary(model1)
</code></pre>

<p>and the summary is this: </p>

<pre><code>  Linear mixed model fit by REML ['lmerMod']
  Formula: sqrt(dist + 6) ~ (1 | snail) + food + stress + food:stress +  
weight + OriginalL + FollowedL

REML criterion at convergence: 561.1

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.2941 -0.7698 -0.3347  0.7515  1.9564 

Random effects:
 Groups   Name        Variance Std.Dev.
 snail    (Intercept) 0.000    0.000   
 Residual             2.334    1.528   
Number of obs: 148, groups:  snail, 37

Fixed effects:
                               Estimate Std. Error t value
(Intercept)                    4.960927   0.662947   7.483
foodSweetPotato               -0.219039   0.357768  -0.612
stressshelter                 -0.246649   0.355999  -0.693
weight                         0.002520   0.063259   0.040
OriginalL                      0.015549   0.013072   1.189
FollowedL                     -0.008044   0.005972  -1.347
foodSweetPotato:stressshelter -0.300143   0.503215  -0.596

Correlation of Fixed Effects:
            (Intr) fdSwtP strsss weight OrgnlL FllwdL
foodSwetPtt -0.309                                   
stressshltr -0.315  0.502                            
weight      -0.615  0.008  0.009                     
OriginalL   -0.617 -0.021  0.032  0.123              
FollowedL   -0.470  0.118  0.059  0.087 -0.004       
fdSwtPtt:st  0.230 -0.707 -0.708 -0.008 -0.024 -0.055
</code></pre>

<p>Should I remove the least significant factor or remove the interactions first?</p>

<p>And after this is it a simple anova between my first model and most simplified model?</p>
"
"0.0474578997876249","0.0475651494154494","204451","<p>I have data from various points during an experiment, 4 days. I want to account for the repeated measures design because I was sampling the same block. in lmer in r I am unable to use ""block"" as my random effect (1|block) because it has only 1 level. </p>

<p>Am I going about this all wrong? </p>

<p>I tried to use an anova but I do not know where to specify block as a repeated measure in this.</p>

<p>a1=aov(log(mgChl_m3)~Day, data=Ext_CoreT2)
Anova(a1, type=""III"", test.statistic=F)</p>
"
"0.142373699362875","0.126840398441198","205151","<p>I am using a generalized Linear Mixed-Effects model to look at the effects of different treatments on a density of trichomes.</p>

<p>The model is :</p>

<pre><code>fitPoisson = glmer(Count_trichomes ~ Treatment1*Treatment2*Treatment3 + 
                         (1 | Block/Code) + offset(log(Length)), family=poisson(), data=dataset)
</code></pre>

<p>Treatment 1 and 2 has 2 levels (0 and 1) and Treatment 3 has 3 levels (0,1,2). Block accounts for the replicates and Code, for each individual. Length is in cm.</p>

<p>An anova(fitPoisson) told me that treatments 1 and 3 are significant and that there is no interactions. What I want now is to know what the density is for each level of treatments.</p>

<p>So I used a lsmeans to look at the differences : </p>

<pre><code>    &gt; lsmeans(fitPoisson, ~ Treatment1)

     Treatment1   lsmean         SE df asymp.LCL asymp.UCL
     0           5.309106 0.06113705 NA  5.189280  5.428933
     1           5.471452 0.06114033 NA  5.351619  5.591285

     Results are averaged over the levels of: Treatment2, Treatment3
     Results are given on the log (not the response) scale. 
     Confidence level used: 0.95
</code></pre>

<p>I can see that the density of level 0 is lower than the density of level 1, but I dont understand what are the units used. It doesn't seems like it is for trichomes/cm, since the mean for level 0 is 107 trichomes/cm and the mean for level 1 is 131 trichomes/cm (calculated in excel).</p>

<p>When I transform back from the log scale, it gives me : </p>

<pre><code>    &gt; summary(lsmeans(fitPoisson, ~ Treatment1), type = ""response"")

     Treatment1   rate       SE df asymp.LCL asymp.UCL
     0           202.1694 12.36004 NA  179.3393  227.9058
     1           237.8053 14.53949 NA  210.9496  268.0799

    Results are averaged over the levels of: Treatment2, Treatment3 
    Confidence level used: 0.95 
    Intervals are back-transformed from the log scale 
</code></pre>

<p>Which is still far from the means I found in excel.</p>

<p>Maybe I just don't understand the information lsmeans is giving me, or I am not using the right function.</p>
"
"0.157400046933839","0.157755753708238","205160","<p>I have subjects that each provide arbitrary number of ratings (on a scale 0-5) to songs of 20 different genres. Each of the subjects can be of one of the five personality types (A, B, C, D, E) with scores ranging from 1-7 for each of the five types. I am interested in finding the relationship between personality and ratings for each genre, and here is what I tried so far and what concerns I have:</p>

<ol>
<li><p>Since there are repeated measures per subject, for each genre, I first tried to take just one measurement per subject, i.e., median rating for that genre. I also tried to categorize the personality scores into two categories high (4-7) and low (1-4). For each genre I tried to compare median rating of high and low groups using a wilcox test. But I felt aggregation was killing the signal since most users have about the same median rating. </p></li>
<li><p>I want to use repeated measures of ratings by subjects as they are, but still see how users with high and low personality scores on any one personality type compare against each other. Because there is imbalance in the number of ratings each individual subject can produce, I'm assuming I should go with repeated-measures ANOVA with imbalance - something like an lmer? So is something like this correct in R, where A1 is A transformed into high and low scale from 1-7 scale.</p>

<pre><code> lmer(subj_rating ~ A1 + (1 | subjId), data=dat.jazz.A)   
</code></pre></li>
</ol>

<p>Since similar ratings can be produced by users for different songs, should I also include (1|songId) in the expression above?</p>

<ol start=""3"">
<li><p>Is there a way I can do #2 above, without making that conversion of A to A1, i.e, as a continuous variable. Which statistical approach should I then use?</p></li>
<li><p>Because I am trying to run multiple individual tests for each genre and personality, is this the case of a multiple-comparison problem? Do I need any adjustments?</p></li>
</ol>

<p>Here is how my sample data looks like (filtered for one genre, songId skipped)</p>

<pre><code>subjId, rating, A, A1
38675, 3.5, 6.0, 1
38675, 3.0, 6.0, 1
38675, 2.5, 6.0, 1
38676, 4.0, 5.5, 1
38676, 2.0, 5.5, 1
38683, 5.0, 3.5, 0
38683, 4.5, 3.5, 0
38683, 4.0, 3.5, 0
38683, 4.0, 3.5, 0
38683, 3.5, 3.5, 0
38683, 4.0, 3.5, 0
...
</code></pre>
"
"0.0968730322865161","0.116510345607093","207851","<p>I am trying to get an ANOVA table for a mixed model in R with nested design fit with package <code>lme4</code>, function <code>lmer()</code> in R. When I obtain the ANOVA table, R includes only the sums of squares for the fixed factor, and does not include anything for the random factor. Beyond R usage I'm questioning the underlying statistical process at work here - shouldn't random factors be included on an ANOVA table for a mixed model?</p>

<p>For example, I fit the model as:  </p>

<pre><code>lmer(response ~ A + (1 | A:B))  # where A is fixed, B is random, and B is nested within A. 
</code></pre>

<p>I use the code below and just obtain output for the A fixed factor:  </p>

<pre><code>anova(lmer(response ~ A + (1 | A:D)))

   Sum Sq  Mean Sq NumDF DenDF F.value    Pr(&gt;F)    
A 0.19812 0.099058     2     9  33.437 6.818e-05 ***
</code></pre>
"
"0.212479222860543","0.22310032643658","208273","<p>I want to analyse the effect of different treatment types (<code>control, treatment1, ..., treatment4</code>) on the surface of specimens made of certain materials (<code>plastic, metal</code>). The undamaged area of the surface is measured <code>before</code> and <code>after</code> the treatment.</p>

<p>According to this design I specified a mixed model using lme4 as follows:</p>

<pre><code>require(""lme4"")
data &lt;- read.csv(""http://pastebin.com/raw/G4D8dh1f"")
mm1  &lt;- lmer(undamaged_area ~ time*material*treatment + (1|specimen_id), data)
</code></pre>

<p><strong>Questions:</strong> </p>

<ol>
<li><p>Is the mixed model the optimal choice in this case? I found some hints that an ANCOVA (something like <code>lm(undamaged_area_after ~ material*treatment + undamaged_area_before, data)</code>) might be an alternative approach.</p>

<p>A closer look on the diagnostic plots of the mixed model makes me very suspicious:</p>

<pre><code>plot(mm1); require(""lattice""); qqmath(mm1)  
</code></pre>

<p><a href=""http://i.stack.imgur.com/LDxEY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LDxEY.png"" alt=""diagnostic plots""></a></p></li>
<li><p>Does the plots actually indicate a violation of model assumptions? Does the strange pattern come from a misspecification of the model? </p></li>
</ol>

<p><strong>Progress after <a href=""http://stats.stackexchange.com/a/208463/112794"">donlelek's answer</a> (=mixed model not required):</strong></p>

<p>Just to be clear: The treatments were measured in different pieces of metal/plastic. So every piece is exactly measured twice - before and after the treatment. Thus, we are aiming for the ANOVA on damage, I guess. I had the impression to lose informations by just substracting the pre-post values. I did further research in the literature (with my limited knowledge in statistics). But according to <a href=""https://pdfs.semanticscholar.org/b764/e331525ec9ba814b51ee890aea7f663e175d.pdf"" rel=""nofollow"">""Pretest-posttest designs and measurement of
change""</a> the use of such gain scores seems to be ok:</p>

<blockquote>
  <p>""First, contrary to the
  traditional misconception, the reliability of gain scores is high in many practical situations, particularly when the pre- and posttest scores do not have equal variance and equal reliability.""</p>
</blockquote>

<p>so we have the following model:</p>

<pre><code>library(tidyr)
library(dplyr)

data &lt;- read.csv(""http://pastebin.com/raw/G4D8dh1f"")    
data_wide  &lt;- data %&gt;% 
  spread(time, undamaged_area) %&gt;% 
  separate(specimen_id, c(""mat"", ""id"", ""tx"")) %&gt;% 
  mutate(damage = before - after, 
         unique_id = paste(mat, id, sep = ""_"")) %&gt;% 
  select(-mat, -tx, -id)

# model for full factorial with replications
mm2  &lt;- lm(damage ~ material * treatment , data = data_wide)
</code></pre>

<p>The variance problem still remains. Confirmed by Levene's test:</p>

<pre><code>library(car)
leveneTest(damage ~ material * treatment, data_wide)

# Levene's Test for Homogeneity of Variance (center = median)
#       Df F value    Pr(&gt;F)    
# group  9  4.8619 2.646e-05 ***
#       90                      
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1    
</code></pre>

<p>Following the link suggested by donlelek I found different approaches for <a href=""http://stats.stackexchange.com/questions/91872/alternatives-to-one-way-anova-for-heteroskedastic-data/91881#91881"">anovas with heteroskedastic data</a>. I tried to stabilize the variance by using log-transformation. Then Levene's test says that heterogeneity of the variance diappears:</p>

<pre><code>data_wide &lt;- within(data_wide, log_damage &lt;- log(damage+1))
leveneTest(log_damage ~ material * treatment, data_wide)

# Levene's Test for Homogeneity of Variance (center = median)
#       Df F value Pr(&gt;F)
# group  9  0.6916 0.7147
#       90
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1   
</code></pre>

<p>The diagnostic plots seems not to be as weird as the previous ones (see <a href=""http://stats.stackexchange.com/a/208463/112794"">donlelek's answer</a>):</p>

<pre><code>mm3 &lt;- lm(log_damage ~ material * treatment, data_wide)
plot(fitted(mm3), residuals(mm3, type = ""pearson""))
qqnorm(residuals(mm3, type = ""pearson""))
</code></pre>

<p><a href=""http://i.stack.imgur.com/NDTAj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NDTAj.png"" alt=""enter image description here""></a></p>

<p>The anova table gives the following output:</p>

<pre><code>anova(mm3)

# Analysis of Variance Table
#
# Response: log_damage
#                    Df Sum Sq Mean Sq F value    Pr(&gt;F)    
# material            1  0.436  0.4362  0.7462      0.39    
# treatment           4 83.652 20.9129 35.7786 &lt; 2.2e-16 ***
# material:treatment  4 20.213  5.0532  8.6452 5.966e-06 ***
# Residuals          90 52.606  0.5845                      
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Just for double checking:</p>

<p>The <code>Anova</code> function from <code>car</code> package offers an option for heteroscedasticity correction. Interestingly, this function generates a roughly similar table for the ""non-transformed"" <code>mm2</code>:</p>

<pre><code>Anova(mm2, white.adjust=TRUE)

# Analysis of Deviance Table (Type II tests)
# 
# Response: damage
#                    Df       F    Pr(&gt;F)    
# material            1  1.4251    0.2357    
# treatment           4 28.2422 3.329e-15 ***
# material:treatment  4  9.5739 1.701e-06 ***
# Residuals          90                      
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p><strong>New questions:</strong> </p>

<p>This double check gives me more confidence in the results. But do you think that the log-transformation is a reasonable approach? Can I trust the model now?</p>
"
"0.142373699362875","0.126840398441198","208913","<p>I have a repeated data on which Iâ€™d like to apply <code>lmer</code> from package <code>lme4</code>.</p>

<p>My data: every quarter of a certain year, the dependent and a independent continuous variables (<strong><em>y</em></strong> and <strong><em>x</em></strong>) of my subjects are measured. Some of those subjects receive a treatment starting in a random period (the treatment goes on for the following periods). For instance, if subject <strong><em>i</em></strong> receive the treatment on the 2 quarter, heâ€™ll also receive it on quarters 3 and 4. A comparable example of my data can be generated with the following code:</p>

<pre><code>require(""dplyr"")

set.seed(321)

nsubj = 100
subject = paste0(""subj"",1:nsubj)
time  = 1:4
trtmt.time = data.frame(subject = subject, time = sample(time,nsubj,replace = TRUE), trtmt = sample(0:1,nsubj,replace = TRUE))
df.tmp = expand.grid(subject = subject,time = time)
df.tmp = merge(x = df.tmp, y = trtmt.time,by=c(""subject"",""time""), all.x = TRUE)
df.tmp[is.na(df.tmp)] = 0

df = data.frame(mutate(group_by(df.tmp,subject), trtmt=cumsum(trtmt)))
df$x = rnorm(nrow(df),10,2)
    df$y = rnorm(nrow(df),150,30)

### A little extract from the data:
# Subject3 didn't receive the treatement
# Subject4 started receiving the treatement from period 2
&gt; print(df[9:16,])

   subject time trtmt         x        y
9    subj3    1     0  9.572928 185.1341
10   subj3    2     0  9.314505 200.2399
11   subj3    3     0  6.872656 114.9252
12   subj3    4     0 11.039379  89.4818
13   subj4    1     0 10.499021 127.7390
14   subj4    2     1 10.849371 110.1334
15   subj4    3     1 13.086679 173.4492
16   subj4    4     1 10.675099 161.6651
</code></pre>

<p>My final goal is to test if the treatment is effective.  Hereâ€™s how I proceeded: first I test if <strong><em>time</em></strong> and <strong><em>x</em></strong> are significant on the model:</p>

<pre><code>require(lme4)
# Baseline model
baseline   &lt;- lmer(y ~ (1|subject), df, REML=FALSE)

# Compact model
model.fit1  &lt;- lmer(y ~ x+time+(time|subject), df, REML=FALSE)
anova(baseline,model.fit1)
</code></pre>

<p>Of course, with the example data I provided here, the comparison between the models says that the model with <strong><em>time</em></strong> and <strong><em>x</em></strong> is not better. However, this is not the case for my real data (p-value&lt;0.001). Next, I compare model.fit1 with a model including the treatment:</p>

<pre><code># Augmented model
model.fit2 &lt;- lmer(y ~ trtmt+x+time+(time|subject), df, REML=FALSE)
anova(model.fit1,model.fit2)
</code></pre>

<p>Since Iâ€™m new on repeated analysis, I was wondering if what Iâ€™m doing is a good approach. Should I include <strong><em>x</em></strong> on the random part of the model?</p>
"
"0.0848952719956018","0.0850871259623034","209939","<p>I would be extremely grateful for some advice on how to correctly fit linear mixed effects models with my repeated measures design!</p>

<p>In my experiment, subjects completed a task with 3 difficulty conditions: easy, medium, and hard. In addition, I have assessed subjects' depressive symptoms on a continuous scale. The outcome measure is accuracy.</p>

<p>""Medium"" here serves as a comparison condition. I hypothesized that depressive symptom severity would moderate the impact of difficulty on accuracy, such that for individuals who are low in depressive symptoms, difficulty would have little impact on accuracy. By contrast, I hypothesized that individuals who are high in depressive symptoms would perform worse during hard rounds and better during easy rounds. Thus, I planned to examine the interaction between difficulty and depressive symptoms.</p>

<p>To accomplish this analysis, my thought was to fit linear mixed effects models using lme4 package in R -- a full model and a reduced model. Then I would implement a likelihood ratio test. I planned to model both random slopes and random intercepts for subjects.</p>

<p>Here's how I would have thought to examine the interaction of a 2-level within-subjects factor (dummy coded) and a centered continuous predictor:</p>

<pre><code>full.model &lt;- lmer(accuracy ~ dummy_difficulty * depression + 
  (1 + dummy_difficulty|subject), REML=FALSE)
reduced.model &lt;- lmer(accuracy ~ dummy_difficulty + depression + 
  (1 + dummy_difficulty|subject), REML=FALSE)
anova(reduced.model, full.model)
</code></pre>

<p>However, my difficulty factor actually has 3 levels.  Since the ""medium"" condition is the comparison condition, I created two dummy variables as follows:</p>

<blockquote>
  <p>dummy_1: easy = 1, medium = -1, hard = 0</p>
  
  <p>dummy_2: easy = 0, medium = -1, hard = 1</p>
</blockquote>

<p>But now, with the two dummy variables, I'm at a loss as to how to model random slopes and random intercepts.  Can anyone help me out?  I would really, really appreciate any advice you might have to offer!</p>
"
"0.171111891110113","0.171498585142509","210532","<p>I am using lmer (from the lme4 R package) on a dataset with 6 variables:
SubjectID, ImageID, Category, Brightness, Contrast and ResponseTime, where the last three are continuous variables.</p>

<p>(and yes, I am new to this).</p>

<p>Below is an example of the data:        </p>

<pre><code>SubjectID   ImageID Category    Brightness  Contrast    ResponseTime
s1          1       1           11          6           1235
s1          2       1           12          8           1143
s1          3       2           14          3           1343
s1          4       2           13          4           1432
s2          1       1           11          6           1732
s2          2       1           12          8           1456
s2          3       2           14          3           1723
... 
</code></pre>

<p>This is from an experiment where subjects name a collection of images while their latency is recorded in the ""ResponseTime"" variable. Each unique image can be identified by ""ImageID"". The image collection can be divided into two Categories (1 or 2).</p>

<p>My goal here is to attempt to take the individual images into account while testing for an effect of the ""Category"" variable (A repeated-measures ANOVA with aggreated ""ResponseTime"" values on Category+SubjectID shows a significant difference between the two image categories, but I know that the categories are unbalanced with regard to some image statistics).</p>

<p>My initial attempt were the following:</p>

<pre><code>model1 = lmer(ResponseTime~Category + (1+Category|SubjectID) + (1+Category|ImageID), data=x)
null = lmer(ResponseTime~1 + (1+Category|SubjectID) + (1+Category|ImageID), data=x)
anova(model1,null)
</code></pre>

<p>The result show no significant difference between the reduced model and the model including ""Category"" as a fixed effect. </p>

<p>The images are unbalanced with regards to both Contrast and Brightness and I want to check if these variables can be used to test if the influence of Category is still significant while taking these into account.
(I am looking for some variables that can help explain the null result, so that an improved experiment can be performed).</p>

<p>My question is thus:
Is the approach below correct with regards to testing the hypothesis that taking ""Brightness"" and ""Contrast"" into account reduces the contribution of the ""Category"" variable?</p>

<p>If not, then what is a better approach?</p>

<pre><code>model1 = lmer(ResponseTime ~ Category + Brightness + Contrast + (1|SubjectID), data=x)

null = lmer(ResponseTime ~ Brightness + Contrast + (1|SubjectID), data=x)

anova(model1,null)
</code></pre>

<p>(I am particullary concerned with the error term here as it appears to take much less into account than the former approach).</p>
"
"0.106119089994502","0.106358907452879","211642","<p>I have a question according the following example:</p>

<p>What I want to find out is whether two fertilizers (A and B) have different effects on the biomass of my plants. My explanatory variable is 'fertilizer' (categorial) with the levels 'A' and 'B'. My response variable is 'biomass' (continuous). Besides that, I have two other continuous variables 'seed mass' of the plants (measured before they germinated) and 'growth duration' (=harvesting date minus germination date). These two variables are expected explain some variance in my data and therefore I want to include them as covariates. (For each of the two levels I have 30 plants without any missing values.)</p>

<p>I read that my factor is a fixed effect and the covariates are random effects, as they represent a random sample out of the natural population. So I would do an ANCOVA in R using the lmer function (package lmerTest) like this:</p>

<pre><code>model &lt;- lmer(biomass~fertilizer+(1|seed_mass)+(1|growth_duration), data=dataset)
anova(model)
</code></pre>

<p>My question - are my considerations and the way I'm performing the analysis correct? Or is this analysis not appropriate for my question? My special concern is about the covariates, if I should include them in a different way in the model.</p>
"
"0.232495277487639","0.21360230027967","212199","<p>I would really appreciate some help with a few models I am trying to run. Essentially my data looks at how often a subject was visited depending on treatment and subject type across two years. The data looks like this:</p>

<pre><code>Year    | Subject | SubjectType | Treatment | BlockNum | NumberOfVisits | DurationOfVisits
--------+---------+-------------+-----------+----------+----------------+---------------+
1       | 1       | Type1       | Treatment1| 1        | 14             | 15.6
2       | 1       | Type1       | Treatment1| 1        | 0              | 0
1       | 2       | Type2       | Treatment2| 2        | 3              | 4.3
2       | 2       | Type2       | Treatment2| 2        | 0              | 0
</code></pre>

<p>and so on for 200 subjects with a measurement for each year.</p>

<p>Essentially I want to create a model that tests if the number of visits / duration of visits are different between treatment and subject across both years, and if there are any interactions. BlockNum refers to the experimental design being split into three randomised blocks (three blocks of plants growing in a greenhouse). I have tried a bunch of different models and cant seem to get a good resolution:</p>

<p>Repeated Measures ANOVA:</p>

<pre><code>model1 &lt;- aov(NumberOfVisits ~ SubjectType*Treatment*Year*BlockNum + Error(Subject/Year), data=dframe1)
</code></pre>

<p>However, the issue with this is that the data is left skewed with zeros in it (so log wont work), and I cannot successfully transform with any of the following:</p>

<pre><code>trans_Y &lt;- (dframe1$NumberOfVisits)^3
trans_Y &lt;- (dframe1$NumberOfVisits)^(1/9) 
trans_Y &lt;- log(dframe1$NumberOfVisits) 
trans_Y &lt;- log(dframe1$NumberOfVisits+0.1)
trans_Y &lt;- log(dframe1$NumberOfVisits+0.000001)
trans_Y &lt;- log10(dframe1$NumberOfVisits) 
trans_Y &lt;- exp(dframe1$NumberOfVisits) 
trans_Y &lt;- abs(dframe1$NumberOfVisits) 
trans_Y &lt;- sin(dframe1$NumberOfVisits) 
trans_Y &lt;- asin(dframe1$NumberOfVisits) 
</code></pre>

<p>As such I then tried a Generalised GLMM:</p>

<pre><code>library(lme4)

model1 &lt;- glmer(NumberOfVisits ~ SubjectType*Treatment*BlockNum + (1|Year), family = gaussian (link = inverse), data = dframe1)    
</code></pre>

<p>However this returns:</p>

<pre><code>    Warning message:
In glmer(NumberOfVisits ~ SubjectType*Treatment*BlockNum + (1 | Year),  :
  calling glmer() with family=gaussian (identity link) as a shortcut to lmer() is deprecated; please call lmer() directly
</code></pre>

<p>And so trying lmer:</p>

<pre><code>model1 &lt;- lmer(NumberOfVisits ~ SubjectType*Treatment*BlockNum + (1|Year), family = gaussian (link = identity), data = dframe1)    
Warning in lme4::lmer(formula = NumberOfVisits ~ SubjectType * Treatment * BlockNum +  :
  passing control as list is deprecated: please use lmerControl() instead
Error in (function (optimizer = ""bobyqa"", restart_edge = TRUE, boundary.tol = 1e-05,  : 
  unused arguments (tolPwrss = 1e-07, compDev = TRUE, nAGQ0initStep = TRUE, checkControl = list(check.nobs.vs.rankZ = ""ignore"", check.nobs.vs.nlev = ""stop"", check.nlev.gtreq.5 = ""ignore"", check.nlev.gtr.1 = ""stop"", check.nobs.vs.nRE = ""stop"", check.rankX = ""message+drop.cols"", check.scaleX = ""warning"", check.formula.LHS = ""stop"", check.response.not.const = ""stop""), checkConv = list(check.conv.grad = list(action = ""warning"", tol = 0.001, relTol = NULL), check.conv.singular = list(action = ""ignore"", tol = 1e-04), 
    check.conv.hess = list(action = ""warning"", tol = 1e-06)))
In addition: Warning messages:
1: In lmer(NumberOfVisits ~ SubjectType * Treatment * BlockNum + (1 | Year),  :
  calling lmer with 'family' is deprecated; please use glmer() instead
2: In lme4::glmer(formula = NumberOfVisits ~ SubjectType * Treatment * BlockNum +  :
  calling glmer() with family=gaussian (identity link) as a shortcut to lmer() is deprecated; please call lmer() directly
</code></pre>

<p>And when I a log/inverse link function (I presume this wont work because of the zeros?):</p>

<pre><code>model1 &lt;- glmer(NumberOfVisits ~ SubjectType*Treatment*BlockNum + (1|Year), family = gaussian (link = log), data = dframe1)    # random intercept
Error in eval(expr, envir, enclos) : 
  cannot find valid starting values: please specify some
</code></pre>

<p>The following will work for 'Number of visits', but not 'Duration of visits' (as it is non-integer values)</p>

<pre><code>model1 &lt;- glmer(DurationOfVisits ~ SubjectType*Treatment*BlockNum + (1|Year), family = poisson, data = dframe1)
</code></pre>

<p>However this returns the following, which doesn't tell me the significance of 'Treatment' itself, but rather the significance of each subset within treatment:</p>

<pre><code>summary(model1): 

 Call:
lm(formula = DurationOfVisits ~ SubjectType*Treatment*BlockNum, data = dframe1)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3251 -3.9093 -0.5325  2.1748 18.9394 

Coefficients:
                                          Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)                                 3.9967     1.6053   2.490   0.0138 *
Treatment2                                  3.1750     2.1018   1.511   0.1328  
Treatment3                                  0.1306     2.1018   0.062   0.9505  
Treatment4                                 -0.7279     2.1018  -0.346   0.7295  
</code></pre>

<p>...    </p>

<p>I would really appreciate some help with this. I feel like I'm missing something obvious here, it has been quite a number of long days deep in R and my brain is a bit frazzled.</p>

<p>I'm relatively new to R, so explanations in relatively simple terms would be appreciated!</p>

<p>Thanks a lot ahead of time!</p>
"
"0.183803655523452","0.184219031545903","212397","<p>I would like to test the effect of a treatment (""crop"") on species richness. I would rather use a glm for richness as it is a kind of count data.</p>

<p>Besides, I have a nested sampling design (5 values per plot, 5 plot per treatment). Thus I should use a GLMM.</p>

<p>So I write my model :</p>

<pre><code>&gt; GLMM_ric = glmer(richness ~ Crop + (1| Plot),  family=poisson)
&gt; summary(GLMM_ric)

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
 glmerMod]
 Family: poisson ( log )
 Formula: richness ~ Crop + (1 | Plot)
 Data: Com_agg

 AIC      BIC   logLik deviance df.resid 
433.8    446.9   -211.9    423.8       95 

Scaled residuals: 
   Min       1Q   Median       3Q      Max 
-1.33174 -0.41445 -0.08382  0.39853  1.73324 

Random effects:
 Groups Name        Variance Std.Dev.
  Plot   (Intercept) 0.08432  0.2904  
 Number of obs: 100, groups: Plot, 20

 Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)   1.9621     0.1503  13.056   &lt;2e-16 ***
 CropM        -0.5351     0.2211  -2.420   0.0155 *  
 CropYR       -0.3814     0.2181  -1.748   0.0804 .  
 CropOR       -0.3393     0.2175  -1.560   0.1188    
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Correlation of Fixed Effects:
        (Intr) CropM  CropYR
 CropM  -0.678              
 CropYR -0.686  0.467       
 CropOR -0.687  0.468  0.475
</code></pre>

<p>and then a simpler model to compare with :</p>

<pre><code> &gt; GLMM_ric0 = glmer(richness ~ (1| Plot), data=Com_agg, family=poisson,    glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))

 &gt;summary(GLMM_ric0)

 Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod]
  Family: poisson ( log )
 Formula: richness ~ (1 | Plot)
    Data: Com_agg
 Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))

 AIC      BIC   logLik deviance df.resid 
 433.3    438.5   -214.7    429.3       98 

 Scaled residuals: 
 Min       1Q   Median       3Q      Max 
 -1.27211 -0.39830 -0.03309  0.38204  1.66734 

 Random effects:
  Groups Name        Variance Std.Dev.
  Plot   (Intercept) 0.1251   0.3537  
 Number of obs: 100, groups: Plot, 20

 Fixed effects:
        Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)  1.64739    0.09114   18.07   &lt;2e-16 ***
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And then I compare both models :</p>

<pre><code>&gt; anova(GLMM_ric0, GLMM_ric)
Data: Com_agg
Models:
GLMM_ric0: richness ~ (1 | Plot)
GLMM_ric: richness ~ Crop + (1 | Plot)
              Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
GLMM_ric0  2 433.32 438.53 -214.66   429.32                         
GLMM_ric   5 433.84 446.86 -211.92   423.84 5.4851      3     0.1395
</code></pre>

<p>So according to my anova, the factor ""crop"" is not significant. Yet in the summary of my model some of the modalities appear to be significant. How should I interpret this ?</p>

<p>I have looked around for a while (e.g. <a href=""http://stats.stackexchange.com/questions/9587/glmm-test-of-significance"">here</a> or <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">here</a>) but I could not find much for this precise situation.</p>
"
"0.107624400500126","0.125845556426908","213187","<p>I'm very new to R and stats in general but I think I need to do a repeated measures analysis perhaps using a linear mixed model on my data.</p>

<p>Data are behavioural measurements e.g. distance swum for 21 fish of 2 different genotypes. Each fish was tested for 10 minutes, and data taken for each minute. I've made graphs to show this (B and C).</p>

<p><img src=""http://i.stack.imgur.com/8oKg9.png"" alt=""""></p>

<p>My issue is that I can't do just a normal regression as each of the points is linked over time by the same fish. I've been taught to use linear mixed models such as:</p>

<pre><code>model &lt;- lmer(swimdurbot~1+start+(1+start|file), data=tdautodisc)
model2 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
model3 &lt;- lmer(swimdurbot~Genotype+(1+start|file), data=tdautodisc)
model4 &lt;- lmer(swimdurbot~Genotype+start+ Genotype*start +(1+start|file), data=tdautodisc)
model5 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
</code></pre>

<p>and to then perform anovas on them but I honestly can't work out what each model is showing, how they are different from each other and which part of the analysis that R comes up with tells me what I want to know.</p>

<p>I want to find out from my data: Does fish behaviour change over 10 mintutes (gradient)? Is the behaviour different between genotypes? and Does genotype affect the change in behaviour over time?</p>

<p>I would really appreciate some help on this as I can't get my head around it, and the graphs don't show any obvious trend so aren't useful for me to guess which part of the analysis relates to which aspect of the graphs.</p>
"
"0.183803655523452","0.171937762776176","217511","<p>I want to compare several models built using the codes I have written in R for a mixed-effects model. I already knew that <code>anova()</code> function in car package provides <code>AIC</code>, which is a factor that we can use to compare models in a mixed-effects modelling analysis. However, I realised that <code>model.sel</code> function in <code>MuMIn</code> package seems to do the same thing and I need help as I am confused and the R help did not quite help. How are these functions different? Which one should I use? (I had 100 participants, 50 from each language group, from which each 25 participants in a language group received a different either list A or list B of the items.)</p>

<p>[packages <code>lme4</code> and <code>lmertest</code> were used to build the models]
Following are the codes I used to build the models:</p>

<pre><code>m1.1.1&lt;-lmer (RT~ Language*Col + (1+Col|Subject) + (1+Language|Item), data=RQ1.lmm.data.1) 
m1.1.2&lt;-lmer(RT~ Language*Col + (1|Subject) + (1+Language|Item), data=RQ1.lmm.data.1) 
m1.1.3&lt;-lmer(RT~ Language*Col + (1+Col|Subject) + (1|Item), data=RQ1.lmm.data.1)   
m1.1.4&lt;-lmer(RT~ Language*Col + (1|Subject) + (1|Item), data=RQ1.lmm.data.1) 
</code></pre>

<p>This is the function I used for comparing the models in order to find the most optimal model:</p>

<pre><code>anova(m1.1.1, m1.1.2, m1.1.3, m1.1.4)
</code></pre>

<p>[as this function shows the AIC for each model]</p>

<p>Resulting table:</p>

<pre><code>Data: RQ1.lmm.data.1 Models:

..3: RT ~ Language*Col + (1|Subject) + (1|Item)
..1: RT ~ Language*Col + (1|Subject) + (1+Language|Item)
..2: RT ~ Language*Col + (1+Col|Subject) + (1|Item)
object: RT ~ Language * Col + (1 + Col|Subject)+(1 + Language | Item)

   Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
..3     7 15912 15947 -7949.2    15898                             
..1     9 15874 15919 -7928.1    15856 42.322      2  6.456e-10 ***
..2     9 15902 15947 -7941.9    15884  0.000      0          1    
object 11 15865 15920 -7921.6    15843 40.634      2  1.501e-09 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>As I have also seen the following function to be used for what seems like the same type of comparison, I used it and the following table was the result</p>

<pre><code>model.sel(m1.1.1, m1.1.2, m1.1.3, m1.1.4)

Model selection table 

       (Int) Cll Lng Cll:Lng      random df    logLik    AICc delta weight

m1.1.1 820.3   +   +       + 1+C|S+1+L|I 11 -7901.947 15826.1  0.00 0.988
m1.1.2 819.2   +   +       +     S+1+L|I  9 -7908.437 15835.0  8.90 0.012
m1.1.3 851.4   +   +       +     1+C|S+I  9 -7922.365 15862.9 36.76 0.000
m1.1.4 850.5   +   +       +         S+I  7 -7929.688 15873.5 47.34 0.000
Models ranked by AICc(x) 
Random terms: 
1+C|S = â€˜1 + Col|Subjectâ€™
1+L|I = â€˜1 + Language|Itemâ€™
S = â€˜1|Subjectâ€™
I = â€˜1|Itemâ€™
</code></pre>

<p>I'm guessing the difference between the two is only the <code>AIC</code> being the corrected version in the <code>model.sel</code> function (which seems to be more appropriate for small sample sizes, but not exactly sure of it.
And am I correct to interpret that, based on the results, my core model (model m1.1.1) is the most optimal model in this case?</p>

<p>(any comments on the interpretation of the model would be much appreciated as I am  quite new to this type of analysis technique.)</p>
"
"0.0949157995752499","0.0951302988308988","219674","<p>I am fitting a mixed model with the command </p>

<pre><code>model=lmer(Activity ~ 1 + Novelty*Valence*ROI + (1 | Subject))
</code></pre>

<p>Activity is a measure of brain activity, Novelty and Valence are categorical variables coding the type of stimulus used to elicit the response and ROI is a categorical variable coding three regions of the brain that we have sampled this activity from. Subject is an ID number for the individuals the data was sampled from (n=94).</p>

<p>In attempting to get a single estimate for ROI (since it has 3 levels), I ran</p>

<pre><code>model_test=lmer(Activity ~ 1 + Novelty*Valence*ROI - ROI + (1 | Subject))
anova(model,model_test)
</code></pre>

<p>The output of my anova command suggests these models are equivalent</p>

<pre><code>object: Activity ~ 1 + Novelty * Valence * ROI + (1 | Subject)
..1: Activity ~ 1 + Novelty * Valence * ROI - ROI + (1 | Subject)
       Df     AIC     BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
object 14 -721.88 -641.78 374.94  -749.88                        
..1    14 -721.88 -641.78 374.94  -749.88     0      0          1
</code></pre>

<p>Playing around with different models, this seems to happen any time I eliminate an effect or interaction that is qualified by a higher-order interaction. What is the correct way to get an estimate for this categorical variable?</p>
"
"0.164398987305357","0.164770510914327","221721","<p>I have a dataset containing one dependent variable which is the concentration of antibiotic needed to kill a bacteria, which was measured for several different antibiotics for three different microorganisms. The antibiotics are also divided in two groups based on their origin (synthetic or natural).</p>

<p>The data can be described as follows:</p>

<pre><code> $ ID: Factor w/ 3977 levels ""1"",""2"",""3"",""4"",..: 4 5 9 10 11 12 13 14 15 16 ...
 $ OR: Factor w/ 2 levels ""natural"", ""synthetic"": 2 2 2 2 2 2 2 2 1 2 ...
 $ MC: Factor w/ 3 levels ""M1"",""M2"",""M3"": 1 1 1 1 1 1 1 1 1 1 ...
 $ Y : num  1.745 0.125 2.301 -1.615 -2.026 ... 
</code></pre>

<p>Additionally, as you can see the dataset is quite unbalanced and as a lot of missing values.</p>

<pre><code>                    MR
OR          M1      M2        M3
natural   1267    1032       400
synthetic 2129    2044       944
</code></pre>

<p>I have specified a couple of formulas for the lmer() model.</p>

<pre><code>(a) Y ~ OR * MC + (1|ID) 
(b) Y ~ OR + MC + (1|ID)
(c) Y ~ OR + MC + (OR+MC|ID)
</code></pre>

<p>For model (a), Anova with type 3 error showed that OR:MC is not significant.</p>

<p>Model (b), shows a slope on the residuals, so i tried model c.</p>

<p>Model (c) does not run in R (Error: number of observations (=7816) &lt;= number of random effects (=27839)) so i turned to matlab (also runs on julia), and also shows the residuals to have a slope.</p>

<p>The slope in the residuals can be attributed, from what i understand, to several issues, poorly specified random effects or autocorrelation.
The fact is that there might be autocorrelation as some antibiotics differ from other antibiotics in just a few atoms.</p>

<p>Any idea on how to properly specify the model?</p>

<hr>

<p>Edit:</p>

<p>y=residuals;  x=fitted
<a href=""http://i.stack.imgur.com/kQeLh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kQeLh.png"" alt=""residuals vs fitted""></a></p>

<p>y=residuals;  x=observed
<a href=""http://i.stack.imgur.com/1IkYG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1IkYG.png"" alt=""residuals vs observed""></a></p>

<p>Model d (with a random slope and intercept for all levels of OR:MC)</p>

<pre><code>(b) Y ~ OR * MC + (OR:MC|ID)
</code></pre>

<p>I believe both model b, c and now d are well specified, model b as a logLik of -7981, c of -7944 and d of -7933. Suggesting d is the better.</p>
"
"0.1898315991505","0.190260597661798","223439","<p>I am running an analysis on a national sample of 20,000, representative at the province level (34 provinces)</p>

<p>After checking for linearity and normality of my dependent variable I have run a preliminary OLS in order to see how the covariates perform in explaining the variation of the variable of interest. 
I have selected the relevant independent variables following accreditee literature in my field of analysis, explored the covariance matrix in order to avoid problems of multicollinearity etc..</p>

<p>The result from the OLS is good in term of significance level of the coefficients, the sign and the magnitute of the latter fullfil my expectations and match the results find by other analysis.
However, the value of <code>R^2</code> is quite low: only <code>0.09</code> . Thus, knowing that some variation could be explained by the differences between provinces I have first estimate the OLS adding first provincial dummies and after distric dummies (398 districts).</p>

<p>The <code>R^2</code> improved much, reaching respectively the <code>36%</code> and <code>41%</code>.
However, what I would like to see are the underling cause of the regional differences: why do they perform differently? </p>

<p>Among the variables I have some take a unique value according to each observation's province. I cannot use them in the OLS while using the province dummies because there would be perfect collinearity.</p>

<p>In my view using a mixed linear model would help.</p>

<p>I have run a random intercept null.model in which only the dependent variable is regressed against an intercept. For the estimation I have used the command <code>lmer</code> from the <code>{lme4}</code>package in <code>R</code>.</p>

<p>The InterCorrelation Coeffient equals <code>0.30</code>, suggesting that the 30% of the variation happens between groups, the values of the group-mean reliance are quite high too (not less than <code>0.9</code>)- I repeat myself: the sample is province representative.</p>

<p>I finally run a set of random mixed intercept model with 2 levels:</p>

<p>where: <code>i</code> indicates the household and <code>j</code> indicates the province.</p>

<ol>
<li><code>Y_i = beta0j + beta1 X_i + e_ij</code> </li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 Z_j + e_ij</code></li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 W_j + e_ij</code></li>
</ol>

<p>The <strong>first question</strong> is: am I allowed to include a variable which varies only at the provincial level (as Z and W do) given that their coefficient is not computed by taking in cosideration the groups?
As far as I have understood it would be a mistake to use those variables in the random part of the model in order to get random coefficients.</p>

<p>The <strong>second question</strong>: given that by running the command <code>anova()</code>, also in <code>lme4</code>, model 1 is statistically different from model 2, and model 2 is  statistically not different from model 3, can I say that Z and W have the same power in explaing the variation in Y despite the fact that Z's coefficient is significant and W's is not significant?</p>

<p>Think as if Z and W were proxies for the same dimension. They are in fact statistically and conceptually high correlated.</p>

<p>Sorry but I cannot give more details on the actual problem I am woking on.</p>

<p>Thanks in advance. </p>
"
"0.259408755940805","0.28970870444117","223626","<p>In R, I'm wondering how the functions <code>anova()</code> (<code>stats</code> package) and <code>Anova()</code> (<code>car</code> package) differ when being used to compare nested models fit using the <code>glmer()</code> (generalized linear mixed effects model; <code>lme4</code> package) and <code>glm.nb</code> (negative binomial; <code>MASS</code> package) functions. </p>

<p>I've found the two ANOVA functions do not produce the same results for tests of fixed effects in a Poisson mixed model, or a negative binomial fixed effects model (no random effects). Results from both are shown below.</p>

<p><em>My goal</em>: Correctly test the overall significance of a multi-level categorical predictor (fixed; <em>Species</em>). I'm looking for a type III SS-type <em>p</em>-value.</p>

<hr>

<p><em>First</em>: If one fits a <strong>fixed effects</strong> generalized linear model (Poisson here) using <code>glm()</code>, then these two functions <strong>do produce the same results</strong> given the arguments as in the following dummy example:</p>

<pre><code>mod01 &lt;- glm(Count ~ Species + offset(log(Area)), data=data01, family=poisson)

####################
# Anova() function #
####################

library(car)
Anova(mod01, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   255.44  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod01x &lt;- update(mod01, . ~ . - Species)
anova(mod01x, mod01, test=""Chisq"")

# Model 1: Count ~ offset(log(Area))
# Model 2: Count ~ Species + offset(log(Area))

#   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
# 1      1063     1456.4                          
# 2      1055     1201.0  8   255.44 &lt; 2.2e-16 ***

# Test statistics are the SAME (255.44) for the fixed effects model
</code></pre>

<hr>

<p><em>However</em>: For a generalized linear <strong>mixed effects</strong> model (using <code>glmer()</code> with random effect for <em>Group</em>), analogous code <strong>gives a different test statistic across the two functions</strong>:</p>

<pre><code>library(lme4)
mod02 &lt;- glmer(Count ~ 1 + Species + (1 | Group) + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod02, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod02x &lt;- update(mod02, . ~ . - Species)
anova(mod02x, mod02, test=""Chisq"")

# mod02x: Count ~ (1 | Group) + offset(log(Area))
# mod02: Count ~ 1 + Species + (1 | Group) + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod02x  2 1423.9 1433.8 -709.95   1419.9                             
# mod02  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Now the test statistics are DIFFERENT (197.9012 vs. 248.21)

#####################################################################

# Not a matter of type I vs. III SS since whether the fixed or random
# effect is fit first in the model does not affect results:

# List random effect (Group) before fixed (Species):

mod03 &lt;- glmer(Count ~ 1 + (1 | Group) + Species + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod03, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod03x &lt;- update(mod03, . ~ . - Species)
anova(mod03x, mod03, test=""Chisq"")

# mod03x: Count ~ (1 | Group) + offset(log(Area))
# mod03: Count ~ 1 + (1 | Group) + Species + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod03x  2 1423.9 1433.8 -709.95   1419.9                             
# mod03  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Respective test statistics are the same as above case where order of fixed
# and random effects was reversed
</code></pre>

<hr>

<p>Another example of inconsistent test statistics: <strong>Fixed effects negative binomial model</strong>:</p>

<pre><code>library(MASS)
mod04 &lt;- glm.nb(Count ~ Species + offset(log(Area)), data=data01)

####################
# Anova() function #
####################

Anova(mod04, type=3)

# Analysis of Deviance Table (Type III tests)

# Response: Spiders_Tree
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   101.08  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod04x &lt;- update(mod04, . ~ . - Species)
anova(mod04x, mod04)

# Likelihood ratio tests of Negative Binomial Models

# Response: Count
#                            Model     theta Resid. df  2 x log-lik.   Test df LR stat.       Pr(Chi)
# 1           offset(log(Area_M2)) 0.2164382      1063     -1500.688                      
# 2 Species + offset(log(Area_M2)) 0.3488095      1055     -1413.651 1 vs 2  8 87.03677  1.887379e-15 

# Test statistics are also DIFFERENT here (101.08 vs. 87.03677)
</code></pre>

<hr>

<p><em>In summary</em>: The problem:</p>

<ol>
<li>Isn't restricted to only mixed or only fixed effects models</li>
<li>Isn't a matter of type I or III SS, since an example with only one predictor (negative binomial fixed effects model) showed the same problem, and even in the case of more than one predictor (mixed model example), the test is only for the removal of one predictor (<em>Species</em>), so I believe the two types of SS should be equivalent in this case.</li>
</ol>

<p>Could it have to do with the offset? Maybe the functions were written to ""behave well"" with the <code>glm()</code> function, but process others (such as <code>glmer()</code> and <code>glm.nb()</code>) inconsistently? Something else I'm not thinking of?</p>

<hr>

<p>I'm not providing data for my example code above, as I'm assuming someone can comment on the differing theories of each function without a minimal working example. However, if you would like to verify the results really do differ (as shown above), I will add a dummy dataset.</p>
"
"0.213120671030335","0.21360230027967","225241","<p>Consider a mixed model as follows.</p>

<pre><code>library(lme4)
# Load data
data &lt;- structure(list(blk = c(1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3L),
                       gent = c(1, 2, 3, 4, 7, 11, 12, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 8, 6, 10L),
                       yld = c(83, 77, 78, 78, 70, 75, 74, 79, 81, 81, 91, 79, 78, 92, 79, 87, 81, 96, 89, 82L),
                       syld = c(250, 240, 268, 287, 226, 395, 450, 260, 220, 237, 227, 281, 311, 258, 224, 238, 278, 347, 300, 289L)),
                  .Names = c(""blk"", ""gent"", ""yld"", ""syld""), class = ""data.frame"", row.names = c(NA, -20L))
data$blk &lt;- as.factor(data$blk)
data$gent &lt;- as.factor(data$gent)
</code></pre>

<p>The data is unbalanced.</p>

<pre><code># Mixed effect model
frmla &lt;- ""syld ~ 1 + gent + (1|blk)""
library(lme4)
model &lt;- lmer(formula(frmla), data = data)

model
Linear mixed model fit by REML ['merModLmerTest']
Formula: syld ~ 1 + gent + (1 | blk)
   Data: data
REML criterion at convergence: 73.9572
Random effects:
 Groups   Name        Std.Dev.
 blk      (Intercept)  9.385  
 Residual             16.919  
Number of obs: 20, groups:  blk, 3
Fixed Effects:
(Intercept)        gent2        gent3        gent4        gent5        gent6        gent7        gent8        gent9  
    256.000      -28.000       -8.333        8.000       32.127       43.678      -36.805       90.678       62.127  
     gent10       gent11       gent12  
     32.678      132.195      187.195  
</code></pre>

<p>Primarily I want to compare the <code>gent</code> levels by LS means.</p>

<pre><code>library(""lmerTest"")
lsmeans(model)
Least Squares Means table:
         gent Estimate Standard Error   DF t-value Lower CI Upper CI p-value    
gent  1   1.0    256.0           11.2  6.9    22.9      229      283  &lt;2e-16 ***
gent  2   5.0    228.0           11.2  6.9    20.4      201      255  &lt;2e-16 ***
gent  3   6.0    247.7           11.2  6.9    22.2      221      274  &lt;2e-16 ***
gent  4   7.0    264.0           11.2  6.9    23.6      237      291  &lt;2e-16 ***
gent  5   8.0    288.1           18.5  8.0    15.6      245      331  &lt;2e-16 ***
gent  6   9.0    299.7           18.5  8.0    16.2      257      342  &lt;2e-16 ***
gent  7  10.0    219.2           18.5  8.0    11.8      177      262  &lt;2e-16 ***
gent  8  11.0    346.7           18.5  8.0    18.8      304      389  &lt;2e-16 ***
gent  9  12.0    318.1           18.5  8.0    17.2      275      361  &lt;2e-16 ***
gent  10  2.0    288.7           18.5  8.0    15.6      246      331  &lt;2e-16 ***
gent  11  3.0    388.2           18.5  8.0    21.0      346      431  &lt;2e-16 ***
gent  12  4.0    443.2           18.5  8.0    24.0      401      486  &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In addition I am interested in variance partitioning.</p>

<p>The variance component due to random effect and residual can be estimated as follows.</p>

<pre><code>VCrandom &lt;- VarCorr(model)
print(VCrandom, comp = ""Variance"")
 Groups   Name        Variance
 blk      (Intercept)  88.083 
 Residual             286.250
</code></pre>

<p>How to partition the total variance into components due to each of the factors <code>gent</code> and <code>blk</code> along with the residual ? Something similar to the output given by <code>PROC MIXED</code> of <code>SAS</code>, where MSE is computed even when estimation is by ML or REML instead of least squares.</p>

<p>Should I treat the fixed effect as random just for the purpouse of getting variance component ?</p>

<pre><code>frmla2 &lt;- ""syld ~ 1 + (1|gent) + (1|blk)""
model2 &lt;- lmer(formula(frmla2), data = data)
model2

VCrandom2 &lt;- VarCorr(model2)
print(VCrandom2, comp = ""Variance"")
 Groups   Name        Variance
 gent     (Intercept) 4152.08 
 blk      (Intercept)  116.11 
 Residual              274.92 
</code></pre>

<p>If there is no random effect, variance components can be estimated using the least squares approach (ANOVA, Sum of squares, MSE).</p>

<p>The package <code>mixlm</code> has provision for variance partitioning using SS in case of mixed models.</p>

<pre><code>library(mixlm)

mixlm &lt;- lm(syld ~ 1 + r(gent) + r(blk), data)

Anova(mixlm, type=""III"")

Analysis of variance (unrestricted model)
Response: syld
          Mean Sq   Sum Sq Df F value Pr(&gt;F)
gent      5360.49 58965.36 11   18.73 0.0009
blk        638.58  1277.17  2    2.23 0.1886
Residuals  286.25  1717.50  6       -      -

            Err.term(s) Err.df VC(SS)
1 gent              (3)      6 3044.5
2 blk               (3)      6   52.8
3 Residuals           -      -  286.3
(VC = variance component)

               Expected mean squares
gent      (3) + 1.66666666666667 (1)
blk       (3) + 6.66666666666667 (2)
Residuals (3)                       

WARNING: Unbalanced data may lead to poor estimates
</code></pre>

<p>The estimates are different</p>

<pre><code># Total variance
var(data$syld)

|source   |  model1|  model2|  mixlm|
|:--------|-------:|-------:|------:|
|gent     |      NA| 4152.08| 3044.5|
|blk      |  88.083|  116.11|   52.8|
|Residual | 286.250|  274.92|  286.3|
</code></pre>

<p>Can fixed effect variance be extracted using <code>predict</code> function as suggested here <a href=""https://sites.google.com/site/alexandrecourtiol/what-did-i-learn-today/inrhowtoextractthedifferentcomponentsofvarianceinalinearmixedmodel"" rel=""nofollow"">In R: How to extract the different components of variance in a linear mixed model!</a> ?</p>

<pre><code>var(predict(model))
</code></pre>

<p>Which is the most appropriate method compatible with <code>(RE)ML</code> estimates in lme4 ?</p>
"
"0.116247638743819","0.116510345607093","225904","<p>I collected the following variables, as I thought they might be in some relationship, but without any strict hypothesis. Note: this is a repeated measures design.</p>

<ul>
<li><strong>phy</strong> = continuous DV (a physiological measure)</li>
<li><strong>lag</strong> = interval, quadratic, IV (a setting of the experiment, ranges from -5 to +5 seconds)</li>
<li><strong>group</strong> = factor, 2 level, IV (two different conditions, say males VS females)</li>
<li><strong>quest1</strong> = continuous covariate (a questionnaire related to the measured stuff)</li>
<li><strong>quest2</strong> = continuous covariate (another scale of the same questionnaire)</li>
<li><strong>id</strong> = factor subject id</li>
</ul>

<p>I collected 15 subjects, each measured for 11 lags, for a total of 165 data points.
<strong>My goal is to decide wether there is a credible difference between the two groups in the phy response,</strong> controlling for all the other variables, and to describe such difference.</p>

<p>So my logic was to build a full model:</p>

<pre><code>MODEL1 &lt;- lmer(phy ~ lag * I(lag^2) * group * quest1 * quest2 + (1|id))
</code></pre>

<p>and then to stepwise remove the interactions and to compare the models with likelihood ratio test, AIC and BIC, trough the anova() function.</p>

<p>My results are:</p>

<pre><code>        Df    AIC    BIC  logLik deviance   Chisq Chi Df Pr(&gt;Chisq)    
MODEL8  10 385.70 416.76 -182.85   365.70                              
MODEL4  13 390.16 430.54 -182.08   364.16  1.5380      3  0.6735395    
MODEL5  13 420.45 460.82 -197.22   394.45  0.0000      0  1.0000000    
MODEL6  18 365.84 421.74 -164.92   329.84 64.6106      5   1.35e-12 ***
MODEL7  18 397.84 453.75 -180.92   361.84  0.0000      0  1.0000000    
MODEL9  18 390.63 446.53 -177.31   354.63  7.2144      0  &lt; 2.2e-16 ***
MODEL10 18 465.24 521.14 -214.62   429.24  0.0000      0  1.0000000    
MODEL11 18 413.73 469.64 -188.87   377.73 51.5046      0  &lt; 2.2e-16 ***
MODEL2  19 408.63 467.64 -185.31   370.63  7.1007      1  0.0077053 ** 
MODEL3  19 367.78 426.79 -164.89   329.78 40.8490      0  &lt; 2.2e-16 ***
MODEL1  34 355.60 461.20 -143.80   287.60 42.1812     15  0.0002108 ***
</code></pre>

<p>Where MODEL1 is the full model, and the higher numbers are increasingly simple models. So except BIC, which is obviously penalizing the model complexity, AIC and likelihood test are telling me to keep a 5-way interaction!?</p>

<p>My questions are:</p>

<ol>
<li>Is my logic right?</li>
<li>Should I believe in such a complex model?</li>
<li>Is it ok with my relatively few datapoints 15 subjects x 11 repetitions</li>
<li>How can I even begin to interpret or to plot the effects?</li>
</ol>

<p>thank you for any suggestion on how to proceed!</p>
"
"0.117452309662454","0.134534558799262","226266","<p>I am trying to fit some mixed models for unbalanced data as follows.</p>

<pre><code>library(easyanova)
data(data13)
</code></pre>

<h3>1) <code>genotypes</code> as random</h3>

<pre><code>frmla &lt;- ""yield ~ 1 + (1|blocks) + (1|genotypes)""

model1 &lt;- lmer(formula(frmla), data = data13)

# adjusted means - BLUPs for genotypes
newdata13 &lt;- expand.grid(genotypes = levels(data13$genotypes), blocks = levels(data13$blocks))
newdata13$pred &lt;- predict(model1, newdata=newdata13)
tapply(newdata13$pred, newdata13$genotypes, mean)
</code></pre>

<h3>1) <code>genotypes</code> as fixed</h3>

<pre><code>frmla &lt;- ""yield ~ 1 + (1|blocks) + genotypes""

model2 &lt;- lmer(formula(frmla), data = data13)

# adjusted means - BLUEs for genotypes
newdata13 &lt;- expand.grid(genotypes = levels(data13$genotypes), blocks = levels(data13$blocks))
newdata13$pred &lt;- predict(model2, newdata=newdata13)
tapply(newdata13$pred, newdata13$genotypes, mean)
</code></pre>

<p>For further calculations I need to compute <strong>mean variance of difference of adjusted means</strong> (BLUPs or BLUEs). A method is given (<a href=""https://static-content.springer.com/esm/art%3A10.1186%2F1471-2164-14-860/MediaObjects/12864_2013_5591_MOESM1_ESM.doc"" rel=""nofollow"">https://static-content.springer.com/esm/art%3A10.1186%2F1471-2164-14-860/MediaObjects/12864_2013_5591_MOESM1_ESM.doc</a>)
to compute it from <strong>variance-covariance matrix of adjusted means</strong>.</p>

<p>How to get the variance-covariance matrix of adjusted means for <code>model1</code> and <code>model2</code> ?</p>

<p>When <code>genotypes</code> are fixed in <code>model2</code> does <code>vcov(model2)</code> give the variance-covariance matrix of adjusted means ?</p>
"
"0.0671156055214024","0.0672672793996312","226884","<p>In many textbooks and R tutorials the repeated measures (within subjects) ANOVA seems to be very straightforward, following the formula</p>

<pre><code>aov.ww &lt;- aov(y ~ w1*w2 + Error(subject/(w1*w2)), data=data.long)
</code></pre>

<p>where <code>w1</code> and <code>w2</code> are two within-subjects variables and hence <code>subject</code> is included in the error term. However, this formula does not seem to treat the observations as dependent, evinced by the very large df. </p>

<p>I have used workarounds like first computing means across participants and then running the <code>aov</code> function on the reduced dataset. Also, many sources point to <code>lmer</code> as an alternative to <code>aov</code> for repeated measures design.</p>

<p>What would be the most robust and straightforward (in terms on interpretation or the results) method for analyzing data from repeated measures experiments?</p>
"
"0.134231211042805","0.117717738949355","226946","<p>Having trouble finding straightforward information this topic.</p>

<p>Basically, I'm trying to use the lme4 package to analyze my data, and the model looks something like (A ~ BCD) + (random effects term 1) + (random effects term 2).</p>

<p>'A' is a yes/no response, which, based on what I've read, indicates that I should use glmer. However, my experiment uses repeated measures - each subject undergoes many trials. It's a psychophysical experiment, so there are many subjects who essentially make yes/no judgements about many, many images. I've read that when there are many trials within a subject, you should use lmer.</p>

<p>What's the best way to go here? Sorry if the info given is too sparse; if anyone thinks they can help me out with this, I'll provide as much info as necessary.</p>

<p><strong>TL;DR: When exactly should one use lmer vs glmer, especially in the context of psychophysical experiments where one subject will undergo many trials with binomial outcomes?</strong></p>

<p>More info/part 2 of question: I initially analyzed my data using ANOVAs in SPSS. The SPSS indicated a highly significant interaction, one that is logical and predicted. When running the same data to modeled in glmer, that interaction in highly insignificant. When running through lmer, it is significant again.</p>

<p>If anyone can help shed some light on whether this makes sense or why it would be so, I'd appreciate it very much.</p>
"
"0.157400046933839","0.157755753708238","230911","<p>I'm not entirely sure of fitting the model for experiment we've made. The variables and relevant description are as follows:</p>

<ul>
<li>ID - participant ID </li>
<li>Trial - 60 for each participant</li>
<li>Memory - between subject binary factor</li>
<li>State - within subject binary factor  </li>
</ul>

<hr>

<ul>
<li>Correct - whether classification a participant made was correct or not</li>
<li>Rating - the judgement made after each trial on four point Likert scale</li>
</ul>

<p>Procedure brief: each participant (N=60) was randomly assigned to experimental or control group (Memory) and had 120 Trials (60 for State = 0 and 60 for State = 1). Each trial composed of perceptual classification (Correct) and judgment of how easy it was (Rating). The classification problem was randomly selected from two groups each trial (State).</p>

<p>I would like to calculate what impacts the performance (Correct) most - is it memory, state, a specific rating on a scale or any combination of above? I'm not interested in between subject variance, on the oposite, it is a random factor here. Also, it appears that there is bias in responses on Likert scales, so that part of variance should be excluded too. </p>

<p>The way I was thinking to approach this is generalized mixed linear model, but I'm not sure I'm doing it right; there is what I've got so far:</p>

<pre><code>model = glmer(Correct ~ (1|ID/Rating) + Memory * State * Rating, data, family=binomial, 
              control = glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun=100000)))
</code></pre>

<p>Is this approach correct? I'll appreciate your input.</p>

<p>Relevant resources I used: </p>

<ul>
<li><a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">Formulae in R: ANOVA and other models, mixed and fixed</a></li>
<li><a href=""http://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/"" rel=""nofollow"">The Difference Between Crossed and Nested Factors</a> </li>
<li><a href=""http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model"">When is it ok to remove the intercept in a linear regression model?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/225198/nested-random-factor-with-confounding-random-variable"">Nested random factor with confounding (random?) variable</a></li>
</ul>
"
"0.116247638743819","0.116510345607093","231377","<p>I am trying to carry out an lmerTest on two separate datasets, and for some reason I am getting the following error for one of the datasets.</p>

<blockquote>
  <p>In pf(F.stat, qr(Lc)$rank, nu.F) : NaNs produced</p>
</blockquote>

<p><a href=""https://drive.google.com/file/d/0B9jz9CiotnoER1dzUzRrVllVcm8/view?usp=sharing"" rel=""nofollow"">This dataset</a> gives me the p-value of the interaction term between <code>habitat</code> and <code>soil</code> without issue.</p>

<blockquote>
  <p>anova(lmer(sqrt(abs) ~ habitat*soil + (1|species), data=frl_light,
  REML=T))</p>
</blockquote>

<pre><code>Analysis of Variance Table of type III  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq  Mean Sq NumDF  DenDF F.value  Pr(&gt;F)  
habitat      0.057617 0.028809     2 8.8434  1.0880 0.37805  
soil         0.232708 0.232708     1 2.6732  8.7888 0.06848 .
habitat:soil 0.308003 0.154001     2 2.7134  5.8163 0.10443  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p><a href=""https://drive.google.com/file/d/0B9jz9CiotnoEdWkzbGhHM0RSVnM/view?usp=sharing"" rel=""nofollow"">This dataset</a> which has a similar structure however throws the error, and fails to give the p-value for the interaction between <code>habitat</code> and <code>light</code>. The density degree of freedom measurement is also 0, which is probably the problem.</p>

<blockquote>
  <p>anova(lmer(sqrt(abs) ~ habitat*light + (1|species), data=frl_soil,
  REML=T))</p>
</blockquote>

<pre><code>Analysis of Variance Table of type III  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq  Mean Sq NumDF  DenDF F.value Pr(&gt;F)
habitat       0.00845 0.004223     2 7.9751  0.3494 0.7154
light         0.01634 0.016336     1 1.9241  1.3517 0.3689
habitat:light 0.42813 0.214067     2 0.0000 17.7124       
Warning message:
In pf(F.stat, qr(Lc)$rank, nu.F) : NaNs produced
</code></pre>

<p>I have no idea why lmerTest works for one dataset but not the other as both datasets appear to me at least, to be virtually indistinguishable from one another. If there is anyone who can shed light on the matter, please help.</p>
"
"0.171550078488555","0.184219031545903","232109","<p>I have been looking through <a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">this overview of lm/lmer R formulas by @conjugateprior</a> and got confused by the following entry:</p>

<blockquote>
  <p>Now assume A is random, but B is fixed and B is nested within A.</p>

<pre><code>aov(Y ~ B + Error(A/B), data=d)
</code></pre>
</blockquote>

<p>Below analogous mixed model formula <code>lmer(Y ~ B + (1 | A:B), data=d)</code>  is provided for the same case.</p>

<p><strong>I do not quite understand what it means.</strong> In an experiment where subjects are divided into several groups, we would have a random factor (subjects) nested within a fixed factor (groups). But how can a fixed factor be nested within a random factor? Something fixed nested within random subjects? Is it even possible? If it is not possible, do these R formulas make sense?</p>

<hr>

<p>This overview is mentioned to be partially based on the <a href=""http://www.personality-project.org/r/r.guide.html#anova"" rel=""nofollow"">personality-project's pages on doing ANOVA in R</a> based itself on this <a href=""http://www.jason-french.com/tutorials/repeatedmeasures.html"" rel=""nofollow"">tutorial on repeated measures in R</a>. There the following example for the repeated measures ANOVA is given:</p>

<pre><code>aov(Recall ~ Valence + Error(Subject/Valence), data.ex3)
</code></pre>

<p>Here subjects are presented with words of varying valence (factor with three levels) and their recall time is measured. Each subject is presented with words of all three valence levels. I do not see anything nested in this design (it appears crossed, as per <a href=""http://stats.stackexchange.com/questions/228800"">the great answer here</a>), and so I would naively think that <code>Error(Subject)</code> or <code>(1 | Subject)</code> should be appropriate random term in this case. The <code>Subject/Valence</code> ""nesting"" (?) is confusing.</p>

<p>Note that I do understand that <code>Valence</code> is a <em>within-subject</em> factor. But I think it is <em>not</em> a ""nested"" factor within subjects (because all subjects experience all three levels of <code>Valence</code>).</p>

<hr>

<p><strong>Update.</strong> I am exploring questions on CV about coding repeated measures ANOVA in R.</p>

<ul>
<li><p><a href=""http://stats.stackexchange.com/questions/14088"">Here</a> the following is used for fixed within-subject/repeated-measures A and random <code>subject</code>: </p>

<pre><code>summary(aov(Y ~ A + Error(subject/A), data = d))
anova(lme(Y ~ A, random = ~1|subject, data = d))
</code></pre></li>
<li><p><a href=""http://stats.stackexchange.com/questions/13784"">Here</a> for two fixed within-subject/repeated-measures effects A and B:</p>

<pre><code>summary(aov(Y ~ A*B + Error(subject/(A*B)), data=d))
lmer(Y ~ A*B + (1|subject) + (1|A:subject) + (1|B:subject), data=d) 
</code></pre></li>
<li><p><a href=""http://stats.stackexchange.com/questions/117660"">Here</a> for three within-subject effects A, B, and C:</p>

<pre><code>summary(aov(Y ~ A*B*C + Error(subject/(A*B*C)), data=d))
lmer(Y ~ A*B*C + (1|subject) + (0+A|subject) + (0+B|subject) + (0+C|subject) + (0+A:B|subject) + (0+A:C|subject) + (0+B:C|subject), data = d)
</code></pre></li>
</ul>

<p>My questions: </p>

<ol>
<li>Why <code>Error(subject/A)</code> and not <code>Error(subject)</code>? </li>
<li>Is it <code>(1|subject)</code> or <code>(1|subject)+(1|A:subject)</code> or simply <code>(1|A:subject)</code>? </li>
<li>Is it <code>(1|subject) + (1|A:subject)</code> or <code>(1|subject) + (0+A|subject)</code>, and why not simply <code>(A|subject)</code>?</li>
</ol>

<p>By now I have seen some threads that claim that some of these things are equivalent (e.g., the first: <a href=""http://stats.stackexchange.com/questions/60108"">a claim that they are the same</a> but <a href=""http://stackoverflow.com/questions/37497948/"">an opposite claim on SO</a>; the third: kind of <a href=""http://stats.stackexchange.com/a/122662/2866"">a claim that they are the same</a>). Are they?</p>
"
"0.164398987305357","0.164770510914327","235018","<p>I am trying to use lmer function from lme4 package to estimate differences between two response curves from a control and treatment responses over time, leaving Subjects as random effect. Here the data:</p>

<pre><code>&gt; df
   Day Subject    Levels   Response
1   10    A001   Control 0.19672131
2   10    A002 Treatment 0.16830515
3   10    A003   Control 0.21355398
4   10    A004   Control 0.18644068
5   10    A005 Treatment 0.17231538
6   10    A007 Treatment 0.18448729
7   11    A001   Control 0.23774081
8   11    A002 Treatment 0.25000000
9   11    A003   Control 0.17288616
10  11    A004   Control 0.25843209
11  11    A005 Treatment 0.29505507
12  11    A007 Treatment 0.27315358
13  12    A001   Control 0.37851189
14  12    A002 Treatment 0.39753941
15  12    A003   Control 0.30925738
16  12    A004   Control 0.45247148
17  12    A005 Treatment 0.37485050
18  12    A007 Treatment 0.41668477
19  13    A001   Control 0.47589286
20  13    A002 Treatment 0.48965316
21  13    A003   Control 0.46696617
22  13    A004   Control 0.50611299
23  13    A005 Treatment 0.41968785
24  13    A007 Treatment 0.51708049
25  14    A001   Control 0.58793970
26  14    A002 Treatment 0.45247189
27  14    A003   Control 0.43121189
28  14    A004   Control 0.56663276
29  14    A005 Treatment 0.37929057
30  14    A007 Treatment 0.46441606
31  15    A001   Control 0.44310684
32  15    A002 Treatment 0.38066676
33  15    A003   Control 0.32576304
34  15    A004   Control 0.39422772
35  15    A005 Treatment 0.28628568
36  15    A007 Treatment 0.34023209
37  16    A001   Control 0.25967359
38  16    A002 Treatment 0.20789686
39  16    A003   Control 0.23629368
40  16    A004   Control 0.22833444
41  16    A005 Treatment 0.24163539
42  16    A007 Treatment 0.21100646
43  17    A001   Control 0.17009653
44  17    A002 Treatment 0.13781610
45  17    A003   Control 0.19149637
46  17    A004   Control 0.21317316
47  17    A005 Treatment 0.17746651
48  17    A007 Treatment 0.15096285
49  18    A001   Control 0.15408115
50  18    A002 Treatment 0.16038546
51  18    A003   Control 0.18361628
52  18    A004   Control 0.18867523
53  18    A005 Treatment 0.20131984
54  18    A007 Treatment 0.19504027
55  19    A001   Control 0.21285064
56  19    A002 Treatment 0.19435679
57  19    A003   Control 0.23979739
58  19    A004   Control 0.24010952
59  19    A005 Treatment 0.20209201
60  19    A007 Treatment 0.25806452
61  20    A001   Control 0.23613019
62  20    A002 Treatment 0.20014232
63  20    A003   Control 0.26122983
64  20    A004   Control 0.26375544
65  20    A005 Treatment 0.17656201
66  20    A007 Treatment 0.22391777
67  21    A001   Control 0.20523904
68  21    A002 Treatment 0.18967355
69  21    A003   Control 0.22878808
70  21    A004   Control 0.26186233
71  21    A005 Treatment 0.18644467
72  21    A007 Treatment 0.18347698
73  22    A001   Control 0.19849361
74  22    A002 Treatment 0.16430202
75  22    A003   Control 0.23331322
76  22    A004   Control 0.25791045
77  22    A005 Treatment 0.18159936
78  22    A007 Treatment 0.17076203
79  23    A001   Control 0.17558492
80  23    A002 Treatment 0.12551814
81  23    A003   Control 0.21406131
82  23    A004   Control 0.22028128
83  23    A005 Treatment 0.17529323
84  23    A007 Treatment 0.14576150
85  24    A001   Control 0.15733775
86  24    A002 Treatment 0.12099877
87  24    A003   Control 0.22833499
88  24    A004   Control 0.15324628
89  24    A005 Treatment 0.15217124
90  24    A007 Treatment 0.09604689
</code></pre>

<p>Now I try to fit a 6th order polynomial with a base model with no categorical variables, one to assess the intercept and one to assess the interaction between terms</p>

<pre><code>library(lme4)

model.base=lmer(Response ~ poly(Day, 6, raw=FALSE)+(Day | Subject), df)
model.1=lmer(Response ~ poly(Day, 6, raw=FALSE)+Levels+(Day | Subject), df)
model.2=lmer(Response ~ poly(Day, 6, raw=FALSE)*Levels+(Day | Subject), df)
</code></pre>

<p>Then I use <code>anova</code> function to assess the model improvement</p>

<pre><code>&gt; anova(model.base,model.1,model.2)
refitting model(s) with ML (instead of REML)
Data: df
Models:
model.base: Response ~ poly(Day, 6, raw = FALSE) + (Day | Subject)
model.1: Response ~ poly(Day, 6, raw = FALSE) + Levels + (Day | Subject)
model.2: Response ~ poly(Day, 6, raw = FALSE) * Levels + (Day | Subject)
           Df     AIC     BIC logLik deviance   Chisq Chi Df Pr(&gt;Chisq)   
model.base 11 -302.85 -275.35 162.42  -324.85                             
model.1    12 -309.60 -279.61 166.80  -333.60  8.7579      1   0.003083 **
model.2    18 -313.00 -268.00 174.50  -349.00 15.3978      6   0.017378 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>and now my question is how can I plot the fitted data from the model and their confidence interval around the fitted lines similar to this example in <code>ggplot</code></p>

<pre><code>ggplot(df, aes(Day, Response, color = Levels)) +
  geom_point()+
  scale_x_continuous(breaks = c(seq(10,26,2)), limits = c(9.5,26.5))+
  stat_smooth(method=""lm"", se=TRUE, 
              formula=y ~ poly(x, 6, raw=FALSE))
</code></pre>

<p><a href=""http://i.stack.imgur.com/9pEhF.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9pEhF.jpg"" alt=""Model fit""></a></p>

<p>So far I have tried <code>confint</code>, <code>effects</code> and <code>lsmeans</code> packages to extract the confidence intervals, being unsuccessful.</p>

<p>Do you have any idea how this could be done?</p>
"
"0.212479222860543","0.212959402507645","235168","<p>I'm studying Design and Analysis of Experiments, 8th Edition. Douglas C. Montgomery is the author. I'm trying to replicate the first example he gives in Chapter 13, Experiments with Random Factors.</p>

<p>In this example, there are measurements in a critical dimension on a part. 20 parts are randomly selected and measured by 3 operators, also selected at random. I want to fit two models to this data. The first one I call full model and it is given by</p>

<p>$$y_{ijk} = \mu + \tau_i + \beta_j + (\tau\beta)_{ij} + \varepsilon_{ijk}$$</p>

<p>The other model I call reduced model ant it is given by</p>

<p>$$y_{ijk} = \mu + \tau_i + \beta_j + \varepsilon_{ijk}$$</p>

<p>Both $\tau_i, i=1, \cdots, 20$ and $\beta_j, j=1, 2, 3$ are random effects. The code I'm using to analyze my problem is below:</p>

<pre><code>gauge &lt;- structure(list(part = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 1L, 2L, 3L, 
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 
18L, 19L, 20L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 1L, 2L, 3L, 4L, 
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 
19L, 20L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 1L, 2L, 3L, 4L, 5L, 6L, 
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 
20L), operator = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), replication = c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L), measurement = c(21L, 24L, 20L, 27L, 
19L, 23L, 22L, 19L, 24L, 25L, 21L, 18L, 23L, 24L, 29L, 26L, 20L, 
19L, 25L, 19L, 20L, 23L, 21L, 27L, 18L, 21L, 21L, 17L, 23L, 23L, 
20L, 19L, 25L, 24L, 30L, 26L, 20L, 21L, 26L, 19L, 20L, 24L, 19L, 
28L, 19L, 24L, 22L, 18L, 25L, 26L, 20L, 17L, 25L, 23L, 30L, 25L, 
19L, 19L, 25L, 18L, 20L, 24L, 21L, 26L, 18L, 21L, 24L, 20L, 23L, 
25L, 20L, 19L, 25L, 25L, 28L, 26L, 20L, 19L, 24L, 17L, 19L, 23L, 
20L, 27L, 18L, 23L, 22L, 19L, 24L, 24L, 21L, 18L, 25L, 24L, 31L, 
25L, 20L, 21L, 25L, 19L, 21L, 24L, 22L, 28L, 21L, 22L, 20L, 18L, 
24L, 25L, 20L, 19L, 25L, 25L, 30L, 27L, 20L, 23L, 25L, 17L)), .Names = c(""part"", 
""operator"", ""replication"", ""measurement""), class = ""data.frame"", row.names = c(NA, 
-120L))

###############
# full model
fit.full &lt;- lmer(measurement ~ (1|part) + (1|operator) + (1|part:operator), data=montgomery)
summary(fit.full)
Linear mixed model fit by REML ['lmerMod']
Formula: measurement ~ (1 | part) + (1 | operator) + (1 | part:operator)
   Data: montgomery

REML criterion at convergence: 409.4

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0313 -0.6595  0.1270  0.5374  2.7345 

Random effects:
 Groups        Name        Variance Std.Dev.
 part:operator (Intercept)  0.00000 0.0000  
 part          (Intercept) 10.25127 3.2018  
 operator      (Intercept)  0.01063 0.1031  
 Residual                   0.88316 0.9398  
Number of obs: 120, groups:  part:operator, 60; part, 20; operator, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  22.3917     0.7235   30.95

###############
# reduced model
fit.reduced &lt;- lmer(measurement ~ (1|part) + (1|operator), data=montgomery)
summary(fit.reduced)
Linear mixed model fit by REML ['lmerMod']
Formula: measurement ~ (1 | part) + (1 | operator)
   Data: montgomery

REML criterion at convergence: 409.4

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0313 -0.6595  0.1270  0.5374  2.7345 

Random effects:
 Groups   Name        Variance Std.Dev.
 part     (Intercept) 10.25127 3.2018  
 operator (Intercept)  0.01063 0.1031  
 Residual              0.88316 0.9398  
Number of obs: 120, groups:  part, 20; operator, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  22.3917     0.7235   30.95    
</code></pre>

<p>However, I'm getting different estimates from the ones in the book. Montgomery used Minitab to fit its model and here are his results for the full model:</p>

<p><a href=""http://i.stack.imgur.com/1aeGL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1aeGL.png"" alt=""Anova Table for the Full Model""></a></p>

<p>They are different from mine. Notice how his <code>part*operator</code> has a negative estimation, while mine is zero. However, his estimates for the reduced model are the same as mine:</p>

<p><a href=""http://i.stack.imgur.com/SaGVu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SaGVu.png"" alt=""Anova Table for the Reduced Model""></a></p>

<p>So, my question about his problem are:</p>

<ol>
<li><p>Why our estimates differ for the full model? I understand that I can't have a negative variance like the one he got, but why does Minitab doesn't set it to zero? </p></li>
<li><p>Using R, where (or how) can I get an ANOVA table like the one Minitab presents? I couldn't test my hypothesis in this problem because I can't find the p-values associated with the parameters I'm testing.</p></li>
</ol>
"
