"V1","V2","V3","V4"
"0.362738125055006","0.367607311046904"," 51302","<p>For a two-way repeated measures design, we can specify the model using <code>aov</code> in the following fashion:</p>

<pre><code>aov(dv ~ iv1 + iv2 + Error(subject/(iv1*iv2)), data=dataset)
</code></pre>

<p>Take the sample data from <code>personality-project.org</code> (see code below), the error term for this particular model is <code>Error(Subject/(Task*Valence))</code>.</p>

<pre><code>datafilename=""http://personality-project.org/r/datasets/R.appendix4.data""
data.ex4=read.table(datafilename,header=T)   #read the data into a table
data.ex4                                      #show the data
aov.ex4=aov(Recall~(Task*Valence)+Error(Subject/(Task*Valence)),data.ex4 )
</code></pre>

<p>What if I specify the error term instead as <code>Error(Subject)</code>. <strong>What kind of design is this?</strong>   </p>

<p>From what I recall, whether you include a factor - e.g., whether <code>Task</code> and <code>Valence</code> are included - in the error term depends on whether a factor is a ""within-subjects"" factor. If, say, this particular recall study also included ""sex"" as a predictor, then we would not include it in the error term because it is a between-subjects factor. But I am not sure what it means when we don't include any of the predictors in the error term. <strong>Is it basically an intercept-only model much like the one below specified using <code>lmer</code></strong>?</p>

<pre><code>lmer(Recall~Task*Valence + (1|Subject), data=data.ex4)
</code></pre>
"
"NaN","NaN"," 76594","<p>I am trying to fit a linear model (<code>lme4::lmer()</code>) to my data in R. I would like to look at a number of things, including ""<strong>scrambling</strong>"" of visual stimuli and ""<strong>intensity</strong>"" of the emotions portrayed therein. These things are stored the ""<strong>scrambling</strong>"" and ""<strong>intensity</strong>"" columns of my dataframe.</p>

<p>To ease your comprehension you may see a graphic plot of my data in <a href=""http://stats.stackexchange.com/questions/76134/determining-best-approximator-based-on-repeated-measurements"">this other thread</a>.</p>

<p>I have been told that linear model results can be compromised if category names are parsed as integers instead of strings by accident. But since these measures (<strong>scrambling</strong> and <strong>intensity</strong>) are kind-of quantitative, I am thinking it may be better to leave them as integers - or maybe even use both approaches separately.</p>

<p>I am however unsure how my interpretation of results should vary depending on whether or not my category IDs are passed as stings or ints.</p>

<p>Could anyone explain this to me?</p>

<p>Also would this differentiation still hold for when I use <code>stats::aov()</code> on the same data?</p>
"
"0.162221421130763","0.164398987305357","163107","<p>Here is the R code :</p>

<pre><code>library(lme4)
fm1 &lt;- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
confint(fm1)

Computing profile confidence intervals ...
                 2.5 %     97.5 %
.sig01       14.3815760  37.715996
.sig02       -0.4815007   0.684986
.sig03        3.8011641   8.753383
.sigma       22.8982669  28.857997
(Intercept) 237.6806955 265.129515
Days          7.3586533  13.575919
</code></pre>

<p>I understand how  the confidence interval for <code>(Intercept)</code> and <code>coefficient of Days</code> are calculated (though it is <strong>not profile</strong> confidence intervals ) :</p>

<pre><code>res=summary(fm1)
ans=coefficients(res)
estimate = ans[,1]
se = ans[,2]

ci.int = estimate[1]+qnorm(c(.025,.975))*se[1]
[1] 238.0292 264.7810

 ci.day = estimate[2]+qnorm(c(.025,.975))*se[2]
 [1]  7.437595 13.496977
</code></pre>

<p>But I don't know how are the confidence interval of <code>sig01</code> , <code>sig02</code> , <code>sig03</code> , <code>sigma</code> calculated ?</p>
"
"0.229415733870562","0.232495277487639","117660","<p>My question is based on <a href=""http://stats.stackexchange.com/a/13816/442"">this response</a> which showed which <code>lme4::lmer</code> model corresponds to a two-way repeated measures ANOVA:</p>

<pre><code>require(lme4)
set.seed(1234)
d &lt;- data.frame(
    y = rnorm(96),
    subject = factor(rep(1:12, 4)),
    a = factor(rep(1:2, each=24)),
    b = factor(rep(rep(1:2, each=12))),
    c = factor(rep(rep(1:2, each=48))))

# standard two-way repeated measures ANOVA:
summary(aov(y~a*b+Error(subject/(a*b)), d[d$c == ""1"",]))

# corresponding lmer call:
anova(lmer(y ~ a*b+(1|subject) + (1|a:subject) + (1|b:subject), d[d$c == ""1"",]))
</code></pre>

<p>My question now is on how to extend this to the case of a three-way ANOVA:</p>

<pre><code>summary(aov(y~a*b*c+Error(subject/(a*b*c)), d))
## [...]
## Error: subject:a:b:c
##           Df Sum Sq Mean Sq F value Pr(&gt;F)
## a:b:c      1  0.101  0.1014   0.115  0.741
## Residuals 11  9.705  0.8822 
</code></pre>

<p>The natural extension as well as versions thereof do not match the ANOVA results:</p>

<pre><code>anova(lmer(y ~ a*b*c +(1|subject) + (1|a:subject) + (1|b:subject) + (1|c:subject), d))
## [...]
## a:b:c  1 0.1014  0.1014  0.1500

anova(lmer(y ~ a*b*c +(1|subject) + (1|a:subject) + (1|b:subject) + (1|c:subject) + 
               (1|a:b:subject) + (1|a:c:subject) + (1|b:c:subject), d))
## [...]
## a:b:c  1 0.1014  0.1014  0.1539
</code></pre>

<p>Note that a very similar question has been asked <a href=""http://stats.stackexchange.com/q/99765/442"">before</a>. However, it was missing example data (which is provided here).</p>
"
"0.561951486949016","0.569494797451499"," 31118","<p>I performed an experiment where I raised different families coming from two different source populations, where each family was split up into a different treatments. After the experiment I measured several traits on each individual. 
To test for an effect of either treatment or source as well as their interaction, I used a linear mixed effect model with family as random factor, i.e.</p>

<pre><code>lme(fixed=Trait~Treatment*Source,random=~1|Family,method=""ML"")
</code></pre>

<p>so far so good,
Now I have to calculate the relative variance components, i.e. the percentage of variation that is explained by either treatment or source as well as the interaction.</p>

<p>Without a random effect, I could easily use the sums of squares (SS) to calculate the variance explained by each factor. But for a mixed model (with ML estimation), there are no SS, hence I thought I could use Treatment and Source as random effects too to estimate the variance, i.e.</p>

<pre><code>lme(fixed=Trait~1,random=~(Treatment*Source)|Family, method=""REML"")
</code></pre>

<p>However, in some cases, lme does not converge, hence I used lmer from the lme4 package:</p>

<pre><code>lmer(Trait~1+(Treatment*Source|Family),data=DATA)
</code></pre>

<p>Where I extract the variances from the model using the summary function:</p>

<pre><code>model&lt;-lmer(Trait~1+(Treatment*Source|Family),data=regrexpdat)
results&lt;-model@REmat
variances&lt;-results[,3]
</code></pre>

<p>I get the same values as with the VarCorr function. I use then these values to calculate the actual percentage of variation taking the sum as the total variation.</p>

<p>Where I am struggling is with the interpretation of the results from the initial lme model (with treatment and source as fixed effects) and the random model to estimate the variance components (with treatment and source as random effect). I find in most cases that the percentage of variance explained by each factor does not correspond to the significance of the fixed effect.</p>

<p>For example for the trait HD,
The initial lme suggests a tendency for the interaction as well as a significance for Treatment. Using a backward procedure, I find that Treatment has a close to significant tendency. However, estimating variance components, I find that Source has the highest variance, making up to 26.7% of the total variance.</p>

<p>The lme:</p>

<pre><code>anova(lme(fixed=HD~as.factor(Treatment)*as.factor(Source),random=~1|as.factor(Family),method=""ML"",data=test),type=""m"")
                                      numDF denDF  F-value p-value
(Intercept)                                1   426 0.044523  0.8330
as.factor(Treatment)                       1   426 5.935189  0.0153
as.factor(Source)                          1    11 0.042662  0.8401
as.factor(Treatment):as.factor(Source)     1   426 3.754112  0.0533
</code></pre>

<p>And the lmer:</p>

<pre><code>summary(lmer(HD~1+(as.factor(Treatment)*as.factor(Source)|Family),data=regrexpdat))
Linear mixed model fit by REML 
Formula: HD ~ 1 + (as.factor(Treatment) * as.factor(Source) | Family) 
   Data: regrexpdat 
    AIC    BIC logLik deviance REMLdev
 -103.5 -54.43  63.75   -132.5  -127.5
Random effects:
 Groups   Name                                      Variance  Std.Dev. Corr                 
 Family   (Intercept)                               0.0113276 0.106431                      
          as.factor(Treatment)                      0.0063710 0.079819  0.405               
          as.factor(Source)                         0.0235294 0.153393 -0.134 -0.157        
          as.factor(Treatment)L:as.factor(Source)   0.0076353 0.087380 -0.578 -0.589 -0.585 
 Residual                                           0.0394610 0.198648                      
Number of obs: 441, groups: Family, 13

Fixed effects:
            Estimate Std. Error t value
(Intercept) -0.02740    0.03237  -0.846
</code></pre>

<p>Hence my question is, is it correct what I am doing? Or should I use another way to estimate the amount of variance explained by each factor (i.e. Treatment, Source and their interaction). For example, would the effect sizes be a more appropriate way to go?</p>

<p>Thanks!</p>

<p>Kay Lucek</p>
"
"0.672592709134549","0.681621203167467","223626","<p>In R, I'm wondering how the functions <code>anova()</code> (<code>stats</code> package) and <code>Anova()</code> (<code>car</code> package) differ when being used to compare nested models fit using the <code>glmer()</code> (generalized linear mixed effects model; <code>lme4</code> package) and <code>glm.nb</code> (negative binomial; <code>MASS</code> package) functions. </p>

<p>I've found the two ANOVA functions do not produce the same results for tests of fixed effects in a Poisson mixed model, or a negative binomial fixed effects model (no random effects). Results from both are shown below.</p>

<p><em>My goal</em>: Correctly test the overall significance of a multi-level categorical predictor (fixed; <em>Species</em>). I'm looking for a type III SS-type <em>p</em>-value.</p>

<hr>

<p><em>First</em>: If one fits a <strong>fixed effects</strong> generalized linear model (Poisson here) using <code>glm()</code>, then these two functions <strong>do produce the same results</strong> given the arguments as in the following dummy example:</p>

<pre><code>mod01 &lt;- glm(Count ~ Species + offset(log(Area)), data=data01, family=poisson)

####################
# Anova() function #
####################

library(car)
Anova(mod01, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   255.44  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod01x &lt;- update(mod01, . ~ . - Species)
anova(mod01x, mod01, test=""Chisq"")

# Model 1: Count ~ offset(log(Area))
# Model 2: Count ~ Species + offset(log(Area))

#   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
# 1      1063     1456.4                          
# 2      1055     1201.0  8   255.44 &lt; 2.2e-16 ***

# Test statistics are the SAME (255.44) for the fixed effects model
</code></pre>

<hr>

<p><em>However</em>: For a generalized linear <strong>mixed effects</strong> model (using <code>glmer()</code> with random effect for <em>Group</em>), analogous code <strong>gives a different test statistic across the two functions</strong>:</p>

<pre><code>library(lme4)
mod02 &lt;- glmer(Count ~ 1 + Species + (1 | Group) + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod02, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod02x &lt;- update(mod02, . ~ . - Species)
anova(mod02x, mod02, test=""Chisq"")

# mod02x: Count ~ (1 | Group) + offset(log(Area))
# mod02: Count ~ 1 + Species + (1 | Group) + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod02x  2 1423.9 1433.8 -709.95   1419.9                             
# mod02  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Now the test statistics are DIFFERENT (197.9012 vs. 248.21)

#####################################################################

# Not a matter of type I vs. III SS since whether the fixed or random
# effect is fit first in the model does not affect results:

# List random effect (Group) before fixed (Species):

mod03 &lt;- glmer(Count ~ 1 + (1 | Group) + Species + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod03, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod03x &lt;- update(mod03, . ~ . - Species)
anova(mod03x, mod03, test=""Chisq"")

# mod03x: Count ~ (1 | Group) + offset(log(Area))
# mod03: Count ~ 1 + (1 | Group) + Species + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod03x  2 1423.9 1433.8 -709.95   1419.9                             
# mod03  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Respective test statistics are the same as above case where order of fixed
# and random effects was reversed
</code></pre>

<hr>

<p>Another example of inconsistent test statistics: <strong>Fixed effects negative binomial model</strong>:</p>

<pre><code>library(MASS)
mod04 &lt;- glm.nb(Count ~ Species + offset(log(Area)), data=data01)

####################
# Anova() function #
####################

Anova(mod04, type=3)

# Analysis of Deviance Table (Type III tests)

# Response: Spiders_Tree
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   101.08  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod04x &lt;- update(mod04, . ~ . - Species)
anova(mod04x, mod04)

# Likelihood ratio tests of Negative Binomial Models

# Response: Count
#                            Model     theta Resid. df  2 x log-lik.   Test df LR stat.       Pr(Chi)
# 1           offset(log(Area_M2)) 0.2164382      1063     -1500.688                      
# 2 Species + offset(log(Area_M2)) 0.3488095      1055     -1413.651 1 vs 2  8 87.03677  1.887379e-15 

# Test statistics are also DIFFERENT here (101.08 vs. 87.03677)
</code></pre>

<hr>

<p><em>In summary</em>: The problem:</p>

<ol>
<li>Isn't restricted to only mixed or only fixed effects models</li>
<li>Isn't a matter of type I or III SS, since an example with only one predictor (negative binomial fixed effects model) showed the same problem, and even in the case of more than one predictor (mixed model example), the test is only for the removal of one predictor (<em>Species</em>), so I believe the two types of SS should be equivalent in this case.</li>
</ol>

<p>Could it have to do with the offset? Maybe the functions were written to ""behave well"" with the <code>glm()</code> function, but process others (such as <code>glmer()</code> and <code>glm.nb()</code>) inconsistently? Something else I'm not thinking of?</p>

<hr>

<p>I'm not providing data for my example code above, as I'm assuming someone can comment on the differing theories of each function without a minimal working example. However, if you would like to verify the results really do differ (as shown above), I will add a dummy dataset.</p>
"
"0.584897651865602","0.592748978363819"," 58745","<p>EDIT 2: I originally thought I needed to run a two-factor ANOVA with repeated measures on one factor, but I now think a linear mixed-effect model will work better for my data. I think I nearly know what needs to happen, but am still confused by few points.</p>

<p>The experiments I need to analyze look like this: </p>

<ul>
<li>Subjects were assigned to one of several treatment groups</li>
<li>Measurements of each subject were taken on multiple days</li>
<li>So:
<ul>
<li>Subject is nested within treatment</li>
<li>Treatment is crossed with day</li>
</ul></li>
</ul>

<p>(each subject is assigned to only one treatment, and measurements are taken on each subject on each day)</p>

<p>My dataset contains the following information:</p>

<ul>
<li>Subject = blocking factor (random factor)</li>
<li>Day = within subject or repeated measures factor (fixed factor)</li>
<li>Treatment = between subject factor (fixed factor)</li>
<li>Obs = measured (dependent) variable</li>
</ul>

<p><strong>UPDATE</strong>
OK, so I went and talked to a statistician, but he's an SAS user.  He thinks that the model should be:</p>

<p><strong>Treatment + Day + Subject(Treatment) + Day*Subject(Treatment)</strong></p>

<p>Obviously his notation is different from the R syntax, but this model is supposed to account for:</p>

<ul>
<li>Treatment   (fixed)</li>
<li>Day   (fixed)</li>
<li>the Treatment*Day interaction</li>
<li>Subject nested within Treatment  (random)</li>
<li>Day crossed with ""Subject within Treatment""   (random)</li>
</ul>

<p>So, is this the correct syntax to use? </p>

<pre><code>m4 &lt;- lmer(Obs~Treatment*Day + (1+Treatment/Subject) + (1+Day*Treatment/Subject), mydata)
</code></pre>

<p>I'm particularly concerned about whether the Day crossed with ""Subject within Treatment"" part is right.  Is anyone familiar with SAS, or confident that they understand what's going on in his model, able to comment on whether my sad attempt at R syntax matches?</p>

<p>Here are my previous attempts at building a model and writing syntax (discussed in answers &amp; comments):</p>

<pre><code>m1 &lt;- lmer(Obs ~ Treatment * Day + (1 | Subject), mydata)
</code></pre>

<p>How do I deal with the fact that subject is nested within treatment?  How does <code>m1</code> differ from: </p>

<pre><code>m2 &lt;- lmer(Obs ~ Treatment * Day + (Treatment|Subject), mydata)
m3 &lt;- lmer(Obs ~ Treatment * Day + (Treatment:Subject), mydata)
</code></pre>

<p>and are <code>m2</code> and <code>m3</code> equivalent (and if not, why)?</p>

<p>Also, do I need to be using nlme instead of lme4 if I want to specify the correlation structure (like <code>correlation = corAR1</code>)?  According to <a href=""http://circ.ahajournals.org/content/117/9/1238.full"">Repeated Measures</a>, for a repeated-measures analysis with repeated measures on one factor, the covariance structure (the nature of the correlations between measurements of the same subject) is important. </p>

<p>When I was trying to do a repeated-measures ANOVA, I'd decided to use a Type II SS; is this still relevant, and if so, how do I go about specifying that?</p>

<p>Here's an example of what the data look like:</p>

<pre><code>mydata &lt;- data.frame(
  Subject  = c(13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 
               34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 
               19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 
               40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 
               29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65), 
  Day       = c(rep(c(""Day1"", ""Day3"", ""Day6""), each=28)), 
  Treatment = c(rep(c(""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", 
                      ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A""), each = 4)), 
  Obs       = c(6.472687, 7.017110, 6.200715, 6.613928, 6.829968, 7.387583, 7.367293, 
                8.018853, 7.527408, 6.746739, 7.296910, 6.983360, 6.816621, 6.571689, 
                5.911261, 6.954988, 7.624122, 7.669865, 7.676225, 7.263593, 7.704737, 
                7.328716, 7.295610, 5.964180, 6.880814, 6.926342, 6.926342, 7.562293, 
                6.677607, 7.023526, 6.441864, 7.020875, 7.478931, 7.495336, 7.427709, 
                7.633020, 7.382091, 7.359731, 7.285889, 7.496863, 6.632403, 6.171196, 
                6.306012, 7.253833, 7.594852, 6.915225, 7.220147, 7.298227, 7.573612, 
                7.366550, 7.560513, 7.289078, 7.287802, 7.155336, 7.394452, 7.465383, 
                6.976048, 7.222966, 6.584153, 7.013223, 7.569905, 7.459185, 7.504068, 
                7.801867, 7.598728, 7.475841, 7.511873, 7.518384, 6.618589, 5.854754, 
                6.125749, 6.962720, 7.540600, 7.379861, 7.344189, 7.362815, 7.805802, 
                7.764172, 7.789844, 7.616437, NA, NA, NA, NA))
</code></pre>
"
"0.162221421130763","0.164398987305357"," 92616","<p>What's going on here?</p>

<pre><code>data.2
         subj phon f.amp
    1     1    V   100
    2     2    V    60
    3     3    V   124
    4     4    V    42
    5     5    V   210
    6     6    V   104
    7     7    V   150
    8     1    Ê”    92
    9     2    Ê”    33
    10    3    Ê”    92
    11    4    Ê”    32
    12    5    Ê”    90
    13    6    Ê”    65
    14    7    Ê”   105
    15    1    h   142
    16    2    h    72
    17    3    h   141
    18    4    h    60
    19    5    h   268
    20    6    h   134
    21    7    h   145
</code></pre>

<p>Pairwise comparison of levels PHON<sub>V</sub> and PHON<sub>h</sub> by running ANOVA on the pertinent subset of data:</p>

<pre><code>library(lme4)
anova(lmer(f.amp~phon+(1|subj),data.2[which(data.2[,2]!=""Ê”""),]))
   Analysis of Variance Table
        Df Sum Sq Mean Sq F value
   phon  1 2113.1  2113.1  9.8144
</code></pre>

<p>Pairwise comparison of the same levels by direct definition of contrast coefficients; different resulting <em>F</em>-ratio:</p>

<pre><code>contrasts(data.2[,2],1)=matrix(c(-1,0,1),nrow=3)
contrasts(data.2[,2])
    [,1]
  V   -1
  Ê”    0
  h    1
anova(lmer(f.amp~phon+(1|subj),data.2))
   Analysis of Variance Table
        Df Sum Sq Mean Sq F value
   phon  1 2113.1  2113.1  1.2566
</code></pre>

<p>Since <em>df</em><sub>PHON</sub>, <em>SS</em><sub>PHON</sub> and <em>MS</em><sub>PHON</sub> are the same for both analyses; and since <em>F</em><sub>PHON</sub> = <em>MS</em><sub>PHON</sub> / <em>MS</em><sub>PHON x <em>S</em></sub>, I deduce that the analyses differ regarding the handling of <em>S</em>.</p>

<p>Any ideas as to how and why precisely?</p>
"
"0.229415733870562","0.232495277487639"," 84312","<p>I have data from a split-plot (or repeated measure) experiment with three factors: A is random, B is fixed and nested within A, and C is fixed and nested within B. I can test for the effect of B, C and their interaction with the following code:</p>

<pre><code>aov(x~B*C+Error(A/B/C))
</code></pre>

<p>I'd like to replicate this model using either lmer or lme. Numerous references online seem to indicate that <code>Error(A/B)</code> is analogous to <code>(1|A/B)</code> for lmer or <code>random=~1|A/B</code> for lme. Extending that logic, I tried:</p>

<pre><code>lme(x~B*C,random=~1|A/B/C)
lmer(x~B*C+(1|A/B/C)
</code></pre>

<p>But these tables both gave different F values than the table generated with aov (. Can anyone tell me where I've gone wrong and how I can translate my initial aov model into either lme or lmer?</p>

<p>Thanks much!</p>
"
"0.280975743474508","0.28474739872575"," 86495","<p>I don't know if this belongs here or in StackExchange, it is a mixed but probably pretty simple question. How do I normally report a Likelihood Ratio Test? I would love a good reference in your answer, I have searched but could not find any good answers.</p>

<pre><code>&gt; glmm0 &lt;- glmer(yngel ~ (1|nest), data, family=poisson(link=""log""))
&gt; glmm &lt;- glmer(yngel ~ age.level + (1|nest) + 0, data, family=poisson(link=""log""))
&gt; anova(glmm0, glmm)
Data: data
Models:
glmm0: yngel ~ (1 | nest)
glmm: yngel ~ age.level + (1 | nest) + 0
      Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
glmm0  2 682.33 689.38 -339.16   678.33                             
glmm   3 672.37 682.95 -333.18   666.37 11.959      1   0.000544 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My best guess so far is: I used likelihood ratio test to compare the model with the fixed effect to a model without it. The model including the fixed effect (age-level) was a better fit ($\chi^2(df=?)=11.96$, $p=0.00054$).</p>

<p>And I cannot actually figure out how many df to report from that. The there is 2 for one model and 3 for the other, and 1 between them.</p>

<p>Thank you for your help.</p>
"
"0.162221421130763","0.164398987305357","101566","<p>I am fitting a <code>glmer</code> model in the <code>lme4</code> R package. I'm looking for an anova table with p-value shown therein, but I cannot find any package that fits it. Is it possible to do it in R?</p>

<p>The model I am fitting is of the form:</p>

<pre><code>model1&lt;-glmer(dmn~period*teethTreated+(1|fullName), 
   family=""poisson"", 
   data=subset(dataset, 
          group=='Four times a year'),
   control=glmerControl(optimizer=""bobyqa""))
</code></pre>
"
"0.53802758684897","0.49568159709661"," 71914","<p>Hopefully this is a question that someone here can answer for me on the nature of decomposing sums of squares from a mixed-effects model fit with <code>lmer</code> (from the <a href=""http://cran.r-project.org/web/packages/lme4/index.html"">lme4</a> R package).</p>

<p>First off I should say that I am aware of the controversy with using this approach, and in practise I would be more likely to use a bootstrapped LRT to compare models (as suggested by Faraway, 2006). However, I am puzzled at how to replicate the results, and so for my own sanity I thought I would ask here.</p>

<p>Basically, I am getting to grips with using mixed-effects models fit by the <code>lme4</code> package. I know that you can use the <code>anova()</code> command to give a summary of sequentially testing the fixed-effects in the model. As far as I know this is what Faraway (2006) refers to as the 'Expected mean squares' approach. What I want to know is how are the sums of squares calculated?</p>

<p>I know that I could take the estimated values from a particular model (using <code>coef()</code>), assume that they are fixed, and then make tests using the sums of squares of model residuals with and without the factors of interest. This is fine for a model containing a single within-subject factor. However, when implementing a split-plot design the sums of squares value I get is equivalent to the value produced by R using <code>aov()</code> with an appropriate <code>Error()</code> designation. However, this is <em>not</em> the same as the sums of squares produced by the <code>anova()</code> command on the model object, despite the fact that the F-ratios are the same. </p>

<p>Of course this makes complete sense as there is no need for the <code>Error()</code> strata in a mixed-model. However, this must mean that the sums of squares are penalised somehow in a mixed-model in order to provide appropriate F-ratios. How is this achieved? And how does the model somehow correct the between-plot sum of squares but not correct the within-plot sum of squares. Evidently this is something that is necessary for a classical split-plot ANOVA that was achieved by designating different error values for the different effects, so how does a mixed-effect model allow for this?</p>

<p>Basically, I want to be able to replicate the results from the <code>anova()</code> command applied to a lmer model object myself to verify the results and my understanding, however, at present I can achieve this for a normal within-subject design but not for the split-plot design and I can't seem to find out why this is the case.   </p>

<p>As an example:</p>

<pre><code>library(faraway)
library(lme4)
data(irrigation)

anova(lmer(yield ~ irrigation + variety + (1|field), data = irrigation))

Analysis of Variance Table
           Df Sum Sq Mean Sq F value
irrigation  3 1.6605  0.5535  0.3882
variety     1 2.2500  2.2500  1.5782

summary(aov(yield ~ irrigation + variety + Error(field/irrigation), data = irrigation))

Error: field
           Df Sum Sq Mean Sq F value Pr(&gt;F)
irrigation  3  40.19   13.40   0.388  0.769
Residuals   4 138.03   34.51               

Error: Within
          Df Sum Sq Mean Sq F value Pr(&gt;F)
variety    1   2.25   2.250   1.578  0.249
Residuals  7   9.98   1.426               
</code></pre>

<p>As can be seen above all the F-ratios agree. The sums of squares for variety also agree. However, the sums of squares for irrigation do not agree, however it appears the lmer output is scaled. So what does the anova() command actually do?</p>
"
"0.229415733870562","0.232495277487639","175069","<p>I am trying to use a three stage nested design to do an analysis on incomes for different households. The data can be visualized as follows:</p>

<pre><code>                  State 1             State 2             State 3
             Town 1    Town 2    Town 1    Town 2    Town 1    Town 2
Household 1  10        7         6         6         15        12
Household 2  13        12        5         12        18        15
Household 3  16        11        9         7         20        18
Household 4  12        9         3         10        19        16
</code></pre>

<p>Where <code>Household</code> is nested in <code>Town</code>, which is nested in <code>State</code>. The problem that I'm running into is a lack of replication within levels of the nested design. I've done similar analyses but I had multiple measurements, with this data I'm just working with the mean incomes of household. When I try and fit the models</p>

<pre><code>house.mod1 &lt;- aov(Income ~ State + State:Town + State:Town:Household)
house.mod2 &lt;- lmer(Income ~ 1 + (1|State) + (1|State:Town) + (1|State:Town:Household))
</code></pre>

<p>I'm left without any degrees of freedom for the error term and so I can't try to estimate the variance components. Is this even possible to do with only one replicate or am I just thinking of the data and the model in the wrong way?</p>
"
"0.229415733870562","0.232495277487639","226884","<p>In many textbooks and R tutorials the repeated measures (within subjects) ANOVA seems to be very straightforward, following the formula</p>

<pre><code>aov.ww &lt;- aov(y ~ w1*w2 + Error(subject/(w1*w2)), data=data.long)
</code></pre>

<p>where <code>w1</code> and <code>w2</code> are two within-subjects variables and hence <code>subject</code> is included in the error term. However, this formula does not seem to treat the observations as dependent, evinced by the very large df. </p>

<p>I have used workarounds like first computing means across participants and then running the <code>aov</code> function on the reduced dataset. Also, many sources point to <code>lmer</code> as an alternative to <code>aov</code> for repeated measures design.</p>

<p>What would be the most robust and straightforward (in terms on interpretation or the results) method for analyzing data from repeated measures experiments?</p>
"
"NaN","NaN","143964","<p>Please could anyone tell me if my R code is correct?</p>

<p>I have a two-way model with one fixed factor, habitat, and one random factor, site.  The code I am using is:</p>

<pre><code>lmer (x ~ habitat*site + (1|site))
</code></pre>

<p>This appears to be equivalent to: </p>

<pre><code>lmer x ~ habitat + (1|site) + (1|site:habitat)
</code></pre>

<p>Thanks,</p>

<p>Martine</p>
"
"0.429197537639476","0.43495883620084"," 99765","<p>I'm trying to run a tree-way repeated measures ANOVA on the following data: I have a completely balanced design with three within-subject factors (type, form and ch - channel) and one dependent variable amp (amplitude).</p>

<p>I'm inclined to believe the results I got using <code>aov</code> function:</p>

<pre><code>res &lt;- aov(amp ~ type*form*ch + Error(sbj/(type*form*ch)), data = p3vals)
</code></pre>

<p>Here is the anova table I have:</p>

<pre><code>Error: Within
               Df Sum Sq Mean Sq F value   Pr(&gt;F)    
type            1   25.0  24.950  12.315 0.000462 ***
form            1   12.9  12.910   6.372 0.011693 *  
ch              1    0.9   0.875   0.432 0.511113    
type:form       1    3.1   3.123   1.542 0.214581    
type:ch         1    0.9   0.938   0.463 0.496404    
form:ch         1    1.3   1.256   0.620 0.431239    
type:form:ch    1    3.0   2.960   1.461 0.226974    
Residuals    1514 3067.3   2.026                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, after reading several pages of similar examples(including <a href=""http://stats.stackexchange.com/questions/14088/why-do-lme-and-aov-return-different-results-for-repeated-measures-anova-in-r"">Why do lme and aov return different results for repeated measures ANOVA in R?</a> ) I decided to try <code>lme</code> and <code>lmer</code> functions from name and lme4 packages for further pairwise multiple comparisons using <code>glht</code> from multcomp package.
In the example mentioned above F values are at least closet those obtained using <code>aov</code>, but I cannot figure out how to get any meaningful results.</p>

<pre><code>lme_p3amp = lmer(amp ~ type*form*ch + (1|sbj) + (1|type:sbj) + (1|form:sbj) + (1|ch:sbj), data = p3vals)
&gt; anova(lme_p3amp)
Analysis of Variance Table
             Df  Sum Sq Mean Sq F value
type          1  0.0266  0.0266  0.0433
form          1  2.0782  2.0782  3.3863
ch            1 28.5513 28.5513 46.5227
type:form     1  2.1980  2.1980  3.5815
type:ch       1  2.9789  2.9789  4.8539
form:ch       1  0.9278  0.9278  1.5118
type:form:ch  1  6.6072  6.6072 10.7661
</code></pre>

<p>and lme produces the following result:</p>

<pre><code> anova(lme(amp ~ type*form*ch, random=list(sbj=pdBlocked(list(~1, pdIdent(~type-1), pdIdent(~form-1), pdIdent(~ch-1)))), data=p3vals))
             numDF denDF  F-value p-value
(Intercept)      1  1508 32.56485  &lt;.0001
type             1  1508  0.02920  0.8643
form             1  1508  3.54422  0.0599
ch               1  1508  8.05747  0.0046
type:form        1  1508  2.79623  0.0947
type:ch          1  1508  3.78969  0.0518
form:ch          1  1508  1.18037  0.2775
type:form:ch     1  1508  8.40553  0.0038
</code></pre>

<p>I'd appreciate if you tell me what is wrong with my code and how can I perform a valid post hoc analysis.</p>
"
