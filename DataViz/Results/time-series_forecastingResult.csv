"V1","V2","V3","V4"
"0.0443678254708057","0.059868434008925","9512","<p>I have a time series data of 30 years and found that ARIMA(0,1,1) has best model among others. I have used the simulate.Arima (forecast package) function to simulate the series into the future.</p>

<pre><code>library(forecast)

series &lt;- ts(seq(25,55), start=c(1976,1))

arima_s &lt;- Arima(series, c(0,1,1))

simulate(arima_s, nsim=50, future=TRUE)
</code></pre>

<p>Later on, i have found the updated value of first forecasted year (i.e. series[31] &lt;- 65). Now i want to simulate the series with this updated value. I am wondering how to do this in R.</p>
"
"0.0443678254708057","0.059868434008925","8055","<p>Could someone walk me through an example on how to use DLM Kalman filtering in R on a time series. Say I have a these values (quarterly values with yearly seasonality); how would you use DLM to predict the next values? And BTW, do I have enough historical data (what is the minimum)?</p>

<pre><code>89  2009Q1  
82  2009Q2  
89  2009Q3  
131 2009Q4  
97  2010Q1  
94  2010Q2  
101 2010Q3  
151 2010Q4  
100 2011Q1  
?   2011Q2
</code></pre>

<p>I'm looking for a R code cookbook-style how-to step-by-step type of answer. Accuracy of the prediction is not my main goal, I just want to learn the sequence of code that gives me a number for 2011Q2, even if I don't have enough data. Thanks!  </p>
"
"NaN","NaN","10109","<p>I am trying to use R to develop a corporate financial model. </p>

<p>The model includes various line items, X, of the following form with actual values for time period 1, 2.. n and projected values for periods n+1, n+2,.. n+k. g is the average growth rate for the forecast period.</p>

<p>I need to construct a vector of the following form:</p>

<pre><code>  X=c(X1,X2,...,Xn,Xn+1=(1+g)Xn,Xn+2=(1+g)Xn+1,...,Xn+k=(1+g)Xn+k-1))
</code></pre>

<p>How would I do this in R?</p>

<p>I have tried looking up the R literature on lagged variables but could not find a simple example which does what is required. I look forward to any guidance that can be given.</p>
"
"0.076847327936784","0.103695169473043","45663","<p>I want to model and predict trends in search behavior. To improve the predictive accuracy I want to use similar trends and learn from their behavior (from my research this seems to be called an exogenous input). This could be similar behavior at the same point in time (probably easier) but also points in the past. One example might be that the interest in Whitney Houston's death behaves similar to Michael Jackson's death a little bit before that. Is there any machinery that automatically finds these correspondences in time series and uses them? Maybe something that aligns peaks or uses correlation or dynamic time warping to align the sequences the right way for modeling/prediction.</p>"
"NaN","NaN","<p>Here are two more examples:  </p>",""
"NaN","NaN","<ul>",""
"NaN","NaN","<li><a href=http://www.google.com/trends/explore#q=%22madden%20nfl%2011%22%2C%20%22madden%20nfl%2012%22%2C%20%22madden%20nfl%2010%22%2C%20%22madden%20nfl%2013%22&amp;cmpt=q rel=nofollow>Google trends for Madden NFL games</a>. One time series has little information about how the future might look like but similar trends (previous games) exhibit very similar behavior.</li>",""
"NaN","NaN","<li><a href=http://www.google.com/trends/explore#q=penn%20state%2C%20joe%20paterno%2C%20penn%20state%20football%2C%20penn%20state%20scandal%2C%20penn%20state%20football&amp;date=1%2F2008%2060m&amp;cmpt=q rel=nofollow>Google trends for state football</a>. This is an example that is a little more tricky. Here some trends are somewhat independent over most of the time interval but at the time of the peak they are suddenly very related (football scandal and head coach). It would be great to have some way of automatically figuring out at the beginning of the peak that now I could really use the information of the other time series.</li>",""
"NaN","NaN","</ul>",""
"NaN","NaN","<p>Any ideas and pointers are very welcome. Coding-wise I would prefer to use Python although it seems I might have to learn how to use R. Maybe I could also combine the two (ie. calling R methods from Python). </p>",""
"NaN","NaN","","<r><time-series><forecasting><python>"
"0.118314201255482","0.159649157357133","13950","<p>As with my previous question, I'm looking at ways to impute missing data in a hierarchical time series data.</p>

<p>With al my other procedures, including the experimentation of imputation packages (<code>Amelia</code>, <code>HoltWinters</code> from <code>Forecast</code> and <code>MICE</code> imputation) I've only been able to use the time series data prior to the missing gap.</p>

<pre><code>     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2001 220 194 238 190 217 244 242 225 242 259 267 244
2002 212 246 250 236 261 286 265 269 226 267 234 246
2003 202 199 297 272 236 266 235 226 260 183 226 265
2004 211 215 219 213 240 236 273 266 262 244 241 235
2005 212 198 233 251 259 282 305 267 241 264 222 269
2006 182 220 250 287 279 281 286 332 300 272 221 233
2007  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
2008 193 215 235 242 246 315 326 280 279 239 236 258
2009 246 189 257 241 268 223 260 288 234 260 216 195
</code></pre>

<p>I'm trying to do simple imputation procedure that uses forecasting and backcasting estimates from the time series model. Forecasting using prior data to predict the future and backcasting  using the later data to â€œpredictâ€ the past.</p>

<p>I would then like to combine the forecast and backcast value to use as imputation. After which I will look at the fit etc.</p>

<p>How do I go about this in coding? </p>

<p>For example, I'm able to determine what SARIMA model exist for the first period 2001-end2006. But not the full period (because my basic functions I know from R does not support the NA values.)</p>

<p>This is only for the period 2001-end2006:</p>

<pre><code>ARIMA(2,0,2)(1,0,1)[12] with non-zero mean 

Call: auto.arima(x = ts.datt) 

Coefficients:
         ar1      ar2      ma1     ma2    sar1     sma1  intercept
      1.3610  -0.8258  -1.2407  0.9191  0.8982  -0.7560   244.8374
s.e.  0.0884   0.0960   0.0878  0.1127  0.2190   0.3335     6.1894

sigma^2 estimated as 605.9:  log likelihood = -335.01
AIC = 686.02   AICc = 688.3   BIC = 704.23
</code></pre>

<p>Should I just model the first period, predict by <code>forecast</code>; model then the last period separately and then backcast? How will I do this backcasting (ie. 'predicting' the past)?</p>

<p><strong>EDIT:</strong>
What I'm asking:
1) How do I use the data from years 2008 &amp; 2009 to BACKCAST? I already know how to use 2001-2006 to forecast. </p>

<p>2) How do I determine the SARIMA model for the whole period? (2001-2009) ie. </p>
"
"0.140303383316578","0.113592366849413","13984","<p>I would like to combine the forecasted and backcasted (viz. the predicted past values) of a  time-series data set into one time-series by minimizing the Mean Squared Prediction Error.</p>

<p>Say I have time series from 2001-2010 with a gap for the year 2007. I have been able to forecast 2007 using the 2001-2007 data (red line - called it $Y_f$) and to backcast using the 2008-2009 data (light blue line - call it $Y_b$).</p>

<p>I would like to combine the data points of $Y_f$ and $Y_b$ into a imputed data point Y_i for each month. Ideally I would like to obtain the weight $w$ such that it minimizes the Mean Squared Prediction Error (MSPE) of $Y_i$. If this is not possible, how would I just find the average between the two time-series' data points?</p>

<p>$$Y_i = w\cdot Y_f + (1-w)\cdot Y_b$$</p>

<p>As a quick example:</p>

<pre><code>tt_f &lt;- ts(1:12, start = 2007, freq = 12)
tt_b &lt;- ts(10:21, start=2007, freq=12)

tt_f
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2007   1   2   3   4   5   6   7   8   9  10  11  12
tt_b
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2007  10  11  12  13  14  15  16  17  18  19  20  21
</code></pre>

<p>I would like to get (just showing the averaging... Ideally minimizing the MSPE)</p>

<pre><code>tt_i
     Jan Feb Mar Apr May Jun  Jul  Aug  Sep  Oct  Nov  Dec
2007 5.5 6.5 7.5 8.5 9.5 10.5 11.5 12.5 13.5 14.5 15.5 16.5
</code></pre>

<p><img src=""http://i.stack.imgur.com/VscVU.jpg"" alt=""enter image description here""></p>
"
"0.0443678254708057","0.059868434008925","15068","<p>I have a binary time series with 1 when the car is not moving, and 0 when the car is moving. I want to make a forecast for a time horizon up to 36 hours ahead and for each hour. </p>

<p>My first approach was to use a Naive Bayes using the following inputs: t-24 (daily seasonal), t-48 (weekly seasonal), hour of the day. However, the results are not very good.</p>

<p>Which articles or software do you recommend for this problem?</p>
"
"0.0627455805138159","0.0846667513334603","55168","<p>Hi all I'm trying to do one step ahead forecast. Lets say I have 1000 data and fit an ARIMA model with it and then I do a forecast for one period ahead. When I get more data I would like to forecast another step using the new data without having to reestimate all coefficients and so on...</p>

<p>This is my code but for some reason it's very slow for a bigger dataset and am not too sure that is doing what I want:</p>

<pre><code>set.seed(1234)
y=ts(log(35+10*rnorm(1000)))
set.seed(4567)
new.data=ts(log(35+10*rnorm(10)))

library(forecast)
model = auto.arima(y)

onestep.for=forecast(model,h=1)
for (i in 1:10) {
  data=c()
  data=c(y,new.data[1:i])
  newfit=Arima(data, model=model)
  forec=forecast(newfit,h=1)
  onestep.for=c(onestep.for,forec)
}
</code></pre>
"
"0.0627455805138159","0.0846667513334603","20746","<p>Iâ€™m trying  to estimate the out-of sample forecast of an ARIMA model, I tried the code below, but it totally doesnâ€™t work!</p>

<pre><code>for(i in 1:83) {
   mod[i] &lt;- arima(window(betahat[,1], start=1, end=109+i),order=c(1,0,0),include.mean=TRUE)
   pre[i] &lt;- predict(mod[i],12)
   error[i] &lt;- pre[i]$pred[12]-betahat[(109+i+12),1]
}
</code></pre>

<p>the data are taken monthly and I divided the data into 2 subsets, the first  composed by 109 obs and the second by 83 observations. From the code I would like to obtain the error for each 12 month forecast, so about  59 errors. In the code I probably have to add an if , the argument in [109+i+12] has to be lower than 192, but itâ€™s not the problem.<br>
I donâ€™t know how to obtain each error, I would like that the outcome of the loop is the list of all the errors.
I would appreciate any suggestions. </p>
"
"0.108678533400333","0.146647115021353","55489","<p>Using the excellent forecast package by Rob Hyndman, I came across the need to not only have prediction intervals, but to simulate a number of future paths, given past observations of a time series with complex seasonalities.
There is something for less complex time series with one or two seasonalities only (simulate.ets() in the forecast package), but in my case, I would require the equivalent of simulate.ets() for the more complex tbats model. </p>

<p>I assume that the data necessary for creating such paths is already present in the fit object, yet the possibility to create sample paths seems to be not directly accessible. Therefore, I have come up with a naive solution and would like to know, whether this approach is correct.</p>

<pre><code>require(forecast)
fit = bats(test,use.parallel=T,use.damped.trend=F,use.trend=T,seasonal.periods=seasonal.periods)
</code></pre>

<p>Naively, I imagine that sample paths can be constructed by using the point forecast from </p>

<pre><code>fit 

&gt; forecast(fit)
         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95
1960.016       24.48576 23.82518 25.14633 23.47550 25.49602
1960.032       24.79870 23.88004 25.71735 23.39374 26.20366
1960.048       25.31743 24.39878 26.23608 23.91247 26.72239
1960.065       25.69254 24.77389 26.61120 24.28759 27.09750 
1960.081       26.06863 25.14998 26.98729 24.66367 27.47359
1960.097       26.43215 25.51350 27.35080 25.02719 27.83711
1960.113       26.77674 25.85809 27.69540 25.37179 28.18170
</code></pre>

<p>and simply adding randomly drawn values from the model fitting procedure.</p>

<pre><code>&gt; fit$errors
Time Series:
Start = c(1959, 2) 
End = c(1960, 1) 
Frequency = 365 
  [1]  0.140656913 -0.455335141 -0.558989185  1.697532911 -0.114406022  0.366182718 -0.377056927  0.396144296
</code></pre>

<p>Therfore, with</p>

<pre><code>prediction = forecast(fit)
errors = fit$errors

path = prediction$mean + sample(errors, size = length(prediction$mean))
plot(ts(path))
</code></pre>

<p>one sample path can be constructed. </p>

<p><a href=""http://i.stack.imgur.com/mO83W.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mO83W.png"" alt=""enter image description here""></a></p>

<p>Is this a valid way of constructing sample paths? If not, what would be a correct way?</p>

<p>Thanks a lot for any help!</p>
"
"NaN","NaN","20763","<p>I have to forecast a time series in R of a Internet network traffic bitrate.
The data are in file <a href=""http://www.forumaltavilla.it/joomla/datitesi/dati.dat"" rel=""nofollow"">http://www.forumaltavilla.it/joomla/datitesi/dati.dat</a> and the sampling time is every 0.05 seconds.
Now, i want to use HoltWinters forecasting. My problem is setting the parameter deltat. On the Internet i saw deltat is the number of samples in a year but in this case deltat=1.58443823e-9 (0.05 seconds in years). Is it true or i should set deltat=0.05?</p>

<p>This is my script.</p>

<pre><code>deltat=0.05
dati.ts=ts(scan(""dati.dat"", deltat),start=0,deltat)
model=HoltWinters(dati.ts)
dati.forecast=forecast(model,h=100)
plot(dati.forecast)
</code></pre>

<p>Thank you very much,</p>
"
"0.076847327936784","0.0691301129820284","121408","<p>I have average life expectancy at birth data for an 8 year period and I would like to use that 8 year period to predict the trend for average life expectancy for the next 5 years. I would then like to ask whether this deviates significantly from the actual average life expectancy over the next 5 years.</p>"
"NaN","NaN","<ol>",""
"NaN","NaN","<li>What's the best regression model to fit to the observation base data in order to get predictions for next 5 years?</li>",""
"NaN","NaN","<li>How can I assess whether the difference between the predicted and observed trend is significant?</li>",""
"NaN","NaN","<li>How can I implement #1 and #2 in R?</li>",""
"NaN","NaN","</ol>",""
"NaN","NaN","","<r><regression><time-series><forecasting><life-expectancy>"
"0.0627455805138159","0.0846667513334603","55716","<p>I would like to decompose the following time series data into seasonal, trend, and residual componenets. The data is an hourly Cooling Energy Profile from a commercial building:</p>

<pre><code>TotalCoolingForDecompose.ts &lt;- ts(TotalCoolingForDecompose, start=c(2012,3,18), freq=8765.81)
plot(TotalCoolingForDecompose.ts)
</code></pre>

<p><img src=""http://i.stack.imgur.com/IQcQ1.png"" alt=""Cooling Energy Time Series""></p>

<p>There are obvious daily and weekly seasonal effects therefore based on the advice from: <a href=""http://stats.stackexchange.com/questions/25203/how-to-decompose-a-time-series-with-multiple-seasonal-components/43203#43203"">How to decompose a time series with multiple seasonal components?</a>, I used the <code>tbats</code> function from the <code>forecast</code> package:</p>

<pre><code>TotalCooling.tbats &lt;- tbats(TotalCoolingForDecompose.ts, seasonal.periods=c(24,168), use.trend=TRUE, use.parallel=TRUE)
plot(TotalCooling.tbats)
</code></pre>

<p>Which results in:</p>

<p><img src=""http://i.stack.imgur.com/fUvBk.png"" alt=""enter image description here""></p>

<p>What do the <code>level</code> and <code>slope</code> components of this model describe? How can I get the <code>trend</code> and <code>remainder</code> components similar to the paper referenced by this package (<a href=""http://robjhyndman.com/papers/complex-seasonality/"" rel=""nofollow"">De Livera, Hyndman and Snyder (JASA, 2011)</a>)?</p>
"
"NaN","NaN","55937","<p>I have fitted an ARIMA(1,1,2) to time series <code>TS1</code> as below:</p>

<pre><code>arima112 &lt;- arima(TS1, c(1,1,2))
</code></pre>

<p>Now I want to use the coefficients of AR and MA that I got from <code>arima112</code> to forecast another time series (<code>TS2</code>). How can I apply the <code>arima112</code> model on <code>TS2</code>?</p>
"
"0.0627455805138159","0.0846667513334603","56002","<p>In this <a href=""http://stats.stackexchange.com/questions/55168/one-step-ahead-forecast-with-new-data-collected-sequentially"">post</a> it was asked how to do one step ahead forecasts using Arima form the forecast package. Now I'm using an example with hourly seasonal data and would like to do something similar but the forecast will be 24 hours ahead. When I get new 24 hours of data I will add them up and produce another 24h forecast. Again without re-estimating the model.</p>

<p>I'm not sure whether the following code is right since the fitted values are theoretically only for the next hour but not for the next 24 hours:</p>

<pre><code>library(expsmooth)
data(utility)
n=length(utility)
y=ts(log(utility[1:(n-28*24)]),f=24)
new.data=ts(log(utility[(n-28*24+1):n]))

library(forecast)
model = auto.arima(y)
newfit &lt;- Arima(new.data, model=model)
onestep.for &lt;- fitted(newfit)
</code></pre>
"
"0.0313727902569079","0.0846667513334603","203105","<p>My apologies in advance for asking what I suspect is a dumb question. I have looked around and I can't figure this out.</p>

<p>I've done an auto.arima model in r.</p>

<pre><code>revenue = ts(arima$both.markets, frequency = 7)
    media=ts (arima$all_media, frequency = 7)
xreg &lt;- cbind(media=model.matrix(~as.factor(media)))
xreg &lt;- xreg[,-1]
modArima &lt;- auto.arima(revenue, xreg=xreg)
</code></pre>

<p>The output includes this:</p>

<pre><code>                ma1     ma2     sar1    (media)1    (media)2    (media)3    media)4 (media)5    (media)6    (media)7    (media)8    (media)9    (media)11   (media)13   (media)17   (media)18   (media)20   (media)23   (media)39
Coefficients:   -0.4081 -0.5391 0.6145  -8345.84    20129.82    1809.952    -14906.92   -42454.82   1885.815    -101350.54  12055.98    56197.28    -49130.22   128427.87   45600.38    -28911.02   46118.11    -95280.16   62833.34
s.e.            0.0791  0.0778  0.0718  11672.8     13384.69    21822.541   34298.06    23533.55    30755.171   35534.13    57394.97    44116.15    55263.58    55887.56    56920.24    60269.17    40252.73    49884.66    63023.11
</code></pre>

<p>Are the various <code>media</code> outputs lags of the media variable? If not, how can I include lags in the model?</p>
"
"0.076847327936784","0.0691301129820284","56568","<p>I am trying to decompose and forecast a weekly time series which is believed to be affected by moving holidays (e.g. Chinese New Year, which happens in different weeks of a year). 
I would like create a regressor variable to reflect the holiday effect on the series. 
Is it correct to use the regressor variable as xreg in forecasting stl object / stlf?</p>

<p>Also, I would like to know the difference between the following methods, and whether they are doing the job I wanted.</p>

<p>1) decompose using stl, then forecast the decomposed object, i.e.</p>

<pre><code>   model&lt;-stl(tseries,""periodic"")
   forecast&lt;-forecast(model,h=10,method=""arima"",xreg=xreg,newxreg=newxreg)
</code></pre>

<p>2) use stlf directly, i.e.</p>

<pre><code>forecast&lt;-stlf(tseries,h=10,method=""arima"",xreg=xreg,newxreg=newxreg)
</code></pre>

<p>Thanks in advance</p>
"
"0.0627455805138159","0.0846667513334603","21332","<p>I would like to use a set of weather-related historical data to fit a time series (let's say 1970-2000, Fourier terms plus ARIMA terms), but then use the fit on recent data (i.e., the last week/month of data) to forecast the upcoming day/week/month. All of the functions that I've found forecast from the endpoint of the dataset used to fit.</p>

<p>Can someone point me in the right direction? Or let me know it doesn't exist and I have to write it out the long way (i.e. <code>Tomorrow = fit$coef[1]*Yesterday + ...</code>)?</p>
"
"0.0627455805138159","0.0846667513334603","179378","<p>I was thinking, is it possible to implement a quarterly forecast for one year ahead such that its sum over year equals some constant number?</p>

<p>This problem may arise if we have, for example, some external forecast over next year, and we need to produce a quarterly forecast that is consistent with the yearly one.</p>

<p>Theoretically, I can write down an ML-maximization problem, and write then some stack of code. But is there maybe some existing solutions?</p>

<p>Thanks!</p>
"
"0.0443678254708057","0","22064","<p>Probably my question is a bit stupid, but I'm having some problems in writing down in R the out-of-sample forecasting with a Random Walk. I have a multivariate time series (y) and I want to estimate the out of sample forecasting (y(t+k)-y^(t+k/t) result with a RW, for k =1,6,12.
I write down this code, it works, but it seems to give me too much low errors.</p>

<pre><code>residuals1 &lt;- rep(0,58)
residuals6 &lt;- rep(0,58)
residuals12 &lt;- rep(0,58)

y1 &lt;- t(y[,1])
for (i in 1:58) {       
  residuals1[i] &lt;- y1[134+i+1]-y1[134+i]    
  residuals6[i] &lt;- y1[134+i+6]-y1[134+i] 
  residuals12[i] &lt;- y1[134+i+12]-y1[134+i] 
}
</code></pre>

<p>I think that I'm doing some mistakes, do you have any suggestions?</p>
"
"0.0627455805138159","0.0423333756667302","22156","<p>Iâ€™ve already written this question, but probably I didnâ€™t specified it well, for this reason I write it again.
I need to use a random walk model (no-change) yt = yt(1+t) to compute
the ratio of RMSFE. 
What I would like to do is:</p>

<ol>
<li>Fit the model to the data <code>yt,...,yt+kâˆ’1</code> and let <code>yË†t+k</code> be the forecast for the next observation.</li>
<li>Compute the forecast error as <code>et=yË†t+kâˆ’yt+k</code>.</li>
<li><p>Repeat for <code>t=1,...,nâˆ’k</code>""</p>

<pre><code>residuals1 &lt;- rep(0,58)
residuals6 &lt;- rep(0,58)
residuals12 &lt;- rep(0,58)

y1 &lt;- t(y[,1])
for (i in 1:58) {       
  residuals1[i] &lt;- y1[134+i+1]-y1[134+i]    
  residuals6[i] &lt;- y1[134+i+6]-y1[134+i] 
  residuals12[i] &lt;- y1[134+i+12]-y1[134+i] 
}
</code></pre></li>
</ol>

<p>Is it a correct way to compute the out.of sample forecasting errors or am I missing something?
I would appreciate any suggestions. Thanks!</p>
"
"0.0627455805138159","0.0846667513334603","129168","<p>I am relatively new to time series forecasting, I have worked previously with continuous data at regular intervals successfully, Now I have a data set with missing values, 
for example look at the below sample :</p>

<blockquote>
  <p>2012-01-13  4804<br>
    2012-01-14  58<br>
    2012-01-16  6031</p>
</blockquote>

<p>Here we see that 15th is missing out, I have many such missing values. Can I apply Holt Winters, Artificial Neural Networks or ARIMA on such data? If not how do I work with such data. Please advise</p>
"
"0.0627455805138159","0.0846667513334603","115271","<p>I have been given data to forecast however it has a negative figure within the data which then, when doing a log transformation to make the series stationary, the ARIMA script i have written won't work.</p>

<pre><code>datan&lt;-c(144627.7451,575166.2487,854245.7137,1230639.153,1160052.421,479928.7072,-261427.4238,1181746.229,168251.621,556741.5149,1840484.518,1704679.404,1878380.278,1865288.502,1849340.253,1965974.112,2093192.242,1912399.391,2633179.421,2134618.008,2070856.492,1238565.331)

freqdata&lt;-4
startdata&lt;-c(9,2)
horiz&lt;-4
datats&lt;-ts(datan,frequency=freqdata,start=startdata)
force.log&lt;-""log""
datadates&lt;-as.character(c(""9q2"",""9q3"",""9q4"",""10q1"",""10q2"",""10q3"",""10q4"",""11q1"",""11q2"",""11q3"",""11q4"",""12q1"",""12q2"",""12q3"",""12q4"",""13q1"",""13q2"",""13q3"",""13q4"",""14q1"",""14q2"",""14q3""))
dataMAT&lt;-matrix(0,ncol=freqdata,nrow=(length(datats)+freqdata),byrow=TRUE)
for (i in 1:freqdata)
  {dataMAT[,i]&lt;-c(rep(0,length=i-1),lag(datats,k=-i+1),rep(0,length=freqdata-i+1))}
dataind&lt;-dataMAT[c(-1:(-freqdata+1),-(length(dataMAT[,1])-freqdata+1):-(length(dataMAT[,1]))),]
dataind2&lt;-data.frame(dataind)
lm1&lt;-lm(X1~.,data=dataind2)
lm2&lt;-lm(X1~X2+dataind2[,length(dataind2[1,])],data=dataind2)
library(lmtest)
library(car)
bptest1&lt;-bptest(lm1)
bptest2&lt;-bptest(lm2)
gqtest1&lt;-gqtest(lm1)
ncvtest1&lt;-ncvTest(lm1)
ncvtest2&lt;-ncvTest(lm2)
if(force.log==""level"") 
  {aslog&lt;-""n""}else
    {{if(force.log==""log"")
       {aslog&lt;-""y""}else
         {if(bptest1$p.value&lt;0.1|bptest2$p.value&lt;0.1|gqtest1$p.value&lt;0.1|ncvtest1$p&lt;0.1|ncvtest2$p&lt;0.1)
           {aslog&lt;-""y""}else
              {aslog&lt;-""n""}}}}
if(aslog==""y"")
  {dataa&lt;-log(datats)}else
    {dataa&lt;-datats}
startLa&lt;-startdata[1]+trunc((1/freqdata)*(length(dataa)-horiz))
startLb&lt;-1+((1/freqdata)*(length(dataa)-horiz)-trunc((1/freqdata)*(length(dataa)-horiz)))*freqdata
startL&lt;-c(startLa,startLb)
K&lt;-ts(rep(dataa,length=length(dataa)-horiz),frequency=freqdata,start=startdata)
L&lt;-ts(dataa[-1:-(length(dataa)-horiz)],frequency=freqdata,start=startL)
library(strucchange)
efp1rc&lt;-efp(lm1,data=dataind2,type=""Rec-CUSUM"")
efp2rc&lt;-efp(lm2,data=dataind2,type=""Rec-CUSUM"")
efp1rm&lt;-efp(lm1,data=dataind2,type=""Rec-MOSUM"")
efp2rm&lt;-efp(lm2,data=dataind2,type=""Rec-MOSUM"")
plot(efp2rc)
lines(efp1rc$process,col =""darkblue"")
plot(efp2rm)
lines(efp1rm$process,col=""darkblue"")
gefp2&lt;-gefp(lm2,data=dataind2)
plot(gefp2)
plot(dataa)
pacf(dataa)
sctest(efp2rc)
cat(""log series,y/n?:"",aslog)
</code></pre>

<p>then i want to run arima to get the forecasts</p>

<pre><code>library(tseries)
library(forecast)
max.sdiff&lt;-3
arima.force.seasonality&lt;-""n""
kpssW&lt;-kpss.test(dataa,null=""Level"")
ppW&lt;-tryCatch({ppW&lt;-pp.test(dataa,alternative=""stationary"")},error=function(ppW){ppW&lt;-list(error=""TRUE"",p.value=0.99)})
adfW&lt;-adf.test(dataa,alternative=""stationary"",k=trunc((length(dataa)-1)^(1/3)))
if(kpssW$p.value&lt;0.05|ppW$p.value&gt;0.05|adfW$p.value&gt;0.05)
  {ndiffsW=1}else
    {ndiffsW=0}
aaW&lt;-auto.arima(dataa,max.D=max.sdiff,d=ndiffsW,seasonal=TRUE,allowdrift=FALSE,stepwise=FALSE,trace=TRUE,seasonal.test=""ch"")
orderWA&lt;-c(aaW$arma[1],aaW$arma[6],aaW$arma[2])
orderWS&lt;-c(aaW$arma[3],aaW$arma[7],aaW$arma[4])
if(sum(aaW$arma[1:2])==0)
  {orderWA[1]&lt;-1}else
    {NULL}
if(arima.force.seasonality==""y"")
  {if(sum(aaW$arma[3:4])==0)
    {orderWS[1]&lt;-1}else
      {NULL}}else
        {NULL}
Arimab&lt;-Arima(dataa,order=orderWA,seasonal=list(order=orderWS),method=""ML"")
fArimab&lt;-forecast(Arimab,h=8,simulate=TRUE,fan=TRUE)
if(aslog==""y"")
  {fArimabF&lt;-exp(fArimab$mean[1:horiz])}else
    {fArimabF&lt;-fArimab$mean[1:horiz]}
plot(fArimab,main=""ARIMA Forecast"",sub=""blue=fitted,red=actual"") 
lines(dataa,col=""red"",lwd=2) #changes colour and size of dataa
lines(ts(append(fitted(Arimab),fArimab$mean[1]),frequency=freqdata,start=startdata),col=""blue"",lwd=2)
if(aslog==""y"")
  {Arimab2f&lt;-exp(fArimab$mean[1:horiz])}else
    {Arimab2f&lt;-fArimab$mean[1:horiz]} 
start(fArimab$mean)-&gt;startARIMA
ArimaALTf&lt;-ts(prettyNum(Arimab2f,big.interval=3L,big.mark="",""),frequency=freqdata,start=startARIMA)
View(ArimaALTf,title=""ARIMA2 final forecast"") #brings up table of the forecasts
summary(Arimab)
</code></pre>

<p>If anyone can help me figure out how to forecast this data with the negative i will be really grateful!!</p>
"
"0.117386232408469","0.113140705550355","23245","<p>As a side hobby, I have been exploring forecasting time series (in particular, using R).</p>

<p>For my data, I have the number of visits per day, for every day going back almost 4 years. In this data there are some distinct patterns:</p>

<ol>
<li>Monday-Fri has a lot of visits (highest on Mon/Tue), but drastically less on Sat-Sun.</li>
<li>Certain times of the year drop (i.e. many less visits around U.S. Holidays, summers show less growth)</li>
<li>Significant growth year-to-year</li>
</ol>

<p>It would be nice to be able to forecast an upcoming year with this data, and also use it to have seasonally adjusted month-to-month growth. The main thing that throws me off with a monthly view is:</p>

<ul>
<li>Certain months will have more Mon/Tue than other months (and that isn't consistent over years either). Therefore a month that happens to more weekdays needs to be adjusted accordingly.</li>
</ul>

<p>Exploring weeks also seems difficult since the week numbering systems change from 52-53 depending on the year, and it seems <code>ts</code> doesn't handle that.</p>

<p>I'm pondering taking an average for the weekdays of the month, but the resulting unit is a bit strange (Growth in Avg. Weekday Visits) and that would be dropping data which is valid.</p>

<p>I feel this sort of data would be common in time series, (say for example electricity usage in office building might be something like this), anyone have any advice on how to model it, in particular, in R? </p>

<p>The data I am working with is pretty straight forward, it starts like:</p>

<pre><code>            [,1]
2008-10-05 17607
2008-10-06 36368
2008-10-07 40250
2008-10-08 39631
2008-10-09 40870
2008-10-10 35706
2008-10-11 18245
2008-10-12 23528
2008-10-13 48077
2008-10-14 48500
2008-10-15 49017
2008-10-16 50733
2008-10-17 46909
2008-10-18 22467
</code></pre>

<p>and continues like this up to the present, with an overall trend of growth, some dips around US holiday weeks, and growth generally slowing during the summer.</p>
"
"0.0887356509416114","0.11973686801785","185608","<p>Hi I have a problem with my project. I have 1000 data and I used it as training data to forecast with Timeseries. But my data have some subsets and it's impossible to timeseries forecasting and modelling in R . I already searched from the Internet and asked some experts that R doesn't support Timeseries Model and Forecasting for subsets. But one of the requirement for my project, I must use R as statistic tool. </p>

<p>Do you have any suggestion or R script which will support Timeseries model and forecasting for subsets in R?</p>
"
"0.153694655873568","0.172825282455071","123723","<p>Can anyone tell me the formula behind the <code>forecast</code> function in R? Preferably in the form easily understood by mathematicians (e.g  x_t, Î¸ etc)</p>

<p>Here is my code in case it helps</p>

<pre><code>suppressMessages(library(lmtest))
suppressMessages(library(car))
suppressMessages(library(tseries))
suppressMessages(library(forecast))
suppressMessages(library(TTR))
suppressMessages(library(geoR))
suppressMessages(library(MASS))
suppressMessages(library(gtools))
#-------------------------------------------------------------------------------
Model &lt;- ""choosing ARIMA""
Series.title &lt;- ""EMEA GAM&lt;250K""
#-------------------------------------------------------------------------------
Input.data &lt;- matrix(c(""08Q1"",""08Q2"",""08Q3"",""08Q4"",""09Q1"",""09Q2"",""09Q3"",""09Q4"",""10Q1"",""10Q2"",""10Q3"",""10Q4"",""11Q1"",""11Q2"",""11Q3"",""11Q4"",""12Q1"",""12Q2"",""12Q3"",""12Q4"",""13Q1"",""13Q2"",""13Q3"",""13Q4"",""14Q1"",""14Q2"",""14Q3"",5403.675741,6773.504993,7231.117289,7835.55156,5236.709983,5526.619467,6555.781711,11464.72728,7210.068674,7501.610403,8670.903486,10872.93518,8209.022658,8153.393088,10196.44775,13244.50201,8356.732878,10188.44157,10601.32205,12617.82102,11786.52641,10044.98676,11006.0051,15101.9456,10992.27282,11421.18922,10731.31198),ncol=2,byrow=FALSE)

#-------------------------------------------------------------------------------
# The frequency of the data. 1/4 for QUARTERLY, 1/12 for MONTHLY

Frequency &lt;- 1/4

#-------------------------------------------------------------------------------
# How many quarters/months to forecast

Forecast.horizon &lt;- 4

#-------------------------------------------------------------------------------
# The first date in the series. Use c(8, 1) to denote 2008 q1

Start.date &lt;- c(8, 1)

#-------------------------------------------------------------------------------
# The dates of the forecasts

Forecast.dates &lt;- c(""14Q4"", ""15Q1"", ""15Q2"", ""15Q3"")

#-------------------------------------------------------------------------------
# Selects the data column from Input.data

Data.col &lt;- as.numeric(Input.data[, 2])

#-------------------------------------------------------------------------------
# Turns the Data.col into a time-series

Data.col.ts &lt;- ts(Data.col, deltat=Frequency, start = Start.date)

#-------------------------------------------------------------------------------
# A character vector of the dates from Input.data

Dates.col &lt;- as.character(Input.data[,1])

#------- Transform ------------------------------------------------------------------------
# Starts the testing to see if the data should be logged

transform.method &lt;- round(BoxCox.lambda(Data.col.ts, method = ""loglik""), 5)

log.values &lt;- seq(0, 0.24999, by = 0.00001)
sqrt.values &lt;- seq(0.25, 0.74999, by = 0.00001)

which.transform.log &lt;- transform.method %in% log.values
which.transform.sqrt &lt;- transform.method %in% sqrt.values

if (which.transform.log == ""TRUE""){
  as.log &lt;- ""log""
  Data.new &lt;- log(Data.col.ts)
} else {
  if (which.transform.sqrt == ""TRUE""){
    as.log &lt;- ""sqrt""
    Data.new &lt;- sqrt(Data.col.ts)
  } else {
    as.log &lt;- ""no""
    Data.new &lt;- Data.col.ts
  }
}

#----- Find best ARIMA model ---------------------------------------------------

a &lt;- permutations(n = 3, r = 6, v = c(0:2), repeats.allowed = TRUE)
a &lt;- a[ifelse((a[, 1] + a[, 4] &gt; 2 | a[, 2] + a[, 5] &gt; 2 | a[, 3] + a[, 6] &gt; 2),
              FALSE, TRUE), ]

Arimafit &lt;- matrix(0,
                   ncol  = length(Data.new),
                   nrow  = length(a[, 1]),
                   byrow = TRUE)

totb &lt;- matrix(0, ncol = 1, nrow = length(a[, 1]))
arimaerror &lt;- matrix(0, ncol = length(Data.new), nrow = 1)

for (i in 1:length(a[, 1])){
  ArimaData.new &lt;- try(Arima(Data.new,
                             order    = a[i, c(1:3)],
                             seasonal = list(order = a[i, c(4:6)]),
                             method   = ""ML""),
                       silent = TRUE)

  if (is(ArimaData.new, ""try-error"")){
    ArimaData.new &lt;- arimaerror
  } else {
    ArimaData.new &lt;- ArimaData.new
  }

  arimafitted &lt;- try(fitted(ArimaData.new), silent = TRUE)

  if (is(arimafitted, ""try-error"")){
    fitarima &lt;- arimaerror
  } else {
    fitarima &lt;- arimafitted
  }

  if (as.log == ""log""){
    Arimafit[i, ] &lt;- c(exp(fitarima))
    Datanew &lt;- c(exp(Data.new))
  } else {
    if (as.log == ""sqrt""){
      Arimafit[i, ] &lt;- c((fitarima)^2)
      Datanew &lt;- c((Data.new)^2)
    } else {
      Arimafit[i, ] &lt;- c(fitarima)
      Datanew &lt;- c(Data.new)
    }
  }

  data &lt;- c(Datanew)

  arima.fits &lt;- c(Arimafit[i, ])

  fullres &lt;- data - arima.fits

  v &lt;- acf(fullres, plot = FALSE)

  w &lt;- pacf(fullres, plot = FALSE)

  if (v$acf[2]&gt;0.4|v$acf[2]&lt;(-0.4)|v$acf[3]&gt;0.4|v$acf[3]&lt;(-0.4)|v$acf[4]&gt;0.4|v$acf[4]&lt;(-0.4)|v$acf[5]&gt;0.4|v$acf[5]&lt;(-0.4)|v$acf[6]&gt;0.4|v$acf[6]&lt;(-0.4)|v$acf[7]&gt;0.4|v$acf[7]&lt;(-0.4)|w$acf[1]&gt;0.4|w$acf[1]&lt;(-0.4)|w$acf[2]&gt;0.4|w$acf[2]&lt;(-0.4)|w$acf[3]&gt;0.4|w$acf[3]&lt;(-0.4)|w$acf[4]&gt;0.4|w$acf[4]&lt;(-0.4)|w$acf[5]&gt;0.4|w$acf[5]&lt;(-0.4)|w$acf[6]&gt;0.4|w$acf[6]&lt;(-0.4)){
    totb[i] &lt;- ""n""
  } else {
    totb[i] &lt;- sum(abs(w$acf[1:4]))
  }

  j &lt;- match(min(totb), totb)

  order.arima &lt;- a[j, c(1:3)]

  order.seasonal.arima &lt;- a[j, c(4:6)]
}

#----- ARIMA -------------------------------------------------------------------
# Fits an ARIMA model with the orders set
Arima.Data.new &lt;- Arima(Data.new,
                        order    = order.arima,
                        seasonal = list(order=order.seasonal.arima),
                        method   = ""ML"")

#-------------------------------------------------------------------------------
# Forecasts from the ARIMA model

suppressWarnings(forecast.Data.new &lt;- forecast(Arima.Data.new,
                                               h        = ifelse(frequency(Arima.Data.new) &gt; 1, 2 * frequency(Arima.Data.new), 10),
                                               simulate = TRUE,
                                               fan      = TRUE))
</code></pre>
"
"0.0443678254708057","0.059868434008925","123887","<p>I have 852 days of daily attendance data and need to use the first 800 days data to predict the next 52 days and match it with my actual values. How do i decide which is the best model to use for forecasting? will it capture seasonality and trend? What should be the values of input parameter?</p>"
"NaN","NaN","","<r><time-series><forecasting>"
"0.204091997165706","0.251447422837485","124707","<p>Sorry for the rather long introduction, but since I was (legitimately) critizised for not explaining my cause and questions enough, I will do so now. </p>

<p>I would like to conduct a <strong><em>(price)-forecast</em></strong> based on a multiple time series VAR-Model (vector autoregressive Model) with multiple endogeneous variables and two exogeneous. Since I am not that skillfull with regards to neither statistics nor R I want to keep is as simple as possible (Trend forecast for 3 months is sufficient).</p>

<p>I am using the ""vars"" - Package, <a href=""http://cran.r-project.org/web/packages/vars/vars.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/vars/vars.pdf</a> and all in all those four functions: decompose(), VARselect(), VAR(), and predict()</p>

<p>I have 1 dependent time series (y, in my model referred to as ""RH"", or ""raRH""), 4-5 endogeneous predictors and 2 exogeneous predictors.
All timeseries have a length of 1-91 observations and are monthly data without any gaps.</p>

<p><strong><em>Data description:</em></strong>
My y (dependent var) are sawlog prices, sawlogs are raw material for plenty of follow up products.<br> My endogeneous (since they all kind of correlate with each other and y) are follow up product-prices or further elaborated sawlogs. <br>My 2 exogeneous predictors are economic indicators similar to BIP etc.</p>

<p>All the time series are <em>non-stationary</em>, since I have read that you should use stationary data in order to gain a valid VAR-Model, I used the decompose() - function in R to split each variable into trend, season and the randwom walk. </p>

<pre><code> raKVH&lt;-decompose(KVH)$random
raKVH&lt;-na.omit(raKVH)
raSNS&lt;-decompose(SNP_S)$random
raSNS&lt;-na.omit(raSNS)
</code></pre>

<p>... and so on for every variable.<br><br>
What I'm interested now in order to do some forecasting are predictions of the randwom walk (right?!). Anyways, I found out that all my data is first-order-integrated, since taking the logarithm makes them all stationary timeseries (ts), tested via Dickey-Fuller-Test. </p>

<p>The picture also provides data example, first picture shows the raw-data, <img src=""http://i.stack.imgur.com/BcMOT.png"" alt=""enter image description here""></p>

<p>second picture the random walks gained by decomposing$random the raw-data. 
<img src=""http://i.stack.imgur.com/96yii.png"" alt=""enter image description here""></p>

<p>I used the command VARselect that automatically computes the optimal lag for my model, whereas tsall is my time-series matrix containing all the timeseries mentioned above.</p>

<pre><code>VARselect(tsall)
</code></pre>

<p>proceeding now with the estimation of the model VAR(p=number of lags given by VARselect), <strong><em>I encountered the following problem</em></strong>: how should I use the attribute ""type"" within the VAR-function? What does ""trend"",""none"", ""const"", ""both"" exactly mean? Since I have stationary data, there won't be any trend right? How can I check if there is a constant? Since the default value is ""const"", I chose to go with that.
<br><br>
<strong><em>The main question I have is the following:</em></strong><br>
How do I get ""real"" forecasts out of the prediction of the randwom walks anyways? If I want to predict the price of yt+3, I need more than the prediction of the random walks here, I need ""real figures"" like in graphic 1. How can I ""add back"" trend and season?</p>

<p>Third picture shows the Forecast of the random walk of my ""target Variable"" Y, but what's the next step here? 
<img src=""http://i.stack.imgur.com/ZtInk.png"" alt=""enter image description here""></p>

<p>Thank you for any help, if my questions/introduction are insufficient, please let me know. I'll try to explain myself better then.</p>
"
"0.117386232408469","0.113140705550355","185755","<p>I am trying to predict the proportion of due accounts type on a given day. </p>

<p>To elaborate a little, everyday I will have a list containing all the past due accounts on that day and the number of days its past due. (i.e. everyday I will have a bar-chart type of data with x-axis = account due days (from 1 to 60) and y-axis = proportion of today's accounts)</p>

<p>My goal is to predict the chart (or the proportions of the accounts) using historical data. Since this is not a single time series, I suppose I need to use a group/hierarchical time series analysis.</p>

<p>However, there's not too many examples online on how to forecast this type of data. The only useful package in R I found is hts or gts from package <a href=""https://cran.r-project.org/web/packages/hts/index.html"" rel=""nofollow"">hts</a>, but I am not familiar with it and aren't sure how to setup the data to fit the package.</p>

<p>I would imagine the way to fit the data should be something like this:</p>

<p><img src=""http://holland.pk/uptow/i4/a7bff32d16084050af1a805e1d17638d.jpg"" alt=""model""></p>

<p>Since I am new to time-series analysis and forecasting, I am hoping someone can provide some insights on forecasting on such data? and if possible, could you provide a general flow to check and run for forecasting?</p>

<p>Thanks!</p>
"
"0.0627455805138159","0.0423333756667302","124911","<p>I'm working on a small independent project in R, trying to make my own (very crude) forecasting method. The general idea of the component that is giving me trouble is trying to compare two windows of a given length of a single indicator over time. I want to find a way to go back in the history of the indicator and find a window ""most similar"" to the current window in terms of behavior relative to the time. I've looked at a few textbooks for normalization methods so I could run more simple comparisons, but I couldn't find enough to be able to actually code it up.</p>
"
"0.0627455805138159","0.0846667513334603","40697","<p>The question title doesn't help, but i'll try to explain it:</p>

<p>I have historical data (weekly points) from 2001/01/01 to yesterday and
my variable has a strong seasonal pattern of 12 months (weather data).</p>

<p>I'm using the stlf() function (from the R 'forecast' package) to make weekly forecasts - and getting very good results by the way.</p>

<p>The problem is that for some models i don't have the 2 last years of data, so for these models my data goes from 2001/01/01 to 2010/12/31. What can i do to bring these forecasts 'to the future'? In other words, i need to extrapolate these forecasts to the current week.</p>

<p>What i have: data from 2001/01/01 - 2010/12/31
What i need: make forecasts for the next weeks.</p>

<p>To add confusion, i do have some points from the last month, but i don't know how to take advantage of this.</p>

<p>Thanks for any help!</p>
"
"0.108678533400333","0.146647115021353","124862","<p>Cross-posting this from Stack Overflow, because it's a bit of a stats/ technology cross-over.</p>

<p>I'm relatively new to R and the forecast package I believe authored by Rob Hyndman.</p>

<p>I'm having trouble understanding how the objects (time series, model, forecast) exactly relate to each other, but more importantly, the proper arguments for the forecast() function.</p>

<p>Say I have a time-series object called Sales.ts</p>

<p>Now, I wanted to verify that I understood the forecast() function -- which can accept both raw time series, or models based on time series, or both, as inputs.</p>

<p>First, I did Sales.ets&lt;-ets(Sales.ts). Here the ets() function chose a best-fit ets model that estimates the Sales.ts time series, now called Sales.ets.</p>

<p>Now I do forecast(Sales.ets,h=12) to predict the next 12 values in the future, based on the ets model.</p>

<p>I can also do forecast(Sales.ts,model=Sales.ets,h=12). I wanted to check that if it forecasted the Sales.ts time series using the same ets model, it should produce the same results as the first method. MAINLY, because I want to validate partitions of the data using the Sales.ets model.</p>

<p>HOWEVER, here's the problem:</p>

<p>forecast(Sales.ets, h=12) and forecast(Sales.ts, model=Sales.ets,h=12) produce slightly, but signficantly enough, different forecasts. My question is --- why? Why does it do this?</p>

<p>My follow up question would be how to validate the Sales.ets model. I was going to try to validate it by doing (Sales.ts(1:k),model=Sales.ets,h=1) to check the accuracy of 1-step forecasts at each point in history in the past. Any help appreciated - thanks!</p>
"
"0.0627455805138159","0.0846667513334603","40749","<p>I have a linear model (with seasonal dummy variables) that produces monthly
forecasts. I'm using R together with the 'forecast' package:</p>

<pre><code>require(forecast)
model = tslm(waterflow ~ rainfall + season, data = model.df, lambda = lambda)
forec = forecast(model, newdata = rainfall.df, lambda = lambda)
</code></pre>

<p>I did a cross-validation and it looks great. Now, what i need is to generate
<em>weekly data points</em> from these month forecasts - in other words, i need to generate a synthetic time-series that have monthly means equal to the forecasts above. So my function would look like:</p>

<pre><code>generate.data = function(monthly.means, start.date, end.date)
{
   #code here
}
</code></pre>

<p>I'm not sure how to do this (interpolation?), so any help is welcome.
Thanks!</p>
"
"0.108678533400333","0.146647115021353","185932","<p>I'm trying to forecast sales time-series data across a product line (in R).  I can identify groups of products that have very similar seasonality profiles that I would like to apply to others in a couple of situations:</p>

<ul>
<li>Forecasting new items that have no history.</li>
<li>Forecasting Individual products where I want to use an aggregate of multiple products to smooth out the seasonality.</li>
</ul>

<p>In the past I have typically used something like a classic multiplicative decomposition and applied that to a subjective number of recent periods for the item I'm trying to forecast to determine a deseasonalized trend which I can then apply a model like exponential smoothing.</p>

<p>I would like to use STL to identify the seasonal components (allowing for seasonality shifts over time).  But I can't figure out how to scale either the seasonal component from STL or my target product to combine them.</p>

<p>Is there a way to do this, or should I just use simple multiplicative decomposition?  FYI, the data I'm currently working has very high seasonality, but I'm not sure that matters.</p>
"
"NaN","NaN","185962","<p>I would like to model a time series with a GARCH(1,1), but with an additive dummy as in the equation below:
\begin{eqnarray*}
h_t = (1+ \gamma D_t)(\alpha_0 + \alpha_{1} \epsilon_{t-1}^2 + \alpha_{2}h_{t-1})
\end{eqnarray*}
where $D_t$ is the dummy.</p>

<p>However, my wish is to keep using <a href=""/questions/tagged/r"" class=""post-tag"" title=""show questions tagged &#39;r&#39;"" rel=""tag"">r</a> <a href=""/questions/tagged/rugarch"" class=""post-tag"" title=""show questions tagged &#39;rugarch&#39;"" rel=""tag"">rugarch</a>. Is it possible to do this?</p>
"
"0.0627455805138159","0.0846667513334603","186190","<p>I am trying to make a prediction of imbalance prices in the elctricity market. My dataset consists of data for every 15 minutes (this is the time period in which a price is determined) during 11 months. I have several exogenous factors (like the spot market price) included mentioned here as x1 etc.</p>

<p>In forecasting the price I am using the following code: </p>

<pre><code>lag &lt;- function(x, k){c(rep(NA, k), x)[1 : length(x)]}
mydata$y_lag1 &lt;- lag(mydata$y, 1)
mydata$y_lag2 &lt;- lag(mydata$y, 2)
mydata$x1_lag1 &lt;- lag(mydata$x1, 1)
mydata$x2_lag1 &lt;- lag(mydata$x2, 1)
mydata$x3_lag1 &lt;- lag(mydata$x3, 1

f&lt;- y ~ y_lag1 + y_lag2 + x1_lag1 + x2_lag1 + x3_lag1
fit &lt;- lm(formula = f, data = mydata)
mydata$P_imb_pred &lt;- predict(fit, newdata = mydata)

pred &lt;- data.frame(time=mydata$time, price=mydata$P_imb_pred)
</code></pre>

<p>My code works, but I am unsure if it does wat I want it to. I am trying to predict the price only 1 time unit (so 15 minutes) ahead. That's why I have lagged variables in the function. Can someone help me out? Should I additionally specify how much time ahead I want to forecast? If so, can you tell me how?</p>

<p>Thanks for your help! </p>
"
"0.0627455805138159","0.0846667513334603","23746","<p>I have forecasts and actuals for panel data (i.e. time-series cross-sectional data). The forecasts are already generated and provided by some source outside of R. I'd like to evaluate the quality of the forecasts.</p>

<p>Are there standard tools in R that perform various diagnostics on the residuals? By diagnostics I mean tests such as: </p>

<ul>
<li>auto-correlation of residuals across the cross-section</li>
<li>auto-correlation of residuals along the time series for a given member</li>
<li>tests for fixed effects vs. random effects</li>
<li>heteroskedasticity, etc.</li>
</ul>

<p>Or is the best way to perform these diagnostics to perhaps build a panel model using the forecast as the predictor in the panel model?</p>
"
"0.108678533400333","0.146647115021353","199264","<p>I have time-series power consumption data for one month. The data is sampled at minutes frequency. Thus, for each day I have 1440 observations and for the month (30 days) I have 4320 observations. On the same data, I use <code>stl()</code> function to observe seasonality, trend, etc. I have following doubts:</p>

<ol>
<li>What should be the frequency value while creating the <code>ts</code> (timeSeries) object? Is it 1440 (no. of observations per day) or it is 30 (no. of days for which observations are recorded)</li>
<li>In the <code>stl</code> function, I need to provide values for <code>s.window</code>, <code>t.window</code>. Should I use the value of frequency for <code>s.window</code>? I am clueless about trend window.</li>
</ol>

<p>I am following <a href=""https://www.otexts.org/fpp/6/1"" rel=""nofollow"">this</a> page to understand these concepts.</p>

<p><strong>UPDATE</strong></p>

<ol>
<li>My Ist doubt got cleared while reading this <a href=""http://robjhyndman.com/hyndsight/seasonal-periods/"" rel=""nofollow"">page</a>, where Dr. Rob mentions that frequency is the number of observations per season. Thus, the answer to my first doubt is 1440 as season is represented by each day in my data.</li>
</ol>
"
"NaN","NaN","44072","<p>I am using the <code>forecast</code> package in R. I wanted to know how the <code>ets()</code> function finds the value of $\alpha$, $\beta$ and $\gamma$? </p>

<p>Is it by minimizing the SSE or some other criteria?</p>
"
"0.243139124491036","0.275166941833746","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.0627455805138159","0.0846667513334603","81632","<p>i want to use an ARIMA model in R for predicting an electrical load on a minutely basis. By examining the ACF I figured out which model could suit. The ACF has shown that the value one day ahead has a periodic autocorrelation. Therefore I'd like to implement a seasonal difference with a lag of 1440 (min/day).</p>

<p>Thus, I found this page (<a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) describing how to deal with long seasonal periods in R.</p>

<p>However, by applying that method, I experienced the following problem in R:</p>

<pre><code>&gt;Arima(x,order=c(2,0,2),xreg=fourier1(1:length(x),4,1440))

Error in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg,  : 
lengths of 'x' and 'xreg' do not match
</code></pre>

<p>x is the dataset as a zoo-Object with the following structure (it's just an example, I do not have access to the real structure at the moment; main difference: much more data!):</p>

<pre><code>&gt; str(x)
â€˜zooâ€™ series from 2010-01-01 00:00:00 to 2010-01-01 00:06:00
Data: num [1:7] 1 2 3 4 5 6 7
Index:  chr [1:7] ""2010-01-01 00:00:00"" ""2010-01-01 00:01:00"" ...
</code></pre>

<p>Since the number of rows in xreg should be exactly the same as in x, they are apparently not.</p>

<p>Does anyone has any suggestions about or experiecend this?</p>

<p>I'll appreciate any hints!</p>

<p>Marc</p>
"
"0.076847327936784","0.0691301129820284","205967","<p>Suppose I have a website which has some baseline hourly traffic. I also run TV advertising intermittently which drives up my web traffic. I want to determine how much effect my TV advertising is having in terms of driving up web traffic.</p>

<p>If I fit an ARMAX model with hourly TV advertising spend or impressions as exogenous variables, is it valid to claim that the AR terms represent the ""baseline traffic"" while the regression terms represent the traffic that should be attributed to TV advertising?</p>

<p>Here is some example code of what I'm trying to do:</p>

<pre><code>library(forecast)

xmat &lt;- as.matrix(cbind(data[,c(""AdSpend"",""Impressions"")]))
xvar &lt;- data$WebSessions

fit &lt;- Arima(x=xvar, xreg=xmat, order=c(12,0,0), include.constant=FALSE)

reg_terms &lt;- fit$coef[""AdSpend""] * data$AdSpend + fit$coef[""Impressions""] * data$Impressions
AR_terms &lt;- fitted(fit) - reg_terms
</code></pre>

<p>I can then create a stacked area chart using AR_terms (the baseline hourly web traffic) and reg_terms (the TV attributed hourly traffic).</p>

<p><a href=""http://i.stack.imgur.com/PjaLr.png""><img src=""http://i.stack.imgur.com/PjaLr.png"" alt=""enter image description here""></a></p>

<p>Is this a valid approach?</p>

<p>Thanks for the help.</p>
"
"0.0627455805138159","0.0846667513334603","187030","<p>I have a data set of 6 time series variables, $y$ and $x_1,\dotsc,x_5$ for the past 15 months (15 $\times$ 6 = 90 items). </p>

<p>I want to use a statistical model to predict the next 5 months of $y$ as function of $y$ and $x_1,\dotsc,x_5$, </p>

<p>$$ y = f(y,x_1,x_2,x_3,x_4,x_5) $$</p>

<p>Question: what model to use? I can think of time series cross sectional model; is it correct?</p>
"
"0.0627455805138159","0.0846667513334603","59936","<p>Just wondering how to setup xreg variables for ARIMAX models? I am particularly interested in whether I should be grouping together events for dummy variables. For example should I create 1 dummy variable and store all public holidays or should I create a dummy variable for each public holiday (this could lead to many dummy variables). BTW - I am using R and the Forecast package.</p>"
"NaN","NaN","","<r><time-series><forecasting>"
"0.0627455805138159","0.0846667513334603","181533","<p>I have a time series (quarterly data) that I will use to predict the upcoming 4 quarters.The total number of observations is 20 quarters, thus, I need to predict quarter 21 -> 24.
First I took the diff(data) to have a stationary data and I want to fit AR(1). I am using the following in R:</p>

<p><code>arima(diff(data), order=c(1,0,0))</code> and I obtained: ar1 (- 0.2441) and Intercept (1.2004) </p>

<p>Is the following correct?</p>

<pre><code>âˆ†y(t+1) = 1.2004 - 0.2441*âˆ†y(t)

âˆ†y(t+2) = 1.2004 - 0.2441*âˆ†y(t+1)
</code></pre>

<p>If I want to predict y(t+1), do I find âˆ†y(t+1) and then</p>

<pre><code>y(t+1) = y(t) + âˆ†y(t+1)

y(t+2) = y(t) + âˆ†y(t+1)+ âˆ†y(t+2)
</code></pre>

<p>and so on... until y(t+4)</p>

<p>Is this analogy correct? Do you know how can I get the predicted value in R without doing it manually? How can I tell R that first I need to predict the delta and then the original value.</p>
"
"0.126273044984921","0.132524427990982","145193","<p>I'm trying to model the responses from a direct mail marketing campaign so that I can use it to forecast for future campaigns. In the code below, I started with the average number of responses by day of a historical campaign (contained in the vector: ""responses""). I was then able to fit a 63-day (8-wk) smooth curve to model the data. But I now need a way to use this curve to help me with forecasting. For example, if I think I'll get x number of total responses from a campaign, I need to know when those responses are most likely to happen. In other words, I need the daily ""factors"" (i.e. the percentage of the total responses that is most likely to respond on each day).  Thanks!</p>

<p>p.s. if anyone has a better way of approaching this I'd love to hear!</p>

<pre><code>#vector of direct mail marketing responses over 63 days 
responses &lt;- c(
24.16093706,
41.59607507,
68.20083052,
85.19109064,
100.0704403,
58.6600221,
86.08475816,
88.97439581,
65.58341418,
49.25588053,
53.63602085,
47.03620672,
29.71552264,
32.85862747,
31.29118096,
23.67961069,
19.81261675,
18.69300933,
17.25738435,
12.01161679,
12.36734071,
14.32360673,
11.02390849,
9.108021409,
9.647965622,
8.815576548,
5.67225654,
5.739220185,
6.233999138,
5.527376627,
5.024065761,
5.565266355,
4.626749364,
3.480761716,
4.621902301,
4.518554271,
4.075985188,
3.204946787,
3.174020873,
2.966915873,
2.129178828,
2.673009031,
2.410429043,
2.331287075,
2.509300578,
2.13820695,
2.53433787,
1.603934405,
1.555813592,
1.834605068,
1.842905685,
1.454045577,
2.08684322,
1.318276487,
0.807666643,
1.333167088,
1.004526525,
1.180110123,
1.078079735,
1.151394678,
1.426747942,
0.699119833,
0.583347236)


set.seed(2)
install.packages(""MASS"")
library(""MASS"")


shape_and_scale &lt;- fitdistr(responses,'weibull')

#check the shape and scale
shape_and_scale

#plug in the shape and scale
#essentially taking the total number of respondants and for each, doing a random simulation for what day they'll respond- according to a weibull distribution
#rweibull makes it a random generation
#also need to create a variable for the total number of responses
total_responses &lt;- 1121
day_response &lt;- round(rweibull(total_responses,0.70730466,13.79467490)+.5)

day_response

day_response_frequency_table &lt;- as.data.frame(table(round(rweibull(total_responses,0.70730466,13.79467490)+.5)))

day_response_frequency_table
#notice that it extends beyond our 63 day limit for modeling a campaign

#create a factor with levels so that we can limit our distribution to 63 days
day_response_with_levels &lt;- factor(day_response, levels=0:63)
day_response_with_levels
response_frequency &lt;- as.data.frame(table(day_response_with_levels))
response_frequency

#now use dweibull and the curve() function to create a curve
?dweibull 
curve(x*dweibull(x,0.70730466,13.79467490),from=0, to=63)
</code></pre>
"
"0.117386232408469","0.0905125644402841","60804","<p>The MARSS package in R offers function for dynamic factor analysis. In this package, the dynamic factor model is written as a special form of state space model and they assume the common trends follow AR(1) process. As I am not very familiar with those two methods, I come with two questions:</p>

<p>Is the Dynamic Factor Analysis a special form of State Space Model? What is the difference between those two methods? </p>

<p>In addition, the Dynamic Factor Analysis does not necessary assume the common trends as AR(1) process. Is there any package that allows the the common trends as seasonal ARIMA (or some other) process?</p>
"
"0.154151401771656","0.17600563227035","60648","<p>I am trying to forecast electricity consumption in GWh for 2 years ahead (from June 2013 ahead), using R (the forecast package). For that purpose, I tried regression with ARIMA errors. I fitted the model using the <code>auto.arima</code> function, and I used the following variables in the <code>xreg</code> argument in the <code>forecast.Arima</code> function: </p>

<p>- Heating and Cooling Degree Days,<br>
- Dummies for all 12 months and<br>
- Moving holidays dummies (Easter and Ramadan)  </p>

<p>I have several questions regarding the model:</p>

<p>1) Is it correct to use all 12 dummies for monthly seasonality, since when I tried to include 11, the function returned error. The <code>Auto.arima</code> function returned the model ARIMA(0,1,2)</p>

<p>2)The model returned the following coefficients (I won't specify all of them as there are too many coefficients):</p>

<pre><code>ma1      ma2     HDD     CDD   January  February  March     April
-0.52 -0.16      0.27    0.12  525.84   475.13    472.57    399.01
</code></pre>

<p>I am trying to determine the influence of the temperature component over electricity load. In percentages, (interpreting the coefficients just as with the usual regression) the temperature components (<code>HDD</code>+<code>CDD</code>) account for 11,3% of the electricity consumption. Isn't this too little, considering the fact that the electricity consumption is mostly influenced by the weather component? On the other hand, taking look at the dummies' coefficients, it turns out that the seasonality accounts for the greater part of the load. Why is this? Is the model completely incorrect?</p>

<p>I tried linear regression, and the temperature component accounts for 20%, but it is still a low percentage. Why is this?</p>

<p>3) I am obviously making some mistakes in the use of <code>forecast.Arima</code> or the plot function parameters since when I plot the forecasts, I get a picture of the original time series which is continued (merged) with the forecasts for the whole time series period (from 2004 until 2015). I don't know how to explain this better, I tried to paste the picture, but it seems I cannot paste pictures here.</p>
"
"0.0887356509416114","0.11973686801785","109826","<p>I am trying to quantify the effect of a future random shocks on my seasonal ARIMA model. If I have understood the theory correctly, the easiest way is to express my seasonal ARIMA model in its ""random shock"" form, and calculate the corresponding psi weights.</p>

<p>Is there a way to do this in R? There is ARMAtoMA, but I think this only works for ARMA models, and not seasonal ARIMA models.</p>

<p>Thank you for your help.</p>

<hr>

<p>UPDATE: Apologies, I'll post the question about R onto stack overflow. It would be good to get confirmation that this is the correct method to quantify the effect of future random shocks to a seasonal ARIMA model.</p>
"
"NaN","NaN","25316","<p>How would we measure the predictive power of predictors in time series models. For e.g. in linear regression we have the magnitude and direction of the regression co-efficients and their p-values.</p>"
"NaN","NaN","<p>Is there any measure like that to evaluate the performance of predictors in kalman filter?</p>",""
"NaN","NaN","","<r><regression><time-series><forecasting><kalman-filter>"
"0.0887356509416114","0.0898026510133874","25203","<p>I have a time series that contains double seasonal components and I would like to decompose the series into the following time series components (trend, seasonal component 1, seasonal component 2 and irregular component). As far as I know, the STL procedure for decomposing a series in R only allows one seasonal component, so I have tried decomposing the series twice. First, by setting the frequency to be the first seasonal component using the following code:</p>

<pre><code>ser = ts(data, freq=48)
dec_1 = stl(ser, s.window=""per"")
</code></pre>

<p>Then, I decomposed the irregular component of the decomposed series (<code>dec_1</code>) by setting the frequency to be the second seasonal component, such that:</p>

<pre><code>ser2 = ts(dec_1$time.series[,3], freq=336)
dec_2 = stl(ser2, s.window=""per"")
</code></pre>

<p>I'm not very confident with this approach. And I would like to know if there are any other ways to decompose a series that has multiple seasonalities. Also,I have noticed that the <code>tbats()</code> function in the R <a href=""http://cran.r-project.org/web/packages/forecast/index.html"">forecast</a> package allows one to fit a model to a series with multiple seasonalities however, it doesn't say how to decompose a series with it.</p>
"
"0.0992094737665681","0.107095910520333","103775","<p>For work, I'm working on an app where you essentially forecast the failure rate of the overall machine through different factors such as the historical failure rates for the components used to build it or the failure rates of the factories that manufacture it, or even the historical rate for the machine itself. The idea is that for any machine you can make a solid prediction, so I need some algorithm to self-build a good model for each of the 1000s of machines.</p>

<p>I've been able to implement this using ARIMAX models, but I just don't feel good about using auto.arima and then just cross-validating to see how many external regressors to add in. I've also tried SVM, but what seemed to happen was that the model was not good at dropping irrelevant factors, and therefore the prediction was a flat line.</p>

<p>I feel like boosting would be a promising area, but I was wondering if anyone had other options and could more importantly, point me to examples of how the specific algorithm was implemented in R? I'm actually an undergrad intern majoring in statistics, so I'm not too strong in the actual programming side of things, so am not very good at implementing the theory I read about into R code.</p>

<p>Also, would a normal GLM be good enough? I used ARIMAX because I wanted to correct for autocorrelation.</p>
"
"0.0992094737665681","0.107095910520333","25230","<p>I am presently trying to learn R.  I would like to be able to apply it more in my work environment as I am an analyst in the Health Care industry.  I am presently trying to use R to forecast.  What is the best forecasting package in R?  </p>

<p>I am presently using the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"" rel=""nofollow"">forecast</a> package.  I have tried to fit the <code>ets</code> models to my data but I feel that it is giving me some fairly unreasonable solutions.  The data is flat, meaning that it does not linearly increase and there are some fluctuations, but I have not been able to assess whether or not those fluctuations are seasonal.  I am assuming they are not. </p>

<p>How can I calculate the out of sample error when I am comparing forecasting models?  Also, is there a way to plot my forecasted data against the actual values?  Lastly, how can I determine the model that is generated from the forecast?  </p>

<p>Thanks for all of your help in advance.</p>
"
"0.193637065371959","0.209029813778565","168655","<p>I have got monthly data from 1993 to 2015 and would like to do forecasting on these data. I used tsoutliers package to detect the outliers, but I do not know how do I continue to forecast with my set of data .</p>

<p>This is my code:</p>

<pre><code>product.outlier&lt;-tso(product,types=c(""AO"",""LS"",""TC""))
plot(product.outlier)
</code></pre>

<p>This is my output from tsoutliers package</p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p><a href=""http://i.stack.imgur.com/qKI4N.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qKI4N.jpg"" alt=""This is my plot""></a></p>

<p>I have these warning messages as well.</p>

<pre><code>Warning messages:
1: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
2: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
3: In locate.outliers.oloop(y = y, fit = fit, types = types, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
4: In arima(x, order = c(1, d, 0), xreg = xreg) :
  possible convergence problem: optim gave code = 1
5: In auto.arima(x = c(5.77, 5.79, 5.79, 5.79, 5.79, 5.79, 5.78, 5.78,  :
  Unable to fit final model using maximum likelihood. AIC value approximated
</code></pre>

<p><strong>Doubts:</strong></p>

<ol>
<li>If I am not wrong, tsoutliers package will remove the outliers it detect and through the use of the dataset with outliers removed, it
will give us the best arima model suited for the data set, is it
correct?</li>
<li>The adjust series data set is being shifted down by a lot due to remove of the level shift,etc. Doesn't this mean that if the forecasting is done on the adjusted series, the output of the forecast will be very inaccurate, since the more recent data are already more than 12, while adjusted data shift it to around 7-8.</li>
<li>What does warning message 4 and 5 means? Does it mean it cannot do auto.arima using the adjusted series?</li>
<li>What does the [12] in ARIMA(0,1,0)(0,0,1)[12] mean? Is it just my frequency/periodicity of my dataset, which I set it to monthly? And does this also means that my data series is seasonal as well? </li>
<li>How do I detect seasonality in my data set? As from the visualisation of the time series plot, I cant see any obvious trend, and if I use the decompose function, it will assume that there is a seasonal trend? So do I just believe what the tsoutliers tell me, where there is seasonal trend, since there is MA of order 1?</li>
<li>How do I continue to do my forecasting with this data after identifying these outliers?</li>
<li><strong>How to incorporate these outliers to other forecasting models - Exponential Smoothing, ARIMA, Strutural Model, Random Walk, theta? I am sure I cannot remove the outliers since there are level shift, and if I only take adjusted series data, the values will be too small, so what do I do?</strong></li>
</ol>

<p><strong>Do I need to add these outliers as regressor in the auto.arima for forecasting? How does this work then?</strong></p>
"
"0.108678533400333","0.146647115021353","228981","<p>I am very new to R and the forecast package authored by Rob Hyndman.
I am working on a time series with 24 samples per hour. I trained a random forest regressor to forecast 6 hour ahead values and am using MAPE(Mean Absolute Percentage Error) on a held out duration as the accuracy metric. </p>

<p>I want to compare its accuracy with standard time series methods like ARMA and ARIMA models.</p>

<p>Time Series Sample</p>

<p><a href=""http://i.stack.imgur.com/H25Wt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/H25Wt.png"" alt=""enter image description here""></a></p>

<p>Here is what I have currently. <a href=""https://gist.github.com/sivapvarma/aa277c81f7a617b1f584d0ea32856eb4"" rel=""nofollow"" title=""data.csv"">data.csv</a> has 576*16(16 days worth) samples and I wish to measure forecast accuracy on last 3 days.</p>

<pre><code>library(forecast)
pv_data = read.csv(""data.csv"", header=FALSE)
pv_ts &lt;- ts(pv_data$V2)
train_ts &lt;- window(pv_ts, end=576*13)
test_ts &lt;- window(pv_ts, start=576*13+1)
fit &lt;- auto.arima(train_ts)
accuracy(forecast(fit, h=576*3), test_ts)
</code></pre>

<p>This gives me MAPE  which is average of <code>h = 1 to 576*3</code> samples ahead point forecast absolute errors. </p>

<p><strong>Question:</strong> How to find the average of <code>h=144</code> ahead forecast absolute percent error of the estimates of samples in <code>test_ts</code>? Specifically, how to calculate $ \frac {\sum\limits_{T=576*13-h}^{576*16-h} \left\lvert \tfrac{\hat{e}_{T+h}}{y_{T+h}}\right\rvert }{576*3}$ with <code>h=144</code> where  $\hat{e}_{T+h}=\hat{y}_{T+h | T}-y_{T+h}$?</p>
"
"0.0887356509416114","0.11973686801785","104304","<p>I'm trying to forecast hourly data for 30 days for a process.</p>

<p>I have used the following code:</p>

<pre><code>#The packages required for projection are loaded
library(""forecast"")
library(""zoo"")
</code></pre>

<h3>Data Preparation steps</h3>

<p>There is an assumption that we have all the data for all the 24 hours of the month of May</p>

<pre><code>time_index &lt;- seq(from = as.POSIXct(""2014-05-01 07:00""),
                  to = as.POSIXct(""2014-05-31 18:00""), by = ""hour"")

value &lt;- round(runif(n = length(time_index),100,500))
</code></pre>

<p>Using <code>zoo</code> function , we merge data with the date and hour to create an extensible time series object</p>

<pre><code>eventdata &lt;- zoo(value, order.by = time_index)
</code></pre>

<p>As forecast package requires all the objects to be time series objects, the below command is used </p>

<pre><code>eventdata &lt;- ts(value, order.by = time_index)
</code></pre>

<p>For forecasting the values for the next 30 days, the below command is used</p>

<pre><code>z&lt;-hw(t,h=30)
plot(z)
</code></pre>

<p>I feel the output of this code, is not working fine.
<img src=""http://i.stack.imgur.com/NdZRM.jpg"" alt=""enter image description here"">
The forecasted line looks wrong and the dates are not getting correcting projected on the chart.</p>

<p>I'm not sure the fault lies in the data preparation or the output is as expected.</p>
"
"0.140303383316578","0.189320611415688","153204","<p>I have a time series for sales data on a weekly and monthly basis.  I tried using <code>holt.winter</code> and <code>auto.arima</code>. <code>holt.winter</code> can work only on monthly data (freq = 12 &lt; 24), and gives good results, but <code>auto.arima</code> gives very bad results on both monthly and weekly data, just a straight line in the following figures:</p>

<p><img src=""http://i.stack.imgur.com/zNZ9i.png"" alt=""arima monthly"">  </p>

<p><img src=""http://i.stack.imgur.com/b6W12.png"" alt=""enter image description here""></p>

<p>I have the following questions:  </p>

<ol>
<li>Can anyone provide some theoretical basis on why ARIMA performs poorly and why HW performs better?  </li>
<li>Also what model should I use for relatively high frequency data (weekly or daily)?  </li>
</ol>

<p>Also if someone can guide me to advanced books in that area (I have done some reading in Brockwell, 2002, <em>Introduction to Time Series</em>).</p>

<p>[Update]</p>

<p>I tried holt-winter . auto.arima . arima and got the following results</p>

<pre><code>ARIMA (1,0,1)(1,1,1) : sigma^2 estimated as 94587:  log likelihood = -266.51,  aic = 543.02

AUTO.ARIMA =&gt;ARIMA(0,0,0) with non-zero mean : sigma^2 estimated as 141005:  log likelihood=-352.67
AIC=709.33   AICc=709.6   BIC=713.07

Holt-Winter : ETS(A,A,A) 
  Smoothing parameters:
    alpha = 0.0298 
    beta  = 1e-04 
    gamma = 0.0133 
sigma:  306.1749
sigma^2 : 93743.06939001
     AIC     AICc      BIC 
767.3367 784.8851 797.2759 
</code></pre>

<p>it seems arima(1,0,1)(1,1,1) gives better AIC and log-likelihood that result of auto arima , also HW detects seasonality , is that auto.arima stucking at some local optima</p>
"
"0.0887356509416114","0.11973686801785","153199","<p>I've used tslm() under the R-package fpp to analyse two time series, which seem similar:</p>

<pre><code>library(fpp)
a&lt;-ts(c(1,10,2,3,4,5,6,7,8,9,10,11,12,2,21,4,6,8,10,11,12,13,14,18), start = c(1959, 1), frequency=12)
b&lt;-ts(c(1,10,2,3,4,5,6,7,8,9,10,11,12,2,21,4,6,8,10,11,12,13,14,18), start = c(1959, 3), frequency=12)
</code></pre>

<p>However, the results of a simple time series regression </p>

<pre><code>summary(tslm(a~trend+season))
</code></pre>

<p>and </p>

<pre><code>summary(tslm(b~trend+season))
</code></pre>

<p>look different - why isn't this the same? How can the trend be the same, but the other results be different? I'd understand shifted seasonal results, but these are really different.</p>

<p>Idea: the function tslm() expects full years, thus for time series b months jan and feb '59 as well as the rest of 1960 are filled with values computed from the given data. But is that idea true?</p>
"
"0.171835849155868","0.200953521496073","89851","<p>I've heard a bit about using neural networks to forecast time series. </p>

<p>How can I compare, which method for forecasting my time-series (daily retail data) is better: auto.arima(x), ets(x) or nnetar(x).</p>

<p>I can compare auto.arima with ets by AIC or BIC. But how I can compare them with neural networks?</p>

<p>For example:</p>

<pre><code>   &gt; dput(x)
 c(1774, 1706, 1288, 1276, 2350, 1821, 1712, 1654, 1680, 1451, 
 1275, 2140, 1747, 1749, 1770, 1797, 1485, 1299, 2330, 1822, 1627, 
 1847, 1797, 1452, 1328, 2363, 1998, 1864, 2088, 2084, 594, 884, 
 1968, 1858, 1640, 1823, 1938, 1490, 1312, 2312, 1937, 1617, 1643, 
 1468, 1381, 1276, 2228, 1756, 1465, 1716, 1601, 1340, 1192, 2231, 
 1768, 1623, 1444, 1575, 1375, 1267, 2475, 1630, 1505, 1810, 1601, 
 1123, 1324, 2245, 1844, 1613, 1710, 1546, 1290, 1366, 2427, 1783, 
 1588, 1505, 1398, 1226, 1321, 2299, 1047, 1735, 1633, 1508, 1323, 
 1317, 2323, 1826, 1615, 1750, 1572, 1273, 1365, 2373, 2074, 1809, 
 1889, 1521, 1314, 1512, 2462, 1836, 1750, 1808, 1585, 1387, 1428, 
 2176, 1732, 1752, 1665, 1425, 1028, 1194, 2159, 1840, 1684, 1711, 
 1653, 1360, 1422, 2328, 1798, 1723, 1827, 1499, 1289, 1476, 2219, 
 1824, 1606, 1627, 1459, 1324, 1354, 2150, 1728, 1743, 1697, 1511, 
 1285, 1426, 2076, 1792, 1519, 1478, 1191, 1122, 1241, 2105, 1818, 
 1599, 1663, 1319, 1219, 1452, 2091, 1771, 1710, 2000, 1518, 1479, 
 1586, 1848, 2113, 1648, 1542, 1220, 1299, 1452, 2290, 1944, 1701, 
 1709, 1462, 1312, 1365, 2326, 1971, 1709, 1700, 1687, 1493, 1523, 
 2382, 1938, 1658, 1713, 1525, 1413, 1363, 2349, 1923, 1726, 1862, 
 1686, 1534, 1280, 2233, 1733, 1520, 1537, 1569, 1367, 1129, 2024, 
 1645, 1510, 1469, 1533, 1281, 1212, 2099, 1769, 1684, 1842, 1654, 
 1369, 1353, 2415, 1948, 1841, 1928, 1790, 1547, 1465, 2260, 1895, 
 1700, 1838, 1614, 1528, 1268, 2192, 1705, 1494, 1697, 1588, 1324, 
 1193, 2049, 1672, 1801, 1487, 1319, 1289, 1302, 2316, 1945, 1771, 
 2027, 2053, 1639, 1372, 2198, 1692, 1546, 1809, 1787, 1360, 1182, 
 2157, 1690, 1494, 1731, 1633, 1299, 1291, 2164, 1667, 1535, 1822, 
 1813, 1510, 1396, 2308, 2110, 2128, 2316, 2249, 1789, 1886, 2463, 
 2257, 2212, 2608, 2284, 2034, 1996, 2686, 2459, 2340, 2383, 2507, 
 2304, 2740, 1869, 654, 1068, 1720, 1904, 1666, 1877, 2100, 504, 
 1482, 1686, 1707, 1306, 1417, 2135, 1787, 1675, 1934, 1931, 1456)
</code></pre>

<p>Using auto.arima:</p>

<pre><code>y=auto.arima(x)
plot(forecast(y,h=30))
points(1:length(x),fitted(y),type=""l"",col=""green"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/uwSqY.png"" alt=""enter image description here""></p>

<pre><code>&gt; summary(y)
Series: x 
ARIMA(5,1,5)                    

Coefficients:
         ar1      ar2     ar3      ar4      ar5      ma1     ma2      ma3     ma4      ma5
      0.2560  -1.0056  0.0716  -0.5516  -0.4822  -0.9584  1.2627  -1.0745  0.8545  -0.2819
s.e.  0.1014   0.0778  0.1296   0.0859   0.0844   0.1184  0.1322   0.1289  0.1388   0.0903

sigma^2 estimated as 58026:  log likelihood=-2191.97
AIC=4405.95   AICc=4406.81   BIC=4447.3

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 1.457729 240.5059 173.9242 -2.312207 11.62531 0.6157512
</code></pre>

<p>Using ets:</p>

<pre><code>fit &lt;- ets(x)
plot(forecast(fit,h=30))
points(1:length(x),fitted(fit),type=""l"",col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/9UngX.png"" alt=""enter image description here""></p>

<pre><code> &gt; summary(fit)
 ETS(M,N,N) 

 Call:
  ets(y = x) 

   Smoothing parameters:
     alpha = 0.0449 

   Initial states:
     l = 1689.128 

   sigma:  0.2094

      AIC     AICc      BIC 
 5570.373 5570.411 5577.897 

 Training set error measures:
                    ME     RMSE      MAE      MPE     MAPE      MASE
 Training set 7.842061 359.3611 276.4327 -4.81967 17.98136 0.9786665
</code></pre>

<p>In this case auto.arima fits better then ets.</p>

<p>Let's try sing neural network:</p>

<pre><code> library(caret)
 fit &lt;- nnetar(x)
 plot(forecast(fit,h=60))
 points(1:length(x),fitted(fit),type=""l"",col=""green"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/M8HIT.png"" alt=""enter image description here""></p>

<p>From the graph, I can see, that neural network model fits quite well, but how can I compare it with auto.arima/ets? How can I compute AIC?</p>

<p>Another question is, how to add confidence interval for neural network,if it is possible, like it is added automatically for auto.arima/ets.?</p>

<p>Any help and advises would be really appreciated.</p>
"
"NaN","NaN","143976","<p>I currently have timeseries data (of gold prices) and I am trying to use a simple smoothing forecast to estimate gold prices for the next 12 months.</p>"
"NaN","NaN","<p>I am not sure what function to use to accomplish this ( I am pretty new to R)</p>",""
"NaN","NaN","<p>Thank you for your help!</p>",""
"NaN","NaN","","<r><time-series><forecasting>"
"0.147151429850636","0.198561132393757","56374","<p>as I am stepping into forecasting with ARIMA models, I am trying to understand how I can improve a forecast based on ARIMA fit with seasonality and drift. </p>

<p>My data is the following time series ( over 3 years, with clear trend upwards and visible seasonality, which seems to be not supported by autocorrelation at lags 12, 24, 36??). </p>

<pre><code>    &gt; bal2sum3years.ts
             Jan     Feb     Mar     Apr     May     Jun     Jul     Aug          
    2010 2540346 2139440 2218652 2176167 2287778 1861061 2000102 2560729 
    2011 3119573 2704986 2594432 2362869 2509506 2434504 2680088 2689888 
    2012 3619060 3204588 2800260 2973428 2737696 2744716 3043868 2867416 
             Sep     Oct     Nov     Dec
    2010 2232261 2394644 2468479 2816287
    2011 2480940 2699780 2760268 3206372
    2012 2951516 3119176 3032960 3738256
</code></pre>

<p>The model that was suggested by <code>auto.arima(bal2sum3years.ts)</code> gave me the following model:</p>

<pre><code>    Series: bal2sum3years.ts 
    ARIMA(0,0,0)(0,1,0)[12] with drift         

    Coefficients:
              drift
          31725.567
    s.e.   2651.693

    sigma^2 estimated as 2.43e+10:  log likelihood=-321.02
    AIC=646.04   AICc=646.61   BIC=648.39
</code></pre>

<p>However, the <code>acf(bal2sum3years.ts,max.lag=35)</code> does not show acf coefficients higher than 0.3. The seasonality of the data is, however, pretty obvious - spike at the beginning of every year. This is what the series looks like on the graph:
<img src=""http://i.stack.imgur.com/kQi5N.png"" alt=""Original Time Series""></p>

<p>The forecast using <code>fit=Arima(bal2sum3years.ts,seasonal=list(order=c(0,1,0),period=12),include.drift=TRUE)</code> , called by function <code>forecast(fit)</code>, results in the next 12months's means being equal to the last 12 months of the data plus constant. This can be seen by calling <code>plot(forecast(fit))</code>, </p>

<p><img src=""http://i.stack.imgur.com/GJqcG.png"" alt=""Actual and Forecasted Data""></p>

<p>I have also checked the residuals, which are not autocorrelated but have positive mean ( non zero). </p>

<p>The fit does not model the original time series precisely, in my opinion ( blue the original time series, red is the <code>fitted(fit)</code>:</p>

<p><img src=""http://i.stack.imgur.com/ux3i7.png"" alt=""Original vs fit""></p>

<p>The guestion is, is the model incorrect? Am I missing something? How can I improve the model? It seems that the model literally takes the last 12 months and adds a constant to achieve the next 12 months. </p>

<p>I am a relative beginner in time series forecasting models and statistics. </p>

<p>Thank you very much for your answers!</p>
"
"0.203529284882695","0.249668628757713","169468","<p>I have monthly time series data, and would like to do forecasting with detection of outliers .</p>

<p><strong>This is the sample of my data set:</strong></p>

<pre><code>       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
2006  7.55  7.63  7.62  7.50  7.47  7.53  7.55  7.47  7.65  7.72  7.78  7.81
2007  7.71  7.67  7.85  7.82  7.91  7.91  8.00  7.82  7.90  7.93  7.99  7.93
2008  8.46  8.48  9.03  9.43 11.58 12.19 12.23 11.98 12.26 12.31 12.13 11.99
2009 11.51 11.75 11.87 11.91 11.87 11.69 11.66 11.23 11.37 11.71 11.88 11.93
2010 11.99 11.84 12.33 12.55 12.58 12.67 12.57 12.35 12.30 12.67 12.71 12.63
2011 12.60 12.41 12.68 12.48 12.50 12.30 12.39 12.16 12.38 12.36 12.52 12.63
</code></pre>

<p>I have referred to <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r?lq=1"">Timeseries analysis procedure and methods using R</a>, to do a series of different model of forecasting, however it does not seems to be accurate. In additional, I am not sure how to incorporate the tsoutliers into it as well.</p>

<p>I have got another post regarding my enquiry of tsoutliers and arima modelling and procedure over <a href=""http://stats.stackexchange.com/questions/168655/how-to-interpret-and-do-forecasting-using-tsoutliers-package-and-auto-arima/168869#168869"">here</a> as well.</p>

<p>So these are my code currently, which is similar to link no.1.</p>

<p><strong>Code:</strong></p>

<pre><code>product&lt;-ts(product, start=c(1993,1),frequency=12)

#Modelling product Retail Price

#Training set
product.mod&lt;-window(product,end=c(2012,12))
#Test set
product.test&lt;-window(product,start=c(2013,1))
#Range of time of test set
period&lt;-(end(product.test)[1]-start(product.test)[1])*12 + #No of month * no. of yr
(end(product.test)[2]-start(product.test)[2]+1) #No of months
#Model using different method
#arima, expo smooth, theta, random walk, structural time series
models&lt;-list(
#arima
product.arima&lt;-forecast(auto.arima(product.mod),h=period),
#exp smoothing
product.ets&lt;-forecast(ets(product.mod),h=period),
#theta
product.tht&lt;-thetaf(product.mod,h=period),
#random walk
product.rwf&lt;-rwf(product.mod,h=period),
#Structts
product.struc&lt;-forecast(StructTS(product.mod),h=period)
)

##Compare the training set forecast with test set
par(mfrow=c(2, 3))
for (f in models){
    plot(f)
    lines(product.test,col='red')
}

##To see its accuracy on its Test set, 
#as training set would be ""accurate"" in the first place
acc.test&lt;-lapply(models, function(f){
    accuracy(f, product.test)[2,]
})
acc.test &lt;- Reduce(rbind, acc.test)
row.names(acc.test)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.test &lt;- acc.test[order(acc.test[,'MASE']),]

##Look at training set to see if there are overfitting of the forecasting
##on training set
acc.train&lt;-lapply(models, function(f){
    accuracy(f, product.test)[1,]
})
acc.train &lt;- Reduce(rbind, acc.train)
row.names(acc.train)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.train &lt;- acc.train[order(acc.train[,'MASE']),]

 ##Note that we look at MAE, MAPE or MASE value. The lower the better the fit.
</code></pre>

<p>This is the plot of my different forecasting, which doesn't seem very reliable/accurate, through the comparison of the red""test set"", and blue""forecasted"" set.
<strong>Plot of different forecast</strong>
<a href=""http://i.stack.imgur.com/WZSNq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WZSNq.jpg"" alt=""Different forecast""></a></p>

<p><strong>Different accuracy of the respective models of test and training set</strong></p>

<pre><code>Test set
                    ME      RMSE       MAE        MPE     MAPE      MASE      ACF1 Theil's U
theta      -0.07408833 0.2277015 0.1881167 -0.6037191 1.460549 0.2944165 0.1956893 0.8322151
expsmooth  -0.12237967 0.2681452 0.2268248 -0.9823104 1.765287 0.3549976 0.3432275 0.9847223
randomwalk  0.11965517 0.2916008 0.2362069  0.8823040 1.807434 0.3696813 0.4529428 1.0626775
arima      -0.32556886 0.3943527 0.3255689 -2.5326397 2.532640 0.5095394 0.2076844 1.4452932
struc      -0.39735804 0.4573140 0.3973580 -3.0794740 3.079474 0.6218948 0.3841505 1.6767075

Training set
                     ME      RMSE       MAE         MPE     MAPE      MASE    ACF1 Theil's U
theta      2.934494e-02 0.2101747 0.1046614  0.30793753 1.143115 0.1638029  0.2191889194        NA
randomwalk 2.953975e-02 0.2106058 0.1050209  0.31049479 1.146559 0.1643655  0.2190857676        NA
expsmooth  1.277048e-02 0.2037005 0.1078265  0.14375355 1.176651 0.1687565 -0.0007393747        NA
arima      4.001011e-05 0.2006623 0.1079862 -0.03405395 1.192417 0.1690063 -0.0091275716        NA
struc      5.011615e-03 1.0068396 0.5520857  0.18206018 5.989414 0.8640550  0.1499843508        NA
</code></pre>

<p>From the models accuracy, we can see that the most accurate model would be theta model.
I am not sure why is the forecast very inaccurate, and I think that one of the reasons would be that, I did not treat the ""outliers"" in my data set, resulting in a bad forecast for all model.</p>

<p><strong>This is my outliers plot</strong></p>

<p><strong>Outliers Plot</strong>
<a href=""http://i.stack.imgur.com/bZDQv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bZDQv.jpg"" alt=""Outliers""></a></p>

<p><strong>tsoutliers output</strong></p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p>I would like to know how can I further ""analyse""/forecast my data, with these relevant data set and detection of outliers, etc.
Please do help me in treatment of my outliers as well to do my forecasting as well . </p>

<p>Lastly, I would like to know how to combined the different model forecasting together, as from what @forecaster had mentioned in link no.1, combining the different model will most likely result in a better forecasting/prediction.</p>

<p><strong>EDITED</strong></p>

<p>I would like to incorporate the outliers in other models are well.</p>

<p>I have tried some codes, eg. </p>

<pre><code>forecast.ets( res$fit ,h=period,xreg=newxreg)
    Error in if (object$components[1] == ""A"" &amp; is.element(object$components[2], : argument is of length zero

forecast.StructTS(res$fit,h=period,xreg=newxreg)
Error in predict.Arima(object, n.ahead = h) : 'xreg' and 'newxreg' have different numbers of columns
</code></pre>

<p>There are some errors produced, and I am unsure about the correct code to incorporate the outliers as regressors.
Furthermore, how do I work with thetaf or rwf, as there are no forecast.theta or forecast.rwf?</p>
"
"0.140886767884104","0.172825282455071","229721","<p>I have 4 years electrical load data. I split the data into 3 years (75%) training data, 1 year for testing (25%). Also I have the temperature data for each day during the previous period. (The link to the dataset: <a href=""https://drive.google.com/open?id=0B08HdcWBksWcTUxqc1ByOW1UVEU"" rel=""nofollow"">here</a>.) </p>

<p>I want to make use of the temperature data to enhance the forecasting using argument <code>xreg</code> in <code>arima</code> function. </p>

<p>Here is my code:</p>

<pre><code>mydata1&lt;-read.csv(""1st pape/kaggle_data.csv"");
mydata&lt;-ts(mydata1[,2],start = c(2004),frequency = 365)

#split the data into trainData and test data
trainData = window(mydata, end=c(2007))
testData = window(mydata, start=c(2007))
temp&lt;-ts(mydata1[,3],start = c(2004),frequency = 365)

#split the temperature into trainData and test data
trainReg = window(temp, end=c(2007))
testReg = window(temp, start=c(2007))
</code></pre>

<p>Apply ARIMA model without using <code>xreg</code>:</p>

<pre><code>mod_arima &lt;- auto.arima(trainData, ic='aicc', stepwise=FALSE)
summary(mod_arima)
Series: trainData 
ARIMA(1,0,3) with non-zero mean 

Coefficients:
         ar1      ma1      ma2      ma3  intercept
      0.9642  -0.2098  -0.2157  -0.1693  24008.122
s.e.  0.0110   0.0322   0.0330   0.0325   1018.007

sigma^2 estimated as 9318421:  log likelihood=-10347.38
AIC=20706.75   AICc=20706.83   BIC=20736.75

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 6.102332 3045.638 2293.946 -1.519484 9.625694 0.5151126
                    ACF1
Training set 0.004483007

plot(forecast(mod_arima)); lines(testData , col=""red"", start= c(2007,1,1)); 
legend(""topleft"", lty=1,col=c(4,2),legend=c(""forecasted data"",""real data""))

y &lt;- msts(trainData, c(7,365)) # multiseasonal ts
x &lt;- msts(trainReg, c(7,365)) # multiseasonal ts

fit &lt;- auto.arima(y, xreg=(fourier(y, K=c(3,30))))
fit_f &lt;- forecast(fit, xreg= fourier(y, K=c(3,30), 365), 365)
plot(fit_f)
</code></pre>

<p>the red line is the actual data, while the blue is the foretasted data. The left plot is appeared before using fourier function, while the right after using it. </p>

<p><a href=""http://i.stack.imgur.com/QxvKC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QxvKC.png"" alt=""enter image description here""></a></p>

<p>Apply ARIMA model using <code>xreg</code>:</p>

<pre><code>mod_arima2 &lt;- auto.arima(trainData ,xreg = trainReg, ic='aicc', stepwise=FALSE)
summary(mod_arima2)
Series: trainData 
ARIMA(1,0,3) with non-zero mean 

Coefficients:
         ar1      ma1      ma2      ma3  intercept  trainReg
      0.9709  -0.2403  -0.2108  -0.1609  29984.188  -88.3976
s.e.  0.0094   0.0320   0.0330   0.0321   1468.108   13.1966

sigma^2 estimated as 8955023:  log likelihood=-10325.13
AIC=20664.26   AICc=20664.36   BIC=20699.26

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 6.030471 2984.292 2267.803 -1.464553 9.529988 0.5092422
                    ACF1
Training set 0.005526977

plot(forecast(mod_arima2,xreg = testReg)); lines(testData , col=""red"", start= c(2007,1,1)); 
legend(""topleft"", lty=1,col=c(4,2), legend=c(""forecasted data"",""real data""))

l = (fourier(y, K=c(3,30)))
z = cbind(l,x)
fit2 &lt;- auto.arima(y, xreg=z)
fit_f2 &lt;- forecast(fit, xreg= z, 365)
plot(fit_f2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/TgJE5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TgJE5.png"" alt=""enter image description here""></a>
<strong>Questions</strong>:</p>

<ol>
<li>Did I use <code>xreg</code> correctly?</li>
<li>If yes, why is the summary the same without using <code>xreg</code>?</li>
<li>Why are the forecasts far away from the real data?</li>
</ol>
"
"0.0992094737665681","0.133869888150416","169601","<p>I trying to model a time-series using <code>tbats</code> from <code>forecast</code> package in <code>R</code>. I have divided the data into training and testing set. I want to use the parameters from the trained model to fit the testing data.</p>

<p>For example:</p>

<pre><code>library(forecast)
trainDf &lt;- read.zoo(file='Train.csv')
testDf &lt;- read.zoo(file='Test.csv')
...
trainedModel &lt;- tbats(trainDf)
</code></pre>

<p>How to use this <code>trainedModel</code> to fit the testing data, using the parameters of the trained model?</p>

<p>PS:
<code>forecast</code> allows to reuse <code>Arima</code> parameters to fit a testing series.</p>

<pre><code>library(forecast)
trainDf &lt;- read.zoo(file='Train.csv')
testDf &lt;- read.zoo(file='Test.csv')

trainedModel &lt;- auto.arima(trainDf)
fit &lt;- Arima(testDf, model=trainedModel)
plot(residuals(fit)
</code></pre>

<p>Thanks in advance.</p>
"
"0.0887356509416114","0.0898026510133874","169564","<p>The <code>arimax</code> function in the <code>TSA</code> package is to my knowledge the only <code>R</code> package that will fit a transfer function for intervention models. It lacks a <a href=""http://stats.stackexchange.com/questions/34106/forecasting-with-arimax-model-including-xtransf"">predict function</a> though which is sometimes needed.</p>

<p>Is the following a work-around for this issue, leveraging the excellent <code>forecast</code> package? Will the predictive intervals be correct? In my example, the std errors are ""close"" for the components.</p>

<ol>
<li>Use the forecast package arima function to determine the pre-intervention noise series and add any outlier adjustment.</li>
<li>Fit the same model in <code>arimax</code> but add the transfer function</li>
<li>Take the fitted values for the transfer function (coefficients from <code>arimax</code>) and add them as xreg in <code>arima</code>. </li>
<li>Forecast with <code>arima</code></li>
</ol>

<blockquote>
<pre><code>library(TSA)
library(forecast)
data(airmiles)
air.m1&lt;-arimax(log(airmiles),order=c(0,0,1),
              xtransf=data.frame(I911=1*(seq(airmiles)==69)),
              transfer=list(c(1,0))
              )
</code></pre>
  
  <p>air.m1</p>
</blockquote>

<p>Output:</p>

<pre><code>Coefficients:
  ma1  intercept  I911-AR1  I911-MA0
0.5197    17.5172    0.5521   -0.4937
s.e.  0.0798     0.0165    0.2273    0.1103

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.09   BIC=-155.02
</code></pre>

<p>This is the filter, extended out 5 more periods that the data</p>

<pre><code>tf&lt;-filter(1*(seq(1:(length(airmiles)+5))==69),filter=0.5521330,method='recursive',side=1)*(-0.4936508)
forecast.arima&lt;-Arima(log(airmiles),order=c(0,0,1),xreg=tf[1:(length(tf)-5)])
forecast.arima
</code></pre>

<p>Output:</p>

<pre><code>Coefficients:
         ma1  intercept  tf[1:(length(tf) - 5)]
      0.5197    17.5173                  1.0000
s.e.  0.0792     0.0159                  0.2183

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.28   BIC=-157.74
</code></pre>

<p>Then to Predict</p>

<pre><code>predict(forecast.arima,n.ahead = 5, newxreg=tf[114:length(tf)])
</code></pre>
"
"0.0443678254708057","0.059868434008925","61317","<p>I am using <code>tbats</code> and <code>bats</code> functions and must be doing something wrong. I am using the following command for standard Holt-Winters with bats / tbats but getting an error. </p>

<pre><code>tbats(y = data, use.box.cox = FALSE, use.trend = TRUE, use.damped.trend=FALSE,
      seasonal.periods = 24, use.arma.errors = FALSE)
</code></pre>

<p>This the error generated:  </p>

<pre><code>Error in 1:nrow(control.array) : argument of length 0
</code></pre>

<p>Same thing happens when I use bats. </p>

<p>But when I use the above command without <code>use.damped.trend=FALSE</code> I do get the answer. The problem is that the value of $\alpha=1.4$, which in my opinion is wrong. </p>
"
"0.0887356509416114","0.11973686801785","5170","<p>I am new to forecasting in R and am trying to automatically fit an ARIMA model to what I believe is a univariate dataset.</p>

<pre><code>&gt; str(p1.z)
'zoo' series from 2009-04-05 to 2010-10-31
  Data: int [1:83] 360 570 540 585 570 690 495 660 510 690 ...
  Index: Class 'Date'  num [1:83] 14339 14346 14353 14360 14367 ...

&gt;  head(p1.z) 
  2009-04-05 2009-04-12 2009-04-19 2009-04-26 2009-05-03 2009-05-10 
         360        570        540        585        570        690
</code></pre>

<p>But when I try to fit the model, I get the error as seen below.</p>

<pre><code>&gt; p1.arima &lt;- auto.arima(p1.z)
Error in nsdiffs(xx) : Non seasonal data
</code></pre>

<p>It is my understanding that the forecast package and the auto.arima function would be able to fit my data seasonal or not.  I am trying to learn time series forecasting and am using a dataset that appears to be ideal for this sort of task .  Also, the function ets() was able to find a model.</p>

<p>Any help you can provide will be greatly appreciated</p>
"
"0.133103476412417","0.159649157357133","171338","<p>Thanks all I have edited the questions based on your replies</p>

<p>I have a univariate time series with high frequency (the data is generated minutely) with 10 days of data so far.</p>

<p>The data is coming from sensors located on different parking sites that count vacancy availability, my objective is to do a forecast for each parking site for the next 24 hours.</p>

<pre><code>&gt; str(stations)
'data.frame':   76234 obs. of  3 variables:
 $ siteid    : Factor w/ 5 levels ""15031"",""15032"",..: 3 4 1 5 2 3 4 1 5 2 ...
     $ Count     : int  13 16 30 2 9 13 16 30 2 9 ...
 $ LastUpdate: POSIXct, format: ""2015-08-27 06:31:00"" ""2015-08-27 06:31:00"" ""2015-08-27 06:31:00"" ""2015-08-27 06:31:00"" ...
&gt; head(stations)
  siteid Count          LastUpdate
1  15033    13 2015-08-27 06:31:00
2  15034    16 2015-08-27 06:31:00
3  15031    30 2015-08-27 06:31:00
4  15035     2 2015-08-27 06:31:00
5  15032     9 2015-08-27 06:31:00
6  15033    13 2015-08-27 06:32:00
</code></pre>

<p>I have read through forums that arima models are not suited for high frequency data, when I decompose the ts that initiate with a frequency of 24*60.</p>

<pre><code>&gt; s15033&lt;- stations[stations$siteid == 15033,]
    &gt; s15033.ts &lt;- ts(s15033$Count, start = c(2015,239), frequency = 1440)
</code></pre>

<p>Thank you for the reply </p>

<p>Here is an updated hourly <a href=""http://1drv.ms/1EQhBlD"" rel=""nofollow"">sample</a> of the dataset as requested</p>

<p><strong>What would be the right approach and  recommended method to use for such high frequency?</strong></p>

<p><strong>Edit</strong></p>

<p>Thank you so much for your input, I had to digest all of the information you provided.</p>

<p>In my case, I have no choice but to use R so as that incoporate the model into the workflow of my application.</p>

<p>I understand that I will be better off, forecast at the hour level and use external regressors to help with the shifts and sudden spike that may appear in the time series.</p>

<p>As follow up, I have a couple of questions and apologies if the questions may sound a bit rudimentary as I am still learning about forecasting and R</p>

<ul>
<li>How can I implement in R the hybrid causal/ARIMA/pulse you came up with in Autobox</li>
<li>I have about 1000 parking site, would I need a  model for each parking site</li>
<li>Would machine learning  model help with predicting at lower granularity</li>
</ul>
"
"0.0443678254708057","0.059868434008925","234446","<p>I have a time series $Y_t$ that is stationary, and several explanatory variables $X$ .. $Z$ (stationary as well). </p>

<p>Is there an R package (or Python one) that can automatically fit all the possible predictive regressions of the form</p>

<p>$Y_t$ = a + b * L(X) + c * L(Y) + d * L(Z) + epsilon</p>

<p>where indicates the vector of lagged X valuesm ie L(X) = ($X_{t-1}$, $X_{t-2}$, ..). The package should tell me automatically (for a given max lag of course) which specification has the best in-sample prediction accuracy (by eventually dropping some of the predictive variables)?</p>

<p>Thanks!</p>
"
"0.0992094737665681","0.133869888150416","194756","<p>I've got a question regarding ARIMA modeling. 
I am having a hard time to make to model out the seasonalities of my time series. The pictures below shows my tries in modeling. 
The topic is to forecast sales of a shop. 
It shows both the real Sales Values and the fitted/forecasted values.</p>

<p><a href=""http://i.stack.imgur.com/cOnr9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cOnr9.png"" alt=""enter image description here""></a></p>

<p>Below you can see my avalable data. <code>Promo</code> means whether there is a promotion on a certain day in the store.
<code>Holiday1-3</code> are different kind of holidays, like Easter holidays (<code>Holiday2</code>) and Christmas holidays (<code>Holiday1</code>).
The Variables <code>PxH1</code>, <code>PxH2</code>, <code>Pxh3</code> and <code>PxS</code> are made by me and multiplicate the value of <code>Promo</code> with each holiday. A Promo and school holiday at the same time give a higher bump in Sales then just a promo or just a school holiday.
Promo usually is every 2 weeks for a whole week. That's where the 2 weekly bump comes from. The 2 weekly bump and the holiday bumps are already modeled quite nice in my opinion...</p>

<p><a href=""http://i.stack.imgur.com/Fvnri.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Fvnri.png"" alt=""enter image description here""></a></p>

<p>My question is on how to model out the yearly seasonality properly. The sales clearly stay the same from New Year to Easter, grow slowly from Easter until beginning of August, with August being the peak of sales, and then continue to fall from August to New Year.</p>

<p>If you can help me with this topic with something different than SPSS Modeler, like R, then feel free to answer in R.
If you need additional information please feel free to tell me!</p>

<p>Thank you a lot!</p>
"
"0.133103476412417","0.179605302026775","209874","<p>I have a model fitted with <code>auto.arima</code>, the model is ARIMA(0,1,0)x(0,1,0)[6] with seasonal period 6. The data is bi-monthly so there is an annual seasonality. There is only one regressor indicating an intervention (dummy). </p>

<p>Then I used this model to old data to see what would have happened if the intervention would have done since and earlier period, using the model and forecast from an earlier data. <strong>The thing I do not understand yet</strong> is that if I suppose the intervention only occur in one period, the series only differ in this period. Therefore, there is no persistence on the intervention.</p>

<p>As I understand, the model has ARIMA errors. The error in the intervention period should change and so there should be an effect in the next periods when using forecast to predict futures values. If the intervention occurs in only one period, <strong>why</strong> in the forecast the intervention does not affect futures predictions?</p>

<hr>

<p>EDIT:</p>

<p>The code I am using is</p>

<pre><code>model1&lt;-auto.arima(ts,xreg = X.ts)
</code></pre>

<p>Where <code>X.ts</code> is a <code>ts</code> object with <code>0</code> and a period with intervention. </p>

<p>Then I used </p>

<pre><code>model2&lt;-Arima(Xold, xreg= X.ts.old, model=model1)
</code></pre>

<p>So I used the first model on earlier data to make the following</p>

<pre><code>forecast(model2, xreg=cbin(c(0,1,1,1,1...))
</code></pre>

<p>So I am trying to show what would have been expected from an earlier period (the forecast) if the intervention would have started earlier.</p>

<p>The thing I do not understand yet is that for instance</p>

<pre><code>forecast(model2, xreg=cbin(c(0,1,1,1,0...))
forecast(model2, xreg=cbin(c(0,1,1,1,1...))
</code></pre>

<p>only differ in the periods the <code>xreg</code> differ, with no persistence of these differences. I did not expect this, <strong>why is that?</strong></p>
"
"0.140886767884104","0.172825282455071","234192","<p>I referred to <a href=""https://www.otexts.org/fpp/9/4"" rel=""nofollow"">this link</a> and I have the following questions regarding my data. Let me start by explaining the time series that I am dealing with.</p>

<p>I have <strong>daily</strong> hospital data with various <strong>departments</strong> and numerous <strong>doctors</strong> working in each department. I have several years of data and my forecast horizon is for the next 365 days. My data has weekly and annual seasonality. Moreover I intend to capture the effects of holidays and Sundays in my forecasts. As a result I have not created a hierarchical time series as suggested towards the end of the link(primarily because I am not sure whether we can pass a regressor to it and more so because I do not know how many doctors I end up predicting for in each department). </p>

<p>The reason for this is that some doctors do not have good data(short time series or sparse data). In this case I collect these doctors and aggregate them to form something I call ""OtherDocs"". Typically in <code>DeptXYZ -&gt; Doc1 , Doc2 , Doc3 , Doc4 , Doc5 and Doc6</code> I could end up creating forecasts for <code>DeptXYZ -&gt; Doc1 , Doc3 , Doc4 , Doc6 and OtherDocs</code>. If <code>OtherDocs</code> is still not predictable I generate a naive forecast. In this fashion I created <strong>base forecasts for every level in the hierarchy individually using <code>arima</code> and passing my <code>xreg</code> to it and selecting the best model on the basis of AIC</strong>.</p>

<p>Now, consider this example - </p>

<p><code>Total -&gt; DeptX and DeptY</code></p>

<p><code>DeptX -&gt; DocA and DocB</code></p>

<p><code>DeptY -&gt; Doc1 , Doc2 and Doc3</code></p>

<p>There are cases where <code>DocA</code> has a time series that starts from ""2011-03-11"" and ends on ""2016-09-07"" while <code>DocB</code> has a time series that starts from ""2011-05-17"" and ends on ""2016-09-07"". Generating the base forecasts for <code>DocA</code> and <code>DocB</code> results in the predicted values(<code>fit$mean</code>) being of a time series from ""2016-09-08"" to ""2017-09-07"". As long as the time series refers to the same dates within the Department I believe we are good to go.</p>

<p>In my attempt to reconcile the forecasts from each level I employed the forecasted proportions like so -</p>

<p>$\Largeá»¹_{DocA,365} = \frac{Å·_{DocA,365}*Å·_{DeptX,365}}{(Å·_{DocA,365}+Å·_{DocB,365})*(Å·_{DeptX,365}+Å·_{DeptY,365})}Å·_{Total,365}$</p>

<p><strong>1. Am I doing anything wrong in the above step?</strong></p>

<p><strong>2. Suppose for one moment that the topmost level forecasted values do not capture the low points of data in the case of Holidays and Sundays. Does that intuitively mean that revised forecasts for DocA might not correctly capture the same(being a proportion of $Å·_{Total,365}$)?</strong></p>

<p>Another query I have is to do with the Optimal Combination Approach -</p>

<p>$\Largeá»¹_h = S(Sâ€²S)^{-1}Sâ€²Å·_h$</p>

<p><strong>3. I am unfamiliar with this matrix notation $S'$. Is it the inverse of $S$? Could you shed some light on this? And how do you suggest I calculate the summing up matrix in my case?(Is it absolutely necessary to proceed with the exact knowledge of the number of doctors in each department?)</strong></p>
"
"0.108678533400333","0.122205929184461","157157","<p>I'm trying to evaluate a model for a time series, given many time series (plural). 
For example, i'm using the <code>forecast</code> package and in particular the <code>ets</code> function to forecast based on a time series.</p>

<p>My data was not continuously gathered, so I have around 50 sessions of 1-2 hours each, where each session was recorded on a different day.</p>

<p>How do I evaluate the parameters of a time-series model using multiple experiment sessions data? concatenating the time series is obviously not a good idea because the last samples of session <code>k-1</code> have no affect on the first samples at session <code>k</code>.</p>

<p>This is a special case of an irregular time series, but I don't think it should be treated as one.</p>

<p>here is an example code:</p>

<pre><code># original time series, one per recording session:
ts1 &lt;- ts(rnorm(n = 10, mean = 1, sd = 1),start = as.POSIXct(1433679895,origin=""1970-01-01""),frequency = 1)
ts2 &lt;- ts(rnorm(n = 10, mean = 1.7, sd = 1.8),start = as.POSIXct(1433766295,origin=""1970-01-01""),frequency = 1)
ts3 &lt;- ts(rnorm(n = 10, mean = 1.5, sd = 1.3),start = as.POSIXct(1433852695,origin=""1970-01-01""),frequency = 1)

# concatenate all time series to an its (irregular time series) object,     just as a way to represent the combined ts
library(its)
dates &lt;- as.POSIXct(c(time(ts1),time(ts2),time(ts3)),origin=""1970-01-01"")
ts.all &lt;- its(x = c(ts1,ts2,ts3), dates)

library(forecast)
ets.model &lt;- ets(ts.all,model='ZNN',alpha = 0.3)
</code></pre>

<p>So the model assumes that this is a regular time series, even though it is not.
Is there a way to iteratively evaluate the parameters of the model given multiple sessions of data?</p>

<p>This is actually a general question regarding time series analysis in chunks. This problem can happen with any analysis and any package.</p>

<p>Thanks!</p>
"
"0.076847327936784","0.0691301129820284","157396","<p>I am curious if R or any other open source code can deal with forecasting changes in water elevation based on a predicted/forecasted value of rain.</p>

<p>I have a ton of data that shows water elevations (height ft) measured every hour for almost 30 years.  I would like to quantify the relationship between the volume of rainfall received (inches) and change in water elevation (river/lake).  In the end, creating a distribution curve that shows the relationship and expected outcome of change in water elevation based on  rainfall volumes.</p>

<p>I am only interested in a statistical approach based on data trends over time at this point.  I understand when dealing with changes to water body elevations, there are numerous other factors to consider. </p>

<p>Any suggestions on available open source code or references to similar would be appreciated.</p>

<p>Martin</p>
"
"0.0627455805138159","0.0846667513334603","6152","<p>I have a (I suspect) simple question. I have time series cross section data on voting behaviour in the Council of the European Union (the monthly number of yes, no and abstentions for each member state from 1999 to 2007). So basically the variables are counts, thus a Poisson/negative binomial regression would be appropriate, possibly with lagged dependent variables on the right hand side to control for time dependencies. I have seen papers with people using such negative binomial models to forecast, for instance the number of monthly legislative acts adopted in the future, and I have three questions in this regard:</p>

<ol>
<li><p>How can i run a negative binomial regression on panel data without making any inferential mistakes?</p></li>
<li><p>How can I use a negative binomial model with lags to forecast future values of the dependent variable.</p></li>
<li><p>Can this be done in R?</p></li>
</ol>

<p>Thomas</p>
"
"0.076847327936784","0.103695169473043","6330","<p>I have previously used <a href=""http://www.forecastpro.com/"">forecast pro</a> to forecast univariate time series, but am switching my workflow over to R.  The forecast package for R contains a lot of useful functions, but one thing it doesn't do is any kind of data transformation before running auto.arima().  In some cases forecast pro decides to log transform data before doing forecasts, but I haven't yet figured out why.</p>

<p>So my question is: when should I log-transform my time series before trying ARIMA methods on it?</p>

<p>/edit: after reading your answers, I'm going to use something like this, where x is my time series:</p>

<pre><code>library(lmtest)
if ((gqtest(x~1)$p.value &lt; 0.10) {
    x&lt;-log(x)
}
</code></pre>

<p>Does this make sense?</p>
"
"0.117386232408469","0.135768846660426","6329","<p>I've been using the ets() and auto.arima() functions from the <a href=""http://robjhyndman.com/software/forecast/"">forecast package</a> to forecast a large number of univariate time series.  I've been using the following function to choose between the 2 methods, but I was wondering if CrossValidated had any better (or less naive) ideas for automatic forecasting.</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"") {
    XP=ets(x, ic=ic) 
    AR=auto.arima(x, ic=ic)

    if (get(ic,AR)&lt;get(ic,XP)) {
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
        model
}
</code></pre>

<p>/edit: What about this function?</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"",holdout=0) {
    S&lt;-start(x)[1]+(start(x)[2]-1)/frequency(x) #Convert YM vector to decimal year
    E&lt;-end(x)[1]+(end(x)[2]-1)/frequency(x)
    holdout&lt;-holdout/frequency(x) #Convert holdout in months to decimal year
    fitperiod&lt;-window(x,S,E-holdout) #Determine fit window

    if (holdout==0) {
        testperiod&lt;-fitperiod
    }
    else {
        testperiod&lt;-window(x,E-holdout+1/frequency(x),E) #Determine test window
    }

    XP=ets(fitperiod, ic=ic)
    AR=auto.arima(fitperiod, ic=ic)

    if (holdout==0) {
        AR_acc&lt;-accuracy(AR)
        XP_acc&lt;-accuracy(XP)
    }
    else {
        AR_acc&lt;-accuracy(forecast(AR,holdout*frequency(x)),testperiod)
        XP_acc&lt;-accuracy(forecast(XP,holdout*frequency(x)),testperiod)
    }

    if (AR_acc[3]&lt;XP_acc[3]) { #Use MAE
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
    model
}
</code></pre>

<p>The ""holdout"" is the number of periods you wish to use as an out of sample test.  The function then calculates a fit window and a test window based on this parameter.  Then it runs the auto.arima and ets functions on the fit window, and chooses the one with the lowest MAE in the test window.  If the holdout is equal to 0, it tests the in-sample fit.</p>

<p>Is there a way to automatically update the chosen model with the complete dataset, once it has been selected?</p>
"
"0.076847327936784","0.103695169473043","63250","<p>I was trying to forecast using the ""hw"" method in R. 
I have data which follows:</p>

<pre><code>Period 1: 
568
485
360
523
514
370
332
Period 2: 
841
685
719
647
615
389
367
Period 3: 
731
721
819
662
581
436
394
Period 4: 
865
805
952
759
677
429
424
Period 5:
598
868
888
849
707
458
426
Period 6:
950
806
826
804
730
541
439
Period 7:
1,070
770
989
863
737
525
461
Period 8:
1,041
863
989
833
783
506
496
</code></pre>

<blockquote>
<pre><code>[...] And more periods
</code></pre>
</blockquote>

<p>The period here represent weeks and each of the data in periods are quantities on a given day. So we can see a pattern that the first 3 days usually shows the largest quantities.
I decided to forecast with HW method. </p>

<p>So I did the following in R:</p>

<pre><code>library(forecast)
x &lt;- data$weeklydata
x &lt;- ts(x, frequency = 7)
plot(hw(x, 6), byt = ""1"")
</code></pre>

<p>However, the graph I get ranges from -200 to 300, with my forecast being showed to near 0 to negative quantity. Am making a fundamental mistake here with my concepts, or is there something wrong with my R coding?</p>
"
"0.0443678254708057","0.059868434008925","210117","<p>I have the code below which trains ARIMA models for a range of order combinations. I'm getting the error below in the step training the ARIMA models.  The code worked just fine with the <code>hsales</code> time-series provided for Hyndman's text book in the ""fpp"" package in R. If anyone can point out the issue or suggest how to solve it, I would be grateful.</p>

<p>Code:</p>

<pre><code>library(""forecast"")
library(""tseries"")
library(""sqldf"")
library(""manipulate"")
library(""dplyr"")
library(""xts"")

tsTrain &lt;- tsTrain
tsTest &lt;- tsValidation

pvar&lt;-1:17
dvar&lt;-1:2
qvar&lt;-1:17

##Creating All Combingations

OrderGrid&lt;-expand.grid(pvar,dvar,qvar)

##Vectorize Suggestion

n &lt;- function(a,b,c) {Arima(tsTrain, order=c(a,b,c),method=""ML"")}
mod_fit &lt;- do.call(Vectorize(n, SIMPLIFY=FALSE), unname(OrderGrid))
</code></pre>

<p>Error:</p>

<pre><code>Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
  non-finite finite-difference value [3] 
</code></pre>

<p>Data:</p>

<pre><code>c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 8.5, 4, 5, 9, 10, 11, 
7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 2, 15, 2.5, 17, 5, 5.5, 
7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 9.5, 3.5, 5, 4, 4, 9, 4.5, 
6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 12, 17.5, 19, 7, 14, 17, 3.5, 
6, 15, 11, 10.5, 11, 13, 9.5, 9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 
19, 6, 7, 7.5, 7.5, 7, 6.5, 9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 
5, 12, 6, NA, 4, 2, 5, 7.5, 11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 
7, 4.5, 9, 3, 4, 6, 17.5, 11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 
7, 7, 4, 7.5, 11, 6, 11, 7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 
6, 8.5, 7.5, 6, 5, 8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 
11.5, 3, 4, 16, 3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 
6.5, 9, 12, 17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 
6.5, 15, 8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 
16.5, 2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 
13, 10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 11.5, 
12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 10, 10, 
13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 5.5, 6, 14, 
16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 13, 6, 7, 3, 5.5, 
7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 13, NA, 12, 1.5, 7, 
7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 8, 6, 3, 7.5, 4, 7, 7.5, 
NA, NA, NA, NA, 6.5, 2, 16.5, 7.5, 8, 8, 5, 2, 7, 4, 6.5, 4.5, 
10, 6, 4.5, 6.5, 9, 2, 6, 3.5, NA, 5, 7, 3.5, 4, 4.5, 13, 19, 
8.5, 10, 8, 13, 10, 10, 6, 13.5, 12, 11, 5.5, 6, 3.5, 9, 8, NA, 
6, 5, 8.5, 3, 12, 10, 9.5, 7, 24, 7, 9, 11.5, 5, 7, 11, 6, 5.5, 
3, 4.5, 4, 5, 5, 3, 4.5, 6, 10, 5, 4, 4, 9.5, 5, 7, 6, 3, 13, 
5.5, 5, 7.5, 3, 5, 6.5, 5, 5.5, 6, 4, 3, 5, NA, 5, 5, 6, 7, 8, 
5, 5.5, 9, 6, 8.5, 9.5, 8, 9, 6, 12, 5, 7, 5, 3.5, 4, 7.5, 7, 
5, 4, 4, NA, 7, 5.5, 6, 8.5, 6.5, 9, 3, 2, 8, 15, 6, 4, 10, 7, 
13, 14, 9.5, 9, 18, 6, 5, 4, 6, 4, 11.5, 17.5, 7, 8, 10, 4, 7, 
5, 9, 6, 5, 4, 8, 4, 2, 1.5, 3.5, 6, 5.5, 5, 4, 8, 10.5, 4, 11, 
9.5, 5, 6, 11, 21, 9.5, 11, 13.5, 7.5, 13, 10, 7, 9.5, 6, 10, 
5.5, 6.5, 12, 10, 10, 6.5, 2, 8, NA, 10, 5, 4, 4.5, 5, 7.5, 12, 
22, 5, 8.5)
</code></pre>
"
"0.0992094737665681","0.133869888150416","187733","<p>I am using forecast package for time series analysis. I have a dataset with hourly data from 1 June 2015 to 30 November 2015. The dataset exhibits weekly and daily seasonality. 
I used the following code to convert my data into a time series object. </p>

<pre><code>data.ts &lt;- ts(value, freq = 365.25*24, start = 2015+151/365.5) 
hw &lt;- HoltWinters(data.ts)
</code></pre>

<p>When I try to use HoltWinters algorithm I get the following error: </p>

<blockquote>
  <p>Error in decompose(ts(x[1L:wind], start = start(x), frequency = f), seasonal) : 
    time series has no or less than 2 periods</p>
</blockquote>

<p>When I tried implementing TBATS algorithm, I got the following error. </p>

<blockquote>
  <p>Error in optim(par = param.vector$vect, fn = calcLikelihoodTBATS, method = ""Nelder-Mead"",  : 
    function cannot be evaluated at initial parameters</p>
</blockquote>

<p>How do I create an appropriate time series object?</p>
"
"0.076847327936784","0.103695169473043","192400","<p>I have done fitted a DCC-GARCH model using the <code>dccfit</code> function from the ""rmgarch"" package in R. The output is below:</p>

<pre><code>*---------------------------------*
*          DCC GARCH Fit          *
*---------------------------------*

Distribution         :  mvnorm
Model                :  DCC(1,1)
No. Parameters       :  62
[VAR GARCH DCC UncQ] : [0+32+2+28]
No. Series           :  8
No. Obs.             :  240
Log-Likelihood       :  4896.6
Av.Log-Likelihood    :  20.4 

Optimal Parameters
-----------------------------------
                  Estimate   Std. Error  t value  Pr(&gt;|t|)
[FTSE100].mu      0.005599    0.003457 1.6195e+00 0.105339
[FTSE100].omega   0.000100    0.000160 6.2312e-01 0.533205
[FTSE100].alpha1  0.176637    0.124341 1.4206e+00 0.155436
[FTSE100].beta1   0.807578    0.072324 1.1166e+01 0.000000
[MSUSAML].mu      0.007760    0.003077 2.5219e+00 0.011673
[MSUSAML].omega   0.000056    0.000053 1.0484e+00 0.294455
[MSUSAML].alpha1  0.092896    0.040348 2.3023e+00 0.021316
[MSUSAML].beta1   0.886704    0.028933 3.0647e+01 0.000000
[MSEXUK.].mu      0.009228    0.003421 2.6976e+00 0.006984
[MSEXUK.].omega   0.000114    0.000189 6.0293e-01 0.546552
[MSEXUK.].alpha1  0.070957    0.046983 1.5103e+00 0.130978
[MSEXUK.].beta1   0.889084    0.091959 9.6682e+00 0.000000
[DAXINDX].mu      0.010099    0.004489 2.2496e+00 0.024474
[DAXINDX].omega   0.001005    0.000794 1.2650e+00 0.205864
[DAXINDX].alpha1  0.191733    0.113491 1.6894e+00 0.091142
[DAXINDX].beta1   0.600585    0.225184 2.6671e+00 0.007651
[BMUK10Y].mu      0.001496    0.001295 1.1548e+00 0.248181
[BMUK10Y].omega   0.000000    0.000027 0.0000e+00 1.000000
[BMUK10Y].alpha1  0.025774    0.174068 1.4807e-01 0.882287
[BMUK10Y].beta1   0.969964    0.178467 5.4350e+00 0.000000
[BMUS10Y].mu      0.001069    0.001481 7.2147e-01 0.470623
[BMUS10Y].omega   0.000021    0.000014 1.4980e+00 0.134123
[BMUS10Y].alpha1  0.025983    0.024924 1.0425e+00 0.297181
[BMUS10Y].beta1   0.928892    0.037850 2.4542e+01 0.000000
[BMBD10Y].mu      0.000893    0.001088 8.2098e-01 0.411657
[BMBD10Y].omega   0.000000    0.000000 1.2974e-01 0.896774
[BMBD10Y].alpha1  0.000000    0.000089 7.8000e-05 0.999938
[BMBD10Y].beta1   0.999000    0.000075 1.3363e+04 0.000000
[LHUSTRY].mu      0.000170    0.000950 1.7931e-01 0.857694
[LHUSTRY].omega   0.000007    0.000000 2.2820e+01 0.000000
[LHUSTRY].alpha1  0.024463    0.001250 1.9571e+01 0.000000
[LHUSTRY].beta1   0.941022    0.005656 1.6638e+02 0.000000
[Joint]dcca1      0.017443    0.005703 3.0584e+00 0.002225
[Joint]dccb1      0.942324    0.012105 7.7843e+01 0.000000

Information Criteria
---------------------

Akaike       -40.288
Bayes        -39.389
Shibata      -40.388
Hannan-Quinn -39.926
</code></pre>

<p>Can someone tell me what is the meaning of <code>Pr(&gt;|t|)</code>? Is it the p value for the parameter? If it is, then I have lots of insignificant parameters which indicates a very bad model I have there. I have tried run examples from the <code>rmgarch.tests</code> folder as well but the <code>Pr(&gt;|t|)</code> values for the example are also big (greater than 0.05). What can I do here?</p>
"
"0.0887356509416114","0.0898026510133874","163330","<p>I have estimated a univariate time series model, consisting of a random walk and an AR component. Now the goal is to make forecast about a couple of steps ahead as new data comes in, in an online fashion. so in R, using <a href=""https://cran.r-project.org/web/packages/dlm/index.html"" rel=""nofollow"">DLM</a> package:</p>

<pre><code>filter &lt;- dlmFilter(training_data, model)
forecast &lt;- dlmForecast(filter, 2)
</code></pre>

<p>Now the problem is that it seems every time a new data point comes in, I have to reconstruct the filter and then do the forecast, which is very slow. Also, I think, one of the promises of using DLM is that it does not need to store the whole dataset. Is there a way to update the filter as new data comes in without going through all the data? </p>
"
"0.153694655873568","0.190107810700578","192739","<p>I have been working with the forecast package in R a lot, recently. And my question might seem trivial (or not, maybe I'm missing something), but for the life of me I can't seem to find a way to fit an Arima model with exogenous variables (<code>xreg</code> argument) that has been computed by the <code>auto.arima</code> function to previously unseen test data.</p>

<p>So, I'm basically trying to do the following:</p>

<pre><code>library(forecast)
fit &lt;- auto.arima(trainingdata, xreg = trainingvariables)
</code></pre>

<p>...and then I would like to ""apply"" the model to new test data, for which I also have new exogenous variables available. I can see the following methods:</p>

<pre><code>fitted(fit)
</code></pre>

<p>That returns one-step in-sample forecasts, so, in effect, that's exactly what I want. Except that it's in-sample. However, I would like to calculate <strong>one-step out-of-sample forecasts</strong> (with <strong>new exogenous variables</strong> that I have available). Another method:</p>

<pre><code>forecast(fit, xreg = newvariables, h = ...)
</code></pre>

<p>That works for exactly one step, but then seems to merely forecast the trainingdata stored in the model fit. But I don't think I can use new testdata here? (So, I can't use this method for testing one-step prediction accuracy.) One more idea:</p>

<pre><code>fit2 &lt;- Arima(testdata, model = fit)
</code></pre>

<p>According to the manual, if the <code>model</code> parameter is used, ""this same model is
fitted to [testdata] without re-estimating any parameters"". Great, but I don't think I can supply any new exogenous variables, can I?</p>

<p>I really think, I must be missing something simple. Any help would be much appreciated.</p>
"
"0.117386232408469","0.135768846660426","193054","<p>I'm fairly new to time series analysis. I want to analyze two series of variables in a span of time to predict a binary outcome.</p>

<p>For example i collect data over time at my home of two variables:</p>

<p>VarA the temperature over time</p>

<p>VarB the humidity over time</p>

<p>Then at 12:00 am i stop collecting this data and i see at 4:00 pm if it rains or not.</p>

<p>With a big dataset i want to predict given the time seres data collected till 12:00 am if it will rain at 4:00 pm. How can i accomplish this?</p>

<p>I was thinking about a k-nearest neighbors regression type analysis but i'm not sure how can i implement this.</p>

<p>EDIT:
Here a fictional data set, i don't have already the data because i'm still defining the details. I want to know if studying the two time series (the difference from the start or other parameters) is there a way to predict the outcome of the day</p>

<p><a href=""http://i.stack.imgur.com/VOJsa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VOJsa.png"" alt=""""></a></p>
"
"0.0443678254708057","0.059868434008925","193178","<p>Please consider the following code (in R)</p>

<pre><code>library(forecast)
tt&lt;-structure(c(1494.5, 1367.57, 1357.57, 1222.23, 1124.02, 1011.64, 
4575.64, 3201.87, 3050.04, 2173.38, 1967.88, 1838.55, 1666.05, 
1656.05, 1524.96, 835.96, 775.36, 592.36, 494.15, 4058.15, 2624.36, 
2448.47, 1598.47, 1398.47, 1264.14, 1165.88, 1053.67, 941.36, 
821.36, 471.36, 373.15, 259.91, 3808.91, 2262.26, 1940.39, 1011.39, 
800.81, 790.81), index = structure(c(16563L, 16565L, 16570L, 
16572L, 16577L, 16579L, 16584L, 16585L, 16586L, 16587L, 16588L, 
16589L, 16590L, 16592L, 16593L, 16599L, 16606L, 16607L, 16608L, 
16612L, 16613L, 16614L, 16617L, 16618L, 16619L, 16620L, 16621L, 
16628L, 16633L, 16635L, 16638L, 16642L, 16647L, 16648L, 16649L, 
16650L, 16651L, 16654L), class = ""Date""), class = ""zoo"")

tt2&lt;-as.ts(tt)
tt2&lt;-na.locf(tt2) #I replace the NA with the previous non-NA value
mm&lt;-auto.arima(tt2)

plot(forecast(mm, h=60))
</code></pre>

<p>The results of the auto.arima function is puzzling...
There is a clear seasonality in the data (this is the balance of an account: every month a salary is cashed in and there is a spike in the value of the series, followed by a decrease until the next salary is received). I would like to forecast a couple of cycles, but the auto.arima forecast is nothing like I expect.
Does anybody have any suggestions (also outside the auto.arima)?
Any suggestion is welcome.</p>
"
"0.076847327936784","0.103695169473043","193354","<p>I am struggling with a forecast project. I have a time series of daily financial data for a personal account. The goal is to have a time series model for future daily cash flows. Right now,for better or worse, I have removed the predictable events (rent payment and monthly salary), so I am focussing on the time series of the daily expenses. This is of course a non-negative time series, which is zero in the days when no expenses are made.
I am left with the following time series</p>

<pre><code>myts&lt;-structure(c(5.5, 0, 126.93, 0, 0, 0, 0, 10, 0, 135.34, 0, 0, 
0, 0, 98.21, 0, 112.38, 0, 0, 0, 0, 0, 1373.77, 151.83, 26.66, 
205.5, 129.33, 172.5, 0, 10, 131.09, 0, 0, 0, 0, 0, 689, 0, 0, 
0, 0, 0, 0, 60.6, 183, 98.21, 0, 0, 0, 0, 1433.79, 175.89, 0, 
0, 0, 200, 134.33, 98.26, 112.21, 0, 0, 0, 0, 0, 0, 112.31, 0, 
0, 0, 0, 120, 0, 350, 0, 0, 98.21, 0, 0, 0, 113.24, 0, 0, 0, 
0, 15, 696.65, 321.87, 929, 210.58, 0, 0, 10), .Tsp = c(16563, 
16654, 1), class = ""ts"")
</code></pre>

<p>which looks like this</p>

<p><a href=""http://i.stack.imgur.com/l92zh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/l92zh.png"" alt=""enter image description here""></a></p>

<p>(the time origin is a bit funny, but what matters is that the data is given on a daily basis).
I have tried a few things in R, but essentially I am banging my head against the floor. ARIMA processes seem to lead nowhere, also because the residuals cannot be Gaussian distributed.
I have a time series of strictly non-negative values and I played with tscount and acp (so, I tried to look at this as a Poisson autoregressive process), but without success.
Any suggestion is helpful and some R code is invaluable!</p>
"
"0.212780616013727","0.212218334444056","193384","<p>I am trying to forecast stock market returns using a rolling time frame.
I want to fit a model on a 20 (trading-) day period and then <code>predict</code> one step ahead - the 21st day. I measure the error as the difference between my prediction and the actual value (simplifying things here).
<a href=""http://stats.stackexchange.com/questions/20725/rolling-analysis-with-out-of-sample"">This</a> is the most similar question I could find which makes me think I have done something incorrectly.</p>

<p>I'm having problems getting straight in my head which data I am allowed to use for the modelling step and the prediction step. I think what I might have done it to use information that would (technically) be unavailable to me in a real-world implementation. Can somewhere explain the </p>

<p>I have provided a complete example below to show what I have been doing. Is there an error at the point that I make my prediction, where I am using information from, say tomorrow, to predict tomorrow's outcome?
I have naÃ¯vely used the <code>createTimeSlices</code> function from the {caret} package, but am now thinking I should have also shifted my outcomes column up by 1, before performing any modelling/predictions...</p>

<pre><code>## Packages
library(quantmod)
library(xts)
library(data.table)

## Get data for Dow Jones, S&amp;P500 and Apple
getSymbols(c(""DJIA"", ""GSPC"", ""AAPL""))

## Create the log-returns
dow &lt;- DJIA[""20130111/20150914""][,6]    #extract the adjusted returns
dow &lt;- diff(log(dow))                   #create the log returns
dow &lt;- dow[2:672,]                      #remove first NA element
## Same for GSPC and AAPL
sp500 &lt;- GSPC[""20130114/20150914""][,6]  #extract the adjusted returns
sp500 &lt;- diff(log(sp500))               #create the log returns
sp500 &lt;- sp500[2:672,]                  #remove first NA element
apple &lt;- AAPL[""20130114/20150914""][,6]  #extract the adjusted returns
apple &lt;- diff(log(apple))               #create the log returns
apple &lt;- apple[2:672,]                  #remove first NA element

## Create a data table with all three, keeping a date column - and view it
print(my_data &lt;- data.table(as.data.table(dow), sp500, apple))
##           index DJIA.Adjusted GSPC.Adjusted AAPL.Adjusted
##   1: 2013-01-14   0.001399526   0.004024253  -0.032057998
##   2: 2013-01-15   0.002038986   0.018994382   0.040670461
##   3: 2013-01-16  -0.001749544   0.015202322  -0.006760648
##   4: 2013-01-17   0.002696300  -0.006486296  -0.005345707
##   5: 2013-01-18   0.007500031   0.015071383   0.009494758
##  ---                                                     
## 667: 2015-09-04  -0.016774031   0.004864994   0.027441025
## 668: 2015-09-08   0.023949547   0.013681170  -0.019419791
## 669: 2015-09-09  -0.014604031  -0.010201765   0.021732156
## 670: 2015-09-10   0.004715829   0.003895656   0.014463601
## 671: 2015-09-11   0.006268550   0.005822433   0.009585282

slices &lt;- createTimeSlices(my_data$DJIA.Adjusted,      #essentially supplying time-series length
                           initialWindow = 20,         #20-day frame
                           horizon = 1,                #predict one step ahead only
                           fixedWindow = TRUE)         #rolling frame of fixed size

## Have a look at the train and test sets
str(slices, list.len = 5)

## List of 2
##  $ train:List of 651
##   ..$ Training001: int [1:20] 1 2 3 4 5 6 7 8 9 10 ...
##   ..$ Training002: int [1:20] 2 3 4 5 6 7 8 9 10 11 ...
##   ..$ Training003: int [1:20] 3 4 5 6 7 8 9 10 11 12 ...
##   ..$ Training004: int [1:20] 4 5 6 7 8 9 10 11 12 13 ...
##   ..$ Training005: int [1:20] 5 6 7 8 9 10 11 12 13 14 ...
##   .. [list output truncated]
##  $ test :List of 651
##   ..$ Testing001: int 21
##   ..$ Testing002: int 22
##   ..$ Testing003: int 23
##   ..$ Testing004: int 24
##   ..$ Testing005: int 25
##   .. [list output truncated]

## ================================= ##
##  Fit models and make predictions  ##
## ================================= ##
## Create data table to store results (we'll make 10 predictions)
results &lt;- data.table(actual = rep(0, 10), prediction = rep(0, 10), error = rep(0, 10))

## Use a for-loop to work through all the sets (10 is enough)
for(i in 1:10) {

    ## Model used isn't important - use lm()
    my_fit &lt;- lm(DJIA.Adjusted ~  GSPC.Adjusted + AAPL.Adjusted,
                 my_data[slices$train[[i]]]) #provide rows 1:20

    my_pred &lt;- predict(my_fit, newdata = my_data[slices$test[[i]]])
        real_value &lt;- my_data$DJIA.Adjusted[slices$test[[i]]]
    my_error &lt;- real_value - my_pred

    ## Assign to results
    results$actual[i] &lt;- real_value
        results$prediction[i] &lt;- my_pred
    results$error[i] &lt;- my_error

}

## Combine and inspect
print(my_output &lt;- as.xts(cbind(my_data$index[1:10], results)))
##                   actual    prediction         error
## 2013-01-14  0.0033912188  0.0011792448  0.0022119740
## 2013-01-15 -0.0025562857  0.0021618213 -0.0047181071
## 2013-01-16 -0.0006810993  0.0009277869 -0.0016088862
## 2013-01-17  0.0005988248  0.0008679029 -0.0002690781
## 2013-01-18  0.0038483346  0.0061939031 -0.0023455685
## 2013-01-22 -0.0077337632  0.0010042278 -0.0087379909
## 2013-01-23 -0.0033745466 -0.0006517499 -0.0027227968
## 2013-01-24  0.0086044343  0.0026341100  0.0059703242
## 2013-01-25 -0.0155772387 -0.0017427651 -0.0138344736
## 2013-01-28  0.0083773576  0.0006795398  0.0076978177

## Plot results
matplot(x = my_data$index[1:10], y = results, type = c(""l""), col = 1:4)
legend(""bottomleft"", legend = names(results), col = 1:4, pch = 24)
</code></pre>
"
"0.193637065371959","0.235158540500886","193125","<p>I have fitted a DCC-GARCH model to my multivariate financial data and do the forecasting. Now, I would like to automate the procedure for a data set that I have. </p>

<pre><code># load libraries
library(rugarch)
library(rmgarch)
library(FinTS)
library(tseries)
Dat = dji30retw[, 1:8, drop = FALSE]

uspec = ugarchspec(mean.model = list(armaOrder = c(0,0)), variance.model = list(garchOrder = c(1,1), model = ""sGARCH""), distribution.model = ""norm"")
spec1 = dccspec(uspec = multispec( replicate(8, uspec)), dccOrder = c(1,1), distribution = ""mvnorm"")
fit1 = dccfit(spec1, data = Dat, out.sample = 141, fit.control = list(eval.se=T))
print(fit1)

#Forecast
dcc.focast=dccforecast(fit1, n.ahead = 1, n.roll = 0) 
</code></pre>

<p>I have 1141 observations with 8 assets. I want to fit a multivariate DCC-GARCH model to the first 1000 data points and use the remaining 114 data points as the out of sample forecasting period. For example :-</p>

<pre><code>1) Data[1:1000,] In-sample data, forecast for Data[1001,]
2) Data[1:1001,] In-sample data, forecast for Data[1002,]
3) Data[1:1002,] In-sample data, forecast for Data[1003,]
.. 
4) Data[1:1113,] In-sample data, forecast for Data[1141,]
</code></pre>

<p>How do I automate the process? I have never done looping before but I have tried to do the following.</p>

<p>Please find the example below:-</p>

<pre><code>for (i in 1:2)
{Dat.Initial = dji30retw[, 1:8, drop = FALSE]

 Dat &lt;- Dat.Initial[1:(1000+(i-1)), ] 

  #Fitting the data

  fit1 &lt;- list()
  spec1 &lt;-list()
  uspec = ugarchspec(mean.model = list(armaOrder = c(0,0)), variance.model = list(garchOrder = c(1,1), model = ""sGARCH""), distribution.model = ""norm"")
  spec1[[i]] = dccspec(uspec = multispec( replicate(8, uspec)), dccOrder = c(1,1), distribution = ""mvnorm"")
  fit1[[i]] = dccfit(spec1[[i]], data = Dat, out.sample = 114, fit.control = list(eval.se=T))
  print(summary(fit1[[i]])) }

#Out of sample forecasting
dcc.focast &lt;- list()
dcc.focast[[i]]=dccforecast(fit1, n.ahead = 1, n.roll = 0)
print(dcc.focast[[i]]) 
}
</code></pre>

<p>However, when I run the code, it comes out like this:</p>

<pre><code>  Length  Class   Mode 
   1 DCCfit     S4 
</code></pre>

<p>So where are my results?</p>

<p><strong>Update:</strong> I have obtained the result for the above question by executing the following command.</p>

<pre><code> for (i in 1:2)
 {Dat.Initial = dji30retw[, 1:8, drop = FALSE]
 Dat &lt;- Dat.Initial[1:(1000+(i-1)), ] 

 fit1 &lt;- list()
 uspec = ugarchspec(mean.model = list(armaOrder = c(0,0)), variance.model = list(garchOrder = c(1,1), model = ""sGARCH""), distribution.model = ""norm"")
 spec1 = dccspec(uspec = multispec( replicate(8, uspec)), dccOrder = c(1,1), distribution = ""mvnorm"")
 fit1[[i]] = dccfit(spec1, data = Dat, out.sample = 114, fit.control = list(eval.se=T))
 print(summary(fit1[[i]])) 
 #Out of sample forecasting
dcc.focast &lt;- list()
dcc.focast[[i]]=dccforecast(fit1[[i]], n.ahead = 1, n.roll = 0)
print(dcc.focast[[i]])
} 
</code></pre>

<p>Suppose that I don't want to over-write the fitted model and forecasting objects, though. From the forecasting, I will get the mean return and the variance-covariance matrix. If the for loop is working, I should get 114 values of mean returns and 114 set of variance covariance matrix.</p>

<pre><code>covmat.focast= rcov(dcc.focast)  ##--&gt;Only one covariance matrix.
mean.focast = fitted(dcc.focast)  ##Mean forecast matrix
</code></pre>

<p>Now, I want to get the mean returns forecast. </p>

<pre><code>for (i in 1:2)
{mean.focast &lt;- list()
mean.focast[[i]] = fitted(dcc.focast[[i]]) 
print(mean.focast[[i]]) 
}

#Error in fitted(dcc.focast[[i]]) : error in evaluating the argument 'object' in selecting a method for function 'fitted': Error in dcc.focast[[i]] : this S4 class is not subsettable
</code></pre>

<p>I have not manage to get the variance covariance matrix from the forecast. I have tried to use the following command:-</p>

<pre><code>for (i in 1:2)
{
covmat.focast &lt;- list()
covmat.focast[[i]]= dcc.focast@mforecast[i]
print(covmat.focast[[i]]) }
#Error: trying to get slot ""mforecast"" from an object of a basic class (""list"") with no slots
</code></pre>

<p>Anyone can help me pls?</p>
"
"0.0443678254708057","0.059868434008925","193550","<p>How can we decide the size or portion of the data given to get the ARIMA that has the best forecasting properties?</p>

<p>I mean, for example, we have a hourly series with over 28.000 elements.</p>

<p>Which is the criteria that tells us: do ARIMA over last 100 elements, or 250 last elements, so the ARIMA we get is better for forecasting?
I am interested in short time prediction, like for 24 hours.</p>

<p>I read everywhere but found no criterion yet.</p>
"
"0.0443678254708057","0","193596","<p>If the values of alpha, beta and gamma are not Null it implies that the data has trend and seasonality. What does the negative value of beta or gamma implies? 
ex - gamma is: -0.0000038297240918093. </p>
"
"0.0443678254708057","0.059868434008925","160244","<p>I have a time series which shows an yearly spike around summer but otherwise is predictable by an AR(1) model. The tests on the data also show that the time series shows stationarity and is non-seasonal. How do I model the spike?"
"NaN","NaN","(More details about the time series here: <a href=http://stats.stackexchange.com/questions/159769/what-does-the-following-acf-curve-mean-picture-attached?noredirect=1#comment304724_159769>What does the following ACF curve mean ? (Picture attached)</a>)</p>",""
"NaN","NaN","<p><img src=http://i.stack.imgur.com/8ayfF.png alt=enter image description here></p>",""
"NaN","NaN","","<r><time-series><forecasting><arima>"
"0.0992094737665681","0.133869888150416","65585","<p>I have a daily weather data set, which has, unsurprisingly, very strong seasonal effect.</p>

<p><img src=""http://i.stack.imgur.com/B5Zpo.jpg"" alt=""enter image description here""></p>

<p>I adapted an ARIMA model to this data set using the function auto.arima from forecast package.
To my surprise the function does not apply any seasonal operations- seasonal differencing, seasonal ar or ma components. Here is the model it estimated:</p>

<pre><code>library(forecast)
data&lt;-ts(data,frequency=365)
auto.arima(Berlin)

Series: data
ARIMA(3,0,1) with non-zero mean 

Coefficients:
         ar1      ar2     ar3      ma1  intercept
      1.7722  -0.9166  0.1412  -0.8487   283.0378
s.e.  0.0260   0.0326  0.0177   0.0214     1.7990

sigma^2 estimated as 5.56:  log likelihood=-8313.74
AIC=16639.49   AICc=16639.51   BIC=16676.7
</code></pre>

<p>And also the forecasts using this model are not really satisfying. Here is the plot of the forecast:
<img src=""http://i.stack.imgur.com/IkpIq.jpg"" alt=""enter image description here""></p>

<p>Can anyone give me a hint what is wrong here?</p>
"
"0.172172452513037","0.203283192872215","88841","<p>I am trying to do time series modeling and forecasting using R based on weekly data like below -</p>

<pre><code>biz week     Amount        Count
2006-12-27   973710.7     816570
2007-01-03  4503493.2    3223259
2007-01-10  2593355.9    1659136
2007-01-17  2897670.9    2127792
2007-01-24  3590427.5    2919482
2007-01-31  3761025.7    2981363
2007-02-07  3550213.1    2773988
2007-02-14  3978005.1    3219907
2007-02-21  4020536.0    3027837
2007-02-28  4038007.9    3191570
2007-03-07  3504142.2    2816720
2007-03-14  3427323.1    2703761
...
2014-02-26  99999999.9   1234567
</code></pre>

<p>Regarding my data, as seen above, each week is labeled by first day for the week (my weeks start on Wednesdays and end on Tuesdays).  When I construct my <code>ts</code> object, I tried: </p>

<pre><code>ts &lt;- ts(df, frequency=52, start=c(2007,1))
</code></pre>

<p>The problems I have are: </p>

<ol>
<li>Some years may have 53 weeks, so <code>frequency=52</code> will not work for those years.</li>
<li><p>My starting week / date is <code>2006-12-27</code>, how should I set the start parameter? Should I use: <code>start=c(2006,52) or start=c(2007,1)</code>, since week of <code>2006-12-27</code> really crosses the year boundary?</p>

<p>Also, for modeling, is it better to have complete year worth of data (say for 2007 my start year if I only have partial year worth of data, is it better I should not use 2007, instead to start with 2008. What about 2014 since it is not complete year yet, shall I use what I have for model or not? Either way, I still have issue of whether or not to include those weeks in the year boundary like 2006-12-27, shall I include it as week 1 for 2007 or last week of 2006? </p></li>
<li><p>When I use <code>ts &lt;- ts(df, frequency=52, start=c(2007,1))</code> and then print it, I got results shown below, so instead of <code>2007.01, 2007.02, 2007.52, ...</code>, I got <code>2007.000, 2007.019, ...</code> which it gets from <code>1/52=0.019</code>, which is mathematically correct but not really easy to interpret. Is there a way to label it as the date itself just like data frame or at least <code>2007 wk1, 2007 wk2, ...</code>?</p>

<p>=========</p>

<pre><code>Time Series:
Start = c(2007, 1) 
End = c(2014, 11) 
Frequency = 52 
          Amount        Count
2007.000   645575.4     493717
2007.019  2185193.2    1659577
    2007.038  1016711.8     860777
2007.058  1894056.4    1450101
2007.077  2317517.6    1757219
2007.096  2522955.8    1794512
2007.115  2266107.3    1723002 
</code></pre></li>
<li><p>My goal is to model this weekly data, then try to decompose it to see seasonal component, it seems like I have to use <code>ts()</code> function to convert to <code>ts</code> object then I can use <code>decompose()</code> function, I tried <code>xts()</code> function, and I got error stating ""time series has no or less than 2 periods"" I guess reason is because <code>xts()</code> won't let me specify the frequency? </p>

<pre><code>xts &lt;- xts(df,order.by=businessWeekDate)
</code></pre></li>
<li><p>I looked for the answer in this forum and other places as well, most of the examples are monthly, there are some weekly time series question, but none of the answers are straight forward.</p></li>
</ol>
"
"NaN","NaN","194185","<pre><code>      Date   Products   Sales_Amount"
"NaN","NaN","12/05/2014      Shirt             58",""
"NaN","NaN","12/05/2014       Pant             25",""
"NaN","NaN","13/05/2014     Blouse             41",""
"NaN","NaN","14/05/2014     Blouse             41",""
"NaN","NaN","15/05/2014      Shirt             75",""
"NaN","NaN","18/05/2014      Dress             36",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>My dataframe looks like the above table. I have different products sales transaction on single day and on different days. For example <code>Shirt</code> and <code>Pant</code> are sold on single day <code>12/05/2014</code> then the <code>Shirt</code> is not sold on two consecutive days and then on <code>15/05/2014</code> the shirt sales happen. But I have data for all days for different products.</p>",""
"NaN","NaN","<p>How to forecast the sales amount for every product with the help of time series in R?</p>",""
"NaN","NaN","","<r><time-series><forecasting>"
"0.118314201255482","0.159649157357133","212773","<p>I have half-hourly electricity data of several homes for a duration of one month. This data is represented in <code>xts</code> time-series format. Now, I need to make half-hourly forecasts using the same data for coming day. The forecasting interval is one day (i.e., very short term forecast). I assume that electricity usage follows daywise seasonality as the number of users/occupants remain fixed. This assumption is obeyed in some homes while some other follow random electricity usage. </p>

<p>Currently, I use historical one month data to make half-hourly forecasts of coming day using <code>auto.arima</code> found in <code>forecast</code> pacakage. Using this approach I do get forecasts for next day (48 forecasted values). 48 forecasted values correspond to 48 half-hours of the day. But, I do not know </p>

<ol>
<li>How should I specify the seasonality fact, i.e., how should I mention that data is assumed seasonal day-wise. In other words, how should I mention within historical data of one month, there are 30 periods and each period consist of 48 observations. </li>
<li>Is <code>xts</code> representation suitable for this task or I need to represent this in <code>ts</code> format?</li>
</ol>

<p>Here I have attached half-hourly data of 26 days. I have removed timestamps in order to fit the data according to stackExchange limits. This data does not contain any missing readings. It contains 1248 (26 x 48) observations</p>

<pre><code>data &lt;- structure(c(1642.8, 1467.1, 165.57, 1630.99, 1618.65, 1629.29, 
1598.93, 1839.9, 1604.52, 1606.73, 1473.82, 1669.17, 1698.9, 
2111.21, 2056.41, 3671.29, 2808.01, 1336.15, 794.11, 1212.15, 
377.36, 888.54, 174.58, 218.54, 420.76, 389.58, 397.77, 395.31, 
359.11, 364.8, 376.13, 389.37, 929.5, 1702.38, 519.65, 2452.28, 
1354.45, 1842.96, 725.41, 661.11, 528.44, 733.4, 429.51, 310.47, 
279.72, 407.83, 1791.1, 1754.53, 1536.73, 1608.37, 1432.23, 1401.72, 
1582.14, 1558.75, 1536.24, 1745.59, 1375.61, 1556.71, 1671.12, 
1206.77, 1391.84, 876.23, 1617.3, 1638.99, 1833.61, 1591.42, 
1455, 183.87, 177.55, 184.36, 332.99, 352.95, 425.1, 945.67, 
342.3, 348.45, 227.18, 382.15, 268.91, 335.88, 326.94, 233.23, 
169.71, 179.51, 195.3, 207.23, 1681.9, 1493.32, 941.52, 980.36, 
924.31, 379.02, 1229.89, 1590.21, 1250.92, 1149.24, 1124.04, 
993.78, 883.98, 860.69, 934.17, 969.31, 1049.55, 1104.94, 904.3, 
1220.23, 1183.9, 891.26, 825.33, 787.77, 1060.93, 1029.1, 982.25, 
193.13, 182.65, 181.75, 167.68, 165.89, 291.02, 300.2, 418.4, 
297.66, 231.89, 305.66, 701.18, 338.5, 337.24, 332.11, 332.66, 
187.8, 179.03, 130.22, 177.73, 172.24, 173.45, 334.23, 810.53, 
359.41, 330.23, 333.29, 568.85, 2462.46, 1660.32, 1156.13, 1136.2, 
1189.07, 832.73, 181.91, 185.93, 1076.77, 672.78, 1376.71, 1020.17, 
382.18, 1160.74, 791.36, 1569.4, 817.78, 850.71, 747.21, 826.12, 
1306.46, 506.23, 140.05, 132.6, 304.19, 308.14, 406.13, 290.73, 
188.9, 165.59, 174.56, 145.97, 151.83, 142.29, 443.58, 799.9, 
279.36, 223.88, 221.03, 291.26, 374.97, 431.36, 598.98, 625.82, 
1052.02, 2036.83, 1230.03, 1429.81, 1099.34, 1646.03, 1668.56, 
1631.79, 1604.04, 2849.49, 2998.63, 2476.96, 1601.04, 1216.42, 
2004.2, 1868.51, 1961.91, 1813.35, 1500.22, 1276.94, 1369.29, 
632.43, 238.15, 488.76, 467.6, 330.27, 144.67, 153.36, 924.12, 
1348.18, 799.01, 524.3, 420.5, 264.34, 283.86, 198.95, 206.52, 
217.33, 356.16, 207.83, 197.6, 194.03, 193.01, 249.85, 271.22, 
244.25, 442.5, 660.49, 245.66, 356.13, 443.32, 336.15, 849.74, 
1709.21, 1542.09, 1315.69, 2628.6, 2261.8, 1576.42, 1776.48, 
1239.64, 1401.12, 1106.17, 1378.55, 1315.57, 1141, 1642.63, 2484.13, 
1968.94, 3059.42, 1317.32, 905.05, 484.42, 486.86, 96.79, 183.45, 
173.94, 342, 255.96, 320.54, 106.19, 147.88, 150.77, 176.17, 
344.75, 371.73, 309.46, 237.86, 187.32, 202.61, 292.5, 248.28, 
259.3, 283.67, 365.09, 230.47, 326.15, 350.92, 335.42, 419.39, 
345.31, 1093.22, 1392.87, 1298.11, 919.16, 1654.53, 1045.99, 
558.42, 437.29, 857.9, 758.34, 1220.04, 1390.62, 956.74, 909.93, 
584.67, 409.87, 387.3, 387.93, 1276.28, 871.06, 413.8, 313.2, 
199.5, 330.21, 210.9, 358.28, 352.13, 233.62, 259.18, 123.57, 
255.58, 411.78, 427.65, 318.95, 298.5, 283.23, 279.85, 200.45, 
205.97, 254.24, 307.98, 1090.53, 289.71, 215.14, 286.63, 328.55, 
288.96, 1281.19, 1354.8, 1302.05, 1254.46, 261.95, 270.09, 243.28, 
696.61, 314.27, 241.73, 245.68, 157.74, 222.55, 294.36, 185.46, 
203.49, 182.14, 246.69, 178.26, 397.5, 330.2, 212.02, 248.72, 
265.48, 249.37, 130.59, 248.97, 279.94, 319.07, 358.5, 278.98, 
251.92, 304.66, 455.05, 365.95, 340.93, 287.51, 264.82, 260.18, 
34.35, 35.11, 184.09, 247.18, 160.9, 139.27, 284.96, 296.31, 
252.3, 342.65, 353.03, 380.52, 346.19, 350.06, 218.52, 133.94, 
173.7, 128.26, 167.8, 112.77, 147.8, 129, 170.54, 89.88, 243.08, 
97.61, 190.31, 193.94, 268.17, 233.5, 205.27, 92.29, 167.43, 
168.34, 151.99, 193.84, 379.1, 318.69, 327.28, 487.39, 414.01, 
336.06, 278.02, 168.05, 155.6, 236.4, 264.94, 296.05, 326.46, 
357.43, 356.31, 340.29, 319.81, 312.79, 341.53, 317.36, 309.62, 
440.6, 285.5, 282.06, 288.99, 334.48, 196.54, 144.24, 218.55, 
173.64, 242.29, 251.78, 186.81, 184.36, 141.62, 208.91, 157.53, 
154.03, 139.44, 137.66, 256.75, 1202.05, 177.36, 177.93, 72.83, 
252.9, 231.35, 1090.39, 442.91, 363.12, 248.96, 478.75, 249.64, 
297.29, 227.28, 365.82, 879.7, 488.93, 184.79, 138.13, 151.77, 
123.18, 175.76, 251.84, 208.06, 126.68, 246.3, 307.34, 319.79, 
324.3, 379.6, 309.53, 253.17, 221.91, 228.42, 150.24, 148.59, 
118.79, 86.89, 140.51, 200.43, 212.15, 276.14, 441.81, 125.77, 
152.42, 329.28, 269.21, 177.35, 1106.29, 128.92, 96.35, 63.53, 
520.62, 940.25, 1014.34, 314.99, 390.2, 330.1, 377.04, 341.35, 
342.79, 241.79, 249.9, 391.92, 292.68, 105.02, 179.99, 118.53, 
154.17, 90.53, 206.7, 345.33, 244.75, 291.68, 820.57, 1777.84, 
1805.83, 1753.73, 1416.7, 279.2, 262.82, 1345.88, 467.98, 1136.66, 
170.02, 159.96, 1478.8, 1414.12, 1347.93, 1505.59, 1341.69, 445.53, 
277.59, 1609.61, 1476.45, 244.08, 192.57, 213.55, 439.02, 112.86, 
128.54, 376.09, 251.15, 116.27, 254.82, 302.56, 304.6, 198.5, 
240.05, 219.35, 70.3, 190.96, 211.95, 328.53, 714.62, 3176.56, 
2604.09, 191.65, 145.25, 93.93, 83.6, 81.13, 146.12, 331.75, 
250.24, 1144.53, 1616.47, 1008.7, 316.65, 311.46, 1152.99, 1504.86, 
1543.21, 1081.54, 1428.07, 1358.23, 1349.75, 190.23, 2398.92, 
2196.11, 1466.94, 2249.77, 2150.2, 2542.25, 618.03, 453.22, 880.99, 
1497.86, 440.96, 161.85, 324.88, 434.11, 316.33, 444.66, 359.14, 
277.41, 1237.28, 761.41, 183.53, 309.44, 213.48, 121.64, 346.7, 
149.86, 2060.39, 1102.13, 347.97, 600.24, 912.6, 590.77, 1805.76, 
1673.93, 1573.91, 505.74, 446.76, 1033.41, 1668.68, 1293.9, 383.81, 
1419.99, 1349.4, 711.55, 218.63, 182, 401.93, 1876, 1486.34, 
1543.11, 2313.8, 478.57, 615.19, 542.68, 971.98, 531.11, 766.21, 
489.76, 344.47, 319.86, 321.26, 311.41, 288.67, 310.67, 305.15, 
419.33, 422.84, 950.08, 2188.88, 3454.92, 1989.54, 590.33, 327.05, 
354.78, 578.41, 1583.29, 2016.66, 1481.03, 293.21, 1864.84, 399.65, 
366.76, 357.7, 2074.97, 1626.86, 1133, 1624.61, 1506.93, 628.4, 
1405.68, 217.8, 1223.11, 1356.97, 1171.72, 1182.86, 1642.11, 
2289.02, 814.39, 595.76, 542.78, 1596.41, 884.97, 235.25, 1540.68, 
781.95, 115.71, 1204.98, 718.66, 452.09, 305.58, 444.67, 356.76, 
182.54, 674.47, 153.8, 862.25, 1322.88, 323.33, 1659.64, 496.72, 
304.74, 246.6, 327.12, 239.31, 246.72, 225.72, 234.7, 324.07, 
304.27, 171.86, 97.64, 242.69, 295, 324.53, 513.81, 1100.65, 
1151.77, 231.56, 189.88, 786.3, 1164.87, 676.09, 882.82, 1496.3, 
1027.91, 872.92, 809.1, 840.31, 1302.18, 2055.87, 677.74, 934.66, 
263.91, 186.68, 248.5, 214.62, 371.54, 298, 294.52, 304.86, 1295.77, 
942.5, 305, 265.78, 255.89, 255.63, 151.54, 108.16, 116.81, 100.19, 
224.75, 84.11, 1143.89, 262.15, 784.21, 1728.29, 1506.79, 434.94, 
374.29, 265.43, 560.74, 1651.49, 1063.07, 1054.69, 1298.4, 1261.59, 
1132.75, 692.82, 660.57, 198.25, 97.81, 1258.67, 833.64, 796.35, 
868.76, 999.86, 2240.02, 885.72, 1317.52, 1267.18, 167.93, 133.22, 
364.44, 267.17, 406.13, 412.52, 1036.04, 779.34, 655.43, 1901.2, 
270.18, 266.31, 284.21, 288.66, 135.38, 176.11, 154.86, 160.21, 
146.28, 163.72, 139.75, 278.12, 253.51, 319.62, 396.39, 1662.69, 
1577.09, 1059.71, 241.64, 407.54, 290.49, 846.17, 1325.31, 1418.23, 
1432.5, 1412.6, 1015.87, 1619.88, 1426.58, 1333.32, 1963.35, 
1638.11, 1081.89, 285.56, 1084.58, 2038.77, 1022.39, 1145.92, 
513.87, 107.7, 176.19, 143.77, 374.3, 373.99, 221.76, 148.3, 
331.01, 2323.12, 1502.09, 347.17, 296.7, 306.06, 313.03, 221.69, 
295.35, 301.95, 250.92, 231.54, 140.6, 717.63, 863.5, 402.15, 
1337.78, 1575.44, 1738.49, 1675.57, 1617.66, 1365.58, 242.8, 
286.29, 712.34, 1559.59, 1600.34, 3447.88, 3432.89, 3337.23, 
1472.21, 1323.76, 1265.31, 1221.65, 1312.63, 2016.09, 2972.13, 
1451.67, 735.67, 130.13, 379.87, 162.21, 226.48, 417.18, 357.51, 
346.37, 200.89, 190.15, 276.05, 942.74, 1471.99, 1047.53, 1240.06, 
742.62, 169.34, 144.28, 220.58, 165.23, 344.8, 227.47, 254.64, 
742.37, 1809.01, 436.11, 1692.87, 1697.74, 1474.91, 1635.68, 
1664.2, 1489.85, 1316.4, 1364.07, 1510.02, 1497.89, 1709.17, 
2846.71, 2736.08, 1015.43, 1017.08, 1195.21, 751.18, 523.89, 
199.69, 2148.71, 1151.39, 1182.84, 788.91, 259.31, 146.29, 141.09, 
299.38, 349.53, 404.08, 449.9, 391.77, 251.89, 222.25, 281.04, 
565.4, 371.24, 219.75, 324.45, 227, 157.67, 212.48, 201.69, 140.84, 
220.67, 187.64, 399.79, 157.92, 275.49, 326.99, 1340.51, 1578.68, 
1599.41, 1667.74, 1129.07, 1312.55, 1393.09, 1368.69, 1130.85, 
968.71, 1130.2, 1223.1, 1124.5, 1077.09, 1052.42, 1255.31, 918.21, 
1263.93, 706.21, 3080.29, 1620.18, 1122.48, 750.06, 262.89, 110.02, 
236.83, 413, 227.53, 355, 277.51, 258.03, 368.44, 656.68, 1808.17, 
1493.41, 1272.21, 330.67, 1674.18, 719.45, 1089.68, 784.01, 275.73, 
312.69, 345.11, 761.8, 1020.11, 259.16, 345.98, 270.23, 580.15, 
1303.68, 1659.7, 1732.94, 204.9, 373.58, 373.36, 1381.52, 1437.74, 
1262.78, 1264.6, 1184.54, 1175.12, 857.09, 1428.34, 841.92, 232.47, 
223.22, 473.24, 382.7, 189.84, 1737.48, 1689.34, 378.48, 872.56, 
180.12, 363.03, 301.38, 412.57, 401.17, 387.35, 417.63, 300.61, 
376.82, 284.31, 232.31, 269.96, 188.41, 203.79, 134.88, 193.66, 
57.86, 89.71, 167.66, 60.84, 197.03, 703.66, 1638.7, 1467.03, 
347.22, 1397.23, 1511.92, 1362.25, 1397.18, 1106.19, 826.5, 1033.04, 
1039.46, 584.98, 706.35, 548.07, 373.39, 681.6, 1231.28, 288.94, 
649.36, 79.28, 209.23, 290.75, 304.12, 132.57, 91.2, 355.37, 
197.45, 343.17, 339.2, 284.65, 229, 234.73, 322.36, 323.43, 295.1, 
197.03, 308.17, 223.88, 235.08, 225.16, 172.83, 236.26, 135.17, 
394.89, 479.2, 315.34, 280.9, 282.36, 204.78, 367.24, 1712.29, 
1521, 1686.09, 960.19, 1019.12, 1062.77, 851.88, 1369.98, 689.2, 
580.5, 751.74, 547.67, 556.64, 493.85, 404.15, 428.07, 716.2, 
1442.05, 1045.81, 1497.07, 567.59, 155.07, 537.72, 446.03, 282.57, 
642.88, 409.37, 338.91, 173.89, 358.63, 195.42, 209.95, 186.44, 
152.34, 105.51, 132.26, 82.14, 122.88, 149.38, 211.42, 350.17, 
429.72, 336.4, 982.21, 1436.25, 1726.87, 1830.42, 1282.05, 1293.4, 
1121.01, 946.2, 707.1, 154.53, 767.56, 607.78, 448, 288.23, 270.32, 
223.93, 166.22, 262.45, 223.74, 159.31, 210.44, 257.94, 183.99, 
151.38, 206.11, 193.43, 388.95, 577.98, 304.64, 285.13, 256.59, 
420.26, 289.34, 356.02, 358.08, 325.22, 275.95, 164.46, 213.23, 
142.99, 221.66, 270.61, 206.56, 213.68, 254.33, 250.15, 267.99, 
403.95, 671.2, 1574.11, 396.34, 477.88, 631.08, 618.25, 1366.88, 
298.19, 287.05, 290.38, 332.44, 235.9, 229.79, 831.6, 1320.86, 
477.31, 944.84, 547.33, 411.21, 705.43, 873.49, 572.81, 585.36, 
1229.69, 701.01, 653.49, 74.81, 162.47, 179.54, 330.27, 544.51, 
332.41, 296.66, 130.66, 1055.61, 556.79, 265.43, 383.44, 398.22, 
362.66, 223.99, 130.35, 193.67, 217.68, 273.3, 247.84, 161.66, 
320.08, 322.52, 274.61, 811.44, 353.85, 323.41, 383.61, 389.5
), .Dim = c(1248L, 1L), .Dimnames = list(NULL, ""power""))
</code></pre>
"
"0.0627455805138159","0.0846667513334603","65865","<p>This is the dataset on which I am working currently, which is production data.</p>

<p>Data:</p>

<pre><code>&gt; test.ts
        Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct    Nov    Dec
1990                                                            0.0   10.8  180.0  418.2
1991  561.9  517.9  531.3  448.1  254.9   49.0    3.2    0.0    0.0   10.4  207.7  526.2
1992  597.2  581.5  596.4  518.4  378.3  209.9   32.1    0.0    0.0    7.9  166.7  571.7
1993  650.4  578.5  565.7  280.5   35.3    0.7    0.0    0.0    0.0   39.5  289.2  638.9
1994  643.8  533.8  410.9  159.3    0.0    0.0    0.0    0.0    0.0   38.3  322.8  684.9
1995  695.6  665.8  640.2  415.4  113.0   20.7   12.1    0.0    0.0   13.6  316.3  677.5
1996  754.5  683.4  719.6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  678.0
1997  774.5  808.1  847.9  677.8  208.7    9.9    0.0    0.0    0.0    5.2  296.4  794.9
1998  952.0  873.1  732.0  264.6    3.9    0.0    0.0    0.0    0.0    0.0  245.8  833.0
1999  843.5  812.3  708.3  275.2   10.8    0.0    0.0    0.0    0.0    5.4  300.4  884.9
2000  949.0  898.6  892.7  474.7  130.0   19.8    0.0    0.0    0.0    8.5  367.2 1000.8
2001 1092.1  987.7  864.3  392.8   41.8    0.0    0.0    0.0    0.0    7.0  425.0  968.0
2002  983.6  925.0 1018.0  696.0  209.0   26.0    0.0    0.0    0.0    0.0   63.0  823.0
2003 1066.0  930.0  929.0 1071.0  614.0  125.0   29.0    0.0    0.0    0.5  300.0 1005.8
2004 1043.0 1051.9  863.2  279.1    8.0    0.0    0.0    0.0    0.0   67.8  597.1 1120.3
2005 1087.9 1015.9  855.7  292.9    0.8    0.0    0.0    0.0    0.0   78.3  683.3 1139.2
2006 1185.5 1162.1 1131.3  386.9   16.4    1.2    0.0    0.0    0.0    7.1  728.5 1493.0
2007 1572.9 1341.0 1652.9 1279.3  386.4   14.3    0.0    0.0    0.0    0.0  102.5 1570.1
2008 1864.7 1786.7 1523.9  422.7   48.1    0.8    0.0    0.0    0.0    0.0  192.4 1556.9
2009 1260.8  763.8  284.1    6.1    0.0    0.0    0.0    0.0    0.0    0.0   73.8 1495.6
2010 1280.8 1248.8  887.2  185.6    7.3    0.0    0.0    0.0    0.0    0.8  182.0 1524.9
2011 1461.5 1497.7 1111.5  108.6    0.0    0.0    0.0    0.0    0.0    2.9  519.3 1652.5
2012 1552.5 1563.2 1380.4  295.2    7.7    0.0    0.0    0.0    0.0    0.1  225.0 1677.6
2013 1686.2 1420.0 1691.0  795.0    0.0  
</code></pre>

<p>I used auto.arima() from forecast package.</p>

<p>Code:</p>

<pre><code>ARIMAfit &lt;- auto.arima(test.ts)
test.ar &lt;- forecast(ARIMAfit, level=70, h=12)
</code></pre>

<p>Following is the output I got</p>

<p>Output:</p>

<pre><code>&gt; test.ar
         Point Forecast       Lo 70     Hi 70
Jun 2013      -4.429870  -186.37952  177.5198
Jul 2013      -4.429870  -261.74553  252.8858
Aug 2013      -4.429870  -319.57590  310.7162
Sep 2013      -4.429870  -368.32916  359.4694
Oct 2013      -3.416802  -410.26858  403.4350
Nov 2013     296.121405  -149.56239  741.8052
Dec 2013    1505.197792  1023.80428 1986.5913
Jan 2014    1477.195886   962.56457 1991.8272
Feb 2014    1327.574562   781.72562 1873.4235
Mar 2014    1423.251183   847.87588 1998.6265
Apr 2014     550.206881   -53.25183 1153.6656
May 2014      -1.892754  -632.18482  628.3993
</code></pre>

<p>Questions:</p>

<ol>
<li><p>Why does the output shows negative value, when there has been no negative value in the historic data? The data is of production, which cannot be negative.</p></li>
<li><p>Is there any other model class which handles ""zero"" values appropriately?</p></li>
</ol>

<p>Kindly help.</p>
"
"NaN","NaN","188128","<p>I am currently using Holt-Winters model to predict time series data. When new data becomes available how do I update my model to account for the change in seasonality and trends? </p>"
"NaN","NaN","","<r><time-series><forecasting><predictive-models>"
"0.076847327936784","0.103695169473043","89386","<p>There are several methods to make forecasts of equidistant time series (e.g. Holt-Winters, ARIMA, ...). However I am currently working on the following irregular spaced data set, which has a varying amount of data points per year and no regular time intervals between those points:</p>

<p>Plot:
<img src=""http://i.imgur.com/TV671rN.png"" alt=""plot"">
Sample Data:</p>

<pre><code>structure(list(date = structure(c(664239600, 665449200, 666658800, 
670888800, 672184800, 673394400, 674517600, 675727200, 676936800, 
678146400, 679356000, 680565600, 682984800, 684194400, 685404000, 
686613600, 687823200, 689036400, 690246000, 691455600, 692665200, 
695084400, 696294000, 697503600, 698713200, 699922800, 701132400, 
703548000, 705967200, 707176800, 708472800, 709682400, 710805600, 
712015200, 713224800, 714434400, 715644000, 716853600, 718063200, 
719272800, 720486000, 721695600, 722905200, 724114800, 726534000, 
727743600, 728953200, 730162800, 732668400, 733788000, 734911200, 
737416800, 739144800, 741650400, 744069600, 746575200, 751413600, 
756169200, 761612400, 766533600, 771285600, 776124000, 780962400, 
785804400, 790642800, 795481200, 800316000, 805154400, 808869600, 
813708000, 818463600, 823302000, 828741600, 833580000, 838418400, 
843256800, 848098800, 853542000, 858380400, 863215200, 868053600, 
872892000, 875311200, 880153200, 884991600, 892291920, 897122048, 
901956780, 907055160, 912501900, 917083860, 919500720, 924354660, 
929104882, 934013100, 938851554, 948540840, 958809480, 963647580
), class = c(""POSIXct"", ""POSIXt""), tzone = """"), y = c(3.36153, 
-0.48246, 5.21884, 18.74093, 37.91793, 28.54938, 33.61709, 63.06235, 
68.65387, 77.23859, 87.11039, 84.03281, 93.62154, 99.91251, 100.50264, 
93.77179, 84.5999, 67.36365, 41.30507, 18.19424, 0.958, -15.81843, 
-14.5947, 5.63223, 6.98581, 4.49837, 12.14337, 26.38595, 38.18156, 
39.49169, 45.91298, 64.2627, 65.20289, 95.34555, 98.09912, 102.53325, 
101.76982, 95.17178, 93.00834, 81.43244, 59.84896, 44.55941, 
22.71526, 8.64943, 12.36012, -3.73631, -1.29231, -1.24887, 27.38948, 
33.22064, 28.50297, 39.53514, 52.27092, 64.83294, 79.8159, 107.36236, 
69.52707, 12.95026, 13.36662, 27.65264, 61.13918, 82.24249, 85.89012, 
13.9803, -11.97099, 8.03575, 55.61148, 93.62154, 107.10067, 88.11689, 
18.06141, -32.83151, 18.01798, 60.92196, 100.39437, 112.40503, 
54.1048, 2.59809, 31.10314, 56.46477, 58.4749, 124.68055, 100.5016, 
43.5316, -7.5386, 35.20915, 37.08925, 83.0716, 83.22325, 29.5081, 
-32.7452, -50.63345, 29.00605, 58.2997, 85.3864, 110.4178, -38.66195, 
16.16515, 71.64925)), .Names = c(""date"", ""y""), row.names = c(NA, 
-99L), class = ""data.frame"")
</code></pre>

<p>My first thought was aggregating the data by calculating monthly averages. However this will lead to many months with missing values and secondly accuracy will be lost if multiple values within a month are replaced by a mean aggregat. To solve the first problem one could propose to calculate quarterly aggregates. But in this case the data sample would get relatively small.</p>

<p>So my question is how your approach would look like to make a forecast of the next data point for the given data set (if possible with R). Are there any best practices to handle the irregular spaced time series?</p>
"
"0.0887356509416114","0.11973686801785","89422","<p>I have many time series(retail data). Some with trends, some seasonal, 
and some with neither. With period day, week or month. I need to make forecast, for each time serie. </p>

<p>I'm looking for the most efficient methods for forecasting in R ?
Which significant things should I know for it? 
Maybe someone has experience with random forest forecasting and would share with me?</p>

<p>Any help would be truly appreciated.</p>

<p>UPDATE 1:
For example, one of mine time series is x:</p>

<pre><code>   &gt; dput(x)
 c(1.07328072153326, 1.07385697538101, 1.10947204968944, 1.10501567398119, 
1.08808510638298, 1.07468423942889, 1.06658878504673, 1.10157194679565, 
1.10297619047619, 1.09510682288077, 1.07372549019608, 1.08457943925234, 
1.09101316542645, 1.10577472841624, 1.08926553672316, 1.0929326655537, 
1.08484848484848, 1.09699769053118, 1.10987124463519, 1.08726673984632, 
1.09157959434542, 1.10070384407147, 1.08625486922649, 1.11432506887052, 
1.0828313253012, 1.08040626322471, 1.07157157157157, 1.08369098712446, 
1.08045977011494, 1.10748560460653, 1.11616161616162, 1.08371040723982, 
1.10213414634146, 1.06835306781485, 1.07926829268293, 1.08721886999451, 
1.10216718266254, 1.1241610738255, 1.08231707317073, 1.07698961937716, 
1.08569953536396, 1.09771181199753, 1.07181984175289, 1.07288828337875, 
1.07820419985518, 1.07210031347962, 1.07450628366248, 1.06662870159453, 
1.07235494880546, 1.0979020979021, 1.08494690818239, 1.06716417910448, 
1.08305369127517, 1.08023307933662, 1.07635746606335, 1.07701786814541, 
1.08310249307479, 1.0768253968254, 1.096, 1.06787687450671, 1.07353535353535, 
1.11226993865031, 1.07641196013289, 1.08066298342541, 1.09431605246721, 
1.06678539626002, 1.06646525679758, 1.09977728285078, 1.07646420824295, 
1.0973341599504, 1.0906432748538, 1.09831824062096, 1.09302325581395, 
1.08199121522694, 1.073753605274, 1.0616937745373, 1.07997481108312, 
1.08239202657807, 1.08798283261803, 1.07748776508972, 1.0552611657835, 
1.0817746846455, 1.08978032473734, 1.08414985590778, 1.08205756276791, 
1.11405835543767, 1.11866969009826, 1.07441154138193, 1.09642703400775, 
1.07393209200438, 1.08049535603715, 1.09371428571429, 1.09732824427481, 
1.10526315789474, 1.11575091575092, 1.08680994521702, 1.10028929604629, 
1.09176340519624, 1.07464266807835, 1.10190664036818, 1.08295281582953, 
1.08928571428571, 1.09341998375305, 1.0958605664488, 1.07885714285714, 
1.07466814159292, 1.09463722397476, 1.07281903388609, 1.0812324929972, 
1.08226102941176, 1.07101616628176, 1.08390410958904, 1.08528528528529, 
1.09333333333333, 1.08073929961089, 1.09380234505863, 1.08012968967114, 
1.07717391304348, 1.07066508313539, 1.06838106370544, 1.07199032062916, 
1.08235294117647, 1.08157524613221, 1.11082474226804, 1.08620689655172, 
1.08299477655252, 1.10016420361248, 1.10140093395597, 1.08766485647789, 
1.10094850948509, 1.13925191527715, 1.11293859649123, 1.12204234122042, 
1.10141364474493, 1.11103495544894, 1.09365558912387, 1.10044313146233, 
1.11116279069767, 1.11053240740741, 1.09810671256454, 1.09899823217443, 
1.10986101919259, 1.09649805447471, 1.08765778401122, 1.09922928709056, 
1.07868303571429, 1.07439104674128, 1.08457374830852, 1.09739714525609, 
1.0873440285205, 1.07574536663981, 1.10498812351544, 1.11056105610561, 
1.09443402126329, 1.09200240529164, 1.1076573161486, 1.10090237899918, 
1.09986225895317, 1.10569105691057, 1.09090909090909, 1.10409356725146, 
1.107, 1.15349143610013, 1.08992562542258, 1.09016393442623, 
1.08549783549784, 1.07950780880265, 1.08859223300971, 1.06225680933852, 
1.08606557377049, 1.07929176289453, 1.09641873278237, 1.07554585152838, 
1.05761316872428, 1.08054085831864, 1.09245172615565, 1.09028727770178, 
1.06859756097561, 1.08278388278388, 1.06620808254514, 1.07001522070015, 
1.06319485078994, 1.06764705882353, 1.08654416123296, 1.09310113864702, 
1.06369008535785, 1.13811922753988, 1.12487100103199, 1.14294330518697, 
1.15353181552831, 1.14426229508197, 1.1380042462845, 1.16727806309611, 
1.09280544912729, 1.10660426417057, 1.13093858632677, 1.12244897959184, 
1.09134045077106, 1.10821382007823, 1.09921875, 1.12583967756382, 
1.0998268897865, 1.10657894736842, 1.12752114508783, 1.08413001912046, 
1.14484272128749, 1.0859167404783, 1.09041501976285, 1.0887537993921, 
1.05695364238411, 1.04765146358067, 1.04174820613177, 1.05854800936768, 
1.04042904290429, 1.07479752262982, 1.07179197286603, 1.05997624703088, 
1.06460369163952, 1.07920193470375, 1.081811541271, 1.08351810790835, 
1.0703933747412, 1.07135523613963, 1.0532319391635, 1.05964730290456, 
1.07206703910615, 1.07498383968972, 1.05938566552901, 1.08185840707965, 
1.06121372031662, 1.05117647058824, 1.0734494015234, 1.05576208178439, 
1.08180628272251, 1.06072555205047, 1.09534671532847, 1.08269794721408, 
1.0863453815261, 1.07660577489688, 1.11460957178841, 1.09818731117825, 
1.06873428331936, 1.08247925817472, 1.06818181818182, 1.09494725152693, 
1.11903160726295, 1.10917361637604, 1.09464701318852, 1.10445468509985, 
1.08333333333333, 1.06683804627249, 1.06380575945793, 1.07498766650222, 
1.07160253287871, 1.07565588773642, 1.05174927113703, 1.07279344858963, 
1.06560283687943, 1.06727037516171, 1.05085682697623, 1.06547285954113, 
1.08014705882353, 1.0575296108291, 1.05748725081131, 1.04852071005917, 
1.05421686746988, 1.05314846909301, 1.0538885486834, 1.04618937644342, 
1.04105344694036, 1.06053604436229, 1.06058788242352, 1.04755700325733, 
1.04994511525796, 1.05405405405405, 1.06622516556291, 1.07163323782235, 
1.07538994800693, 1.06018957345972, 1.07800751879699, 1.07815198618307, 
1.07247665629169, 1.07490217998882, 1.06998939554613, 1.05968331303289, 
1.05139565795304, 1.07414104882459, 1.09087423312883, 1.06742556917688, 
1.06096361848574, 1.07464929859719, 1.09754281459419, 1.10085400569337, 
1.08974358974359, 1.09106168694922, 1.09333865177503, 1.08897569444444, 
1.07627737226277, 1.14392723381487, 1.06422018348624, 1.07022471910112, 
1.07848837209302, 1.06617647058824, 1.0828331332533, 1.08257858284497, 
1.07761904761905, 1.06547619047619, 1.07017543859649, 1.06287069988138, 
1.09431751611013, 1.09341500765697, 1.06916019760056, 1.06135831381733, 
1.06491326245104, 1.06208955223881, 1.06825232678387, 1.06939409632315, 
1.05837912087912)
</code></pre>

<hr>

<pre><code>  x&lt;-ts(x, frequency=7)
</code></pre>

<p>When I try to:</p>

<pre><code>  plot(forecast(ets(x),h=60))
  plot(forecast(x,h=60))
</code></pre>

<p><img src=""http://i.stack.imgur.com/qubPZ.png"" alt=""plot""></p>

<p>I get the same results. Maybe someone could explain, why exponential smoothing in this case makes no difference?</p>

<p>Also I have tryed to use </p>

<pre><code> &gt; plot(forecast(auto.arima(x),h=60))

 Warning message:
In auto.arima(x) :
  Unable to fit final model using maximum likelihood. AIC value approximated
</code></pre>
"
"0.188236741541448","0.225778003555894","89531","<p>I'm expanding a question I posed earlier because I think it was lacking detail. </p>

<p>I'm attempting to forecast daily demand for a restaurant that sells take away food, primarily to office workers on their lunch breaks. They are located in the downtown core of a major city.</p>

<p>They are only open on workdays - no holidays, no weekends. I'm familiar with models that take into account seasonality and trend - Holt-Winters triple exponential smoothing, for example. I'm also familiar with models that take into account complex seasonality and trend - the TBATS package for R, for example.</p>

<p>My problem is that I've identified 8 components that determine sales on a given day:</p>

<ol>
<li>The yearly seasonal component. Sales are lower in the summer, for example, when many office workers are on vacation.</li>
<li>The weekly component. Sales very obviously peak on Thursdays (in the absence of other effects - see below)</li>
<li>The <em>Friday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the coming Friday is a holiday. Wednesday will typically have higher sales, for example.</li>
<li>The <em>Post-Friday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the week before was shortened due to the Friday being a holiday.</li>
<li>The <em>Monday-Long-Weekend-Effect</em>. The weekly sales pattern changes in week $t$ if the Monday in week $t+1$ is a holiday. For example, sales are much lower on Fridays preceding Monday-Long-Weekends. Presumably people are leaving the office early and skipping lunch.</li>
<li>The <em>Post-Monday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the week is shortened due to the Monday being a holiday.</li>
<li>The trend component. </li>
<li>The noise component.</li>
</ol>

<p>If holidays fell on the same date every year, then the ""long-weekend-effects"" would be captured in the yearly seasonal component. However, they don't. </p>

<p>My first thought was to include dummy variables. For example, let $X_{M+1}$ be the ""Monday-long-weekend-effect"" component, and $\beta_{M+1}$ be the associated coefficient, for a given day. Then for the Friday preceding a Monday-Long-Weekend, $X_{M+1}=1$, and for a Friday not preceding a Monday-Long-Weekend, $X=0$.</p>

<p>I'm only using three years of data, so it would be easy for me to change the $X_{M+1}$ values to 0 or 1 by hand for each year. However, I don't know how to include such dummy variables in models like those that I've mentioned.</p>

<p>Any input as to a model that can take into account the components I've mentioned would be greatly appreciated. It seems like I need to capture moving-holiday-effects, day-of-the-week effects, seasonal patterns, and trend, all in one.</p>

<p><strong>Question: Is there a model I can use that can be implemented in R and take into account the components I've listed?</strong></p>

<p><em>My background: I'm a forth year mathematics and economics student. I've also taken statistics classes, and I'm using R to perform my analysis. This is for a final report for a forth year data analysis class.</em></p>
"
"0.117386232408469","0.158396987770497","194635","<p>I'm having a doubt with a time series. I have to find the best model for it and use it to do some forecast. The data are about the arctic oscillation (AO) from 1950 to 2015.</p>

<p><a href=""http://i.stack.imgur.com/NA84u.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NA84u.png"" alt=""Plot of the ts""></a></p>

<p>The series is clearly stationary, and the augmented Dickey-Fuller (ADF) test confirms it.</p>

<p>The ACF and PACF for absolute values of the series are depicted below.</p>

<p><a href=""http://i.stack.imgur.com/DyLWQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DyLWQ.png"" alt=""enter image description here""></a></p>

<p>Running <code>seasonplot</code> from ""forecast"" package in R, I can see that in the summer months the values look clearly more clustered, while during the end/beginning of the year the values are more scattered.</p>

<p>The question is: How can I find the best model? Based on the PACF, before seeing the seasonality, I chose an AR(1), and it was good till I discovered the seasonal thing. How can I find the best model to do forecasting?</p>

<p>Thanks!</p>
"
"0.0627455805138159","0.0846667513334603","147525","<p>I have a dataset for weekly number of calls to  a call center for three years.The data is seasonal (I know this from practitioners knowledge) which means that calls normally come on summer and winter. However, without having practitioners knowledge, how can I check if my data is intermittent or this is seasonality which is impacting zeros for specific time intervals. Is there anyway in R that I can do that? The reason is that I need to do further analysis on those parts of the data which are not zero and I need to classify my data to in-season and off-season. Any help is appreciated.</p>
"
"0.0887356509416114","0.0898026510133874","48921","<p>I'd like to use exponential smoothing to forecast the following data. 
The data is daily based. Because of some policy reasons, every $29^\text{th}$, $30^\text{th}$ and $31^\text{th}$ of each month, the data will drop to just hundreds.</p>

<ol>
<li>Do I need to take out the two/three days at the end of each month since they are not following the same pattern with other days of each month? </li>
<li>If I use <code>Holtwinter()</code> to do the triple exponential smoothing, R will show error message: <code>time series has no or less than 2 periods.</code></li>
<li>If I remove the two days at the end of each month, the time range would be <code>3/12/1998</code>  to <code>3/28/1998</code>, <code>4/1/1998</code> to <code>4/28/1998</code>, <code>5/1/1998</code> to <code>5/28/1998</code>. How can I run R since the data are not in the complete month period?</li>
</ol>

<pre>
18000   3/12/1998
61000   3/13/1998
59000   3/14/1998
59000   3/15/1998
66000   3/16/1998
38000   3/17/1998
37000   3/18/1998
20000   3/19/1998
72000   3/20/1998
44000   3/21/1998
37000   3/22/1998
33000   3/23/1998
28000   3/24/1998
54000   3/25/1998
24000   3/26/1998
66000   3/27/1998
52000   3/28/1998
280     3/29/1998
200     3/30/1998
400     3/31/1998
186000  4/1/1998
31000   4/2/1998
82000   4/3/1998
39000   4/4/1998
58000   4/5/1998
26000   4/6/1998
41000   4/7/1998
37000   4/8/1998
19000   4/9/1998
65000   4/10/1998
54000   4/11/1998
55000   4/12/1998
56000   4/13/1998
40000   4/14/1998
34000   4/15/1998
27000   4/16/1998
72000   4/17/1998
56000   4/18/1998
56000   4/19/1998
60000   4/20/1998
39000   4/21/1998
43000   4/22/1998
22000   4/23/1998
63000   4/24/1998
35000   4/25/1998
36000   4/26/1998
34000   4/27/1998
43000   4/28/1998
300     4/29/1998
250     4/30/1998
133000  5/1/1998
28000   5/2/1998
63000   5/3/1998
65000   5/4/1998
33000   5/5/1998
29000   5/6/1998
21000   5/7/1998
75000   5/8/1998
34000   5/9/1998
77000   5/10/1998
54000   5/11/1998
32000   5/12/1998
26000   5/13/1998
19000   5/14/1998
64000   5/15/1998
54000   5/16/1998
64000   5/17/1998
58000   5/18/1998
29000   5/19/1998
29000   5/20/1998
16000   5/21/1998
62000   5/22/1998
32000   5/23/1998
38000   5/24/1998
29000   5/25/1998
38000   5/26/1998
36000   5/27/1998
34000   5/28/1998
160     5/29/1998
150     5/30/1998
</pre>
"
"0.166379345515521","0.209539519031237","213159","<p>I have two time series $d_t(t)$, $d_c(t)$, where I'm modelling charge as a function of time. Lengths of time series, $N$ are equal to $101$ data points. For the $d_t(t)$ (test sample, short-term) the time interval between observations is $t_1 = 0.1$ sec, and for the $d_c(t)$ (control sample, long-term) the time interval is $t_2 = 1$ sec. Thus, the right boundary of time for $d_t(t)$ is equal to $0.1\times 100 = 10$ sec, and the right boundary of time for $d_c(t)$ is equal to $1 \times 100 = 100$ sec. Both $d_t(t)$, $d_c(t)$ are correspond to the one experiment. The accuracy of measurements is $tol=0.001$. </p>

<p>Here's some sample data and visualization,</p>

<pre><code>dtest &lt;- data.frame(
   TIME = seq(0,100,by=1),
   CHARGE=c(-1.824, -1.150, -1.112, -1.097, -1.090, -1.085, -1.080, -1.075, -1.072, -1.069, 
            -1.067, -1.064, -1.061, -1.060, -1.058, -1.056, -1.055, -1.052, -1.051, -1.050, 
            -1.049, -1.048, -1.048, -1.045, -1.044, -1.043, -1.042, -1.041, -1.040, -1.039, 
            -1.038, -1.037, -1.037, -1.036, -1.036, -1.034, -1.034, -1.033, -1.032, -1.032, 
            -1.031, -1.031, -1.030, -1.030, -1.029, -1.029, -1.028, -1.027, -1.027, -1.028, 
            -1.028, -1.026, -1.025, -1.025, -1.026, -1.024, -1.025, -1.023, -1.023, -1.023, 
            -1.023, -1.023, -1.022, -1.021, -1.020, -1.020, -1.020, -1.019, -1.019, -1.018, 
            -1.018, -1.018, -1.018, -1.017, -1.016, -1.017, -1.017, -1.016, -1.015, -1.015, 
            -1.015, -1.014, -1.014, -1.013, -1.013, -1.012, -1.012, -1.011, -1.011, -1.011, 
            -1.011, -1.010, -1.011, -1.010, -1.010, -1.009, -1.008, -1.008, -1.008, -1.008, -1.008))


dcont &lt;- data.frame(
   TIME = seq(0,100,by=1),
   CHARGE=c(-1.827, -1.071, -1.056, -1.047, -1.039, -1.034, -1.030, -1.027, -1.025, -1.020, 
            -1.017, -1.016, -1.013, -1.010, -1.009, -1.006, -1.007, -1.004, -1.004, -1.002, 
            -1.000, -0.999, -0.997, -0.997, -0.995, -0.995, -0.993, -0.991, -0.991, -0.991, 
            -0.989, -0.988, -0.988, -0.986, -0.985, -0.984, -0.984, -0.984, -0.982, -0.982, 
            -0.981, -0.981, -0.979, -0.978, -0.977, -0.976, -0.975, -0.975, -0.975, -0.974, 
            -0.973, -0.973, -0.972, -0.972, -0.971, -0.970, -0.970, -0.970, -0.969, -0.967, 
            -0.966, -0.966, -0.966, -0.966, -0.966, -0.965, -0.965, -0.964, -0.964, -0.963, 
            -0.962, -0.961, -0.962, -0.962, -0.962, -0.960, -0.960, -0.959, -0.959, -0.958, 
            -0.958, -0.958, -0.958, -0.957, -0.956, -0.956, -0.955, -0.955, -0.955, -0.955, 
            -0.955, -0.954, -0.953, -0.953, -0.954, -0.952, -0.952, -0.951, -0.952, -0.951, -0.952))
</code></pre>

<p><a href=""http://i.stack.imgur.com/Tg2cT.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Tg2cT.jpg"" alt=""enter image description here""></a></p>

<p>As we can see from figure above the data from start region is increasing exponentially $f(x)=(1+exp(-x/t))$, later they are increasing (stationary) with linear rule, like $f(x)=kx+b$.</p>

<p>I'd like to forecast a value of $d_t(t)$ at time point $t=100$ sec, i.e. I have 10 seconds of history and want to forecast out 90 seconds. Then verify the forecasting $d_t^p(t=100)$ with corresponding value from the $d_c(t=100)$. The forecasting is satisfactory, if $$0.95 \times d_c(t) \leq d_t^p(t)\leq 1.05 \times d_c(t).$$
On physical grounds, the experimental data can be described with an exponential function $f(x) = a \times (1+exp(-(x/\tau)))$, where $a$, $\tau$ are parameters. I have been doing curve-fitting $d(t)$ in <strong>R</strong> using <code>nlminb</code>. The initial values of the parameters for optimization are chosen based on the physical characteristics of the process: <code>parConv &lt;- c(a=-0.762,tau=5.88)</code>. The result is below: $a= -1.03084$, $\tau= 0.50464$. Unfortunatly, the forecasting $f(t=100)=-1.030845$ does not satisfy to the range: </p>

<p>$$f(t=100) = -1.030845 \bar{\in} [0.95 \cdot (-0.952), 1.05 \cdot (-0.952)]=[-0.9044, -0.9996].$$</p>

<p>I have tried to split the original time series $d_t(t)$ into two parts: $d_{t_1}(t=1..10)$ and $d_{t_2}(t=11..101)$, then I have approximated the $d_{t_2}(t=11..101)$ using a linear function $f_1(x)=kx+b$ and a polynomial $f_2(x)=ax^2+bx+c$. </p>

<p>Result are better but not satisfy to the range: </p>

<p>$f_1(x)=-1.05851+x \cdot 5.61364\cdot 10^{-4}$, and $f_2(x)=-1.0718+x\cdot 0.0012+x^2 \cdot (-5.86943\cdot 10^{-6})$, <code>Adj. R-Square = 0,9815</code>. Then I have obtained the forecasts: $f_1(t=100)=-1.002374$ and $f_2(t=100)= -1.010494$. </p>

<p><strong>My question is</strong>: How to improve a bad long-term forecasting of time series in common case?</p>

<p><strong>Another possible solutions are:</strong></p>

<ol>
<li><p>Apply function <code>ln()</code> to the original data $d_t(t)$ and repeat fitting.</p></li>
<li><p>To use an exponential function and to assign greater weight to the $k$ last points (like <a href=""http://stackoverflow.com/questions/33539287/how-to-force-specific-points-in-curve-fitting"">here</a>).</p></li>
<li><p>To use some alternative models, for example, <a href=""https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model"" rel=""nofollow"">Autoregressive moving-average model</a>, _https://en.wikipedia.org/wiki/Backcasting.</p></li>
</ol>

<p>Example code:</p>

<pre><code>library(minpack.lm)
library(ggplot2)
library(optimx)

d &lt;- data.frame(
   TIME = seq(0,100,by=1),
   CHARGE=c(-1.824, -1.150, -1.112, -1.097, -1.090, -1.085, -1.080, -1.075, -1.072, -1.069, 
            -1.067, -1.064, -1.061, -1.060, -1.058, -1.056, -1.055, -1.052, -1.051, -1.050, 
            -1.049, -1.048, -1.048, -1.045, -1.044, -1.043, -1.042, -1.041, -1.040, -1.039, 
            -1.038, -1.037, -1.037, -1.036, -1.036, -1.034, -1.034, -1.033, -1.032, -1.032, 
            -1.031, -1.031, -1.030, -1.030, -1.029, -1.029, -1.028, -1.027, -1.027, -1.028, 
            -1.028, -1.026, -1.025, -1.025, -1.026, -1.024, -1.025, -1.023, -1.023, -1.023, 
            -1.023, -1.023, -1.022, -1.021, -1.020, -1.020, -1.020, -1.019, -1.019, -1.018, 
            -1.018, -1.018, -1.018, -1.017, -1.016, -1.017, -1.017, -1.016, -1.015, -1.015, 
            -1.015, -1.014, -1.014, -1.013, -1.013, -1.012, -1.012, -1.011, -1.011, -1.011, 
            -1.011, -1.010, -1.011, -1.010, -1.010, -1.009, -1.008, -1.008, -1.008, -1.008, -1.008))


d1 &lt;- data.frame(
   TIME = seq(0,100,by=1),
   CHARGE=c(-1.827, -1.071, -1.056, -1.047, -1.039, -1.034, -1.030, -1.027, -1.025, -1.020, 
            -1.017, -1.016, -1.013, -1.010, -1.009, -1.006, -1.007, -1.004, -1.004, -1.002, 
            -1.000, -0.999, -0.997, -0.997, -0.995, -0.995, -0.993, -0.991, -0.991, -0.991, 
            -0.989, -0.988, -0.988, -0.986, -0.985, -0.984, -0.984, -0.984, -0.982, -0.982, 
            -0.981, -0.981, -0.979, -0.978, -0.977, -0.976, -0.975, -0.975, -0.975, -0.974, 
            -0.973, -0.973, -0.972, -0.972, -0.971, -0.970, -0.970, -0.970, -0.969, -0.967, 
            -0.966, -0.966, -0.966, -0.966, -0.966, -0.965, -0.965, -0.964, -0.964, -0.963, 
            -0.962, -0.961, -0.962, -0.962, -0.962, -0.960, -0.960, -0.959, -0.959, -0.958, 
            -0.958, -0.958, -0.958, -0.957, -0.956, -0.956, -0.955, -0.955, -0.955, -0.955, 
            -0.955, -0.954, -0.953, -0.953, -0.954, -0.952, -0.952, -0.951, -0.952, -0.951, -0.952))


(g1 &lt;- ggplot(d,aes(TIME,CHARGE))+geom_point())
g1+geom_smooth()  ## with loess fit

# Parameter choices:
parConv &lt;- c(a=-0.762,tau=5.88) #

#Perturbed parameters:
parStart      &lt;- parConv
parStart[""a""] &lt;- parStart[""a""]+3e-4

Ebos &lt;- -1.161 # start value at x=0

#The formulae:
RCCircuits&lt;-function(parS,x)
    with(as.list(parS), 
                       ifelse(x==0, Ebos, a*(1+exp(-(x/tau))) ) 
         )

# A sum-of-squares function
ssqfun &lt;- function(parS, Observed, x) {
   sum(ResidFun(parS, Observed, x)^2)
}

# Local minimizer for smooth nonlinear functions subject to bound-constrained parameters
opt1 &lt;- nlminb(start=parStart, objective = ssqfun,
    Observed = d$CHARGE, x = d$TIME,
    control= list(eval.max=5000,iter.max=5000))

parNLM &lt;- opt1$par

#SSE Review:
sapply(list(parConv,parNLM),
  ssqfun,Observed=d$CHARGE,x=d$TIME)  

pred0 &lt;- RCCircuits(as.list(parConv), d$TIME)
pred1 &lt;- RCCircuits(as.list(parNLM),  d$TIME)

# forecasting at t=100 sec
pred100 &lt;- RCCircuits(as.list(parNLM), 1000) 
</code></pre>
"
"NaN","NaN","119931","<p>How can I in R fit a time series, $x_t$, with external regressors, $v_t$, and an autoregressive error? This time series model is given as follows,
$x_t = \beta v_t + \epsilon_t$ where $\epsilon_t = w_t + \sum_{i = 1}^p \gamma_i\epsilon_{t - i}$ and $w_t \sim N(0, \sigma^2)$.</p>
"
"0.140886767884104","0.190107810700578","212840","<p>I read Chen et al. <a href=""http://onlinelibrary.wiley.com/doi/10.1002/for.1134/abstract"" rel=""nofollow"">""Forecasting volatility with support vector machine-based GARCH model""</a> (2010) where they implented a recurrent SVM procedure to estimate volatility by a GARCH based model. 
The model is of the form </p>

<p>$y_t = f(y_{t-1}) + u_t \qquad \qquad \ \ \ (1)$ </p>

<p>$u^2_t = g(u^2_{t-1}, w_{t-1}) + w_t \qquad  (2)$ </p>

<p>At first they got estimates for $u_t$ by estimating $(1)$ by a SVM. Then, the following recurrent SVM algorithm was proposed to estimate $(2)$.</p>

<hr>

<p><strong><em>Recurrent SVM Algorithm:</em></strong></p>

<p><strong>Step 1:</strong> Set $i = 1$ and start with all residuals at zero: $w_t^{(1)} = 0 $.</p>

<p><strong>Step 2:</strong> Run an SVM procedure to get the decision function $f^{(i)}$ to the points $\{x_t, y_t\} = \{u_{t - 1}^2, u_t^2 \}$ with all inputs $x_t = \{u_{t - 1}^2, w_{t-1} \}$</p>

<p><strong>Step 3:</strong> Compute the new residuals $w_t^{i+1} = u_t^2 - f^{(i)}$.</p>

<p><strong>Step 4:</strong> Terminate the computaion process if the stopping criterion is satisfied; otherwise, set $i = i + 1$ and go back to Step 2.</p>

<hr>

<p>The proposed stopping critrerion is based a Ljung-Box-Test for the residuals $w_t$. Only if the $p$-values of the test in five consecutive periods are higher than 0.1 the process is stopped. </p>

<p>As real world example the log-returns of the New York Stock Exchange (NYSE) composite stock index for the period from January 8, 2004 to December 31, 2007 was used. The last 60 observations where used as test sample. Hence, the estimation was done with the first 940 observations. In their study, the process converged after 121 interations. <strong>(Question:) However, my implementation in R does not converge. I think I have a misunderstanding of the concept.</strong> Because I think I implemented it exactly as stated. My R code is the following</p>

<pre><code>rm(list = ls())

library(quantmod)
library(e1071)

#Get NYSE data and convert to log returns
id     &lt;- ""^NYA""
data   &lt;- getSymbols(id, source = ""yahoo"", auto.assign = FALSE, 
                     from = ""2004-01-08"", to = ""2007-12-31"")
series &lt;- data[,6]  #Get adjusted closing prices
series &lt;- na.omit(diff(log(series)))*100  #Compute log returns

#Lagged data for analysis
x      &lt;- na.omit(cbind(series, lag(series)))

#Set parameters as in paper
svm_eps   &lt;- 0.05
svm_cost  &lt;- 0.005
sigma     &lt;- 0.02
svm_gamma &lt;- 1/(2*sigma^2)


#SVM to get u_t
svm     &lt;- svm(x = x[,-1], y = x[,1], scale = FALSE,
               type = ""eps-regression"", kernel = ""radial"",
               gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

u    &lt;- svm$residuals  #Extract u_t
n    &lt;- 60  #Size of test set
u_tr &lt;- u[1:(nrow(u) - n)]  #Subset to training set
u_tr &lt;- na.omit(cbind(u_tr, lag(u_tr)))^2  #Final training set


#Recurrent SVM for vola estimation
i       &lt;- 1
p_count &lt;- 0

while(p_count &lt; 5){

  print(i)  #Print number of loops

  #Estimate SVM for u^2
  svmr     &lt;- svm(x = u_tr[,-1], y = u_tr[,1], scale = FALSE,
                  type = ""eps-regression"", kernel = ""radial"",
                  gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

  #Test autocorrelation of residuals to lag 1
  test    &lt;- Box.test(svmr$residuals, lag = 1, type = ""Ljung-Box"")
  p_val   &lt;- test$p.value
  p_count &lt;- ifelse(p_val &gt; 0.1, p_count + 1, 0)

  #Extract residuals for next estimation step
  w        &lt;- svmr$residuals
  w        &lt;- c(0, w[-length(w)])  #lag 1

  u_tr &lt;- cbind(u_tr[,1:2], w)

  i &lt;- i + 1
}
</code></pre>
"
"0.188236741541448","0.239889128778138","213192","<p>I have half-hourly electricity data of several homes for a duration of one month. Also, I have ambient temperature at same sampling rate. Now, I need to make half-hourly forecasts using historical electricity data and forecasted temperature for coming day.</p>

<p>Currently, I am facing a problem with seasonality, i.e., in some homes I observe (via visual inspection) day-wise seasonality and in some other homes no pattern is found. <a href=""http://stats.stackexchange.com/a/212797/60072"">Stephan</a> suggested to check for weekly seasonality as well, but visually I do not find any. So, I thought to try models with different forced seasonalities to find the prediction accuracy. I can think of two options:</p>

<ol>
<li>Model_1 with daily seasonality (frequency = 48 observations)</li>
<li>Model_2 with weekly and daily seasonality (48, 7*48)</li>
</ol>

<p>Keeping the above approach in mind, I am facing the following issue:</p>

<ol>
<li>For Model_1, I can use <code>auto.arima()</code> with <code>xreg</code> option to specify an extra <code>temperature</code> predictor. However, this only works for a single seasonality.</li>
<li>For Model_2, I tried to use the <code>tbats()</code> forecasting function, which models multiple seasonalities, but does not allow extra predictor variables. </li>
</ol>

<p>Is there a forecasting function which allows <em>both</em> multiple predictors and multiple seasonalities?</p>

<p>Here is the electricity data of one month. </p>

<pre><code>    data &lt;- structure(c(1642.8, 1467.1, 165.57, 1630.99, 1618.65, 1629.29, 
        1598.93, 1839.9, 1604.52, 1606.73, 1473.82, 1669.17, 1698.9, 
        2111.21, 2056.41, 3671.29, 2808.01, 1336.15, 794.11, 1212.15, 
        377.36, 888.54, 174.58, 218.54, 420.76, 389.58, 397.77, 395.31, 
        359.11, 364.8, 376.13, 389.37, 929.5, 1702.38, 519.65, 2452.28, 
        1354.45, 1842.96, 725.41, 661.11, 528.44, 733.4, 429.51, 310.47, 
        279.72, 407.83, 1791.1, 1754.53, 1536.73, 1608.37, 1432.23, 1401.72, 
        1582.14, 1558.75, 1536.24, 1745.59, 1375.61, 1556.71, 1671.12, 
        1206.77, 1391.84, 876.23, 1617.3, 1638.99, 1833.61, 1591.42, 
        1455, 183.87, 177.55, 184.36, 332.99, 352.95, 425.1, 945.67, 
        342.3, 348.45, 227.18, 382.15, 268.91, 335.88, 326.94, 233.23, 
        169.71, 179.51, 195.3, 207.23, 1681.9, 1493.32, 941.52, 980.36, 
        924.31, 379.02, 1229.89, 1590.21, 1250.92, 1149.24, 1124.04, 
        993.78, 883.98, 860.69, 934.17, 969.31, 1049.55, 1104.94, 904.3, 
        1220.23, 1183.9, 891.26, 825.33, 787.77, 1060.93, 1029.1, 982.25, 
        193.13, 182.65, 181.75, 167.68, 165.89, 291.02, 300.2, 418.4, 
        297.66, 231.89, 305.66, 701.18, 338.5, 337.24, 332.11, 332.66, 
        187.8, 179.03, 130.22, 177.73, 172.24, 173.45, 334.23, 810.53, 
        359.41, 330.23, 333.29, 568.85, 2462.46, 1660.32, 1156.13, 1136.2, 
        1189.07, 832.73, 181.91, 185.93, 1076.77, 672.78, 1376.71, 1020.17, 
        382.18, 1160.74, 791.36, 1569.4, 817.78, 850.71, 747.21, 826.12, 
        1306.46, 506.23, 140.05, 132.6, 304.19, 308.14, 406.13, 290.73, 
        188.9, 165.59, 174.56, 145.97, 151.83, 142.29, 443.58, 799.9, 
        279.36, 223.88, 221.03, 291.26, 374.97, 431.36, 598.98, 625.82, 
        1052.02, 2036.83, 1230.03, 1429.81, 1099.34, 1646.03, 1668.56, 
        1631.79, 1604.04, 2849.49, 2998.63, 2476.96, 1601.04, 1216.42, 
        2004.2, 1868.51, 1961.91, 1813.35, 1500.22, 1276.94, 1369.29, 
        632.43, 238.15, 488.76, 467.6, 330.27, 144.67, 153.36, 924.12, 
        1348.18, 799.01, 524.3, 420.5, 264.34, 283.86, 198.95, 206.52, 
        217.33, 356.16, 207.83, 197.6, 194.03, 193.01, 249.85, 271.22, 
        244.25, 442.5, 660.49, 245.66, 356.13, 443.32, 336.15, 849.74, 
        1709.21, 1542.09, 1315.69, 2628.6, 2261.8, 1576.42, 1776.48, 
        1239.64, 1401.12, 1106.17, 1378.55, 1315.57, 1141, 1642.63, 2484.13, 
        1968.94, 3059.42, 1317.32, 905.05, 484.42, 486.86, 96.79, 183.45, 
        173.94, 342, 255.96, 320.54, 106.19, 147.88, 150.77, 176.17, 
        344.75, 371.73, 309.46, 237.86, 187.32, 202.61, 292.5, 248.28, 
        259.3, 283.67, 365.09, 230.47, 326.15, 350.92, 335.42, 419.39, 
        345.31, 1093.22, 1392.87, 1298.11, 919.16, 1654.53, 1045.99, 
        558.42, 437.29, 857.9, 758.34, 1220.04, 1390.62, 956.74, 909.93, 
        584.67, 409.87, 387.3, 387.93, 1276.28, 871.06, 413.8, 313.2, 
        199.5, 330.21, 210.9, 358.28, 352.13, 233.62, 259.18, 123.57, 
        255.58, 411.78, 427.65, 318.95, 298.5, 283.23, 279.85, 200.45, 
        205.97, 254.24, 307.98, 1090.53, 289.71, 215.14, 286.63, 328.55, 
        288.96, 1281.19, 1354.8, 1302.05, 1254.46, 261.95, 270.09, 243.28, 
        696.61, 314.27, 241.73, 245.68, 157.74, 222.55, 294.36, 185.46, 
        203.49, 182.14, 246.69, 178.26, 397.5, 330.2, 212.02, 248.72, 
        265.48, 249.37, 130.59, 248.97, 279.94, 319.07, 358.5, 278.98, 
        251.92, 304.66, 455.05, 365.95, 340.93, 287.51, 264.82, 260.18, 
        34.35, 35.11, 184.09, 247.18, 160.9, 139.27, 284.96, 296.31, 
        252.3, 342.65, 353.03, 380.52, 346.19, 350.06, 218.52, 133.94, 
        173.7, 128.26, 167.8, 112.77, 147.8, 129, 170.54, 89.88, 243.08, 
        97.61, 190.31, 193.94, 268.17, 233.5, 205.27, 92.29, 167.43, 
        168.34, 151.99, 193.84, 379.1, 318.69, 327.28, 487.39, 414.01, 
        336.06, 278.02, 168.05, 155.6, 236.4, 264.94, 296.05, 326.46, 
        357.43, 356.31, 340.29, 319.81, 312.79, 341.53, 317.36, 309.62, 
        440.6, 285.5, 282.06, 288.99, 334.48, 196.54, 144.24, 218.55, 
        173.64, 242.29, 251.78, 186.81, 184.36, 141.62, 208.91, 157.53, 
        154.03, 139.44, 137.66, 256.75, 1202.05, 177.36, 177.93, 72.83, 
        252.9, 231.35, 1090.39, 442.91, 363.12, 248.96, 478.75, 249.64, 
        297.29, 227.28, 365.82, 879.7, 488.93, 184.79, 138.13, 151.77, 
        123.18, 175.76, 251.84, 208.06, 126.68, 246.3, 307.34, 319.79, 
        324.3, 379.6, 309.53, 253.17, 221.91, 228.42, 150.24, 148.59, 
        118.79, 86.89, 140.51, 200.43, 212.15, 276.14, 441.81, 125.77, 
        152.42, 329.28, 269.21, 177.35, 1106.29, 128.92, 96.35, 63.53, 
        520.62, 940.25, 1014.34, 314.99, 390.2, 330.1, 377.04, 341.35, 
        342.79, 241.79, 249.9, 391.92, 292.68, 105.02, 179.99, 118.53, 
        154.17, 90.53, 206.7, 345.33, 244.75, 291.68, 820.57, 1777.84, 
        1805.83, 1753.73, 1416.7, 279.2, 262.82, 1345.88, 467.98, 1136.66, 
        170.02, 159.96, 1478.8, 1414.12, 1347.93, 1505.59, 1341.69, 445.53, 
        277.59, 1609.61, 1476.45, 244.08, 192.57, 213.55, 439.02, 112.86, 
        128.54, 376.09, 251.15, 116.27, 254.82, 302.56, 304.6, 198.5, 
        240.05, 219.35, 70.3, 190.96, 211.95, 328.53, 714.62, 3176.56, 
        2604.09, 191.65, 145.25, 93.93, 83.6, 81.13, 146.12, 331.75, 
        250.24, 1144.53, 1616.47, 1008.7, 316.65, 311.46, 1152.99, 1504.86, 
        1543.21, 1081.54, 1428.07, 1358.23, 1349.75, 190.23, 2398.92, 
        2196.11, 1466.94, 2249.77, 2150.2, 2542.25, 618.03, 453.22, 880.99, 
        1497.86, 440.96, 161.85, 324.88, 434.11, 316.33, 444.66, 359.14, 
        277.41, 1237.28, 761.41, 183.53, 309.44, 213.48, 121.64, 346.7, 
        149.86, 2060.39, 1102.13, 347.97, 600.24, 912.6, 590.77, 1805.76, 
        1673.93, 1573.91, 505.74, 446.76, 1033.41, 1668.68, 1293.9, 383.81, 
        1419.99, 1349.4, 711.55, 218.63, 182, 401.93, 1876, 1486.34, 
        1543.11, 2313.8, 478.57, 615.19, 542.68, 971.98, 531.11, 766.21, 
        489.76, 344.47, 319.86, 321.26, 311.41, 288.67, 310.67, 305.15, 
        419.33, 422.84, 950.08, 2188.88, 3454.92, 1989.54, 590.33, 327.05, 
        354.78, 578.41, 1583.29, 2016.66, 1481.03, 293.21, 1864.84, 399.65, 
        366.76, 357.7, 2074.97, 1626.86, 1133, 1624.61, 1506.93, 628.4, 
        1405.68, 217.8, 1223.11, 1356.97, 1171.72, 1182.86, 1642.11, 
        2289.02, 814.39, 595.76, 542.78, 1596.41, 884.97, 235.25, 1540.68, 
        781.95, 115.71, 1204.98, 718.66, 452.09, 305.58, 444.67, 356.76, 
        182.54, 674.47, 153.8, 862.25, 1322.88, 323.33, 1659.64, 496.72, 
        304.74, 246.6, 327.12, 239.31, 246.72, 225.72, 234.7, 324.07, 
        304.27, 171.86, 97.64, 242.69, 295, 324.53, 513.81, 1100.65, 
        1151.77, 231.56, 189.88, 786.3, 1164.87, 676.09, 882.82, 1496.3, 
        1027.91, 872.92, 809.1, 840.31, 1302.18, 2055.87, 677.74, 934.66, 
        263.91, 186.68, 248.5, 214.62, 371.54, 298, 294.52, 304.86, 1295.77, 
        942.5, 305, 265.78, 255.89, 255.63, 151.54, 108.16, 116.81, 100.19, 
        224.75, 84.11, 1143.89, 262.15, 784.21, 1728.29, 1506.79, 434.94, 
        374.29, 265.43, 560.74, 1651.49, 1063.07, 1054.69, 1298.4, 1261.59, 
        1132.75, 692.82, 660.57, 198.25, 97.81, 1258.67, 833.64, 796.35, 
        868.76, 999.86, 2240.02, 885.72, 1317.52, 1267.18, 167.93, 133.22, 
        364.44, 267.17, 406.13, 412.52, 1036.04, 779.34, 655.43, 1901.2, 
        270.18, 266.31, 284.21, 288.66, 135.38, 176.11, 154.86, 160.21, 
        146.28, 163.72, 139.75, 278.12, 253.51, 319.62, 396.39, 1662.69, 
        1577.09, 1059.71, 241.64, 407.54, 290.49, 846.17, 1325.31, 1418.23, 
        1432.5, 1412.6, 1015.87, 1619.88, 1426.58, 1333.32, 1963.35, 
        1638.11, 1081.89, 285.56, 1084.58, 2038.77, 1022.39, 1145.92, 
        513.87, 107.7, 176.19, 143.77, 374.3, 373.99, 221.76, 148.3, 
        331.01, 2323.12, 1502.09, 347.17, 296.7, 306.06, 313.03, 221.69, 
        295.35, 301.95, 250.92, 231.54, 140.6, 717.63, 863.5, 402.15, 
        1337.78, 1575.44, 1738.49, 1675.57, 1617.66, 1365.58, 242.8, 
        286.29, 712.34, 1559.59, 1600.34, 3447.88, 3432.89, 3337.23, 
        1472.21, 1323.76, 1265.31, 1221.65, 1312.63, 2016.09, 2972.13, 
        1451.67, 735.67, 130.13, 379.87, 162.21, 226.48, 417.18, 357.51, 
        346.37, 200.89, 190.15, 276.05, 942.74, 1471.99, 1047.53, 1240.06, 
        742.62, 169.34, 144.28, 220.58, 165.23, 344.8, 227.47, 254.64, 
        742.37, 1809.01, 436.11, 1692.87, 1697.74, 1474.91, 1635.68, 
        1664.2, 1489.85, 1316.4, 1364.07, 1510.02, 1497.89, 1709.17, 
        2846.71, 2736.08, 1015.43, 1017.08, 1195.21, 751.18, 523.89, 
        199.69, 2148.71, 1151.39, 1182.84, 788.91, 259.31, 146.29, 141.09, 
        299.38, 349.53, 404.08, 449.9, 391.77, 251.89, 222.25, 281.04, 
        565.4, 371.24, 219.75, 324.45, 227, 157.67, 212.48, 201.69, 140.84, 
        220.67, 187.64, 399.79, 157.92, 275.49, 326.99, 1340.51, 1578.68, 
        1599.41, 1667.74, 1129.07, 1312.55, 1393.09, 1368.69, 1130.85, 
        968.71, 1130.2, 1223.1, 1124.5, 1077.09, 1052.42, 1255.31, 918.21, 
        1263.93, 706.21, 3080.29, 1620.18, 1122.48, 750.06, 262.89, 110.02, 
        236.83, 413, 227.53, 355, 277.51, 258.03, 368.44, 656.68, 1808.17, 
        1493.41, 1272.21, 330.67, 1674.18, 719.45, 1089.68, 784.01, 275.73, 
        312.69, 345.11, 761.8, 1020.11, 259.16, 345.98, 270.23, 580.15, 
        1303.68, 1659.7, 1732.94, 204.9, 373.58, 373.36, 1381.52, 1437.74, 
        1262.78, 1264.6, 1184.54, 1175.12, 857.09, 1428.34, 841.92, 232.47, 
        223.22, 473.24, 382.7, 189.84, 1737.48, 1689.34, 378.48, 872.56, 
        180.12, 363.03, 301.38, 412.57, 401.17, 387.35, 417.63, 300.61, 
        376.82, 284.31, 232.31, 269.96, 188.41, 203.79, 134.88, 193.66, 
        57.86, 89.71, 167.66, 60.84, 197.03, 703.66, 1638.7, 1467.03, 
        347.22, 1397.23, 1511.92, 1362.25, 1397.18, 1106.19, 826.5, 1033.04, 
        1039.46, 584.98, 706.35, 548.07, 373.39, 681.6, 1231.28, 288.94, 
        649.36, 79.28, 209.23, 290.75, 304.12, 132.57, 91.2, 355.37, 
        197.45, 343.17, 339.2, 284.65, 229, 234.73, 322.36, 323.43, 295.1, 
        197.03, 308.17, 223.88, 235.08, 225.16, 172.83, 236.26, 135.17, 
        394.89, 479.2, 315.34, 280.9, 282.36, 204.78, 367.24, 1712.29, 
        1521, 1686.09, 960.19, 1019.12, 1062.77, 851.88, 1369.98, 689.2, 
        580.5, 751.74, 547.67, 556.64, 493.85, 404.15, 428.07, 716.2, 
        1442.05, 1045.81, 1497.07, 567.59, 155.07, 537.72, 446.03, 282.57, 
        642.88, 409.37, 338.91, 173.89, 358.63, 195.42, 209.95, 186.44, 
        152.34, 105.51, 132.26, 82.14, 122.88, 149.38, 211.42, 350.17, 
        429.72, 336.4, 982.21, 1436.25, 1726.87, 1830.42, 1282.05, 1293.4, 
        1121.01, 946.2, 707.1, 154.53, 767.56, 607.78, 448, 288.23, 270.32, 
        223.93, 166.22, 262.45, 223.74, 159.31, 210.44, 257.94, 183.99, 
        151.38, 206.11, 193.43, 388.95, 577.98, 304.64, 285.13, 256.59, 
        420.26, 289.34, 356.02, 358.08, 325.22, 275.95, 164.46, 213.23, 
        142.99, 221.66, 270.61, 206.56, 213.68, 254.33, 250.15, 267.99, 
        403.95, 671.2, 1574.11, 396.34, 477.88, 631.08, 618.25, 1366.88, 
        298.19, 287.05, 290.38, 332.44, 235.9, 229.79, 831.6, 1320.86, 
        477.31, 944.84, 547.33, 411.21, 705.43, 873.49, 572.81, 585.36, 
        1229.69, 701.01, 653.49, 74.81, 162.47, 179.54, 330.27, 544.51, 
        332.41, 296.66, 130.66, 1055.61, 556.79, 265.43, 383.44, 398.22, 
        362.66, 223.99, 130.35, 193.67, 217.68, 273.3, 247.84, 161.66, 
        320.08, 322.52, 274.61, 811.44, 353.85, 323.41, 383.61, 389.5
        ), .Dim = c(1248L, 1L), .Dimnames = list(NULL, ""power""))
</code></pre>

<p>Temperature predictor values are:</p>

<pre><code>temp&lt;-structure(c(31, 31, 31, 31, 30, 29, 28, 27.5, 27, 26, 26, 26, 
26, 26, 26, 28, 29, 29, 30, 31, 32, 33, 33, 34, 34, 35, 36, 36, 
36.5, 37, 38, 38, 39, 39, 39, 38, 37, 36, 36, 35, 34, 33, 33, 
33, 32.5, 32, 32, 31, 30, 26, 24, 24, 24, 24, 24, 24, 25, 25, 
25, 25, 26, 26, 26, 27, 28.3, 29.7, 31, 32, 33, 33, 34, 34, 34, 
35, 35, 35, 36, 37, 38, 39, 39, 39, 39, 38.5, 38, 37, 36, 35, 
34, 34, 33, 32.5, 32, 30, 30, 30, 28, 28, 27, 25, 25, 24, 24, 
24, 24, 24, 25, 25, 25, 25, 25, 27, 28, 30, 31, 33, 34, 37, 37, 
38, 38, 39, 39.3, 39.7, 40, 40, 40, 40, 41, 40, 40, 38, 38, 36, 
35, 34, 33, 31, 31, 30, 31, 30, 30, 29, 28, 28, 28, 27, 27, 28, 
27, 24, 24, 24, 23, 23, 23, 24, 24, 26, 27, 28, 29.7, 31.3, 33, 
34, 35.3, 36.7, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 
39, 38, 37, 36, 34, 33, 32, 32, 30, 28.5, 27, 27, 27, 27, 27, 
26, 26, 26, 25.3, 24.7, 24, 24, 24, 24, 23, 23, 24.5, 26, 28, 
29.7, 31.3, 33, 34.5, 36, 37, 38, 39, 40, 40, 40.3, 40.7, 41, 
41, 41, 41, 41, 41, 41, 39.5, 38, 37, 37, 35, 34, 34, 33, 33, 
33, 32, 31, 31, 30, 30, 30, 29, 29, 28.5, 28, 28, 28, 29, 29, 
28.5, 28, 29, 29, 30, 31, 31, 32.5, 34, 35, 37, 39.5, 42, 42, 
42, 42, 42, 43, 44, 45, 45, 44, 43, 42, 42, 41, 39, 36.5, 34, 
34, 33, 33, 33, 33, 33, 33, 32, 32, 32, 32, 31.5, 31, 30, 30, 
30, 29, 29, 28, 27, 28, 29, 30, 31, 32.5, 34, 34, 35, 37, 40, 
41, 41, 42, 42, 42.5, 43, 43, 43, 42, 42, 43, 42, 42, 41, 40, 
39, 37.5, 36, 35, 35, 34, 34, 33, 32, 32, 32, 32, 31, 31, 31, 
31, 31, 28, 28, 28, 28, 28, 28.5, 29, 30, 30.5, 31, 32, 33.5, 
35, 36, 37, 38, 39, 38, 39, 40, 41, 42, 42, 43, 42, 42, 42, 41, 
41, 40, 39, 38, 37, 36, 35, 34, 34, 33, 32, 32, 31, 30.5, 30, 
30, 30, 30, 29, 29, 29, 28, 27, 27, 27, 27, 28.5, 30, 31.5, 33, 
34, 35, 36, 37, 38, 38, 39, 40, 41, 42, 42, 42, 42, 42, 42, 42, 
42, 42, 42, 42, 40, 40, 39, 37, 37, 36.5, 36, 35, 34, 34, 33, 
33, 32, 32, 32, 32, 32, 32, 31, 31, 30, 30, 30, 30, 31, 31, 31, 
32, 33.5, 35, 35.5, 36, 37, 37, 38, 39, 40, 41, 41, 42, 42, 42, 
43, 43, 42, 41, 41, 40, 38, 36, 35, 34, 34, 35, 36, 36, 35, 35, 
33, 33, 32, 31, 31, 30, 30, 29, 29, 29, 29, 29, 31, 31.5, 32, 
31, 30, 31, 32, 33, 34, 34, 35, 36, 36, 37, 37, 38, 38, 38, 39, 
39, 39, 39, 39, 39, 39, 38, 37, 37, 37, 37, 35, 33, 31, 31, 31, 
30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 28, 28, 27, 27, 26, 29, 
29, 30, 31, 32, 32, 33, 33, 33, 34, 34, 35, 35, 37, 37, 37, 37, 
37, 38, 38, 38, 38, 37, 37, 36, 35, 34.5, 34, 34, 33, 33, 32, 
32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 30, 30, 29.5, 29, 
29, 30, 30, 31, 32, 32, 33, 33.5, 34, 34, 34, 30.7, 27.3, 24, 
24, 24, 24.7, 25.3, 26, 27, 28, 29, 29, 29, 28, 28, 27, 27, 26, 
25, 25, 25, 24.5, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 
23, 23, 23, 24, 24, 24, 25, 25, 26, 28, 29, 30, 31, 31, 32, 33, 
34, 35, 35, 36, 37, 37, 37, 37, 37, 37, 37, 35, 34, 33, 33, 33, 
32, 31.7, 31.3, 31, 31, 31, 31, 30, 30, 30, 30, 28, 28, 27, 27, 
26, 26, 25, 25, 25, 25, 25, 26, 27, 28, 29.7, 31.3, 33, 34, 34, 
35, 36, 37, 36, 36, 35, 35, 34, 32, 32, 33, 31.3, 29.7, 28, 28, 
29, 29, 29, 29, 28, 27.5, 27, 26.5, 26, 26, 26, 26, 26, 26, 25, 
25, 25, 25, 25, 24, 24, 23, 23, 23, 24.5, 26, 27, 28, 29, 30, 
31, 32, 32, 32, 33, 34, 35, 36, 36, 36, 36, 36, 36, 36, 37, 37, 
37, 36, 36, 34, 34, 33, 32, 32, 31, 31, 30, 28, 28, 28, 28, 27, 
27, 27, 27, 27, 27, 27, 27, 26.5, 26, 26, 26, 26, 27, 28.8, 30.5, 
32.2, 34, 34, 34, 35, 36, 37, 38, 38, 38, 39, 40, 39, 39, 39, 
40, 40, 40, 39, 39, 38, 37, 35, 35, 34, 34, 34, 33, 32, 32, 32, 
32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 33, 34, 
35, 37, 37, 37.3, 37.7, 38, 39, 40, 43, 43, 43, 44, 44, 43, 43, 
43, 43, 43, 43, 42, 41, 40, 39, 38, 37, 36, 35, 35, 34, 34, 34, 
34, 34, 33, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 34, 
35, 36, 37, 38, 38, 40, 40.5, 41, 42, 42, 42, 42, 41, 40, 40, 
39, 39, 37, 32, 27, 26, 26, 27, 28, 27, 27, 27, 26, 25, 25, 25, 
25, 25, 25, 25, 25, 25, 25, 25, 24, 23, 23, 22.5, 22, 22, 23, 
24, 25, 26.3, 27.7, 29, 30, 32, 33, 34, 36, 37, 37, 39, 40, 40.5, 
41, 42, 42, 42, 43, 43, 42, 41.5, 41, 39, 38, 35, 35, 35, 34, 
33, 33, 33, 33, 32, 32, 31, 31, 30, 30, 30, 29, 29, 28, 28, 28, 
28.5, 29, 29, 29, 30, 32, 32, 34, 35, 37, 38, 39, 40, 41, 42, 
42, 43, 44, 44, 44, 45, 45, 45, 44, 43, 43, 42, 40, 38.5, 37, 
35, 35, 34, 34, 33, 34, 33, 33, 33, 33, 32, 32, 31, 32, 31, 30, 
29, 29, 29, 29, 30, 30, 31, 32.7, 34.3, 36, 37, 38, 39, 40, 41, 
41, 42, 42, 43, 44, 44, 44, 44, 44, 43.5, 43, 43, 42, 42, 40, 
38, 38, 37, 36, 36, 36, 36, 35, 35, 34, 34, 34, 34, 32, 32, 31.5, 
31, 31, 30, 30, 30, 30, 31, 33, 34, 35, 37, 38, 39, 40, 40, 42, 
42, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 42, 40, 40, 
40, 39, 38, 38, 38, 37, 37, 36, 36, 35.5, 35, 34.5, 34, 33, 33, 
32, 32, 31, 30, 30, 30, 31, 32, 33.5, 35, 36.5, 38, 38, 39, 40, 
41, 41, 41, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 
42, 40.5, 39, 38, 38, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 
35, 35, 34, 34, 34, 32, 32, 31, 31, 33, 34, 35, 35, 36, 38, 39, 
40, 41, 42, 42.5, 43, 43, 44, 44, 45, 45, 45, 45, 45, 45, 45, 
44, 43.5, 43, 43, 42, 41, 40, 40, 39, 39, 38, 38, 37, 36, 36, 
35, 34, 34, 34, 32, 32, 32, 32, 32, 32, 32, 34, 35, 35, 37, 38, 
38, 39, 39, 40, 41, 42, 43, 44, 44, 44, 45, 45, 46, 46, 46, 45, 
45, 45, 44, 42, 40, 38, 37, 37, 37, 37, 37, 36, 35), .Dim = c(1248L, 
1L), .Dimnames = list(NULL, ""temperature""))
</code></pre>

<p>Forecasted temperature values are:</p>

<pre><code>forecast_temp &lt;- structure(c(35, 34, 34, 33, 33, 30, 29.7, 29.3, 29, 28, 28, 27, 
27.3, 27.7, 28, 29, 30.5, 32, 33.5, 35, 36, 36, 37, 39, 39.5, 
40, 40, 41, 42, 42, 43, 43, 43, 44, 44, 44, 43, 42, 40, 39, 39, 
38, 37, 37, 36, 35, 35, 34), .Dim = c(48L, 1L), .Dimnames = list(
    NULL, ""temperature""))
</code></pre>

<h3><em>UPDATE-1</em></h3>

<p>From @Stephan's suggestion, I followed the approach mentioned by Prof. Rob at <a href=""http://robjhyndman.com/hyndsight/dailydata/"" rel=""nofollow"">link1</a>, <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">link2</a> as</p>

<pre><code>library(forecast)
tsob &lt;- ts(train_power,frequency = 48) #training electriciy data at daily frequency
tsob_weekly &lt;- fourier(ts(train_power,frequency = 7*48),K=3) #training data at weekly frequency
tempob &lt;- ts(train_temperature,frequency = 48) #Temperature, another predictor variable
fit &lt;- auto.arima(tsob,xreg=cbind(tsob_weekly,tempob),seasonal=FALSE)
tempob_forecast &lt;- ts(test_temperature,frequency = 48) #forecasted temperauture values
forecast_val &lt;- forecast(fit,xreg=tempob_forecast,h=48*5) #forecast for coming 5 days
</code></pre>

<p>As evident from the code, I have used Fourier transformation suggested at above links to show weekly seasonal affect. Up to <code>auto.arima()</code>, it works properly but at forecast function an error is thrown as:</p>

<pre><code>Error in forecast.Arima(fit, xreg = tempob_forecast, h = 48 * 5) : 
  Number of regressors does not match fitted model
</code></pre>

<p>The error is clear, i.e., I do not provide the forecasted regressor values of <code>tsob_weekly</code>, which I don't have. How should I handle this issue? Prof. Rob has used the same value of regressor for both the <code>auto.arima</code> and <code>forecast</code> functions.</p>
"
"0.076847327936784","0.103695169473043","213201","<p>I am having basically the same issue than in <a href=""http://stats.stackexchange.com/questions/65585/auto-arima-does-not-recognize-seasonal-pattern"">this thread</a>, except one thing:</p>

<p>The difference, in my case, is that my data is measured <strong>weekly</strong> and not daily, so the argument of a too high seasonality (> 350) does not hold for my data, since the seasonality in my case is <strong>52</strong> (52 weeks in a year). </p>

<p>And yet, when I use <code>auto.arima()</code>, R returns the ARIMA model (p,d,q) = (2,1,1) and (P,D,Q) = (0,0,0), while the seasonal pattern in my data is blatant... How could you explain that R completely dismisses the seasonality in my data?</p>

<p>Since I'm still in a learning phase, I am using the data set <code>cmort</code> available in the <code>astsa</code> library, so everyone here can use the same data as me. </p>

<p>And I have done <code>cmort &lt;- ts(cmort,frequency=52)</code> to be sure that the seasonality in my data is taken account of, but it didn't change anything.</p>
"
"NaN","NaN","44974","<p>I'd like to use R to generate two correlated series that follow IMA(1,1) process. 
rho is a correlation between the error terms, but when I changed the rho the plot does not change. Is it wrong if I use  <code>d[i,] &lt;- d[i-1,] - theta*(e[i-1,]+e[i,])</code> ?</p>

<pre><code>rho &lt; 0.1
mu &lt;- c(400,400)
theta &lt;- c(0.1,0.1)
d &lt;- ts(matrix(0,ncol=2,nrow=1001))
e &lt;- ts(rmvnorm(1001,sigma=cbind(c(400,rho*400),c(rho*400,400))))
for(i in 2:1001)
  d[i,] &lt;- mu + d[i-1,] - theta*(e[i-1,]+e[i,])

plot(d)
</code></pre>

<p><img src=""http://i.stack.imgur.com/VPu8p.png"" alt=""enter image description here""></p>
"
"0.0443678254708057","0.059868434008925","198887","<p>I am trying to model a time series that contains a sequence of zeros. I tried fitting an ARIMA model using <code>auto.arima</code> function from the forecast package in R but the MAPE is reported as infinity (probably due to division by zero). Moreover, the <code>auto.arima</code> fits an ARIMA(0,1,0) model over the data. </p>

<p>Can you suggest any types of models that may be appropriate for such data?</p>
"
"0.0443678254708057","0.059868434008925","116842","<p>I have a SarimaX model with three regressor variables:</p>

<pre><code>ARIMA(1,0,0)(0,1,1)[7]                    

Coefficients:
          ar1       sma1   C1 (for xreg1)   C2 (for xreg2)   C3 (for xreg3)
      -0.0260    -0.9216          -0.0354           0.0316           0.9404
s.e.   0.0291     0.0350           0.0016           0.0017           0.0128
</code></pre>

<p>I would like to know how to use these coefficients to obtain the actual equation, like:</p>

<pre><code>y[t] = f(ar1, sma1, C1|xreg1[t], C2|xreg2[t], C3|xreg3[t])
</code></pre>

<p>I have read the following:</p>

<p><a href=""https://www.otexts.org/fpp/8/9"" rel=""nofollow"">https://www.otexts.org/fpp/8/9</a> - I'm using the forecast package in R, so I'm quite grateful for Mr. Hyndman's work,</p>

<p><a href=""http://people.duke.edu/~rnau/arimreg.htm"" rel=""nofollow"">http://people.duke.edu/~rnau/arimreg.htm</a></p>

<p>and others, and I devised some formulas, but they generated values less acurate than those from the R forecast. Somehow, my error-related terms are probably wrong.</p>

<hr>

<p><strong>EDIT</strong>: This is what I have so far:</p>

<p>$$ \ (1-ar1*B)*(1-B^7)*y_t=$$
$$ = (1-ar1*B)*(1-B^7)*(C1*xreg1_t + C2*xreg2_t+C3*xreg3_t)+ $$
$$ + e_t + sma1*e_{t-7}$$</p>

<p>I would like to know if this formula is correct, could anyone please help? Thank you.</p>
"
"0.0627455805138159","0.0846667513334603","199010","<p>First of all I'd like to point out that I was unsure whether to post this question here or on stack overflow, considering it is rather code heavy; but as I think it plays more on the logic and application of cross validation, a topic this forum has more expertise in than the other, I chose to post it here. Please move it to stack-overflow if deemed inappropriate.</p>

<p><strong>My question is as follows:</strong></p>

<p>When attempting to run the below code, adapted from <a href=""http://robjhyndman.com/hyndsight/tscvexample/"" rel=""nofollow"">this tutorial by Rob Hyndman</a>, I run into trouble when populating the <code>mae</code> matrix in the below for loop. This I gather is from the handling of <code>dx</code> and its length, which due to its bivariate nature is double the actual time frame (more of course if multivariate). </p>

<p>Now, while I have an understanding of the principle behind using training and testing sets, I am not very familiar with how to apply it, and as a result am thoroughly stuck at the above problem.</p>

<p>Any and all help is greatly appreciated as I'm at my wits end, this being the last major hurdle in what to me is an important project.</p>

<pre><code>library(""forecast"")
library(""vars"")

d &lt;- rnorm(70)
x &lt;- rnorm(70)

dx &lt;- cbind(d,x)
dx &lt;- as.ts(dx)

# Forecast Accuracy
k &lt;- 58 # data length less forecast horison (as minimum)
n &lt;- length(dx)
mae &lt;- matrix(NA, n-k, 12)
st &lt;- tsp(dx)[1]+(k-2)/12

for (i in 1:(n-k)) {
  dxshort &lt;- window(dx, end=st+i/12)
  dxnext &lt;- window(dx, start=st + (i+1)/12, end=st+(i+12)/12)
  fit &lt;- VAR(dxshort, p = 2)
  fcast &lt;- forecast(fit, h = 12)
  fcastmean &lt;- do.call('cbind', fcast[['mean']])
  mae[i,1:length(dxnext)] &lt;- (fcastmean - dxnext)
}
</code></pre>

<p>The above code uses random numbers with a normal distribution for illustration and should run as is.</p>
"
"0.117386232408469","0.135768846660426","163074","<p>So I've been learning how to forecast over this summer and I've been using Rob Hyndman's book Forecasting: principles and practice.  I've been using R, but my questions aren't about code.  For the data I've been using, I've found that an average forecast of multiple models has produced higher accuracy levels that any sole model by itself.  </p>

<p>Recently I read an blog that talked about averaging forecasting methods and assigning weights to them.  So in my case, lets say I assign 11 different models to my set of data (Arima, ETS, Holt Winters, naive, snaive, and so forth) and I want to average a few of these to get a forecast.  Has anyone had any experience with this or can point me to an article that might give some insight on the best way of going about this?</p>

<p>As of right now, I'm using cross validation and Mean Absolute Error to figure out which models perform best and which perform worst. I can even use this to identify the top k # of models.</p>

<p>I guess my questions are</p>

<p>1) How many models would you suggest selecting? (2,3,4,5,6, etc)</p>

<p>2) Any ideas on weights?  (50% to the best, 25% to the second best, 15% third best, 10% to the 4th best, etc)</p>

<p>3) Are any of these forecasting models redundant and shouldn't be included? 
(Arima, snaive, naive, HW's ""additive"", ETS, HoltWinters exponential smoothing, HoltWinters smoothing w/ trend, HoltWinters w/ trend/seasonality, multiple regression)</p>
"
"0.0665517382062085","0.0898026510133874","116896","<p>I used tbats to fit a model for a 3 years of historic data and the values work fine but as I did not include holidays, holiday predictions are really off. I used arima with regressor (holidays at regressors) and the predictions for holidays are much better than the one by tbats but tbats got more accurate results for normal days. I know it sounds un reasonable but is this ok that I use the arima model for holidays and the tbats model for normal days?</p>
"
"0.108678533400333","0.146647115021353","66655","<p>I am doing my master's thesis and I must compare various forecasting techniques at different frequencies of datasets. I am using my universities dataset, the REDD dataset, UCI dataset and CER Ireland dataset for this purpose. The data I use is in seconds for a time span of a month and this gives > 3 million records.</p>

<p>I have been trying to understand how to make good use of all this data but couldn't exactly get to a solution. I have read <a href=""http://stats.stackexchange.com/questions/29424/time-series-modeling-with-high-frequency-data"">Time series modeling with high-frequency data</a>, but I don't understand and couldn't find resources how to apply it to my problem. I have tried reading several blogs and books to get an understanding of time series forecasting but most literature has examples with granularity only as low as hourly data. Some references I found were about high granularity data but only for a short period of time. </p>

<p>I have read in Prof. Rob Hyndman's blog that practically the ARIMA model can only calculate till 200 autoregressive points and if my understanding is correct then for data with a frequency in seconds, I could achieve daily trends only with $3600*24 = 86400$ previous values? </p>

<p>I am not sure how I should deal with this. </p>

<p>Here is how the data look (the y-axis is watts):<br>
<img src=""http://i.stack.imgur.com/uKC7j.png"" alt=""http://postimg.org/image/i0txbwz6h/""></p>
"
"0.109804765899178","0.105833439166825","163371","<p>I made a time series decomposition with tbats. There is weekly and yearly seasonality in the data (and maybe also monthly - not really important for the question)</p>

<pre><code>x &lt;- msts(data, start=c(2005,1,1), seasonal.period=c(7,30.4,365.25))
fit &lt;- tbats(x, use.box.cox=FALSE)
plot(fit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/pxr6g.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pxr6g.png"" alt=""enter image description here""></a></p>

<p>As far as i understand the tbats result is an ""additive decomposition"", right?</p>

<p>I can now access the different parts of the tbats decomposition:</p>

<pre><code>level &lt;- as.numeric(tbats.components(fit)[,'level'])
season1 &lt;- as.numeric(tbats.components(fit)[,'season1'])
season2 &lt;- as.numeric(tbats.components(fit)[,'season2'])
season3 &lt;- as.numeric(tbats.components(fit)[,'season3'])
</code></pre>

<p>Since i suppressed 'Box-Cox transformation' earlier, level is roughly the same then trend according to a post from Rob J Hyndman  in the comments section here: <a href=""http://robjhyndman.com/hyndsight/dailydata/"" rel=""nofollow""> here</a></p>

<p>In order to get the remainder part of the decomposition is it a legit way to just subtract all the parts from the original data?</p>

<pre><code>remainder &lt;- data - level - season1 - season2 - season3
</code></pre>

<p>What do i get when i use this:</p>

<pre><code>y &lt;- resid(fit)
</code></pre>

<p>I am a bit confused right now about the right way to do it... Many thanks for all your input!</p>

<p><strong>Update:</strong></p>

<pre><code>resid(fit)
fit$errors
</code></pre>

<p>Those two are both the same. I guess these values are related to the tbats method. I am doubting that i can take them as the ""remainder"" of the decomposition? Is there a way to extract the remainder out of the tbats method? In his paper '<em>Forecasting time series with complex seasonal patterns using exponential smoothing</em>' Rob J Hyndman shows remainder graphs for the tbats method so that's why i think it is possible to get in R as well.
Anyone any thoughts about that?</p>
"
"0.107019221709553","0.126357084250573","163520","<p>As I asked in <a href=""http://stackoverflow.com/questions/31210688/what-is-proper-way-of-forecasting-grouped-time-series-specified-via-hts-package"">here</a> I was trying to forecast grouped time series with two grouping variables and I find some limitation of hierarchical forecasting methods. In particular, using <em>hts</em> package from R, <strong>we can't use top-down methods.</strong> </p>

<p>I consider grouped time series which can be viewed as:</p>

<pre><code>     Total
   |       | 
   A       B
 |   |    |   |
AX  AY   BX  BY

     Total
   |       | 
   X       Y
 |   |   |   |
 AX  BX  AY  BY
</code></pre>

<p>(It's described in more details in this <a href=""http://stats.stackexchange.com/questions/31473/forecasting-hierarchical-time-series-r-package"">post</a> and for example in this <a href=""http://robjhyndman.com/papers/hgts6.pdf"" rel=""nofollow"">paper</a>)</p>

<p>According to the notation specified in <a href=""http://robjhyndman.com/papers/Hierarchical6.pdf"" rel=""nofollow"">this paper</a> we can write such grouped time series as $\mathbf{Y_t} = \mathbf{S} \mathbf{Y_{K,t}}$, where $\mathbf{S}$ is a summing matrix and $\mathbf{Y_{K,t}}$ is a vector of bottom level series (which according to assumption in hts package have to be equal). In this case it looks like:</p>

<p>$$   \begin{bmatrix}
      Y_t \\
      Y_{A,t} \\
      Y_{B,t} \\
      Y_{X,t} \\
      Y_{Y,t} \\
      Y_{AX,t} \\
      Y_{AY,t} \\
      Y_{BX,t} \\
      Y_{BY,t} \\
     \end{bmatrix} = \begin{bmatrix}
     1 &amp; 1 &amp; 1 &amp; 1  \\ 
     1 &amp; 1 &amp; 0 &amp; 0  \\ 
     0 &amp; 0 &amp; 1 &amp; 1  \\ 
     1 &amp; 0 &amp; 1 &amp; 0  \\ 
     0 &amp; 1 &amp; 0 &amp; 1  \\ 
     1 &amp; 0 &amp; 0 &amp; 0  \\ 
     0 &amp; 1 &amp; 0 &amp; 0  \\ 
     0 &amp; 0 &amp; 1 &amp; 0  \\ 
     0 &amp; 0 &amp; 0 &amp; 1  \\ 
     \end{bmatrix} \begin{bmatrix}
      Y_{AX,t} \\
      Y_{AY,t} \\
      Y_{BX,t} \\
      Y_{BY,t} \\
     \end{bmatrix}
$$</p>

<p>Revised forecast (what I am looking for) can be written as $\mathbf{\tilde{Y}_n(h) = SP\hat{Y}_n(h)}$ and in case of top-down method matrix $\mathbf{P}$ is defined as 
$\mathbf{P} = \begin{bmatrix}
     \mathbf{p} | \mathbf{0}_{m_K \times (m-1)} 
\end{bmatrix}$, where $ \mathbf{p} = [p_1, p_2, ..., p_{m_K}]^T$  is a vector of proportions. Not going into more details, in this example $m_K = 4$ and $m=9$, so $\mathbf{P} = \begin{bmatrix}
     \mathbf{p_1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 
\mathbf{p_2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\mathbf{p_3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\mathbf{p_4} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix}$ </p>

<p>and revised forecasts can be written as:</p>

<p>$$   \begin{bmatrix}
      \tilde{Y_t} \\
      \tilde{Y}_{A,t} \\
      \tilde{Y}_{B,t} \\
      \tilde{Y}_{X,t} \\
      \tilde{Y}_{Y,t} \\
      \tilde{Y}_{AX,t} \\
      \tilde{Y}_{AY,t} \\
      \tilde{Y}_{BX,t} \\
      \tilde{Y}_{BY,t} \\
     \end{bmatrix} = \begin{bmatrix}
     1 &amp; 1 &amp; 1 &amp; 1  \\ 
     1 &amp; 1 &amp; 0 &amp; 0  \\ 
     0 &amp; 0 &amp; 1 &amp; 1  \\ 
     1 &amp; 0 &amp; 1 &amp; 0  \\ 
     0 &amp; 1 &amp; 0 &amp; 1  \\ 
     1 &amp; 0 &amp; 0 &amp; 0  \\ 
     0 &amp; 1 &amp; 0 &amp; 0  \\ 
     0 &amp; 0 &amp; 1 &amp; 0  \\ 
     0 &amp; 0 &amp; 0 &amp; 1  \\ 
     \end{bmatrix} \begin{bmatrix}
     p_1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 
p_2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
p_3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
p_4 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix} \begin{bmatrix}
      \hat{Y_t} \\
      \hat{Y}_{A,t} \\
      \hat{Y}_{B,t} \\
      \hat{Y}_{X,t} \\
      \hat{Y}_{Y,t} \\
      \hat{Y}_{AX,t} \\
      \hat{Y}_{AY,t} \\
      \hat{Y}_{BX,t} \\
      \hat{Y}_{BY,t} \\
     \end{bmatrix}
$$</p>

<p>and after calculations:</p>

<p>$$   \begin{bmatrix}
      \tilde{Y_t} \\
      \tilde{Y}_{A,t} \\
      \tilde{Y}_{B,t} \\
      \tilde{Y}_{X,t} \\
      \tilde{Y}_{Y,t} \\
      \tilde{Y}_{AX,t} \\
      \tilde{Y}_{AY,t} \\
      \tilde{Y}_{BX,t} \\
      \tilde{Y}_{BY,t} \\
     \end{bmatrix} = \begin{bmatrix}
      p_1\hat{Y_t} + p_2\hat{Y_t} + p_3\hat{Y_t} + p_4\hat{Y_t} \\
      p_1\hat{Y_t} + p_2\hat{Y_t} \\
      p_3\hat{Y_t} + p_4\hat{Y_t} \\
      p_1\hat{Y_t} + p_3\hat{Y_t} \\
      p_2\hat{Y_t} + p_4\hat{Y_t} \\
      p_1\hat{Y_t} \\
      p_2\hat{Y_t} \\
      p_3\hat{Y_t} \\
      p_4\hat{Y_t} \\
     \end{bmatrix}
$$</p>

<p>Which seems OK for me. I was hoping that somebody could point out <strong>why this method can't be used in forecasting grouped time series</strong> and point out when my calculations are wrong?</p>
"
"0.0627455805138159","0.0846667513334603","163580","<p>For testing I generated a very simple time series with a clear recurring pattern. I expected that auto.arima will generate a model, that can forecast that pattern, but Ã³bviously it doesn't. Can anyone give me some hints how I can improve the model in order to predict that pattern correctly?</p>

<pre><code>library(forecast)

ts&lt;-c(1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1)
fit &lt;- auto.arima(ts)
plot(forecast(fit,h=20))
</code></pre>

<p><a href=""http://i.stack.imgur.com/AgVha.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AgVha.png"" alt=""enter image description here""></a>
Thanks!</p>
"
"NaN","NaN","211022","<p>I have 288 data points of the Wolf's sunspot data for the years 1700 to 1987. "
"NaN","NaN","I need to predict one step ahead forecasts for a forecast horizon of 25.",""
"NaN","NaN","I kept the last 25 data points of the time series to test against the predictions. </p>",""
"NaN","NaN","<p>Will fitting an Arima to (288-25 =) 263 data points like suggested here <a href=http://stats.stackexchange.com/questions/55168/one-step-ahead-forecast-with-new-data-collected-sequentially?rq=1>One step ahead forecast with new data collected sequentially</a> work?",""
"NaN","NaN","Or do I need to iteratively increase the size of training data by 1 and then predict the next value?</p>",""
"NaN","NaN","","<r><time-series><forecasting><arima>"
"0.188236741541448","0.197555753111407","211079","<p>This question is also linked to <a href=""http://stats.stackexchange.com/questions/209790/how-to-detect-a-relatively-small-level-shiftleakage-in-an-hourly-water-flux-ti"">How to detect a relatively small level shift(leakage) in an hourly water flux time series in an area?</a> which I asked a week ago...</p>

<h3>Background</h3>

<p>I've got a series of water flux data among about four month. The data is hourly collected, and I'm trying to develop an approach to justify whether there is a leakage or not. In the end, I want to implement my approach in R.<br>
As the other post mentioned, since I'm kind of new to the field of time series analysis, I've already tried some approaches or black boxes to solve the problem, but the result seems not good. And in the course of digging deep into the problem, I start to suspect the approaches I used.<br>
<strong>So I'm here to ask for help, is there any advices/procedures/references which I could refer to?</strong></p>

<h3>What I've tried</h3>

<ol>
<li>I used ARIMA,ETS,TBATS,STLM models provided by the forecests package to directly get a model to predict, and I try to using the model to predict, and then compare the prediction with the test value. The accuracy test showed it's not a good idea. Since non of the prediction is better than the snaive, which also got a MASE > 1.  </li>
<li>I detected the single outliers, and replaced it with a moving average of nearest 7 days. Then using the tsoutliers::tso with the only type of ""LS"", but the outcomes even failed at the manually modificated data I created.  </li>
<li>I seperated the data at zero and other points(totally 24 groups), and within each group I used the same idea as the first to find a model, make a prediction, and then check the residuals in series. This time with the cross validation, results showed an ARIMA model fits the best averagely. But then I got lost the model I got returns an intercept which is constantly increasing. Definitely, there exists other effects such as temperature which could interpret this variation. This somehow leads me crestfallen, since I've only got four month data, without comparing with the last year data, <strong>is it possible to estimate the appropriate coefficient with the temperature, and solve the problem in the meantime???</strong>   

<blockquote>
  <p>mod_arima  0.369<br>
  naive     0.725<br>
  mod_exp   0.891<br>
  mod_stl   0.913<br>
  mod_tbats 1.067  </p>
</blockquote></li>
</ol>

<h3>Characteristics of the data</h3>

<p>I think there are some characteristics to help you have a better understanding of my data.  </p>

<ol>
<li><p>In my view, the leackage I want to detect is relatively small(I doubt, there's only about 5%/10% of the mean value).  </p>

<blockquote>
  <p>train_h &lt;- ts(data_h$Navigator[1393:(1392+14*24)], frequency = 24)<br>
  excess &lt;- ts(c(rep(0,279),rep(mean(train_h,na.rm = T)*0.1,57)),frequency = 24)<br>
  plot(train_h)<br>
  lines(train_h+excess,col = ""red"")<br>
  <a href=""http://i.stack.imgur.com/Qcm3g.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Qcm3g.jpg"" alt=""enter image description here""></a>
  As you could see there isn't so much difference...</p>
</blockquote></li>
<li><p>There's a huge calendar effect in my data during the Chinese New Year, So all the approaches I explored above only used the time span I think could ignore this effect.  </p>

<blockquote>
  <p>tsdata_d &lt;- ts(data_d$Navigator[1:114],frequency = 7)<br>
  plot(tsdata_d)
  <a href=""http://i.stack.imgur.com/gl6BY.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gl6BY.jpg"" alt=""enter image description here""></a>
  And I used the second half.</p>
</blockquote></li>
<li><p>There are other effect could not be interpreted simply by the two-weeks or one-month prediciton model, but it's crutial. I personally think it's the varied temperature along the whole year.</p></li>
</ol>
"
"0.0627455805138159","0.0846667513334603","118637","<p>I use: <code>fit = auto.arima(Y, xreg=X)</code> in R to get ARIMA(1,0,0), result as follows:  </p>

<pre><code>ar1: 0.3793;    intercept: 9132.46;    X: 22.0469
</code></pre>

<p>Then:  </p>

<ol>
<li>I build the function: <code>(Y(t) - 9132.46) = 0.3793*(Y(t-1) - 9132.46) + 22.0469*X(t)</code> in Excel to calculate fitted value and predicted value manually.</li>
<li>In R, I use <code>fitted(fit)</code> to get fitted value, <code>forecast()</code> to get predicted value.</li>
</ol>

<p>But I found the results of 1) and 2) are different, is there anything wrong with the function I built? </p>
"
"0.076847327936784","0.103695169473043","31073","<p>I have a simple time series of one hour intervals:</p>

<pre><code>library('forecast')

# load an hourly time series of points
usage &lt;- ts(scan('http://cl.ly/102L0j3o1p2m0m3p0t2o/usage'), frequency = 24)
plot.ts(usage)
</code></pre>

<p>the Holt-Winters forecast looks as expected. <a href=""http://cl.ly/081o3I1P2A010a0Z2r3t"" rel=""nofollow"">plot</a></p>

<pre><code>usage_forecast_hw &lt;- forecast(HoltWinters(usage), h = 168)

# extremely low p-value - 2.2e-16
Box.test(usage_forecast_hw$residuals, lag = 20, type = 'Ljung-Box')

# prediction intervals look as expected.
plot(usage_forecast_hw)
</code></pre>

<p>but when I try to use ETS to do the forecast, it comes out flat. <a href=""http://cl.ly/173e2h200U360i43371p"" rel=""nofollow"">plot</a></p>

<pre><code># forecast using ets, it uses M, Md, N for model parameters (i'm not sure what Md is)
# AIC = 78323.94
usage_forecast_ets &lt;- forecast(ets(usage), h = 168)

# extremely low p-value - 5.551e-16
Box.test(usage_forecast_ets$residuals, lag = 20, type = 'Ljung-Box')

plot(usage_forecast_ets)
</code></pre>

<p>any help would be appreciated</p>
"
"0.0627455805138159","0.0846667513334603","28472","<p>A Regression with ARIMA errors is given by the following formula (saw on Hyndman et al, 1998):</p>

<p>$Y_t = b_0 + b_1 X_{1,t} + \dots + b_k X_{k,t} + N_t$</p>

<p>where $N_t$ is modeled as an ARIMA process.</p>

<p>If we have that the model for $N_t$ is ARIMA$(0,0,0)$, then $N_t = e_t$, and $Y_t$ is modeled by an ordinary regression.</p>

<p>Suppose the following data:</p>

<pre><code>a &lt;- structure(c(29305, 9900, 9802, 17743, 49300, 17700, 24100, 11000, 
10625, 23644, 38011, 16404, 14900, 16300, 18700, 11814, 13934, 
12124, 18097, 30026, 3600, 15700, 12300, 14600), .Tsp = c(2010.25, 
2012.16666666667, 12), class = ""ts"")
b &lt;- structure(c(1.108528016, 1.136920872, 1.100239002, 1.057191265, 
1.044200511, 1.102063834, 1.083847756, 1.068585841, 1.084879628, 
1.232979511, 1.168894672, 1.257302058, 1.264967051, 1.234793782, 
1.306452369, 1.252644047, 1.178593218, 1.124432965, 1.132878661, 
1.189926986, 1.17249669, 1.176285957, 1.176552, 1.179178082), .Tsp = 
c(2010.25, 2012.16666666667, 12), class = ""ts"")
</code></pre>

<p>If I model it using <code>auto.arima</code> function, I have:</p>

<pre><code>auto.arima(a, xreg=b)
Series: a 
ARIMA(0,0,0) with zero mean     

Coefficients:
              b
      15639.266
s.e.   1773.186

sigma^2 estimated as 101878176:  log likelihood=-255.33
AIC=514.65   AICc=515.22   BIC=517.01

lm(a~b)

Call:
lm(formula = a ~ b)

Coefficients:
(Intercept)            b  
      48638       -26143  
</code></pre>

<p>Coefficients from the models differ. Shouldn't they be the same? What am I missing?</p>
"
"0.0627455805138159","0.0846667513334603","119946","<p>I have some time series data where I'm modelling temperature as a function of various predictors. On physical grounds, I can expect that</p>

<p>$$\frac{dT}{dt} \propto T_a - T$$</p>

<p>where $T_a$ is the ambient temperature (which can vary over time, but whose values are known). I thus fit models of the form</p>

<p>$$\Delta T(t) \sim \alpha + \beta \left[ T_a(t) -T(t) \right] + \gamma X(t)$$</p>

<p>with $X$ being the other covariates, and $\alpha$, $\beta$ and $\gamma$ are the regression parameters. I can fit these easily enough in R:</p>

<pre><code>lm(diff(T) ~ I(Ta - T) + x, data=df)
</code></pre>

<p>and I can get predictions for the change in $T$. However, what I really want are predictions for $T$ itself. At the moment I'm calculating these via a loop, where I plug $\hat{T}(t)$ into the regression equation to obtain $\hat{\Delta T}(t+1)$.</p>

<p>Is there any R package, probably time series-related, that will do these calculations automatically?</p>

<p>Also, if there are any issues with this approach, I'd be happy to know about them.</p>
"
"0.140303383316578","0.170388550274119","68131","<p>Just asking if someone knows why the prediction intervals are quite different when one uses a time series analytic method of estimation <em>versus</em> when one simulates such time series. </p>

<p>For example, I used the forecast package's <code>auto.arima</code> function to get the best fit to my data, say it was an ARIMA(1,1,1), and then, on the one hand, I simulated such process doing around 10 thousand simulations and then calculating the 95% percentile with ""quantile"" function, and on the other hand, I used R's <code>forecast</code> package to do it. So I realized that these different approaches gave prediction intervals with different width (actually, those related with simulation approach are closer than those obtained with forecast package). The way I simulated such time series process is simulating the parameters as random variables distributed normally with mean equal to its estimated value and standard deviation equal to its related standard error. The ""white noise"" variables related with the Moving Average (MA) part of the process were simulated as normally distributed with mean zero and variance equal to the variance of the residuals.</p>

<p>Thanks in advance for your help.</p>
"
"0.108678533400333","0.122205929184461","120008","<p>I am interested in fitting an ARIMAX model using R.
As known, ARIMAX can be understood as a composition of ARIMA models and regression models with exogenous (independent) variables. I have a time series $Y_i$, and want to estimate the ARIMA and nonlinear coefficients. The nonlinear model is the following:</p>

<p>$y_i=Î²_0+Î²_1t_i+Î²_2d+Î²_3 sin(2Ï€t_i/Î²_4 )+Î²_5 (-1^{t_i})+Îµ_i$,  nonlinear 
regression with an exogenous variable.
Where 
$t_i$ =1, 2â€¦, 60
and</p>

<p>d = dummy variable with 20 0's and 40 number 1's</p>

<pre><code>d=c(rep(0,20),rep(1,40))
</code></pre>

<p>And an ARIMA model (1,1,1) for $Y_i$. Therefore, I want to estimate simultaneously the $Î²_i$ and the ARIMA coefficients in order to avoid the confusion between the exogenous coefficients and ARIMA coefficients.  I know that $arima()$ can deal with this formulation but, how do the nonlinear model can be set within function function?. It seems that the <em>xreg</em> term only deals with linear parameters.</p>
"
"0.125491161027632","0.148166814833556","217507","<p>I am trying to build an R tool for forecasting a (hopefully) wide range of time-series. I have settled on using several models, taking the forecasts from each, and deriving a weighed average of them using some weights.</p>

<p>My approach for arriving at appropriate weights for the averaging is to evaluate each model several times on parts of the historical data. For example, for monthly series I do the following:</p>

<blockquote>
  <p>I evaluate a one-step forecast for each model (five of them) for each of the last 12 months in the historical data $\{a_{i,j}\mid i\in\{1,\ldots,5\},j\in\{1,\ldots,12\}\}$ with $a'_{j}$ the actual observations. I evaluate six non-overlapping (ex. Oct+Nov+Dec, then Jul+Aug+Sep, etc.) three-step forecasts for each model, taking the mean of the forecasts for each of the five models at a time $\{b_{i,j}\mid i\in \{1,\ldots,5\},j\in\{1,\ldots,6\}\}$ with $b'_{j}$ as the mean of the relevant actuals at each time. Finally, I evaluate four six-month-overlapping (ex. Jan through Dec, Jul through Jun, etc.) 12-step forecasts for each model, taking again the mean for each model, getting the final set $\{c_{i,j}\mid i\in\{1,\ldots,5\},j\in\{1,\ldots,4\}\}$ with $c'_{j}$ the means of the relevant actuals.</p>
  
  <p>I put </p>
  
  <p>$$A=\left(\begin{array}{c}a_{i,j}\\b_{i,j}\\c_{i,j}\end{array}\right), x=\left(\begin{array}{c}w_1\\\ldots\\w_5\end{array}\right), b=\left(\begin{array}{c}a'_j\\b'_j\\c'_j\end{array}\right)$$
  ! and use <code>optim</code> from the <code>stats</code> package to optimise $x$ to give the least MAE between the two vectors $Ax$ and $b$.</p>
</blockquote>

<p>So my question is</p>

<blockquote>
  <p><em>Is this approach conceptually valid, considering that this evaluates something like the whether the <strong>model procedure</strong> is approriate for the time series, and not whether a <strong>particular model-with-parameters</strong> is?</em></p>
</blockquote>

<p>EDIT: Question paraphrased significantly to focus on aspects not answered <a href=""http://stats.stackexchange.com/questions/163074/assigning-weights-to-an-averaged-forecast"">here</a>.</p>
"
"NaN","NaN","28752","<p>I'm curious whether something I tried makes sense statistically...</p>

<p>I took a pile of time series inputs and performed an SVD.  I want to predict variable Y on the basis of its own time series, and the first 50 <code>SVD$u</code> factors as external regressors.</p>

<p>It looked like an ARIMA(2,2,2) was a good fit for my variable Y with external regressors from the SVDu terms.  Now, to forecast it on the basis of a path of the SVD...</p>

<p>To project the factors, I used a <code>VAR()</code> on <code>SVD\$u</code> because it was convenient.  But was this foolish?  Since the u values are orthogonal by construction, there should be no multivariate correlation, and this should deliver me a random walk, right?  Or should I expect spurious correlation that will  mess everything up, and that I should project each component of svd$u independently?  I hesitate to do the latter because I like the way VAR() conveniently returns standard errors.</p>
"
"0.076847327936784","0.103695169473043","68261","<p>I started evaluating and comparing some methods in forecasting. I used Price of dozen eggs in US, 1900â€“1993, in constant dollars in the R software FMA package. I held out the last 10 years for assessment of forecast. Below are the results:</p>

<p>I used auto arima method in the R software. Obviously the results are way off. Am I doing something incorrect ? Below is the forecast. It does not recognize the declining trend. </p>

<p><img src=""http://i.stack.imgur.com/KIM9O.jpg"" alt=""auto arima""></p>

<p>I also used an unobserved components model (UCM) and obtained a good forecast,  as below.</p>

<ol>
<li>Without outliers/level shifts there are very large standard errors and therefore wide confidence bands. <img src=""http://i.stack.imgur.com/dlIXM.png"" alt=""UCM without outliers level shifts""></li>
<li>After some iterative work, below is the output with outliers/level shifts (I know I'm overfitting here) but it did a pretty good job in forecasting; there are also narrow confidence bands. <img src=""http://i.stack.imgur.com/5SrYJ.png"" alt=""UCM with outliers level shifts""></li>
</ol>

<p>In looking at just this example the UCM seems to predict the hold-out sample more accurately than auto.arima.</p>

<p><strong>Why is auto.arima not providing a reasonable forecast?</strong></p>

<p><strong>Are state space models/UCMs better for forecasting long range?</strong></p>

<p><strong>Are there any benefits of using one method over other?</strong></p>
"
"0.076847327936784","0.103695169473043","177340","<p>As an example : I have data for production of a plant for a period of 2 years.</p>

<p>Data set: Jan 2013-Jan 2015</p>

<p>I have created a machine learning model to predict production.</p>

<p>The sales are to be predicted for the upcoming month. I can use the data from the previous month, or use the data from the previous 12-24 months. The minimum RMSE does not conform to one particular method. How can I create two separate models and combine them in a way to minimize error. I want to capture long term trends as well as catch short term (previous month's) nuances.</p>

<p>Let me know if this is clear</p>
"
"0.203318918640557","0.235158540500886","217955","<p>I'm doing some time series modeling using R and the <code>forecast</code> package, and found a minor difference I couldn't figure out. I'll reproduce my steps below. </p>

<p>First, I generate some data. While I have ""real"" data, I'll just use simulated data so that anyone can reproduce them (it makes no difference). The generated data is divided into training and test sets.</p>

<pre><code>&gt; set.seed(1234)
&gt; mydata &lt;- arima.sim(list(order = c(1,0,0), ar = 0.8), n = 500)
&gt; training &lt;- mydata[1:400]   # training set
&gt; testing &lt;- mydata[401:500]  # test set
</code></pre>

<p>Then, I fit a model to my training data:</p>

<pre><code>&gt; library(forecast)
&gt; (fit &lt;- Arima(training, order=c(1,0,0)))
Series: training 
ARIMA(1,0,0) with non-zero mean 

Coefficients:
         ar1  intercept
      0.8336     0.0462
s.e.  0.0274     0.2987

sigma^2 estimated as 1.013:  log likelihood=-570.68
AIC=1147.37   AICc=1147.43   BIC=1159.34
</code></pre>

<p>Next, I calculate one-step ahead forecasts using the test set:</p>

<pre><code>&gt; refit &lt;- Arima(testing, model=fit)
</code></pre>

<p>For my purposes, a forecast horizon of 1 is fine. So, I should evaluate model accuracy comparing the one-step ahead forecasts -- given by  <code>fitted(forecast(refit))</code> -- to the test set (<code>testing</code>).</p>

<p>I thought the first forecast value obtained using the original model (<code>fit</code>) should be equal to the first point forecast using the <code>refit</code> model, since (I assume) both forecasts are calculated from the training data. However, they're different:</p>

<pre><code>&gt; fitted(refit)[1]
[1] 0.02706320

&gt; forecast(fit)$mean[1]
[1] 1.3180435
</code></pre>

<p>Could anyone explain this difference, please? Am I assuming something wrong here?</p>

<p>For what it's worth, this particular system has R 3.2.5 with <code>forecast</code> version 5.4, but an installation with the latest <code>forecast</code> exhibits the same behavior.</p>

<pre><code>&gt; R.version.string
[1] ""R version 3.2.5 (2016-04-14)""

&gt; packageVersion(""forecast"")
[1] â€˜5.4â€™
</code></pre>

<p>EDIT 1: I had erroneously fit the model to the entire dataset, not just the training set. I corrected it above.</p>

<p>EDIT 2: Stephan's answer below prompted me to dig a little deeper. <code>forecast(refit)</code> gives forecasts past the end of the test set:</p>

<pre><code>&gt; forecast(refit, h=3)
    Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95
101     -0.1714176 -1.633258 1.290423 -2.407110 2.064275
102     -0.1352187 -2.038402 1.767965 -3.045887 2.775450
103     -0.1050416 -2.262407 2.052323 -3.404447 3.194363
</code></pre>

<p>So, it doesn't seem to be what I want (one-step ahead forecasts using observed data).</p>

<p>The AR(1) model obtained using <code>auto.arima()</code> is $\hat{y}_t=0.8336y_{t-1} + 0.0462 + e_t$. I calculated by hand the first few forecasts using this model:</p>

<pre><code>&gt; (test.5 &lt;- mydata[400:404])  # last observation from the training set, first four from the test set
[1]  1.571841404  0.003474084  0.744644046 -0.627186378 -2.420643234

&gt; 0.8336*test.5 + 0.0462  # forecasts for y(401)...y(405)
[1]  1.3564870  0.0490960  0.6669353 -0.4766226 -1.9716482

&gt; fitted(refit)[1:5]
[1]  0.02706320  0.01057917  0.62845310 -0.51516887 -2.01027834
</code></pre>

<p>With the exception of the first forecast, the numbers agree (assuming the differences are due to rounding). On the other hand, the first forecast calculated by hand (1.3565) is not too different from the first forecast given by <code>forecast(fit)</code>, which is 1.3180. So, it seems that <code>fitted(refit)</code> is what I'm after, I just don't understand why it gives a different value for the first forecast.</p>

<p>EDIT 3: Rob's answer below mostly solves the issue. I'm still puzzled by the fact that the forecasts given by <code>forecast()</code> differ from those calculated by hand, and by a seemingly fixed amount:</p>

<pre><code>&gt; (by.hand &lt;- coef(fit)['ar1']*test.5 + coef(fit)['intercept'])
[1]  1.35654540  0.04908109  0.66695502 -0.47666695 -1.97177642

&gt; (auto &lt;- c(forecast(fit)$mean[1], fitted(refit)[2:5]))
[1]  1.31804348  0.01057917  0.62845310 -0.51516887 -2.01027834

&gt; by.hand - auto
[1] 0.03850192 0.03850192 0.03850192 0.03850192 0.03850192
</code></pre>

<p>Can anybody shed some light on this?</p>
"
"0.076847327936784","0.103695169473043","125308","<p>I just fit a model to a time series. I am now required to generate a 10-year extrapolation forecast of my model. My model includes a time term, a time^2 term, 12 seasonal dummies, and 4 lagged dependent variables. If I am correct, I thought that I could not do an h step ahead forecast because of the lags, so I am trying to run 120 1-step ahead forecasts. I am using the predict() function and trying to make a for-loop for times i=313:432 (that is 120 months following our initial 312 observations).</p>

<p>I am confused with the predict () function and with for loops, and any guidance on how to generate this loop would be extremely appreciated!</p>
"
"0.108678533400333","0.146647115021353","177642","<p>I am trying to predict values based on a dataset which may contain weekly, monthly and yearly seasonal data. To simplify things I am assuming that all months have four weeks (28 days) and the year has twelve months (336 days).</p>

<p>I am using the following code in R:</p>

<pre><code>forecast(tbats(data,seasonal.periods=c(7,28,336)))
</code></pre>

<p>this does not pick up the yearly seasonality. What am I doing wrong? 
Interestingly the following line delivers different results:</p>

<pre><code>forecast(tbats(msts(b,seasonal.periods=c(7,28,336))))
</code></pre>

<p>Whats the reason?</p>

<p>The dataset:</p>

<pre><code>c(1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,29,29,29,29,29,29,29,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,29,29,29,29,29,29,29,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7)
</code></pre>

<p>Thanks a lot!</p>

<p><strong>Update:</strong></p>

<p>As suggested in the comments I updated the dataset to use 13 quad weeks or a real year with 365 days (posted for 365 days):</p>

<pre><code>c(1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,29,29,29,29,29,29,29,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,29,29,29,29,29,29,29,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,7,7,1,1,1,1,1,14,14,1,1,1,1,1,7,7)
</code></pre>

<p>The algorithm is still not picking up the yearly seasonality and the results for using an msts object and providing the seasonal periods directly in the tbats call are still different.</p>
"
"0.230689153939487","0.266814938386028","29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"0.0627455805138159","0.0846667513334603","69203","<p>I am using <code>ets()</code> from the R <code>forecast</code> package and AICc criterion to select the best model.</p>

<p>Suppose we have a time series denoted by <code>y</code>:</p>

<pre><code>x &lt;- BoxCox(y, BoxCox.lambda(y))
fit &lt;- ets(y)
fit2 &lt;- ets(x, lambda=BoxCox.lambda(y))
</code></pre>

<p>Is AICc from <code>fit2</code> comparable with AICc fit? In other words, Does <code>ets()</code> make some internal transformation that provides a comparabe AICc?</p>
"
"0.0992094737665681","0.133869888150416","68802","<p>I'm trying to understand some concepts related to predictive modeling. So let's say that I have the following data sample and am trying to regress <code>sales</code> on <code>clicks</code> and <code>calls</code>. Ultimately, I'm interested in predicting sales figures for any given month (e.g., <code>Jan</code>, <code>Feb</code>, etc.) given a change in clicks or calls. <code>Month</code> is not included in the model, but I'm not sure how I would go about building a model which would allow me to predict sales figures for <code>Feb</code> given in a change in <code>clicks</code> or <code>calls</code>.</p>

<ol>
<li><p>My data  </p>

<pre><code>df = data.frame(month_main=c(""Jan"",""Feb"",""Mar"",""Apr"",""May""),
                sales=c(50,35,60,20,50),
                month_attr=c(""Jan"",""Feb"",""Mar"",""Apr"",""May""),
                clicks=c(300,350,500,550,250),
                calls=c(100,150,200,150,150))
</code></pre></li>
<li><p>Regression Model</p>

<pre><code>m1 = lm(sales ~ clicks + calls, data=df)
summary(m1)
</code></pre></li>
</ol>

<p></p>

<ul>
<li><p>I want to build a model to predict sales for <code>Feb</code> given a change in either predictor</p></li>
<li><p>Model doesn't include <code>month</code>, but I want to predict for a given month.</p></li>
<li><p>How should I be thinking about this problem?</p></li>
<li><p>How would I perform this task in R?</p></li>
</ul>
"
"0.108678533400333","0.146647115021353","139164","<p>I'm trying to find out how to do forecasting with a mixture model (averaging the forecasts of an <code>ets</code>, an <code>arima</code> and an <code>stlf</code> model). I do not have a huge amount of statistics experience and so I'm struggling with finding out how to do it.</p>

<p>The point forecasts will just be the average of the point forecasts of the three methods, no problem.</p>

<p>The problem is how to calculate the prediction intervals. </p>

<p>I have found an R script with an attempt to do it, but the mixture prediction intervals are just calculated as an average of the prediction intervals of the models, and I am pretty sceptical about this approach - is it really that easy?</p>

<p>If not, how do I go about calculating them?</p>
"
"NaN","NaN","199579","<p>I would like to forecast some data. But I'm not sure whether I have implemented everything correctly. The accuracy is bad and I'm not sure whether it relates to methodology or some mistakes:</p>

<pre><code>library(caret)
library(forecast)
data(economics)
# Here I would like to use that approach and compare other models to arima
# via training/testing
timeSlices &lt;- createTimeSlices(1:nrow(economics), 
                           initialWindow = 500, horizon = 74, fixedWindow = TRUE)

trainSlices &lt;- timeSlices[[1]]
testSlices &lt;- timeSlices[[2]]


economics[trainSlices[[1]],]
economics[testSlices[[1]],]

# Here I fit the model
fit &lt;- Arima(economics[trainSlices[[1]],]$unemploy, order=c(4,1,3), seasonal = list(order = c(1, 0, 1), period = 7), lambda=2,method=""ML"")
    # Here I predict on new data
    pred &lt;- forecast(fit,h=length(economics[testSlices[[1]],]$unemploy))
# Here I extract the estimate
yHat &lt;- pred$mean
    # Here I check the accuracy
    accuracy(yHat,economics[testSlices[[1]],]$unemploy)
</code></pre>
"
"0.0627455805138159","0.0846667513334603","199563","<p>I did a prediction model starting from HoltWinters method on a time series (24h*30days)=720 values.</p>

<pre><code>x    &lt;- ts(data=DfIn,frequency=24*7)
HW   &lt;- HoltWinters(x,beta = FALSE)
Pred &lt;- forecast.HoltWinters(HW,180)
</code></pre>

<p>Accuracy</p>

<pre><code>                ME     RMSE      MAE       MPE     MAPE
Test set -1.983281 277.9997 197.5956 -6.295154 18.45785
</code></pre>

<p>From previous question, I read that I need to do a significance test, so I did a Ljung Box-Test, but I can't interpret the result:</p>

<pre><code>    Box-Ljung test

data:  Pred$residuals
X-squared = 16.9804, df = 20, p-value = 0.6542
</code></pre>

<p><strong>Question</strong>: Should this test tell me if this model is significant or if it can be improved? From the result, what can I see?</p>

<p>Acf:  </p>

<p><a href=""http://i.stack.imgur.com/n7JYH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/n7JYH.jpg"" alt=""enter image description here""></a></p>

<p>Also, how should I interpret it?</p>

<p>Residual: </p>

<p><a href=""http://i.stack.imgur.com/tAFGM.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tAFGM.jpg"" alt=""enter image description here""></a></p>
"
"0.172172452513037","0.188762964809914","94774","<p>I'm new in the page and pretty new in statistics and R. I'm working on a project for college with the objective of finding the correlation between rain and water flow level in rivers. Once the correlation is proved I want to forecast/predict it.</p>

<p><strong>The data</strong>
I have a set of data of several years(taken every 5 minutes) for a particular rivers containing: </p>

<ul>
<li>Rainfall in millimetres</li>
<li>River flow in cubic meters per second</li>
</ul>

<p>This river doesn't have snow, so the model is just based on rain and time. There are occasionally freezing temperatures, but I'm thinking on removing those periods out of the data as outliers as that situation is out of scope for my project.</p>

<p><strong>Examples</strong>
Here you have a couple of plots of sample data the from a rain and the rise of water a few hours later.</p>

<p><img src=""http://i.stack.imgur.com/ssmtM.jpg"" alt=""Bigger example a few days""></p>

<p><img src=""http://i.stack.imgur.com/XSkvv.jpg"" alt=""Shorter example just one rainfall period""></p>

<p>The red line is the river flow. The orange is the rain. You can see it always rains before water raises in river. There is some rain starting again at the end of the time series, but it will affect the river flow later.</p>

<p>The correlation is there. Here is what I've done in R to prove the correlation using ccf in R: </p>

<ul>
<li>the cross-correlation</li>
<li>the leading variable</li>
<li>the lag</li>
</ul>

<p>This is my R line used for the second example (one rainfall period):</p>

<pre><code>ccf(arnoiaex1$Caudal, arnoiaex1$Precip, lag.max=1000, plot=TRUE, main=""Flow &amp; Rain"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/IT62e.jpg"" alt=""ccf result for small example 2""></p>

<p>My interpretation is: </p>

<ul>
<li>that the rain leads (happens first),</li>
<li>there is a significant correlation that peaks at a lag of $\approx 450$ (I can check the exact number, I know that part). </li>
<li>I don't know how to find out the time that correlation affects the river flow, I think the name is â€œretentionâ€. What I see is the graph follows the same shape of the first graph, when the river losing the water after the rain. I don't if based on that I can say the retention lasts from $\approx 450$ when it peaks to $\approx 800$ (I can check this in the object created in the dataframe returned by <code>ccf</code> and see when the water level comes back to the value of â€œbefore rainâ€. Is that right? Is there a better way to find the retention?</li>
</ul>

<p>Am I right?</p>

<p><strong>About the time series</strong>.
This time series doesn't have periodicity or seasonality. Rain can come any time and cause an effect. It does reduce in summer, but it still happens, it's an area with a lot of rain all year around.</p>

<p><strong>Model and forecast.</strong>
I don't know how to create a model to be able to do a forecast that tells me how much is a river going to increase the volume after a period of rain. I've been trying some <code>arima</code>, <code>auto arima</code> but haven't been very successful. Should I use <code>Arima</code>, <code>vars</code> or other different multivariate model? Any link to a example would be of great help.</p>

<p>Please, let me know if you know the best way to create this prediction, what model should I use. There are a few other things I'm considering doing but taken them out of this explanation for simplicity.
I can share some data if required.</p>
"
"0.117386232408469","0.135768846660426","69906","<p>Is there a way/method/approach to decompose a time series data using regression splines:</p>

<ol>
<li>Seasonal time series into trend+seasonal+random component ?</li>
<li>A non seasonal time series into trend+random component ?</li>
</ol>

<p>I'm familiar with STL, Census and classical decomposition in R. All these techniques require time series data with seasonal component. We cannot extract trend if the time series is non seasonal (i.e., Frequency = 1). </p>

<p>I recently came across this interesting article which is data driven in the recent 2013 ISF. Any insights on methods like these that are data driven decomposition using regression splines and that can be readily programmed in software packages such R would be greatly helpful.</p>

<p>Thanks so much</p>

<p><strong>Detrending time series with cycle and seasonal components</strong>
<em>Tatyana Krivobokova and Francisco Rosales</em>
In this work we discuss a nonparametric and completely data-driven approach to the decomposition of time series into a trend (cycle), seasonal and random components. Two former are modeled with penalized splines, while the latter is assumed to follow an ARMA structure. Empirical Bayesian approach allows to estimate both smoothing parameters and the orders of the ARMA process simultaneously resulting in an efficient, fast and data-driven decomposition procedure. The practical relevance of the approach is illustrated by real-data examples. The work is the extension of Kauermann, G., Krivobokova, T., Semmler, W. (2011) Filtering time series with penalized splines. <em>Studies in Nonlinear Dynamics &amp; Econometrics.</em></p>
"
"0.0627455805138159","0.0423333756667302","31357","<p>Coming from basically no time series back ground, this is likely a simple question, but what is the relationship between ""being able to"" use an additive decomposition of a series into seasonal, trend and remainder and a Box Cox transformation?</p>

<p>From Professor Hyndman's <a href=""http://robjhyndman.com/researchtips/tscharacteristics/"" rel=""nofollow"">blog</a>:</p>

<blockquote>
  <p>Because not all data could be decomÂ­posed addiÂ­tively, we first needed
  to apply an autoÂ­mated Box-â€‹â€‹Cox transÂ­forÂ­maÂ­tion.</p>
</blockquote>

<p>I was wondering:</p>

<p>1) What makes an additive decomposition attractive relative to a multiplicative one (which I understand is basically the other choice).</p>

<p>2) What is the requirement for an additive decomp and what does Box Cox do to make this possible? I think of Box Cox for ANOVA and reducing heteroskedasticity. Is there a tie in with decomposition of a series?</p>
"
"0.203529284882695","0.249668628757713","31374","<p>Motivation: I was hired as an intern a few weeks ago to figure out if my company needed to buy new machines six months in advance. Database machines take up to 4 months to install and there is a 2 month grace period.</p>

<p>I signed an NDA, so I don't think I can give any actual data.</p>

<p>The only reliable information I have now, is information on the number of logins and registrations for an education company from 2002 to 2011. I think I can get more recent information on registrations, and people are working on getting login information. We stopped logging login information in 2011 so there will be a gap of no data when I try to forecast :(</p>

<p>The information is collected daily.</p>

<p>I've created a time series forecast of the data using R. I used this tutorial
<a href=""http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html#arima-models"" rel=""nofollow"">http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html#arima-models</a> To make a holt winters exponential model with daily frequency (frequency = 365). I've removed February 29 from the data. Unfortunately the gap in login data means I will have to try a more specific ARIMA right? Will I be able to use arima if there are long gaps in the data? Also, the arima function in R doesn't allow for frequencies greater than 350, and it runs out of memory quickly, so I'd have to use a monthly model (freq = 12). I have tried using fourier but the predictions didn't look right intuitively. Since I want to know what the peak usages are though, I think I might want to be more specific. Is it ok to use a weekly frequency (freq = 52) and just remove Dec 31?</p>

<p>Is daily frequency allowable? Like can I use exponential smoothing with daily frequency even though Sept 7, 2012 might fall on a Sunday, whereas Sept 7, 2011 and 2010 and 2009 might all be weekdays. There is a daily, weekly, and yearly seasonality in demand and number of logins. Eg. 6pm, and Monday, and September are more loaded in general than 4am, and Saturday, and May. There is a yearly seasonality in number of registrations.</p>

<p>I've been having some issues with the login predictions
The problem is that variability increases too much before 6 months have even passed. At the 80% confidence interval. The projection line extends into 2012 and the orange area is the 80% confidence interval. Logging and using additive exponential smoothing gave me much more variability than multiplicative exponential smoothing.</p>

<p>It's not useful to the company to say that ""well you might have 8 jillion logins sometime in the next 6 months and you might have 20% more than you had last year."" How do I reduce the variance in the projection?</p>

<p><a href=""http://img836.imageshack.us/img836/8460/holtwintersloginmultipl.png"" rel=""nofollow"">http://img836.imageshack.us/img836/8460/holtwintersloginmultipl.png</a></p>

<p>Finally, I was thinking that after I got accurate projections, I'd put logins and registrations in a neural network, and I'd put something like average wait time on a few machines as the ouput variable, and I'd forecast peak projected processing power demand in 6 months. There are other variables to consider, like software releases that change cpu demand per user, but I'm hoping the neural network will learn when these happen, or that they are easy to detect and account for. I don't have any good data on average wait time yet, but assuming I find some, is this a good plan?</p>
"
"0.208300522350638","0.21997067253203","146223","<p>I'm trying to build a model to forecast direct mail marketing campaign responses. In the ""response"" vector are the average number of responses from a marketing campaign from day 1 to day 63 (8 weeks).  Before I took the average, I first normalized the individual campaign responses to adjust for seasonality (day-of-the-week, holidays, promotions, etc.) and lined up the campaigns from the first day to the last.  My goal is to model this ""response time series"" with a smooth curve (i.e. continuous probability) via a Weibull curve. Then I want to differentiate between days to get the % of total responses that I'll get in a certain day.  In the past I've had success using the Weibull curve to model data that takes on this ""response curve shape"" but this time it's not working well.  Any suggestions? </p>

<p>Here's my code: </p>

<pre><code>#The ""responses"" vector contains an average number of responses to a Direct Mail marketing campaign from day 1 to day 63 (8 weeks)
#these repsonses have been normalized to account for day-of-the-week variability, holidays, promotions, etc., then lined up by the first starting day

responses &lt;- c(
24.16093706,
41.59607507,
68.20083052,
85.19109064,
100.0704403,
58.6600221,
86.08475816,
88.97439581,
65.58341418,
49.25588053,
53.63602085,
47.03620672,
29.71552264,
32.85862747,
31.29118096,
23.67961069,
19.81261675,
18.69300933,
17.25738435,
12.01161679,
12.36734071,
14.32360673,
11.02390849,
9.108021409,
9.647965622,
8.815576548,
5.67225654,
5.739220185,
6.233999138,
5.527376627,
5.024065761,
5.565266355,
4.626749364,
3.480761716,
4.621902301,
4.518554271,
4.075985188,
3.204946787,
3.174020873,
2.966915873,
2.129178828,
2.673009031,
2.410429043,
2.331287075,
2.509300578,
2.13820695,
2.53433787,
1.603934405,
1.555813592,
1.834605068,
1.842905685,
1.454045577,
2.08684322,
1.318276487,
0.807666643,
1.333167088,
1.004526525,
1.180110123,
1.078079735,
1.151394678,
1.426747942,
0.699119833,
0.583347236)




set.seed(2)
install.packages(""MASS"")
library(""MASS"")


shape_and_scale &lt;- fitdistr(responses,'weibull')

#check the shape and scale
shape_and_scale


#now use the curve () function along with dweibull () and the shape/scale factors to fit a smooth curve. 
#to look at the curve from the interval from 0 to 63, set 0 and 63 as parameters 
?dweibull 
curve_results &lt;- curve(dweibull(x,shape_and_scale$estimate[1],shape_and_scale$estimate[2]),from=0, to=63, n=500)
#the above only works if the first argument of dweibull is x (because it's nested in the curve function and it's passing 0 to 63 through). 
#...in order to run dweibull() as a stand alone function you'd need to assign a vector of numeric values as its first argument, or explicitly state a value
#the more ""n's"" you choose, the tighter the fit. 

curve_results
#by default the curve() function evaluates at 101 points. This is why it increases in increments of .63 instead of 1. 

#notice that when you integrate the dweibull function from 0 to 63 the area (i.e. density) under this part of the curve is only .94.
integrate(dweibull, 0, 63, shape_and_scale$estimate[1],shape_and_scale$estimate[2])
#in order to have the density equal 1, you'd need to integrate from 0 to Inf
#integrate(dweibull, 0, Inf, shape = 0.70730466,scale = 13.79467490)


#These are the densities from 0 to 63
diff(pweibull(0:63,shape_and_scale$estimate[1],shape_and_scale$estimate[2]))
#notice that they sum to .94
sum(diff(pweibull(0:63,shape_and_scale$estimate[1],shape_and_scale$estimate[2])))


#now we want to scale the densities so that represent a % of the total area under the weibull distribution from 0 to 63
diff(pweibull(0:63,shape_and_scale$estimate[1],shape_and_scale$estimate[2]))

scaled_densities &lt;- diff(pweibull(0:63,shape_and_scale$estimate[1],shape_and_scale$estimate[2]))/sum(diff(pweibull(0:63,shape_and_scale$estimate[1],shape_and_scale$estimate[2])))
sum(scaled_densities)
</code></pre>

<p>Compare the actual response data with the smooth curve created using the <code>weibull ()</code> function. </p>

<p><img src=""http://i.stack.imgur.com/XBlrs.png"" alt=""Actual Responses""></p>

<p><img src=""http://i.stack.imgur.com/50aSt.png"" alt=""Weibull output""></p>

<p>Comparison: </p>

<p><img src=""http://i.stack.imgur.com/eKfcL.png"" alt=""enter image description here""></p>

<hr>

<p>After @tristen suggested that I need to make my ""responses"" vector the actual observations (as opposed to a histogram of the total by day), this is the new ""responses"" vector I should be using.  However, as you'll see, the curve is still not capturing the height of the actual data: </p>

<pre><code>responses &lt;- c(
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  2,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  3,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  9,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  10,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  11,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  12,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  13,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  14,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  15,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  16,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  17,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  18,
  19,
  19,
  19,
  19,
  19,
  19,
  19,
  19,
  19,
  19,
  19,
  19,
  19,
  19,
  19,
  19,
  19,
  20,
  20,
  20,
  20,
  20,
  20,
  20,
  20,
  20,
  20,
  20,
  20,
  21,
  21,
  21,
  21,
  21,
  21,
  21,
  21,
  21,
  21,
  21,
  21,
  22,
  22,
  22,
  22,
  22,
  22,
  22,
  22,
  22,
  22,
  22,
  22,
  22,
  22,
  23,
  23,
  23,
  23,
  23,
  23,
  23,
  23,
  23,
  23,
  23,
  24,
  24,
  24,
  24,
  24,
  24,
  24,
  24,
  24,
  25,
  25,
  25,
  25,
  25,
  25,
  25,
  25,
  25,
  25,
  26,
  26,
  26,
  26,
  26,
  26,
  26,
  26,
  26,
  27,
  27,
  27,
  27,
  27,
  27,
  28,
  28,
  28,
  28,
  28,
  28,
  29,
  29,
  29,
  29,
  29,
  29,
  30,
  30,
  30,
  30,
  30,
  30,
  31,
  31,
  31,
  31,
  31,
  32,
  32,
  32,
  32,
  32,
  32,
  33,
  33,
  33,
  33,
  33,
  34,
  34,
  34,
  35,
  35,
  35,
  35,
  35,
  36,
  36,
  36,
  36,
  36,
  37,
  37,
  37,
  37,
  38,
  38,
  38,
  39,
  39,
  39,
  40,
  40,
  40,
  41,
  41,
  42,
  42,
  42,
  43,
  43,
  44,
  44,
  45,
  45,
  45,
  46,
  46,
  47,
  47,
  47,
  48,
  48,
  49,
  49,
  50,
  50,
  51,
  51,
  52,
  53,
  53,
  54,
  55,
  56,
  57,
  58,
  59,
  60,
  61,
  62,
  63
)
</code></pre>

<p>However, the curve is still not capturing the height of my original data (see picture below). Any thoughts?   </p>

<p><img src=""http://i.stack.imgur.com/vgDS0.png"" alt=""enter image description here""></p>

<hr>

<p>@tristen suggested that I use a log-normal distribution as opposed to a Weibull distribution which essentially solved my problem.  BIG THANK YOU! See below for the finalized product: </p>

<p><img src=""http://i.stack.imgur.com/mkwFe.png"" alt=""enter image description here""></p>
"
"0.0887356509416114","0.0898026510133874","31473","<p>I have to forecast a large set of (hierarchical) time series and since the R package <a href=""http://cran.r-project.org/web/packages/hts/index.html"" rel=""nofollow"">hts</a> allows for confidence intervals for their ensemble, I'd like to use it. I haven't found an example of how to use it, yet. How should I forecast the following series with it: The time series have a simple form. The sum of $n_3$ level-$3$-series make up a level-$2$-series and the sum of $n_2$ level-$2$-series make up a level-$1$-series and the sum of $n_1$ level-$1$-series make up the top-most level-$0$-series (where sums are taken over series in one point in time, $n_j$ natural numbers for  $j=0,1,2,3$).</p>
"
"0.117386232408469","0.135768846660426","69976","<p>Given a rainfall forecast of <code>d days</code> ahead and historic data collected over <code>y years</code>, what's a simple (but correct) way to associate the rainfall probability for each forecast day? </p>

<p>I know this is a complex problem, but my purpose is to quantify the probability of a given rainfall volume 'happen' (ie, observed rainfall >= forecast for that day).</p>

<p>My first take was to use a simple frequency approach over the years:</p>

<pre><code>target_day = 15
target_month = 1 
target_day_forecast = 5

data = select over years such that day = target_day AND month = target_month
n = number of days where forecast &gt;= target_day_forecast
total = number of points in data

return n/total
</code></pre>

<p>On the other hand, i had trouble fitting a <code>gamma</code> distribution:</p>

<pre><code> require(MASS)
 fit = fitdistr(serie, ""gamma"")

 #Error in optim(x = c(0, 0, 0, 0, 0, 0, 0.3, 0, 0.1, 0, 0.8, 0, 0, 0, 0,  : 
 #initial value in 'vmmin' is not finite
</code></pre>

<p>If a sum a small quantity to <code>serie</code>, it works:</p>

<pre><code>fit = fitdistr(serie + 0.1, ""gamma"")
</code></pre>

<p>My idea is to fit a <code>gamma</code> for each month, then i can extract probabilities easily. So, how can i handle the <code>gamma</code> fit problem above?</p>

<p>I also appreciate any ideas on this, 
Thanks!</p>
"
"NaN","NaN","139488","<p>I have a multivariate time series like this </p>"
"NaN","NaN","<pre><code>Time      Price      Volume       Buy_Side_Broker_Type   Sell_Side_Broker_Type",""
"NaN","NaN","09:00:00  4.590       60              HiInfBroker           LInfBroker",""
"NaN","NaN","09:00:23  4.662       34              MInfBroker            LInfBroker",""
"NaN","NaN","09:00:45  4.562       41              LInfBroker            HiInfBroker",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>I want to forecast the price using volume and Buy side broker type and sell side broker type as independent variables. How to do it for such stochastic process in R?</p>",""
"NaN","NaN","","<r><time-series><forecasting><stochastic-processes>"
"0.0992094737665681","0.107095910520333","219440","<p>I am using the excellent <code>tsoutliers</code> R package to detect outliers (additive outliers, temporary changes etc.), but the <code>cval</code> parameter in the <code>tso</code> function is providing me with inconsistent, or at least counter-intuitive results. I was under the impression that a lower value for <code>cval</code> would include more outliers (but possibly also irrelevant ones), but this doesn't always seem the case. For example, using the following data and logic, I get the following output, which is nearly exactly what I was after:</p>

<pre><code>data &lt;- c(121.54, 119.79, 119.18, 118.56, 104, 65.52, 66, 119.18,
123.42, 119.18, 118.56, 99, 61.74, 67.98, 119.18, 123.42, 120.36,
115.14, 98, 62.37, 67.98, 122.72, 121, 116.82, 117.42, 98, 83.538,
103.096, 165.332, 185.6955, 145.848, 129.162, 101, 62.37, 64.68,
115.64, 124.63, 115.64, 118.56, 102, 62.37, 67.32, 115.64, 122.21,
121.54, 114, 103, 62.37, 65.34, 118, 122.21, 119.18, 114, 99, 65.52,
65.34, 118, 122.21, 115.64, 117.42, 73.5, 40.131, 41.184, 79.4376,
95.832, 105.138, 117.42, 100, 63, 66.66, 122.72, 123.42, 116.82, 114,
98, 61.74, 64.68, 116.82, 121, 188.152, 114, 99, 61.74, 66, 122.72,
118.58, 115.64, 112.86, 101, 63.63, 66.66)

# simple tso function
volume &lt;- ts(data, start = c(2016,1,1), frequency = 7)
data.ts.outliers &lt;- tso(volume, types = c(""AO"", ""LS"", ""TC""), cval = 3.0)
data.ts.outliers
plot(data.ts.outliers)
</code></pre>

<p><a href=""http://i.stack.imgur.com/CLf7Y.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CLf7Y.png"" alt=""Expected results, cval = 3.0""></a></p>

<p>However, using <code>cval = 2.9</code>, as well as most other values for <code>cval</code> above or below 3.0, I get the following results, which is missing some key outliers:</p>

<p><a href=""http://i.stack.imgur.com/i6dIO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/i6dIO.png"" alt=""Missing outliers, cval = 2.9""></a></p>

<p>As I want to use this package without manual review for each timeseries, Ideally I'd be able to use a slightly lower <code>cval</code> value to ensure I am capturing most outliers, but the inconsistency that I am seeing is not allowing for this. Anything I am missing?</p>
"
"0.0887356509416114","0.11973686801785","139999","<p>Using historical daily order totals, I'm wanting to forecast the totals of the next 7 days. It's known in my field that these totals fall subject to weekly and yearly seasonal trends. Called <code>data</code>, below are the historical order totals for the past 795 days:</p>

<pre><code>12  17  17 171 164 173 151  86  15 158 189 192 131 133  45  27 130 167 182 175 111  37  19 152 178 177 222 158  69  30 170 187 190 190 185  76  22 155 215 166 201 154  48  27 135 156 204 192 149  68  27 150 181 192 188 118  79  26 153 160 191 213 159  68  23 144 203 201 198 157  70  42 160 213 218 220 146  65  36 155 177 232 188 164  60  31 152 196 207 221 160  68 24 168 192 191 232 189  73  13 151 174 215 222 181  57  25 162 194 205 170 151  67  21 157 246 235 207 148  66  20 137 189 168 224 160  66  41 153 179 211 162 127  54  19 139 185 192 220 154  69  25 162 202 203 174 165  74  21  27 152 173 168 194  73  32 149 205 235 224 190 58  28 158 178 216 248 179  79  19 150 177 224 237 157  62  23 134 187 203 214 131 101  33 179 186  91 187 127  81  27 156 171 244 232 169  90  34 173 177 176 167 129  71  21 143 172 191 205 157  71  35 137 156 196 179 131 101  41 138 151 181 196 122  59  31 133 141 201 173 144  50  20 113 154 205 200 151  92  40 140 153 199 194 137  62  39  15 152 180 201 114  88 51 150 140 170 202 170  67  35 170 166 189 211 142  94  32 167 172 200 216 177  68  39 164 163 217 201 159  77  24 131 192 221 182 161  70  33 175 161 194 199 132  87  24 165 156 234 181 123  68  37 181 202 179 191 131  78  47 185 158 182 183 127  94  42 161 184 222 202 167 74  33 152 172 149  49  81  47  10 124 203 171 181 139  76  20 155 197 174 201 152  60  34 171 160 196 202 140  74  24 155 210 188 158 124  69  31  30   1 104   1 172 157  69  15   1 48  26 148 209 182 109  29 180 180 197 209 155  89  45 148 128 161 160 116  66  24 149 144 166 194 125  61  38 154 164 155 142 140  41  21 100 139 204 185 113  80  32 130 144 175 174 129  61  25 153 156 200 217 101  68  26 146 115 167 169 139  70  35 175 111 133 168 122  68 31 124 127 160 190 132  99  33 140 166 205 230 131  61  32 156 179 193 192 158  72  15 148 146 176 219 164  79  22 123 180 193 187 128  89  24 133 158 166 131 111  62  18 126 112 106 169 140  83  36 124 146 158 133 138  62  18 141 127 174 142 105  45  19 147 167 176 192 116 62  31 133 160 151 191 134  78  27  91 118 171 182 137  90  32 178 154 175 196 114  84  23 167 169 167 206 120  74  23 154 162 185 152 119  81  21 134 155 199 183 157  89  28 160 188 164  71  84  86  27 138 178 159 214 132  89  29 158 188 186 184 107  71  25 128 150 175 150 124  81  39 123 142 178 179 126  76  21 149 169 203 185 128  63  35 155 166 195 174 118  72 31 128 144 171 182 149  94  25 136 167 213 177 106  72  25  18 142 152 178 160 111  12 166 211 195 206 160  75  34 145 166 186 156 137  45  32 136 172 196 218 134  72  31 143 189 186 176 122  83  34 142 144 169 180 111  67  23 139 122 170 168 105  72  21 145 201 181 199  93 60  34 120 147 150 133  77  83  34 119 155 136 157 109  70  31 124 158 183 186  99  79  19 161 175 166 178 117  80  31 128 163 118  34  75  23  28 138 191 205 195 154  76  33 151 170 169 159  89  59  24  98 137 162 159 103  73  27 148  65 106  97  81  29 123  95   1   6  22 85  60  20 124 134 121 111  97  48  28 115 164 164 175 141  62  26 131 174 175 180 134  72 15 174 156 251 164 146  70   4  80 121 143 132  93  45  33 156 165 174 169 334  36  17 159 161 158 129 105  44  25 137 164 169 121  87  43  27
</code></pre>

<p>Using <code>ts.plot(data)</code> yields the following graph:
<img src=""http://i.stack.imgur.com/D6NBs.jpg"" alt=""OrderData""></p>

<p>Following along with <a href=""http://robjhyndman.com/hyndsight/dailydata/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/dailydata/</a>, my code to perform this forecasting follows:</p>

<pre><code>myts&lt;-msts(data, seasonal.periods=c(7,365.25))
fit&lt;-tbats(myts)
fc&lt;-forecast(fit, h=7)
summary(fc)
plot(fc)
</code></pre>

<p>The range of values in <code>data</code> spans from 1 to 334. However, `summary(fc)' gives me forecast output of</p>

<pre><code>&gt; summary(fc)

Forecast method: TBATS(1, {5,5}, 1,{-})

Model Information:
BATS(1, {5,5}, 1, -)

Call: tbats(y = myts)

Parameters
  Lambda: 0.999952
  Alpha: 0.1874778
  Beta: 0.004592643
  Damping Parameter: 1
  AR coefficients: 0.4336 -1.171171 0.298187 -0.708633 -0.321975
  MA coefficients: -0.178688 1.155448 -0.175872 0.555014 0.152693

Seed States:
            [,1]
 [1,] 108.161300
 [2,]   3.930371
 [3,]   0.000000
 [4,]   0.000000
 [5,]   0.000000
 [6,]   0.000000
 [7,]   0.000000
 [8,]   0.000000
 [9,]   0.000000
[10,]   0.000000
[11,]   0.000000
[12,]   0.000000

Sigma: 9.027342e+72
AIC: 10999.28

Error measures:
                        ME         RMSE          MAE          MPE         MAPE        MASE
Training set -1.564113e+71 9.100417e+72 9.472706e+71 -5.89332e+71 1.561621e+72 1.80209e+70

Forecasts:
         Point Forecast        Lo 80         Hi 80        Lo 95         Hi 95
3.178082  -2.941534e+74 0.000000e+00 -2.824896e+74 0.000000e+00 -2.763152e+74
3.180822   3.428400e+73 2.150951e+73  4.705872e+73 1.474724e+73  5.382132e+73
3.183562   5.005464e+73 3.638168e+73  6.372778e+73 2.914374e+73  7.096597e+73
3.186301  -1.512812e+74 0.000000e+00 -1.362879e+74 0.000000e+00 -1.283509e+74
3.189041  -1.568978e+74 0.000000e+00 -1.379985e+74 0.000000e+00 -1.279939e+74
3.191781   8.620310e+72 0.000000e+00  2.809916e+73 0.000000e+00  3.841095e+73
3.194521  -2.646199e+74 0.000000e+00 -2.447683e+74 0.000000e+00 -2.342596e+74
</code></pre>

<p>With the addition of the forecast yielding a graph of:
<img src=""http://i.stack.imgur.com/AoBNs.jpg"" alt=""OrderData Plus Forecast""></p>

<p>So while my historical data ranges from 1-334, the point forecasts for the next 7 days using MSTS/TBATS are pushing magnitudes of 3e+74.</p>

<p>Needless to say, I'm quite confused as to how this output is so extreme. As far as I know I'm following the blueprint correctly to perform such forecasting. Does anyone know why I'm receiving such crazy numbers for my forecast? </p>

<p>I'll continue to triple-check everything but I'm at a complete loss as to what is going on. Any help at all would be greatly appreciated.</p>
"
"0.153694655873568","0.207390338946085","140542","<p>It is my understanding that if one wants to build multiple time series models on a time series that goes from 2000 to today (2015) monthly; and one wanted to use that information to forecast 3 months in the future, it is common approach to split your data into ""train"" and ""test"" datasets.</p>

<p>Your test dataset would be the last 3 months of your time series (jan 2015, feb 2015, march 2015) (pretend we're already in april for simplicity sake). You would then 'define' your model on your training dataset, and then compute it's errors against your test dataset (defined as predicted vs actual).</p>

<p>This way you could try out many multiple models and pick the one with the lowest ""forecast prediction error"".</p>

<p>However my question is: By ignorning those last 3 months of data, how do you then use that model to forecast values later in time?  Example: say you wanted to forecast April-June. Is it standard procedure to apply the same model (that wasn't built on the last 3 months) to the April - June forecast period?</p>

<p>If so is this something you can do in R with a package? It seems like the <code>forecast</code> function only works to forecast forward from the training dataset, and you can't apply it to other time series objects.</p>

<p>Or does one 're-build' the model on the entire time series (2000 to 2015 March) and then use that model to forecast into April-June?</p>

<p>I am pretty confused by this and any help would be appreciated.</p>
"
"NaN","NaN","223666","<p>I have a forecast object in R. When I look at the summary I can see 'Model Information: BATS(1, {1,1}, -, -)'</p>

<p>What do these numbers in the parentheses stand for?</p>
"
"0.133103476412417","0.139693012687492","223043","<p>I have half hourly multi-seasonal(daily, weekly, quarterly, yearly) time series data and I divided them into training part and testing part. </p>

<pre><code>demand.train&lt;-tbats(demand.train.ts)
forecast&lt;-forecast(demand.train,h=48*27)
</code></pre>

<p>I would like to </p>

<ol>
<li>Plot fitted value with confidence interval with true observation. (I can not find confidence interval in the <code>str(demand.train)</code>).</li>
<li>Plot forecasted value with confidence interval only. (I know plot(forecast) could do, but I am wondering how to plot forecasted part only, without showing all previous values.)</li>
<li>Add test data to the second plot. </li>
</ol>

<p>I tried the following but don't know how to add test data in: </p>

<pre><code>autoplot(forecast,include = F) 
</code></pre>

<p><a href=""http://i.stack.imgur.com/dyNQ3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dyNQ3.png"" alt=""enter image description here""></a></p>

<p>I also tried </p>

<pre><code>plot(forecast$mean)
lines(demand.test, col=""red"")
legend(""top left"",lty=1,bty = ""n"",col=c(""red"",""black""),c(""test data"",""forecast data""))
</code></pre>

<p><a href=""http://i.stack.imgur.com/2BjO7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2BjO7.png"" alt=""enter image description here""></a></p>

<p>but I don't know how to add confidence interval in. I think they're stored in <code>forecast$upper</code> and <code>forecast$lower</code>.</p>

<p>Anyone can help me to improve the plot? Also I don't understand what the x-axis mean in the plot. </p>

<p>Many thank!</p>
"
"0.0443678254708057","0.059868434008925","32528","<p>I have fitted ARIMA(5,1,2) model using <code>auto.arima()</code> function in R and by looking order we can say this is not a best model to forecast. If outliers exist in the data series, what is the method to fit a model to such data?</p>
"
"0.117386232408469","0.135768846660426","222832","<p>We have data values pertaining to BPS (bits per second) traffic on a networking device. We have data from for a particular month (October) from the past 4 years. The data points are available in a 1 minute interval.</p>

<p>So we have 31 days * 24 hours * 60 minutes = 44640 values for each year. Multiply that by 4 and we get ~180,000 data points in our CSV file. We have tried several model including TBATS, ARIMA etc. to make future predictions. For example, we need to predict 44640 values for the october of next year. Problem is that so many data values means that fitting a model takes forever and it's not worth waiting an entire day of processing just to find that model is predicting a straight line trend around the average of previous values.</p>

<p>We are looking for possibly reducing this data using something like exponential smoothing but we do not know how. The past data values are as follows:</p>

<pre><code>8839
29191985
3825997
439694949
5186727
5747251
4814919
489752985
481456366
53712118
51364413
57449919
48123322
473151317
529185483
51284866
528115232
597333333
535883672
594275668
549679615
589267353
54916916
756419719
65492594
587599734
616325563
68434481
63351749
624134894
61665387
697646113
61722678
689499647
6884953
618223888
67283625
7451432
773956231
72682555
748525567
682498934
71892441
8527712
752342356
68912676
746693391
7241629
712685465
748971655
74339677
773571787
81173992
9369364
885665416
969439265
99578482
13281261
127297176
1577597
129853832
13882798
1184388675
115559261
118735937
121685158
1128946618
1157798227
1143165165
11632918
122479785
11341628
116385628
12621439
1172845976
1214564385
1795176
12957522
1183316274
12619916
12519533
135765784
1399453354
1399224864
1372868436
1331569834
137852813
128497677
1297789678
135918171
1294935824
1384582825
1362893276
145228865
142459451
1523728929
1553973554
155186563
149211641
159253766
141712263
14764913
148991924
1562214535
164371933
1546871
163188462
168156746
168938876
16835799
171595761
1663196329
1692558573
17636281
18258581
177213887
1652531676
19852331
1789876462
1789629233
1748867173
181994385
176165681
1969791999
19861387
1947295162
287128848
235583965
1968433253
217279852
2212697598
1953822855
2212983294
2166245238
29418584
28276258
22111712
21361513
2114169137
2314153846
216195463
1948538537
2131395686
22873135
2121744212
2261766416
1952463426
231837712
211836243
21321957
231673786
23586221
237934824
23857991
226835959
22527878
229163528
223611724
242565252
2451523242
242146954
22592296
2524295439
24288788
239426786
2488167389
23614618
2387528327
224687321
2352352153
219349398
211514732
223242859
2114838493
2275546998
224398369
2271632485
2237118326
231972341
223867472
232943687
2616184865
2264386319
241637212
2577586277
2473823845
247444156
261553512
264819946
2643896421
274781277
26189985
272488724
2727773421
269784662
2923184161
2835866726
29476972
313529872
3899199
2979386981
37853218
38881954
297738289
3113766229
32723531
2773715317
3137525998
367757942
36456197
297769411
2882461831
342295362
344496963
39439679
3136141447
3324496997
3253434742
338259
2895698259
31183592
374252594
38459536
312441788
3239434239
3161928
3166617496
31916915
3162371549
319837457
3141362857
32638795
3157587728
3281771348
3142241484
3368347612
327583987
3241925869
33183412
32491351
3383213
3573783926
3299445212
3268651
33138667
329333539
329314786
343676884
342544137
3286497278
34854846
31553839
3553121791
3295782535
333871824
3357511167
364861848
3412626294
33294747
348641163
327124157
3392738132
325626931
33883856
334594742
32942374
31897973
3834926556
365132813
35475637
336384187
366552233
36141892
34887985
34695147
3576451651
3335458644
3326826563
3341539
339894997
337912327
336649223
3555534642
329266359
356461957
3439773899
328435177
3758339514
346635125
361774558
335482465
3486783351
349275
341392357
37215
3497621877
364242974
3624311875
377361582
367461755
363526377
34877241
47832182
371281677
376216515
3741615717
3695335388
3628351931
372717255
3792287845
36549945
372238998
3869247316
3822289851
386173797
372368834
345429379
417153116
38749739
395119594
3746367111
383839372
391378292
367872746
372373178
3625754
379946415
37778181
3746261571
3918932444
386892529
3695653853
3959862748
415346593
42977194
412162553
41582129
41732773
471311973
4415543
3838746827
43135679
41259122
416451147
382524677
3829914759
396922256
392669399
383285533
3915829759
497197157
471337265
494296438
395495
41562899
3973355519
398198495
376359951
397532419
438115941
383579951
39116435
425944659
3961366459
3997619677
4575215
415522986
3947337112
394636114
392714147
385221299
47237153
...and so on
</code></pre>

<p>Keeping in mind that we have 
~180,000 such past data values, and we wish to predict the next 44640 future values, how do we go about making such a prediction in R?</p>

<p>We are new at R so actual code rather than abstract concepts would help a lot!</p>

<p><strong>EDIT to show how out ARIMA model got stuck in computations:</strong></p>

<p>This is the code we used for auto-arima fitting that got stuck for hours until we had to abort:</p>

<pre><code>mydata &lt;- load.csv(""2.csv"")
mydata &lt;- ts(mydata, start = c(2012,1), frequency = 44640)
require(forecast)
arimafit &lt;- auto.arima(mydata)
</code></pre>

<p>What are we doing wrong in the ARIMA model that's taking so long? </p>
"
"0.0992094737665681","0.133869888150416","76322","<p>I need to do forecasting of weekly sales using Holt-Winters technique. My data have max 92 weeks of information. I'm planning to consider 72 weeks of data for training &amp; 20 weeks of data for validation &amp; I have only available s/w to do the forecast is R. I'm preparing my training &amp; validation data set using following command</p>

<pre><code>data_ts_s = ts(data$SUM.SALES_UNITS.[c(1:72)], frequency=52)
data_ts_c = ts(data$SUM.SALES_UNITS.[c(73:92)], frequency=52)
</code></pre>

<p>But for doing forecast using HW, R needs at least 2 periods of data. Can you please help me how to do the forecasting with Holt-Winters technique without 104 weeks of data.</p>
"
"0.238928052294469","0.277932227485446","188595","<p>I have already read</p>

<p><a href=""http://stats.stackexchange.com/questions/126525/time-series-forecast-convert-differenced-forecast-back-to-before-difference-lev"">Time Series Forecast: Convert differenced forecast back to before difference level</a></p>

<p>and</p>

<p><a href=""http://stats.stackexchange.com/questions/130448/how-to-undifference-a-time-series-variable"">How to &quot;undifference&quot; a time series variable</a></p>

<p>None of these unfortunately gives any clear answer how to convert forecast done in ARIMA using differenced method(diff()) to reach at stationary series.</p>

<p>code sample.</p>

<pre><code>## read data and start from 1 jan 2014
dat&lt;-read.csv(""rev forecast 2014-23 dec 2015.csv"")
val.ts &lt;- ts(dat$Actual,start=c(2014,1,1),freq=365)
##Check how we can get stationary series
plot((diff(val.ts)))
plot(diff(diff(val.ts)))
plot(log(val.ts))
plot(log(diff(val.ts)))
plot(sqrt(val.ts))
plot(sqrt(diff(val.ts)))
##I found that double differencing. i.e.diff(diff(val.ts)) gives stationary series.

#I ran below code to get value of 3 parameters for ARIMA from auto.arima
ARIMAfit &lt;- auto.arima(diff(diff(val.ts)), approximation=FALSE,trace=FALSE, xreg=diff(diff(xreg)))
#Finally ran ARIMA
fit &lt;- Arima(diff(diff(val.ts)),order=c(5,0,2),xreg = diff(diff(xreg)))

#plot original to see fit
plot(diff(diff(val.ts)),col=""orange"")
#plot fitted
lines(fitted(fit),col=""blue"")
</code></pre>

<p>This gives me a perfect fit time series. However, how do i reconvert fitted values into their original metric from the current form it is now in? i mean from double differencing into actual number? For log i know we can do 10^fitted(fit) for square root there is similar solution, however what to do for differencing, that too double differencing?</p>

<p>Any help on this please in R? After days of rigorous exercise, i am stuck at this point.</p>

<p>Edit: Let me paste images from 3 iterations i ran to test if differencing has any impact on model fit of auto.arima function and found that it does. so auto.arima can't handle non stationary series and it requires some effort on part of analyst to convert the series to stationary.</p>

<p>Firstly, auto.arima without any differencing. Orange color is actual value, blue is fitted.</p>

<pre><code>ARIMAfit &lt;- auto.arima(val.ts, approximation=FALSE,trace=FALSE, xreg=xreg)
plot(val.ts,col=""orange"")
lines(fitted(ARIMAfit),col=""blue"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/VWVHK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VWVHK.png"" alt=""enter image description here""></a></p>

<p>secondly, i tried differencing</p>

<pre><code>ARIMAfit &lt;- auto.arima(diff(val.ts), approximation=FALSE,trace=FALSE, xreg=diff(xreg))
plot(diff(val.ts),col=""orange"")
lines(fitted(ARIMAfit),col=""blue"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/sTnxQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sTnxQ.png"" alt=""enter image description here""></a> </p>

<p>thirdly, i did differencing 2 times.</p>

<pre><code>ARIMAfit &lt;- auto.arima(diff(diff(val.ts)), approximation=FALSE,trace=FALSE, 
xreg=diff(diff(xreg)))
plot(diff(diff(val.ts)),col=""orange"")
lines(fitted(ARIMAfit),col=""blue"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/1x8ex.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1x8ex.png"" alt=""enter image description here""></a></p>

<p>A visual inspection can suggest that 3rd graph is more accurate out of all. This i am aware of. The challenge is how to reconvert this fitted value which is in the form of double differenced form into the actual metric!</p>

<p>Edit2: Why it is not so simple. Let me explain by below example.</p>

<p>Actual data with single difference and double difference.
<a href=""http://i.stack.imgur.com/hJSOF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hJSOF.png"" alt=""enter image description here""></a></p>

<p>Lets go back to actual data by using differences and first value of prior series.</p>

<p><a href=""http://i.stack.imgur.com/IW6js.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IW6js.png"" alt=""enter image description here""></a></p>

<p>If i use diff(diff(val.ts)) in auto.arima as input data, i get below fitted values. However i do not have first value of first order difference of fitted value and neither i have first data point in fitted value in original metric format! This is where i am struck!</p>

<p><a href=""http://i.stack.imgur.com/llFtr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/llFtr.png"" alt=""enter image description here""></a></p>

<p>What if i use Richard Hardy's advice and use data from actual series as reference. This gives me negative numbers. Can you imagine negative sales? And to clarify my original numbers do not have ANY negative number and it does not have any returns or cancellation data!</p>

<p><a href=""http://i.stack.imgur.com/IEKrJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IEKrJ.png"" alt=""enter image description here""></a></p>
"
"0.076847327936784","0.103695169473043","222793","<p>I have a multivariate time series data stream. I am looking for a method that can forecast the next value of one of the variables <em>as the data comes in</em>. (It would be a major advantage if there's an R package that does it.)</p>

<p>Currently I am using a VAR to do this. Say I have 1000 records. I estimate the VAR for those 1000 rows, and do my forecast. Then when the 1001st row comes in, I estimate a brand new VAR <em>from scratch</em>, based on the 1001 rows that I now have. I assume there's a more efficient method, some sort of model that can be updated as each new record is available.</p>

<p>I don't know much statistics, so if you can keep the explanation simple, that would be greatly appreciated.</p>
"
"0.108678533400333","0.146647115021353","222727","<p>I would like to create a linear distributed lag model in order to do some forecast and also being able to interpret the results.</p>

<p>Unfortunately I'm a bit confused with the process I should follow.Concept of time series is quite new for me so I'm looking for something simple.</p>

<p>I have a variable Y that I want to express by the lags of several other variables X1,...X4. It seems that the R-package <code>dynlm</code> is well adapted for this kind of model.</p>

<p>At the end, I would like to have this kind of relation :</p>

<p><a href=""http://i.stack.imgur.com/LYhXV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LYhXV.png"" alt=""what I want""></a></p>

<p>So I would like to ascertain which lags of my exogeneous variables are significant for modeling Y. I first thought using cross-correlation (<code>ccf()</code> in R) but after browsing on CrossValidated, it seems that this is not that simple.</p>

<p>Indeed, all of my variables except one(X3) are not stationary. I could difference all of them but how can I then interpret the results ?</p>

<p>Furthermore, should I also prewhiten my data? (I know there is a function <code>prewhithen()</code> included in <code>TSA</code> package).</p>

<p>Here are my time series :</p>

<pre><code>    ############################################## CROSS VALIDATED ##################################################################
    library(dynlm)
    library(tseries)

    Y&lt;-c(2.39,2.29,2.54,2.53,2.57,2.59,2.58,2.64,2.79,2.78,2.81,2.79,2.38,3.09,2.94,2.91,3.15,2.93,2.83,2.92,3.18,3.08,3.10,3.13,0.91,3.28,3.72,3.89,3.97,6.00,5.84,5.66,6.35,6.26,6.14,6.04,4.28,4.55,7.78,7.12,6.43,5.93,5.32,5.26,5.77,5.65,5.52,5.05,4.56,5.21,3.66,4.01,4.11,4.19,3.87,4.06,4.14,4.12,4.15,4.37,4.58,4.32,4.11,3.83,3.66,3.58,3.34,3.41,3.61,3.55,3.51,3.25,3.09,3.14,2.80,2.92,3.09,3.07,2.89,2.93,2.97,2.92,2.83,3.01,2.75,2.60,1.17,1.52,1.80,1.69,1.76,2.30,2.13)
    X1&lt;-c(3.8,4.0,4.3,4.4,4.7,4.4,5.0,5.2,5.2,5.2,5.4,5.5,5.8,6.3,6.3,6.7,6.9,6.5,5.8,5.5,5.0,5.0,4.9,4.8,5.0,5.0,4.9,5.0,4.8,4.7,4.7,4.7,4.6,4.8,3.6,3.6,3.5,3.3,3.2,3.3,3.4,3.2,3.1,3.0,3.1,3.1,3.0,3.0,3.0,3.2,3.1,3.2,3.1,2.9,2.7,2.8,3.0,2.9,3.0,3.0,3.0,2.9,3.0,2.9,2.8,2.6,2.5,2.5,2.6,2.5,2.6,2.6,2.5,2.5,2.6,2.6,2.7,2.5,2.3,2.4,2.4,2.3,2.3,2.3,2.3,2.3,2.2,2.2,2.2,2.2,2.0,2.1,2.2)
    X2&lt;-c(NA,6.6,6.9,7.4,6.2,7.3,7.1,7.3,8.1,8.1,8.7,8.3,8.7,9.7,10.1,10.4,9.8,9.4,9.1,9.3,9.8,9.8,9.6,9.0,8.8,8.7,8.1,8.0,8.0,7.7,6.7,6.9,7.9,7.8,7.2,6.8,6.8,7.1,6.7,6.9,6.5,6.5,5.8,6.2,6.1,6.3,7.0,6.1,6.3,6.8,6.1,6.5,6.3,6.0,5.5,6.1,5.6,5.7,5.7,5.7,5.8,5.8,5.8,5.4,5.2,5.0,4.7,4.9,4.9,4.9,4.7,4.5,4.7,4.9,5.0,5.1,5.0,4.5,4.3,4.5,4.3,4.4,4.4,4.1,4.0,4.1,3.9,4.0,3.9,4.2,3.8,4.1,4.1)
    X3&lt;-c(NA, NA, NA, 9.7, 10.3, 9.8, 10.8, 12.0, 10.7, 12.0, 10.2, 10.7, 10.0, 10.4, 10.3, 10.9, 11.4, 12.5, 11.7, 10.9, 10.4, 9.6, 8.9, 8.2, 8.3, 8.8, 9.3, 14.1, 10.7, 10.3, 9.4, 8.8, 8.8, 10.1, 10.4, 10.0, 11.0, 11.2, 10.4, 10.3, 11.0, 11.3, 10.9, 10.6, 10.2, 12.3, 11.9, 11.1, 10.8, 10.8, 12.1, 11.6, 11.3, 11.8, 11.4, 9.8, 10.2, 12.1, 10.9, 11.4, 12.2, 11.8, 12.0, 11.3, 11.6, 10.4, 10.9, 10.4, 10.2, 11.4, 11.4, 10.6, 11.2, 11.2, 12.1, 12.2, 11.5, 10.7, 10.4, 9.8, 10.6, 11.7, 10.6, 11.0, 10.7, 11.0, 11.2, 10.2, 11.1, 12.1, 10.4, 9.9, 9.5)
    X4&lt;-c(2.4,2.2,3.0,2.5,2.7,2.7,2.5,3.1,4.0,2.7,3.1,2.5,2.4,3.8,2.7,2.8,4.1,1.8,2.2,3.6,5.3,2.1,3.3,3.5,0.9,5.6,7.8,5.7,4.9,30.9,3.8,3.1,16.9,4.8,4.0,4.2,4.3,4.8,14.2,5.2,3.7,3.4,1.7,4.9,9.8,4.6,4.2,0.0,4.6,5.9,0.6,5.1,4.5,4.6,1.9,5.4,4.8,4.0,4.4,6.8,4.6,4.1,3.7,3.0,3.0,3.2,1.9,3.9,5.3,3.0,3.2,0.2,3.1,3.2,2.1,3.3,3.8,2.9,1.8,3.2,3.3,2.5,1.9,5.0,2.7,2.5,-1.7,2.6,2.9,1.2,2.2,5.9,0.8)

    ## Time series Creation
    Yts&lt;-ts(Y, start=c(1998,1), end=c(2005,9), frequency = 12)
    X1ts&lt;-ts(X1,start = c(1998,1),end = c(2005,9), frequency = 12)
    X2ts&lt;-ts(X2,start = c(1998,1),end = c(2005,9), frequency = 12)
    X3ts&lt;-ts(X3,start = c(1998,1),end = c(2005,9), frequency = 12)
    X4ts&lt;-ts(X4,start = c(1998,1),end = c(2005,9), frequency = 12)
</code></pre>

<p>And this is a plot of my time series :
<a href=""http://i.stack.imgur.com/iQ8TZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iQ8TZ.png"" alt=""enter image description here""></a></p>

<p>Tell me if something is unclear, and sorry for my english.</p>

<p>Any help would be much appreciated!</p>

<p><strong>edit</strong> : I reduced a bit my message to make it more concise :) </p>
"
"0.0627455805138159","0.0846667513334603","76466","<p>I've been working through the HW work in the online book <a href=""http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html"" rel=""nofollow""><em>A little book of R for time series analysis</em></a>, and have started testing with some ""live"" customer data. I have a dataset that looks like:</p>

<pre><code>CustomerName | Sales
123456         $5,000
123456         $3,455
123456         $7,540
123456         $2,300
987654         $5,600
987654         $6,700
987654         $1,300
987654         $690
</code></pre>

<p>Where I have <code>Sales</code> values by customer for the previous 60 months. There are ~200 customers for which I'm looking to generate a forecast. I'm able to generate a forecast for a single customer at a time, but I am having trouble finding guidance on how to run the forecast for the whole group of customers and output the results. </p>

<p>Ideally, the output would be the regular forecast output but with <code>CustomerID</code> included like so:</p>

<pre><code>CustomerID | Month | Point.Forecast | Lo.80 |Hi.80 | Lo.95 | Hi.95
</code></pre>
"
"0.117386232408469","0.158396987770497","140505","<p>I am using Basic Structs to forecast my time series. My forecast is exactly overlapping my data. I am sure no model can predict with 100% accuracy. I know I am missing something, can someone point me what the mistake is? My data and code are given below.</p>

<p>As you can see, when I run the below code, I get the following image, which shows that my model is too good. </p>

<p><img src=""http://i.stack.imgur.com/C5kdC.png"" alt=""enter image description here""></p>

<pre><code>Year  &lt;- c(2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009, 
           2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010, 
           2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 
           2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 
           2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 
           2012, 2012, 2013, 2013)
Month &lt;- c(4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 
           12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 
           8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2) 

base.oil&lt;-c(995,1090,1290,1430,1440,1420,1175,840,730,625,570,510,500,575,680,745,765,780,780,780,795,790,790,865,920,960,960,960,960,975,970,988,1035,1095,1165,1270,1300,1310,1370,1370,1353,1245,1172.5,1070,1060,1070,1085,1140,1197.5,1215,1180,1025,1025,1022.5,1000,960,942.5,855,857.5)


gl&lt;- data.frame(Year = Year, Month = Month, base.oil=base.oil)


#convert the dataset to a Time Series(TS) object
gl.ts&lt;-ts(gl,start=c(2008,4),end=c(2013,2),frequency=12)

#Removing outliers in 2008 data, Selecting data only from 2009 for analysis
gl.train&lt;-window(gl.ts,start=c(2009,1),end=c(2012,3))


#Creating a test set
gl.test&lt;-window(gl.ts,start=c(2012,4))


#Creating a list of models--using only StructTS
models &lt;- list(
  mod_sts= StructTS(gl.train[,3])
)
#Forecast on Test Dataset
forecasts &lt;- lapply(models, forecast, 11)


par(mfrow=c(1, 1))
for(f in forecasts){
  plot(f)
  lines(gl.test[,3], col='red')
}

#Overlapping StructTS over the training data
plot(gl.train[,3],main=""Overlapping StructTS over the training data"",ylab=""Oil Prices"")
lines(fitted(forecasts$mod_sts),col=""red"")
</code></pre>

<p><strong>EDIT:</strong>
On the test set, it performs better compared to other models I tried
Error Stats and plots are given below
<img src=""http://i.stack.imgur.com/ZCqeL.jpg"" alt=""enter image description here""></p>
"
"0.0887356509416114","0.11973686801785","200889","<p>I have been requested to predict National league(NL) OPS (to start with) for the coming season and compare to a given team predictions to be able to evaluate a chanse of winning a certain number of games.
I haven't worked with time series before. I have read many advices here and on hyndersight, as well as read the little book of r for time series and much more.</p>

<p>However, I still struggle.
I have stats for the NL from 2008 to 2015 at a game level from baseball-reference.com.
I have cleaned the data, evaluated for correlations, etc.
Now I have data for all 15 teams that play on average 162 games per season. i haven't excluded the games that were cut due to rain, so double records are there. I wasn't sure what is the better way to do it: exclude at all or average on the score.</p>

<p>I struggle with time series conversion.
The original data looks like this (if aggregated by week otherwise it is too noisy)
<a href=""http://i.stack.imgur.com/shuzP.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/shuzP.jpg"" alt=""weekly avg OPS""></a> </p>

<p>I have transformed the data into time series, however have a doubt what should submit to the frequensy parameter, since we have 15 teams playing 162 games. Is it simply 162? I tryed to use frequency as 162. But the trend doesn't match the average OPS . I would expect them to be similar and start going back up at the end of 2015.
<a href=""http://i.stack.imgur.com/1g5Fc.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1g5Fc.jpg"" alt=""OPS time series decomposition with stl()""></a></p>

<p>Another question that I had a doubt about is training/testing data. I was thinking to split it by year, leavng 2015 (or half of it) as a test set. I'm wondering if using cross-validation would be a better option?</p>

<p>I hope the questions do not seem too naive. I really haven't seen a time series example with multiple sourses for the same time frame and do not know how to go about it.</p>

<p>Thanks,
Katya</p>
"
"0.0793675790132545","0.107095910520333","221818","<p>I currently have half-hourly electricity demand, half-hourly electricity price and hourly temperature from 2012 to 2016, and I would like to do both short-term and long-term forecast of electricity demand. </p>

<p>As the data is multi-seasonal, I aim to use <code>tbats</code> model from <code>forecast</code> package in R, and I know that there is a function which can forecast the demand directly, but how can I also take price and temperature into account in terms of forecast, any recommend approach?</p>

<p>I am also a bit confused with the result shown from <code>tbats</code> model in R. Should I expect  <code>reside(tbats)</code> to be white noise? How to tell whether the <code>tbats</code> model that fitted to the data is good or not? </p>
"
"0.0992094737665681","0.133869888150416","143358","<p>I'm working on a forecast for the following data:</p>

<pre><code>data &lt;-
c(1932, 4807, 6907, 8650, 10259, 11374, 8809, 6745, 7429, 
8041, 9740, 10971, 11953, 9227, 7401, 8355, 9681, 10438, 
11092, 11543, 9181, 7428, 8358, 10049, 10938, 12280, 
13063, 10022, 8125, 8763, 9330, 9919, 11309, 12169, 11063, 
10112, 10621, 11506, 12425, 12929, 13025, 10938, 9437, 
9910, 11104, 11985, 13024, 13962, 11900, 9576, 9590, 
10740, 11689, 13084, 13829, 11975, 10224, 10493, 11899, 
12697, 13959, 14415, 11650, 9477, 11166, 12327, 13238, 
13801, 13493, 11118, 9073, 9954, 11077, 12509, 12985, 
13380, 11454, 9265, 10053, 11443, 12132, 13733, 13850, 
11560, 9401, 9921, 11401, 12622, 14224, 14289, 12097, 
9623, 10630, 11572, 12816, 14180, 14125, 11667, 9328, 
9936, 11159, 12536, 13953, 13840, 11430, 9313, 9926, 
11557, 12428, 13802, 13041, 9927, 7448, 9143, 10872, 
12331, 14370, 14496, 13237, 11176, 11936, 12661, 14442, 
15005, 15359, 12871, 10505, 11231, 12078, 13307, 14027, 
14368, 12057, 9965, 10121, 11414, 13375, 14525, 14686, 
12243, 9833, 10722, 11778, 13143, 14844, 14856, 12745, 
9134, 7856, 9429, 11539, 13241, 14324, 12102, 10136, 
11107, 12028, 13999, 15130, 15488, 13379, 11028, 11708, 
13280, 14665, 15362, 15600, 12950, 10716, 10988, 12350, 
14163, 15264, 15724, 13374, 11764, 12711, 13239, 14849, 
15455, 15914, 13541, 10570, 9376, 10132, 11725, 12328, 
13105, 11022, 9710, 10659, 12068, 12890, 14242, 14294, 
11847, 9776, 10681, 12413, 13571, 14344, 14500, 12234, 
9961, 10699, 11626, 13135, 14387, 15282, 13028, 11211, 
11992, 13524, 15131, 15741, 15357, 12489, 9985, 10786, 
11492, 13851, 14509, 14751, 12327, 10023, 11315, 12363, 
13487, 14944, 15006, 12290, 9867, 11540, 12179, 14094, 
14941, 15006, 13585, 10769, 11408, 12634, 14073, 15361, 
15236, 13151, 9580, 8934, 10128, 12475, 13890, 14740, 
12617, 10358, 11648, 12418, 14094, 15127, 15775, 13647, 
11281, 11773, 13407, 15441, 15601, 15951, 13865, 11447, 
12422, 13725, 15766, 16389, 16868, 15221, 12503, 12780, 
14525, 16479, 17032, 17403, 14553, 12484, 13204, 13792, 
14896, 15673, 16332, 14196, 11749, 12977, 13886, 14931, 
15955, 16037, 14082, 11271, 12512, 13942, 16362, 17456, 
17446, 15509, 13069, 13524, 14918, 16161, 17524, 18138, 
14604, 12993, 13763, 14945, 16686, 17717, 17947, 15744, 
13388, 13177, 14588, 16075, 16705, 17074, 14415, 12766, 
13372, 14033, 14300, 12508, 11502, 9391, 7689, 9613, 
12291, 14448, 15075, 15670, 13929, 10989, 11875, 13409, 
15203, 15654, 16150, 13387, 10931, 11492, 12479, 13674, 
14519, 14241, 11685, 9486, 9990, 11440, 12415, 13505, 
12103, 10311, 8267, 7510, 8595, 10620, 11664, 3182, 6241, 
9365, 10965, 12372, 9958, 8088, 9290, 10665, 12132, 12827, 
13040, 10692, 8882, 9538, 10027, 12086, 13276, 13107, 
10680, 9136, 10744, 11733, 13334, 14654, 14830, 12189, 
9613, 11399, 12837, 13661, 15007, 15579, 12268, 9703, 
10627, 12077, 13287, 14459, 14825, 11958, 10049, 11512, 
12770, 13869, 14873, 15233, 12056, 9654, 10386, 11465, 
13354, 14601, 15161, 12324, 9782, 10791, 12502, 14111, 
14914, 15250, 12366, 10333, 11638, 12449, 13518, 14637, 
14756, 12011, 9878, 10976, 12464, 13674, 14979, 15312, 
12106, 10127, 11666, 12843, 13910, 15024, 15333, 12308, 
9992, 11278, 13364, 14966, 15231, 15507, 13744, 11417, 
12232, 14414, 15245, 15988, 15168, 11905, 9165, 10536, 
12570, 14106, 15204, 15509, 12821, 10321, 11282, 13133, 
14174, 15099, 14750, 12817, 10384, 11368, 12994, 14591, 
16154, 15904, 12784, 10737, 11865, 13809, 14721, 15202, 
15322, 12722, 10741, 11991, 13546, 14716, 15817, 15879, 
12679, 10390, 11524, 13140, 14426, 15613, 16212, 13088, 
10720, 11730, 13776, 14477, 15758, 15922, 13119, 9220, 
8372, 10239, 12397, 14740, 15550, 13306, 10833, 11892, 
13630, 15186, 16154, 16678, 12898, 10485, 11313, 13705, 
15572, 16086, 16305, 14129, 11066, 12251, 13830, 15345, 
16550, 16518, 13700, 10890, 12301, 14163, 15890, 16985, 
17544, 15337, 12633, 13383, 12813, 12051, 13149, 13636, 
10914, 9617, 10619, 12224, 13954, 15325, 15473, 12418, 
9730, 11214, 12572, 14565, 15287, 15721, 12519, 10689, 
11662, 13139, 14902, 16374, 16392, 13895, 11777, 12948, 
14326, 15625, 16745, 16980, 13946, 11181, 12665, 13678, 
15269, 16279, 16634, 14399, 11142, 11900, 13800, 14783, 
16626, 16861, 13917, 11228, 12531, 14206, 15773, 16344, 
16930, 13945, 11110, 12427, 14085, 15627, 16854, 17106, 
14677, 10410, 8550, 10626, 13366, 15337, 16460, 13619, 
11630, 12582, 13926, 15297, 16715, 17036, 14063, 11368, 
12246, 14111, 15525, 16900, 17272, 14254, 11961, 13155, 
14579, 16260, 17187, 17919, 15493, 13162, 13771, 15231, 
15836, 16880, 16976, 14728, 12106, 13030, 13848, 15344, 
16475, 17122, 13601, 10921, 12043, 14114, 15846, 16190, 
17125, 13769, 10768, 12336, 13849, 16138, 17507, 18050, 
15492, 12905, 12847, 14181, 15967, 16704, 17762, 14882, 
12591, 13807, 14959, 16933, 17369, 17453, 14351, 11582, 
13102, 14328, 16185, 16321, 16843, 13773, 11053, 12199, 
14147, 14470, 12598, 11916, 9185, 7903, 9742, 12691, 
15153, 15945, 16254, 13630, 11437, 12235, 14040, 15161, 
15995, 16291, 12944, 10947, 12055, 13444, 14852, 16029, 
16361, 13658, 10885, 11604, 13030, 13959, 14291, 14786, 
12002, 9014, 7610, 7426, 9602, 11077, 12544, 11334, 5710, 
9874, 11949, 10321, 8945, 10152, 11821, 13434, 15187, 
15269, 12661, 10699, 12040, 13154, 14149, 15472, 16569, 
13008, 10521, 11674, 13272, 14025, 15803, 16791, 13615, 
11043, 12448, 13929, 15158, 16610, 17520, 13900, 11095, 
11735, 13652, 14939, 16001, 16265, 13371, 11198, 11583, 
13377, 15361, 16420, 16765, 13800, 10866, 12026, 13908, 
14902, 16044, 16807, 13694, 11475, 13009, 14453, 16231, 
17093, 17411, 14433, 12242, 13035, 14304, 16309, 17026, 
16811, 13986, 11812, 13216, 14397, 16026, 17780, 17463, 
14717, 12029, 13046, 14820, 16626, 17564, 17802, 14134, 
13158, 15356, 16573, 16887, 17494, 17326, 13525, 11517, 
12410, 13817, 14933, 16399, 17019, 14008, 11808, 12599, 
14639, 16339, 17521, 17820, 14444, 11530, 13352, 14997, 
16038, 17631, 17614, 15601, 15176, 16930, 17979, 18772, 
19728, 19452, 16272, 14006, 15510, 17299, 17774, 18345, 
19080, 16486, 14242, 15465, 16973, 17971, 19068, 19075, 
15606, 13315, 14784, 16505, 17910, 18586, 18315, 15659, 
13621, 14673, 16037, 17467, 17972, 17676, 15452, 11850, 
10959, 13641, 15217, 16813, 17641, 15404, 13102, 14391, 
15764, 17326, 17715, 17947, 15272, 13078, 13962, 15372, 
18292, 18569, 16427, 13374, 14725, 15957, 17425, 18530, 
19251, 17094, 13711, 15275, 16663, 18254, 19023, 19787, 
16636, 14398, 15392, 16302, 15844, 14301, 14559, 11739, 
10080, 11690, 14352, 16702, 17810, 17898, 15159, 12527, 
14250, 15788, 17012, 18219, 17743, 15183, 12633, 14033, 
15528, 16984, 18041, 18388, 15248, 12831, 14289, 16143, 
17340, 18863, 18597, 15984, 13697, 14653, 16143, 17262, 
17805, 18565, 16147, 14734, 16548, 17410, 18044, 18705, 
18462, 15706, 13242, 14977, 16168, 17683, 18224, 18454, 
15784, 14003, 16605, 18013, 19361, 19204, 18970, 16655, 
12928, 11502, 13233, 15211, 16883, 17454, 15043, 12953, 
14515, 15846, 17501, 18922, 18903, 16175, 13492, 14150, 
15710, 18297, 18872, 19490, 15921, 13935, 14943, 16457, 
18425, 19975, 20440, 17716, 15059, 16086, 17290, 18477, 
19896, 20115, 17580, 15001, 15640, 17915, 18951, 20029, 
20221, 16653, 15063, 15726, 16849, 18121, 18843, 19112, 
16516, 13960, 15255, 16910, 18895, 20091, 20663, 17698, 
15441, 16775, 18158, 19897, 20424, 20111, 17784, 15044, 
16869, 17773, 19783, 21255, 20632, 18081, 15891, 17180, 
18143, 20197, 20926, 20639, 18407, 16313, 16998, 17860, 
19177, 19618, 19919, 17662, 16033, 17439, 18741, 18108, 
16641, 16319, 13221, 11160, 12783, 14876, 16831, 18379, 
18858, 16191, 14632, 16089, 16828, 18169, 19512, 18828, 
17364, 15516, 17065, 18245, 18684, 19472, 19235, 16885, 
14854, 14526, 12921, 12675, 14884, 15284, 13492, 11457, 
5938, 9694, 9429, 9142, 10648, 13235, 15610, 16868, 17364, 
16043, 14497, 15329, 16839, 17548, 18818, 19320, 15884, 
13834, 14748, 15784, 16729, 18274, 19138, 17413, 15394, 
16596, 17853, 18934, 20310, 20165, 18870, 16562, 16823, 
18051, 18816, 20410, 21211, 18551, 16274, 17289, 18317, 
20259, 19993, 19831, 18166, 16517, 17114, 17763, 19011, 
20541, 19974, 18105, 16130, 17422, 18472, 20213, 20721, 
20803, 19250, 16246, 16582, 18410, 19559, 20821, 20412, 
18576, 16272, 16917, 19027, 19917, 20418, 21188, 18382, 
16842, 17911, 19126, 20471, 21120, 20756, 18190, 15873, 
16924, 18468, 19579, 20877, 20726, 18525, 16110, 17480, 
19313, 20323, 20661, 20541, 18284, 16124, 17312, 18361, 
19170, 19945, 20548, 17605, 15973, 17488, 17444, 19086, 
19775, 19827, 17269, 14616, 15690, 16469, 18626, 19288, 
20111, 17769, 15738, 17060, 18885, 20010, 21371, 21541, 
18682, 15971, 16714, 18659, 19934, 21499, 22118, 18952, 
16025, 18120, 18897, 20630, 20286, 21077, 17710, 14857, 
16050, 17877, 19928, 21299, 21202, 18858, 14339, 13172, 
15521, 17434, 19823, 20679, 18288, 16798, 18673, 20628, 
21462, 22720, 22241, 20064, 17327, 18720, 19896, 19710, 
21185, 21916, 19661, 17134, 18027, 19449, 20912, 21234, 
21950, 19495, 17023, 18473, 19080, 20875, 21031, 21492, 
20091, 17511, 18834, 19126, 19922, 21215, 19017, 15506, 
12854, 14605, 16279, 18129, 20043, 21248, 18518, 15467, 
16586, 18277, 18915, 20597, 21244, 19024, 16294, 17234, 
18786, 20960, 21345, 22068, 19774, 17491, 18279, 19809, 
20757, 21618, 22131, 20214, 17581, 18321, 19590, 21486, 
22492, 23194, 20020, 16819, 17892, 18948, 20921, 21696, 
22549, 19559, 16404, 17301, 18659, 20430, 22300, 22569, 
19630, 16800, 17898, 19584, 21190, 21926, 22359, 20157, 
15823, 14136, 15930, 18341, 21044, 21204, 18994, 16973, 
18171, 19378, 20794, 22442, 22144, 19874, 17859, 18703, 
19082, 20781, 21860, 21536, 20172, 18429, 19221, 19824, 
21326, 22504, 23381, 21733, 19231, 20312, 21994, 22609, 
23317, 23074, 22005, 19209, 20734, 22513, 23017, 23698, 
24385, 22512, 19471, 20061, 21235, 22351, 22532, 22869, 
20409, 17908, 18722, 19894, 20960, 21999, 22125, 20797, 
19091, 19910, 20463, 22106, 22737, 22827, 21695, 19498, 
20180, 21204, 22272, 22803, 22808, 20979, 18952, 20365, 
20875, 22944, 23022, 22786, 21284, 19302, 20394, 21144, 
22633, 23511, 23355, 21979, 19988, 20143, 21966, 22574, 
19974, 19410, 15641, 13265, 14880, 16838, 19262, 19941, 
20479, 18929, 17760, 18078, 19055, 20553, 21732, 21671, 
19218, 18485, 18864, 20278, 21120, 21747, 21087, 17982, 
15115, 16518, 16282, 15032, 15658, 14966, 12172, 10336, 
12669, 14238, 14031, 12441, 13313, 11047, 10158, 12438, 
14255, 16434, 17873, 18481, 16360, 14479, 15595, 17392, 
18878, 19999, 19958, 16748, 13852, 14931, 16410, 18097, 
19654, 19480, 16387, 14515, 15205, 16854, 18544, 19510, 
20382, 17838, 14878, 15041, 16661, 19008, 20265, 20947, 
18048, 16472, 16434, 18250, 19571, 21148, 20117, 17788, 
14321, 14996, 15779, 17789, 18804, 18934, 17488, 15095, 
15859, 16691, 18369, 20012, 21073, 18029, 15582, 17247, 
18608, 19783, 20322, 20908, 18221, 15919, 17107, 18404, 
19262, 21741, 21514, 19798, 17410, 17973, 18469, 17910, 
14901)
</code></pre>

<p>The <code>ts.plot(data)</code> gives:<img src=""http://i.stack.imgur.com/E6WU0.jpg"" alt=""enter image description here""></p>

<p>With this data, I'm looking to forecast the values for the next year. This data is victim to both weekly and yearly seasonality. Due to this, I first attempted to use <code>tbats</code> from the <code>forecast</code> package but received an improper forecast that mirrors that found at <a href=""http://www.github.com/robjhyndman/forecast/issues/87"" rel=""nofollow"">http://www.github.com/robjhyndman/forecast/issues/87</a></p>

<p>Instead, I used the following code:</p>

<pre><code>n&lt;-length(data)
bestfit &lt;- list(aicc=Inf)
bestk &lt;- 0
for(i in 1:20)
{
fit &lt;- auto.arima(data, xreg = fourier(1:n,i,m1) + fourier(1:n,i,m2), max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
if(fit$aicc &lt; bestfit$aicc)
{
    bestfit &lt;- fit
    bestk &lt;- i
}
}

k &lt;- bestk

bestfit &lt;- auto.arima(data, xreg = fourier(1:n,k,m1) + fourier(1:n,k,m2), max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
accuracy(bestfit)
fc &lt;- forecast(bestfit, xreg = fourier((n+1):(n+365),k,m1) + fourier((n+1):(n+365),k,m2), level = c(50,80,90), bootstrap = TRUE)
plot(fc)
</code></pre>

<p>This code is searching for the best ARIMA model through the use of Fourier terms in <code>xreg</code> to capture both seasonality components. This Fourier function is defined (per <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) as:</p>

<pre><code>fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}
</code></pre>

<p>This forecasting gives me the following plot:<img src=""http://i.stack.imgur.com/2IsSD.jpg"" alt=""enter image description here""></p>

<p>In looking at this forecast, it seems by my naked eye to be off. Just by observation it appears that my forecast is not properly catching the small, but visible, increasing trend. Instead of being ""centered"" around the extended trendline, it appears that the forecast is ""centered"" around the mean of the entire dataset.</p>

<p>First off, am I doing something that is just blatantly wrong? (my mind is a little fuzzy this morning)</p>

<p>If my forecast is correct, how is it that it falls so much below the extended trendline?</p>

<p>Lastly, are there any other suggestions which might be beneficial to my forecasting?</p>
"
"0.117386232408469","0.113140705550355","32866","<h2>Situation description</h2>

<p>I'm trying to implement a prediction (or trending) algorithm for my performance gathering system in order to see when a Linux server's resources will end (for example, free space on a storage or free memory).</p>

<p>The result of the performance gathering process is a graph. So, I need to get on my graph something like this:</p>

<p><a href=""http://imgur.com/GJCyh"" rel=""nofollow""><img src=""http://i.imgur.com/GJCyh.png"" alt="""" title=""Hosted by imgur.com"" /></a></p>

<p>This is an example of RRDtool graph (data collected from Cacti monitoring tool). 
Here are 3 dashed lines that are trend lines for the disk utilization history graph.
That is similar to what I need.</p>

<h2>What I would like to get</h2>

<p>The prediction of when the performance will reach some value (e.g. 90%).
Which one of the plenty of prediction (trending) algorithms should I use in my case?</p>

<h2>What I've researched</h2>

<p>Holt-Winters algorithm, time series prediction. But I do not know how to use them in this particular case. May be there is other solution?</p>

<h2>Additional information</h2>

<p>[30,45,50,10,20,30]-> this is example of disk utilization array (in %).</p>

<p>--------------------> Time, minutes </p>

<p>Critical threshold boundary is 90%.</p>

<p>I need to know when the disk utilization will reach 90% threshold (based on the already collected history of data).</p>

<p>Any examples in Matlab or in R are welcome.</p>
"
"0.108678533400333","0.146647115021353","92743","<p>I have an example of call center data for 2013. There are 261 days of data (excluding weekends).<br>
For 2013, I have included a holiday dummy variable (<code>holiday</code>) for the days where there were no stats.<br>
For 2014, I have also included a future holiday dummy variable (<code>holidayf</code>).<br>
<strong>My objective is to assess how accurate this code is in making predictions for 2014.</strong></p>

<p>I tried this code below but when looking at <code>fc$fitted</code>, the forecasts don't seem to be correct. For the first 8 days of January 2014, it forecasts the exact number of calls that were received in the first 8 days of January 2013, which seems wrong. Also, where there is a public holiday in 2014, the future forecast for that day predicts a normal to high volume of calls, so it seems that the forecast is using the <code>holiday</code> variable and not the <code>holidayf</code> variable.</p>

<pre><code>library(forecast)
y &lt;- ts(calls,frequency=5)
z &lt;- fourier(ts(calls,frequency=261),K=12)
zf &lt;- fourier(ts(calls,frequency=261),K=12,h=261)
fit &lt;- auto.arima(y,xreg=cbind(z,holiday))
fc &lt;- forecast(fit,xreg=cbind(zf,holidayf),h=261)
plot(fc)
</code></pre>

<p>Data:</p>

<pre><code>calls &lt;- 
  c(0,145,175,129,266,219,156,184,167,241,218,194,192,162,236,219,212,191,162,216,
  235, 218,180,150,245,209,210,211,151,236,197,217,140,164,200,156,152,153,141,224,178,
  159,153,137,207,173,197,213,206,305,284,248,289,269,359,333,257,0,244,325,292,267,
  206,0,0,360,261,327,284,385,377,317,327,271,372,191,320,268,261,376,320,280,251,200,
  200,200,0,236,161,259,200,190,166,174,225,228,202,201,155,241,207,199,179,178,249,
  243,230,177,181,264,250,219,204,178,244,249,185,184,164,0,253,216,217,165,170,185,
  175,160,148,231,223,196,162,149,228,213,190,177,139,212,205,221,190,170,196,210,
  198,192,131,220,185,199,153,166,240,176,200,145,0,255,202,220,220,181,250,171,164,
  142,118,179,197,167,130,124,180,214,203,153,140,161,200,191,159,141,227,170,166,
  166,106,131,0,176,156,109,196,175,175,174,161,230,191,159,150,91,180,188,173,157,
  107,193,172,172,172,116,195,183,169,146,125,208,160,160,177,128,191,176,149,175,
  136,217,162,178,130,99,158,154,135,146,106,155,148,119,137,96,161,106,114,139,84,
  0,97,95,82,65,59,23,0,0,48,83,48)



holiday &lt;- c(1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,
             0,0,0,1,1,1,0,0,0)

 holidayf &lt;- c(1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,
             0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,
             0,0,0,0,1,1,0,0,0)
</code></pre>
"
"0.0887356509416114","0.11973686801785","16915","<p>Note that I do most of my analysis using R and Excel.</p>

<p>Let's take this data set for example. I modified it as the data itself is proprietary: the years are also different:</p>

<pre><code>1967    2,033,407
1968    2,162,275
1969    2,159,640
1970    2,312,352
1971    2,554,449
1972    2,548,425
1973    2,101,225
1974    1,951,944
1975    2,106,250
1976    1,687,625
1977    1,636,496
1978    1,494,525
1979    1,606,825
1980    1,460,937
1981    1,310,494
1982    1,319,750
1983    1,263,643
1984    1,171,656
1985    1,194,950
</code></pre>

<p>What I usually do:</p>

<ol>
<li>A linear regression</li>
<li>Some form of polynomial trending</li>
<li>Moving average and double moving average</li>
<li>Basic ARIMA using p = 1, q = 0.</li>
<li>I calculate the errors for all these as well</li>
<li>I average all the forecasts out and the error to have my final result.</li>
</ol>

<p>Note that I'm an engineer that wants to get into statistics and the ability to properly validate and calibrate my models.</p>

<h2>Question</h2>

<p>What is the correct way to forecast this to 5, 10, or even 15 future years?</p>

<p>In a way I'm looking to move beyond the plugging data into a model and believe the data. Yes, I'm aware I can look at the errors. I mainly use RMSE or MAE. But I still am not confident when it comes to just predicting data the right way.</p>

<h3>Note</h3>

<p>this is also related to <a href=""http://stats.stackexchange.com/questions/16545/how-can-i-be-confident-about-my-forecasts-and-improve-my-methodologies"">this question</a> I posted here before.</p>
"
"0.133103476412417","0.159649157357133","202302","<p>I have time-series data (<code>xts</code> form) of power consumption at a 10 minutes rate and I do have temperature and humidity values as well. So my data looks like this:</p>

<pre><code>                      power temperature humidity 
2015-08-01 00:00:00      NA    28.00000 79.00000   
2015-08-01 00:10:00 122.941    27.66667 80.66667   
2015-08-01 00:20:00  67.596    27.33333 82.33333   
2015-08-01 00:30:00 184.180    27.00000 84.00000   
2015-08-01 00:40:00 186.879    27.00000 84.00000   
         :               :         :        :
</code></pre>

<p>Using this historical data, I want to predict/forecast the power consumption in real time. From this data, I am planning to use following features for my forecasting model:</p>

<ol>
<li>Power usage of last three days at the same time instant for which we want to predict. For example, If I want to predict for 15 March, 1300 hours then I will use the power consumption at 1300 hours on 12, 13, and 14 March.</li>
<li>Power usage for the last three hours. Continuing with above example, here I would like to use the power consumption at 1000, 1100, and 1200 hours</li>
<li>Temperatue and humidity values at the forecasting time. I assume I can get these values from the weather forecasting services.</li>
</ol>

<p>For the forecasting model, I need to model the data in a proper format suitable for model evaluation. I mean how should I arrange this <code>xts</code> object in a format (data.frame) so that I can build my generalized forecasting model for any time instant of the day. The question is toally about arranging the historical <code>xts</code> data in format suitable for usage.</p>

<p>Any pointer/reference for arranging this data suitable for processing will be great help for me!</p>
"
"0.140303383316578","0.170388550274119","202319","<p>I have daily sales data for a department store for the past 850 days. I have indicators on the major holidays and the days leading up to the major holidays. The number of days before the holidays that are included was chosen by AIC. The issue I'm having is that there are outliers throughout the data that I'm not sure how to handle. Or, at least that's what I think is happening since I don't seem to get accurate forecasts. I'm using a CV to calculate the MAPE of forecasts two weeks out, using the first 450 days as the initial training set and the rest to see how well the model forecasts the data.</p>

<p>I've used tso() from the tsoutliers package and tsoutliers from the forecast package to find outliers. They both give different results.</p>

<pre><code>tsoutliers(data$Sales)

$index
[1] 230 270 271 328 635

$replacements
[1] 2222.160 2088.573 2231.577 1812.380 2138.655

train = 454
trainingdata = data$Sales[1:train]
trainingdata = ts(trainingdata,frequency = 7)
tso(trainingdata,types = c(""AO"", ""LS"", ""TC""))

Series: trainingdata 
ARIMA(2,1,1)(2,0,0)[7]                    

Coefficients:
     ar1     ar2      ma1    sar1    sar2      AO52      TC68       TC80      AO86
  0.2872  0.1331  -0.9717  0.3567  0.4607  885.2061  890.3690  -863.4296  836.8638
s.e.  0.0508  0.0480   0.0107  0.0436  0.0429  169.2521  163.4243   166.0282  169.8535
     AO111     AO121      TC229     AO259      TC270     AO328     AO416
  754.1791  691.0849  1236.8523  711.3954  1790.0292  764.9712  920.1783
s.e.  169.2042  167.7273   163.1458  167.9835   163.9663  170.0103  168.9235

sigma^2 estimated as 44080:  log likelihood=-3064.92
AIC=6152.24   AICc=6153.65   BIC=6222.21

Outliers:
type ind  time coefhat  tstat
1    AO  52  8:03   885.2  5.230
2    TC  68 10:05   890.4  5.448
3    TC  80 12:03  -863.4 -5.200
4    AO  86 13:02   836.9  4.927
5    AO 111 16:06   754.2  4.457
6    AO 121 18:02   691.1  4.120
7    TC 229 33:05  1236.9  7.581
8    AO 259 37:07   711.4  4.235
9    TC 270 39:04  1790.0 10.917
10   AO 328 47:06   765.0  4.500
11   AO 416 60:03   920.2  5.447
</code></pre>

<p>Running BoxCox on the data it recommends a transform of the data</p>

<pre><code>lambda &lt;- BoxCox.lambda(data$Sales)
trainingdata = BoxCox(trainingdata,lambda)
tso(trainingdata,types = c(""AO"", ""LS"", ""TC""))
Series: trainingdata 
ARIMA(3,1,1)(2,0,0)[7]                    

Coefficients:
     ar1     ar2      ar3      ma1    sar1    sar2      LS3    AO52     AO53    TC68
  0.3918  0.0993  -0.0587  -0.9856  0.3632  0.4144  13.5805  5.7218  -7.7957  6.3960
s.e.  0.0383  0.0418   0.0416   0.0142  0.0361  0.0341   1.3201  1.2980   1.3041  1.2763
      AO80   AO121   TC229   TC270   AO416     AO445   TC634   AO780
  -23.3707  5.5352  5.8088  7.0446  7.9304  -23.6372  5.5475  6.7194
s.e.    1.2376  1.2307  1.2594  1.2640  1.2476    1.2393  1.2598  1.2353

sigma^2 estimated as 2.332:  log likelihood=-1482.63
AIC=3003.26   AICc=3004.23   BIC=3092.34

Outliers:
type ind   time coefhat   tstat
1    LS   3   1:03  13.581  10.287
2    AO  52   8:03   5.722   4.408
3    AO  53   8:04  -7.796  -5.978
4    TC  68  10:05   6.396   5.012
5    AO  80  12:03 -23.371 -18.883
6    AO 121  18:02   5.535   4.498
7    TC 229  33:05   5.809   4.612
8    TC 270  39:04   7.045   5.573
9    AO 416  60:03   7.930   6.356
10   AO 445  64:04 -23.637 -19.073
11   TC 634  91:04   5.547   4.404
12   AO 780 112:03   6.719   5.439
</code></pre>

<p>Some of these outliers are already taken care of since they're the holidays. I'm not sure how to handle the rest of the outliers when fitting the model and in the CV.</p>

<p>What is the best way to go about taking care of the outliers? I can reset the values of the training data where it's predicted as an outliers to the recommended value if it's not a holiday for fitting the model and then still calculate the MAPE off of the original data. However, there's a LS at index 3 so I'm not sure that would make sense for that.</p>
"
"0.154151401771656","0.17600563227035","220837","<p>I want to forecast demand of various products using time series data of 2 years (using loops on products in R), frequency is daily and demand is to be forecasted for next 90 days</p>

<p>I have used the following models till now</p>

<ol>
<li>ARIMA </li>
<li>SImple Exponential Model</li>
<li>Holt Exponential Model (with trend)</li>
<li>Holt Winters Exponential Model (with trend and seasonality)</li>
</ol>

<p>My ARIMA results gave same point forecasts for many products for all 90 days, while the Exponential models are giving high errors (seen through MAPE)</p>

<blockquote>
  <ol>
  <li>Why does ARIMA model give same point forecast for all 90 days.</li>
  <li>What other models can be used for time series forecasting other than these four</li>
  <li>Should ARCH-GARCH models or  sarima be used??</li>
  </ol>
</blockquote>

<pre><code>   #  Demand Forecasting - ARIMA model- Weekly Demand Forecasting for next 13 weeks using 104 weeks data- 27-6-2016

#Required libraries
library(forecast)
require(forecast)
library(MASS)
require(MASS)

#Setting up working directory
getwd()
setwd(""C:/Users/21202/Desktop/ARIMA_27-6"") 
getwd()

#Reading the data
weeklyproductdemand&lt;-as.matrix(read.csv(""117weeks.csv""))

#Checking for the no. of products in the data 
nproducts&lt;-ncol(weeklyproductdemand)-1 # -1 because first column is product code
nproducts

product &lt;- matrix(c(rep(1, nproducts*117)), nrow = 117, ncol = nproducts)
forecastoutput &lt;- matrix(c(rep(1, nproducts*13)), nrow = 13, ncol = nproducts)
actualdata &lt;- matrix(c(rep(1, nproducts*13)), nrow = 13, ncol = nproducts)
arima_error_metrics &lt;- matrix(c(rep(1,nproducts*13)), nrow = 13, ncol = nproducts)
arima_pcerror_metrics &lt;- matrix(c(rep(1,nproducts*13)), nrow = 13, ncol = nproducts)
arima_mean_metrics &lt;- matrix(c(rep(1,nproducts*3)), nrow = 3, ncol = nproducts)

for (i in 1:nproducts)
{
  product[,i] &lt;- as.numeric(as.matrix(weeklyproductdemand[,i+1])) #fetching the weekly demand of the required product
  NonNAindex &lt;- which(!is.na(product[,i]))
  #FirstnonNA &lt;- min(NonNAindex)
  LastnonNA &lt;- max(NonNAindex)
  #product[,i]&lt;-product[is.finite(product[,i])] # cleans up the NA in the end of the array
  Aproduct&lt;-as.matrix(product[1:(LastnonNA - 13), i]) # cleans up the NA in the end of the array and filters out the last 13 values of the array

  #Plotting the weekly demand of the required product
  product_rawdata&lt;-ts(Aproduct)
  ts.plot(product_rawdata)

  #Forecasting using autoarima
  product_forecast&lt;-auto.arima(product_rawdata)
  product_forecast
  summary(product_forecast)
  #acf(Aproduct, ylim = c(-0.999,0.999))
  #pacf(Aproduct, ylim = c(-0.999,0.999))

  #Estimating MAPE,MAD and MSE

  #Checking for actual Q12016 values vs. Q1 predicted values
  predictedvalues&lt;-data.frame(forecast(product_rawdata,h=13))
  #predictedvalues
  #predictedvalues&lt;-data.frame(AForecast)
  #predictedvalues&lt;-as.matrix(AForecast)
  actualvalues&lt;-tail(product[(1:LastnonNA), i],13)
  error &lt;- actualvalues-predictedvalues$Point.Forecast #Calculating deviation
  perror &lt;- error/actualvalues # Calculating percentage deviation
  perror &lt;-perror[!is.infinite(perror)] #filters out the 'Inf' 
  perror &lt;-perror[is.finite(perror)] #Retains only the numerical values
  MAPE_Actuals&lt;- mean(abs(perror))# Calcuating Mean Average Percentage Error
  MAD_Actuals&lt;-mean(error)#Calculating Mean Average Deviation 
  MSE_Actuals&lt;-mean(error^2)#Calculating Mean square of errors
  MSE_Actuals
  MAPE_Actuals
  MAD_Actuals

  #storing the product forecast
  forecastoutput[,i]&lt;- predictedvalues$Point.Forecast
  actualdata[,i] &lt;- actualvalues
  arima_error_metrics[,i]&lt;-matrix(error,nrow=13,ncol = 1)
  arima_pcerror_metrics[,i]&lt;-matrix(perror,nrow=13,ncol = 1)
  arima_mean_metrics[,i]&lt;-matrix(c(MAPE_Actuals,MAD_Actuals,MSE_Actuals),nrow=3,ncol = 1)
}

#Exporting to .csv 
write.csv(forecastoutput,file=""20160627 13weeks_Forecast_ARIMA_v0.2.csv"")
write.csv(actualdata,file=""20160627 13weeks_Actual_v0.2.csv"")
write.csv(arima_error_metrics,file=""20160627 13weeks_Error_Metrics_ARIMA v0.2.csv"")
write.csv(arima_pcerror_metrics,file=""20160627 13weeks_pcError_Metrics_ARIMA v0.2.csv"")
write.csv(arima_mean_metrics,file=""20160627 13weeks_Actual_Mean-Error_Metrics_v0.2.csv"")
</code></pre>
"
"0.108678533400333","0.122205929184461","202215","<p>I am working on a daily response variable.  As part of weekly prediction methods, multiple linear regression is also used.  We also have monthly prediction on the same response variable.  In the monthly prediction, I used 4 months, 5 months, and 12 months lag of the response variable as the predictors.  For the weekly prediction, should I use 4 weeks, 5 weeks, and 12 weeks as lag or is it something else?</p>
"
"0.133103476412417","0.159649157357133","126196","<p>I'm developing an app in C# (WPF) that amongst other things, it makes a time-series based forecast of sales (4-5 months into the future). I'm an industrial engineer so I'm not pro in statistics nor in programming (basic knowledge of both).</p>

<p>What I'm doing right now is to aggregate my daily data into monthly data, then I test for monthly seasonality, and then either go for a <strong>Holt</strong>'s exponential smoothing or for a <strong>Holt-Winters</strong>'s one depending on the result. </p>

<p>For determining the <strong>smoothing parameters</strong> I'm using <strong>brute force</strong> (i.e. testing a lot of possible combinations) and keeping the one that would have predict the past year (backtesting) with minimum <a href=""http://en.wikipedia.org/wiki/Mean_absolute_error"" rel=""nofollow"">MAE</a>.</p>

<p>A <strong>problem</strong> arises: this method is SLOW (obviously, as always with brute force). It takes about 0,5s only trying the smoothing parameters in 0.05 intervals which doesn't give much accuracy. I need to do this with 1000+ items so it goes over 8 minutes (too much).</p>

<p>So I have a few <strong>questions</strong>:</p>

<ul>
<li>Is there any method to determine optimal smoothing parameters without testing all of them?</li>
<li>Using <em>R.NET</em> to use the forecast package of R will be faster?</li>
<li><p>If so, should I:</p>

<ul>
<li>Use daily or monthly data?</li>
<li>Make also an auto.arima? How to determine which model is better?</li>
</ul></li>
<li><p>Is my method of backtesting (make a model only with data previous to that point) valid to determine if a model is better than another?</p></li>
</ul>

<p><strong><em>EDIT:</em></strong> I have tried implementing R.NET. Time for <code>ets</code> is about 0,1s if I set which model to use and use only mae as <code>opt.crit</code> (if not, it goes up to 5s). </p>

<p>This is good enough <strong>IF</strong> I could get the same out-of-sample predictions I mention in the comment. If it's not possible then I would have to run it 12 times, adding up to 1,2s which is not fast enough.</p>

<ul>
<li>How can I do that (get predictions over the last 12 data without considering them in the model) in R?</li>
</ul>
"
"0.076847327936784","0.103695169473043","143367","<p>I got a question about modeling time series in R.
my data consist of the following matrix:</p>

<pre><code>1   0.03333333 0.01111111 0.9555556
2   0.03810624 0.02309469 0.9387991
3   0.00000000 0.03846154 0.9615385
4   0.03776683 0.03119869 0.9310345
5   0.06606607 0.01201201 0.9219219
6   0.03900325 0.02058505 0.9404117
7   0.03125000 0.01562500 0.9531250
8   0.00000000 0.00000000 1.0000000
9   0.04927885 0.01802885 0.9326923
10  0.06106870 0.02290076 0.9160305
11  0.03846154 0.00000000 0.9615385
12  0.00000000 0.00000000 1.0000000
13  0.06028636 0.03843256 0.9012811
14  0.09646302 0.05144695 0.8520900
15  0.04444444 0.06666667 0.8888889
</code></pre>

<p>these matrix has in total 200 rows.</p>

<p>as you can see in each situation the sum of each row is 1, that becomes because the values are the percentage of a whole. for example row 1 contains 3.33% of variable a, 1.11% of variable 2 and 95.5% of veriable 3.
the first collomn indicates the year that the values are measured.</p>

<p>my target is to make a prediction for the next 5 years, so from year 200 to 205.</p>

<p>I can doe that by making three normal time series forecast. But for that forecast the total sum is never equal to 1, which is very important. 
Normaly is use techniques like arima and exponential smoothing.</p>

<p>Does somebody know a method to make a forecast for such a problem?</p>
"
"0.140303383316578","0.151456489132551","126525","<p>I am using R and I need an easier way to produce forecasts of data at the original level based on forecasts using differenced data. </p>

<p>The situation, in more detail, is this: I am using several different models (including SVM and a few others) to forecast a time series. My models are based on differenced data since the original data is not stationary. Now I have a vector of predicted values for each model, but all the forecasts are for the differenced data. How can I get forecasts of the original data before it's differenced?</p>

<p>In other words, if I have forecasts of returns, how do I automatically get the forecasts of stock prices of the same period in R? I know the hard-coded way to do it, but I am looking for an easier way. In testing situations like rolling windows with different forecast horizons, things could be trickier I believe. To be more specific, if the window is rolling with horizon 7 (days), then $\hat{y}_{t+1}$ until $\hat{y}_{t+7}$ is easy to calculate by Glen's answer below. However, after we roll the window once we will be standing at time $t+7$, where we have both $y_{t+7}$ and $\hat{y}_{t+7}$. I want to calculate $\hat{y}_{t+8}$ as $y_{t+7}+\hat{z}_{t+8}$ and $\hat{y}_{t+9}$ as $y_{t+7}+\hat{z}_{t+8}+\hat{z}_{t+9}$, and so on.  So, for $\hat{y}_{t+8}$ to $\hat{y}_{t+14}$ the value $y_{t+7}$ needs to be used and this could go on until the end of the dataset. Is there any R function I can use to make this calculation conveniently?</p>
"
"0.159970469715827","0.199254192554687","219792","<p>My objective it to manually compute one-step ahead forecast using the estimated coefficientes given by the <code>arima</code> function in R.</p>

<p>I will consider the specific model ARIMA(0,0,0)(0,1,3) with weekly seasonality (<code>period = 7</code>). The equation for this model is:</p>

<p>$$ x_{t} = x_{t-7} + \Theta_{1}e_{t-7} + \Theta_{2}e_{t-14} + \Theta_{3}e_{t-21} + e_{t} $$</p>

<p>I will start by computing the one-step ahead forecast using the <code>predict</code> function and then compare it's value with the result given from the above equation. So first I will have to compute <code>theta</code> vector and the residuals vector <code>e_t</code>.</p>

<p>My data consists of daily observations for 35 days.</p>

<pre><code>data &lt;- c(2570,4530,3990,4480,5880,3380,1340,4180,4600,4170,1980,5170,2900,940,7430,6330,7310,9210,8460,3080,1020,4400,2980,5090,7230,3670,2440,1980,2090,3380,2410,3630,3930,2450,1590)
</code></pre>

<p>I start by fitting the model:</p>

<pre><code>fit &lt;- arima(data, order=c(0,0,0), seasonal=list(order=c(0,1,3), period=7), method=""ML"")
</code></pre>

<p>Then I recover the estimated <code>theta</code> coefficients and the last 3 observed residuals. Note that the seasonality period is 7, so the last 3 residuals regarding this seasonality are as stated:</p>

<ul>
<li>Last residual is given by position <code>35 - 7 + 1 = 29</code></li>
<li>Before last residual is given by position <code>35 - 14 + 1 = 22</code></li>
<li>Before before last residual is given by position <code>35 - 28 + 1 = 15</code></li>
</ul>

<p>So that's the reason I have the funny indexes in line two of the following code:</p>

<pre><code>theta &lt;- as.vector(fit$coef)
e_t &lt;- fit$residuals[c(29,22,15)]
</code></pre>

<p>Finnaly, I also fetch the last observation (given seasonality period 7)</p>

<pre><code>z_t &lt;- data[29]
</code></pre>

<p>And when I compute the above formula:</p>

<pre><code>sum(e_t * theta) + z_t)
</code></pre>

<p>I get the value of <code>4613.141</code> which is different from </p>

<pre><code>predict(fit)$pred[1]
</code></pre>

<p>which returns the value <code>4671.607</code>.</p>

<p>Can you please explain where is my error? I've tried this procedure with several different samples and sample sizes and I never get the same forecast as the R function.</p>
"
"0.0627455805138159","0.0423333756667302","127123","<p>I am Using Holt-Winters model for the forecasting.</p>

<p>Below is the way I am proceeding:</p>

<pre><code>x&lt;-read.csv(""C:/Users/Navneet/Desktop/retail_data_12_08.csv"", header=TRUE)
xf&lt;-data.frame(year_quarter=as.yearqtr(x$year_quarter),sales_revenue=x$sales_revenue)
dput(xf)
</code></pre>

<p>output of the dput(xf) is: </p>

<pre><code>structure(list(year_quarter = structure(c(2009, 2009.25, 2009.5, 
 2009.75, 2010, 2010.25, 2010.5, 2010.75, 2011, 2011.25, 2011.5, 
 2011.75, 2012, 2012.25), class = ""yearqtr""), sales_revenue = c(3008L, 
 3244L, 8000L, 8719L, 3008L, 3244L, 78L, 7379L, 3735L, 7339L, 
 17240L, 20465L, 13134L, 15039L)), .Names = c(""year_quarter"", 
 ""sales_revenue""), row.names = c(NA, -14L), class = ""data.frame"")

xf.ts&lt;-ts(xf, frequency=4, start=c(2009,1), end=c(2012,2))
print(xf.ts)
</code></pre>

<p>output of the above line is:</p>

<pre><code>        year_quarter sales_revenue
2009 Q1      2009.00          3008
2009 Q2      2009.25          3244
2009 Q3      2009.50          8000
2009 Q4      2009.75          8719
2010 Q1      2010.00          3008
2010 Q2      2010.25          3244
2010 Q3      2010.50            78
2010 Q4      2010.75          7379
2011 Q1      2011.00          3735
2011 Q2      2011.25          7339
2011 Q3      2011.50         17240
2011 Q4      2011.75         20465
2012 Q1      2012.00         13134
2012 Q2      2012.25         15039
</code></pre>

<p>Now if I am applying the HoltWinters function</p>

<pre><code>fit&lt;-HoltWinters(xf.ts, alpha=NULL, beta=NULL, gamma=NULL, seasonal=""additive"")
forecast(fit,6)
</code></pre>

<p>it shows the 2016 quarters like this:</p>

<pre><code>        Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95
2016 Q1       17742.13 18025.67 17458.60 18175.77 17308.50
2016 Q2       18393.12      NaN      NaN      NaN      NaN
2016 Q3       13141.48      NaN      NaN      NaN      NaN
2016 Q4       15606.02 18076.96 13135.09 19385.00 11827.05
</code></pre>

<p>It should provide the 2012 Q3, 2012 Q4, 2013 Q1, 2013 Q2, 2013 Q3 and 2013 Q4.</p>

<p>Is there any thing I am doing wrong?  </p>

<p>Why are <code>NaN</code> values are coming out in the forecasting?</p>
"
"0.0627455805138159","0.0846667513334603","59065","<p>I have a monthly time series (for 2009-2012 non-stationary, with seasonality). I can use ARIMA (or ETS) to obtain point and interval forecasts for each month of 2013, but I am interested in forecasting the total for the whole year, including prediction intervals. Is there an easy way in R to obtain interval forecasts for the total for 2013?</p>
"
"0.125491161027632","0.148166814833556","181457","<p>I'm working on daily data frames for one month. The main variables for each data frame are like this:</p>

<pre><code>Date_Heure           Fonction     Presence
2015-09-02 08:01:28  Acce         1
2015-09-02 08:15:56  check-out    0
2015-09-02 08:16:23  Alarme       0
</code></pre>

<p>The idea is to learn over 15 days the habits of the owner in his home, the rate of his presence each time slot, and when he activates the home's alarm. After building a model from these historical data, we want to know (to forecast) the next day (the 16th day), i.e., when he will activate his alarm based on the information we calculated.</p>

<p>Basically the historical data should be transformed into a model, but I cannot figure out how to do this !</p>

<p>What I have at hand are my inputs (I suppose): the percentage of presence in the two half_hour before and after activating the alarm, and my input normally should be the time that the alarm should be activated, so what I have is like this: </p>

<pre><code>Presence 1st Time slot  Presence 2nd Time slot   Date_Heure                
0.87                    0                        2015-09-02 08:16:23 
0.91                    0                        2015-09-03 08:19:02  
0.85                    0                        2015-09-04 08:18:11  
</code></pre>

<p>I have the mean of the activated hour, of the percentage of presence in the two time slot and every new day will be added to the historical data (to the model, so the historical dataset gets bigger every day by one day and the parameters;the mean, the max and the min of my indicators will change of course). 
It is like we are doing ""statistical learning"".</p>

<p>so any ideas how to start, any clue will be helpful for me</p>
"
"0.133774027136942","0.162459108322165","165166","<p>So for the last few months I've been doing a lot of forecasting for my company and specifically I've been looking at monthly forecasts of total weight of different categories of products output's each month.  I've been using time series models such as <strong>Arima</strong>, <strong>ETS</strong>, and <strong>tslm()</strong> within <strong>R</strong> to do my forecasting, as well as using cross validation to select a model. Over the last two days I've been presenting my results and discussing implementation of my forecasts.  But, I've been asked the same question multiple times and I don't know the answer to it, so let me ask you guys.  If this has been asked before, I apologize.  I'll write out a few questions that hopefully will make clear what I'm trying to understand.  Also, I'd like to keep technical answers about the models in the context of R, since that's what I'm using.</p>

<ul>
<li><p><strong><em>Do time series models take into account the number of days in a month?</em></strong></p></li>
<li><p><strong><em>Particularly, do time series models consider the number of business days in a month? (or is there a way to incorporate this?)</em></strong></p></li>
<li><p><strong><em>Do we even need to worry about this when forecasting using a time series model or does the model account for this?</em></strong></p></li>
</ul>

<p>For instance, lets say in October of 2014 a certain category sold 35,000 lbs of a product, and that there were 31 days, 23 of which were business days.  Well for this year, 2015, there are still 31 days, but now there are only 22 business days.</p>

<p>Just some background on the data, I have monthly data that starts in August of 2008. </p>

<ul>
<li><em>So would it possibly be better to average the weight per month with the number of business days and forecast out this way since the # of business days change month to month and year to year?</em></li>
</ul>
"
"0.0665517382062085","0.0898026510133874","59018","<p>I have hourly login data for a web site. Certain hours of the day for example between 09:00 and 12:00, there are heavy traffic on the site. I would like to forecast the hourly data for about one year.</p>

<p>I have seen the usage of forecast package for monthly data, but I need to do forecast of the hourly data so that I can create what-if scenarios for the hourly CPU utilization.</p>

<p>Is it possible to perform forecast on the hourly data?</p>

<p>My data points are as follows:</p>

<p>dput(head(tt,100))</p>

<pre><code>structure(list(DATETIME = structure(c(1362114000, 1362117600, 
1362121200, 1362124800, 1362128400, 1362132000, 1362135600, 1362139200, 
1362142800, 1362146400, 1362150000, 1362153600, 1362157200, 1362160800, 
1362164400, 1362168000, 1362171600, 1362175200, 1362178800, 1362182400, 
1362186000, 1362189600, 1362193200, 1362196800, 1362200400, 1362204000, 
1362207600, 1362211200, 1362214800, 1362218400, 1362222000, 1362225600, 
1362229200, 1362232800, 1362236400, 1362240000, 1362243600, 1362247200, 
1362250800, 1362254400, 1362258000, 1362261600, 1362265200, 1362268800, 
1362272400, 1362276000, 1362279600, 1362283200, 1362286800, 1362290400, 
1362294000, 1362297600, 1362301200, 1362304800, 1362308400, 1362312000, 
1362315600, 1362319200, 1362322800, 1362326400, 1362330000, 1362333600, 
1362337200, 1362340800, 1362344400, 1362348000, 1362351600, 1362355200, 
1362358800, 1362362400, 1362366000, 1362369600, 1362373200, 1362376800, 
1362380400, 1362384000, 1362387600, 1362391200, 1362394800, 1362398400, 
1362402000, 1362405600, 1362409200, 1362412800, 1362416400, 1362420000, 
1362423600, 1362427200, 1362430800, 1362434400, 1362438000, 1362441600, 
1362445200, 1362448800, 1362452400, 1362456000, 1362459600, 1362463200, 
1362466800, 1362470400), class = c(""POSIXct"", ""POSIXt""), tzone = """"), 
    LOGINS = c(432576L, 358379L, 347103L, 333591L, 271118L, 332924L, 
    522028L, 841686L, 953788L, 1084630L, 1243345L, 1327191L, 
    1257679L, 1261271L, 1093757L, 1009539L, 918686L, 817274L, 
    731382L, 657496L, 653997L, 632712L, 499769L, 434182L, 333138L, 
    252089L, 213827L, 195443L, 155659L, 167594L, 235485L, 382961L, 
    543660L, 721460L, 791414L, 790107L, 748118L, 728592L, 683574L, 
    643504L, 614126L, 571742L, 528514L, 386003L, 356637L, 332419L, 
    296185L, 272693L, 215263L, 225642L, 175703L, 120502L, 88052L, 
    80048L, 106441L, 186326L, 293553L, 413201L, 501498L, 540321L, 
    540622L, 582647L, 567774L, 555800L, 547662L, 541056L, 523127L, 
    521416L, 521093L, 511747L, 466803L, 408279L, 312245L, 229661L, 
    175773L, 152918L, 134578L, 165888L, 262662L, 432163L, 618198L, 
    790108L, 861403L, 894266L, 851507L, 847954L, 809230L, 785501L, 
    783844L, 765385L, 720353L, 695988L, 666363L, 628106L, 553925L, 
    467805L, 350987L, 242916L, 207419L, 180090L)), .Names = c(""DATETIME"", 
""LOGINS""), row.names = c(NA, 100L), class = ""data.frame"")
</code></pre>
"
"NaN","NaN","231053","<p>I have these data below; I am trying to get a model that make the year as predictor for total number of messages - I have tried time series (ARIMA) but not sure how to use it well even for one project. But I want your help please to know which the most model can fit the data? As I have many projects within different classes (I made them) and need to get one equation for forecasting total messages of any year given (even past).</p>"
"NaN","NaN","<pre><code>&gt; class(ProjectA60TM)",""
"NaN","NaN","[1] data.frame     ",""
"NaN","NaN","&gt; data.frame(ProjectA60TM)     ",""
"NaN","NaN","   total.ALL proYear     ",""
"NaN","NaN","1        467    2002     ",""
"NaN","NaN","2       2451    2003    ",""
"NaN","NaN","3       5380    2004    ",""
"NaN","NaN","4       6167    2005    ",""
"NaN","NaN","5       7032    2006    ",""
"NaN","NaN","6       7799    2007    ",""
"NaN","NaN","7       9040    2008    ",""
"NaN","NaN","8       7491    2009    ",""
"NaN","NaN","9       5620    2010    ",""
"NaN","NaN","10      5233    2011    ",""
"NaN","NaN","11      5279    2012    ",""
"NaN","NaN","12      4016    2013     ",""
"NaN","NaN","13      2463    2014     ",""
"NaN","NaN","14      1538    2015     ",""
"NaN","NaN","15       842    2016   ",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","","<r><time-series><forecasting><stationarity>"
"0.0992094737665681","0.133869888150416","127394","<p>Iâ€™m trying to produce a hourly, daily forecast for revenue in R. I set seasonal periods to 24, for 24 hours, and 365.25 for days in a year. I attached the fit vs actual plot and the forecast produced by R.</p>

<p>I then fit the time series with the tbats model due to the high seasonal periods. I then try and forecast 8112 periods or just under 1 year.</p>

<p>My problem is that I keep getting a flat model$mean. However, the fitted vs actuals looks like its catching the seasonality.</p>

<pre><code>rev_ts       &lt;- msts(revenue_data, seasonal.periods=c(24,365.25))
rev_fit      &lt;- tbats(rev_ts)
rev_forecast &lt;- forecast(rev_fit,h=8112)
plot(rev_forecast )
</code></pre>

<p><img src=""http://i.stack.imgur.com/4g46E.png"" alt=""Forecast Plot""></p>

<p><img src=""http://i.stack.imgur.com/2o3tk.png"" alt=""Fitted vs Actual""></p>

<p><strong>UPDATE:</strong></p>

<p>Trying to reference your write-up here Rob:</p>

<p><a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a></p>

<p>So m=365.25? And n= # of observations? Sorry I'm a current student and new to R (and modeling for that matter). Where does this take into account hourly seasonality</p>

<p>Trying to implement your code from post using these lines of code:</p>

<p>m=365.25 (Where does this take into account hourly seasonality)</p>

<p>n= 25656 (number of observations, historical data)</p>

<p>rev_fit &lt;- Arima(rev_ts, order=c(2,0,1), xreg=fourier(1:n,4,m))</p>

<p>plot(forecast(rev_fit, h=2*m, xreg=fourier(n+1:(2*m),4,m)))</p>

<p>Any explanation on the theory and what this is actually doing? Sorry for all the questions, but I'd love to understand this more fully.</p>

<p>Thanks!</p>
"
"0.0793675790132545","0.107095910520333","19549","<p>I have univariate time series data (windspeed at a particular place) measured at 1 hour interval for 5 years. </p>

<p>I used <code>auto.arima()</code> to get the following parameters:</p>

<pre><code>              ar1      ar2     ma1     ma2    intercept
             1.5314  -0.55   -0.1261  0.032    10.1223
     s.e.    0.0105  0.0103   0.011   0.006     0.1211

     sigma^2 estimated as 0.4865 : log likelihood = -83546.65
     AIC = 167105.3   AICc = 167105.3    BIC = 167161    
</code></pre>

<p>I am forecasting using the following equation:</p>

<pre><code>e[t] &lt;- rnorm(1, 0, sqrt(sigma^2))
x[t] &lt;- ar1*x[t-1] + ar2*x[t-2] + e[t] + ma1*e[t-1] + ma2*e[t-2]
</code></pre>

<p>When the result is compared with <code>forecast()</code> function, I get completely different answers. The freq spectrum of <code>forecast()</code> function's output resembles original time-series freq spectrum. While the manual forecast signal looks like noise in freq spectrum.</p>

<p>I can't use <code>forecast()</code> function because the application is in C++. Are the equations correct? What's the right way of forecasting from coefficients?    </p>
"
"0.0443678254708057","0.059868434008925","19620","<p>I've heard a bit about using <a href=""http://stats.stackexchange.com/questions/9842/getting-started-with-neural-networks-for-forecasting"">neural networks to forecast time series</a>, specifically <a href=""http://stats.stackexchange.com/questions/8000/proper-way-of-using-recurrent-neural-network-for-time-series-analysis"">recurrent neural networks</a>.</p>

<p>I was wondering, is there a recurrent neural network package for R?  I can't seem to find one on <a href=""http://cran.r-project.org/web/views/TimeSeries.html"">CRAN</a>.  The closest I've come is the <a href=""http://www.oga-lab.net/RGM2/func.php?rd_id=tsDyn%3annet"">nnetTs</a> function in the <a href=""http://cran.r-project.org/web/packages/tsDyn/index.html"">tsDyn</a> package, but that just calls the <a href=""http://www.oga-lab.net/RGM2/func.php?rd_id=nnet%3annet"">nnet</a> function from the <a href=""http://cran.r-project.org/web/packages/nnet/index.html"">nnet</a> package.  There's nothing special or ""reccurant"" about it.</p>
"
"NaN","NaN","20586","<p>I was wondering: is there are a package in R for automated <a href=""http://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity#GARCH"" rel=""nofollow"">GARCH</a> model selection?  I'm thinking of something like what the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"" rel=""nofollow"">forecast package</a> does for <a href=""http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=forecast%3aauto.arima"" rel=""nofollow"">ARIMA models</a>.</p>

<p>If I implement this myself, would it be appropriate to just do a grid search over the possible parameters for the GARCH and ARIMA parts of the model (using the <a href=""http://cran.r-project.org/web/packages/rugarch/index.html"" rel=""nofollow"">rugarch package</a>), and select the one with the lowest AIC (or BIC)?</p>
"
"0.147665048968456","0.132836128369791","130256","<p>I have a number of time series with strong seasonality and I am using auto.arima() from R's Forecast package along with Fourier and dummy/explanatory variables to address the seasonality to make forecasts for each time series.  In one part of the time series there are two peaks of activity.  I am looking at previous data to see how well my model predicts out-of-sample-error.  For most of my time series, my ARIMA models do a really good job at forecasting the peaks and troughs of activity.   The models will do a good job when estimating the peaks before they happen and also if I were to update the model with recent data during the middle of the first peak.  </p>

<p>My model, however, gets wonky if the first peak was higher than expected.  In this situation, if I estimate the future using only data before the first peak, my model underestimates the first peak but it accurately forecasts the following trough and does a reasonable job at forecasting the second peak.  (See below - Red is estimated activity; Black is observed activity) </p>

<p><img src=""http://i.stack.imgur.com/SXeSp.png"" alt=""Forecast before first peak""></p>

<p>If I try updating the model with recent data during the middle of the first peak, the forecast then substantially overestimates activity during the remainder of the time series. (See below - Red is estimated activity; Black is observed activity)</p>

<p><img src=""http://i.stack.imgur.com/YvP4N.png"" alt=""Forecast made during first peak""></p>

<p>Why does an updated model do this?  And is there a way to address this issue? I know from domain knowledge that even if the first peak is higher than expected the following trough should return back down, more or less, to the originally expected level.  I have tried playing with the Fourier parameter and manually testing out different ARIMA models.  </p>
"
"0.0887356509416114","0.0898026510133874","183145","<p>I would like to use a Kalman Filter to forecast price levels in some financial time-series data. Some googling has lead me to a few functions in R namely StructTS and KalmanForecast. Currently I am using StructTS to fit a model to a subset of the data and then using the fitted model to forecast a few days into the future. The problem I am having is that the model does not seem to be fitting. Right now I'm not sure if I am training the model wrong? Or if the model is not converging using optim? </p>

<p>My code and an example output is shown below:</p>

<pre><code>alsi &lt;- read.csv(""http://www.turingfinance.com/wp-content/uploads/2015/11/ALSI.csv"")
alsi &lt;- as.vector(t(alsi['ALSI']))

kDays &lt;- length(alsi)
kDays.sample &lt;- as.integer(kDays*0.9)

alsi.train &lt;- alsi[1:kDays.sample]
alsi.test &lt;- alsi[kDays.sample:kDays]

fitted.model &lt;- StructTS(alsi.train, type = ""level"")

alsi.test.forecast &lt;- KalmanForecast(n.ahead = length(alsi.test), mod = fitted.model$model)
plot.ts(alsi.test, col = 'blue')
lines(alsi.test.forecast$pred, col = 'red')

alsi.train.forecast &lt;- KalmanForecast(n.ahead =  length(alsi.train), mod = fitted.model$model)
plot.ts(alsi.train, col = 'blue')
lines(alsi.train.forecast$pred, col = 'red')
</code></pre>

<p><a href=""http://i.stack.imgur.com/2afxw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2afxw.png"" alt=""enter image description here""></a></p>

<p>As you can see, the model isn't really fitting. I have searched on Google quite a bit before posting this question, and I have read the docs in R for <code>?KalmanLike</code> and <code>?StructTS</code>. Please help. Thanks!</p>

<p>And just in case anybody else is working with Kalman Filters in R in the future, here is a link that I personally found quite helpful:</p>

<p><a href=""http://www.jstatsoft.org/article/view/v039i02/v39i02.pdf"" rel=""nofollow"">Kalman Filtering in R (survey of packages)</a> </p>
"
"0.076847327936784","0","232413","<p>As written in the title, I am looking for the stability condition of a vector error correction model (VECM). </p>

<p>I have found this phrase: </p>

<blockquote>
  <p>the companion matrix of a VECM with ð¾ endogenous variables and ð‘Ÿ cointegrating equations has ð¾ âˆ’ ð‘Ÿ
  unit eigenvalues. If the process is stable, the moduli of the remaining ð‘Ÿ eigenvalues are strictly less than one. </p>
</blockquote>

<ul>
<li>Source: <a href=""http://www.stata.com/manuals13/tsvecintro.pdf"" rel=""nofollow"">http://www.stata.com/manuals13/tsvecintro.pdf</a></li>
</ul>

<p>Is this the correct condition? If so, could anyone tell me how we define ""the companion matrix of a VECM""? Is it the coefficient matrix of X_(t-1) ? </p>

<p>In the end, how could I test this condition in R? Is there a package to do this?</p>
"
"0.076847327936784","0.0691301129820284","115710","<p>I have been using the forecast package in R to make forecasts based on an ARIMA model and have noticed a difference in the output of the forecast and simulate functions when calculating confidence intervals.</p>

<p>For example the 95% quantile calculated by the forecast function is about 0.5% higher than that based on 10000 applications of the simulate() function.  Also the mean of the simulated values and the point forecasts provided by the forecast functions are slightly different.</p>

<p>Which one of the functions will do the job better?  Or are the differences too small to worry about?  (The only reason I decided to try simulate was so that a distribution could be fitted to the simulated data).</p>

<p>Edit 1: Example</p>

<pre><code>library(forecast)

#Fit arima model to data
dm1 = arima(DAP, order = c(1,1,0), method = ""ML"", seasonal = list(order = c(0,1,1)))   

#Simulate 10000 times
n.mnths = 7
    n.sim = 10000
    domesticsimulator = function(i){
      simulate(dm1, nsim = n.mnths)
    }

sim.d &lt;- sapply(1:n.sim, function(x)domesticsimulator(x))
distr.d.mat&lt;-t(sim.d); distr.d.mat
distr.d&lt;-data.frame(Jun = distr.d.mat[,1],Jul = distr.d.mat[,2], Aug = distr.d.mat[,3], Sep = distr.d.mat[,4], Oct = distr.d.mat[,5], Nov = distr.d.mat[,6], Dec = distr.d.mat[,7]); distr.d

#Compare to forecast
forecast(dm1)
</code></pre>

<p>Edit 2: Data</p>

<blockquote>
  <p>dput(DAP)
  structure(c(43032450L, 41166780L, 49992700L, 47033260L, 49152352L, 
  52209516L, 55810773L, 53920973L, 44213408L, 49944935L, 47059495L, 
  49757124L, 43815481L, 45306644L, 54147227L, 53253194L, 53030873L, 
  56959142L, 59614287L, 57380873L, 47671785L, 54167489L, 51782564L, 
  52640057L, 47977657L, 47074882L, 58838975L, 54908859L, 57323876L, 
  59724061L, 62396446L, 59110633L, 50600325L, 53738093L, 52766404L, 
  52801276L, 48886043L, 47348142L, 58286011L, 55828555L, 57145193L, 
  59297121L, 60838606L, 58303233L, 49949551L, 55088986L, 53852209L, 
  53538970L, 50022168L, 47766421L, 59244232L, 57398267L, 59285571L, 
  61493934L, 63457403L, 62660179L, 52310402L, 57208618L, 55047116L, 
  53291139L, 50245100L, 50118363L, 59213077L, 55611053L, 58047400L, 
  59559171L, 61401480L, 58966473L, 47680101L, 52956023L, 47658141L, 
  50253800L, 44825056L, 43680328L, 53534891L, 52247781L, 52951246L, 
  55898027L, 59468957L, 56568180L, 48235025L, 52279405L, 48584832L, 
  49793527L, 45501620L, 42440614L, 54424077L, 52498074L, 53842422L, 
  56689853L, 59142493L, 57370748L, 50304708L, 54826050L, 51420519L, 
  51076415L, 46305000L, 43657818L, 55649428L, 52858479L, 55982234L, 
  57778699L, 60310568L, 57403835L, 50982170L, 54124363L, 51660083L, 
  51534990L, 47080840L, 46405385L, 56200391L, 53691570L, 55749349L, 
  57903293L, 59688267L, 58646304L, 50134504L, 53779646L, 51844482L, 
  51165451L, 47814031L, 45736763L, 56564538L, 53226735L, 56557964L, 
  57986530L, 59306473L, 58110953L, 50761250L, 54682312L, 50538227L, 
  54329096L, 47941907L, 45486064L, 57729464L, 54821717L, 57145762L
  ), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>
"
"0.076847327936784","0.103695169473043","131393","<p>I would like to have the best ARIMA model prediction that has the lowest MAPE or lowest AIC/BIC. For example, I would want to change the Arima order automatically with loop or some other way and want to test with all possible combinations like below</p>

<pre><code>c(1,0,0)
c(1,1,0)
.
.
c(x,y,z)
</code></pre>

<p>Below is the reproducible example code but I do not know how to go with multiple order execution and comparison of MAPE/AIC/BIC.</p>

<pre><code>set.seed(1)
tsdata &lt;- ts(rnorm(50), start = c(1980,1), frequency = 12)
myts &lt;- tsdata

fit &lt;- Arima(myts,order=c(2,1,0))
forecast(fit, 3)
plot(forecast(fit, 3))
fit
accuracy(fit)
</code></pre>

<p>Is it possible to save all the Accuracy measures (<code>MAPE, AIC, BIC</code>) in a data frame or in a list then select the best order to execute a Arima model? I tested with <code>auto.arima</code> in my real data but it did not give me the best order. Thanks in advance for your help !</p>
"
"0.0627455805138159","0.0846667513334603","233312","<p>am new to data science.i have used forecast package in R and got some accuracy measurements like RMSE,ME,MAPE etc.</p>

<p>can anyone explain me this measurements in a practical approach?</p>

<p>Please see my example code.</p>

<pre><code>&gt; library(forecast)

&gt; data=read.csv(""experiment.csv"")
&gt; head(data)
  BATCH value value1 value2
1     I     5.77  21.32   34.82
2    II     4.46  20.36   46.89
3   III     4.57  22.64   42.95
4    IV     3.54  23.63   48.45
5     V     6.34  19.33   36.43
6    VI     4.56  25.36   39.58

&gt; dataset=data.frame(value=data$value,value1=data$value1,value2=data$value2)
&gt; fit.nn=nnetar(dataset$value)
&gt; modelaccuracy.nn=accuracy(fit.nn)
&gt; modelaccuracy.nn
                         ME             RMSE            MAE             MPE             MAPE        MASE        ACF1   
Training set 0.001978004     0.4052840794   0.3743702393    -0.9462031738   9.191369199 0.3249114902    -0.2064674413
</code></pre>

<p>here some measurements has negative values.what it actually means?whether it's a bad dataset or forecasting accuracy would be not good?</p>

<p>many thanks in advance.</p>
"
"0.108678533400333","0.122205929184461","172550","<p>I want to forecast time-series data using the forecast package methods, but with holidays as dummy variables, as in the following:
<a href=""http://www.r-bloggers.com/forecasting-with-daily-data/"" rel=""nofollow"">http://www.r-bloggers.com/forecasting-with-daily-data/</a>
(see also:)
<a href=""http://stats.stackexchange.com/questions/92743/forecasting-with-holiday-dummy-variables"">Forecasting with holiday dummy variables</a>
I wanted to get the code for finding public holiday dates automatically, so I don't need to upload my data (which the StackExchange user had to do).</p>

<p>The function <a href=""http://www.inside-r.org/packages/cran/forecast/docs/bizdays"" rel=""nofollow"">bizdays</a> can count the number of ""business days"" in a month/or quarter. But its usage example is </p>

<pre><code>bizdays(wineind, FinCenter = ""Sydney"")
</code></pre>

<p>I looked at the source for bizdays, and in  particular the lines:</p>

<pre><code>days.len &lt;- as.timeDate(seq(start, end, by = ""days""), 
                    FinCenter = FinCenter)
biz &lt;- days.len[isBizday(days.len, holidays = unique(format(days.len, 
                                                        ""%Y"")))] 
</code></pre>

<p>However, when I applied the second line to a day.len sequence of dates, it returned the dates from day.len, minus weekends. It did not eliminate Sydney public holidays, as I would expect.</p>

<p>So my question is, what is the point of specifying ""FinCenter"" parameter in biz days, if the function just returns generic 5-day week sequence of dates. How does the FinCenter impact the function?</p>

<p>Also, is there any way of automatically retrieving public holidays for a given financial center, or do I have to load it myself?</p>

<p>Am I better off removing the holidays for e.g. stock exchange data (as weekends are removed), before the forecast? Or is it better to model the public holiday as an extra dummy variable? (As in the example). </p>

<p>I am assuming daily data in this question. </p>
"
"0.133103476412417","0.159649157357133","33862","<p>I have some models built with the <code>auto.arima</code> function from the <code>forecast</code> package. I'm modeling a variable called 'natural efluent energy' (ena), which is how much energy you can extract from some Hydrography region. There are 2 regressor variables (rainfall precipitation from period $t$ and $t-1$.)</p>

<p>Each region has it's own model - some series show positive trend, some shows negative trend, and some seems stationary. The problem is that some forecasts 'from <code>auto.arima</code>' are giving values higher/lower than usual (some forecasts give me negative values, which are not possible).</p>

<p>My original call is below:</p>

<pre><code> m1 = auto.arima(serie, xreg = regvars)
</code></pre>

<p>For the data on the link, I changed it to</p>

<pre><code> m1 = auto.arima(serie, xreg = regvars, max.P = 0, max.Q = 0, stationary = TRUE)
</code></pre>

<p>Then I get good forecasts in this case. My question is, what these parameters(<code>max.P</code>, <code>max.Q</code>) actually control, and how they relate to the trend show by my model variable?</p>

<p>Here is a link for the historic data:
<a href=""http://www.datafilehost.com/download-7718b3fc.html"" rel=""nofollow"">http://www.datafilehost.com/download-7718b3fc.html</a></p>

<p>And here a link for the forecast regressors:
<a href=""http://www.datafilehost.com/download-ca44dfa4.html"" rel=""nofollow"">http://www.datafilehost.com/download-ca44dfa4.html</a></p>

<p>And here a link of mean historic values, the forecast must fall between these values:
<a href=""http://www.datafilehost.com/download-e1e265b7.html"" rel=""nofollow"">http://www.datafilehost.com/download-e1e265b7.html</a></p>

<p>My data starts at 2001/Jun, so the serie is:</p>

<pre><code>  y = ts(dframe$ena, freq = 12, start = c(2001, 6))
</code></pre>
"
"0.147151429850636","0.180510120357961","140163","<p>I am working on a small project where we are trying to predict the prices of commodities (Oil, Aluminium, Tin, etc.) for the next 6 months. I have 12 such variables to predict and I have data from Apr, 2008 - May, 2013.</p>

<p>How should I go about prediction? I have done the following:</p>

<ul>
<li>Imported data as a Timeseries dataset </li>
<li>All variable's seasonality tends to vary with Trend, so I am going to multiplicative model. </li>
<li>I took log of the variable to convert into additive model </li>
<li>For each variable decomposed the data using STL</li>
</ul>

<p>I am planning to use Holt Winters exponential smoothing, ARIMA and neural net to forecast. I split the data as training and testing (80, 20). Planning to choose the model with less MAE, MPE, MAPE and MASE.</p>

<p>Am I doing it right?</p>

<p>Also one question I had was, before passing to ARIMA or neural net should I smooth the data? If yes, using what? The data shows both Seasonality and trend.</p>

<p>EDIT:</p>

<p>Attaching the timeseries plot and data
<img src=""http://i.stack.imgur.com/V0wes.png"" alt=""enter image description here""></p>

<pre><code>Year  &lt;- c(2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009, 
           2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010, 
           2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 
           2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 
           2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 
           2012, 2012, 2013, 2013)
Month &lt;- c(4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 
           12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 
           8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2) 
Coil  &lt;- c(44000, 44500, 42000, 45000, 42500, 41000, 39000, 35000, 34000, 
           29700, 29700, 29000, 30000, 30000, 31000, 31000, 33500, 33500, 
           33000, 31500, 34000, 35000, 35000, 36000, 38500, 38500, 35500, 
           33500, 34500, 36000, 35500, 34500, 35500, 38500, 44500, 40700, 
           40500, 39100, 39100, 39100, 38600, 39500, 39500, 38500, 39500, 
           40000, 40000, 40500, 41000, 41000, 41000, 40500, 40000, 39300, 
           39300, 39300, 39300, 39300, 39800)
coil &lt;- data.frame(Year = Year, Month = Month, Coil = Coil)
</code></pre>

<p><strong>EDIT 2:</strong>
One question, can you please tell me if my data has any seasonality or trend? And also please give me some tips on how to identify them.
<img src=""http://i.stack.imgur.com/Hg1yp.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/PdNwJ.png"" alt=""enter image description here""></p>
"
"0.0887356509416114","0.0898026510133874","105605","<p>I'd like to forecast (or predict) a time series with weights.</p>

<p>The following works using the regular <code>l</code>inear <code>m</code>odelling techniques of <code>lm</code> by applying a (sigmoidal) weight distribution to the input data, essentially weighing the latter data points more heavily than the former:</p>

<pre><code>library(""stats"")
lm.weight.function &lt;- function(x) {10 / (1 + exp(-x))} # Sigmoidal
lm.weights &lt;- lapply(seq(-13, 14, length.out = 27), lm.weight.function)
lm.input &lt;- as.data.frame(c(23957, 46771, 60767, 73284, 60296, 73122, 78304, 87154, 80459, 76885, 56479, 18809, 13453, 13951, 25140, 12035, 11920, 20683, 30357, 35019, 37732, 46150, 47856, 41931, 20985, 32526, 27283))
lm.input &lt;- cbind(1:27, lm.input)
colnames(lm.input) &lt;- c('x', 'y')
lm.model &lt;- lm(formula = y ~ log(x), data = lm.input, weights = unlist(lm.weights))
predict.input &lt;- as.data.frame(28:55)
colnames(predict.input) &lt;- 'x'
predict.model &lt;- predict(lm.model, predict.input)
plot(1:(27+28), c(lm.input$y, predict.model), type = 'l', xlab = 'x', ylab = 'y')
</code></pre>

<p><img src=""http://i.stack.imgur.com/5M4FP.png"" alt=""enter image description here""></p>

<p>Now I wish to do the same using the <code>forecast</code> package. However, I'm having difficulty specifying the <code>weights</code>:</p>

<pre><code>library(""forecast"")
ts.weight.function &lt;- function(x) {10 / (1 + exp(-x))} # Sigmoidal
ts.weights &lt;- as.data.frame(lapply(seq(-13, 14, length.out = 27), ts.weight.function))
colnames(ts.weights) &lt;- 'trend'
ts.input &lt;- ts(c(23957, 46771, 60767, 73284, 60296, 73122, 78304, 87154, 80459, 76885, 56479, 18809, 13453, 13951, 25140, 12035, 11920, 20683, 30357, 35019, 37732, 46150, 47856, 41931, 20985, 32526, 27283), frequency = 1)
ts.model &lt;- tslm(formula = ts.input ~ log(trend), weights = unlist(ts.weights))
</code></pre>

<p>The above prints an error:</p>

<pre><code>Error in eval(expr, envir, enclos) : 
  ..1 used in an incorrect context, no ... to look in
</code></pre>

<p>How can I use <code>tslm</code> to forecast a time series with weights?</p>
"
"0.0443678254708057","0.059868434008925","146098","<p>I am working on an alogorithm in R to automatize a monthly forecast calculation.
I am using, among others, the ets() function from the forecast package to calculate forecast. It is working very well.</p>

<p>Unfortunately, for some specific time series, the result I get is weird.</p>

<p>Please, find below the code i am using :</p>

<pre><code>train_ts&lt;- ts(values, frequency=12)
fit2&lt;-ets(train_ts, model=""ZZZ"", damped=TRUE, alpha=NULL, beta=NULL, gamma=NULL, 
            phi=NULL, additive.only=FALSE, lambda=TRUE, 
            lower=c(0.0001,0.0001,0.0001,0.8),upper=c(0.9999,0.9999,0.9999,0.98), 
            opt.crit=c(""lik"",""amse"",""mse"",""sigma"",""mae""), nmse=3, 
            bounds=c(""both"",""usual"",""admissible""), ic=c(""aicc"",""aic"",""bic""),
            restrict=TRUE)  
ets &lt;- forecast(fit2,h=forecasthorizon,method ='ets')   
</code></pre>

<p>Please, you will find below the concerned history data set :</p>

<pre><code> values &lt;- c(27, 27, 7, 24, 39, 40, 24, 45, 36, 37, 31, 47, 16, 24, 6, 21, 
35, 36, 21, 40, 32, 33, 27, 42, 14, 21, 5, 19, 31, 32, 19, 36, 
29, 29, 24, 42, 15, 24, 21)
</code></pre>

<p>Here, on the graph, you will see the historical data (black), the fitted value (green) and the forecast(blue). The forecast is definitely not in lines with the fitted value.</p>

<p>Do you have any idea on how to ""bound"" the forecat to be ""in line"" with the historical sales?
Thank you very very much for your kind help.</p>

<p>Best regards.<img src=""http://i.stack.imgur.com/7ORhv.jpg"" alt=""enter image description here""></p>
"
"0.117386232408469","0.135768846660426","184226","<p><strong>Background:</strong></p>

<p>I have an overall time series of close to 3 years of data. I need to forecast for different slices of data. When I slice the data, some slices results in a shorter time series. We go with the assumption that the slices of data will have the same seasonality as in the aggregated time series across all series.</p>

<p>For the shorter time series, I plan to use some simple methodologies like exponential smoothing and then use the overall seasonality factor and adjust the forecast.</p>

<p><strong>Problem:</strong></p>

<p>Is there any simple methodology to extract the monthly seasonality factors in a time series (Assuming that the data is of monthly granularity). Say, will a simple linear regression on the response variable with dummy month variables etc. Is there any better/robust methodology which gives the seasonality factors for each month of the year from the data given ?</p>
"
"0.076847327936784","0.0691301129820284","107730","<p>When using the combinef function from Rob Hyndman's very useful <a href=""http://cran.r-project.org/web/packages/hts/index.html"" rel=""nofollow"">hts package</a> for forecasting hierarchical and grouped time series, there does not seem to be a way to constrain the optimally combined forecasts to be positive- the starting forecasts can be positive, but can go negative through the reconciliation process.</p>

<p>The forecast.gts and forecast.hts functions have an argument to keep forecasts positive, but this does not seem to be an option when using combinef by itself with forecasts obtained by other methods.</p>

<p>Am I correct in this understanding, and if so is there a decent workaround? </p>
"
"0.0443678254708057","0.059868434008925","83194","<p>I tried to use the non-centred moving average, that means just using past values by setting the option centre  = FALSE, but unfortunately you get the centred results.
Can anyone detect the failure here in the ma function?</p>

<blockquote>
  <p>getAnywhere(ma)
      A single object matching â€˜maâ€™ was found
      It was found in the following places
        package:forecast
        namespace:forecast
      with value</p>
</blockquote>

<pre><code>function (x, order, centre = TRUE) 
{
    tt &lt;- 1:length(x)
    if (order%%2) {
        temp1 &lt;- ts(ksmooth(tt, x, x.points = tt, bandwidth = order - 
            1)$y)
            j &lt;- trunc(order/2)
            temp1[c(1:j, length(x) - (1:j) + 1)] &lt;- NA
        }
        else {
            temp1 &lt;- ts(ksmooth(tt, x, x.points = tt + 0.5, bandwidth = order - 
                1)$y)
            j &lt;- trunc(order/2)
            temp1[c(1:(j - 1), length(x) - (1:j) + 1)] &lt;- NA
            if (centre) {
                temp2 &lt;- ksmooth(tt, x, x.points = tt - 0.5, bandwidth = order - 
                    1)$y
            temp2[c(1:j, length(x) - (1:(j - 1)) + 1)] &lt;- NA
            temp1 &lt;- ts((temp1 + temp2)/2)
        }
    }
    tsp(temp1) &lt;- tsp(x)
    return(temp1)
}
</code></pre>
"
"0.0443678254708057","0.059868434008925","107823","<p>I am working with time series values which are all in the closed interval [0, 1]; these values represent relative frequencies, i.e., empirical probabilities. I would like to create a model such that all forecasted values are within [0, 1], but it would also be fine if the model's output was strictly within the open interval (0, 1).</p>

<p>This answered question tackles the lower bound aspect of my question, but not the upper bound aspect: <a href=""http://stats.stackexchange.com/questions/80859/how-to-achieve-strictly-positive-forecasts"">How to achieve strictly positive forecasts?</a></p>

<p>I'd like to use the R <code>forecast</code> package if possible to achieve this, but I am open to other suggestions.</p>
"
"0.108678533400333","0.146647115021353","34493","<p>I am using both R and SAS for the time series modeling. There is an option in SAS that I could not find so far in any packages developed in R for the time series modeling such as TSA or forecast package, at least to the best of my knowledge! To explain more, if we use the windowing environment in SAS to fit an ARIMA model with a regressor, we basically choose:  </p>

<p>Solution->Analysis->Time series Forecasting System->Develop Models<br>
Then Fit ARIMA model -> Predictors->Dynamic Regressors</p>

<p>If we ask to forecast this model, SAS says â€œThe following regressor(s) do not have any forecasting models. The system will automatically select forecasting models for these regressorsâ€. This means that we have not provided the values of the regressors over the forecasting period, and the system tries to find a model for that.</p>

<p>My questions:</p>

<ol>
<li>Is there any package in R with the same capability (explained above) as in SAS to forecast an ARIMA model?  </li>
<li>How can SAS automatically forecast the regressor(s) and based on what models?</li>
</ol>
"
"NaN","NaN","207987","<p>I have fit an ARIMA model to a time series with function <code>auto.arima</code> from ""forecast"" package in R. I wanted to check prediction intervals for robustness by changing the ARIMA terms. </p>

<p>Here is my R code:</p>

<pre><code>library(""forecast"", lib.loc=""~/R/win-library/3.2"")
library(""tseries"", lib.loc=""~/R/win-library/3.2"")

price = c(256, 223, 190, 170 ,140, 123, 133, 133, 125, 120, 125, 140, 166, 186, 206, 206, 206, 206, 206, 206,
       229, 263, 273, 273 ,273 ,273 ,258, 239, 233, 226, 226, 226, 249, 249, 249, 249, 249, 269, 279, 279,
       279, 279, 299, 316, 316, 316, 316, 316, 316, 316, 299, 299, 299 ,319, 319, 339 ,339, 356 ,356, 356,
       343, 343, 333 ,343 ,442 ,599, 599, 599, 599, 549, 516, 336, 336, 336, 309, 309 ,319, 565, 665, 832,
       832, 698, 632, 532, 499, 526, 526, 526, 526, 499, 466, 333 ,233, 233, 216, 200, 200, 200, 226, 239,
       279, 316, 333 ,366 ,366 ,366, 366 ,366 ,333 ,349 ,349, 349 ,359 ,359, 442 ,459 ,449 ,449, 449, 449,
       449, 449 ,449 ,459, 459 ,459, 459, 459, 446, 446, 446, 446, 459, 459, 439, 439, 439, 439, 482, 482,
       482, 482 ,516,516, 532, 532, 532 ,532 ,532 ,549, 599, 632 ,632 ,632, 632, 599 ,565 ,532, 482, 482,
       482, 482, 499 ,475 ,449, 416)

ts.plot(price)

auto.arima(price)

arima.fit&lt;-Arima(price, c(2,1,4), include.drift=TRUE)
plot(forecast.Arima(arima.fit, 60), ylim=c(-300,1300))

arima.fit&lt;-Arima(price, c(2,1,3), include.drift=TRUE)
plot(forecast.Arima(arima.fit, 60), ylim=c(-300,1300))
</code></pre>

<p>What I saw surprised me quite a bit:</p>

<p><a href=""http://i.stack.imgur.com/SHPAE.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SHPAE.jpg"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/9pNVK.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9pNVK.jpg"" alt=""enter image description here""></a></p>

<p>Why do the prediction intervals widen in the MA(3) case and hardly so in the MA(4) case? </p>
"
"0.076847327936784","0.103695169473043","147279","<p>I am trying to do time series analysis in R. 
I have data time series data set like this. </p>

<pre><code>    Month       Year    Value 
    December    2013    5300
    January     2014    289329.8
    February    2014    596518
    March       2014    328457
    April       2014    459600
    May         2014    391356
    June        2014    406288
    July        2014    644339
    August      2014    251238
    September   2014    386466.5
    October     2014    459792
    November    2014    641724
    December    2014    399831
    January     2015    210759
    February    2015    121690
    March       2015    280070
    April       2015    41336
</code></pre>

<p>Googling I found I can use auto.arima function to forecast the result. 
I managed to write R code to do forecast using auto.arima function </p>

<pre><code>    data &lt;- c(5300,289329.8,596518,328457,459600,391356,406288,644339,251238,386466.5,459792,641724,399831,210759,121690,280070,41336)
    data.ts &lt;- ts(data, start=c(2013, 12), end=c(2015, 4), frequency=12) 
    plot(data.ts)
    fit &lt;- auto.arima(data.ts)
    forec &lt;- forecast(fit)
    plot(forec)
</code></pre>

<p>Problem is my forecast result always remain same. </p>

<p><img src=""http://i.stack.imgur.com/SuJ6a.png"" alt=""enter image description here""></p>

<p>Could  any tell me what is going wrong. or help me to correct my forecast result. Thanks</p>
"
"0.0627455805138159","0.0846667513334603","208091","<p>I'm trying to understand how the rolling forecast example below from <a href=""http://robjhyndman.com/hyndsight/rolling-forecasts/"" rel=""nofollow"">Rob Hyndman's blog</a> works. In the final line of the <code>for</code> loop, is <code>fc</code> forecasting horizons into the future beyond the end of test?  Or is <code>fc</code> meant to be a forecasted version of test, that could be compared to check for accuracy? </p>

<p>My own goal is to create something similar that would train a model and forecast it several horizons in to the future.</p>

<p>Code:</p>

<pre><code>library(""fpp"")
library(""forecast"")

##Multi-step forecasts without re-estimation

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, model=fit)
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}
</code></pre>
"
"0.076847327936784","0.0691301129820284","208080","<p>I compared the <code>auto.arima</code> forecast <code>checkts</code> below  to the rolling forecast <code>fc</code> and noticed that every of the error measures is lower for <code>fc</code>.  </p>

<p>Will rolling forecasts have lower errors than a forecasted <code>auto.arima</code> model in general?<br>
Why might that happen? </p>

<p>The data to run the code below is in the ""fpp"" package. Code:</p>

<pre><code>library(""fpp"")
library(""forecast"")

##Multi-step forecasts without re-estimation

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, model=fit)
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}


checkts&lt;-forecast(fit,h=71)

accuracy(checkts$mean,test)
	accuracy(fc,test) ##All Error measures are lower than Checkts$mean
</code></pre>
"
"0.0887356509416114","0.11973686801785","208321","<p>I am trying to forecast the median wait time each hour for a customer to get served in a call center.  I know the median wait times each hour and the number of customers who called in each hour (CustCount) in the past, but I don't know how many operators were staffed each hour to answer calls.  I imagine the call center increases staff during the busy times of day, but I don't know.  My data is also very noisy and it's hard to see any clear patterns.</p>

<p>If anyone can suggest strategy or point to a similar example I would be grateful.  I've been experimenting with Arima models with predictors.  </p>

<p>I'm really wondering how much the staffing levels matter and how they could be identified or addressed.  I was thinking maybe looking for level shifts might be an approach.</p>

<p>I have some sample data below.</p>

<p>Data:</p>

<pre><code>dput(dfE86[1:525,c(""DateTime"",""WaitTime"",""CustCount"")])
</code></pre>

<p>structure(list(DateTime = c(""2015-01-01 00:00"", ""2015-01-01 01:00"", 
""2015-01-01 02:00"", ""2015-01-01 03:00"", ""2015-01-01 04:00"", ""2015-01-01 05:00"", 
""2015-01-01 06:00"", ""2015-01-01 07:00"", ""2015-01-01 08:00"", ""2015-01-01 09:00"", 
""2015-01-01 10:00"", ""2015-01-01 11:00"", ""2015-01-01 12:00"", ""2015-01-01 13:00"", 
""2015-01-01 14:00"", ""2015-01-01 15:00"", ""2015-01-01 16:00"", ""2015-01-01 17:00"", 
""2015-01-01 18:00"", ""2015-01-01 19:00"", ""2015-01-01 20:00"", ""2015-01-01 21:00"", 
""2015-01-01 22:00"", ""2015-01-01 23:00"", ""2015-01-02 00:00"", ""2015-01-02 01:00"", 
""2015-01-02 02:00"", ""2015-01-02 03:00"", ""2015-01-02 04:00"", ""2015-01-02 05:00"", 
""2015-01-02 06:00"", ""2015-01-02 07:00"", ""2015-01-02 08:00"", ""2015-01-02 09:00"", 
""2015-01-02 10:00"", ""2015-01-02 11:00"", ""2015-01-02 12:00"", ""2015-01-02 13:00"", 
""2015-01-02 14:00"", ""2015-01-02 15:00"", ""2015-01-02 16:00"", ""2015-01-02 17:00"", 
""2015-01-02 18:00"", ""2015-01-02 19:00"", ""2015-01-02 20:00"", ""2015-01-02 21:00"", 
""2015-01-02 22:00"", ""2015-01-02 23:00"", ""2015-01-03 00:00"", ""2015-01-03 01:00"", 
""2015-01-03 02:00"", ""2015-01-03 03:00"", ""2015-01-03 04:00"", ""2015-01-03 05:00"", 
""2015-01-03 06:00"", ""2015-01-03 07:00"", ""2015-01-03 08:00"", ""2015-01-03 09:00"", 
""2015-01-03 10:00"", ""2015-01-03 11:00"", ""2015-01-03 12:00"", ""2015-01-03 13:00"", 
""2015-01-03 14:00"", ""2015-01-03 15:00"", ""2015-01-03 16:00"", ""2015-01-03 17:00"", 
""2015-01-03 18:00"", ""2015-01-03 19:00"", ""2015-01-03 20:00"", ""2015-01-03 21:00"", 
""2015-01-03 22:00"", ""2015-01-03 23:00"", ""2015-01-04 00:00"", ""2015-01-04 01:00"", 
""2015-01-04 02:00"", ""2015-01-04 03:00"", ""2015-01-04 04:00"", ""2015-01-04 05:00"", 
""2015-01-04 06:00"", ""2015-01-04 07:00"", ""2015-01-04 08:00"", ""2015-01-04 09:00"", 
""2015-01-04 10:00"", ""2015-01-04 11:00"", ""2015-01-04 12:00"", ""2015-01-04 13:00"", 
""2015-01-04 14:00"", ""2015-01-04 15:00"", ""2015-01-04 16:00"", ""2015-01-04 17:00"", 
""2015-01-04 18:00"", ""2015-01-04 19:00"", ""2015-01-04 20:00"", ""2015-01-04 21:00"", 
""2015-01-04 22:00"", ""2015-01-04 23:00"", ""2015-01-05 00:00"", ""2015-01-05 01:00"", 
""2015-01-05 02:00"", ""2015-01-05 03:00"", ""2015-01-05 04:00"", ""2015-01-05 05:00"", 
""2015-01-05 06:00"", ""2015-01-05 07:00"", ""2015-01-05 08:00"", ""2015-01-05 09:00"", 
""2015-01-05 10:00"", ""2015-01-05 11:00"", ""2015-01-05 12:00"", ""2015-01-05 13:00"", 
""2015-01-05 14:00"", ""2015-01-05 15:00"", ""2015-01-05 16:00"", ""2015-01-05 17:00"", 
""2015-01-05 18:00"", ""2015-01-05 19:00"", ""2015-01-05 20:00"", ""2015-01-05 21:00"", 
""2015-01-05 22:00"", ""2015-01-05 23:00"", ""2015-01-06 00:00"", ""2015-01-06 01:00"", 
""2015-01-06 02:00"", ""2015-01-06 03:00"", ""2015-01-06 04:00"", ""2015-01-06 05:00"", 
""2015-01-06 06:00"", ""2015-01-06 07:00"", ""2015-01-06 08:00"", ""2015-01-06 09:00"", 
""2015-01-06 10:00"", ""2015-01-06 11:00"", ""2015-01-06 12:00"", ""2015-01-06 13:00"", 
""2015-01-06 14:00"", ""2015-01-06 15:00"", ""2015-01-06 16:00"", ""2015-01-06 17:00"", 
""2015-01-06 18:00"", ""2015-01-06 19:00"", ""2015-01-06 20:00"", ""2015-01-06 21:00"", 
""2015-01-06 22:00"", ""2015-01-06 23:00"", ""2015-01-07 00:00"", ""2015-01-07 01:00"", 
""2015-01-07 02:00"", ""2015-01-07 03:00"", ""2015-01-07 04:00"", ""2015-01-07 05:00"", 
""2015-01-07 06:00"", ""2015-01-07 07:00"", ""2015-01-07 08:00"", ""2015-01-07 09:00"", 
""2015-01-07 10:00"", ""2015-01-07 11:00"", ""2015-01-07 12:00"", ""2015-01-07 13:00"", 
""2015-01-07 14:00"", ""2015-01-07 15:00"", ""2015-01-07 16:00"", ""2015-01-07 17:00"", 
""2015-01-07 18:00"", ""2015-01-07 19:00"", ""2015-01-07 20:00"", ""2015-01-07 21:00"", 
""2015-01-07 22:00"", ""2015-01-07 23:00"", ""2015-01-08 00:00"", ""2015-01-08 01:00"", 
""2015-01-08 02:00"", ""2015-01-08 03:00"", ""2015-01-08 04:00"", ""2015-01-08 05:00"", 
""2015-01-08 06:00"", ""2015-01-08 07:00"", ""2015-01-08 08:00"", ""2015-01-08 09:00"", 
""2015-01-08 10:00"", ""2015-01-08 11:00"", ""2015-01-08 12:00"", ""2015-01-08 13:00"", 
""2015-01-08 14:00"", ""2015-01-08 15:00"", ""2015-01-08 16:00"", ""2015-01-08 17:00"", 
""2015-01-08 18:00"", ""2015-01-08 19:00"", ""2015-01-08 20:00"", ""2015-01-08 21:00"", 
""2015-01-08 22:00"", ""2015-01-08 23:00"", ""2015-01-09 00:00"", ""2015-01-09 01:00"", 
""2015-01-09 02:00"", ""2015-01-09 03:00"", ""2015-01-09 04:00"", ""2015-01-09 05:00"", 
""2015-01-09 06:00"", ""2015-01-09 07:00"", ""2015-01-09 08:00"", ""2015-01-09 09:00"", 
""2015-01-09 10:00"", ""2015-01-09 11:00"", ""2015-01-09 12:00"", ""2015-01-09 13:00"", 
""2015-01-09 14:00"", ""2015-01-09 15:00"", ""2015-01-09 16:00"", ""2015-01-09 17:00"", 
""2015-01-09 18:00"", ""2015-01-09 19:00"", ""2015-01-09 20:00"", ""2015-01-09 21:00"", 
""2015-01-09 22:00"", ""2015-01-09 23:00"", ""2015-01-10 00:00"", ""2015-01-10 01:00"", 
""2015-01-10 02:00"", ""2015-01-10 03:00"", ""2015-01-10 04:00"", ""2015-01-10 05:00"", 
""2015-01-10 06:00"", ""2015-01-10 07:00"", ""2015-01-10 08:00"", ""2015-01-10 09:00"", 
""2015-01-10 10:00"", ""2015-01-10 11:00"", ""2015-01-10 12:00"", ""2015-01-10 13:00"", 
""2015-01-10 14:00"", ""2015-01-10 15:00"", ""2015-01-10 16:00"", ""2015-01-10 17:00"", 
""2015-01-10 18:00"", ""2015-01-10 19:00"", ""2015-01-10 20:00"", ""2015-01-10 21:00"", 
""2015-01-10 22:00"", ""2015-01-10 23:00"", ""2015-01-11 00:00"", ""2015-01-11 01:00"", 
""2015-01-11 02:00"", ""2015-01-11 03:00"", ""2015-01-11 04:00"", ""2015-01-11 05:00"", 
""2015-01-11 06:00"", ""2015-01-11 07:00"", ""2015-01-11 08:00"", ""2015-01-11 09:00"", 
""2015-01-11 10:00"", ""2015-01-11 11:00"", ""2015-01-11 12:00"", ""2015-01-11 13:00"", 
""2015-01-11 14:00"", ""2015-01-11 15:00"", ""2015-01-11 16:00"", ""2015-01-11 17:00"", 
""2015-01-11 18:00"", ""2015-01-11 19:00"", ""2015-01-11 20:00"", ""2015-01-11 21:00"", 
""2015-01-11 22:00"", ""2015-01-11 23:00"", ""2015-01-12 00:00"", ""2015-01-12 01:00"", 
""2015-01-12 02:00"", ""2015-01-12 03:00"", ""2015-01-12 04:00"", ""2015-01-12 05:00"", 
""2015-01-12 06:00"", ""2015-01-12 07:00"", ""2015-01-12 08:00"", ""2015-01-12 09:00"", 
""2015-01-12 10:00"", ""2015-01-12 11:00"", ""2015-01-12 12:00"", ""2015-01-12 13:00"", 
""2015-01-12 14:00"", ""2015-01-12 15:00"", ""2015-01-12 16:00"", ""2015-01-12 17:00"", 
""2015-01-12 18:00"", ""2015-01-12 19:00"", ""2015-01-12 20:00"", ""2015-01-12 21:00"", 
""2015-01-12 22:00"", ""2015-01-12 23:00"", ""2015-01-13 00:00"", ""2015-01-13 01:00"", 
""2015-01-13 02:00"", ""2015-01-13 03:00"", ""2015-01-13 04:00"", ""2015-01-13 05:00"", 
""2015-01-13 06:00"", ""2015-01-13 07:00"", ""2015-01-13 08:00"", ""2015-01-13 09:00"", 
""2015-01-13 10:00"", ""2015-01-13 11:00"", ""2015-01-13 12:00"", ""2015-01-13 13:00"", 
""2015-01-13 14:00"", ""2015-01-13 15:00"", ""2015-01-13 16:00"", ""2015-01-13 17:00"", 
""2015-01-13 18:00"", ""2015-01-13 19:00"", ""2015-01-13 20:00"", ""2015-01-13 21:00"", 
""2015-01-13 22:00"", ""2015-01-13 23:00"", ""2015-01-14 00:00"", ""2015-01-14 01:00"", 
""2015-01-14 02:00"", ""2015-01-14 03:00"", ""2015-01-14 04:00"", ""2015-01-14 05:00"", 
""2015-01-14 06:00"", ""2015-01-14 07:00"", ""2015-01-14 08:00"", ""2015-01-14 09:00"", 
""2015-01-14 10:00"", ""2015-01-14 11:00"", ""2015-01-14 12:00"", ""2015-01-14 13:00"", 
""2015-01-14 14:00"", ""2015-01-14 15:00"", ""2015-01-14 16:00"", ""2015-01-14 17:00"", 
""2015-01-14 18:00"", ""2015-01-14 19:00"", ""2015-01-14 20:00"", ""2015-01-14 21:00"", 
""2015-01-14 22:00"", ""2015-01-14 23:00"", ""2015-01-15 00:00"", ""2015-01-15 01:00"", 
""2015-01-15 02:00"", ""2015-01-15 03:00"", ""2015-01-15 04:00"", ""2015-01-15 05:00"", 
""2015-01-15 06:00"", ""2015-01-15 07:00"", ""2015-01-15 08:00"", ""2015-01-15 09:00"", 
""2015-01-15 10:00"", ""2015-01-15 11:00"", ""2015-01-15 12:00"", ""2015-01-15 13:00"", 
""2015-01-15 14:00"", ""2015-01-15 15:00"", ""2015-01-15 16:00"", ""2015-01-15 17:00"", 
""2015-01-15 18:00"", ""2015-01-15 19:00"", ""2015-01-15 20:00"", ""2015-01-15 21:00"", 
""2015-01-15 22:00"", ""2015-01-15 23:00"", ""2015-01-16 00:00"", ""2015-01-16 01:00"", 
""2015-01-16 02:00"", ""2015-01-16 03:00"", ""2015-01-16 04:00"", ""2015-01-16 05:00"", 
""2015-01-16 06:00"", ""2015-01-16 07:00"", ""2015-01-16 08:00"", ""2015-01-16 09:00"", 
""2015-01-16 10:00"", ""2015-01-16 11:00"", ""2015-01-16 12:00"", ""2015-01-16 13:00"", 
""2015-01-16 14:00"", ""2015-01-16 15:00"", ""2015-01-16 16:00"", ""2015-01-16 17:00"", 
""2015-01-16 18:00"", ""2015-01-16 19:00"", ""2015-01-16 20:00"", ""2015-01-16 21:00"", 
""2015-01-16 22:00"", ""2015-01-16 23:00"", ""2015-01-17 00:00"", ""2015-01-17 01:00"", 
""2015-01-17 02:00"", ""2015-01-17 03:00"", ""2015-01-17 04:00"", ""2015-01-17 05:00"", 
""2015-01-17 06:00"", ""2015-01-17 07:00"", ""2015-01-17 08:00"", ""2015-01-17 09:00"", 
""2015-01-17 10:00"", ""2015-01-17 11:00"", ""2015-01-17 12:00"", ""2015-01-17 13:00"", 
""2015-01-17 14:00"", ""2015-01-17 15:00"", ""2015-01-17 16:00"", ""2015-01-17 17:00"", 
""2015-01-17 18:00"", ""2015-01-17 19:00"", ""2015-01-17 20:00"", ""2015-01-17 21:00"", 
""2015-01-17 22:00"", ""2015-01-17 23:00"", ""2015-01-18 00:00"", ""2015-01-18 01:00"", 
""2015-01-18 02:00"", ""2015-01-18 03:00"", ""2015-01-18 04:00"", ""2015-01-18 05:00"", 
""2015-01-18 06:00"", ""2015-01-18 07:00"", ""2015-01-18 08:00"", ""2015-01-18 09:00"", 
""2015-01-18 10:00"", ""2015-01-18 11:00"", ""2015-01-18 12:00"", ""2015-01-18 13:00"", 
""2015-01-18 14:00"", ""2015-01-18 15:00"", ""2015-01-18 16:00"", ""2015-01-18 17:00"", 
""2015-01-18 18:00"", ""2015-01-18 19:00"", ""2015-01-18 20:00"", ""2015-01-18 21:00"", 
""2015-01-18 22:00"", ""2015-01-18 23:00"", ""2015-01-19 00:00"", ""2015-01-19 01:00"", 
""2015-01-19 02:00"", ""2015-01-19 03:00"", ""2015-01-19 04:00"", ""2015-01-19 05:00"", 
""2015-01-19 06:00"", ""2015-01-19 07:00"", ""2015-01-19 08:00"", ""2015-01-19 09:00"", 
""2015-01-19 10:00"", ""2015-01-19 11:00"", ""2015-01-19 12:00"", ""2015-01-19 13:00"", 
""2015-01-19 14:00"", ""2015-01-19 15:00"", ""2015-01-19 16:00"", ""2015-01-19 17:00"", 
""2015-01-19 18:00"", ""2015-01-19 19:00"", ""2015-01-19 20:00"", ""2015-01-19 21:00"", 
""2015-01-19 22:00"", ""2015-01-19 23:00"", ""2015-01-20 00:00"", ""2015-01-20 01:00"", 
""2015-01-20 02:00"", ""2015-01-20 03:00"", ""2015-01-20 04:00"", ""2015-01-20 05:00"", 
""2015-01-20 06:00"", ""2015-01-20 07:00"", ""2015-01-20 08:00"", ""2015-01-20 09:00"", 
""2015-01-20 10:00"", ""2015-01-20 11:00"", ""2015-01-20 12:00"", ""2015-01-20 13:00"", 
""2015-01-20 14:00"", ""2015-01-20 15:00"", ""2015-01-20 16:00"", ""2015-01-20 17:00"", 
""2015-01-20 18:00"", ""2015-01-20 19:00"", ""2015-01-20 20:00"", ""2015-01-20 21:00"", 
""2015-01-20 22:00"", ""2015-01-20 23:00"", ""2015-01-21 00:00"", ""2015-01-21 01:00"", 
""2015-01-21 02:00"", ""2015-01-21 03:00"", ""2015-01-21 04:00"", ""2015-01-21 05:00"", 
""2015-01-21 06:00"", ""2015-01-21 07:00"", ""2015-01-21 08:00"", ""2015-01-21 09:00"", 
""2015-01-21 10:00"", ""2015-01-21 11:00"", ""2015-01-21 12:00"", ""2015-01-21 13:00"", 
""2015-01-21 14:00"", ""2015-01-21 15:00"", ""2015-01-21 16:00"", ""2015-01-21 17:00"", 
""2015-01-21 18:00"", ""2015-01-21 19:00"", ""2015-01-21 20:00"", ""2015-01-21 21:00"", 
""2015-01-21 22:00"", ""2015-01-21 23:00"", ""2015-01-22 00:00"", ""2015-01-22 01:00"", 
""2015-01-22 02:00"", ""2015-01-22 03:00"", ""2015-01-22 04:00"", ""2015-01-22 05:00"", 
""2015-01-22 06:00"", ""2015-01-22 07:00"", ""2015-01-22 08:00"", ""2015-01-22 09:00"", 
""2015-01-22 10:00"", ""2015-01-22 11:00"", ""2015-01-22 12:00"", ""2015-01-22 13:00"", 
""2015-01-22 14:00"", ""2015-01-22 15:00"", ""2015-01-22 16:00"", ""2015-01-22 17:00"", 
""2015-01-22 18:00"", ""2015-01-22 19:00"", ""2015-01-22 20:00""), 
    WaitTime = c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 
    8.5, 4, 5, 9, 10, 11, 7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 
    2, 15, 2.5, 17, 5, 5.5, 7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 
    9.5, 3.5, 5, 4, 4, 9, 4.5, 6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 
    12, 17.5, 19, 7, 14, 17, 3.5, 6, 15, 11, 10.5, 11, 13, 9.5, 
    9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 19, 6, 7, 7.5, 7.5, 7, 6.5, 
    9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 5, 12, 6, NA, 4, 2, 5, 7.5, 
    11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 7, 4.5, 9, 3, 4, 6, 17.5, 
    11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 7, 7, 4, 7.5, 11, 6, 11, 
    7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 6, 8.5, 7.5, 6, 5, 
    8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 11.5, 3, 4, 16, 
    3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 6.5, 9, 12, 
    17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 6.5, 15, 
    8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 16.5, 
    2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 13, 
    10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
    NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 
    11.5, 12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 
    10, 10, 13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 
    5.5, 6, 14, 16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 
    13, 6, 7, 3, 5.5, 7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 
    13, NA, 12, 1.5, 7, 7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 
    8, 6, 3, 7.5, 4, 7, 7.5, NA, NA, NA, NA, 6.5, 2, 16.5, 7.5, 
    8, 8, 5, 2, 7, 4, 6.5, 4.5, 10, 6, 4.5, 6.5, 9, 2, 6, 3.5, 
    NA, 5, 7, 3.5, 4, 4.5, 13, 19, 8.5, 10, 8, 13, 10, 10, 6, 
    13.5, 12, 11, 5.5, 6, 3.5, 9, 8, NA, 6, 5, 8.5, 3, 12, 10, 
    9.5, 7, 24, 7, 9, 11.5, 5, 7, 11, 6, 5.5, 3, 4.5, 4, 5, 5, 
    3, 4.5, 6, 10, 5, 4, 4, 9.5, 5, 7, 6, 3, 13, 5.5, 5, 7.5, 
    3, 5, 6.5, 5, 5.5, 6, 4, 3, 5, NA, 5, 5, 6, 7, 8, 5, 5.5, 
    9, 6, 8.5, 9.5, 8, 9, 6, 12, 5, 7, 5, 3.5, 4, 7.5, 7, 5, 
    4, 4, NA, 7, 5.5, 6, 8.5, 6.5, 9, 3, 2, 8, 15, 6, 4, 10, 
    7, 13, 14, 9.5, 9, 18, 6, 5, 4, 6, 4, 11.5, 17.5, 7, 8, 10, 
    4, 7, 5, 9, 6, 5, 4, 8, 4, 2, 1.5, 3.5, 6, 5.5, 5, 4, 8, 
    10.5, 4, 11, 9.5, 5, 6, 11, 21, 9.5, 11, 13.5, 7.5, 13, 10, 
    7, 9.5, 6, 10), CustCount = c(2, 6, 3, 5, 3, 2, 2, NA, 2, 6, 
    12, 11, 9, 10, 13, 9, 11, 7, 12, 8, 6, 4, 10, 6, 2, 7, 2, 
    1, 3, 2, 1, 3, 8, 7, 7, 8, 13, 13, 13, 11, 12, 4, 12, 18, 
    12, 7, 5, 4, 6, 4, 3, 3, NA, 4, 2, 8, 8, 8, 7, 3, 5, 3, 7, 
    8, 7, 7, 11, 8, 10, 3, 10, 6, 5, 5, 3, 1, 2, 1, 1, 3, 4, 
    8, 8, 5, 9, 12, 12, 11, 8, 5, 9, 10, 7, 8, 4, 6, 4, 1, 3, 
    1, 3, NA, 2, 1, 4, 10, 7, 13, 6, 9, 6, 16, 12, 11, 10, 12, 
    9, 7, 7, 7, 6, 2, 3, 1, 1, 2, 2, 3, 11, 10, 9, 8, 9, 13, 
    6, 6, 10, 9, 11, 10, 8, 7, 6, 4, 2, 3, 5, 3, 2, 4, 4, 4, 
    8, 5, 12, 8, 7, 12, 9, 12, 12, 12, 13, 12, 9, 8, 9, 10, 4, 
    7, 4, 2, 2, 4, 1, 7, 6, 6, 8, 11, 11, 5, 7, 6, 9, 12, 15, 
    9, 11, 5, 10, 5, 4, 4, 2, 3, 3, 2, 5, 4, 7, 8, 6, 6, 5, 12, 
    10, 8, 10, 10, 4, 13, 12, 6, 8, 6, 3, 1, 4, 2, NA, 4, 3, 
    2, 6, 5, 8, 10, 4, 13, 2, 13, 8, 11, 13, 8, 9, 10, 9, 5, 
    1, NA, 1, 1, 2, NA, 1, 7, 6, 10, 7, 8, 12, 12, 9, 5, 6, 8, 
    13, 13, 13, 8, 8, 1, 5, 7, 6, 2, NA, 2, 1, 2, 7, 9, 12, 12, 
    10, 10, 10, 6, 8, 2, 8, 3, 4, 5, 6, 2, 2, 1, 4, 1, NA, 3, 
    1, 3, 8, 8, 11, 11, 12, 5, 7, 14, 9, 10, 14, 11, 8, 6, 8, 
    7, 5, 4, 3, 4, 9, NA, 2, 4, 5, 8, 2, 12, 8, 15, 12, 8, 9, 
    12, 9, 9, 12, 7, 7, 8, 7, 5, 4, NA, 1, NA, NA, 4, 9, 8, 8, 
    8, 12, 13, 7, 11, 8, 14, 12, 13, 15, 8, 6, 4, 4, 5, 2, NA, 
    2, 5, 4, 5, 6, 15, 11, 10, 16, 10, 5, 5, 10, 13, 10, 9, 8, 
    7, 5, 4, 5, 6, NA, 2, 5, 4, 1, 6, 5, 8, 4, 3, 10, 11, 8, 
    12, 10, 10, 10, 12, 10, 10, 7, 5, 7, 3, 4, 3, 3, 3, 3, 8, 
    4, 8, 10, 5, 10, 10, 10, 11, 10, 11, 7, 10, 7, 6, 7, 7, 3, 
    3, NA, 3, 6, 5, 3, 3, 5, 6, 6, 13, 14, 14, 7, 13, 9, 10, 
    4, 9, 10, 8, 3, 6, 10, 5, 2, 1, NA, 3, 4, 4, 12, 12, 11, 
    12, 11, 13, 10, 9, 11, 11, 14, 10, 13, 10, 7, 11, 1, 3, 1, 
    4, 1, 2, 2, 3, 9, 6, 9, 9, 8, 9, 7, 12, 17, 13, 9, 10, 8, 
    8, 10, 2, 3, 3, 6, 2, 2, 1, 6, 8, 7, 9, 5, 11, 8, 8, 12, 
    13, 14, 10, 7, 5, 11)), .Names = c(""DateTime"", ""WaitTime"", 
""CustCount""), row.names = c(NA, 525L), class = ""data.frame"")</p>
"
"0.0443678254708057","0.059868434008925","35732","<p>I have four variables and would like to construct a VAR model I would then like to make a VAR forecast on one of the variables using my own data for the forecasts of the other three variables. <strong>Is there an R package or process from the model fit that allows me to do this?</strong></p>"
"NaN","NaN","","<r><time-series><forecasting><var>"
"NaN","NaN","108551","<p>I have daily sales data from 2011 to 2013. I have to do prediction for 2014.I have used arima and exponential method to predict the daily sale, but it is not giving the better result. MAPE is around 25%. </p>

<p><code>y=ts(x,frequency=7)</code></p>

<p><code>fit &lt;- auto.arima(y)</code></p>

<p><code>fc &lt;- forecast(fit, h=265)</code></p>

<p><code>plot(fc)</code></p>

<p><code>fit &lt;- ets(y)</code></p>

<p><code>fc &lt;- forecast(fit,h=265)</code></p>

<p>Below is the link for data:</p>

<p><a href=""https://drive.google.com/file/d/0B5en_TDcZWi3QWNjMzd3ZkdIcW8/edit?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B5en_TDcZWi3QWNjMzd3ZkdIcW8/edit?usp=sharing</a></p>

<p>Is there a way to improve the MAPE? I am new to time series,I would appreciate any kind of help.</p>
"
"0.100616770635831","0.113140705550355","208515","<p>I'm trying to understand the steps in Rob Hyndman's Multi-step forecasts without re-estimation example below.  I'm wondering what the purpose is of </p>

<pre><code>refit &lt;- Arima(x, model=fit)
</code></pre>

<p>The model has already been determined and trained by auto.arima in the ""fit"" step.  So in the ""refit"" step are we re-training the model on a new data set?  If so, what is the point of retraining the same model on a new data set?</p>

<p>url:
<a href=""http://robjhyndman.com/hyndsight/rolling-forecasts/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/rolling-forecasts/</a></p>

<p>Code:</p>

<pre><code>library(fpp)

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, model=fit)
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}
</code></pre>

<p>Updated Code to re-estimate coefficients:</p>

<pre><code>h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
order &lt;- arimaorder(fit)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, order=order[1:3],seasonal=order[4:6])
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}
</code></pre>
"
"0.0627455805138159","0.0423333756667302","63883","<p>Is there a method to find the right distance function in non-parametric regression?
I use some time series to learn forecasting. Series are nonlinear and non-gaussian.
I can get the right dimension and delay. I can find the right bandwidth with the hdrcde library.
I have no problem with kernel functions.<br>
My problem is with distances. I use Euclidian, Cosine and Correlation weighting functions.
These are the kernel which give good results, but one time, Euclidian is good, then after adding some data, one to 5-7 generally, Euclidian give nothing, even with very little change in statistics.
So, my question is if there is a method to choose the right distance. I would like to have advices on that point and on articles that will help solve this problem. What package in R could eventually help?</p>

<p>Thank you. </p>
"
"NaN","NaN","209173","<p>I am working on a project, and I am absolutely new to forecasting and not so strong in statistics. I have an employee data for the last 7 years, along with the other variables like economic growth, employee turnover, vacancies, and some other economical factors. 
I have to do forecasting for the next 5 years. I have some questions: </p>

<ul>
<li>Is it possible to do a time series analysis with more than one explanatory variable? Can this be done in R? </li>
</ul>

<p>I would appreciate any kind of help. Thanks in advance!! </p>
"
"0.140303383316578","0.151456489132551","148101","<p>I have a data set that contains outliers (big orders) i need to forecast this series taking the outliers into consideration. I already know what the top 11 big orders are so i dont need to detect them first. I have tried a few ways to deal with this 1) forecast the data 10 times each time replacing the biggest outlier with the next biggest until the last set is run with them all replaced and then compare results 2) forecast the data another 10 times removing the outliers in each until they are all removed in the last set. Both of these work but they dont consistently give accurate forecasts. I was wondering if anyone knew another way to approach this?</p>

<p>One way i was thinking was running a weighted ARIMA and work it so that less/minimal weight is put on those specific data points. Is this possible?</p>

<p>I just want to point out that removing the known outliers does not delete that point completely, only minimizes it as there are other deals that happened in that quarter</p>

<p>One of my data sets is the following:</p>

<pre><code>data &lt;- matrix(c(""08Q1"",    ""08Q2"", ""08Q3"", ""08Q4"", ""09Q1"", ""09Q2"", ""09Q3"", ""09Q4"", ""10Q1"", ""10Q2"", ""10Q3"", ""10Q4"", ""11Q1"", ""11Q2"", ""11Q3"", ""11Q4"", ""12Q1"", ""12Q2"", ""12Q3"", ""12Q4"", ""13Q1"", ""13Q2"", ""13Q3"", ""13Q4"", ""14Q1"", ""14Q2"", ""14Q3"", ""14Q4"",155782698,   159463653.4,    172741125.6,    204547180,  126049319.8,    138648461.5,    135678842.1,    242568446.1,    177019289.3,    200397120.6,    182516217.1,    306143365.6,    222890269.2,    239062450.2,    229124263.2,    370575382.9,    257757410.5,    256125841.6,    231879306.6,    419580274,  268211059,  276378232.1,    261739468.7,    429127062.8,    254776725.6,    329429882.8,    264012891.6,    496745973.9),ncol=2,byrow=FALSE)
</code></pre>

<p>the known outliers in this series are:</p>

<pre><code>outliers &lt;- matrix(c(""14Q4"",""14Q2"",""12Q1"",""13Q1"",""14Q2"",""11Q1"",""11Q4"",""14Q2"",""13Q4"",""14Q4"",""13Q1"",20193525.68,18319234.7,12896323.62,12718744.01,12353002.09,11936190.13,11356476.28,11351192.31,10101527.85,9723641.25,9643214.018),ncol=2,byrow=FALSE)
</code></pre>

<p>please do not say about seasonality as this is only one type of data set, i have many ones without seaonality and i need the code to work for both types.</p>

<p>Edit by javlacalle: This is a plot of the observed data and the time points defined in the first column of <code>outliers</code>.</p>

<p><img src=""http://i.stack.imgur.com/EL22o.png"" alt=""original data and outliers""></p>
"
"0.076847327936784","0.0691301129820284","214382","<p>If a predictor is negatively correlated with a variable you are trying to forecast in an Arima model, will Arima pick up the negative correlation when you add the predictor in the xreg argument?  Is there anything that needs to be done to the predictor when it is added in the xreg argument in order to indicate that it is negatively correlated with the variable you are trying to predict?</p>
"
"0.0443678254708057","0.059868434008925","58559","<p>I have just started to learn about forecasting. I thought it would be easy to create forecast models for a daily time series but have encountered a number of difficulties. Firstly most examples and available datasets are either in months or quarters. It is rare to find examples for weeks and days. Secondly it also appears difficult to create a timeseries object for days (365) and weeks (52) as these vary between years. This may just be the way the timeseries object works in R. I have had to use Zoo. I also have a concern that my data may not be properly modeled for use in packages like Forecast and HTS.</p>"
"NaN","NaN","<p>I am interested in how best to approach this problem. Any examples of forecasting to daily events that may cycle across years would be greatly appreciated.</p>",""
"NaN","NaN","","<r><time-series><forecasting>"
"0.0887356509416114","0.11973686801785","108925","<p>The following code shows a forecast of the next 24 hours of my electricity prices with two exogenous variables. 
My problem is, that I don't know how to build a forecast for the next 3 days or more because for example I have to take the first 24 hours into account when I want to predict the prices for the second day(25-48). And the time of my dummys and variables also have to grow in 24 hours steps. </p>

<p>I know that a loop is a solution but I  don't know how to create the loop. </p>

<p>I hope you understand my problem.  </p>

<p>My next problem is that I have to create a neural network with this data. Can someone give me a hint how to do this? </p>

<p>Thanks for your help =)</p>

<pre><code>tm1 &lt;- (25:6552)
arma.model =  auto.arima(price$Price[tm1],start.p=5,start.q=5,max.p=5,max.q=5, 
                      xreg=cbind(sol.prod$Production[tm1],wind.prod$Production[tm1],
                           price$Price[1:6528]),
                    trace=TRUE, stationary=TRUE)

arma.model 


PriceForecast = predict(object=arma.model,n.ahead=24, 
                    xreg=cbind(sol.prod$Production[tm1],wind.prod$Production[tm1],
                               price$Price[1:6528]),
                        newxreg=cbind(sol.prod$Production[6553:6576],wind.prod$Production[6553:6576],
                                  price$Price[6529:6552]))
</code></pre>
"
"0.125491161027632","0.169333502666921","126072","<p>I want to understand how forecast from STL function in R works. So, I am not giving any reproducible code here.</p>

<p>Below is the procedure that I worked on time series</p>

<ol>
<li><p>I used STL decomposition on my time series.</p></li>
<li><p>Checked residuals component from step 1 for white noise using Box.test</p></li>
<li>Found that residuals are not white-noise. So, used ARIMA model to fit a forecasting model.
Now, my task is to compute forecast values that consist of a. Seasonal and Trend component from step 1 above b. Residuals component from ARIMA model - from step 3 above.</li>
</ol>

<p>If I use</p>

<pre><code>forecast(stl(..)), 
</code></pre>

<p>it gives me</p>

<pre><code> Point Forecast     Lo 80    Hi 80    Lo 95    Hi 95 
</code></pre>

<p>However, I am interested in only seasonal and trend parts of forecast. How can I get seasonal trend components?</p>

<p>What components does constitute forecast(stl(..))</p>

<p>Please advise.</p>
"
"NaN","NaN","58647","<p>I have a non-stationary timeseries with a mean (Âµ) and standard deviation (SD) which both vary  across time. The distribution of the timeseries is skewed, so the left and right sides of the distribution must be treated separately.</p>

<p>I am wondering what is the best way to accurately track the 25% and 75% quantile?</p>

<p><strong>What I have tried</strong></p>

<p>I have tried calculating the 25% quantile within a rolling window, then feeding these values into the forecast() package to smooth them out. I'm wondering if there is a better way of doing this, because one of the constraints is that there is not much data: I only have about 50 samples within each window, any more and the SD and mean have changed so much that they are out of date.</p>
"
"0.172172452513037","0.232323648996817","58657","<p>I'm using a daily time series of sales data that contains about 2 years of daily data points. Based on some of the online-tutorials / examples I tried to identify the seasonality in the data. It seems that there is a weekly, monthly and probably a yearly periodicity / seasonality.</p>

<p>For example, there are paydays, particularly on 1st payday of the month effect that lasts for few days during the week. There are also some specific Holiday effects, clearly identifiable by noting the observations.</p>

<p>Equipped with some of these observations, I tried the following:</p>

<ol>
<li><p>ARIMA (with <code>Arima</code> and <code>auto.arima</code> from R-forecast package), using regressor (and other default values needed in the function).  The regressor I created is basically a matrix of 0/1 values:</p>

<ul>
<li>11 month (n-1) variables</li>
<li>12 holiday variables</li>
<li>Could not figure out the payday part...since it's little more complicated effect than I thought. The payday effect works differently, depending on the weekday of the 1st of month.</li>
</ul>

<p>I used 7 (i.e., weekly frequency) to model the time series. I tried the test - forecasting 7 days at a time. The results are reasonable: average accuracy for a forecast of 11 weeks comes to weekly avg RMSE to 5%.</p></li>
<li><p>TBATS model (from R-forecast package) - using multiple seasonality (7, 30.4375, 365.25) and obviously no regressor. The accuracy is surprisingly better than the ARIMA model at weekly avg RMSE 3.5% .</p>

<p>In this case, the model without ARMA errors perform slightly better. Now If I apply the coefficients for just the Holiday Effects from the ARIMA model described in #1, to the results of the TBATS model the weekly avg RMSE improves to 2.95%</p></li>
</ol>

<p>Now without having much background or knowledge on the underlying theories of these models, I'm in a dilemma whether this TBATS approach is even a valid one. Even though it's improving the RMSE significantly in the 11 weeks test, I'm wondering whether it can sustain this accuracy in the future. Or even if applying Holiday effects from ARIMA to the TBATS result is justifiable. Any thoughts from any / all the contributors will be highly appreciated. </p>

<p><a href=""https://s3.amazonaws.com/CKI-FILE-SHARE/TS+Test+Data.txt"">Link for Test Data</a></p>

<p>Note: Do ""Save Link As"", to download the file.</p>
"
"0.133103476412417","0.179605302026775","63681","<p>I have been adamantly searching the web to learn how to successfully implement a dynamic regression time series in the forecast package for R. The time series data that I am using is weekly data (frequency=52) of incoming call volume and prediction variables are mailers sent out every now and then. They are a significant predictor of the data for the week that they hit, the following week, and the week after that. I have created lagged variables and use these three as the predictors. </p>

<p>My main concern is that the arima model is not taking into account the time series frequency. When I tell it to recognize the ts with a frequency of 52 it has an error. 
I have looked at the <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">fortrain function</a> but do not understand it. I also have looked at the tbats suggested but found that those will not work with prediction variables. </p>

<p>The Zoo function recognizes 52 frequency but it is not advised to use with the <a href=""http://stackoverflow.com/questions/16050684/using-the-combination-forecastauto-arima"">forecast package</a>.</p>

<p>Here is the basic code. The problem is that the time series calwater[,5] is not recognized as such. It is imputed as a simple vector as an integer...</p>

<pre><code>#this works without taking into acount the ts
fit2 &lt;- auto.arima(calwater[6:96,5], xreg=calwater[6:96,6:8], d=0)
fccal &lt;- forecast(fit2, xreg=calwater[97:106,6:8], h=10)
fccal
plot(fccal, main=""Forecast Cal Water"", ylab=""Calls"")

#to form a ts object
calincall&lt;-ts(calwater[1:106,5],start=c(2011,23),frequency=52)

#once the ts is added to the model this dispalys
#Error in `[.default`(calincall, 2:100, 1) : incorrect number of dimensions
</code></pre>

<p>Maybe the error is because there is just a little over two years of data. </p>

<pre><code>#Time Series: Start = c(2011, 23), End = c(2013, 24),Frequency = 52 
</code></pre>

<p>I would be very grateful for any guidance in for this particular issue. I am using the forecast package and prefer to continue within the package but I am open to suggestions. </p>
"
"0.0313727902569079","0.0423333756667302","37908","<p>I am new to R. I am trying to apply forecasting model Time Series (TS) Model
as follows:  </p>

<ol>
<li>Plotting original data, </li>
<li>Simple Moving Average,  </li>
<li>Auto correction(AC), Partial AC, Differencing of TS etc to get stationary time series,  </li>
<li>Fitting optimal model which gives minimum AIC, residuals from ARIMA/ARMA  </li>
<li>Normality test for residuals  </li>
<li>forecasting for future values  </li>
</ol>

<p>The forecast figures are not coming out with the accuracy that I expected. Please find following weekly incidents. </p>

<p><strong>Can anyone please help me with the right approach and sample code?</strong></p>

<p>There are some outliers in the data (# of incidents per week) due to new release of application, seasonality effect and holiday period.  </p>

<pre><code>March 11, 2011/ March 25, 2011/ June 24, 2011/December 02, 2011/ December 30, 2011/ 
March 30, 2012/ April 20, 2012/


            Time_Stamp Wkly_Cnt
1    November 19, 2010        9
2    November 26, 2010       22
3    December 03, 2010       11
4    December 10, 2010       12
5    December 17, 2010       18
6    December 31, 2010       17
7     January 07, 2011       14
8     January 14, 2011       21
9     January 21, 2011       16
10    January 28, 2011       22
11   February 04, 2011       20
12   February 11, 2011       31
13   February 18, 2011       38
14   February 25, 2011       37
15      March 04, 2011       32
16      March 18, 2011       34
17      April 01, 2011       28
18      April 08, 2011       32
19      April 15, 2011       30
20      April 29, 2011       30
21        May 06, 2011       25
22        May 13, 2011       19
23        May 20, 2011       17
24        May 27, 2011       28
25       June 03, 2011       13
26       June 10, 2011       17
27       June 17, 2011       17
28       July 01, 2011       14
29       July 08, 2011       22
30       July 15, 2011       19
31       July 22, 2011       11
32       July 29, 2011       14
33     August 05, 2011       14
34     August 12, 2011       21
35     August 19, 2011       20
36     August 26, 2011       16
37  September 02, 2011       16
38  September 09, 2011       10
39  September 16, 2011       24
40  September 23, 2011       12
41  September 30, 2011       17
42    October 07, 2011       32
43    October 14, 2011       29
44    October 21, 2011       19
45    October 28, 2011       13
46   November 04, 2011       12
47   November 11, 2011       18
48   November 18, 2011       14
49   November 25, 2011       17
50   December 09, 2011       36
51   December 16, 2011       20
52   December 23, 2011       22
53    January 06, 2012       31
54    January 13, 2012       29
55    January 20, 2012       20
56    January 27, 2012       27
57   February 03, 2012       14
58   February 10, 2012       23
59   February 17, 2012       20
60   February 24, 2012       15
61      March 02, 2012       26
62      March 09, 2012       19
63      March 16, 2012       25
64      March 23, 2012       26
65      April 06, 2012       12
66      April 13, 2012       20
67      April 27, 2012       20
68        May 04, 2012       16
69        May 11, 2012       17
70        May 18, 2012       17
71        May 25, 2012       20
72       June 01, 2012       14
73       June 08, 2012       23
74       June 15, 2012       21
75       June 22, 2012       22
76       June 29, 2012       19
</code></pre>
"
"0.0887356509416114","0.0898026510133874","148489","<p>Once outliers in time series are detected in R how exactly are they dealt with before forecasting? </p>

<p>I dont want commands to use i would like the method.</p>

<p>Please do not give any answers to do with other programs, i only want help with R or clear statistical help. </p>

<p>I have been asked for code and data, however everytime i have included data people have focused on it and pushed seasonality. I just want an answer about how usual outliers in time-series are dealt with in R once they are found.</p>

<p>Here is some data if it does help. But please do not focus just on seasonality.</p>

<pre><code>data &lt;- matrix(c(""08Q1"",    ""08Q2"", ""08Q3"", ""08Q4"", ""09Q1"", ""09Q2"", ""09Q3"", ""09Q4"", ""10Q1"", ""10Q2"", ""10Q3"", ""10Q4"", ""11Q1"", ""11Q2"", ""11Q3"", ""11Q4"", ""12Q1"", ""12Q2"", ""12Q3"", ""12Q4"", ""13Q1"", ""13Q2"", ""13Q3"", ""13Q4"", ""14Q1"", ""14Q2"", ""14Q3"", ""14Q4"", 42276697.62,    47076334.07,    63256116.91,    65590858.1, 31894750.53,    44051289.07,    43240372.01,    78599947.68,    53580697.84,    58358231.42,    62449936.27,    105183185.7,    75073116.79,    76785853.65,    85386730.46,    120665293,  75261413.75,    79689757.7, 84155668.45,    145212040,  91050406.6, 85376291.1, 98682147.97,    128723263.1,    68248467.5, 119022600.7,    71264936.93,    144658584.9),ncol=2,byrow=FALSE)
</code></pre>

<p>Here are also the known outliers (big orders) from this series</p>

<pre><code>outliers &lt;- matrix(c(""11Q1"",""13Q1"",""13Q3"",""10Q1"",""13Q1"",""12Q4"",""11Q2"",""12Q4"",""08Q1"",""08Q3"",""13Q1"",11936190.13,7698187.164,7263059.321,7015590.095,5345533.2,4842687.605,4356023.999,3968599.17,3917142.48,3813230.87,3538187.716),ncol=2,byrow=FALSE)
</code></pre>
"
"0.193637065371959","0.209029813778565","209790","<h2>Background</h2>

<p>I'm working on a project which aims to use the history data about a water flux to detect whether there is a leakage happened. The data is hourly collected and among about 4 months.  </p>

<p>I've already read the book which Professor Hyndman write about the forecast and some posts about outliers/anomaly detection on the site, but still I get confused how to realize this in R. In the meantime, I think I've got things mixed up and want to know the basic procedure to accomplish it.</p>

<h2>What I've tried</h2>

<p>At first, I think the basic idea is to fit a model on my train data and forecast it with the test part. Then use the model to check the residual in the whole data whether they are all normal distributed or at least has zero mean.  </p>

<p>So according to <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r?lq=1"">Timeseries analysis procedure and methods using R</a>, I've tried ARIMA, Exponential Soomthing and TBATS, but the result isn't ideal. And I'm also afraid that this could lead to a flaw since I didn't consider the outliers and anomaly.<br>
Here is my code</p>

<pre><code>model &lt;- list(
   mod_arima &lt;- auto.arima(train_h, ic = ""aic""),
   mod_exp &lt;- ets(train_h, ic = ""aic""),
   mod_tbats &lt;- tbats(train_h,ic = ""aic"")
)
forecasts &lt;- lapply(model, forecast, h = 24)
par(mfrow = c(2,2));
for (i in forecasts) {plot(i); lines(test_h,col = ""red"")}
</code></pre>

<p><a href=""http://i.stack.imgur.com/a80fL.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/a80fL.jpg"" alt=""enter image description here""></a> </p>

<p>Then according to <a href=""http://stats.stackexchange.com/questions/1142/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series"">Simple algorithm for online outlier detection of a generic time series</a>, I find I could detect those single point that in my data through the answer by professor Hyndman, but I fail to change to detect the small level shift. (I've tried to create a 0.05*mean shift level, removing the outliers, then using the tso to detech the level shift, however it fail totally...)</p>

<h2>My Problems</h2>

<p>My problems mainly falls in the following two parts:  </p>

<ol>
<li><p>Even though it seems that there is a relativity between the flux and the flux an hour ago(Looking from the plot), could I use the hourly data directly to fit a model or should I first select the data at the same time each day to fit a model each?  </p>

<p><a href=""http://i.stack.imgur.com/dhPFL.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dhPFL.jpg"" alt=""The plot of the relation between the data &amp;the data an hour ago""></a>  </p></li>
<li><p>Now I think my problem could be partly solved by directly detecting the level shift in the data, but I think that the leakage in the flux data should be relatively small if any(maybe just 5%,10% of the mean). While I've mannually create a shift in a try, when I use the tsoutliers::tso in R directly, the result isn't ideal. Is this idea right or should I fit a model still? And how could I detect such a small change in the level shift in a time series, particularly in R?   </p></li>
</ol>

<p>ps:Since I'm new to Cross Validated, I fail to find a way to upload the data may be easy for you solve my problem, is there any advice?</p>
"
"0.0793675790132545","0.0803219328902499","64383","<p>I have a time series data (1 minute and sometimes 5 minute data) data I would like use <code>forecasting</code> package to forecast couple hours ahead.</p>

<p>Here is my data:</p>

<pre><code>dput(head(p,20))
structure(list(time = structure(c(1373889420, 1373889480, 1373889540, 
1373889600, 1373889660, 1373889720, 1373889780, 1373889840, 1373889900, 
1373889960, 1373890020, 1373890080, 1373890140, 1373890200, 1373890260, 
1373890320, 1373890380, 1373890440, 1373890500, 1373890560), class = c(""POSIXct"", 
""POSIXt""), tzone = ""America/New_York""), cpu = c(2.25892, 2.04144, 
5.04823333333333, 4.9947, 1.72982857142857, 4.82655, 3.6168625, 
4.7357, 2.42683333333333, 3.62635, 5.02315714285714, 2.57147142857143, 
7.16005, 2.34253333333333, 2.82315714285714, 5.17668, 2.2899375, 
6.92, 5.172375, 4.63735), name = c(""servers"", ""servers"", ""servers"", 
""servers"", ""servers"", ""servers"", ""servers"", ""servers"", ""servers"", 
""servers"", ""servers"", ""servers"", ""servers"", ""servers"", ""servers"", 
""servers"", ""servers"", ""servers"", ""servers"", ""servers"")), .Names = c(""time"", 
""cpu"", ""name""), row.names = c(1116L, 1411L, 123L, 226L, 1014L, 
435L, 538L, 569L, 1081L, 342L, 74L, 865L, 178L, 890L, 281L, 166L, 
1035L, 143L, 112L, 91L), class = ""data.frame"")

x.xts &lt;- xts(p$cpu, p$time)
x.ts &lt;- as.ts(x.xts)
x.ets &lt;- ets(x.ts)
x.fore &lt;- forecast(x.ets, h=120)
f&lt;-data.frame(x.fore$mean)
    DateTime&lt;-tail(z,1)$time
f$DATE &lt;- DateTime + 60 * (seq_len(nrow(f))-1)
    colnames(f)&lt;-c(""cpu"", ""time"")
    f$name&lt;-c(""forecast"")
</code></pre>

<p>I see that <code>cpu</code> is the same for all future data times:</p>

<pre><code> cpu                time     name
1 6.020207 2013-07-15 11:57:00 forecast
2 6.020207 2013-07-15 11:58:00 forecast
3 6.020207 2013-07-15 11:59:00 forecast
4 6.020207 2013-07-15 12:00:00 forecast
5 6.020207 2013-07-15 12:01:00 forecast
6 6.020207 2013-07-15 12:02:00 forecast
</code></pre>

<p>Is there a better forecasting model besides <code>ets</code> for time series data?</p>
"
"0.076847327936784","0.0691301129820284","148820","<p>I am working on an alogorithm in R to automatize a monthly forecast calculation. I am using, among others, the forecast(method='arima') function from the forecast package to calculate forecast. It is working very well. But for some times series some forecast are quite strange.</p>

<p>Please find below the code i'm using:</p>

<p><code>train_ts&lt;- ts(values, frequency=12)
fit1 &lt;- stl(train_ts, s.window=""periodic"",t.window=24, )
arima &lt;- forecast(fit1,h=forecasthorizon,method ='arima')</code></p>

<p><code>values &lt;- c(27, 27, 7, 24, 39, 40, 24, 45, 36, 37, 31, 47, 16, 24, 6, 21, 35, 36, 21, 40, 32, 33, 27, 42, 14, 21, 5,   19, 31, 32, 19, 36, 29, 29, 24, 42, 15, 24, 21)</code></p>

<p>Here, on the graph, you will see the historical data (black), the fitted value (green) and the forecast(blue). The forecast is not in lines with the fitted value.</p>

<p><img src=""http://i.stack.imgur.com/5530d.png"" alt=""enter image description here"">
As you can see the Forecast is not in line with the history,
My question is ""does a setup for Arima to bound the forecast in line with the history exist"" ?</p>
"
"0.0992094737665681","0.133869888150416","64621","<p>I am new to time series modeling in R. I have sales data of one year and three months only. I am trying to do sales forecasting at the day level or max at the week level. Following is the step I intend to follow</p>

<ol>
<li>Convert it into time series object using <code>ts(data$qty, frequency= ??)</code>. Here I am very confused about frequency. I can see in data that there is some seasonality like sales is picking up in May, June, July and then again in festival seasons. I guess I cannot use 365 as I have only one year data. Please suggest what should be the frequency</li>
<li>Decompose the time series. Subtract the seasonality and trend from the actual time series model </li>
<li>Fit ARIMA to get a prediction</li>
<li>Again add seasonality and trend to output the final forecast</li>
</ol>

<p>Please provide feedback on this if its correct approach or not or if there is any other better way to handle it.</p>
"
"0.142293601635375","0.208006656319505","221881","<p>This question is sort of a follow up of <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r"">this</a> great thread.</p>

<p>I have a Time Series Analysis project in which I have to create a model to predict new values given historical univariate data using R. My plan is to run several models, assess their accuracies and choose the best one to use in production.</p>

<p>I must admit that I'm a beginner in TS analysis, so I'm not sure about the procedure that I must take. The main question is: Should I do a seasonal adjust in the data before fitting it into models?</p>

<p>I've done some simulation myself and the results are quite interesting:</p>

<h2>Raw data (without adjusting)</h2>

<pre><code>library(seasonal)
library(forecast)
data(""AirPassengers"")

training &lt;- window(AirPassengers, end = c(1959, 12))
test &lt;- window(AirPassengers, start = c(1960, 1))

models &lt;- list(
  mod_arima = auto.arima(training, ic='aicc', stepwise=FALSE),
  mod_etrainingp = ets(training, ic='aicc', restrict=FALSE),
  mod_neural = nnetar(training, p=12, size=25),
  mod_tbats = tbats(training, ic='aicc', seasonal.periods=12),
  mod_bats = bats(training, ic='aicc', seasonal.periods=12),
  mod_stl = stlm(training, s.window=12, ic='aicc', robust=TRUE, method='ets'),
  mod_sts = StructTS(training)
)

forecasts &lt;- lapply(models, forecast, 12)

acc &lt;- lapply(forecasts, function(f){
  accuracy(f, test)[2,,drop=FALSE]
})
acc &lt;- Reduce(rbind, acc)
row.names(acc) &lt;- names(forecasts)
(acc &lt;- round(acc, 2))
                   ME   RMSE    MAE   MPE  MAPE MASE ACF1 Theil's U
mod_arima      -15.72  22.96  17.91 -3.68  4.05 0.59 0.08      0.51
mod_etrainingp   4.99  19.01  14.40  0.75  3.03 0.47 0.27      0.41
mod_neural      12.97  26.08  23.38  2.55  4.94 0.77 0.16      0.55
mod_tbats      -15.49  25.67  18.20 -3.71  4.14 0.60 0.17      0.58
mod_bats         0.69  23.12  16.48 -0.35  3.45 0.54 0.38      0.50
mod_stl         31.35  57.93  39.95  5.31  7.43 1.31 0.65      1.01
mod_sts        177.41 199.41 177.41 36.02 36.02 5.83 0.77      3.76
</code></pre>

<h2>Adjusted data</h2>

<pre><code>adj &lt;- seas(AirPassengers)
adj &lt;- final(adj)

adj_training &lt;- window(adj, end = c(1959, 12))
adj_test &lt;- window(adj, start = c(1960, 1))

models_adj &lt;- list(
  mod_arima = auto.arima(adj_training, ic='aicc', stepwise=FALSE),
  mod_etrainingp = ets(adj_training, ic='aicc', restrict=FALSE),
  mod_neural = nnetar(adj_training, p=12, size=25),
  mod_tbats = tbats(adj_training, ic='aicc', seasonal.periods=12),
  mod_bats = bats(adj_training, ic='aicc', seasonal.periods=12),
  mod_stl = stlm(adj_training, s.window=12, ic='aicc', robust=TRUE, method='ets'),
  mod_sts = StructTS(adj_training)
)

forecasts_adj &lt;- lapply(models_adj, forecast, 12)

acc_adj &lt;- lapply(forecasts_adj, function(f) accuracy(f, adj_test)[2,,drop=FALSE])
acc_adj &lt;- Reduce(rbind, acc_adj)
row.names(acc_adj) &lt;- names(forecasts_adj)
(acc_adj &lt;- round(acc_adj, 2))
                   ME  RMSE   MAE   MPE MAPE MASE ACF1 Theil's U
mod_arima        6.02 10.05  9.49  1.22 1.99 0.31 0.64      1.26
mod_etrainingp  -2.76  7.12  4.43 -0.62 0.96 0.15 0.40      0.90
mod_neural     -30.84 33.27 30.84 -6.47 6.47 1.02 0.56      4.11
mod_tbats      -15.38 16.60 15.38 -3.26 3.26 0.51 0.05      2.09
mod_bats        -7.13 10.41  7.31 -1.55 1.58 0.24 0.43      1.34
mod_stl         -1.77  7.01  4.67 -0.41 1.01 0.15 0.39      0.89
mod_sts         -9.59 11.05  9.59 -2.04 2.04 0.32 0.11      1.38
</code></pre>

<p>Now, a comparison table to highlight the differences:</p>

<pre><code>abs(acc_adj) &lt; abs(acc)
                  ME  RMSE   MAE   MPE  MAPE  MASE  ACF1 Theil's U
mod_arima       TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE     FALSE
mod_etrainingp  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE     FALSE
mod_neural     FALSE FALSE FALSE FALSE FALSE FALSE FALSE     FALSE
mod_tbats       TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE     FALSE
mod_bats       FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE     FALSE
mod_stl         TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE      TRUE
mod_sts         TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE      TRUE
as.data.frame(rowMeans(abs(acc_adj) &lt; abs(acc)))
               rowMeans(abs(acc_adj) &lt; abs(acc))
mod_arima                                  0.750
mod_etrainingp                             0.750
mod_neural                                 0.000
mod_tbats                                  0.875
mod_bats                                   0.500
mod_stl                                    1.000
mod_sts                                    1.000
</code></pre>

<p>The only model that has better accuracy in raw data is <code>mod_neural</code>, created with neural networks.</p>

<p>So, what should I learn from this experiment? Does it make sense to use X13ARIMA-SEATS to adjust a series before fitting models? </p>
"
"0.117386232408469","0.158396987770497","135419","<p>I have tried forecasting in R using ets(). I let ets choose the best model for my data. The problem is i observed that eventhough the data shows an increasing trend and exhibits seasonality, ets is giving MNN model while MAM gave best results(i have chosen MAM after seeing the graph of the time series). ets selects a model based on low AIC,right? why is it selecting MNN when MAM is giving relatively low AIC value.So kindly list a procedure to forecast future values for a timeseries whose seasonality, trend are not known before hand i.e. automation of the forecast procedure.</p>

<p><strong>New Edit:</strong>
my data is given below:</p>

<pre><code>date,value
01/08/2012,262830
01/09/2012,4849602
01/10/2012,6341298
01/11/2012,6814589
01/12/2012,9494411
01/01/2013,10559931
01/02/2013,12113638
01/03/2013,15668512
01/04/2013,933441
01/05/2013,2701218
01/06/2013,4332092
01/07/2013,7537763
01/08/2013,8485541
01/09/2013,10171206
01/10/2013,11501464
01/11/2013,11464229
01/12/2013,16046044
01/01/2014,16881837
01/02/2014,17942038
01/03/2014,22527927
01/04/2014,944640
01/05/2014,3246315
01/06/2014,5796971
01/07/2014,8759231
</code></pre>

<p>I used frequency=12 in ts object creation. i used na.approx to interpolate values for missing dates if any. Then i used ets with model=""ZZZ"" and damped=NULL. ets has chosen MNN model but the data has increasing trend and also exhibits seasonality. Shouldn't it choose MAM?
Here is the graph of input and outptut</p>

<p><img src=""http://i.stack.imgur.com/ZUQl5.png"" alt=""enter image description here""></p>

<p>data in orange is given data</p>

<p>adding the code here: ('modval' is  obtained after interpolation is applied to 'value' in case of missing dates)</p>

<pre><code>myts&lt;-ts(modval,frequency=12)
fit&lt;-ets(myts,model=""ZZZ"",damped=NULL)
result&lt;-forecast(fit,h=12,level=95)
resultframe&lt;-as.data.frame(result)
pointforecasts&lt;-resultframe[,1]
lowerboundofPI&lt;-testframe[,2]
upperboundofPI&lt;-testframe[,3]
</code></pre>
"
"NaN","NaN","125909","<p>I am trying to predict values using arima(0,1,1).
After doing <code>predict(mod,n.ahead=5)</code> (in <code>R</code>) am getting the same value for all the predictions: </p>

<pre><code>5947.681 5947.681 5947.681 5947.681 5947.681 
</code></pre>

<p>Is it correct?</p>
"
"0.076847327936784","0.103695169473043","86248","<p>I've been attempting to forecast natural gas power demand and how it is affected by temperature and price. I'm not sure if I have done everything correctly (relatively new to R), but I do seem to get relevant data other than I can't seem to change my forecast period, nor am I sure this is an appropriate model for this data. Hopefully someone can provide me with some guidance.</p>

<p>Data: <a href=""https://www.dropbox.com/s/g9uytz3guyjrbq2/demand.csv"" rel=""nofollow"">demand.csv</a></p>

<pre><code>library(forecast)
data = read.csv(""demand.csv"")

# Create matrix of numeric predictors
xreg &lt;- cbind(weather=data$Weather,price=data$Price,m1=data$M1,
    m2=data$M2,m3=data$M3,m4=data$M4,m5=data$M5,m6=data$M6,
m7=data$M7,m8=data$M8,m9=data$M9,m10=data$M10,m11=data$M11)

# Rename columns
colnames(xreg) &lt;- c(""Weather"",""Price"",""Jan"",""Feb"",""Mar"",""Apr"",
""May"",""Jun"",""Jul"",""Aug"",""Sep"",""Oct"",""Nov"")

# Variable to be modelled
demandTS &lt;- ts(data$Demand, frequency=12)

# Find ARIMAX model
demandArima &lt;- auto.arima(demandTS, xreg=xreg)
demand.fcast &lt;- forecast(demandArima, xreg=xreg)
plot(demand.fcast)
</code></pre>

<p>Thank you for any help.</p>

<p>References:</p>

<p><a href=""http://stats.stackexchange.com/questions/41070/how-to-setup-xreg-argument-in-auto-arima-in-r"">How to setup xreg argument in auto ARIMA in R</a>
<a href=""http://stackoverflow.com/questions/10606295/from-auto-arima-to-forecast-in-r"">From auto ARIMA to forecast in R</a></p>
"
"0.147151429850636","0.198561132393757","187870","<p>I'm working on a sales forecasting package which should be easy to use for the end user. Given a time series with historical sales data I would like to automatically select one of the three forecasts: Auto.Arima, ETS and STLF. 
The idea is to split historical data into 80% train set and 20% test (holdout) set. Then run Auto.Arima, ETS, STLF and choose the one that has best MAPE on the test set. </p>

<p>Now comes the part that is not entirely clear to me. Once I figured out that e.g. ETS gives me the best result should I now </p>

<ol>
<li>Retrain ETS on the entire set of historical data and generate
forecast using this new model? My reservation here is that after I
run ETS again it may even change the class of the algorithm as well
as the fit parameters which will render the MAPE I got on the test
set irrelevant.  </li>
<li>Just generate the forecast using the model that was trained on the
80% train set? My problem with this approach is that we are ignoring
the last 20% of data which is probably the most important
information for the forecast.</li>
<li>The third idea is to use the same model fit parameters that we got
after training the model on the 80% train set. But then use the
entire set of data for        forecasting. This seems like a
reasonable approach but I cannot figure out how to do it for ETS and
STL (For Arima we can do it by supplying the original fit as the model
parameter of the arima function)</li>
</ol>

<p>Could you please let me know what is the right way to approach this problem?</p>
"
"0.133103476412417","0.159649157357133","86280","<p>I am using R for time-series analysis and predictions, the package 'forecast' to be more precise. I am in a dilemma. I have hourly data that needs a prediction and needs to be analysed. I am using the STLF function, since I set the frequency to 24 (because it's <a href=""http://robjhyndman.com/hyndsight/forecast3/"" rel=""nofollow"">greater than 13</a>). But, when I make the forecasts for the next 6 hours, with a data set containing 300 points, I get the following forecast:</p>

<pre><code>Point          Forecast Lo 80    Hi 80    Lo 95    Hi 95
13.50000       29.60251 21.28421 37.92081 16.88077 42.32425
13.54167       27.84124 18.89136 36.79111 14.15358 41.52889
13.58333       30.89487 21.33420 40.45554 16.27309 45.51665
13.62500       36.04991 25.89498 46.20484 20.51928 51.58053
13.66667       40.40386 29.66798 51.13975 23.98474 56.82298
13.70833       41.13250 29.82644 52.43856 23.84138 58.42362
</code></pre>

<p>As you can see, the next points are 13.5, 13.54, ... etc. This is like this probably because the data set is containing 300 points, and 300/24 = 12.5, 301/24 = 12.54167, ... etc, and assuming that the first point is considered to be 1 and not 0, so there you have the way the points are provided. </p>

<p>My question is: will I get better results if I adjust the seasonality in a way that will give me forecasts for each hour? I.e. if the next point is 12, then 13, then 14, ... etc, up until 23 and then to start from 0 (24 hours span). If yes, please tell me how to adjust the seasonality to my data? Is there a way for making even more complicated seasonalities? (say, if the data is taken every 5 minutes or so).</p>

<p>Thank you in advance for your answer.</p>
"
"0.0443678254708057","0.059868434008925","86354","<p>I'm looking to join the field of statistics and more exactly to forecasting. I'm a software developer and I just started playing with R.</p>

<p>Can you recommend me some tutorials related to forecasting, but something which beginners can handle. It will be great also to read and try some forecasting related problems in which the input data needs some cleaning. </p>
"
"0.0887356509416114","0.11973686801785","173610","<p>I create an ARIMA model for my <code>ts</code>-object. My data is available in seconds or even miliseconds. I didn't find a way to specify the time information for the start- and end-parameters when creating the <code>ts</code> object?</p>

<p>I need the exact time, because I want to extract the time information when I do the forecast based on the ARIMA model to return the exact times for my forecasted values. It would be enough to store the end-time information somewhere in my ARIMA model, so that I can use it later when I do the forecast.</p>

<p>How is this done usually with ARIMA models?</p>

<p>Thanks!</p>
"
"NaN","NaN","174112","<p>I would like to predict stocks of a company for 6 months. I would like to use neural networks for this prediction. </p>"
"NaN","NaN","<ol>",""
"NaN","NaN","<li>Can anyone suggest how many hidden layers and hidden nodes to be used? </li>",""
"NaN","NaN","</ol>",""
"NaN","NaN","<p>I have modeled a neural network with 3 input nodes and 2 hidden nodes and 1 output node in R but I am getting forecast accuracy of around 35% only. </p>",""
"NaN","NaN","<ol start=2>",""
"NaN","NaN","<li>Can anyone suggest how I can improve the forecast accuracy of my model? </li>",""
"NaN","NaN","<li>Are there any other models which give better accuracy for this kind of data?</li>",""
"NaN","NaN","</ol>",""
"NaN","NaN","","<r><time-series><neural-networks><forecasting>"
"0.076847327936784","0.103695169473043","135011","<p>I'm having trouble finding a time series technique to deal with a data set I am working on. It contains multiple subjects and multiple variables, not all of which will likely be part of the time series. It looks something like this:</p>

<pre><code>Subject  Date      T1  T2  V1  V2  V3
A        1/1/2012  1   5   9   13  17
A        2/1/2012  2   6   10  14  18
...
B        1/1/2012  3   7   11  15  19
B        2/1/2012  4   8   12  16  20
...
</code></pre>

<p>Where T1, T2 are likely time series, and V1, V2, and V3 are likely not. I'm sure that this distinction is probably unnecessary, since techniques like Box-Jenkins should detect autoregression in any variable.</p>

<p>Ultimately, I want to be able to do forecasting on other subjects that were probably not used to build this model.</p>

<p>If you know of any R package(s) that can take this on, please let me know. Some example code would also be greatly appreciated. Thank you for any insight you can provide.</p>

<p>Edit: I am looking into dynamic linear regression using the <code>dynlm</code> package, but am having trouble coding it to include the dates and subjects.</p>
"
"0.159970469715827","0.149440644416015","173111","<p>There are two posts on CV about differentiation between short-term and long-term forecasting. e.g. <a href=""http://stats.stackexchange.com/questions/124843/what-is-the-distinction-between-short-term-and-long-term-forecasting"">here</a>. There is nothing on CV about combining (incorporating) long-term dynamics into short term forecasting. Is this irrelevant?</p>

<p>I wonder if someone can show/explain what is the statistical procedure of incorporating long term time series dynamics into short term forecasting? </p>

<p>I would like to forecast short-term by incorporating the long term: trend etc. Quite often is the case that short term forecasting is insufficient (short sighted) as in number of cases the time series converges to long term mean. (I understand this assumption is data dependent, but this is type of data I'm dealing with). </p>

<p>Is also the model choice in this case important? So the question is how to combine these two (short/long-term dynamics)? </p>

<p><strong>Actual example (fitted model) within <code>R</code> would be desired result. For nice answer I'm offering double the current bounty.</strong>   </p>

<p>EDIT: I'm going to disappoint in term of data and provide data from the <code>forecast</code> R package, since I think (for my purpose) it is going to be sufficient to answer this question. </p>

<p>Demonstration in R:</p>

<pre><code>library(forecast)
data(gold)   

plot(gold)
abline(h=mean(gold, na.rm=T), col=""orange3"", lwd=2)
lines(fitted(lm(gold[1:800]~index(gold[1:800]))), col=""blue3"", lwd=2)
abline(v=800, col=""red3"", lty=2, lwd=3)
</code></pre>

<p>I fitted number of models ""plainly"" to the data <code>(gold[1:800])</code> and all models would blindly forecast the upward trend. Here is example (picture nr. 2):</p>

<pre><code>plot(forecast(auto.arima(gold[1:800]), h=200))
</code></pre>

<p><a href=""http://i.stack.imgur.com/erUB3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/erUB3.png"" alt=""enter image description here""></a></p>

<p>We divide the sample data up to obs <code>800</code>. (what we would observe-""in-sample""). In light of the ""long-term"" mean value (approx. mean 390) and the last observation value just bellow <code>500</code>, I would like to forecast up to step 1000. (<code>index(gold)</code>) taking into account the <code>long-term mean value</code> and hence incorporate it into forecast. </p>

<p>Since here we are dealing with steps rather than time, imagine that steps 1:800 is ""long-term"" (in years) and from <code>800&gt;</code> the steps are of small scale, let say ""hours"". (possible the time representation doesn't matter). </p>

<p>So starting with forecast from the observation <code>800</code> forecast next 200 steps by taking into account the ""long-term"" mean value. </p>
"
"0.125491161027632","0.148166814833556","189050","<p>I have a time series of number of visitors of an website for two years. I have to do the forecast of the number of visitors for the next semestre. To do so, I used three forecast models: ARIMA, auto_ARIMA and Holt Winters.</p>

<p>My problem is how to choose the most approprite model based on these accuracy function outputs:</p>

<p>accuracy function output for arima model:</p>

<p><a href=""http://i.stack.imgur.com/5TLKH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5TLKH.png"" alt=""enter image description here""></a></p>

<p>accuracy function output for auto-arima model:</p>

<p><a href=""http://i.stack.imgur.com/yFcPs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yFcPs.png"" alt=""enter image description here""></a></p>

<p>accuracy function output for Holt Winters model:</p>

<p><a href=""http://i.stack.imgur.com/QdffX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QdffX.png"" alt=""enter image description here""></a></p>

<p>Here's the graph of the initial time series:</p>

<p><a href=""http://i.stack.imgur.com/xtJ1J.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xtJ1J.png"" alt=""enter image description here""></a></p>
"
"0.0887356509416114","0.11973686801785","87726","<p>In the Arima package, using a Box-Cox transformation give wrong results when later applied to the forecast method.</p>

<p>For example, consider this data:</p>

<pre><code>library(forecast)
data&lt;-c(2,3,2,3,2,3)
</code></pre>

<p>And for the sake of simplicity, consider an ARIMA(0,0,0) model. (The mean of this series is 2.5.)</p>

<p>The mean forecast made without a Box-Cox transformation is correct:</p>

<pre><code>forecast(Arima(data,order=c(0,0,0)))$mean
 [1] 2.5 2.5 2.5 2.5 2.5 2.5 2.5 2.5 2.5 2.5
</code></pre>

<p>However if we use a Box-Cox transformation, such as a log transformation with lambda=0, the ""mean"" forecast is wrong:</p>

<pre><code>forecast(Arima(data,order=c(0,0,0), lambda=0))$mean
[1] 2.44949 2.44949 2.44949 2.44949 2.44949 2.44949 2.44949 2.44949 2.44949 2.44949
</code></pre>

<p>It seems that to produce the mean forecast of Y=exp(X), it does E(Y)=exp(E(X)).</p>

<p>Is there a way to correct this?
Is there a package with a correct implementation of forecasts with Box-Cox transformations?</p>
"
"NaN","NaN","88074","<p>Is the package fpp (or any of its previous incarnations like forecast) supported in Ubuntu 12.04 using AWS? It is the only package that R downloads but when you load the library it throws an error. </p>"
"NaN","NaN","<p>Here is the error</p>",""
"NaN","NaN","<pre><code>Error in library(forecast) : there is no package called â€˜forecastâ€™",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","","<r><time-series><forecasting>"
"0.125491161027632","0.169333502666921","188597","<p>I have daily data for 3 years. This sales data is of seasonal nature as business has spikes and downfall by month. Also, sales differ by each day of the week. for example, monday in general in a month tend to have similar pattern.</p>

<p>I have used ARIMA and created a matrix of month dummy variables and day of week dummy variables and have passed that in ARIMA. however i hit the bottom when i couldn't reconvert differenced stationary number forecasts into the actual sales metric. <a href=""http://stats.stackexchange.com/questions/188595/convert-double-differenced-forecast-into-actual-value"">Posted here already</a></p>

<p>I have also tried dummy regression using sales as dependent variable and 11 month dummy variables and 6 day of week dummy variables. i abandoned this as R square was low at 48% and MAPE from the forecasted results was more than 20%</p>

<p>Edit: I have tried auto.arima as well.
My question: What technique can i use for forecasting sales for next 365 days? that will consider this month of the year and day of the week seasonality?</p>
"
"0.0887356509416114","0.11973686801785","92177","<p>I'm doing a project related to identifying sales dynamics. My database contains 26 weeks after launching the product (so 26 time-series observations equally spaced in time). </p>

<p><img src=""http://i.stack.imgur.com/Dquwy.jpg"" alt=""http://imageshack.com/a/img18/5628/l5qg.jpg""></p>

<p><img src=""http://i.stack.imgur.com/8Dh2C.jpg"" alt=""http://imageshack.com/a/img34/8953/yh6i.jpg""></p>

<p>I used two methods of time-series clustering to see which patterns dominate in different groups (clustering by <code>units_sold_that_week</code>). The first method is based on k-medoids and the second one connected with clustering by parameters of growth models.</p>

<p>My next step is to make forecasts based on these clusters. Is there any special method for forecasting based on time-series clusters? In my project, I have to combine the topic of clustering and forecasting on clusters.</p>

<p>I am running my analyses in R, so I would be grateful for any suggestions regarding R procedures.</p>

<p>Please note that I am relatively new to time series analysis so any clarity you could provide, on R or any package you could recommend that would help accomplish this task efficiently, would be appreciated.</p>
"
"0.0887356509416114","0.11973686801785","43804","<p>I tried to fit <code>auto.arima()</code> with a <code>ts</code> data. But it is not giving the right forecast. For many it is coming as <code>arima(0,1,0)</code> model which is not good at all. Can I fit a GARCH model to the original series in this case? How do you get fitted and forecasted values of original data using <code>garch(1,1,)</code> or some other model? I tried to use code for GARCH but it is not giving the fitted and forecast of original values.</p>
"
"0.0992094737665681","0.133869888150416","94519","<p>Considering that we want to use optimize() on the interval [0,1] how can I write an R code for finding the value of Î² that produces the minimum forecast error without using external packages like <code>forecast</code>?</p>

<p><img src=""http://i.stack.imgur.com/DE1Hn.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/gb7Bh.png"" alt=""enter image description here""></p>

<p>For simplicity assume that:
<img src=""http://i.stack.imgur.com/B9c2A.png"" alt=""enter image description here""></p>

<p>I want to use the following package:</p>

<pre><code>&gt; require(datasets)
&gt; str(nhtemp)
 Time-Series [1:60] from 1912 to 1971: 49.9 52.3 49.4 51.1 49.4 47.9 49.8 50.9 49.3 51.9 ...
</code></pre>

<p>in which <code>nhtemp</code> is the <code>Yearly Average Temperatures in New Haven CT</code>.</p>
"
"0.0443678254708057","0.059868434008925","70317","<p>We use turbidity to estimate suspended-sediment concentration (SSC)- our data was serially correlated. We ran an ARMA process and ended up with a AR (2) model. Our equation in log form is:</p>

<p>estlogi(SSC) = 1.35149 + 0.785099logi(Turbidity) - 0.00095 + 0.7492ei-1+ 0.2052ei-2</p>

<p>How can we input the ei-1 &amp; ei-2 values in real time, that is, how do we enter residuals for current or future values when our turbidity/SSC data pairs are in the past (2010 to 2011). Someone did suggest an R script that will estimate K steps ahead for a specified ARMA model and past data- does anyone know this script?</p>
"
"0.0887356509416114","0.0898026510133874","92935","<p>I'm making a project connected with identifying the dynamics of sales. My database concerns 26 weeks (so equally in 26 time-series observations) after launching the product.</p>

<p>This is what my database looks like: <a href=""https://imageshack.com/i/0yyh6ij"" rel=""nofollow"">https://imageshack.com/i/0yyh6ij</a> </p>

<p>I want to make forecast based on S-curve for clusters of time-series. The main aim was to compare two methods of forecasting:</p>

<ol>
<li>based on parameters of logistic curve</li>
<li>based on ARIMA</li>
</ol>

<p>However, I do not know how to compare these two methods = measure their performance.</p>

<p>That's a plot with prediction based on S-curve</p>

<p><a href=""http://imageshack.com/a/img850/6600/rzkp.jpg"" rel=""nofollow"">http://imageshack.com/a/img850/6600/rzkp.jpg</a></p>

<p>So my questions are:</p>

<ol>
<li>How to measure performance=forecast errors based on logistic curve?</li>
<li>How to compare forecasting based on logistic curve and ARIMA - what is the main difference between these two approaches if I base on one variable - units_sold_that_week?</li>
</ol>

<p>I would be grateful for any explanation.</p>
"
"0.147151429850636","0.198561132393757","189983","<p>I have daily data from last 2 years.</p>

<p>I want to do ARIMAX and the regressor component being autoregressive distributed lag of the same variable. Since it has impact, along with dummy variables to account for seasonality in the <code>xreg</code> paratemer in <code>auto.arima</code> function.</p>

<p>The challenge i am facing is predicting my predictor for future. For example, i used daily data for 2 year for model building. For forecasting into future, i also need values of lag variable, which i do not know. If i use 2 lags of daily data in the model, then in order to predict for future i will also need value of those lag variables as well. So to predict $Value$ at time $t$ i will need $Value$ at $t-1$ and $t-2$ which i have from past records. However, if i want to find value at $t+5$ then i will need to find $t+3$ and $t+4$. Not sure how to proceed in this direction. As stated earlier, i am using <code>auto.arima</code> function from <code>forecast</code> package in <code>R</code> . </p>

<p>My ultimate goal is to predict for next 365 days. What i assume to be a solution is that i predict for $t+1$ as it will require $t$ and $t-1$ as lag component which i already have. once done i can use this predicted $t+1$ component to predict for $t+2$ as i will know value of $t+1$ from previous iteration and $t$ from original values. Is it the right approach?</p>
"
"0.0443678254708057","0.059868434008925","214945","<p>I have read that you should use log transformations when the fluctuations on your data are increasing over time, but what do you do if the fluctuations level out over time?</p>

<p>A plot of the time series(1) and a log transform(2) are shown below:</p>

<p><a href=""http://i.stack.imgur.com/Y46PV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Y46PV.png"" alt=""Time Seires Data""></a></p>

<p><a href=""http://i.stack.imgur.com/e7WFL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/e7WFL.png"" alt=""Log Transform of Data""></a></p>
"
"0.133103476412417","0.139693012687492","44617","<p>I'm trying to measure the impact that rainfall causes in the number of incoming calls in a insurance-company. I have 4 years of daily data.</p>

<p>The plots below shown the correlation plot for each year:
<img src=""http://i.stack.imgur.com/KTdSX.png"" alt=""enter image description here""></p>

<p>The same plots as above, but now taken the weekly-mean for each variable:
<img src=""http://i.stack.imgur.com/0iZnQ.png"" alt=""enter image description here""></p>

<p>The rainfall has a <strong>yearly</strong> seasonality and the call-center data has a <strong>weekly pattern</strong>. The idea is to come up with a <em>weekly-based model</em>, so that i can measure the impact that a <em>weekly mean rainfall forecast</em> will cause.</p>

<p>Plotting the whole dataset, taken weekly-means (image below):
<img src=""http://i.stack.imgur.com/CrSb9.png"" alt=""enter image description here""></p>

<p>I'd like some suggestions on how to measure this variable 'impact' - i can try to split the rainfall data into 3 categories (low, normal, high), then build some model.</p>

<p>Thanks for any help! (i'm using R for analysis).</p>
"
"0.0443678254708057","0.059868434008925","44984","<p>I'm beginner in R, Could you please explain how to use ses in forecast package of R <a href=""http://cran.r-project.org/web/packages/forecast/forecast.pdf"" rel=""nofollow"">forecast</a>?
I'd like to choose the number of initial periods and smoothing constant.</p>

<pre><code>d &lt;- c(3,4,41,10,9,86,56,20,18,36,24,59,82,51,31,29,13,7,26,19,20,103,141,145,24,99,40,51,72,58,94,78,11,15,17,53,44,34,12,15,32,14,15,26,75,110,56,43,19,17,33,26,40,42,18,24,69,18,18,25,86,106,104,35,43,12,4,20,16,8)
</code></pre>

<p>I have 70 periods, I'd like to use 40 Periods for initial and 30 for out-of sample.</p>

<pre><code>ses(d, h=30, level=c(80,95), fan=FALSE,initial=c(""simple""), alpha=.1)
</code></pre>

<p>Is it correct?</p>
"
"0.147151429850636","0.180510120357961","191120","<p>I am a beginner in time series analysis and I would like discuss a couple of numerical examples here implemented in R. I am reading some interesting books, but I also need some expert advice to get started.
The time series are</p>

<pre><code>ts1&lt;-structure(c(196, 196, 178, 165, 155, 138, 131, 132, 135, 146, 
160, 173, 180, 186, 180, 163, 132, 129, 134, 146, 159, 157, 161, 
179, 209, 225, 228, 196, 151, 144, 145, 157, 168, 161, 162, 176, 
205, 219, 219, 190, 147, 142, 146, 160, 175, 169, 171, 188, 220, 
235, 236, 202, 154, 146, 145, 155, 168, 158, 156, 168, 190, 202, 
204, 177, 135, 127, 125, 133, 145, 139, 143, 160, 190, 205, 200, 
160, 119, 113, 118, 129, 142, 135, 133, 142, 159, 171, 177, 164, 
135, 130, 130, 139, 152, 149, 152, 168, 195, 209, 211, 180, 138, 
134, 139, 152, 165, 158, 157, 168, 192, 207, 219, 206, 169, 164, 
161, 172, 182, 180, 182, 196, 218, 223, 229, 230, 196, 197, 200, 
209, 222, 219, 207, 210, 209, 221, 234, 224, 225, 221, 235, 216, 
224, 229, 229, 214, 230, 240, 243, 222, 189, 221, 217, 189, 197, 
194, 195, 202, 197, 224, 204, 218, 212, 191, 217, 215, 183, 186, 
191, 166, 177, 194, 180, 159, 158, 147, 166, 184, 159, 159, 187, 
194, 196, 204, 213, 236, 210, 218, 251, 227, 251, 214, 245, 209, 
215, 242, 196, 237, 212, 171, 206, 200, 204, 192, 185, 182, 194, 
242, 199, 200, 191, 172, 179, 165, 173, 198, 214, 197, 175, 227, 
197, 202, 205, 212, 216, 223, 222, 201, 217, 209, 239, 241, 251, 
225, 212, 210, 241, 223, 238, 226, 242, 228, 257, 248, 264, 229, 
223, 255, 251, 231, 254, 235, 246, 246, 243, 254, 256, 261, 254, 
247, 249, 243, 257, 228, 272), na.action = structure(c(1L, 2L, 
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 
30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 
43L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 55L, 
56L, 57L, 58L, 59L, 60L, 61L, 62L, 63L, 64L, 65L, 66L, 67L, 68L, 
69L, 70L, 71L, 72L, 73L, 74L, 75L, 76L, 77L, 78L, 79L, 80L, 81L, 
82L, 83L, 84L, 85L, 86L, 87L, 88L, 89L, 90L, 91L, 92L, 93L, 94L, 
95L, 96L, 97L, 98L, 99L, 100L, 101L, 102L, 103L, 104L, 105L, 
106L, 107L, 108L, 109L, 110L, 111L, 112L, 113L, 114L, 115L, 116L, 
117L, 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 
128L, 129L, 130L, 131L, 132L, 396L), class = ""omit""), .Tsp = c(1994, 
2015.83333333333, 12), class = ""ts"")

ts2&lt;-structure(c(3756, 3867, 3686, 3490, 3446, 3357, 3421, 3447,3321, 
3198, 3331, 3360, 3312, 3270, 3251, 3213, 2937, 3152, 3022, 2931, 
2697, 2626, 2775, 3030, 3067, 3349, 3225, 3175, 3061, 3089, 3166, 
3193, 3035, 2901, 2932, 2981, 3242, 3268, 3084, 2902, 2790, 2695, 
2756, 2649, 2627, 2643, 2554, 2638, 2783, 2660, 2618, 2383, 2319, 
2415, 2434, 2427, 2164, 2114, 2246, 2224, 2552, 2390, 2213, 2130, 
2274, 2140, 2317, 2191, 2086, 2112, 2134, 2153, 2401, 2450, 2273, 
2154, 2140, 2201, 2156, 2078, 2110, 2101, 2075, 2043, 2305, 2266, 
2227, 2134, 2002, 2008, 1945, 2110, 2045, 2017, 2106, 1913, 2068, 
2209, 2025, 2033, 1892, 1934, 1914, 1818, 1808, 
1851, 1939),na.action   = structure(c(1L, 
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 
29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 
42L, 43L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 
55L, 56L, 57L, 58L, 59L, 60L, 61L, 62L, 63L, 64L, 65L, 66L, 67L, 
68L, 69L, 70L, 71L, 72L, 73L, 74L, 75L, 76L, 77L, 78L, 79L, 80L, 
81L, 82L, 83L, 84L, 85L, 86L, 87L, 88L, 89L, 90L, 91L, 92L, 93L, 
94L, 95L, 96L, 97L, 98L, 99L, 100L, 101L, 102L, 103L, 104L, 105L, 
106L, 107L, 108L, 109L, 110L, 111L, 112L, 113L, 114L, 115L, 116L, 
117L, 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 
128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L, 
139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L, 
150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L, 
161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 
172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L, 
183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L, 
194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L, 
205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L, 
216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L, 
227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L, 
238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L, 
249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L, 
260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L, 
271L, 272L, 273L, 274L, 275L, 276L, 277L, 278L, 279L, 280L, 281L, 
282L, 283L, 284L, 285L, 286L, 287L, 288L, 396L),
class = ""omit""),.Tsp   = c(2007, 
2015.83333333333, 12), class = ""ts"")
</code></pre>

<p>I would prefer to avoid the use of auto.arima from the (excellent) forecast package, or at least not to use it as a black box.
I started looking at the plots of the first differences</p>

<pre><code>plot(diff(ts1))
</code></pre>

<p><a href=""http://i.stack.imgur.com/5EVBz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5EVBz.png"" alt=""enter image description here""></a></p>

<pre><code>plot(diff(ts2))
</code></pre>

<p><a href=""http://i.stack.imgur.com/ZWNea.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZWNea.png"" alt=""enter image description here""></a></p>

<p>which should remove any trend. I also looked at the decomposition: </p>

<pre><code>plot(decompose(ts1))
</code></pre>

<p><a href=""http://i.stack.imgur.com/VpDyN.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VpDyN.png"" alt=""enter image description here""></a></p>

<pre><code>plot(decompose(ts2))
</code></pre>

<p><a href=""http://i.stack.imgur.com/M3lkU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/M3lkU.png"" alt=""enter image description here""></a></p>

<p>I would tend to conclude that in both cases there is a seasonality in the data. 
However, diff(ts2) appears (to me, by eye) to yield a stationary process with constant variance, whereas diff(ts1) does not seem to have a constant variance. I tried diff(diff(ts1)) and diff(log(ts2)), but I am puzzled by what I see.
If I look at  </p>

<pre><code> acf(ts1)
</code></pre>

<p><a href=""http://i.stack.imgur.com/qPbtI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qPbtI.png"" alt=""enter image description here""></a></p>

<pre><code> acf(ts2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/TlNto.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TlNto.png"" alt=""enter image description here""></a></p>

<p>I see that in both cases the autocorrelation decays slowly and when I resort to</p>

<pre><code>acf(diff(ts1))
</code></pre>

<p><a href=""http://i.stack.imgur.com/Q62OT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Q62OT.png"" alt=""enter image description here""></a></p>

<pre><code>acf(diff(ts2))
</code></pre>

<p><a href=""http://i.stack.imgur.com/nxycs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nxycs.png"" alt=""enter image description here""></a></p>

<p>I see some spikes which I do not know how to interpret.
Essentially, I am at a loss about how to link these findings with a SARIMA model.
Any suggestion on either/both time series is very appreciated!</p>
"
"0.076847327936784","0.103695169473043","101372","<p>I am kind of confused with what I should actually do with predicted volatility values that I obtained via a ARCH/GARCH model other than feeling happy that I know when volatility rises/falls. Is there a way that I can incorporate the predicted volatility values via a GARCH/ARCH model into a prediction model for my actual time series or is what I am saying erroneous? I use R as my primary tool.</p>"
"NaN","NaN","","<r><time-series><forecasting><garch>"
"0.0313727902569079","0.0423333756667302","113138","<p>cty year qtr tl</p>

<p>Argentina 2009 Q4 3</p>

<p>Argentina 2010 Q1 2</p>

<p>Argentina 2010 Q2 7</p>

<p>Argentina 2010 Q3 7</p>

<p>Argentina 2010 Q4 10</p>

<p>Argentina 2011 Q1 7</p>

<p>Argentina 2011 Q2 7</p>

<p>Argentina 2011 Q3 1</p>

<p>Argentina 2011 Q4 7</p>

<p>Argentina 2012 Q1 5</p>

<p>The data set has around 40 countries with each country having data for 5 years and four quarters. cty=country, year=Year, qtr Quarter, var=count of people. As I am new to R, can someone assist me with how to prepare the dataset for time series and fit a ARIMA/ARMA models etc. followed by the forecast in R? Some basic codes to prepare the data, to perform analysis and forecast for this series as an example would be helpful.</p>
"
"0.076847327936784","0.0691301129820284","214379","<p>I'm working on an Arima model to forecast a given variable and so I'm looking in my data for variables with correlation to the variable I'm trying to predict, to add as predictors in the xreg argument.  I've found several that have correlation between 0.1 and 0.3.  I was wondering is there a way to combine predictors with lower correlation to a variable to create a predictor with higher correlation to a variable?</p>
"
"0.108678533400333","0.122205929184461","113490","<p>I have a time series (shown below) that comes from a sensor whose calibration was changed in the middle of last year.  As part of this change, the sensor's reading of the variance (or volatility) of the observations increased, although actually, this volatility did occur before the change in calibration, but it was not being fully detected.</p>

<p>As the data the sensor detects is highly seasonal (at both the weekly and yearly level), I want to create an accurate forecast of what the future readings will be, using the new level of calibration.  I don't want to throw out the data from before the calibration, as we have only collected a little more than a year's worth of data.</p>

<p>I've tried traditional transformations (like Box-Cox) and a seasonal decomposition using <code>stl()</code> in R's <code>forecast</code> package, but the transformation does not solve this variance issue, as there's different variances at the same level of the data (say at 0.0, for example).</p>

<p>Are there any other techniques are there (that are available in the <code>R</code> programming language) that allow me to transform the data from before mid-2013 to match the variance of the data that comes later? </p>

<p><img src=""http://i.stack.imgur.com/WXBSg.png"" alt=""time series""></p>
"
"0.126012956047636","0.123663705536045","208985","<p>I'm wondering if a rolling forecast technique like the ones mentioned in Rob Hyndman's blogs, and the example below, could be used to select the order for an ARIMA model?</p>

<p>In the examples I've looked at, like the ones below, it seems like the order of the ARIMA model is already specified, or is determined once by auto.arima and then the single model is evaluated using the forloop in the rolling forecast. </p>

<p>I'm wondering how you could use the rolling forecast technique to select the order of the ARIMA model.  If anyone has a suggestion or example, that would be great.</p>

<p>Examples:
<a href=""http://robjhyndman.com/hyndsight/tscvexample/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/tscvexample/</a>
<a href=""http://robjhyndman.com/hyndsight/rolling-forecasts/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/rolling-forecasts/</a></p>

<p>Code:</p>

<pre><code>library(""fpp"")

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, model=fit)
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}
</code></pre>

<p><strong>Update:</strong></p>

<p>Pseudo code:</p>

<pre><code>library(""fpp"")

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1

##Create models for all combinations of p 10 to 0, d 2 to 0, q 10 to 0

fit1 &lt;- Arima(train, order=c(10,2,10)
fit2 &lt;- Arima(train, order=c(9,2,10)
fit3 &lt;- Arima(train, order=c(8,2,10)
.
.
.
fit10 &lt;- Arima(train, order=c(0,2,10)
fc1 &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
fc2 &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
fc3 &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
.
.
.
fc10 &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit1 &lt;- Arima(x, model=fit1)
  refit2 &lt;- Arima(x, model=fit2)
  refit3 &lt;- Arima(x, model=fit3)
  .
  .
  .
  refit10 &lt;- Arima(x, model=fit10)
  fc1[i] &lt;- forecast(refit1, h=h)$mean[h]
	  fc2[i] &lt;- forecast(refit2, h=h)$mean[h]
  fc3[i] &lt;- forecast(refit3, h=h)$mean[h]
	  .
	  .
	  .
	  fc10[i] &lt;- forecast(refit10, h=h)$mean[h]
}

##Calculating mape for forecasts

Accuracy(fc1$mean,test)[,5]
	Accuracy(fc2$mean,test)[,5]
Accuracy(fc3$mean,test)[,5]
	.
	.
	.
	Accuracy(fc10$mean,test)[,5]

##Return the order of the Arima model that has the lowest mape 
</code></pre>
"
"0.147151429850636","0.180510120357961","72244","<p>I must be doing something very wrong here, as auto.arima in R is completely dying, but I can't see what it is.  I have the latest version of forecast and R and I think this happens on both Windows and Unix.  It works for some/most time series I tried (equities) but fails for others.  It seems to fail more often when I diff the high/low with the previous day's close as opposed to just diffing the closes as below.  Is this a bug or am I somehow giving arima bad data? (and causing it to die with a horrible error message)  I tried searching for this error but didn't come up with much.  Thanks a lot.</p>

<pre><code>library(tseries)
library(forecast)
dwa &lt;- get.hist.quote(instrument=""DWA"", start=""2010-01-01"", end=""2013-10-31"")
logreturns &lt;- diff(log(dwa$Close))*100 
fit &lt;- auto.arima(logreturns, trace=TRUE)
</code></pre>

<p>Output:</p>

<pre><code>This is forecast 4.8 

trying URL 'http://chart.yahoo.com/table.csv?s=DWA&amp;a=0&amp;b=01&amp;c=2010&amp;d=9&amp;e=31&amp;f=2013&amp;g=d&amp;q=q&amp;y=0&amp;z=DWA&amp;x=.csv'
Content type 'text/csv' length unknown
opened URL
.......... .......... .......... .......... ....
downloaded 44 Kb

time series starts 2010-01-04
time series ends   2013-10-07

 ARIMA(2,1,2) with drift         : 1e+20 *
 ARIMA(0,1,0) with drift         : 1e+20 *
 ARIMA(1,1,0) with drift         : 1e+20 *
 ARIMA(0,1,1) with drift         : 1e+20 *
 ARIMA(1,1,2) with drift         : 1e+20 *
 ARIMA(3,1,2) with drift         : 1e+20 *
 ARIMA(2,1,1) with drift         : 1e+20 *
 ARIMA(2,1,3) with drift         : 1e+20 *
 ARIMA(1,1,1) with drift         : 1e+20 *
 ARIMA(3,1,3) with drift         : 1e+20 *
 ARIMA(2,1,2)          : 1e+20 *Error in if (diffs == 1 &amp; constant) { : argument is of length zero
Calls: auto.arima -&gt; myarima
In addition: Warning messages:
1: In if (is.constant(x)) { :
  the condition has length &gt; 1 and only the first element will be used
2: In if (is.constant(x)) return(d) :
  the condition has length &gt; 1 and only the first element will be used
3: In if (is.constant(dx)) { :
  the condition has length &gt; 1 and only the first element will be used
Execution halted
</code></pre>
"
"NaN","NaN","115642","<p>I wanted to fit an ARIMA model to a daily database for three years but <code>auto.arima</code> couldn't find a model and showed the following error:</p>"
"NaN","NaN","<pre><code>Unable to fit final model using maximum likelihood. AIC value approximated",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>Is it because ARIMA is not good for data with long seasonality?</p>",""
"NaN","NaN","","<r><time-series><forecasting><error-message>"
"NaN","NaN","46246","<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days."
"NaN","NaN","If data.ts is my time series then I would like to use something like</p>",""
"NaN","NaN","<pre><code>tslm(data.ts~season|businesss.dummy)",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>Thus I want to model season given that the dummy for this hour is True or False.",""
"NaN","NaN","I don't want</p>",""
"NaN","NaN","<pre><code>tslm(data.ts~season + businesss.dummy)",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>as this would just give a parallel shift on business days.",""
"NaN","NaN","I know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?",""
"NaN","NaN","Thanks!</p>",""
"NaN","NaN","","<r><regression><time-series><forecasting>"
"0.133103476412417","0.159649157357133","114675","<p>I really want to understand how the math is working here. I am trying to get the standard error of the fitted values for a time series regression model. In the non-time series regression, I know I can take the transpose of the data multiplied by the variance - covariance matrix of the model coefficients and then multiply by the data values again to get the standard errors of the fitted values.</p>

<p>But I'm not sure how to do this when I am including an autoregressive term.</p>

<pre><code>require(forecast)
require(tserieS)
</code></pre>

<p>Response variable</p>

<pre><code>Sablects &lt;- rnorm(10)
</code></pre>

<p>Covariates</p>

<pre><code>my.xreg &lt;- cbind(rnorm(10),rbinom(10,1,0.5))
</code></pre>

<p>In my actual data, values are normalized so I set the intercept equal to zero here.</p>

<pre><code>m4&lt;-arima(Sablects, order=c(2,0,0),fixed=c(0,NA,0,NA,NA),xreg=my.xreg) 
</code></pre>

<p>The predict function will give me standard errors on my in-sample prediction (the fitted values of my model).</p>

<pre><code>my.se &lt;- predict(m4, newxreg = my.xreg, n.ahead = 10)$se         

my.se
</code></pre>

<p>Now to compare the output of my.se, I want to do this mathematically but I don't know what to use for the values of the ar2 term. I use 1's as a placeholder to demonstrate that my output does not equal the values from <code>my.se</code> above</p>

<pre><code>C &lt;- cbind(rep(1, nrow(my.xreg)), my.xreg[, 1], my.xreg[, 2])

C
</code></pre>

<p>I think this value should equal the first value in my.se, but is not producing the same value as my.se</p>

<pre><code>sqrt(t(C[1, ]) %*% vcov(m4) %*% C[1, ])
</code></pre>

<p>Also, I'm not so great with matrix multiplication but here is my work around for getting all of the se values.</p>

<pre><code>se.output &lt;- matrix(nrow=nrow(C))
</code></pre>

<p>Specify that the max number of i is equal to number of rows of <code>C</code>.</p>

<pre><code>  for(i in 1:nrow(C)){

    # Loop through your multiplication for each row (i) of `C`. For each iteration, save the new data into the new row of se.output

    se.output[i] &lt;- sqrt(t(C[i, ]) %*% vcov(m4) %*% C[i, ])  
    }

se.output
</code></pre>
"
"0.140303383316578","0.170388550274119","115506","<p>Forecasting airline passengers seasonal time series using auto arima</p>

<p>Hi, I am trying to model some airline data in an attempt to provide an accurate monthly forecast for June-December this year using monthly data from January 2003 onwards.  The data is taken from: <a href=""http://www.transtats.bts.gov/Data_Elements.aspx?Data=1"" rel=""nofollow"">http://www.transtats.bts.gov/Data_Elements.aspx?Data=1</a></p>

<p>Here is the time series plot and ACF</p>

<p><a href=""http://imgur.com/EGh40pR"" rel=""nofollow""><img src=""http://i.imgur.com/EGh40pR.jpg"" title=""Hosted by imgur.com""/></a> </p>

<p><a href=""http://imgur.com/BJy78dn"" rel=""nofollow""><img src=""http://i.imgur.com/BJy78dn.jpg"" title=""Hosted by imgur.com""/></a></p>

<p>I have used auto.arima to develop two models and checked that they correspond to the autocorrelation functions.  Basically I am having trouble deciding whether to use:</p>

<ol>
<li>The following seasonal ARIMA model</li>
</ol>

<p><a href=""http://imgur.com/0k2Q8I4"" rel=""nofollow""><img src=""http://i.imgur.com/0k2Q8I4.jpg"" title=""Hosted by imgur.com""/></a></p>

<ol start=""2"">
<li><p>The following non-seasonal ARIMA model of $N_t$ after I first decomposed the model into a trend, seasonal component and random component $X_t = T_t +S_t +N_t $ using a 12-point moving average (basically did the same thing as the <code>decompose()</code> function manually)</p>

<p><a href=""http://imgur.com/r4TkpxX"" rel=""nofollow""><img src=""http://i.imgur.com/r4TkpxX.jpg"" title=""Hosted by imgur.com""/></a></p></li>
</ol>

<p>I have analysed the important properties of both models such as ensuring residuals are close to a white noise process and so on but am unsure which of the above 2 models is most suitable for forecasting purposes and why?</p>

<p>Also I am unsure how to compute forecast for the trend component vector if I use the classical decomposition model $X_t = T_t + S_t +N_t$.  Is it even possible to create forecasts using this type of model?</p>

<p>Edit:
Here is the output of <code>dput(IAP)</code> (the raw data without trend or seasonal component removed)</p>

<blockquote>
  <p>dput(IAP)
  structure(c(9726436L, 8283372L, 9538653L, 8309305L, 8801873L, 
  10347900L, 11705206L, 11799672L, 9454647L, 9608358L, 9481886L, 
  10512547L, 10252443L, 9310317L, 10976440L, 10802022L, 10971254L, 
  12159514L, 13502913L, 13203566L, 10570682L, 10772177L, 10174320L, 
  11244427L, 11387275L, 9945067L, 12479643L, 11521174L, 12164600L, 
  13140061L, 14421209L, 13703334L, 11325800L, 11107586L, 10580099L, 
  11812574L, 11724098L, 10167275L, 12707241L, 12619137L, 12610793L, 
  13690835L, 14912621L, 14171796L, 12010922L, 11517228L, 11222687L, 
  12385958L, 12072442L, 10590281L, 13246293L, 12795517L, 12978086L, 
  14170877L, 15470687L, 15120200L, 12321953L, 12381689L, 12004268L, 
  13098697L, 12767516L, 11648482L, 14194753L, 12961165L, 13602014L, 
  14413771L, 15449821L, 15327739L, 11731364L, 11921490L, 11256163L, 
  12463351L, 12075267L, 10412676L, 12508793L, 12629805L, 11806548L, 
  13199636L, 14953615L, 14844821L, 11659775L, 11905529L, 11093714L, 
  12659154L, 12393439L, 10694165L, 13279320L, 12398700L, 13380664L, 
  14406776L, 16026852L, 15317926L, 12599149L, 12874707L, 11651314L, 
  12915663L, 12668763L, 10944610L, 13473705L, 13537152L, 13935132L, 
  14814672L, 16623674L, 15753387L, 13220884L, 13185627L, 12144742L, 
  13546071L, 13206682L, 11732944L, 14387677L, 13995377L, 14291285L, 
  15582335L, 16969590L, 16621336L, 13791714L, 13397785L, 12762536L, 
  14096567L, 13766673L, 12023339L, 15177069L, 14278932L, 15306328L, 
  16232176L, 17645538L, 17517022L, 14239561L, 14209627L, 13133257L, 
  15083929L, 14589637L, 12385546L, 15486317L, 14857685L, 15615732L
  ), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>

<p>Here is the output of <code>dput(IAP.res)</code> (the random component from the decomposition)</p>

<blockquote>
  <p>dput(IAP.res)
  structure(c(NA, NA, NA, NA, NA, NA, -669127.347569446, -168943.285069446, 
  225871.456597222, 271337.106597223, 711896.11076389, 284583.435763889, 
  165401.360763887, 622993.194097221, -268299.21423611, -9406.73506944434, 
  -233904.910069446, -147124.755902779, -260973.055902776, -163628.243402778, 
  -43056.7100694457, 121365.814930555, 205106.485763889, -107464.272569445, 
  247575.569097221, 279399.444097225, 309270.160763888, -166333.068402778, 
  129823.798263889, 22571.1190972265, -113455.59756944, -384199.160069444, 
  62061.8315972222, -155858.226736111, 13600.0274305546, -87564.1475694429, 
  71845.7357638887, 8145.86076388881, 47627.494097226, 442212.72326389, 
  73639.5065972234, 60882.5774305568, -135204.389236112, -437744.576736112, 
  203832.581597222, -264145.435069444, 179945.61076389, 15812.1024305553, 
  -49648.0975694434, -61460.8059027772, 89656.3690972241, 118205.931597224, 
  -84196.4517361106, 4197.78576389072, -134118.722569442, -87234.4517361117, 
  -126555.418402776, -57714.9350694417, 293250.152430556, 59462.6857638892, 
  10340.8190972245, 416646.652430557, 526459.702430556, -135041.068402776, 
  239767.631597222, 67034.9940972247, -221066.180902774, 207611.839930556, 
  -424486.00173611, -94779.3517361115, 89796.4857638886, 130285.644097223, 
  104776.152430555, 16099.8607638888, -317097.047569448, 335867.264930556, 
  -796342.285069446, -446777.464236111, -93681.7225694442, 242962.798263888, 
  -143380.293402778, 135423.439930556, 28934.7357638923, 186390.185763891, 
  116969.777430558, -113617.264236109, -39733.9225694438, -471572.526736109, 
  130389.423263891, 80446.7857638926, 298895.444097222, 38486.7982638846, 
  143712.123263886, 419260.898263889, -113385.347569445, -181233.730902779, 
  -178686.680902779, -412733.597569445, -380106.797569444, 172783.973263888, 
  220863.173263891, 11443.2440972247, 392297.319097224, -62825.8267361117, 
  176278.664930556, 139372.439930556, -174159.88923611, -111755.439236109, 
  -206233.264236111, -197431.097569445, -55065.5892361099, 48314.3065972236, 
  -6745.32673610683, 193492.494097225, 155009.569097224, 241747.214930556, 
  209670.99826389, -173438.47673611, -101510.63923611, -128948.689236113, 
  -222773.597569443, -498474.472569441, 146856.619097224, -275463.026736109, 
  386273.214930557, 213400.994097223, 171865.11076389, 464391.381597217, 
  1489.99826388643, -9918.39340277936, -362009.847569447, NA, NA, 
  NA, NA, NA, NA), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>
"
"0.133103476412417","0.159649157357133","47185","<p>I am impressed by the R <code>forecast</code> package, as well as e.g. the <code>zoo</code> package for irregular time series and interpolation of missing values.</p>

<p>My application is in the area of call center traffic forecasting, so data on weekends is (nearly) always missing, which can be nicely handled by <code>zoo</code>. Also, some discrete points may be missing, I just use R's <code>NA</code> for that.</p>

<p>The thing is: all the nice magic of the forecast package, such as <code>eta()</code>, <code>auto.arima()</code> etc, seem to expect plain <code>ts</code> objects, i.e. equispaced time series not containing any missing data. I think real world applications for equispaced-only time series are definitely existent, but - to my opinion -  v e r y  limited.</p>

<p>The problem of a few discrete <code>NA</code> values can easily be solved by using any of the offered interpolation functions in <code>zoo</code> as well as by <code>forecast::interp</code>. After that, I run the forecast.  </p>

<p>My questions:  </p>

<ol>
<li>Does anyone suggest a better solution?</li>
<li><p><strong>(my main question)</strong> At least in my application domain, call center traffic forecasting (and as far as I can imagine most other problem domains), time series are not equispaced. At least we have recurring ""business days"" scheme or something. What's the best way to handle that and still use all the cool magic of the forecast package?  </p>

<p>Should I just ""compress"" the time series to fill the weekends, do the forecast, and then ""inflate"" the data again to re-insert NA values in the weekends? (That would be a shame, I think?)  </p>

<p>Are there any plans to make the forecast package fully compatible with irregular time series packages like zoo or its? If yes, when and if no, why not?  </p></li>
</ol>

<p>I'm quite new to forecasting (and statistics in general), so I might overlook something important.</p>
"
"NaN","NaN","174591","<p>Given a VAR model for the second differences of a vector time series, $\Delta^2 y$, how to obtain the one-step-ahead (and possibly $h$-step-ahead) prediction intervals for the series in levels, $y$? </p>

<p>Any suggestion or resources regarding how to implement this, e.g. in R, would also be appreciated.</p>
"
"0.140303383316578","0.189320611415688","96867","<p>This question builds on my previous question <a href=""http://stats.stackexchange.com/questions/96027/forecasting-hourly-time-series-based-on-previous-weeks-and-same-period-in-previo"">Forecasting Hourly Time Series based on previous weeks and same period in previous year/s</a>. My project is to forecast the number of ~400 different types of events expected in each hourly interval with enough accuracy for staffing decisions to be made.</p>

<p>Based on my knowledge of the data I know that each interval is related to the same hour band from the previous few weeks and the same time in the previous few years. Thanks to a comment by Rob Hyndman I am now using <code>tbats()</code> to forecast with mixed results. When comparing the forecasted data to the actual data the monthly totals are consistently within 1-3%.</p>

<p>However, when I compare the forecast to the actual for individual intervals the results are not very reliable at all. I have calculated the difference between the actual and the forecast as a percentage of the forecast for each interval and get an interquartile range of 50% to 150% with a mean difference of ~70%. This level of accuracy is unacceptable for what I need to use the data to do.</p>

<p>I am pretty certain that there are correlations between the frequency of different types of events and some measurable environmental factors. Is there an easy way to feed R a time series for the count of each event type as well as some environmental factors and have it find the correlations and create a forecast?</p>

<p>I am not trying to be lazy, the forecast tool is going to be automated and needs to be able to run without human input.</p>

<p>The method I am currently using is:</p>

<pre><code>data &lt;- scan(""data.csv"")
fcast &lt;- forecast(tbats(msts(data, seasonal.periods=c(168,8766))),1464)
</code></pre>

<p><em>The csv is a single column containing an hourly count of a specific event type over 2 years.</em></p>
"
"0.0627455805138159","0.0846667513334603","72950","<p>I am attempting to build a model to forecast attendance in a given week in the current year based on this year's attendance values up until the present, and data from two previous years. My data looks like this:</p>

<pre><code>   Week 11-12 Cumulative ADA    12-13 Cumulative ADA    13-14 Cumulative ADA
   1    0.9941                  0.9941                  0.9914
   2    0.9907                  0.991                   0.989
   3    0.9888                  0.9888                  0.9879
   4    0.9877                  0.987                   0.9869
   5    0.9869                  0.9865                  0.9867
   6    0.9862                  0.985                   0.9859
   7    0.9856                  0.9842                  0.9857
   8    0.9856                  0.984                   NA
   9    0.9852                  0.9839                  NA
   10   0.9848                  0.9834                  NA
</code></pre>

<p>Any guidance on how to predict the three NAs based on the past two years data and this year's values would be much appreciated.</p>

<p>Thanks!</p>
"
"0.159970469715827","0.199254192554687","48253","<p>I have a time series with daily observations over the course of multiple years (interest in topic ""superbowl"" over time). The seasonality in the data is yearly as well and it is very spiky (almost nothing all year and big increase/spike in January/February). I have started using R for this task (<code>forecast</code> package) and have little experience with statistics.</p>

<pre><code>x &lt;- ts(myts, frequency=365)
fit &lt;- HoltWinters(x)
plot(forecast(fit))
</code></pre>

<p>This works great and captures the seasonality of the data.</p>

<p>Now, I have read more about exponential smoothing (at <a href=""http://otexts.com/fpp/7/"" rel=""nofollow"">http://otexts.com/fpp/7/</a>) and understood that the HoltWinters model is one instance of the state space models implemented in ets. Unfortunately, I could not use ets so far since it complains about the high data frequency. I definitely need daily forecast (on the order of 30-60 steps).</p>

<pre><code>fit &lt;- ets(x, 'AAA')
Error in ets(x, ""AAA"") : Frequency too high
</code></pre>

<p>Why can HoltWinters deal with this but not ets? Is there a good workaround? I have the same problem for seasonal ARIMA models and considered splitting up the data in years and using past years as exogenous input.</p>

<p>On a side note: How do you usually deal with leap days that screw up your 365 day period? Simply delete them?</p>

<p>Thank you very much!</p>

<p>PS: I am aware of this: <a href=""http://robjhyndman.com/researchtips/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/researchtips/longseasonality/</a>
However, I couldn't get it too work well on my data, yet. On the other hand, HoltWinters worked fairly well.</p>

<hr>

<p>Thanks for all the helpful comments and discussion. 
I uploaded the data at <a href=""http://timalthoff.de/data/data.zip"" rel=""nofollow"">http://timalthoff.de/data/data.zip</a> 
The plot below shows Super_bowl.dat.</p>

<p>I took the liberty of including more time series if you'd like to check out more examples.</p>

<p>At certain points in time I want to forecast the time series on the order of 60 days. These points in time usually are on the left flank of a big spike that represents a sudden interest in a topic. See example.png for an example (the vertical red lines are these points in time to start an out-of-sample forecast). For more info check out the README.</p>

<p><img src=""http://i.stack.imgur.com/iKy6A.jpg"" alt=""enter image description here""></p>
"
"0.0627455805138159","0.0846667513334603","167944","<p>I have the following time series of <em>count data</em>:</p>

<pre><code>x &lt;- ts(c(21337, 56994, 95497, 138829, 146346, 157182, 128136,
          104615, 103659, 102082, 109968, 113945, 118067, 93867, 54930))
</code></pre>

<p>To which I have associated the following model</p>

<pre><code>&gt; library(forecast)
...
&gt; ets(x)
ETS(A,N,N) 

Call:
 ets(y = x) 

  Smoothing parameters:
    alpha = 0.9999 

  Initial states:
    l = 105466.6663 

  sigma:  32125.45

     AIC     AICc      BIC 
355.9429 356.9429 357.3590 
</code></pre>

<p>Which gives me negative prediction boundaries at 95% confidence:</p>

<pre><code>&gt; forecast(ets(x), level = .95)
   Point Forecast       Lo 95    Hi 95
16       54933.94   -8030.795 117898.7
17       54933.94  -34107.138 143975.0
18       54933.94  -54116.824 163984.7
...
</code></pre>

<p>Since we're dealing with count data, I've decided to hide the negative values from my final plot:</p>

<pre><code>plot(forecast(ets(x), level = .95), ylim = c(0, 260e3))
</code></pre>

<p><a href=""http://i.stack.imgur.com/Vp2wN.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Vp2wN.png"" alt=""plot""></a></p>

<p><strong>My questions are:</strong></p>

<ol>
<li><strong>How many Statistics professors have I just aggravated with that procedure?</strong></li>
<li><strong>How could I get away with such a model without having to resort to transforming my data (I'm trying to avoid the back-and-forth of log-transformation)?</strong></li>
</ol>

<p>Related questions:</p>

<ul>
<li><a href=""http://stats.stackexchange.com/q/92443/27433"">Can a mathematically sound prediction interval have a negative lower bound?</a></li>
<li><a href=""http://stats.stackexchange.com/q/143129/27433"">Getting Negative Forecasting Values</a></li>
</ul>
"
"0.159970469715827","0.149440644416015","215771","<p>I'm trying to build a time series model based on a cumulative variable that never decreases.</p>

<p>I'm interested in knowing when the observable will reach a certain value (i.e., when it will intersect with the blue line in the image below).</p>

<p><a href=""http://i.stack.imgur.com/aCAVF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aCAVF.png"" alt=""enter image description here""></a></p>

<p>The orange line is fixed to the last known data point and increases based on the average of the last 5 observables.</p>

<p>The red line is not fixed and represents a linear fit based on the last 5 observables. This seems problematic because in Time Period 108 in the graph, the predicted value is less than the observable in the previous time period, which will never physically happen.</p>

<p>The green line is not fixed and represents a linear fit based on all observables.</p>

<p>I'm wondering if someone can suggest an alternative/better approach to modelling this type of situation.</p>

<p><strong>EDIT / MORE INFO:</strong>
The time periods (e.g., 101, 102, etc.) represent 2-week intervals.</p>

<p>The cumulative information is the number of items completed. E.g., at the end of time period 107, there were approximately 200 total items completed. At the beginning of time period 106, there were about 150 items completed. So between time points 106 and 107, approximately 50 items were completed.</p>

<p>I'm interested in when approximately 410 items are completed (represented by the blue line).</p>

<p>The number of items completed starts at 0. Time period 101 is actually the first time period, it's just 101 has a separate (shouldn't be relevant for this post) meaning to our business. It can basically be treated as a label.</p>

<p>I see no particular reason why the number of items completed should follow an S-shape curve would.</p>
"
"0.125491161027632","0.148166814833556","116339","<p>I have a set of 3 years of daily data. I saw weekly and annual seasonality in the data so I used <code>msts</code> time series and <code>tbats</code> (from the <code>forecast</code> package in R) to fit the best fitted model. </p>

<p>The predicted values for weekdays are with 5% of the actual data but it has very off predictions for weekend. I did not expect that as I included daily seasonality in a week (different weekday and weekend patterns) in my time series which I though will consider the seasonality correctly. I wonder if anybody have any idea whats going wrong with my data. </p>

<p>I also used ts with single seasonality of frequency 7 and again used <code>tbats</code> to fit a model. Th new model has better predictions for weekend but worse predictions for weekday. I also tried <code>auto.arima</code> (also from the <code>forecast</code> package) but as I have a huge number of data points, arima was not able to find a good model. </p>
"
"NaN","NaN","129358","<p>Suppose you fit a time series with the <code>ets</code> function from the <code>forecast</code> package in <code>R</code>:</p>

<pre><code>library(forecast)
fit &lt;- ets(USAccDeaths, model=""MAM"")
summary(fit)

# ETS(M,A,M) 
# 
# Call:
#   ets(y = USAccDeaths, model = ""MAM"") 
# 
# Smoothing parameters:
#   alpha = 0.4761 
# beta  = 0.0326 
# gamma = 1e-04 
# ...more output...
</code></pre>

<p>Is it possible to generate confidence intervals for the parameters (e.g. <code>alpha</code>, <code>beta</code>, <code>gamma</code>)?  If methods exist, are they restricted to certain taxonomies?  That is, can this method be extended to ARIMA models?</p>
"
"0.212780616013727","0.212218334444056","116145","<p>I have downloaded the daily stock Adjusted Close price of one stock from sep 2011 to till date. As per my study plan, I have plotted some basic plots to understand the daily stock Adjusted closing price.</p>

<p>Here is the xyplot of the stock closing price by date and the code used to plot(My x axis not visible).</p>

<pre><code>Stock_T=stocks[which(symbol=='Stock_T'),]
xyplot(Adj.Close~Date,type='l',data=Stock_T,main='Adj.Close Price of the Stock_T')
</code></pre>

<p><img src=""http://i.stack.imgur.com/Ivlmk.png"" alt=""Timeseries plot of the raw data- Adjusted Closing price of the Stock""></p>

<p>By seeing this plot, the closing price was stable for period but had sudden huge increase in the stock price, it might had some other indicator which caused this much change in the stock price. Now my objective is to learn some ARIMA modeling concepts using this stock prices and try to do some forecasting of the stock price for few weeks. </p>

<p>As I have basic knowledge in ARIMA modeling, and I learned in the books that we should have stationary series before applying the ARIMA Model.</p>

<p>So, now I have plotted the ACF and PACF of the above raw data timeseries.</p>

<pre><code>acf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/rpy8S.png"" alt=""Raw data ACF Plot""></p>

<pre><code>pacf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/QKQge.png"" alt=""Raw data PACF plot""></p>

<p>From the above ACF and PACF plot, the series is not stationary and have huge autocorrelation (please correct me if am wrong), by differencing the series we will have stationary series (please correct me if am wrong). Here is the below plot.</p>

<pre><code>Stock_T_d1=diff(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/sD9Tj.png"" alt=""First difference of the raw series""></p>

<p>Here the differencing series and its ACF AND PACF plots. ACF plot shows that there is no auto correlation and the series is stationary (please correct me if I am wrong) but I am unable to interpret the PACF plots, can someone explain it to me?  </p>

<p><img src=""http://i.stack.imgur.com/cAoVf.png"" alt=""ACF plot of Difference series""></p>

<p><img src=""http://i.stack.imgur.com/U10g8.png"" alt=""PACF plot of Difference series""></p>

<p>The above difference series shows some unequal variance in the series and so I am taking log transformation before differencing and its ACF and PACF.</p>

<pre><code>Stock_T_logd1=diff(log(Stock_T$Adj.Close))
</code></pre>

<p><img src=""http://i.stack.imgur.com/MWovq.png"" alt=""Difference Logged series ""></p>

<p><img src=""http://i.stack.imgur.com/oYd2d.png"" alt=""ACF of Difference logged Series""></p>

<p><img src=""http://i.stack.imgur.com/HQxZw.png"" alt=""PACF of Difference logged Series""></p>

<p>Now I will try to ask my questions.</p>

<ol>
<li>Should we have stationary series before we apply ARIMA?</li>
<li>Could you please explain me the ACF and PACF of the original series, and what we should do if we have this kind of series?</li>
<li>Could you please explain me the ACF and PACF of the difference series, and what will be the next step?</li>
<li>Could you please explain me the ACF and PACF of the difference logged series, and what will be the next step?</li>
<li>Should we use difference series or difference logged series?</li>
<li>What will be the ARIMA orders of this series?</li>
<li>Is there any R code to find the ARIMA order automatically of the original series?</li>
</ol>
"
"0.0887356509416114","0.11973686801785","174687","<p>I have around 10000 time series and I want to train ARIMA model using 8000 of them.</p>

<p>I wanted to use auto.arima function <a href=""http://www.inside-r.org/packages/cran/forecast/docs/auto.arima"" rel=""nofollow"">http://www.inside-r.org/packages/cran/forecast/docs/auto.arima</a>
however I am unable to find best ARIMA model for many time series. </p>

<p>Here is the code, I can always use x as my time series.  but how to train it using more time series and find best model?</p>

<pre><code>y=auto.arima(x)
plot(forecast(y,h=30))
</code></pre>

<p>Sample 
time series 1</p>

<pre><code>0.0003748,0.0003929,0.0003653,0.0003557,0.0004463,0.000349,0.0003099,0.0003395,0.0003157,0.0002871,0.0002604,0.0002422,0.0001917,0.0002117,0.0002689
</code></pre>

<p>time series 2</p>

<pre><code>0.0003977,0.0003481,0.0002413,0.0002069,0.0002127,0.0002108,0.0002003,0.0002174,0.0002098,0.0002069,0.0001955,0.0001926,0.0002108,0.0002146,0.0002079
</code></pre>
"
"0.133103476412417","0.179605302026775","174692","<p>I have around 10000 time series showing one particular metric over 5 hours. </p>

<p>I used <a href=""http://www.inside-r.org/packages/cran/forecast/docs/auto.arima"" rel=""nofollow"">auto.arima function</a> </p>

<p>In my previous question, people suggested that I have to use auto.arima for each time series, hold off some of data points and test the prediction with my hold off points.</p>

<p>I am holding off 20% of data points (if you see sample out of 40 I will hold off 8) and then let auto.arima predict. Then I can compare generated 8 values with actual 8 values.
But is there a formal way to test accuracy in ARIMA model? Is my approach correcT?</p>

<p>Is there a prebuilt function to test the accuracy of Arima.</p>

<p>Here is the code, I can always use x as my time series.</p>

<pre><code>y=auto.arima(x)
plot(forecast(y,h=30))
</code></pre>

<p>Sample 
time series 1</p>

<pre><code>0.0003748,0.0003929,0.0003653,0.0003557,0.0004463,0.000349,0.0003099,0.0003395,0.0003157,0.0002871,0.0002604,0.0002422,0.0001917,0.0002117,0.0002689
</code></pre>

<p>time series 2</p>

<pre><code>0.0003977,0.0003481,0.0002413,0.0002069,0.0002127,0.0002108,0.0002003,0.0002174,0.0002098,0.0002069,0.0001955,0.0001926,0.0002108,0.0002146,0.0002079
</code></pre>

<p>Both have 40 points. I can hold off 20% of them (8) and compare after auto.arima predicts.   But is there a simpler way I can test accuracy?</p>
"
"NaN","NaN","43333","<p>How to choose the best values of alpha and beta in Holt's exponential smoothing? Leaving it upon R gives me $\alpha$ =1. Is this appropriate? </p>"
"NaN","NaN","<p>Entering different values of alpha and then comparing with the real data shows best result for $\alpha$ = 0.45. But then R calculates $\beta$=0.99. Is this fine?</p>",""
"NaN","NaN","","<r><time-series><forecasting><exponential-smoothing>"
"0.118314201255482","0.159649157357133","116267","<p>We come to this toy example showing MAPE and MASE are not consistent when measuring forecasting accuracy.</p>

<p>Data consist of 100 white noise and 100 $AR(1)$ time series with length $N=500$, mean $\mu=1$ and standard deviation $\sigma=1$. </p>

<pre><code># parameters
N &lt;- 500
mu &lt;- 10
sigma &lt;- 1 

# generate white noise
set.seed(1)
WNts &lt;- list(NULL)
for (i in 1:100){
  WNts[[i]] &lt;- ts(rnorm(N,mu,sigma))}

# generate AR(1)
ar1 &lt;- list(NULL)
for (i in 1:100){
  ar1[[i]] &lt;- arima.sim(model=list(ar=c(0.7)),n=N,sd=sqrt(sigma-0.7^2))+mu}

# data used
SimData &lt;- c(WNts,ar1)
</code></pre>

<p>Each time series are split into training and test set. What we are looking at are the MAPE and MASE on test set. To take a further look at MASE, we also calculate MAE and Q, which are numerator and denominator of MASE.</p>

<pre><code># forecasting accuracy on test set
SimDataAccuracy &lt;- foreach (i = 1:200,.combine = rbind)%dopar%{      
  x &lt;- SimData[[i]]
  trainx &lt;- window(x,end=400)
  testx &lt;- window(x,start=401,end=500)
  fit &lt;- auto.arima(trainx)
  accuracyArima &lt;- accuracy(forecast(fit,100),testx)
  Q &lt;- mean(abs(diff(trainx)))
  c(accuracyArima[2,5],accuracyArima[2,6],accuracyArima[2,3],Q)
}
colnames(SimDataAccuracy) &lt;- c(""MAPE"",""MASE"",""MAE"",""Q"")

# plot
par(mfrow=c(2,2))
# MAPE
plot(SimDataAccuracy[,1],ylab='MAPE',xlab='')
# MASE
plot(SimDataAccuracy[,2],ylab='MASE',xlab='')
# MAE
plot(SimDataAccuracy[,3],ylab='MAE (numerator of MASE)',xlab='')
# Q
plot(SimDataAccuracy[,4],ylab='Q (denomintor of MASE)',xlab='')
</code></pre>

<p>The plots show forecasting on white noise has smaller MASE just because of the larger Q. From both MAPE and MAE, white noise and $AR(1)$ time series have rather similar forecasting accuracy. </p>

<p>Does that mean</p>

<ul>
<li><p>White noise is easier to predict? (I cannot see a reason), or</p></li>
<li><p>They have similar forecastability and MASE is telling some disturbing information here?</p></li>
</ul>

<p><img src=""http://i.stack.imgur.com/qYRPN.jpg"" alt=""enter image description here""></p>
"
"0.0627455805138159","0.0846667513334603","175035","<p>I'm using ARIMA models to estimate sales forecast for a company. The company's sales channel is broken down into 4 sales channels and I'm running 4 different models to estimate the sales for each channel. Eventually, I'm going to aggregate the sales of these channels to find the total forecasted sales for the whole company. My questions is, how should i go about finding the confidence interval for the overall forecast? Adding up the confidence intervals of each channel is not correct since that will give me a very large interval.</p>

<p>I'd really appreciate if anyone can give me some idea on how to approach this sort of issue. Thanks in advance!</p>
"
"0.076847327936784","0.103695169473043","226934","<p>I have bi-weekly data for an event for which I am trying to build a forecasting model. When I plot the ACF and PACF, I get the following plots:</p>

<p><a href=""http://i.stack.imgur.com/Ak0gL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ak0gL.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/7LAjO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7LAjO.png"" alt=""enter image description here""></a></p>

<p>From what I understand, the plots show that the data are seasonal and seasonality has almost a fixed period of length 13 (as there are 13 bars in each block in the ACF plot). The data also seem to have a downward trend because of the auto correlation diminishes from left to right in the plot. My questions are:</p>

<ol>
<li>Am I interpreting the plots correctly?</li>
<li>What types of models should I try with such data?</li>
</ol>

<p>I have already tried <code>auto.arima()</code> and <code>HoltWinters()</code> from the <code>forecast</code> package without much success. Any guidance is appreciated! Thanks!</p>
"
"0.0627455805138159","0.0846667513334603","226997","<p>I have a time series $Y_{t}$, which is a noisy and biased leading estimator of a lagged time series $X_t$.</p>

<p>$$
Y_{i-lag} = X_i + b_i + e_i
$$</p>

<p>What is the best method to combine the time series in real time to create an unbiased time series?</p>
"
"0.171835849155868","0.185495558304067","195443","<p>I am looking at two time series, from 01/01/2000 to the present: <br></p>

<ul>
<li>The <a href=""https://research.stlouisfed.org/fred2/series/NAPMNOI/"" rel=""nofollow"" title=""ISM Manufacturing: New Orders Index"">ISM Manufacturing: New Orders Index</a>, only available seasonally adjusted</li>
<li>The manufacturing industry unemployment rate, only available unadjusted (<a href=""https://research.stlouisfed.org/fred2/series/LNU04032232"" rel=""nofollow"">https://research.stlouisfed.org/fred2/series/LNU04032232</a>)</li>
</ul>

<p>I was <em>hoping</em> to construct a multivariate ts model, and use the <strong>New Orders Index</strong> to forecast the <strong>manufacturing industry unemployment rate</strong>. However, am I correct in assuming it is not 'ideal' to use seasonally adjusted data to predict another time series? Because doesn't SA cause (ideally) all the seasonal time series structure to be removed from the data?</p>

<h3>EDIT:</h3>

<p>Sorry, it just now hit me to link to the data I was using by putting it on Google Drive. It's in .csv files, for easy viewing with any program.</p>

<ul>
<li>Manufacturing new orders index data, in <strong>OrdersIndex.csv</strong><br><a href=""https://drive.google.com/file/d/0B2Y54SySHrVwZXczR1N4LXZMdXc/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B2Y54SySHrVwZXczR1N4LXZMdXc/view?usp=sharing</a></li>
<li>Manufacturing industry unemployment rate, in <strong>Unem.csv</strong>
<br><a href=""https://drive.google.com/file/d/0B2Y54SySHrVweFVpRjJFanAwQmc/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B2Y54SySHrVweFVpRjJFanAwQmc/view?usp=sharing</a></li>
</ul>

<p>Below is the New Orders Index time series, with the dashed line indicating the mean of 54.61. It looks fairly stationary to me; a decent spike in 2008, but definitely reverts to the mean.</p>

<pre><code>&gt; plot.ts(OrdersIndex[,2])
&gt; mean(OrdersIndex[,2])
[1] 54.60829
&gt; abline(h=c(54.61), lty=2)
&gt; 
</code></pre>

<p><a href=""http://i.stack.imgur.com/C61sm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/C61sm.png"" alt=""New Orders Index""></a></p>

<p>The ACF and PACF of the series are below. ACF displays dampened sine-wave behavior, PACF has a sharp cut-off after lag 1. This suggests an AR(1) model, as the ACF's slow dying off (at lags > 1) is due to the auto correlation at lag 1.</p>

<pre><code>&gt; Acf(OrdersIndex[,2], plot=T)   #the Acf() function is part of 'forecast' package
&gt; Acf(OrdersIndex[,2], plot=T, type=c('partial'))
&gt;
</code></pre>

<p><a href=""http://i.stack.imgur.com/Dg2Es.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Dg2Es.png"" alt=""ACF plot""></a>
<a href=""http://i.stack.imgur.com/0PqBR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0PqBR.png"" alt=""PACF plot""></a></p>

<p>After running an arima(1,0,0) model with a mean, the ACF and PACF of the residuals do not show significant spikes at any lags.</p>

<pre><code>&gt; OrdersIndex100 &lt;- arima(OrdersIndex[,2], order=c(1,0,0))
&gt; OrdersIndex100

Call:
arima(x = OrdersIndex[, 2], order = c(1, 0, 0))

Coefficients:
         ar1  intercept
      0.8738    54.6979
s.e.  0.0341     1.9399

sigma^2 estimated as 12.39:  log likelihood = -517.44,  aic = 1040.88
&gt;
</code></pre>

<p>Running an Ljung-Box test on the residuals indicates there is not any time series structure left in the data.</p>

<pre><code>&gt; LBQPlot(OrdersIndex100$residuals, k=1)   # LBQPlot() is part of 'FitAR' package
&gt;
</code></pre>

<p><a href=""http://i.stack.imgur.com/xXQKc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xXQKc.png"" alt=""Ljung-Box Test""></a></p>

<h3>Conclusion</h3>

<p>The conclusion I arrive at is that the seasonally adjusting done to the data by the ISM (Institute of Supply Management) effectively removed all the seasonality from the data. So, this SA data would be less useful in modeling than non-SA data (this is assuming that I would be using this data series as the Input, and the unemployment data series as the Output). Is this a valid conclusion? You all see any glaring problems with my analysis?</p>
"
"0.177779144789145","0.211666878333651","99488","<p>I am relatively new to statistics and not formally trained but have been given a complex problem to solve and need some guidance. I realise that I am out of my depth a bit here but would appreciate whatever help I can get bearing in mind that there is no budget for this and as a result it is not possible to purchase software or hire consultants.</p>

<p><strong>The Problem</strong></p>

<p>The business I work for has a large number of mobile representatives that can be dispatched to a variety of different jobs. There are ~100 different job types and each job can be broken up into 4 different final outcomes. Each of these 400 outcomes requires an allocation of man hours to complete. I have a count of how many times each one of these outcomes occurred in each hourband for the past 5 years.</p>

<p>I have been asked to forecast how many of each outcome will occur in each hourband for the 28days from the present. The resulting forecast will be used to anticipate staffing requirements on an hour-by-hour basis. As a result the forecasts for each hourband need to been fairly accurate.</p>

<p><strong>Factors</strong></p>

<p>In my data there are clearly some yearly, weekly, and daily seasonal effects. In general each outcome is more likely to occur at certain times of the day on certain days of the week and with some yearly trends.</p>

<p>Each different outcome is likely to be related to the frequency of a number of different outcomes. i.e. if <em>x</em> happens then <em>y</em> and/or <em>z</em> are likely but <em>a</em> and/or <em>b</em> are not.</p>

<p>There are a large number of environmental factors that contribute to the frequency of each outcome. These can include, but are not limited to weather, sociopolitical, financial trends, one off events.</p>

<p><strong>What I have tried</strong></p>

<p>So far I have tried using simple auto.arima, holtwinters and ets forecasts. holtwinters ended up producing a flat line (i.e. 5 and hour for the next 672 hours). ets doesnt work because the seasons are longer than 24 intervals. auto.arima produced the best results but they were still a long way off being accurate.</p>

<p>It was then suggested that I try tbats() and provide it with multiple seasonal lengths. I achieved best results by giving it seasonal lengths of 8760 (1yr) and 168 (1wk). Frustratingly, these results are within 1% when viewed as a sum of all hourbands in a 1 month block but are anything up to 300% (avg 20%) off when considering each individual hourband.</p>

<p>Both of these approaches have been applied over an individual outcome rather than considering all possible outcomes (and their correlation to each other).</p>

<p><strong>My thoughts so far</strong></p>

<p>At this stage I feel like my two options are to either to find a way to use something similar to tbats() that will look at the relationships between the multiple different outcomes as well as the seasonality and forecast based on that information.</p>

<p>or</p>

<p>Abandon that approach for a Neural Network model. My understanding (limited) is that using the Neural Network approach I may be able to 'factor' for the multitude of unknown environmental factors without having to actually identify them. I know this is lazy but my feeling for the data is that there are going to be a fair few unknown factors to identify and forecasting them may end up being a job in itself (i.e. weather conditions)</p>

<p><strong>The Question</strong> (finally)</p>

<p>What I am looking for is some guidance.</p>

<p>Considering the information above and the fact that I am pretty much limited to R, what is the best approach??</p>

<p>and</p>

<p>What are the basic steps I need to follow?</p>

<p>While I cant post my data online (due to my employers restrictions) I can send it an individual or two if someone was interested in giving us a hand to find a solution.</p>
"
"0.0887356509416114","0.11973686801785","179212","<p>I am working on predicting a time series of daily data for one month that looks like this:
<a href=""http://i.stack.imgur.com/kxwc1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kxwc1.jpg"" alt=""Time-series daily data""></a></p>

<p>As can be seen, the time-series has a weekly seasonality. I am trying to predict the next week's data (horizon=7), updating my forecasts every day, so at each time-step, I am getting forecasts for the next 7 time-steps. </p>

<p>I have tried a number of methods but I would expect at least the snaive method to give me something reasonable. The code I am using is (tseries is an XTS object with the daily data):</p>

<pre><code>for (t in horizon:(length(a)-horizon)) { # Every day
  timeseries &lt;- ts(a[1:(t+horizon)], frequency=7)
  fit &lt;- snaive(timeseries[1:t], h=horizon)
  plot(forecast(fit, h=horizon))
  lines(1:(t+horizon),timeseries, col='black')
}
</code></pre>

<p>The method consistently gives me flat predictions, looking like this:
<a href=""http://i.stack.imgur.com/fYq2j.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fYq2j.jpg"" alt=""SNaive prediction for the next 7 days""></a></p>

<p>Has anyone had any similar problems? Is it because I set frequency=7 for my daily data?</p>
"
"0.076847327936784","0.0691301129820284","196781","<p>My data frame consists of 3 input columns (factors 1, 2 and 3) and output column, i.e., revenue which are time varying parameters. I am trying to predict the revenue using neural networks for the subsequent 12 instances (say months). I have trained a network to work on the 3 inputs to calculate the revenue for all the instances till now. To predict future instances of revenue, I have creared a dataframe of forecast(nnetar(col1),nnetar(col2),nnetar(col3)) i.e., forecasted each input separately and then used the same neural network to work on forecasted inputs to predict the future values of revenue.</p>

<p>But I don't find this very accurate. Are there any better ways to actually do this?</p>
"
"0.0992094737665681","0.107095910520333","178787","<p>Im really new in regression estimation but my problem here is about forecasting confrontation. </p>

<p>This is my model:</p>

<p>$Y_t = \beta_0 + \beta_1 X_t + \epsilon_t$ </p>

<p>My OLS estimation using r function ""lm"" was:</p>

<pre><code>set.seed(123)
data &lt;- matrix(rnorm(50*2),nrow=50)
m &lt;- data.frame(data )


Model1&lt;- lm(X1 ~ X2 -1 , data = m)
&gt; Modelo1$coef
        X2 
-0.0296194 
</code></pre>

<p>My Quantile Regression (Median, $\tau = 0.5$) was:</p>

<pre><code>&gt; ModeloRQ1&lt;-rq(X1 ~ X2 -1, tau = 0.5,method=""br"", data=m) 

&gt; ModeloRQ1$coef
        X2 
-0.1256418 
</code></pre>

<p>The estimation procedure i understand. </p>

<p>But the Forecasting Procedure i dont understand. 
I know that after making the forecast i should compare using RMSFE statistics, for example.</p>

<p>But when i use the ""forecast"" function gives me the same point forecast (same values) when i use ""predict"" function.</p>

<p>I have read some papers which do not detail this procedure. Only say that ""OLS and QR (0.5) forecasts are Confronted against each other"". </p>

<p>How should i do this procedure? Simply by using the function predict/forecasting? this would be a commonly used procedure?</p>
"
"0.125491161027632","0.169333502666921","178922","<p>I have a number of time series data with daily observations (approximately 2 months of data) used for back testing. The system should automatically update the model at the end of each day, when new daily data becomes available, making forecasts for the next 7 days.</p>

<p>I tried to see how the ets method would look like, so I start with data for a full week (days 1:7) and build a model predicting the next 7 days (8:15), then use days 1:8, predict days 9:16 and so on.</p>

<p>I have a loop like this:</p>

<p><code>for (t in 7:49) {
  ts_data &lt;- ts(data[1:t], frequency=7)
  fit &lt;- ets(ts_data)
  plot(forecast(fit,7))
}</code></p>

<p>Once the for loop reaches the second repetition, I get an error at ets:</p>

<p>Error in ...fourier(x, K, 1:length(x)) : 
  K must be not be greater than period/2 </p>

<p>So is there a rule for setting K in the fourier function? Apparently, I cannot set this completely manually as the data changes and there are a lot of time series datasets to predict. It is the first time I am working on time series model, so apologies if this is a naive question.</p>

<p>I also found <a href=""http://stackoverflow.com/questions/28841142/forecast-throws-error-k-must-be-not-be-greater-than-period-2"">this</a> but apparently even if t=14 (multiple of 7), I still get the same error, and generally I do not understand why I would need to have exact multiples of the frequency (weekly here).</p>

<p>Note: I have tried to remove the frequency=7 and ets works fine but it does not get any seasonality patterns, in contrast to when the frequency=7 where I get much better results.</p>
"
"0.0443678254708057","0.059868434008925","198662","<p>I am using the code below:</p>

<pre><code>#Training seasonal ARIMAx model on input dataset
fit&lt;-arima(visits_ts, order=c(1,0,0),seasonal=c(1,0,0),xreg=reg,method=""CSS"")

#Forecasting for future
pred&lt;-predict(fit,n.ahead=13, newxreg=nreg)
</code></pre>

<p>The code only generates out of sample forecast. However, I would also like to see the in-sample forecast for the training data set. How can I get the in-sample forecast?</p>
"
"0.108678533400333","0.146647115021353","198695","<p>My aim is to forecast the daily number of registrations in two different channels.</p>

<p>Week seasonality is quite strong, especially the weekends and also observed annual effects. Moreover, I have a few special event days, which significantly differ from the others days.</p>

<p>Dataset: <a href=""https://www.dropbox.com/s/6xevfepu94gnx3l/OUTPUT_DATA.csv?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/6xevfepu94gnx3l/OUTPUT_DATA.csv?dl=0</a></p>

<p>First, I applied a TBATS model on these two channels.</p>

<pre><code>x.msts &lt;- msts(Channel1_reg,seasonal.periods=c(7,365.25))
# fit model
fit &lt;- tbats(x.msts)
fit
plot(fit)
forecast_channel1 &lt;- forecast(fit,h=30)
</code></pre>

<p>First channel:</p>

<pre><code>TBATS(0, {2,3}, -, {&lt;7,3&gt;, &lt;365.25,2&gt;})

Call: tbats(y = x.msts)

Parameters
  Lambda: 0
  Alpha: 0.0001804516
  Gamma-1 Values: -1.517954e-05 1.004701e-05
  Gamma-2 Values: -3.059654e-06 -2.796211e-05
  AR coefficients: 0.249944 0.544593
  MA coefficients: 0.215696 -0.361379 -0.21082
</code></pre>

<p>Second channel:</p>

<pre><code>BATS(0, {2,2}, 0.929, -)

Call: tbats(y = y.msts)

Parameters
  Lambda: 0
  Alpha: 0.1652762
  Beta: -0.008057904
  Damping Parameter: 0.928972
  AR coefficients: -0.586163 -0.676921
  MA coefficients: 0.924758 0.743675
</code></pre>

<p>If I forecast the second channel, I only get blank values instead of any forecasts.</p>

<p>Could you please help why is that so?
Do you have any suggestion how to build in the specific event days into this model?
Thank you all!</p>
"
"0.137468679324694","0.170037595112062","180521","<p>I have a time series that includes some rare extreme values. We are talking about daily data, in total 1461 observations and 11 extreme values. I adjusted those 11 values with a multiple regression. Now I am using the <code>tbats()</code> on the original time series and the adjusted one. </p>

<pre><code>accuracy(original)
&gt;                   ME    RMSE      MAE MPE MAPE      MASE          ACF1
&gt;Training set 10.23539 4202.19 2921.593 NaN  Inf 0.6777689 -0.0003493096
accuracy(adjusted)
&gt;                   ME    RMSE      MAE MPE MAPE      MASE          ACF1
&gt;Training set 43.35625 3803.618 2787.39 NaN  Inf 0.6827622 -0.004749092

#original AIC
&gt;35101.43
#adjusted AIC
&gt;34798.24
</code></pre>

<p>How can I see if the model improves due to the adjustment or not? Since I reduced those 11 extreme values, I can't just compare MAE, RMSE or AIC. MASE is the only measure that should work?</p>

<p>I could divide MAE, RMSE and AIC by the mean of the respective time series.</p>

<pre><code># original
0.4962245 # MAE/mean(original)
0.7137304 # RMSE/mean(original)
5.96188 # AIC/mean(original)

# adjusted
0.4862567 # MAE/mean(adjusted)
0.6635364 # RMSE/mean(adjusted)
6.07051 # AIC/mean(adjusted)
</code></pre>

<p>Is that a legitimate way to compare the results?</p>

<p>Here are the <code>pacf</code>-diagrams of both models:</p>

<p><strong>original</strong>:</p>

<p><a href=""http://i.stack.imgur.com/nFARp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nFARp.png"" alt=""original""></a></p>

<p><strong>adjusted</strong>:</p>

<p><a href=""http://i.stack.imgur.com/YXIGF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YXIGF.png"" alt=""adjusted""></a></p>

<p><strong>Update:</strong></p>

<p>I just realized that when i use the <code>accuracy()</code> function of the <code>forecast</code> package with a <code>tbats()</code> based on a <code>msts()</code> object the resulting MASE is using an in-sample naive forecast for scaling. I guess that is not optimal? It should be better to use an in-sample naive seasonal forecast with the longest season of the <code>msts()</code> object.</p>

<pre><code>MASE(original) # scaled with a in-sample naive seasonal forecast (365)
&gt; 0.6339

MASE(adjusted) # scaled with a in-sample naive seasonal forecast (365)
&gt; 0.6287
</code></pre>
"
"0.133103476412417","0.159649157357133","224115","<p>I apologise, if these question are trivial, but I am currently trying to estimate a family of GARCH models for a selection of return series as part of an undergraduate project for which I am having to do a fair amount of self-teaching.</p>

<p>I am following the instruction given in Tsay: Analysis of Financial Time series and upon finding the expected ARCH effects using LM/LB tests, would now like to proceed estimating the models.</p>

<ol>
<li><p>However, I am having difficulties selecting the order of the mean equation, which I assume to be some form of ARMA model. Given that the mean equation and GARCH model should be jointly estimated, how do I go ahead selecting the order of the mean equation?</p>

<p>Tsay suggests the typical Box-Jenkins methodology of either using AIC/BIC/AICc or PACF plots when building an ARMA model, however, I believe this would lead to a sequential estimation process, based upon wrong assumptions, as volatility is not constant over time.</p>

<p>Do I estimate (for instance) 25 different ARMA(p,q)-GARCH(1,1) models and compare their AIC/AICc/BIC or how would I go ahead selecting the mean equation? </p></li>
<li><p>Furthermore, is there a package in R that gives me a series of one-day ahead forecasts of volatility based upon previously estimated parameters?</p>

<p>So, given an GARCH-model that has been estimated in-sample, can I use the in-sample coefficients to predict one-day ahead forecasts out-of-sample?</p>

<p>I luckily have a series of Realised Variance to work with when comparing the models.</p></li>
</ol>
"
"0.159970469715827","0.182649676508463","223888","<p>I am interested in forecasting with a vector error correction model (VECM). I am facing a problem of not being able to transform a cointegrated series into a VECM model using the stationary series. </p>

<p>In multivariate forecast like VAR or VECM it is important to see which of the two models to use for forecasting. To decide whether to use a VAR or a VECM:</p>

<ul>
<li>First, we do a cointegration test using the <code>ca.jo</code> function from ""urca"" package in R. </li>
<li>If we find that there is no cointegrating vector suggested by the Johansen procedure, then we can run a VAR model. But if we find evidence of cointegration then we have to use a VECM model in order to incorporate the error correction coefficients in the model. </li>
<li>To test if there is cointegration in the series we use Johansen test on the data <strong>in levels</strong>, i.e. in non-stationary form. But after we find evidence of cointegration we have to incorporate as many cointegrating vectors in the VECM as the number suggested by the Johansen test. But then this time the VECM should have been run on stationary series having made them <strong>differenced</strong>. </li>
<li>But in R I am not getting the option as to how to make a VECM model differenced and then forecast it. R manuals are suggesting that we should use the function <code>vec2var</code> to convert a VECM to a VAR model and then forecast the VAR model thus obtained. </li>
<li>But the VAR model thus obtained from the VECM is <strong>at levels</strong> and <em>not</em> at <strong>differenced</strong> form. Hence, inference from this may be biased. </li>
</ul>

<p>I just want to run a VECM in <strong>differenced</strong> series (not <strong>in levels</strong>) and also to include the error correction term. Please help me with this. </p>
"
"0.108678533400333","0.146647115021353","223872","<p>Data consisting of 30 values is stored in a time series <code>time</code>.<br>
After applying ARIMA modelling on <code>time</code>, I used <code>forecast</code> function to predict future values:</p>

<pre><code>model = arima(time, order = c(3,2,1))
prediction = forecast.Arima(model,h=10)
prediction step is not working and showing error 
Error in ts(x) : object is not a matrix
</code></pre>

<p>As you see above, I am getting an error message. But if I do</p>

<pre><code>model = arima(time[1:25], order = c(3,2,1))
prediction = forecast.Arima(model,h=10)
</code></pre>

<p>it works. Why is it so?</p>

<p>When I used the <code>predict</code> function </p>

<pre><code>model = arima(time, order = c(3,2,1))
prediction=predict(model,n.ahead=10)
</code></pre>

<p>it also works.</p>

<p><strong>Which</strong> function would be better to use, <code>predict</code> or <code>forecast</code>, for ARIMA models in R, and <strong>why</strong>?</p>
"
"0.159970469715827","0.166045160462239","161818","<p>I am looking at some high frequency data and I would like to know how to interpret and compare Realized volatility (RV) and Two Scale Realized Volatility (TSRV). References below. Given X is the log return of a stock</p>

<p>$$
     [X,X]_{T}^{all} =  \sum\limits_{i=1}^{n} (X_{t_{i+1}} - X_{t_{i}})^2 
$$</p>

<p>Here subscript all means use all the data. In my case my data is second by second so it would be the sum of the differences of squared log returns 1 second apart.</p>

<p>To compute RV in R I have a function that takes prices, takes there log, then differences, squares them and sums them up:</p>

<pre><code>RV&lt;-function(prices)
{
  logprices = log(as.numeric(prices))
  logreturns = diff(logprices)
  return(sum(logreturns^2))
}
</code></pre>

<p>the Two Scale Realized Volatility (TSRV) partitons the whole sample 1 to n in to K subsamples. In my case K= 300. So there will be a moving window time301-time1, time302 -time 2...and the RV for those windows will be averaged over.</p>

<p>$$[X,X]_{T}^{K} = \dfrac 1K\sum\limits_{i=1}^{n-K+1} (X_{t_{i+K}} - X_{t_{i}})^2  $$</p>

<p>THEN</p>

<p>$$ TSRV =  (1- \dfrac zn)^{-1} [X,X]_{T}^{K} - \dfrac zn [X,X]_{T}^{all}$$</p>

<p>where z = (n-K+1)/K. </p>

<p>Taking the difference between $$[X,X]_{T}^{K}$$ and  $$[X,X]_{T}^{all}$$ cancels the effect of microstructure noise. The factor (1-z/n)^-1 is a coefficient to adjust for finite sample bias.</p>

<p>In R there is a function to calculate TSRV:</p>

<pre><code>myTSRV&lt;-function (pdata, K = 300, J = 1) 
{ 
  #pdata contains prices for a stock
  #K the slow time scale = 300 seconds
  #J is the fast time scale = 1 second
  logprices = log(as.numeric(pdata))
  n = length(logprices)
  nbarK = (n - K + 1)/(K) 
  nbarJ = (n - J + 1)/(J)
  adj = (1 - (nbarK/nbarJ))^-1  #adjust for finite sample bias
  logreturns_K = logreturns_J = c()
  for (k in 1:K) {
    sel = seq(k, n, K)
    logreturns_K = c(logreturns_K, diff(logprices[sel]))
  }
  for (j in 1:J) {
    sel = seq(j, n, J)
    logreturns_J = c(logreturns_J, diff(logprices[sel]))
  }
  TSRV = adj * ((1/K) * sum(logreturns_K^2) - ((nbarK/nbarJ) *  (1/J) * sum(logreturns_J^2)))
  return(TSRV)
}
</code></pre>

<p>I took tick data for IBM for about 2 hours and calculated the RV  and and TSRV with K= 300 seconds and J= 1 second for about 2 hours. </p>

<p>I have a few questions.</p>

<ul>
<li>The RV is in the range of .00002 to .00005. How do I interpret this? In the literature RV is also called integrated variance. I want the volatility so do I need to square root these number to get to .0044 to .007?</li>
<li>Even if I square root them what does .0044 or .007 mean? The volatility for IBm during those 2 hours was .44% to .7%?</li>
<li>Does .0044 and .007 need to be normalized to an annual or daily number somehow? Can you suggest how?</li>
<li>How does one compare the RV or TSRV from different length intervals. Let's say I have an RV that is calculated using 2 hours of data. How do I compare it to and RV using 6 hours of data?</li>
</ul>

<p>References</p>

<p>All of my post  is from: <a href=""https://lirias.kuleuven.be/bitstream/123456789/282532/1/AFI_1048.pdf"" rel=""nofollow"">https://lirias.kuleuven.be/bitstream/123456789/282532/1/AFI_1048.pdf</a></p>

<p>original paper for TSRV: <a href=""http://wwwf.imperial.ac.uk/~pavl/AitSahalia2005.pdf"" rel=""nofollow"">http://wwwf.imperial.ac.uk/~pavl/AitSahalia2005.pdf</a></p>

<p>R code getAnywhere(""TSRV"")</p>
"
"0.243012588378125","0.295121626102779","176129","<p>I've been working on some various time series forecasts and I've begun to notice a trend (pardon the pun) in my analyses. For about 5-7 datasets that I've worked with so far, it would be helpful to allow for multiple seasonal periods along with an option for holiday dummies. I've tried various methods and usually stick with <code>tbats</code> since <code>auto.arima()</code> with regressors has been giving me issues. By this point, it's probably obvious I'm working in R.</p>

<p>Before I get too far, let me give some sample data. Hopefully the following link works: <a href=""https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0"" rel=""nofollow"">https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0</a>.</p>

<p>This data yields the following time series plot:
<a href=""http://i.stack.imgur.com/FYS1x.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FYS1x.jpg"" alt=""Time Series Plot""></a>
The large dips are around Christmas and New Years, however there are also smaller dips around Thanksgiving. In the code below, I name this dataset <code>traindata</code>.</p>

<p>Now, <code>ets</code> and ""plain"" <code>auto.arima</code> don't look so hot in the long run since they are limited to only one seasonal period (I choose weekly). However for my test set that I held out they performed fairly well for the month's worth of data (with the exception of Labor Day weekend). This being said, forecasting out for a year would be ideal.</p>

<p>I next tried <code>tbats</code> with weekly and yearly seasonal periods. That results in the following forecast:
<a href=""http://i.stack.imgur.com/kcXmd.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kcXmd.jpg"" alt=""TBATS Forecast""></a></p>

<p>Now this looks pretty good. From the naked eye it looks great at taking into account the weekly and yearly seasonal periods as well as Christmas and New Years effects (since they obviously fall on the same dates each year). It would be best if I could include the holidays (and the days around them) as dummy variables. Hence my attempts at <code>auto.arima</code> with <code>xreg</code> regressors.</p>

<p>For ARIMA with regressors, I've followed Dr. Hyndman's suggestions for the fourier function (given here: <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) as well as his selection of the number of fourier terms (given here: <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/forecasting-weekly-data/</a>)</p>

<p>My code is as follows:</p>

<pre><code>fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep=""""),period,sep=""_"")
  return(X)
}

fcdaysout&lt;-365
m1&lt;-7
m2&lt;-30.4375
m3&lt;-365.25

hol&lt;-cbind(traindata$CPY_HOL, traindata$DAY_BEFORE_CPY_HOL, traindata$DAY_AFTER_CPY_HOL)
hol&lt;-as.matrix(hol)

n &lt;- nrow(traindata)
bestfit &lt;- list(aicc=Inf)
bestk &lt;- 0

for(i in 1:m1)
{
    fake_xreg = cbind(fourier(1:n,i,m1), fourier(1:n,i,m3), hol)
    fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = fake_xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
    	if(fit$aicc &lt; bestfit$aicc)
    {
        bestfit &lt;- fit
        bestk &lt;- i
    }
    else
    {
    }
}

k &lt;- bestk
k
##k&lt;-3

xreg&lt;-cbind(fourier(1:n,k,m1), fourier(1:n,k,m3), hol)
xreg&lt;-as.matrix(xreg)

aacov_fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aic"", allowdrift=TRUE)
summary(aacov_fit)
</code></pre>

<p>Where my issues come in is inside the for loop to determine the <code>k</code>, the number of fourier terms, that minimizes AIC. In all of my attempts at ARIMA with regressors, it always produces an error when <code>k&gt;3</code> (or <code>i&gt;3</code> if we're talking about inside my loop). The error being <code>Error in solve.default(res$hessian * n.used, A) : system is computationally singular: reciprocal condition number = 1.39139e-34</code>. Simply setting <code>k=3</code> gives some decent results for my test set but for the next year it doesn't appear to adequately catch the steep drops around the end of the year and is much smoother than imagined as evidenced in this forecast:<a href=""http://i.stack.imgur.com/rj30h.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rj30h.jpg"" alt=""AutoArima with Covariates (k=3)""></a></p>

<p>I assume this general smoothness is due to the small number of fourier pairs. Is there an oversight in my code in that I'm just royally screwing up the procedure provided by Dr. Hyndman? Or is there a theoretical issue that I'm unknowingly running into by trying to find more than 3 pairs of fourier terms for the multiple seasons I'm attempting to account for? Is there a better way to include the multiple seasonalities and dummy variables?</p>

<p>Any help in getting these covariates into the arima model with an appropriate number of fourier terms would be appreciated. If not, I'd at least like to know whether or not what I'm attempting is possible in general with larger number of fourier pairs.</p>
"
"0.0443678254708057","0.059868434008925","180981","<p>I'm using the <code>rpart</code> library to try forecasting the electricity consumption from Australia (example from the book Introductory Time Series with R): </p>

<pre><code> CBE : 
choc    beer    elec
1451    96.3    1497
....
All the data is separate in months from 1958 til 1970

library(rpart)
www &lt;- ""http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/cbe.dat""
CBE &lt;- read.table(www, header = T)
Elec.ts &lt;- ts(CBE[, 3], start = 1958, freq = 12)

plot(cbind(Elec.ts))


fit &lt;- rpart(elec~elec, method=""anova"", data=CBE)
pre &lt;- predict(fit)

Elec.predict &lt;- ts(pre[], start = 1958, freq = 12)
plot(cbind(Elec.ts,Elec.predict ))
</code></pre>

<p>It's really simple, the R program <strong>does not run</strong>, if I try to create a model using the <strong>elec data it self</strong>.</p>

<p>Am I using it wrong?<br/>
How Can I use this library properly ?</p>
"
"0.0443678254708057","0","181158","<p>I'm having trouble forecasting a time series with a trigonometric component using dlmModTrig.</p>

<p>So far I have:
buildFun&lt;-function(x){</p>

<p>dlmInven&lt;- dlmModTrig (s=12, dV=0, q=2, dW=exp (x<a href=""http://i.stack.imgur.com/y9LWs.png"" rel=""nofollow"">1</a>))+dlmModPoly(order=2, dV=exp(x[2]), dW=c(exp(x[3]),exp(x[4])))
    return(dlmInven)
}</p>

<p>And the convergence is 0 after I check </p>

<p>(fit&lt;-dlmMLE(lInven, parm=rep(0,4), build=buildFun))$conv.</p>

<p>However, the predicted graph is very far off. In fact, even the smoothed portion is far off:</p>

<p><a href=""http://i.stack.imgur.com/y9LWs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y9LWs.png"" alt=""enter image description here""></a></p>

<p>If I do the dlmModTrig and the dlmModPoly individually. The fits are  better.</p>

<p>Any idea why this is happening? </p>
"
"0.0887356509416114","0.11973686801785","181166","<p>I have a data set of deposits and withdrawals from bank locations, so each record includes a bank identifier, date stamp, number of deposits, and number of withdrawals. I have included reproducible code below.  Note that I have hundreds of days for each of thousands of agents. I have a few questions:</p>

<ol>
<li><p>I would like to use the forecast package, but I'd like it to take into account day of week patterns and month of year seasonality. The one worry I have about using the ""frequency"" flag is that I have some randomly missing days for each agent. How should I best deal with this?</p></li>
<li><p>Deposits and withdrawals cannot be negative so all of the data is non-negative. Can I force the forecast to be non-negative? I know I can use the lambda=0 flag, but this only works when everything is strictly positive (but not applicable in this case because deposits and withdrawals can be zero)</p></li>
<li><p>Is there anything I can do to increase predictive accuracy by ""clustering"" banks? Right now, I'm only using a particular bank's data, but given that I have data from thousands of more banks, perhaps I can take advantage of this?</p></li>
</ol>

<pre class=""lang-r prettyprint-override""><code>library(lubridate)
library(dplyr)
library(forecast)

Date = c(today()-1, today()-3, today()-4, today()-5) %&gt;% as.POSIXct
D    = c(10,3,4,3)
W    = c(13,2,4,4)
Bank = c(rep(1,4), rep(2,4))
A    = cbind(Date,D,W) %&gt;% as.data.frame 
A$Date = A$Date %&gt;% as.POSIXct(origin=""1970-01-01"")
B   = A
B$D = B$D - 1
B$W = B$W + 3
A   = A %&gt;% rbind(B)
A   = A %&gt;% cbind(Bank,.)
</code></pre>
"
"0.0627455805138159","0.0846667513334603","181216","<p>I am new to R and forecasting .I have data for a certain product. It contains value sales and its promotions. The data is weekly and there are about 104 data points. </p>

<p>I converted the sales into a ts object and created seasonaldummy's to capture seasonality. </p>

<pre><code>actual_val = ts(sku1$Sal , frequency = 52)
dummy_val = seasonaldummy(actual_val) 
</code></pre>

<p>The dummy's were later combined with the promo variables to create external regressors xreg_val for the model. Those promotions which were not held for this sku were removed before combining the two.</p>

<pre><code>model_value &lt;- try(auto.arima(actual_val , xreg = xreg_val ) , silent = TRUE)
</code></pre>

<p>I have received the following error</p>

<pre><code>Error in optim(init[mask], armaCSS, method = optim.method, hessian = FALSE,    
non-finite value supplied by optim
</code></pre>

<p>I could not understand where exactly I have gone wrong in this.</p>

<p>Attaching a sample of the data. Kindly help me with this</p>

<p><a href=""https://drive.google.com/file/d/0B6sOv1da0JMeb01XYW92UzRSZ0U/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B6sOv1da0JMeb01XYW92UzRSZ0U/view?usp=sharing</a></p>
"
