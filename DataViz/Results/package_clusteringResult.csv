"V1","V2","V3","V4"
"0.060084176812611","0.0662266178532522","  2728","<p>I am clustering a dataset using the pam command (from {cluster} package), and I wish to decide on the number of clusters to use.</p>

<p>I was able to implement The_Elbow_Method in R (<a href=""http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method"" rel=""nofollow"">see wiki</a>) for doing that.  But that doesn't provide me with any solid criteria (like AIC, <a href=""http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#Information_Criterion_Approach"" rel=""nofollow"">for example</a>) for decision.</p>

<p>I came by the {clValid} package which looks promising, but I wanted to know if there are <strong>any other R solutions (you know of) for choosing the number of clusters for pam?</strong></p>

<p>Here's some dummy code if someone wants to show examples:</p>

<pre><code>data(iris)
head(iris)
require(cluster)
pam(iris[,1:4], 3)
</code></pre>
"
"0.171002565057714","0.188484258731263","  3271","<p>I have seen a few queries on clustering in time series and specifically on clustering, but I don't think they answer my question. </p>

<p><strong>Background:</strong> I want to cluster genes in a time course experiment in yeast. There are four time points say: <em>t1</em>  <em>t2</em>  <em>t3</em>  and  <em>t4</em> and total number of genes <em>G</em>. I have the data in form a matrix <em>M</em> in which the columns represent the treatments (or time points)  <em>t1</em>  <em>t2</em>  <em>t3</em>  and  <em>t4</em>  and the rows represent the genes. Therefore, <em>M</em> is a Gx4 matrix. </p>

<p><strong>Problem:</strong> I want to cluster the genes which behave the same across all time points <em>t1</em>  <em>t2</em>  <em>t3</em>  and  <em>t4</em>  as well as within a particular time point <em>ti</em> , where i is in {1, 2, 3, 4} (In case we cannot do both the clusterings together, the clustering within a time point is more important than clustering across time points). In addition to this, I also want to draw a heatmap.</p>

<p><strong>My Solution:</strong> 
I use the R code below to obtain a heatmap as well as the clusters using <code>hclust</code> function in R (performs hierarchical clustering with euclidean distance)</p>

<pre><code>    row.scaled.expr &lt;- (expr.diff - rowMeans(expr.diff)) / rowSds(expr.diff)

    breaks.expr &lt;- c(quantile(row.scaled.expr[row.scaled.expr &lt; 0],
                               seq(0,1,length=10)[-9]), 0,
                               quantile(row.scaled.expr[row.scaled.expr &gt; 0],
                               seq(0,1,length=10))[-1] )


    blue.red.expr &lt;- maPalette(low = ""blue"", high = ""red"", mid = ""white"",
                     k=length(breaks.expr) - 1)

    pdf(""images/clust.pdf"",
         height=30,width=20,pointsize=20)
    ht1 &lt;- heatmap.2(row.scaled.expr, col = blue.red.expr, Colv = FALSE, key = FALSE, 
      dendrogram = ""row"", scale = ""none"", trace = ""none"",
      cex=1.5, cexRow=1, cexCol=2,
      density.info = ""none"", breaks = breaks.expr, 
      labCol = colnames(row.scaled.expr),
      labRow="""",
      lmat=rbind( c(0, 3), c(2,1), c(0,4) ), lhei=c(0.25, 4, 0.25 ),
      main=expression(""Heat Map""),
      ylab=""Genes in the Microarray"",
      xlab=""Treatments""
      )
    dev.off()
</code></pre>

<p>I recently discovered <code>hopach</code> package in <em>Bioconductor</em> which can be used to estimate the number of clusters. Previously, I was randomly assigning the number of bins for the heatmap and cutting the tree at an appropriate height to get a pre-specified number of clusters. </p>

<p><strong>Possible Problems in my solution:</strong></p>

<ol>
<li>I may be not clustering the genes within a particular treatment and clustering genes only across treatments or vice versa.</li>
<li>There may be better ways of obtaining a heatmap for the pattern I want to see (similar genes within a treatment and across treatments).</li>
<li>There may be better visualization methods which I am not aware of.</li>
</ol>

<p><strong>Note:</strong></p>

<ol>
<li><p><em>csgillespie</em> (moderator) has a more general document on his website in which he discusses all the aspects of time course analysis (including heatmaps and clustering). I would appreciate if you can point me to an articles which describe heatmaps and clustering in detail.</p></li>
<li><p>I have tried the <code>pvclust</code> package, but it complains that <em>M</em> is singular and then it crashes.</p></li>
</ol>
"
"0.0849718577324175","0.0936585811581694","  4694","<p><a href=""http://www.ambion.com/techlib/tn/95/954.html"" rel=""nofollow"">Here</a> is an example of hierarchical clustering of genes in the microarray data using the <strong>weighted pair gene method</strong> in <code>Spotfire</code>. I am not sure how to do this in <code>R</code>. In the <code>hclust</code> function, I see <code>ward"", ""single"", ""complete"", ""average"", ""mcquitty"", ""median"" or ""centroid""</code> as the methods. </p>

<p>Also, lets say I have performed hierarchical clustering and found groups of genes using <code>cuttree</code> method. I wanted to plot the expression of genes in a group across columns (which may represent treatment, time, etc.). And I want to do this for all the groups separately. In a way similar to the <a href=""http://www.bioconductor.org/packages/release/bioc/html/Mfuzz.html"" rel=""nofollow"">Mfuzz</a> package's way of showing clusters.</p>

<p>Can any one please help me?</p>

<p>TIA for any pointers.</p>
"
"0.190792886112389","0.191179778225468","  5087","<p>There are numerous procedures for functional data clustering based on orthonormal basis functions. I have a series of models built with the GAMM models, using the <code>gamm()</code> from the mgcv package in R. For fitting a long-term trend, I use a thin plate regression spline. Next to that, I introduce a CAR1 model in the random component to correct for autocorrelation. For more info, see eg the paper of Simon Wood on <a href=""http://r.789695.n4.nabble.com/attachment/2063352/0/tprs.pdf"">thin plate regression splines</a> or his <a href=""http://rads.stackoverflow.com/amzn/click/1584884746"">book on GAM models</a>.</p>

<p>Now I'm a bit puzzled in how I get the correct coefficients out of the models. And I'm even less confident that the coefficients I can extract, are the ones I should use to cluster different models. </p>

<p>A simple example, using:</p>

<pre><code>#runnable code
require(mgcv)
require(nlme)
library(RLRsim)
library(RColorBrewer)

x1 &lt;- 1:1000
x2 &lt;- runif(1000,10,500)

fx1 &lt;- -4*sin(x1/50)
fx2 &lt;- -10*(x2)^(1/4)
y &lt;- 60+ fx1 + fx2 + rnorm(1000,0,5)

test &lt;- gamm(y~s(x1)+s(x2))
# end runnable code
</code></pre>

<p>Then I can construct the original basis using smoothCon :</p>

<pre><code>#runnable code
um &lt;- smoothCon(s(x1),data=data.frame(x1=x1),
         knots=NULL,absorb.cons=FALSE)
#end runnable code
</code></pre>

<p>Now,when I look at the basis functions I can extract using </p>

<pre><code># runnable code
X &lt;- extract.lmeDesign(test$lme)$X
Z &lt;- extract.lmeDesign(test$lme)$Z

op &lt;- par(mfrow=c(2,5),mar=c(4,4,1,1))
plot(x1,X[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,X[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,8],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,7],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,6],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,5],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,4],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,3],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
par(op)
# end runnable code
</code></pre>

<p>they look already quite different. I can get the final coefficients used to build the smoother by</p>

<pre><code>#runnable code
Fcoef &lt;- test$lme$coef$fixed
Rcoef &lt;- unlist(test$lme$coef$random)
#end runnable code
</code></pre>

<p>but I'm far from sure these are the coefficients I look for. I fear I can't just use those coefficients as data in a clustering procedure. I would really like to know which coefficients are used to transform the basis functions from the ones I get with <code>smoothCon()</code> to the ones I extract from the lme-part of the gamm-object. And if possible, where I can find them. I've read the related articles, but somehow I fail to figure it out myself. All help is appreciated.</p>
"
"NaN","NaN","  5160","<p>I am looking for a good tutorial on clustering data in <code>R</code> using hierarchical dirichlet process (HDP) (one of the recent and popular nonparametric Bayesian methods). </p>

<p>There is <code>DPpackage</code> (IMHO, the most comprehensive of all the available ones) in <code>R</code> for nonparametric Bayesian analysis. But I am unable to understand the examples provided in <code>R News</code> or in the package reference manual well enough to code HDP.</p>

<p>Any help or pointer is appreciated.</p>

<p>A C++ implementation of HDP for topic modeling is available <a href=""http://www.cs.princeton.edu/~blei/topicmodeling.html"">here</a> (please look at the bottom for C++ code)</p>
"
"0.199276670324803","0.21964884255349","  7175","<p>I'm experimenting with classifying data into groups. I'm quite new to this topic, and trying to understand the output of some of the analysis.</p>

<p>Using examples from <a href=""http://www.statmethods.net/advstats/cluster.html"">Quick-R</a>, several <code>R</code> packages are suggested. I have tried using two of these packages (<code>fpc</code> using the <code>kmeans</code> function,  and <code>mclust</code>). One aspect of this analysis that I do not understand is the comparison of the results.</p>

<pre><code># comparing 2 cluster solutions
library(fpc)
cluster.stats(d, fit1$cluster, fit2$cluster)
</code></pre>

<p>I've read through the relevant parts of the <code>fpc</code> <a href=""http://cran.r-project.org/web/packages/fpc/fpc.pdf"">manual</a> and am still not clear on what I should be aiming for. For example, this is the output of comparing two different clustering approaches:</p>

<pre><code>$n
[1] 521

$cluster.number
[1] 4

$cluster.size
[1] 250 119  78  74

$diameter
[1]  5.278162  9.773658 16.460074  7.328020

$average.distance
[1] 1.632656 2.106422 3.461598 2.622574

$median.distance
[1] 1.562625 1.788113 2.763217 2.463826

$separation
[1] 0.2797048 0.3754188 0.2797048 0.3557264

$average.toother
[1] 3.442575 3.929158 4.068230 4.425910

$separation.matrix
          [,1]      [,2]      [,3]      [,4]
[1,] 0.0000000 0.3754188 0.2797048 0.3557264
[2,] 0.3754188 0.0000000 0.6299734 2.9020383
[3,] 0.2797048 0.6299734 0.0000000 0.6803704
[4,] 0.3557264 2.9020383 0.6803704 0.0000000

$average.between
[1] 3.865142

$average.within
[1] 1.894740

$n.between
[1] 91610

$n.within
[1] 43850

$within.cluster.ss
[1] 1785.935

$clus.avg.silwidths
         1          2          3          4 
0.42072895 0.31672350 0.01810699 0.23728253 

$avg.silwidth
[1] 0.3106403

$g2
NULL

$g3
NULL

$pearsongamma
[1] 0.4869491

$dunn
[1] 0.01699292

$entropy
[1] 1.251134

$wb.ratio
[1] 0.4902123

$ch
[1] 178.9074

$corrected.rand
[1] 0.2046704

$vi
[1] 1.56189
</code></pre>

<hr>

<p>My primary question here is to better understand how to interpret the results of this cluster comparison.</p>

<hr>

<p>Previously, I had asked more about the effect of scaling data, and calculating a distance matrix. However that was answered clearly by mariana soffer, and I'm just reorganizing my question to emphasize that I am interested in the intrepretation of my output which is a comparison of two different clustering algorithms.</p>

<p><em>Previous part of question</em>: 
If I am doing any type of clustering, should I always scale data? For example, I am using the function <code>dist()</code> on my scaled dataset as input to the <code>cluster.stats()</code> function, however I don't fully understand what is going on. I read about <code>dist()</code> <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/dist.html"">here</a> and it states that:</p>

<blockquote>
  <p>this function computes and returns the distance matrix computed by using the specified distance measure to compute the distances between the rows of a data matrix.</p>
</blockquote>
"
"0.171002565057714","0.188484258731263","  7250","<p>I'm having difficulty understanding one or two aspects of the cluster package. I'm following the example from <a href=""http://www.statmethods.net/advstats/cluster.html"">Quick-R</a> closely, but don't understand one or two aspects of the analysis. I've included the code that I am using for this particular example.</p>

<pre><code>## Libraries
library(stats)
library(fpc) 

## Data
mydata = structure(list(a = c(461.4210925, 1549.524107, 936.42856, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131.4349206, 0, 762.6110846, 
3837.850406), b = c(19578.64174, 2233.308842, 4714.514274, 0, 
2760.510002, 1225.392118, 3706.428246, 2693.353714, 2674.126613, 
592.7384164, 1820.976961, 1318.654162, 1075.854792, 1211.248996, 
1851.363623, 3245.540062, 1711.817955, 2127.285272, 2186.671242
), c = c(1101.899095, 3.166506463, 0, 0, 0, 1130.890295, 0, 654.5054857, 
100.9491289, 0, 0, 0, 0, 0, 789.091922, 0, 0, 0, 0), d = c(33184.53871, 
11777.47447, 15961.71874, 10951.32402, 12840.14983, 13305.26424, 
12193.16597, 14873.26461, 11129.10269, 11642.93146, 9684.238583, 
15946.48195, 11025.08607, 11686.32213, 10608.82649, 8635.844964, 
10837.96219, 10772.53223, 14844.76478), e = c(13252.50358, 2509.5037, 
1418.364947, 2217.952853, 166.92007, 3585.488983, 1776.410835, 
3445.14319, 1675.722506, 1902.396338, 945.5376228, 1205.456943, 
2048.880329, 2883.497101, 1253.020175, 1507.442736, 0, 1686.548559, 
5662.704559), f = c(44.24828759, 0, 485.9617601, 372.108855, 
0, 509.4916263, 0, 0, 0, 212.9541122, 80.62920455, 0, 0, 30.16525587, 
135.0501384, 68.38023073, 0, 21.9317122, 65.09052886), g = c(415.8909649, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 637.2629479, 0, 0, 
0), h = c(583.2213618, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0), i = c(68206.47387, 18072.97762, 23516.98828, 
13541.38572, 15767.5799, 19756.52726, 17676.00505, 21666.267, 
15579.90094, 14351.02033, 12531.38237, 18470.59306, 14149.82119, 
15811.23348, 14637.35235, 13588.64291, 12549.78014, 15370.90886, 
26597.08152)), .Names = c(""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", 
""h"", ""i""), row.names = c(NA, -19L), class = ""data.frame"")
</code></pre>

<p>Then I standardize the variables:</p>

<pre><code># standardize variables
mydata &lt;- scale(mydata) 

## K-means Clustering 

# Determine number of clusters
wss &lt;- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] &lt;- sum(kmeans(mydata, centers=i)$withinss)
# Q1
plot(1:15, wss, type=""b"", xlab=""Number of Clusters"",  ylab=""Within groups sum of squares"") 

# K-Means Cluster Analysis
fit &lt;- kmeans(mydata, 3) # number of values in cluster solution

# get cluster means 
aggregate(mydata,by=list(fit$cluster),FUN=mean)

# append cluster assignment
mydata &lt;- data.frame(mydata, cluster = fit$cluster)

# Cluster Plot against 1st 2 principal components - vary parameters for most readable graph
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=0, lines=0) # Q2

# Centroid Plot against 1st 2 discriminant functions
plotcluster(mydata, fit$cluster)
</code></pre>

<p>My question is, how can the plot which shows the number of clusters (marked <code>Q1</code> in my code) be related to the actual values (cluster number and variable name) ? </p>

<p>Update: I now understand that the <code>clusplot()</code> function is a bivariate plot, with PCA1 and PCA2. However, I don't understand the link between the PCA components and the cluster groups. What is the relationship between the PCA values and the clustering groups? I've read elsewhere about the link between kmeans and PCA, but I still don't understand how they can be displayed on the same bivariate graph. </p>
"
"0.134352303725115","0.118469775551818","  8152","<p>I have a few questions regarding multiple imputation for nested data. 
Context: I have repeated measures (4 times) from a survey and these are clustered in workplaces (205 workplaces). There are about 180 items on this survey.</p>

<p>q1. Is it possible to take both the repeated measures and the workplace clustering into consideration or do i have to decide for one of the two?</p>

<p>q2. If i can only take into consideration one of the two clusterings (repeated measures vs workplace) which one would you recommend</p>

<p>q3. I have about 10000 observations and about 400 of them have missing values for the workplace. What would you recommend to do in this case? (also i should mention that the 205 workplaces are nested in 17 Organizations - For the moment i use general categories based on the organization: e.g. Organization1-Unclassified). Is there a meaningful way to actually impute these categories? </p>

<p>q4. Would you recommend to use all 180 items for imputation or the items that i intend to use in each of my models? </p>

<p>I use R for analysis and it would be greatly appreciated if you can recommend any packages for multiple imputation for clustered data.</p>

<p>Thanks in advance</p>
"
"0.224814403999137","0.212397697621437"," 10017","<p>I am trying to understand standard error ""clustering"" and how to execute in R (it is trivial in Stata). In R I have been unsuccessful using either <code>plm</code> or writing my own function. I'll use the <code>diamonds</code> data from the <code>ggplot2</code> package.</p>

<p>I can do fixed effects with either dummy variables</p>

<pre><code>&gt; library(plyr)
&gt; library(ggplot2)
&gt; library(lmtest)
&gt; library(sandwich)
&gt; # with dummies to create fixed effects
&gt; fe.lsdv &lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)
&gt; ct.lsdv &lt;- coeftest(fe.lsdv, vcov. = vcovHC)
&gt; ct.lsdv

t test of coefficients:

                      Estimate Std. Error  t value  Pr(&gt;|t|)    
carat                 7871.082     24.892  316.207 &lt; 2.2e-16 ***
factor(cut)Fair      -3875.470     51.190  -75.707 &lt; 2.2e-16 ***
factor(cut)Good      -2755.138     26.570 -103.692 &lt; 2.2e-16 ***
factor(cut)Very Good -2365.334     20.548 -115.111 &lt; 2.2e-16 ***
factor(cut)Premium   -2436.393     21.172 -115.075 &lt; 2.2e-16 ***
factor(cut)Ideal     -2074.546     16.092 -128.920 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>or by de-meaning both left- and right-hand sides (no time invariant regressors here) and correcting degrees of freedom.</p>

<pre><code>&gt; # by demeaning with degrees of freedom correction
&gt; diamonds &lt;- ddply(diamonds, .(cut), transform, price.dm = price - mean(price), carat.dm = carat  .... [TRUNCATED] 
&gt; fe.dm &lt;- lm(price.dm ~ carat.dm + 0, data = diamonds)
&gt; ct.dm &lt;- coeftest(fe.dm, vcov. = vcovHC, df = nrow(diamonds) - 1 - 5)
&gt; ct.dm

t test of coefficients:

         Estimate Std. Error t value  Pr(&gt;|t|)    
carat.dm 7871.082     24.888  316.26 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I can't replicate these results with <code>plm</code>, because I don't have a ""time"" index (i.e., this isn't really a panel, just clusters that could have a common bias in their error terms).</p>

<pre><code>&gt; plm.temp &lt;- plm(price ~ carat, data = diamonds, index = ""cut"")
duplicate couples (time-id)
Error in pdim.default(index[[1]], index[[2]]) : 
</code></pre>

<p>I also tried to code my own covariance matrix with clustered standard error using Stata's explanation of their <code>cluster</code> option (<a href=""http://www.stata.com/support/faqs/stat/cluster.html"">explained here</a>), which is to solve $$\hat V_{cluster} = (X&#39;X)^{-1} \left( \sum_{j=1}^{n_c} u_j&#39;u_j \right) (X&#39;X)^{-1}$$ where $u_j = \sum_{cluster~j} e_i * x_i$, $n_c$ si the number of clusters, $e_i$ is the residual for the $i^{th}$ observation and $x_i$ is the row vector of predictors, including the constant (this also appears as equation (7.22) in Wooldridge's <em>Cross Section and Panel Data</em>). But the following code gives very large covariance matrices. Are these very large values given the small number of clusters I have? Given that I can't get <code>plm</code> to do clusters on one factor, I'm not sure how to benchmark my code.</p>

<pre><code>&gt; # with cluster robust se
&gt; lm.temp &lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)
&gt; 
&gt; # using the model that Stata uses
&gt; stata.clustering &lt;- function(x, clu, res) {
+     x &lt;- as.matrix(x)
+     clu &lt;- as.vector(clu)
+     res &lt;- as.vector(res)
+     fac &lt;- unique(clu)
+     num.fac &lt;- length(fac)
+     num.reg &lt;- ncol(x)
+     u &lt;- matrix(NA, nrow = num.fac, ncol = num.reg)
+     meat &lt;- matrix(NA, nrow = num.reg, ncol = num.reg)
+     
+     # outer terms (X'X)^-1
+     outer &lt;- solve(t(x) %*% x)
+ 
+     # inner term sum_j u_j'u_j where u_j = sum_i e_i * x_i
+     for (i in seq(num.fac)) {
+         index.loop &lt;- clu == fac[i]
+         res.loop &lt;- res[index.loop]
+         x.loop &lt;- x[clu == fac[i], ]
+         u[i, ] &lt;- as.vector(colSums(res.loop * x.loop))
+     }
+     inner &lt;- t(u) %*% u
+ 
+     # 
+     V &lt;- outer %*% inner %*% outer
+     return(V)
+ }
&gt; x.temp &lt;- data.frame(const = 1, diamonds[, ""carat""])
&gt; summary(lm.temp)

Call:
lm(formula = price ~ carat + factor(cut) + 0, data = diamonds)

Residuals:
     Min       1Q   Median       3Q      Max 
-17540.7   -791.6    -37.6    522.1  12721.4 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
carat                 7871.08      13.98   563.0   &lt;2e-16 ***
factor(cut)Fair      -3875.47      40.41   -95.9   &lt;2e-16 ***
factor(cut)Good      -2755.14      24.63  -111.9   &lt;2e-16 ***
factor(cut)Very Good -2365.33      17.78  -133.0   &lt;2e-16 ***
factor(cut)Premium   -2436.39      17.92  -136.0   &lt;2e-16 ***
factor(cut)Ideal     -2074.55      14.23  -145.8   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 1511 on 53934 degrees of freedom
Multiple R-squared: 0.9272, Adjusted R-squared: 0.9272 
F-statistic: 1.145e+05 on 6 and 53934 DF,  p-value: &lt; 2.2e-16 

&gt; stata.clustering(x = x.temp, clu = diamonds$cut, res = lm.temp$residuals)
                        const diamonds....carat..
const                11352.64           -14227.44
diamonds....carat.. -14227.44            17830.22
</code></pre>

<p>Can this be done in R? It is a fairly common technique in econometrics (there's a brief tutorial in <a href=""http://sekhon.berkeley.edu/causalinf/sp2010/section/week7.pdf"">this lecture</a>), but I can't figure it out in R. Thanks!</p>
"
"NaN","NaN"," 10347","<p>I have made a heatmap based upon a regular data matrix in R, the package I use is <code>pheatmap</code>. Regular clustering of my samples is performed by the <code>distfun</code> function within the package. </p>

<p>Now I want to attach a precomputed distance matrix (generated by Unifrac) to my previously generated matrix/heatmap. Is this possible?</p>
"
"0.120168353625222","0.132453235706504"," 13995","<p>I have a very large (36k items) spatial dataset of locations of commercial landuses with their corresponding square footages. I am hoping to use the <code>pam()</code> command in R (from {cluster} package) to form clusters around a set of centers determined by other methods.</p>

<p>I am trying to figure out how to weight the individual points such that large square footages have more attraction to other point than small square footages. My initial thought was to duplicate each point once per 1000 square feet, such that a 
100,000 square foot point would be duplicated 100 times. However, I've read elsewhere that the clustering algorithms are computationally intense - the package documentation suggests using <code>clara()</code> for large datasets, but this method won't allow me to specify the medoids beforehand.</p>

<p>Is there another method for weighted clustering? Am I perhaps going though this all wrong?</p>
"
"0.224814403999137","0.194697889486317"," 14051","<p>Thanks for reading my question.</p>

<p>I have several thousand data points scattered on an (x,y) grid that I am trying to cluster.  The data points are not uniformly distributed across the grid, but are concentrated in certain areas.  I am most interested in identifying the centers of the clusters as representing starting points that minimize the average (Euclidean) distance from a point to the nearest cluster center.</p>

<p>Depending on the specific model and data set, there are between 3 and 7 clusters.  The number of clusters is known beforehand in each instance, and does not need to be determined by an algorithm.  In each situation, some of the centers of the clusters are known (0 to 4 known starting points), but the rest are unknown.  The goal is to identify the centers of the unknown clusters that minimize the average distance to a center across all the data points.</p>

<p>How can I run a clustering algorithm where I can specify a certain number of cluster centers, and solve for the others?  I am using R, and have looked primarily at package mclust.  My thought was that specifying priors for mean and scale to Mclust with very small scale variances for the known centers and very large scale variances (uninformative prior) for the unknowns would be a good approach, but I am having trouble coding it.  The available examples for specifying priors in the package documentation aren't terribly helpful (to me), and might be used for a completely different purpose than what I'm trying to do.</p>

<p>My attempt to code this in R looks something like:</p>

<pre><code># create data matrix of points
x &lt;- rnorm(100, 50, 25)
y &lt;- rnorm(100, 50, 25)
my.data &lt;- cbind(x,y)

# Two known centers, rest are unknown so provide mean of x,y as default starting point
#
# known centers are (25, 10) and (90, 65), assume midpoints of grid for others
x.prior.mean &lt;- c(25, 90, 50, 50, 50)
y.prior.mean &lt;- c(10, 65, 50, 50, 50)

# Provide small scale (variance) for known centers, large scale for unknown centers (uninformative prior)
x.prior.scale &lt;- c( 0.1, 0.1, 100, 100, 100)
y.prior.scale &lt;- c( 0.1, 0.1, 100, 100, 100)

# Create a cluster model with no prior specified
my.clust.noprior &lt;- Mclust(data=my.data, G=5)

# Now add a prior for mean
my.clust.prior &lt;-   Mclust(data=my.data, G=5, prior=priorControl(mean=cbind(x=x.prior.mean, y=y.prior.mean)))

# Compare what I think are the centers of the clusters (mean of parameters).
# The centers in the prior-specified case don't seem to reflect the known centers
my.clust.noprior$parameters$mean
my.clust.prior$parameters$mean

# Commented out, but attempting the following statement that adds scale parameter yields an error:
#    Error in chol.default(priorParams$scale) : non-square matrix in 'chol'
#
# my.clust &lt;-   Mclust(data=my.data, G=5, prior=priorControl(mean=cbind(x=x.prior.mean, y=y.prior.mean), scale=cbind(x.prior.scale, y.prior.scale)))
</code></pre>

<p>Is there a way to accomplish what I'm trying to do?  I am open to using other R packages besides mclust if there's one better suited for this problem.</p>

<p>Thank you</p>
"
"0.134352303725115","0.148087219439773"," 15839","<p>I am interested in determining the optimal number of clusters calculated by the PAM clustering algorithm using the Calinski-Harabasz (CH) index. To that end, I found 2 different R functions calculating CH values for a given clustering, but which returned different results: <a href=""http://rss.acs.unt.edu/Rdoc/library/fpc/html/cluster.stats.html"" rel=""nofollow"">?cluster.stats</a> (in the <a href=""http://cran.r-project.org/web/packages/fpc/index.html"" rel=""nofollow"">fpc package</a>), and <a href=""http://rss.acs.unt.edu/Rdoc/library/clusterSim/html/index.G1.html"" rel=""nofollow"">?index.G1</a> (in the <a href=""http://cran.r-project.org/web/packages/clusterSim/index.html"" rel=""nofollow"">clusterSim package</a>).</p>

<p>First one is called via:</p>

<pre><code>pam.res &lt;- pam(dist.matrix, 2, diss=TRUE)
ch1     &lt;- cluster.stats(dist.matrix, pam.res$clustering, silhouette=TRUE)$ch
</code></pre>

<p>Second one is called via:</p>

<pre><code>ch2 &lt;- index.G1(t(dataframe), pam.res$clustering, d=dist.matrix)
</code></pre>

<p>Data may be found here: <a href=""http://www.megafileupload.com/en/file/327255/dataframe-RData.html"" rel=""nofollow"">dataframe.RData</a>, or here: <a href=""http://www.megafileupload.com/en/file/327262/dist-matrix-RData.html"" rel=""nofollow"">dist.matrix.RData</a> [dead links].</p>

<ul>
<li><p><strong>Can anybody explain the difference between these two CH index calculations to me?</strong></p>

<p>Using <code>cluster.stats()</code>, the highest CH index is obtained for 2 clusters ($\approx32$); while using <code>index.G1()</code>, the highest CH index is obtained for 3 clusters ($\approx60$, and the value for 2 clusters is totally different from the previous, $\approx54$).</p></li>
<li><p><strong>Which function is normally used to calculate the CH index?</strong></p></li>
</ul>
"
"0.158967789576202","0.150187852296528"," 16137","<p>Are there any tools in R that could be used to optimize the allocation of customers amongst possible offers, given constraints? Can anyone give hints/examples on their use? Hope my setup makes sense...</p>

<p>Here is the problem setup:</p>

<p><strong>There are the following:</strong></p>

<ul>
<li>$N$ customers ($N$ is large)</li>
<li>$F$ offers (the offers that can be made to a customer;  $F$ is relatively small)</li>
<li>$P_{nf}$ -- the probability of acceptance of offer $f$ by customer $n$</li>
<li>$D_{nf}$ -- the expected monetary value if customer $n$ accepts offer $f$</li>
<li>$C_f$ -- the cost of offering offer $f$ to any customer</li>
<li>$E_{nf}$ -- the expected profit of offering offer $f$ to customer $n$ ($P_{nf} D_{nf} - C_{f}$)</li>
</ul>

<p><strong>Constraints:</strong></p>

<ul>
<li>Each customer can be allocated to only 1 offer (not every customer need receive anything).</li>
<li>The total number of offers made (call it $T$) between a and b. </li>
<li>The total cost $TC&lt;c$.</li>
</ul>

<p>The percentage of $T$ comprised by each offer $f$ is $\geq d$. This means that sometimes an offer has to be made at least $d$ times. There is one of these rules for each offer.</p>

<p><strong>Goal:</strong></p>

<ul>
<li>Maximize profit.</li>
</ul>

<p>Anything in R?</p>

<p><strong>EDIT:</strong></p>

<ul>
<li><p>I wonder about using something along the lines of <a href=""http://cran.r-project.org/web/packages/Rglpk/index.html"" rel=""nofollow"">http://cran.r-project.org/web/packages/Rglpk/index.html</a>
which appears to support ""large"" problems and the types of constraints I have.
Of course, ""large"" in the context appears to be much less than the millions for N I have. </p></li>
<li><p>One thought I had was to caculate the expected profit for each customer and each offer. Then, run a clustering algorithm (row = customers and columns = expected profit for each promotion) like k-means with k large (e.g. 1,000). Then assign each of the customers into a cluster and use the cluster centroid as the value of the expected profit for the optimizer.</p></li>
</ul>

<p><strong>EDIT AGAIN</strong></p>

<p>For the sake of helping others, the conclusion I came to was to indeed cluster the customers and then use a standard linear solver (I got lpSolve in R to work well). </p>

<p>The other option is to use a non linear approximation. Robert Agnew helped me tremendously on this question - using his dual formulation. See this <a href=""http://www.r-bloggers.com/marketing-optimization-using-the-nonlinear-minimization-function-nlm/"" rel=""nofollow"">post</a>.  His R script is also linked and works great - changing from equality constraints for the offer quantity to inequality constraints requires use of nlminb(). </p>
"
"0.060084176812611","0.0662266178532522"," 18493","<p>I'm working on improving a random forest model.  I've done some clustering of the data using the <code>pvclust</code> package in R.  I use ward and euclidean distances.  My question is how do I go from the results of the clustering analysis to a better random forest.  Do I need to run multiple <code>rf</code> objects with data split along the edges defined in the clustering results or is it something else.  Any suggestions would be appreciated.  Most of my work is in R.</p>
"
"0.104068846970394","0.114707866935281"," 18616","<p>I am going to be using R for text analysis (mostly clustering, classification and some visualization) and was wondering what mechanisms R provides for handling high dimensional, sparse data sets. If I understand correctly, R does provide some packages (e.g., <a href=""http://cran.r-project.org/web/packages/Matrix/"">matrix library</a>) for handling large and sparse matrices - which brings me to my question. </p>

<p>Specifically, I would like to know:</p>

<ol>
<li><p>Which R libraries are most appropriate for storing and processing high dimensional sparse data? Just FYI, my data will fit into memory. </p></li>
<li><p>Do such libraries inter-operate with existing text analysis (clustering/classification) packages? Would I need to convert these sparse data structures to and from data frames if I need to text analysis? Wouldn't that add additional time overhead to the computations?  </p></li>
</ol>

<p>I am fairly new to R, so please excuse me if this sounds vague (or too general). </p>
"
"0.134352303725115","0.148087219439773"," 18969","<p>I want to use model-based clustering to classify 1,225 time series (24 periods each). I have decomposed these time series using the fast Fourier transform and selected the harmonics that explain at least a threshold percentage of time series variance for all time series in the sample. I want to do model-based clustering on the real and imaginary parts for each transform element of a give time series because it would potentially save me from having to account for temporal autocorrelation in model based clustering across periods of a time series. I know that each complex element of the fast Fourier transform is independent from other elements, but I do not know if the imaginary and real parts of the output for a given output element are independent. I would like to know because if they were, it would allow me to maintain the default assumption of the Mclust package in R for model-based clustering that the variables analyzed have a multivariate Gaussian distribution.</p>

<p>NOTE: The input is real-valued, and I have converted from a two-sided to a one-sided spectrum by removing redundant frequency elements and multiplying the positive frequencies (other than the mean component) by two per the advice I got from another StackOverflow answer here: <a href=""http://stackoverflow.com/questions/8264530/how-do-i-calculate-amplitude-and-phase-angle-of-fft-output-from-real-valued-in"">http://stackoverflow.com/questions/8264530/how-do-i-calculate-amplitude-and-phase-angle-of-fft-output-from-real-valued-in</a></p>
"
"0.0849718577324175","0.0936585811581694"," 24540","<p>I wonder whether it is possible to perform within R a clustering of data having mixed data variables. In other words I have a data set containing both numerical and categorical variables within and I'm finding the best way to cluster them. In SPSS I would use two - step cluster. I wonder whether in R can I find a similar techniques. Thanks in advance. I was told about poLCA package, but I'm not sure...</p>
"
"0.224814403999137","0.230097505756556"," 28492","<p>For fun, I tried to replicate the results of <a href=""http://rpproxy.iii.com:9797/MuseSessionID=248c435aa056d82d70d390e949c628fb/MuseHost=rfs.oxfordjournals.org/MusePath/content/22/1/435.abstract"" rel=""nofollow"">Petersen (2009)</a> who deals with the correct estimation of standard errors in finance panel data sets. </p>

<p>In a nutshell, he estimates the following standard regression for a panel data set:</p>

<p>$$
Y_{it} = X_{it} \beta + \epsilon_{it}
$$ </p>

<p>where $\epsilon_{it} = \gamma_i + \eta_{it}$ and $x_{it} = \mu_{i} + \nu_{it}$. Hence, both the residual and the independent variable have a firm-specific component. Petersen goes on to show that this results in biased standard errors when applying the standard OLS. For example, he shows in table 1 of his paper that if both the residual volatility and the variable volatility are driven by 50% by a firm-specific component, the true standard errors are nearly twice as large as the ones given by OLS.</p>

<p>He shows that in a MCS and I reproduced those results in R, as you can see from the code below. Naturally, I asked myself how I would compute the correct standard errors in R and the package of choice seemed to be <code>plm</code>. However, I just don't get the correct results out of it and I don't know what I miss.</p>

<p>Here is my code:</p>

<pre><code>library(plm)
runMCS &lt;- function(runs, nrN, nrT, fracFirmX, fracFirmEps, sd_X, sd_eps, beta) {

  betas    &lt;- numeric(runs)
  se_betas &lt;- numeric(runs)
  panel_betas    &lt;- numeric(runs)
  se_panel_betas &lt;- numeric(runs)

  for (i in 1:runs) {

    #Model epsilon, X, and Y
    eps &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_eps * sqrt(fracFirmEps)), 
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_eps * sqrt(1-fracFirmEps))
    X   &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_X   * sqrt(fracFirmX)),   
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_X   * sqrt(1-fracFirmX))
    Y   &lt;- beta * X + eps

    #Compute regression (OLS)
    reg &lt;- summary(lm(Y ~ X))

    #Save results
    betas[i]    &lt;- reg$coef[2, 1]
    se_betas[i] &lt;- reg$coef[2, 2]

    #Try plm
    df &lt;- data.frame(Firm = rep(1:nrN, each=nrT),
                     Time = rep(1:nrT, times=nrN),
                     Y = Y,
                     X = X)
    preg &lt;- summary(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")) #within is fixed effects
    panel_betas[i]    &lt;- preg$coef[1, 1]
    se_panel_betas[i] &lt;- preg$coef[1, 2]
  }

  return(c(avg_beta = mean(betas), 
           true_se = sd(betas), 
           avg_se = mean(se_betas), 
           avg_clustered = mean(panel_betas),
           se_clustered = mean(se_panel_betas)))

}
MCS_50_50 &lt;- runMCS(50, 500, 10, 0.5, 0.5, 1, 2, 1)
MCS_50_50
     avg_beta       true_se        avg_se avg_clustered  se_clustered 
   1.00503955    0.06020203    0.02825567    1.00433092    0.02985546
</code></pre>

<p>Note that I only run the simulation 50 times here because the plm function slows it down considerably. So basically, it makes virtually no difference if I call <code>lm</code> or <code>plm</code>. I'm pretty confident that I set the <code>index</code> and <code>model</code> option correct after reading the vignette of the package. However, I must miss something here! Interestingly, the package also has the <code>fixef</code> function and if I call that on one run, I get something like  this:</p>

<pre><code>summary(fixef(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")))
1      13.60377     0.44112    30.8391 &lt; 2.2e-16 ***
2    -830.74707     0.44136 -1882.2236 &lt; 2.2e-16 ***
3    -326.96042     0.44137  -740.7840 &lt; 2.2e-16 ***
4     169.16463     0.44246   382.3287 &lt; 2.2e-16 ***
...
</code></pre>

<p>I'm not quite sure how to interpret those results, but here, I get considerably larger standard errors for each firm separately. If I would average those, I would end up with something above 0.44 which is considerably closer to the true standard errors, but still not right.</p>

<p>So, again a very long question from me, sorry for that ;-) Note that I did check answers before and I found this interesting <a href=""http://stats.stackexchange.com/questions/10017/standard-error-clustering-in-r-either-manually-or-in-plm"">link</a>. The white paper that is referred to in the answer is interestingly the same person that implemented the solution on Petersen's <a href=""http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm"" rel=""nofollow"">webpage</a>. So I'm pretty sure that I could get the correct standard errors by implementing Mahmood Arai's solution. But I'm looking for an already implemented and therefore safe option and I just wonder why that plm function does not work.</p>
"
"0.060084176812611","0.0662266178532522"," 28620","<p>I recently read a <a href=""http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/"" rel=""nofollow"">fascinating article</a> describing methods for clustering data without assuming a fixed number of clusters.</p>

<p>The article even includes some sample code, in a mix of Ruby, Python, and R.  However, the meat of the analysis is performed using <a href=""http://scikit-learn.sourceforge.net/dev/index.html"" rel=""nofollow"">scikit-learn</a>'s <a href=""http://scikit-learn.sourceforge.net/dev/modules/mixture.html"" rel=""nofollow"">Dirichlet Process Gaussian Mixture Model</a> to actually find clusters in some sample data taken from McDonald's menu.</p>

<p>Obviously, this a a great excuse to learn some more python, but I'm lazy and would like to find a ready-made R package that can take a dataframe and return clusters, in a manner similar to the <a href=""http://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html"" rel=""nofollow"">kmeans</a> function.  <a href=""http://cran.r-project.org/web/views/Cluster.html"" rel=""nofollow"">A quick search on CRAN</a> reveals the packages <a href=""http://cran.r-project.org/web/packages/dpmixsim/index.html"" rel=""nofollow"">dpmixsim</a> and <a href=""http://cran.r-project.org/web/packages/profdpm/index.html"" rel=""nofollow"">profdpm</a>.  Any suggestions for the best place to start?</p>
"
"NaN","NaN"," 29114","<p>Does anybody know if any package calculates the cubic clustering criterion (CCC) index in R to aid the selection of optimal number of clusters? </p>
"
"0.060084176812611","0.0662266178532522"," 31565","<p>I would like to make a heatmap with row clustering based on cosine distances. I'm using R and <code>heatmap.2()</code> for making the figure. I can see that there's a <code>dist</code> parameter in <code>heatmap.2</code> but I cannot find a function to generate the cosine dissimilarity matrix. The builtin <code>dist</code> function doesn't support cosine distances, I also found a package called <code>arules</code> with a <code>dissimilarity()</code> function but it only works on binary data.</p>
"
"NaN","NaN"," 32239","<p>As described in Merlo et al (<a href=""http://www.ncbi.nlm.nih.gov/pubmed/16537344"" rel=""nofollow"">J Epidem Comm Health 2006</a>), the 95% credible interval for MOR is calculated using MCMC. MOR is defined as $\exp(\sqrt{2\sigma^2}\times 0.675)$, where $\sigma$ is the level-2 variance of the random intercept $u$ from a null model of a hierarchical logistic regression.  </p>

<p>Does anyone have an idea of how to write a program for an Markov chain Monte Carlo to calculate the standard error of the  median odds ratio (MOR) using <a href=""http://cran.r-project.org/web/packages/rjags/index.html"" rel=""nofollow"">rjags</a>?<br>
My dependent variable is outcome(alive/dead) and the clustering (level2)variable is Hospital. There are 140 hospitals and would like to see variations in outcome between hospitals. Other risk factors will be included later as independent level1 variables.</p>
"
"0.134352303725115","0.118469775551818"," 46798","<p>I am looking for a clustering algorithm. My idealized dataset looks like this:<br>
<img src=""http://i.stack.imgur.com/pqEpC.png"" alt=""http://i.stack.imgur.com/bSlNU.png""></p>

<p>The clustering result should look like the Rapidminer density plot:<br>
<img src=""http://i.stack.imgur.com/uMRQ4.png"" alt=""http://i.stack.imgur.com/Sk3xK.png""> </p>

<p>Means 3 or 4 clusters should be the clustering result: One Cluster (1|1) to (5|6), one or two for the points on Z = 5 and one for the overlaying area (6|1) to (11|6).</p>

<p>Based on ""density"" I gave the <code>DBSCAN</code> R package a shot. The best result I could get was the one in the above picture. So not exactly what I expected, as the overlaying area was not recognized as a separate cluster. Any ideas which algorithm would provide the results I expect?
<img src=""http://i.stack.imgur.com/pqEpC.png"" alt=""enter image description here""></p>
"
"0.169943715464835","0.187317162316339"," 46821","<p>I am producing a script for creating bootstrap samples from the <code>cats</code> dataset (from the <code>-MASS-</code> package). </p>

<p>Following the Davidson and Hinkley textbook [1] I ran a simple linear regression and adopted a fundamental non-parametric procedure for bootstrapping from iid observations, namely <strong>pairs resampling</strong>.</p>

<p>The original sample is in the form:</p>

<pre><code>Bwt   Hwt

2.0   7.0
2.1   7.2

...

1.9    6.8
</code></pre>

<p>Through an univariate linear model we want to explain cats hearth weight through their brain weight. </p>

<p>The code is:</p>

<pre><code>library(MASS)
library(boot)


##################
#   CATS MODEL   #
##################

cats.lm &lt;- glm(Hwt ~ Bwt, data=cats)
cats.diag &lt;- glm.diag.plots(cats.lm, ret=T)


#######################
#   CASE resampling   #
#######################

cats.fit &lt;- function(data) coef(glm(data$Hwt ~ data$Bwt)) 
statistic.coef &lt;- function(data, i) cats.fit(data[i,]) 

bootl &lt;- boot(data=cats, statistic=statistic.coef, R=999)
</code></pre>

<p>Suppose now that there exists a clustering variable <code>cluster = 1, 2,..., 24</code> (for instance, each cat belongs to a given litter). For simplicity, suppose that data are balanced: we have 6 observations for each cluster. Hence, each of the 24 litters is made up of 6 cats (i.e. <code>n_cluster = 6</code> and <code>n = 144</code>).</p>

<p>It is possible to create a fake <code>cluster</code> variable through:</p>

<pre><code>q &lt;- rep(1:24, times=6)
cluster &lt;- sample(q)
c.data &lt;- cbind(cats, cluster)
</code></pre>

<p>I have two related questions:</p>

<p>How to simulate samples in accordance with the (clustered) dataset strucure? That is, <strong>how to resample at the cluster level?</strong> I would like to sample the clusters with replacement and to set the observations within each selected cluster as in the original dataset (i.e. sampling with replacenment the clusters and without replacement the observations within each cluster). </p>

<p>This is the strategy proposed by Davidson (p. 100). 
Suppose we draw <code>B = 100</code> samples. Each of them should be composed by 24 possibly recurrent clusters (e.g. <code>cluster = 3, 3, 1, 4, 12, 11, 12, 5, 6, 8, 17, 19, 10, 9, 7, 7, 16, 18, 24, 23, 11, 15, 20, 1</code>), and each cluster should contain the same 6 observations of the original dataset. How to do that in <code>R</code>? (either with or without the <code>-boot-</code> package.) Do you have alternative suggestions for proceeding?</p>

<p>The second question concerns the initial regression model. Suppose I adopt a <strong>fixed-effects model</strong>, with cluster-level intercepts. <strong>Does it change the resampling procedure</strong> adopted? </p>

<p>[1] Davidson, A. C., Hinkley, D. V. (1997). <em>Bootstrap methods and their applications</em>. Cambridge University press.</p>
"
"0.180252530437833","0.154528774990922"," 46978","<p>I am fitting a <em>Fixed-Effects</em> model, with intercepts at <code>cluster</code> level.</p>

<p>One of the most direct ways is probably to use the <code>-plm-</code> package. Another well-known possibility is to apply OLS (i.e. to adopt <code>-lm-</code>) to the <em>demeaned data</em>, where the means are taken at the clustering level.</p>

<p>This second approach is usually referred to as the <strong>within transformation</strong>. It is quite convenient from a computational standpoint, because we are still controlling unobserved heterogeneity at clustering level, but we do not need to estimate all the time-fixed intercepts.</p>

<p>I have tried both of these approaches, and I came to a strange result. In practice, the coefficient of the regressor of interest, <code>x</code>, is the same in both cases. However, its standard error (and actually all the other relevant quantities of the regression: R squared, F test, etc.) is different.</p>

<p>Please, notice that I have carefully read both the <em>R documentation</em> about <code>-plm-</code> and the <a href=""http://www.google.it/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CD4QFjAB&amp;url=http://www.jstatsoft.org/v27/i02/paper&amp;ei=7f3mUP_0DYrXtAaD7oDADw&amp;usg=AFQjCNFu_xrsnFYsC8j8DDh9mRQnoyQ6jg&amp;bvm=bv.1355534169,d.bGE"" rel=""nofollow"">related paper of the authors</a>, where it is stated that the package apply the <em>within transformation</em> and then apply OLS, as I did...</p>

<p>The R script is:</p>

<pre><code># set seed, load packages, create fake sample

set.seed(999)
library(plyr)
library(plm)

dat &lt;- expand.grid(id=factor(1:3), cluster=factor(1:6))
dat &lt;- cbind(dat, x=runif(18), y=runif(18, 2, 5))


############################
#   FE model using -plm-   #
############################

# model fit  
fe.1 &lt;- plm(y ~ x, data=dat, index=""cluster"", model=""within"")

# estimated coefficient and standard error of x
b.1 &lt;- summary(fe.1)$coefficients[,1]
    se.1 &lt;- summary(fe.1)$coefficients[,2]


######################################
#   OLS on within-transformed data   #
######################################

# augmenting data frame with cluster-mean centered variables 
dat.2 &lt;- ddply(dat, .(cluster), transform, dem_x=x-mean(x), dem_y=y-mean(y))

# model fit
fe.2 &lt;- lm(dem_y ~ dem_x - 1, data=dat.2)

# estimated coefficient and standard error of x
b.2 &lt;- summary(fe.2)$coefficients[1,1]
    se.2 &lt;- summary(fe.2)$coefficients[1,2]


#########################
#   models comparison   #
#########################

b.1; b.2
se.1; se.2

summary(fe.1)
summary(fe.2)
</code></pre>

<p>Notice that in the second model it is necessary to manually eliminate the intercept from the model. </p>
"
"0.147175574806061","0.135184517608969"," 49243","<p>R's randomForest package can not handle factor with more than 32 levels. When it is given more than 32 levels, it emits an error message:</p>

<blockquote>
  <p>Can not handle categorical predictors with more than 32 categories.</p>
</blockquote>

<p>But the data I have has several factors. Some of them have 1000+ levels and some of them have 100+. It even has 'state' of united states which is 52. </p>

<p>So, here's my question.</p>

<ol>
<li><p>Why is there such limitation? randomForest refuse to run even for the simple case.</p>

<pre><code>&gt; d &lt;- data.frame(x=factor(1:50), y=1:50)
&gt; randomForest(y ~ x, data=d)
  Error in randomForest.default(m, y, ...) : 
  Can not handle categorical predictors with more than 32 categories.
</code></pre>

<p>If it is simply due to memory limitation, how can scikit learn's randomForeestRegressor run with more than 32 levels?</p></li>
<li><p>What is the best way to handle this problem? Suppose that I have X1, X2, ..., X50 independent variables and Y is dependent variable. And suppose that X1, X2 and X3 has more than 32 levels. What should I do?</p>

<p>What I'm thinking of is running clustering algorithm for each of X1, X2 and X3 where distance is defined as difference in Y. I'll run three clusterings as there are three problematic variables. And in each clustering, I wish I can find similar levels. And I'll merge them.</p>

<p>How does this sound?</p></li>
</ol>
"
"0.104068846970394","0.114707866935281"," 50011","<p>I'm looking for some assistance in statistical analysis with R, but also some general stats advice.</p>

<p>I am analysing cardiac phenotype data by comparing 2 groups. The 2 groups are unmatched individuals, but within each group, they are clustered in family subgroups (of between 1 and ~6).</p>

<p>I want to report the difference in prevalence of a specific ECG appearance (binary - i.e. either present or absent in each individual) between the 2 groups.</p>

<p>For example:</p>

<blockquote>
  <p>Group 1 consists of 157 individuals comprised of 41 family clusters. 
  Group 2 consists of 463 individuals comprised of 163 family clusters. 
  Prevalence of x in Group 1 = 22.9% Prevalence of x in Group 2 = 24.6%. 
  Group 1 are cases and Group 2 controls (i.e. not randomized and defined by phenotype in an observational study). </p>
</blockquote>

<p>What test is most appropriate in this circumstance, and which package in R provides the easiest way to account for the clustering of relatives within families?</p>

<p>Having looked around, I have found:</p>

<ul>
<li>Ratio estimate chi-square test</li>
<li>Generalized estimating equation</li>
</ul>

<p>But I have no experience of either of these techniques, and can't find any examples of their use in R.</p>

<p>Any advice on how best to proceed?</p>

<p>EDIT: See comment below for update.
I believe the Donner (1989) chi-square correction may be the most appropriate (provided by R function donner).  Second opinions and correct use of R command appreciated. Thanks.</p>
"
"0.104068846970394","0.114707866935281"," 54522","<p>I am working for a big company with a restrictive internet policy and Excel-addicted colleagues. I am currently working on evolution of market correlations which implies some statistics, data analysis, clustering, data visualization ... From what I have seen on the internet it's not a good idea to do it in Excel. (see here a general study: <a href=""http://stats.stackexchange.com/questions/3392/excel-as-a-statistics-workbench/3398#3398"">Excel as a statistics workbench</a>)</p>

<p>After a struggle of 2 weeks, I have finally got from IT a working version of R and some interesting packages. My market data are stocked in a .txt file, I work on it with R and create a results.txt file, then I load the results.txt file in Excel and I plot what my boss wants. </p>

<p>I admit that Excel is useful for manipulating a lot of data sets and graphs at the same place. It's the only good point compared to R for what I want to do. I think my cheap .txt solution to do calculations in R is correct and simple ... (for the anecdote things like Rexcel to connect R and Excel are forbidden where I work - don't ask why - so I have tried a macro which create a .bat to launch R and do the calculation; too complex for my colleagues)</p>

<p>But for data visualization Excel is very poor; I really miss some graphs I have in R.
Dendograms, boxplots, histograms, correlation circles, summarized correlations, and heatmaps are very interesting for me, but not available. So my question is how to get them in Excel ? (Remember the strict internet policy; I can't download any add-ins). Is there a (easy) way to plot complex things with macro or workbooks ? Do you have some sources?</p>
"
"NaN","NaN"," 55147","<p>I'm wondering if there is a good way to calculate the clustering criterion based on BIC formula, for a k-means output in R? I'm a bit confused as to how to calculate that BIC so that I can compare it with other clustering models. Currently I'm using the stats package implementation of k-means.</p>
"
"0.224814403999137","0.194697889486317"," 55232","<p>I am trying to fit a finite mixture model to a dependent variable which is bounded (practically) between -0.594 and 1 (theoretically, the latent variable is bounded between -Inf - 1). The data are also bimodal, with a large number of values at '1'. The objective of the analysis is prediction of the dependent variable.</p>

<p>My current approach has been to fit a mixture of normal distributions using the <code>flexmix</code> package in R, but I'd really like to account for the bounded nature of the data, as a recent study found this to be important (I also choose k=3 components based on this study). Using <code>flexmix</code> for truncated data appears non-trivial, as suggested <a href=""http://r.789695.n4.nabble.com/model-based-clustering-with-flexmix-td908418.html"" rel=""nofollow"">here</a>.</p>

<p>Is there an R package that will permit mixture models with bounded data? I've noticed that actually predicted values do not seem to fall outside the bounded range; i.e. predicted values are not in practice greater than 1. Is this just a fluke of my data, or is it a feature of the methods I've used? Is the bounding even a problem in this context?</p>

<p>As an alternative, I've tried transforming the data by simply taking 1-the dependent variable, thereby giving me a (zero-inflated) variable bounded by 0 and Inf which I have tried to model as a mixture of zero-inflated poisson models but I get the error:    </p>

<pre><code>Error in FLXfit(model = model, concomitant = concomitant, control = control,  
: 1 Log-    likelihood: NaN
</code></pre>

<p>Is it possible to model non-integers with the poisson family in this context? Any suggestions or thoughts would be greatly appreciated, I'm very new to mixture modelling and indeed GLMs etc.</p>

<p>Here's some simulated data: <a href=""https://dl.dropbox.com/u/65336009/mydata.csv"" rel=""nofollow"">https://dl.dropbox.com/u/65336009/mydata.csv</a></p>

<p>Here's my code:</p>

<pre><code>require(flexmix)
require(ggplot2)
mydata &lt;- data.frame(read.csv(""mydata.csv"", head=T))
attach(mydata)

#Plot of y var
summary(y)
ggplot(mydata, aes(y)) + geom_histogram(binwidth = .1)

#Simplified example of my current 'best' approach####
m1 &lt;- flexmix(y ~ x1 + x2 + x3,
              data = mydata,
              k = 3)

#Predict cluster membership
clusters &lt;- data.frame(clusters(m1, newdata = mydata))

#Predict y
a &lt;- data.frame(predict(m1, newdata = mydata))

#Select prediction based on predicted cluster membership
mydata$flexmix.norm &lt;- ifelse(clusters[,1]==1, a[,1],
                                   ifelse(clusters[,1] == 2,
                                          a[,2], a[,3]))
    print(max(mydata$flexmix.norm))

#Plot predicted values
ggplot(mydata, aes(flexmix.norm)) + geom_histogram(binwidth = .1)

#Maybe it's more natural to model as 1 - y, which is bounded (0,Inf) ####
y.d &lt;- 1 - y
ggplot(mydata, aes(y.d)) + geom_histogram(binwidth = .1)

#Error here ***
m2 &lt;- flexmix(y.d ~ x1 + x2 + x3,
              data = mydata,
              k = 3,
              model=FLXMRziglm(family=""poisson""))
rm2 = refit(m2)

#Predict cluster membership
clusters &lt;- NULL
clusters &lt;- data.frame(clusters(m2, newdata = mydata))

#Predict y (note back on original scale of y)
b &lt;- 1 - data.frame(predict(m2, newdata = mydata))

#Select prediction based on predicted cluster membership
preds$flexmix.pois &lt;- ifelse(clusters[,1]==1, b[,1],
                              ifelse(clusters[,1] == 2,
                                     b[,2], b[,3]))

ggplot(mydata, aes(flexmix.pois)) + geom_histogram(binwidth = .1)
</code></pre>

<p>Thanks</p>
"
"0.104068846970394","0.114707866935281"," 58725","<p>I am experimenting with creating a distance matrix between time series for clustering and similarity searching. The main reference I am using is for the Similarity procedure in SAS (<a href=""http://support.sas.com/rnd/app/ets/papers/similarityanalysis.pdf%E2%80%8E"" rel=""nofollow"">Paper</a>). I would like to conduct the analysis in R using the <code>dtw</code> <a href=""http://dtw.r-forge.r-project.org/"" rel=""nofollow"">package</a>.</p>

<p>What I am confused about it the application of DTW to series of different lengths.</p>

<p>1) Is this possible?
2) Any hints on how to do it with the R package?</p>

<p>Regarding question #2, trying to calculate a distance matrix on a matrix of series that differ in length immediately fails:</p>

<pre><code>dist_mat&lt;-dist(appliances_t,method=""DTW"")

Error in dtw(distance.only = TRUE, ...) : 
  No warping paths exists that is allowed by costraints
</code></pre>

<blockquote>
  <p>dput(appliances_t)</p>
</blockquote>

<pre><code>structure(c(1L, 14L, 1L, 1L, 2L, 1L, 1L, 7L, 1L, 33L, 20L, 1L, 
1L, 8L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 14L, 
0L, 1L, 0L, 1L, 1L, 6L, 1L, 32L, 20L, 1L, 2L, 8L, 0L, 0L, 2L, 
1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 19L, 0L, 3L, 6L, 1L, 1L, 
7L, 1L, 42L, 27L, 1L, 3L, 10L, 0L, 1L, 3L, 1L, 3L, 1L, 1L, 1L, 
0L, 0L, 1L, 1L, 22L, 1L, 7L, 4L, 1L, 5L, 7L, 1L, 51L, 32L, 5L, 
4L, 12L, 1L, 1L, 9L, 5L, 7L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 33L, 
1L, 6L, 4L, 3L, 5L, 5L, 1L, 80L, 49L, 5L, 5L, 19L, 1L, 1L, 9L, 
5L, 7L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 28L, 1L, 7L, 8L, 3L, 5L, 
6L, 3L, 63L, 41L, 5L, 6L, 15L, 1L, 1L, 9L, 5L, 8L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 30L, 1L, 10L, 22L, 10L, 8L, 7L, 5L, 70L, 44L, 
8L, 7L, 16L, 1L, 1L, 12L, 8L, 11L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
32L, 2L, 13L, 30L, 10L, 10L, 11L, 12L, 74L, 47L, 10L, 8L, 18L, 
2L, 1L, 16L, 10L, 14L, 1L, 1L, 1L, 1L, 2L, 1L, 5L, 23L, 4L, 6L, 
30L, 10L, 5L, 9L, 12L, 53L, 33L, 5L, 9L, 12L, 5L, 1L, 9L, 5L, 
8L, 5L, 5L, 5L, 3L, 5L, 5L, 7L, 27L, 7L, 9L, 22L, 14L, 7L, 11L, 
12L, 61L, 36L, 7L, 10L, 14L, 7L, 1L, 11L, 7L, 11L, 8L, 8L, 8L, 
6L, 7L, 8L, 14L, 27L, 14L, 13L, 38L, 19L, 10L, 7L, 15L, 61L, 
37L, 10L, 11L, 14L, 14L, 1L, 14L, 10L, 14L, 12L, 12L, 12L, 9L, 
14L, 12L, 14L, 38L, 14L, 14L, 44L, 10L, 11L, 9L, 20L, 86L, 54L, 
11L, 12L, 20L, 14L, 1L, 16L, 11L, 14L, 22L, 22L, 22L, 16L, 14L, 
22L, 14L, 27L, 14L, 16L, 42L, 13L, 13L, 12L, 11L, 61L, 39L, 13L, 
13L, 14L, 14L, 1L, 19L, 13L, 18L, 22L, 22L, 22L, 16L, 14L, 22L, 
20L, 28L, 19L, 10L, 40L, 16L, 9L, 12L, 14L, 66L, 41L, 9L, 14L, 
16L, 20L, 1L, 13L, 9L, 12L, 23L, 23L, 23L, 18L, 20L, 23L, 27L, 
16L, 27L, 10L, 60L, 19L, 8L, 11L, 20L, 39L, 23L, 8L, 15L, 9L, 
27L, 4L, 12L, 8L, 11L, 30L, 30L, 30L, 22L, 27L, 30L, 14L, 24L, 
14L, 14L, 136L, 21L, 11L, 4L, 21L, 56L, 33L, 11L, 16L, 12L, 14L, 
3L, 16L, 11L, 14L, 42L, 42L, 42L, 31L, 14L, 42L, 18L, 23L, 18L, 
16L, 206L, 14L, 14L, 3L, 25L, 54L, 33L, 14L, 17L, 12L, 18L, 2L, 
20L, 14L, 18L, 22L, 22L, 22L, 16L, 18L, 22L, 24L, 28L, 23L, 25L, 
398L, 14L, 20L, 6L, 16L, 65L, 40L, 20L, 18L, 16L, 24L, 4L, 29L, 
20L, 28L, 28L, 28L, 28L, 21L, 24L, 28L, 27L, 18L, 27L, 20L, 380L, 
19L, 16L, 5L, 16L, 40L, 25L, 16L, 19L, 10L, 27L, 3L, 25L, 16L, 
21L, 39L, 39L, 39L, 28L, 27L, 39L, 32L, 19L, 30L, 21L, 406L, 
22L, 18L, 3L, 20L, 42L, 27L, 18L, 20L, 10L, 32L, 3L, 27L, 18L, 
24L, 43L, 43L, 43L, 32L, 32L, 43L, 20L, 21L, 20L, 22L, 504L, 
33L, 19L, 7L, 27L, 49L, 31L, 19L, 21L, 12L, 20L, 2L, 28L, 19L, 
27L, 49L, 49L, 49L, 37L, 20L, 49L, 20L, 20L, 20L, 16L, 682L, 
28L, 14L, 7L, 41L, 48L, 30L, 14L, 22L, 12L, 20L, 2L, 20L, 14L, 
18L, 33L, 33L, 33L, 25L, 20L, 33L, 27L, 13L, 27L, 19L, 374L, 
30L, 14L, 8L, 32L, 29L, 19L, 14L, 23L, 6L, 27L, 3L, 21L, 14L, 
20L, 32L, 32L, 32L, 23L, 27L, 32L, 32L, 20L, 32L, 19L, 489L, 
32L, 14L, 7L, 33L, 47L, 28L, 14L, 24L, 11L, 32L, 3L, 21L, 14L, 
20L, 42L, 42L, 42L, 32L, 32L, 42L, 49L, 16L, 49L, 27L, 628L, 
23L, 21L, 13L, 36L, 35L, 21L, 21L, 25L, 8L, 49L, 4L, 32L, 21L, 
30L, 51L, 51L, 51L, 40L, 49L, 51L, 41L, 27L, 41L, 19L, 791L, 
27L, 15L, 14L, 27L, 62L, 39L, 15L, 26L, 14L, 41L, 1L, 22L, 15L, 
20L, 80L, 80L, 80L, 61L, 41L, 80L, 44L, 33L, 44L, 20L, 898L, 
27L, 16L, 16L, 29L, 77L, 48L, 16L, 27L, 18L, 44L, 1L, 25L, 16L, 
21L, 63L, 63L, 63L, 48L, 44L, 63L, 47L, 21L, 46L, 13L, 439L, 
38L, 10L, 21L, 30L, 51L, 32L, 10L, 28L, 12L, 47L, 2L, 14L, 10L, 
14L, 70L, 70L, 70L, 51L, 47L, 70L, 33L, 32L, 33L, 18L, 515L, 
27L, 14L, 22L, 43L, 75L, 47L, 14L, 29L, 18L, 33L, 2L, 20L, 14L, 
20L, 74L, 74L, 74L, 54L, 33L, 74L, 36L, 33L, 36L, 16L, 450L, 
28L, 14L, 17L, 30L, 76L, 48L, 14L, 30L, 18L, 36L, 3L, 20L, 14L, 
18L, 53L, 53L, 53L, 40L, 36L, 53L, 37L, 33L, 37L, 20L, 726L, 
16L, 16L, 14L, 32L, 78L, 48L, 16L, 31L, 19L, 37L, 3L, 24L, 16L, 
21L, 61L, 61L, 61L, 45L, 37L, 61L, 54L, 48L, 53L, 13L, 1069L, 
24L, 10L, 17L, 19L, 109L, 68L, 10L, 32L, 27L, 54L, 3L, 14L, 10L, 
14L, 61L, 61L, 61L, 45L, 54L, 61L, 39L, 51L, 37L, 13L, 889L, 
23L, 11L, 19L, 28L, 120L, 76L, 11L, 33L, 28L, 39L, 3L, 16L, 11L, 
14L, 86L, 86L, 86L, 63L, 39L, 86L, 41L, 43L, 40L, 16L, 1083L, 
28L, 13L, 22L, 28L, 97L, 62L, 13L, 34L, 22L, 41L, 3L, 19L, 13L, 
18L, 61L, 61L, 61L, 46L, 41L, 61L, 23L, 50L, 23L, 15L, 1110L, 
18L, 13L, 25L, 32L, 118L, 73L, 12L, 35L, 28L, 23L, 2L, 19L, 13L, 
17L, 66L, 66L, 66L, 49L, 23L, 66L, 33L, 80L, 33L, 10L, 803L, 
19L, 7L, 32L, 20L, 185L, 116L, 7L, 36L, 43L, 33L, 7L, 11L, 7L, 
11L, 39L, 39L, 39L, 28L, 33L, 39L, 33L, 63L, 33L, 14L, 828L, 
21L, 12L, 33L, 20L, 146L, 91L, 12L, 37L, 34L, 33L, 9L, 18L, 12L, 
16L, 56L, 56L, 56L, 42L, 33L, 56L, 40L, 68L, 40L, 11L, 1001L, 
20L, 9L, 29L, 25L, 157L, 97L, 9L, 38L, 35L, 40L, 11L, 14L, 9L, 
13L, 54L, 54L, 54L, 41L, 40L, 54L, 25L, 80L, 24L, 20L, 1281L, 
13L, 16L, 32L, 23L, 181L, 113L, 16L, 39L, 43L, 25L, 3L, 22L, 
16L, 21L, 65L, 65L, 65L, 48L, 25L, 65L, 27L, 68L, 27L, 24L, 918L, 
20L, 20L, 32L, 14L, 155L, 96L, 20L, 40L, 35L, 27L, 1L, 29L, 20L, 
27L, 40L, 40L, 40L, 29L, 27L, 40L, 31L, 85L, 30L, 16L, 974L, 
16L, 14L, 45L, 23L, 194L, 120L, 13L, 41L, 47L, 31L, 4L, 20L, 
14L, 18L, 42L, 42L, 42L, 31L, 31L, 42L, 30L, 76L, 29L, 23L, 524L, 
27L, 19L, 51L, 18L, 172L, 109L, 19L, 42L, 40L, 30L, 7L, 28L, 
19L, 27L, 49L, 49L, 49L, 36L, 30L, 49L, 19L, 74L, 19L, 23L, 486L, 
33L, 20L, 40L, 30L, 168L, 105L, 19L, 43L, 39L, 19L, 3L, 28L, 
20L, 27L, 48L, 48L, 48L, 36L, 19L, 48L, 28L, 80L, 28L, 24L, 380L, 
21L, 20L, 51L, 39L, 185L, 115L, 20L, 44L, 43L, 28L, 3L, 29L, 
20L, 28L, 29L, 29L, 29L, 21L, 28L, 29L, 21L, 131L, 21L, 33L, 
456L, 32L, 27L, 54L, 27L, 300L, 188L, 27L, 45L, 70L, 21L, 3L, 
40L, 27L, 38L, 47L, 47L, 47L, 34L, 21L, 47L, 39L, 139L, 38L, 
37L, 360L, 33L, 30L, 63L, 37L, 315L, 198L, 30L, 46L, 74L, 39L, 
3L, 47L, 30L, 41L, 35L, 35L, 35L, 27L, 39L, 35L, 48L, 197L, 48L, 
30L, 488L, 33L, 24L, 90L, 38L, 449L, 280L, 24L, 47L, 104L, 48L, 
8L, 36L, 24L, 33L, 62L, 62L, 62L, 47L, 48L, 62L, 32L, 175L, 32L, 
36L, 622L, 48L, 29L, 86L, 39L, 401L, 249L, 29L, 48L, 95L, 32L, 
3L, 44L, 29L, 40L, 77L, 77L, 77L, 58L, 32L, 77L, 47L, 147L, 47L, 
58L, 522L, 51L, 47L, 63L, 53L, 336L, 211L, 47L, 49L, 79L, 47L, 
3L, 69L, 47L, 64L, 51L, 51L, 51L, 40L, 47L, 51L, 48L, 212L, 48L, 
47L, 306L, 43L, 36L, 76L, 60L, 483L, 304L, 35L, 50L, 114L, 48L, 
3L, 54L, 36L, 50L, 75L, 75L, 75L, 56L, 48L, 75L, 48L, 175L, 48L, 
48L, 628L, 50L, 39L, 106L, 48L, 401L, 249L, 39L, 51L, 95L, 48L, 
0L, 58L, 39L, 52L, 76L, 76L, 76L, 57L, 48L, 76L, 68L, 186L, 67L, 
56L, 320L, 80L, 47L, 85L, 59L, 426L, 266L, 47L, 52L, 100L, 68L, 
1L, 68L, 47L, 63L, 78L, 78L, 78L, 59L, 68L, 78L, 76L, 144L, 75L, 
48L, 646L, 63L, 39L, 69L, 92L, 332L, 207L, 39L, 53L, 77L, 76L, 
2L, 58L, 39L, 52L, 109L, 109L, 109L, 80L, 76L, 109L, 62L, 140L, 
61L, 61L, 790L, 68L, 48L, 83L, 72L, 323L, 200L, 48L, 54L, 76L, 
62L, 3L, 72L, 48L, 66L, 120L, 120L, 120L, 91L, 62L, 120L, 73L, 
170L, 72L, 54L, 556L, 80L, 43L, 96L, 78L, 390L, 244L, 43L, 55L, 
91L, 73L, 6L, 63L, 43L, 60L, 97L, 97L, 97L, 73L, 73L, 97L, 116L, 
157L, 114L, 51L, 552L, 68L, 43L, 113L, 91L, 357L, 224L, 43L, 
56L, 84L, 116L, 7L, 63L, 43L, 58L, 118L, 118L, 118L, 88L, 116L, 
118L, 91L, 225L, 91L, 58L, 806L, 85L, 47L, 96L, 78L, 512L, 321L, 
47L, 57L, 119L, 91L, 5L, 68L, 47L, 65L, 185L, 185L, 185L, 138L, 
91L, 185L, 97L, 103L, 96L, 95L, 855L, 76L, 75L, 108L, 96L, 236L, 
148L, 75L, 58L, 56L, 97L, NA, 112L, 76L, 104L, 146L, 146L, 146L, 
109L, 97L, 146L, 113L, 128L, 112L, 97L, 1050L, 74L, 78L, 116L, 
86L, 294L, 184L, 78L, 59L, 69L, 113L, NA, 118L, 78L, 110L, 157L, 
157L, 157L, 117L, 113L, 157L, 96L, 140L, 96L, 140L, 1120L, 80L, 
112L, 123L, 83L, 321L, 199L, 112L, 60L, 75L, 96L, NA, 167L, 112L, 
155L, 181L, 181L, 181L, 137L, 96L, 181L, 120L, 146L, 119L, 126L, 
908L, 131L, 99L, 192L, 92L, 334L, 209L, 99L, 61L, 78L, 120L, 
NA, 150L, 99L, 138L, 155L, 155L, 155L, 117L, 120L, 155L, 109L, 
150L, 107L, 105L, 1068L, 139L, 84L, 373L, 150L, 343L, 214L, 83L, 
62L, 80L, 109L, NA, 126L, 84L, 117L, 194L, 194L, 194L, 146L, 
109L, 194L, 105L, 158L, 104L, 151L, 1696L, 197L, 120L, 195L, 
158L, 363L, 227L, 120L, 63L, 84L, 105L, NA, 181L, 121L, 168L, 
172L, 172L, 172L, 129L, 105L, 172L, 115L, 160L, 115L, 125L, 1658L, 
175L, 99L, 213L, 224L, 366L, 228L, 99L, 64L, 86L, 115L, NA, 150L, 
100L, 138L, 168L, 168L, 168L, 126L, 115L, 168L, 188L, 179L, 186L, 
132L, 1872L, 147L, 106L, 215L, 199L, 408L, 256L, 106L, 65L, 96L, 
188L, NA, 160L, 106L, 149L, 185L, 185L, 185L, 138L, 188L, 185L, 
198L, 148L, 195L, 103L, 2262L, 212L, 81L, 232L, 167L, 338L, 212L, 
81L, 66L, 80L, 198L, NA, 125L, 81L, 115L, 300L, 300L, 300L, 226L, 
198L, 300L, 280L, 145L, 278L, 100L, 2120L, 175L, 80L, 318L, 242L, 
332L, 209L, 80L, 67L, 138L, 280L, NA, 120L, 80L, 112L, 315L, 
315L, 315L, 236L, 280L, 315L, 249L, 151L, 248L, 122L, 2215L, 
186L, 97L, 184L, 199L, 348L, 226L, 97L, 68L, 137L, 249L, NA, 
146L, 97L, 137L, 449L, 449L, 449L, 337L, 249L, 449L, 211L, 166L, 
208L, 111L, 1756L, 144L, 91L, 195L, 213L, 402L, 434L, 91L, 69L, 
190L, 211L, NA, 134L, 91L, 124L, 401L, 401L, 401L, 299L, 211L, 
401L, 304L, 159L, 299L, 160L, 2010L, 140L, 128L, 243L, 165L, 
422L, 380L, 128L, 70L, 149L, 304L, NA, 192L, 128L, 177L, 336L, 
336L, 336L, 253L, 304L, 336L, 249L, 188L, 248L, 74L, 2278L, 170L, 
60L, 242L, 160L, 492L, 501L, 59L, 71L, 127L, 249L, NA, 89L, 60L, 
81L, 483L, 483L, 483L, 364L, 249L, 483L, 266L, 119L, 263L, 92L, 
3064L, 157L, 74L, 264L, 195L, 403L, 359L, 74L, 72L, 128L, 266L, 
NA, 110L, 74L, 102L, 401L, 401L, 401L, 300L, 266L, 401L, 207L, 
237L, 206L, 99L, 3001L, 225L, 80L, 199L, 177L, 892L, 719L, 80L, 
73L, 291L, 207L, NA, 119L, 80L, 112L, 426L, 426L, 426L, 319L, 
207L, 426L, 200L, 383L, 199L, 104L, 4429L, 103L, 83L, 173L, 256L, 
1418L, 710L, 82L, 74L, 342L, 200L, NA, 126L, 83L, 117L, 332L, 
332L, 332L, 247L, 200L, 332L, 244L, 163L, 242L, 109L, 4118L, 
128L, 86L, 207L, 118L, 1011L, 541L, 86L, 75L, 223L, 244L, NA, 
128L, 86L, 118L, 323L, 323L, 323L, 241L, 244L, 323L, 224L, 463L, 
221L, 113L, 3112L, 140L, 91L, 152L, 145L, 1268L, 436L, 91L, 76L, 
276L, 224L, NA, 137L, 91L, 125L, 390L, 390L, 390L, 293L, 224L, 
390L, 321L, 325L, 318L, 115L, 3968L, 146L, 92L, 173L, 160L, 1112L, 
286L, 92L, 77L, 278L, 321L, NA, 138L, 92L, 125L, 357L, 357L, 
357L, 268L, 321L, 357L, 148L, 579L, 145L, 127L, 3395L, 150L, 
102L, 146L, 166L, 1606L, 382L, 102L, 78L, 253L, 148L, NA, 151L, 
102L, 141L, 512L, 512L, 512L, 385L, 148L, 512L, 184L, 431L, 181L, 
106L, 3693L, 158L, 84L, 158L, 171L, 1221L, 432L, 83L, 79L, 324L, 
184L, NA, 126L, 85L, 118L, 236L, 236L, 236L, 178L, 184L, 236L, 
199L, 833L, 198L, 103L, 3038L, 160L, 82L, 145L, 179L, 814L, 210L, 
81L, 80L, 243L, 199L, NA, 125L, 82L, 115L, 294L, 294L, 294L, 
220L, 199L, 294L, 209L, 580L, 207L, 109L, 2303L, 179L, 87L, 156L, 
182L, 952L, 280L, 87L, 81L, 244L, 209L, NA, 131L, 88L, 120L, 
321L, 321L, 321L, 240L, 209L, 321L, 1063L, 654L, 212L, 118L, 
2959L, 148L, 95L, 162L, 202L, 920L, 332L, 95L, 82L, 202L, 214L, 
NA, 142L, 95L, 130L, 334L, 334L, 334L, 249L, 214L, 334L, 1535L, 
950L, 225L, 113L, 2509L, 306L, 91L, 297L, 167L, 907L, 393L, 91L, 
83L, 201L, 227L, NA, 137L, 91L, 125L, 343L, 343L, 343L, 258L, 
227L, 343L, 1705L, 1252L, 227L, 135L, 2718L, 397L, 109L, 332L, 
166L, 804L, 320L, 108L, 84L, 121L, 228L, NA, 162L, 109L, 150L, 
363L, 363L, 363L, 271L, 228L, 363L, 1203L, 1192L, 253L, 95L, 
1813L, 481L, 76L, 451L, 174L, 869L, 308L, 75L, 85L, 91L, 256L, 
NA, 113L, 76L, 104L, 366L, 366L, 366L, 275L, 256L, 366L, 1176L, 
1127L, 210L, 140L, 2103L, 441L, 111L, 425L, 188L, 872L, 447L, 
111L, 86L, 160L, 212L, NA, 167L, 111L, 154L, 408L, 408L, 408L, 
306L, 212L, 408L, 943L, 1168L, 206L, 176L, 2156L, 600L, 141L, 
346L, 179L, 980L, 478L, 141L, 87L, 169L, 209L, NA, 213L, 141L, 
197L, 338L, 338L, 338L, 256L, 209L, 338L, 685L, 1469L, 215L, 
129L, 2212L, 414L, 104L, 373L, 214L, 1195L, 764L, 104L, 88L, 
234L, 217L, NA, 157L, 104L, 143L, 332L, 332L, 332L, 248L, 217L, 
332L, 951L, 690L, 234L, 164L, 1995L, 358L, 130L, 522L, 150L, 
849L, 441L, 129L, 89L, 145L, 236L, NA, 196L, 130L, 182L, 348L, 
348L, 348L, 261L, 236L, 348L, 779L, NA, 227L, 147L, 1995L, 648L, 
118L, 398L, 222L, NA, NA, 118L, 90L, NA, 228L, NA, 178L, 118L, 
165L, 378L, 378L, 378L, 283L, 228L, 378L, 853L, NA, 266L, 167L, 
2252L, 352L, 133L, 437L, 282L, NA, NA, 135L, 91L, NA, 270L, NA, 
202L, 136L, 188L, 364L, 364L, 364L, 274L, 270L, 364L, 593L, NA, 
186L, 213L, 2142L, 415L, 160L, 346L, 208L, NA, NA, 170L, 92L, 
NA, 188L, NA, 257L, 170L, 236L, 430L, 430L, 430L, 324L, 188L, 
430L, 844L, NA, 276L, 129L, 1933L, 584L, 135L, 423L, 261L, NA, 
NA, 104L, 93L, NA, 280L, NA, 157L, 104L, 143L, 301L, 301L, 301L, 
227L, 280L, 301L, 1055L, NA, 352L, 150L, 1973L, 606L, 177L, 507L, 
236L, NA, NA, 119L, 94L, NA, 354L, NA, 180L, 119L, 167L, 447L, 
447L, 447L, 334L, 354L, 447L, 773L, NA, 258L, 159L, 1656L, 1012L, 
281L, 399L, 269L, NA, NA, 126L, 95L, NA, 261L, NA, 191L, 127L, 
175L, 567L, 567L, 567L, 424L, 261L, 567L, 722L, NA, 324L, 167L, 
1883L, 617L, 267L, 430L, 340L, NA, NA, 134L, 96L, NA, 326L, NA, 
202L, 134L, 187L, 417L, 417L, 417L, 311L, 326L, 417L, 687L, NA, 
293L, 169L, 1788L, 530L, 275L, 444L, 206L, NA, NA, 136L, 97L, 
NA, 295L, NA, 205L, 136L, 189L, 521L, 521L, 521L, 392L, 295L, 
521L, 711L, NA, 334L, 186L, 1763L, 597L, 234L, 460L, 240L, NA, 
NA, 147L, 98L, NA, 338L, NA, 222L, 147L, 205L, 473L, 473L, 473L, 
354L, 338L, 473L, 889L, NA, 422L, 198L, 1337L, 446L, 303L, 584L, 
253L, NA, NA, 166L, 99L, NA, 427L, NA, 237L, 158L, 220L, 540L, 
540L, 540L, 405L, 427L, 540L, 635L, NA, 258L, 310L, 1749L, 558L, 
396L, 600L, 269L, NA, NA, 239L, 100L, NA, 261L, NA, 373L, 247L, 
345L, 683L, 683L, 683L, 512L, 261L, 683L, 669L, NA, 298L, 359L, 
1960L, 590L, 608L, 588L, 271L, NA, NA, 372L, 101L, NA, 303L, 
NA, 431L, 288L, 398L, 416L, 416L, 416L, 311L, 303L, 416L, 712L, 
NA, 315L, 214L, 1383L, 576L, 355L, 893L, 295L, NA, NA, 256L, 
102L, NA, 319L, NA, 257L, 170L, 236L, 482L, 482L, 482L, 361L, 
319L, 482L, 727L, NA, 334L, NA, 1631L, 730L, NA, 864L, 316L, 
NA, NA, NA, 103L, NA, 336L, NA, NA, NA, NA, 509L, 509L, 509L, 
382L, 336L, 509L, 887L, NA, 339L, NA, 1513L, 736L, NA, 580L, 
496L, NA, NA, NA, 104L, NA, 340L, NA, NA, NA, NA, 539L, 539L, 
539L, 403L, 340L, 539L, 794L, NA, 369L, NA, 1687L, 647L, NA, 
632L, 572L, NA, NA, NA, 105L, NA, 264L, NA, NA, NA, NA, 546L, 
546L, 546L, 408L, 325L, 546L, 838L, NA, 392L, NA, 2145L, NA, 
NA, 712L, 340L, NA, NA, NA, 106L, NA, 357L, NA, NA, NA, NA, 594L, 
594L, 382L, 443L, 462L, 594L, 880L, NA, 616L, NA, 1522L, NA, 
NA, 700L, NA, NA, NA, NA, 107L, NA, 456L, NA, NA, NA, NA, 634L, 
634L, 424L, 476L, 447L, 634L, 1030L, NA, 712L, NA, 1578L, NA, 
NA, 703L, NA, NA, NA, NA, 108L, NA, 873L, NA, NA, NA, NA, 996L, 
996L, 497L, 746L, 518L, 996L, 842L, NA, 256L, NA, 1546L, NA, 
NA, 753L, NA, NA, NA, NA, 109L, NA, 337L, NA, NA, NA, NA, 1151L, 
1151L, 515L, 862L, 465L, 1151L, NA, NA, NA, NA, 1685L, NA, NA, 
699L, NA, NA, NA, NA, 110L, NA, NA, NA, NA, NA, NA, 685L, 685L, 
310L, 512L, NA, 685L, NA, NA, NA, NA, 1726L, NA, NA, 857L, NA, 
NA, NA, NA, 111L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, 1561L, NA, NA, 1099L, NA, NA, NA, NA, 112L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
1411L, NA, NA, 1156L, NA, NA, NA, NA, 113L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1714L, NA, NA, 3876L, 
NA, NA, NA, NA, 114L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, 2258L, NA, NA, 1507L, NA, NA, NA, NA, 
115L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, 1442L, NA, NA, 1524L, NA, NA, NA, NA, 116L, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
2073L, NA, NA, NA, NA, 117L, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2725L, NA, NA, NA, 
NA, 118L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, 2255L, NA, NA, NA, NA, 119L, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, 1634L, NA, NA, NA, NA, 120L, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1360L, NA, NA, 
NA, NA, 121L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, 1270L, NA, NA, NA, NA, 122L, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, 1328L, NA, NA, NA, NA, 123L, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1470L, NA, 
NA, NA, NA, 124L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, 1040L, NA, NA, NA, NA, 125L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1095L, NA, NA, NA, NA, 126L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1823L, 
NA, NA, NA, NA, 127L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 3221L, NA, NA, NA, NA, 128L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 3986L, NA, NA, NA, NA, 129L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3632L, 
NA, NA, NA, NA, 130L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 3760L, NA, NA, NA, NA, 131L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 3225L, NA, NA, NA, NA, 132L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3473L, 
NA, NA, NA, NA, 133L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 3359L, NA, NA, NA, NA, 134L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 2032L, NA, NA, NA, NA, 135L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3337L, 
NA, NA, NA, NA, 136L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 2474L, NA, NA, NA, NA, 137L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1748L, NA, NA, NA, NA, 138L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1581L, 
NA, NA, NA, NA, 139L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 2590L, NA, NA, NA, NA, 140L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 2769L, NA, NA, NA, NA, 141L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2134L, 
NA, NA, NA, NA, 142L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 2312L, NA, NA, NA, NA, 143L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1822L, NA, NA, NA, NA, 144L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1890L, 
NA, NA, NA, NA, 145L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 1770L, NA, NA, NA, NA, 146L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1664L, NA, NA, NA, NA, 147L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1533L, 
NA, NA, NA, NA, 148L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 1628L, NA, NA, NA, NA, 149L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1682L, NA, NA, NA, NA, 150L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1416L, 
NA, NA, NA, NA, 151L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 1721L, NA, NA, NA, NA, 152L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1732L, NA, NA, NA, NA, 153L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1748L, 
NA, NA, NA, NA, 154L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 1312L, NA, NA, NA, NA, 155L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1659L, NA, NA, NA, NA, 156L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA), .Dim = c(25L, 156L), .Dimnames = list(
    c(""units_1"", ""units_2"", ""units_3"", ""units_4"", ""units_5"", 
    ""units_6"", ""units_7"", ""units_8"", ""units_9"", ""units_10"", ""units_11"", 
    ""units_12"", ""units_13"", ""units_14"", ""units_15"", ""units_16"", 
    ""units_17"", ""units_18"", ""units_19"", ""units_20"", ""units_21"", 
    ""units_22"", ""units_23"", ""units_24"", ""units_25""), NULL))
</code></pre>
"
"0.0849718577324175","0.0468292905790847"," 59467","<p>I am trying to learn the difference between the three approaches and their applications.</p>

<p>a) As I understand,</p>

<pre><code>AIC = -LL+K 

BIC = -LL+(K*logN)/2
</code></pre>

<p>Unless I am missing something, shouldn't the K that minimizes the AIC minimize BIC as well since N is constant. </p>

<p>I looked at this <a href=""http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other/767#767"">thread</a> but couldn't find a satisfactory answer. </p>

<p>b) According to Witten's book on Data Mining (pg 267) the definition of MDL for evaluating the quality of network is the same as BIC. Is there a difference between BIC and MDL?</p>

<p>c) What are the different approaches to compute MDL? I am looking for its application in Clustering, Time Series Analysis (ARIMA and Regime Switching) and Attribute Selection. While almost all commonly used packages in R report AIC and BIC, I couldn't find any that implements MDL and I wanted to see if I can write it myself.</p>

<p>Thank you.</p>
"
"0.147175574806061","0.162221421130763"," 64131","<p>I am trying to apply R depmixS4 package in order to cluster time series with model based clustering. The model consists of K components, each being a first order Markov models. The Expectation-Maximization algorithm is then used to estimate model parameters. </p>

<p>My time series are multivariate and of arbitrary length ( i.e. can be 400, can be 1). </p>

<p>Now, there are 2 problems:</p>

<ol>
<li>depmixS4 is oriented towards Hidden Markov Models, not the basic first order ones. </li>
<li>I do not completely understand the E and M-steps of model based clustering when applied to first-order Markov model components. Overwhelmingly, scientific literature talks about Hidden Markov Models. </li>
</ol>

<p>However, it seems to me that simple first-order Markov models can be seen as a particular case of hidden ones (where the response probability distribution is just identity, response equals state - thus the states are actually visible). So the clustering process does not have to decode the hidden states. However, the E-M process itself is still unclear, and I am not sure if I can apply the depmixS4 package methods by adjusting them or should I develop my own algorithm in R. </p>

<p>The mixture Markov model used in my research, is the following:</p>

<p>$$p(v | \Theta) = \sum_{t=1\ldots K}p(c_k)p_k(v | \Theta_k),$$
 where $v=v_1,\ldots,v_L$ - vector of arbitrarily length, and 
$$p_k(v |\Theta_k)=p(v_1| \Theta_{k_i})\prod_{i=2\ldots L} p(v_i |v_{i-1}, \Theta_{k_T}).$$ 
Given are also vector of initial state prior distribution and transition probability matrix $T_p$. </p>
"
"0.300643335692249","0.305887645160749"," 65411","<p>I have carried out a clustering of coordinate points (longitude, latitude) and found surprising, adverse results from clustering criteria for the optimal number of clusters. The criteria are taken from the <code>clusterCrit()</code> package. The points which I am trying to cluster on a plot (the geographic characteristics of the data set is clearly visible) :</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/EAgVj.jpg"" alt=""Plot of all observations""></p>
</blockquote>

<p>The full procedure was the following :</p>

<ol>
<li>Carried out hierarchical clustering on 10k points and saved
medoids for 2 : 150 clusters.</li>
<li>Took the medoids from (1) as seeds for kmeans clustering of 163k observations. </li>
<li>Checked 6 different clustering criteria for the optimal number of clusters.</li>
</ol>

<p>Only 2 clustering criteria gave results that make sense for me â€“ the Silhouette and Davies-Bouldin criteria. For both of them one should look for the maximum on the plot. It seems both give the answer â€œ22 Clusters is a good numberâ€. For the graphs below: on the x axis is the number of clusters and on the y axis the value of the criterion, sorry for the wrong descriptions on the image. Silhouette and Davies-Bouldin respectively :</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/9tlDB.jpg"" alt=""Silhoette Criterion Plot"">
  <img src=""http://i.stack.imgur.com/USELa.jpg"" alt=""Davies-Bouldin Criterion Plot""></p>
</blockquote>

<p>Now letâ€™s look at Calinski-Harabasz and Log_SS values. The maximum is to be found on the plot. The graph indicates that the higher the value the better the clustering. Such a steady growth is quite surprising, I think 150 clusters is already a quite high number. Below the plots for Calinski-Harabasz and Log_SS values respectively.</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/toHAM.jpg"" alt=""Calinski-Harabasz Criterion Plot"">
  <img src=""http://i.stack.imgur.com/yJiG0.jpg"" alt=""Log_SS Criterion Plot""></p>
</blockquote>

<p>Now for the most surprising part the last two criteria. For the Ball-Hall the biggest difference between two clusterings is desired and for Ratkowsky-Lance the maximum. Ball-Hall and Ratkowsky-Lance plots respectively :</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/09zWT.jpg"" alt=""Ball-Hall Criterion Plot"">
  <img src=""http://i.stack.imgur.com/UpE3b.jpg"" alt=""Ratkowsky-Lance Criterion Plot""></p>
</blockquote>

<p>The last two criteria give completely adverse answers (the smaller the number of clusters the better) than the 3rd and 4th criteria. How is that possible? For me it seems like only the first two criteria were able to make any sense of the clustering. A Silhouette width of around 0.6 is not that bad. Should I just skip the indicators that give strange answers and believe in those that give reasonable answers? </p>

<p><em>Edit: Plot for 22 clusters<img src=""http://i.stack.imgur.com/gNbON.jpg"" alt=""22 cluster solution""></em></p>

<hr>

<p><strong>Edit</strong></p>

<p>You can see that the data is quite nicely clustered in 22 groups so criteria indicating that you should choose 2 clusters seem to have weaknesses, the heuristic isn't working properly. It is ok when I can plot the data or when the data can be packed in less than 4 principal components and then plotted. But if not? How should I choose the number of clusters other than by using a criterion? I have seen tests which indicated Calinski and Ratkowsky as very good criteria and still they give adverse results for an seemingly easy data set. So maybe the question shouldn't be ""why are the results differing"" but ""how much can we trust those criteria?"".</p>

<p>Why is an euclidian metric not good? I am not really interested in the actual, exact distance between them. I understand the true distance is spheric but for all points A,B,C,D if Spheric(A,B) > Spheric(C,D) than also Euclidian(A,B) > Euclidian(C,D) which should be sufficient for for a clustering metric. </p>

<p>Why I want to cluster those points? I want to build a predictive model and there is a lot of information contained in the location of each observation. For each observation I also have cities and regions. But there are too many different cities and I don't want to make for example 5000 factor variables; therefore I thought about clustering them by coordinates. It worked pretty well as the densities in different regions are different and the algorithm found it, 22 factor variables would be all right. I could also judge the goodness of the clustering by the results of the predictive model but I am not sure if this would be wise computationally. Thanks for the new algorithms, I will definitely try them if they work fast on huge data sets.</p>
"
"0.158967789576202","0.175219161012616"," 65589","<p>I am currently implementing a Kmeans clustering algorithm in R. I am not using any packages and I wrote it from scratch. I am using only one set of initial guesses, and my action upon finding an empty cluster is to select a new data point randomly and use that as the new mean for the empty cluster.</p>

<p>I have gathered from reading online that the solution does not always converge, and it is highly sensitive to the initial means, so when I see that behavior I am not surprised. But I am finding that sometimes my solution is actually cycling between two or more different solutions. So I have two questions associated with this observation:</p>

<p>1) Within a solution cycle, one solution is always better than the others as measured by the total sum of squared distances of all points to their nearest clusters. So this implies that not only does the algorithm not necessarily find the global optimum, but also it sometimes does not even improve the total sum of squared distances from one iteration to the next? I thought the solution was at least always improving... </p>

<p>2) What is the best way to get around this problem? Do I have to program it to recognize cycles and then select the iteration in the cycle with the lowest total distance? Or is there an easier way? </p>

<p>Any help would be greatly appreciated.<br>
Thanks.</p>
"
"0.104068846970394","0.114707866935281"," 67686","<p>I am exploring the flexibility of partitional clustering algorithms. In particular, I would like to introduce more general distances than the ones which are used by default. </p>

<p>Let us consider, for simplicity, <code>dbscan</code> contained in the R-package <code>fpc</code>. It allows the user to specify a ""data matrix, data.frame, dissimilarity matrix or dist-object"". </p>

<p>My idea would be to compute the distance matrix of the given data w.r.t. my chosen distance, and run <code>dbscan</code>. </p>

<p>Here comes the point where I am stuck. Is it true that specifying a distance matrix should lead inevitably to a hierarchical clustering? My intuition says that a hierarchical clustering in presence of  a distance matrix makes more sense that a partitional one on the elements of the matrix itself. As I am no expert in clustering, I cannot judge the above statement properly.</p>

<p>Would you use a partitional algorithm on the distance matrix of a given dataset? Is this correct?</p>

<p>Thank you all!</p>
"
"0.060084176812611","0.0662266178532522"," 69514","<p>I have a clustering results by different unsupervised algorithms in the form of coordinates (x,y) and class (it's currently binary). I would like evaluate the quality of clustering by some value. Most of the used techniques assuming that the closely located entries in the plot are related, and if they are from different classes that's not good.</p>

<p>I have checked the R packages like clusterSim and clv (cluster evaluatin) by all of them seems to work with original data, but not with the already produced coordinates.</p>

<p>Can you please suggest me the quantitative evaluation technique for described situation? </p>

<p>PS: My data is imbalanced with ratio around 1:10</p>
"
"0.120168353625222","0.132453235706504"," 69740","<p>Background - I want to cluster analyze a mixed dataset, clustering the variables on the basis of correlational similarity. SPSS gives me this option, but doesn't allow me to evaluate the clustering solutions by providing statistical measures of heterogeneity change (e.g. pseudo F statistic) or direct measures of heterogeneity (e.g. CCC).</p>

<p>As such, I'm learning R. I've managed to cluster my variables using both the varclus and hclustvar procedures, both of which generate nice dendrograms that I can interpret visually. However I'm struggling to get R to provide me with some actual numbers, such as the statistics mentioned above, which might indicate what constitutes the ""best"" clustering solution. How can I do this? I've been through the documentation for both the Hmisc and ClustOfVar packages and can't find any way to do this.</p>

<p>I've read <a href=""http://stats.stackexchange.com/questions/49549/how-to-choose-clusters-from-variable-clustering-varclus-procedure"">elsewhere</a> that clustering criteria can be applied using the NBClust package, but as far as I can see the NBClust function only works on dissimilarity matrices / distance measures, which aren't an option for me as I'm interested in correlational similarity.</p>

<p>Any suggestions? </p>
"
"0.190002850064127","0.167541563316678"," 73162","<p>I'm attempting to estimate the effect of 2 drugs (<code>drug1</code>, <code>drug2</code>) on the likelihood of a patient falling (<code>event</code>).  The patients can fall more than once and can be put on or taken off of the the drugs at any point.  </p>

<p>My question is how the data should be structured with regard to the time period (days), specifically whether there needs to be overlap between the days.  There are two reasons why I think my structure is wrong, the first being a seemingly incorrect <code>N</code>.  I am also getting some errors where the time period is a single day (i.e. <code>time1=4</code>, <code>time2=4</code>) and am unsure how these should be coded.  Should the start time of subsequent entries be the stop time of the previous entry?  I've tried it both ways (with and without overlap), and while having overlap gets rid of the warning, the <code>N</code> is still incorrect. </p>

<pre><code>Warning message:
In Surv(time = c(0, 2, 7, 15, 20, 0, 18, 27, 32, 35, 39, 46, 53,  :
  Stop time must be &gt; start time, NA created
</code></pre>

<p>Right now I have the data set up where the beginning of the next entry is the next day.  Unique patients are identified by their <code>chart numbers</code>.  </p>

<pre><code>Time1    Time2    Drug1    Drug2   Event    ChartNo
    0        2        1        0       0        123
    3       10        1        1       1        123
   11       14        1        1       1        123
    0       11        0        1       0        345
    0       19        1        0       1        678
    0        4        0        1       0        900
    5       18        1        1       0        900
</code></pre>

<p>Patient 123 was on drug1 at the start to day 2, after which point they had drug2 added.  They went from day 3 to day 10 on both drugs before falling the first time, then fell a second time on day 14 while still on both drugs.  Patient 345 went 11 days on drug2 without falling (then was censored), etc.</p>

<p>The actual estimation looks like this:</p>

<pre><code>S &lt;- Srv(time=time1, time2=time2, event=event)
cox.rms &lt;- cph(S ~ Drug1 + Drug2 + cluster(ChartNo), surv=T)
</code></pre>

<p>My main concern is that the <code>n</code> for my analysis is reported to be <code>2017</code> (the number of rows in the data), when in actuality I only have <code>314</code> unique patients.  I am unsure if this is normal or the result of some error I've made along the way.</p>

<pre><code>&gt; cox.rms$n
Status
No Event    Event 
    1884      133 
</code></pre>

<p>The same is true when using <code>coxph()</code> from the survival package.</p>

<pre><code> n= 2017, number of events= 133
</code></pre>

<p>The number of events is correct however.  </p>

<p><a href=""http://stats.stackexchange.com/questions/58079/extended-cox-model-with-continuous-time-dependent-covariate-how-to-structure-d"">This Post</a> seems to have it set up with the 'overlap' I described, but I am unsure about the <code>N</code>, and they don't seem to be clustering by <code>ID</code>.  </p>
"
"0.120168353625222","0.132453235706504"," 76316","<p>Currently I am working on some subspace clustering issues. I found one useful package in R called orclus, which implemented one subspace clustering algorithm called orclus.</p>

<p>As stated in the package description, there are two key parameters to be determined. One is the subspace dimensionality and the other one is the cluster number. It is stated that to determine the optimal value of subspace dimensionality, one statistic, the cluster sparsity coefficient can be used. The closer the statistic to zero, the better the performance. However, when actually trying this implementation, I found that the statistic is minimal when subspace dimensionality is 1; and the larger the subspace dimensionality, the larger the statistic. Does it make sense? I was not expecting such monotonic trend. </p>
"
"0.147175574806061","0.135184517608969"," 77660","<p>I have 11 scale parameters for each of 218 observations belonging to subjects, I did standardized PCA to reduce dimensionality of the data and found two meaningful components. Using Euclidean distances this was followed by cluster analysis of these two components (explaining about 75% of the variance) with bottom-up approach using the hierarchical agglomerative clustering (HAC) by <code>FactoMineR</code> R package and Ward's linkage method.
The optimal number of clusters was 4 as suggested by the package based on minimizing the ratio of two successive partition inter-clusters inertia gains.<br>
This is just the number of observations per cluster:  </p>

<pre><code>&gt; table(df$clust)

  1   2   3   4 
  6  21  46 145
</code></pre>

<p>These 4 clusters turned out to be clinically important and subjects with cluster 1 were severely affected by disease. Cluster 4 were non-reactive subjects, Cluster 3 showed some reaction, and finally cluster 2 was like a special entity protected from disease. I don't know if these clusters can assume some kind of ordinal ranking or not. It is difficult to judge from the theoretical point of view related to the field, but I can say that cluster 4->3->1 is somehow showing some direction, and hence could be regarded as ordinal, on the other hand, cluster 2 is a little bit different but very important as subjects with this clusters were protected from disease. So, I am really confused as whether to consider these 4 clusters ordinal or not.  </p>

<p>Suppose that I have another set of 11 new readings of the scale parameters for one subject as new data, what statistical analysis would be useful to predict the membership of this subject to those 4 clusters? Could you please refer to a similar example with R code if possible? that would be greatly appreciated.  </p>

<p>Providing a professional answer would be highly esteemed, but also recommending some books using R code would also be encouraged, as I am searching for such a book that covers this topic thoroughly, many books are out there but it is difficult to judge which one would do the job. May be someone, has more experience with this kind of problems and can give a word of advise here.  </p>
"
"0.147175574806061","0.162221421130763"," 77672","<p>Suppose I have 50 scale parameters, these are all genes measured for one sample from a subject at the clinic, after data reduction by PCA, two meaningful components were extracted. This was followed by cluster analysis and turned out to be 4 meaningful clusters of subjects based on the two components of these 50 genes.  </p>

<p>Since investigating 50 genes for one subject would be costy, one would like to reduce that number so that the same clustering pattern can still be obtained but with minimal costs possible ( there should be some measures here to say acceptable clustering or not, I wonder what kind of measures would fit this case though).  </p>

<p>Of course, the more genes investigated, the more information gained, but there should be some measure to tell when to stop wasting more money when the same result is <em>satisfactorily</em> achievable will less number of genes.  </p>

<p>Is there any R package that already implemented this approach? what would be the statistical approach in this case to select the most important genes that would preserve the clustering pattern? what criteria to be used in order to reach the minimum clustering pattern?  </p>
"
"0.208137693940788","0.172061800402921"," 78938","<p>I've been working on SOMs and how to get the best clustering results. 
One approach could be to try many runs and choose the clustering with the lowest within sum of squared errors.</p>

<p>However, I do not only want to initialize random values and have several tries, but also want to choose good parameters.
I have read in ""Influence of Learning Rates and Neighboring Functions on Self-Organizing Maps"" (Stefanovic 2011) that if you do not know which parameters for the neighborhood function and learning rate to choose, it is probably the best option to choose a gaussian function and a nonlinear learning rate.</p>

<p>My data is a time series lets say:</p>

<pre><code>matrix(c(sample(seq(from = 10, to = 20, by = runif(1,min=1,max=3)), size = 5000, replace = TRUE),(sample(seq(from = 15, to = 22, by = runif(1,min=1,max=4)), size = 5000, replace = TRUE)),(sample(seq(from = 18, to = 24, by = runif(1,min=1,max=3)), size = 5000, replace = TRUE))),nrow=300,ncol=50,byrow = TRUE) -&gt; data
</code></pre>

<p>which has 300 observations with 50 values each. 100 observations each tend to be more similar.</p>

<p>I'm working with the kohonen package.</p>

<p>The code:</p>

<pre><code>grid&lt;-somgrid(4,3,""hexagonal"")

kohonen&lt;-som(data,grid)

matplot(t(kohonen$codes),col=kohonen$unit.classif,type=""l"")
</code></pre>

<p>gives me clusters with values <strong>between 10 to 22</strong>, which is similar to the obersations</p>

<p><img src=""http://i.stack.imgur.com/eNd7N.jpg"" alt=""enter image description here""></p>

<p>I tried the ""som"" package, too, which offers a gaussian neighborhood function and a inverse-time learning rate.</p>

<pre><code>som&lt;-som(data,4,3,init=""random"",alphaType=""inverse"",neigh=""gaussian"")

som$visual[,4]&lt;-with(som$visual,interaction(som$visual[,1],som$visual[,2]))

som$visual[,4]&lt;-as.numeric(as.factor(som$visual[,4]))

matplot(t(som$code),col=som$visual[,4],type=""l"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/vaSJV.jpg"" alt=""enter image description here""></p>

<p>Here I get clusters with values <strong>between 15 and 18</strong>, so all clusters ""shrink"" and get more similar. With different input series I get the same phenomena</p>

<p>My two questions:</p>

<ol>
<li>Why do clusters from the self-organizing map with the som package get so extraordinarily similar and shrink to a much smaller range, even though it is said that you get good clusters with gaussian neigborhood function and non linear learning rate?</li>
<li>How can I avoid this range shrinking with a gaussian neigborhood function and a non linear learning rate in order to get appropriate clusters?</li>
</ol>
"
"0.169943715464835","0.187317162316339"," 81396","<p>I'm trying to compile a list of clustering algorithms that are:</p>

<ol>
<li>Implemented in R</li>
<li>Operate on sparse <em>data matrices</em> (not (dis)similarity matrices), such as those created by the <a href=""http://www.rdocumentation.org/packages/Matrix/functions/sparseMatrix"" rel=""nofollow"">sparseMatrix</a> function.</li>
</ol>

<p>There are several other questions on CV that discuss this concept, but none of them link to R packages that can operate directly on sparse matrices:</p>

<ol>
<li><a href=""http://stats.stackexchange.com/questions/10411/clustering-large-and-sparse-datasets"">Clustering large and sparse datasets</a></li>
<li><a href=""http://stats.stackexchange.com/questions/44640/clustering-high-dimensional-sparse-binary-data"">Clustering high-dimensional sparse binary data</a></li>
<li><a href=""http://stats.stackexchange.com/questions/10122/looking-for-sparse-and-high-dimensional-clustering-implementation"">Looking for sparse and high-dimensional clustering implementation</a></li>
<li><a href=""http://stats.stackexchange.com/questions/9778/space-efficient-clustering/"">Space-efficient clustering</a></li>
</ol>

<p>So far, I've found exactly one function in R that can cluster sparse matrices:</p>

<h1><a href=""http://www.rdocumentation.org/packages/skmeans/functions/skmeans"" rel=""nofollow"">skmeans</a>: spherical kmeans</h1>

<p>From the <a href=""http://cran.r-project.org/web/packages/skmeans/index.html"" rel=""nofollow"">skmeans package</a>. kmeans using <a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow"">cosine distance</a>.  Operates on dgTMatrix objects. Provides an interface to a genetic k-means algorithm, pclust, CLUTO, gmeans, and kmndirs.</p>

<p>Example:</p>

<pre><code>library(Matrix)
set.seed(42)

nrow &lt;- 1000
ncol &lt;- 10000
i &lt;- rep(1:nrow, sample(5:100, nrow, replace=TRUE))
nnz &lt;- length(i)
M1 &lt;- sparseMatrix(i = i,
                   j = sample(ncol, nnz, replace = TRUE),
                   x = sample(0:1 , nnz, replace = TRUE), 
                   dims = c(nrow, ncol))
M1 &lt;- M1[rowSums(M1) != 0, colSums(M1) != 0]

library(skmeans)
library(cluster)
clust_sk &lt;- skmeans(M1, 10, method='pclust', control=list(verbose=TRUE))
summary(silhouette(clust_sk))
</code></pre>

<hr>

<p>The following algorithms get honerable mentions: they're not quite clustering algorithms, but operate on sparse matrices.</p>

<h1><a href=""http://www.rdocumentation.org/packages/arules/functions/apriori"" rel=""nofollow"">apriori</a>: association rules mining</h1>

<p>From the <a href=""http://cran.r-project.org/web/packages/arules/index.html"" rel=""nofollow"">arules package</a>. Operates on ""transactions"" objects, which can be coerced from ngCMatrix objects.  Can be used to make recommendations.</p>

<p>example:</p>

<pre><code>library(arules)
M1_trans &lt;- as(as(t(M1), 'ngCMatrix'), 'transactions')
rules &lt;- apriori(M1_trans, parameter = 
list(supp = 0.01, conf = 0.01, target = ""rules""))
summary(rules)
</code></pre>

<h1><a href=""http://www.rdocumentation.org/packages/irlba/functions/irlba"" rel=""nofollow"">irlba</a>:  sparse SVD</h1>

<p>From the <a href=""http://cran.r-project.org/web/packages/irlba/index.html"" rel=""nofollow"">irlba package</a>.  Does SVD on sparse matrices.  Can be used to reduced the dimensionality of sparse matrices prior to clustering with traditional R packages.</p>

<p>example:</p>

<pre><code>library(irlba)
s &lt;- irlba(M1, nu = 0, nv=10)
M1_reduced &lt;- as.matrix(M1 %*% s$v)
    clust_kmeans &lt;- kmeans(M1, 10)
    summary(silhouette(clust_kmeans$cluster, dist(M1_reduced)))
</code></pre>

<h1><a href=""http://cran.r-project.org/web/packages/apcluster/index.html"" rel=""nofollow"">apcluster</a>:  Affinity Propagation Clustering</h1>

<pre><code>library(apcluster)
sim &lt;- crossprod(M1)
sim &lt;- sim / sqrt(sim)
clust_ap &lt;- apcluster(sim) #Takes a while
</code></pre>

<p>What other functions are out there?</p>
"
"0.407510713907571","0.390583283432254"," 81727","<p>I have 4 clusters (see plot below) extracted from data of medical samples <code>N=218</code> measured for 11 genes/predictors <code>P=11</code> by this method: first PCA analysis validated to have 2 important PCs that explained 75% of the data, then different clustering algorithms, distances, linkages (in hierarchical approach only) were compared: the majority support the presence of 4 distinct clusters. Taking the scientific hypothesis into consideration, clusters out of $K$-means algorithm were found the most plausible and were the most balanced clusters too: class #1 <code>n=12</code>, class #2 <code>n=21</code>, class #3 <code>n=79</code>, and class #4 <code>n=106</code>.  </p>

<p>Projecting the observations on plane 1-2 component scores, revealed the below scatter plot with each cluster color coded.
<img src=""http://i.stack.imgur.com/09sTF.png"" alt=""enter image description here""></p>

<p><strong>The aim is to find a global optimum classifier using R after doing PLS to the data.</strong>  </p>

<p>Knowing that these 4 clusters were actually the product of latent PCA components, it was natural to think of PCR as a next step to predict classes, but that approach turned out to be sub-optimal for two reasons: first, results do not related to probibilities (0-1), second, it does not relate well with the classes as the outcome variable. As many know, this would be better solved with PLS-DA method + softmax to find probabilities of class (0-1).  </p>

<p>However, many reports confirm the superiority of using LDA as a second step using the <em>scores</em> of PLS, given that same standardization parameters (mean, sd) be used of the training set on the holdout-test set, even using the PLS projections out of the training set on the test set in order to get the <em>scores</em> which would be the <em>actual</em> holdout-test set to validate the classifier in question.  </p>

<p>From the methodology point of view, this path is potentially encompassed with many dangers and subtle errors when one is un/misinformed about the tools used in context.  </p>

<p>The <code>caret</code> package which is unique of its kind given the consistent infrastructure it provides to train and validate an array of different models making use of <em>de facto</em> standard respective R-packages, and hence <code>caret</code> promotes itself as a road map to a validated modeling leveraging off R rich libraries. As heart to blood vessels, so <code>caret</code> to other packages in my opinion. That being said, unwatchful playing with the heart could cost you dearly, and might lead also to a stand-still or a <em>model-arrest</em> of your data. R is free, many free books out there, but buying <code>caret</code> only book paid off, i.e. <code>Applied Predictive Modeling</code>. The help files, companion website (very appealing btw), are great resources but they won't substitute the text inside the book IMHO. However, in the book, I couldn't find a direct answer to the PLS-Classifier two step method amid others. The potential with <code>caret</code> is immense, thanks to Max Kuhn and his colleagues, that primarily encouraged me to post this question.  </p>

<p>Back to the example above and the methodology of wish:<br>
<strong>Data splitting:</strong>  </p>

<p>Training set (77% <code>n=168</code>) for 10K-cross-validation: tuning (model-specific parameters, feature selection <code>P=11</code>, and cost to deal with imbalanced clusters). For CV this would be roughly <code>n=150</code> for fitting the model using differnt parameters of wish and 'n=17<code>for evaluation of parameters (I would call the</code>n=17' the CV-test to avoid confusion later on). Repetition = 5, so this will make 10 folds x 5 times = 50 training folds (<code>n=150</code> each) and 50 CV-test folds (<code>n=17</code> each). Holdout-test set (23% <code>n=50</code>).  </p>

<p><strong>Q1</strong> I know that one can do parameters' tuning along with feature selection at the same time (i.e., parallel), but how to evaluate the cost/weights if one would like to evaluate cost-sensitive models (SVM, CART, C5.0) using the PLS scores to counteract class imbalance?    </p>

<p><strong>Q2</strong> What is the alternative approach when reserving a separate data set for cost evaluation (i.e., <code>evaluation set</code>), as recommended, is not possible given the small sample size in this case? can one do tuning of model parameter, feature selection, and cost for imbalance all three at the same time? if not what is the best practice in this case?  </p>

<p><strong>Q3</strong> Given the small sample size, is bootstrapping preferred to CV? if yes how would it be implemented to do exhaustive tuning like above for the PLS scores?    </p>

<p><strong>Q4</strong> Given the imbalance above, is there a way to ensure that each CV training fold would include the minimum number of <code>hard</code> class(es) in order to have good estimation on the CV-test fold? is there any argument to pass to ensure presence of the small classes each time fold would be generated?  </p>

<p><strong>PLS special notes</strong>  </p>

<p>This is the approach in my mind (please correct me if I am missing something somewhere during the course):  </p>

<p>In each CV iteration on the many CV-traning folds, there should be a unique PLS projection matrix for each iteration that would be used in the next second step of getting PLS scores for the the respective CV-test set inheriting the same standardization parameters (<code>mean</code>, <code>sd</code>), this means that two things would be inherited; the PLS projection and the standardization parameters (mean and sd) in order to apply them to the CV-test folds, this way, given the example here, 50 values would be returned hoping to reach the best parameter in question. One complication though, there should be an argument to specify the desired number of PLS components to retain and to be used in calculation of scores out of each CV-test fold (better to be pre-defined in a previous tuning step may be). My expectation, is that after deciding on the best model, there should be a way to get the PLS projection matrix for the whole training set (i.e.<code>n=168</code>) along with (<code>mean</code>, <code>sd</code>) to apply them on the holdout-test to validate the best model. So in total, there would be 50 different PLS projection matrices, means, sds from CV step and 1 extra frothe whole training set, am I right?<br>
Feature selection in this method would entail two things: predictors space and the PLS components space.</p>

<p><strong>Q5</strong> How to perform these two selections (predictor and PLS component) in <code>caret</code>? this is because feature selection here is different than otherwise since here we deal with scores rather than the observations themselves to determine best predictors that to construct the PLS components.</p>

<p><strong>Note:</strong> When one is happy with the best final model, it would be recommended to fit the model on the whole data set <code>n=218</code> to get the correct estimates withe the least uncertainty.  </p>

<p>A similar procedure is implemented in <code>caret::train()</code> function that can be fed with <code>preProc</code> argument to specify the type of desired pre-processing of data (most are mentioned in the help system but I couldn't find <strong>PLS</strong> among them, better if with an argument to specify the desired components similar to PCA pre-processing). I am aware of the fact, that inheriting pre-processing parameters to holdout test set and to CV-test, can only be performed using the <code>predict.train()</code> function, as opposed to calling the generic <code>predict()</code> function to the <code>$finalModel</code> that won't inherit pre-processing parameters.     </p>

<p><strong>Q6</strong> How to implement this strategy (if correctly described) to train and validate the two-step PLS-[classifier] methodology using PLS scores subspace instead of observations making use of <code>caret</code> infrastructure?  </p>

<p>Thanks in advance.</p>
"
"0.120168353625222","0.132453235706504"," 82981","<p>I am exploring hierarchical logistic regression, using glmer from the lme4 package. To my understanding, one of the first steps in multilevel modeling is to estimate the degree of clustering of level-1 units within level-2 units, given by the intraclass correlation (to ""justify"" the additional cost of estimating parameters to account for the clustering). When I run a fully unconditional model with glmer</p>

<pre><code>fitMLnull &lt;- glmer(outcome ~ 1 + (1|level2.ID), family=binomial)
</code></pre>

<p>the glmer fit gives me the variance in the intercept for outcome at level-2, but the residual level-1 variance in outcome is nowhere to be found. I read somewhere (can't track it down now) that the residual level-1 variance is <em>not</em> estimated in HGLM (or at least in glmer). Is this true? If so, is there an alternative way to approximate the degree of clustering in the data? If not, how can I access this value to calculate the ICC?</p>
"
"0.104068846970394","0.114707866935281"," 83446","<p>Suppose you want to find clusters based on a set of variables $Y$, and that you want to estimate the effects of some variables $X$ on membership in those clusters. Here is how I am doing it now.</p>

<p>Step 1: Perform model-based clustering on the variables $Y$ (using the <code>mclust</code> package for this).</p>

<p>Step 2: Optimize a multinomial regression model with cluster membership as the outcome variable.</p>

<p>It seems like there must be a better way in which the models are estimated simultaneously. Anyone know a good tool in R for this and, even better, a good set of references for (a) the statistical model that the package implements, and (b) how to use the package?</p>

<p>Thanks</p>
"
"NaN","NaN"," 85757","<p>I have used <code>hclust</code> function from R for the hierarchical clustering of vectors which are already labeled. </p>

<pre><code>dissimilarity &lt;- 1 - cor(data)
distance &lt;- as.dist(dissimilarity)
plot(hclust(distance),  main=""Dissimilarity = 1 - Correlation"", xlab="""")
</code></pre>

<p>Now I want to evaluate if the vectors with the same label are clustered in the same group. However, I don't know how to find the optimal cutting points in the deprogram. Is there a package for it?</p>

<p>Thanks for your help.</p>
"
"0.104068846970394","0.0764719112901873"," 87818","<p>I need to perform clustering and classification of time series of weekly sales of different products. My data are weekly sales of different products in differest areas (stores). The challenges on this problem are:<br>
 - Time-series are usually short: 10-52points(weeks).<br>
 - Time-series may have a lot of zeros - sparce data. Products do not sell every week. <br>
 - Not all products start to sell on the same date. This can result in time-shifted time-series. Even the same typical lifecycle of a product can be time-shifted in calendar along different stores. <br>
 - Sales may have noise such as random events, promotions etc.</p>

<p>A sample of data is like this:</p>

<p>20140105,prod1,store1,5<br>
20140119,prod1,store1,10<br>
20140126,prod1,store1,2<br>
....<br>
20140105,prod1,store2,2<br>
20140112,prod1,store2,3<br>
....<br>
20140112,prod2,store3,4<br>
20140126,prod2,store3,7<br></p>

<p>Can somebody share any insight on how to do this? Is it good to use a method such as DTW to compare time-series?If so, how am I going to handle the timeshifts?
As for the implementation R seems a good way to go. Which packages would you recommend?</p>
"
"0.2753402883129","0.245681158031834"," 89204","<p>I'm looking for advice on how to analyze complex survey data with multilevel models in R. I've used the <code>survey</code> package to weight for unequal probabilities of selection in one-level models, but this package does not have functions for multilevel modeling. The <code>lme4</code> package is great for multilevel modeling, but there is not a way that I know to include weights at different levels of clustering. <a href=""http://www.statmodel.com/download/asparouhovgmms.pdf"">Asparouhov (2006)</a> sets up the problem:</p>

<blockquote>
  <p>Multilevel models are frequently used to analyze data from cluster sampling designs. Such sampling designs however often use unequal probability of selection at the cluster level and at the individual level. Sampling weights are assigned at one or both levels to reflect these probabilities. If the sampling weights are ignored at either level the parameter estimates can be substantially biased.</p>
</blockquote>

<p>One approach for two-level models is the multilevel pseudo maximum likelihood (MPML) estimator that is implemented in MPLUS (<a href=""http://www.statmodel.com/download/SurveyJSM1.pdf"">Asparouhov et al, ?</a>). <a href=""http://www.biomedcentral.com/1471-2288/9/49"">Carle (2009)</a> reviews major software packages and makes a few recommendations about how to proceed: </p>

<blockquote>
  <p>To properly conduct MLM with complex survey data and design weights, analysts need software that can include weights scaled outside of the program and include the ""new"" scaled weights without automatic program modification. Currently, three of the major MLM software programs allow this: Mplus (5.2), MLwiN (2.02), and GLLAMM. Unfortunately, neither HLM nor SAS can do this.</p>
</blockquote>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3630376/"">West and Galecki (2013)</a> give a more updated review, and I'll quote the relevant passage at length:</p>

<blockquote>
  <p>Occasionally, analysts wish to fit LMMs to survey data sets collected from samples with complex designs (see Heeringa et al, 2010, Chapter 12). Complex sample designs are generally characterized by division of the population into strata, multi-stage selection of clusters of individuals from within the strata, and unequal probabilities of selection for both clusters and the ultimate individuals sampled. These unequal probabilities of selection generally lead to the construction of sampling weights for individuals, which ensure unbiased estimation of descriptive parameters when incorporated into an analysis. These weights might be further adjusted for survey nonresponse and calibrated to known population totals. Traditionally, analysts might consider a design-based approach to incorporating these complex sampling features when estimating regression models (Heeringa et al., 2010). More recently, statisticians have started to explore model-based approaches to analyzing these data, using LMMs to incorporate fixed effects of sampling strata and random effects of sampled clusters.</p>
  
  <p>The primary difficulty with the development of model-based approaches to analyzing these data has been choosing appropriate methods for incorporating the sampling weights (see Gelman, 2007 for a summary of the issues). Pfeffermann et al. (1998), Asparouhov and Muthen (2006), and Rabe-Hesketh and Skrondal (2006) have developed theory for estimating multilevel models in a way that incorporates the survey weights, and Rabe-Hesketh and Skrondal (2006), Carle (2009) and Heeringa et al. (2010, Chapter 12) have presented applications using current software procedures, but this continues to be an active area of statistical research. Software procedures capable of fitting LMMs are at various stages of implementing the approaches that have been proposed in the literature thus far for incorporating complex design features, and analysts need to consider this when fitting LMMs to complex sample survey data. Analysts interested in fitting LMMs to data collected from complex sample surveys will be attracted to procedures that are capable of correctly incorporating the survey weights into the estimation procedures (HLM, MLwiN, Mplus, xtmixed, and gllamm), consistent with the present literature in this area.</p>
</blockquote>

<p>This brings me to my question: does anyone have best practice recommendations for fitting LMMs to complex survey data in R?</p>
"
"0.171002565057714","0.188484258731263"," 91348","<p>I am working on data analysis.</p>

<p>Given a group of data vectors, each of them has the same dimension. Each element in a vector is a floating point number. </p>

<pre><code>V1 [  ,   ,   , â€¦ ] 
V2[  ,   ,   , â€¦ ] 
...
Vn [  ,   ,   , â€¦ ] 
</code></pre>

<p>Suppose that each vector has M numbers. M can be 10000.</p>

<p>n can be 200. </p>

<p>I need to find out how to partition the n vectors into sub-groups such that each vector in one subgroup can be represented by a basic vector in the subgroup. </p>

<p>For example, </p>

<p>W = union of V1, V2, V3 â€¦ Vn</p>

<p>Find subgroup i, j, â€¦ t :</p>

<pre><code>Gi = [  V1, V6, V3, V5, â€¦ , Vx ]
Gj = [V22, V11, V56, V45, â€¦ , Vy]
â€¦
Gt = [V78, V90, V9, V12, â€¦ , Vz]
</code></pre>

<p>Such that :</p>

<p>Union of Gi , Gj, â€¦ , Gt is equal to W and there is no overlap among  all Gi , Gj, â€¦ , Gt. </p>

<p>Also , each subgroup has a basic vector that has strong correlation with all other element vector in the subgroup. For example, in Gi, we may have vector Vx as the basic vector such that all other vectors have <strong>strong (linear) correlation</strong>  with Vx. <strong>Here, we measure the linear correlation betwwen two vectors not two data points.</strong> </p>

<p>Moreover, we need to minimize the number of the subgroups, here, it is  "" t "" . It means that given 200 vectors ( n = 200), we prefer a subgroup G1, G2, â€¦, Gt, and t is minimized. For example, we prefer t = 5 over t = 6. if t is more than 10, it may not be useful. </p>

<p>My questions:
What kind of knowledge domain this problem belongs to ? </p>

<p>Is it a clustering analysis ? But, in cluster analysis, one data point is a number, but, here one data point  is a vector.</p>

<p>Are there some statistics models or algorithm can be used to do this kind of analysis ?  Are there some software tools or packages that solve this problem ? </p>

<p>If my questions are not a good fit for this forum, please tell me where I should post it. </p>

<p>R packages do the clustering for data points not for data vector by correlation.</p>

<p>Any help would be appreciated. </p>
"
"0.104068846970394","0.114707866935281"," 92177","<p>I'm doing a project related to identifying sales dynamics. My database contains 26 weeks after launching the product (so 26 time-series observations equally spaced in time). </p>

<p><img src=""http://i.stack.imgur.com/Dquwy.jpg"" alt=""http://imageshack.com/a/img18/5628/l5qg.jpg""></p>

<p><img src=""http://i.stack.imgur.com/8Dh2C.jpg"" alt=""http://imageshack.com/a/img34/8953/yh6i.jpg""></p>

<p>I used two methods of time-series clustering to see which patterns dominate in different groups (clustering by <code>units_sold_that_week</code>). The first method is based on k-medoids and the second one connected with clustering by parameters of growth models.</p>

<p>My next step is to make forecasts based on these clusters. Is there any special method for forecasting based on time-series clusters? In my project, I have to combine the topic of clustering and forecasting on clusters.</p>

<p>I am running my analyses in R, so I would be grateful for any suggestions regarding R procedures.</p>

<p>Please note that I am relatively new to time series analysis so any clarity you could provide, on R or any package you could recommend that would help accomplish this task efficiently, would be appreciated.</p>
"
"0.0849718577324175","0.0936585811581694"," 93815","<p>I have some experiences with time series modelling, in the form of simple ARIMA models and so on. Now I have some data that exhibits volatility clustering, and I would like to try to start with fitting a GARCH (1,1) model on the data. </p>

<p>I have a data series and a number of variables I think influence it. So in basic regression terms, it looks like: </p>

<p>$$
y_t = \alpha + \beta_1 x_{t1} + \beta_2 x_{t2} + \epsilon_t .
$$</p>

<p>But I am at a complete loss at how to implement this into a GARCH (1,1) - model? I've looked at the <code>rugarch</code>-package and the <code>fGarch</code>-package in <code>R</code>, but I haven't been able to do anything meaningful besides the examples one can find on the internet. </p>
"
"0.120168353625222","0.132453235706504","101254","<p>I have some problems in finding the outliers using clustering. </p>

<p>The data.frame is ~20000 observations and each row has mixed types of variables(numeric, nominal and binary). What I want to do is to detect the outliers by clustering.</p>

<p>I have calculated the dissimilarity matrix using daisy() function in R:    </p>

<pre><code>diss = daisy(data,metric=""gower"")
</code></pre>

<p>And I know I can use pam() and hclust() functions to do the clustering. But how do I find the outliers after that?</p>

<p>Here is my R code to find the outliers from pam():</p>

<pre><code>kmedoid = pam(diss,k=10,diss=T)
centers = kmedoid$id.med
distMat = as.matrix(diss)   
distances = rep(-99,20000)   
for (k in 1:20000) {   
  distances[k] = min(distMat[centers,k])   
}   
outliers = order(distances, decreasing=T)[1:5]   
outliers = data[outliers]   
outliers
</code></pre>

<p>I don't know whether it is correct, because the result seems to be pretty different each time when I tried different value of k in pam(). </p>

<p>So the main question is: <strong>Once I have the ""kmedoid"" and ""hc"" calculated below, how do I find the outliers?</strong> </p>

<pre><code>kmedoid = pam(data,k=10,diss=T)
hc = hclust(data)
</code></pre>

<p>I did search Google, but there wasn't much info about this. I am not fluent in programming, so just using existing package and function in R would help me a lot:)</p>

<p>And <strong>is there any better method to find the outliers?</strong> </p>

<p>Thanks! </p>
"
"0.134352303725115","0.148087219439773","102984","<p>I have a dataset consists of 5 features : A, B, C, D, E. They are all numeric values. Instead of doing a density-based clustering, what I want to do is to cluster the data in a decision-tree-like manner.</p>

<p>The approach I mean is something like this:</p>

<p>The algorithm may divide the data into X initial clusters based on feature C, i.e. the X clusters may have small C, medium C, large C and very large C values etc. Next, under each of the X cluster nodes, the algorithm further divide the data into Y clusters based on feature A. The algorithm continues until all the features are used.</p>

<p>The algorithm that I described above is like a decision-tree algorithm. But I need it for unsupervised clustering, instead of supervised classification. </p>

<p>My questions are the following:</p>

<ol>
<li>Do such algorithms already exists? What is the correct name for such algorithm</li>
<li>Is there a R/python package/library which has an implementation of this kind of algorithms?</li>
</ol>
"
"0.147175574806061","0.162221421130763","103280","<p>I'm attempting a project where I need to statistically rank available cars based on several variables such as cost, mpg, seating, milage, etc.. I wish to rank these cars in order decide which car would be the best choice (highest ""worth"") to buy (or best several cars if I was informing multiple people of the best cars to get). As the list of available cars changes from day to day, I will also need to re-run the code every day to allow the rankings to give me the best decision for this new day. </p>

<p>What statistical methods should I use to go about this ranking system? I plan on determining which factors I find most important so the variables used will be subjective in choice. I thought about trying MDS or clustering but I didn't know if that would be relevant since I'm already subjectively determining what variables are to be used. I don't see how regression can be used since I can't get a handle on the ""worth"" of previous cars as that is what I'm trying to rank by. Also, I will be attempting this in R so any helpful packages/functions would be great to know as well.</p>

<p>Any help with how to go about this ranking scheme would be helpful as I'm at a loss.</p>

<p>Thanks so much</p>
"
"0.104068846970394","0.114707866935281","108256","<p>I wish to test my time series data for volatility clustering, i.e. conditional heteroskedasticity.</p>

<p>So far, I have used the ACF test on the squared and absolute returns of my data, as well as the Ljung-Box test on the squared data (i.e. McLeod.Li.test).</p>

<p>In a recent paper (<a href=""http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1771862"" rel=""nofollow"">http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1771862</a>, the test is reported on page 8) co-authored by a well-known researcher, they have employed the White test (<a href=""http://ideas.repec.org/a/ecm/emetrp/v48y1980i4p817-38.html"" rel=""nofollow"">http://ideas.repec.org/a/ecm/emetrp/v48y1980i4p817-38.html</a>) to directly test for heteroskedasticity.</p>

<p>I have tried the same approach, however was unable to do so.
From my understanding, the White test needs residual variance (usually from a linear regression model) as an input.</p>

<p>Now my question is: How did the researchers perform the White test? I do not understand which inputs they used for their White test.</p>

<p>While searching for solutions, I have found the sandwich package which uses the vcovHC and vcovHAC functions to estimate a heteroskedasticity-consistent covariance matrix, however the input is also a fitted linear regression model..</p>
"
"NaN","NaN","109597","<p>I am attempting simple Ward type clustering. However, the R package is proving several choices to use for the distance matrix. I am wondering how I am supposed to determine the right distance matrix method.</p>

<p>Are there any generally acceptable criteria for specific sets of problems?</p>
"
"0.104068846970394","0.0764719112901873","113232","<p>I'm trying to run a discrete-time multilevel hazard analysis comparable to the model proposed by Barber et al. I am attempting to model the hazard of migrating internationally using predictors at the individual, household, community, and regional levels.
(Most of the variables of interest are at the individual, community, and regional levels--I just need to account for clustering by household)</p>

<p>Is there a package that will allow me to do this in R?  I've used lme4 for regular multilevel modeling, but can it also be used for multilevel survival models?  How would I go about coding such a model? (And if this can't be done in R, can it be done in Stata, or do I have to bite the bullet and buy and learn HLM or MLwiN?)  </p>

<p>Please let me know if you need any additional information.  Thanks!</p>

<p>ETA: Barber et al. refers to:
<a href=""http://onlinelibrary.wiley.com/doi/10.1111/0081-1750.00079/abstract"" rel=""nofollow"">Barber, Jennifer S., Susan A. Murphy, William Axinn, and Jerry Maples. 2000. ""Discrete-Time Multilevel Hazard Analysis."" Sociological Methodology, 30: 201-235.</a></p>
"
"0.160224471500296","0.110377696422087","113504","<p>I have some code that looks for clusters in x,y data. To check the number of clusters I use, I want to get the BIC. This is not possible (easily) using <code>kmeans()</code>, and so I've switched to the <a href=""http://cran.r-project.org/web/packages/mclust/index.html"" rel=""nofollow"">mclust package</a>. Specifically, I'm trying to replace <code>kmeans()</code> from the R stats package, with <code>Mclust()</code> from the mclust package. </p>

<p>Using <code>Mclust()</code> requires me to specify which model should be used for the clustering. According to <code>?Mclust</code>, the following models can be used in <code>Mclust()</code>:</p>

<pre><code>univariate mixture      
""E""  =   equal variance (one-dimensional)
""V""  =   variable variance (one-dimensional)
multivariate mixture        
""EII""    =   spherical, equal volume
""VII""    =   spherical, unequal volume
""EEI""    =   diagonal, equal volume and shape
""VEI""    =   diagonal, varying volume, equal shape
""EVI""    =   diagonal, equal volume, varying shape
""VVI""    =   diagonal, varying volume and shape
""EEE""    =   ellipsoidal, equal volume, shape, and orientation
""EEV""    =   ellipsoidal, equal volume and equal shape
""VEV""    =   ellipsoidal, equal shape
""VVV""    =   ellipsoidal, varying volume, shape, and orientation
single component        
""X""  =   univariate normal
""XII""    =   spherical multivariate normal
""XXI""    =   diagonal multivariate normal
""XXX""    =   ellipsoidal multivariate normal
</code></pre>

<p>I'm presuming that k-means in stats is a ""spherical, unequal volume"" model, ie. to get <code>k-means(x = data, centers = 6)</code> to match <code>mclust()</code>, I should use <code>mclust(data, G = 6, modelNames = c(""VII""))</code>. </p>

<p>However, in the limited tests I've done, this gives different cluster centroids. The example below uses 6 clusters with some test data. The centroids obtained through each method are shown.</p>

<p><img src=""http://i.stack.imgur.com/NJrQ0.png"" alt=""enter image description here""></p>

<p>Can anyone confirm which <code>mclust()</code> model is equivalent to <code>kmeans()</code>?</p>
"
"NaN","NaN","114100","<p>Are there any packages that implement the Autoclass/ Naive Bayes Clustering algorithm in R or Python? </p>

<p>Alternatively, what are some other clustering algorithms that can handle both categorical and numeric variables that are implemented in either R or Python?</p>
"
"0.104068846970394","0.114707866935281","121170","<p>Most clustering algorithms assume that data points in each row are independent.  I have some data with repeated measurements from individuals.</p>

<p>I can use a standard algorithm, and then check to see if samples from the same person end up in the same cluster (for example by manual inspection of a dendrogram, or by looking at within group homogeneity and stability measures using <a href=""http://www.inside-r.org/packages/cran/clValid/docs/clValid"" rel=""nofollow""><code>clValid</code></a>'s ""biological"" validation).</p>

<p><strong>Are there any clustering algorithms (preferably with an implementation in R) that take account of the repeated measurements while calculating clusters?</strong></p>

<p>Bonus features:<br>
My dataset is very wide (more variables than samples), so being able to deal with that situation would be very useful.<br>
Also, there are different numbers of measurements for individuals, so it would also be nice for the algorithms to deal with that.<br>
The variables in my dataset are continuous rather than categorical.</p>

<p>Related:<br>
<a href=""https://stats.stackexchange.com/questions/3238/time-series-clustering-in-r"">Time series &#39;clustering&#39; in R</a><br>
<a href=""https://stats.stackexchange.com/questions/17772/how-to-cluster-longitudinal-variables"">How to cluster longitudinal variables?</a>  </p>
"
"0.120168353625222","0.132453235706504","121506","<p>I have both numeric and binary data in my data set with 73 observations.
I read a lot about which distance metric and which clustering technique to use especially from this web site.
I decided to use Gower distance metrics and K-medoids.
In R, I used package ""cluster"", and function ""daisy"" with metric=""gower"".
So I got a 73*73 matrix.
Now, as I understood, this is not a distance matrix, it is a similarity matrix that I am confused what to do after now.
I use function pam: pam(x, k, diss = inherits(x, ""dist"")...
Should I use the 73*73 matrix which I got from daisy function? </p>
"
"0.060084176812611","0.0662266178532522","122532","<p>I have 17 numeric and 5 binary (0-1) variables, with 73 samples in my dataset. I know that the Gower distance is a good metric for datasets with mixed variables.</p>

<p>When I use daisy function in cluster package, with metric=""gower"" I don't want to standardize my numeric variables, bcs. they are already in percent (0.25, 0.35...).</p>

<p>I also don't want it, becuase I couldn't get any good results when I used this Gower distance matrix for clustering with pam {cluster}, and I thought this standardization process may cause bad results. </p>

<p>So, is there any way not to standardize?</p>
"
"0.158967789576202","0.150187852296528","127536","<p>Sampling weights, the inverse probability of a unit's selection into the sample, and other more complex and adjusted weights are very often used in the social sciences. There is statistical software that allows weighting of observations/cases, like the <code>hclust</code> function from the <code>R</code>-package <code>cluster</code>. </p>

<p>In regression analysis, there is an ongoing debate when the usage of observation weights is appropriate (see e.g. Winship/Radbill 1994). I could not find anything concerning observation weights in textbooks about cluster analysis, if weighting is discussed, it is mostly about variable weighting. One exemption is the manual of the <code>R</code>-package <code>WeightedCluster</code>, which discusses observation weighting in more detail. The documentation of the <code>cluster</code> package is not very helpful, as it only shows a trivial example using the weighting option <code>hclust(..., members=""..."")</code> where the number or weight of cases is untouched.</p>

<ol>
<li>Therefore, I am looking for references and recommendations with observation/case weighting in cluster analysis, especially hierarchical cluster analysis. </li>
<li>As I could not find the actual formula for the <code>hclust(..., members=""..."")</code> function : Which parameters changes in the hierarchical cluster algorithm if one uses observation weights? How does that affect the algorithm?</li>
</ol>

<p>In order to get an idea of the difference between clustering with and without case weights, here is an example using weights from survey data and the R-code:
<img src=""http://i.stack.imgur.com/BYiLY.png"" alt=""Reweighting of clustering by using membership""></p>

<pre><code>require(survey)
data(api)
whc &lt;- hclust(dist(apiclus2[, c(""pct.resp"", ""meals"")]), method=""ward.D2"", 
              members=apiclus2$pw)
uwhc &lt;- hclust(dist(apiclus2[, c(""pct.resp"", ""meals"")]), method=""ward.D2"")
opar &lt;- par(mfrow = c(1, 2))
plot(whc,  labels = FALSE, hang = -1, main = ""Weighted survey data"")
plot(uwhc, labels = FALSE, hang = -1, main = ""Unweighted survey data"")
</code></pre>

<h3>References</h3>

<ul>
<li>Studer, M., 2013: WeightedCluster Library Manual. A practical guide to creating typologies of trajectories in the social sciences with R. LIVES Working Papers 24. Lausanne.</li>
<li>Winship, C. &amp; L. Radbill, 1994: Sampling Weights and Regression Analysis. Sociological Methods &amp; Research 23: 230â€“257.</li>
</ul>
"
"0.208137693940788","0.229415733870562","132629","<p>I was trying to implement some clustering tendency tools in R, namely the Hopkin's index and the Coxâ€“Lewis index.</p>

<p>Here is the <a href=""https://books.google.com.sg/books?id=QgD-3Tcj8DkC&amp;pg=PA899&amp;lpg=PA899&amp;dq=clustering+tendency+%5BSergios_Theodoridis,_Konstantinos_Koutroumbas&amp;source=bl&amp;ots=lUUv2H7uh4&amp;sig=oq2Ax1wtX2MnHSrMvuOqLSSj9cU&amp;hl=en&amp;sa=X&amp;ei=VuitVO6kBoySuATXo4KYCA&amp;ved=0CCMQ6AEwAQ#v=onepage&amp;q=clustering%20tendency%20%5BSergios_Theodoridis%2C_Konstantinos_Koutroumbas&amp;f=false"" rel=""nofollow"">link</a> at page 901 to show what they are</p>

<p>This is what I managed to come up with in R. For the distance, I use the Euclidean distance. I also use additional R packages: data.table and RANN</p>

<pre><code>library(data.table)
library(RANN)
hopkins=function (data)
{
   #Number of samples
   m = round(0.2 * nrow(data))

   #Index of my data to be choosen as data samples
   sample = round(runif(m, 0, nrow(data)))

   #Get distance to nearest neigbour for each sample (W)
   #Get rid of first column as it is 0
   nearestW = nn2(data,data[sample,],k=2)$nn.dists[,-1]

   #Get the random sample from uniform distribution for each column
   samp = function(data,no.of.samples,min,max)
   {
    #Get the first and last quantile for every column
    quantile = quantile(x=data,probs=c(0.25,0.75))
    points = runif(n=no.of.samples,min=quantile[1],max=quantile[2])
    return(points)
   }

   uniform.sample = data[, lapply(.SD,samp,no.of.samples=m), .SDcols=names(data)]

  #Get distance to nearest data for each uniform sample (U)
  nearestU = nn2(data,uniform.sample,k=1)$nn.dists[,1]

  #Apply Hopkins index
  return(sum(nearestU)/(sum(nearestW)+sum(nearestU)))
}

CoxLouis=function (data)
{
  #Number of samples
  m = round(0.2 * nrow(data))

  #Get the random sample from uniform distribution for each column
  samp = function(data,no.of.samples,min,max)
  {
    #Get the first and last quantile for every column
    quantile = quantile(x=data,probs=c(0.25,0.75))
    points = runif(n=no.of.samples,min=quantile[1],max=quantile[2])
    return(points)
  }

  uniform.sample = data[, lapply(.SD,samp,no.of.samples=m), .SDcols=names(data)]

  #Get distance to nearest data for each uniform sample (U)
  nearest = nn2(data,uniform.sample,k=1)
  nearestU = nearest$nn.dists[,1]

  #Get the index of the nearest data
  nearest.dataindex = nearest$nn.idx[,1]

  #Get distance to nearest neigbour for each data (W)
  #Get rid of first column as it is 0
  nearestW = nn2(data,data[nearest.dataindex,],k=2)$nn.dists[,-1]

  #Apply Coxâ€“Lewis index
  return(mean(nearestU/nearestW))
}
</code></pre>

<p>Running hopkins(data.table(iris[,1:4])) gives 0.756792 and Coxâ€“Lewis(data) gives 2.709349.</p>

<p>On the other hand, the lecture notes from this <a href=""http://www.cs.rpi.edu/~zaki/www-new/pmwiki.php/Dmcourse/Main?action=download&amp;upname=chap18.pdf"" rel=""nofollow"">link</a> gives a much higher value of 0.935 for the Hopkin's index. I suspect that it has something to do with the way I am creating samples from a uniform distribution. </p>

<p>What I did was to generate uniform samples at random along each dimension. As for the min and max values, I use the first and last quantile respectively instead of the data min and max value. I thought that it will be more robust.</p>

<p>Can someone enlighten me if I am indeed going on the right direction for that ?  </p>
"
"0.0849718577324175","0.0936585811581694","134538","<p>I have multiple dataframes each representing traffic speed for each day of the year (366 dataframes for 366 days of the year). The raws of the dataframe are timestamp from 00:00 to 23:55 at 5 minute intervals and the columns are mileposts at 0.5 mile intervals and the entries are speed of traffic corresponding to the specific time and milepost. </p>

<p>I want to group days of similar traffic conditions to examine daily traffic patterns/variations, which is standard for traffic analysis at a macro level, e.g., examining traffic patterns during weekdays and weekends.</p>

<p>To do this, I will have to measure similarity of the dataframes and apply clustering algorithms. Any idea on how to calculate similarity of dataframes and cluster them? Any R package that can do this?</p>

<p>Thanks</p>
"
"0.169943715464835","0.187317162316339","139490","<p>I am looking to group/merge nodes in a graph using graph clustering in 'r'.</p>

<p>Here is a stunningly toy variation of my problem.</p>

<ul>
<li>There are two ""clusters""</li>
<li>There is a ""bridge"" connecting the clusters</li>
</ul>

<p>Here is a candidate network:<br>
<img src=""http://i.stack.imgur.com/2dqy4.png"" alt=""enter image description here""></p>

<p>When I look at the connection distance, the ""hopcount"", if you will, then I can get the following matrix :</p>

<pre><code> mymatrix &lt;- rbind(
     c(1,1,2,3,3,3,2,1,1,1),
     c(1,1,1,2,2,2,1,1,1,1),
     c(2,1,1,1,1,1,1,1,2,2),
     c(3,2,1,1,1,1,1,2,3,3),
     c(3,2,1,1,1,1,1,2,3,3),
     c(3,2,1,1,1,1,1,2,2,2),
     c(2,1,1,1,1,1,1,1,2,2),
     c(1,1,1,2,2,2,1,1,1,1),
     c(1,1,2,3,3,2,2,1,1,1),
     c(1,1,2,3,3,2,2,1,1,1))
</code></pre>

<p>Thoughts here:</p>

<ul>
<li>By luck or due to the simplicity of the toy the matrix has obvious patches this is not going to be the case in the (very large) matrix.  If I randomized the relationship between point and row then it would not be so clean.</li>
<li>I might have got one wrong - so if I have a typo, let me know.</li>
<li>Hop-count here is shortest number of hops to connect point on row i with point on column j.  A self-hop is still a hop, so the diagonal is all ones.</li>
</ul>

<p>So in this matrix larger distance (hops) has a higher number.  If I wanted a matrix showing ""connectivity"" instead of distance, then I could do a dot-inverse, where each cell of the matrix is replaced with its multiplicative inverse.</p>

<p><strong>Questions:</strong>   </p>

<p>To help me find my own way:</p>

<ul>
<li>What are the terms for reducing the number of nodes on a graph by combining them?  Is it clustering, merging, munging - what are the words that I should use?</li>
<li>What are the proven techniques?  Is there a textbook on the topic?  Can you point to papers or websites?</li>
<li>Now I tried to look here first  - it is a great ""first check"" spot. 
I didn't find what I was looking for.  If I missed it (not unlikely)
can you point me to an answered question or two on the topic here at
CV?</li>
</ul>

<p>To get me where I am going:    </p>

<ul>
<li>Is there an 'R' package that will properly cluster the nodes on the network?</li>
<li>Could you point me to example code to do this?</li>
<li>Is there an 'R' package that will graphically present the resulting reduced network?</li>
<li>Could you point me to example code to do this?</li>
</ul>

<p>Thanks in advance.</p>
"
"0.147175574806061","0.162221421130763","140523","<h2>Outline of clustering technique using Random Forest</h2>

<p>A synthetic data is created by randomly sampling from the data of interest. It is used as the base line to measure the ""structureness"" or ""clustering"" in the data of interest.
The real data and synthetic data are combined and fed into the randomForest() to do classification.The distance matrix is calculated from the proximity measure of the outputted random forest and is clustering is done on the distance matrix using other clustering techniques.</p>

<h2>Issues:</h2>

<p>1)Sampling technique </p>

<p>The paper by Shi et al.  (<a href=""http://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering/RandomForestHorvath.pdf"" rel=""nofollow"">http://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering/RandomForestHorvath.pdf</a>)
describes two sampling techniques- (1)random sampling from the product of empirical marginal distributions of the variables of the data and (2)random sampling (uniform distribution) from the hyper rectangle containing the data.</p>

<p>2)No. of forests</p>

<p>Shi et al. reported that ""RF dissimilarity can vary considerably as a function of the particular realization of the synthetic data"". So a number of forests are grown and are combined to get the final result.</p>

<h2>Question:</h2>

<p>Which sampling technique does randomForest() function from randomForest package uses ? Also, how many forests are grown?</p>
"
"NaN","NaN","148597","<p>I am working on a clustering model with the kmeans() function in the package stats and I have a question about the output. </p>

<p>My data is a sample from several tech companies and AAPL._UP is a variable equal to ""1"" if apple was up on that particular day. </p>

<p>I ran a kmeans algorithm with a k=16 and it gave me some output. I can interpret most of it but I'm just not sure what I'm looking at here. Can some one let me know what these numbers mean?</p>

<p>There is a picture of what I am looking at <img src=""http://imgur.com/OnKHCLX.jpg"" alt=""picture""></p>

<p>If it helps, here are the cluster assignments <img src=""http://imgur.com/jom3h05.jpg"" alt=""picture""></p>
"
"0.134352303725115","0.148087219439773","149707","<p>I have data that refer to the number of occurrences of specific variable in samples:</p>

<pre><code>       V1  V2  V3 ...
sample1 0   2   1
sample2 7   1   0
sample3 1   4   1  
....
</code></pre>

<p>The data refers to the occurrence of genes(V1...) in different genomes (sample1..).</p>

<p>I want to perform a cluster analysis combined with an heat map.
I used the function <code>heatmap.2</code> in the <code>gplot</code> package in R.
I used Euclidian distances for calculating the distance among the samples. 
The clustering algorithm is the default one for the function <code>hclust</code> in R (<code>hclust(d, method = ""complete"", members = NULL)</code>).
However, I am not completely sure it is the right method. 
Any suggestion on how to choose the right method to calculate the distances among my samples?</p>

<p><strong>EDIT</strong>
The aim is to describe the distribution of the variables (genes) among the samples (genome), and cluster the samples(genomes) according with the values that each variables assume (meaning, how many specific genes are present)</p>
"
"0.060084176812611","0.0662266178532522","149852","<p>I want to calculate the cophenetic correlation coefficient.
reading previous posts  </p>

<p><a href=""http://stats.stackexchange.com/questions/92546/comparison-of-cophenetic-correlation-coefficients-on-different-data-sets"">Comparison of cophenetic correlation coefficients on different data sets</a></p>

<p><a href=""http://stats.stackexchange.com/questions/33066/on-cophenetic-correlation-for-dendrogram-clustering"">On cophenetic correlation for dendrogram clustering</a></p>

<p><a href=""http://stackoverflow.com/questions/5639794/in-r-how-can-i-plot-a-similarity-matrix-like-a-block-graph-after-clustering-d"">http://stackoverflow.com/questions/5639794/in-r-how-can-i-plot-a-similarity-matrix-like-a-block-graph-after-clustering-d</a></p>

<p>I used the <code>cophenetic</code> function in the package <code>stats</code>. 
As far as I understand the results are cophenetic distances for the hierarchical clusteringis, in a new object of class <code>dis</code>. </p>

<pre><code>coph&lt;-cophenetic(hclsut_result)
</code></pre>

<p>To have an overview I clustered the cophenetic matrix, and I obtained the same clustering  as the one performed on my dataset.</p>

<p>However, I wanted to have an unique value that indicate the fidelity with wich my clsutering represent my distance matrix. Therefore, I correlated the <code>dis_matrix_for_my_dataset</code> with the <code>coph</code>.</p>

<pre><code>cor(euclidian_dist, coph)
</code></pre>

<p>Am I understanding right that the value I obtain indicates the <strong>cophenetic correlation coefficient</strong>?</p>
"
"0.190002850064127","0.209426954145848","151961","<p>*Please note this question is about the Platt probabilistic output and SVM class assignment, not about the code or the package itself. It just happens to be the code where I stumbled on the issue.</p>

<p>In <a href=""http://stats.stackexchange.com/questions/147260/including-class-probabilities-might-skew-a-model-in-caret"">another question</a> I asked about bad models coming from <code>caret</code> and associated <code>kernlab</code> when <code>prob.model=TRUE</code>. I found the answer myself, in both <a href=""http://stackoverflow.com/questions/29766951/different-results-with-caret-when-classprobs-true"">stackoverflow</a> and <a href=""http://r.789695.n4.nabble.com/Inconsistent-results-between-caret-kernlab-versions-td4680500.html"" rel=""nofollow"">from Max Kuhn himself</a>:</p>

<blockquote>
<pre><code>&gt; predict(newSVM, df[43,-1]) [1] O32078 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
&gt; predict(newSVM, df[43,-1], type = ""probabilities"")
     O27479     O31403     O32057    O32059    O32060     O32078
[1,] 0.08791826 0.05911645 0.2424997 0.1036943 0.06968587 0.1648394
     O32089     O32663     O32668     O32676
[1,] 0.04890477 0.05210836 0.09838892 0.07284396
</code></pre>
  
  <p>Note that, based on the probability model, the class with the largest
  probability is O32057 (p = 0.24) while the basic SVM model predicts
  O32078 (p = 0.16).</p>
  
  <p><strong>Somebody (maybe me) saw this discrepancy and that led to me to follow
  this rule:</strong></p>

<pre><code>if(prob.model = TRUE) use the class with the maximum probability   
  else use the class prediction from ksvm().
</code></pre>
  
  <p>Therefore:</p>

<pre><code>predict(svm.m1, df[43,-1])
 [1] O32057
 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
</code></pre>
</blockquote>

<p>Isn't that innacurate? <code>kernlab</code> searches for the optimal probability cutoff that minimizes error, that's why the assigned class and the maximum probability don't match: they don't have to.</p>

<p>Check this reproducible example. I excluded two cherrypicked <code>virginica</code> samples.</p>

<pre><code>require(kernlab);require(caret);
#kernel=polynomial; degree=3; scale=0.1; C=0.31
set.seed(101);SVM&lt;-ksvm(Species~., data=iris[-c(135,150),], kernel='polydot',C=.31, kpar=list( scale=.1, degree=3), prob.model=T)
</code></pre>

<p>Here's the resulting model </p>

<pre><code>&gt; SVM
Support Vector Machine object of class ""ksvm"" 

SV type: C-svc  (classification) 
 parameter : cost C = 0.31 

Polynomial kernel function. 
 Hyperparameters : degree =  3  scale =  0.1  offset =  1 

Number of Support Vectors : 58 

Objective Function Value : -1.4591 -0.7955 -10.2392 
Training error : 0.033784 
Probability model included. 
</code></pre>

<p>Now let's check the predicted class probabilities in those two samples</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5], type=""probabilities"")
          setosa versicolor virginica
[1,] 0.008286638  0.4414114  0.550302
[2,] 0.013824451  0.3035556  0.682620
</code></pre>

<p>And the class predictions</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5])
[1] versicolor virginica 
Levels: setosa versicolor virginica
</code></pre>

<p>Sample 150 was assigned to <code>virginica</code>, with a class probability of around 0.68. Sample 135 was assigned to <code>versicolor</code> with a probability of around 0.44, yet <code>virginica</code> probability nicely sits around 0.55.
Looking at several CV folds, we perceive that kernlab only assigns <code>virginica</code> when its probability is over a given value (way higher than 0.5). That's the cutoff I mentioned, and it happens thanks to the well known bad clustering in <code>iris</code> between <code>virginica</code> and <code>versicolor</code>.</p>

<p>So, am I right on these suppositions and therefore is <code>caret</code> class assignment model (maximum probability) wrong?</p>

<p>EDIT:
I've been experimenting with pairwise probability coupling of Platt scaling (logistic regression fit), isotononic regression and a model I'm working on. A weakness (?) I perceived in Platt's model is the probability isn't bound to be 0.5 when the binary SVM decision output is 0, which is the expected result as the instance would lie exactly on the separating hyperplane.</p>
"
"0.0849718577324175","0.0936585811581694","157666","<p>I wish to try clustering a matrix of numerical data using swarm intelligence.
(Matrix is 28000 X 53 and sparse). I'm working in R and found the REPPlab package and used the EPPlab function. My question is how do I go from here to actually cluster my data? I know PSO can be used for clustering as well as optimisation but I'm stuck! </p>
"
"0.120168353625222","0.0993399267798783","157686","<p>While estimating from the <strong>survey data</strong> involving <strong>stratification</strong> &amp; <strong>clustering</strong> survey design and using survey package of <strong>r</strong>, is it possible to estimate at the <strong>cluster level</strong>? For eg; for following survey design:</p>

<pre><code>data(api)
## syntax for stratified cluster sample
dclus=svydesign(id=~dnum, strata=~stype, weights=~pw, data=apistrat, nest=TRUE)
</code></pre>

<p>This is an example which is reproduced from the survey package. Here, <em>dnum</em> is district, <em>stype</em> is Elementary/Middle/High School and <em>pw</em> is weight (inverse of probalitlity of the students selected from each of the strata). In this case, can I estimate population parameter at district level? For example, to estimate total enrollment for each of the district:</p>

<pre><code>svyby(~enroll,~dnum, design=dclus, svytotal)
</code></pre>

<p>I got the following output: </p>

<pre><code>dnum   enroll       se
 19   21751.32   21751.32
 20   10494.50   10494.50
 25    9416.73    9416.73
 27   26923.30   26923.30
 40   14843.30   14843.30
 41   25774.43   25774.43
</code></pre>

<p>I believe this estimation is not correct, as no where I have given the break up of total students at district level (cluster level). 
Any help would be greatly appreciated.</p>
"
"0.0849718577324175","0.0468292905790847","157849","<p>I have a df containing samples of a time varying quantity (namely, exposure to electromagnetic field generated by a GSM station), and I use the mixtools package to find a fitting mixture model (tipically, a 2- or 3-component guassian mixture) because I assume that (or rather, want to test if) samples come from different pdf's based on the hour of the day (e.g., with larger mean during peak hours and lower ones during off-peak hours).</p>

<p>It seems to me that normalmixEM function included in the package does not return any threshold values that can be used to assign samples to the various distributions: how can I achieve that?</p>

<p>Should that function be unavailable in the mixtools package, could you please point me to a more general tool that would help me clustering values, also considering that it should make its own initial guess of the threshold values?</p>

<p>Many thanks!
Nicola</p>
"
"NaN","NaN","164720","<p>I have a dataset with 200K samples (cases) and 30 variables. Every distance-based method for clustering or dimension reduction technique that I use, such as <em>DBSCAN</em>, <em>Hierarchical Clustering</em>, <em>LLE</em>, <em>Isomap</em> and ... fail to run on my machine (normally I get <code>R Session Terminated</code> error) due to the large distance file being generated. (<em>Distance calculation requires o(n^2) time and space</em>)</p>

<p>Is there any solution to handle this problem? Is there any good package for the mentioned clustering or dimensionality reduction in R or Matlab that is suitable ?</p>
"
"0.104068846970394","0.114707866935281","167734","<p>The data contains information about the genes causing breast cancer in our body. My aim is to find the most effective gene on breast cancer in our body. The code that I have first divides my sample by clustering and then shows number of genes which can separate samples into those clusters. To do the second function it uses "" mRMR.classic (mRMRe.ensemble)"" package from R. We want in this code is that instead of dividing samples by hierarchial clustering, we want to define clusters by ourselves and then this should find probesets to differentiate those two groups. However, I don't have any idea about how I can make this change. If you help me, I'm really gonna be happy.</p>
"
"0.140196412562759","0.176604314275339","168202","<p>I'm currently checking some clustering evaluation indexes in R, and now I'm using Silhouette and its respective function in R, ""silhouette"" (in ""cluster"" package). To test the method, I used the following code:</p>

<pre><code>data &lt;- matrix(c(1,2, 2,1, 1,1, 2,2, 8,9, 9,8, 9,9, 8,8, 1,15, 2,15, 1,14, 2,14),12,2,byrow=T)
clust &lt;- c(1,1,1,1, 2,2,2,2, 3,3,3,3)
diss &lt;- as.matrix(dist(data))
sil &lt;- mean(silhouette(clust,dmatrix=diss)[,3])
</code></pre>

<p>Using this data and with ""clust"" being the obtained configuration (from k-means), I would evaluate the silhouette of this configuration by the mean of the silhouettes for each datum. The point is that I searched for its use with k-means and found this page:</p>

<p><a href=""https://stat.ethz.ch/pipermail/r-help/2008-March/155939.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help/2008-March/155939.html</a></p>

<p>And it's recommended to use the squared distance matrix instead, making <code>sil &lt;- mean(silhouette(clust,dmatrix=diss^2)[,3])</code>. This use changes the result from 0.8793842 to 0.9850074.</p>

<p>The point for me is the evaluation of the configuration itself, and as I created the data to clearly show three groups, the higher silhouette for this configuration makes more sense to me than the lower one.</p>

<p>I'm not sure if I understood it right, but the use of the squared distance matrix on a k-means clustering evaluation is because of the squared distance of its cost functions. But is its use needed? I mean, the evaluation using the distance matrix would be enough to evaluate two different configurations (both resulting from k-means) and point which one is better.</p>

<p>So, should I use the squared distance for a k-means clustering evaluation? And as I'm evaluating the configuration, shouldn't the same distance matrix be used to evaluate many different methods?</p>

<p>Thanks in advance!</p>
"
"0.060084176812611","0","168717","<p>this is my first post.  I have an irregular time series that exhibits large shifts in both mean and in the direction of the trend.  It looks something like this (though this is far cleaner than reality):</p>

<pre><code>set.seed(1)
x = sort(rep(seq(as.POSIXct('2015/01/01'),as.POSIXct('2015/01/10'),by='day'),100) + runif(1000,0,80000))
y = c(seq(300),seq(90,70,-.2),seq(20,50,.1),seq(238.5,90,-.5)) + runif(1000,50,80)
plot(x,y)
</code></pre>

<p>The task is to:
 1. accurately partition/segment the data
 2. extract the change point indices
 3. fit regression separately for each segment</p>

<p>I have tried several routes, including:
a) hierarchical clustering based on dissimilarity matrix
b) function cpt.mean/var/meanvar from package changepoint (does not seem to work well)
c) function 'breakpoints' from package strucchange (slow and often inaccurate)
d) various types of kmeans (inappropriate, I know)</p>

<p>I have also explored some other packages, such as TSclust, urca, and DTW, but these seem better suited to clustering <em>sets</em> of time-series, not individual values within a time series.</p>

<p>Can someone point me in the correct direction? Perhaps I am not considering the appropriate data model?</p>

<p><strong>UPDATE</strong>
Thank you all for your very considered responses.  I went back to the strucchange package, and after some fiddling, have gotten it to work quite well.  I had not initially appreciated the h 'minimal segment size' argument.
Finished product:
<a href=""http://i.stack.imgur.com/qBgeY.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qBgeY.jpg"" alt=""enter image description here""></a></p>
"
"0.120168353625222","0.0993399267798783","168922","<p>My research buddy and I are conducting cluster analysis on survey data using a 7-point relevance scale (1=Not relevant, 7=Extremely Relevant). </p>

<p>We have 58 variables, arranged in 10 groups of variables (like marketing channel as a variable group, consisting of variables asking respondents to classify the relevance of a type of marketing channel to their business).</p>

<p>The individual variables have very varying distributions of responses, and the overall distribution across all responses is bimodal at 1 and 7, but still with a significant share of responses in the middle values. </p>

<p>We aim to find underlying constructs (in this instance, business models), by clustering (N=635) on their responses to these 58 variables. The research is highly exploratory, and we only have intuition for the variable relationships, both in and between variable groups. We have also conducted a PCA analysis giving 16 factors, which seem to make intuitive sense, but where we thereafter have some trouble applying and interpreting factor scores to a cluster context.</p>

<p>We would greatly appreciate any pointers to what clustering method would be appropriate for this type of dataset? If so, what would be the R package for this approach?</p>
"
"0.190002850064127","0.188484258731263","172617","<p><strong>Problem:</strong></p>

<p>I am figuring out the best way to find clusters for a dataset with observations that are densely packed together. The dataset is retail stores with three numeric variables based on operations metrics.</p>

<p>I do not know how to create a simulated dataset for an example like this. I have densely clustered data and outliers, but under 4k observations. </p>

<p><strong>Business objective:</strong></p>

<p>We need to separate the dataset into groups based on several variables.</p>

<p>The goal is to narrow down the stores with greater priority. Later on, we will use inference statistics for determining the cause of the operation metrics stated. Segmenting the stores based on priority makes sense through the three operations variables included.</p>

<p>I tried two different types of partitioning clustering methods, k-values, and different variables, but all yeilded poor validation results. Hereâ€™s the steps I took:</p>

<p><strong>Clustering with 2/3 variables:</strong></p>

<ol>
<li><p>Standardize in daisy dissimilarity matrix with euclidean distance <code>daisy()</code> function from <code>cluster</code> package in CRAN.</p></li>
<li><p>Chose k for k-means by looking at SSE chart <code>kmeans()</code> function.</p></li>
<li><p>Chose k for k-medoid by <code>pamk()</code> function in <code>fpc</code> package in CRAN for highest average silhouette width among clusters - resulted in a 0.23 average silhouette width. K-medoid was used with the <code>pam()</code> function from <code>cluster</code> package in CRAN.</p></li>
<li><p>Choose clustering algorithm by dunn-index - highest clustering result was k-medoids with 0.002. I used the <code>cluster-stats()</code> function in <code>fpc</code>.</p></li>
</ol>

<p><strong>Clustering with all three variables:</strong>
-same procedure as above.</p>

<p><strong>Result:</strong>
K-medoids with 2 clusters using two variables represented the algorithm with the highest dunn-indes. </p>

<p><strong>Overview:</strong>
After selecting the optimal number of clusters for each clustering method and comparing the best one using dunn-index, the results have overlap. </p>

<p>What is the recommended method for performing cluster analysis on densely clustered datasets? Do I need to perform clustering multiple times in order to segment the data further? </p>

<p><strong>EDIT: Added scatterplot showing clustering with 3 variables</strong></p>

<p><a href=""http://i.stack.imgur.com/ZoVyj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZoVyj.png"" alt=""Scatterplot with cluster labels color-coded""></a></p>
"
"0.147175574806061","0.135184517608969","176578","<p>I have multiple images from a 3D-Scanner in point cloud form. Part of the image is a fixture to hold the object to be scanned. I want to extract the object itself by classifying the fixture and the object in two separate classes = estimating a discriminant hyperplane, that cuts the cloud exactly at the points where the fixture touches the object. </p>

<p>2D Image of an SVM estimation (via the excellent klaR package) as an example:
<a href=""http://i.stack.imgur.com/vsohB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vsohB.png"" alt=""enter image description here""></a> 
Some bullet points about the external conditions:</p>

<ul>
<li>The objects vary in shape and size. </li>
<li>The position of the fixture varies a bit between images, since it is flexible</li>
<li>The origin of the images varies, since the calibration is sometimes off (due to temparature changes etc.)</li>
</ul>

<p><strong>The problem resulting from this</strong>: </p>

<p>The absolute position of an estimated hyperplane is the same, independent of the data. The point, where the point cloud of the object ends and the point cloud of the fixture begins, varies,  but this is the point where the hyperplane is needed in every picture. Optimally, they would change position dependent on the data. </p>

<p>Some bullet points about what i have tried so far:</p>

<ul>
<li>Despite knowing I exclude myself from excellent libraries such as PCL, I use R</li>
<li>Clustering works rather badly, since the point where the fixture touches the object is too large so it separates them not very well.</li>
<li>LDA, QDA, RPart, KNN and SVMLight give me between 70-90% accuracy when comparing to a manual classification, when centering and standardizing the images by themselves</li>
</ul>

<p><strong>My Question(s):</strong></p>

<p>What would you propose could be done about the fix hyperplanes? Do you think one could derive some sort of parameter from the image so the planes will be moved accordingly? </p>

<p>Is there maybe another Discriminant analysis method, that would be better suited here, preferrably with an R implementation? </p>

<p>Is there a feature that i should calculate, that could help me in the separation when added as a discriminating variable?</p>
"
"0.169943715464835","0.187317162316339","177796","<p>I am trying to make group together different datasets using unsupervised algorithms (clustering). The problem is that I have many features (~500) and a small amount of cases (200-300).</p>

<p>So far I used to do only classification problems for which I always had labeled data as training sets. There I used some criterion (i.e. random.forest.importance or information.gain) for preselection of the features and then I used a sequential forward selection for different learners to find the relevant features.</p>

<p>Now I see that in case of unsupervised learning I have neither any criterion for preselection nor can I use the sequential forward selection (at least not in the mlr package).</p>

<p>I was wondering if I could do a principal component analysis before to find a small number of features to fead to my clustering algorithm. Or do you have any other idea?</p>

<p>Thanks</p>

<p>edit:</p>

<p>Ok, so after some research online I can update my question a bit:
First of all I have read some articles that discourage the use of PCA before clustering algorithms, due to two reasons:</p>

<ul>
<li><p>The PCs are functions of all features so it is hard to relate the result to the inital data set and thus it is harder to interpret</p></li>
<li><p>Moreover, if you have the problem that in truth only a very small fraction of your features are helpful to do the clustering, it is not said that these features are also describing the largest variance among the samples (which is what the PCs do)</p></li>
</ul>

<p>So PCA is off the table...</p>

<p>Now I am back to my initial idea to do a sequential forward selection for clustering.</p>

<p>What performance measure would you recommend? (I thought about the Dunn-Index)
Which clustering algorithm would lead to clusters of more or less the same size? (for hierarchical clustering I usually get one cluster with a single outlier and another with all the rest -> so I would need something that somehow protects against outliers)</p>

<p>Hope you guys can help me...</p>
"
"0.120168353625222","0.132453235706504","181166","<p>I have a data set of deposits and withdrawals from bank locations, so each record includes a bank identifier, date stamp, number of deposits, and number of withdrawals. I have included reproducible code below.  Note that I have hundreds of days for each of thousands of agents. I have a few questions:</p>

<ol>
<li><p>I would like to use the forecast package, but I'd like it to take into account day of week patterns and month of year seasonality. The one worry I have about using the ""frequency"" flag is that I have some randomly missing days for each agent. How should I best deal with this?</p></li>
<li><p>Deposits and withdrawals cannot be negative so all of the data is non-negative. Can I force the forecast to be non-negative? I know I can use the lambda=0 flag, but this only works when everything is strictly positive (but not applicable in this case because deposits and withdrawals can be zero)</p></li>
<li><p>Is there anything I can do to increase predictive accuracy by ""clustering"" banks? Right now, I'm only using a particular bank's data, but given that I have data from thousands of more banks, perhaps I can take advantage of this?</p></li>
</ol>

<pre class=""lang-r prettyprint-override""><code>library(lubridate)
library(dplyr)
library(forecast)

Date = c(today()-1, today()-3, today()-4, today()-5) %&gt;% as.POSIXct
D    = c(10,3,4,3)
W    = c(13,2,4,4)
Bank = c(rep(1,4), rep(2,4))
A    = cbind(Date,D,W) %&gt;% as.data.frame 
A$Date = A$Date %&gt;% as.POSIXct(origin=""1970-01-01"")
B   = A
B$D = B$D - 1
B$W = B$W + 3
A   = A %&gt;% rbind(B)
A   = A %&gt;% cbind(Bank,.)
</code></pre>
"
"0.147175574806061","0.162221421130763","181566","<p>I have <strong>time-series</strong> data of 12 consumers. The data corresponding to 12 consumers (named as <code>a ... l</code>) is
<a href=""http://i.stack.imgur.com/hcHPc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hcHPc.png"" alt=""enter image description here""></a> </p>

<p>I want to cluster these consumers so that I may know which of the consumers have utmost similar consumption behavior. Accordingly, I found clustering method <a href=""http://www.inside-r.org/packages/cran/fpc/docs/pamk"" rel=""nofollow"">pamk</a>, which automatically calculates the number of clusters in input data.</p>

<p>I assume that I have only two options to calculate the distance between any two time-series, i.e., <a href=""https://en.wikipedia.org/wiki/Euclidean_distance"" rel=""nofollow"">Euclidean</a>, and <a href=""https://en.wikipedia.org/wiki/Dynamic_time_warping"" rel=""nofollow"">DTW</a>. I tried both of them and I do get different clusters. Now the question is which one should I rely upon? and why?</p>

<p>When I use <code>Eulidean</code> distance I got following clusters:
<a href=""http://i.stack.imgur.com/0Irqg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0Irqg.png"" alt=""enter image description here""></a></p>

<p>and using <code>DTW</code> distance I got
<a href=""http://i.stack.imgur.com/AUwub.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AUwub.png"" alt=""enter image description here""></a></p>

<p><strong>Conclusion:</strong>
  How will you decide which clustering approach is the best in this case?</p>
"
"0.208137693940788","0.210297756048015","182232","<p>I have time-series data containing 1440 observations and the plot of the data is
<a href=""http://i.stack.imgur.com/LWkw7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LWkw7.png"" alt=""enter image description here""></a></p>

<p>I want to fit the Gaussian Mixture Models (GMM) to the above plot, and for the same I am using Mclust function of <a href=""https://cran.fhcrc.org/web/packages/mclust/index.html"" rel=""nofollow"">mclust</a> package. Finally, I want a fit somewhat like this:
<a href=""http://i.stack.imgur.com/zTtjJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zTtjJ.png"" alt=""enter image description here""></a></p>

<p>On using Mclust function, I do get following statistics</p>

<pre><code>   mclus_data &lt;- Mclust(givendataseries)
   &gt; summary(mclus_data)
----------------------------------------------------
Gaussian finite mixture model fitted by EM algorithm 
----------------------------------------------------

Mclust E (univariate, equal variance) model with 8 components:

 log.likelihood    n df      BIC      ICL
       9525.438 1440 16 18934.52 18183.67

Clustering table:
   1    2    3    4    5    6    7    8 
1262    0    0    0    0   13  114   51 
</code></pre>

<p>In the above statistic, I can not understand following:</p>

<ol>
<li>Significance of <code>log.likelihood</code>, <code>BIC</code> and <code>ICL</code>. I can understand what each of them is, but what their magnitude/value refers to?</li>
<li>It shows there are 8 clusters, but why cluster no. <code>2,3,4,5</code> has <code>0</code> values? What does this mean?</li>
<li>From the plot it is clear that there must be two Guassians, but why <code>Mclust</code> function shows there are 8 Guassians?</li>
</ol>

<p><strong>Update:</strong>
Actually, I want to do model based clustering of time series data. But currently  I want to fit the distribution to my raw data, as shown in Figure 1 on page no. 3 of <a href=""https://www.dropbox.com/s/q50e9q168lt27si/VerstileClusteringMethod.pdf?dl=0"" rel=""nofollow"">this</a> paper. For your quick reference, mentioned figure in said paper is
<a href=""http://i.stack.imgur.com/8Jq1B.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8Jq1B.png"" alt=""enter image description here""></a></p>
"
"0.19997222800792","0.220415507511194","188235","<p>Recently I tried to cluster the <code>semeion.data</code> file available in <a href=""http://archive.ics.uci.edu/ml/datasets/Semeion+Handwritten+Digit"" rel=""nofollow"">UCI repository</a> using <code>kmeans</code> clustering as well as using the <code>neuralgas</code> method available in R's <code>flexcust</code> package. This file contains handwritten digits and pixel values are either 0 or 1 in an image of 16 X 16 pixels. </p>

<p>I have slightly modified the <code>semeion.data</code> file in that it contains just 257 columns with 256 pixel-values, as usual, and the last column is the digit label for that image: (0,1,2..9). Data set size is 1593 X 257.</p>

<p>I removed the 257th column and then tried to cluster the remaining dataset first, using k-means and then using <code>neuralgas</code> from the <code>flexclust</code> package. The results from <code>neuralgas</code> method are quite superior. The R-code and the results in both cases are as follows:</p>

<pre><code># R-code using kmeans without neuralgas
data &lt;- read.csv(""mod_semeion.csv"", header=FALSE)   # Data has no headers   
df   &lt;- data[,-257]                                 # df Data frame without digits labels
cl   &lt;- kmeans(df, 10 , iter.max = 500, nstart=200) # create 10 clusters
table(cl$cluster,data[,257])                         # Which cluster has which digits

       0   1   2   3   4   5   6   7   8   9
  1    0   0   1   4   9  69   1   5   8  75    # Cluster 1 has both digits 5 and 9
  2   85   0   0   0   0   1   1   0   0   1    # Cluster 2 has mainly digit 2
  3   70   0   0   0   0   2  38   0   1   1
  4    0  55   6   0  19   6   5 131   5   8    # Cluster 4 has mainlt digit 7
  5    0   2   0  96   0  63   1   0  19  44
  6    0  99  14   7   9   0   0  20   1   3
  7    0   4  96  43   2   6   0   0  17   6
  8    3   1   3   0 119   2   1   1   0   1    # Has mostly digit 4
  9    2   0  35   9   0   3   0   1 103  19
  10   1   1   4   0   3   7 114   0   1   0    # has mostly digit 6
</code></pre>

<p>Columnwise, for example, digit 7 is mostly clustered in cluster 4,
and digit 6 is mostly clustered in cluster 10. </p>

<pre><code># Distribution of digits in the data-set are as:
table(data[,257])   # Also, the column totals in each of the above 10 columns   (0-9)   
  0   1   2   3   4   5   6   7   8   9 
161 162 159 159 161 159 161 158 155 158 

# Cluster-wise points distribution are as:
table(cl$cluster)    # row-population/per-cluster
  1   2   3   4   5   6   7   8   9  10 
 172  88 112 235 225 153 174 131 172 131 

# Average relative cluster purity with kmeans: 
#  (max-of-a-digit)/cluster-population = (max-per-row/row-population)
((75/172) + (85/88) + (70/112) + (131/235) + (96/225) + (99/153) + (96/174) + 
 (119/131) + (103/172) + (114/131))/10
[1] 0.6587315  
</code></pre>

<p>Using the neural gas code, the average relative cluster purity is around 70% as against 65.8% as above. The code and the results are as below:</p>

<pre><code># R-code with neuralgas method
library(flexclust)  
obj &lt;- new(""cclustControl"")
obj@iter.max &lt;- 500
cl1 &lt;- cclust(df, 10, dist=""manhattan"", method=""neuralgas"", control=obj)
table(cl1@cluster, data[,257])  # which cluster has which digits

       0   1   2   3   4   5   6   7   8   9
  1  150   0   0   0   2   2  38   0   1   3    # Has mostly digit 0
  2    2   0  22   7   0   0   0   0  94  11    # has mostly digit 8
  3    0 105  24  11   9   0   0  25   2   6
  4    6   3   4   1   7  11 113   1   2   1
  5    2   1   4   0 118   1   0   0   0   0
  6    0  45   0   0  17   2   4 129   3   4
  7    1   2   1   1   4  71   1   3   8  29
  8    0   2   0 135   0  69   5   0  25  37
  9    0   4 104   3   1   3   0   0   8   1
  10   0   0   0   1   3   0   0   0  12  66


# Cluster-wise points distribution is as:
table(cl1@cluster)      # row-wise-total / per-cluster

  1   2   3   4   5   6   7   8   9  10 
196 136 182 149 126 204 121 273 124  82 

# Average relative cluster purity with neuralgas
((150/196) + (94/136) + (105/182) + (113/149) + (118/126) + (129/204) +  
 (71/121) + (135/273) + (104/124) + (66/82))/10
[1] 0.7085526
</code></pre>

<p>As can be seen from the above two cross-tables, neuralgas improves 
cluster purity and some digits are recognized much better using
neuralgas. For example, most 0's are now clustered together in cluster 1.</p>

<p>My question is: What is the neural gas method? Is it possible to explain how it works conceptually / in simple terms?</p>
"
"0","0.0662266178532522","188816","<p>I am trying to do k-means clustering in R using the cclust package.
In k-means clustering, the initial centroid assignment greatly affects the final allocation. The kmeans package has an nstart option, which guarantees that your results are based on 'nstart' number of initial configurations. Is there an equivalent option for the same in the cclust package?</p>

<p>Thanks.</p>

<p>TAK</p>
"
"0.104068846970394","0.114707866935281","189163","<p>I want to use the <code>blockcluster</code> package in <code>R</code> to perform co-clustering on my data. But the function requires the number of row and column clusters to be prespecified. How do you decide the numbers of row and column clusters? Shall I do k-means clustering on the rows and columns separately prior to the use of <code>blockcluster</code>? </p>
"
"0.147175574806061","0.162221421130763","189164","<p>I have a data set with 9000 instances and 40 attributes of mixed data, that is categorical and numeric. My target is to group them into clusters using whichever clustering algorithm works best. I've heard/read that for such a data set Gower distance is suitable. My question is can I combine two (or n) metrics for calculating distances between instances, for example I would like to use let's say Euclidean distance on numeric attributes and Gower distance on categorical attributes. I could always divide my data set into two data sets, one with numeric attributes and the other with categorical. But how could one interpret each result? Summing them up just sounds ... wrong.</p>

<p>My second question is what exactly does Gower distance do with numeric values? Does my first question even make sense? <br>
<br>
Here is a snippet of my code, I am using R and functions <code>daisy</code>, <code>agnes</code>from package <code>cluster</code>: <br></p>

<pre><code>df.diss &lt;- daisy(df, metric = ""gower"", type = list(numeric = c(1, 4, 6, 8, 9, 11, 12, 13, 14, 17 : 37), symm = c(2, 3, 5, 7)), stand = FALSE)
df.clust &lt;- agnes(df.diss)
</code></pre>

<p>Using these functions or even R is not a must.</p>
"
"0.134352303725115","0.148087219439773","190168","<p>i'm not really experienced in spatial stats yet, but i'm growing into it. </p>

<p>I basically want to ascertain if certain values in a raster are a) autocorrelated and b) are more likely to exist in a certain spatial area (k-means? i'm unsure)</p>

<p>the data is a single raster showing land use change from 1 year to another (sorry I can't post the data) and there are about 50 possible changes (some far more prevalent than others). Quickly viewing the raster, it is clear that some changes are more prevalent in northern extremes, some in areas of upland farming etc etc, patterns do exist. But I want to prove this with stats. </p>

<p>For a) Local Moran's I (using a simple binary queen's spatial-weights matrix) gives us indication of spatial autocorrelation - this is useful for finding 'clumps' of similar data, correct?</p>

<p>For b) I'd like to explore whether each change combination is more likely to exist in a certain part of the UK (in Scotland, in western extremes etc). Would this be some sort of k-means clustering?</p>

<p>I'm doing all this in R,</p>

<p>thanks for any help 
(<a href=""http://gis.stackexchange.com/questions/90691/r-raster-package-morans-i-interpretation/108558#108558"">this question</a> had some good info re Moran's I),
(this <a href=""http://stats.stackexchange.com/questions/9739/clustering-spatial-data-in-r"">question</a> seems to start out with a similar goal, finding regional patterns in surface sea temps but fizzled out).</p>
"
"0.158967789576202","0.175219161012616","190284","<p>I am using the <code>mice</code> package in R to create multivariate imputed datasets. For this, I am using <code>mice(data, meth= ""rf"")</code>function. I want all the variables to use this method for prediction because there are many non-linear relationships and hypothetical interactions between several of the variables. <a href=""http://aje.oxfordjournals.org/content/early/2014/01/12/aje.kwt312.full"" rel=""nofollow"">http://aje.oxfordjournals.org/content/early/2014/01/12/aje.kwt312.full</a></p>

<p>My dataset is also a hierarchical dataset with clustering. Is there a way to take this clustering into account when specifying the prediction matrix for the <code>mice</code> function? I am aware that if you use other methods like <code>meth = 2l.pan</code> or <code>meth =2l.norm</code>, you can take clustering into account, but I want to avoid these methods because I think the non-linear relationships trump the homogeneity introduced by the cluster effect. </p>

<p>If there is a way to use the random forest method with MICE and take clustering into account, could you also provide some mock code for specifying the prediction matrix?</p>
"
"0.0849718577324175","0.0936585811581694","191033","<p>I've just started reading about clustering and classification. It's a djungle, a fascinating one. Currently, however I have a rather urgent task, i.e to perform a sort of cluster analysis in the sense that I'd like to cluster my patients according to their phenotypes (biomarkers - continuous and categorical variables) and examine whether survival differs according to cluster. I'm not interested in any specific predictor, the purpose is merely to examine whether there are specific clusters of patients and whether the phenotypes associate with outcomes.</p>

<p>I'm looking for general advice on what type of <strong>method</strong> to use as well as recommended <strong>R package</strong>. I have 10 variables that are relevant for the phenotype. I could attach some data but I doubt it would contribute to the question, which is of more general character.</p>

<p>Thanks in advance.</p>

<p><strong>Update</strong>:
I'm looking for pros and cons of various techniques, with application to these kind of data. And I humbly understand that clustering may not be that straight forward.</p>
"
"0.104068846970394","0.114707866935281","197669","<p>I'm doing the following maximum likelihood estimation using mle2 function from <em><a href=""https://cran.r-project.org/web/packages/bbmle/index.html"" rel=""nofollow"">bbmle</a></em> package:</p>

<pre><code>library(bbmle)
llik.probit2&lt;- function(aL,beta, Ks, Kw, Bs, Bw, dta){
  Y &lt;- as.matrix(dta$qualified)
  sce1 &lt;- as.matrix(dta$eds)
  wce1 &lt;- as.matrix(dta$edw)
  sce1_obs &lt;- (as.matrix(dta$eds_obs))
  wce1_obs &lt;- (as.matrix(dta$edw_obs))
  obs &lt;- as.matrix(dta$CombinedObservable1)
  c &lt;- as.matrix(dta$const)
  phi &lt;- pnorm(ifelse(Y == 0, -1, 1) * (-aL*c + beta*obs + (Bs+Bs*Ks-aL)*sce1 -beta*Ks*sce1_obs + (Bw+Bw*Kw-aL)*wce1 - beta*Kw*wce1_obs), log.p = TRUE)
  -sum(phi)
}

starting.pointmle2 &lt;- list(aL=1.4, beta=0.3, Ks=0.5, Kw=0.5, Bs=0.5, Bw=0.5)

result1 &lt;- mle2(llik.probit2, start = starting.pointmle2, data=list(dta=Mydata), skip.hessian=FALSE)
</code></pre>

<p>I need to estimate clustered standard  errors of this model, clustering on variable <em>c_id</em>.  I'm trying to implement the <em>sandwich</em> estimator but cannot retrieve 'meat' part from mle2 output. I have also tried with <em>nlm</em>, and <em>optim</em> (mle2 is basically wrapper for these methods)
Any advice how to do it? In general is there any package which provides functions to calculate clustered standard errors for MLE estimation of a general function?  </p>

<p>Here is is a small sample of the data file (MyData) in csv format:</p>

<pre><code>const,qualified,eds,edw,eds_obs,edw_obs,CombinedObservable1,c_id
1,0,0,1,0,0,-0.6838316166,1
1,1,0,1,0,0,-0.1433684328,1
1,0,0,1,0,0.0758113685,0.0758113685,1
1,0,1,0,0.2084778637,0,0.2084778637,34
1,0,0,1,0,0,-0.1622519262,34
1,0,0,1,0,0,-0.5061082675,34
1,0,0,1,0,0,-0.6952748613,34
1,0,1,0,0.9883178986,0,0.9883178986,34
1,0,0,1,0,0,-0.5311805315,34
1,0,0,1,0,0,2.7546881325,34
1,1,1,0,-0.2174263974,0,-0.2174263974,34
1,0,0,1,0,0,-0.4397037288,1
1,0,0,1,0,0,-0.3189328097,1
1,0,0,1,0,0,-0.6132276964,12
1,0,0,1,0,0,0.2459941348,12
1,0,0,1,0,0.0589196966,0.0589196966,12
</code></pre>
"
"0.0849718577324175","0.0936585811581694","200687","<p>I am using ClustOfVar package in R to cluster both nominal and interval variables for dimensionality reduction.</p>

<p>Can someone please explain what is squared loading values. how to pick representative from each cluster based on squared loading? I am working on an anonymised data without data dictionary.</p>

<p>Here is the sample data generated in R to test the clustering. My original data is much similar to this.</p>

<pre><code>m&lt;- matrix(sample(c(1:40),1000, replace = T),100,byrow=T)

df&lt;- as.data.frame(m)

df$V11&lt;- as.factor(sample(c('one','two','three'),50,replace =T))

x.quanti&lt;-df[,sapply(df,is.numeric)]

x.quali&lt;-df[,sapply(df,is.factor)]

tree&lt;- hclustvar(x.quanti,x.quali)

part_hier&lt;-cutreevar(tree,6)

part_hier$var
</code></pre>
"
"0.060084176812611","0.0662266178532522","203185","<p>I have used HDDC (high dimensional data clustering) from the HDClassify package in R where I have around 400+ variables and 350 obs and have obtained 6 clusters. I need to identify the most distinguishing features in each cluster. I have tried comparing the within cluster and in between cluster variance but due to large number of variables, it gets confusing. Is there an easier way to do that , say using Scree plots or anything else?</p>
"
"0.216636580341913","0.23878346647046","204760","<p>I would like to know how I can use clustering methods in R (in this case, Kmeans) if I have an ""unkind"" input matrix (I get this error log: </p>

<blockquote>
  <p>The TSS matrix is indefinite. There must be too many missing values. The index cannot be calculated.)</p>
</blockquote>

<p>I could see that I might get this error if my matrix produces negative eigenvalues (like, here: <a href=""http://stackoverflow.com/questions/20669596/nbclust-package-error"">http://stackoverflow.com/questions/20669596/nbclust-package-error</a>), but what I'm missing is the ""next step"" part. I could see a suggestion was to ""go back to the Data"", but what should I do then? Is there any transformation or something that might help? (I'm pretty new to R and clustering in general...)</p>

<p>The Data I'm using are the result of a survey (which I briefly transformed and scaled via the <code>scale</code> function in R) so I was wondering if there were some algorithms or methods I could use in order to go on with my analysis (from literature I couldn't find great help). Or, if you think this is unfixable or simply non the best solution, do you have any other suggestion for clustering my data? What I'm willing to do is to identify some clusters of possible users/customers of some services, depending on their usual habits (e.g.: if they use many social networks they will be more likely to use chat/whatsapp/app to ask for bank account information - I have both the information of their social network usage and their ways of communicating with a ""bank assistant"").</p>

<p>The Dataset consists of 994 rows and 103 columns. Don't know if it may help, but the code is simply this:</p>

<pre><code>Data2&lt;- read.csv(...)
bDataScale &lt;- scale(Data2)
nc &lt;- NbClust(bDataScale, min.nc=2, max.nc=993, method=""kmeans"")
</code></pre>

<p>And I get:</p>

<blockquote>
  <p>Error in NbClust(bDataScale, min.nc = 2, max.nc = 993, method = ""kmeans"") : 
    The TSS matrix is indefinite. There must be too many missing values. The index cannot be calculated.</p>
</blockquote>

<p>Thank you in advance for your help or any corrections,</p>

<p>Julia</p>

<p>P.S.: as it would be logical to expect, I get the same error also with the unscaled matrix.</p>
"
"0.0849718577324175","0.0936585811581694","205604","<p>I'm running a regression in R's <code>plm</code> package similar to this post <a href=""http://stackoverflow.com/questions/33155638/clustered-standard-errors-in-r-using-plm-with-fixed-effects"">Clustered standard errors in R using plm (with fixed effects)</a>. I.e. panel data with fixed effects and the within-model from <code>plm</code>.</p>

<p>My Question is the following: I'm trying to figure out how to cluster my standard errors according to a different variable than the variable called state from the dataset <code>Cigar</code>, which is seemingly automatically used by the <code>cluster = 'group'</code> option in <code>vcovHC</code>. Specifically, if I e.g. have a variable called <code>id</code>, how can I tell <code>vcovHC</code> to use it as my cluster?</p>

<p>A very related question is the process of how <code>vcocHC</code> is selecting the variable for clustering, is it always just the first column in the dataset?</p>
"
"0.158967789576202","0.175219161012616","206867","<p>I want to do cluster analysis of a product monthly sales during 5 years in 30 stores (my data are time series). I want to cluster the stores according to its seasonality.
This is an example of my data:</p>

<blockquote>
  <p>Month    Year   Shop1   Shop2   Shop3  ...</p>
  
  <p>12       2008   3000    5000     700 ...</p>
  
  <p>1        2009   2000    4000     500 ...</p>
  
  <p>2        2009   6000    5000     300 ...</p>
  
  <p>3        2009   7000    7000     600 ...</p>
  
  <p>4        2009   5000    4000     900 ...</p>
  
  <p>5        2009    5000    8000     1000 ...
  ...</p>
</blockquote>

<p>I have read several questions about this topic but I still do not understand the procedure or how to deal with this problem.</p>

<ol>
<li><p>I have found the package TSclust and I am considering using the dissimilarity index CORT. It covers both conventional measures for the proximity on observations and temporal correlation for the behavior proximity estimation. Do you think that is a good approach to use this measure?</p></li>
<li><p>I have also found the following procedure in: (<a href=""http://stats.stackexchange.com/questions/9475/time-series-clustering/19042#19042"">Time series clustering</a>), that consists in:</p></li>
</ol>

<p>Step 1</p>

<p>Perform a fast Fourier transform on the time series data. This decomposes your time series data into mean and frequency components and allows you to use variables for clustering that do not show heavy autocorrelation like many raw time series.</p>

<p>Step 2</p>

<p>If time series is real-valued, discard the second half of the fast Fourier transform elements because they are redundant.</p>

<p>Step 3</p>

<p>Separate the real and imaginary parts of each fast Fourier transform element.</p>

<p>Step 4</p>

<p>Perform model-based clustering on the real and imaginary parts of each frequency element.</p>

<p>Step 5</p>

<p>Plot the percentiles of the time series by cluster to examine their shape.</p>

<p>Have you ever done something like that? If so, could you provide an example code to carry out these steps?
Or do you know other steps?</p>

<ol start=""3"">
<li>I have also read the paper of Kumar, Patel and Woo: ""Clustering seasonality patterns in the presence of errors"", but i do not know how to reproduce their procedure in R.</li>
</ol>

<p>Any help would be helpful!</p>
"
"0.158967789576202","0.175219161012616","207404","<p>First of all, I know that this question has been addressed a certain number of times, but I didn't find an answer concerning the <strong>clustering of variables</strong>, instead of observations.</p>

<p>Concretely, I am using the function <code>varclus</code> from the package <code>Hmisc</code> to perform variables clustering.</p>

<p>As an example, I want to perform a cluster analysis on the variables of the dataset <code>ionosphere</code> (available in the package <code>dprep</code>).</p>

<p>My code is as follow :</p>

<pre><code>&gt; library(Hmisc) 
&gt; library(dprep) 
&gt; data(ionosphere) 
&gt; iono_min_l_col &lt;- ionosphere[-length(ionosphere)] 
&gt; iono_mx_min_l_col &lt;- data.matrix(iono_min_l_col) 
&gt; iono_clus &lt;- varclus(iono_mx_min_l_col)
&gt; plot(iono_clus)
</code></pre>

<p>When using <code>plot()</code> I get the following dendrogram :</p>

<p><a href=""http://i.stack.imgur.com/LSDiI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LSDiI.png"" alt=""enter image description here""></a></p>

<p>However, I don't know in how many clusters I should group my variables.
I can used the following code to know in which cluster are my variables (e.g. V1 Cluster 1, V2 Cluster 3, etc...), but I don't know <strong>how to get the optimal number of clusters</strong> ? (i.e. <strong><em>k</em></strong> in the following code)</p>

<pre><code>&gt; groups &lt;- cutree(varclus(iono_mx_min_l_col)$hclust, k= ***???***)
</code></pre>

<p>Does someone know how to get this optimal number, <strong><em>k</em></strong> ?</p>

<p>Thanks a lot !</p>
"
"0.0849718577324175","0.0936585811581694","209364","<p>I'm working on segmentation/clustering and trying to use Gaussian Mixture Modelling for Model-Based Clustering. I'm using the R package Mclust in order to come up with the best fit for my data.</p>

<p>All data is transformed to a uniform distribution with mean zero, standard deviation one (I know, not Gaussian) and the variables included are chosen based on earlier attempts using k-means, where the given variables seemed to be discriminating. Of course, k-means comes with some drawbacks (lack of statistical foundation, no control of cross-correlation etc,), and that's the reason I want to use Model-Based Clustering (or latent class analysis, with the package poLCA).</p>

<p>When using mclustBIC, many of the possible BICs are actually NA. I tried to reduce the dimension of the data, but this didn't improve the output. For example the VEV is only calculated for nr clusters 1:3, while it looks like it could improve for more clusters (see plot below).</p>

<p>Someone who experienced similar problems? And can someone help me into the right direction for finding the best model, using mclust? I would like to calculate other BICs with a higher number of clusters.</p>

<p>Help would be appreciated!</p>

<p><a href=""http://i.stack.imgur.com/YboGv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YboGv.png"" alt=""enter image description here""></a></p>
"
"0.180252530437833","0.198679853559757","211102","<p>I have two data sets which contains information about subsystems in a bacterial metabolic model<br>
DataSet1: Behavior data of the subsystems<br>
DataSet2: Structural data of the same subsystems<br>
Then perform hierarchical clustering on the two data sets and obtain two dendrograms. My goal is to find how similar these dendrograms are.
I used FM index as suggested in <a href=""https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html"" rel=""nofollow"">this article</a> as it gives some sort of confidence interval on the final answer.</p>

<p>This is the code I used:</p>

<pre><code>#Hierarchical clustering for dataset 1
dInit &lt;- dist(initDataSource)
hcInit = hclust(dInit)

#Hierarchical clustering for dataset 2
dFinal &lt;- dist(finalDataSource)
hcFinal = hclust(dFinal)

#calculating the FM index
FM_index(cutree(hcInit, k=2), cutree(hcFinal , k=3))
</code></pre>

<p>Output:</p>

<pre><code>[1] 0.7462025
attr(,""E_FM"")
[1] 0.6253888
attr(,""V_FM"")
[1] 0.007626263
</code></pre>

<p>I need some help interpreting the output of FM index function. As stated in <a href=""https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html"" rel=""nofollow"">this article</a> there are three values in the output; FM index, expected FM index and the variance.  </p>

<p><strong>Question:</strong>
Is the variance value some sort of a p-value in which we could accept the or reject the H0 (null hypothesis -  the two trees are not similar). For example in the below scenario as variance is 0.007 (which is less than 0.01) we could reject H0 and the two dendrograms are similar by a value of 0.7462. </p>

<p>In a case where we get a very high FM index but a fairly high variance does that mean we are unable to reject the H0 and hence the dendrograms are not similar? </p>

<pre><code>&gt; FM_index(cutree(hcInit, k=3), cutree(hcFinal, k=3))
[1] 0.8461141
attr(,""E_FM"")
[1] 0.5515411
attr(,""V_FM"")
[1] 0.01347924
</code></pre>
"
"0.122646312338385","0.162221421130763","212293","<p>I have seen other users ask about recreating SAS's CCC output in other programs. This question, <a href=""http://stats.stackexchange.com/questions/29114/cubic-clustering-criterion-in-r"">Cubic clustering criterion in R</a>, has an answer that says to use <code>NbClust</code> to calculate, but that function does not handle large datasets well. It makes a call to <code>dist</code> that must allocate a 50 gig object. I have tried replacing the function with <code>cluster::daisy</code>, and <code>proxy::dist</code> from this SO question with the same memory problems.</p>

<p>Avoiding the <code>dist</code> call altogether may be the best option. I am looking to other options to recreate it. In this question <a href=""http://stats.stackexchange.com/questions/9016/how-to-define-number-of-clusters-in-k-means-clustering/9019#9019"">How to define number of clusters in K-means clustering?</a>, a user goes through the math provided by SAS. But I do not have the stats chops to translate that into R code. </p>

<p>Keeping it simple, I have <code>kmeans</code> output that provides total sum of squares (tot.ss), within.ss, between.ss, and I also calculated the $R^2$. </p>

<pre><code>kmeans(x = mydata, centers = 23, iter.max = ITER)
Within cluster sum of squares by cluster:
 [1]  91248.77  72122.06  78680.32  90402.25  86341.35 153533.51  73988.63  64903.32
 [9]  38334.98  84125.14  92366.93  74721.24 110313.76  96859.55  84516.37  56068.08
[17]  76201.69  86194.35  59526.00  53709.75  72503.21  50767.36  80531.94
 (between_SS / total_SS =  36.5 %)

Available components:

[1] ""cluster""      ""centers""      ""totss""        ""withinss""     ""tot.withinss"" ""betweenss""   
[7] ""size""         ""iter""         ""ifault""
</code></pre>

<p><strong><em>Can I calculate the CCC using these measures?</em></strong></p>

<hr>

<p>The second question has a long description from the SAS pdf. But I saw a <a href=""http://www.palgrave-journals.com/jibs/journal/v37/n4/fig_tab/8400206t2.html"" rel=""nofollow"">simplified equation here</a>.</p>

<p><a href=""http://i.stack.imgur.com/wKiHu.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wKiHu.gif"" alt=""enter image description here""></a> </p>

<p>where $E(R^2)$ is the expected $R^2$, and $R^2$ is the observed $R^2$, and $K$ is the variance-stabilizing transformation.</p>

<p>*Can this equation be completed by R's <code>kmeans</code> output and a calculated $R^2$</p>

<p><strong><em>Edit</em></strong></p>

<p>One reason why I am focusing on <code>kmeans</code> is that SAS users utilize <code>PROC FASTCLUS</code> when running large datasets. It is equivalent to R's kmeans function. The package <code>NbClust</code> calculates the CCC that I'm looking for, but it does it on the full data with euclidean distance, which is impossible for most computers. That is equivalent to SAS's <code>PROC CLUSTER</code>.</p>
"
"0.190002850064127","0.209426954145848","215961","<p>I have a set of distributions corresponding to predictions for how each of hundreds of players will perform.  I am looking to identify the distinct distributions of players.  In other words, I'm looking to identify the <a href=""https://stats.stackexchange.com/questions/57921/how-to-prove-the-number-of-distinct-distributions-in-a-group-of-distributions"">distinct distributions in a group of distributions</a>.</p>

<p>I know <code>Mclust()</code> can perform clustering on a vector, e.g.:</p>

<pre><code>library(""mclust"")

mydata &lt;- c(1,1,2,2,3,3,5,7,8,9,10)

summary(Mclust(mydata), parameters=TRUE)
Mclust(mydata)$classification
</code></pre>

<p>However, my data are a series of vectors (i.e., distributions)---one vector for each player, e.g.:</p>

<pre><code>set.seed(12345)
playerA &lt;- rnorm(10, mean=1, sd=.1)
playerB &lt;- rnorm(100, mean=1, sd=1)
playerC &lt;- rnorm(10, mean=2, sd=1)
playerD &lt;- rnorm(5, mean=2, sd=2)
playerE &lt;- rnorm(2, mean=3, sd=1)
playerF &lt;- rnorm(20, mean=5, sd=1)
playerG &lt;- rnorm(100, mean=7, sd=.5)
playerH &lt;- rnorm(10, mean=8, sd=2)
playerI &lt;- rnorm(5, mean=9, sd=1)
playerJ &lt;- rnorm(10, mean=10, sd=.5)
</code></pre>

<p>How can I perform clustering to identify the distinct clusters of players based on their distributions, focusing on differences in their means, rather than their variances.  I don't want to just cluster the mean values, though, because I want to take into account the variances to know whether their means are in the same or in a different cluster (e.g., high variability in two players' distributions may indicate that two players with different means are in the same cluster).  Ideally, I'd like two players with the same mean and different variability distributions to be in the same cluster.  Is there a way to do this using the <code>mclust</code> or another package in R?  I've considered doing pairwise t-tests, but this seems that it would be heavily dependent on the sample size in each distribution (which I'd rather it not be <em>too</em> dependent on sample size, if possible).  I've also considered comparisons based on effect size (Cohen's d).  I'm not sure what other options there are (e.g., Tukey's HSD, hierarchical clustering, etc.)</p>
"
"0.0849718577324175","0.0936585811581694","216046","<p>I am trying to find a clustering solution with the help of <code>flexclust</code> package in R. The following code has been adapted from the vignette for the <code>flexclust</code> package:</p>

<pre><code>library(flexclust)
library(ISLR)

Auto &lt;- read.table(Auto)
AutoMinus &lt;- Auto[ -c(8:9)]
AutoMinus.mat &lt;- as.matrix(AutoMinus)

# Setting the parameters

fc_cont &lt;- new(""flexclustControl"")  
fc_cont@tolerance &lt;- 0.01   
fc_cont@iter.max &lt;- 25
fc_cont@verbose &lt;- 1
fc_family &lt;- ""kmeans""             

seed1 &lt;- 12345
fc_seed &lt;- seed1
num_clusters &lt;- 3
set.seed(fc_seed)

AutoMinus.cl &lt;- kcca(AutoMinus.mat, k = num_clusters, save.data = TRUE, control = fc_cont, family = kccaFamily(fc_family))
summary(AutoMinus.cl)

cluster info:
  size  av_dist max_dist separation
1  122 263.0698 525.3528   480.3347
2  180 217.1523 610.9503   478.7658
3   90 290.9777 905.5731   551.2422
</code></pre>

<p>Every time I change the seed, the output changes. I am evaluating different outputs based on lowest <code>av_dist</code>, lowest <code>max_dist</code> and minimum <code>separation</code>. My understanding is <code>separation</code> is within a given cluster. I tried to find the definition of <code>separation</code> in the documentation, but couldn't find it. My questions are:</p>

<ol>
<li>How do I set the seed that will give me the best/optimal(?) solution?</li>
<li>Are there any general good practices for initializing a seed value?</li>
<li>Is my understanding of <code>separation</code> correct?</li>
</ol>

<p>Thank you!</p>
"
"0.104068846970394","0.114707866935281","218301","<p>I have a matrix of pairwise Gower dissimilarities (i.e. a non-Euclidean metric) among 48 objects derived from observations of 54 variables.</p>

<p>I want to use fuzzy clustering to obtain a matrix of group membership probabilities for these objects.</p>

<p>I successfully applied function <code>fanny()</code> from package {cluster} with k (number of groups) = 4.</p>

<p>However, for each sample, the resulting group membership probabilities for groups 1, 2 and 4 are identical. That is, they differ <em>between samples</em>, but <em>within a sample</em> they are identical, and differ from the membership probability for group 3.</p>

<p>I tested the function with k of 5, 6 and 7 and obtained the same result. For each k, the group membership probabilities were identical (within an object), except for group 3.</p>

<p>Does anyone know why this might be happening? I've read up on the fanny() algorithm a little but I'm out of my depth.</p>

<p>Reproducible code (NB: actual data follow due to size):</p>

<pre><code>install.packages(""cluster"")
library(""cluster"")
WRS_fanny &lt;- fanny(WRS_abiotic_gower, k = 4, memb.exp = 1.6)
boxplot(WRS_fanny$membership) # Note identical probabilities except group 3
</code></pre>

<p>Used dump() to generate this expression to recreate my data (warning: 294 lines):</p>

<pre><code>WRS_abiotic_gower &lt;-
structure(c(0.146778126529408, 0.233395130892759, 0.230707834408444, 
0.107759542541761, 0.138550796015413, 0.250065889538291, 0.296457361775224, 
0.304125211829354, 0.249782393842249, 0.282431780327495, 0.25970664977036, 
0.251596311648883, 0.224076607905674, 0.251036300151136, 0.196484552504834, 
0.271536343873702, 0.265568257890046, 0.301386804224653, 0.333063701373033, 
0.243600972251144, 0.270222184812288, 0.202047498980117, 0.25607063775203, 
0.28432667481917, 0.24092419965811, 0.275398059343259, 0.353270223771568, 
0.410364714292939, 0.313001378801364, 0.31925605598341, 0.357701120460457, 
0.374474795997226, 0.189102942457199, 0.202055462100059, 0.247583708657342, 
0.30145619413158, 0.296669857875146, 0.339864474596941, 0.229211008702165, 
0.352892440280298, 0.419167648505487, 0.375568557956858, 0.408480716527057, 
0.357157487108125, 0.32104738586095, 0.340665462218034, 0.41112214935789, 
0.23830604032674, 0.220478976300685, 0.144406224105715, 0.131515885009171, 
0.216908812847801, 0.243564070814677, 0.303148174678537, 0.259847622410418, 
0.247305687153991, 0.247843057599577, 0.26631554306528, 0.199890462938375, 
0.28044167137188, 0.236573345659676, 0.264633258654337, 0.250360278612784, 
0.282905316954283, 0.28148360026523, 0.172675632607707, 0.227157538886476, 
0.236004956068931, 0.275979625457648, 0.277843987366973, 0.262417869004574, 
0.281232496061673, 0.329892724118953, 0.394824895894531, 0.30265363625644, 
0.30621966032146, 0.34479178515701, 0.374683684901749, 0.21424820401553, 
0.195572916452556, 0.241393648501288, 0.316572600921197, 0.287133512655544, 
0.328215112402773, 0.193182214935908, 0.310935856747777, 0.373166961778132, 
0.332783318074841, 0.372234839389727, 0.330623742512386, 0.292078433204333, 
0.319545503079129, 0.361321516105163, 0.113028782648282, 0.206941169638988, 
0.20512668072602, 0.24722346523851, 0.308687244396935, 0.204828946418389, 
0.178919076974325, 0.183060785216104, 0.141776960257189, 0.134674581851856, 
0.25068097397968, 0.123958293735287, 0.141125483626521, 0.207978432341651, 
0.164949716625781, 0.190523417026307, 0.309000477229615, 0.277339046266308, 
0.313375054970243, 0.146793857852403, 0.137385394610465, 0.167578392653176, 
0.16804367314817, 0.171809114878496, 0.381176414723976, 0.457052466454844, 
0.282197699921021, 0.280739699172475, 0.302250296861788, 0.304432773653582, 
0.15957945901212, 0.235669994183549, 0.178160953420343, 0.282371461962546, 
0.257100104835312, 0.482706722426562, 0.369944192447982, 0.309346584211358, 
0.397210372391609, 0.380583112959886, 0.407248184382802, 0.346695417169584, 
0.307512606916341, 0.269834685656257, 0.409392428767859, 0.217511509030952, 
0.197481259895392, 0.275939181123016, 0.341312211621435, 0.228966719294035, 
0.188816895891077, 0.212202206485866, 0.194008885613182, 0.166989986763656, 
0.302101187053358, 0.162535754002973, 0.189263438443349, 0.23307417010102, 
0.199298488543463, 0.229916212413065, 0.31125875417897, 0.239297777451497, 
0.305776102461332, 0.173662348649529, 0.190044248152056, 0.206426618003735, 
0.227187931768048, 0.214824072433246, 0.4073186103118, 0.462804478914285, 
0.323506990767481, 0.317824328727341, 0.335245136426698, 0.327528602013808, 
0.185454797734023, 0.208467311764833, 0.135322000283485, 0.288836940944495, 
0.230306186711877, 0.468573333942388, 0.33313401147229, 0.321113074211735, 
0.391514664657562, 0.389710934558845, 0.400420981243808, 0.354101457148739, 
0.320629082713082, 0.286100626725499, 0.421671705461507, 0.091850113801114, 
0.246467223254679, 0.293547268483122, 0.297659239034206, 0.25115255052786, 
0.262514071750454, 0.238133038325391, 0.212967201979357, 0.17308253395046, 
0.2183920945381, 0.176101353198788, 0.261139515088514, 0.25711901874235, 
0.272641088130901, 0.330933912386848, 0.250239547033252, 0.236589210839988, 
0.172880533811394, 0.250922420164451, 0.268025493833902, 0.23575916232682, 
0.267647109568082, 0.373655060045483, 0.433323699677838, 0.31391657516819, 
0.31852460096763, 0.351258764865077, 0.367159819051078, 0.16890813868034, 
0.185782013611323, 0.226257665322096, 0.24834469424724, 0.272259607740838, 
0.354585293042834, 0.229701146851508, 0.362102176288281, 0.400717876478831, 
0.36797070590943, 0.409120131929636, 0.34498319198954, 0.313386283542958, 
0.331016784762028, 0.384254565970275, 0.22441721238447, 0.290598912970728, 
0.298643689422108, 0.250361457068466, 0.244928708085188, 0.237713058207576, 
0.222774529861816, 0.210265153710359, 0.235732385132754, 0.20156761561874, 
0.262015784293767, 0.259883660151181, 0.288278458785541, 0.333285662718476, 
0.232716863284767, 0.254484337442487, 0.187205171211865, 0.243100325138505, 
0.267048020813995, 0.242204808310273, 0.261920262726918, 0.380811106291807, 
0.446215322818477, 0.316835287136048, 0.321422073497405, 0.348976941617111, 
0.382762337202703, 0.182491613841568, 0.215977568407392, 0.226450258451914, 
0.283179718136393, 0.274849250694097, 0.369155318338422, 0.220594923901695, 
0.355713153122709, 0.412492946083189, 0.382970482523399, 0.431482115624372, 
0.34255354874582, 0.347795938665851, 0.34955910382006, 0.420245136110264, 
0.157812677685306, 0.272010868942531, 0.229427228507228, 0.145577425709495, 
0.263408013897356, 0.321273834678714, 0.253868813151803, 0.303315771029787, 
0.250638531578023, 0.202686048164595, 0.23542311767865, 0.262524337712699, 
0.213762545265516, 0.198592502804593, 0.289755608760079, 0.291055349247771, 
0.21471221044029, 0.233375528166048, 0.214477494584816, 0.227000414408994, 
0.253378560228127, 0.327233596552946, 0.256069951388544, 0.241731882033173, 
0.239086155905498, 0.301355179254736, 0.268623586092357, 0.216004437464011, 
0.278442814851357, 0.351355765511319, 0.306053836025685, 0.385664345928335, 
0.325740726127523, 0.224406635623224, 0.272261471513539, 0.269731865914167, 
0.301560017458275, 0.225721846096926, 0.229981204614567, 0.242518355133113, 
0.328463376740603, 0.329111361926846, 0.264116154396599, 0.217010031466845, 
0.313943303264908, 0.363879592011647, 0.257108399675198, 0.358515803949734, 
0.321567175057474, 0.296784046059782, 0.301131134388745, 0.331842299269796, 
0.253436530987525, 0.259557088485383, 0.301283759065815, 0.343075324682922, 
0.283687903142085, 0.283592914904744, 0.264059283720674, 0.287527855889611, 
0.188324610310514, 0.230742624643993, 0.228533751670285, 0.193858413416879, 
0.196462593078236, 0.26351603992366, 0.320788863971364, 0.24636937131769, 
0.318763516486424, 0.426609873329301, 0.366219134961131, 0.378499967117754, 
0.337661758976989, 0.271583947417078, 0.314249960298854, 0.270201122918241, 
0.246946815781063, 0.215102704787387, 0.236134173342779, 0.286232978295626, 
0.350696874593699, 0.164627981394906, 0.205527499202434, 0.270434850920206, 
0.243134003521468, 0.351050952165932, 0.219183969305488, 0.242246176734031, 
0.172067162427782, 0.128130370968335, 0.134122968993382, 0.257281689657044, 
0.299403006982152, 0.422642508788569, 0.228408163742806, 0.11957268044525, 
0.159438669856483, 0.14868234714726, 0.136918222749012, 0.286755138375814, 
0.354634021682536, 0.181731358904613, 0.193212405876045, 0.224667151942299, 
0.203734363544203, 0.243482257894911, 0.289366500456441, 0.252436960541827, 
0.405666704589028, 0.373065503109924, 0.524026234338112, 0.435325641487625, 
0.2554099549592, 0.333929718431462, 0.296689595118324, 0.317661622659771, 
0.259343552195757, 0.220649936386088, 0.163406146367423, 0.385366843416753, 
0.17361870009857, 0.202311917654341, 0.227649249888414, 0.272307292144977, 
0.209258820602416, 0.19319876548376, 0.176481382795848, 0.155899766048858, 
0.20344820709624, 0.264821880206708, 0.261204867940355, 0.327272602852553, 
0.189095535111204, 0.133280535811196, 0.113962382478816, 0.133224369074271, 
0.140065242738296, 0.324893451169475, 0.363116475102291, 0.237103967908121, 
0.242041052773962, 0.270256227204134, 0.287338997227553, 0.173155053909376, 
0.189424483944132, 0.187175303832514, 0.348464842038111, 0.288446217937334, 
0.470765306868106, 0.343131708273399, 0.252938015143712, 0.336888277495862, 
0.301191263319341, 0.356500481509341, 0.301312706960525, 0.267001426016706, 
0.237755741422434, 0.373181832858731, 0.210970882604491, 0.251097175595452, 
0.263098712824128, 0.239269004043719, 0.227818986615428, 0.204715476224745, 
0.207743502877001, 0.217031686744224, 0.250740570083614, 0.228549792422889, 
0.32207055314749, 0.214569147096824, 0.168053926099824, 0.181896968150084, 
0.17623993474032, 0.188910784920792, 0.329236353931487, 0.406757003827438, 
0.21912863831964, 0.222918637259912, 0.233057683009712, 0.279883139925161, 
0.254158158514965, 0.20158263820538, 0.240723178073065, 0.357575151419053, 
0.315084566107509, 0.420672711597265, 0.367274077713619, 0.249934355335581, 
0.318059702631574, 0.31801009061305, 0.307702024506444, 0.272796141745462, 
0.248783767482011, 0.224074353894395, 0.36569822014514, 0.187028191009587, 
0.241049094175669, 0.179339223070881, 0.156157306332836, 0.227513629426693, 
0.202623928640432, 0.246026699193367, 0.33156658615291, 0.315376540691078, 
0.289484137778374, 0.184409694862884, 0.21876550228219, 0.191376542617272, 
0.225070995275441, 0.210475769133431, 0.406320939903412, 0.470184911435262, 
0.283288686913745, 0.310862038618383, 0.326344244418466, 0.354367208429555, 
0.145546897876123, 0.236312084449821, 0.228164131483865, 0.299496818623249, 
0.262731741465948, 0.481980331569159, 0.365549756268789, 0.304206259846763, 
0.404226312619428, 0.378666518283215, 0.427829473290829, 0.410076196767098, 
0.354844586875853, 0.322995610040716, 0.442244062657121, 0.264970270646247, 
0.108746238733874, 0.193352911535009, 0.266719534733286, 0.227013610318163, 
0.238449830517912, 0.347321221639697, 0.323042902522987, 0.309345473650974, 
0.152174920727296, 0.213300152179139, 0.214985014168209, 0.235666919359841, 
0.252242068781554, 0.42709913553219, 0.485189051211046, 0.293088179216135, 
0.329679671628233, 0.33035798397786, 0.346527393314769, 0.179522572086687, 
0.278345689579152, 0.232251261676667, 0.287551912653317, 0.278265167614771, 
0.484670259785051, 0.377718756380607, 0.359121022926639, 0.445377414543919, 
0.401184662139802, 0.451462061652599, 0.400960453366808, 0.357848562271756, 
0.32669976446978, 0.453658939812252, 0.240146917990639, 0.230300655485122, 
0.311042661085334, 0.317612855832712, 0.337564267124495, 0.349956630228225, 
0.262974415301128, 0.242242668807266, 0.253380207155479, 0.289677826481165, 
0.293660554070656, 0.293778024707369, 0.316830788871491, 0.350345490654708, 
0.395195391031732, 0.33513375069398, 0.337179146242041, 0.371234921705163, 
0.39695509160543, 0.242891611224244, 0.250345225015281, 0.335701615323364, 
0.312734019064895, 0.325318419658978, 0.337878664200963, 0.253340263691725, 
0.374161000604498, 0.42535575946078, 0.357273207425347, 0.390814126325831, 
0.37145277419384, 0.331269154759241, 0.35876947913107, 0.352172345260752, 
0.142765226117177, 0.235679681653265, 0.184818072498785, 0.191253138910713, 
0.348695224683716, 0.310876918777759, 0.356345526377818, 0.119856934587125, 
0.18558608732922, 0.218834189019569, 0.222467207064484, 0.214665638336601, 
0.409416658003954, 0.486474197840857, 0.297515550101008, 0.295394752220839, 
0.322174630895094, 0.325738052143967, 0.132596245884175, 0.246268939070485, 
0.237445084809875, 0.296085703183638, 0.30967198389911, 0.514235890211653, 
0.410886481296546, 0.356238394380928, 0.444278663072277, 0.413799296345893, 
0.423845525648207, 0.385961408460294, 0.331592257597649, 0.288799203168772, 
0.442915990670296, 0.197022259697264, 0.175507567684546, 0.194255251918258, 
0.33082685023071, 0.309916538911981, 0.31654082290413, 0.108210668457586, 
0.190395423695379, 0.19168132680145, 0.208726249609115, 0.189951871401416, 
0.380558729308507, 0.450713427014334, 0.29221794299639, 0.289788592951392, 
0.318621931141074, 0.342289641611819, 0.111392920848469, 0.238016295941358, 
0.233024569363059, 0.283486747172187, 0.296984641557404, 0.501886736277605, 
0.354691839582922, 0.340610686032757, 0.422424356332051, 0.384338929876594, 
0.40094086257798, 0.379526405421168, 0.312309452815408, 0.288243747328821, 
0.442756689368451, 0.0978020742858896, 0.14238454435033, 0.224074878211512, 
0.272789288372412, 0.336958268324688, 0.230384388811536, 0.153772250555725, 
0.131424564857863, 0.135138107161081, 0.133890774320702, 0.272263168743922, 
0.334611561341041, 0.201012647896807, 0.201584220387737, 0.208839157389968, 
0.256579613931989, 0.195090930348769, 0.239667311414397, 0.229812436589804, 
0.325585931893025, 0.283778980685324, 0.478526612005615, 0.356639373275897, 
0.241909246023086, 0.305460399238216, 0.27867526153966, 0.322412981417394, 
0.277171351783218, 0.229900157930983, 0.195715920825032, 0.39572261842839, 
0.0885419178364805, 0.260829077668881, 0.285604227212454, 0.358309944800871, 
0.191297070527283, 0.126235720835498, 0.109249713673671, 0.14254483589829, 
0.114098495759652, 0.30083929190514, 0.37020704542986, 0.1958702964902, 
0.197870970618193, 0.236326226170556, 0.252004892063645, 0.172915994311976, 
0.243168478593422, 0.215401946288142, 0.34974266738716, 0.299793278166279, 
0.495198823266587, 0.367763979943646, 0.242791995631133, 0.337673441490117, 
0.307139045113223, 0.337144126944256, 0.307001967739705, 0.247488011208742, 
0.193434061388901, 0.428281034806268, 0.284047884441708, 0.297179320626955, 
0.398472912489279, 0.189422027088779, 0.139174701703958, 0.13546793385265, 
0.161371430634921, 0.168372359428332, 0.311782400400275, 0.399530125695683, 
0.204724384598228, 0.201801211542734, 0.228630624490025, 0.244227565066874, 
0.231236249512786, 0.282283778304694, 0.270960643953082, 0.339857569718471, 
0.326823797526486, 0.50790279274637, 0.416875008383526, 0.282152460994984, 
0.357858660816522, 0.333641345463762, 0.329844433417204, 0.303489492239208, 
0.248763158823102, 0.18972472338955, 0.421183075535043, 0.238876684527487, 
0.345436792453969, 0.336061495089022, 0.263723105832666, 0.269158312858497, 
0.257569542737216, 0.254101022613423, 0.232340609914943, 0.288048749871818, 
0.23893258972811, 0.235307782197714, 0.245368590919873, 0.218291406006654, 
0.316141295974322, 0.24955937510941, 0.275398481408491, 0.397779405844502, 
0.33235739043674, 0.474488798783166, 0.379555493234099, 0.117498927565631, 
0.150698463433954, 0.139639944784483, 0.201302528451723, 0.147727036921414, 
0.110443516933975, 0.207973626843108, 0.2349144537945, 0.261908907223673, 
0.282645509769686, 0.276899906898298, 0.298349708345499, 0.269341140045141, 
0.280406506125865, 0.288913750994849, 0.347891634321888, 0.303388807579801, 
0.281966082920489, 0.30160734823309, 0.312672145150089, 0.289560106847955, 
0.232441075572291, 0.251245870030593, 0.307878521523741, 0.253177898598542, 
0.367223667445015, 0.261958529238038, 0.281362238133638, 0.28801781739124, 
0.270009069548595, 0.299522785484746, 0.257219705028688, 0.221415622921201, 
0.255820765239848, 0.28828981955977, 0.31671582928228, 0.363745505468365, 
0.347939689625386, 0.345267789614021, 0.350823668439948, 0.364495975648929, 
0.366870882161216, 0.385114088795543, 0.394103211364437, 0.395775639313918, 
0.431759745459385, 0.285752359669562, 0.218397737527923, 0.267855815397473, 
0.276016579434273, 0.212495440354826, 0.291102629917947, 0.213970649897841, 
0.366795421768988, 0.346449786640165, 0.342290797217175, 0.415599276580519, 
0.380336211685697, 0.349467695816019, 0.364180183329975, 0.337126810775529, 
0.181804519234005, 0.18062902539593, 0.205007684433831, 0.215420308066645, 
0.374535630704164, 0.448671109202557, 0.274007859625578, 0.289718981226722, 
0.317449365559063, 0.344436310953225, 0.0867015061200141, 0.214204649166454, 
0.215447575226592, 0.264794874484955, 0.282581926460504, 0.484608914272993, 
0.348667454528334, 0.32893804711248, 0.414555319322939, 0.385124997651254, 
0.388063868142764, 0.354930509942296, 0.314210175323553, 0.299205156883234, 
0.421392543584554, 0.100685059696492, 0.0866995692711282, 0.0969095988503732, 
0.303792476166297, 0.382897246418713, 0.200846642991439, 0.207028847006805, 
0.235355971825502, 0.242986796699078, 0.207057482359656, 0.2687412718472, 
0.221962952421413, 0.344331259710073, 0.330688322004791, 0.474942339215751, 
0.385638847415413, 0.251038412436203, 0.331532314029424, 0.3114598183918, 
0.347948011007012, 0.270448613080749, 0.244297789908784, 0.204995495462879, 
0.350844315404954, 0.100911244045508, 0.103185755602834, 0.303917784269364, 
0.362849882087611, 0.202367178640256, 0.223049633935698, 0.249463049219082, 
0.283916535235254, 0.181320446598951, 0.245084575129691, 0.214677803390247, 
0.366103538705351, 0.305876565476593, 0.494825605888924, 0.361478033843841, 
0.248975697592638, 0.337313396652572, 0.301301052404624, 0.341462191338638, 
0.312817106125548, 0.27311011394114, 0.239013977072195, 0.396845918590528, 
0.107924690281289, 0.270050462910416, 0.348802966709437, 0.184161895866205, 
0.19278817419054, 0.208020692062944, 0.242414996189593, 0.220172380542586, 
0.233584142548741, 0.217618000702973, 0.345697450080864, 0.313839355336015, 
0.434547000124388, 0.359186021565848, 0.267208517073225, 0.32330173927609, 
0.30518274090545, 0.34529655389533, 0.262508097267023, 0.227176931450904, 
0.204689406161101, 0.361105343888977, 0.30571330885646, 0.367599747989144, 
0.217475081775334, 0.225772086317862, 0.254256629249112, 0.247586478503354, 
0.202555021235713, 0.238681639695241, 0.197751473539488, 0.37176189816412, 
0.317335702325625, 0.49533353822436, 0.363789246350344, 0.254818852710107, 
0.310725186653894, 0.293853581633784, 0.345640005875408, 0.289048614316403, 
0.248412043950396, 0.214293019014908, 0.387475214055207, 0.127715354802369, 
0.223361776933178, 0.20504999812424, 0.19419269144831, 0.222443662573356, 
0.363848133977628, 0.310299864585179, 0.356418806901375, 0.426643530706364, 
0.384599879799222, 0.456927976196206, 0.3856587938183, 0.235397342626925, 
0.276899592755987, 0.231249267705255, 0.22606279999052, 0.183932484187818, 
0.183332809218275, 0.233138159218437, 0.319631133798301, 0.276221233500258, 
0.249619172891433, 0.234864688890259, 0.2534011126405, 0.422547172125712, 
0.354971954715825, 0.399303016835905, 0.425140805653638, 0.396632912535974, 
0.473365299786863, 0.387309733261674, 0.290139703736607, 0.255261478630608, 
0.192826774657939, 0.196399242991551, 0.221163737535614, 0.245646551500451, 
0.270630500732627, 0.350486952979949, 0.0812114642376115, 0.108500113815981, 
0.152246295653802, 0.277240627186762, 0.276720261356458, 0.31032055938721, 
0.388673098686631, 0.349432716682413, 0.475610259715878, 0.438195382601128, 
0.235959199160122, 0.316260782927578, 0.245911688896529, 0.23743988210527, 
0.218712106892772, 0.178816541166305, 0.178230220796603, 0.379833762104026, 
0.0784463617489709, 0.129997140263035, 0.291085148419097, 0.273257526667692, 
0.306557221428694, 0.402153280417114, 0.364282996217115, 0.463352273325088, 
0.431063703093528, 0.222304690496031, 0.291674227758824, 0.241221392372485, 
0.210744722052842, 0.194787521424541, 0.167160218115454, 0.150755475480528, 
0.37885166947424, 0.135279579197833, 0.327926450188976, 0.298559515112429, 
0.318417810243592, 0.409380013643058, 0.364524911114103, 0.488857362760066, 
0.466801626528242, 0.231271124782347, 0.290704260349066, 0.257441555634786, 
0.219539177105795, 0.185785463529339, 0.190539666493475, 0.164885095495906, 
0.400661447880847, 0.351670664342995, 0.293106495448539, 0.278927612321467, 
0.41056998548736, 0.352891397933641, 0.518833547693511, 0.491661977916097, 
0.229382920274352, 0.292629764083707, 0.226989823245877, 0.216704505053967, 
0.182937025703239, 0.158253530543942, 0.158729282293997, 0.356682252224064, 
0.189510186934412, 0.200144695957883, 0.267522818117134, 0.247590590638521, 
0.490647848397164, 0.325163873345429, 0.291292316062298, 0.390440113550956, 
0.357936796896738, 0.405263499885725, 0.370252764588065, 0.322692323050225, 
0.306120087223696, 0.421325571167844, 0.167539006000027, 0.269020719809449, 
0.223042339002822, 0.38760404469828, 0.275161689978891, 0.242363068276669, 
0.299763159742022, 0.295721854984686, 0.316816505510713, 0.280958787086803, 
0.242920147198185, 0.27387569756998, 0.346646422960218, 0.296032978704237, 
0.171471904196341, 0.455226711060175, 0.306302086043222, 0.270092447713217, 
0.330576945364237, 0.328073318126048, 0.379232696180287, 0.305191216068878, 
0.287928376535759, 0.281784904626341, 0.400754470522899, 0.183297028965347, 
0.439603470995397, 0.324142275296304, 0.416842097225044, 0.334144320356283, 
0.331280399040641, 0.361642381662566, 0.395220939617569, 0.348409804637902, 
0.33437908671065, 0.413423564393322, 0.413752235847781, 0.309263065288595, 
0.335337798740467, 0.313696283268453, 0.324026564106507, 0.370850321127701, 
0.367711294816184, 0.318257687382319, 0.282250272793762, 0.405957887669014, 
0.289062022286134, 0.500989225264607, 0.469590271510581, 0.465402652530193, 
0.4884729148024, 0.474410320519324, 0.453387206453657, 0.472498468887031, 
0.428171148682882, 0.415945155744945, 0.361501568460874, 0.351686833699556, 
0.425332115878206, 0.420330132379335, 0.3849261129704, 0.416859526057763, 
0.408207585207454, 0.144955775692482, 0.162810593772454, 0.222289443174366, 
0.174772219403487, 0.153368630325598, 0.212286474750986, 0.27740056732249, 
0.107301136884498, 0.159183884060135, 0.189989069644874, 0.172538300775432, 
0.202263114833207, 0.260439883936975, 0.118388242752457, 0.165081626468155, 
0.140322059005816, 0.185185767078914, 0.260065955820617, 0.137793449327493, 
0.147346740832392, 0.191053507374825, 0.322728564748582, 0.105659036172812, 
0.205131400541044, 0.265657065294917, 0.138708922010367, 0.250058789949686, 
0.310423445004488), class = c(""dissimilarity"", ""dist""), Labels = c(""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", 
""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"", ""23"", ""24"", 
""25"", ""26"", ""27"", ""28"", ""29"", ""30"", ""31"", ""32"", ""33"", ""34"", ""35"", 
""36"", ""37"", ""38"", ""39"", ""40"", ""41"", ""42"", ""43"", ""44"", ""45"", ""46"", 
""47"", ""48""), Size = 48L, Metric = ""mixed"", Types = c(""I"", ""I"", 
""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", 
""I"", ""I"", ""I"", ""N"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", 
""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", 
""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I""
))
</code></pre>
"
"0.190002850064127","0.188484258731263","223035","<p>I am an ecology graduate with a decent practical familiarity with statistics in R, but limited experience of approaches such as PCA, and Cluster Analysis. 
I am currently faced with the challenge of trying to apply my skills to an entirely unfamiliar problem:  my dad is writing a book on archaeological finds of blades, has collected data on 176 finds and has tasked me with analysing it.  </p>

<p>The data selected for analysis is structured thus:   </p>

<pre><code> Blade.length     Max.width     Max.thickness     Shape     Broken.back        Type   

 Min.   :165.0   Min.   :20.00   Min.   : 3.500   A   :70   Min.   :0.0000   Cs   :39  
 1st Qu.:220.0   1st Qu.:28.75   1st Qu.: 5.000   B   : 8   1st Qu.:0.0000   Hbs  :15  
 Median :270.0   Median :34.00   Median : 6.000   C   :14   Median :0.0000   Lbs  :17  
 Mean   :311.5   Mean   :35.20   Mean   : 6.464   D   :14   Mean   :0.2686   Ls   :23  
 3rd Qu.:353.0   3rd Qu.:39.00   3rd Qu.: 7.875   E   :30   3rd Qu.:0.5000   Ns   :43  
 Max.   :760.0   Max.   :62.00   Max.   :11.000   F   :12   Max.   :1.0000   Small:35  
 NA's   :9       NA's   :4       NA's   :86       NA's:28   NA's   :1        NA's : 4 
</code></pre>

<p>Shape is a variable of categories pertaining to the shape of the tip of the blades - these categories are in no particular order.  Broken.back is a different way of looking at ""shape"", effectively binary, although some cases are ""in between"" and have been entered as 0.5.   ""Type"" is a supplementary variable referring to what each blade has been identified as, using a pre-existing typology. Part of the exercise is to examine if this pre-existing typology is fit for purpose. </p>

<p>The dataset is, necessarily, incomplete, with NAs in all variables, although blades with lots of missing data have been excluded from the analysis.  Within the sample remaining, the most incomplete column is blade thickness, with 48% NAs.  </p>

<p>So far I have attempted to visualise the data by means of factorial analysis of mixed data, with imputation, using packages MissMDA and FactoMineR.   However I've found myself bewildered by the number of options and what approach is appropriate for the sort of data I have. </p>

<p>More importantly, I am looking to conduct hierarchical cluster analysis of the data to examine the relatedness of different finds and try and statistically define types (<a href=""http://www.r-bloggers.com/hierarchical-clustering-in-r-2/"" rel=""nofollow"">http://www.r-bloggers.com/hierarchical-clustering-in-r-2/</a>), so far using HCLUST, Dist, and vegdist (package: Vegan).   However, I am not clear as to;  </p>

<ul>
<li>How to manage, prepare or transform the types of data I have for this type of analysis.</li>
<li>What dissimilarity index method would be most appropriate in this context.</li>
<li>What type of clustering / linkage method would be most appropriate in this context. </li>
</ul>

<p>Sorry for the long question. As you can see I am quite bewildered and out of my depth.  Thanks in advance. </p>
"
"0.13625810535103","0.150187852296528","224449","<p>I am conducting clustering analysis in which I am using three clustering algorithms <code>K-means</code>, <code>Spectral Clustering</code>, and <code>Hierarchical clustering</code> on 3 datasets in UCI repository. </p>

<p>I have used <code>R</code> packages to conduct clustering analysis and got the results such as Size of clusters, cluster vector, cluster means, Within cluster sum of squares, and grouping of cluster by Class. </p>

<p>Following is an example of my <code>K-means</code> on the Pima Indian diabetes data in UCI repository:</p>

<pre><code>diabetes &lt;- read.csv(url(""http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data""), header = FALSE)

names(diabetes)&lt;- c(""No.ofTimesPregnant"", ""GlucoseConcentration"", ""BloodPressure"", ""TricepSkinThickness"", ""insulin"", ""BMI"", ""PedigreeFunction"", ""Age"", ""Class"") 

set.seed(20)

KmeansCluster &lt;- kmeans(diabetes[, 1:8], 4, nstart = 20, iter.max=10)

pcol &lt;- as.character(diabetes$Class)
pairs(diabetes[1:8], pch = pcol, col = c(""green"", ""red"") KmeansCluster$cluster])
KmeansCluster
table(KmeansCluster$cluster, diabetes$Class)
</code></pre>

<p>I wish to know how I can compare the results of each clustering algorithm? So that I can say that particular clustering algorithm is best for this dataset. More specifically to say, what metric should I choose and how I can get that metric in <code>R</code> (For example, it would be helpful if you could tell me how to get those metric on my above <code>R</code> code for <code>K-means</code>)?.  </p>

<p>As I know the diameter of the cluster and average distance of each cluster is used as a measure to compare clustering algorithm in general. </p>
"
"0.180252530437833","0.176604314275339","224509","<p>I'm conducting a meta-analysis on standardised mean difference scores. Some studies provide multiple effect sizes, thereby violating the assumption of independence. An example is given below (all effect sizes were calculated with regard to a pre-test). In study A, all participants received the same treatment (watching a video), and were tested repeatedly. In study B, there were two different treatment groups (one group watched a video, the other group listened to an audio book), and everyone was tested once. Study C provided only one effect size.</p>

<pre><code>study        treatment          testing_moment         effect_size

A            video              immediately            0.6
A            video              delayed                0.5
B            video              immediately            0.9
B            audio_book         immediately            0.7
C            audio_book         delayed                0.4
</code></pre>

<p>I'm using the <em>metafor</em> package in <em>R</em>, in which you can fit a multilevel model to account for non-independent sampling errors. </p>

<p>What I've done:</p>

<pre><code>rma.mv(effect_size_vector, variance_vector, mods = ~ testing_moment, 
  random = ~ 1 | treatment/study, data = rev)
</code></pre>

<p>Could anyone please have a look whether this approach is correct? I'm especially unsure about whether I've correctly indicated the clustering using slash (/) (this decision was based on <a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">this page</a>), and whether the model as a result indeed takes into account the non-independence of effect sizes. </p>

<p>I'm also wondering whether somehow it should be corrected that the samples in study A are dependent and in study B they are independent. Or is that already accounted for by virtue of the treatment variable being the same for both samples in study A?</p>
"
"0.148700751031731","0.140487871737254","226907","<p>I am new to R and I am trying to complete a project in R. I have two data-sets one containing information on companies and the second containing information on tenders. The company data-sets has columns Company Name, Company industry, location, description while the Tender data-set has tender description, tender industry and location. I want to compare and match the company description to tender description, company industry with tender industry and company location with tender location to find the most appropriate tenders that a company can target. All the columns involve text and I have already used the text mining package to experiment the data sets separately. I also used clustering to group companies in the same industry and svm to automatically put new companies to the groups. But I have no idea on how to relate these two datasets and then match them. I would like to use multiple variables to match companies with tenders.</p>

<p>Category:
Do the company specialties match the contract CPV descriptions.
Do the company SIC code descriptions match the contract CPV descriptions.
Do the company specialties match the contract long descriptions.
Do the company SIC code descriptions match the contract long descriptions.</p>

<p>Category history:
Has the company won a contract with these CPV codes before.
Has the company won a contract in this cluster (descriptions) before.
Has another company in the cluster won a contract with these CPV codes before.
Has another company in the cluster won a contract in this cluster (descriptions) before.</p>

<p>Relationship:
Has the company won a contract from this buyer before.
Has the company won a contract from a similar buyer lately (industry/geography cluster).</p>

<p>In my dataset I have all the columns like company specialities, CPV code descriptions, SIC code descriptions etc that are necessary.
Any ideas and example codes will be appreciated. Thank you very much.  </p>
"
"0.0849718577324175","0.0936585811581694","232269","<p><strong>Clustering sea waves data in R</strong></p>

<p>I have proceed in clustering of storm's energy data using different clustering  methods (kmeans, hclust, agnes, funny) in R (always in 5 clusters) but even if it is easy to choose the best method for my work, I need a computational (and not theoretical) method or a package to compare and evaluate the methods via their results. Do you believe that there is something?</p>

<p>Thanks in advance,</p>
"
