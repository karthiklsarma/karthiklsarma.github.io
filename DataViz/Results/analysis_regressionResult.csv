"V1","V2","V3","V4"
"0.0446990156267674","0.0220540545695615","  5293","<p>My name is Tuhin.
I came up with a couple of questions when I was doing an
analysis in R.</p>

<p>I did a logistic regression analysis in R and tried to check
how good the model fits the data.</p>

<p>But, I got stuck as I could not get the pseudo R square value
for the model which could give me some idea about the variation
explained by the model.</p>

<p>Could you please guide me on how to achieve this value (pseudo
R square for Logistic regression analysis).
It would also be helpful if you could show me a way to get the
Hosmer Lemeshow statistic for the model as well. I found out a
user defined function to do it, but if there is a quicker way
possible, it would be really helpful.</p>

<p>I would be very grateful if you can provide me the answers to
my queries.</p>

<p>Eagerly waiting for your response.</p>

<p>Regards</p>
"
"0.0547448901451359","0.0540211804549215","  6391","<p>I wonder how to reason when selecting samples. I am doing a panel data regression analysis about how the euro membership correlates with the budget deficit in the member countriues.</p>

<p>Using R that is:</p>

<pre><code>BUDGETDEFICIT ~ EURODUMMY + some_controll_variables
</code></pre>

<p>EURODUMMY is the variable of interest. Using panel data I have anual observations for many countriies. My question is how selecting what sample to use effects the regression analysis.</p>

<p>Should I only use countries that has introduced the Euro?</p>

<p>Is adding other European countries or western like countries benefitial as they add more datapoints and/or acts like a reference gropup?</p>

<p>Or is id only bad for the data to add countries that has not introduced the euro as they have no variation in the variable of interest (EURODUMMY)?</p>

<p>Other comments regarding this problem is also welcome.</p>
"
"0.0364965934300906","0.0540211804549215","  6400","<p>I am conducting a mulitple first order regression analysis of genetic data. The vectors of y-values do not all follow a normal distribution, therefore I need to implement a non-parametric regression using ranks.</p>

<p>Is the <code>lm()</code> function in R suitable for this, i.e.,</p>

<pre><code>lin.reg &lt;- lm(Y~X*Z)
</code></pre>

<p>where Y, X and Z are vectors of ordinal categorical variables?</p>

<p>I am interested in the p-value assigned to the coefficient of the interaction term in the first order model. The <code>lm()</code> function obtains this from a t-test, i.e., is the interaction coefficient significantly different from zero.</p>

<p>Is the automatic implementation of a t-test to determine this p-value appropriate when the regression model is carried out on data as described?</p>

<p>Thanks.</p>

<p><strong>EDIT</strong></p>

<p>Sample data for clarity:</p>

<pre><code>Y &lt;- c(4, 1, 2, 3) # A vector of ranks
X &lt;- c(0, 2, 1, 1) # A vector of genotypes (0 = aa, 1 = ab, 2 = bb)
Z &lt;- c(2, 2, 1, 0)
</code></pre>
"
"NaN","NaN","  7101","<p>I am interested in applying Bayesian additive regression trees (BART) for classification analysis of gene expression data. I am relatively new to R (and Bioconductor packages) and I am unable to find some code or vignette that I can use to learn from. I will be thankful if someone can point me in a good direction.</p>
"
"0.0999500374687773","0.0986287303940589","  7527","<p>I'm trying to implement a ""change point"" analysis, or a multiphase regression using <code>nls()</code> in R. </p>

<p><a href=""http://i.stack.imgur.com/27f1S.png"" rel=""nofollow"">Here's some fake data I've made</a>. The formula I want to use to fit the data is:</p>

<p>$y = \beta_0 + \beta_1x + \beta_2\max(0,x-\delta)$</p>

<p>What this is supposed to do is fit the data up to a certain point with a certain intercept and slope ($\beta_0$ and $\beta_1$), then, after a certain x value ($\delta$), augment the slope by $\beta_2$. That's what the whole max thing is about. Before the $\delta$ point, it'll equal 0, and $\beta_2$ will be zeroed out.</p>

<p>So, here's my function to do this:</p>

<pre><code>changePoint &lt;- function(x, b0, slope1, slope2, delta){ 
   b0 + (x*slope1) + (max(0, x-delta) * slope2)
}
</code></pre>

<p>And I try to fit the model this way</p>

<pre><code>nls(y ~ changePoint(x, b0, slope1, slope2, delta), 
    data = data, 
    start = c(b0 = 50, slope1 = 0, slope2 = 2, delta = 48))
</code></pre>

<p>I chose those starting parameters, because I <em>know</em> those are the starting parameters, because I made the data up.</p>

<p>However, I get this error:</p>

<pre><code>Error in nlsModel(formula, mf, start, wts) : 
  singular gradient matrix at initial parameter estimates
</code></pre>

<p>Have I just made unfortunate data? I tried fitting this on real data first, and was getting the same error, and I just figured that my initial starting parameters weren't good enough. </p>
"
"0.0446990156267674","0.0441081091391231","  7639","<p>I have 4301 lines of data from my science project.  I have to do unorthodox statistical analysis for my results because they are 4-dimensional; I have three independent variables.</p>

<p>With Excel I can do a lot of analysis, even multivariable linear regression, but what I need is a step up:  Multivariable nonlinear regression.</p>

<p>I have R.  Is there a way I can do it with R?</p>

<p>Also, is there a way to generate multiple 3-dimensional graphs with slices of the data automatically?</p>

<p>EDIT:  I've searched with no luck.  Is there <em>any</em> way of doing multivariable nonlinear regression automatically?</p>
"
"0.0547448901451359","0.0540211804549215","  7799","<p>I am a programmer who, in a past life, used the products RS/1, RS/Explore, and RS/Discover in a manufacturing engineer career.  Now, years later, I would like to do some multivariate regression on some real-world data (sales data from my wife's store).  The point would be to highlight what sales days are truly exceptional, not just part of the normal distribution, after taking into account month, day of week, weather, etc.</p>

<p>If I still had access to RS/1 and associated products, I'd know how to do this, but that is not the case, so I'm thinking I'd want to use R.  However, most R tutorials I have found just cover the very basics, and don't get to the point of multivariate regression.  Can you recommend an R tutorial that takes one past the basics of plotting a histogram, etc. into in-depth analysis of real-world ad-hoc data, presumably using multi-variate regression?</p>

<p>Thanks!</p>
"
"0.0632139541241014","0.0623782861551805","  7899","<p>I need to draw a complex graphics for visual data analysis.
I have 2 variables and a big number of cases (>1000). For example (number is 100 if to make dispersion less ""normal""):</p>

<pre><code>x &lt;- rnorm(100,mean=95,sd=50)
y &lt;- rnorm(100,mean=35,sd=20)
d &lt;- data.frame(x=x,y=y)
</code></pre>

<p>1) I need to plot raw data with point size, corresponding the relative frequency of coincidences, so <code>plot(x,y)</code> is not an option - I need point sizes. What should be done to achieve this?</p>

<p>2) On the same plot I need to plot 95% confidence interval ellipse and line representing change of correlation (do not know how to name it correctly) - something like this:</p>

<pre><code>library(corrgram)
corrgram(d, order=TRUE, lower.panel=panel.ellipse, upper.panel=panel.pts)
</code></pre>

<p><img src=""http://i.stack.imgur.com/R561Q.png"" alt=""correlogramm""></p>

<p>but with both graphs at one plot.</p>

<p>3) Finally, I need to draw a resulting linar regression model on top of this all:</p>

<pre><code>r&lt;-lm(y~x, data=d)
abline(r,col=2,lwd=2)
</code></pre>

<p>but with error range... something like on QQ-plot:</p>

<p><img src=""http://i.stack.imgur.com/vgvRr.png"" alt=""QQ-plot""></p>

<p>but for fitting errors, if it is possible.</p>

<p>So the question is: </p>

<p><strong>How to achieve all of this at one graph?</strong></p>
"
"NaN","NaN","  7919","<p>I am working on a linear regression with R and there are many 0 values in my predictor variables. How are these handled in R's <code>lm()</code> function? Should I remove this data for more accurate analysis? </p>

<p>Any advice is appreciated. Thanks. </p>
"
"0.0446990156267674","0.0441081091391231","  7975","<p>Having worked mostly with cross sectional data so far and very very recently browsing, scanning stumbling through a bunch of introductory time series literature I wonder what which role explanatory variables are playing in time series analysis. </p>

<p>I would like to <em>explain a trend</em> instead of de-trending.
Most of what I read as an introduction assumes that the series is stemming from some stochastic process. I read about AR(p) and MA processes as well as ARIMA modelling. Wanting to deal with more information than only autoregressive processes I found VAR / VECM and ran some examples, but still I wonder if there is some case that is related closer to what explanatories do in cross sections. </p>

<p>The motivation behind this is that decomposition of my series shows that the trend is the major contributor while remainder and seasonal effect hardly play a role. I would like to explain this trend.</p>

<p>Can / should I regress my series on multiple different series? Intuitively I would use gls because of serial correlation (I am not so sure about the cor structure). I heard about spurious regression and understand that this is a pitfall, nevertheless I am looking for a way to explain a trend. </p>

<p>Is this completely wrong or uncommon? Or have I just missed the right chapter so far?</p>
"
"0.104828483672192","0.103442685121077","  7996","<p>I am evaluating a scenario's output parameter's dependence on three parameters: A, B and C. For this, I am conducting the following experiments:</p>

<ul>
<li>Fix A+B, Vary C - Total four sets of (A+B) each having 4 variations of C</li>
<li>Fix B+C, Vary A - Total four sets of (B+C) each having 3 variations of C</li>
<li>Fix C+A, Vary B - Total four sets of (C+A) each having 6 variations of C</li>
</ul>

<p>The output of any simulation is the value of a variable over time. For instance, A could be the area, B could be the velocity and C could be the number of vehicles. The output variable I am observing is the number of car crashes over time. </p>

<p>I am trying to determine which parameter(s) dominate the outcome of the experiment. By dominate, I mean that sometimes, the outcomes just does not change when one of the parameters change but when some other parameter is changed even by a small amount, a large change in the output is observed. I need to capture this effect and output some analysis from which I can understand the dependence of the output on the input parameters. A friend suggested Sensitivity Analysis but am not sure if there are simpler ways of doing it. Can someone please help me with a good (possibly easy because I don't have a Stats background) technique? It would be great if all this can be done in R.</p>

<p><strong>Update:</strong> 
I used linear regression to obtain the following:</p>

<pre><code>lm(formula = T ~ A + S + V)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35928 -0.06842 -0.00698  0.05591  0.42844 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.01606    0.16437  -0.098 0.923391    
A            0.80199    0.15792   5.078 0.000112 ***
S           -0.27440    0.13160  -2.085 0.053441 .  
V           -0.31898    0.14889  -2.142 0.047892 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1665 on 16 degrees of freedom
Multiple R-squared: 0.6563, Adjusted R-squared: 0.5919 
F-statistic: 10.18 on 3 and 16 DF,  p-value: 0.0005416 
</code></pre>

<p>Does this mean that the output depends mostly on A and less on V?</p>
"
"0.14824986333222","0.146290048226517","  8340","<p><strong>Update: I wanted to clarify that this is a simulation. Sorry if I confused everyone. I have also used meaningful names for my variables.</strong></p>

<p>I am not a statistician so please correct me if I make a blunder in explaining what I want. In regard to my <a href=""http://stats.stackexchange.com/questions/7996/what-is-a-good-way-of-estimating-the-dependence-of-an-output-variable-on-the-inpu"">previous question</a>, I have reproduced parts of my question here for reference.</p>

<blockquote>
  <p>I am evaluating a scenario's output
  dependence on three
  variables: Area, Speed and NumOfVehicles. For this, I am
  conducting the following experiments:</p>
  
  <ul>
  <li>Fix Area+Speed, Vary NumOfVehicles - Total four sets of (Area+Speed) each having 4 variations of NumOfVehicles</li>
  <li>Fix Speed+NumOfVehicles, Vary Area - Total four sets of (Speed+NumOfVehicles) each having 3 variations of Area</li>
  <li>Fix NumOfVehicles+Area, Vary Speed - Total four sets of (NumOfVehicles+Area) each having 6 variations of Speed</li>
  </ul>
  
  <p>The output of any simulation is the
  value of a variable over time. The output
  variable I am observing is the time at which 80% of the cars crashe.</p>
  
  <p>I am trying to determine which
  parameter(s) dominate the outcome of
  the experiment. By dominate, I mean
  that sometimes, the outcomes just does
  not change when one of the parameters
  change but when some other parameter
  is changed even by a small amount, a
  large change in the output is
  observed. I need to capture this
  effect and output some analysis from
  which I can understand the dependence
  of the output on the input parameters.
  A friend suggested Sensitivity
  Analysis but am not sure if there are
  simpler ways of doing it. Can someone
  please help me with a good (possibly
  easy because I don't have a Stats
  background) technique? It would be
  great if all this can be done in R.</p>
</blockquote>

<p>My previous result was not very satisfactory looking at the regression results. So what I did was that I went ahead and repeated all my experiments 20 times each with different variations of each variable (so for instance, instead of 4 variations of Area, I now have 8 and so on). Following is the summary I obtained out of R after using linear regression:</p>

<pre><code>Call:
lm(formula = T ~ Area + Speed + NumOfVehicles)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.13315 -0.06332 -0.01346  0.04484  0.29676 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      0.04285    0.02953   1.451    0.148    
Area             0.70285    0.02390  29.406  &lt; 2e-16 ***
Speed           -0.15560    0.02080  -7.479 2.12e-12 ***
NumOfVehicles   -0.27447    0.02927  -9.376  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.08659 on 206 degrees of freedom
Multiple R-squared: 0.8304, Adjusted R-squared: 0.8279 
F-statistic: 336.2 on 3 and 206 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>as opposed to my previous result:</p>

<pre><code>lm(formula = T ~ Area + Speed + NumOfVehicles)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35928 -0.06842 -0.00698  0.05591  0.42844 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   -0.01606    0.16437  -0.098 0.923391    
Area           0.80199    0.15792   5.078 0.000112 ***
Speed         -0.27440    0.13160  -2.085 0.053441 .  
NumOfVehicles -0.31898    0.14889  -2.142 0.047892 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1665 on 16 degrees of freedom
Multiple R-squared: 0.6563, Adjusted R-squared: 0.5919 
F-statistic: 10.18 on 3 and 16 DF,  p-value: 0.0005416 
</code></pre>

<p>From my understanding, my current results have a lower standard error so that is good. In addition the Pr value also seems quite low which tells me that this result is better than my previous result. So can I go ahead and say that A has the maximum effect on the output and then come S and V in that order? Can I make any other deductions from this result?</p>

<p>Also, I was suggested that I look into adding additional variates like $A^2$ etc. but if $A$ is the area, what does saying ""time"" depends on $A^2$ actually mean? </p>
"
"0.078223277346843","0.0882162182782462","  8545","<p>I have some problems in using (and finding) the Chow test for structural breaks in a regression analysis using R. I want to find out if there are some structural changes including another variable (represents 3 spatial subregions).</p>

<p>Namely, is the regression with the subregions better than the overall model. Therefore I need some statistical validation. </p>

<p>I hope my problem is clear, isn't it?</p>

<p>Kind regards<br>
marco</p>

<p>Toy example in R:</p>

<pre><code>library(mlbench) # dataset
data(""BostonHousing"")

# data preparation
BostonHousing$region &lt;- ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[2], 1, 
                        ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[3], 2,
                        ifelse(BostonHousing$medv &gt; 
                               quantile(BostonHousing$medv)[4], 3, 1)))

BostonHousing$region &lt;- as.factor(BostonHousing$region)

# regression without any subregion 
reg1&lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)

summary(reg1)

# are there structural breaks using the factor ""region"" which
# indicates 3 spatial subregions
reg2&lt;- lm(medv ~ crim + indus + rm + region, data=BostonHousing)
</code></pre>

<p>------- subsequent entry</p>

<p>I struggled with your suggested package ""strucchange"", not knowing how to use the ""from"" and ""to"" arguments correctly with my factor ""region"". Nevertheless, I found one hint to calculate it by hand (https://stat.ethz.ch/pipermail/r-help/2007-June/133540.html). This results in the following output, but now I am not sure if my interpetation is valid. The results from the example above below.</p>

<p>Does this mean that region 3 is significant different from region 1? Contrary, region 2 is not? Further, each parameter (eg region1:crim) represents the beta for each regime and the model for this region respectively? Finally, the ANOVA states that there is a signif. difference between these models and that the consideration of regimes leads to a better model?</p>

<p>Thank you for your advices!
Best Marco</p>

<pre><code>fm0 &lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)
summary(fm0)
fm1 &lt;- lm(medv  ~ region / (crim + indus + rm), data=BostonHousing)
summary(fm1)
anova(fm0, fm1)
</code></pre>

<p>Results:</p>

<pre><code>Call:
lm(formula = medv ~ region/(crim + indus + rm), data = BostonHousing)

Residuals:
       Min         1Q     Median         3Q        Max 
-21.079383  -1.899551   0.005642   1.745593  23.588334 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    12.40774    3.07656   4.033 6.38e-05 ***
region2         6.01111    7.25917   0.828 0.408030    
region3       -34.65903    4.95836  -6.990 8.95e-12 ***
region1:crim   -0.19758    0.02415  -8.182 2.39e-15 ***
region2:crim   -0.03883    0.11787  -0.329 0.741954    
region3:crim    0.78882    0.22454   3.513 0.000484 ***
region1:indus  -0.34420    0.04314  -7.978 1.04e-14 ***
region2:indus  -0.02127    0.06172  -0.345 0.730550    
region3:indus   0.33876    0.09244   3.665 0.000275 ***
region1:rm      1.85877    0.47409   3.921 0.000101 ***
region2:rm      0.20768    1.10873   0.187 0.851491    
region3:rm      7.78018    0.53402  14.569  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.008 on 494 degrees of freedom
Multiple R-squared: 0.8142,     Adjusted R-squared: 0.8101 
F-statistic: 196.8 on 11 and 494 DF,  p-value: &lt; 2.2e-16

&gt; anova(fm0, fm1)
Analysis of Variance Table

Model 1: medv ~ crim + indus + rm
Model 2: medv ~ region/(crim + indus + rm)
  Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    
1    502 18559.4                                 
2    494  7936.6  8     10623 82.65 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0364965934300906","0.0540211804549215","  8661","<p>I'm trying to undertake a logistic regression analysis in <code>R</code>. I have attended courses covering this material using STATA. I am finding it very difficult to replicate functionality in <code>R</code>. Is it mature in this area? There seems to be little documentation or guidance available. Producing odds ratio output seems to require installing <code>epicalc</code> and/or <code>epitools</code> and/or others, none of which I can get to work, are outdated or lack documentation. I've used <code>glm</code> to do the logistic regression. Any suggestions would be welcome.  </p>

<p>I'd better make this a real question. How do I run a logistic regression and produce odds rations in <code>R</code>?  </p>

<p>Here's what I've done for a univariate analysis:  </p>

<p><code>x = glm(Outcome ~ Age, family=binomial(link=""logit""))</code>  </p>

<p>And for multivariate:  </p>

<p><code>y = glm(Outcome ~ Age + B + C, family=binomial(link=""logit""))</code>  </p>

<p>I've then looked at <code>x</code>, <code>y</code>, <code>summary(x)</code> and <code>summary(y)</code>.  </p>

<p>Is <code>x$coefficients</code> of any value?</p>
"
"0.078223277346843","0.0882162182782462","  8807","<p>I've been using the <a href=""http://cran.r-project.org/web/packages/caret/index.html"">caret package</a> in R to build predictive models for classification and regression.  Caret provides a unified interface to tune model hyper-parameters by cross validation or boot strapping.  For example, if you are building a simple 'nearest neighbors' model for classification, how many neighbors should you use?  2? 10? 100? Caret helps you answer this question by re-sampling your data, trying different parameters, and then aggregating the results to decide which yield the best predictive accuracy.</p>

<p>I like this approach because it is provides a robust methodology for choosing model hyper-parameters, and once you've chosen the final hyper-parameters it provides a cross-validated estimate of how 'good' the model is, using accuracy for classification models and RMSE for regression models.</p>

<p>I now have some time-series data that I want to build a regression model for, probably using a random forest. What is a good technique to assess the predictive accuracy of my model, given the nature of the data? If random forests don't really apply to time series data, what's the best way to build an accurate ensemble model for time series analysis?</p>
"
"0.0999500374687773","0.0986287303940589","  9506","<p>I am new to R and to time series analysis. I am trying to find the trend of a long (40 years) daily temperature time series and tried to different approximations. First one is just a simple linear regression and second one is Seasonal Decomposition of Time Series by Loess.</p>

<p>In the latter it appears that the seasonal component is greater than the trend. But, how do I quantify the trend? I would like just a number telling how strong is that trend.</p>

<pre><code>     Call:  stl(x = tsdata, s.window = ""periodic"")
     Time.series components:
        seasonal                trend            remainder               
Min.   :-8.482470191   Min.   :20.76670   Min.   :-11.863290365      
1st Qu.:-5.799037090   1st Qu.:22.17939   1st Qu.: -1.661246674 
Median :-0.756729578   Median :22.56694   Median :  0.026579468      
Mean   :-0.005442784   Mean   :22.53063   Mean   : -0.003716813 
3rd Qu.:5.695720249    3rd Qu.:22.91756   3rd Qu.:  1.700826647    
Max.   :9.919315613    Max.   :24.98834   Max.   : 12.305103891   

 IQR:
         STL.seasonal STL.trend STL.remainder data   
         11.4948       0.7382    3.3621       10.8051
       % 106.4          6.8      31.1         100.0  
     Weights: all == 1
     Other components: List of 5   
$ win  : Named num [1:3] 153411 549 365  
    $ deg  : Named int [1:3] 0 1 1   
$ jump : Named num [1:3] 15342 55 37  
    $ inner: int 2  
$ outer: int 0
</code></pre>

<p><img src=""http://i.stack.imgur.com/jwCSr.png"" alt=""enter image description here""></p>
"
"0.0999500374687773","0.0986287303940589","  9738","<p>I am attempting to build a Multinomial Logit model with dummy variables of the following form:</p>

<ul>
<li>The dependent variable represents 0-8 discrete choices.</li>
<li>Dummy Variable 1: 965 dummy vars</li>
<li>Dummy Variable 2: 805 dummy vars</li>
</ul>

<p>The data set I am using has the dummy columns pre-created, so it's a table of 72,381 rows and 1770 columns.</p>

<p>The first 965 columns represent the dummy columns for Variable 1; the next 805 columns represent the dummy columns for Variable 2.</p>

<p>I'm on a Sun Grid Machine at my university, so memory won't be an issue...</p>

<p>I have been able to generate the factors and generate <code>mlogit</code> data using code:</p>

<pre><code>mldata&lt;-mlogit.data(mydata, varying=NULL, choice=""pitch_type_1"", shape=""wide"")
</code></pre>

<p>my <code>mlogit</code> data looks like:</p>

<pre><code>""dependent_var"",""A variable"",""B Var"",""chid"",""alt""
FALSE,""110"",""19"",1,""0""
FALSE,""110"",""19"",1,""1""
FALSE,""110"",""19"",1,""2""
FALSE,""110"",""19"",1,""3""
FALSE,""110"",""19"",1,""4""
TRUE,""110"",""19"",1,""5""
FALSE,""110"",""19"",1,""6""
FALSE,""110"",""19"",1,""7""
FALSE,""110"",""19"",1,""8""
FALSE,""110"",""19"",2,""0""
FALSE,""110"",""19"",2,""1""
FALSE,""110"",""19"",2,""2""
FALSE,""110"",""19"",2,""3""
FALSE,""110"",""19"",2,""4""
FALSE,""110"",""19"",2,""5""
TRUE,""110"",""19"",2,""6""
FALSE,""110"",""19"",2,""7""
FALSE,""110"",""19"",2,""8""
TRUE,""110"",""561"",3,""0""

...
</code></pre>

<p>The <code>mldata</code> contains 651,431 rows.</p>

<p>If I try to run this full data set I get the following error:</p>

<pre><code>&gt; mlogit.model&lt;- mlogit(dependent_var~0|A+B, data = mldata, reflevel=""0"")
Error in model.matrix.default(formula, data) :
allocMatrix: too many elements specified
Calls: mlogit ... model.matrix.mFormula -&gt; model.matrix -&gt; model.matrix.default
Execution halted
</code></pre>

<p>Smaller datasets (<code>mldata</code> with only 595 rows) and <code>mlogit</code> works fine and generates the expected regression output.</p>

<p>Is there a problem with <code>mlogit</code> and huge datasets?</p>

<p>I suppose this is perhaps not the best way to assess this kind of data, but I am trying to replicate a previous analysis that was completed on a similar amount of similar data. </p>
"
"0.0774209661138764","0.0763974860547543"," 10774","<h2>Background</h2>

<p>I have data from a field study in which there are four treatment levels and six replicates in each of two blocks. (4x6x2=48 observations)</p>

<p>The blocks are about 1 mile apart, and within the blocks, there is a grid of 42, 2m x 4m plots and a 1m wide walkway; my study only used 24 plots in each block.</p>

<p>I would like to evaluate evaluate spatial covariance.</p>

<p>Here is an example analysis using the data from a single block, without accounting for spatial covariance. In the dataset, <code>plot</code> is the plot id, <code>x</code> is the x location and <code>y</code> the y location of each plot with plot 1 centered on 0, 0. <code>level</code> is the treatment level and <code>response</code> is the response variable. </p>

<pre><code>layout &lt;- structure(list(plot = c(1L, 3L, 5L, 7L, 8L, 11L, 12L, 15L, 16L, 
17L, 18L, 22L, 23L, 26L, 28L, 30L, 31L, 32L, 35L, 36L, 37L, 39L, 
40L, 42L), level = c(0L, 10L, 1L, 4L, 10L, 0L, 4L, 10L, 0L, 4L, 
0L, 1L, 0L, 10L, 1L, 10L, 4L, 4L, 1L, 1L, 1L, 0L, 10L, 4L), response = c(5.93, 
5.16, 5.42, 5.11, 5.46, 5.44, 5.78, 5.44, 5.15, 5.16, 5.17, 5.82, 
5.75, 4.48, 5.25, 5.49, 4.74, 4.09, 5.93, 5.91, 5.15, 4.5, 4.82, 
5.84), x = c(0, 0, 0, 3, 3, 3, 3, 6, 6, 6, 6, 9, 9, 12, 12, 12, 
15, 15, 15, 15, 18, 18, 18, 18), y = c(0, 10, 20, 0, 5, 20, 25, 
10, 15, 20, 25, 15, 20, 0, 15, 25, 0, 5, 20, 25, 0, 10, 20, 
25)), .Names = c(""plot"", ""level"", ""response"", ""x"", ""y""), row.names = c(NA, 
-24L), class = ""data.frame"")

model &lt;- lm(response ~ level, data = layout)      
summary(model)
</code></pre>

<h2>Questions</h2>

<ol>
<li>How can I calculate a covariance matrix and include it in my regression?</li>
<li>The blocks are very different, and there are strong treatment * block interactions. Is it appropriate to analyze them separately?</li>
</ol>
"
"0.0893980312535348","0.0771891909934654"," 10986","<p>I am trying to find some help with something that is called an ""Adjusted Analysis"" (or also Covariate Adjusted Logistic Regression);  a typical response has been that I might just want multivariable logistic regression, but this is not quite what I am looking for. The trouble I have is with what exactly an ""adjusted"" analysis is.</p>

<p>As an example, I have at my disposal a software suite that performs this type of adjusted analysis. We have some genes and various clinical variables from patients; what the method seems to do is adjust the p-values of the genes. But I can't figure out why, or how.  So I am trying to move outside of this software suite to truly understand what the underlying mathematics of this statistical technique is. </p>

<p>When I've posted this question in <a href=""http://stackoverflow.com/questions/6061305/using-r-for-covariate-adjusted-logistic-regression"">other places</a> the response has been that I should just take more courses in statistics. So while acknowledging my short comings, I would like to please ask if anyone can point me in a somewhat correct direction. I have been trying to find resources to help however I think I am not posing my question correctly enough.  As an aside I have a background in computer science and  more recently I am branching into biostatistics and I don't like using black box software so I would eventually like to re-implement this technique in R.</p>

<p>Thank you for any help that can be offered. Please let me know if there is a way I can pose my question clearer.</p>
"
"0.0547448901451359","0.0540211804549215"," 11039","<p>I want to run a multiple regression analysis measuring the effect of various independent variables on a continuous dependent variable measuring the strength of a political institution. The problem is that the data is measured in different years. Thus I might have data for countries A and B only for the year 2009 and for countries C and D only for the year 2010. In order to increase my sample, I would like to use all countries. How can I do that, ie which statistical method is most adequate? Thank you sooo much!</p>
"
"0.0821173352177038","0.0900353007582025"," 11107","<p>I need to do a logistic regression using R on my data. My response variable (<code>y</code>) is survival at weaning (<code>surv=0</code>; did not <code>surv=1</code>) and I have several independent variables which are binary and categoricals in nature.</p>

<p>I am following some examples on this website <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a> and trying to run some models.</p>

<p>Running the model: </p>

<pre><code>&gt; mysurv2 &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                 as.factor(pmtone), family=binomial(link=""logit""), data=ap)
&gt; summary(mysurv2)

Call:
glm(formula = surv ~ as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
    as.factor(pmtone), family = binomial(link = ""logit""), data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2837  -0.5121  -0.5121  -0.5058   2.0590  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7892.6  on 8791  degrees of freedom
Residual deviance: 7252.8  on 8784  degrees of freedom
  (341 observations deleted due to missingness)
AIC: 7268.8

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Adding the <code>na.action=na.pass</code> at the end of the model gave me an error message. I thought that this would take care NA's in my independent variables.</p>

<pre><code>&gt; mysurv &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                as.factor(pmtone), family=binomial(link=""logit""), data=ap, 
                na.action=na.pass)
Error: NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<p>Since this is my first time to venture into logistic regression, I am wondering whether there is any package in R that would be more suitable?</p>

<p>I am also tryng to understand the regression coefficients. The independent variables used in the model are:</p>

<ol>
<li><p>rectal temperature: </p>

<ul>
<li><code>(PTEM)1</code> = newborns with rectal temp. below 35.4 0C</li>
<li><code>(PTEM)2</code> = newborns with rectal temp. between 35.4 to 36.9 0C</li>
<li><code>(PTEM)3</code> = newborns with rectal temp. above 37.0 0C</li>
</ul></li>
<li><p>shivering:</p>

<ul>
<li><code>(pshiv)1</code> = newborns that were not shivering</li>
<li><code>(pshiv)2</code> = newborns that were shivering</li>
</ul></li>
<li><p>respiration:</p>

<ul>
<li><code>(presp)1</code> = newborns with normal respiration</li>
<li><code>(presp)2</code> = newborns with slight respiration problem</li>
<li><code>(presp)3</code> = newborns with poor respiration</li>
</ul></li>
<li><p>muscle tone:</p>

<ul>
<li><code>(pmtone)1</code> = newborns with normal muscle tone</li>
<li><code>(pmtone)2</code> = newborns with moderate muscle tone</li>
<li><code>(pmtone)1</code> = newborns with poor muscle tone</li>
</ul></li>
</ol>

<p>Looking at the coefficients, I got the following:</p>

<pre><code>                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>In my other analysis, I found that newborns:  </p>

<p>a) with higher rectal temperature<br>
b) do not shiver<br>
c) good respiration and<br>
d) good muscle tone at birth were more likely to survive.  </p>

<p>I am a bit confused with the coefficients I am getting above. I am wondering whether whether I am not interpreting the results correctly or is it something else?</p>
"
"0.0893980312535348","0.0771891909934654"," 11679","<p>I have a nested-case control study that I have been using for analysis. At the end of my work I have deduced a set of variables that I use later to to classify new cases. One example of a simple classifier I am using is a naive Bayes, which will output simply a probability. </p>

<p>So here is my question:</p>

<p>Could I make my probabilities reflect the real world? In my specific example, the condition that I am testing for has a prevalence of 33% in my study, but a it has a population prevalence of only 10%.  Bayes factors have been suggested to me as a way to achieve this, however I am little unsure how to set up the problem. </p>

<p>As an example I have seen a Bayes factor as a logit between the true vs. study prevalence of the outcome. The classifier however was a logistic regression, and in that case the Bayes factor was just added to the linear predictors. I think the example there was very specific, and perhaps an inappropriate method for probabilities of a naive Bayes. Instead what I did was add the logit Bayes factor to the logged probabilities, but I am also not convinced this is right either. I also think a simpler solution would be to use Bayes theorem directly, but there I am not sure how to represented my study vs.population prevalences. The method below isn't quite right, but gets at what I want:</p>

<pre><code>        p_final = classier_posterior*(population_prev)/(study_prev)
</code></pre>

<p>I should contextualize that I use the probabilities to establish a threshold for classification down stream.</p>
"
"0.0774209661138764","0.0763974860547543"," 11959","<p>In R the <code>princomp()</code>and the <code>factanal()</code> are somewhat similar. At least their output looks pretty similar. I learned that this is not surprising since the print function of <code>princomp</code> comes from <code>factanal</code>. I understand that SS loadings do not make much sense for <code>princomp</code> as it is bounded to <code>1</code> anyway. Moreover, as Joris stated on nabble, the proportion of variance is only printed because of the common print function, but does not contain valuable information when princomp is used. </p>

<p>What I do not understand is rather not an R question but more a multivariate stats question what is the conceptual difference between these PCA and Factor Analysis functions as they are used in R? This question relates particularly to the scores (let's assume ""regression"" scores for FA) respectively the difference between scores in both concepts? 
What should I rather use when I want to use to resulting scores in a regression model (for example in order to circumvent multicollinearity)? I also understand that PCA has a fixed number of components while FA has fewer factors than variables. </p>

<p>richiemorrisroe's answer in the thread suggested by Rob Hyndman might go into that direction.</p>
"
"0","0.0311891430775903"," 12469","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/7527/change-point-analysis-using-rs-nls"">Change point analysis using R&#39;s nls()</a>  </p>
</blockquote>



<p>I want to do a nonlinear regression with nls() but also include a specific type of segmented or piecewise regression. The Formula I want to implement is:</p>

<pre><code>S ~ b0 + (A &gt; T) * b1 * (A - T)
</code></pre>

<p><code>T</code> should be the threshold value or breakpoint as identified by the nonlinear-segmented regression. I know that I can use <code>""algorithm = plinear""</code> but that does not work at all.</p>

<hr>

<p>The data I have is:</p>

<pre><code>A   S
0.000809371 1
0.003642171 3
0.009712455 4
0.010521827 2
0.004046856 4
0.015378054 5
0.000404686 0
0.000404686 0
0.000404686 0
0.000809371 0
0.000809371 3
0.037635765 3
0.008903084 2
0.016187426 5
0.043301364 1
0.000404686 1
0.002428114 1
0.003642171 1
0.013759312 4
0.051395077 9
0.394568501 9
0.005665599 1
0.013354626 1
0.028732681 3
0.026304567 2
0.004451542 1
0.050585705 2
0.00647497  1
0.010926512 0
0.013354626 1
1.695632841 4
0.013354626 2
</code></pre>
"
"0.0446990156267674","0.0441081091391231"," 12885","<p>I always believed that time should not be used as a predictor in regressions (incl. gam's) because, then, one would simply ""describe"" the trend itself.  If the aim of a study is to find environmental parameters like temperature etc. that explain the variance in, letÂ´s say, activity of an animal, then I wonder, how can time be of any use? as a proxy for unmeasured parameters? </p>

<p>Some trends in time on activity data of harbor porpoises can be seen here:
-> <a href=""http://stats.stackexchange.com/questions/12712/how-to-handle-gaps-in-a-time-series-when-doing-gamm"">How to handle gaps in a time series when doing GAMM?</a></p>

<p>my problem is: when I include time in my model (measured in julian days), then 90% of all other parameters become insignificant (ts-shrinkage smoother from mgcv kick them out). If I leave time out, then some of them are significant...</p>

<p>The question is: is time allowed as a predictor (maybe even needed?) or is it messing up my analysis?</p>

<p>many thanks in advance</p>
"
"0.0446990156267674","0.0441081091391231"," 13069","<p>I am very interested about the potential of statistical analysis for simulation/forecasting/function estimation, etc. </p>

<p>However, I don't know much about it and my mathematical knowledge is still quite limited -- I am a junior undergraduate student in software engineering. </p>

<p>I am looking for a book that would get me started on certain things which I keep reading about: linear regression and other kinds of regression, bayesian methods, monte carlo methods, machine learning, etc.
I also want to get started with R so if there was a book that combined both, that would be awesome. </p>

<p>Preferably, I would like the book to explain things conceptually and not in too much technical details -- I would like statistics to be very intuitive to me, because I understand there are very many risky pitfalls in statistics. </p>

<p>I am off course willing to read more books to improve my understanding of topics which I deem valuable.</p>
"
"0.070675349274022","0.0697410440814588"," 13091","<p>I have this model:</p>

<pre><code>model &lt;- zelig(dv~(product*intervention), model = ""negbin"", data = data)
</code></pre>

<p>intervention has <strong>two levels</strong>: neutral(=0), treatment(=1)<br />
product has <strong>two levels</strong>: product1(=0), product2(=1)</p>

<p>I build f_all to just have one factor with 4 groups for comparison analysis.</p>

<p>Thus I have <strong>4 groups</strong> in f_all<br />
1. product1-neutral<br />
2. product1-treatment<br />
3. product2-neutral<br />
4. product2-treament<br /></p>

<p><strong>My interaction hypothesis is that treatment only works for product2.</strong></p>

<p>Zelig gives me my predicted significant interaction. <br /></p>

<p>Yet, I need planned contrasts to test my specific hypothesis: c(-1,1,0,0) and c(0,0,1,-1)</p>

<p>I researched and found a description of doing this with multcomp on this page: <a href=""http://stats.stackexchange.com/questions/12993/how-to-setup-and-interpret-anova-contrasts-with-the-car-package-in-r"">post comparisons</a></p>

<p>The regression output shows my predicted interaction</p>

<pre><code>(Intercept)  1.34223    0.08024  16.728   &lt;2e-16 ***
product      0.08747    0.08025   1.090   0.2757
intervention 0.07437    0.07731   0.962   0.3361
interaction  0.45645    0.22263   2.050   0.0403 * 
</code></pre>

<p>However, it said multcomp and the glht function is for linear models, but I am using a negbin model.</p>

<p><strong>3 Questions regarding this problem:</strong><br />
1. Can I do planned comparisons on my negbin model using multcomp?<br />
2. If not what appropriate method is there to do this for my negbin model?<br />
3. Based on R using treatment contrasts per default could I just interpret the interaction coefficient as the contrast comparing product2-neutral versus product2-treatment? Can I then interpret the intervention coefficient as contrast comparing product1-neutral versus product1-treament?</p>
"
"0.0547448901451359","0.0540211804549215"," 13152","<p>I always use <code>lm()</code> in R to perform linear regression of $y$ on $x$. That function returns a coefficient $\beta$ such that $$y = \beta x.$$</p>

<p>Today I learned about <strong>total least squares</strong> and that <code>princomp()</code> function (principal component analysis, PCA) can be used to perform it. It should be good for me (more accurate). I have done some tests using <code>princomp()</code>, like:</p>

<pre><code>r &lt;- princomp(Â ~Â xÂ +Â y)
</code></pre>

<p>My problem is: how to interpret its results? How can I get the regression coefficient? By ""coefficient"" I mean the number $\beta$ that I have to use to multiply the $x$ value to give a number close to $y$.</p>
"
"0.100365631932749","0.108042360909843"," 13172","<p>I would like to use a binary logistic regression model in the context of streaming data (multidimensional time series) in order to predict the value of the dependent variable of the data (i.e. row) that just arrived, given the past observations. As far as I know, logistic regression is traditionally used for postmortem analysis, where each dependent variable has already been set (either by inspection, or by the nature of the study). </p>

<p>What happens in the case of time series though,  where we want to make prediction (on the fly) about the dependent variable in terms of historical data (for example in a time window of the last $t$ seconds) and, of course, the previous estimates of the dependent variable?</p>

<p>And if you see the above system over time, how it should be constructed in order for the regression to work? Do we have to train it first by labeling, let's say, the first 50 rows of our data (i.e. setting the dependent variable to 0 or 1) and then use the current estimate of vector ${\beta}$ to estimate the new probability of the dependent variable being 0 or 1 for the data that just arrived (i.e. the new row that was just added to the system)?</p>

<p>To make my problem more clear, I am trying to build a system that parses a dataset row by row and tries to make prediction of a binary outcome (dependent variable) , given the knowledge (observation or estimation) of all the previous dependent or explanatory variables that have arrived in a fixed time window. My system is in Rerl and uses R for the inference. </p>
"
"0.126647210942508","0.132324327417369"," 14694","<p>I have a dataset of genomic information which I'm going to be comparing with various biochemical markers. Unfortunately a lot of the biochemical markers have limited ranges in their assays, so I have a lot of data that looks like ""40"", "">45"", ""35"", "">45"" for tests that have a threshold at 45 (for example).
My intended analysis for most of this data is linear regression in R. So what is the statistically correct way to deal with this data?</p>

<ol>
<li><p>Ignore it, let R cast the values with "">"" to <code>NA</code> and potentially lose information about important associations</p></li>
<li><p>Make the over threshold values equal to the threshold. This has similar problems to 1)</p></li>
<li><p>It depends. Sigh. Could you please give me some pointers as to what other considerations I should be thinking about or information you might need to answer my question? </p></li>
</ol>

<p>Edit: Based on the comments I've given more information about my datasets. The values which are out of range (GFR and Fol) are independent variables which I'll use in linear regression like so:</p>

<pre><code>lm(H~allele+Age+Sex+as.double(GFR)+as.double(Fol))
</code></pre>

<p>GFR looks like: </p>

<pre><code>summary(as.double(GFR)) 
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
31.00   70.00   77.00   75.66   83.00  100.00  105.00
</code></pre>

<p>and appears to be normally distributed:</p>

<pre><code>V = qqnorm(na.omit(as.double(GFR))
cor(V$x, V$y)
[1] 0.9911351
</code></pre>

<p>There are 105 values coded as "">90"" (not sure why the summary said Max is 100) out of 434.</p>

<p>Fol is distributed like so:</p>

<pre><code>summary(as.double(Fol))
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
6.10   23.20   29.80   29.14   35.70   45.30    8.00
</code></pre>

<p>and also appears to be normally distributed:</p>

<pre><code>V  = qqnorm(na.omit(as.double(Fol)))   
cor(V$x, V$y)
[1] 0.9911351
</code></pre>

<p>There are 8 out of 434 variables in Fol coded are "">45.3"". I took my cue for calling these normally distributed from <a href=""http://www.math.utah.edu/~davar/ps-pdf-files/Assessing_Normality.pdf"" rel=""nofollow"">this assessment of normality guide</a> ).</p>

<p>I also have another variable CRP which is a dependent variable, which I'd like to do linear regression on similarly to the above. CRP has 11 out of 434 coded as ""&lt;0.2"". Its distribution is:</p>

<pre><code>summary(as.double(CRP))
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
0.200   0.600   1.300   2.674   2.650 112.400  11.000
</code></pre>

<p>The data graphed is clearly not normaly and it has a correlation with qqnorm of 0.5153663. The value of 112 is a clear outlier. </p>

<p>I hope that makes it more clear. Please let me know if you need more information. Thanks for your help.</p>
"
"0.070675349274022","0.0697410440814588"," 15145","<p>I have performed the path analysis using the <code>sem</code> function in R. The model which I fitted consists of both direct and indirect paths. I have some trouble in interpreting the estimates of the SEM coefficients. </p>

<ul>
<li>Does R gives the value of total effect = (direct effect + indirect effect) directly or do I have to multiply the coefficients which are on the indirect path and then add them to the coefficients which is on the direct path? This is the usual way of doing path analysis with the raw/absolute correlation coefficients.</li>
</ul>

<p>For example consider X (independent variable), Y (dependent variable) and M (Mediating variable). </p>

<p>The raw/absolute correlation/ standardized regression coefficients between them are X and Y  -0.06; X and M 0.22 and M and Y 0.28 whereas on the path analysis/sem in R, the above coefficients are X and Y -0.13; X and M 0.22 and M and Y 0.31. </p>

<ul>
<li>Thus  is the total effect of X and Y  equal to -0.13?</li>
<li>Alternatively how should I interpret this coefficient considering the effect of variable M into the account?</li>
</ul>
"
"0.144991226648596","0.136571103769086"," 15160","<p>I have a large dataset with patients and I'm studying a rare outcome (~ 2%) and death is a competing risk (mean age ~69 years). I've used the R ""cmprsk"" package for my statistics and it seems that competing risks and the Cox regression are performing similarly although the competing risk analysis is more conservative giving hazard ratios closer to 1.</p>

<p>I've been suggested to do a Poisson regression on the data but the results don't make any sense and I would be really grateful to get some input on the benefits of doing this kind of analysis on survival data. I've created this simulation for creating a dataset with similar risk factors:</p>

<pre><code>library(""cmprsk"")
# The time for the study
accrual_time &lt;- 10
followup_time &lt;- 1

base_risk &lt;- list(""event"" = .015, ""cmprsk"" = .1)

risk_factors &lt;- list(list(""frequency""=.1, 
                ""event"" = base_risk$event*.5, 
                ""cmprsk"" = base_risk$cmprsk*2),
        list(""frequency""=.05, 
                ""event"" = base_risk$event*1, 
                ""cmprsk"" = base_risk$cmprsk*1),
        list(""frequency""=.05, 
                ""event"" = base_risk$event*-.5, 
                ""cmprsk"" = base_risk$cmprsk*0))

# Number of subjects
n &lt;- 5000

# Create base time, sequential inclusion
time_in_study &lt;- rep(c(1:n)/n*accrual_time + followup_time, 1)

set.seed(100)

# Create empty sets
x &lt;- matrix(0, ncol=length(risk_factors), nrow=n)
time_2_event &lt;- rep(0, n)
time_2_comprsk &lt;- rep(0, n)

# Create each studied observation and outcome
for(i in 1:n){
    # Set base risk
    event_risk &lt;- base_risk$event 
    comp_risk &lt;- base_risk$cmprsk

    for(j in 1:length(risk_factors)){
        x[i, j] &lt;- rbinom(1, 1, risk_factors[[j]]$frequency)[1]

        # If there is a risk factor defined
        if (x[i, j] &gt; 0){
            event_risk &lt;- event_risk +
                    risk_factors[[j]]$event
            comp_risk &lt;- comp_risk + 
                    risk_factors[[j]]$cmprsk
        }
    }

    # Time 2 event/risk is 1/rate meaning that higher number -&gt; shorter time
    time_2_event[i] &lt;- rexp(1, rate=event_risk)[1]
    time_2_comprsk[i] &lt;- rexp(1, rate=comp_risk)[1]
}

cn &lt;- c()
for(i in 1:length(risk_factors)){
    ev_rsk &lt;- risk_factors[[i]]$event/base_risk$event+1
    cmp_rsk &lt;- risk_factors[[i]]$cmprsk/base_risk$cmprsk+1
    name &lt;- paste(""Risk factor no: "", i, ""\n * ev="", ev_rsk, "" cr="", cmp_rsk, "" *"", sep="""")
    cn &lt;- c(cn, name)
}
colnames(x) &lt;- cn

# Select the event that happens first: study ends, evenent occurs, a competing event occurs
time &lt;- apply(cbind(time_in_study, time_2_event, time_2_comprsk), 1, min)

# Outcome identifiers
event &lt;- (time_2_event == time) + 0
comprsk &lt;- (time_2_comprsk == time) + 0
cens &lt;- event+2*(event==0 &amp; comprsk==1)

out.cox_ev &lt;- coxph(Surv(time, event)~x)
summary(out.cox_ev)

out.crr_ev &lt;- crr(time, cens, x, failcode=1)
summary(out.crr_ev)

out.cox_cmprsk &lt;- coxph(Surv(time, comprsk)~x)
summary(out.cox_cmprsk)

out.crr_cmprsk &lt;- crr(time, cens, x, failcode=2)
summary(out.crr_cmprsk)
</code></pre>

<p>The output makes sense but when I do a:</p>

<pre><code>out.glm_pr &lt;- glm(event ~ x, family=""poisson"")
summary(out.glm_pr)
</code></pre>

<p>It gives estimates of:</p>

<ul>
<li>RF 1 ~ .14 </li>
<li>RF 2 ~ .41 </li>
<li>RF 3 ~ -.23</li>
</ul>

<p>My questions: </p>

<ul>
<li>Is the glm() code correct or should I somehow transform my data?</li>
<li>Does the Poisson output make any sense and how should if so interpret it?</li>
<li>What are the benefits/pitfalls in using Poisson regression for survival data?</li>
</ul>

<p>Thanks!</p>

<hr>

<h2>UPDATE</h2>

<p>After adding exp(out.glm_pr$coefficients) the results are almost identical to the competing risk regression, here's a forest plot that compares the three:</p>

<p><img src=""http://i.stack.imgur.com/14Zt0.png"" alt=""A forestplot comparing the different methods - Poisson: 1.152  1.509  0.794, CRR: 1.151 1.524 0.812, Cox PH: 1.897 1.931 0.798""></p>

<p>The x-axis is perhaps not entirely valid (should be ""incident rate ratios"" for the Poisson regression) but why are the outcomes for CRR &amp; poisson almost identical?</p>

<p>As for testing over-dispersion I've found these two methods:</p>

<pre><code>&gt; library(qcc)
&gt; qcc.overdispersion.test(event)

Overdispersion test Obs.Var/Theor.Var Statistic p-value
       poisson data         0.9391878      4695 0.99902
&gt; 
&gt; library(pscl)
&gt; out.glm_nb &lt;- glm.nb(event ~ x)
Warning messages:
1: In theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace = control$trace &gt;  :
  iteration limit reached
2: In theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace = control$trace &gt;  :
  iteration limit reached
&gt; odTest(out.glm_nb)
Likelihood ratio test of H0: Poisson, as restricted NB model:
n.b., the distribution of the test-statistic under H0 is non-standard
e.g., see help(odTest) for details/references

Critical value of test statistic at the alpha= 0.05 level: 2.7055 
Chi-Square Test Statistic =  -0.0139 p-value = 0.5 
</code></pre>

<p>I conclude that there isn't any evidence of over-dispersion or are there other methods better suited for testing over-dispersion in this kind of survival data?</p>

<p>The quasipoisson analysis gives similar values:</p>

<pre><code>&gt; out.glm_quasi_pr &lt;- glm(event ~ x, family=quasipoisson(link=""log""))
&gt; round(exp(out.glm_quasi_pr$coefficients), 3)
(Intercept)       xRF 1       xRF 2       xRF 3 
      0.059       1.152       1.509       0.794 
</code></pre>
"
"0.0842852721654685","0.0935674292327708"," 15623","<p>I am completely new to R, just downloaded and installed it today. I am familiar with SAS and Stata; I am using R because I have found out that in survey regression analysis, R is capable of using data that have stratum with one PSU. However, I cannot figure out how to write the code at all.</p>

<p>Here is what I have done so far: read a Stata dataset and save the .RData file. I have also put in the MASS, pscl, and survey (for svyglm) packages.</p>

<p>Here's what I need to do:
1) I am using survey data, so I have a ""weight"" variable, a ""strata"" variable, and a ""PSU"" variable. I need to incorporate those; I know how to use svyset in Stata, but no idea in R.
2) I have stratum with singleton PSUs. I need to use an option called survey.lonely.psu I believe, and I have no idea where to even begin with that. This is the reason why I am using R instead of Stata as I do not want to collapse stratum or delete observations.
3) The types of regression models I have to run: survey negative binomial, survey zero-inflated negative binomial (need to also determine the predictors of zeros), survey logistic, and survey OLS regression.
4)I also really can't make much sense in R of how to write the model in R code. In Stata, I can simply write the model as:</p>

<p>svy: nbreg dependent_var independent_var1 independent_var2 independent_var3</p>

<p>I can't figure out how to do that at all in R.</p>

<p>Any and all help will be greatly appreciated.</p>
"
"0.0632139541241014","0.0623782861551805"," 16915","<p>Note that I do most of my analysis using R and Excel.</p>

<p>Let's take this data set for example. I modified it as the data itself is proprietary: the years are also different:</p>

<pre><code>1967    2,033,407
1968    2,162,275
1969    2,159,640
1970    2,312,352
1971    2,554,449
1972    2,548,425
1973    2,101,225
1974    1,951,944
1975    2,106,250
1976    1,687,625
1977    1,636,496
1978    1,494,525
1979    1,606,825
1980    1,460,937
1981    1,310,494
1982    1,319,750
1983    1,263,643
1984    1,171,656
1985    1,194,950
</code></pre>

<p>What I usually do:</p>

<ol>
<li>A linear regression</li>
<li>Some form of polynomial trending</li>
<li>Moving average and double moving average</li>
<li>Basic ARIMA using p = 1, q = 0.</li>
<li>I calculate the errors for all these as well</li>
<li>I average all the forecasts out and the error to have my final result.</li>
</ol>

<p>Note that I'm an engineer that wants to get into statistics and the ability to properly validate and calibrate my models.</p>

<h2>Question</h2>

<p>What is the correct way to forecast this to 5, 10, or even 15 future years?</p>

<p>In a way I'm looking to move beyond the plugging data into a model and believe the data. Yes, I'm aware I can look at the errors. I mainly use RMSE or MAE. But I still am not confident when it comes to just predicting data the right way.</p>

<h3>Note</h3>

<p>this is also related to <a href=""http://stats.stackexchange.com/questions/16545/how-can-i-be-confident-about-my-forecasts-and-improve-my-methodologies"">this question</a> I posted here before.</p>
"
"0.0857687593681569","0.103442685121077"," 17480","<p>I've created a few Cox regression models and I would like to see how well these models perform and I thought that perhaps a ROC-curve or a c-statistic might be useful similar to this articles use:</p>

<p><a href=""http://onlinelibrary.wiley.com/doi/10.1002/bjs.6930/abstract"" rel=""nofollow"">J. N. Armitage och J. H. van der Meulen, â€Identifying coâ€morbidity in surgical patients using administrative data with the Royal College of Surgeons Charlson Scoreâ€, British Journal of Surgery, vol. 97, num. 5, ss. 772-781, Maj 2010.</a></p>

<p>Armitage used logistic regression but I wonder if it's possible to use a model from the survival package, the <a href=""http://cran.r-project.org/web/packages/survivalROC/index.html"" rel=""nofollow"">survivalROC</a> gives a hint of this being possible but I can't figure out how to get that to work with a regular Cox regression. </p>

<p>I would be grateful if someone would show me how to do a ROC-analysis on this example:</p>

<pre><code>library(survival)
data(veteran)

attach(veteran)
surv &lt;- Surv(time, status)
fit &lt;- coxph(surv ~ trt + age + prior, data=veteran)
summary(fit)
</code></pre>

<p>If possible I would appreciate both the raw c-statics output and a nice graph</p>

<p>Thanks!</p>

<h2>Update</h2>

<p>Thank you very much for answers. @Dwin: I would just like to be sure that I've understood it right before selecting your answer. </p>

<p>The calculation as I understand it according to DWin's suggestion:</p>

<pre><code>library(survival)
library(rms)
data(veteran)

fit.cph &lt;- cph(surv ~ trt + age + prior, data=veteran, x=TRUE, y=TRUE, surv=TRUE)

# Summary fails!?
#summary(fit.cph)

# Get the Dxy
v &lt;- validate(fit.cph, dxy=TRUE, B=100)
# Is this the correct value?
Dxy = v[rownames(v)==""Dxy"", colnames(v)==""index.corrected""]

# The c-statistic according to the Dxy=2(c-0.5)
Dxy/2+0.5
</code></pre>

<p>I'm unfamiliar with the validate function and bootstrapping but after looking at prof. Frank Harrel's answer <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">here on R-help</a> I figured that it's probably the way to get the Dxy. The help for validate states:</p>

<blockquote>
  <p>... Somers' Dxy rank correlation to be computed at each resample (this
  takes a bit longer than the likelihood based statistics). The values
  corresponting to the row Dxy are equal to 2 * (C - 0.5) where C is the
  C-index or concordance probability.</p>
</blockquote>

<p>I guess I'm mostly confused by the columns. I figured that the corrected value is the one I should use but I haven't really understood the validate output:</p>

<pre><code>      index.orig training    test optimism index.corrected   n
Dxy      -0.0137  -0.0715 -0.0071  -0.0644          0.0507 100
R2        0.0079   0.0278  0.0037   0.0242         -0.0162 100
Slope     1.0000   1.0000  0.2939   0.7061          0.2939 100
...
</code></pre>

<p>In the <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">R-help question</a> I've understood that I should have ""surv=TRUE"" in the cph if I have strata but I'm uncertain on what the purpose of the ""u=60"" parameter in the validate function is. I would be grateful if you could help me understand these and check that I haven't made any mistakes.</p>
"
"0.113960576459638","0.112454054604034"," 17552","<p>I have a gene expression data-set with log2-transformed expression values (no NAs) for 495 genes for 59 samples for which values of a continuous response variable (r) are also known (no NAs). I want to use leave-one-out cross validation to test if r of the test sample can be predicted from the sample's gene expression.</p>

<p>For this, I intend to use the <a href=""http://cran.r-project.org/web/packages/samr/index.html"" rel=""nofollow"">samr</a> R package for Significance Analysis of Microarrays to identify significant genes associated with r in the training set of samples. Then, I want to generate a linear model using the significant genes as variables, which will then be used to predict r of the test sample. I have tried the following code to begin with, but when I generate the model and examine it, I see many NAs in the model summary, which makes me suspect that I am doing something wrong.</p>

<p>Can someone tell me what I might be doing wrong?</p>

<p>Secondly, I will appreciate any comment on the use of nperms (in SAM) with a value of 100. Is it too low for an expression data-set for 495 genes. </p>

<pre><code># rVals with the r values is read as a vector from a row of a table for phenotypic data read from a tab-delimited file with sample-names as column names and phenotype features as row-names
# geneVals is the log2-transformed gene expression data-set read as a matrix from a tab-delimited file with sample-names as column names and gene-names as row-names

# Perform SAM with FDR of 5% and obtain list of significant genes

sam &lt;- SAM(x=geneVals, y=rVals, resp.type=c(""Quantitative""),
testStatistic=c(""standard""), regression.method=c(""standard""), logged2=TRUE, 
fdr.output=0.05, eigengene.number=1, knn.neighbors=10, nperms=100, 
genenames=as.vector(rownames(geneVals)))

sigGenes &lt;- rbind(sam$siggenes.table$genes.up, sam$siggenes.table$genes.lo)

# Generate linear model
toModel &lt;- data.frame(t(rbind(rVals, geneVals)), check.names=FALSE)
myModel &lt;- lm(toModel[c('r', sigGenes[,c(""Gene ID"")])])

# Examine model
summary(myModel)

...output...

Call:
lm(formula = toModel[c(""rVals"", sigGenes[, c(""Gene ID"")])])

Residuals:
ALL 59 residuals are 0: no residual degrees of freedom!

Coefficients: (58 not defined because of singularities)
           Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   -18.29363         NA      NA       NA
`let-7e`       -1.70545         NA      NA       NA
`miR-125a-5p`   2.43177         NA      NA       NA
`miR-151-5p`    2.67439         NA      NA       NA
...
</code></pre>
"
"0.158034885310254","0.149707886772433"," 18576","<p><strong>Update</strong>: Sorry for another update but I've found some possible solutions with fractional polynomials and the competing risk-package that I need some help with.</p>

<hr>

<h2>The problem</h2>

<p>I can't find an easy way to do a time dependent coefficient analysis is in R. I want to be able to take my variables coefficient and do it into a time dependent coefficient (not variable) and then plot the variation against time:</p>

<p>$\beta_{my\_variable}=\beta_0+\beta_1*t+\beta_2*t^2...$</p>

<h2>Possible solutions</h2>

<h3>1) Splitting the dataset</h3>

<p>I've looked at <a href=""http://anson.ucdavis.edu/~johnson/st222/lab8/lab8.htm"">this</a> example (Se part 2 of the lab session) but the creation of a separate dataset seems complicated, computationally costly and not very intuitive...</p>

<h3>2) Reduced Rank models - The coxvc package</h3>

<p>The <a href=""https://www.msbi.nl/dnn/Research/SurvivalAnalysis/Coxmodelswithtimevaryingeffects.aspx"">coxvc package</a> provides an elegant way of dealing with the problem - here's a <a href=""https://openaccess.leidenuniv.nl/bitstream/handle/1887/4918/Appendix.pdf?sequence=5"">manual</a>. The problem is that the author is no longer developing the package (last version is since 05/23/2007), after some e-mail conversation I've gotten the package to work but one run took 5 hours on my dataset (140 000 entries) and gives extreme estimates at the end of the period. You can find a slightly updated <a href=""http://pastebin.com/uiPy0ueF"">package here</a> - I've mostly just updated the plot function. </p>

<p>It might be just a question of tweaking but since the software doesn't easily provide confidence intervals and the process is so time consuming I'm looking right now at other solutions. </p>

<h3>3) The timereg package</h3>

<p>The impressive <a href=""http://cran.r-project.org/web/packages/timereg/index.html"">timereg package</a> also addresses the problem but I'm not certain of how to use it and it doesn't give me a smooth plot.</p>

<h3>4) Fractional Polynomial Time (FPT) model</h3>

<p>I found Anika Buchholz' excellent dissertation on <a href=""http://deposit.ddb.de/cgi-bin/dokserv?idn=1008218782&amp;dok_var=d1&amp;dok_ext=pdf&amp;filename=1008218782.pdf"">""Assessment of timeâ€“varying longâ€“term effects of therapies and prognostic factors""</a> that does an excellent job covering different models. She concludes that <a href=""http://onlinelibrary.wiley.com/doi/10.1002/bimj.200610328/abstract"">Sauerbrei et al's proposed FPT</a> seems to be the most appropriate for time-dependent coefficients:</p>

<blockquote>
  <p>FPT is very good at detecting time-varying effects, while the Reduced Rank approach results in far too complex models, as it does not include selection of time-varying effects.</p>
</blockquote>

<p>The research seems very complete but it's slightly out of reach for me. I'm also a little wondering since she happens to work with Sauerbrei. It seems sound though and I  guess the analysis could be done with the <a href=""http://cran.r-project.org/web/packages/mfp/index.html"">mfp package</a> but I'm not sure how.</p>

<h3>5) The cmprsk package</h3>

<p>I've been thinking of doing my competing risk analysis but the calculations have been to time-consuming so I switched to the regular cox regression. The <a href=""http://www.inside-r.org/packages/cran/cmprsk/docs/crr"">crr</a> has thoug an option for time dependent covariates:</p>

<pre><code>....
cov2        matrix of covariates that will be multiplied 
            by functions of time; if used, often these 
            covariates would also appear in cov1 to give 
            a prop hazards effect plus a time interaction
....
</code></pre>

<p>There is the quadratic example but I'm don't quite follow where the time actually appears and I'm not sure of how to display it. I've also looked at the test.R file but the example there is basically the same...</p>

<h2>My example code</h2>

<p>Here's an example that I use to test the different possibilities</p>

<pre><code>library(""survival"")
library(""timereg"")
data(sTRACE)

# Basic cox regression    
surv &lt;- with(sTRACE, Surv(time/365,status==9))
fit1 &lt;- coxph(surv~age+sex+diabetes+chf+vf, data=sTRACE)
check &lt;- cox.zph(fit1)
print(check)
plot(check, resid=F)
# vf seems to be the most time varying

######################################
# Do the analysis with the code from #
# the example that I've found        #
######################################

# Split the dataset according to the splitSurv() from prof. Wesley O. Johnson
# http://anson.ucdavis.edu/~johnson/st222/lab8/splitSurv.ssc
new_split_dataset = splitSuv(sTRACE$time/365, sTRACE$status==9, sTRACE[, grep(""(age|sex|diabetes|chf|vf)"", names(sTRACE))])

surv2 &lt;- with(new_split_dataset, Surv(start, stop, event))
fit2 &lt;- coxph(surv2~age+sex+diabetes+chf+I(pspline(stop)*vf), data=new_split_dataset)
print(fit2)

######################################
# Do the analysis by just straifying #
######################################
fit3 &lt;- coxph(surv~age+sex+diabetes+chf+strata(vf), data=sTRACE)
print(fit3)

# High computational cost!
# The price for 259 events
sum((sTRACE$status==9)*1)
# ~240 times larger dataset!
NROW(new_split_dataset)/NROW(sTRACE)

########################################
# Do the analysis with the coxvc and   #
# the timecox from the timereg library #
########################################
Ft_1 &lt;- cbind(rep(1,nrow(sTRACE)),bs(sTRACE$time/365,df=3))
fit_coxvc1 &lt;- coxvc(surv~vf+sex, Ft_1, rank=2, data=sTRACE)

fit_coxvc2 &lt;- coxvc(surv~vf+sex, Ft_1, rank=1, data=sTRACE)

Ft_3 &lt;- cbind(rep(1,nrow(sTRACE)),bs(sTRACE$time/365,df=5))
fit_coxvc3 &lt;- coxvc(surv~vf+sex, Ft_3, rank=2, data=sTRACE)

layout(matrix(1:3, ncol=1))
my_plotcoxvc &lt;- function(fit, fun=""effects""){
    plotcoxvc(fit,fun=fun,xlab='time in years', ylim=c(-1,1), legend_x=.010)
    abline(0,0, lty=2, col=rgb(.5,.5,.5,.5))
    title(paste(""B-spline ="", NCOL(fit$Ftime)-1, ""df and rank ="", fit$rank))
}
my_plotcoxvc(fit_coxvc1)
my_plotcoxvc(fit_coxvc2)
my_plotcoxvc(fit_coxvc3)

# Next group
my_plotcoxvc(fit_coxvc1)

fit_timecox1&lt;-timecox(surv~sex + vf, data=sTRACE)
plot(fit_timecox1, xlab=""time in years"", specific.comps=c(2,3))
</code></pre>

<p>The code results in these graphs: Comparison of <a href=""http://i.stack.imgur.com/e2aSr.png"">different settings for coxvc</a> and  of the 
<a href=""http://i.stack.imgur.com/zXz1H.png"">coxvc and the timecox</a> plots. I guess the results are ok but I don't think I'll be able to explain the timecox graph - it seems to complex...</p>

<h2>My (current) questions</h2>

<ul>
<li>How do I do the FPT analysis in R?</li>
<li>How do I use the time covariate in cmprsk?</li>
<li>How do I plot the result (preferably with confidence intervals)?</li>
</ul>
"
"0.214771855635472","0.22058295326176"," 18709","<p>I want to fit mixed model using lme4, nlme, baysian regression package or any available. </p>

<p><em><strong>Mixed model in Asreml- R  coding conventions</em></strong></p>

<p>before going into specifics, we might want to have details on asreml-R conventions, for those who are unfamiliar with ASREML codes.</p>

<pre><code>y = XÏ„ + Zu + e ........................(1) ; 
</code></pre>

<p>the usual mixed model with, y denotes the n Ã— 1 vector of observations,where Ï„ is the pÃ—1 vector of ï¬xed eï¬€ects, X is an nÃ—p design matrix of full column rank which associates observations with the appropriate combination of ï¬xed eï¬€ects, u is the q Ã— 1 vector of random eï¬€ects, Z is the n Ã— q design matrix which associates observations with the appropriate combination of random eï¬€ects, and e is the n Ã— 1 vector of residual errors.The model (1) is called a linear mixed model or linear mixed eï¬€ects model. It is assumed </p>

<p><img src=""http://i.stack.imgur.com/gxdur.jpg"" alt=""enter image description here""></p>

<p>where the matrices G and R are functions of parameters Î³ and Ï†, respectively.</p>

<p>The parameter Î¸ is a variance parameter which we will refer to as the scale parameter.</p>

<p>In mixed eï¬€ects models with more than one residual variance, arising for example in the
analysis of data with more than one section or variate, the parameter Î¸ is
ï¬xed to one. In mixed eï¬€ects models with a single residual variance then Î¸ is equal to
the residual variance (Ïƒ2). In this case R must be correlation matrix. Further details on the models are provided in the <a href=""http://www.vsni.co.uk/downloads/asreml/release2/doc/asreml-R.pdf"">Asreml manual (link)</a>. </p>

<p>Variance structures for the errors: R structure and Variance structures for the random eï¬€ects: G structures can be specified.</p>

<p><img src=""http://i.stack.imgur.com/or4Gj.jpg"" alt=""enter image description here""><img src=""http://i.stack.imgur.com/oXTgc.jpg"" alt=""enter image description here""></p>

<p>variance modelling in asreml() it is important to understand the formation of variance structures via direct products. The usual least squares assumption (and the default in asreml()) is that these are independently and identically distributed (IID). However, if the data was from a field experiment laid out in a rectangular array of r rows by c columns, say, we could arrange the residuals e as a matrix and potentially consider that they were autocorrelated within rows and columns.Writing the residuals as a vector in field order, that is, by sorting the residuals rows
within columns (plots within blocks) the variance of the residuals might then be</p>

<p><img src=""http://i.stack.imgur.com/SPE5b.jpg"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/IcikW.jpg"" alt=""enter image description here""> are correlation matrices for the row model (order r, autocorrelation parameter Â½r) and column model (order c, autocorrelation parameter Â½c)
respectively. More specifically, a two-dimensional separable autoregressive spatial structure
(AR1 x Â­ AR1) is sometimes assumed for the common errors in a field trial analysis.</p>

<p><em><strong>The example data:</em></strong></p>

<p>nin89 is from asreml-R library, where different varities were grown in replications / blocks in rectangular field. To control additional variability in row or column direction each plot is referenced as Row and Column variables (row column design). Thus this row column design with blocking. Yield is measured variable. </p>

<p><strong>Example models</strong> </p>

<p>I need something equivalent to the asreml-R codes:</p>

<p>The simple model syntax will look like the follows:</p>

<pre><code> rcb.asr &lt;- asreml(yield âˆ¼ Variety, random = âˆ¼ Replicate, data = nin89)  
 .....model 0
</code></pre>

<p>The linear model is specified in the fixed (required), random (optional) and rcov (error
component) arguments as formula objects.The default is a simple error term and does not need to be formally specified for error term as in the model 0. </p>

<p>here the variety is fixed effect and random is replicates (blocks). Beside random and fixed terms we can specify error term. Which is default in this model 0. The residual or error component of the model is specified in a formula object through the rcov argument, see the following models 1:4. </p>

<p>The following model1 is more complex in which both G (random) and R (error) structure are specified.</p>

<p><strong>Model 1:</strong> </p>

<pre><code>data(nin89)


 # Model 1: RCB analysis with G and R structure
     rcb.asr &lt;- asreml(yield ~ Variety, random = ~ idv(Replicate), 
      rcov = ~ idv(units), data = nin89)
</code></pre>

<p>This model is equivalent to above model 0, and introduces the use of G and R variance model. Here the option random and rcov specifies random and rcov formulae to explicitly specify the G and R structures. where idv() is the special model function in asreml() that identifies the variance model. The expression idv(units) explicitly sets the variance matrix for e to a scaled identity.</p>

<p><em><strong># Model 2: two-dimensional spatial model with correlation in one direction</em></strong></p>

<pre><code>  sp.asr &lt;- asreml(yield ~ Variety, rcov = ~ Column:ar1(Row), data = nin89)
</code></pre>

<p>experimental units of nin89 are indexed by Column and Row. So we expect random variation in two direction - row and column direction in this case. where ar1() is a special function specifying a first order autoregressive variance model for Row. This call specifies a two-dimensional spatial structure for error but with spatial correlation in the row direction only.The variance model for Column is identity (id()) but does not need to be formally
specified as this is the default.</p>

<p><em><strong># model 3: two-dimensional spatial model, error structure in both direction</em></strong></p>

<pre><code> sp.asr &lt;- asreml(yield ~ Variety, rcov = ~ ar1(Column):ar1(Row),  
 data = nin89)
sp.asr &lt;- asreml(yield ~ Variety, random = ~ units, 
 rcov = ~ ar1(Column):ar1(Row), data = nin89)
</code></pre>

<p>similar to above model  2, however the correlation is two direction - autoregressive one. </p>

<p>I am not sure how much of these models are possible with open source R packages. Even if solution of any one of these models will be of great help. <strong><em>Even if the bouty of +50 can stimulate to develop such package will be of great help !</em></strong></p>

<p><em><strong>See MAYSaseen has provided output from each model and data  (as answer)  for comparision.</em></strong> </p>

<p><em><strong>Edits: 
The following is suggestion I received in mixed model discussion forum:</em></strong>
"" You might look at the regress and spatialCovariance packages of David Clifford.  The former allows fitting of (Gaussian) mixed models where you can specify the structure of the covariance matrix very flexibly (for example, I have used it for pedigree data).  The spatialCovariance package uses regress to provide more elaborate models than AR1xAR1, but may be applicable.  You may have to correspond with the author about applying it to your exact problem."" </p>
"
"0.0446990156267674","0.0441081091391231"," 18738","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/12398/how-to-interpret-f-and-p-value-in-anova"">How to interpret F- and p-value in ANOVA?</a>  </p>
</blockquote>



<p>I found that I can use ANOVA also for ONE Model, doing something like:</p>

<pre><code>&gt; anova(lm(a~b))
Analysis of Variance Table

Response: a
           Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    
b           1 0.002679 0.0026791  11.191 0.0009001 ***
Residuals 398 0.095282 0.0002394                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I know that ANOVA check the means BUT what test is that if I use only ONE model?
If the p.value is above 0.05 it means that the regression fit good?</p>
"
"0.122413295785282","0.112742029607188"," 18909","<p>I have an ordinal variable related to an outcome that is comprised of many levels and IÂ´d like to collapse the number of ordinal values as much as possible. </p>

<pre><code>&gt; require(ipred)
&gt; require(party)
&gt; data(GBSG2)
&gt; head(GBSG2)
  horTh age menostat tsize tgrade pnodes progrec estrec time cens
1    no  70     Post    21     II      3      48     66 1814    1
2   yes  56     Post    12     II      7      61     77 2018    1
3   yes  58     Post    35     II      9      52    271  712    1
4   yes  59     Post    17     II      4      60     29 1807    1
5    no  73     Post    35     II      1      26     65  772    1
6    no  32      Pre    57    III     24       0     13  448    1
&gt; table(GBSG2$tgrade)

  I  II III 
 81 444 161 
&gt; ctree(Surv(time,cens)~tgrade,data=GBSG2) -&gt; mn
&gt; plot(mn)
</code></pre>

<p><img src=""http://i.stack.imgur.com/WYIUd.png"" alt=""enter image description here""></p>

<p>Would it be correct to claim that <code>tgrade</code> here could be collapsed into two instead of three values?</p>

<p>edit:</p>

<p>Running the usual parametric analysis I get:</p>

<pre><code>&gt;     anova(i1,i2)
Analysis of Deviance Table
 Cox model: response is  Surv(time, cens)
 Model 1: ~ tgrade
 Model 2: ~ tgrade == ""I""
   loglik  Chisq Df P(&gt;|Chi|)  
1 -1776.0                      
2 -1778.1 4.3049  1     0.038 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt;     anova(i1,i3)
Analysis of Deviance Table
 Cox model: response is  Surv(time, cens)
 Model 1: ~ tgrade
 Model 2: ~ tgrade != ""III""
  loglik  Chisq Df P(&gt;|Chi|)    
1  -1776                        
2  -1784 16.033  1 6.225e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; anova(i2,i3)
Analysis of Deviance Table
 Cox model: response is  Surv(time, cens)
 Model 1: ~ tgrade == ""I""
 Model 2: ~ tgrade != ""III""
   loglik  Chisq Df P(&gt;|Chi|)    
1 -1778.1                        
2 -1784.0 11.728  0 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; extractAIC(i1)
[1]    2.000 3555.975
&gt; extractAIC(i2)
[1]    1.00 3558.28
&gt;   extractAIC(i3)
[1]    1.000 3570.008  
</code></pre>

<p>Hence the i1 model provides a better fit than i2 and i3, and i2 fits significantly better than i3. So now all three categories are warranted with respect to survival, which is at odds with the ctree approach. Can anyone explain this? Is this due to the conditional nature of ctree instead of the semiparametric nature of cox regression?</p>
"
"0","0.0311891430775903"," 19361","<p>Here is a sample output:</p>

<pre><code>anova(fit1,fit2);
Quantile Regression Analysis of Deviance Table

Model: op ~ inp1 + inp2 + inp3 + inp4 + inp5 + inp6 + inp7 + inp8 + inp9
Joint Test of Equality of Slopes: tau in {  0.15 0.3  }

  Df Resid Df F value Pr(&gt;F)
1  9     1337  0.5256 0.8568

Warning messages:
1: In summary.rq(x, se = ""nid"", covariance = TRUE) : 93 non-positive fis
2: In summary.rq(x, se = ""nid"", covariance = TRUE) : 138 non-positive fis
</code></pre>

<p>How to interpret the above results??
Does the <code>anova()</code> function give the best model, for <code>tau=0.15</code> vs. <code>tau=0.3</code>?</p>
"
"0.141511233180756","0.139640500579857"," 20452","<p>My primary question is how to interpret the output (coefficients, F, P) when conducting a Type I (sequential) ANOVA? </p>

<p>My specific research problem is a bit more complex, so I will break my example into parts. First, if I am interested in the effect of spider density (X1) on say plant growth (Y1) and I planted seedlings in enclosures and manipulated spider density, then I can analyze the data with a simple ANOVA or linear regression. Then it wouldn't matter if I used Type I, II, or III Sum of Squares (SS) for my ANOVA. In my case, I have 4 replicates of 5 density levels, so I can use density as a factor or as a continuous variable. In this case, I prefer to interpret it as a continuous independent (predictor) variable. In R I might run the following:</p>

<pre><code>lm1 &lt;- lm(y1 ~ density, data = Ena)
summary(lm1)
anova(lm1)
</code></pre>

<p>Running the anova function will make sense for comparison later hopefully, so please ignore the oddness of it here. The output is:</p>

<pre><code>Response: y1
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density    1 0.48357 0.48357  3.4279 0.08058 .
Residuals 18 2.53920 0.14107 
</code></pre>

<p>Now, let's say I suspect that the starting level of inorganic nitrogen in the soil, which I couldn't control, may have also significantly affected the plant growth. I'm not particularly interested in this effect but would like to potentially account for the variation it causes. Really, my primary interest is in the effects of spider density (hypothesis: increased spider density causes increased plant growth - presumably through reduction of herbivorous insects but I'm only testing the effect not the mechanism). I could add the effect of inorganic N to my analysis. </p>

<p>For the sake of my question, let's pretend that I test the interaction density*inorganicN and it's non-significant so I remove it from the analysis and run the following main effects:</p>

<pre><code>&gt; lm2 &lt;- lm(y1 ~ density + inorganicN, data = Ena)
&gt; anova(lm2)
Analysis of Variance Table

Response: y1
           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density     1 0.48357 0.48357  3.4113 0.08223 .
inorganicN  1 0.12936 0.12936  0.9126 0.35282  
Residuals  17 2.40983 0.14175 
</code></pre>

<p>Now, it makes a difference whether I use Type I or Type II SS (I know some people object to the terms Type I &amp; II etc. but given the popularity of SAS it's easy short-hand). R anova{stats} uses Type I by default. I can calculate the type II SS, F, and P for density by reversing the order of my main effects or I can use Dr. John Fox's ""car"" package (companion to applied regression). I prefer the latter method since it is easier for more complex problems.</p>

<pre><code>library(car)
Anova(lm2)
            Sum Sq Df F value  Pr(&gt;F)  
density    0.58425  1  4.1216 0.05829 .
inorganicN 0.12936  1  0.9126 0.35282  
Residuals  2.40983 17  
</code></pre>

<p>My understanding is that type II hypotheses would be, ""There is no linear effect of x1 on y1 given the effect of (holding constant?) x2"" and the same for x2 given x1. I guess this is where I get confused. <strong>What is the hypothesis being tested by the ANOVA using the type I (sequential) method above compared to the hypothesis using the type II method?</strong></p>

<p>In reality, my data is a bit more complex because I measured numerous metrics of plant growth as well as nutrient dynamics and litter decomposition. My actual analysis is something like:</p>

<pre><code>Y &lt;- cbind(y1 + y2 + y3 + y4 + y5)
# Type II
mlm1 &lt;- lm(Y ~ density + nitrate + Npred, data = Ena)
Manova(mlm1)

Type II MANOVA Tests: Pillai test statistic
        Df test stat approx F num Df den Df  Pr(&gt;F)    
density  1   0.34397        1      5     12 0.34269    
nitrate  1   0.99994    40337      5     12 &lt; 2e-16 ***
Npred    1   0.65582        5      5     12 0.01445 * 


# Type I
maov1 &lt;- manova(Y ~ density + nitrate + Npred, data = Ena)
summary(maov1)

          Df  Pillai approx F num Df den Df  Pr(&gt;F)    
density    1 0.99950     4762      5     12 &lt; 2e-16 ***
nitrate    1 0.99995    46248      5     12 &lt; 2e-16 ***
Npred      1 0.65582        5      5     12 0.01445 *  
Residuals 16                                           
</code></pre>
"
"0.104828483672192","0.103442685121077"," 20613","<p>Hello statistical gurus and R programming wizards,</p>

<p>I am interested in modeling animal captures as a function of environmental conditions and day of the year. As part of another study, I have counts of captures on ~160 days over three years. On each of these days I have temperature, rainfall, windspeed, relative humidity, etc. Because the data was collected repeatedly from the same 5 plots, I use plot as a random effect.</p>

<p>My understanding is that nlme can easily account for temporal autocorrelation in the residuals but doesn't handle non-Gaussian link functions like lme4 (which can't handle the autocorrelation?). Currently, I think it might work to use the nlme package in R on log(count). So my solution right now would be to run something like:</p>

<pre><code>m1 &lt;- lme(lcount ~ AirT + I(AirT^2) + RainAmt24 + I(RainAmt24^2) + RHpct + windspeed + 
      sin(2*pi/360*DOY) + cos(2*pi/360*DOY), random = ~1|plot, correlation =
      corARMA(p = 1, q = 1, form = ~DOY|plot), data = Data)
</code></pre>

<p>where DOY = Day of the Year. There may be more interactions in the final model, but this is my general idea. I could also potentially try to model the variance structure further with something like </p>

<pre><code>weights = v1Pow
</code></pre>

<p>I'm not sure if there is a better way to do with with a Poisson mixed model regression or anything? I just found mathematical discussion in Chapter 4 of ""Regression Models for Time Series Analysis"" by Kedem and Fokianos. It was a bit beyond me at the moment, especially in application (coding it in R). I also saw a MCMC solution in Zuur et al. Mixed Effects Models book (Chp 23) in the BUGS language (using winBUGS or JAG). Is that my best option? Is there an easy MCMC package in R that would handle this? I'm not really familiar with GAMM or GEE techniques but would be willing to explore these possibilities if people thought they'd provide better insight. <strong>My main objective is to create a model to predict animal captures given environmental conditions. Secondarily, I would like to explain what the animals a responding to in terms of their activity.</strong></p>

<p>Any thoughts on the best way to proceed (philosophically), how to code this in R, or in BUGS would be appreciated. I'm fairly new to R and BUGS (winBUGS) but am learning. This is also the first time I've ever tried to address temporal autocorrelation.</p>

<p>Thanks,
Dan</p>
"
"0.0446990156267674","0.0441081091391231"," 20725","<p>I have a model that looks like </p>

<pre><code>lm(y ~ lag(x, -1) + lag(z, -1))
</code></pre>

<p>So basically, this is a time series regression with exogenous variables, and I want to carry out a rolling analysis of sample forecasts, meaning that:
I first used a subsample (e.g., 1990-1995) for estimation, then I performed a one step ahead forecast, then I added one observation and made another one step ahead forecast, and so on.</p>

<p>I have tried to work with <code>rollapply</code>, defining the model as <code>arima(0,0,0)</code> with <code>xreg=lags</code> of the other variables, but that doesn't work. </p>

<p>Your help would be much appreciated!</p>
"
"0.0446990156267674","0.0441081091391231"," 21058","<p>I have some time series data where the measured variable is discrete positive integers (counts). I want to test if there is an upward trend over time (or not). The independent variable (x) is in the range 0-500 and the dependent variable (y) is in the the range 0-8.</p>

<p>I thought that I answer this by fitting a regression of the form <code>y =  floor(a*x + b)</code> using ordinary least squares (OLS).</p>

<p>How would I go about doing this using R (or Python)? Is there an existing package for it, or am I better off writing my own algorithm?</p>

<p>PS: I know this is not the ideal technique, but I need to do a relatively simple analysis that I can actually understand - my background is biology not maths. I know I am violating assumptions about error in measured variable, and independence of measurements over time.</p>
"
"0.070675349274022","0.0697410440814588"," 24187","<p>I have a dataset containing multiple proportions that add up to 1.
I am interested in the change of these proportions along a gradient (see below for example data).</p>

<pre><code>gradient &lt;- 1:99
A1 &lt;- gradient * 0.005
A2 &lt;- gradient * 0.004
A3 &lt;- 1 - (A1 + A2)

df &lt;- data.frame(gradient = gradient,
                 A1 = A1,
                 A2 = A2,
                 A3 = A3)

require(ggplot2)
require(reshape2)
dfm &lt;- melt(df, id = ""gradient"")
ggplot(dfm, aes(x = gradient, y = value, fill = variable)) +
  geom_area()
</code></pre>

<p><img src=""http://i.stack.imgur.com/aBbHc.png"" alt=""enter image description here""></p>

<p><strong>Additional information:</strong>
It need not be necessarily linear, I did this just for easiness of the example.
The original counts from which these proportions are calculated are also available. 
The real dataset contains more variable adding up to 1 (eg. B1, B2 &amp; B3, C1 to C4, etc) - so a hint for a multivariate solution is would be also helpful... But for now I'll stick on the univariate side of statistics.</p>

<p><strong>Question:</strong> 
How can one analyze such kind of data? 
IÂ´ve read a little bit around, and perhaps a multinomial model or a glm is suited? - If I run 3 (or 2) glms, how can I incorporate the constraint that the predicted values sum up to 1?
I dont want to only plot such kind of data, I also want to do a deeper regression like analysis.
I preferably want to use R - how can i do this in R?</p>
"
"0.0446990156267674","0.0441081091391231"," 24193","<p>I am performing a returns analysis. The idea is to regress a time-series of returns on the returns of various asset classes. The beta coefficients must be constrained such that sum of the coefficients is 1 and no coefficient is less than 0 or greater than 1. These beta coefficients can then be interpreted as explaining what % of returns are explained by exposure to the various asset classes.</p>

<p>Are there any packages in R that let me setup the above regression and benefit from the attendant reporting on model fit statistics? Or do I need to do some homework on setting up constrained least squares optimization in R (please provide any references to recommended R packages)?</p>
"
"0.126647210942508","0.132324327417369"," 24452","<p>I hope you all don't mind this question, but I need help interpreting output for a linear mixed effects model output I've been trying to learn to do in R. I am new to longitudinal data analysis and linear mixed effects regression. I have a model I fitted with weeks as the time predictor, and score on an employment course as my outcome. I modeled score with weeks (time) and several fixed effects, sex and race. My model includes random effects. I need help understanding what the variance and correlation means. The output is the following:</p>

<pre><code>Random effects  
Group   Name    Variance  
EmpId intercept 680.236  
weeks           13.562  
Residual 774.256  
</code></pre>

<p>The correlaton is .231.</p>

<p>I can interpret the correlation as there is a a positive relationship between weeks and score but I want to be able to say it in terms of ""23% of ..."".</p>

<p>I really appreciate the help. </p>

<hr>

<p>Thanks ""guest"" and Macro for replying. Sorry, for not replying, I was out at a conference and Iâ€™m now catching up. 
Here is the output and the context. </p>

<p>Here is the summary for the LMER model I ran. </p>

<pre><code>&gt;summary(LMER.EduA)  
Linear mixed model fit by maximum likelihood  
Formula: Score ~ Weeks + (1 + Weeks | EmpID)   
   Data: emp.LMER4 

  AIC     BIC   logLik   deviance   REMLdev   
 1815     1834  -732.6     1693    1685

Random effects:    
 Groups   Name       Variance Std.Dev. Corr  
 EmpID   (Intercept)  680.236  26.08133        
          Weeks         13.562 3.682662  0.231   
 Residual             774.256  27.82546        
Number of obs: 174, groups: EmpID, 18


Fixed effects:    
            Estimate Std. Error  t value  
(Intercept)  261.171      6.23     37.25    
Weeks          11.151      1.780    6.93

Correlation of Fixed Effects:  
     (Intr)  
Days -0.101
</code></pre>

<p>I donâ€™t understand how to interpret the variance and residual for the random effects and explain it to someone else. I also donâ€™t know how to interpret the correlation, other than it is positive which indicates that those with higher intercepts have higher slopes and those with those with lower intercepts have lower slopes but I donâ€™t know how to explain the correlation in terms of 23% of . . . . (I donâ€™t know how to finish the sentence or even if it makes sense to do so). This is a different type analysis for us as we (me) are trying to move into longitudinal analyses. </p>

<p>I hope this helps.</p>

<p>Thanks for your help so far. </p>

<p>Zeda</p>
"
"0.0952986215201744","0.103442685121077"," 24731","<p>this is, I think, an easy question... I did a regression analysis in R, where I wanted to check the fit of my data to a specific formula I provided... I got this to work, I see the graph and the line showing the fit of the data, but I also would like to get actual regression value obtained from this calculation, how do I do this? and How do I  get the actual equation printed in the graph?</p>

<hr>

<p>Hi thanks a lot for the replies.</p>

<p>The example was very useful, but I still didnt obtain the Regression value, so I still dont know how well my data fits the equation... below Im pasting the code I used</p>

<pre><code># import the input data file ""LD251-chilR.txt"", for the input; 
# the first column is pairwise distance and the second is LD estimate, 
# need to put the file in the same folder with R program
CT251chil&lt;-read.table(""LD251-chilR.txt"", sep=""\t"", dec="","", header=TRUE) 

# your LD estimate (r2 in this case which located in column 2)
r2=CT251chil[,2] 

# run nls function for getting rho estimate
nls(r2~1/(1+rho*CT251chil[,1]), start=list(rho=0.3)) 

# getting this rho estimate after running nls function in R
rho=0.02206872 

#sort the data, no need if your data already sorted
dist&lt;-sort(CT251chil[,1]) 

# put parameters in the equation as shown in my MBE paper
eq &lt;- (((10+rho*dist)/((2+rho*dist)*(11+rho*dist)))*(1+((3+rho*dist)*(12+12*rho*dist+(rho*dist)^2)/(46*(2+rho*dist)*(11+rho*dist))))) 

# plotting the graph between LD estimate and distance
plot(CT251chil[,1],CT251chil[,2], col=""black"", pch=20, 
     ylab=expression(R^2), xlab=""Pairwise distance"", 
     main=""CT251-chilense"", las=1) 

# getting regression line
lines(dist,eq, col=""black"",lwd=2,lty=1) 
</code></pre>

<p>After this point I get a graph with a line, all good. But so far I dont know how to output the actual equation on the graph get the value of R.</p>

<p>thank you for any further help!</p>
"
"0.070675349274022","0.0697410440814588"," 25285","<p>I have a data set of 91 variables. It is related to a qualitative analysis I made to analyse a few phenomena. For this reason, the questions are grouped in 10 clusters, since they express different aspects of the analysis. </p>

<p>I made a subset using the following R command:</p>

<pre><code>example1&lt;- subset(data, select=c(""a1"", ""a2"", ""a14"", ""a21""))
</code></pre>

<p>then I made a dependence matrix to check the dependence among the single variables in each cluster:</p>

<pre><code>p1 &lt;- stat1 &lt;- diag(ncol(example1))

colnames(p1) &lt;- rownames(p1) &lt;- colnames(exemple1)

colnames(stat1) &lt;- rownames(stat1) &lt;- colnames(example1) 
rn &lt;- rownames(p1) 
cn &lt;- colnames(p1)

###loop for the p-values
for(i in 1:ncol(example1)){ 
    for(j in 1:ncol(example1)){ 
        a &lt;- example1[, rn[i]] 
        b &lt;- example1[, cn[j]] 
        r &lt;- chisq.test(a,b)$p.value 
        p1[i, j] &lt;- r 
    }
}

###loop for the statistic
for(i in 1:ncol(example1)){ 
    for(j in 1:ncol(example1)){ 
        a &lt;- example1[, rn[i]] 
        b &lt;- example1[, cn[j]] 
        r &lt;- chisq.test(a,b)$statistic 
        stat1[i, j] &lt;- r 
    }
}

### placing the p-values in the upper diagonal of stat
stat1[upper.tri(stat1)] &lt;- p1[upper.tri(p1)] 
diag(stat1) &lt;- 1 
stat1  # this is for the first subset
</code></pre>

<p>But now I have two questions:</p>

<ol>
<li>How can I make the dependence hypothesis test among the variables and set my alpha? </li>
<li>How can I make the regression analysis?</li>
</ol>
"
"0.0565402794192176","0.0557928352651671"," 25839","<p>First off, I'll say I am a biologist and new to the statistics side of things so excuse my ignorance</p>

<p>I have a data set that consists of a binary outcome and then a bunch of trinary explanatory variables that looks something like this:</p>

<pre><code>head()
 Category block21_hap1 block21_hap2 block21_hap3 block21_check
1        1            1            1            0             2
2        1            2            0            0             2
3        1            1            0            1             2
4        1            1            0            1             2
5        1            1            1            0             2
6        1            1            1            0             2
</code></pre>

<p>A quick summary of the data</p>

<pre><code>summary()
Category block21_hap1 block21_hap2 block21_hap3 block21_check
 1:718    0:293        0:777        0:1026       2:1467       
 0:749    1:709        1:577        1: 390                    
          2:465        2:113        2:  51  
</code></pre>

<p>and another summary grouped by outcome levels</p>

<pre><code>by(hap.ped.final, hap.ped.final$Category, summary)
hap.ped.final$Category: 1
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:146        0:374        0:518        2:718        
 1:336        1:286        1:174                     
 2:236        2: 58        2: 26                     
---------------------------------------------------------------------------- 
hap.ped.final$Category: 0
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:147        0:403        0:508        2:749        
 1:373        1:291        1:216                     
 2:229        2: 55        2: 25          
</code></pre>

<p>So I am trying to run logistic regression on this data. When I do this:</p>

<pre><code>fit = glm(Category~ block21_hap1 + block21_hap2 + block21_hap3, data = hap.ped.final ,family = ""binomial"")
summary(fit)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.301  -1.177   1.059   1.177   1.200  

Coefficients: (1 not defined because of singularities)
                             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)                 -0.039221   0.280110  -0.140    0.889
hap.ped.final$block21_hap11  0.123555   0.183087   0.675    0.500
hap.ped.final$block21_hap12  0.009111   0.295069   0.031    0.975
hap.ped.final$block21_hap21 -0.084334   0.183087  -0.461    0.645
hap.ped.final$block21_hap22 -0.013889   0.337468  -0.041    0.967
hap.ped.final$block21_hap31  0.201113   0.183087   1.098    0.272
hap.ped.final$block21_hap32        NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2033  on 1466  degrees of freedom
Residual deviance: 2028  on 1461  degrees of freedom
AIC: 2040

Number of Fisher Scoring iterations: 3
</code></pre>

<p>So I don't really know what a singularity is or what's going wrong here that is throwing up NA's as a result of my analysis. Is it my data, or what I'm doing to it.
I tried googling the warning (or whatever you might call it) and I got some pages talking about collinearity and multilinearity, which I do not understand at all. 
Again, sorry for lack of knowledge here. I wish I had done more maths in undergrad. </p>
"
"0.114252409399597","0.112742029607188"," 26500","<p>Hello after struggling with using R for the last couple of days I was hoping someone could help me with a statistical analysis I am completing for an environmental science honours project. Using R statistics is not something we have been taught and I am worried that I may have bitten of more then I can chew, however my whole project is based around the <strong>hierarchical partitioning method and the exhaustive search multiple regression analysis method.</strong></p>

<p>The <a href=""http://cran.r-project.org/web/packages/hier.part/index.html"" rel=""nofollow"">hier.part</a> package was installed along with <a href=""http://cran.r-project.org/web/packages/gtools/index.html"" rel=""nofollow"">gtools</a>.</p>

<p>I have converted my dataset to a .csv file with seven independent variables and one dependant variable with around 400 replicates (my intention is to do this analysis on eight datasets in total with different amounts of replicates and another dependant variable, but I am starting with this one). The dependant variable is GPP, the independent variables are, NDVI, Temperature, Precipitation, Solar Radiation, Nutrient Availability and Soil Available Water Capacity.</p>

<p>Secondly I imported the .csv file into R using the script</p>

<pre><code>GPPANDDRIVER &lt;- read.table(""C:\\etc, header=T, sep="","")
</code></pre>

<p>This works fine and I can edit the table using </p>

<pre><code>edit(GPPANDDRIVER)
</code></pre>

<p>After looking at the <code>hier.part</code> package documentation available <a href=""http://cran.r-project.org/web/packages/hier.part/hier.part.pdf"" rel=""nofollow"">here</a> it seems like I need to define Y which in the script below is the dependent variable and define <code>scan</code> which is the independent variables (mentioned before).</p>

<pre><code>hier.part(y, xcan, family = ""gaussian"", gof = ""RMSPE"", barplot = TRUE)
</code></pre>

<p>I was defining the dependant <code>y</code> vector as </p>

<pre><code>y &lt;- as.vector(GPPANDDRIVER[""GPP""])
</code></pre>

<p>This also works fine and I have my y vector. However I am not sure how to load independent variables onto the xcan dataframe part of the script. I have tried typing in two scripts but they have not worked.</p>

<pre><code>xcan &lt;- as.vector(GPPANDDRIVER[-GPP])
## AND
xcan &lt;- data.frame(GPPANDDRIVER[-GPP])
</code></pre>

<p>If anyone could help me find the right script for representing my independant variables as xcan that would be greatly appreciated. Also once defined if I entered in the hier.part script mentioned above would R then show me results of the analysis after processing? I will be moving onto to the regression analysis after this if anyone can shed some light on this first problem.</p>

<pre><code>*information on hier.part arguments.*

**Arguments**

y a vector containing the dependent variables

xcan a dataframe containing the n independent variables

family family argument of glm

gof Goodness-of-fit measure. Currently ""RMSPE"", Root-mean-square â€™predictionâ€™

error, ""logLik"", Log-Likelihood or ""Rsqu"", R-squared

print.vars if FALSE, the function returns a vector of goodness-of-fit measures. If TRUE, a data frame is returned with first column listing variable combinations and the
second column listing goodness-of-fit measures.
</code></pre>
"
"0.0870136540574445","0.0930186419232112"," 26831","<p>Still on running logistic regression models and would like to ask a few questions around it.</p>

<p><strong>Question 1</strong>:
Is there a simple way of getting the p-values of each independent factor in a logistic regression model. For example, I am running this model:</p>

<pre><code>mymod3 &lt;- as.formula(surv~as.factor(tdate)+as.factor(sline)+as.factor(pgrp)
                                          +as.factor(weight5)+as.factor(backfat5)
                                          +as.factor(srect2)+as.factor(bcs)
                                          +as.factor(agit)+as.factor(uscore)
                                          +as.factor(loco)+as.factor(teat2)
                                          +as.factor(uscoref)+as.factor(colos)
                                          +as.factor(tb5)+as.factor(nerve)
                                          +as.factor(feed5)+as.factor(fos)
                                          +as.factor(gest3)+as.factor(int3)
                                          +as.factor(psex)+as.factor(bwt5)
                                          +as.factor(presp2)+as.factor(mtone2)
                                          +as.factor(pscolor)+as.factor(pmstain)
                                          +as.factor(pshiv)+as.factor(ppscore)
                                          +as.factor(pincis)+as.factor(prectem5)
                                          +as.factor(pcon12)+as.factor(crum5)
                                          +as.factor(pindx5))

sofNoMis3 &lt;- apf[which(complete.cases(apf[,all.vars(mymod3)])),]
FulMod3 &lt;- glm(mymod3,family=binomial(link=""logit""),data=sofNoMis3)
summary(FulMod3)
</code></pre>

<p>I am using this to look at the significant level of each factor:</p>

<pre><code>anova(FulMod3,test=""Chisq"")
</code></pre>

<p>and got this:</p>

<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: surv

Terms added sequentially (first to last)


                    Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                 7791     7096.2              
as.factor(tdate)    15    50.71      7776     7045.4 9.215e-06 ***
as.factor(sline)     1    13.90      7775     7031.5 0.0001924 ***
as.factor(pgrp)      3     8.83      7772     7022.7 0.0316335 *  
as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    
as.factor(bcs)       3     6.46      7760     7005.1 0.0910745 .  
as.factor(agit)      2    13.44      7758     6991.6 0.0012075 ** 
as.factor(uscore)    2     2.16      7756     6989.5 0.3401845    
as.factor(loco)      2     1.58      7754     6987.9 0.4530983    
as.factor(teat2)     2    25.45      7752     6962.4 2.980e-06 ***
as.factor(uscoref)   2     0.48      7750     6962.0 0.7861675    
as.factor(colos)     1     1.06      7749     6960.9 0.3034592    
as.factor(tb5)       4    49.22      7745     6911.7 5.265e-10 ***
as.factor(nerve)     2     0.99      7743     6910.7 0.6105452    
as.factor(feed5)     4    11.79      7739     6898.9 0.0190170 *  
as.factor(fos)       1    47.10      7738     6851.8 6.732e-12 ***
as.factor(gest3)     2    22.60      7736     6829.2 1.235e-05 ***
as.factor(int3)      2     6.61      7734     6822.6 0.0367298 *  
as.factor(psex)      1     9.50      7733     6813.1 0.0020493 ** 
as.factor(bwt5)      4   348.42      7729     6464.7 &lt; 2.2e-16 ***
as.factor(presp2)    1   106.23      7728     6358.4 &lt; 2.2e-16 ***
as.factor(mtone2)    1    34.13      7727     6324.3 5.146e-09 ***
as.factor(pscolor)   1    12.57      7726     6311.7 0.0003928 ***
as.factor(pmstain)   1     0.30      7725     6311.4 0.5845095    
as.factor(pshiv)     1    32.29      7724     6279.2 1.328e-08 ***
as.factor(ppscore)   1    16.71      7723     6262.4 4.351e-05 ***
as.factor(pincis)    1     0.02      7722     6262.4 0.8892848    
as.factor(prectem5)  4   126.06      7718     6136.4 &lt; 2.2e-16 ***
as.factor(pcon12)    1    17.88      7717     6118.5 2.350e-05 ***
as.factor(crum5)     4    15.25      7713     6103.2 0.0042137 ** 
as.factor(pindx5)    4    25.58      7709     6077.6 3.838e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>but it does not always agree with the final model after applying backward elimination:</p>

<p>Example: </p>

<p>these three factors were not significant above but they still appeared in the final model below</p>

<pre><code>as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    

step(FulMod3,direction=""backward"",trace=FALSE)
</code></pre>

<p>which gives:</p>

<pre><code>Call:  glm(formula = surv ~ as.factor(tdate) + as.factor(pgrp) + as.factor(weight5) + 
    as.factor(backfat5) + as.factor(srect2) + as.factor(agit) + 
    as.factor(uscore) + as.factor(teat2) + as.factor(uscoref) + 
    as.factor(fos) + as.factor(gest3) + as.factor(int3) + as.factor(psex) + 
    as.factor(bwt5) + as.factor(presp2) + as.factor(mtone2) + 
    as.factor(pscolor) + as.factor(pshiv) + as.factor(ppscore) + 
    as.factor(prectem5) + as.factor(pcon12) + as.factor(pindx5), 
    family = binomial(link = ""logit""), data = sofNoMis3)

Coefficients:
               (Intercept)  as.factor(tdate)2009-09-11  as.factor(tdate)2009-09-15  as.factor(tdate)2009-09-18  as.factor(tdate)2009-09-22  
                   1.34799                     0.18414                    -0.19490                    -0.15552                    -0.16822  
as.factor(tdate)2009-09-25  as.factor(tdate)2009-09-29  as.factor(tdate)2010-01-26  as.factor(tdate)2010-01-29  as.factor(tdate)2010-02-02  
                   0.60046                     0.80784                    -1.03442                    -1.30562                    -1.01486  
as.factor(tdate)2010-02-05  as.factor(tdate)2010-02-09  as.factor(tdate)2010-02-12  as.factor(tdate)2010-02-16  as.factor(tdate)2010-02-19  
                  -1.04438                    -0.89311                    -1.06260                    -0.79833                    -1.09651  
as.factor(tdate)2010-02-23            as.factor(pgrp)2            as.factor(pgrp)3            as.factor(pgrp)4         as.factor(weight5)2  
                  -0.55411                     0.12659                    -0.04727                     0.21817                    -0.22592  
       as.factor(weight5)3         as.factor(weight5)4         as.factor(weight5)5        as.factor(backfat5)2        as.factor(backfat5)3  
                  -0.10143                    -0.31562                    -0.37656                    -0.19883                    -0.01188  
      as.factor(backfat5)4        as.factor(backfat5)5          as.factor(srect2)2            as.factor(agit)2            as.factor(agit)3  
                   0.08293                    -0.17116                    -0.18201                    -0.49145                    -0.36659  
        as.factor(uscore)2          as.factor(uscore)3           as.factor(teat2)2           as.factor(teat2)3         as.factor(uscoref)2  
                  -0.12265                     0.15334                     0.16575                     0.21520                     0.24166  
       as.factor(uscoref)3             as.factor(fos)2           as.factor(gest3)2           as.factor(gest3)3            as.factor(int3)2  
                  -0.24363                    -0.29506                     0.09747                     0.81894                    -0.25595  
          as.factor(int3)3            as.factor(psex)2            as.factor(bwt5)2            as.factor(bwt5)3            as.factor(bwt5)4  
                  -1.21086                     0.20025                     0.30753                     0.29614                     0.56753  
          as.factor(bwt5)5          as.factor(presp2)2          as.factor(mtone2)2         as.factor(pscolor)2           as.factor(pshiv)2  
                   0.86479                    -0.29270                    -0.40912                    -0.72782                    -0.33848  
       as.factor(ppscore)2        as.factor(prectem5)2        as.factor(prectem5)3        as.factor(prectem5)4        as.factor(prectem5)5  
                  -0.25958                     0.73842                     0.77476                     0.92158                     0.96269  
        as.factor(pcon12)2          as.factor(pindx5)2          as.factor(pindx5)3          as.factor(pindx5)4          as.factor(pindx5)5  
                   0.38119                     0.43199                     0.44496                     0.73458                     0.59771  

Degrees of Freedom: 7791 Total (i.e. Null);  7732 Residual
Null Deviance:      7096 
Residual Deviance: 6102         AIC: 6222
</code></pre>

<p><strong>Question 2</strong>:</p>

<p>I would like to calculate the standard errors of the odds ratio of each factor level </p>

<pre><code>exp(NewMod3$coefficients)  #Odds ratios
</code></pre>

<p><strong>Question 3:</strong></p>

<p>Lastly, to tell whether the levels of each factor are significantly different or not </p>

<pre><code>               (Intercept) as.factor(tdate)2009-09-11 as.factor(tdate)2009-09-15 as.factor(tdate)2009-09-18 as.factor(tdate)2009-09-22 
                 3.8496863                  1.2021883                  0.8229141                  0.8559688                  0.8451676 
as.factor(tdate)2009-09-25 as.factor(tdate)2009-09-29 as.factor(tdate)2010-01-26 as.factor(tdate)2010-01-29 as.factor(tdate)2010-02-02 
                 1.8229563                  2.2430525                  0.3554327                  0.2710041                  0.3624544 
as.factor(tdate)2010-02-05 as.factor(tdate)2010-02-09 as.factor(tdate)2010-02-12 as.factor(tdate)2010-02-16 as.factor(tdate)2010-02-19 
                 0.3519109                  0.4093819                  0.3455567                  0.4500787                  0.3340336 
as.factor(tdate)2010-02-23           as.factor(pgrp)2           as.factor(pgrp)3           as.factor(pgrp)4        as.factor(weight5)2 
                 0.5745817                  1.1349500                  0.9538339                  1.2437928                  0.7977835 
       as.factor(weight5)3        as.factor(weight5)4        as.factor(weight5)5       as.factor(backfat5)2       as.factor(backfat5)3 
                 0.9035410                  0.7293337                  0.6862173                  0.8196866                  0.9881871 
      as.factor(backfat5)4       as.factor(backfat5)5         as.factor(srect2)2           as.factor(agit)2           as.factor(agit)3 
                 1.0864697                  0.8426844                  0.8335940                  0.6117399                  0.6930936 
        as.factor(uscore)2         as.factor(uscore)3          as.factor(teat2)2          as.factor(teat2)3        as.factor(uscoref)2 
                 0.8845715                  1.1657233                  1.1802836                  1.2401126                  1.2733576 
       as.factor(uscoref)3            as.factor(fos)2          as.factor(gest3)2          as.factor(gest3)3           as.factor(int3)2 
                 0.7837753                  0.7444886                  1.1023798                  2.2681046                  0.7741829 
          as.factor(int3)3           as.factor(psex)2           as.factor(bwt5)2           as.factor(bwt5)3           as.factor(bwt5)4 
                 0.2979401                  1.2217088                  1.3600609                  1.3446543                  1.7639063 
          as.factor(bwt5)5         as.factor(presp2)2         as.factor(mtone2)2        as.factor(pscolor)2          as.factor(pshiv)2 
                 2.3745019                  0.7462454                  0.6642372                  0.4829602                  0.7128545 
       as.factor(ppscore)2       as.factor(prectem5)2       as.factor(prectem5)3       as.factor(prectem5)4       as.factor(prectem5)5 
                 0.7713779                  2.0926314                  2.1700692                  2.5132469                  2.6187261 
        as.factor(pcon12)2         as.factor(pindx5)2         as.factor(pindx5)3         as.factor(pindx5)4         as.factor(pindx5)5 
                 1.4640265                  1.5403203                  1.5604231                  2.0845978                  1.8179532 
</code></pre>

<p>Example:</p>

<p>I would like to have a table like this:</p>

<pre><code>Factor           levels  Odds ratio

Parity group      (1)    1.00Â±standard error   a
                   2     1.50Â±standard errror  b
                  3-4    1.17Â±standard error   c
                   &gt;5    1.19Â±standard error   c
</code></pre>

<p>I would really appreciate your help on these 3 areas.</p>

<p>Baz</p>
"
"0.070675349274022","0.0557928352651671"," 27400","<p>I'm reading A. Agresti (2007), <em><a href=""http://rads.stackoverflow.com/amzn/click/0471226181"">An Introduction to Categorical Data Analysis</a></em>, 2nd. edition, and am not sure if I understand this paragraph (p.106, 4.2.1) correctly (although it should be easy):</p>

<blockquote>
  <p>In Table 3.1 on snoring and heart disease in the previous chapter, 254
  subjects reported snoring every night, of whom 30 had heart disease.
  If the data file has grouped binary data, a line in the data file
  reports these data as 30 cases of heart disease out of a sample size
  of 254. If the data file has ungrouped binary data, each line in the
  data file refers to a separate subject, so 30 lines contain a 1 for
  heart disease and 224 lines contain a 0 for heart disease. The ML
  estimates and SE values are the same for either type of data file.</p>
</blockquote>

<p>Transforming a set of ungrouped data (1 dependent, 1 independent) would take more then ""a line"" to include all the information!? </p>

<p>In the following example a (unrealistic!) simple data set is created and a logistic regression model is build. </p>

<p>How would grouped data actually look like (variable tab?)? How could the same model be build using grouped data? </p>

<pre><code>&gt; dat = data.frame(y=c(0,1,0,1,0), x=c(1,1,0,0,0))
&gt; dat
  y x
1 0 1
2 1 1
3 0 0
4 1 0
5 0 0
&gt; tab=table(dat)
&gt; tab
   x
y   0 1
  0 2 1
  1 1 1
&gt; mod1=glm(y~x, data=dat, family=binomial())
</code></pre>
"
"0.181568259800641","0.173738653382841"," 27553","<h2>Background</h2>

<p>There is a lot of discussion around this, so I thought that I could find my answer from earlier treads on StackExchange and by googling furiously. After using half a day trying to find only one reference book for (bio)statistics with R, I got utterly confused and had to give up. Maybe the free material combined is actually better than any of the books you can buy at the moment. Letâ€™s it find out. </p>

<p>The internet is full of good <a href=""http://cran.r-project.org/other-docs.html"">free literature for R language</a>, so there is really no point paying for a mediocre book, which ends up being used as an office decoration most of the time. The R home site lists <a href=""http://www.r-project.org/doc/bib/R-books.html"">books related to R</a> and there are a lot of them. To be more exact: 115. Only one of them is advertised with words â€œ<a href=""http://www.biomedical-engineering-online.com/content/4/1/18"">standalone statistics reference book</a>â€. It is 8 years old now and may be outdated. The fourth edition of <a href=""http://www.stats.ox.ac.uk/pub/MASS4/"">Modern Applied Statistics with S</a> is even older. <a href=""http://onlinelibrary.wiley.com/book/10.1002/9780470515075"">The R Book</a> is often chewed out as <a href=""http://rads.stackoverflow.com/amzn/click/0470510242"">too basic</a> and <a href=""http://www.springerlink.com/content/l36754377r182731/fulltext.pdf"">not recommended</a> because of lack of references, poorly formatted code and sloppy finish. </p>

<p>However, I am looking for <strong>one book</strong>, which I could use as a <strong>standalone reference to practical statistics</strong> (first and foremost) <strong>with R</strong> (secondary). The book should live on my office desk collecting annotations, coffee stains and greasy finger prints instead of dust on the book shelf. It should replace the collection of free pdfâ€™s I have been using so far, not forgetting that R comes with an excellent reference library. â€œ<em>What is the right approach?</em>â€, â€œ<em>why?</em>"" and â€œ<em>technically, how does it work?</em>â€ are often more burning questions than â€œ<em>how to do it with R?</em>â€ </p>

<p>Since I am an ecologist, I am mostly interested about applications to biostatistics. However, since these things are often connected, an interdisciplinary general reference would be the most valuable for me.</p>

<h2>The task</h2>

<p>If such a book exists (I doubt it), please provide the name of the book (only one per answer) and a short review of the book explaining why it should be named as the reference book for the topic. Since this question is not very different than the existing ones, please use <a href=""http://stats.stackexchange.com/questions/421/what-book-would-you-recommend-for-non-statistician-scientists""><strong>this tread</strong></a> for your answer. You can also list flaws of the book so that we can list those as the features for the ideal reference book.</p>

<p>My question is <strong>what should the reference book for statistics (of most used kinds)  with R contain?</strong></p>

<p>Some initial thoughts are following general features (please, update):</p>

<ul>
<li>Thick as a brick</li>
<li>Concise, but understandable</li>
<li>Filled with figures (with the R code provided)</li>
<li>Easy to understand tables and diagrams describing the most important details from the text</li>
<li>Easy to understand, descriptive text about the statistics / methods containing the most important equations.</li>
<li>Good examples for each approach (with R code)</li>
<li>Broad and up-to-date list of references</li>
<li>Minimal number of typos</li>
</ul>

<p><strong>Table of contents</strong></p>

<p>Since I am not a statistician and would need this (not existing?) book to answer the question it's hard for me to write about the contents. Because <a href=""http://onlinelibrary.wiley.com/book/10.1002/9780470515075"">The R Book</a> clearly intends to be the reference book for statistics with R, but is often criticized, I copied the table of contents from the book as a starting point for the table of contents for the standalone R statistics reference book. Additional task: please, provide additions, suggestions, deletions, etc for the table of contents. </p>

<ol>
<li>Getting Started </li>
<li>Essentials of the R Language </li>
<li>Data Input </li>
<li>Dataframes </li>
<li>Graphics </li>
<li>Tables </li>
<li>Mathematics </li>
<li>Classical Tests </li>
<li>Statistical Modelling </li>
<li>Regression</li>
<li>Analysis of Variance</li>
<li>Analysis of Covariance</li>
<li>Generalized Linear Models</li>
<li>Count Data</li>
<li>Count Data in Tables</li>
<li>Proportion Data</li>
<li>Binary Response Variables</li>
<li>Generalized Additive Models</li>
<li>Mixed-Effects Models</li>
<li>Non-linear Regression</li>
<li>Tree Models</li>
<li>Time Series Analysis</li>
<li>Multivariate Statistics</li>
<li>Spatial Statistics</li>
<li>Survival Analysis </li>
<li>Simulation Models</li>
<li>Changing the Look of Graphics</li>
<li>References and Further Reading</li>
<li>Index </li>
</ol>

<h2>What has been said earlier?</h2>

<p>StackExhange contains several treads asking statistics and R book suggestions. <a href=""http://stackoverflow.com/questions/192369/books-for-learning-the-r-language"">Books for learning the R language</a> asks about a reference book learning R language without statistics aspect. <a href=""http://www.use-r.org/downloads/_The_Art_of_R_Programming__A_Tour_of_Statistical_Software_Design.pdf"">The Art of R Programming</a> is ranked out as the best single suggestion. <a href=""http://stats.stackexchange.com/questions/25632/what-book-is-recommendable-to-start-learning-statistics-using-r-at-the-same-time"">Book to Learn Statistics using R</a> asks for an ideal introductory book to statistics, which is really not the same thing than a reference book. <a href=""http://stats.stackexchange.com/questions/614/open-source-statistical-textbooks"">Open Source statistical textbooks</a> ranks <a href=""http://www.opentextbook.org/2009/04/03/multivariate-statistics-with-r/"">Multivariate statistics with R</a> as the best alternative. <a href=""http://stats.stackexchange.com/questions/421/what-book-would-you-recommend-for-non-statistician-scientists"">What book would you recommend for non-statistician scientists?</a> asks about the best statistics reference book without specifying the program of choice. <a href=""http://stats.stackexchange.com/questions/10532/reference-or-book-on-simulation-of-experimental-design-data-in-r"">Reference or book on simulation of experimental design data in R</a> scores perhaps closest to my question. <a href=""http://rads.stackoverflow.com/amzn/click/1420068725"">Introduction to Scientific Programming and Simulation Using R</a> is the most recommended book here and might be close to what I am looking for. However, this book either won't suffice as a single reference book to statistics with R.</p>

<h2>Some suggestions for the reference book and their flaws</h2>

<p><a href=""http://www.manning.com/kabacoff/"">R in Action</a> has received better reviews than The R Book, yet it is apparently <a href=""http://www.jstatsoft.org/v46/b02/paper"">rather introductory</a>.</p>

<p><a href=""http://users.monash.edu.au/~murray/BDAR/index.html"">Biostatistical design and analysis using R: a practical guide</a> is perhaps close to what I am looking for. It has received a <a href=""http://www.tandfonline.com/doi/abs/10.1080/03949370.2011.618191"">good review</a>, but apparently also this one contains many typos. In addition, this book does not concentrate on explaining statistics, but rather gives statistical analyses as readymade recipes for researchers to use. </p>

<p><a href=""http://press.princeton.edu/titles/8709.html"">Ecological Models and Data in R</a> skips the introductory level. This is a very useful feature seeing that word ""introduction"", scores 43 occurrences in <a href=""http://www.r-project.org/doc/bib/R-books.html"">the R book list</a>,  but perhaps not entirely satisfying, if weâ€™re after the reference book for statisticsâ€¦?</p>

<p><a href=""http://www.crcpress.com/product/isbn/9781420068726"">Introduction to Scientific Programming and Simulation Using R</a> received very <a href=""http://www.jstatsoft.org/v36/b04/paper"">positive review</a>, but is limited to data simulation.</p>

<p>Richiemorrisroe suggests that <a href=""http://www.stats.ox.ac.uk/pub/MASS4/"">Modern Applied Statistics with S</a> is sufficient for a standalone statistics reference book with R. This book has received excellent reviews (<a href=""http://rads.stackoverflow.com/amzn/click/0387954570"">1</a>,<a href=""http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-0258%2819970715%2916:13%3C1548%3a%3aAID-SIM561%3E3.0.CO;2-U/abstract"">2</a>) and is probably the best candidate for the title at the moment? The most recent version came out 10 years ago, which is quite a long time considering program development.</p>

<p>Dimitriy V. Masterov suggests <a href=""http://www.stat.columbia.edu/~gelman/arm/"">Data Analysis Using Regression and Multilevel/Hierarchical Models</a>. Haven't checked this book out yet.</p>

<hr>

<p>After reading lots of book reviews, it seems apparent that the perfect book asked here does not exist yet. However, it is perhaps possible to choose one that is pretty close. This tread is intended as a community wiki for statistics users to find the best existing reference book and as a motivation for the new and old book writers to improve their work. </p>
"
"0.0446990156267674","0.0441081091391231"," 27753","<p>Would there be any problem with using principal component analysis (e.g. for reduction of dimensionality) so that principal components scores could be used as predictors in a mixed-model? For non-mixed models this strategy is frequently applied (principal component regression) but I am not sure if it is applicable in the context of mixed-models?</p>

<p>Please see below a dummy example in R:</p>

<pre><code>library(lme4)
USArrests$score &lt;- prcomp(USArrests[,-1], scale = TRUE)$x[,1]
USArrests$group[1:25)] &lt;- ""A""
USArrests$group[26:50] &lt;- ""B""
m1 &lt;- lmer(Murder~1+score+(1|group), data=USArrests)
summary(m1)
</code></pre>
"
"0.0836242010007091","0.0825187161885156"," 27945","<p>What is the meaning and effect of %in% in a model formula?</p>

<p>It is apparently used for nesting of one variable into another in a variety of analysis (manova, anova, regressions) in a few published articles.</p>

<p>From ?formula, b%in%a is a:b, so why use %in%?<br>
How is a:b nesting?</p>

<p>I am probably mistaken, but my understanding is that nesting b in a should not lead to the same mean square as the interaction of a and b denoted by a:b?</p>

<pre><code>library(lme4)  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>with(sleepstudy, Days%in%Subject)
  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ...  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fit&lt;-aov(data=sleepstudy, Reaction~Days + Days%in%Subject)
anova(fit)


               Df Sum Sq Mean Sq F value    Pr(&gt;F)    
 Days           1 162703  162703  193.23 &lt; 2.2e-16 ***
 Days:Subject  17 269685   15864   18.84 &lt; 2.2e-16 ***
 Residuals    161 135567     842
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
anova(fm1)


      Df Sum Sq Mean Sq F value
 Days  1  29986   29986  45.785
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction~Days + Days%in%Subject + (1|Subject), sleepstudy)
anova(fm1)

Analysis of Variance Table
             Df Sum Sq Mean Sq  F value
Days          1 162703  162703 248.4233
Days:Subject 17  73391    4317   6.5916
</code></pre>
"
"0.0899550337218996","0.0986287303940589"," 28115","<p>I am using the estimated county-level poverty measure from the Small Area Income and Poverty Estimates <a href=""http://www.census.gov/did/www/saipe/"" rel=""nofollow"">(SAIPE)</a> as the dependent variable in a regression analysis. This value is itself the result of a model and comes complete with upper and lower 90% confidence interval bounds. To be clear, each of my 3000+ observations has an estimated value and its own confidence interval based on the sample size for that county (targeted at 2.5% of population)</p>

<p>I am wondering about the best way to incorporate the uncertainty in my dependent variable into my regression model.</p>

<p>One way I have imagined is doing a random draw from a distribution using the estimated value and confidence interval for each observation. I would then re-run my regression using this simulated value for the dependent variable in my regression analysis and compare model outcomes to the model using the estimated value. By simulating new values and comparing many times I would gain an understanding of how sensitive my findings are to the estimates on the dependent variable.</p>

<p>A key component of this is pulling a random number based on the estimated value and the upper and lower confidence intervals. The data is left and right censored at 0 and 100 and the estimated value is not centered within the confidence interval that is: abs(estimate-cl) != abs(estimate-cu)</p>

<p>I was intrigued by the discussion here: <a href=""http://stats.stackexchange.com/questions/12742/sampling-random-numbers-from-a-distribution-with-asymmetric-confidence-intervals"">Sampling random numbers from a distribution with asymmetric confidence intervals generated by a bootstrapped estimate</a></p>

<p>but a modified version of the code just generates the estimated value
Here is an example using the first 6 records from the 2008 SAIPE</p>

<pre><code>sample.size&lt;-c(1258.850,4405.300,745.900,539.725,1444.850,273.02)
POV08L90&lt;-c(8.77,8.24,18.08,13.90,10.55,22.45)
POV08H90&lt;-c(12.54,11.18,24.82,20.87,15.27,33.83)
POV08&lt;-c(10.7,9.9,24.5,18.5,13.1,33.6)
test.data&lt;-data.frame(sample.size,POV08L90,POV08H90,POV08)

gammaGenerate&lt;-function(dat){
  for(i in 1:length(dat$sample.size)){
    n&lt;-dat[i,""sample.size""]
    cl&lt;-dat[i,""POV08L90""]
    cu&lt;-dat[i,""POV08H90""]
    barx&lt;-dat[i,""POV08""]
    talpha = qt(p=0.95,df=n-1)
    s = (cu - cl)*sqrt(n)/(2*talpha)
    kappa = 6*s*s*n*( cl - barx + talpha*s/sqrt(n) )
    gamma.shape = 4/(kappa*kappa)
    gamma.scale = s/sqrt(gamma.shape)
    gamma.shift = barx - gamma.shape*gamma.scale
    print(c(barx,(rgamma(n = 5, shape = gamma.shape) + gamma.shift)))    
  }
}
gammaGenerate(test.data)
</code></pre>

<p>Any help you can offer--either directing me to a better method of dealing with the uncertainty in my dependent variable, or an explanation for why my rgamma always lands at 0 would be very welcome.</p>
"
"0.113960576459638","0.103803742711416"," 30061","<p>What approaches exist to observe the time lag between two variables?</p>

<p>I need to analyze the relationship between blood pressure and some other factor, such as exercise. The data set I am drawing from has around 1800 individuals, with an average of 100 entries a piece. It is generally known that there is a strong relationship between exercise level and blood pressure. However, if a person increases their steps to 8000+ a day, how long will it take for their blood pressure to drop? I am new to this type of analysis, and this is a challenge I have been thinking about for weeks. </p>

<p>I don't know if anyone wants to comment on possible approaches to this challenge or any issues surrounding it.</p>

<p>Some issues I have been dealing with:</p>

<ol>
<li><p>Is it better to treat this as a times series analysis or longitudinal data analysis?</p>

<p>My understanding is that time series usually focuses on one variable with no missing data and is observed at consistent intervals, where as longitudinal is over a longer period and has inconsistent time intervals, dropouts, and missing data.</p>

<p>The data I have seems to fit the longitudinal description more, but it also seems like time series could be used if I averaged the values by week so there would be no missing entries. I'm not sure about the pros and cons of each approach.</p></li>
<li><p>Should I be fitting a causal model, or would some other method like regression be more helpful?</p>

<p>I've been looking at various possible causal models, for example Marginal Structural Models (MSM) or Structural Nested Models (SNM), but there seem to be very little information on their application. I did find one R package that applied inverse probability weights and then used Cox proportional hazards regression model on a survival object (MSM), but that seemed to be focus on weighting for confounding and right censoring. Its result was a correlation coefficient, which I don't think helps me.</p>

<p>So I'm not sure if fitting a causal model is what I want, because that seems to be more focused on the making intellectually satisfying assumptions about relationships within the data and then determining the degree of causality, rather than providing information about time lag.</p>

<p>If anyone knows about MSM, SNM, their use in R, or how they might relate to this problem, that would be awesome to hear.</p></li>
<li><p>What about survival analysis or SEM?</p>

<p>I haven't explored these options very in-depth yet but they sound potentially relevant.</p></li>
</ol>

<p>I've kind of stalled, so any hints about what direction I might want to go would be really appreciated. </p>

<p>Thanks in advance.</p>
"
"0.070675349274022","0.0697410440814588"," 30415","<p>I am trying to replicate a colleague's work and am moving the analysis from Stata to R. The models she employs invoke the ""cluster"" option within the nbreg function to cluster the standard errors.</p>

<p>See <a href=""http://repec.org/usug2007/crse.pdf"">http://repec.org/usug2007/crse.pdf</a> for a fairly complete description of the what and why of this option</p>

<p>My question is how to invoke this same option for negative binomial regression within R?</p>

<p>The primary model in our paper is specified in Stata as follows</p>

<pre><code> xi: nbreg cntpd09 logpop08 pcbnkthft07 pccrunion07 urbanpop pov00 pov002 edu4yr ///
 black04 hispanic04 respop i.pdpolicy i.maxloan rollover i.region if isser4 != 1,   
 cluster(state)
</code></pre>

<p>and I have replaced this with </p>

<pre><code>pday&lt;-glm.nb(cntpd09~logpop08+pcbnkthft07+pccrunion07+urbanpop+pov00+pov002+edu4yr+
black04+hispanic04+respop+as.factor(pdpolicy)+as.factor(maxloan)+rollover+
as.factor(region),data=data[which(data$isser4 != 1),])
</code></pre>

<p>which obviously lacks the clustered errors piece.</p>

<p>Is it possible to do an exact replication? If so how? If not, what are some reasonable alternatives?</p>

<p>Thanks</p>

<p>[Edit]
As noted in the comments, I was hoping for a solution that didn't take me into the realm of multilevel models. While my training allows me to see that these things should be related, it is more of a leap than I am comfortable taking on my own. As such I kept digging and found this link:
<a href=""http://landroni.wordpress.com/2012/06/02/fama-macbeth-and-cluster-robust-by-firm-and-time-standard-errors-in-r/"">http://landroni.wordpress.com/2012/06/02/fama-macbeth-and-cluster-robust-by-firm-and-time-standard-errors-in-r/</a></p>

<p>that points to some fairly straightforward code to do what I want:</p>

<pre><code>library(lmtest)
pday&lt;-glm.nb(cntpd09~logpop08+pcbnkthft07+pccrunion07+urbanpop+pov00+pov002+edu4yr+
 black04+hispanic04+respop+as.factor(pdpolicy)+as.factor(maxloan)+rollover+
 as.factor(region),data=data[which(data$isser4 != 1),])
summary(pday)

coeftest(pday, vcov=function(x) vcovHC(x, cluster=""state"", type=""HC1""))
</code></pre>

<p>This doesn't replicate the results from the analysis in Stata though, probably because it is designed to work on OLS not negative binomial. So the search goes on. Any pointers on where I am going wrong would be much appreciated</p>
"
"0.0645174717615637","0.0763974860547543"," 32313","<p>I have a linear regression model that is used to forecast the 'afluent natural energy' (ANE) of some region.</p>

<p>The predictors for this model are:</p>

<ul>
<li>the previous month ANE (<code>ANE0</code>)</li>
<li>the previous month rain volume (<code>PREC0</code>)</li>
<li>the current month forecast for rain volume (<code>PREC1</code>)</li>
</ul>

<p>We have 7 years of historical data for all of these variables, for each month. The current model just runs a OLS linear regression. I feel there's a lot of improvements to be done, but i'm not a time series specialist.</p>

<p>The first thing I notice is that the predictors are highly correlated (multicollinearity).
I'm not certain of the impacts of multicollinearity on prediction confidence.</p>

<p>I decided to try a time series approach, so I ran a ACF and PACF on the historic data:
The ACF shows a sine wave pattern, and the PACF has a spike at 1 and 2. So I tried both <code>ARIMA (2, 0, 0)</code> and <code>ARIMA(2,0,1)</code> to predict 20 periods ahead.</p>

<p>The ARIMA(2,0,1) shows good results, but I'm not certain as to how to compare it to the linear regression model.</p>

<p>What's the best way to test the performance of these model?  I'm using R as analysis tool (together with the <code>forecast</code> package). </p>
"
"0.0842852721654685","0.0935674292327708"," 32735","<p>As a financial institution, we often run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  Recently we are building another model in which I believe we have regression with autocorrelated errors.The residuals from linear model have <code>lm(object)</code> has clearly a AR(1) structure, as evident from ACF and PACF.  I took two different approaches, the first one was obviously to fit the model using Generalized least squares <code>gls()</code> in R. My expectation was that the residuals from gls(object) would be a white noise (independent errors).  But the residuals from <code>gls(object)</code> still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing that I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (the residuals are white noise). I really want to use <code>gls()</code> in <code>nlme</code> package so that coding will be lot simpler and easier. What would be the approach I should take here? Am I supposed to use REML? or is my expectation of non-correlated residuals (white noise) from gls() object wrong?</p>

<pre><code>gls.bk_ai &lt;- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, 
                 correlation=corARMA(p=1), method='ML',  data  = fit.cap01A)

gls2.bk_ai  &lt;- update(gls.bk_ai, correlation = corARMA(p=2))

gls3.bk_ai &lt;- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai &lt;- update(gls.bk_ai, correlation = NULL)

anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  
     ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise
</code></pre>

<p>Is there something wrong with what I am doing???????</p>
"
"0.070675349274022","0.0697410440814588"," 33025","<p>Is there a way to use the factor scores from one dataset to ""partial out"" the effects from another dataset which has the same variables? Basically I have two datasets: healthy people and sick people. I'd run factor analysis on the healthy set (called ""base""), restricting to only one factor so that it acts like a summary score:</p>

<pre><code>fa=factanal(base,1,rotation=""varimax"",scores=""regression"")
</code></pre>

<p>Then I'd like to use the factor scores from the healthy population and regress out the factor from the sick population. The aim of this is to partial out any underlying relationships in the variables which may not be due to the people being sick. I know how to partial out the scores from the ""base"" data (see below) but the dimensions for fa$scores differ from the healthy vs sick people... any ideas? Is this doable?</p>

<pre><code>pdata=as.data.frame(matrix(0,0,nrow=nrow(base),ncol=ncol(base)))
for (i in 1:ncol(base)){
    pdata[,i]=residuals(lm(base[,i]~fa$scores))
}
</code></pre>
"
"0.145392094485433","0.143470058156915"," 33105","<p>I'm working on an ongoing data analysis project about a series of live educational seminars. Each of my data points represents one such event, and for each one I have a multitude of categorical variables, as well as a couple quantitative ones that are my desired response variables (total revenue and number of attendees).</p>

<p>One trend I'm interested in looking at is how the frequency of these events affects my two response variables. Over the years, we have increased the frequency of the events and I'd like to determine whether or not it makes sense to continue doing so. I've created a couple of variables to help track this frequency:</p>

<p><code>NEAREST.SEM</code> - the number of days between this event and the nearest one to it chronologically in either direction</p>

<p><code>LAST.SEM</code> - the number of days between this event and the nearest one to it chronologically <em>before</em> it</p>

<p><code>WEEKLY.SEMS</code> - the total number of events held during the 7-day period starting on Monday within which this event falls</p>

<p>Depending on how I do the analysis, these three variables seem to have varying significance, but the one that seems to consistently come out on top is <code>NEAREST.SEM</code>, which I have found to be significant at the 0.01 level in one test and the 0.001 level in another. The other two variables are significant in predicting revenue but not number of attendees, which is not ideal since we are more interested in number of attendees. (The data for revenue is not representative of the total revenue for each event due to certain special offers for repeat customers that aren't taken into account there.)</p>

<p>Increasing the frequency of events seems to decrease each event's individual performance, but has so far increased overall performance. I'd like to determine the ""turning point"" at which overall performance will either dip or level off. Unfortunately, this is going to be tough to predict because my best-fitted variable, <code>NEAREST.SEM</code>, isn't as good a representation of increased frequency. Note, for example, that it would look exactly the same whether 4 or 5 events were held per week--it would always have the value of 1 in such situations. In fact, any time that events are grouped in clusters of consecutive days, we'll always get 1 for them on this variable...</p>

<p>One option would be to just use <code>WEEKLY.SEMS</code> as a predictor of revenue, which it is well correlated with, but as I said, we'd much rather do this analysis based on number of attendees, a better measure of an event's success.</p>

<p>So I really have two questions here:</p>

<ol>
<li><p>Any suggestions on my dilemma of which variable to use and how to deal with the problems I laid out above?</p></li>
<li><p>Once I decide on a predictor factor, how can I go about estimating the average decrease in revenue increasing to various frequencies will have? Should I run a multiple regression using all my variables and use the coefficient on the predictor factor? Or should I run a regression with just the one factor and my response and use that coefficient? Or is there a better test than regression to use?</p></li>
</ol>

<p>(By the way, I'm using R for my analysis and I'd appreciate any advice specifically tailored for that language.)</p>

<p>UPDATE: I have tried creating two new measures, one that's the average distance in days of the nearest event on either side, and one that's the number of events within 3 days in both directions...neither of them had any significant correlation. I'm running out of ideas here...</p>
"
"0.109815159255115","0.116699087583415"," 33463","<p>I've got a dataset for Temperature &amp; KwH and I'm currently performing the regression below. (further regression based on coeffs is performed within PHP)</p>

<pre><code># Some kind of List structure..
UsageDataFrame  &lt;- data.frame(Energy, Temperatures);

# lm is used to fit linear models. It can be used to carry out regression,
# single stratum analysis of variance and analysis of covariance (although
# aov may provide a more convenient interface for these).
LinearModel     &lt;- lm(Energy ~ 1+Temperatures+I(Temperatures^2), data = UsageDataFrame)

# coefficients
Coefficients    &lt;- coefficients(LinearModel)

system('clear');

cat(""--- Coefficients ---\n"");
print(Coefficients);
cat('\n\n');
</code></pre>

<p>The issue comes with our data, we can't ensure there isn't random communication failures or just random errors. This can leave us with values like</p>

<pre><code>Temperatures &lt;- c(16,15,13,18,20,17,20);
Energy &lt;- c(4,3,3,4,0,60,4)

Temperatures &lt;- c(17,17,14,17,21,16,19);
Energy &lt;- c(4,3,3,4,0,0,4)
</code></pre>

<p>Now as humans we can clearly see that the 60 for Kwh is a mistake based on the temperature, however we have over 2,000 systems each with multiple meters and each in different locations all over the country.. and with different levels of normal Energy usage.</p>

<p>A normal dataset would be 48 values for both Temperatures &amp; Energy per day, per meter. In a full year its likely we could have around 0-500 bad points per meter out of 17520 points.</p>

<p>I've read other posts about the <code>tawny</code> package however I've not really seen any examples which would me to pass a <code>data.frame</code> and it process them through cross analysis.</p>

<p>I understand not much can be done, however big massive values surely could be stripped based on the temperature? And the number of times it occurs..  </p>

<p>Since R is maths based I see no reason to move this into any other language.</p>

<p>Please note: I'm a Software Developer and have never used R before.</p>

<p>-- Edit --</p>

<p>Okay here's a real world example, seems this meter is a good example. You can see the Zeros are building up then a massive value is inserted. ""23, 65, 22, 24"" being examples of this. This happens when its in comms failure and it holds the data value and continues to add it up on the device. </p>

<p>(Just to say the comms failures are out of my hands nor can I change the software)</p>

<p>However because Zero is a valid value im wanting to remove any massive numbers against the temperatures or Zeros where its clear they are an Error.</p>

<p>The thought of detecting this and averaging the data back isn't a fix for this either, however it was discussed but since this meter data is every 30mins and comms failures can happen for days.</p>

<p>Most systems are using more Energy then this so its perhaps a bad example from a removing Zero's point of view.</p>

<p>Energy: <a href=""http://pastebin.com/gBa8y5sM"" rel=""nofollow"">http://pastebin.com/gBa8y5sM</a>
Temperatures: <a href=""http://pastie.org/4371735"" rel=""nofollow"">http://pastie.org/4371735</a></p>

<p>(Pastebin seems to have gone down for me after posting such a big file)</p>
"
"0.141350698548044","0.139482088162918"," 33712","<p>I have a question about which prediction variance to use to calculate prediction intervals from a fitted <code>lm</code> object in R. </p>

<p>For a certain multiple linear regression model I have obtained an error variance with leave-one-out-cross-validation (LOOCV) by taking the mean of the squared difference between observed and predicted values (i.e., mean squared prediction error). I am aware of some of the drawbacks of LOOCV (e.g., <a href=""http://stats.stackexchange.com/questions/2352/when-are-shaos-results-on-leave-one-out-cross-validation-applicable"">When are Shao&#39;s results on leave-one-out cross-validation applicable?</a>), but for my specific application this was the easiest (and probably the only realistically) implementable CV method. The final fitted linear model (<code>fitted_lm</code>) is fitted with all observations and with this model I would like to make predictions for new observations (<code>new_observations</code>). For this I am using the <code>predict.lm</code>  function in R.</p>

<pre><code>predict(fitted_lm, new_observations, interval = ""prediction"", pred.var = ???)
</code></pre>

<p>My questions are:  </p>

<ul>
<li>What value do I use for <code>pred.var</code> (i.e., â€œthe variance(s) for future observations to be assumed for prediction intervalsâ€) in order to obtain realistic prediction intervals for my new_observations?  </li>
<li>Do I use the error variance obtained from the LOOCV, or do I use the functionâ€™s default (i.e., â€œthe default is to assume that future observations have the same error variance as those used for fittingâ€)?  </li>
<li>Is the mean squared prediction error not appropriate in this case?</li>
</ul>

<p>Following up on Michael Chernick's answer hereunder, I had a look in the Draper &amp; Smith (1998) book  (â€œApplied regression analysis. 3rd Editionâ€). In this book <em>s<sup>2</sup></em> is defined as â€œvariance about the regressionâ€ (p 32). This is, I presume, what we describe below as the model estimate of residual variance. Furthermore, this book mentions: </p>

<blockquote>
  <p>â€œSince the actual observed value of <em>Y</em> varies about the true mean value <em>Ïƒ<sup>2</sup></em> [independent of the <em>V(Å¶)</em>], a predicted value of an individual observation will still be given with <em>Å¶</em> but will have variance</p>
  
  <p><img src=""http://i.stack.imgur.com/uUPXs.jpg"" alt=""formula""></p>
  
  <p>With corresponding estimated value obtained by inserting <em>s<sup>2</sup></em> for <em>Ïƒ<sup>2</sup></em>â€ (pp 82-81).</p>
</blockquote>

<p>Thus, as far as I understand, in the D &amp; S book they only use the model estimate of residual variance to calculate confidence intervals. This would be the default setting in the <code>predict</code> function (function help: â€œthe default is to assume that future observations have the same error variance as those used for fittingâ€). However, as fosgen states below, â€œalthough LOOCV mean squared prediction error is not equal to the real mean squared prediction error, it is much more close to real than error variance of fitted modelâ€.</p>

<p>To make this more concrete; in my dataset I get a model estimate of residual variance of <code>0.005998</code> and a LOOCV mean squared prediction error of <code>0.007293</code>. What should I then fill in as <code>pred.var</code> in the <code>predict.lm</code> function:</p>

<ul>
<li>Nothing (i.e. use the default, which would equal to the model estimate of residual variance)</li>
<li><code>0.007293</code> (i.e. the LOOCV mean squared prediction error) </li>
<li><code>0.005998 + 0.007293</code> (Michael Chernick: â€œThe model estimate of residual variance gets added to the error variance due to estimating the parameters to get the prediction error variance for a new observationâ€).</li>
</ul>
"
"0.141350698548044","0.132507983754772"," 33981","<p>I would like to use <a href=""http://en.wikipedia.org/wiki/Exponential_smoothing#Double_exponential_smoothing"">double exponential smoothing</a> to predict prevalence rates of care dependency in Austrian federal states. </p>

<p>My data is very detailed, thus I would like to make use of that in order to refine my predictions. I have the percentage of people in care dependency levels 1â€“7 aged 50â€“99 in 9 Austrian federal states.</p>

<pre><code> str(daten[1:12][daten$jahr&gt;1996,])
'data.frame':   39600 obs. of  12 variables:
 $ age       : num  50 51 52 53 54 55 56 57 58 59 ...
 $ gender    : Factor w/ 2 levels ""male"",""female"": 1 1 1 1 1 1 1 1 1 1 ...
 $ bundesland: Factor w/ 9 levels ""Bgld"",""Ktn"",""Noe"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ jahr      : num  1997 1997 1997 1997 1997 ...
 $ PfSt0     : num  0.992 0.989 0.985 0.985 0.985 ...
 $ PfSt1     : num  0.001458 0.000967 0.001459 0 0.002199 ...
 $ PfSt2     : num  0.00437 0.00193 0.00802 0.00793 0.00587 ...
 $ PfSt3     : num  0.00146 0.0058 0.00073 0.00433 0.0044 ...
 $ PfSt4     : num  0.000729 0 0.002188 0.002163 0.000733 ...
 $ PfSt5     : num  0 0.000967 0.002188 0.000721 0.002199 ...
 $ PfSt6     : num  0 0.000967 0 0 0 ...
 $ PfSt7     : num  0 0 0.00073 0 0 ...
</code></pre>

<p>DES is a time series analysis method. Time series analysis explains a data series by its past values only. While it is true that I use only past data of care dependency, one could regard age, gender and federal state as explanatory variables. Instead of computing individual double exponential smoothing forecasts for each age, gender, federal state combination, I could assume structural uniformity within these time series. Thus, my data might be regarded a multilevel panel dataset, with 50 observations per year (age groups) nested in 9 federal states each. (I plan to do separate regressions for males and females.) </p>

<p>I would like to use federal state, age and age squared as explanatory variables apart from previous value and previous trend, as done in double exponential smoothing. </p>

<p>However, in panel data analysis, time trends are typically covered by including the year variable in the regression, and rarely ever by including lags. <strong>How could I realize a forcasting method similar to double exponential smoothing in a panel dataset, i.e. including also other explanatory variables?</strong> (Preferably in R)</p>

<p>(Matters are complicated further by the fact that I have 7 instead of 1 dependent variables.)</p>
"
"0.0964281800812321","0.103803742711416"," 34080","<p>EDIT: I have solved this problem myself. The problem with the simulation below is that the omitted variable should not be included in the 'true model'. I have written a blog post with a more detailed analysis <a href=""http://diffuseprior.wordpress.com/2012/08/15/probit-models-with-endogeneity/"" rel=""nofollow"">here</a>.</p>

<p>I am trying to calculate the Average Structural Function (ASF) for a binary response regression model with an endogenous variable. The ASF is known as the policy relevant result obtained from these models because it shows how the conditional probability of the outcome (one or zero) changes in response to changes in any of the explanatory variables.</p>

<p>To estimate the regression model, I have used a two-step control function approach, wherein the first stage regression residuals ($\textbf{v}_{i}$) are included as a right-hand-side variable in the second stage probit regression Ã  la Rivers and Vuong (1988). </p>

<p>Based on my reading of a paper by Blundell and Powell (2004) (and also <a href=""http://www.cemfi.es/~arellano/binary-endogeneity.pdf"" rel=""nofollow"">these lecture notes</a>) the ASF can be calculated as follows:</p>

<p>$P(y|\bar{\textbf{X}},v)=\widehat{ASF}=\frac{1}{N}\sum^{N}_{i} \Phi(\bar{\textbf{X}}\boldsymbol{\hat{\beta}}+\rho \hat{\textbf{v}_{i}}) $</p>

<p>where the $\textbf{X}$ values are held at a constant level (say their mean), and we average over all of the first-stage residuals (multiplied by the second stage coefficient $\rho$). In effect, this formalization will allow one to calculate how the probability of the outcome varies as the one of the x-variables changes, while all of the other values are (typically) held at their means.</p>

<p>Or so you would think. However, I have attempted this calculation on a simple simulation with R and have not been able to replicate the ASF. My R code is below. Basically, this is a simple setup where we want to measure the effect of y1 on y2 (the binary outcome). There is one omitted variable (x1) that renders y1 endogenous the regression equation of interest.</p>

<p>A picture of my attempt is:</p>

<p><img src=""http://i.stack.imgur.com/OZBA8.jpg"" alt=""enter image description here""></p>

<p>When $x_1$ is available, everything should be fine. Just estimate a standard probit of $y_2$  on $x_1$ and $y_1$. The ASF for this is just the normal CDF for changes in $y_1$. When $x_1$ is not observed, it becomes necessary to instrument $y_1$. </p>

<p>From the IV regression I have calculated the ASF as in the above, and plotted this with comparisons to the model where $x_1$ is observed (the blue line in the picture), and also where $x_1$ is not observed and $y_1$ is not instrumented (the green line).</p>

<p>The red line is my attempt to construct the ASF from the method described in the above. It is clear that this line is not matching the blue line as it should. I have gone wrong somewhere here but I am not sure where. Would somebody be able to help me with this please? </p>

<pre><code>rm(list=ls())
x1 &lt;- rnorm(10000)
x2 &lt;- rnorm(10000)
y1 &lt;- 1 + 0.5*x1 + x2 + rnorm(10000)
y2 &lt;- ifelse(0.5 + 0.5*y1 - 1.5*x1 + rnorm(10000) &gt; 0, 1, 0)

# true
r1 &lt;- glm(y2~y1+x1,binomial(link=""probit""))
data &lt;- data.frame(cbind(seq(-4,6,0.2),mean(x1)))
names(data) &lt;- c(""y1"",""x1"")
asf1 &lt;- cbind(data$y1,pnorm(predict(r1,data)))
plot(asf1,type=""l"",col=""blue"",xlab=""y1"",ylab=""P(y2)"")

# no endog correction
r2 &lt;- glm(y2~y1,binomial(link=""probit""))
data &lt;- data.frame(cbind(seq(-4,6,0.2)))
names(data) &lt;- c(""y1"")
asf2 &lt;- cbind(data$y1,pnorm(predict(r2,data)))
lines(asf2,type=""l"",col=""green"")

# control function approach
v1 &lt;- (residuals(lm(y1~x2)))/sd(residuals(lm(y1~x2)))
r3 &lt;- glm(y2~y1+v1,binomial(link=""probit""))
# proceedure to get asf
asf3 &lt;- cbind(seq(-4,6,0.2),NA)
for(i in 1:dim(asf3)[1]){
    dat2 &lt;- data.frame(cbind(asf3[i,1],v1))
    names(dat2) &lt;- c(""y1"",""v1"")
    asf3[i,2] &lt;- mean(pnorm(predict(r3,dat2)))
}
lines(asf3,type=""l"",col=""red"")
</code></pre>
"
"0.078223277346843","0.0882162182782462"," 34930","<p>I have a large set of data for 37 different clinical units (all oncology) in their respective 37 hospitals. There are two specific outcome variables that I need to analyse:</p>

<p>First, drug usage for specific drugs types and classes (aggregated drugs) that are expressed as a rate â€“  DDD (Defined Daily Doses) per 100 patient days. There are individual patient drug use figures for this set.</p>

<p>Question1: Which regression approach should I take? From what I can gather I can use a Poisson regression model. IF there is overdispersion in the outcome I could resort to a negative binomial model.</p>

<p>Second: I have antibiotic resistance data that is expressed as proportion in the range 0 â€“ 1.These are not available as individual patient data points but aggregated to each of the 37 hospitals.</p>

<p>Question 2: Again, which approach? From what I have read I can use a logistic regression model. I have been advised by another statistician to initiall use a logit model and then use a probit model and compare goodness of fit for each model.</p>

<p>Does this sound like a reasonable approach? Is there a specific text that you could direct me to in order to upgrade my basic regression modelling skills. I will be using R for the analysis.</p>

<p>Thanks in advance.</p>
"
"0.104828483672192","0.103442685121077"," 35173","<p>I am currently conducting a meta-analysis in which I need to use a mixed treatment comparison method.  As I understand it, this method works in the following way:</p>

<p>Say you have a group of studies that make the following set of treatment comparisons:</p>

<ul>
<li>Intervention 1</li>
<li>Intervention 2</li>
<li>Intervention 3</li>
<li>Control</li>
</ul>

<p>You are interested in all possible comparisons between these treatments.  So, not only are you interested in intervention 1 versus control, intervention 2 versus control, and intervention 3 versus control, but also intervention 1 versus intervention 2, intervention 1 versus intervention 3, etc.  The problem occurs in that not all of the studies in your meta analysis include each intervention type.  So, while study 1 may have tested intervention 1, intervention 2, and a control group, study 2 tested intervention 2 and intervention 3 versus a control group.  And so on.  Mixed treatment comparisons (<a href=""http://www.bmj.com/content/331/7521/897?view=long&amp;pmid=16223826"" rel=""nofollow"">Caldwell, Ades, &amp; Higgins, 2005</a>; <a href=""http://www.ncbi.nlm.nih.gov/pubmed/15449338"" rel=""nofollow"">Lu &amp; Ades, 2004</a>; <a href=""http://www.dovepress.com/multiple-treatment-comparison-meta-analyses-a-step-forward-into-comple-peer-reviewed-article-CLEP"" rel=""nofollow"">Mills et al., 2011</a>) arose as a way of using the indirect information from your sample of studies to estimate the magnitude of the missing comparisons.</p>

<p>For my study, I am interested in how several different moderators affect the magnitude of the various treatment comparisons.  I stumbled across a paper (<a href=""http://www.ncbi.nlm.nih.gov/pubmed/16900557"" rel=""nofollow"">Nixon, Bansback, &amp; Brennan, 2007</a>) that combines the mixed treatment comparison method with meta-regression.  My problem is finding a good software implementation for this method (preferably an implementation in R, since I'm most familiar with R).  As far as I can tell, the <a href=""http://www.metafor-project.org/"" rel=""nofollow"">metafor</a> package isn't able to handle mixed treatment comparisons.  Does anybody know whether there's a package out there that's able to handle both mixed treatment comparisons and meta-regression?</p>
"
"0.0999500374687773","0.0887658573546531"," 35407","<p>I'm currently running a perception experiment: </p>

<ul>
<li>DV: error (this in degrees- how much an observer was away from the real answer) </li>
<li>IV: the time bin (5 levels) in which the unique stimulus appeared</li>
<li>IV2: True/False- whether the unique stimulus occurred before or after stimulus2. </li>
<li>Covariate: Distance from the fixation point this stimulus appeared (continuous)</li>
</ul>

<p>So, I have decided to use repeated measures ANCOVA since, all the observers were exposed to all 5 levels of the IV multiple times. </p>

<p>Currently I have written: </p>

<pre><code>data.aov &lt;-aov(error~(timebin*after*distance) + 
               Error(subject/timebin*after*distance), d)
summary(data.aov)
</code></pre>

<p><strong>Is this the correct way to specify my repeated measures ANCOVA in R?</strong></p>

<p>Also, I wanted to run this analysis in SPSS to check that I get the same results. However, SPSS doesn't like the format of my data. 
For repeated measures analysis, the 5 bins should be in 5 different columns, but in my data file, they are all together under the same heading <code>timebins</code>.</p>

<p><strong>How can I run the repeated measures ANCOVA in SPSS if my data are in long format?</strong></p>

<hr>

<p>Thanks everyone for great answers! </p>

<p>@Marcus, just one thing- regarding regression. 
If I were to use regression, would I be looking at trends? 
I am actually more interested in comparing bin 1 and bin 5. This is why I was going to use a t-test, but now it seems like I am using within-subjects anova. </p>

<p>Also, I've had a look at distance and (as you said) might not be a covariate, since it was randomised for every condition! </p>

<p>Then the codes should look like this? </p>

<pre><code>data.aov &lt;-aov(error~(timebin*after) + Error(subject/timebin*after), d)
summary(data.aov)
</code></pre>

<p>But what is the difference between using <code>*</code> and using <code>+</code> ? 
Should I use <code>*</code> to get an interaction effect? </p>
"
"0.12721562869324","0.139482088162918"," 35489","<p>I have real daily market data which I'm looking at to create a model for forecasting. The model that I created (below) used autoregressive terms within a linear regression.</p>

<p>I was sharing this with a colleague and he said ""autoregressive variables are correlated with the other variables in multiple linear setting which creates multicollinarity problem, creating unreliable result.""</p>

<p>So I'm turning to the group for help. Here is the data and the analysis that I performed in R.</p>

<pre><code>#Read in Data
MarketData = read.table('http://sharp-waterfall-3397.herokuapp.com/MarketCategories6.txt', header=TRUE,na.strings = ""NA"", sep="","")
MarketData$Month &lt;- as.factor(MarketData$Month)
MarketData$Weekday &lt;- as.factor(MarketData$Weekday)

str(MarketData)
</code></pre>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/PERregress/index.html"" rel=""nofollow"">PERregress</a> library to help with the autoregression using the <code>back()</code> function and to help with the residual diagnostics:</p>

<pre><code>library(PERregress)
descStat(MarketData)
</code></pre>

<p>Subsetting the data for model building and prediction purposes:</p>

<pre><code>Total = MarketData
MarketData = MarketData[1:268,]
attach(MarketData)
</code></pre>

<p>Here is a regression with everything that I can think of. Note you can have higher autoregressive terms but this will start to mask events since R will ignore the first several rows. Also just an FYI for some reason the residual analysis is breaking which I liked to look for points with undue leverage.</p>

<pre><code>#Market1Category1 Regression for the markets with everything that I can think of it
Market1Category1Output=lm(Market1Category1 ~ Trend+Month2+Month3+Month4+
                          Month5+Month6+Month7+Month8+Month9+Monday+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday2+Holiday3+Holiday4+
                          Event1+Event2+Event3+Event4+Event5+Event6+Event7+
                          Event8+Event9+Event10+Event11+Event12+Event13+
                          Event14+Event15+Event16+Event17+Event18+Event19+
                          Event20+Event21+Event22+Event23+Event24+Event25+
                          Event26+Event27+Event28+
                          back(Market1Category1)+back(Market1Category1, 2))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is the final equation. I'd like to say that I reduced the variables using partial f-test but I couldn't find an easy way to do this so if you know a function please let me know. Basically I looked at the change in adjusted $R^2$.</p>

<pre><code>#Final regression equation 
Market1Category1Output=lm(Market1Category1 ~ Month5+Month6+Month7+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday3+Event2+Event7+Event10+
                          Event13+Event16+Event25+Event28+
                          back(Market1Category1)+back(Market1Category1, 6))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is a plot of the actuals in green vs the predictions in blue but there's a problem:</p>

<pre><code>plot(Time, Market1Category1, col='green')
points(Time, predict(Market1Category1Output, MarketData), col='blue', pch=20)
</code></pre>

<p>The issue is that predict will use the data values instead of it's predicted values for the autoregressive terms. In order to make it use predicted terms I created this loop. If you know a better way let me know.</p>

<pre><code>dataSet2 &lt;- Total
dataSet2[8:length(dataSet2$Time),""Market1Category1""] &lt;- NA
    for (i in (1:(length(dataSet2$Time)-7))) {
  dataSet2[6+i+1,""Market1Category1""] &lt;- 1
  dataSet2[6+i+1,""Market1Category1""] &lt;- predict(Market1Category1Output, 
                                                dataSet2[0:6+i+1,])[6+1] 
}
</code></pre>

<p>Here is the plot again with the results in blue using the predicted results for the autoregressive terms (with the exception of the first 7 since the model needs those to <code>predict</code>):</p>

<pre><code>plot(Total$Time, Total$Market1Category1, col='green')
points(dataSet2$Time, dataSet2$Market1Category1, col='blue', pch=20)
</code></pre>

<p>So here are my questions in order of importance:</p>

<ol>
<li>Does using autoregressive and linear terms violate any fundamental assumptions?</li>
<li>What issues can this cause and what analysis/steps should I do take to avoid these problems?</li>
<li>Is there a better approach to modeling this timeseries?</li>
<li>Is there a more efficient approach?</li>
<li>Given the residuals what steps would you take?</li>
</ol>

<p>Finally two questions which is just causing me more work than possibly necessary:</p>

<ol>
<li>As you can see instead of using the factors for weekday and month I'm using separate conditional variables. I'm doing this because if I use the factor and a level turns out to be insignificant (e.g., Monday for days of the week). I can't remove it. Perhaps there's a way?</li>
<li>Is there a quick way to run a partial F-statistic to understand whether removing a variable makes sense?</li>
</ol>
"
"0.101367839312414","0.100027789357213"," 35590","<p>I was unable to figure out how to perform linear regression in R in for a repeated measure design. In a <a href=""http://stackoverflow.com/questions/12182373/plot-of-a-linear-regression-with-interactions"">previous question</a> (still unanswered) it was suggested to me to not use <code>lm</code> but rather to use mixed models. I used <code>lm</code> in the following way:  </p>

<pre><code>lm.velocity_vs_Velocity_response &lt;- lm(Velocity_response~Velocity*Subject, data=mydata)
</code></pre>

<p>(more details on the dataset can be found at the link above)</p>

<p>However I was not able to find on the internet any example with R code showing how to perform a linear regression analysis.</p>

<p>What I want is on one hand a plot of the data with the line fitting the data, and on the other hand the $R^2$ value along with the p-value for the test of significance for the model.</p>

<p>Is there anyone who can provide some suggestions? Any R code example could be of great help.</p>

<hr>

<p><strong>Edit</strong><br>
According to the suggestion I received so far, the solution to my analyze my data in order to understand if there is a linear relation between the two variables Velocity_response (deriving from the questionnaire) and Velocity (deriving from the performance) should be this:</p>

<pre><code>library(nlme)
summary(lme(Velocity_response ~ Velocity*Subject, data=scrd, random= ~1|Subject))
</code></pre>

<p>The result of summary gives this:</p>

<pre><code>    &gt; summary(lme(Velocity_response ~ Velocity*Subject, data=scrd, random= ~1|Subject))
    Linear mixed-effects model fit by REML
     Data: scrd 
           AIC      BIC   logLik
      104.2542 126.1603 -30.1271

    Random effects:
     Formula: ~1 | Subject
            (Intercept) Residual
    StdDev:    2.833804 2.125353

Fixed effects: Velocity_response ~ Velocity * Subject 
                              Value Std.Error DF    t-value p-value
(Intercept)               -26.99558  25.82249 20 -1.0454288  0.3083
Velocity                   24.52675  19.28159 20  1.2720292  0.2180
SubjectSubject10           21.69377  27.18904  0  0.7978865     NaN
SubjectSubject11           11.31468  33.51749  0  0.3375754     NaN
SubjectSubject13           52.45966  53.96342  0  0.9721337     NaN
SubjectSubject2           -14.90571  34.16940  0 -0.4362299     NaN
SubjectSubject3            26.65853  29.41574  0  0.9062674     NaN
SubjectSubject6            37.28252  50.06033  0  0.7447517     NaN
SubjectSubject7            12.66581  26.58159  0  0.4764880     NaN
SubjectSubject8            14.28029  31.88142  0  0.4479188     NaN
SubjectSubject9             5.65504  34.54357  0  0.1637076     NaN
Velocity:SubjectSubject10 -11.89464  21.07070 20 -0.5645111  0.5787
Velocity:SubjectSubject11  -5.22544  27.68192 20 -0.1887672  0.8522
Velocity:SubjectSubject13 -41.06777  44.43318 20 -0.9242591  0.3664
Velocity:SubjectSubject2   11.53397  25.41780 20  0.4537754  0.6549
Velocity:SubjectSubject3  -19.47392  23.26966 20 -0.8368804  0.4125
Velocity:SubjectSubject6  -29.60138  41.47500 20 -0.7137162  0.4836
Velocity:SubjectSubject7   -6.85539  19.92271 20 -0.3440992  0.7344
Velocity:SubjectSubject8  -12.51390  22.54724 20 -0.5550080  0.5850
Velocity:SubjectSubject9   -2.22888  27.49938 20 -0.0810519  0.9362
 Correlation: 
                          (Intr) Velcty SbjS10 SbjS11 SbjS13 SbjcS2 SbjcS3 SbjcS6 SbjcS7 SbjcS8 SbjcS9 V:SS10 V:SS11 V:SS13 Vl:SS2 Vl:SS3
Velocity                  -0.993                                                                                                         
SubjectSubject10          -0.950  0.943                                                                                                  
SubjectSubject11          -0.770  0.765  0.732                                                                                           
SubjectSubject13          -0.479  0.475  0.454  0.369                                                                                    
SubjectSubject2           -0.756  0.751  0.718  0.582  0.362                                                                             
SubjectSubject3           -0.878  0.872  0.834  0.676  0.420  0.663                                                                      
SubjectSubject6           -0.516  0.512  0.490  0.397  0.247  0.390  0.453                                                               
SubjectSubject7           -0.971  0.965  0.923  0.748  0.465  0.734  0.853  0.501                                                        
SubjectSubject8           -0.810  0.804  0.769  0.624  0.388  0.612  0.711  0.418  0.787                                                 
SubjectSubject9           -0.748  0.742  0.710  0.576  0.358  0.565  0.656  0.386  0.726  0.605                                          
Velocity:SubjectSubject10  0.909 -0.915 -0.981 -0.700 -0.435 -0.687 -0.798 -0.469 -0.883 -0.736 -0.679                                   
Velocity:SubjectSubject11  0.692 -0.697 -0.657 -0.986 -0.331 -0.523 -0.607 -0.357 -0.672 -0.560 -0.517  0.637                            
Velocity:SubjectSubject13  0.431 -0.434 -0.409 -0.332 -0.996 -0.326 -0.378 -0.222 -0.419 -0.349 -0.322  0.397  0.302                     
Velocity:SubjectSubject2   0.753 -0.759 -0.715 -0.580 -0.360 -0.992 -0.661 -0.389 -0.732 -0.610 -0.563  0.694  0.528  0.329              
Velocity:SubjectSubject3   0.823 -0.829 -0.782 -0.634 -0.394 -0.622 -0.984 -0.424 -0.799 -0.667 -0.615  0.758  0.577  0.360  0.629       
Velocity:SubjectSubject6   0.462 -0.465 -0.438 -0.356 -0.221 -0.349 -0.405 -0.995 -0.449 -0.374 -0.345  0.425  0.324  0.202  0.353  0.385
Velocity:SubjectSubject7   0.961 -0.968 -0.913 -0.740 -0.460 -0.726 -0.844 -0.496 -0.986 -0.778 -0.718  0.886  0.674  0.420  0.734  0.802
Velocity:SubjectSubject8   0.849 -0.855 -0.807 -0.654 -0.406 -0.642 -0.746 -0.438 -0.825 -0.988 -0.635  0.783  0.596  0.371  0.649  0.709
Velocity:SubjectSubject9   0.696 -0.701 -0.661 -0.536 -0.333 -0.526 -0.611 -0.359 -0.676 -0.564 -0.990  0.642  0.488  0.304  0.532  0.581
                          Vl:SS6 Vl:SS7 Vl:SS8
Velocity                                      
SubjectSubject10                              
SubjectSubject11                              
SubjectSubject13                              
SubjectSubject2                               
SubjectSubject3                               
SubjectSubject6                               
SubjectSubject7                               
SubjectSubject8                               
SubjectSubject9                               
Velocity:SubjectSubject10                     
Velocity:SubjectSubject11                     
Velocity:SubjectSubject13                     
Velocity:SubjectSubject2                      
Velocity:SubjectSubject3                      
Velocity:SubjectSubject6                      
Velocity:SubjectSubject7   0.450              
Velocity:SubjectSubject8   0.398  0.828       
Velocity:SubjectSubject9   0.326  0.679  0.600

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-1.47194581 -0.46509026 -0.05537193  0.39069634  1.89436646 

Number of Observations: 40
Number of Groups: 10 
Warning message:
In pt(q, df, lower.tail, log.p) : NaNs produced
&gt; 
</code></pre>

<p>Now, I do not understand where I can get the R^2 and the corresponding p-values indicating me wether there is a linear relationship between the two variables or not,
nor I have understood how my data can be plotted with the line fitting the regression.</p>

<p>Can anyone be so kind to enlighten me? I really need your help guys...</p>
"
"0.11852616398269","0.124756572310361"," 35940","<p>This question is in response to an answer given by @Greg Snow in regards to a <a href=""http://stats.stackexchange.com/questions/35918/logistic-regression-the-standard-deviation-used-in-glmpower"">question</a> I asked concerning power analysis with logistic regression and SAS <code>Proc GLMPOWER</code>.</p>

<p>If I am designing an experiment and will analze the results in a factorial logistic regression, how can I use <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression/22410#22410"">simulation</a> ( and <a href=""http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html"">here</a> ) to conduct a power analysis?</p>

<p>Here is a simple example where there are two variables, the first takes on three possible values {0.03, 0.06, 0.09} and the second is a dummy indicator {0,1}. For each we estimate the response rate for each combination (# of responders / number of people marketed to). Further, we wish to have 3 times as many of the first combination of factors as the others (which can be considered equal) because this first combination is our tried and true version. This is a setup like given in the SAS course mentioned in the linked question.</p>

<p><img src=""http://i.stack.imgur.com/4LIvh.jpg"" alt=""enter image description here""></p>

<p>The model that will be used to analyze the results will be a logistic regression, with main effects and interaction (response is 0 or 1). </p>

<pre><code>mod &lt;- glm(response ~ Var1 + Var2 + I(Var1*Var2))
</code></pre>

<p>How can I simulate a data set to use with this model to conduct a power analysis?</p>

<p>When I run this through SAS <code>Proc GLMPOWER</code> (using <code>STDDEV =0.05486016</code>
 which corresponds to <code>sqrt(p(1-p))</code> where p is the weighted average of the shown response rates):  </p>

<pre class=""lang-sas prettyprint-override""><code>data exemplar;
  input Var1 $ Var2 $ response weight;
  datalines;
    3 0 0.0025  3
    3 1 0.00395 1
    6 0 0.003   1
    6 1 0.0042  1
    9 0 0.0035  1
    9 1 0.002   1;
run;

proc glmpower data=exemplar;
  weight weight;
  class Var1 Var2;
  model response = Var1 | Var2;
  power
    power=0.8
    ntotal=.
    stddev=0.05486016;
run;
</code></pre>

<p>Note: <code>GLMPOWER</code> only will use class (nominal) variables so 3, 6, 9 above are treated as characters and could have been low, mid and high or any other three strings. When the real analysis is conducted, Var1 will be used a numeric (and we will include a polynomial term Var1*Var1) to account for any curvature.</p>

<p>The output from SAS is </p>

<p><img src=""http://i.stack.imgur.com/T44tM.png"" alt=""enter image description here""></p>

<p>So we see that we need 762,112 as our sample size (Var2 main effect is the hardest to estimate) with power equal to 0.80 and alpha equal to 0.05. We would allocate these so that 3 times as many were the baseline combination (i.e. 0.375 * 762112) and the remainder just fall equally into the other 5 combinations.</p>
"
"0.0632139541241014","0.0623782861551805"," 37466","<p>I am taking a graduate course in Applied Statistics that uses the following textbook (to give you a feel for the level of the material being covered): <a href=""http://amzn.com/0471072044"">Statistical Concepts and Methods</a>, by G. K. Bhattacharyya and R. A. Johnson.</p>

<p>The Professor requires us to use SAS for the homeworks. </p>

<p>My question is that: is there a Java library(ies), that can be used instead of SAS for problems typically seen in such classes.</p>

<p>I am currently trying to make do with <a href=""http://commons.apache.org/math/"">Apache Math Commons</a> and though I am impressed with the library (it's ease of use and understandability) it seems to lack even simple things such as the ability to draw histograms (thinking of combining it with a charting library).</p>

<p>I have looked at Colt, but my initial interest died down pretty quickly. </p>

<p>Would appreciate any input -- and I've looked at similar questions on Stackoverflow but have not found anything compelling.</p>

<p>NOTE: I am aware of R, SciPy and Octave and java libraries that make calls to them -- I am looking for a Java native library or set of libraries that can together provide the features I'm looking for.</p>

<p>NOTE: The topics covered in such a class typically include: one-samle and two-sample tests and confidence intervals for means and medians, descriptive statistics, goodness-of-fit tests, one- and two-way ANOVA, simultaneous inference, testing variances, regression analysis, and categorical data analysis.</p>
"
"0.0547448901451359","0.0540211804549215"," 37714","<p>I would like to make a prediction for a (new) subject to have a certain outcome given the historical data and the model:</p>

<pre><code>glm(outcome ~ age + treatment + history, family=binomial, ...) 
</code></pre>

<p>however in the historical data that will be fitted by the model, I have some sort of repeated measurements on some of the subjects (and I don't know if repeated measures is the appropriate term to be used here, hence using lmer etc is doubtful); example:<br></p>

<pre><code>subject_ID    age    treatment    history    outcome
S_1           33      T_1         H_1        0
S_2           27      T_2         H_2        1
S_2           27      T_3         H_2        1
S_3           56      T_1         H_11       0
etc...
</code></pre>

<p>In this example subject_2 (S_2) has two rows because he had simultaneously two different treatments at the same time. could a logistic regression still be used or should cases like subject_2 be removed from the analysis?</p>
"
"0.0836242010007091","0.0825187161885156"," 37785","<p>I come from an SPSS background and am attempting to move to <code>R</code> for it's superior flexibility and data manipulation abilities. I have some concerns however as to whether the <code>lm()</code> is really using partial correlations.</p>

<p>I'm basically trying to run a linear regression, using something similar to the ""enter"" setting in SPSS, which essentially builds the model one variable at a time, reporting the change in $R^2$ with each additional variable. This allows you to determine how much predictive power each variable adds to the model.</p>

<p>When I run the same analysis in <code>R</code> however, I don't get any information on the $R^2$ contributed by individual variables, and I'm not even sure that it's using partial corrrelations to calculate the p-values it's reporting!</p>

<p>My code follows:</p>

<pre><code>summary(m1 &lt;- lm(totalprop ~ cos(Angle) + Alignment + colour + 
  Angle*Alignment, dataset))
</code></pre>

<p>My questions:</p>

<ol>
<li>Does R use partial correlations to determine reported p-values from <code>lm()</code>?</li>
<li>How can I make <code>R</code> report change in $R^2$ with each additional variable.</li>
<li>How can I make <code>R</code> act like SPSS in calulating the model piece by piece? Is this possible without running multiple iterations of the lm() function? If not, how does one control for the effects of covariates in R?</li>
</ol>
"
"0.0774209661138764","0.0763974860547543"," 39000","<p>I assessed the internal reliability of a self-created scale with eight items ($N = 150$) by calculating Cronbachâ€™s $\alpha$. It appears that one item correlates low with the overall score of the scale (item 4 in the example below). The corrected item-total correlation, i.e. the correlation of this item with the scale total excluding that item, is only $r= .046$. </p>

<pre><code>library(psych)
scale&lt;-mydata[,c(24,25,26,27,28,29,30,31)]
alpha(scale)

Reliability analysis   
Call: alpha(x = scale)

          0.62      0.64    0.66      0.18  4.3 0.79

 Reliability if an item is dropped:
      raw_alpha std.alpha G6(smc) average_r
item1      0.56      0.59    0.62      0.17
item2      0.53      0.57    0.58      0.16
item3      0.54      0.56    0.58      0.16
item4      0.66      0.67    0.67      0.23
item5      0.60      0.62    0.63      0.19
item6      0.55      0.59    0.62      0.17
item7      0.58      0.61    0.63      0.18
item8      0.63      0.65    0.67      0.21

 Item statistics 
        n    r r.cor r.drop mean   sd
item1 144 0.60  0.51  0.395  4.5 0.71
item2 145 0.65  0.62  0.499  4.6 0.71
item3 142 0.67  0.64  0.484  4.5 0.72
item4 146 0.33  0.15  0.046  4.6 0.81
item5 147 0.51  0.41  0.298  4.9 0.41
item6 139 0.59  0.50  0.404  4.4 0.82
item7 136 0.53  0.43  0.339  4.2 1.03
item8 135 0.39  0.21  0.190  4.3 0.94

Non missing response frequency for each item
         1    2    3    4    5 miss
item1 0.01 0.01 0.04 0.34 0.60 0.04
item2 0.01 0.01 0.03 0.24 0.71 0.03
item3 0.00 0.01 0.11 0.28 0.60 0.05
item4 0.01 0.03 0.05 0.14 0.77 0.03
item5 0.00 0.00 0.02 0.11 0.87 0.02
item6 0.01 0.02 0.10 0.29 0.58 0.07
item7 0.04 0.02 0.15 0.25 0.54 0.09
item8 0.02 0.03 0.11 0.32 0.52 0.10
</code></pre>

<p><strong>PROBLEM</strong>: I would like to report this low correlation with the degrees of freedom in parentheses and the significance level in the main text. Yet, I am not sure whether I calculated the correct p-value. What I did is a simple regression with item 4 as the dependent variable:</p>

<pre><code>scale &lt;- as.data.frame(scale)
summary(lm(item4 ~ item1+item2+item3+item5+item6+item7+item8, data=scale))

Call:
lm(formula = item4 ~ item1 + item2 + item3 + item5 + item6 + 
    item7 + item8, data = scale)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.4256 -0.0465  0.2869  0.3500  1.3405 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.65098    1.00288   1.646 0.102492    
item1        0.12916    0.11560   1.117 0.266262    
item2        0.02387    0.12921   0.185 0.853760    
item3       -0.07323    0.12718  -0.576 0.565921    
item5        0.64204    0.18636   3.445 0.000802 ***
item6       -0.04596    0.10230  -0.449 0.654120    
item7       -0.13217    0.08030  -1.646 0.102545    
item8        0.05609    0.08758   0.641 0.523136    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.8235 on 113 degrees of freedom
  (29 observations deleted due to missingness)
Multiple R-squared: 0.1385,     Adjusted R-squared: 0.08518 
F-statistic: 2.596 on 7 and 113 DF,  p-value: 0.01604
</code></pre>

<p><strong>QUESTION:</strong> Is it correct if I report something like ""Item 4 correlates only weakly with the overall score of the scale $(r(113)= .046, p= .02)$"" - or did I make a rather large error in reasoning here? </p>
"
"0.0364965934300906","0.0540211804549215"," 40667","<p>Currently, I am working on a count data set measuring events in a third-order administrative unit. The frequency of events varies for each third-order administrative unit to account for the variability, I am using a negative binomial regression model. </p>

<p>A crude graphical analysis of the residual of the regression indicates that no spatial autocorrelation exists. I would like to find a method to assess spatial autocorrelation in a negative binomial regression model, since a regular test of spatial autocorrelation are not feasible (ex. Global Moran's I). </p>

<p>Do you have any suggestions?</p>
"
"0.197385508487931","0.194776136091312"," 41362","<p>I have no formal training in statistics so please correct me if I use the wrong terms as I try to explain my problem.</p>

<p>I have a set of data (less than 80 points) with essentially 1 single outcome (a float we will call <code>dcl</code>) that can potentially depends on 10 of other variables, most of them floats maybe one or two boolean.</p>

<p>While I might ask some multi-variate regression question later, let's start with something simple. </p>

<p>Historically, people in my field have focused on the strongest correlation between <code>dcl</code> and variable <code>J</code> and some of my data suggests some other dependence on a float <code>Re</code> which is I'm sure at least weakly correlated with 'J' as they share some variables in their respective expressions. So my first question is how do I test the correlation and/or the independence of 'Re' and 'J' on the outcome 'dlc'? Intuitively and physically, I expect 'dlc' to depend strongly on 'J' and weakly on 'Re'. How do I prove this with a statistical analysis?</p>

<p>Here are a few graphs to illustrate the data:</p>

<p><img src=""http://i.stack.imgur.com/dVRMa.png"" alt=""Re vs J"">   </p>

<p><img src=""http://i.stack.imgur.com/hNPvb.png"" alt=""dcl vs J""></p>

<p><img src=""http://i.stack.imgur.com/D3YJe.png"" alt=""dcl vs Re""></p>

<p>Final point, in terms of software, I have python and R installed, I'm fairly proficient in python but I just installed R and know pretty much nothing about it.</p>

<p>EDIT 1: </p>

<p>Following gung's suggestion, I ran my dataset through R:</p>

<pre><code>Call:
lm(formula = dcl ~ J + I(J^2) + Re + I(Re^2), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.0078 -3.7930 -0.4458  2.0869 21.2538 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.648e+01  1.232e+00  13.380  &lt; 2e-16 ***
J           -2.662e+00  3.773e-01  -7.054 6.58e-10 ***
I(J^2)       1.096e-01  2.071e-02   5.293 1.10e-06 ***
Re           1.966e-06  1.621e-05   0.121    0.904    
I(Re^2)      2.191e-11  3.441e-11   0.637    0.526    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 5.369 on 77 degrees of freedom
Multiple R-squared: 0.4831, Adjusted R-squared: 0.4562 
F-statistic: 17.99 on 4 and 77 DF,  p-value: 1.818e-10
</code></pre>

<p>So now I need some help to decipher this (but I will look into R documentation too). I don't know if it's relevant but on physical grounds only, it's probable the dependency in J is $dcl \sim \frac{1}{\sqrt{J}}$. Can I put this directly into the model? Does that already tell me something about the dependency on $J$ vs $Re$?</p>

<pre><code>Call:
lm(formula = dcl ~ J + I(J^(-0.5)) + Re + I(Re^(-0.5)), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8119 -3.0097 -0.8504  1.8506 12.1439 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   8.175e-01  1.634e+00   0.500   0.6184    
J            -2.946e-01  1.258e-01  -2.343   0.0217 *  
I(J^(-0.5))   4.516e+00  7.053e-01   6.403 1.09e-08 ***
Re            3.332e-05  6.684e-06   4.985 3.72e-06 ***
I(Re^(-0.5))  6.009e+02  1.354e+02   4.438 2.98e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.426 on 77 degrees of freedom
Multiple R-squared: 0.6487, Adjusted R-squared: 0.6305 
F-statistic: 35.55 on 4 and 77 DF,  p-value: &lt; 2.2e-16

&gt; model = lm(dcl ~ J+I(J^(-0.5)) + Re+I(Re^(-0.5)), data=df)
&gt; summary(model)
</code></pre>

<p><strong>EDIT 2</strong>: OK I'm starting to understand things better. Also, again based on physical grounds, I would think that the dependency is more something like $dcl ~ \frac{1}{\sqrt{J}} Re^{n}$ with possibly other variables in that product that I ignore. So when I enter such model in R (can I still use <code>lm</code> for something non-linear?), here is what I get:</p>

<pre><code>Call:
lm(formula = dcl ~ I(J^(-0.5)) * I(Re^(-0.1)), data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.5363  -3.0192  -0.2556   1.4373  17.1494 

Coefficients:
                         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               -43.220      9.164  -4.716 1.03e-05 ***
I(J^(-0.5))                63.813     11.088   5.755 1.62e-07 ***
I(Re^(-0.1))              124.245     24.038   5.169 1.77e-06 ***
I(J^(-0.5)):I(Re^(-0.1)) -142.744     27.269  -5.235 1.36e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.668 on 78 degrees of freedom
Multiple R-squared: 0.6042, Adjusted R-squared: 0.5889 
F-statistic: 39.68 on 3 and 78 DF,  p-value: 1.122e-15
</code></pre>

<p>Does that 4th line tell me something about the dependence between $J$ and $Re$? What kind of tools could I use to get an estimate on the exponent on Re? Because right now I'm just trying a few different numbers empirically to see how the errors evolve. Next for me is to plot the dcl vs the new model and see how well the data collapses visually...</p>

<p>EDIT 3:</p>

<p>In the end, I used <code>nls</code> to explore the possible exponents of my fit. I also removed some outliers in my data that used a different experimental method. I settled on a fit that gave me decent Pr(>|t|) and residuals and which visually produce a decent fit. The last outlier is actually another experiment with a different setup, but one that I trust. So in a sense it's good that it shows up as an outlier as it hints at other parameters that need to be explored. Thank you gung, I accept your answer as I believe it guided me in the right direction.</p>

<pre><code>&gt; model = nls(L.D ~ C*I(J^(c1))*I(Re_s^(c2)), start=list(C=10,c1=-0.25,c2=-0.25),data=df)
&gt; summary(model)
Formula: L.D ~ C * I(J^(c1)) * I(Re_s^(c2))
Parameters:
   Estimate Std. Error t value Pr(&gt;|t|)    
C  57.20389   26.40011   2.167   0.0337 *  
c1 -0.27721    0.05901  -4.698 1.27e-05 ***
c2 -0.16424    0.04936  -3.327   0.0014 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
Residual standard error: 4 on 70 degrees of freedom
Number of iterations to convergence: 8 
Achieved convergence tolerance: 2.91e-06 
</code></pre>

<p><img src=""http://i.stack.imgur.com/dfgWX.png"" alt=""enter image description here""></p>
"
"0.173208685553724","0.170918922914102"," 43040","<p>I need some guidance related to regression model verification using validation data. 
I am new to R-tool &amp; statistics and trying my best to learn. I did search on internet too but I couldn't get a final answer to my questions. 
Actually I have a lot of questions, I may try my best to explain the problems:
I am experimenting with network packets and R-tool.
I have captured some packets from a network using a custom made packet sniffer in java. The sniffer will capture some packets and save the information of packet header like: tcp window size, tcp sequence numbers, date-time, ip header length, ip time to live etc... in a csv file.</p>

<p>Also the sniffer will add category number to each csv file so that we can know which packet belongs to which category. I created 9 different categories saved in 9 different csv files.
Now I extracted 1000 observation from each of the csv files and created a data set named ""alldata"".</p>

<p>Then I created training data set and validation data set from ""alldata"" data set.</p>

<p>Now I want to perform linear regression, logistic regression, decision tree analysis, cluster analysis etc on this ""alldata"" data set.</p>

<p>So my plan was to use training data set to create models and then later use validation data set to verify my models. </p>

<p>Category will be my target variable in any case. I want to predict the category from other independent variables.</p>

<ol>
<li><p>My first confusion is that after I created scatter plot of category with other independent variables and I don't see any linear relationship between them. Moreover I even don't know what relation exists between category and independent variables. From scatter plots it seems to me that there is no specific relation between category and other independent variables(except date_time it is bit linear to category). Am I doing the correct interpretation ?
Here are some of the plots:
<a href=""http://imageshack.us/photo/my-images/211/tcpdport.png/"" rel=""nofollow"">plot 1</a>
<a href=""http://imageshack.us/photo/my-images/547/tcpchksum.png/"" rel=""nofollow"">plot 2</a></p></li>
<li><p>I think doing linear regression won't make any sense now after having a look at scatter plots. Is this correct assumption?</p></li>
<li><p>Although I tried to do make some regression models with training data set, but the R-square values for all the models is quite low (for example like 0.00019, 0.0035, 0.018 etc. ) 
So can I assume that these models are not good due to very low r-square vales?</p></li>
<li><p>As logistic regression is used when we have target variables having only two values 1 or 0, or some probabilities between 0.0 - 1.0.
This means performing logistic regression is not possible for this type of data set.
Is my assumption true?</p></li>
<li><p>My main question was how to verify a model created with training data set by using validation data set?
Please let me know the commands and the procedure.
Please let me know if I am doing this in wrong way or if you can suggest me a better way to do this whole work. I think if someone could please clear my doubts then I may ask further more questions.</p></li>
</ol>

<p>If you don't understand my problem we can discuss in more detail
I look forward for your replies.
Thank you!</p>

<hr>

<p>@Wayne</p>

<p>Hello thanks for the reply, but the thing is for each category I have almost same range of values of independent variables like(tcpheader, ipttl, iplen). For example iptype is only having two values 6 and 17. So most of the categories are having iptype value of 6 &amp; 17.
So it is also same is for tcpheader, tcp sequence number, tcp acknowledgement number etc. I don't think there is any way to distinguish a particular packet based on these independent variables. Only the independent variable that can be helpful is time.
But when I created a model with time it had good r-squared value but the regression line equation doesn't predict category with any value of date_time.
I don't understand this behaviour.</p>

<p>Thanks.</p>
"
"0.0893980312535348","0.0882162182782462"," 43675","<p>I am working on a housing problem in which I use dichotomous and ratio data to predict
housing production (units constructed in a year-ratio) in a 17 year time period. At this time, I am using OLS and as I get better at stats, I shall attempt this problem using time-series analysis.  That said, I have used R to standardize all of my ratio predicting data and left the dichotomous data raw.  And I have also transformed the response variable to a Natural log to normalize the distribution (i.e. many, many zeros>>yes, I know Poisson or Zero-populated counts in the future).</p>

<p>I have read the post on ""interpret coefficients from a quantile regression on standardized data"" and also the ""convert my unstandardized independent variables to standardized."" Based on those, I think that can do the following interpretation based on the following output. The variable <code>region_id</code> is dichotomous, <code>supply</code> is standardized.</p>

<pre><code>Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          2.687e+00  2.171e-01  12.379  &lt; 2e-16 ***

region_id            1.805e+00  1.383e-01  13.049  &lt; 2e-16 ***

supply              -2.205e+01  2.204e+00 -10.005  &lt; 2e-16 ***
</code></pre>

<p>Region Interpretation:<br>
For every on city that is located in the Houston region, you can expect that annual housing production will increase by 1.8%.  </p>

<p>Supply Interpretation:<br>
For every one-unit increase in the standard deviation of housing supply, you can expect that annual housing production will decrease by -22.05%.</p>

<p>Nota bene.<br>
I am not a stats or math person at all,
but I have been using R for the past three years
and I am quite familiar with OLS, but if you throw
up an equation it will look ""appropriately"" Greek to me. :)</p>
"
"0.0632139541241014","0.0623782861551805"," 44227","<p>I am a newbie in data mining world. I have a general question.
I have a data set which has 10 independent variables and one target variable named as category which has 9 values like: 1, 2, 3, 4, 5, 6, 7, 8, 9.
the 10 independent variables have different kind of range of values. some of them have values between 0 - 5000, some have big range like 5,000,000 - 100,000,000 etc.</p>

<p>Moreover there is no specific relation  (linear etc.) existing between target and independent variables.</p>

<p>I am basically trying to predict the target variable category by using all of these independent variables.</p>

<p>Can someone suggest what should be my approach? I am very confused. Should I use regression models, decision trees or cluster analysis?</p>
"
"0.078223277346843","0.0771891909934654"," 44895","<p>I am doing a regression analysis which troubled me. </p>

<p>My independent variable are 4 interplanetary condition components, and the dependent variable is the latitude of auroral oval boundary. 
So far, the specific relationship is still unknown in physical principle, what we want to do is to get a model (function expression) from the massive data which shows how these independent variables affect the dependent variable.</p>

<p>I used the Matlab statistical toolbox to do the regression analysis, but the results were very bad. The p values of the F statistic and t statistic are very small, but the RÂ² is also very low, about 20%. </p>

<p>So how should I improve the RÂ²? Are there good methods? I see that SVM (or LS-SVM) can do regression anaysis, is it a good way to manage the massive data, multiple independent variables regression anaysis?</p>

<p>The following are the results:</p>

<pre><code>mdl = 
Linear regression model:
    y ~ 1 + x1*x2 + x1*x3 + x1*x4 + x2*x3 + x2*x4 + x1^2 + x2^2 + x3^2 + x4^2

Number of observations: 18471, Error degrees of freedom: 18457    
Root Mean Squared Error: 2.44  
R-squared: 0.225,  Adjusted R-Squared 0.225  
F-statistic vs. constant model: 413, p-value = 0  
</code></pre>

<p>when we add another predictor, i.e., the independent variables become 5, the resuts of the regression analysis are:</p>

<p>Linear regression model:</p>

<pre><code>y ~ 1 + x1*x2 + x1*x3 + x1*x4 + x1*x5 + x2*x3 + x2*x4 + x2*x5 + x3*x5 + x2^2 + x3^2 + x4^2 + x5^2
</code></pre>

<p>Number of observations: 18457, Error degrees of freedom: 18439
Root Mean Squared Error: 2.21
R-squared: 0.366,  Adjusted R-Squared 0.366
F-statistic vs. constant model: 627, p-value = 0</p>
"
"0.113960576459638","0.112454054604034"," 44922","<p>I am trying to recreate analysis of diamond data in R and compare it to minitab output cited in <a href=""http://www.amstat.org/publications/jse/v9n2/datasets.chu.html"" rel=""nofollow"">http://www.amstat.org/publications/jse/v9n2/datasets.chu.html</a> </p>

<p>I'm almost certain its the same data set yet my results are different. I suspect I'm modeling it differently. I also noticed that I'm missing colourD and some others. Did the the minitab model somehow pick different dummys to exclude?</p>

<p><strong>R OUTPUT</strong></p>

<pre><code>require(""Ecdat"")

data(Diamond)

Diamond$ln_price &lt;- log(Diamond$price)

summary(lm(ln_price~carat+colour+clarity+certification, data=Diamond))


Call:
lm(formula = ln_price ~ carat + colour + clarity + certification, 
    data = Diamond)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.31236 -0.11520  0.01613  0.10833  0.36339 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       6.8011915  0.0496111 137.090  &lt; 2e-16 ***
carat             2.8550130  0.0369676  77.230  &lt; 2e-16 ***
colourE          -0.0295093  0.0404611  -0.729  0.46638    
colourF          -0.1063582  0.0379807  -2.800  0.00544 ** 
colourG          -0.2063494  0.0389848  -5.293 2.35e-07 ***
colourH          -0.2878756  0.0394752  -7.293 2.81e-12 ***
colourI          -0.4165565  0.0413818 -10.066  &lt; 2e-16 ***
clarityVS1       -0.2019313  0.0310634  -6.501 3.41e-10 ***
clarityVS2       -0.2985406  0.0333027  -8.964  &lt; 2e-16 ***
clarityVVS1      -0.0007058  0.0311121  -0.023  0.98192    
clarityVVS2      -0.0966174  0.0289396  -3.339  0.00095 ***
certificationHRD -0.0088557  0.0208641  -0.424  0.67155    
certificationIGI -0.1827107  0.0249516  -7.323 2.33e-12 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1382 on 295 degrees of freedom
Multiple R-squared: 0.9723,     Adjusted R-squared: 0.9712 
F-statistic: 863.6 on 12 and 295 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p><strong>MINITAB OUTPUT</strong></p>

<pre><code>ln_price = 6.08 + 2.86 Carat + 0.417 D + 0.387 E + 0.310 F + 0.210 G + 0.129 H
          + 0.299 IF + 0.298 VVS1 + 0.202 VVS2 + 0.0966 VS1 + 0.0089 GIA
           - 0.174 IGI

Predictor        Coef       StDev          T        P       
Constant      6.07724     0.04809     126.37    0.000
Carat         2.85501     0.03697      77.23    0.000      
D             0.41656     0.04138      10.07    0.000       
E             0.38705     0.03082      12.56    0.000       
F             0.31020     0.02748      11.29    0.000       
G             0.21021     0.02836       7.41    0.000       
H             0.12868     0.02852       4.51    0.000       
IF            0.29854     0.03330       8.96    0.000       
VVS1          0.29783     0.02810      10.60    0.000       
VVS2          0.20192     0.02534       7.97    0.000       
VS1           0.09661     0.02492       3.88    0.000       
GIA           0.00886     0.02086       0.42    0.672       
IGI          -0.17385     0.02867      -6.06    0.000       

S = 0.1382      R-Sq = 97.2%     R-Sq(adj) = 97.1%

Analysis of Variance

Source            DF          SS          MS         F        P
Regression        12     197.939      16.495    863.64    0.000
Residual Error   295       5.634       0.019
Total            307     203.574
</code></pre>

<p><strong>UPDATE</strong>
Glen_b's solution produces the correct results, but the output is not desirable. </p>

<pre><code>                       Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)           6.3846350  0.0411520 155.148  &lt; 2e-16 ***
carat                 2.8550130  0.0369676  77.230  &lt; 2e-16 ***
C(colour, base = 6)1  0.4165565  0.0413818  10.066  &lt; 2e-16 ***
C(colour, base = 6)2  0.3870472  0.0308241  12.557  &lt; 2e-16 ***
C(colour, base = 6)3  0.3101983  0.0274791  11.288  &lt; 2e-16 ***
C(colour, base = 6)4  0.2102072  0.0283593   7.412 1.32e-12 ***
C(colour, base = 6)5  0.1286809  0.0285231   4.511 9.31e-06 ***
</code></pre>

<p>Is there a way to get the original factor character displayed instead of the level?</p>

<p><strong>UPDATE</strong></p>

<p>Using relevel to set the base to the same bases as the minitab produces the same results and is still readable.</p>

<pre><code>  Diamond$colour &lt;- relevel(Diamond$colour, ref=""I"")

  Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
  (Intercept)       6.3846350  0.0411520 155.148  &lt; 2e-16 ***
  carat             2.8550130  0.0369676  77.230  &lt; 2e-16 ***
  colourD           0.4165565  0.0413818  10.066  &lt; 2e-16 ***
  colourE           0.3870472  0.0308241  12.557  &lt; 2e-16 ***
  colourF           0.3101983  0.0274791  11.288  &lt; 2e-16 ***
  colourG           0.2102072  0.0283593   7.412 1.32e-12 ***
  colourH           0.1286809  0.0285231   4.511 9.31e-06 ***
</code></pre>
"
"0.0632139541241014","0.0623782861551805"," 45184","<p><a href=""http://www.nber.org/papers/w14723.pdf"" rel=""nofollow"">Lee and Lemieux</a> (p. 31, 2009) suggest the researcher to present the graphs while doing Regression discontinuity design analysis (RDD). They suggest the following procedure: </p>

<blockquote>
  <p>""...for some bandwidth $h$, and for some number of bins $K_0$ and
  $K_1$ to the left and right of the cutoff value, respectively, the
  idea is to construct bins ($b_k$,$b_{k+1}$], for $k = 1, . . . ,K =
 K_0$+$K_1$, where $b_k = câˆ’(K_0âˆ’k+1) \cdot h.$""</p>
</blockquote>

<pre><code>c=cutoff point or threshold value of assignment variable
h=bandwidth or window width.
</code></pre>

<p>...then compare the mean outcomes just to the left and right of the cutoff point...""</p>

<p>..in all cases, we also show the ï¬tted values from a quartic regression model estimated separately on each side of the cutoff point...(p. 34 of the same paper)</p>

<p>My question is how do we program that procedure in <code>Stata</code> or <code>R</code> for plotting the graphs of outcome variable against assignment variable (with confidence intervals) for the sharp RDD.. A sample example in <code>Stata</code> is mentioned <a href=""http://www.stata.com/statalist/archive/2010-11/msg00131.html"" rel=""nofollow"">here</a> and <a href=""http://www.stata.com/statalist/archive/2011-05/msg01640.html"" rel=""nofollow"">here</a> (replace rd with rd_obs) and a sample example in <code>R</code> is <a href=""http://blog.lib.umn.edu/moor0554/canoemoore/2010/02/regression_discontinuity_gallery_nonparametric.html"" rel=""nofollow"">here</a>. However, I think both of these didn't implement the step 1. Note, that both have the raw data along with the fitted lines in the plots.</p>

<p>Sample graph without confidence variable [Lee and Lemieux,2009] <img src=""http://i.stack.imgur.com/a2KPD.png"" alt=""enter image description here"">
Thank you in advance.  </p>
"
"0.109489780290272","0.0990388308340227"," 46789","<p>I collected data to find whether the presence or absence of vision, sound, and touch during a task affected the successful completion of that task. However, there were no samples collected where all three senses were absent. So the dependent variable is boolean success but I have a question about how to model the independent variables in a logistic regression.</p>

<p>My initial analysis used a single categorical variable with seven levels representing each combination of senses (seven because there were no cases where all three senses were absent).</p>

<pre><code>summary( glmer( Success ~ Condition + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>When I tried to build a model with the Vision, Sound, and Touch as separate variables, the analysis fails. <a href=""http://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q4/004552.html"" rel=""nofollow"" title=""[R-sig-ME] Structural zeros in lme4"">I believe this is because I have empty cells when including the vision*sound*touch interaction</a> because we did not collect results where all senses were absent.</p>

<pre><code>summary( glmer( Success ~ Vision + Sound + Touch + Vision*Sound + Vision*Touch + 
                Sound*Touch + Vision*Sound*Touch + ( 1 | Participant ), 
                family=binomial, data=trials))
</code></pre>

<p>I followed the suggestion linked above to use the <code>interaction</code> function to drop the unused factor (all three senses absent). However, this seems to create a variable that looks like my original single categorical variable.</p>

<pre><code>senses &lt;- interaction( trials$Vision, trials$Sound, trials$Touch, drop=TRUE )
summary( glmer( Success ~ senses + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>As I try to refine this analysis, is there a way to model the senses as separate variables to make the interaction between these variables clearer? That is, to appropriately model the contribution of vision in the <code>vision</code>, <code>vision*sound</code>, <code>vision*touch</code> and <code>vision*sound*touch</code> conditions. From the initial analysis, the <code>vision*sound*touch</code> interaction is the most interesting.</p>
"
"0.0446990156267674","0.0441081091391231"," 47008","<p>I have a fairly simple dataset consisting of one independent variable, one dependent variable, and a categorical variable. 
I have plenty of experience running frequentist tests like <code>aov()</code> and <code>lm()</code>, but I cannot figure out how to perform their bayesian equivalents in R. </p>

<p>I would like to run a bayesian linear regression on the first two variables and a bayesian analysis of variance using the categorical variable as the groupings, but I cannot find any simple examples on how to do this with R. Can someone provide a basic example for both? Additionally, what exactly are the output statistics created by bayesian analysis and what do they express?</p>

<p>I am not very well-versed in stats, but the consensus seems to be that using basic tests with p-values is now thought to be somewhat misguided, and I am trying to keep up.
Regards.</p>
"
"0.070675349274022","0.0697410440814588"," 47302","<p>I am currently working on time series modeling, especially on stationarity tests. For this purpose, I am extensively using Pfaff's book ""<a href=""http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-75966-1"" rel=""nofollow"">Analysis of integrated and cointegrated time series with R</a>"" and I have some questions :</p>

<ol>
<li><p>On page 63, there is a nice ordinogram (Figure 3.3) explaining how all the ADF tests are related, and what should be the underlying decision tree. First of all, one needs to estimate the ADF equation with a linear trend and test for $\pi=0$ (this statistic is called <code>tau3</code> in the associated package <a href=""http://cran.r-project.org/web/packages/urca/index.html"" rel=""nofollow"">urca</a>). If we reject the null hypothesis, then there is no unit root. If we cannot reject, we test for $\beta_2=0$ given $\pi=0$ (this statistic is called <code>phi3</code> in <code>urca</code>). If we reject, then Pfaff writes ""test again for a unit root using a standardized normal"" with no further explanation.</p>

<p>Does anyone understand what he is talking about? Does this ""normal test"" appear somewhere in the urca implementation?</p></li>
<li><p>Suppose the test <code>tau3</code> for $\pi=0$ is rejected. Then the conclusion should be that there is no unit root but a trend in the series (the series is trend stationary). I have at disposal  the underlying linear regression result given by <code>ur.df()</code> from the package <code>urca</code>. Is it correct to conclude that there is actually no trend when the p-value of the t-statistic for the trend coefficient is significant? </p></li>
</ol>

<p>Thanks in advance for your help.</p>
"
"0.0774209661138764","0.0763974860547543"," 48455","<p>I have 1 categorical factor (3 treatments) and 1 continuous factor (weight) and then I have 5 continuous response variables.</p>

<p>From what I have read, I should not use a two way ANOVA as one of the factors is continuous.  Is this correct? Should I be using a Multiple Regression instead?</p>

<p>I was advised that I can use ANOVA, but I'm not sure if this is correct based on what I have read.  I could convert the continuous factor to categorical, but I have also read on this site that this is not the preferred option.</p>

<p>My aim with the data is to see if there is a significant difference between the 3 treatments in regards to the response variables, which would be a standard one-way ANOVA, but I also want to see if weight effects the response variables.  </p>

<p>My analysis will be with R.</p>
"
"0.113960576459638","0.103803742711416"," 49607","<h1>Background</h1>

<h2>Introduction</h2>

<p>I have a data set consisting of data collected from a questionnaire that I wish to validate. I have chosen to use confirmatory factor analysis to analyse this data set.</p>

<h2>Instrument</h2>

<p>The instrument consists of 11 subscales. There is a total of 68 items in the 11 subscales. Each item is scored on an integer scale between 1 to 4.</p>

<h2>Confirmatory factor analysis (CFA) setup</h2>

<p>I use the <code>sem</code> package to conduct the CFA. My code is as below:</p>

<pre><code>cov.mat &lt;- as.matrix(read.table(""http://dl.dropbox.com/u/1445171/cov.mat.csv"", sep = "","", header = TRUE))
rownames(cov.mat) &lt;- colnames(cov.mat)

model &lt;- cfa(file = ""http://dl.dropbox.com/u/1445171/cfa.model.txt"", reference.indicators = FALSE)
cfa.output &lt;- sem(model, cov.mat, N = 900, maxiter = 80000, optimizer = optimizerOptim)

Warning message:
In eval(expr, envir, enclos) : Negative parameter variances.
Model may be underidentified.
</code></pre>

<p>Straight off you might notice a few anomalies, let me explain.</p>

<ul>
<li>Why is the optimizer chosen to be <code>optimizerOptim</code>? </li>
</ul>

<p>ANS: I originally stuck with the default <code>optimizerSem</code> but no matter how many iterations I run, either I run out of memory first (8GB RAM setup) or it would report <code>no convergence</code> Things ""seemed"" a little better when I switched to <code>optimizerOptim</code> where by it would conclude successfully but throws up the error that the model is underidentified. Upon closer inspection, I realise that the output shows <code>convergence</code> as <code>TRUE</code> but <code>iterations</code> is <code>NA</code> so I am not sure what is exactly happening.</p>

<ul>
<li>The <code>maxiter</code> is too high.</li>
</ul>

<p>ANS: If I set it to a lower value, it refuses to converge, although as mentioned above, I doubt real convergence actually occurred.</p>

<h1>Problem</h1>

<p>So by now I guess that the model is really underidentified so I looked for resources to resolve this problem and found:</p>

<ul>
<li><a href=""http://davidakenny.net/cm/identify_formal.htm"" rel=""nofollow"">http://davidakenny.net/cm/identify_formal.htm</a></li>
<li><a href=""http://faculty.ucr.edu/~hanneman/soc203b/lectures/identify.html"" rel=""nofollow"">http://faculty.ucr.edu/~hanneman/soc203b/lectures/identify.html</a></li>
</ul>

<p>I followed the 2nd link quite closely and applied the t-rule:</p>

<ul>
<li>I have 68 observed variables, providing me with 68 variances and 2278 covariances between variables = <strong>2346 data points</strong>.</li>
<li>I also have 68 regression coefficients, 68 error variances of variables, 11 factor variances and 55 factor covariances to estimate making it a total of 191 parameters.</li>
<li>Since I will be fixing the variances of the 11 latent factors to 1 for scaling, I would remove them from the parameters to estimate making it a total of <strong>180 parameters to estimate</strong>.
<ul>
<li>My degrees of freedom is therefore 2346 - 180 = 2166, making it an over identified model by the t-rule.</li>
</ul></li>
</ul>

<h1>Questions</h1>

<ol>
<li>Is the low variance of some of my items a possible cause for the underidentification? I asked a previous question on items with zero variance which led me to think about items which are very close to zero. Should they be removed too? <a href=""http://stats.stackexchange.com/questions/49359/confirmatory-factor-analysis-using-sem-what-do-we-do-with-items-with-zero-varia"">Confirmatory factor analysis using SEM: What do we do with items with zero variance?</a></li>
<li>After reading much, I surmise that the underidentification might be a case of empirical underidentification. Is there a systematic way of diagnosing what kind of underidentification it is? And what are my options to proceed with my analysis?</li>
</ol>

<p>I have more questions but let's take it at these 2 for now. Thanks for any help!</p>
"
"0.155302087572745","0.1532490443501"," 50086","<p>Assume for example a trivariate Gaussian model:
$$
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \quad (*)
$$
with ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$. </p>

<p>The Bayesian conjugate theory of this model is well known. 
This model is the most simple case of a  multivariate linear regression model. 
And more generally, there is a well known Bayesian conjugate theory of multivariate linear regression, which is  the extension to the case when  the multivariate mean  ${\boldsymbol \mu}= {\boldsymbol \mu}(x_i)$ is allowed to depend on the covariates $x_i$ of individual $i \in \{1, \ldots, n \}$, with linear constraints about the multivariate means ${\boldsymbol \mu}(x_i)$. See for instance <a href=""http://books.google.be/books?id=GL8VS9i_B2AC&amp;dq=bayesian%20econometrics%20bayesm&amp;hl=fr&amp;source=gbs_navlinks_s"" rel=""nofollow"">Rossi &amp; al's book</a> accompanied by the crantastic <a href=""http://cran.r-project.org/web/packages/bayesm/index.html"" rel=""nofollow""><code>bayesm</code></a> package for <code>R</code>. 
We know in addition that the Jeffreys prior is a limit form of the conjugate prior  distributions.</p>

<p>Let us come back to the simple multivariate Gaussian model $(*)$. Instead of generalizing this model to the case of linearly dependent multivariate means ${\boldsymbol \mu}(x_i)$ depending on individuals $i=1,\ldots,n$, we can consider a more restrictive model by assuming linear constraints about the components $\mu_1$, $\mu_2$, $\mu_3$ of the multivariate mean ${\boldsymbol \mu}$. </p>

<h3>Example</h3>

<p>Consider some concentrations $x_{i,t}$ of $4$ blood samples $i=1,2,3,4$ measured at $3$ timepoints $t=t_1,t_2,t_3$. 
Assume that the samples are independent and that the series of the three measurements 
$(x_{i,t_1},x_{i,t_2},x_{i,t_3}) \sim {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right)$ for each sample $i$ with a mean 
${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$ whose three components are 
linearly related to the timepoints: $\mu_j = \alpha + \beta t_j$.</p>

<p>This example falls into the context of <em>Multivariate linear regression with a within-design structure</em>. 
See for instance <a href=""http://wweb.uta.edu/management/Dr.Casper/Fall10/BSAD6314/Coursematerial/O%27Brien%20&amp;%20Kaiser%201985%20-%20MANOVA%20-%20RM%20-%20Psy%20Bull%2085.pdf"" rel=""nofollow"">O'Brien &amp; Kaiser 1985</a> and <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a></p>

<p>So my example is a simple example of this situation because there are only some predictors (the timepoints) for the components of the mean, but there are no predictors for individuals. This example could be written as follows:
$$
(**) \left\{\begin{matrix} 
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \\ 
{\boldsymbol \mu} = X {\boldsymbol \beta}
\end{matrix}\right.
$$
with ${\boldsymbol \beta}=(\alpha, \beta)'$ and $X=\begin{pmatrix} 1 &amp; t_1 \\ 1 &amp; t_2 \\ 1 &amp; t_3 \end{pmatrix}$ is the matrix of covariates for the components of the multivariate mean ${\boldsymbol \mu}$. The second line of $(**)$ could be termed as the <em>within design</em>, or the <em>repeated measures design</em>, or the <em>structural design</em> (I would appreciate if a specialist had some comments about this vocabulary).</p>

<p>I think such a model can be fitted as a generalized least-squares model, as follows in  <code>R</code> :</p>

<pre><code>gls(response ~ ""between covariates"" , data=dat, 
  correlation=corSymm(form=  ~ ""within covariates"" | individual ))
</code></pre>

<p>(after stacking the data in long format).</p>

<p><strong>My first question</strong> is Bayesian: what about the Bayesian analysis of model $(**)$ and more generally the Bayesian analysis of the multivariate linear regression model with a structural design ? 
Is there a conjugate family ? What about the Jeffreys prior ? Is there an appropriate R package to perform this Bayesian analysis ?</p>

<p><strong>My second question</strong> is not Bayesian: I have recently discovered some possibilities of John Fox's great <code>car</code> package to analyse 
such models with ordinary least squares theory (the <code>Anova()</code> function with the <code>idesign</code> argument --- see <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a>). Perhaps I'm wrong, but I am under the impression that this package only allows to get the MANOVA table (sum of squares analysis) with an orthogonal matrix $X$, and I'd like to get (exact) confidence intervals about the within-design parameters for an arbitrary matrix $X$, as well as prediction intervals. Is there a way to do so with <code>R</code> using ordinary least squares ?  </p>
"
"0.0547448901451359","0.0540211804549215"," 50180","<p>In my research I have performed a series of measurements on 5 different brands of blocks. Each block has been inspected for deformation under incremental forces (20, 30, 40, 50, 60, 70, 80, 90, 100, 110 and 120 N). The deformation for each force was measured 3 times and the mean values were assigned to each brand for a specific amount of force. I was successful in creating linear regression graphs for these 5 different brands.</p>

<p>Now my wish is to see whether a brand makes a significant difference in deformation values and to perform a post-hoc analysis to compare brands among themselves. In other words to compare the linear regression lines. Sorry if what I am saying makes no sense.</p>

<p>So far, I have tried the following commands:</p>

<pre><code>anova(lm(Deformation~Force*Brand, data=Data))
lm(Deformation~Force, data=Data))

# and
aov.data = aov(Deformation~Force*Brand, Data)
</code></pre>

<p>I have gotten suspiciously low p-values (<em>*</em>) which clearly indicates that I might be doing something wrong. I would be grateful if you could help me with this issue.</p>

<pre><code>Force   Brand   Deformation  
20  Brand1  0.65  
30  Brand1  1.23  
40  Brand1  1.25  
50  Brand1  2.39  
60  Brand1  2.45  
70  Brand1  2.93  
80  Brand1  3.13  
90  Brand1  3.57  
100 Brand1  4.68  
110 Brand1  4.84  
120 Brand1  5.33  
20  Brand2  1.24  
30  Brand2  1.11  
40  Brand2  1.6  
50  Brand2  2.13  
60  Brand2  2.69  
70  Brand2  3.60  
80  Brand2  3.90  
90  Brand2  3.99  
100 Brand2  4.51  
110 Brand2  4.74  
120 Brand2  5.98  
20  Brand3  1.21  
30  Brand3  1.37  
40  Brand3  2.56  
50  Brand3  2.49  
60  Brand3  3.17  
70  Brand3  3.33  
80  Brand3  3.38  
90  Brand3  4.2  
100 Brand3  4.22  
110 Brand3  5.22  
120 Brand3  6.28  
20  Brand4  0.92  
30  Brand4  0.89  
40  Brand4  1.2  
50  Brand4  1.67  
60  Brand4  1.98  
70  Brand4  2.25  
80  Brand4  3.8  
90  Brand4  4.17  
100 Brand4  4.94  
110 Brand4  5.4  
120 Brand4  5.76  
20  Brand5  0.69  
30  Brand5  1.26  
40  Brand5  1.61  
50  Brand5  2.17  
60  Brand5  2.07  
70  Brand5  3.35  
80  Brand5  3.27  
90  Brand5  4.13  
100 Brand5  4.25  
110 Brand5  4.59  
120 Brand5  5  
</code></pre>
"
"0.0836242010007091","0.0825187161885156"," 50284","<p>Using R, I'd like to test whether multiple parameters in a regression model are
  equal to specific values (by default, are multiple parameters equal to 0).</p>

<p>For example, in this regression model:</p>

<p>score ~ beta0 + beta1*i1 + beta2*i2 + beta3*age + beta4*i1*age + beta5*i2*age</p>

<p>I want to test H0: (beta2 = 0) and (beta5 = 0)</p>

<p>SAS can do this in PROC REG using the TEST statement.</p>

<p>SAS Program editor contents:</p>

<pre><code>proc reg data=tolerate;
  model score=i1 i2 age i1age i2age;
  test i2=0, i2age=0;    * do assc prof have same reg line as asst? ;
run;
</code></pre>

<p>SAS Output window contents:</p>

<pre><code>       Test 1 Results for Dependent Variable score
                                Mean
Source             DF         Square    F Value    Pr &gt; F
Numerator           2        0.15581       0.38    0.6859
Denominator        24        0.40678
</code></pre>

<p>Here's code in R that starts the analysis:</p>

<pre><code># data description: http://statacumen.com/teach/ADA2/ADA2_HW_07_S13.pdf
tolerate &lt;- read.csv(""http://statacumen.com/teach/ADA2/ADA2_HW_07_tolerate.csv"")
tolerate$rank &lt;- factor(tolerate$rank)
tolerate$rank &lt;- relevel(tolerate$rank, ""3"")
str(tolerate)

tolerate.manual &lt;- data.frame(score = tolerate$score
                                , i1 = (tolerate$rank==1)
                            , i2 = (tolerate$rank==2)
                                , age = tolerate$age
                            , i1age = tolerate$age * (tolerate$rank==1)
                            , i2age = tolerate$age * (tolerate$rank==2)
                            )
lm.man &lt;- lm(score ~ i1 + i2 + age + i1age + i2age, data = tolerate.manual)
summary(lm.man)
</code></pre>

<p>I have been unable to find a solution using library multcomp, contrast, or C().
Ideally, I could do this without creating separate terms in the model, but directly from this lm() statement:</p>

<pre><code>lm.s.a.r.ar &lt;- lm(score ~ age*rank, data = tolerate)
</code></pre>

<p>This gets close using a Wald test, but I'm looking for the same F-test SAS uses.</p>

<pre><code>summary(lm.s.a.r.ar)
library(aod) # for wald.test()
coef.test.values &lt;- rep(0, length(coef(lm.s.a.r.ar))) # typically, this will be all 0s
wald.test(b = coef(lm.s.a.r.ar) - coef.test.values
        , Sigma = vcov(lm.s.a.r.ar)
        , Terms = c(4,6))

Wald test:
----------
Chi-squared test:
X2 = 0.77, df = 2, P(&gt; X2) = 0.68
</code></pre>

<p>Thanks for considering this question.</p>
"
"0.0446990156267674","0.0441081091391231"," 52035","<p>I have a weekly time series representing costs for a cohort. I want to tell whether an intervention on the cohort (we can assume it happened in a single week) has decreased costs for the cohort. I happen to know that the trend over this period for the population from which this cohort was taken was -120 per week per week.</p>

<p>My initial thought was simply to do a linear regression <code>lm(Costs~Weeks,offset=-120*Weeks)</code> but (obviously) the significance is not only a function of the effect of the intervention but also how far back I look (if I look back to $-\infty$ it will of course appear non-significant).</p>

<p>I looked at this website: <a href=""http://www.r-bloggers.com/time-series-intervention-analysis-wih-r-and-sas/"" rel=""nofollow"">http://www.r-bloggers.com/time-series-intervention-analysis-wih-r-and-sas/</a> and tried to replicate the R code with my data, but when I enter the arimax() command, I got the error message </p>

<pre><code>Error in stats:::arima(x=x,order=order,seasonal=seasonal,fixed=par[1:narma], : wrong length for 'fixed'
</code></pre>

<p>Now, I'm not sure what to do. Can anyone give me some guidance?</p>
"
"0.0316069770620507","0.0311891430775903"," 52513","<p><img src=""http://i.stack.imgur.com/gMBYg.gif"" alt=""[Here is an image for details!](https://onlinecourses.science.psu.edu/stat504/sites/onlinecourses.science.psu.edu.stat504/files/lesson07/crab_r_output_02.gif)""></p>

<p>Disregarding ""Deviance"" in the image, the output of multiple regression analysis in R looks pretty much like this.
As far as I understand, residuals are errors. Do the 5 value summary refer to residuals as errors of estimate, or are these different? Or is my understanding of residuals mistaken?</p>
"
"0.070675349274022","0.0697410440814588"," 52516","<p>I have a data set with the following:</p>

<p>N = 60;
x = developmental stage (range 25 to 44);
y = proportion of 10 minute trial performing a behavior (range 0 to 0.81; 30 zeros)</p>

<p>A scatterplot produces a quadratic looking curve where those in mid-development clearly performed the behavior for more time. Most of the zeros are in the youngest and oldest individuals. If I break up the data into 5 groups according to developmental stage, an ANOVA/Tukey strongly supports this pattern. However, I would like to analyze this data continuously without breaking it into groups.</p>

<p>I have considered arcsine square root transformed proportion data in a linear regression, but I am unsure if that can incorporate a quadratic term, and this analysis results in a very small R squared value (less than 0.1). I have also considered arcsine square root transformed proportion data in a GLM containing a quadratic term or a beta regression (zeros??), but am not sure where to go from here.</p>

<p>I am planning to say in the paper that the individuals in mid-development perform the behavior more than those in early or late development, but am struggling to interpret the data in a way that supports that statement.</p>

<p>I appreciate any suggestions, thank you!</p>
"
"0.0446990156267674","0.0441081091391231"," 55946","<p>I conducted a regression analysis using R's lm() function. One of the independent variables shows no significance (p = 0.89), which contradicts the hypothesis that is should have a significantly positive effect on the dependent variable. </p>

<p>How do you interpret that? Can you say that it has no positive effect on the dependent variable, just because it is not significant - even though it is not significantly negative?</p>
"
"0.0948209311861521","0.0935674292327708"," 56055","<p>I have data from a randomized survey experiment in which each respondent was assigned to one of 4 groups, one of which can be considered a ""control"" or ""no treatment"" group. The key question asked in the survey was a binary one: i.e. each respondent was faced with a choice between two products given some stimulus based on the assigned group. Of course, there are several other questions to be controlled for (demographics, pre-existing preferences, etc.).</p>

<p>I want to know what effect, if any, being in a particular group had on the respondent's choice for that key question, controlling for the other factors. Since my response variable is categorical I can't use ANOVA (at least R doesn't appear willing to let me have a non-numeric response variable). I have tried to do a logistic regression but it seems like the structure of my data means that this would result in the respondents in each group being compared to the rest of the respondents which seems like it would be incorrect.</p>

<p>My data resembles the following in structure:</p>

<pre><code>| Id | Group | Product Chosen | ... (other variables)
| 1  |     1 | A              | ...
| 2  |     4 | B              | ...
| 3  |     3 | B              | ...
| 4  |     2 | B              | ...
| 5  |     1 | A              | ...
| 5  |     2 | B              | ...
| 5  |     4 | A              | ...
| 5  |     3 | B              | ...
</code></pre>

<p>etc.</p>

<p>In case it is relevant, I have been using R for my analysis.</p>

<p><strong>Update:</strong> Just so it's clear, my working hypothesis is that respondents in non-control groups were more likely to choose product A than B (and less importantly, but similarly, that respondents in group 2 were more likely than those in group 3, and those in group 3 were more likely than those in group 4).</p>
"
"0.0632139541241014","0.0623782861551805"," 56427","<p>So a colleague and myself are using principal component analysis (PCA) or non metric multidimensional scaling (NMDS) to examine how environmental variables influence patterns in benthic community composition. A common method is to fit environmental vectors on to an ordination. The length and direction of the vectors seems somewhat straighforward but I don't understand how an R squared value or a p-value is calculated for these vectors. I have looked at a dozen papers and the most I can gather is that these numbers are calculated using permutations of the data. This does not seem very intuitive. What data is being permuted? How does this permutation create an R squared value and what variance is being explained? My limited understanding of an R squared value comes from linear regressions. I need to explain this to people who have little to no background in statistics so any help understanding these concepts or a link to an available text would be greatly appreciated. Thanks so much! </p>
"
"0.0799600299750219","0.0986287303940589"," 56521","<p>I need to calculate the regression variance ($\sigma^2$) in order to estimate both the confidence intervals and the prediction intervals in a gls regression analysis.  For the analysis, the covariance matrix ($V$) of the response variable ($y$) is known in advance, and so I use it directly as the weighting matrix (=$V^{-1}$) in the gls regression analysis.</p>

<p>The regression variance is a weighted sum of the residual error:
$\sigma^2 = \frac{ (Y â€“ X\beta)^T C^{-1} (Y â€“ X\beta)}{n â€“ p}$</p>

<p>My question/problem is how to determine the weighting matrix $C^{-1}$?  $C$ cannot be set equal to $V$ since (according to the above equation) $C$ must be dimensionless while $V$ has the same units as $\sigma^2$.</p>

<p>Based on my reading of the literature and available texts, it seems that $C$ is the correlation matrix and is a scaled or normalized form of the covariance matrix $V$.  i.e., $V = Var(\epsilon^2) = \sigma^2 C$.  But my problem is that $\sigma^2$ is not yet known, and so I need another way find $C$ from $V$.</p>

<p>R functions such as gls() will compute the regression variance (if I knew how gls() does this, it would answer my question).  However I cannot use gls() in this case since I am specifying a user-defined covariance (weighting) matrix, and gls() only accepts a limited set of specific correlation structures.</p>

<p>In fact a possible solution can be found in this <a href=""http://stats.stackexchange.com/questions/14426/prediction-with-gls"">earlier post</a> where an equation for the SEE (or sigma2) for a GLS regression was cited :</p>

<p>GLS calc of SEE: sqrt( sum( ( residuals from linear model) ^ 2 * glsWeight ) ) / sum( glsWeight ) * length( glsWeight ) / residualDegreeFreedom )</p>

<p>However I am unable to ascertain the validity of this equation and cannot find its source reference.</p>
"
"0.0547448901451359","0.0540211804549215"," 56608","<p>I'm doing some clinical database research and in an effort to lessen the burden on our statistical staff, I started to look for different software solutions to get the analyses I need, and that's where BIDS (business intelligence development studio).  BIDS allows queries to be run against a SQL Server and store the results of those queries in a table or view.  That table or view is then consumed by BIDS and logistic and linear regressions as while as CART analyses can be done on them.<br>
BIDS Version</p>

<p>The x axis that is cut off on the lift chart is 'overall population %'
<img src=""http://i.stack.imgur.com/LbpXf.jpg"" alt=""enter image description here""> </p>

<p>A mining accuracy chart of the CART
<img src=""http://i.stack.imgur.com/Vm5Te.jpg"" alt=""enter image description here""></p>

<p>I'm not quite sure how this works in other stats packages, but in BIDS you define a certain percentage of your data set to train the model and the rest of the set is compared against that model and the lift chart shows the improvement in identifying the outcome you desire vs. a guess.  I'm only vaguely familiar with CART analyses in the first place, and don't know the first thing about R, but this same analysis was done in R with very similar results.  The red portion of what looks like a health meter in a video game corresponds nearly identical to the analysis done in R.  However, there are no p values in the BIDS version.  Consider the node Max Total Poly Pharm >=7 and &lt; 13.
BIDS shows me (not present in the picture)
value         cases    probability
not present   2133     91.89</p>

<p>Is there any way to ascertain a p value from that.  And does R use any of it's cases as training data for the model?</p>
"
"0.0364965934300906","0.0540211804549215"," 56900","<p>I have a 20-yr dataset of an annual count of species abundance for a set of polygons (~200 irregularly shaped, continuous polygons).  I have been using regression analysis to infer trends (change in count per year) for each polygon, as well as aggregations of polygon data based on management boundaries.</p>

<p>I am sure that there is spatial autocorrelation in the data, which is sure to impact the regression analysis for the aggregated data.  My question is - how do I run a SAC test for time series data?  Do I need to look at the SAC of residuals from my regression for each year (global Moran's I)?  Or can I run one test with all years?</p>

<p>Once I've tested that yes there is SAC, is there an easy was to address this?  My stats background is minimal and everything I've read on spatio-temporal modeling sounds very complex.  I know that R has a distance-weighted autocovariate function - is this at all simple to use?</p>

<p>I'm really quite confused on how to assess/addess SAC for this problem and would very much appreciate any suggestions, links, or references. Thanks in advance!</p>
"
"0.144991226648596","0.143074489662852"," 58321","<p>I need some help with the statistical analysis of a study of a particular surgery to remove a particular cancer. I am using the statistical program R to conduct my analysis. My data are saved in the object <code>study_data</code>.</p>

<h3>Data</h3>

<pre><code># Create reproducible example data
set.seed(50)

study_data &lt;- data.frame(
              Patient_ID = 1:500,
              Institution = sample(c(""New York"",""San Francisco"",""Houston"",""Chicago""),500,T),
              Gender = sample(c(""Male"",""Female""),500,T),
              Race = sample(c(""White"",""Black"",""Hispanic"",""Asian""),500,T),
              Tumor_grade = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Pathologic_stage = sample(c(""P0"",""Pa"",""Pis"",""P1"",""P2a"",""P2b"",""P3a"",""P3b"",""P4a"",""P4b""),500,T),
              Treatment_arm = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Surgery_age = round(runif(500,20,100)),
              Nodes_removed = round(runif(500,1,130)))
</code></pre>

<p>Here is what the data look like:</p>

<pre><code># Peak at the first six lines of the data
head(study_data)

  Patient_ID   Institution Gender     Race Tumor_grade Pathologic_stage Treatment_arm Surgery_age Nodes_removed
1          1       Houston   Male Hispanic         One              P2b           Two          77           130
2          2 San Francisco Female Hispanic       Three               Pa           Two          38           112
3          3      New York Female    Black        Four               P0          Four          90            90
4          4       Chicago   Male Hispanic         Two              Pis          Four          46             4
5          5       Houston Female    Black        Four              P2a          Four          96           114
6          6      New York   Male    Black       Three              P3b          Four          92             7
</code></pre>

<h3>My interest</h3>

<p>I am interested in learning more about what variables are associated with the number of lymph nodes removed during the surgery. My first thought was to simply stratify the data by a particular variable and then calculate the median number of nodes removed.</p>

<p>For example, to see if the institution at which the surgery was performed mattered, I could write:</p>

<pre><code>cbind(do.call(rbind, by(study_data$Nodes_removed, study_data$Institution, summary)))

              Min. 1st Qu. Median  Mean 3rd Qu. Max.
Chicago          1   25.50   65.5 64.48   98.75  129
Houston          1   40.00   71.0 69.26  100.00  130
New York         4   36.00   67.0 67.96  100.00  129
San Francisco    3   36.75   61.0 65.76   99.00  127
</code></pre>

<p>This lets me compare the median nodes removed in each institutional city.</p>

<h3>My question</h3>

<p>I would like to fully examine the association between all of my variables and the outcome <code>Nodes_removed</code>.</p>

<ol>
<li>Should I just do these simple summary statistics for all of my variables?</li>
<li>Do I need to perform some sort of hypothesis test for all of the associations to say whether or not the summary statistics differ? For example, should I calculate a median and a confidence interval for each comparison?</li>
<li>Or should I be using t-tests to compare one group to another?</li>
<li>In the case of a multi-level variable, should I use ANOVA?</li>
<li>Is there any role for linear regression analysis here? </li>
<li>If I wanted to build a single model that includes every possible predictor variable, what method should I use?</li>
</ol>

<p>For example, say that I am most interested in the association between the age at which the surgery was performed, <code>Surgery_age</code>, and <code>Nodes_removed</code>. However, I would like to adjust this association for potential confounders like gender, race, tumor grade, treatment arm, etc. What is the best way for me to do this?</p>

<p>Thanks for any advice you can give!</p>
"
"0.0999500374687773","0.0986287303940589"," 58448","<p>I'm stuck with a regression modeling problem. I have panel data where the dependent variable is a probability. Below is an excerpt from my data. The complete panel covers more countries and years, however it is unbalanced. What I can observe is the number of events and the number of trials. The event probability was derived from those values (estimation of this probability should be quite good, given the large number of trials). All independent variables are county-year specific.</p>

<pre><code>     country  year  event_prob  events trials    x    x_lag2 ... more variables
  1   Cyprus  2008  0.03902140  11342  290661   4.60   4.13  ...
  2   Cyprus  2009  0.04586650  13482  293940   4.60   4.48  ...
  3   Cyprus  2010  0.05188398  15206  293077   4.60   4.60  ...
  4   Cyprus  2011  0.06433411  18505  287639   5.79   4.60  ...
  5  Estonia  2008  0.07872978  21686  275449   6.02   4.11  ...
  6  Estonia  2009  0.09516270  33599  353069  13.18   4.91  ...
  7  Estonia  2010  0.08645905  36180  418464   7.95   6.03  ...
  8  Estonia  2011  0.07731997  31590  408562   5.53  13.18  ...
  ...
165  USA  2011  0.06100000  9192822  150702000   2.73  3.27  ...
</code></pre>

<p>My goal is to use regression analysis to find out which variables are significant for the event probability. In R-terminology, I'm looking for a model of the form <code>event_prob ~ x + x_lag2 + ...</code> .</p>

<p>The problem is as follows: <code>event_prob</code> has to be between 0 and 1, hence using <code>event_prob ~ x + x_lag2 + ...</code> might not be the best idea. So I was thinking of using the logit transform of <code>event_prob</code> such that <code>logit(event_prob)</code> ranges from $-\infty$ to $\infty$. The first idea was to use the R's <code>plm</code> package, i.e. <code>plm(logit(event_prob)~x+x_lag2,data,index=c(""country"",""year""),model=""random"")</code> or <code>model=""within""</code> (see below). Is that a reasonable approach or am I violating some essential assumptions?</p>

<p>I was also thinking of using panel generalized linear models from the package <code>pglm</code> (with the logit link function), however since I don't know the outcome of the binary events (only the total number of events and trials) is known, I got stuck there. Maybe someone can help me how to proceed here.</p>

<p>Since I have panel data, I'd like to compute both fixed-effects models and random-effects model and then apply the Hausman (1978) test to decide which model is more appropriate.</p>

<p>Do my first attempts at modeling make sense? I'm really not sure how to correctly address this problem. I hope the description of my problem is detailed enough. If not, I'm happy to provide more details</p>

<p>In terms of software, I'd prefer R. SAS and SPSS are also ok since my university has licences for them. I just don't have much experience with them.</p>
"
"0.0716778865720364","0.0825187161885156"," 58538","<p>I'm looking for breakpoints in species abundance as a function of spatial distance, using the <code>segmented</code> package for R. 'segmented' appears to return a breakpoint no matter what; I don't understand whether it returns an estimate of the signifcance of the breakpoint (or whether the segmented linear model is better than an unsegmented model). </p>

<p>For instance (R code): </p>

<pre><code>require(segmented)
set.seed(1)
x &lt;- 1:100
y &lt;- rnorm(100) + x # No real breakpoint
y2 &lt;- c(50-x[1:50], x[51:100-50]) + rnorm(100) # Clear breakpoint at 50
plot(x,y)
points(x, y2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/DRysM.png"" alt=""enter image description here""></p>

<pre><code># Segmented model for the unsegmented data
testM &lt;- lm(y ~ x)
testMs &lt;- segmented(testM, seg.Z = ~x, psi=90)
summary(testMs)
</code></pre>

<p>Despite the fact that there is clearly no breakpoint in the data, <code>summary</code> reports that <code>t value for the gap-variable(s) V:  0</code>, and the standard error is fairly small (3.28).</p>

<pre><code>testM2 &lt;- lm(y2 ~ x)
testM2s &lt;- segmented(testM2, seg.Z=~x, psi=50)
summary(testM2s)
</code></pre>

<p>Here the guess is correct (49.78), the standard error is even smaller (0.2), and the t value is the same. How do I interpret this result? </p>

<p><strong>Note:</strong> I have no particular attachment to the <code>segmented</code> package - I just want to test the hypothesis that a segmented regression models my data better than a single-domain regression. But the other breakpoint-analysis packages I've looked at seem to require that points on the domain be evenly-spaced (e.g. timeseries) and there is a single dependent value per independent value. These assumptions are not met for my spatial data.</p>
"
"0.126427908248203","0.124756572310361"," 58962","<p>I am building a multiple regression model - wrapped in a function - with one dependent variable and a dozen independent variables. The reason why I am building a function is that I need to do this analysis with approximately 75 different datasets. </p>

<p>The challenge is that the independent variables correlate better with the dependent variable when they are lagged in time. Unfortunately, not all time lags are the same for each variable and I would like to determine the optimal mix of time lags for each variable while getting the most optimum Adjusted R^2 value for the multiple regression model. Moreover, after building an initial model I will try to reduce the model using the <code>step(modelbase, direction=""both"")</code> function on the model. </p>

<p>In the approach I currently have I time lag all the independent variables with the same number of weeks. This results in the best possible model where all independent variables have the same time lag, but I believe (with a valid hypothesis supporting this) that there is a better model out there when we differ the time lag for each independent variable. My question is what is the best strategy to determine the best fit model without making the number of options huge. If I want to determine between 0 and 20 weeks time lag in weekly steps for 12 independent variables I am quickly up to trying to find a match between 4.096e+15 variables (=20^12). </p>

<p>I can imagine reducing the problem with the following strategy: Start by finding the best fit model with one independent variable at different time lags. The second step will be to add a second independent variable with its different time lags and find the best model with the two independent variables where the second is tried at different time lags while the first is kept constant. Then add a third variable for which we take a similar approach as the second by keeping the first two variables constant and change try the third with different time lags. Something tells me that this strategy might be decent approach, but something that there also might be a better overall model that contains the not optimal variables for each individual independent variable. </p>

<p>Is there anybody who shine some light on how to tackle this challenge? </p>
"
"0.0912414835752265","0.0990388308340227"," 59166","<p>Iâ€™ve simulated some data consisting of one response variable (â€˜yâ€™) and two collinear predictor variables (â€˜Amountâ€™ and â€˜MPSâ€™), where collinearity arises from one of two causes: (1) Amount causes MPS, or (2) Amount and MPS are jointly affected by an unmeasured variable. </p>

<p>What I'm trying to do is figure out whether path analysis can discriminate between these two causes of collinearity. But I'm having trouble specifying a path model for collinearity scenario (2). </p>

<p>My question:
<strong>Is it possible to specify a path model that implies that two exogenous variables are jointly influenced by an unmeasured variable?</strong></p>

<p>I'm working in lavaan, but answers for how to do this conceptually would also be appreciated (if you aren't familiar with lavaan).</p>

<p>Here are my data, simulated in R:</p>

<pre><code># collinearity cause (1)
Amount &lt;- rnorm(n=350, mean=0, sd=1)   
MPS &lt;- rnorm(n=350, mean=0.76*Amount, sd=0.653) 
y &lt;- rnorm(n=350, mean=0.367*Amount + 0.367*MPS, sd=0.72) 

# collinearity cause (2)
Lurking &lt;- rnorm(n=350, mean=0, sd=1) 
Amount &lt;- rnorm(n=350, mean=0.872*Lurking, sd=0.486)  
MPS &lt;- rnorm(n=350, mean=0.872*Lurking, sd=0.486)  
y &lt;- rnorm(n=350, mean=0.367*Amount + 0.367*MPS, sd=0.72)         
</code></pre>

<p>And this is my path model for (1), specified in lavaan:</p>

<pre><code>model1 &lt;- '
  #regressions
  y ~ Amount
  y ~ MPS
  MPS ~ Amount
  '
</code></pre>

<p>And this is a path model I tried for (2):</p>

<pre><code>model2&lt;- '
  #regressions
  y ~ Amount
  y ~ MPS
  #residual correlations
  MPS ~~ Amount
  '
</code></pre>

<p>so for path model (2) my approach was to specify a residual correlation between MPS and Amount. I'm uncertain if this is the correct approach. but even if it is, it doesn't work â€“ to make it work I have to specify that exogenous variables are not fixed, and this uses up my degrees of freedom so I can't test the model.</p>

<p>If anyone has any suggestions for how I can do this â€“ or if it is possible at all - I'd really appreciate it.</p>
"
"0.0632139541241014","0.0623782861551805"," 59311","<p>I'm working on a behavoural scorecard modelling exercise, and many of the decisions taken to date have been based on the experience of a consulting credit analyst (whose experience software-wise is SAS) as I am primarily in BI. So far I have:</p>

<ul>
<li>a linux pc with 32gb of ram and an i7 processor</li>
<li>an observation window  </li>
<li>~90 potential characteristics </li>
<li>a binary outcome</li>
</ul>

<p>In <strong>R</strong>, I have</p>

<ol>
<li>loaded the dataset (225k obs of 88 vars, 1 outcome)</li>
<li>split the dataset up based on the recommendations/examples in the package <strong>caret</strong> i.e. predictors and outcomes split up (150k obs in training sample)</li>
<li>removed any variables showing a high degree of correlation (caret::findCorrelation)</li>
<li>cut all continuous variables into categorical intervals </li>
<li>reduced the number of variables based on near zero variance, missing values, and low information value (IV) (150k obs of 48 vars)</li>
<li>tried bestglm::bestglm, caret::train (with glm and glmnet), FWDselect::selection, FWDselect::qselection but eventually had to interrupt each of these due to not completing after 4 hours of 100% CPU usage</li>
<li>used FactoMineR:MCA to perform a multiple correspondence analysis (on predictors only)</li>
</ol>

<p>What I would like to do is have a selection of logistic regression models for say 4, 8, 12, and 16 variables that are the most predictive models at each point.  I'm not sure if I'm going in the correct direction here with MCA as I've mainly been simply trying to find something that works in a timely fashion for reducing my variables further or going directly to variable selection steps.</p>

<p>I would appreciate any advice on how to do any of step 6 better, whether 7 makes sense and what step 8 should be. </p>

<p>Thanks,</p>

<p>Steph</p>

<p>PS Design decisions up to 6&amp;7 can't be revised so please, no telling me off for them! </p>
"
"0.0316069770620507","0.0311891430775903"," 59918","<p>I need to create a graph like the one below.The idea is that I have three data groups with 5 replications on the first two groups and 3 replications on the last one. I must obtain a regression curve for each group and link it to the mean and SD for each group, like the image below. I chose to do the analysis in R; I can get the models but have no idea about how to link it to the mean and SD per group. If any of you has some tutorial, example or code which can help it would be really appreciated.</p>

<p><img src=""http://i.stack.imgur.com/zFicp.jpg"" alt=""enter image description here""></p>

<p>Thanks in advance. </p>
"
"0.110624419717177","0.109162000771566"," 61869","<p>I am trying to replicate a path analysis SEM model using Lavaan in R, and was very confused about the results that it gave regarding the model fit statistics. </p>

<p><strong>The code is as follows:</strong> </p>

<pre><code>#Import Package
library(lavaan)

#Input Correlation Matrix
sigma &lt;- matrix(c(1.00, -0.03,  0.39, -0.05, -0.08,
                 -0.03,  1.00,  0.07, -0.23, -0.16,
                  0.39,  0.07,  1.00, -0.13, -0.29,
                 -0.05, -0.23, -0.13,  1.00,  0.34,
                 -0.08, -0.16 ,-0.29,  0.34,  1.00), nr=5, byrow=TRUE)
rownames(sigma) &lt;-c(""Exercise"", ""Hardiness"", ""Fitness"", ""Stress"", ""Illness"")
colnames(sigma) &lt;-c(""Exercise"", ""Hardiness"", ""Fitness"", ""Stress"", ""Illness"")

#Create Covariance Matrix
sdevs &lt;-c(66.5, 3.8, 18.4, 6.7, 624.8)
covmax &lt;- cor2cov(sigma, sdevs)
as.matrix(covmax)

#Specify Model 
mymodel&lt;-'Illness ~ Exercise + Fitness
Illness ~ Hardiness + Stress
Fitness ~ Exercise + Hardiness 
Stress ~ Exercise + Hardiness + Fitness 
Exercise ~~ Exercise 
Hardiness ~~ Hardiness 
Exercise ~~ Hardiness'

#Fit the model with the covariance matrix
N = 363
fit.path &lt;-sem(mymodel,sample.cov=covmax, sample.nobs=N, fixed.x=FALSE)

#Summary of the model fit
summary(fit.path, fit.measures = TRUE)
</code></pre>

<p><strong>And the output I get is as follows:</strong> </p>

<pre><code> lavaan (0.5-12) converged normally after  93 iterations

 Number of observations                         37300

 Estimator                                         ML
 Minimum Function Test Statistic                0.000
 Degrees of freedom                                 0
 P-value (Chi-square)                           1.000

 Model test baseline model:

 Minimum Function Test Statistic            16594.387
 Degrees of freedom                                10
 P-value                                        0.000

 Full model versus baseline model:

 Comparative Fit Index (CFI)                    1.000
 Tucker-Lewis Index (TLI)                       1.000

 Loglikelihood and Information Criteria:

 Loglikelihood user model (H0)             -882379.005
 Loglikelihood unrestricted model (H1)     -882379.005

 Number of free parameters                         15
 Akaike (AIC)                              1764788.009
 Bayesian (BIC)                            1764915.910
 Sample-size adjusted Bayesian (BIC)       1764868.240

 Root Mean Square Error of Approximation:

 RMSEA                                          0.000
 90 Percent Confidence Interval          0.000  0.000
 P-value RMSEA &lt;= 0.05                          1.000

 Standardized Root Mean Square Residual:

 SRMR                                           0.000

 Parameter estimates:

 Information                                 Expected
 Standard Errors                             Standard

                Estimate  Std.err  Z-value  P(&gt;|z|)
 Regressions:
 Illness ~
 Exercise          0.318    0.048    6.640    0.000
 Fitness          -8.835    0.174  -50.737    0.000
 Hardiness       -12.146    0.793  -15.321    0.000
 Stress           27.125    0.451   60.079    0.000
 Fitness ~
 Exercise          0.109    0.001   82.602    0.000
 Hardiness         0.396    0.023   17.211    0.000
 Stress ~
 Exercise         -0.001    0.001   -2.614    0.009
 Hardiness        -0.393    0.009  -44.332    0.000
 Fitness          -0.040    0.002  -19.953    0.000

 Covariances:
 Exercise ~~
 Hardiness        -7.581    1.309   -5.791    0.000

 Variances:
 Exercise       4422.131   32.381
 Hardiness        14.440    0.106
 Illness       318744.406 2334.012
 Fitness         284.796    2.085
 Stress           41.921    0.307
</code></pre>

<p><strong>These are my questions:</strong>  </p>

<ul>
<li>Why does the chi-squared say that there are no degrees of freedom? </li>
<li>Why are the p-values exactly 1? Why is the CFI and TLI exactly 1? </li>
<li><p>Why is the RMSEA 0?</p></li>
<li><p>What would I need to do to simulate a more realistic model that doesn't appear artificially ""perfect""? </p></li>
<li>Does it have to do with the model specification? </li>
</ul>
"
"0.0446990156267674","0.0441081091391231"," 62180","<p>I want to do some regression analysis that constrains the coefficients to vary smoothly as a function of their sequence.
It is similar to the ""Phoneme Recognition"" example in the part 5 ""Basis Expansions and Regularization"" of <em><a href=""http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf"" rel=""nofollow"">The Elements of Statistical Learning</a></em> (Hastie, Tibshirani &amp; Friedman, 2008).</p>

<p>in the book it says:</p>

<blockquote>
  <p>The smooth red curve was obtained through a very simple use of natural cubic splines. We can represent the coefficient function as an expansion of splines $\beta(f)=\sum_{m=1}^M h_m(f)\theta_m$. In practice this means that $\beta=\mathbf{H}\theta$ where, $\mathbf{H}$ is a $p Ã— M$ basis matrix of natural cubic splines, defined on the set of frequencies. Here we used $M = 12$ basis functions, with knots uniformly placed over the integers 1, 2, . . . , 256 representing the frequencies. Since $x^T\beta=x^T\mathbf{H}\theta$, we can simply replace the input features $x$ by their filtered versions $x^* = \mathbf{H}^T x$, and fit $\theta$ by linear logistic regression on the $x^*$. The red curve is thus $\hat\beta(f) = h(f)^T\hat \theta$.</p>
</blockquote>

<p>But I am not sure about how to create the basis matrix $\mathbf{H}$. When using ns() function in R, how to set the knots?</p>

<p>Any hint will be appreciated.</p>
"
"0.122413295785282","0.112742029607188"," 62208","<p>I've tried to simulate data for a power analysis of a logistic regression. The results of the power analysis look reasonable: power=90% for a sample of 6000 persons. But I feel that the analysis lacks something. So, my question is: when generating the data should I include something about how the variables are correlated, or their covariance, other than just defining their linear relationship as I have done in the example below, and if so where do I write that into the code?</p>

<p>I know other questions look like this but I'm not confident that they answer this question.</p>

<pre><code>library(plyr) # functions
## Define Function
simfunktion &lt;- function() {
   # Number in each sample
  antal &lt;- 6000
  beta0 &lt;- log(0.16) # logit in reference group
  beta1 &lt;- log(1.1)  # logit given smoking
  beta2 &lt;- log(1.1)  # logit given SNP(genevariation)
  beta3 &lt;- log(1.2)  # logit for interactioncoefficient for SNP*rygning
   ## Smoking variable, with probabilities defined according to empirical studies.
  smoking  &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,25,40))
   ## SNP variables with probabilities defined according to empirical studies
  SNP      &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,40,20))
   ## calculated probabilites given the model:
  pi.x     &lt;- exp( beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) / 
              ( 1 + exp(beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) )
   ## binoial events given the probabilities:
  sim.y    &lt;- rbinom( n = antal, size = 1, prob = pi.x)  
  sim.data &lt;- data.frame(sim.y, smoking, SNP)
   #################### p-value of the interaction is extracted:
   ## the model is run:
  glm1     &lt;- glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial )
   ## p-value of the interactionterm is extracted:
  summary(    glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial ))$coef[4,4]
}
pvalue     &lt;- as.vector(replicate( 100 , simfunktion()))
mean(pvalue &lt; 0.05)
</code></pre>
"
"0.070675349274022","0.0697410440814588"," 62249","<p>I have count data (observed) and expected counts calculated by regression analysis. I want perform multiple hypothesis test to check which elements show maximum differences between observed and expected. I am using R and I ran this comand,</p>

<pre><code>p.values=apply(matrix,1,function(x){t.test(x[1],x[2],alternative=""g"")$p.value})
</code></pre>

<p>It gives me error saying <code>not enough x observation</code> -it is obvious that I don't have any replicates. But the paper which I am following (they have done similar kind of study) says that: </p>

<blockquote>
  <p>To detect hotspots, we performed a one-tailed test (alpha&lt;0.001) using Poisson distribution and the expected counts</p>
</blockquote>

<p>I don't understand how did they do it. </p>

<p>I can detect the elements using fold-change between observed and expected, but here they say they have done hypothesis test. </p>
"
"0.0316069770620507","0.0311891430775903"," 62483","<p>I've got a conditional logistic regression setup using <code>clogit</code> in <code>R</code> like this: </p>

<pre><code>m&lt;-clogit(PHENO==2 ~ x + as.factor(COVAR[,1]) + strata(COVAR[,2]) )
</code></pre>

<p>I wanted to try doing GLMM analysis in <code>R</code>. I'm a little confused on the syntax for the <code>lme4</code> package in <code>R</code>. <code>COVAR[,2]</code> is the matching variable in my data. Can someone explain to me the difference between some of these statements:</p>

<pre><code>m &lt;-  lmer(PHENO==2 ~ x + as.factor(COVAR[,1]) + (x|COVAR[,2]) )
m &lt;-  lmer(PHENO==2 ~ x + as.factor(COVAR[,1]) + (1|COVAR[,2]) )
</code></pre>

<p>Which one is more appropriate? </p>
"
"0.0316069770620507","0.0311891430775903"," 63055","<p>I am doing survival analysis using ridge regression. I'm using this R command:</p>

<pre><code>coxph(Surv(time, status) ~ ridge(x1, x2, x3), data=DATA)
</code></pre>

<p>As far as I know, <code>lambda</code> (the regulation parameter) is estimated using cross validation, but then this R code should result in different results with different random seeds. But I got always the same coefficients; how can that happen? </p>

<p>Is <code>coxph(Surv()~.)</code> not a commonly used approach? Should I use <code>glmnet</code> or any other functions?  </p>
"
"NaN","NaN"," 63698","<p>I have been checking out which diagnostics to use for a GEE analysis. It seem that influence measures are appropriate (Preisser, 1996). Does anyone know of a package that can be used in R to examine influence measures in GEE? The ""stats"" package does not work: it returns an error when I run</p>

<pre><code>influence.measures(model) 
</code></pre>

<p>on a gee model, but works fine with an ordinary linear regression. </p>
"
"0.0948209311861521","0.0935674292327708"," 63913","<p>I conducted an experiment in a factorial design: I measured light (PAR) in three herbivore treatments as well as six nutrient treatments. The experiment was blocked.</p>

<p>I've run the linear model as follows (you can download the data from my website to replicate)</p>

<pre><code>dat &lt;- read.csv('http://www.natelemoine.com/testDat.csv')
mod1 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
</code></pre>

<p>The residual plots look pretty good</p>

<pre><code>par(mfrow=c(2,2))
plot(mod1)
</code></pre>

<p>When I look at the ANOVA table, I see main effects of Nutrient and Herbivore. </p>

<pre><code>anova(mod1)

Analysis of Variance Table 

Response: light 
                    Df  Sum Sq Mean Sq F value    Pr(&gt;F)     
Nutrient             5  4.5603 0.91206  7.1198 5.152e-06 *** 
Herbivore            2  2.1358 1.06791  8.3364 0.0003661 *** 
BlockID              9  5.6186 0.62429  4.8734 9.663e-06 *** 
Nutrient:Herbivore  10  1.7372 0.17372  1.3561 0.2058882     
Residuals          153 19.5996 0.12810                       
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
</code></pre>

<p>However, the regression table shows non-significant main effects and significant interactions.</p>

<pre><code>summary(mod1)

Call: 
lm(formula = light ~ Nutrient * Herbivore + BlockID, data = dat) 

Residuals: 
     Min       1Q   Median       3Q      Max  
-0.96084 -0.19573  0.01328  0.24176  0.74200  

Coefficients: 
                           Estimate Std. Error t value Pr(&gt;|t|)     
(Intercept)                1.351669   0.138619   9.751  &lt; 2e-16 *** 
Nutrientb                  0.170548   0.160064   1.066  0.28833     
Nutrientc                 -0.002172   0.160064  -0.014  0.98919     
Nutrientd                 -0.163537   0.160064  -1.022  0.30854     
Nutriente                 -0.392894   0.160064  -2.455  0.01522 *   
Nutrientf                  0.137610   0.160064   0.860  0.39129     
HerbivorePaired           -0.074901   0.160064  -0.468  0.64049     
HerbivoreZebra            -0.036931   0.160064  -0.231  0.81784     
... 
Nutrientb:HerbivorePaired  0.040539   0.226364   0.179  0.85811     
Nutrientc:HerbivorePaired  0.323127   0.226364   1.427  0.15548     
Nutrientd:HerbivorePaired  0.642734   0.226364   2.839  0.00513 **  
Nutriente:HerbivorePaired  0.454013   0.226364   2.006  0.04665 *   
Nutrientf:HerbivorePaired  0.384195   0.226364   1.697  0.09168 .   
Nutrientb:HerbivoreZebra   0.064540   0.226364   0.285  0.77594     
Nutrientc:HerbivoreZebra   0.279311   0.226364   1.234  0.21913     
Nutrientd:HerbivoreZebra   0.536160   0.226364   2.369  0.01911 *   
Nutriente:HerbivoreZebra   0.394504   0.226364   1.743  0.08338 .   
Nutrientf:HerbivoreZebra   0.324598   0.226364   1.434  0.15362     
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3579 on 153 degrees of freedom 
Multiple R-squared:  0.4176,    Adjusted R-squared:  0.3186  
F-statistic: 4.219 on 26 and 153 DF,  p-value: 8.643e-09 
</code></pre>

<p>I know that this question has been previously <a href=""http://stats.stackexchange.com/questions/20002/regression-vs-anova-discrepancy"">asked and answered</a> in <a href=""http://stats.stackexchange.com/questions/28938/why-do-linear-regression-and-anova-give-different-p-value-in-case-of-consideri"">multiple posts</a>. In the earlier posts, the issue revolved around the different types of SS used in anova() and lm(). However, I don't think that is the issue here. First of all, the design is balanced:</p>

<pre><code>with(dat, tapply(light, list(Nutrient, Herbivore), length))
</code></pre>

<p>Second, using the Anova() option doesn't change the anova table. This isn't a surprise because the design is balanced.</p>

<pre><code>Anova(mod1, type=2)
Anova(mod1, type=3)
</code></pre>

<p>Changing the contrast doesn't change the results (qualitatively). I still get pretty much backwards intepretations from anova() vs. summary().</p>

<pre><code>options(contrasts=c(""contr.sum"",""contr.poly""))
mod2 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
anova(mod2)
summary(mod2)
</code></pre>

<p>I'm confused because everything I've read on regression not agreeing with ANOVA implicates differences in the way R uses SS for summary() and anova() functions. However, in the balanced design, the SS types are equivalent, and the results here don't change. How can I have completely opposite interpretations depending on which output I use?</p>
"
"0.047410465593076","0.0467837146163854"," 65174","<p>I am doing a logistic regression analysis using the glm command in R. It is to identify causes of valve narrowing beyond a certain threshold; 0=no narrowing, 1=narrowed. One of my variables is the size of a medical device that is implanted (range 25-36mm). Sometimes the device isn't implanted and I've left this as a blank field, but of course this is interpreted as a missing field. Not implanting the device seems to have a significant effect using Chi-sq analysis, and the size of the device has a significant effect using a t-test. How do I get around this in a linear regression model?</p>

<p>To make it more complicated I actually have two different makes of the device: ""C"" and ""D"" with sizes 25-36mm, another device without a size ""S"" and then no device ""N"". Can it all be entered together or is it best to analyze separately outside of regression?</p>

<p>What effect does the ""missingness"" have on various other variables that are in the analysis?</p>

<p>Please &amp; thankyou</p>
"
"NaN","NaN"," 65191","<p>I have a test dataset on which I run the following regression analysis in r:</p>

<p><code>fit &lt;- lm(Cost ~ Slope + YardDist + Removals + TreeVol, data = test)</code></p>

<p>I get an r-squared of 0.83</p>

<p>Next I change the regression equation to:</p>

<p><code>fit &lt;- lm(Cost ~ Slope + YardDist + Removals+ I(TreeVol^(-0.82)), data = test)</code></p>

<p>I get an r-squared of 0.9872.</p>

<p>I am wondering what the expression of <code>I(TreeVol^(-0.82))</code> exactly means?</p>

<p>When I hold all variables constant besides TreeVol, the TreeVol/Cost graph looks like the red one. I guess that is related but I don't fully understand it.
<img src=""http://i.stack.imgur.com/g1Ide.png"" alt=""enter image description here""></p>
"
"0.0948209311861521","0.0935674292327708"," 65859","<p>Recently, I have read an article which name is â€œ<a href=""http://www.ncbi.nlm.nih.gov/pubmed/9618776"" rel=""nofollow""><em>Feed forward neural networks for the analysis of censored survival data: A partial logistic regression approach</em></a>â€.</p>

<p>Without a math background, I catch only a little about the ANN applying the analysis of censored survival data. </p>

<p>In the training group, each subject is replicated for all the intervals in which the subjects is observed and coupled with the event; why is this done? Especially, in the testing group, indicator each subjects are replicated into full number of time interval of observed with all event indicator as zero?</p>

<p>I know this process can take into account the censoring data.</p>

<p>In addition, are the output of neural networks are conditional probabilities of failure? My problem is how to produce the survival curve using the output data. Does anybody know the R code or MATLAB code to perform this whole process? Or give me some suggestion to find answers! The following R code is my try on this method but I can't go on it, for I don't know draw the survival curve depending on the output conditional failure probabilities!</p>

<pre><code>dat&lt;-read.csv(""traininglj.csv"",header=T)
tt &lt;- dat$time &lt;- as.numeric(cut(dat$TTR,c(0,6,12,18,24,30,36,42,48,54,200)))
dat2 &lt;- dat[rep(1:nrow(dat), tt), ]
time2 &lt;- NULL
for (i in 1:length(tt)) time2 &lt;- c(time2, 1:tt[i])
dat2$time &lt;- time2
    dat2$Recurrence &lt;- 0
dat2$Recurrence[cumsum(tt)] &lt;- dat$Recurrence
write.csv(dat2,file=""result.csv"")
mydat &lt;- apply(dat2[,13:23],2,function(x)(x-min(x,na.rm=TRUE))/
    (range(x,na.rm=TRUE)[2]-range(x,na.rm=TRUE)[1]))
training &lt;- cbind(dat2[,1:12],mydat,dat2[24])
library(nnet)
library(lattice)
attach(training)
dat.net &lt;- nnet(Recurrence ~ time+ALT+ALB+PLT+INR+age+MELD+logAFP+Diameter
        +sex+number+Gstage+HBsAg+AN+MVI, 
    data = training, 
    size = 12, 
    decay=0.025,
    maxit = 1000,
    entropy=TRUE,
    trace=TRUE)
</code></pre>
"
"0.0948209311861521","0.0935674292327708"," 66250","<p>I have the following data frame</p>

<pre><code>structure(list(Chi = structure(c(1L, 1L, 5L, 5L, 6L, 9L, 9L, 
12L, 13L, 14L, 14L, 16L, 16L, 19L, 19L, 20L, 20L, 23L, 24L, 24L, 
26L, 26L, 31L, 31L, 33L, 33L, 36L, 37L, 37L, 40L, 40L, 43L, 43L, 
44L, 44L, 45L, 46L, 47L, 47L, 48L, 48L, 52L, 52L, 54L, 54L, 55L, 
55L, 56L, 59L, 59L, 61L, 61L, 63L, 63L, 64L, 64L, 65L, 65L, 69L, 
69L, 70L, 70L, 71L, 71L, 72L, 72L, 75L, 75L, 76L, 76L, 77L, 77L, 
79L, 79L, 86L, 86L, 87L, 87L, 88L, 88L, 91L, 91L, 92L, 92L, 93L, 
95L, 96L, 96L, 97L, 97L, 98L, 98L, 99L, 99L, 100L, 100L, 101L, 
101L, 103L, 103L, 104L, 104L, 107L, 108L, 108L, 112L, 112L, 113L, 
116L, 116L, 117L, 120L, 125L, 125L, 127L, 127L, 129L, 129L, 130L, 
131L, 131L, 132L, 132L, 134L, 134L, 135L, 135L, 136L, 136L, 139L, 
141L, 141L, 143L, 144L, 144L, 145L, 145L, 146L, 150L, 150L, 151L, 
151L, 153L, 153L, 155L, 155L, 157L, 162L, 162L, 163L, 163L, 164L, 
164L, 167L, 167L, 168L, 169L, 169L, 171L, 171L, 172L, 172L, 174L, 
174L, 175L, 175L, 177L, 177L, 180L, 180L, 183L, 187L, 27L, 83L, 
83L, 165L, 165L, 85L, 85L, 156L, 156L, 17L, 17L, 123L, 123L, 
124L, 124L, 57L, 57L, 42L, 42L, 159L, 159L, 38L, 38L, 82L, 82L, 
41L, 41L, 142L, 142L), .Label = c(""0106610856"", ""0107470802"", 
""0108490513"", ""0108590534"", ""0109480651"", ""0111290260"", ""0111410339"", 
""0201390418"", ""0207570604"", ""0208360352"", ""0212323105"", ""0212380362"", 
""0301310432"", ""0302705635"", ""0303450495"", ""0304260266"", ""0304440574"", 
""0305280546"", ""0305380338"", ""0305381393"", ""0305510576"", ""0305542214"", 
""0308610733"", ""0309370345"", ""0309665035"", ""0310380545"", ""0403320259"", 
""0403360374"", ""0404360343"", ""0406270198"", ""0501451137"", ""0504460676"", 
""0511310366"", ""0605270511"", ""0605340560"", ""0605410461"", ""0605410585"", 
""0606260684"", ""0606270353"", ""0609360507"", ""0702520535"", ""0702570818"", 
""0705430421"", ""0710380364"", ""0801330378"", ""0801430275"", ""0802320430"", 
""0803510802"", ""0805390383"", ""0806560533"", ""0809430460"", ""0902380354"", 
""0904340252"", ""0904370445"", ""0906340403"", ""0907380379"", ""0909415420"", 
""0910300100"", ""0911430253"", ""1001270460"", ""1001360389"", ""1002455294"", 
""1005280487"", ""1006330445"", ""1009350447"", ""1010375156"", ""1011270447"", 
""1012350312"", ""1012400441"", ""1102570648"", ""1105450589"", ""1106230566"", 
""1106330587"", ""1204530475"", ""1206350342"", ""1208330373"", ""1209280345"", 
""1209400502"", ""1209400561"", ""1210380536"", ""1302240455"", ""1305751256"", 
""1306370353"", ""1307260470"", ""1310340250"", ""1312430613"", ""1312440597"", 
""1312690593"", ""1404430512"", ""1404530479"", ""1405330376"", ""1406310360"", 
""1406350419"", ""1406430439"", ""1408460602"", ""1412360366"", ""1502385236"", 
""1503370488"", ""1503470628"", ""1503660400"", ""1506390447"", ""1508340196"", 
""1510340688"", ""1510440453"", ""1603310622"", ""1604440376"", ""1606370014"", 
""1609650549"", ""1610345304"", ""1610345304x"", ""1612300367"", ""1702330397"", 
""1704330181"", ""1706330316"", ""1712560522"", ""1802340270"", ""1804310336"", 
""1808430417"", ""1810400244"", ""1902340299"", ""1902610679"", ""1905360355"", 
""1906320438"", ""1906390525"", ""1909310514"", ""1912460408"", ""2002440204"", 
""2004350288"", ""2007350203"", ""2009400364"", ""2009460669"", ""2011410428"", 
""2011500524"", ""2103335236"", ""2109370262"", ""2112290355"", ""2201330484"", 
""2201600686"", ""2203290471"", ""2203406259"", ""2205430513"", ""2207340473"", 
""2208340396"", ""2303430410"", ""2303530717"", ""2308290390"", ""2309420506"", 
""2310370398"", ""2310370398.0"", ""2312280310"", ""2404436295"", ""2406640663"", 
""2411420404"", ""2501520858"", ""2505330239"", ""2505380376"", ""2511320428"", 
""2511320436"", ""2511360306"", ""2601490470"", ""2601520566"", ""2608450598"", 
""2611400237"", ""2701470625"", ""2702230407"", ""2702340342"", ""2703470916"", 
""2704380538"", ""2709250586"", ""2712350545"", ""2712541146"", ""2805310438"", 
""2805350472"", ""2807360475"", ""2807480594"", ""2809325316"", ""2809470634"", 
""2902400411"", ""2903350442"", ""2905330376"", ""2906450480"", ""2910240363"", 
""3004510529"", ""3007230195"", ""3012410333"", ""3107440299"", ""3108350420""
), class = ""factor""), Sex = structure(c(2L, 2L, 2L, 2L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 
2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 
2L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 
1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
2L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 
2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 
2L, 2L, 2L), .Label = c(""F"", ""M""), class = ""factor""), Age = c(50L, 
50L, 63L, 63L, 83L, 55L, 55L, 72L, 81L, 42L, 42L, 86L, 86L, 74L, 
74L, 74L, 74L, 50L, 74L, 74L, 73L, 73L, 67L, 67L, 79L, 79L, 71L, 
70L, 70L, 75L, 75L, 68L, 68L, 73L, 73L, 79L, 69L, 79L, 79L, 61L, 
61L, 74L, 74L, 74L, 74L, 77L, 77L, 73L, 68L, 68L, 76L, 76L, 84L, 
84L, 78L, 78L, 77L, 77L, 71L, 71L, 55L, 55L, 67L, 67L, 88L, 88L, 
77L, 77L, 78L, 78L, 84L, 84L, 71L, 71L, 69L, 69L, 67L, 67L, 41L, 
41L, 78L, 78L, 80L, 80L, 76L, 66L, 76L, 76L, 73L, 73L, 74L, 74L, 
64L, 64L, 46L, 46L, 72L, 72L, 78L, 78L, 67L, 67L, 74L, 47L, 47L, 
79L, 79L, 79L, 78L, 78L, 81L, 77L, 79L, 79L, 67L, 67L, 76L, 76L, 
70L, 64L, 64L, 70L, 70L, 79L, 79L, 74L, 74L, 82L, 82L, 83L, 69L, 
69L, 76L, 69L, 69L, 58L, 58L, 83L, 83L, 83L, 68L, 68L, 69L, 69L, 
79L, 79L, 79L, 66L, 66L, 70L, 70L, 65L, 65L, 65L, 65L, 72L, 87L, 
87L, 57L, 57L, 80L, 80L, 76L, 76L, 63L, 63L, 64L, 64L, 78L, 78L, 
60L, 76L, 80L, 75L, 75L, 90L, 90L, 78L, 78L, 74L, 74L, 69L, 69L, 
80L, 80L, 73L, 73L, 71L, 71L, 56L, 56L, 76L, 76L, 87L, 87L, 38L, 
38L, 61L, 61L, 78L, 78L), SBR = c(12.061, 11.447, 9.403, 9.136, 
9.747, 8.648, 7.934, 7.914, 9.349, 11.224, 10.433, 4.897, 5.823, 
8.683, 8.692, 13.018, 13.386, 7.817, 7.384, 7.518, 11.091, 11.028, 
8.372, 8.497, 10.751, 10.488, 4.347, 2.593, 2.203, 6.461, 7.272, 
4.581, 4.593, 10.31, 9.004, 10.362, 10.307, 9.266, 10.163, 9.24, 
8.732, 8.449, 7.823, 10.427, 10.669, 8.695, 8.729, 8.653, 12.299, 
12.158, 11.748, 11.19, 8.431, 8.717, 8.253, 8.412, 6.911, 6.805, 
9.468, 11.413, 6.603, 7.697, 7.762, 7.097, 10.607, 8.162, 5.419, 
5.575, 7.007, 6.974, 8.708, 8.419, 9.47, 8.42, 8.229, 8.027, 
5.294, 4.628, 11.475, 10.328, 7.905, 8.491, 10.724, 9.02, 9.095, 
5.754, 9.805, 7.332, 6.669, 5.118, 12.443, 11.972, 13.309, 13.906, 
14.963, 15.119, 6.465, 6.38, 6.949, 6.064, 6.541, 6.648, 3.542, 
11.148, 11.918, 9.743, 9.795, 6.103, 6.025, 3.917, 7.304, 7.628, 
8.092, 7.347, 9.051, 8.206, 10.697, 10.286, 4.564, 10.62, 9.84, 
9.105, 7.998, 6.437, 5.707, 6.949, 6.315, 6.165, 6.68, 8.86, 
8.326, 8.6, 7.776, 5.193, 5.456, 11.864, 11.381, 6.385, 10.972, 
9.87, 9.645, 7.738, 10.096, 9.667, 9.687, 8.255, 4.606, 8.738, 
8.519, 7.002, 6.288, 10.425, 10.303, 8.278, 8.342, 6.657, 6.111, 
5.928, 13.06, 12.747, 5.545, 5.845, 9.338, 9.534, 9.635, 8.716, 
7.765, 7.254, 7.517, 7.317, 7.335, 5.628, 4.864, 7.1, 7.02, 6.734, 
5.622, 7.167, 7.391, 6.443, 6.874, 8.373, 7.573, 5.701, 6.355, 
6.884, 6.296, 9.097, 9.645, 7.068, 7.252, 6, 5.794, 8.074, 9.267, 
12.584, 10.723, 9.39, 9.165, 9.635, 8.814), Diagnosis = structure(c(2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""A"", ""N""), class = ""factor""), 
    fit = c(10.1654358296645, 10.1654358296645, 9.07109655193284, 
    9.07109655193284, 7.38749766311491, 9.74453610746002, 9.74453610746002, 
    8.31347705196477, 7.5558575519967, 10.8388753851917, 10.8388753851917, 
    7.13495782979222, 7.13495782979222, 8.14511716308298, 8.14511716308298, 
    8.14511716308298, 8.14511716308298, 10.1654358296645, 8.14511716308298, 
    8.14511716308298, 8.22929710752388, 8.22929710752388, 8.73437677416926, 
    8.73437677416926, 7.7242174408785, 7.7242174408785, 8.39765699640567, 
    8.48183694084657, 8.48183694084657, 8.06093721864208, 8.06093721864208, 
    8.65019682972836, 8.65019682972836, 8.22929710752388, 8.22929710752388, 
    7.7242174408785, 8.56601688528746, 7.7242174408785, 7.7242174408785, 
    9.23945644081464, 9.23945644081464, 8.14511716308298, 8.14511716308298, 
    8.14511716308298, 8.14511716308298, 7.89257732976029, 7.89257732976029, 
    8.22929710752388, 8.65019682972836, 8.65019682972836, 7.97675727420119, 
    7.97675727420119, 7.30331771867401, 7.30331771867401, 7.80839738531939, 
    7.80839738531939, 7.89257732976029, 7.89257732976029, 8.39765699640567, 
    8.39765699640567, 9.74453610746002, 9.74453610746002, 8.73437677416926, 
    8.73437677416926, 6.96659794091043, 6.96659794091043, 7.89257732976029, 
    7.89257732976029, 7.80839738531939, 7.80839738531939, 7.30331771867401, 
    7.30331771867401, 8.39765699640567, 8.39765699640567, 8.56601688528746, 
    8.56601688528746, 8.73437677416926, 8.73437677416926, 10.9230553296326, 
    10.9230553296326, 7.80839738531939, 7.80839738531939, 7.6400374964376, 
    7.6400374964376, 7.97675727420119, 8.81855671861015, 7.97675727420119, 
    7.97675727420119, 8.22929710752388, 8.22929710752388, 8.14511716308298, 
    8.14511716308298, 8.98691660749195, 8.98691660749195, 10.5021556074281, 
    10.5021556074281, 8.31347705196477, 8.31347705196477, 7.80839738531939, 
    7.80839738531939, 8.73437677416926, 8.73437677416926, 8.14511716308298, 
    10.4179756629872, 10.4179756629872, 7.7242174408785, 7.7242174408785, 
    7.7242174408785, 7.80839738531939, 7.80839738531939, 7.5558575519967, 
    7.89257732976029, 7.7242174408785, 7.7242174408785, 8.73437677416926, 
    8.73437677416926, 7.97675727420119, 7.97675727420119, 8.48183694084657, 
    8.98691660749195, 8.98691660749195, 8.48183694084657, 8.48183694084657, 
    7.7242174408785, 7.7242174408785, 8.14511716308298, 8.14511716308298, 
    7.47167760755581, 7.47167760755581, 7.38749766311491, 8.56601688528746, 
    8.56601688528746, 7.97675727420119, 8.56601688528746, 8.56601688528746, 
    9.49199627413733, 9.49199627413733, 7.38749766311491, 7.38749766311491, 
    7.38749766311491, 8.65019682972836, 8.65019682972836, 8.56601688528746, 
    8.56601688528746, 7.7242174408785, 7.7242174408785, 7.7242174408785, 
    8.81855671861015, 8.81855671861015, 8.48183694084657, 8.48183694084657, 
    8.90273666305105, 8.90273666305105, 8.90273666305105, 8.90273666305105, 
    8.31347705196477, 7.05077788535132, 7.05077788535132, 9.57617621857822, 
    9.57617621857822, 7.6400374964376, 7.6400374964376, 7.97675727420119, 
    7.97675727420119, 9.07109655193284, 9.07109655193284, 8.98691660749195, 
    8.98691660749195, 7.80839738531939, 7.80839738531939, 9.32363638525553, 
    7.97675727420119, 7.6400374964376, 8.06093721864208, 8.06093721864208, 
    6.79823805202863, 6.79823805202863, 7.80839738531939, 7.80839738531939, 
    8.14511716308298, 8.14511716308298, 8.56601688528746, 8.56601688528746, 
    7.6400374964376, 7.6400374964376, 8.22929710752388, 8.22929710752388, 
    8.39765699640567, 8.39765699640567, 9.66035616301912, 9.66035616301912, 
    7.97675727420119, 7.97675727420119, 7.05077788535132, 7.05077788535132, 
    11.1755951629553, 11.1755951629553, 9.23945644081464, 9.23945644081464, 
    7.80839738531939, 7.80839738531939), lwr = c(5.90999794584117, 
    5.90999794584117, 4.85411038352648, 4.85411038352648, 3.16383967274129, 
    5.5078318233643, 5.5078318233643, 4.10340735387365, 3.33646831235876, 
    6.54339594841324, 6.54339594841324, 2.90340734886211, 2.90340734886211, 
    3.93437871118446, 3.93437871118446, 3.93437871118446, 3.93437871118446, 
    5.90999794584117, 3.93437871118446, 3.93437871118446, 4.01899333351728, 
    4.01899333351728, 4.52246825860982, 4.52246825860982, 3.50829974380329, 
    3.50829974380329, 4.18762073879345, 4.27163348349591, 4.27163348349591, 
    3.84956354899072, 3.84956354899072, 4.43905717652239, 4.43905717652239, 
    4.01899333351728, 4.01899333351728, 3.50829974380329, 4.35544561188301, 
    3.50829974380329, 3.50829974380329, 5.0187327527967, 5.0187327527967, 
    3.93437871118446, 3.93437871118446, 3.93437871118446, 3.93437871118446, 
    3.67933199647321, 3.67933199647321, 4.01899333351728, 4.43905717652239, 
    4.43905717652239, 3.76454793766266, 3.76454793766266, 3.07722713717148, 
    3.07722713717148, 3.59391587315661, 3.59391587315661, 3.67933199647321, 
    3.67933199647321, 4.18762073879345, 4.18762073879345, 5.5078318233643, 
    5.5078318233643, 4.52246825860982, 4.52246825860982, 2.72879720476565, 
    2.72879720476565, 3.67933199647321, 3.67933199647321, 3.59391587315661, 
    3.59391587315661, 3.07722713717148, 3.07722713717148, 4.18762073879345, 
    4.18762073879345, 4.35544561188301, 4.35544561188301, 4.52246825860982, 
    4.52246825860982, 6.62171309179384, 6.62171309179384, 3.59391587315661, 
    3.59391587315661, 3.42248381273475, 3.42248381273475, 3.76454793766266, 
    4.60567896791123, 3.76454793766266, 3.76454793766266, 4.01899333351728, 
    4.01899333351728, 3.93437871118446, 3.93437871118446, 4.77149984958079, 
    4.77149984958079, 6.22823093408572, 6.22823093408572, 4.10340735387365, 
    4.10340735387365, 3.59391587315661, 3.59391587315661, 4.52246825860982, 
    4.52246825860982, 3.93437871118446, 6.14896197954192, 6.14896197954192, 
    3.50829974380329, 3.50829974380329, 3.50829974380329, 3.59391587315661, 
    3.59391587315661, 3.33646831235876, 3.67933199647321, 3.50829974380329, 
    3.50829974380329, 4.52246825860982, 4.52246825860982, 3.76454793766266, 
    3.76454793766266, 4.27163348349591, 4.77149984958079, 4.77149984958079, 
    4.27163348349591, 4.27163348349591, 3.50829974380329, 3.50829974380329, 
    3.93437871118446, 3.93437871118446, 3.25025350300496, 3.25025350300496, 
    3.16383967274129, 4.35544561188301, 4.35544561188301, 3.76454793766266, 
    4.35544561188301, 4.35544561188301, 5.26417374160422, 5.26417374160422, 
    3.16383967274129, 3.16383967274129, 3.16383967274129, 4.43905717652239, 
    4.43905717652239, 4.35544561188301, 4.35544561188301, 3.50829974380329, 
    3.50829974380329, 3.50829974380329, 4.60567896791123, 4.60567896791123, 
    4.27163348349591, 4.27163348349591, 4.68868944268447, 4.68868944268447, 
    4.68868944268447, 4.68868944268447, 4.10340735387365, 2.81620086292774, 
    2.81620086292774, 5.34559069482765, 5.34559069482765, 3.42248381273475, 
    3.42248381273475, 3.76454793766266, 3.76454793766266, 4.85411038352648, 
    4.85411038352648, 4.77149984958079, 4.77149984958079, 3.59391587315661, 
    3.59391587315661, 5.10074511800645, 3.76454793766266, 3.42248381273475, 
    3.84956354899072, 3.84956354899072, 2.55340019612735, 2.55340019612735, 
    3.59391587315661, 3.59391587315661, 3.93437871118446, 3.93437871118446, 
    4.35544561188301, 4.35544561188301, 3.42248381273475, 3.42248381273475, 
    4.01899333351728, 4.01899333351728, 4.18762073879345, 4.18762073879345, 
    5.42680991723477, 5.42680991723477, 3.76454793766266, 3.76454793766266, 
    2.81620086292774, 2.81620086292774, 6.85553891121225, 6.85553891121225, 
    5.0187327527967, 5.0187327527967, 3.59391587315661, 3.59391587315661
    ), upr = c(14.4208737134878, 14.4208737134878, 13.2880827203392, 
    13.2880827203392, 11.6111556534885, 13.9812403915557, 13.9812403915557, 
    12.5235467500559, 11.7752467916346, 15.1343548219701, 15.1343548219701, 
    11.3665083107223, 11.3665083107223, 12.3558556149815, 12.3558556149815, 
    12.3558556149815, 12.3558556149815, 14.4208737134878, 12.3558556149815, 
    12.3558556149815, 12.4396008815305, 12.4396008815305, 12.9462852897287, 
    12.9462852897287, 11.9401351379537, 11.9401351379537, 12.6076932540179, 
    12.6920403981972, 12.6920403981972, 12.2723108882934, 12.2723108882934, 
    12.8613364829343, 12.8613364829343, 12.4396008815305, 12.4396008815305, 
    11.9401351379537, 12.7765881586919, 11.9401351379537, 11.9401351379537, 
    13.4601801288326, 13.4601801288326, 12.3558556149815, 12.3558556149815, 
    12.3558556149815, 12.3558556149815, 12.1058226630474, 12.1058226630474, 
    12.4396008815305, 12.8613364829343, 12.8613364829343, 12.1889666107397, 
    12.1889666107397, 11.5294083001765, 11.5294083001765, 12.0228788974822, 
    12.0228788974822, 12.1058226630474, 12.1058226630474, 12.6076932540179, 
    12.6076932540179, 13.9812403915557, 13.9812403915557, 12.9462852897287, 
    12.9462852897287, 11.2043986770552, 11.2043986770552, 12.1058226630474, 
    12.1058226630474, 12.0228788974822, 12.0228788974822, 11.5294083001765, 
    11.5294083001765, 12.6076932540179, 12.6076932540179, 12.7765881586919, 
    12.7765881586919, 12.9462852897287, 12.9462852897287, 15.2243975674713, 
    15.2243975674713, 12.0228788974822, 12.0228788974822, 11.8575911801404, 
    11.8575911801404, 12.1889666107397, 13.0314344693091, 12.1889666107397, 
    12.1889666107397, 12.4396008815305, 12.4396008815305, 12.3558556149815, 
    12.3558556149815, 13.2023333654031, 13.2023333654031, 14.7760802807705, 
    14.7760802807705, 12.5235467500559, 12.5235467500559, 12.0228788974822, 
    12.0228788974822, 12.9462852897287, 12.9462852897287, 12.3558556149815, 
    14.6869893464325, 14.6869893464325, 11.9401351379537, 11.9401351379537, 
    11.9401351379537, 12.0228788974822, 12.0228788974822, 11.7752467916346, 
    12.1058226630474, 11.9401351379537, 11.9401351379537, 12.9462852897287, 
    12.9462852897287, 12.1889666107397, 12.1889666107397, 12.6920403981972, 
    13.2023333654031, 13.2023333654031, 12.6920403981972, 12.6920403981972, 
    11.9401351379537, 11.9401351379537, 12.3558556149815, 12.3558556149815, 
    11.6931017121067, 11.6931017121067, 11.6111556534885, 12.7765881586919, 
    12.7765881586919, 12.1889666107397, 12.7765881586919, 12.7765881586919, 
    13.7198188066704, 13.7198188066704, 11.6111556534885, 11.6111556534885, 
    11.6111556534885, 12.8613364829343, 12.8613364829343, 12.7765881586919, 
    12.7765881586919, 11.9401351379537, 11.9401351379537, 11.9401351379537, 
    13.0314344693091, 13.0314344693091, 12.6920403981972, 12.6920403981972, 
    13.1167838834176, 13.1167838834176, 13.1167838834176, 13.1167838834176, 
    12.5235467500559, 11.2853549077749, 11.2853549077749, 13.8067617423288, 
    13.8067617423288, 11.8575911801404, 11.8575911801404, 12.1889666107397, 
    12.1889666107397, 13.2880827203392, 13.2880827203392, 13.2023333654031, 
    13.2023333654031, 12.0228788974822, 12.0228788974822, 13.5465276525046, 
    12.1889666107397, 11.8575911801404, 12.2723108882934, 12.2723108882934, 
    11.0430759079299, 11.0430759079299, 12.0228788974822, 12.0228788974822, 
    12.3558556149815, 12.3558556149815, 12.7765881586919, 12.7765881586919, 
    11.8575911801404, 11.8575911801404, 12.4396008815305, 12.4396008815305, 
    12.6076932540179, 12.6076932540179, 13.8939024088035, 13.8939024088035, 
    12.1889666107397, 12.1889666107397, 11.2853549077749, 11.2853549077749, 
    15.4956514146983, 15.4956514146983, 13.4601801288326, 13.4601801288326, 
    12.0228788974822, 12.0228788974822)), .Names = c(""Chi"", ""Sex"", 
""Age"", ""SBR"", ""Diagnosis"", ""fit"", ""lwr"", ""upr""), row.names = c(NA, 
201L), class = ""data.frame"")
</code></pre>

<p>I plot SBR v Age for each Sex using <code>ggplot2</code></p>

<pre><code>p &lt;- ggplot(sbr_with_pred, aes(x=Age, y=SBR)) + geom_point(aes(col=Sex), 
                                                           shape=19, alpha=0.4) + 
            geom_smooth(aes(col=Sex),method = 'lm', se=FALSE,linetype=2) + 
            geom_ribbon(aes(y = fit, ymin = lwr, ymax = upr, fill = 'prediction'), 
                        linetype =2,alpha = 0.1) + 
            scale_fill_manual('Interval', values = c('blue')) + theme_bw() + 
            theme(legend.position = ""right"") + 
            scale_y_continuous(limits = c(-3,15.5),breaks = c(0,5,10,15)) + 
            scale_color_manual(""Sex"", values = c('red','blue'))
</code></pre>

<p>which gives the following</p>

<p><img src=""http://i.stack.imgur.com/j3DxP.png"" alt=""enter image description here""></p>

<p>I can get the equation of each regression fit easy enough</p>

<pre><code>lm(formula = SBR ~ Age, data = subset(sbr_with_pred, Sex == ""F""))
lm(formula = SBR ~ Age, data = subset(sbr_with_pred, Sex == ""M""))
</code></pre>

<p>However how do I test whether or not they are significantly different (which they are not). I think analysis of covariance is the appropriate test but I do not know how to implement this in R</p>
"
"0.0316069770620507","0.0311891430775903"," 66253","<p>I am running a Cox regression analysis in R where both my predictors are binary (<code>Animal_Number</code>: <em>single</em> vs. <em>paired</em>; and <code>Treatment</code>: <em>control</em> vs. <em>exposed</em>). I therefore have four groups (<em>single control</em>, <em>paired control</em>, <em>single exposed</em> and <em>paired exposed</em>). When I run my Cox regression, I get the following output (excerpt):</p>

<pre><code>                                        coef exp(coef) se(coef)      z Pr(&gt;|z|)   
Animal_NumberPaired                   0.2617    1.2991   0.3303  0.792  0.42829   
TreatmentExposed                     -1.4031    0.2458   0.4715 -2.976  0.00292 **
Animal_NumberPaired:TreatmentExposed  1.0246    2.7861   0.5817  1.762  0.07814 .
</code></pre>

<p>As I understand it, the three lines presented here are simply the levels of the factors being compared to the baseline experimental group, which was set as <em>single control</em>, i.e., the first line is comparing single control animals with paired control animals.</p>

<p>If this is the case, how then does one find information (<code>coefs</code>, <code>z</code>, <code>p-value</code>, etc.) for purely the effect of <code>Animal_Number</code>, <code>Treatment</code> and their interaction? I can seem to do this in SPSS, and get the following:</p>

<pre><code>                           B    SE      Wald    df  Sig.   Exp(B)
Animal_Number             .244  .331    .545    1   .461   1.276
Treatment               -1.283  .472    7.399   1   .007    .277
Animal_Number*Treatment   .959  .582    2.717   1   .099   2.609
</code></pre>

<p>But I want to be able to do it in R. I considered using likelihood ratio tests, to compare full and reduced models, but then information about the coefficients and z is lost. </p>
"
"0.0774209661138764","0.0763974860547543"," 67275","<p>I'm reading a paper that does not report the coefficients from two OLS regressions. In both cases there is 1 response variable and 1 predictor variable.  The predictor variable is the same in both cases. I know the subject matter of the paper well, which leads me to believe that the means of the slopes are almost certainly between 0 and 1. Although the slopes for these two regressions are not reported, the author does report that neither slope is significantly different from 0 (p â‰¥ 0.05).</p>

<p>If neither slope is different from 0, but both slopes are between 0 and 1, could the slopes be different from each other?</p>

<p>To try to figure this out, I did a quick test in R.  I used two slopes that were very different (0.99 and 0.01), but chose s.e.'s for each that would make them barely ""insignificant"". To compare the slopes, I used the formula from the answer to <a href=""http://stats.stackexchange.com/questions/55501/test-a-significant-difference-between-two-slope-values"">THIS</a> question.</p>

<pre><code>pnorm( 
  (0.99 - 0.01)/ #difference between means
  sqrt(0.61^2 + 0.0062^2), #sqrt of sum squares of s.e.'s
  lower.tail=FALSE
)
</code></pre>

<p>OK, so this quick-and-dirty test suggests that the two slopes in the author's analysis can't be different.  </p>

<p>Is it necessarily true that if two slopes are between 0 and 1, and neither different from 0, that they cannot be significantly different from each other?</p>
"
"0.0632139541241014","0.0467837146163854"," 67312","<p>I am trying to analyse a dataset with at minimum 50 explanatory variables coded as 0 and 1 for presence/absence and a binary response variable (case/control). The goal is to see how the variables can predict the separation between case and control.</p>

<p>As there are more variables than observations I applied a partial least square discriminant analysis (PLS-DA) using the package mixOmics in R. However, when I want to test the significance of the analysis with PLSDA.test (package RVAideMemoire) I get a lot of warnings :</p>

<pre><code>1: In pls(X, ind.mat, ncomp = ncomp, mode = ""regression"",  ... :
 Zero- or near-zero variance predictors. 
 Reset predictors matrix to not near-zero variance predictors.
 See $nzv for problematic predictors.
</code></pre>

<p>I guess the problem with near-zero variance results from the 0/1 coding of the predictor variables. I tried to convert the variables to factors, but this doesn't help. Is there a different analysis more suitable? How can I deal with presence/ absence variables as predictors?</p>
"
"0.0446990156267674","0.0441081091391231"," 67662","<p>I am a research scholar and monitoring phenological events of timber line at a Himalayan region from past 4 years. During data analysis I found a research paper ""Estimation and comparison of flowering curve"" similar to my work.</p>

<p>In this paper the <code>R</code> package <code>bbmle</code> was used and five parameters $(\beta_{0},\ldots,\beta_{4})$ which describe: (i) the height; (ii) the peak date; (iii) the range; (iv) the symmetry; and (v) the peakedness of the regression curve were calculated. I also read the appendix table and followed the code to estimate these parameter but as a newbie I failed to do the calculations.</p>

<p>The full appendix table is here:
<img src=""http://i.stack.imgur.com/wOkDn.jpg"" alt=""Appendix table""></p>

<p>I am also posting the <code>startvals</code> for the year 2007 as mentioned in the appendix. Look at the values in figure below for year 2007.
<img src=""http://i.stack.imgur.com/ZPtKH.jpg"" alt=""Start values for year 2007""></p>

<p>My main problem is how to calculate <code>startvals</code> for example data of year 2007 as shown in figure where $(\beta_{0},\ldots,\beta_{4})$ describe: </p>

<ol>
<li>the height</li>
<li>the peak date</li>
<li>the range</li>
<li>the symmetry</li>
<li>the peakedness</li>
</ol>

<p>of the regression curve.</p>
"
"0.104828483672192","0.0940388046555249"," 67790","<p>I am dealing with a unreplicated factorial design. I have some illustrative examples but I need to simulate some unreplicated factorial designs. I do not how and what to use. Can $R$ handle this?</p>

<p>For example, I would like to analyse a $2^{4}$ factorial design (factors are A, B, C and D) with only one run and 15 contrasts. I have a single column for response. I would like to compare some methods in the literature to see which method detects active effects better. Thus, I set the active effects to have the same magnitude of $1.5\sigma$ and I would like to generate $100$ response vectors using errors that are i.i.d. with $\mathcal N(0 ,1)$. My true model has four active effects and I would like to simulate $100$ response vectors using this true model $y=3+1.5A+1.5B+1.5C+1.5BC$. But I do not know how to generate data like this using R. </p>

<p><img src=""http://i.stack.imgur.com/e7F8i.png"" alt=""An example from Montgomery for $2^4$ unreplicated factorial design""></p>

<hr>

<p>Thanks gung for your reply. I just wrote a simple code before I saw your answer here. I think, I need to build up a bit more R knowledge. Anyway, here it is:</p>

<p>For the analysis of unreplicated factorial designs with $k$ factors and $p=2^{k}-1$ factorial effects (the main effects and interactions), the following model is generally used</p>

<p>\begin{equation}
y=\sum\limits_{i=0}^{p}x_{i}\beta_{i}+\varepsilon_{i}
\end{equation}</p>

<p>So, Firstly I introduced my sign table for $2^{4}$ and $\beta$ coefficients of so-called active effects. </p>

<p>Sign table consists of rows (runs) and columns (contrasts with general mean).
<img src=""http://i.stack.imgur.com/e7F8i.png"" alt=""enter image description here""></p>

<p>And then, I created my regression equation with magnitudes of active effects and zeros of remaining inactive effects. My simulated model, for example, was $y=3+1.5A+1.5B+1.5C+1.5BC$.
<img src=""http://i.stack.imgur.com/9N4GA.png"" alt=""enter image description here""></p>

<p>And then, I run the code below</p>

<pre><code>x=read.csv(""sign2.txt"", header=TRUE)
sign= as.matrix(x)
is.matrix(sign)

y=read.csv(""beta2.txt"", header=TRUE)
beta= as.matrix(y)
is.matrix(beta)

signt=t(sign)

bs=t(beta %*% signt)

epsilon=matrix( rnorm(16*1,mean=0,sd=1), 16, 1) 

response=bs+epsilon
</code></pre>

<p>However, unfortunately, it's for one simulation. I will put a loop command to run the simulation n-times.</p>
"
"0.204892036268375","0.206885370242155"," 67873","<p><strong>TLDR</strong>: How can I perform inference for the between group differences in a possibly logistic growth with time in the presence of outliers, unequal measurement times and frequency, bounded measurements and possible random effects on individual and per study level?</p>

<p>I am attempting to analyse a dataset where measurements for individuals were made at different time points. Measurements start low at time 0 and follow (very roughly) a logistic growth pattern with time. I am trying to establish if there are differences between two groups of individuals. The analysis is complicated by the following factors:</p>

<ul>
<li>The effect of time is non-linear, so either a non-linear logistic regression (biologically plausible, but not particularly well fitting) or a non-parametric regression seem appropriate</li>
<li>There are massive outliers, so regression using the sum of squared residuals seems off the table. Quantile regression seems appropriate.</li>
<li>Random effects may be appropriate on a per individual and per study level. Mixed effects models seems appropriate.</li>
<li>Measurement times, number of available measurements and end of monitoring differ between individuals. Survival analysis techniques seem appropriate. Possibly also applying weights equal to 1 / number of observations for individual.</li>
<li>Measurements are bounded below at 0 and while there is no obvious boundary above, arbitrarily high measurements seem biologically implausible. However, quite a few individuals have some measurements of zero (partly due to the measurement accuracy of the device).</li>
<li>A few models I tried so far failed to fit, usually with an unhelpful error related to the numerical procedure. This leads me to believe that I will need a reasonably robust method able to deal with this somewhat ugly dataset.</li>
<li>Finally, I want to produce inference of the form ""group 1 has faster growth than group 2"" or ""group 1 has a higher asymptotic level than group 2"".</li>
</ul>

<p>What I have tried so far (all in R) - I was aware that most of the below are not particularly appropriate for the dataset, but I wanted to see which models could actually be fitted without numerical errors:</p>

<ul>
<li>Non-parametric regression using crs in the crs package. Nicely produces a curve reasonably close to logistic growth for most of the time period with some strange behavious toward the end of the monitoring period (where there are fewer observations). Using individuals as fixed effects reveals some outliers. Using the variable of interest as fixed effects shows some difference. However, I am not sure if there is any way to assess fits and do inference on a model this complex.</li>
<li>Non-linear mixed effects regression using nlme in package nlme and SSlogis. Gradually building up the model with update() works reasonably well. Getting too complex with the fixed effects or the random effects leads to convergence failure. Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further. Edit: I have recently become aware that it is possible to specify autocorrelated residuals in nlme. However, at the moment it seems I cannot even get fixed weights to work. Advice on the correct syntax is welcome.</li>
<li>Non-linear mixed effects regression using nlmer in package LME4 and a custom likelihood for the logistic growth model. Works fairly well, but standard errors on the fixed effects get massive, probably due to the outliers. I also have the slight suspicion that some of the models fail to fit without error, as I sometimes get tiny random effects (about 10^10 smaller than with slightly simpler models). Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further.</li>
<li>Non-linear quantile regression using nlrq in package quantreg and SSlogis. Fits reliably and quickly, but percentile lines intersect. This means that an area containing 90% of the data is not fully contained in an area containing 95% of the data.</li>
<li>Non-parametric quantile regression using the LMS method with package VGAM. Even trivial models failed with obscure errors using this dataset. I believe the number of zeros in the dataset and / or the large range of the data while also getting close to zero may be an issue.</li>
<li>To complete this list, I should probably also mention the lqmm package for Linear Quantile Mixed Models, which I have not used yet. While the package cannot use non-linear models as far as I know, transforming the time variable may produce something reasonably close.</li>
</ul>

<p>I would appreciate feedback if these or any other method might be used to produce reasonably robust inference in this scenario. Maybe regression is not needed at all and another, possibly simpler method is sufficient. I'd be happy to provide an example dataset, if required, but think this question might also be of interest beyond the current dataset.</p>
"
"0.0446990156267674","0.0441081091391231"," 68183","<p>I was asked to do some regression analysis on anonymised data, which I don't have yet. It will have a few categorical inputs and ~100 boolean outputs, something like this:</p>

<pre><code>Category   Type         Size       Blood pressure     Y1   Y2    Y3  ....   Y99  
-------------------------------------------------------------------------------
cat        persian      small      high               1    0     0   ....   1
cat        persian      big        low                0    1     1   ....   0
dog        wolfhound    big        normal             1    1     0   ....   0
...        ...          ...        ...                      ......   
duck       scoter       small      above normal       0    1     1   ....   1  
</code></pre>

<p>(I just made up the meanings here). The number of samples in the real data set will be about a million, without missing values. The task is to predict the output, eg <code>P(Y86 = 1 | {cat, siamese, big, low})</code> and generally understand the relationship between inputs and outputs. </p>

<p>My questions: </p>

<ol>
<li>which approaches are worth trying, what are the pros/cons?</li>
<li>which <code>R</code> packages can be of help?</li>
</ol>
"
"NaN","NaN"," 68291","<p>I know there is a series of regression diagnostics procedures (correlation, beta, residual, etc.) before, during, and after regression analysis. But, is there any common procedure to follow for cluster analysis (like, Ward)? What are the R commands? Thanks!</p>
"
"0.0774209661138764","0.0763974860547543"," 68388","<p>I have a great interest in learning new methods(at least to me) of variable selection in regards to binary logistic regression when I am working with over 500 potential predictor variables and have the duty of selecting 8 to 15 variables to build a parsimonious predictive model without using the notorious stepwise techniques. </p>

<p>With that being said, I was wondering if anyone has any experience using <code>proc factor</code> for binary logistic regression variable selection? I assume my factors will correlate, and thus I will use <code>promax</code> rotation, but with the results of the Exploratory Factor Analysis (EFA), I will simply retain the variable within each factor that has the highest loading on its own factor (latent variables models would confuse the hell out of the end-user of 99.999% of my models!) for further variable reduction through another technique such as <code>randomForest</code> until the number of variables is small enough to build a model that has fewer than 15 variables in it. </p>

<p>Does anyone have any thoughts in regards to this process? Any suggested readings or input would be greatly appreciated. Thanks!</p>
"
"0.0893980312535348","0.0882162182782462"," 68553","<p>I am trying to run a logistic regression analysis in R using the speedglm package. 
The data is CNVs (a type of genetic variant), whether that CNV occurred in a case or control and wether genes in a pathway is hit by the CNV or not (Pathway.hit), and how many genes were hit by the CNV that were not in the pathway (Pathway.out).
I run two models with and without the Pathway.hit covariate and compare to see if a pathway is preferentially hit by cases vs controls.  </p>

<p>the models and comparison of said are as follows:</p>

<pre><code>fit1 = speedglm(status~size+Pathway.out, data=cnvs, family=binomial('logit'))
fit2 = speedglm(status~size+Pathway.out+Pathway.hit, data=cnvs,family=binomial('logit'))
P.anova = 1-pchisq(abs(fit1$deviance - fit2$deviance), abs(fit1$df - fit2$df))
</code></pre>

<p>It seems to work okay for most data I throw at it, but in a few cases I get the error:</p>

<pre><code>Error in solve.default(XTX, XTz, tol = tol.solve) : 
  system is computationally singular: reciprocal condition number = 1.87978e-16
</code></pre>

<p>After some googling around I think I found what's causing the problem:</p>

<pre><code>by(cnvs$Pathway.hit, cnvs$status, summary)
cnvs$status: 1 (controls)
        0     1 
    13333     0 
    ------------------------------------ 
    cnvs$status: 2 (cases)
    0     1 
10258     2 
</code></pre>

<p>So here there no observations in controls and only 2 in cases. </p>

<p>If I use with normal glm method however, then it does not throw an error (but that of course doesn't necessarily mean the results will be meaningful). The reason I am using the speedglm package is that I have approximately 16,000 of these analyses to run, and using the base glm function for all 16,000 takes about 20 hours, where as I think speedglm can reduce it down to 8 or so.</p>

<p>So my question is, should I ignore those analyses which throw an error and list the results as NA as there were too few observations, or when speed glm fails should I retry with normal glm? In the above example there are 2 observations of the covariate in cases, but 0 in controls. Might this not be interesting? Would the analysis also fail if there were 0 in controls and 20 in cases - that would certainly be interesting would it not?</p>

<p>Thanks for the help in advance,
Cheers,
Davy</p>
"
"0.070675349274022","0.0697410440814588"," 69088","<p>For those of you familiar with <a href=""http://en.wikipedia.org/wiki/Exploratory_factor_analysis"" rel=""nofollow"">Exploratory Factor Analysis</a> (EFA) and <a href=""http://en.wikipedia.org/wiki/Random_forest"" rel=""nofollow"">Random Forest</a> (RF), I have recently had an idea of combining these two methods to reduce the number of potential predictor variables for use in a parsimonious binary logistic regression model. For the purposes of this post, assume large <em>n</em> (200k or more) and 1000 potential predictor variables.</p>

<p>To employ this idea, the first step would be to perform an EFA with all potential predictor variables using <code>proc varclus</code>. Additionally, using <code>randomForest</code> to rank all potential predictor variables by <code>IncNodePurity</code> (<a href=""http://en.wikipedia.org/wiki/Gini_coefficient"" rel=""nofollow"">Gini Index</a>). </p>

<p>After these two methods are independently used, I propose retaining the variable with the largest <code>IncNodePurity</code> (from RF) within each factor (from EFA).</p>

<p>Does anyone have any thoughts/concerns with this methodology (or lack thereof) for feature selection? I am aware that this ""picking and choosing"" of methods may be complete garbage, but I had this random thought and wanted to share. Thanks!</p>
"
"0.0364965934300906","0.0540211804549215"," 69860","<p>For the purpose of model selection, I am using the Bayes' factor to compare different combinations of predictors in a linear regression model.</p>

<p>I have used the function <code>regressionBF()</code> from the <code>library(BayesFactor)</code>, and I got the following results:</p>

<pre><code># &gt; regressionBF(return ~ FSCR + VAL, data = dataf)

# Bayes factor analysis
# --------------
#[1] FSCR       : 65.17482  Â±0%
#[2] VAL        : 0.1979875 Â±0.02%
#[3] FSCR + VAL : 23.58704  Â±0%

#Against denominator:
#  Intercept only 
</code></pre>

<p>I am not sure how to interpret these results. What do the percentage numbers next to the Bayes' factors mean?
Also, 65 and 23 seem pretty high for a Bayes' factor. How can I interpret that?</p>

<p>Any help would be appreciated. Thanks!</p>
"
"0.0565402794192176","0.0557928352651671"," 69949","<p>To use the scores of the extracted components/factors in a further regression analysis, like mixed effects model regression as predictors to an outcome variable or DV. Would be there any discrepancies in the results coming out of the regression analysis when using scores of the following scenarios (<code>psych</code> package in R):<br>
- <code>none</code> non-rotated principal components<br>
- <code>varimax</code> orthogonally rotated principal components<br>
- <code>promax</code> obliquely rotated principal components<br>
- <code>promax</code> obliquely rotated factors using <code>ml</code> (maximum likelihood) extraction<br>
- <code>promax</code> obliquely rotated factors using <code>pa</code> (principal axes) extraction? </p>

<p>Would it be invalid to use any of the above scores in a further regression analysis? any known issues in this field? or previous similar experiences?    </p>
"
"0.0316069770620507","0.0311891430775903"," 70322","<p>Is it possible to illustrate partial correlation scatter plots for 2 subgroups on the same graph? </p>

<p>e.g. I want to make scatter plots of data controlled for age, differentiated by males or females.</p>

<p>I've tried doing partial regression plots generated by linear regression analysis, but I can't split it by groups. </p>

<p>Options to do it in excel or R would be fine too. Thanks!</p>
"
"0.154965954620265","0.15903404916487"," 71414","<p>I'm trying to use a zero-inflated gamma model (or a gamma 'hurdle' model). The model is a mixture of logistic regression and generalized linear modeling. I can do this analysis in two steps: 1) do a logistic regression against presence/absence data and then 2) Use a generalized linear model with a gamma distribution on the positive values. I'm trying to set the model up where, if y = 0, then E(y) = p, but if y > 0, then E(y) is gamma distributed.</p>

<p>I'm trying to set this up in BUGS/JAGS, because I've seen these models <a href=""http://stats.stackexchange.com/questions/15967/how-can-i-set-up-a-zero-inflated-poisson-in-jags"">worked before for poisson-distributions</a>. I tried to adapt the code from that link into a gamma distribution, but I can't quite seem to get it to work. I know the code won't work because my data have zeroes, and the likelihood function can't be evaluated with zeroes. However, even when restricted to positive values, I get errors for invalid parent values. Even so, I'm not even sure the model is specified correctly.</p>

<p>Here is the model:</p>

<pre><code># For the ones trick
C &lt;- 10000

# for every observation
for(i in 1:N){
    # log-likelihood of the observation from the gamma likelihood
    LogPos[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - N*shape[i]*log(scale[i]) - N*loggam(scale[i])
    #likelihood
    Lpos[i] &lt;- exp(LogPos[i])

    # redefine the shape and rate parameters as a function of the mean and sd
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # mu is a function of MTD: use the inverse link
    #mu[i] &lt;- 1/eta[i]
    mu[i] &lt;- beta0 + beta1*MTD[i]


    # zero-inflated part, where w[i] is the probability of being zero
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # ones trick
    p[i] &lt;- Lpos[i] / C
    ones[i] ~ dbern(p[i])

    # Full likelihood
    Lik[i] &lt;- Lpos[i]*(1 - w[i]) + equals(y[i], 0)*w[i]
  } 

# PRIORS
beta0 ~ dnorm(0, 0.001)
beta1 ~ dnorm(0, 0.001)

gamma0 ~ dnorm(0, 0.001)
gamma1 ~ dnorm(0, 0.001)

sd ~ dunif(0, 100)
</code></pre>

<p>Has anyone set a model up like this or have any advice on how to set it up correctly?</p>

<p><strong>UPDATE</strong></p>

<p>I've tried a new set of code that's similar, but slight different. I still have not gotten it to work</p>

<pre><code>model{

  # For the ones trick
  C &lt;- 10000

  # for every observation
  for(i in 1:N){

    # make a dummy variable that is 0 if y is &lt; 0.0001 and 1 if y &gt; 0.0001. This is essentially a presence
    # absence dummy variable
    z[i] &lt;- step(y[i] - 0.0001)

    # define the logistic regression model, where w is the probability of occurance.
    # use the logistic transformation exp(z)/(1 + exp(z)), where z is a linear function
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # define the gamma regression model for the mean. use the log link the ensure positive, non-zero mu
    mu[i] &lt;- exp(eta[i])
    eta[i] &lt;- beta0 + beta1*MTD[i]

    # redefine the mu and sd of the continuous part into the shape and scale parameters
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # for readability, define the log-likelihood of the gamma here
    logGamma[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i])

    # define the total likelihood, where the likelihood is (1 - w) if y &lt; 0.0001 (z = 0) or
    # the likelihood is w * gammalik if y &gt;= 0.0001 (z = 1). So if z = 1, then the first bit must be
    # 0 and the second bit 1. Use 1 - z, which is 0 if y &gt; 0.0001 and 1 if y &lt; 0.0001
    logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + logGamma[i] )

    # Use the ones trick
    p[i] &lt;- logLik[i] / C
    ones[i] ~ dbern(p[i])
  } 

  # PRIORS
  beta0 ~ dnorm(0, 0.001)
  beta1 ~ dnorm(0, 0.001)

  gamma0 ~ dnorm(0, 0.001)
  gamma1 ~ dnorm(0, 0.001)

  sd ~ dgamma(1, 2)

}
</code></pre>

<p><strong>UPDATE 2</strong></p>

<p>I've gotten it to run by deleting the definition of logGamma[i] and putting it directly into the likelihood function, which now reads:</p>

<pre><code>logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i]) )
</code></pre>

<p>The problem was that JAGS was trying to evaluate the log likelihood at all observations first, resulting in NA's for the 0's. In the new way, it only evaluates it if z = 1 (I think). I'm still having trouble getting the parameter estimates to line up. For example, the gamma's are almost identical to a logistic regression of the same form (hooray!). But the betas are pretty far off from a gamma GLM of the positive values. I don't know if this is normal or not, but I suspect there are still problems with my model specification.</p>
"
"0.0316069770620507","0.0311891430775903"," 72334","<p>I am not a statistician but a Java/R programmer. So even the topic could be wrong. Please bear with me.</p>

<p>I am collecting details from production servers. These details could be the number of active sessions, CPU utilization etc. I draw graphs. So for example, I have a R graph showing Time(x), Web Server hits(left y-axis) and MBytes transferred(right y-axis). Another one is the number of active sessions over time.</p>

<p>I need to understand the statistical growth pattern of this data before and after an event. These are all distributions.</p>

<ol>
<li>How do I go about measuring the growth ? I use R.</li>
<li>How do I understand what causes that growth ? I think this is about regression.</li>
</ol>

<p>The more complex question from my perspective.</p>

<ol>
<li>What is the statistical process to predict future growth in these cases ? I already read Capacity Planning books. This is not about Capacity planning which probably is the next step.</li>
</ol>

<p>I have come across topics like 'Growth Analysis curves and Visualization' but couldn't access any material even after searching.</p>

<p>Mohan</p>
"
"0.122413295785282","0.120795031721987"," 72421","<p>I have data for a network of weather stations across the United States. This gives me a data frame that contains date, latitude, longitude, and some measured value. Assume that data are collected once per day and driven by regional-scale weather (no, we are not going to get into that discussion). </p>

<p>I'd like to show graphically how simultaneously-measured values are correlated across time and space. My goal is to show the regional homogeneity (or lack thereof) of the value that is being investigated. </p>

<h2>Data set</h2>

<p>To start with, I took a group of stations in the region of Massachusetts and Maine. I selected sites by latitude and longitude from an index file that is available on NOAA's FTP site.</p>

<p><img src=""http://i.stack.imgur.com/aZm4N.jpg"" alt=""enter image description here""></p>

<p>Straight away you see one problem: there are lots of sites that have similar identifiers or are very close. FWIW, I identify them using both the USAF and WBAN codes. Looking deeper in to the metadata I saw that they have different coordinates and elevations, and data stop at one site then start at another. So, because I don't know any better, I have to treat them as separate stations. This means the data contains pairs of stations that are very close to each other.</p>

<h2>Preliminary Analysis</h2>

<p>I tried grouping the data by calendar month and then calculating the ordinary least squares regression between different pairs of data. I then plot the correlation between all pairs as a line connecting the stations (below). The line color shows the value of R2 from the OLS fit. The figure then shows how the 30+ data points from January, February, etc. are correlated between different stations in the area of interest. </p>

<p><img src=""http://i.stack.imgur.com/X4YZI.jpg"" alt=""correlation between daily data during each calendar month""></p>

<p>I've written the underlying codes so that the daily mean is only calculated if there are data points every 6-hour period, so data should be comparable across sites.</p>

<h3>Problems</h3>

<p>Unfortunately, there is simply too much data to make sense of on one plot. That can't be fixed by reducing the size of the lines. </p>

<p>I've tried plotting the correlations between the nearest neighbors in the region, but that turns into a mess very quickly. The facets below show the network without correlation values, using $k$ nearest neighbors from a subset of the stations. This figure was just to test the concept.
<img src=""http://i.stack.imgur.com/NWzm2.jpg"" alt=""enter image description here""></p>

<p>The network appears to be too complex, so I think I need to figure out a way to reduce the complexity, or apply some kind of spatial kernel.</p>

<p>I am also not sure what is the most appropriate metric to show correlation, but for the intended (non-technical) audience, the correlation coefficient from OLS might just be the simplest to explain. I may need to present some other information like the gradient or standard error as well.</p>

<h3>Questions</h3>

<p>I'm learning my way into this field and R at the same time, and would appreciate suggestions on:</p>

<ol>
<li>What's the more formal name for what I'm trying to do? Are there some helpful terms that would let me find more literature? My searches are drawing blanks for what must be a common application.</li>
<li>Are there more appropriate methods to show the correlation between multiple data sets separated in space?</li>
<li>... in particular, methods that are easy to show results from visually?</li>
<li>Are any of these implemented in R?</li>
<li>Do any of these approaches lend themselves to automation?</li>
</ol>
"
"0.078223277346843","0.0771891909934654"," 72516","<p>I am examining how English ivy affects the occurrence of a salamander species under cover objects (e.g., logs). Soil moisture is assumed to be the major factor that affect their occurrence. </p>

<p>My hypothesized pathway: The presence/absence of salamanders under cover objects is either a direct consequence of changes in ivy-induced abioitc environment (i.e., drier soil) or an indirect result of changes in prey community that resulted from altered abiotic factors. But, there are multiple factors, other than English ivy, that affect soil moisture.</p>

<p><img src=""http://i.stack.imgur.com/k65Ag.jpg"" alt=""enter image description here""></p>

<p>My questions are:</p>

<ol>
<li><p>I think that a path analysis is most suitable for testing my causal mechanisms. But, given a small sample size (n = 71), is a path analysis appropriate?</p></li>
<li><p>Another potential problem for a path analysis is that the effects of English ivy on soil moisture seem to depend on the other factors (e.g., the number of overstory trees), as shown below. Are there any way to account for such patterns in a path analysis?</p>

<p><img src=""http://i.stack.imgur.com/ArgZm.jpg"" alt=""The relationship between soil moisture and English ivy cover on cover objects (&quot;the number of overstory trees&quot; for the left graph) for different levels of the surrounding overstory trees (&quot;English ivy cover on cover objects&quot; for the left graph""></p></li>
<li><p>Are there any other analyses suitable for testing my hypothesized relationships? I am considering multiple (linear and logistic) regressions, but again my sample size is small <strong>AND</strong> regressions do not reflect my hypothesized causal relationships accurately.</p></li>
</ol>

<p>I am using R, so any recommended code would be greatly helpful (I am a relatively new R user, though). </p>
"
"NaN","NaN"," 72892","<p>Why is the F-test for overall significance (OLS regression analysis) invalid when residuals are heteroscedastic? 
Is there a way to calculate it in a consistent way under heteroscedasticity?
Is there any function in R to accomplish that?</p>
"
"0.0948209311861521","0.0935674292327708"," 73191","<p>For ordinary linear regression with Gaussian noise, it is easy to interpret the significance of a variable.  This is consistent with a partial F test.  The square of the t-test for the second variable equals to the partial F-test statistic, and their p-values are the same.</p>

<p>I wrote simple R codes to confirm this.</p>

<p>Is there something like this for logistic regression?  I thought/hoped that the likelihood ratio test would correspond to this, but no.  What should I do if the variable and the likelihood ratio test (of adding that particular variable) do not have the same (in)significant effect?</p>

<p>I appreciate your time and help,</p>

<pre><code>rm(list=ls(all=TRUE)) 
n = 100   ;       x1 = runif(n,-4,4)   ;       x2 = runif(n,6,10)
y = 3*x1 + 8*x2 + rnorm(n,2,4)
l1 = lm(y~x1)  ;  l2 = lm(y~x1+x2)  ;  a = anova(l1,l2)

summary(l1)$coeff
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) 66.093853  1.0123131 65.289929 1.385202e-82
x1           3.199212  0.4292828  7.452458 3.664499e-11

summary(l2)$coeff
            Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) 2.767750  2.7871368  0.9930441 3.231592e-01
x1          2.870897  0.1707022 16.8181610 1.648852e-30
x2          7.871545  0.3428392 22.9598753 5.370614e-41

(summary(l2)$coeff[3,3])^2
527.1559
&gt;     a 
    Analysis of Variance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     98 9899.1                                  
2     97 1538.4  1    8360.6 527.16 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt;     a$F ; a$Pr
   [1]       NA 527.1559
[1]           NA 5.370614e-41
&gt; 
&gt; 
&gt; 
&gt; rm(list=ls(all=TRUE)) 
&gt; n = 100
&gt; x1 = runif(n,-4,4)
&gt; x2 = runif(n,6,10)
&gt; 
&gt; y = rbinom(n,1,1/(1+exp(-3*x1 - 2*x2 + 20)))
&gt; 
&gt; l1 = glm(y~x1,family=binomial)
&gt; l2 = glm(y~x1+x2,family=binomial)
&gt; 
&gt; a = anova(l1,l2)
&gt; 
&gt; summary(l1)$coeff
                 Estimate Std. Error   z value     Pr(&gt;|z|)
    (Intercept) -2.988069   0.812041 -3.679702 2.335068e-04
    x1           2.115333   0.498431  4.243984 2.195858e-05
    &gt; summary(l2)$coeff
              Estimate Std. Error   z value     Pr(&gt;|z|)
(Intercept) -17.215960  5.5710699 -3.090243 0.0019999276
x1            3.048657  0.8618367  3.537395 0.0004040949
x2            1.675323  0.5976386  2.803238 0.0050592272
&gt; 
&gt; (summary(l2)$coeff[3,3])^2
    [1] 7.858145
    &gt; 
    &gt; l1$deviance -  l2$deviance
    [1] 13.65371
    &gt; pchisq(l1$deviance -  l2$deviance,df=1)
[1] 0.9997802
&gt; 
&gt; a
Analysis of Deviance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Resid. Df Resid. Dev Df Deviance
1        98     45.534            
2        97     31.880  1   13.654
&gt; a$F
    NULL
    &gt; a$Pr
    NULL
</code></pre>
"
"0.0547448901451359","0.0540211804549215"," 74549","<p>This is from the book <em>The statistical sleuth--A course in methods of Data analysis</em> Chapter 20, Exercise 12(c)-(e). I am using logistic regression to predict carrier with possible predictors <code>CK</code> and <code>H</code>. Here is my solution:</p>

<pre><code>Carrier &lt;- c(0,0,0,0,0,1,1,1,1,1)  
CK      &lt;- c(52,20,28,30,40,167,104,30,65,440)  
H       &lt;- c(83.5,77,86.5,104,83,89,81,108,87,107)  
logCK   &lt;- log(CK)  
fit4    &lt;- glm(Carrier~logCK+H, family=""binomial"", control=list(maxit=100))  
Warning message:  
glm.fit: fitted probabilities numerically 0 or 1 occurred   
summary(fit4)
## 
## Call:
## glm(formula = Carrier ~ logCK + H, family = ""binomial"", control = list(maxit = 100))
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -1.480e-05  -2.110e-08   0.000e+00   2.110e-08   1.376e-05  
##
## Coefficients:  
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   -2292.8  4130902.8  -0.001        1  
## logCK           315.6   589675.2   0.001        1  
## H                11.5    21279.6   0.001        1
</code></pre>

<p>This results appear to be weird, because it seems that all coefficients are not significant.  Also the next question is to do a drop-in-deviance test for this full model and the reduced model that neither of <code>logCK</code> and <code>H</code> is useful predictor. I get:  </p>

<pre><code>fit5 &lt;- glm(Carrier~1, family=""binomial"")  
1-pchisq(deviance(fit5)-deviance(fit4), df.residual(fit5)-df.residual(fit4))  
## [1] 0.0009765625
</code></pre>

<p>So the p-value indicates that at least one of <code>logCK</code> and <code>H</code> is useful. Then I'm stuck at the next question, it asks me to calculate odds ratio for a woman with (CK, H)=(300,100) over one with (CK, H)=(80, 85).  </p>

<p>But how can I get a meaningful result with all coefficients in this model ranging so wildly? Is there anything wrong with the way I did this logistic regression? Are there any remedial measures?  </p>
"
"0.0632139541241014","0.0623782861551805"," 74628","<p>I have completed analysis on the effects of two drug treatments over a period of time on the CD4 cell count of a number of patients. I have taken the square root of the initial CD4 count as a covariate and I have taken a summary measure of the 'slopes' for each patient. </p>

<p>My model is the following:</p>

<pre><code>&gt; slopes.aov &lt;- aov(individual.slope.trans[-29] ~ sqrt(initialCD4)[-29] + treatment.fac[-29])

&gt; summary(slopes.aov)
                   Df Sum Sq  Mean Sq F value Pr(&gt;F)
sqrt(initialCD4)[-29]   1 0.0060 0.006027   0.990  0.322
treatment.fac[-29]      1 0.0082 0.008184   1.344  0.249
Residuals             109 0.6638 0.006090         
</code></pre>

<p>I am quite new to data analysis I am not quite sure how to interpret this model?</p>

<p>Can I still use the regression coefficients to describe my summary measure even though we have no significance. I am really struggling with this. Also how can we describe to effect of the covariate.?</p>

<p>I understand that I have no evidence to suggest that the treatments have an effect on the 'slopes' (my summary measure) and so I have no evidence to say that one treatment is performing better than another.      </p>
"
"NaN","NaN"," 74847","<p>I'm trying to do some exploratory analysis on some weather. I'd like to do a multiple linear regression on my data and then plot the predicted value against the actual value. Here's where I've got so far: </p>

<pre><code>data&lt;-read.csv(""Amsterdam.csv"", header=TRUE)
data2&lt;-data[SolarAltitude&gt;0,]
data2.lm&lt;-lm(DirectRadiation ~ DryBulbTemperature + RelHum
   +AtmosphericPressure, data=data2)
data.data.frame(data2,fitted.value=fitted(data2.lm),residual=resid(data2.lm)) 
</code></pre>

<p>If you could help, I would be very grateful,</p>
"
"0.047410465593076","0.0623782861551805"," 76212","<p>I'm working in R. I'd like to run a regression analysis for predicting price against terms in a text field. </p>

<p>I have a dataset of jewellery auction listings, with price paid, date, and an unstructured description of the item type: </p>

<pre><code>text,date,price_usd
""Ruby necklace, Spanish"",1925,45000
""Diamond ring, 0.7 carat, bezier cut"",1972,24000
""Diamond necklace"",1980,87000
...
</code></pre>

<p>I know how to run a linear regression for price against date: </p>

<pre><code>data &lt;- read.csv('jewels.csv')
lm1 &lt;- lm(data$price~data$date)
summary(lm1)
</code></pre>

<p>Now what I'd like to do is build a similar model, using the words in the description field that are most associated with higher prices. </p>

<p>Intuitively I'd guess these include ""diamond"" and ""necklace"", while (say) ""amethyst"" and ""ring"" were associated with lower prices, but is there a way I can build a model to look at this?</p>

<p>My sense is that I need to do the following things: </p>

<ul>
<li>turn the text field into a bag of words (vector)</li>
<li>remove stop words</li>
<li>normalize each word for overall count(?)</li>
<li>run some kind of regression against price. </li>
</ul>

<p>I'd really welcome some guidance on how to approach each step. </p>
"
"0.109815159255115","0.116699087583415"," 76250","<p>I am new to statistics and I am trying to understand the difference between ANOVA and linear regression. I am using R to explore this. I read various articles about why ANOVA and regression are different but still the same and how the can be visualised etc. I think I am pretty there but one bit is still missing.</p>

<p>I understand that ANOVA compares the variance within groups with the variance between groups to determine whether there is or is not a difference between any of the groups tested. (<a href=""https://controls.engin.umich.edu/wiki/index.php/Factor_analysis_and_ANOVA"" rel=""nofollow"">https://controls.engin.umich.edu/wiki/index.php/Factor_analysis_and_ANOVA</a>)</p>

<p>For linear regression, I found a post in this forum which says that the same can be tested when we test whether b (slope) = 0.
(<a href=""http://stats.stackexchange.com/questions/555/why-is-anova-taught-used-as-if-it-is-a-different-research-methodology-compared"">Why is ANOVA taught / used as if it is a different research methodology compared to linear regression?</a>)</p>

<p>For more than two groups I found a website stating:</p>

<p>The null hypothesis is: $\text{H}_0: Âµ_1 = Âµ_2 = Âµ_3$</p>

<p>The linear regression model is: $y = b_0 + b_1X_1 + b_2X_2 + e$</p>

<p>The output of the linear regression is, however, then the intercept for one group and the difference to this intercept for the other two groups. 
(<a href=""http://www.real-statistics.com/multiple-regression/anova-using-regression/"" rel=""nofollow"">http://www.real-statistics.com/multiple-regression/anova-using-regression/</a>)</p>

<p>for me, this looks like that actually the intercepts are compared and not the slopes?</p>

<p>Another example where they compare intercepts rather than the slopes can be found here:
(<a href=""http://www.theanalysisfactor.com/why-anova-and-linear-regression-are-the-same-analysis/"" rel=""nofollow"">http://www.theanalysisfactor.com/why-anova-and-linear-regression-are-the-same-analysis/</a>)</p>

<p>I am now struggling to understand what is actually compared in the linear regression? the slopes, the intercepts or both? </p>
"
"0.113960576459638","0.112454054604034"," 76490","<h2>Background</h2>

<p>A laboratory wants to evaluate whether a certain form of <a href=""http://en.wikipedia.org/wiki/Polyacrylamide_gel_electrophoresis"" rel=""nofollow"">gel electrophoresis</a> is suited as a classification method for the quality of a certain substance. Several gels were loaded, each with a clean sample of the substance and with a sample that contains impurities. In addition, a molecular marker was also loaded which serves as a reference. The following picture illustrates the setup (the picture doesn't show the actual experiment, I have taken it from Wikipedia for illustration):</p>

<p><img src=""http://i.stack.imgur.com/vC53q.png"" alt=""Example of a gel electrophoresis""></p>

<p>Two parameters were measured for each gel and each lane:</p>

<ol>
<li>The <strong>molecular weight</strong> (that is how ""high up"" a compound wandered during the electrophoresis)</li>
<li>The <strong>relative quantity.</strong> The total quantity of each lane is normalized to 1 and the density of each band is measured which results in the relative quantity of each band.</li>
</ol>

<p>A scatterplot of the relative quantity vs. molecular weight is then produced which could look something like this (it's artificial data):</p>

<p><img src=""http://i.stack.imgur.com/ndzh9.png"" alt=""Example scatterplot""></p>

<p>This graphic can be read as follows: Both the ""good"" (blue points) and ""impure"" (red points) substance exhibit two bands, one at around a molecular weight of 120 and one at around 165. The bands of the ""impure"" substance at a molecular weight around 120 are considerably less dense than the ""good"" substance and can be well distinguished.</p>

<hr>

<h2>Goal</h2>

<p>The goal is to determine two boxed (see graphic below) which determine a ""good"" substance. These boxes will then be used for classification of the substance in the future into ""good"" and ""impure"". If a substance exhibits lanes that fall within the boxes it is classified as ""good"" and else as ""impure"".</p>

<p>These decision-rules should be <em>simple</em> to apply for someone in the laboratory. That's why it should be boxes instead of curved decision boundaries.</p>

<p>False-negatives (i.e. classify a sample as ""impure"" when it's really ""good"") are considered worse than false-positives. That is, an emphasis should be placed on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity"" rel=""nofollow"">sensitivity</a>, rather than on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity"" rel=""nofollow"">specificity</a>.</p>

<p><img src=""http://i.stack.imgur.com/vhzEW.png"" alt=""Example decision boxed""></p>

<hr>

<h2>Question</h2>

<p>I'm am no expert in machine learning. I know, however, that there are quite a few machine learning algorithms/techniques that could be helpful: $k$-nearest neighbors (e.g. <code>knn</code> in <code>R</code>), classification trees (e.g. <code>rpart</code> or <code>ctree</code>), support vector machines (<code>ksvm</code>), logistic regression, boosting and bagging methods and many more.</p>

<p>One problem of many of those algorithms is that they don't provide a simple ruleset or linear boundaries. In addition, the <strong>sample size</strong> is around <strong>70.</strong></p>

<p>My questions are:</p>

<ul>
<li>Has anyone an idea of how to proceed here?</li>
<li>Does it make sense to split the dataset into training- and test-set?</li>
<li>What proportion of the data should the training set be (I thought around a 60/40-split).</li>
<li>What, in general, is the workflow for such an analysis? Something like: Splitting dataset -> fit algorithm on the training set -> predict outcome for the test set?</li>
<li>How to avoid overfitting (i.e. boxes that are too small)?</li>
<li>What is a good statistic to assess the predictive performance in this case? AUC? Accurary? Positive predictive value? <a href=""http://en.wikipedia.org/wiki/Matthews_correlation_coefficient"" rel=""nofollow"">Matthews correlation coefficient</a>?</li>
</ul>

<p>Assume that I'm familiar with <code>R</code> and the <code>caret</code> package. Thank you very much for you time and help.</p>

<hr>

<h2>Example data</h2>

<p>Here is an example dataset.</p>

<pre><code>structure(list(mol.wt = c(125.145401455869, 118.210252208676, 
165.048583787746, 126.003687476776, 170.149347112565, 127.761533014759, 
155.523172614798, 120.094514977175, 161.234986765321, 168.471542655269, 
156.522990530521, 154.377948321209, 165.365756398877, 167.965538771316, 
116.132241687833, 115.143539160903, 156.696830822196, 162.578494491556, 
136.830624758899, 123.886594633942, 124.247484227948, 126.257226352824, 
160.684010454816, 166.618872115047, 126.599387146887, 165.690375912529, 
159.786861142652, 114.520735974329, 125.753594471656, 157.551537154148, 
157.320636890647, 171.5759136115, 158.580005438661, 125.647463565197, 
130.404710783509, 127.128218318572, 162.144126888907, 161.804616951055, 
167.917268243627, 168.582197247178), rel.qtd = c(57.68339235957, 
54.0514508510085, 25.0703901938793, 37.6933881305906, 36.6853653723001, 
53.6650555524679, 52.268438087776, 52.8621831466857, 43.1242291166037, 
46.6771236380788, 38.0328239221277, 40.0454611708371, 44.6406366176158, 
40.8238699987682, 51.9464749018547, 54.0302533272953, 37.9792331383524, 
48.3853988095525, 38.2093977349102, 42.2636098418388, 42.9876895407144, 
40.8018728193786, 40.1097096927465, 38.7432550253867, 39.2633283608111, 
43.4673723102812, 53.3740718733815, 49.1067921475768, 52.3002598744634, 
44.9847844953241, 44.3014423068017, 44.0191971364465, 47.0805245356855, 
55.0124134796556, 57.9938440244052, 62.8314454977068, 45.8093815891894, 
43.2300677500964, 39.4801550161538, 51.6253515591173), quality = structure(c(2L, 
2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 1L), .Label = c(""bad"", ""good""), class = ""factor"")), .Names = c(""mol.wt"", 
""rel.qtd"", ""quality""), row.names = c(10L, 14L, 47L, 16L, 57L, 
54L, 45L, 12L, 43L, 67L, 25L, 21L, 1L, 55L, 20L, 22L, 37L, 15L, 
8L, 38L, 46L, 64L, 51L, 65L, 52L, 61L, 63L, 32L, 50L, 27L, 19L, 
69L, 23L, 42L, 6L, 48L, 11L, 13L, 5L, 71L), class = ""data.frame"")
</code></pre>
"
"0.122653086996056","0.121031652970736"," 76918","<p>I have a question about how to do analysis of an experiment that has already been done, I hope you can help me with some advice!</p>

<p>I will try to keep it as simple as possible, but will give some detail so you know what I'm talking about!</p>

<p>What has been done is a ""screening trial"" to look at the activity of about 50 subjects (fungi) as antagonists (against a pest), the 50 individuals are members of groups (species), but some groups have many more members than others</p>

<p>I have results of several types of screening tests for each of the 50 individuals, with reps of each.  The screening tests look at different aspects, like growth rate, direct effects, and indirect effects.  </p>

<p>I can rank the isolates by their results in each screening test, and there looks like a lot of variability.</p>

<p>I want to be able to report the findings of screens, for each screening test and also to see if some individuals are in top ranks in different screening tests (and also the opposite, if some are great at some tests but not at others). I think what I want is to know if the results of the tests correlate for each individual....? </p>

<p>I am not sure how to say - this individual is the best - how can I tell if it is different than the next in the rank?
If I list the top ten from each screening test, I would like to know that they are statistically different from those I excluded from the list.  I would also like to compare them as groups, to be able to say, this species was the best, but with different numbers of representatives within the species, I dont think I could do this (please advise)</p>

<p>This seems like it would be a common research experiment, for example, for testing drugs in medical experiments, so I am looking for examples of what others have done to present this type of result.</p>

<p>I have seen a similar experiment to what I have to analyse but that had been done on a small scale, and the researchers used ANOVA to test differences among individuals and among groups, and some posthoc test to give each group little letters designating their means different than other groups.  </p>

<p>This seems to be unwieldy for 50 subjects, and I'm not sure about this.... I think I need some kind of mixed model regression to put all the test results in a model to test for correlation/covariance, but my understanding is weak!</p>

<p>I have been learning R and would like to do this analysis using R.</p>

<p>Can you give me advice/suggestions?  I would appreciate any help in understanding and clarifying this problem and solutions!  </p>
"
"0.0952986215201744","0.0940388046555249"," 77057","<p>I have a data set consisting of six independent environmental variables (all binomial: present / absent) and one dependent variable (binomial: disease present / absent). 
In order to determine the combination of factors that have the highest probability of leading to disease, I first need to conduct an Expert Opinion poll where I will have several experts rank all possible combinations of variables according to their probability of leading to the occurrence of disease. Then, I will obtain regression parameters for each variable using a conjoint analysis approach where each expert conforms a level (hierarchical design), the six environmental variables are independent variables, and the rankings are the dependent variable. There being six factor variables, there exist a total of 64 possible orthogonal combinations. 
I reduced this overwhelming number of possible combinations (while retaining orthogonality) using the <code>AlgDesign</code> package of R. Here is the code followed only by relevant pieces of output:</p>

<pre><code>levels.design = c(2,2,2,2,2,2)
full.design &lt;- gen.factorial(levels.design)

   X1 X2 X3 X4 X5 X6
1  -1 -1 -1 -1 -1 -1
2   1 -1 -1 -1 -1 -1
3  -1  1 -1 -1 -1 -1
   .................
63 -1  1  1  1  1  1
64  1  1  1  1  1  1

set.seed(69)
fractional &lt;- optFederov(~., data=full.design, approximate=FALSE, criterion=""D"")
fractional
</code></pre>

<p>The result is a subset of 12 combinations to be included in the conjoint analysis:</p>

<pre><code>$design
    X1 X2 X3 X4 X5 X6
4   1  1 -1 -1 -1 -1
5  -1 -1  1 -1 -1 -1
    ................
57 -1 -1 -1  1  1  1 
</code></pre>

<p>From what I understand, doing a regression analysis on all 64 combinations should lead to the same regression parameters as those obtained if I use only the reduced set (i.e. 12 combinations from the fractional factorial design).  </p>

<p>Questions: </p>

<ol>
<li>Do the code and the resulting output make sense?</li>
<li>Could anyone point me to a good and simple reference on how this fractional design works? I am afraid I might be doing things wrong by selecting a subset that produces different results from those obtained if a full factorial design was employed.</li>
</ol>
"
"0.100365631932749","0.108042360909843"," 77851","<p>I am trying to get an optimal cut-off value dividing group with minimum sums of squares of residuals (=observed y - estimated y) the model is like below.</p>

<blockquote>
  <p>In group 1 : model y= a1x + b1z + C1v ...</p>
  
  <p>In group 2 : model y= a2x + b1z + C1v ...</p>
</blockquote>

<p>I have the data of y, x, z, v... The problem is the group 1 and 2 are not divided yet and the purpose of the analysis is finding optimal cut-off point of x using regression models.</p>

<p>I searched again and again, but couldn't find the way to make models varying 'a' and share b1 and c1... and fitting it to data.</p>

<p>I asked similar quesion in stackexchange, and somebody advised me the problems of this kind of approach, however, I need this approach, because it's some clinical research want to 'find' optimal (not perfect) cut-off point of x.</p>

<p>The article I read described below
The authors of the article mentioned that they used R, but I cannot find any reference or examples about this kind of analysis.</p>

<blockquote>
  <p>To determine the relationship between serum 25(OH)D and iPTH concen-
  trations while adjusting for confounders that could affect serum
  25(OH)D concentrations (i.e., age, gender, body weight, calcium
  intake, physical activity, and season of year), we considered two
  linear regression models, one for subjects below a certain
  concentration of serum 25(OH)D and the other for subjects above that
  concentration. To determine the specific cutoffs, we fitted the two
  linear regression models described above and calculated the sums of
  squares of residuals (=observed PTH - estimated PTH) from the two
  models for each concentration of serum 25(OH)D. The models with the
  lowest residual sums of squares were our best models, and the
  corresponding concentrations of serum 25(OH)D were defined as the
  optimal cutoff values.</p>
</blockquote>

<p>Somebody said that this question is already answered in ""regression model fitting for define cut-off"" but, I don't think so... It's not regression discontinued design, because there is no a-priori cut-off. Finding cutoff is the purpose of analysis.
Thanks.</p>
"
"0.0316069770620507","0.0311891430775903"," 78360","<p>I need your help with a Statistical Learning homework in R.
I have to perform classification over this dataset: <a href=""http://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/"" rel=""nofollow"">mammographic masses</a> predicting Severity (0=""not severe"",1 = ""severe) using these predictors:</p>

<ul>
<li>Age (quantitative)</li>
<li>Margin (qualitative)</li>
<li>Shape (qualitative)</li>
</ul>

<p>Everything is fine and understandable when I use logistic regression, but I don't know if it's possible to run QDA (or linear discriminant analysis either), since two of the variables are qualitative.</p>
"
"NaN","NaN"," 78762","<p>I have an outcome which is truncated from both left and right sides. I would like to know which regression method may account this kind of analysis. Here is an example:</p>

<pre><code>set.seed(123)
z&lt;-rnorm(300)
y&lt;-z[z&gt;=-0.8 &amp; z&lt;=1.2] #truncated
x1&lt;-sample(1:3, length(y), T)
x2&lt;-rbinom(length(y), 1, 0.3)
</code></pre>

<p>What's the best regression method for modeling <code>y~x1+x2</code>?  The R package <code>truncreg</code> seems just have arguments for either left or right truncation rather than both sides like this example.</p>
"
"NaN","NaN"," 79942","<p>I have elemental concentrations for 18 elements in 36 samples, determined by neutron activation analysis. I would like to calculate correlation coefficients, etc. by regression analysis according to the York method.</p>
"
"0.0316069770620507","0.0311891430775903"," 80172","<p>I performed a multivariate linear regression such that:</p>

<pre><code>fit&lt;-lm(as.matrix(y)~mwtkg+mbmi+mage,data=x)
</code></pre>

<p>where $y$ is a $500 \times 26$ multivariate outcomes. Then, I am wondering how to explain the <code>anova(fit)</code>:</p>

<pre><code>&gt; anova(fit)
Analysis of Variance Table

             Df  Pillai approx F num Df den Df    Pr(&gt;F)    
(Intercept)   1 0.99959    63064     25    651 &lt; 2.2e-16 ***
mwtkg         1 0.03506        1     25    651    0.5403    
mbmi          1 0.20862        7     25    651 &lt; 2.2e-16 ***
mage          1 0.09016        3     25    651 4.567e-05 ***
Residuals   675                                             
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the three Dfs, Pillai, and P values mean for the model?</p>
"
"0.0547448901451359","0.0540211804549215"," 81699","<p>I'm new to data analysis so this is kind of a simple question.</p>

<p>I would like to understand why I cannot reproduce a survival curve generated by a fitted exponential model from Stata. I use the coefficients and make my function in R to plot but it looks nothing similar. I believe it's one of those daft problems where I am not interpreting something properly. I illustrate below.</p>

<p>First, some data in Stata:</p>

<pre><code>use http://www.ats.ucla.edu/stat/data/uis.dta, clear
gen id = ID
drop ID
stset time, failure(censor)
</code></pre>

<p>Then we can fit a null exponential model</p>

<pre><code>streg, dist(exponential) nohr
</code></pre>

<p>Which gives the following output:</p>

<pre><code>Exponential regression -- log relative-hazard form 

No. of subjects =          628                     Number of obs   =       628
No. of failures =          508
Time at risk    =       147394
                                                   LR chi2(0)      =     -0.00
Log likelihood  =    -1043.531                     Prob &gt; chi2     =         .

------------------------------------------------------------------------------
          _t |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       _cons |  -5.670383   .0443678  -127.80   0.000    -5.757342   -5.583424
------------------------------------------------------------------------------
</code></pre>

<p>I take the survivor function from Stata's documentation:</p>

<p>$$
S(t)=\exp(-\lambda_{j} t_{j})
$$</p>

<p>So, in R, I plot this out with the following:</p>

<pre><code>S &lt;- function(x) exp(-5.670383*x) # 'x' acts like 't'
curve(S, 0, 1000)
</code></pre>

<p>This curve is not equivalent to Stata's, given by:</p>

<pre><code>stcurve, surv
</code></pre>

<p>Where did I go wrong in my interpretation? Is my equation using the correct parameterization?</p>

<p>P.S. Why am I reproducing these curves? A little to do with overlaying curves but now that I have this problem, I have to know where I went wrong.</p>
"
"0.0547448901451359","0.0540211804549215"," 81760","<p>Let's say I have monthly data on cars on the road and number of car accidents. I want to know whether cars on the road is somehow related to the number of car accidents. Of course, I could conduct regression analysis or correlation analysis. However, when I have a set of 50 or so months with the number of cars on the road and number of accidents, how can I perform hypothesis testing to test whether a rise in cars on the road was related to an increase in automobile accidents.</p>

<p>Tagged as R because I'll be using R though my question relates to 'how would I solve this basic problem""</p>
"
"0.070675349274022","0.0697410440814588"," 82153","<p>I have a multivariate time series dataset including interacting biological and environmental variables (plus possibly some exogenous variables). Beside seasonality, there is no clear long-term trend in the data. My purpose is to see which variables are related to each other. Forecasting is not really looked for. </p>

<p>Being new to time-series analysis, I read several references. As far as I understand, Vector Autoregressive (VAR) model would be appropriate, but I donâ€™t feel comfortable with seasonality and most examples I found concerned economics field (as often with time series analysisâ€¦) without seasonality.</p>

<p>What should I do with my seasonal data?
I considered deseasonalizing them â€“ for example in R, I would use <code>decompose</code> and then use the <code>$trend + $rand</code> values to obtain a signal which appears pretty stationary (as judged per <code>acf</code>).
Results of the VAR model are confusing me (a 1-lag model is selected while I would have intuitively expected more, and only coefficients for autoregression â€“ and not for regression with other lagged variables - are significant). 
Am I doing anything wrong, or should I conclude that my variables are not (linearly) related / my model is not the good one (subsidiary question: is there a non-linear equivalent to VAR?).</p>

<p>[Alternatively, I read I could probably use dummy seasonal variables, though I canâ€™t figure out exactly how to implement it].</p>

<p>Step-by-step suggestions would be very appreciated, since details for experienced users might actually be informative to me (and R code snippets or links towards concrete examples are very welcome, of course). Thank you.</p>
"
"0.0836242010007091","0.0825187161885156"," 82698","<p>I've run a simulation study in order to determine type I error rate of a statistic.My simulation design includes threes factors as sample size (4 levels), test length or number of items (3 levels) and estimator (3 levels). The statistic is developed to measure person fit with test data in educational testing situation.I've replicated the analysis in each cell (i.e. the design is fully-crossed) 100 times.</p>

<p>Now, I have the results and type I error rates range from 0.005 to 0.105 (i.e. across the whole analysis). I want to analyze how factors affect type I error rate using something similar to ANOVA. I tried Beta Regression in R using <code>betareg</code> package but I received this error message:</p>

<blockquote>
  <p>invalid dependent variable, all observations must be in (0, 1)</p>
</blockquote>

<p>Any idea on how to determine the effect of design factors on type I error rate?</p>
"
"0.0632139541241014","0.0623782861551805"," 82992","<p>I have a dataset $D$ made of $m$ samples and $n$ features with $n \gg m$.
For each sample I have a score $s$ which I would like to be able to predict.</p>

<p>As the number of features is very high (compared to the number of samples),
I have performed PCA (Principal Component Analysis) and built a regression
model for $s$ based on the first 3 principal components (out of 45). I cross-validated the results and am happy with the performances of my model.</p>

<p>Now I would like to understand what is happening in terms of my original features. In particular I would like to understand which features are relevant in predicting $s$, and select them to build a model which does not involve PCA.</p>

<p>How can I perform this last step (if you have R based solutions it is better, but not essential)?</p>
"
"0.134772603029291","0.146290048226517"," 83469","<p>I have a problem I've been going over and over for months to find the right statistical analysis method. I'm planning to execute the analysis in R, so any mention of appropriate packages is also appreciated. I appreciate any advice anyone can offer. Here's an analogy to my data.</p>

<p>Consider that I have 5 colleges. My hypothesis is that one particular college has awarded more degrees than the other colleges. So I want to test whether the number of degrees awarded by the different colleges differs significantly. My data includes 3 data sets as follows:</p>

<p>1)The total number of degrees earned by students from each college, since the opening of each respective college. This is not a random sample, but the actual count. All colleges have different opening dates, so they differ in age. I do have the age of each college if it turns out to be relevant. This data includes second, third, and fourth degrees earned by individual students in the count. So the data includes two columns (college name, and number of degrees awarded), and 5 rows (one for each college).</p>

<p>2)The number of degrees per student per school. I have the list of about 5000 students in the total data set, and the number of degrees each student obtained at each school. Some students attended more than one of the schools, and many earned degrees from more than one school. Every degree ever awarded by these five colleges is represented in this data set as well. This data set does not include students who attended, but never earned a degree at these five colleges. This data set has six columns (one for student ID, and five for college name), and about 5000 rows (one for each student). The totals for each college column are equal to the values in the first data set.</p>

<p>3)The total number of students attending each school over the 30 year period, including those who did not earn a degree. The total number of students, of course, varies at each school. So five rows (one for each college) and two columns (college name, and total students attended in the history of the college).  </p>

<p>So I have count data with unequal sample sizes. My categorical predictor needs to be the college, as that's what I'm interested in addressing. </p>

<p>I've considered different types of regression models, but I'm not sure if these would be most appropriate. For a regression model I guess my response would simply be the number of degrees for each school. Or maybe the proportion of degrees with respect to the total students per school. Would this be sufficient to correct for the unequal sample sizes? </p>

<p>Any thoughts on which regression model would be most appropriate? Binomial doesn't work, because I need to take into account students with more than one degree, so it's not a binomial response. I'm not sure what type of distribution should be assumed, as picking one to test against seems very arbitrary to me. </p>

<p>Would some type of contingency table be more appropriate? </p>

<p>Thanks in advance for any suggestions you can offer.</p>
"
"0.0223495078133837","0.0441081091391231"," 83861","<p>Let's say I have the following data on leads, monthly media spend, and clicks</p>

<pre><code>Month  Leads   Media     Clicks
Jan     150    1000       500
Feb     200    1000       550 
March   300    1200       800
...
</code></pre>

<p>Let's say I run a linear regression where y is leads and the predictors are media and clicks. That's good, I know the relationships between these variable and can generate some lags to produce predictions. But what if I had spent 500 (or 2000 or 0) on media, what would have occurred. How do I perform this type of 'counter-factual' analysis where I attempt to find the results of a model if the actual value from one or two of the predictors was lower or higher? What is the standard approach (aka statistically proper approach)? Is it just a matter to ""adjusting"" the data to the 'new' number and rerunning the regression? or maybe simulating a regression 100+ times with 100+ different values for media?</p>
"
"0.078223277346843","0.0661621637086846"," 83908","<p>I am working in program R. I am modeling the incidence of flight in a seabird in relation to distance to the nearest ship (potential disturbance, range = 0 to 74 km from the bird). 1= flight during observation, 0 = no flight. The bird does fly with some unknown probability when no ships are present or really ""far"" away. I am trying to find this really far distance and associated probability of flight using binary logistic regression.</p>

<p>Model = Flight ~ ship distance. Other variables were explored but fell out with stepwise selection.</p>

<p>During exploratory analysis I truncated the data down only looking at smaller distances from the ship (20, 15, 10 km). These models are highly significant and predict that as the ship gets closer the probability of flight increases. However when I include all the data (out to 74 km) the intercept is significant (and predicts the true % of observed flight events) but the slope term is non-significant. </p>

<p>Can I use a weighting scheme to give more weight to observations when the ship was closer?</p>

<p>Thanks.</p>

<p>Edit: I am working through the suggestions made by @Scortchi and @Underminer. Here is a plot of a loess smooth on the observed data to better help visualize the pattern. </p>

<p><img src=""http://i.stack.imgur.com/ZabQh.jpg"" alt=""Loess smooth of probability of flight as a function of distance to nearest ship""></p>

<p>The distance to the ship data does not discriminate between approaching ships and departing ships it is just a straight line measure to the nearest ship. The dip in the probability of flight at 8.5 I believe can be attributed to ""unaffected"" birds that did not fly as the ship passed by them. So as the ship departs and gets further from the observation site we were more like to be observing birds that for whatever reason did not fly when the ship passed and are less likely to fly for ""naturally occurring"" reason. As additional birds fill back into the observation area the ""baseline"" flushing rate is resumed and birds start to fly at ""normal"" probabilities. </p>
"
"0.0774209661138764","0.0763974860547543"," 84307","<p>I am having trouble modeling a regression with seemingly non-independent, nested observations. I am examining the effect of relatedness on proportion of offspring in cases of multiple males contributing to a brood of eggs. I have estimates of the genetic relatedness between each male and the female it mated with, as well as the proportion of offspring a male sired in that female's brood of eggs. There are up to 5 males contributing to a brood, and 9 broods sampled for paternity.</p>

<p>So the outcome is proportion sired, and the dependent variable is relatedness. Seems straightforward. However, the observations of male offspring proportion are not independent within a brood (the proportions sum to 1).  Should observations be nested within female, or should female be treated as a random effect (9 broods from 9 females) or both?And should Relatedness remain a fixed effect? </p>

<p>I only have one predictor and one outcome, proportion ~ relatedness, so I initially thought it would be relatively simple, but the female ""treatment"" is random...and male data within a female are not really independent.  I am looking to do this analysis in lme4 in R. </p>

<p>Here is an example of the data:</p>

<pre><code>ID  Female  Relat.  Prop.
1   A        .12      .3
2   A        .03      .02
3   B         .02      .33
</code></pre>

<p>...
Thanks for any help or advice,
LP</p>
"
"0.0446990156267674","0.0441081091391231"," 85586","<p>No regular here will be unaware of the perils of using stepwise and similar automatic methods for variable selection in regression analysis. But preferred alternatives, such as the lasso or elasticnet, have there own difficulties.</p>

<p>I can't find anywhere in the archive here a discussion of the methods provided by the <strong>subselect</strong> package in R, which I've just come across - the package has, so far as I can see, existed for a decade and more, and presumably has proved useful.</p>

<p>In addition to a variation on the leaps procedure, <strong>subselect</strong> offers three algorithms (which it calls anneal, genetic and improve) for variable selection for different kinds of analysis. </p>

<p>Have these procedures (or any of them) proved to be of value in variable selection? </p>
"
"0.0316069770620507","0.0311891430775903"," 85798","<p>If I do a multiple regression such as:</p>

<pre><code>df&lt;-data.frame(y1=rnorm(100,2,3),
y2=rnorm(100,3,2),
x1=rbinom(100,1,0.5),
x2=rnorm(100,100,10))

fit&lt;-lm(cbind(y1,y2)~x1+x2,data=df)
&gt; anova(fit)
Analysis of Variance Table

            Df  Pillai approx F num Df den Df Pr(&gt;F)    
(Intercept)  1 0.75423  147.306      2     96 &lt;2e-16 ***
x1           1 0.00720    0.348      2     96 0.7069    
x2           1 0.00928    0.450      2     96 0.6391    
Residuals   97                                          
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I am wondering how to explain this ANOVA object where two models have different responses and the same set of predictors.</p>
"
"0.0547448901451359","0.0540211804549215"," 86057","<p>Here's <a href=""http://stats.stackexchange.com/questions/125/what-is-the-best-introductory-bayesian-statistics-textbook"">a link</a> to a good question regarding Textbooks on Bayesian statistics from some time ago.</p>

<p>People suggested John Kruschke's ""Doing Bayesian Data Analysis: A Tutorial Introduction with R and BUGS"" as one of the best options to get an introduction to Bayesian statistics.
Meanwhile, a potentially interesting book called ""Bayesian and Frequentist Regression Methods"" by Jon Wakefield was released, which also provides code for R and BUGS. 
Thus, they esentially both seem to cover the same topics.</p>

<p>Question 1: If you have read the book, would you recommend it to a frequentist economics masters graduate as both an introduction to Bayesian statstics and reference book for both frequentist and bayesian approaches?</p>

<p>Question 2: If you have read both Wakefield's and Kruschke's book, which one would you recommend better?</p>
"
"0.0632139541241014","0.0623782861551805"," 87071","<p>I am looking to use a mathematical model developed by Firbank &amp; Watkinson (1985) J. App. Ecol. 22:503-517 for the analysis of competition between plants grown in mixture.</p>

<p>The model is as follows:</p>

<p>$$W_{A}=W_{mA}\left(1 + a_{A}\left(N_{A}+\alpha N_{B}\right)\right)^{-b_{A}}$$</p>

<p>where $W_{A}$ is the mean yield per plant of species $A$ grown in the experiment, $W_{mA}$ is the mean yield of isolated plants of species $A$, $a_{A}$ is the surface area required to reach size $W_{mA}$, $N_{A}$ is the planting density of species $A$, $\alpha$ is the competition coefficient, and $-b_{A}$ is the 'resource use efficiency' parameter. </p>

<p>The model is a regression model as I understand it. I have data for density of species $A$ and $B$ and ($N_{A}$ and $N_{B}$) as well as the response variable $W_{A}$. I am unsure how I can use R to estimate the remaining values, most important of which is the competition coefficient, $\alpha$. If there is any more information that I need to provide please let me know.</p>
"
"0.141511233180756","0.146290048226517"," 87278","<p>I don't know if a similar problem has been asked before so if it has been, please provide me a link to the related/duplicate questions. I am sorry if I seem to be asking too much. But I really like to learn this stuff and this seems to be a good place to start asking.</p>

<p>I have been teaching myself statistics through self-study and I found Logan's <a href=""http://as.wiley.com/WileyCDA/WileyTitle/productCd-1405190086.html"" rel=""nofollow""><strong>Biostatistical Design and Analysis Using R</strong></a> very helpful in that it shows how the actual computations are done (in R) and how the results are interpreted. I particularly like the part about multiple regression (Chapter 9). I use R since it is the most accessible (and free) software that I can get my hands into. </p>

<p>Right now, I am trying to learn multivariate multiple regression. But unfortunately, I can't find a good resource. My specific problem is finding the best linear model for each response variable for the following morphometric data of a plant species (that some of my high school biology students are investigating), where <code>Leaves</code>, <code>CorL</code>, <code>CorD</code>, <code>FilL</code>, <code>AntL</code>, <code>AntW</code>, <code>StaL</code>, <code>StiW</code>, and <code>HeiP</code> are the response variables and <code>pH</code>, <code>OM</code>, <code>P</code>, <code>K</code> (nutrient variables), <code>Elev</code>, <code>SoilTemp</code>, and <code>AirTemp</code> (environment variables) are the independent variables.</p>

<p>I don't know if it is okay to proceed as in the case of only one dependent variable, but I went through the steps of Example 9B of Logan anyhow.</p>

<h3>lily.csv</h3>

<pre><code>Leaves,CorL,CorD,FilL,AntL,AntW,StaL,StiW,HeiP,Elev,pH,OM,P,K,SoilTemp,AirTemp
55,213.4,114.6,170.3,10.6,2.35,210,6.7,0.93,1431,6.37,1,3,170,29,26
44,192.15,95.25,160.6,7.1,2.25,176.4,6.55,0.79,1471,6.02,1,0,180,25,23
38,156.75,95.5,155.2,5.65,1.8,170.9,4.4,0.78,1471,6.02,1,0,180,25,23
29,191.8,88.35,155.2,10,2.5,178.25,5.9,0.75,1464,5.99,1,3,150,25,22
36,200.85,99.4,161.9,6.5,1.55,187.4,6.15,0.8,1464,5.99,1,3,150,25,22
43,210.2,74,147,7,1,170,5,0.8,1464,5.99,1,3,150,25,22
34,183.2,97.3,149.5,6.9,1.85,168.8,5.45,0.71,1464,5.99,1,3,150,25,22
52,233.3,107.7,179.6,9.2,3.05,210,6.45,0.82,1464,5.99,1,3,150,25,22
43,205.7,108.8,164.6,9.4,2,190.9,5.15,0.66,1464,5.99,1,3,150,25,22
28,203.15,119.35,160.6,8.9,2.3,180,6.85,0.77,1503,5.98,3,2,240,29.5,25.5
45,188.85,100.5,150.6,6.4,2.3,174.85,7.7,0.84,1503,5.98,3,2,240,29.5,25.5
49,205.2,126.85,150.8,10.1,2.8,177.5,9,0.84,1487,6.09,4,4,180,26,25
35,187.7,102.35,142.1,5.55,1.85,175.35,5.75,0.56,1485,6.17,3.5,1,220,24,23
23,181.05,94.6,136.6,6.9,1.8,169.3,5.8,0.59,1485,6.17,3.5,1,220,24,23
31,172.5,63.7,113.6,5.2,1.5,151.2,4.7,0.57,1482,6.29,5,2,280,24,23
34,190.5,93.1,151.9,5.65,1.85,172.5,5.25,0.68,1482,6.29,5,2,280,24,23
41,185.85,85.2,148.6,5.9,1.05,169.6,5.9,0.62,1472,6.48,0.5,3,170,25.22,22.89
29,195,159.2,159.3,15,4,185,6.3,0.59,1472,6.48,0.5,3,170,25.22,22.89
31,115.6,108.6,165.8,8.5,3,200.5,7.5,0.83,1454,5.53,5,14,350,25.22,22.89
27,176.65,93.1,128.65,6.65,2.85,180.5,6.65,0.53,1454,5.53,5,14,350,25.22,22.89
33,210,119,148,7,3,193,6,0.62,1454,5.53,5,14,350,25.22,22.89
42,200,93,166,18.3,4.55,177,8,1.12,1454,5.53,5,14,350,25.22,22.89
42,205,101.4,166.8,9,2.5,190,8.2,1.12,1454,5.53,5,14,350,25.22,22.89
25,192.9,94.15,147.8,6.45,2.3,167.65,7.15,0.61,1445,5.59,4,7,260,25.22,22.89
36,187.95,65.05,150.2,6.55,2.7,177.5,6.55,0.52,1445,5.59,4,7,260,25.22,22.89
32,110.4,11.6,168.15,7.6,2,197.95,7.85,0.73,1481,6.29,1.5,1,80,25.22,22.89
29,185,80,143,9,2,179,7.5,0.69,1481,6.29,1.5,1,80,25.22,22.89
29,179.8,70.6,134.8,11.15,3.2,165.65,5.3,0.6,1481,6.29,1.5,1,80,25.22,22.89
</code></pre>

<p>Firstly, I tried to investigate for possible collinearity among the variables.</p>

<pre><code>library(car)
lily = read.csv(""lily.csv"",header=T)
scatterplotMatrix(lily,diag=""boxplot"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/Ytsnd.png"" alt=""enter image description here""></p>

<p><code>FilL</code>, <code>AntL</code>, <code>AntW</code>, and <code>HeiP</code> seem to be non-normal so I made <code>log10</code> transformations. This <em>seems</em> to work fine. (And it is fine for you to educate me at this point if I am doing it wrong. I'd appreciate it very much.)</p>

<pre><code>scatterplotMatrix(~Elev + pH + OM + P + K + SoilTemp + AirTemp +
AirTemp + Leaves + CorL + CorD + log10(FilL) + log10(log10(log10(AntL)+0.1)+0.1) + 
log10(AntW) + StaL + StiW + log10(HeiP),data=lily,diag=""boxplot"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/t9nmI.png"" alt=""enter image description here""></p>

<p>I check for multicolinearity among the independent variables.</p>

<pre><code>&gt; cor(lily[,10:16])
                Elev          pH          OM           P           K
Elev      1.00000000  0.48252995 -0.06601928 -0.56726786 -0.28159580
pH        0.48252995  1.00000000 -0.58587694 -0.81673123 -0.70434283
OM       -0.06601928 -0.58587694  1.00000000  0.65931857  0.86478172
P        -0.56726786 -0.81673123  0.65931857  1.00000000  0.79782480
K        -0.28159580 -0.70434283  0.86478172  0.79782480  1.00000000
SoilTemp  0.14558365  0.01543524 -0.10436250 -0.05023853 -0.01041523
AirTemp   0.26450883  0.15711849  0.16862694 -0.09735977  0.11655030
            SoilTemp     AirTemp
Elev      0.14558365  0.26450883
pH        0.01543524  0.15711849
OM       -0.10436250  0.16862694
P        -0.05023853 -0.09735977
K        -0.01041523  0.11655030
SoilTemp  1.00000000  0.83202496
AirTemp   0.83202496  1.00000000
</code></pre>

<p>Among the independent variables, pairs <code>P</code> and <code>pH</code>, <code>K</code> and <code>pH</code>, <code>P</code> and <code>K</code>, <code>OM</code> and <code>K</code>, and <code>SoilTemp</code> and <code>AirTemp</code> have strong collinearity. </p>

<p>I also checked for collinearity among the dependent variables although I don't have an idea if this is a alright.</p>

<pre><code>&gt; cor(lily[,1:9])
            Leaves        CorL       CorD      FilL      AntL        AntW
Leaves 1.000000000  0.44495257 0.17903019 0.5222644 0.1495016 0.004680606
CorL   0.444952572  1.00000000 0.51084625 0.1319070 0.2101801 0.097530007
CorD   0.179030187  0.51084625 1.00000000 0.2368117 0.3297344 0.376806953
FilL   0.522264352  0.13190704 0.23681171 1.0000000 0.3932006 0.284738542
AntL   0.149501570  0.21018008 0.32973443 0.3932006 1.0000000 0.796401542
AntW   0.004680606  0.09753001 0.37680695 0.2847385 0.7964015 1.000000000
StaL   0.416083096  0.06574503 0.23272070 0.7762797 0.2701401 0.318744025
StiW   0.194927129 -0.05594094 0.08322138 0.3752195 0.3755628 0.445964273
HeiP   0.577737137  0.17603412 0.13911530 0.6348948 0.4583508 0.254173681
             StaL        StiW      HeiP
Leaves 0.41608310  0.19492713 0.5777371
CorL   0.06574503 -0.05594094 0.1760341
CorD   0.23272070  0.08322138 0.1391153
FilL   0.77627970  0.37521953 0.6348948
AntL   0.27014013  0.37556279 0.4583508
AntW   0.31874403  0.44596427 0.2541737
StaL   1.00000000  0.38306631 0.3794643
StiW   0.38306631  1.00000000 0.5039679
HeiP   0.37946433  0.50396793 1.0000000
</code></pre>

<p>From here, I can check for variance inflation and their inverses and possibly investigate interactions but I am really not sure now how to proceed or if it is alright at all to do these things in the multivariate case. And it seems to be a long way still to assessing the best multivariate model. In the case of the one dependent variable case, I can use the <code>MuMIn</code> package to automate the determination of the best fit but it doesn't work in the multiple response  case.</p>

<p>How do I proceed from this point? I will also appreciate it very much if you can point me to a good book or online material (preferably with applications in R).</p>
"
"0.0547448901451359","0.0540211804549215"," 87694","<p>I'm running a linear regression analysis in R. One variable is a continuous outcome variable (<code>score2</code>) and the other is a categorical variable for treatment group (<code>group</code>). I also have a covariate which is the time 1 measure of the outcome variable (<code>score1</code>). So the model in R looks like this:</p>

<pre><code>lm(score2~group + score1, data=dataSet)
</code></pre>

<p>Here's the question: When I plot the residuals, the plots shows evidence of non-linearity (a U-shaped plot). If both of my variables were continuous rather than categorical, I'd be sure that I need to correct for non-linearity (using Box-Cox for example). Does having a categorical predictor change this or do I still need to make the correction?</p>
"
"0.0632139541241014","0.0623782861551805"," 87956","<p>I have a repeated-measures experiment where the dependent variable is a percentage, and I have multiple factors as independent variables. I'd like to use <code>glmer</code> from the R package <code>lme4</code> to treat it as a logistic regression problem (by specifying <code>family=binomial</code>) since it seems to accommodate this setup directly.</p>

<p>My data looks like this:</p>

<pre><code> &gt; head(data.xvsy)
   foldnum      featureset noisered pooldur dpoolmode       auc
 1       0         mfcc-ms      nr0       1      mean 0.6760438
 2       1         mfcc-ms      nr0       1      mean 0.6739482
 3       0    melspec-maxp    nr075       1       max 0.8141421
 4       1    melspec-maxp    nr075       1       max 0.7822994
 5       0 chrmpeak-tpor1d    nr075       1       max 0.6547476
 6       1 chrmpeak-tpor1d    nr075       1       max 0.6699825
</code></pre>

<p>and here's the R command that I was hoping would be appropriate:</p>

<pre><code> glmer(auc~1+featureset*noisered*pooldur*dpoolmode+(1|foldnum), data.xvsy, family=binomial)
</code></pre>

<p>The problem with this is that the command complains about my dependent variable not being integers:</p>

<pre><code>In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>and the analysis of this (pilot) data gives weird answers as a result.</p>

<p>I understand why the <code>binomial</code> family expects integers (yes-no counts), but it seems it should be OK to regress percentage data directly. How to do this?</p>
"
"0.0857687593681569","0.0940388046555249"," 88796","<p>I am exploring the probability of flight in a seabird (1=flight, 0=no flight) using binomial logistic regression. My predictors are distance to a disturbance (continuous), hour of the day (continuous), site (factor), season (factor), sea state (dichotomous), and group size (dichotomous). I have explored the use of piecewise regression in relation to the distance to a disturbance as this variable spans a large range (out to 74 km) and there is no way that this is affecting flight at the largest distance. </p>

<p>When the model was fit with just reference to distance to a disturbance within the R program 'segmented' it points to a break in the data at 3.9 km. The slope up to this distance is negative and statistically significant while the slope estimate for distances further than 3.9 km is estimated to be 0 and non-significant.</p>

<p>I would like to now sequentially add in additional terms to the model to see if there is any reduction in the deviance when the additional terms are added. Can a term be added just to the section before or after the break? I cannot seem to find any information in the literature regarding this </p>

<p>My questions is can I do this? Or do I need to split the data into two chunks, before and after the breakpoint and explore additional terms this way.</p>

<p>Also the motivation to do this analysis is more to find and identify the breakpoint. Instead of adding in terms after I assess the breakpoint should I explore the breakpoint within a the model including all the terms? Would this find the break in the data in relation to the other terms or does the algorithm completely ignore the other terms in the model when searching for a break in the distance to disturbance variable.</p>

<p>Thanks, </p>
"
"0.0774209661138764","0.0763974860547543"," 88880","<p>I performed principal component analysis (PCA) with R using two different functions (<code>prcomp</code> and <code>princomp</code>) and observed that the PCA scores differed in sign. How can it be?</p>

<p>Consider this:</p>

<pre><code>set.seed(999)
prcomp(data.frame(1:10,rnorm(10)))$x

            PC1        PC2
 [1,] -4.508620 -0.2567655
 [2,] -3.373772 -1.1369417
 [3,] -2.679669  1.0903445
 [4,] -1.615837  0.7108631
 [5,] -0.548879  0.3093389
 [6,]  0.481756  0.1639112
 [7,]  1.656178 -0.9952875
 [8,]  2.560345 -0.2490548
 [9,]  3.508442  0.1874520
[10,]  4.520055  0.1761397

set.seed(999)
princomp(data.frame(1:10,rnorm(10)))$scores
         Comp.1     Comp.2
 [1,]  4.508620  0.2567655
 [2,]  3.373772  1.1369417
 [3,]  2.679669 -1.0903445
 [4,]  1.615837 -0.7108631
 [5,]  0.548879 -0.3093389
 [6,] -0.481756 -0.1639112
 [7,] -1.656178  0.9952875
 [8,] -2.560345  0.2490548
 [9,] -3.508442 -0.1874520
[10,] -4.520055 -0.1761397
</code></pre>

<p>Why do the signs (<code>+/-</code>) differ for the two analyses? If I was then using principal components <code>PC1</code> and <code>PC2</code> as predictors in a regression, i.e. <code>lm(y ~ PC1 + PC2)</code>, this would completely change my understanding of the effect of the two variables on <code>y</code> depending on which method I used! How could I then say that <code>PC1</code> has e.g. a positive effect on <code>y</code> and <code>PC2</code> has e.g. a negative effect on <code>y</code>?</p>

<hr>

<p><strong>In addition:</strong> If the sign of PCA components is meaningless, is this true for factor analysis (FA) as well? Is it acceptable to flip (reverse) the sign of individual PCA/FA component scores (or of loadings, as a column of loading matrix)?</p>
"
"0.161164592805076","0.15903404916487"," 89204","<p>I'm looking for advice on how to analyze complex survey data with multilevel models in R. I've used the <code>survey</code> package to weight for unequal probabilities of selection in one-level models, but this package does not have functions for multilevel modeling. The <code>lme4</code> package is great for multilevel modeling, but there is not a way that I know to include weights at different levels of clustering. <a href=""http://www.statmodel.com/download/asparouhovgmms.pdf"">Asparouhov (2006)</a> sets up the problem:</p>

<blockquote>
  <p>Multilevel models are frequently used to analyze data from cluster sampling designs. Such sampling designs however often use unequal probability of selection at the cluster level and at the individual level. Sampling weights are assigned at one or both levels to reflect these probabilities. If the sampling weights are ignored at either level the parameter estimates can be substantially biased.</p>
</blockquote>

<p>One approach for two-level models is the multilevel pseudo maximum likelihood (MPML) estimator that is implemented in MPLUS (<a href=""http://www.statmodel.com/download/SurveyJSM1.pdf"">Asparouhov et al, ?</a>). <a href=""http://www.biomedcentral.com/1471-2288/9/49"">Carle (2009)</a> reviews major software packages and makes a few recommendations about how to proceed: </p>

<blockquote>
  <p>To properly conduct MLM with complex survey data and design weights, analysts need software that can include weights scaled outside of the program and include the ""new"" scaled weights without automatic program modification. Currently, three of the major MLM software programs allow this: Mplus (5.2), MLwiN (2.02), and GLLAMM. Unfortunately, neither HLM nor SAS can do this.</p>
</blockquote>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3630376/"">West and Galecki (2013)</a> give a more updated review, and I'll quote the relevant passage at length:</p>

<blockquote>
  <p>Occasionally, analysts wish to fit LMMs to survey data sets collected from samples with complex designs (see Heeringa et al, 2010, Chapter 12). Complex sample designs are generally characterized by division of the population into strata, multi-stage selection of clusters of individuals from within the strata, and unequal probabilities of selection for both clusters and the ultimate individuals sampled. These unequal probabilities of selection generally lead to the construction of sampling weights for individuals, which ensure unbiased estimation of descriptive parameters when incorporated into an analysis. These weights might be further adjusted for survey nonresponse and calibrated to known population totals. Traditionally, analysts might consider a design-based approach to incorporating these complex sampling features when estimating regression models (Heeringa et al., 2010). More recently, statisticians have started to explore model-based approaches to analyzing these data, using LMMs to incorporate fixed effects of sampling strata and random effects of sampled clusters.</p>
  
  <p>The primary difficulty with the development of model-based approaches to analyzing these data has been choosing appropriate methods for incorporating the sampling weights (see Gelman, 2007 for a summary of the issues). Pfeffermann et al. (1998), Asparouhov and Muthen (2006), and Rabe-Hesketh and Skrondal (2006) have developed theory for estimating multilevel models in a way that incorporates the survey weights, and Rabe-Hesketh and Skrondal (2006), Carle (2009) and Heeringa et al. (2010, Chapter 12) have presented applications using current software procedures, but this continues to be an active area of statistical research. Software procedures capable of fitting LMMs are at various stages of implementing the approaches that have been proposed in the literature thus far for incorporating complex design features, and analysts need to consider this when fitting LMMs to complex sample survey data. Analysts interested in fitting LMMs to data collected from complex sample surveys will be attracted to procedures that are capable of correctly incorporating the survey weights into the estimation procedures (HLM, MLwiN, Mplus, xtmixed, and gllamm), consistent with the present literature in this area.</p>
</blockquote>

<p>This brings me to my question: does anyone have best practice recommendations for fitting LMMs to complex survey data in R?</p>
"
"0.0999500374687773","0.0986287303940589"," 89692","<p>My data has 3 major inputs: <code>BLDDAY</code> (a factor), <code>BLDMNT</code> (a factor), and <code>D_BLD_SER</code> (days as an integer variable).  The output is whether input variable has any impact on failure.  My model is: <code>model = glm(FAILED~BLDDAY+BLDMNT+D_BLD_SER, family=""binomial"", data=data_list)</code>.  (I used <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">UCLA's statistics help site's guide to logistic regression in R</a> to build this model.)  </p>

<p>Output: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3282  -0.9123  -0.8128   1.4056   2.1053  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -0.7672583  0.1317132  -5.825 5.70e-09 ***
BLDDAYMonday    -0.1545646  0.0839380  -1.841  0.06556 .  
BLDDAYSaturday  -0.1257976  0.2028259  -0.620  0.53511    
BLDDAYSunday    -0.1183008  0.1868713  -0.633  0.52669    
BLDDAYThursday  -0.2007452  0.0772653  -2.598  0.00937 ** 
BLDDAYTuesday    0.0480453  0.0758603   0.633  0.52651    
BLDDAYWednesday -0.0358585  0.0760027  -0.472  0.63707    
BLDMNTAug        0.3009445  0.1405545   2.141  0.03226 *  
BLDMNTDec        0.5562170  0.1338467   4.156 3.24e-05 ***
BLDMNTFeb        0.3334978  0.2133475   1.563  0.11801    
BLDMNTJan        0.4076504  0.2277978   1.790  0.07353 .  
BLDMNTJul        0.1306585  0.1415302   0.923  0.35591    
BLDMNTJun       -0.0357361  0.1428105  -0.250  0.80241    
BLDMNTMar        0.4570491  0.1949815   2.344  0.01907 *  
BLDMNTMay       -0.2292620  0.1614577  -1.420  0.15562    
BLDMNTNov        0.3060012  0.1334034   2.294  0.02180 *  
BLDMNTOct        0.2390501  0.1341877   1.781  0.07484 .  
BLDMNTSep        0.2481405  0.1384901   1.792  0.07317 .  
D_BLD_SER       -0.0020960  0.0003367  -6.225 4.82e-10 ***

(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 10288  on 8182  degrees of freedom
Residual deviance: 10154  on 8164  degrees of freedom
AIC: 10192
Number of Fisher Scoring iterations: 4
</code></pre>

<p>The ANOVA table is the following:</p>

<pre><code>anova(model, test=""Chisq"")
Analysis of Deviance Table
Model: binomial, link: logit
Response: FAILED
Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                       8182      10288              
BLDDAY     6   20.392      8176      10268  0.002357 ** 
BLDMNT    11   70.662      8165      10197 9.142e-11 ***
D_BLD_SER  1   43.797      8164      10154 3.642e-11 ***
</code></pre>

<p>My questions are:</p>

<ol>
<li><p>Although the p-values for all three components are less than 0.05, which can be considered as significant, the deviance reduced due to each component is less than 1% of the total deviance. <strong>Normally the interpretation of output like this is input parameter affects output and it's better to use this parameter then using noting.</strong> But does it really make sense of taking this parameter as significant input?</p></li>
<li><p>The p-values for <code>BLDDAY</code> and <code>BLDMNT</code> given by <code>anova()</code> is the overall p-value,  which is significant, but <code>summary()</code> gives detailed impact of each factor level. If I consider the p-values for each factor overall <code>BLDDAY</code> is significant but individually only <code>BLDDAYThursday</code> is significant. I am bit confused not as whether to consider <code>BLDDAY</code> as significant input, or Thursday only, or Thursday &amp; Friday both.</p></li>
</ol>
"
"0","0.0311891430775903"," 89795","<p>Sorry for my bad English, could you suggest R-code for the implementation of difference-in-difference regression? I don't understand how many coefficients I need.
In my analysis I compare the effect of a new law on the stock exchange volume, I have 2 periods and 2 samples. 
Thanks a lot!</p>

<p>Thanks Jeremy, but in the Imbens's model the regression contains a series of multiplications between variables and dummies. I used: </p>

<pre><code>lr1 &lt;- lm(VOLUME ~ DUMMYCAP + DUMMYTIME + DUMMYCAP*DUMMYTIME ) 
</code></pre>

<p>but the result of regression is not significant. </p>

<p>This question is about code and its economic significance. I'm sorry if this post is not appropriate for the site. But if someone has made a test of this type and knows how to write the code I would be grateful </p>
"
"0.0547448901451359","0.0540211804549215"," 89886","<p>I'm trying to produce a linear regression model, but I only have 25 observations and 34 predictors.</p>

<p>I'm trying feature selection,</p>

<pre><code>library(MASS)
full.m &lt;- lm(fmla, data=mydata)
fsel.m &lt;- step(full.m, direction = ""both"")
</code></pre>

<p>but I get this error,</p>

<pre><code>Error in step(full.m, direction = ""both"") : 
  AIC is -infinity for this model, so 'step' cannot proceed
</code></pre>

<p>Well, then I tried PCA and factor analysis but I get this weird image from PCA.</p>

<pre><code>r &lt;- prcomp(formula=pca.fmla, mydata, scale=FALSE)
</code></pre>

<p><img src=""http://i.stack.imgur.com/gg45b.png"" alt=""enter image description here""></p>

<p>In factor analysis,</p>

<pre><code>f &lt;- factanal(x=pca.fmla, mydata, factors=2)
</code></pre>

<p>I get,</p>

<pre><code>Error in solve.default(cv) : 
  system is computationally singular: reciprocal condition number = 1.09137e-18
</code></pre>

<p>Any help please?</p>

<p>EDIT: I can answer this.</p>
"
"0.11852616398269","0.116959286540963"," 91243","<p>I have a large data frame in the following form (I apologize for this formatting):</p>

<pre><code>Site    Season  T          SC    pH    Chl   DO.S   DO      BGA  Tur    fDOM    Flow    Rainfall    Solar      Rain
300N    Winter  14.05   1692.77 7.93    NA  82.26   8.42    NA  9.25    NA      NA      0.00          219.18     no
</code></pre>

<p>If you can't understand the formatting, there are 12 numerical factors, and 3 categorical factors (<code>Site</code>, <code>Season</code>, <code>Rain</code> [yes/no]). Each row represents the average daily values that I have calculated from 15-minute time series. I have spent a good amount of time doing data exploration (linear regression analysis, looking at time series plots for patterns), but haven't found a method that works for me yet. I have also worked with <code>corrplot</code>, correlation matrices, and covariance functions in an arduous way, where I subset each categorical combination and found <code>corrplot</code>s for each (I have also tried it with <code>ddply</code>, but the resulting format is not in the correlation matrix format that is easy to plot). I have also attempted PCA on the data to little avail.</p>

<p>My question is first and foremost, does anyone have an idea for data visualization of this kind of dataset? The main question I am after is, ""What are the factors that influence <code>DO</code> (dissolved oxygen)?"". How does this change by location (<code>Site</code>), <code>Season</code>, and with the influence of <code>Rain</code>. I would really like a quick method for shooting out correlation matrices (or heat maps; I have tried both) for each categorical subset. I tried this with <code>ggplot</code> and <code>facet_wrap</code>, but it wasn't happening for me. I also tried <code>ggpairs</code> from the GGally package, but honestly didn't spend too much time with that method.</p>

<p>I was starting to get into the idea of star graphs (on polar coordinates), which can be used to visualize repeating periodicity in time series, but am running out of time and decided to seek the advisement of Stack Overflow. I really appreciate any advice or thoughts on visualizing this data that come to your mind. I feel like some combination of <code>ddply</code> and graphing is what I need, but I haven't gotten there yet.
Thank you for your time.</p>

<p>EDIT:
<code>dput</code> of the data frame in question:</p>

<pre><code>structure(list(Site = structure(c(2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""2100S"", 
""300N"", ""3300S"", ""800S"", ""Burnham"", ""Center""), class = ""factor""), 
    Season = structure(c(4L, 4L, 4L, 4L, 2L, 2L), .Label = c(""Fall"", 
    ""Spring"", ""Summer"", ""Winter""), class = ""factor""), T = c(14.05, 
    14.18, 14.5, 14.58, 14.07, 11.91), SC = c(1692.77, 1671.31, 
    1680.71, 1661.79, 1549.56, 1039.63), pH = c(7.93, 7.92, 7.96, 
    7.95, 7.93, 7.79), Chl = c(NA_real_, NA_real_, NA_real_, 
    NA_real_, NA_real_, NA_real_), DO.S = c(82.26, 78.79, 82.05, 
    80.92, 74.33, 73.96), DO = c(8.42, 8.04, 8.31, 8.18, 7.61, 
    7.97), BGA = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, 
    NA_real_), Tur = c(9.25, 9.77, 9.41, 10.6, 40.38, 50.25), 
    fDOM = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, 
    NA_real_), Flow = c(NA, 178.08, 178.53, 188.13, 306.15, 382.22
    ), Rainfall = c(0, 0, 0, 0, 0.01, 0.81), Solar = c(219.18, 
    228.33, 244.3, 247.69, 105.15, 220.73), Rain = structure(c(1L, 
    1L, 1L, 1L, 2L, 2L), .Label = c(""no"", ""yes""), class = ""factor"")), .Names = c(""Site"", 
""Season"", ""T"", ""SC"", ""pH"", ""Chl"", ""DO.S"", ""DO"", ""BGA"", ""Tur"", 
""fDOM"", ""Flow"", ""Rainfall"", ""Solar"", ""Rain""), row.names = c(NA, 
6L), class = ""data.frame"")
</code></pre>
"
"0.0948209311861521","0.0935674292327708"," 91745","<p>Please, i need to do a network meta-analysis and metaregression or a multivariate meta analysis and metaregression. I have multi arm randomized controlled studies.
I tried the metafor package</p>

<h1>Dummy database with controlled multiarm treatment studies</h1>

<pre><code>Study = c(1,2,3,1:2,4:10,1,2,7)
Group1 = as.factor(c(rep(1,3),rep(2,9),rep(3,3)))
numberInt= c(rep(30,5),rep(28,7),rep(40,3))
InterventionMean1  = c(rnorm(3, mean = 350, sd = 2), rnorm(9, mean = 540, sd = 2),rnorm(3, mean = 860, sd = 2))
InterventionSD  = rnorm(15, mean = 80, sd = 15)
numberCont= c(rep(29,4), 28,rep(30,7),rep(42,3))
ControlMean  = rnorm(15, mean = 230, sd = 2)
ControlSD  = rnorm(15, mean = 55, sd = 9)
Dose = c(40,50,40,100,100,100,100,100,100,100,100,100,200,200,200)
x=as.factor(c(1,2,1,rep(1,5),rep(2,3),rep(1,4)))
data = data.frame(cbind(Study,Group, numberInt,InterventionMean1,InterventionSD,
             numberCont, ControlMean, ControlSD,Dose,x))
</code></pre>

<h1>x and Dose are covariates</h1>

<h1>Loading libraries</h1>

<pre><code>library(metafor)
library(Matrix)
</code></pre>

<h1>Effect size : mean difference</h1>

<p>dataEscalc = escalc(measure = ""MD"", m1i = InterventionMean1, sd1i = InterventionSD, n1i = numberInt,
                    m2i = ControlMean, sd2i = ControlSD, n2i = numberCont,
                    data=data)</p>

<h1>Number of patients per studies</h1>

<pre><code>dataEscalc$Ni &lt;- unlist(lapply(split(data, data$Study), function(x) rep(sum(x$numberInt) + x$numberCont[1], each=nrow(x))))
</code></pre>

<h1>Variance-covariance matrix</h1>

<pre><code>    calc.v &lt;- function(x) {
    v &lt;- matrix(vi/x$numberCont[1] + outer(x$yi, x$yi, ""*"")/(2*x$Ni[1]), nrow=nrow(x), ncol=nrow(x))
      diag(v) &lt;- x$vi
          v
          }
    V &lt;- lapply(split(dataEscalc, dataEscalc$Study), calc.v)
V &lt;- as.matrix(bdiag(V))
V
</code></pre>

<h1>Conducting multivariate meta anlaysis</h1>

<p>names(dataEscalc)</p>

<pre><code>res &lt;- rma.mv(yi, V, mods = ~ factor(Group1)  - 1,
              random = ~ factor(Group1) | Study,
              data=dataEscalc, method=""REML"")
</code></pre>

<blockquote>
  <p>I get this error message</p>
  
  <p>Error in rma.mv(yi, V, mods = ~factor(Group1) - 1, random =
  ~factor(Group1) |  : 
                    Error during optimization.</p>
</blockquote>

<h1>Conducting multivariate metaregression</h1>

<pre><code>res &lt;- rma.mv(yi, V, mods = ~ factor(Group1) + Group1:I(Dose) + Group1:I(x) - 1,
              random = ~ factor(Group1) | Study,
              data=dataEscalc, method=""REML"")
</code></pre>

<blockquote>
  <p>I get this error message</p>
  
  <p>Error in rma.mv(yi, V, mods = ~factor(Group1) + Group1:I(Dose) +
  Group1:I(x) -  : 
                    Model matrix not of full rank. Cannot fit model.</p>
</blockquote>

<p>I followed the instructions from 
<a href=""http://www.metafor-project.org/doku.php/analyses%3agleser2009"" rel=""nofollow"">http://www.metafor-project.org/doku.php/analyses:gleser2009</a>
http://www.metafor-project.org/doku.php/analyses:vanhouwelingen2002</p>

<p>Thanks in advance</p>
"
"0.078223277346843","0.0882162182782462"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"0.109815159255115","0.116699087583415"," 92150","<p>Actually, I thought I had understood what one can show a with partial dependence plot, but using a very simple hypothetical example, I got rather puzzled. In the following chunk of code I generate three independent variables (<em>a</em>, <em>b</em>, <em>c</em>) and one dependent variable (<em>y</em>) with <em>c</em> showing a close linear relationship with <em>y</em>, while <em>a</em> and <em>b</em> are uncorrelated with <em>y</em>. I make a regression analysis with a boosted regression tree using the R package <code>gbm</code>:</p>

<pre><code>a &lt;- runif(100, 1, 100)
b &lt;- runif(100, 1, 100)
c &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
y &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
par(mfrow = c(2,2))
plot(y ~ a); plot(y ~ b); plot(y ~ c)
Data &lt;- data.frame(matrix(c(y, a, b, c), ncol = 4))
names(Data) &lt;- c(""y"", ""a"", ""b"", ""c"")
library(gbm)
gbm.gaus &lt;- gbm(y ~ a + b + c, data = Data, distribution = ""gaussian"")
par(mfrow = c(2,2))
plot(gbm.gaus, i.var = 1)
plot(gbm.gaus, i.var = 2)
plot(gbm.gaus, i.var = 3)
</code></pre>

<p>Not surprisingly, for variables <em>a</em> and <em>b</em> the partial dependence plots yield horizontal lines around the mean of <em>a</em>. What me puzzles is the plot for variable <em>c</em>. I get horizontal lines for the ranges <em>c</em> &lt; 40 and <em>c</em> > 60 and the y-axis is restricted to values close to the mean of <em>y</em>. Since <em>a</em> and <em>b</em> are completely unrelated to <em>y</em> (and thus there variable importance in the model is 0), I expected that <em>c</em> would show partial dependence along its entire range instead of that sigmoid shape for a very restricted range of its values. I tried to find information in Friedman (2001) ""Greedy function approximation: a gradient boosting machine"" and in Hastie et al. (2011) ""Elements of Statistical Learning"", but my mathematical skills are too low to understand all the equations and formulae therein. Thus my question: What determines the shape of the partial dependence plot for variable <em>c</em>? (Please explain in words comprehensible to a non-mathematician!)     </p>

<p>ADDED on 17th April 2014:</p>

<p>While waiting for a response, I used the same example data for an analysis with R-package <code>randomForest</code>. The partial dependence plots of randomForest resemble much more to what I expected from the gbm plots: the partial dependence of explanatory variables <em>a</em> and <em>b</em> vary randomly and closely around 50, while explanatory variable <em>c</em> shows partial dependence over its entire range (and over almost the entire range of <em>y</em>). What could be the reasons for these different shapes of the partial dependence plots in <code>gbm</code> and <code>randomForest</code>?</p>

<p><img src=""http://i.stack.imgur.com/PrlC1.jpg"" alt=""partial plots of gbm and randomForest""></p>

<p>Here the modified code that compares the plots:</p>

<pre><code>a &lt;- runif(100, 1, 100)
b &lt;- runif(100, 1, 100)
c &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
y &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
par(mfrow = c(2,2))
plot(y ~ a); plot(y ~ b); plot(y ~ c)
Data &lt;- data.frame(matrix(c(y, a, b, c), ncol = 4))
names(Data) &lt;- c(""y"", ""a"", ""b"", ""c"")

library(gbm)
gbm.gaus &lt;- gbm(y ~ a + b + c, data = Data, distribution = ""gaussian"")

library(randomForest)
rf.model &lt;- randomForest(y ~ a + b + c, data = Data)

x11(height = 8, width = 5)
par(mfrow = c(3,2))
par(oma = c(1,1,4,1))
plot(gbm.gaus, i.var = 1)
partialPlot(rf.model, Data[,2:4], x.var = ""a"")
plot(gbm.gaus, i.var = 2)
partialPlot(rf.model, Data[,2:4], x.var = ""b"")
plot(gbm.gaus, i.var = 3)
partialPlot(rf.model, Data[,2:4], x.var = ""c"")
title(main = ""Boosted regression tree"", outer = TRUE, adj = 0.15)
title(main = ""Random forest"", outer = TRUE, adj = 0.85)
</code></pre>
"
"0.151581736950805","0.143074489662852"," 92892","<p>I'm working on a meta-analysis of prevalence data. The aim is to get estimates of prevalence at the country level. The main issue is that the disease is highly correlated with age, and the sample ages of included studies are highly heterogeneous. Only median age is available for most studies, so I can't use SMR-like tricks. I figured I could use meta-regression to solve this, including age as a fixed-effect and introducing study-level and country-level random-effects.</p>

<p>The idea (that I took from <a href=""http://www.thelancet.com/journals/lancet/article/PIIS0140-6736%2813%2961249-0/abstract"" rel=""nofollow"">Fowkes et al</a>) was to use this model to make country-specific predictions of prevalence for each 5-year age group from 15 to 60 (using the median age of the group), and to apply these predictions to the actual population size of each of those groups in the selected country, in order to obtain total infected population and to calculate age-adjusted prevalence in the 15-60 population from that.</p>

<p>I tried several ways to do this using R with packages <code>meta</code> and <code>mgcv</code>. I got some satisfying results, but I'm not that confident with my results and would appreciate some feedback.</p>

<p>First is some simulated data, then the description of my different approaches:</p>

<pre><code>data&lt;-data.frame(id_study=c(""UK1"",""UK2"",""UK3"",""FRA1"",""FRA2"",""BEL1"",""GER1"",""GER2"",""GER3""),
                 country=c(""UK"",""UK"",""UK"",""FRANCE"",""FRANCE"",""BELGIUM"",""GERMANY"",""GERMANY"",""GERMANY""),
                 n_events=c(91,49,18,10,50,6,9,10,22),
                 n_total=c(3041,580,252,480,887,256,400,206,300),
                 study_median_age=c(25,50,58,30,42,26,27,28,36))
</code></pre>

<p><strong>Standard random-effect meta-analysis</strong> with package <code>meta</code>.</p>

<p>I used <code>metaprop()</code> to get a first estimate of the prevalence in each country without taking age into account, and to obtain weights. As expected, heterogeneity was very high, so I used weights from the random-effects model.</p>

<pre><code> meta &lt;- metaprop(event=n_events,n=n_total,byvar=country,sm=""PLOGIT"",method.tau=""REML"",data=data)
 summary(meta)
 data$weight&lt;-meta$w.random
</code></pre>

<p>I used meta to get a first estimate of the prevalence without taking age into account, and to obtain weights. As expected, heterogeneity was very high, so I used weights from the random-effects model.</p>

<p><strong>Generalized additive model</strong> to include age with package <code>mgcv</code>.</p>

<p>The <code>gam()</code> model parameters (k and sp) were chosen using BIC and GCV number (not shown here).</p>

<pre><code> model &lt;- gam( cbind(n_events,n_total-n_events) ~ s(study_median_age,bs=""cr"",k=4,sp=2) + s(country,bs=""re""), weights=weight, data=data, family=""binomial""(link=logit), method=""REML"")
 plot(model,pages=1,residuals=T, all.terms=T, shade=T)
</code></pre>

<p>Predictions for each age group were obtained from this model as explained earlier. CI were obtained directly using <code>predict.gam()</code>, that uses the  Bayesian posterior covariance matrix of the parameters. For exemple considering UK:</p>

<pre><code> newdat&lt;-data.frame(country=""UK"",study_median_age=seq(17,57,5))
 link&lt;-predict(model,newdat,type=""link"",se.fit=T)$fit
 linkse&lt;-predict(model,newdat,type=""link"",se.fit=T)$se
 newdat$prev&lt;-model$family$linkinv(link)
 newdat$CIinf&lt;-model$family$linkinv(link-1.96*linkse)
 newdat$CIsup&lt;-model$family$linkinv(link+1.96*linkse)
 plot(newdat$prev~newdat$study_median_age, type=""l"",ylim=c(0,.12))
 lines(newdat$CIinf~newdat$study_median_age, lty=2)
 lines(newdat$CIsup~newdat$study_median_age, lty=2)
</code></pre>

<p>The results were satisfying, representing the augmentation of the prevalence with advanced age, with coherent confidence intervals. I obtained a total prevalence for the country using the country population structure (not shown, I hope it is clear enough).</p>

<p>However, I figured I needed to include study-level random-effects since there was a high heterogeneity (even though I did not calculate heterogeneity after the meta-regression).</p>

<p><strong>Introducing study-level random-effect</strong> with package <code>gamm4</code>.</p>

<p>Since <code>mgcv</code> models can't handle that much random-effect parameters, I had to switch to <code>gamm4</code>.</p>

<pre><code> model2 &lt;- gamm4(cbind(n_events,n_total-n_events) ~ s(study_median_age,bs=""cr"",k=4) + s(country,bs=""re""), random=~(1|id_study), data=data, weights=weight, family=""binomial""(link=logit))
 plot(model2$gam,pages=1,residuals=T, all.terms=T, shade=T)

 link&lt;-predict(model2$gam,newdat,type=""link"",se.fit=T)$fit
 linkse&lt;-predict(model2$gam,newdat,type=""link"",se.fit=T)$se
 newdat$prev2&lt;-model$family$linkinv(link)
 newdat$CIinf2&lt;-model$family$linkinv(link-1.96*linkse)
 newdat$CIsup2&lt;-model$family$linkinv(link+1.96*linkse)
 plot(newdat$prev2~newdat$study_median_age, type=""l"",col=""red"",ylim=c(0,0.11))
 lines(newdat$CIinf2~newdat$study_median_age, lty=2,col=""red"")
 lines(newdat$CIsup2~newdat$study_median_age, lty=2,col=""red"")
 lines(newdat$prev~newdat$study_median_age, type=""l"",ylim=c(0,.12))
 lines(newdat$CIinf~newdat$study_median_age, lty=2)
 lines(newdat$CIsup~newdat$study_median_age, lty=2)
</code></pre>

<p>Since the study-level random effect was in the mer part of the fit, I didn't have to handle it. </p>

<p>As you can see, I obtain rather different results, with a much smoother relation between age and prevalence, and quite different confidence intervals. It is even more different in the full-data analysis, where the CI are much wider in the model including study-level RE, to the point it is sometimes almost uninformative (prevalence between 0 and 15%, but if it is the way it is...). Moreover, the study-level RE model seems to be more stable when outliers are excluded.</p>

<p><strong>So, my questions are:</strong></p>

<ul>
<li>Did I properly extract the weights from the metaprop() function and used them further?</li>
<li>Did I properly built my <code>gam()</code> and <code>gamm4()</code> models? I read a lot about this, but I'm not used to this king of models.</li>
<li>Which of these models should I use?</li>
</ul>

<p>I would really appreciate some help, since neither my teachers nor my colleagues could. It was a really harsh to conduct the systematic review, and very frustrating to struggle with the analysis... Thank you in advance!</p>
"
"0.109489780290272","0.108042360909843"," 93392","<p>First of all, sorry i am new about this and any helps are really welcome.</p>

<p>I am reading a reaserch paper where the authors report: <em>Stepwise forward regression (Zar 1996) was used to select the most informative variables, which were included in a multiple (linear) regression model. A 5% significance level was chosen as a threshold for the inclusion of the model variables.</em></p>

<p>with a private email the first author told me that the variable selection was performed using stepAIC of MASS library using direction ""forward"" and they considered only for the final model the variables with a significance level of &lt; 5%.</p>

<p>using junk data i tried to rewrite the analysis in order to understand the procedure</p>

<pre><code>state.x77
st = as.data.frame(state.x77) str(st) colnames(st)[4] = ""Life.Exp""
colnames(st)[6] = ""HS.Grad"" st[,9] = st$Population * 1000 / st$Area colnames(st)[9] = ""Density""
str(st) model1 = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + + HS.Grad + Frost + Area + Density, data=st)
model1.stepAIC &lt;- stepAIC(model1, direction=c(""both""))
summary(model1.stepAIC)


Call:
lm(formula = Life.Exp ~ Population + Murder + HS.Grad + Frost, 
    data = st)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.47095 -0.53464 -0.03701  0.57621  1.50683 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  7.103e+01  9.529e-01  74.542  &lt; 2e-16 ***
Population   5.014e-05  2.512e-05   1.996  0.05201 .  
Murder      -3.001e-01  3.661e-02  -8.199 1.77e-10 ***
HS.Grad      4.658e-02  1.483e-02   3.142  0.00297 ** 
Frost       -5.943e-03  2.421e-03  -2.455  0.01802 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7197 on 45 degrees of freedom
Multiple R-squared:  0.736,     Adjusted R-squared:  0.7126 
F-statistic: 31.37 on 4 and 45 DF,  p-value: 1.696e-12
</code></pre>

<p>followint the protocol of the paper the final model is </p>

<pre><code>Life.Exp ~ Murder + HS.Grad + Frost (final model) 
</code></pre>

<p>because Population is > 0.05.</p>

<p>I wish to know if this final model approach is correct, and then:</p>

<pre><code>fmodel = lm(Life.Exp ~ Murder + HS.Grad + Frost, data=st)
summary(fmodel)

Call:
lm(formula = Life.Exp ~ Murder + HS.Grad + Frost, data = st)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.5015 -0.5391  0.1014  0.5921  1.2268 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 71.036379   0.983262  72.246  &lt; 2e-16 ***
Murder      -0.283065   0.036731  -7.706 8.04e-10 ***
HS.Grad      0.049949   0.015201   3.286  0.00195 ** 
Frost       -0.006912   0.002447  -2.824  0.00699 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7427 on 46 degrees of freedom
Multiple R-squared:  0.7127,    Adjusted R-squared:  0.6939 
F-statistic: 38.03 on 3 and 46 DF,  p-value: 1.634e-12
</code></pre>
"
"0.100365631932749","0.108042360909843"," 93454","<p><strong>Base Data</strong>: I have ~1,000 people marked with assessments: '1,' [good] '2,' [middle] or '3' [bad] -- these are the values I'm trying to predict for people in the future. In addition to that, I have some demographic information: gender (categorical: M / F), age (numerical: 17-80), and race (categorical: black / caucasian / latino).</p>

<p><strong>I mainly have four questions:</strong></p>

<ol>
<li><p>I was initially trying to run the dataset described above as a multiple regression analysis. But I recently learned that since my dependent is an ordered factor and not a continuous variable, I should use ordinal logistic regression for something like this. I was initially using something like <code>mod &lt;- lm(assessment ~ age + gender + race, data = dataset)</code>, can anybody point me in the right direction?</p></li>
<li><p>From there, assuming I get coefficients I feel comfortable with, I understand how to plug solely numerical values in for x1, x2, etc. -- but how would I deal with race, for example, where there are multiple responses: black / caucasian / latino? So if it tells me the caucasian coefficient is 0.289 and somebody I'm trying to predict is caucasian, how do I plug that back in since the value's not numerical?</p></li>
<li><p>I also have random values that are missing -- some for race, some for gender, etc. Do I have to do anything additional to make sure this isn't skewing anything? (I noticed when my dataset gets loaded into R-Studio, when the missing data gets loaded as <code>NA</code>, R says something like <code>(162 observations deleted due to missingness)</code> -- but if they get loaded as blanks, it does nothing.)</p></li>
<li><p>Assuming all of this works out and I have new data with gender, age, and race that I want to predict on -- is there an easier way in R to run all of that through whatever my formula with new coefficients turns out to be, rather than doing it manually? (If this question isn't appropriate here, I can take it back to the R forum.)</p></li>
</ol>
"
"0.0547448901451359","0.0540211804549215"," 94257","<p>I'm working with a biased sample of web users.  I'm only able to track responses of users who have navigated my site in a certain way, and I'd like to run an analysis to determine how certain factors (which products they are into, etc) influence how much they buy from my online store.</p>

<p>I've searched around online and found some information on the Heckman Correction, which somehow both won a Nobel Prize and also has extremely little information online, including no R support or youtube videos that I can find.</p>

<p>What are some ways you work with building models that try to correct for biased data?  Are there easy to use software libraries in R?  I have a ton of unlabeled data (products that every user has seen), so it's possible that I could create a naive model for determining probability that a user ends up in my sample if that helps, and I'd be willing to work with more advanced learners like SVM (which I've read can work better for these types of problems) in case linear regressions are a bad choice.</p>
"
"0.130520481086167","0.128795042662908"," 94468","<p>I am completely out of my depth on this, and all the reading I try to do just confuses me. I'm hoping you can explain things to me in a way that makes sense. (As always seems to be the case, ""It shouldn't be this hard!"")</p>

<p>I'm trying to help a student who is looking at the effect of social systems on prevalence of diseases in various canid host species. We want to consider social system (e.g., group-living vs. solitary) as a fixed effect, and host species as a random effect nested within social system (i.e., each species only ever has one social system type).</p>

<p>My understanding is that the best way to do this would be to do a mixed-effects logistic regression. We've done this, and it works, and we were happy. Unfortunately, her advisor is insisting that she calculate the amount of variation due to social system vs. host species vs. residual. I can't figure out how to do this via mixed-effects logistic regression, and <a href=""http://stats.stackexchange.com/questions/93450/partitioning-variance-from-logistic-regression"">my previous question on this topic</a> went unanswered.</p>

<p>Her advisor suggested doing ANOVA instead, logit-transforming disease prevalence values (the fraction of each population that is infected). This presented a problem because some of the prevalence values are 0 or 1, which would result in $-\infty$ or $\infty$ once logit-transformed. Her advisor's ""solution"" was to just substitute $-5$ and $5$ for $-\infty$ or $\infty$, respectively. This feels really kludgey and makes me cringe pretty hard. But he's the one grading her, and at this point I just want to be done with this, so if he's fine with it then whatever.</p>

<p>We are using R for this analysis. The code can be downloaded <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_code.R"">here</a>, and the input data <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_data.csv"">here</a>. The data file includes data on two different pathogens (A and B), which we are analyzing separately (as shown in the code).</p>

<p>Here's the ANOVA setup we made for Pathogen B:</p>

<pre><code>mod1.lm &lt;- lm(Seroprevalence_logit ~ Social.System + Social.System/Host.Species,
              data = prev_B)
print(mod1.anova &lt;- anova(mod1.lm))
</code></pre>

<p>This leads to my first question: <strong>Is this correct and appropriate?</strong> Factors to consider:</p>

<ul>
<li>We want to have a Model II (random effect) variable nested within a Model I (fixed effect) variable.</li>
<li>Not every social system has the same number of host species nested within it.</li>
<li>Not every host species has the same number of populations examined.</li>
<li>Not every population examined had the same number of individuals (column N_indiv in mydata.csv). This is more of a weighting problem than something more fundamental, I think.</li>
</ul>

<p>My next question, and the main one of this post, is: <strong>How do I partition the variance?</strong> Here's what we were thinking:</p>

<pre><code>MS_A &lt;- mod1.anova$""Mean Sq""[1]
MS_BinA &lt;- mod1.anova$""Mean Sq""[2]
MS_resid &lt;- mod1.anova$""Mean Sq""[3]
n &lt;- length(unique(prev_A$Social.System))
r &lt;- length(unique(prev_A$Host.Species))
VC_A &lt;- (MS_A - MS_BinA)/(n*r)
VC_BinA &lt;- (MS_BinA - MS_resid)/n
VC_resid &lt;- MS_resid
</code></pre>

<p>Unfortunately, this results in sadness using the ANOVA specification I detailed above. Here are the results for Pathogen B:</p>

<ul>
<li><code>VC_A</code> (i.e., Social.System): $-1.48$</li>
<li><code>VC_BinA</code> (i.e., Host.Species): $13.8$</li>
<li><code>VC_resid</code>: $5.57$</li>
</ul>

<p>Research leads me to believe that this should result in variance component percentages of 0%, 71.3%, and 28.7%, respectively. However, this is unsatisfying for two reasons:</p>

<ul>
<li>The p-value for Social.System from the ANOVA was ~$0.025$, suggesting that it should account for at least <em>some</em> of the observed variance. (Host.Species had a p-value of ~$3*10^{-5}$.)</li>
<li>I'm concerned that a negative variance component might be a red flag for something.</li>
</ul>

<p>Please, any assistance you can render on either of these questions would be greatly appreciated. I TA'd an undergraduate course on biostatistics, so I've got some background, but I just can't seem to figure out these specific issues. Thanks in advance.</p>
"
"0.0547448901451359","0.0540211804549215"," 95001","<p>I have a regression model and I want to determine the relative importance of the predictors. I used the package <code>relaimpo</code> in r.</p>

<p>The package says that 98.84% of the variance is explained by the model</p>

<pre><code>Response variable: C 
Total response variance: 115.4857 
Analysis based on 161187 observations 

10 Regressors: 
I(VPT^(-0.66)) TPA S SD TPA:VPT S:SD TPA:SD TPA:S SD:VPT S:VPT 
Proportion of variance explained by model: 98.84%
</code></pre>

<p>I did not normalize the metrics.</p>

<pre><code>Metrics are not normalized (rela=FALSE). 
</code></pre>

<p>These are my relative importance metrics:</p>

<pre><code>Relative importance metrics: 

                        lmg
I(VPT^(-0.66)) 0.2647988336
TPA            0.0776291924
S              0.0816760485
SD             0.2661375234
TPA:VPT        0.0779008229
S:SD           0.0036628215
TPA:SD         0.0002298444
TPA:S          0.0002684624
SD:VPT         0.0065747570
S:VPT          0.0114800491
</code></pre>

<p>They sum up <code>0.79</code>. Shouldn't they sum up to <code>0.9884</code>? If so, why don't they?</p>
"
"0.0836242010007091","0.0825187161885156"," 95190","<p>I am conducting a <strong>meta-analysis</strong> from a large number of studies. In each study are <strong>compared weights of two groups</strong> (fishes with and without internal parasite). I am interested if the <strong>weight can explain the presence/absence of a parasite</strong>. From forest plot it seems to be clear that in each continent (and for world as a whole)  there is a <strong>clear preference towards bigger fishes</strong>.</p>

<p>I have studies from all over the world, that means <strong>the term ""bigger"" fish is a relative</strong>. The bigger fish in Africa could be very small in comparison with smallest fish in Australia. <strong>Because of this fact I have used random-effect model</strong>. It should be a good choice according to some textbooks.</p>

<p><strong>Is random-effect model appropriate for my situation?</strong></p>

<p>Could you please help me with <strong>interpretation of model output</strong>? There are some things I do not understand:</p>

<p>P-value from model results is highly significant. So it is highly probable that the weight has something to do with parasite infestation. However, the Test for Heterogeneity is also significant. Does it means that <strong>my model do not meet the assumption for normal distribution of residuals?</strong> Similarly like a in the case of some simple regression?</p>

<p><strong>And what about tau^2, tau, I^2 and H^2?
Is any of them similar to R-squared?</strong></p>

<p>Please give me some guidance (in layman terms if possible).</p>

<pre><code>library(metafor)
mod_weight &lt;- rma(yi, vi, data = dat_weight); summary(mod_weight)

Random-Effects Model (k = 62; tau^2 estimator: REML)

  logLik  deviance       AIC       BIC      AICc  
-65.0051  130.0103  134.0103  138.2320  134.2172  

tau^2 (estimated amount of total heterogeneity): 0.3618 (SE = 0.0808)
tau (square root of estimated tau^2 value):      0.6015
I^2 (total heterogeneity / total variability):   86.67%
H^2 (total variability / sampling variability):  7.50

Test for Heterogeneity: 
Q(df = 61) = 395.7163, p-val &lt; .0001

Model Results:

estimate       se     zval     pval    ci.lb    ci.ub          
  0.8007   0.0853   9.3830   &lt;.0001   0.6334   0.9679      *** 
</code></pre>

<p><strong><em>P.S. the fish-parasite research is a made-up story :)</em></strong></p>
"
"NaN","NaN"," 95203","<p>I need to perform stepwise binary logistic regression (The horror! The horror!) on 1.5 million observations.  This takes far too long in SAS, so I'm wondering if I can use R to process it in a multicore environment.  Apparently package gmulti (<a href=""http://www.jstatsoft.org/v34/i12/paper"" rel=""nofollow"">http://www.jstatsoft.org/v34/i12/paper</a>) will do the trick, but it's not clear to me if it will do that outside of its genetic algorithm.  That still might work for me, but I don't have a large number of variables (about 30) so it's not necessary.  As long as the results of the brute force and ga approach could be assured to be similar, then I might try it.  However, I see others have had problems getting the parallel feature to run: <a href=""https://stat.ethz.ch/pipermail/r-help/2013-April/351820.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help/2013-April/351820.html</a>.  Any other suggestions 
on how to parallelize logistic regression in R?  A web search turned up a couple of papers, but not much that seemed specific to R.  And please spare me a lecture about stepwise regression-I'm very well aware of the pitfalls.  I'm replicating someone else's analysis.  I'm using a Windows 64 bit system.</p>
"
"0.109489780290272","0.108042360909843"," 95451","<p>I've been having an argument with a friend of mine, and it's very possible I'm wrong here.</p>

<p>We are performing binary logistic regression on a dataset with 10000 observations, classifying action as ""good"" or bad"".  There are two independent variables (x1, x2), and class variable (y, with values ""good"" or ""bad"").  In this dataset, we  have 7,500 observations classified as ""bad"" and 2,500 classified as ""good"".  This is because there are several different ways for a user to perform a ""bad"" action, but only one way for them to perform a ""good"" action.</p>

<p>We are doing our analysis in R using the <code>glm()</code> function.</p>

<p>We create training data by randomly sampling 7,500 observations from the dataset, and create test data from the other 2,500 observations.  we then build a model using binary logistic regression on the training data, then test it on the test data.  The accuracy of our model is 75%.</p>

<p><strong>Can we say our model is better than guessing?</strong></p>

<p>He says that this model is no better than guessing.  Even though the error rate is better than 50%, because the original data had a prevalence of ""bad"" classifiers, we would need our model to predict better than 75% in order to say it performs better than random guessing.</p>

<p>I disagree...but I can't defend my point with anything other than ""that doesn't seem right"".  Can someone shed some light on the correct interpretation, and the reason for it?</p>
"
"0.0547448901451359","0.0540211804549215"," 95494","<p>I am doing a regression analysis with multiple variables and comparing it to a one-variable null hypothesis. The goal is to see which model provides a better explanation. The topic is information diffusion, hence the names DIFfusion, INDustry, etc. Also I must say that DIF is many times zero (has a long tail) and all variables go between 0 and 1. Anyways, the hypothesis can be translated unto:</p>

<p>H_0: DIF ~ REL</p>

<p>H_1: DIF ~ REL+COM*REL+IND*REL</p>

<p>H_2: DIF ~ REL+COM*REL+IND*REL+SIZE1+SIZE2</p>

<p>REL multiplies the other two variables as theoretically they are related and together they 'should' give a better prediction of DIF.</p>

<p>Now, the tricky things for me are two.</p>

<p>First, (and most importantly), although R values are over 0.6 -good enough for this topic- when I see the residuals they tend to follow a pattern that is similar for all three hypotheses. I really don't know why or how to address it.</p>

<p><img src=""http://i.stack.imgur.com/XRh2q.png"" alt=""Residuals""></p>

<p><img src=""http://i.stack.imgur.com/JXeCI.png"" alt=""QQplot""></p>

<p><img src=""http://i.stack.imgur.com/ZCvJe.png"" alt=""histogram""></p>

<p>Second, I have the intuition that the dependent variable (DIF) behaves like an iceberg and sea-level. Meaning that the lower the sea level (represented by SIZE1-2), I could see more of the shape of the iceberg, where the shape of the iceberg is given by the other variables. Have you encountered situations like this? How would you model/test it?</p>

<p>Any advice?</p>

<p>Using R by the way.</p>
"
"0.0999500374687773","0.0887658573546531"," 96991","<p>I have done survival analysis. I used Kaplan-Meir to do the survival analysis. </p>

<p>Description of data: 
My data set is large and data table has close 120,000 records of survival information belong to 6 groups.</p>

<p>Sample: </p>

<pre><code>   user_id   time_in_days   event total_likes total_para_length group
1:       2          4657     1       38867        431117212   AA
2:       2          3056     1       31392        948984460   BB
3:       2            49     1          15            67770   CC
4:       3          4181     1       15778        379211806   BB
5:       3            17     1           3            19032   CC
6:       3          2885     1       12001        106259666   EE
</code></pre>

<p>After fitting the survival curves and plotting it, I see they are similar but yet at any given point in time their survival proportions don't seem to look like identical.</p>

<p>Here is the plot:
<img src=""http://i.stack.imgur.com/9wLYH.png"" alt=""Survival Curves""></p>

<p>I ran a hypothesis test where my H0: There is not difference between the survival curves and here is the results that I got. </p>

<pre><code>&gt; survdiff(formula= Surv(time, event) ~ group, rh=0)
Call:
survdiff(formula = Surv(time, event) ~ group, rho = 0)

             N Observed Expected (O-E)^2/E (O-E)^2/V
group=FF 28310    27993    28632      14.3      19.0
group=AA 64732    63984    67853     220.6     460.1
group=BB 19017    18690    16839     203.4     245.6
group=CC  9687     9536     8699      80.6      91.0
group=DD 13438    13187    11891     141.3     164.2
group=EE  3910     3847     3324      82.4      89.7

 Chisq= 788  on 5 degrees of freedom, p= 0  
</code></pre>

<p>I am little confuse by trying to figure out what it means, specially since I got <code>p-value=0</code>. </p>

<p>I am fairly new to survival analysis so after reading and digging through I realized that this is a non-parametric as I understand which means that it doesn't make any assumptions of the underline distributions of the time.</p>

<p>After reading about cox-proportional hazard function and going over <a href=""http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf"" rel=""nofollow"">c-cran pdf</a> I performed a cox regression test and here is what I got from that: </p>

<pre><code>&gt; cox_model &lt;- coxph(Surv(time, event) ~ X)
&gt; summary(cox_model)
Call:
coxph(formula = Surv(time, event) ~ X)

  n= 139094, number of events= 137237 

         coef  exp(coef)   se(coef)       z Pr(&gt;|z|)    
X1 -7.655e-05  9.999e-01  1.504e-06 -50.897   &lt;2e-16 ***
X2 -1.649e-10  1.000e+00  5.715e-11  -2.886   0.0039 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

   exp(coef) exp(-coef) lower .95 upper .95
X1    0.9999          1    0.9999    0.9999
X2    1.0000          1    1.0000    1.0000

Concordance= 0.847  (se = 0.001 )
Rsquare= 0.111   (max possible= 1 )
Likelihood ratio test= 16307  on 2 df,   p=0
Wald test            = 7379  on 2 df,   p=0
Score (logrank) test = 4628  on 2 df,   p=0
</code></pre>

<p>My big X is generated by doing rbind on total_like and total_para_length. Looking at Rsquare and P-Values I am not sure what really is going on here. If I can't throw away the Null-Hypothesis I should give a larger p-value. </p>
"
"NaN","NaN"," 97774","<p>I am currently analyzing a data set having to do with oak tree mortality. I am trying to understand the correlation between dead crowns (dead = <code>0</code>, live = <code>1</code>) and the distance to <strong>(1)</strong> nearest dead crown and <strong>(2)</strong> distance to nearest bay tree. (See Below).</p>

<p>I have run a probit regression model:</p>

<pre><code>Living&lt;-sample(0:1,10,replace=T)
    Dead_Dist&lt;-c(158.68,99.62,64.42,64.42,86.92,117.77,41.81,41.81,54.73,64.35)
    Bay_Dist&lt;-c(92.47,179.92,317.73,365.23,58.70,193.23,330.36,123.14,88.65,72.34)
    mydata&lt;-cbind(Living,Dead_Dist,Bay_Dist)
    mydata1&lt;-data.frame(mydata)
    myprobit &lt;- glm(Living ~ Bay_Dist + Dead_Dist, family = binomial(link = ""probit""),data = mydata1)
    myprobit
</code></pre>

<p>But I am at a loss for how to plot and interpret the results, as I am fairly new to R and regressions.<br>
If anyone has suggestions with how to proceed with this analysis, I would appreciate it!</p>
"
"0.070675349274022","0.0697410440814588"," 97811","<p>I am using leave-one-out cross-validation to evaluate a linear regression model. In subsequent analysis, I need three specific values for each observation: observed value, predicted value, prediction standard error. Prediction standard error values can be retrieved from function <code>predict.lm</code> setting argument <code>se.fit = TRUE</code>. The following code (adapted from <a href=""http://www.analyticbridge.com/profiles/blogs/cross-validation-in-r-a-do-it-yourself-and-a-black-box-approach"" rel=""nofollow"">here</a>) can be used to do what I currently need:</p>

<pre><code>library(faraway)
gala[1:3, ]
c1 &lt;- c(1:30)
gala2 &lt;- cbind(gala, c1)
gala2[1:3, ]
obs  &lt;- numeric(30)
pred &lt;- numeric(30)
se   &lt;- numeric(30)
for (i in 1:30) {
     model1  &lt;- lm(Species ~ Endemics + Area + Elevation,
                   subset = (c1 != i), data = gala2)
     specpr  &lt;- predict(model1, gala2[i, ], se.fit = TRUE)
     obs[i]  &lt;- gala2[i, 1]
     pred[i] &lt;- specpr$fit
     se[i]   &lt;- specpr$se.fit
}
res &lt;- data.frame(obs, pred, se)
head(res)
  obs       pred       se
1  58  70.185063 5.524249
2  31  72.942732 6.509655
3   3  -8.303608 7.055163
4  25  20.948932 6.998093
5   2 -15.953141 7.403062
6  18  27.274440 6.220029
</code></pre>

<p>I searched through the documentation of some of the packages that offer functions for cross-validation, but did not find any that saves prediction standard errors. Is there any package that already offers such functionality?</p>
"
"0.0547448901451359","0.0540211804549215"," 99924","<p>For regression analysis, it is often useful to know the data generating process to check how the used method works. While it is fairly simple to do this for a simple linear regression, this is not the case when the dependent variable has to follow a specific distribution. </p>

<p>Consider a simple linear regression:</p>

<pre><code>N    &lt;- 100
x    &lt;- rnorm(N)
beta &lt;- 3 + 0.4*rnorm(N)
y    &lt;- 1 + x * beta + .75*rnorm(N)
</code></pre>

<p>Is there any way to use the same approach but to get <code>y</code> be other than normal, say left skewed?</p>
"
"0.137771618924287","0.135950322810847","100101","<p>I am currently working at work on a project that attempts to predict an environmental change variable. I am personally not a huge fan of the project, but I still want to do the best job possible. Anyhow, let me first describe the properties of the data, and then state my question. </p>

<p>The environmental variable we are trying to model is continuous, and ranges from 0 to 25 (it can have values such as 0.1345 or 1.2335 or 5.674). The environmental variable also has a significant mass at zero (around 30% of the data are zero values), and most of the data is in the range between > 0 and &lt; 1. To complicate things further, around 3% of the data have extreme values of greater than 2. In my opinion, the distribution of the data resembles a Tweedie distribution. </p>

<p>We have around 5 million observations in the dataset. We are going to predict the environmental variable using a set of eight explanatory variables.</p>

<p>We have been modeling the prediction of the environmental for a week, and the predictive power of our results has been meager. Here are the different modeling approaches I have used:</p>

<ol>
<li><p>GLM model with a tweedie distribution (glm function in r). This model approach overestimate the environmental change when the variables has low values. Most values between 0 - 1 are significantly overestimated, and the model does not predict high values very well either. The GLM approach produces a very bad fit for our data.</p></li>
<li><p>Generalized Additive Model with a tweedie distribution (bam function from mgcv package in r). This approaches fits the data slightly better than the model above, but still significantly overestimates values in the low range.</p></li>
<li><p>Regression tree model (rpart model in r). The regression tree model most accurately predicts values in the lower range of the distribution, but fails to predict zero values, and performs also poorly for values greater than 2. </p></li>
<li><p>Boosted regression model (dismo package in r using gbm.step). The model significantly overestimate values in the lower range of the distribution. For example, if values are 0.23 it predicts values to be 1.67 and so forth. I believe the gbm.step model to be inadequate for our data, since the family of distribution in the package only models the bernoulli (=binomial), poisson, laplace or gaussian family. None of which accurately describe our distribution.</p></li>
</ol>

<p>Since our dataset is very large, I split the data 50/50 into a test and training dataset, and evaluated model fit on a variety of test statistics for the test data set. </p>

<p>Knowing the structure of our data, can anyone think of our modeling approaches that would possibly produce better predictive results? I am still new to machine learning techniques, and I hope that someone might know of other techniques that could be employed.   </p>

<p>My strongest program language is R, but I can also do this analysis in Python or Stata. </p>
"
"0.0632139541241014","0.0623782861551805","100451","<p>I am confused about the usage of <code>lm</code> and <code>predict</code> in R. I have a predictor variable $x$ and an output variable $y$, and I want to perform a univariate regression analysis. I have $15$ predictor samples from $x$, and $15$ output samples from $y$ which I will use for training. Then, I want to give a number as a predictor to the trained regression model and get the output value from the model. Here is an example R code:</p>

<pre><code>set.seed(23)
x &lt;- rnorm(15)
y &lt;- x + 1
predict(lm(y ~ x), .3)
</code></pre>

<p>But the result of this <code>predict</code> call is a vector of size $15$, which has actually the same values as in $y$, although I was expecting it to be just a single number, which is the output of the test predictor $.3$. What's wrong with that code, and how should I change it to get what I want to get? I know this is a very basic question, but I am new to that stuff. Thanks!</p>
"
"0.158034885310254","0.149707886772433","101077","<p>I have very big data and low number of observations. So I decided to use PCA to reduce dimension of the data. The following is R example (just an dummy example - for workout):</p>

<pre><code>xmat &lt;- matrix(sample(-1:1, 100000, replace = TRUE), ncol = 1000)
colnames(xmat) &lt;- paste (""V"", 1:1000, sep ="""")
rownames(xmat) &lt;- paste(""S"", 1:100, sep = """")
</code></pre>

<p>In this example dataset I have <code>1000</code> variables and <code>100</code> observations / subjects. </p>

<p>I am doing PCA. Lets say.</p>

<pre><code>out &lt;- princomp(xmat)
Error in princomp.default(xmat) : 
  'princomp' can only be used with more units than variables
</code></pre>

<p>Q1: is there a way to reduce dimensionality with <code>p &gt; n</code> ? I would like to use all variables information as opposed to representative ones. Without having proper solution I went anyway to use cluster analysis of variables to categorize the variables and pick the randomly from the clusters. </p>

<p>To create a list of representative variables I tried to cluster the variables.</p>

<pre><code># cluster variables 
d &lt;- dist(t(xmat), method = ""euclidean"") # distance matrix
fit &lt;- hclust(d, method=""ward"")
plot(fit)
groups = cutree(fit,40)
groupd &lt;- data.frame(var = names(groups), group = groups)
</code></pre>

<p>What I am thinking is randomly pick one variable from each group above and use this in PCA. Assume that I have the following y variable.</p>

<pre><code>set.seed(1234)
yvar.d &lt;- data.frame (subject = c(paste(""S"", 1:100, sep = """")), yvar = rnorm (100, 50,10))
</code></pre>

<p><strong>Here is my question</strong>: </p>

<ol>
<li>What could be statistical challenge of using cluster analysis ?</li>
<li><p>Can we use PCA scores in predictions of y. How ? Just multiple
regression or we can introduce something such as variance explained
by each components in the model ?</p>

<p><strong>Edits:</strong></p>

<p>Based on the discussions (see the comments below), I am using different function to do PC analysis.</p></li>
</ol>

<p>""The calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using eigen on the covariance matrix. This is generally the preferred method for numerical accuracy. The print method for these objects prints the results in a nice format and the plot method produces a scree plot."" - from function help. </p>

<pre><code>     out1 &lt;- prcomp(xmat)
      out1$x[1:3,1:3]
                      PC1        PC2       PC3
S1  2.940862 -2.7379835  6.527103
S2 -1.081124 -0.5294796 -0.276591
S3  2.375710  0.4505205 -4.236289

   out1$sdev
 screeplot(out1,npcs=30, type=""lines"",col=3) # 30 PCA plotted
</code></pre>

<p><img src=""http://i.stack.imgur.com/gMJys.jpg"" alt=""enter image description here""></p>

<pre><code> out1$rotation
</code></pre>

<p>I also come to see an example in SO <a href=""http://stackoverflow.com/questions/10876040/principal-component-analysis-in-r"">how to use PCA in prediction</a>. Here is my workout: </p>

<pre><code>## take our training and test sets
YY &lt;-  yvar.d$yvar 
prop &lt;- 0.5
train = sample(1:length(YY), round(length(YY)*prop,0))


# data for testing model purpose 
testid = setdiff (1:length(YY), train)
YY1 &lt;- YY
newXPCA &lt;- data.frame(out1$x)
test.data &lt;- data.frame (y = YY1[testid],newXPCA[testid,]) 
test.data[1:10,1:10]

train.data &lt;- data.frame(y= YY1[train],newXPCA [train,])
train.data[1:10,1:10]

## fit the PCA
pc &lt;- prcomp(train.data[, -1])
trainwPC &lt;- data.frame (y = train.data$y, pc$x)

model1 &lt;- lm(y ~ ., data = trainwPC)

#predict() method for class ""prcomp""
test.p &lt;- predict(pc, newdata = test.data)
pred &lt;- predict(model1, newdata = data.frame(test.p), type = ""response"")
pred 
Warning message:
In predict.lm(model1, newdata = data.frame(test.p), type = ""response"") :
  prediction from a rank-deficient fit may be misleading
</code></pre>

<p>I just adopted this script from the SO link, I am not sure about accuracy of the script. </p>

<p>I still have technical questions remaining such as clarification to <strong>remaining question 2</strong> above: </p>

<p>(1) If I want to split data into training and test set by sampling <code>50% of data</code> (as show in the script). Should I do just multiple regression with y and the <code>out1$x</code> ? how many components to use ? is variance of each component play role in good model selection such as avoid over-fitting ? How ? </p>

<p>(2) Clustering (using x clusters) vs PCA analysis (with subset of x components vs all ) what would be statistically favorite for predictions in the situations where have <code>p &gt; n</code> ? As I said to my mind the PCA analysis can use all information but I do not know if there is downside of such information such as <code>over-fitting</code> and ""error consumption"". </p>

<p>Worked example appreciated.   </p>
"
"0.047410465593076","0.0623782861551805","102689","<p>I have a problem with some analysis I need to do.</p>

<p>I have a series of regressions. Some of the predictors of these regression are categorical with multiple levels. I performed regressions, both linear and logistic, choosing a baseline for these category according to various factors.</p>

<p>The problem is that my colleagues asked not only for a confrontation of the factors to a baseline but also a pairwise confrontation. Like you it's used to do with a post-hoc test for ANOVA (they are pretty new to regressions and their benefits).</p>

<p>How should I approach this?
I thought of some solutions:</p>

<ul>
<li>Subsetting: That is subset the data to include two factors at time, and therefore repeating the regression once per every subset.</li>
<li>Splitting: Splitting the category column in a column for every factor and put 0 and 1 as levels. This approach can furthermore be conducted in two ways:
<ul>
<li>Putting all the new columns in the regression (minding that they are mutually exclusive).</li>
<li>Putting one column at time, multiplying the regressions.</li>
</ul></li>
</ul>

<p>Which approach would you suggest, minding statistical correctness and workload?</p>

<p>Especially, what's the conceptual difference between the three methods?</p>

<p>Thanks a lot!</p>
"
"0.0899550337218996","0.0986287303940589","102892","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Statistical test chosen: logistic regression</p>

<p>I need to find the variables that best explain variations in the outcome variable (I am not interested in making predictions).</p>

<p>The problem: This question is a follow-up on the 2 questions listed below. From them, I got that performing automated stepwise regression has its downsides. Anyway, it seems that my sample size would be too small for that. It seems that my sample is also too small to enter all variables at once (using the SPSS 'Enter' method). This leaves me with my issue unresolved: how can I select a subset of variables from my original long list in order to perform multivariate logistic regression analysis?</p>

<p>UPDATE1: I am not an statistician, so I would appreciate if jargons can be reduced to the minimum. I am working with SPSS and am not familiar with other packages, so options that could be run with that software would be highly preferable.</p>

<p>UPDATE2: It seems that SPSS does not support LASSO for logistic regression. So following one of your suggestions, I am now struggling with R. I have passed through the basics, and managed to run a univariate logistic regression routine successfully using the glm code. But as I tried glmnet with the same dataset, I am receiving an error message. How could I fix it? Below is the code I used, followed by the error message:</p>

<pre><code>data1 &lt;- read.table(""C:\\\data1.csv"",header=TRUE,sep="";"",na.string=99:9999)

y &lt;- data1[,1]

x &lt;- data1[,2:45]

glmnet(x,y,family=""binomial"",alpha=1)  

**in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  : 
(list) object cannot be coerced to type 'double'**
</code></pre>

<p>UPDATE3: I got another error message, now related to missing values. My question concerning that matter is <a href=""http://stats.stackexchange.com/questions/104194/how-to-handle-with-missing-values-in-order-to-prepare-data-for-feature-selection"">here</a>. </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/88482/can-univariate-linear-regression-be-used-to-identify-useful-variables-for-a-subs"">Can univariate linear regression be used to identify useful variables for a subsequent multiple logistic regression?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856"">Algorithms for automatic model selection</a></li>
</ul>
"
"0.0316069770620507","0.0311891430775903","102902","<p>i have data that includes clicks, spend, signups and date. </p>

<p>for 1 week, i turn off advertising spend to see what clicks and signups are.
the next week, i turn advertising back on to see what the new clicks or signups are.</p>

<p>Given this 2 sets of data, how can i run regression analysis to see how impactful advertising is?</p>

<p>Should i run a regression analysis on y=(signups_week2-signups_week1) and x=(spend_week2-spend_week1) ?</p>

<p>Thanks,
J </p>
"
"0.0952986215201744","0.103442685121077","103077","<p>I am having a lot of fun with regression analysis at the moment, and by fun I mean bashing myself repeatedly over the head. I have a set of 200 data points, by filtering on a property of interest, I end up with 153 points of use. </p>

<p>I initially used these 153 points to generate a linear regression, with an excellent R${^2}$ and a plot of fitted vs actual variables of almost a perfect diagonal. Great! However, it was suggested that this might only be an internally predictive model (which as I understand it means the model fits the data, rather than the opposite). So, I then tried this: I randomly selected a sample of 100 of the 153 results, and built the same model, it still gave a relatively good fit. I then used the predict function in R to try to predict the outcome of the other 53 records. It did not go well. What I got was one of 2 things.</p>

<ol>
<li>the predictions made no sense at all, not even on the same scale as the actual values.</li>
<li>most of the predictions made sense (although weren't very accurate) and one or two, were on an entirely different scale (orders of magnitude larger, or smaller).</li>
</ol>

<p>Since the model I am fitting has time as the response variable, it was suggested I use a Gamma fit regression instead of a plain old linear regression. I tried this and ended up essentially with the result.</p>

<p>So, am I using R correctly, was Gamma a good choice for this? I'm pretty sure my data is good (non biased) so if I am unable to predict, despite the good model - does this mean my model is useless? I've been working on this for some weeks now, and it would be great if I could salvage something.</p>

<p>The R commands I have used:</p>

<pre><code>modelSet&lt;-sample(1:nrow(myData),100)
modelData&lt;-myData[modelSet,]
predictData&lt;-myData[-modelSet,]

fit&lt;-lm(""time~(x1+x2+x3+x4+x5+x6)^3"", data=modelData)
pred&lt;-predict(fit, predictData)
plot(predictData$time, pred) &lt;- gives a really not useful plot


fit2&lt;-glm(""time~(x1+x2+x3+x4+x5+x6)^3"", data=modelData, family=Gamma) # tried with link=log too
pred2&lt;-predict(fit2, predictData)
plot(predictData$time, pred2) &lt;- gives an even less useful plot
</code></pre>
"
"0.0632139541241014","0.0623782861551805","103842","<p>Is there a way to statistically compare r-squared across 2 groups using nested models in multigroup analysis? I know how to use lavaan to test various other parameters across groups (e.g. regression coefficients, intercepts, variances...etc), but I can't find documentation for how to compare R-squared. </p>

<p>Here is an example of what I mean:</p>

<pre><code>HS.model &lt;-  'visual  =~ x1 + x2 + x3
              textual =~ x4 + x5 + x6
              speed   =~ x7 + x8 + x9 
              visual + textual + speed ~ grade
              textual + speed ~ visual''
fit &lt;- cfa(HS.model, data=HolzingerSwineford1939, group=""sex"")
summary(fit, fit.measures=TRUE, rsquare=TRUE)
</code></pre>

<p>This produces (among other things) the following R-squared statistics:</p>

<pre><code>R-Square Group 1:

x1                0.519
x2                0.110
x3                0.375
x4                0.705
x5                0.729
x6                0.662
x7                0.458
x8                0.668
x9                0.266
visual            0.057
textual           0.217
speed             0.194

R-Square Group 2:

x1                0.669
x2                0.266
x3                0.304
x4                0.763
x5                0.721
x6                0.731
x7                0.320
x8                0.495
x9                0.515
visual            0.057
textual           0.258
speed             0.393
</code></pre>

<p>If I wanted to determine if ""grade"" and ""vision"" (predictors in the example) accounted for significantly more variability in speed for boys (.194) than girls (.393), how would I do that? </p>
"
"0.0899550337218996","0.0986287303940589","104194","<p>My situation:</p>

<ul>
<li>small sample size: 116 </li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
<li>most cases in the sample and most variables have missing values.</li>
</ul>

<p>Approach to feature selection chosen: LASSO</p>

<p>R's glmnet package won't let me run the glmnet routine, apparently due to the existence of missing values in my data set. There seems to be various methods for handling missing data, so I would like to know:</p>

<ul>
<li>Does LASSO impose any restriction in terms of the method of imputation that I can use?</li>
<li>What would be the best bet for imputation method? Ideally, I need a method that I could run on SPSS (preferably) or R.</li>
</ul>

<p>UPDATE1: It became clear from some of the answers below that I have do deal with more basic issues before considering imputation methods. I would like to add here new questions regarding that. On the the answer suggesting the coding as constant value and the creation of a new variable in order to deal with 'not applicable' values and the usage of group lasso:</p>

<ul>
<li>Would you say that if I use group LASSO, I would be able to use the approach suggested to continuous predictors also to categorical predictors? If so, I assume it would be equivalent to creating a new category - I am wary that this may introduce bias.</li>
<li>Does anyone know if R's glmnet package supports group LASSO? If not, would anyone suggest another one that does that in combination with logistic regression? Several options mentioning group LASSO can be found in CRAN repository, any suggestions of the most appropriate for my case? Maybe SGL?</li>
</ul>

<p>This is a follow-up on a previous question of mine (<a href=""http://stats.stackexchange.com/questions/102892/how-to-select-a-subset-of-variables-from-my-original-long-list-in-order-to-perfo"">How to select a subset of variables from my original long list in order to perform logistic regression analysis?</a>).</p>

<p>OBS: I am not a statistician.</p>
"
"0.070675349274022","0.0697410440814588","104306","<p>One common thing to do when doing Principal Component Analysis (PCA) is to plot two loadings against each other to investigate the relationships between the variables. In the paper accompanying the <a href=""http://www.jstatsoft.org/v18/i02"">PLS R package</a> for doing Principal Component Regression and PLS regression there is a different plot, called the <em>correlation loadings plot</em> (see figure 7 and page 15 in the paper). The <em>correlation loading</em>, as it is explained, is the correlation between the scores (from the PCA or PLS) and the actual observed data. </p>

<p>It seems to me that loadings and correlation loadings are pretty similar, except that they are scaled a bit differently. A reproducible example in R, with the built in data set mtcars is as follows:</p>

<pre><code>data(mtcars)
pca &lt;- prcomp(mtcars, center=TRUE, scale=TRUE)

#loading plot
plot(pca$rotation[,1], pca$rotation[,2],
     xlim=c(-1,1), ylim=c(-1,1),
     main='Loadings for PC1 vs. PC2')

#correlation loading plot
correlationloadings &lt;- cor(mtcars, pca$x)
plot(correlationloadings[,1], correlationloadings[,2],
     xlim=c(-1,1), ylim=c(-1,1),
     main='Correlation Loadings for PC1 vs. PC2')
</code></pre>

<p><img src=""http://i.stack.imgur.com/GJA4c.png"" alt=""loadingplot"">
<img src=""http://i.stack.imgur.com/y32sb.png"" alt=""correlationloadinsplot""></p>

<p>What is the difference in interpretation of these plots? And which plot (if any) is best to use in practice?</p>
"
"0.0316069770620507","0.0311891430775903","104559","<p>Assume I have an outcome like this:</p>

<pre><code>x&lt;-c(rep(2.83,3),
     rep(3.17,4),
     rep(3.83,4),
     rep(4.17,5),
     rep(4.83,8),
     rep(5.5,3),
     rep(7.17,5),
     rep(8.17,7),
     rep(8.83,12),
     rep(9.5, 12),
     rep(9.83,17),
     rep(10.17,50)) 
hist(x)
hist(log(x))
</code></pre>

<p>I would use it as a continuous dependent variable in a regression analysis but have no idea how to make it normal-like. I have tried few algorithms like log, quadratic, or boxcox, but it seems neither way is making better.  Does anybody here have some suggestions?</p>
"
"0.100365631932749","0.0900353007582025","104595","<p>I've been reading <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a>, <a href=""http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html"" rel=""nofollow"">http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html</a>, and <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and I'm still a little lost on how to do a power analysis for my data. I want to be able to determine what N I should have if I have an interaction between a categorical variable (with 3 levels) and a continuous variable.</p>

<p><a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a> provides some information, but I can't figure out how to simulate the relationship between the categorical and continuous variables and outcome.</p>

<blockquote>
  <p>set.seed(1)<br></p>
  
  <p>repetitions = 1000<br>
  N = 10000<br>
  n = N/8<br>
  var1  = c(   .03,    .03,    .03,    .03,    .06,    .06,    .09,   .09)<br>
  var2  = c(     0,      0,      0,      1,      0,      1,      0,     1)<br>
  rates = c(0.0025, 0.0025, 0.0025, 0.00395, 0.003, 0.0042, 0.0035, 0.002)<br></p>
  
  <p>var1    = rep(var1, times=n)<br>
  var2    = rep(var2, times=n)<br>
  var12   = var1**2<br>
  var1x2  = var1 *var2<br>
  var12x2 = var12*var2<br></p>
  
  <p>significant = matrix(nrow=repetitions, ncol=7)<br></p>
  
  <p>startT = proc.time()[3]<br>
  for(i in 1:repetitions){<br>
   responses          = rbinom(n=N, size=1, prob=rates)<br>
   model              = glm(responses~var1+var2+var12+var1x2+var12x2, <br>
                            family=binomial(link=""logit""))<br>
   significant[i,1:5] = (summary(model)$coefficients[2:6,4]&lt;.05)&lt;br&gt;
&gt;      significant[i,6]   = sum(significant[i,1:5])&lt;br&gt;
&gt;      modelDev           = model$null.deviance-model$deviance<br>
   significant[i,7]   = (1-pchisq(modelDev, 5))&lt;.05<br>
  }<br>
  endT = proc.time()[3]<br>
  endT-startT<br></p>
  
  <p>sum(significant[,1])/repetitions      # pre-specified effect power for var1<br>
  [1] 0.042<br>
  sum(significant[,2])/repetitions      # pre-specified effect power for var2<br>
  [1] 0.017<br>
  sum(significant[,3])/repetitions      # pre-specified effect power for var12<br>
  [1] 0.035<br>
  sum(significant[,4])/repetitions      # pre-specified effect power for var1X2<br>
  [1] 0.019<br>
  sum(significant[,5])/repetitions      # pre-specified effect power for var12X2<br>
  [1] 0.022<br>
  sum(significant[,7])/repetitions      # power for likelihood ratio test of model<br>
  [1] 0.168<br>
  sum(significant[,6]==5)/repetitions   # all effects power<br>
  [1] 0.001<br>
  sum(significant[,6]>0)/repetitions    # any effect power<br>
  [1] 0.065<br>
  sum(significant[,4]&amp;significant[,5])/repetitions   # power for interaction terms<br>
  [1] 0.017<br></p>
</blockquote>

<p>I feel like I should be able to adapt the code from <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and that this would be a better, more succinct option</p>

<blockquote>
  <p>library(rms)</p>
  
  <p>tmpfun &lt;- function(n, beta0, beta1, beta2) { <br>
     x &lt;- runif(n, 0, 10) <br>
     eta1 &lt;- beta0 + beta1*x <br>
     eta2 &lt;- eta1 + beta2 <br>
     p1 &lt;- exp(eta1)/(1+exp(eta1)) <br>
     p2 &lt;- exp(eta2)/(1+exp(eta2)) <br>
     tmp &lt;- runif(n) <br>
     y &lt;- (tmp &lt; p1) + (tmp &lt; p2) <br>
     fit &lt;- lrm(y~x) <br>
     fit$stats[5] <br>
  } <br></p>
  
  <p>out &lt;- replicate(1000, tmpfun(100, -1/2, 1/4, 1/4)) <br>
  mean( out &lt; 0.05 ) <br></p>
</blockquote>

<p>but I'm not completely sure how to do so. I'm assuming tmpfun(100,-1/2, 1/4,1/4) is specifying the N and betas that you want, but how do I adjust tmpfun to another (categorical) variable and include an interaction term? Ultimately the equation should include 6 betas: the intercept, the beta for x, the beta for z1, the beta for z2, the interaction term between x and z1, and the interaction term between x and z2. </p>

<p>Finally, I can't find any reliable sources on what sorts of ""effect sizes"" I should be using as small or medium. </p>

<p>Let me know if I can provide more information!</p>
"
"0.137944157024362","0.142926609013865","106347","<p>I am somewhat familiar with various ways of testing mediation for factors in different types of regression analysis.  (I'm using R and currently working with a multilevel binary logistic regression.)  But now I have a situation in which I'd like to test whether one interaction between factors mediates another, and I'm not sure how this could be done properly.  </p>

<p>To give a simplified example of what I am interested in doing:</p>

<p>I have a multilevel binary model using student characteristics to predict pass/fail for students who are in a control versus experimental group.  </p>

<p>Let's say that the ""intervention"" appears to affect women more strongly than men, because the interaction gender*intervention is significant.  But then adding in a number of co-variates (and their interactions with the intervention),  results in a decrease in the magnitude of the coefficient and the significance of the fenale*intervention interaction, suggesting that once we control for these co-variates and their interactions with the intervention, differences between how the intervention ""affects"" men and women are no longer significantly different.  </p>

<p>I would like to be able to say something about mediation, and I understand how to test the individual factors for mediation of the gender*intervention interaction, but what if there is another interaction, such as (hours spent on childcare)*intervention, which I think may mediate the gender*intervention interaction?  Is there a way to test whether the first interaction mediates the second one?</p>

<p>EDIT:</p>

<p>As requested, here is a simple example equation which I think explains what I want to do.  For the purposes of simplicity, I am specifying this as a simple binary logistic regression model instead of a multilevel binary logistic regression model.  </p>

<p>Let's suppose there are three IV being used as factors:
intervention = whether student was in control or experimental group
gender
GPA</p>

<p>And the DV is whether the student passed or failed.  </p>

<p>And let's suppose we consider the following models (I'm just listing the factors here, without coefficients or error terms, for ease of readability):</p>

<p>M1: OUTCOME = INTERVENTION + GENDER + INTERVENTION*GENDER</p>

<p>M2: OUTCOME = INTERVENTION + GENDER + GPA + INTERVENTION*GENDER + INTERVENTION*GPA</p>

<p>Suppose in M1 that INTERVENTION*GENDER was significant, so that women benefited significantly more than men from the intervention.  </p>

<p>Then supposed that INTERVENTION*GENDER was not significant (and the female*experimental coefficient had a smaller magnitude) in M2, and we suspect that this is because INTERVENTION*GPA mediates INTERVENTION*GENDER.   </p>

<p>What I would like to know is if there is a way to test whether or not INTERVENTION*GPA mediates INTERVENTION*GENDER for these two models....</p>
"
"0.0223495078133837","0.0220540545695615","107584","<p>I adjust the partial least squares regression for one categorical factor (2 levels â€“ <code>be</code> or <code>nottobe</code>) with with the <code>pls</code> package in R. I try to use <code>round()</code> function in the predict values for take the decision if the result are the first or second level in my factor. Does this approach sound correct?</p>

<pre><code>require(pls) 

#Artificial data  

T&lt;-as.factor(sort(rep(c(""be"", ""nottobe""), 100))) 

y1 &lt;- c(rnorm(100,1,0.1),rnorm(100,1,0.1)) 
y2 &lt;- c(rnorm(100,10,0.3),rnorm(100,10,0.6)) 
y3 &lt;- c(rnorm(100,10,2.3),rnorm(100,11,2.6)) 
y4 &lt;- c(rnorm(100,5,0.5),rnorm(100,7,0.5)) 
y5 &lt;- c(rnorm(100,0,0.1),rnorm(100,0,0.1)) 

#Create the data frame 
avaliacao &lt;- as.numeric(T) 
espectro &lt;- cbind(y1,y2,y3,y4,y5) 
dados &lt;- data.frame(avaliacao = I(as.matrix(avaliacao)), bands = I(as.matrix(espectro))) 

#PLS regression
taumato &lt;- plsr(avaliacao ~ bands, ncomp = 5, validation = ""LOO"", data=dados) 
summary(taumato) 

#Components analysis 
plot(taumato, plottype = ""scores"", comps = 1:5) 


#Cross validation 
taumato.cv &lt;- crossval(taumato, segments = 10) 
plot(MSEP(taumato.cv), legendpos = ""topright"") 
summary(taumato.cv, what = ""validation"") 
plot(taumato, xlab =""mediÃ§Ã£o"", ylab=""prediÃ§Ã£o"", ncomp = 3, asp = 1, main="" "", line = TRUE) 


#Predition for 3 components 
T&lt;-as.factor(sort(rep(c(""be"", ""nottobe""), 50))) 

y1 &lt;- c(rnorm(100,1,0.1),rnorm(100,1,0.1)) 
y2 &lt;- c(rnorm(100,10,0.3),rnorm(100,10,0.6)) 
y3 &lt;- c(rnorm(100,10,2.3),rnorm(100,11,2.6)) 
y4 &lt;- c(rnorm(100,5,0.5),rnorm(100,7,0.5)) 
y5 &lt;- c(rnorm(100,0,0.1),rnorm(100,0,0.1)) 

espectro2 &lt;- cbind(y1,y2,y3,y4,y5) 
new.dados &lt;- data.frame(bands = I(as.matrix(espectro2))) 
round(predict(taumato, ncomp = 3, newdata = new.dados))##
</code></pre>
"
"0.0836242010007091","0.0825187161885156","107865","<p>I'm investigating environmental effects (wind) on acoustic receiver detection probability for two types of transmitters using a binomial glmer. While my model analysis indicates that there's a significant effect between wind speed and transmitter type, graphical visualisation does not confirm this. If I'm correct, an interaction should demonstrate different regression slopes.</p>

<pre><code>m1 &lt;- glmer(cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth + 
               Receiver.depth + Water.temperature + Wind.speed + Transmitter + 
               Distance + Habitat + Replicate + (1 | Day) + (Distance | SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat + 
               Receiver.depth:Habitat + Wind.speed:Transmitter, data=df, family=binomial(link=logit))
</code></pre>

<p>The model summary is as follows:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth +  
    Receiver.depth + Water.temperature + Wind.speed + Transmitter +  
    Distance + Habitat + Replicate + (1 | Day) + (Distance |  
    SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat +      Receiver.depth:Habitat + Wind.speed:Transmitter
   Data: df

     AIC      BIC   logLik deviance df.resid 
  3941.9   4043.8  -1953.9   3907.9     2943 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-9.4911  0.0000  0.0000  0.5666  1.9143 

Random effects:
 Groups Name        Variance Std.Dev. Corr
 SUR.ID (Intercept)  0.33414 0.5781       
        Distance     0.09469 0.3077   1.00
 Day    (Intercept) 15.96629 3.9958       
Number of obs: 2960, groups:  SUR.ID, 20 Day, 6

Fixed effects:
                                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      3.20222    2.84984   1.124  0.26116    
Transmitter.depth               -0.35015    0.11794  -2.969  0.00299 ** 
Receiver.depth                  -0.57331    0.51919  -1.104  0.26949    
Water.temperature               -0.26595    0.11861  -2.242  0.02495 *  
Wind.speed                       1.31735    1.50457   0.876  0.38127    
TransmitterPT-04                -0.68854    0.08016  -8.590  &lt; 2e-16 ***
Distance                        -0.39547    0.09228  -4.286 1.82e-05 ***
HabitatFinger                   -0.23746    3.57783  -0.066  0.94708    
Replicate2                      -0.21559    0.08009  -2.692  0.00710 ** 
TransmitterPT-04:Distance       -0.27874    0.08426  -3.308  0.00094 ***
Transmitter.depth:HabitatFinger  0.73965    0.28612   2.585  0.00973 ** 
Receiver.depth:HabitatFinger     3.02083    0.74546   4.052 5.07e-05 ***
Wind.speed:TransmitterPT-04     -0.15540    0.06572  -2.364  0.01806 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Trnsm. Rcvr.d Wtr.tm Wnd.sp TrPT-04 Distnc HbttFn Rplct2 TPT-04: Tr.:HF Rc.:HF
Trnsmttr.dp -0.024                                                                               
Recevr.dpth -0.120 -0.267                                                                        
Watr.tmprtr  0.019 -0.159  0.007                                                                 
Wind.speed   0.130  0.073 -0.974  0.040                                                          
TrnsmtPT-04 -0.015  0.027  0.020 -0.018 -0.024                                                   
Distance     0.022 -0.080  0.151 -0.052 -0.141 -0.164                                            
HabitatFngr -0.813  0.010  0.241 -0.025 -0.253  0.009   0.029                                    
Replicate2  -0.067  0.033  0.377 -0.293 -0.394  0.010   0.085  0.103                             
TrnsPT-04:D -0.006  0.043 -0.007 -0.050 -0.003  0.516  -0.373  0.004  0.006                      
Trnsmtt.:HF  0.017 -0.352  0.021  0.055  0.049  0.026  -0.142  0.031 -0.088  0.025               
Rcvr.dpt:HF  0.103  0.189 -0.830  0.051  0.817 -0.036  -0.143 -0.224 -0.385 -0.003  -0.229       
Wnd.:TPT-04 -0.002  0.026 -0.015  0.003 -0.009  0.176  -0.114 -0.002  0.016  0.306  -0.008  0.014
</code></pre>

<p><img src=""http://i.stack.imgur.com/DFxyQ.png"" alt=""enter image description here""></p>

<p>A side question: I noticed a strong negative correlation between the intercept and a dichotome categorical predictor. I wonder if this causes any problems for my data analysis. All the covariates are centered and scaled for numerical stability during modelling.  </p>
"
"0.130520481086167","0.135950322810847","108315","<p>I am running multinomial logistic regression analysis on my data.  The response variable is the number of calves produced each year (0,1, or 2).  I am trying to evaluate the influence of the <em>X</em> variables on the odds of producing a calf.  My <em>X</em> variables are predation risk (WR; continuous), age of mother (age; categorical or continuous), time (wolf; categorical).</p>

<p>First, I have 4 different age classification schemes (i only show 2) - I want to know which one of the age of mother would be ""best"" to use. I could use it as continuous variable - or as categories based on biological reasoning for senescence in older moose (old ladies don't invest in reproduction as much).  So, I thought I would use a likelihood ratio test.</p>

<pre><code>library(mlogit)

modata.model1 &lt;- mlogit(no.C ~ 1 | 1, data=modata, reflevel=""1"", na.action = na.omit) 
modata.model2 &lt;- mlogit(no.C ~ 1 | age, data=modata, reflevel=""1"", na.action = na.omit) 
modata.model3 &lt;- mlogit(no.C ~ 1 | age2, data=modata, reflevel=""1"", na.action = na.omit)  
</code></pre>

<hr>

<pre><code> lrtest(modata.model2,modata.model3)

Likelihood ratio test

Model 1: no.C ~ 1 | age
Model 2: no.C ~ 1 | age2
  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
1   4 -213.22                         
2   4 -207.57  0 11.309  &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>QUESTION: to interpret this output - there was a significant difference in the loglikelihood when we comparing the continuous age to a categorical age with 2 classes.  The loglik is smaller for model 1 and therefore it would be better to use? Or do I have that backwards? </p>

<p>Next I was going to use the Walds test to evaluate nested models.  To see if the addition of a variable was worth it.</p>

<pre><code>modata.model7 &lt;- mlogit(no.C ~ 1 | age+WR, data=modata, reflevel=""1"", na.action = na.omit) 

modata.model8 &lt;- mlogit(no.C ~ 1 | age+WR+wolf, data=modata, reflevel=""1"", na.action = na.omit) 
</code></pre>

<hr>

<pre><code>Wald test

Model 1: no.C ~ 1 | age + WR
Model 2: no.C ~ 1 | age + WR + wolf
  Res.Df Df  Chisq Pr(&gt;Chisq)
1    244                     
2    242  2 0.5828     0.7472
</code></pre>

<p>QUESTION: this tells me that there is no significant improvement when there is an additional variable of wolf added??  So, then I can use the smaller model or do I use the one with the smaller Res.DF?</p>

<p>In addition to confirming my interpretations of the results I have 2 side questions...  </p>

<p>1)to get the null model for <code>mlogit</code> library - is my <code>modata.model1</code> correct?  I want the intercept only model to compare against.</p>

<p>2) Hosmer and Lemshow suggest by getting Wald values to get significance levels for each coefficient - in mlogit, thats the same as using <code>summary(model)</code> and there they provide the t-values with p instead of needing to do an additional Walds test? (NOTE in the below model i use 2 category age class instead of continuous)</p>

<pre><code>summary(modata.model8)

Call:
mlogit(formula = no.C ~ 1 | age2 + WR + wolf, data = modata, 
    na.action = na.omit, reflevel = ""1"", method = ""nr"", print.level = 0)

Frequencies of alternatives:
    1     0     2 
0.652 0.244 0.104 

nr method
6 iterations, 0h:0m:0s 
g'(-H)^-1g = 7.4E-06 
successive function values within tolerance limits 

Coefficients :
              Estimate Std. Error t-value Pr(&gt;|t|)   
0:(intercept)  0.38730    0.40144  0.9648 0.334657   
2:(intercept) -2.40317    1.04546 -2.2987 0.021523 * 
0:age21       -1.39607    0.43989 -3.1737 0.001505 **
2:age21        0.53952    1.07275  0.5029 0.615012   
0:WR          -1.46584    0.64797 -2.2622 0.023686 * 
2:WR          -0.19214    0.60856 -0.3157 0.752206   
0:wolf1        0.56055    0.63292  0.8857 0.375797   
2:wolf1        0.42642    0.72375  0.5892 0.555744   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -203.11
McFadden R^2:  0.053606 
Likelihood ratio test : chisq = 23.009 (p.value = 0.00079359)
</code></pre>
"
"0.113960576459638","0.112454054604034","108374","<p>I have a monthly time series with an intervention and I would like to quantify the effect of this intervention on the outcome. I realize the series is rather short and the effect is not yet concluded.</p>

<p><strong>The Data</strong></p>

<pre><code>  cds&lt;- structure(c(2580L, 2263L, 3679L, 3461L, 3645L, 3716L, 3955L, 
    3362L, 2637L, 2524L, 2084L, 2031L, 2256L, 2401L, 3253L, 2881L, 
    2555L, 2585L, 3015L, 2608L, 3676L, 5763L, 4626L, 3848L, 4523L, 
    4186L, 4070L, 4000L, 3498L), .Dim = c(29L, 1L), .Dimnames = list(
        NULL, ""CD""), .Tsp = c(2012, 2014.33333333333, 12), class = ""ts"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/lNOEk.jpg"" alt=""enter image description here""></p>

<p><strong>The methodology</strong></p>

<p>1) The pre-intervention series (up until October 2013) was used with the <code>auto.arima</code> function. The model suggested was ARIMA(1,0,0) with non-zero mean. The ACF plot looked good.</p>

<pre><code>pre&lt;-window(cds,start = c(2012,01), end=c(2013,09))

mod.pre&lt;-auto.arima(log(pre))

Coefficients:
         ar1  intercept
      0.5821     7.9652
s.e.  0.1763     0.0810

sigma^2 estimated as 0.02709:  log likelihood=7.89
AIC=-9.77   AICc=-8.36   BIC=-6.64
</code></pre>

<p>2) Given the plot of the full series, the pulse response was chosen below, with T = Oct 2013,</p>

<p><img src=""http://i.stack.imgur.com/YU3nB.jpg"" alt=""enter image description here""></p>

<p>which according to cryer and chan can be fit as follows with the arimax function:</p>

<pre><code>   mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
            xtransf=data.frame(Oct13=1*(seq(cds)==22)),
            transfer=list(c(1,1))
          )

    mod.arimax


Series: log(cds) 
ARIMA(1,0,0) with non-zero mean 

Coefficients:
         ar1  intercept  Oct13-AR1  Oct13-MA0  Oct13-MA1
      0.7619     8.0345    -0.4429     0.4261     0.3567
s.e.  0.1206     0.1090     0.3993     0.1340     0.1557

sigma^2 estimated as 0.02289:  log likelihood=12.71
AIC=-15.42   AICc=-11.61   BIC=-7.22
</code></pre>

<p>The residuals from this appeared OK:</p>

<p><img src=""http://i.stack.imgur.com/wvdXD.jpg"" alt=""enter image description here""></p>

<p>The plot of fitted and actuals:</p>

<pre><code>plot(fitted(mod.arimax),col=""red"", type=""b"")
lines(window(log(cds),start=c(2012,02)),type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/kJ1pj.jpg"" alt=""enter image description here""></p>

<p><strong>The Questions</strong></p>

<p>1) Is this methodology correct for intervention analysis?</p>

<p>2) Can I look at estimate/SE for the components of the transfer function and say that the effect of the intervention was significant?</p>

<p>3) How can one visualize the transfer function effect (plot it?)</p>

<p>4) Is there a way to estimate how much the intervention increased the output after 'x' months? I guess for this (and maybe #3) I am asking how to work with an equation of the model - if this were simple linear regression with dummy variables (for example) I could run scenarios with and without the intervention and measure the impact - but I am just unsure how to work this this type of model.</p>

<p><strong>ADD</strong></p>

<p>Per request, here are the residuals from the two parametrizations.</p>

<p>First from the fit:</p>

<pre><code>fit &lt;- arimax(log(cds), order = c(1,0,0), 
              xtransf = data.frame(Oct13a = 1*(seq_along(cds)==22), Oct13b = 1*(seq_along(cds)==22)),
              transfer = list(c(0,0), c(1,0)))

plot(resid(fit), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/sqMZN.jpg"" alt=""enter image description here""></p>

<p>Then, from this fit</p>

<pre><code>mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
                   xtransf=data.frame(Oct13=1*(seq(cds)==22)),
                   transfer=list(c(1,1))
)

mod.arimax
plot(resid(mod.arimax), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/DjAyu.jpg"" alt=""enter image description here""></p>
"
"0.0223495078133837","0.0441081091391231","108899","<p>Can anyone explain the theory (or the formula) about computing Sum Sq (bold highligh below) related to regression items?  The Wikipedia <a href=""http://en.wikipedia.org/wiki/Partition_of_sums_of_squares"" rel=""nofollow"">link</a> gives an introduction on how to calculate the total, model, and regression sum of squares. Is it similar to the Sum Sq computation? Is the regression sum of squares equal to (0.000437+ 0.002545+ 0.060984+ 0.062330+ 0.060480)?</p>

<pre><code>TraingData &lt;- data.frame(x1 = c(3.532,2.868,2.868,3.532,2.868,2.536,3.864),
                         x2 = c(1.992,1.992,1.328,1.328,1.328,1.66,1.66),
                         y  = c(9.040330254,8.900894412,8.701929163,9.057944749,
                                8.701929163,8.74317832,9.10859913)
                         )
lm.sol &lt;- lm(y~1+x1+x2+I(x1^2)+I(x2^2)+I(x1*x2), data=TraingData)
anova(lm.sol)

Analysis of Variance Table

Response: y
            Df   **Sum Sq**     Mean       Sq F    value Pr(&gt;F)
x1          1   0.000437  0.000437    0.1055    0.8001
x2          1   0.002545  0.002545    0.6141    0.5768
I(x1^2)     1   0.060984  0.060984   14.7162    0.1623
I(x2^2)     1   0.062330  0.062330   15.0409    0.1607
I(x1 * x2)  1   0.060480  0.060480   14.5945    0.1630
Residuals   1   0.004144  0.004144  
</code></pre>
"
"0.0774209661138764","0.0763974860547543","108908","<p>I'm trying to test the relationship between the number of adults counted and the percentage heather cover over 3 areas. </p>

<p>The data looks like this:</p>

<pre><code>Area      Cover    Count
Field1     53.7      216
Field2     19.5        2
Field3     39.8     6106
</code></pre>

<p>Given that count data is involved I was going to carry out a Poisson regression. </p>

<p>I know a relationship is unlikely given this data but my question is, is 3 data points enough for a regression analysis? This is important as I also want to test the relationship between adult counts and nest number, adult counts and vegetation height etc. but all of which only have 3 data points as I was only studying 3 areas on a site. </p>

<p>With this type of data, I don't know if any other test could be used given that it is count data. </p>

<p>I'm still a beginner when it comes to stats so any help would be much appreciated. </p>

<p>I'm using R to carry out all my analyses.</p>
"
"0.0952986215201744","0.0940388046555249","109222","<p>I am running an analysis where I have 2500 cases and 2500 controls. The cases have disease A, and the controls do not. I am trying to see if having disease A increases the odds of various diseases. For the sake of simplicity, we can focus on one disease, call it disease B.</p>

<p>D = 1 if disease B present, 0 otherwise</p>

<p>E = 1 if disease A present, 0 otherwise</p>

<p>I am also including in the model a measure of healthcare utilization. </p>

<p>F is a positive integer proportional to an individual's utilization of healthcare.</p>

<p>I am running the logistic regression model as such in R:</p>

<pre><code>glm(D ~ E + F, family = ""binomial"") 
</code></pre>

<p>Now, this works fine. </p>

<p>However, when I try to run conditional logistic regression, it gives me an error:</p>

<pre><code>library(survival)
clogit(D ~ E + F, strata(matched.pairs))
Error in fitter(X, Y, strats, offset, init, control, weights = weights,  :
  NA/NaN/Inf in foreign function call (arg 5)
In addition: Warning message:
In fitter(X, Y, strats, offset, init, control, weights = weights,  :
  Ran out of iterations and did not converge
</code></pre>

<p>I have tried different strata, including dividing the individuals into quantile bins based on F. It does not seem to change anything. (note: pairs are matched on age, gender, race, and F)</p>

<p>This occurs only when I run it on a larger sample size. I ran this same analysis on a sample size of 200 (100 cases and 100 controls) and it worked fine. When I use a sample size of 5000, I get the above error. </p>

<p>I also made sure that at least 10 cases and 10 controls had the disease in question (disease B, for this example). </p>

<p>I am not sure why logistic regression runs fine when conditional logistic regression does not. Can anyone offer me any advice?</p>

<p>Thank you all in advance.</p>
"
"0.0632139541241014","0.0623782861551805","109261","<p>Do you know of an approach/package that facilitates mixed model regression of ordinal dependent variables on multiply imputed datasets in R?</p>

<p>Ideally, the function takes:</p>

<p>a list of multiply imputed datasets</p>

<p>a list of target variables (dependent variables, one or more)</p>

<p>a list of factors (independent variables, one or more)</p>

<p>a list of dummy coded conditions (for analysis of multifactor IVs)</p>

<p>and returns a table similar to the result of other regressions in r</p>

<p>After extensive searching, I had to create such a function using CLMM in the ordinal package. If you can't answer the first question perhaps you can advise me wrt code adaptations, statistical appropriateness of my approach, efficiency (it takes a LONG time with many imputed datasets), etc</p>

<p>here's some data that mirrors mine</p>

<pre><code>numimpdatasets = 3; N = 170; datalist = list()
for (datasetnum in 1:numimpdatasets){
    dvone = sample(1:5, N, replace=T)   
    dvtwo = sample(1:5, N, replace=T)
    teacher = c('Tom','Dick','Harry')[sample(1:3, N, replace=T)]
    studentclass = c('Class1','Class2','Class3','Class4','Class5',
                     'Class6')[sample(1:6, N, replace=T)]
    aptitude = runif(N, -3.5, 3.5)
    randord = sample(1:3, N, replace=T)
    conddummycode1 = c('a_cond1', 'b_cond2', 'c_cond3')[randord]
    conddummycode2 = c('c_cond1', 'b_cond2', 'a_cond3')[randord]
    datalist[[datasetnum]] = data.frame(cbind(dvone, dvtwo,teacher,
            studentclass, aptitude, conddummycode1, conddummycode2))
    }
dvs = colnames(datalist[[1]])[1:2]
conditions = colnames(datalist[[1]])[6:7]
ivs = c(""+as.numeric(aptitude)"",""+(1|teacher/studentclass)"")
</code></pre>
"
"0.0446990156267674","0.0441081091391231","109274","<p>I hope the title is self-explanatory, but essentially I want to know which method is better: does it make sense to use a PCA to reduce a number of response Y variables and then conduct a univariate multiple regression OR conduct a multivariate multiple regression using all response variables as they are?</p>

<p>My variables are all continuous. I will be conducted the analysis in R. </p>

<p>Thanks in advance!</p>
"
"0.176304289544452","0.184517458813533","109464","<p>I am new to regression and having problem in solving Heteroscedasticity in OLS. Have done lots of homework and test before seeking your advice. Sharing the background and what I have done to solve the problem. Hope you can share your thoughts if my approach was correct.</p>

<p><strong>Objectives:</strong></p>

<ol>
<li>To find the relationship (model) between an explanatory variable (x) and an explained variable (y) using OLS regression.</li>
<li>if a model (relationship) is found, its usefulness and accuracy of prediction will be studied.</li>
</ol>

<p><strong>Dataset (Cross-sectional):</strong></p>

<ol>
<li>Have 4 datasets, with each 350 sample size.</li>
<li>Each dataset obtained using different intensity of experiment and this is already captured by the explanatory variable in x.</li>
<li>Due to the heterogenity of data, not possible to lump all into a single dataset.</li>
</ol>

<p><strong>Requirement:</strong></p>

<p>One common and statistically acceptable model for all the 4 datasets using OLS</p>

<p><strong>Steps Followed:</strong></p>

<ol>
<li><p>Explanatory Analysis: Found Non-linear relationship </p></li>
<li><p>As intending to use OLS, did 3 transformations of variables in attempt to have linearity:
a) ln(x) ~ ln(y);
b) ln(x) ~ y;
c) x ~ ln(y).
<strong>Note:</strong> Kept d) x ~ y as benchmark</p></li>
<li><p>Did heteroscedasticity test using Breusch-Pagan (BP) test in R for 2(a)-(d) for all the datasets in attempt to find valid model(s).
On the best case i.e 2b), only 2 out of 4 datasets passed the BP test (p-value>0.05)</p></li>
<li><p>As the aim is to have one common model for all the 4 datasets, another variable transformation is done using Tukey's Ladder of Transformation in attempt to have homoscedasticity:
a) ? ? {-2,-1,-0.5, 0.5, 1, 2} is used for x/y/x and y for each of the models in 2(a)-(d). Have total of 64 models (16 x 4) to consider. X and Y refer to the transformed x and y;
b) Now have 2 models passed BP test for 3 out of 4 datasets in the best case;
c) The one that failed has p-value &lt;2.20E-16.</p></li>
<li><p>[deadlock unable to find one valid model that passes all the 4 datasets]</p></li>
<li><p>Proceeded to take the two valid models in Step 4 and done inference Test:
a) the p-values for t-test and F-test are below 0.05 for all the 4 datasets;
b) R-square are above 0.9402 for all the 4 datasets.</p></li>
<li><p>Did cross validation and selected the best model using the smallest mean square error against the two ""valid"" models. Did back transformation on the original scale first before the selection is done so that its apple to apple data comparison. The mean average percentage error for the best model is below 10%</p></li>
<li><p>Now tried to use the best model for prediction:
a) Selected 20 random x values which were not part of the dataset;
b) Predicted y and compared it against Measured y;
c) the  mean average percentage error is below 8% and within the model's mean average percentage error i.e below 10%.</p></li>
</ol>

<p><strong>The problem:</strong></p>

<p>With the steps above I am unable to get a model that passes the heteroscedasticity test all the 4 datasets. Have I done anything incorrectly or is there anything more can be done in Step 4? </p>

<p>Believe mis-specification issue has duly been attended. Not intending to use GLS as I need to use .OLS</p>

<p>I have used heteroscedasticity robust standard errors as a remedy of heteroscedasticity on the one dataset that failed BP test per the Youtube below.
Refer - <a href=""https://www.youtube.com/watch?v=hFoDDwTF4KY"" rel=""nofollow"">https://www.youtube.com/watch?v=hFoDDwTF4KY</a></p>

<p>The standard error increased and t-value decreased for Y for the HC3 corrected dataset. 
But the Y= a  + b X model remain the same.</p>

<p>Is it sufficient to show the p-value for t-test and F-test for the corrected dataset are still below 0.05 hence its ok to use the same Y= a+bX though it failed the BP test earlier?</p>

<p>Hope you can share your thoughts as I am new to regression. </p>

<p>Using many reference books to learn such as </p>

<ol>
<li>Introduction to Econometrics by Wooldridge</li>
<li>Basic Econometrics by Gujerati</li>
<li>Regression Analysis by Example by Chatterjee</li>
</ol>

<p><strong>Original:</strong></p>

<pre><code>Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -0.612116   0.009006  -68.76   &lt;2e-16 ***
Y                     5.955984   0.039653  145.65   &lt;2e-16 ***
---

Residual standard error: 0.04138 on 348 degrees of freedom
Multiple R-squared:  0.9832,    Adjusted R-squared:  0.9831 
F-statistic: 2.092e+04 on 1 and 348 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Heteroskedasticity Robust Standard Errors corrected using HC3:</strong></p>

<pre><code>Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -0.61212    0.01767  -33.77   &lt;2e-16 ***
Y                     5.95598    0.08432   69.12   &lt;2e-16 ***
---

Residual standard error: 0.04138 on 348 degrees of freedom
Multiple R-squared:  0.9832,    Adjusted R-squared:  0.9831 
F-statistic:  4640 on 1 and 348 DF,  p-value: &lt; 2.2e-16

Note: Heteroscedasticity-consistent standard errors using adjustment hc3 
</code></pre>

<p>Thanks</p>
"
"0.141350698548044","0.132507983754772","110033","<p>I am running a post-hoc analysis on the data collected during an experiment in which 15 unique stimuli were presented to participants. Having run a least squares regression using the lm() function in R I have found significant results for a subset of the data including 90 observations from 6 participants with two continuous variables and their interaction.</p>

<p>Taking advice from an article by Judd, Westfall &amp; Kenny (2012) I attempted to use a combination of the lmer() function found in the lme4 package in combination with a Kenward-Roger approximation through the KRmodcomp() function in the pbkrtest package (see the appendix in the article) in order to control for random effects:</p>

<pre><code>lmer(Prediction_Difference_Scale~Diff_AWD_LRTI_End_Scale*Diff_AWD_BD_End_Scale + (1|Unique_ID) + (Diff_AWD_LRTI_End_Scale*Diff_AWD_BD_End_Scale|Block),data=Data)
</code></pre>

<p>The first variable after the DV is the fixed effect, the second variable in parentheses indicates that the intercept is random with respect the unique stimuli (Unique_ID) and the third variable in parentheses indicates that both the intercept and the Condition slopes are random with respect to participant (Block) and that a covariance between the effects should be estimated. </p>

<p>When running the lmer() function I get the following error message:</p>

<pre><code>Error in checkNlevels(reTrms$flist, n = n, control) : 
  number of levels of each grouping factor must be &lt; number of observations
</code></pre>

<p>This is obviously because the number of observations equal the number of unique stimuli.</p>

<p>The function works when excluding the (1|Unique_ID) random  effect, which if I understand correctly is the same as carrying out a 'by stimulus' analysis. However, the authors warn against this by stating: ""Conceptually, a significant by-participant result suggests that experimental results would be likely to replicate for a new set of participants, but only using the same sample of stimuli. A significant by-stimulus result, on the other hand, suggests that experimental results would be likely to replicate for a new set of stimuli, but only using the same sample of participants. However, it is a fallacy to assume that the conjunction of these two results implies that a result would be likely to replicate with simultaneously new samples of both participants and stimuli.""</p>

<p>I would like to control for the random effects of both stimuli and participants, but I am unsure how to proceed?</p>

<p>The article can be accessed here: <a href=""http://jakewestfall.org/publications/JWK.pdf"" rel=""nofollow"">http://jakewestfall.org/publications/JWK.pdf</a></p>

<hr>

<p>To clarify the question regarding the 15 unique stimuli, this is 15 unique stimuli per participant, meaning the sample of 90 observations consists of 6 participants. The stimuli for all of the 90 observations are unique however.</p>

<p>I suppose what my question boils down to is whether there is even a need to include the (1|Unique_ID) 'variable' in the function formula as there is no error dependence between any of the stimuli?</p>
"
"0.0565402794192176","0.0697410440814588","110469","<p>So the background is that the I collected yield data for past 5-6 decades and location from where I collected yield data had high yielding varieties introduced over time. I am looking at the relationship between yield and rainfall but this introduction of HYV might affect the true impact of monsoon on yield and therefore I am detrending the data to remove the effect of HYV.</p>

<p>I did a linear regression of yield against time in R:</p>

<pre><code>mdl1 &lt;- lm(yield ~ time, data=data)
</code></pre>

<p>and then removed the linear trend by taking the residuals of the above regression: </p>

<pre><code>yield.res &lt;- resid(mdl1)
</code></pre>

<p>Now I am using these residuals for my subsequent analysis. For example, the relationship between yield and rainfall is: </p>

<pre><code> mdl2 &lt;- lm(yield.res ~ rain, data=data)
</code></pre>

<p>In this case, do my <code>yield.res</code> have to be normally distributed before I do this regression? If yes, what sort of transformation do I need to use? Since <code>yield.res</code> consists of both negative and positive numbers, I am slightly confused how to go about it.</p>
"
"0.0547448901451359","0.0540211804549215","110569","<p>I have a data set based on 14 field sites, the dependent variable I am investigating is count data recorded at each site (on 3-5 visits).  The independent variables (I have around 45) are fixed at each site (apart from wind speed and temperature).  An example data set, illustrating this is below:</p>

<pre><code>            Trees    water   butterflycount 
Site 1      6        8       3

Site 1      6        8       12

Site 2      3        3       8

Site 2      3        3       0
</code></pre>

<p>What is the best way of preparing this kind of  data for a multiple regression analysis in R?  Would data1.glm = glm(butterflycount ~ 1, data = data1, family = poisson) and the working through adding the lowest AIC valued variables to the equation work?  Any help would be greatly appreciated.</p>
"
"NaN","NaN","110570","<p>For my survey data analysis, I ran an Ordinal Logistic regression using the 'polr' function.
The summary of the regression is as follows:</p>

<p><img src=""http://i.stack.imgur.com/csKGq.png"" alt=""enter image description here""></p>

<p>My question is:</p>

<ol>
<li>Do I need to standardize my  beta values?</li>
<li>If so, is lm.beta the right approach (as per my understanding, it only works for linear models)? And if not, could you please provide a method to do so.</li>
</ol>

<p>Thanks everyone!</p>
"
"0.0547448901451359","0.0540211804549215","110932","<p>I am working on some <strong>non-parametric bayesian based predictive analysis</strong> using <strong>R</strong>. I have a set of data which denotes various parameters of an online transaction. Based on these parameters I want to develop a model which will provide predictions for future online transactions.</p>

<p>The training data consist of records in this format:</p>

<pre><code>transaction_id (numeric)| duration (integer)| amount | is_holiday (boolean) | status(1 or 0)
                        |                   |        |                      |
                        |                   |        |                      | 
</code></pre>

<p>The problem that I am facing is that I do not know how to proceed ahead. I am do know know what are the steps that I need to follow. I looked up and found that there are few packages in R like <code>DPpackage</code> which have some functions for non-parametric bayesian modeling but there is no concrete example available about how to use it in order to perform various steps of training and testing.</p>

<p>It would be helpful for me if someone could provide me some guidance as in which process will be better for such kind of predictive/regression analysis and how to proceed ahead, like what steps should I perform to get the training and testing done.</p>

<p>Thanks in advance!  </p>
"
"0.0316069770620507","0.0311891430775903","111308","<p>I'm conducting a power analysis to derive the required sample size for a study - basically compared exposed / non-exposed with 30-day mortality as outcome. I'll check for crude mortality rates with chi-square, but also use logistic regression with probable confounders.</p>

<p>When I run a power analysis - power 0.8, significance level 0.05, effect size 0.15 and estimated 10 confounders I get that I'd need only n=117 which seem quite small.
comparing with chi-square - it suggest that I'd need 350.</p>

<p>I'm using R and <code>pwr</code>:</p>

<pre><code>pwr.f2.test(u=10, v=NULL, f2=0.15, sig.level=0.05, power=0.8)
pwr.chisq.test(w=0.15, N=NULL, df=1 , sig.level=0.05, power=0.8 )
</code></pre>

<p>Is this predictable or am I misusing this?</p>
"
"0.0547448901451359","0.036014120303281","111424","<p>I have one dependent variable name as ""win ration"" of the deal contested and more than 30 independent variables, all are categorical variable name as role of the customer, geo, region, and 27 competencies marks between 1 to 12 for each competency (this is like the performance parameter ""1"" is high and ""12"" is low rating) name as comp1,comp2,....,comp27 </p>

<p>My question is that which is the best model or predictor model to filter out which all competencies and other variables are really affection the win rate.</p>

<p>for this i used beta regression but non of the competencies are coming out significant and when i perform the step wise for dim reduction, this method is not working on beta reg. 
In this case all competencies are treated as quantitative variable, the reason is that all are bound between 1 to 12 and if i will categorize this all 27 competencies will have 12 categories </p>

<p>Please help me to do this analysis      </p>
"
"0.0446990156267674","0.0441081091391231","111541","<p>Could anyone provide me the details of how to determine the lag order of the  distributed lags for an $\text{ADL}(p,q)$ model in Matlab or another statistical package (and very much preferably in combination with the autoregression lags)? </p>

<p>Full working examples with model selection criteria ($\text{AIC}$ and $\text{BIC}$) seem to be available on the Matlab website for $\text{VAR}$ models, $\text{ARMA}$ models etc. but I can't find one for the $\text{ADL}(p,q)$ model. I would not have a clue how to rewrite those models to $\text{ADL}(p,q)$ myself but I have a vague feeling that such a thing would be possible.</p>

<p>In the end I want to automate this analysis by first checking the lag orders $p$, and $q$ and then using these numbers automatically to create the regressions out of this. So basically I'm looking for a fully working example. (I want to skip part of adding and deleting regressors by hand as much as possible to get a quick idea of the distributed lags of several assets).</p>
"
"0.0948209311861521","0.0935674292327708","111549","<p>I am trying to fit a NB GLMM with a gemoetric distribution. I have come across very little information on this form of regression. And would like some pointers/reasurance.</p>

<p>some literature is available for glm method here: <a href=""http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf</a></p>

<p>but I cannot find anything on a mixed model to use with this.</p>

<p>main points:</p>

<p>Is this code correct to run such a model?
Is this the best package for this analysis?
Does model selection and validation follow that for regular Negative binomial models?</p>

<p>sample data</p>

<pre><code>DF&lt;- structure(list(code = structure(c(1L, 1L, 6L, 6L, 7L, 9L, 10L, 
10L, 10L, 10L, 10L, 11L, 11L, 12L, 13L, 14L, 14L, 16L, 16L, 17L, 
17L, 23L, 24L, 26L, 27L, 27L, 27L, 28L, 28L, 29L, 30L, 30L, 31L, 
32L, 34L, 35L, 8L, 8L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 
25L, 25L, 25L, 15L, 33L, 33L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 
19L, 19L, 18L, 5L, 5L, 4L, 4L, 22L, 21L, 21L, 20L, 20L), .Label = c(""15010212"", 
""15010213"", ""15010215"", ""15010216"", ""15010220"", ""15010222"", ""15010245"", 
""15010269"", ""15010274"", ""15010284"", ""15010285"", ""15010287"", ""15010290"", 
""15010291"", ""15010292"", ""15010294"", ""15010299"", ""15020313"", ""15020314"", 
""15020315"", ""15020316"", ""15020317"", ""15020326"", ""15020345"", ""15020348"", 
""15020384"", ""15020395"", ""15020396"", ""15020397"", ""15030312"", ""15030317"", 
""15030349"", ""15030392"", ""15030394"", ""15030395""), class = ""factor""), 
flow = c(15.97766667, 14.226, 17.15724762, 14.7465, 39.579, 
23.355, 110.2926923, 71.95709524, 50.283, 66.66754955, 38.9218, 
72.73666667, 32.37466667, 50.34905172, 27.98471429, 49.244, 
109.1759778, 77.71733333, 37.446875, 101.23875, 67.78534615, 
21.359, 36.54257143, 34.13961111, 64.35253333, 80.98554545, 
68.0554, 61.50857143, 48.983, 63.81072727, 26.105, 46.783, 
23.0605, 33.61557143, 46.31042857, 62.37061905, 12.565, 42.31983721, 
15.3982, 14.49625, 16.40853846, 17.84350847, 14.625375, 13.10714286, 
13.35466667, 12.94033333, 13.54236364, 14.10023711, 12.5747807, 
23.77425, 25.626, 15.23888523, 74.62485714, 170.1547778, 
91.292, 71.422, 42.50887568, 53.89983761, 141.7211667, 50.67125, 
48.098, 66.83644444, 76.564875, 80.63189189, 136.0573243, 
136.3484, 86.68688889, 34.82169565, 70.00415385, 64.67233333, 
81.72766667, 57.74522034), success = c(0L, 1L, 0L, 1L, 1L, 
1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 
1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 
1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 
0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 
0L, 1L, 1L, 0L, 1L, 0L, 1L), length = c(595, 595, 582, 582, 
565, 537, 585, 585, 585, 585, 585, 595, 595, 607, 625, 627, 
627, 607, 607, 644, 644, 620, 560, 567, 615, 615, 615, 595, 
595, 546, 580, 580, 594, 605, 610, 640, 575, 575, 632, 632, 
632, 632, 632, 632, 632, 632, 632, 632, 632, 525, 585, 585, 
624, 624, 624, 624, 624, 624, 624, 608, 635, 635, 655, 670, 
670, 584, 584, 707, 680, 680, 740, 740), attempt = structure(c(1L, 
2L, 1L, 2L, 1L, 1L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 1L, 1L, 1L, 
2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 3L, 1L, 2L, 1L, 1L, 
2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 
9L, 10L, 11L, 1L, 1L, 2L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 
1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L), .Label = c(""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11""), class = ""factor"")), .Names =         c(""code"", 
""flow"", ""success"", ""length"", ""attempt""), row.names = c(NA, -72L
), class = ""data.frame"")
</code></pre>

<p>model is as follows. setting theta = 1 should determine a geometric distribution.</p>

<pre><code>library(MASS)
M1&lt;-glmmPQL(success ~ length + flow + attempt,
          random = ~ 1|code,
          family = negative.binomial(theta = 1),
          data = DF)

summary(M1)
</code></pre>

<p>Ultimately I am trying predict success (0 = fail, 1 = success). However this is measured for many different individuals (code), essentially a repeated measure and hence should be included as a random effect. each individual may only have one success but can have multiple attempts. Predictors of success come in the form of ""length"" of the individual, ""attempt"" number... a factor of the number of the attempt, and ""flow"" which is river flow at time of the attempt and so a continuous variable.</p>

<p>Thanks in advance</p>
"
"0.0316069770620507","0.0311891430775903","111841","<p>I ran an ordinal logistic regression in R using the polr function on a survey analysis dataset. The responses of the dependent variable range from Poor to Excellent.
The responses to the independent variables range from 1 to 5 (1 being Poor and 5 being Excellent). I obtained the following result:</p>

<p><img src=""http://i.stack.imgur.com/1ZBij.png"" alt=""enter image description here""></p>

<p>I want to measure the individual percentage contribution of my independents (R1, R2,...,R17) to the dependent variable. </p>

<p>Is there a way to do this.</p>

<p>Thanks for any help.</p>
"
"0.0836242010007091","0.0825187161885156","112247","<p>I'm trying to use the Match() function from the Matching package in R to do a propensity score analysis.</p>

<p>My outcome of interest is a binary variable (0/1).  My treatment is also a binary variable (0/1).  In addition, I have a number of other variables that I want to control for in this analysis.</p>

<p>First, I fit a logistic regression to define a propensity score for the treatment:</p>

<pre><code>glm1 = glm(Treatment ~ variable1 + variable2 + variable3 + ..., 
           data=dataset, family=""binomial"")
</code></pre>

<p>Then, I used the Match function to estimate the average treatment effect on the treated:</p>

<pre><code>rr1 = Match(Y = Outcome, Tr = Treatment, X = glm1$fitted)
</code></pre>

<p>Finally, I called for a summary:</p>

<pre><code>summary(rr1)
</code></pre>

<p>My question is how to interpret the output.  I get:</p>

<pre><code>Estimate... -0.349,
AI SE... 0.124,
T-stat... -2.827,
p.val... 0.005
</code></pre>

<p>What does this mean?  In particular, what is Estimate?  The documentation says it's ""The estimated average causal effect.""  But what are the units?  Can I interpret this to mean that the treatment reduced the outcome by a relative 35%?  Or by an absolute 0.35?  Or do I need to exponentiate?</p>

<p>Any help on the interpretation would be much appreciated!</p>
"
"0.122413295785282","0.120795031721987","112646","<p>I am working on a project whose aim is to analyze the relationship between machine elements and their price. </p>

<p>My data consists of thousands of machine elements, their price, as well as technical and non-technical attributes. Most of the data is categorical</p>

<p>The technical data contains information such as weight, dimensions, material. Each element (identified by unique code) has distinct technical attributes. </p>

<p>The non-technical data contain information such as supplier name, contract type, INCOTERMS, first day of validity of the contract, last day of validity, minimum order quantity, etc. In some cases the prices depend on the order quantity.</p>

<p>One of the problems I am facing is that often each element has a different price depending on the financial data, i.e. element X will cost 100 dollars if Incoterms is A, 200 dollars if Incoterms is B, etc. In other words, there are rows that contain price information on the same element, but one of the columns has a different value and so the price is different.</p>

<p>In other cases the price is 100 if 50-100 elements are ordered, 80 if 100-200 elements are ordered, 60 if 200-500, etc. </p>

<p>I am planning to do some correlations as well as regression. I will probably also try data mining (using Rattle and R). </p>

<p>I need advice on how to treat the (rather) similar observations that each object has in case of correlation, regression and in general for data mining. Should I try to select a single observation per element or do the analysis for â€œall inâ€. I guess the last option wonâ€™t work for correlation at all. </p>

<p>Perhaps including a sample screenshot of the data with clarify my problem. So what is visible in the picture happens quite often: one object, different attributes, different price. I am wondering whether I should (try to) have only one row per object with only one set of attributes (we donâ€™t need to discuss the criteria) or whether I should keep the multiple rows for the same object when running a correlation analysis or a regression.</p>

<p>Thank you in advance. </p>

<p><img src=""http://i.stack.imgur.com/DEOqg.png"" alt=""enter image description here""></p>
"
"0.0893980312535348","0.0882162182782462","112801","<p>I am seemingly blindly following this <a href=""http://www.cfc.umt.edu/grizzlybearrecovery/pdfs/Schwartz%20et%20al.%202006e.pdf"" rel=""nofollow"">publication</a> that has done work very similar to what I need to accomplish (page 18-21).  My analysis is a multinomial logistic regression where I have 3 possible outcomes 0, 1, or 2 offspring produced.  In the publication, they have recommended a Hosmer-Lemshow and a Persons test for goodness-of-fit.  I have only figured out how to do the Hosmer-Lemshow test and my results are not so good (i.e. P-val is 0.00002).  I have no idea how to do the Pearsons test (suggestions are appreciated).</p>

<p>The paper I am following, of course their tests are ""good"" for model fit (page 21).  But they then go onto suggest that Somers D, the Goodman-Kruskala gamma and the Kendall's tau-a all indicate that their models are a good fit.  But the paper does not report any of the values for these indices or how they calculated them. </p>

<p>I have just found a package <code>ryouready</code> that runs all of these tests.  However, I have been having difficulties finding any help explaining what the values mean, let alone knowing if I have input my variables correctly.</p>

<p>My response variable is number of offspring, most of my explanatory variables are continuous like age or risk.  Do I need to calculate the mean of each explanatory variable within each response variable (get the mean risk for 1 offspring, mean risk for 2 offspring etc...)and then compare those? It also seems that these tests are for 2x2 tables.  If I am just looking at risk, my table would be a 1x3.  However, my complete model will have 4 variables (age, risk, bp, and #offspring year before).  </p>

<p>As you can likely tell, I am in the dark here on where to start. I would appreciate suggested readings, pdf lectures or videos of lectures would even be better!    </p>

<p>EDIT/UPDATE:
I have run the tests over my counts - I have 2 time periods (before/after) and then the count of offspring in each class (0,1,2).  I do not know how to interpret the values - what is ""good"".  What should I be looking for?  Any source that explains these values would be nice to see.  </p>

<pre><code>Kendall's (and Stuart's) Tau statistics
    Tau-b: 0.143
    Tau-c: 0.130
Somers' d:
    Columns dependent: 0.151 
    Rows dependent: 0.136 
    Symmetric: 0.143 
Goodman-Kruskal Gamma: 0.312 
Warning message:
In formatC(x, digits, format = ""f"") : class of 'x' was discarded
</code></pre>
"
"0.0446990156267674","0.0441081091391231","112858","<p>I am currently trying to fit a survival analysis model which has the following survival function:</p>

<p>$S(t) = \lambda_i e^{-\lambda_i t}$</p>

<p>but with </p>

<p>$\lambda_i = e^{\beta_0 +\beta_1 log(1+X_i)}$</p>

<p>where $X_i$ represents each unique observation. </p>

<p>I am trying to use the default survival package but am not sure where I can program in the regression formula for the hazard rate. I believe I might be misunderstanding what the survival analysis package does but would anyone be able to tell me if this can be even done in the R package? Thanks!</p>
"
"0.0632139541241014","0.0623782861551805","112889","<p>I'm trying to use rjags to predict what the curve-maximums will be for different groups of incomplete data (one metric x and one metric y). Here is an example of the indata:</p>

<p><img src=""http://i.stack.imgur.com/315aR.png"" alt=""enter image description here""></p>

<pre><code>in.data &lt;- structure(list(x = c(10.4055555555556, 16.1694444444444, 25.25, 
                                33.3305555555556, 37.4736111111111, 31.2805555555556, 24.3347222222222, 
                                24.1638888888889, 27.8222222222222, 34.0055555555556, 31.6611111111111, 
                                33.8666666666667, 26.6486111111111, 25.3986111111111, 24.8611111111111, 
                                15.2125, 6.33194444444444, 11.7541666666667, 15.6694444444444, 
                                27.9430555555556, 34.1180555555556, 36.5092592592593, 34.4763888888889, 
                                32.8055555555556, 26.9041666666667, 27.9319444444444, 28.7319444444444, 
                                30.3638888888889, 31.3861111111111, 26.4722222222222, 27.0152777777778, 
                                26.6333333333333, 17.3902777777778, 8.09861111111111, 12.9305555555556, 
                                21.8277777777778, 30.8652777777778, 36.9486111111111, 33.5722222222222, 
                                29.1652777777778, 25.6444444444444, 28.7222222222222, 32.9319444444444, 
                                32.8375, 31.9402777777778, 29.4569444444444, 26.9013888888889, 
                                25.7152777777778, 24.9638888888889, 14.3722222222222, 7.36527777777778, 
                                13.4722222222222, 20.7263888888889, 31.6388888888889, 41.6555555555556, 
                                49.0513888888889, 39.1333333333333, 33.5847222222222, 28.2722222222222, 
                                28.2583333333333, 31.2361111111111, 33.1569444444444, 29.9166666666667, 
                                27.925, 26.9680555555556, 24.7611111111111, 18.8319444444444, 
                                6.33055555555556, 9.87083333333333, 14.5541666666667, 18.5041666666667, 
                                23.0652777777778, 21.8875, 20.7291666666667, 14.7375, 11.4083333333333, 
                                12.7930555555556, 12.8013888888889, 16.8277777777778, 14.8263888888889, 
                                13.3222222222222, 9.74305555555556, 6.42638888888889, 6.275, 
                                9.76666666666667, 9.98472222222222, 8.5, 11.4958333333333, 10.9777777777778, 
                                13.3083333333333, 8.69722222222222, 6.24305555555556, 5.11111111111111, 
                                6.41527777777778, 8.04583333333333, 9.46111111111111, 11.0333333333333, 
                                7.3, 9.42916666666667, 8.86805555555556, 9.84305555555556, 10.8555555555556, 
                                7.32916666666667, 5.77361111111111, 7.52083333333333, 9.99305555555556, 
                                10.9875, 10.0888888888889, 11.1, 13.8277777777778, 15.3458333333333, 
                                9.74166666666667, 6.59305555555556, 7.28611111111111, 5.95833333333333, 
                                9.20972222222222, 8.85138888888889, 11.6277777777778, 12.9680555555556, 
                                12.8541666666667, 14.1041666666667, 11.1236111111111, 6.77777777777778, 
                                6.48055555555556, 9.77361111111111, 8.14444444444444, 7.92222222222222, 
                                7.85555555555556, 6.63611111111111, 5.40416666666667, 0, 1, 2, 
                                3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 
                                20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 
                                36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 
                                52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 
                                68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 
                                84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 
                                100), y = c(1.36111111111111, 2.08805555555556, 2.83666666666667, 
                                            3.17583333333333, 3.53194444444444, 1.4125, 2.44166666666667, 
                                            2.45777777777778, 2.78527777777778, 3.17611111111111, 3.02138888888889, 
                                            2.60861111111111, 2.88805555555556, 2.79527777777778, 2.575, 
                                            1.77333333333333, 0.937777777777778, 1.57027777777778, 1.69888888888889, 
                                            2.74, 3.32888888888889, 2.6975, 3.42444444444444, 3.40555555555556, 
                                            2.77277777777778, 2.6425, 2.96166666666667, 3.25472222222222, 
                                            3.23777777777778, 2.70472222222222, 2.76805555555556, 2.39694444444444, 
                                            1.97083333333333, 1.00722222222222, 1.58055555555556, 2.52138888888889, 
                                            3.16083333333333, 3.30555555555556, 3.61805555555556, 3.11472222222222, 
                                            2.7825, 3.11416666666667, 3.25944444444444, 3.12666666666667, 
                                            3.50555555555556, 3.07972222222222, 2.92666666666667, 2.45361111111111, 
                                            2.33055555555556, 1.61555555555556, 0.814444444444444, 1.41722222222222, 
                                            2.30222222222222, 3.27166666666667, 3.70638888888889, 5.32527777777778, 
                                            3.8375, 3.42, 2.985, 2.99194444444444, 3.355, 3.36472222222222, 
                                            3.07388888888889, 3.095, 3.00222222222222, 2.61583333333333, 
                                            2.38277777777778, 0.776111111111111, 1.15611111111111, 1.53361111111111, 
                                            2.16611111111111, 2.62861111111111, 2.60777777777778, 2.46111111111111, 
                                            1.91111111111111, 1.35083333333333, 1.50833333333333, 1.50722222222222, 
                                            1.63555555555556, 1.61527777777778, 1.53333333333333, 1.09055555555556, 
                                            0.396666666666667, 0.401666666666667, 0.547777777777778, 0.594722222222222, 
                                            0.356388888888889, 0.790277777777778, 0.510555555555556, 0.823611111111111, 
                                            0.563611111111111, 0.260277777777778, 0.146944444444444, 0.363333333333333, 
                                            0.369444444444444, 0.585, 0.740833333333333, 0.320833333333333, 
                                            0.765277777777778, 0.698611111111111, 0.6625, 0.764722222222222, 
                                            0.611388888888889, 0.276388888888889, 0.518611111111111, 0.735, 
                                            0.779722222222222, 0.732222222222222, 0.882222222222222, 1.02444444444444, 
                                            1.06638888888889, 0.730555555555556, 0.469444444444444, 0.565277777777778, 
                                            0.573333333333333, 0.720277777777778, 0.780277777777778, 0.898888888888889, 
                                            1.02305555555556, 0.910277777777778, 1.01194444444444, 0.884722222222222, 
                                            0.541388888888889, 0.469166666666667, 0.709166666666667, 0.599166666666667, 
                                            0.502777777777778, 0.523333333333333, 0.547222222222222, 0.243333333333333, 
                                            0.0156494522691706, 0.264475743348983, 0.510172143974961, 0.752738654147105, 
                                            0.992175273865415, 1.22848200312989, 1.46165884194053, 1.69170579029734, 
                                            1.91862284820031, 2.14241001564945, 2.36306729264476, 2.58059467918623, 
                                            2.79499217527387, 3.00625978090767, 3.21439749608764, 3.41940532081377, 
                                            3.62128325508607, 3.82003129890454, 4.01564945226917, 4.20813771517997, 
                                            4.39749608763693, 4.58372456964006, 4.76682316118936, 4.94679186228482, 
                                            5.12363067292645, 5.29733959311424, 5.4679186228482, 5.63536776212833, 
                                            5.79968701095462, 5.96087636932707, 6.1189358372457, 6.27386541471049, 
                                            6.42566510172144, 6.57433489827856, 6.71987480438185, 6.8622848200313, 
                                            7.00156494522692, 7.1377151799687, 7.27073552425665, 7.40062597809077, 
                                            7.52738654147105, 7.6510172143975, 7.77151799687011, 7.88888888888889, 
                                            8.00312989045383, 8.11424100156495, 8.22222222222222, 8.32707355242566, 
                                            8.42879499217527, 8.52738654147105, 8.62284820031299, 8.7151799687011, 
                                            8.80438184663537, 8.89045383411581, 8.97339593114241, 9.05320813771518, 
                                            9.12989045383412, 9.20344287949922, 9.27386541471049, 9.34115805946792, 
                                            9.40532081377152, 9.46635367762128, 9.52425665101721, 9.57902973395931, 
                                            9.63067292644758, 9.679186228482, 9.7245696400626, 9.76682316118936, 
                                            9.80594679186228, 9.84194053208138, 9.87480438184664, 9.90453834115806, 
                                            9.93114241001565, 9.95461658841941, 9.97496087636933, 9.99217527386542, 
                                            10.0062597809077, 10.0172143974961, 10.0250391236307, 10.0297339593114, 
                                            10.0312989045383, 10.0297339593114, 10.0250391236307, 10.0172143974961, 
                                            10.0062597809077, 9.99217527386542, 9.97496087636933, 9.95461658841941, 
                                            9.93114241001565, 9.90453834115806, 9.87480438184664, 9.84194053208138, 
                                            9.80594679186228, 9.76682316118936, 9.7245696400626, 9.679186228482, 
                                            9.63067292644758, 9.57902973395931, 9.52425665101721, 9.46635367762128, 
                                            9.40532081377152), Group_ID = c(""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"")), .Names = c(""x"", ""y"", ""Group_ID""
                                                                            ), row.names = c(NA, -231L), class = ""data.frame"")

library(ggplot2)
p &lt;- ggplot(in.data, aes(x=x, y=y, group=Group_ID, color=Group_ID))
p &lt;- p + geom_point()
p
</code></pre>

<p>Group_1 and Group_2 will follow the same parabolic shape as the prior but, as can be seen in the graph, will have lower maximums when x increase.</p>

<p>Can Bayesian Regression be used for this? If so, how could the model for rjags look? The closest I think I've gotten is this, but can't get it to work:</p>

<pre><code>N[i] ~ dpois(z[i])
log(z[i]) &lt; beta0 + beta1 * x[i] + beta2 * x[i]^2
</code></pre>

<p>I've studied the truly great book by Kruschke (Doing Bayesian Data Analysis) where chapter 16 explains many steps, but when I want to expand from linear y = b1 + b2 * x to a more advanced curve-shape I get stuck.</p>
"
"0.0774209661138764","0.0763974860547543","113252","<p>I am so sorry, I am beginner in statistic analysis, I have project using R to analyze the correlation between dependent variables and independents variables. </p>

<p>In this case I have two dependent variables (1. Extrovert, 2. Introvert). 
And the independent variables i have the data from (Call Log-> how long they call everyday, how many they call everyday, SMS log-> how length text in SMS body every day, how many they sent/received sms for each day).</p>

<p>I am so confused how I can do it, please anyone can give me some good references about it. 
I also have some questions such as : </p>

<ol>
<li>I use the different type of variables, independent variables (data type : numeric) but dependent variable (data type is categorical), so it is possible to apply logistic regression and Pearson? </li>
<li>Or any someone will give me some advice the better solution such as another methods for solving this problem. </li>
</ol>

<p>The example of data from dput()</p>

<pre><code>structure(list(sumcallin = c(462L, 998L, 335L, 179L, 34L, 0L, 
0L, 0L, 0L, 0L), caountcallin = c(7L, 5L, 8L, 5L, 1L, 1L, 0L, 
1L, 1L, 1L), sumcallout = c(1068L, 81L, 519L, 393L, 342L, 0L, 
583L, 1902L, 358L, 1017L), countcallout = c(15L, 3L, 10L, 5L, 
6L, 0L, 3L, 3L, 3L, 3L), sumreceived = c(322L, 75L, 20L, 35L, 
8L, 35L, 135L, 103L, 471L, 173L), countreceived = c(15L, 4L, 
2L, 3L, 1L, 2L, 7L, 3L, 18L, 5L), sumsent = c(171L, 31L, 25L, 
23L, 8L, 55L, 87L, 9L, 400L, 258L), countsent = c(10L, 4L, 1L, 
3L, 1L, 3L, 4L, 1L, 13L, 8L), personality = structure(c(2L, 2L, 
2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L), .Label = c(""extro"", ""intro""), class = ""factor"")), .Names = c(""sumcallin"", 
""caountcallin"", ""sumcallout"", ""countcallout"", ""sumreceived"", 
""countreceived"", ""sumsent"", ""countsent"", ""personality""), row.names = c(1L, 
2L, 3L, 4L, 5L, 37L, 38L, 39L, 40L, 41L), class = ""data.frame"")
</code></pre>

<p>Thank you for your help.</p>
"
"0.118262479197817","0.108363438470314","114728","<p>I'm trying to run a QAP logistic regression to predict the odds of a tie in a social network (represented as a binary adjacency matrix) given two independent variables (also binary matrices) but am getting opposite results depending on whether I run the analysis in R or UCINET.</p>

<p>All three matrices are rectangular (30 x 75). The 30 rows are people I've interviewed and the 75 columns are the entire population (including the 30 interviewees). All matrices include the person IDs as row and column names.</p>

<p>Running the analysis in R (see code at the bottom of the question), I get the following output:</p>

<pre><code>            Estimate  Exp(b)       Pr(&lt;=b) Pr(&gt;=b) Pr(&gt;=|b|)
(intercept) -5.298525  0.004998961 0.0001  0.9999  0.0001   
indep1       1.797138  6.032358591 0.9693  0.0307  0.2393   
indep2       3.194162 24.389724184 1.0000  0.0000  0.0030   
</code></pre>

<p>But after exporting the variables to .csv files and re-running it in UCINET, I get:</p>

<pre><code>                  1       2       3       4       5       6       7       8       9 
               Coef OddsRat     Sig      SD     Avg     Min     Max   P(ge)   P(le) 
            ------- ------- ------- ------- ------- ------- ------- ------- ------- 
1 Intercept  -3.931   0.020   0.000   0.540  -2.785  -3.931  -1.783       1   0.000 
2    indep1   2.391  10.925   0.003   1.508  -0.239  -5.089  15.437   0.003   0.998 
3    indep2   1.458   4.296   0.000   0.504  -0.026 -15.991   1.458   0.000       1 
</code></pre>

<p>Any ideas why this might be happening?</p>

<p>In case it's important, the (QAP) correlation coefficient between the two independent variables is 0.382</p>

<p>I've only included the 30 interviewees in the matrix rows because they are the only people from whom there might be a tie. The networks and QAP regressions are directed.</p>

<p>Incidentally, if I run the QAP logit using full 75x75 adjacency matrices (all people in the columns also appear as rows), I get the same output in both programs.</p>

<p>I also have a related question... A colleague suggested I could run the analysis using the 75x75 matrices but replace the rows of people I haven't interviewed with NAs. This gives me the same results in R and UCINET. Does this seem like a sensible approach, rather than using rectangular matrices?</p>

<p>Thanks!</p>

<p>Reproducible example in R:</p>

<pre><code>library(sna)

# row and column labels
rowIDs &lt;- c(""1"",  ""2"",  ""3"",  ""5"",  ""9"",  ""16"", ""18"", ""19"", ""26"", ""27"", ""34"", ""35"", ""36"", ""40"", ""46"", ""49"", ""60"", ""64"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""82"", ""85"", ""86"", ""97"", ""100"")
colIDs &lt;- c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""23"", ""26"", ""27"", ""34"", ""35"", ""36"", ""38"", ""40"", ""41"", ""43"", ""45"", ""46"", ""47"", ""49"", ""51"", ""52"", ""53"", ""57"", ""59"", ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", ""66"", ""67"", ""68"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""78"", ""79"", ""81"", ""82"", ""83"", ""84"", ""85"", ""86"", ""87"", ""88"", ""89"", ""91"", ""92"", ""93"", ""94"", ""95"", ""96"", ""97"", ""100"", ""101"")

# create matrices
adj.dep &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0""))

adj.indep1 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

adj.indep2 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

# assign row/column names
rownames(adj.dep) &lt;- rowIDs
colnames(adj.dep) &lt;- colIDs
rownames(adj.indep1) &lt;- rowIDs
colnames(adj.indep1) &lt;- colIDs
rownames(adj.indep2) &lt;- rowIDs
colnames(adj.indep2) &lt;- colIDs

# set up independent variables
g.indeps &lt;- array(dim=c(2, nrow(adj.indep1), ncol(adj.indep1)))
g.indeps[1,,] &lt;- adj.indep1
g.indeps[2,,] &lt;- adj.indep2

# run the analysis
# (warning, this command takes a bit of time to run with 10,000 reps)
nl &lt;- netlogit(adj.dep, g.indeps, reps=10000, nullhyp=""qap"")
# print output
summary(nl)
</code></pre>
"
"0.184374032861962","0.18193666795261","115065","<p>I am working with data from a computer task which has 288 total trials, each of which can be categorically classified according to <strong>Trial Type</strong>, <strong>Number of Stimuli</strong>, and <strong>Probe Location</strong>.  Because I want to also examine a continuous variable, the total Cartesian <strong>Distance</strong> between stimuli per trial (divided by number of stimuli to control for varying numbers), I have opted to use a mixed linear model with repeated measures.  In addition to each of these task variables, I am also interested in whether folks in various diagnostic groups perform differently on the task, as well as whether or not there is a <strong>Dx</strong> interaction with any of the above variables.  Thus (if I'm not mistaken), I have the following effects in my model:</p>

<p><strong>Trial Type</strong>, a fixed effect
<strong>Number of Stimuli</strong>, a fixed effect
<strong>Probe Location</strong>, a fixed effect
<strong>Dist</strong>(ance), a fixed effect
<strong>Dx</strong>, a fixed effect
<strong>Dx*Trial Type</strong>, a fixed effect
<strong>Dx*Number of Stimuli</strong>, a fixed effect
<strong>Dx*Probe Location</strong>, a fixed effect
<strong>Dx*Dist</strong>, a fixed effect
<strong>Trial</strong>, a random effect, nested within
<strong>SubID</strong>, a random effect</p>

<p>Based on my examination of documentation, it seems that the nesting of random effects does not seem to be important to lme4, and so I specify my model as follows:</p>

<p><code>tab.lmer &lt;- lmer(Correct ~  Dx+No_of_Stim+Trial_Type+Probe_Loc+Dist+Dx*No_of_Stim+Dx*Trial_Type+Dx*Probe_Loc+Dx*Dist+(1|Trial)+(1|SubID),data=bigdf)</code></p>

<p>This would be my first question: <strong>1) Is the above model specification correct?</strong></p>

<p>Assuming so, I am a bit troubled by my results, but as I read and recall my instruction on such models, I am wondering if interpretation of particular coefficients is bad practice in this case:</p>

<pre><code>Linear mixed model fit by REML ['merModLmerTest']
Formula: Correct ~ Dx + No_of_Stim + Trial_Type + Probe_Loc + Dist + Dx *  
    No_of_Stim + Dx * Trial_Type + Dx * Probe_Loc + Dx * Dist +  
    (1 | Trial) + (1 | SubID)
   Data: bigdf

REML criterion at convergence: 13600.4

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.89810 -0.03306  0.27004  0.55363  2.81656 

Random effects:
 Groups   Name        Variance Std.Dev.
 Trial    (Intercept) 0.013256 0.11513 
 SubID    (Intercept) 0.006299 0.07937 
 Residual             0.131522 0.36266 
Number of obs: 15840, groups:  Trial, 288; SubID, 55

Fixed effects:
                         Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)             4.196e-01  4.229e-02  4.570e+02   9.922  &lt; 2e-16 ***
DxPROBAND               8.662e-02  4.330e-02  2.920e+02   2.000  0.04640 *  
DxRELATIVE              9.917e-02  4.009e-02  2.920e+02   2.474  0.01394 *  
No_of_Stim3            -9.281e-02  1.999e-02  4.520e+02  -4.642 4.53e-06 ***
Trial_Type1             3.656e-02  2.020e-02  4.520e+02   1.810  0.07097 .  
Probe_Loc1              3.502e-01  2.266e-02  4.520e+02  15.456  &lt; 2e-16 ***
Probe_Loc2              3.535e-01  3.110e-02  4.520e+02  11.369  &lt; 2e-16 ***
Dist                    1.817e-01  2.794e-02  4.520e+02   6.505 2.06e-10 ***
DxPROBAND:No_of_Stim3  -1.744e-02  1.759e-02  1.548e+04  -0.992  0.32144    
DxRELATIVE:No_of_Stim3 -2.886e-02  1.628e-02  1.548e+04  -1.773  0.07628 .  
DxPROBAND:Trial_Type1  -9.250e-03  1.777e-02  1.548e+04  -0.521  0.60267    
DxRELATIVE:Trial_Type1  1.336e-02  1.645e-02  1.548e+04   0.812  0.41682    
DxPROBAND:Probe_Loc1   -8.696e-02  1.993e-02  1.548e+04  -4.363 1.29e-05 ***
DxRELATIVE:Probe_Loc1  -4.287e-02  1.845e-02  1.548e+04  -2.323  0.02018 *  
DxPROBAND:Probe_Loc2   -1.389e-01  2.735e-02  1.548e+04  -5.079 3.83e-07 ***
DxRELATIVE:Probe_Loc2  -8.036e-02  2.532e-02  1.548e+04  -3.173  0.00151 ** 
DxPROBAND:Dist         -3.920e-02  2.457e-02  1.548e+04  -1.595  0.11066    
DxRELATIVE:Dist        -1.485e-02  2.275e-02  1.548e+04  -0.653  0.51390    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In general, these results make sense to me.  The troubling portion, however, comes in the positive, significant (yes, I am using LmerTest) p-value for DxProband, particularly in light of the fact that in terms of performance means, Probands are performing worse than Controls.  So, this mismatch concerns me.  Examining the corresponding ANOVA:</p>

<pre><code>&gt; anova(tab.lmer)
Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq Mean Sq NumDF   DenDF F.value    Pr(&gt;F)    
Dx             0.8615  0.4308     2   159.0   1.412   0.24662    
No_of_Stim     0.6984  0.6984     1   283.5  37.043 3.741e-09 ***
Trial_Type     8.3413  8.3413     1   283.5   4.456   0.03565 *  
Probe_Loc     25.7223 12.8612     2   283.5 116.405 &lt; 2.2e-16 ***
Dist           5.8596  5.8596     1   283.5  43.399 2.166e-10 ***
Dx:No_of_Stim  1.4103  0.7051     2 15483.7   1.590   0.20395    
Dx:Trial_Type  2.0323  1.0162     2 15483.7   0.841   0.43128    
Dx:Probe_Loc   3.5740  0.8935     4 15483.7   7.299 7.224e-06 ***
Dx:Dist        0.3360  0.1680     2 15483.7   1.277   0.27885    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...the results seem to more or less line up with the regression, with the exception of the <strong>Dx</strong> variable.  So, my second question is <strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong></p>

<p>Finally, as a basic (and somewhat embarrassing) afterthought, <strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p>In summation,
<strong>1) Is the above model specification correct?</strong>
<strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong>
<strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p><strong>ADDENDUM</strong></p>

<p>By request, I'll describe the task and data a little further.  The data come from a computer task in which participants are presented a number of stimuli, either two or three, in various locations about the screen.  These stimuli can either be ""targets"" or ""distractors"".  After these stimuli, a probe stimulus is presented; if it appears in a position where a previous target has appeared, participants should respond ""yes""; if it appears in the position of a previous distractor or elsewhere, the correct answer is ""no.""  There are 288 trials of this nature; some have two stimuli and some have three, and some lack distractors entirely.  The variables in my model, then, can be elaborated as follows:</p>

<p><strong>Number of Stimuli:</strong> 2 or 3 (2 levels)</p>

<p><strong>Trial Type:</strong> No Distractor (0) or Distractor (1) (2 levels)</p>

<p><strong>Probe Location:</strong> Probe at Target (1), Probe at Distractor (2), or Probe Elsewhere (0) (3 levels)</p>

<p><strong>Distance:</strong> Total Cartesian distance between stimuli, divided by number of stimuli per trial (Continuous)</p>

<p><strong>Dx:</strong> Participant's clinical categorization</p>

<p><strong>Sub ID:</strong> Unique subject identifier (random effect)</p>

<p><strong>Trial:</strong> Trial number (1:288) (random effect)</p>

<p><strong>Correct:</strong> Response classification, either incorrect (0) or correct (1) per trial</p>

<p>Note that the task design makes it inherently imbalanced, as trials without distractors cannot have Probe Location ""Probe at Distractor""; this makes R mad when I try to run RM ANOVAs, and it is another reason I opted for a regression.</p>

<p>Below is a sample of my data (with SubID altered, just in case anyone might get mad):</p>

<pre><code>     SubID      Dx Correct No_of_Stim Trial_Type Probe_Loc      Dist Trial
1 99999999 PROBAND       1          3          0         1 0.9217487     1
2 99999999 PROBAND       0          3          0         0 1.2808184     2
3 99999999 PROBAND       1          3          0         0 1.0645292     3
4 99999999 PROBAND       1          3          1         2 0.7838786     4
5 99999999 PROBAND       0          3          0         0 1.0968788     5
6 99999999 PROBAND       1          3          1         1 1.3076598     6
</code></pre>

<p>Hopefully, with the above variable descriptions these data should be self-explanatory.</p>

<p>Any assistance that people can provide in this matter is very much appreciated.</p>

<p>Sincerely,
peteralynn</p>
"
"0.0948209311861521","0.0935674292327708","115126","<p>I need to do a Multiple Imputation on a dataset with several missing values, and I need to do it with mice, because later I'll have to compare the results with those of imputations ran with other programs.</p>

<p>My colleague obtained a completa dataset by running a MI on the incomplete dataset, with 5 iterations, and then taking the 5 imputed datasets and manually calculating mean values. Se essentially he pooled manually. I'm far from an expert so I don't know if this operation is valid. </p>

<p>Anyways, in MICE so far I could run the imputation (again, with maxit=5), using the function ""imp&lt;-mice(eco)"", where ""eco"" is the incomplete dataset. So I obtained the 5 imputed datasets, stored in the object ""imp"", of class ""MIDS"". Now I just need to pool the 5 completed datasets to obtain a unique complete one, i don't wanna run analyses on the 5 datasets and then pool the results. Can that be done? If I got it right from the manual, it seems that MICE allows you to pool only after you ran some analysis on the imputed datasets. The analysis is repeated on each dataset and the results are stored in an object of class ""MIRA"". I tried to run the function ""pool()"" on ""imp"" but it can't be done because imp is of class ""mids"" and you can only pool mira objects.</p>

<p>The manual also says that to pool you need a variance/covariance relation, and in the example given there they run a linear regression and then they pool the results of the regression. But I doubt if it's what I need. I'm confused</p>
"
"0.0952986215201744","0.0940388046555249","115133","<p>I have a data-set with 32 effect size estimates- only 11 of which report a value for the continuous moderator of interest (the samples anxiety level). A complete case analysis (restricted to the 11 cases) shows that anxiety is a significant predictor of the effect sizes in meta-regression. </p>

<p>I would like to use imputation techniques to ""fill in"" the missing values to see if the relationship between anxiety and effect size (d) is still significant. If I do this using the ""mice"" function in R, it automatically selects d (the effect size) as a predictor to impute the plausible values of anxiety, as shown in the predictor matrix.</p>

<p>My issue is that this seems <em>circular</em>- I already know that anxiety predicts d, so using d to predict the missing values of anxiety seems to be ""playing tennis without the net"" and will surely artificially strengthen the relationship.</p>

<p>On the other hand- I <em>don't know for sure</em> that increased anxiety predicts d or whether increased anxiety is an outcome of the effect size- they may have a mutual influence. (d reflects a bias for threat, which literature suggests could be a <em>cause or consequence</em> of anxiety.....). This would seem to make using d in imputation more legitimate. Also, the recommendation seems to be to use all variables that will appear in the model applied after imputation (which will of course include d as the outcome) in the imputation process (van Buuren, 1999).</p>

<p>So given this issue, should I simply remove the effect size as a predictor of anxiety for imputation and instead rely on other demographic variables, or just random sampling of the observed data? Doing this also seems wrong, since this seems to generate a false ""uncertainty"" in the imputed data, given we have an idea of the relation between d and anxiety.  </p>

<p>Any help or references to resolve this problem would be much appreciated. Please let me know if anything is unclear.         </p>
"
"0.0952986215201744","0.0940388046555249","115219","<p><strong><em>Imagine the situation:</em></strong> Mythical Seafolk use holes in the seabed as their burrows. Each hole has two parameters - diameter and depth. <strong>Majority of holes are unoccupied</strong> due to their surplus (n = 235). Occupied holes (n = 15) are (generally) expected to be much deeper and with larger diameter than random.</p>

<pre><code># generate data
set.seed(1234)
x &lt;- runif(250, min=0, max=10)
y &lt;- runif(250, min=0, max=10)
rbPal   &lt;- colorRampPalette(c(""deepskyblue"",""darkblue""))
my.data &lt;- data.frame(x_coor = x, y_coor = y,
                      diameter = c(abs(rnorm(15)+2.5), abs(rnorm(235))),
                      depth = c(abs(rnorm(15)+2.5), abs(rnorm(235))),
                      usage = rep(c(1,0), times = c(15, 235)))
my.data$col &lt;- rbPal(10)[as.numeric(cut(my.data$depth,breaks = 10))]

# look at the situation
# occupied holes are marked by red circles
plot(my.data$x, my.data$y, cex = my.data$diameter, col = my.data$col, pch = 20)
grid(5, 5, lwd = 0.75, lty = 2, col = ""grey"") 
points(my.data$x[1:25], my.data$y[1:25], pch = 1, cex = 2,
       col = ""red"", lwd = 1.75)
</code></pre>

<p><img src=""http://i.stack.imgur.com/8O1Xg.png"" alt=""enter image description here""></p>

<p>Although this analysis seem to be rather straightforward, <strong>I have some doubts about comparing unequal samples (15 vs 235)</strong>. Such problem do not occur in other <em>Seafolk burrow selection studies</em> because <strong>mapping of seabed is very expensive and time demanding</strong> and when researchers find 10 used holes, they continue to map the seabed only till they get find additonal 10 nonused holes (which are most probably located in close vicinity of used holes). Due to this
data collection approach the sample sizes of compared groups are usually almost equal.
<strong>However, we were able to find ALL holes in seabed - which can be now considered as a ""handicap"".</strong></p>

<p><strong><em>We do not want to do random sampling of 15 holes from 235 to obtain equal sizes (15 vs 15). Why to throw away such large amount of data!? Is there any solution/approach to this situation which is able to fully embrace such unique dataset?</em></strong></p>

<p><em>I have in mind two possible ways (please comment or add others):</em></p>

<p><strong>1.</strong> Take the sample of unoccupied holes (n = 235) and randomly resample them to (for example 50) smaller groups (all with n = 15). Subsequently, each of these reduced groups will be compared with sample of occupied holes by binomial logistic regression. By this I will obtain 50 logistic curves which will
be ""averaged"" into just one curve - final model.</p>

<p><strong>2.</strong> Take the sample of unoccupied holes (n = 235) and randomly resample them to (for example 50) smaller groups (all with n = 15). Subsequently, from each of these reduced groups (and from sample of used holes also) we gain (mean, sd, sample size). Thus, we will have six numbers for each comparison and we calculate effect size - standardized mean difference (SMD). Effect sizes will be inserted into meta-analytic model and tested if there is a heterogeneity present among effect sizes (<em>to see if the ""randomnes"" of creating unused holes sample has the significant impact on test result</em>).</p>
"
"0.0316069770620507","0.0311891430775903","115225","<p>I am a bit confused on whether or not I have to use a fixed-effect panel time-series method or SUR (seemingly unrelated regression). To get a background of what I am trying to do, I have 10 panels of 25 weeks of data with four independent variables and one dependent variable and I am trying to find how these four independent variables effect the dependent variable. I am currently using R to do my analysis.</p>
"
"0.118262479197817","0.116699087583415","115304","<p>I am learning about building linear regression models by looking over someone elses R code.  Here is the example data I am using:</p>

<pre><code>v1  v2  v3  response
0.417655013 -0.012026453    -0.528416414    48.55555556
-0.018445979    -0.460809371    0.054017873 47.76666667
-0.246110341    0.092230159 0.057435968 49.14444444
-0.521980295    -0.428499038    0.119640369 51.08888889
0.633310578 -0.224215856    -0.153917427    48.97777778
0.41522316  0.050609412 -0.642394965    48.5
-0.07349941 0.547128578 -0.539018121    53.95555556
-0.313950353    0.207853678 0.713903994 48.16666667
0.404643796 -0.326782199    -0.785848428    47.7
0.028246796 -0.424323318    0.289313911 49.34444444
0.720822953 -0.166712488    0.323246062 50.78888889
-0.430825851    -0.308119827    0.543823856 52.65555556
-0.964175294    0.661700584 -0.11905972 51.03333333
-0.178955757    -0.11148414 -0.151179885    48.28888889
0.488388035 0.515903257 -0.087738159    48.68888889
-0.097527627    0.188292773 0.207321867 49.86666667
0.481853599 0.21142728  -0.226700254    48.38888889
1.139561277 -0.293574756    0.574855693 54.55555556
0.104077762 0.16075114  -0.131124443    48.61111111
</code></pre>

<p>I read in the data and use a call to <code>lm()</code> to build a model:</p>

<pre><code>&gt; my_data&lt;- read.table(""data.csv"", header = T, sep = "","")
&gt; my_lm &lt;- lm(response~v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, data=my_data)
&gt; summary(my_lm)

Call:
lm(formula = response ~ v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, 
data = my_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.0603 -0.6615 -0.1891  1.0395  1.8280 

Coefficients:
         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  49.33944    0.42089 117.226  &lt; 2e-16 ***
v1            0.06611    0.82320   0.080  0.93732    
v2           -0.36725    1.06359  -0.345  0.73585    
v3            0.72741    1.00973   0.720  0.48508    
v1:v2        -2.54544    2.21663  -1.148  0.27321    
v1:v3         0.80641    2.77603   0.290  0.77640    
v2:v3       -12.16017    3.62473  -3.355  0.00573 ** 
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.375 on 12 degrees of freedom
Multiple R-squared:  0.697, Adjusted R-squared:  0.5455 
F-statistic:   4.6 on 6 and 12 DF,  p-value: 0.01191
</code></pre>

<p>Following along with their code I then use a call to <code>anova()</code>:</p>

<pre><code>&gt; my_lm_anova &lt;- anova(my_lm)
&gt; my_lm_anova
Analysis of Variance Table

Response: response
          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
v1         1  0.0010  0.0010  0.0005 0.982400   
v2         1  0.2842  0.2842  0.1503 0.705036   
v3         1  9.8059  9.8059  5.1856 0.041891 * 
v1:v2      1  4.3653  4.3653  2.3084 0.154573   
v1:v3      1 16.4582 16.4582  8.7034 0.012141 * 
v2:v3      1 21.2824 21.2824 11.2545 0.005729 **
Residuals 12 22.6921  1.8910                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, I am not sure:</p>

<ol>
<li>Why I would use the call to ANOVA in this situation, and</li>
<li>What the ANOVA table is telling me about the predictor variables.</li>
</ol>

<p>From the code they appear to use the ANOVA table as follows.  For predictor variable v1, the result of </p>

<ul>
<li>Adding the 'Sum Sq' entry for v1 together with half of the 'Sum Sq' entry for v1:v2 and half of the 'Sum Sq' entry for v1:v3, </li>
<li>Dividing by the sum of the entire 'Sum Sq' column, and</li>
<li>Multiplying by 100</li>
</ul>

<p>gives the percent of variance of the response variable that is explained by predictor variable v1 in the <code>lm()</code> model.  I don't see why this is nor why half of the 'Sum Sq' entry for v1:v2 is attributed to v1 and half to v2.  Is this just convenience?</p>
"
"0.0632139541241014","0.0623782861551805","115843","<p>I want to perform an exploratory Cox regression analysis of medical data using R. I am practicing using the pbc data from the survival function.</p>

<p>Would you recommend performing a backward selection multivariate analysis? Are there any summary data / tables I should create for covariates before modelling? Are there any model diagnostics I should perform? And what would be the consequence of doing this?</p>

<p>I would be very grateful for your help and examples using R; also easy to understand literature recommendations (paper, book, and so on) would be nice. </p>

<hr>

<p>To renew my former question: I understand that a stepwise backward regression will lead to inflated coefficients, deflated p-values, and inflated model fit statistics. However, this approach is very common in medical reports. Would it be possible to draw the conclusion that a covariate is independently associated with an outcome, irrespective the above mentioned drawbacks? And when yes, how reliable would it be?</p>

<p>And again <em>being a Little afraid to ask this</em> what would be the best way in R to perform such an analysis?</p>
"
"0.145392094485433","0.155945715387951","116007","<p>I have a fairly simple dataset looking at the relationship between the first nesting date of a bird in a given year (Date) and the birds overall fledgling production from that year (Fledge; count data from 0-3 fledglings). I want to determine the optimal laying date for this species (i.e. the date where fledgling production is highest); however, I have been struggling to work out which statistical analysis is most appropriate for my data.</p>

<p>Most birds produce no fledglings in a year, so there are many zero values in the data. With this in mind, I thought that a zero inflated poisson regression might be most appropriate. To test this in R, I fitted a regular glm with poisson distribution (model1 below) and a zero inflated poisson model using zeroinfl() from the pscl library (model2 below). I then compared the two using vuong test statistic (output below).</p>

<pre><code>model1&lt;-glm(Fledge~Date,data=OPT,family=""poisson"")
model2&lt;-zeroinfl(Fledge~Date,data=OPT,dist=""poisson"")
vuong(model1,model2)

Vuong Non-Nested Hypothesis Test-Statistic: 4.25169 
(test-statistic is asymptotically distributed N(0,1) under the
null that the models are indistinguishible)
in this case:
model1 &gt; model2, with p-value 1.0608e-05
</code></pre>

<p>According to the vuong output, a regular non-zero inflated poisson regression is most appropriate. This wasn't that surprising as, from my understanding, the zeros in my data are 'true zeroes' i.e. they are legitimate data points and not due to sampling technique or design. Next I tested for overdispersion by fitting a glm with a quasipoisson distribution to check the dispersion parameter (model3).</p>

<pre><code>model3&lt;-glm(Fledge~Date,data=OPT,family=""quasipoisson"")
summary(model3)

Call:
glm(formula = Fledge ~ Date, family = ""quasipoisson"", data = OPT)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7877  -0.6258  -0.5578  -0.4504   3.3648  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept) -0.15109    0.54874  -0.275  0.78315   
Date        -0.03288    0.01133  -2.901  0.00385 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 1.296628)

Null deviance: 455.36  on 642  degrees of freedom
Residual deviance: 443.40  on 641  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 6
</code></pre>

<p>As you can see, the output showed a dispersion parameter close to one (1.297), and a null deviance (455.36) very close to the residual deviance (443.40). So I concluded that overdispersion was not a big problem, and a poisson, rather than negative binomial, distribution was required.</p>

<p>After doing all this, I was fairly confident that my regular poisson regression (model1) was best for my data. HOWEVER, when I plotted the model outputs of the zero inflated and non-zero inflated data, the non-zero inflated model output didn't appear to fit my data at all while the zero inflated model fitted much better.</p>

<pre><code>ggplot(OPT,aes(x=Date,y=Fledge))+
geom_point()+
theme_bw()+
geom_line(data=cbind(OPT,optpred=predict(model1)),aes(y=optpred),size=1,colour=""red"")+
geom_line(data=cbind(OPT,optpred2=predict(model2)),aes(y=optpred2),size=1,colour=""blue)
</code></pre>

<p><img src=""http://i.stack.imgur.com/GFp5F.jpg"" alt=""enter image description here""></p>

<p>As you can see, the non-zero inflated model (red line) doesn't seem to fit the data at all, as it is predicting fledgling production less than 0 on all dates (obviously not biologically possible)! Conversely, the zero inflated model (blue line) seems to fit the data very well. So I have two questions to ask;</p>

<ol>
<li><p>Were the methods I used to test zero inflation (and overdispersion) appropriate and interpreted correctly?</p></li>
<li><p>If so, is my application of a poisson regression appropriate? Or are there other more appropriate options for my data outside of the two I've tried here?</p></li>
</ol>

<p>I've included a dput() of my data below to allow for replication.</p>

<p>Thanks for the help!</p>

<pre><code>structure(list(Date = c(45L, 40L, 42L, 41L, 39L, 34L, 40L, 44L, 
36L, 32L, 33L, 89L, 58L, 50L, 46L, 56L, 48L, 69L, 64L, 56L, 61L, 
58L, 66L, 63L, 57L, 58L, 60L, 65L, 31L, 48L, 42L, 41L, 46L, 38L, 
59L, 41L, 65L, 41L, 34L, 41L, 36L, 60L, 42L, 39L, 43L, 46L, 47L, 
38L, 38L, 71L, 65L, 51L, 42L, 37L, 51L, 41L, 65L, 59L, 44L, 50L, 
51L, 47L, 40L, 53L, 56L, 62L, 50L, 46L, 51L, 55L, 50L, 46L, 45L, 
39L, 36L, 52L, 50L, 73L, 42L, 38L, 51L, 49L, 43L, 45L, 44L, 76L, 
68L, 65L, 70L, 56L, 40L, 45L, 49L, 52L, 66L, 80L, 45L, 42L, 44L, 
37L, 48L, 43L, 53L, 31L, 47L, 49L, 44L, 46L, 54L, 55L, 48L, 53L, 
55L, 72L, 54L, 45L, 83L, 59L, 48L, 47L, 52L, 72L, 51L, 70L, 48L, 
44L, 42L, 38L, 48L, 43L, 45L, 39L, 45L, 42L, 64L, 46L, 56L, 34L, 
50L, 48L, 47L, 47L, 60L, 50L, 61L, 40L, 72L, 63L, 55L, 66L, 69L, 
66L, 61L, 60L, 60L, 40L, 70L, 45L, 40L, 41L, 42L, 71L, 54L, 45L, 
52L, 48L, 40L, 39L, 49L, 42L, 43L, 53L, 38L, 53L, 52L, 68L, 61L, 
62L, 87L, 41L, 45L, 37L, 44L, 45L, 43L, 72L, 39L, 56L, 34L, 74L, 
62L, 46L, 43L, 47L, 35L, 54L, 61L, 44L, 49L, 54L, 61L, 37L, 51L, 
48L, 52L, 48L, 48L, 44L, 45L, 44L, 45L, 68L, 61L, 87L, 51L, 52L, 
50L, 50L, 56L, 55L, 56L, 57L, 65L, 41L, 63L, 76L, 52L, 62L, 50L, 
50L, 54L, 63L, 48L, 54L, 46L, 57L, 54L, 52L, 45L, 41L, 54L, 74L, 
69L, 68L, 51L, 60L, 54L, 44L, 67L, 52L, 49L, 43L, 41L, 44L, 49L, 
46L, 43L, 46L, 49L, 46L, 47L, 54L, 55L, 67L, 52L, 55L, 52L, 49L, 
50L, 51L, 57L, 48L, 34L, 54L, 49L, 47L, 71L, 62L, 43L, 45L, 45L, 
49L, 58L, 57L, 55L, 54L, 52L, 51L, 41L, 54L, 70L, 52L, 53L, 53L, 
50L, 71L, 56L, 48L, 33L, 43L, 41L, 68L, 42L, 38L, 39L, 46L, 55L, 
64L, 62L, 56L, 69L, 44L, 49L, 54L, 86L, 46L, 46L, 50L, 44L, 45L, 
55L, 55L, 52L, 49L, 49L, 56L, 41L, 34L, 50L, 62L, 39L, 41L, 56L, 
42L, 40L, 43L, 44L, 45L, 43L, 48L, 41L, 45L, 62L, 49L, 47L, 49L, 
63L, 69L, 46L, 53L, 49L, 59L, 54L, 33L, 46L, 44L, 49L, 36L, 41L, 
33L, 66L, 56L, 67L, 43L, 66L, 31L, 51L, 59L, 57L, 51L, 39L, 44L, 
31L, 40L, 39L, 42L, 27L, 43L, 42L, 78L, 60L, 70L, 64L, 67L, 66L, 
67L, 66L, 62L, 58L, 51L, 50L, 60L, 38L, 45L, 34L, 69L, 38L, 45L, 
39L, 44L, 39L, 44L, 43L, 46L, 37L, 59L, 74L, 59L, 39L, 43L, 40L, 
38L, 45L, 45L, 42L, 36L, 33L, 51L, 64L, 52L, 40L, 89L, 49L, 37L, 
51L, 70L, 65L, 71L, 62L, 61L, 68L, 59L, 54L, 75L, 57L, 55L, 58L, 
52L, 58L, 45L, 50L, 41L, 64L, 49L, 50L, 67L, 54L, 43L, 49L, 54L, 
55L, 53L, 53L, 59L, 47L, 47L, 48L, 45L, 50L, 39L, 48L, 51L, 54L, 
44L, 43L, 56L, 51L, 38L, 71L, 62L, 56L, 65L, 69L, 68L, 52L, 47L, 
47L, 47L, 80L, 51L, 48L, 36L, 32L, 39L, 45L, 31L, 43L, 57L, 65L, 
60L, 62L, 36L, 53L, 64L, 57L, 43L, 71L, 66L, 63L, 49L, 39L, 49L, 
43L, 32L, 47L, 44L, 35L, 35L, 41L, 54L, 50L, 44L, 44L, 48L, 50L, 
41L, 40L, 46L, 48L, 38L, 43L, 54L, 52L, 36L, 62L, 72L, 47L, 66L, 
50L, 51L, 50L, 56L, 47L, 67L, 50L, 35L, 40L, 43L, 42L, 31L, 35L, 
43L, 46L, 45L, 46L, 39L, 40L, 40L, 39L, 36L, 45L, 43L, 44L, 44L, 
44L, 38L, 49L, 52L, 49L, 43L, 42L, 47L, 56L, 51L, 51L, 51L, 59L, 
64L, 46L, 40L, 75L, 65L, 51L, 91L, 56L, 83L, 56L, 57L, 58L, 51L, 
50L, 56L, 40L, 69L, 54L, 45L, 35L, 41L, 48L, 60L, 54L, 39L, 39L, 
31L, 92L, 39L, 66L, 56L, 48L, 44L, 40L, 42L, 47L, 51L, 47L, 45L, 
49L, 69L, 48L, 42L, 58L, 56L, 58L, 61L, 42L, 36L, 47L, 52L, 45L, 
54L, 55L, 62L, 48L, 44L, 54L, 51L, 46L, 44L, 50L, 37L, 33L, 40L, 
57L, 54L, 64L, 59L, 69L, 46L, 40L, 51L, 53L, 79L, 60L), Fledge = c(1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 3L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 
0L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 2L, 0L, 0L, 0L, 0L, 
0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 1L, 2L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 
0L, 1L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 3L, 0L, 0L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
2L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 2L, 1L, 2L, 0L, 0L, 0L, 1L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 3L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 2L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 
0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 2L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
2L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 2L, 0L, 0L, 0L, 1L, 0L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 1L, 1L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L)), .Names = c(""Date"", ""Fledge""), class = ""data.frame"", row.names = c(NA, 
    -643L))
</code></pre>
"
"0.0547448901451359","0.0540211804549215","116274","<p>I'm relatively new to statistics, and am currently working some data collected as a part of an interview survey. I have a response variable in ordinal form, which mostly looks into people's perceptions of an animal, and a range of predictor variables that include discrete numeric, ordinal and binary variables. I have a couple of questions regarding the analyses:</p>

<p>Would it be appropriate to use Classfication and Regression trees to analyse the data? If yes, is there a limit on the number of predictor variables that can be used for sample sizes of 50 and 139 (two sets of data to be analysed separately)? </p>

<p>Would it make sense to use MDS or Factor analysis to reduce the number of variables?</p>

<p>Thanks and regards.</p>
"
"0.0893980312535348","0.0882162182782462","116659","<p>I have a collection of continuous data from the literature, including the mean, the standard deviation and the number of observations for both experimental and control groups, as well some environmental variables. A meta-analysis coupled with a meta-regression could be done. However, some studies had several experimental sites, i.e. each line is not a single study, but a single site taken from a study: a random site effect could thus be nested in studies. The following meta mixed-model can be described.</p>

<ol>
<li>effect-size as response</li>
<li>environmental variables as fixed effects,</li>
<li>sites nested in studies as random effects and</li>
<li>each observation is weighted on the sampling variance.</li>
</ol>

<p>I tried two approaches with R, that returned somewhat different results.</p>

<p>As first step, let's create a dummy data frame.</p>

<pre><code>set.seed(40)
Study = factor(c(1,1,1,1,1,2,2,3,3,3,4,5,5,5,5))
Site = factor(c(1,2,3,4,5,1,2,1,2,3,1,1,2,3,4))
n_case = length(Study)
experimental_mean = rnorm(n_case,5,2)
experimental_sd = abs(rnorm(n_case,1,0.1))
experimental_n = round(runif(n_case, 2, 10))
control_mean = experimental_mean+runif(n_case,0,5)
control_sd = abs(rnorm(n_case,1,0.1))
control_n = round(runif(n_case, 2, 10))
A = experimental_mean/control_mean * runif(n_case, 0.5, 0.8)
B = rnorm(n_case,-1,1)
C = factor(letters[round(runif(n_case, 1, 3))])

data_table = data.frame(Study, Site, 
                        experimental_mean, experimental_sd, experimental_n,
                        control_mean, control_sd, control_n,
                        A, B, C)
</code></pre>

<p>Then, compute the effect size and the sampling variance.</p>

<pre><code>library(metafor)
meta_table = escalc(measure='ROM', data=data_table,
                  m1i=experimental_mean, m2i=control_mean,
                  sd1i=experimental_sd, sd2i=control_sd,
                  n1i=experimental_n, n2i=control_n)
</code></pre>

<p>The rma.mv function from the metafor package could be used to run the meta mixed-model.</p>

<pre><code>res = rma.mv(yi, vi, data=meta_table, method=""REML"", level=95,
              mods = ~ A+B+C, random=~1|Study/Site)
summary(res)
</code></pre>

<p>Another option is to run a mixed-model on the effect-size.</p>

<pre><code>library(nlme)
mixed_meta_model = lme(data = meta_table,
                       fixed = yi ~ A+B+C, # yi is the effect size
                       random = ~1|Study/Site,
                       weights = varIdent(~vi)) # vi is the sampling variance
summary(mixed_meta_model)
</code></pre>

<p>Three questions:</p>

<ol>
<li>Is the global approach valid?</li>
<li>Which approach would you suggest?</li>
<li>Is my R code correct? - with special attention to the weights argument <strong>varIdent(~vi)</strong> in the lme function.</li>
</ol>

<p>Many thanks,</p>

<p>S.-Ã‰. Parent
Laval University, Canada</p>
"
"0.0597315721433636","0.0707303281615848","116825","<p>I'm exploring linear regressions in R and Python, and usually get the same results but this is an instance I do not. </p>

<p>I added the sum of <code>Agriculture</code> and <code>Education</code> to the <code>swiss</code> dataset as an additional explanatory variable, with <code>Fertility</code> as the regressor.</p>

<p>R gives me an <code>NA</code> for the $\beta$ value of <code>z</code>, but Python gives me a numeric value for <code>z</code> and a warning about a very small eigenvalue. Is there a way to make Python and statmodels explicitly tell me that <code>z</code> adds no information to the regressor?</p>

<p>Additionally, I originally did this analysis in an iPython notebook, where there is no need to do an explicit <code>print</code> of the regression summary results <code>reg_results</code>, and when the <code>print</code> command is omitted there is no warning about the low eigenvalues which makes it more difficult to know that <code>z</code> is worthless.</p>

<p>R code:</p>

<pre><code>data(swiss)
swiss$z &lt;- swiss$Agriculture + swiss$Education
formula &lt;- 'Fertility ~ .'
print(lm(formula, data=swiss))
</code></pre>

<p>R output:</p>

<pre><code>Call:
lm(formula = formula, data = swiss)

Coefficients:
     (Intercept)       Agriculture       Examination         Education
         66.9152           -0.1721           -0.2580           -0.8709
        Catholic  Infant.Mortality                 z
          0.1041            1.0770                NA
</code></pre>

<p>Python Code:</p>

<pre><code>import statsmodels.formula.api as sm
import pandas.rpy.common as com

swiss = com.load_data('swiss')

# get rid of periods in column names
swiss.columns = [_.replace('.', '_') for _ in swiss.columns]

# add clearly duplicative data
swiss['z'] = swiss['Agriculture'] + swiss['Education']

y = 'Fertility'
x = ""+"".join(swiss.columns - [y])
formula = '%s ~ %s' % (y, x)
reg_results = sm.ols(formula, data=swiss).fit().summary()
print(reg_results)
</code></pre>

<p>Python output:</p>

<pre><code>                            OLS Regression Results
==============================================================================
Dep. Variable:              Fertility   R-squared:                       0.707
Model:                            OLS   Adj. R-squared:                  0.671
Method:                 Least Squares   F-statistic:                     19.76
Date:                Thu, 25 Sep 2014   Prob (F-statistic):           5.59e-10
Time:                        22:55:42   Log-Likelihood:                -156.04
No. Observations:                  47   AIC:                             324.1
Df Residuals:                      41   BIC:                             335.2
Df Model:                           5
====================================================================================
                       coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------------
Intercept           66.9152     10.706      6.250      0.000        45.294    88.536
Agriculture          0.1756      0.062      2.852      0.007         0.051     0.300
Catholic             0.1041      0.035      2.953      0.005         0.033     0.175
Education           -0.5233      0.115     -4.536      0.000        -0.756    -0.290
Examination         -0.2580      0.254     -1.016      0.315        -0.771     0.255
Infant_Mortality     1.0770      0.382      2.822      0.007         0.306     1.848
z                   -0.3477      0.073     -4.760      0.000        -0.495    -0.200
==============================================================================
Omnibus:                        0.058   Durbin-Watson:                   1.454
Prob(Omnibus):                  0.971   Jarque-Bera (JB):                0.155
Skew:                          -0.077   Prob(JB):                        0.925
Kurtosis:                       2.764   Cond. No.                     1.11e+08
==============================================================================

Warnings:
[1] The smallest eigenvalue is 3.87e-11. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
</code></pre>

<p>```</p>
"
"0.0632139541241014","0.0623782861551805","116850","<p>I have a dataset consisting of repeated measures data of graded toxicity scores (0-4) in a large number of patients being treated with a anti-cancer drug. We would like to identify predictors for developing dose limiting toxicity (e.g. score 3 or 4).</p>

<p>Initially the dataset was analyzed using a cumulative link mixed model to identify such predictors (clmm in R package ordinal).</p>

<p>However, there is also informative dropout occuring in the dataset (MNAR), e.g. the risk of dropping out is related to the severity of toxicity. As this therefore may bias parameter estimates, it seems a joint model that includes a survival model for dropout would be best. </p>

<p>My questions are:</p>

<ol>
<li><p>Is a joint model for dropout and outcome the only suitable option to account for dropout?  Or would there be other ways as well ? The only ugly alternative would be to analyze only the first part of the dataset, but I would rather like to use ALL the available data.</p></li>
<li><p>Is there an R package available that would readily allow such an analysis. E.g. longitudinal mixed-effect modelling of ordered categorical data, together with a dropout survival model. Or, alternatively, a mixed effect model logistic regression model with dropout (e.g. in this case we would reduce the dataset to having a dose limiting toxicity yes/no) ? The JM and joinR packages appear to be based on continuous data for the repeated measures.</p></li>
</ol>
"
"0.0446990156267674","0.0441081091391231","117224","<p>I have a data set containing  10 subjects and students grade in those subjects(A+,A,B+,B,C+,C,D+,D)
i want to find out out if the grades of one subject is affecting the grades for other subjects.
Should i go with regression analysis?if yes then should i replace grades with some numbers before doing the regression ?    </p>

<blockquote>
<pre><code> A+   A+   B+   A    A    A+   A+   A+  A+  A+
</code></pre>
</blockquote>

<p>this is just 1 row of the data set with each grade belonging to 10 different subjects.This row signifies grades of a particular student in 10 different subjects in the exam.I want to know if grade in any subject influences the grade of other.</p>
"
"0.0223495078133837","0.0441081091391231","117437","<p>I want to replicate a fuzzy regression using a linear programming problem approach. </p>

<p>I have the following information: "" A fuzzy regression analysis with only one independent variable X results in the following bivariate regression model:
$$ \hat{Y}=\tilde{A_o}+\tilde{A_1}X,$$</p>

<p>where $\tilde{A_o}$ is a is a fuzzy intercept, $\tilde{A_1}$is a fuzzy slope coefficient, the parameters are expressed as $\tilde{A_i}=(m_i,c_i)$ where $m_i$ is a centre  and $c_i$ is the fuzzy half-width.""</p>

<p>To determine the fuzzy coefficients the following linear programming problem is used:</p>

<p>minimize $$ S= nc_0 + c_1\sum_{i=1}^{n}|X_i|$$</p>

<p>subject to $$c_0\geqslant0,\geqslant0,$$</p>

<p>$$\sum_{j=0}^{l}m_iX_{ij}+(1-h)\sum_{j=0}^{l}c_i|X_{ij}| \geqslant Y_i+(1-h), \mbox{for i=1 to n}$$</p>

<p>$$\sum_{j=0}^{l}m_iX_{ij}-(1-h)\sum_{j=0}^{l}c_i|X_{ij}| \geqslant Y_i-(1-h), \mbox{for i=1 to n}$$</p>

<p>where $h=0$</p>

<p>I have the following data :
$[X_i : Y_i]=[(2:14),(4:16),(6:14),(8:18),(10:18),(12:22),(14:18),
(16:22)]$</p>

<p>How to  solve the LP using R?</p>
"
"0.0316069770620507","0.0311891430775903","117489","<p>From my limited statistical knowledge, I could use MANOVA if I had multiple independent variables (x1, x2...xn). What can I do (specifically in R) with one ""x"" variable and multiple ""y"" groups? I'm trying to see if there is any relationship between the y's with respect to their regression with x. I've already set up a loop that computes bivariate, piecewise linear regressions between each pair (x-y1, x-y2, ... x-yn), but that does not include any analysis of variation between the y variables. Does anybody know how I might do this (in a statistically sound manner, of course) in R? My data looks like this:</p>

<pre><code>x         y1       y2      y3      y4      y5
4.19    5.51    19.76   50.00   19.36   54.07
8.60    10.16   33.01   82.99   38.48   44.95
8.03    7.82    31.29   79.05   40.12   59.18
6.64    8.99    27.13   69.13   30.44   59.02
7.03    8.22    25.29   74.45   36.02   50.88
1.50    5.90    10.69   22.88   10.34   34.50
4.36    7.61    19.27   44.47   20.06   24.62
7.17    8.30    26.72   68.68   31.61   20.16
2.68    5.61    14.25   37.07   15.20   67.75
7.91    7.75    30.93   82.01   38.62   65.36
3.74    5.24    16.42   40.17   17.54   15.19
</code></pre>
"
"0.0948209311861521","0.0935674292327708","118206","<p>I'd like to perform vector autoregression on a two variable system. I know that the signals $x$ and $y$ have a time lag of > 100 time points, and thus any fit with that many time lag parameters is likely to be bad. While I could take the data at a lower frequency, the data is produced at one given frequency. Striding the data (taking every $s$-th observation) is possible, but it seems like it would be throwing out quite a bit of potentially useful data, and would make the analysis far more susceptible to the noise - in my experience, if I stride and then vary the time point I start from just a little, I get very different results. </p>

<p>I believe the right thing to do is obviously to fit this equation: </p>

<p>$$x_t = x_{t-s} + x_{t-2s} + x_{t-3s} + ... + x_{t-ps} + y_{t-s} + y_{t-2s} + y_{t-3s} + ... + y_{t-ps}$$</p>

<p>However, I can't seem to find existing code in R to do that. <code>VAR</code> and <code>ar</code> don't seem to have options to do this, as far as I can find. Does anyone have any suggestions, or do I need to code this up from scratch? Is there a name for this type of model that I'm unaware of?</p>

<p>I've thought about applying smoothing with the window size of $s$ and then striding, and when I do that, I get a significant result irregardless of if I vary the time point I begin at. To me, this indicates that there is certainly a significant time lag contribution (using the Wald test), but I think using the above equation is more rigorous. </p>

<p>EDIT: I wrote R code that allows you do to OLS fitting of vector autoregression when the lag is t - i*s. It functionally gives you everying ar() or VAR() does, but has the added flexibility of the s parameter. I'm working it into my own workflow, so it's a bit optimized, but if anyone needs it, comment and I'll send it over in a more workable form.</p>
"
"0.212076166259788","0.213821992222448","118215","<p>This is my first post on StackExchange, but I have been using it as a resource for quite a while, I will do my best to use the appropriate format and make the appropriate edits. Also, this is a multi-part question. I wasn't sure if I should split the question into several different posts or just one. Since the questions are all from one section in the same text I thought it would be more relevant to post as one question.</p>

<p>I am researching habitat use of a large mammal species for a Master's Thesis. The goal of this project is to provide forest managers (who are most likely not statisticians) with a practical framework to assess the quality of habitat on the lands they manage in regard to this species. This animal is relatively elusive, a habitat specialist, and usually located in remote areas. Relatively few studies have been carried out regarding the distribution of the species, especially seasonally.  Several animals were fitted with GPS collars for a period of one year. One hundred locations (50 summer and 50 winter) were randomly selected from each animal's GPS collar data. In addition, 50 points were randomly generated within each animal's home range to serve as ""available"" or ""pseudo-absence"" locations. The locations from the GPS collars are coded a 1 and the randomly selected available locations are coded as 0.</p>

<p>For each location, several habitat variables were sampled in the field (tree diameters, horizontal cover, coarse woody debris, etc) and several were sampled remotely through GIS (elevation, distance to road, ruggedness, etc). The variables are mostly continuous except for 1 categorical variable that has 7 levels.</p>

<p>My goal is to use regression modelling to build resource selection functions (RSF) to model the relative probability of use of resource units. I would like to build a seasonal (winter and summer) RSF for the population of animals (design type I) as well as each individual animal (design type III).</p>

<p>I am using R to perform the statistical analysis.</p>

<p>The <strong>primary text</strong> I have been using isâ€¦</p>

<ul>
<li>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</li>
</ul>

<p>The majority of the examples in Hosmer et al. use STATA, <strong>I have also been using the following 2 texts for reference with R</strong>.</p>

<ul>
<li>""Crawley, M. J. 2005. Statistics : an introduction using R. J. Wiley,
Chichester, West Sussex, England.""</li>
<li>""Plant, R. E. 2012. Spatial Data Analysis in Ecology and Agriculture 
Using R. CRC Press, London, GBR.""</li>
</ul>

<p>I am currently following the steps in <strong>Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates""</strong> and have a few questions about the process. I have outlined the first few steps in the text below to aid in my questions.</p>

<ol>
<li>Step 1: A univariable analysis of each independent variable (I used a
univariable logistic regression). Any variable whose univariable test
has a p-value of less than 0.25 should be included in the first
multivariable model.</li>
<li>Step 2: Fit a multivariable model containing all covariates
identified for inclusion at step 1 and to assess the importance of
each covariate using the p-value of its Wald statistic. Variables
that do not contribute at traditional levels of significance should
be eliminated and a new model fit. The newer, smaller model should be
compared to the old, larger model using the partial likelihood ratio
test.</li>
<li>Step 3: Compare the values of the estimated coefficients in the
smaller model to their respective values from the large model. Any
variable whose coefficient has changed markedly in magnitude should
be added back into the model as it is important in the sense of
providing a needed adjustment of the effect of the variables that
remain in the model. Cycle through steps 2 and 3 until it appears that all of the important variables are included in the model and those excluded are clinically and/or statistically unimportant. Hosmer et al. use the ""<em>delta-beta-hat-percent</em>""
as a measure of the change in magnitude of the coefficients. They
suggest a significant change as a <em>delta-beta-hat-percent</em> of >20%. Hosmer et al. define the <em>delta-beta-hat-percent</em> as 
$\Delta\hat{\beta}\%=100\frac{\hat{\theta}_{1}-\hat{\beta}_{1}}{\hat{\beta}_{1}}$.
Where $\hat{\theta}_{1}$ is the coefficient from the smaller model and $\hat{\beta}_{1}$ is the coefficient from the larger model.</li>
<li>Step 4: Add each variable not selected in Step 1 to the model
obtained at the end of step 3, one at a time, and check its
significance either by the Wald statistic p-value or the partial
likelihood ratio test if it is a categorical variable with more than
2 levels. This step is vital for identifying variables that, by
themselves, are not significantly related to the outcome but make an
important contribution in the presence of other variables. We refer
to the model at the end of Step 4 as the <em>preliminary main effects
model</em>.</li>
<li>Steps 5-7: I have not progressed to this point so I will leave these
steps out for now, or save them for a different question.</li>
</ol>

<p><strong>My questions:</strong> </p>

<ol>
<li>In step 2, what would be appropriate as a traditional level of
significance, a p-value of &lt;0.05 something larger like &lt;.25?</li>
<li>In step 2 again, I want to make sure the R code I have been using for the partial likelihood test is correct and I want to make sure I am interpreting the results correctly. Here is what I have been doing&hellip;<code>anova(smallmodel,largemodel,test='Chisq')</code> If the p-value is significant (&lt;0.05) I add the variable back to the model, if it is insignificant I proceed with deletion?</li>
<li>In step 3, I have a question regarding the <em>delta-beta-hat-percent</em> and when it is appropriate to add an excluded variable back to the model. For example, I exclude one variable from the model and it changes the $\Delta\hat{\beta}\%$ for a different variable by >20%. However, the variable with the >20% change in $\Delta\hat{\beta}\%$ seems to be insignificant and looks as if it will be excluded from the model in the next few cycles of Steps 2 and 3. How can I make a determination if both variables should be included or excluded from the model? Because I am proceeding by excluding 1 variable at a time by deleting the least significant variables first, I am hesitant to exclude a variable out of order.</li>
<li><p>Finally, I want to make sure the code I am using to calculate $\Delta\hat{\beta}\%$ is correct. I have been using the following code. If there is a package that will do this for me or a more simple way of doing it I am open to suggestions.  </p>

<p><code>100*((smallmodel$coef[2]-largemodel$coef[2])/largemodel$coef[2])</code></p></li>
</ol>
"
"0.105194378270435","0.112454054604034","118394","<p>I have a dataset in which individuals were assessed at two time points during the study on a cognitive test, as such I was wondering which statistical model would be more appropriate for my data, either linear regression or mixed effects models? </p>

<p>The average length of follow up for my data is 59 months with a standard deviation of 43.03 (range is 0.63-167 months) with 88 (33%) of people having data for only one time point. </p>

<p>For linear regression, the approach I was thinking utilising was taking the delta of the test score between the two time points and regressing that against time (months between test scores). </p>

<p>If I used mixed effects models, the main issue I have is how to handle individuals who have only wave of data? While I know mixed effects models are especially robust in regards to the analysis of unbalanced data, would 33% missingnes cause issues?</p>

<p>Just sample R code highlighting the output using either linear regression or mixed models.</p>

<pre><code>fm1 &lt;- lm(mmse_difference ~ mmse_months_between*ORgrs_apoe, data = dat.wide)
summary(fm1)

Call:
lm(formula = mmse_difference ~ mmse_months_between * ORgrs_apoe, 
    data = newdat)

Residuals:
    Min      1Q  Median      3Q     Max 
-20.960  -3.957   1.854   5.200  12.550 

Coefficients:
                               Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)                    -2.74185    2.20667  -1.243    0.216
mmse_months_between            -0.01768    0.03051  -0.579    0.563
ORgrs_apoe                      0.35163    1.17782   0.299    0.766
mmse_months_between:ORgrs_apoe -0.01973    0.01748  -1.129    0.261

Residual standard error: 7.3 on 170 degrees of freedom
  (88 observations deleted due to missingness)
Multiple R-squared:  0.08481,   Adjusted R-squared:  0.06866 
F-statistic: 5.251 on 3 and 170 DF,  p-value: 0.001725
Num. obs. 174

fm2 &lt;- lme(mmse ~ mmse_months*ORgrs_apoe, random = ~mmse_months|patientid, data = dat.long, method = ""ML"", na.action = na.exclude)
summary(fm2)
Linear mixed-effects model fit by maximum likelihood
 Data: dat.long 
       AIC      BIC    logLik
  2797.467 2829.537 -1390.733

Random effects:
 Formula: ~mmse_months | patientid
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr  
(Intercept) 7.2972822 (Intr)
mmse_months 0.1132399 0.85  
Residual    2.9431616       

Fixed effects: mmse ~ mmse_months * ORgrs_apoe 
                           Value Std.Error  DF   t-value p-value
(Intercept)            24.635821 1.0959420 231 22.479130  0.0000
mmse_months            -0.069918 0.0223198 172 -3.132544  0.0020
ORgrs_apoe             -1.283348 0.6062892 231 -2.116726  0.0354
mmse_months:ORgrs_apoe -0.024952 0.0130561 172 -1.911103  0.0577
 Correlation: 
                       (Intr) mms_mn ORgrs_
mmse_months             0.438              
ORgrs_apoe             -0.882 -0.377       
mmse_months:ORgrs_apoe -0.357 -0.891  0.397

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-3.48643949 -0.31734164  0.07636708  0.26575764  2.49901891 

Number of Observations: 407
Number of Groups: 233 
</code></pre>

<p>Thanks.     </p>
"
"0.131810206044178","0.117060946087788","120549","<p>It is a basic question but I could not find clear answer on my reading. I am trying to find independent predictors of Infant.Mortality in data frame 'swiss' in R. </p>

<pre><code>&gt; head(swiss)
             Fertility Agriculture Examination Education Catholic Infant.Mortality
Courtelary        80.2        17.0          15        12     9.96             22.2
Delemont          83.1        45.1           6         9    84.84             22.2
Franches-Mnt      92.5        39.7           5         5    93.40             20.2
Moutier           85.8        36.5          12         7    33.77             20.3
Neuveville        76.9        43.5          17        15     5.16             20.6
Porrentruy        76.1        35.3           9         7    90.57             26.6
</code></pre>

<p>Following are the results using lm and I find only Fertility to be a significant predictor: </p>

<pre><code>&gt; fit = lm(Infant.Mortality~., data=swiss)
&gt; summary(fit)

Call:
lm(formula = Infant.Mortality ~ ., data = swiss)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.2512 -1.2860  0.1821  1.6914  6.0937 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  8.667e+00  5.435e+00   1.595  0.11850
Fertility    1.510e-01  5.351e-02   2.822  0.00734    #  &lt;&lt;&lt;&lt; NOTE P VALUE HERE
Agriculture -1.175e-02  2.812e-02  -0.418  0.67827
Examination  3.695e-02  9.607e-02   0.385  0.70250
Education    6.099e-02  8.484e-02   0.719  0.47631
Catholic     6.711e-05  1.454e-02   0.005  0.99634

Residual standard error: 2.683 on 41 degrees of freedom
Multiple R-squared:  0.2439,    Adjusted R-squared:  0.1517 
F-statistic: 2.645 on 5 and 41 DF,  p-value: 0.03665
</code></pre>

<p>Following are the graphs:</p>

<pre><code>plot(fit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/lopHb.png"" alt=""enter image description here""></p>

<p>On performing stepwise regression, following are the results: </p>

<pre><code>&gt; step &lt;- stepAIC(fit, direction=""both""); 
Start:  AIC=98.34
Infant.Mortality ~ Fertility + Agriculture + Examination + Education + 
    Catholic

              Df Sum of Sq    RSS     AIC
- Catholic     1     0.000 295.07  96.341
- Examination  1     1.065 296.13  96.511
- Agriculture  1     1.256 296.32  96.541
- Education    1     3.719 298.79  96.930
&lt;none&gt;                     295.07  98.341
- Fertility    1    57.295 352.36 104.682

Step:  AIC=96.34
Infant.Mortality ~ Fertility + Agriculture + Examination + Education

              Df Sum of Sq    RSS     AIC
- Examination  1     1.320 296.39  94.551
- Agriculture  1     1.395 296.46  94.563
- Education    1     5.774 300.84  95.252
&lt;none&gt;                     295.07  96.341
+ Catholic     1     0.000 295.07  98.341
- Fertility    1    72.609 367.68 104.681

Step:  AIC=94.55
Infant.Mortality ~ Fertility + Agriculture + Education

              Df Sum of Sq    RSS     AIC
- Agriculture  1     4.250 300.64  93.220
- Education    1     6.875 303.26  93.629
&lt;none&gt;                     296.39  94.551
+ Examination  1     1.320 295.07  96.341
+ Catholic     1     0.255 296.13  96.511
- Fertility    1    79.804 376.19 103.758

Step:  AIC=93.22
Infant.Mortality ~ Fertility + Education

              Df Sum of Sq    RSS     AIC
&lt;none&gt;                     300.64  93.220
- Education    1    21.902 322.54  94.525
+ Agriculture  1     4.250 296.39  94.551
+ Examination  1     4.175 296.46  94.563
+ Catholic     1     2.318 298.32  94.857
- Fertility    1    85.769 386.41 103.017
&gt; 
&gt; 
&gt; step$anova
Stepwise Model Path 
Analysis of Deviance Table

Initial Model:
Infant.Mortality ~ Fertility + Agriculture + Examination + Education + 
    Catholic

Final Model:
Infant.Mortality ~ Fertility + Education


           Step Df     Deviance Resid. Df Resid. Dev      AIC
1                                      41   295.0662 98.34145
2    - Catholic  1 0.0001533995        42   295.0663 96.34147
3 - Examination  1 1.3199421028        43   296.3863 94.55125
4 - Agriculture  1 4.2499886025        44   300.6363 93.22041
&gt; 
&gt; 
</code></pre>

<p>Summary shows Education also has trend towards significant association: </p>

<pre><code>summary(step)

Call:
lm(formula = Infant.Mortality ~ Fertility + Education, data = swiss)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.6927 -1.4049  0.2218  1.7751  6.1685 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  8.63758    3.33524   2.590 0.012973
Fertility    0.14615    0.04125   3.543 0.000951
Education    0.09595    0.05359   1.790 0.080273

Residual standard error: 2.614 on 44 degrees of freedom
Multiple R-squared:  0.2296,    Adjusted R-squared:  0.1946 
F-statistic: 6.558 on 2 and 44 DF,  p-value: 0.003215
</code></pre>

<p>What do I conclude? Is Education an important predictor or not?</p>

<p>Also, do the graphs using plot(fit) add any significant information?</p>

<p>Thanks for your help.</p>

<hr>

<p>Edit: 
I ran shapiro test on all columns and found 2 are not normally distributed: </p>

<pre><code>Fertility : P= 0.3449466 (Normally distributed) 
Agriculture : P= 0.1930223 (Normally distributed) 
Examination : P= 0.2562701 (Normally distributed) 
Education : P= 1.31202e-07 (--- NOT Normally distributed! ---) 
Catholic : P= 1.20461e-07 (--- NOT Normally distributed! ---) 
Infant.Mortality : P= 0.4978056 (Normally distributed) 
</code></pre>

<p>Does that make a difference? </p>
"
"0.0774209661138764","0.0763974860547543","121037","<p>I have a dataset with approximately <strong>4000 rows and 150 columns</strong>. I want to predict the values of a single column (= target).</p>

<p>The data is on cities (demography, social, economic, ... indicators). A lot of these are highly correlated, so I want to do a PCA - Principal Component Analysis. </p>

<p>The problem is, that <strong>~40% of the values are missing</strong>.</p>

<p>My current approach is:
Remove target indicator and do <strong>PCA with mean/median imputation of missing values</strong>.
Select x principal components (PC).
Append target indicator to these PC.
Use PC as predictors for the target variable and try common regression techniques, e.g. knn, linear regression, random forest etc.</p>

<p>With this approach, I'm getting quite good results. My metric is RMSE% - root mean squared relative prediction error. I tried this for all columns in the dataset, the RMSE% is between 0.5% and 8% (depending on the column). These errors are for values I actually know, NOT imputed values.</p>

<p>So, here's my problem: <strong>I'm not sure how much my data is distorted by replacing the missing values with the column mean/median</strong>. Is there any other way of imputing the missing values with minimal effect on the PCA results?</p>
"
"0.0446990156267674","0.0441081091391231","121131","<p>I'm new to R and I've been searching for a while for a function which can reduce the number of explanatory variables in my lda function (linear discriminant analysis).</p>

<p>Basically, I've loaded the dataset and ran the lda function on my binomial dependent variable explained by 30 independent variables. (received a warning that the independent variables are collinear).</p>

<p>My professor has shown us stepwise feature selection (leaps package, regsubsets function) in a regression framework, but these codes aren't compatible for LDA/QDA.</p>

<p>Thanks in advance,
Marvin</p>
"
"0.047410465593076","0.0623782861551805","121315","<p>I am very new to statistical analysis and R. Recently I worked on a simple linear regression model to predict values. For example: consider the below data set</p>

<pre><code>Col A    Col B
1         10
2         16
3         67
4         ?
5         ?
</code></pre>

<p>For such data i was able to predict the values using <code>lm</code> and <code>predict</code> functions in R. </p>

<p>Now, suppose I am given the below data set:</p>

<pre><code>inventory  jan-sales   feb-sales   mar-sales   apr-sales  may-sales
12         4           0           2           ?          ?
190        54          67          89          ?          ? 
123        67          22          11          ?          ? 
654       167          100        300          ?          ?
789       567          10         80           ?          ? 
543       223          221        0            ?          ?
</code></pre>

<p>In this data set, each line has total available units of a particular item and how many of those items were sold on each month. Now based on this data if I am asked to predict how many will be sold in the months of April and May, how do I use R and Linear regression or multiple linear regression to predict sales for April and May?</p>
"
"NaN","NaN","122640","<p>I have the following table:</p>

<pre><code>d &lt;- read.table(textConnection(""y x1 x2 x3 x4
                  40 5 10 8 2 
                  60 9 19 9 9 
                  75 18 27 19 5 
                  80 15 36 25 20 
                  115 25 45 39 30 
                  120 35 48 40 19""), header=TRUE)
</code></pre>

<p>I did linear regression analysis:</p>

<pre><code>summary(lm(y~., data=d))

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 30.50427    7.24960   4.208    0.149
x1           2.46856    1.13807   2.169    0.275
x2          -0.04313    0.91344  -0.047    0.970
x3          -0.45983    1.01308  -0.454    0.729
x4           1.35522    0.82608   1.641    0.348

Residual standard error: 4.855 on 1 degrees of freedom
Multiple R-squared:  0.9951,    Adjusted R-squared:  0.9756 
F-statistic: 51.02 on 4 and 1 DF,  p-value: 0.1046
</code></pre>

<p>How can I figure out if I need to do nonlinear-regression analysis or not?</p>
"
"0.0547448901451359","0.0540211804549215","122875","<p>I am doing multiple linear regression analysis in R and I got the following summary:</p>

<pre><code>Call:
lm(formula = Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + 
    X10 + X11 + X12 + X13)

Residuals:
ALL 20 residuals are 0: no residual degrees of freedom!

Coefficients: (151 not defined because of singularities)
                   Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)       -15462.94         NA      NA       NA
X1                    63.31         NA      NA       NA
X2                  1363.12         NA      NA       NA
X31,266,019,376     5518.54         NA      NA       NA
X31,483,786,035    29894.78         NA      NA       NA
X31,619,000,000    39338.01         NA      NA       NA
X31,687,000,000    65308.07         NA      NA       NA
X31,720,264,324    35548.79         NA      NA       NA
X31,749,000,000    31693.75         NA      NA       NA

.......................................................

X13692,062,808           NA         NA      NA       NA
X13693,179,733           NA         NA      NA       NA
X13724,817,439           NA         NA      NA       NA

Residual standard error: NaN on 0 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:    NaN 
F-statistic:   NaN on 19 and 0 DF,  p-value: NA
</code></pre>

<p>Could anybody explain what does that result mean? And what should I do?</p>

<p>Thank you!</p>

<p>Another question.  What should I do if the following error appears: </p>

<pre><code>Error in step(model) : 
number of rows in use has changed: remove missing values?
</code></pre>
"
"0.0223495078133837","0.0441081091391231","122935","<p>I have a design involving 1 between-subjects categorical factor, 1 between-subjects continuous factor and 2 within-subjects categorical factors.</p>

<p>This is theoretically a case of multiple regression, but as in this question <a href=""http://stats.stackexchange.com/questions/48455/anova-or-regression-1-continuous-factor-1-categorical-factor-with-continuous"">here</a>, I was able to use <code>lm()</code> in <code>R</code> to specify my model (actually, using the <code>aov.car()</code> function in the <code>afex</code> package).</p>

<p>This means that my output was in the form of an ANOVA table, with F and p-values. Can I report my analysis as though it was an ANOVA, or should I find a way to force the output of regression coefficients and report the analysis as though it were a multiple regression? I know that I shouldn't use an ANCOVA in this case, because my continuous predictor variable is certainly not a nuisance variable!</p>
"
"0.0547448901451359","0.0540211804549215","123172","<p>I am making a table from results of an analysis using generalised linear model which involves detecting association of a categorical predictor variable over multiple outcome variables. Of those multiple outcome variables, few are binary where I display the odds ratio for each category of the predictor (as we do in logistic regression); while few are continuous outcome variables, in which case I can display the beta estimate for each category of the predictor. My question is will it be ok if exponentiate the beta value  and express it as odds ratios. Can I do that?</p>
"
"0.0842852721654685","0.0935674292327708","124512","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures across the whole database with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 1-75
GCS - Glasgow Coma Scale = 3-15
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>There are two additional variables where the data is only available from 2004:</p>

<pre><code>Pupil reactivity (1 is brisk, 2 is sluggish and 3 is unreactive)
Pupil size (continuous variable in mm - 1-10)
</code></pre>

<p>Based on literature these are significant predictors of outcome. However, out of a series of 2140, I am missing 936. Secondly, the measure is not missing at random, having only been collected in recent years.</p>

<p>My questions are the following in order to address the year range 1994-2013:</p>

<p>1) My data is heavily skewed to later years; how can I adapt the logistic regression to reduce the effect of this when assessing the effect of the year of procedure on outcome?</p>

<p>2) Can I exclude pupil reactivity since it was not collected before 2004 in performing this analysis even if it is a strong predictor?</p>

<p>3) If I should include pupil reactivity, can a multivariate regression be built with the variables above with which to perform imputation to create data for 1994-2003 given 43% of the data is missing?</p>

<p>4) If not possible, could imputation be performed based on data since 2009 where ~15% is missing?</p>

<p>I perform all statistical analyses exclusively with R and would be grateful if you could add known packages/formulae to execute your suggestions.</p>
"
"0.0999500374687773","0.0986287303940589","125130","<p>I realize that a similar question to this has been asked, but it was not ultimately resolved. I have tried the suggestions posted to that question <a href=""https://stackoverflow.com/questions/23347467/is-there-any-way-to-fit-a-glm-so-that-all-levels-are-included-i-e-no-refer"">here</a>, but have had no success. I am using the following code:       </p>

<pre><code>allinfa4.exp = glm(survive ~ year + julianvisit + class + sitedist + roaddist
+ ngwdist, family = binomial(logexp(alldata$expos)), data=alldata)
summary(allinfa4.exp)

 Call:
glm(formula = survive ~ year + julianvisit + class + sitedist + 
roaddist + ngwdist, family = binomial(logexp(alldata$expos)), 
data = alldata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.6435   0.3477   0.4164   0.4960   0.9488  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  4.458e+00  7.117e-01   6.265 3.74e-10 ***
year2013     3.680e-01  1.862e-01   1.976  0.04819 *  
year2014     2.136e-02  1.802e-01   0.119  0.90564    
julianvisit -5.714e-03  3.890e-03  -1.469  0.14192    
classb       2.863e-02  2.194e-01   0.131  0.89615    
classc      -2.394e-01  2.277e-01  -1.051  0.29304    
classd      -1.868e-01  2.479e-01  -0.754  0.45109    
classe      -4.500e-01  2.076e-01  -2.167  0.03021 *  
classf      -5.728e-01  2.005e-01  -2.858  0.00427 ** 
classg      -8.495e-01  3.554e-01  -2.390  0.01684 *  
classh      -1.858e-01  2.224e-01  -0.835  0.40351    
classi      -3.196e-01  4.417e-01  -0.724  0.46932    
sitedist    -2.607e-04  5.043e-04  -0.517  0.60520    
roaddist     6.768e-05  4.311e-04   0.157  0.87525    
ngwdist     -5.751e-05  9.456e-05  -0.608  0.54306
</code></pre>

<p>The main thing to note here is that I have two categorical variables, <code>year</code> and <code>class</code>, and R has combined the first level of each (2012 and class a) into a reference level intercept term. Not only do I need to know the intercept term for these levels individually, but I also need to know the base intercept terms itself (beta0), just as SAS produces. </p>

<p>I have tried changing the contrasts and deviation coding to accomplish this, but although doing so allows me to extract different levels, it changes the way they are calculated and still does not produce beta0. I've also tried adding +0 and -1, but this also does not provide what I need. Is what I'm trying to do simply impossible in R? It may seem like a strange request, but beta0 is necessary to convert the results of logistic exposure (special kind of logistic regression for nest-survival data) to daily survival rates. Any help would be hugely appreciated. Thanks!</p>

<p>Here is an example of SAS output I want to emulate (taken from a similar analysis done by my lab mate) :</p>

<pre><code>Parameter Estimates
Parameter   Estimate    Standard Error  DF  t Value Pr &gt; |t|              
beta0       7.8404      2.8479          19  2.75    0.0127  
NT         -3.8786      1.8831          19  -2.06   0.0534  
bgdensity  -0.1127      0.1614          19  -0.70   0.4935  
nwh         1.3466      1.4625          19  0.92    0.3687      
NRD        -2.6981      1.9496          19  -1.38   0.1824      
NAGW       -0.4898      2.2518          19  -0.22   0.8301      
</code></pre>
"
"0.0446990156267674","0.0441081091391231","125275","<p>I have a dataset which is having multiple columns where some column has string values with comma seperated</p>

<pre><code>row.names   MOVIE   Year    Released    Genre
1   2   Jilla   2014    10/01/14    Action, Drama, Thriller
2   3   Pursuit of Happyness    2005    16/07/05    Documentary
3   4   The Karate Kid  1984    22/06/84    Action, Drama, Family
4   5   Yes Man 2008    19/12/08    Comedy, Romance
5   6   The Butterfly Effect    2004    23/01/04    Sci-Fi, Thriller
6   7   Unknown 2011    18/02/11    Action, Mystery, Thriller
</code></pre>

<p>Now I have dataset which is having String value in a column but that is required for regression analysis.
Now I want to deal with this Genre Column which is extremely important for my analysis (regression).</p>

<p>So how can I deal with such column where I have string values.</p>

<p>I thought of finding all unique String values in the column and assign a number to each one of them.</p>

<pre><code>Sci-Fi - 1
Thriller - 2
Comedy - 3

Now if any genre says -
Sci-Fi,Comedy  - 13 
</code></pre>

<p>I will replace as above but I am not sure about this technique.</p>

<p>Is there any more proper mathematical approach I must follow</p>

<p>I am using both R and Pandas for doing the same.</p>
"
"0.141511233180756","0.139640500579857","125414","<p>I have two variables:</p>

<ul>
<li>urban areas</li>
<li>protected areas.</li>
</ul>

<p>My observations are urban areas and protected areas in each year. But these observations are the cumulative ones, so observations in each variable have auto-correlation.</p>

<p>Can I use the general correlation such as yielded by <code>cor()</code> in R to measure the correlation between these two variables? If not, which indicator or method can I use?</p>

<p>I have the scatter plot: the horizontal variable is urban area in a specific year, and the vertical variable is another one in that specific year. And these two variables are increasing as years pass. I can see these two variables present a linear relationship. And my purpose is to find a indicator which can measure this linear relationship. I actually have tested the linear regression: the urban area as independent variable, the protected area as dependent variable, and I put 14 pairs of each year into the regression model, and the coefficients can pass the t-test, and model can pass the t-test, the $R^2$ can reach more than 0.9. </p>

<p>I want to research the relationship between urban development and protected area development. And the scatter plot below is urban and protected area pairs on global scale for 1950-2014 with 5 year intervals (except for 2010 and 2014).</p>

<p>I want to test two questions: First, are these two areas (urban and protected areas) both increasing over the research period? Second, does urbanization (here I mean the development of urban area) cause the development of protected areas?</p>

<p>I want to use some correlation analysis to solve the first question, such as correlation, linear regression or MIC value. However, because my data are time series, I'm not sure it can be used in the calculation of correlation? So I raise this question. In addition, I don't know other methods that could be used to measure strength of linear relationship between two time series. </p>

<p>And for the second question, I want to use Granger causality test to test the causality relationship between these two areas statistically. I know the result of Granger causality can't be sure to determine the causality relationship. And in my opinion, the reasons to improve the development of urban areas or protected areas are both complex, and some of them may be shared. At this level, I simply want to test the causality relationship between these two variables.</p>

<p><img src=""http://i.stack.imgur.com/tHlOm.jpg"" alt=""scatter plot between urban and farm land, the point is a variable pair in a specific year""></p>
"
"0.0446990156267674","0.0441081091391231","125455","<p>I want to meta-analyze the interaction effect of a 2x2 ANOVA.</p>

<p>(I am <em>not</em> talking about an interaction in the meta-regression, as in <a href=""http://stats.stackexchange.com/questions/71404/main-effects-and-interaction-in-multivariate-meta-analysis-network-meta-analysi"">this question</a> but about an interaction as the focal effect that should be meta-analytically summarized).</p>

<p><strong>What is the best way to code the interaction effect size for a subsequent meta-analysis?</strong>
(preferably in the <code>metafor</code> package)</p>
"
"0.070675349274022","0.0557928352651671","125465","<p>I'm searching for a built-in function in R for calculating the required sample size (given certain power, alpha,..) of a mixed ANOVA (2 between, 1 within variable, 2x2x2 design). Does this function exist? I didn't find it in the following packages:</p>

<p>â€¢pwr is the oldest power-analysis library; some introductory info can be found on Quick-R</p>

<p>â€¢PoweR: Computation of power and level tables for hypothesis tests</p>

<p>â€¢Power2Stage: Power and Sample size distribution of 2-stage BE studies via simulations</p>

<p>â€¢powerAnalysis: Power analysis in experimental design</p>

<p>â€¢powerGWASinteraction: Power Calculations for Interactions for GWAS</p>

<p>â€¢powerMediation: Power/Sample size calculation for mediation analysis, simple linear 
regression, logistic regression, or longitudinal study</p>

<p>â€¢powerpkg: Power analyses for the affected sib pair and the TDT design</p>

<p>â€¢powerSurvEpi: Power and sample size calculation for survival analysis of epidemiological studies</p>

<p>â€¢PowerTOST: Power and Sample size based on two one-sided t-tests (TOST) for (bio)equivalence studies</p>

<p>â€¢longpower: Power and sample size for linear model of longitudinal data</p>

<p>Thanks!</p>
"
"0.105194378270435","0.112454054604034","125787","<p>I would like to learn what is the correct way to approach analysis of this data. I have done some reading on the subject, but I still feel uncertain. Perhaps many approaches are valid, but simply  that some are more conservative than others?</p>

<p>My study: </p>

<p>I have 5 grasslands, and in each grassland I have 30 spiders. For each spider I have an estimate of what proportion of herbivores it consumes ""Diet"" (so 5 x 30, n = 150). For each grassland I also have an estimate of the overall biomass of herbivores that exist there ""Biomass"". Thus I have 5 values of ""Biomass"" (one for each grassland) and 150 of ""Diet"" (30 spiders per grassland). Both Diet and Biomass are continous variables. </p>

<p>I would like to run an anlysis that tests how Diet changes across Biomass and derive a slope value, thus keeping Biomass as a continous variable:</p>

<p>Diet ~ Biomass</p>

<p>As I understand it, if I use raw data for Diet (n=150) then using anova is more approrpiate, and grassland becomes a factor with 5 levels.</p>

<p>Or I could run it as a linear regression and thus keep Biomass as a continuous variable and derive a slope value. However, as a linear regression, should I use the raw data (n=150) or mean values (so 5 means - one for each grassland based on 30 samples). Which of the 2 linear regression approaches is correct? (means or raw data). </p>

<p>While I am familiar with the notion that both anova and regression have the same underlying mathematics and are now regarded as general linear modelling, I still don't know how this affects the data that I should be using when running a linear model of the form:  Diet ~ Biomass</p>

<p>Using raw data seems better because it captures the variability in the dataset, but if i use it with Biomass as a continous variable to get a slope value (i.e regression analysis) I am concerned that it inflates the degrees of freedom (df=1,149) and is psuedo-replicated, so inaccurately increases my chances of a significant result? Therefore, is it incorrect to model the raw data (n=150) against only 5 values of ""Biomass"" in a linear form (and not as factors as required in an anova)?</p>
"
"0.0446990156267674","0.0441081091391231","126510","<p>How do we do two-way ANOVA (one observation per mean), as testing H_A in Section 8.5 in Seber and Lee's Linear Regression Analysis, in R?
Note that the linear model for this case doesn't have interaction between the row and column factors.</p>

<p>For example, I want to test in the following 3 x 2 table, if the mean of each row is the same. </p>

<p>5 | 4<br>
7 | 6<br>
4 | 7  </p>

<p>Note that I used <code>lm</code> for one-way ANOVA, but couldn't find out which function and arguments to do two-way ANOVA (one observation per mean). I am not trying to implement it in R.</p>

<p>Thanks.</p>
"
"0.0446990156267674","0.0441081091391231","126976","<p>I am playing a data without any background information. First, I try multiple linear regression. The model fits well, since the $r^2$ is larger than 90%. I deleted several variables by AIC. The fits improves a little bit. I am interesting what are possible directions that I should look into, if the model fits well at the very beginning.</p>

<p>The target of the analysis is find the model that make the most accurate prediction.</p>

<p>What I can come up with:</p>

<ol>
<li><p>Try other models like regression tree, random forest, SVM or whatever data mining models. The prediction may or may not become better.</p></li>
<li><p>Perform cross validation.</p></li>
<li><p>Check overfitting.</p></li>
<li><p>Diagnostic residual.</p></li>
</ol>
"
"0.104828483672192","0.103442685121077","126990","<p>I would like to conduct a meta-analysis in the context where I have studies available that measure a continuos variable at multiple time points (0, 1, 2, 3, 4, 5). Time 0 represents the baseline where values are at 100%. Right afterwards there is an intervention and the effect of the intervention is measured over time (114% represents a 14% change relative to baseline). Also I have given two different groups that received different interventions.</p>

<p>Please consider the following dummy data set:</p>

<pre><code>library(ggplot2)
library(metafor)
library(dplyr)
n &lt;- 10
a &lt;- c(rnorm(n,100,0), rnorm(n, 110,2), rnorm(n,130,2), rnorm(n,135,2), rnorm(n,130,2), rnorm(n,125,2))
b &lt;- c(rnorm(n,100,0), rnorm(n,107,2), rnorm(n,122,2), rnorm(n,128,2), rnorm(n,122,2), rnorm(n,125,2))
sd &lt;- rnorm(n,10,1)
my_dat &lt;- data.frame(mean=c(a, b), sd=rep(sd,12), time=rep(c(rep(0,n), rep(1,n), rep(2,n), rep(3,n), rep(4,n), rep(5,n)),2), group=c(rep(""A"", 60), rep(""B"",60)), n=rep(n,120))
my_dat$study &lt;- 1:10
p &lt;- ggplot(aes(y=mean, x=time, colour=group), data=my_dat)
p + geom_jitter() + geom_smooth() + ylab(""% relative to baseline"") + xlab(""time"") 
</code></pre>

<p><img src=""http://i.stack.imgur.com/Lx6TY.png"" alt=""raw data example""></p>

<p>I would like to :</p>

<p>1) investigate the main effect of time (as well as post-hoc tests) for each group individually using the metafor package.</p>

<p>2) investigate the main effect of group (as well as post-hoc tests) for each point in time using the metafor package.</p>

<p>3) investigate group-time interactions.</p>

<p>Thus I rearrange the data and calculate hegdes g relative to baseline t0:</p>

<pre><code>t0_dat &lt;- summarise(group_by(my_dat[my_dat$time==0,], study, group), t0_mean=mean(mean), t0_sd=mean(sd))
my_dat &lt;- merge(my_dat, t0_dat, by=c(""study"", ""group""), all.x=T)
my_dat &lt;- escalc(m1i=mean, m2i=t0_mean, sd1i=sd, sd2i=t0_sd, n1i=n, n2i=n, measure=""SMD"", data=my_dat, append=T)
p &lt;- ggplot(aes(y=yi, x=time, xmin=yi-vi, xmax=yi+vi, colour=group), data=my_dat)
p + geom_point() + geom_smooth() + ylab(""hedges g"") + xlab(""time"") + xlim(c(0,5)) + ylim(c(0,5))
</code></pre>

<p><img src=""http://i.stack.imgur.com/Gf11j.png"" alt=""enter image description here""></p>

<p>Finally I can run the meta-analysis:</p>

<pre><code>m1 &lt;- rma(yi,vi, data=my_dat, mods=~time*group)
summary(m1)
</code></pre>

<p>This indicates a sig. effect of time, a sig. effect of group but no interaction:
Model Results:</p>

<pre><code>         estimate      se     zval    pval    ci.lb    ci.ub     
intrcpt        1.4500  0.2159   6.7158  &lt;.0001   1.0269   1.8732  ***
time           0.3294  0.0667   4.9363  &lt;.0001   0.1986   0.4602  ***
groupB        -0.6808  0.2994  -2.2738  0.0230  -1.2676  -0.0940    *
time:groupB    0.0802  0.0932   0.8609  0.3893  -0.1024   0.2628     

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Is this an valid approach?
Would it be appropriate to instead of converting to effect size (hedges g) to use the percentage values (as extracted from the papers) and log-transform them as suggested <a href=""http://stats.stackexchange.com/questions/34057/estimating-percentages-as-the-dependent-variable-in-regression"">in this question</a>, <a href=""http://stackoverflow.com/questions/9958722/r-variable-selection-for-multiple-regression-w-percentage-dependent-variable"">in this question</a> or in the comments below?
Hints to papers that conducted comparable analysis are more then welcome!</p>
"
"0.0836242010007091","0.0707303281615848","127536","<p>Sampling weights, the inverse probability of a unit's selection into the sample, and other more complex and adjusted weights are very often used in the social sciences. There is statistical software that allows weighting of observations/cases, like the <code>hclust</code> function from the <code>R</code>-package <code>cluster</code>. </p>

<p>In regression analysis, there is an ongoing debate when the usage of observation weights is appropriate (see e.g. Winship/Radbill 1994). I could not find anything concerning observation weights in textbooks about cluster analysis, if weighting is discussed, it is mostly about variable weighting. One exemption is the manual of the <code>R</code>-package <code>WeightedCluster</code>, which discusses observation weighting in more detail. The documentation of the <code>cluster</code> package is not very helpful, as it only shows a trivial example using the weighting option <code>hclust(..., members=""..."")</code> where the number or weight of cases is untouched.</p>

<ol>
<li>Therefore, I am looking for references and recommendations with observation/case weighting in cluster analysis, especially hierarchical cluster analysis. </li>
<li>As I could not find the actual formula for the <code>hclust(..., members=""..."")</code> function : Which parameters changes in the hierarchical cluster algorithm if one uses observation weights? How does that affect the algorithm?</li>
</ol>

<p>In order to get an idea of the difference between clustering with and without case weights, here is an example using weights from survey data and the R-code:
<img src=""http://i.stack.imgur.com/BYiLY.png"" alt=""Reweighting of clustering by using membership""></p>

<pre><code>require(survey)
data(api)
whc &lt;- hclust(dist(apiclus2[, c(""pct.resp"", ""meals"")]), method=""ward.D2"", 
              members=apiclus2$pw)
uwhc &lt;- hclust(dist(apiclus2[, c(""pct.resp"", ""meals"")]), method=""ward.D2"")
opar &lt;- par(mfrow = c(1, 2))
plot(whc,  labels = FALSE, hang = -1, main = ""Weighted survey data"")
plot(uwhc, labels = FALSE, hang = -1, main = ""Unweighted survey data"")
</code></pre>

<h3>References</h3>

<ul>
<li>Studer, M., 2013: WeightedCluster Library Manual. A practical guide to creating typologies of trajectories in the social sciences with R. LIVES Working Papers 24. Lausanne.</li>
<li>Winship, C. &amp; L. Radbill, 1994: Sampling Weights and Regression Analysis. Sociological Methods &amp; Research 23: 230â€“257.</li>
</ul>
"
"0.0446990156267674","0.0441081091391231","128653","<p>I'd like to do a regression analysis with interactions, my data has two levels (school classes and pupils). My variables are: Predictor = dummy variable on Level 1, dependent Variable = metric on level 1, moderator variable = metric, Level 2</p>

<p>I used the following command, but I am not sure if that right:</p>

<p><code>model &lt;- lmer(DV ~ 1 + IV1 + IV2 + IV1 * IV2 + (1|cluster), daten)</code></p>

<p>Can you tell me if that's correct?</p>
"
"0.0316069770620507","0.0311891430775903","129093","<p>I am new to statistics and am developing an interest in learning regression analysis.
To be more precise, can you point me to some online resources where i can find real world data sets and regression models so that I can practice and check if my thought of process is on the right track.</p>

<p>P.S. I am currently using <strong>R</strong> to learn modeling</p>
"
"0.0876619818920292","0.103803742711416","129739","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as <code>Outcome30</code> measure). Other measures across the whole database with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Yeardecimal - Date of procedure = 1994.0-2013.99
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
ISS - Injury Severity Score = 1-75
Age - Age of patient = 16.0-101.5
GCS - Glasgow Coma Scale = 3-15
Sex - Gender of patient = Male or Female
rcteyemi - Pupil reactivity (1 = neither, 2 = one, 3 = both)
neuroFirst2 - Location of admission (Neurosurgical unit or not)
Other - other traums (0 - No, 1 - Yes)
othopYN - Other operation required
LOS - Length of stay in days
LOSCC - Length of stay in critical care in days 
</code></pre>

<p>When I conduct univariate analysis of the variables, I have conducted a logistic regression for each continuous variable. I am unable to model Yeardecimal however, with the following result:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ Yeardecimal, data = ASDH_Paper1.1)
singular information matrix in lrm.fit (rank= 1 ).  Offending variable(s):
Yeardecimal 
Error in lrm(formula = Survive ~ Yeardecimal, data = ASDH_Paper1.1) : 
  Unable to fit model using â€œlrm.fitâ€
</code></pre>

<p>However, the restricted cubic spline works:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ rcs(Yeardecimal), data = ASDH_Paper1.1)
&gt; 
&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ rcs(Yeardecimal), data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2     106.61    R2       0.027    C       0.578    
 0           1281    d.f.             4    g        0.319    Dxy     0.155    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       1.376    gamma   0.160    
max |deriv| 2e-08                          gp       0.057    tau-a   0.052    
                                           Brier    0.165                     

               Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept      -68.3035 45.8473 -1.49  0.1363  
Yeardecimal      0.0345  0.0229  1.51  0.1321  
Yeardecimal'     0.1071  0.0482  2.22  0.0262  
Yeardecimal''   -2.0008  0.6340 -3.16  0.0016  
Yeardecimal'''  11.3582  4.0002  2.84  0.0045  
</code></pre>

<p>Could anyone explain why this is? I am nervous about using a mode complicated model if I am unable to model with a simpler approach.</p>

<p>I am currently using restricted cubic splines to model Age, ISS and Yeardecimal. Would anyone recommend any alternative approach?</p>
"
"0.152477794432279","0.183375669078274","129761","<p>These multiple imputation results relate to data I have previously described and shown here - <a href=""http://stats.stackexchange.com/questions/129739/skewed-distributions-for-logistic-regression"">Skewed Distributions for Logistic Regression</a></p>

<p>Three variables I am using have missing data. Their names, descriptions and % missing are shown below.</p>

<pre><code>inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis) - 58% missing
GCS - Glasgow Coma Scale = 3-15 - 37% missing
rcteyemi - Pupil reactivity (1 = neither, 2 = one, 3 = both) - 56% missing
</code></pre>

<p>I have been using mutliple imputation to model the missing data above following advice in a previous post here - <a href=""http://stats.stackexchange.com/questions/127134/describing-results-from-logistic-regression-with-restricted-cubic-splines-using"">Describing Results from Logistic Regression with Restricted Cubic Splines Using rms in R</a></p>

<p>Given this is a longitudinal analysis, a key variable of importance is the year of the treatment so we can investigate how our patient management has improved. The variable in question, <code>Yeardecimal</code> is highly significant in univariate analysis:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)
&gt; 
&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2      91.47    R2       0.023    C       0.572    
 0           1281    d.f.             1    g        0.309    Dxy     0.143    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       1.362    gamma   0.146    
max |deriv| 3e-12                          gp       0.054    tau-a   0.048    
                                           Brier    0.165                     

             Coef   S.E.   Wald Z Pr(&gt;|Z|)
Intercept    0.8696 0.0530 16.42  &lt;0.0001 
Yeardecimalc 0.0551 0.0057  9.70  &lt;0.0001 
</code></pre>

<p>To deal with missingness, I used <code>aregImpute</code> and <code>fit.mult.impute</code> to conduct multiple imputation prior to multivariate logisic regression. When including Yeardecimal, the results were as follows:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS + Yeardecimalc, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS + Yeardecimalc, data = ASDH_Paper1.1, n.impute = 10, 
    nk = 4)

n: 5998     p: 12   Imputations: 10     nk: 4 

Number of NAs:
   Outcome30          Age          GCS        Other          ISS    inctoCran     rcteyemi   neuroFirst      neuroYN 
           0            0         2242            0            0         3500         3376            0            0 
   Mechanism          LOS Yeardecimalc 
           0            0            0 

             type d.f.
Outcome30       c    1
Age             s    3
GCS             s    3
Other           c    1
ISS             s    3
inctoCran       s    3
rcteyemi        l    1
neuroFirst      l    1
neuroYN         l    1
Mechanism       c    4
LOS             s    3
Yeardecimalc    s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.421     0.181     0.358 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)

&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1609.98    R2       0.365    C       0.836    
 0           1281    d.f.            25    g        1.584    Dxy     0.672    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.875    gamma   0.674    
max |deriv| 0.001                          gp       0.222    tau-a   0.226    
                                           Brier    0.121                     

                              Coef    S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     21.3339 67.4400  0.32  0.7517  
Age                           -0.0088  0.0132 -0.67  0.5052  
Age'                          -0.0294  0.0643 -0.46  0.6471  
Age''                         -0.0134  0.2479 -0.05  0.9570  
Age'''                         0.2588  0.3534  0.73  0.4639  
GCS                            0.1100  0.0145  7.61  &lt;0.0001 
Mechanism=Fall &gt; 2m           -0.0651  0.1162 -0.56  0.5754  
Mechanism=Other                0.2285  0.1338  1.71  0.0876  
Mechanism=RTC                  0.0449  0.1332  0.34  0.7360  
Mechanism=Shooting / Stabbing  2.1150  1.1142  1.90  0.0577  
ISS                           -0.1069  0.0318 -3.36  0.0008  
ISS'                          -0.0359  0.1306 -0.27  0.7835  
ISS''                          1.8296  1.9259  0.95  0.3421  
neuroFirst                    -0.3483  0.0973 -3.58  0.0003  
inctoCrand                     0.0001  0.0053  0.02  0.9872  
inctoCrand'                   -0.0745  0.3060 -0.24  0.8077  
inctoCrand''                   0.1696  0.5901  0.29  0.7738  
inctoCrand'''                 -0.1167  0.3150 -0.37  0.7110  
inctoCranYN                   -0.2814  0.6165 -0.46  0.6480  
Yeardecimalc                  -0.0101  0.0337 -0.30  0.7641  
Yeardecimalc'                  0.0386  0.0651  0.59  0.5536  
Yeardecimalc''                -0.7417  0.8210 -0.90  0.3663  
Yeardecimalc'''                7.0367  4.9344  1.43  0.1539  
Sex=Male                       0.0668  0.0891  0.75  0.4534  
Other=1                        0.3238  0.1611  2.01  0.0445  
rcteyemi                       1.1589  0.1050 11.04  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              83.07      4   &lt;.0001
  Nonlinear        5.97      3   0.1131
 GCS              57.89      1   &lt;.0001
 Mechanism         8.14      4   0.0867
 ISS              77.31      3   &lt;.0001
  Nonlinear       35.04      2   &lt;.0001
 neuroFirst       12.81      1   0.0003
 inctoCrand        2.32      4   0.6777
  Nonlinear        2.29      3   0.5149
 inctoCranYN       0.21      1   0.6480
 Yeardecimalc      4.19      4   0.3807
  Nonlinear        3.77      3   0.2874
 Sex               0.56      1   0.4534
 Other             4.04      1   0.0445
 rcteyemi        121.80      1   &lt;.0001
 TOTAL NONLINEAR  47.27     11   &lt;.0001
 TOTAL           679.09     25   &lt;.0001
&gt; 
</code></pre>

<p>Yeardecimal is no longer significant. However, if I exclude Yeardecimal from aregImpute only, I have the alternative result below:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS, data = ASDH_Paper1.1, n.impute = 10, nk = 4)

n: 5998     p: 11   Imputations: 10     nk: 4 

Number of NAs:
 Outcome30        Age        GCS      Other        ISS  inctoCran   rcteyemi neuroFirst    neuroYN  Mechanism        LOS 
         0          0       2242          0          0       3500       3376          0          0          0          0 

           type d.f.
Outcome30     c    1
Age           s    3
GCS           s    3
Other         c    1
ISS           s    3
inctoCran     s    3
rcteyemi      l    1
neuroFirst    l    1
neuroYN       l    1
Mechanism     c    4
LOS           s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.407     0.194     0.320 
&gt; 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)
&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1607.92    R2       0.364    C       0.834    
 0           1281    d.f.            25    g        1.578    Dxy     0.667    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.846    gamma   0.669    
max |deriv| 0.003                          gp       0.221    tau-a   0.224    
                                           Brier    0.120                     

                              Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     -55.6574 58.3464 -0.95  0.3401  
Age                            -0.0084  0.0128 -0.66  0.5105  
Age'                           -0.0335  0.0612 -0.55  0.5838  
Age''                           0.0050  0.2365  0.02  0.9830  
Age'''                          0.2321  0.3387  0.69  0.4930  
GCS                             0.1099  0.0124  8.88  &lt;0.0001 
Mechanism=Fall &gt; 2m            -0.0631  0.1138 -0.55  0.5793  
Mechanism=Other                 0.2354  0.1381  1.70  0.0883  
Mechanism=RTC                   0.0315  0.1319  0.24  0.8114  
Mechanism=Shooting / Stabbing   1.9297  1.0930  1.77  0.0775  
ISS                            -0.1012  0.0335 -3.02  0.0025  
ISS'                           -0.0599  0.1366 -0.44  0.6613  
ISS''                           2.1581  2.0120  1.07  0.2834  
neuroFirst                     -0.3753  0.0888 -4.23  &lt;0.0001 
inctoCrand                     -0.0007  0.0054 -0.13  0.9002  
inctoCrand'                    -0.0496  0.3116 -0.16  0.8734  
inctoCrand''                    0.1316  0.6021  0.22  0.8270  
inctoCrand'''                  -0.1078  0.3224 -0.33  0.7381  
inctoCranYN                    -0.1697  0.6172 -0.27  0.7834  
Yeardecimalc                    0.0281  0.0291  0.96  0.3349  
Yeardecimalc'                   0.0682  0.0600  1.14  0.2553  
Yeardecimalc''                 -1.4037  0.7685 -1.83  0.0678  
Yeardecimalc'''                10.2513  4.8156  2.13  0.0333  
Sex=Male                        0.0595  0.0890  0.67  0.5037  
Other=1                         0.3579  0.1641  2.18  0.0292  
rcteyemi                        1.1862  0.0799 14.85  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              78.39      4   &lt;.0001
  Nonlinear        6.23      3   0.1011
 GCS              78.86      1   &lt;.0001
 Mechanism         7.53      4   0.1104
 ISS              76.46      3   &lt;.0001
  Nonlinear       31.16      2   &lt;.0001
 neuroFirst       17.87      1   &lt;.0001
 inctoCrand        3.22      4   0.5214
  Nonlinear        3.19      3   0.3630
 inctoCranYN       0.08      1   0.7834
 Yeardecimalc     44.83      4   &lt;.0001
  Nonlinear        4.67      3   0.1979
 Sex               0.45      1   0.5037
 Other             4.76      1   0.0292
 rcteyemi        220.51      1   &lt;.0001
 TOTAL NONLINEAR  45.39     11   &lt;.0001
 TOTAL           715.22     25   &lt;.0001
&gt; 
</code></pre>

<p>Can anyone help me understand why the statistical results for Yeardecimal are so starkly different?</p>
"
"0.104828483672192","0.103442685121077","131152","<p>Let say I've ran this linear regression:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt + vs, mtcars)
</code></pre>

<p>I can use <code>anova()</code> to see the amount of variance in the dependent variable accounted for by the two predictors:</p>

<pre><code>anova(lm_mtcars)

Analysis of Variance Table

Response: mpg
          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
wt         1 847.73  847.73 109.7042 2.284e-11 ***
vs         1  54.23   54.23   7.0177   0.01293 *  
Residuals 29 224.09    7.73                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Lets say I now add a random intercept for <code>cyl</code>:</p>

<pre><code>library(lme4)
lmer_mtcars &lt;- lmer(mpg ~ wt + vs + (1 | cyl), mtcars)
summary(lmer_mtcars)

Linear mixed model fit by REML ['lmerMod']
Formula: mpg ~ wt + vs + (1 | cyl)
   Data: mtcars

REML criterion at convergence: 148.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.67088 -0.68589 -0.08363  0.48294  2.16959 

Random effects:
 Groups   Name        Variance Std.Dev.
 cyl      (Intercept) 3.624    1.904   
 Residual             6.784    2.605   
Number of obs: 32, groups:  cyl, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  31.4788     2.6007  12.104
wt           -3.8054     0.6989  -5.445
vs            1.9500     1.4315   1.362

Correlation of Fixed Effects:
   (Intr) wt    
wt -0.846       
vs -0.272  0.006
</code></pre>

<p>The variance accounted for by each fixed effect now drops because the random intercept for <code>cyl</code> is now accounting for some of the variance in <code>mpg</code>:</p>

<pre><code>anova(lmer_mtcars)

Analysis of Variance Table
   Df  Sum Sq Mean Sq F value
wt  1 201.707 201.707 29.7345
vs  1  12.587  12.587  1.8555
</code></pre>

<p>But in <code>lmer_mtcars</code>, how can I tell what proportion of the variance is being accounted for by <code>wt</code>, <code>vs</code> and the random intecept for <code>cyl</code>?</p>
"
"0.186989398001691","0.184517458813533","131312","<p>I have some R code (which I did not write) and which performs some state space analysis on some time-series. The data itself is shown as dots (scatter plot) and the Kalman filtered and smoothed state is the solid line.</p>

<p><img src=""http://i.stack.imgur.com/41jaI.png"" alt=""Plot""></p>

<p>My question is regarding the confidence intervals shown in this plot. I calculate <em>my own</em> confidence intervals using the standard method (my C# code is below)</p>

<pre><code>public static double ConfidenceInterval(
    IEnumerable&lt;double&gt; samples, double interval)
{
    Contract.Requires(interval &gt; 0 &amp;&amp; interval &lt; 1.0);

    double theta = (interval + 1.0) / 2;
    int sampleSize = samples.Count();
    double alpha = 1.0 - interval;
    double mean = samples.Mean();
    double sd = samples.StandardDeviation();

    var student = new StudentT(0, 1, samples.Count() - 1);
    double T = student.InverseCumulativeDistribution(theta);
    return T * (sd / Math.Sqrt(samples.Count()));
}
</code></pre>

<p>Now this will return a single interval (and it does it correctly) which I will add/subtract from each point on the series I have applied the calculation to to give me my confidence interval. But this is a constant and the R implementation seems to change over the time-series.</p>

<p>My question is why is <strong>the confidence interval changing for the R implementation? Should I be implementing my confidence levels/intervals differently?</strong></p>

<p>Thanks for your time.</p>

<hr>

<p>For reference the R code that produces this plot is below:</p>

<pre><code>install.packages('KFAS')
require(KFAS)

# Example of local level model for Nile series
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='BFGS',control=list(REPORT=1,trace=1))$model

# Can use different optimisation: 
# should be one of â€œNelder-Meadâ€, â€œBFGSâ€, â€œCGâ€, â€œL-BFGS-Bâ€, â€œSANNâ€, â€œBrentâ€
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='L-BFGS-B',control=list(REPORT=1,trace=1))$model

# Filtering and state smoothing
out&lt;-KFS(modelNile,filtering='state',smoothing='state')
out$model$H
out$model$Q
out

# Confidence and prediction intervals for the expected value and the observations.
# Note that predict uses original model object, not the output from KFS.
conf&lt;-predict(modelNile,interval='confidence')
pred&lt;-predict(modelNile,interval='prediction')
ts.plot(cbind(Nile,pred,conf[,-1]),col=c(1:2,3,3,4,4),
ylab='Predicted Annual flow', main='River Nile')
KFAS 13

# Missing observations, using same parameter estimates
y&lt;-Nile
y[c(21:40,61:80)]&lt;-NA
modelNile&lt;-SSModel(y~SSMtrend(1,Q=list(modelNile$Q)),H=modelNile$H)
out&lt;-KFS(modelNile,filtering='mean',smoothing='mean')

# Filtered and smoothed states
plot.ts(cbind(y,fitted(out,filtered=TRUE),fitted(out)), plot.type='single',
col=1:3, ylab='Predicted Annual flow', main='River Nile')

# Example of multivariate local level model with only one state
# Two series of average global temperature deviations for years 1880-1987
# See Shumway and Stoffer (2006), p. 327 for details
data(GlobalTemp)
model&lt;-SSModel(GlobalTemp~SSMtrend(1,Q=NA,type='common'),H=matrix(NA,2,2))

# Estimating the variance parameters
inits&lt;-chol(cov(GlobalTemp))[c(1,4,3)]
inits[1:2]&lt;-log(inits[1:2])
fit&lt;-fitSSM(inits=c(0.5*log(.1),inits),model=model,method='BFGS')
out&lt;-KFS(fit$model)
    ts.plot(cbind(model$y,coef(out)),col=1:3)
legend('bottomright',legend=c(colnames(GlobalTemp), 'Smoothed signal'), col=1:3, lty=1)

# Seatbelts data
## Not run:
model&lt;-SSModel(log(drivers)~SSMtrend(1,Q=list(NA))+
SSMseasonal(period=12,sea.type='trigonometric',Q=NA)+
log(PetrolPrice)+law,data=Seatbelts,H=NA)

# As trigonometric seasonal contains several disturbances which are all
# identically distributed, default behaviour of fitSSM is not enough,
# as we have constrained Q. We can either provide our own
# model updating function with fitSSM, or just use optim directly:
# option 1:
ownupdatefn&lt;-function(pars,model,...){
model$H[]&lt;-exp(pars[1])
    diag(model$Q[,,1])&lt;-exp(c(pars[2],rep(pars[3],11)))
model #for option 2, replace this with -logLik(model) and call optim directly
}
14 KFAS
fit&lt;-fitSSM(inits=log(c(var(log(Seatbelts[,'drivers'])),0.001,0.0001)),
model=model,updatefn=ownupdatefn,method='BFGS')
out&lt;-KFS(fit$model,smoothing=c('state','mean'))
    out
    ts.plot(cbind(out$model$y,fitted(out)),lty=1:2,col=1:2,
    main='Observations and smoothed signal with and without seasonal component')
    lines(signal(out,states=c(""regression"",""trend""))$signal,col=4,lty=1)
legend('bottomleft',
legend=c('Observations', 'Smoothed signal','Smoothed level'),
col=c(1,2,4), lty=c(1,2,1))

# Multivariate model with constant seasonal pattern,
# using the the seat belt law dummy only for the front seat passangers,
# and restricting the rank of the level component by using custom component
# note the small inconvinience in regression component,
# you must remove the intercept from the additional regression parts manually
model&lt;-SSModel(log(cbind(front,rear))~ -1 + log(PetrolPrice) + log(kms)
+ SSMregression(~-1+law,data=Seatbelts,index=1)
+ SSMcustom(Z=diag(2),T=diag(2),R=matrix(1,2,1),
Q=matrix(1),P1inf=diag(2))
+ SSMseasonal(period=12,sea.type='trigonometric'),
data=Seatbelts,H=matrix(NA,2,2))
likfn&lt;-function(pars,model,estimate=TRUE){
model$H[,,1]&lt;-exp(0.5*pars[1:2])
    model$H[1,2,1]&lt;-model$H[2,1,1]&lt;-tanh(pars[3])*prod(sqrt(exp(0.5*pars[1:2])))
    model$R[28:29]&lt;-exp(pars[4:5])
if(estimate) return(-logLik(model))
model
}
fit&lt;-optim(f=likfn,p=c(-7,-7,1,-1,-3),method='BFGS',model=model)
model&lt;-likfn(fit$p,model,estimate=FALSE)
    model$R[28:29,,1]%*%t(model$R[28:29,,1])
    model$H
out&lt;-KFS(model)
out
ts.plot(cbind(signal(out,states=c('custom','regression'))$signal,model$y),col=1:4)

# For confidence or prediction intervals, use predict on the original model
pred &lt;- predict(model,states=c('custom','regression'),interval='prediction')
ts.plot(pred$front,pred$rear,model$y,col=c(1,2,2,3,4,4,5,6),lty=c(1,2,2,1,2,2,1,1))

## End(Not run)
## Not run:
# Poisson model
model&lt;-SSModel(VanKilled~law+SSMtrend(1,Q=list(matrix(NA)))+
SSMseasonal(period=12,sea.type='dummy',Q=NA),
KFAS 15
data=Seatbelts, distribution='poisson')

# Estimate variance parameters
fit&lt;-fitSSM(inits=c(-4,-7,2), model=model,method='BFGS')
model&lt;-fit$model

# use approximating model, gives posterior mode of the signal and the linear predictor
out_nosim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=0)

# State smoothing via importance sampling
out_sim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=1000)
out_nosim
out_sim

## End(Not run)
# Example of generalized linear modelling with KFS
# Same example as in ?glm
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
print(d.AD &lt;- data.frame(treatment, outcome, counts))
glm.D93 &lt;- glm(counts ~ outcome + treatment, family = poisson())
model&lt;-SSModel(counts ~ outcome + treatment, data=d.AD,
distribution = 'poisson')
out&lt;-KFS(model)
coef(out,start=1,end=1)
coef(glm.D93)
summary(glm.D93)$cov.s
    out$V[,,1]
outnosim&lt;-KFS(model,smoothing=c('state','signal','mean'))
set.seed(1)
outsim&lt;-KFS(model,smoothing=c('state','signal','mean'),nsim=1000)

## linear
# GLM
glm.D93$linear.predictor

# approximate model, this is the posterior mode of p(theta|y)
c(outnosim$thetahat)

# importance sampling on theta, gives E(theta|y)
c(outsim$thetahat)

## predictions on response scale
16 KFAS

# GLM
fitted(glm.D93)

# approximate model with backtransform, equals GLM
c(fitted(outnosim))

# importance sampling on exp(theta)
fitted(outsim)

# prediction variances on link scale
# GLM
as.numeric(predict(glm.D93,type='link',se.fit=TRUE)$se.fit^2)

# approx, equals to GLM results
c(outnosim$V_theta)
    # importance sampling on theta
    c(outsim$V_theta)
# prediction variances on response scale
# GLM
as.numeric(predict(glm.D93,type='response',se.fit=TRUE)$se.fit^2)
    # approx, equals to GLM results
    c(outnosim$V_mu)
# importance sampling on theta
c(outsim$V_mu)
    ## Not run:
    data(sexratio)
    model&lt;-SSModel(Male~SSMtrend(1,Q=list(NA)),u=sexratio[,'Total'],data=sexratio,
    distribution='binomial')
    fit&lt;-fitSSM(model,inits=-15,method='BFGS',control=list(trace=1,REPORT=1))
    fit$model$Q #1.107652e-06

# Computing confidence intervals in response scale
# Uses importance sampling on response scale (4000 samples with antithetics)
pred&lt;-predict(fit$model,type='response',interval='conf',nsim=1000)
    ts.plot(cbind(model$y/model$u,pred),col=c(1,2,3,3),lty=c(1,1,2,2))

# Now with sex ratio instead of the probabilities:
imp&lt;-importanceSSM(fit$model,nsim=1000,antithetics=TRUE)
    sexratio.smooth&lt;-numeric(length(model$y))
sexratio.ci&lt;-matrix(0,length(model$y),2)
    w&lt;-imp$w/sum(imp$w)
    for(i in 1:length(model$y)){
sexr&lt;-exp(imp$sample[i,1,])
sexratio.smooth[i]&lt;-sum(sexr*w)
oo&lt;-order(sexr)
sexratio.ci[i,]&lt;-c(sexr[oo][which.min(abs(cumsum(w[oo]) - 0.05))],
+ sexr[oo][which.min(abs(cumsum(w[oo]) - 0.95))])
}

# Same by direct transformation:
out&lt;-KFS(fit$model,smoothing='signal',nsim=1000)
    KFS 17
    sexratio.smooth2 &lt;- exp(out$thetahat)
sexratio.ci2&lt;-exp(c(out$thetahat)
    + qnorm(0.025) * sqrt(drop(out$V_theta))%o%c(1, -1))
ts.plot(cbind(sexratio.smooth,sexratio.ci,sexratio.smooth2,sexratio.ci2),
col=c(1,1,1,2,2,2),lty=c(1,2,2,1,2,2))

## End(Not run)
# Example of Cubic spline smoothing
## Not run:
require(MASS)
data(mcycle)
model&lt;-SSModel(accel~-1+SSMcustom(Z=matrix(c(1,0),1,2),
T=array(diag(2),c(2,2,nrow(mcycle))),
Q=array(0,c(2,2,nrow(mcycle))),
P1inf=diag(2),P1=diag(0,2)),data=mcycle)
model$T[1,2,]&lt;-c(diff(mcycle$times),1)
model$Q[1,1,]&lt;-c(diff(mcycle$times),1)^3/3
model$Q[1,2,]&lt;-model$Q[2,1,]&lt;-c(diff(mcycle$times),1)^2/2
    model$Q[2,2,]&lt;-c(diff(mcycle$times),1)
    updatefn&lt;-function(pars,model,...){
    model$H[]&lt;-exp(pars[1])
    model$Q[]&lt;-model$Q[]*exp(pars[2])
    model
    }
    fit&lt;-fitSSM(model,inits=c(4,4),updatefn=updatefn,method=""BFGS"")
    pred&lt;-predict(fit$model,interval=""conf"",level=0.95)
plot(x=mcycle$times,y=mcycle$accel,pch=19)
lines(x=mcycle$times,y=pred[,1])
    lines(x=mcycle$times,y=pred[,2],lty=2)
lines(x=mcycle$times,y=pred[,3],lty=2)
## End(Not run)
</code></pre>

<p>The time-series data is:</p>

<pre><code>Time, 2.4, 2.6, 3.2, 3.6, 4, 6.2, 6.6, 6.8, 7.8, 8.2, 8.8, 8.8, 9.6, 10, 10.2, 10.6, 11, 11.4, 13.2, 13.6, 13.8, 14.6, 14.6, 14.6, 14.6, 14.6, 14.6, 14.8, 15.4, 15.4, 15.4, 15.4, 15.6, 15.6, 15.8, 15.8, 16, 16, 16.2, 16.2, 16.2, 16.4, 16.4, 16.6, 16.8, 16.8, 16.8, 17.6, 17.6, 17.6, 17.6, 17.8, 17.8, 18.6, 18.6, 19.2, 19.4, 19.4, 19.6, 20.2, 20.4, 21.2, 21.4, 21.8, 22, 23.2, 23.4, 24, 24.2, 24.2, 24.6, 25, 25, 25.4, 25.4, 25.6, 26, 26.2, 26.2, 26.4, 27, 27.2, 27.2, 27.2, 27.6, 28.2, 28.4, 28.4, 28.6, 29.4, 30.2, 31, 31.2, 32, 32, 32.8, 33.4, 33.8, 34.4, 34.8, 35.2, 35.2, 35.4, 35.6, 35.6, 36.2, 36.2, 38, 38, 39.2, 39.4, 40, 40.4, 41.6, 41.6, 42.4, 42.8, 42.8, 43, 44, 44.4, 45, 46.6, 47.8, 47.8, 48.8, 50.6, 52, 53.2, 55, 55, 55.4, 57.6                                                                                                
mcycle, 0, -1.3, -2.7, 0, -2.7, -2.7, -2.7, -1.3, -2.7, -2.7, -1.3, -2.7, -2.7, -2.7, -5.4, -2.7, -5.4, 0, -2.7, -2.7, 0, -13.3, -5.4, -5.4, -9.3, -16, -22.8, -2.7, -22.8, -32.1, -53.5, -54.9, -40.2, -21.5, -21.5, -50.8, -42.9, -26.8, -21.5, -50.8, -61.7, -5.4, -80.4, -59, -71, -91.1, -77.7, -37.5, -85.6, -123.1, -101.9, -99.1, -104.4, -112.5, -50.8, -123.1, -85.6, -72.3, -127.2, -123.1, -117.9, -134, -101.9, -108.4, -123.1, -123.1, -128.5, -112.5, -95.1, -81.8, -53.5, -64.4, -57.6, -72.3, -44.3, -26.8, -5.4, -107.1, -21.5, -65.6, -16, -45.6, -24.2, 9.5, 4, 12, -21.5, 37.5, 46.9, -17.4, 36.2, 75, 8.1, 54.9, 48.2, 46.9, 16, 45.6, 1.3, 75, -16, -54.9, 69.6, 34.8, 32.1, -37.5, 22.8, 46.9, 10.7, 5.4, -1.3, -21.5, -13.3, 30.8, -10.7, 29.4, 0, -10.7, 14.7, -1.3, 0, 10.7, 10.7, -26.8, -14.7, -13.3, 0, 10.7, -14.7, -2.7, 10.7, -2.7, 10.7
</code></pre>
"
"0.0948209311861521","0.0831710482069074","131331","<p>This is the first time I am posting a question, so please excuse any etiquette violations and poorly worded questions!</p>

<p>I am working on the analysis for a chapter of my thesis. I am examining the behavioural response of an animal to a visual stimulus, and trying to determine which of eight explanatory variables (and their two-way interactions) affect this response. I recorded the response on an ordinal scale of 0 (no response), 1 (attention to but no avoidance of stimulus) or 2 (escape response to stimulus). I am leaning towards collapsing categories and using logistic regression where a 1 is an escape response, and 0 is anything else because logistic regression seems much easier to interpret. </p>

<p>I have 794 observations. I am including observer and location (because field sites differed) as random effects, although I am unsure this is a good approach. </p>

<p>I am having trouble with model selection. I ran all possible subsets using the dredge function in packing 'MuMIn'. I thought I was avoiding data dredging by </p>

<ul>
<li>including main effects which were selected because I thought they would have an effect (rather than all conceivable variables)</li>
<li>including only the two-way interactions of interest (R will not run if the global model includes all possible two-way interactions because of the huge number of terms/models)</li>
</ul>

<p>I've come to realise that the second point may be problematic because it leads to an unbalanced model set as in Burnham and Anderson (2002). </p>

<blockquote>
  <p>Page 169: When assessing the relative importance of variables using sums of the AIC    weights, it is important to achieve a balance in the number of models that contain each variable j.</p>
</blockquote>

<p>My questions are</p>

<ol>
<li><p>Is it possible to have a balanced model set without it being considered data dredging? If so, how? </p></li>
<li><p>Is my approach at all reasonable? If not, are there other avenues I should explore? I started with Hosmer&amp;Lemeshow purposeful forward selection, as advocated by my supervisor, but I had some issues with this which I can elaborate on if necessary. </p></li>
</ol>
"
"0.070675349274022","0.0697410440814588","132761","<p>I am wanting to calculate hazard ratio in a matched cohort design.
For such I am using the <code>coxph()</code> function in R.
But I have been recommended to stratify by matched pair.
My matching has been simulated using the <code>match()</code> and <code>MatchBalance()</code> commands.</p>

<p>But I am confused as to what 'stratify by matched pair' means.
Can someone explain in simple terms please?
ie. in the simulation does this mean that each matched pair is given a unique identity</p>

<p>OR </p>

<p>are the all the case with exposure given an identity, (say 1), and all case without exposure given a different identity (say 2).....</p>

<p>I have been unable to find any literature in relation to R, or even a tutorial to follow.
The <code>â€˜RcmdrPlugin.EZR</code> can be used for the simulation, but it does explain the <em>Stratified Cox proportional hazard regression for matched-pair analysis</em></p>
"
"0.0316069770620507","0.0311891430775903","132774","<p>I trying to conduct linear discriminant analysis using the lda package and I keep getting a warning message saying that the variables are collinear.</p>

<p>I want to pinpoint and remove the redundant variables. What is the best method for doing this in R?</p>

<p>I've read about solutions such as stepwise selection which can be used to do this but this doesn't work with discriminant analysis.</p>

<p>I tried lasso regression but this shrank my 66 variables down to just 12 - the optimal set and it's hard to identify the order in which it's done this as I would prefer to keep a larger number. </p>
"
"0.0836242010007091","0.0825187161885156","133248","<p>Iâ€™m analyzing student performance data. In my dataset, each row corresponds to a student and each column contains several performance metrics (continuous) and the student type (categorical, 4 types). The student type was computed in another analysis, using Expectation-Maximization, based on how students were graded over time. My sample is small, with 50 students.</p>

<p>I want to understand what characterizes each student type, regarding the performance features I have. I want to understand things like â€œthe more grade they have the more likely is to belong to a particular clusterâ€ and so on, if they are present at all in my data.</p>

<p>My first question is:
1) I believe that what I need is Multinomial Logistic Regression. Am I right or is there a better way to achieve this?</p>

<p>If yes, Iâ€™ve been exploring Multinomial Logistic Regression in R, using the multinom of the nnet package, but I need help with the following:</p>

<p>2) Understanding if the model is any good. So far I have the percentage of correctly classified instances, but I know this is not a very good goodness of fit measure. </p>

<p>3) How to assess how good each individual predictor is. I know how to look for the exponentiated B, but I donâ€™t know how to assess its significance. I read that using the t-distribution to compute the p-value here is usually a mistake. I found a <a href=""http://stats.stackexchange.com/questions/63222/getting-p-values-for-multinorm-in-r-nnet-package"">similar post here</a>, but a clear answer was not provided.</p>

<p>Thank you in advance for any answer, suggestion or comment.</p>
"
"0.0632139541241014","0.0623782861551805","133488","<p>I am trying to run regression on financial data in R. I am new to regression analysis so I am finding it to difficult to interpret certain scenarios. I have the code as follows:</p>

<pre><code>#regression analysis
fit &lt;- lm(fiveMinReturns~RegressionData, data=maindata)
summary(fit) # show results
#correlation
cor(maindata$fiveMinReturns,maindata$RegressionData,use=""everything"")
</code></pre>

<p>My output is: </p>

<pre><code>Call:
lm(formula = fiveMinReturns ~ RegressionData, data = maindata)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.205790 -0.001144 -0.000062  0.001117  0.156418 

Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    6.346e-05  8.785e-06   7.223 5.09e-13 ***
RegressionData 1.597e-07  1.432e-08  11.155  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.004035 on 210912 degrees of freedom
Multiple R-squared:  0.0005896, Adjusted R-squared:  0.0005849 
F-statistic: 124.4 on 1 and 210912 DF,  p-value: &lt; 2.2e-16

cor(maindata$fiveMinReturns,maindata$RegressionData,use=""everything"")
[1] 0.02428219
</code></pre>

<p>p-value is very small that means two variables are tightly coupled, but correlation is small too.
My question is how do I evaluate this situation?
Can we say that this equation will give correct results almost every time?
Which scenario suggests both p-value and correlation both to be really small?
What measures should i take to improve the result? </p>
"
"0.130520481086167","0.135950322810847","133571","<p>I know there are already lots of questions around this topic (especially <a href=""http://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models/16415#16415"">this one</a> and <a href=""http://stats.stackexchange.com/questions/24689/interpreting-coefficients-of-ordinal-logistic-regression-when-there-is-clusterin/29701#29701"">this one</a>) but I haven't really seen anything that directly helps me (It will be obvious I'm not a great statistician, but I'll do my best to explain). </p>

<p>I am running an ordinal regression in R (<code>clm</code> and <code>clmm</code>). My response variable is a rating between 0 and 4. I have two types of explanatory variables: individual and scenario variables [let's say <code>IVs</code> and <code>SVs</code>]. </p>

<p>Six different scenario variables (all dummies with at most 4 different values) represent potential collaboration scenarios that get rated by the respondent (between 0 and 4) creating the response variable. (Research design is a conjoint analysis; there are a total of 192 different scenarios possible)</p>

<p>On top of that I have a variety of individual characteristics about the respondent (age, gender, work experience, networking skills, ...) all derived from a survey.</p>

<p>Every respondent rates between 3 and 16 different scenarios (average 8.1); every scenario is rated by at least 8 respondents. Every respondent and every scenario have a unique identifier (called <code>IVid</code> and <code>SVid</code>). So they are non nested within each other.</p>

<p>Thus the basic regression looks like this:</p>

<pre><code>clm.base &lt;- clm(rating ~ SVs + IVs, data = dt) 
</code></pre>

<p>The hypothesis I am trying to test is that there are specific individual characteristics, that will influence the rating of the scenarios, independent of the actual content of the scenarios. Basically, some people are more or less favourable to all types of collaboration scenarios. </p>

<p>Now a reviewer of my paper asks me to include individual fixed effects (which in management [my field] basically means dummies for each individual). My assumption originally was that this would result in all individual variables being dropped. This is exactly what happens when I use another model (package <code>lfe</code>)</p>

<pre><code>felm.complete &lt;- felm(rating ~ SVs + IVs | SVid + IVid | 0 | IVid, data = dt) 
</code></pre>

<p>In this regression basically all my variables are perfectly collinear as expected.
However, when I approximate this in the ordinal package, there is no perfect collinearity. I presume this is related that <code>clmm</code> adds so-called 'random effects'. The regression takes a couple of minutes to run but eventually returns results</p>

<pre><code>clmm.complete &lt;- clmm(rating ~ SVs + IVs + (1|SVid) + (1|IVid), data = dt)
</code></pre>

<p>Now, the results here are pretty useless:</p>

<ul>
<li>All but one of my IVs are insignificant</li>
</ul>

<p>I am trying to understand what exactly happens when adding the <code>(1|IVid)</code> term in the <code>clmm</code> model. If it basically adds something like an individual dummy than the fact almost everything is now insignificant is no surprise. The coefficients of the <code>IVid</code> dummies would capture the effect I am looking for (some people rate all scenarios higher or lower, regardless of scenario content) most accurately.</p>

<p>Now I wonder whether this interpretation is correct or whether the results I got from running the simple <code>clm</code> regression are just not reliable? </p>

<p>Concretely, I'd like to find out:</p>

<ul>
<li>What happens when adding a random effect to <code>clmm</code></li>
<li>A laymen explanation of how the Laplace approximation works</li>
<li>How to group errors around individuals when running <code>clm</code></li>
<li>Is it possible to extract the coefficients of these random effects <code>(1|id)</code> for as far as there is such a thing?</li>
</ul>
"
"0.0948209311861521","0.0935674292327708","134705","<p>I'm using R (factanal) to analyze some data.  I know from reading that there are various ways of picking how many factors to use in the analysis.  I don't know which to choose, or how to do any of them.</p>

<p>Here's the data I have so far from factanal.  I don't understand what SS loadings are, or why degrees_of_freedom is not = min(#rows,#columns) - #factors.</p>

<p>Just judging from the Cumulative Var, which I <em>think</em> I understand, I would guess that 2 is the right number of factors, but am I right?  And if so, how can I convince others that this is the right number of factors?</p>

<pre><code>factanal(x = charges[3:8], factors = 1)

               Factor1
SS loadings      4.779
Proportion Var   0.797
</code></pre>

<p>Test of the hypothesis that 1 factor is sufficient.
The chi square statistic is 279.13 on 9 degrees of freedom.
The p-value is 6.9e-55 </p>

<pre><code>factanal(x = charges[3:8], factors = 2, scores = ""regression"")

               Factor1 Factor2
SS loadings      2.817   2.544
Proportion Var   0.470   0.424
Cumulative Var   0.470   0.894
</code></pre>

<p>Test of the hypothesis that 2 factors are sufficient.
The chi square statistic is 77.1 on 4 degrees of freedom.
The p-value is 7.15e-16 
Error: object 'scores' not found</p>

<pre><code>factanal(x = charges[3:8], factors = 3)

               Factor1 Factor2 Factor3
SS loadings      2.769   2.618   0.063
Proportion Var   0.461   0.436   0.010
Cumulative Var   0.461   0.898   0.908
</code></pre>

<p>The degrees of freedom for the model is 0 and the fit was 0.1047 </p>
"
"0.105194378270435","0.112454054604034","134803","<p>Suppose we have a system that essentially evolves as follows:</p>

<pre><code>stock_t+1  = stock_t + inflows_t - outflows_t 
inflows_t  = a1*predictor11_t + a2*predictor12_t+.... error1_t
outflows_t = b1*predictor21_t + b2*predictor22_t+.... error2_t
</code></pre>

<p>I have observations for each of these variables, i.e. I can observe the stock, the inflows, outflows, and the sum of the flows (they add up correctly), as well as each of the predictors. All variables are time series, and simple time series analysis goes a long way. That said, while the stock is not overly volatile and relatively easy to predict, the flow variables are much more volatile and more challenging to model.</p>

<p>Using regression analysis, in the beginning I've only attempted to model the evolution of the stock using a combination of the flow predictors. I found that the best predictors of the stock are close to the best predictors of the flows (though not identical, i.e. some additional transformation is required). Recently, I have also attempted to model the flows. </p>

<p>The trouble is - not unexpectedly, I should say - that using the best regression models for each of the series, the system does not follow the add-up constraint; or put differently: if I calculate the evolution of the stock using the first equation, based on a starting value and equations 2-3, then my prediction of the stock is quite a bit different than my direct forecast of the stock. At the same time, since the stock model is the model that I have the most faith in, I'd rather not move away from these predictions.</p>

<p>So I was thinking that there surely must be a way to model the entire system directly, rather than estimate each equation separately. Right now a state-space approach comes to mind; before I go off into that direction, though, I am wondering whether I am missing something and whether anyone has a different suggestion.</p>

<p>PS I'm using R</p>
"
"NaN","NaN","134885","<p>Since a feedforward NN with a logistic function as activation function is not linear, does it make sense to reduce variables first with principal components or discriminant analysis?</p>

<p>Because shouldn't be done this before training the NN as with logistic regression?</p>
"
"0.0547448901451359","0.0540211804549215","135332","<p>I did a regression analysis with the following variables: Predictor = dummy variable, dependent Variable = metric, moderator variable = metric. I now want to show my results in a figure. The interaction should be shown by three regression lines. One for moderator = Mean (0), one for moderator = -1 SD, and one for moderator = + 1 SD.</p>

<p>How can I do this in R?</p>
"
"0.104828483672192","0.103442685121077","135792","<p>I'm interested in building a set of candidate models in R for an analysis using logistic regression. Once I build the set of candidate models and evaluate their fit to the data using AICc (<code>aicc = dredge(results, eval=TRUE, rank=""AICc"")</code>), I would like to use k-fold cross fold validation to evaluate the predictability of the final model chosen from the analysis. I have a few questions associated to k-fold cross validation: </p>

<ol>
<li><p>I assume you use your entire data set for initially building your candidate set of models. For example, say I have 20,000 data values, wouldn't I first build my candidate set of models based on the entire 20,000 data values? Then do use AIC to rank the models and select the most parsimonious model?</p></li>
<li><p>After you select the final model (or model averaged model), would you then conduct a k-fold cross validation to evaluate the predictability of the model? </p></li>
<li><p>What is the easiest way to code a k-fold cross-validation in R? </p></li>
<li><p>Does the k-fold cross validation code break up your entire data set (e.g., 20,000 data values) into training and validation sets automatically? Or do you have to subset the data manually? </p></li>
</ol>
"
"0.0893980312535348","0.0882162182782462","135852","<p>I'm trying to replicate an analysis done in Stata with R that involves calculating the autocorrelation for a particular outcome measured in many different areas. I've already run a linear regression on the data, which produced residuals for the outcome of interest. While I can't post my actual data, here is what it sort of looks like:</p>

<pre><code>year&lt;-c('2003', '2004', '2005','2003', '2004', '2005','2003', '2004', '2005')
location&lt;-c(rep('North', 3), rep('South',3), rep('West',3))
resid &lt;-c(-2.42, -3.563, -2.112, -0.543, 2.391, -1.556, -0.177, 0.983, 1.225)
mydata&lt;-data.frame(location,year, resid)
mydata

  location year resid
1    North 2003  -2.420
2    North 2004  -3.563
3    North 2005  -2.112
4    South 2003  -0.543
5    South 2004   2.391
6    South 2005  -1.556
7     West 2003  -0.177
8     West 2004   0.983
9     West 2005   1.225
</code></pre>

<p>I'm interested in the autocorrelation in the residuals, so I'm running the following:</p>

<pre><code> myacf &lt;-acf(mydata$resid, plot=F)
</code></pre>

<p>I'm not getting the same autocorrelation values as I got with Stata. I'm wondering if I should be specifying somehow that my data aren't a continuous time series from t=1 to t=9, but rather 3 sets of 3 time points that were measured in different locations (so really there can only be lag of 1 or 2, at least in the toy data above). </p>

<p>Also, above, I've made it so every region has 3 years of outcomes, but in my real (more complicated) dataset, some regions might have more rows than others (e.g., North might have 7 rows for 2000-2006, but South might only have 5 because we missed the outcome in 2003 and 2006 so there are no residuals for those 2 years, etc.). Any help would be appreciated in understanding whether acf() is the right way to approach this problem or not, and if so, what I might consider changing.</p>
"
"0.0316069770620507","0.0311891430775903","136071","<p>I am hoping someone can check this code to ensure that I have interpreted the various pieces of PCA correctly. I am trying to figure out a way to identify the leading contributors to the performance of multiple securities. For example, one idea I had was to run a multivariate regression using the securities returns as dependent variables and include things like oil, the dollar, the euro, treasury yields, etc.
E.g.,
SBUX + AAPL + MCD + BAC + TWTR = intercept + oil + dollar + euro + steel + gold + e</p>

<p>I then thought that PCA would probably be better suited for this type of exercise. Here is my code from R. The csv file consists of a matrix of 900 securities and 30 rows (30 daily returns for 900 securities)</p>

<pre><code>FD &lt;- read.csv(""U:/Personal Projects/R/Data Files/FD Securities Jan 2015.csv"")

#Removes columns with any na values
FD1 &lt;- FD[, sapply(FD, function(x) !any(is.na(x)))]

#removes ""zero/constant variance"" columns, which I think are NaNs I couldnt erase using is.nan
FD2 &lt;- FD1[,apply(FD1, 2, var, na.rm=TRUE) != 0]

#Calc PCs using singe value decomposition (prcomp). Should data be a correlation matrix of the variables? I get reasonable looking results both ways, i.e., PC1 explains 30-50% of variance, PC2 ~10%-15%, etc.
FD2.pca &lt;- prcomp(cor(FD2), retx = TRUE, scale = TRUE)
summary(FD2.pca)
plot(FD2.pca)

#These are the 'loadings', i.e., coefficients used for each linear combination?
as.matrix(FD2.pca$rotation[,1])

#I think these ""scores"" are the coefficients of interest, as they incorporate the factor     weightings because the output is pca$rotation * scale (stddev of each factor)
as.matrix(FD2.pca$x[,1]) as.matrix(FD2.pca$x[,2]) as.matrix(FD2.pca$x[,3])     as.matrix(FD2.pca$x[,4])

#Scatterplot of the first two principal components. Not sure if this is right of if $x should be used.
plot(x = FD2.pca$rotation[,1], FD2.pca$rotation[,2], xlab = ""PC1"", ylab = ""PC2"", main=""Principal Component Analysis:"")
</code></pre>
"
"0.0632139541241014","0.0623782861551805","136437","<p>Iâ€™m having trouble figuring out how to apply the LME function to a set of data. What I have is a list of Stores and their respected customer count, by week, with various external factors for each store: Crime Rate per 100k people, % of people with a college Degree, Level of inequality, and so forth (sample Below.)</p>

<pre><code>Store   Week    CustomerCount   CrimeRate   %collegeDegree  Inequality  Median Income
1       1         200              5        0.25            0.4         25000
1       2         259               5       0.25            0.4         25000
1       3         234              5        0.25            0.44        25000
â€¦                       
5       1         106               1       0.2         0.43            26000
5       2         96                1       0.2         0.42            26000
5       3         101               2       0.21        0.42            26000
</code></pre>

<p>Now, what Iâ€™m trying to do is this: Iâ€™m trying to run an <strong>annual</strong> regression of the data to determine if some of the external factors (crime Rate, college rate, inequality level) have an effect on store customer count.</p>

<p>Now, I need to do a repeated measure analysis on this where the repeated measures are Week, and entity of store has the repeated values. Iâ€™ve set up the LME function thusly:</p>

<p>ïƒ˜   Lme(CustomerCount ~ CrimeRate + CollegeRate + InequalitlyLevel, random = ???? , data=the.data)</p>

<p>So, my question is: what do I put in for the random variable? Is it â€œâ€¦,random = ~1|week/Store,â€¦â€ or a variation thereof?</p>
"
"0.113960576459638","0.112454054604034","137120","<p>I am using a software in my analysis, from which I obtained the $R^2$ and estimated effects ($\beta$) of a linear regression model. In addition, it outputs the design matrix ($X$) of the model. To understand how $R^2$ was calculated in this software, I tried to calculate it in R with the design matrix and estimated effects. </p>

<p>Full model: y = cross + x</p>

<p>Reduced model: y = cross</p>

<p>The formula for calculating $R^2$ is 1-RSS_full/RSS_red, where RSS_full refers to the residual sum of squares of the full model and RSS_red is the residual sum of squares of the reduced model. I calculated the RSS_full with the residuals obtained from lm function in R. For RSS_red, I tried two methods to get the residuals and then RSS_red. </p>

<p>Method 1: apply lm function in R, which is sum(lm(y~cross-1,tol=1e-4)$residuals^2)</p>

<p>Method 2: firstly calculate the fitted value of the reduced model (y_red) with its design matrix and the estimated effects from the full model obtained from the software (y_red = cross%*%cross_effects), and then get the RSS_red by sum((y-y_red)^2).</p>

<p>The results of the two methods are different. However, the result of method 1 is the same as the one obtained from the Software. I do not know why they differ. Could someone give me some suggestions? Thank you! The design matrix, R codes and the output from the sotware are as follows.</p>

<p>Design matrix (ModelSJ file):</p>

<p>It can be downloaded from the dropbox share link: <a href=""https://www.dropbox.com/s/hd3po8g8ixs2wod/ModelSJ?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/hd3po8g8ixs2wod/ModelSJ?dl=0</a></p>

<p>R codes for calculating $R^2$:</p>

<pre><code>data=as.matrix(read.table(file=""https://dl.dropboxusercontent.com/u/24381951/ModelSJ"")) # reads the file from dropbox
y=data[,1] # the first column is the response/observed values.
x=data[,-1] # design matrix of the full model
cross=x[,10:12] # design matrix of the reduced model
RSS_full &lt;- sum(lm(y~x-1,tol=1e-4)$residuals^2) # RSS of the full model

# Method 1
RSS_red &lt;- sum(lm(y~cross-1,tol=1e-4)$residuals^2)
R2_1 &lt;- 1-RSS_full/RSS_red 

# Method 2
cross_effects &lt;- c(3.9171, 4.4411, 3.6381) # the estimated effects ÃŸ obtained 
#from the software based on the full model

y_red &lt;- cross%*%cross_effects # fitted value
RSS_red &lt;- sum((y-y_red)^2)
R2_2 &lt;- 1-RSS_full/RSS_red

# Why are R2_1 and R2_2 not equal?
</code></pre>

<p>Output from the software:</p>

<p>it can be downloaded from <a href=""https://www.dropbox.com/s/8hp8p3kcf0d2nc5/testSJ_DON_iQTLm.xml?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/8hp8p3kcf0d2nc5/testSJ_DON_iQTLm.xml?dl=0</a></p>
"
"0.0893980312535348","0.0882162182782462","137209","<p>When I run regression analysis I find it important to run some model diagnostics, such as detection of outliers, influential observations, multi-collinearity (much like these examples <a href=""http://www.statmethods.net/stats/rdiagnostics.html"" rel=""nofollow"">http://www.statmethods.net/stats/rdiagnostics.html</a>).</p>

<p>Example of Diagnostics I use:</p>

<pre><code>#Assessing the Assumption of Independence, using Durbin Watson Test
dwt(lmModel)

#Controlling for Multicollinearity
vif(lmModel)
1/vif(lmModel)
mean(vif(lmModel))
</code></pre>

<p>I have a sample with a lot of missing data across most variables. Thus, I need to use multiple imputations. </p>

<p>However, model diagnostics seems to be impossible to explore when using multiple imputations. So far, I have used the mice package and since I am still a novice at R my multiple imputation script basically looks like this:</p>

<pre><code>#Imputes 5 datasets    
imp &lt;- mice(myData, m=5)    

#Runs regression analysis on each imputed dataset    
fit &lt;- with(imp, lm(A ~ B + C))   

#Pools the results
pooled &lt;- pool(fit)
summary(pooled)
</code></pre>

<p>Is there some way to use the diagnostic test on the pooled data? or do I have to use diagnostic tests on each imputed dataset (before being pooled)? or is there some other smart way of solving this issue?</p>

<p>Thanks for your time</p>
"
"0.144991226648596","0.136571103769086","138230","<p>I want to perform propensity score matching of observational data of an Intensive Care Unit in order to find out wheather hydroxyethyl starch is better or worse than colloids in terms of renal replacement therapy (RRT), Akute Kidney Injury (AKI) and mortality. </p>

<p>I use the MatchIt package in R (King et al. 2007 - <a href=""http://gking.harvard.edu/matchit"" rel=""nofollow"">http://gking.harvard.edu/matchit</a>). This package is quite well documented. But there are some things that I dont understand.
First I matched on sociodemographic covariates (as this seems standard protocol with matching): Gender, weight, height and age.
Nearest neighbor matching seems to have worked:</p>

<p>NN matching</p>

<pre><code>m.out.nn

Call: 
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0

Treatment status treat1 is HES = yes , Colloids otherwise btw. I did a numerical balance check and balance actually WORSENED after matching. Overall as well as some of the covariates drastically:
Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9168        0.9145     0.0131    0.0022  0.0018   0.0023   0.0388
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056   1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362   1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390   1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462  30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604  30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916   6.2000
BMI                      28.4858       27.8005    15.1559    0.6853  0.7080   2.1550 347.8520


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9357        0.9145     0.0131    0.0212  0.0172   0.0212   0.0687
Geschlecht                0.0446        0.0000     0.0000    0.0446  0.0000   0.0446   1.0000
Geschlechtm               0.9053        0.6100     0.4884    0.2953  0.0000   0.2953   1.0000
Geschlechtw               0.0501        0.3900     0.4884   -0.3398  0.0000   0.3398   1.0000
Gewicht.kg               98.1744       77.8930    18.2092   20.2813 17.0000  20.2813  62.0000
Groesse.cm              164.7103      169.9861    11.9693   -5.2758  2.0000   5.4540  77.0000
Alter.bei.ITS.Aufn       72.5702       63.4808    14.4918    9.0894  7.0000   9.0894  26.5000
BMI                      44.1753       27.8005    15.1559   16.3748  6.7500  16.3748 258.9020

Percent Balance Improvement:
                   Mean Diff.   eQQ Med  eQQ Mean   eQQ Max
distance            -852.5593 -839.8420 -808.2029  -77.0723
Geschlecht          -998.6072    0.0000 -700.0000    0.0000
Geschlechtm         -714.0668    0.0000 -715.3846    0.0000
Geschlechtw         -742.6908    0.0000 -771.4286    0.0000
Gewicht.kg         -1533.1522 -750.0000 -998.5214 -106.6667
Groesse.cm         -7691.0845      -Inf -617.2161 -156.6667
Alter.bei.ITS.Aufn  -715.7611 -775.0000 -603.7093 -327.4194
BMI                -2289.4307 -853.3898 -659.8482   25.5712

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0
</code></pre>

<p>How can this be possible?</p>

<p>I also did genetic matching (Sekhon 2011 - <a href=""http://sekhon.berkeley.edu/matching/"" rel=""nofollow"">http://sekhon.berkeley.edu/matching/</a>). This is a fancy algorithm that automatically optimizes covariate balance. There covariate balance has indeed improved (as it should have):</p>

<pre><code>Genetic matching
load(file=""m.out.genetic.RData"")
Numerical Balance Check 
summary(m.out.genetic)

Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn, data = hes.vs.kristall.clean, method = ""genetic"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9147     0.0126    0.0021  0.0019   0.0022  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362  1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390  1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462 30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604 30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916  6.2000


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9164     0.0105    0.0003  0.0018   0.0021  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6481     0.4782   -0.0018  0.0000   0.0364  1.0000
Geschlechtw               0.3496        0.3519     0.4782   -0.0023  0.0000   0.0392  1.0000
Gewicht.kg               79.1349       79.0556    15.9832    0.0793  2.0000   1.7801 30.0000
Groesse.cm              169.9184      170.0479    10.6992   -0.1296  0.0000   0.7703 30.0000
Alter.bei.ITS.Aufn       64.5950       64.7378    13.3160   -0.1428  0.8000   1.2440  6.2000

Percent Balance Improvement:
                   Mean Diff. eQQ Med eQQ Mean eQQ Max
distance              83.7418  3.8423   2.7923       0
Geschlecht             0.0000  0.0000  -0.5602       0
Geschlechtm           95.1066  0.0000  -0.5602       0
Geschlechtw           94.3414  0.0000  -0.5602       0
Gewicht.kg            93.6115  0.0000   3.5817       0
Groesse.cm           -91.3359  0.0000  -1.2969       0
Alter.bei.ITS.Aufn    87.1817  0.0000   3.6903       0

Sample sizes:
          Control Treated
All           359    3944
Matched       357    3944
Unmatched       2       0
Discarded       0       0
</code></pre>

<p>I also checked balance graphically and it did improve (despite being good pre-matching).</p>

<p>Now my questions are:</p>

<ol>
<li><p>Can I use the nearest neighbor matched data? How could I change this so that balance does improve? What kind of distance metric does Nearest neighbor matching use (by default) (Euclidean ?). Because with Euclidean the non-Boolean covariates (Gender) could be made more important than they are.</p></li>
<li><p>How can I perform analysis after matching? - How can I get the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATET) in terms of HES for AKI, RRT and mortaility and does that make sense for these response variables (AKI, RRT and mortaility)? Or should I get the odds ratio for Akute Kidney Injury, renal replacement therapy and mortaility from the matched observational data? How do I get these values?
I know that MatchIt recommends using Zelig to get these values but that didn't seem to work with my data. 
Can I use logistic regression with the matched data to get the odds ratio of HES vs. Cristalloids of AKI, RRT and mortality ?</p></li>
</ol>
"
"0.109815159255115","0.116699087583415","138506","<p>I have created regression models using robust regression - in particular, LTS and MM-estimators (using the R package robustbase).  I am now looking to creation prediction intervals.</p>

<p>The standard formula for prediction intervals for linear regression is:
$$
\hat{y_0} \pm t_{\alpha/2, n-p} \sqrt{\hat{\sigma^2}(1+x_0'(X'X)^{-1}x_0)}
$$
(see Montgomery and Peck, Introduction to Linear Regression Analysis, 1992)</p>

<p>For robust regression, obviously, the term $x_0'(X'X)^{-1}x_0$ cannot be used.  The Hat Matrix is different due to the weights.  It would seem to me that we can instead use the Hat Matrix modified with the weights:</p>

<p>$$
x_0'(X'WX)^{-1}x_0
$$</p>

<p>(see page 44 of the PhD thesis by Christopher Assaid at Virigina Tech)(<a href=""http://scholar.lib.vt.edu/theses/available/etd-3649212139711101/unrestricted/Ch6.PDF"" rel=""nofollow"">http://scholar.lib.vt.edu/theses/available/etd-3649212139711101/unrestricted/Ch6.PDF</a>)</p>

<p>We can approximate $\hat{\sigma^2}$ from the data as
$$
\hat{\sigma^2} = \frac{1}{df}\sum e_i^2
$$
where $df$ are the number of degrees of freedom.</p>

<p>I have three questions on this formulation for prediction intervals:</p>

<p>Is my formula correct?  Is the simple adjustment by factoring in the Weight Matrix enough to adapt the OLS formula for prediction intervals to robust regression.</p>

<p>If it is correct, does it apply to <em>all</em> types of robust regression, or just a subset?</p>

<p>Is it correct to estimate the variance as above?  If so, it would seem to me that  robust regression will always have a larger variance, and thus larger prediction intervals, than OLS.  The reason is is that OLS, by definition, is set up to minimize the residual sum of squares.  Robust regression, on the other hand, by definition of down-weighting potential outliers, even though it may give an overall better fit, will see a larger net residual sum of squares because of the contribution of the squared residuals from the outlier points.  Consequently, since the length of the interval is $\sqrt{\hat{\sigma}^2(1+\delta)}$ (where granted $\delta$ is not necessarily small, but the interval is always $\sigma$ plus something), if $\hat{\sigma}_{RR} &gt; \hat{\sigma}_{OLS}$, in general, the length of the interval for RR will be larger.  It seems counter-intuitive to me that if data is fit with both OLS and robust regression and prediction intervals are made, those from OLS will be by definition narrower and may even be contained within the robust ones.  It seems to thus minimize the power of robust regression.</p>

<p>Any answers to these questions or other suggestion/advice on creating prediction intervals for LTS and MM regression would be appreciated.</p>
"
"0.0547448901451359","0.0540211804549215","138923","<p>I'm working on a dataset where I have dates as the main unit of analysis. I'm trying to see if two events are related; that is, if the first event happens, will the second event happen within a month of the first event? </p>

<p>I have the dates for both, but I'm kind of at a loss for how to proceed. I thought I should be using logistic regression because the variables (outcomes) are binary: either the second event happened within a month of the first event, or it didn't. Should I? How do I make this month-long envelope after the first event? Am I totally on the wrong track?  </p>
"
"0.0836242010007091","0.0825187161885156","138938","<p>What is the correct way to calculate the standard errors of the coefficients in a weighted linear regression?</p>

<p>The regression equation I am using is $y_i = a + bx_i$, and I have weights, $w_i = 1/\sigma_i$.  The numerical recipes formula for a straight line fit, and the formula given in ""An introduction to error analysis"" by J. R Taylor, (and Wikipedia too) state that the standard error in the $b$ coefficient is calculated as $$\sigma_b = \sqrt{\frac{\sum w_i}{\sum w_i\sum w_i x_i^2-(\sum w_i x_i)^2}}$$ (or alternatively in matrix form the standard errors are, $\sigma^2 = (X'WX)^{-1}$).  This formula can be derived from propagation of errors. </p>

<p>Using R's $lm()$ function (and python's StatsModels), I get a standard error in the $b$ coefficient which appears* to be calculated as $$\sigma_b = \sigma_e\sqrt{\frac{\sum w_i}{\sum w_i\sum w_i x_i^2-(\sum w_i x_i)^2}}$$
where $\sigma_e^2 = \sum w_i(y_i - a - bx_i)^2/(N-2)$ (alternatively, $\sigma^2 = \sigma_e^2(X'WX)^{-1}$ ).  So they are the same, except for the $\sigma_e$ multiplier in R and StatsModel.</p>

<p>Is it possible that these actually different measures that are just being called the same thing? Is one preferred over the other for an estimate of the standard error?</p>

<p>*I say ""appears"" because I couldn't find the actual formula anywhere.</p>

<p>edited because I had omitted the weight terms in the denominators.   </p>
"
"0.114252409399597","0.120795031721987","139528","<p>When modelling continuous proportions (e.g. proportional vegetation cover at survey quadrats, or proportion of time engaged in an activity), logistic regression is considered inappropriate (e.g. <a href=""http://www.esajournals.org/doi/full/10.1890/10-0340.1"" rel=""nofollow"">Warton &amp; Hui (2011) The arcsine is asinine: the analysis of proportions in ecology</a>). Rather, OLS regression after logit-transforming the proportions, or perhaps beta regression, are more appropriate.</p>

<p>Under what conditions do the coefficient estimates of logit-linear regression and logistic regression differ when using R's <code>lm</code> and <code>glm</code>?</p>

<p>Take the following simulated dataset, where we can assume that <code>p</code> are our raw data (i.e. continuous proportions, rather than representing ${n_{successes}\over n_{trials}}$):</p>

<pre><code>set.seed(1)
x &lt;- rnorm(1000)
a &lt;- runif(1)
b &lt;- runif(1)
logit.p &lt;- a + b*x + rnorm(1000, 0, 0.2)
p &lt;- plogis(logit.p)

plot(p ~ x, ylim=c(0, 1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/AzWOX.png"" alt=""enter image description here""></p>

<p>Fitting a logit-linear model, we obtain:</p>

<pre><code>summary(lm(logit.p ~ x))
## 
## Call:
## lm(formula = logit.p ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.64702 -0.13747 -0.00345  0.15077  0.73148 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.868148   0.006579   131.9   &lt;2e-16 ***
## x           0.967129   0.006360   152.1   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## Residual standard error: 0.208 on 998 degrees of freedom
## Multiple R-squared:  0.9586, Adjusted R-squared:  0.9586 
## F-statistic: 2.312e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Logistic regression yields:</p>

<pre><code>summary(glm(p ~ x, family=binomial))
## 
## Call:
## glm(formula = p ~ x, family = binomial)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.32099  -0.05475   0.00066   0.05948   0.36307  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.86242    0.07684   11.22   &lt;2e-16 ***
## x            0.96128    0.08395   11.45   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 176.1082  on 999  degrees of freedom
## Residual deviance:   7.9899  on 998  degrees of freedom
## AIC: 701.71
## 
## Number of Fisher Scoring iterations: 5
## 
## Warning message:
## In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>Will the logistic regression coefficient estimates always be unbiased with respect to the logit-linear model's estimates?</p>
"
"0.0547448901451359","0.0540211804549215","139653","<p>Original post on stackoverflow:
<a href=""http://stackoverflow.com/questions/28773153/how-to-do-regression-model-selection-if-dummy-variables-are-involved"">http://stackoverflow.com/questions/28773153/how-to-do-regression-model-selection-if-dummy-variables-are-involved</a></p>

<p>I am trying to do a logistic regression analysis in R with two continuous explanatory variables and six other explanatory categorical variables, and find a good regression model to do predictions. When I do step-wise model selections, there are always some levels of certain categorical variables identified as insignificant. I am just wondering how should I deal with this situation. Should I simply drop these levels, or I should force the program to keep all levels of the categorical variables and try to drop the relatively insignificant variables?</p>

<p>Thanks a lot!</p>
"
"0.110624419717177","0.109162000771566","139861","<p>I'm examining the effect of income (categorized into quintiles) on a response variable during different years (from 2003 to 2014). I adjust for some other covariates and have repeated measurements on the same individual.</p>

<p><strong>The problem:</strong> To obtain the effect of income on the response variable each year, I include an interaction term between year and income quintile. This gives me implausible results, since those who are rich tend to have higher adjusted values, which is unlikely. So I redid the analysis, fitting one separate regression model each year, and that gave me plausible results; i.e the rich had lower adjusted values.</p>

<p><em>Shouldn't the two methods yield somewhat similar results?</em></p>

<p>My calculations:</p>

<pre><code>    &gt; # START
    &gt; 
    &gt; # Converting the year variable into a factor
    &gt; data$year &lt;- as.factor(data$year)
    &gt; 
    &gt; # Fitting a model with random effects for person
&gt; require(lme4)
&gt; fit  &lt;- lmer(response ~ income*year + sex + age + education + biomarker + (1 | id), data=data)
&gt; # Using lsmeans to predict means (95% confidence intervals)
&gt; require(lsmeans)
&gt; results1 &lt;- lsmeans(fit, ~ income*year)
&gt; summary(results1)
 income     year   lsmean        SE       df lower.CL upper.CL
 Quintile 1 2003 64.95472 0.2826723 190216.2 64.40069 65.50875
 Quintile 2 2003 65.49504 0.2716893 189962.8 64.96254 66.02755
 Quintile 3 2003 65.39961 0.2713204 189648.3 64.86782 65.93139
 Quintile 4 2003 65.51872 0.2715941 189734.3 64.98640 66.05103
 Quintile 5 2003 65.47502 0.2744592 190425.5 64.93709 66.01296
 Quintile 1 2004 64.03440 0.2541402 191982.7 63.53630 64.53251
 Quintile 2 2004 65.04364 0.2472738 191720.0 64.55899 65.52829
 Quintile 3 2004 64.98069 0.2425866 191644.4 64.50523 65.45616
 Quintile 4 2004 65.14219 0.2459067 191477.1 64.66022 65.62416
 Quintile 5 2004 65.25515 0.2496510 191947.6 64.76584 65.74446
 Quintile 1 2005 63.62453 0.2345767 192988.5 63.16476 64.08429
 Quintile 2 2005 64.18179 0.2294655 192853.2 63.73205 64.63154
 Quintile 3 2005 64.00250 0.2308264 192458.4 63.55008 64.45491
 Quintile 4 2005 63.79620 0.2276547 192775.7 63.35000 64.24240
 Quintile 5 2005 64.70116 0.2344171 193212.2 64.24171 65.16062
 Quintile 1 2006 64.17463 0.2228096 193482.8 63.73793 64.61134
 Quintile 2 2006 64.37025 0.2158588 193425.3 63.94717 64.79333
 Quintile 3 2006 64.30762 0.2141169 193366.8 63.88795 64.72728
 Quintile 4 2006 64.38315 0.2168376 193329.6 63.95815 64.80814
 Quintile 5 2006 64.22019 0.2198772 193526.5 63.78923 64.65114
 Quintile 1 2007 63.84188 0.2185725 193602.5 63.41349 64.27028
 Quintile 2 2007 63.92346 0.2104634 193527.6 63.51096 64.33597
 Quintile 3 2007 63.67954 0.2094212 193511.7 63.26908 64.09000
 Quintile 4 2007 64.08718 0.2103912 193478.5 63.67482 64.49955
 Quintile 5 2007 64.03627 0.2113704 193649.9 63.62199 64.45055
 Quintile 1 2008 64.65587 0.2043770 193372.1 64.25529 65.05644
 Quintile 2 2008 64.30141 0.1957655 193499.2 63.91772 64.68511
 Quintile 3 2008 65.04454 0.1955247 193589.4 64.66131 65.42776
 Quintile 4 2008 64.94073 0.1947715 193488.8 64.55898 65.32247
 Quintile 5 2008 65.00096 0.1979129 193119.2 64.61305 65.38886
 Quintile 1 2009 64.74611 0.1941592 191979.5 64.36556 65.12666
 Quintile 2 2009 64.68663 0.1872505 192801.9 64.31962 65.05363
 Quintile 3 2009 64.89048 0.1858611 192919.0 64.52620 65.25476
 Quintile 4 2009 65.19469 0.1848586 192734.9 64.83238 65.55701
 Quintile 5 2009 65.18344 0.1896058 192025.8 64.81182 65.55506
 Quintile 1 2010 64.99407 0.1863001 188874.1 64.62893 65.35922
 Quintile 2 2010 65.14159 0.1758624 190272.3 64.79691 65.48628
 Quintile 3 2010 65.21003 0.1740553 190655.7 64.86889 65.55118
 Quintile 4 2010 65.65492 0.1731845 190157.8 65.31548 65.99435
 Quintile 5 2010 65.42802 0.1774762 189156.3 65.08017 65.77587
 Quintile 1 2011 65.77711 0.1807193 179035.8 65.42290 66.13131
 Quintile 2 2011 65.81373 0.1719076 186787.1 65.47679 66.15066
 Quintile 3 2011 66.25649 0.1692650 187174.9 65.92473 66.58824
 Quintile 4 2011 66.24046 0.1672252 185968.9 65.91271 66.56822
 Quintile 5 2011 66.31895 0.1717513 184631.5 65.98232 66.65558
 Quintile 1 2012 66.45412 0.1815487 178604.9 66.09829 66.80995
 Quintile 2 2012 66.39743 0.1691640 185466.1 66.06587 66.72899
 Quintile 3 2012 66.61760 0.1674829 185934.4 66.28934 66.94586
 Quintile 4 2012 66.69909 0.1672601 185774.9 66.37126 67.02692
 Quintile 5 2012 66.74211 0.1697808 183081.3 66.40934 67.07487
 Quintile 1 2013 66.20088 0.1804738 177511.5 65.84716 66.55461
 Quintile 2 2013 66.05285 0.1710873 185354.9 65.71752 66.38817
 Quintile 3 2013 65.57061 0.1667456 185103.9 65.24379 65.89742
 Quintile 4 2013 65.96563 0.1669031 184335.3 65.63851 66.29276
 Quintile 5 2013 66.19121 0.1723801 183746.1 65.85335 66.52907
 Quintile 1 2014 65.37137 0.3060358 191891.8 64.77155 65.97120
 Quintile 2 2014 66.37503 0.2882805 188873.8 65.81001 66.94006
 Quintile 3 2014 65.49851 0.2876551 188265.5 64.93471 66.06231
 Quintile 4 2014 66.08503 0.2867954 188729.4 65.52291 66.64714
 Quintile 5 2014 65.98435 0.2901786 189169.5 65.41561 66.55309

Results are averaged over the levels of: sex, education 
Confidence level used: 0.95 
&gt; # As you can see above, the richest (quintile 5) has higher values of a harmful biomarker, this is not likelybiomarker, this is not likely
    &gt; 
    &gt; # Redo the analysis by stratification, i.e one regression each year (I'll give 4 examples, years 2005, 2008, 2010 and 2014)
    &gt; fit2005  &lt;- lmer(response ~ income + sex + age + education + biomarker + (1 | id), data=data[data$year==""2005"",])
        &gt; fit2008  &lt;- lmer(response ~ income + sex + age + education + biomarker + (1 | id), data=data[data$year==""2008"",])
    &gt; fit2010  &lt;- lmer(response ~ income + sex + age + education + biomarker + (1 | id), data=data[data$year==""2010"",])
        &gt; fit2014  &lt;- lmer(response ~ income + sex + age + education + biomarker + (1 | id), data=data[data$year==""2014"",])
    &gt; 
    &gt; lsmeans(fit2005, ""income"")
     income       lsmean        SE      df lower.CL upper.CL
     Quintile 1 64.57677 0.3644627 7104.87 63.86232 65.29123
     Quintile 2 63.65426 0.3715929 7114.04 62.92582 64.38269
     Quintile 3 63.97948 0.3773103 7119.28 63.23984 64.71912
     Quintile 4 63.46368 0.3727073 7117.62 62.73306 64.19429
     Quintile 5 63.57556 0.3853513 7117.67 62.82016 64.33096

    Results are averaged over the levels of: sex, education 
    Confidence level used: 0.95 
    &gt; lsmeans(fit2008, ""income"")
     income       lsmean        SE      df lower.CL upper.CL
     Quintile 1 64.27986 0.3268165 9880.81 63.63924 64.92049
     Quintile 2 65.07827 0.3288624 9866.53 64.43363 65.72291
     Quintile 3 64.98781 0.3265577 9859.02 64.34769 65.62793
     Quintile 4 64.89630 0.3305190 9868.49 64.24842 65.54419
     Quintile 5 63.66509 0.3428045 9898.35 62.99313 64.33706

    Results are averaged over the levels of: sex, education 
    Confidence level used: 0.95 
    &gt; lsmeans(fit2010, ""income"")
     income       lsmean        SE       df lower.CL upper.CL
     Quintile 1 65.39530 0.2961521 11897.22 64.81480 65.97581
     Quintile 2 65.61791 0.2911321 11892.26 65.04724 66.18858
     Quintile 3 65.48423 0.2947050 11892.60 64.90656 66.06190
     Quintile 4 65.14303 0.2914349 11925.62 64.57177 65.71429
     Quintile 5 63.89145 0.3030998 11935.15 63.29733 64.48558

    Results are averaged over the levels of: sex, education 
    Confidence level used: 0.95 
    &gt; lsmeans(fit2014, ""income"")
     income       lsmean        SE      df lower.CL upper.CL
     Quintile 1 67.05015 0.4888236 4523.42 66.09182 68.00849
     Quintile 2 65.41100 0.4801968 4523.32 64.46958 66.35242
     Quintile 3 65.35658 0.4740396 4525.63 64.42723 66.28592
     Quintile 4 65.03556 0.4793119 4526.56 64.09587 65.97524
     Quintile 5 64.94690 0.5001898 4529.88 63.96628 65.92751

    Results are averaged over the levels of: sex, education 
    Confidence level used: 0.95 
</code></pre>

<p>I must have misunderstood something (important) here...</p>

<p>Any advice?</p>
"
"0.0364965934300906","0.0540211804549215","139928","<p>how to simulate data (in R) to generate , sample values
1) variables with specific correlation values for a particular model AND 
2) with predefined regression coefficients? 
3) Can we also set the mean and SD in the same process? 
4) Also how does one simulate the p value/significance of the variable. </p>

<p>This is for imitating existing models for analysis and teaching purposes</p>

<p>Sorry for not being specific : this is for multiple regression, sample values. I would like to specify the mean and SD if possible (apparently not, I can specify only one in order to specify the regression coefficients?)</p>

<p>Thanks for the help.</p>
"
"0.106091523013911","0.112742029607188","140761","<p>I have a question regarding the use of propensity score in a survival analysis with use of mutliple imputation to handle missing data. The question is of theoretical nature and may well apply to other situations.</p>

<p>I have a data set of <em>n</em> individuals. The aim is to estimate the effect of a treatment on a binary outcome (death). The analysis is based on propensity score; the propensity score is derived by means of logistic regression, which includes 30 predictors variables. Effect estimation is carried out by means of Cox regression (which uses the propensity score in various ways [stratification, covariate adjustments etc]). There are a large number of patients, and on average 2â€“7% missing for each variable (of which there are 30 included in the prop. score).</p>

<p>Thus, I have a large data set with a substantial amount of missing data (at least in terms of complete cases) which is why I use multiple imputation - 5 complete data sets are imputed. Now the question is what to do with the muliply imputed data sets; which one of the strategies below should I prefer?</p>

<p><strong>1.</strong> Calculate one average propensity score for each individual using the 5 separate data sets. That way, each individual will have one propensity score, which is the average from the n complete data sets. Then do the Cox regression..</p>

<p><strong>2.</strong> Analyze each separate multiply imputed data set (with Cox regression), and then pool the 5 hazard ratio estimates to one hazard ratio.</p>

<p>The second method appears to be used more often, but is it better/worse?</p>

<p>Any thoughts about this?</p>
"
"0.0774209661138764","0.0636645717122953","140929","<p>I wish to carry out logistic regression analysis using Firth's method, as implemented in R logistf package, to analyse SNP case/control data, for rare variants, whilst controlling for stratification using PCA eigenvectors as covariates. I wish to obtain p-values for each SNP (additive model).</p>

<p>Previously I have performed logistic regression analysis using PLINK:</p>

<pre><code>plink  --bfile snpdata --logistic --ci 0.95 --covar plink2_pca.eigenvec --covar-number 1-2 --out snpout
</code></pre>

<p>I would like to perform similar analysis, but wish to handle quasi-complete separation of the rare variants in my data sets.</p>

<p>I have followed a SNP analysis example provided with logistf and been able to obtain P values:</p>

<p>A very small sample of the snpdata (cases: case 1, control 0; SNP additive allele counts for minor allele: 0, 1, 2):</p>

<pre><code>           PC01         PC02 case exm226_A exm401_A exm4584_A exm146_A
1  -0.003092320 -0.002737810    1            0       0       0       0
2   0.015637300  0.008232330    1            0       0       0       0
3   0.006746730  0.008704400    1            0       1       0       1
4   0.001438270  0.000875751    0            0       0       0       0
5  -0.004161490  0.011407500    0            0       0       2       0

for(i in 1:ncol(snpdata)) snpdata[,i] &lt;-as.factor(snpdata[,i])
snpdata &lt;- snpdata[sapply(snpdata,function(x) length(levels(x))&gt;=2)] 
fitsnp &lt;- logistf(data=snpdata, formula=case~1, pl=FALSE)
add1(fitsnp)
</code></pre>

<p>But I am not clear on how to pass the eigenvectors in as covariates, or whether I can used the eigenvector values as is, or need to convert to these as factors?   </p>

<pre><code>fitsnp &lt;- logistf(data=snpdata,formula=case~(1+PC01+PC02), pl=FALSE)
</code></pre>

<p>I'm not sure if I am on the right track here and can't find a sufficiently similar example on-line to follow.</p>

<p>I would appreciate any assistance, or explanation if I am going completely wrong here.</p>

<p>Thanks in advance.</p>
"
"0.130520481086167","0.128795042662908","140972","<p>Iâ€™m using a maximal logistic regression model to analyze some data. I would like to keep using this technique if possible, just include more data in the model. The main data Iâ€™m looking at is counts of a particular behavior over items in a sequence, and I would like my analysis to also include data from a post-experiment questionnaire (8 items, 1-9 Likert scored). Hereâ€™s some info about my data:</p>

<pre><code>'data.frame':
Pair          : Factor w/ 36 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1 ...
SpeakerID     : Factor w/ 72 levels ""10A"",""10B"",""11A"",..: 21 22 21 22 22 21 22 21 21 22 ...
Speaker       : Factor w/ 2 levels ""A"",""B"": 1 2 1 2 2 1 2 1 1 2 ...
Condition1     : Factor w/ 4 levels ""ANTI"",""CONTROL"",..: 1 1 1 1 1 1 1 1 1 1 ...
..- attr(*, ""contrasts"")= num [1:4, 1:3] -0.333 1 -0.333 -0.333 0.25 ...
.. ..- attr(*, ""dimnames"")=List of 2
.. .. .. : chr  ""ANTI"" ""CONTROL"" ""IN"" ""OUT""
.. .. .. : NULL
Condition2         : Factor w/ 3 levels ""0"",""90"",""180"": 2 3 1 1 2 1 1 2 2 3 ...
..- attr(*, ""contrasts"")= num [1:3, 1:2] 0 -0.5 0.5 -0.5 0.25 0.25
.. ..- attr(*, ""dimnames"")=List of 2
.. .. ..$ : chr  ""0"" ""90"" ""180""
    .. .. ..$ : NULL
Item         : Factor w/ 16 levels ""MAP1"",""MAP10"",..: 1 9 10 11 12 13 14 15 16 2 ...
Foo       : num  0.847 1.099 1.946 -1.099 -0.452 ...
wtsFoo          : num  0.952 0.889 2.286 0.889 0.468 ...
Close      : num  -1.798 0.202 -1.798 0.202 0.202 ...
Similar    : num  0.505 0.505 0.505 0.505 0.505 ...
Like       : num  -0.833 0.167 -0.833 0.167 0.167 ...
Task1Hard   : num  -0.89 4.11 -0.89 4.11 4.11 ...
Task2Hard: num  -1.02 2.98 -1.02 2.98 2.98 ...
</code></pre>

<p>My analysis is based on this guide to empirical logit analyses:
<a href=""http://talklab.psy.gla.ac.uk/tvw/elogit-wt.html"" rel=""nofollow"">http://talklab.psy.gla.ac.uk/tvw/elogit-wt.html</a>
So far, so good. In my regression model, Iâ€™m testing the fixed effects of Condition1 (4 levels) and Condition2 (3 levels) on Foo (the behavior, expressed as a proportion converted into empirical logit form, see link for how and why). Pair, Pair:Subject (Subject nested within Pair) and Item are included as random effects. Condition1 is between-subjects/pairs and Condition2 is within-subjects. Hereâ€™s the model Iâ€™m using in R:</p>

<pre><code>model &lt;- lmer(Foo ~ Condition1*Condition2 + (1+Condition1 | Pair) 
+ (1+Condition1 | Pair:Subject) + (1+Condition2 | Item), weights=1/wtsFoo, data)
</code></pre>

<p>This all works fine, but hereâ€™s where it gets fun. Where should the questionnaire data go? </p>

<p>Bad idea #1: Each participant has one score for each questionnaire item, so each questionnaire item type should be included as a fixed effect, so that Foo can be predicted by any of the variables discovered in the post-experiment questionnaire (things like social closeness and task difficulty). This is a terrible idea because the questionnaire items are NOT independent variables from Condition1 and Condition2, and if I include them as fixed effects it will introduce a mess of multicollinearity and will just be flat-out wrong.</p>

<p>Bad idea #2: Analyzing the questionnaire data separately. Not such a bad idea, just one that my committee doesnâ€™t like. </p>

<p>Less bad ideas: please suggest a model that allows me to observe the effects of Condition1 and Condition2 on questionnaire items (Close, Similar, Like, Task1Hard, Task2Hard) AND allows me to observe the effects of questionnaire items on Foo. Failing that, explain to me why the only good thing to do is analyze the questionnaire separately from the observation data.</p>

<p>I've read around on Stackexchange and I haven't seen this particular problem covered, although some answers come close to looking useful, I don't yet have the R or stats chops to make them work for me. If I've missed something obvious, please clue me.</p>
"
"0.109815159255115","0.116699087583415","141423","<p>I use the <code>decompose</code> function in <code>R</code> and come up with the 3 components of my monthly time series (trend, seasonal and random). If I plot the chart or look at the table, I can clearly see that the time series is affected by seasonality.</p>

<p>However, when I regress the time series onto the 11 seasonal dummy variables, all the coefficients are not statistically significant, suggesting there is no seasonality.</p>

<p>I don't understand why I come up with two very different results. Did this happen to anybody? Am I doing something wrong?</p>

<hr>

<p>I add here some useful details.</p>

<p>This is my time series and the corresponding monthly change. In both charts, you can see there is seasonality (or this is what I would like to assess). Especially, in the second chart (which is the monthly change of the series) I can see a recurrent pattern (high points and low points in the same months of the year).</p>

<p><img src=""http://i.stack.imgur.com/rILAU.jpg"" alt=""TimeSeries""></p>

<p><img src=""http://i.stack.imgur.com/LdVnv.jpg"" alt=""MonthlyChange""></p>

<p>Below is the output of the <code>decompose</code> function. I appreciate that, as @RichardHardy said, the function does not test whether there is actual seasonality. But the decomposition seems to confirm what I think.</p>

<p><img src=""http://i.stack.imgur.com/ZaVRV.jpg"" alt=""Decompose""></p>

<p>However, when I regress the time series on 11 seasonal dummy variables (January to November, excluding December) I find the following:</p>

<pre><code>    Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept) 5144454056  372840549  13.798   &lt;2e-16 ***
    Jan     -616669492  527276161  -1.170    0.248    
    Feb     -586884419  527276161  -1.113    0.271    
    Mar     -461990149  527276161  -0.876    0.385    
    Apr     -407860396  527276161  -0.774    0.443    
    May     -395942771  527276161  -0.751    0.456    
    Jun     -382312331  527276161  -0.725    0.472    
    Jul     -342137426  527276161  -0.649    0.520    
    Aug     -308931830  527276161  -0.586    0.561    
    Sep     -275129629  527276161  -0.522    0.604    
    Oct     -218035419  527276161  -0.414    0.681    
    Nov     -159814080  527276161  -0.303    0.763
</code></pre>

<p>Basically, all the seasonality coefficients are not statistically significant.</p>

<p>To run linear regression I use the following function:</p>

<p><code>lm.r = lm(Yvar~Var$Jan+Var$Feb+Var$Mar+Var$Apr+Var$May+Var$Jun+Var$Jul+Var$Aug+Var$Sep+Var$Oct+Var$Nov)</code></p>

<p>where I set up Yvar as a time series variable with monthly frequency (frequency = 12).</p>

<p>I also try to take into account the trending component of the time series including a trend variable to the regression. However, the result does not change.</p>

<pre><code>                  Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept) 3600646404   96286811  37.395   &lt;2e-16 ***
    Jan     -144950487  117138294  -1.237    0.222    
    Feb     -158048960  116963281  -1.351    0.183    
    Mar      -76038236  116804709  -0.651    0.518    
    Apr      -64792029  116662646  -0.555    0.581    
    May      -95757949  116537153  -0.822    0.415    
    Jun     -125011055  116428283  -1.074    0.288    
    Jul     -127719697  116336082  -1.098    0.278    
    Aug     -137397646  116260591  -1.182    0.243    
    Sep     -146478991  116201842  -1.261    0.214    
    Oct     -132268327  116159860  -1.139    0.261    
    Nov     -116930534  116134664  -1.007    0.319    
    trend     42883546    1396782  30.702   &lt;2e-16 ***
</code></pre>

<p>Hence my question is: am I doing something wrong in the regression analysis?</p>
"
"0.0446990156267674","0.0441081091391231","141509","<p>I have 12 questions weighted on a 5-point Likert scale.
20 people responded to each question, so we have 20 rows and 12 columns, with each cell ranging from 1 to 5. Questions 1-4 measure a random variable R1, 5-8 measure R2, and 9-12 measure variable R3. Finally, these 20 people come from 4 different areas, A,B,C and D. So I want to use respondents from A as some control group, then see how different the other groups responded.</p>

<p>So my question is; should I use a regression analysis or is there a better option? I am doing the analysis with R</p>
"
"0.116018205409926","0.128795042662908","141820","<p>I want to find which soil variables better explain plant productivity, using a database that contains information for about 100 forests plots across Europe.
These plots have only one species per plot, but overall there are 4 different species in the dataset. These plots also have different climate conditions (temperature, precipitation,...). My final goal is finding out which combination of the more than 20 different soil variables better explain plant productivity. However, both climate and species may confound the analysis because both affect plant growth (some species grow more than others, and plants grown in warmer climates may grow more). I am only interested in plant growth due to soil characteristics, so I need to get rid of the species and climate effects on plant productivity that may confound the analysis. According to what I have read I could just include all variables in the model: soil, climate and species (factor of 4 levels), like this:</p>

<pre><code>fit &lt;- lm(scale(IVMean)~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+
                        scale(EXCHCA)+scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+
                        scale(EXCHNA)+scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+
                        scale(S_SO4)+scale(N_NH4)+scale(BS)+scale(CN)+scale(Temp)+
                        scale(Precip)+scale(Rad)+scale(PET)+species)
</code></pre>

<p>IVMean = mean stem volume increment (productivity). Note climate variables (temperature, precipitation, radiation and potential evapotranspiration -PET-) and species at the end, and the standardisation of all variables with <code>scale()</code>.</p>

<p>After this, I could run a stepwise regression analysis to preliminarily find which variables are the most important explaining plant productivity.</p>

<pre><code>library(MASS)
step &lt;- stepAIC(fit, direction=""backward"")
step$anova # display results
</code></pre>

<p>Which renders the following best minimal model:</p>

<pre><code>Final Model:
scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
    scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species

&gt; model &lt;- lm(scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
+               scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species, 
+             data = icp)
&gt; summary(model)

Call:
lm(formula = scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + 
    scale(EXCHMG) + scale(EXCHMN) + scale(BS) + scale(Temp) + 
    scale(PET) + species, data = icp)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.13836 -0.41522 -0.02816  0.35094  1.65587 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -0.37587    0.16967  -2.215 0.030897 *  
scale(PHCACL2)      0.58776    0.20617   2.851 0.006128 ** 
scale(EXCHCA)      -0.38061    0.19025  -2.001 0.050381 .  
scale(EXCHMG)      -0.37374    0.14686  -2.545 0.013769 *  
scale(EXCHMN)       0.13102    0.09970   1.314 0.194241    
scale(BS)           0.39502    0.19428   2.033 0.046871 *  
scale(Temp)         1.34654    0.32033   4.204 9.74e-05 ***
scale(PET)         -0.62177    0.29749  -2.090 0.041250 *  
speciesoak         -1.24553    0.34788  -3.580 0.000726 ***
speciespicea_abies  1.38679    0.25031   5.540 8.79e-07 ***
speciesscots_pine   0.02627    0.25960   0.101 0.919769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.6411 on 55 degrees of freedom
Multiple R-squared:  0.6522,    Adjusted R-squared:  0.5889 
F-statistic: 10.31 on 10 and 55 DF,  p-value: 1.602e-09
</code></pre>

<p>The final model includes 5 soil variables, 2 out of 4 climate variables, and species. So far so good?</p>

<p>However, this seems to be not good enough for my supervisor. Rather, he asked me to do an analysis of the residuals to â€œget rid of climate and species effectsâ€! To be honest, I have no idea what he is talking about, and I was afraid to ask because he sounded like something I should know since my childhood. Perhaps he meant I should study which SOIL variables can explain the residuals of productivity ~ climate * species? Please, help me find out which type of analysis of the residuals would make sense to focus on soil effects eliminating climate and species effects.</p>

<p>This is the only thing I can think of:  </p>

<pre><code># Study the importance of confounding effects:
confounding     &lt;- IVMean ~ (Temp + Precip + PET + Rad) * species 
confounding.res &lt;- residuals(confounding)
lm(confounding.res ~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+scale(EXCHCA)+
                    scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+scale(EXCHNA)+
                    scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+scale(S_SO4)+
                    scale(N_NH4)+scale(BS)+scale(CN))
</code></pre>

<p>This way maybe I could study which soil variables explain what climate and species effects could not explain? I donâ€™t know if it makes any sense. I am open to suggestions and alternatives. </p>
"
"0.126647210942508","0.124972975894182","141844","<p>I tried to plot the results of an ordered logistic regression analysis by calculating the probabilities of endorsing every answer category of the dependent variable (6-point Likert scale, ranging from ""1"" to ""6""). However, I've received strange probabilities when I calculated the probabilities based on this formula: $\rm{Pr}(y_i \le k|X_i) = \rm{logit}^{-1}(X_i\beta)$.</p>

<p>Below you see how exactly I tried to calculate the probabilities and plot the results of the ordered logistic regression model (<code>m2</code>) that I fitted using the <code>polr</code> function (<code>MASS</code> package). The probabilities (<code>probLALR</code>) that I calculated and used to plot an ""expected mean score"" are puzzling as the expcected mean score in the plot increases along the RIV.st continuum while the coefficient for <code>RIV.st</code> is negative (-0.1636). I would have expected that the expected mean score decreases due to the negative main effect of <code>RIV.st</code> and the irrelevance of the interaction terms for the low admiration and low rivalry condition (LALR) of the current 2 by 2 design (first factor = <code>f.adm</code>; second factor = <code>f.riv</code>; dummy coding 0 and 1).</p>

<p>Any idea of how to make sense of the found pattern? Is this the right way to calculate the probabilities? The way I used the intercepts in the formula to calculate the probabilities might be problematic (cf., <a href=""https://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression"">Negative coefficient in ordered logistic regression</a>).</p>

<pre><code>m2 &lt;- polr(short.f ~ 1 + f.adm*f.riv + f.adm*RIV.st + f.riv*RIV.st, data=sampleNS)

# f.adm  = dummy (first factor of 2 by 2 design);
# f.riv  = dummy (second factor of 2 by 2 design);
# RIV.st = continuous predictor (standardized)
summary(m2)
Coefficients:
                Value Std. Error t value
f.adm1         1.0203    0.14959  6.8203
f.riv1        -0.8611    0.14535 -5.9240
RIV.st        -0.1636    0.09398 -1.7403
f.adm1:f.riv1 -1.2793    0.20759 -6.1625
f.adm1:RIV.st  0.0390    0.10584  0.3685
f.riv1:RIV.st  0.6989    0.10759  6.4953

Intercepts:
    Value    Std. Error t value 
1|2  -2.6563   0.1389   -19.1278
2|3  -1.2139   0.1136   -10.6898
3|4  -0.3598   0.1069    -3.3660
4|5   0.9861   0.1121     8.7967
5|6   3.1997   0.1720    18.6008
</code></pre>

<p>Here you see how I tried to calculate the probabilities (<code>probLALR</code>) for 1 of the 4 conditions of the 2 by 2 design:</p>

<pre><code>inv.logit  &lt;- function(x){ return(exp(x)/(1+exp(x))) }
Pred       &lt;- seq(-3, 3, by=0.01)
b = c(-2.6563,-1.2139,-0.3598,0.9861,3.1997) # intercepts of model m2
a = c(1.0203,-0.8611,-0.1636,-1.2793,0.0390,0.6989) # coefficients of m2
probLALR   &lt;- data.frame(matrix(NA,601,5))
for (k in 1:5){ 
    probLALR[,k] &lt;- inv.logit(b[k] + a[1]*0 + a[2]*0 + 
                               a[3]*Pred  + a[4]*0*0 + 
                               a[5]*Pred*0 + a[6]*Pred*0)
}

plot(Pred,probLALR[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,probLALR[,2],col=""red"")             # p(1 or 2)
lines(Pred,probLALR[,3],col=""green"")           # P(1 or 2 or 3)
lines(Pred,probLALR[,4],col=""orange"")          # P(1 or 2 or 3 or 4)
lines(Pred,probLALR[,5],col=""orange"")          # P(1 or 2 or 3 or 4 or 5)

# option response functions:

orc = matrix(NA,601,6)
orc[,6] = 1-probLALR[,5]        # prob of 6
orc[,5]= probLALR[,5]-probLALR[,4]  # prob of 5
orc[,4]= probLALR[,4]-probLALR[,3]  # prob of 4
orc[,3]= probLALR[,3]-probLALR[,2]  # prob of 3
orc[,2]= probLALR[,2]-probLALR[,1]  # prob of 2
orc[,1]= probLALR[,1]           # prob of 1


plot(Pred,orc[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,orc[,2],col=""red"")             # p(2)
lines(Pred,orc[,3],col=""green"")           # P(3)
lines(Pred,orc[,4],col=""orange"")          # P(4)
lines(Pred,orc[,5],col=""purple"")          # P(5)
lines(Pred,orc[,6],col=""purple"")          # P(6)

# mean score

mean = orc[,1]*1+orc[,2]*2+orc[,3]*3+orc[,4]*4+orc[,5]*5+orc[,6]*6
plot(Pred,mean,type=""l"",xlab=""RIV.st"",ylab=""expected mean score"",ylim=c(1,6))  
</code></pre>
"
"0.178878210943523","0.176513499017262","142489","<p>I'm analysing PAM fluorescence data from an experimental set-up that I duplicated from an earlier experiment with a missing control. That's why I haven't given the statistics of the experiment much (if any) thought in advance.</p>

<p>The set-up consisted of 8 containers with peat moss (<em>Sphagnum magellanicum</em>), divided over 4 treatments, so that each treatment was performed in duplicate. At regular (weekly) intervals, over the course of 3 months, I performed life PAM fluorescence measurements on a number of capitula (growth tops) in each container to determine a kinetic response curve for each of these capitula.</p>

<p>To minimize intraleaf (in my case, intra<em>capitula</em>) variance, ideally, PAM fluorescence measurements would have been repeated for the same leaf every week in the 3-month time series, but for practical reasons, my AOIs (areas of interest) for the fluorescence meter where located on different capitula every week. This is also my first subquestion: can I consider measurements at different time points in the same container as <em>repeated measures</em>, or would this only be valid if I had been measuring the same AOIs every week? And does this depend on whether I aggregate the measured values of the different AOIs per container before further analysis?</p>

<p>After nightfall, once every week, for 5â€“7 AOIs in each container, I determined a kinetic curve, for which the PAM software performs 20 measurements. The first measurement represents the dark-adapted fluorescence values, after which an actinic light source (at a wavelength that can facilitate photosynthesis) is started for the 19 remaining measurements. From the start of the kinetic curve (the dark adapted $\phi_{PSII}$ values), I determine $F_v/F_m$ and from the end of the curve (the flat part), I determine $\text{mean}(\phi_{PSII})$. $\phi_{PSII}$ and $F_v/F_m$ measure the quantum yield of photosystem II and the max. efficiency of photosystem II, respectively; $F_v/F_m = \phi_{PSII}$ in a dark-adapted state.</p>

<p>I'm interested in building two models, one in which the response (dependent) variable is $\phi_{PSII}$ and one in which it is $F_v/F_m$. The (independent) predictor variables are:</p>

<ul>
<li><code>AOI</code> (factor): a number between 1â€“6; </li>
<li><code>Container</code> (factor): a number between 1â€“8; </li>
<li><code>Treatment</code>: (factor): a number between 1â€“4; and</li>
<li><code>DaysTreated</code> (integer): the number of days since the treatments began.</li>
</ul>

<p>My guess is that I should treat <code>AOI</code> and <code>Container</code> as random effects variables, with <code>AOI</code> nested in <code>Container</code> and <code>Container</code> nested in the fixed effect variable <code>Treatment</code>. <code>DaysTreated</code>, then, would be my continuous predictor (covariate). For $\phi_{PSII}$, I would model this in R like this:</p>

<pre><code>library(nlme)
YII_m1 &lt;- lme(mean_YII ~ DaysTreated * Treatment,
              random = ~1 | Container / AOI,
              method = ""ML"",
              data = fluor_aoi)
# fluor_aoi is a data-frame in which each AOI kinetic curve is
# aggregated into one row, where mean_YII = mean( YII[15:19] )
# and FvFm = YII[1]
</code></pre>

<p>I'm not sure if this is the most parsimious model. To find out, I want to try different models with different fixed effects but all with the same random effects. <code>anova.lme()</code> warned me that comparing between these models is a <a href=""http://stats.stackexchange.com/questions/116770/"">no-go</a> when using the default method (<code>method = ""REML""</code>), which is why I use <code>method = ""ML""</code>.</p>

<pre><code>anova(YII_m1, # ~ DaysTreated * Treatment
      YII_m2, # ~ DaysTreated:Treatment + Treatment
      YII_m3, # ~ DaysTreated:Treatment + DaysTreated
      YII_m4, # ~ DaysTreated:Treatment
      YII_m5, # ~ DaysTreated + Treatment
      YII_m6, # ~ DaysTreated
      YII_m7  # ~ Treatment
     )

       Model df       AIC       BIC   logLik   Test  L.Ratio p-value
YII_m1     1 11 -2390.337 -2340.578 1206.168                        
YII_m2     2 11 -2390.337 -2340.578 1206.168                        
YII_m3     3  8 -2390.347 -2354.158 1203.173 2 vs 3  5.99019  0.1121
YII_m4     4  8 -2390.347 -2354.158 1203.173                        
YII_m5     5  8 -2366.481 -2330.293 1191.241                        
YII_m6     6  5 -2363.842 -2341.224 1186.921 5 vs 6  8.63915  0.0345
YII_m7     7  7 -2264.868 -2233.203 1139.434 6 vs 7 94.97389  &lt;.0001
</code></pre>

<p>I would have liked it if the <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">best fit</a> was model 2 with the fixed effects formula <code>~ DaysTreated:Treatment + Treatment</code>, because my expectation at the onset of my experiment was to see a decline in <em>Sphagnum</em> vitality, but only for some of the treatments and hopefully not in the controls. (The acclimatization period was very long, hoping that any effects on the mosses of the new (greenhouse) environment would have flattened out by the onset of the treatments.)</p>

<p><strong>Edit 2015-May-1:</strong> First I compared only 6 models; model 4 was missing from my initial question. Also, I forgot to factorize treatment, so that instead of model 2, now, different models give the â€˜best fitâ€™.</p>

<p>Anyway, so far (unless you tell me otherwise), I feel I can continue to use model 2, which also best fits the visual observation that 4 of the 8 containers where doing very badly at the end of the experiment while the other 4 seemed to do ok.</p>

<pre><code>anova(YII_m2)
                  numDF denDF  F-value p-value
(Intercept)           1   620 526.9698  &lt;.0001
Treatment             3     4   5.0769  0.0753
DaysTreated:Treatment 4   620  36.4539  &lt;.0001
</code></pre>

<p>An ANCOVA test on model 2 reveals that only the interaction between <code>DaysTreated</code> and <code>Treatment</code> is significant, which makes sense to me, given that the containers started out in roughly the same condition after acclimatization. There was visible difference between containers in the same treatments, but that should have been taken care of by correcting for the random error effect.</p>

<p>Mean $\phi_{PSII}$ plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments:</p>

<p><img src=""http://i.stack.imgur.com/GgGhG.png"" alt=""Mean Y_II plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments.""></p>

<p>Now that I've made an <em>attempt</em> at constructing and testing a somewhat decent model (which I'd love to receive criticism on), I'd like to perform a multiple pairwise comparison to find out which treatments diverge significantly from each other over time, but I have no idea what is the proper way to approach this.</p>

<p>Also, I want to try a linear correlation, but again, I'm clueless as to how. Is there an appropriate way to integrate this in my model or should I try to model a regression per treatment? </p>

<p>Please forgive the ignorance in my approach and my questions. I'm a BSc student whose statistical background mainly consists of a brief entry-level course, followed by a recipe-level R course. RTFM comments are definitely welcome, as long as they include a link to TFM.</p>
"
"0.0316069770620507","0.0311891430775903","142580","<p>I have calculated simple slopes for a multiple regression using simpleSlope from package pequod(). Is there a way to easily compute an effect size for a single simple slope analysis from the output?</p>

<p>Example:</p>

<pre><code>library(pequod)

dat &lt;- data.frame(""DV""=rnorm(20,0,1), ""PRED""=rnorm(20,0,1),
                  ""MOD1""=rnorm(20,0,1), ""MOD2""=rnorm(20,0,1))

fit &lt;- lmres(DV~PRED*MOD1*MOD2, dat)

slopes &lt;- simpleSlope(fit, pred=""PRED"", mod1=""MOD1"", mod2=""MOD2"")
</code></pre>
"
"0.113960576459638","0.112454054604034","143399","<p>I am trying to do some survival analysis in R and as a starting point, I want to make sure I can replicate a previous analysis. I notice differences and I will demonstrate them here. I feel like there is a daft explanation the user community can provide.</p>

<p>Let's start by using the ovarian dataset in R. We will fit a weibull distribution with residual disease and ECOG performance status as covariates. Then we will print the output using proportional hazards specification to match Stata's HR output.</p>

<pre><code>require(survival)
require(flexsurvreg)
require(dplyr)
attach(ovarian)
ovarian &lt;- ovarian %&gt;% mutate(resid.ds=resid.ds-1, ecog.ps=ecog.ps-1, futime=futime/365.25) # Make it 0,1
write.dta(ovarian %&gt;% mutate(resid.ds=resid.ds+1, ecog.ps=ecog.ps+1), ""data/ovarian.dta"") # Write dta
s.weib &lt;- flexsurvreg(Surv(futime, fustat) ~ age + resid.ds + ecog.ps, data=ovarian, dist=""weibull"") # Fit weibull

# Function to convert AFT to PH
flexsurvPHcoef &lt;- function(x) return(c(exp(x$coef[-(1:2)]*(-1)*exp(x$coef[""shape""])), exp(x$coef[""shape""]), exp(-x$coef[""scale""])))
flexsurvPHcoef(s.weib)
     age     resid.ds      ecog.ps        shape        scale 
1.150309872 2.702038142 1.060599568 1.752446996 0.002799146 
</code></pre>

<hr>

<p>Now let's compare to Stata.</p>

<pre><code>quietly stset futime, f(fustat)
streg age i.resid_ds i.ecog_ps, d(weib)
Weibull regression -- log relative-hazard form 

No. of subjects =           26                     Number of obs   =        26
No. of failures =           12
Time at risk    =  42.67761807
                                                   LR chi2(3)      =     17.88
Log likelihood  =   -20.828884                     Prob &gt; chi2     =    0.0005

------------------------------------------------------------------------------
          _t | Haz. Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         age |    1.15031   .0515502     3.12   0.002     1.053583    1.255916
  2.resid_ds |   2.702038    2.00098     1.34   0.180     .6329054    11.53571
   2.ecog_ps |     1.0606   .6620784     0.09   0.925     .3120252    3.605066
       _cons |   .0000336   .0000906    -3.82   0.000     1.69e-07    .0066602
-------------+----------------------------------------------------------------
       /ln_p |   .5610131    .238929     2.35   0.019     .0927209    1.029305
-------------+----------------------------------------------------------------
           p |   1.752447   .4187103                      1.097156     2.79912
         1/p |   .5706307   .1363402                      .3572551    .9114478
------------------------------------------------------------------------------
</code></pre>

<p>We can see the resid.ds and ecog.ps are the same. As well, the shape. But the scale is off. </p>

<p><strong>So my question is, any thoughts on why only the scale parameter is different?</strong></p>

<hr>

<p>Let's move on to estimation. flexsurvreg has an interesting ability to predict at multiple time points. Let's assume a woman is 45, has residual disease and ECOG is 0.</p>

<pre><code>summary(s.weib, newdata=data.frame(age=45, resid.ds=1, ecog.ps=0), t=c(1:5))
age=45, resid.ds=1, ecog.ps=0 
  time       est         lcl       ucl
1    1 0.9517266 0.807744496 0.9930684
2    2 0.8464501 0.488174229 0.9681961
3    3 0.7122949 0.193976895 0.9285058
4    4 0.5702529 0.038982054 0.8762493
5    5 0.4358519 0.002925942 0.8097636
</code></pre>

<p>Not the most precise. Anyways, how does this compare to how weibull is parameterized (PH). From Stata's manual:
$$
S = exp(-exp(x{B}){t}^p)
$$</p>

<p>No we have to use the log scale coefficients. But let's go ahead and try and predict this manually.</p>

<pre><code>p.weib &lt;- function(cons, age, resid, t, p) return(exp(-exp(cons+age*45+resid)*t^p))
coef &lt;- flexsurvPHcoef(s.weib)
data.frame(time=1:5) %&gt;% mutate(S=p.weib(log(coef[""scale""]), log(coef[""age""]), log(coef[""resid.ds""]), time, coef[""shape""]), S=round(S, 5))
  time       S
1    1 0.01617
2    2 0.00000
3    3 0.00000
4    4 0.00000
5    5 0.00000
</code></pre>

<p>That obviously didn't really work out. What happens if we substitute the scale from Stata (-10.30166)?</p>

<pre><code>data.frame(time=1:5) %&gt;% mutate(S=p.weib(-10.30166, log(coef[""age""]), log(coef[""resid.ds""]), time, coef[""shape""]), S=round(S, 5))
  time       S
1    1 0.95173
2    2 0.84645
3    3 0.71230
4    4 0.57025
5    5 0.43585
</code></pre>

<p>It's just as flexsurvreg predicted. So now that I write this, maybe I've transformed the AFT scale incorrectly. Back to my original question, why are the scales different?</p>

<p>Finally, some aside questions. I couldn't reproduce my actual data problem. I can't really put up that much data here. Again, <code>summary.flexsurvreg</code> gives me predicted estimates that are not the same as the <code>p.weib</code>. But, when I substitute the scale from Stata for p.weib, I get estimates different to the original summary.flexsurvreg, albeit much closer than with the scale from my log(coef) of PH. Any thoughts?</p>
"
"NaN","NaN","143714","<p>Suppose in a regression analysis in R, I have a factor type independent variable with 3 levels in my train dataset. But in the test data set that same factor variable has 5 levels. Therefore I can not predict the response values for test dataset. What should be done in this case?</p>
"
"NaN","NaN","144520","<p>I would like to conduct nominal logistic regression analysis using <code>molt</code>in R, but don't know how to enter the data below into R. Any advice?</p>

<p><img src=""http://i.stack.imgur.com/sAnc4.png"" alt=""enter image description here""></p>
"
"0.0446990156267674","0.0441081091391231","144650","<p>Numerous books and lecture slides start to discuss regression analysis as follows:<br>
$$Y=X\beta+\epsilon;\text{ where, } Y\sim N(X\beta, \sigma^2 ) \text{ and } \epsilon\sim N(0, \sigma^2)$$
George Seber wrote in his book (Linear Regression Analysis, Ed 2) at page 42:
$$Var[Y] = Var[Y-X\beta] = Var[\epsilon]$$
I was intended to verify it as follows:</p>

<pre><code>set.seed(123)
n=10000
int=rep(1,n)
x1=rnorm(n,5,3)
x2=rnorm(n, 20, 10)
x3=rnorm(n, -10, 2)
x=cbind(int,x1,x2, x3)
beta=c(10, 2, 0.5, 1.5)
err=rnorm(n, 0, 7)
y=x%*%beta+err
mean(err) # very close to zero
var(err) # very close to 49
mean(y) # very close to 15
var(y) # very close to 118
</code></pre>

<p>Somehow I triggered to check whether <code>R square</code> have a role in equating $Var[Y]$ and $Var[\epsilon]$. Then I checked:</p>

<pre><code>var(y)*(1-(summary(lm(y~x1+x2+x3))$r.squared)) # very close to 49
</code></pre>

<p>Yes it is. So $Var[\epsilon] = Var[Y]\times (1-R^2)$. I do not know what I am missing here! Why George Seber and many others never mention it? Certainly, I am wrong, not George Seber. But what is my mistake?<br>
Any lights on my confusion will be appreciated.<br>
Thanks.</p>
"
"0.0632139541241014","0.0623782861551805","144690","<p>I have high dimensional data (1000 genes for 100 patients). I have few clinical parameters for this data, some demographic data and some different metal levels in blood. In trying to evaluate how each gene responds, I'm trying to fit multiple linear regression using all these clinical, demographic and  metal parameters, using gene as my outcome variable. Since I have 1000's of these genes, I am not sure how to go about model selection. Some genes differentiate on race, some on other factors. Some have very low $R^2$ values, I tried to see a few dozens. I am trying to perform univariate analysis to eliminate few genes which are not useful and then take the rest for the multivariate approach.</p>
"
"0.0999500374687773","0.0887658573546531","145657","<p>Basically I'm attempting to recreate the results of an example from class in R. What I'm trying to do is decide whether it's best to use a single regression line for an entire data set or two lines based on a categorical variable. The teacher indicates there are three steps to this:</p>

<ol>
<li>Determine if two different lines are required</li>
<li>If yes, determine if they differ in slope</li>
<li>If yes, determine if they differ in intercept</li>
</ol>

<p>Here is my data:</p>

<pre><code>&gt; example
   Predictor Response Group
1         21       11     A
2         24       21     A
3         26       23     A
4         29       29     A
5         35       34     A
6         45       51     A
7         51       59     A
8         68       73     A
9         72       83     A
10        76       95     A
11        17       11     B
12        21       55     B
13        26       34     B
14        28       44     B
15        32       26     B
16        36       34     B
17        40       15     B
18        45       21     B
19        51       16     B
20        68       21     B
</code></pre>

<p>I've realized that if I add the interaction and group terms to the model:</p>

<pre><code>ex_mod &lt;- lm(Response ~ Predictor,data = example)
ex_mod2 &lt;- lm(Response ~ Predictor + Group + Predictor:Group,data = example)
</code></pre>

<p>And then perform ANOVA on this. I get the right answer for step 1:</p>

<pre><code>&gt; anova(ex_mod,ex_mod2)
Analysis of Variance Table

Model 1: Response ~ Predictor
Model 2: Response ~ Predictor + Group + Predictor:Group
  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    
1     18 6616.4                                 
2     16 1583.8  2    5032.6 25.42 1.078e-05 ***   
</code></pre>

<p>Which means I need different lines, but now I need to know if they differ in slope or y-intercept or both. And here is where I'm stuck. I cant seem to get the right answer (F = 293.17 for slope, and F = 170.77 for intercept). </p>

<p>The teacher indicates that the next steps are: 1) to generate RSS in which the slope is fixed,but the y-intercepts are allowed to vary; and 2) generate RSS in which the y-intercept is fixed, but the slopes are allowed to vary.</p>

<p>I apologize if the question is confusing or simplistic, but I dont know how to proceed from here.</p>

<p>Thanks</p>
"
"0.070675349274022","0.0697410440814588","145790","<p>I'm trying to figure out how to produce an ANOVA Table in R for a multiple regression model. So far I can only produce it for each regressor, and the Mean Square is calculating as the same as Sum Of Squares.</p>

<pre><code>&gt; anova(nflwin.lm)
Analysis of Variance Table

Response: wins
             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
pass_yard     1  76.193  76.193  26.172 3.100e-05 ***
percent_rush  1 139.501 139.501  47.918 3.698e-07 ***
oppo_rush     1  41.400  41.400  14.221 0.0009378 ***
Residuals    24  69.870   2.911                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I'm trying to produce something like</p>

<pre><code>Analysis of Variance Table

Response: wins
             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
Model         3  76.193  76.193  26.172 3.100e-05 ***
Residuals    24  69.870   2.911                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.164339512240171","0.162166997194876","145849","<p>Iâ€™ve got a question concerning the R package <em>strucchange</em> that I use for testing and dating structural breaks in my PhD thesis.  To be specific, I use the generalized fluctuation test framework with CUSUM/MOSUM and in particular Moving Estimates (<strong>ME</strong>) tests for my analysis. Thus, the following description focuses on the ME test, but in principle is more general to all fluctuation tests.</p>

<p><strong>The problem:</strong> I am testing time series data for structural breaks with the ME test that draws on the function <strong>efp</strong> provided by strucchange. Given the nature of time series data, I want to tackle potential heteroskedasticity and autocorrelation in the data. Strucchange provides some functionality with respect to calculating <em>heteroskedasticity</em> (<strong>HC</strong>) and <em>autocorrelation</em> (<strong>HAC</strong>) consistent covariance matrices,  e.g., the approaches suggested by Newey-West (1987) or Andrews (1991). </p>

<p>However, this functionality in strucchange is limited to the function <strong>gefp</strong> that calculates Generalized Empirical M-Fluctuation Processes that as far as I know does not allow to perform estimates-based tests such as the ME test. Thus, I cannot use <strong>efp</strong> to estimate ME tests (or other tests that are available in this function) using HAC covariance matrices. </p>

<p><strong>The question:</strong> Does anybody know how I could make use of the <strong>efp</strong> function in <em>strucchange</em> for testing and dating structural changes but use HAC covariance matrices to take heteroskedasticity and autocorrelation into account? Maybe there is some way to use the sandwich package for this?</p>

<p><strong>Many thanks for any help!</strong></p>

<p>Here is a minimal working example to show the problem</p>

<pre><code>library(foreign)
library(strucchange)

data(""Nile"")

#using the function efp to perform a moving estimates test
#assuming sperical disturbances
ocus.nile &lt;- efp(Nile ~ 1, type = ""ME"")
sctest(ocus.nile)

#applying the vcov function with the kernHAC option to take heteroskedasticity and autocorrelation does not work, i.e., the option is not used and the result is the same
sctest(ocus.nile, vcov=kernHAC)

#using the function gefp to perform a generalized M-fluctuation process however works with vcov
#assuming spherical disturbances
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm)
sctest(ocus.nile2)

#controlling for heteroskedasticity and autocorrelation using an appropriate covariance matrix changes the result, i.e. works
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm, vcov= kernHAC)
sctest(ocus.nile2)
</code></pre>

<p><strong>Some background</strong></p>

<p>Though probably not necessary, here is some more in-depth background about the problem for the interested reader (and the archive). The formulas are taken from Zeileis et al., 2005, â€Monitoring structural change in dynamic econometric modelsâ€. </p>

<p>The ME test is used to detect structural breaks in the standard linear regression model over time. What it does it in essence partitioning the data and rather than estimating the regression based on the whole sample, it sequentially moves â€œthroughâ€ time in a fixed-width windows containing only a sub-sample of the observations and in each window it estimates the model. These estimates are used to the computation of empirical fluctuation processes that capture fluctuations in regression coefficients and residuals over time. Significant fluctuations of the coefficients are signs of a structural break in the regression. The test statistic of the Moving estimates test is</p>

<p><img src=""http://i.stack.imgur.com/qQ0FY.png"" alt=""Moving estimates test statistic""></p>

<p>where <em>n</em> is the number of observations, <em>h</em> is the bandwith (how many percent of the total number of observations are used for the window), <em>nh</em> is thus the size of the window, Q_(n)=X_(n)^T X_(N)/n, i=[k+t(n-k)], and sigma^2 is an estimate of the variance. The way I understand the above statistic is that it compares the difference between the sub-sample estimate of beta with the whole sample (the window) estimate and how this difference develops over time. A zero difference would indicate a sub-sample estimate that perfectly equals the whole-sample estimate, which would indicate perfect stability of the coefficient. In my understanding, the efp function in strucchange calculates sigma^2 based on the standard OLS residuals u^ i.e., sigma^2=(1/n-k)âˆ‘_(i=1)^n u_i^2 . Thus, in the presence of heteroskedasticity or autocorrelation, the OLS assumption of spherical disturbances will be violated. Thus, ideally, sigma^2 should be estimated based on a HAC covariance matrix to avoid wrong inference.</p>

<p>The question that comes to my mind is whether there is a way to use the ME test based on a HAC estimate. If not, it seems to me that it is limited to spherical disturbances of residuals, which seems to be violated in most applications.</p>
"
"0.0632139541241014","0.0623782861551805","148529","<p>I am estimating a Weighted Spatial Simultaneous Autoregression Model (<code>spdep::spautolm</code> --> <a href=""http://cran.r-project.org/web/packages/spdep/spdep.pdf"" rel=""nofollow"">Link</a>) in R and I would like to do some residual analysis.</p>

<p>Unfortunately functions such as <code>hatvalues</code>,<code>cooks.distance</code> or <code>plot.lm</code> do not work for <code>spautolm</code> objects. Yet, I would like to calculate leverages and cook distances for my model (see also <a href=""https://stackoverflow.com/questions/29747519/how-to-do-residual-regression-deletion-diagnostics-on-spautolm-sarlm-objects/29756024#29756024"">my post on stackoverflow</a>).</p>

<p>My Model looks like this:</p>

<p>$Y = X^T\beta + \lambda W(Y âˆ’ X^T\beta)+ \epsilon$ with $\epsilon\sim^{iid} N(0,\sigma^2)$</p>

<p>Thus:</p>

<p>$Var[Y]=\Sigma_{SAR} = \sigmaÂ²(I-\lambda W)^{-1}V(I-\lambda W)^{-1}$ with $V=diag[1/n_i]$</p>

<p>$\rho$ is my spatial autoregression parameter and $W$ the matrix that represents spatial dependence.</p>

<p>Obviously, a simple calculation of the hatmatrix $H$ via $H=X(X^TX)^{-1}X^T$ is not valid here due to the weighting and the modeled spatial autocorrelation.</p>

<p><strong>Any ideas how to calculate the leverages and cook's distances by hand in R for this model?</strong></p>
"
"0.0645174717615637","0.0763974860547543","148808","<p>I'm trying to evaluate the value of an object, depending on his characteristics. In order to do this, I'm building the following regression model <code>price ~ .</code>, using similar objects and for each variable I got min 20 observations</p>

<p>I encountered following problem: none of the regression models worked for all my data, so I decided to use all of the followings methods:</p>

<pre><code>model.lm &lt;- lm(price ~ .)
model.lmLog &lt;- lm(log(price) ~ .)
model.ltsReg &lt;- ltsReg(price ~ .)
model.ltsRegLog &lt;- lts(log(price) ~ .)
model.lmrob &lt;- lmrob(price ~ .)
model.lmrobLog &lt;- lmrob(log(price) ~ .)
model.lmRob &lt;- lmRob(price ~ .)
model.lmRobLog &lt;- lmRob(log(price) ~ .)
model.glm &lt;- glm(price ~ .)
model.glmLog &lt;- glm(price ~ ., family=gaussian(link=""log""))
</code></pre>

<p>My question is: how can I decide which of this models fits best for the current data, without plotting the results?</p>

<p>As far as I know, the <code>r-squared</code> aren't trusty, because I the data is corrupted, so will be the <code>r-squared</code>.</p>

<p>Any ideas?</p>

<p>Thank you!</p>

<p><strong>[UPDATE]</strong></p>

<p>what do you think about using <code>BIC</code> or <code>AIC</code> and choosing the one with the lowest value?</p>

<p>what do you think about choosing the variables for the regression upon the analysis of <code>anova</code>?</p>

<p>I have 17 variables from which 10 are dummy variables, is that a problem?</p>
"
"0.100365631932749","0.108042360909843","149064","<p>I have a nominal categorical predictor and a continuous dependent variable..I want to perform linear regression using lm in R. If the contrasts are such that the resulting dummy variables are uncorrelated then the regression is merely the direct linear combination of dummy variables weighted by their respective coefficients obtained from regression  of continuous variable with individual dummy variable..To have this advantage what way should the categorical predictor be contrast coded?I found this method <a href=""http://www.psychstat.missouristate.edu/multibook/mlt08m.html"" rel=""nofollow"">here</a> ..
It is helpful but the only problem is the order seems to be important here..The relation between only adjacent categories can  be interpreted from the result of linear regression..</p>

<p>So my question is - for nominal categorical predictor is there anyway to get good insights about dependent variable at category level of the predictor  from regression analysis.</p>

<p><strong>edit</strong> :</p>

<p>I'd like to  provide some clarifications here</p>

<p>Why do i need uncorrelated dummies? </p>

<p>bcoz in case of uncorrelated dummies i need not worry about which dummy enters the regression model first. The p value for the dummy1 is different when it enters the model second when compared to that when it enters first..By 'enters the model' i mean stepwise linear regression..So to avoid that problems i want them to be uncorrelated.</p>

<p>But if you see the pain vs treatment regression from the link provided by me the order certainly matters while doing contrast coding..I have no prior knowledge of the categories of my nominal category variable..so i cant order them like in pain vs treatment case. For more details - my dependent variable is Sales and category variable is product category which has 15 categories.</p>
"
"0.0547448901451359","0.0540211804549215","149502","<p>I want to study the internal Italian migration using the network analysis. </p>

<p>My nodes are the Italian cities, the edges are people who move from a city to another. I built my edge list in SPSS. I have three columns (source node, target node and weight). </p>

<p>Other variables in my database are relative to the node, some of those are numerical (i.e. city population, GDP pro capita) others categorical (i.e. if in the city there is a university, quality of life). </p>

<p>Other variables are relative to the actors who moves from a node (city) to another: gender, age, and so on. </p>

<p>I want to study the relevance of these variables to structure the internal Italian migration network. In other words I would like to construct a regression model (may be logit) where the dependent variable is my (valued) network and the independent variables are the attributes of nodes and edges. The aim is to understand which of those attributes explains the structure of the network. </p>

<p>How can I do it? Do I need to implement random graph models? p2 or p *? Is there a simple tutorial for R (sna, ergm or other packages)? </p>
"
"0.0547448901451359","0.0540211804549215","151547","<p>I am looking for examples where linear regression analysis is used in answering real problems. That is, from formulating real questions as a statistical question, validating assumptions so on to making conclusions.</p>

<p>I think I have a reasonable understanding of various statistical methods. I want to be able to put them in a ordered fashion.     </p>

<p>EDIT: I guess I am looking for examples like <a href=""http://www.stat.ncsu.edu/people/nelson/courses/st512/Multiple%20Regression%20Example.pdf"" rel=""nofollow"">this one (pdf)</a>. In this they look for the relative importance of variables and see whether they can drop any variables or need additional variables. Some examples that include concepts of mediation, controlling for variables, so on would be great. Even if they are in books or research articles. </p>
"
"0.070675349274022","0.0697410440814588","152868","<p>I have an administrative database with hospital readmissions (binomial: yes/no) and a couple of predictors. I've fitted a multilevel model with the function <code>glmer</code> from the package <code>lme4</code> to estimate the effect of these predictors on readmissions. 
The model has two levels: <code>hospital</code> and <code>patient</code>.
When I calculate the predicted probabilities (the chance of a readmission), and afterwards calculate the readmission ratio's for each hospital (by dividing the observed readmissions by the predicted readmissions), all my ratio's are around 1 which can't be correct.</p>

<p>Before I've calculated the predicted probabilities with a normal logistic regression, which gives more plausible ratio's (from 0,64 to 1,5)</p>

<p>This is my code to calculate predicted probabilities for the multilevel model:</p>

<pre><code>database$predprob &lt;- fitted(model1)
</code></pre>

<p>I've also tried this one, but it gives exactly the same predicted probabilities:</p>

<pre><code>database$predprob &lt;- predict(model1, newdata = database, type = ""response"", na.action = na.omit)
</code></pre>

<p>Does anybody know how to calculate predicted probabilities for a multilevel analysis? I suppose there must be another way to calculate it as my calculated ratios (observed/predicted) are all around 1.</p>
"
"0.0948209311861521","0.0935674292327708","153122","<p>As we know, if we are doing many tests or multiple comparison, we don't use the same $\alpha$ value and use some $\alpha$ correction methods like Bonferroni. This is done because when we do multiple tests, we have higher chance of getting something as significant compared to doing for fewer numbers of tests. </p>

<p>But my main question is this: </p>

<p>1) It is said if you are comparing multiple sample means using ANOVA and once you find there is some significant difference then you can do a <em>post hoc</em> analysis by doing pairwise comparison. But now you don't have to actually do a Bonferroni correction. Why is that? Isn't this <em>post hoc</em> analysis same as other pairwise t test where we use Bonferroni correction?</p>

<p>2) If Bonferroni correction is required because more tests leads to more chances of getting something significant then why we don't use the same thing, where we are doing something like regression where we are testing significance of $\beta$ estimates, or whether a variable is significant or not for feature selection using p value/F score? In that case also we are doing multiple comparison in checking whether each variable is significant or not. Then why don't we use Bonferroni correction on critical $\alpha$ there?</p>

<p>Please advise.</p>
"
"NaN","NaN","153480","<p>I need to implement a SAR model with no covariates. To be more specific, the regression I have to estimate is y=bWy+e where: </p>

<ul>
<li>y is the dependent variable;</li>
<li>b is the coefficient parameter to be estimated;</li>
<li>W is the adjacency matrix;</li>
<li>e is the error.</li>
</ul>

<p>My idea was to use the lagsarlm function of the spdep package. But I've gone through spdep documentation and it seems that this function works only adding covariates: i.e. y=bWy+cX+e, and I don't know how to erase the X term.</p>

<p>Note: For those who are acquainted with network analysis literature and not with spatial econometrics, in a way this is a method to estimate a parameter for bonachich centrality.</p>
"
"0.070675349274022","0.0697410440814588","153510","<p>I am trying to fit a regularized logistic regression to my data using glmnet. Using $\alpha=1$ I get a LASSO-regression, which is what I want. My problem is though that I don't know how the intercept is fitted. In glmnet one has the option to put <code>Intercept=TRUE</code> or <code>Intercept=FALSE</code>. As far as I understand <code>FALSE</code> sets my intercept to 0. When <code>TRUE</code>, I understood that the intercept was fitted as the mean of the $y$-values. Since my data is balanced binary data with values 0 and 1, $\bar{y}=0.5$, but my analysis gives me the value -2.6. </p>

<p>I read <a href=""http://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet"">How is the intercept computed in GLMnet? </a> but I don't understand it, so I hope someone will give some details. Also, in the link's article there is a likelihood function (13) and (14) on page 8 and I don't understand why it has $1/N$ in front.  </p>
"
"0.0899550337218996","0.0887658573546531","153761","<p>I don't have a lot of experience working with time series data. Now I have a 3 year, monthly data for several entities (you can think about them as different stores), that I would like to do some analysis, e.g. regression. I am not sure if there are trend and seasonality effects on these series.
Using the package <code>Forecast</code> in <code>R</code>, and applying the function <code>stl</code>, I decomposed the series and plotted them. I have attached some of the resulting plots (for different stores):</p>

<p><img src=""http://i.stack.imgur.com/np21E.jpg"" alt=""trend 2""></p>

<p><img src=""http://i.stack.imgur.com/ISKHJ.jpg"" alt=""trend 3""></p>

<p><img src=""http://i.stack.imgur.com/Rsxhw.jpg"" alt=""trend 4""></p>

<p>When I print the result of the fitted <code>stl</code> function, I get something like this: </p>

<pre><code> Call:
 stl(x = dlr1, s.window = ""period"")

 Components
            seasonal      trend   remainder
 Jan 2010 -0.05643233 -0.2151193 -0.02526416
 Feb 2010 -0.14799311 -0.2193160  0.13137861
 Mar 2010  0.10125889 -0.2235127  0.13747509
 Apr 2010 -0.29720645 -0.2266819 -0.47611165
 May 2010 -0.22746429 -0.2298511  0.28988090
 Jun 2010  0.12403035 -0.2320100  0.05470502
 Jul 2010 -0.10418880 -0.2341688  0.15684340
 Aug 2010  0.14560622 -0.2358294 -0.25225647
 Sep 2010  0.16699531 -0.2374901 -0.35570221
 Oct 2010 -0.21709617 -0.2402783  0.13772671
 Nov 2010  0.20363225 -0.2430665  0.19027750
 Dec 2010  0.30885826 -0.2444804 -0.11424289
 Jan 2011 -0.05643233 -0.2458944  0.16533482
 Feb 2011 -0.14799311 -0.2329029  0.09169095
 Mar 2011  0.10125889 -0.2199115 -0.33798701
 Apr 2011 -0.29720645 -0.2111018  0.58659147
 May 2011 -0.22746429 -0.2022921 -0.56724011
 Jun 2011  0.12403035 -0.2105492 -0.38534202
 Jul 2011 -0.10418880 -0.2188064  0.45324407
 Aug 2011  0.14560622 -0.2282670  0.15884344
 Sep 2011  0.16699531 -0.2377275  0.07834284
 Oct 2011 -0.21709617 -0.2372438  0.38658989
 Nov 2011  0.20363225 -0.2367601 -0.38545840
 Dec 2011  0.30885826 -0.2406277 -0.02293191
 Jan 2012 -0.05643233 -0.2444953 -0.14810135
 Feb 2012 -0.14799311 -0.2603740 -0.23092833
 Mar 2012  0.10125889 -0.2762527  0.19282561
 Apr 2012 -0.29720645 -0.2778357 -0.11752792
 May 2012 -0.22746429 -0.2794186  0.27095247
 Jun 2012  0.12403035 -0.2747109  0.32488093
 Jul 2012 -0.10418880 -0.2700031 -0.61519386
 Aug 2012  0.14560622 -0.2649596  0.08891071
 Sep 2012  0.16699531 -0.2599160  0.27346079
 Oct 2012 -0.21709617 -0.2550556 -0.52784826
 Nov 2012  0.20363225 -0.2501951  0.19201780
 Dec 2012  0.30885826 -0.2451385  0.13415736
</code></pre>

<p>Now based on these results, I am not sure how to decide whether there is a strong/weak trend and seasonality, and whether I should remove the trend and seasonality effects in order to build my regression model?
In another words I would like to know how to interpret the plots and the resulting trend and seasonality numbers, e.g. what does it mean in Dec 2012 when it says ""seasonal"" = 0.30885826, etc.?</p>

<p>Thanks</p>
"
"NaN","NaN","154027","<p>When I run linear regression on my test data I get the following report:</p>

<p><img src=""http://i.stack.imgur.com/kMm7M.png"" alt=""enter image description here""></p>

<p>You can find the test data in <a href=""http://paste.ofcode.org/33VQKk2X82NnAHQ2uYF2s7K"" rel=""nofollow"">here</a>.</p>

<p>The graph of actual vs predicted looks like:
<img src=""http://i.stack.imgur.com/TU8T7.png"" alt=""enter image description here""></p>

<p>I would like to know if this is fairly a good fit because summary gives R2 value is 0.43 which says that the variable has no significant linearity. But the avg error percentage is close to 97%.</p>

<p>Should I still keep this results as my analysis? </p>
"
"0.0632139541241014","0.0623782861551805","154043","<p>In polynomial regression, it is recommended to center predictor input variables to break multi colinear relationships of x to x^2.</p>

<p>From Wikipedia: The underlying monomials can be highly correlated ""For example, x and x2 have correlation around 0.97 when x is uniformly distributed on the interval (0, 1). ""</p>

<p>When a variable x is between -1 and 1, x^2 makes the magnitude smaller while when x is outside of that range, x^2 makes x's magnitude larger.</p>

<p>Making the variable into an integer variable could change the behavior.</p>

<p>E.g.</p>

<pre><code>df$x=round((df$x - mean(df$))*100)
</code></pre>

<p>Any opinions on the scale especially in regards to interval [-1,1] vs [-100,100]</p>

<p>It is common to normalize predictors subtracting the mean and dividing by the standard deviation when doing inference analysis but this question pertains to regression prediction.</p>

<p>Asking a similar question in regards to natural log, a variable that has a range (0,1] has a dramatically different transformed value than [1,100].</p>

<pre><code>log(seq(0.1,1,.1)) #mostly negative
log(seq(0.1,1,.1)*100) #rather positive
</code></pre>

<p>If the predictor variable in the case of log happened to be sometimes less than 1 and others greater than 1, that could make the transformation act a little ""wild"". Would it be best to transform the variable to be within (0,1] or [1,] but not both?</p>
"
"0.0446990156267674","0.0441081091391231","154534","<p>Currently I am working on a project where I need to find out whether financial aid (per capita) has an influence on the country's HDI. I have compiled a dataset, which you can find here: <a href=""http://paste.ofcode.org/bKHLd3fLcY9awWQqQ6ebgX"" rel=""nofollow"">http://paste.ofcode.org/bKHLd3fLcY9awWQqQ6ebgX</a></p>

<p>I am looking at the best way to find out whether my research question is true. Obviously I am going to needs some kind of regression analysis, but I do not really know where to start. I have read something about panel data/longitudinal data analysis, but I am really unsure whether that is indeed what I am looking for.</p>

<p>I tried some things, obviously, like the following:</p>

<pre><code>plm(HDI ~ Aid, data=HDIData, index=c(""Country"", ""Year""), model=""within"")
</code></pre>

<p>But this gave me the result that an increase of one unit in aid would result in an increase of 0.0355 in HDI. This seems highly unlikely, as some countries have received 800 dollars per capita in financial aid. Granted, the P value was 0.8432, so it would be statistically insignificant, but even then, I have got no clue whether this is even remotely correct.</p>

<p>I also saw people running the same command, but then using <code>log(HDI) ~ log(Aid)</code>...?</p>

<p>Could someone point me in the right direction? Much obliged!</p>
"
"0.126647210942508","0.132324327417369","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"0.070675349274022","0.0697410440814588","154986","<p>We want to do the logistic regression analysis to consider the effect of Age, CD4 on drug resistance mutations. The code that we wrote is:</p>

<pre><code>logist.summary(glm(DRM ~ Age, data = Database, family = binomial),""wald"")
</code></pre>

<p>The results are: </p>

<pre><code>            log.OR OR lower.CI upper.CI p.value
(Intercept)  -0.31 0.74     0.05     9.95  0.8169
Age          -0.07 0.93     0.86     1.00  0.0525
</code></pre>

<p>However, we want to do the test like, we will consider whether, 20 years old differences between the subjects, what the results is? Is it relative to DRMs? We wrote:</p>

<pre><code>logist.summary(glm(DRM ~ I(Age+20), data = Database, family = binomial),""wald"")
</code></pre>

<p>Results:</p>

<pre><code>            log.OR   OR lower.CI upper.CI p.value
(Intercept)   1.17 3.22     0.05   190.62  0.5742
I(Age + 20)  -0.07 0.93     0.86     1.00  0.0525
</code></pre>

<p>I want to ask:</p>

<ul>
<li>Is the code we wrote correct?</li>
<li>Can you help me explain what is meaning of these table?</li>
<li>Why it is the same results for the Age and Age+20? But differences in the Intercept? What does intercept meaning in this case?</li>
</ul>
"
"0.109489780290272","0.108042360909843","155040","<p>I am struggling with interpreting coefficients from a multiple regression analysis with multiple categorical (dummy) variables. I am running a linear mixed model with biodiversity (<code>LnS_Add1</code>) as independent variable, and several continuous and categorical dependent variables.</p>

<p>With a single categorical/dummy variable (e.g. <code>LnS_Add1 ~ AREA_AM_2.5 + System_Type3</code>; where <code>AREA_AM_2.5</code> is continuous and <code>System_Type3</code> is categorical with 3 levels, i.e. <em>Arable</em>, <em>Grassland</em> and <em>Orchard</em>) this is pretty straightforward. In this case the intercept represents the mean of the reference dummy variable (e.g. <em>Arable</em>) and the mean of the 2nd and 3rd levels <em>Grassland</em> and <em>Orchard</em> can be calculated manually by adding intercept to the slope coefficient.</p>

<pre><code>globmod1 &lt;- lmer(LnS_Add1 ~ AREA_AM_2.5 + System_Type3 + 
     (1|Study_Code/Pair_Code), data1_plant)
summary(globmod1)
</code></pre>

<p>Which returns</p>

<pre><code>Fixed effects:
                        Estimate Std. Error t value
(Intercept)            0.3585534  0.1238470   2.895
AREA_AM_2.5            0.0004256  0.0001371   3.104
System_Type3Grassland -0.5227684  0.0915722  -5.709
System_Type3Orchard   -0.4057969  0.5477567  -0.741
</code></pre>

<p>To get a summary output that shows the means of both <em>Arable</em>, <em>Grassland</em> and <em>Orchard</em> in R I suppress the intercept by adding a -1 (or +0) to the model.</p>

<pre><code>globmod1.coef &lt;- lmer(LnS_Add1 ~ AREA_AM_2.5 + System_Type3 -1 +
                   (1|Study_Code/Pair_Code), data1_plant)
summary(globmod1.coef)
</code></pre>

<p>Which returns:</p>

<pre><code>Fixed effects:
                        Estimate Std. Error t value
AREA_AM_2.5            0.0004256  0.0001371   3.104
System_Type3Arable     0.3585534  0.1238470   2.895
System_Type3Grassland -0.1642149  0.1341851  -1.224
System_Type3Orchard   -0.0472434  0.5457304  -0.087
</code></pre>

<p>But what do I do if I have multiple categorical variables (e.g. <code>LnS_Add1 ~ AREA_AM_2.5 + System_Type3 + Habitat2</code>; where Habitat2 is a categorical variable with 3 levels, i.e. <em>Farm aggregated</em>, <em>Outside field</em>, and <em>Within field</em>)?. Now the intercept represents the mean of the reference level of a combination of <code>System_Type3</code> and <code>Habitat2</code> (e.g. all data in arable systems and measured at farm aggregate level). But what I am interested in are the means for the different levels of each of my 2 categorical variables, holding everything else constant.</p>

<p>How do I create a summary table that contains means of all levels of all categorical variables in my model? The -1 command doesnt help me anymore, as it removes the intercept but the intercept now represents a mean of 2 reference dummy variables. I am only interested here in the fixed effect estimates, not in any hypothesis testing.</p>
"
"0.047410465593076","0.0623782861551805","156034","<p>I am dealing with a heteroscedastic censored dataset. I tried to use the survival analysis package in R to estimate a linear model for it. So before doing that, I conducted a simulation study, where I generate a sample for x:
$x  \sim Unif[-2,2]$
and y:
$y \sim 1+0.3x+0.6(1-0.3x)\epsilon $ where $\epsilon\sim N(0,1)$.
y is censored as $y=y$ if $y&lt;x$; $y=x$ otherwise.</p>

<p>Then I use the survreg function in R to estimate a linear model for it.</p>

<p><code>x&lt;-runif(10000,-2,2)</code></p>

<p><code>y&lt;-1+0.3*x+rnorm(length(x))*0.6*(1-0.3*x)</code></p>

<p><code>y[y&gt;=x]=x[y&gt;=x]</code></p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian')</code></p>

<p>The result is really poor, the estimate is around(intercept = 0.5, slope = 0.6) and it is very stable no matter what initial points I gave.</p>

<p>The result does not get significant improved even if I feed the true weight to it:</p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian', 'weights=1/(1-0.3x)^2')</code></p>

<p>But the estimation is great when no heteroscedasticity is presented:</p>

<p><code>x&lt;-runif(10000,-2,2)</code></p>

<p><code>y&lt;-1+0.3*x+rnorm(length(x))*0.6</code></p>

<p><code>y[y&gt;=x]=x[y&gt;=x]</code></p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian')</code></p>

<p>I also tried quantile regression for censored data with tau=0.5. (basically LAD regression which is proposed by J.Powell 1984). The result is still poor under this parameter setting.</p>

<p>Is there any good way to get a consistent estimation for heteroscedastic censored data?</p>

<p>Any suggestion on package, software or papers are welcome.</p>

<p>Thanks a lot.</p>
"
"0.0836242010007091","0.0707303281615848","156202","<p>I am working on a project where I am to do the intervention analysis and forecasting based on the time series. The problem is something like:</p>

<p><em>I have a normal time series entries but in between them some known event like natural calamities (storm, tornado) happens. I have the data for that and it affects the normal time series. Now my objective is to forecast the value of time series both in normal mode and also when I have a prediction of storm coming.</em></p>

<p>I have been reading <a href=""http://rads.stackoverflow.com/amzn/click/0471615285"" rel=""nofollow"">Forecasting with dynamic regression</a> chapter 7 about intervention analysis. I am also reading about the transfer function modeling. Can you please help me as in which model is good for this kind of time series analysis? Or may be some link which can guide me as how to do it? I will appreciate a link with some example in R or some examples.</p>

<p>EDIT: I guess I was not correct in description but I know the exact time information of all the previous storm events and I sort of want to find out the effect of storm intervention on the time series and I can forecast more closely if I know that there is a storm happening right now.</p>
"
"0.0632139541241014","0.0623782861551805","156519","<p>I've been reading through Gelmans book: Data Analysis Using Regression and Multilevel/Hierarchical Models trying to learn more about how to implement  hierarchal models. I have a dataset that I think is appropriate for this type of modeling however I want to get some other opinions. Basically the data I have is structured like this:</p>

<pre><code>BRAND       YEAR         Y           X1          X2         X3
company_1   2012    0.638042396 0.226787359 0.192104136 0.929220784
company_2   2012    0.983422117 0.308550049 0.527779594 0.106629747
company_n   2012    0.209276388 0.700314863 0.741787081 0.491451885
company_1   2013    0.833955686 0.735844101 0.518474158 0.117670754
company_2   2013    0.480778935 0.290739025 0.156177295 0.212643611
company_n   2013    0.69922326  0.188574282 0.448743735 0.609844836
company_1   2014    0.942147995 0.176500074 0.820207708 0.388313924
company_2   2014    0.503095705 0.987218933 0.834039587 0.42661805
company_n   2014    0.46569344  0.310693712 0.852694246 0.17574502 
</code></pre>

<p>where I have about 15 different companies for each year. My thought was to have a model like this: </p>

<pre><code>lmer(Y ~  X1 + X2 + X3 + (1 | BRAND) , h.data)
</code></pre>

<p>where I have a varying intercept for each company. So my question here is whether or not it makes sense to use a hierarchal model and if my data fits the archetype of hierarchal data? Also should I be including YEAR into the model somehow?</p>
"
"0.0316069770620507","0.0311891430775903","158076","<p>I am trying to use principal component analysis (PCA) to reduce dimensionality before applying linear regression. The problem is that my first 10 components are so weak (explaining only tiny variances - the 10th component's cumulative is 0.2577). What else could I try to apply the PCA? </p>

<p>Also I understand the whole process is referred to as principal component regression (PCR). Is there a tutorial or example I could learn in Stata/R?</p>
"
"0.0716778865720364","0.0825187161885156","158433","<p>I keep trying to perform parametric bootstrap on simple regression analysis to grasp the concept. The internet is full of tutorials on non-parametric one, but I found no explanation or steps concerning parametric bootstrap, so I did it on my own. Since I'm not sure if what was done is o.k., I kindly ask you to correct if I'm wrong (... or praise me if I'm right:) ).</p>

<pre><code>library(car)
library(boot)
attach(Anscombe) # I'm going to use Anscombe data

lm.out&lt;-lm(education~income, data=Anscombe) #simple regression to obtain coef.
regre.mle&lt;-coef(lm.out)
</code></pre>

<p>The model was built, so I was able to get sample $\sigma$ for errors</p>

<pre><code>mean(lm.out$resid) # 3.957485e-15
sd(lm.out$resid)   # 34.58725

regre.sim &lt;- function(data, mle){
  n &lt;- dim(data)[1]
  data$education &lt;- mle[2]*data$income+mle[1]+rnorm(n, mean=0, sd=34.58725)
  return(data)
}

regre.stat&lt;- function(data) {
  lm.out&lt;-lm(education~income, data=data)
  return(lm.out$coefficients)
}

boot.out&lt;-boot(Anscombe, statistic=regre.stat, R=100,
               ran.gen=regre.sim,
               sim=""parametric"", mle=regre.mle)

boot.ci(boot.out, type = ""basic"", index = 1) #for intercept
boot.ci(boot.out, type = ""basic"", index = 2) #for beta coef
</code></pre>

<p>Finally I get this:</p>

<pre><code>BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 99 bootstrap replicates

CALL : 
boot.ci(boot.out = boot.out, type = ""basic"", index = 2)

Intervals : 
Level      Basic         
95%   ( 0.0359,  0.0727 )  
Calculations and Intervals on Original Scale
Some basic intervals may be unstable
</code></pre>

<p>But the question is - this is it (parametric bootstrap on regression)? </p>
"
"0.070675349274022","0.0557928352651671","158713","<p>I have a question regarding multiple regression with an unbalanced grouping factor. Essentially what I am doing is an ANCOVA, but the interaction term ends up significant (which is interesting!) so I've chosen not to call it a true ANCOVA.</p>

<h1>The Data</h1>

<p>The dataset is comprised of 72 individuals who responded to many different measures for the purposes of conducting a cluster analysis to uncover relatively heterogenous subgroups within the dataset. Three clusters resulted form this analysis, where the resulting cluster sizes were n=30, n=32, and n=10. These clusters were interpreted for the purpose of a descriptive analysis. </p>

<p>An independent dataset describes these same 72 individuals on two separate continuous measures: <strong>score</strong>, and <strong>dv</strong>. The hope for my current project is to asses the effect of <strong>group</strong> (cluster membership, unbalanced) and <strong>score</strong> (and the interaction) on the <strong>dv</strong>. </p>

<h1>The Data (Example)</h1>

<pre><code>g1    &lt;-rep(1,30)
g2    &lt;-rep(2,32) 
g3    &lt;-rep(3,10)
group &lt;-as.factor(c(g1,g2,g3))
score &lt;-as.numeric(sample(1:10,72,replace=T))
dv    &lt;-as.numeric(sample(1:7,72,replace=T))
data  &lt;-data.frame(cbind(group, score, dv))
head(data)

head(data)
      group     score       dv
1     1          9          5
2     1          3          6
3     1         10          6
4     1         10          6
5     1         10          6
6     1          4          5
</code></pre>

<h1>My Question</h1>

<p>1) Can I run an analysis despite my groups being so unbalanced? If I understand correctly, by using type III SS, all groups will be weighted equally but I'm not sure if this solves my issue so simply.</p>

<p>For example:</p>

<pre><code>lm&lt;-lm(dv~1+score*group,data=data)
library(car)
Anova(lm,type=""III)
</code></pre>

<p>2) If not, am I unable to proceed in some other way? </p>

<p>I am looking for any suggestions / guidance as I try to sort this out.</p>

<p>Thanks!</p>
"
"0.0316069770620507","0.0311891430775903","159053","<p>I'm using the CausalImpact to evaluate the effect of a programme. My covariates are seasonal and I wonder whether I need to deseasonalise/ detrend the regressors before using the R package?</p>

<p>Hal Varian did this in the below analysis: </p>

<p><a href=""http://people.ischool.berkeley.edu/~hal/Papers/2013/pred-present-talk.pdf"" rel=""nofollow"">http://people.ischool.berkeley.edu/~hal/Papers/2013/pred-present-talk.pdf</a></p>

<p>slide 24:</p>

<ul>
<li>Deseasonalize predictors using R command stl</li>
<li>Detrend predictors using simple linear regression</li>
<li>Let bsts choose predictors</li>
</ul>
"
"NaN","NaN","159346","<p>I am fairly new to survival analysis and am playing around in R. I have a fairly simple cox model </p>

<pre><code>library(survival)
data(kidney)
cox&lt;-coxph(Surv(time, type)~delta, data=kidney)
baseline &lt;- basehaz(cox , centered=FALSE)
cox.survfit&lt;- survfit(cox)
plot(cox.survfit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/FIdvJ.png"" alt=""enter image description here""></p>

<p>My question is how do i calculate the survival rates myself without calling the survfit function. I tried to look at <a href=""http://stats.stackexchange.com/questions/36015/prediction-in-cox-regression"">this stackoverflow post</a> which i kind of understand but am not exactly able to turn it into code.  </p>

<p><img src=""http://i.stack.imgur.com/ei0To.png"" alt=""enter image description here""></p>

<p>I have the h0 from basehaz but i am not sure about rest of the calculations . Any ideas ?</p>

<p>EDIT:-
Added the kidney dataset in csv format(in case you don't have the same columns in your kidney datset ) :- </p>

<pre><code>""time"",""delta"",""type""
1.5,1,1
3.5,1,1
4.5,1,1
4.5,1,1
5.5,1,1
8.5,1,1
8.5,1,1
9.5,1,1
10.5,1,1
11.5,1,1
15.5,1,1
16.5,1,1
18.5,1,1
23.5,1,1
26.5,1,1
2.5,0,1
2.5,0,1
3.5,0,1
3.5,0,1
3.5,0,1
4.5,0,1
5.5,0,1
6.5,0,1
6.5,0,1
7.5,0,1
7.5,0,1
7.5,0,1
7.5,0,1
8.5,0,1
9.5,0,1
10.5,0,1
11.5,0,1
12.5,0,1
12.5,0,1
13.5,0,1
14.5,0,1
14.5,0,1
21.5,0,1
21.5,0,1
22.5,0,1
22.5,0,1
25.5,0,1
27.5,0,1
0.5,1,2
0.5,1,2
0.5,1,2
0.5,1,2
0.5,1,2
0.5,1,2
2.5,1,2
2.5,1,2
3.5,1,2
6.5,1,2
15.5,1,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
1.5,0,2
1.5,0,2
1.5,0,2
1.5,0,2
2.5,0,2
2.5,0,2
2.5,0,2
2.5,0,2
2.5,0,2
3.5,0,2
3.5,0,2
3.5,0,2
3.5,0,2
3.5,0,2
4.5,0,2
4.5,0,2
4.5,0,2
5.5,0,2
5.5,0,2
5.5,0,2
5.5,0,2
5.5,0,2
6.5,0,2
7.5,0,2
7.5,0,2
7.5,0,2
8.5,0,2
8.5,0,2
8.5,0,2
9.5,0,2
9.5,0,2
10.5,0,2
10.5,0,2
10.5,0,2
11.5,0,2
11.5,0,2
12.5,0,2
12.5,0,2
12.5,0,2
12.5,0,2
14.5,0,2
14.5,0,2
16.5,0,2
16.5,0,2
18.5,0,2
19.5,0,2
19.5,0,2
19.5,0,2
20.5,0,2
22.5,0,2
24.5,0,2
25.5,0,2
26.5,0,2
26.5,0,2
28.5,0,2
</code></pre>
"
"0.159920059950044","0.1676688416699","159355","<p>I performed regression with robust variances (after Stata 12.1 lnskew transformation). A question of overfitting has been raised.</p>

<p>To summarise what I did: </p>

<ol>
<li>Comparison of disease B (disgrp=2) versus disease C (disgrp=3)
patients with 45-54 dependent observations (FibrosisP, continuous variable) in each disease group. Each patient had observations taken from Regions P, Q and R and Walls X, Y and Z (i.e. 9 observations per patient). </li>
<li>lnskew0 transformation (natural log transformation with zero skew of
resulting distribution) of FibrosisP to give lfibr. </li>
<li>Simple and then multiple regression with clustered robust variances/standard errors (clustered by patient and using independent categorical variables Disease, Wall and Region).</li>
</ol>

<p>Note that Disease A is excluded from this analysis (and is not provided in data set below).</p>

<p>$$ multipleregression: lfibr \sim Disease + Wall + Region $$</p>

<p>I have copied the original Stata v12.1 log file below, which will hopefully tell the whole story. </p>

<pre><code>. gen disgrp=.
(153 missing values generated)

. replace disgrp=1 if disease==""A""
(54 real changes made)

. replace disgrp=2 if disease==""B""
(54 real changes made)

. replace disgrp=3 if disease==""C""
(45 real changes made)

. lnskew0 lfibr= fibrosisp

       Transform |         k     [95% Conf. Interval]       Skewness
-----------------+--------------------------------------------------
  ln(fibrosis-k) |   .0116473      (not calculated)        -2.77e-08

    . gen region1=.
(153 missing values generated)

. replace region1=1 if region==""P""
(51 real changes made)

. replace region1=2 if region==""Q""
(51 real changes made)

. replace region1=3 if region==""R""
(51 real changes made)

. gen wall1=.
(153 missing values generated)

. replace wall1=1 if wall==""X""
(51 real changes made)

. replace wall1=2 if wall==""Y""
(51 real changes made)

. replace wall1=3 if wall==""Z""
(51 real changes made)

. **Comparing C with B**

. xi:regress lfibr disgrp if disgrp &gt; 1  , cluster(pat)

Linear regression                                      Number of obs =      99
                                                       F(  1,    10) =   20.51
                                                       Prob &gt; F      =  0.0011
                                                       R-squared     =  0.3884
                                                       Root MSE      =   .5833

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .9241372    .204061     4.53   0.001     .4694609    1.378813
       _cons |  -4.667246   .4602243   -10.14   0.000    -5.692689   -3.641802
------------------------------------------------------------------------------

. xi:regress lfibr region1 if disgrp &gt; 1  , cluster(pat)

Linear regression                                      Number of obs =      99
                                                       F(  1,    10) =    3.17
                                                       Prob &gt; F      =  0.1055
                                                       R-squared     =  0.0068
                                                       Root MSE      =  .74333

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
     region1 |   .0748154   .0420368     1.78   0.105    -.0188483    .1684792
       _cons |   -2.54854    .177876   -14.33   0.000    -2.944872   -2.152207
------------------------------------------------------------------------------

. xi:regress lfibr i.region1 if disgrp &gt; 1  , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  2,    10) =    5.59
                                                       Prob &gt; F      =  0.0235
                                                       R-squared     =  0.0612
                                                       Root MSE      =  .72645

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
 _Iregion1_2 |   .4400513   .1398977     3.15   0.010     .1283397    .7517628
 _Iregion1_3 |   .1496308   .0845103     1.77   0.107    -.0386698    .3379314
       _cons |   -2.59547   .1905071   -13.62   0.000    -3.019946   -2.170993
------------------------------------------------------------------------------

. xi:regress lfibr i.wall1 if disgrp &gt; 1  , cluster(pat)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  2,    10) =    6.17
                                                       Prob &gt; F      =  0.0180
                                                       R-squared     =  0.0630
                                                       Root MSE      =  .72575

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
   _Iwall1_2 |   .3285724   .1654396     1.99   0.075    -.0400499    .6971948
   _Iwall1_3 |   .4356131   .1305289     3.34   0.008     .1447766    .7264496
       _cons |  -2.653637   .1780801   -14.90   0.000    -3.050425    -2.25685
------------------------------------------------------------------------------

. xi3:regress lfibr disgrp*i.region1*i.wall1 if disgrp &gt; 1 , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  8,    10) =       .
                                                       Prob &gt; F      =       .
                                                       R-squared     =  0.5401
                                                       Root MSE      =  .55355

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .5419458   .4154947     1.30   0.221    -.3838342    1.467726
 _Iregion1_2 |  -.2963738   1.409317    -0.21   0.838    -3.436529    2.843781
 _Iregion1_3 |  -.2791626   1.335259    -0.21   0.839    -3.254304    2.695979
   _Iwall1_2 |  -1.039268     .97762    -1.06   0.313    -3.217541    1.139005
   _Iwall1_3 |  -.9227228   1.131622    -0.82   0.434    -3.444133    1.598687
    _IdiXre2 |    .305921   .5859783     0.52   0.613    -.9997201    1.611562
    _IdiXre3 |   .2146185   .5228838     0.41   0.690    -.9504393    1.379676
    _IdiXwa2 |   .5887627   .4158743     1.42   0.187    -.3378629    1.515388
    _IdiXwa3 |   .5677226   .5322211     1.07   0.311      -.61814    1.753585
   _Ire2Xwa2 |   .9560212   1.372943     0.70   0.502    -2.103087    4.015129
   _Ire2Xwa3 |   1.876106   1.632401     1.15   0.277    -1.761111    5.513323
   _Ire3Xwa2 |   .1403149   1.711091     0.08   0.936    -3.672233    3.952863
   _Ire3Xwa3 |   .5961959   1.627029     0.37   0.722     -3.02905    4.221442
_IdiXre2Xwa2 |  -.4387073   .5346165    -0.82   0.431    -1.629907    .7524925
_IdiXre2Xwa3 |  -.7328102   .7126107    -1.03   0.328    -2.320606    .8549855
_IdiXre3Xwa2 |  -.1024311   .6268405    -0.16   0.873    -1.499119    1.294257
_IdiXre3Xwa3 |  -.3174033   .6961228    -0.46   0.658    -1.868461    1.233655
       _cons |  -4.217918   .9792797    -4.31   0.002     -6.39989   -2.035947
------------------------------------------------------------------------------

. xi3:regress lfibr disgrp i.region1 i.wall1 if disgrp &gt; 1 , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  5,    10) =   10.60
                                                       Prob &gt; F      =  0.0010
                                                       R-squared     =  0.5127
                                                       Root MSE      =  .53177

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .9241372   .2084032     4.43   0.001     .4597858    1.388488
 _Iregion1_2 |   .4400513   .1421362     3.10   0.011      .123352    .7567505
 _Iregion1_3 |   .1496308   .0858625     1.74   0.112    -.0416828    .3409444
   _Iwall1_2 |   .3285724   .1680868     1.95   0.079    -.0459482    .7030931
   _Iwall1_3 |   .4356131   .1326175     3.28   0.008     .1401229    .7311033
       _cons |  -5.118535   .4532473   -11.29   0.000    -6.128433   -4.108637
------------------------------------------------------------------------------
</code></pre>

<p>csv data:</p>

<pre><code>""row"",""PatientID"",""Disease"",""Wall"",""Region"",""FibrosisP""
""1"",1,""C"",""X"",""P"",0.11574464021797
""2"",1,""C"",""X"",""Q"",0.06409239204845
""3"",1,""C"",""X"",""R"",0.05589004594181
""4"",2,""C"",""X"",""P"",0.08452786770152
""5"",2,""C"",""X"",""Q"",0.19765474370344
""6"",2,""C"",""X"",""R"",0.29491566808792
""7"",3,""C"",""X"",""P"",0.13849556170319
""8"",3,""C"",""X"",""Q"",0.21529108879539
""9"",3,""C"",""X"",""R"",0.23260346696877
""10"",4,""C"",""X"",""P"",0.03242538798989
""11"",4,""C"",""X"",""Q"",0.18213249953927
""12"",4,""C"",""X"",""R"",0.0464009382069
""13"",17,""C"",""X"",""P"",0.12925196186539
""14"",17,""C"",""X"",""Q"",0.16685146683109
""15"",17,""C"",""X"",""R"",0.16298253982187
""16"",5,""B"",""X"",""P"",0.06082167946576
""17"",5,""B"",""X"",""Q"",0.06179248715729
""18"",5,""B"",""X"",""R"",0.04635879285168
""19"",6,""B"",""X"",""P"",0.0512284261286
""20"",6,""B"",""X"",""Q"",0.05560175796177
""21"",6,""B"",""X"",""R"",0.05038057719884
""22"",7,""B"",""X"",""P"",0.03485909775192
""23"",7,""B"",""X"",""Q"",0.07526805988175
""24"",7,""B"",""X"",""R"",0.03989544438546
""25"",8,""B"",""X"",""P"",0.05069990522336
""26"",8,""B"",""X"",""Q"",0.11638788902232
""27"",8,""B"",""X"",""R"",0.23086071670409
""28"",9,""B"",""X"",""P"",0.12712370092246
""29"",9,""B"",""X"",""Q"",0.05070659692429
""30"",9,""B"",""X"",""R"",0.06183074530974
""31"",10,""B"",""X"",""P"",0.04509566111129
""32"",10,""B"",""X"",""Q"",0.09050081347533
""33"",10,""B"",""X"",""R"",0.05178363738579
""52"",1,""C"",""Y"",""P"",0.14421181658066
""53"",1,""C"",""Y"",""Q"",0.1299066509205
""54"",1,""C"",""Y"",""R"",0.14904819595697
""55"",2,""C"",""Y"",""P"",0.08801608368174
""56"",2,""C"",""Y"",""Q"",0.24864891863453
""57"",2,""C"",""Y"",""R"",0.15962998919524
""58"",3,""C"",""Y"",""P"",0.4272296674396
""59"",3,""C"",""Y"",""Q"",0.2593375589095
""60"",3,""C"",""Y"",""R"",0.26700346966879
""61"",4,""C"",""Y"",""P"",0.14002780500134
""62"",4,""C"",""Y"",""Q"",0.28346720806288
""63"",4,""C"",""Y"",""R"",0.19312813953225
""64"",17,""C"",""Y"",""P"",0.17668051188556
""65"",17,""C"",""Y"",""Q"",0.18609876357474
""66"",17,""C"",""Y"",""R"",0.26587590290484
""67"",5,""B"",""Y"",""P"",0.05356234036154
""68"",5,""B"",""Y"",""Q"",0.04731210983269
""69"",5,""B"",""Y"",""R"",0.04877515848359
""70"",6,""B"",""Y"",""P"",0.06240572241178
""71"",6,""B"",""Y"",""Q"",0.13301297541279
""72"",6,""B"",""Y"",""R"",0.17973855854636
""73"",7,""B"",""Y"",""P"",0.06463245380331
""74"",7,""B"",""Y"",""Q"",0.10244742460486
""75"",7,""B"",""Y"",""R"",0.0599854720435
""76"",8,""B"",""Y"",""P"",0.05824947941558
""77"",8,""B"",""Y"",""Q"",0.11926213239492
""78"",8,""B"",""Y"",""R"",0.04685947691071
""79"",9,""B"",""Y"",""P"",0.06752011460398
""80"",9,""B"",""Y"",""Q"",0.09542812038592
""81"",9,""B"",""Y"",""R"",0.08668150350578
""82"",10,""B"",""Y"",""P"",0.06486814661182
""83"",10,""B"",""Y"",""Q"",0.05854476138367
""84"",10,""B"",""Y"",""R"",0.04438863783229
""103"",1,""C"",""Z"",""P"",0.05133333746688
""104"",1,""C"",""Z"",""Q"",0.14821006659988
""105"",1,""C"",""Z"",""R"",0.08174176027544
""106"",2,""C"",""Z"",""P"",0.23884995419341
""107"",2,""C"",""Z"",""Q"",0.2099355433643
""108"",2,""C"",""Z"",""R"",0.13176723596276
""109"",3,""C"",""Z"",""P"",0.46479557484677
""110"",3,""C"",""Z"",""Q"",0.33304596595977
""111"",3,""C"",""Z"",""R"",0.29770388592371
""112"",4,""C"",""Z"",""P"",0.15308213537672
""113"",4,""C"",""Z"",""Q"",0.28081619128875
""114"",4,""C"",""Z"",""R"",0.24592983188039
""115"",17,""C"",""Z"",""P"",0.21312809357862
""116"",17,""C"",""Z"",""Q"",0.23336174725733
""117"",17,""C"",""Z"",""R"",0.22714195157817
""118"",5,""B"",""Z"",""P"",0.06818263568709
""119"",5,""B"",""Z"",""Q"",0.07257093444773
""120"",5,""B"",""Z"",""R"",0.08201262934886
""121"",6,""B"",""Z"",""P"",0.0644884733419
""122"",6,""B"",""Z"",""Q"",0.11937946452025
""123"",6,""B"",""Z"",""R"",0.07081608918845
""124"",7,""B"",""Z"",""P"",0.06720225949377
""125"",7,""B"",""Z"",""Q"",0.12509595330262
""126"",7,""B"",""Z"",""R"",0.06657357031905
""127"",8,""B"",""Z"",""P"",0.05878644062606
""128"",8,""B"",""Z"",""Q"",0.26638352132337
""129"",8,""B"",""Z"",""R"",0.06789933388591
""130"",9,""B"",""Z"",""P"",0.0908078338911
""131"",9,""B"",""Z"",""Q"",0.17670466924957
""132"",9,""B"",""Z"",""R"",0.10642489420997
""133"",10,""B"",""Z"",""P"",0.05107976253608
""134"",10,""B"",""Z"",""Q"",0.07242867177979
""135"",10,""B"",""Z"",""R"",0.05074329491013
</code></pre>

<p>My interpretation is that we have 99 observations for 3 categorical variables (2-3 categories each) in the regression analysis; Root MSE = 0.53177.</p>

<p>Is overfitting a valid concern here? If so, is there any way to address it?</p>

<p>A general answer would be helpful. Moreover, I'm trying to move to R (rather than Stata), so advice on replicating the analysis and/or addressing overfitting with R would be gratefully received.</p>

<p>ADDENDUM1:
I've partially worked out how to replicate analysis in R. Haven't yet replicated lnskew0, though <a href=""https://rpubs.com/chrisbrunsdon/skewness"" rel=""nofollow"">https://rpubs.com/chrisbrunsdon/skewness</a> looks similar.</p>

<pre><code>p2.df &lt;- read.table(""data_above.csv"", header=TRUE, sep="","")

library(foreign)
library(sandwich)
library(lmtest)
library(DAAG)

options(digits = 8)  # for more exact comparison with Stata's output

#create ln FibrosisP (need to replicate lnskew0 from Stata - manual k entered here from Stata calculation)

p2.df$FibrosisPln &lt;- log(p2.df$FibrosisP-0.0116473)

p1.df &lt;- p2.df[c(""PatientID"", ""DiseaseG"", ""WallG"", ""RegionG"", ""FibrosisPln"")]

p1.df$DWR &lt;- paste(p1.df$DiseaseG, p1.df$WallG, p1.df$RegionG)

p.df &lt;- pdata.frame(p1.df, index = c(""PatientID"", ""DWR""), drop.index = F, row.names = T)

# tools_reg.R from http://www.existencia.org/pro/?p=134
source(""tools_reg.R"")
mod &lt;- lm(FibrosisPln~factor(DiseaseG)+factor(WallG)+factor(RegionG),data=p.df)
get.coef.clust(mod, p.df$PatientID) # identical to Stata output
</code></pre>

<p>ADDENDUM2:
I have performed 10-fold cross-validation with CVlm from R package DAAG. However, I am uncertain how to interpret the results. Does the presence of overlapping lines in the plot for all 10 folds suggest no overfitting is present?</p>

<pre><code>library(foreign)
library(sandwich)
library(lmtest)
library(DAAG)
CVlm(df=p.df, form.lm=mod, m=10, plotit = c(""Observed"",""Residual""), main=""Small symbols show cross-validation predicted values"", legend.pos=""topleft"", printit=TRUE)
</code></pre>

<p><img src=""http://i.stack.imgur.com/HqTzi.png"" alt=""CVlm plot""></p>

<pre><code>t test of coefficients:

                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -3.2703     0.1441  -22.70  &lt; 2e-16 ***
factor(DiseaseG)C   0.9241     0.2084    4.43  2.5e-05 ***
factor(WallG)Y      0.3286     0.1681    1.95   0.0536 .  
factor(WallG)Z      0.4356     0.1326    3.28   0.0014 ** 
factor(RegionG)Q    0.4401     0.1421    3.10   0.0026 ** 
factor(RegionG)R    0.1496     0.0859    1.74   0.0847 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>ADDENDUM3:
After a lot of searching, the only explanation I could find online (which I presume is accurate) is given at <a href=""http://rstatistics.net/regression-modelling/"" rel=""nofollow"">http://rstatistics.net/regression-modelling/</a></p>

<blockquote>
  <p>""The fitted lines of different colors are parallel and on-top of each other. Indicating a stable model direction and less influence of outliers.""</p>
</blockquote>
"
"0.0948209311861521","0.0935674292327708","159469","<p>I am relatively new to R. The short version of the data looks ike this:   </p>

<pre><code>sNumber  blockNo running TrialNo    wordTar   wordTar1   Freq Len code code2
1        1       1       5           spouse    violent   5011   6    1     2
1        1       1       5          violent     spouse  17873   7    2     1
1        1       1       5           spouse    aviator   5011   6    1     1
1        1       1       5          aviator       wife    515   7    1     1
1        1       1       5             wife    aviator  87205   4    1     1
1        1       1       5          aviator     spouse    515   7    1     1
1        1       1       9        stability    usually  12642   9    1     3
1        1       1       9          usually   requires  60074   7    3     4
1        1       1       9         requires     client  25949   8    4     1
1        1       1       9           client   requires  16964   6    1     4
2        2       1       5            grimy      cloth    757   5    2     1
2        2       1       5            cloth       eats   8693   5    1     4
2        2       1       5             eats    whitens   3494   4    4     4
2        2       1       5          whitens      woman     18   7    4     1
2        2       1       5            woman    penguin 162541   5    1     1
2        2       1       9              pie   customer   8909   3    1     1
2        2       1       9         customer  sometimes  13399   8    1     3
2        2       1       9        sometimes reimburses  96341   9    3     4
2        2       1       9       reimburses  sometimes     65  10    4     3
2        2       1       9        sometimes   gangster  96341   9    3     1
</code></pre>

<p>I have a code for ordinal regression analysis for one participant for one trial (eye-tracking data - eyeData) that looks like this:</p>

<pre><code>#------------set the path and import the library-----------------
setwd(""/AscTask-3/Data"")
library(ordinal)

#-------------read the data----------------
read.delim(file.choose(), header=TRUE) -&gt; eyeData

#-------------extract 1 trial from one participant---------------
ss &lt;- subset(eyeData, sNumber == 1 &amp; runningTrialNo == 5) # extract the 5th trial from the 1st participant

#-------------delete duplicates = refixations-----------------
ss.s &lt;- ss[!duplicated(ss$wordTar), ] 

#-------------change the raw frequencies to log freq--------------
ss.s$lFreq &lt;- log(ss.s$Freq)

#-------------add a new column with sequential numbers as a factor ------------------
ss.s$rankF &lt;- as.factor(seq(nrow(ss.s))) 

#------------ estimate an ordered logistic regression model - fit ordered    logit model----------
m &lt;- clm(rankF~lFreq*Len, data=ss.s, link='probit')
summary(m)

#---------------get confidence intervals (CI)------------------
(ci &lt;- confint(m)) 

#----------odd ratios (OR)--------------
exp(coef(m))
</code></pre>

<p>The eyeData file is a huge massive of data consisting of 91832 observations with 11 variables. In total there are 41 participants with 78 trials each. In my code I extract data from one trial from each participant to run the anaysis. However, it takes a long time to run the analysis manually for all trials for all participants. Could you, please, help me to create a loop that will read in all 78 trials from all 41 participants and save the output of statistics (I want to save <strong>summary(m), ci, and coef(m)</strong>) in one file.</p>
"
"0.11852616398269","0.124756572310361","160638","<h1>General question</h1>

<p>When I perform a logistic regression using lrm and specify weights for the observations, I get the following warning message:</p>

<blockquote>
  <p>Warning message:
  In lrm(Tag ~ DLL, weights = W, data = tagdata, x = TRUE, y = TRUE) :
    currently weights are ignored in model validation and bootstrapping lrm fits</p>
</blockquote>

<p>My interpretation is that everything that the rms package will tell me regarding goodness-of-fit, notably using the residuals.lrm tool, is wrong. Is this correct?</p>

<h1>Specific example</h1>

<p>To be more specific, I have working example. All the code and output can be found in this <a href=""https://github.com/jwimberley/crossvalidated-posts/tree/master/lrm_gof"" rel=""nofollow"">GitHub repository</a>. I have two CSV tables of data, <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toystudy.csv"" rel=""nofollow"">toystudy.csv</a> and <a href=""https://github.com/jwimberley/crossvalidated-posts/raw/master/lrm_gof/realstudy.csv"" rel=""nofollow"">realstudy.csv</a>. There are three columns in each:</p>

<ol>
<li>The binomial response $y$ (0 or 1) [called Tag in code]</li>
<li>The predictor $x$ [called DLL in code]</li>
<li>The weight for the observation [called W in code]</li>
</ol>

<p>The former is simulated data, where all the weights are unity and where a logistic regression $log(\pi) = \theta_0 + \theta_1 x$ should fit the data perfectly. The latter is real data from my analysis, where the validity of this simple model is in question. The real data has weighted observations. (Some of the weights are negative, but there is a well-defined reason for this). The analysis code in contained completely in <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/regressionTest.R"" rel=""nofollow"">regressionTest.R</a>; the meat of the code is</p>

<pre><code>library(rms)
fit &lt;- lrm(Tag ~ DLL, weights = W, data = tagdata, x=TRUE, y=TRUE)
residuals(fit,""gof"")
</code></pre>

<p>Here are the results for the two tables of data.</p>

<h3>Case 1: Toy data</h3>

<p>The goodness-of-fit claimed by lrm (which is something called the le Cessie-van Houwelingen-Copas-Hosmer test, I understand?) is very good:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toy/residuals.png"" alt=""enter image description here""></p>

<p>This is confirmed by grouping the data into 20 quantiles of the predictor and overlaying the predicted success rate over the average actual success rate:</p>

<p><img src=""http://i.stack.imgur.com/hOEFs.png"" alt=""enter image description here""></p>

<h3>Case 2: Real data</h3>

<p>In this case, the goodness-of-fit reported by lrm is horrendous:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/real/residuals.png"" alt=""enter image description here""></p>

<p>However, I don't think it should be that bad. Again grouping the data into quantiles, and taking into account the weights when computing the average values in each bin:</p>

<p><img src=""http://i.stack.imgur.com/mgzhc.png"" alt=""enter image description here""></p>

<p>Comparing the prediction to the observed values and their standard errors, I don't think this is that bad (the error bars here depend on how the standard error on a weighted mean is computed, so they might not be 100% right, but should at least be close). On the other hand, if I produce the same plot while ignoring the weights:</p>

<p><img src=""http://i.stack.imgur.com/dId9F.png"" alt=""enter image description here""></p>

<p>I can definitely imagine this fit being as poor as the goodness-of-fit test says.</p>

<h2>Conclusion</h2>

<p>So, is residuals.rm simply ignoring the weights when it calculates its goodness-of-fit statistic? And if so, is there any R package that will do this correctly?</p>
"
"0.0547448901451359","0.0540211804549215","160696","<p>As a pet project, I have been learning some data analysis and machine learning skills (mainly text analytics) with the Analytics Edge course on edX. I decided to put some of my new skills at use analysing a dataset from UCI Machine Learning: <a href=""https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"" rel=""nofollow"">https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection</a></p>

<p>I did some analysis already (can be seen at <a href=""https://github.com/Khaltar/Portfolio/blob/master/R/Machine%20Learning/SMS.R"" rel=""nofollow"">https://github.com/Khaltar/Portfolio/blob/master/R/Machine%20Learning/SMS.R</a>) and computed a LogRegression Model, a RF Model and a CART Model. The Random Forest Model seems to be getting the best results regarding AUC and accuracy but I'm still not happy with it.</p>

<p>A friend of mine suggested using a bagging approach joining the three models and using some kind of ""voting"" system to classify predictions and achieve better results but I am completely at a loss on how to do that.  My doubt is how to actually implement a bootstrapping model in R using RF to raise accuracy of the model. I tried using bagRboostR package (sample code in my sms.R file in github) but I can't figure out how to use it or if there is a simpler solution to implement it.</p>

<p>Thanks in advance</p>
"
"0.0632139541241014","0.0623782861551805","161121","<p>I have an R question. I'm wondering why there is a difference in p-values in the original regression analysis using lm versus in the k-fold cross-validation using the DAAG package.</p>

<p>So, first I run the regression.</p>

<pre><code>Model = lm(ExampleData$DependentVariable ~ ExampleData$IV1  + 
           ExampleData$IV2  + ExampleData$IV3  + ExampleData$IV4)
</code></pre>

<p>This gives me the p-values for the predictors.</p>

<pre><code>Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)     -55.6644    23.4690  -2.372  0.01958 * 
ExampleData$IV1   1.2118     0.6277   1.931  0.05631 .
ExampleData$IV2   6.2636     2.0563   3.046  0.00295 **
ExampleData$IV3   2.1531     0.7490   2.875  0.00492 **
ExampleData$IV4  -5.4468     1.8859  -2.888  0.00473 **
</code></pre>

<p>Then, I go to cross-validate the model using cv.lm in the DAAG package.</p>

<pre><code>cv.lm(df=ExampleData, Model_forCV, m=5)
</code></pre>

<p>This gives me the cross-validation results along with the p-values for the predictors.</p>

<pre><code>Response: DependentVariable
           Df Sum Sq Mean Sq F value  Pr(&gt;F)    
IV1         1  26755   26755    3.23 0.07541 .  
IV2         1 104332  104332   12.58 0.00059 ***
IV3         1  36119   36119    4.36 0.03938 *  
IV4         1  69167   69167    8.34 0.00473 ** 
Residuals 102 845806    8292   
</code></pre>

<p>Why are the p-values different?</p>

<p>Thank you! </p>
"
"NaN","NaN","161439","<p>I'm in the process of evaluating some behavioral data I've collected and cannot find a package to obtain relative factor importance for the following linear model:
<code>
cbind(accuracy, rt) ~ A + B + C + D
</code></p>

<p>In an earlier, parallel â€“ but univariate â€“ experiment I was able to use the <code>relaimpo</code> R package, which worked fine. But it is unable, I believe, to support multivariate analysis. I'm aware of Tonidandel &amp; LeBreton's <a href=""http://relativeimportance.davidson.edu/multivariateregression.html"" rel=""nofollow"">online tool/downloadable R code</a>, but was hoping their might be an alternative package-based solution. Am I in luck, or is their code the best bet out there right now?</p>
"
"0.172766317839757","0.175353328962333","161941","<p>Something I rather vaguely asked a few months back, saw the tumbleweed roll by (actually hacked some hardware in the time, to get a few answers) before the question was deleted, so I'll try again on a slightly more specific note, as there will be a cleaner / quicker way to get the answer.</p>

<p>If I know a regression equation derives a particular answer from a given set of variables and coefficients, but I don't know the values for at least one set, how can I model this in R / get R to have a guess at deriving the unknowns.    </p>

<p>I ask as I was curious as to the maths embedded in number of Bioelectrical Impedance Analysis (BIA) devices I had around (A mix of Salter and Withings).
A quick Google revealed <a href=""http://pubs.sciepub.com/ijcn/2/1/1/"" rel=""nofollow"">dozens of published regression formula</a>, <a href=""http://ajcn.nutrition.org/content/64/3/436S.full.pdf"" rel=""nofollow"">to estimate: Total body Water / Fat / Lean Mass</a>, so was interested in which had made their way into the devices eg.</p>

<blockquote>
  <p>Lukaski &amp; Bolonchuk's (1988):</p>
  
  <p><strong>TBW</strong> = (0.372 * HeightCM * HeightCM /
  Resistance)  + (0.142 * massKg) - (0.069 * ageYears) + (3.05 * isMale)</p>
</blockquote>

<p>...</p>

<blockquote>
  <p>Matias et al. (2015): </p>
  
  <p><strong>TBW</strong> = 0.286 + (0.195 * HeightCM * HeightCM /
  Resistance) + (0.385 * massKg) + (5.086 * isMale)</p>
</blockquote>

<p>Anyway the devices show the values of all but any Resistance (R) and possibly imaginary Rectance (X) values used (Withings), so thought I'd pester / learn a bit of R to see if there was an alternative to an afternoon without socks, but with a multimeter, a few bits of wire and a tweaked iPad.</p>

<p><strong>FYI:</strong> My original five min play, in April, with a <strong>Salter 9141 WH3R</strong> suggested that:-</p>

<blockquote>
  <p><strong>BodyFat %</strong>  ~= x + (0.1 * AgeYears) + (0.4 * MassKg) - (8 * isMale) + y</p>
  
  <p><strong>TBW %</strong>  ~= x - (0.2 * AgeYears) - (0.27 * MassKg) + (7.58 * isMale) + y</p>
</blockquote>

<p>where x:  is some combination of: </p>

<p><strong>a * HeightCM^2 [+/] b * Resistance + c * HeightCM</strong></p>

<p>and with a <strong>Withings WS-50</strong> that:-</p>

<blockquote>
  <p><strong>BodyFat %</strong>  ~= X + (0.12 * AgeYears) + (0.29 * BodyMassKg) - (16.64 * isMale) + (5 * isJapanese) + z</p>
</blockquote>

<p>where X:  is some combination of: </p>

<p><strong>a * HeightCM^2 [+/] b * Resistance + c * HeightCM + d * Reactance</strong></p>

<p>So is there a way that I a can ger <strong>R</strong> to have a guess at solving these unknowns, or must I stick with a: screwdriver, multimeter, tweaked iPad and / or logging proxy (mitProxy / HoneyProxy).</p>

<pre><code># Quick play to try to identify a few of the constants used by the Salter 9141 WH3R body fat scale
# sex ( 1 = male, 0 = female)
sex &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
ageYrs &lt;- c(33, 33, 43, 43, 53, 53, 33, 43, 43, 53, 33, 43, 53, 33, 33, 43, 43, 53, 53, 33, 43, 43, 53, 33, 43, 53)
heightCM &lt;- c(191, 191, 191, 191, 191, 191, 181, 181, 181, 181, 171, 171, 171, 191, 191, 191, 191, 191, 191, 181, 181, 181, 181, 171, 171, 171)
heightCM2 &lt;- heightCM * heightCM
massKg &lt;- c(81.6, 83, 81.6, 83, 81.6, 83, 81.6, 81.6, 83, 81.6, 81.6, 81.6, 81.6, 81.6, 83, 81.6, 83, 81.6, 83, 81.6, 81.6, 83, 81.6, 81.6, 81.6, 81.6)
bodyWaterPct &lt;- c(62, 61.6, 59.9, 59.6, 57.9, 57.5, 58, 55.9, 55.5, 53.9, 53.2, 51.2, 49.2, 54.3, 54, 52.3, 52, 50.3, 50, 50.4, 48.4, 47.9, 46.4, 45.7, 43.6, 41.6)
bodyFatPct &lt;- c(17.1, 17.7, 18.1, 18.7, 19.2, 19.8, 22.6, 23.7, 24.2, 24.7, 29.1, 30.1, 31.1, 25.2, 25.7, 26.2, 26.7, 27.2, 27.7, 30.5, 31.6, 32.3, 32.9, 37, 38.1, 39.1)

bodyFat.Salter = data.frame(sex, ageYrs, heightCM, heightCM2, massKg, bodyWaterPct, bodyFatPct)
bodyFat.Salter
summary(bodyFat.Salter)
fitBodyFat1 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + massKg, data=bodyFat.Salter)
fitBodyFat2 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM2 + massKg, data=bodyFat.Salter)
fitBodyFat3 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Salter)
fitBodyWater1 &lt;- lm ( bodyWaterPct ~ sex + ageYrs + heightCM + massKg, data=bodyFat.Salter)
fitBodyWater2 &lt;- lm ( bodyWaterPct ~ sex + ageYrs + heightCM2 + massKg, data=bodyFat.Salter)
fitBodyWater3 &lt;- lm ( bodyWaterPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Salter)
summary(fitBodyFat1)
summary(fitBodyFat2)
summary(fitBodyFat3)
summary(fitBodyWater1)
summary(fitBodyWater2)
summary(fitBodyWater3)

library(glmulti)
fitBodyFatG1 &lt;- glm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Salter)
test.model1 &lt;- glmulti(fitBodyFatG1, level = 1, crit=""aicc"")
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>&gt; summary(fitBodyFat1)

Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM + massKg, data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.42607 -0.20940  0.08889  0.15588  0.27051 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 101.747418   6.105948  16.664 1.39e-13 ***
sex          -8.007692   0.092545 -86.528  &lt; 2e-16 ***
ageYrs        0.105000   0.005899  17.801 3.80e-14 ***
heightCM     -0.591111   0.006422 -92.051  &lt; 2e-16 ***
massKg        0.400794   0.079446   5.045 5.39e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2359 on 21 degrees of freedom
Multiple R-squared:  0.9988,    Adjusted R-squared:  0.9986 
F-statistic:  4442 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyFat2)

Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM2 + massKg, 
    data = bodyFat.Salter)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.5345 -0.3141  0.1192  0.2024  0.3494 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  4.843e+01  8.409e+00   5.759 1.02e-05 ***
sex         -8.008e+00  1.238e-01 -64.696  &lt; 2e-16 ***
ageYrs       1.050e-01  7.889e-03  13.310 1.05e-11 ***
heightCM2   -1.626e-03  2.365e-05 -68.758  &lt; 2e-16 ***
massKg       3.972e-01  1.063e-01   3.738  0.00121 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.3156 on 21 degrees of freedom
Multiple R-squared:  0.9979,    Adjusted R-squared:  0.9975 
F-statistic:  2481 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyFat3)

Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + 
    massKg, data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.12607 -0.02949 -0.01111  0.03162  0.17393 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 249.776918   9.660823   25.86  &lt; 2e-16 ***
sex          -8.007692   0.026174 -305.94  &lt; 2e-16 ***
ageYrs        0.105000   0.001668   62.94  &lt; 2e-16 ***
heightCM     -2.225111   0.104938  -21.20 3.53e-15 ***
heightCM2     0.004500   0.000289   15.57 1.20e-12 ***
massKg        0.400794   0.022469   17.84 9.48e-14 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.06673 on 20 degrees of freedom
Multiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 
F-statistic: 4.448e+04 on 5 and 20 DF,  p-value: &lt; 2.2e-16

...

Call:
lm(formula = bodyWaterPct ~ sex + ageYrs + heightCM + massKg, 
    data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.19309 -0.12797 -0.06759  0.18355  0.31346 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.639382   4.922117   0.130  0.89788    
sex          7.576923   0.074602 101.565  &lt; 2e-16 ***
ageYrs      -0.202500   0.004755 -42.587  &lt; 2e-16 ***
heightCM     0.432037   0.005177  83.460  &lt; 2e-16 ***
massKg      -0.269841   0.064043  -4.213  0.00039 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.1902 on 21 degrees of freedom
Multiple R-squared:  0.999, Adjusted R-squared:  0.9988 
F-statistic:  5087 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyWater2)

Call:
lm(formula = bodyWaterPct ~ sex + ageYrs + heightCM2 + massKg, 
    data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.25119 -0.16896 -0.09268  0.25881  0.39269 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.960e+01  6.631e+00   5.972  6.3e-06 ***
sex          7.577e+00  9.760e-02  77.635  &lt; 2e-16 ***
ageYrs      -2.025e-01  6.221e-03 -32.553  &lt; 2e-16 ***
heightCM2    1.188e-03  1.865e-05  63.728  &lt; 2e-16 ***
massKg      -2.670e-01  8.378e-02  -3.187  0.00443 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2488 on 21 degrees of freedom
Multiple R-squared:  0.9982,    Adjusted R-squared:  0.9979 
F-statistic:  2970 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyWater3)

Call:
lm(formula = bodyWaterPct ~ sex + ageYrs + heightCM + heightCM2 + 
    massKg, data = bodyFat.Salter)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.078205 -0.027991 -0.002778  0.038408  0.069017 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.200e+02  6.685e+00  -17.95 8.46e-14 ***
sex          7.577e+00  1.811e-02  418.32  &lt; 2e-16 ***
ageYrs      -2.025e-01  1.154e-03 -175.41  &lt; 2e-16 ***
heightCM     1.763e+00  7.262e-02   24.28 2.58e-16 ***
heightCM2   -3.667e-03  2.000e-04  -18.34 5.63e-14 ***
massKg      -2.698e-01  1.555e-02  -17.35 1.59e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.04618 on 20 degrees of freedom
Multiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 
F-statistic: 6.911e+04 on 5 and 20 DF,  p-value: &lt; 2.2e-16

...

Initialization...
TASK: Exhaustive screening of candidate set.
Fitting...

After 50 models:
Best model: bodyFatPct~1+sex+ageYrs+heightCM+heightCM2+massKg
Crit= -53.5831033999792
Mean crit= 117.663945821469
Completed.
</code></pre>

<p><strong>Withings</strong></p>

<pre><code>...
fitBodyFat1 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + massKg, data=bodyFat.Withings)
fitBodyFat2 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM2 + massKg, data=bodyFat.Withings)
fitBodyFat3 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Withings)
summary(fitBodyFat1)
summary(fitBodyFat2)
summary(fitBodyFat3)

library(glmulti)
fitBodyFatG1 &lt;- glm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Withings)
test.model1 &lt;- glmulti(fitBodyFatG1, level = 1, crit=""aicc"")
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM + massKg, data = bodyFat.Withings)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.3348 -0.5042  0.0597  0.5361  6.5767 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 120.39668    9.97149  12.074  &lt; 2e-16 ***
sex         -16.64221    0.19636 -84.754  &lt; 2e-16 ***
ageYrs        0.12039    0.01490   8.082 2.78e-13 ***
heightCM     -0.61641    0.01784 -34.552  &lt; 2e-16 ***
massKg        0.29065    0.11652   2.494   0.0138 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.9957 on 139 degrees of freedom
Multiple R-squared:  0.9848,    Adjusted R-squared:  0.9844 
F-statistic:  2251 on 4 and 139 DF,  p-value: &lt; 2.2e-16

...

Initialization...
TASK: Exhaustive screening of candidate set.
Fitting...

After 50 models:
Best model: bodyFatPct~1+sex+ageYrs+heightCM+massKg
Crit= 414.926000298605
Mean crit= 752.216409496029
Completed.
</code></pre>
"
"0.109489780290272","0.108042360909843","162463","<p>I am doing some data analysis on a fairly large health data set of patients with diagnoses and the respective procedures received for each event. I was asked to run a multinomial logistic regression on my data.</p>

<p>The dataset has around 4,000 columns of attributes, of which around 3,000 are unique diagnoses. The diagnosis variables take on the value of 1 if the patient had that diagnosis and 0 if he or she did not.  The remaining approximately 1,000 variables pertain to unique procedures, which also take on the value of 1 if the patient has received it, and 0 if he or she did not.</p>

<p>The dataset contains information on approximately 30,000 patients. I, admittedly naively, ran a the multinom function in the multinom package in R on all 4,000 variables, with the dependent variable being the very last procedure the patient has received (marked as ""Final procedure"" in the matrix), but R isn't able to complete the computation. </p>

<p>I would like some overall advice in perhaps a different package I could use for running regressions on large data sets (cannot use bigmemory however because this is on windows) or even perhaps reformatting my data. </p>

<p>Initially, my data set had around 50 columns, because the maximum number of diagnoses and procedures a patient had was 25 diagnoses and 25 procedures, so each column was marked as ""Diagnosis X"" and ""Procedure x,"" with the corresponding element being the actual diagnosis/procedure identifier. For all the patients who did I have all 25 diagnoses/procedures (so most of them), the values in the data frame would just be NA. Now I am wondering if I could perhaps resort to using this data frame instead and have a nicer, smaller matrix to work with? The only real reason I reformatted my data set into the much larger matrix was because my grad student asked me to do so, but maybe this isn't the way to go.</p>
"
"0.0632139541241014","0.0623782861551805","162599","<p>This question is in response to an answer given by @gung in regards to this <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">question</a></p>

<p>I am also wanting to use simulation to conduct a power analysis on a multiple logistic regression. To keep it simple <strong>I want to do a post-hoc power analysis to determine the power associated with my regression</strong>. Lets take an <code>alpha=0.05</code></p>

<p>Lets say we have 1000 samples of data. Our dataset can be assumed as:</p>

<pre><code>set.seed(123)
N &lt;- 1000
var1 &lt;- runif(N, min=0, max=0.5)
var2 &lt;- runif(N, min=0.3, max=0.7)
var3 &lt;- rbinom(n=N, size=1, prob = 0.15)
output &lt;- rbinom(n=N, size=1, prob = 0.05)

df &lt;- data.frame(var1, var2, var3, output)
</code></pre>

<p>And a simple logistic model we are using is</p>

<pre><code>model &lt;- glm(output~var1+var2+var3, 
             data=df,
             family = binomial()
</code></pre>

<p>Now where this question differs from the example, we have our binary output (<code>output</code>) and not a continuous rate. </p>

<p>From what I have read, when unsure of the effect size, select a medium rate.</p>
"
"0.0316069770620507","0.0311891430775903","162699","<p>At the zero-order level, X is not correlated with Y. When I add X and A into a regression analysis to predict Y, only A is a significant predictor. A itself is correlated highly with Y at zero-order. However, when I add an interaction term into the analysis, A*X and X significantly predicts Y, and A no longer predicts. What does this mean? What kind of variable is X, and what kind of variable is Y?</p>

<pre><code># additive model
Call:lm(formula = Y ~ X + A, data = dat)
Residuals:
Min      1Q  Median      3Q     Max 
-30.247  -8.150  -1.416   8.692  24.263 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  67.3713     9.8646   6.830 2.04e-09 ***
X             0.4805     2.1935   0.219    0.827
A           -10.4961     1.8172  -5.776 1.69e-07 ***

# interaction model
Call:lm(formula = Y ~ X * A, data = dat)

Residuals:
Min      1Q  Median      3Q     Max 
-20.778  -8.834  -2.135   8.925  24.410 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)     -17.620     40.762  -0.432   0.6668  
X                30.294     14.057   2.155   0.0345 *
A                10.275      9.841   1.044   0.2999  
X:A              -7.297      3.400  -2.146   0.0352 *
</code></pre>
"
"0.0632139541241014","0.0623782861551805","163092","<p>Iâ€™m looking to build an ARIMA model in R to help me predict the number of shots a football player is going to take in a game. </p>

<p>I have last season's data to analyse to determine the optimal lags for my AR and MA parameters. I have a data frame in R, with the columns for the player name, date of match and the number of shots. </p>

<p>Unfortunately, I only have a maximum 38 data points for each player which isnâ€™t enough to build a statistically confident model. I suspect I need a way to analyse the data holistically/all-at-once to help me determine the optimal lags.</p>

<p>I donâ€™t, however, know how to do that or even if this is a statistically sound technique. </p>

<p>At the moment I am just analysing my residuals (which have come from a linear regression with independent variables such as Home/Away and Team Possession) with code such as the following:</p>

<pre><code>arima(residuals, order=c(3,0,0))
</code></pre>

<p>Is there a way to instruct R to perform this ARIMA analysis whilst looking at lots of mini-groups (where the groups are categorised by player name)?</p>

<p>Any help would be much appreciated. </p>

<p>Will </p>
"
"0.0547448901451359","0.0540211804549215","163356","<p>I am trying to estimate a midplane of a 3D model using the midpoints of paired landmarks, in order to reconstruct missing data (midplane refers here to the middle/saggital plane of the cranium which cuts the skull into two symmetrical halves, left and right).</p>

<p><strong>I therefore need to estimate a plane from 27 points in 3D.</strong> I need to get the equation of the plane as $$ax+by+cz=d.$$</p>

<p>I have looked into orthogonal regression and principal component analysis (PCA) as methods, however I didn't take maths past A-levels and am struggling. I know I can supposedly use the eigenvectors to get the equation of the plane of best fit, but need someone to explain exactly how. I am using R for the PCA but am not great at R either.</p>

<p>Alternatively, if there's a better way to estimate the plane I would be glad to hear it!</p>
"
"0.0632139541241014","0.0623782861551805","163819","<p>I am running a multinomial logistic regression model (with 3 possible outcomes) in R. I am trying to find the best way to assess the predictive power/accuracy of the model, and the best thing I've come up with is using a ROC curve.</p>

<p>For multi-class ROC analysis, I know that there is the one vs. one comparison or the one vs. all comparison. For the one vs. one comparison, would I need three separate ROC curves for each possible combination of outcome comparisons? If so, do I need to make a third model for comparing the two outcomes that were initially being compared to the baseline outcome?</p>

<p>For one vs. all comparison is the threshold for a r ""random model"" now 33% instead of 50%?</p>

<p>And finally, is there a better way to go about doing this/visualizing it?</p>

<p>EDIT: I know the pROC package has a multi class.roc function, but I don't totally get what it does.</p>
"
"0.0316069770620507","0.0311891430775903","163986","<p>I effectively want to model the probability of a player winning his service point (a point in which he is the server) based on the values of explanatory variables (namely court surface and opponent world ranking)</p>

<p>Can this be done using a binary response logistic regression?</p>

<p>Consider the fact that I can view my response variable as number of successes out of a total number of trials (for which I have the data). Will it work considering I have both categorical and numerical explanatory variables?</p>

<p>Any feedback on why this will/won't work or how I can make it work would be hugely appreciated! I am doing the analysis in R, so pointers on functions or packages would also be welcome! </p>
"
"0.118262479197817","0.116699087583415","164228","<p>GLM (family=binomial) is foucusd on when the response is dichotomous(yes/no, male/female, etc..). I'm wondering how to judge if the model we built is good eough? As we know, in OLS regression some criterion like R^2 and adjusted R^2 can tell us how much variations are explained but not for GLM. See example I performed:</p>

<pre><code>    &gt; summary(fit.full)
    Call:
    glm(formula = ynaffair ~ gender + age + yearsmarried + children + 
    +religiousness + education + occupation + rating, family = binomial(), 
    data = Affairs)

    Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
    -1.6575  -0.7459  -0.5714  -0.2552   2.5099  

    Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)    0.71792    0.96165   0.747 0.455336    
    gendermale     0.28665    0.23973   1.196 0.231811    
    age           -0.04494    0.01831  -2.454 0.014142 *  
    yearsmarried   0.09686    0.03236   2.993 0.002758 ** 
    childrenyes    0.37088    0.29466   1.259 0.208147    
    religiousness -0.32230    0.09003  -3.580 0.000344 ***
    education      0.01795    0.05088   0.353 0.724329    
    occupation     0.03210    0.07194   0.446 0.655444    
    rating2       -0.02312    0.58177  -0.040 0.968303    
    rating3       -0.84532    0.57619  -1.467 0.142354    
    rating4       -1.13916    0.55740  -2.044 0.040981 *  
    rating5       -1.61050    0.56649  -2.843 0.004470 ** 
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 675.38  on 600  degrees of freedom
    Residual deviance: 608.22  on 589  degrees of freedom
    AIC: 632.22
</code></pre>

<p>After removed the insignificant variables, the reduced model look like below,although the AIC decreasd, we still do not know if this is the model with the lowest AIC we can achieved:</p>

<pre><code>    &gt; summary(fit.reduced)
    Call:
    glm(formula = ynaffair ~ age + yearsmarried + religiousness + 
        +rating, family = binomial(), data = Affairs)

    Deviance Residuals: 
    Min        1Q      Median      3Q      Max  
   -1.5117  -0.7541  -0.5722  -0.2592   2.4123  

    Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)    1.10220    0.71849   1.534 0.125014    
    age           -0.03588    0.01740  -2.062 0.039224 *  
    yearsmarried   0.10113    0.02933   3.448 0.000565 ***
    religiousness -0.32571    0.08971  -3.631 0.000282 ***
    rating2        0.11848    0.57258   0.207 0.836068    
    rating3       -0.70168    0.56671  -1.238 0.215658    
    rating4       -0.96190    0.54230  -1.774 0.076109 .  
    rating5       -1.49502    0.55550  -2.691 0.007118 ** 
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 675.38  on 600  degrees of freedom
    Residual deviance: 613.63  on 593  degrees of freedom
    AIC: 629.63
</code></pre>

<p>And we perform the ANOVA, suggesting that the reduced model with
four predictors fits as well as the full model:</p>

<pre><code>    &gt; anova(fit.reduced, fit.full, test=""Chisq"")
    Analysis of Deviance Table

    Model 1: ynaffair ~ age + yearsmarried + religiousness + +rating
    Model 2: ynaffair ~ gender + age + yearsmarried + children + 
             +religiousness + education + occupation + rating
    Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
     1       593     613.63                     
     2       589     608.22  4   5.4124   0.2475
</code></pre>
"
"0.0899550337218996","0.0986287303940589","164705","<p>I'm working on analyzing a time series of physical variables in many lakes in Florida for an associate, and I've run into an issue. I'm attempting to run a regression for each time series of physical variables in each lake. I can get regression results in R easily, but they don't match up with my coworker's JMP results. Anyway, here's a sample from the data:</p>

<pre><code>Year = seq(1987,2015)
TP = c(14, 12, 14, 14, 17, 16, 15, 12, 18, 14, 15, 18, 18, 21, 21, 17, 17, 20, 19, 17, 18, 18, 26, 20, 18, 21, 21, 20, 18)
summary(lm(TP~Year))
</code></pre>

<p>gives </p>

<pre><code>Call:
lm(formula = TP ~ Year)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.7310 -1.3724 -0.4305  0.9685  6.3675 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -502.90542   98.13981  -5.124 2.18e-05 ***
Year           0.26010    0.04904   5.303 1.35e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.21 on 27 degrees of freedom
Multiple R-squared:  0.5102,    Adjusted R-squared:  0.4921 
F-statistic: 28.12 on 1 and 27 DF,  p-value: 1.35e-05
</code></pre>

<p>His JMP analysis spits out the following:</p>

<pre><code>Parameter Estimates
Term        Estimate    Std Error   t Ratio Prob&gt;|t|
Intercept   -500.4634   96.74332    -5.17   &lt;.0001*
Year        0.2588707   0.048347    5.35    &lt;.0001*
</code></pre>

<p>For all lakes and all parameters of interest, the SS, slope estimates, etc. are all slightly off. I have looked into different types of Sum of Squares for ANOVA, but changing to different types (e.g. Type III using Anova()) still doesn't get the results to match up. What am I missing? Any assistance would be appreciated.</p>

<p>Edit: Thanks for y'all's help. Sorry for the belated response, I had to meet up with my colleague. To address the questions:</p>

<ul>
<li>I have hardcoded the data in my question, but it's merely a subset of a much larger dataset from Excel. We are using the same data and the remainder of my code is working properly. <a href=""https://www.dropbox.com/s/k21v38sfdbm0ola/LWFormatted.csv?dl=0"" rel=""nofollow"">Here's what the actual data look like.</a></li>
<li>I know OLS isn't great, but it's being used for some really basic trend descriptions for informing stakeholders. I may pursue a better option in the future.</li>
<li>The JMP model is setup using Y by X with the Bivariate option, then applying a linear regression. Below is a screenshot.</li>
</ul>

<p>Thanks again for your help!</p>

<p><a href=""http://i.stack.imgur.com/ndtBr.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ndtBr.jpg"" alt=""JMP Input""></a></p>
"
"0.0547448901451359","0.0540211804549215","164973","<p>I am trying to fit a survival analysis in <code>R</code> with non-recurrent events and time-varying coefficients. The baseline distribution is exponential or Weibull and the frailty distribution is gamma distributed. I have roughly 900.000 rows. </p>

<p>So far I have tried the <code>parfm</code> and <code>frailtypack</code>. Though, neither has worked â€“ they just keep running and never return. The calls for <code>parfm</code> and <code>frailtypack</code> are similar to respectively: </p>

<pre><code>frailtyPenal(Surv(stop-start,event)~.-start-stop-event-year+cluster(temp$year), 
             data= regressionData, hazard=""Weibull"", RandDist=â€Gammaâ€)

parfm(Surv(stop-start,event)~.-start-stop-event-year,cluster=â€yearâ€, 
      regressionData, dist=""exponential"", frailty = ""gamma"")
</code></pre>

<p>Where <code>event</code> is zero-one coded. My guess so far is to use the <code>lme4</code> package with the function <code>glmer</code> where the family is Poisson, the respond are zero-one coded, the offset is the difference in time and random effect is an intercept for the year factor. I.e. something like:</p>

<pre><code>glmer(event ~.-start-stop-event-year+(1|year), family = Poisson(), offset=stop-start)
</code></pre>

<p>I know that this will yield Gaussian distributed random effects and not Gamma. Further, I am not sure that I get the model I want. My goal is to have exponential distributed conditional waiting times and hence I chose the Poisson distribution. Question is whether this is correct? Any suggestions on other packages that will do the job?</p>
"
"0.101367839312414","0.116699087583415","166584","<p>I am conducting a regression in order to predict a tennis player's service point win % i.e. the percentage of points he wins when he is the server.
Model 1 If my DV data lies in the range 0.3-0.9, does it make sense to use a logistic regression? If using logistic I would endeavor to build a model with serve win % as my DV and my IV's as:</p>

<p>+average serve win % of last n matches (maybe n=5 or 10) to account for form </p>

<p>+surface </p>

<p>+player ranking </p>

<p>+opposition ranking</p>

<p>..... Would this be a good model to use? Preliminary logistic regressions just involving serve win % regressed on surface + player ranking + opponent ranking ... are showing some strange results so im losing faith in logistic for this data.</p>

<p>An alternative I'm considering is to use raw variables in a linear regression type model with interactions.... Along the lines of Aiken &amp; West 1991
My dependent variable will be number of service points won in match, and my independent variables will be:</p>

<p>+no. service points played in match +the surface the match played on </p>

<p>+the player's ranking points +the opponents ranking points</p>

<p>+an interaction between player and opponent ranking points </p>

<p>+an interaction between surface and no. points played </p>

<p>+average service points won in last n matches</p>

<p>+average % of service points won in last m matches</p>

<p>Do either of these models stand out as smart or appropriate ways to model this data? For context, for each player I have between 100-350 matches worth of data. I would love to hear what you guys think, or if you have any other suggestions on how to predict serve win % using the stated variables I would really appreciate it. I'm conducting this analysis in R so any code/package suggestions would also be great </p>
"
"0.100365631932749","0.108042360909843","167363","<p>I have no training in Bayesian data analysis, so I can't wrap my head around how to start solving the following problem and am hoping you can help:</p>

<p>I am using linear regression to forecast the net scores (home - visitor) of (American) pro-football games from differences in team-strength scores (home - visitor). Those strength scores fall on a 0-100 scale, and they represent the percent chance that the team in question would beat another team selected at random from the 31 others in the league. The differences between those strength scores and the net game scores are both normally distributed.</p>

<p>Right now, I am using team-strength scores that are fixed for the entire season in a mixed-effects model that also includes random intercepts for each team as the home team. The strength scores are fixed because they come from a preseason survey. I would like to see if I can make the predictions more accurate by using Bayesian updating to allow that team-strength score to vary over the course of the season, as we learn more about how teams are performing relative to preseason expectations.</p>

<p>The single piece of information that strikes me as most useful in that regard is the cumulative sum of each team's prediction errors --- in other words, the cumulative sum of the differences between the team's predicted game performance (based on the preseason strength scores and where each game is played) and its actual game performance. </p>

<p>How might I go about doing that? In R, I have gotten as far as computing those cumulative errors, which turn out to be normally distributed for the season with a mean of ~0 and sd of ~50. I have tinkered with algebraic ways to adjust the strength scores as a function of that cumulative error. The forecasts based on those algebraic adjustments are slightly more accurate, but the approach seems clunky, and I'd like to use this problem as an opportunity to learn about Bayesian updating if I can. Any suggestions on how to do that in the context of this problem --- and, ideally, in R --- would be much appreciated.</p>
"
"0.047410465593076","0.0623782861551805","167947","<p>I understand total variance and R squared in linear regression outputs, but I have difficulty to understand the percent of variation explained by each covariates in a multiple regression analysis. I have two question.
1. How could I explaine the %var explained by one covariates in multiple regression?. 
2. Above all how could implement this in Stata or R?. A paper on Table 4, page 7 of the link below has provided such output.  Could anyone explain the 1.3%  output of Gas stove?  The adjusted R squared for the whole regression is 0.79 and the % variance explained by the covariates is 56.8. What does explain the rest?</p>

<p><a href=""http://www3.imperial.ac.uk/portal/pls/portallive/docs/1/7292108.PDF"" rel=""nofollow"">http://www3.imperial.ac.uk/portal/pls/portallive/docs/1/7292108.PDF</a></p>

<p>Thanks</p>
"
"0.137771618924287","0.135950322810847","168068","<p>I am using R for this analysis, and so examples and graphics will be produced in this language. I am willing to provide equivalent examples in similar languages if it will help someone, and am willing to accept answers in terms of other languages.</p>

<p>In this question, I intend to display graphs produced in order to verify assumptions, and ask for help in getting a better model. I understand that this may be considered too specific. However, it is my opinion that it would be helpful to have more examples of bad models and how to correct them on this site. If a moderator finds this not to be the case, I will happily delete this post.</p>

<p>I have conducted an initial linear model (lm) in R. It is multiple categorical regression with approx 100,000 cases, two categorical regressors and a continuous regressand. The goal of this regression is prediction: specifically, I would like to estimate prediction intervals. Find below some diagnostics of the initial model:</p>

<p>Residuals histogram (full) below. It may be difficult (impossible) to see, but there exist (sparse) values between 300 and 2000, as well as -50 and -500. Between -50 and 300, values are very dense. This indicates, to my understanding, heavy tails.</p>

<p><a href=""http://i.stack.imgur.com/FoGN7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FoGN7.png"" alt=""Residuals Historgram""></a></p>

<p>Residuals histogram (partial) below. Same image as above, but zoomed to the dense area.</p>

<p><a href=""http://i.stack.imgur.com/Q9bBl.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Q9bBl.png"" alt=""enter image description here""></a></p>

<p>A normal Quantile Quantile (normal QQ plot) is found below. Again, according to the <a href=""http://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot"">holy grail of qqplots</a>, (super) heavy tails are indicated.</p>

<p><a href=""http://i.stack.imgur.com/TrATp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TrATp.png"" alt=""Initial QQPlot""></a></p>

<p>Below is predicted vs residuals. Clearly, funky stuff is going on, suggesting heteroscedasticity:</p>

<p><a href=""http://i.stack.imgur.com/oOMRU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oOMRU.png"" alt=""Resid Vs Predicted""></a></p>

<p>I first tried some transformations. BoxCox yields a value very close to zero. So I will try to take the log of the regressand (in accordance with <a href=""https://en.wikipedia.org/wiki/Power_transform#Box.E2.80.93Cox_transformation"" rel=""nofollow"">the Wikipedia page</a>). </p>

<p><a href=""http://i.stack.imgur.com/3IbMv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3IbMv.png"" alt=""boxcox""></a></p>

<p><strong>Log Transform:</strong></p>

<p>Log transformed histogram, looks a lot better, but we still have some skew:</p>

<p><a href=""http://i.stack.imgur.com/exSgd.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/exSgd.png"" alt=""Histogram of Log transform""></a></p>

<p>And the NormalQQ Plot. Still seems that the residuals are not normally distributed.</p>

<p><a href=""http://i.stack.imgur.com/JosvR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JosvR.png"" alt=""log QQPlot""></a></p>

<p>Logarithm transformed Residual vs Predicted. Seems we have some decreasing variance now, but I would be willing to accept this assumption.</p>

<p><a href=""http://i.stack.imgur.com/Sg4B9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Sg4B9.png"" alt=""Log Resid Vs Predicted""></a></p>

<p>Other transformations I tried: raising regressand to powers 1/2, 1/3 and -1. None of these had satisfactory results; I choose not to include information about these transformations in order to save space, but will happily provide such information should it be requested.</p>

<p><strong>Here lie my questions:</strong></p>

<p>1) Is the solution to this problem simply to keep trying increasingly wacky transformations (ex: $1/log(x^{\pi/3})$)?</p>

<p>2) I have been looking (intermittently over a period of weeks) at Generalized Linear Models, which seem to allow a non-normal distribution of residuals. Unfortunately, I have not been able to understand them, and non of my (undergraduate statistics) peers have knowledge of them. If GLM's present a solution to this issue, I would be grateful if someone could explain them in this context. (Even if they are not a solution, I would be grateful for a simple explanation, or a reference to one).</p>

<p>2i) If GLM's are a good fit, I believe I would still need a distribution to model error by. What ways are there of detecting which (family) of distribution is the best fit for the residuals, after which I assume I can perform MLE to get the parameters? I've been having issues trying to evaluate heavy tailed distributions with respect to skew, because they tend not to have any moments, and so have $\infty$ or indeterminate skew.</p>

<p>3) Is there another class of models not aforementioned I should look into?</p>

<p>4) Is my current model sufficient for prediction intervals, despite the non-normality of residuals?</p>

<p>Some more information about the model: I am predicting a cost, thus the log transform is appealing in that my predicted values are positive reals.</p>

<p>I will be hanging around my computer all day, and have R gui open on my other monitor, so should be able to fulfill most requests for additional information.</p>
"
"0.0547448901451359","0.0540211804549215","168182","<p>I am trying to create a model of refrigeration having the energy consumption and the temperature over time. So far, I've tried regression but fitting this data into linear model seems impossible. Another thing that I've tried is cross correlation but it's insignificant (around 0.11 at lag 0). I also clustered the data and for another fridge I was able to state that if the fridge is in 'idle mode' (e.g. not consuming electricity) the temperature goes above certain value. However, for this fridge, this doesn't work as the data seems pretty random. Here is a scatter plot of the data, the bigger the circle, the higher the frequency.
<a href=""http://i.stack.imgur.com/1w5lU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1w5lU.png"" alt=""enter image description here""></a></p>

<p>Any ideas what type of analysis can I use to derive insights from this? I would like to know if there is any correlation between the kW data and the temperature data. A new plot for the full duration that I have:</p>

<p><a href=""http://i.stack.imgur.com/yfZAr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yfZAr.png"" alt=""enter image description here""></a></p>
"
"0.070675349274022","0.0697410440814588","168308","<p>I am currently performing a meta-analysis on bank relationship and firm performance with 27 different studies; almost all of them report different cases.</p>

<p>I calculated the partial effect size (I have different dependent and independent variable), its variance and confidence interval using the formula provided by Aloe &amp; Thompson, ""The synthesis of partial effect size"".</p>

<p>Due to the studies-specifics I have now to face two issues:
- Studies have different sample size and different specification, so heteroskedasticity is likely to arise;
- Studies present several cases, so some of the observation in my meta-analysis are not independent;</p>

<p>I thought to face the second issue by using what in econometrics would be called an ""study-fixed effect regression"" that, if I got correctly, in the metafor package should be:</p>

<pre><code>  res &lt;- rma(Yi, Vi, mods = ~ I(study))
  res
</code></pre>

<p>That is, a Mixed-Effects Model. I would also like to compare these results with the one of a multilevel\hierarchical model (Gelman &amp; Hill, 2006).</p>

<p>Is the <code>ram.mv</code> package good for that or I have to manually write the function following Gelman &amp; Hill? If it is good, how can I can calculate my Variance-Covariance Matrix considering the fact that I have different independent variable? Last, how can I account for heteroskedasticity?</p>

<p>Thank you for your time</p>
"
"0.0446990156267674","0.0441081091391231","168336","<p>I ran a multiple regression analysis and got significant results for lFreq, Len variables, and interaction lFreq x Len. Now I need to report these results and I am a bit confused whether F(7, 924) = 9.876 and R^2= 0.06961 stand for all variables? It seems to me that each variable should have its own F and R^2 values... How should I calculate them? </p>

<pre><code>Call:
lm(formula = duration ~ lFreq * Len * group, data = df.eyes)

 Residuals:
  Min        1Q    Median        3Q       Max 
-0.079878 -0.010185 -0.001281  0.009893  0.192742 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       0.2547662  0.0110199  23.119   &lt;2e-16 ***
lFreq            -0.0038043  0.0012332  -3.085   0.0021 ** 
Len              -0.0034335  0.0014160  -2.425   0.0155 *  
group2            0.0186209  0.0156136   1.193   0.2333    
lFreq:Len         0.0003494  0.0001617   2.160   0.0310 *  
lFreq:group2     -0.0005048  0.0017458  -0.289   0.7725    
Len:group2       -0.0035563  0.0020054  -1.773   0.0765 .  
lFreq:Len:group2  0.0001673  0.0002290   0.731   0.4652    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.01765 on 924 degrees of freedom
Multiple R-squared:  0.06961,   Adjusted R-squared:  0.06256 
F-statistic: 9.876 on 7 and 924 DF,  p-value: 6.388e-12
</code></pre>
"
"0.0716778865720364","0.0825187161885156","168857","<p>I have a fairly large dataset of the following form, and I want to run a linear regression returning coefficients for each factor:</p>

<pre><code>Case    Variable1   Variable2   Result
1       Factor1     FactorA     50
2       Factor2     FactorA     60
3       Factor1     FactorB     55
4       Factor2     FactorB     65
...     ...         ...         ...
</code></pre>

<p>Running a linear regression using <code>lm()</code> on this would be very straightforward, but the size of the dataset seems to be too large.  I have about 1,000,000 cases, with about 10,000 factors in each variable.</p>

<p><code>lm()</code> (or other standard linear regression methods) translates this to an extremely wide matrix where each factor is a Boolean variable, correct?  So ~ 20,000 wide x 1,000,000 tall?  Running <code>lm()</code> on just a 25,000 case sample still takes several minutes and over a gb of memory.</p>

<p>My initial thought was to attack this regression problem using package <code>biglm</code>, but for it to behave properly, I believe <code>biglm</code> requires every factor to be present in every ""chunk"" of data it digests.  This would not occur in my data; some of the factors are only present a few times.  This is called <em>rank deficiency</em>, I believe. (However, an answer at <a href=""http://stackoverflow.com/questions/10502882/r-biglm-with-categorical-variables"">this StackOverflow question</a> indicates there might be a workaround?)</p>

<p>So my question: is there a better way to structure my data to run this regression?  Is there a better package or approach I should be using to run this analysis?</p>
"
"0.0446990156267674","0.0441081091391231","169188","<p>During my analysis that I perform with a collaborator we get consistently different ouputs in cox regression between R and SPSS. Betas and p-values differ minutely, usually somewhere in the third or fourth decimal, though sometimes more. This occurs both for factors and continuous variables, so it does not appear to be a rounding issue in covariates. </p>

<p>In essence the entire database is prepared in SPSS, which includes calculation of follow-up. The file is then loaded into R via get.spps in HMISC. 
The follow-up does not consist of whole numbers (for days), could it be that one of the software packages rounds follow-up to whole numbers? Or are the methods calculation different? We use SPSS version 22, and 3.12 with coxph() from the survival package. </p>

<p>Did anyone encounter something similar? </p>
"
"NaN","NaN","171569","<p>I'm doing survival analysis with the ""riskregression"" package in R. 
What I want to do is to obtain the Fine&amp;Gray CIF with confidence intervals for my individual outcome(s) with death as a competing risk. 
Further investigated covariate influence on survival - similar to as one would in a cox regression model.  </p>

<p>I'm pretty new as far as R go, but I have some Ph.D. courses on it. 
My problem is mostly coding related.
Before it get any further into my problem is there anyone who can offer som help?
It would be much much appreciated!</p>

<p>Best</p>
"
"0.0547448901451359","0.0540211804549215","171658","<p>I have two groups, a treatment group and a comparison group.
Each group is measured on a variable 'A' 6 times before the treatment was given, and 6 times after.
I read that using a difference-in-difference analysis would be the way to analyze this data, as it's longitudinal data without a proper randomized control trial.</p>

<p>But DID is typically done with just one value before and after treatment - in this case, do I sum up all 6 values into one, to be the value before treatment?</p>

<p>I know how to calculate the DID estimator by hand, but how do I figure out its significance in R (I believe it's supposed to be done using regression)?</p>

<p>ie. is this correct?</p>

<pre><code>lm(value ~ group + pre_post_flag + group*pre_post_flag, data=df)
</code></pre>
"
"0.078223277346843","0.0771891909934654","171745","<p>[Moved from stack overflow)
While I have done a good amount of reading regarding the usage of PSM, I am still struggling a bit to see if it can be used in my application. </p>

<p>I am trying to analyze the impact of an advertising campaign on in-store sales lift. While I know Propensity Scores are typically used in non-randomized studies, this study will be randomized (some stores will randomly see the advertising campaign, some will not). I have sales level data by store (such as pre-sales volume etc), and I want to conduct a sub-group analysis, to make sure I am comparing stores that behave similarly (start out at same level of sales volume, received roughly the same amount of spend, etc).<br>
Can anyone advise the best way to go about this?</p>

<p>I was initially thinking about using propensity scores to formulate the matching, but I know that seems counter-intuitive in this randomized setting.  However, in what way should I go about making a matched control group based on those variables? Would it still use the MatchIt package- but does that strictly use some variation of Propensity Scores? </p>

<p>In addition, after a matched set, would a simple multiple regression be the best means of finding the treatment effect? </p>

<p>Any thoughts/help greatly appreciated, thanks!</p>
"
"0.0547448901451359","0.0540211804549215","171763","<p>I am working on a paper about sexual coherence in women. Sexual coherence is defined as the relationship between subjective (SA) and genital sexual (GA) arousal. There is research that shows that this coherence can be higher or lower, depending on other factors, like age or arousability...</p>

<p>Both measures (SA and GA) have been measured continuously over a period of 5 minutes. I divided these 5-minutes into 15-second intervalls and calculated the mean for both arousal measures for each section.</p>

<p>Additionally, I have 2 questionnaire scores (P1, P2) that might influence SA, GA or (most importantly) the relationship between SA and GA</p>

<p>I use the package nlme in R and my data is transformed into long format.</p>

<p>My first question: Is it, in your opinion, possible to assess sexual coherence between GA and SA with a regression analysis, in which SA is the outcome and GA is the predictor?</p>

<p>My second question: If I want to investigate the impact of P1 and P2 on sexual concordance (the association between GA and SA), is it feasible to add the questionnaires to the above mentioned regression?
The model would look something like this: Coherence.model &lt;-nlme ( SA ~ GA + P1 + P2 + (GAP1) + (GAP2) + (P1*P2)) My idea is that you can assess the direct influence of GA, P1 and P2 on SA and (if the interaction terms (GA*P1) is significant) you can say that, e.g., P1 is a moderator of the relationship between GA and SA.</p>

<p>What do you think? Or do you have another idea, who to work with an ""coherence measure"" as outcome variable?</p>

<p>Best, Julia</p>

<p>Please excuse that I did not get into detail regarding syntax or programming. But I hope that this is not necessary at this moment.</p>
"
"0.0446990156267674","0.0441081091391231","172958","<p>I am working on a school enrollment admission project to see how high school students react to scholarship in admission. The purpose is to redesign the scholarship level.</p>

<p>The original policy is 3 levels(0,2000,4000,6000) and used as training data. 
The other attributes are like GPA, ACT/SAT, gender,etc.. Y={enrolled, not enrolled}</p>

<p>What I did is manually expand the levels to (0,1000,2000,...,6000) for this year as testing data. And I used logistic regression and regression tree(LOTUS). </p>

<p>Ideally the probability will increase as the scholarship increases and it will give a sigmoid or S-curve, but not all the plots shown this. I think the reason is there are no data in the training set has the new levels.</p>

<p>I tried conjoint analysis but I don't know what does it mean.</p>

<p>what methods should I use or do I miss something here? </p>
"
"0.0799600299750219","0.0789029843152472","173026","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>

<p><strong>EDIT</strong> The result of the features reversed as commented by @Michael M:</p>

<pre><code>&gt; model_All2 &lt;- lm(y ~ x2 + x1, data=df)
&gt; anova(model_All2)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x2         1 17.468  17.468  22.907 0.0001718 ***
x1         1 53.612  53.612  70.304 1.914e-07 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0670485234401511","0.0771891909934654","173047","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>
"
"0.0516139774092509","0.0763974860547543","173394","<p>I'm working on an analysis of a team-based dataset, where teams of 3 compete against each other under specific, identical tasks. The hierarchy therefore is person :: team :: task. Or, put another way, 6 people :: 2 teams :: 1 task. I am using <code>lme4</code> in R for linear regression (to predict a continuous variable).</p>

<p>For the hierarchy, I'm writing in the regression equation <code>(1|team)</code> and <code>(1|task)</code> for random effects (with a bunch of other fixed effect variables). However, in our dataset, each set of teams within any task is coded as ""left"" or ""right"" team (ie., 0 or 1, across each task). </p>

<p>When setting up the regression equation in <code>lme4</code>, do I need to recode team_id as a unique value taking into account task_id? E.g., I could concatenate to get unique team values, so task_1_team_0, task_1_team_1, task_2_team_0, task_2_team_1, etc.? Or will the <code>lme4</code> package take care of this for me, and I can keep the 2nd level values as 0 and 1?</p>

<p>I want to make sure that it is not putting half the dataset into one team and half into another team, because they are all coded the same, and then applying the task level on top of ""two"" teams.</p>
"
"NaN","NaN","173692","<p>What's the difference between:... ?</p>

<p><code>lm( cbind(y1,y2) ~ x)</code></p>

<p>and</p>

<p><code>lm( y1 + y2 ~ x)</code></p>

<p>Where y1 and y2 are dependent variables, and I want to do a regression analysis of both at the same time, to get more power.
What method assumes the same variance?</p>
"
"0.126427908248203","0.124756572310361","174136","<p>I have a dataset of a metric predictor variable $X$, and an ordered categorical predicted value $Y$ for several individuals. The dataset are from two groups $G_1$ and $G_2$. I want to estimate $Y$ from $X$, and I want to be able to compare the forecast accuracy of models, in group and individual level. For example, I want to know if these models helps to estimate $Y$ from a $X$ for a new user of a category, or a new experiment from the same user of a known category?</p>

<p>In <a href=""https://en.wikipedia.org/wiki/Ordered_probit"" rel=""nofollow"">ordered probit</a>, we suppose that $Y^*$ is the exact but unobserved dependent variable, and $X$ is the vector of independent variables, and $\beta$ is the a regression coefficient which we wish to estimate.</p>

<p>$Y^* = \mathbf{x}' \beta + \epsilon$</p>

<p>We can not observer $y*$ directly, but we instead can only observe the categories of response:</p>

<p>$
Y= \begin{cases}
0~~ \text{if}~~y^* \le 0, \\
1~~ \text{if}~~0&lt;y^* \le \mu_1, \\
2~~ \text{if}~~\mu_1 &lt;y^* \le \mu_2 \\
\vdots \\
N~~ \text{if}~~ \mu_{N-1} &lt; y^*.
\end{cases}
$</p>

<p>I came across this article from Gelman et al. that describes Bayesian Hierarchical Model: <a href=""https://en.wikipedia.org/wiki/Ordered_probit"" rel=""nofollow"">Multilevel (Hierarchical) Modeling: What It Can and Cannot Do</a>, which has been implemented in Python <a href=""http://nbviewer.ipython.org/github/fonnesbeck/multilevel_modeling/blob/master/multilevel_modeling.ipynb"" rel=""nofollow"">here</a>.</p>

<p>I am processing data in R, and I have selected a <strong>thresholded Bayesian hierarchical model</strong> to use with the <strong>generalized linear model</strong>. I have calculated the parameters of it using MCMC. My question is that how should I compare accuracy of ordered probit, and the equivalent Bayesian hierarchical model in R?</p>

<p>Gelman has used <a href=""https://en.wikipedia.org/wiki/Root-mean-square_deviation"" rel=""nofollow"">RMSE</a> for comparison using cross-validation. First he <em>removed single data points and checked the prediction from the model fit to the rest of the data, then removed single counties and performed the same procedure. For each cross-validation step, we compare complete-pooling, no-pooling, and multilevel estimates.</em></p>

<p>I have done MCMC simulation using RJags, which gave me the posterior distribution of the parameters, but how can I compare posterior distribution with a single point estimate of <strong>ordered probit</strong> to compare accuracy? Should I do as Gleman did and use RMSE? How? Or should I compare posterior distribution with results of several experiments with ordered probit? Is <a href=""http://www.stat.columbia.edu/~gelman/presentations/ggr.pdf"" rel=""nofollow"">posterior predictive check</a> usable here? I usually prefer cross-validation, but I don't know how to do this here.</p>

<p>PS: The notion of <strong>Goodness of fit</strong> in Bayesian analysis is ambigious to me. <a href=""http://people.stat.sfu.ca/~tim/papers/survey.pdf"" rel=""nofollow"">This paper</a> states:</p>

<blockquote>
  <p>GOODNESS-OF-FIT:</p>
  
  <p>In Bayesian statistics, there is no consensus on the
  correct"" approach to the assessment of goodness-of fit. When Bayesian
  model assessment is considered, it appears that the prominent modern
  approaches are based on the posterior predictive distribution (Gelman,
  Meng and Stern 1996).</p>
</blockquote>
"
"0.178878210943523","0.181862392926876","174257","<p>I want to do a path analysis with lavaan but encounter a few problems and would appreciate any help.</p>

<p>The structural model looks like this:</p>

<p><a href=""http://i.stack.imgur.com/y8ZZh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y8ZZh.png"" alt=""structural model""></a></p>

<p>The relation between one observed independent (s) and one observed dependent variable (v) is mediated through a latent variable (m) that is defined by two observed indicator variables (x1, x2). This is basically a simplified version of the <a href=""http://lavaan.ugent.be/tutorial/sem.html"" rel=""nofollow"">SEM example</a> in the tutorial on the lavaan project website.</p>

<p>When I enter my code (given further below) into R, I encounter two problems:</p>

<p>(1) The results change when I change the order of the indicator variables.</p>

<p>This model:</p>

<pre><code># measurement model
    m =~ x1 + x2
</code></pre>

<p>returns a different result than this model:</p>

<pre><code># measurement model
    m =~ x2 + x1
</code></pre>

<p>How can that be? Isn't the order of the indicators arbitrary? And if not, how do I know which is the correct order, if my model does not presuppose a specific order?</p>

<p>(2) There are a few warnings that I don't understand: for the first model, no standard errors could be computed; and the second model did not ""converge"" (whatever that means). The warnings are given in context in the full code posted below.</p>

<p>What do I have to do to obtain reliable estimates?</p>

<hr>

<p>Here is the full R output to provide context to my questions.</p>

<pre><code># data

s &lt;- c(2, 5, 4, 4, 4, 8, 2, 9, 1, 1, 3, 3, 2, 3, 2, 5, 5, 7, 4, 7, 8, 4, 10, 10, 2, 4, 0, 2, 4, NA, 1, 5, 2, 6, 3, 5, 0, 5, 3, 6, 4, 9, 4, 9, 4, 5, 6, 1, 8, 0, 6, 9, 1, 5, 1, 6, 2, 5, 0, 5, 6, 2, 4, 10, 3, 4)
v &lt;- c(8, 10, 1, 4, 0, 2, 3, 2, 1, 1, 2, 5, 1, 5, 0, 5, 4, 5, 2, 10, 0, 6, 5, 5, 6, 1, 1, 0, 0, NA, 1, 0, 1, 8, 1, 3, 0, 5, 6, 3, 2, 10, 0, 5, 5, 10, 4, 1, 1, 0, 0, 0, 2, 10, 1, 8, 2, 3, 2, 2, 4, 4, 2, 5, 6, 2)
x1 &lt;- c(2.500000, 3.789474, 1.514563, 5.846868, 4.588235, 5.600000, 5.066667, 11.647059, 2.000000, NA, 4.461538, 18.000000, 1.058824, 9.217391, 27.840000, 15.375000, NA, 6.000000, 9.714286, 12.484848, 16.503497, 20.666667, 3.500000, 4.658824, 4.750000, 4.000000, 2.800000, 14.228571, 11.000000, NA, 2.666667, 3.764706, 4.705882, 13.272727, 2.000000, 18.444444, 17.555556, 14.222222, 2.000000, 4.000000, 8.461538, 19.200000, 13.902439, 13.000000, 3.000000, NA, 7.360000, 1.611374, 1.500000, 3.365854, 22.375000, 10.838710, 2.923077, 3.488372, 5.176471, 37.666667, 1.176471, 7.454545, 36.235294, 6.823529, 2.222222, 6.133333, 11.428571, 42.705882, 28.105263, 18.333333)
x2 &lt;- c(8.125000, 14.273684, 7.339806, 23.387471, 113.058824, 22.200000, 17.466667, 43.647059, 9.230769, NA, 13.538462, 83.555556, 5.058824, 37.391304, 100.000000, 59.250000, NA, 22.470588, 38.428571, 50.787879, 76.223776, 92.888889, 15.375000, 16.235294, 18.875000, 13.647059, 10.133333, 55.885714, 36.428571, NA, 6.933333, 13.294118, 14.117647, 81.818182, 6.117647, 67.777778, 76.333333, 51.888889, 6.428571, 14.200000, 34.000000, 59.680000, 68.634146, 40.500000, 12.250000, NA, 29.760000, 8.909953, 5.400000, NA, 71.125000, 39.741935, 9.846154, 13.116279, 18.823529, 204.000000, 4.588235, 49.090909, 188.470588, 19.647059, 10.222222, 22.933333, 38.285714, 140.235294, 137.526316, 79.000000)
dat &lt;- data.frame(cbind(s, v, x1, x2))

# first model

model &lt;- '
    # measurement model
        m =~ x2 + x1
    # regressions
        m ~ s
        v ~ s + m
    # residual correlations
        x1 ~~ x2
'
fit &lt;- sem(model, data = dat, missing = ""fiml"")

# Warning messages:
# 1: In lav_data_full(data = data, group = group, group.label = group.label,  :
#   lavaan WARNING: some cases are empty and will be removed:
#   30
# 2: In lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats,  :
#   lavaan WARNING: could not compute standard errors!
#   lavaan NOTE: this may be a symptom that the model is not identified.

summary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)

# lavaan (0.5-18) converged normally after 147 iterations
#
#                                                   Used       Total
#   Number of observations                            65          66
#
#   Number of missing patterns                         3
#
#   Estimator                                         ML
#   Minimum Function Test Statistic                0.565
#   Degrees of freedom                                 0
#   Minimum Function Value               0.0043451960201
#
# Model test baseline model:
#
#   Minimum Function Test Statistic              126.904
#   Degrees of freedom                                 6
#   P-value                                        0.000
#
# User model versus baseline model:
#
#   Comparative Fit Index (CFI)                    0.995
#   Tucker-Lewis Index (TLI)                       1.000
#
# Loglikelihood and Information Criteria:
#
#   Loglikelihood user model (H0)               -797.558
#   Loglikelihood unrestricted model (H1)       -797.275
#
#   Number of free parameters                         12
#   Akaike (AIC)                                1619.115
#   Bayesian (BIC)                              1645.208
#   Sample-size adjusted Bayesian (BIC)         1607.435
#
# Root Mean Square Error of Approximation:
#
#   RMSEA                                          0.000
#   90 Percent Confidence Interval          0.000  0.000
#   P-value RMSEA &lt;= 0.05                          1.000
#
# Standardized Root Mean Square Residual:
#
#   SRMR                                           0.027
#
# Parameter estimates:
#
#   Information                                 Observed
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(&gt;|z|)   Std.lv  Std.all
# Latent variables:
#   m =~
#     x2                1.000                              14.272    0.330
#     x1                0.384                               5.482    0.588
#
# Regressions:
#   m ~
#     s                 1.732                               0.121    0.323
#   v ~
#     s                 0.335                               0.335    0.306
#     m                 0.012                               0.171    0.059
#
# Covariances:
#   x2 ~~
#     x1              292.112                             292.112    0.951
#
# Intercepts:
#     x2               35.558                              35.558    0.823
#     x1                7.220                               7.220    0.775
#     v                 1.761                               1.761    0.604
#     m                 0.000                               0.000    0.000
#
# Variances:
#     x2             1663.119                            1663.119    0.891
#     x1               56.783                              56.783    0.654
#     v                 7.591                               7.591    0.892
#     m               182.367                               0.895    0.895
#
# R-Square:
#
#     x2                0.109
#     x1                0.346
#     v                 0.108
#     m                 0.105

model &lt;- '
    # measurement model
        m =~ x1 + x2
    # regressions
        m ~ s
        v ~ s + m
    # residual correlations
        x1 ~~ x2
'
fit &lt;- sem(model, data = dat, missing = ""fiml"")

# Warning messages:
# 1: In lav_data_full(data = data, group = group, group.label = group.label,  :
#   lavaan WARNING: some cases are empty and will be removed:
#   30
# 2: In lavaan::lavaan(model = model, data = dat, missing = ""fiml"", model.type = ""sem"",  :
#   lavaan WARNING: model has NOT converged!

summary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)

# ** WARNING ** lavaan (0.5-18) did NOT converge after 9438 iterations
# ** WARNING ** Estimates below are most likely unreliable
#
#                                                   Used       Total
#   Number of observations                            65          66
#
#   Number of missing patterns                         3
#
#   Estimator                                         ML
#   Minimum Function Test Statistic                   NA
#   Degrees of freedom                                NA
#   P-value                                           NA
#
# Parameter estimates:
#
#   Information                                 Observed
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(&gt;|z|)   Std.lv  Std.all
# Latent variables:
#   m =~
#     x1                1.000                               0.526    0.056
#     x2             1606.326                             845.326   19.343
#
# Regressions:
#   m ~
#     s                -0.001                              -0.001   -0.004
#   v ~
#     s                 0.355                               0.355    0.325
#     m                 0.004                               0.002    0.001
#
# Covariances:
#   x1 ~~
#     x2              -69.375                             -69.375   -0.009
#
# Intercepts:
#     x1               10.099                              10.099    1.083
#     x2               48.281                              48.281    1.105
#     v                 1.761                               1.761    0.604
#     m                 0.000                               0.000    0.000
#
# Variances:
#     x1               86.614                              86.614    0.997
#     x2            -712666.446                            -712666.446 -373.157
#     v                 7.617                               7.617    0.895
#     m                 0.277                               1.000    1.000
#
# R-Square:
#
#     x1                0.003
#     x2                   NA
#     v                 0.105
#     m                 0.000
# Warning message:
# In .local(object, ...) :
#   lavaan WARNING: fit measures not available if model did not converge
</code></pre>

<hr>

<p><em>Note.</em> I have posted the same question to the <a href=""https://groups.google.com/forum/#!forum/lavaan"" rel=""nofollow"">lavaan Google Group</a>, but this is part of my bachelor's thesis, which I have to turn in on Monday, so I'm a bit pressed for time and hope you forgive me for crossposting.</p>
"
"0.0774209661138764","0.0763974860547543","174518","<p>consider the following data set:</p>

<pre><code>a &lt;- c(1, 2, 3, 1, 4, 1968, 2, 1)
b &lt;- c(2, 1, 2, 4, 3, 1984, 2, 0)
c &lt;- c(3, 3, 4, 2, 1, 1945, 1, 0)
d &lt;- c(4, 1, 4, 3, 2, 1975, 3, 1)
df &lt;- data.frame(rbind(a,b,c,d))
names(df) &lt;- c(""ID"", ""OptionW"", ""OptionX"", ""OptionY"", ""OptionZ"", ""yearofBirth"", ""education"", ""sex"")


ID OptionW OptionX OptionY OptionZ yearofBirth education sex
1       2       3       1       4        1968         2   1
2       1       2       4       3        1984         2   0
3       3       4       2       1        1945         1   0
4       1       4       3       2        1975         3   1
</code></pre>

<p>Two hundred people where asked to rank Options W to Z from 1 to 4 in their effectiveness to lower crime rates in their community. Their age, highest academic degree and sex are annotated as well.
I want to find out:</p>

<ul>
<li>which options are preferred by the majority of citizens?</li>
<li>are there significant differences in what men or women, old or young, well or less well educated citizens believe?</li>
<li>how likely is the ranking order going to change if the person is older/younger, has had more or less formal education and is male or female? </li>
</ul>

<p>I read that a multinomial logistic regression might be the way to go, but I find it hard to adapt the examples I find to my data set. Often they allow for only one option to be chosen, making each choice (W, X Y Z) a level of one variable (Options). But in my case I have several variables (OptionW, OptionX, OptionY, OptionZ) where the ranking placement appears to be the level (1,2,3,..10). Or am I looking at it the wrong way?</p>

<p>Which function from what package would be suitable? And are there other methods available apart from mlr?</p>

<p>I use R mostly for spatial analysis and am not very fluent in statistics. Hopefully you can help me here.</p>
"
"0.0899550337218996","0.0887658573546531","174861","<p>Here is <a href=""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"" rel=""nofollow"">sample data</a>:</p>

<pre><code>    brainIQ &lt;- 
  read.table (file= ""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"",
 head = TRUE)
</code></pre>

<p>I am trying to fit multiple linear regression.</p>

<pre><code>mylm &lt;- lm(PIQ ~  Brain + Height + Weight, data = brainIQ)
anova(mylm)
</code></pre>

<p>Default function anova in R provides sequential sum of squares (type I) sum of square. </p>

<pre><code>Analysis of Variance Table

Response: PIQ
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
Brain      1  2697.1 2697.09  6.8835 0.01293 *
Height     1  2875.6 2875.65  7.3392 0.01049 *
Weight     1     0.0    0.00  0.0000 0.99775  
Residuals 34 13321.8  391.82                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I belief, thus the SS are Brain, Height | Brain, Weight | (Brain, Weight) and residuals respectively.</p>

<p>Using package car we can also get type II sum of square. </p>

<pre><code>library(car)
Anova(mylm, type=""II"")
Anova Table (Type II tests)

Response: PIQ
           Sum Sq Df F value    Pr(&gt;F)    
Brain      5239.2  1 13.3716 0.0008556 ***
Height     1934.7  1  4.9378 0.0330338 *  
Weight        0.0  1  0.0000 0.9977495    
Residuals 13321.8 34                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Here sum of squares are like: Brian | (Height, Weight), Height | (Brain, Weight), Weight | (Brain, Height).</p>

<p>Which look pretty like Mintab output:</p>

<p><a href=""http://i.stack.imgur.com/0iXgH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0iXgH.png"" alt=""enter image description here""></a></p>

<p>My question is how can I calculate the regression row in the above table in R ? </p>
"
"0.047410465593076","0.0623782861551805","174920","<p>Are there any easy to use alternatives to stepwise variable selection for GLMMs? I have seen implementations of e.g. LASSO for linear regression, but so far not seen anything for mixed models. Mixed models seem non-trivial in general, so I am wondering if any of the fancy new methods have been adapted from them (and possibly implemented in R). Using whatever selection procedure you like and then validating the results seems a sensible way to go in the meantime.</p>

<p>To give some context: in my current project, I am looking at approximately 700 variables and 5000 binary observations. Stepwise selection takes about 1 day; many variables have about 10% missingness.</p>

<p>Edit: Thank you for the very interesting answers so far! Two concerns that I have are: do these new methods have longer runtimes than stepwise selection and can they deal with missing data (if each variable has different missingness, than for hundreds of variables it is very easy to loose all observations in a complete case analysis - something that stepwise selection can deal with by only using small subsets of the available variables at the same time).</p>
"
"0.0632139541241014","0.0623782861551805","174970","<p>I just started learning churn analysis, and I'm trying to analyze the churn data found here:
<a href=""http://www.sgi.com/tech/mlc/db/churn.data"" rel=""nofollow"">http://www.sgi.com/tech/mlc/db/churn.data</a>
in R, and there are a few concepts I don't entirely get and I would be very grateful if I could get them answered. </p>

<p>I used Kaplan-Meier Estimators since the data is right-censored, and obtained the churn rate.</p>

<p>Now when I tried analyzing different trends (I focused on international plan and voicemail plan), I got a p-value of 0 for the international plan! Is this possible? </p>

<p>I further performed a Cox regression on the data, again, just with those two variables, and ran coxzph, and it said that both p values were below 0.05. So they were basically useless? Should I throw them out when making predictions?</p>

<p>And how does one get to building a predictive model with something like this?</p>

<p>Thank you very much! Here's a copy of my code:</p>

<pre><code>library(survival)
dat &lt;- read.csv('/Users/priyanksmehta/Desktop/churn.csv')
dat$survival &lt;- Surv(dat$AccountLength, dat$Churn == 1)
plotting &lt;- survfit(survival ~ intlact, data = dat)
plotting2 &lt;- survfit(survival ~ vmailact, data = dat)
plot(plotting2, conf.int = FALSE, mark.time = FALSE, xlab = 'Subscribed since', ylab = 'Churn rate', col = c('blue', 'red'))
lnames &lt;- c('Voice Mail Activation', 'No Voice Mail')
legend('topright', lnames, col = c('blue', 'red'), lty = 1)
logrank &lt;- survdiff(formula = survival ~ intlact, data = dat)
logrank2 &lt;- survdiff(formula = survival ~ vmailact, data = dat)
print(logrank)
print(logrank2)
results &lt;- coxph(survival ~ intlact + vmailact, data = dat)
print(results)
coxsig &lt;- cox.zph(results)
print(coxsig)
</code></pre>
"
"0.0446990156267674","0.0441081091391231","175111","<p>I've understood that relative importance of predictors is a tricky question. Suggested methods range from very complex models to very simple variable transformations. I've understood that the brightest still debate which way to go on this matter. I'm looking for an easy but still appealing method to approach this in survival analysis (Cox regression).</p>

<p>My aim is to answer the question: which predictor is the most important one (in terms of predicting the outcome). The reason is simple: clinicians want to know which risk factor to adress first. I understand that ""important"" in clinical setting is not equal to ""important"" in the regression-world, but there is a link.</p>

<p>Should I compute the proportion of explainable log-likelihood that is explained by each variable (see Frank Harrell <a href=""http://stats.stackexchange.com/questions/155246/which-variable-relative-importance-method-to-use"">post</a>), by using:</p>

<pre><code>library(survival); library(rms)
data(lung)
S &lt;- Surv(lung$time, lung$status)
f &lt;- cph(S ~ rcs(age,4) + sex, x=TRUE, y=TRUE, data=lung)
plot(anova(f), what='proportion chisq')
</code></pre>

<p>As I understand it, its only possible to use the 'proportion chisq' for Cox models and this should suffice to convey some sense of each variables relative importance. Or should I perhaps use the default plot(anova()), which displays Wald Ï‡2 statistic minus its degrees of freedom for assessing the partial effect of each variable?</p>

<p>I would appreciate some guidance if anyone has any experience on this matter.</p>
"
"0.0446990156267674","0.0441081091391231","175267","<p>I'm familiar with conventional regression models and classical epidemiological methods. I recently encountered principal component analysis and made an effort to learn about it, but I could not find an example or answer that would answer my main question: can I use PCA to fine tune disease categories, based on a set of covariates and then analyse these categories in a traditional regression model?</p>

<p>I have a dataset with 4.000 individuals with hypertension (high blood pressure) and they are all diagnosed as ""hypertensive patients"" with no further subgroup classifcation; it's a bot unsophisticated since we know that this patient category is very heterogenous. For these persons I have about 20 continuous and categoricql covariates and I wanted to use PCA to:
1) let the data, given the distribution of these covariates, show which ""subtypes"" of hypertension there might be, and
2) classify each individual to each category he/she belongs and then perform a survival analysis (endpoint = death) to se whether the hazard differs in the identified groups.</p>

<p>Is that a good or very bad idea?</p>
"
"0.0645174717615637","0.0763974860547543","175585","<p>I am trying to help a co-worker out with analysis. Currently he has normalized the dependent variable in his regression formula by another variable - in this case he has normalized brain region volume by intracranial volume (this is a normal thing to do in imaging analysis, but I'm pretty sure it can apply to other realms of research outside of brain imaging). However, he has also included this normalizing variable (ICV here) as an independent variable in the regression formula. While I can find papers looking at whether regressing out ICV or normalizing by it are better for your particular question, I can not clearly find anything that explains why you can't do both. I am pretty sure you can't, but I am unable find a good explanation as to why. I would appreciate any help on this!</p>

<p>To be clear, the current formula can be approximated as:</p>

<p><code>lm(Region.Volume/ICV ~ ICV + Gender + Age)</code></p>
"
"0.122653086996056","0.121031652970736","175597","<p>I am using the <code>quantreg</code> package in R to develop quantile estimates at different taus, then using <code>anova</code> to test whether the Beta Estimates at different quantiles are equal ($H_0$) or not ($H_1$). Thus </p>

<pre><code>library(quantreg)
data(Mammals) # sample data in quantreg
</code></pre>

<p>for taus 0.1, 0.25, 0.5, 0.75 and 0.9</p>

<pre><code>fit1 &lt;- rq(weight ~ speed + hoppers + specials, tau = .1, data = Mammals)
fit2 &lt;- rq(weight ~ speed + hoppers + specials, tau = .25, data = Mammals)
fit3 &lt;- rq(weight ~ speed + hoppers + specials, tau = .5, data = Mammals)
fit4 &lt;- rq(weight ~ speed + hoppers + specials, tau = .75, data = Mammals)
fit5 &lt;- rq(weight ~ speed + hoppers + specials, tau = .9, data = Mammals)

anova(fit1, fit2, fit3, fit4, fit5, test=""Wald"", joint=FALSE)
</code></pre>

<p>Results in </p>

<pre><code>Quantile Regression Analysis of Deviance Table

Model: weight ~ speed + hoppers + specials
Tests of Equality of Distinct Slopes: tau in {  0.1 0.25 0.5 0.75 0.9  }

             Df Resid Df F value  Pr(&gt;F)  
speed         4      531  1.0952 0.35810  
hoppersTRUE   4      531  2.5898 0.03599 *
specialsTRUE  4      531  1.3774 0.24046  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However changing the order of the models to say;</p>

<pre><code>anova(fit3, fit1, fit2, fit4, fit5, test=""Wald"", joint=FALSE)
</code></pre>

<p>Produces the <strong>exact same result!</strong></p>

<h2>My question is basically, what gives?</h2>

<p><strong>(1)</strong> Is <code>anova</code> truly comparing all the models to one another (ie all estimates from different taus, ${_nC_r} = {_5C_2} = 10$ <strong>separate</strong> comparisons) </p>

<p><strong>OR</strong> </p>

<p><strong>(2)</strong> Is <code>anova</code> selecting the model with the lowest tau and comparing the remaining models to that?</p>

<p>I've extracted (and annotated) the relevant segments of the of <code>anova</code> function called in the <code>quantreg</code> environment bellow.</p>

<pre><code>getAnywhere(anova.rqlist)
sum.fit1 &lt;- summary(fit1, covariance=TRUE); sum.fit2 &lt;- summary(fit2, covariance=TRUE); 
sum.fit3 &lt;- summary(fit3, covariance=TRUE); sum.fit4 &lt;- summary(fit4, covariance=TRUE); 
sum.fit5 &lt;- summary(fit5, covariance=TRUE)
objects &lt;- list(); objects[[1]] &lt;- sum.fit1; objects[[2]] &lt;- sum.fit2 
objects[[3]] &lt;- sum.fit3; objects[[4]] &lt;- sum.fit4; objects[[5]] &lt;- sum.fit5
taus &lt;- c(0.1, 0.25, 0.5, 0.75, 0.9)
m &lt;- length(taus)
n &lt;- length(fit1$y)
    Omega &lt;- outer(taus, taus, pmin) - outer(taus, taus) ##!!!HERE!!!###
    J &lt;- objects[[1]]$J 
# From help file on summary.rq: J is Unscaled Outer product of gradient matrix returned if cov=TRUE and se != ""iid"". The Huber sandwich is cov = tau (1-tau) Hinv %*% J %*% Hinv. 
p &lt;- dim(J)[1]
H &lt;- array(unlist(lapply(objects, function(x) x$Hinv)), c(p, p, m))
# From help file on summary.rq: Hinv : inverse of the estimated Hessian matrix returned if cov=TRUE and se %in% c(""nid"",""ker"") , note that for se = ""boot"" there is no way to split the estimated covariance matrix into its sandwich constituent parts.    
H &lt;- matrix(aperm(H, c(1, 3, 2)), p * m, p) %*% t(chol(J))
W &lt;- (H %*% t(H)) * (kronecker(Omega, outer(rep(1, p), rep(1, p)))) ##!!!HERE!!!###
coef &lt;- unlist(lapply(objects, function(x) coef(x)[, 1]))
Tn &lt;- pvalue &lt;- rep(0, p - 1)
ndf &lt;- m - 1
ddf &lt;- n * m - (m - 1)
for (i in 2:p) {
  E &lt;- matrix(0, 1, p)
  E[1, i] &lt;- 1
  D &lt;- kronecker(diff(diag(m)), E)
  Tn[i - 1] &lt;- t(D %*% coef) %*% solve(D %*% W %*% 
                                         t(D), D %*% coef)/ndf
  pvalue[i - 1] &lt;- 1 - pf(Tn[i - 1], ndf, ddf)
}
pvalue
</code></pre>

<p>The reason i care is that if explanation <strong>(1)</strong> is being implemented then all the estimates are truly being compared, while if explanation <strong>(2)</strong> is being implemented, then technically the models are only being compared to minimum tau and <strong>NOT</strong> to one another. </p>

<p><strong>Note:</strong> The lines that define <code>Omega</code> and <code>W</code> suggest to me that the latter interpretation <strong>(2)</strong> is being implemented, but I'm not sure.</p>
"
"0.0565402794192176","0.0697410440814588","175726","<p>Say I have a data set, and I want to model the dependent variable <code>y</code>, as a function of <code>x1</code>,<code>x2</code>,<code>x3</code> and <code>x4</code>. I specify my model as:</p>

<pre><code>y ~ x1 + x2 + x3 + x4 + x1*x2
</code></pre>

<p>I specify an interaction between <code>x1</code> and <code>x2</code> based on prior knowledge. However, the r2 value of the single regression model <code>y ~ x1</code> is ~0.90. Is it fair to still test for this interaction, as well as effects of <code>x2</code> and <code>x3</code> on <code>y</code>? Or is it better to do an analysis of the residuals of the model <code>y~x1</code>? The problem I forsee with the analysis of the residuals is that it can only test for a main effect of <code>x2</code> on <code>y</code>, it cannot test for an interaction with <code>x1</code>. Any thoughts on how to best handle this type of problem would be greatly appreciated. </p>
"
"0.130520481086167","0.128795042662908","176111","<p>I need to conduct a meta-analysis for a publication, but this is my first meta-analysis and I still donâ€™t feel confident. I will describe the steps I have followed, and hopefully some of you might find errors on my methods, and suggest alternatives.</p>

<p>For this particular analysis, long-term studies should be more important than short-term because a few experiments showed transient effects, hence short-term studies might fail to capture that the effect is not really significant in the long-term. On my dataset, about half the studies have several non-independent measurements taken at different time-points (i.e. several annual measurements). Other experiments, despite having been carried out for several years, show the data already aggregated, with only one row per study with mean and standard deviation. I considered running a multivariate meta-analysis to solve this issue, but it would unbalance the analysis, giving more importance to the experiments with several rows of data (annual measurements) than to the experiments with aggregated data in only one row (pooled across several years). Am I right? This is an example of the dataset:</p>

<ul>
<li>Study 1, Year 1, Effect Size 1 </li>
<li>Study 1, Year 2, Effect Size 2 </li>
<li>Study 1, Year 3, Effect Size 3    </li>
<li>Study 2, Year 1, Effect Size 4    </li>
<li>Study 3, Years 1-4, Effect Size 5</li>
<li>Study 4, Years 1-3, Effect Size 6</li>
</ul>

<p>Alternatively, I decided to try and aggregate the data, so that finally there is only one row per study. I followed these steps:</p>

<p>Calculate effect sites for each row, including those studies with several rows (annual data). In this case, I calculated the log response ratio (ROM):</p>

<pre><code>dat &lt;- escalc (measure=""ROMâ€, n1i=elev.rep, n2i=control.rep, m1i=elev.ANPP.mean, m2i=control.ANPP.mean, sd1i=elev.SD, sd2i=control.SD, data=all)
</code></pre>

<p>Aggregate studies using the function agg {MAd}. I used the Borenstein et al. 2009 method, and correlation=1:</p>

<pre><code>datAgg &lt;- agg(id = id,es = yi,var = vi, cor =1,method = ""BHHR"", data = dat)
</code></pre>

<p>I have now only one row per study. However, since long-term experiments are more important, I have created user-defined weights that take into account the number of replicates and the number of years of each study:</p>

<pre><code>datAgg$weightsTime &lt;- with(datAgg, ((control.rep * elev.rep)/(control.rep + elev.rep)) + ((nyears^2)/(2*nyears)))
</code></pre>

<p>Run the mixed-effects meta-regression with two moderators, using Hedges Estimator (HE) and the Knapp and Hartung approach:</p>

<pre><code>m &lt;- rma.uni(yi, vi, mods= ~ factor(A) * factor(B), method=""HE"", data=datAgg, weights=weightsTime, knha=TRUE)
</code></pre>

<p>Am I doing something wrong? Can this method be improved? So far the results confirm my hypothesis, but of course I might be using a sub-optimal approach. Many thanks</p>
"
"0.126647210942508","0.124972975894182","176586","<p>This question stems from <a href=""http://stats.stackexchange.com/questions/175853/what-type-of-hypothesis-test-for-multivariate-testing-website"">another I asked last week</a>, where the person answering stated </p>

<blockquote>
  <p>""Finally, and this is very, very important: please don't just run the
  code I've provided, and consider your job complete. If you don't
  actually read up and understand some of how these analyses work, all
  of this information will be less than useless.""</p>
</blockquote>

<p>This is my intention, to really understand what is going on as well as how to interpret.</p>

<p>Context is website testing. Show people a different landing page, change the design and look of each page with a goal of getting more people to purchase online (""success"").</p>

<p>Here is my data:</p>

<pre><code>variant successes   failures
Original    757 49114
Date    553 41794
Cranberry   494 41495
Apple   546 41835
</code></pre>

<p>My script and output are below. I think I understand how to interpret it but just wanted to make sure. My questions:</p>

<ol>
<li>The first thing I want to do is check if there is a difference between the variance overall, or if it's just ebbs n flows. With a p-value of 8.55e-05 translates to 0.0000855 (right?) then yes, there is a meaningful variance between the groups. Is that a correct statement?</li>
<li>Since I'm comparing each group to the original (It's really a case of ""which test can beat the original), then it looks like only first Vs. 4th (Original Vs. Apple) is the only real difference statistically because the p-value is 0.0098. Is this a correct statement?</li>
<li>In my contrast function I have assumed data are read int he order they appear in test2. Is this correct?</li>
<li>Reading more about logistic regression it seems to be used to measure the impact of incrementing a predictor up or down a unit (resulting in the log unit increase or decrease). But in the context of measuring a web page variant performance in this way, why is logistic regression an appropriate method of determining whether or not the variants are different? Put another way, I'm hypothesis testing rather than predicting the impact of each variant, since an observation can only be one variant, not a combination of 1 or more predictors (they can only ever see one of the test pages, not 2 or more test pages).</li>
<li>I edited my data to include only visits from one state, just to experiment and play around. The output I got in this instance was a p-value of 0.001721 in the anova of m whereas the p-values for contrast where between 0.2 -0.3 (reject). If the script says overall there is a variance but at an individual test level there is not, how would I interpret that? I can provide the output if desired.</li>
</ol>

<p>Here is my script &amp; output:</p>

<pre><code>&gt; test2 &lt;- read.csv(""test2.csv"")
&gt; 
&gt; m &lt;- glm(cbind(successes, failures) ~ variant, family=binomial, data=test2)
&gt; anova(m, test='Chisq') # Tests if there's a difference between the variants
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(successes, failures)

Terms added sequentially (first to last)


        Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
NULL                        3     21.435             
variant  3   21.435         0      0.000 8.55e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; 
&gt; library(lsmeans)
&gt; #lsmeans(m, pairwise ~ variant) # Compares every variant to every other one
&gt; 
&gt; m.comparisons = lsmeans(m, specs = pairwise ~ variant)
&gt; contrast(m.comparisons,
+          list(
+            first.vs.second = c(1,-1,0,0),
+            first.vs.third =  c(1,0,-1,0),
+            first.vs.fourth = c(1,0,0,-1)
+            ), adjust=""tukey"")
 contrast           estimate         SE df    z.ratio p.value
 first.vs.second  0.09192309 0.06248035 NA  1.4712319  0.3667
 first.vs.third  -0.01371955 0.06072602 NA -0.2259254  0.9943
 first.vs.fourth -0.16633346 0.05653998 NA -2.9418735  0.0098

P value adjustment: sidak method for 3 tests 
</code></pre>
"
"0.0645174717615637","0.0763974860547543","177388","<p>I am a beginner and trying to learn new concepts in statistical analysis.</p>

<p>I have some very basic question. With a given data set of individuals I am trying to ascertain as to whether or not they are eligible to be granted for a loan - credit scoring.</p>

<p>My data set has 300 obs of 10 variables which I figured out using:
str(data)
summary(data):</p>

<p><a href=""http://i.stack.imgur.com/2q8Dl.png"" rel=""nofollow"">Detailed Data Desc</a></p>

<p>After applying linear regression on all variables:  </p>

<pre><code>linreg=lm(Rating~.,data=data)  
cor(linreg$fitted.values,data$Rating)
</code></pre>

<p><a href=""http://i.stack.imgur.com/DtbQk.png"" rel=""nofollow"">Linear Regression detailed</a></p>

<p><strong>I understand:</strong>  </p>

<ol>
<li>Having 3 stars - p value means very significant and high and positive Estimates indicats that it has a positive significance and vice versa  </li>
<li>The correlation between fitted values and Rating comes to  0.9867324</li>
</ol>

<p><strong>Questions:</strong>  </p>

<ol>
<li>Does this mean regression predicts correctly 98.67% of the observations  </li>
<li>If everything else are equal then variables like Education and Gender have a positive impact on the rating? Because they have lowest p-values.  </li>
<li>What about Student and Income variables?  </li>
<li>Also does it mean individuals with high Income will have a greater rating, everything else being equal?  </li>
</ol>
"
"0.0774209661138764","0.0763974860547543","177987","<p>Good afternoon,
I need help in intpretting the significance results when performing a linear regression with a categorical interaction.  I'm running this analysis in R.</p>

<p>The question is whether I can use qsec as an explantory variable in the below model given that one of the categorical variables is not significantly different to the other.</p>

<p>Model &amp; Outputs</p>

<pre><code>lm(mpg~qsec+am+qsec*am, data=mtcars)

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  -9.0099     8.2179  -1.096  0.28226   
qsec          1.4385     0.4500   3.197  0.00343 **
am1         -14.5107    12.4812  -1.163  0.25481   
qsec:am1      1.3214     0.7017   1.883  0.07012 . 
</code></pre>

<p>The second coefficient is not statistical significant - though only just with a p value of 0.07.
Does this mean that I should look for another explanatory variable?</p>

<p>When I run the regression on one of the values - am1 - alone, then it is still significantly different to zero.  As is am0 from the above results.</p>

<p>When I look at the data, the slope appears significantly different so I think I can use it however the understanding of the theory says no - which is frustrating.  I would add my plot to highlight my point but I'm struggling to load it.</p>

<p>I hope this isn't a stupid question to ask...I've found some good explanations of categorical variables in general, however nothing that tells me how I should interpret this secondary variable.</p>

<p>James</p>

<p>** edit changed data to be qsec per question below</p>
"
"0.192257735769037","0.184588687327598","179105","<p>I get a couple of puzzling results in my (repeated event) cox model when I introduce interaction effects. I will here pose several questions about interaction effects (in survival analysis context) in order to â€“ hopefullyâ€“ once for all to get the answers to these questions. I've checked similar posts, related to this matter (<a href=""http://stats.stackexchange.com/questions/147310/r-dichotomous-time-interaction-in-a-cox-model"">1</a>, <a href=""http://stats.stackexchange.com/questions/161849/interpretation-interaction-in-cox-regression"">2</a>, <a href=""http://stats.stackexchange.com/questions/175525/cox-ph-interaction-model-test-p-value-equivalence"">3</a>, <a href=""http://stats.stackexchange.com/questions/32225/cox-proportional-hazard-model-and-interpretation-of-coefficients-when-higher-cas"">4</a>, <a href=""http://stats.stackexchange.com/questions/137180/interpretation-of-interaction-between-covariates-and-time-in-cox-regression"">5</a>, <a href=""http://stats.stackexchange.com/questions/157275/cox-regression-testing-for-effect-in-subgroup"">6</a>, <a href=""http://stats.stackexchange.com/questions/46322/understanding-signficant-interaction-with-non-significant-main-effects"">7</a>, <a href=""http://stats.stackexchange.com/questions/163299/cox-time-series-data-analysis-of-interaction-terms"">8</a> ), and some of them are unanswered, while the others are answered ambiguously. Some of them are helpful. In general,  I belive there is a need (and interest in) for some clarification about interaction effects â€“ a quite complicated area for all quantitative methodsâ€“focused student/professionals. </p>

<p>Ultimately, my questions relate to the logic behind interactions and their subsequent interpretation in the analysis. Below I present 5 different scenarios/models derived from my data analysis â€“ but I extend them a bit to also include other examples that might be of help for me and (hopefully) for other people on this website.  </p>

<p>For every scenario, I provide my own interpretations (in order to capture the essence and logic, they're not comprehensive interpretations) â€“ so those of you who are able to answer, please reject or support them. If possible, provide a correct answer and elaborate why something was incorrect. </p>

<ul>
<li><strong>Scenario 1</strong></li>
</ul>

<p>Suppose that I have a model with 2 covariates where one of the covariates is my main explanatory variable (note that it makes sense to have this variable without an interaction term as well). Guided by my theoretical considerations, I (also) introduce an interaction term between them. </p>

<p>My main explanatory variable <strong>(X)</strong> is on the scale 0 to 10 (think of number of appearances) and the other covariate <strong>(D)</strong> is also a continuous variable (ranging from 0 to 10). The model with interaction term:  </p>

<pre><code>model.1&lt;â€“coxph(start, stop, event)~X+D+X:D+cluster(ID)+strata(enum), data=mydata)  

                    exp(coef) exp(-coef) lower .95 upper .95
X                    1.069     0.9356    0.9798     1.166
D                    1.046***  0.9561    1.0213     1.071
X*D                  1.000     0.9999    0.9876     1.013
</code></pre>

<p>Suppose now that in model with only X+D (with no interaction term), my main variable X was significant. It is not significant in the interaction model (see above result). </p>

<p><strong>My interpretation</strong> 1) I simply state that there were no interaction effects between X and D. However, while the  D variable is significant (with increasing hazard rate) the X is not. Thus, my main explanatory variable is not sufficient to explain this. Alternatively, 2) I state that there were no interaction effects, and the coef. of X in the interaction model does not make any sense or is hard to interpret. I don't even show this results, but put it on a note. </p>

<p><strong>Question:</strong> how should I interpret interaction effects between two continuous variables in this model?  </p>

<ul>
<li><strong>Scenario 2</strong></li>
</ul>

<p>In this scenario the X variable is still a continuous variable 0-10, but the D-variable is now dichotomous. </p>

<pre><code>                    exp(coef) exp(-coef) lower .95 upper .95
X                   1.0677.    0.9366    0.9933     1.148
D                   1.3628***  0.7338    1.1351     1.636
X*D                 0.9994     1.0006    0.9150     1.092
</code></pre>

<p><strong>My interpretation</strong>: ""X:D"" is decreasing, i.e. when D=0 and X increasing, the hazard for experiencing the event is decreasing(weak), but the effect is not significant. When ""D"" is = 1, the hazard is increasing. </p>

<ul>
<li><strong>Scenario 3</strong></li>
</ul>

<p>""X"" is till continuous, but the ""D"" is now categorical (0 = no appearances, 1 = one appearance, 2 = two appearances, 3 = three appearances).  </p>

<pre><code>                     exp(coef) exp(-coef) lower .95 upper .95
X                     1.0491***  0.9532    1.0226     1.076
factor(D)1            1.2237     0.8172    0.8350     1.793
factor(D)2            1.7871.    0.5596    0.9910     3.223
factor(D)3            1.0578     0.9453    0.4625     2.420
X*factor(D)1          0.9849     1.0153    0.9336     1.039
X*factor(D)2          0.9859     1.0143    0.9021     1.077
X*factor(D)3          1.0390     0.9625    0.9230     1.170   
</code></pre>

<p><strong>Question</strong>: How should I interpret the interaction term here? </p>

<ul>
<li><strong>Scenario 4</strong></li>
</ul>

<p>Now the ""X"" becomes dichotomous (1/0) and the ""D"" remains categorical as in Scenario 3. </p>

<pre><code>                   exp(coef) exp(-coef) lower .95 upper .95
X                    1.386**   0.7214    1.1315     1.698
factor(D)1           1.195     0.8370    0.8435     1.692
factor(D)2           1.659.    0.6029    0.9635     2.855
factor(D)3           1.061     0.9425    0.4820     2.336
X*factor(D)1         0.900     1.1111    0.5848     1.385
X*factor(D)2         0.986     1.0142    0.4979     1.952
X*factor(D)3         1.352     0.7394    0.5097     3.589
</code></pre>

<p><strong>My interpretation</strong>: The interaction term is not significant, as in all Scenarios. But the interpretation would be that when X is = 1, the D = 1 and D = 2 are decreasing (compared to D=0) but when X=1 and D=3, the hazard is increasing. </p>

<ul>
<li><strong>Scenario 5</strong></li>
</ul>

<p>Suppose now that the ""X"" and the ""D"" variables are exactly the same as in the previous scenario. However, this time, variable ""X"" violates the PH assumption. So I am introducing an interaction term between X and stop/start time (years). I know that some would argue that one needs to split the data before doing this, while others would not necessary recommend this. This is somehow a side-debate here. Interesting, but not really relevant here for our example. It's also been discussed elsewhere here. Nevertheless, here is the model: </p>

<pre><code>            exp(coef) exp(-coef) lower .95 upper .95
X             1.5848*    0.6310    1.0795    2.3268
factor(D)1    1.1301     0.8849    0.9192    1.3893
factor(D)2    1.6507**   0.6058    1.1655    2.3378
factor(D)3    1.2698     0.7875    0.7991    2.0179
X*stop        0.9488*    1.0540    0.9026    0.9973
</code></pre>

<p><strong>My interpretation</strong>: The interaction with time does correct for the violation of the assumption: X is decreasing with years. However, X alone is increasing. What is going on here? It doesn't make any sense to me. Unless, the X = 0 (alone), and X = 1 with * stop in the model. If so, the interpretation is then that X = 1 * stop is decreasing over time, while when X = 0, the hazard rate increases with 1.58. </p>

<p><strong>EDIT (additional information):</strong>  </p>

<p>The variables ""X""  and ""D"" are actually discrete (1, 2, 3, 4,..10) but they are treated as continuous. </p>

<p>I use conditional model ( or ""PWP""-model), and the time scale is ""time since entry"". </p>

<p>Both X and D are time-dependent (or time-varying) variables. </p>
"
"0.0774209661138764","0.0763974860547543","180191","<p>We can apply the Hosmer-Lemeshow goodness of fit to logistic regression modelling and to test if an underlying assumption is not applicable.</p>

<p>This <a href=""https://www.youtube.com/watch?v=MYW8gA1EQCQ"" rel=""nofollow"">link</a> shows a video of the application to a standard <code>glm()</code> model</p>

<p>This <a href=""http://stats.stackexchange.com/questions/132652/how-to-determine-which-distribution-fits-my-data-best-r"">detailed question</a>, outlines various simulation-based tests one can run to assess underlying distributions.</p>

<p><strong>But I want to apply the Hosmer-Lemeshow goodness of fit to survival analysis with assumed underlying data distributions</strong>.</p>

<p>Much literature points one towards a cox proportional hazards model, but from what I understand, a cox ph model does not assume an underlying distribution of data.
Therefore lets take some random data from the <code>survreg()</code> function of the <code>survival</code> package</p>

<pre><code>library(survival)

data(ovarian)

head(ovarian)

s &lt;- Surv(ovarian$futime, ovarian$fustat)
sWei &lt;- survreg(s ~ age,dist='weibull',data=ovarian)
</code></pre>

<p>How can we applying a H+L G.O.F statistic test?
I had hoped to follow this <a href=""http://thestatsgeek.com/2014/02/16/the-hosmer-lemeshow-goodness-of-fit-test-for-logistic-regression/"" rel=""nofollow"">link</a>, however the <code>survreg()</code> does not allow a <code>fitted()</code> function. Thus this does not work</p>

<pre><code>library(ResourceSelection)
hl &lt;- hoslem.test(sWei$y, fitted(sWei), g=10))
</code></pre>
"
"0.100365631932749","0.108042360909843","181333","<p>I am analyzing count data with a lot of zeros and found that although the data do not fit a poisson glm, they fit the zero-inflated poisson (ZIP) regression significantly better than the standard poisson glm. </p>

<p>This analysis is for a BACI study, in which I have data before, during, and after the treatment, in three zones: control, on-trail (treatment1) and off-trail (treatment2). </p>

<p>I am interested in the difference in change of detection rate of a species between the on-trail (treatment) site vs. control site, before and after the treatment. I have performed contrast on this data to determine this difference with a simple linear regression model (using lm), but I'm unsure how to find this difference using the zero-inflated poisson model. </p>

<p>The results of my ZIP model are here (ZP = Zone+Phase combined into one variable; the ""After"" phase is called ""Open"" in the dataset)</p>

<pre><code>Call:
zeroinfl(formula = Deer ~ ZP | ZP, data = zinb)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.6756 -0.5180 -0.4137 -0.1243 14.8998 

Count model coefficients (poisson with log link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.076730   0.031034  34.695  &lt; 2e-16 ***
ZPCDuring    0.080611   0.055391   1.455    0.146    
ZPCOpen     -0.696793   0.092432  -7.538 4.76e-14 ***
ZPFBefore    0.062467   0.042638   1.465    0.143    
ZPFDuring    0.112727   0.067928   1.659    0.097 .  
ZPFOpen     -0.765391   0.080475  -9.511  &lt; 2e-16 ***
ZPTBefore   -0.008428   0.045729  -0.184    0.854    
ZPTDuring   -0.063361   0.063193  -1.003    0.316    
ZPTOpen     -0.717266   0.078422  -9.146  &lt; 2e-16 ***

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.40428    0.06617   6.110 9.97e-10 ***
ZPCDuring    0.85610    0.11076   7.729 1.08e-14 ***
ZPCOpen      0.79893    0.13448   5.941 2.84e-09 ***
ZPFBefore   -0.04894    0.09287  -0.527    0.598    
ZPFDuring    1.51339    0.13035  11.610  &lt; 2e-16 ***
ZPFOpen      0.20254    0.12932   1.566    0.117    
ZPTBefore   -0.08598    0.09830  -0.875    0.382    
ZPTDuring    0.98729    0.11720   8.424  &lt; 2e-16 ***
ZPTOpen      0.17416    0.12823   1.358    0.174    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 25 
Log-likelihood: -8572 on 18 Df
</code></pre>

<p>and the code i have been using to get the contrasts using the lm model is here (based on <a href=""http://people.stat.sfu.ca/~cschwarz/Stat-650/Notes/PDFbigbook-R/R-part013.pdf"" rel=""nofollow"">this tutorial</a>): </p>

<pre><code>result.lm &lt;- lm(Deer ~ Zone + PhaseBDO + Zone:PhaseBDO, data=counts)
options(contrasts=c(unordered=""contr.sum"", ordered=""contr.poly"")) 
result.lsmo.SP &lt;- lsmeans::lsmeans(result.lm, ~Zone:PhaseBDO)

contrast(result.lsmo.SP, list(bact=c(1, 0, -1, 0, 0, 0, -1, 0, 1)))
confint(contrast(result.lsmo.SP, list(bact=c(1, 0, -1, 0, 0, 0, -1, 0, 1))))
</code></pre>

<p>Can anyone suggest how I can calculate the contrast from my ZIP regression model to test for significant differences between the difference in detection rates between the two time periods (before/after) in the two zones (control/treatment)? </p>

<p>For example, I want to be able to say if there was a significantly larger increase (or decrease) in the detection rate of deer after the treatment was implemented, in the treatment zone compared with the control zone. </p>

<p>Thanks in advance for your suggestions.</p>
"
"0.0547448901451359","0.0540211804549215","182068","<p>I have data where I'm interested in the effect of treatment on individual decisions: Options 1, 2, &amp; 3. Individuals made multiple decisions (level 1) in groups (level 2).</p>

<p>I want to know the effect of treatment on selecting a particular option over not selecting that option. To do this, I dichotomized each option into 3 variables: Option 1 or not Option 1, Option 2 or not Option 2, Option 3 or not Option 3.</p>

<p>I tried to run a binomial regression using glmer: e.g., <code>glmer(Option1 ~ Predictors + (1|Level_1) + (1|Level_2), family=binomial)</code>, which did not converge.</p>

<p>I ran the same analysis using nnet::multinom: e.g., <code>multinom(Option1 ~ Predictors, random=~ 1| Level_1/Level_2, family=binomial)</code>. This did converge and gave sensible results.</p>

<p>What's going on? Is it justifiable to use multinom instead of glmer on the dichotomized data?</p>

<p>Any insight would be appreciated!</p>
"
"0.0547448901451359","0.0540211804549215","182071","<p>I'm analyzing presence/absence data using Rmark occupancy analysis (i.e. Rmark is the package which runs Program MARK in R). This is for a BACI (before-after-control-impact) design and I want to answer the question of ""is the amount of change between before and after phases significantly different for the impact site as compared with the control site?"".  I'm able to do this using regression for the count data, but am not sure how to calculate lsmeans to use in the BACI contrast using the occupancy results. </p>

<p>Here is an example: </p>

<p>Analysis of the count data using zero-inflated poisson regression: 
(ZP = zone/phase combination: zones are control/impact, phases are before/after). Note: extra script from this answer was needed to run lsmeans on the zerofinl object (<a href=""http://stats.stackexchange.com/questions/181333/zero-inflated-poisson-regression-how-can-i-calculate-contrasts-for-baci-before/181502?noredirect=1#comment345516_181502"">see this answer for more details</a>).</p>

<pre><code>summary(m2 &lt;- zeroinfl(Opossum ~ ZP|ZP, data = bact))
result.lsmo.SP2 &lt;- lsmeans::lsmeans(m2, ~ZP)
contrast(result.lsmo.SP2, list(bact=c(1, -1, -1, 1)))
confint(contrast(result.lsmo.SP2, list(bact=c(1, -1, -1, 1))))
</code></pre>

<p>Analysis of the presence/absence data using occupancy analysis in Rmark: </p>

<pre><code>BEAR.models=mark(BEAR, model=""Occupancy"", group=c(""Zone"", ""Phase""), model.parameters=list(p=list(formula=~WeekDay+DetectionDist+Trail),Psi=list(formula=~Zone+Phase)), invisible=FALSE)
</code></pre>

<p>Any ideas of how I can calculate lsmeans from the occupancy results (which includes occupancy estimate (psi), standard error, and upper/lower 95% confidence interval limits)? Thanks!</p>
"
"0.0446990156267674","0.0441081091391231","182079","<p>I am running a meta-analysis using <strong>metafor</strong> R package. I am comparing studies on a continuous variable, that can be synthetized by the mean. Such studies can be grouped in three blocks. I used a meta regression, e.g. </p>

<pre><code>dat &lt;- escalc(measure=""MN"", mi=mean, sdi=sd, ni=num, data=dbtemp)
res &lt;- rma(yi, vi, mods =~ factor(group), data=dat)
</code></pre>

<p>I know that I can know whether group is significant or not. But how can I assess whether each level within group is significant from the other. I.e., how I can perform multiple comparisons (post hoc analysis) within a meta - analysis context?</p>
"
"0.078223277346843","0.0882162182782462","182286","<p>I am doing a regression analysis for an ordinal response variable with 5 explanatory variables. I will be using the <code>polr()</code> or <code>lrm()</code> functions to do the ordinal logistic regression. For my non-ordinal response variables (e.g., count and binary data), I have been using glmulti for model selection, but this doesn't seem to be compatible with the <code>polr()</code> and <code>lrm()</code> R functions. I've also tried <code>stepAIC()</code>, <code>step()</code> and <code>leap()</code> functions without any luck. The summary of the <code>polr()</code> regression shows an AIC score.</p>

<pre><code>&gt; model1 &lt;- polr(x ~ Age + Gender + StudentType + StudentYear + RacialGroup,
+ data = question8a, Hess =TRUE)
&gt; summary(model1)
Call:
polr(formula = x ~ Age + Gender + StudentType + 
    StudentYear + RacialGroup, data = question8a, Hess = TRUE)

Coefficients:
                                   Value Std. Error  t value
Age                             -0.16691    0.04925 -3.38872
GenderWoman                      0.05514    0.24655  0.22366
StudentTypeUndergraduatestudent -1.36414    0.50748 -2.68807
StudentYear2ndyear              -0.02042    0.29600 -0.06899
StudentYear3rdyear              -0.05997    0.38253 -0.15676
StudentYear4+years               0.89921    0.66430  1.35363
StudentYear4thyear               0.25324    0.42433  0.59680
RacialGroupNon-Indigenous       -2.13460    0.42163 -5.06268

Intercepts:
    Value   Std. Error t value
1|2 -9.9335  1.5283    -6.4999
2|3 -8.3051  1.4752    -5.6298
3|4 -7.2498  1.4567    -4.9770
4|5 -4.8720  1.4240    -3.4214

Residual Deviance: 657.086 
AIC: 681.086 
</code></pre>

<p>I tried to follow this suggestion: <a href=""http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R"" rel=""nofollow"">http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R</a>, but wasn't able to get it to work. </p>

<p>Has anyone been able to get this to work? Or do I need to compare the 2^5 = 32 model AIC scores by hand? </p>
"
"0.0774209661138764","0.0763974860547543","182656","<p>I have data in longitudinal or clustered format (please see the example below). My response variable is dichotomous. I want to examine which factors explains why a subject in the dataset gets Y=1. In the example below I show only one predictor â€“ X.</p>

<p>Since I have a dichotomous response variable, I am thinking of logistic regression. However, the longitudinal format violates the distributional assumption of ML-theory. <strong>So my question is which logistic model would be appropriate here?</strong> And if possible, which R-package would be relevant (if not covered by the standard stats)? </p>

<p>All subjects are countries and observed (let's say) from 1990-1994. A country can get more than 1 Y per year, from different Z's.  I have been thinking of logistic panel models. Although I am not sure which specific model would be appropriate (assuming that panel models are more appropriate group of models). Perhaps random effects as each observation is not of the same nature (a country can get Y=1 from different groups, see variable Z). The Z variable is not a part of analysis though. Grateful for all suggestions!</p>

<pre><code>COUNTRY      YEAR  Y           X        Z

    A        1990  0           0        K
    A        1991  1           0        K
    A        1992  0           0        K
    A        1993  1           0        L
    B        1994  0           1        L
    B        1990  0           1        L
    B        1991  0           1        L
    B        1992  1           1        L
    C        1990  1           0        K
    C        1991  1           0        K
    C        1992  0           0        L
    C        1993  0           1        K
    C        1994  0           1        L
    D        1990  0           1        L
    D        1991  0           0        K
    D        1992  0           0        K
    D        1993  0           1        K
    D        1994  0           1        K
</code></pre>
"
"0.0632139541241014","0.0623782861551805","182689","<p>My name is Ashley. I'm working on the analyses for my dissertation which involves a meta-analysis of 4 predictors, 1 mediator, and one outcome. So far, I've calculated the meta-analytic correlation matrix between predictors and outcome(s) and harmonic mean of N to run analyses on the model level. 
Out of two major studies published in my area of the social sciences (DeChurch &amp; Mesmer-Magnus, 2010; Joseph et al., 2015), this is all of the information provided for running meta-regression with more than one predictor. No programs are specified and no other matrices are indicated. </p>

<p>I'm having trouble identifying how to run the multivariate metaregression analysis. From what I've found so far, mvmeta package in R is the closest that I've come to identifying a program/package that will produce the estimates that I need. However, I would have to calculate corrected rho for each study/predictor-outcome relationship individually. Also, there is no place to indicate harmonic mean of N. And because of this, I'm skeptical in using this package. </p>

<p>Is anyone aware of another R package, SPSS macro or other statistical software program/package that can handle this type of analysis? Or is mvmeta the best bet?</p>
"
"0.109815159255115","0.108363438470314","182947","<p>In the context of a (multilevel) meta-analysis, I want to caluclate grand effect size estimates for subsets of data corresponding to the levels of a categorical variable.
Using the example below, I'd like to calculate the grand effect size for Gifted students and NotGifted students. I thought of approaching this two ways: run two meta-analyses (one with Gifted students, the other with NotGifted students), or include Gifted status as a moderator in an analysis inclusive of all students (i.e. a meta-regression (though I knwo this term is frowned upon)).
To my confusion, the two approaches produce different values for effect sizes for Gifted and NotGifted students. I have two questions. </p>

<p>Why are the results of the two methods different? Which approach is better, i.e. which is a better estimator of the true effect size for Gifted or NotGifted students?</p>

<p>Any help would be greatly appreciated. Thank you</p>

<pre><code>library(metafor)

set.seed(123)
df &lt;- data.frame(
School = rep(1:5, each = 4),
Class = rep(1:2, times = 10),
Pupil = rep(letters[1:4], times = 5),
Gifted = sample(c('Yes', 'No'), size = 20, replace = TRUE),
yi = rnorm(20, 70, 15),
V = rnorm(20, mean = 5, 2))

Gifted &lt;- rma.mv(yi, V, random = list(~ 1 | Class, ~ 1 | School), 
             method = 'REML', data = subset(df, Gifted == 'Yes'))
NotGifted &lt;- rma.mv(yi, V, random = list(~ 1 | Class, ~ 1 | School), 
                method = 'REML', data = subset(df, Gifted == 'No'))
Regression &lt;- rma.mv(yi ~ Gifted, V, random = list(~ 1 | Class, ~ 1 | School), 
                 method = 'REML', data = df)

matrix(round(c(NotGifted$b, Gifted$b, Regression$b[1], Regression$b[1] + Regression$b[2]), 3), 
   2, 2, TRUE, list(' ' = c('Subset', 'Regression'), ' ' = c('NotGifted', 'Gifted')))
</code></pre>
"
"0","0.0311891430775903","183245","<p>I'm new to regression and I am trying to perform regression analysis on two time series <code>Price A</code> (x-variable) and <code>Price B</code> (y-variable). When doing LASSO regression, the R-squared score is extremely low at 0.01. Log transformations made it worse.</p>

<p>Plotting out the scatter plot, we can see 2 different clusters. How should we handle such a data set? And how do we interpret such a scatter plot?</p>

<pre><code>plt.scatter(df['priceA'], df['priceB'])
plt.xlabel('Price A')
plt.ylabel('Price B')
</code></pre>

<p><a href=""http://i.stack.imgur.com/MZM1Z.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MZM1Z.png"" alt=""enter image description here""></a></p>
"
"0.0446990156267674","0.0441081091391231","183451","<p>I have yearly rainfall projections from five climate models for the period of 2010 through 2099.</p>

<p>This is a sample of my data:</p>

<pre><code>df=structure(list(Year = c(2010, 2011, 2012, 2013, 2014, 2015, 2016, 
2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 
2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 
2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 
2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 
2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 
2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 
2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 
2094, 2095, 2096, 2097, 2098, 2099), CanESM2 = c(1163.46560706632, 
1045.27764563553, 1192.99035592859, 1039.18159594737, 1069.85056057463, 
1109.61718257189, 1080.22225446686, 996.465673784495, 1330.31482267773, 
1135.09951956191, 1036.5620174695, 1171.19645849667, 1230.07980354375, 
1224.51936031341, 1059.0667847652, 1119.61709915399, 1093.01684435802, 
1123.72623649933, 1088.75970830321, 1096.55713940808, 1136.73460669118, 
1225.56589926383, 1230.47636335948, 971.878201373981, 1077.94938659653, 
1067.66509425384, 1278.93601510725, 1132.26048108291, 1172.28768446317, 
1152.31800688181, 1226.66419877102, 1284.17386265492, 1065.46324990181, 
1126.16088253523, 1199.68965697911, 1048.76918572934, 1067.22448843151, 
1233.49853962937, 1125.31148343304, 1110.58047100806, 1096.19567721952, 
1277.141766876, 1024.98721582214, 1065.74405206603, 1067.80045192583, 
1115.30598493354, 1189.9948818424, 1437.13931627192, 1157.00834795935, 
976.858312813719, 1116.52499339511, 1111.09759483922, 1199.08438212517, 
1071.78135303469, 1007.25547265899, 1071.39018135202, 1089.81301384961, 
1312.96135982164, 1119.14850815955, 1128.62351901672, 1085.83469993455, 
1490.14291868226, 1082.82332451136, 1094.93028550704, 1138.81957874767, 
1097.4652709511, 1106.83810592802, 1229.91613882742, 1133.7894640904, 
1138.07334287779, 1281.84520786833, 1044.82250530464, 1187.35200775616, 
1156.74028624375, 1128.97358661251, 1168.80857077186, 1029.65489771463, 
1024.77267482949, 1085.6743046951, 1400.35428529776, 1165.82818082067, 
1117.43356446918, 1158.87599998443, 1188.6860869167, 985.854151901199, 
1147.12318390124, 1077.81579069868, 1154.53800683884, 1129.23840124991, 
1230.68311935564), `GFDL-ESM2M` = c(1110.40656291483, 1165.52810385097, 
986.538514416152, 1095.38727970136, 908.291964523058, 1015.48130275974, 
910.223751648018, 1037.17545203031, 1064.46407778575, 821.925055711163, 
1096.30232777925, 1145.56041166767, 994.615426183943, 1017.5660122588, 
1019.44059497268, 1074.6107636677, 1199.64822769138, 965.649952638057, 
947.58734190787, 946.097731390942, 1001.45702762066, 1115.81807500611, 
1037.58905387885, 917.925884051693, 974.408569840561, 1065.98187555499, 
988.032759990321, 1156.4922081412, 1053.23214676772, 868.078070630601, 
1117.82655721168, 1129.78430893212, 1078.10041660786, 792.460976651464, 
913.253156177221, 919.321633035608, 1040.56072059552, 1090.87082960431, 
1071.27625924979, 1033.46091240528, 883.895086023128, 939.489773000334, 
1102.2485093512, 779.851230298964, 1100.51135836672, 966.685826958901, 
961.744760284696, 989.753864405085, 826.629851232945, 909.284738712619, 
873.476834043498, 940.715939174158, 1005.83652099677, 1146.69688740834, 
978.082249009606, 1075.71089568088, 1101.2751541009, 962.851445193924, 
1024.69987402044, 1114.93474064477, 1024.47604232346, 1056.49825285734, 
975.492763800606, 1101.02607222684, 920.749421732859, 903.596908739447, 
912.489759611543, 987.64092666866, 808.10762021923, 973.408506903507, 
969.489829593018, 968.373783576046, 993.01049783438, 1080.98673871791, 
869.923091297771, 934.285784415424, 956.3081757549, 1013.38311159891, 
856.448744053944, 874.516625599295, 1061.51117387629, 1128.54448558007, 
1009.21257737855, 1017.16456465614, 833.802010570892, 927.740880449226, 
1001.38592283682, 952.145679780795, 996.893937239244, 931.957115223409
), inmcm4 = c(997.516654764994, 1145.13308691203, 959.542879520275, 
903.786765066992, 937.432661124838, 1007.46247945661, 1013.04583737278, 
1061.26827856464, 1056.61600975265, 892.433893874083, 1061.97862849247, 
978.554696188197, 939.55857188082, 996.416292720483, 1079.06762584418, 
1068.98156370385, 822.342905469512, 1019.26614712021, 1116.36245976214, 
1008.22817419423, 902.855504214986, 1087.38893040866, 885.956566984022, 
1016.16044809847, 1230.18564397572, 870.800309733513, 1037.21587559441, 
861.236791929213, 948.771432189766, 871.382698104435, 1089.3915218932, 
1054.49129365067, 941.903074357385, 973.558427724714, 938.638805666033, 
1006.66304703962, 863.28436030157, 806.021669160746, 869.369012721652, 
934.149986789314, 974.449974533922, 953.915114968656, 998.865234642777, 
949.027492106268, 967.701742440291, 952.306409621697, 959.027780253679, 
1033.69516116182, 992.987130941704, 1132.85542518598, 857.240436050392, 
1125.7240784834, 905.912164925948, 791.911242057151, 950.50688942409, 
684.62210670821, 928.505401464914, 898.455471168554, 881.937660493059, 
1043.17046368419, 990.173635925243, 872.642891720416, 968.112830276678, 
1156.70704372798, 954.681404468287, 1208.90589934053, 717.191066272881, 
985.50086154963, 1019.84388106248, 836.40573448035, 923.512492484388, 
942.453644704655, 1297.40333367214, 993.491117038875, 1035.26544859565, 
1056.57888641097, 968.592730428887, 887.591583109264, 1109.2935814163, 
923.189139410888, 972.33707020049, 1071.81484320842, 921.974872423456, 
956.677679924482, 886.258785523372, 903.500244794601, 1006.38055244068, 
914.283438551077, 779.704380831126, 1121.49522817412), 
`MRI-CGCM3` = c(848.301135629757, 
1065.98505740902, 888.813467571221, 1070.05226905271, 929.86581326247, 
949.338629810498, 1055.34828003874, 950.605580125944, 1025.98651471033, 
1062.58098758771, 1129.04220061327, 1113.69640112993, 983.264122816401, 
909.350171139101, 1049.76982306699, 991.477943714084, 1050.37713605355, 
957.954418661432, 1198.88425343781, 1019.03177011263, 892.820372968912, 
1117.83057226823, 1172.12350726162, 1045.54420748303, 1021.44374290702, 
1226.69766824788, 1076.26278716493, 892.835182446986, 1081.36552627508, 
1119.61825607435, 1101.7977981361, 1166.29589473449, 1114.24627378901, 
1111.95127710174, 1188.29593429323, 1005.43226899398, 1062.11294060983, 
1177.29197380333, 1233.89949211259, 949.362448506262, 1080.82477119637, 
848.404135782853, 1173.85210197539, 983.618625752799, 1023.98178052282, 
1174.67507789645, 1061.16686611069, 1145.73728230606, 1109.37233672472, 
1131.39932373277, 1192.52567101682, 1035.72075610234, 1170.69030772479, 
1402.30470791808, 828.360972784472, 1119.41148932651, 1418.65415788168, 
1111.1623167992, 1031.72445008109, 1133.74439397242, 1256.44423788796, 
1205.53255936418, 1246.028794886, 1353.06641467104, 1203.75084317503, 
1260.7607967303, 1133.17128359068, 1128.75225848513, 1248.50107501562, 
1125.34125579299, 1107.47674713103, 1286.39203071485, 1134.21641721783, 
1368.91059626765, 1249.62807137382, 900.188446846731, 1215.72863596362, 
1217.36216136844, 1239.46334238843, 1225.6431042983, 1293.28424474932, 
1316.91205393836, 1344.5824880094, 1431.35684547467, 1228.35587386956, 
1063.69382941901, 1366.01835107431, 1178.54993691379, 1425.59197731446, 
1439.18433794687), `NorESM1-M` = c(1104.36330130108, 1143.59864836685, 
1156.96992286211, 1225.47647032022, 1051.47222270552, 1225.96709059087, 
1169.02846946487, 1008.33133254077, 1041.4377020821, 1094.28488840048, 
1055.7942901173, 1182.85110699334, 1074.64700888046, 1220.40875347693, 
974.413995298429, 1082.02850933558, 1149.38849910412, 1177.55048525853, 
1159.36173358997, 1159.47946095381, 1013.84251995501, 1039.38081678502, 
1175.99482174589, 1075.9123379481, 1124.78923639009, 1072.4595220739, 
1059.99103769952, 1147.12456096131, 1054.66087104772, 1176.13768054148, 
1051.63089069775, 1124.1299767682, 958.783755377516, 1117.04595412512, 
1187.68311854194, 922.314885386565, 1118.97706673769, 1089.94177070272, 
1104.64546357211, 1129.91397601017, 1076.23273471979, 1117.36435679852, 
950.901478997885, 1171.61731400339, 1161.05025755828, 1122.1168542936, 
1132.68254406784, 1116.39483899695, 1052.38202713855, 1136.25445489531, 
1212.37181415053, 1113.58170133876, 988.657267351285, 1077.72113312282, 
1132.82563231238, 1060.24278563685, 1054.61879253374, 1138.19823195196, 
1086.79680899531, 1089.53650740066, 1101.56581663204, 1163.2291892284, 
1139.8293826996, 1125.56954148811, 1024.34059527431, 1152.0981675668, 
1064.32755358318, 1009.13933210229, 1210.55906508268, 1136.30532890842, 
1167.56870811327, 1163.3444730351, 1030.3320225021, 1334.11561872902, 
1186.00627022209, 1208.89776922561, 1172.03331588195, 1118.47337542874, 
1087.92709804022, 942.085245232004, 1159.78077235011, 1112.90807832063, 
1105.76965865985, 1104.31469614124, 1231.97413518222, 1228.57728152883, 
1208.44334175124, 1120.57185259478, 1014.7309899663, 1144.9384886849
)), .Names = c(""Year"", ""CanESM2"", ""GFDL-ESM2M"", ""inmcm4"", ""MRI-CGCM3"", 
""NorESM1-M""), row.names = c(NA, -90L), class = ""data.frame"")
</code></pre>

<p>Based on visual analysis, I suspect that there is a increasing <code>precip</code> trend in all models. But I need some kind of metric that can confirm an quantify this trend.</p>

<ul>
<li><strong>For each model in this dataset, how can I calculate the rainfall trends for every year and for decades (2010-2019, 2020-2029 and so on)?</strong>  </li>
<li><strong>Is it a simple linear regression of year against <code>precip</code> sufficient to show the trend?</strong></li>
</ul>
"
"0.0948209311861521","0.0935674292327708","183603","<p>Please apologize for this potentially ""stupid"" question. But I am currently attempting to test a mediation in for a multilevel dataset. Unfortunately, the residuals of the regressions do not follow a Normal distribution and they do not have a constant variance. Ideally, I would thus use bootstrapping to obtain confidence intervals. However, the Mediation package in R does not provide this function for multilevel datasets. Instead, it calculates Quasi-Bayesian intervals in this case.</p>

<p>My question is:
<strong>Can I use Quasi-Bayesian Confidence intervals, if I am aware the residuals do not follow a Normal distribution and that they are heteroskedastic? If not, which package/functions could I use instead?</strong></p>

<p>Here is the code I have used. Unfortunately I cannot share a sample of my data, since it is confidential.</p>

<pre><code>&gt; med.fit &lt;- lmer(OTIF ~  LDLV + COLT + Slack2 + (1 | BU), data = Data_P5)
&gt; out.fit &lt;- lmer(EBIT ~ OTIF + Slack2 + LDLV + COLT  + (1 | BU), data = Data_P5)
&gt; med.out &lt;- mediate(med.fit, out.fit, treat = ""Slack2"", mediator = ""OTIF"", sims = 100)
&gt; summary(med.out)

Causal Mediation Analysis 
Quasi-Bayesian Confidence Intervals
Mediator Groups: BU 
Outcome Groups: BU 

Output Based on Overall Averages Across Groups 

            Estimate 95% CI Lower 95% CI Upper p-value
ACME            5.12e-03     1.49e-03     1.06e-02    0.00
ADE            -5.64e-03    -1.77e-02     8.46e-03    0.50
Total Effect   -5.24e-04    -1.38e-02     1.41e-02    0.86
Prop. Mediated -3.21e-01    -1.34e+01     6.01e+00    0.86

Sample Size Used: 167 


Simulations: 100 
</code></pre>

<p>Since bootstrapping is not available for multilevel models I get an error message:</p>

<pre><code>&gt; med.fit &lt;- lmer(OTIF ~  LDLV + COLT + Slack2 + (1 | BU), data = Data_P5)
&gt; out.fit &lt;- lmer(EBIT ~ OTIF + Slack2 + LDLV + COLT  + (1 | BU), data = Data_P5)
&gt; med.out &lt;- mediate(med.fit, out.fit, treat = ""Slack2"", mediator = ""OTIF"", sims = 100, 
+ boot = TRUE)
Error in mediate(med.fit, out.fit, treat = ""Slack2"", mediator = ""OTIF"",  : 
'boot' must be 'FALSE' for models used
</code></pre>
"
"0.0446990156267674","0.0441081091391231","183926","<p>I'm doing a cox regression analysis. My model is given by</p>

<pre><code>final_model  = coxph(S ~ gearbox_model + cumGwh + manufac + turbine_model + 
                  gearbox_model:cumGwh + cumGwh:turbine_model + 
                  manufac:turbine_model, timelist),
</code></pre>

<p>where </p>

<pre><code>S = Surv(tdm$start, tdm$end, tdm$delta)
</code></pre>

<p>and timelist consists of my data. Now, from R I get the following (which is just the weird part of the analysis. All the other coefficients are estimated by R).</p>

<pre><code>Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

                                 exp(coef) exp(-coef) lower .95 upper .95
manufacManufacturer2:turbine_model12        NA         NA        NA        NA
manufacManufacturer3:turbine_model12        NA         NA        NA        NA
</code></pre>

<p>I've read somewhere that I should consider collinearity, but I dont know how. Also, I've read that it could be due to not having enough data. For example, there is no events/deaths/ (delta = 1) for the combination of turbine_model =3 and Manufacturer 3.</p>

<pre><code>, , turbine_model = 3
              delta
  Manufacturer1   2
  Manufacturer2   0
  Manufacturer3   0
  Manufacturer4  15
</code></pre>

<p>But I'm kinda clueless how to proceed.</p>
"
"0.0223495078133837","0.0441081091391231","184391","<p>I am attempting to perform a piecewise/segmented logistic regression on survey data using  <a href=""http://www.asdfree.com/2015/11/statistically-significant-trends-with.html"" rel=""nofollow"">this tutorial</a> as my basis. I have data for the period 2006 to 2013, however 2012 is missing.</p>

<p>The analysis proceeds as expected until the point in step 8 where I add the segmented variable with one breakpoint (the final line of code in the example below).</p>

<pre><code>library(segmented)
df &lt;- data.frame(yr=c(2006:2011,2013),
             mean= c(0.11290830, 0.12814364, 0.11149552, 0.12071058, 0.11776731, 0.10363014, 0.09888132),
             wgt = c(602.2272, 546.2958, 594.1818, 756.0167, 579.1533, 481.9694, 654.3281))
o &lt;- lm( log( mean ) ~ yr , weights = wgt , data = df )
os &lt;- segmented( o , ~yr)
</code></pre>

<p>At this point I get the error message:</p>

<blockquote>
  <p>""Error in segmented.lm(o, ~yr) : only 1 datum in an interval: breakpoint(s) at the boundary or too close each other""</p>
</blockquote>

<p>From my reading, in particular <a href=""http://r.789695.n4.nabble.com/Estimating-and-predicting-using-quot-segmented-quot-Package-td4682541.html"" rel=""nofollow"">here</a>, this is because the breakpoint falls at 2007, thus leaving 2006 on it's own and unable to have a slope calculated for it. I understand that this is likely because I have so few data points.</p>

<p>Does anyone have any tips for getting around this or another package / technique that would be more appropriate? The second link suggests using additional dummy data but I'm a bit wary of this approach.</p>
"
"0.070675349274022","0.0557928352651671","184713","<p>I am fairly new to R. I have attempted to read up on time series analysis and have already finished </p>

<ol>
<li>Shumway and Stoffer's <a href=""http://www.stat.pitt.edu/stoffer/tsa3/"" rel=""nofollow"">Time series analysis and its applications 3rd Edition</a>,</li>
<li>Hyndman's excellent <a href=""https://www.otexts.org/fpp"" rel=""nofollow"">Forecasting: principles and practice</a></li>
<li>Avril Coghlan's <a href=""http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html"" rel=""nofollow"">Using R for Time Series Analysis</a></li>
<li>A. Ian McLeod et al <a href=""http://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf"" rel=""nofollow"">Time Series Analysis with R</a></li>
<li>Dr. Marcel Dettling's <a href=""https://stat.ethz.ch/education/semesters/ss2013/atsa/ATSA-Scriptum-SS2013_130218.pdf"" rel=""nofollow"">Applied Time Series Analysis</a></li>
</ol>

<p>Edit: I'm not sure how to handle this but I found a usefull resource outside of Cross Validated. I wanted to include it here in case anyone stumbles upon this question. </p>

<p><a href=""http://isites.harvard.edu/fs/docs/icb.topic79832.files/L06_Program_Evaluation_2/Segmented_Regression.Wagner.2002.pdf"" rel=""nofollow"">Segmented regression analysis of interrupted time series studies in medication use research</a></p>

<p>I have a univariate time series of the number of items consumed (count data) measured daily for 7 years. An intervention was applied to the study population at roughly the middle of the time series. This intervention is not expected to produce an immediate effect and the timing of the onset of effect is essentially unknowable.</p>

<p>Using Hyndman's <code>forecast</code> package I have fitted an ARIMA model to the pre-intervention data using <code>auto.arima()</code>. But I am unsure of how to use this fit to answer whether there has been a statistically significant change in trend and quantify the amount.</p>

<pre><code># for simplification I will aggregate to monthly counts
# I can later generalize any teachings the community supplies
count &lt;- c(2464, 2683, 2426, 2258, 1950, 1548, 1108,  991, 1616, 1809, 1688, 2168, 2226, 2379, 2211, 1925, 1998, 1740, 1305,  924, 1487, 1792, 1485, 1701, 1962, 2896, 2862, 2051, 1776, 1358, 1110,  939, 1446, 1550, 1809, 2370, 2401, 2641, 2301, 1902, 2056, 1798, 1198,  994, 1507, 1604, 1761, 2080, 2069, 2279, 2290, 1758, 1850, 1598, 1032,  916, 1428, 1708, 2067, 2626, 2194, 2046, 1905, 1712, 1672, 1473, 1052,  874, 1358, 1694, 1875, 2220, 2141, 2129, 1920, 1595, 1445, 1308, 1039,  828, 1724, 2045, 1715, 1840)
# for explanatory purposes
# month &lt;- rep(month.name, 7)
# year &lt;- 1999:2005
ts &lt;- ts(count, start(1999, 1))
train_month &lt;- window(ts, start=c(1999,1), end = c(2001,1))
require(forecast)
arima_train &lt;- auto.arima(train_month)
fit_month &lt;- Arima(train_month, order = c(2,0,0), seasonal = c(1,1,0), lambda = 0)
plot(forecast(fit_month, 36)); lines(ts, col=""red"")
</code></pre>

<p>Are there any resources specifically dealing with interrupted time series analysis in R? I have found <a href=""http://epoc.cochrane.org/sites/epoc.cochrane.org/files/uploads/21%20Interrupted%20time%20series%20analyses%202013%2008%2012_1.pdf"" rel=""nofollow"">this</a> dealing with ITS in SPSS but I have not been able to translate this to R. </p>
"
"0.141350698548044","0.139482088162918","185116","<p>Goal is to evaluate chess players using a novel analysis system I'm been working on -- not all wins are created equal, finding the only move in razor sharp positions is better than finding the best move when the ten-best-alternates are negligibly worse, etc.</p>

<p>Current dataset I'm working with towards proof of concept has 30 players. The design matrix has players as the columns, but each player gets two columns: one for when they're playing as white, one for when they're playing as black. Each row of the design matrix represents half of a match, and 1/0/-1 dummies are used for white/not present/black.</p>

<p>Example: if Player 4 and Player 9 played a match, the design matrix will have two rows for this match. One row will have p4w assigned a ""1"" and p9b assigned a ""-1"". The other row will have p4b assigned a ""-1"" and p9w assigned a ""1"". All other player columns are 0.</p>

<p>The result vector is the Engine's score for the player playing as white in that half of the match.</p>

<p>There's also two other columns, Sw and Sb, to attempt to quantify the value of being white first in any given match and if a penalty exists for the player who started as black once they switch to white -- since white always moves first, and white wins more games than black, black is more likely to be disadvantaged after the first game.</p>

<p>Using matrix math rather than an R function.</p>

<pre><code>csv &lt;- read.csv(""~/chess.csv"", header=TRUE)
engine &lt;- as.numeric(csv$Engine)

# ready design matrix/remove dropped variables
csv$Engine &lt;- NULL
csv$Sb &lt;- NULL
csv$P30w &lt;- NULL
csv$P30b &lt;- NULL

# readies X and Y
X &lt;- data.matrix(csv)
Y &lt;- engine

# remove copies
remove(csv)
remove(engine)

# Add one column of ""1"" to X
one.col &lt;- matrix(1, nrow(X), 1)
X &lt;- cbind(X, one.col)

# transposing X
X.t = t(X)

# X'X, X'Y
X.t.X &lt;- X.t %*% X
X.t.Y &lt;- X.t %*% Y

# MATHS
betahat = solve(X.t.X) %*% X.t.Y
</code></pre>

<p>Here's the CSV: <a href=""http://www.filedropper.com/chess_1"" rel=""nofollow"">http://www.filedropper.com/chess_1</a></p>

<p>Right from the top, I have to drop Sb -- it's redundant. I then am forced to drop a player to defeat the ""system is computationally singular"" error. In this case, I'm dropping the same player lm() would: the last one.</p>

<p>I have no philosophical objections to dropping variables but for the purposes of this, for evaluating players against each other, the incompleteness is troublesome.</p>

<p>Using Ridge Regression ""works"" to prevent any variable from being dropped, but this is unsatisfying -- are the results really then meaning what they should? X + 0 doesn't help matters for this problem either.</p>

<p>Are there any other tools I'm missing? Is ridge regression the right path to take for this problem but, rather than penalize towards zero, penalize towards priors?</p>
"
"0.113960576459638","0.112454054604034","185247","<p>I am trying to use random forest to show the results from Cox regression analysis.</p>

<p>I have 757 samples with complete follow-up data (456 cases and 301 control). The case is defined as <code>scores</code> &lt;12, and control was <code>scores</code> >=12. The <code>status</code> of case is coded as 1, and control is 0.The follow-up time is showed in another variable called <code>time</code>. The other variables were <code>age</code>, glucose <code>glu</code>, interested molecular levels <code>marker</code> and the other confounders.</p>

<p>Firstly, I used the survival analysis to find out the most significant model as below:</p>

<pre><code> coxph(Surv(time, status) ~ age + glu + marker + 
       hdl + smoking + drinking, data=raw)
</code></pre>

<p>It turns out the <code>glu</code> and <code>marker</code> has significant interaction: </p>

<p><a href=""http://i.stack.imgur.com/b72U1.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/b72U1.png"" alt=""enter image description here""></a></p>

<p>the <code>marker</code> is significantly higher in the patients than the control if the patients also has high <code>glu</code>.</p>

<p>The I use the random forest in two models. One is for <code>scores</code>, the other is for <code>status</code>:</p>

<pre><code> model1&lt;-randomForest(formula = as.factor(scores) ~ age + glu + 
             marker + hdl + smoking + drinking + time,
             data = raw, importance = TRUE, proximity = TRUE, 
             na.action = na.omit) 

 model2&lt;-model1&lt;-randomForest(formula = as.factor(status) ~ age + glu
             + marker + hdl + smoking + drinking + time, 
             data = raw, importance = TRUE, proximity = TRUE, 
             na.action = na.omit) 
</code></pre>

<p>The importance of predictors in the two models were like this:</p>

<p><a href=""http://i.stack.imgur.com/X4agu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/X4agu.png"" alt=""enter image description here""></a></p>

<pre><code>model1: OOB estimate of  error rate: 82.3%
model2: OOB estimate of  error rate: 33.55%
</code></pre>

<p>I prefer to use model1 because the MeanDecreaseAccuracy is better to be showed in the paper, but its accuracy is only 17.7%. Do you think this accuracy is acceptable? Or the model2 is better?</p>

<p>What is the other suggestion you would give in this case?</p>

<p>I posted this question in the wrong place stackoverflow: <a href=""http://stackoverflow.com/questions/34111732/how-to-interperate-the-accuracy-and-importance-in-randomforest-test-from-cox-reg"">http://stackoverflow.com/questions/34111732/how-to-interperate-the-accuracy-and-importance-in-randomforest-test-from-cox-reg</a>
so I migrated it to cross-validated.</p>
"
"0.0565402794192176","0.0697410440814588","185495","<p>I am developing some stochastic simulations in which I have four explanatory variables that I named <code>Birth Rate</code>, <code>Inactivation Rate</code>, <code>Deletion Rate</code> and <code>Model</code>. They are all continuous data. I have several responses. One of these responses is a categorical value that can take 5 categories that I just labelled 1 to 5. I called this response an outcome.</p>

<p>I am having lots of trouble figuring out an analysis that I could use to understand the influence of my explanatory variables (all continuous data) on my response (categorical data). For example, I am interested in understanding how the outcome is influenced by <code>Birth Rate</code>, <code>Inactivation Rate</code>, <code>Deletion Rate</code> and <code>Model</code>. </p>

<p>I thought about a multinomial logistic regression, but I am not quite sure whether I can actually apply it to my data.</p>

<hr>

<p><em>Edit</em>: I have followed <a href=""https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;ved=0ahUKEwj30O-9u8zJAhXIqg4KHWlwDjQQFggsMAI&amp;url=https%3A%2F%2Fstatsthewayilikeit.files.wordpress.com%2F2015%2F05%2Fmultinomial-logistic-regression.docx&amp;usg=AFQjCNHj4IdMOAdjfavqcd-Q8HG66vYoag&amp;sig2=ScEoSVJGu1dDvMyIiAyljQ&amp;cad=rja"" rel=""nofollow"">this Word document</a> to do a multinomial logistic regression. An example of one of my plots after performing the regression is reproduced below (the x-axis is in log10 for the <code>Birth Rate</code>):  </p>

<p><a href=""http://i.stack.imgur.com/jjITQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jjITQ.png"" alt=""enter image description here""></a></p>

<p>My code in R:</p>

<pre><code>all.m        &lt;- multinom(code_Final_Info ~ Model+Birth_Rate+Inact_Rate+Del_Rate1+Time, 
                         data=all.mod)
PredProb     &lt;- cbind(preds, predict(all.m, newdata=preds, type='probs', se=TRUE))
PredProbMelt &lt;- melt(PredProb, value.name=""Probability"", 
                     id.vars=c(""Model"",""Birth_Rate"",""Inact_Rate"",""Del_Rate1"",""Time""))

(p &lt;- ggplot(PredProbMelt, aes(x=Del_Rate1, y=Probability, colour=Time)) + 
                           geom_line() +
                           facet_grid(variable ~., scales=""free"") + 
                           theme_bw()) 
</code></pre>

<p>My data.frame looks like:  </p>

<pre><code>&gt; str(all.mod)
'data.frame':   900000 obs. of  6 variables:
 $ Model          : num  0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 ...
 $ Birth_Rate     : num  -4.05 -4.05 -4.05 -4.05 -4.05 ...
 $ Inact_Rate     : num  -3.14 -3.14 -3.14 -3.14 -3.14 ...
 $ Del_Rate1      : num  -4.26 -4.26 -4.26 -4.26 -4.26 ...
 $ code_Final_Info: Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ Time           : Factor w/ 3 levels ""0T250"",""1T500"",..: 1 1 1 1 1 1 1 1 1 1 ...
</code></pre>
"
"0.0948209311861521","0.0831710482069074","185757","<p>I'm doing a meta-analysis which involves some multiple logistic regression.  As it's a meta-analysis, I'm compiling data from various studies, which therefore differ slightly in their methodology.  One particular aspect of methodology is likely to affect the response variable, and I therefore decided to include it as a random effect in my models. </p>

<p>However, now I'm being asked to provide more information about the likely effect of this predictor.  Is it significant?  How strong is the effect?  The only way I can think to do so is to treat it as a fixed effect and see if adding/removing it improves the model or not, using F tests to compare models (they are quasi-binomial models).  However, I had a feeling that treating variables as both random and fixed variables was wrong - it should be one of the other.  I've never quite got a grip on the difference between the two, and would appreciate any advice.
Thanks</p>

<p>edit: Here's a bit more info about my study.  In my meta-analysis I'm comparing the results of other studies to my own study.  The response in the model is the similarity in the data.  Regarding this particular aspect of methodology, I used ""method A"". If all the other studies used ""method A"" it would be fine, but some used ""method A"", some ""method B"", some ""method C"" and some ""method D"". Each different method will introduce bias, but may do so to a different degree.  Hope that makes sense.</p>
"
"0.070675349274022","0.0697410440814588","186240","<p>Hi I am trying a mediation analysis (using library(""mediation"") in R)</p>

<p>My model has 3 predictors and one mediator (n=455), but I am only interested in predictor 1. There is some collinerarity between predictor 1 and 2 - 0.383444 (Pearson). No collinerarity between predictor 3 and the others. The Mediator is correlated with IV1 and slightly with IV2. Predictors, Mediator and dependent variable are all continuous.</p>

<pre><code>lm(DV ~ IV1 + IV2 + IV3 , data = data)
</code></pre>

<p>Only IV2 is significant, R2 = 0.050</p>

<pre><code>lm(DV ~ Mediator + IV1 + IV2 + IV3 , data = data)
</code></pre>

<p>Mediator and IV2 is significant, R2 = 0.056</p>

<p>I have a much bigger dataset with n = 1200, but unfortunately I don't have Mediator information available for them. If I do a linear regression to predict DV with this dataset, IV1 and IV2 are both highly significant, the standardized beta meaningful.</p>

<ol>
<li><p>With this information can I investigate the mediating effect of the mediator on IV1 with my small dataset with 455 subjects (using the mediate()-Function of the ""mediation""-package in R) , even though the dataset itself is too small to show a significant effect of IV1 on the DV?</p></li>
<li><p>Also, I was wondering whether my mediator might mediate IV2-effect. The correlation between IV1 and the mediator is higher than between IV2 and the mediator though. </p></li>
</ol>

<p>I am thankful for any ideas.</p>
"
"0.109489780290272","0.0990388308340227","186725","<p>Short version: How would one be able to quantify an intervention effect in time-series analysis when the intervention decreases seasonal amplitude variation but doesn't directly effect the median?</p>

<p><a href=""https://www.dropbox.com/s/hb3g7j17igeqnoc/dat.csv?dl=0"" rel=""nofollow"">Here</a> is a link to my raw data.</p>

<p>I have a complex time-series of daily incidence numbers for a population over 7 years, totaling 2557 observations. There is a strong weekly and yearly seasonality (high incidence in winter months and low incidence in summer months). There is a baseline negative trend which is orders of magnitude smaller than the seasonality. An intervention was introduced at time = 1700. This intervention should theoretically not cause a level shift. My aim is to detect whether the intervention increases the baseline negative trend.</p>

<p>I have attempted to fit a dynamic linear regression with ARIMA errors in R using <code>auto.arima()</code> in the <code>forecast</code> package. I modeled the weekly season using a dummy variable for each weekday and the weekend. I modeled the monthly seasonality with harmonics using <code>fourier()</code> function in the <code>forecast</code> package. An the intervention effect was coded in by specifying the time index and post-intervention times as independent variables using the methods described in <a href=""http://isites.harvard.edu/fs/docs/icb.topic79832.files/L06_Program_Evaluation_2/Segmented_Regression.Wagner.2002.pdf"" rel=""nofollow"">Segmented regression analysis of interrupted time series studies in medication use research</a>. With these variables specified <code>auto.arima()</code> suggests an ARMA(7,7) process. The coefficients for baseline trend and post-intervention trend are however non-significant.  </p>

<p>I am concerned that by using fourier terms to model away the seasonality I am artificially removing any intervention effect, as visual analysis of the time series indicates that the intervention is specifically decreasing incidence during the winter months and therefore reducing the yearly seasonal variability. </p>
"
"0.148390185051596","0.152794972109509","187100","<p>I have a certain knowledge in stochastic processes (specially analysis of nonstationary signals), but in addition to be a beginner in R, I have never worked with regression models before.
Well, I have some doubts on understanding the outcome of the function summary() in R, when using with the results of a glm model fitted to my data. Well, suppose I used the following command to fit a generalized linear model to my data:**</p>

<pre><code>glm_model &lt;- glm(Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
</code></pre>

<p>Then I use summary(glm_model) to obtain the following:</p>

<pre><code>Call: 
glm(formula = Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-7.4583  -0.8985   0.1628   1.0670   6.0673  
Coefficients:

Estimate Std. Error t value Pr(&gt;|t|)    

(Intercept)        8.522e+00  6.553e-02 130.041  &lt; 2e-16 ***

Input1            -3.819e-04  3.021e-05 -12.642  &lt; 2e-16 ***

Input2            -2.557e-04  2.518e-05 -10.156  &lt; 2e-16 ***

Input3            -3.202e-02  1.102e-02  -2.906  0.00367 ** 

Input4            -1.268e-01  7.608e-02  -1.666  0.09570 .  

Input1:Input2      1.525e-08  2.521e-09   6.051 1.53e-09 ***


Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 2.487504)
    Null deviance: 18544  on 5959  degrees of freedom
Residual deviance: 14811  on 5954  degrees of freedom
  (1708 observations deleted due to missingness)
AIC: 22353
Number of Fisher Scoring iterations: 2
</code></pre>

<p>From a estimation theory perspective, I understand that ""estimate"" and ""Std. Error"" are the estimates and the standard deviation of the unknown parameters (beta1, beta2,...) of my model. However, there are some things I do not understand:</p>

<p>1) How can I assess how good my fit is from the output of <code>summary()</code>? We could not use only the information of the standard deviation of the parameter estimators to assess the goodness-of-fit. I would expect to have access to the sampling distribution of a given parameter estimator to know the % of estimates within +- 1std, +-0.5std or any +-x*std, for example. Other option would be knowing the theoretical distribution of the parameter estimator, so as to try to calculate its Cramer Rao Lower Bound and compare with the calculated std.</p>

<p>2) What does the t value (or Pr(>|t|) ) have to do with the goodness-of-fit? Since I am not familiar with regression models, I do not know the connection between the student t distribution and the estimation of the model parameters. What does it mean? Is the parameter estimator of the glm model distributed according to the student t pdf (like the sample estimator for small samples of an unknown population)? What conclusions should I take from Pr(>|t|)?</p>

<p>3) Do we have a more general form of assessing the goodness-of-fit, like a measure of the variability of the data my model can capture, maybe a table of critical values for such a measure given a certain significance level?** </p>

<p>4) When fitting a glm model, do we need to specify a significance level? If yes, why such an information is not provided by the summary function?</p>

<p>5) The summary function outputs some measures based on information theory, like AIC: 22353. Can we define an optimal reference value for AIC? What is a good AIC value? My intuition is that we could not do so, like other information theory measures (mutual information, entropym,...)</p>

<p>Thank you for your help!</p>
"
"0.0632139541241014","0.0623782861551805","187468","<p>I have a question about analyzing a dataset that I'm currently working with. Each row of the dataset represents an individual songbird, and its reproductive success over the course of a breeding season. Reproductive success was recorded as a score or rank that was based on breeding activities that we observed for each bird. Scores were recorded as follows:</p>

<pre><code>1 - unpaired
2 - paired
3 - successfully raised 1 brood of fledglings
4 - successfully raised 2 broods
5 - successfully raised 3 broods
</code></pre>

<p>In my analysis, these scores will be the response variables, and several environmental covariates will be used as predictor variables. Typically I see that ranked data is analyzed using ordinal logistic regression, but would it also be reasonable to model this data using a poisson glm/glmm? I have experience with poisson glm's, and this distribution is often used in my field (wildlife ecology) for count data or to model age structure. This reproductive index is not commonly used, so there are not many examples I've come across that attempted to do a similar analysis.</p>

<p>Thanks!
Jay</p>
"
"0.0446990156267674","0.0441081091391231","187509","<p>With a small book-exercise with four metric variables on 10 cases (one dependent/outcome, three independent/predictor) I ran <em>linear regression</em> in <code>SPSS</code> and <code>R</code>, and <em>ANOVA</em> (in <code>SPSS</code> declaring the predictors as ""covariates"").<br>
I found the output of the <em>SSQ</em> (Sum-of-Squares) different - and obviously from this also the F-test statistic and the p-values. Except from the last predictor the displayed values are different (the predictors in <code>R</code>may be reordered and the analysis be rerun to find all <code>SPSS</code>- coefficients).                   </p>

<p>By reengineering the computations in matrix-formulae I could reproduce the SPSS-values as well as the R-values and found, that <code>SPSS</code> uses the (partial) SSQ based on the logic of the ""usefulness""-coefficients for each predictor (which is sort of semipartial coefficient), while <code>R</code> simply uses the (hierarchically) partial SSQ. <em>(Unfortunately I'm not sure how to express that two approaches correctly so this toy-characterizing might be improved)</em> .             </p>

<p><strong><em>Q:</em></strong> Has that property of different focuses/philosophies been discussed anywhere? Is there some advantage of one over the other?   </p>

<p><hr>
Data: (taken from M. Backhaus et al., multivariate Verfahren)             </p>

<pre><code>predictors                   outcome-item
---------------------------+-------------
Preis   VerkFoer  Vertreter  Absatzmenge
12.50      2000      109      2298
10.00       550      107      1814
 9.95      1000       99      1647
11.50       800       70      1496
12.00         0       81       969
10.00      1500      102      1918
 8.00       800      110      1810
 9.00      1200       92      1896
 9.50      1100       87      1715
12.50      1300       79      1699
</code></pre>

<p>The comparision of the output:
<a href=""http://i.stack.imgur.com/zNJDB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zNJDB.png"" alt=""bild""></a></p>
"
"0.0670485234401511","0.0882162182782462","188510","<p>I have developed a model (TSM) which is very good at forecasting daily revenue, however it is very black box. The TSM is univariate, whereas the regression models are multivariate. My goal here is to identify the causal factors of revenue for policy recommendations using regression analysis in R. The models that I am using are decision tree, random forest, linear model (some variables), linear model 2 (all variables), xgboost, and lasso regression. All variables are numeric. </p>

<p>Looking at random forest vs linear regression, (both rmse and mae are similar for these two), we see something odd: Looking at the variable importance plot for random forest, variable x is the least important. However, for the regression model, it is highly significant. Furthermore, lasso regression also indicates that the variable is very important. How can this be so? </p>

<p>This variable has a important policy implication to the business, so which model is right - is variable x the most important or is it the least important?  I vaguely recall Breiman writing a paper some time ago discussing something along the lines of this. </p>

<p>The below chart is the out of sample accuracy of the models over a 4 month time period. </p>

<pre><code>&gt; a
    tsm      tree        rf       lm       lm2      xgb      lasso
 0.9715964 0.9854246 0.9904363 0.981333 0.9817757 0.974603  0.997324
</code></pre>

<p>PS, I am sorry I cannot disclose the variables themselves since they are confidential to the company.</p>

<p>Thank you for your assistance!</p>
"
"0.104828483672192","0.103442685121077","189903","<p>I am conducting logistic regression analysis: The data includes 107 observations, dependent variable is a binary one, there is about 5 covariates which are both continuous, binary and multi-categorical variables. I want to use some cut_off points to predict the outcome.</p>

<p>So basicly, I select one cut_off point (based on the requirement that the sensitivity >70% and specificity > 70%). Then I devide my data into train set (85% data points) and test set (15% data points).</p>

<p>I fit the model with 5 covariates on the train set, and use the model to predict the outcome on the test set. I use the glm() function to fit the model, and glm.predict() function to predict on test sets. </p>

<p>Since there is missing data, I create 40 imputed data sets using MICE package in R. The procedure above is repeated over 40 imputed data sets. For each data set, I obtain the mis-classification errors.</p>

<p>So, to get the overall mis-classification errors, I averaged over 40 mis-classification error rates.</p>

<p>My question is: </p>

<p>How to assess the variability of this overall mis-classification errors?</p>

<p>As I am thinking that we can not use the usual formula to calculate the variance for this number, as the mis-classification errors over different imputed data sets might be correlated to each other.</p>

<p>Does any one have a suggestion or reference to do this?
(I am using R).
Thank you for any inputs.</p>
"
"0.0547448901451359","0.0540211804549215","190586","<p>I am using cross correlation to demonstrate a potential link between two time series (ext &amp; co). Both series are strongly autocorrelated, so it is difficult to assess the dependence between the two series. For a quick preliminary analysis, the cross correlation shows a clear (somehow delayed) link between the two time series, although it might spurious. <a href=""http://i.stack.imgur.com/eHUnj.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eHUnj.jpg"" alt=""CCF""></a>. Prewhitening seems to be the best option; I will prewhiten my x variable by fitting an ARIMA process and then use the coefficients to filter my variable y. My question is if I should estimate the coefficients of the ARIMA process (for example using <code>auto.arima</code>) using my series x or by using the residuals of the OLS regression of x on y.</p>
"
"0.226068104669573","0.218948454735618","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.0836242010007091","0.0825187161885156","193054","<p>I'm fairly new to time series analysis. I want to analyze two series of variables in a span of time to predict a binary outcome.</p>

<p>For example i collect data over time at my home of two variables:</p>

<p>VarA the temperature over time</p>

<p>VarB the humidity over time</p>

<p>Then at 12:00 am i stop collecting this data and i see at 4:00 pm if it rains or not.</p>

<p>With a big dataset i want to predict given the time seres data collected till 12:00 am if it will rain at 4:00 pm. How can i accomplish this?</p>

<p>I was thinking about a k-nearest neighbors regression type analysis but i'm not sure how can i implement this.</p>

<p>EDIT:
Here a fictional data set, i don't have already the data because i'm still defining the details. I want to know if studying the two time series (the difference from the start or other parameters) is there a way to predict the outcome of the day</p>

<p><a href=""http://i.stack.imgur.com/VOJsa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VOJsa.png"" alt=""""></a></p>
"
"0.0446990156267674","0.0441081091391231","193609","<p>I would be very appreciated if someone help me. I have F(x) (Cumulative density) values for specific x values which is not equal interval but I have not any sample data to estimate parameters of distribution by using <code>fitdistr()</code> function. I know that data probably have weibull distribution and regression analysis for detecting parameters probably yields wrong results. How can I calculate the weibull parameters for this CDF data by using MLE method in R?</p>

<p>My Data ex</p>

<pre><code>     x       F(x)
 0,363       0,01    
 0,417       0,12    
 0,479       0,38    
 0,550       0,75    
 0,631       1,22    
 0,72        1,74    
 0,83        2,29    
 0,96        2,85    
 1,10        3,41    
 1,26        3,97    
 1,45        4,55    
 1,66        5,15    
 1,91        5,79    
 2,19        6,50    
 2,51        7,26    
 2,88        8,08    
 3,31        8,98    
 3,80        9,96    
 4,37        11,04    
 5,01        12,21    
 5,75        13,49    
 6,61        14,89    
 7,59        16,42    
 8,71        18,09    
 10,00       19,92    
 11,48       21,93    
 13,18       24,18    
 15,14       26,73    
 17,38       29,67    
 19,95       33,09    
 22,91       37,08    
 26,30       41,71    
 30,20       47,01    
 34,67       52,94    
 39,81       59,38    
 45,71       66,12    
 52,48       72,89    
 60,26       79,38    
 69,18       85,28    
 79,43       90,33    
 91,20       94,35    
 104,71      97,29    
 120,23      99,26    
 138,04      99,94    
</code></pre>
"
"0.0899550337218996","0.0887658573546531","193752","<p>I have one response variable $Y$ and one predictor $X$. I am trying to fit a polynomial regression model and try to compare different model with different highest power term, the output of ANOVA in R is the following</p>

<pre><code>Analysis of Variance Table
Model 1: Y ~ X
Model 2: Y ~ X + I(X^2)
Model 3: Y ~ X + I(X^2) + I(X^3)
Model 4: Y ~ poly(X, 5)

  Res.Df    RSS   Df  Sum of Sq       F    Pr(&gt;F)    
    504    19472                                   
    503    15347  1    4125.1     151.693 &lt; 2.2e-16 ***
    502    14616  1     731.8     26.909 3.104e-07 ***
    500    13597  2    1018.4     18.726 1.438e-08 ***

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I understand how to derive all these numbers in the table, but there are some contradiction. Here is my question: in this table, the last model is the biggest model in the sense that it contains all the predictors in all previous models, the ""Sum of Sq"" for Model 4 is 1018.4 = 14616-13597, i.e., the difference of the sum of residuals between model 3 and model 4 and the F statistic for Model 4, which is 18.726 is obtained by $\frac{RSS_3-RSS_4}{502-500}\div\frac{13597}{500}$, i.e., the difference in RSS between model 3 and model 4 divide by the difference of degree of freedom and then divide by the MSE of model 4. This makes a lot of sense. However, when I compute the F statistic for model 3, I am so confused. The ""Sum of Sq"" for model 3 is obtained via $731.8=15347-14616$, i.e,. the difference in RSS of model 2 and model 3. But the F statistic for model 3 is obtained via $\frac{RSS_2-RSS_3}{503-502}\div\frac{13597}{500}$, i.e., the difference in RSS of model 2 and model 3 divide by their difference in degree of freedom, BUTTTT then divide by the MSE of model 4. In my mind, it should finally divide the MSE of model 3 rather than model 4, since we are comparing the difference between model 2 and model 3. </p>
"
"0.0632139541241014","0.0623782861551805","194373","<p>First, I am new to analyzing public opinion polls and the r package ""Survey"".  I would like some advice.  I am running a regression model with weights from a Pew survey, however, I noticed that a significant portion of my data is missing because of the covariates.  As a robustness check, I would like to impute my data set.  I would like some advice on the best way to handle this in r.</p>

<p>I am most familiar with using the ""mice"" package in r to handle missing data.  I don't believe ""survey"" can accommodate mice.  Should I separate each imputed dataset and then perform a regression analysis (using ""survey"") for each dataset?  Is this the most efficient method?  Finally, how do you pool the estimates?  You don't average the estimates from all the imputed data sets, do you? </p>
"
"0.0446990156267674","0.0441081091391231","194470","<p>I am new to R and I am trying to find a relationship between Wing length (mm) and Weight (g) of black-capped chickadees using a data set of over 4000 data points. </p>

<p>I did a regression analysis and made a residual plot, and the result is attached. I'm new to R and new to statistics so I'm not sure what this means exactly...any insight would be helpful!</p>

<p>Also any tips on other ways to analyse this data would be great. </p>

<p><a href=""http://i.stack.imgur.com/4FQJW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4FQJW.png"" alt=""enter image description here""></a><a href=""http://i.stack.imgur.com/1fQ2u.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1fQ2u.png"" alt=""enter image description here""></a></p>
"
"0.0632139541241014","0.0623782861551805","196586","<p>I would like to do a gene x environment interaction analysis in a matched (1-1) case control samples. I referred all related previous publications and in most of the papers authors used either STATA or SAS. I got few references for performing conditional logisitic regression in R, for example using survival (clogit) package. But I couldn't find any reference for adding interaction terms in conditional logistic models in R. Can someone help me with references for interaction analysis using conditional logistic regression in R?</p>
"
"0.0316069770620507","0.0311891430775903","198272","<p>I am fairly new to machine learning, I am understanding the basics of regression analysis currently. I have come across this dataset California Housing <a href=""https://archive.ics.uci.edu/ml/datasets/Housing"" rel=""nofollow"">https://archive.ics.uci.edu/ml/datasets/Housing</a> provided by UCI. Is there any place that walks through for above dataset ( or any large dataset in general ) how to break down a unknown dataset and build a regression model out of it?</p>

<p>This will really help me in giving a high level picture and then i can work on details.</p>

<p>Thanks in advance!</p>
"
"0.118262479197817","0.108363438470314","198372","<p>I have a series of single-armed trials where the outcome is a binary response. Imagine a trial where you have no control arm; you merely give 100 patients a procedure (which can be done in many different ways) and see how many are 'well' (more later) at the end of the year. There are hundreds of these trials for me to look at.</p>

<p>I believe I can meta-analyse these as big group as follows, assuming x is the number well, n is n, and they're in df.</p>

<pre><code>model &lt;- rma(measure=""PLO"", xi=x, ni=n, data=df) #PLO = logit transformed proportion (log odds)
print(res, digits=3) #This will print the log odds
predict(model, transf=transf.ilogit, digits=3) #This will back-transform with the inverse logit transformation
</code></pre>

<p>I can plot this quite nicely with:</p>

<pre><code>forest(model,transf=transf.ilogit)
</code></pre>

<p>The thing is, as alluded to, there are lots of different ways to do the procedure and lots of different classifications of whether the patient is 'well'.</p>

<p>I want to do meta-regression/MV analysis on these trials (I may have over 100) to see if the characteristics of the trial predict the outcomes significantly.</p>

<p>I've done a lot of reading e.g <a href=""http://www.metafor-project.org/doku.php/tips:regression_with_rma"" rel=""nofollow"">http://www.metafor-project.org/doku.php/tips:regression_with_rma</a> but my problem is all the examples of meta-regression seem to treat each 'row' equally, when of course they should be weighted by n.</p>

<p>I was wondering if it would be valid to supply my predictors in question merely via the mods argument and otherwise performing the analysis as I did for the meta-analysis, e.g.:</p>

<pre><code>model_2 &lt;- rma(measure=""PLO"", xi=x, ni=n, data=df, mods=~predictor1 + predictor2 + predictor3)
</code></pre>

<p>If I do I end up with something like:</p>

<pre><code>Mixed-Effects Model (k = 60; tau^2 estimator: REML)

tau^2 (estimated amount of residual heterogeneity):     0.3651 (SE = 0.0908)
tau (square root of estimated tau^2 value):             0.6042
I^2 (residual heterogeneity / unaccounted variability): 81.40%
H^2 (unaccounted variability / sampling variability):   5.38
R^2 (amount of heterogeneity accounted for):            0.00%

Test for Residual Heterogeneity: 
QE(df = 57) = 311.1484, p-val &lt; .0001

Test of Moderators (coefficient(s) 2,3): 
QM(df = 2) = 0.2739, p-val = 0.8720

Model Results:

                      estimate      se     zval    pval    ci.lb   ci.ub     
intrcpt                  1.1155  0.2997   3.7220  0.0002   0.5281  1.7030  ***
predictor1               0.0974  0.2763   0.3525  0.7244  -0.4441  0.6390     
predictor2              -0.0818  0.2085  -0.3923  0.6949  -0.4905  0.3269  
</code></pre>

<p>1) Is this the appropriate way of doing this?</p>

<p>2) Also, when I used to do patient-level multivariate regression, my practice was to include variables in the multivariate analysis if they were significant on univariate analysis; is this standard practice for my example, too? As in should I supply them individually as single <code>mods=~predictor</code> and look for significance before including them in a model?</p>

<p>Thank you</p>
"
"0.120148093765837","0.125533879346626","198484","<p>Consider this example:</p>

<pre><code>foo &lt;-data.frame(x=c(0.010355057,0.013228936,0.016313905,0.019261687,0.021710159,0.023973474,0.025968176,0.027767232,0.029459730,0.030213807,0.023582566,0.008689883,0.006558429,0.005144958),
                 y=c(971.3800,1025.2271,1104.1505,1034.2607,902.6324,713.9053,621.4824,521.7672,428.9838,381.4685,741.7900, 979.7046,1065.5245,1118.0616))
Model3 &lt;- lm(y~poly(x,3),data=foo)
Model4 &lt;- lm(y~poly(x,4),data=foo)
</code></pre>

<p>For <code>Model3</code>, the <code>poly(x,3)</code> term is not significant:</p>

<pre><code>&gt; summary(Model3)

Call:
lm(formula = y ~ poly(x, 3), data = foo)

Residuals:
   Min     1Q Median     3Q    Max 
-76.47 -51.61  -0.55  38.22 100.57 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   829.31      17.85  46.463 5.14e-13 ***
poly(x, 3)1  -819.37      66.78 -12.269 2.37e-07 ***
poly(x, 3)2  -373.05      66.78  -5.586 0.000232 ***
poly(x, 3)3   -87.85      66.78  -1.315 0.217740    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 66.78 on 10 degrees of freedom
Multiple R-squared:  0.9483,    Adjusted R-squared:  0.9328 
F-statistic: 61.15 on 3 and 10 DF,  p-value: 9.771e-07
</code></pre>

<p>However, for <code>Model4</code> it is:</p>

<pre><code>&gt; summary(Model4)

Call:
lm(formula = y ~ poly(x, 4), data = foo)

Residuals:
    Min      1Q  Median      3Q     Max 
-34.344 -19.982   1.229  18.499  33.116 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  829.310      7.924 104.655 3.37e-15 ***
poly(x, 4)1 -819.372     29.650 -27.635 5.16e-10 ***
poly(x, 4)2 -373.052     29.650 -12.582 5.14e-07 ***
poly(x, 4)3  -87.846     29.650  -2.963 0.015887 *  
poly(x, 4)4  191.543     29.650   6.460 0.000117 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 29.65 on 9 degrees of freedom
Multiple R-squared:  0.9908,    Adjusted R-squared:  0.9868 
F-statistic: 243.1 on 4 and 9 DF,  p-value: 3.695e-09
</code></pre>

<p>Why does this happen? Note that the estimate of all coefficients is the same in both cases, since the polynomials are orthogonal. However, the significance is not. This seems to me difficult to understand: if I performed a degree 3 regression, it looks like I could drop the <code>poly(x, 4)3</code> term, thus reverting to a degree 2 orthogonal regression. However, if I performed a degree 4 regression, I shouldn't, even though the coefficients of the common terms have exactly the same estimate. What do I conclude? Probably that one should never trust subset selection :) An <code>anova</code> analysis says that the difference among the degree 2, degree 3 and degree 4 models is significant:</p>

<pre><code>&gt; Model2 &lt;- lm(y~poly(x,2),data=foo)     
&gt; anova(Model2,Model3,Model4)
Analysis of Variance Table

Model 1: y ~ poly(x, 2)
Model 2: y ~ poly(x, 3)
Model 3: y ~ poly(x, 4)
  Res.Df   RSS Df Sum of Sq       F    Pr(&gt;F)    
1     11 52318                                   
2     10 44601  1      7717  8.7782 0.0158868 *  
3      9  7912  1     36689 41.7341 0.0001167 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>EDIT: following a suggestion in comments, I add the residual vs fitted plots for <code>Model2</code>, <code>Model3</code> and <code>Model4</code></p>

<p><a href=""http://i.stack.imgur.com/9ZU8h.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9ZU8h.png"" alt=""enter image description here""></a>` </p>

<p>It's true that the maximum residual error is more or less the same for <code>Model2</code> and <code>Model3</code>, and it becomes nearly one third going from <code>Model3</code> to <code>Model4</code>. There seems to be still some kind of trend in the residuals, though it is less evident than for <code>Model2</code> and <code>Model3</code>. However, why does this invalidate the <em>p</em>-values? Which hypothesis of the linear model paradigm is violated here? I seem to remember that the residuals only had to be uncorrelated with the predictor. However, if they also have to uncorrelated among themselves, then clearly this assumption is violated and the <em>p</em>-values based on the t-test are invalid.</p>
"
"0.0597315721433636","0.0707303281615848","198925","<p>Although there has been some detailed discussions about power analysis on this website (for example <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression/22410#22410"">here</a> and <a href=""http://stackoverflow.com/questions/27234696/how-do-you-conduct-a-power-analysis-for-logistic-regression-in-r"">here</a>), the answer provided to this question has  outlines the steps to simulating a power analysis, <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">here</a>.</p>

<p>Say we take some data (data was linked to a <a href=""http://stats.stackexchange.com/questions/59829/which-bootstrapped-regression-model-should-i-choose"">bootstrapping question</a>)</p>

<p>We create a regression that will predict <code>admit</code> based on the two continous variables <code>gpa</code> and <code>gre</code></p>

<ul>
<li>Now we have a <code>n=400</code>. </li>
<li>We can then elect our power level, <code>alpha = 0.5</code></li>
<li>The effect size you would like to detect, e.g., odds ratios  (we obtain this from our regression)</li>
</ul>

<p>So in following the detailed method provided by @gung <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">here</a>, I want to run the simulation. Here is the code I have adjusted, but my output is not correct. Can someone outline what I have not understood</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
head(mydata)

set.seed(1234)

my.mod &lt;- glm(admit ~ gre + gpa , data = mydata, family = ""binomial"")


repetitions &lt;- length(mydata$admit)

gre &lt;- mydata$gre
    gpa &lt;- mydata$gpa


significant = matrix(nrow=repetitions, ncol=4)

for(i in 1:repetitions){
  responses          = mydata$admit
      #responses          = rbinom(n=N, size=1, prob=mydata$admit)      # we can interchange this comment
  model              = glm(responses ~ gre + gpa, family = binomial(link=""logit""))
  significant[i,1:2] = (summary(model)$coefficients[2:3,4]&lt;.05)
      significant[i,3]   = sum(significant[i,1:2])
      modelDev           = model$null.deviance-model$deviance
  significant[i,4]   = (1-pchisq(modelDev, 2))&lt;.05
}



sum(significant[,1])/repetitions      # pre-specified effect power for gre

sum(significant[,2])/repetitions      # pre-specified effect power for gpa

sum(significant[,4])/repetitions  # power for likelihood ratio test of model

sum(significant[,3]==2)/repetitions   # all effects power

sum(significant[,3]&gt;0)/repetitions    # any effect power
</code></pre>
"
"0.0316069770620507","0.0311891430775903","199141","<p>I would like to perform an anomaly temperature trends analysis in R. I have temperature data from 1901 to 2012. My idea to compute anomaly temperature trends was as follows: 
1. Compute temperature means per month over this time period
2. Subtract temperature means from actual data, to get an anomaly in temperature per month. 
3. Perform a linear regression on this anomaly values to get the anomaly temperature trends.</p>

<p>However, I am not quite sure about the approach. Anyone has done this before and could give me some hints? </p>
"
"NaN","NaN","200103","<p>This is with reference to <a href=""http://analyticspro.org/2016/03/05/r-tutorial-residual-analysis-for-regression/"">http://analyticspro.org/2016/03/05/r-tutorial-residual-analysis-for-regression/</a>.</p>

<p><a href=""http://i.stack.imgur.com/UJpHy.png""><img src=""http://i.stack.imgur.com/UJpHy.png"" alt=""Figure""></a></p>

<p>For Residual plot (b), where the residuals are increasing linearly (with respect to the predicted values), can we interpret that we are missing a variable? What else is the interpretation?</p>
"
"0.122653086996056","0.113467174660065","201105","<p>I'm about to have data from intercept surveys conducted in parks. The goal of the survey is to determine which characteristics of parks users find most important to park quality (do they care a lot about safety, a little about the facilities, and not at all about who else is there?).</p>

<p>We've designed a survey with open-ended questions to answer this question. The current plan is to take down the responses, and then, once we have them, group them into categories (safety, facilities, social environment, accessibility, etc). </p>

<p>For example, one question on the survey asks the user why they came to park. </p>

<p>Each user's response (we're allowing them to list as many reasons as they like, but are asking for primary reasons first, then secondary reasons and so on) will then be associated with some field coding. For one user it might be, say, facilities and park aesthetics, for another it might be easy access. We'll also have some demographic data (age, sex, ethnicity, activity at the park) for each user.</p>

<p><strong>Question 1:</strong> We want to determine which of the categories is most important to users, and if possible, by how much. I've never done any categorical data analysis, and I have no idea what to do here. For some questions we're just going to have counts: 16 people came for facilities, 10 for open spaces, etc.</p>

<p><strong>Question 2:</strong> A separate series of questions asks users to categorize park quality on a Likert-like scale (low to high quality), and also to rate sub-components of park quality in the same way (quality of facilities, from low to high, and so on). We want to determine which predictors have the largest effect on perceived park quality here as well.</p>

<p><strong>I want to know what type of models to fit to our data, and why.</strong></p>

<p>I'm presuming we want some categorical analogue of regression. I want to pick up theoretical underpinnings, learn how to fit models in R, and also how to perform diagnostics on them. </p>

<p>Once I've decided on the appropriate analysis and have picked up the necessary background, I'd like to pre-register my data analysis plan. I've never done this before and am curious what the convention is for this.</p>

<p>Some details about the sample of parks: the city Parks and Recreation department has selected 10 parks for us to visit. Their park selection criteria is not entirely known, but I think they want to visit some well developed and some under developed parks. There are five pairs of parks that the Parks department thinks are comparable. In each pair of parks, one has recently undergone renovation, and the other hasn't.</p>

<p>My background:</p>

<p>I have taken a first course in math-stat, a course on linear regression, and am halfway through a course on experimental design/ANOVA/EM/Bootstrap. I have some pure math, multi, lin-alg and optimization background as well. I have some limited experience in R as well.</p>
"
"0.0446990156267674","0.0441081091391231","201487","<p>I have two independent variable and one dependent variable. This is my summary when I use <code>lm(y~x_1+x_2)</code>:</p>

<pre><code>Residuals:
    Min      1Q  Median      3Q     Max 
-22.265  -9.563  -1.916   6.405  39.319 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  23.0107    18.2849   1.258  0.21407   
x_1          23.6386     6.8479   3.452  0.00114 **
x_2          -0.7147     0.3014  -2.371  0.02163 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 14.84 on 50 degrees of freedom
Multiple R-squared:  0.2018,    Adjusted R-squared:  0.1699 
F-statistic: 6.321 on 2 and 50 DF,  p-value: 0.00357
</code></pre>

<p>I got stuck because neither the F value and R squared are very significant. However the p-value is less than 0.05. Does it mean that y depends on both variables? What should I do next in my regression analysis?</p>
"
"0.0446990156267674","0.0220540545695615","202090","<p>There are now several different approaches to perform a network meta-analysis or mixed treatment comparison.</p>

<p>The most commonly used and accessible ones are probably the following:</p>

<ul>
<li><p><em>in a Bayesian framework</em>:</p>

<ul>
<li>design-by-treatment interaction approach in WinBUGS (eg <a href=""http://www.ncbi.nlm.nih.gov/pubmed/24777711"" rel=""nofollow"">Jackson et al</a>);</li>
<li>hierarchical arm-based Bayesian modeling in WinBUGS (eg <a href=""http://www.ncbi.nlm.nih.gov/pubmed/27037506"" rel=""nofollow"">Zhao et al</a>);</li>
<li>hierarchical contrast-based (i.e. node-splitting) Bayesian modeling, either with WinBUGS or through <code>gemtc</code> in R (eg <a href=""http://onlinelibrary.wiley.com/doi/10.1002/sim.3767/abstract"" rel=""nofollow"">Dias et al</a>);</li>
<li>integrated nested Laplace approximations (INLA) in WinBUGS (eg <a href=""http://www.ncbi.nlm.nih.gov/pubmed/26360927"" rel=""nofollow"">Sauter et al</a>);</li>
</ul></li>
<li><p><em>in a frequentist framework</em>:</p>

<ul>
<li>factorial analysis-of-variance in SAS (eg <a href=""http://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-61"" rel=""nofollow"">Piepho</a>);</li>
<li>multilevel network meta-analysis in SAS (eg <a href=""http://www.contemporaryclinicaltrials.com/article/S1551-7144%2815%2900060-9/abstract"" rel=""nofollow"">Greco et al</a>);</li>
<li>multivariate meta-regression with <code>mvmeta</code> in
Stata or R (eg <a href=""http://onlinelibrary.wiley.com/doi/10.1002/jrsm.1045/abstract"" rel=""nofollow"">White et al</a>);</li>
<li>network meta-analysis with <code>lme</code> and <code>netmeta</code> in R (eg <a href=""http://onlinelibrary.wiley.com/doi/10.1002/sim.1201/abstract"" rel=""nofollow"">Lumley</a>, which is however limited to two-arm trials, or <a href=""http://onlinelibrary.wiley.com/doi/10.1002/jrsm.1143/abstract;jsessionid=23A23C30DA4B95F1DC1CCB1F6EF19DFA.f01t03"" rel=""nofollow"">Rucker et al</a>).</li>
</ul></li>
</ul>

<p>My question is, simply: are they roughly equivalent or is there one which is preferable in most cases for the primary analysis (thus reserving the others for ancillary ones)?</p>
"
"0.0547448901451359","0.0540211804549215","204119","<p>There is a package in R called <code>pwr</code>. This is useful to make power analysis when designing the sampling of a project. here are few examples: </p>

<pre><code>library(pwr)
pwr.anova.test(k = 4, f = 0.5, sig.level = 0.05, power = .9)
pwr.2p.test(h = 0.5, sig.level =0.05, power = .9)
pwr.f2.test(u = 4, f2 = .5, sig.level = 0.05, power = .8)
</code></pre>

<p>However, is it possible to run a power analysis for a spline regression (or a generalized additive model (GAM))? I want to know how may organisms I would have to sample to detect an effect of selection, that is a shift in morphology of the beak of birds of only 0.5Â mm, given that my sig.level = 0.05 and that I have 4 species. </p>

<p>Also, Iâ€™m recapturing birds in a population each year since 2003. Is there a power calculation to estimate how many birds should I sample to get a probability of recapture of 25%? Iâ€™m running a recapture model in Bayesian statistics, so there is not a function in the package <code>pwr</code> that can do this. </p>
"
"0.0899550337218996","0.0986287303940589","204145","<p><strong>Background and Problem</strong></p>

<p>I have a question concerning a meta-analysis combining effects from between- and within-subject designs using log-odds ratios (OR) as the metric of interest. I am familiar with conducting meta-analyses and will be undertaking my calculations in R (using the <code>metafor</code> and <code>lme4</code> packages). To provide greater context, the studies in question ask research subjects to make a binary decision with respect to a personal preference across one of two conditions. In some cases, each participant is assigned to a single condition (making only a single binary response); in others, each subject takes part in both conditions (making two binary responses). For now, presume I have the raw data in all cases. The issue I face is how best to calculate an OR that is comparable across design and whether I should take the correlation between conditions into account for the within-subject designs.</p>

<p><strong>My Current Approach</strong></p>

<p>I presently use logistic regression to estimate the OR for between-subject designs. The slope represents the OR and the sampling variance can be calculated by squaring the SE of the slope coefficient. Using this approach produces estimates comparable to equations reported in common texts such The Handbook of Research Synthesis and Meta-Analysis, 2nd Edition (p. 243). I then extend this approach to use a multilevel logistic regression model including a random intercept by subject to estimate the OR for within-subject designs while account for the dependency between conditions. The OR and sampling variance are otherwise calculated in the same fashion. </p>

<p><strong>My Questions:</strong></p>

<p>With this in mind, I would like to ask:</p>

<ol>
<li>Is it reasonable to meta-analytically aggregate OR calculated using standard and multilevel logistic regression?</li>
<li>Would it be better to use standard logistic regression for both designs (ignoring the correlation between conditions for the within-subject designs)?</li>
</ol>
"
"0.104828483672192","0.103442685121077","204763","<p>Using linear regression as an equation for prediction is straightforward with,</p>

<p>$$ Y_i = \beta_0 + \beta_1 X_i. $$</p>

<p>Once the betas are estimated I can insert different values of $X$ to use as a what-if analysis for different scenarios. </p>

<p>But trying to do the same with ARIMA models is proving difficult to translate. For example with an ARIMA(2,1,1) model, how do I create an equation where I can try out different scenarios to see how the projection changes? </p>

<p>Below I have the output for a projection of sales based on past sales and extra regressors. I see that a unit change in <code>poc0_3_PER</code> results in a <code>135.2229</code> change in sales. But how do I account for the moving average and auto-regression components?</p>

<pre><code>arima(ts.count, order=c(2,1,1), xreg=df.back[3:4])

Call:
arima(x = ts.count, order = c(2, 1, 1), xreg = df.back[3:4])

Coefficients:
          ar1     ar2     ma1  poc0_3_PER
      -0.4569  0.2458  0.9455    135.2229
</code></pre>

<p>I have <code>ar1</code> and <code>ar2</code> estimates along with <code>ma1</code> and the extra regressors. How do I convert this into a working equation wherein I can try out different scenarios for the extra regressors to see how the prediction is affected?</p>

<p>I'm hoping that the solution is not an equation like <a href=""http://stats.stackexchange.com/questions/69407/how-do-i-write-a-mathematical-equation-for-arima-2-1-0-x-0-2-2-period-12?rq=1"">this post here</a>. I do have SARIMA models at times with orders like <code>SARIMA(2,0,1)(1,0,1)[12]</code>.</p>
"
"0.0836242010007091","0.058941940134654","204839","<p>Further to <a href=""http://stats.stackexchange.com/questions/200460/multiple-imputation-for-predictive-analysis-using-mice-package-in-r"">my prior question</a> on multivariable adjustment in regression models, using covariates which are available only for some cases, I have researched in some detail the main methods for limited dependent variables, including Heckman correction or tobit models. However, I fear that they do not apply to my issue, which has more to do with <strong>limited independent variables</strong>.</p>

<p>In particular, I am giving below an example of the dataset and the possible analysis in R (disregard the overfitting, it's just to make an example, my actual dataset has at least 10,000 cases):</p>

<pre><code>dep &lt;- c(8, 9, 21, -3, 4, 6, 9, 10, 8, 9, 11, 39, 91, 51, 38, 28, 21)
cov1 &lt;- c(68, 58, 42, 19, 39, 49, 29, 38, 25, 22, 19, 36, 39,90, 105, 73, 25)
cov2 &lt;- c(0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0)
cov3 &lt;- c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1)
cov4 &lt;- c(NA, NA, NA, NA, NA, NA, 56, 33, 45, 44, 56, 49, 36, 39, 40, 41, 59)
cov5 &lt;- c(NA, NA, NA, NA, NA, NA, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0)
mydata &lt;- data.frame(cbind(dep, cov1, cov2, cov3, cov4, cov5)) 
mydata

reg1 &lt;- lm(dep ~ cov1 + cov2, data = mydata, na.action = na.omit)
anova(reg1)
summary(reg1)

reg2 &lt;- lm(dep ~ cov1 + cov2 + cov3 + cov4 + cov5, data = mydata, na.action = na.omit)
anova(reg2)
summary(reg2)
</code></pre>

<p>What should I do to best adjust for covariates cov1, cov2, cov3, cov4 and cov5, having dep as dependent variable, given that cov4 and cov5 are available only for patients with cov3 = 1? </p>

<p>Should I discard all cases with cov3 = 0, or should I conduct two separate analyses and then pool the regression coefficients according to their standard error? Or is there any other more reasonable approach?</p>

<p>Unfortunately I did not find anything meaningful searching Google, Google Scholar, or PubMed:</p>

<p><a href=""https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable"" rel=""nofollow"">https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable</a></p>

<p><a href=""https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable"" rel=""nofollow"">https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable</a></p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable</a>*</p>

<p>To further clarify what is at stake, this is my real problem: I want to create a clinical prediction score (to predict prognosis and future quality of life) for patients undergoing myocardial perfusion imaging (a non-invasive cardiac test used in subjects with or at risk for coronary artery disease). The imaging test follows immediately an exercise stress test in fit patients, and a pharmacologic stress test in those who are not fit. The latter test is worse than the former, and does not provide several important prognostic features (eg maximum heart rate, or workload), so I must include exercise test variables in the multivariable model. But if I do so, I lose more than 1000 patients who only underwent a pharmacologic stress test.</p>
"
"0.0364965934300906","0.0540211804549215","205395","<p>I'm conducting regression analysis on sleeping time data. The data is survey data and the answer possibilities are of type ""less than 4 h"", ""5 h"", ""6 h"", etc. so they can be thought to be interval censored answers of individuals' real sleep times (E.g. if a person sleeps 6.8 hours a night, s/he might answer ""7 h"". Then if someone answered ""7 h"" we know that the real sleep length is somewhere between 6.5 and 7.5 hours). The data contains repeated measurements. Every individual has answered the same question on 4 different studies, which are a couple of years apart from each other. This introduces a grouping factor, frailty, in survival regression terms.</p>

<p>I have searched without result an R package, a Python module or something else that can model survival regression with Weibull distribution, interval censoring and frailty. Does someone know one?</p>
"
"0.0547448901451359","0.0540211804549215","205614","<p>I'm running some regression analyses and got pretty confused about R's output when it comes to robust regression models.
When I run a <code>OLS</code> -- using the command <code>lm()</code> -- I get this as output:</p>

<blockquote>
  <p>Coefficients:</p>

<pre><code>                Estimate Std. Error t value Pr(&gt;|t|)
</code></pre>
</blockquote>

<p>But when I run a robust linear model using the <code>rlm()</code> command, the output looks like so:</p>

<blockquote>
  <p>Coefficients:  </p>

<pre><code>                Value   Std. Error t value
</code></pre>
</blockquote>

<p>How do I get  the <code>p-values</code> and the significance-levels in an <code>rlm</code> then? Without that, the whole analysis is somewhat pointless.
Unfortunately, I couldn't find an answer to that anywhere, so I hope someone around here can help me out. Thanks a lot!!</p>
"
"0.070675349274022","0.0697410440814588","205817","<p>Thanks in advance for someone who can help.</p>

<p>I realize that, in regression analysis, if there is an interaction term included,  it is recommended to center the variables  (subtracting variable's mean).</p>

<p>My query is, should we </p>

<p>1) centering variable first, then create the interaction term, </p>

<p>or </p>

<p>2ï¼‰create the interaction term first, then centering all variables including the interaction termï¼Ÿ</p>

<p>these two operations can give different resultï¼Œ I am wondering which one is correct.</p>

<p>For example, we have 2 independent variables v1 &amp; v2, and an interaction term v1 * v2,</p>

<p>let:
v1 &lt;- c(3,   5,   4,  2,    9)
v2 &lt;- c(1,   0,   1,  1,    0)</p>

<p>when creating the interaction term, I am confusing which one is correct: </p>

<p>method 1</p>

<blockquote>
  <p>v1*v2 - mean(v1*v2)<br/> 
  [1]  1.2 -1.8  2.2  0.2 -1.8</p>
</blockquote>

<p>or </p>

<p>method 2</p>

<blockquote>
  <p>(v1-mean(v1)) * (v2-mean(v2)) <br/> 
  [1] -0.64 -0.24 -0.24 -1.04 -2.64  <br/> </p>
</blockquote>
"
"0.167621308600378","0.170918922914102","206042","<p>I implement <code>n</code> permutations into a regression analysis, to test the model for stability. Thus I obtain <code>n</code> odds ratios (ORs) and <code>n</code> associated 95% CI intervals. </p>

<p>Each permutation represents a matched-pair study. We pair similar <code>case</code>'s with <code>control</code>'s and then run a conditional logistic regression to obtain a measure of association between the outcome of interest and exposure variable (treatment status).</p>

<p>Taking the following example I have implemented into a <code>R</code> script.
In short what I have done is:</p>

<ol>
<li>Take a portion of the a given population</li>
<li>We assign at dummy variable to the population (1/0) to indicate treatment status</li>
<li>based on a set of parameters we pair those with treatment status==1 to equivalent treatment status==0</li>
<li>we define an outcome of interest that we wish to measure if treatment had an effect on the outcome</li>
<li>We conduct a logistic regression to determine the ORs associated with treatment status</li>
<li>we repeat this n time, each time obtaining an ORs and associated 95% confidence interval</li>
</ol>

<p><strong>But what I am not sure, is how I can report on the spread of my data. I generate a different odds ratio and 95% CI for each permutation.</strong></p>

<p>Taking the following hypothetical example, we run a simulation 100 times. It only takes a minute to simulate.</p>

<p>We take an worked exampled from the <a href=""https://cran.r-project.org/web/packages/Matching/Matching.pdf"" rel=""nofollow"">Matching package</a> in R. </p>

<pre><code>set.seed(123)    
# preamble, prepare the data for the simulation
    #1.
    library(Matching)
    library(survival)
    #2.
    require(doParallel)
    cl&lt;-makeCluster(2)
    registerDoParallel(cl)
    #3.
    clusterEvalQ(cl,library(Matching))
    clusterEvalQ(cl,library(survival))

    m &lt;- 100


    Result = foreach(i=1:m,.combine=cbind) %do%{

      # attach the data
      data(lalonde)

      # we want to assess if treatment is associated with greater odds for the outcome of interest
      # lets create our hypothetical outcome of interest
      lalonde$success &lt;- with(lalonde, ifelse(re78 &gt; 8125, 1, 0))

      # lets take a portion of the original population, say only 395
      n &lt;- sample(1:445,420, replace = F)
      n &lt;- sort(n, decreasing = F)
      lalonde &lt;- lalonde[n,]
      head(lalonde$age)

      # taking from the example from GenMatch (in Matching package)
      #The covariates we want to match on
      # but we only include some of the original variables (we come back to the others later)
      X = cbind(lalonde$age, lalonde$educ, lalonde$black, lalonde$hisp, 
                lalonde$married, lalonde$nodegr)

      #The covariates we want to obtain balance on
      BalanceMat &lt;- X

      # creat our matrix
      genout &lt;- GenMatch(Tr=lalonde$treat, X=X, BalanceMatrix=BalanceMat, 
                         pop.size=16, max.generations=10, wait.generations=1)


      # match our collisions on a 1-1 basis
      mout &lt;- Match(Y=NULL, Tr=lalonde$treat, X=X, Weight.matrix=genout, ties = F, replace = F)
      summary(mout)

      # here we create our case and control populations
      treat &lt;- lalonde[mout$index.treat,]
          control &lt;- lalonde[mout$index.control,]

      # and we want to apply a unique identifier for each pair
      # we call this command during the regression
      treat$Pair_ID &lt;- c(1:length(treat$age))
      control$Pair_ID &lt;- treat$Pair_ID 

      # finally we combine the data
      matched &lt;- rbind(treat, control)

      # now we run a conditional logitic regression on the paired data to determine the Odds Ratio associated with treatment
      # we account for the difference in pairs by the strata() command
      # we account for some of the original matching parameters that we removed from the matching process
      model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

      OR_M1 &lt;- exp(model_1$coefficients[1])
      CI_U1 &lt;- exp(confint(model_1))[1,2]
      CI_L1 &lt;- exp(confint(model_1))[1,1]

      Result &lt;- rbind(OR_M1, CI_U1, CI_L1)

    }
</code></pre>

<p>To summarise the script:</p>

<ul>
<li>we take 420 people from the original population (of 445)</li>
<li>we define the outcome of interest is. That is if the person had <code>re78 &gt; 8125</code> yes or no</li>
<li>for each treat==1, we find an equivalent treat==0 based on age, educ,  black, hisp, married, nodegr. We only want exact 1-1 matching</li>
<li>we assign an unique indicator variable for each pair 1,2,3.....x</li>
<li>We then develop a regression model to determine the OR for our outcome of interest (<code>re78 &gt; 8125</code>) associated with the treatment status (=1 relative to =0). </li>
<li>we save the ORs and 95%CI</li>
</ul>

<p>We can then plot the ORs and shade the 95%CI</p>

<pre><code>plot(Result[1,], ylim=c(0,2.5))
polygon(c(1:m,m:1), c(Result[3,],Result[2,]),col=adjustcolor(""grey"", alpha=0.4), border = NA)
</code></pre>

<p><strong>But how can I summarise the several ORs I obtained, the spread of it and/or an associated confidence level?</strong></p>

<p><strong>EDIT</strong>
Am I able to assess my study as if it was a meta-analysis. If so, one could implement the solution proposed by @Bernd Weiss <a href=""http://stats.stackexchange.com/questions/9483/how-to-calculate-confidence-intervals-for-pooled-odd-ratios-in-meta-analysis?rq=1"">here</a>?</p>

<p>For this we need to obtain the natural log of the ORs and the std. err.?</p>

<p>We update the last part of the command to:</p>

<pre><code>.......    
model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

  OR_M1 &lt;- exp(model_1$coefficients[1])

  l_OR_T2 &lt;- model_1$coefficients[1]
  s_e &lt;- coef(summary(model_1))[1,3]

  CI_U1 &lt;- exp(confint(model_1))[1,2]
  CI_L1 &lt;- exp(confint(model_1))[1,1]

  Result &lt;- rbind(OR_M1, l_OR_T2, s_e, CI_U1, CI_L1)
</code></pre>

<p>Using we can then call upon the <code>metagen()</code>, command</p>

<pre><code>library(meta)
or.fem &lt;- metagen(as.numeric(Result[2,]), as.numeric(Result[3,]), sm = ""OR"")
</code></pre>

<p>Where <code>as.numeric(Result[2,])</code> is the log(OR) and <code>as.numeric(Result[3,])</code> is the std. err. Thus we obtain a 95% CI ...... But have we introduced a bias in the CI by the imputations. We see our 95% range is significant (greater than 1), however for each permutation, we only get a lower 95% CI > 1 </p>

<pre><code>sum(as.numeric(Result[5,])&gt;1.00)
</code></pre>

<p>times. Therefore I think the large <code>n</code> and thus <code>degrees of freedom</code> in the meta-analysis are giving us a significant result </p>
"
"NaN","NaN","206576","<p><strong>Background</strong>:</p>

<p>The most recent statistics class I've taken in university goes over topics such as experimental design, tests for Normality, comparing two samples, categorical data analysis, linear regression, MLR, and power analysis. As a result, I am working towards a conceptual base in these topics.</p>

<p><strong>Question</strong>:</p>

<p>Are there any books that use R to reinforce these concepts and build on them towards advanced statistics such as advanced pattern recognition, if so which ones?</p>
"
"0.0774209661138764","0.0763974860547543","208149","<p>I have been struggling with this problem for several months. I would really appreciate if someone could help me solve this.
I am working on a pairwise relationship as shown in the data below (<code>new_cars</code>) which shows the theoretical efficiency of cars if combined together(i.e. hybrids). I have principal component analysis done based on their multiple specifications(including oil efficiency, engine power etc.) obtained from their practical use. I want to correct for any biases in the theoretical values for the hybrid cars using pca data. I want to fit the linear regression model and replace the values with <code>fitted.values</code>. The formula for fitted value is <code>fit1&lt;-lm(a_car~PCA1+PCA2+PCA3+PCA4)</code>, but the problem is I need to do this for pairwise cars. Since PCA is for point, I want to expand this to pairwise relationship and correct for the respective pairs using these prinicipal components. Any suggestion would be really appreciated. </p>

<p>Fitted.values can be obtained from fit1$fitted.values</p>

<pre><code>  new_cars&lt;-structure(list(mercedes = c(0.8, 0.7, 0.5, 0.3, 0.9), vw = c(0.7, 
0.6, 0.4, 0.2, 0.8), camry = c(0.5, 0.4, 0.3, 0.1, 0.6), civic = c(0.3, 
0.2, 0.1, 0.05, 0.4), ferari = c(0.9, 0.8, 0.6, 0.4, 0.93), PCA1 = c(0.021122, 
0.019087, 0.022184, 0.021464, 0.021122), PCA2 = c(0.023872, 0.024295, 
0.022471, 0.027509, 0.023872), PCA3 = c(0.000784, 0.001996, 0.003911, 
0.006119, 0.000784), PCA4 = c(-0.004811, -0.003296, 0.001868, 
-0.001636, -0.004811)), .Names = c(""mercedes"", ""vw"", ""camry"", 
""civic"", ""ferari"", ""PCA1"", ""PCA2"", ""PCA3"", ""PCA4""), row.names = c(""mercedes"", 
""vw"", ""camry"", ""civic"", ""ferari""), class = ""data.frame"")
</code></pre>
"
"0.0547448901451359","0.0540211804549215","208677","<p>I want to model patient visits. My assumptions are:</p>

<ol>
<li><p>Patients visit the hospital until they stop visiting at all. I don't know if their last visit was the last one.</p></li>
<li><p>Patients visit at certain intervals. These intervals vary across patients, but are roughly the same for a patient. </p></li>
</ol>

<p>I've been searching and reading all day long and my head is about to explode, so I decided to ask what are the ways to approach this. </p>

<p>My best guess is some sort of survival analysis and it looks like survival regression supports recurring events. The problem is that there are multiple ways to do this and I don't know which one to use. </p>

<p>What I'm trying to get out of the model:</p>

<ol>
<li>Probability the patient return at all, given time elapsed from his
last visit.</li>
<li>The number of visits a patient will make over a period T.</li>
</ol>

<p>Can I model this with one regression? Which one?</p>
"
"0.0547448901451359","0.0540211804549215","209055","<p>I am interested in panel data analysis with more than 20 variables in R using the package ""plm"". Right now, I am looking at adjusted R-square for the set of variables that best explain my dependent variable. My panel has data for around 65 days from 50 countries.</p>

<ol>
<li>Is there any other test statistic apart from adjusted R-square to judge the feature selection of the model?</li>
<li>Is it also possible to do panel data regression using LASSO methodology as I came to know that it can handle a large number of variables?</li>
</ol>
"
"0.073749613144785","0.0727746671810439","209243","<p>I am doing some regression analysis and in certain cases I'm getting errors I cannot understand.</p>

<p>R environment image is shared here: <a href=""https://www.dropbox.com/s/n6opew73l7xgcbz/data.RData?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/n6opew73l7xgcbz/data.RData?dl=0</a></p>

<p>And what I am doing with errors I get:</p>

<p>Something about infinite vector length:</p>

<pre><code>regsubsets(data$UzaverRekonstrukce ~  data$PacientRocnik + data$PacientIchs + data$PacientDm + data$PacientHyperlipidemie + data$PacientAh + data$PacientKoureni + data$Koncetina + data$UmisteniOperace + data$Material + data$BercoveTepny + data$PedalniOblouk + data$Antikoagulace + data$Antiagregace + data$Vykon + data$ChirRevProxAnas + data$ChirPlastProxAnas + data$TrombectomieBypassu + data$Eni + data$PtaDistAnas + data$Infekce + data$TerapieInfekce , nvmax = 5, really.big = T , nbest = 3, data)

Error in numeric(nbest * nvmax) : vector size cannot be infinite
In addition: Warning messages:
1: In cbind(1, xx) :
  number of rows of result is not a multiple of vector length (arg 1)
2: In leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in =     force.in,  :
  31  linear dependencies found
3: In max((1:np)[!rightorder]) :
  no non-missing arguments to max; returning -Inf
4: In leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in =     force.in,  :
  nvmax reduced to  -Inf
5: In leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in =     force.in,  :
  NAs introduced by coercion to integer range
</code></pre>

<p>This one is OK:</p>

<pre><code>regsubsets(data$UzaverRekonstrukce ~  data$PacientRocnik + data$PacientIchs + data$PacientDm + data$PacientHyperlipidemie + data$PacientAh + data$PacientKoureni + data$Koncetina + data$UmisteniOperace + data$Material + data$BercoveTepny + data$PedalniOblouk + data$Antikoagulace + data$Antiagregace + data$Vykon + data$ChirRevProxAnas + data$ChirPlastProxAnas + data$TrombectomieBypassu + data$Eni + data$PtaDistAnas + data$Infekce, nvmax = 5, really.big = T , nbest = 3, data)
</code></pre>

<p>And something about different input data length:</p>

<pre><code>regsubsets(data$UzaverRekonstrukce ~  data$PacientRocnik + data$PacientIchs + data$PacientDm + data$PacientHyperlipidemie + data$PacientAh + data$PacientKoureni + data$Koncetina + data$UmisteniOperace + data$Material + data$BercoveTepny + data$PedalniOblouk + data$Antikoagulace + data$Antiagregace + data$Vykon + data$ChirRevProxAnas + data$ChirPlastProxAnas + data$TrombectomieBypassu + data$Eni + data$PtaDistAnas + data$Infekce + data$PuvodceInfekce , nvmax = 5, really.big = T , nbest = 3, data)

Error in leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in = force.in,  : 
  y and x different lengths
</code></pre>

<p>I am very confused of this messages. I understand there are many variables and only few observations (in this dataset). But I don't understand that in one case it is OK, add one predictor - infinite length error, remove one variable - different X and Y errors.</p>

<p>I would be glad if someone could explain me the problem. Optimally how to detect problems leading to this errors before running the regsubsets.</p>
"
"0.0316069770620507","0.0311891430775903","209307","<p>I am working with 8 variables that are correlated with each other, and want to use factor analysis to construct two factors and then use them in my regression analysis. I typed this in R:</p>

<pre><code>fit.2 &lt;- factanal(mydata, factor = 2, rotation=""varimax"")
print(fit.2)
</code></pre>

<p>and it gives me that</p>

<pre><code>Loadings:
   Factor1 Factor2
V1 -0.174   0.341 
V2 -0.902  -0.124 
V3 -0.141  -0.882 
V4  0.855  -0.198 
V5  0.800         
V6  0.938  -0.121 
V7          0.931 
V8 -0.949         

               Factor1 Factor2
SS loadings      4.018   1.837
Proportion Var   0.502   0.230
Cumulative Var   0.502   0.732
</code></pre>

<p>What should I do next to create these two new variables?</p>
"
"0.134097046880302","0.124972975894182","209864","<p>This is a very simple exercise that I'm hoping may help people with limited knowledge in statistical analysis (like myself). 
I am having trouble deciding what statistical analysis I can perform (in R) to determine whether or not my data are closer to one linear model or another. </p>

<p>For example: I have measurements of sodium and chloride in various dilute solutions: </p>

<pre><code>#
Na &lt;- c(1.56, 1.00, 1.60, 3.23, 2.02, 2.81, 2.09, 26.24, 1.59, 0.42)
Cl &lt;- c(1.40, 0.91, 1.22, 2.67, 1.67, 3.01, 2.17, 27.42, 1.45, 0.51)
</code></pre>

<p>For simplicity, this solution is a dilution of either table salt dissolved in water or natural seawater. For each case, Cl/Na will be a specific ratio that reflects the composition of the original solution. We can visualize this by:</p>

<pre><code>plot(Na,Cl)
abline(0,1)    # expected slope for table salt dissolved in water
abline(0,1.16) # expected slope for natural seawater.
</code></pre>

<p>I want to know which model, table salt in water or seawater, is a more statistically accurate fit to the provided data. Linear regression analysis in R gives a line of best fit with a slope of 1.05 (<code>lm(Cl~Na)</code>), right in between the two models.</p>

<p>So, which solution do I more likely have and why? The line of best fit slope is closer to that of table salt dissolved in water, but that does not seem very statistically sound. Thoughts? </p>

<p>Edit: @whuber mentioned that there is one anomaly in the dataset - in reality, the provided data is just a subset of the original data. There are actually hundreds of data points in between the apparent outlier and the rest of the provided data.</p>

<p>Also, here is a <code>log(Na)-log(Cl)</code> summary of the complete dataset:</p>

<pre><code>    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's 
-0.46870 -0.06186  0.02654  0.02218  0.12780  0.47510      183 
</code></pre>

<p>Edit2: As for the ""true nature of my investigation"": The 'solution' in question is likely a mixture of both table salt water and natural seawater. What I'd like to do is find a definitive way (through statistical analysis) to show that I have more of one or the other. I had hoped that my simplified question/dataset would yield an answer from the community, but it seems I was off base. If it helps, a complete dataset is now hosted below:</p>

<p><a href=""http://www.filedropper.com/clna"" rel=""nofollow"">http://www.filedropper.com/clna</a></p>

<p>Looking at the distribution of the complete data shows I have more Cl/Na about 1.00, but this does not seem 'sound enough' to back up an argument. The probability that I have one solution or the other is unknown. I have the raw data and relevant models for Cl to Na to run with.</p>

<p>For clarification, the original question is still the one I'd like to solve.  An alternative question could be: Which solution do I have <em>more</em> of and what analysis did I use to come to that conclusion?</p>
"
"0.104828483672192","0.103442685121077","210015","<p>I have a statistical question.</p>

<p>I have data from an experiment with two conditions (dichotomous IV: 'condition'). I also want to make use of another IV which is metric ('hh'). My DV is also metric ('attention.hh'). I've already run a multiple regression model with an interaction of my IVs. Therefore, I centered the metric IV by doing this:</p>

<pre><code>hh.cen &lt;- as.numeric(scale(data$hh, scale = FALSE))
</code></pre>

<p>with these variables I ran the following analysis:</p>

<pre><code>model.hh &lt;- lm(attention.hh ~ hh.cen * condition, data = data)
summary(model.hh)
</code></pre>

<p>The results are as follows:</p>

<pre><code>Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)        0.04309    3.83335   0.011    0.991
hh.cen             4.97842    7.80610   0.638    0.525
condition          4.70662    5.63801   0.835    0.406
hh.cen:condition -13.83022   11.06636  -1.250    0.215
</code></pre>

<p>However, the theory behind my analysis tells me, that I should expect a quadratic relation of my metric IV (hh) and the DV (but only in one condition).</p>

<p>Looking at the plot, one could at least imply this relation:
<a href=""http://i.stack.imgur.com/k47jD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/k47jD.png"" alt=""enter image description here""></a></p>

<p>Of course I want to test this statistically. However, I'm struggling now if and when to center the metric IV, because the interaction is still important (as there is only a quadratic relation in one condition).</p>

<p>Do I first center the variable and then compute the quadratic term or the other way round?
Do I compute the interaction with both the linear and the quadratic term or only one of them?</p>

<p>My gut feeling would suggest something like this:</p>

<pre><code>hh.sqr &lt;- hh * hh

sqr.model.hh &lt;- lm(attention.hh ~ hh.sqr + hh.cen * condition, data = data)
    summary(sqr.model.hh)
</code></pre>

<p>In a nutshell, I want to test whether there is a quadratic relation in one of my conditions.
However, I am not sure which terms I have to include into the model (or whether I calculate hh.sqr * condition vs. hh.cen * condition -- or both)?!</p>
"
"0.0645174717615637","0.0763974860547543","210646","<p>I am replicating an analysis that models tree mortality data. Data are structured such that forest sites are revisted at some random interval, which is recorded. It is then determined if a tree lived or died over that random interval, generating 0 1 mortality data (if a tree dies, it gets a 1 in the dependent variable). The interval between initial and final observation varies continuously, from 5-15 years. This is relevant, as the more time that passes, the more likely a tree will die. </p>

<p>Here are some pseudo data for R:</p>

<pre><code>mort &lt;- c(0,1,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,1,0)
interval &lt;- runif(length(mort), 5, 15)
pollution &lt;- rnorm(length(mort), 25,5)
data&lt;- data.frame(mort, interval, pollution)
</code></pre>

<p>I am trying to replicate an analysis which uses a logistic regression model for binary mortality data using the the logit transformation. Authors then model how pollution affects tree mortality rates. In the manuscript the authors write, ""because recensus is not annual, we relate annual mortality probability, <code>pi</code>, of tree <code>i</code> to the observed binomial data on whether that tree lived or died <code>Mi</code> via a Bernoulli likelihood,</p>

<p><a href=""http://i.stack.imgur.com/7i7jA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7i7jA.png"" alt=""enter image description here""></a></p>

<p>where <code>ti</code> is the time interval between successive censuses.""</p>

<p>My question: How would I implement this using the <code>glm</code> function, or something analagous, in R? Note: I understand modeling this as a hazard function would also be appropriate, but it is not what I am interested in.</p>
"
"0.0899550337218996","0.0986287303940589","210900","<p>I have one outcome/dependent variable that can be ordinal or nominal and 3 independent variables. I have learned so far how to perform ordinal and multinomial logistic regression in SPSS between a single independent variable and the outcome variable.</p>

<p>However, I am more interested in examining the effect of the combinations of independent variables on the outcome variable. I could not find an answer online and I am new to regressions.</p>

<p>For example, the first independent variable X1 levels are friends and public, and he second independent variable X2 levels are location and time. So, I want to examine the effect on the outcome var. (what is the coefficient or estimate) in the following four cases that as a mix of X1 and X2 level:</p>

<ul>
<li>X1= friends and X2= location</li>
<li>X1= friends and X2= time</li>
<li>X1= public and X2= location </li>
<li>X1= public and X2= time</li>
</ul>

<p>I would really appreciate it if you have any thought of to perform such nested analysis whether ordinal or multinomial regression. The only way I thought of is to have the different combinations of X1 and X2 merged in one column and have another column for the outcome variable and run the analysis in SPSS but I am not sure if this the right way.
If there is no way to do that is SPSS, I would be happy to know to do it in R.</p>

<p>Many thanks.</p>
"
"0.0446990156267674","0.0441081091391231","212261","<p>I was wondering if there is a survival framework in R (or any other language for that matter) for doing the following:  </p>

<ol>
<li>Fitting an extended (i.e., time-varying covariates) cox survival model</li>
<li>Applying ridge regression to regularize the model parameters (or any other regularization is also fine - lasso, bayesian, etc.)</li>
<li>Specifying the X matrix [start stop status covariates...] as a sparse matrix</li>
</ol>

<p>The motivation is that I have been performing a survival analysis, but the number of covariates is starting to grow, and for many of these covariates, they will be 0 for all or most of the customer history. If I don't formulate as a sparse matrix, the data size can get up into the 10ish GB range.</p>
"
"0.0948209311861521","0.0935674292327708","212355","<p>I am using beta regression to model fulfillment ratio of a store (orders delivered/orders placed), which is between 0 and 1. I am using betareg package in RStudio to run this analysis. I have 24 predictors (many of these correlated with each other) with which i am trying to build the model. For certain variables i am getting this error:</p>

<pre><code>    Error in chol.default(K) : 
      the leading minor of order 18 is not positive definite
    In addition: Warning message:
    In sqrt(wpp) : NaNs produced
    Error in chol.default(K) : 
      the leading minor of order 18 is not positive definite
    In addition: Warning messages:
1: In betareg.fit(X, Y, Z, weights, offset, link, link.phi, type, control) :
  failed to invert the information matrix: iteration stopped prematurely
2: In sqrt(wpp) : NaNs produced
</code></pre>

<p>For a few other variables i am getting this error:</p>

<pre><code>Warning message:
In betareg.fit(X, Y, Z, weights, offset, link, link.phi, type, control) :
  optimization failed to converge
</code></pre>

<p>I do not what these errors mean and how to fix them. There are no missing data points in the dataset. Will be glad if someone could help me out here. Thanks.</p>

<hr>

<p>Thank you for letting me know my info is insufficient. Approximately i have 3,900 observations.</p>

<p>I am posting the summary of my dataset since i cannot post the actual data. Let me know if this provides insight into the data i am dealing with:</p>

<pre><code>   chain_id       timediff        ratio_store     fulfilled_store 
 0      :3781   Min.   :  0.000   Min.   :0.0000   Min.   :   0.0  
 11     :  37   1st Qu.:  0.000   1st Qu.:0.4138   1st Qu.:   8.0  
 3      :  25   Median :  7.771   Median :0.7349   Median :  39.0  
 13     :  18   Mean   : 31.179   Mean   :0.6331   Mean   : 134.7  
 2      :   9   3rd Qu.: 47.771   3rd Qu.:0.8889   3rd Qu.:  97.0  
 12     :   8   Max.   :153.771   Max.   :1.0000   Max.   :1294.0  
 (Other):  19                                                      
 pricecount_monthly store_pricecount  ratio_monthly    fulfilled_monthly
 Min.   :   0       Min.   :    2.0   Min.   :0.0000   Min.   :  0.00   
 1st Qu.:  16       1st Qu.:  199.0   1st Qu.:0.4000   1st Qu.:  2.00   
 Median :  79       Median :  479.0   Median :0.7727   Median : 11.00   
 Mean   : 193       Mean   :  829.7   Mean   :0.6347   Mean   : 36.53   
 3rd Qu.: 223       3rd Qu.: 1095.0   3rd Qu.:0.9130   3rd Qu.: 37.00   
 Max.   :3654       Max.   :13561.0   Max.   :1.0000   Max.   :379.00   

 pricecount_sp     allpricecount_product monthlypricecount_product
 Min.   :  1.000   Min.   :    1.0       Min.   :   0.0           
 1st Qu.:  1.000   1st Qu.:   26.0       1st Qu.:   4.0           
 Median :  2.000   Median :   80.0       Median :  15.0           
 Mean   :  4.027   Mean   :  533.2       Mean   : 115.9           
 3rd Qu.:  5.000   3rd Qu.:  375.0       3rd Qu.:  86.0           
 Max.   :116.000   Max.   :28907.0       Max.   :1412.0           

 ratio_product    total_fulfilledproduct monthlyratio_product
 Min.   :0.0000   Min.   :   0.00        Min.   :0.0000      
 1st Qu.:0.4167   1st Qu.:   2.00        1st Qu.:0.4000      
 Median :0.6667   Median :   6.00        Median :0.7692      
 Mean   :0.6130   Mean   :  70.67        Mean   :0.6442      
 3rd Qu.:0.8621   3rd Qu.:  40.00        3rd Qu.:1.0000      
 Max.   :1.0000   Max.   :1245.00        Max.   :1.0000      

 monthly_fulfilledproduct monthlyratio_sp  monthpricecount_sp
 Min.   :  0.00           Min.   :0.0000   Min.   : 0.000    
 1st Qu.:  1.00           1st Qu.:0.0000   1st Qu.: 0.000    
 Median :  2.00           Median :1.0000   Median : 1.000    
 Mean   : 23.55           Mean   :0.6174   Mean   : 1.374    
 3rd Qu.:  9.00           3rd Qu.:1.0000   3rd Qu.: 2.000    
 Max.   :414.00           Max.   :1.0000   Max.   :32.000    

 monthlystorecount_product storecount_product productcount_store
 Min.   :  0.00            Min.   :  1.0      Min.   :   1.0    
 1st Qu.:  3.00            1st Qu.: 13.0      1st Qu.: 111.0    
 Median :  8.00            Median : 37.0      Median : 213.0    
 Mean   : 45.92            Mean   :115.8      Mean   : 324.6    
 3rd Qu.: 37.00            3rd Qu.:131.0      3rd Qu.: 436.0    
 Max.   :330.00            Max.   :870.0      Max.   :2084.0    

 monthlyproductcount_store leafordersplaced_sp  leafratio_sp   
 Min.   :   0.00           Min.   :  1.00      Min.   :0.0000  
 1st Qu.:  12.00           1st Qu.:  3.00      1st Qu.:0.3571  
 Median :  50.00           Median : 11.00      Median :0.7692  
 Mean   :  96.38           Mean   : 46.99      Mean   :0.6345  
 3rd Qu.: 125.00           3rd Qu.: 39.00      3rd Qu.:0.9524  
 Max.   :1148.00           Max.   :620.00      Max.   :1.0000  

 leafmonthlycount_sp leafmonthlyfulfilled_sp averageprice_sp   
 Min.   :  1.00      Min.   :  0.00          Min.   :       5  
 1st Qu.:  2.00      1st Qu.:  1.00          1st Qu.:    1119  
 Median :  4.00      Median :  2.00          Median :    2253  
 Mean   : 14.89      Mean   : 12.31          Mean   :   10187  
 3rd Qu.: 13.00      3rd Qu.:  9.00          3rd Qu.:    8141  
 Max.   :268.00      Max.   :211.00          Max.   :14280251  

   dependent        
 Min.   :0.0001282  
 1st Qu.:0.0001282  
 Median :0.9998718  
 Mean   :0.6173614  
 3rd Qu.:0.9998718  
 Max.   :0.9998718  
</code></pre>
"
"0.0645174717615637","0.0763974860547543","212453","<p>In my research project I have to do a regression of the financial risk on the business risk of the year before.
As a reference, I have a paper showing the results for several countries. The paper states that, for the country I am interested in, the coefficient of the regression is negative. their year span :1995-2008</p>

<p>I have to perform the same analysis at region level for the country I am interested in. I find a negative lagged correlation coefficient. However, the coefficient in my regression is positive and significant. My year span:2000-20014.</p>

<p>Should I not have a negative coefficient as in the country-level regression (paper).
Thank you.</p>
"
"0.0893980312535348","0.0882162182782462","212623","<p>I am monitoring tree death caused by insects and potential impact of human treatment on yearly amount of tree mortality in areas with and without human intervention. My data are recorded by remote sensing, thus my results represents the area of trees killed per year. </p>

<p>Please, is there a way how to evaluate the effect of human intervention? My data contain one observation per year per category intervention/non-intervention. Is this the case of dynamic regression? </p>

<p>Dataset: two areas, differing by management or non-management approaches - one without chemical treatment (A), another with chemical treatment (B). Both contain the area of killed trees in yearly time step. </p>

<p>How can I decide if these two types of management differs, or if one potentially leads to decreased tree mortality? Is there a way how can I include in my analysis the amount of treatment applied each year and its impact in subsequent year? The insects develop within a year, and the treatment is applied by aerial treatment, not on specific tree thus I don't observe the immediate effect on chemical on the insect individual, as the treatment can stay on leafs of bark.</p>

<p>Thank you a lot for your suggestions !</p>

<p><a href=""http://i.stack.imgur.com/rSHAt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rSHAt.png"" alt=""enter image description here""></a></p>

<p>sample data:</p>

<pre><code>library(ggplot2)
library(gridExtra)

set.seed(100)
df&lt;-data.frame(year = c(2001:2010),
               A = floor(runif(10, min=0, max=200)),
               B = floor(runif(10, min=0, max=60)))

a&lt;- ggplot(df, aes(factor(year), y = A)) + ggtitle(""Non Management"") +
geom_bar(stat = ""identity"") + ylim(0,200) + ylab(""area in ha"") +
theme_classic() + theme(axis.text.x = element_text(angle = 90))

b &lt;- ggplot(df, aes(factor(year), y =B)) + ggtitle(""Management"") +
  geom_bar(stat = ""identity"") + ylim(0,200) + ylab(""area in ha"") +
  theme_classic() + theme(axis.text.x = element_text(angle = 90))

grid.arrange(a,b, ncol = 2)
</code></pre>
"
"0.070675349274022","0.0697410440814588","212652","<p>Say that I have 2 dependent variables, one continuous and one count:</p>

<ul>
<li>amount of adrenaline <strong>(continuous)</strong></li>
<li>number of remembered digits in a task <strong>(count)</strong></li>
</ul>

<p>and I want to check if these two variables differ between various groups, controlling by age and other covariates.</p>

<p>If I were doing two univariate regressions I would choose a poisson distribution for my count DV. In R:</p>

<pre><code>lm(adrenaline ~ groups + covariates)
glm(digits ~ groups + covariates, family=""poisson"")
</code></pre>

<p>But since I suppose that both variables are interacting and correlated, I would use a multivariate regression. In R:</p>

<pre><code>lm(cbind(adrenaline, digits) ~ groups + covariates)
</code></pre>

<p>Here I am conceptually stuck: it seems to me both wrong to use normal distribution of errors when they might not be, and to model both variables independently, as they are NOT independent.</p>

<p>So I see two choices: either embrace complexity and use some exotic (and <a href=""https://cran.r-project.org/web/packages/sabreR/index.html"" rel=""nofollow"">unmantained</a> ) package as <a href=""http://sabre.lancs.ac.uk/sabreR_coursebook5.pdf"" rel=""nofollow"">sabreR</a>. Or simplify the problem in a way that I cannot yet see.</p>

<p>As it seems to me a pretty common scenario, I really hope that I'm misunderstanding something and there's an easy way out.</p>

<p>I would like to know how to proceed with such an analysis. Examples in R would be welcome! :)</p>

<p>Thank you!</p>
"
"0.113960576459638","0.112454054604034","212840","<p>I read Chen et al. <a href=""http://onlinelibrary.wiley.com/doi/10.1002/for.1134/abstract"" rel=""nofollow"">""Forecasting volatility with support vector machine-based GARCH model""</a> (2010) where they implented a recurrent SVM procedure to estimate volatility by a GARCH based model. 
The model is of the form </p>

<p>$y_t = f(y_{t-1}) + u_t \qquad \qquad \ \ \ (1)$ </p>

<p>$u^2_t = g(u^2_{t-1}, w_{t-1}) + w_t \qquad  (2)$ </p>

<p>At first they got estimates for $u_t$ by estimating $(1)$ by a SVM. Then, the following recurrent SVM algorithm was proposed to estimate $(2)$.</p>

<hr>

<p><strong><em>Recurrent SVM Algorithm:</em></strong></p>

<p><strong>Step 1:</strong> Set $i = 1$ and start with all residuals at zero: $w_t^{(1)} = 0 $.</p>

<p><strong>Step 2:</strong> Run an SVM procedure to get the decision function $f^{(i)}$ to the points $\{x_t, y_t\} = \{u_{t - 1}^2, u_t^2 \}$ with all inputs $x_t = \{u_{t - 1}^2, w_{t-1} \}$</p>

<p><strong>Step 3:</strong> Compute the new residuals $w_t^{i+1} = u_t^2 - f^{(i)}$.</p>

<p><strong>Step 4:</strong> Terminate the computaion process if the stopping criterion is satisfied; otherwise, set $i = i + 1$ and go back to Step 2.</p>

<hr>

<p>The proposed stopping critrerion is based a Ljung-Box-Test for the residuals $w_t$. Only if the $p$-values of the test in five consecutive periods are higher than 0.1 the process is stopped. </p>

<p>As real world example the log-returns of the New York Stock Exchange (NYSE) composite stock index for the period from January 8, 2004 to December 31, 2007 was used. The last 60 observations where used as test sample. Hence, the estimation was done with the first 940 observations. In their study, the process converged after 121 interations. <strong>(Question:) However, my implementation in R does not converge. I think I have a misunderstanding of the concept.</strong> Because I think I implemented it exactly as stated. My R code is the following</p>

<pre><code>rm(list = ls())

library(quantmod)
library(e1071)

#Get NYSE data and convert to log returns
id     &lt;- ""^NYA""
data   &lt;- getSymbols(id, source = ""yahoo"", auto.assign = FALSE, 
                     from = ""2004-01-08"", to = ""2007-12-31"")
series &lt;- data[,6]  #Get adjusted closing prices
series &lt;- na.omit(diff(log(series)))*100  #Compute log returns

#Lagged data for analysis
x      &lt;- na.omit(cbind(series, lag(series)))

#Set parameters as in paper
svm_eps   &lt;- 0.05
svm_cost  &lt;- 0.005
sigma     &lt;- 0.02
svm_gamma &lt;- 1/(2*sigma^2)


#SVM to get u_t
svm     &lt;- svm(x = x[,-1], y = x[,1], scale = FALSE,
               type = ""eps-regression"", kernel = ""radial"",
               gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

u    &lt;- svm$residuals  #Extract u_t
n    &lt;- 60  #Size of test set
u_tr &lt;- u[1:(nrow(u) - n)]  #Subset to training set
u_tr &lt;- na.omit(cbind(u_tr, lag(u_tr)))^2  #Final training set


#Recurrent SVM for vola estimation
i       &lt;- 1
p_count &lt;- 0

while(p_count &lt; 5){

  print(i)  #Print number of loops

  #Estimate SVM for u^2
  svmr     &lt;- svm(x = u_tr[,-1], y = u_tr[,1], scale = FALSE,
                  type = ""eps-regression"", kernel = ""radial"",
                  gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

  #Test autocorrelation of residuals to lag 1
  test    &lt;- Box.test(svmr$residuals, lag = 1, type = ""Ljung-Box"")
  p_val   &lt;- test$p.value
  p_count &lt;- ifelse(p_val &gt; 0.1, p_count + 1, 0)

  #Extract residuals for next estimation step
  w        &lt;- svmr$residuals
  w        &lt;- c(0, w[-length(w)])  #lag 1

  u_tr &lt;- cbind(u_tr[,1:2], w)

  i &lt;- i + 1
}
</code></pre>
"
"0.0774209661138764","0.0636645717122953","213187","<p>I'm very new to R and stats in general but I think I need to do a repeated measures analysis perhaps using a linear mixed model on my data.</p>

<p>Data are behavioural measurements e.g. distance swum for 21 fish of 2 different genotypes. Each fish was tested for 10 minutes, and data taken for each minute. I've made graphs to show this (B and C).</p>

<p><img src=""http://i.stack.imgur.com/8oKg9.png"" alt=""""></p>

<p>My issue is that I can't do just a normal regression as each of the points is linked over time by the same fish. I've been taught to use linear mixed models such as:</p>

<pre><code>model &lt;- lmer(swimdurbot~1+start+(1+start|file), data=tdautodisc)
model2 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
model3 &lt;- lmer(swimdurbot~Genotype+(1+start|file), data=tdautodisc)
model4 &lt;- lmer(swimdurbot~Genotype+start+ Genotype*start +(1+start|file), data=tdautodisc)
model5 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
</code></pre>

<p>and to then perform anovas on them but I honestly can't work out what each model is showing, how they are different from each other and which part of the analysis that R comes up with tells me what I want to know.</p>

<p>I want to find out from my data: Does fish behaviour change over 10 mintutes (gradient)? Is the behaviour different between genotypes? and Does genotype affect the change in behaviour over time?</p>

<p>I would really appreciate some help on this as I can't get my head around it, and the graphs don't show any obvious trend so aren't useful for me to guess which part of the analysis relates to which aspect of the graphs.</p>
"
"0.0774209661138764","0.0763974860547543","213804","<p>I am running some linear regressions in R. I am dealing with a linear dependent and linear as well as categorical independent variables using <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html"" rel=""nofollow"">lm</a>. So far, I have looked at the output that <code>summary(model)</code> gives me. </p>

<p>Other studies instead run <a href=""http://www.inside-r.org/packages/cran/car/docs/Anova"" rel=""nofollow"">Anova()</a> from the <a href=""https://cran.r-project.org/web/packages/car/index.html"" rel=""nofollow"">car</a> package on their linear model, which returns a similar table. The docs for <code>Anova()</code> state that it</p>

<blockquote>
  <p>Calculates type-II or type-III analysis-of-variance tables for model objects. </p>
</blockquote>

<p>I am under the impression that this <code>Anova()</code> returns an F instead of the t-statistic but is ~ equivalent in what its tell me. (sample output below). So I was wondering</p>

<ul>
<li><p>Are standard R <code>summary(lm)</code> and car <code>Anova(lm)</code> indeed doing pretty much the same calculations here? If not, what is the difference?</p></li>
<li><p>They both report the same p-value, however the F-statistic at the bottom of the standard output is different from the <code>Anova()</code> one. Why is that?</p></li>
<li><p>What are applications where one would choose one over the other?</p></li>
</ul>

<p>Any help is much appreciated!</p>

<p>Sample output:</p>

<p>Standard R</p>

<pre><code>summary(linreg)
...
         Estimate    t value    Pr(&gt;|t|)
Age      -18.016     -3.917     0.000107
Gender   -45.4912    -4.916     1.35e-06
---
Residual standard error: 85.81 on 359 degrees of freedom
F-statistic: 16.71 on 2 and 359 DF, p-value: 1.147e-07
</code></pre>

<p>Anova() output</p>

<pre><code>Anova(linreg)

Anova Table (Type II tests)

           Sum Sq    F value   Pr (&gt;F)
Age        112997    15.345    0.0001072
Gender     1777936   24.164    1.348e-06
</code></pre>
"
"NaN","NaN","213857","<p>I am in the process of learning about Bayesian statistics with the help of R, and I would like to know the kind of analysis questions I should pose. 
Say for instance, with this <a href=""https://docs.google.com/spreadsheets/d/1Bjfr3pgiqEHWgIGiu6OKVKP8X4d0yDgKkpUKF2mx8l0/edit?usp=sharing"" rel=""nofollow"">dataset</a> of food production in various countries. So far, I've just created simple linear regression models. </p>
"
"NaN","NaN","214017","<p>I have done a multi-level meta analysis with R, using <a href=""https://cran.r-project.org/web/packages/metafor/metafor.pdf"" rel=""nofollow"">metafor package</a>. My questions relates to funnel plot asymmetry. </p>

<p>(1) If a regression test suggested funnel asymmetry (i.e. suggested a publication bias is present), Does this mean that my meta-analysis model is bad? and my results of meta-analysis are bad?</p>

<p>(2) What are other possible reasons, besides publication bias, that could lead to asymmetry in the funnel plots? </p>
"
"0.126647210942508","0.132324327417369","214449","<p>I am trying to determine if immigration status is a determinant of risk preferences. To do this, I am using the 2014 Health and Retirement Study data which has approximately ~20,000 participants and is representative of residents in the US over age 50. I have two different measures of risk preferences that I am using in this analysis.</p>

<p>The first risk measure is based on a 0-10 rating that participants gave themselves for how risky they are in general situations. The regression using this risk measure looks like this:</p>

<pre><code>fit1_usesRaw &lt;-vglm(Risk_Pct ~ is.native + is.male + oh + cjs + ms + age2 + tw,propodds, data = dummydata2)
</code></pre>

<p>Where the significant variables are ""is.native"", ""is.male"", and ""ms""(marital status). Is.native has a positive coefficient and is.male and ms each have negative coefficients. </p>

<p>The second risk measure is based on the percentage of the participant's retirement accounts kept in stocks. The regression for this risk measure looks like this:</p>

<pre><code>fit2_usesRaw &lt;-vglm(PCT_Stocks_MF_1 ~ is.native + is.male + oh + cjs + ms + age2 + tw + sme,propodds, data = dummydata2)
</code></pre>

<p>Where the significant variables are ""is.native"" (negative coefficient), ""is.male"" (positive coefficient), ""age2"" (positive coefficient), ""tw"" (total wealth, negative coefficient), and ""cjs"" (current job status, negative coefficient). </p>

<p>How can I test to see if the differences between the two risk measures are significant? Is there any way to determine which risk measure is ""right""? I tried a ANOVA test, but I'm unsure if that would be correct. The output for the Anova was:</p>

<pre><code> Anova(w1.mod, test = ""Roy"")

Type II MANOVA Tests: Roy test statistic
          Df test stat approx F num Df den Df    Pr(&gt;F)    
is.native  1  0.008895    7.441      2   1673 0.0006063 ***
is.male    1  0.048695   40.733      2   1673 &lt; 2.2e-16 ***
age2       1  0.020496   17.145      2   1673  4.26e-08 ***
oh         1  0.000329    0.275      2   1673 0.7596626    
ms         1  0.002546    2.130      2   1673 0.1191937    
tw         1  0.002674    2.236      2   1673 0.1071594    
sme        1  0.000688    0.576      2   1673 0.5624582    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Does this tell me there are significant differences between ""is.native"", ""is.male"" and ""age2"" using the two different risk measures? Or am I reading this output completely incorrectly?</p>

<p>Thanks for any help!</p>
"
"0.0547448901451359","0.0540211804549215","214613","<p>I am trying to make a simple linear regression to see if my variable ""totalssq"" has an influence on my variable ""hadsa"". (my data is ""dstatss"") Both are quantitative.
I made a model with lm() and tested it with an ANOVA.
Here are the outputs :</p>

<pre><code>    Analysis of Variance Table

    Response: dstatss$hadsa
             Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
      dstatss$totalssq  1  88.272  88.272  5.6848 0.03623 *
     Residuals        11 170.805  15.528                  
     ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The p-value is significant, but i don't know what it should mean to me ?
Does it means that there is a significant relationship between my variables ? I don't really know how to interpret this.</p>
"
"0.0316069770620507","0.0467837146163854","214665","<p>I have 15-minute streamflow observations for a small stream, but the dataset has some gaps in it. I want to fill the gaps with a regression using observations from a nearby stream (and quantify the uncertainty caused by these gaps). </p>

<p>If I use a linear relationship between the two streams, I get negative predictions for my target stream when the other has low flow, because my target stream dries out before the predictor stream. If I force the regression through zero (it has just a slope), then the model doesnâ€™t fit very well, which influences my results for the uncertainty analysis.</p>

<p>Is a truncated or a linear regression a good solution?  If so, how do I fit the two parameters?</p>

<p>Is there a better model?  It should be zero when my predictor stream has low flow, and then increase roughly in proportion to flow at the predictor stream. I'm working in R if anyone has suggestions for functions.</p>
"
"0.0774209661138764","0.0763974860547543","214882","<p>I am currently performing a retrospective study that is comparing a surgical procedure vs a modified version of the same procedure. There is obvious selection bias because of the selection criteria necessary to perform the modified procedure. I was wondering how I would control for these 3 variables (all are simple T/F requirements)? Should I just perform a logistic regression for each dependent variable we are investigating and hope none of them reach significance? Or is there a statistical test that automatically adjusts for these 3 selected covariates?</p>

<p>Initially I did not perform any statistical tests between these groups for this reason, but if we were to prove that these variables are not confounding then I could simply perform the appropriate two sample test, correct?</p>

<p>My Data:</p>

<ul>
<li>Independent Variable (2 groups) = Procedure 1, Procedure 2</li>
<li>We also have multiple dependent variables we want to compare between the two groups: Numerical and Logical (e.g. Length of Stay,
or Re-operation within 30 days, etc.)</li>
<li>But I have 3 Logical variables that I am afraid are confounding.</li>
</ul>

<p>PS. I'm using R to carry out this analysis and any reference to R functions would be a plus. </p>
"
"0.0836242010007091","0.0707303281615848","215256","<p>I wanted to do something equivalent to a PCA on a mixed data set containing categorical variables and continuous numerical predictor variables which are normally distributed but measured in very different units. The aims are (a) to explore/describe the variable relationships, and (b) hopefully to reduce the dimensions of the data set for predictive modelling. </p>

<p>Based on this <a href=""http://stats.stackexchange.com/questions/5774/can-principal-component-analysis-be-applied-to-datasets-containing-a-mix-of-cont/5777#5777"">cross validated post</a> I have been using (and liking!) the Factor Analysis of Mixed Data function FAMD() of the FactoMineR package in R.  </p>

<p>But I can't work out if this analysis can be treated the same way as a PCA. Two specific questions:</p>

<ol>
<li>I understand the <a href=""http://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia"">need to scale</a> (ie subtract mean and divide by sd) such numerical variables in a PCA to stop some variables unhelpfully dominating the components. But is this necessary in factor analysis of mixed data as done by the FactoMineR package? Running with both scaled and unscaled as supplementary variables seems to show no difference. 

<ol start=""2"">
<li>Can the principle component dimensions of a mixed data analysis be extracted (from a training set) and applied to a test set, as we might with a true PCA? E.g. if I extract the coordinates of each individual in the training set of Dimension1, then run a regression using the original variables predicting Dimension1, and use the coefficients as the weights of each variable to make a new composite 'Dimension 1 variable' which can be applied to the test set variables - would that be valid?</li>
</ol></li>
</ol>
"
"0.0899550337218996","0.0986287303940589","215447","<p>I have one outcome/dependent variable that can be ordinal or nominal and 3 independent variables. I have learned so far how to perform ordinal and multinomial logistic regression in SPSS between a single independent variable and the outcome variable.</p>

<p>However, I am more interested in examining the effect of the combinations of independent variables on the outcome variable. I could not find an answer online and I am new to regressions.</p>

<p>For example, the first independent variable X1 levels are friends and public, and the second independent variable X2 levels are location and time. So, I want to examine the effect on the outcome var. (what is the coefficient or estimate) in the following four cases that as a mix of X1 and X2 level:</p>

<pre><code>X1= friends and X2= location
X1= friends and X2= time
X1= public and X2= location
X1= public and X2= time
</code></pre>

<p>I would really appreciate it if you have any thought of to perform such nested analysis whether ordinal or multinomial regression. </p>

<p>The only way I thought of is to have the different combinations of X1 and X2 merged in one column and have another column for the outcome variable and run the analysis in SPSS but I am not sure if this the right way. If there is no way to do that is SPSS, I would be happy to know to do it in R.</p>

<p>Many thanks in advance.</p>
"
"0.0446990156267674","0.0441081091391231","215560","<p>I have three data sets that, when joined, have O(320) independent variables for a classification problem.  </p>

<p>Principal component analysis (PCA) seems out of the question because the data is mostly factors, not continuous.</p>

<p>I'm at a loss as to how to proceed.  </p>

<p>How do experienced analysts go about winnowing a large data set with hundreds of columns to something manageable?  How do you decide between variables?  What calculations can you go on to supplement your gut and experience?  How do you avoid throwing away significant variables?</p>

<p>A large number of columns might not be a problem for R, given enough CPU and RAM, but coming up with a cogent story should include identifying what is truly significant.  How to accomplish that?</p>

<p>Should I just toss all of it into a logistic regression and see what happens, without any forethought?</p>

<p>More detail in response to comments:</p>

<ol>
<li>Classification. </li>
<li>Many more observations than columns. </li>
<li>Yes, big oh notation meaning approximately. </li>
<li>Linear model at first. Also interested in boosted models in addition to logistic regression. </li>
</ol>
"
"NaN","NaN","216099","<p>I'm running a linear regression in R.</p>

<p>If i have an independent variable <code>Gender</code> with only values 0 (for Male) and 1 (for Male), do i need to convert them to factor or character?</p>

<p>What is going to be the impact on my analysis?</p>
"
"0.0836242010007091","0.0825187161885156","217380","<p>I am conducting a meta-analysis of cognitive-behavioural interventions on self-esteem and I could use some assistance navigating through the statistical procedures.  </p>

<p>I have extracted pre-post scores from single group designs and experiment/control scores from RCTs and calculated g for self-esteem rating scales and depression rating scales.  Based on the nature of the studies, there are four themes that I would like to explore and I would like to ensure that I use the correct statistical tools and set the correct hypotheses.  </p>

<p>The first task is to see if there is a difference between single-day workshops and multi-session therapy over several weeks.  The hypothesis is that the former will yield smaller effect sizes than the latter.  Second, we understand that single-group designs tend to have larger effect sizes than RCTs and we would like to see if that is the case.  Third, we would like to test to see if the three different CBT-based interventions that are used are significantly different.  We believe that they will not be.  Lastly, we would like to know if the level of self-esteem prior to treatment is a predictor of outcome.  It is unclear as to the direction that this might take, though. It could be argued that there is a ceiling effect, which limits the impact of an intervention on people with high self-esteem, or that low self-esteem is enduring enough to resist treatment compared to those with higher self-reported self-esteem. </p>

<p>Would it be as simple as creating dichotomous variables for each of the above and then using a meta-regression analysis of the four variables using R or would some of them be better suited to a Z-test between subgroups? I assume that as we would see in primary studies, multiple Z-tests would lead to a rise in the overall alpha. I am mindful that the third variable (intervention) is an a priori acknowledgment of the null hypothesis and the fourth one might be too vague for inclusion, so I would appreciate any guidance on how to proceed correctly.</p>
"
"0.0316069770620507","0.0311891430775903","217688","<p>I have survey responses that are between 1 and 10. The questions are related to a category. For example, out of 20 questions, 5 are related to category A, 6 are for category B, and 9 are for category C. How can I analyze this data to determine which category is the most important driver of the overall quality score. </p>

<p>Regression seems like a good initial run but I would think that the categories would be lost in the analysis. FWIW, I am using R for this analysis.</p>
"
"0.0446990156267674","0.0441081091391231","217972","<p>Hoping for some help related to a survival analysis using R and the <code>survival</code> package. I've been relying heavily on a series of blog posts done by Dayne Batten, particularly this portion:
[<a href=""http://daynebatten.com/2015/12/survival-analysis-customer-churn-time-varying-covariates/]"" rel=""nofollow"">http://daynebatten.com/2015/12/survival-analysis-customer-churn-time-varying-covariates/]</a></p>

<p>I've collected and merged the data as instructed using the <code>tmerge</code> function. My model relies on a cumulative time-dependent covariate, which strays away from the example provided. So my first question is does a cumulative covariate affect the validity of the Cox Regression? Here is my code at the moment:</p>

<pre><code>fit &lt;- coxph( Surv(tstart, tstop, had_event) ~ review_event, data = newdatatestcum)
</code></pre>

<p>My second question pertains to the lack of an ID being assigned within this model. For each customer ID within this data I have up to a few hundred lines of events with my covariate. I don't see how this regression could possibly be accounting for that.</p>
"
"0.0632139541241014","0.0831710482069074","218399","<p>I am working on a project where the explanatory variables include soil attributes, land use and land cover properties, stream flow and climate (precipitation, temperature etc) measurements recorded at multiple locations across a study area. I am proposing to use a random forest regression model to predict the response (an eco-indicator) at these locations.</p>

<ol>
<li>Soil attributes and land-use/land-cover data are available as single measurements at each location.</li>
<li>Stream flow and climate data are available as a time series. From these time series data I was hoping to extract long-term representative values such as mean daily stream flow, average annual precipitation total, average annual temperature and so on. </li>
</ol>

<p>However during exploratory data analysis I noticed that at several locations in the study area the annual time series of stream flow and climate variables are exhibiting non-stationarity. I am aware that it is possible stationarize these series using techniques such as detrending or differencing. However, I would like to know if random forests can handle non stationary inputs (without stationarizing)? Are there any best practices when dealing with such data sets?</p>

<p>Thanks!</p>
"
"0.070675349274022","0.0697410440814588","218477","<p>I have a database which contains 100k records. It includes 2 continuous and 6 categorical variables. The output is categorical as well and it can take one of 8 different values (e.g. 1, 2, 3...8). My goal is:</p>

<ol>
<li><p>Investigate which of the variables are the most significant ones to determine my output.</p></li>
<li><p>After the analysis, to be able to calculate the probability for any of those outputs to happen if I only know what are the values for (e.g.) two variables? For example, to have some coefficients for every possible categorical value in order to calculate the probability...</p></li>
</ol>

<p>I tried this with the logistic regression but somehow I have big deviation from manually calculated probability (e.g. when I use the number of positive outputs and the total number of the records contained within my database). Anybody has a better idea how I could analyse this? Sth better than logistic regression?
Thanks in advance!</p>
"
"0.0632139541241014","0.0623782861551805","218671","<p>I tried looking this question up on google and didn't find material that answered my question. But my questions are:</p>

<p>(1)  Is there a method to determine how long it takes a leading indicator to affect a variable ? So if we are looking at the affects of oil production on sales, when oil drops how long does it take to affect sales.</p>

<p>Could I use survival analysis for this? <a href=""http://stats.stackexchange.com/questions/30061/identifying-the-time-lag-between-cause-and-effect?rq=1"">This</a> seems related but in a biological context </p>

<p>(2) Can we measure the degree to which oil production affects sales? If oil production drops by 10% it affects sales by 17%.</p>

<p>(3) What's the best way to determine the most important leading indicator? Univariate regression and compare models?</p>

<p>(4) Is there a package in R that could be used for this?</p>
"
"0.104828483672192","0.103442685121077","218738","<p>I want to build a linear regression model where I predict a mean of a group of participants (how they rate something on average). Predictors should be </p>

<ol>
<li>age (continuous)</li>
<li>origin (deviation coded, each level compared to grand mean, levels=1,2,3,4)</li>
<li>education (Helmert coded, each level compared to subsequent ones, haven't decided on number of levels yet)</li>
<li>gender/sex (dummy coded, 0/1)</li>
</ol>

<p>Following questions:</p>

<p><strong>1.</strong> In R, I use the following code for the coding, for example for 4) sex:</p>

<pre><code>    data$sex &lt;- factor(data$sex, labels=c(""1"",""2"")) 
    contr.treatment(2) 
    contrasts(data$sex) = contr.treatment(2)  
</code></pre>

<p>That gives me the right (dummy) coding, for the other 2 and 3 a little differently. Can I use run this kind of code for each predictor (except age) and then throw all predictors into a model like this:</p>

<pre><code>    model &lt;- lm(Mean ~ age +  sex + educ..., data)
</code></pre>

<p>It seems wrong because: what is the common intercept going to be with these different coding systems? It's different for each coding system.
But then, how am I going to enter these different predictors into a model?</p>

<p><strong>2.</strong> Can I leave age in there as it is, unchanged, continuous?</p>

<p><strong>3. Quite a different question:</strong> This is my <em>participant analysis</em>. For the <em>item analysis</em>, I used a logistic regression based on medians instead of means. That's because I did four rating surveys with Likert scales. 4 surveys - 4 participant groups - each group rated the items on <strong>one</strong> property only, such that each item was rated on 4 properties by different people.</p>

<p>Given this, is it okay if I use linear models and means in this analysis now? And can I even build my model as I suggested above?</p>

<p><strong>Many thanks</strong> for any input! I've been trying some things, but confusion isn't fading yet...</p>
"
"0.0716778865720364","0.0707303281615848","218900","<p>My situation is a bit too complex for me, given I'm a newbie in R and analysis. I have roughly nine different score types, each of which was a model itself built upon various variables for each score type. I do not have the different variables, only their resulting scores. Each person has nine scores, one from each score type. I have no additional information on these people. The dependent variable is a continuous variable from 0 to 100. </p>

<p><strong>BACKGROUND OF THE DATA:</strong> Each score should represent the risk of a person. Generally speaking, the higher the risk, the lower their score, and vice versa. One such outcome variable that I'm looking to predict is what percentage of a loan the person has paid relative to how much they have been charged to date (i.e someone has been charged 500 dollars out of a 1000 dollar loan, and they have paid $400 of it, so their outcome is 0.8, or 80 percent.)</p>

<p>My goal is to see, by combining these different score types together, whether the predictive power of some outcome variable can be increased versus using just a single score type.</p>

<p>Now, I've already partitioned my dataset into a 70/30 training/testing data frames and I know that I need to perform some sort of regression analysis on it. Past this, I'm completely lost on how to go about getting to my goal. Any assistance is greatly appreciated. I will edit as needed for additional information if this is too vague.</p>

<p>Edit: Here is a sample of the data</p>

<p>Only scoretypes 8 and 9 are categorical. The rest are continuous.</p>

<pre><code>+--------+------------+------------+------------+------------+------------+------------+------------+------------+------------+
| userID | scoretype1 | scoretype2 | scoretype3 | scoretype4 | scoretype5 | scoretype6 | scoretype7 | scoretype8 | scoretype9 |
+--------+------------+------------+------------+------------+------------+------------+------------+------------+------------+
|     55 |        702 |        794 |        611 |        642 |        799 |        750 |        902 | A          |         50 |
|   1239 |        731 |        792 |        613 |        689 |        887 |        645 |        872 | B          |         50 |
|   1248 |        716 |        831 |        579 |        674 |        792 |        726 |        839 | A          |         50 |
|   1371 |        812 |        857 |        658 |        823 |        843 |        715 |        964 | A          |         50 |
|   1509 |        710 |        708 |        593 |        595 |        811 |        525 |        831 | A          |         50 |
|   1613 |        709 |        756 |        601 |        619 |        886 |        465 |        853 | B          |         50 |
+--------+------------+------------+------------+------------+------------+------------+------------+------------+------------+
</code></pre>
"
"0.0952986215201744","0.103442685121077","219304","<p>I am examining social interaction data in individuals within two groups. Each social encounter has been coded to one of 4 categories, and these encounters are nested within individual, whom are nested within groups. The number of social encounters per individual is variable and my groups are unequal sample sizes. </p>

<p>I want to examine whether the proportion of social encounters in different categories significantly differ as a function of group. I previously examined a different DV in this data that was continuous, not categorical, and used a multilevel model in R (nlme package) to do so (data nested within individuals within groups). I have done some looking online and as far as I can tell, R should be able to run a multilevel model with categorical dependent variable as well. (i.e., <a href=""http://www.upa.pdx.edu/IOA/newsom/mlrclass/ho_binary.pdf"" rel=""nofollow"">http://www.upa.pdx.edu/IOA/newsom/mlrclass/ho_binary.pdf</a>). However, I am not sure how to implement this and I think that the naming online is inconsistent (some sources referring to this analysis as MLM with categorical variable, others calling it a multinomial logistic regression). </p>

<p>Is it possible to modify my current R script for continuous DV so that it analyzes for a categorical DV instead? Or do I need a different script? Thank you in advance for any help.</p>
"
"0.0565402794192176","0.0557928352651671","219390","<p><a href=""http://i.stack.imgur.com/fUmBg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fUmBg.png"" alt=""enter image description here""></a></p>

<p>This is a graph of revenues for different products with the Y-axis showing normalized revenues (mean of 3 and SD of 1) and X-axis is weeks. I need do a regression analysis of sorts on this data and am unsure how to find a curve/function in R that fits this data. </p>

<p>The data points can be interpreted as being: Week 0 of product release yield normalized revenues between 2.25 to 3.25, etc.</p>

<p>Any help regarding what kind of statistical analysis I can use to create a regression model (linear and logistic wouldn't work clearly) with the end goal being to do predictive analysis (ie. if a new product is released, what normalized revenues would it yield in the first 6 weeks)</p>

<p>Thanks</p>
"
"0.070675349274022","0.0697410440814588","220429","<p>I'm running a meta-regression/multi-level analysis that contains only categorical variables. </p>

<p>The printout of the data is as follows:</p>

<blockquote>
  <p>res.fe</p>
</blockquote>

<pre><code>Multivariate Meta-Analysis Model (k = 19; method: REML)

Variance Components: none

Test for Residual Heterogeneity: 
QE(df = 7) = 5.9504, p-val = 0.5456

Test of Moderators (coefficient(s) 2,3,4,5,6,7,8,9,10,11,12): 
QM(df = 11) = 39.3316, p-val &lt; .0001

Model Results:

                             estimate      se     zval    pval    ci.lb    ci.ub     
intrcpt                        0.8995  0.0807  11.1477  &lt;.0001   0.7414   1.0577  ***
as.factor(Z)2          -0.1090  0.0825  -1.3213  0.1864  -0.2708   0.0527     
as.factor(Z)3          -0.1299  0.1785  -0.7276  0.4668  -0.4797   0.2199     
as.factor(Z)4          -0.2180  0.2015  -1.0820  0.2793  -0.6128   0.1769     
as.factor(Z)5           0.4280  0.1510   2.8352  0.0046   0.1321   0.7240   **
as.factor(W)1            0.1059  0.1091   0.9707  0.3317  -0.1079   0.3196     
as.factor(W)2            0.1215  0.1584   0.7673  0.4429  -0.1889   0.4319     
as.factor(W)3            0.3696  0.1141   3.2381  0.0012   0.1459   0.5933   **
as.factor(U)2   -0.0575  0.1289  -0.4463  0.6554  -0.3101   0.1950     
as.factor(V)3             -0.1709  0.0981  -1.7417  0.0816  -0.3632   0.0214    .
as.factor(V)4             -0.2247  0.2495  -0.9007  0.3677  -0.7138   0.2643     
as.factor(V)5             -0.4078  0.1181  -3.4542  0.0006  -0.6391  -0.1764  ***

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Variable Z has levels 1, 2, 3, 4, and 5, but the printout only shows levels 2-5; variable V has levels 0, 3, 4, and 5, but only prints levels 3-5;variable U has levels 1 and 2, but the printout only shows level 2.</p>

<p>My understanding is that the intercept is the value when all other variables are explanatory variables are equal to zero. </p>

<p>Does this mean that the intercept is equal to the omitted variables with their respective levels? (i.e. intercept = Z(level=1) + U(level=1)+ V(level=0))</p>

<p>If so, how would I obtain estimates for each of the omitted variables at specified levels?</p>

<p>Thanks for any information that you can provide me with. </p>
"
"0.126427908248203","0.124756572310361","220868","<p>The goal of this regression is to determine whether the amount of leaf disk that an insect consumed varied by what tree the leaf material came from. I'll acknowledge upfront that my coding is rarely pretty/efficient, but hopefully it works (usually).</p>

<ul>
<li>Variables:

<ul>
<li>Response: pctrans; the percent of a 7 mm diameter leaf disk that was consumed.  Values have been transformed to fit (0,1).</li>
<li>Explanatory: tree; a categorical (factor) variable of six tree types.</li>
</ul></li>
</ul>

<p>When I use betareg(), which as I understand it, is best suited to data of this sort, I get no significance:</p>

<pre><code>model.beta &lt;- betareg(pctrans ~ tree, data=BT.data, link=""logit"")
modelnull.beta &lt;- betareg(pctrans ~ tree, data=BT.data, link=""logit"")
lrtest(model1.beta, modelnull.beta)
</code></pre>

<p>Results:</p>

<pre><code>Call:
betareg(formula = pctrans ~ tree, data = BT.data, link = ""logit"")

Standardized weighted residuals 2:
    Min      1Q  Median      3Q     Max 
-2.7716 -0.5800  0.0472  0.5351  3.5109 

Coefficients (mean model with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.111504   0.069191 -16.064  &lt; 2e-16 ***
treeBC3F3   -0.050940   0.095889  -0.531  0.59525    
treeD54     -0.279927   0.096470  -2.902  0.00371 ** 
treeD58     -0.034000   0.095716  -0.355  0.72242    
treeEllis1  -0.006764   0.095175  -0.071  0.94334    
treeQing     0.785992   0.094003   8.361  &lt; 2e-16 ***

Phi coefficients (precision model with identity link):
      Estimate Std. Error z value Pr(&gt;|z|)    
(phi)   3.5549     0.1352   26.29   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Type of estimator: ML (maximum likelihood)
Log-likelihood: 529.8 on 7 Df
Pseudo R-squared: 0.1105
Number of iterations: 20 (BFGS) + 2 (Fisher scoring) 

Likelihood ratio test

Model 1: pctrans ~ tree
Model 2: pctrans ~ 1
  #Df LogLik Df  Chisq Pr(&gt;Chisq)    
1   7 529.82                         
2   2 460.70 -5 138.25  &lt; 2.2e-16 ***
</code></pre>

<p>As I've been told, since the model is significantly worse than the null, no comparisons can be made between treatment means.</p>

<p>HOWEVER...
If I run the same model using glm the model is significantly better than the null.</p>

<pre><code>beta.glm &lt;- glm(pctrans ~ tree, data=BT.data, family=quasibinomial)
</code></pre>

<p>Results:</p>

<pre><code>Call:
glm(formula = pctrans ~ tree, family = quasibinomial, data = BT.data)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.94474  -0.38492  -0.08785   0.22725   1.80291  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.22601    0.07643 -16.042  &lt; 2e-16 ***
treeBC3F3    0.06826    0.10660   0.640  0.52205    
treeD54     -0.33864    0.11312  -2.994  0.00281 ** 
treeD58     -0.19878    0.11062  -1.797  0.07260 .  
treeEllis1  -0.07763    0.10808  -0.718  0.47276    
treeQing     0.88596    0.09978   8.879  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasibinomial family taken to be 0.2069603)

    Null deviance: 307.54  on 1240  degrees of freedom
Residual deviance: 267.59  on 1235  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 4

Analysis of Deviance Table
Model: quasibinomial, link: logit
Response: pctrans
Terms added sequentially (first to last)

     Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                  1240     307.54              
tree  5   39.951      1235     267.59 &lt; 2.2e-16 ***
</code></pre>

<p>Where do I go from here?</p>
"
"0.104828483672192","0.0940388046555249","221046","<p>I have a small data set of cases from an outbreak. For each case I have collected data for a few variables (age, HIV status, hospitalisation, country of infection etc. ). There are only 28 cases and so I imagine that it won't be possible to do any meaningful inferential statistical analysis. I would however like to present the data in as scientific manner as possible. </p>

<p>I would like to, for example, tabulate the data showing the difference between the cases that are HIV positive and HIV negative. I've included some R script and the results of what I've managed to do so far: </p>

<pre><code>data &lt;- read.csv(""Shigella.csv"")
library(dplyr)
library(epiR)
data$Hospitalised &lt;- factor(data$Hospitalised, levels = c(""Yes"", ""No""), labels = c(""Hospitalised"", ""Not Hospitalised""))
data$HIV &lt;- factor(data$HIV, levels = c(""Positive"", ""Negative""))
ttab &lt;- table(data$Hospitalised, data$HIV)
ttab


                   Positive Negative
  Hospitalised            2        1
  Not Hospitalised       13       11


Odds_Ratio &lt;- epi.2by2(ttab, method = ""case.control"", conf.level = 0.95)
Odds_Ratio

&gt; Odds_Ratio
             Outcome +    Outcome -      Total        Prevalence *        Odds
Exposed +            2            1          3                66.7        2.00
Exposed -           13           11         24                54.2        1.18
Total               15           12         27                55.6        1.25
Point estimates and 95 % CIs:
-------------------------------------------------------------------
Odds ratio (W)                               1.69 (0.13, 21.27)
Attrib prevalence *                          12.50 (-44.45, 69.45)
Attrib prevalence in population *            1.39 (-25.97, 28.75)
Attrib fraction (est) in exposed  (%)        39.79 (-1206.69, 99.08)
Attrib fraction (est) in population (%)      5.45 (-22.83, 27.23)
-------------------------------------------------------------------
 X2 test statistic: 0.169 p-value: 0.681
 Wald confidence limits
 * Outcomes per 100 population units
</code></pre>

<p>From the above, my interpreting is that the odds of a hospitalised case being HIV postive are 1.69 that of a non-hospitalised case being HIV positive. The 95% CI is (0.13, 21.27). </p>

<p>I am not sure that the p value is however. I see a p value of 0.681 but that seems to be for the X2 value. </p>

<p>Here is what I'm stuck with: 
1) am I using the right statistical test (or should I be doing logistic regression or something else)? 
2) how do I get a p value for the odds ratio (or is the p value provided above actually for the odds ratio)? </p>

<p>With appreciation! </p>

<p>Greg</p>
"
"NaN","NaN","221052","<p>I am performing regression analysis on prices of product that we have purchased, based on size and other attributes.</p>

<p>However there are often buys in odd circumstances which factor into the price, that is not (and cannot be) addressed directly in the features of the analysis.</p>

<p>Each time I run a regression, I will check the 20 with the largest error manually, and 90%+ of the time they will be odd buys like mentioned before, and for my purposes can be completely ignored.</p>

<p>I have been looking into cooks distance to remove these, however I'm not sure how to best set the threshold, or if there is a better method to use.</p>
"
"0.0547448901451359","0.0540211804549215","221205","<p>I am performing an hierarchical regression. One of the binary factor variables is if someone is 65 or older (1) or if they are  younger than 65 (0). However, I am also applying stratification for age (and gender).</p>

<p>I received feedback with the question why I did this. So now I am questioning my own approach... Is it redundant in your opinion to have two age variables, one normal variable and one in stratification, in my analysis? </p>

<p>An example of my strata information:</p>

<blockquote>
  <p>strata[1:5,2:7]</p>
</blockquote>

<pre><code>      agegrp_1   agegrp_2  agegrp_3  agegrp_4   agegrp_5   agegrp_6
[1,] 0.3408521 0.10025063 0.1378446 0.2731830 0.13784461 0.01002506
[2,] 0.3471074 0.23966942 0.1157025 0.1900826 0.10743802 0.00000000
[3,] 0.3646209 0.13357401 0.1516245 0.2527076 0.09747292 0.00000000
[4,] 0.3934426 0.26229508 0.1147541 0.2131148 0.01639344 0.00000000
[5,] 0.2844828 0.08189655 0.1724138 0.3146552 0.13362069 0.01293103
</code></pre>

<p>Where I make use of six age ranges (15-30, 31-45, 46-60, 61-75, 76-90 and 91-105).</p>

<p>Help is appreciated! </p>
"
"0.123269343248046","0.135950322810847","221510","<p>I'm new to logistic regression analysis, and was unable to find an answer elsewhere in Cross Validated or Stack Overflow. </p>

<p>Consider a standard logistic regression analysis of a binary outcome (admission to college) based on continuous covariates gre score and high school gpa, and ordinal categorical rank prestige of the undergraduate institution (data from the nice UCLA stats dept. logistic regression in R tutorial: <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a>)</p>

<pre><code>&gt; admissions.data &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; admissions.data$rank &lt;- as.factor(admissions.data$rank)
&gt; summary(admissions.data)
     admit             gre             gpa        rank
 Min.   :0.0000   Min.   :220.0   Min.   :2.260   1: 61
 1st Qu.:0.0000   1st Qu.:520.0   1st Qu.:3.130   2:151
 Median :0.0000   Median :580.0   Median :3.395   3:121
 Mean   :0.3175   Mean   :587.7   Mean   :3.390   4: 67
 3rd Qu.:1.0000   3rd Qu.:660.0   3rd Qu.:3.670
 Max.   :1.0000   Max.   :800.0   Max.   :4.000

&gt; fit1 &lt;- glm(admit ~ gre + gpa + rank, data = admissions.data, family=""binomial"")
&gt; summary(fit1)

Call:
glm(formula = admit ~ gre + gpa + rank, family = ""binomial"",
    data = admissions.data)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6268  -0.8662  -0.6388   1.1490   2.0790

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *
gpa          0.804038   0.331819   2.423 0.015388 *
rank2       -0.675443   0.316490  -2.134 0.032829 *
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4

# Odds Ratios
&gt; exp(coef(fit1))
(Intercept)         gre         gpa       rank2       rank3       rank4
  0.0185001   1.0022670   2.2345448   0.5089310   0.2617923   0.2119375

# 95% confidence intervals
&gt; exp(confint(fit1))
Waiting for profiling to be done...
                  2.5 %    97.5 %
(Intercept) 0.001889165 0.1665354
gre         1.000137602 1.0044457
gpa         1.173858216 4.3238349
rank2       0.272289674 0.9448343
rank3       0.131641717 0.5115181
rank4       0.090715546 0.4706961
</code></pre>

<p>My questions are:</p>

<p>1) In R, is there a straight-forward way to determine ORs with 95% CIs for specific values of the covariates? E.g., based on this model, what are the odds of college acceptance for students applying to a rank 2 schools with a gpa of 3 and a gre score of 750, compared with a student applying to a rank 3 school with the same gpa and gre score? I could calculate ORs by hand given the model coefficient estimates and these specific covariate values, but am unsure how to correctly propagate SEs to calculate 95% CIs.</p>

<p>2) Would this particular example be considered a case-control study design, and therefore odds ratios could be estimated, but not predictions? (See: <a href=""http://stats.stackexchange.com/questions/69561/case-control-study-and-logistic-regression"">Case-control study and Logistic regression</a>)</p>
"
"0.070675349274022","0.0697410440814588","222426","<p>I would like to analyze a Randomized Response variable as the final response variable in a Structural Equation Model (SEM), with <code>R</code>. However, I found no example about this. To the best of my knowledge <code>R</code> packages enable users to fit multivariate logistic regressions only. 
However, I have seen that Randomized Response can be analyzed with SEM in Mplus (Hox, J., &amp; Lensvelt-Mulders, G. (2004). <a href=""http://www.tandfonline.com/doi/abs/10.1207/s15328007sem1104_6?journalCode=hsem20"" rel=""nofollow"">Randomized response analysis in Mplus</a>. Structural equation modeling, 11(4), 615-620.). </p>

<p>Given that <code>R</code> has good packages for latent variable analysis, like <code>lavaan</code>, could you tell me how could I model a randomized response with SEM in <code>R</code>?</p>
"
"0.0893980312535348","0.0882162182782462","222803","<p>I am using elastic net regression on a dataset with quite a small number of observations (clinical risk scores) and large number (1000+) of potential predictor variables (gene expression values). The ultimate aim is to identify variables (genes) that could be explored further experimentally. </p>

<p>However, I noticed that the variables being selected (e.g. coefficients not equal to zero) vary when I leave out a single observation from the dataset (some are maintained, some drop, some are added in), and I would like get some more confidence regarding which variables are relatively robust to such changes.</p>

<ol>
<li><p>Would it be methodologically acceptable to generate either jackknife or bootstrap datasets from my original dataset, and repeat the whole model selection procedure (e.g. repeated selection of tuning parameters based on cross validation and the associated model) for each of these datasets, and then for each of those models determine how often a variable was selected?</p></li>
<li><p>Would it be acceptable to re-run a linear regression analysis with sets of selected genes of decreasing frequency of selection (e.g. only use the top 3 genes selected most often, or the top 4 etc), and select the cut-off of frequency of selection of genes based on the cross validation metrics I get for each of these models? If I do this I see a sort of leveling of cross validation prediction error metrics after a certain point (e.g. adding further variables does not help to reduce prediction error metrics). This second approach feels like a ""wrong"" thing to do. However, it is just to determine a threshold of which subset of selected genes I want to focus on further.</p></li>
</ol>
"
"0.0446990156267674","0.0441081091391231","223154","<p>I'm a Master student researching for his thesis.</p>

<p>In a panel data analysis, I have a sample of 141 observations related to airports. First lines are:</p>

<pre><code>   Airport       Date Year PH.tot.seats Seat.year Seasonality
1      OLB 22/08/2014 2014         2534   2751080      2.6000
2      OLB 23/08/2015 2015         2449   2798846      2.5700
3      MAH 03/08/2014 2014         2460   2987482      2.5500
4      OLB 02/08/2013 2013         1740   2516710      2.5300
5      MAH 17/08/2015 2015         3686   3431666      2.5000
6      MAH 20/08/2012 2012         2681   2758799      2.4600
7      WMI 04/11/2012 2012         1863   1320165      2.4400
8      OLB 20/08/2012 2012         1932   2326278      2.4200
9      MAH 01/08/2011 2011         2344   2860084      2.3800
10     IBZ 03/08/2014 2014         3801   7170858      2.3700 
11     IBZ 06/08/2015 2015         3956   7465915      2.3500 
12     IBZ 13/08/2012 2012         3561   6345849      2.3400 
13     IBZ 03/08/2011 2011         3297   6616744      2.3100 
14     RHO 25/08/2015 2015         2703   3850181      2.3000 
15     IBZ 02/08/2013 2013         3438   6711774      2.2800 
</code></pre>

<p>The panel is unbalanced, with every unit between 1 and 7 times in the panel. After I find out that the fixed effect model is the most suitable and I run it, I test the usual assumptions. I followed this document as a guideline: <a href=""http://www.princeton.edu/~otorres/Panel101R.pdf"" rel=""nofollow"">http://www.princeton.edu/~otorres/Panel101R.pdf</a>
This is the code of the bit that does not work as I expected:</p>

<pre><code>#panel regression-fixed effects n entity specific intercepts
fixed = plm(log(PH.tot.seats)~log(Seat.year)+log(Seasonality),data=db, 
index=c(""Airport"", ""Year""),model=""within"")
summary(fixed)
fixef(fixed)
#tests for cross sectional dependence in panels
pcdtest(fixed,test=c(""lm""))
pcdtest(fixed,test=c(""cd""))
</code></pre>

<p>And this is the output for both the Pesaran and the BP test.</p>

<p><a href=""http://i.stack.imgur.com/ti8Uo.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ti8Uo.png"" alt=""X-sectional tests output""></a></p>

<p>Is there any problems with the dataset or the code? Any suggestion would be highly appreciated.</p>
"
"0.0446990156267674","0.0441081091391231","223917","<p>I have multiple time series of air passenger demand with specific classification data. Data looks like this (some rows may lack some data):</p>

<pre><code>Origin  Destination Time  {more classifiers}  Apr. 1st   May 2nd   Jun. 20th {more dates} 
------------------------------------------------------------------------------------------
Madrid  London      early ...                       10        15          20  ...  
{more rows}
London  Rio         late  ...                       12        10          15  ...   
</code></pre>

<p>The task: The target is to predict values for the next month/day(s). We made an approach with regression trees (ctree, R package partykit) but this results in quite inaccurate results. That is why we want to give time series analysis a try.</p>

<p>My Question: Which algorithm/method would you suggest for this problem? I think it should be a mixture between time series analysis and classification. Any hints/suggestions are highly appreciated. Thanks!</p>
"
"0.0446990156267674","0.0441081091391231","224153","<p>I am performing a mediation analysis to investigate the relationship between National Error Management Culture (error_is) on Entrepreneurial Activity (TEA) with the mediator Innovation Index (innov_perc). So first I ran these regressions:</p>

<pre><code>lm(TEA~error_is) --&gt; 5,085 / p=0,0053 
lm(innov_perc~error_is) --&gt; -0,313 / p=0,0001 
lm(TEA~error_is+innov_perc) --&gt; for error_is X: 0,25, p not significant / for innov_perc: -15,55, p=0,007
</code></pre>

<p>Am I correct to conclude, that there is a full mediation? If so, how can I explain it, because Y~X is positive, and M~X and Y~M is negative. What does this mean?</p>

<p>I then ran the mediate function:</p>

<pre><code>model.1 &lt;- lm(innov_perc~error_is, data)
model.2 &lt;- lm(TEA~ error_is+innov_perc, data)
out.1 &lt;- mediate(model.1, model.2, treat = ""error_is"", sims=1000, dropobs = TRUE, mediator = ""innov_perc"")
summary(out.1)
</code></pre>

<p>With these results:</p>

<pre><code>               Estimate 95% CI Lower 95% CI Upper p-value
ACME              4.723        1.518        8.661    0.00
ADE               0.270       -3.902        4.756    0.92
Total Effect      4.993        1.474        8.425    0.01
Prop. Mediated    0.954        0.278        2.784    0.01

Sample Size Used: 35 


Simulations: 1000 
</code></pre>

<p>Does this mean that Error_is has a negative influence on innoc_perc and therefore TEA increases because Entrepreneurial Activity is often not innovative?</p>

<p>Please help me to understand how I can interpret this!! 
Thanks a lot!!</p>
"
"0.0364965934300906","0.0540211804549215","224896","<p>I am relative new in regression analysis. I would like to know, if there is a way in regression analyis to estimate the risk or calculate the risk for future values?</p>

<p>An example:</p>

<p>We want to predict the export of a country for next year. We can use linear regression to estimate the export value for the next year. This value may be influenced by another parameter for example Weather catastrophe. Is there any way to estimate such a risk in linear regrssion?</p>
"
"0.0948209311861521","0.0935674292327708","224947","<p>What are some of the best practices and steps to building models for prediction and or inferences? </p>

<p>What have been taught to me during my classes was the steps outlined in Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates"". The method to screen a large dataset with many potential predictors is to use a algorithmic approach such as Stepwise, best subset regression, etc. Then verify the model after the fact for potential collinearity, confounders, etc.  However, I have read much criticism on this site in regards to those said steps and methods.</p>

<p>For example - if I was provided a dataset with ~100 potential predictors, what would be the best practice to selecting those said predictors for inclusion or exclusion of the model for prediction/inference ? </p>

<p>According to Hosmer et al., the steps would be to perform univariate analysis to screen for all of those potential predictors (p &lt; .25), then move to inclusion of those said predictors to a multivariate model. Take a stepwise approach to removing insignificant predictors, then add back and verify the significance of each non significant predictor. </p>

<p>However - the more I've read on this site the more confused I've gotten about what is considered best practices, and I've come to question more and more of what was taught during my classes.</p>

<p>Once again just to reiterate - </p>

<ol>
<li><p>What would be the best practices for building a model for obtaining unbiased measure of association for each individual predictors?</p></li>
<li><p>What would be the best practices for building a model strictly for prediction?</p></li>
</ol>

<p>I'm still learning much about the world of data science and appreciate any help that is provided!</p>

<p>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</p>
"
"0.0836242010007091","0.0825187161885156","228493","<p>The literature on Survival Analysis is mainly from the Medical science where tipically the researcher want to evaluate the effect of a treatment to that of another one. So far, all the example I read and studied thus contain one or more categorical variable (with at least 2 levels) and possibly some continuous variable as a covariate. Anyway the main interest is on a categorical variable (e.g. treatment).</p>

<p>Is it possible and correct to run a non parametric Cox model (or alternatively a parametric one) using only one or more continuous variables? In particular without categorizing the continuous var into 2 or more groups? </p>

<p>Something like a logistic regression. </p>

<p>To give you a more practical example, I'm trying to model the survival of say bush in a field depending on the number of cows in the same field. </p>

<p>I'm pretty sure it can be done but the lack of examples leave me in the doubt.</p>

<p>If possible how can one use the predict function for example to predict the survival when the predictor has a specified value? like survival of my plant when 10 cows are in the field...</p>

<p>any help is welcome!</p>
"
"0.0836242010007091","0.0825187161885156","229336","<p>I have a data set looking into whether a farm experienced a livestock disease or not in the year 2011 and 2012  and if several factors could be predictors for the livestock disease.</p>

<p>The independent variables were also collected for both years though some variables did not change e.g Thistles remained the same for both years.</p>

<p>I am looking for an appropriate method that will allow statistical comparison between the two years rather than treating analysis as two separate sets of analyses (i.e not to treating 2011 and 2012 as two separate data set)</p>

<p>Whilst trying to do the analysis I have created dependent variable as farm having the disease or not between year 2011 and 2012(Orf.Yes.No2011.2012)against the dependent variables using logistic regression:</p>

<p>I'm just wondering whether I doing the right thing or what could be the best statistical approach which will allow for statistical comparison between the two years? Any help will be very much appreciated</p>

<pre><code>Here is the R output and sample of dataset:





 &gt; mod=glm(Orf.Yes.No2011.2012~F2011+ F2012+as.factor(Breed)+ 
                                  D2011+D2012,family=binomial, data=orf)
      summary(mod)

    Call:
    glm(formula = Orf.Yes.No2011.2012 ~ F2011 + F2012 + as.factor(Breed) + 
        D2011 + D2012, family = binomial, data = orf)

    Deviance Residuals: 
       Min      1Q  Median      3Q     Max  
    -1.862  -1.293   1.023   1.065   1.318  

    Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)  
    (Intercept)        0.3917290  0.1626769   2.408    0.016 *
    F2011              0.0003269  0.0002782   1.175    0.240  
    F2012             -0.0003596  0.0002786  -1.291    0.197  
    as.factor(Breed)2  0.0558285  0.1489246   0.375    0.708  
    D2011             -0.0311978  0.0272068  -1.147    0.252  
    D2012              0.0226963  0.0274981   0.825    0.409  
    Small sample data set:

    F2011   F2012   Breed   Orf.Yes.No2011  Orf.Yes.No2012  Orf.Yes.No2011.2012
    155     150     1       0               0               0
    740     760     2       0               1               1
    1000    850     1       0               0               0
    1630    1520    1       1               1               1
    0       460     1       0               0               0
    1300    1335    1       0               1               1
    450     450     1       0               0               0
    390     730     1       1               0               1
    390     380     2       0               0               0
    600     600     2       0               0               0
</code></pre>
"
"0.0446990156267674","0.0441081091391231","229709","<p>I have several slightly related variables measured in two instruments on the same sample at different time points. I'm trying to know how well the differences between the two instruments can be explained by other variables and the time point.</p>

<p>1.- I'd like to know if my method is adequate: I have chosen to perform a multivariate regression (with lm or glm) that includes a ""day"" as a polinomial variable and the average of both instruments (ex. AVGvar2) and differences between them (ex. DIFvar2) as linear variables. And I perform a StepAIC on the fit. All this for each variable (so that I eventually can say <strong>what variables significantly influde in observed differences</strong> for each variable).</p>

<pre><code>fit &lt;- glm(var1dif~var1+AVGvar2+DIFvar2+AVGvar3+DIFvar3+AVGvar4+DIFvar4+AVGvar5+DIFvar5+day+I(day^2)+I(day^3)+I(day^4)+I(day^5)+I(day^5); step1 &lt;-stepAIC(fit,direction=""both"")
fit2 &lt;- lm(var1dif~var1+AVGvar2+DIFvar2+AVGvar3+DIFvar3+AVGvar4+DIFvar4+AVGvar5+DIFvar5+poly(day,6); step2 &lt;-stepAIC(fit2,direction=""both"")
</code></pre>

<p>(the same would go for DIFvar2, DIFvar3, DIFvar4 and DIFvar5)
Any correction, advice or further step?</p>

<p>2.- When I compare summary(step) and summary(step2) the output (estimates, std error, coefficients) is the same for the variables. The results for variable day differ when the linear model has orthogonal or raw polinomials. Which one is better for my regression?</p>

<p>NOTE: I understand stepwise is frowned upon, but I think for this retrospective analysis it is decent and cost-effective enough.</p>

<p>Thank you.</p>
"
"0.104828483672192","0.103442685121077","229884","<p>I have a cancer classification problem (type A vs type B) on radiological images from which i have generated 756 texture-based predictive features (wavelet transform followed by texture analysis, i.e., features described by Haralick, Amasadun etc) and 8 semantic features based on subjective assessment by expert radiologist. This is entirely for research and publication to show that these predictive features may be useful in this particular problem. I do not intend to deploy the model for practitioners. </p>

<p>I have 107 cases. 60% cases are type A and 40% type B (in keeping with their natural proportions in population). I have done several iterations of model development with varying results. One particular method is giving me an 80% 80% classification accuracy but I am suspicious that my method is not going to stand critical analysis. I am going to outline my method and a few alternatives. I will be grateful if someone can pick if it is flawed. I have used R for this:</p>

<p>Step 1: Split into 71 training and 36 test cases.<br>
Step 2: remove correlated features from training dataset (766 -> 240) using findcorrelation function in R (caret package)<br>
Step 3: rank training data features using Gini index (Corelearn package)<br>
Step 4: Train multivariate logistic regression models on top 10 ranked features using subsets of sizes 3 , 4, 5 ,and 6 in all possible combination (<sup>10</sup>C<sub>3</sub>=252, <sup>10</sup>C<sub>4</sub>=504, <sup>10</sup>C<sub>5</sub>=630). So <strong>total 1386 multivariate logistic regression models were trained</strong> using 10-fold cross-validation and tested on test dataset.<br>
Step 5: Of these I selected a model which gave the best combination of training and test dataset accuracy, i.e., 3 feature model with 80% 80% accuracy.<p></p>

<p>Somehow running 1300 permutations seems quite dodgy to me and seems to have introduced some false discovery. Just want to confirm if this is a valid ML technique or whether I should skip step 4 and only train on top 5 ranked features without running and permutations.</p>

<p>Thanks. <p> PS I experiemented a bit with naive bayes and random forests but get rubbish test set accuracy so dropped them</p>

<p>====================</p>

<h1>UPDATE</h1>

<p>Following discussion with SO members, i have changed the model drastically and thus moved more recent questions regarding model optimisation into a new post <a href=""http://stats.stackexchange.com/questions/232829/lasso-regularised-classification-highly-variable-choice-of-lambda-min-on-repeate"">LASSO regularised classification highly variable choice of lambda.min on repeated cv</a></p>
"
"0.0948209311861521","0.0935674292327708","230532","<p>Trying to get the Bayes Factor for a correlation between two variables in my data, I tried three different functions. All implement the Jeffreysâ€“Zellnerâ€“Siow (JZS) prior, but I get quite different results with the three approaches. Two questions:</p>

<ol>
<li><p>Is this suspicious, or is it reasonable that they produce different values, as the implementations are slightly different?</p></li>
<li><p>Is there a consensus on the best measure to use?  </p></li>
</ol>

<p>My data:</p>

<pre><code>a=rnorm(100,1,2)
b=rnorm(100,.8,1.5)
myData &lt;- data.frame(a=a, b=b)
</code></pre>

<p>I try the <code>jzs_corbf</code> function, described and implemented <a href=""http://www.ncbi.nlm.nih.gov/pubmed/22798023"" rel=""nofollow"">here</a> (<a href=""http://dsquintana.com/post/98962697485/how-to-calculate-a-bayes-factor-for-correlations"" rel=""nofollow"">shorter version</a>)</p>

<pre><code>cor.resu.a_b &lt;- cor.test(myData$a, myData$b, method=c(""pearson""))
cor.resu.a_b$estimate
n = 100
r = cor.resu.a_b$estimate
jzs_corbf(r,n)
[1] 0.08206358
</code></pre>

<p>I also tried the convenience function from the <code>BayesFactor</code> package:</p>

<pre><code>require(BayesFactor)
regressionBF(b ~ a, data = myData, progress=FALSE)

Bayes factor analysis
--------------
[1] a : 0.2181081 Â±0%

Against denominator:
  Intercept only 
---
Bayes factor type: BFlinearModel, JZS
</code></pre>

<p>And I also tried the a function described <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4891395/"" rel=""nofollow"">recently</a> (<a href=""https://osf.io/9d4ip/"" rel=""nofollow"">code</a>)</p>

<pre><code>bf10JeffreysIntegrate(n=100, r=r)

      cor 
0.1297927
</code></pre>

<p>While in this case the differences are only numerical, in my real data I get quite big differences that make it more difficult to decide on an interpretation. </p>

<p><a href=""http://stats.stackexchange.com/questions/184950/calculating-bayes-factor-from-a-correlation-coefficient"">Related</a></p>
"
"0.113960576459638","0.112454054604034","230911","<p>I'm not entirely sure of fitting the model for experiment we've made. The variables and relevant description are as follows:</p>

<ul>
<li>ID - participant ID </li>
<li>Trial - 60 for each participant</li>
<li>Memory - between subject binary factor</li>
<li>State - within subject binary factor  </li>
</ul>

<hr>

<ul>
<li>Correct - whether classification a participant made was correct or not</li>
<li>Rating - the judgement made after each trial on four point Likert scale</li>
</ul>

<p>Procedure brief: each participant (N=60) was randomly assigned to experimental or control group (Memory) and had 120 Trials (60 for State = 0 and 60 for State = 1). Each trial composed of perceptual classification (Correct) and judgment of how easy it was (Rating). The classification problem was randomly selected from two groups each trial (State).</p>

<p>I would like to calculate what impacts the performance (Correct) most - is it memory, state, a specific rating on a scale or any combination of above? I'm not interested in between subject variance, on the oposite, it is a random factor here. Also, it appears that there is bias in responses on Likert scales, so that part of variance should be excluded too. </p>

<p>The way I was thinking to approach this is generalized mixed linear model, but I'm not sure I'm doing it right; there is what I've got so far:</p>

<pre><code>model = glmer(Correct ~ (1|ID/Rating) + Memory * State * Rating, data, family=binomial, 
              control = glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun=100000)))
</code></pre>

<p>Is this approach correct? I'll appreciate your input.</p>

<p>Relevant resources I used: </p>

<ul>
<li><a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">Formulae in R: ANOVA and other models, mixed and fixed</a></li>
<li><a href=""http://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/"" rel=""nofollow"">The Difference Between Crossed and Nested Factors</a> </li>
<li><a href=""http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model"">When is it ok to remove the intercept in a linear regression model?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/225198/nested-random-factor-with-confounding-random-variable"">Nested random factor with confounding (random?) variable</a></li>
</ul>
"
"0.134283163620642","0.139482088162918","231066","<p>*EDIT: I ran test again with data set provided and realized that the cause of problem is definitely rank deficiency, because estimated values of parameters in nonlinear regression showed non existing p values and there was no way to create confidence intervals with this data. </p>

<h2>Thank you all for reading and help! This question is closed.</h2>

<p>I researched seed germination. I took 75 seed replicates and put them in  different ecological parameters (like temperature) and took data about sprouts in different time intervals. </p>

<p>Reading statistical science papers about this topic, I found that I should analyze my data in a time-to-event model (dose response curve), where I can use log-logistic regression or nonlinear regression (Ritz et al., 2013 -<a href=""http://dx.doi.org/10.1016/j.eja.2012.10.003"" rel=""nofollow"">http://dx.doi.org/10.1016/j.eja.2012.10.003</a>). </p>

<p>Two models (nonlinear and log-logistic) lead to quantitatively very similar fitted germination curves, i.e., similar parameter estimates, but qualitatively different statements about the precision of estimates. Nonlinear regression model yields an overly precise estimate of the proportion of seeds that germinated during the experiment, so the precision reported by the nonlinear regression is too high.</p>

<p>Similarly, the 95% confidence intervals of the fitted curves also demonstrate the dramatic difference in precision of the two models: Accurate prediction of germination percentages is not warranted by the data unless very low percentages are of interest.</p>

<p>Because of that I choose log-logistic regression as a model. First few data sets; treatments analyzed in R using analysis of Dose-Response Curves (drc package) went smooth, and I was able to plot and get final graph. Such data, which was successfully analyzed, contained treatments where max seeds germination was for example 50% of total seed number.</p>

<p>Example:</p>

<p><a href=""http://i.stack.imgur.com/yUqOu.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yUqOu.jpg"" alt=""Example of successful data analysis""></a></p>

<p>The problems arose when I entered the log-logistic model with treatment where all the seeds germinated in a short amount of time (meaning the treatment for this set of seeds is most adequate for their successful sprouting). For example, 100% of seeds germinated in only 5 days, so there are only two or three time intervals and a large number of sprouted seeds. The R program here reported  convergence error:</p>

<pre><code>Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
non-finite value supplied by optim
Error in drmOpt(opfct, opdfct1, startVecSc, optMethod, constrained, warnVal,  : 
Convergence failed 
</code></pre>

<p>Since I'm still a student in biology I have a very basic knowledge in statistics, so I tried to solve the problem with literature. </p>

<p>At first I thought that convergence failed because of perfect or complete separation, but through longer research it seems that the problem lies in rank deficiency. </p>

<p>When I analyzed the same data with nonlinear regression I've managed to fit curve and plot a graph without a problem.  </p>

<p>So, is there a way to make log-logistic model work even though I have obviously small data in cases of 100% germination? Should I switch to nonlinear regression  even though the reported precision would be too high. </p>
"
"0.0565402794192176","0.0557928352651671","231260","<p>This is my first post here, so please bear with me! I'm comparing several biomarkers with Kaplan-Meier curves and calculating hazard ratios for different risk groups (defined by a certain, well established cut-off value of the biomarker) by using Cox regression in R. We have 3 tiers with a low, intermediate and high risk group, however the low risk group for one biomarker contains no events.</p>

<p>This, so my understanding, leads to quasi-separation in the Cox regression and hence infinite values and large coefficients and SEs. I understand the Likelihood ratio is still valid, but what I'm obviously interested in is a calculation of the HR from exp(coef). A sample of the data is displayed below:</p>

<pre><code>   &gt; head(riskgroups)
   ID FU_3y_death FU_3y_death_days biomarker bm.riskcat
   1           0             1095           58.2    group.3
   2           1               79           11.5    group.2
   22           0             1095           11.7    group.2
   27           0              929            9.0    group.2
   44           0              949            7.0    group.2
   46           0             1095            7.5    group.2
</code></pre>

<p>Now I have found that using Firth's method might allow a workaround, hence I've tried to run the analysis using coxphf with the following code:</p>

<pre><code>cox.groups &lt;- coxphf(riskgroups, formula=Surv(FU_3y_death_days,FU_3y_death) ~ bm.riskcat, pl=T, firth = T)
</code></pre>

<p>Rather bizarrely, this results in the following error:</p>

<blockquote>
  <p>Error in coxphf(riskgroups, formula = Surv(FU_3y_death_days, FU_3y_death) ~  : 
    NA/NaN/Inf in foreign function call (arg 3)</p>
</blockquote>

<p>I would have assumed that this is exactly what coxphf is trying to avoid? When setting pl to FALSE (to base the tests on the Wald method instead of profile penalised LL) I get results with all NaN. Of course the fact that there is no event in the lowest risk group is in itself an important message, but I do require hazard ratios for the second and third tier of risk categories to compare the different biomarkers. Any bright thoughts on this, my research into this has hit a wall after 3 days of reading...</p>
"
"0.0316069770620507","0.0311891430775903","232327","<p>I have Two factor A and B where B is nested under A and count dependent variable (Y). I have used Poisson Regression Model in SPSS using following syntax command.</p>

<pre><code> *Generalized Linear Models. 
  GENLIN Y BY A B (ORDER=ASCENDING)
 /MODEL  A B(A) INTERCEPT=YES 
 DISTRIBUTION=POISSON LINK=LOG 
 /CRITERIA METHOD=FISHER(1) SCALE=1 COVB=MODEL MAXITERATIONS=100 MAXSTEPHALVING=5 PCONVERGE=1E-006(ABSOLUTE) SINGULAR=1E-012 ANALYSISTYPE=3(WALD)  CILEVEL=95 CITYPE=WALD LIKELIHOOD=FULL 
 /PRINT CPS DESCRIPTIVES MODELINFO FIT SUMMARY SOLUTION.   
</code></pre>

<p>I found Factor A insignificant But B(A) significant. 
Please Guide me </p>

<ol>
<li>Can I interpret this B(A)factor? and</li>
<li>If Yes, How can I develop pairwise comparison?</li>
<li>Is there some batter option in R language? </li>
</ol>
"
"0.141511233180756","0.132990952933197","232829","<p>I am using CT scans to classify lung cancer into one of two types (Adenocarcinoma vs. Squamous carcinoma; we can abbreviate them A &amp; B). I am applying LASSO penalized logistic regression to a data set containing 756 CT-derived texture features and 12 radiologist identified categorical (yes/no) features. The latter are based on literature for relevance whereas the former are computer generated with no prior proof of being useful. I have 107 cases so my final dataframe (df) is 107 x 768 dimensional:</p>

<p>i)Texture features (mathematical quantities n=756) are continuous variables scaled and centered. Their names are stored in list <code>â€˜texVarsâ€™</code></p>

<p>ii)Semantic features(Qualitative features subjective assessed by experienced radiologist, n=12). These are usually categorical binary inputs of yes / no type. Their names are stored in list <code>â€˜semVarsâ€™</code>.</p>

<p>Following comments from community on my original (very different) model <a href=""http://stats.stackexchange.com/questions/229884/is-my-high-dimensional-data-logistic-regression-workflow-correct"">Is my high dimensional data logistic regression workflow correct?</a>, I performed my LR development in three steps:</p>

<p>1)Feature selection: I used principle components analysis to reduce texture feature-space from 756 to 30. I kept 4 most relevant (from literature) semantic features. This gave me 34 final features. I used the following command:</p>

<pre><code>trans = preProcess(df[,texVars], method=c(""BoxCox"", ""center"",   ""scale"", ""pca""),thresh=.95)  # only column-names matching â€˜texVarsâ€™ are included.
neodf2 &lt;- predict(trans,df[,texVars]).
neodf.sem &lt;- neodf2[,c(""Tumour"",""AirBronchogram"", ""Cavity"", ""GroundglassComponent"",""Shape"")]  # this DF is 107 x 4 dimensional, containing only 4 semantic features (most relevant from prior knowledge).
neodf.tex &lt;- neodf2[,c(""Tumour"",setdiff(names(neodf2),names(neodf.sem)))] # this only has the 30 PCA vectors (labelled PC1 â€“ PC30).
</code></pre>

<p>2) Model development (LASSO) and penalty term tuning (10fold cross-validation) using cv.glmnet command  Deviance was used as determinant of model quality. Using this method, I developed a model incorporating only semantic features, a second model incorporating only texture features, and a third model incorporating both semantic and texture features. Here are the commands:</p>

<pre><code>#Converting to model.matrix for glmnet 
xall &lt;- model.matrix(Tumour~.,neodf2)[,-1]
xtex &lt;- model.matrix(Tumour~.,neodf.tex)[,-1]
xsem &lt;- model.matrix(Tumour~.,neodf.sem)[,-1]
y &lt;- neodf$Tumour
require(glmnet)
grid &lt;- 10^seq(10,-2,length=100)

lasso.all &lt;- cv.glmnet(xall,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"") 
lasso.tex &lt;- cv.glmnet(xtex,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
lasso.sem &lt;- cv.glmnet(xsem,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
</code></pre>

<p>3) Testing model classification accuracy on entire dataset. The following is the backbone of  bootstrap to generate 95% confidence intervals of predictive accuracy:</p>

<pre><code>pred &lt;- predict(lasso.all, newx = xall, s = ""lambda.min"", ""class"")
tabl &lt;- table(pred,y)
sum(diag(prop.table(tabl)))
</code></pre>

<p>4) As an alternative means to assess model performance than classification accuracy, I used ROC area under curve on entire dataset and compared AUROC curves from different models using DeLong's method (pROC package)</p>

<p>The results are interesting</p>

<pre><code> =================================================
</code></pre>

<p>LR MODEL BASED ON SEMANTIC FEATURES ALONE:
    lasso.sem$lambda.min
     0.01</p>

<p>Plot cv lambda vs. binomial devance <a href=""http://i.stack.imgur.com/6Modw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6Modw.png"" alt=""cv lambda vs binomia deviance""></a></p>

<pre><code>             Feature          Odds Ratio
1                (Intercept)  0.1292604
2      AirBronchogramPresent  0.1145378
3              CavityPresent 35.4350358
4 GroundglassComponentAbsent  4.3657928
5                 ShapeOvoid  2.4752881

AUC: .84


=================================================    
</code></pre>

<p>LR MODEL BASED ON TEXTURE FEATURES ALONE:</p>

<pre><code>lasso.tex$lambda.min
1e+10   
</code></pre>

<p>Plot  cv lambda vs binomial deviance (texture alone). Note how the 95% CI's are all overlapping! <a href=""http://i.stack.imgur.com/1Mk7M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1Mk7M.png"" alt="" cv lambda vs binomial deviance ""></a></p>

<pre><code>   Feature OddsRatio
1 (Intercept) 0.6461538



============================================================
</code></pre>

<p>LR MODEL BASED ON TEXTURE + SEMANTIC FEATURES:</p>

<pre><code>lasso.all$lambda.min
0.05 
</code></pre>

<p>Plot  cv lambda vs binomial deviance <a href=""http://i.stack.imgur.com/p1AHX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p1AHX.png"" alt=""cv lambda vs binomial deviance""></a></p>

<pre><code>                          Feature   OddsRatio
1                (Intercept)        0.3136489
2                       PC23        0.9404430
3                       PC27        0.8564001
4      AirBronchogramPresent        0.2691959
5              CavityPresent        6.7422427
6 GroundglassComponentAbsent        2.0514275
7                 ShapeOvoid        1.5974378

 AUC : .88
</code></pre>

<p>Plot showing loglambda vs coefficients. The dashed vertical line shows the cross-validated optimum lambda:<a href=""http://i.stack.imgur.com/d6FaO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d6FaO.jpg"" alt=""enter image description here""></a></p>

<p>Having rejected the texture only model, which only contains intercept, i was left with two models - semantic and combined texture+semantic. I created ROC curves for both and compared them using DeLong's method:</p>

<pre><code>pred.sem&lt;- predict(lasso.sem, newx = xsem, s = ""lambda.min"")
pred.all&lt;- predict(lasso.all, newx = xall, s = ""lambda.min"")

roc.sem&lt;- roc(y,as.numeric(pred.sem), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)    

roc.all&lt;- roc(y,as.numeric(pred.all), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)
</code></pre>

<p>Outputs of ROC analysis are:</p>

<pre><code>data:  roc.sem and roc.all
Z = -2.1212, p-value = 0.0339
alternative hypothesis: true difference in AUC is not equal to 0
sample estimates:
AUC of roc1 AUC of roc2 
  0.8369963   0.8809524 
</code></pre>

<p>Showing that combined model ROC curve is significantly better despite the modest improvement in AUC (83% vs 88%).
Questions are:</p>

<p>a) is my methodology airtight from a publication point of view now? Apologies in advance for any gross errors in my presentation of this problem.</p>

<p>b) what is the formal inference that texture model is intercept only.</p>

<p>c) if texture model is useless, how do its variables become useful once added to semantic features and yield a higher overall accuracy in the combined result? perhaps that means the effect of texture features alone is too small to be detected in this small dataset but becomes apparent when combined with a stronger predictor (i.e., semantic features). </p>

<p>Any further comments are welcome.</p>
"
"0.078223277346843","0.0882162182782462","233406","<p>I am using the <code>Cochrane.orcutt</code> procedure to do a time series analysis.</p>

<p>I did the initial regression with the <code>lm</code> function, and then past the result to <code>Cochrane.orcutt</code>. </p>

<pre><code>reg &lt;- lm(data$Y ~ data$Y_Lag1 + data$X1 + data$X2)

regfinal &lt;- cochrane.orcutt(reg)
regfinal
Cochrane.Orcutt

Call:
lm(formula = YB ~ XB - 1)

Residuals:
 Min       1Q   Median       3Q      Max 
-0.72984 -0.32750  0.03553  0.17989  0.69595 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
XB(Intercept)                0.39347    0.09500   4.142  0.00252 ** 
XBdata$Y_Lag1                1.05427    0.09556  11.033 1.57e-06 ***
XBdata$X1                   -3.16739    0.80754  -3.922  0.00350 ** 
XBdata$X2                   -3.30504    0.86569  -3.818  0.00410 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.447 on 9 degrees of freedom
Multiple R-squared:  0.9451,    Adjusted R-squared:  0.9208 
F-statistic: 38.76 on 4 and 9 DF,  p-value: 1.115e-05


$rho
[1] -0.6855955

$number.interaction
[1] 8
</code></pre>

<hr>

<p>I am doing a time series with regressors.</p>

<pre><code>Y_t = a + b*Y_t-1 +c*X1_t +d*X2_t
</code></pre>

<p>I just have a few questions.</p>

<ol>
<li><p>This Cochrane procedure looks like it gives back a list, not a lm. I want to plot the fitted vs actuals, and also use the predict function. The code below gives an error (whereas it works on stuff from ""lm"")</p>

<pre><code>plot(data$Y,col=""red"")
    lines(data$Y)
lines(fitted(regfinal),col=""blue"")
</code></pre></li>
<li><p>Is what I am doing with the regression correct for a theory point of view? That is, including in the lagged value of the response? I am new to Regression with ARMA errors. I have seen somewhere that you are supposed to run the standard regression with only the predictors (no Y_Lag1), and then you get the residuals and you build a model with it. The problem is, when I exclude Y_Lag1 from my lm, the significance of everything else drops away:</p></li>
</ol>

  

<pre><code>Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)
XB(Intercept)               -0.02025    0.73246  -0.028    0.978
XBdata$X1                   -1.31001    0.98394  -1.331    0.213
XBdata$X2                   -1.32077    1.42550  -0.927    0.376

Residual standard error: 0.8196 on 10 degrees of freedom
Multiple R-squared:  0.2635,    Adjusted R-squared:  0.04256 
F-statistic: 1.193 on 3 and 10 DF,  p-value: 0.3616


$rho
[1] 0.6886191

$number.interaction
[1] 6
</code></pre>
"
"0.0446990156267674","0.0441081091391231","233802","<p>I've been given a data set containing 155 training examples and 108 features.I removed the features with more than 79 NA values and brought then them down to 99. I trained using the first 140 examples and used the rest for testing the prediction.The target variable is a discrete value i.e 0,1,2,3,4 though not restricted to those. I tried using multinomial logistic regression and random forest and got poor accuracy. Is there an algorithm that performs well on small data sets? Decision trees and regression don't seem to be working well for this. I am using R for the data analysis</p>
"
"0.0446990156267674","0.0441081091391231","234220","<p>I am new to time series analysis. I am trying to use the R package dlm for time-varying regression with multiple regressors. I got the basic dlmModReg to work, so I can get the coefficients for all the regressors as a function of time. My question is, what's the correct way to estimate the relative importance of each regressor at each time point (or within some window)? Thank you!</p>
"
"0.0893980312535348","0.0882162182782462","234537","<p>I've split my data set into a training and test set. I've performed a principal component analysis on the training set and have used the first 3 principal components to generate a logistic regression model for my response.</p>

<p>I now want to use this model to make predictions for my test data set and check if this is true. </p>

<p>I've been trying to use the predict function but obviously the model uses the principal components of the training set as the predictors whereas my test set just has all the original predictors so obviously they're not compatible.</p>

<p><strong>How do I go about 'projecting' my test data onto the principal components I've already generated so I can use my model to make predictions?</strong></p>

<p>Ideally I'd like to do this without using any external packages (it's for university). I am working in R.</p>
"
"0.0999500374687773","0.0986287303940589","234541","<p>I am currently doing an analysis for my Master Thesis and encountered some results I cannot explain.</p>

<p>In my paper, I am trying to explore factors that decide whether people joined a local energy initiative or not. Since I have a lot of different variables, my instructor suggested a model building approach. Concretely, I am adding sets of predictors to my logistic regression and only keep those that are significant in the model, before adding the next set. To assess model fit, I was told to use classification tables.</p>

<p>My problem now is the following:</p>

<p>I start with a set of dummies to control for participants coming from different neighbourhoods. This basic model classifies 56% of cases correctly. Now I add the second set of predictors and some of them are significant, so I keep those in the model. If I now use the classification table again, my classification got worse. Even worse than chance! (48%).</p>

<p>How can I find significant predictors but my model gets worse than chance?</p>

<p>EDIT FOR ADDITIONAL INFO:</p>

<p>My Dataset consits of 636 cases. 318 are partakers of the initiative, 318 are not partakers. The sets of variables I use are structured as follows:</p>

<p>1) ""Control"": People come from 30 different neighbourhoods, so I added 29 dummy variables to control for differences due to neighbourhood membership (not the best approach, I know, but IÂ´m just following orders on this one)</p>

<p>2) Individual predictors: 15 demographic and psychological variables</p>

<p>3) Assessment of group predictors: 8 variables that measure how individuals perceive the group of potential partakers</p>

<p>I used the classification tables on the same data that I used for building the model, unfortunately I only have this one dataset and IÂ´m trying to figure out which predictors are most promising for future (causational) research.</p>
"
"0.104828483672192","0.103442685121077","234690","<p>I am new to survival analysis. Below is my data with very unbalanced sample size (treat group has 2 samples with 1 event, 1 censored and control group has 700+ samples). I use Cox regression in 'survival' package in R and results show 3 different tests (likelihood ratio test, log rank test and Wald test). </p>

<pre><code>sample   trt    censor time
A7       TRT     0 1.0219178
BH       TRT     1 0.6136986
SB        C      0 0.7095890
SD        C      0 1.1972603
SE        C      0 3.6191781
..       ..     ..  ..
A1        C      0 4.0082192
</code></pre>

<p>My code:</p>

<pre><code>coxph(Surv(time,censor)~trt, data=dataAll)
</code></pre>

<p>Result:</p>

<pre><code>&gt; coxfit
Call:
coxph(formula = Surv(time, censor) ~ trt, data = dataAll)

  n= 772, number of events= 100 

                 coef exp(coef) se(coef)      z Pr(&gt;|z|)    
trtC -3.80047   0.02236  1.04854 -3.625 0.000289 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

             exp(coef) exp(-coef) lower .95 upper .95
trtC   0.02236      44.72  0.002864    0.1746

Concordance= 0.513  (se = 0.002 )
Rsquare= 0.007   (max possible= 0.73 )
Likelihood ratio test= 5.55  on 1 df,   p=0.01845
Wald test            = 13.14  on 1 df,   p=0.0002895
Score (logrank) test = 38.85  on 1 df,   p=4.579e-10
</code></pre>

<p>My questions are: </p>

<ol>
<li>There are 3 tests giving different p values, and they look quite different with the likelihood ratio test the most conservative. Do they all test for the significant of the Cox coefficient? Which one should I choose?</li>
<li>Give the fact that the treatment group has so few samples, could the p value trustable?</li>
<li>Is it appropriate to apply Cox regression to unbalanced sample? If no, is there any alternative methods?</li>
</ol>

<p>Thanks a lot!</p>

<p>J</p>
"
"0.070675349274022","0.0418446264488753","234846","<p>I'm trying to perform non-proportional survival analysis with competing risk in R.</p>

<p>Without the competing risk, to perform non-proportional analysis, i used timeSplitter in package Greg to split the dataset and then create time-dependent dummy variables:</p>

<p><code>spl_female_oadlt &lt;- timeSplitter(female_oadlt, 
                               by = 1,
                               time_var = ""CensorAge"",
                               event_var = ""Censor"")
spl_female_oadlt$SEI1_Time &lt;- (spl_female_oadlt$SEICat_End == 1)*spl_female_oadlt$Start_time
spl_female_oadlt$SEI2_Time &lt;- (spl_female_oadlt$SEICat_End == 2)*spl_female_oadlt$Start_time
spl_female_oadlt$SEI4_Time &lt;- (spl_female_oadlt$SEICat_End == 4)*spl_female_oadlt$Start_time
</code></p>

<p>and then use coxph to perform Cox regression:</p>

<pre><code>NonProp_20yrSEIcat &lt;- coxph(Surv(Start_time, Stop_time, CensorCOD_Cancer == 1) ~ factor(SEICat_End) + Age + cluster(id) + 
                          SEI1_Time + SEI2_Time + SEI4_Time,
                        ties = ""efron"",
                        data = spl_female_20yr_fu)
</code></pre>

<p>Without non-proportional analysis, to perform competing risk, I used coxph but censored for both alive and people died from other causes. CensorCOD_Cancer has 3 values (0 = Alive, 1 = Died from cancer, 2 = Died from other causes):</p>

<pre><code>CmpRsk_20yrSEIcat &lt;- coxph(Surv(CensorAge, CensorCOD_Cancer == 1) ~ factor(SEICat_End) + Age + cluster(id) + 
                          SEI1_Time + SEI2_Time + SEI4_Time,
                        ties = ""efron"",
                        data = spl_female_20yr_fu)
</code></pre>

<p>Anyone has any idea on how to do competing risk in non-proportional hazard models in R? And am I doing competing risk correctly?</p>

<p>Thanks.</p>
"
