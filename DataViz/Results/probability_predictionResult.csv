"V1","V2","V3","V4"
"0.166666666666667","0.166998341953264"," 12223","<p>I am trying to figure out how to control the smoothing parameters in an mgcv:gam model.</p>

<p>I have a binomial variable I am trying to model as primarily a function of x and y coordinates on a fixed grid, plus some other variables with more minor influences.  In the past I have constructed a reasonably good local regression model using package locfit and just the (x,y) values.  </p>

<p>However, I want to try incorporating the other variables into the model, and it looked like generalized additive models (GAM) were a good possibility.  After looking at packages gam and mgcv, both of which have a GAM function, I opted for the latter since a number of comments in mailing list threads seem to recommend it.  One downside is that it doesn't seem to support a local regression smoother like loess or locfit.</p>

<p>To start, I just wanted to try to replicate approximately the locfit model, using just (x,y) coordinates.  I tried with both regular and tensor product smooths:</p>

<pre><code>my.gam.te &lt;- gam(z ~ te(x, y), family=binomial(logit), data=my.data, scale = -1)

my.gam.s  &lt;- gam(z ~  s(x, y), family=binomial(logit), data=my.data, scale = -1)
</code></pre>

<p>However, plotting the predictions from the model, they are much much more smoothed compared to the locfit model.  So I've been trying to tune the model to not oversmooth as much.  I've tried adjusting the parameters sp and k, but it's not clear to me how they affect the smoothing.  In locfit, the nn parameter controls the span of the neighborhood used, with smaller values allowing for less smoothing and more ""wiggling"", which helps to capture some areas on the grid where the probability of the binomial outcomes changes rapidly.  How would I go about setting up the gam model to enable it to behave similarly?</p>
"
"0.208927723509336","0.209343500484775"," 13172","<p>I would like to use a binary logistic regression model in the context of streaming data (multidimensional time series) in order to predict the value of the dependent variable of the data (i.e. row) that just arrived, given the past observations. As far as I know, logistic regression is traditionally used for postmortem analysis, where each dependent variable has already been set (either by inspection, or by the nature of the study). </p>

<p>What happens in the case of time series though,  where we want to make prediction (on the fly) about the dependent variable in terms of historical data (for example in a time window of the last $t$ seconds) and, of course, the previous estimates of the dependent variable?</p>

<p>And if you see the above system over time, how it should be constructed in order for the regression to work? Do we have to train it first by labeling, let's say, the first 50 rows of our data (i.e. setting the dependent variable to 0 or 1) and then use the current estimate of vector ${\beta}$ to estimate the new probability of the dependent variable being 0 or 1 for the data that just arrived (i.e. the new row that was just added to the system)?</p>

<p>To make my problem more clear, I am trying to build a system that parses a dataset row by row and tries to make prediction of a binary outcome (dependent variable) , given the knowledge (observation or estimation) of all the previous dependent or explanatory variables that have arrived in a fixed time window. My system is in Rerl and uses R for the inference. </p>
"
"NaN","NaN"," 17160","<p>Is there a convenient way in R to calculate a <a href=""http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section2"" rel=""nofollow"">probability prediction interval</a>
for a count sampled from Poisson($\lambda$), for known lambda? That is, for a given
$\alpha$, find $a$ and $b$ such that $P(a\le X\le b)\ge1-\alpha$,
where $X\sim\text{Pois}(\lambda)$. Is there a generic way to do this
in R for any of its built-in distributions? </p>
"
"0.140859042454753","0.141139359234409"," 20001","<p>I am trying to cross validate a logistic regression model with probability sampling weights (weights representing number of subjects in the population).  I am not sure how to handle the weights in each of the 'folds' (cross-validation steps).  I don't think it is as simple as leaving out the observations, I believe the weights need to be rescaled at each step.</p>

<p>SAS has an option in proc surveylogistic to get cross validated (leave one out) prediction probabilities.  Unfortunately I cannot find in the documentation any details on how these were calculated.  I would like to reproduce those probabilities in R.  So far I have not had success and am not sure if my approach is correct.  </p>

<p>I hope someone can recommend an appropriate method to do the cross validation with the sampling weights.  If they could match the SAS results that would be great too.</p>

<p>R code for leave-one-out cross validated probabilities (produces error):</p>

<pre><code>library(bootstrap)
library(survey)
fitLogistic = function(x,y){
  tmp=as.data.frame(cbind(y,x))
  dsn=svydesign(ids=~0,weights=wt,data=tmp)
  svyglm(y~x1+x2, 
         data=tmp,family = quasibinomial,design=dsn)
} 
predict.logistic = function(fitLog,x){
  pred.logistic=predict(fitLog,newdata=x,type='response')
  print(pred.logistic)
  ifelse(pred.logistic&gt;=.5,1,0)
} 
CV_Res= crossval(x=data1[,-1], y=data1[,1], fitLogistic, predict.logistic, ngroup = 13)
</code></pre>

<p>Sample Data Set:</p>

<pre><code>y   x1  x2  wt
0   0   1   2479.223
1   0   1   374.7355
1   0   2   1953.4025
1   1   2   1914.0136
0   0   2   2162.8524
1   0   2   491.0571
0   0   1   1842.1192
0   0   1   400.8098
0   1   1   995.5307
0   0   1   955.6634
1   0   2   2260.7749
0   1   1   1707.6085
0   0   2   1969.9993
</code></pre>

<p>SAS proc surveylogistic leave-one-out cross validated probabilities for sample data set:</p>

<p>.0072, 1 .884, .954, ...</p>

<p>SAS Code:</p>

<pre><code>proc surveylogistic;
model y=x1 x2;
weight wt;
output out=a2 predprobs=x;
run;
</code></pre>
"
"0.125988157669742","0.126238880619561"," 23248","<p>I have a problem of the form ""what is the probability that a user will 'like' a certain movie?"" For a bunch of users, I know the movies each has watched historically, and the movies each has liked. Additionally, for each movie I know the name of the director.</p>

<p>I calibrated a logistic regression for each user of the form:</p>

<p><code>glm(liked_by_user_1 ~ liked_by_user_2 + ... + liked_by_user_k + factor(director), family=binomial, data = subset(MovieWatchings, user_id == 1))</code></p>

<p>But my problem is: say that in the past, user 1 has watched movies from directors <code>D1</code> through <code>DM</code>, but next month <code>U1</code> watches a movie directed by <code>DN</code>? In that case the R <code>predict()</code> function will give an error, because the glm model for user 1 doesn't have an estimated parameter for the case of <code>director = DN</code>. But I must know something about <code>U1's</code> probability of liking the new movie, because I still know which other users have seen and liked this movie, and that has some predictive power.</p>

<p>How can I set up my model so that I can take into account other users' liking behavior, AND user 1's director preferences, but still have sensible predictions when user 1 sees his first movie from a new director? Is logistic regression even the right type of model for this case?</p>
"
"0.140859042454753","0.141139359234409"," 26568","<p>I would like to understand how to generate <em>prediction intervals</em> for logistic regression estimates. </p>

<p>I was advised to follow the procedures in Collett's <em>Modelling Binary Data</em>, 2nd Ed p.98-99. After implementing this procedure and comparing it to R's <code>predict.glm</code>, I actually think this book is showing the procedure for computing <em>confidence intervals</em>, not prediction intervals.</p>

<p>Implementation of the procedure from Collett, with a comparison to <code>predict.glm</code>, is shown below.</p>

<p>I would like to know: how do I go from here to producing a prediction interval instead of a confidence interval?</p>

<pre><code>#Derived from Collett 'Modelling Binary Data' 2nd Edition p.98-99
#Need reproducible ""random"" numbers.
seed &lt;- 67

num.students &lt;- 1000
which.student &lt;- 1

#Generate data frame with made-up data from students:
set.seed(seed) #reset seed
v1 &lt;- rbinom(num.students,1,0.7)
v2 &lt;- rnorm(length(v1),0.7,0.3)
v3 &lt;- rpois(length(v1),1)

#Create df representing students
students &lt;- data.frame(
    intercept = rep(1,length(v1)),
    outcome = v1,
    score1 = v2,
    score2 = v3
)
print(head(students))

predict.and.append &lt;- function(input){
    #Create a vanilla logistic model as a function of score1 and score2
    data.model &lt;- glm(outcome ~ score1 + score2, data=input, family=binomial)

    #Calculate predictions and SE.fit with the R package's internal method
    # These are in logits.
    predictions &lt;- as.data.frame(predict(data.model, se.fit=TRUE, type='link'))

    predictions$actual &lt;- input$outcome
    predictions$lower &lt;- plogis(predictions$fit - 1.96 * predictions$se.fit)
    predictions$prediction &lt;- plogis(predictions$fit)
    predictions$upper &lt;- plogis(predictions$fit + 1.96 * predictions$se.fit)


    return (list(data.model, predictions))
}

output &lt;- predict.and.append(students)

data.model &lt;- output[[1]]

#summary(data.model)

#Export vcov matrix 
model.vcov &lt;- vcov(data.model)

# Now our goal is to reproduce 'predictions' and the se.fit manually using the vcov matrix
this.student.predictors &lt;- as.matrix(students[which.student,c(1,3,4)])

#Prediction:
this.student.prediction &lt;- sum(this.student.predictors * coef(data.model))
square.student &lt;- t(this.student.predictors) %*% this.student.predictors
se.student &lt;- sqrt(sum(model.vcov * square.student))

manual.prediction &lt;- data.frame(lower = plogis(this.student.prediction - 1.96*se.student), 
    prediction = plogis(this.student.prediction), 
    upper = plogis(this.student.prediction + 1.96*se.student))

print(""Data preview:"")
print(head(students))
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by Collett's procedure:""))
manual.prediction
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by R's predict.glm:""))    
print(output[[2]][which.student,c('lower','prediction','upper')])
</code></pre>
"
"0.227128381289749","0.227580378515502"," 28738","<p>I have a data set which is highly imbalanced and I have used the SMOTE algorithm (using the R package DMwR) to balance the binary class in the data set. I have been using the R Ada package to then train an Ada Boost model on this data set to predict the binary class, with very good results.</p>

<p>In the same data set, I have another class variable which has multiple values (6 in total). In this case I realise that I can't use the AdaBoost algorithm as implemented in the ada package as it only deals with the binary case.</p>

<p>I therefore have 2 problems:</p>

<ol>
<li><p>I'd like to use the SMOTE algorithm on second class variable but this also only works with binary classes. Is there an algorithm or package I can use in R to ""rebalance"" a data set based on a class with multiple values in a similar way to SMOTE?</p></li>
<li><p>I'd like to use a classifier to predict the multiple class variable. I have tried using the one-vs-all approach with AdaBoost but I cannot get this to work well (my approach is below). Boosting seems to work well with this data set. Are there any other boosting algorithms or other approaches I could use in R that handle classes with multiple values. I have tried using Random Forest but one of my nominal inputs has too many discrete values to use it.</p></li>
</ol>

<p><strong>Approach for AdaBoost one-vs-all</strong></p>

<ul>
<li>Build a vector with a binary variable for each discrete class value</li>
<li>Train one AdaBoost model against each binary class vector</li>
<li>Generate probability prediction for each AdaBoost model</li>
<li>Select the class with the highest probability</li>
</ul>

<p>Many thanks</p>
"
"0.109108945117996","0.109326077561851"," 28819","<p>I have created a multiple linear regression model with R using <code>lm</code> and <code>glm</code>. I am using <code>lm</code> on a training set and <code>predict</code> on a testing set to validate the model. In one test my results are within 80% of what they should be for 80% of the cases. It correlates with 40% for one response variable and with 63% for another response variable (but the response variable with 63% correlation isn't near the actual values of the prediction). I have 53 predicates. What is the probability of that occurring randomly?
I've tried to build an multi-class svm off of the features using the predicates but so far the svm has been unable to properly predict the results.</p>
"
"0.26763218066403","0.268164782545377"," 40572","<p>I have a series of descriptors, some continuous, some discrete and an output variable which is dichotomous.</p>

<p>I have several parameters, but for the sake of simplicity let's say my data look like:</p>

<pre><code> Sex  |  Age  |  Genotype  | Dose 1  |  Dose  2  |  Outcome
------|-------|------------|---------|-----------|------------
  M   |  32   |    AABB    |   150   |     30    |    YES
  F   |  65   |    aaBb    |   110   |     30    |    YES
  M   |  42   |    AaBb    |   200   |     50    |    NO
...
</code></pre>

<p>I would like to make a predictive model to determine the optimal combination of <code>Dose 1</code> and <code>Dose 2</code> to have a desired outcome.</p>

<p>So my question is, if I have a new male subject of given genotype and age, what is the best combination of doses that will give a positive outcome with the highest probability? Or, to see things the other way around, given the other parameters, what are the odds of having a positive outcome with a given set of doses?</p>

<p>I thought I could use R to generate a linear model with <code>glm</code>, and then use <code>predict</code> to predict the outcome. However, I never really dealt with this type of problems before and I am a bit confused on how to use these functions and interpret their results. Also, I am not sure if this is the correct way to deal with the problem.</p>

<p>For instance, let's generate some random data:</p>

<pre><code>set.seed(12345)

num.obs &lt;- 50
sex &lt;- sample(c(""M"", ""F""), num.obs, replace=T)
age &lt;- sample(20:80, num.obs, replace=T)
genotype &lt;- sample(LETTERS[1:8], num.obs, replace=T)
dose.1 &lt;- sample(100:200, num.obs, replace=T)
dose.2 &lt;- sample(30:70, num.obs, replace=T)
outcome &lt;- sample(0:1, num.obs, replace=T)

data &lt;- data.frame(sex=sex, age=age, genotype=genotype,
              dose.1=dose.1, dose.2=dose.2, outcome=outcome)
</code></pre>

<p>Which gives 50 observation such as</p>

<pre><code>&gt; head(data)
  sex age genotype dose.1 dose.2 outcome
1   F  78        C    183     54       0
2   F  70        E    156     66       1
3   F  39        H    180     35       0
4   F  32        E    135     51       0
5   M  64        E    121     57       1
6   M  50        H    179     61       1
</code></pre>

<p>Now, I generate a model with</p>

<pre><code>model &lt;- glm(outcome ~ sex + age + genotype + dose.1 + dose.2, 
    data=data, family=""binomial"")
</code></pre>

<p>First question: without any <em>a priori</em> knowledge of the interactions between the descriptors, <strong>how do I choose the correct formula</strong>? Should I try various interactions and see which models gives the best fit e.g. looking at residual deviance or AIC? Are there functions to do this for me or should I try all of the combinations manually? </p>

<p>OK, let's say I found the model is good, now I use <code>predict</code></p>

<pre><code>new.data &lt;- list(sex=factor(""M"", levels=c(""M"", ""F"")), age=35, 
                 genotype=factor(""C"", levels=LETTERS[1:8]), 
                 dose.1=150, dose.2=30)
outcome &lt;- predict(model, new.data, se=T)
</code></pre>

<p>Which gives:</p>

<pre><code>$fit
        1 
-2.774538 

$se.fit
[1] 1.492594

$residual.scale
[1] 1
</code></pre>

<p>So... what do I do with this? <code>?predict.glm</code> says <code>$fit</code> is the prediction but obviously that is not a yes/no type of prediction... what I would ideally need is something on the lines of ""89% YES / 11% NO"".</p>

<p><strong>How do I interpret the result of <code>predict</code> and how would I go about having the type of result I want?</strong></p>

<p>Finally, <strong>are there functions to explore the parameter space so that I get a graph with the outcome in the dose1 vs dose2 space?</strong></p>

<p>EDIT: just to clarify: I do not have a specific reason to use a generalized linear model, it is just something that came to my mind as a possible solution. If anyone can think of other ways to solve this type of problem I would gladly welcome their suggestion!</p>
"
"0.125988157669742","0.126238880619561"," 49141","<p>My predictions coming from a logistic regression model (glm in R) are not bounded between 0 and 1 like I would expected. My understanding of logistic regression is that your input and model parameters are combined linearly and the response is transformed into a probability using the logit link function. Since the logit function is bounded between 0 and 1, I expected my predictions to be bounded between 0 and 1.</p>

<p>However that's not what I see when I implement logistic regression in R:</p>

<pre><code>data(iris)
iris.sub &lt;- subset(iris, Species%in%c(""versicolor"",""virginica""))
model    &lt;- glm(Species ~ Sepal.Length + Sepal.Width, data = iris.sub, 
                family = binomial(link = ""logit""))
hist(predict(model))
</code></pre>

<p><img src=""http://i.stack.imgur.com/0BHU5.png"" alt=""enter image description here""></p>

<p>If anything the output of predict(model) looks normal to me. Can anyone explain to me why the values I get are not probabilities?</p>
"
"0.199204768222399","0.19960119601395"," 55393","<p>I have a PDF (Probability Density Function) generated from a vector of 1,000,000 empirical values. This empirical PDF is heavily skewed to the right.</p>

<p>In this form, I can't make accurate predictions using a linear regression.</p>

<p>To fix this, is there some method to find the function F(x) to transform (i.e. ""squash"") the values in the vector into a standard normal distribution, so I can feed said transformed vector into a linear regression?</p>

<p>Of course, this would also involve finding the inverse of F(x) that transforms (i.e. ""de-squashes"") any predictions back into the original empirical PDF.</p>

<p><strong>What I have tried</strong></p>

<p>So far, I have managed to generate the density function from the empirical data:</p>

<p><img src=""http://i.stack.imgur.com/HIBUP.png"" alt=""enter image description here""></p>

<p>Here is the R code:</p>

<pre><code>par(mfrow=c(2,1))

install.packages(""bootstrap"")
library(bootstrap)
data(stamp)
nobs &lt;- dim(stamp)[1]
hist(stamp$Thickness,col=""grey"",breaks=100,freq=F)
	dens &lt;- density(stamp$Thickness)
lines(dens,col=""blue"",lwd=3)

plot(density(stamp$Thickness),col=""black"",lwd=3, main=""Simulation to choose density plot"")
	for(i in 1:10)
	{
		newThick &lt;- rnorm(nobs,mean=stamp$Thickness,sd=dens$bw*1.5)
		lines(density(newThick,bw=dens$bw),col=""grey"",lwd=3)
}

# If I wanted to do a linear regression to predict stamp thickness,
# what is the function F(x) to ""squash"" (i.e. transform) the ""stamp""
# vector into a normal distribution, and the corresponding inverse 
# function Finv(x) to ""desquash"" (i.e. untransform) any predictions back 
# into the original prediction?
</code></pre>

<p><strong>Update 1</strong></p>

<p>@Andre Silva sugggested that:</p>

<blockquote>
  <p>What need to have normal distribution are the residuals (predicted
  versus observed) derived from your (multiple) linear regression model.</p>
</blockquote>

<p>According to <a href=""http://www.stat.yale.edu/Courses/1997-98/101/linmult.htm"" rel=""nofollow"">post on Multiple Linear Regression</a>:</p>

<blockquote>
  <p>After fitting the regression line, it is important to investigate the
  residuals to determine whether or not they appear to fit the
  assumption of a normal distribution. A normal quantile plot of the
  standardized residuals y -  is shown to the left. Despite two large
  values which may be outliers in the data, the residuals do not seem to
  deviate from a random sample from a normal distribution in any
  systematic manner.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/3ybm0.gif"" alt=""enter image description here""></p>

<p><strong>Update 2</strong></p>

<p>See <a href=""http://stats.stackexchange.com/questions/11351/left-skewed-vs-symmetric-distribution-observed/11352#11352"">Left skewed vs. symmetric distribution observed</a> for R code that illustrates that the only relevant concern is if the residuals are normally distributed.</p>
"
"0.199204768222399","0.179641076412555"," 63128","<p>I am working on BASEL II IRB models and we have to estimate loss based on historic defaults.  </p>

<p>There are different outcomes/scenarios we have identified that a default can encounter that will affect the LGD like CURE, FORCED SALE, LOSS, PARTIAL CURE.  We have the data needed to be able to allow the classification to happen, but I am unsure of what form of analysis is best suited to process this data and produce the necessary segmentations and subsequent estimates of loss for each one.</p>

<p>We have data currently in a format with one row per month per default for every month including and since the account first defaulted.  A number of scenarios involve activity over a number of months for a defaulted account. </p>

<p>I am unsure what form of analysis I shoud be performing.  I considered classification trees but as a criteria can span multiple months (in rows) I don't think this would work and I also considered time series analysis but this is a new area for me and I'm not sure whether it can pull out events.</p>

<p>What form of analysis should I be using and are there any specific resources for it in R that you think would be helpful?</p>

<p><Br><br><br>
<strong>UPDATE</strong></p>

<p>I have reduced the dataset to a series of 6 month snapshots and classifed each account at each position such that I have a dataset like</p>

<p><code>ID,Age,LGDstatus,LTV,BTV,[OTHER CHARACTERISTICS],Bal,BalChangeSinceInception</code></p>

<p>where a predictive model needs to be built such that we can predict how much BalChangeSinceInception is expected for each LGDstatus at each point, and the likelihood that an account will go to the various LGDstatus.</p>

<p>Simplisticly then I could build a prediction which says...
An account has gone into default, based on these x factors, by month Y we expect a loss of z% based on an a% probability of experiencing LOSS and a b% probability of experiencing a CURE</p>

<p>However, that may be the wrong approach and direction would be invaluable.</p>
"
"0.208927723509336","0.209343500484775"," 64634","<p>I am new to the <code>gamlss</code> package and would like to check that I am using the correct family for proportion data (tree species cover after treatment), which is bounded between zero and one. According to the documentation, the correct distribution family for data of this type is BEINF (Beta inflated), since it allows both 0 and 1 values. The data are highly zero-inflated, with 43/82 observations having a zero response, and 3/82 values at 1. I run the following model:</p>

<pre><code>m1 &lt;- gamlss(y ~ x1 + x2, 
sigma.formula=~1, 
nu.formula=~x1 + x3 + x4, 
tau.formula=~x5, 
family=BEINF, 
data=df)
</code></pre>

<p>The mean response values (and SEs) given by:</p>

<pre><code>pred &lt;- predict(m1, type='response', se.fit=T)
</code></pre>

<p>seem reasonable. </p>

<p>I am also interested in the probability of obtaining a zero response (i.e. the probability of having no individuals of the target species post-treatment). However, when i try to extract the fitted values of the nu parameter (which I believe to be the probability of obtaining a zero value), using:</p>

<pre><code>prednu &lt;- predict(m1, type=""response"", what=""nu"")
</code></pre>

<p>I am getting predicted values of the response in the range 0.01-44.6, which I find strange. I have tried this for both the model-fitting data and new data, with the same result. However, when I use <code>family=ZAGA</code> (i.e. zero adjusted Gamma distribution which allows for a response with no upper bound), I get predictions between 0 and 1 for the response for the nu parameter, which seem more reasonable.</p>

<p>I therefore have 3 questions about my approach:</p>

<ol>
<li><p>Is BEINF the correct choice of distribution family for zero-inflated proportion data bounded between zero and one, and including both zeros and one values?</p></li>
<li><p>Does <code>predict(m1, type=""response"", what=""nu"")</code> give the probability of obtaining a zero response?</p></li>
<li><p>Why would <code>predict(m1, type=""response"", what=""nu"")</code> give values far outside the $[0,1]$ bounded range of the response variable?</p></li>
</ol>

<p>We are happy to provide data if that would be helpful.</p>

<p>Any assistance you can provide regarding the correct use of these gamlss functionalities with my dataset would be greatly appreciated.
Kind regards,</p>

<p>PS â€“ This question has been sent to GAMLSS team a week ago but hasnâ€™t been answered yet.</p>
"
"0.188982236504614","0.189358320929341"," 65690","<p>I fit a logistic on three numeric continuous variables, followed by a categorical factor [Y, N].</p>

<pre><code>logit2A &lt;- glm(DisclosedDriver ~ VehDrvr_Dif+POL_SEQ_NUM+PRMTOTAL+SAFE_DRVR_PLEDGE_FLG, data = DF, family = ""binomial"") 
</code></pre>

<p>Fit looks wonderful.</p>

<pre><code>Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -2.204e+00  2.253e-01  -9.782  &lt; 2e-16 ***
VehDrvr_Dif            2.918e-01  1.026e-01   2.845 0.004440 ** 
POL_SEQ_NUM           -1.893e-01  5.617e-02  -3.370 0.000751 ***
PRMTOTAL               1.109e-04  5.526e-05   2.006 0.044804 *  
SAFE_DRVR_PLEDGE_FLGY -7.220e-01  1.633e-01  -4.422 9.76e-06 ***
</code></pre>

<p>So obviously R took the Safe_Drvr_Pledge_Flg categorical factor variable and placed all 'N' values in reference or intercept as opposed to the listed 'Y'.</p>

<p>Now I want to take my fit and calculate the probabilities that my model determines. And here comes the error:</p>

<pre><code>&gt; DF$P_GLM&lt;- predict.glm(logit2A, DF, type=""response"", se.fit=FALSE)
    Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
factor SAFE_DRVR_PLEDGE_FLG has new levels 
</code></pre>

<p>Umm... no it doesn't, because I just fit the model with the exact same data I'm trying to use for the prediction. What's the problem?</p>

<p>Trying to respond to first comment:
Don't know what you mean. I've got 3500 rows of data... It's a logistic regression on 4 continuous variables and one categorical. The categorical has two values, Y or N. My glm fit give the numbers given. I just want to plug it all back in with the predict function and it gives me that error. Here's the categorical variable:</p>

<pre><code> &gt; DF$SAFE_DRVR_PLEDGE_FLG
 [1] Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y Y N Y Y Y Y Y N Y Y Y Y Y Y
 [60] Y Y Y Y N Y Y Y Y Y Y Y Y N Y Y Y N N Y N Y Y Y Y Y N Y Y N Y N N Y Y Y N Y Y Y Y N Y Y Y Y Y N Y N Y N Y Y Y Y Y N Y
 [119] N Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y N Y Y Y N Y Y Y N N Y N N N Y N Y Y Y N N Y Y N Y Y Y Y N N Y Y Y Y N N Y N N
 Levels:  N Y
</code></pre>

<p>What do you mean by a working example? The fit works. The probability output of the predict function doesn't...</p>
"
"0.154303349962092","0.154610421609022"," 66299","<p>I currently have a gradient boosting model that uses the <code>gbm</code> package in R that classifies observations at the end of a year. Daily behaviors are logged for each observation (with most of them zero). The model is built on observations which have a year's worth of complete data and performs well out of sample. </p>

<p>For illustrative purposes, assume the complete data look like this:</p>

<pre><code>id day1 day2 day3 day4 day5, ..., day365
 1    1    1    2    0    0, ...,      0
 2    3    0    1    0    0, ...,      0
 3    1    1    2    0    0, ...,      0
 4    1    0    0    0    0, ...,      1
</code></pre>

<p>However, I want to predict the probability of each class for new individuals who have not yet been in the sampling frame for a year. Thus, all of the days past the present day will be NA. This precludes the use of many algorithms, but treating them as 0s also implies having more information than I actually have.</p>

<p>What is the best way to treat the unobserved, but observable missing data when making predictions? I will not be re-estimating the model on a daily basis but rather using the model fit from the existing training set.</p>
"
"0.109108945117996","0.109326077561851"," 67470","<p>I want to predict a categorical variable using also categorical predictors. Currently, I am looking at classification and regression trees (CART).</p>

<p>The prediction quality is ""good enough"", except for the presence of impossible combinations. In the following minimal example, the combination <code>a==2, b==2</code> is impossible, yet the estimation decides not to use <code>b</code> for splitting.</p>

<pre><code>&gt; library(rpart)
&gt; d &lt;- data.frame(a=rep(factor(c(1,1,2)), 100000), b=factor(c(1,2,1)))
&gt; xtabs(~., d)
   b
a       1     2
  1 1e+05 1e+05
  2 1e+05 0e+00
&gt; (tr &lt;- rpart(a~b, d))
n= 300000 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 300000 1e+05 1 (0.6666667 0.3333333) *
</code></pre>

<p>When simulating stochastically from this model (by choosing the leaf value by sampling using the annotated probability vector, here $(2/3, 1/3)$, as weights), the combination <code>2, 2</code> will occur:</p>

<pre><code>&gt; prob.m &lt;- predict(tr, d, type=""prob"")
&gt; d$a.sim &lt;- apply(prob.m, 1, function(x) sample.int(length(x), size=1, prob=x))
&gt; xtabs(~a.sim+b, d)
     b
a.sim      1      2
    1 133041  66615
    2  66959  33385
</code></pre>

<p>Is there a way to avoid this, perhaps using another method?</p>

<p>This is just a small example for a more general case. I have around 10 predictors, and I want to exclude all combinations of two (or perhaps three) attributes that have no observation in the sample.</p>

<p>I am aware of the ""loss matrix"" that can be specified as a parameter to <code>rpart</code>, but this is prohibitive if many predictors are used.</p>
"
"0.17817416127495","0.178528737070981"," 82659","<p>I am new to survival analysis and I've recently learned that there are different ways to do it given a certain goal. I am interested in actual implementation and appropriateness of these methods.</p>

<p>I was presented with the traditional <em>Cox Proportional-Hazards</em>, <em>Accelerated failure time models</em> and <em>neural networks</em> (multilayer perceptron) as methods to get survival of a patient given their time, status and other medical data. The study is said to be determined in five years and the goal is to give survival risks each year for new records to be given.</p>

<p>I found two instances where other methods where chosen over the Cox PH:</p>

<ol>
<li><p>I found ""<a href=""http://stats.stackexchange.com/a/79375/37466""><em>How to get predictions in terms of survival time from a Cox PH model</em></a>"" and it was mentioned that:</p>

<blockquote>
  <p>If you are particularly interested in obtaining estimates of the probability of survival at particular time points, I would point you towards <strong>parametric survival models (aka accelerated failure time models)</strong>. These are implemented in the survival package for R, and will give you parametric survival time distributions, wherein you can simply plug in the time you are interested in and get back a survival probability.</p>
</blockquote>

<p>I went to the recommended site and found one in the <a href=""http://cran.r-project.org/web/packages/survival/survival.pdf""><code>survival</code> package</a> - the function <code>survreg</code>. </p></li>
<li><p>Neural networks were suggested in <a href=""http://stats.stackexchange.com/questions/80049/obtaining-r-pec-survival-patient-risk-percentage#comment156911_80049"">this comment</a>:</p>

<blockquote>
  <p>... One advantage of neural net approaches to survival analysis is that they do not rely on the assumptions that underlie Cox analysis...</p>
</blockquote>

<p>Another person with the question ""<a href=""http://stats.stackexchange.com/questions/81261/r-neural-network-model-with-target-vector-as-output-containing-survival-predicti""><em>R neural network model with target vector as output containing survival predictions</em></a>"" gave an exhaustive way of determining survival in both neural networks and Cox PH.</p>

<p>The R code for getting the survival would be like this:</p>

<pre><code>mymodel &lt;- neuralnet(T1+T2+T3+T4+T5~covar1+covar2+covar3+..., data=mydata, hidden=1)
compute(mymodel,data=mydata)
</code></pre></li>
<li><p>I went to the R forums and found <a href=""http://r.789695.n4.nabble.com/predict-coxph-and-predict-survreg-tp3037408p3038065.html"">this answer in the question ""<em>predict.coxph and predict.survreg</em>""</a>:</p>

<blockquote>
  <p>Indeed, from the <code>predict()</code> function of the <code>coxph</code> you cannot get 
  directly ""time"" predictions, but only linear and exponential risk  scores. This is because, in order to get the time, a baseline hazard has to be computed and it is not straightforward since it is implicit in the Cox model. </p>
</blockquote></li>
</ol>

<p>I was wondering if which of the three (or two considering the arguments over Cox PH) is best for getting survival percentages for time periods of interest? I am confused which of them to use in survival analysis.</p>
"
"0.109108945117996","0.109326077561851"," 83576","<p>I have a feature x, that I use to predict a probability y.</p>

<hr>

<p><strong>Some background on (x,y)</strong></p>

<p>I can't go into too much details, but hopefully the following should be enough to explain what x and y are, at least conceptually <em>[square and circles are NOT the actual label I am working with]</em>:</p>

<p><strong>y</strong></p>

<p>y is the probability of an image being of Class 0 or 1, with: </p>

<ul>
<li>Class 0 means that the image contains a <em>square</em>.</li>
<li>Class 1 means that the image contains a <em>circle</em>.</li>
</ul>

<p>100 people watched the training images, and classified them.
y is the result probability, so y=0 means there is definitely a square, y=1 means there is definitely a round.</p>

<p><strong>x</strong></p>

<p>x is a feature derived from the images, by <em>trying to fit them to a model of a circle</em>, and calculating the error.
So for example when x is very low, the probability of the image having a circle is high (relatively).</p>

<hr>

<p>plot(x,y)</p>

<p><img src=""http://i.stack.imgur.com/05230.png"" alt=""enter image description here""></p>

<p>x,y (1000 values for each) pasted here:
<a href=""http://tny.cz/c320180d"" rel=""nofollow"">http://tny.cz/c320180d</a></p>

<p>Using mean(y) as a predictor, I get <strong>RMSE = 0.285204</strong>:</p>

<pre><code>N = length(x)
average = mean(y)
RMSE = sqrt( 1/N * sum( (average-y)^2 ) )
RMSE
[1] 0.285204
</code></pre>

<p>Then using a linear regression on log(x), I could improve a little bit the <strong>RMSE = 0.2694513</strong>:</p>

<pre><code>log_x = log(x)
plot(log_x,y)
lm.result = lm(formula = y ~ log_x)
abline(lm.result, col=""blue"") # not working very well
linear_prediction = predict( lm.result, new, se.fit = TRUE)
prediction_linear_regression = matrix(0,N,1)
prediction_linear_regression = linear_prediction$fit
RMSE_linear_regression = sqrt( 1/N * sum( (prediction_linear_regression-y)^2 ) )
RMSE_linear_regression
[1] 0.2694513
</code></pre>

<p><img src=""http://i.stack.imgur.com/59Etc.png"" alt=""enter image description here""></p>

<p>Can the RMSE be further improved? What should I try?</p>
"
"0.166666666666667","0.166998341953264"," 90347","<p>I am trying to predict species presence or absence using randomForest in R (classification). In fact, I am trying to do it for several species, in separate models. </p>

<p>For a couple of the species, the training data are quite unbalanced e.g., 70 observations of species presence, and and 6500 observations of species absence. </p>

<p>This is my code: </p>

<pre><code>#read in data frame containing observations of species presence/absence and predictor     variables
mydata &lt;- read.csv('mydata.csv')

#fit random forests model
fitmodelA &lt;- randomForest(SPECIESA ~ var1 + var2 + var3 + var4 + var5 +var6 + var7 +   var8 + var9 + var10, data=mydata, mytry=3, ntrees=500, replace=TRUE, importance=TRUE,   keep.forest=TRUE)

#predict to new data
predictmodel &lt;- predict(fitmodelA, newdata, type=""prob"")
</code></pre>

<p>In the output prediction, almost the entire study area is predicted with prob > 0.7. I take this to be predictions of species occurrence? or is it the probability of species absence? </p>

<p>I want to try to balance the data by forcing the model to select equal sample sizes from observations of presence and absence, e.g., adding the argument </p>

<pre><code>sampsize(70,70)
</code></pre>

<p>But I get the error message ""Error in if (ncol(x) != ncol(xtest)) stop(""x and xtest must have same number of columns"")"" </p>

<p>What am I doing wrong here?  </p>
"
"0.17817416127495","0.178528737070981"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"0.227128381289749","0.227580378515502"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.140859042454753","0.141139359234409"," 99862","<p>Here's my situation.  </p>

<p>I have a multiple linear regression which I've used to come up with a prediction interval to predict a value y for a given (x1,x2,x3,x4,x5,x6).   It reads something like lower: 30, upper:48.  </p>

<p>I also have the same exact thing to predict a value y* at another given (x1*,x2*,x3*,x4*,x5*,x6*).  It reads something like lower:35, upper:51. </p>

<p>I want to answer this question:<br>
What is the probability that the value y* is greater than the value y?</p>

<p>I think it's a basic question, but I'm not sure. 
I could likely come up with this probability if I knew the formula for how the prediction interval is calculated in a multi-variable situation.<br>
Here's what I think should be done, but I wanted to run it by you guys first. </p>

<p>Prediction Intervals are based on a t-distribution with (n-6) degrees of freedom (I have a forced 0 y-int).  So I believe the margin of error calculated is then some constant multiplied by the corresponding value from the t-distribution (t_.05/2 with n-6 degrees of freedom).  The ""some constant"" would be the standard error of this particular estimate. </p>

<p>I then just do a basic 2 sample t-test using the point estimate prediction as the means and these constants as the standard errors with my n-6 degrees of freedom.   Is this accurate? </p>

<p>Is there a better way?</p>

<p>Thanks</p>
"
"0.0890870806374748","0.0892643685354904","105427","<p>Is there a way to train a RandomForest, GBM or other classification model using a LogLoss error measure?</p>

<p>So far I have trained it, but I get an OOB error rate. From there I calculate the sigmoid and use that as my probability in order to minimize the LogLoss... but I have a hard time to find literature or examples on how to optimize an algorithm training using log loss.</p>

<p>Here is the LogLoss function in R:</p>

<pre><code>llfun &lt;- function(actual, prediction) {
    epsilon &lt;- .000000000000001
    yhat &lt;- pmin(pmax(prediction, epsilon), 1-epsilon)
    logloss &lt;- -mean(actual*log(yhat)
                     + (1-actual)*log(1 - yhat))
    return(logloss)
}
</code></pre>
"
"0.218217890235992","0.218652155123701","109230","<p>What I am trying to do is to fit a log-normal distribution to a data-set, and then determine confidence and prediction intervals for the fitted distribution - not just for the mean and sd estimates.</p>

<p>My final goal is to be able to say that if we repeated a set of measurements, then 95 % of the values would fall below some specific value. I can obviously do that from the fitted distribution itself (or rather the cumulative version), but I'm thinking that will under-estimate the probability because I need to include the variability in the estimates and/or fit itself first?</p>

<p>If I use the following code:</p>

<pre><code>require(MASS)
set.seed(123)
x&lt;-rlnorm(100)
fit&lt;-fitdistr(x,""lognormal"")
</code></pre>

<p>then R will calculate a log-normal distribution fitted to my data. The fitdistr function will return the estimated mean and sd (along with standard errors for these estimates).</p>

<pre><code>   meanlog       sdlog   
 0.09040591   0.90824033 
(0.09082403) (0.06422229)
</code></pre>

<p>I understand that these will then allow me to plot the fitted distribution (and histogram) using ggplot2 with the following code:</p>

<pre><code>meanlog&lt;-fit$estimate[[1]]
    sdlog&lt;-fit$estimate[[2]]
binwidth&lt;-abs(max(x)-min(x))/20

qplot(x,geom=""blank"")+geom_histogram(binwidth=binwidth,aes(y= ..density..))+stat_function(fun=dlnorm,arg=list(meanlog=meanlog,sdlog=sdlog),colour=""red"")
</code></pre>

<p>However, what I really want to do is to plot the confidence interval and/or prediction interval of this fitted distribution. Something similar to how ggplot2 does with stat_smooth, something like:</p>

<pre><code>x&lt;-seq(1,100)
y&lt;-x+rnorm(x,sd=10)
qplot(x,y,geom=""point"")+stat_smooth(method='lm',se=T)
</code></pre>

<p>I can use confint(fit) to extract the confidence intervals for the estimated mean and sd, but I think I misunderstand the maths because I can't for the life of me work out how to use those in order to be able to calculate the confidence interval for the actual distribution. So neither can I work out the prediction interval. I've tried writing my own function for the log-normal distribution to input various combinations of the confidence intervals from confint manually - but that doesn't work. Obviously a confidence interval of the estimate does not directly give you the confidence interval of the line. And, therefore, neither the prediction interval.</p>

<p>I would really appreciate anyone who can walk me through this please!</p>
"
"0.109108945117996","0.109326077561851","109822","<p>I have a two-dimensional predictor plane, 0-1-Observations and a priori knowledge of minimal probabilities per combination of the predictors. I would like to fit a model (e.g. GAM from the mgcv package) that yields predictions that lie above the minimal values.</p>

<p>The problem is how to specify the model. The following is a toy example where the minimal probability is 0.5.</p>

<pre><code>rm(list=ls())
require(mgcv)
set.seed(1)
l&lt;-1000
x&lt;-seq(0,1,l=l)
p&lt;-0.5
y&lt;-as.numeric(runif(length(x))&lt;1-x^4)

df&lt;-data.frame(x=x,y=y,min=p)
m1&lt;-gam(y~s(x)+min,family=binomial(link=""logit""),data=df)
predict(m1,newdata=data.frame(x=1,min=0.5),type=""response"")
</code></pre>

<p>I found that offset does not help.</p>

<p>Subtracting the minimal probability from the response does not help as this yields negative responses which do not make sense.</p>

<p>How can I adapt the model to accomodate for the given minimal value?</p>
"
"0.154303349962092","0.154610421609022","109851","<p>I am using logistic regression to predict likelihood of an event occurring. Ultimately, these probabilities are put into a production environment, where we focus as much as possible on hitting our ""Yes"" predictions. It is therefore useful for us to have an idea of what definitive ""hits"" or ""non-hits"" might be <em>a priori</em> (before running in production), in addition to other measures we use for informing this determination.</p>

<p>My question is, what would be the proper way to predict a definitive class (1,0) based on  the predicted probability? Specifically, I use R's <code>glmnet</code> package for my modeling. This package arbitrarily picks .5 probability as threshold for a yes or no. I believe that I need to take the results of a proper scoring rule, based on predicted probabilities, to extrapolate  to a definitive class. An example of my modeling process is below:</p>

<pre><code>mods &lt;- c('glmnet', 'scoring')
lapply(mods, require, character.only = T)

# run cross-validated LASSO regression
fit &lt;- cv.glmnet(x = df1[, c(2:100)]), y = df1[, 1], family = 'binomial', 
type.measure = 'auc')

# generate predicted probabilities across new data
df2$prob &lt;- predict(fit, type=""response"", newx = df2[, c(2:100)], s = 'lambda.min')

# calculate Brier score for each record
df2$propscore &lt;- brierscore(df2[,1] ~ df2$prob, data = df2)
</code></pre>

<p>So I now have a series of Brier scores for each prediction, but then how do I use the Brier score to appropriately weight each likelihood being a yes or no?</p>

<p>I understand that there are other methods to make this determination as well, such as Random Forest.</p>
"
"0.140859042454753","0.141139359234409","113309","<p>Objective: To predict time to event of a customer using R
I developed a Non parametric model but as per my understanding we cannot perform predictions from these models, so I have to build a model which can predict for any test data. I hope extended cox model will work for my case (we have time depended co variants)</p>

<p>Questions:</p>

<ol>
<li><p>How the data structure would be for extended cox model?</p></li>
<li><p>What we will predict from Cox model (In logistic model we will predict probability in the similar lines what is for survival model)?</p></li>
<li>Variable Importance</li>
<li><p>Can we predict time when a customer will leave from the study?</p></li>
<li><p>How to validate the model?</p></li>
</ol>

<p>It would be great if someone provide the R code for the above questions 2, 3, and 4</p>

<p>Thanks in advance.</p>
"
"0.140859042454753","0.141139359234409","113373","<p>My name is Abhi and I am trying to understand the difference between predict and prediction. </p>

<p>I am using the r language and my ide is rstudio. I have created a random forest model (r package randomForest)</p>

<pre><code>myModel &lt;- randomForest(Survived ~ .,data = modelData[,-1],importance = T)
modelResponses = predict(model,type = ""prob"") # I am guessing this gives probability of survival for each passenger 
temp1 = modelResponses[,2]
pred = prediction(temp1,trainData$Survived) #Not Sure whats is the pred object 
</code></pre>

<p>Now here are my questions </p>

<ol>
<li>What is the pred object?</li>
<li>I have seen some code which uses the pred object to plot the auc curve. I know temp1 is the probability of survival for each record. Say the probability of survival for a particular record is 0.55. How does the prediction function know to classify this as survived or not-survived?</li>
<li>How do I use this model to classify new data. Until now I was using <code>modelResponses = predict(model,type = ""prob"")</code> , but now I am not so sure. Again the same confusion as item 2, how does the system determine the best cut off point for probabilities. </li>
</ol>

<p>Thanks a lot guys. Any help would be much appreciated.</p>

<p>Regards,</p>
"
"0.208927723509336","0.209343500484775","121383","<p>I have read through other topics on partial dependence plots and most of them are on how you actually plot them with different packages, not how you can accurately interpret them, So: </p>

<p>I have been reading into and creating a fair amount of partial dependence plots. I know they measure the marginal effect of a variable Ï‡s on the function Æ’S (Ï‡S ) with the average affect of all other variables (Ï‡c) from my model. Higher y values mean they have a greater influence on accurately predicting my class. However, I'm not satisfied with this qualitative interpretation.</p>

<p><a href=""http://i.stack.imgur.com/UEu6F.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UEu6F.png"" alt=""This link shows one of my many plots. http://imgur.com/RXqlOky""></a></p>

<p>My model (random forest) is predicting two discreet classes. ""Yes trees"" and ""No trees"". TRI is a variable that has proven to be a good variable for this.</p>

<p>What I began to think is the Y value is showing a probability for correct classification.
Example:
y(0.2) is showing that TRI values of > ~30 have a 20% chance of correctly identifying a True Positive classification.</p>

<p>Where conversely</p>

<p>y(-0.2) is showing that TRI values of &lt; ~15 have a 20% chance of correctly identifying a True Negative classification.</p>

<p>General interpretations that are made in the literature would sound like this ""Values greater than TRI 30 begin to have a positive influence for classification in your model"" and that's it. It sounds so vague and pointless for a plot that can potentially speak so much about your data.</p>

<p>Also, all of my plots cap out at -1 to 1 in range for the y axis. I have seen other plots that are -10 to 10 etc. Is this a function of how many classes you are trying to predict?</p>

<p>I was wondering if anyone can speak to this problem. Maybe show me how I should be interpreting these plots or some literature that can help me out. Maybe I am reading too far into this?</p>

<p>I have read very thoroughly The elements of statistical learning: data mining, inference and prediction and it has been a great starting point but that's about it.</p>
"
"0.227128381289749","0.227580378515502","122212","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 0-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>Using these models, given the dichotomous dependent variable, I have built a logistic regression using lrm.</p>

<p>The method of model variable selection was based on existing clinical literature modelling the same diagnosis. All have been modelled with a linear fit with the exception of ISS which has been modelled traditionally through fractional polynomials. No publication has identified known significant interactions between the above variables.</p>

<p>Following advice from Frank Harrell, I have proceeded with the use of regression splines to model ISS (there are advantages to this approach highlighted in the comments below). The model was thus pre-specified as follows:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ Age + GCS + rcs(ISS) +
    Year + inctoCran + oth, data = ASDH_Paper1.1, x=TRUE, y=TRUE)
</code></pre>

<p>Results of the model were:</p>

<pre><code>&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Age + GCS + rcs(ISS) + Year + inctoCran + 
    oth, data = ASDH_Paper1.1, x = TRUE, y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          2135    LR chi2     342.48    R2       0.211    C       0.743    
 0            629    d.f.             8    g        1.195    Dxy     0.486    
 1           1506    Pr(&gt; chi2) &lt;0.0001    gr       3.303    gamma   0.487    
max |deriv| 5e-05                          gp       0.202    tau-a   0.202    
                                           Brier    0.176                     

          Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept -62.1040 18.8611 -3.29  0.0010  
Age        -0.0266  0.0030 -8.83  &lt;0.0001 
GCS         0.1423  0.0135 10.56  &lt;0.0001 
ISS        -0.2125  0.0393 -5.40  &lt;0.0001 
ISS'        0.3706  0.1948  1.90  0.0572  
ISS''      -0.9544  0.7409 -1.29  0.1976  
Year        0.0339  0.0094  3.60  0.0003  
inctoCran   0.0003  0.0001  2.78  0.0054  
oth=1       0.3577  0.2009  1.78  0.0750  
</code></pre>

<p>I then used the calibrate function in the rms package in order to assess accuracy of the predictions from the model. The following results were obtained:</p>

<pre><code>plot(calibrate(rcs.ASDH, B=1000), main=""rcs.ASDH"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/HYTsp.png"" alt=""Bootstrap calibration curves penalized for overfitting""></p>

<p>Following completion of the model design, I created the following graph to demonstrate the effect of the Year of incident on survival, basing values of the median in continuous variables and the mode in categorical variables:</p>

<pre><code>ASDH &lt;- Predict(rcs.ASDH, Year=seq(1994,2013,by=1),Age=48.7,ISS=25,inctoCran=356,Other=0,GCS=8,Sex=""Male"",neuroYN=1,neuroFirst=1)
Probabilities &lt;- data.frame(cbind(ASDH$yhat,exp(ASDH$yhat)/(1+exp(ASDH$yhat)),exp(ASDH$lower)/(1+exp(ASDH$lower)),exp(ASDH$upper)/(1+exp(ASDH$upper))))
names(Probabilities) &lt;- c(""yhat"",""p.yhat"",""p.lower"",""p.upper"")
ASDH&lt;-merge(ASDH,Probabilities,by=""yhat"")
plot(ASDH$Year,ASDH$p.yhat,xlab=""Year"",ylab=""Probability of Survival"",main=""30 Day Outcome Following Craniotomy for Acute SDH by Year"", ylim=range(c(ASDH$p.lower,ASDH$p.upper)),pch=19)
arrows(ASDH$Year,ASDH$p.lower,ASDH$Year,ASDH$p.upper,length=0.05,angle=90,code=3)
</code></pre>

<p>The code above resulted in the following output:</p>

<p><img src=""http://i.stack.imgur.com/KGYcz.png"" alt=""Year trend with lower and upper""></p>

<p><strong><em>My remaining questions are the following:</em></strong></p>

<p><strong>1. Spline Interpretation</strong> - How can I calculate the p-value for the splines combined for the overall variable?</p>
"
"0.356348322549899","0.357057474141961","123123","<p>Say someone who is well practiced (appears to have reached a performance plateau) shoots 20 free throws on 15 different days and is successful the number of times shown in the upper histogram (<code>dat</code> in the code). </p>

<ol>
<li><p>My understanding is that the distribution of outcomes should be predicted by the binomial distribution. Is this correct?</p></li>
<li><p>The expected variance is $np(1-p)$, where $n = 20$ (the number of trials per session) and $p = {\rm mean}/n =.65$ or the average percent of successes.</p></li>
<li><p>I could not figure out how to theoretically calculate the distribution of <em>sample variances</em>, so ran a Monte Carlo simulation. These results are shown in the lower panel. the mean of these variances matches with the theoretical expected variance, but the variance of the data is much less.</p></li>
</ol>

<p><strong>R code:</strong></p>

<pre><code>dat  &lt;- c(12,12,13,12,13,12,12,14,13,13,14,13,14,13,14)
n    &lt;- 20
p    &lt;- mean(dat)/n
Nobs &lt;- length(dat)

sim.vars = matrix(nrow=10000)
for(s in 1:10000){
  sim.vars[s] &lt;- var(rbinom(Nobs, n, p))
}
par(mfrow=c(2,1))
hist(dat,      breaks=seq(0,20,by=1))
hist(sim.vars, breaks=20)


&gt; var(dat)       # Variance of Data
[1] 0.6380952
&gt; n*p*(1-p)      # Expected Variance given binomial model
[1] 4.569778
&gt; mean(sim.vars) # Mean of simulated sample variances
[1] 4.542159
</code></pre>

<p><img src=""http://i.stack.imgur.com/1arqJ.png"" alt=""enter image description here""></p>

<p>@Whuber, I hit enter when the cursor was outside the text box and it submitted before completing the question. I apologize. The first thing I wanted to know if I have made some error anywhere in my thinking (choice of binomial model, simulation, calculation), which your comment suggests I have not. </p>

<p>The second is what processes could possibly generate such data? I have >30 like this from multiple sources, so it is probably not data entry error or made up data. The actual task is not shooting free throws but you can take my word that it really is an equivalent situation. </p>

<p>This peculiarity of the data has not been noted previously. Others have interpreted such data as representing the max performance level achieved, and compared group averages under different conditions. Difference between individuals have been interpreted as differences in skill level, somehow related to neurological characteristics. As far as I can tell, this interpretation (as plateau/ asymptote/ max performance) implies sampling from a binomial distribution, which is really inconsistent with the underdispersion.</p>

<p>An analogous situation would be someone flipping a coin 20 times and always getting 9/10/11 heads. This is too consistent. The only mechanism I have thought of is introducing negative correlation between consecutive trials. Something like:</p>

<pre><code>if(dat[t-1]=success){ p=0 }else{ p=0.95 } # Arbitrary probs used for example
dat[t]=sample(c(miss,success),1,prob=c(1-p,p))
</code></pre>

<p>What other processes could result in this underdispersion? The literature on underdispersion appears to be very sparse. I found it consists mostly of simply finding distributions that can fit such data that lack any clear physical interpretation. That type of analysis is not of interest to me here. Perhaps I missed something due to using inappropriate terminology?</p>

<p><strong>Edit2:</strong>
@whuber In response to your second comment: It really is just like the free throws, almost any explanation that works for that will also apply. An exception is that a person may purposefully miss on the free throw task to maintain a certain score, while that is implausible here. </p>

<p>The task requires motor coordination to attain a goal. A success requires performing a sequence of movements in the correct order, each in the correct fashion (of course with some level of variation). There may also be multiple strategies that can yield success with different/same probability (ie underhand vs overhand shots). It is possible these are used in different trials by the same subject. Unfortunately, the only data available is number of successes per session (20 trials). </p>

<p>I do not think I am looking for ""ways to construct probability models of underdispersed phenomena"", at least not in general. I am not interested in only describing the data, rather for a process that can result in this type of data. The goal is to elucidate what may actually be being measured here if not max/asymptotic/plateau performance level.</p>

<p>To clarify what I mean by ""process"", I am thinking that a monte carlo simulation can be created using some combination of if/then statements and (possibly multiple per trial) samples of correct/incorrect actions, states, and/or events that occur with various probabilities. However, there may be other ways of modeling this.</p>

<p><strong>Edit3:</strong>
@gung I do not think we will be able to <em>identify</em> a process/mechanism from this data alone, but we can hypothesize a few consistent with the data. These will then make predictions regarding other/more detailed measurements (eg trial-to-trial scores) before running the study. This is useful because it suggests what it is important to look for and record when performing the experiments.</p>

<p>I thought of another possible mechanism. The model below simulates a situation where the subject is ""satisfied"" after a threshold # of successes (here thresh=12). The output shown had variance=0.495. If this model were accurate, rather than performance, these experiments appear to measure some kind of motivation threshold. This would be completely different than measuring a skill level, and really alter how these results are interpreted. However, this model predicts many more successes at the beginning of the session than the end. While I do not have actual data recorded regarding this, the prediction is inconsistent with my memory/impression of what unfolded. If anything, I suspect the opposite would be true.</p>

<p>I am looking for further ideas on what the explanation may be as I could not find any hints in the literature.</p>

<pre><code>p.motivated=.9; p.unmotivated=.1; n=20; thresh=12; sessions=15
results&lt;-matrix(nrow=sessions)
for(s in 1:sessions){
  session.dat&lt;-matrix(nrow=n,0)
  for(t in 1:n){
    if(sum(session.dat)&lt;thresh){
      session.dat[t]&lt;-sample(c(0,1),1,prob=c(1-p.motivated,p.motivated))
    }else{
      session.dat[t]&lt;-sample(c(0,1),1,prob=c(1-p.unmotivated,p.unmotivated))
    }
  }
  results[s]&lt;-sum(session.dat)
}

hist(results,breaks=seq(0,20,by=1))
var(results)
</code></pre>

<p><img src=""http://i.stack.imgur.com/QbkrR.png"" alt=""enter image description here""></p>
"
"0.125988157669742","0.126238880619561","125089","<p>I'm trying to apply Naive bayes to the following supervised problem:</p>

<ul>
<li>It's a binary classification problem</li>
<li>The classes are unbalanced. The target class represents the 0.004266432 of the total and the mayoritary class the 0.995733568. </li>
<li><p>There is also an unbalanced cost scheme given by the following formula:</p>

<p>Profit = 5000 * TP - 100 * FP</p>

<p>TP: True Positive - FP: False Positive</p>

<p>The objective is to maximize the Profit function.</p></li>
</ul>

<p>I'm using the <code>klaR</code> package in R to fit the model, so it's posible to adjust the priors.</p>

<p><strong>Questions:</strong></p>

<p>1) Is it posible using the prior probabilities to improve the model taking in consideration the asymetric cost scheme or/and the class inbalance?</p>

<p>2) The predict() function outputs a class prediction and a probability. The problem is that the probabilities of the minoritary class are too small. Is it posible to use the priors or scale the probabilities in a clever way to get a better cut off point?</p>

<p>So far, the results I get using Bayes are half as good compared to other methods (random forest, lasso). So I'm pretty sure there is a way to improve the naive bayes approach.</p>
"
"0.166666666666667","0.166998341953264","126338","<p>I'm running a binary prediction using a supervised topic modeling package in R (<code>lda</code> package, using <code>slda.predict</code> function). The result of the prediction returns results in linear space. From Googling around, people say that I need to take a sigmoid  to convert the result to a logical value. I'm not really sure what this means. </p>

<p>Basically I have list of documents, and their corresponding labels. What I am trying to do is set 80% of these documents and their labels, and train them using supervised LDA. The label of the document is 0 or 1. I manage to train the document just fine using this piece of code:</p>

<pre><code>example &lt;- c(""I am the role model"",""I have a major crazy   headache"",""i don't have money"", ""you are money crazy major"")
corpus = lexicalize(example, lower=TRUE)
label = c(1,1,0,0)
params &lt;- sample(c(1, 0), 2, replace=TRUE)
result &lt;- slda.em(documents=corpus$documents,
              K=2,
              vocab=poliblog.vocab,
              num.e.iterations=10,
              num.m.iterations=4,
              alpha=1.0, eta=0.1,
              label,
              params,
              variance=0.25,
              lambda=1.0,
              logistic=TRUE,
              method=""sLDA"")
</code></pre>

<p>for simplicity purpose, i'll try to predict the same document given the model above.</p>

<pre><code>predictions &lt;- slda.predict(corpus$documents,
                            result$topics, 
                        result$model,
                        alpha = 1.0,
                        eta=0.1)
</code></pre>

<p>Now, my problem is, the result of the prediction isn't binary. it's continuous value. I need to convert it back to binary using some sort of sigmoid(according to an <a href=""https://lists.cs.princeton.edu/pipermail/topic-models/2012-June/001912.html"" rel=""nofollow"">article here</a>) </p>

<p>The result i'm getting doesn't seem like a probability. For the 4 documents above, this is the output of the predictions variable</p>

<pre><code>           [,1]
[1,]  44.827420
[2,]  53.895682
[3,] -17.139034
[4,]   1.299764
</code></pre>

<p>How do I do this in R?</p>
"
"0.0629940788348712","0.0631194403097803","127471","<p>I have fitted a survival model in R which is below. However, I am not sure how to make predictions. I tried predicting the survival probability that a patient whose design matrix is <code>X</code> lives longer than 100 days, but no matter what design matrix I use, the probability is always <code>0</code>. What do you think I am doing wrong?</p>

<pre><code>library(survival)
attach
weibull &lt;- survreg(Surv(time,status)~celltype + karno+diagtime+age+prior+trt ,dist=""w"")
beta[,1] &lt;- as.matrix(c(weibull$coef))
x &lt;- as.matrix(c(1,1,0,0,80,10,65,0,2)) #Design matrix
lambda &lt;- beta[,1]%*%x
1 - (1 - exp(-lambda*100))
</code></pre>
"
"0.0890870806374748","0.0892643685354904","128885","<p>First, I should say I'm beginner in data mining and use Matlab. I have a huge dataset: approximately 40 millions observation. It consists of 24 features including numeric and nominal values like below:</p>

<pre><code>id, click, hour, C1, banner_pos, site_id, site_domain, site_category, app_id, app_domain, app_category, device_id, device_ip, device_model, device_typ,e device_conn_type, C14
</code></pre>

<p>one observation is:</p>

<pre><code>1.00E+18,   0,  14102100,   1005,   0,  1fbe01fe,   f3845767,   28905ebd,   ecad2386,   7801e8d9,   07d7df22,   a99f214a,   ddd2926e,   44956a24,   1,  2,  15706
</code></pre>

<p>All the features and some headers are annonymized.</p>

<p>The problem is to estimate the probability of clicking an advertisement id (click is target feature). What I need is a probabilistic classifier to estimate this that also uses both nominal and numeric features. Lately I used Naive Bayes but it just use numeric and does not give accurate predictions. What is your suggestion?</p>
"
"0.140859042454753","0.141139359234409","129339","<p>I am trying to do predictions on plant growth based on cumulative of time series data. Unfortunately I am not a statistician, just a programmer tasked with writing the application that does this (PHP or R) and my last statistics course was well over a decade ago.</p>

<p>I have a time series representing energy. For example (in days, but I also have it in hours and I can even interpolate)</p>

<p><img src=""http://i.imgur.com/lEuWodJ.png"" alt=""Example data""></p>

<p>And I have a little plant that needs between, say, 1200 and 3000 energy units to flower. The probability distribution of this plant flowering may be uniform between 1200 and 3000. Or it may be a normal distribution with mean 2100 and a standard deviation of 450 (the point is that I want to be able to change that distribution function and see what best matches reality).</p>

<p>Now I need some method or formula that gives the the probability that the plant will flower before a certain time, or between two times. For example, what is the chance that the plant will flower before noon on the 5th? Or the probability it will flower on the 7th between 9am and 6pm?</p>

<p>I need some way to take the distribution of the plants required energy values and calculate with it based on dates (using the time series).</p>

<p>Apart from creating a massive table and calculating everything minute-by-minute (or hour-by-hour) by hand, I can't figure out how to do this. There has to be a smarter way using statistics (and PHP or R or something) that I forgot about in the past decade.</p>

<p>Thanks in advance for any help.</p>
"
"0.185194633167905","0.185563180066099","130264","<p>We conducted a study to predict deleterious mutations from a list of around 5000 mutations (which contains both neutral and deleterious mutations; the real state of each mutation is unknown), using four publicly available SNP classifiers (prediction tools) e.g. Classifier_1, Classifier_2, Classifier_3 and Classifier_4.   </p>

<p>Let's say, 
       Classifier_1 predicted 100 mutations as deleterious (i.e. remaining 4900 mutations as neutral) from the given 5000 mutations;  classifier_2 predicted 80 SNPs as deleterious (i.e. remaining 4920 mutations as neutral) from the given 5000 mutations; classifier_3 predicted 75 SNPs as deleterious (i.e. remaining 4925 mutations as neutral) from the given 5000 mutations, and classifier_4 predicted 95 SNPs as deleterious (i.e. remaining 4905 mutations as neutral) from the given 5000 mutations. </p>

<p>Then we calculated the prediction of deleterious SNPs from a combination of any two tools (e.g. deleterious SNPs from: classifier_1 &amp; classifier_2), any three tools (e.g. deleterious SNPs from: classifier_1 &amp; classifier_2 &amp; classifier_3), and a combination of all four tools (e.g. deleterious SNPs from: classifier_1 &amp; classifier_2 &amp; classifier_3 &amp; classifier_4). Predicted deleterious SNPs from these combinations are:</p>

<pre><code>   classifier_1 = 100 deleterious SNPs,
   classifier_2 = 80 deleterious SNPs,
   classifier_3 = 75 deleterious SNPs,
   classifier_4 = 95 deleterious SNPs,
   classifier_1&amp;2 = 44 deleterious SNPs,
   classifier_1&amp;3 = 27 deleterious SNPs,
   classifier_1&amp;4 = 32 deleterious SNPs,
   classifier_2&amp;3 = 38 deleterious SNPs,
   classifier_2&amp;4 = 32 deleterious SNPs,
   classifier_3&amp;4 = 20 deleterious SNPs,
   classifier_1&amp;2&amp;3 = 18 deleterious SNPs,
   classifier_1&amp;2&amp;4 = 17 deleterious SNPs,
   classifier_1&amp;3&amp;4 = 11 deleterious SNPs,
   classifier_2&amp;3&amp;4 = 13 deleterious SNPs,
   classifier_1&amp;2&amp;3&amp;4 = 10 deleterious SNPs.
</code></pre>

<p>Under this scenario, we want to calculate the probability of selecting these deleterious SNPs, in each level of prioritization (i.e. using one tool, a combination of two tools, a combination of three tools, and a combination of all four tools), simply by chance. </p>

<p>This analysis will assist us in inferring whether our prioritization scheme (i.e. application of one prediction tool, a combination of two tools, a combination of three tools, and a combination of all four tools) is effective or not.  </p>

<p>We tried pbinom(x, n, p) and binom.test(x, n, p) where x = number of predicted deleterious SNPs e.g. 100 SNPs by classifier_1, n = total number of SNPs considered e.g. 5000, and p = 0.5 (i.e. random guessing); but not sure whether it is correct or not, and how to address all the situations.  </p>

<blockquote>
  <p>##Example: </p>
</blockquote>

<pre><code> Classifier_1 = binom.test(c(100, 4900), p = 0.5)
   Classifier_1     
   Exact binomial test

   data:  c(100, 4900)
   number of successes = 100, number of trials = 5000, p-value &lt; 2.2e-16
   alternative hypothesis: true probability of success is not equal to 0.5
   95 percent confidence interval:
     0.01630168 0.02427257
   sample estimates:
   probability of success 
         0.02 

  Classifier_1 = pbinom(100, 5000, p = 0.5)
  Classifier_1
  [1] 0
</code></pre>

<p>I will really appreciate, if you guide me - how to calculate the probability of selecting these SNPs (from each tools, and a combination of all) simply by chance (i.e. prediction scheme is not effective).</p>

<p>Thank you all for your kind help .</p>
"
"0.125988157669742","0.126238880619561","135347","<p>Say I have a set of data</p>

<ol>
<li>abc-def-ghi        </li>
<li>jkl-mno-pqr         </li>
<li>stu-vwx-yza         </li>
</ol>

<p>and lots of other training samples which are catagorized as **names*.</p>

<p>The above dataset does not have </p>

<p><strong>nef-orl-kwq</strong></p>

<p>But it is present in the test set.
Can a <strong>Naive Bayes classifier</strong> classify it as a <strong>name</strong> based on this <strong>xxx-xxx-xxx</strong>pattern alone?</p>

<p>I know Naive Bayes calculates the frequency of each occurred word to give the probabilities, but that won't be applicable in this case right?</p>

<p>I've tried adding laplace smoothing for Naive Bayes to account for the fact that <strong>nef-orl-kwq</strong> was not present in the training set and hence it will have a zero probability if the prediction is based on frequency alone.</p>

<p><strong>Edit 1</strong>:</p>

<p>I have a <strong>training set</strong> which goes like this:</p>

<ul>
<li><p>Independent variable <strong>Dependent Variable</strong></p></li>
<li><p>abc-def-ghi     <strong>name</strong></p></li>
<li>jkl-mno-pqr     <strong>name</strong></li>
<li>stu-vwx-yza   <strong>name</strong></li>
<li>....</li>
<li>123-456789 <strong>phone number</strong></li>
<li>789-123456 <strong>phone number</strong></li>
<li>178-234706 <strong>phone number</strong></li>
<li>....</li>
</ul>

<p><strong>test set</strong></p>

<ul>
<li>nef-orl-kwq</li>
</ul>

<p>This should be classified as either phone number or name. But frequency of occurrence won't apply here because it has not occurred even once in the training set. Can Naive Bayes use the pattern of the data <strong>xxx-xxx-xxx</strong> to predict it as name?</p>

<p><strong>Edit 2</strong>:</p>

<p>There are definite patterns for phone number and name - one has digits, the other has characters. But this will fail when address is also included because it will have both digits and characters. I've also included as.factor() for the independent variable(feature). Here is the code I've used:</p>

<pre><code>library(e1071)
train&lt;-read.csv(""C:\\Users\\Desktop\\train.csv"")
test&lt;-read.csv(""C:\\Users\\Desktop\\test.csv"")

train$data&lt;-as.factor(train$data)
test$data&lt;-as.factor(test$data)
model&lt;-naiveBayes(type ~.,data=train,laplace=5)
predicted&lt;-predict(model,test)
result&lt;-cbind(predicted,test)
write.csv(result,""results.csv"")
</code></pre>
"
"0.188982236504614","0.189358320929341","135412","<p>I am using features to predict a dataset classification.
I have use the Gradient Boosting Classifier of scikit-learn for the prediction and tune it to reduce the error classification.
The error classification has been done using MAE, i.e., reduce the sum of the difference between predicted and real value of all test set.
In the following image (<a href=""https://www.dropbox.com/s/19efwx1wmigkqax/AccArray.png"" rel=""nofollow"">link</a>) it can be seen a table to understand the differences, where a perfect classification should have only numbers on the diagonal.</p>

<p>As it can be seen, the high classified values are wrongly predicted and giving a lower value.
This is due to the low number of training examples with high classification that make them as a anomaly values and the distribution all test set.
In the following plotting (<a href=""https://www.dropbox.com/s/ttpdzxf4odz0rm0/Var1-Var2.png"" rel=""nofollow"">link</a>) of 2 variables it can be seen that there is a region that high values are more probable, but due to the number of lower values in the same region the prediction is not good.</p>

<p>I cannot have more data of high classification values because they are anomalies, but I cannot reduce the training set too much because the Machine-Learning does not work.</p>

<p>Is there any possibility to define a classifier probability distribution for each region?
Or, is there any other better score method for this purpose?</p>
"
"0.253546276418555","0.254050846621936","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"0.109108945117996","0.109326077561851","142255","<p>I have difficulties to understand <code>predict.svm</code>. 
Please find an illustration of my confusion below.
As we can see, results are different depending on the <code>probability</code> argument of <code>predict()</code>.
So my question is: what is the difference between <code>probability=TRUE</code> and <code>probability=FALSE</code>?</p>

<pre><code>library(e1071)

# some illustrative data
data(iris)
attach(iris)
x &lt;- subset(iris, select=-Species)
y &lt;- factor(Species == ""setosa"")

# SVM
SVM &lt;- svm(y ~ x[, 1], probability=TRUE) 

# predictions with 'probability=FALSE'
pred &lt;- predict(SVM, x[, 1], probability=FALSE)
table(true=iris$Species == ""setosa"", pred=pred)
#        pred
# true    FALSE TRUE
#   FALSE    93    7
#   TRUE      5   45

# predictions with 'probability=TRUE'
pred &lt;- predict(SVM, x[, 1], probability=TRUE)
table(true=iris$Species == ""setosa"", pred=pred)
#        pred
# true    FALSE TRUE
#   FALSE    94    6
#   TRUE     10   40
</code></pre>
"
"0.218217890235992","0.200431142196726","144348","<p><strong>Note</strong><br>
I've edited the example to be more intuitive and closer to my real data</p>

<p><strong>Intro</strong><br>
I've got data on customers purchases and with it am trying to predict which customers are more likely to make next purchase at some time in the future. Data consist of customers' features like sex, age etc., and their prior purchase behavior like total spendings and number of orders, one row for every customer. The last two columns are the indicator of wether he have made next purchase or not, and number of days till purchase or till today, in case of no purchase.  </p>

<p><strong>Problem</strong><br>
I am building a Cox regression and then want to predict probability of next purchase for individual observations in, say, 30 days from last purchase.</p>

<p>Reproducible example:</p>

<pre><code>library(survival)
library(rms)
library(pec)
library(ggplot2)

data(cost)

# split into train and test sets
set.seed(1)
ind &lt;- sample(1:nrow(cost), 100)
test.set &lt;- cost[ind, ]
train.set &lt;- cost[-ind, ]
</code></pre>

<p>For Cox regression I use <code>cph</code> from <code>rms</code> package, for prediction - <code>predictSurvProb</code> from <code>pec</code> package as suggested in <a href=""http://stats.stackexchange.com/a/36016/72401"">this</a> discussion.</p>

<pre><code># fit Cox model
fit &lt;- cph(Surv(time, status) ~ ., data = train.set, surv = TRUE)

# predict pobability of event in 30 days
test.set$predicted.probs &lt;- 1 - predictSurvProb(fit, newdata = test.set, times = 1000)[, 1]
</code></pre>

<p>Thus, for every customer we have his probability of making a purchase in 1000 units of time. I want to validate prediction against real data.  </p>

<p><strong>Now to the question:</strong> what is the best/valid way to do it?  </p>

<p>Here's what I've tried:<br>
I expect that valid model would predict higher probabilities for customers who made their purchase earlier so correlation between probabilities and number of days to event' would be negative and strong (e.g. for customer who actualy made next purchase in 2 days, probability of buying in 30 days would be very high).</p>

<pre><code>with(test.set, cor(predicted.probs, time))
# [1] -0.5221604
</code></pre>

<p>Also, probability for those who made purchase (status = 1) would be higher than for those who didn't.</p>

<pre><code>with(test.set, by(predicted.probs, status, mean))
# status: 0
# [1] 0.2371247
# --------------
#   status: 1
# [1] 0.4083586
</code></pre>

<p>And a graph to eyeball my assumptions:</p>

<pre><code>qplot(data = test.set, x = time, y = predicted.probs, color = time)
</code></pre>

<p>Am I correct in my reasoning?</p>
"
"0.282889474930502","0.296336641417292","147816","<p>This is a revision/rephrasing of <a href=""http://stackoverflow.com/questions/29705265/how-to-present-multiple-time-series-data-to-an-svm-ksvm-in-r-or-how-to-prese"">my question originally posted on stackoverflow</a>.</p>

<p>How should I create the training/input dataset for a <a href=""http://www.inside-r.org/node/63499"" rel=""nofollow"">ksvm</a> model with multi-dimensional input data?</p>

<p>The process for which I need a binary yes/no prediction model has six non-periodic time series inputs, all with the same sampling frequency. An event triggers the start of data collection, and after a pre-determined time I need a yes/no prediction (preferably including a probability-of-correctness output).</p>

<p>I'm pretty new to both R and SVM's, but I think I want to use an SVM model (<a href=""http://www.inside-r.org/node/63499"" rel=""nofollow"">kernlab's ksvm</a>). I'm having trouble figuring out how to present the input data to it. There are two basic strategies I've tried with dismal results (well, the resulting models were better than blind guessing, but not much).</p>

<p>First of all, not being familiar with R, I used the Rattle GUI front-end to R. I have a feeling that by doing so I may be limiting my options. But anyway, here's what I've done.....</p>

<p>Example known result files (shown with only 4 sensors instead of 6, and only 7 time samples instead of 100):</p>

<p>training168_yes.csv</p>

<pre><code>Seconds Since 1/1/2000,sensor1,sensor2,sensor3,sensor4
454768042.4,           0,      0,      0,      0
454768042.6,           51,     60,     0,      172
454768043.3,           0,      0,      0,      0
454768043.7,           300,    0,      0,      37
454768044.0,           0,      0,      1518,   0
454768044.3,           0,      0,      0,      0
454768044.7,           335,    0,      0,      4273
</code></pre>

<p>training169_no.csv</p>

<pre><code>Seconds Since 1/1/2000,sensor1,sensor2,sensor3,sensor4
454767904.5,           0,      0,      0,      0
454767904.8,           51,     0,      498,    0
454767905.0,           633,    0,      204,    55
454767905.3,           0,      0,      0,      512
454767905.6,           202,    655,    739,    656
454767905.8,           0,      0,      0,      0
454767906.0,           0,      934,    0,      7814
</code></pre>

<p>The only way I know to get the data for all training samples into Rattle is to massage &amp; combine all result files into a single .csv file, with one sample result per line. I can think of only two ways to do that, so I tried them both (and I knew when I was doing it that by doing this I'm hiding potentially important information, which is the point of this question):</p>

<p><strong><em>TRIAL #1:</em></strong> For each result file, add each sensor's samples into a single number, blasting away all temporal information:</p>

<pre><code>result,sensor1,sensor2,sensor3,sensor4
no,    886,    1589,   1441,   9037
yes,   686,    60,     1518,   4482
no,    632,    1289,   1173,   9152
yes,   411,    67,     988,    5030
no,    772,    1703,   1351,   9008
yes,   490,    70,     1348,   4909
</code></pre>

<p>When I get done using Rattle to generate the SVM, Rattle's log tab gives me the following script which can be used to generate &amp; train an SVM in RGui:</p>

<pre><code>library(rattle)
building &lt;- TRUE
scoring  &lt;- ! building
library(colorspace)
crv$seed &lt;- 42
    crs$dataset &lt;- read.csv(""file:///C:/Users/mminich/Desktop/stackoverflow/trainingSummary1.csv"",na.strings=c(""."", ""NA"", """", ""?""), strip.white=TRUE, encoding=""UTF-8"")
set.seed(crv$seed) 
    crs$nobs &lt;- nrow(crs$dataset) # 6 observations 
    crs$sample &lt;- crs$train &lt;- sample(nrow(crs$dataset), 0.67*crs$nobs) # 4 observations
    crs$validate &lt;- NULL
crs$test &lt;- setdiff(setdiff(seq_len(nrow(crs$dataset)), crs$train), crs$validate) # 2 observations
# The following variable selections have been noted.
crs$input &lt;- c(""sensor1"", ""sensor2"", ""sensor3"", ""sensor4"")
    crs$numeric &lt;- c(""sensor1"", ""sensor2"", ""sensor3"", ""sensor4"")
crs$categoric &lt;- NULL
    crs$target  &lt;- ""result""
crs$risk    &lt;- NULL
    crs$ident   &lt;- NULL
crs$ignore  &lt;- NULL
    crs$weights &lt;- NULL
require(kernlab, quietly=TRUE)
set.seed(crv$seed)
    crs$ksvm &lt;- ksvm(as.factor(result) ~ .,
      data=crs$dataset[,c(crs$input, crs$target)],
      kernel=""polydot"",
      kpar=list(""degree""=1),
      prob.model=TRUE)
</code></pre>

<p><strong><em>TRIAL #2:</em></strong> For each result file, add the samples for all sensors for each time into a single number, blasting away any information about individual sensors:</p>

<pre><code>result,time1, time2, time3, time4, time5, time6, time7
no,    0,     549,   892,   512,   2252,  0,     8748
yes,   0,     283,   0,     337,   1518,  0,     4608
no,    0,     555,   753,   518,   2501,  0,     8984
yes,   0,     278,   12,    349,   1438,  3,     4441
no,    0,     602,   901,   499,   2391,  0,     7989
yes,   0,     271,   3,     364,   1474,  1,     4599
</code></pre>

<p>And again I use Rattle to generate the SVM, and Rattle's log tab gives me the following script:</p>

<pre><code>library(rattle)
building &lt;- TRUE
scoring  &lt;- ! building
library(colorspace)
crv$seed &lt;- 42 
    crs$dataset &lt;- read.csv(""file:///C:/Users/mminich/Desktop/stackoverflow/trainingSummary2.csv"",na.strings=c(""."", ""NA"", """", ""?""), strip.white=TRUE, encoding=""UTF-8"")
set.seed(crv$seed) 
    crs$nobs &lt;- nrow(crs$dataset) # 6 observations 
    crs$sample &lt;- crs$train &lt;- sample(nrow(crs$dataset), 0.67*crs$nobs) # 4 observations
    crs$validate &lt;- NULL
crs$test &lt;- setdiff(setdiff(seq_len(nrow(crs$dataset)), crs$train), crs$validate) # 2 observations
# The following variable selections have been noted.
crs$input &lt;- c(""time1"", ""time2"", ""time3"", ""time4"", ""time5"", ""time6"", ""time7"")
    crs$numeric &lt;- c(""time1"", ""time2"", ""time3"", ""time4"", ""time5"", ""time6"", ""time7"")
crs$categoric &lt;- NULL
    crs$target  &lt;- ""result""
crs$risk    &lt;- NULL
    crs$ident   &lt;- NULL
crs$ignore  &lt;- NULL
    crs$weights &lt;- NULL
require(kernlab, quietly=TRUE)
set.seed(crv$seed)
    crs$ksvm &lt;- ksvm(as.factor(result) ~ .,
      data=crs$dataset[,c(crs$input, crs$target)],
      kernel=""polydot"",
      kpar=list(""degree""=1),
      prob.model=TRUE)
</code></pre>

<p>Unfortunately even with nearly 1000 training datasets, both of the resulting models give me only slightly better results than I would get by just random chance. I'm pretty sure it would do better if there's a way to avoid blasting away either the temporal data or the distinction between different sensors. But it would seem that somehow I'd need to present a 2-dimensional array (matrix? I'm not a math guy so I don't know if I should be using the term ""matrix"" here) for each observation instead of a single-dimensional array (i.e. it seems like instead of each observation being one line of a .csv file, each observation would need to be an entire 2-dimensional .csv file itself, and I'd need to present an array/list of the .csv files, meaning the input training set would be a 3-dimensional structure of some sort. How can I create such a structure for input to <code>ksvm()</code>?</p>
"
"0.339941709062547","0.340618210645226","147836","<p>I want to get a prediction interval around a prediction from a lmer() model. I have found some discussion about this:</p>

<p><a href=""http://rstudio-pubs-static.s3.amazonaws.com/24365_2803ab8299934e888a60e7b16113f619.html"" rel=""nofollow"">http://rstudio-pubs-static.s3.amazonaws.com/24365_2803ab8299934e888a60e7b16113f619.html</a></p>

<p><a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">http://glmm.wikidot.com/faq</a></p>

<p>but they seem to not take the uncertainty of the random effects into account.</p>

<p>Here's a specific example. I am racing gold fish. I have data on the past 100 races. I want to predict the 101st, taking into account uncertainty of my RE estimates, and FE estimates. I am including a random intercept for fish (there are 10 different fish), and fixed effect for weight (less heavy fish are quicker).</p>

<pre><code>library(""lme4"")

fish &lt;- as.factor(rep(letters[1:10], each=100))
race &lt;- as.factor(rep(900:999, 10))
oz &lt;- round(1 + rnorm(1000)/10, 3)
sec &lt;- 9 + rep(1:10, rep(100,10))/10 + oz + rnorm(1000)/10

fishDat &lt;- data.frame(fishID = fish, 
      raceID = race, fishWt = oz, time = sec)
head(fishDat)
plot(fishDat$fishID, fishDat$time)

lme1 &lt;- lmer(time ~ fishWt + (1 | fishID), data=fishDat)
summary(lme1)
</code></pre>

<p>Now, to predict the 101st race. The fish have been weighed and are ready to go:</p>

<pre><code>newDat &lt;- data.frame(fishID = letters[1:10], 
    raceID = rep(1000, 10),
    fishWt = 1 + round(rnorm(10)/10, 3))
newDat$pred &lt;- predict(lme1, newDat)
newDat

   fishID raceID fishWt     pred
1       a   1000  1.073 10.15348
2       b   1000  1.001 10.20107
3       c   1000  0.945 10.25978
4       d   1000  1.110 10.51753
5       e   1000  0.910 10.41511
6       f   1000  0.848 10.44547
7       g   1000  0.991 10.68678
8       h   1000  0.737 10.56929
9       i   1000  0.993 10.89564
10      j   1000  0.649 10.65480
</code></pre>

<p>Fish D has really let himself go (1.11 oz) and is actually predicted to lose to Fish E and Fish F, both of whom he has been better than in the past. However, now I want to be able to say, ""Fish E (weighing 0.91oz) will beat Fish D (weighing 1.11oz) with probability p."" Is there a way to make such a statement using lme4? I want my probability p to take into account my uncertainty in both the fixed effect, and the random effect.</p>

<p>Thanks!</p>

<p>PS looking at the <code>predict.merMod</code> documentation, it suggests ""There is no option for computing standard errors of predictions because it is difficult to define an efficient method that incorporates uncertainty in the variance parameters; we recommend <code>bootMer</code> for this task,"" but by golly, I cannot see how to use <code>bootMer</code> to do this. It seems <code>bootMer</code> would be used to get bootstrapped confidence intervals for parameter estimates, but I could be wrong. </p>

<h2><strong>UPDATED Q:</strong></h2>

<p>OK, I think I was asking the wrong question. I want to be able to say, ""Fish A, weighing w oz, will have a race time that is (lcl, ucl) 90% of the time."" </p>

<p>In the example I have laid out, Fish A, weighing 1.0 oz, will have a race time of <code>9 + 0.1 + 1 = 10.1 sec</code> on average, with a standard deviation of 0.1. Thus, his observed race time will be between</p>

<pre><code>x &lt;- rnorm(mean = 10.1, sd = 0.1, n=10000)
quantile(x, c(0.05,0.50,0.95))
       5%       50%       95% 
 9.938541 10.100032 10.261243 
</code></pre>

<p>90% of the time. I want a prediction function that attempts to give me that answer. Setting all <code>fishWt = 1.0</code> in <code>newDat</code>, re-running the sim, and using (as suggested by Ben Bolker below)</p>

<pre><code>bb &lt;- bootMer(lme1,nsim=1000,FUN=predFun, use.u = FALSE)
</code></pre>

<p>gives</p>

<pre><code>&gt; quantile(predMat[,1], c(0.05,0.50,0.95))
      5%      50%      95% 
10.01362 10.55646 11.05462 
</code></pre>

<p>This seems to actually be centered around the population average? As if it's not taking the FishID effect into account? I thought maybe it was a sample size issue, but when I bumped the number of observed races from 100 to 10000, I still get similar results.</p>

<p>On the flip side, using</p>

<pre><code>bb &lt;- bootMer(lme1,nsim=1000,FUN=predFun, use.u = TRUE)
</code></pre>

<p>gives</p>

<pre><code>&gt; quantile(predMat[,1], c(0.05,0.50,0.95))
      5%      50%      95% 
10.09970 10.10128 10.10270 
</code></pre>

<p>That interval is too narrow, and would seem to be a confidence interval for Fish A's mean time. I want a confidence interval for Fish A's observed race time, not his average race time. How can I get that?</p>

<h2><strong>UPDATE 2, ALMOST:</strong></h2>

<p>I <em>thought</em> I found what I was looking for in <a href=""http://www.stat.columbia.edu/~gelman/arm/"" rel=""nofollow"">Gelman and Hill (2007)</a> , page 273. Need to utilize the <code>arm</code> package.</p>

<pre><code>library(""arm"")
</code></pre>

<p>For Fish A: </p>

<pre><code>x.tilde &lt;- 1    #observed fishWt for new race
sigma.y.hat &lt;- sigma.hat(lme1)$sigma$data      #get uncertainty estimate of our model
coef.hat &lt;- as.matrix(coef(lme1)$fishID)[1,]    #get intercept (random) and fishWt (fixed) parameter estimates
y.tilde &lt;- rnorm(1000, coef.hat %*% c(1, x.tilde), sigma.y.hat) #simulate
quantile (y.tilde, c(.05, .5, .95))

  5%       50%       95% 
 9.930695 10.100209 10.263551 
</code></pre>

<p>For all the fishes:</p>

<pre><code>x.tilde &lt;- rep(1,10)  #assume all fish weight 1 oz
#x.tilde &lt;- 1 + rnorm(10)/10  #alternatively, draw random weights as in original example
sigma.y.hat &lt;- sigma.hat(lme1)$sigma$data
coef.hat &lt;- as.matrix(coef(lme1)$fishID)
y.tilde &lt;- matrix(rnorm(1000, coef.hat %*% matrix(c(rep(1,10), x.tilde), nrow = 2 , byrow = TRUE), sigma.y.hat), ncol = 10, byrow = TRUE)
quantile (y.tilde[,1], c(.05, .5, .95))
       5%       50%       95% 
 9.937138 10.102627 10.234616 
</code></pre>

<p>Actually, this probably isn't exactly what I want. I'm only taking into account the overall model uncertainty. In a situation where I have, say, 5 observed races for Fish K and 1000 observed races for Fish L, I think the uncertainty associated with my prediction for Fish K should be much larger than the uncertainty associated with my prediction for Fish L. </p>

<p>Will look further into Gelman and Hill 2007. I feel I may end up having to switch to BUGS (or Stan).</p>

<h2><strong>UPDATE THE 3rd:</strong></h2>

<p>Perhaps I am conceptualizing things poorly. Using the <code>predictInterval()</code> function given by Jared Knowles in an answer below gives intervals that aren't quite what I would expect...</p>

<pre><code>library(""lattice"")
library(""lme4"")
library(""ggplot2"")

fish &lt;- c(rep(letters[1:10], each = 100), rep(""k"", 995), rep(""l"", 5))
oz &lt;- round(1 + rnorm(2000)/10, 3)
sec &lt;- 9 + c(rep(1:10, each = 100)/10,rep(1.1, 995), rep(1.2, 5)) + oz + rnorm(2000)

fishDat &lt;- data.frame(fishID = fish, fishWt = oz, time = sec)
dim(fishDat)
head(fishDat)
plot(fishDat$fishID, fishDat$time)

lme1 &lt;- lmer(time ~ fishWt + (1 | fishID), data=fishDat)
summary(lme1)
dotplot(ranef(lme1, condVar = TRUE))
</code></pre>

<p>I have added two new fish. Fish K, for whom we have observed 995 races, and Fish L, for whom we have observed 5 races. We have observed 100 races for Fish A-J. I fit the same <code>lmer()</code> as before. Looking at the <code>dotplot()</code> from the <code>lattice</code> package:</p>

<p><a href=""http://i.stack.imgur.com/EoG9Z.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/EoG9Z.png"" alt=""FishID Estimates""></a></p>

<p>By default, <code>dotplot()</code> reorders the random effects by their point estimate. The estimate for Fish L is on the top line, and has a very wide confidence interval. Fish K is on the third line, and has a very narrow confidence interval. This makes sense to me. We have lots of data on Fish K, but not a lot of data on Fish L, so we are more confident in our guesstimate about Fish K's true swimming speed. Now, I would think this would lead to a narrow prediction interval for Fish K, and a wide prediction interval for Fish L when using <code>predictInterval()</code>. Howeva:</p>

<pre><code>newDat &lt;- data.frame(fishID = letters[1:12],
                     fishWt = 1)

preds &lt;- predictInterval(lme1, newdata = newDat, n.sims = 999)
preds
ggplot(aes(x=letters[1:12], y=fit, ymin=lwr, ymax=upr), data=preds) +
  geom_point() + 
  geom_linerange() +
  labs(x=""Index"", y=""Prediction w/ 95% PI"") + theme_bw()
</code></pre>

<p><a href=""http://i.stack.imgur.com/2PUyP.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2PUyP.png"" alt=""Prediction Interval for Fish""></a></p>

<p>All of those prediction intervals appear to be identical in width. Why isn't our prediction for Fish K narrower the others? Why isn't our prediction for Fish L wider than others?</p>
"
"0.199204768222399","0.19960119601395","148699","<p>For a current piece of work Iâ€™m trying to model the probability of tree death for beech trees in a woodland in the UK. I have records of whether trees were alive or dead for 3 different census periods along with data on their diameter and growth rate. Each tree has an ID number so it can be identified at each time interval. However, the census intervals vary so that for the time between one survey and another is either 4, 12 or 18 years. Obviously the longer the census period the greater the probability a tree will have died by the time it is next surveyed. <strong>I had problems making a realistic reproducible example so you can find the <a href=""https://github.com/PhilAMartin/Denny_mortality/blob/master/Data/Stack_dead.csv"" rel=""nofollow"">data here</a>.</strong></p>

<p>The variables in the dataset are:</p>

<ol>
<li>ID - Unique ID for tree</li>
<li>Block - the ID for the 20x20m plot in which the tree was located</li>
<li>Dead - Status of tree, either dead (1) or alive (0)</li>
<li>GR - Annual growth rate from previous survey</li>
<li>DBH - diameter of tree at breast height</li>
<li>SL - Length of time between censuses in years</li>
</ol>

<p>Once a tree is recorded as dead it disappears from subsequent surveys.</p>

<p>Ideally I would like to be able to estimate the annual probability of mortality of a tree using information on diameter and growth rate. Having searched around for quite a while I have seen that logistic exposure models appear able to account for differences in census periods by using an altered version of logit link for binomial models as detailed by Ben Bolker <a href=""https://rpubs.com/bbolker/logregexp"" rel=""nofollow"">here</a>. This was originally used by Shaffer to determine the daily probability of bird nest survival where the age (and therefore exposure) of the nest differed. I've not seen it used outside of the context of models of nest survival but it seems like I should be able to use it to model survival/mortality where the exposure differs.</p>

<pre><code>require(MASS)
logexp &lt;- function(exposure = 1)
{
  linkfun &lt;- function(mu) qlogis(mu^(1/exposure))
  ## FIXME: is there some trick we can play here to allow
  ##   evaluation in the context of the 'data' argument?
  linkinv &lt;- function(eta)  plogis(eta)^exposure
  logit_mu_eta &lt;- function(eta) {
    ifelse(abs(eta)&gt;30,.Machine$double.eps,
           exp(eta)/(1+exp(eta))^2)
    ## OR .Call(stats:::C_logit_mu_eta, eta, PACKAGE = ""stats"")
  }
  mu.eta &lt;- function(eta) {       
    exposure * plogis(eta)^(exposure-1) *
      logit_mu_eta(eta)
  }
  valideta &lt;- function(eta) TRUE
  link &lt;- paste(""logexp("", deparse(substitute(exposure)), "")"",
                sep="""")
  structure(list(linkfun = linkfun, linkinv = linkinv,
                 mu.eta = mu.eta, valideta = valideta, 
                 name = link),
            class = ""link-glm"")
}
</code></pre>

<p>At the moment my model looks like this, but I will incorporate more variables as I go along:</p>

<pre><code>require(lme4)
Dead&lt;-read.csv(""Stack_dead.csv"",)


M1&lt;-glmer(Dead~DBH+(1|ID),data=Dead,family=binomial(logexp(Dead$SL))) 
#I use (1|ID) here to account for the repeated measurements of the same individuals
    summary(M1)

plot(Dead$DBH,plogis(predict(M1,re.form=NA)))
</code></pre>

<p><strong>Primarily I want to know</strong>:</p>

<ol>
<li><strong>Does the statistical technique I am using to control for the difference in time between census seem sensible? If it isn't, can you think of a better way to deal with the problem?</strong></li>
<li><strong>If the answer to the first question is yes, is using the inverse logit (plogis) the correct way to get predictions expressed as probabilities?</strong></li>
</ol>

<p>Thanks in advance for any help!</p>
"
"0.17817416127495","0.178528737070981","149559","<p>I have a historical data that has discrete variables. Let say I have data points with class labels 1, 2,3,4,5. For a given classification problem, I can use the training data and then get the trained model. Using the trained model, I can classify the labels. However, I am also interested in prediction of class labels. My requirements is that for a given N points, lets say 5 data points, I want to know what is the probability that class label 5 more likely or less likely occurs from 0 to 1. Can anyone give me any ideas on this ? For eg: My output will be a  prediction probability for 5 instances: 0, 0.1, 0.2, 0.3, 0.5. This means first data point has zero probability for class label 5 to occur. The 5th data point has highest probability to occur around 50%. Can anyone give me idea on how to go about this problem ?</p>
"
"0.189934294099397","0.209343500484775","151961","<p>*Please note this question is about the Platt probabilistic output and SVM class assignment, not about the code or the package itself. It just happens to be the code where I stumbled on the issue.</p>

<p>In <a href=""http://stats.stackexchange.com/questions/147260/including-class-probabilities-might-skew-a-model-in-caret"">another question</a> I asked about bad models coming from <code>caret</code> and associated <code>kernlab</code> when <code>prob.model=TRUE</code>. I found the answer myself, in both <a href=""http://stackoverflow.com/questions/29766951/different-results-with-caret-when-classprobs-true"">stackoverflow</a> and <a href=""http://r.789695.n4.nabble.com/Inconsistent-results-between-caret-kernlab-versions-td4680500.html"" rel=""nofollow"">from Max Kuhn himself</a>:</p>

<blockquote>
<pre><code>&gt; predict(newSVM, df[43,-1]) [1] O32078 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
&gt; predict(newSVM, df[43,-1], type = ""probabilities"")
     O27479     O31403     O32057    O32059    O32060     O32078
[1,] 0.08791826 0.05911645 0.2424997 0.1036943 0.06968587 0.1648394
     O32089     O32663     O32668     O32676
[1,] 0.04890477 0.05210836 0.09838892 0.07284396
</code></pre>
  
  <p>Note that, based on the probability model, the class with the largest
  probability is O32057 (p = 0.24) while the basic SVM model predicts
  O32078 (p = 0.16).</p>
  
  <p><strong>Somebody (maybe me) saw this discrepancy and that led to me to follow
  this rule:</strong></p>

<pre><code>if(prob.model = TRUE) use the class with the maximum probability   
  else use the class prediction from ksvm().
</code></pre>
  
  <p>Therefore:</p>

<pre><code>predict(svm.m1, df[43,-1])
 [1] O32057
 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
</code></pre>
</blockquote>

<p>Isn't that innacurate? <code>kernlab</code> searches for the optimal probability cutoff that minimizes error, that's why the assigned class and the maximum probability don't match: they don't have to.</p>

<p>Check this reproducible example. I excluded two cherrypicked <code>virginica</code> samples.</p>

<pre><code>require(kernlab);require(caret);
#kernel=polynomial; degree=3; scale=0.1; C=0.31
set.seed(101);SVM&lt;-ksvm(Species~., data=iris[-c(135,150),], kernel='polydot',C=.31, kpar=list( scale=.1, degree=3), prob.model=T)
</code></pre>

<p>Here's the resulting model </p>

<pre><code>&gt; SVM
Support Vector Machine object of class ""ksvm"" 

SV type: C-svc  (classification) 
 parameter : cost C = 0.31 

Polynomial kernel function. 
 Hyperparameters : degree =  3  scale =  0.1  offset =  1 

Number of Support Vectors : 58 

Objective Function Value : -1.4591 -0.7955 -10.2392 
Training error : 0.033784 
Probability model included. 
</code></pre>

<p>Now let's check the predicted class probabilities in those two samples</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5], type=""probabilities"")
          setosa versicolor virginica
[1,] 0.008286638  0.4414114  0.550302
[2,] 0.013824451  0.3035556  0.682620
</code></pre>

<p>And the class predictions</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5])
[1] versicolor virginica 
Levels: setosa versicolor virginica
</code></pre>

<p>Sample 150 was assigned to <code>virginica</code>, with a class probability of around 0.68. Sample 135 was assigned to <code>versicolor</code> with a probability of around 0.44, yet <code>virginica</code> probability nicely sits around 0.55.
Looking at several CV folds, we perceive that kernlab only assigns <code>virginica</code> when its probability is over a given value (way higher than 0.5). That's the cutoff I mentioned, and it happens thanks to the well known bad clustering in <code>iris</code> between <code>virginica</code> and <code>versicolor</code>.</p>

<p>So, am I right on these suppositions and therefore is <code>caret</code> class assignment model (maximum probability) wrong?</p>

<p>EDIT:
I've been experimenting with pairwise probability coupling of Platt scaling (logistic regression fit), isotononic regression and a model I'm working on. A weakness (?) I perceived in Platt's model is the probability isn't bound to be 0.5 when the binary SVM decision output is 0, which is the expected result as the instance would lie exactly on the separating hyperplane.</p>
"
"0.0890870806374748","0.0892643685354904","156036","<p>I have built a model to predict Upsell probability. When I use the function confusionMatirx from caret package, I get the following results:</p>

<pre><code>&gt; confusionMatrix(data = predict_svm_test_5, test_td$UpSell_Ind)
Confusion Matrix and Statistics

             Reference
Prediction    0    1
          0 7976 2886
          1  217  644

           Accuracy : 0.7353          
             95% CI : (0.7272, 0.7433)
No Information Rate : 0.6989          
P-Value [Acc &gt; NIR] : &lt; 2.2e-16       

              Kappa : 0.1987          
Mcnemar's Test P-Value : &lt; 2.2e-16       

        Sensitivity : 0.9735          
        Specificity : 0.1824          
     Pos Pred Value : 0.7343          
     Neg Pred Value : 0.7480          
         Prevalence : 0.6989          
     Detection Rate : 0.6804          
Detection Prevalence : 0.9266          
  Balanced Accuracy : 0.5780          

   'Positive' Class : 0  
</code></pre>

<p>However, I expected to see the confusion matrix as follows:</p>

<pre><code>             Reference
Prediction   1      0
      1     644    217
      0    2886   7976
Specificity(TPR): 0.9735
Sensitivity(TNR): 0.1824
</code></pre>

<p>1 meaning there was an Upsell (Event) and 0 meaning no Upsell (No Event) based on the PDF of Caret Package. Link is <a href=""http://cran.r-project.org/web/packages/caret/caret.pdf"" rel=""nofollow"">here</a> Page 24, 25</p>

<p>Now my question: How do I interpret the results of confusionMatrix? The values given by the function are different from values that I calculate.</p>

<p>Thanks in advance for the help.</p>
"
"0.218866384652979","0.236171320084126","157054","<p>I am very, <em>very</em> confused about ROC curves. I have a Bayesian model which outputs a prevalence on a continuous scale between 0 and 1. I have a classification I would like to use that classifies that prevalence into an endemicity class based on a two sided cut-off (i.e. the prevalence cut off for being in the class is $ 0.35 &lt;= P &lt;= 0.6 $.</p>

<p>So, I have the true observed prevalence, the predicted prevalence from my model (which is taken as the mean of the posterior distribution for that observation) I also have a probability for each prediction, that it belongs in the endemicity class of interest. This probability is an output of my Bayesian model and is simply the number of times the predicted prevalence <em>did</em> fall between those endemicity cut-offs divided by the number of posterior draws from my model (so a probability of $1$ means that all posterior realisations fell between 0.35 and 0.6).</p>

<p>Here is an example of my data:</p>

<pre><code>&gt; head(mydat)
    Obs  Pred  pClass Class
1 0.441 0.503 0.56125     1
2 0.664 0.698 0.22225     0
3 0.252 0.468 0.58725     0
4 0.226 0.374 0.39325     0
5 0.014 0.107 0.02975     0
6 0.713 0.571 0.46425     0
</code></pre>

<p>Where:<br>
<code>Obs</code> = the observed true prevalence<br>
<code>Pred</code> = the predicted prevalence<br>
<code>Pclass</code> = the proportion of posterior realisations from my model which fell within the endemicity class of interest.<br>
<code>Class</code> - is the true class label.</p>

<p>I used the following R code to calculate what I <em>think</em> a ROC curve should be:</p>

<pre><code>require(ROCR)
pred &lt;- with( mydat , prediction( pred=Pclass ,labels=Class )
plot(perf &lt;- performance(pred,'tpr','fpr') )
</code></pre>

<p>Producing this plot: 
<img src=""http://i.stack.imgur.com/1y1Sx.png"" alt=""enter image description here""></p>

<p><strong>Have I calculated this correctly?<br>
Have I used the right data?! 
Do I have the right data for calculating a ROC curve?<br>
Can you produce a valid ROC curve for a classifier which has a two-sided cut-off?</strong></p>

<p>I did read many tutorials on ROC curves, but my intuition fails me and I can't quite work out if I am applying it to my data in a valid way.
Here is a copy/paste to put my data into R...</p>

<pre><code>mydat &lt;- structure(list(Obs = c(0.441, 0.664, 0.252, 0.226, 0.014, 0.713, 
0.543, 0.777, 0.472, 0.512, 0.436, 0.312, 0.403, 0.709, 0.472, 
0.625, 0.056, 0.335, 0.596, 0.679, 0.143, 0.68, 0.489, 0.319, 
0.706, 0.789, 0.14, 0.261, 0.592, 0.05, 0.736), PRed = c(0.503, 
0.698, 0.468, 0.374, 0.107, 0.571, 0.619, 0.693, 0.569, 0.584, 
0.463, 0.03, 0.363, 0.562, 0.471, 0.313, 0.44, 0.448, 0.506, 
0.617, 0.378, 0.643, 0.603, 0.403, 0.61, 0.743, 0.29, 0.666, 
0.685, 0.044, 0.625), pClass = c(0.56125, 0.22225, 0.58725, 0.39325, 
0.02975, 0.46425, 0.359, 0.2315, 0.4135, 0.419, 0.587, 0, 0.47025, 
0.38625, 0.48825, 0.32425, 0.4725, 0.5615, 0.43975, 0.3605, 0.40825, 
0.313, 0.411, 0.48525, 0.35475, 0.13475, 0.2725, 0.26875, 0.245, 
5e-04, 0.316), Class = c(1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 
1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0)), .Names = c(""Obs"", 
""Pred"", ""pClass"", ""Class""), row.names = c(NA, -31L), class = ""data.frame"")
</code></pre>
"
"0.140859042454753","0.141139359234409","158492","<p>I have fitted a (Cragg's) truncated normal hurdle model over a dataset in which the dependent variable is either zero or positive. The model consists of two parts: a probit which estimates the probability of the value being zero and a truncated regression which is estimated over the subsample of positive values of the dependent variable.</p>

<p>The output of the estimation (using package <code>mhurdle</code> in R) consists of two columns: one gives the probability of y being 0 and the other gives the estimated value for an uncensored observation (let's call this y*).</p>

<p>Now I would like to use these results for prediction, but instead of probabilities and estimated values of the uncensored y I would like to have the estimated values of the dependent variable, including some zeros (or almost zeros). Should I multiply the probability of y NOT being zero by the value of y*? Or should I take all observations for which P(y=0) > 0.5 to be zero and all the others to be equal to y*?</p>

<p>Sorry if the question is trivial but I'm fairly new to statistics and I have not been able to find the answer so far.</p>
"
"0.227128381289749","0.227580378515502","159647","<p>I've been studying (and applying) SVMs for some time now, mostly through <code>kernlab</code> in <code>R</code>.</p>

<p><code>kernlab</code> allows probabilistic estimation of the outcomes through Platt Scaling, but the same could be achieved with a Pool Adjacent Violators (PAV) isotonic regression (Zadrozny and Elkan, 2002).</p>

<p>I've been wrapping my head over this and came with a (clunky, but it works, or yet I think it does) code to try the PAV algorithm.</p>

<p>I divided the task into three pairwise binary classification task, estimated the probabilities on the training data and coupled the pairwise probabilities to get class probabilities (Wu, Lin, and Weng, 2004).</p>

<p>Predictions were made on the training set. I set the Cost really low <code>C=0.001</code> to try to get some misclassifications. </p>

<p>The Brier Score is defined as:</p>

<p>$$BS=\frac{1}N\sum_{t=1}^N\sum_{i=1}^R(f_{ti}-o_{ti})^2 $$</p>

<p>Where $R$ is the number of classes, $N$ is the number of instances, $f_{ti}$ is the forecast probability of the $t$-th instance belonging to the $i$-th class, and $o_{ti}$ is $1$, if the actual class $y_t$ is equal to $i$ and $0$, if the class $y_t$ is different from $i$.</p>

<pre><code>require(isotone)
require(kernlab)

##PAVA SET/VER
data1   &lt;-  iris[1:100,]        #only setosa and versicolor
MR1 &lt;-  c(rep(0,50),rep(1,100)) #target probabilities
KSVM1   &lt;-  ksvm(Species~., data=data1, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED1   &lt;-  predict(KSVM1,iris, type=""decision"")    #SVM decision function
PAVA1   &lt;-  gpava(PRED1, MR1)               #generalized pool adjacent violators algorithm 

##PAVA SET/VIR
data2   &lt;-  iris[c(1:50,101:150),]      #only setosa and virginica
MR2 &lt;-  c(rep(0,50),rep(1,50),rep(0,50))    #target probabilities
KSVM2   &lt;-  ksvm(Species~., data=data2, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED2   &lt;-  predict(KSVM2,iris, type=""decision"")
PAVA2   &lt;-  gpava(PRED2, MR2)

##PAVA VER/VIR
data3   &lt;-  iris[51:150,]   #only versicolor and virginica
MR3 &lt;-  c(rep(0,100),rep(1,50)) #target probabilities
KSVM3   &lt;-  ksvm(Species~., data=data3, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED3   &lt;-  predict(KSVM3,iris, type=""decision"")
PAVA3   &lt;-  gpava(PRED3, MR3)

#Usual pairwise binary SVM
KSVM    &lt;-  ksvm(Species~.,data=iris, type=""C-svc"", kernel=""rbfdot"", C=.001,prob.model=TRUE)

#probabilities on the training data through Platt scaling and pairwise coupling
PRED    &lt;-  predict(KSVM,iris,type=""probabilities"")

#The usual KSVM response based on the sign of the decision function
RES &lt;-  predict(KSVM,iris)

#pairwise probabilities coupling algorithm on kernlab
PROBS   &lt;-  kernlab::couple(cbind(1-PAVA1$x,1-PAVA2$x,1-PAVA3$x))
colnames(PROBS) &lt;- c(""setosa"",""versicolor"",""virginica"")

#Brier score multiclass definition
BRIER.PAVA  &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PROBS[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PROBS[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PROBS[101:150,])^2)/150

#Brier score multiclass definition
BRIER.PLATT &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PRED[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PRED[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PRED[101:150,])^2)/150

BRIER.PAVA

BRIER.PLATT
</code></pre>

<p>Soon I'll clean up a bit and write a proper wrapper function to do it all, but this result's really worrisome for me.</p>

<pre><code>BRIER.PAVA 
[1] 0.09801759
BRIER.PLATT 
[1] 0.6710232
</code></pre>

<p>The Brier Score I got from the probabilities estimated through PAVA is way better than the one we get on Platt Scaling.</p>

<p>If you check <code>PRED</code> you will see all probabilites fall on the ~0.33 range, while on <code>PROB</code> more extreme values (1 or 0) are expected, which was quite unexpected to me as I'm using a really low <code>C</code>.</p>

<p>References:</p>

<p><a href=""http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf"" rel=""nofollow"">Zadrozny, B., and Elkan, C. ""Transforming classifier scores into accurate multiclass probability estimates."" Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002.</a></p>

<p><a href=""http://papers.nips.cc/paper/2454-probability-estimates-for-multi-class-classification-by-pairwise-coupling.pdf"" rel=""nofollow"">T.-F. Wu, C.-J. Lin, and Weng, R.C. ""Probability estimates for multi-class classification by pairwise coupling."" The Journal of Machine Learning Research 5 (2004): 975-1005.</a></p>

<p>EDIT:</p>

<p>Also, if you check the AUC of the different probabilities, they are quite high.</p>

<pre><code>requires(caTools)

AUC.PAVA&lt;-caTools::colAUC(PROBS,iris$Species)

AUC.PLATT&lt;-caTools::colAUC(PRED,iris$Species)

colMeans(AUC.PAVA)
colMeans(AUC.PLATT)
</code></pre>

<p>And here's the result</p>

<pre><code>&gt; colMeans(AUC.PAVA)
    setosa versicolor  virginica 
 0.9988667  0.9988667  0.8455333 
&gt; colMeans(AUC.PLATT)
    setosa versicolor  virginica 
 0.8913333  0.8626667  0.9656000 
</code></pre>

<p>Looking at these AUC, I would say Platt Scaling is a really underconfident technique.</p>
"
"0.188982236504614","0.189358320929341","161929","<p>I've recently jumped into the deep end of statistical analysis of revenue. I've learned a ton about statistics, probability, decomposition (stl), and the Python and R languages. I feel like I'm climbing the learning curve well, even though it's a steep one. Many of my largest hurdles are simply discovering the correct terms to search for on Google in this new field to me.</p>

<p>TL;DR: <strong>What search terms, statistical concepts, or R packages would you use to find the best method of filtering revenue caused by marketing campaigns out of the baseline seasonality of a business?</strong></p>

<p>The end goal that I have now is to filter out marketing effects on historical data to gain a base and to then run some predictive analysis on  our gross revenue prior to future marketing efforts. </p>

<p>I currently have about 4 years of monthly sales revenue and have been able to successfully decompose it with the stl() function in R, do some prediction with various methods including Holt-Winters. </p>

<p>My problem is that I believe some aggressive marketing campaigns in our past is severely corrupting my seasonal vector (and therefore my predictions).</p>

<p>In addition to the 4 years of sales data I have 4 years of monthly data about our email campaigns and have been able to determine a strong correlation between emails sent per month and gross revenue (.74) I was hoping that I might be able to use that data to filter out the marketing effects on gross_revenue and find a better representation of our seasonal trends without marketing effects. </p>

<p>I've looked a little into Marketing Mix Modeling but I wasn't convinced that was the correct tree to bark up and if i could correlate it with my predictive efforts anyway.</p>

<p>I know that pictures and data often speak better than i do, so here's some data I've generated to better communicate my current problem. In both cases I feel like the spikes where we pushed marketing in the top bars have too much influence on the seasonality in the 3rd bars.</p>

<p>Spikes show marketing efforts and are distorting seasonality</p>

<p><img src=""http://i.stack.imgur.com/lHlZS.png"" alt=""Decomposition of time series""></p>

<p>Code for above plot:</p>

<pre><code>ds2 &lt;- ts(ds$gross, start=c(2011,7), frequency=12)
fit &lt;- decompose(ds2, type=""multiplicative"")
plot(fit)
</code></pre>

<p>This one is decomposed with stl()
<img src=""http://i.stack.imgur.com/Dm8u6.png"" alt=""Decomposed with stl() ""></p>

<p>Code used to generate above plot</p>

<pre><code>ds2 &lt;- ts(ds$gross, start=c(2011,7), frequency=12)
fit &lt;- decompose(ds2, type=""multiplicative"")
plot(fit)
</code></pre>
"
"0.125988157669742","0.126238880619561","164912","<p>I am modelling invertebrate.biomass ~ habitat.type * calendar.day + habitat.type * calendar.day ^ 2, with a random intercept of transect.id (50 transects were repeated 5 times)</p>

<p>My response is zero-heavy - about 25% are 0s - and the non-zeroes are strongly right-skewed. </p>

<p>I understand a possible way of dealing with this is to construct 2 models - one modelling a binary response in a logistic regression and the other modelling the non-zero response in a (e.g.) Gamma regression. I'm working in R and following the ideas in <a href=""http://seananderson.ca/2014/05/18/gamma-hurdle.html"" rel=""nofollow"">this post</a>.</p>

<p>I want to check the method of combining the results of these 2 models, in order to generate quantitative predictions (ultimately with CI). Am I correct in multiplying the predicted probabilities from the logistic regression with the predicted (non-zero) biomass from the Gamma regression? Thus, the predicted (non-zero) biomass gets down-weighted according to the probability of there actually being an invertebrate present at all. This makes sense in my head, but feels too easy to be true. </p>

<p>See plots below which demonstrate my method in it's current form.
<a href=""http://i.stack.imgur.com/MVmJc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MVmJc.png"" alt=""Gamma hurdle model""></a></p>

<p>Assuming I'm right so far, how would I then go about generating a SE / CI for the predictions combining two models? </p>
"
"0.251976315339485","0.252477761239121","168725","<p>This question relates to whether it is a good starting point for a cut point in binary classification with logistic regression to the use the mean of the binary response variable as the initial cut point rather than simply 0.5.</p>

<p>Traditionally when people use logistic regression, people with use 0.5 as the threshold to determine when the model predicts YES/positive versus NO/negative.</p>

<p>People may run into trouble when the model only predicts one ""answer"" when using an imbalanced training set.</p>

<p>One way of dealing with this is to balance the training set via oversampling or under-sampling and keeping the test holdout set with the original balance.</p>

<p>However, I suspect that a good starting point for a cut point appears to be the mean of the binary response variable.  Is this usually true?</p>

<p>I created two models, one on a balanced training set and another on the original imbalanced training set.
<code>print(table(actual=test$y, predicted=test$fit&gt;0.5))</code></p>

<pre><code>       predicted
 actual FALSE TRUE
      0  2359  500
      1    11  130
</code></pre>

<p>With the imbalanced training, I used the mean of the binary response variable:</p>

<pre><code>print(table(actual=test$y, predicted=test$fit&gt;0.0496))

       predicted
 actual FALSE TRUE
      0  2317  542
      1     7  134
</code></pre>

<p>If one just uses 0.5, it looks like the model is a complete failure:</p>

<pre><code>`print(table(actual=test$y, predicted=test$fit&gt;0.5))`

       predicted
 actual FALSE
      0  2848
      1   152
</code></pre>

<p>They both had a KS of 0.76, so it seems like sound advice.</p>

<p>Example R code:</p>

<pre><code>require(ROCR)
require(lattice)
#
x=1:10000/10000;
y=ifelse(runif(10000)-0.7&gt;jitter(x),1,0)
#y=ifelse(rnorm(10000)-0.99&gt;x,1,0)
mean(y)

s=sample(length(x),length(x)*0.7);

df=data.frame(x=x,y=y)


##undersample
train=df[s,]
train=rbind(train[train$y==1,],train[sample(which(train$y==0),sum(train$y==1)),])
    ##oversample
    train=df[s,]
    train=rbind(train[train$y==0,],train[sample(which(train$y==1),sum(train$y==0),replace = T),])
mean(train$y) #now balanced
    threshold=0.5
    test=df[-s,] #unbalanced
    mean(test$y)
#

ex=glm(y~x,train, family = ""binomial"")
summary(ex)
nrow(test)
test$fit=predict(ex,newdata = test,type=""response"")
    message(""threshold="",threshold)
    print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

#+results
pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 

#+ imbalanced approach
#############imbalance approach

train=df[s,]
threshold=mean(y)
message(""threshold="",threshold)
ex=glm(y~x,train, family = ""binomial"")
summary(ex)
test$fit=predict(ex,test,type = ""response"")
    summary(test$fit)
print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

print(table(actual=test$y, predicted=test$fit&gt;0.5)) 

pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 
</code></pre>

<p>I noticed a similar question asked <a href=""http://stats.stackexchange.com/questions/91305/how-to-choose-the-cutoff-probability-for-a-rare-event-logistic-regression"">How to choose the cutoff probability for a rare event Logistic Regression</a></p>

<p>I like the answer given here which states to maximize the specificity or sensitivity:
<a href=""http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit/25398#25398"">Obtaining predicted values (Y=1 or 0) from a logistic regression model fit</a></p>

<p>But I also suspect that the usual starting cut off of 0.5 is bad advice.</p>

<p>Comments?</p>
"
"0.288675134594813","0.275475821945538","171375","<p>The concept is simpler than my title.  I have a data.table that represents a sample taken from a population. My goal is to test the performance of several different prediction algorithms across sample sizes with varying levels of support.   But if a particular sample happens to lack sufficient support for a particular row type, I want to adjust that sample to have the bare minimum support necessary to test the algorithm.</p>

<pre><code>n = 1000
samp = data.table(type=sample(10, n, replace=T), # category for this subject
                  prob=runif(n),                 # probability of treatment
                  rand=runif(n),                 # used to assign treatment
                  id=sample(10*n, n))            # id from some larger population
</code></pre>

<p>In reality this sample is drawn from some larger population, and the 'id' column represents the original index in that population and can be used to access additional data collected for this subject.  Normally, treatment is randomly assigned to TRUE or FALSE based on the probability of treatment for that subject.</p>

<pre><code>samp[, treat:=rand&lt;prob]
</code></pre>

<p>But instead I want to be able to require that some minimum number of subjects of each type are treated and untreated.   Let's call this integer variable 'support'.  It will slightly skew my sample to meet this requirement, so I'd like to do so in a minimally invasive manner.</p>

<p>To ensure the requested level of support, I was planning to count the number of each type that are already treated, find the difference between this and 'support', and then switch the 'treat' value to TRUE for the untreated subjects in that category with the highest probability of receiving treatment.   </p>

<p>And vice versa for the untreated, setting 'treat' to false for least likely to be treated among the subjects who did receive treatment.   Oh, and making things a little more complicated, I need to keep the original row order in the final sample.  It's possible that the algorithms I'm ultimately testing will care about ordering, so I need to keep the row order unchanged despite the intervention.  </p>

<p>So what I think I'm looking for is an efficient approach for a function like this:</p>

<pre><code>forceMinSupport = function(samp, support) {
    count = samp[,.N,by=.(type, treat)]
    for (t in c(unique(count$type), 12)) {
        treated = count[type==t &amp; treat==T, N][[1]]
        untreated = count[type==t &amp; treat==F, N][[1]]
        if (treated &lt; support) {  
            numToSwitch = support - treated
            # force treat=T for the numToSwitch untreated with highest prob
        }
        else if (untreated &lt; support) { 
            numToSwitch = support - untreated
            # force treat=F for the numToSwitch treated with lowest prob
        }
    }
}
</code></pre>

<p>I haven't yet figured out a reasonable way to order by one column (prob), subset by another (type), and then modify a value in the third column (treat) for a variable number of rows at the top or bottom of the order.  </p>

<p>Performance is important, as the goal is to test performance over lots of samples.  Sizes will be large for R, but easily small enough to fit into RAM.  The full population from which the sample is drawn will be hundreds of millions, and, the number of rows in the sample will be in the millions.  </p>

<p>The number of rows that will need to be changed in each category will remain small (less than 1000), and for a given type, I should only need to change treatment in a single direction (treated to untreated, or untreated to treated, but never both within the same type).  </p>

<p>So far as possible, I'd like the modifications to be in place.  It's possible that I'll want to be able to increase the level of support in the same sample several times, each time switching only one more subject in each category.  </p>

<p>Suggestions for entirely different but completely better ways to accomplish this are welcome.  </p>
"
"0.0890870806374748","0.0892643685354904","172889","<p>Recently I'm using R survival package to try to predict the probability of people going to churn. I found some <a href=""http://stackoverflow.com/questions/27408862/how-to-predict-survival-probabilities-in-r"">examples</a> on stack overflow and also tried that to my own data. Here is my prediction model and output:</p>

<pre><code>&gt; Status_by_Time &lt;- Surv(time = Duration, event = Gift_Status_ind)
&gt; model.fit2 &lt;- survreg(Status_by_Time ~ Age
                + Gender_ind 
                + Fundraiser_ind
                + Monthly.Recurring.Amount
                + Frequency_ind
                + Monthly.first.gift.amount
                + Monthly.last.gift.amount
                #+ Duration
                #+ Saved.
                + Upgrade.first.time
                + Upgrade.second.time,
                dist = ""logistic""
)

&gt; summary(model.fit2)
                            Value Std. Error      z         p
(Intercept)                81.525    1.46725  55.56  0.00e+00
Age                         0.156    0.01889   8.27  1.33e-16
Gender_ind2                 2.278    0.55955   4.07  4.68e-05
Gender_ind3                -9.514    1.09689  -8.67  4.18e-18
Fundraiser_ind2            -8.798    0.69303 -12.70  6.25e-37
Fundraiser_ind3             4.028    0.90970   4.43  9.52e-06
Monthly.Recurring.Amount   -1.211    0.04856 -24.95 2.39e-137
Frequency_ind2            257.319    0.00000    Inf  0.00e+00
Frequency_ind3              8.562    2.71423   3.15  1.61e-03
Frequency_ind4            -89.067    1.39379 -63.90  0.00e+00
Monthly.first.gift.amount  -2.538    0.03721 -68.22  0.00e+00
Monthly.last.gift.amount    1.827    0.04981  36.67 2.38e-294
Upgrade.first.time          6.467    0.82381   7.85  4.15e-15
Upgrade.second.time        10.849    2.72927   3.98  7.04e-05
Log(scale)                  2.869    0.00841 341.02  0.00e+00

Scale= 17.6 

Logistic distribution
Loglik(model)= -51841.8   Loglik(intercept only)= -55404
Chisq= 7124.45 on 13 degrees of freedom, p= 0 
Number of Newton-Raphson Iterations: 8 
n= 18097 

&gt; predicted.values &lt;- predict(model.fit2, newdata = churn.df.trim, type = ""quantile"", p = (1:9)/10) # 10 times event???
&gt; head(predicted.values)
            [,1]      [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]     [,9]
 [1,]   2.219425 16.513993 26.01508 33.80343 40.95072 48.09800 55.88635 65.38744 79.68201
 [2,]  11.088257 25.382825 34.88392 42.67227 49.81955 56.96683 64.75518 74.25627 88.55084
 [3,] -11.996590  2.297977 11.79907 19.58742 26.73470 33.88198 41.67033 51.17143 65.46599
 [4,]   5.456971 19.751539 29.25263 37.04098 44.18826 51.33555 59.12390 68.62499 82.91955
 [5,]  19.690749 33.985316 43.48641 51.27476 58.42204 65.56932 73.35767 82.85876 97.15333
 [6,]  -8.187469  6.107099 15.60819 23.39654 30.54382 37.69111 45.47946 54.98055 69.27511
</code></pre>

<p>I reckon all these numbers are not probabilities. Is there some way to interpret these numbers or turn them into probabilities? Also if I use <code>p = (1:9)/10</code> does that mean I'm calculating the probability for the next 9 or 10 period?</p>

<p>Much appreciate if someone could give me a straight forward explanation (none academic one). </p>
"
"NaN","NaN","174059","<p>I have fit an extended cox model in R (i.e., some of the covariates change with time), and am now interested in predicting, for the censored observations, the probability that they will survive an additional year. I am fine with kaplan-meier survival curve estimates, and if needed, smoothed estimates for the hazard. </p>

<p>How would I go about getting these probability predictions? Tools for doing this and/or a theoretical understanding of how to do it would be helpful to me.</p>

<p>Thanks!</p>
"
"0.0445435403187374","0.0892643685354904","177062","<p>I noticed that computing ROC with caret package and PROC packege sometimes gives different results. Usually they are the same, but if the predictions are worse than chance, caret will flip them and output 1 - ROC from PRROC. However there is sometimes exception when PRROC = 1-caret's.</p>

<p>example data and code:</p>

<pre><code>library(caret)
library(PRROC)

# True labels
obs  &lt;- c(""c2"", ""c1"", ""c1"", ""c2"", ""c2"", ""c1"", ""c2"", ""c1"", ""c1"")
# Probability of class1
c1 &lt;- c(0.968, 0.282, 0.940, 0.940, 0.532, 0.312, 0.308, 0.730, 0.676)
# probability of class2
c2 &lt;- 1 - c1
# actual prediction
pred &lt;- c(""c1"", ""c2"", ""c1"", ""c1"", ""c1"", ""c2"", ""c2"", ""c1"", ""c1"")
dat1  &lt;- data.frame(obs = obs, c1 = c1, c2 = c2, pred = pred)

obs  &lt;- c(""c2"", ""c1"", ""c2"", ""c2"", ""c1"", ""c1"", ""c1"", ""c2"", ""c1"")
c1 &lt;- c(0.622, 0.816, 0.662, 0.400, 0.434, 0.634, 0.550, 0.500, 0.482)
c2   &lt;- 1 - c1
pred &lt;- c(""c1"", ""c1"", ""c1"", ""c2"", ""c2"", ""c1"", ""c1"", ""c2"", ""c2"")
dat2 &lt;- data.frame(obs = obs, c1 = c1, c2 = c2, pred = pred)

dat &lt;- dat1
#caret ROC
twoClassSummary(dat, lev = c(""c1"", ""c2"")) 
# -&gt; ROC 0.625
#PRROC ROC
roc.curve(dat[dat$obs == ""c1"", ""c1""], dat[dat$obs == ""c2"", ""c1""]) 
# -&gt; ROC 0.375 

dat &lt;- dat2
twoClassSummary(dat, lev = c(""c1"", ""c2""))
# -&gt; ROC 0.45 
roc.curve(dat[dat$obs == ""c1"", ""c1""], dat[dat$obs == ""c2"", ""c1""])
# -&gt; 0.55 
</code></pre>

<p>Can ROC be &lt; 0.5? Which package should I use? I suppose caret will artificially inflate ROC in cross-validation if there is no signal in the data.</p>
"
"0.17817416127495","0.178528737070981","184712","<p>I am trying to </p>

<p>1) classify a bunch of [0,1] ratios into two groups  Group 0: Ratio = 0, Group 1: Ratio != 0.</p>

<p>2) predict the actual response with multiple predictors in R.</p>

<p>My question would then be:</p>

<p>Q1: Can I use the scaled predicted probability as the predicted response? </p>

<p>Q2: Should I classify the group before the regression before running the regression to solve the warning message? Would the data structure/predicted be affected?</p>

<p>I thought of achieving Goal 1 and Goal 2 separately but I can't seem to find a way to fit a unbalanced [0,1] non-censored data with good prediction.</p>

<hr>

<p>Basically my response is something like this</p>

<pre><code>y&lt;-c(rep(0,100),0.3,0.4,0.8,1.0)
x&lt;-cbind(rnorm(104,20,2),as.factor(c(rep(0,90),rep(1,5),rep(0,8),rep(1,1)))
,as.factor(sample(c(1:3),104,TRUE,prob = c(0.6,0.3,0.1))))

data&lt;-data.frame(cbind(y,x))
</code></pre>

<p>and y is strictly between 0 to 1.</p>

<p>I then fit it with a logistic regression and get the predicted probability:</p>

<pre><code>fit&lt;-glm(y~.,data=data, family = ""binomial"")  
fit.prob&lt;-predict(fit,type=""response"")
</code></pre>

<p>I used the probability to make classification model (Goal 1)</p>

<pre><code>class&lt;-y;class[y==0]=""0"";class[y!=0]=""1""

cutoff&lt;-0.06
fit.pred=rep(0,length(fit.prob)); fit.pred[fit.prob &gt;=cutoff]=1
table(fit.pred,class)
</code></pre>

<p>However, I also want to predict y from new data set, this is probably wrong, but here's what I did</p>

<pre><code>se&lt;-fit.prob&lt;-predict(fit,type=""response"",se=T)$se.fit
scaled.fit&lt;-fit.prob/max(fit.prob)
scale.fit.UL&lt;-scaled.fit+1.96*se
scale.fit.LL&lt;-scaled.fit-1.96*se
</code></pre>

<p>and I used this to be the prediction interval for y. Is there any other way to do it other than this?</p>
"
"0.25973124082466","0.260248119427093","186393","<p>I built a multivariate regression tree using the <code>party</code> package in R. The depth of the tree (max. number of splits) is 13. For the first 3/4 splits the tree is relatively easy to interpret which is useful in our case. However with an increase in the number of splits interpretation becomes impossible. The idea is to get a measure of variable importance from this tree, similar to the idea of variable importance in random forests. For random forests there is function <code>varimp</code> but for regression trees it does not seem to exist. I'm aware of the <code>caret</code> package but it is built for CART of the <code>rpart</code> package.</p>

<p>Now, I have an idea of how to measure variable importance in CART but i'm a little lost on how to implement it using the <code>party</code>/<code>partykit</code> package. From <em>Ishwaran (2007)</em>:</p>

<blockquote>
  <p>We define the VIMP for a variable x<sub>v</sub> as the difference between prediction error when x<sub>v</sub> is â€œnoised upâ€ versus the prediction error otherwise. To noise up x<sub>v</sub> we adopt the following convention. To assign a terminal value to a case x, drop x down T [which is your tree] and follow its path until either a terminal node is reached or a node with a split depending upon x<sub>v</sub> is reached. In the latter case choose the right or left daughter of the node with equal probability. Now continue down the tree, randomly choosing right and left daughter nodes whenever a split is encountered (whether the split depends upon x<sub>v</sub> or not) until reaching a terminal node. Assign x the node membership of this terminal node.</p>
</blockquote>

<p>However:</p>

<blockquote>
  <p>This type of scenario shows that a non-informative variable can appear informative over a single tree under our noising up process...Moreover, for a single tree, this kind of problem can be resolved by slightly modifying the noising up process. Rather than using random left-right assignments on all nodes beneath x<sub>v</sub>, use random assignments for only those nodes that split on x<sub>v</sub>. This will impact prediction only when x<sub>v</sub> is informative and not affect prediction for non-informative variables</p>
</blockquote>

<p>How do I go about implementing this procedure? It seems that the <code>fitted_node()</code> function from the <code>partykit</code> package should do the trick. <code>fitted_node()</code> takes the following arguments:</p>

<pre><code>fitted_node(node, data, vmatch = 1:ncol(data), obs = 1:nrow(data), perm = NULL)
</code></pre>

<p>where</p>

<blockquote>
  <p><strong>node</strong>:  an object of class partynode<br>
   <strong>data</strong>: a list or data.frame<br>
   <strong>vmatch</strong>: a permutation of the variable numbers in data<br>
   <strong>obs</strong>: a logical or integer vector indicating a subset of the           observations in  data<br>
  <strong>perm</strong>: a vector of integers specifying the variables to be permuted
  prior before splitting (i.e., for computing permutation variable
  importances).  The default NULL doesnâ€™t alter the data.</p>
</blockquote>

<p>I can recursively partition the <code>data</code> using the tree specified in <code>node</code>. However how do I ""noise up"" one of the splitting variables in my tree? It is not clear to me whether i should use the <code>vmatch</code> and/or <code>perm</code> arguments and how i should specify them (for example do <code>perm</code> and <code>vmatch</code> refer to the column number of the covariate or do they refer to the cells in <code>data</code>?)</p>

<h2>References</h2>

<ol>
<li>Ishwaran, H. (2007). Variable importance in binary regression trees
and forests. Electronic Journal of Statistics, 1, 519â€“537.
<a href=""http://doi.org/10.1214/07-EJS039"" rel=""nofollow"">http://doi.org/10.1214/07-EJS039</a></li>
</ol>
"
"0.154303349962092","0.154610421609022","187657","<p>I have a question about how continuous variables can be used for building models and prediction in a bayesian network.</p>

<p>With some help, I was able to get it to work for continuous variables as follows</p>

<pre><code>#making the data set
col1 = c(1,2,3,4,5)
col2 = c(10,20,30,40,50)
col3 = c(0,1,1,0,0)
df = data.frame(col1,col2,col3)
df$col3 = as.factor(df$col3)

#learning the network
hcbn = hc(df)
hcbn.fit = bn.fit(hcbn,df)

#doing prediction
&gt; predict(hcbn.fit,""col3"",method = ""bayes-lw"",data = df[3,])
[1] 0
Levels: 0 1
</code></pre>

<p>However, this only gives me the class predictions; I would like to see the probability of each class, maybe so , I can fix a class as the prediction only if the probability is greater than a certain threshold etc.</p>

<p>I had initially posted this on stack overflow <a href=""http://stackoverflow.com/questions/34350921/bayesian-network-learning-and-inference-in-r-for-continuous-variables?noredirect=1#comment56506499_34350921"">here</a>, but was suggested that it would be good to ask the same question here. Please have a look at my original post on stack overflow at the link.</p>

<p>As I have mentioned in the original post, if continuous variables are not used, I'm able to get the individual probabilities as follows</p>

<pre><code>#gives me probabilities if continuous variables are not used
mynetwork = hc(dataset,score='bic',restart = 0)
mynetwork.fitted = bn.fit(mynetwork , dataset, method='bayes')
mynetwork.grain &lt;&lt;- as.grain(mynetwork.fitted)

predict(mynetwork.grain, response = c(""myresponsevariable""), newdata = mytestdata, predictors = mypredictors, type = ""distribution"")$pred$myresponsevariable

             0         1
[1,] 0.8745255 0.1254745
</code></pre>

<p>However, this fails when continuous variables like ""numeric"", ""integer"" etc are used with the following error</p>

<p><a href=""http://i.stack.imgur.com/V1Hta.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/V1Hta.png"" alt=""enter image description here""></a></p>
"
"NaN","NaN","189953","<p>In this link <a href=""http://stats.stackexchange.com/questions/96271/prediction-of-continuous-variable-using-bnlearn-package-in-r"">Prediction of continuous variable using &quot;bnlearn&quot; package in R</a> , the author talk about how I can find the conditionl probability of P(node(C)\ the rest node)=P(C\A,B,D,E,F,G) and ot was so nice, but now, How I can fine teh reverse thing: P(A,B,D,E,F,G/C) using bnlearn package. Thanks in advance.</p>
"
"0.0629940788348712","0.0631194403097803","191334","<p>Say I have a CausalImpact Summary like this:</p>

<pre><code>&gt; summary(impact)
Posterior inference {CausalImpact}

                         Average        Cumulative    
Actual                   3              65            
Prediction (s.d.)        2.7 (0.46)     60.4 (10.21)  
95% CI                   [1.8, 3.6]     [40.4, 79.6]  

Absolute effect (s.d.)   0.21 (0.46)    4.56 (10.21)  
95% CI                   [-0.66, 1.1]   [-14.63, 24.6]

Relative effect (s.d.)   7.5% (17%)     7.5% (17%)    
95% CI                   [-24%, 41%]    [-24%, 41%]   

Posterior tail-area probability p:   0.32852
Posterior prob. of a causal effect:  67%

For more details, type: summary(impact, ""report"")
</code></pre>

<p>How would I extract the <code>Actual</code> <code>Average</code> of 3?
How would I extract the <code>Posterior tail-area probability p</code>?</p>

<p>I've tried summarizing numbers from the impact$series dataframe, but the average of the actual values after the date of intervention doesn't equal 3, so I'm stumped.</p>

<p>Thanks!</p>
"
"0.154303349962092","0.154610421609022","192897","<p>I've been a big fan of the gbm package for some time, but am having difficulty understanding the output from the partial dependence plots in the case for multinomial classification problems. </p>

<p>Below is the partial dependence plot for 3 of the many variables I'm considering:</p>

<p><a href=""http://i.stack.imgur.com/B0XET.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/B0XET.png"" alt=""""></a></p>

<p>My problem is that the probability for â€˜Goodâ€™ is always much higher than the others (including on variables not pictured here), but it's not <em>overly</em> prevalent in the data <strong>OR</strong> in the predictions from the model. Here is some further output to clarify that point:</p>

<pre><code>Confusion Matrix and Statistics

          Reference
Prediction  BAD MIDDLE GOOD
    BAD     875    107  163
    MIDDLE  150   1016  231
    GOOD    396    209 1651

Overall Statistics

               Accuracy : 0.7382         
                 95% CI : (0.7255, 0.7506)
    No Information Rate : 0.4262         
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      

                  Kappa : 0.5961         
 Mcnemar's Test P-Value : &lt; 2.2e-16      

Statistics by Class:

                     Class: BAD Class: MIDDLE Class: GOOD
Sensitivity              0.6158        0.7628      0.8073
Specificity              0.9200        0.8901      0.7802
Pos Pred Value           0.7642        0.7273      0.7318
Neg Pred Value           0.8505        0.9071      0.8450
Prevalence               0.2962        0.2776      0.4262
Detection Rate           0.1824        0.2118      0.3441
Detection Prevalence     0.2386        0.2912      0.4702
Balanced Accuracy        0.7679        0.8264      0.7938
</code></pre>

<p>So while the model is able to correctly classify many of the targets as 'Neutral/Middle' (forgive my labelling), the partial dependence plots would suggest this hardly ever happens... </p>

<p>I'm guessing there is something obvious I have missed.. Any insight or thoughts would be hugely appreciated!</p>
"
"0.112687233963802","0.141139359234409","198389","<p>I am using 2-class SVM from <code>e1071</code> R-Package. 
I have been recommended to calibrate the output of classifier.
In this regard, I found this <a href=""http://danielnee.com/tag/platt-scaling/"" rel=""nofollow"">page</a>. The code works for me and I want to apply it in my project; however, I am not sure about the vector of <code>pred</code> which its definition is ""vector of predictions of each observation from the classifier. Should be real number.""</p>

<p>The output of my classifier is like this:</p>

<pre><code>    Decision values:
t34a t34b 
   0    0 
attr(,""decision.values"")
            1/0
t34a -0.2859799
t34b -0.4527416
attr(,""probabilities"")
              1         0
t34a 0.17057557 0.8294244
t34b 0.07377427 0.9262257
Levels: 0 1
</code></pre>

<p>Do you know I should use which of this information for <code>pred</code>? decision values? or probability values of target class, say class 1, i.e. first column of probability values?</p>

<p>Also, do you know how I should apply it in learning procedure?
Typically, the learning procedure is as follows:</p>

<p>1) estimation of performance of SVM using Leave-One-Out cross-validation. </p>

<p>2) training SVM with whole training samples, </p>

<p>3) and then evaluating the model by testing samples.</p>

<p>If I want to use calibration technique, where I can do that?</p>

<p>I appreciated if you could help me.</p>
"
"0.199204768222399","0.19960119601395","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.140859042454753","0.141139359234409","202028","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>I am not too familiar with logistic regression, so I have a few questions about how to properly predict on a new test set using this model:</p>

<p>1) Unlike a regular regression, I cannot simply 'plug-in' the variables and get a meaningful numeric output. Instead, I must first set a threshold probability above which values will be 1 and below which values will be 0. Is this correct?</p>

<p>2) I cannot make use of this sample model or get the same results as the person who provided it until I have the probability threshold that was used for prediction. Is this correct?</p>

<p>3) If I wanted to split the outputs into tiers, would I use the probabilities for that and map them to some other value? How would that process work (feel free to let me know if this is out of scope).</p>

<p>Thanks!</p>
"
"0.0890870806374748","0.0892643685354904","203125","<p>I am building models to predict probability of failures against a list of approximately 500K assets.</p>

<p>I want to optimize my models for maximum predictive performance on a fixed (limited) number of assets e.g. Most accurate predictions on the top 10% (most likely to fail) assets.</p>

<p>I'm not sure how to think about what I want to do or whether it makes sense and would value any advice or suggestions please!</p>

<p>Currently, I'm building models using randomForest and GBM in R and evaluating model performance against random asset selection and ROC curves.</p>
"
"0.17817416127495","0.178528737070981","205944","<p>I am using NaiveBayes for text classification, I am interested on tagging a text (like a blog post). What I am finding is that normally I have results in which a tag has a probability of 0.9999 of being applied to a text and then the closest tag that could also be applied to such text maybe has a probability of 0.000000001. </p>

<p>I started then playing with the ""laplace"" argument when training the NaiveBayes model and also wit the ""threshold"" and ""eps"" when predicting finding that it affects a lot the results... Unfortunately the package documentation does not help me much (maybe due to my lack of stats knowledge?). </p>

<p>The question is of course, which ones would be reasonable values for ""eps"" and ""threshold"" </p>

<p>As an example of a prediction done with the default values of ""eps"" and ""threshold"" (eps = 0 and threshold = 0.001)</p>

<pre><code>                    TAG   PROBABILITY
11          small group  1.000000e+00
9       party/nightlife  2.409428e-22
14                urban  9.573928e-30
4  family friendly/kids  2.428296e-32
2               couples  2.152852e-33
10   rural/country side  8.579935e-55
1             adventure  3.086100e-68
12               sports  1.443652e-93
13             transfer 1.405512e-111
5        food and drink 1.588900e-125
7                nature 1.729492e-127
3              cultural 1.142188e-177
8      outdoor/open air 8.541728e-247
6            historical 9.396091e-252
</code></pre>

<p>As you can see, one tag has a P=1 and the nthe next ones are really far away from it. But, if now I predict using ""eps=1"" and ""threshold=0.1"" I get this</p>

<pre><code>                    TAG  PROBABILITY
11          small group 0.5830973522
6            historical 0.2032655484
14                urban 0.0525206899
13             transfer 0.0503435678
4  family friendly/kids 0.0438961333
9       party/nightlife 0.0159622303
8      outdoor/open air 0.0155044089
3              cultural 0.0110031289
7                nature 0.0082023324
5        food and drink 0.0049013938
12               sports 0.0041011662
10   rural/country side 0.0033009387
1             adventure 0.0031008818
2               couples 0.0008002276
</code></pre>

<p>And for example if I use ""eps=1"" and ""threshold=0.5"" I get this</p>

<pre><code>                TAG PROBABILITY
8      outdoor/open air 0.234848485
3              cultural 0.166666667
7                nature 0.124242424
6            historical 0.116666667
5        food and drink 0.074242424
12               sports 0.062121212
13             transfer 0.053030303
10   rural/country side 0.050000000
1             adventure 0.046969697
14                urban 0.024242424
4  family friendly/kids 0.016666667
2               couples 0.012121212
11          small group 0.012121212
9       party/nightlife 0.006060606
</code></pre>
"
"0.17817416127495","0.178528737070981","206075","<p>I'm relatively new to machine learning (started about 5 months ago), and I'm looking at potentially implementing an ensemble classifier as part of my research. </p>

<p>I have built 3 models that I use to classify whether sales data is going to win or lose. Each model produces the probability of the sale winning or losing, and then I apply thresholds to those to classify them as either a ""Win"", ""Loss"" or ""Borderline Loss"". There are 25 variables, all of which are discrete. </p>

<p>The three models are Naive Bayes, Tree Augmented Naive Bayes (TAN) and Logistic Regression. I am using the bnlearn package for the bayesian classifiers, and a simple glm for the Logistic Regression. All models have high accuracy performances when tested on unseen data:</p>

<p>Naive Bayes Accuracy: 88% </p>

<p>TAN Accuracy: 91%</p>

<p>Logistic Regression Accuracy: 92%</p>

<p>I want to try implementing an ensemble classifier to see if I can get the best possible accuracy across all three models. My question is, how do I go about implementing something like this? I can't find too many examples online, at least not with these models for implementing one. From what I have read, one way to do it is to have a voting system, where if the 2 models predict the sale will win, but 1 predicts with will lose, then it is classified as a win. But what happens in this case if all 3 models had different predictions? I have all my prediction data ready, as in I have all the test data and each models prediction for each sale, my question so is, how would I proceed from here? </p>

<p>If someone knows of any available resources or tutorials that may help, I would greatly appreciate it!</p>
"
"0.125988157669742","0.126238880619561","207549","<p>I have a set of sequences (dataset) where I have sequences of letters. I also have a corresponding response sequences where the known state of the sequence are. I would very much like to make a prediction model that given a sequences of letters will predict the states of the sequence. There are also some known limitations such that the transition probability between staten 1 and 3, and 3 and 1 are equal to zero, such that the sequence are oscilating between 1 and 3. And there also seems to be some kind of pattern of number of ""stays"" at the different states before transition.</p>

<p>example:</p>

<pre><code>position_from, position_to, state
22, 27, state_2
28, 28, state_3
39, 44, state_2
45, 55, state_1
56, 68, state_2
69, 70, state_3
71, 83, state_2
84, 97, state_1
98, 106, state_2
107, 108, state_3
</code></pre>

<p>letter sequence:</p>

<pre><code>MKRNILAVVIPALLVAGTANAAEIFNKDGNKLDLYGKVDVRHQFADKRSSEDGDDSYARI
GIKGETQISDQLTGFGRWEYNVKAKGTEAAVAESSTRLAFAGLKFANY
</code></pre>

<p>Hidden Markov Model does come to my mind for this kind of problem, but maybe there might be other more suitable models for performing this kind of predictions.</p>
"
"0.109108945117996","0.0728840517079004","212027","<p>I am using <code>gbm</code> to predict an imbalanced binary outcome, with the intent of obtaining a ranking by class probability estimation that produces a strong class separation on out-of-sample data.  (I am combining this class probability with other predictions, including from logistic regression, in an ensemble model.)</p>

<p>According to <a href=""http://bioconductor.wustl.edu/extra/vignettes/gbm/inst/doc/gbm.pdf"" rel=""nofollow"">this gbm vignette</a> (Ridgeway, 2007), under ""common user options"" for loss functions:</p>

<blockquote>
  <p>This should be easily dictated by the application.  For most
  classification problems either <code>bernoulli</code> or <code>adaboost</code> will be
  appropriate, the former being recommended. (p. 5)</p>
</blockquote>

<p>There's no explanation provided for favoring bernoulli over <a href=""http://stats.stackexchange.com/questions/37497/how-to-use-r-gbm-with-distribution-adaboost"">adaboost</a> nor any mention of the option for <a href=""https://en.wikipedia.org/wiki/Huber_loss"" rel=""nofollow""><code>huberized</code> loss function</a>, although this function may have been added at a later date.</p>

<p>Related question, but broader than mine:  <a href=""http://stats.stackexchange.com/questions/112359/choosing-between-loss-functions-for-binary-classification"">Choosing between loss functions for binary classification</a>.  This answer references <a href=""http://www.eecs.berkeley.edu/~wainwrig/stat241b/bartlettetal.pdf"" rel=""nofollow"">Bartlett (2006)</a> which is a challenging read for me.</p>

<p>Although performance is satisfactory under the bernoulli loss function, I am having a hard time understanding the justification for selecting one over another.  I'm trying all of them, but are there any theoretical justifications that are at least somewhat intuitive?</p>
"
"0.0629940788348712","0.0631194403097803","215439","<p>I'm currently using <code>naiveBayes</code> from <code>{e1071}</code>. My response is simply a prediction based on my independent variables. Is there a way to get the probability for each possible prediction as a response with naiveBayes? So if I was trying to predict the outcome of a race, the predicted results would look like this :</p>

<pre><code>Name    position  prob
Tom          1    0.1
Tom          2    0.2
Tom          3    0.7
Jim          1    0.2
Jim          2    0.5
Jim          3    0.3
Elizabeth    1    0.8
Elizabeth    2    0.1
Elizabeth    3    0.1
</code></pre>

<p>As opposed to just</p>

<pre><code>Name        position
Tom            3
Jim            2
Elizabeth      1
</code></pre>

<p>If this is not possible with Naive Bayes, are there any other standard models that would support this? Like a Random Forest for instance? I would appreciate any guidance/literature on the matter. </p>
"
"0.140859042454753","0.112911487387527","217344","<p>I have several algorithms which solve a binary classification (with response 0 or 1) problem by assigning to each observation a probability of the target value being equal to 1. All the algorithms try to minimize the log loss function given by the expression </p>

<p>$$
-\frac{1}{N}\sum_{i=1}^N {(y_i\log(p_i) + (1 - y_i)\log(1 - p_i))}
$$</p>

<p>where $N$ is the number of observations, $y_i$ is the actual target value and $p_i$ is the probability of 1 predicted by the algorithm. Here is some R code with sample data:</p>

<pre><code>actual.response = c(1,0,0,0,1)
prediction.df = data.frame(
  method1 = c(0.5080349,0.5155535,0.5338271,0.4434838,0.5002529),
  method2 = c(0.5229466,0.5298336,0.5360780,0.4217748,0.4998602),
  method3 = c(0.5175378,0.5157711,0.5133765,0.4372109,0.5215695),
  method4 = c(0.5155535,0.5094510,0.5201827,0.4351625,0.5069823)
)

log.loss = colSums(-1/length(actual.response)*(actual.response*log(prediction.df)+(1-actual.response)*log(1-prediction.df)))
</code></pre>

<p>The sample code gives the log loss for each algorithm:</p>

<pre><code>method1   method3   method2   method4 
0.6887705 0.6659796 0.6824404 0.6719181 
</code></pre>

<p>Now I want to combine this algorithms so I can minimize the log loss even further. Is there any R package which can do this for me? I will appreciate references to any algorithms, articles, books or research papers which solve this kind of problem. Note that as a final result I want to have the predicted probabilities of each class and note plain 0,1 responses.</p>
"
"0.275838642183685","0.276387575347891","217375","<h2>Introduction</h2>

<p>Suppose I observe 30 subjects attempt a given task in 3 separate occasions and I give them a score. To analyse each subject's performance over time, I can use a multilevel / mixed effect / hierarchical model in which intercepts and slopes are allowed to vary by subject as follows (unfortunately the terminology and the notation vary across disciplines, so bear with me here):</p>

<p>$$
Score_{ij} = \beta_{0j} + \beta_{1j} \times Time_{ij} + e_{ij}
$$</p>

<p>Where:</p>

<p>$$
\beta_{0j} = \beta_{0} + u_{0j}
$$
$$
\beta_{1j} = \beta_{1} + u_{1j}
$$</p>

<p>Here, <em>j</em> = {1, 2 ... 30} indexes subjects and <em>i</em> = {1, 2, 3} indexes the observation. In some disciplines, observations are considered to be <em>nested within</em> subjects, so subjects are the level-2 index and observations the level-1 index (hence ""multilevel"" or ""hierarchical"" models; but again, in some literature one finds that higher numbers indicate more aggregate levels whereas in other literature it is the opposite). </p>

<p><em>Time</em> is a categorical variable that is zeroed at the first observation to model linear change between subjects and within subjects over time (note the two indices) and thus it takes the values <em>Time</em> = {0, 1, 2}. </p>

<p>Now, while $\beta_0$ and $\beta_1$ give the mean intercept and slope that are the same across all subjects (some call them ""fixed"" effects but the terminology is disputed), $u_{0j}$ and $u_{1j}$ are parameters that vary by subjects and are assigned probability distributions (which is why some call them ""random"" effects). Indeed:</p>

<p>$$
u_j \sim N(0,\Omega_u)
$$
$$
\Omega_u = \begin{bmatrix}\sigma^2_{u0} &amp; \\ \sigma_{u01} &amp; \sigma^2_{u1}\end{bmatrix}
$$
And $e_{ij}$ is the residual error, that is, how far the observed values vary around <strong>each subject's</strong> prediction, which is $\hat{y}_j = \hat{\beta} + \hat{u}_j$.
$$
e_{ij} \sim N(0, \sigma^2)
$$</p>

<p>One can say that $u$ are the level-2 random effects and $e$ the level-1 residual.</p>

<p>Now that the definitions are out of the way, here comes the question.</p>

<h2>Question</h2>

<p>How do I calculate the standard error and confidence intervals around a subject's fitted value at each time point? Is my attempt below correct?</p>

<h2>My take</h2>

<p>Let us consider the first subject, so that <em>j</em> = 1 (I will not include the subscript in the section below for clarity). At <strong>time 0</strong> (i.e., <em>Time</em> = 0, <em>i</em> = 1), the fitted value for this individual is:
$$
\hat{y}_{i=1} = \hat{\beta}_0 + \hat{u}_0 + 0 \times (\hat{\beta}_1 + \hat{u}_1) = \hat{\beta}_0 + \hat{u}_0
$$
This is the fitted intercept for subject 1. Clearly the residual error is not part of the equation because it is $e_{ij} = y_{ij} - \hat{y}_{ij}$. Because $Y$ is a sum of random variables, the variance on this subject's intercept should be:
$$
var(\hat{y}_{i=1}) = var(\hat{\beta}_0) + var(\hat{u}_0) + 2 \times cov(\hat{\beta}_0, \hat{u}_0)
$$</p>

<p>However, <a href=""http://glmm.wikidot.com/faq#toc41"" rel=""nofollow"">this</a> page tells me that, and I quote, </p>

<blockquote>
  <p>the model assumes the fixed and random effects to be orthogonal</p>
</blockquote>

<p>so $cov(\hat{\beta}_0, \hat{u}_0) = 0$ and the variance reduces to $var(\hat{y}_{i=1}) = var(\hat{\beta}_0) + var(\hat{u}_0)$.</p>

<p>At <strong>time 1</strong> (<em>Time</em> = 1, <em>i</em> = 2), the fitted score for subject 1 is:</p>

<p>$$
\hat{y}_{i=2} = \hat{\beta}_0 + \hat{u}_0 + 1 \times (\hat{\beta}_1 + \hat{u}_1) = \hat{\beta}_0 + \hat{u}_0 + \hat{\beta}_1 + \hat{u}_1
$$</p>

<p>Again a sum of random variables with $\beta_1$ and $u_1$ being multiplied by a factor $Time = x = 1$, so the variance is:</p>

<p>$$
var(\hat{y}_{i=2}) = var(\hat{\beta}_0) + var(\hat{u}_0) + var(\hat{\beta}_1) + var(\hat{u}_1) + 2  \times (cov(\hat{\beta}_0, \hat{\beta}_1) + cov(\hat{u}_0, \hat{u}_1))
$$</p>

<p>I think that the formula is justified because $\beta$s and $u$s are orthogonal across each other, but not between each other: the global intercept and slope are correlated, and so are the subject's intercept ans slope (and indeed packages like <code>lme4</code> in R will return these correlations, we'll get to this later).</p>

<p>Finally, at <strong>time 2</strong> (<em>Time</em> = 2, <em>i</em> = 3), we have:</p>

<p>$$
\hat{y}_{i=3} = \hat{\beta}_0 + \hat{u}_0 + 2 \times (\hat{\beta}_1 + \hat{u}_1) = \hat{\beta}_0 + \hat{u}_0 + 2\hat{\beta}_1 + 2\hat{u}_1
$$</p>

<p>And:</p>

<p>$$
var(\hat{y}_{i=3}) = var(\hat{\beta}_0) + var(\hat{u}_0) + 2^2var(\hat{\beta}_1) + 2^2var(\hat{u}_1) + 2 \times 2cov(\hat{\beta}_0, \hat{\beta}_1) + 2 \times 2cov(\hat{u}_0, \hat{u}_1)
$$</p>

<p>Since $\beta_1$ and $u_1$ have now a factor $Time = x = 2$ in front of them, their variance is multiplied by $x^2 = 2^2$. </p>

<p>Likewise, we have to take two times the sum of the covariances. The covariance between $\beta_0$ and $\beta_1$ is $x_{\beta_0}x_{\beta_1}cov(\hat{\beta}_0, \hat{\beta}_1)$, that is, $1\cdot{2}\cdot{cov(\hat{\beta}_0, \hat{\beta}_1)}$. The covariance between $u_0$ and $u_1$ is $x_{u_0}x_{u_1}cov(\hat{u}_0, \hat{u}_1)$, that is, $1\cdot{2}\cdot{cov(\hat{u}_0, \hat{u}_1)}$. Since we have to take twice their sum, this expands as in the equation above, $2 \times 2cov(\hat{\beta}_0, \hat{\beta}_1) + 2 \times 2cov(\hat{u}_0, \hat{u}_1)$. Right?</p>

<p>Finally, if I want to calculate the 95% confidence interval around any of the fitted values (i.e., any of the $y$s at time 0, 1, or 2) for my subject I calculate:</p>

<p>$$
CI_{\hat{y}_i} = \pm 1.96 \times \sqrt{var(\hat{y}_i)}
$$</p>

<p>Is this correct?</p>

<p>I am asking because the CIs I calculate using these formulas are rather different from those simulated by <code>lme4</code> and <code>merTools</code> (yes, those are simulation, but I have run <em>many</em> of them and results are consistenly different), so I am wondering whether it is my theory that is flawed. </p>

<p>If the theory is correct, I have a reproducible example ready to add that compares the results using my approach and these two packages and we'll figure out why outcomes differ so much.</p>

<p>Thank you,</p>

<p>k.</p>
"
"0.0629940788348712","0.0631194403097803","218842","<p>I'm using a random forest in R (randomForest) to predict a binary output (1,0) for a dataset that is heavily unbalanced. In this example let's assume the population has 1% 1's and 99% 0's.</p>

<p>Building the random forest on such unbalanced data is difficult and I get much better results when building it on a 50:50 sample.  When predicting a validation set, I obtain the % of trees that predicted that data point to be a 1.  For example, customer A has a 75% probability of being a 1 (based on the # of trees that predicted 1)</p>

<p>If I want to re-scale these predictions back to the original population ratio of 1:99, is there a good way to do this?</p>

<p>In the past I've used logistic regression, and I can adjust the intercept accordingly to down-scale the predicted probability.</p>

<p>Is there a good way to think about this from the RF point of view?  Can I simply just down-weight the predictions from the 50:50 sample by 50 (50% down to 1%)?</p>

<p>Thanks in advance for any thoughts and help</p>
"
"0.166666666666667","0.166998341953264","219385","<p>I have a model of a bernoulli random process I fit using JAGS via the <code>rjags</code> package in R. Here are some example data, as well as code to fit the given models in JAGS via <code>rjags</code>:</p>

<pre><code>#generate some binary 0/1 data
set.seed(666)
x1 = rnorm(1000)           # some continuous variables 
x2 = rnorm(1000)
z = 1 + 2*x1 + 3*x2        # linear combination with a bias
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = rbinom(1000,1,pr)      # bernoulli response variable

#fit a JAGS model. 
require(rjags)

jags.model = ""
model {
for (i in 1:N){
y[i] ~ dbern(p[i])
p[i] &lt;- 1 / (1 + exp(-z[i]))
z[i] &lt;- a + b*x1[i] + c*x2[i]
}
a ~ dnorm(0, .0001)
b ~ dnorm(0, .0001)
c ~ dnorm(0, .0001)
}
""
#setup data as list
data = list(y=y, x1=x1, x2=x2, N = length(y))

#run JAGS model
j.model &lt;- jags.model(file = textConnection(jags.model),
                      data=data,
                      n.chains=3)

#sample from the posterior
jags.out   &lt;- coda.samples (model = j.model,
                            variable.names = c('a','b','c'),
                            n.iter = 1000)
</code></pre>

<p>To prove the model fits, here is some code to generate predicted vs. observed of the z variable (probability of being 0 or 1). </p>

<pre><code>#grab coefficients, compare fitted vs observed of z to prove this fits. 
cf &lt;- summary(jags.out)$statistics[,1]
fitted &lt;- cf[1] + cf[2]*x1 + cf[3]*x2
plot(z ~ fitted, pch=16, cex = 0.2, ylab='observed',xlab='fitted')
</code></pre>

<p>Now that the model fits, What I would like to do is plot the relationship between one of the predictor variables, say <code>x1</code>, and the probabilty of the 0/1 outcome (the prediction for <code>z</code>), <strong>as well as +/- the uncertainty on this prediction</strong>. Getting the mean prediction across the range of <code>x1</code> values is fairly straightforward using the model coefficients:</p>

<pre><code>#okay, not plot the relationship between x1 and z, holding x2 constant
x1.range &lt;- seq(min(x1),max(x1), by = (max(x1)-min(x1))/100)
z.fit &lt;- cf[1] + x1.range*cf[2] + mean(x2) * cf[3]

#plot relationship
plot(z.fit~ x1.range, cex=0)
lines(smooth.spline(z.fit~x1.range), lwd=2, col='purple')
</code></pre>

<p><a href=""http://i.stack.imgur.com/sW3U4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sW3U4.png"" alt=""enter image description here""></a></p>

<p>My question: <strong>How do I estimate the uncertainty associated with this relationship?</strong> An estimate of the standard deviation or the variance would be fine. Anything that would allow me to shade a confidence interval on this figure, given the model I have fit. </p>
"
"0.273809523809524","0.286282871919882","221426","<p>I am working on a problem in which I have multiple pairs of currently living males <code>i</code> that each have a presumed paternal ancestor <code>ni</code> generations ago (based on genealogical evidence) and where I have info on whether there is a mismatch in their Y chromosomal genotype (exclusively paternally inherited, <code>xi</code> = 1 for mismatch, 0 if there is a match). If there is no mismatch, they indeed have a common paternal ancestor, but if there is there must have been a kink in the chain as a result of one or more extra-marital affairs (I can only detect though if either none or at least one such extra-pair paternity event happened, ie the dependent variable is censored). What I am interested in is obtaining a maximum likelihood estimate (plus 95% confidence limits) not just of the mean extra-pair paternity (EPP) rate (probability that per generation a child would be derived from an extra-pair copulation), but also to try to infer how the extra-pair paternity rate may have changed as a function of time (as the nr of generations that separated the common paternal ancestor should have some info on this - when there is a mismatch I don't know though when the EPPs would have happened, as it could have been anywhere between the generation of that presumed ancestor and the present, but when there is a match we are sure there were no EPPs in any of the preceding generations). Hence, both my dependent binomial variable and independent covariate generation/time are censored. Based on a somewhat similar problem posted <a href=""http://stats.stackexchange.com/questions/152111/censored-binomial-model-log-likelihood"">here</a> I already figured out how I could make a maximum-likelihood estimate of the population and time-average extra-pair paternity rate <code>phat</code> plus 95% profile likelihood confidence intervals in R as follows :</p>

<pre><code># Function to make overall ML estimate of EPP rate p plus 95% profile likelihood confidence intervals, 
# taking into account that for pairs with mismatches multiple EPP events could have occured
#
# input is 
#     x=vector of booleans or 0 and 1s specifying if there was a mismatch or not (1 or TRUE = mismatch)
#     n=vector with nr of generations that separate common ancestor
# output is mle2 maximum likelihood fit with best-fit param EPP rate p

estimateP = function(x, n) {
  if (!is.logical(x[[1]])) x = (x==1)
  neglogL = function(p, x, n)  -sum((log(1 - (1-p)^n))[x]) -sum((n*log(1-p))[!x]) # negative log likelihood, see see http://stats.stackexchange.com/questions/152111/censored-binomial-model-log-likelihood
  require(bbmle)
  fit = mle2(neglogL, start=list(p=0.01), data=list(x=x, n=n))
  return(fit)
}
</code></pre>

<p>Example with some pilot data (from <a href=""https://bio.kuleuven.be/ento/pdfs/larmuseau_etal_procb_2013.pdf"" rel=""nofollow"">Larmuseau et al. ProcB 2010</a>):</p>

<pre><code>n = c(7, 7, 7, 7, 7, 8, 9, 9, 9, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 13, 13, 13, 13, 15, 15, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 19, 20, 20, 20, 20, 21, 21, 21, 21, 22, 22, 22, 23, 23, 24, 24, 25, 27, 31) # number of generations/meioses that separate presumed common paternal ancestor
x = c(rep(0,6), 1, rep(0,7), 1, 1, 1, 0, 1, rep(0,20), 1, rep(0,13), 1, 1, rep(0,5)) # whether pair of individuals had non-matching Y chromosomal genotypes
</code></pre>

<p>Maximum-likelihood estimate of population and time-average extra-pair paternity rate plus 95% confidence limits :</p>

<pre><code>fit = estimateP(x,n)
c(coef(fit),confint(fit))*100 # estimated p and profile likelihood confidence intervals
#           p     2.5 %    97.5 % 
#   0.9415172 0.4306652 1.7458847
</code></pre>

<p>i.e. 0.9% [0.43-1.75% 95% C.L.s] of all kids were derived from a father that was different than the supposed one.</p>

<p>I then wanted to go a step further, and also try to estimate a possible temporal trend in the extra-pair paternity rate <code>p</code> as a function of generation <code>ni</code> (for simplicity assuming a linear relationship between the log odds of observing an extra-pair paternity event and generation), taking into account that if a mismatch occurs the EPP events could have taken place anywhere between the generation of the common ancestor <code>ni</code> and the present (generation 0), and that if there was no mismatch that no EPP event could have taken place in any of the previous generations for that particular pair of individuals.</p>

<p>If before we assumed the probability of a child being derived from an extra-pair copulation $p$ to be constant, and if $X$ was a random variable equal to $1$ when a Y chromosome mismatch was observed (corresponding to 1 or more EPP events) and $0$ otherwise, then the probability of observing no mismatch (that is, $X=0$) when the paternal ancestor lived $n$ generations ago ($n = 1, 2, 3, \ldots$) was $(1-p)^n$, whereas the chance of observing an EPP event was</p>

<p>$$\Pr(X=1\,|\, n) = 1 - (1-p)^n.$$</p>

<p>In a dataset of independent observations $\mathbf{x} = (x_1, x_2, \ldots)$ of $X$ with paternal ancestors living $\mathbf{n} = (n_1, n_2, \ldots)$ generations ago, the likelihood therefore was</p>

<p>$$L(p; \mathbf{x}, \mathbf{n}) = \prod_{x_i=1} \left(1 - (1-p)^{n_i}\right)\prod_{x_j=0} (1-p)^{n_j},$$</p>

<p>resulting in a log likelihood of</p>

<p>$$\Lambda(p) = \sum_{x_i=1} \log\left(1 - (1-p)^{n_i}\right) + \sum_{x_j=0} {n_j} \log (1-p).$$</p>

<p>Taking into account that in my more complex model incorporating temporal dynamics I want $p$ to be a function of $n$ now, with 
$$log (p/(1-p)) = a + b.n$$
, i.e. 
$$p(a,b,n) = \exp(a+b.n) / (1+\exp(a+b.n))$$</p>

<p>I then changed the definition of the likelihood function above accordingly and maximized it using function <code>mle2</code> from package <code>bbmle</code> :</p>

<pre><code># ML estimation, assuming that EPP rate p shows a temporal trend
# where log(p/(1-p))=a+b*n
# input is 
#     x=vector of booleans or 0 and 1s specifying if there was a mismatch or not (1 or TRUE = mismatch)
#     n=vector with nr of generations that separate common ancestor
# output is mle2 maximum likelihood fit with best-fit params a and b

estimatePtemp = function(x, n) {
  if (!is.logical(x[[1]])) x = (x==1)
  pfun = function(a, b, n) exp(a+b*n)/(1+exp(a+b*n)) # we now write p as a function of a, b and n
  logL = function(a, b, x, n)  sum((log(1 - (1-pfun(a, b, n))^n))[x]) + 
    sum((n*log(1-pfun(a, b, n)))[!x]) # a and b are params to be estimated, modified from http://stats.stackexchange.com/questions/152111/censored-binomial-model-log-likelihood
  neglogL = function(a, b, x, n)  -logL(a, b, x, n) # negative log-likelihood
  require(bbmle)
  fit = mle2(neglogL, start=list(a=-3, b=-0.1), data=list(x=x, n=n))
  return(fit)
}

# fitted coefficients
estfit = estimatePtemp(x, n)
cbind(coef(estfit),confint(estfit)) # parameter estimates and profile likelihood confidence intervals
#                      2.5 %      97.5 %
#   a -3.09054167 -5.3191406 -1.12078519
#   b -0.09870851 -0.2396262  0.02848305
summary(estfit)
# Coefficients:
#      Estimate Std. Error z value    Pr(z)   
#   a -3.090542   1.057382 -2.9228 0.003469 **
#   b -0.098709   0.067361 -1.4654 0.142819   
</code></pre>

<p>This gives me a reasonable looking historical estimate of the evolution of the EPP rate $p$ over time :</p>

<pre><code>pfun = function(a, b, n) exp(a+b*n)/(1+exp(a+b*n)) 
xvals=1:max(n)
par(mfrow=c(1,1))
plot(xvals,sapply(xvals,function (n) pfun(coef(estfit)[1], coef(estfit)[2], n)), 
     type=""l"", xlab=""Generations ago (n)"", ylab=""EPP rate (p)"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/1WazP.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1WazP.png"" alt=""enter image description here""></a></p>

<p>However, I am still a little stuck on how to calculate the 95% confidence intervals on the overall prediction of this model. Would anybody know how to do that by any chance? Maybe using <a href=""http://ms.mcmaster.ca/~bolker/emdbook/chap7A.pdf"" rel=""nofollow"">population prediction intervals</a> (by resampling parameters according to the fit following a multivariate normal distribution) (or would the delta method also work?)? And could somebody comment on whether my logic above is correct? I was also wondering if this kind of censored binomial model is known under some standard name in the literature, and if anyone knows of any published work on doing these kind of ML calculations under this kind of model? (I have a feeling that the problem should be fairly standard and correspond to something that's been done already, but can't seem to find anything...)</p>

<p>[PS Papers with more background on this topic/problem are available <a href=""https://bio.kuleuven.be/ento/pdfs/larmuseau_etal_procb_2013.pdf"" rel=""nofollow"">here</a> and <a href=""https://bio.kuleuven.be/ento/pdfs/larmuseau_etal_tree_2016.pdf"" rel=""nofollow"">here]</a></p>
"
"0.208927723509336","0.209343500484775","222479","<p>I'm new in this area, hope my question is understandable.
I need to fit conditional logistic regression model in R and use it for predictions on unseen data (output should be probability).
My datasets are  quite large (over 150k rows) and contains many (~500) noisy features.
I found package called <strong>clogitboost</strong> and tried to use it with relatively small number of boosting iterations (max 30, because with larger values it takes too long to compute and raises an error in the end - perhaps, it's resources limitations) - results are mediocre. I tried to use unconditional approach with regularization - <strong>glmnet</strong> and got better results, however, due nature of data I guess it will be better to use conditional regression with regularization similar to what is used in <strong>glmnet</strong> (tried to remove some features and apply <strong>clogitboost</strong> again and got slightly better results). There is package called <strong>clogitL1</strong> , which seems to do that, I tried to use it and it fits model quickly, but it doesn't provide <strong>predict()</strong> function, Usage described in  paper with attached R code here:
<a href=""https://www.jstatsoft.org/article/view/v058i12"" rel=""nofollow"">https://www.jstatsoft.org/article/view/v058i12</a>, they made some predictions in some way, but I can't understand it. Can I somehow manually predict using  unseen data, just like it's possible with <strong>clogitboost</strong> <strong>predict()</strong> (parameters are Model, X and Strata column) using model that was fitted with <strong>clogitL1</strong>? Note: in description of package <strong>clogitL1</strong> - ""Tools for the fitting and cross validation of <strong><em>exact</em></strong> conditional logistic regression models"" - so I'm not sure about what ""exact"" means here and  if it makes sense to use that package for my purposes. If it's not possible to predict, then, should I manually select features by checking their ""importance"" that can be found in <strong>clogitL1</strong> model? </p>
"
"0.125988157669742","0.126238880619561","222910","<p>The following code are used to produce the probability output of binary classification with Random Forest.</p>

<pre><code>library(randomForest) 

rf &lt;- randomForest(train, train_label,importance=TRUE,proximity=TRUE)
prediction&lt;-predict(rf, test, type=""prob"")
</code></pre>

<p>Then the result about prediction is as follows:</p>

<p><a href=""http://i.stack.imgur.com/gQTBS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gQTBS.png"" alt=""enter image description here""></a></p>

<p>The true label about test data are known (named <strong>test_label</strong>).  Now I want to compute <a href=""http://www.r-bloggers.com/making-sense-of-logarithmic-loss/"" rel=""nofollow"">logarithmic loss</a> for probability output of binary classification. The function about LogLoss is as follows.</p>

<pre><code>LogLoss=function(actual, predicted)
{
  result=-1/length(actual)*(sum((actual*log(predicted)+(1-actual)*log(1-predicted))))
  return(result)
}
</code></pre>

<p>How to compute logarithmic loss with probability output of binary classification. Thank you.</p>
"
"0.109108945117996","0.109326077561851","223136","<p>A sample model code written in <code>R</code> is given below:</p>

<pre><code>fit &lt;- rpart(Species ~ ., data = iris, weights = w)
# where w = rep(1/dim(iris)[1], dim(iris)[1])
</code></pre>

<p>What is the purpose of weights and how they are influence the model prediction? How they are useful in re-calibrating the model? Usually the concept of weights is famous in <code>Adaboost</code> and <code>Neural nets</code>. My understanding is For a data point - the higher the weight value gives high probability of correct classification for that point.</p>

<p>Also <a href=""https://docs.tibco.com/pub/enterprise-runtime-for-R/3.1.0/doc/html/Language_Reference/stats/loess.html"" rel=""nofollow"">here</a> I read:</p>

<blockquote>
  <p><strong>weights</strong> - an optional expression for weights to give to individual observations in the sum of squared residuals that forms the local fitting criterion. By default, an unweighted fit is carried out. If it is supplied, weights is treated as an expression to evaluate in the same data frame as the model formula. It should evaluate to a non-negative numeric vector. If the different observations have nonequal variances, weights should be inversely proportional to the variances.</p>
</blockquote>

<p>Please explain the above explanation if it is true and also explain the concept of weights with a detailed example.</p>
"
"0.0890870806374748","0.0892643685354904","224890","<p>I have trained my data with naiveBayes algo in e1071 package. I have 6 classes in my data. I have predicted test data. the prediction returns only one class for each data point but I would like to know the probability of all 6 classes for each data point in test set. Please help me to know how to calculate the same. </p>
"
"0.125988157669742","0.0946791604646705","226109","<p><strong>How does <code>randomForest</code> package estimate class probabilities when I use <code>predict(model, data, type = ""prob"")</code>?</strong></p>

<p>I was using <code>ranger</code> for training random forests using the <code>probability = T</code> argument to predict probabilities. <code>ranger</code> says in documentation that it:</p>

<blockquote>
  <p>Grow a probability forest as in Malley et al. (2012).</p>
</blockquote>

<p>I simulated some data and tried both packages and obtained very different results (see code below)</p>

<p><a href=""http://i.stack.imgur.com/KY48O.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KY48O.png"" alt=""enter image description here""></a></p>

<p>So I know that it uses a different technique (then ranger) to estimate probabilities. But which one?</p>

<pre><code>simulate_data &lt;- function(n){
  X &lt;- data.frame(matrix(runif(n*10), ncol = 10))
  Y &lt;- data.frame(Y = rbinom(n, size = 1, prob = apply(X, 1, sum) %&gt;%
                               pnorm(mean = 5)
                             ) %&gt;% 
                    as.factor()

  ) 
  dplyr::bind_cols(X, Y)
}

treino &lt;- simulate_data(10000)
teste &lt;- simulate_data(10000)

library(ranger)
modelo_ranger &lt;- ranger(Y ~., data = treino, 
                                num.trees = 100, 
                                mtry = floor(sqrt(10)), 
                                write.forest = T, 
                                min.node.size = 100, 
                                probability = T
                                )

modelo_randomForest &lt;- randomForest(Y ~., data = treino,
                                    ntree = 100, 
                                    mtry = floor(sqrt(10)),
                                    nodesize = 100
                                    )

pred_ranger &lt;- predict(modelo_ranger, teste)$predictions[,1]
pred_randomForest &lt;- predict(modelo_randomForest, teste, type = ""prob"")[,2]
prob_real &lt;- apply(teste[,1:10], 1, sum) %&gt;% pnorm(mean = 5)

data.frame(prob_real, pred_ranger, pred_randomForest) %&gt;%
  tidyr::gather(pacote, prob, -prob_real) %&gt;%
  ggplot(aes(x = prob, y = prob_real)) + geom_point(size = 0.1) + facet_wrap(~pacote)
</code></pre>
"
"0.251976315339485","0.252477761239121","226330","<p>Overall, I'd like to be able to say that, for the logistic prediction for this row, ColA was more influential in driving up the resultant probability (ie, y_hat) than ColB. (We'll use y_hat as it's usually defined for logistic.) But is this possible? Some data scientists I've talked to say yes, but I've also seen push-back.</p>

<p>From what I've read, it seems that GLMs make it easiest to get at a per-row variable importance (see <a href=""http://stats.stackexchange.com/q/190482"">this limited discussion</a> on logit in particular, including push-back). But can they actually do it?</p>

<p>If B1 and B2 are coefficients and the cols in X represent our features, it would seem that if <em>B1</em>*X1 is greater than <em>B2</em>*X2 then <em>B1</em>*X1 would drive the resultant probability towards 1 more than <em>B2</em>*X2. Here's an example (which brings in a factor col, for a full treatment).</p>

<p>We create features X1 and X2, where X1 is random and X2 (I think we can agree) has a large positive impact on y:</p>

<pre><code>set.seed(33)
X1 &lt;- runif(10, 0.0, 1.0)
X2 &lt;- c(1,0,1,0,1,0,1,0,1,0)
y &lt;-  c(1,1,1,0,1,0,1,0,1,0)
df &lt;- data.frame(X1,X2,y)
dforig &lt;- df #Need a copy bc multiplying below doesn't work with factors
df$X2 &lt;- as.factor(df$X2)
</code></pre>

<p>Now we create the logit model:</p>

<pre><code>fit.logit = glm(
formula = y~.,
data = df,
family = binomial(link = ""logit""))

                         X1          X21  
Coefficients:       -1.2353      22.0041
Wald statistic:      -0.267        0.003
</code></pre>

<p>Now if we multiply <em>B1</em> and <em>B2</em> by <em>X1</em> and <em>X2</em> respectively and print the results:</p>

<pre><code>coefftemp &lt;- fit.logit$coefficients
coefficients &lt;- coefftemp[2:length(coefftemp)] # drop intercept
multiply_res &lt;- sweep(dforig[,1:2], 2, coefficients, `*`)

          X1       X2
1  -0.55087679 22.00411
2  -0.48751729  0.00000
3  -0.59755734 22.00411
4  -1.13510089  0.00000
5  -1.04245907 22.00411
6  -0.63908954  0.00000
7  -0.53998690 22.00411
8  -0.42395777  0.00000
9  -0.01916833 22.00411
10 -0.14575621  0.00000
</code></pre>

<p>We see that in the rows where X2 = 1 then <em>B2</em>*X2 (ie the second column) is much higher than <em>B1</em>*X1 (ie the first column). So it would seem that we could say that for those rows that X2 would be the dominant feature driving up the resultant prediction towards 1.</p>

<p>If one reverses the y dependency on X2 by replacing zeros for ones in X2, then after doing the multiplication, <em>B2</em>*X2 has a much lower value than <em>B1</em>*X1 when X2 = 1, which makes sense (since X2 now pushes y_hat towards 0 when X2 = 1). Thus, for these rows, X1 is actually more ""responsible"" for driving y_hat towards 1. (Note that if both results are negative, then the least negative would be the feature more responsible for y_hat being as high as it is.) Because of this, it would seem that this method of per-row feature ranking still works. What am I missing? </p>

<p>In case it helps, the code for the latter (reversed dependency) case is below: </p>

<pre><code># Reverse y dependency on X2
set.seed(33)
X1 &lt;- runif(10, 0.0, 1.0)
X2 &lt;- c(0,1,0,1,0,1,0,1,0,1)
y &lt;-  c(1,1,1,0,1,0,1,0,1,0)
df &lt;- data.frame(X1,X2,y)
dforig &lt;- df #Need a copy bc multiplying below doesn't work with factors
df$X2 &lt;- as.factor(df$X2)

fit.logit = glm(
  formula = y~.,
  data = df,
  family = binomial(link = ""logit""))

                         X1          X21  
Coefficients:        -1.235      -22.004
Wald statistic:      -0.267       -0.003

coefftemp &lt;- fit.logit$coefficients
coefficients &lt;- coefftemp[2:length(coefftemp)] # drop intercept
multiply_res &lt;- sweep(dforig[,1:2], 2, coefficients, `*`)
multiply_res

            X1        X2
1  -0.55087679   0.00000
2  -0.48751729 -22.00411
3  -0.59755734   0.00000
4  -1.13510089 -22.00411
5  -1.04245907   0.00000
6  -0.63908954 -22.00411
7  -0.53998690   0.00000
8  -0.42395777 -22.00411
9  -0.01916833   0.00000
10 -0.14575621 -22.00411
</code></pre>

<p>Overall, for logistic, can we accurately say (for example) that feature A drives y_hat toward 1 more than feature B, for this individual prediction? </p>

<p>Thanks, all!</p>
"
"0.154303349962092","0.154610421609022","233366","<p>I am trying to use <code>lme4::glmer()</code> to fit a binomial GLMM with dependent variable that is not binary, but a continuous variable between zero and one. One can think of this variable as a probability; in fact it <em>is</em> probability as reported by human subjects (in an experiment that I help analyzing). The <code>glmer()</code> yields a model that is clearly off, and very far from the one I get with <code>glm()</code>, so something goes wrong. Why? What can I do? </p>

<hr>

<p><strong>More details</strong></p>

<p>Apparently it is possible to use logistic regression not only for binary DV but also for continuous DV between zero and one. Indeed, when I run </p>

<pre><code>glm(reportedProbability ~ a + b + c, myData, family=""binomial"")
</code></pre>

<p>I get a warning message</p>

<pre class=""lang-none prettyprint-override""><code>Warning message:
In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>but a very reasonable fit (all factors are categorical, so I can easily check whether model predictions are close to the across-subjects-means, and they are). </p>

<p>However, what I actually want to use is</p>

<pre><code>glmer(reportedProbability ~ a + b + c + (1 | subject), myData, family=""binomial"")
</code></pre>

<p>It gives me the identical warning, returns a model, but this model is clearly very much off; the estimates of the fixed effects are very far from the <code>glm()</code> ones and from the across-subject-means. (And I need to include <code>glmerControl(optimizer=""bobyqa"")</code> into the <code>glmer</code> call, otherwise it does not converge at all.)</p>
"
