"V1","V2","V3","V4"
"0.124514561272938","0.14106912317172","  5170","<p>I am new to forecasting in R and am trying to automatically fit an ARIMA model to what I believe is a univariate dataset.</p>

<pre><code>&gt; str(p1.z)
'zoo' series from 2009-04-05 to 2010-10-31
  Data: int [1:83] 360 570 540 585 570 690 495 660 510 690 ...
  Index: Class 'Date'  num [1:83] 14339 14346 14353 14360 14367 ...

&gt;  head(p1.z) 
  2009-04-05 2009-04-12 2009-04-19 2009-04-26 2009-05-03 2009-05-10 
         360        570        540        585        570        690
</code></pre>

<p>But when I try to fit the model, I get the error as seen below.</p>

<pre><code>&gt; p1.arima &lt;- auto.arima(p1.z)
Error in nsdiffs(xx) : Non seasonal data
</code></pre>

<p>It is my understanding that the forecast package and the auto.arima function would be able to fit my data seasonal or not.  I am trying to learn time series forecasting and am using a dataset that appears to be ideal for this sort of task .  Also, the function ets() was able to find a model.</p>

<p>Any help you can provide will be greatly appreciated</p>
"
"0.241121411085206","0.236755291373533","  5479","<p>I am teaching myself DLM's using R's <code>dlm</code> package and have two strange results. I am modeling a time series using three combined elements: a trend (<code>dlmModPoly</code>), seasonality (<code>dlmModTrig</code>), and moving seasonality (<code>dlmModReg</code>).</p>

<p>The first strange result is with the <code>$f</code> (one-step-ahead foreacast) result. Most of this forecast appears to be one month behind the actual data, which I believe I've seen in many examples of one-step-ahead forecasting online and in books. The strange thing is that the moving seasonality is NOT similarly lagged, but hits exactly where it should. Is this normal?</p>

<p>If I use the result's <code>$m</code> to manually assemble the componenet, everything lines up perfectly, so it's weird, though it makes sense in a way: the moving seasonality has exogenous data to help it while the rest of the forecast does not. (Still, it'd be nice to simply <code>lag</code> the resulting <code>$f</code> and see a nice match.)</p>

<p>More troubling is the difference I see if I change the degree of <code>dlmModPoly</code>'s polynomial (from 1 to 2) in an attempt to get a smoother level. This introduces a huge spike in all three components at month 9. The spikes all basically cancel out in the composite, but obviously make each piece, say the level or the seasonality, look rather ridiculous there.</p>

<p>Is this just one of those things that happens and I should be prepared to throw away the result's first year of data as ""break-in""? Or is it an indication that something is wrong? (Even in the degree 1 polynomial case, the first year's moving seasonality's level is a bit unsettled, but no huge spike as when I use a degree 2 polynomial.)</p>

<p>Here is my R code:</p>

<pre><code>lvl0 &lt;- log (my.data[1])
slp0 &lt;- mean (diff (log (my.data)))

buildPTR2 &lt;- function (x)
   {
   pm &lt;- dlmModPoly (order=1, dV=exp (x[1]), dW=exp (x[2]), m0=lvl0)
   tm &lt;- dlmModTrig (s=12, dV=exp (x[1]), q=2, dW=exp (x[3:4]))
   rm &lt;- dlmModReg (moving.season, dV=exp (x[1]))

   ptrm &lt;- pm + tm + rm
   return (ptrm)
   }

mlptr2 &lt;- dlmMLE (log (my.data), rep (1, 6), buildPTR2)
dptr2 &lt;- buildPTR2 (mlptr2$par)
dptrf2 &lt;- dlmFilter (log (my.data), dptr2)

tsdiag (dptrf2)

buildPTR3 &lt;- function (x)
   {
   pm &lt;- dlmModPoly (order=2, dV=exp (x[1]), dW=c(0, exp (x[2])), m0=c(lvl0, slp0))
   tm &lt;- dlmModTrig (s=12, dV=exp (x[1]), q=2, dW=exp (x[3:4]))
   rm &lt;- dlmModReg (moving.season, dV=exp (x[1]))

   ptrm &lt;- pm + tm + rm
   return (ptrm)
   }

mlptr3 &lt;- dlmMLE (log (my.data), rep (1, 8), buildPTR3)
dptr3 &lt;- buildPTR3 (mlptr3$par)
dptrf3 &lt;- dlmFilter (log (my.data), dptr3) 
</code></pre>

<p>Per the follow-on question: the data itself is monthly data for 10 years, with each month being the weekly average attendance at a theatrical production. The data definitely has seasonal and moving seasonal effects. I want to model the trend and the seasonal effects to give the management some insight, and to prepare for forecasting. (Which is not directly possible with <code>dlm</code> when you include a <code>dlmModReg</code> component, though that's the next step.)</p>

<p>(I am trying to use an order=2 polynomial component that I believe creates an IRW trend, which is supposed to be nicely smooth.)</p>

<p>If it matters, my moving seasonality is a yearly Big Bash Gala event that can fall in two different months, and I indicate it with 0 for most months and 1 for months in which the Big Bash falls.</p>
"
"0.164717281867254","0.159957350392418","  6329","<p>I've been using the ets() and auto.arima() functions from the <a href=""http://robjhyndman.com/software/forecast/"">forecast package</a> to forecast a large number of univariate time series.  I've been using the following function to choose between the 2 methods, but I was wondering if CrossValidated had any better (or less naive) ideas for automatic forecasting.</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"") {
    XP=ets(x, ic=ic) 
    AR=auto.arima(x, ic=ic)

    if (get(ic,AR)&lt;get(ic,XP)) {
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
        model
}
</code></pre>

<p>/edit: What about this function?</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"",holdout=0) {
    S&lt;-start(x)[1]+(start(x)[2]-1)/frequency(x) #Convert YM vector to decimal year
    E&lt;-end(x)[1]+(end(x)[2]-1)/frequency(x)
    holdout&lt;-holdout/frequency(x) #Convert holdout in months to decimal year
    fitperiod&lt;-window(x,S,E-holdout) #Determine fit window

    if (holdout==0) {
        testperiod&lt;-fitperiod
    }
    else {
        testperiod&lt;-window(x,E-holdout+1/frequency(x),E) #Determine test window
    }

    XP=ets(fitperiod, ic=ic)
    AR=auto.arima(fitperiod, ic=ic)

    if (holdout==0) {
        AR_acc&lt;-accuracy(AR)
        XP_acc&lt;-accuracy(XP)
    }
    else {
        AR_acc&lt;-accuracy(forecast(AR,holdout*frequency(x)),testperiod)
        XP_acc&lt;-accuracy(forecast(XP,holdout*frequency(x)),testperiod)
    }

    if (AR_acc[3]&lt;XP_acc[3]) { #Use MAE
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
    model
}
</code></pre>

<p>The ""holdout"" is the number of periods you wish to use as an out of sample test.  The function then calculates a fit window and a test window based on this parameter.  Then it runs the auto.arima and ets functions on the fit window, and chooses the one with the lowest MAE in the test window.  If the holdout is equal to 0, it tests the in-sample fit.</p>

<p>Is there a way to automatically update the chosen model with the complete dataset, once it has been selected?</p>
"
"0.062257280636469","0"," 11120","<p>I have 5 emerging market foreign exchange total return series, for which I am forecasting single period future returns (1 year). I would like to construct a Markowitz mean variance optimized portfolio of the 5 series, using historical variances and covariances (1) and my own forecast expected returns. Does R have an (easy) way/library to do this? In addition how would I go about calculating (1) is there a built in function?</p>

<p>For interest sake my currencies are USDTRY, USDZAR, USDRUB, USDHUF and USDPLN.</p>
"
"0.062257280636469","0.0705345615858598"," 13069","<p>I am very interested about the potential of statistical analysis for simulation/forecasting/function estimation, etc. </p>

<p>However, I don't know much about it and my mathematical knowledge is still quite limited -- I am a junior undergraduate student in software engineering. </p>

<p>I am looking for a book that would get me started on certain things which I keep reading about: linear regression and other kinds of regression, bayesian methods, monte carlo methods, machine learning, etc.
I also want to get started with R so if there was a book that combined both, that would be awesome. </p>

<p>Preferably, I would like the book to explain things conceptually and not in too much technical details -- I would like statistics to be very intuitive to me, because I understand there are very many risky pitfalls in statistics. </p>

<p>I am off course willing to read more books to improve my understanding of topics which I deem valuable.</p>
"
"0.176090181265125","0.199501867221527"," 13950","<p>As with my previous question, I'm looking at ways to impute missing data in a hierarchical time series data.</p>

<p>With al my other procedures, including the experimentation of imputation packages (<code>Amelia</code>, <code>HoltWinters</code> from <code>Forecast</code> and <code>MICE</code> imputation) I've only been able to use the time series data prior to the missing gap.</p>

<pre><code>     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2001 220 194 238 190 217 244 242 225 242 259 267 244
2002 212 246 250 236 261 286 265 269 226 267 234 246
2003 202 199 297 272 236 266 235 226 260 183 226 265
2004 211 215 219 213 240 236 273 266 262 244 241 235
2005 212 198 233 251 259 282 305 267 241 264 222 269
2006 182 220 250 287 279 281 286 332 300 272 221 233
2007  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
2008 193 215 235 242 246 315 326 280 279 239 236 258
2009 246 189 257 241 268 223 260 288 234 260 216 195
</code></pre>

<p>I'm trying to do simple imputation procedure that uses forecasting and backcasting estimates from the time series model. Forecasting using prior data to predict the future and backcasting  using the later data to â€œpredictâ€ the past.</p>

<p>I would then like to combine the forecast and backcast value to use as imputation. After which I will look at the fit etc.</p>

<p>How do I go about this in coding? </p>

<p>For example, I'm able to determine what SARIMA model exist for the first period 2001-end2006. But not the full period (because my basic functions I know from R does not support the NA values.)</p>

<p>This is only for the period 2001-end2006:</p>

<pre><code>ARIMA(2,0,2)(1,0,1)[12] with non-zero mean 

Call: auto.arima(x = ts.datt) 

Coefficients:
         ar1      ar2      ma1     ma2    sar1     sma1  intercept
      1.3610  -0.8258  -1.2407  0.9191  0.8982  -0.7560   244.8374
s.e.  0.0884   0.0960   0.0878  0.1127  0.2190   0.3335     6.1894

sigma^2 estimated as 605.9:  log likelihood = -335.01
AIC = 686.02   AICc = 688.3   BIC = 704.23
</code></pre>

<p>Should I just model the first period, predict by <code>forecast</code>; model then the last period separately and then backcast? How will I do this backcasting (ie. 'predicting' the past)?</p>

<p><strong>EDIT:</strong>
What I'm asking:
1) How do I use the data from years 2008 &amp; 2009 to BACKCAST? I already know how to use 2001-2006 to forecast. </p>

<p>2) How do I determine the SARIMA model for the whole period? (2001-2009) ie. </p>
"
"0.124514561272938","0.0705345615858598"," 19549","<p>I have univariate time series data (windspeed at a particular place) measured at 1 hour interval for 5 years. </p>

<p>I used <code>auto.arima()</code> to get the following parameters:</p>

<pre><code>              ar1      ar2     ma1     ma2    intercept
             1.5314  -0.55   -0.1261  0.032    10.1223
     s.e.    0.0105  0.0103   0.011   0.006     0.1211

     sigma^2 estimated as 0.4865 : log likelihood = -83546.65
     AIC = 167105.3   AICc = 167105.3    BIC = 167161    
</code></pre>

<p>I am forecasting using the following equation:</p>

<pre><code>e[t] &lt;- rnorm(1, 0, sqrt(sigma^2))
x[t] &lt;- ar1*x[t-1] + ar2*x[t-2] + e[t] + ma1*e[t-1] + ma2*e[t-2]
</code></pre>

<p>When the result is compared with <code>forecast()</code> function, I get completely different answers. The freq spectrum of <code>forecast()</code> function's output resembles original time-series freq spectrum. While the manual forecast signal looks like noise in freq spectrum.</p>

<p>I can't use <code>forecast()</code> function because the application is in C++. Are the equations correct? What's the right way of forecasting from coefficients?    </p>
"
"0.062257280636469","0.0705345615858598"," 19620","<p>I've heard a bit about using <a href=""http://stats.stackexchange.com/questions/9842/getting-started-with-neural-networks-for-forecasting"">neural networks to forecast time series</a>, specifically <a href=""http://stats.stackexchange.com/questions/8000/proper-way-of-using-recurrent-neural-network-for-time-series-analysis"">recurrent neural networks</a>.</p>

<p>I was wondering, is there a recurrent neural network package for R?  I can't seem to find one on <a href=""http://cran.r-project.org/web/views/TimeSeries.html"">CRAN</a>.  The closest I've come is the <a href=""http://www.oga-lab.net/RGM2/func.php?rd_id=tsDyn%3annet"">nnetTs</a> function in the <a href=""http://cran.r-project.org/web/packages/tsDyn/index.html"">tsDyn</a> package, but that just calls the <a href=""http://www.oga-lab.net/RGM2/func.php?rd_id=nnet%3annet"">nnet</a> function from the <a href=""http://cran.r-project.org/web/packages/nnet/index.html"">nnet</a> package.  There's nothing special or ""reccurant"" about it.</p>
"
"0.0880450906325624","0.0498754668053816"," 26183","<p>I would like to convert an ARIMA model developed in R using the <code>forecast</code> library to Java code. Note that I need to implement only the forecasting part. The fitting can be done in R itself. I am going to look at the <code>predict</code> function and translate it to Java code. I was just wondering if anyone else had been in a similar situation before and managed to successfully use a Java library for the same. </p>

<p>Along similar lines, and perhaps this is a more general question without a concrete answer; What is the best way to deal with situations where in model building can be done in Matlab/R but the prediction/forecasting needs to be done in Java/C++? Increasingly, I have been encountering such a situation over and over again. I guess you have to bite the bullet and write the code yourself and this is not generally as hard as writing the fitting/estimation yourself. Any advice on the topic would be helpful. </p>
"
"0.164717281867254","0.186616908791155"," 28737","<p>I have time series as </p>

<pre><code>0.4385487 0.7024281 0.9381081 0.8235792 0.7779642 1.1670665 1.1958634 1.1958634 0.8235792 0.8530141 0.8802216 1.1958634 1.1235897 1.3542734 1.3245534 0.9381081 1.1670665 1.1958634 0.8802216 1.3542734 1.1670665 4.9167998 0.9651803 0.8221709 1.1070461 1.2006974 1.3542734 0.9651803 0.9381081 0.9651803 0.8854192 1.3245534 1.1235897 1.2006974 1.1958634 0.4385487 1.3245534 4.9167998 1.2277843 0.8530141 1.0018480 0.3588158 0.8530141 0.8867365 1.3542734 1.1958634 1.1958634 0.9651803 0.8802216 0.8235792 4.9167998 1.1958634 0.9651803 0.8854192 0.8854192 1.2006974 0.8867365 0.9381081 0.8235792 0.9651803 0.4385487 0.9936722 0.8821301 1.3542734 1.1235897 1.6132899 1.3245534 1.3542734 0.8132233 0.8530141 1.1958634 1.2279813 0.8354292 1.3578511 1.1070461 0.8530141 0.9670581 1.1958634 0.7779642 1.2006974 1.1958634 0.8235792 1.3245534 0.5119648 2.3386331 0.8890464 0.8867365 4.9167998 1.2006974 1.2006974 0.6715839 4.9167998 0.7747481 4.9167998 0.8867365 1.2277843 0.8890464 1.2277843 0.8890464 1.0541099 0.8821301 
</code></pre>

<p>I am using package ""itsmr""-autofit(),""forecast""-auto.arima(),""package""--functions</p>

<ol>
<li><p>Autoregressive model</p>

<pre><code>&gt; ar(t)

Call:
    ar(x = t)

    Order selected 0  sigma^2 estimated as  0.9222 
</code></pre></li>
<li><p>ARMA model</p>

<pre><code>&gt; autofit(t)
    $phi
    [1] 0

    $theta
    [1] 0

    $sigma2
    [1] 0.9130698

    $aicc
    [1] 279.4807

    $se.phi
    [1] 0

    $se.theta
    [1] 0
</code></pre></li>
<li><p>ARIMA model</p>

<pre><code>    &gt; auto.arima(t)
    Series: t 
    ARIMA(0,0,0) with non-zero mean 

    Coefficients:
          intercept
             1.2623
    s.e.     0.0951

    sigma^2 estimated as 0.9131:  log likelihood=-138.72
    AIC=281.44   AICc=281.56   BIC=286.67
</code></pre>

<p>The auto.arima function automatically differences time series: we don't have to worry about transformation.</p>

<pre><code>&gt; auto.arima(AirPassengers)
Series: AirPassengers 
ARIMA(0,1,1)(0,1,0)[12]                    

Coefficients:
          ma1
      -0.3184
s.e.   0.0877

sigma^2 estimated as 137.3:  log likelihood=-508.32
AIC=1020.64   AICc=1020.73   BIC=1026.39`
</code></pre></li>
</ol>

<p>Which model should I select to get p,q values &amp; for forecasting purpose?</p>
"
"0.329434563734508","0.333244479984205"," 29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"0.152498570332605","0.172773685116272"," 29424","<p>I'm looking for some forecasting advice when dealing with seasonal time series data that has a large number of observations.  By ""large"" I only mean a few thousand --- I'm used to such sizes in Data Mining being considered pretty small, but it seems that in time series modeling that's pretty unwieldy for many of the tools I've tried.</p>

<p>For example, here's a toy data set that records an observation once per minute, for five days:</p>

<pre><code>set.seed(123)
t &lt;- 1:(5*24*60)
x &lt;- ts(15 + 0.001*t + 10*sin(2*pi*t/(length(t)/5)) + rnorm(length(t)), freq=length(t)/5)
plot(x, type='l')
</code></pre>

<p><img src=""http://i.stack.imgur.com/xVSCN.png"" alt=""time series plot""></p>

<p>(In my real operational data set, the values are observed at irregular intervals, but I've regularized them by doing something like <code>x &lt;- approx(d$t, d$x, xout=1:(5*24*60))</code> first.  Advice on whether that's advisable, or alternative approaches, is welcome too.)</p>

<p>So the seasonality in this data set has a lag of 1,440 observations, which seems to be way outside the range that things like <code>auto.arima()</code> (in the <code>forecast</code> package) will find:</p>

<pre><code>m1 &lt;- auto.arima(x)
plot(forecast(m1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/ccnGc.png"" alt=""prediction plot""></p>

<p>And I'm not quite sure how to interpret the <code>ets()</code> function here, but it doesn't seem to be able to handle this size data, and it didn't seem to pick up on the seasonality:</p>

<pre><code>&gt; m2 &lt;- ets(x, 'MAZ')
&gt; plot(forecast(m2))
Error in forecast.ets(m2) : Forecast horizon out of bounds
&gt; m2$method
[1] ""ETS(M,A,N)""
</code></pre>

<p>Where to go from here?  Any suggestions?  Thanks.</p>
"
"0.0880450906325624","0.0997509336107633"," 32657","<p>I was playing with the <a href=""http://cran.r-project.org/web/packages/TSA/index.html"" rel=""nofollow"">TSA</a> package in R and wanted to test the <code>arimax</code> function to the solution provided in Pankratz's <em>Forecasting with Dynamic Regression Models</em>, chapter 8. The savings rate and the function seems to provide similar results as the ones in the book except for the IO weights which are quite different. I bet there is a transformation that I might be missing.</p>

<p>Any help on understanding why IO coefficients are so different would be appreciated...</p>

<p>the solution states </p>

<pre><code>AO @ t=82,43,89
LS @ t=99
IO @ t=62,55
</code></pre>

<p>with Parameters estimates</p>

<pre><code>C = 6.1635
w82 = 2.3346
w99 = -1.5114
w43 = 1.1378
w62 = 1.4574
w55 = -1.4915
w89 = -1.0702
AR1 = 0.7976
MA2 = -0.3762
</code></pre>

<p>To fit the model in R, I used
(<code>saving</code> is the data)</p>

<pre><code>arimax(saving, order = c(1,0,2), fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA), io=c(55,62), 
       xreg=data.frame(AO82=1*(seq(saving)==82),
                       AO43=1*(seq(saving)==43),
                       AO89=1*(seq(saving)==89),
                       LS99=1*(seq(saving)&gt;=99)),
       method='ML')
</code></pre>

<p>The savings rate data is (100 points)</p>

<p>4.9
5.2
5.7
5.7
6.2
6.7
6.9
7.1
6.6
7
6.9
6.4
6.6
6.4
7
7.3
6
6.3
4.8
5.3
5.4
4.7
4.9
4.4
5.1
5.3
6
5.9
5.9
5.6
5.3
4.5
4.7
4.6
4.3
5
5.2
6.2
5.8
6.7
5.7
6.1
7.2
6.5
6.1
6.3
6.4
7
7.6
7.2
7.5
7.8
7.2
7.5
5.6
5.7
4.9
5.1
6.2
6
6.1
7.5
7.8
8
8
8.1
7.6
7.1
6.6
5.6
5.9
6.6
6.8
7.8
7.9
8.7
7.7
7.3
6.7
7.5
6.4
9.7
7.5
7.1
6.4
6
5.7
5
4.2
5.1
5.4
5.1
5.3
5
4.8
4.7
5
5.4
4.3
3.5</p>

<p>here it is my output</p>

<pre><code>&gt; arimax(saving, order = c(1,0,2),fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA),io=c(55,62),xreg=data.frame(AO82=1*(seq(saving)==82),
+ AO43=1*(seq(saving)==43),AO89=1*(seq(saving)==89),LS99=1*(seq(saving)&gt;=99)),method='ML')

Call:
arimax(x = saving, order = c(1, 0, 2), xreg = data.frame(AO82 = 1 * (seq(saving) == 
    82), AO43 = 1 * (seq(saving) == 43), AO89 = 1 * (seq(saving) == 
    89), LS99 = 1 * (seq(saving) &gt;= 99)), fixed = c(NA, 0, NA, NA, NA, NA, 
    NA, NA, NA, NA), method = ""ML"", io = c(55, 62))

Coefficients:
         ar1  ma1     ma2  intercept    AO82    AO43     AO89     LS99    IO-55   IO-62
      0.7918    0  0.3406     6.0628  2.3800  1.1297  -1.0466  -1.4885  -0.5958  0.5517
s.e.  0.0674    0  0.1060     0.3209  0.3969  0.3780   0.3835   0.5150   0.4044  0.3772

sigma^2 estimated as 0.2611:  log likelihood = -75.57,  aic = 169.14
</code></pre>
"
"0.139211511597426","0.157720074469128"," 32694","<p>I'm using R together with the <code>forecast</code> package to set up a ARIMA model, that will be used to predict a energy related variable. I used <code>auto.arima()</code> to fit different models (according to geographic region), and I need to put the model coefficients in our database, so that the IT folks can automate things. That's exactly the problem: I simply don't know how set up the equations by looking at the model:</p>

<pre><code>ARIMA(1,0,1)(2,0,1)[12] with non-zero mean 

Coefficients:

       ar1     ma1    sar1    sar2     sma1   intercept    prec0    prec1
     0.3561  0.3290  0.6857  0.2855  -0.7079  11333.240   15.5291  28.0817

s.e. 0.2079  0.1845  0.2764  0.2251   0.3887   2211.302    6.2147   6.0906
</code></pre>

<p>I have 2 regressor variables (prec0 and prec1). Given the residuals, the ARIMA vector <code>ARIMA(1,0,1)(2,0,1)[12]</code>, the time series up to period $t$, the number $h$ of forecasting periods and the regressor matrix reg, how can I set a function to return the forecast values? I.e:</p>

<pre><code>do.forecast = function(residuals, ARIMA, timeSeries, h, regMatrix)
{
  p = ARIMA[1]
  q = ARIMA[3]

  ## arima equations here...
}
</code></pre>

<p>Thanks!  </p>

<p>PS: I know this is a possible duplicate of <a href=""http://stats.stackexchange.com/questions/23881/reproducing-arima-model-outside-r"">Reproducing ARIMA model outside R</a>, but my model seems very different, and I really don't know how to start with.</p>
"
"0.0880450906325624","0"," 34690","<p>I wanted to focus on volatility forecasting, so instead of asking R to compute a GARCH where it would compute the errors on the returns, I wanted to model the volatility as an ARMA and add an external regressor using the argument xreg in the arima function.</p>

<p>I have two questions:</p>

<ul>
<li><p>Is it exactly equivalent to compute an ARMA(p,q) on the volatility with external regressors as the squared returns and to compute a GARCH (for the volatility forecast)</p></li>
<li><p>Is it the correct way to do it in R ?</p></li>
</ul>

<p>Tony</p>
"
"0.27137319479402","0.291271287770251"," 35489","<p>I have real daily market data which I'm looking at to create a model for forecasting. The model that I created (below) used autoregressive terms within a linear regression.</p>

<p>I was sharing this with a colleague and he said ""autoregressive variables are correlated with the other variables in multiple linear setting which creates multicollinarity problem, creating unreliable result.""</p>

<p>So I'm turning to the group for help. Here is the data and the analysis that I performed in R.</p>

<pre><code>#Read in Data
MarketData = read.table('http://sharp-waterfall-3397.herokuapp.com/MarketCategories6.txt', header=TRUE,na.strings = ""NA"", sep="","")
MarketData$Month &lt;- as.factor(MarketData$Month)
MarketData$Weekday &lt;- as.factor(MarketData$Weekday)

str(MarketData)
</code></pre>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/PERregress/index.html"" rel=""nofollow"">PERregress</a> library to help with the autoregression using the <code>back()</code> function and to help with the residual diagnostics:</p>

<pre><code>library(PERregress)
descStat(MarketData)
</code></pre>

<p>Subsetting the data for model building and prediction purposes:</p>

<pre><code>Total = MarketData
MarketData = MarketData[1:268,]
attach(MarketData)
</code></pre>

<p>Here is a regression with everything that I can think of. Note you can have higher autoregressive terms but this will start to mask events since R will ignore the first several rows. Also just an FYI for some reason the residual analysis is breaking which I liked to look for points with undue leverage.</p>

<pre><code>#Market1Category1 Regression for the markets with everything that I can think of it
Market1Category1Output=lm(Market1Category1 ~ Trend+Month2+Month3+Month4+
                          Month5+Month6+Month7+Month8+Month9+Monday+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday2+Holiday3+Holiday4+
                          Event1+Event2+Event3+Event4+Event5+Event6+Event7+
                          Event8+Event9+Event10+Event11+Event12+Event13+
                          Event14+Event15+Event16+Event17+Event18+Event19+
                          Event20+Event21+Event22+Event23+Event24+Event25+
                          Event26+Event27+Event28+
                          back(Market1Category1)+back(Market1Category1, 2))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is the final equation. I'd like to say that I reduced the variables using partial f-test but I couldn't find an easy way to do this so if you know a function please let me know. Basically I looked at the change in adjusted $R^2$.</p>

<pre><code>#Final regression equation 
Market1Category1Output=lm(Market1Category1 ~ Month5+Month6+Month7+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday3+Event2+Event7+Event10+
                          Event13+Event16+Event25+Event28+
                          back(Market1Category1)+back(Market1Category1, 6))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is a plot of the actuals in green vs the predictions in blue but there's a problem:</p>

<pre><code>plot(Time, Market1Category1, col='green')
points(Time, predict(Market1Category1Output, MarketData), col='blue', pch=20)
</code></pre>

<p>The issue is that predict will use the data values instead of it's predicted values for the autoregressive terms. In order to make it use predicted terms I created this loop. If you know a better way let me know.</p>

<pre><code>dataSet2 &lt;- Total
dataSet2[8:length(dataSet2$Time),""Market1Category1""] &lt;- NA
    for (i in (1:(length(dataSet2$Time)-7))) {
  dataSet2[6+i+1,""Market1Category1""] &lt;- 1
  dataSet2[6+i+1,""Market1Category1""] &lt;- predict(Market1Category1Output, 
                                                dataSet2[0:6+i+1,])[6+1] 
}
</code></pre>

<p>Here is the plot again with the results in blue using the predicted results for the autoregressive terms (with the exception of the first 7 since the model needs those to <code>predict</code>):</p>

<pre><code>plot(Total$Time, Total$Market1Category1, col='green')
points(dataSet2$Time, dataSet2$Market1Category1, col='blue', pch=20)
</code></pre>

<p>So here are my questions in order of importance:</p>

<ol>
<li>Does using autoregressive and linear terms violate any fundamental assumptions?</li>
<li>What issues can this cause and what analysis/steps should I do take to avoid these problems?</li>
<li>Is there a better approach to modeling this timeseries?</li>
<li>Is there a more efficient approach?</li>
<li>Given the residuals what steps would you take?</li>
</ol>

<p>Finally two questions which is just causing me more work than possibly necessary:</p>

<ol>
<li>As you can see instead of using the factors for weekday and month I'm using separate conditional variables. I'm doing this because if I use the factor and a level turns out to be insignificant (e.g., Monday for days of the week). I can't remove it. Perhaps there's a way?</li>
<li>Is there a quick way to run a partial F-statistic to understand whether removing a variable makes sense?</li>
</ol>
"
"0.176090181265125","0.199501867221527"," 47185","<p>I am impressed by the R <code>forecast</code> package, as well as e.g. the <code>zoo</code> package for irregular time series and interpolation of missing values.</p>

<p>My application is in the area of call center traffic forecasting, so data on weekends is (nearly) always missing, which can be nicely handled by <code>zoo</code>. Also, some discrete points may be missing, I just use R's <code>NA</code> for that.</p>

<p>The thing is: all the nice magic of the forecast package, such as <code>eta()</code>, <code>auto.arima()</code> etc, seem to expect plain <code>ts</code> objects, i.e. equispaced time series not containing any missing data. I think real world applications for equispaced-only time series are definitely existent, but - to my opinion -  v e r y  limited.</p>

<p>The problem of a few discrete <code>NA</code> values can easily be solved by using any of the offered interpolation functions in <code>zoo</code> as well as by <code>forecast::interp</code>. After that, I run the forecast.  </p>

<p>My questions:  </p>

<ol>
<li>Does anyone suggest a better solution?</li>
<li><p><strong>(my main question)</strong> At least in my application domain, call center traffic forecasting (and as far as I can imagine most other problem domains), time series are not equispaced. At least we have recurring ""business days"" scheme or something. What's the best way to handle that and still use all the cool magic of the forecast package?  </p>

<p>Should I just ""compress"" the time series to fill the weekends, do the forecast, and then ""inflate"" the data again to re-insert NA values in the weekends? (That would be a shame, I think?)  </p>

<p>Are there any plans to make the forecast package fully compatible with irregular time series packages like zoo or its? If yes, when and if no, why not?  </p></li>
</ol>

<p>I'm quite new to forecasting (and statistics in general), so I might overlook something important.</p>
"
"0.124514561272938","0.0705345615858598"," 47416","<p>We know that dealing with model involving MA factors is not easy to estimate, since there are past values of errors to be computed recursively. And this recursive estimation requires preliminary (initial) estimates of the parameters. For example, an ARMA(1,2)
$$z_t=\phi z_{t-1}-\theta_1 \varepsilon_{t-1}-\theta_2 \varepsilon_{t-2}+\varepsilon_t$$
To estimate the parameters, we need to compute first the values of $\varepsilon_{t-1}$ and $\varepsilon_{t-2}$, since these are not available yet. And they are computed using
$$\varepsilon_t=z_t-\phi z_{t-1}+\theta_1 \varepsilon_{t-1}+\theta_2 \varepsilon_{t-2}$$
Procedures for obtaining preliminary estimates of the parameters is available in Box and Jenkins, Time Series: Forecasting and Control. And this estimation is already available in much statistical software. My question is, ""Is there a function for obtaining a preliminary estimate of the parameters in R?""</p>

<p>I need this to obtain a preliminary estimate for my Space-Time ARIMA. Another question is, ""How does <code>arima</code> function of R compute preliminary estimates of the parameters when there are MA factors involved?""</p>

<p>Thanks in advance!</p>
"
"0.196874807739539","0.223049868372735"," 56374","<p>as I am stepping into forecasting with ARIMA models, I am trying to understand how I can improve a forecast based on ARIMA fit with seasonality and drift. </p>

<p>My data is the following time series ( over 3 years, with clear trend upwards and visible seasonality, which seems to be not supported by autocorrelation at lags 12, 24, 36??). </p>

<pre><code>    &gt; bal2sum3years.ts
             Jan     Feb     Mar     Apr     May     Jun     Jul     Aug          
    2010 2540346 2139440 2218652 2176167 2287778 1861061 2000102 2560729 
    2011 3119573 2704986 2594432 2362869 2509506 2434504 2680088 2689888 
    2012 3619060 3204588 2800260 2973428 2737696 2744716 3043868 2867416 
             Sep     Oct     Nov     Dec
    2010 2232261 2394644 2468479 2816287
    2011 2480940 2699780 2760268 3206372
    2012 2951516 3119176 3032960 3738256
</code></pre>

<p>The model that was suggested by <code>auto.arima(bal2sum3years.ts)</code> gave me the following model:</p>

<pre><code>    Series: bal2sum3years.ts 
    ARIMA(0,0,0)(0,1,0)[12] with drift         

    Coefficients:
              drift
          31725.567
    s.e.   2651.693

    sigma^2 estimated as 2.43e+10:  log likelihood=-321.02
    AIC=646.04   AICc=646.61   BIC=648.39
</code></pre>

<p>However, the <code>acf(bal2sum3years.ts,max.lag=35)</code> does not show acf coefficients higher than 0.3. The seasonality of the data is, however, pretty obvious - spike at the beginning of every year. This is what the series looks like on the graph:
<img src=""http://i.stack.imgur.com/kQi5N.png"" alt=""Original Time Series""></p>

<p>The forecast using <code>fit=Arima(bal2sum3years.ts,seasonal=list(order=c(0,1,0),period=12),include.drift=TRUE)</code> , called by function <code>forecast(fit)</code>, results in the next 12months's means being equal to the last 12 months of the data plus constant. This can be seen by calling <code>plot(forecast(fit))</code>, </p>

<p><img src=""http://i.stack.imgur.com/GJqcG.png"" alt=""Actual and Forecasted Data""></p>

<p>I have also checked the residuals, which are not autocorrelated but have positive mean ( non zero). </p>

<p>The fit does not model the original time series precisely, in my opinion ( blue the original time series, red is the <code>fitted(fit)</code>:</p>

<p><img src=""http://i.stack.imgur.com/ux3i7.png"" alt=""Original vs fit""></p>

<p>The guestion is, is the model incorrect? Am I missing something? How can I improve the model? It seems that the model literally takes the last 12 months and adds a constant to achieve the next 12 months. </p>

<p>I am a relative beginner in time series forecasting models and statistics. </p>

<p>Thank you very much for your answers!</p>
"
"0.164717281867254","0.159957350392418"," 58407","<p>I think this is a basic question, but maybe I am confusing the concepts.</p>

<p>Suppose I fit an ARIMA model to a time series using, for example, the function auto.arima() in the R forecast package. The model assumes constant variance. How do I obtain that variance? Is it the variance of the residuals?</p>

<p>If I use the model for forecasting, I know that it gives me the conditional mean. I'd like to know the (constant) variance as well.</p>

<p>Thank you.</p>

<p>Bruno</p>

<hr>

<h2>Update 1:</h2>

<p>I added some code below. The variance given by <code>sigma2</code> isn't close to the one calculated from the fitted values. I'm still wondering if <code>sigma2</code> is the right option. See figure below for time series plot.</p>

<pre><code>demand.train &lt;- c(10.06286, 9.56286, 10.51914, 12.39571, 14.72857, 15.89429, 15.89429, 17.06143, 
              17.72857, 16.56286, 14.23000, 15.39571, 13.06286, 15.39571, 15.39571, 16.56286,
              16.21765, 15.93449, 14.74856, 14.46465, 15.38132)
timePoints.train &lt;- c(""Q12006"", ""Q22006"", ""Q32006"", ""Q12007"", ""Q22007"", ""Q32007"", ""Q12008"", ""Q22008"",
                      ""Q32008"", ""Q12009"", ""Q22009"", ""Q32009"", ""Q12010"", ""Q22010"", ""Q32010"", ""Q12011"",
                      ""Q22011"", ""Q32011"", ""Q12012"", ""Q22012"", ""Q32012"")

plot(1:length(timePoints.train), demand.train, type=""o"", xaxt=""n"", ylim=c(0, max(demand.train) + 2), 
     ylab=""Demand"", xlab=""Quadrimestre"")

title(main=""Time Series Demand of Product C"", font.main=4)
axis(1, at=1:length(timePoints.train), labels=timePoints.train)
box()

### ARIMA Fit
library(forecast)

# Time series
demandts.freq &lt;- 3
demandts.train &lt;- ts(demand.train, frequency=demandts.freq, start=c(2006, 1))

# Model fitting
demandts.train.arima &lt;- auto.arima(demandts.train, max.p=10, max.q=10, max.P=10, max.Q=10, max.order=10)
print(demandts.train.arima)
summary(demandts.train.arima)
demandts.train.arima.fit &lt;- fitted(demandts.train.arima)

# Forecast ARIMA (conditional means)
demandts.arima.forecast &lt;- forecast(demandts.train.arima, h = 3, level=95)
print(demandts.arima.forecast)

# Constant variance from ARIMA
demandts.arima.var &lt;- demandts.train.arima$sigma2
print(demandts.arima.var)

# Variance from fitted values
print(var(demandts.train.arima.fit))
</code></pre>

<p><img src=""http://i.stack.imgur.com/E5gv0.png"" alt=""Time Series Plot""></p>
"
"0.233464802386759","0.264504605946974"," 58657","<p>I'm using a daily time series of sales data that contains about 2 years of daily data points. Based on some of the online-tutorials / examples I tried to identify the seasonality in the data. It seems that there is a weekly, monthly and probably a yearly periodicity / seasonality.</p>

<p>For example, there are paydays, particularly on 1st payday of the month effect that lasts for few days during the week. There are also some specific Holiday effects, clearly identifiable by noting the observations.</p>

<p>Equipped with some of these observations, I tried the following:</p>

<ol>
<li><p>ARIMA (with <code>Arima</code> and <code>auto.arima</code> from R-forecast package), using regressor (and other default values needed in the function).  The regressor I created is basically a matrix of 0/1 values:</p>

<ul>
<li>11 month (n-1) variables</li>
<li>12 holiday variables</li>
<li>Could not figure out the payday part...since it's little more complicated effect than I thought. The payday effect works differently, depending on the weekday of the 1st of month.</li>
</ul>

<p>I used 7 (i.e., weekly frequency) to model the time series. I tried the test - forecasting 7 days at a time. The results are reasonable: average accuracy for a forecast of 11 weeks comes to weekly avg RMSE to 5%.</p></li>
<li><p>TBATS model (from R-forecast package) - using multiple seasonality (7, 30.4375, 365.25) and obviously no regressor. The accuracy is surprisingly better than the ARIMA model at weekly avg RMSE 3.5% .</p>

<p>In this case, the model without ARMA errors perform slightly better. Now If I apply the coefficients for just the Holiday Effects from the ARIMA model described in #1, to the results of the TBATS model the weekly avg RMSE improves to 2.95%</p></li>
</ol>

<p>Now without having much background or knowledge on the underlying theories of these models, I'm in a dilemma whether this TBATS approach is even a valid one. Even though it's improving the RMSE significantly in the 11 weeks test, I'm wondering whether it can sustain this accuracy in the future. Or even if applying Holiday effects from ARIMA to the TBATS result is justifiable. Any thoughts from any / all the contributors will be highly appreciated. </p>

<p><a href=""https://s3.amazonaws.com/CKI-FILE-SHARE/TS+Test+Data.txt"">Link for Test Data</a></p>

<p>Note: Do ""Save Link As"", to download the file.</p>
"
"0.186771841909407","0.211603684757579"," 59058","<p>I'm trying to forecast a seasonal time series based on its historical values, and also two more time series (that are seasonal themselves.)  </p>

<p>I'm trying to use an <strong>auto.arima</strong>, and I'm going to input the other two time series (the exogeneous regressors) as a contatenated list of dummy variables, in auto.arima's <strong>xreg</strong> parameter.</p>

<p>I am having difficulty how to use the forecast function after this point.  I've written up the following code, but I don't understand what I should put in the <strong>xreg</strong> and <strong>newxreg</strong> parameters of the forecast function.</p>

<pre><code>tempfit&lt;-auto.arima(dnew, xreg=dExt)
plot(forecast(tempfit, xreg=dnew1,newxreg=dExt1))
</code></pre>

<p>Also, my data points for these three series were all values per day that had a seven day seasonality. In order to let auto.arima calculate the (p,q,d) for seasonality, I converted them to time series with a frequency of 7. Now, after forecasting is done, the plot shows one unit for every seven days.  How can I covert this back to one unit per day?</p>

<p>Further, do you happen to know how we can input a set of external regressors to an ETS model?</p>

<p>I would greatly appreciate your inputs!</p>

<p>Thank you.</p>

<p><strong>EDIT</strong>:</p>

<p>I just saw the following page from Dr. Hyndman:
<a href=""http://stats.stackexchange.com/questions/34493/time-series-modeling-with-dynamic-regressors-in-sas-vs-in-r"">Time series modeling with dynamic regressors in SAS vs. in R</a></p>

<p>Is it safe to assume that I don't need to enter a newxreg parameter for my forecast?</p>

<p>Also, I really want to know if it's statistically correct to use the two external regressors in xreg, but then also use a number of dummy variables in xreg that will represent the seasonality of these two variables.  </p>
"
"0.107832773203438","0.122169444356305"," 62237","<p>I am working on a data set. After using some model identification techniques, I came out with an ARIMA(0,2,1) model. </p>

<p>I used the <code>detectIO</code> function in the package <code>TSA</code> in R to detect an <em>innovative</em> outlier (IO) at the 48th observation of my original data set. </p>

<p>How do I incorporate this outlier into my model so I can use it for forecasting purposes? I don't want to use the ARIMAX model since I might not be able to make any predictions from that in R. Are there any other ways I could do this?  </p>

<p>Here are my values in order:</p>

<pre><code>VALUE &lt;- scan()
  4.6  4.5  4.4  4.5  4.4  4.6  4.7  4.6  4.7  4.7  4.7  5.0  5.0  4.9  5.1  5.0  5.4
  5.6  5.8  6.1  6.1  6.5  6.8  7.3  7.8  8.3  8.7  9.0  9.4  9.5  9.5  9.6  9.8 10.0
  9.9  9.9  9.8  9.8  9.9  9.9  9.6  9.4  9.5  9.5  9.5  9.5  9.8  9.3  9.1  9.0  8.9
  9.0  9.0  9.1  9.0  9.0  9.0  8.9  8.6  8.5  8.3  8.3  8.2  8.1  8.2  8.2  8.2  8.1
  7.8  7.9  7.8  7.8
</code></pre>

<p>That is actually my data. They are unemployment rates over a period of 6 years. There are 72 observations then . Each value is to at most one decimal place</p>
"
"0.0880450906325624","0.0997509336107633"," 63883","<p>Is there a method to find the right distance function in non-parametric regression?
I use some time series to learn forecasting. Series are nonlinear and non-gaussian.
I can get the right dimension and delay. I can find the right bandwidth with the hdrcde library.
I have no problem with kernel functions.<br>
My problem is with distances. I use Euclidian, Cosine and Correlation weighting functions.
These are the kernel which give good results, but one time, Euclidian is good, then after adding some data, one to 5-7 generally, Euclidian give nothing, even with very little change in statistics.
So, my question is if there is a method to choose the right distance. I would like to have advices on that point and on articles that will help solve this problem. What package in R could eventually help?</p>

<p>Thank you. </p>
"
"0.249029122545876","0.246870965550509"," 64711","<p>I have a time series I am trying to forecast, for which I have used the seasonal ARIMA(0,0,0)(0,1,0)[12] model (=fit2). It is different from what R suggested with auto.arima (R calculated ARIMA(0,1,1)(0,1,0)[12] would be a better fit, I named it fit1). However, in the last 12 months of my time series my model (fit2) seems to be a better fit when adjusted (it was chronically biased, I have added the residual mean and the new fit seems to sit more snugly around the original time series. Here is the example of the last 12 months and MAPE for 12 most recent months for both fits:</p>

<p><img src=""http://i.stack.imgur.com/kkUOb.png"" alt=""fit1, fit2 and original data""></p>

<p>The time series looks like this:</p>

<p><img src=""http://i.stack.imgur.com/twNkT.png"" alt=""original time series""></p>

<p>So far so good. I have performed residual analysis for both models, and here is the confusion. </p>

<p>The acf(resid(fit1)) looks great, very white-noisey:</p>

<p><img src=""http://i.stack.imgur.com/gyIv3.png"" alt=""acf of fit1""></p>

<p>However, Ljung-Box test doesn't look good for , for instance, 20 lags: </p>

<pre><code>    Box.test(resid(fit1),type=""Ljung"",lag=20,fitdf=1)
</code></pre>

<p>I get the following results:</p>

<pre><code>    X-squared = 26.8511, df = 19, p-value = 0.1082
</code></pre>

<p>To my understanding, this is the confirmation that the residuals are not independent ( p-value is too big to stay with the Independence Hypothesis). </p>

<p>However, for lag 1 everything is great:</p>

<pre><code>    Box.test(resid(fit1),type=""Ljung"",lag=1,fitdf=1)
</code></pre>

<p>gives me the result: </p>

<pre><code>    X-squared = 0.3512, df = 0, p-value &lt; 2.2e-16
</code></pre>

<p>Either I am not understanding the test, or it is slightly contradicting to what I see on the acf plot. The autocorrelation is laughably low. </p>

<p>Then I checked fit2. The autocorrelation function looks like this:</p>

<p><img src=""http://i.stack.imgur.com/JZ7Sc.png"" alt=""acf fit2""></p>

<p>Despite such obvious autocorrelation at several first lags, the Ljung-Box test gave me much better results at 20 lags, than fit1:</p>

<pre><code>    Box.test(resid(fit2),type=""Ljung"",lag=20,fitdf=0)
</code></pre>

<p>results in :</p>

<pre><code>    X-squared = 147.4062, df = 20, p-value &lt; 2.2e-16
</code></pre>

<p>whereas just checking autocorrelation at lag1, also gives me the confirmation of the null-hypothesis! </p>

<pre><code>    Box.test(resid(arima2.fit),type=""Ljung"",lag=1,fitdf=0)
    X-squared = 30.8958, df = 1, p-value = 2.723e-08 
</code></pre>

<p>Am I understanding the test correctly? The p-value should be preferrably smaller than 0.05 in order to confirm the null hypothesis of residuals independence. Which fit is better to use for forecasting, fit1 or fit2? </p>

<p>Additional info: residuals of fit1 display normal distribution, those of fit2 do not.  </p>
"
"0.124514561272938","0.14106912317172"," 66927","<p>I need to take the output parameters from an ARIMA model fitted in R from the following set (1,0,1), (0,1,0), (1,1,0), (0,1,1), (1,1,1) of models and implement the prediction function in C. I DO NOT HAVE THE OPTION of calling predict or any other R package for that step. </p>

<p>Obviously, I can eventually track down all the source code in predict and figure it out. But I was hoping there is somewhere that will walk me through a simple example of how to map the various output parameters of Arima() with X, Y, a, b, E, t (no upper and lower case thetas and B^t's) since every paper loves to include those already. </p>

<p>I think this request is slightly duplicative except in previous versions the question was retired without an answer or a link.</p>

<p>UPDATE: So, first, I HIGHLY second all recommendations for <a href=""http://otexts.com/fpp/"" rel=""nofollow"">Forecasting: Principles and Practice by Hyndman&amp;Athanasopoulos</a>. </p>

<p>I think what I've been missing is that ""d"" isn't a model parameter -- it changes what is being modeled. So while I'm not all the way to where I want to be, I'm starting to be able to write predictive equations based on R output. I will update with my eventual findings if nobody else posts something better. </p>
"
"0.124514561272938","0.0352672807929299"," 70275","<p>When so many warnings, what does it in fact means? Is there a problem with the validity of the stochastic forecasting models?</p>

<p>I am doing a stochastic forecasting of a small population with approx. 50.000 people. I am using one year age intervals 0-90+. Because so small dataset, I am borrowing the mortality rates from a population which is similar in life expectancy. Fertility rates with 0 are replaced with 1/10000. Net migrations are calculated with the function â€netmigrationâ€ in the â€demographyâ€ package.</p>

<p>Then I use the function â€pop.simâ€ for simulating say 1.000 sample paths of population 40 years ahead.</p>

<p>When doing so with set.seed(505) and N=1000, I don't get any warning. But nearly all other values in set.seed and or N=1000 give me the warning messages: NAs produced.</p>

<p>For example when my code is</p>

<pre><code>set.seed(300)
sim300 &lt;- pop.sim(mort=mort.fcast, fert=fert.fcast, mig=mig.fcast, 
                  firstyearpop=mort.fo,N=300, mfratio=mfratio,
                  bootstrap=FALSE)
</code></pre>

<p>The first ten messages looks like this:</p>

<pre><code>Warning messages:

1: In rpois(rep(1, length(fert$age)), lambda) : NAs produced  
    2: In rbinom(1, B, mfratio/(1 + mfratio)) : NAs produced  
    3: In rpois(1, Ef0 * mort.sim$female[1, j, i]) : NAs produced  
4: In rpois(1, Em0 * mort.sim$male[1, j, i]) : NAs produced  
    5: In rpois(rep(1, p), Ef * mort.sim$female[, j, i]) : NAs produced  
6: In rpois(rep(1, p), Em * mort.sim$male[, j, i]) : NAs produced  
    7: In rpois(1, Ef0 * mort.sim$female[1, j, i]) : NAs produced  
8: In rpois(1, Em0 * mort.sim$male[1, j, i]) : NAs produced
    9: In rpois(rep(1, p), Ef * mort.sim$female[, j, i]) : NAs produced
10: In rpois(rep(1, p), Em * mort.sim$male[, j, i]) : NAs produced  
</code></pre>

<p>How shall I interpret these messages?
Does this indicate that something is wrong in the data, models, forecasting or simulation procedures?</p>

<p>My first guess is that, in message nr. 1, a negative value is assigned to â€lambdaâ€?, if so, is there a way to prevent that?</p>

<p>The most important question is whether these NAs produced is an indicator of the validity of the models and or the forecast of the population?</p>

<p>Is anyone out there who can say something about what is going on here?</p>
"
"0.062257280636469","0.0705345615858598"," 85592","<p>I'm having issues forecasting a model of the following form.</p>

<pre><code>y1 &lt;- tslm(data_ts~ season+t+I(t^2)+I(t^3)+0)
</code></pre>

<p>It fits my data very well, but I run into a problem when attempting to do this:</p>

<pre><code>forecast(y1,h=72)
</code></pre>

<p>This is the error that R gives me.</p>

<pre><code>""Error in model.frame.default(Terms, newdata, na.action = na.action,  
 xlev = object$xlevels) : 
   variable lengths differ (found for 't')
In addition: Warning message:
'newdata' had 72 rows but variables found have 1000 rows"" 
</code></pre>

<p>As far as I can tell, this has something do with using <code>tslm</code> and having the cubic function in it. If I just use <code>tslm(data_ds~season+trend)</code> everything works out fine, but I specifically need the model mentioned earlier. How can I forecast my model?</p>
"
"0.241121411085206","0.254967236863805"," 88145","<p>I am using the ar() function to fit an AR model to some data, and this object will return the in sample residuals. I also know the syntax for how to get the corresponding predicted values, but I want to compute these predicted values manually (just to check my own understanding) for a simple AR(1) example. </p>

<p>My problem is that my manually computed residuals (based on my manually computed predictions) do not match the in sample residuals stored in the ar object (well the 1st residual does match, but not the rest).</p>

<p>From the documentation, I see that</p>

<pre><code>x[t]  = m + a[1]*(x[t-1] - m) 
</code></pre>

<p>where m is the sample mean of the series. Here is an example of what I am doing manually.</p>

<pre><code># Create some true AR(1) data
set.seed(123)
x = w = rnorm(30) + 3 ; for (t in 2:30) x[t] = .60*x[t-1] + w[t]
# Fit an ar model
x.model = ar(x) # coefficient is .49, mean value of x is 6.98
# Manually create predictions
x.MyPred = rep(0,30) ; x.MyPred[1] = x[1]
for (t in 2:30) x.MyPred[t] = 6.984234 + .4988327*(x.MyPred[t-1] - 6.984234)
MyResid = x - x.MyPred
cbind(MyResid, x.model$res) # does not match
</code></pre>

<p>And interestingly, the first residual (at observation 2) does match, but the rest do not. Thanks in advance.</p>

<p><strong>Update</strong></p>

<p>The answer given below is basically highlighting the difference between so called static forecasting and dynamic forecasting, here are a few more details. </p>

<p>The two possible choices to make the fitted values are </p>

<pre><code># Method 1:
for (t in 2:30) x.MyPred[t] = 6.984234 + .4988327*(x.MyPred[t-1] - 6.984234)

# Method 2:
for (t in 2:30) x.MyPred[t] = 6.984234 + .4988327*(x[t-1] - 6.984234)
</code></pre>

<p>Method 1 is taking the forecasted value from the previous step and using this to plug in for the lagged values on the RHS.
This could feasibly be used to create out of sample predictions forever. Also, depending on what time point you start to make predictions, the resulting predictions for a given time period can be different.</p>

<p>Method 2 is taking the actual known historical values to plug in for the lagged values on the RHS. It will never be able to forecast more than 1 step out of sample. Also, the forecast values will always be the same no matter where you start.</p>

<p>Both of these will give the same 1 step ahead forecast value. Method 2 is what produces the residuals returned in the ar() object.</p>

<p>The Eviews software documentation has a good discussion of this. This R documentation was problematic in not specifying which of the x values are fitted values versus known historical values. A better expression to show in the documentation would be</p>

<pre><code>\hat{x}[t]  = m + a[1]*(x[t-1] - m)
</code></pre>

<p>and, dare I say, possibly a few sentences on this very topic. But good documentation is hardly something to expect in R.</p>
"
"0.241121411085206","0.236755291373533"," 88841","<p>I am trying to do time series modeling and forecasting using R based on weekly data like below -</p>

<pre><code>biz week     Amount        Count
2006-12-27   973710.7     816570
2007-01-03  4503493.2    3223259
2007-01-10  2593355.9    1659136
2007-01-17  2897670.9    2127792
2007-01-24  3590427.5    2919482
2007-01-31  3761025.7    2981363
2007-02-07  3550213.1    2773988
2007-02-14  3978005.1    3219907
2007-02-21  4020536.0    3027837
2007-02-28  4038007.9    3191570
2007-03-07  3504142.2    2816720
2007-03-14  3427323.1    2703761
...
2014-02-26  99999999.9   1234567
</code></pre>

<p>Regarding my data, as seen above, each week is labeled by first day for the week (my weeks start on Wednesdays and end on Tuesdays).  When I construct my <code>ts</code> object, I tried: </p>

<pre><code>ts &lt;- ts(df, frequency=52, start=c(2007,1))
</code></pre>

<p>The problems I have are: </p>

<ol>
<li>Some years may have 53 weeks, so <code>frequency=52</code> will not work for those years.</li>
<li><p>My starting week / date is <code>2006-12-27</code>, how should I set the start parameter? Should I use: <code>start=c(2006,52) or start=c(2007,1)</code>, since week of <code>2006-12-27</code> really crosses the year boundary?</p>

<p>Also, for modeling, is it better to have complete year worth of data (say for 2007 my start year if I only have partial year worth of data, is it better I should not use 2007, instead to start with 2008. What about 2014 since it is not complete year yet, shall I use what I have for model or not? Either way, I still have issue of whether or not to include those weeks in the year boundary like 2006-12-27, shall I include it as week 1 for 2007 or last week of 2006? </p></li>
<li><p>When I use <code>ts &lt;- ts(df, frequency=52, start=c(2007,1))</code> and then print it, I got results shown below, so instead of <code>2007.01, 2007.02, 2007.52, ...</code>, I got <code>2007.000, 2007.019, ...</code> which it gets from <code>1/52=0.019</code>, which is mathematically correct but not really easy to interpret. Is there a way to label it as the date itself just like data frame or at least <code>2007 wk1, 2007 wk2, ...</code>?</p>

<p>=========</p>

<pre><code>Time Series:
Start = c(2007, 1) 
End = c(2014, 11) 
Frequency = 52 
          Amount        Count
2007.000   645575.4     493717
2007.019  2185193.2    1659577
    2007.038  1016711.8     860777
2007.058  1894056.4    1450101
2007.077  2317517.6    1757219
2007.096  2522955.8    1794512
2007.115  2266107.3    1723002 
</code></pre></li>
<li><p>My goal is to model this weekly data, then try to decompose it to see seasonal component, it seems like I have to use <code>ts()</code> function to convert to <code>ts</code> object then I can use <code>decompose()</code> function, I tried <code>xts()</code> function, and I got error stating ""time series has no or less than 2 periods"" I guess reason is because <code>xts()</code> won't let me specify the frequency? </p>

<pre><code>xts &lt;- xts(df,order.by=businessWeekDate)
</code></pre></li>
<li><p>I looked for the answer in this forum and other places as well, most of the examples are monthly, there are some weekly time series question, but none of the answers are straight forward.</p></li>
</ol>
"
"0.152498570332605","0.143978070930227"," 91675","<p>I have been looking for a function that can make recursive window out-of-sample forecasts, but seems there is none. So I'm thinking about about making a function that can be used for recursive window forecasting in an ARIMA model. However I know little about programming, so I'm seeking for help.</p>

<p>What I want to do is use the function <code>forecast.Arima</code> (<strong>forecast</strong> package) to predict future values in a expanding window. Suppose 20 years is the initial window, and I expand the window by 1 year on each iteration until it is of size  30 years. More specifically, use 20 years data to predict one value, use 21 years data to predict the next value, etc.</p>
"
"0.139211511597426","0.157720074469128","100363","<p>I have a question regarding the use of the dlm CRAN package for forecasting values of a seasonal time series.</p>

<p>I've built a dlm model combining a stochastic local level model with a stochastic trigonometric (Fourier representation) seasonal component of period 96 (measurements every 15 mins with a daily cycle).</p>

<p>I used dlmMLE to estimate the parameters for my data and filtered and smoothed the series which all seems to be working fine.</p>

<p>However, when I try to use the dlmForecast function to predict out-of-sample observations, the predictions stay constant. The value of all ""predictions"" are equal to the sum of the filtered level and filtered seasonal components for the final observation in the series.</p>

<p>I have used dlmForecast with several other models including a model with a seasonal factor component but never before with a trigonometric seasonal component.</p>

<p>I notice in the documentation for dlmForecast it says ""Currently, only constant models are allowed"" so I wonder if this applies to trigonometric seasonal models.</p>
"
"0.0880450906325624","0.0997509336107633","103737","<p>As per my job requirement I have to do forecasting using only Holt Winter technique in R.I have weekly data for 2 and half years &amp; I have to predict weekly.I'm planning to build time series with frequency 52. I'm seeing that HoltWinters() function in R can do only Additive trend-Additive Seasonal (A,A) &amp; Additive trend -Multiplicative Seasonal Model (A,M). I want to try other models like (M,A),(M,M).
I saw these models can be done by ets() but the ets() has maximum frequency restriction of 24.</p>

<p>Can you please suggest me some method in R where I can use these models with 52 weeks of frequency?</p>

<p>Thanks in advance</p>
"
"0.124514561272938","0.14106912317172","104304","<p>I'm trying to forecast hourly data for 30 days for a process.</p>

<p>I have used the following code:</p>

<pre><code>#The packages required for projection are loaded
library(""forecast"")
library(""zoo"")
</code></pre>

<h3>Data Preparation steps</h3>

<p>There is an assumption that we have all the data for all the 24 hours of the month of May</p>

<pre><code>time_index &lt;- seq(from = as.POSIXct(""2014-05-01 07:00""),
                  to = as.POSIXct(""2014-05-31 18:00""), by = ""hour"")

value &lt;- round(runif(n = length(time_index),100,500))
</code></pre>

<p>Using <code>zoo</code> function , we merge data with the date and hour to create an extensible time series object</p>

<pre><code>eventdata &lt;- zoo(value, order.by = time_index)
</code></pre>

<p>As forecast package requires all the objects to be time series objects, the below command is used </p>

<pre><code>eventdata &lt;- ts(value, order.by = time_index)
</code></pre>

<p>For forecasting the values for the next 30 days, the below command is used</p>

<pre><code>z&lt;-hw(t,h=30)
plot(z)
</code></pre>

<p>I feel the output of this code, is not working fine.
<img src=""http://i.stack.imgur.com/NdZRM.jpg"" alt=""enter image description here"">
The forecasted line looks wrong and the dates are not getting correcting projected on the chart.</p>

<p>I'm not sure the fault lies in the data preparation or the output is as expected.</p>
"
"0.0880450906325624","0.0997509336107633","107730","<p>When using the combinef function from Rob Hyndman's very useful <a href=""http://cran.r-project.org/web/packages/hts/index.html"" rel=""nofollow"">hts package</a> for forecasting hierarchical and grouped time series, there does not seem to be a way to constrain the optimally combined forecasts to be positive- the starting forecasts can be positive, but can go negative through the reconciliation process.</p>

<p>The forecast.gts and forecast.hts functions have an argument to keep forecasts positive, but this does not seem to be an option when using combinef by itself with forecasts obtained by other methods.</p>

<p>Am I correct in this understanding, and if so is there a decent workaround? </p>
"
"0.233464802386759","0.246870965550509","108495","<p>I am trying to calculate the average of hourly data of three sensors but the hourly timestamps of all three sensors are different. How is it possible to measure the average of hourly data of all three sensors at a regular interval of hours say- 01:00,02:00,03:00 and so on.</p>

<p>And also is it possible to start an extensible time series with a specific time which is before the first timestamp of the given dataset so that the extra timestamps have NA values and i can use na.rm=TRUE while calculating mean.:</p>

<pre><code>  library(xts)
 library(forecast)
 data1$V1&lt;-as.POSIXct(paste(data1$V1, data1$V2), format=""%Y-%m-%d %H:%M:%S"") 
     sensid&lt;-as.factor(data1$V4)

 #For Temp

 for(i in 1:length(levels(sensid)))
  {
   l[[i]]=data.frame(data1$V6[data1$V4==i])
   l[[i]]&lt;-xts(l[[i]],order.by=data1$V1[data1$V4==i])
   if(length(which (is.na(l[[i]])))&lt;length(l[[i]]))
     {      
      wAvgt[[i]]&lt;-apply.weekly(l[[i]], function(x) sapply(x,mean,na.rm=TRUE)) #Weekly   Average of Temp
      dAvgt[[i]]&lt;- apply.daily(l[[i]], function(x) sapply(x,mean,na.rm=TRUE)) #Daily Average of Temp
      hrAvgt[[i]] &lt;- period.apply(l[[i]], endpoints(l[[i]], on = ""hours"", 1), function(x) sapply(x,mean,na.rm=TRUE)) #Hourly Average of Temp

     #For forecasting future values
     aritw[[i]]&lt;-auto.arima(wAvgt[[i]]) #Weekly Forecast ARIMA model using ARIMA 
     foretw[[i]]&lt;-forecast.Arima(aritw[[i]],h=5) #Forecast of Weekly Average of Temp

     arith[[i]]&lt;-auto.arima(hrAvgt[[i]]) #Hourly Forecast ARIMA model using ARIMA 
     foreth[[i]]&lt;-forecast.Arima(arith[[i]],h=50)#Forecast of Hourly Average of Temp

     aritd[[i]]&lt;-auto.arima(dAvgt[[i]]) #Daily Forecast ARIMA model using ARIMA 
     foretd[[i]]&lt;-forecast.Arima(aritd[[i]],h=10)#Forecast of Daily Average of Temp
    }
else
    {
     next
    }
 }
</code></pre>

<p>I have 27 sensors data. so i m trying to caclulate hourly,weekly and daily average of each sensors but each sensor provides 4 critical parameters like - blood pressure, temperature, Pulse and WBC. The above loop provided is only for temperature of 27 sensors. I have a bed connected to some 3 sensors and each sensor has 4 critical parameters to measure. I m concerned only with temperature for now. I need to calculate hourly average of temp for that bed only. So, initially i measure hourly average of any 3 given sensors each and then i intend to take the hourly average between these 3 sensors.</p>

<p>The problem i m facing here is each hourly average has different number of rows and different timestamps. How to normalise all the timestamps?</p>

<p>If the starting timestamp of sensor 1 is 2004-02-28 00:59:16, sensor 2 is 2004-02-28 01:08:23, sensor 3 is 2004-02-28 01:19:34, it is not possible to normalise them by any methods i know of yet.</p>

<p>Please help me find the hourly average of three sensors.</p>
"
"0.278423023194852","0.28389613404443","109835","<p>While working on a big data set made of 10-minutes-points of information - i.e. <code>144</code> points per day, <code>1008</code> per week and <code>52560</code> per year - I encountered a few problem in R. The information concerns electricity load on a source substation during the year.</p>

<h3>Multiple seasonality :</h3>

<p>The data set clearly shows multiple seasonalities, which are daily, weekly and yearly. From <a href=""http://stats.stackexchange.com/questions/47729/two-seasonal-periods-in-arima-using-r"">there</a> I understood that R doesn't handle multiple seasonality within the ARIMA modeling functions.  I would really like to work with ARIMA models though, because my previous work is based on ARIMA models and I know approximatively how to translate a model into an equation.  </p>

<h3>Long seasonality :</h3>

<p>Each of the seasonalities is of high value, with the shortest one being the daily seasonality at 144. Unfortunately from the SARIMA general equation which is<br>
$\phi(B)\Phi(B^s)W_t = \theta(B)\Theta(B^s)Z_t$<br>
I guessed that the maximum lag for a given model <code>SARIMA(p,d,q)(P,D,Q)144</code> is<br>
$max((p+P*144), (q+Q*144))$</p>

<p>I would really like to try and fit models with values of P and/or Q greater than 1, but R doesn't allow me since the <code>maximum supported lag = 350</code>. To do so I found <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">this link</a> which is really interesting and led to new functions in the forecast package by M. Hyndman, called <code>fourier</code> and <code>fourierf</code> which you can find <a href=""http://www.inside-r.org/packages/cran/forecast/docs/fourier"" rel=""nofollow"">here</a>. But since I am not a specialist in forecasting nor in statistics, I have some difficulties understanding how I can make this work.  </p>

<hr>

<p>The thing is I thing this whole fourier regressors package could help me a lot. From what I understood I could use it to simulate the long-seasonality of my data set, maybe use it to simulate multiple seasonality, and even more it could allow me to introduce exogenous variables - which are the <code>temperature</code> and (<code>public holiday + sundays</code>).<br>
I also tried doing some regression following <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">this example</a> but I couldn't make it work because :</p>

<pre><code>Error in forecast.Arima(bestfit, xreg = fourierf(gas, K = 12, h = 1008)) : 
Number of regressors does not match fitted model
</code></pre>

<p>I really hope somebody can help me get a better understanding of these functions. Thanks.</p>

<p><strong>Edit :</strong> So I tried my best with the fourier example given <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a> but couldn't figure out how it handles the fitting. Here is the code (I copy-pasted M. Hyndman one and adapted to my data set - unsuccessfully) :</p>

<pre><code>n &lt;- 50000
m &lt;- 144
y &lt;- read.table(""auch.txt"", skip=1)
fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}

library(forecast)
fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008)))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m), fourier(n+1:(14*m),4,1008))))
</code></pre>

<p>So I wanted to ""force"" the model to be a <code>SARIMA(2,1,5)(1,2,8)[144]</code> but when I type <code>arimod</code>this is the result of the Arima fitting :</p>

<pre><code>&gt; fit  
Series: y[1:n, 1] , 
ARIMA(2,1,5)                  

sigma^2 estimated as 696895:  log likelihood=-407290.2  
AIC=814628.3   AICc=814628.3   BIC=814840
</code></pre>

<p>It doesn't even take into consideration the seasonal part of the model, and I don't know much about the range the AIC values can take, but it seems way too high to be a good fitting model right there. I think it all comes down to my misunderstanding of the use of Fourier terms as regressors, but I can't figure out why.</p>

<p><strong>Edit 2 :</strong> Also I can't seem to be able to add another exogenous variable to the Arima function. I need to use <code>temperature</code> - probably as a lead - to fit the <code>SARIMAX</code> model, but as soon as I write this :</p>

<pre><code>fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008), tmp[1:n]))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m),fourier(n+1:(14*m),4,1008), tmp[n+1:(14*m)])))
</code></pre>

<p>Nothing is plotted besides the initial data set. There is no forecast while without <code>tmp</code> as an <code>xreg</code> I still get some results.</p>
"
"0.176090181265125","0.199501867221527","115506","<p>Forecasting airline passengers seasonal time series using auto arima</p>

<p>Hi, I am trying to model some airline data in an attempt to provide an accurate monthly forecast for June-December this year using monthly data from January 2003 onwards.  The data is taken from: <a href=""http://www.transtats.bts.gov/Data_Elements.aspx?Data=1"" rel=""nofollow"">http://www.transtats.bts.gov/Data_Elements.aspx?Data=1</a></p>

<p>Here is the time series plot and ACF</p>

<p><a href=""http://imgur.com/EGh40pR"" rel=""nofollow""><img src=""http://i.imgur.com/EGh40pR.jpg"" title=""Hosted by imgur.com""/></a> </p>

<p><a href=""http://imgur.com/BJy78dn"" rel=""nofollow""><img src=""http://i.imgur.com/BJy78dn.jpg"" title=""Hosted by imgur.com""/></a></p>

<p>I have used auto.arima to develop two models and checked that they correspond to the autocorrelation functions.  Basically I am having trouble deciding whether to use:</p>

<ol>
<li>The following seasonal ARIMA model</li>
</ol>

<p><a href=""http://imgur.com/0k2Q8I4"" rel=""nofollow""><img src=""http://i.imgur.com/0k2Q8I4.jpg"" title=""Hosted by imgur.com""/></a></p>

<ol start=""2"">
<li><p>The following non-seasonal ARIMA model of $N_t$ after I first decomposed the model into a trend, seasonal component and random component $X_t = T_t +S_t +N_t $ using a 12-point moving average (basically did the same thing as the <code>decompose()</code> function manually)</p>

<p><a href=""http://imgur.com/r4TkpxX"" rel=""nofollow""><img src=""http://i.imgur.com/r4TkpxX.jpg"" title=""Hosted by imgur.com""/></a></p></li>
</ol>

<p>I have analysed the important properties of both models such as ensuring residuals are close to a white noise process and so on but am unsure which of the above 2 models is most suitable for forecasting purposes and why?</p>

<p>Also I am unsure how to compute forecast for the trend component vector if I use the classical decomposition model $X_t = T_t + S_t +N_t$.  Is it even possible to create forecasts using this type of model?</p>

<p>Edit:
Here is the output of <code>dput(IAP)</code> (the raw data without trend or seasonal component removed)</p>

<blockquote>
  <p>dput(IAP)
  structure(c(9726436L, 8283372L, 9538653L, 8309305L, 8801873L, 
  10347900L, 11705206L, 11799672L, 9454647L, 9608358L, 9481886L, 
  10512547L, 10252443L, 9310317L, 10976440L, 10802022L, 10971254L, 
  12159514L, 13502913L, 13203566L, 10570682L, 10772177L, 10174320L, 
  11244427L, 11387275L, 9945067L, 12479643L, 11521174L, 12164600L, 
  13140061L, 14421209L, 13703334L, 11325800L, 11107586L, 10580099L, 
  11812574L, 11724098L, 10167275L, 12707241L, 12619137L, 12610793L, 
  13690835L, 14912621L, 14171796L, 12010922L, 11517228L, 11222687L, 
  12385958L, 12072442L, 10590281L, 13246293L, 12795517L, 12978086L, 
  14170877L, 15470687L, 15120200L, 12321953L, 12381689L, 12004268L, 
  13098697L, 12767516L, 11648482L, 14194753L, 12961165L, 13602014L, 
  14413771L, 15449821L, 15327739L, 11731364L, 11921490L, 11256163L, 
  12463351L, 12075267L, 10412676L, 12508793L, 12629805L, 11806548L, 
  13199636L, 14953615L, 14844821L, 11659775L, 11905529L, 11093714L, 
  12659154L, 12393439L, 10694165L, 13279320L, 12398700L, 13380664L, 
  14406776L, 16026852L, 15317926L, 12599149L, 12874707L, 11651314L, 
  12915663L, 12668763L, 10944610L, 13473705L, 13537152L, 13935132L, 
  14814672L, 16623674L, 15753387L, 13220884L, 13185627L, 12144742L, 
  13546071L, 13206682L, 11732944L, 14387677L, 13995377L, 14291285L, 
  15582335L, 16969590L, 16621336L, 13791714L, 13397785L, 12762536L, 
  14096567L, 13766673L, 12023339L, 15177069L, 14278932L, 15306328L, 
  16232176L, 17645538L, 17517022L, 14239561L, 14209627L, 13133257L, 
  15083929L, 14589637L, 12385546L, 15486317L, 14857685L, 15615732L
  ), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>

<p>Here is the output of <code>dput(IAP.res)</code> (the random component from the decomposition)</p>

<blockquote>
  <p>dput(IAP.res)
  structure(c(NA, NA, NA, NA, NA, NA, -669127.347569446, -168943.285069446, 
  225871.456597222, 271337.106597223, 711896.11076389, 284583.435763889, 
  165401.360763887, 622993.194097221, -268299.21423611, -9406.73506944434, 
  -233904.910069446, -147124.755902779, -260973.055902776, -163628.243402778, 
  -43056.7100694457, 121365.814930555, 205106.485763889, -107464.272569445, 
  247575.569097221, 279399.444097225, 309270.160763888, -166333.068402778, 
  129823.798263889, 22571.1190972265, -113455.59756944, -384199.160069444, 
  62061.8315972222, -155858.226736111, 13600.0274305546, -87564.1475694429, 
  71845.7357638887, 8145.86076388881, 47627.494097226, 442212.72326389, 
  73639.5065972234, 60882.5774305568, -135204.389236112, -437744.576736112, 
  203832.581597222, -264145.435069444, 179945.61076389, 15812.1024305553, 
  -49648.0975694434, -61460.8059027772, 89656.3690972241, 118205.931597224, 
  -84196.4517361106, 4197.78576389072, -134118.722569442, -87234.4517361117, 
  -126555.418402776, -57714.9350694417, 293250.152430556, 59462.6857638892, 
  10340.8190972245, 416646.652430557, 526459.702430556, -135041.068402776, 
  239767.631597222, 67034.9940972247, -221066.180902774, 207611.839930556, 
  -424486.00173611, -94779.3517361115, 89796.4857638886, 130285.644097223, 
  104776.152430555, 16099.8607638888, -317097.047569448, 335867.264930556, 
  -796342.285069446, -446777.464236111, -93681.7225694442, 242962.798263888, 
  -143380.293402778, 135423.439930556, 28934.7357638923, 186390.185763891, 
  116969.777430558, -113617.264236109, -39733.9225694438, -471572.526736109, 
  130389.423263891, 80446.7857638926, 298895.444097222, 38486.7982638846, 
  143712.123263886, 419260.898263889, -113385.347569445, -181233.730902779, 
  -178686.680902779, -412733.597569445, -380106.797569444, 172783.973263888, 
  220863.173263891, 11443.2440972247, 392297.319097224, -62825.8267361117, 
  176278.664930556, 139372.439930556, -174159.88923611, -111755.439236109, 
  -206233.264236111, -197431.097569445, -55065.5892361099, 48314.3065972236, 
  -6745.32673610683, 193492.494097225, 155009.569097224, 241747.214930556, 
  209670.99826389, -173438.47673611, -101510.63923611, -128948.689236113, 
  -222773.597569443, -498474.472569441, 146856.619097224, -275463.026736109, 
  386273.214930557, 213400.994097223, 171865.11076389, 464391.381597217, 
  1489.99826388643, -9918.39340277936, -362009.847569447, NA, NA, 
  NA, NA, NA, NA), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>
"
"0.176090181265125","0.199501867221527","120806","<p>I'm using R(3.1.1), and ARIMA models for forecasting. 
I would like to know <strong>what should be the ""frequency"" parameter, which is assigned in the <code>ts()</code> function</strong>, if im using time series data which is:</p>

<ol>
<li>separated by <strong>minutes</strong> and is spread over 180 days (1440 minutes/day) </li>
<li>separated by <strong>seconds</strong> and is spread over 180 days (86,400 seconds/day).</li>
</ol>

<p>If I recall right the definition, a ""frequency"" in ts in R, is the number of observations per ""season"". </p>

<h2>Question part 1:</h2>

<p>then, what is the ""season"" in my case?</p>

<p>If the season is ""day"", then is the ""frequency"" for minutes = 1440 and 86,400 for seconds?</p>

<h2>Question part 2:</h2>

<p><strong>Could the ""frequency""  also depend on what I am trying to achieve/forecast?</strong>
for example, in my case, I'd like to have a very short-term forecast. 
One-step ahead of 10minutes each time. 
<strong>Would it then be possible to consider the season as an hour instead of a day?</strong>
In that case frequency= 60 for minutes, frequency = 3600 for seconds?</p>

<p>I've tried for example to use frequency = 60 for the minute data and got better results compared to frequency = 1440 (used <code>fourier</code> see link below by Hyndman)
<a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"">http://robjhyndman.com/hyndsight/forecasting-weekly-data/</a> </p>

<p>(The comparison was made by using MAPE for the measure of forecast accuracy)</p>

<p><strong>In case the results are complete arbitrary, and the frequency cannot be changed. 
What would be actually the interpretation of using freq = 60 on my data?</strong> </p>

<p>I also think it's worth mentioning that my data contains seasonality at every hour and every two hours (by observing the raw data and the Autocorrelation function)</p>

<p>Thanks!</p>
"
"0.317669043601133","0.306584921585469","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.299535481120662","0.285062036831379","123576","<p>I am trying to test the effect on the heat flux between indoors and outdoors before and after removing insulation.</p>

<p>Briefly, I have 26 sensors on a wall, measuring heat flow between indoors and outdoors over a number of days. The wall was part of a real world experimental setup so that the insulation on the wall was removed halfway through the experiment. Â What I care about is to have a measure of the effect of the removal of the insulation (I am not interested in any form of forecasting). Â I am exploring the use of a SARIMA/ARIMAX models with one regressor because, aside from the removal of the insulation, the heat flow between indoors and outdoors was affected by daily cyclical and random environmental effects (heating on or off, daily temperature changes, wind, etc).  Here I will present that data and analysis of one sensor.  My data has been collected hourly, and I have transformed the variable â€˜insulatedâ€™ â€˜not insulatedâ€™ as a factor of 0s and 1s as indicator.</p>

<pre><code>heat.flux = c(8.677048,6.558642,5.920314,5.583614,5.373176,5.253928,4.938272,7.358305,9.743266,10.46577,11.06201,10.90067,11.49691,13.15236,12.10017,10.60606,10.45875,10.03788,9.588945,9.287318,8.578844,8.024691,10.26936,11.8757,10.20623,8.634961,8.305275,8.101852,8.12991,7.947531,7.814254,10.40264,13.08221,14.3729,14.94809,15.08838,15.20763,15.75477,14.57632,12.79461,11.97391,10.97082,10.33249,9.701178,9.715208,9.083895,10.63412,12.07912,9.736251,7.638889,6.453423,5.983446,5.499439,5.099607,4.70679,6.972503,9.259259,9.981762,10.24832,10.17116,10.27637,10.27637,9.546857,7.568743,7.168911,6.867284,6.705948,6.916386,8.319304,8.424523,11.41274,13.52413,11.70034,9.532828,8.957632,9.07688,9.694164,9.301347,9.048822,12.28255,14.95511,15.22868,15.24972,15.12346,15.08838,15.17256,13.68547,12.18434,12.1633,12.13524,11.81257,11.58109,11.44781,11.27946,13.87486,15.92312,14.07828,11.90376,10.46577,9.518799,8.978676,8.803311,8.684063,11.65123,14.39394,15.69865,16.61756,16.828,16.83502,16.16863,14.23962,12.19837,12.09315,11.5881,11.20932,10.50786,10.59203,10.64815,13.51712,15.71268,13.92396,12.10718,12.2615,11.65123,11.05499,10.31846,9.834456,12.9349,15.41807,15.78283,15.8179,16.11953,15.95118,15.63552,13.1243,11.22334,10.21324,8.705107,7.526655,6.15881,5.30303,5.597643,8.599888,11.17424,9.631033,8.038721,7.638889,7.203984,7.161897,6.76908,6.888328,9.518799,12.40881,13.21549,14.28872,14.43603,14.8078,14.81481,13.60129,12.59119,11.86167,11.91779,11.73541,12.04405,11.51796,11.74242,13.7486,15.85999,14.84989,12.63328,10.68322,9.343434,8.592873,8.333333,8.445567,10.97783,13.82576,15.12346,16.58249,17.61364,18.30808,19.10774,17.97138,16.62458,15.867,16.07744,15.63552,16.0073,15.42508,15.01122,17.10157,18.94641,22.44669,18.94641,16.01431,14.55527,13.88889,12.77357,11.66526,12.46493,15.41807,16.75786,17.27694,17.03143,16.84905,16.828,16.02834,16.35802,16.04237,15.03928,14.00112,14.1344,13.86785,13.99411,15.30584,18.20286,19.49355,16.16162,14.05022,12.05107,12.27553,13.01207,12.5491,13.72054,16.91218,18.62374,18.79209,20.80527,19.50758,20.18799,20.63692,18.49747,17.25589,17.38215,18.40629,18.60269,19.12177,18.66582,21.09989,24.45286,26.71156,23.54798,20.01964,17.98541,14.83586,14.31678,15.15152,15.30584,17.95735,19.71801,20.30724,20.19501,20.2862,20.1459,20.10382,18.20988,16.54742,15.22868,13.96605,12.71044,11.61616,10.71829,12.12121,14.77273,14.04321,12.44388,10.94978,10.2413,9.708193,9.638047,9.322391,11.27245,14.24663,14.77273,14.75168,14.92705,15.47419,15.48822,14.73765,13.68547,12.65432,12.35269,12.34568,12.32464,12.7385,12.84371,14.16947,17.34007,17.09456,15.0954,13.40488,11.70735,10.8165,10.64815,12.01599,13.55219,16.7298,17.45932,17.61364,19.58474,20.02666,19.79517,19.38833,17.32604,16.11953,15.62851,15.01122,14.70258,14.5693,14.35887,16.28086,18.69388,18.92536,16.56846,15.97222,13.34877,12.81566,12.04405,13.23653,14.1835,16.75786,17.55752,17.98541,18.85522,18.8482,19.02357,18.96044,17.31201,15.42508,14.38692,13.57323,12.36672,12.03002,11.41274,13.15236,15.88103,14.66049,12.8858,11.67228,11.03395,9.399551,8.375421,8.073793,10.6271,13.57323,13.61532,14.31678,14.73765,15.08838,15.62149,16.6807,15.28479,14.07127,13.14534,12.61223,12.57015,12.02301,12.17031,14.33782,18.83418,20.45455,18.67985,18.40629,16.51235,14.45006,14.61841,15.20763,15.57941,18.06958,19.88636,20.51066,21.633,23.24635,24.28451,24.70539,24.19332,22.81145,21.97671,21.58389,21.3945,21.21212,20.89646,21.1069,23.86364)

insulation = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
</code></pre>

<p>First off, the time series plot of the heat flux is this (the red line is when the insulation is removed):</p>

<p><img src=""http://i.stack.imgur.com/SYSQj.jpg"" alt=""Time series plot of heat flux""></p>

<p>Than this are the ACF and PACF plots of the same data:</p>

<p><img src=""http://i.stack.imgur.com/7keT7.jpg"" alt=""ACF and PACF of the data""></p>

<p>For my data, an <code>stl()</code> decomposition, run as <code>stl(ts(heat.flux, frequency = 24), 'period')</code></p>

<p>shows a strong â€˜seasonalâ€™ (i.e daily) component and a trend in the series. Â </p>

<p><img src=""http://i.stack.imgur.com/CUsta.jpg"" alt=""STL of the data""></p>

<p>Firs off I am trying to determine the best parameters for a SARIMA or ARIMAX model so that I can get an estimation of the effect removing the insulation. Despite the fact I can produce the ACF and PACF plots there is no way I can figure out the proper orders, so I load the library <code>forecast</code> and I run:</p>

<pre><code>library(forecast)
auto.arima(ts(heat.flux, frequency = 24), xreg = insulation, max.p = 10, max.q = 10, max.P = 10, max.Q = 10, stationary = F)Â 
</code></pre>

<p>The reason why I do not specify a stationary model is because of the trend I see with <code>stl()</code> and because I assume an effect of removing the insulation.</p>

<p>from <code>auto.arima()</code> I get:</p>

<pre><code>Series: ts(heat.flux, frequency = 24) 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept  carp.hour$interv
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449            4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075            0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=840.55   AICc=841.03   BIC=876.11
</code></pre>

<p>If I try to use the <code>TSA</code> package and use <code>arimax()</code> with those orders I get basically the same stuff:</p>

<pre><code>library(TSA)
arimax(ts(heat.flux, frequency = 24), xreg = insulation, order = c(2,0,2), seasonal = list(order = c(1,0,1), frequency = 24))
Series: x 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=838.55   AICc=839.03   BIC=874.11
</code></pre>

<p>And all is apparently well (Irrespective of the function I choose I get an estimate of the effect of the removal of the insulation and a se with it with is what I want). Â Unfortunately, when I test the fit of this model with the function <code>sarima()</code> from the <code>astsa</code>package I get significant Ljung-Box p-values for all my sensors and for all the lags:</p>

<pre><code>library(astsa)
sarima(ts(heat.flux, frequency = 24), p = 2, d = 0, q = 2, P =1, D = 0, Q = 1, S = 24, xreg = insulation)
$fit

Call:
stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, 
Q), period = S), xreg = xreg, optim.control = list(trace = trc, REPORT = 1, 
reltol = tol))

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood = -411.28,  aic = 840.55
</code></pre>

<p>but the plot that comes with is shows that at every single lag the Ljung-Box statistics is significant:</p>

<p><img src=""http://i.stack.imgur.com/ZMGKN.jpg"" alt=""SARIMA""></p>

<p>What is going on?  To sum it up:</p>

<ol>
<li>which of these models is the most correct to estimate the effect of insulation?</li>
<li>why are the Ljung-Box p-values all significant?  I would have though that the ARIMA/ARIMAX/SARIMA would have sorted that issue</li>
<li>If the orders calculated by <code>auto.arima()</code> are the problem, how could I find them in a different way (which is computationally feasible and does not take days).</li>
</ol>

<p>Finally, two notes.  I also have collected variables such as internal and external temperatures, windspeed, etc, but I would have though that integrating these in the model would be superfluous given the fact it is already an ARIMA model to start with.  Second, I am not at all wedded to this kind of analysis, but I am aware that a straightforward linear model would not be acceptable given the autocorrelation between the data points.</p>
"
"0.285298701078728","0.292446161141753","124707","<p>Sorry for the rather long introduction, but since I was (legitimately) critizised for not explaining my cause and questions enough, I will do so now. </p>

<p>I would like to conduct a <strong><em>(price)-forecast</em></strong> based on a multiple time series VAR-Model (vector autoregressive Model) with multiple endogeneous variables and two exogeneous. Since I am not that skillfull with regards to neither statistics nor R I want to keep is as simple as possible (Trend forecast for 3 months is sufficient).</p>

<p>I am using the ""vars"" - Package, <a href=""http://cran.r-project.org/web/packages/vars/vars.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/vars/vars.pdf</a> and all in all those four functions: decompose(), VARselect(), VAR(), and predict()</p>

<p>I have 1 dependent time series (y, in my model referred to as ""RH"", or ""raRH""), 4-5 endogeneous predictors and 2 exogeneous predictors.
All timeseries have a length of 1-91 observations and are monthly data without any gaps.</p>

<p><strong><em>Data description:</em></strong>
My y (dependent var) are sawlog prices, sawlogs are raw material for plenty of follow up products.<br> My endogeneous (since they all kind of correlate with each other and y) are follow up product-prices or further elaborated sawlogs. <br>My 2 exogeneous predictors are economic indicators similar to BIP etc.</p>

<p>All the time series are <em>non-stationary</em>, since I have read that you should use stationary data in order to gain a valid VAR-Model, I used the decompose() - function in R to split each variable into trend, season and the randwom walk. </p>

<pre><code> raKVH&lt;-decompose(KVH)$random
raKVH&lt;-na.omit(raKVH)
raSNS&lt;-decompose(SNP_S)$random
raSNS&lt;-na.omit(raSNS)
</code></pre>

<p>... and so on for every variable.<br><br>
What I'm interested now in order to do some forecasting are predictions of the randwom walk (right?!). Anyways, I found out that all my data is first-order-integrated, since taking the logarithm makes them all stationary timeseries (ts), tested via Dickey-Fuller-Test. </p>

<p>The picture also provides data example, first picture shows the raw-data, <img src=""http://i.stack.imgur.com/BcMOT.png"" alt=""enter image description here""></p>

<p>second picture the random walks gained by decomposing$random the raw-data. 
<img src=""http://i.stack.imgur.com/96yii.png"" alt=""enter image description here""></p>

<p>I used the command VARselect that automatically computes the optimal lag for my model, whereas tsall is my time-series matrix containing all the timeseries mentioned above.</p>

<pre><code>VARselect(tsall)
</code></pre>

<p>proceeding now with the estimation of the model VAR(p=number of lags given by VARselect), <strong><em>I encountered the following problem</em></strong>: how should I use the attribute ""type"" within the VAR-function? What does ""trend"",""none"", ""const"", ""both"" exactly mean? Since I have stationary data, there won't be any trend right? How can I check if there is a constant? Since the default value is ""const"", I chose to go with that.
<br><br>
<strong><em>The main question I have is the following:</em></strong><br>
How do I get ""real"" forecasts out of the prediction of the randwom walks anyways? If I want to predict the price of yt+3, I need more than the prediction of the random walks here, I need ""real figures"" like in graphic 1. How can I ""add back"" trend and season?</p>

<p>Third picture shows the Forecast of the random walk of my ""target Variable"" Y, but what's the next step here? 
<img src=""http://i.stack.imgur.com/ZtInk.png"" alt=""enter image description here""></p>

<p>Thank you for any help, if my questions/introduction are insufficient, please let me know. I'll try to explain myself better then.</p>
"
"0.176090181265125","0.199501867221527","126072","<p>I want to understand how forecast from STL function in R works. So, I am not giving any reproducible code here.</p>

<p>Below is the procedure that I worked on time series</p>

<ol>
<li><p>I used STL decomposition on my time series.</p></li>
<li><p>Checked residuals component from step 1 for white noise using Box.test</p></li>
<li>Found that residuals are not white-noise. So, used ARIMA model to fit a forecasting model.
Now, my task is to compute forecast values that consist of a. Seasonal and Trend component from step 1 above b. Residuals component from ARIMA model - from step 3 above.</li>
</ol>

<p>If I use</p>

<pre><code>forecast(stl(..)), 
</code></pre>

<p>it gives me</p>

<pre><code> Point Forecast     Lo 80    Hi 80    Lo 95    Hi 95 
</code></pre>

<p>However, I am interested in only seasonal and trend parts of forecast. How can I get seasonal trend components?</p>

<p>What components does constitute forecast(stl(..))</p>

<p>Please advise.</p>
"
"0.0880450906325624","0.0498754668053816","127123","<p>I am Using Holt-Winters model for the forecasting.</p>

<p>Below is the way I am proceeding:</p>

<pre><code>x&lt;-read.csv(""C:/Users/Navneet/Desktop/retail_data_12_08.csv"", header=TRUE)
xf&lt;-data.frame(year_quarter=as.yearqtr(x$year_quarter),sales_revenue=x$sales_revenue)
dput(xf)
</code></pre>

<p>output of the dput(xf) is: </p>

<pre><code>structure(list(year_quarter = structure(c(2009, 2009.25, 2009.5, 
 2009.75, 2010, 2010.25, 2010.5, 2010.75, 2011, 2011.25, 2011.5, 
 2011.75, 2012, 2012.25), class = ""yearqtr""), sales_revenue = c(3008L, 
 3244L, 8000L, 8719L, 3008L, 3244L, 78L, 7379L, 3735L, 7339L, 
 17240L, 20465L, 13134L, 15039L)), .Names = c(""year_quarter"", 
 ""sales_revenue""), row.names = c(NA, -14L), class = ""data.frame"")

xf.ts&lt;-ts(xf, frequency=4, start=c(2009,1), end=c(2012,2))
print(xf.ts)
</code></pre>

<p>output of the above line is:</p>

<pre><code>        year_quarter sales_revenue
2009 Q1      2009.00          3008
2009 Q2      2009.25          3244
2009 Q3      2009.50          8000
2009 Q4      2009.75          8719
2010 Q1      2010.00          3008
2010 Q2      2010.25          3244
2010 Q3      2010.50            78
2010 Q4      2010.75          7379
2011 Q1      2011.00          3735
2011 Q2      2011.25          7339
2011 Q3      2011.50         17240
2011 Q4      2011.75         20465
2012 Q1      2012.00         13134
2012 Q2      2012.25         15039
</code></pre>

<p>Now if I am applying the HoltWinters function</p>

<pre><code>fit&lt;-HoltWinters(xf.ts, alpha=NULL, beta=NULL, gamma=NULL, seasonal=""additive"")
forecast(fit,6)
</code></pre>

<p>it shows the 2016 quarters like this:</p>

<pre><code>        Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95
2016 Q1       17742.13 18025.67 17458.60 18175.77 17308.50
2016 Q2       18393.12      NaN      NaN      NaN      NaN
2016 Q3       13141.48      NaN      NaN      NaN      NaN
2016 Q4       15606.02 18076.96 13135.09 19385.00 11827.05
</code></pre>

<p>It should provide the 2012 Q3, 2012 Q4, 2013 Q1, 2013 Q2, 2013 Q3 and 2013 Q4.</p>

<p>Is there any thing I am doing wrong?  </p>

<p>Why are <code>NaN</code> values are coming out in the forecasting?</p>
"
"NaN","NaN","127337","<p>I am using <code>crost()</code> function of R for analyzing and forecasting intermittent demand/slow
moving items time series. I am having difficulty in understanding the output. Could anypne help in understanding the model in layman's terms.</p>

<p>Below is the code and output of the model:</p>

<pre><code>v &lt;- c(1910,874,1920,350,160,685,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,176,0,16,826,0,66,3798,800,1274,638,192,160,0,0,0,0,0,0,0,
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,28,0,0,276,0,0,1072,80,1776,240,80,528,3081,566,1483,112,272,120,0,0,0,
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,160,0,808,0,0,608,0,1480,184)
t &lt;- ts(v, f=52)
x&lt;-crost(t,h=52)
x
</code></pre>

<p>I have read the document of the  Package â€˜tsintermittentâ€™. But still did not get what are weights, frc.in, frc.out. I would like to discuss this on thread as I am totally new to this method. Thanks in advance.</p>
"
"0.0880450906325624","0.0997509336107633","141339","<p>I'm having trouble in taking a direction of my research project. I have independent variables that are commonly used as economic indicators and I want to include variables/indicators that are not commonly used to improve my eventual forecasts. I have 31 independent variables with 607 monthly observations after making it stationary and applying the scale function.(scale was applied cause my variable series are of different units/measures)
   I used the PCA function and got down to 13 components that capture 80% cumulative of the variance.
   Question is now that I have 13 new independent variables and the one dependent variable that is ternary in the sense that in the 607 observations it indicates 1 for peak, 0 for nothing, and -1 as trough, what model is best for forecasting/predicting the next 1 &amp; -1 of my dependent variable series based on my 13 independent principal components?</p>

<p>FYI: I have looked at VAR, Cointegration, Granger Causality, Multiple Linear Regression, but can't really make sense if what I'm using is correct and appropriate for my topic.</p>
"
"0.139211511597426","0.157720074469128","143358","<p>I'm working on a forecast for the following data:</p>

<pre><code>data &lt;-
c(1932, 4807, 6907, 8650, 10259, 11374, 8809, 6745, 7429, 
8041, 9740, 10971, 11953, 9227, 7401, 8355, 9681, 10438, 
11092, 11543, 9181, 7428, 8358, 10049, 10938, 12280, 
13063, 10022, 8125, 8763, 9330, 9919, 11309, 12169, 11063, 
10112, 10621, 11506, 12425, 12929, 13025, 10938, 9437, 
9910, 11104, 11985, 13024, 13962, 11900, 9576, 9590, 
10740, 11689, 13084, 13829, 11975, 10224, 10493, 11899, 
12697, 13959, 14415, 11650, 9477, 11166, 12327, 13238, 
13801, 13493, 11118, 9073, 9954, 11077, 12509, 12985, 
13380, 11454, 9265, 10053, 11443, 12132, 13733, 13850, 
11560, 9401, 9921, 11401, 12622, 14224, 14289, 12097, 
9623, 10630, 11572, 12816, 14180, 14125, 11667, 9328, 
9936, 11159, 12536, 13953, 13840, 11430, 9313, 9926, 
11557, 12428, 13802, 13041, 9927, 7448, 9143, 10872, 
12331, 14370, 14496, 13237, 11176, 11936, 12661, 14442, 
15005, 15359, 12871, 10505, 11231, 12078, 13307, 14027, 
14368, 12057, 9965, 10121, 11414, 13375, 14525, 14686, 
12243, 9833, 10722, 11778, 13143, 14844, 14856, 12745, 
9134, 7856, 9429, 11539, 13241, 14324, 12102, 10136, 
11107, 12028, 13999, 15130, 15488, 13379, 11028, 11708, 
13280, 14665, 15362, 15600, 12950, 10716, 10988, 12350, 
14163, 15264, 15724, 13374, 11764, 12711, 13239, 14849, 
15455, 15914, 13541, 10570, 9376, 10132, 11725, 12328, 
13105, 11022, 9710, 10659, 12068, 12890, 14242, 14294, 
11847, 9776, 10681, 12413, 13571, 14344, 14500, 12234, 
9961, 10699, 11626, 13135, 14387, 15282, 13028, 11211, 
11992, 13524, 15131, 15741, 15357, 12489, 9985, 10786, 
11492, 13851, 14509, 14751, 12327, 10023, 11315, 12363, 
13487, 14944, 15006, 12290, 9867, 11540, 12179, 14094, 
14941, 15006, 13585, 10769, 11408, 12634, 14073, 15361, 
15236, 13151, 9580, 8934, 10128, 12475, 13890, 14740, 
12617, 10358, 11648, 12418, 14094, 15127, 15775, 13647, 
11281, 11773, 13407, 15441, 15601, 15951, 13865, 11447, 
12422, 13725, 15766, 16389, 16868, 15221, 12503, 12780, 
14525, 16479, 17032, 17403, 14553, 12484, 13204, 13792, 
14896, 15673, 16332, 14196, 11749, 12977, 13886, 14931, 
15955, 16037, 14082, 11271, 12512, 13942, 16362, 17456, 
17446, 15509, 13069, 13524, 14918, 16161, 17524, 18138, 
14604, 12993, 13763, 14945, 16686, 17717, 17947, 15744, 
13388, 13177, 14588, 16075, 16705, 17074, 14415, 12766, 
13372, 14033, 14300, 12508, 11502, 9391, 7689, 9613, 
12291, 14448, 15075, 15670, 13929, 10989, 11875, 13409, 
15203, 15654, 16150, 13387, 10931, 11492, 12479, 13674, 
14519, 14241, 11685, 9486, 9990, 11440, 12415, 13505, 
12103, 10311, 8267, 7510, 8595, 10620, 11664, 3182, 6241, 
9365, 10965, 12372, 9958, 8088, 9290, 10665, 12132, 12827, 
13040, 10692, 8882, 9538, 10027, 12086, 13276, 13107, 
10680, 9136, 10744, 11733, 13334, 14654, 14830, 12189, 
9613, 11399, 12837, 13661, 15007, 15579, 12268, 9703, 
10627, 12077, 13287, 14459, 14825, 11958, 10049, 11512, 
12770, 13869, 14873, 15233, 12056, 9654, 10386, 11465, 
13354, 14601, 15161, 12324, 9782, 10791, 12502, 14111, 
14914, 15250, 12366, 10333, 11638, 12449, 13518, 14637, 
14756, 12011, 9878, 10976, 12464, 13674, 14979, 15312, 
12106, 10127, 11666, 12843, 13910, 15024, 15333, 12308, 
9992, 11278, 13364, 14966, 15231, 15507, 13744, 11417, 
12232, 14414, 15245, 15988, 15168, 11905, 9165, 10536, 
12570, 14106, 15204, 15509, 12821, 10321, 11282, 13133, 
14174, 15099, 14750, 12817, 10384, 11368, 12994, 14591, 
16154, 15904, 12784, 10737, 11865, 13809, 14721, 15202, 
15322, 12722, 10741, 11991, 13546, 14716, 15817, 15879, 
12679, 10390, 11524, 13140, 14426, 15613, 16212, 13088, 
10720, 11730, 13776, 14477, 15758, 15922, 13119, 9220, 
8372, 10239, 12397, 14740, 15550, 13306, 10833, 11892, 
13630, 15186, 16154, 16678, 12898, 10485, 11313, 13705, 
15572, 16086, 16305, 14129, 11066, 12251, 13830, 15345, 
16550, 16518, 13700, 10890, 12301, 14163, 15890, 16985, 
17544, 15337, 12633, 13383, 12813, 12051, 13149, 13636, 
10914, 9617, 10619, 12224, 13954, 15325, 15473, 12418, 
9730, 11214, 12572, 14565, 15287, 15721, 12519, 10689, 
11662, 13139, 14902, 16374, 16392, 13895, 11777, 12948, 
14326, 15625, 16745, 16980, 13946, 11181, 12665, 13678, 
15269, 16279, 16634, 14399, 11142, 11900, 13800, 14783, 
16626, 16861, 13917, 11228, 12531, 14206, 15773, 16344, 
16930, 13945, 11110, 12427, 14085, 15627, 16854, 17106, 
14677, 10410, 8550, 10626, 13366, 15337, 16460, 13619, 
11630, 12582, 13926, 15297, 16715, 17036, 14063, 11368, 
12246, 14111, 15525, 16900, 17272, 14254, 11961, 13155, 
14579, 16260, 17187, 17919, 15493, 13162, 13771, 15231, 
15836, 16880, 16976, 14728, 12106, 13030, 13848, 15344, 
16475, 17122, 13601, 10921, 12043, 14114, 15846, 16190, 
17125, 13769, 10768, 12336, 13849, 16138, 17507, 18050, 
15492, 12905, 12847, 14181, 15967, 16704, 17762, 14882, 
12591, 13807, 14959, 16933, 17369, 17453, 14351, 11582, 
13102, 14328, 16185, 16321, 16843, 13773, 11053, 12199, 
14147, 14470, 12598, 11916, 9185, 7903, 9742, 12691, 
15153, 15945, 16254, 13630, 11437, 12235, 14040, 15161, 
15995, 16291, 12944, 10947, 12055, 13444, 14852, 16029, 
16361, 13658, 10885, 11604, 13030, 13959, 14291, 14786, 
12002, 9014, 7610, 7426, 9602, 11077, 12544, 11334, 5710, 
9874, 11949, 10321, 8945, 10152, 11821, 13434, 15187, 
15269, 12661, 10699, 12040, 13154, 14149, 15472, 16569, 
13008, 10521, 11674, 13272, 14025, 15803, 16791, 13615, 
11043, 12448, 13929, 15158, 16610, 17520, 13900, 11095, 
11735, 13652, 14939, 16001, 16265, 13371, 11198, 11583, 
13377, 15361, 16420, 16765, 13800, 10866, 12026, 13908, 
14902, 16044, 16807, 13694, 11475, 13009, 14453, 16231, 
17093, 17411, 14433, 12242, 13035, 14304, 16309, 17026, 
16811, 13986, 11812, 13216, 14397, 16026, 17780, 17463, 
14717, 12029, 13046, 14820, 16626, 17564, 17802, 14134, 
13158, 15356, 16573, 16887, 17494, 17326, 13525, 11517, 
12410, 13817, 14933, 16399, 17019, 14008, 11808, 12599, 
14639, 16339, 17521, 17820, 14444, 11530, 13352, 14997, 
16038, 17631, 17614, 15601, 15176, 16930, 17979, 18772, 
19728, 19452, 16272, 14006, 15510, 17299, 17774, 18345, 
19080, 16486, 14242, 15465, 16973, 17971, 19068, 19075, 
15606, 13315, 14784, 16505, 17910, 18586, 18315, 15659, 
13621, 14673, 16037, 17467, 17972, 17676, 15452, 11850, 
10959, 13641, 15217, 16813, 17641, 15404, 13102, 14391, 
15764, 17326, 17715, 17947, 15272, 13078, 13962, 15372, 
18292, 18569, 16427, 13374, 14725, 15957, 17425, 18530, 
19251, 17094, 13711, 15275, 16663, 18254, 19023, 19787, 
16636, 14398, 15392, 16302, 15844, 14301, 14559, 11739, 
10080, 11690, 14352, 16702, 17810, 17898, 15159, 12527, 
14250, 15788, 17012, 18219, 17743, 15183, 12633, 14033, 
15528, 16984, 18041, 18388, 15248, 12831, 14289, 16143, 
17340, 18863, 18597, 15984, 13697, 14653, 16143, 17262, 
17805, 18565, 16147, 14734, 16548, 17410, 18044, 18705, 
18462, 15706, 13242, 14977, 16168, 17683, 18224, 18454, 
15784, 14003, 16605, 18013, 19361, 19204, 18970, 16655, 
12928, 11502, 13233, 15211, 16883, 17454, 15043, 12953, 
14515, 15846, 17501, 18922, 18903, 16175, 13492, 14150, 
15710, 18297, 18872, 19490, 15921, 13935, 14943, 16457, 
18425, 19975, 20440, 17716, 15059, 16086, 17290, 18477, 
19896, 20115, 17580, 15001, 15640, 17915, 18951, 20029, 
20221, 16653, 15063, 15726, 16849, 18121, 18843, 19112, 
16516, 13960, 15255, 16910, 18895, 20091, 20663, 17698, 
15441, 16775, 18158, 19897, 20424, 20111, 17784, 15044, 
16869, 17773, 19783, 21255, 20632, 18081, 15891, 17180, 
18143, 20197, 20926, 20639, 18407, 16313, 16998, 17860, 
19177, 19618, 19919, 17662, 16033, 17439, 18741, 18108, 
16641, 16319, 13221, 11160, 12783, 14876, 16831, 18379, 
18858, 16191, 14632, 16089, 16828, 18169, 19512, 18828, 
17364, 15516, 17065, 18245, 18684, 19472, 19235, 16885, 
14854, 14526, 12921, 12675, 14884, 15284, 13492, 11457, 
5938, 9694, 9429, 9142, 10648, 13235, 15610, 16868, 17364, 
16043, 14497, 15329, 16839, 17548, 18818, 19320, 15884, 
13834, 14748, 15784, 16729, 18274, 19138, 17413, 15394, 
16596, 17853, 18934, 20310, 20165, 18870, 16562, 16823, 
18051, 18816, 20410, 21211, 18551, 16274, 17289, 18317, 
20259, 19993, 19831, 18166, 16517, 17114, 17763, 19011, 
20541, 19974, 18105, 16130, 17422, 18472, 20213, 20721, 
20803, 19250, 16246, 16582, 18410, 19559, 20821, 20412, 
18576, 16272, 16917, 19027, 19917, 20418, 21188, 18382, 
16842, 17911, 19126, 20471, 21120, 20756, 18190, 15873, 
16924, 18468, 19579, 20877, 20726, 18525, 16110, 17480, 
19313, 20323, 20661, 20541, 18284, 16124, 17312, 18361, 
19170, 19945, 20548, 17605, 15973, 17488, 17444, 19086, 
19775, 19827, 17269, 14616, 15690, 16469, 18626, 19288, 
20111, 17769, 15738, 17060, 18885, 20010, 21371, 21541, 
18682, 15971, 16714, 18659, 19934, 21499, 22118, 18952, 
16025, 18120, 18897, 20630, 20286, 21077, 17710, 14857, 
16050, 17877, 19928, 21299, 21202, 18858, 14339, 13172, 
15521, 17434, 19823, 20679, 18288, 16798, 18673, 20628, 
21462, 22720, 22241, 20064, 17327, 18720, 19896, 19710, 
21185, 21916, 19661, 17134, 18027, 19449, 20912, 21234, 
21950, 19495, 17023, 18473, 19080, 20875, 21031, 21492, 
20091, 17511, 18834, 19126, 19922, 21215, 19017, 15506, 
12854, 14605, 16279, 18129, 20043, 21248, 18518, 15467, 
16586, 18277, 18915, 20597, 21244, 19024, 16294, 17234, 
18786, 20960, 21345, 22068, 19774, 17491, 18279, 19809, 
20757, 21618, 22131, 20214, 17581, 18321, 19590, 21486, 
22492, 23194, 20020, 16819, 17892, 18948, 20921, 21696, 
22549, 19559, 16404, 17301, 18659, 20430, 22300, 22569, 
19630, 16800, 17898, 19584, 21190, 21926, 22359, 20157, 
15823, 14136, 15930, 18341, 21044, 21204, 18994, 16973, 
18171, 19378, 20794, 22442, 22144, 19874, 17859, 18703, 
19082, 20781, 21860, 21536, 20172, 18429, 19221, 19824, 
21326, 22504, 23381, 21733, 19231, 20312, 21994, 22609, 
23317, 23074, 22005, 19209, 20734, 22513, 23017, 23698, 
24385, 22512, 19471, 20061, 21235, 22351, 22532, 22869, 
20409, 17908, 18722, 19894, 20960, 21999, 22125, 20797, 
19091, 19910, 20463, 22106, 22737, 22827, 21695, 19498, 
20180, 21204, 22272, 22803, 22808, 20979, 18952, 20365, 
20875, 22944, 23022, 22786, 21284, 19302, 20394, 21144, 
22633, 23511, 23355, 21979, 19988, 20143, 21966, 22574, 
19974, 19410, 15641, 13265, 14880, 16838, 19262, 19941, 
20479, 18929, 17760, 18078, 19055, 20553, 21732, 21671, 
19218, 18485, 18864, 20278, 21120, 21747, 21087, 17982, 
15115, 16518, 16282, 15032, 15658, 14966, 12172, 10336, 
12669, 14238, 14031, 12441, 13313, 11047, 10158, 12438, 
14255, 16434, 17873, 18481, 16360, 14479, 15595, 17392, 
18878, 19999, 19958, 16748, 13852, 14931, 16410, 18097, 
19654, 19480, 16387, 14515, 15205, 16854, 18544, 19510, 
20382, 17838, 14878, 15041, 16661, 19008, 20265, 20947, 
18048, 16472, 16434, 18250, 19571, 21148, 20117, 17788, 
14321, 14996, 15779, 17789, 18804, 18934, 17488, 15095, 
15859, 16691, 18369, 20012, 21073, 18029, 15582, 17247, 
18608, 19783, 20322, 20908, 18221, 15919, 17107, 18404, 
19262, 21741, 21514, 19798, 17410, 17973, 18469, 17910, 
14901)
</code></pre>

<p>The <code>ts.plot(data)</code> gives:<img src=""http://i.stack.imgur.com/E6WU0.jpg"" alt=""enter image description here""></p>

<p>With this data, I'm looking to forecast the values for the next year. This data is victim to both weekly and yearly seasonality. Due to this, I first attempted to use <code>tbats</code> from the <code>forecast</code> package but received an improper forecast that mirrors that found at <a href=""http://www.github.com/robjhyndman/forecast/issues/87"" rel=""nofollow"">http://www.github.com/robjhyndman/forecast/issues/87</a></p>

<p>Instead, I used the following code:</p>

<pre><code>n&lt;-length(data)
bestfit &lt;- list(aicc=Inf)
bestk &lt;- 0
for(i in 1:20)
{
fit &lt;- auto.arima(data, xreg = fourier(1:n,i,m1) + fourier(1:n,i,m2), max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
if(fit$aicc &lt; bestfit$aicc)
{
    bestfit &lt;- fit
    bestk &lt;- i
}
}

k &lt;- bestk

bestfit &lt;- auto.arima(data, xreg = fourier(1:n,k,m1) + fourier(1:n,k,m2), max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
accuracy(bestfit)
fc &lt;- forecast(bestfit, xreg = fourier((n+1):(n+365),k,m1) + fourier((n+1):(n+365),k,m2), level = c(50,80,90), bootstrap = TRUE)
plot(fc)
</code></pre>

<p>This code is searching for the best ARIMA model through the use of Fourier terms in <code>xreg</code> to capture both seasonality components. This Fourier function is defined (per <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) as:</p>

<pre><code>fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}
</code></pre>

<p>This forecasting gives me the following plot:<img src=""http://i.stack.imgur.com/2IsSD.jpg"" alt=""enter image description here""></p>

<p>In looking at this forecast, it seems by my naked eye to be off. Just by observation it appears that my forecast is not properly catching the small, but visible, increasing trend. Instead of being ""centered"" around the extended trendline, it appears that the forecast is ""centered"" around the mean of the entire dataset.</p>

<p>First off, am I doing something that is just blatantly wrong? (my mind is a little fuzzy this morning)</p>

<p>If my forecast is correct, how is it that it falls so much below the extended trendline?</p>

<p>Lastly, are there any other suggestions which might be beneficial to my forecasting?</p>
"
"0.186771841909407","0.16458064370034","144158","<p>I am trying to do time series analysis and am new to this field. I have daily count of an event from 2006-2009 and I want to fit a time series model to it. Here is the progress that I have made:</p>

<pre><code>timeSeriesObj = ts(x,start=c(2006,1,1),frequency=365.25)
plot.ts(timeSeriesObj)
</code></pre>

<p>The resulting plot I get is:</p>

<p><img src=""http://i.stack.imgur.com/q2Gf5.jpg"" alt=""Time Series Plot""></p>

<p>In order to verify whether there is seasonality and trend in the data or not, I follow the steps mentioned in this <a href=""http://stats.stackexchange.com/questions/57705/identify-seasonality-in-time-series-data"">post</a> :</p>

<pre><code>ets(x)
fit &lt;- tbats(x)
seasonal &lt;- !is.null(fit$seasonal)
seasonal
</code></pre>

<p>and in Rob J Hyndman's <a href=""http://robjhyndman.com/hyndsight/detecting-seasonality/"" rel=""nofollow"">blog</a>:</p>

<pre><code>library(fma)
fit1 &lt;- ets(x)
fit2 &lt;- ets(x,model=""ANN"")

deviance &lt;- 2*c(logLik(fit1) - logLik(fit2))
df &lt;- attributes(logLik(fit1))$df - attributes(logLik(fit2))$df 
#P value
1-pchisq(deviance,df)
</code></pre>

<p>Both cases indicate that there is no seasonality.</p>

<p>When I plot the ACF &amp; PACF of the series, here is what I get:</p>

<p><img src=""http://i.stack.imgur.com/mgBav.jpg"" alt=""ACF"">
<img src=""http://i.stack.imgur.com/p4DYo.jpg"" alt=""PACF""></p>

<p>My questions are:</p>

<ol>
<li><p>Is this the way to handle daily time series data? This <a href=""http://www.r-bloggers.com/forecasting-with-daily-data/"" rel=""nofollow"">page</a> suggests that I should be looking at both weekly and annual patterns but the approach is not clear to me.</p></li>
<li><p>I do not know how to proceed once I have the ACF and PACF plots.</p></li>
<li><p>Can I simply use the auto.arima function?</p>

<p>fit &lt;- arima(myts, order=c(p, d, q)</p></li>
</ol>

<p>*****Updated Auto.Arima results******</p>

<p>When i change the frequency of the data to 7 according to Rob Hyndman's comments <a href=""http://stats.stackexchange.com/questions/14742/auto-arima-with-daily-data-how-to-capture-seasonality-periodicity"">here</a>, auto.arima selects a seasonal ARIMA model and outputs:</p>

<pre><code>Series: timeSeriesObj 
ARIMA(1,1,2)(1,0,1)[7]                    

Coefficients:
       ar1      ma1     ma2    sar1     sma1
      0.89  -1.7877  0.7892  0.9870  -0.9278
s.e.   NaN      NaN     NaN  0.0061   0.0162

sigma^2 estimated as 21.72:  log likelihood=-4319.23
AIC=8650.46   AICc=8650.52   BIC=8682.18 
</code></pre>

<p>******Updated Seasonality Check******</p>

<p>When I test seasonality with frequency 7, it outputs True but with seasonality 365.25, it outputs false. <strong>Is this enough to conclude a lack of yearly seasonality?</strong></p>

<pre><code>timeSeriesObj = ts(x,start=c(2006,1,1),frequency=7)
fit &lt;- tbats(timeSeriesObj)
seasonal &lt;- !is.null(fit$seasonal)
seasonal
</code></pre>

<p>returns:</p>

<pre><code>True
</code></pre>

<p>while </p>

<pre><code>timeSeriesObj = ts(x,start=c(2006,1,1),frequency=365.25)
fit &lt;- tbats(timeSeriesObj)
seasonal &lt;- !is.null(fit$seasonal)
seasonal
</code></pre>

<p>returns:</p>

<pre><code>False
</code></pre>
"
"0.482307635398602","0.465810710523962","144745","<p>I have 17 years (1995 to 2011) of death certificate data related to suicide deaths for a state in the U.S. There is a lot of mythology out there about suicides and the months/seasons, much of it contradictory, and of the literature I've reviewed, I do not get a clear sense of methods used or confidence in results.</p>

<p>So I've set out to see if I can determine whether suicides are more or less likely to occur in any given month within my data set. All of my analyses are done in R.</p>

<p>The total number of suicides in the data is 13,909.</p>

<p>If you look at the year with the fewest suicides, they occur on 309/365 days (85%). If you look at the year with the most suicides, they occur on 339/365 days (93%).</p>

<p>So there are a fair number of days each year without suicides. However, when aggregated across all 17 years, there are suicides on every day of the year, including February 29 (although only 5 when the average is 38).</p>

<p><img src=""http://i.stack.imgur.com/VMQYa.jpg"" alt=""enter image description here""></p>

<p>Simply adding up the number of suicides on each day of the year doesn't indicate a clear seasonality (to my eye).</p>

<p>Aggregated at the monthly level, average suicides per month range from:</p>

<p>(m=65, sd=7.4, to m=72, sd=11.1)</p>

<p>My first approach was to aggregate the data set by month for all years and do a chi-square test after computing the expected probabilities for the null hypothesis, that there was no systematic variance in suicide counts by month. I computed the probabilities for each month taking into account the number of days (and adjusting February for leap years).</p>

<p>The chi-square results indicated no significant variation by month:</p>

<pre><code># So does the sample match  expected values?
chisq.test(monthDat$suicideCounts, p=monthlyProb)
# Yes, X-squared = 12.7048, df = 11, p-value = 0.3131
</code></pre>

<p>The image below indicates total counts per month. The horizontal red lines are positioned at the expected values for February, 30 day months, and 31 day months respectively. Consistent with the chi-square test, no month is outside the 95% confidence interval for expected counts.
<img src=""http://i.stack.imgur.com/XRCzM.jpg"" alt=""enter image description here""></p>

<p>I thought I was done until I started to investigate time series data. As I imagine many people do, I started with the non-parametric seasonal decomposition method using the <code>stl</code> function in the stats package. </p>

<p>To create the time series data, I started with the aggregated monthly data:</p>

<pre><code>suicideByMonthTs &lt;- ts(suicideByMonth$monthlySuicideCount, start=c(1995, 1), end=c(2011, 12), frequency=12) 

# Plot the monthly suicide count, note the trend, but seasonality?
plot(suicideByMonthTs, xlab=""Year"",
  ylab=""Annual  monthly  suicides"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/xSWJm.jpg"" alt=""enter image description here""></p>

<pre><code>     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
1995  62  47  55  74  71  70  67  69  61  76  68  68
1996  64  69  68  53  72  73  62  63  64  72  55  61
1997  71  61  64  63  60  64  67  50  48  49  59  72
1998  67  54  72  69  78  45  59  53  48  65  64  44
1999  69  64  65  58  73  83  70  73  58  75  71  58
2000  60  54  67  59  54  69  62  60  58  61  68  56
2001  67  60  54  57  51  61  67  63  55  70  54  55
2002  65  68  65  72  79  72  64  70  59  66  63  66
2003  69  50  59  67  73  77  64  66  71  68  59  69
2004  68  61  66  62  69  84  73  62  71  64  59  70
2005  67  53  76  65  77  68  65  60  68  71  60  79
2006  65  54  65  68  69  68  81  64  69  71  67  67
2007  77  63  61  78  73  69  92  68  72  61  65  77
2008  67  73  81  73  66  63  96  71  75  74  81  63
2009  80  68  76  65  82  69  74  88  80  86  78  76
2010  80  77  82  80  77  70  81  89  91  82  71  73
2011  93  64  87  75 101  89  87  78 106  84  64  71
</code></pre>

<p>And then performed the <code>stl()</code> decomposition</p>

<pre><code># Seasonal decomposition
suicideByMonthFit &lt;- stl(suicideByMonthTs, s.window=""periodic"")
plot(suicideByMonthFit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/cS5pE.jpg"" alt=""enter image description here""></p>

<p>At this point I became concerned because it appears to me that there is both a seasonal component and a trend. After much internet research I decided to follow the instructions of Rob Hyndman and George AthanaÂ­sopouÂ­los as laid out in their on-line text ""Forecasting: principles and practice"", specifically to apply a seasonal ARIMA model.</p>

<p>I used <code>adf.test()</code> and <code>kpss.test()</code> to assess for <em>stationarity</em> and got conflicting results. They both rejected the null hypothesis (noting that they test the opposite hypothesis).</p>

<pre><code>adfResults &lt;- adf.test(suicideByMonthTs, alternative = ""stationary"") # The p &lt; .05 value 
adfResults

    Augmented Dickey-Fuller Test

data:  suicideByMonthTs
Dickey-Fuller = -4.5033, Lag order = 5, p-value = 0.01
alternative hypothesis: stationary

kpssResults &lt;- kpss.test(suicideByMonthTs)
kpssResults

    KPSS Test for Level Stationarity

data:  suicideByMonthTs
KPSS Level = 2.9954, Truncation lag parameter = 3, p-value = 0.01
</code></pre>

<p>I then used the algorithm in the book to see if I could determine the amount of differencing that needed to be done for both the trend and season. I ended  with 
nd = 1, ns = 0.</p>

<p>I then ran <code>auto.arima</code>, which chose a model that had both a trend and a seasonal component along with a ""drift"" type constant.</p>

<pre><code># Extract the best model, it takes time as I've turned off the shortcuts (results differ with it on)
bestFit &lt;- auto.arima(suicideByMonthTs, stepwise=FALSE, approximation=FALSE)
plot(theForecast &lt;- forecast(bestFit, h=12))
theForecast
</code></pre>

<p><img src=""http://i.stack.imgur.com/qTUi9.jpg"" alt=""enter image description here""></p>

<pre><code>&gt; summary(bestFit)
Series: suicideByMonthFromMonthTs 
ARIMA(0,1,1)(1,0,1)[12] with drift         

Coefficients:
          ma1    sar1     sma1   drift
      -0.9299  0.8930  -0.7728  0.0921
s.e.   0.0278  0.1123   0.1621  0.0700

sigma^2 estimated as 64.95:  log likelihood=-709.55
AIC=1429.1   AICc=1429.4   BIC=1445.67

Training set error measures:
                    ME    RMSE     MAE       MPE     MAPE     MASE       ACF1
Training set 0.2753657 8.01942 6.32144 -1.045278 9.512259 0.707026 0.03813434
</code></pre>

<p>Finally, I looked at the residuals from the fit and if I understand this correctly, since all values are within the threshold limits, they are behaving like white noise and thus the model is fairly reasonable. I ran a <em>portmanteau test</em> as described in the text, which had a p value well above 0.05, but I'm not sure that I have the parameters correct.</p>

<pre><code>Acf(residuals(bestFit))
</code></pre>

<p><img src=""http://i.stack.imgur.com/gso3q.jpg"" alt=""enter image description here""></p>

<pre><code>Box.test(residuals(bestFit), lag=12, fitdf=4, type=""Ljung"")

    Box-Ljung test

data:  residuals(bestFit)
X-squared = 7.5201, df = 8, p-value = 0.4817
</code></pre>

<p>Having gone back and read the chapter on arima modeling again, I realize now that <code>auto.arima</code> did choose to model trend and season. And I'm also realizing that forecasting is not specifically the analysis I should probably be doing. I want to know if a specific month (or more generally time of year) should be flagged as a high risk month. It seems that the tools in the forecasting literature are highly pertinent, but perhaps not the best for my question. Any and all input is much appreciated.</p>

<p>I'm posting a link to a csv file that contains the daily counts. The file looks like this:</p>

<pre><code>head(suicideByDay)

        date year month day_of_month t count
1 1995-01-01 1995    01           01 1     2
2 1995-01-03 1995    01           03 2     1
3 1995-01-04 1995    01           04 3     3
4 1995-01-05 1995    01           05 4     2
5 1995-01-06 1995    01           06 5     3
6 1995-01-07 1995    01           07 6     2
</code></pre>

<p><a href=""https://dl.dropboxusercontent.com/u/1252082/daily_suicide_counts.csv"" rel=""nofollow"">daily_suicide_data.csv</a></p>

<p>Count is the number of suicides that happened on that day. ""t"" is a numeric sequence from 1 to the total number of days in the table (5533).</p>

<p>I've taken note of comments below and thought about two things related to modeling suicide and seasons. First, with respect to my question, months are simply proxies for marking change of season, I am not interested in wether or not a particular month is different from others (that of course is an interesting question, but it's not what I set out to investigate). Hence, I think it makes sense to <strong>equalize</strong> the months by simply using the first 28 days of all months. When you do this, you get a slightly worse fit, which I am interpreting as more evidence towards a lack of seasonality. In the output below, the first fit is a reproduction from an answer below using months with their true number of days, followed by a data set <strong>suicideByShortMonth</strong> in which suicide counts were computed from the first 28 days of all months. I'm interested in what people think about wether or not this adjustment is a good idea, not necessary, or harmful?</p>

<pre><code>&gt; summary(seasonFit)

Call:
glm(formula = count ~ t + days_in_month + cos(2 * pi * t/12) + 
    sin(2 * pi * t/12), family = ""poisson"", data = suicideByMonth)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.4782  -0.7095  -0.0544   0.6471   3.2236  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)         2.8662459  0.3382020   8.475  &lt; 2e-16 ***
t                   0.0013711  0.0001444   9.493  &lt; 2e-16 ***
days_in_month       0.0397990  0.0110877   3.589 0.000331 ***
cos(2 * pi * t/12) -0.0299170  0.0120295  -2.487 0.012884 *  
sin(2 * pi * t/12)  0.0026999  0.0123930   0.218 0.827541    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 302.67  on 203  degrees of freedom
Residual deviance: 190.37  on 199  degrees of freedom
AIC: 1434.9

Number of Fisher Scoring iterations: 4

&gt; summary(shortSeasonFit)

Call:
glm(formula = shortMonthCount ~ t + cos(2 * pi * t/12) + sin(2 * 
    pi * t/12), family = ""poisson"", data = suicideByShortMonth)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.2414  -0.7588  -0.0710   0.7170   3.3074  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)         4.0022084  0.0182211 219.647   &lt;2e-16 ***
t                   0.0013738  0.0001501   9.153   &lt;2e-16 ***
cos(2 * pi * t/12) -0.0281767  0.0124693  -2.260   0.0238 *  
sin(2 * pi * t/12)  0.0143912  0.0124712   1.154   0.2485    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 295.41  on 203  degrees of freedom
Residual deviance: 205.30  on 200  degrees of freedom
AIC: 1432

Number of Fisher Scoring iterations: 4
</code></pre>

<p>The second thing I've looked into more is the issue of using month as a proxy for season. Perhaps a better indicator of season is the number of daylight hours an area receives. This data comes from a northern state that has substantial variation in daylight. Below is a graph of the daylight from the year 2002. </p>

<p><img src=""http://i.stack.imgur.com/yvVXl.jpg"" alt=""enter image description here""></p>

<p>When I use this data rather than month of the year, the effect is still significant, but the effect is very, very small. The residual deviance is much larger than the models above. If daylight hours is a better model for seasons, and the fit is not as good, is this more evidence of very small seasonal effect? </p>

<pre><code>&gt; summary(daylightFit)

Call:
glm(formula = aggregatedDailyCount ~ t + daylightMinutes, family = ""poisson"", 
    data = aggregatedDailyNoLeap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.0003  -0.6684  -0.0407   0.5930   3.8269  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      3.545e+00  4.759e-02  74.493   &lt;2e-16 ***
t               -5.230e-05  8.216e-05  -0.637   0.5244    
daylightMinutes  1.418e-04  5.720e-05   2.479   0.0132 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 380.22  on 364  degrees of freedom
Residual deviance: 373.01  on 362  degrees of freedom
AIC: 2375

Number of Fisher Scoring iterations: 4
</code></pre>

<p>I'm posting the daylight hours in case anyone wants to play around with this. Note, this is not a leap year, so if you want to put in the minutes for the leap years, either extrapolate or retrieve the data.</p>

<p><a href=""https://dl.dropboxusercontent.com/u/1252082/state.daylight.2002.csv"" rel=""nofollow"">state.daylight.2002.csv</a></p>

<p>[<strong>Edit</strong> to add plot from deleted answer (hopefully rnso doesn't mind me moving the plot in the deleted answer up here to the question. svannoy, if you don't want this added after all, you can revert it)]</p>

<p><img src=""http://i.stack.imgur.com/WiuvE.png"" alt=""enter image description here""></p>
"
"0.164717281867254","0.186616908791155","145193","<p>I'm trying to model the responses from a direct mail marketing campaign so that I can use it to forecast for future campaigns. In the code below, I started with the average number of responses by day of a historical campaign (contained in the vector: ""responses""). I was then able to fit a 63-day (8-wk) smooth curve to model the data. But I now need a way to use this curve to help me with forecasting. For example, if I think I'll get x number of total responses from a campaign, I need to know when those responses are most likely to happen. In other words, I need the daily ""factors"" (i.e. the percentage of the total responses that is most likely to respond on each day).  Thanks!</p>

<p>p.s. if anyone has a better way of approaching this I'd love to hear!</p>

<pre><code>#vector of direct mail marketing responses over 63 days 
responses &lt;- c(
24.16093706,
41.59607507,
68.20083052,
85.19109064,
100.0704403,
58.6600221,
86.08475816,
88.97439581,
65.58341418,
49.25588053,
53.63602085,
47.03620672,
29.71552264,
32.85862747,
31.29118096,
23.67961069,
19.81261675,
18.69300933,
17.25738435,
12.01161679,
12.36734071,
14.32360673,
11.02390849,
9.108021409,
9.647965622,
8.815576548,
5.67225654,
5.739220185,
6.233999138,
5.527376627,
5.024065761,
5.565266355,
4.626749364,
3.480761716,
4.621902301,
4.518554271,
4.075985188,
3.204946787,
3.174020873,
2.966915873,
2.129178828,
2.673009031,
2.410429043,
2.331287075,
2.509300578,
2.13820695,
2.53433787,
1.603934405,
1.555813592,
1.834605068,
1.842905685,
1.454045577,
2.08684322,
1.318276487,
0.807666643,
1.333167088,
1.004526525,
1.180110123,
1.078079735,
1.151394678,
1.426747942,
0.699119833,
0.583347236)


set.seed(2)
install.packages(""MASS"")
library(""MASS"")


shape_and_scale &lt;- fitdistr(responses,'weibull')

#check the shape and scale
shape_and_scale

#plug in the shape and scale
#essentially taking the total number of respondants and for each, doing a random simulation for what day they'll respond- according to a weibull distribution
#rweibull makes it a random generation
#also need to create a variable for the total number of responses
total_responses &lt;- 1121
day_response &lt;- round(rweibull(total_responses,0.70730466,13.79467490)+.5)

day_response

day_response_frequency_table &lt;- as.data.frame(table(round(rweibull(total_responses,0.70730466,13.79467490)+.5)))

day_response_frequency_table
#notice that it extends beyond our 63 day limit for modeling a campaign

#create a factor with levels so that we can limit our distribution to 63 days
day_response_with_levels &lt;- factor(day_response, levels=0:63)
day_response_with_levels
response_frequency &lt;- as.data.frame(table(day_response_with_levels))
response_frequency

#now use dweibull and the curve() function to create a curve
?dweibull 
curve(x*dweibull(x,0.70730466,13.79467490),from=0, to=63)
</code></pre>
"
"0.124514561272938","0.10580184237879","152012","<p>This might fit better here than on stackoverflow, I guess.</p>

<p>I was <a href=""http://stackoverflow.com/questions/30139874/r-dynamic-linear-regression-with-dynlm-package-how-to-predict"">trying to build a dynamic regression model with the dynlm</a> package, but it did not work out. After reading <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">this</a> by Hyndman, I now switched to an ARMAX model:</p>

<pre><code>y_t = a_1*x1_t + a_2*x2_t + ... + a_k*xk_t + n_t
</code></pre>

<p>where the error term follows an ARMA model</p>

<pre><code>n_t ~ ARMA(p,q)
</code></pre>

<p>So far I am using the function <code>auto.arima(y, xreg=cbind(x1, ..., xk))</code> from the <code>forecast</code>package, which is doing the job!</p>

<p>As a benchmark I am running a pure multiple regression with <code>lm()</code>, where I make use of the <code>step()</code> function to kick out non relevant variables (about 100 variables, from which 96 are dummies) to optimize the model according to <code>AIC</code>.</p>

<p>The in-sample forecasting for both models is more or less equal. As the ARMAX model always includes <strong>all</strong> independent variables <code>(x1, ..., xk)</code>, I am pretty sure that, if I could apply the <code>step()</code> function on it, I would achieve a further improvement here.</p>

<p>The problem is that the <code>step()</code> function does not work on <code>auto.arima()</code>?!</p>

<p>Do you have any suggestions how I could still do this? Or would I need a totally new approach?</p>

<p>(I have not provide a reproducible example, as this is a rather general question of which methods/functions/packages to use. If the question is not clear enough, please tell me and I will try to provide one)</p>
"
"0.139211511597426","0.157720074469128","152976","<p>My goal is to forecast sensor measurements (e.g. temperature, humidity) in a lightweight (and real-time) fashion.</p>

<p>To this end I use ARIMA forecasting as implemented in R, where I retrain the model for every new forecast. Ideally I'd like to retrain the model as little as possible to save computing power, but this is where I get confused. Let's say we use the hypothetical ARIMA(2,0,2) model as follows:</p>

<p>$$
y_t = 0.3 y\prime_{t-1} + 0.4 y\prime_{t-2} + 0.4 e_{t-1} + 0.5 e_{t-2} + c + e_t
$$</p>

<p>Now say I train some model based on measurements from $$t_0 \text{ to } t_{250}$$ If at this point I want to forecast for t = 251, we use the measurements for t-1=250 and t-2=249 in the above equation. Now I want to forecast for t=252. If I use the same approach (in this case use t-1=251 and t-2=250), is this essentially the same as L-step forecasting? And if setting the n.ahead parameter of the R <a href=""https://stat.ethz.ch/R-manual/R-patched/library/stats/html/predict.arima.html"" rel=""nofollow"">predict</a> function, does this actually work in this fashion (using previously predicted values)?</p>

<p>If I were to fit a model, and then use it for say, 2000 forecasts. Would this essentially be L-step forecasting with L=2000? </p>
"
"0.241121411085206","0.254967236863805","153404","<p>I am fairly new to R so my data manipulation experience isn't as strong as it is with other software packages. I have been primarily using the high level functions that others have written. The forecast package written by Hyndman is fantastic and I have also enjoyed reading his book.</p>

<p>What I am struggling with is how to create ARIMA fitted values that are greater than one step forward. The current function will painlessly create fitted values for a one time period projection on historical data. You can use the accuracy function to determine RMSE, MAPE, MASE to test the accuracy of your modelling forecasts. However, I would like to variably test how well the forecast performs for different projections into the future. </p>

<p>For example, I am currently working with weekly time series data and the current goal is to forecast one month into the future. The fitted values and accuracy function show how well my forecast would have performed if I were only projecting 1 week into the future instead of 4. Is there a quick and efficient way to select how many periods into the future we would like to estimate the accuracy for historical data?</p>

<p>Forgive me if I have missed a fundamental aspect of forecasting. I know that I can simply aggregate my weekly time series data into monthly data and then use my ARIMA model to forecast monthly instead of weekly periods. However, since I have multiple years of data, I am largely concerned with seasonality and it would be nice to see how the data ramps up in each week of the month (also would be nice to account for holidays that may affect weekly results). I also realize that I can simply forecast monthly data and then use cubic spline interpolation.</p>

<p>Thank you in advance!</p>
"
"0.139211511597426","0.157720074469128","156202","<p>I am working on a project where I am to do the intervention analysis and forecasting based on the time series. The problem is something like:</p>

<p><em>I have a normal time series entries but in between them some known event like natural calamities (storm, tornado) happens. I have the data for that and it affects the normal time series. Now my objective is to forecast the value of time series both in normal mode and also when I have a prediction of storm coming.</em></p>

<p>I have been reading <a href=""http://rads.stackoverflow.com/amzn/click/0471615285"" rel=""nofollow"">Forecasting with dynamic regression</a> chapter 7 about intervention analysis. I am also reading about the transfer function modeling. Can you please help me as in which model is good for this kind of time series analysis? Or may be some link which can guide me as how to do it? I will appreciate a link with some example in R or some examples.</p>

<p>EDIT: I guess I was not correct in description but I know the exact time information of all the previous storm events and I sort of want to find out the effect of storm intervention on the time series and I can forecast more closely if I know that there is a storm happening right now.</p>
"
"0.206484040339026","0.212669705029557","160716","<p>I have a set of mortality data that I'd like to smooth so that I can run it through a Lee-Carter model for forecasting. The set of data focuses on the cohorts of aged 1-11 people for the years of 2004-2014.</p>

<p>I've tried to run the smooth.demogdata from the demography package in R, but it produces this error: </p>

<pre><code>""Error in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots) : 
A term has fewer unique covariate combinations than specified maximum degrees of freedom""
</code></pre>

<p>I've also tried to run a smooth.spline function on the data and it hasn't worked either.</p>

<p>The data I'm working with are as follows: </p>

<pre><code>US_counts.age
135618, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA
150988, 125213, NA, NA, NA, NA, NA, NA, NA, NA, NA
144797, 144686, 117643, NA, NA, NA, NA, NA, NA, NA, NA
145921, 138953, 136791, 110374, NA, NA, NA, NA, NA, NA, NA
146350, 139452, 131145, 128469, 103897, NA, NA, NA, NA, NA, NA
159301, 139080, 130705, 122500, 120655, 97922, NA, NA, NA, NA, NA
169750, 151355, 130195, 121681, 114789, 113711, 92623, NA, NA, NA, NA
166925, 158914, 142749, 122450, 114941, 108932, 108085, 88116, NA, NA, NA
174177, 158635, 150225, 134504, 116111, 109365, 103898, 103520, 84283, NA, NA
174938, 165078, 149825, 141967, 127656, 110712, 104557, 99706, 99568, 80944, NA
169517, 165777, 155530, 141601, 134922, 121745, 105924, 100205, 95836, 95749, 77716



US_rates.age
0.0034287484, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA
0.0031989297, 0.0036178352, NA, NA, NA, NA, NA, NA, NA, NA, NA
0.0029213312, 0.0032415023, 0.0036636264, NA, NA, NA, NA, NA, NA, NA, NA
0.0022957628, 0.0028930646, 0.0031873442, 0.0035334408, NA, NA, NA, NA, NA, NA, NA
0.0017902289, 0.0023162092, 0.0027298029, 0.0030513198, 0.0033013465, NA, NA, NA, NA, NA, NA
0.0015254142, 0.0017328156, 0.0022187369, 0.0025224490, 0.0028925449, 0.0029717530, NA, NA, NA, NA, NA
0.0011310751, 0.0014733573, 0.0016052844, 0.0020052432, 0.0023434301, 0.0025855018, 0.0027207065, NA, NA, NA, NA
0.0008926164, 0.0011389808, 0.0013730394, 0.0014373214, 0.0018531246, 0.0021389491, 0.0022297266, 0.0024286168, NA, NA, NA
0.0004994919, 0.0008068837, 0.0009452488, 0.0011300779, 0.0012057428, 0.0015818589, 0.0017805925, 0.0019513138, 0.0020882028, NA, NA
0.0003029645, 0.0004422152, 0.0006407475, 0.0007466524, 0.0007676882, 0.0008761471, 0.0012815976, 0.0014442461, 0.0014362044, 0.0016184028, NA
0.0001415787, 0.0002050948, 0.0002829036, 0.0003954774, 0.0003705845, 0.0004517639, 0.0004342736, 0.0006885884, 0.0008138904, 0.0008772938, 0.0009264502
</code></pre>

<p>Note: it's cohort data, which is why there are missing values in the upper right of both matrices (which are supposed to be shown as matrices, but I don't know how to display matrices on this forum).</p>

<p>I've taken these matrices and input them into a demogdata(...) call as follows:</p>

<pre><code>US.demog.age = demogdata(US_rates.age, US_counts.age, ages = 1:11, years = 2004:2014, type = ""mortality"", name=""Total"", label = ""US"")
</code></pre>

<p>When I try to then smooth this data using the smooth.demogdata(...) function as I normally would with a larger set of mortality data, I get the error I displayed above. I read somewhere about manually changing the knot values to fix the error and to be able to actually smooth the data, but my attempts have been unsuccessful thus far. The information I read pertained to the s(...) function of the mgcv package where you could set k to some value under the default 10 to override these issues for smaller data sets, but all of the examples I saw were related to linear mixed effects models, not to a matrix specifically, so I'm at a loss for how I could use that method with matrix data and get the smoothing function to work.</p>

<p>Am I barking up the wrong tree trying to smooth the demography data with the smooth.demogdata call? Is there a better way to smooth the data? I need to be to use the Lee-Carter model in the ilc package.</p>

<p>Thanks</p>
"
"0.0880450906325624","0.0997509336107633","161614","<p>I want to solve the first exercice of the Multiple Regression Chapter of R. Hyndman's online book on Time Series Forecasting (see <a href=""https://www.otexts.org/fpp/5/8"" rel=""nofollow"">https://www.otexts.org/fpp/5/8</a>). I use <code>R</code> with <code>fpp</code> package as wanted in the exercise.</p>

<p>I am blocked in the following question:
c. Use R to fit a regression model to the logarithms of these sales data with a linear trend, seasonal dummies and a â€œsurfing festivalâ€ dummy variable.</p>

<p>Indeed, I don't know how to make the function <code>tslm</code> work with my dummy vector for the surfing festival. Here is my code.</p>

<pre><code>library(fpp)
log_fancy = log(fancy)
dummy_fest_mat = matrix(0, nrow=84, ncol=1)
for(h in 1:84)
    if(h%%12 == 3)   #this loop builds a vector of length 84 with
        dummy_fest_mat[h,1] = 1   #1 corresponding to each month March
dummy_fest_mat[3,1] = 0 #festival started one year later

dummy_fest = ts(dummy_fest_mat, freq = 12, start=c(1987,1))
fit = tslm(log_fancy ~ trend + season + dummy_fest)
</code></pre>

<p>When I do <code>summary(fit)</code>, I see that the regression coefficients have been well calculated, but when I continue with <code>forecast(fit)</code>
I get the following error : </p>

<pre><code>Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
  variables have not equal length (found for 'factor(dummy_fest)')
In addition: Warning message:
'newdata' had 50 rows but variables found have 84 rows 
</code></pre>

<p>But what is even stranger is that when I do <code>forecast(fit, h=84)</code>, it works!!
I don't know what is happening here, can someone explain me?</p>
"
"0.139211511597426","0.126176059575302","166485","<p>I created for the following data set a multiple regression. Now I would like to forecast the next 20 data points.</p>

<pre><code>&gt; dput(datSel)
structure(list(oenb_dependent = c(1.0227039, -5.0683144, 0.6657713, 
3.3161374, -2.1586704, -0.7833623, -0.2203209, 2.416144, -1.7625406, 
-0.1565037, -7.9803936, 9.4594715, -4.8104584, 8.4827107, -6.1895262, 
1.4288595, 1.4896459, -0.4198522, -5.1583964, 5.2502294, 1.0567102, 
-1.0923342, -1.5852298, 0.6061936, -0.3752335, 2.5008664, -1.3999729, 
2.2802166, -2.1468756, -1.4890328, -0.79254376, 3.21804705, -0.94407886, 
-0.27802316, -0.20753079, -1.12610048, 2.0883735, -0.7424854, 
0.44203729, -1.48905938, 1.39644424, -3.8917377, 11.25665848, 
-9.22884035, 3.26856762, -0.00179541, -2.39664325, 4.00455574, 
-5.60891295, 4.6556348, -4.40536951, 6.64234497, -7.34787319, 
7.56303006, -8.23083674, 4.43247855, 1.31090412), carReg = c(0.73435946, 
0.24001161, 16.90532537, -14.60281976, 6.47603166, -8.35815849, 
3.55576685, 7.10705794, -4.6955223, 10.9623709, 5.5801857, -6.4499936, 
-9.46196502, 9.36289122, -8.52630424, 5.45070994, -4.5346405, 
-2.26716538, 2.56870398, 0.013737, 5.7750101, -27.1060826, 1.08977179, 
4.94934712, 17.55391859, -13.91160577, 10.38981128, -11.81349246, 
-0.0831467, 2.79748237, 1.84865463, -1.98736934, -6.24191695, 
13.33602659, -3.86527871, 0.78720993, 4.73360651, -4.1674034, 
9.37426802, -5.90660464, -0.4915792, -5.84811629, 9.67648643, 
-6.96872719, -7.6535767, 0.24847595, 0.18685263, -2.28766949, 
1.1544631, -3.87636933, -2.4731545, 4.33876671, 1.08836339, 5.64525271, 
1.90743854, -3.94709355, -0.84611324), cpi = c(1.16, -3.26, 0.22, 
-3.51, 0.84, -2.81, -0.34, -4.57, -0.12, -3.95, -1.37, -2.73, 
0.35, -5.38, -4.43, -3.08, 0.74, -3.03, -1.09, -2, 0.35, -1.52, 
1.28, 0.2, -0.25, -4.55, -2.49, -4.24, -0.31, -2.96, -2.24, -0.46, 
-0.06, -2.67, -1.27, -1.4, -0.7, -0.96, -2.18, -2.53, -0.52, 
-1.74, -2.18, -1.4, -0.34, -0.09, -1.65, -1.15, -0.17, -2.01, 
-1.38, -1.24, 0.09, -2.44, -1.92, -2.61, -0.34), primConstTot = c(-0.33334, 
-0.93333, -0.16667, -0.33333, -0.16667, -0.86666, -0.3, -0.4, 
-0.26667, -1.56667, -0.73333, 0.1, -0.23333, -0.26667, -1.5774, 
-0.19284, 0.38568, -2.42423, -0.93663, 0.08265, -0.63361, 0.0551, 
-0.49587, 2.39668, -1.70798, -3.36085, -2.56196, 0.16529, 0, 
-1.84572, -1.3774, -0.49586, -1.70798, -1.90081, -0.55096, -0.77134, 
-0.16529, -0.30303, -0.17066, -0.23853, -0.64401, -1.52657, -1.57426, 
-0.28623, -0.54861, -1.07336, -0.71558, 0.02385, -0.38164, -1.09721, 
0, 0.14311, -0.38164, -1.02566, -0.42934, -0.35779, -0.4532), 
    resProp.Dwell = c(0.8, -4, -3.2, 2.7, -1.6, -1, -2.4, -0.4, 
    -0.8, 1, -12.1, 0.2, -5.2, 3.7, -2.7, -1.7, 1.5, 0.7, -7.9, 
    0.3, 0.3, 1.4, -3.3, -1, -1.6, 1.5, 0.5, 1.5, -1, -2.2, -3.5, 
    0.5, 0.5, -0.9, -0.4, -3.4, 0.9, 0.1, -0.2, -2.8, -0.8, -6.2, 
    11.3, -4.6, 1, 1.1, -1.7, 4.1, -5, 2.3, -2.3, 4.6, -6.3, 
    6.3, -6.9, 0, 2.4), cbre.office.primeYield = c(0, 0, 0.15, 
    0.15, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.2, 0.15, 0.1, 
    0.05, 0.15, 0.3, 0.35, 0.4, 0.3, 0.2, 0, -0.15, -0.85, -1, 
    -0.85, -0.75, -0.1, 0, 0, 0, 0.05, 0.05, 0.05, 0.05, 0, 0, 
    0, 0.2, 0.2, 0.2, 0.2, 0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25, 
    0, 0, 0, 0, 0, 0, 0), cbre.retail.capitalValue = c(-1882.35294, 
    230.76923, -230.76923, -226.41509, -670.78117, -436.13707, 
    -222.22223, 0, -205.91233, -202.16847, 0, -393.5065, -403.91909, 
    -186.30647, -539.81107, -748.11463, -764.70588, -311.47541, 
    -301.42782, -627.09677, -480, 720, 782.6087, 645.96273, 251.42857, 
    1386.66667, -533.33334, -533.33333, -533.33333, 0, 0, -1024.56141, 
    -192.10526, 0, -730, 0, 0, 0, 0, 0, -834.28571, 0, -1450.93168, 
    0, 0, 0, -700.78261, 0, 0, 0, 0, 0, 0, 0, -1452, 0, 0)), .Names = c(""oenb_dependent"", 
""carReg"", ""cpi"", ""primConstTot"", ""resProp.Dwell"", ""cbre.office.primeYield"", 
""cbre.retail.capitalValue""), row.names = c(NA, -57L), class = ""data.frame"")
&gt; 
&gt; fit &lt;- lm(oenb_dependent ~ carReg + cpi + primConstTot + 
+             resProp.Dwell + cbre.office.primeYield + cbre.retail.capitalValue , data = datSel)
&gt; summary(fit) # show results

Call:
lm(formula = oenb_dependent ~ carReg + cpi + primConstTot + resProp.Dwell + 
    cbre.office.primeYield + cbre.retail.capitalValue, data = datSel)

Residuals:
   Min     1Q Median     3Q    Max 
-5.166 -1.447 -0.162  1.448  7.903 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               0.831630   0.492297    1.69    0.097 .  
carReg                    0.085208   0.039600    2.15    0.036 *  
cpi                      -0.349192   0.212044   -1.65    0.106    
primConstTot              0.752772   0.383810    1.96    0.055 .  
resProp.Dwell             0.994356   0.086812   11.45  1.4e-15 ***
cbre.office.primeYield    1.274734   1.212782    1.05    0.298    
cbre.retail.capitalValue  0.000528   0.000643    0.82    0.416    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.24 on 50 degrees of freedom
Multiple R-squared:  0.754, Adjusted R-squared:  0.725 
F-statistic: 25.6 on 6 and 50 DF,  p-value: 1.2e-13
</code></pre>

<p>I tried the following:</p>

<pre><code>vals.multipleRegr &lt;- forecast(fit, h = 20)
Error: could not find function ""forecast""
</code></pre>

<p>However, this does not work as the function forecast cannot be found. I am using the following packages in my code, <code>library(bootstrap)</code>, <code>library(DAAG)</code> and <code>library(relaimpo)</code>. </p>

<p>Any suggestion how to forecasting using multiple regression?</p>

<p>I appreciate your replies!</p>
"
"0.127082141943837","0.172773685116272","166953","<p><strong>Issue</strong>: Cannot forecast sales accurately using quantile regression in R. I am using rq function from ""quantreg"" package which is giving me warning ""Result might have Non unique solutions""</p>

<p><strong>Aim</strong>: I am trying to forecast hourly sales of a store using quantile regression. </p>

<p>Below are the columns in my source table for forecasting.</p>

<ul>
<li><em>transaction_date</em> : sales date (input)</li>
<li><em>hr1 to hr24</em> : column with hourly sales info. (24 columns) (input)</li>
<li><em>totala</em> : total of 24 column hr1 to hr24 (not using currently)</li>
<li><em>location, department, sales_type</em>: forecasting will be done for each location, sales_type and department. (used to select data)</li>
<li><em>f1 to f24 :</em> columns I want to forecast for each hour (24 columns) (output)</li>
</ul>

<p>Packages Used: forecast, quantreg, Metrics</p>

<p><strong>Code</strong>: 
I have extracted date features from transaction_date eg. weekend, week of month and also holidays (1 if it is holiday 0 for regular days).</p>

<pre><code>attach(train_data) 
Y &lt;- cbind(hr) 
X &lt;- cbind(transation_date, Years, Months, Days, WeekDay, WeekofYear, Weekend, WeekofMonth, holidays) 

quantreg.all &lt;- rq(Y ~ X, tau = seq(0.05, 0.95, by = 0.05))
prediction_train &lt;- data.frame(predict(quantreg.all))
</code></pre>

<p>I have 19 models in prediction_train for each tau from 0.05 to 0.95, I select best model based on rmse value and than forecast using that tau.</p>

<pre><code>rmse(actual, predicted)
</code></pre>

<p>transaction_date is Date type, quantreg.all is rqs class and rest are numeric.</p>

<p><strong>Note:</strong> Stores are not open 24 hours, hence many hour columns will be 0 (time when store was close). Currently for most of such hours rq is predicting 0 or some negative values.</p>

<p>Weather  does not have major impact on sales.</p>
"
"0.27137319479402","0.242726073141876","168655","<p>I have got monthly data from 1993 to 2015 and would like to do forecasting on these data. I used tsoutliers package to detect the outliers, but I do not know how do I continue to forecast with my set of data .</p>

<p>This is my code:</p>

<pre><code>product.outlier&lt;-tso(product,types=c(""AO"",""LS"",""TC""))
plot(product.outlier)
</code></pre>

<p>This is my output from tsoutliers package</p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p><a href=""http://i.stack.imgur.com/qKI4N.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qKI4N.jpg"" alt=""This is my plot""></a></p>

<p>I have these warning messages as well.</p>

<pre><code>Warning messages:
1: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
2: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
3: In locate.outliers.oloop(y = y, fit = fit, types = types, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
4: In arima(x, order = c(1, d, 0), xreg = xreg) :
  possible convergence problem: optim gave code = 1
5: In auto.arima(x = c(5.77, 5.79, 5.79, 5.79, 5.79, 5.79, 5.78, 5.78,  :
  Unable to fit final model using maximum likelihood. AIC value approximated
</code></pre>

<p><strong>Doubts:</strong></p>

<ol>
<li>If I am not wrong, tsoutliers package will remove the outliers it detect and through the use of the dataset with outliers removed, it
will give us the best arima model suited for the data set, is it
correct?</li>
<li>The adjust series data set is being shifted down by a lot due to remove of the level shift,etc. Doesn't this mean that if the forecasting is done on the adjusted series, the output of the forecast will be very inaccurate, since the more recent data are already more than 12, while adjusted data shift it to around 7-8.</li>
<li>What does warning message 4 and 5 means? Does it mean it cannot do auto.arima using the adjusted series?</li>
<li>What does the [12] in ARIMA(0,1,0)(0,0,1)[12] mean? Is it just my frequency/periodicity of my dataset, which I set it to monthly? And does this also means that my data series is seasonal as well? </li>
<li>How do I detect seasonality in my data set? As from the visualisation of the time series plot, I cant see any obvious trend, and if I use the decompose function, it will assume that there is a seasonal trend? So do I just believe what the tsoutliers tell me, where there is seasonal trend, since there is MA of order 1?</li>
<li>How do I continue to do my forecasting with this data after identifying these outliers?</li>
<li><strong>How to incorporate these outliers to other forecasting models - Exponential Smoothing, ARIMA, Strutural Model, Random Walk, theta? I am sure I cannot remove the outliers since there are level shift, and if I only take adjusted series data, the values will be too small, so what do I do?</strong></li>
</ol>

<p><strong>Do I need to add these outliers as regressor in the auto.arima for forecasting? How does this work then?</strong></p>
"
"0.27137319479402","0.291271287770251","169468","<p>I have monthly time series data, and would like to do forecasting with detection of outliers .</p>

<p><strong>This is the sample of my data set:</strong></p>

<pre><code>       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
2006  7.55  7.63  7.62  7.50  7.47  7.53  7.55  7.47  7.65  7.72  7.78  7.81
2007  7.71  7.67  7.85  7.82  7.91  7.91  8.00  7.82  7.90  7.93  7.99  7.93
2008  8.46  8.48  9.03  9.43 11.58 12.19 12.23 11.98 12.26 12.31 12.13 11.99
2009 11.51 11.75 11.87 11.91 11.87 11.69 11.66 11.23 11.37 11.71 11.88 11.93
2010 11.99 11.84 12.33 12.55 12.58 12.67 12.57 12.35 12.30 12.67 12.71 12.63
2011 12.60 12.41 12.68 12.48 12.50 12.30 12.39 12.16 12.38 12.36 12.52 12.63
</code></pre>

<p>I have referred to <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r?lq=1"">Timeseries analysis procedure and methods using R</a>, to do a series of different model of forecasting, however it does not seems to be accurate. In additional, I am not sure how to incorporate the tsoutliers into it as well.</p>

<p>I have got another post regarding my enquiry of tsoutliers and arima modelling and procedure over <a href=""http://stats.stackexchange.com/questions/168655/how-to-interpret-and-do-forecasting-using-tsoutliers-package-and-auto-arima/168869#168869"">here</a> as well.</p>

<p>So these are my code currently, which is similar to link no.1.</p>

<p><strong>Code:</strong></p>

<pre><code>product&lt;-ts(product, start=c(1993,1),frequency=12)

#Modelling product Retail Price

#Training set
product.mod&lt;-window(product,end=c(2012,12))
#Test set
product.test&lt;-window(product,start=c(2013,1))
#Range of time of test set
period&lt;-(end(product.test)[1]-start(product.test)[1])*12 + #No of month * no. of yr
(end(product.test)[2]-start(product.test)[2]+1) #No of months
#Model using different method
#arima, expo smooth, theta, random walk, structural time series
models&lt;-list(
#arima
product.arima&lt;-forecast(auto.arima(product.mod),h=period),
#exp smoothing
product.ets&lt;-forecast(ets(product.mod),h=period),
#theta
product.tht&lt;-thetaf(product.mod,h=period),
#random walk
product.rwf&lt;-rwf(product.mod,h=period),
#Structts
product.struc&lt;-forecast(StructTS(product.mod),h=period)
)

##Compare the training set forecast with test set
par(mfrow=c(2, 3))
for (f in models){
    plot(f)
    lines(product.test,col='red')
}

##To see its accuracy on its Test set, 
#as training set would be ""accurate"" in the first place
acc.test&lt;-lapply(models, function(f){
    accuracy(f, product.test)[2,]
})
acc.test &lt;- Reduce(rbind, acc.test)
row.names(acc.test)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.test &lt;- acc.test[order(acc.test[,'MASE']),]

##Look at training set to see if there are overfitting of the forecasting
##on training set
acc.train&lt;-lapply(models, function(f){
    accuracy(f, product.test)[1,]
})
acc.train &lt;- Reduce(rbind, acc.train)
row.names(acc.train)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.train &lt;- acc.train[order(acc.train[,'MASE']),]

 ##Note that we look at MAE, MAPE or MASE value. The lower the better the fit.
</code></pre>

<p>This is the plot of my different forecasting, which doesn't seem very reliable/accurate, through the comparison of the red""test set"", and blue""forecasted"" set.
<strong>Plot of different forecast</strong>
<a href=""http://i.stack.imgur.com/WZSNq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WZSNq.jpg"" alt=""Different forecast""></a></p>

<p><strong>Different accuracy of the respective models of test and training set</strong></p>

<pre><code>Test set
                    ME      RMSE       MAE        MPE     MAPE      MASE      ACF1 Theil's U
theta      -0.07408833 0.2277015 0.1881167 -0.6037191 1.460549 0.2944165 0.1956893 0.8322151
expsmooth  -0.12237967 0.2681452 0.2268248 -0.9823104 1.765287 0.3549976 0.3432275 0.9847223
randomwalk  0.11965517 0.2916008 0.2362069  0.8823040 1.807434 0.3696813 0.4529428 1.0626775
arima      -0.32556886 0.3943527 0.3255689 -2.5326397 2.532640 0.5095394 0.2076844 1.4452932
struc      -0.39735804 0.4573140 0.3973580 -3.0794740 3.079474 0.6218948 0.3841505 1.6767075

Training set
                     ME      RMSE       MAE         MPE     MAPE      MASE    ACF1 Theil's U
theta      2.934494e-02 0.2101747 0.1046614  0.30793753 1.143115 0.1638029  0.2191889194        NA
randomwalk 2.953975e-02 0.2106058 0.1050209  0.31049479 1.146559 0.1643655  0.2190857676        NA
expsmooth  1.277048e-02 0.2037005 0.1078265  0.14375355 1.176651 0.1687565 -0.0007393747        NA
arima      4.001011e-05 0.2006623 0.1079862 -0.03405395 1.192417 0.1690063 -0.0091275716        NA
struc      5.011615e-03 1.0068396 0.5520857  0.18206018 5.989414 0.8640550  0.1499843508        NA
</code></pre>

<p>From the models accuracy, we can see that the most accurate model would be theta model.
I am not sure why is the forecast very inaccurate, and I think that one of the reasons would be that, I did not treat the ""outliers"" in my data set, resulting in a bad forecast for all model.</p>

<p><strong>This is my outliers plot</strong></p>

<p><strong>Outliers Plot</strong>
<a href=""http://i.stack.imgur.com/bZDQv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bZDQv.jpg"" alt=""Outliers""></a></p>

<p><strong>tsoutliers output</strong></p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p>I would like to know how can I further ""analyse""/forecast my data, with these relevant data set and detection of outliers, etc.
Please do help me in treatment of my outliers as well to do my forecasting as well . </p>

<p>Lastly, I would like to know how to combined the different model forecasting together, as from what @forecaster had mentioned in link no.1, combining the different model will most likely result in a better forecasting/prediction.</p>

<p><strong>EDITED</strong></p>

<p>I would like to incorporate the outliers in other models are well.</p>

<p>I have tried some codes, eg. </p>

<pre><code>forecast.ets( res$fit ,h=period,xreg=newxreg)
    Error in if (object$components[1] == ""A"" &amp; is.element(object$components[2], : argument is of length zero

forecast.StructTS(res$fit,h=period,xreg=newxreg)
Error in predict.Arima(object, n.ahead = h) : 'xreg' and 'newxreg' have different numbers of columns
</code></pre>

<p>There are some errors produced, and I am unsure about the correct code to incorporate the outliers as regressors.
Furthermore, how do I work with thetaf or rwf, as there are no forecast.theta or forecast.rwf?</p>
"
"0.107832773203438","0.0814462962375368","169564","<p>The <code>arimax</code> function in the <code>TSA</code> package is to my knowledge the only <code>R</code> package that will fit a transfer function for intervention models. It lacks a <a href=""http://stats.stackexchange.com/questions/34106/forecasting-with-arimax-model-including-xtransf"">predict function</a> though which is sometimes needed.</p>

<p>Is the following a work-around for this issue, leveraging the excellent <code>forecast</code> package? Will the predictive intervals be correct? In my example, the std errors are ""close"" for the components.</p>

<ol>
<li>Use the forecast package arima function to determine the pre-intervention noise series and add any outlier adjustment.</li>
<li>Fit the same model in <code>arimax</code> but add the transfer function</li>
<li>Take the fitted values for the transfer function (coefficients from <code>arimax</code>) and add them as xreg in <code>arima</code>. </li>
<li>Forecast with <code>arima</code></li>
</ol>

<blockquote>
<pre><code>library(TSA)
library(forecast)
data(airmiles)
air.m1&lt;-arimax(log(airmiles),order=c(0,0,1),
              xtransf=data.frame(I911=1*(seq(airmiles)==69)),
              transfer=list(c(1,0))
              )
</code></pre>
  
  <p>air.m1</p>
</blockquote>

<p>Output:</p>

<pre><code>Coefficients:
  ma1  intercept  I911-AR1  I911-MA0
0.5197    17.5172    0.5521   -0.4937
s.e.  0.0798     0.0165    0.2273    0.1103

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.09   BIC=-155.02
</code></pre>

<p>This is the filter, extended out 5 more periods that the data</p>

<pre><code>tf&lt;-filter(1*(seq(1:(length(airmiles)+5))==69),filter=0.5521330,method='recursive',side=1)*(-0.4936508)
forecast.arima&lt;-Arima(log(airmiles),order=c(0,0,1),xreg=tf[1:(length(tf)-5)])
forecast.arima
</code></pre>

<p>Output:</p>

<pre><code>Coefficients:
         ma1  intercept  tf[1:(length(tf) - 5)]
      0.5197    17.5173                  1.0000
s.e.  0.0792     0.0159                  0.2183

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.28   BIC=-157.74
</code></pre>

<p>Then to Predict</p>

<pre><code>predict(forecast.arima,n.ahead = 5, newxreg=tf[114:length(tf)])
</code></pre>
"
"0.127082141943837","0.172773685116272","172550","<p>I want to forecast time-series data using the forecast package methods, but with holidays as dummy variables, as in the following:
<a href=""http://www.r-bloggers.com/forecasting-with-daily-data/"" rel=""nofollow"">http://www.r-bloggers.com/forecasting-with-daily-data/</a>
(see also:)
<a href=""http://stats.stackexchange.com/questions/92743/forecasting-with-holiday-dummy-variables"">Forecasting with holiday dummy variables</a>
I wanted to get the code for finding public holiday dates automatically, so I don't need to upload my data (which the StackExchange user had to do).</p>

<p>The function <a href=""http://www.inside-r.org/packages/cran/forecast/docs/bizdays"" rel=""nofollow"">bizdays</a> can count the number of ""business days"" in a month/or quarter. But its usage example is </p>

<pre><code>bizdays(wineind, FinCenter = ""Sydney"")
</code></pre>

<p>I looked at the source for bizdays, and in  particular the lines:</p>

<pre><code>days.len &lt;- as.timeDate(seq(start, end, by = ""days""), 
                    FinCenter = FinCenter)
biz &lt;- days.len[isBizday(days.len, holidays = unique(format(days.len, 
                                                        ""%Y"")))] 
</code></pre>

<p>However, when I applied the second line to a day.len sequence of dates, it returned the dates from day.len, minus weekends. It did not eliminate Sydney public holidays, as I would expect.</p>

<p>So my question is, what is the point of specifying ""FinCenter"" parameter in biz days, if the function just returns generic 5-day week sequence of dates. How does the FinCenter impact the function?</p>

<p>Also, is there any way of automatically retrieving public holidays for a given financial center, or do I have to load it myself?</p>

<p>Am I better off removing the holidays for e.g. stock exchange data (as weekends are removed), before the forecast? Or is it better to model the public holiday as an extra dummy variable? (As in the example). </p>

<p>I am assuming daily data in this question. </p>
"
"0.164717281867254","0.186616908791155","173042","<p>I am new to R and forecasting. I have access to weekly data (104 weeks) for a certain SKU, its value and volume sales and a few promo variables.</p>

<p>Promo 1 and Promo 2 are continuous variables (unfortunately Promo 1 is 0 here for this SKU) while Promo 3 and Promo 4 are categorical variables.</p>

<p>I tried forecasting the volume sales for this SKU for the next 72 weeks. I included dummy variables using <code>seasonaldummy</code> function</p>

<pre><code>actual_vol = ts(data$Volume , frequency =52)
    dummy = seasonaldummy(actual_vol)
    xreg = cbind(data$Promo1 , data$Promo2 , data$Promo3 , data$Promo4 , dummy)

fit = auto.arima(actual_vol , xreg = xreg)
</code></pre>

<p>I am trying to forecast sales for the next 72 weeks by keeping my promo variables as 0 (basically baseline sales). I used <code>seasonaldummyf</code> and promo variables as 0 for forecast.</p>

<p>The plot looks something like this
<a href=""http://i.stack.imgur.com/tdsuO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tdsuO.png"" alt=""Forecast - Arima""></a></p>

<p>As you can see the forecast looks exactly the same as the previous data (same as using <code>snaive</code>) and it seems promo had no effect at all on volume sales.</p>

<p>Kindly let me know if the method is correct and if not how can I improve it.</p>

<p><a href=""https://drive.google.com/file/d/0B6sOv1da0JMeVHl1SlRMZmJDODQ/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B6sOv1da0JMeVHl1SlRMZmJDODQ/view?usp=sharing</a></p>
"
"0.335265716671409","0.340546420938265","176129","<p>I've been working on some various time series forecasts and I've begun to notice a trend (pardon the pun) in my analyses. For about 5-7 datasets that I've worked with so far, it would be helpful to allow for multiple seasonal periods along with an option for holiday dummies. I've tried various methods and usually stick with <code>tbats</code> since <code>auto.arima()</code> with regressors has been giving me issues. By this point, it's probably obvious I'm working in R.</p>

<p>Before I get too far, let me give some sample data. Hopefully the following link works: <a href=""https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0"" rel=""nofollow"">https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0</a>.</p>

<p>This data yields the following time series plot:
<a href=""http://i.stack.imgur.com/FYS1x.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FYS1x.jpg"" alt=""Time Series Plot""></a>
The large dips are around Christmas and New Years, however there are also smaller dips around Thanksgiving. In the code below, I name this dataset <code>traindata</code>.</p>

<p>Now, <code>ets</code> and ""plain"" <code>auto.arima</code> don't look so hot in the long run since they are limited to only one seasonal period (I choose weekly). However for my test set that I held out they performed fairly well for the month's worth of data (with the exception of Labor Day weekend). This being said, forecasting out for a year would be ideal.</p>

<p>I next tried <code>tbats</code> with weekly and yearly seasonal periods. That results in the following forecast:
<a href=""http://i.stack.imgur.com/kcXmd.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kcXmd.jpg"" alt=""TBATS Forecast""></a></p>

<p>Now this looks pretty good. From the naked eye it looks great at taking into account the weekly and yearly seasonal periods as well as Christmas and New Years effects (since they obviously fall on the same dates each year). It would be best if I could include the holidays (and the days around them) as dummy variables. Hence my attempts at <code>auto.arima</code> with <code>xreg</code> regressors.</p>

<p>For ARIMA with regressors, I've followed Dr. Hyndman's suggestions for the fourier function (given here: <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) as well as his selection of the number of fourier terms (given here: <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/forecasting-weekly-data/</a>)</p>

<p>My code is as follows:</p>

<pre><code>fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep=""""),period,sep=""_"")
  return(X)
}

fcdaysout&lt;-365
m1&lt;-7
m2&lt;-30.4375
m3&lt;-365.25

hol&lt;-cbind(traindata$CPY_HOL, traindata$DAY_BEFORE_CPY_HOL, traindata$DAY_AFTER_CPY_HOL)
hol&lt;-as.matrix(hol)

n &lt;- nrow(traindata)
bestfit &lt;- list(aicc=Inf)
bestk &lt;- 0

for(i in 1:m1)
{
    fake_xreg = cbind(fourier(1:n,i,m1), fourier(1:n,i,m3), hol)
    fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = fake_xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
    	if(fit$aicc &lt; bestfit$aicc)
    {
        bestfit &lt;- fit
        bestk &lt;- i
    }
    else
    {
    }
}

k &lt;- bestk
k
##k&lt;-3

xreg&lt;-cbind(fourier(1:n,k,m1), fourier(1:n,k,m3), hol)
xreg&lt;-as.matrix(xreg)

aacov_fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aic"", allowdrift=TRUE)
summary(aacov_fit)
</code></pre>

<p>Where my issues come in is inside the for loop to determine the <code>k</code>, the number of fourier terms, that minimizes AIC. In all of my attempts at ARIMA with regressors, it always produces an error when <code>k&gt;3</code> (or <code>i&gt;3</code> if we're talking about inside my loop). The error being <code>Error in solve.default(res$hessian * n.used, A) : system is computationally singular: reciprocal condition number = 1.39139e-34</code>. Simply setting <code>k=3</code> gives some decent results for my test set but for the next year it doesn't appear to adequately catch the steep drops around the end of the year and is much smoother than imagined as evidenced in this forecast:<a href=""http://i.stack.imgur.com/rj30h.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rj30h.jpg"" alt=""AutoArima with Covariates (k=3)""></a></p>

<p>I assume this general smoothness is due to the small number of fourier pairs. Is there an oversight in my code in that I'm just royally screwing up the procedure provided by Dr. Hyndman? Or is there a theoretical issue that I'm unknowingly running into by trying to find more than 3 pairs of fourier terms for the multiple seasons I'm attempting to account for? Is there a better way to include the multiple seasonalities and dummy variables?</p>

<p>Any help in getting these covariates into the arima model with an appropriate number of fourier terms would be appreciated. If not, I'd at least like to know whether or not what I'm attempting is possible in general with larger number of fourier pairs.</p>
"
"0.139211511597426","0.0946320446814768","178787","<p>Im really new in regression estimation but my problem here is about forecasting confrontation. </p>

<p>This is my model:</p>

<p>$Y_t = \beta_0 + \beta_1 X_t + \epsilon_t$ </p>

<p>My OLS estimation using r function ""lm"" was:</p>

<pre><code>set.seed(123)
data &lt;- matrix(rnorm(50*2),nrow=50)
m &lt;- data.frame(data )


Model1&lt;- lm(X1 ~ X2 -1 , data = m)
&gt; Modelo1$coef
        X2 
-0.0296194 
</code></pre>

<p>My Quantile Regression (Median, $\tau = 0.5$) was:</p>

<pre><code>&gt; ModeloRQ1&lt;-rq(X1 ~ X2 -1, tau = 0.5,method=""br"", data=m) 

&gt; ModeloRQ1$coef
        X2 
-0.1256418 
</code></pre>

<p>The estimation procedure i understand. </p>

<p>But the Forecasting Procedure i dont understand. 
I know that after making the forecast i should compare using RMSFE statistics, for example.</p>

<p>But when i use the ""forecast"" function gives me the same point forecast (same values) when i use ""predict"" function.</p>

<p>I have read some papers which do not detail this procedure. Only say that ""OLS and QR (0.5) forecasts are Confronted against each other"". </p>

<p>How should i do this procedure? Simply by using the function predict/forecasting? this would be a commonly used procedure?</p>
"
"0.062257280636469","0.0705345615858598","181158","<p>I'm having trouble forecasting a time series with a trigonometric component using dlmModTrig.</p>

<p>So far I have:
buildFun&lt;-function(x){</p>

<p>dlmInven&lt;- dlmModTrig (s=12, dV=0, q=2, dW=exp (x<a href=""http://i.stack.imgur.com/y9LWs.png"" rel=""nofollow"">1</a>))+dlmModPoly(order=2, dV=exp(x[2]), dW=c(exp(x[3]),exp(x[4])))
    return(dlmInven)
}</p>

<p>And the convergence is 0 after I check </p>

<p>(fit&lt;-dlmMLE(lInven, parm=rep(0,4), build=buildFun))$conv.</p>

<p>However, the predicted graph is very far off. In fact, even the smoothed portion is far off:</p>

<p><a href=""http://i.stack.imgur.com/y9LWs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y9LWs.png"" alt=""enter image description here""></a></p>

<p>If I do the dlmModTrig and the dlmModPoly individually. The fits are  better.</p>

<p>Any idea why this is happening? </p>
"
"0.062257280636469","0","185816","<p>I am trying to understand how the ets function of R in the forecast package computes initial level $l_0$, and initial trend $b_0$. I was under the impression that they are set to the intercept and slope of the whole data, but it seems it's not true. I would appreciate any explanation on this.</p>

<p>Also, Excel 2016 introduced <a href=""https://support.office.com/en-us/article/Forecasting-functions-897a2fe9-6595-4680-a0b0-93e0308d5f6e"" rel=""nofollow"">FORECAST.ETS</a> function, whose results don't seem to match those produced by R's ets function. What would be possible explanations for the discrepancies in the results.  </p>

<p>Another thing is that when I apply ets on <a href=""http://homepage.stat.uiowa.edu/~kchan/TSA/Datasets/airpass.dat"" rel=""nofollow"">airpass</a> dataset, it doesn't detect seasonality period (12) correctly. The model it picks is ETS(M,A,N), and when I force it to use model=""AAA"" it shows the following error message: ""Error in ets(airpass, model = ""AAA"") : Nonseasonal data.""</p>

<p>Any help is really appreciated.</p>
"
"0.0880450906325624","0.0997509336107633","186190","<p>I am trying to make a prediction of imbalance prices in the elctricity market. My dataset consists of data for every 15 minutes (this is the time period in which a price is determined) during 11 months. I have several exogenous factors (like the spot market price) included mentioned here as x1 etc.</p>

<p>In forecasting the price I am using the following code: </p>

<pre><code>lag &lt;- function(x, k){c(rep(NA, k), x)[1 : length(x)]}
mydata$y_lag1 &lt;- lag(mydata$y, 1)
mydata$y_lag2 &lt;- lag(mydata$y, 2)
mydata$x1_lag1 &lt;- lag(mydata$x1, 1)
mydata$x2_lag1 &lt;- lag(mydata$x2, 1)
mydata$x3_lag1 &lt;- lag(mydata$x3, 1

f&lt;- y ~ y_lag1 + y_lag2 + x1_lag1 + x2_lag1 + x3_lag1
fit &lt;- lm(formula = f, data = mydata)
mydata$P_imb_pred &lt;- predict(fit, newdata = mydata)

pred &lt;- data.frame(time=mydata$time, price=mydata$P_imb_pred)
</code></pre>

<p>My code works, but I am unsure if it does wat I want it to. I am trying to predict the price only 1 time unit (so 15 minutes) ahead. That's why I have lagged variables in the function. Can someone help me out? Should I additionally specify how much time ahead I want to forecast? If so, can you tell me how?</p>

<p>Thanks for your help! </p>
"
"0.164717281867254","0.133297791993682","186505","<p>I am working on project to forecast sales of stores to learn forecasting. Until now I have successfully used simple <code>auto.arima</code> function for forecasting. But to make these forecast more accurate I can make use of covariates. I have defined covariates like holidays, promotion which affect on sales of store using <code>xreg</code> argument with the help of this post:
<a href=""http://stats.stackexchange.com/questions/41070/how-to-setup-xreg-argument-in-auto-arima-in-r"">How to setup xreg argument in auto.arima() in R?</a></p>

<p>But my code fails at line:</p>

<pre><code>ARIMAfit &lt;- auto.arima(saledata, xreg=covariates)
</code></pre>

<p>and gives error saying:</p>

<pre><code>Error in model.frame.default(formula = x ~ xreg, drop.unused.levels = TRUE) : 
  variable lengths differ (found for 'xreg')
In addition: Warning message:
In !is.na(x) &amp; !is.na(rowSums(xreg)) :
  longer object length is not a multiple of shorter object length
</code></pre>

<p>Below is link to my Dataset:
<a href=""https://drive.google.com/file/d/0B-KJYBgmb044blZGSWhHNEoxaHM/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B-KJYBgmb044blZGSWhHNEoxaHM/view?usp=sharing</a></p>

<p>This is my code:</p>

<pre><code>data = read.csv(""xdata.csv"")[1:96,]
View(data)

saledata &lt;- ts(data[1:96,4],start=1, end=96,frequency =7 )
View(saledata)

saledata[saledata == 0] &lt;- 1
View(saledata)

covariates = cbind(DayOfWeek=model.matrix(~as.factor(data$DayOfWeek)),
                 Customers=data$Customers,
             Open=data$Open,
                 Promo=data$Promo,
             SchoolHoliday=data$SchoolHoliday)
View(head(covariates))


# Remove intercept
covariates &lt;- covariates[,-1]
View(covariates)

require(forecast)
ARIMAfit &lt;- auto.arima(saledata, xreg=covariates)//HERE IS ERROR LINE
summary(ARIMAfit)
</code></pre>

<p>Also tell me how I can forecast for the next 48 days. I know how to forecast using simple <code>auto.arima</code> using the argument <code>n.ahead</code> but I don't know how to do it when the argument <code>xreg</code> is used.</p>
"
"0.206484040339026","0.233936675532512","187870","<p>I'm working on a sales forecasting package which should be easy to use for the end user. Given a time series with historical sales data I would like to automatically select one of the three forecasts: Auto.Arima, ETS and STLF. 
The idea is to split historical data into 80% train set and 20% test (holdout) set. Then run Auto.Arima, ETS, STLF and choose the one that has best MAPE on the test set. </p>

<p>Now comes the part that is not entirely clear to me. Once I figured out that e.g. ETS gives me the best result should I now </p>

<ol>
<li>Retrain ETS on the entire set of historical data and generate
forecast using this new model? My reservation here is that after I
run ETS again it may even change the class of the algorithm as well
as the fit parameters which will render the MAPE I got on the test
set irrelevant.  </li>
<li>Just generate the forecast using the model that was trained on the
80% train set? My problem with this approach is that we are ignoring
the last 20% of data which is probably the most important
information for the forecast.</li>
<li>The third idea is to use the same model fit parameters that we got
after training the model on the 80% train set. But then use the
entire set of data for        forecasting. This seems like a
reasonable approach but I cannot figure out how to do it for ETS and
STL (For Arima we can do it by supplying the original fit as the model
parameter of the arima function)</li>
</ol>

<p>Could you please let me know what is the right way to approach this problem?</p>
"
"0.206484040339026","0.233936675532512","189983","<p>I have daily data from last 2 years.</p>

<p>I want to do ARIMAX and the regressor component being autoregressive distributed lag of the same variable. Since it has impact, along with dummy variables to account for seasonality in the <code>xreg</code> paratemer in <code>auto.arima</code> function.</p>

<p>The challenge i am facing is predicting my predictor for future. For example, i used daily data for 2 year for model building. For forecasting into future, i also need values of lag variable, which i do not know. If i use 2 lags of daily data in the model, then in order to predict for future i will also need value of those lag variables as well. So to predict $Value$ at time $t$ i will need $Value$ at $t-1$ and $t-2$ which i have from past records. However, if i want to find value at $t+5$ then i will need to find $t+3$ and $t+4$. Not sure how to proceed in this direction. As stated earlier, i am using <code>auto.arima</code> function from <code>forecast</code> package in <code>R</code> . </p>

<p>My ultimate goal is to predict for next 365 days. What i assume to be a solution is that i predict for $t+1$ as it will require $t$ and $t-1$ as lag component which i already have. once done i can use this predicted $t+1$ component to predict for $t+2$ as i will know value of $t+1$ from previous iteration and $t$ from original values. Is it the right approach?</p>
"
"0.062257280636469","0.0705345615858598","191001","<p>I am using the following bayesGARCH <a href=""https://cran.r-project.org/web/packages/bayesGARCH/index.html/"" rel=""nofollow"">here</a> package in R. I am interested in forecasting $h_t$, the model setup is given bellow. </p>

<p>$r_t$ = $\varepsilon_t(\frac{v-2}{v}\omega_th_t)^{1/2}$ $\quad$ with $\quad$ $t=1,...,T$ </p>

<p>$\varepsilon_t \overset{iid}{\sim}N(0,1)$ </p>

<p>$\omega_t \overset{iid}{\sim}IG(\frac{v}{2},\frac{v}{2})$ </p>

<p>$h_t = \alpha_0 + \alpha_1r^{2}_{t-1}+\beta h_{t-1}$</p>

<p>The package only provides simulated estimates of the parameter coefficients, namely $\alpha_0$, $\alpha_1$, $\beta$ and $v$. From my understanding the BayesGARCH does not have a function to forecast $h_t$, so I will have to forecast this manually. Any advice on forecasting this conditional volatility would be much appreciated. </p>
"
"0.27137319479402","0.242726073141876","193125","<p>I have fitted a DCC-GARCH model to my multivariate financial data and do the forecasting. Now, I would like to automate the procedure for a data set that I have. </p>

<pre><code># load libraries
library(rugarch)
library(rmgarch)
library(FinTS)
library(tseries)
Dat = dji30retw[, 1:8, drop = FALSE]

uspec = ugarchspec(mean.model = list(armaOrder = c(0,0)), variance.model = list(garchOrder = c(1,1), model = ""sGARCH""), distribution.model = ""norm"")
spec1 = dccspec(uspec = multispec( replicate(8, uspec)), dccOrder = c(1,1), distribution = ""mvnorm"")
fit1 = dccfit(spec1, data = Dat, out.sample = 141, fit.control = list(eval.se=T))
print(fit1)

#Forecast
dcc.focast=dccforecast(fit1, n.ahead = 1, n.roll = 0) 
</code></pre>

<p>I have 1141 observations with 8 assets. I want to fit a multivariate DCC-GARCH model to the first 1000 data points and use the remaining 114 data points as the out of sample forecasting period. For example :-</p>

<pre><code>1) Data[1:1000,] In-sample data, forecast for Data[1001,]
2) Data[1:1001,] In-sample data, forecast for Data[1002,]
3) Data[1:1002,] In-sample data, forecast for Data[1003,]
.. 
4) Data[1:1113,] In-sample data, forecast for Data[1141,]
</code></pre>

<p>How do I automate the process? I have never done looping before but I have tried to do the following.</p>

<p>Please find the example below:-</p>

<pre><code>for (i in 1:2)
{Dat.Initial = dji30retw[, 1:8, drop = FALSE]

 Dat &lt;- Dat.Initial[1:(1000+(i-1)), ] 

  #Fitting the data

  fit1 &lt;- list()
  spec1 &lt;-list()
  uspec = ugarchspec(mean.model = list(armaOrder = c(0,0)), variance.model = list(garchOrder = c(1,1), model = ""sGARCH""), distribution.model = ""norm"")
  spec1[[i]] = dccspec(uspec = multispec( replicate(8, uspec)), dccOrder = c(1,1), distribution = ""mvnorm"")
  fit1[[i]] = dccfit(spec1[[i]], data = Dat, out.sample = 114, fit.control = list(eval.se=T))
  print(summary(fit1[[i]])) }

#Out of sample forecasting
dcc.focast &lt;- list()
dcc.focast[[i]]=dccforecast(fit1, n.ahead = 1, n.roll = 0)
print(dcc.focast[[i]]) 
}
</code></pre>

<p>However, when I run the code, it comes out like this:</p>

<pre><code>  Length  Class   Mode 
   1 DCCfit     S4 
</code></pre>

<p>So where are my results?</p>

<p><strong>Update:</strong> I have obtained the result for the above question by executing the following command.</p>

<pre><code> for (i in 1:2)
 {Dat.Initial = dji30retw[, 1:8, drop = FALSE]
 Dat &lt;- Dat.Initial[1:(1000+(i-1)), ] 

 fit1 &lt;- list()
 uspec = ugarchspec(mean.model = list(armaOrder = c(0,0)), variance.model = list(garchOrder = c(1,1), model = ""sGARCH""), distribution.model = ""norm"")
 spec1 = dccspec(uspec = multispec( replicate(8, uspec)), dccOrder = c(1,1), distribution = ""mvnorm"")
 fit1[[i]] = dccfit(spec1, data = Dat, out.sample = 114, fit.control = list(eval.se=T))
 print(summary(fit1[[i]])) 
 #Out of sample forecasting
dcc.focast &lt;- list()
dcc.focast[[i]]=dccforecast(fit1[[i]], n.ahead = 1, n.roll = 0)
print(dcc.focast[[i]])
} 
</code></pre>

<p>Suppose that I don't want to over-write the fitted model and forecasting objects, though. From the forecasting, I will get the mean return and the variance-covariance matrix. If the for loop is working, I should get 114 values of mean returns and 114 set of variance covariance matrix.</p>

<pre><code>covmat.focast= rcov(dcc.focast)  ##--&gt;Only one covariance matrix.
mean.focast = fitted(dcc.focast)  ##Mean forecast matrix
</code></pre>

<p>Now, I want to get the mean returns forecast. </p>

<pre><code>for (i in 1:2)
{mean.focast &lt;- list()
mean.focast[[i]] = fitted(dcc.focast[[i]]) 
print(mean.focast[[i]]) 
}

#Error in fitted(dcc.focast[[i]]) : error in evaluating the argument 'object' in selecting a method for function 'fitted': Error in dcc.focast[[i]] : this S4 class is not subsettable
</code></pre>

<p>I have not manage to get the variance covariance matrix from the forecast. I have tried to use the following command:-</p>

<pre><code>for (i in 1:2)
{
covmat.focast &lt;- list()
covmat.focast[[i]]= dcc.focast@mforecast[i]
print(covmat.focast[[i]]) }
#Error: trying to get slot ""mforecast"" from an object of a basic class (""list"") with no slots
</code></pre>

<p>Anyone can help me pls?</p>
"
"0.062257280636469","0.0705345615858598","193945","<p>I want to use ARIMA for forecasting website visits after some fixed amount of seconds, say 100 seconds.</p>

<p>I have csv file available that contain two columns; one is the time and another is the visits, like below</p>

<pre><code>time,visitors

0,0.23171857

100,0.255455594

200,0.38544406

300,0.302929642

400,0.292756632

500,0.339100346
</code></pre>

<p>What is the frequency I should use in function <code>ts</code>?</p>

<p>I want to use one-day data for forecasting for the second day.</p>
"
"0.107832773203438","0.122169444356305","194130","<p>I want to get the cross-correlation of two time series <code>x</code> and <code>y</code> in R. </p>

<p>I have calculated an ARIMA model, and I can get the <code>mod1$residuals</code> from signal <code>x</code>. These residuals almost have no autocorrelation, so that's great. </p>

<pre><code>xts &lt;- ts(x,start=1,frequency=12) #convert to a time series
library(fpp)  #load forecasting package
mod1 &lt;- auto.arima(xts)
</code></pre>

<p>I now did the same procedure on signal <code>y</code>. </p>

<p>My question is: is this correct? Or should I somehow deduct the <code>mod1</code> (based on <code>x</code>) from <code>y</code> to de-trend it? </p>

<pre><code>ccf(mod1$residuals, mod2$residuals)
</code></pre>

<p>Secondly, I am confused about the order of operations. Should I prewhiten the data before calculating the model? </p>

<p>I found this code: </p>

<pre><code>prewhiten(x, y, x.model = ar.res,ylab=""CCF"", ...)
</code></pre>

<p>Should I estimate the <code>mod1</code> first and then supply it to the function <code>prewhiten</code>? And are <code>x</code> and <code>y</code> the two time series? Many thanks!</p>
"
"0.062257280636469","0.0705345615858598","202413","<p>I'm analyzing the Brent Barrel evolution over time (data attached to the question) from 1987 up to last week. It's weekly data, adjusted closing price every friday. </p>

<p>I'd like to carry out some forecasting on the evolution of Brent returns. So far, I've tried with a GARCH(1,1) and TARCH model, you can see here the graph of both. </p>

<p><strong>Given this data, which approach do you think is the best? A Garch(p,q) using any other lag, Tarch or another model?</strong></p>

<p>I attach here the code and the dataset I'm working out with.
Link to script: <a href=""https://www.dropbox.com/s/hf1muwxghu6lqvn/brent.Rmd?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/hf1muwxghu6lqvn/brent.Rmd?dl=0</a></p>

<p>Link to the working environment with the data and functions: <a href=""https://www.dropbox.com/s/q84lz6uye2wjlr4/datasetsbrent.RData?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/q84lz6uye2wjlr4/datasetsbrent.RData?dl=0</a>
<a href=""http://i.stack.imgur.com/KZuYi.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KZuYi.jpg"" alt=""GARCH vs TARCH""></a></p>
"
"0.186771841909407","0.211603684757579","212840","<p>I read Chen et al. <a href=""http://onlinelibrary.wiley.com/doi/10.1002/for.1134/abstract"" rel=""nofollow"">""Forecasting volatility with support vector machine-based GARCH model""</a> (2010) where they implented a recurrent SVM procedure to estimate volatility by a GARCH based model. 
The model is of the form </p>

<p>$y_t = f(y_{t-1}) + u_t \qquad \qquad \ \ \ (1)$ </p>

<p>$u^2_t = g(u^2_{t-1}, w_{t-1}) + w_t \qquad  (2)$ </p>

<p>At first they got estimates for $u_t$ by estimating $(1)$ by a SVM. Then, the following recurrent SVM algorithm was proposed to estimate $(2)$.</p>

<hr>

<p><strong><em>Recurrent SVM Algorithm:</em></strong></p>

<p><strong>Step 1:</strong> Set $i = 1$ and start with all residuals at zero: $w_t^{(1)} = 0 $.</p>

<p><strong>Step 2:</strong> Run an SVM procedure to get the decision function $f^{(i)}$ to the points $\{x_t, y_t\} = \{u_{t - 1}^2, u_t^2 \}$ with all inputs $x_t = \{u_{t - 1}^2, w_{t-1} \}$</p>

<p><strong>Step 3:</strong> Compute the new residuals $w_t^{i+1} = u_t^2 - f^{(i)}$.</p>

<p><strong>Step 4:</strong> Terminate the computaion process if the stopping criterion is satisfied; otherwise, set $i = i + 1$ and go back to Step 2.</p>

<hr>

<p>The proposed stopping critrerion is based a Ljung-Box-Test for the residuals $w_t$. Only if the $p$-values of the test in five consecutive periods are higher than 0.1 the process is stopped. </p>

<p>As real world example the log-returns of the New York Stock Exchange (NYSE) composite stock index for the period from January 8, 2004 to December 31, 2007 was used. The last 60 observations where used as test sample. Hence, the estimation was done with the first 940 observations. In their study, the process converged after 121 interations. <strong>(Question:) However, my implementation in R does not converge. I think I have a misunderstanding of the concept.</strong> Because I think I implemented it exactly as stated. My R code is the following</p>

<pre><code>rm(list = ls())

library(quantmod)
library(e1071)

#Get NYSE data and convert to log returns
id     &lt;- ""^NYA""
data   &lt;- getSymbols(id, source = ""yahoo"", auto.assign = FALSE, 
                     from = ""2004-01-08"", to = ""2007-12-31"")
series &lt;- data[,6]  #Get adjusted closing prices
series &lt;- na.omit(diff(log(series)))*100  #Compute log returns

#Lagged data for analysis
x      &lt;- na.omit(cbind(series, lag(series)))

#Set parameters as in paper
svm_eps   &lt;- 0.05
svm_cost  &lt;- 0.005
sigma     &lt;- 0.02
svm_gamma &lt;- 1/(2*sigma^2)


#SVM to get u_t
svm     &lt;- svm(x = x[,-1], y = x[,1], scale = FALSE,
               type = ""eps-regression"", kernel = ""radial"",
               gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

u    &lt;- svm$residuals  #Extract u_t
n    &lt;- 60  #Size of test set
u_tr &lt;- u[1:(nrow(u) - n)]  #Subset to training set
u_tr &lt;- na.omit(cbind(u_tr, lag(u_tr)))^2  #Final training set


#Recurrent SVM for vola estimation
i       &lt;- 1
p_count &lt;- 0

while(p_count &lt; 5){

  print(i)  #Print number of loops

  #Estimate SVM for u^2
  svmr     &lt;- svm(x = u_tr[,-1], y = u_tr[,1], scale = FALSE,
                  type = ""eps-regression"", kernel = ""radial"",
                  gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

  #Test autocorrelation of residuals to lag 1
  test    &lt;- Box.test(svmr$residuals, lag = 1, type = ""Ljung-Box"")
  p_val   &lt;- test$p.value
  p_count &lt;- ifelse(p_val &gt; 0.1, p_count + 1, 0)

  #Extract residuals for next estimation step
  w        &lt;- svmr$residuals
  w        &lt;- c(0, w[-length(w)])  #lag 1

  u_tr &lt;- cbind(u_tr[,1:2], w)

  i &lt;- i + 1
}
</code></pre>
"
"0.207204754712991","0.215190443337853","213159","<p>I have two time series $d_t(t)$, $d_c(t)$, where I'm modelling charge as a function of time. Lengths of time series, $N$ are equal to $101$ data points. For the $d_t(t)$ (test sample, short-term) the time interval between observations is $t_1 = 0.1$ sec, and for the $d_c(t)$ (control sample, long-term) the time interval is $t_2 = 1$ sec. Thus, the right boundary of time for $d_t(t)$ is equal to $0.1\times 100 = 10$ sec, and the right boundary of time for $d_c(t)$ is equal to $1 \times 100 = 100$ sec. Both $d_t(t)$, $d_c(t)$ are correspond to the one experiment. The accuracy of measurements is $tol=0.001$. </p>

<p>Here's some sample data and visualization,</p>

<pre><code>dtest &lt;- data.frame(
   TIME = seq(0,100,by=1),
   CHARGE=c(-1.824, -1.150, -1.112, -1.097, -1.090, -1.085, -1.080, -1.075, -1.072, -1.069, 
            -1.067, -1.064, -1.061, -1.060, -1.058, -1.056, -1.055, -1.052, -1.051, -1.050, 
            -1.049, -1.048, -1.048, -1.045, -1.044, -1.043, -1.042, -1.041, -1.040, -1.039, 
            -1.038, -1.037, -1.037, -1.036, -1.036, -1.034, -1.034, -1.033, -1.032, -1.032, 
            -1.031, -1.031, -1.030, -1.030, -1.029, -1.029, -1.028, -1.027, -1.027, -1.028, 
            -1.028, -1.026, -1.025, -1.025, -1.026, -1.024, -1.025, -1.023, -1.023, -1.023, 
            -1.023, -1.023, -1.022, -1.021, -1.020, -1.020, -1.020, -1.019, -1.019, -1.018, 
            -1.018, -1.018, -1.018, -1.017, -1.016, -1.017, -1.017, -1.016, -1.015, -1.015, 
            -1.015, -1.014, -1.014, -1.013, -1.013, -1.012, -1.012, -1.011, -1.011, -1.011, 
            -1.011, -1.010, -1.011, -1.010, -1.010, -1.009, -1.008, -1.008, -1.008, -1.008, -1.008))


dcont &lt;- data.frame(
   TIME = seq(0,100,by=1),
   CHARGE=c(-1.827, -1.071, -1.056, -1.047, -1.039, -1.034, -1.030, -1.027, -1.025, -1.020, 
            -1.017, -1.016, -1.013, -1.010, -1.009, -1.006, -1.007, -1.004, -1.004, -1.002, 
            -1.000, -0.999, -0.997, -0.997, -0.995, -0.995, -0.993, -0.991, -0.991, -0.991, 
            -0.989, -0.988, -0.988, -0.986, -0.985, -0.984, -0.984, -0.984, -0.982, -0.982, 
            -0.981, -0.981, -0.979, -0.978, -0.977, -0.976, -0.975, -0.975, -0.975, -0.974, 
            -0.973, -0.973, -0.972, -0.972, -0.971, -0.970, -0.970, -0.970, -0.969, -0.967, 
            -0.966, -0.966, -0.966, -0.966, -0.966, -0.965, -0.965, -0.964, -0.964, -0.963, 
            -0.962, -0.961, -0.962, -0.962, -0.962, -0.960, -0.960, -0.959, -0.959, -0.958, 
            -0.958, -0.958, -0.958, -0.957, -0.956, -0.956, -0.955, -0.955, -0.955, -0.955, 
            -0.955, -0.954, -0.953, -0.953, -0.954, -0.952, -0.952, -0.951, -0.952, -0.951, -0.952))
</code></pre>

<p><a href=""http://i.stack.imgur.com/Tg2cT.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Tg2cT.jpg"" alt=""enter image description here""></a></p>

<p>As we can see from figure above the data from start region is increasing exponentially $f(x)=(1+exp(-x/t))$, later they are increasing (stationary) with linear rule, like $f(x)=kx+b$.</p>

<p>I'd like to forecast a value of $d_t(t)$ at time point $t=100$ sec, i.e. I have 10 seconds of history and want to forecast out 90 seconds. Then verify the forecasting $d_t^p(t=100)$ with corresponding value from the $d_c(t=100)$. The forecasting is satisfactory, if $$0.95 \times d_c(t) \leq d_t^p(t)\leq 1.05 \times d_c(t).$$
On physical grounds, the experimental data can be described with an exponential function $f(x) = a \times (1+exp(-(x/\tau)))$, where $a$, $\tau$ are parameters. I have been doing curve-fitting $d(t)$ in <strong>R</strong> using <code>nlminb</code>. The initial values of the parameters for optimization are chosen based on the physical characteristics of the process: <code>parConv &lt;- c(a=-0.762,tau=5.88)</code>. The result is below: $a= -1.03084$, $\tau= 0.50464$. Unfortunatly, the forecasting $f(t=100)=-1.030845$ does not satisfy to the range: </p>

<p>$$f(t=100) = -1.030845 \bar{\in} [0.95 \cdot (-0.952), 1.05 \cdot (-0.952)]=[-0.9044, -0.9996].$$</p>

<p>I have tried to split the original time series $d_t(t)$ into two parts: $d_{t_1}(t=1..10)$ and $d_{t_2}(t=11..101)$, then I have approximated the $d_{t_2}(t=11..101)$ using a linear function $f_1(x)=kx+b$ and a polynomial $f_2(x)=ax^2+bx+c$. </p>

<p>Result are better but not satisfy to the range: </p>

<p>$f_1(x)=-1.05851+x \cdot 5.61364\cdot 10^{-4}$, and $f_2(x)=-1.0718+x\cdot 0.0012+x^2 \cdot (-5.86943\cdot 10^{-6})$, <code>Adj. R-Square = 0,9815</code>. Then I have obtained the forecasts: $f_1(t=100)=-1.002374$ and $f_2(t=100)= -1.010494$. </p>

<p><strong>My question is</strong>: How to improve a bad long-term forecasting of time series in common case?</p>

<p><strong>Another possible solutions are:</strong></p>

<ol>
<li><p>Apply function <code>ln()</code> to the original data $d_t(t)$ and repeat fitting.</p></li>
<li><p>To use an exponential function and to assign greater weight to the $k$ last points (like <a href=""http://stackoverflow.com/questions/33539287/how-to-force-specific-points-in-curve-fitting"">here</a>).</p></li>
<li><p>To use some alternative models, for example, <a href=""https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model"" rel=""nofollow"">Autoregressive moving-average model</a>, _https://en.wikipedia.org/wiki/Backcasting.</p></li>
</ol>

<p>Example code:</p>

<pre><code>library(minpack.lm)
library(ggplot2)
library(optimx)

d &lt;- data.frame(
   TIME = seq(0,100,by=1),
   CHARGE=c(-1.824, -1.150, -1.112, -1.097, -1.090, -1.085, -1.080, -1.075, -1.072, -1.069, 
            -1.067, -1.064, -1.061, -1.060, -1.058, -1.056, -1.055, -1.052, -1.051, -1.050, 
            -1.049, -1.048, -1.048, -1.045, -1.044, -1.043, -1.042, -1.041, -1.040, -1.039, 
            -1.038, -1.037, -1.037, -1.036, -1.036, -1.034, -1.034, -1.033, -1.032, -1.032, 
            -1.031, -1.031, -1.030, -1.030, -1.029, -1.029, -1.028, -1.027, -1.027, -1.028, 
            -1.028, -1.026, -1.025, -1.025, -1.026, -1.024, -1.025, -1.023, -1.023, -1.023, 
            -1.023, -1.023, -1.022, -1.021, -1.020, -1.020, -1.020, -1.019, -1.019, -1.018, 
            -1.018, -1.018, -1.018, -1.017, -1.016, -1.017, -1.017, -1.016, -1.015, -1.015, 
            -1.015, -1.014, -1.014, -1.013, -1.013, -1.012, -1.012, -1.011, -1.011, -1.011, 
            -1.011, -1.010, -1.011, -1.010, -1.010, -1.009, -1.008, -1.008, -1.008, -1.008, -1.008))


d1 &lt;- data.frame(
   TIME = seq(0,100,by=1),
   CHARGE=c(-1.827, -1.071, -1.056, -1.047, -1.039, -1.034, -1.030, -1.027, -1.025, -1.020, 
            -1.017, -1.016, -1.013, -1.010, -1.009, -1.006, -1.007, -1.004, -1.004, -1.002, 
            -1.000, -0.999, -0.997, -0.997, -0.995, -0.995, -0.993, -0.991, -0.991, -0.991, 
            -0.989, -0.988, -0.988, -0.986, -0.985, -0.984, -0.984, -0.984, -0.982, -0.982, 
            -0.981, -0.981, -0.979, -0.978, -0.977, -0.976, -0.975, -0.975, -0.975, -0.974, 
            -0.973, -0.973, -0.972, -0.972, -0.971, -0.970, -0.970, -0.970, -0.969, -0.967, 
            -0.966, -0.966, -0.966, -0.966, -0.966, -0.965, -0.965, -0.964, -0.964, -0.963, 
            -0.962, -0.961, -0.962, -0.962, -0.962, -0.960, -0.960, -0.959, -0.959, -0.958, 
            -0.958, -0.958, -0.958, -0.957, -0.956, -0.956, -0.955, -0.955, -0.955, -0.955, 
            -0.955, -0.954, -0.953, -0.953, -0.954, -0.952, -0.952, -0.951, -0.952, -0.951, -0.952))


(g1 &lt;- ggplot(d,aes(TIME,CHARGE))+geom_point())
g1+geom_smooth()  ## with loess fit

# Parameter choices:
parConv &lt;- c(a=-0.762,tau=5.88) #

#Perturbed parameters:
parStart      &lt;- parConv
parStart[""a""] &lt;- parStart[""a""]+3e-4

Ebos &lt;- -1.161 # start value at x=0

#The formulae:
RCCircuits&lt;-function(parS,x)
    with(as.list(parS), 
                       ifelse(x==0, Ebos, a*(1+exp(-(x/tau))) ) 
         )

# A sum-of-squares function
ssqfun &lt;- function(parS, Observed, x) {
   sum(ResidFun(parS, Observed, x)^2)
}

# Local minimizer for smooth nonlinear functions subject to bound-constrained parameters
opt1 &lt;- nlminb(start=parStart, objective = ssqfun,
    Observed = d$CHARGE, x = d$TIME,
    control= list(eval.max=5000,iter.max=5000))

parNLM &lt;- opt1$par

#SSE Review:
sapply(list(parConv,parNLM),
  ssqfun,Observed=d$CHARGE,x=d$TIME)  

pred0 &lt;- RCCircuits(as.list(parConv), d$TIME)
pred1 &lt;- RCCircuits(as.list(parNLM),  d$TIME)

# forecasting at t=100 sec
pred100 &lt;- RCCircuits(as.list(parNLM), 1000) 
</code></pre>
"
"0.249029122545876","0.264504605946974","213192","<p>I have half-hourly electricity data of several homes for a duration of one month. Also, I have ambient temperature at same sampling rate. Now, I need to make half-hourly forecasts using historical electricity data and forecasted temperature for coming day.</p>

<p>Currently, I am facing a problem with seasonality, i.e., in some homes I observe (via visual inspection) day-wise seasonality and in some other homes no pattern is found. <a href=""http://stats.stackexchange.com/a/212797/60072"">Stephan</a> suggested to check for weekly seasonality as well, but visually I do not find any. So, I thought to try models with different forced seasonalities to find the prediction accuracy. I can think of two options:</p>

<ol>
<li>Model_1 with daily seasonality (frequency = 48 observations)</li>
<li>Model_2 with weekly and daily seasonality (48, 7*48)</li>
</ol>

<p>Keeping the above approach in mind, I am facing the following issue:</p>

<ol>
<li>For Model_1, I can use <code>auto.arima()</code> with <code>xreg</code> option to specify an extra <code>temperature</code> predictor. However, this only works for a single seasonality.</li>
<li>For Model_2, I tried to use the <code>tbats()</code> forecasting function, which models multiple seasonalities, but does not allow extra predictor variables. </li>
</ol>

<p>Is there a forecasting function which allows <em>both</em> multiple predictors and multiple seasonalities?</p>

<p>Here is the electricity data of one month. </p>

<pre><code>    data &lt;- structure(c(1642.8, 1467.1, 165.57, 1630.99, 1618.65, 1629.29, 
        1598.93, 1839.9, 1604.52, 1606.73, 1473.82, 1669.17, 1698.9, 
        2111.21, 2056.41, 3671.29, 2808.01, 1336.15, 794.11, 1212.15, 
        377.36, 888.54, 174.58, 218.54, 420.76, 389.58, 397.77, 395.31, 
        359.11, 364.8, 376.13, 389.37, 929.5, 1702.38, 519.65, 2452.28, 
        1354.45, 1842.96, 725.41, 661.11, 528.44, 733.4, 429.51, 310.47, 
        279.72, 407.83, 1791.1, 1754.53, 1536.73, 1608.37, 1432.23, 1401.72, 
        1582.14, 1558.75, 1536.24, 1745.59, 1375.61, 1556.71, 1671.12, 
        1206.77, 1391.84, 876.23, 1617.3, 1638.99, 1833.61, 1591.42, 
        1455, 183.87, 177.55, 184.36, 332.99, 352.95, 425.1, 945.67, 
        342.3, 348.45, 227.18, 382.15, 268.91, 335.88, 326.94, 233.23, 
        169.71, 179.51, 195.3, 207.23, 1681.9, 1493.32, 941.52, 980.36, 
        924.31, 379.02, 1229.89, 1590.21, 1250.92, 1149.24, 1124.04, 
        993.78, 883.98, 860.69, 934.17, 969.31, 1049.55, 1104.94, 904.3, 
        1220.23, 1183.9, 891.26, 825.33, 787.77, 1060.93, 1029.1, 982.25, 
        193.13, 182.65, 181.75, 167.68, 165.89, 291.02, 300.2, 418.4, 
        297.66, 231.89, 305.66, 701.18, 338.5, 337.24, 332.11, 332.66, 
        187.8, 179.03, 130.22, 177.73, 172.24, 173.45, 334.23, 810.53, 
        359.41, 330.23, 333.29, 568.85, 2462.46, 1660.32, 1156.13, 1136.2, 
        1189.07, 832.73, 181.91, 185.93, 1076.77, 672.78, 1376.71, 1020.17, 
        382.18, 1160.74, 791.36, 1569.4, 817.78, 850.71, 747.21, 826.12, 
        1306.46, 506.23, 140.05, 132.6, 304.19, 308.14, 406.13, 290.73, 
        188.9, 165.59, 174.56, 145.97, 151.83, 142.29, 443.58, 799.9, 
        279.36, 223.88, 221.03, 291.26, 374.97, 431.36, 598.98, 625.82, 
        1052.02, 2036.83, 1230.03, 1429.81, 1099.34, 1646.03, 1668.56, 
        1631.79, 1604.04, 2849.49, 2998.63, 2476.96, 1601.04, 1216.42, 
        2004.2, 1868.51, 1961.91, 1813.35, 1500.22, 1276.94, 1369.29, 
        632.43, 238.15, 488.76, 467.6, 330.27, 144.67, 153.36, 924.12, 
        1348.18, 799.01, 524.3, 420.5, 264.34, 283.86, 198.95, 206.52, 
        217.33, 356.16, 207.83, 197.6, 194.03, 193.01, 249.85, 271.22, 
        244.25, 442.5, 660.49, 245.66, 356.13, 443.32, 336.15, 849.74, 
        1709.21, 1542.09, 1315.69, 2628.6, 2261.8, 1576.42, 1776.48, 
        1239.64, 1401.12, 1106.17, 1378.55, 1315.57, 1141, 1642.63, 2484.13, 
        1968.94, 3059.42, 1317.32, 905.05, 484.42, 486.86, 96.79, 183.45, 
        173.94, 342, 255.96, 320.54, 106.19, 147.88, 150.77, 176.17, 
        344.75, 371.73, 309.46, 237.86, 187.32, 202.61, 292.5, 248.28, 
        259.3, 283.67, 365.09, 230.47, 326.15, 350.92, 335.42, 419.39, 
        345.31, 1093.22, 1392.87, 1298.11, 919.16, 1654.53, 1045.99, 
        558.42, 437.29, 857.9, 758.34, 1220.04, 1390.62, 956.74, 909.93, 
        584.67, 409.87, 387.3, 387.93, 1276.28, 871.06, 413.8, 313.2, 
        199.5, 330.21, 210.9, 358.28, 352.13, 233.62, 259.18, 123.57, 
        255.58, 411.78, 427.65, 318.95, 298.5, 283.23, 279.85, 200.45, 
        205.97, 254.24, 307.98, 1090.53, 289.71, 215.14, 286.63, 328.55, 
        288.96, 1281.19, 1354.8, 1302.05, 1254.46, 261.95, 270.09, 243.28, 
        696.61, 314.27, 241.73, 245.68, 157.74, 222.55, 294.36, 185.46, 
        203.49, 182.14, 246.69, 178.26, 397.5, 330.2, 212.02, 248.72, 
        265.48, 249.37, 130.59, 248.97, 279.94, 319.07, 358.5, 278.98, 
        251.92, 304.66, 455.05, 365.95, 340.93, 287.51, 264.82, 260.18, 
        34.35, 35.11, 184.09, 247.18, 160.9, 139.27, 284.96, 296.31, 
        252.3, 342.65, 353.03, 380.52, 346.19, 350.06, 218.52, 133.94, 
        173.7, 128.26, 167.8, 112.77, 147.8, 129, 170.54, 89.88, 243.08, 
        97.61, 190.31, 193.94, 268.17, 233.5, 205.27, 92.29, 167.43, 
        168.34, 151.99, 193.84, 379.1, 318.69, 327.28, 487.39, 414.01, 
        336.06, 278.02, 168.05, 155.6, 236.4, 264.94, 296.05, 326.46, 
        357.43, 356.31, 340.29, 319.81, 312.79, 341.53, 317.36, 309.62, 
        440.6, 285.5, 282.06, 288.99, 334.48, 196.54, 144.24, 218.55, 
        173.64, 242.29, 251.78, 186.81, 184.36, 141.62, 208.91, 157.53, 
        154.03, 139.44, 137.66, 256.75, 1202.05, 177.36, 177.93, 72.83, 
        252.9, 231.35, 1090.39, 442.91, 363.12, 248.96, 478.75, 249.64, 
        297.29, 227.28, 365.82, 879.7, 488.93, 184.79, 138.13, 151.77, 
        123.18, 175.76, 251.84, 208.06, 126.68, 246.3, 307.34, 319.79, 
        324.3, 379.6, 309.53, 253.17, 221.91, 228.42, 150.24, 148.59, 
        118.79, 86.89, 140.51, 200.43, 212.15, 276.14, 441.81, 125.77, 
        152.42, 329.28, 269.21, 177.35, 1106.29, 128.92, 96.35, 63.53, 
        520.62, 940.25, 1014.34, 314.99, 390.2, 330.1, 377.04, 341.35, 
        342.79, 241.79, 249.9, 391.92, 292.68, 105.02, 179.99, 118.53, 
        154.17, 90.53, 206.7, 345.33, 244.75, 291.68, 820.57, 1777.84, 
        1805.83, 1753.73, 1416.7, 279.2, 262.82, 1345.88, 467.98, 1136.66, 
        170.02, 159.96, 1478.8, 1414.12, 1347.93, 1505.59, 1341.69, 445.53, 
        277.59, 1609.61, 1476.45, 244.08, 192.57, 213.55, 439.02, 112.86, 
        128.54, 376.09, 251.15, 116.27, 254.82, 302.56, 304.6, 198.5, 
        240.05, 219.35, 70.3, 190.96, 211.95, 328.53, 714.62, 3176.56, 
        2604.09, 191.65, 145.25, 93.93, 83.6, 81.13, 146.12, 331.75, 
        250.24, 1144.53, 1616.47, 1008.7, 316.65, 311.46, 1152.99, 1504.86, 
        1543.21, 1081.54, 1428.07, 1358.23, 1349.75, 190.23, 2398.92, 
        2196.11, 1466.94, 2249.77, 2150.2, 2542.25, 618.03, 453.22, 880.99, 
        1497.86, 440.96, 161.85, 324.88, 434.11, 316.33, 444.66, 359.14, 
        277.41, 1237.28, 761.41, 183.53, 309.44, 213.48, 121.64, 346.7, 
        149.86, 2060.39, 1102.13, 347.97, 600.24, 912.6, 590.77, 1805.76, 
        1673.93, 1573.91, 505.74, 446.76, 1033.41, 1668.68, 1293.9, 383.81, 
        1419.99, 1349.4, 711.55, 218.63, 182, 401.93, 1876, 1486.34, 
        1543.11, 2313.8, 478.57, 615.19, 542.68, 971.98, 531.11, 766.21, 
        489.76, 344.47, 319.86, 321.26, 311.41, 288.67, 310.67, 305.15, 
        419.33, 422.84, 950.08, 2188.88, 3454.92, 1989.54, 590.33, 327.05, 
        354.78, 578.41, 1583.29, 2016.66, 1481.03, 293.21, 1864.84, 399.65, 
        366.76, 357.7, 2074.97, 1626.86, 1133, 1624.61, 1506.93, 628.4, 
        1405.68, 217.8, 1223.11, 1356.97, 1171.72, 1182.86, 1642.11, 
        2289.02, 814.39, 595.76, 542.78, 1596.41, 884.97, 235.25, 1540.68, 
        781.95, 115.71, 1204.98, 718.66, 452.09, 305.58, 444.67, 356.76, 
        182.54, 674.47, 153.8, 862.25, 1322.88, 323.33, 1659.64, 496.72, 
        304.74, 246.6, 327.12, 239.31, 246.72, 225.72, 234.7, 324.07, 
        304.27, 171.86, 97.64, 242.69, 295, 324.53, 513.81, 1100.65, 
        1151.77, 231.56, 189.88, 786.3, 1164.87, 676.09, 882.82, 1496.3, 
        1027.91, 872.92, 809.1, 840.31, 1302.18, 2055.87, 677.74, 934.66, 
        263.91, 186.68, 248.5, 214.62, 371.54, 298, 294.52, 304.86, 1295.77, 
        942.5, 305, 265.78, 255.89, 255.63, 151.54, 108.16, 116.81, 100.19, 
        224.75, 84.11, 1143.89, 262.15, 784.21, 1728.29, 1506.79, 434.94, 
        374.29, 265.43, 560.74, 1651.49, 1063.07, 1054.69, 1298.4, 1261.59, 
        1132.75, 692.82, 660.57, 198.25, 97.81, 1258.67, 833.64, 796.35, 
        868.76, 999.86, 2240.02, 885.72, 1317.52, 1267.18, 167.93, 133.22, 
        364.44, 267.17, 406.13, 412.52, 1036.04, 779.34, 655.43, 1901.2, 
        270.18, 266.31, 284.21, 288.66, 135.38, 176.11, 154.86, 160.21, 
        146.28, 163.72, 139.75, 278.12, 253.51, 319.62, 396.39, 1662.69, 
        1577.09, 1059.71, 241.64, 407.54, 290.49, 846.17, 1325.31, 1418.23, 
        1432.5, 1412.6, 1015.87, 1619.88, 1426.58, 1333.32, 1963.35, 
        1638.11, 1081.89, 285.56, 1084.58, 2038.77, 1022.39, 1145.92, 
        513.87, 107.7, 176.19, 143.77, 374.3, 373.99, 221.76, 148.3, 
        331.01, 2323.12, 1502.09, 347.17, 296.7, 306.06, 313.03, 221.69, 
        295.35, 301.95, 250.92, 231.54, 140.6, 717.63, 863.5, 402.15, 
        1337.78, 1575.44, 1738.49, 1675.57, 1617.66, 1365.58, 242.8, 
        286.29, 712.34, 1559.59, 1600.34, 3447.88, 3432.89, 3337.23, 
        1472.21, 1323.76, 1265.31, 1221.65, 1312.63, 2016.09, 2972.13, 
        1451.67, 735.67, 130.13, 379.87, 162.21, 226.48, 417.18, 357.51, 
        346.37, 200.89, 190.15, 276.05, 942.74, 1471.99, 1047.53, 1240.06, 
        742.62, 169.34, 144.28, 220.58, 165.23, 344.8, 227.47, 254.64, 
        742.37, 1809.01, 436.11, 1692.87, 1697.74, 1474.91, 1635.68, 
        1664.2, 1489.85, 1316.4, 1364.07, 1510.02, 1497.89, 1709.17, 
        2846.71, 2736.08, 1015.43, 1017.08, 1195.21, 751.18, 523.89, 
        199.69, 2148.71, 1151.39, 1182.84, 788.91, 259.31, 146.29, 141.09, 
        299.38, 349.53, 404.08, 449.9, 391.77, 251.89, 222.25, 281.04, 
        565.4, 371.24, 219.75, 324.45, 227, 157.67, 212.48, 201.69, 140.84, 
        220.67, 187.64, 399.79, 157.92, 275.49, 326.99, 1340.51, 1578.68, 
        1599.41, 1667.74, 1129.07, 1312.55, 1393.09, 1368.69, 1130.85, 
        968.71, 1130.2, 1223.1, 1124.5, 1077.09, 1052.42, 1255.31, 918.21, 
        1263.93, 706.21, 3080.29, 1620.18, 1122.48, 750.06, 262.89, 110.02, 
        236.83, 413, 227.53, 355, 277.51, 258.03, 368.44, 656.68, 1808.17, 
        1493.41, 1272.21, 330.67, 1674.18, 719.45, 1089.68, 784.01, 275.73, 
        312.69, 345.11, 761.8, 1020.11, 259.16, 345.98, 270.23, 580.15, 
        1303.68, 1659.7, 1732.94, 204.9, 373.58, 373.36, 1381.52, 1437.74, 
        1262.78, 1264.6, 1184.54, 1175.12, 857.09, 1428.34, 841.92, 232.47, 
        223.22, 473.24, 382.7, 189.84, 1737.48, 1689.34, 378.48, 872.56, 
        180.12, 363.03, 301.38, 412.57, 401.17, 387.35, 417.63, 300.61, 
        376.82, 284.31, 232.31, 269.96, 188.41, 203.79, 134.88, 193.66, 
        57.86, 89.71, 167.66, 60.84, 197.03, 703.66, 1638.7, 1467.03, 
        347.22, 1397.23, 1511.92, 1362.25, 1397.18, 1106.19, 826.5, 1033.04, 
        1039.46, 584.98, 706.35, 548.07, 373.39, 681.6, 1231.28, 288.94, 
        649.36, 79.28, 209.23, 290.75, 304.12, 132.57, 91.2, 355.37, 
        197.45, 343.17, 339.2, 284.65, 229, 234.73, 322.36, 323.43, 295.1, 
        197.03, 308.17, 223.88, 235.08, 225.16, 172.83, 236.26, 135.17, 
        394.89, 479.2, 315.34, 280.9, 282.36, 204.78, 367.24, 1712.29, 
        1521, 1686.09, 960.19, 1019.12, 1062.77, 851.88, 1369.98, 689.2, 
        580.5, 751.74, 547.67, 556.64, 493.85, 404.15, 428.07, 716.2, 
        1442.05, 1045.81, 1497.07, 567.59, 155.07, 537.72, 446.03, 282.57, 
        642.88, 409.37, 338.91, 173.89, 358.63, 195.42, 209.95, 186.44, 
        152.34, 105.51, 132.26, 82.14, 122.88, 149.38, 211.42, 350.17, 
        429.72, 336.4, 982.21, 1436.25, 1726.87, 1830.42, 1282.05, 1293.4, 
        1121.01, 946.2, 707.1, 154.53, 767.56, 607.78, 448, 288.23, 270.32, 
        223.93, 166.22, 262.45, 223.74, 159.31, 210.44, 257.94, 183.99, 
        151.38, 206.11, 193.43, 388.95, 577.98, 304.64, 285.13, 256.59, 
        420.26, 289.34, 356.02, 358.08, 325.22, 275.95, 164.46, 213.23, 
        142.99, 221.66, 270.61, 206.56, 213.68, 254.33, 250.15, 267.99, 
        403.95, 671.2, 1574.11, 396.34, 477.88, 631.08, 618.25, 1366.88, 
        298.19, 287.05, 290.38, 332.44, 235.9, 229.79, 831.6, 1320.86, 
        477.31, 944.84, 547.33, 411.21, 705.43, 873.49, 572.81, 585.36, 
        1229.69, 701.01, 653.49, 74.81, 162.47, 179.54, 330.27, 544.51, 
        332.41, 296.66, 130.66, 1055.61, 556.79, 265.43, 383.44, 398.22, 
        362.66, 223.99, 130.35, 193.67, 217.68, 273.3, 247.84, 161.66, 
        320.08, 322.52, 274.61, 811.44, 353.85, 323.41, 383.61, 389.5
        ), .Dim = c(1248L, 1L), .Dimnames = list(NULL, ""power""))
</code></pre>

<p>Temperature predictor values are:</p>

<pre><code>temp&lt;-structure(c(31, 31, 31, 31, 30, 29, 28, 27.5, 27, 26, 26, 26, 
26, 26, 26, 28, 29, 29, 30, 31, 32, 33, 33, 34, 34, 35, 36, 36, 
36.5, 37, 38, 38, 39, 39, 39, 38, 37, 36, 36, 35, 34, 33, 33, 
33, 32.5, 32, 32, 31, 30, 26, 24, 24, 24, 24, 24, 24, 25, 25, 
25, 25, 26, 26, 26, 27, 28.3, 29.7, 31, 32, 33, 33, 34, 34, 34, 
35, 35, 35, 36, 37, 38, 39, 39, 39, 39, 38.5, 38, 37, 36, 35, 
34, 34, 33, 32.5, 32, 30, 30, 30, 28, 28, 27, 25, 25, 24, 24, 
24, 24, 24, 25, 25, 25, 25, 25, 27, 28, 30, 31, 33, 34, 37, 37, 
38, 38, 39, 39.3, 39.7, 40, 40, 40, 40, 41, 40, 40, 38, 38, 36, 
35, 34, 33, 31, 31, 30, 31, 30, 30, 29, 28, 28, 28, 27, 27, 28, 
27, 24, 24, 24, 23, 23, 23, 24, 24, 26, 27, 28, 29.7, 31.3, 33, 
34, 35.3, 36.7, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 
39, 38, 37, 36, 34, 33, 32, 32, 30, 28.5, 27, 27, 27, 27, 27, 
26, 26, 26, 25.3, 24.7, 24, 24, 24, 24, 23, 23, 24.5, 26, 28, 
29.7, 31.3, 33, 34.5, 36, 37, 38, 39, 40, 40, 40.3, 40.7, 41, 
41, 41, 41, 41, 41, 41, 39.5, 38, 37, 37, 35, 34, 34, 33, 33, 
33, 32, 31, 31, 30, 30, 30, 29, 29, 28.5, 28, 28, 28, 29, 29, 
28.5, 28, 29, 29, 30, 31, 31, 32.5, 34, 35, 37, 39.5, 42, 42, 
42, 42, 42, 43, 44, 45, 45, 44, 43, 42, 42, 41, 39, 36.5, 34, 
34, 33, 33, 33, 33, 33, 33, 32, 32, 32, 32, 31.5, 31, 30, 30, 
30, 29, 29, 28, 27, 28, 29, 30, 31, 32.5, 34, 34, 35, 37, 40, 
41, 41, 42, 42, 42.5, 43, 43, 43, 42, 42, 43, 42, 42, 41, 40, 
39, 37.5, 36, 35, 35, 34, 34, 33, 32, 32, 32, 32, 31, 31, 31, 
31, 31, 28, 28, 28, 28, 28, 28.5, 29, 30, 30.5, 31, 32, 33.5, 
35, 36, 37, 38, 39, 38, 39, 40, 41, 42, 42, 43, 42, 42, 42, 41, 
41, 40, 39, 38, 37, 36, 35, 34, 34, 33, 32, 32, 31, 30.5, 30, 
30, 30, 30, 29, 29, 29, 28, 27, 27, 27, 27, 28.5, 30, 31.5, 33, 
34, 35, 36, 37, 38, 38, 39, 40, 41, 42, 42, 42, 42, 42, 42, 42, 
42, 42, 42, 42, 40, 40, 39, 37, 37, 36.5, 36, 35, 34, 34, 33, 
33, 32, 32, 32, 32, 32, 32, 31, 31, 30, 30, 30, 30, 31, 31, 31, 
32, 33.5, 35, 35.5, 36, 37, 37, 38, 39, 40, 41, 41, 42, 42, 42, 
43, 43, 42, 41, 41, 40, 38, 36, 35, 34, 34, 35, 36, 36, 35, 35, 
33, 33, 32, 31, 31, 30, 30, 29, 29, 29, 29, 29, 31, 31.5, 32, 
31, 30, 31, 32, 33, 34, 34, 35, 36, 36, 37, 37, 38, 38, 38, 39, 
39, 39, 39, 39, 39, 39, 38, 37, 37, 37, 37, 35, 33, 31, 31, 31, 
30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 28, 28, 27, 27, 26, 29, 
29, 30, 31, 32, 32, 33, 33, 33, 34, 34, 35, 35, 37, 37, 37, 37, 
37, 38, 38, 38, 38, 37, 37, 36, 35, 34.5, 34, 34, 33, 33, 32, 
32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 30, 30, 29.5, 29, 
29, 30, 30, 31, 32, 32, 33, 33.5, 34, 34, 34, 30.7, 27.3, 24, 
24, 24, 24.7, 25.3, 26, 27, 28, 29, 29, 29, 28, 28, 27, 27, 26, 
25, 25, 25, 24.5, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 
23, 23, 23, 24, 24, 24, 25, 25, 26, 28, 29, 30, 31, 31, 32, 33, 
34, 35, 35, 36, 37, 37, 37, 37, 37, 37, 37, 35, 34, 33, 33, 33, 
32, 31.7, 31.3, 31, 31, 31, 31, 30, 30, 30, 30, 28, 28, 27, 27, 
26, 26, 25, 25, 25, 25, 25, 26, 27, 28, 29.7, 31.3, 33, 34, 34, 
35, 36, 37, 36, 36, 35, 35, 34, 32, 32, 33, 31.3, 29.7, 28, 28, 
29, 29, 29, 29, 28, 27.5, 27, 26.5, 26, 26, 26, 26, 26, 26, 25, 
25, 25, 25, 25, 24, 24, 23, 23, 23, 24.5, 26, 27, 28, 29, 30, 
31, 32, 32, 32, 33, 34, 35, 36, 36, 36, 36, 36, 36, 36, 37, 37, 
37, 36, 36, 34, 34, 33, 32, 32, 31, 31, 30, 28, 28, 28, 28, 27, 
27, 27, 27, 27, 27, 27, 27, 26.5, 26, 26, 26, 26, 27, 28.8, 30.5, 
32.2, 34, 34, 34, 35, 36, 37, 38, 38, 38, 39, 40, 39, 39, 39, 
40, 40, 40, 39, 39, 38, 37, 35, 35, 34, 34, 34, 33, 32, 32, 32, 
32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 33, 34, 
35, 37, 37, 37.3, 37.7, 38, 39, 40, 43, 43, 43, 44, 44, 43, 43, 
43, 43, 43, 43, 42, 41, 40, 39, 38, 37, 36, 35, 35, 34, 34, 34, 
34, 34, 33, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 34, 
35, 36, 37, 38, 38, 40, 40.5, 41, 42, 42, 42, 42, 41, 40, 40, 
39, 39, 37, 32, 27, 26, 26, 27, 28, 27, 27, 27, 26, 25, 25, 25, 
25, 25, 25, 25, 25, 25, 25, 25, 24, 23, 23, 22.5, 22, 22, 23, 
24, 25, 26.3, 27.7, 29, 30, 32, 33, 34, 36, 37, 37, 39, 40, 40.5, 
41, 42, 42, 42, 43, 43, 42, 41.5, 41, 39, 38, 35, 35, 35, 34, 
33, 33, 33, 33, 32, 32, 31, 31, 30, 30, 30, 29, 29, 28, 28, 28, 
28.5, 29, 29, 29, 30, 32, 32, 34, 35, 37, 38, 39, 40, 41, 42, 
42, 43, 44, 44, 44, 45, 45, 45, 44, 43, 43, 42, 40, 38.5, 37, 
35, 35, 34, 34, 33, 34, 33, 33, 33, 33, 32, 32, 31, 32, 31, 30, 
29, 29, 29, 29, 30, 30, 31, 32.7, 34.3, 36, 37, 38, 39, 40, 41, 
41, 42, 42, 43, 44, 44, 44, 44, 44, 43.5, 43, 43, 42, 42, 40, 
38, 38, 37, 36, 36, 36, 36, 35, 35, 34, 34, 34, 34, 32, 32, 31.5, 
31, 31, 30, 30, 30, 30, 31, 33, 34, 35, 37, 38, 39, 40, 40, 42, 
42, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 42, 40, 40, 
40, 39, 38, 38, 38, 37, 37, 36, 36, 35.5, 35, 34.5, 34, 33, 33, 
32, 32, 31, 30, 30, 30, 31, 32, 33.5, 35, 36.5, 38, 38, 39, 40, 
41, 41, 41, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 
42, 40.5, 39, 38, 38, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 
35, 35, 34, 34, 34, 32, 32, 31, 31, 33, 34, 35, 35, 36, 38, 39, 
40, 41, 42, 42.5, 43, 43, 44, 44, 45, 45, 45, 45, 45, 45, 45, 
44, 43.5, 43, 43, 42, 41, 40, 40, 39, 39, 38, 38, 37, 36, 36, 
35, 34, 34, 34, 32, 32, 32, 32, 32, 32, 32, 34, 35, 35, 37, 38, 
38, 39, 39, 40, 41, 42, 43, 44, 44, 44, 45, 45, 46, 46, 46, 45, 
45, 45, 44, 42, 40, 38, 37, 37, 37, 37, 37, 36, 35), .Dim = c(1248L, 
1L), .Dimnames = list(NULL, ""temperature""))
</code></pre>

<p>Forecasted temperature values are:</p>

<pre><code>forecast_temp &lt;- structure(c(35, 34, 34, 33, 33, 30, 29.7, 29.3, 29, 28, 28, 27, 
27.3, 27.7, 28, 29, 30.5, 32, 33.5, 35, 36, 36, 37, 39, 39.5, 
40, 40, 41, 42, 42, 43, 43, 43, 44, 44, 44, 43, 42, 40, 39, 39, 
38, 37, 37, 36, 35, 35, 34), .Dim = c(48L, 1L), .Dimnames = list(
    NULL, ""temperature""))
</code></pre>

<h3><em>UPDATE-1</em></h3>

<p>From @Stephan's suggestion, I followed the approach mentioned by Prof. Rob at <a href=""http://robjhyndman.com/hyndsight/dailydata/"" rel=""nofollow"">link1</a>, <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">link2</a> as</p>

<pre><code>library(forecast)
tsob &lt;- ts(train_power,frequency = 48) #training electriciy data at daily frequency
tsob_weekly &lt;- fourier(ts(train_power,frequency = 7*48),K=3) #training data at weekly frequency
tempob &lt;- ts(train_temperature,frequency = 48) #Temperature, another predictor variable
fit &lt;- auto.arima(tsob,xreg=cbind(tsob_weekly,tempob),seasonal=FALSE)
tempob_forecast &lt;- ts(test_temperature,frequency = 48) #forecasted temperauture values
forecast_val &lt;- forecast(fit,xreg=tempob_forecast,h=48*5) #forecast for coming 5 days
</code></pre>

<p>As evident from the code, I have used Fourier transformation suggested at above links to show weekly seasonal affect. Up to <code>auto.arima()</code>, it works properly but at forecast function an error is thrown as:</p>

<pre><code>Error in forecast.Arima(fit, xreg = tempob_forecast, h = 48 * 5) : 
  Number of regressors does not match fitted model
</code></pre>

<p>The error is clear, i.e., I do not provide the forecasted regressor values of <code>tsob_weekly</code>, which I don't have. How should I handle this issue? Prof. Rob has used the same value of regressor for both the <code>auto.arima</code> and <code>forecast</code> functions.</p>
"
"0.22447181760574","0.234753210914022","223888","<p>I am interested in forecasting with a vector error correction model (VECM). I am facing a problem of not being able to transform a cointegrated series into a VECM model using the stationary series. </p>

<p>In multivariate forecast like VAR or VECM it is important to see which of the two models to use for forecasting. To decide whether to use a VAR or a VECM:</p>

<ul>
<li>First, we do a cointegration test using the <code>ca.jo</code> function from ""urca"" package in R. </li>
<li>If we find that there is no cointegrating vector suggested by the Johansen procedure, then we can run a VAR model. But if we find evidence of cointegration then we have to use a VECM model in order to incorporate the error correction coefficients in the model. </li>
<li>To test if there is cointegration in the series we use Johansen test on the data <strong>in levels</strong>, i.e. in non-stationary form. But after we find evidence of cointegration we have to incorporate as many cointegrating vectors in the VECM as the number suggested by the Johansen test. But then this time the VECM should have been run on stationary series having made them <strong>differenced</strong>. </li>
<li>But in R I am not getting the option as to how to make a VECM model differenced and then forecast it. R manuals are suggesting that we should use the function <code>vec2var</code> to convert a VECM to a VAR model and then forecast the VAR model thus obtained. </li>
<li>But the VAR model thus obtained from the VECM is <strong>at levels</strong> and <em>not</em> at <strong>differenced</strong> form. Hence, inference from this may be biased. </li>
</ul>

<p>I just want to run a VECM in <strong>differenced</strong> series (not <strong>in levels</strong>) and also to include the error correction term. Please help me with this. </p>
"
"0.124514561272938","0.0705345615858598","225297","<p>Any given ARIMA(p,d,q) model $y^*_t=\sum_{i=1}^pa_iy^*_{t-i}+e_t+\sum_{i=1}^qb_ie_{t-i}$,where $ y^*_t=\Delta_dy_t$ - the difference of d$^{th}$ order, can be re-written as a dynamic linear model in state space (<a href=""http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/14_state_space.pdf"" rel=""nofollow"">1</a>, <a href=""http://radhakrishna.typepad.com/TimeSeries_Analysis_State_Space_Methods.pdf"" rel=""nofollow"">2</a>) in the following way: 
$$\begin{cases} y_t=ZX_t,\\
X_{t+1}=TX_t+Re_t.
\end{cases}
$$
Such representation is used for fitting model to the observed time series and forecasting future values.
What I'm trying to understand is the way function SSMArima from KFAS estimates the covariance matrix of the nondiffuse part of the initial state vector P1. The particularly important to the answer is this piece of code:</p>

<pre><code>nd &lt;- which(diag(P1inf) == 0)
    mnd &lt;- length(nd)
    temp &lt;- try(solve(a = diag(mnd^2) - matrix(kronecker(T[nd, 
        nd], T[nd, nd]), mnd^2, mnd^2), b = c(R[nd, , drop = FALSE] %*% 
        Q %*% t(R[nd, , drop = FALSE]))), TRUE)
    if (class(temp) == ""try-error"") {
        stop(""ARIMA part is numerically too close to non-stationarity."")
    }
    else P1[nd, nd] &lt;- temp
</code></pre>

<p>The main part is </p>

<pre><code>solve(a = diag(mnd^2) - matrix(kronecker(T[nd, 
    nd], T[nd, nd]), mnd^2, mnd^2), b = c(R[nd, , drop = FALSE] %*% 
    Q %*% t(R[nd, , drop = FALSE])))
</code></pre>

<p>In its simplest case when $d = 0, Q = 1$ the covariance matrix P1 is obtained after solving the equation:
$$
(I_{r^2} - T\otimes T)x = vec(RR^T)
$$
and putting the result into square matrix (devectorize it $devec(x)$).
<br>
The closest thing I came to it (considering $\mathbb{E}X_tX_{t}^T = \mathbb{E}X_{t + 1}X_{t + 1}^T = \Sigma $) is solving
$$
\Sigma = T\Sigma T^T + RR^T
$$
But how are these two approaches equivalent and can we really think that $\mathbb{E}X_tX_{t}^T = \mathbb{E}X_{t + 1}X_{t + 1}^T$? There is nothing about it in the specification of the package.</p>
"
"0.124514561272938","0.0705345615858598","225877","<p>In Patton (2011) the author finds that both the MSE and the QLIKE loss function are robust when used to compare rivalling volatility forecasting models, which means that using a proxy for volatility gives the same ranking as using the true (unobservable) volatility of an asset. </p>

<p>In my current project I am comparing a family of GARCH/AGARCH models and while the MSE suggests that nothing outperforms a GARCH(1,1), the QLIKE statistic suggests that an APARCH(1,1) model performs significantly better.</p>

<p>Is this caused by the two loss functions penalising deviations differently?
Specifically, what do the two loss functions place the highest penalty on, i.e. how do I interpet this?</p>

<p>I am hoping this is not down to some trivial coding error.</p>

<pre><code>#MSE 
MSE&lt;-function(sigmafc,RV){
  MSE=1/length(sigmafc)*sum((sigmafc^2-RV)^2)
  return(MSE)
}

#QLIKE
QLIKE&lt;-function(sigmafc,RV){
  varfc=sigmafc^2
  QLIKE=sum(
    (RV/varfc-log(RV/varfc)-1)
    )
  return(QLIKE)
}
</code></pre>

<p>I gather that the MSE depends on forecast errors, while QLIKE depends on standardised errors, but how would I interpret this?</p>
"
"0.206484040339026","0.212669705029557","229721","<p>I have 4 years electrical load data. I split the data into 3 years (75%) training data, 1 year for testing (25%). Also I have the temperature data for each day during the previous period. (The link to the dataset: <a href=""https://drive.google.com/open?id=0B08HdcWBksWcTUxqc1ByOW1UVEU"" rel=""nofollow"">here</a>.) </p>

<p>I want to make use of the temperature data to enhance the forecasting using argument <code>xreg</code> in <code>arima</code> function. </p>

<p>Here is my code:</p>

<pre><code>mydata1&lt;-read.csv(""1st pape/kaggle_data.csv"");
mydata&lt;-ts(mydata1[,2],start = c(2004),frequency = 365)

#split the data into trainData and test data
trainData = window(mydata, end=c(2007))
testData = window(mydata, start=c(2007))
temp&lt;-ts(mydata1[,3],start = c(2004),frequency = 365)

#split the temperature into trainData and test data
trainReg = window(temp, end=c(2007))
testReg = window(temp, start=c(2007))
</code></pre>

<p>Apply ARIMA model without using <code>xreg</code>:</p>

<pre><code>mod_arima &lt;- auto.arima(trainData, ic='aicc', stepwise=FALSE)
summary(mod_arima)
Series: trainData 
ARIMA(1,0,3) with non-zero mean 

Coefficients:
         ar1      ma1      ma2      ma3  intercept
      0.9642  -0.2098  -0.2157  -0.1693  24008.122
s.e.  0.0110   0.0322   0.0330   0.0325   1018.007

sigma^2 estimated as 9318421:  log likelihood=-10347.38
AIC=20706.75   AICc=20706.83   BIC=20736.75

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 6.102332 3045.638 2293.946 -1.519484 9.625694 0.5151126
                    ACF1
Training set 0.004483007

plot(forecast(mod_arima)); lines(testData , col=""red"", start= c(2007,1,1)); 
legend(""topleft"", lty=1,col=c(4,2),legend=c(""forecasted data"",""real data""))

y &lt;- msts(trainData, c(7,365)) # multiseasonal ts
x &lt;- msts(trainReg, c(7,365)) # multiseasonal ts

fit &lt;- auto.arima(y, xreg=(fourier(y, K=c(3,30))))
fit_f &lt;- forecast(fit, xreg= fourier(y, K=c(3,30), 365), 365)
plot(fit_f)
</code></pre>

<p>the red line is the actual data, while the blue is the foretasted data. The left plot is appeared before using fourier function, while the right after using it. </p>

<p><a href=""http://i.stack.imgur.com/QxvKC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QxvKC.png"" alt=""enter image description here""></a></p>

<p>Apply ARIMA model using <code>xreg</code>:</p>

<pre><code>mod_arima2 &lt;- auto.arima(trainData ,xreg = trainReg, ic='aicc', stepwise=FALSE)
summary(mod_arima2)
Series: trainData 
ARIMA(1,0,3) with non-zero mean 

Coefficients:
         ar1      ma1      ma2      ma3  intercept  trainReg
      0.9709  -0.2403  -0.2108  -0.1609  29984.188  -88.3976
s.e.  0.0094   0.0320   0.0330   0.0321   1468.108   13.1966

sigma^2 estimated as 8955023:  log likelihood=-10325.13
AIC=20664.26   AICc=20664.36   BIC=20699.26

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 6.030471 2984.292 2267.803 -1.464553 9.529988 0.5092422
                    ACF1
Training set 0.005526977

plot(forecast(mod_arima2,xreg = testReg)); lines(testData , col=""red"", start= c(2007,1,1)); 
legend(""topleft"", lty=1,col=c(4,2), legend=c(""forecasted data"",""real data""))

l = (fourier(y, K=c(3,30)))
z = cbind(l,x)
fit2 &lt;- auto.arima(y, xreg=z)
fit_f2 &lt;- forecast(fit, xreg= z, 365)
plot(fit_f2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/TgJE5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TgJE5.png"" alt=""enter image description here""></a>
<strong>Questions</strong>:</p>

<ol>
<li>Did I use <code>xreg</code> correctly?</li>
<li>If yes, why is the summary the same without using <code>xreg</code>?</li>
<li>Why are the forecasts far away from the real data?</li>
</ol>
"
"0.062257280636469","0.0705345615858598","230662","<p>The Paper <strong><em>Triple seasonal methods for short-term electricity demand forecasting</em></strong> by <strong><em>Taylor(2010)</em></strong> mentioned about Triple Seasonal Holt Winters Model, but in I can only find the function for Double Seasonal <code>dshw()</code> in R. Does anyone know whether there is a R function for the triple seasonal one?</p>
"
"0.139211511597426","0.157720074469128","231860","<p>I am trying to compare two forecasts using the Mariano Diebold test in R. Both  forecasts are for 150 days ahead; that is, on day $t$ I forecast $t+1, t+2, \dotsc, t+150$. </p>

<p>I deduced from <a href=""http://stats.stackexchange.com/questions/137261/understanding-forecast-horizon-for-diebold-mariano-tests"">this</a> post that my forecast horizon $h=150$. Using that, the Diebold-Mariano test (implemented using function <code>dm.test</code> in ""forecast"" package in R) gives a p-value of 1 no matter what forecasts I compare. </p>

<p>I looked into the code of this function, and I figured that this is caused by the following 3 lines of code (<code>d</code> is the vector of loss-differential series):</p>

<pre><code>n &lt;- length(d)
k &lt;- ((n + 1 - 2 * h + (h/n) * (h - 1))/n)^(1/2)   
STATISTIC &lt;- STATISTIC * k
</code></pre>

<p>Since in my case <code>n = h</code>, the variable <code>k</code> will always be 0 and therefore the test statistic is always zero. </p>

<p><strong>Questions:</strong></p>

<ol>
<li>Does this mean that we cannot use the Diebold-Mariano test when we are forecasting an entire period at once? I could not find any evidence of this in their paper. </li>
<li>How should I proceed to find a model-based way to compare my forecasts, rather than for example simply taking the MSE?</li>
</ol>
"
"0.139211511597426","0.126176059575302","232590","<p>I have a number of groups with monthly data from 2010 to 2016. It's over 80 groups. I succesfully ran an ARIMA model with the montly data but with the sales data summed up (without groups). </p>

<p>Now I'd like to compare the performance with a per group model that runs an ARIMA model for each group and maybe later consider another type of grouping (geographical location, clustering, etc.)</p>

<p>I ran my original model with the following code:</p>

<pre><code>        Datos &lt;- read.csv(""C:/Users/borja.sanz/Desktop/Borja/Forecasting/V`enter code here`entas/Datos para Forecast.csv"")
        options(scipen=999)
        library(lubridate)
        Datos$Fecha = dmy(Datos$Fecha)

        #Declare time series
        tsDatos&lt;-ts(Datos$VentaLocal,start = c(2010,1),frequency = 12)
        plot(tsDatos)
        library(forecast)
        library(dplyr)

        #AutoArima Model
        m_aa = auto.arima(tsDatos)
        f_aa = forecast(m_aa, h=36)
        plot(f_aa)

#Create the forecasts along with the lower and upper bound
    forecast_df = data.frame(prediction=f_aa$mean,
                             abajo=f_aa$lower[,2],
                             arriba=f_aa$upper[,2],
                             date=last_date + seq(1/12, 3, by=1/12))
    forecast_df
</code></pre>

<p>This is how my data looks like:</p>

<pre><code>       Group    Year    Month   Date    Sales
1   2010    1   1/01/2010   134536.625
1   2010    2   1/02/2010   117506.625
1   2010    3   1/03/2010   132153.75
1   2010    4   1/04/2010   129723.125
1   2010    5   1/05/2010   135834.5
1   2010    6   1/06/2010   130115.375
1   2010    7   1/07/2010   144716
1   2010    8   1/08/2010   137195
1   2010    9   1/09/2010   137522.875
1   2010    10  1/10/2010   187063
1   2010    11  1/11/2010   162002.75
1   2010    12  1/12/2010   262297.375
1   2011    1   1/01/2011   177291.25
1   2011    2   1/02/2011   154816
1   2011    3   1/03/2011   171231.125
1   2011    4   1/04/2011   217717
1   2011    5   1/05/2011   178767.75
1   2011    6   1/06/2011   180817.75
1   2011    7   1/07/2011   216927.125
1   2011    8   1/08/2011   204509.125
1   2011    9   1/09/2011   199449.5
1   2011    10  1/10/2011   243812.125
1   2011    11  1/11/2011   232135.875
1   2011    12  1/12/2011   330854.75
1   2012    1   1/01/2012   217123.875
1   2012    2   1/02/2012   200558
1   2012    3   1/03/2012   215689.5
1   2012    4   1/04/2012   245500.25
1   2012    5   1/05/2012   219687.25
1   2012    6   1/06/2012   243345.625
1   2012    7   1/07/2012   249042
1   2012    8   1/08/2012   198443.75
1   2012    9   1/09/2012   209157.375
1   2012    10  1/10/2012   234089
1   2012    11  1/11/2012   237531
1   2012    12  1/12/2012   365301.25
1   2013    1   1/01/2013   211129.375
1   2013    2   1/02/2013   185249.625
1   2013    3   1/03/2013   256565.625
1   2013    4   1/04/2013   183549.5
1   2013    5   1/05/2013   189698.25
1   2013    6   1/06/2013   207955.625
1   2013    7   1/07/2013   230764.125
1   2013    8   1/08/2013   212551.625
1   2013    9   1/09/2013   201329.5
1   2013    10  1/10/2013   242745.125
1   2013    11  1/11/2013   261893.375
1   2013    12  1/12/2013   418313.25
1   2014    1   1/01/2014   205532.75
1   2014    2   1/02/2014   170487.75
1   2014    3   1/03/2014   196077
1   2014    4   1/04/2014   221760.875
1   2014    5   1/05/2014   198185
1   2014    6   1/06/2014   204919.25
1   2014    7   1/07/2014   218972.75
1   2014    8   1/08/2014   221439.875
1   2014    9   1/09/2014   195888.375
1   2014    10  1/10/2014   234595.75
1   2014    11  1/11/2014   259712.875
1   2014    12  1/12/2014   355691.875
1   2015    1   1/01/2015   205156.25
1   2015    2   1/02/2015   185358.875
1   2015    3   1/03/2015   218555.75
1   2015    4   1/04/2015   204233.625
1   2015    5   1/05/2015   212160.625
1   2015    6   1/06/2015   207217.25
1   2015    7   1/07/2015   225723.75
1   2015    8   1/08/2015   205902.625
1   2015    9   1/09/2015   196940.625
1   2015    10  1/10/2015   250916
1   2015    11  1/11/2015   236835.125
1   2015    12  1/12/2015   358327.625
2   2010    1   1/01/2010   227175.875
2   2010    2   1/02/2010   205042
2   2010    3   1/03/2010   239206.375
2   2010    4   1/04/2010   212059.875
2   2010    5   1/05/2010   232789
2   2010    6   1/06/2010   247876.125
2   2010    7   1/07/2010   278557
2   2010    8   1/08/2010   270410.125
2   2010    9   1/09/2010   251060.375
2   2010    10  1/10/2010   302738.625
2   2010    11  1/11/2010   266869.75
2   2010    12  1/12/2010   272978.75
2   2011    1   1/01/2011   238614.5
2   2011    2   1/02/2011   224240.375
2   2011    3   1/03/2011   245457.375
2   2011    4   1/04/2011   238583.5
2   2011    5   1/05/2011   252392.75
2   2011    6   1/06/2011   256749.5
2   2011    7   1/07/2011   264736.125
2   2011    8   1/08/2011   256414
2   2011    9   1/09/2011   242335.125
2   2011    10  1/10/2011   305224.75
2   2011    11  1/11/2011   289199.875
2   2011    12  1/12/2011   281807.75
2   2012    1   1/01/2012   244886.125
2   2012    2   1/02/2012   232062.375
2   2012    3   1/03/2012   264991.75
2   2012    4   1/04/2012   232750.5
2   2012    5   1/05/2012   248498.375
2   2012    6   1/06/2012   264290.875
2   2012    7   1/07/2012   272689.75
2   2012    8   1/08/2012   260441.25
2   2012    9   1/09/2012   251852.375
2   2012    10  1/10/2012   305929.625
2   2012    11  1/11/2012   276711.625
2   2012    12  1/12/2012   278672.875
2   2013    1   1/01/2013   242613.875
2   2013    2   1/02/2013   227575.75
2   2013    3   1/03/2013   250318.875
2   2013    4   1/04/2013   250150.375
2   2013    5   1/05/2013   258467.25
2   2013    6   1/06/2013   261359.25
2   2013    7   1/07/2013   279113.75
2   2013    8   1/08/2013   258699
2   2013    9   1/09/2013   244841.375
2   2013    10  1/10/2013   308197.25
2   2013    11  1/11/2013   284195.5
2   2013    12  1/12/2013   287718.75
2   2014    1   1/01/2014   239510.375
2   2014    2   1/02/2014   216338.125
2   2014    3   1/03/2014   245626.75
2   2014    4   1/04/2014   230619.875
2   2014    5   1/05/2014   251758.875
2   2014    6   1/06/2014   254946.75
2   2014    7   1/07/2014   276268.75
2   2014    8   1/08/2014   266151.75
2   2014    9   1/09/2014   245859.375
2   2014    10  1/10/2014   317797.5
2   2014    11  1/11/2014   283786.625
2   2014    12  1/12/2014   289767.875
2   2015    1   1/01/2015   244008
2   2015    2   1/02/2015   228638
2   2015    3   1/03/2015   260056
2   2015    4   1/04/2015   232560.875
2   2015    5   1/05/2015   252642.125
2   2015    6   1/06/2015   249018.5
2   2015    7   1/07/2015   278113.125
2   2015    8   1/08/2015   255851
2   2015    9   1/09/2015   263046.625
2   2015    10  1/10/2015   344240.75
2   2015    11  1/11/2015   295486.125
2   2015    12  1/12/2015   293499.375
</code></pre>

<p>I only included two groups in the sample. I would like to use a function like one of the apply (tapply, lapply, sapply, etc.) that can run an AUTO.ARIMA model per group. Then I would like to obtain the forecast for each group for x number of months and also if I could visualize the model coefficients.</p>
"
