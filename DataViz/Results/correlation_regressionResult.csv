"V1","V2","V3","V4"
"0.132686223108569","0.130521167355216","  3841","<p>I have two years of data which looks basically like this:</p>

<p>Date   <strong><em>_</em>__<em></strong>    Violence Y/N? _</em>  Number of patients</p>

<p>1/1/2008    <strong><em>_</em>___<em></strong>    0  <strong></em>__<em>_</em>__<em>_</em>____</strong> 11</p>

<p>2/1/2008 <strong><em>_</em>__<em>_</em></strong>       0  <strong><em>_</em>__<em>_</em>__<em>_</em>__</strong> 11</p>

<p>3/1/2008 <strong><em>_</em>____</strong><em>1  <strong></em>__<em>_</em>__<em>_</em>____</strong> 12</p>

<p>4/1/2008 <strong><em>_</em>____</strong><em>0  <strong></em>__<em>_</em>__<em>_</em>____</strong> 12</p>

<p>...</p>

<p>31/12/2009_<strong><em>_</em>__</strong>      0_<strong><em>_</em>__<em>_</em>__<em>_</em>__</strong>                 14</p>

<p>i.e. two years of observations, one per day, of a psychiatric ward, which indicate whether there was a violence incident on that day (1 is yes, 0 no) as well as the number of patients on the ward. The hypothesis that we wish to test is that more patients on the ward is associated with an increased probability of violence on the ward.</p>

<p>We realise, of course, that we will have to adjust for the fact that when there are more patients on the ward, violence is more likely because there are just more of them- we are interested in whether each individualâ€™s probability of violence goes up when there are more patients on the ward.</p>

<p>I've seen several papers which just use logistic regression, but I think that is wrong because there is an autoregressive structure (although, looking at the autocorrelation function, it doesnâ€™t get above .1 at any lag, although this is above the â€œsignificantâ€ blue dashed line that R draws for me).</p>

<p>Just to make things more complicated, I can if I wish to break down the results into individual patients, so the data would look just as it does above, except I would have the data for each patient, 1/1/2008, 2/1/2008 etc. and an ID code going down the side so the data would show the whole history of incidents for each patient separately (although not all patients are present for all days, not sure whether that matters).</p>

<p>I would like to use lme4 in R to model the autoregressive structure within each patient, but some Googling comes up with the quotation â€œlme4 is not set up to deal with autoregressive structuresâ€. Even if it were, Iâ€™m not sure I grasp how to write the code anyway.</p>

<p>Just in case anyone notices, I asked a question like this a while ago, they are different datasets with different problems, although actually solving this problem will help with that one (someone suggested I use mixed methods previously, but this autoregression thing has made me unsure how to do this).</p>

<p>So Iâ€™m a bit stuck and lost to be honest. Any help gratefully received!</p>
"
"0.139648219234718","0.137369563821848","  5087","<p>There are numerous procedures for functional data clustering based on orthonormal basis functions. I have a series of models built with the GAMM models, using the <code>gamm()</code> from the mgcv package in R. For fitting a long-term trend, I use a thin plate regression spline. Next to that, I introduce a CAR1 model in the random component to correct for autocorrelation. For more info, see eg the paper of Simon Wood on <a href=""http://r.789695.n4.nabble.com/attachment/2063352/0/tprs.pdf"">thin plate regression splines</a> or his <a href=""http://rads.stackoverflow.com/amzn/click/1584884746"">book on GAM models</a>.</p>

<p>Now I'm a bit puzzled in how I get the correct coefficients out of the models. And I'm even less confident that the coefficients I can extract, are the ones I should use to cluster different models. </p>

<p>A simple example, using:</p>

<pre><code>#runnable code
require(mgcv)
require(nlme)
library(RLRsim)
library(RColorBrewer)

x1 &lt;- 1:1000
x2 &lt;- runif(1000,10,500)

fx1 &lt;- -4*sin(x1/50)
fx2 &lt;- -10*(x2)^(1/4)
y &lt;- 60+ fx1 + fx2 + rnorm(1000,0,5)

test &lt;- gamm(y~s(x1)+s(x2))
# end runnable code
</code></pre>

<p>Then I can construct the original basis using smoothCon :</p>

<pre><code>#runnable code
um &lt;- smoothCon(s(x1),data=data.frame(x1=x1),
         knots=NULL,absorb.cons=FALSE)
#end runnable code
</code></pre>

<p>Now,when I look at the basis functions I can extract using </p>

<pre><code># runnable code
X &lt;- extract.lmeDesign(test$lme)$X
Z &lt;- extract.lmeDesign(test$lme)$Z

op &lt;- par(mfrow=c(2,5),mar=c(4,4,1,1))
plot(x1,X[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,X[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,8],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,7],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,6],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,5],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,4],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,3],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
par(op)
# end runnable code
</code></pre>

<p>they look already quite different. I can get the final coefficients used to build the smoother by</p>

<pre><code>#runnable code
Fcoef &lt;- test$lme$coef$fixed
Rcoef &lt;- unlist(test$lme$coef$random)
#end runnable code
</code></pre>

<p>but I'm far from sure these are the coefficients I look for. I fear I can't just use those coefficients as data in a clustering procedure. I would really like to know which coefficients are used to transform the basis functions from the ones I get with <code>smoothCon()</code> to the ones I extract from the lme-part of the gamm-object. And if possible, where I can find them. I've read the related articles, but somehow I fail to figure it out myself. All help is appreciated.</p>
"
"0.10277830647413","0.101101261498861","  5517","<p>The library languageR provides a method (pvals.fnc) to do MCMC significance testing of the fixed effects in a mixed effect regression model fit using lmer.  However, pvals.fnc gives an error when the lmer model includes random slopes.  </p>

<p>Is there a way to do an MCMC hypothesis test of such models?<br>
If so, how?  (To be accepted an answer should have a worked example in R)
If not, is there a conceptual/computation reason why there is no way?</p>

<p>This question might be related to <a href=""http://stats.stackexchange.com/questions/152/is-there-a-standard-method-to-deal-with-label-switching-problem-in-mcmc-estimatio"">this one</a> but I didn't understand the content there well enough to be certain.</p>

<p><strong>Edit 1</strong>: A proof of concept showing that pvals.fnc() still does 'something' with lme4 models, but that it doesn't do anything with random slope models.</p>

<pre><code>library(lme4)
library(languageR)
#the example from pvals.fnc
data(primingHeid) 
# remove extreme outliers
primingHeid = primingHeid[primingHeid$RT &lt; 7.1,]
# fit mixed-effects model
primingHeid.lmer = lmer(RT ~ RTtoPrime * ResponseToPrime + Condition + (1|Subject) + (1|Word), data = primingHeid)
mcmc = pvals.fnc(primingHeid.lmer, nsim=10000, withMCMC=TRUE)
#Subjects are in both conditions...
table(primingHeid$Subject,primingHeid$Condition)
#So I can fit a model that has a random slope of condition by participant
primingHeid.lmer.rs = lmer(RT ~ RTtoPrime * ResponseToPrime + Condition + (1+Condition|Subject) + (1|Word), data = primingHeid)
#However pvals.fnc fails here...
mcmc.rs = pvals.fnc(primingHeid.lmer.rs)
</code></pre>

<p>It says: Error in pvals.fnc(primingHeid.lmer.rs) : 
  MCMC sampling is not yet implemented in lme4_0.999375
  for models with random correlation parameters</p>

<p>Additional question:  Is pvals.fnc performing as expected for random intercept model?  Should the outputs be trusted?</p>
"
"0.0593390829096927","0.0583708405417779","  6268","<p>I'm searching how to (visually) explain simple linear correlation to first year students.</p>

<p>The classical way to visualize would be to give an Y~X scatter plot with a straight regression line.</p>

<p>Recently, I came by the idea of extending this type of graphics by adding to the plot 3 more images, leaving me with: the scatter plot of y~1, then of y~x, resid(y~x)~x and lastly of residuals(y~x)~1 (centered to the mean)</p>

<p>Here is an example of such a visualization:
<img src=""http://i.stack.imgur.com/Pe2ul.png"" alt=""alt text""></p>

<p>And the R code to produce it:</p>

<pre><code>set.seed(345)
x &lt;- runif(50) * 10
y &lt;- x +rnorm(50)


layout(matrix(c(1,2,2,2,2,3 ,3,3,3,4), 1,10))
plot(y~rep(1, length(y)), axes = F, xlab = """", ylim = range(y))
points(1,mean(y), col = 2, pch = 19, cex = 2)
plot(y~x, ylab = """", )
abline(lm(y~x), col = 2, lwd = 2)

plot(c(residuals(lm(y~x)) + mean(y))~x, ylab = """", ylim = range(y))
abline(h =mean(y), col = 2, lwd = 2)

plot(c(residuals(lm(y~x)) + mean(y))~rep(1, length(y)), axes = F, xlab = """", ylab = """", ylim = range(y))
points(1,mean(y), col = 2, pch = 19, cex = 2)
</code></pre>

<p>Which leads me to my question: I would appreciate any <strong>suggestions on how this graph can be enhanced</strong> (either with text, marks, or any other type of relevant visualizations). Adding relevant R code will also be nice.</p>

<p>One direction is to add some information of the R^2 (either by text, or by somehow adding lines presenting the magnitude of the variance before and after the introduction of x)
Another option is to highlight one point and showing how it is ""better explained"" thanks to the regression line.  Any input will be appreciated.</p>
"
"0.0839181358296689","0.0825488343412996","  7899","<p>I need to draw a complex graphics for visual data analysis.
I have 2 variables and a big number of cases (>1000). For example (number is 100 if to make dispersion less ""normal""):</p>

<pre><code>x &lt;- rnorm(100,mean=95,sd=50)
y &lt;- rnorm(100,mean=35,sd=20)
d &lt;- data.frame(x=x,y=y)
</code></pre>

<p>1) I need to plot raw data with point size, corresponding the relative frequency of coincidences, so <code>plot(x,y)</code> is not an option - I need point sizes. What should be done to achieve this?</p>

<p>2) On the same plot I need to plot 95% confidence interval ellipse and line representing change of correlation (do not know how to name it correctly) - something like this:</p>

<pre><code>library(corrgram)
corrgram(d, order=TRUE, lower.panel=panel.ellipse, upper.panel=panel.pts)
</code></pre>

<p><img src=""http://i.stack.imgur.com/R561Q.png"" alt=""correlogramm""></p>

<p>but with both graphs at one plot.</p>

<p>3) Finally, I need to draw a resulting linar regression model on top of this all:</p>

<pre><code>r&lt;-lm(y~x, data=d)
abline(r,col=2,lwd=2)
</code></pre>

<p>but with error range... something like on QQ-plot:</p>

<p><img src=""http://i.stack.imgur.com/vgvRr.png"" alt=""QQ-plot""></p>

<p>but for fitting errors, if it is possible.</p>

<p>So the question is: </p>

<p><strong>How to achieve all of this at one graph?</strong></p>
"
"0.0593390829096927","0.0583708405417779","  7975","<p>Having worked mostly with cross sectional data so far and very very recently browsing, scanning stumbling through a bunch of introductory time series literature I wonder what which role explanatory variables are playing in time series analysis. </p>

<p>I would like to <em>explain a trend</em> instead of de-trending.
Most of what I read as an introduction assumes that the series is stemming from some stochastic process. I read about AR(p) and MA processes as well as ARIMA modelling. Wanting to deal with more information than only autoregressive processes I found VAR / VECM and ran some examples, but still I wonder if there is some case that is related closer to what explanatories do in cross sections. </p>

<p>The motivation behind this is that decomposition of my series shows that the trend is the major contributor while remainder and seasonal effect hardly play a role. I would like to explain this trend.</p>

<p>Can / should I regress my series on multiple different series? Intuitively I would use gls because of serial correlation (I am not so sure about the cor structure). I heard about spurious regression and understand that this is a pitfall, nevertheless I am looking for a way to explain a trend. </p>

<p>Is this completely wrong or uncommon? Or have I just missed the right chapter so far?</p>
"
"0.173001668965327","0.170178781630396"," 11498","<p>I'm fitting a linear model where the response is a function both of time and of static covariates (i.e. ones that are independent of time). The ultimate goal is to identify significant effects of the static covariates.</p>

<p>Is this the best general strategy for selecting variables (in R, using the <code>nlme</code> package)? Anything I can do better?</p>

<ol>
<li>Break the data up by groups and plot it against time. For continuous covariates, bin it and plot the data in each bin against time. Use the group-specific trends to make an initial guess at what time terms to include-- time, time^n, sin(2*pi*time)+cos(2*pi*time), log(time), exp(time), etc.</li>
<li>Add one term at a time, comparing each model to its predecessor, never adding a higher order in the absence of lower order terms. Sin and cos are never added separately. <strong><em>Is it acceptable to pass over a term that significantly improves the fit of the model if there is no physical interpretation of that term?</em></strong>.</li>
<li>With the full dataset, use forward selection to add static variables to the model and then relevant interaction terms with each other and with the time terms. <strong><em>I've seen some strong criticism of stepwise regression, but doesn't forward selection ignore significant higher order terms if the lower order terms they depend on are not significant? And I've noticed that it's hard to pick a starting model for backward elimination that isn't saturated, or singular, or fails to converge. How do you decide between variable selection algorithms?</em></strong></li>
<li>Add random effects to the model. <strong><em>Is this as simple as doing the variable selection using <code>lm()</code> and then putting the final formula into <code>lme()</code> and specifying the random effects? Or should I include random effects from the very start?</em></strong>. Compare the fits of models using a random intercept only, a random interaction with the linear time term, and random interaction with each successive time term. </li>
<li>Plot a semivariogram to see if an autoregressive error term is needed. <strong><em>What should a semivariogram look like if the answer is 'no'? A horizontal line? How straight, how horizontal? Does including autoregression in the model again require checking potential variables and interactions to make sure they're still relevant?</em></strong></li>
<li>Plot the residuals to see if the variance changes as a function of fitted value, time, or any of the other terms. If it does, weigh the variances appropriately (for <code>lme()</code>, use the <code>weights</code> argument to specify a <code>varFunc()</code>) and compare to the unweighted model to see if this improves the fit. <strong><em>Is this the right sequence in which to do this step, or should it be done before autocorrelation?</em></strong>.</li>
<li>Do <code>summary()</code> of the fitted model to identify significant coefficients for numeric covariates. Do <code>Anova()</code> of the fitted model to identify significant effects for qualitative covariates.</li>
</ol>
"
"0.0419590679148345","0.0412744171706498"," 12663","<p>I was recently asked:</p>

<ul>
<li>Is there an R implementation of a significance test for testing whether three or more correlations drawn from independent samples are equal?</li>
</ul>

<p>I found <a href=""http://luna.cas.usf.edu/~mbrannic/files/regression/corr1.html#More%20than%20two"" rel=""nofollow"">this formula</a> which wouldn't be too difficult to implement, but I was curious whether there was an existing implementation either based on the linked formula or on some other method.</p>
"
"0.111013258946721","0.0936015800080059"," 13070","<p>Background:
Generally, pooled time-series cross-sectional regressions utilize a strict factor model (i.e. require the covariance of residuals is zero). However, in time series such as security returns where strong comovements exist, the assumption that returns obey a strict factor model is easily rejected. </p>

<p>In an approximate factor model, a moderate level of correlation and autocorrelation among residuals and factors themselves (as opposed to a strict factor model where the correlation of residuals is zero). Approximate factor models allow only correlations that are not marketwide. When we examine different samples at different points in time, approximate factor models admit only local autocorrelation of residuals. This condition guarantees that when the number of factors goes to infinity (i.e., when the number of assets is very large), eigenvalues of the covariance matrix remain bounded. We will assume that autocorrelation functions of residuals decays to zero. </p>

<p>Connor (2007) provides additonal background <a href=""http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1024709"" rel=""nofollow"">here</a>.</p>

<p>QUESTION: What function do I use to construct an approximate factor model in R? Perhaps this is a variation of the GLS procedure.</p>
"
"0.157346504680629","0.165097668682599"," 14694","<p>I have a dataset of genomic information which I'm going to be comparing with various biochemical markers. Unfortunately a lot of the biochemical markers have limited ranges in their assays, so I have a lot of data that looks like ""40"", "">45"", ""35"", "">45"" for tests that have a threshold at 45 (for example).
My intended analysis for most of this data is linear regression in R. So what is the statistically correct way to deal with this data?</p>

<ol>
<li><p>Ignore it, let R cast the values with "">"" to <code>NA</code> and potentially lose information about important associations</p></li>
<li><p>Make the over threshold values equal to the threshold. This has similar problems to 1)</p></li>
<li><p>It depends. Sigh. Could you please give me some pointers as to what other considerations I should be thinking about or information you might need to answer my question? </p></li>
</ol>

<p>Edit: Based on the comments I've given more information about my datasets. The values which are out of range (GFR and Fol) are independent variables which I'll use in linear regression like so:</p>

<pre><code>lm(H~allele+Age+Sex+as.double(GFR)+as.double(Fol))
</code></pre>

<p>GFR looks like: </p>

<pre><code>summary(as.double(GFR)) 
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
31.00   70.00   77.00   75.66   83.00  100.00  105.00
</code></pre>

<p>and appears to be normally distributed:</p>

<pre><code>V = qqnorm(na.omit(as.double(GFR))
cor(V$x, V$y)
[1] 0.9911351
</code></pre>

<p>There are 105 values coded as "">90"" (not sure why the summary said Max is 100) out of 434.</p>

<p>Fol is distributed like so:</p>

<pre><code>summary(as.double(Fol))
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
6.10   23.20   29.80   29.14   35.70   45.30    8.00
</code></pre>

<p>and also appears to be normally distributed:</p>

<pre><code>V  = qqnorm(na.omit(as.double(Fol)))   
cor(V$x, V$y)
[1] 0.9911351
</code></pre>

<p>There are 8 out of 434 variables in Fol coded are "">45.3"". I took my cue for calling these normally distributed from <a href=""http://www.math.utah.edu/~davar/ps-pdf-files/Assessing_Normality.pdf"" rel=""nofollow"">this assessment of normality guide</a> ).</p>

<p>I also have another variable CRP which is a dependent variable, which I'd like to do linear regression on similarly to the above. CRP has 11 out of 434 coded as ""&lt;0.2"". Its distribution is:</p>

<pre><code>summary(as.double(CRP))
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
0.200   0.600   1.300   2.674   2.650 112.400  11.000
</code></pre>

<p>The data graphed is clearly not normaly and it has a correlation with qqnorm of 0.5153663. The value of 112 is a clear outlier. </p>

<p>I hope that makes it more clear. Please let me know if you need more information. Thanks for your help.</p>
"
"NaN","NaN"," 14701","<p>I run into a problem where I would like to build a GEE in R with cubic regression splines (or any other spline type) for a longitudinal data set and an urgent need for grouping and multiple autocorrelation structures. However, I did not find any package capable allowing for that. </p>

<p>If anyone could give a suggestion for a suitable package, I would really appreciate that.</p>
"
"NaN","NaN"," 14914","<p>I have a matrix with two columns that have many prices (750).
In the image below I plotted the residuals of the follow linear regression:</p>

<pre><code>lm(prices[,1] ~ prices[,2])
</code></pre>

<p>Looking at image, seems to be a very strong autocorrelation of the residuals.</p>

<p>However how can I test if the autocorrelation of those residuals is strong? What method should I use?</p>

<p><img src=""http://i.stack.imgur.com/kwASv.jpg"" alt=""Residuals of the linear regression""></p>

<p>Thank you!</p>
"
"0.0685188709827532","0.0842510512490505"," 15145","<p>I have performed the path analysis using the <code>sem</code> function in R. The model which I fitted consists of both direct and indirect paths. I have some trouble in interpreting the estimates of the SEM coefficients. </p>

<ul>
<li>Does R gives the value of total effect = (direct effect + indirect effect) directly or do I have to multiply the coefficients which are on the indirect path and then add them to the coefficients which is on the direct path? This is the usual way of doing path analysis with the raw/absolute correlation coefficients.</li>
</ul>

<p>For example consider X (independent variable), Y (dependent variable) and M (Mediating variable). </p>

<p>The raw/absolute correlation/ standardized regression coefficients between them are X and Y  -0.06; X and M 0.22 and M and Y 0.28 whereas on the path analysis/sem in R, the above coefficients are X and Y -0.13; X and M 0.22 and M and Y 0.31. </p>

<ul>
<li>Thus  is the total effect of X and Y  equal to -0.13?</li>
<li>Alternatively how should I interpret this coefficient considering the effect of variable M into the account?</li>
</ul>
"
"0.0726752374667264","0.0714893875923587"," 15900","<p>I plan to do a simulation study where I compare the performance of several robust correlation techniques with different distributions (skewed, with outliers, etc.). With <em>robust</em>, I mean the ideal case of being robust against a) skewed distributions, b) outliers, and c) heavy tails.</p>

<p>Along with the Pearson correlation as a baseline, I was thinking to include following more robust measures:</p>

<ul>
<li>Spearman's $\rho$</li>
<li>Percentage bend correlation (Wilcox, 1994, [1])</li>
<li>Minimum volume ellipsoid, minimum covariance determinant (<code>cov.mve</code>/ <code>cov.mcd</code> with the <code>cor=TRUE</code> option)</li>
<li>Probably, the winsorized correlation</li>
</ul>

<p>Of course there are many more options (especially if you include robust regression techniques as well), but I want to restrict myself to the mostly used/ mostly promising approaches.</p>

<p><strong>Now I have three questions (feel free to answer only single ones):</strong></p>

<ol>
<li><strong>Are there other robust correlational methods I could/ should include?</strong></li>
<li><strong>Which robust correlation techniques are</strong> <em><strong>actually</em></strong>  <strong>used in your field?</strong>
<sub>(Speaking for psychological research: Except Spearman's $\rho$, I have never seen any robust correlation technique outside of a technical paper. Bootstrapping is getting more and more popular, but other robust statistics are more or less non-existent so far).</sub></li>
<li><strong>Are there already systematical comparisons of multiple correlation techniques that you know of?</strong></li>
</ol>

<p>Also feel free to comment the list of methods given above.</p>

<hr>

<p>[1] Wilcox, R. R. (1994). The percentage bend correlation coefficient. <em>Psychometrika</em>, 59, 601-616.</p>
"
"0.119417600797712","0.130521167355216"," 17480","<p>I've created a few Cox regression models and I would like to see how well these models perform and I thought that perhaps a ROC-curve or a c-statistic might be useful similar to this articles use:</p>

<p><a href=""http://onlinelibrary.wiley.com/doi/10.1002/bjs.6930/abstract"" rel=""nofollow"">J. N. Armitage och J. H. van der Meulen, â€Identifying coâ€morbidity in surgical patients using administrative data with the Royal College of Surgeons Charlson Scoreâ€, British Journal of Surgery, vol. 97, num. 5, ss. 772-781, Maj 2010.</a></p>

<p>Armitage used logistic regression but I wonder if it's possible to use a model from the survival package, the <a href=""http://cran.r-project.org/web/packages/survivalROC/index.html"" rel=""nofollow"">survivalROC</a> gives a hint of this being possible but I can't figure out how to get that to work with a regular Cox regression. </p>

<p>I would be grateful if someone would show me how to do a ROC-analysis on this example:</p>

<pre><code>library(survival)
data(veteran)

attach(veteran)
surv &lt;- Surv(time, status)
fit &lt;- coxph(surv ~ trt + age + prior, data=veteran)
summary(fit)
</code></pre>

<p>If possible I would appreciate both the raw c-statics output and a nice graph</p>

<p>Thanks!</p>

<h2>Update</h2>

<p>Thank you very much for answers. @Dwin: I would just like to be sure that I've understood it right before selecting your answer. </p>

<p>The calculation as I understand it according to DWin's suggestion:</p>

<pre><code>library(survival)
library(rms)
data(veteran)

fit.cph &lt;- cph(surv ~ trt + age + prior, data=veteran, x=TRUE, y=TRUE, surv=TRUE)

# Summary fails!?
#summary(fit.cph)

# Get the Dxy
v &lt;- validate(fit.cph, dxy=TRUE, B=100)
# Is this the correct value?
Dxy = v[rownames(v)==""Dxy"", colnames(v)==""index.corrected""]

# The c-statistic according to the Dxy=2(c-0.5)
Dxy/2+0.5
</code></pre>

<p>I'm unfamiliar with the validate function and bootstrapping but after looking at prof. Frank Harrel's answer <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">here on R-help</a> I figured that it's probably the way to get the Dxy. The help for validate states:</p>

<blockquote>
  <p>... Somers' Dxy rank correlation to be computed at each resample (this
  takes a bit longer than the likelihood based statistics). The values
  corresponting to the row Dxy are equal to 2 * (C - 0.5) where C is the
  C-index or concordance probability.</p>
</blockquote>

<p>I guess I'm mostly confused by the columns. I figured that the corrected value is the one I should use but I haven't really understood the validate output:</p>

<pre><code>      index.orig training    test optimism index.corrected   n
Dxy      -0.0137  -0.0715 -0.0071  -0.0644          0.0507 100
R2        0.0079   0.0278  0.0037   0.0242         -0.0162 100
Slope     1.0000   1.0000  0.2939   0.7061          0.2939 100
...
</code></pre>

<p>In the <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">R-help question</a> I've understood that I should have ""surv=TRUE"" in the cph if I have strata but I'm uncertain on what the purpose of the ""u=60"" parameter in the validate function is. I would be grateful if you could help me understand these and check that I haven't made any mistakes.</p>
"
"0.284644680078012","0.280000101403405"," 18709","<p>I want to fit mixed model using lme4, nlme, baysian regression package or any available. </p>

<p><em><strong>Mixed model in Asreml- R  coding conventions</em></strong></p>

<p>before going into specifics, we might want to have details on asreml-R conventions, for those who are unfamiliar with ASREML codes.</p>

<pre><code>y = XÏ„ + Zu + e ........................(1) ; 
</code></pre>

<p>the usual mixed model with, y denotes the n Ã— 1 vector of observations,where Ï„ is the pÃ—1 vector of ï¬xed eï¬€ects, X is an nÃ—p design matrix of full column rank which associates observations with the appropriate combination of ï¬xed eï¬€ects, u is the q Ã— 1 vector of random eï¬€ects, Z is the n Ã— q design matrix which associates observations with the appropriate combination of random eï¬€ects, and e is the n Ã— 1 vector of residual errors.The model (1) is called a linear mixed model or linear mixed eï¬€ects model. It is assumed </p>

<p><img src=""http://i.stack.imgur.com/gxdur.jpg"" alt=""enter image description here""></p>

<p>where the matrices G and R are functions of parameters Î³ and Ï†, respectively.</p>

<p>The parameter Î¸ is a variance parameter which we will refer to as the scale parameter.</p>

<p>In mixed eï¬€ects models with more than one residual variance, arising for example in the
analysis of data with more than one section or variate, the parameter Î¸ is
ï¬xed to one. In mixed eï¬€ects models with a single residual variance then Î¸ is equal to
the residual variance (Ïƒ2). In this case R must be correlation matrix. Further details on the models are provided in the <a href=""http://www.vsni.co.uk/downloads/asreml/release2/doc/asreml-R.pdf"">Asreml manual (link)</a>. </p>

<p>Variance structures for the errors: R structure and Variance structures for the random eï¬€ects: G structures can be specified.</p>

<p><img src=""http://i.stack.imgur.com/or4Gj.jpg"" alt=""enter image description here""><img src=""http://i.stack.imgur.com/oXTgc.jpg"" alt=""enter image description here""></p>

<p>variance modelling in asreml() it is important to understand the formation of variance structures via direct products. The usual least squares assumption (and the default in asreml()) is that these are independently and identically distributed (IID). However, if the data was from a field experiment laid out in a rectangular array of r rows by c columns, say, we could arrange the residuals e as a matrix and potentially consider that they were autocorrelated within rows and columns.Writing the residuals as a vector in field order, that is, by sorting the residuals rows
within columns (plots within blocks) the variance of the residuals might then be</p>

<p><img src=""http://i.stack.imgur.com/SPE5b.jpg"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/IcikW.jpg"" alt=""enter image description here""> are correlation matrices for the row model (order r, autocorrelation parameter Â½r) and column model (order c, autocorrelation parameter Â½c)
respectively. More specifically, a two-dimensional separable autoregressive spatial structure
(AR1 x Â­ AR1) is sometimes assumed for the common errors in a field trial analysis.</p>

<p><em><strong>The example data:</em></strong></p>

<p>nin89 is from asreml-R library, where different varities were grown in replications / blocks in rectangular field. To control additional variability in row or column direction each plot is referenced as Row and Column variables (row column design). Thus this row column design with blocking. Yield is measured variable. </p>

<p><strong>Example models</strong> </p>

<p>I need something equivalent to the asreml-R codes:</p>

<p>The simple model syntax will look like the follows:</p>

<pre><code> rcb.asr &lt;- asreml(yield âˆ¼ Variety, random = âˆ¼ Replicate, data = nin89)  
 .....model 0
</code></pre>

<p>The linear model is specified in the fixed (required), random (optional) and rcov (error
component) arguments as formula objects.The default is a simple error term and does not need to be formally specified for error term as in the model 0. </p>

<p>here the variety is fixed effect and random is replicates (blocks). Beside random and fixed terms we can specify error term. Which is default in this model 0. The residual or error component of the model is specified in a formula object through the rcov argument, see the following models 1:4. </p>

<p>The following model1 is more complex in which both G (random) and R (error) structure are specified.</p>

<p><strong>Model 1:</strong> </p>

<pre><code>data(nin89)


 # Model 1: RCB analysis with G and R structure
     rcb.asr &lt;- asreml(yield ~ Variety, random = ~ idv(Replicate), 
      rcov = ~ idv(units), data = nin89)
</code></pre>

<p>This model is equivalent to above model 0, and introduces the use of G and R variance model. Here the option random and rcov specifies random and rcov formulae to explicitly specify the G and R structures. where idv() is the special model function in asreml() that identifies the variance model. The expression idv(units) explicitly sets the variance matrix for e to a scaled identity.</p>

<p><em><strong># Model 2: two-dimensional spatial model with correlation in one direction</em></strong></p>

<pre><code>  sp.asr &lt;- asreml(yield ~ Variety, rcov = ~ Column:ar1(Row), data = nin89)
</code></pre>

<p>experimental units of nin89 are indexed by Column and Row. So we expect random variation in two direction - row and column direction in this case. where ar1() is a special function specifying a first order autoregressive variance model for Row. This call specifies a two-dimensional spatial structure for error but with spatial correlation in the row direction only.The variance model for Column is identity (id()) but does not need to be formally
specified as this is the default.</p>

<p><em><strong># model 3: two-dimensional spatial model, error structure in both direction</em></strong></p>

<pre><code> sp.asr &lt;- asreml(yield ~ Variety, rcov = ~ ar1(Column):ar1(Row),  
 data = nin89)
sp.asr &lt;- asreml(yield ~ Variety, random = ~ units, 
 rcov = ~ ar1(Column):ar1(Row), data = nin89)
</code></pre>

<p>similar to above model  2, however the correlation is two direction - autoregressive one. </p>

<p>I am not sure how much of these models are possible with open source R packages. Even if solution of any one of these models will be of great help. <strong><em>Even if the bouty of +50 can stimulate to develop such package will be of great help !</em></strong></p>

<p><em><strong>See MAYSaseen has provided output from each model and data  (as answer)  for comparision.</em></strong> </p>

<p><em><strong>Edits: 
The following is suggestion I received in mixed model discussion forum:</em></strong>
"" You might look at the regress and spatialCovariance packages of David Clifford.  The former allows fitting of (Gaussian) mixed models where you can specify the structure of the covariance matrix very flexibly (for example, I have used it for pedigree data).  The spatialCovariance package uses regress to provide more elaborate models than AR1xAR1, but may be applicable.  You may have to correspond with the author about applying it to your exact problem."" </p>
"
"0.126511349842314","0.124447050177862"," 19772","<p>Can someone please tell me how to have R estimate the break point in a piecewise linear model (as a fixed or random parameter), when I also need to estimate other random effects? </p>

<p>I've included a toy example below that fits a hockey stick / broken stick regression with random slope variances and a random y-intercept variance for a break point of 4. I want to estimate the break point instead of specifying it. It could be a random effect (preferable) or a fixed effect.</p>

<pre><code>library(lme4)
str(sleepstudy)

#Basis functions
bp = 4
b1 &lt;- function(x, bp) ifelse(x &lt; bp, bp - x, 0)
b2 &lt;- function(x, bp) ifelse(x &lt; bp, 0, x - bp)

#Mixed effects model with break point = 4
(mod &lt;- lmer(Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject), data = sleepstudy))

#Plot with break point = 4
xyplot(
        Reaction ~ Days | Subject, sleepstudy, aspect = ""xy"",
        layout = c(6,3), type = c(""g"", ""p"", ""r""),
        xlab = ""Days of sleep deprivation"",
        ylab = ""Average reaction time (ms)"",
        panel = function(x,y) {
        panel.points(x,y)
        panel.lmline(x,y)
        pred &lt;- predict(lm(y ~ b1(x, bp) + b2(x, bp)), newdata = data.frame(x = 0:9))
            panel.lines(0:9, pred, lwd=1, lty=2, col=""red"")
        }
    )
</code></pre>

<p>Output:</p>

<pre><code>Linear mixed model fit by REML 
Formula: Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject) 
   Data: sleepstudy 
  AIC  BIC logLik deviance REMLdev
 1751 1783 -865.6     1744    1731
Random effects:
 Groups   Name         Variance Std.Dev. Corr          
 Subject  (Intercept)  1709.489 41.3460                
          b1(Days, bp)   90.238  9.4994  -0.797        
          b2(Days, bp)   59.348  7.7038   0.118 -0.008 
 Residual               563.030 23.7283                
Number of obs: 180, groups: Subject, 18

Fixed effects:
             Estimate Std. Error t value
(Intercept)   289.725     10.350  27.994
b1(Days, bp)   -8.781      2.721  -3.227
b2(Days, bp)   11.710      2.184   5.362

Correlation of Fixed Effects:
            (Intr) b1(D,b
b1(Days,bp) -0.761       
b2(Days,bp) -0.054  0.181
</code></pre>

<p><img src=""http://i.stack.imgur.com/HnAfg.jpg"" alt=""Broken stick regression fit to each individual""></p>
"
"0.0419590679148345","0.0412744171706498"," 20438","<p>I fitted a GEE model using the function <code>genZcor</code> with user defined
correlation matrix. I want to get the var-cov matrix of the regression
coefficients. But the output provides only limited information.</p>

<p>I would be very much thankful if you could kindly let me know how to get
it since I am struggling lot getting this.</p>
"
"0.125877203744503","0.123823251511949"," 20613","<p>Hello statistical gurus and R programming wizards,</p>

<p>I am interested in modeling animal captures as a function of environmental conditions and day of the year. As part of another study, I have counts of captures on ~160 days over three years. On each of these days I have temperature, rainfall, windspeed, relative humidity, etc. Because the data was collected repeatedly from the same 5 plots, I use plot as a random effect.</p>

<p>My understanding is that nlme can easily account for temporal autocorrelation in the residuals but doesn't handle non-Gaussian link functions like lme4 (which can't handle the autocorrelation?). Currently, I think it might work to use the nlme package in R on log(count). So my solution right now would be to run something like:</p>

<pre><code>m1 &lt;- lme(lcount ~ AirT + I(AirT^2) + RainAmt24 + I(RainAmt24^2) + RHpct + windspeed + 
      sin(2*pi/360*DOY) + cos(2*pi/360*DOY), random = ~1|plot, correlation =
      corARMA(p = 1, q = 1, form = ~DOY|plot), data = Data)
</code></pre>

<p>where DOY = Day of the Year. There may be more interactions in the final model, but this is my general idea. I could also potentially try to model the variance structure further with something like </p>

<pre><code>weights = v1Pow
</code></pre>

<p>I'm not sure if there is a better way to do with with a Poisson mixed model regression or anything? I just found mathematical discussion in Chapter 4 of ""Regression Models for Time Series Analysis"" by Kedem and Fokianos. It was a bit beyond me at the moment, especially in application (coding it in R). I also saw a MCMC solution in Zuur et al. Mixed Effects Models book (Chp 23) in the BUGS language (using winBUGS or JAG). Is that my best option? Is there an easy MCMC package in R that would handle this? I'm not really familiar with GAMM or GEE techniques but would be willing to explore these possibilities if people thought they'd provide better insight. <strong>My main objective is to create a model to predict animal captures given environmental conditions. Secondarily, I would like to explain what the animals a responding to in terms of their activity.</strong></p>

<p>Any thoughts on the best way to proceed (philosophically), how to code this in R, or in BUGS would be appreciated. I'm fairly new to R and BUGS (winBUGS) but am learning. This is also the first time I've ever tried to address temporal autocorrelation.</p>

<p>Thanks,
Dan</p>
"
"0.10277830647413","0.101101261498861"," 20672","<p>I have two continuous variables, X and Y, that are correlated - they are not independent. To correct for non-independence, I have a known correlation structure, a matrix S.</p>

<p>If one calls <code>gls(Y ~ X, correlation = S)</code>, what I think happens is that, internally, gls() transforms X and Y in some way so that the regression ends up being <code>S^(-1)*Y = S^(-1) * X</code>.</p>

<p>How is this transformation actually performed? From the literature I've consulted, I've seen everything from:</p>

<pre><code>X.transformed &lt;- solve(chol(S)) %*% X 
#The inverse of the Choleski decomposition of S times the vertical vector X, 
#which in my case does nothing to the data
</code></pre>

<p>to</p>

<pre><code>X.transformed &lt;- chol(solve(S)) %*% X 
# which has negative values and gives meaningless values of X
</code></pre>

<p>Another method I've seen is transforming the dependent variable by </p>

<pre><code>chol(solve(S)) %*% Y 
</code></pre>

<p>and the independent variable by </p>

<pre><code>chol(solve(S)) %*% cbind(1,X) 
</code></pre>

<p>and doing the linear model using the transformed intercept terms in the first column of the X matrix: </p>

<pre><code>lm(Y ~ X - 1)
</code></pre>

<p>On a related note, is there any point to manually transforming the data in order to plot it? Do the transformed values have any meaning, or are they simply there to estimate regression coefficients? (In other words, if X is a variable of body mass figures, X values are not necessarily errant if they're negative since they're still linear?) I suppose it would follow from this that an $R^2$ statistic on transformed variables is also meaningless?</p>
"
"0.0593390829096927","0.0583708405417779"," 21715","<p>There are several packages that can apply the Durbin-Watson test for serial correlation. However, I do not see a package that supports the calculation in the case that once has a GLS weighted regression.</p>

<p>For example, CRAN package <a href=""http://cran.r-project.org/web/packages/lmtest/index.html"" rel=""nofollow"">lmtest</a> notes in their <a href=""http://cran.r-project.org/web/packages/lmtest/NEWS"" rel=""nofollow"">changelog</a> that they explicitly do not support weighted regressions (yet). Before the recent release, lmtest would not throw an error when passing weighted regressions. </p>

<p>My concern is that the other dwtest packages may also not be explicitly dealing with this scenario.</p>
"
"0.16250677125654","0.159855130326344"," 23795","<p>I am using a relevance vector machine as implemented in the kernlab-package in R, trained on a dataset with 360 continuous variables (features) and 60 examples (also continuous, so it's a relevance vector regression).</p>

<p>I have several datasets with equivalent dimensions from different subjects. Now it works fine for most of the subjects, but with one particular dataset, I get this strange results:</p>

<p>When using leave-one-out cross validation (so I train the RVM and try to subsequently predict one observation that was left out of the training), most of the predicted values are just around the mean of the example-values.
So I really don't get good predictions, but just a slightly different value than the mean.</p>

<p>It seems like the SVM is not working at all;
When I plot the fitted values against the actual values, I see the same pattern; predictions around the mean. So the RVM is not even able to predict the values it was trained on (for the other datasets I get correlations of around .9 between fitted and actual values).</p>

<p>It seems like, that I can at least improve the fitting (so that the RVM is at least able to predict the values it was trained on) by transforming the dependent variable (the example-values), for example by taking the square root of the dependent variable.</p>

<p>so this is the output for the untransformed dependent variable:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 5 
Variance :  1407.006
Training error : 1383.534902093 
</code></pre>

<p>this, if I first transform the dependent variable by taking the square root:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 55 
Variance :  1.711355
Training error : 0.89601609 
</code></pre>

<p>How is it, that the RVM-results change so dramatically, just by transforming the dependent variable? And what is going wrong, when an SVM just predicts values around the mean of the dependent variable (even for the values and observations it was trained on)?</p>
"
"0.0419590679148345","0.0412744171706498"," 24445","<p>I'm trying to estimate a multiple linear regression in R with an equation like this:</p>

<pre><code>regr &lt;- lm(rate ~ constant + askings + questions + 0)
</code></pre>

<p>askings and questions are quarterly data time-series, constructed with <code>askings &lt;- ts(...)</code>.</p>

<p>The problem now is that I got autocorrelated residuals. I know that it is possible to fit the regression using the gls function, but I don't know how to identify the correct AR or ARMA error structure which I have to implement in the gls function. </p>

<p>I would try to estimate again now with,</p>

<pre><code>gls(rate ~ constant + askings + questions + 0, correlation=corARMA(p=?,q=?))
</code></pre>

<p>but I'm unfortunately neither an R expert nor an statistical expert in general to identify p and q.</p>

<p>I would be pleased If someone could give me a useful hint.
Thank you very much in advance!</p>

<p>Jo</p>
"
"0.173001668965327","0.170178781630396"," 24452","<p>I hope you all don't mind this question, but I need help interpreting output for a linear mixed effects model output I've been trying to learn to do in R. I am new to longitudinal data analysis and linear mixed effects regression. I have a model I fitted with weeks as the time predictor, and score on an employment course as my outcome. I modeled score with weeks (time) and several fixed effects, sex and race. My model includes random effects. I need help understanding what the variance and correlation means. The output is the following:</p>

<pre><code>Random effects  
Group   Name    Variance  
EmpId intercept 680.236  
weeks           13.562  
Residual 774.256  
</code></pre>

<p>The correlaton is .231.</p>

<p>I can interpret the correlation as there is a a positive relationship between weeks and score but I want to be able to say it in terms of ""23% of ..."".</p>

<p>I really appreciate the help. </p>

<hr>

<p>Thanks ""guest"" and Macro for replying. Sorry, for not replying, I was out at a conference and Iâ€™m now catching up. 
Here is the output and the context. </p>

<p>Here is the summary for the LMER model I ran. </p>

<pre><code>&gt;summary(LMER.EduA)  
Linear mixed model fit by maximum likelihood  
Formula: Score ~ Weeks + (1 + Weeks | EmpID)   
   Data: emp.LMER4 

  AIC     BIC   logLik   deviance   REMLdev   
 1815     1834  -732.6     1693    1685

Random effects:    
 Groups   Name       Variance Std.Dev. Corr  
 EmpID   (Intercept)  680.236  26.08133        
          Weeks         13.562 3.682662  0.231   
 Residual             774.256  27.82546        
Number of obs: 174, groups: EmpID, 18


Fixed effects:    
            Estimate Std. Error  t value  
(Intercept)  261.171      6.23     37.25    
Weeks          11.151      1.780    6.93

Correlation of Fixed Effects:  
     (Intr)  
Days -0.101
</code></pre>

<p>I donâ€™t understand how to interpret the variance and residual for the random effects and explain it to someone else. I also donâ€™t know how to interpret the correlation, other than it is positive which indicates that those with higher intercepts have higher slopes and those with those with lower intercepts have lower slopes but I donâ€™t know how to explain the correlation in terms of 23% of . . . . (I donâ€™t know how to finish the sentence or even if it makes sense to do so). This is a different type analysis for us as we (me) are trying to move into longitudinal analyses. </p>

<p>I hope this helps.</p>

<p>Thanks for your help so far. </p>

<p>Zeda</p>
"
"0.139162484826546","0.136891755195648"," 24857","<p>Much like with regression, handling binary dependent variables in SEM requires special considerations. In particular, some of these are noted on Dave Garson's <a href=""http://faculty.chass.ncsu.edu/garson/PA765/structur.htm"" rel=""nofollow"">Structural Equation Modeling</a> and include:</p>

<blockquote>
  <ol>
  <li><p>Polychoric correlation. LISREL/PRELIS uses polyserial, tetrachoric, and polychoric correlations to create the input correlation matrix,
  combined with ADF estimation (see below), for variables which cannot
  be assumed to have a bivariate normal distribution.</p>
  
  <ul>
  <li>Sample size issue. ADF [Asymptotically distribution-free] estimation in turn requires a very large sample size. Yuan and Bentler (1994)
  found satisfactory estimates only with a sample size of at least 2,000
  and preferably 5,000. Violating this requirement may introduce
  problems greater than treating ordinal data as interval and using ML
  estimation. This is also a reason cited for preferring the Bayesian
  estimation approach to ordinal data taken by Amos since Bayesian
  estimation can handle smaller samples than ML or ADF.</li>
  </ul></li>
  </ol>
</blockquote>

<p>I'm currently trying to use the package <a href=""http://cran.r-project.org/web/packages/sem/index.html"" rel=""nofollow"">sem</a> in R to test my model, and the author of the model suggests using polychoric correlations on <a href=""http://r.789695.n4.nabble.com/Link-functions-in-SEM-td859182.html"" rel=""nofollow"">R-help</a>. The problems are: </p>

<ol>
<li>I don't know what estimation method is being used with these correlations (i.e., ADF or ML). </li>
<li>My sample size is small (N = 173). </li>
<li>I'm not familiar with how to interpret polychoric associations (in the case that it is appropriate for me to use them). All the other variables in my model are continuous in nature. </li>
</ol>

<p>Any help and/or links would be greatly appreciated. I'm also considering using other software like OpenMX, but I'm still reading about how it handles binary data. Help with what other software I might want to use would also be appreciated.</p>
"
"0.10277830647413","0.101101261498861"," 27132","<p>I have social network data in which an ""ego"" names a friend ""alter"". I am running a regression in R in which attributes of alter are predictors of outcomes for ego. So each observation is dyadic with variable measures for both ego and alter. </p>

<p>There are multiple observations for each ego which are accounted for by using a gee model, clustering on ego. The problem is that i have been asked to also account for multiple observations of alter, or least to demonstrate that interdependence among the multiple alters is not impacting the final results. There are multiples of the same alter in the dataset as well as multiples of the same ego. </p>

<p>The two options seem to be some kind of cross clustering and I am not sure if that is possible in R. Another option which was suggested was to run a within-group correlation of some sort on the pearson's residuals, with the groups being the alters for each observation.  I had considered some sort of ICC but the number of times any individual alter shows up in the dataset ranges from 1-7. As far as I can tell, ICCs expect that the number of measures for each group in the dataset be the same. </p>

<p>Does anyone know how to do a within group correlation which can handle groups within which there are differing numbers of measures? I have looked online and have not come across anything that seems to address this. </p>

<p>Thanks in advance for any suggestions!</p>
"
"0.16250677125654","0.159855130326344"," 28756","<p>I have two datasets a training and a test dataset. The dependent variable is a proportion and there are 54 predictors which are positive and negative real numbers and another 7 predictors that are text. </p>

<p>There are three response variables. Total the normalized total number of hits. Treatment the normalized total number during treatment and a percent which is a ratio of the other two responses.</p>

<p>At the moment using lm on the percent prediction data I have a corolation of .4. 85% of the varibles are within 20% of their target. For the treatment response variable using glm in poisson mode i have a correlation of .6 percent but the variables do not match the target data at all.</p>

<p>I have two main issues I need advice on: </p>

<p>(1) it rejected the text predictors because it said factor has new level(s)
I would like it to ignore the information for those that have new level but not disregard it for those that have the correct information how do i do that? </p>

<p>(2) To make my dependent variable a real number, rather than a proportion bounded between 0 and 1, I was advised to transform the response using, for example, the logit transform or the Normal quantile function (<code>qnorm</code> in R). The problem is that these transformations (and others like it) will map 0 and 1 to non-finite values. How can I model these data in a regression setting when the response is a proportion that can be 0 or 1? </p>

<p>Using linear regression with outlier removal I am able to get 2239 of 2583 testing data within 20% of their actual value I would like to have that many within 10%. </p>

<p>Using the posson distribution glm the amount of treatment correlates with 69%.</p>

<p>Ignoring this second issue for the moment, I transform the y~x1+x2 such that y=log(y/(1-y)) the correlation of my predictions to actual data drops from 6% to 2%
This is what the data looks like after the logit transform</p>

<p><img src=""http://i.stack.imgur.com/rqkaD.png"" alt=""log distribution""></p>

<p>This is what the data looks like before the log distribution
<img src=""http://i.stack.imgur.com/Mx8sh.png"" alt=""normal percentages""></p>
"
"0.0726752374667264","0.0714893875923587"," 28819","<p>I have created a multiple linear regression model with R using <code>lm</code> and <code>glm</code>. I am using <code>lm</code> on a training set and <code>predict</code> on a testing set to validate the model. In one test my results are within 80% of what they should be for 80% of the cases. It correlates with 40% for one response variable and with 63% for another response variable (but the response variable with 63% correlation isn't near the actual values of the prediction). I have 53 predicates. What is the probability of that occurring randomly?
I've tried to build an multi-class svm off of the features using the predicates but so far the svm has been unable to properly predict the results.</p>
"
"0.133237935355665","0.142978775184717"," 30061","<p>What approaches exist to observe the time lag between two variables?</p>

<p>I need to analyze the relationship between blood pressure and some other factor, such as exercise. The data set I am drawing from has around 1800 individuals, with an average of 100 entries a piece. It is generally known that there is a strong relationship between exercise level and blood pressure. However, if a person increases their steps to 8000+ a day, how long will it take for their blood pressure to drop? I am new to this type of analysis, and this is a challenge I have been thinking about for weeks. </p>

<p>I don't know if anyone wants to comment on possible approaches to this challenge or any issues surrounding it.</p>

<p>Some issues I have been dealing with:</p>

<ol>
<li><p>Is it better to treat this as a times series analysis or longitudinal data analysis?</p>

<p>My understanding is that time series usually focuses on one variable with no missing data and is observed at consistent intervals, where as longitudinal is over a longer period and has inconsistent time intervals, dropouts, and missing data.</p>

<p>The data I have seems to fit the longitudinal description more, but it also seems like time series could be used if I averaged the values by week so there would be no missing entries. I'm not sure about the pros and cons of each approach.</p></li>
<li><p>Should I be fitting a causal model, or would some other method like regression be more helpful?</p>

<p>I've been looking at various possible causal models, for example Marginal Structural Models (MSM) or Structural Nested Models (SNM), but there seem to be very little information on their application. I did find one R package that applied inverse probability weights and then used Cox proportional hazards regression model on a survival object (MSM), but that seemed to be focus on weighting for confounding and right censoring. Its result was a correlation coefficient, which I don't think helps me.</p>

<p>So I'm not sure if fitting a causal model is what I want, because that seems to be more focused on the making intellectually satisfying assumptions about relationships within the data and then determining the degree of causality, rather than providing information about time lag.</p>

<p>If anyone knows about MSM, SNM, their use in R, or how they might relate to this problem, that would be awesome to hear.</p></li>
<li><p>What about survival analysis or SEM?</p>

<p>I haven't explored these options very in-depth yet but they sound potentially relevant.</p></li>
</ol>

<p>I've kind of stalled, so any hints about what direction I might want to go would be really appreciated. </p>

<p>Thanks in advance.</p>
"
"NaN","NaN"," 30243","<p>I've recently embarked on fitting regression mixed models in the Bayesian framework, using a MCMC algorithm (function MCMCglmm in R actually).</p>

<p>I believe I have understood how to diagnose convergence of the estimation process (trace, geweke plot, autocorrelation, posterior distribution...).</p>

<p>One of the thing that strikes me in the Bayesian framework is that much effort seems to devoted to do those diagnostics, whereas very little appears to be done in terms of checking the residuals of the fitted model. For instance in MCMCglmm the residual.mcmc() function does exist but is actually not yet implemented (ie.returns: ""residuals not yet implemented for MCMCglmm objects""; same story for predict.mcmc()). It seems to be lacking from other packages too, and more generally is little discussed in the literature I've found (apart from DIC which is quite heavily discussed too).</p>

<p>Could anyone point me to some useful references, and ideally R code I could play with or modify? </p>

<p>Many thanks.</p>
"
"0.0726752374667264","0.0714893875923587"," 31703","<p>I'm trying to <strong>automate</strong> linear regression with R, although I don't really have a concrete background in statistics. I was wondering:</p>

<p>Are there numerical techniques in determining whether the predictor values is even worth trying to fit against a response value before attempting to do this in R:</p>

<pre><code>lmfit &lt;- lm(x ~ y)
</code></pre>

<p>My initial guess was covariance and correlation, but again what are the accepted values?   Also, what if the relationship between response and predictor isn't linear.</p>

<p>In most scenarios (as mentioned by commenters already) one would base predictors based on an a priori hypothesis. However; in knowledge discovery, there are times where you <strong><em>don't know</em> what the hypothesis is</strong>, hence my motivation towards formulating a set of rules whereby a quick numerical parameter would allow me to decide if a certain predictor variable should be added or removed in the linear regression model.</p>

<p>Note that the discussion above is not data dependent, it should work with any general data that may or may not have linear relationships between variables.</p>
"
"0.201403525991205","0.198117202419119"," 32040","<p>I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>

<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)
&gt; summary(fix1)

Call:
lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5835 -0.3543 -0.0024  0.3944  4.7294 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
Group1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***
Group2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***
Group3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***
Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** 
Group2:Year  0.0032369  0.0031098   1.041  0.29802    
Group3:Year  0.0006081  0.0032666   0.186  0.85235    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.7926 on 2981 degrees of freedom
Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 
F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>

<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>

<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)
&gt; summary(mix1a)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 4727 4775  -2356     4671    4711
Random effects:
 Groups     Name        Variance Std.Dev.
 Individual (Intercept) 0.39357  0.62735 
 Residual               0.24532  0.49530 
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.1010868   45.90
Group2       4.8094268  0.1158095   41.53
Group3       4.5607287  0.1216522   37.49
Group1:Year -0.0084165  0.0016963   -4.96
Group2:Year  0.0032369  0.0019433    1.67
Group3:Year  0.0006081  0.0020414    0.30

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.252  0.000  0.000              
Group2:Year  0.000 -0.252  0.000  0.000       
Group3:Year  0.000  0.000 -0.252  0.000  0.000
</code></pre>

<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>

<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 2941 3001  -1461     2885    2921
Random effects:
 Groups     Name        Variance  Std.Dev. Corr   
 Individual (Intercept) 0.1054790 0.324775        
            Year        0.0017447 0.041769 -0.246 
 Residual               0.1223920 0.349846        
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.0541746   85.64
Group2       4.8094268  0.0620648   77.49
Group3       4.5607287  0.0651960   69.95
Group1:Year -0.0084165  0.0065557   -1.28
Group2:Year  0.0032369  0.0075105    0.43
Group3:Year  0.0006081  0.0078894    0.08

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.285  0.000  0.000              
Group2:Year  0.000 -0.285  0.000  0.000       
Group3:Year  0.000  0.000 -0.285  0.000  0.000
</code></pre>

<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>

<p>How is this possible? I would expect that the random effect will ""eat"" the unexplained variability and increase ""sureness"" of the estimate!</p>

<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>

<h2>Edit</h2>

<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>

<pre><code>indivSlope = c()
for (indiv in 1:103) {
    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])
    indivSlope[indiv] = coef(mod1)['Year']
}

indivGroup = unique(mydata[,c(""Individual"", ""Group"")])[,""Group""]


anova1 = lm(indivSlope ~ 0 + indivGroup)
summary(anova1)

Call:
lm(formula = indivSlope ~ 0 + indivGroup)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.176288 -0.016502  0.004692  0.020316  0.153086 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
indivGroup1 -0.0084165  0.0065555  -1.284    0.202
indivGroup2  0.0032369  0.0075103   0.431    0.667
indivGroup3  0.0006081  0.0078892   0.077    0.939

Residual standard error: 0.04248 on 100 degrees of freedom
Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 
F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 
</code></pre>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>
"
"0.157346504680629","0.165097668682599"," 32498","<p>I realize that similar questions have already been asked and answered, but I am in need of a bit more detail and specific advice as I am new to PCA and statistical methods in general. My question is also a bit broader because I will be putting it in context and I need to know if I'm even headed in the right direction.</p>

<p>I have a great deal of data. For each datapoint, there is one continuous response variable that I am interested in examining. Let's call it X.</p>

<p>There are also five or six categorical variables, most of which have between three and ten possible values. One of them, however (let's call it A), has 154 possible values, and to complicate things further, each datapoint can fall in 1-4 of those 154 categories. For the vast majority of them, they just take one of the 154 values, but about 10% of them take two or three values, and maybe 0.5% of them take four values. (I am actually considering including a discrete but quantitative variable that will be equal to the number of values taken by S, as I think it might also be a relevant factor affecting X.)</p>

<p>My ultimate goal here is twofold: to create a predictive model with multiple regression, and to use ANOVA to determine how much each of my variables' variance explains the variance in X.</p>

<p>Someone more familiar with statistics than I suggested that I start with PCA because both multiple regression and ANOVA assume that all factors are independent. I'm pretty sure there are some correlations between a few of my factors (though I have no idea what they are) so I figured PCA would be a good way to begin disentangling.</p>

<p>My questions are:</p>

<ol>
<li><p>Can I perform PCA given the categorical nature of my data? If so, what method should I use to ""dummy code"" the variables? If not, what method would be more effective?</p></li>
<li><p>Will including a single discrete, quantitative variable (the number of values taken by A) complicate matters?</p></li>
<li><p>Will PCA even do what I want? (namely, disentangling the variables so I can then use multiple regression and ANOVA)</p></li>
<li><p>Whatever you recommend, is it possible in R, and if so, how? (I haven't even downloaded R yet but it's been recommended to me and it's free so I'm inclined to give it a swing. I have some programming experience in Python and C++ so in theory I can learn it without too much difficulty.)</p></li>
</ol>

<p>Thanks very much in advance. </p>
"
"0.103843395091962","0.116741681083556"," 32735","<p>As a financial institution, we often run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  Recently we are building another model in which I believe we have regression with autocorrelated errors.The residuals from linear model have <code>lm(object)</code> has clearly a AR(1) structure, as evident from ACF and PACF.  I took two different approaches, the first one was obviously to fit the model using Generalized least squares <code>gls()</code> in R. My expectation was that the residuals from gls(object) would be a white noise (independent errors).  But the residuals from <code>gls(object)</code> still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing that I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (the residuals are white noise). I really want to use <code>gls()</code> in <code>nlme</code> package so that coding will be lot simpler and easier. What would be the approach I should take here? Am I supposed to use REML? or is my expectation of non-correlated residuals (white noise) from gls() object wrong?</p>

<pre><code>gls.bk_ai &lt;- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, 
                 correlation=corARMA(p=1), method='ML',  data  = fit.cap01A)

gls2.bk_ai  &lt;- update(gls.bk_ai, correlation = corARMA(p=2))

gls3.bk_ai &lt;- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai &lt;- update(gls.bk_ai, correlation = NULL)

anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  
     ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise
</code></pre>

<p>Is there something wrong with what I am doing???????</p>
"
"0.19247955013414","0.197945150973362"," 33105","<p>I'm working on an ongoing data analysis project about a series of live educational seminars. Each of my data points represents one such event, and for each one I have a multitude of categorical variables, as well as a couple quantitative ones that are my desired response variables (total revenue and number of attendees).</p>

<p>One trend I'm interested in looking at is how the frequency of these events affects my two response variables. Over the years, we have increased the frequency of the events and I'd like to determine whether or not it makes sense to continue doing so. I've created a couple of variables to help track this frequency:</p>

<p><code>NEAREST.SEM</code> - the number of days between this event and the nearest one to it chronologically in either direction</p>

<p><code>LAST.SEM</code> - the number of days between this event and the nearest one to it chronologically <em>before</em> it</p>

<p><code>WEEKLY.SEMS</code> - the total number of events held during the 7-day period starting on Monday within which this event falls</p>

<p>Depending on how I do the analysis, these three variables seem to have varying significance, but the one that seems to consistently come out on top is <code>NEAREST.SEM</code>, which I have found to be significant at the 0.01 level in one test and the 0.001 level in another. The other two variables are significant in predicting revenue but not number of attendees, which is not ideal since we are more interested in number of attendees. (The data for revenue is not representative of the total revenue for each event due to certain special offers for repeat customers that aren't taken into account there.)</p>

<p>Increasing the frequency of events seems to decrease each event's individual performance, but has so far increased overall performance. I'd like to determine the ""turning point"" at which overall performance will either dip or level off. Unfortunately, this is going to be tough to predict because my best-fitted variable, <code>NEAREST.SEM</code>, isn't as good a representation of increased frequency. Note, for example, that it would look exactly the same whether 4 or 5 events were held per week--it would always have the value of 1 in such situations. In fact, any time that events are grouped in clusters of consecutive days, we'll always get 1 for them on this variable...</p>

<p>One option would be to just use <code>WEEKLY.SEMS</code> as a predictor of revenue, which it is well correlated with, but as I said, we'd much rather do this analysis based on number of attendees, a better measure of an event's success.</p>

<p>So I really have two questions here:</p>

<ol>
<li><p>Any suggestions on my dilemma of which variable to use and how to deal with the problems I laid out above?</p></li>
<li><p>Once I decide on a predictor factor, how can I go about estimating the average decrease in revenue increasing to various frequencies will have? Should I run a multiple regression using all my variables and use the coefficient on the predictor factor? Or should I run a regression with just the one factor and my response and use that coefficient? Or is there a better test than regression to use?</p></li>
</ol>

<p>(By the way, I'm using R for my analysis and I'd appreciate any advice specifically tailored for that language.)</p>

<p>UPDATE: I have tried creating two new measures, one that's the average distance in days of the nearest event on either side, and one that's the number of events within 3 days in both directions...neither of them had any significant correlation. I'm running out of ideas here...</p>
"
"0.134568391204877","0.132372623906872"," 35590","<p>I was unable to figure out how to perform linear regression in R in for a repeated measure design. In a <a href=""http://stackoverflow.com/questions/12182373/plot-of-a-linear-regression-with-interactions"">previous question</a> (still unanswered) it was suggested to me to not use <code>lm</code> but rather to use mixed models. I used <code>lm</code> in the following way:  </p>

<pre><code>lm.velocity_vs_Velocity_response &lt;- lm(Velocity_response~Velocity*Subject, data=mydata)
</code></pre>

<p>(more details on the dataset can be found at the link above)</p>

<p>However I was not able to find on the internet any example with R code showing how to perform a linear regression analysis.</p>

<p>What I want is on one hand a plot of the data with the line fitting the data, and on the other hand the $R^2$ value along with the p-value for the test of significance for the model.</p>

<p>Is there anyone who can provide some suggestions? Any R code example could be of great help.</p>

<hr>

<p><strong>Edit</strong><br>
According to the suggestion I received so far, the solution to my analyze my data in order to understand if there is a linear relation between the two variables Velocity_response (deriving from the questionnaire) and Velocity (deriving from the performance) should be this:</p>

<pre><code>library(nlme)
summary(lme(Velocity_response ~ Velocity*Subject, data=scrd, random= ~1|Subject))
</code></pre>

<p>The result of summary gives this:</p>

<pre><code>    &gt; summary(lme(Velocity_response ~ Velocity*Subject, data=scrd, random= ~1|Subject))
    Linear mixed-effects model fit by REML
     Data: scrd 
           AIC      BIC   logLik
      104.2542 126.1603 -30.1271

    Random effects:
     Formula: ~1 | Subject
            (Intercept) Residual
    StdDev:    2.833804 2.125353

Fixed effects: Velocity_response ~ Velocity * Subject 
                              Value Std.Error DF    t-value p-value
(Intercept)               -26.99558  25.82249 20 -1.0454288  0.3083
Velocity                   24.52675  19.28159 20  1.2720292  0.2180
SubjectSubject10           21.69377  27.18904  0  0.7978865     NaN
SubjectSubject11           11.31468  33.51749  0  0.3375754     NaN
SubjectSubject13           52.45966  53.96342  0  0.9721337     NaN
SubjectSubject2           -14.90571  34.16940  0 -0.4362299     NaN
SubjectSubject3            26.65853  29.41574  0  0.9062674     NaN
SubjectSubject6            37.28252  50.06033  0  0.7447517     NaN
SubjectSubject7            12.66581  26.58159  0  0.4764880     NaN
SubjectSubject8            14.28029  31.88142  0  0.4479188     NaN
SubjectSubject9             5.65504  34.54357  0  0.1637076     NaN
Velocity:SubjectSubject10 -11.89464  21.07070 20 -0.5645111  0.5787
Velocity:SubjectSubject11  -5.22544  27.68192 20 -0.1887672  0.8522
Velocity:SubjectSubject13 -41.06777  44.43318 20 -0.9242591  0.3664
Velocity:SubjectSubject2   11.53397  25.41780 20  0.4537754  0.6549
Velocity:SubjectSubject3  -19.47392  23.26966 20 -0.8368804  0.4125
Velocity:SubjectSubject6  -29.60138  41.47500 20 -0.7137162  0.4836
Velocity:SubjectSubject7   -6.85539  19.92271 20 -0.3440992  0.7344
Velocity:SubjectSubject8  -12.51390  22.54724 20 -0.5550080  0.5850
Velocity:SubjectSubject9   -2.22888  27.49938 20 -0.0810519  0.9362
 Correlation: 
                          (Intr) Velcty SbjS10 SbjS11 SbjS13 SbjcS2 SbjcS3 SbjcS6 SbjcS7 SbjcS8 SbjcS9 V:SS10 V:SS11 V:SS13 Vl:SS2 Vl:SS3
Velocity                  -0.993                                                                                                         
SubjectSubject10          -0.950  0.943                                                                                                  
SubjectSubject11          -0.770  0.765  0.732                                                                                           
SubjectSubject13          -0.479  0.475  0.454  0.369                                                                                    
SubjectSubject2           -0.756  0.751  0.718  0.582  0.362                                                                             
SubjectSubject3           -0.878  0.872  0.834  0.676  0.420  0.663                                                                      
SubjectSubject6           -0.516  0.512  0.490  0.397  0.247  0.390  0.453                                                               
SubjectSubject7           -0.971  0.965  0.923  0.748  0.465  0.734  0.853  0.501                                                        
SubjectSubject8           -0.810  0.804  0.769  0.624  0.388  0.612  0.711  0.418  0.787                                                 
SubjectSubject9           -0.748  0.742  0.710  0.576  0.358  0.565  0.656  0.386  0.726  0.605                                          
Velocity:SubjectSubject10  0.909 -0.915 -0.981 -0.700 -0.435 -0.687 -0.798 -0.469 -0.883 -0.736 -0.679                                   
Velocity:SubjectSubject11  0.692 -0.697 -0.657 -0.986 -0.331 -0.523 -0.607 -0.357 -0.672 -0.560 -0.517  0.637                            
Velocity:SubjectSubject13  0.431 -0.434 -0.409 -0.332 -0.996 -0.326 -0.378 -0.222 -0.419 -0.349 -0.322  0.397  0.302                     
Velocity:SubjectSubject2   0.753 -0.759 -0.715 -0.580 -0.360 -0.992 -0.661 -0.389 -0.732 -0.610 -0.563  0.694  0.528  0.329              
Velocity:SubjectSubject3   0.823 -0.829 -0.782 -0.634 -0.394 -0.622 -0.984 -0.424 -0.799 -0.667 -0.615  0.758  0.577  0.360  0.629       
Velocity:SubjectSubject6   0.462 -0.465 -0.438 -0.356 -0.221 -0.349 -0.405 -0.995 -0.449 -0.374 -0.345  0.425  0.324  0.202  0.353  0.385
Velocity:SubjectSubject7   0.961 -0.968 -0.913 -0.740 -0.460 -0.726 -0.844 -0.496 -0.986 -0.778 -0.718  0.886  0.674  0.420  0.734  0.802
Velocity:SubjectSubject8   0.849 -0.855 -0.807 -0.654 -0.406 -0.642 -0.746 -0.438 -0.825 -0.988 -0.635  0.783  0.596  0.371  0.649  0.709
Velocity:SubjectSubject9   0.696 -0.701 -0.661 -0.536 -0.333 -0.526 -0.611 -0.359 -0.676 -0.564 -0.990  0.642  0.488  0.304  0.532  0.581
                          Vl:SS6 Vl:SS7 Vl:SS8
Velocity                                      
SubjectSubject10                              
SubjectSubject11                              
SubjectSubject13                              
SubjectSubject2                               
SubjectSubject3                               
SubjectSubject6                               
SubjectSubject7                               
SubjectSubject8                               
SubjectSubject9                               
Velocity:SubjectSubject10                     
Velocity:SubjectSubject11                     
Velocity:SubjectSubject13                     
Velocity:SubjectSubject2                      
Velocity:SubjectSubject3                      
Velocity:SubjectSubject6                      
Velocity:SubjectSubject7   0.450              
Velocity:SubjectSubject8   0.398  0.828       
Velocity:SubjectSubject9   0.326  0.679  0.600

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-1.47194581 -0.46509026 -0.05537193  0.39069634  1.89436646 

Number of Observations: 40
Number of Groups: 10 
Warning message:
In pt(q, df, lower.tail, log.p) : NaNs produced
&gt; 
</code></pre>

<p>Now, I do not understand where I can get the R^2 and the corresponding p-values indicating me wether there is a linear relationship between the two variables or not,
nor I have understood how my data can be plotted with the line fitting the regression.</p>

<p>Can anyone be so kind to enlighten me? I really need your help guys...</p>
"
"0.0419590679148345","0.0412744171706498"," 35936","<p>I am new to R and trying to practice with some exercises. Given a data set with 40  observations and 5 variables. Spending is the the response and there are 4 predictors. I started with a linear model Residuals:</p>

<pre><code>    Min      1Q  Median      3Q     Max 
-51.082 -11.320  -1.451   9.452  94.252 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  22.55565   17.19680   1.312   0.1968    
sex         -22.11833    8.21111  -2.694   0.0101 *  
status        0.05223    0.28111   0.186   0.8535    
income        4.96198    1.02539   4.839 1.79e-05 ***
verbal       -2.95949    2.17215  -1.362   0.1803    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 22.69 on 42 degrees of freedom
Multiple R-squared: 0.5267, Adjusted R-squared: 0.4816 
F-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06 
</code></pre>

<p>First, is this what they mean by fit regression model and Secondly, how do I compute the correlation of the residuals with the fitted values? </p>
"
"0.0419590679148345","0.0412744171706498"," 36145","<p>I want to predict Tree Heights in a certain area using some variables obtained through remote sensing. Like approximate Biomass, etc. I want to first use a linear regression (I know it's not the best idea but it is a must step for my project). I wanted to know how badly can spatial autocorrelation affect it and what is the easiest way to correct this if it is even possible. I'm doing everything in R by the way. Thank you in advanced.</p>

<p>J.</p>
"
"0.0938233281301002","0.0922924025252575"," 37785","<p>I come from an SPSS background and am attempting to move to <code>R</code> for it's superior flexibility and data manipulation abilities. I have some concerns however as to whether the <code>lm()</code> is really using partial correlations.</p>

<p>I'm basically trying to run a linear regression, using something similar to the ""enter"" setting in SPSS, which essentially builds the model one variable at a time, reporting the change in $R^2$ with each additional variable. This allows you to determine how much predictive power each variable adds to the model.</p>

<p>When I run the same analysis in <code>R</code> however, I don't get any information on the $R^2$ contributed by individual variables, and I'm not even sure that it's using partial corrrelations to calculate the p-values it's reporting!</p>

<p>My code follows:</p>

<pre><code>summary(m1 &lt;- lm(totalprop ~ cos(Angle) + Alignment + colour + 
  Angle*Alignment, dataset))
</code></pre>

<p>My questions:</p>

<ol>
<li>Does R use partial correlations to determine reported p-values from <code>lm()</code>?</li>
<li>How can I make <code>R</code> report change in $R^2$ with each additional variable.</li>
<li>How can I make <code>R</code> act like SPSS in calulating the model piece by piece? Is this possible without running multiple iterations of the lm() function? If not, how does one control for the effects of covariates in R?</li>
</ol>
"
"0.121125395777877","0.142978775184717"," 37840","<p>Okay, so I am trying to understand linear regression. I've got a data set and it looks all quite alright, but I am confused. This is my linear model-summary:</p>

<pre><code>Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.2068621  0.0247002   8.375 4.13e-09 ***
temp        0.0031074  0.0004779   6.502 4.79e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.04226 on 28 degrees of freedom
Multiple R-squared: 0.6016, Adjusted R-squared: 0.5874 
F-statistic: 42.28 on 1 and 28 DF,  p-value: 4.789e-07 
</code></pre>

<p>so, the p-value is really low, which means it is very unlikely to get the correlation between x,y just by chance.
If I plot it and then draw the regression line it looks like this:
<a href=""http://s14.directupload.net/images/120923/l83eellv.png"" rel=""nofollow"">http://s14.directupload.net/images/120923/l83eellv.png</a>
(Had it in as a picture but I am - as a new user - currently not allowed to post it)
Blue lines = confidence interval
Green lines = prediction interval</p>

<p>Now, a lot of the points do not fall into the confidence interval, why would that happen? I think none of the datapoints falls on the regression line b/c they are just quite far away from each other, but what I am not sure of: Is this a real problem? They still are around the regression line and you can totally see a pattern. But is that enough?
I'm trying to figure it out, but I just keep asking myself the same questions over and over again.</p>

<p>What I thought of so far:
The confidence interval says that if you calculate CI's over and over again, in 95% of the times the true mean falls into the CI.
So: It it is not a problem that the dp do not fall into it, as these are not the means really.
The prediction interval on the other hand says, that if you calculate PI's over and over again, in 95% of the times the true VALUE falls into the interval. So, it is quite important to have the points in it (which I do have).
Then I've read the PI always has to have a wider range than the CI. Why is that?
This is what I have done:</p>

<pre><code>conf&lt;-predict(fm, interval=c(""confidence""))
prd&lt;-predict(fm, interval=c(""prediction""))
</code></pre>

<p>and then I plotted it by:</p>

<pre><code>matlines(temp,conf[,c(""lwr"",""upr"")], col=""red"")
matlines(temp,prd[,c(""lwr"",""upr"")], col=""red"")
</code></pre>

<p>Now, if I calculate CI and PI for additional data, it does not matter how wide I choose the range, I get the exact same lines as above. I cannot understand. What does that mean?
This would then be:</p>

<pre><code>conf&lt;-predict(fm,newdata=data.frame(x=newx), interval=c(""confidence""))
prd&lt;-predict(fm,newdata=data.frame(x=newx), interval=c(""prediction""))
</code></pre>

<p>for new x I chose different sequences.
If the sequence has a different # of observations than the variables in my regression, I am getting a warning. Why would that be?</p>
"
"0.139162484826546","0.136891755195648"," 38491","<p>If we have a spatial autoregressive process, we can estimate a model to control for the autoregression with a spatial lag,
$$y=\rho W y + X\beta + \epsilon$$
Where $\rho$ is the strength of the spatial correlation, and $W$ is a matrix of spatial weights. The <code>spdep</code> package for R contains the <code>lagsarlm</code> command which is designed to estimate precisely this model. The package contains methods for creating the weights. But there seems to be some discrepancy between the model fit between <code>lagsarlm()</code> and <code>lm()</code> fitted to what should be a similar model.</p>

<p>As an example, consider the example given with <code>?lagsarlm</code> in R. </p>

<pre><code>library(spdep)
data(oldcol)
COL.lag &lt;- lagsarlm(CRIME ~ INC + HOVAL, data=COL.OLD,
                nb2listw(COL.nb, style=""W""), method=""eigen"", quiet=TRUE)
summary(COL.lag)
Residuals:
      Min        1Q    Median        3Q       Max 
-37.68585  -5.35636   0.05421   6.02013  23.20555 

Type: lag 
Coefficients: (asymptotic standard errors) 
             Estimate Std. Error z value  Pr(&gt;|z|)
(Intercept) 45.079251   7.177347  6.2808 3.369e-10
INC         -1.031616   0.305143 -3.3808 0.0007229
HOVAL       -0.265926   0.088499 -3.0049 0.0026570

Rho: 0.43102, LR test value: 9.9736, p-value: 0.001588
Asymptotic standard error: 0.11768
    z-value: 3.6626, p-value: 0.00024962
Wald statistic: 13.415, p-value: 0.00024962
</code></pre>

<p>We can estimate what (I think) should be the same model by computing the actual spatial lag variable,</p>

<pre><code>crime.lag &lt;- lag.listw(nb2listw(COL.nb, style=""W""), COL.OLD$CRIME)
linearlag &lt;- lm(CRIME ~ crime.lag + INC + HOVAL, data=COL.OLD)
Residuals:
    Min      1Q  Median      3Q     Max 
-38.644  -6.103   0.266   6.563  21.610 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 38.18099    9.21531   4.143 0.000149 ***
crime.lag    0.55733    0.15029   3.709 0.000570 ***
INC         -0.86584    0.35541  -2.436 0.018864 *  
HOVAL       -0.26358    0.09136  -2.885 0.005986 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 10.12 on 45 degrees of freedom
Multiple R-squared: 0.6572, Adjusted R-squared: 0.6343 
F-statistic: 28.75 on 3 and 45 DF,  p-value: 1.543e-10 
</code></pre>

<p>The two models, which I think should be identical, are in fact significantly different from each other in every parameter and in model fit (with the <code>linearlag</code> model providing significantly lower AIC). Are there reasons why this should be? Why should I just not use the second model and abandon the special methods?</p>
"
"0.0419590679148345","0.0412744171706498"," 38563","<p>I'm not doing a pure QAR (quantile auto regression) but I do have a lagged dependent variable (AR(1)) as a predictor. I'm using the <code>quantreg</code> package in <code>R</code> to do it. I have two very closely related questions:</p>

<p>(1) I use <code>quantreg</code> in <code>R</code> to do my QRs. Do I need to do anything fancy to be fine by putting an AR(1) term on the RHS?  </p>

<p>(2) Is it fine to use AR(1) term as a regressor to deal with autocorrelation in quantile regression as you can with OLS? Or is QR different in this regards?</p>
"
"0.10277830647413","0.101101261498861"," 39000","<p>I assessed the internal reliability of a self-created scale with eight items ($N = 150$) by calculating Cronbachâ€™s $\alpha$. It appears that one item correlates low with the overall score of the scale (item 4 in the example below). The corrected item-total correlation, i.e. the correlation of this item with the scale total excluding that item, is only $r= .046$. </p>

<pre><code>library(psych)
scale&lt;-mydata[,c(24,25,26,27,28,29,30,31)]
alpha(scale)

Reliability analysis   
Call: alpha(x = scale)

          0.62      0.64    0.66      0.18  4.3 0.79

 Reliability if an item is dropped:
      raw_alpha std.alpha G6(smc) average_r
item1      0.56      0.59    0.62      0.17
item2      0.53      0.57    0.58      0.16
item3      0.54      0.56    0.58      0.16
item4      0.66      0.67    0.67      0.23
item5      0.60      0.62    0.63      0.19
item6      0.55      0.59    0.62      0.17
item7      0.58      0.61    0.63      0.18
item8      0.63      0.65    0.67      0.21

 Item statistics 
        n    r r.cor r.drop mean   sd
item1 144 0.60  0.51  0.395  4.5 0.71
item2 145 0.65  0.62  0.499  4.6 0.71
item3 142 0.67  0.64  0.484  4.5 0.72
item4 146 0.33  0.15  0.046  4.6 0.81
item5 147 0.51  0.41  0.298  4.9 0.41
item6 139 0.59  0.50  0.404  4.4 0.82
item7 136 0.53  0.43  0.339  4.2 1.03
item8 135 0.39  0.21  0.190  4.3 0.94

Non missing response frequency for each item
         1    2    3    4    5 miss
item1 0.01 0.01 0.04 0.34 0.60 0.04
item2 0.01 0.01 0.03 0.24 0.71 0.03
item3 0.00 0.01 0.11 0.28 0.60 0.05
item4 0.01 0.03 0.05 0.14 0.77 0.03
item5 0.00 0.00 0.02 0.11 0.87 0.02
item6 0.01 0.02 0.10 0.29 0.58 0.07
item7 0.04 0.02 0.15 0.25 0.54 0.09
item8 0.02 0.03 0.11 0.32 0.52 0.10
</code></pre>

<p><strong>PROBLEM</strong>: I would like to report this low correlation with the degrees of freedom in parentheses and the significance level in the main text. Yet, I am not sure whether I calculated the correct p-value. What I did is a simple regression with item 4 as the dependent variable:</p>

<pre><code>scale &lt;- as.data.frame(scale)
summary(lm(item4 ~ item1+item2+item3+item5+item6+item7+item8, data=scale))

Call:
lm(formula = item4 ~ item1 + item2 + item3 + item5 + item6 + 
    item7 + item8, data = scale)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.4256 -0.0465  0.2869  0.3500  1.3405 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.65098    1.00288   1.646 0.102492    
item1        0.12916    0.11560   1.117 0.266262    
item2        0.02387    0.12921   0.185 0.853760    
item3       -0.07323    0.12718  -0.576 0.565921    
item5        0.64204    0.18636   3.445 0.000802 ***
item6       -0.04596    0.10230  -0.449 0.654120    
item7       -0.13217    0.08030  -1.646 0.102545    
item8        0.05609    0.08758   0.641 0.523136    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.8235 on 113 degrees of freedom
  (29 observations deleted due to missingness)
Multiple R-squared: 0.1385,     Adjusted R-squared: 0.08518 
F-statistic: 2.596 on 7 and 113 DF,  p-value: 0.01604
</code></pre>

<p><strong>QUESTION:</strong> Is it correct if I report something like ""Item 4 correlates only weakly with the overall score of the scale $(r(113)= .046, p= .02)$"" - or did I make a rather large error in reasoning here? </p>
"
"0.0484501583111509","0.0476595917282392"," 40667","<p>Currently, I am working on a count data set measuring events in a third-order administrative unit. The frequency of events varies for each third-order administrative unit to account for the variability, I am using a negative binomial regression model. </p>

<p>A crude graphical analysis of the residual of the regression indicates that no spatial autocorrelation exists. I would like to find a method to assess spatial autocorrelation in a negative binomial regression model, since a regular test of spatial autocorrelation are not feasible (ex. Global Moran's I). </p>

<p>Do you have any suggestions?</p>
"
"0.193011712408238","0.189862318984989"," 40670","<p>I am familiar with linear regression models but the random section of linear mixed models just melts my mind. I did find an excellent guide that could have helped me but the languageR package is not compatible with newer versions of lme4 so I've been unable to implement it in my work.</p>

<p>For me the fixed effects are very understandable (below lactation and a higher yr2 value both contribute to a higher weight but the lactation effect is more consistent which results in a higher t-value).</p>

<p>The first problem is to understand what I am actually putting in. To a certain extent I understand that <code>(1|P$grupp)</code> means that the mixed model add to the base line (intercept) while <code>(P$grupp|P$lweek)</code> mean that belong to a group is expected to affect the average weight increase (or decrease) while <code>P$lweek</code> adds to the baseline value. But why does all tutorials seem to favor write ups like <code>(1+P$fgrupp|P$lweek)</code> rather than <code>(P$grupp|P$lweek)</code>?</p>

<p>Now on to the actual output (see below for full output). I've used the following models (sorry for the Swenglish but the sample is the weight of cows <code>P$vikt</code> is the weight at certain time points and <code>P$lweek</code> is the time since a calf was born, <code>P$fgrupp</code> is a factor telling if the cow belongs to feed group 1,2 or 3):</p>

<pre><code>Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 | P$fgrupp)            #$
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 + P$fgrupp | P$lweek)
</code></pre>

<p>Where I understand it as the first one being rather useless (essentially it tells us that the average weight of the cows isn't affected of which feed group it belongs too). This is reflected by fgrupp having variance 0 in the first formula below. The second is more interesting as the <code>P$fgrupp|P$lweek</code> as I understand it should show if different feed groups affect the weight increase of cows as function of the time. But I am really not competent enough to understand the input. I understand that variance somehow mean that belonging to group2 or 3 explain some of the variation in the growth curves but I really don't understand how to interpret this.</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr        
 P$lweek  (Intercept)   13.068  3.6149                                     #$
          P$fgrupp2     77.230  8.7881  1.000                              #$
          P$fgrupp3     81.188  9.0104  1.000 1.000                        #$
 Residual             4031.831 63.4967              
Number of obs: 1048, groups: P$lweek, 84
</code></pre>

<p><strong>Full output</strong></p>

<pre><code>#First model#
Linear mixed model fit by REML 
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 | P$fgrupp)            #$
   AIC   BIC logLik deviance REMLdev
 11703 11732  -5845    11698   11691
Random effects:
 Groups   Name        Variance Std.Dev.
 P$fgrupp (Intercept)    0.0    0.000                                      #$
 Residual             4139.9   64.342  
Number of obs: 1048, groups: P$fgrupp, 3                                   #$

Fixed effects:
            Estimate Std. Error t value
(Intercept)  509.593      4.683  108.82
P$lweek        1.028      0.105    9.79                                    #$
P$laktation   22.789      1.454   15.67                                    #$
P$yr2         35.294      4.093    8.62                                    #$

Correlation of Fixed Effects:
            (Intr) P$lwek P$lktt                                           #$
P$lweek     -0.560                                                         #$
P$laktation -0.636  0.030                                                  #$
P$yr2       -0.240 -0.034 -0.141                                           #$


#Second model#

Linear mixed model fit by REML 
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 + P$fgrupp | P$lweek)  #$
       AIC   BIC logLik deviance REMLdev
     11707 11761  -5842    11693   11685
    Random effects:
     Groups   Name        Variance Std.Dev. Corr        
     P$lweek  (Intercept)   13.068  3.6149                                     #$
              P$fgrupp2     77.230  8.7881  1.000                              #$
              P$fgrupp3     81.188  9.0104  1.000 1.000                        #$
     Residual             4031.831 63.4967              
    Number of obs: 1048, groups: P$lweek, 84                                   #$

Fixed effects:
            Estimate Std. Error t value
(Intercept) 508.2291     5.1770   98.17
P$lweek       1.0662     0.1192    8.94                                    #$
P$laktation  22.6525     1.4459   15.67                                    #$
P$yr2        35.6343     4.0848    8.72                                    #$

Correlation of Fixed Effects:
            (Intr) P$lwek P$lktt                                           #$$
P$lweek     -0.627                                                         #$
P$laktation -0.570  0.025                                                  #$
P$yr2       -0.224 -0.018 -0.136                                           #$
</code></pre>
"
"0.245039746552799","0.254432595171137"," 41362","<p>I have no formal training in statistics so please correct me if I use the wrong terms as I try to explain my problem.</p>

<p>I have a set of data (less than 80 points) with essentially 1 single outcome (a float we will call <code>dcl</code>) that can potentially depends on 10 of other variables, most of them floats maybe one or two boolean.</p>

<p>While I might ask some multi-variate regression question later, let's start with something simple. </p>

<p>Historically, people in my field have focused on the strongest correlation between <code>dcl</code> and variable <code>J</code> and some of my data suggests some other dependence on a float <code>Re</code> which is I'm sure at least weakly correlated with 'J' as they share some variables in their respective expressions. So my first question is how do I test the correlation and/or the independence of 'Re' and 'J' on the outcome 'dlc'? Intuitively and physically, I expect 'dlc' to depend strongly on 'J' and weakly on 'Re'. How do I prove this with a statistical analysis?</p>

<p>Here are a few graphs to illustrate the data:</p>

<p><img src=""http://i.stack.imgur.com/dVRMa.png"" alt=""Re vs J"">   </p>

<p><img src=""http://i.stack.imgur.com/hNPvb.png"" alt=""dcl vs J""></p>

<p><img src=""http://i.stack.imgur.com/D3YJe.png"" alt=""dcl vs Re""></p>

<p>Final point, in terms of software, I have python and R installed, I'm fairly proficient in python but I just installed R and know pretty much nothing about it.</p>

<p>EDIT 1: </p>

<p>Following gung's suggestion, I ran my dataset through R:</p>

<pre><code>Call:
lm(formula = dcl ~ J + I(J^2) + Re + I(Re^2), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.0078 -3.7930 -0.4458  2.0869 21.2538 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.648e+01  1.232e+00  13.380  &lt; 2e-16 ***
J           -2.662e+00  3.773e-01  -7.054 6.58e-10 ***
I(J^2)       1.096e-01  2.071e-02   5.293 1.10e-06 ***
Re           1.966e-06  1.621e-05   0.121    0.904    
I(Re^2)      2.191e-11  3.441e-11   0.637    0.526    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 5.369 on 77 degrees of freedom
Multiple R-squared: 0.4831, Adjusted R-squared: 0.4562 
F-statistic: 17.99 on 4 and 77 DF,  p-value: 1.818e-10
</code></pre>

<p>So now I need some help to decipher this (but I will look into R documentation too). I don't know if it's relevant but on physical grounds only, it's probable the dependency in J is $dcl \sim \frac{1}{\sqrt{J}}$. Can I put this directly into the model? Does that already tell me something about the dependency on $J$ vs $Re$?</p>

<pre><code>Call:
lm(formula = dcl ~ J + I(J^(-0.5)) + Re + I(Re^(-0.5)), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8119 -3.0097 -0.8504  1.8506 12.1439 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   8.175e-01  1.634e+00   0.500   0.6184    
J            -2.946e-01  1.258e-01  -2.343   0.0217 *  
I(J^(-0.5))   4.516e+00  7.053e-01   6.403 1.09e-08 ***
Re            3.332e-05  6.684e-06   4.985 3.72e-06 ***
I(Re^(-0.5))  6.009e+02  1.354e+02   4.438 2.98e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.426 on 77 degrees of freedom
Multiple R-squared: 0.6487, Adjusted R-squared: 0.6305 
F-statistic: 35.55 on 4 and 77 DF,  p-value: &lt; 2.2e-16

&gt; model = lm(dcl ~ J+I(J^(-0.5)) + Re+I(Re^(-0.5)), data=df)
&gt; summary(model)
</code></pre>

<p><strong>EDIT 2</strong>: OK I'm starting to understand things better. Also, again based on physical grounds, I would think that the dependency is more something like $dcl ~ \frac{1}{\sqrt{J}} Re^{n}$ with possibly other variables in that product that I ignore. So when I enter such model in R (can I still use <code>lm</code> for something non-linear?), here is what I get:</p>

<pre><code>Call:
lm(formula = dcl ~ I(J^(-0.5)) * I(Re^(-0.1)), data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.5363  -3.0192  -0.2556   1.4373  17.1494 

Coefficients:
                         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               -43.220      9.164  -4.716 1.03e-05 ***
I(J^(-0.5))                63.813     11.088   5.755 1.62e-07 ***
I(Re^(-0.1))              124.245     24.038   5.169 1.77e-06 ***
I(J^(-0.5)):I(Re^(-0.1)) -142.744     27.269  -5.235 1.36e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.668 on 78 degrees of freedom
Multiple R-squared: 0.6042, Adjusted R-squared: 0.5889 
F-statistic: 39.68 on 3 and 78 DF,  p-value: 1.122e-15
</code></pre>

<p>Does that 4th line tell me something about the dependence between $J$ and $Re$? What kind of tools could I use to get an estimate on the exponent on Re? Because right now I'm just trying a few different numbers empirically to see how the errors evolve. Next for me is to plot the dcl vs the new model and see how well the data collapses visually...</p>

<p>EDIT 3:</p>

<p>In the end, I used <code>nls</code> to explore the possible exponents of my fit. I also removed some outliers in my data that used a different experimental method. I settled on a fit that gave me decent Pr(>|t|) and residuals and which visually produce a decent fit. The last outlier is actually another experiment with a different setup, but one that I trust. So in a sense it's good that it shows up as an outlier as it hints at other parameters that need to be explored. Thank you gung, I accept your answer as I believe it guided me in the right direction.</p>

<pre><code>&gt; model = nls(L.D ~ C*I(J^(c1))*I(Re_s^(c2)), start=list(C=10,c1=-0.25,c2=-0.25),data=df)
&gt; summary(model)
Formula: L.D ~ C * I(J^(c1)) * I(Re_s^(c2))
Parameters:
   Estimate Std. Error t value Pr(&gt;|t|)    
C  57.20389   26.40011   2.167   0.0337 *  
c1 -0.27721    0.05901  -4.698 1.27e-05 ***
c2 -0.16424    0.04936  -3.327   0.0014 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
Residual standard error: 4 on 70 degrees of freedom
Number of iterations to convergence: 8 
Achieved convergence tolerance: 2.91e-06 
</code></pre>

<p><img src=""http://i.stack.imgur.com/dfgWX.png"" alt=""enter image description here""></p>
"
"0.0419590679148345","0.0412744171706498"," 41547","<p>I'm trying to implement the ""Free Step-Down Resampling Method"" described by Westfall and Young in ""<a href=""http://books.google.es/books?id=nuQXORVGI1QC&amp;printsec=frontcover&amp;hl=es&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false"" rel=""nofollow"">Resampling-Based Multiple Testing</a>"" (algorithm ~2.8 in the text). My goal is to perform a multivariate linear regression. </p>

<p>So, I have an error estimate (from the original sample) like this (using OLS): </p>

<p>$\epsilon = Y - (\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots \beta_pX_p)$</p>

<p>[BTW, $X_1 \dots X_p$ are dummy variables.] </p>

<p>In order to resample ($i$ times), I have to do: </p>

<p>$Y_i^* = \epsilon_i^*$ </p>

<p>where $\epsilon_i^*$ is a <em>with replacement</em> sample from the original $\epsilon$. </p>

<p>Here is the problem: </p>

<p>In my dataset, responses (rows) are clustered (data come from related individuals); so, I would usually have applied Huber-White estimators to account for correlations in OLS-based linear regressions. </p>

<p>I don't know how to proceed here... Should Huber-White estimators be used? If so, how? </p>

<p>Apologies if my question is too simple, but I'm new in resampling methods... I guess the answer is simple, too. Suggestions are welcome. </p>
"
"0.0593390829096927","0.0583708405417779"," 44358","<p>I am trying to fit a problem with <code>regsubsets</code> with leaps in R. My problem is particularly strongly collinear, which is why I chose to use it in the first place. </p>

<p>The number of variables is about 200 and I have about 2 million independent observations. All the variables have a strong correlation structure with each other. </p>

<p>On running regsubsets with <code>really.big = TRUE</code>, and <code>nvmax = 5</code> and <code>nbest = 1</code>, I get the following:</p>

<blockquote>
  <p>Error in leaps.setup - 31 linear dependencies found </p>
</blockquote>

<p>and it crashes. All I am looking to do is a simple forward stepwise, say order the variables in the order of correlation and run nested regressions. </p>

<p>Is that too much for the software to handle? I think the problem is well posed in that sense. </p>
"
"0.0856485887284415","0.101101261498861"," 44750","<p>I have a panel dataset on seaborne shipment prices for a sample of trading routes. The data is a fixed annual price, based on a) port costs and b) fuel (a function of distance and fuel price). The major unobserved variable is port cost, which differs by port, though may be similar by area. Each observation is a port-pair such that there are two port costs included in the price. </p>

<p>The goal is to predict the price for a number of routes not included in the sample, most likely just at a more aggregate area-area level, since there are port-pairs where we only observe a loadarea-port pair or a loadarea-dischargearea pair for instance.</p>

<pre><code>Year LoadPort LoadCountry LoadArea DischargePort DischargeCountry DischargeArea  Price
2007  ABIDJAN  IVORY COAST WAF      LOOP TERMINAL USA              USG            $X1
</code></pre>

<p>The data is an unbalanced panel dataset over a 5 yr time interval. My questions are the following:</p>

<ol>
<li>Does it make sense to use a fixed effects model with dummies for the LoadPort and DischargePort? I am not sure this is what I want since I need to get an estimate of area-area average and not sure averaging the port-pair constants within areas is robust. I would also like to preserve the model's capability to predict distance correlation when I need a port-port estimate.</li>
<li>If yes to 1, how do you add another type of fixed effect in R? The plm R instructions are for typical panel datasets where you have individuals or firms across time with some constant unobserved, but in this case it's two unobserved constants per observation.</li>
<li>I have not used multilevel regression before but it strikes me that this problem falls into this class, does anyone have a quick and dirty solution? It could be just a route fixed effect, but I don't know how to implement in R. I am not worried about elegance at this stage.</li>
</ol>
"
"0.0726752374667264","0.0714893875923587"," 45696","<p>I'm not sure if I used the concept ""extreme values"" right. Anyhow, I'm trying to produce a model that estimates maximum tree heights / $\text{km}^2$. I have a database of around 24000 points ($\text{km}^2$), each has the max tree height value and 33 predictors. After playing around with random forest I manage to achieve a correlation of 0.67 between the real height and the estimated height on the test sample (20%). A MSE of around 1.6 meters. But Maximum errors of up to 33 meters. What I can see is that patches with very tall trees or very short trees (50 meters - 1 meters) are out of the scope of the model. Thinking in linear regression it is analogous to losing prediction power as you move away from the center of gravity of the observations. Right? How can I cope with this if at all?</p>

<p>p.s. this was implemented in R</p>
"
"0.314179757762644","0.314381756308477"," 48040","<p>I'm trying to test the significance of the ""component"" effect in a multivariate regression model. I'm not sure what is the right way. Using R, I have tried a way with <code>lm()</code> and another way with <code>gls()</code>, and they don't yield compatible results. </p>

<p><strong>Please note that this is not a question about which methodology is the right one to use to analyze my data. By the way these are simulated data. My question is about the understanding in mathematical terms of the R procedures I use.</strong></p>

<p>The dataset:</p>

<pre><code>&gt; str(dat)
'data.frame':   31 obs. of  5 variables:
 $ group: Factor w/ 5 levels ""1"",""5"",""2"",""3"",..: 1 1 1 1 1 1 1 1 3 3 ...
     $ id   : Factor w/ 8 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 1 3 ...
 $ x    : num  2.5 3 3 4 1.2 3.8 3.9 4 2.5 2.9 ...
     $ y    : num  2.6 3.8 3.9 3.8 1.6 5.2 1.3 3.6 4 3.2 ...
 $ z    : num  3.1 3.6 4.9 3.8 2.1 6 2.1 2.9 4.2 2.9 ...
&gt; head(dat,10)
   group id   x   y   z
1      1  1 2.5 2.6 3.1
2      1  2 3.0 3.8 3.6
3      1  3 3.0 3.9 4.9
4      1  4 4.0 3.8 3.8
5      1  5 1.2 1.6 2.1
6      1  6 3.8 5.2 6.0
7      1  7 3.9 1.3 2.1
8      1  8 4.0 3.6 2.9
9      2  1 2.5 4.0 4.2
10     2  3 2.9 3.2 2.9
</code></pre>

<p>I convert this dataset into ""long format"" for graphics (and later for <code>gls()</code>):</p>

<pre><code>dat$subject &lt;- dat$group : dat$id
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

xyplot(value ~ component | group, data=dat.long, 
    pch=16, 
    strip = strip.custom(strip.names=TRUE,var.name=""group"" ), layout=c(5,1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/14KtA.png"" alt=""enter image description here""></p>

<p>Each individual of each group has $3$ repeated measures $x$,$y$,$z$ (I should join the points in the graphic to see the repeated measures).</p>

<p>I want to fit a MANOVA model using group as factor and $(x,y,z)$ is the multivariate response:
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right), \quad i=1,\ldots,5
$$
(of course we could use the default R parameterization $\mu_{ik}=\mu_{1k} + \alpha_{ik}$ by considering <code>group1</code>as the ""intercept"" for each response but I prefer ""my"" parameterization).</p>

<p>This model is fitted as follows using <code>lm()</code>:</p>

<pre><code>###  multivariate least-squares fitting  ###
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )
</code></pre>

<p>I think the model can also  be fitted with <code>gls()</code> as follows (but with a different fitting procedure) :</p>

<pre><code>### generalized least-squares fitting  ###
library(nlme)
gfit &lt;- gls(value ~ group*component, data=dat.long, correlation=corSymm(form= ~ 1 | subject))
</code></pre>

<p>Recall that <code>subject = group:id</code> is the identifier of the individuals. The <code>correlation=corSymm(form= ~ 1 | subject)</code> argument means that the responses $x$, $y$, $z$ for each individual are correlated. Here <code>corSymm</code> means a general, ""unrestricted"",  covariance structure (termed as ""unstructured"" in SAS language).</p>

<p>To check that <code>mfit</code> and <code>gfit</code> are equivalent, we can check for instance that we can deduce the estimated parameters of <code>mfit</code> from the estimated parameters of <code>gfit</code>and vice-versa (so the ""mean"" parameters have exactly the same fitted values):</p>

<pre><code>&gt; coef(mfit)
                  x          y          z
(Intercept)  3.1750  3.2250000  3.5625000
group5      -0.9500 -0.4750000  0.1125000
group2      -1.0750 -0.5678571 -0.2339286
group3      -0.7875 -0.1000000  0.1875000
group4      -0.3750  0.4000000 -0.0125000
&gt; coef(gfit)
      (Intercept)            group5            group2            group3 
        3.1750000        -0.9500000        -1.0750000        -0.7875000 
           group4        componenty        componentz group5:componenty 
       -0.3750000         0.0500000         0.3875000         0.4750000 
group2:componenty group3:componenty group4:componenty group5:componentz 
        0.5071429         0.6875000         0.7750000         1.0625000 
group2:componentz group3:componentz group4:componentz 
        0.8410714         0.9750000         0.3625000 
</code></pre>

<p>Now I want to test the ""component effect"". Rigorously speaking, writing the model as 
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right),
$$
I want to test the hypothesis $\boxed{H_0\colon \{\mu_{1i}=\mu_{2i}=\mu_{3i} \quad \forall i=1,2,3,4,5 \}}$.</p>

<p>Below are my attempts, one attempt with <code>gfit</code> and two attempts with <code>mfit()</code>:</p>

<pre><code>###########################################
## testing significance of the component ##
###########################################

&gt; ### with gfit  ###
&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
&gt; 
&gt; ### with mfit ###
&gt; library(car)
&gt; 
&gt; # first attempt : 
&gt; idata &lt;- data.frame(component=c(""x"",""y"",""z""))
&gt; ( av.ok &lt;- Anova(mfit, idata=idata, idesign=~component, type=""III"") )

Type III Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.84396  140.625      1     26 5.449e-12 ***
group            4   0.10369    0.752      4     26    0.5658    
component        1   0.04913    0.646      2     25    0.5328    
group:component  4   0.22360    0.818      8     52    0.5901    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; 
&gt; # second attempt :
&gt; linearHypothesis(mfit, ""(Intercept) = 0"", idata=idata, idesign=~component, iterms=""component"")

 Response transformation matrix:
  component1 component2
x          1          0
y          0          1
z         -1         -1

Sum of squares and products for the hypothesis:
           component1 component2
component1    1.20125    1.04625
component2    1.04625    0.91125

Sum of squares and products for error:
           component1 component2
component1   31.46179   14.67696
component2   14.67696   21.42304

Multivariate Tests: 
                 Df test stat  approx F num Df den Df  Pr(&gt;F)
Pillai            1 0.0491253 0.6457903      2     25 0.53277
Wilks             1 0.9508747 0.6457903      2     25 0.53277
Hotelling-Lawley  1 0.0516632 0.6457903      2     25 0.53277
Roy               1 0.0516632 0.6457903      2     25 0.53277
</code></pre>

<p>With <code>anova(gfit)</code> the component is significant, but not with my two attempts using <code>mfit</code> and the <code>car</code> package. </p>

<p>I know that <code>gls()</code> use a different fitting method than <code>lm()</code> but this is surely not the cause of the difference. </p>

<p>So my questions are :</p>

<ul>
<li>did I do something wrong ?</li>
<li>which method tests my $H_0$ hypothesis ?</li>
<li>what is the $H_0$ hypothesis of the other methods ?</li>
</ul>

<p>And I have an auxiliary question: how to get $\hat\Sigma$ with <code>mfit</code> and <code>gfit</code> ?</p>

<h2>Update 1</h2>

<p>Below is a reproducible example which simulates the dataset. 
Now I think I understand : both ANOVA methods are correct (the first one with <code>anova(gfit)</code> and the second one with <code>Anova(mfit, ...)</code>, <strong>and they yield very close results when using the type II sum of squares in <code>Anova(mfit, ...)</code></strong>.  For the above example: </p>

<pre><code>&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>is very close to </p>

<pre><code>&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>

<p>Below is the reproducible code with the data sampler (I simulate uncorrelated repeated measures but it suffices to include a covariance matrix in the <code>rmvnorm()</code> function to simulate correlated repeated measures) :</p>

<pre><code>library(mvtnorm)
library(nlme)
library(car)

# set data parameters 
I &lt;- 5 # number of groups
J &lt;- 16 # number of individuals per group
dat &lt;- data.frame(
    group = gl(I,J),
    id = gl(J,1,I*J),
    x=NA, 
    y=NA, 
    z=NA
)
Mu &lt;- c(1:I) # group means of components (assuming E(x)=E(y)=E(z) in each group)

# simulates data: 
for(i in 1:I){
    which.group.i &lt;- which(dat$group==i)
    dat[which.group.i,c(""x"",""y"",""z"")] &lt;- round(rmvnorm(n=J, mean=rep(Mu[i],3)), 1)
}

dat$subject &lt;- droplevels( dat$group : dat$id )
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

# multivariate least-squares fitting 
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )

# gls fitting
dat.long$order.xyz &lt;- as.numeric(dat.long$component)
gfit &lt;- gls(value ~ group*component , data=dat.long, correlation=corSymm(form=  ~ order.xyz | subject)) 

# compares ANOVA : 
anova(gfit)
idata &lt;- data.frame(component=c(""x"",""y"",""z""))
Anova(mfit, idata=idata, idesign=~component, type=""II"")
Anova(mfit, idata=idata, idesign=~component, type=""III"")
</code></pre>

<p>So now I wonder which type of sum of squares is the more appropriate one for my real study... but this is another question</p>

<h2>Update 2</h2>

<p>About my question <em>""how to get $\hat\Sigma$""</em>, here is the answer for <code>gls()</code>:</p>

<pre><code>&gt; getVarCov(gfit)
Marginal variance covariance matrix
        [,1]    [,2]    [,3]
[1,] 0.92909 0.47739 0.24628
[2,] 0.47739 0.92909 0.53369
[3,] 0.24628 0.53369 0.92909
  Standard Deviations: 0.96389 0.96389 0.96389 
</code></pre>

<p>That shows that <strong><code>mfit</code>and <code>gfit</code> were not equivalent models</strong>: <code>gfit</code>assumes the same variance for the three components.</p>

<p>In order to fit a fully unrestricted covariance matrix for the repeated measures, we have to type:</p>

<pre><code>gfit2 &lt;- gls(value ~ group*component , data=dat.long, 
    correlation=corSymm(form=  ~ 1 | subject), 
    weights=varIdent(form = ~1 | component))

&gt; summary(gfit2)
Generalized least squares fit by REML
  Model: value ~ group * component 
  Data: dat.long 
       AIC      BIC    logLik
  264.0077 313.4986 -111.0038

Correlation Structure: General
 Formula: ~1 | subject 
 Parameter estimate(s):
 Correlation: 
  1     2    
2 0.529      
3 0.300 0.616
Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | component 
 Parameter estimates:
       x        y        z 
1.000000 1.253534 1.169335 

....

Residual standard error: 0.8523997 
</code></pre>

<p>But yet I don't understand the extracted covariance matrix given by <code>getVarCov()</code> (but this is not important since we get this matrix with <code>summary(gfit2)</code>): </p>

<pre><code>   &gt; getVarCov(gfit2)
    Error in t(S * sqrt(vars)) : 
      dims [product 9] do not match the length of object [0]
    &gt; getVarCov(gfit2, individual=""1:1"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 0.72659 0.48164 0.25500
    [2,] 0.48164 1.14170 0.65562
    [3,] 0.25500 0.65562 0.99349
      Standard Deviations: 0.8524 1.0685 0.99674 
    &gt; getVarCov(gfit2, individual=""1:2"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 1.14170 0.56319 0.27337
    [2,] 0.56319 0.99349 0.52302
    [3,] 0.27337 0.52302 0.72659
      Standard Deviations: 1.0685 0.99674 0.8524 
</code></pre>

<p>Unfortunately, the <code>anova(gfit2)</code> table is not as close to <code>Anova(mfit, ..., type=""II"")</code> as <code>anova(gfit)</code>:</p>

<pre><code>&gt; anova(gfit2)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 498.1744  &lt;.0001
group               4   1.0514  0.3864
component           2  13.1801  &lt;.0001
group:component     8   0.8310  0.5780

&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>
"
"0.0419590679148345","0.0412744171706498"," 48777","<p>I need to automate the transformation on some linear regression models. There is only one predictor in this case. Sometimes i get a good model with the original variables, sometimes i need to log the predictor, and in some cases log both sides.</p>

<p>I'm using R, so what kind of tests/packages can i use to automate this? I'm using Pearson correlation now, but i'm not sure if it makes sense.</p>

<p>thanks!</p>

<p>PS: This may look a duplicate question, but i couldn't find yet the methodology to apply.</p>
"
"0.11194225638933","0.110115682231433"," 49543","<p>I have sampled 8 individuals (birds) from two regions. For each of these 16 individuals I have sampled 9 feathers that have each grown in sequential order (from 1 to 9). Next I have measure both carbon and nitrogen isotopes in each of the feather samples. I have plotted (Fig. 1) my data and in some individuals the relationship between the isotope value and feather position is linear, in some cases monotonic, and in a few cases neither.</p>

<p><img src=""http://i.stack.imgur.com/Nz6oi.png"" alt=""Delta15N by feather position for all individuals in the &quot;South&quot; region""></p>

<p>I am looking for a non-parametric (?) method to test these three alternative hypotheses for each individual in my dataset.</p>

<p>H1: If the 9 sequentially grown feathers on each individual are grown in the same place under the same diet, the isotope values will be highly correlated with feather position and the regression line essentially flat.</p>

<p>H2: If an individual moves or changes their diet in a systematic way, the isotope values and feather position will correlated but the regression line will be either positive or negative.</p>

<p>H3: If the individual abruptly moves or changes their diet, the isotope values will be uncorrelated and the relationship will not be linear.</p>

<p>Finally, I would like to test the hypothesis that their are differences between the relationship of feather isotopes and feather position is different in individuals between the two regions.</p>

<p>This would more intuitive if all the data (for each individual) was linear or monotonic, but they are not. From my nascent understanding for using either a Pearson's correlation or GLM, these tests assume the data is linear, while the Spearman's assumes the data is monotonic.</p>

<p>Sample Data:</p>

<pre><code>WW_Wing_SI &lt;- structure(list(Individual_ID = c(""WW_08A_02"", ""WW_08A_02"", ""WW_08A_02"", 
""WW_08A_02"", ""WW_08A_02"", ""WW_08A_02"", ""WW_08A_02"", ""WW_08A_02"", 
""WW_08A_02"", ""WW_08A_03"", ""WW_08A_03"", ""WW_08A_03"", ""WW_08A_03"", 
""WW_08A_03"", ""WW_08A_03"", ""WW_08A_03"", ""WW_08A_03"", ""WW_08A_03"", 
""WW_08A_04"", ""WW_08A_04"", ""WW_08A_04"", ""WW_08A_04"", ""WW_08A_04"", 
""WW_08A_04"", ""WW_08A_04"", ""WW_08A_04"", ""WW_08A_04"", ""WW_08A_05"", 
""WW_08A_05"", ""WW_08A_05"", ""WW_08A_05"", ""WW_08A_05"", ""WW_08A_05"", 
""WW_08A_05"", ""WW_08A_05"", ""WW_08A_05"", ""WW_08A_06"", ""WW_08A_06"", 
""WW_08A_06"", ""WW_08A_06"", ""WW_08A_06"", ""WW_08A_06"", ""WW_08A_06"", 
""WW_08A_06"", ""WW_08A_06"", ""WW_08A_08"", ""WW_08A_08"", ""WW_08A_08"", 
""WW_08A_08"", ""WW_08A_08"", ""WW_08A_08"", ""WW_08A_08"", ""WW_08A_08"", 
""WW_08A_08"", ""WW_08A_09"", ""WW_08A_09"", ""WW_08A_09"", ""WW_08A_09"", 
""WW_08A_09"", ""WW_08A_09"", ""WW_08A_09"", ""WW_08A_09"", ""WW_08A_09"", 
""WW_08A_13"", ""WW_08A_13"", ""WW_08A_13"", ""WW_08A_13"", ""WW_08A_13"", 
""WW_08A_13"", ""WW_08A_13"", ""WW_08A_13"", ""WW_08A_13"", ""WW_08B_02"", 
""WW_08B_02"", ""WW_08B_02"", ""WW_08B_02"", ""WW_08B_02"", ""WW_08B_02"", 
""WW_08B_02"", ""WW_08B_02"", ""WW_08B_02"", ""WW_08G_01"", ""WW_08G_01"", 
""WW_08G_01"", ""WW_08G_01"", ""WW_08G_01"", ""WW_08G_01"", ""WW_08G_01"", 
""WW_08G_01"", ""WW_08G_01"", ""WW_08G_02"", ""WW_08G_02"", ""WW_08G_02"", 
""WW_08G_02"", ""WW_08G_02"", ""WW_08G_02"", ""WW_08G_02"", ""WW_08G_02"", 
""WW_08G_02"", ""WW_08G_05"", ""WW_08G_05"", ""WW_08G_05"", ""WW_08G_05"", 
""WW_08G_05"", ""WW_08G_05"", ""WW_08G_05"", ""WW_08G_05"", ""WW_08G_05"", 
""WW_08G_07"", ""WW_08G_07"", ""WW_08G_07"", ""WW_08G_07"", ""WW_08G_07"", 
""WW_08G_07"", ""WW_08G_07"", ""WW_08G_07"", ""WW_08G_07"", ""WW_08I_01"", 
""WW_08I_01"", ""WW_08I_01"", ""WW_08I_01"", ""WW_08I_01"", ""WW_08I_01"", 
""WW_08I_01"", ""WW_08I_01"", ""WW_08I_01"", ""WW_08I_03"", ""WW_08I_03"", 
""WW_08I_03"", ""WW_08I_03"", ""WW_08I_03"", ""WW_08I_03"", ""WW_08I_03"", 
""WW_08I_03"", ""WW_08I_03"", ""WW_08I_07"", ""WW_08I_07"", ""WW_08I_07"", 
""WW_08I_07"", ""WW_08I_07"", ""WW_08I_07"", ""WW_08I_07"", ""WW_08I_07"", 
""WW_08I_07"", ""WW_08I_12"", ""WW_08I_12"", ""WW_08I_12"", ""WW_08I_12"", 
""WW_08I_12"", ""WW_08I_12"", ""WW_08I_12"", ""WW_08I_12"", ""WW_08I_12""
), Feather = c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", 
""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", 
""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", 
""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", 
""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", 
""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", 
""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", 
""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", 
""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", 
""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9""
), Delta13C = c(-18.67, -19.16, -20.38, -20.96, -21.61, -21.65, 
-21.31, -20.8, -21.28, -20.06, -20.3, -21.21, -22.9, -22.87, 
-21.13, -20.68, -20.58, -20.69, -16.54, -15.6, -16.61, -19.65, 
-20.98, -21.18, -21.7, -21.18, -21.33, -20.33, -20.28, -20.58, 
-20.8, -21.24, -20.94, -20.54, -21.04, -20.42, -21.28, -21.24, 
-21.22, -21.2, -21.47, -21.23, -21.89, -21.89, -21.6, -23.86, 
-23.95, -24, -24.16, -24.93, -24.93, -24.48, -24.17, -23.1, -21.3, 
-21.44, -21.49, -21.49, -21.1, -20.84, -20.78, -21.58, -20.76, 
-21.34, -24.13, -23.03, -21.77, -21.4, -21.57, -21.45, -21.32, 
-21.59, -20.87, -20.95, -20.76, -20.9, -21.02, -20.84, -21.11, 
-20.64, -20.11, -20.32, -20.02, -19.92, -20.05, -20.23, -20.73, 
-20.91, -19.87, -19.58, -19.35, -19.38, -19.7, -19.94, -20.43, 
-20.08, -20.81, -20.9, -19.24, -21.2, -21.29, -21.85, -22.22, 
-22.34, -22.42, -22.69, -22.75, -22.73, -21.61, -21.42, -21.84, 
-21.68, -21.79, -21.49, -21.88, -21.62, -21.54, -18.3, -18.53, 
-19.55, -20.18, -20.96, -21.08, -21.5, -17.42, -13.18, -22.3, 
-22.2, -22.18, -22.14, -21.55, -20.85, -23.1, -20.75, -20.9, 
-21.6, -21.77, -22.17, -22.21, -22.24, -22.47, -22.19, -21.89, 
-21.89, -24.12, -24.08, -24, -24.2, -24.16, -22.87, -22.51, -22.12, 
-22.3), Delta15N = c(7.35, 7.27, 7.23, 7.07, 7.13, 7.38, 6.98, 
6.88, 6.72, 5.72, 5.76, 5.51, 6.12, 5.8, 5.34, 5.47, 5.78, 6.2, 
7.33, 7.45, 7.3, 7.19, 7.56, 7.54, 8.12, 7.71, 7.44, 9.45, 9.81, 
9.7, 9.08, 8.6, 9.34, 10.38, 9.67, 10.48, 7.71, 7.76, 7.95, 7.73, 
7.69, 7.24, 6.64, 6.42, 7.31, 8.26, 8.1, 8.07, 8.7, 8.98, 9.44, 
7.84, 7.26, 6.05, 8.04, 7.73, 7.55, 6.77, 6.99, 6.84, 7.09, 6.78, 
7.07, 6.96, 6, 5.91, 6.48, 7.06, 7.27, 8.32, 7.85, 7.45, 6.9, 
6.73, 6.97, 6.67, 6.76, 6.59, 6.58, 6.42, 6.3, 11.64, 11.83, 
11.66, 11.3, 11.32, 11.29, 10.91, 10.77, 11.4, 9.5, 9.55, 9.22, 
8.84, 8.89, 9.14, 9.8, 9.13, 8.51, 7.7, 7.8, 8.29, 9.65, 10.25, 
13.67, 14.66, 13.48, 13.76, 8.7, 8.7, 8.36, 8.11, 8.47, 8.13, 
6.88, 7.21, 7.16, 14.07, 13.91, 14.07, 14.26, 13.99, 13.51, 13.77, 
14.83, 15.13, 10.93, 10.85, 11.31, 11.28, 11.96, 13.41, 8.12, 
12.96, 12.03, 8.16, 8.29, 8.43, 8.53, 8.1, 7.65, 7.6, 7.51, 7.38, 
6.44, 6.18, 6.33, 6.49, 6.34, 8.65, 7.73, 7.13, 7.07), Region = c(""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"")), .Names = c(""Individual_ID"", 
""Feather"", ""Delta13C"", ""Delta15N"", ""Region""), row.names = c(NA, 
153L), class = ""data.frame"")
</code></pre>
"
"0.167836271659338","0.165097668682599"," 49832","<p>In a multi-level model, what are the practical and interpretation-related implications of estimating versus not-estimating random effect correlation parameters?  The practical reason for asking this is that in the lmer framework in R, there is no implemented method for estimating p-values via MCMC techniques when estimates are made in the model of the correlations between parameters. </p>

<p>For example, looking at <a href=""http://stats.stackexchange.com/questions/13166/rs-lmer-cheat-sheet"">this example</a> (portions quoted below), what are the practical implications of M2 versus M3.  Obviously, in one case P5 will not be estimated and in the other it will.</p>

<p>Questions</p>

<ol>
<li>For practical reasons (the desire to get a p-value through MCMC techniques) one might want to fit a model without correlations between random effects even if P5 is substantially non-zero.  If one does this, and then estimates p-values via the MCMC technique, are the results interpretable?  (I know @Ben Bolker has previously mentioned that <a href=""http://stats.stackexchange.com/questions/5517/how-can-one-do-an-mcmc-hypothesis-test-on-a-mixed-effect-regression-model-with-r"">""combining significance testing with MCMC is a little bit incoherent, statistically, although I understand the urge to do so (getting confidence intervals is more supportable)""</a>, so if it will make you sleep better at night pretend I said confidence intervals.)</li>
<li>If one fails to estimate P5, is that the same as asserting that it is 0?</li>
<li>If P5 really is non-zero, then in what way are the estimated values of P1-P4 affected?</li>
<li>If P5 really is non-zero, then in what way are the estimates of error for P1-P4 affected?</li>
<li>If P5 really is non-zero, then in what ways are interpretations of a model failing to include P5 flawed?</li>
</ol>

<p>Borrowing from @Mike Lawrence's answer (those more knowledgeable than I are free to replace this with full model notation, I'm not entirely confident I can do so with reasonable fidelity):</p>

<p>M2:  <code>V1 ~ (1|V2) + V3 + (0+V3|V2)</code> (Estimates P1 - P4)</p>

<p>M3:  <code>V1 ~ (1+V3|V2) + V3</code> (Estimates P1-P5)</p>

<p><em>Parameters that might be estimated:</em></p>

<p><strong>P1</strong>: A global intercept</p>

<p><strong>P2</strong>: Random effect intercepts for V2 (i.e. for each level of V2, that level's intercept's deviation from the global intercept)</p>

<p><strong>P3</strong>: A single global estimate for the effect (slope) of V3</p>

<p><strong>P4</strong>: The effect of V3 within each level of V2 (more specifically, the degree to which the V3 effect within a given level deviates from the global effect of V3), while enforcing a zero correlation between the intercept deviations and V3 effect deviations across levels of V2.</p>

<p><strong>P5</strong>: The correlation between intercept deviations and V3 deviations across levels of V2</p>

<p>Answers derived from a sufficiently large and broad simulation along with accompanying code in R using lmer would be acceptable.</p>
"
"0.19749240560299","0.202364479389724"," 50086","<p>Assume for example a trivariate Gaussian model:
$$
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \quad (*)
$$
with ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$. </p>

<p>The Bayesian conjugate theory of this model is well known. 
This model is the most simple case of a  multivariate linear regression model. 
And more generally, there is a well known Bayesian conjugate theory of multivariate linear regression, which is  the extension to the case when  the multivariate mean  ${\boldsymbol \mu}= {\boldsymbol \mu}(x_i)$ is allowed to depend on the covariates $x_i$ of individual $i \in \{1, \ldots, n \}$, with linear constraints about the multivariate means ${\boldsymbol \mu}(x_i)$. See for instance <a href=""http://books.google.be/books?id=GL8VS9i_B2AC&amp;dq=bayesian%20econometrics%20bayesm&amp;hl=fr&amp;source=gbs_navlinks_s"" rel=""nofollow"">Rossi &amp; al's book</a> accompanied by the crantastic <a href=""http://cran.r-project.org/web/packages/bayesm/index.html"" rel=""nofollow""><code>bayesm</code></a> package for <code>R</code>. 
We know in addition that the Jeffreys prior is a limit form of the conjugate prior  distributions.</p>

<p>Let us come back to the simple multivariate Gaussian model $(*)$. Instead of generalizing this model to the case of linearly dependent multivariate means ${\boldsymbol \mu}(x_i)$ depending on individuals $i=1,\ldots,n$, we can consider a more restrictive model by assuming linear constraints about the components $\mu_1$, $\mu_2$, $\mu_3$ of the multivariate mean ${\boldsymbol \mu}$. </p>

<h3>Example</h3>

<p>Consider some concentrations $x_{i,t}$ of $4$ blood samples $i=1,2,3,4$ measured at $3$ timepoints $t=t_1,t_2,t_3$. 
Assume that the samples are independent and that the series of the three measurements 
$(x_{i,t_1},x_{i,t_2},x_{i,t_3}) \sim {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right)$ for each sample $i$ with a mean 
${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$ whose three components are 
linearly related to the timepoints: $\mu_j = \alpha + \beta t_j$.</p>

<p>This example falls into the context of <em>Multivariate linear regression with a within-design structure</em>. 
See for instance <a href=""http://wweb.uta.edu/management/Dr.Casper/Fall10/BSAD6314/Coursematerial/O%27Brien%20&amp;%20Kaiser%201985%20-%20MANOVA%20-%20RM%20-%20Psy%20Bull%2085.pdf"" rel=""nofollow"">O'Brien &amp; Kaiser 1985</a> and <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a></p>

<p>So my example is a simple example of this situation because there are only some predictors (the timepoints) for the components of the mean, but there are no predictors for individuals. This example could be written as follows:
$$
(**) \left\{\begin{matrix} 
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \\ 
{\boldsymbol \mu} = X {\boldsymbol \beta}
\end{matrix}\right.
$$
with ${\boldsymbol \beta}=(\alpha, \beta)'$ and $X=\begin{pmatrix} 1 &amp; t_1 \\ 1 &amp; t_2 \\ 1 &amp; t_3 \end{pmatrix}$ is the matrix of covariates for the components of the multivariate mean ${\boldsymbol \mu}$. The second line of $(**)$ could be termed as the <em>within design</em>, or the <em>repeated measures design</em>, or the <em>structural design</em> (I would appreciate if a specialist had some comments about this vocabulary).</p>

<p>I think such a model can be fitted as a generalized least-squares model, as follows in  <code>R</code> :</p>

<pre><code>gls(response ~ ""between covariates"" , data=dat, 
  correlation=corSymm(form=  ~ ""within covariates"" | individual ))
</code></pre>

<p>(after stacking the data in long format).</p>

<p><strong>My first question</strong> is Bayesian: what about the Bayesian analysis of model $(**)$ and more generally the Bayesian analysis of the multivariate linear regression model with a structural design ? 
Is there a conjugate family ? What about the Jeffreys prior ? Is there an appropriate R package to perform this Bayesian analysis ?</p>

<p><strong>My second question</strong> is not Bayesian: I have recently discovered some possibilities of John Fox's great <code>car</code> package to analyse 
such models with ordinary least squares theory (the <code>Anova()</code> function with the <code>idesign</code> argument --- see <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a>). Perhaps I'm wrong, but I am under the impression that this package only allows to get the MANOVA table (sum of squares analysis) with an orthogonal matrix $X$, and I'd like to get (exact) confidence intervals about the within-design parameters for an arbitrary matrix $X$, as well as prediction intervals. Is there a way to do so with <code>R</code> using ordinary least squares ?  </p>
"
"0.0726752374667264","0.0714893875923587"," 50454","<p>From what I understood, these models differ from CARTs for regression, mostly because they fit a linear model at the leaves of the tree instead of simply taking an average. They also ""smooth"" the tree by generating linear models in the intermediate steps of the tree growth process.</p>

<p>I have been using the R implementation of them a bit for regression getting very good results. But I wonder about the assumptions of a usual linear model? Multicolinearity, Autocorrelation, Non-Normality being violated doesnt worry people in this case? </p>
"
"0.0419590679148345","0.0412744171706498"," 52785","<p>I'm only a linguist, so my knowledge of statistics is very basic.</p>

<p>I fitted a logistic regression model with R (with <code>lrm(formula, y=T, x=T)</code>), and when I use the option <code>validate(lrm)</code>, I get some statistics I don't really understand.</p>

<pre><code>index.orig   training   test optimism index.corrected     n
Dxy           0.5984   0.6112  0.5461   0.0651          0.5333 40
R2            0.3258   0.3676  0.2929   0.0747          0.2511 40
Intercept     0.0000   0.0000 -0.0105   0.0105         -0.0105 40
Slope         1.0000   1.0000  0.8427   0.1573          0.8427 40
Emax          0.0000   0.0000  0.0399   0.0399          0.0399 40
D             0.2713   0.3176  0.2394   0.0782          0.1931 40
U            -0.0177  -0.0177  0.0092  -0.0269          0.0092 40
Q             0.2890   0.3353  0.2302   0.1051          0.1839 40
B             0.1864   0.1772  0.1972  -0.0201          0.2064 40
g             1.4632   1.6642  1.3460   0.3182          1.1449 40
gp            0.2840   0.3011  0.2703   0.0308          0.2532 40
</code></pre>

<p>I don't really understand most of that. I think <code>R2</code> and <code>Dxy</code> are supposed to be statistics of how good the predictors are, but I'm not sure how I should interpret the values, does the corrected <code>Dyx = 0.651</code> mean that there is a strong correlation, while the corrected <code>R2 = 0.0747</code> means that the correlation is very weak? I think the model is overfitted, but I'm not sure if I'm right.</p>

<p>Also, the other statistics are totally strange to me. What are <code>Emax, D, U, Q, B, g</code>, and <code>gp</code>?</p>
"
"0.0593390829096927","0.0583708405417779"," 54668","<p>I want to know if it is possible for a library in R to evaluate the association of independent variables and create a formula? I am trying to come up with a model to predict power consumption of a machine, using some hardware counters and performance attributes. When I use linear regression, I have no problem since I could represent my formula like <code>power~lm(a1+a2+a3+a4)</code>, but for the non-linear case, I am not sure what would be the formula or which model should I choose. 
I would want to have a way to do this:</p>

<pre><code>power ~ &lt;some-non-linear-reg-pkg&gt;(a1+a2+a3+non-linear(a4))
</code></pre>

<p>I reviewed some packages for non-linear regression such as <code>nls</code> and <code>gnm</code>, and they expect a formula to be provided by the user. I am however able to identify which variables have linear associations and which are non-linear (by performing correlation tests), the problem is building a formula out of them. </p>
"
"0.106148978486855","0.130521167355216"," 56521","<p>I need to calculate the regression variance ($\sigma^2$) in order to estimate both the confidence intervals and the prediction intervals in a gls regression analysis.  For the analysis, the covariance matrix ($V$) of the response variable ($y$) is known in advance, and so I use it directly as the weighting matrix (=$V^{-1}$) in the gls regression analysis.</p>

<p>The regression variance is a weighted sum of the residual error:
$\sigma^2 = \frac{ (Y â€“ X\beta)^T C^{-1} (Y â€“ X\beta)}{n â€“ p}$</p>

<p>My question/problem is how to determine the weighting matrix $C^{-1}$?  $C$ cannot be set equal to $V$ since (according to the above equation) $C$ must be dimensionless while $V$ has the same units as $\sigma^2$.</p>

<p>Based on my reading of the literature and available texts, it seems that $C$ is the correlation matrix and is a scaled or normalized form of the covariance matrix $V$.  i.e., $V = Var(\epsilon^2) = \sigma^2 C$.  But my problem is that $\sigma^2$ is not yet known, and so I need another way find $C$ from $V$.</p>

<p>R functions such as gls() will compute the regression variance (if I knew how gls() does this, it would answer my question).  However I cannot use gls() in this case since I am specifying a user-defined covariance (weighting) matrix, and gls() only accepts a limited set of specific correlation structures.</p>

<p>In fact a possible solution can be found in this <a href=""http://stats.stackexchange.com/questions/14426/prediction-with-gls"">earlier post</a> where an equation for the SEE (or sigma2) for a GLS regression was cited :</p>

<p>GLS calc of SEE: sqrt( sum( ( residuals from linear model) ^ 2 * glsWeight ) ) / sum( glsWeight ) * length( glsWeight ) / residualDegreeFreedom )</p>

<p>However I am unable to ascertain the validity of this equation and cannot find its source reference.</p>
"
"0.0484501583111509","0.0714893875923587"," 56900","<p>I have a 20-yr dataset of an annual count of species abundance for a set of polygons (~200 irregularly shaped, continuous polygons).  I have been using regression analysis to infer trends (change in count per year) for each polygon, as well as aggregations of polygon data based on management boundaries.</p>

<p>I am sure that there is spatial autocorrelation in the data, which is sure to impact the regression analysis for the aggregated data.  My question is - how do I run a SAC test for time series data?  Do I need to look at the SAC of residuals from my regression for each year (global Moran's I)?  Or can I run one test with all years?</p>

<p>Once I've tested that yes there is SAC, is there an easy was to address this?  My stats background is minimal and everything I've read on spatio-temporal modeling sounds very complex.  I know that R has a distance-weighted autocovariate function - is this at all simple to use?</p>

<p>I'm really quite confused on how to assess/addess SAC for this problem and would very much appreciate any suggestions, links, or references. Thanks in advance!</p>
"
"0.0839181358296689","0.0825488343412996"," 57180","<p>I am using a binomial regression model for presence/absence, with 20 independent variables to test. The data has x and y coordinates and I would like to understand how can I take into account the spatial autocorrelation.</p>

<p>I already studied the correlation between the variables and run the same model for 1000 different samples (I have a big dataset that allows me to do this) to understand the distribution of each parameter and check for variables that might be introducing problems in my model. </p>

<p><code>glm_model &lt;- glm(PA ~ Var1 + Var2 + Var3 + Var4 + Var5,family=binomial(link=logit)</code>)</p>

<p>However I believe I also need to account for spatial autocorrelation. I saw that there is a package that might help me (<code>spdep</code>), however I am not sure I completely understand if I can use my model or not. My question is what are my options ?</p>
"
"0.0726752374667264","0.0714893875923587"," 57448","<p>I am running a model (logistic regression) with 20 independent variables in R. </p>

<p>Before running the model I calculated the correlation between all the variables and finally selected my variables by also checking ""visually"" the histograms of each variable in the case of presence and again in the case of absence. In situations where I don't see any obvious distribution associated to both presence &amp; absence, I discard the variable.</p>

<p>I would like to make ""official"" calculations for the level of relation between Presence/Absence and each variable (how much each variable contributes to the Presence/Absence), for example with <code>Cramer's V index</code>, but the available function I find is from the package <code>vcd</code> and has some limitations: 
doesn't give the <code>Cramer's V</code> (as well as the Phi-Coefficient Contingency Coeff.) for each independent variable, and it doesn't run for one independent variable.</p>

<p>I might be missing some other obvious way to do this. Any help is appreciated.</p>
"
"0.0839181358296689","0.0825488343412996"," 58811","<p>1. Which one is NOT a linear regression models? Please give a 1-2 sentences brief
explanation to your choice.<br>
(a)  $y_i = Î²_0 +\exp(Î²_1x_i)+E_i, i = 1, 2, \ldots, n$<br>
(b)  $y_i = Î²_0 + Î²_1x_i + Î²_2 x_{ii} + E_i , i = 1, 2, \ldots, n$<br>
(c)  $y_i =Î²_0\exp(x_i)+Î²_2x_i^7 +E_i, i=1, 2,\ldots, n$  </p>

<p>2. Suppose $X$ and $Y$ has linear correlation coefficient $r = 0.5$, and there are 77 observations, what is the test statistic for the hypothesis test  </p>

<p>$$H0:Î²_1=0 \quad\text{vs.}\quad Ha:Î²_1\neq0 $$</p>

<p>where $Î²_1$ comes from the simple linear regression model below? Please give a 1-2
sentences brief explanation to your choice.   $\quad Y = Î²_0 + Î²_1X + E$  </p>

<p>(a). Not enough information<br>
(b). 5<br>
(c). 0.25  </p>

<p>3. Which model is more possible to have smaller $R^2$? Please give a 1-2 sentences brief explanation to your choice.<br>
A: $Y=Î²_0+Î²_1X_1+E$<br>
B: $Y=Î²_0^*+Î²_1^*X_1+Î²_2^*X_2+E^*$<br>
where $Y$ and $X_1$ in model A and B are the same.</p>

<p>(a). Not enough information<br>
(b). Model A<br>
(c). Model B  </p>
"
"0.101209079873851","0.124447050177862"," 59166","<p>Iâ€™ve simulated some data consisting of one response variable (â€˜yâ€™) and two collinear predictor variables (â€˜Amountâ€™ and â€˜MPSâ€™), where collinearity arises from one of two causes: (1) Amount causes MPS, or (2) Amount and MPS are jointly affected by an unmeasured variable. </p>

<p>What I'm trying to do is figure out whether path analysis can discriminate between these two causes of collinearity. But I'm having trouble specifying a path model for collinearity scenario (2). </p>

<p>My question:
<strong>Is it possible to specify a path model that implies that two exogenous variables are jointly influenced by an unmeasured variable?</strong></p>

<p>I'm working in lavaan, but answers for how to do this conceptually would also be appreciated (if you aren't familiar with lavaan).</p>

<p>Here are my data, simulated in R:</p>

<pre><code># collinearity cause (1)
Amount &lt;- rnorm(n=350, mean=0, sd=1)   
MPS &lt;- rnorm(n=350, mean=0.76*Amount, sd=0.653) 
y &lt;- rnorm(n=350, mean=0.367*Amount + 0.367*MPS, sd=0.72) 

# collinearity cause (2)
Lurking &lt;- rnorm(n=350, mean=0, sd=1) 
Amount &lt;- rnorm(n=350, mean=0.872*Lurking, sd=0.486)  
MPS &lt;- rnorm(n=350, mean=0.872*Lurking, sd=0.486)  
y &lt;- rnorm(n=350, mean=0.367*Amount + 0.367*MPS, sd=0.72)         
</code></pre>

<p>And this is my path model for (1), specified in lavaan:</p>

<pre><code>model1 &lt;- '
  #regressions
  y ~ Amount
  y ~ MPS
  MPS ~ Amount
  '
</code></pre>

<p>And this is a path model I tried for (2):</p>

<pre><code>model2&lt;- '
  #regressions
  y ~ Amount
  y ~ MPS
  #residual correlations
  MPS ~~ Amount
  '
</code></pre>

<p>so for path model (2) my approach was to specify a residual correlation between MPS and Amount. I'm uncertain if this is the correct approach. but even if it is, it doesn't work â€“ to make it work I have to specify that exogenous variables are not fixed, and this uses up my degrees of freedom so I can't test the model.</p>

<p>If anyone has any suggestions for how I can do this â€“ or if it is possible at all - I'd really appreciate it.</p>
"
"0.0839181358296689","0.0825488343412996"," 59311","<p>I'm working on a behavoural scorecard modelling exercise, and many of the decisions taken to date have been based on the experience of a consulting credit analyst (whose experience software-wise is SAS) as I am primarily in BI. So far I have:</p>

<ul>
<li>a linux pc with 32gb of ram and an i7 processor</li>
<li>an observation window  </li>
<li>~90 potential characteristics </li>
<li>a binary outcome</li>
</ul>

<p>In <strong>R</strong>, I have</p>

<ol>
<li>loaded the dataset (225k obs of 88 vars, 1 outcome)</li>
<li>split the dataset up based on the recommendations/examples in the package <strong>caret</strong> i.e. predictors and outcomes split up (150k obs in training sample)</li>
<li>removed any variables showing a high degree of correlation (caret::findCorrelation)</li>
<li>cut all continuous variables into categorical intervals </li>
<li>reduced the number of variables based on near zero variance, missing values, and low information value (IV) (150k obs of 48 vars)</li>
<li>tried bestglm::bestglm, caret::train (with glm and glmnet), FWDselect::selection, FWDselect::qselection but eventually had to interrupt each of these due to not completing after 4 hours of 100% CPU usage</li>
<li>used FactoMineR:MCA to perform a multiple correspondence analysis (on predictors only)</li>
</ol>

<p>What I would like to do is have a selection of logistic regression models for say 4, 8, 12, and 16 variables that are the most predictive models at each point.  I'm not sure if I'm going in the correct direction here with MCA as I've mainly been simply trying to find something that works in a timely fashion for reducing my variables further or going directly to variable selection steps.</p>

<p>I would appreciate any advice on how to do any of step 6 better, whether 7 makes sense and what step 8 should be. </p>

<p>Thanks,</p>

<p>Steph</p>

<p>PS Design decisions up to 6&amp;7 can't be revised so please, no telling me off for them! </p>
"
"0.0419590679148345","0.0412744171706498"," 59866","<p>I have two variables which show significant correlation (Spearman). I would like to graphically show the strength of the relationship, much like showing a linear regression fit with confidence bands. What would be the best correct way of doing that?</p>
"
"0.15699645640569","0.154434727891351"," 61532","<p>I am trying to find the best fit between an species dataset and prevailing climatic conditions, in order to be able to predict the environmental conditions from the species dataset (paleoclimate research). </p>

<p>I have 15 species(sp1-sp15), expressed as relative amounts (some are 0).
I have done some data exploration in excel, and have seen that I get good fits using the ratio  sp2/(sp1+sp2). </p>

<p>I have done multiple linear regression on this dataset, but none give a correlation as good as the ratio I stumbled on. The ultimate goal is of course to test whether this ratio I stumbled upon is yields the best model to explain changes in the climate variable.</p>

<p>I would thus like to create the ratios: <code>sum(spj)/sum(spj)</code>, where spj refers to one of the species. Here, both the numerator and denominator can be any possible combination of all species. </p>

<p>Can I generate a formula model to include a ratio? Can I then select the best model using regbsubsets?</p>

<p><strong>EDIT</strong>
Based on answers below, this will not be possible for all 15 variables. How would it be possibel for 5 variables (I have seen this published before, using R).</p>

<p><strong>EDIT</strong>
On stackoverflow (where I posted the same question), I got the comment to use the function (I), to write expressions inside formulae. However, if I use</p>

<pre><code>leapsMAT&lt;-regsubsets(x1 ~ I(1+ (y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 + y12+ y13 + y14 +y15 )/(y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 + y12+ y13 + y14 + y15)), force.in= FTFALSE,nvmax=15,data=my.data, nbest=1)
</code></pre>

<p>the regsubsets doesn't recognize the variables as variables, of course. How do I implement the I(function), or is there another way to deal with this?</p>

<p>At the moment I have obtained the same linear regression using two methods:</p>

<pre><code>library(leaps)
attach(my.data)
FTFALSE&lt;-c(FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE)
leapsMAT&lt;-regsubsets(x1 ~ 1+ y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 + y12+ y13 + y14 + y15 , force.in= FTFALSE,nvmax=15,data=my.data, nbest=1)
</code></pre>

<p>And, using a longer method specified here (implemented from somewhere else on the internet):</p>

<pre><code>allModelsList &lt;- apply(regMat, 1, function(x) as.formula (paste(c(""x1 ~ 1"", namevar2[x]),collapse="" + "" )))
allModelsList
warnings()

#Calculating the model
my.data
allModelsResults &lt;- lapply(allModelsList, function(x) lm(x, data=my.data))
allModelsResults

dfCoefNum   &lt;- ldply(allModelsResults, function(x) as.data.frame(t(coef(x))))

dfStdErrors &lt;- ldply(allModelsResults, function(x) as.data.frame(t(coef(summary(x))[, ""Std. Error""])))


dftValues   &lt;- ldply(allModelsResults, function(x) as.data.frame(t(coef(summary(x))[, ""t value""])))

dfpValues   &lt;- ldply(allModelsResults, function(x) as.data.frame(t(coef(summary(x))[, ""Pr(&gt;|t|)""]))) 
dfpValues
(warnings)
# rename DFs so we know what the column contains
names(dfStdErrors) &lt;- paste(""se"", names(dfStdErrors), sep=""."")
names(dftValues) &lt;- paste(""t"", names(dftValues), sep=""."")
names(dfpValues) &lt;- paste(""p"", names(dfpValues), sep=""."")

# p-value for overall model fit
calcPval &lt;- function(x){
    fstat &lt;- summary(x)$fstatistic
    pVal &lt;- pf(fstat[1], fstat[2], fstat[3], lower.tail = FALSE)
    return(pVal)
}

# Before creating ONE data frame with all important entries,
# we need to compute some more indices 
NoOfCoef &lt;- unlist(apply(regMat, 1, sum))
R2       &lt;- unlist(lapply(allModelsResults, function(x)
                          summary(x)$r.squared))
    adjR2    &lt;- unlist(lapply(allModelsResults, function(x)
                              summary(x)$adj.r.squared))
RMSE     &lt;- unlist(lapply(allModelsResults, function(x)
                          summary(x)$sigma))
fstats   &lt;- unlist(lapply(allModelsResults, calcPval))



# now we can combine all the data into one data frame
results &lt;- data.frame( model = as.character(allModelsList),
                       NoOfCoef = NoOfCoef,
                       dfCoefNum,
                       dfStdErrors,
                       dftValues,
                       dfpValues,
                       R2 = R2,
                       adjR2 = adjR2,
                       RMSE = RMSE,
                       pF = fstats  )
results[1:20,]
# round the results
results[,-c(1,2)] &lt;- round(results[,-c(1,2)], 3)
results

model.maxRadj&lt;-which(results$adjR2 == max(results$adjR2), arr.ind = TRUE)
maxRadj&lt;-results[model.maxRadj,]
</code></pre>

<p>Many thanks in advance! Please let me know if more information is required.</p>
"
"0.134568391204877","0.143403675899112"," 61869","<p>I am trying to replicate a path analysis SEM model using Lavaan in R, and was very confused about the results that it gave regarding the model fit statistics. </p>

<p><strong>The code is as follows:</strong> </p>

<pre><code>#Import Package
library(lavaan)

#Input Correlation Matrix
sigma &lt;- matrix(c(1.00, -0.03,  0.39, -0.05, -0.08,
                 -0.03,  1.00,  0.07, -0.23, -0.16,
                  0.39,  0.07,  1.00, -0.13, -0.29,
                 -0.05, -0.23, -0.13,  1.00,  0.34,
                 -0.08, -0.16 ,-0.29,  0.34,  1.00), nr=5, byrow=TRUE)
rownames(sigma) &lt;-c(""Exercise"", ""Hardiness"", ""Fitness"", ""Stress"", ""Illness"")
colnames(sigma) &lt;-c(""Exercise"", ""Hardiness"", ""Fitness"", ""Stress"", ""Illness"")

#Create Covariance Matrix
sdevs &lt;-c(66.5, 3.8, 18.4, 6.7, 624.8)
covmax &lt;- cor2cov(sigma, sdevs)
as.matrix(covmax)

#Specify Model 
mymodel&lt;-'Illness ~ Exercise + Fitness
Illness ~ Hardiness + Stress
Fitness ~ Exercise + Hardiness 
Stress ~ Exercise + Hardiness + Fitness 
Exercise ~~ Exercise 
Hardiness ~~ Hardiness 
Exercise ~~ Hardiness'

#Fit the model with the covariance matrix
N = 363
fit.path &lt;-sem(mymodel,sample.cov=covmax, sample.nobs=N, fixed.x=FALSE)

#Summary of the model fit
summary(fit.path, fit.measures = TRUE)
</code></pre>

<p><strong>And the output I get is as follows:</strong> </p>

<pre><code> lavaan (0.5-12) converged normally after  93 iterations

 Number of observations                         37300

 Estimator                                         ML
 Minimum Function Test Statistic                0.000
 Degrees of freedom                                 0
 P-value (Chi-square)                           1.000

 Model test baseline model:

 Minimum Function Test Statistic            16594.387
 Degrees of freedom                                10
 P-value                                        0.000

 Full model versus baseline model:

 Comparative Fit Index (CFI)                    1.000
 Tucker-Lewis Index (TLI)                       1.000

 Loglikelihood and Information Criteria:

 Loglikelihood user model (H0)             -882379.005
 Loglikelihood unrestricted model (H1)     -882379.005

 Number of free parameters                         15
 Akaike (AIC)                              1764788.009
 Bayesian (BIC)                            1764915.910
 Sample-size adjusted Bayesian (BIC)       1764868.240

 Root Mean Square Error of Approximation:

 RMSEA                                          0.000
 90 Percent Confidence Interval          0.000  0.000
 P-value RMSEA &lt;= 0.05                          1.000

 Standardized Root Mean Square Residual:

 SRMR                                           0.000

 Parameter estimates:

 Information                                 Expected
 Standard Errors                             Standard

                Estimate  Std.err  Z-value  P(&gt;|z|)
 Regressions:
 Illness ~
 Exercise          0.318    0.048    6.640    0.000
 Fitness          -8.835    0.174  -50.737    0.000
 Hardiness       -12.146    0.793  -15.321    0.000
 Stress           27.125    0.451   60.079    0.000
 Fitness ~
 Exercise          0.109    0.001   82.602    0.000
 Hardiness         0.396    0.023   17.211    0.000
 Stress ~
 Exercise         -0.001    0.001   -2.614    0.009
 Hardiness        -0.393    0.009  -44.332    0.000
 Fitness          -0.040    0.002  -19.953    0.000

 Covariances:
 Exercise ~~
 Hardiness        -7.581    1.309   -5.791    0.000

 Variances:
 Exercise       4422.131   32.381
 Hardiness        14.440    0.106
 Illness       318744.406 2334.012
 Fitness         284.796    2.085
 Stress           41.921    0.307
</code></pre>

<p><strong>These are my questions:</strong>  </p>

<ul>
<li>Why does the chi-squared say that there are no degrees of freedom? </li>
<li>Why are the p-values exactly 1? Why is the CFI and TLI exactly 1? </li>
<li><p>Why is the RMSEA 0?</p></li>
<li><p>What would I need to do to simulate a more realistic model that doesn't appear artificially ""perfect""? </p></li>
<li>Does it have to do with the model specification? </li>
</ul>
"
"0.167836271659338","0.165097668682599"," 62852","<p>I have implemented a Gibbs Sampler for the <strong>Bayesian Elastic Net</strong> (BEN) according to this paper on <a href=""http://www.stat.ufl.edu/~casella/Papers/BL-Final.pdf"" rel=""nofollow"">Penalized Regression by Kyung et al.</a><br>
In this paper, they execute a simulation study that has been used in other papers on Penalized Regression (LASSO, Bridge, Ridge) to compare the performance of the proposed models.</p>

<p>Here are details of the simulation taken from the above mentioned paper:  </p>

<blockquote>
  <p>We simulate data from the true model
  $$
y=X\beta+\sigma\epsilon \quad\epsilon_i\,{\raise.17ex\hbox{$\scriptstyle\sim$}}\,\text{iid}\,N(0,1)
$$
  We simulate data sets with $n=20$ to fit models and $n=200$ to compare prediction errors of proposed models with eight predictors. We let $\beta=(3,1.5,0,0,2,0,0,0)$ and $\sigma=3$. The pairwise correlation between $x_i$ and $x_j$ was set to be $corr(i,j)=0.5^{|i-j|}$.<br>
  Later on they say, that for the prediction error, they calculate the average mean squared error based on 50 replications. By average they mean the median in this case.</p>
</blockquote>

<p>To simulate this data and calculate the MSE I've used following code in R: </p>

<pre><code># Number of observations
n.train &lt;- 20
n.test  &lt;- 200
# Error variance
sigma &lt;- 3
# Pairwise correlation of X
cor &lt;- 0.5
# Number of predictors
p &lt;- 8
# Create training and test data set (package QRM and mvtnorm required)
Z &lt;- equicorr(p, rho=cor)
X.train &lt;- rmvnorm(n.train,sigma=Z)
X.test  &lt;- rmvnorm(n.test,sigma=Z)
# Create error 
error.train &lt;- rnorm(n.train,mean=0,sd=1)
error.test  &lt;- rnorm(n.test,mean=0,sd=1)
# Create beta
beta.true &lt;- c(3,1.5,0,0,2,0,0,0)
# Create both responses
Y.train &lt;- X.train %*% beta.true + sigma*error.train
Y.test  &lt;- X.test %*% beta.true + sigma*error.test

# Fit the training data set with the BEN Gibbs Sampler
beta.ben &lt;- BEN(X.train,Y.train, iter=11000, burn = 1000)
# Calculate the predicted response
Y.pred   &lt;- X.test %*% beta.ben
# Calculate the mean squared error (MSE)
MSE      &lt;- sum((Y.train - Y.pred)^2)/n.train
</code></pre>

<p>My problem is that my results are not even close to comparable to the ones in the paper which makes me doubt my simulation study ""setup"".<br>
As one of the authors of the paper has uploaded the Gibbs Sampler code and I could check if I did something wrong, I know that the problem doesn't lie there.</p>

<p>So my questions are:</p>

<ol>
<li>Does anybody have experience with this kind of simulation study and can check if I did something wrong?</li>
<li>Is the MSE I calculate the same as the one used in the paper? In researching on this topic I found many different ways to calculate the MSE and it was also sometimes used but actually the mean squared prediction error was meant. For example the Wikipedia article on MSE alone lists three variations.</li>
</ol>

<p>I don't need help with coding, rather more information on how this simulation is typically excecuted so I can figure out what I'm doing wrong.</p>
"
"0.133237935355665","0.142978775184717"," 63796","<p>This is related to a <a href=""http://stats.stackexchange.com/questions/62646/pooled-time-series-regression-in-r"">question</a> I asked a couple weeks ago, but I've got a new question related to the same data. You can find the data and its accompanying explanation in the link provided.</p>

<p>I felt that a regression including year as a covariate along with year dummies would lead to a linear dependence problem, but I was told to try it anyway as </p>

<blockquote>
  <p>""the year dummies as independent variables [may] pick up year-specific
  random effects not accounted for by a time trend, e.g. for example the
  trend over all years could be down by say 2 percent per year which
  could apply to most years, but a negative macro shock in one
  particular year could make that year lie way off the regression
  line--a simple example of why the year dummies are not co-linear with
  a time trend.""</p>
</blockquote>

<p>This makes sense, I suppose, so I ran a regression that simply included year and year dummies for each year as the independent variables (including AR(1) corrections). This looked like the following:</p>

<pre><code>&gt; ## Generate YearFactor and AgeGroupFactor using factor()
&gt; 
&gt; YearFactor &lt;- factor(YearVar)
&gt; AgeGroupFactor &lt;- factor(AgeGroup)
&gt; 
&gt; ## Check to see that YearFactor and AgeGroupFactor are indeed factor variables
&gt; 
&gt; is.factor(YearFactor)
[1] TRUE
&gt; is.factor(AgeGroupFactor)
[1] TRUE
&gt;
&gt; ## Run regressions with both time trend and year dummies to determine if a linear dependence problem exists.
&gt; 
&gt; TrendDummies &lt;- gls(PPHPY ~ YearVar + YearFactor, correlation=corARMA(p=1))
Error in glsEstimate(object, control = control) : 
 computed ""gls"" fit is singular, rank 13
&gt; summary(TrendDummies)
Error in summary(TrendDummies) : object 'TrendDummies' not found
&gt;
</code></pre>

<p>I interpret the error message ""Error in glsEstimate(object, control = control) : 
     computed ""gls"" fit is singular, rank 13"" to mean that there indeed is a linear dependence problem in this case. Am I properly interpreting this? </p>

<p>Also, given the advice in quotes above, would my regression as constructed (if there were no linear dependence problems) capture the effects mentioned therein?</p>

<p>And finally, if I run the same regression as OLS with no AR(1) correlation structure, I do indeed get some results (instead of an error message). Any thoughts on that?</p>
"
"0.0726752374667264","0.0714893875923587"," 63883","<p>Is there a method to find the right distance function in non-parametric regression?
I use some time series to learn forecasting. Series are nonlinear and non-gaussian.
I can get the right dimension and delay. I can find the right bandwidth with the hdrcde library.
I have no problem with kernel functions.<br>
My problem is with distances. I use Euclidian, Cosine and Correlation weighting functions.
These are the kernel which give good results, but one time, Euclidian is good, then after adding some data, one to 5-7 generally, Euclidian give nothing, even with very little change in statistics.
So, my question is if there is a method to choose the right distance. I would like to have advices on that point and on articles that will help solve this problem. What package in R could eventually help?</p>

<p>Thank you. </p>
"
"NaN","NaN"," 64469","<p>If you would fit a small sample like with the standard framework for linear models. Would you suggest from the residuals any special correlation structure like compound symmetry or first order autoregression?</p>

<pre><code>Y&lt;-data.frame(response=c(10,19,27,28,9,13,25,29,4,10,20,18,5,6,12,17),
               treatment=factor(rep(1:4,4)),
               subject=factor(rep(1:4,each=4))
               )
fit&lt;-lm(response~-1+treatment,Y)
</code></pre>
"
"NaN","NaN"," 67257","<p>I have difficulties fitting a joint model in <code>R</code>. My data consists of two responses <code>X</code> &amp; <code>Y</code> and one predictor variable <code>Z</code>. Now I want to model both <code>X</code> and <code>Y</code> in function of <code>Z</code> (just linear regression: $E(X|Z)=Z\alpha$ and $E(Y|Z)=Z\beta$, both outcomes are normally distributed) but while doing so I also want to estimate the variance covariance matrix since it is the correlation between <code>X</code> and <code>Y</code> that I am interested in.</p>

<p>I already looked into a couple of functions (<code>lm</code>, <code>mcer</code>, <code>lme</code>) but it doesn't seem to do the trick. Is there something that I am overlooking in a certain package or a new suggestions to try?</p>
"
"NaN","NaN"," 67363","<p>I am running a Bayesian regression model by WinGUBS via R2WinBUGS package in R. Everything looks fine except for one parameter:</p>

<pre><code>nlssim$summary[37,][c(1,2,8,9)]
    mean           sd         Rhat        n.eff 
3.054326e-05 9.523965e-06 1.000000e+00 1.000000e+00 
</code></pre>

<p>The number of effective size is 1! It seems to indicate that the autocorrelation between samples are extremely large, but the trace plot looks all fine. And I then try the effectiveSize function in code package and the result is 4590.</p>

<p>I am curious how WinBUGS calculate the n.eff statistic, and if n.eff=1 is symptom of some of my mistakes?</p>

<p>I run 3 chains in parallel, and each chain has 60000 iterations. n.burnin=200, n.thin=30</p>

<p>Thank you very much in advance!</p>
"
"NaN","NaN"," 68291","<p>I know there is a series of regression diagnostics procedures (correlation, beta, residual, etc.) before, during, and after regression analysis. But, is there any common procedure to follow for cluster analysis (like, Ward)? What are the R commands? Thanks!</p>
"
"0.111890847772892","0.110065112455066"," 69524","<p>I am trying to fit a nonlinear regression model in R using <code>nls()</code>. I have a form of the equation I want to fit to:</p>

<p>$$y = (a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e)$$</p>

<p>where the coefficients to be found in regression are a,b,c,d, and e. My data is output from a simulation model where $x_{1}$, $x_{2}$, and $x_{3}$ are all integers from $0$ to $10$, with the condition that $x_{1} + x_{2} + x_{3} \le 10$. $y$ is also integer valued and ranges from $0$ to roughly $1000$. The objective is to fit these data to a rate function that will be used in a Markov Chain.</p>

<p>When I try to fit this regression model directly using <code>nls()</code>, my <code>nlsResiduals</code> plot looks like this:</p>

<p><img src=""http://i.stack.imgur.com/6scJ3.png"" alt=""nls residuals""></p>

<p>I know that autocorrelated residuals are problematic, and that non-normal residuals can also be problematic. How can I fix this problem? I was thinking of using transforms on the data like</p>

<p>$$\log(y) = \log((a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e))$$</p>

<p>or</p>

<p>$$y^{1/n} = ((a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e))^{1/n}$$  where $n &gt; 1$. I've noticed if $n$ increases, my autocorrelation graph and QQ-plot look ""better"" (i.e., more scattered and more normal, respectively). </p>

<p>Both of these seem to correct a lot (but not all) of the autocorrelated residuals, and help to make the residuals more normally distributed. Am I on the right track here, or am I committing some cardinal sin in statistics? Once I settle on a transformation, how can I tell which is best?</p>

<p>Any help, suggestions, or comments are very appreciated.</p>
"
"0.125877203744503","0.123823251511949"," 70227","<p>I want to model the infection rates in bees based on weather conditions. The weather variables are rolling means for different time periods and durations. Dependent data is infection levels gathered in March and the independent variables are the weather aggregates (e.g. from 30 day period from Jan1-Jan30, 90 day period from Dec1-Feb28), a few thousands of them and highly correlated.</p>

<p>PCA techniques did not work since the infections are not so strongly related to weather. I have also tried Bayesian Model Averaging and Boosted Regression Trees, since variables could be selected based on variable importance they calculate.</p>

<p>But since, my data is longitudinal and my apiaries have a fixed location, I think mixed-models are a good choice. Is there a way to do variable selection based on mixed-models?</p>

<p>What I have done now is to<br>
1. run <code>glmer</code> for each of the independent variables separately,<br>
2. remove those variable whose p-values for fixed-effect estimates are below 0.05 (not sure if this is a right thing - if the estimate for a variable is not significant, that variable being the only one in the model, it is right to drop that variable, is it?)<br>
3. from the variables that are left over, test for correlation between the variables<br>
4. remove the variables that are highly correlated, giving preference to the variable that has the lowest AIC.  </p>

<p>Or should I at this stage, not worry about p-values of Intercepts and only focus on AIC (or BIC)? since some of the variables have high p-values but AICs lower than than those with low p-values. </p>

<p>I have tried reading up a lot, and there is no one fool-proof solution for variable selection, but would like to know if there is anything inherently wrong with my method. As I am not a statistician, equations often look like beautiful Arabic calligraphy and there lies my dead-end.</p>
"
"0.0419590679148345","0.0412744171706498"," 70322","<p>Is it possible to illustrate partial correlation scatter plots for 2 subgroups on the same graph? </p>

<p>e.g. I want to make scatter plots of data controlled for age, differentiated by males or females.</p>

<p>I've tried doing partial regression plots generated by linear regression analysis, but I can't split it by groups. </p>

<p>Options to do it in excel or R would be fine too. Thanks!</p>
"
"0.0938233281301002","0.0922924025252575"," 70866","<p>I have a modelling dilemma. I am creating a model that attempts to predict demand (leads not sales) based upon the correlation to advertising spend. We know that without advertising spend, demand is driven by seasonality. So our models include seasonal factors like month of the year and even day of the week. 
If I were building a regular linear regression model, I would fit a linear regression model to a training dataset, to get estimates of the coefficients of the seasonal factors and advertising spend to demand. In order to get an estimate of future baseline demand, I would forecast demand using all the coefficients from the model and then I would estimate a baseline by setting adspend equal to zero. 
For ARIMA models, there are additional factors such as AR and MA terms. Would I estimate my baseline the same way by just setting the coefficient on advertising spend equal to zero?
Thanks for any thoughts.</p>
"
"0.111013258946721","0.109201843342674"," 71924","<p><em><a href=""http://stackoverflow.com/q/19186966/1414455"">Cross-posted from SO.</a></em></p>

<p>I am trying to replicate the results of <code>bgtest</code> from the <a href=""http://cran.r-project.org/web/packages/lmtest/lmtest.pdf"" rel=""nofollow""><code>lmtest</code></a> R package.</p>

<p>I am using the following dataset:</p>

<pre><code>           rs   month   r20
1    2.365042  1952m3  4.33
2    2.317500  1952m4  4.23
3    2.350833  1952m5  4.36
4    2.451833  1952m6  4.57
5    2.466167  1952m7  4.36
6    2.468417  1952m8  4.11
7    2.485583  1952m9  4.20
8    2.415125 1952m10  4.19
9    2.389875 1952m11  4.15
10   2.418167 1952m12  4.22
11   2.396042  1953m1  4.13
12   2.401042  1953m2  4.10
13   2.400833  1953m3  4.04
14   2.383500  1953m4  3.94
15   2.366708  1953m5  3.95
16   2.365625  1953m6  4.02
17   2.348583  1953m7  3.98
18   2.334375  1953m8  3.94
19   2.133542  1953m9  3.78
20   2.097375 1953m10  3.80
21   2.097708 1953m11  3.78
22   2.130583 1953m12  3.83
23   2.096000  1954m1  3.79
24   2.064042  1954m2  3.79
25   2.115083  1954m3  3.76
26   2.047333  1954m4  3.71
27   1.713875  1954m5  3.65
28   1.606167  1954m6  3.61
29   1.561667  1954m7  3.35
30   1.613292  1954m8  3.36
31   1.621083  1954m9  3.35
32   1.587667 1954m10  3.35
33   1.637792 1954m11  3.38
34   1.865917 1954m12  3.51
35   2.356417  1955m1  3.64
36   3.810000  1955m2  3.85
37   3.797000  1955m3  3.83
38   3.906000  1955m4  4.15
39   3.937000  1955m5  4.21
40   3.969000  1955m6  4.33
41   3.971000  1955m7  4.47
42   4.005000  1955m8  4.84
43   4.072000  1955m9  4.68
44   4.071000 1955m10  4.50
45   4.104000 1955m11  4.64
46   4.072000 1955m12  4.70
47   4.071000  1956m1  4.84
48   5.218000  1956m2  4.87
49   5.165000  1956m3  5.02
50   5.008000  1956m4  4.85
51   4.955000  1956m5  5.12
52   5.136000  1956m6  5.25
53   4.977000  1956m7  5.27
54   4.027000  1956m8  5.20
55   5.091000  1956m9  5.35
56   4.991000 1956m10  5.33
57   5.020000 1956m11  5.50
58   4.858000 1956m12  5.29
59   4.553000  1957m1  4.91
60   4.148000  1957m2  4.93
61   4.099000  1957m3  5.08
62   3.914000  1957m4  5.11
63   3.921000  1957m5  5.43
64   3.854000  1957m6  5.55
65   3.845000  1957m7  5.60
66   4.121000  1957m8  5.75
67   6.605000  1957m9  5.98
68   6.603000 1957m10  5.84
69   6.459000 1957m11  5.89
70   6.375000 1957m12  5.81
71   6.127000  1958m1  5.66
72   6.014000  1958m2  5.65
73   5.523000  1958m3  5.64
74   5.179000  1958m4  5.45
75   4.816000  1958m5  5.46
76   4.294000  1958m6  5.45
77   4.159000  1958m7  5.46
78   3.760000  1958m8  5.49
79   3.625000  1958m9  5.36
80   3.584000 1958m10  5.35
81   3.305000 1958m11  5.36
82   3.152000 1958m12  5.36
83   3.107000  1959m1  5.20
84   3.276000  1959m2  5.20
85   3.287000  1959m3  5.24
86   3.283000  1959m4  5.22
87   3.382000  1959m5  5.28
88   3.452000  1959m6  5.18
89   3.484000  1959m7  5.13
90   3.488000  1959m8  5.21
91   3.472000  1959m9  5.33
92   3.386000 1959m10  5.06
93   3.400000 1959m11  5.04
94   3.687000 1959m12  5.21
95   4.538000  1960m1  5.34
96   4.554000  1960m2  5.43
97   4.621000  1960m3  5.53
98   4.652000  1960m4  5.59
99   4.556000  1960m5  5.62
100  5.681000  1960m6  5.92
101  5.546000  1960m7  5.97
102  5.588000  1960m8  5.95
103  5.565000  1960m9  5.97
104  5.090000 1960m10  5.97
105  4.639000 1960m11  5.97
106  4.349000 1960m12  6.01
107  4.165000  1961m1  6.01
108  4.399000  1961m2  6.04
109  4.485000  1961m3  6.05
110  4.407000  1961m4  6.01
111  4.436000  1961m5  6.08
112  4.537000  1961m6  6.33
113  6.688000  1961m7  6.52
114  6.700000  1961m8  6.63
115  6.552000  1961m9  6.65
116  5.727000 1961m10  6.33
117  5.389000 1961m11  6.34
118  5.403000 1961m12  6.41
119  5.242000  1962m1  6.35
120  5.531000  1962m2  6.26
121  4.405000  1962m3  6.25
122  4.052000  1962m4  6.24
123  3.816000  1962m5  6.25
124  3.921000  1962m6  6.24
125  3.887000  1962m7  5.98
126  3.752000  1962m8  5.77
127  3.635000  1962m9  5.27
128  3.858000 1962m10  5.37
129  3.689000 1962m11  5.42
130  3.717000 1962m12  5.36
131  3.491000  1963m1  5.54
132  3.426000  1963m2  5.74
133  3.756000  1963m3  5.69
134  3.709000  1963m4  5.50
135  3.635000  1963m5  5.31
136  3.702000  1963m6  5.28
137  3.761000  1963m7  5.20
138  3.723000  1963m8  5.22
139  3.674000  1963m9  5.21
140  3.745000 1963m10  5.26
141  3.739000 1963m11  5.51
142  3.721000 1963m12  5.63
143  3.758000  1964m1  5.64
144  4.307000  1964m2  5.85
145  4.302000  1964m3  5.76
146  4.302000  1964m4  5.93
147  4.384000  1964m5  5.90
148  4.464000  1964m6  5.97
149  4.654000  1964m7  6.02
150  4.656000  1964m8  6.00
151  4.703000  1964m9  6.00
152  4.698000 1964m10  6.06
153  6.630000 1964m11  6.23
154  6.627000 1964m12  6.41
155  6.543000  1965m1  6.41
156  6.442000  1965m2  6.43
157  6.549000  1965m3  6.53
158  6.375000  1965m4  6.61
159  6.364000  1965m5  6.76
160  5.542000  1965m6  6.78
161  5.630000  1965m7  6.80
162  5.559000  1965m8  6.65
163  5.559000  1965m9  6.35
164  5.440000 1965m10  6.37
165  5.395000 1965m11  6.40
166  5.521000 1965m12  6.59
167  5.483000  1966m1  6.52
168  5.620000  1966m2  6.61
169  5.604000  1966m3  6.77
170  5.638000  1966m4  6.78
171  5.659000  1966m5  6.82
172  5.728000  1966m6  7.03
173  6.679000  1966m7  7.29
174  6.726000  1966m8  7.41
175  6.747000  1966m9  7.29
176  6.513000 1966m10  6.96
177  6.738000 1966m11  6.97
178  6.527000 1966m12  6.78
179  6.080000  1967m1  6.58
180  6.035000  1967m2  6.49
181  5.495000  1967m3  6.50
182  5.412000  1967m4  6.46
183  5.248000  1967m5  6.65
184  5.275000  1967m6  6.86
185  5.345000  1967m7  6.92
186  5.291000  1967m8  6.90
187  5.475000  1967m9  6.98
188  5.726000 1967m10  7.00
189  7.553000 1967m11  7.22
190  7.484000 1967m12  7.20
191  7.520000  1968m1  7.28
192  7.374000  1968m2  7.28
193  7.108000  1968m3  7.29
194  7.080000  1968m4  7.34
195  7.241000  1968m5  7.50
196  7.242000  1968m6  7.87
197  7.059000  1968m7  7.63
198  6.945000  1968m8  7.63
199  6.577000  1968m9  7.64
200  6.493000 1968m10  7.70
201  6.789000 1968m11  7.93
202  6.777000 1968m12  8.17
203  6.728000  1969m1  8.47
204  7.711000  1969m2  8.61
205  7.782000  1969m3  8.81
206  7.798000  1969m4  8.90
207  7.850000  1969m5  9.46
208  7.880000  1969m6  9.31
209  7.830000  1969m7  9.19
210  7.790000  1969m8  9.49
211  7.811000  1969m9  9.21
212  7.743000 1969m10  8.95
213  7.738000 1969m11  9.29
214  7.650000 1969m12  9.04
215  7.550000  1970m1  9.03
216  7.600000  1970m2  8.79
217  7.270000  1970m3  8.75
218  6.940000  1970m4  8.94
219  6.190000  1970m5  9.40
220  6.870000  1970m6  9.58
221  6.850000  1970m7  9.33
222  6.820000  1970m8  9.19
223  6.820000  1970m9  9.28
224  6.810000 1970m10  9.15
225  6.810000 1970m11  9.51
226  6.820000 1970m12  9.62
227  6.790000  1971m1  9.51
228  6.750000  1971m2  9.35
229  6.660000  1971m3  9.07
230  5.920000  1971m4  9.07
231  5.650000  1971m5  9.03
232  5.590000  1971m6  9.08
233  5.570000  1971m7  9.22
234  5.750000  1971m8  8.96
235  4.830000  1971m9  8.50
236  4.630000 1971m10  8.51
237  4.480000 1971m11  7.79
238  4.360000 1971m12  8.10
239  4.360000  1972m1  7.93
240  4.370000  1972m2  7.90
241  4.340000  1972m3  8.16
242  4.300000  1972m4  8.26
243  4.270000  1972m5  8.60
244  5.210000  1972m6  9.32
245  5.600000  1972m7  9.23
246  5.790000  1972m8  9.36
247  6.440000  1972m9  9.54
248  6.740000 1972m10  9.46
249  6.880000 1972m11  9.45
250  7.760000 1972m12  9.62
251  8.210000  1973m1  9.56
252  8.080000  1973m2  9.65
253  8.070000  1973m3 10.01
254  7.670000  1973m4  9.93
255  7.330000  1973m5 10.02
256  7.060000  1973m6 10.15
257  8.270000  1973m7 10.60
258 10.910000  1973m8 11.30
259 10.970000  1973m9 11.55
260 10.770000 1973m10 11.28
261 11.730000 1973m11 12.00
262 12.460000 1973m12 12.50
263 12.090000  1974m1 12.89
264 11.920000  1974m2 13.50
265 11.950000  1974m3 13.68
266 11.520000  1974m4 14.21
267 11.360000  1974m5 13.80
268 11.230000  1974m6 14.38
269 11.200000  1974m7 14.88
270 11.240000  1974m8 15.29
271 11.060000  1974m9 14.95
272 10.930000 1974m10 15.68
273 10.980000 1974m11 16.75
274 10.990000 1974m12 17.18
275 10.590000  1975m1 16.02
276  9.880000  1975m2 14.58
277  9.500000  1975m3 13.43
278  9.260000  1975m4 13.89
279  9.470000  1975m5 14.53
280  9.430000  1975m6 14.41
281  9.710000  1975m7 13.93
282 10.430000  1975m8 13.87
283 10.360000  1975m9 13.79
284 11.420000 1975m10 14.66
285 11.100000 1975m11 14.81
286 10.820000 1975m12 14.79
287  9.990000  1976m1 13.79
288  8.760000  1976m2 13.46
289  8.460000  1976m3 13.88
290  9.060000  1976m4 13.77
291 10.440000  1976m5 13.59
292 10.960000  1976m6 14.09
293 10.870000  1976m7 14.16
294 10.880000  1976m8 14.33
295 12.050000  1976m9 14.79
296 14.000000 1976m10 16.03
297 14.140000 1976m11 15.79
298 13.780000 1976m12 15.48
299 12.730000  1977m1 14.48
300 11.020000  1977m2 13.93
301  9.920000  1977m3 13.25
302  8.240000  1977m4 13.05
303  7.400000  1977m5 12.69
304  7.450000  1977m6 13.26
305  7.430000  1977m7 13.62
306  6.540000  1977m8 13.12
307  5.680000  1977m9 11.88
308  4.530000 1977m10 10.98
309  4.960000 1977m11 11.28
310  6.370000 1977m12 11.16
311  5.810000  1978m1 11.06
312  5.960000  1978m2 11.75
313  5.930000  1978m3 11.72
314  6.730000  1978m4 12.39
315  8.400000  1978m5 12.72
316  9.170000  1978m6 12.79
317  9.220000  1978m7 12.72
318  8.900000  1978m8 12.55
319  8.980000  1978m9 12.64
320  9.860000 1978m10 12.91
321 11.510000 1978m11 13.16
322 11.570000 1978m12 13.22
323 11.860000  1979m1 13.68
324 12.630000  1979m2 13.94
325 11.350000  1979m3 12.35
326 11.320000  1979m4 11.68
327 11.350000  1979m5 11.94
328 12.570000  1979m6 12.69
329 13.320000  1979m7 12.25
330 13.320000  1979m8 12.30
331 13.380000  1979m9 12.60
332 13.380000 1979m10 13.16
333 15.330000 1979m11 14.54
334 15.900000 1979m12 14.72
335 15.790000  1980m1 14.17
336 16.140000  1980m2 14.45
337 16.180000  1980m3 14.70
338 16.170000  1980m4 14.27
339 16.090000  1980m5 14.01
340 15.800000  1980m6 13.78
341 14.550000  1980m7 13.07
342 14.860000  1980m8 13.58
343 14.400000  1980m9 13.38
344 14.290000 1980m10 13.12
345 13.950000 1980m11 13.22
346 13.070000 1980m12 13.67
347 12.820000  1981m1 13.96
348 12.090000  1981m2 13.89
349 11.530000  1981m3 13.68
350 11.330000  1981m4 13.64
351 11.350000  1981m5 14.31
352 12.090000  1981m6 14.57
353 13.150000  1981m7 15.14
354 13.420000  1981m8 15.09
355 13.960000  1981m9 15.59
356 15.550000 1981m10 15.95
357 14.080000 1981m11 15.44
358 14.510000 1981m12 15.65
359 14.160000  1982m1 15.58
360 13.300000  1982m2 14.74
361 12.480000  1982m3 13.72
362 12.890000  1982m4 13.96
363 12.530000  1982m5 13.69
364 12.230000  1982m6 13.56
365 11.280000  1982m7 13.20
366 10.080000  1982m8 12.23
367  9.910000  1982m9 11.40
368  8.910000 1982m10 10.50
369  9.220000 1982m11 10.64
370  9.960000 1982m12 11.34
371 10.590000  1983m1 11.60
372 10.740000  1983m2 11.50
373 10.470000  1983m3 10.97
374  9.840000  1983m4 10.56
375  9.700000  1983m5 10.65
376  9.470000  1983m6 10.39
377  9.370000  1983m7 10.95
378  9.340000  1983m8 11.07
379  9.160000  1983m9 10.67
380  8.840000 1983m10 10.61
381  8.840000 1983m11 10.29
382  8.870000 1983m12 10.35
383  8.870000  1984m1 10.28
384  8.850000  1984m2 10.42
385  8.430000  1984m3 10.23
386  8.380000  1984m4 10.40
387  8.820000  1984m5 10.93
388  8.860000  1984m6 11.15
389 10.970000  1984m7 11.67
390 10.210000  1984m8 10.98
391 10.020000  1984m9 10.78
392  9.850000 1984m10 10.69
393  9.230000 1984m11 10.32
394  9.100000 1984m12 10.46
395 10.550000  1985m1 10.96
396 12.690000  1985m2 11.06
397 12.930000  1985m3 10.90
398 11.930000  1985m4 10.68
399 11.940000  1985m5 10.88
400 11.890000  1985m6 10.70
401 11.390000  1985m7 10.44
402 10.960000  1985m8 10.37
403 11.060000  1985m9 10.39
404 11.050000 1985m10 10.22
405 11.110000 1985m11 10.37
406 11.150000 1985m12 10.45
407 11.980000  1986m1 10.80
408 12.020000  1986m2 10.40
409 11.060000  1986m3  9.39
410  9.990000  1986m4  8.76
411  9.700000  1986m5  9.00
412  9.320000  1986m6  9.23
413  9.450000  1986m7  9.37
414  9.390000  1986m8  9.41
415  9.610000  1986m9  9.97
416 10.250000 1986m10 10.62
417 10.630000 1986m11 10.80
418 10.660000 1986m12 10.69
419 10.520000  1987m1 10.09
420 10.290000  1987m2  9.83
421  9.350000  1987m3  9.16
422  9.430000  1987m4  9.12
423  8.460000  1987m5  8.82
424  8.540000  1987m6  8.90
425  8.840000  1987m7  9.23
426  9.790000  1987m8  9.20
427  9.690000  1987m9  9.98
428  9.450000 1987m10  9.88
429  8.430000 1987m11  9.20
430  8.190000 1987m12  9.57
431  8.370000  1988m1  9.57
432  8.790000  1988m2  9.38
433  8.270000  1988m3  9.12
434  7.740000  1988m4  9.12
435  7.540000  1988m5  9.27
436  8.880000  1988m6  9.32
437 10.050000  1988m7  9.51
438 11.130000  1988m8  9.47
439 11.530000  1988m9  9.60
440 11.540000 1988m10  9.23
441 12.070000 1988m11  9.30
442 12.540000 1988m12  9.46
443 12.450000  1989m1  9.35
444 12.390000  1989m2  9.15
445 12.410000  1989m3  9.26
446 12.470000  1989m4  9.52
447 12.540000  1989m5  9.52
448 13.590000  1989m6  9.88
449 13.290000  1989m7  9.53
450 13.320000  1989m8  9.37
451 13.440000  1989m9  9.62
452 14.460000 1989m10  9.81
453 14.450000 1989m11  9.99
454 14.500000 1989m12  9.96
455 14.500000  1990m1 10.28
456 14.450000  1990m2 10.72
457 14.570000  1990m3 11.46
458 14.590000  1990m4 11.77
459 14.500000  1990m5 11.49
460 14.380000  1990m6 11.01
461 14.320000  1990m7 11.03
462 14.310000  1990m8 11.41
463 14.260000  1990m9 11.32
464 13.370000 1990m10 11.12
465 12.920000 1990m11 10.94
466 12.960000 1990m12 10.40
467 13.000000  1991m1 10.22
468 12.390000  1991m2  9.89
469 11.640000  1991m3 10.06
470 11.250000  1991m4  9.99
471 10.840000  1991m5 10.15
472 10.720000  1991m6 10.34
473 10.520000  1991m7 10.10
474 10.200000  1991m8  9.89
475  9.660000  1991m9  9.54
476  9.860000 1991m10  9.62
477  9.980000 1991m11  9.68
478 10.100000 1991m12  9.56
479  9.970000  1992m1  9.34
480  9.800000  1992m2  9.21
481 10.100000  1992m3  9.54
482  9.970000  1992m4  9.33
483  9.430000  1992m5  8.99
484  9.420000  1992m6  9.02
485  9.430000  1992m7  8.90
486  9.650000  1992m8  9.13
487  9.160000  1992m9  9.12
488  7.470000 1992m10  9.24
489  6.490000 1992m11  8.84
490  6.390000 1992m12  8.84
491  6.050000  1993m1  8.92
492  5.370000  1993m2  8.63
493  5.380000  1993m3  8.33
494  5.330000  1993m4  8.39
495  5.300000  1993m5  8.60
496  5.190000  1993m6  8.39
497  5.130000  1993m7  7.96
498  5.060000  1993m8  7.39
499  5.170000  1993m9  7.18
500  5.150000 1993m10  7.09
501  4.950000 1993m11  7.06
502  4.870000 1993m12  6.46
503  4.890000  1994m1  6.41
504  4.760000  1994m2  6.83
505  4.830000  1994m3  7.47
506  4.880000  1994m4  7.83
507  4.810000  1994m5  8.24
508  4.880000  1994m6  8.55
509  5.090000  1994m7  8.41
510  5.340000  1994m8  8.52
511  5.390000  1994m9  8.72
512  5.440000 1994m10  8.63
513  5.630000 1994m11  8.53
514  5.870000 1994m12  8.44
515  5.930000  1995m1  8.61
516  6.160000  1995m2  8.52
517  6.090000  1995m3  8.50
518  6.300000  1995m4  8.39
519  6.200000  1995m5  8.18
520  6.370000  1995m6  8.16
521  6.620000  1995m7  8.36
522  6.590000  1995m8  8.24
523  6.520000  1995m9  8.09
524  6.530000 1995m10  8.34
525  6.380000 1995m11  8.01
526  6.220000 1995m12  7.94
</code></pre>

<p>which is saved as <code>ukrates.csv</code>.</p>

<p>Here is the code to attempt to reproduce the <code>bgtest</code> module.</p>

<pre><code>rm(list = ls())

library(zoo)
library(lmtest)
library(dynlm)

# read in the data
dfUK = read.csv('./data/ukrates.csv', header = TRUE)
summary(dfUK)

# run the time series regression
zooUK = zoo(dfUK[, c('rs', 'r20')], order.by = as.yearmon(dfUK$month, 
                                                              '%Ym%m'))
    zooUKAug = merge(zooUK, 
                     'drs' = diff(zooUK$rs, 1), 
                 'ldr20' = lag(diff(zooUK$r20, 1), -1))
lmUK2 = dynlm(drs ~ ldr20, data = zooUKAug)

# Breusch-Godfrey regression
zooUKBG = merge(zooUKAug, 'resid' = resid(lmUK2))
lmBG = dynlm(as.formula(paste('resid',  
                              '~', 
                              attr(lmUK2$terms, 'term.labels'),
                              ' + L(resid, 1)')),
             data = zooUKBG) 

# BG test using lmtest package
bgtest(lmUK2, order = 1, type = 'Chisq') # 14.5614

# attempt to recreate BG-test 
length(lmBG$residuals)*
      sum(lmBG$fitted^2)/sum(lmBG$residuals^2)
</code></pre>

<p>This is based on the following code for computing the chi-squared statistic directly from the <code>bgtest</code> function code:</p>

<pre><code>&gt; bgtest
function (formula, order = 1, order.by = NULL, type = c(""Chisq"", 
    ""F""), data = list(), fill = 0) 
{
    dname &lt;- paste(deparse(substitute(formula)))
    if (!inherits(formula, ""formula"")) {
        X &lt;- if (is.matrix(formula$x)) 
                formula$x
        else model.matrix(terms(formula), model.frame(formula))
        y &lt;- if (is.vector(formula$y)) 
                formula$y
        else model.response(model.frame(formula))
    }
    else {
        mf &lt;- model.frame(formula, data = data)
        y &lt;- model.response(mf)
        X &lt;- model.matrix(formula, data = data)
    }
    if (!is.null(order.by)) {
        if (inherits(order.by, ""formula"")) {
            z &lt;- model.matrix(order.by, data = data)
            z &lt;- as.vector(z[, ncol(z)])
        }
        else {
            z &lt;- order.by
        }
        X &lt;- as.matrix(X[order(z), ])
        y &lt;- y[order(z)]
    }
    n &lt;- nrow(X)
    k &lt;- ncol(X)
    order &lt;- 1:order
    m &lt;- length(order)
    resi &lt;- lm.fit(X, y)$residuals
        Z &lt;- sapply(order, function(x) c(rep(fill, length.out = x), 
            resi[1:(n - x)]))
        if (any(na &lt;- !complete.cases(Z))) {
            X &lt;- X[!na, , drop = FALSE]
            Z &lt;- Z[!na, , drop = FALSE]
            y &lt;- y[!na]
            resi &lt;- resi[!na]
            n &lt;- nrow(X)
        }
        auxfit &lt;- lm.fit(cbind(X, Z), resi)
        cf &lt;- auxfit$coefficients
    vc &lt;- chol2inv(auxfit$qr$qr) * sum(auxfit$residuals^2)/auxfit$df.residual
    names(cf) &lt;- colnames(vc) &lt;- rownames(vc) &lt;- c(colnames(X), 
        paste(""lag(resid)"", order, sep = ""_""))
    switch(match.arg(type), Chisq = {
        bg &lt;- n * sum(auxfit$fitted^2)/sum(resi^2)
            p.val &lt;- pchisq(bg, m, lower.tail = FALSE)
            df &lt;- m
            names(df) &lt;- ""df""
        }, F = {
            uresi &lt;- auxfit$residuals
        bg &lt;- ((sum(resi^2) - sum(uresi^2))/m)/(sum(uresi^2)/(n - 
            k - m))
        df &lt;- c(m, n - k - m)
        names(df) &lt;- c(""df1"", ""df2"")
        p.val &lt;- pf(bg, df1 = df[1], df2 = df[2], lower.tail = FALSE)
    })
    names(bg) &lt;- ""LM test""
    RVAL &lt;- list(statistic = bg, parameter = df, method = paste(""Breusch-Godfrey test for serial correlation of order up to"", 
        max(order)), p.value = p.val, data.name = dname, coefficients = cf, 
        vcov = vc)
    class(RVAL) &lt;- c(""bgtest"", ""htest"")
    return(RVAL)
}
&lt;environment: namespace:lmtest&gt;
</code></pre>

<p>I am wondering why I am getting the different results.</p>
"
"0.151285570837612","0.148817027473669"," 72421","<p>I have data for a network of weather stations across the United States. This gives me a data frame that contains date, latitude, longitude, and some measured value. Assume that data are collected once per day and driven by regional-scale weather (no, we are not going to get into that discussion). </p>

<p>I'd like to show graphically how simultaneously-measured values are correlated across time and space. My goal is to show the regional homogeneity (or lack thereof) of the value that is being investigated. </p>

<h2>Data set</h2>

<p>To start with, I took a group of stations in the region of Massachusetts and Maine. I selected sites by latitude and longitude from an index file that is available on NOAA's FTP site.</p>

<p><img src=""http://i.stack.imgur.com/aZm4N.jpg"" alt=""enter image description here""></p>

<p>Straight away you see one problem: there are lots of sites that have similar identifiers or are very close. FWIW, I identify them using both the USAF and WBAN codes. Looking deeper in to the metadata I saw that they have different coordinates and elevations, and data stop at one site then start at another. So, because I don't know any better, I have to treat them as separate stations. This means the data contains pairs of stations that are very close to each other.</p>

<h2>Preliminary Analysis</h2>

<p>I tried grouping the data by calendar month and then calculating the ordinary least squares regression between different pairs of data. I then plot the correlation between all pairs as a line connecting the stations (below). The line color shows the value of R2 from the OLS fit. The figure then shows how the 30+ data points from January, February, etc. are correlated between different stations in the area of interest. </p>

<p><img src=""http://i.stack.imgur.com/X4YZI.jpg"" alt=""correlation between daily data during each calendar month""></p>

<p>I've written the underlying codes so that the daily mean is only calculated if there are data points every 6-hour period, so data should be comparable across sites.</p>

<h3>Problems</h3>

<p>Unfortunately, there is simply too much data to make sense of on one plot. That can't be fixed by reducing the size of the lines. </p>

<p>I've tried plotting the correlations between the nearest neighbors in the region, but that turns into a mess very quickly. The facets below show the network without correlation values, using $k$ nearest neighbors from a subset of the stations. This figure was just to test the concept.
<img src=""http://i.stack.imgur.com/NWzm2.jpg"" alt=""enter image description here""></p>

<p>The network appears to be too complex, so I think I need to figure out a way to reduce the complexity, or apply some kind of spatial kernel.</p>

<p>I am also not sure what is the most appropriate metric to show correlation, but for the intended (non-technical) audience, the correlation coefficient from OLS might just be the simplest to explain. I may need to present some other information like the gradient or standard error as well.</p>

<h3>Questions</h3>

<p>I'm learning my way into this field and R at the same time, and would appreciate suggestions on:</p>

<ol>
<li>What's the more formal name for what I'm trying to do? Are there some helpful terms that would let me find more literature? My searches are drawing blanks for what must be a common application.</li>
<li>Are there more appropriate methods to show the correlation between multiple data sets separated in space?</li>
<li>... in particular, methods that are easy to show results from visually?</li>
<li>Are any of these implemented in R?</li>
<li>Do any of these approaches lend themselves to automation?</li>
</ol>
"
"NaN","NaN"," 72458","<p>I am an expert GIS user moving towards R more and more.  I have been using R for some basic regressions and such, but I would like to begin to use and manipulate GIS data in R.</p>

<p>How can I create a basemap graphic similar to the one in this post:
<a href=""http://stats.stackexchange.com/questions/72421/showing-spatial-and-temporal-correlation-on-maps"">Showing spatial and temporal correlation on maps</a></p>

<p>Again, I am a beginner in R and haven't found any other related thread here.</p>
"
"0.118678165819385","0.116741681083556"," 72569","<p>What does it mean when two random effects are highly or perfectly correlated?<br>
That is, in R when you call summary on a mixed model object, under ""Random effects"" ""corr"" is 1 or -1.</p>

<pre><code>summary(model.lmer) 
Random effects:
Groups   Name                    Variance   Std.Dev.  Corr                 
popu     (Intercept)             2.5714e-01 0.5070912                      
          amdclipped              4.2505e-04 0.0206167  1.000               
          nutrientHigh            7.5078e-02 0.2740042  1.000  1.000        
          amdclipped:nutrientHigh 6.5322e-06 0.0025558 -1.000 -1.000 -1.000
</code></pre>

<p>I know this is bad and indicates that the random effects part of the model is too complex, but I'm trying to understand</p>

<ul>
<li>1)what is doing on statistically  </li>
<li>2)what is going on practically with
the structure of the response variables.</li>
</ul>

<p><strong>Example</strong></p>

<p>Here is an example based on ""<a href=""http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CDYQFjAC&amp;url=http://glmm.wdfiles.com/local--files/examples/Banta_ex.pdf&amp;ei=hTNYUpuzBu7J4APN5YHYBg&amp;usg=AFQjCNG65VjvqOLeYLFxJZnzmlMevgEbuA&amp;bvm=bv.53899372,d.dmg"">GLMMs in action: gene-by-environment interaction in total fruit production of wild populations of Arabidopsis thaliana</a>""
by Bolker et al</p>

<p>Download data</p>

<pre><code>download.file(url = ""http://glmm.wdfiles.com/local--files/trondheim/Banta_TotalFruits.csv"", destfile = ""Banta_TotalFruits.csv"")
dat.tf &lt;- read.csv(""Banta_TotalFruits.csv"", header = TRUE)
</code></pre>

<p>Set up factors</p>

<pre><code>dat.tf &lt;- transform(dat.tf,X=factor(X),gen=factor(gen),rack=factor(rack),amd=factor(amd,levels=c(""unclipped"",""clipped"")),nutrient=factor(nutrient,label=c(""Low"",""High"")))
</code></pre>

<p>Modeling log(total.fruits+1) with ""population"" (popu) as random effect</p>

<pre><code>model.lmer &lt;- lmer(log(total.fruits+1) ~ nutrient*amd + (amd*nutrient|popu), data= dat.tf)
</code></pre>

<p>Accessing the Correlation matrix of the random effects show that everything is perfectly correlated</p>

<pre><code>attr(VarCorr(model.lmer)$popu,""correlation"")

                         (Intercept) amdclipped nutrientHigh amdclipped:nutrientHigh
(Intercept)                       1          1            1                      -1
amdclipped                        1          1            1                      -1
nutrientHigh                      1          1            1                      -1
amdclipped:nutrientHigh          -1         -1           -1                       1
</code></pre>

<p>I understand that these are the correlation coefficients of two vectors of random effects coefficients, such as</p>

<pre><code>cor(ranef(model.lmer)$popu$amdclipped, ranef(model.lmer)$popu$nutrientHigh)
</code></pre>

<p>Does a high correlation mean that the two random effects contain redundant information?  Is this analogous to multicollinearity in multiple regression when a model with highly correlated predictors should be simplified?</p>
"
"0.132686223108569","0.130521167355216"," 76490","<h2>Background</h2>

<p>A laboratory wants to evaluate whether a certain form of <a href=""http://en.wikipedia.org/wiki/Polyacrylamide_gel_electrophoresis"" rel=""nofollow"">gel electrophoresis</a> is suited as a classification method for the quality of a certain substance. Several gels were loaded, each with a clean sample of the substance and with a sample that contains impurities. In addition, a molecular marker was also loaded which serves as a reference. The following picture illustrates the setup (the picture doesn't show the actual experiment, I have taken it from Wikipedia for illustration):</p>

<p><img src=""http://i.stack.imgur.com/vC53q.png"" alt=""Example of a gel electrophoresis""></p>

<p>Two parameters were measured for each gel and each lane:</p>

<ol>
<li>The <strong>molecular weight</strong> (that is how ""high up"" a compound wandered during the electrophoresis)</li>
<li>The <strong>relative quantity.</strong> The total quantity of each lane is normalized to 1 and the density of each band is measured which results in the relative quantity of each band.</li>
</ol>

<p>A scatterplot of the relative quantity vs. molecular weight is then produced which could look something like this (it's artificial data):</p>

<p><img src=""http://i.stack.imgur.com/ndzh9.png"" alt=""Example scatterplot""></p>

<p>This graphic can be read as follows: Both the ""good"" (blue points) and ""impure"" (red points) substance exhibit two bands, one at around a molecular weight of 120 and one at around 165. The bands of the ""impure"" substance at a molecular weight around 120 are considerably less dense than the ""good"" substance and can be well distinguished.</p>

<hr>

<h2>Goal</h2>

<p>The goal is to determine two boxed (see graphic below) which determine a ""good"" substance. These boxes will then be used for classification of the substance in the future into ""good"" and ""impure"". If a substance exhibits lanes that fall within the boxes it is classified as ""good"" and else as ""impure"".</p>

<p>These decision-rules should be <em>simple</em> to apply for someone in the laboratory. That's why it should be boxes instead of curved decision boundaries.</p>

<p>False-negatives (i.e. classify a sample as ""impure"" when it's really ""good"") are considered worse than false-positives. That is, an emphasis should be placed on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity"" rel=""nofollow"">sensitivity</a>, rather than on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity"" rel=""nofollow"">specificity</a>.</p>

<p><img src=""http://i.stack.imgur.com/vhzEW.png"" alt=""Example decision boxed""></p>

<hr>

<h2>Question</h2>

<p>I'm am no expert in machine learning. I know, however, that there are quite a few machine learning algorithms/techniques that could be helpful: $k$-nearest neighbors (e.g. <code>knn</code> in <code>R</code>), classification trees (e.g. <code>rpart</code> or <code>ctree</code>), support vector machines (<code>ksvm</code>), logistic regression, boosting and bagging methods and many more.</p>

<p>One problem of many of those algorithms is that they don't provide a simple ruleset or linear boundaries. In addition, the <strong>sample size</strong> is around <strong>70.</strong></p>

<p>My questions are:</p>

<ul>
<li>Has anyone an idea of how to proceed here?</li>
<li>Does it make sense to split the dataset into training- and test-set?</li>
<li>What proportion of the data should the training set be (I thought around a 60/40-split).</li>
<li>What, in general, is the workflow for such an analysis? Something like: Splitting dataset -> fit algorithm on the training set -> predict outcome for the test set?</li>
<li>How to avoid overfitting (i.e. boxes that are too small)?</li>
<li>What is a good statistic to assess the predictive performance in this case? AUC? Accurary? Positive predictive value? <a href=""http://en.wikipedia.org/wiki/Matthews_correlation_coefficient"" rel=""nofollow"">Matthews correlation coefficient</a>?</li>
</ul>

<p>Assume that I'm familiar with <code>R</code> and the <code>caret</code> package. Thank you very much for you time and help.</p>

<hr>

<h2>Example data</h2>

<p>Here is an example dataset.</p>

<pre><code>structure(list(mol.wt = c(125.145401455869, 118.210252208676, 
165.048583787746, 126.003687476776, 170.149347112565, 127.761533014759, 
155.523172614798, 120.094514977175, 161.234986765321, 168.471542655269, 
156.522990530521, 154.377948321209, 165.365756398877, 167.965538771316, 
116.132241687833, 115.143539160903, 156.696830822196, 162.578494491556, 
136.830624758899, 123.886594633942, 124.247484227948, 126.257226352824, 
160.684010454816, 166.618872115047, 126.599387146887, 165.690375912529, 
159.786861142652, 114.520735974329, 125.753594471656, 157.551537154148, 
157.320636890647, 171.5759136115, 158.580005438661, 125.647463565197, 
130.404710783509, 127.128218318572, 162.144126888907, 161.804616951055, 
167.917268243627, 168.582197247178), rel.qtd = c(57.68339235957, 
54.0514508510085, 25.0703901938793, 37.6933881305906, 36.6853653723001, 
53.6650555524679, 52.268438087776, 52.8621831466857, 43.1242291166037, 
46.6771236380788, 38.0328239221277, 40.0454611708371, 44.6406366176158, 
40.8238699987682, 51.9464749018547, 54.0302533272953, 37.9792331383524, 
48.3853988095525, 38.2093977349102, 42.2636098418388, 42.9876895407144, 
40.8018728193786, 40.1097096927465, 38.7432550253867, 39.2633283608111, 
43.4673723102812, 53.3740718733815, 49.1067921475768, 52.3002598744634, 
44.9847844953241, 44.3014423068017, 44.0191971364465, 47.0805245356855, 
55.0124134796556, 57.9938440244052, 62.8314454977068, 45.8093815891894, 
43.2300677500964, 39.4801550161538, 51.6253515591173), quality = structure(c(2L, 
2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 1L), .Label = c(""bad"", ""good""), class = ""factor"")), .Names = c(""mol.wt"", 
""rel.qtd"", ""quality""), row.names = c(10L, 14L, 47L, 16L, 57L, 
54L, 45L, 12L, 43L, 67L, 25L, 21L, 1L, 55L, 20L, 22L, 37L, 15L, 
8L, 38L, 46L, 64L, 51L, 65L, 52L, 61L, 63L, 32L, 50L, 27L, 19L, 
69L, 23L, 42L, 6L, 48L, 11L, 13L, 5L, 71L), class = ""data.frame"")
</code></pre>
"
"0.167836271659338","0.165097668682599"," 76918","<p>I have a question about how to do analysis of an experiment that has already been done, I hope you can help me with some advice!</p>

<p>I will try to keep it as simple as possible, but will give some detail so you know what I'm talking about!</p>

<p>What has been done is a ""screening trial"" to look at the activity of about 50 subjects (fungi) as antagonists (against a pest), the 50 individuals are members of groups (species), but some groups have many more members than others</p>

<p>I have results of several types of screening tests for each of the 50 individuals, with reps of each.  The screening tests look at different aspects, like growth rate, direct effects, and indirect effects.  </p>

<p>I can rank the isolates by their results in each screening test, and there looks like a lot of variability.</p>

<p>I want to be able to report the findings of screens, for each screening test and also to see if some individuals are in top ranks in different screening tests (and also the opposite, if some are great at some tests but not at others). I think what I want is to know if the results of the tests correlate for each individual....? </p>

<p>I am not sure how to say - this individual is the best - how can I tell if it is different than the next in the rank?
If I list the top ten from each screening test, I would like to know that they are statistically different from those I excluded from the list.  I would also like to compare them as groups, to be able to say, this species was the best, but with different numbers of representatives within the species, I dont think I could do this (please advise)</p>

<p>This seems like it would be a common research experiment, for example, for testing drugs in medical experiments, so I am looking for examples of what others have done to present this type of result.</p>

<p>I have seen a similar experiment to what I have to analyse but that had been done on a small scale, and the researchers used ANOVA to test differences among individuals and among groups, and some posthoc test to give each group little letters designating their means different than other groups.  </p>

<p>This seems to be unwieldy for 50 subjects, and I'm not sure about this.... I think I need some kind of mixed model regression to put all the test results in a model to test for correlation/covariance, but my understanding is weak!</p>

<p>I have been learning R and would like to do this analysis using R.</p>

<p>Can you give me advice/suggestions?  I would appreciate any help in understanding and clarifying this problem and solutions!  </p>
"
"0.0419590679148345","0.0412744171706498"," 78445","<p>I have an unbalanced panel on daily data for 8 months for 8 years on a pollutant. Thus, my T is very large and my N(number of cross-section) is 112. My model contains a lagged dependent variable as a regressor or what economists refer to as dynamic panel data model. In addition, I have year and time fixed effects and some other variables. </p>

<p>I wish to test whether my residuals from an OLS regression are spatially correlated where the spatial correlation dies out as the distance between the cross-section units decreases. </p>

<p>I don't know how can I do so in the dynamic panel framework with missing observations and a very large T.So please suggest something. Many thanks in advance. </p>
"
"0.0839181358296689","0.0825488343412996"," 79107","<p>I'm trying my hand at resampling techniques with a dataset I have, and I think either I'm missing a conceptual point with bootstrapping, or I'm doing something incorrectly in <code>R</code>. Basically, I'm trying to use it in a correlation/regression framework, and I'm able to get the original coefficients, the bootstrap bias, and the bootstrap coefficients but I can't find a way to have <code>R</code> easily display the bootstrap model $R^2$ (when I'm working with several predictors), the Pearson $r$, or the $p$-values for individual regression coefficients. (I'm using the <code>Boot</code> function in the <code>car</code> package).</p>

<p>A secondary question...the more general function <code>boot</code> in the <code>boot</code> package requires defining a function to use as an argument. The function must include an argument for the original data set, and a second argument which is a set of indices, frequencies, or weights for the bootstrap sample. I'm a little confused by this. What conceptually are these indices I am specifying, and how do I specify them syntactically within my function?</p>
"
"0.183124385532878","0.180136324713988"," 79216","<p><strong>Problem</strong>: When trying to calculate the variance of timeseries sums I get a negative variance, mostly due to autocovariances at large lag steps. Does not seem realistic.</p>

<p>I have a timeseries which is calculated from another timeseries using a regression equation.
I would like to propagate the uncertainty in the regression to the final timeseries. Then I want to sum (or take mean values) different segments of the timeseries over different timeperiods, and get the uncertainty of the sums. The timeseries is originally in 1 hour frequency and I want to sum over periods of 1 day (resampling to daily frequency) up to several years. The timeseries is strongly autocorrelated at short lag times.</p>

<p>For getting the variance of the sum (in the case of 3 elements being summed):
$$Var(a+b+c)= \\ Var(a)+Var(b)+Var(c) + 2 \times (Cov(a,b) + Cov(a,c)+Cov(b,c))$$</p>

<p>I use <code>r</code> for the calculations. I get the variances for each timeseries element as $SE^2$, where $SE$ is the standard error (<code>se.fit</code>) returned from r's <code>predict()</code> function using the regression model. The covariances I get from the autocovariance function <code>acf()</code>.</p>

<p>Here is some code and a selection of the data (excuse clumsy R code, I'm very new to R):</p>

<pre><code>#tsY is the predicted timeseries from the regression
tsY=c(81.4,  79.0,  83.4,   81.7,   75.7,   68.3,   62.3,   57.2,   52.6,   48.8,   45.4,   42.6,   39.9,   37.6,   35.6,   33.8,   32.2,   30.8,   29.6,   28.4,   27.3,   26.2,   25.0,   23.9)
#tsSE is the standard error from the prediction (se.fit)
tsSE=c(1.55,  1.49, 1.60,   1.56,   1.41,   1.23,   1.09,   0.97,   0.87,   0.78,   0.71,   0.65,   0.60,   0.55,   0.51,   0.48,   0.45,   0.42,   0.40,   0.38,   0.36,   0.34,   0.32,   0.30)

tsVar=tsSE^2

#create a matrix of the autocovariances at different lag times, diagonal is lag=0
#rows and columns are indicies in timeseries
covmat&lt;-matrix(numeric(0), length(tsY),length(tsY)) 
for ( i in (1:(length(tsY)) ) ) {
  if (i == 1) {
    autocov&lt;-acf(tsY, type='covariance', lag.max= length(tsY))
    autocovvec&lt;-autocov$acf[1:nrow(autocov$acf)]
    covmat[i:length(tsY),i]=autocovvec
  }  else {
    autocov&lt;-acf(tsY[-(1:i-1)], type='covariance', lag.max= length(tsY))
    autocovvec&lt;-autocov$acf[1:nrow(autocov$acf)]
    covmat[i:length(tsY),i]=autocovvec
  }

}

# sum the matrix columns, but not the diagonal
sumofColumns &lt;- rep(NA, ncol(covmat))
for (i in (1:ncol(covmat))) {
  if (i == 1) {
    sumofColumns[i]=sum(covmat[-(1),i])  
  } else{ 
    sumofColumns[i]=sum(covmat[-(1:i),i])  
  }
}

sumofCov=sum(sumofColumns) # sum of the covariance (Cov(a,b) + Cov(a,c)+...)
sumofVar=sum(tsVar) # sum of the variances of each timeseries element
varofSum=sumofVar+2*sumofCov # variance of the sum of the timeseries

# from the covmat the negative variance occurs at larger lag times.
acf(tsY, type='covariance', lag.max= length(tsY))

&gt; sumofCov
[1] -1151.529
&gt; varofSum
[1] -2283.246
</code></pre>

<p><strong>So I have the following questions:</strong></p>

<blockquote>
  <ol>
  <li><p>Did I completely misunderstand how to calculate variance of sums?</p></li>
  <li><p>Is it better to use a cutoff from the max lags to be considered in the autocovariance? If so how would one determine this? This would especially be important with the complete data where the length is several thousand. </p></li>
  </ol>
  
  <p><strike>3. Why is the covariance negative in this sample data at large? When plotting tsY  <code>plot(tsY)</code> it looks like the covariance/correlation should remain positive.</strike> Because it is the variation in direction from their means.</p>
</blockquote>

<p><strong>EDIT:</strong></p>

<blockquote>
  <p>Comment on <strong>question 2</strong> above:
  I have realized that using n-1 lags, as above in the code, does not make a lot of sense. There appear to be few different ways to determine the maximum lags to consider.  Box &amp; Jenkins (1970) suggest n/4 and R by default 10*log10(n). This does not answer the question however, of how to determine an appropriate cutoff for summing the covariances.</p>
  
  <p>Does it make sense to look at the partial autocorrelation (function pacf()), in order not to overestimate the effect of the auto covariance in the summation term? The partial autocorrelation for my data is significantly different from zero only at 1 or 2 lags. Similarly, fitting an AR model using ar() function, I also get an order of 1 or 2.</p>
</blockquote>

<p>Cheers</p>

<p>Related post <a href=""http://stats.stackexchange.com/questions/10943/variance-on-the-sum-of-predicted-values-from-a-mixed-effect-model-on-a-timeserie"">Variance on the sum of predicted values from a mixed effect model on a timeseries</a></p>
"
"0.0750586625040802","0.0922924025252575"," 79830","<p>I'm using R to develop regression models, and I need to compare two different models' performance. The question that arises is, ""Is Model 1 statistically better than model 2?"" and I don't seem to have a way to answer that question.</p>

<p>Background: Model 1 consists of Variable A regressed on the endpoint. Model 2 consists of Variables B, C, and D regressed on the endpoint. Both models are developed using lm - ordinary least squares, nothing too fancy here.</p>

<p>Given that these are not nested models, I cannot compare them using ANOVA.</p>

<p>I can look at the R2 of actual vs predicted for each model, and I see that Model 2 is better, but how do I determine if it is statistically significantly better?</p>

<p>I've also used the Concordance Correlation Coefficient, but again, I can't find a way to prove significance. The best I've come up with is that the rho for Model 2 is better than Model 1, but that rho value is within the 95% confidence limits of the rho for Model 1.</p>

<p>I should throw in there that my assessment of predicted vs actual has been on a 60 observation hold-out set (240 observations in the training set).</p>
"
"NaN","NaN"," 79942","<p>I have elemental concentrations for 18 elements in 36 samples, determined by neutron activation analysis. I would like to calculate correlation coefficients, etc. by regression analysis according to the York method.</p>
"
"0.0593390829096927","0.0583708405417779"," 81760","<p>Let's say I have monthly data on cars on the road and number of car accidents. I want to know whether cars on the road is somehow related to the number of car accidents. Of course, I could conduct regression analysis or correlation analysis. However, when I have a set of 50 or so months with the number of cars on the road and number of accidents, how can I perform hypothesis testing to test whether a rise in cars on the road was related to an increase in automobile accidents.</p>

<p>Tagged as R because I'll be using R though my question relates to 'how would I solve this basic problem""</p>
"
"0.0593390829096927","0.0583708405417779"," 82270","<p>I run the following linear model in R :</p>

<pre><code>lm(formula = NA. ~ PC + I(1/SPCI), data = DSET)
</code></pre>

<p>The p-value for each predictor is significant, and it works fairly well with respect to most of the assumptions in linear regression, such as:</p>

<ul>
<li>Normal distribution of errors.</li>
<li>High correlation between predicted values and estimated values.</li>
<li>Homoscedasticity.</li>
<li>Non-collinearity between the predictors: PC and (1/SPCI) are not correlated at all!</li>
</ul>

<p>But, digging more into the topic, there's an assumption that fails in my model, and it says: </p>

<blockquote>
  <p>Linearity of the relationship between dependent and independent variables. </p>
</blockquote>

<p>This kind of contradicts the non-collinearity assumption because, if NA. and PC are highly correlated and NA. and (1/SPCI) are too, then PC and (1/SPCI) are correlated, and this is violating the assumption of non-collinearity. </p>

<p>I think that I misunderstood the assumption, or there's an explanation to this.</p>
"
"0.0593390829096927","0.0583708405417779"," 82974","<p>Suppose I have a bivariate responses with significant correlation. I am trying to compare the two ways to model these outcomes. One way is to model the difference between the two outcomes: 
$$(y_{i2}-y_{i1}=\beta_0+X'\beta)$$
Another way is to use <code>gls</code> or <code>gee</code> to model them:
$$(y_{ij}=\beta_0+\text{time}+X'\beta)$$</p>

<p>Here is a foo example:</p>

<pre><code>#create foo data frame

require(mvtnorm)
require(reshape)
set.seed(123456)
sigma &lt;- matrix(c(4,2,2,3), ncol=2)
y &lt;- rmvnorm(n=500, mean=c(1,2), sigma=sigma)
cor(y)
x1&lt;-rnorm(500)
x2&lt;-rbinom(500,1,0.4)
df.wide&lt;-data.frame(id=seq(1,500,1),y1=y[,1],y2=y[,2],x1,x2)
df.long&lt;-reshape(df.wide,idvar=""id"",varying=list(2:3),v.names=""y"",direction=""long"")
df.long&lt;-df.long[order(df.long$id),]
    df.wide$diff_y&lt;-df.wide$y2-df.wide$y1


#regressions
fit1&lt;-lm(diff_y~x1+x2,data=df.wide)
fit2&lt;-lm(y~time+x1+x2,data=df.long)
fit3&lt;-gls(y~time+x1+x2,data=df.long, correlation = corAR1(form = ~ 1 | time))
</code></pre>

<p>What's the fundamental difference between <code>fit1</code> and <code>fit2</code>? And between <code>fit2</code> and <code>fit3</code>, given they are so close on the $p$ values and estimates? </p>
"
"0.0839181358296689","0.0825488343412996"," 82981","<p>I am exploring hierarchical logistic regression, using glmer from the lme4 package. To my understanding, one of the first steps in multilevel modeling is to estimate the degree of clustering of level-1 units within level-2 units, given by the intraclass correlation (to ""justify"" the additional cost of estimating parameters to account for the clustering). When I run a fully unconditional model with glmer</p>

<pre><code>fitMLnull &lt;- glmer(outcome ~ 1 + (1|level2.ID), family=binomial)
</code></pre>

<p>the glmer fit gives me the variance in the intercept for outcome at level-2, but the residual level-1 variance in outcome is nowhere to be found. I read somewhere (can't track it down now) that the residual level-1 variance is <em>not</em> estimated in HGLM (or at least in glmer). Is this true? If so, is there an alternative way to approximate the degree of clustering in the data? If not, how can I access this value to calculate the ICC?</p>
"
"0.0792951849619439","0.109201843342674"," 83400","<p>I have response variable count data that should be treated as quasipoisson or something similar.  This data also contains outliers which are important to the dataset.  I cannot find an r package that will let me do robust regression with quasipoisson-type distribution.  I could, however, collect weights for each model using robust regression and then try to use those weights in glm.  Note those weights are always > 0.  My problem is that I cannot find a clear definition of how weights are used in glm with family=quasipoisson.  It sounds similar but I can't figure out if actually does what I want it to do (e.g. downweight outlier response variables to decrease their impact).</p>

<p>The response data are numbers of moths caught daily at pheromone traps, and the predictor variables are weather data.  All variables are diffed to handle temporal autocorrelation.  The outliers are high numbers of migratory moths in response to cold front passages.</p>

<p>I just found this reference which explains that low weights are interpreted as representing observations with a high variance, which does make sense in my case.  But does that mean the effect is to reduce the impact of that observation on the regression? <a href=""http://r.789695.n4.nabble.com/weights-in-glm-PR-8720-td910336.html"" rel=""nofollow"">http://r.789695.n4.nabble.com/weights-in-glm-PR-8720-td910336.html</a></p>
"
"0.103843395091962","0.116741681083556"," 84054","<p>I encountered a real-world problem where I want to model the effectiveness of various advertising media of a brand (measured in terms of sales). Basically, the Y in this case is weekly sales, and the X's are media investments in newspaper, magazine, display boards, tv, radio and online, as well as incentive, which is a percentage (like 10% off the original).</p>

<p>There are a few problems with the modelling work:</p>

<ul>
<li><p>all variables should have positive coefficients. Typically, more advertising or incentive is at least as good as less advertising/incentive (maybe this is not true if you buy all of the advertising spots in the world, as then your consumer will start to hate your brand, but this is not going to happen here). However, when I fit a typical regression (e.g. lm, glm, gls etc), some coefficients turn out to be negative (as data may be a bit noiser than expected, hence causing this problem?). I wonder if this can be controlled (I know in nonlinear regressions you can set constraints for parameters)</p></li>
<li><p>there should be some sort of diminishing marginal return of advertising spendings, but I am not exactly sure how to model that. Some ideas include using a log or square root transformation, another idea may be to use a nonlinear regression and estimator something like a*newspaper^b, where a is some coefficient, and b is an exponent between 0 and 1.</p></li>
<li><p>this is serial correlation, but this may not be exactly important here as the goal is only to estimate the parameters (if I use a regression I think I still get the unbiased estimators right? Autocorrelation only screws up the p-values, which is ignored here). Also, how to deal with seasonalities? I don't have much data (2 years) so maybe there is nothing we can do about it, but I have seen adding cos(0.0172*time) + sin(0.0172*time) to the regression equation to adjust for seasonal changes.</p></li>
</ul>

<p>Thanks.</p>
"
"0.15699645640569","0.154434727891351"," 86273","<p>I'm trying to calculate the log-likelihood for a generalized nonlinear least squares regression for the function $f(x)=\frac{\beta_1}{(1+\frac x\beta_2)^{\beta_3}}$ optimized by the <code>gnls</code> function in the R package <code>nlme</code>, using the variance covariance matrix generated by distances on a a phylogenetic tree assuming Brownian motion (<code>corBrownian(phy=tree)</code> from the <code>ape</code> package). The following reproducible R code fits the gnls model using x,y data and a random tree with 9 taxa:</p>

<pre><code>require(ape)
require(nlme)
require(expm)
tree &lt;- rtree(9)
x &lt;- c(0,14.51,32.9,44.41,86.18,136.28,178.21,262.3,521.94)
y &lt;- c(100,93.69,82.09,62.24,32.71,48.4,35.98,15.73,9.71)
data &lt;- data.frame(x,y,row.names=tree$tip.label)
model &lt;- y~beta1/((1+(x/beta2))^beta3)
f=function(beta,x) beta[1]/((1+(x/beta[2]))^beta[3])
start &lt;- c(beta1=103.651004,beta2=119.55067,beta3=1.370105)
correlation &lt;- corBrownian(phy=tree)
fit &lt;- gnls(model=model,data=data,start=start,correlation=correlation)
logLik(fit) 
</code></pre>

<p>I would like to calculate the log-likelihood ""by hand"" (in R, but without use of the <code>logLik</code> function) based on the estimated parameters obtained from <code>gnls</code> so it matches the output from <code>logLik(fit)</code>. NOTE: I am not trying to estimate parameters; I just want to calculate log-likelihood of the parameters estimated by the <code>gnls</code> function (although if someone has a reproducible example of how to estimate parameters without <code>gnls</code>, I would be very interested in seeing it!). </p>

<p>I'm not really sure how to go about doing this in R. The linear algebra notation described in Mixed-Effects Models in S and S-Plus (Pinheiro and Bates) is very much over my head and none of my attempts have matched <code>logLik(fit)</code>. Here are the details described by Pinheiro and Bates:</p>

<p>The log-likelihood for the generalized nonlinear least squares model  $y_i=f_i(\phi_i,v_i)+\epsilon_i$ where $\phi_i=A_i\beta$ is calculated as follows:</p>

<p>$l(\beta,\sigma^2,\delta|y)=-\frac 12 \Bigl\{ N\log(2\pi\sigma^2)+\sum\limits_{i=1}^M{\Bigl[\frac{||y_i^*-f_i^*(\beta)||^2}{\sigma^2}+\log|\Lambda_i|\Bigl]\Bigl\}}$</p>

<p>where $N$ is the number of observations, and $f_i^*(\beta)=f_i^*(\phi_i,v_i)$.</p>

<p>$\Lambda_i$ is positive-definite, $y_i^*=\Lambda_i^{-T/2}y_i$ and $f_i^*(\phi_i,v_i)=\Lambda_i^{-T/2}f_i(\phi_i,v_i)$</p>

<p>For fixed $\beta$ and $\lambda$, the ML estimator of $\sigma^2$ is </p>

<p>$\hat\sigma(\beta,\lambda)=\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2 / N$</p>

<p>and the profiled log-likelihood is</p>

<p>$l(\beta,\lambda|y)=-\frac12\Bigl\{N[\log(2\pi/N)+1]+\log\Bigl(\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2\Bigl)+\sum\limits_{i=1}^M\log|\Lambda_i|\Bigl\}$</p>

<p>which is used with a Gauss-Seidel algorithm to find the ML estimates of $\beta$ and $\lambda$. A less biased estimate of $\sigma^2$ is used:</p>

<p>$\sigma^2=\sum\limits_{i=1}^M\Bigl|\Bigl|\hat\Lambda_i^{-T/2}[y_i-f_i(\hat\beta)]\Bigl|\Bigl|^2/(N-p)$</p>

<p>where $p$ represents the length of $\beta$.</p>

<p>I have compiled a list of specific questions that I am facing:</p>

<ol>
<li>What is $\Lambda_i$? Is it the distance matrix produced by <code>big_lambda &lt;- vcv.phylo(tree)</code> in <code>ape</code>, or does it need to be somehow transformed or parameterized by $\lambda$, or something else entirely?</li>
<li>Would $\sigma^2$ be <code>fit$sigma^2</code>, or the equation for the less biased estimate (the last equation in this post)?</li>
<li>Is it necessary to use $\lambda$ to calculate log-likelihood, or is that just an intermediate step for parameter estimation? Also, how is $\lambda$ used? Is it a single value or a vector, and is it multiplied by all of $\Lambda_i$ or just off-diagonal elements, etc.?</li>
<li>What is $||y-f(\beta)||$? Would that be <code>norm(y-f(fit$coefficients,x),""F"")</code> in the package <code>Matrix</code>? If so, I'm confused about how to calculate the sum $\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2$, because <code>norm()</code> returns a single value, not a vector.</li>
<li>How does one calculate $\log|\Lambda_i|$? Is it <code>log(diag(abs(big_lambda)))</code> where <code>big_lambda</code> is $\Lambda_i$, or is it <code>logm(abs(big_lambda))</code> from the package <code>expm</code>? If it is <code>logm()</code>, how does one take the sum of a matrix (or is it implied that it is just the diagonal elements)?</li>
<li>Just to confirm, is $\Lambda_i^{-T/2}$ calculated like this: <code>t(solve(sqrtm(big_lambda)))</code>?</li>
<li>How are $y_i^*$ and $f_i^*(\beta)$ calculated? Is it either of the following:</li>
</ol>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) %*% y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) %*% f(fit$coefficients,x)</code></p>

<p>or would it be</p>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) * y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) * f(fit$coefficients,x)</code> ?</p>

<p>If all of these questions are answered, in theory, I think the log-likelihood should be calculable to match the output from <code>logLik(fit)</code>. Any help on any of these questions would be greatly appreciated. If anything needs clarification, please let me know. Thanks!</p>

<p><strong>UPDATE</strong>: I have been experimenting with various possibilities for the calculation of the log-likelihood, and here is the best I have come up with so far. <code>logLik_calc</code> is consistently about 1 to 3 off from the value returned by <code>logLik(fit)</code>. Either I'm close to the actual solution, or this is purely by coincidence. Any thoughts?</p>

<pre><code>  C &lt;- vcv.phylo(tree) # variance-covariance matrix
  tC &lt;- t(solve(sqrtm(C))) # C^(-T/2)
  log_C &lt;- log(diag(abs(C))) # log|C|
  N &lt;- length(y)
  y_star &lt;- tC%*%y 
  f_star &lt;- tC%*%f(fit$coefficients,x)
  dif &lt;- y_star-f_star  
  sigma_squared &lt;-  sum(abs(y_star-f_star)^2)/N
  # using fit$sigma^2 also produces a slightly different answer than logLik(fit)
  logLik_calc &lt;- -((N*log(2*pi*(sigma_squared)))+
       sum(((abs(dif)^2)/(sigma_squared))+log_C))/2
</code></pre>
"
"0.103843395091962","0.116741681083556"," 86952","<p>I'm trying to regress some simple pooled data. My data has 60 observations and three columns: Weight, Height, and Sex (female=1, male=0).</p>

<p>If I regress thus, Weight ~ Height + Sex, my model is fairly satisfactory, but the residuals are not homoscedastic (green errors are male, blue female):</p>

<p><img src=""http://i1267.photobucket.com/albums/jj541/nbahmanyar/Rplot_zps69001b34.png"" alt=""plot""></p>

<p>I tried regressing on the log of Weight and/or Height, but that didn't do much. What should I do to make the residuals homescedastic and/or make my model more accurate? Any help would be appreciated.</p>

<p><strong>Edit</strong></p>

<p>Doing a generalized regression model gives the following.</p>

<pre><code>Generalized least squares fit by REML
  Model: Weight ~ h + s 
  Data: P149 
       AIC      BIC    logLik
  514.2221 524.4374 -252.1111

Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | Sex 
 Parameter estimates:
        0         1 
1.0000000 0.6685307 

Coefficients:
                 Value Std.Error   t-value p-value
(Intercept)  27.197499  51.88129  0.524226  0.6022
h             1.852382   0.75634  2.449128  0.0174
s           -25.284478   5.53300 -4.569755  0.0000

 Correlation: 
  (Intr) h     
h -0.997       
s -0.524  0.466

Standardized residuals:
       Min         Q1        Med         Q3        Max 
-1.6655243 -0.6879858 -0.1839396  0.5628971  3.9857544 

Residual standard error: 22.13369 
Degrees of freedom: 60 total; 57 residual
</code></pre>

<p>With this s. residual plot:</p>

<p><img src=""http://i1267.photobucket.com/albums/jj541/nbahmanyar/Rplot1_zps5ee264a0.png"" alt=""""></p>

<p>Could someone please explain how precisely this model is different from a standard multiple regression model? Thanks.</p>
"
"0.0419590679148345","0.0412744171706498"," 88722","<p>I am building a regression model of time series data in R, where my primary interest is the coefficients of the independent variables. The data exhibit strong seasonality with a trend.</p>

<p><img src=""http://i.stack.imgur.com/GYxaU.png"" alt=""Original data""></p>

<p>The model looks good, with four of the six regressors significant:
<img src=""http://i.stack.imgur.com/ZmoSd.png"" alt=""Model""></p>

<p>Here are the OLS residuals:
<img src=""http://i.stack.imgur.com/EIybo.png"" alt=""Residuals""></p>

<p>I used auto.arima to select the sARIMA structure, and it returns the model (0,1,1)(1,1,0)[12].</p>

<pre><code>fit.ar &lt;- auto.arima(at.ts, xreg = xreg1, stepwise=FALSE, approximation=FALSE)
summary(fit.ar)

Series: at.ts 
ARIMA(0,1,1)(1,1,0)[12]                    

Coefficients:
          ma1    sar1      v1       v2      v3       v4         v5
      -0.7058  0.3974  0.0342  -0.0160  0.0349  -0.0042  -113.4196
s.e.   0.1298  0.2043  0.0239   0.0567  0.0555   0.0333   117.1205

sigma^2 estimated as 3.86e+10:  log likelihood=-458.13
AIC=932.26   AICc=936.05   BIC=947.06

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 7906.896 147920.3 103060.4 0.1590107 3.048322 0.1150526
</code></pre>

<p>My question is this: based on the parameter estimates and s.e. of the regressors, I believe that none of them are significant - is this correct, and if so, what does it imply if my goal is to interpret the relative importance of these predictors as opposed to forecasting?</p>

<p>Any other advice relative to the process of building this model is welcome and appreciated.</p>

<p>Here are the ACF and PACF for the residuals:</p>

<p><img src=""http://i.stack.imgur.com/a3Gvy.png"" alt=""ACF-PACF""></p>

<pre><code>&gt; durbinWatsonTest(mod.ols, max.lag=12)
 lag Autocorrelation D-W Statistic p-value
   1     0.120522674     1.6705144   0.106
   2     0.212723044     1.4816530   0.024
   3     0.159828108     1.5814771   0.114
   4     0.031083831     1.8352377   0.744
   5     0.081081308     1.6787808   0.418
   6    -0.024202465     1.8587561   0.954
   7    -0.008399949     1.7720761   0.944
   8     0.040751905     1.6022835   0.512
   9     0.129788310     1.4214391   0.178
  10    -0.015442379     1.6611922   0.822
  11     0.004506292     1.6133994   0.770
  12     0.376037337     0.7191359   0.000
 Alternative hypothesis: rho[lag] != 0
</code></pre>
"
"0.0593390829096927","0.0583708405417779"," 90536","<p>We have two huge correlation matrices at different experimental conditions. If we want to identify the significant differences between these matrices , what would be the ideal method. I have implemented PCA for dimentionality reduction for the two matrices, and compared both row by row. Will it give some information regarding the differences. More over how about LDA and logistic regression. Will it give any information.   </p>
"
"0.139648219234718","0.137369563821848"," 91243","<p>I have a large data frame in the following form (I apologize for this formatting):</p>

<pre><code>Site    Season  T          SC    pH    Chl   DO.S   DO      BGA  Tur    fDOM    Flow    Rainfall    Solar      Rain
300N    Winter  14.05   1692.77 7.93    NA  82.26   8.42    NA  9.25    NA      NA      0.00          219.18     no
</code></pre>

<p>If you can't understand the formatting, there are 12 numerical factors, and 3 categorical factors (<code>Site</code>, <code>Season</code>, <code>Rain</code> [yes/no]). Each row represents the average daily values that I have calculated from 15-minute time series. I have spent a good amount of time doing data exploration (linear regression analysis, looking at time series plots for patterns), but haven't found a method that works for me yet. I have also worked with <code>corrplot</code>, correlation matrices, and covariance functions in an arduous way, where I subset each categorical combination and found <code>corrplot</code>s for each (I have also tried it with <code>ddply</code>, but the resulting format is not in the correlation matrix format that is easy to plot). I have also attempted PCA on the data to little avail.</p>

<p>My question is first and foremost, does anyone have an idea for data visualization of this kind of dataset? The main question I am after is, ""What are the factors that influence <code>DO</code> (dissolved oxygen)?"". How does this change by location (<code>Site</code>), <code>Season</code>, and with the influence of <code>Rain</code>. I would really like a quick method for shooting out correlation matrices (or heat maps; I have tried both) for each categorical subset. I tried this with <code>ggplot</code> and <code>facet_wrap</code>, but it wasn't happening for me. I also tried <code>ggpairs</code> from the GGally package, but honestly didn't spend too much time with that method.</p>

<p>I was starting to get into the idea of star graphs (on polar coordinates), which can be used to visualize repeating periodicity in time series, but am running out of time and decided to seek the advisement of Stack Overflow. I really appreciate any advice or thoughts on visualizing this data that come to your mind. I feel like some combination of <code>ddply</code> and graphing is what I need, but I haven't gotten there yet.
Thank you for your time.</p>

<p>EDIT:
<code>dput</code> of the data frame in question:</p>

<pre><code>structure(list(Site = structure(c(2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""2100S"", 
""300N"", ""3300S"", ""800S"", ""Burnham"", ""Center""), class = ""factor""), 
    Season = structure(c(4L, 4L, 4L, 4L, 2L, 2L), .Label = c(""Fall"", 
    ""Spring"", ""Summer"", ""Winter""), class = ""factor""), T = c(14.05, 
    14.18, 14.5, 14.58, 14.07, 11.91), SC = c(1692.77, 1671.31, 
    1680.71, 1661.79, 1549.56, 1039.63), pH = c(7.93, 7.92, 7.96, 
    7.95, 7.93, 7.79), Chl = c(NA_real_, NA_real_, NA_real_, 
    NA_real_, NA_real_, NA_real_), DO.S = c(82.26, 78.79, 82.05, 
    80.92, 74.33, 73.96), DO = c(8.42, 8.04, 8.31, 8.18, 7.61, 
    7.97), BGA = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, 
    NA_real_), Tur = c(9.25, 9.77, 9.41, 10.6, 40.38, 50.25), 
    fDOM = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, 
    NA_real_), Flow = c(NA, 178.08, 178.53, 188.13, 306.15, 382.22
    ), Rainfall = c(0, 0, 0, 0, 0.01, 0.81), Solar = c(219.18, 
    228.33, 244.3, 247.69, 105.15, 220.73), Rain = structure(c(1L, 
    1L, 1L, 1L, 2L, 2L), .Label = c(""no"", ""yes""), class = ""factor"")), .Names = c(""Site"", 
""Season"", ""T"", ""SC"", ""pH"", ""Chl"", ""DO.S"", ""DO"", ""BGA"", ""Tur"", 
""fDOM"", ""Flow"", ""Rainfall"", ""Solar"", ""Rain""), row.names = c(NA, 
6L), class = ""data.frame"")
</code></pre>
"
"0.118678165819385","0.116741681083556"," 93417","<p>This question is a prolongation of this question: <a href=""http://stats.stackexchange.com/questions/11096/how-to-interpret-coefficients-in-a-poisson-regression"">How to interpret coefficients in a Poisson regression?</a></p>

<p>If we follow the (almost) exact same routine, but we add correlation between the variablese treatment and improved (just for the sake of my question, which is interpreting the output), we get:</p>

<pre><code>treatment     &lt;- factor(rep(c(1, 2), c(43, 41)), 
                        levels = c(1, 2),
                        labels = c(""placebo"", ""treated""))
improved      &lt;- factor(rep(c(1, 2, 3, 1, 2, 3), c(29, 7, 7, 13, 7, 21)),
                        levels = c(1, 2, 3),
                        labels = c(""none"", ""some"", ""marked""))    
numberofdrugs &lt;- rpois(84, 10) + 1    
healthvalue   &lt;- rpois(84, 5)   
y             &lt;- data.frame(healthvalue, numberofdrugs, treatment, improved)
test          &lt;- glm(healthvalue~numberofdrugs+treatment+improved + treatment:improved, y, family=poisson)
summary(test)
</code></pre>

<p>Note the $\textbf{ treatment:improved}$ term I added inside the glm function. </p>

<p>Now, we get the following output:</p>

<pre><code>    Call:
glm(formula = healthvalue ~ numberofdrugs + treatment + improved + 
    treatment:improved, family = poisson, data = y)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.9261  -0.8733  -0.0296   0.5473   2.3358  

Coefficients:
                                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      1.553051   0.184229   8.430   &lt;2e-16 ***
numberofdrugs                    0.004298   0.014242   0.302   0.7628    
treatmenttreated                 0.007399   0.149440   0.050   0.9605    
improvedsome                     0.358897   0.164891   2.177   0.0295 *  
improvedmarked                  -0.178360   0.203756  -0.875   0.3814    
treatmenttreated:improvedsome   -0.330336   0.265310  -1.245   0.2131    
treatmenttreated:improvedmarked  0.050617   0.260203   0.195   0.8458    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 97.805  on 83  degrees of freedom
Residual deviance: 89.276  on 77  degrees of freedom
AIC: 383.29

Number of Fisher Scoring iterations: 5
</code></pre>

<p>If we ignore what seems to be insignificant coefficients, I can ask my question:</p>

<p>I understand that, as in the original post, treatment=placebo and improved=none is the base level for those variables, and thus are set to zero. My question is, why does it not exist any interaction terms with the base lavels for treatment=placebo and improved=none?</p>

<p>I thought setting the base levels to zero was just a construct, and in my mind there should still exist correlation between them...(?)</p>
"
"0.0856485887284415","0.101101261498861"," 93423","<p>please help a sociologist struggling to get to grips with R and statistics in general..!</p>

<p>I've got a data set with 11 variables. I need to investigate the possible relationships between one of the variables (percentage of smokers in a population) and the rest of them- things like unemployment rate, education level, and so on. I'm working purely with linear regression.</p>

<p>I'm getting there with correlation, but having looked around at numerous examples on the web, I'm still unsure with regards to method. Apologies if it's completely obvious, but I can't seem to find an answer anywhere.</p>

<p>As I see it, it seems sensible to look for correlation between smoking and all the other variables firstly with scatterplots, and then by calculating the correlation coefficient if there is an identifiable linear relationship (Pearson's r or Spearman, depending on normality). If we then continue and do separate simple linear regressions between smoking rate and our identified variable, that all seems well and good.</p>

<p>But where does multiple regression fit in here? Does it make any sense, statistically speaking, to do simple linear regression first and then multiple linear regression also? Or would it be best to just jump straight from testing correlation to multiple regression?</p>

<p>Any help much appreciated!</p>
"
"0.0726752374667264","0.0714893875923587"," 94009","<p>I've built a random forest model (regression model) using randomForest package in R, and I calculate the correlation between the predicted values and the actual ones in order to know how the trained model is going to perform, which is very high in my case, so I was wondering in such case how this correlation is build, I mean is it leave one out correlation or any type of cross validation correlation or just random and can't represent the real performance of the model when tested on unseen new cases???? the following is a snapshot of my script for calculating the correlation where x is the data (observations) and y is the numeric values I want the model to learn/predict (in the testing cases):</p>

<pre><code>mytr_all = randomForest(x, y, ntree = 500,corr.bias=TRUE)
cor(mytr_all$y,mytr_all$predicted)
</code></pre>
"
"0.111013258946721","0.109201843342674"," 95891","<p>I'm running a logistic regression model where anecdotally I expected age to be a very large factor. If you see from the charts I made in Excel before running the model through R, this is how the support lines up by age:</p>

<p><img src=""http://i.stack.imgur.com/oEVZQ.jpg"" alt=""enter image description here""></p>

<p>Looks pretty significant.</p>

<p>Though when I run the model, as you can see below, age is the <em>only</em> thing that's not significant -- which was very surprising:</p>

<pre><code>&gt; attach(mydata) 
&gt; 
&gt; # Define variables 
&gt; 
&gt; Y &lt;- cbind(support)
&gt; X &lt;- cbind(sex, region, age, supportscore1, supportscore2, county)
&gt;
&gt; # Logit model coefficients 
&gt; 
&gt; logit &lt;- glm(Y ~ X, family=binomial (link = ""logit""), na.action = na.exclude) 
&gt; 
&gt; summary(logit) 

Call:
glm(formula = Y ~ X, family = binomial(link = ""logit""), na.action = na.exclude)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1019  -0.7609   0.5231   0.7101   2.3965  

Coefficients:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            4.013446   0.440962   9.102  &lt; 2e-16 ***
Xsex                  -0.229256   0.104859  -2.186 0.028792 *  
Xregion               -1.103308   0.091497 -12.058  &lt; 2e-16 ***
Xage                   0.004569   0.003209   1.424 0.154512    
Xsupportscore1        -0.019262   0.005732  -3.360 0.000778 ***
Xsupportscore2         0.019810   0.005264   3.764 0.000168 ***
Xcounty               -0.047581   0.011161  -4.263 2.02e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2871.5  on 2072  degrees of freedom
Residual deviance: 2245.5  on 2066  degrees of freedom
  (66 observations deleted due to missingness)
AIC: 2259.5

Number of Fisher Scoring iterations: 4
</code></pre>

<p>My only guess on this is that the previous support scores (both 0-100 numerical values) I'm using may have already taken age into account, and the model doesn't want to count it twice. Though, to compare, region and county are just two different ways of cutting up the geography -- and those both seem significant.</p>

<p>Can somebody let me know what you would think if your model told you that age wasn't significant when in clearly is? Trying to figure out if there's a way of thinking about it that I'm missing or if something in my code is wrong.</p>

<p>Thanks!</p>

<p>--
<strong>EDIT</strong></p>

<p>Pairs plot added to show correlation (despite some factors being categorical):</p>

<pre><code>pairs(~sex + region +  age + supportscore1 + supportscore2 + county, data=mydata)
</code></pre>

<p><img src=""http://i.stack.imgur.com/N2IG4.jpg"" alt=""enter image description here""></p>
"
"NaN","NaN"," 97774","<p>I am currently analyzing a data set having to do with oak tree mortality. I am trying to understand the correlation between dead crowns (dead = <code>0</code>, live = <code>1</code>) and the distance to <strong>(1)</strong> nearest dead crown and <strong>(2)</strong> distance to nearest bay tree. (See Below).</p>

<p>I have run a probit regression model:</p>

<pre><code>Living&lt;-sample(0:1,10,replace=T)
    Dead_Dist&lt;-c(158.68,99.62,64.42,64.42,86.92,117.77,41.81,41.81,54.73,64.35)
    Bay_Dist&lt;-c(92.47,179.92,317.73,365.23,58.70,193.23,330.36,123.14,88.65,72.34)
    mydata&lt;-cbind(Living,Dead_Dist,Bay_Dist)
    mydata1&lt;-data.frame(mydata)
    myprobit &lt;- glm(Living ~ Bay_Dist + Dead_Dist, family = binomial(link = ""probit""),data = mydata1)
    myprobit
</code></pre>

<p>But I am at a loss for how to plot and interpret the results, as I am fairly new to R and regressions.<br>
If anyone has suggestions with how to proceed with this analysis, I would appreciate it!</p>
"
"0.0839181358296689","0.0825488343412996"," 97834","<p>I ran a mixed model using lme4::glmer for a logistic regression and consistently got these warning messages. I noticed there are still regular results even so, but are they accurate estimates?</p>

<pre><code>    &gt; glmm.ms1&lt;-glmer(as.formula(paste(paste(y[1], x, sep=""~""), mix[1], sep=""+"")),
    +             data=rtf2,control=glmerControl(optimizer=""bobyqa"",
    +             optCtrl=list(maxfun=100000),family=binomial)
Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.8766 (tol = 0.001)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues

&gt; coef(summary(glmm.ms1))
                       Estimate Std. Error    z value  Pr(&gt;|z|)
(Intercept)           1.810e+00  6.558e-01   2.760464 5.772e-03
lepidays             -3.340e+00  2.770e-01 -12.059620 1.726e-33
cldaysbirth          -1.555e+00  5.224e-01  -2.975934 2.921e-03
rotaarm              -2.057e-01  3.209e-01  -0.641102 5.215e-01
cldaysbirth2         -3.072e-01  2.955e-01  -1.039510 2.986e-01
bfh2                 -1.043e+01  1.160e+03  -0.008996 9.928e-01
bfh3                  4.653e-01  4.806e-01   0.968103 3.330e-01
bfh4                  2.547e-01  4.994e-01   0.509966 6.101e-01
bfh5                  3.744e-01  9.926e-01   0.377213 7.060e-01
ruuska               -1.020e-01  5.928e-02  -1.720396 8.536e-02
genderMale           -4.008e-01  2.645e-01  -1.515453 1.297e-01
epiexlbf              6.078e-04  2.796e-03   0.217391 8.279e-01
haz.epi              -7.211e-02  1.373e-01  -0.525039 5.996e-01
cldaysbirth:rotaarm   6.928e-01  4.771e-01   1.452148 1.465e-01
rotaarm:cldaysbirth2  5.181e-01  3.352e-01   1.545527 1.222e-01
Warning messages:
1: In vcov.merMod(object, use.hessian = use.hessian) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite: falling back to var-cov estimated from RX
2: In vcov.merMod(object, correlation = correlation, sigm = sig) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite: falling back to var-cov estimated from RX
</code></pre>

<p>Due to data's sensitivity, I can't post the whole process for generating same messages, but I would like to know how to handle these warnings. I don't think it's suitable to keep my eye blind here.</p>
"
"0.0593390829096927","0.0583708405417779"," 99626","<p>I'm trying to show a correlation between growth in a petri dish of some fungi and its effect on a plant. I have ten strains of fungi which I tested in the plant and in petri dishes. I can put data from both experiments into linear models (lm) to get estimated means and variances. If I run a regression on the estimated means for each strain I find a significant correlation, but this doesn't take uncertainty in the strain means into account.
So far I've tried a parametric bootstrap, but I'm having a hard time figuring out the ultimate test statistic. I've included the parameter estimates and my R code. At the moment it generates simulated strain means from the estimated parameters. I need a p value for the covariance between growth and virulence. Any help would be greatly appreciated! My specific questions are:
1) Is a parametric boostrap the right way to go about analyzing this?
2) How would one go about doing this in R?</p>

<pre><code>Strain  MeanVirulence MeanGrowth    VirulenceVarGrowthVar  
1  -5.26064 0.066716    0.67834 0.053247  
2   -4.05482    -0.055524   0.68385 0.047111  
3   -5.47282    0.029047    0.68385 0.046739  
4   -3.50632    -0.161811   0.68385 0.047083  
5   -4.94051    -0.224949   0.68385 0.04727  
6   -4.04982    -0.062938   0.68385 0.047647  
7   -4.53178    -0.142985   0.68385 0.04788  
8   -3.01697    -0.199349   0.68385 0.047255  
9   -3.81093    -0.254793   0.68385 0.047255  
10  -1.61882    -0.325289   0.68385 0.0469
</code></pre>

<p>And here is my R code:</p>

<pre><code>gendata&lt;-function(par,npar=TRUE,print=TRUE){
    n     = 10
    k     = 2
    x=matrix(data=NA, nrow=n, ncol=k)
    for(i in 1:n){
      x[i,1] = rnorm(1,mean=par[i,2],sd=par[i,4])
      x[i,2] = rnorm(1,mean=par[i,3],sd=par[i,5])
    }
    return(x)
}

lmp &lt;- function (modelobject) {
    f &lt;- summary(modelobject)$fstatistic
    p &lt;- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) &lt;- NULL
    return(p)
}


samp=20000
rescor=matrix(data=NA, samp)
resvar=matrix(data=NA, samp)
pvals=matrix(data=NA, samp)
numsig = 0
numnotsig = 0
for (i in 1:samp){
    x&lt;-gendata(parameters)
    rescor[i]&lt;-cor(x[,1],x[,2], method = ""pearson"")
    resvar[i]&lt;-var(x[,1])
    a&lt;-lm(x[,1]~x[,2])
    pvals[i]&lt;-lmp(a)
    if (pvals[i] &lt; 0.05){
        numsig = numsig + 1
    }
    if (pvals[i] &gt; 0.05){
        numnotsig = numnotsig + 1
    }    
}

Strain Plate Growth  
1      1     200   
1      2     210  
1      3     190  
2      1     150   
2      2     130  
2      3     140  
...  

Strain Plant Growth  
1      1     70  
1      2     40  
1      3     50  
1      4     45  
2      1     80  
2      2     90  
2      3     85  
2      4     75  
...
</code></pre>
"
"0.151285570837612","0.148817027473669","103666","<p>I have trained my random forest model on a 74,000 training examples where each example consists of two proteins Amino Acids sequence (20 characters) and some numeric values representing the similarity between each individual pair of sequences, and finally a numeric value representing the overall similarity between the two proteins, this is a regression model, so I wish for testing I can use just protein sequence and use my trained model to predict the distance between my test case and each of my training protein sequences. A sample of my test case is:</p>

<pre><code>   test= G,Y,L,P,P,S, A,N,L,F,S,N, 1,-2,16,-4,-1,11, 21
</code></pre>

<p>where ""G,Y,L,P,P,S,"" represent a 6 character fragment of the first protein (my testing) and ""A,N,L,F,S,N,"" represent a 6 character fragment of the second protein (my training database) and the numbers ""1,-2,16,-4,-1,11,"" each number represent the similarity between individual pairs of the 6 Amino Acids, e.g., the similarity between ""G and A"" is 1 and the similarity between ""Y and N"" is -2 and the similarity between ""L and L"" is 16, and so on offcourse the higher the number means the higher the similarity between the pairs of characters, finally the last number ""21"" represent the sum of the previous 6 numbers which represents the overall similarity between the two sequences.</p>

<p>when I trained my model on 74,000 of such training datasets the correlation between the predicted distance and actual distance was as high as 0.86, however, when I used the trained model in for testing the correlation was very low 0.17, I strongly believe that this over-fitting problem, however, I'm not sure is it due to the may be not good training datasets or that my features aren't strong enough to give a good prediction especially since the ranking of the features according to their importance was very high for all the features? the following is my features importance according to each node purity. any help on how to recognize the source of the overfitting is highly appreciated:</p>

<pre><code>       IncNodePurity
 V3      24564.326
 V4      22503.744
 V5      25030.450
 V6      24583.235
 V7      24661.309
 V8      20757.662
 V9      22985.824
 V10     22189.759
 V11     23875.170
 V12     23674.853
 V13     23339.595
 V14     19576.762
 V15     10169.309
 V16     19527.972
 V17      5430.600
 V18      4415.307
 V19     12897.114
 V20      3963.717
 V21     62614.692
</code></pre>
"
"0.0938233281301002","0.0922924025252575","104306","<p>One common thing to do when doing Principal Component Analysis (PCA) is to plot two loadings against each other to investigate the relationships between the variables. In the paper accompanying the <a href=""http://www.jstatsoft.org/v18/i02"">PLS R package</a> for doing Principal Component Regression and PLS regression there is a different plot, called the <em>correlation loadings plot</em> (see figure 7 and page 15 in the paper). The <em>correlation loading</em>, as it is explained, is the correlation between the scores (from the PCA or PLS) and the actual observed data. </p>

<p>It seems to me that loadings and correlation loadings are pretty similar, except that they are scaled a bit differently. A reproducible example in R, with the built in data set mtcars is as follows:</p>

<pre><code>data(mtcars)
pca &lt;- prcomp(mtcars, center=TRUE, scale=TRUE)

#loading plot
plot(pca$rotation[,1], pca$rotation[,2],
     xlim=c(-1,1), ylim=c(-1,1),
     main='Loadings for PC1 vs. PC2')

#correlation loading plot
correlationloadings &lt;- cor(mtcars, pca$x)
plot(correlationloadings[,1], correlationloadings[,2],
     xlim=c(-1,1), ylim=c(-1,1),
     main='Correlation Loadings for PC1 vs. PC2')
</code></pre>

<p><img src=""http://i.stack.imgur.com/GJA4c.png"" alt=""loadingplot"">
<img src=""http://i.stack.imgur.com/y32sb.png"" alt=""correlationloadinsplot""></p>

<p>What is the difference in interpretation of these plots? And which plot (if any) is best to use in practice?</p>
"
"0.0839181358296689","0.0825488343412996","104485","<p>I have more of a programming background, and I am fairly new to statistics. I am currently trying to solve some sample exercises to get more familiar with data science / modelling. </p>

<h3>Problem Background</h3>

<p>A user posts a request on a forum. Considering number of responses / number of up votes / users' reputation / etc., can you predict if the request will be fulfilled? There are about 4000 rows of test data available.</p>

<h3>Current Attempts</h3>

<p>I have created basic graphs to investigate correlation, and applied a few transformations to the data. 
I have decided to go with logistic regression, and based on the graphs, have chosen 2 factors for the initial model. </p>

<h3>Problem</h3>

<p>I have created the basic model using R, and I can see some summary stats, but I am a bit lost when I try to understand (a) How well the model fits the data and (b) whether the model is accurate. I tried Googling around, but most of the articles were too technical for me. Is there a cheat sheet/quick test of some sort that I can apply? Or can somebody suggest the simplest ""complicated"" article?</p>
"
"0.132686223108569","0.130521167355216","104889","<p>I am working on cross-validation of prediction of my data with 200 subjects and 1000 variables. I am interested ridge regression as number of variables (I want to use) is greater than number of sample. So I want to use shrinkage estimators.  The following is made up example data:</p>

<pre><code> #random population of 200 subjects with 1000 variables 
    M &lt;- matrix(rep(0,200*100),200,1000)
    for (i in 1:200) {
    set.seed(i)
      M[i,] &lt;- ifelse(runif(1000)&lt;0.5,-1,1)
    }
    rownames(M) &lt;- 1:200

    #random yvars 
    set.seed(1234)
    u &lt;- rnorm(1000)
    g &lt;- as.vector(crossprod(t(M),u))
    h2 &lt;- 0.5 
    set.seed(234)
    y &lt;- g + rnorm(200,mean=0,sd=sqrt((1-h2)/h2*var(g)))

    myd &lt;- data.frame(y=y, M)
myd[1:10,1:10]

y X1 X2 X3 X4 X5 X6 X7 X8 X9
1   -7.443403 -1 -1  1  1 -1  1  1  1  1
2  -63.731438 -1  1  1 -1  1  1 -1  1 -1
3  -48.705165 -1  1 -1 -1  1  1 -1 -1  1
4   15.883502  1 -1 -1 -1  1 -1  1  1  1
5   19.087484 -1  1  1 -1 -1  1  1  1  1
6   44.066119  1  1 -1 -1  1  1  1  1  1
7  -26.871182  1 -1 -1 -1 -1  1 -1  1 -1
8  -63.120595 -1 -1  1  1 -1  1 -1  1  1
9   48.330940 -1 -1 -1 -1 -1 -1 -1 -1  1
10 -18.433047  1 -1 -1  1 -1 -1 -1 -1  1
</code></pre>

<p>I would like to do following for cross validation - </p>

<p>(1) split data into two halts - use first half as training and second half as test </p>

<p>(2) K-fold cross validation (say 10 fold or suggestion on any other appropriate fold for my case are welcome)  </p>

<p>I can simply sample the data into two (gaining and test) and use them: </p>

<pre><code># using holdout (50% of the data) cross validation 
training.id &lt;- sample(1:nrow(myd), round(nrow(myd)/2,0), replace = FALSE)
test.id &lt;- setdiff(1:nrow(myd), training.id)

 myd_train &lt;- myd[training.id,]
 myd_test  &lt;- myd[test.id,]   
</code></pre>

<p>I am using <code>lm.ridge</code> from <code>MASS</code> R package. </p>

<pre><code>library(MASS)
out.ridge=lm.ridge(y~., data=myd_train, lambda=seq(0, 100,0.001))
plot(out.ridge)
select(out.ridge)

lam=0.001
abline(v=lam)

out.ridge1 =lm.ridge(y~., data=myd_train, lambda=lam)
hist(out.ridge1$coef)
    out.ridge1$ym
hist(out.ridge1$xm)
</code></pre>

<p>I have two questions - </p>

<p>(1) How can I predict the test set and calculate accuracy (as correlation of predicted vs actual)?</p>

<p>(2) How can I perform K-fold validation? say 10-fold?</p>
"
"0.0419590679148345","0.0412744171706498","105418","<p>I would like to know if it is possible to create a scatterplot while controlling for covariates, such as in partial correlation. I am using R software and my code is below for the basic scatterplot.</p>

<p>I am not interested in multiple lines of best fit or multiple scatters per graph. I am also not interested in creating a lattice of scatterplots with all of the variables. </p>

<p>I can't seem to find any code that will allow me to parse the effect of the covariates from my x-y scatterplot. All three covariates are continuous. Any ideas would be helpful. Thank you</p>

<pre><code>plot(pmc$reject, pmcp$LPA, main=""r(Parenting, Left Amygdala)"",
    xlab=""Parenting Age 2"", ylab=""Amygdala Reactivity Age 15"", pch=19)
abline(lm(pmc$reject ~ pmcp$LPA), col=""red"") # regression line (y~x)
</code></pre>
"
"0.111013258946721","0.109201843342674","107865","<p>I'm investigating environmental effects (wind) on acoustic receiver detection probability for two types of transmitters using a binomial glmer. While my model analysis indicates that there's a significant effect between wind speed and transmitter type, graphical visualisation does not confirm this. If I'm correct, an interaction should demonstrate different regression slopes.</p>

<pre><code>m1 &lt;- glmer(cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth + 
               Receiver.depth + Water.temperature + Wind.speed + Transmitter + 
               Distance + Habitat + Replicate + (1 | Day) + (Distance | SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat + 
               Receiver.depth:Habitat + Wind.speed:Transmitter, data=df, family=binomial(link=logit))
</code></pre>

<p>The model summary is as follows:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth +  
    Receiver.depth + Water.temperature + Wind.speed + Transmitter +  
    Distance + Habitat + Replicate + (1 | Day) + (Distance |  
    SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat +      Receiver.depth:Habitat + Wind.speed:Transmitter
   Data: df

     AIC      BIC   logLik deviance df.resid 
  3941.9   4043.8  -1953.9   3907.9     2943 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-9.4911  0.0000  0.0000  0.5666  1.9143 

Random effects:
 Groups Name        Variance Std.Dev. Corr
 SUR.ID (Intercept)  0.33414 0.5781       
        Distance     0.09469 0.3077   1.00
 Day    (Intercept) 15.96629 3.9958       
Number of obs: 2960, groups:  SUR.ID, 20 Day, 6

Fixed effects:
                                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      3.20222    2.84984   1.124  0.26116    
Transmitter.depth               -0.35015    0.11794  -2.969  0.00299 ** 
Receiver.depth                  -0.57331    0.51919  -1.104  0.26949    
Water.temperature               -0.26595    0.11861  -2.242  0.02495 *  
Wind.speed                       1.31735    1.50457   0.876  0.38127    
TransmitterPT-04                -0.68854    0.08016  -8.590  &lt; 2e-16 ***
Distance                        -0.39547    0.09228  -4.286 1.82e-05 ***
HabitatFinger                   -0.23746    3.57783  -0.066  0.94708    
Replicate2                      -0.21559    0.08009  -2.692  0.00710 ** 
TransmitterPT-04:Distance       -0.27874    0.08426  -3.308  0.00094 ***
Transmitter.depth:HabitatFinger  0.73965    0.28612   2.585  0.00973 ** 
Receiver.depth:HabitatFinger     3.02083    0.74546   4.052 5.07e-05 ***
Wind.speed:TransmitterPT-04     -0.15540    0.06572  -2.364  0.01806 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Trnsm. Rcvr.d Wtr.tm Wnd.sp TrPT-04 Distnc HbttFn Rplct2 TPT-04: Tr.:HF Rc.:HF
Trnsmttr.dp -0.024                                                                               
Recevr.dpth -0.120 -0.267                                                                        
Watr.tmprtr  0.019 -0.159  0.007                                                                 
Wind.speed   0.130  0.073 -0.974  0.040                                                          
TrnsmtPT-04 -0.015  0.027  0.020 -0.018 -0.024                                                   
Distance     0.022 -0.080  0.151 -0.052 -0.141 -0.164                                            
HabitatFngr -0.813  0.010  0.241 -0.025 -0.253  0.009   0.029                                    
Replicate2  -0.067  0.033  0.377 -0.293 -0.394  0.010   0.085  0.103                             
TrnsPT-04:D -0.006  0.043 -0.007 -0.050 -0.003  0.516  -0.373  0.004  0.006                      
Trnsmtt.:HF  0.017 -0.352  0.021  0.055  0.049  0.026  -0.142  0.031 -0.088  0.025               
Rcvr.dpt:HF  0.103  0.189 -0.830  0.051  0.817 -0.036  -0.143 -0.224 -0.385 -0.003  -0.229       
Wnd.:TPT-04 -0.002  0.026 -0.015  0.003 -0.009  0.176  -0.114 -0.002  0.016  0.306  -0.008  0.014
</code></pre>

<p><img src=""http://i.stack.imgur.com/DFxyQ.png"" alt=""enter image description here""></p>

<p>A side question: I noticed a strong negative correlation between the intercept and a dichotome categorical predictor. I wonder if this causes any problems for my data analysis. All the covariates are centered and scaled for numerical stability during modelling.  </p>
"
"0.132686223108569","0.117469050619694","108088","<p>I have some elementary problems understanding the consequences of using/adding a lagged  dependent variable in my  predictive model.  Iâ€™m trying to predict values $Y_{i,t+\tau}$  for  $\tau=1-3$ with:</p>

<p>$Y_{i,t+1}=a+bY_{i,t}+cX_{i,t}+e_{i,t+1}$</p>

<p>$Y_{i,t+2}=a+bY_{i,t}+cX_{i,t}+e_{i,t+2}$</p>

<p>$Y_{i,t+3}=a+bY_{i,t}+cX_{i,t}+e_{i,t+3}$ </p>

<p>I already performed a pooled regression where you basically ignore individual firm effects and time-effects and treat every subject equally. As I am trying to forecast different levels (in USD) and my data appears to be extremely tailed as it covers a few extremely large subjects (with extremely high values) but also many small subjects the predictions of the model perform rather poor as the intercept $a$ that is equal for all subject seems largely responsible for this. A fixed model however with individual intercepts is not valid with Lagged dependent variables as the LDV is correlated with the within errors. To account for the heavy tailed errors I already estimated the pooled model with the rlm package (robust lm) that produced slightly better results but overall they appear still very unsatisfactory.</p>

<p>I further read that adding LDVâ€™s results in biased and inconsistent estimators as there is severe correlation between the predictor variables and the model errors and that regular procedures for autocorrelation are not valid anymore. One solution I came across is the use of Instrumental Variables with an Anderson-Hsiao Estimator (i.e using a lag -2 that is not correlated with the error term (with non-autocorrelation assumed but how can you assume no autocorrelation if you incorporate a lag?) Another one is the Arellano Bond GMM estimator, however applying GMM you have to set up moment conditions and I have no idea how to do that and I donâ€™t know exactly how this methods work. What I care about is to obtain an unbiased estimator with valid coefficients and not about standard errors as I donâ€™t do inference. Are there any other strategies to cope with LDVâ€™s I am currently unaware of and what is the best/ideal/easiest way to deal with such matter? Do you best take care of some issues while you ignore others (e.g. autocorrelation)? Iâ€™m a little bit lost here.</p>
"
"0.111013258946721","0.109201843342674","108562","<p>I have two time series. One is an environmental variable (<em>n</em> = 108) organized by year and month. The other is a biological variable, also organized by year and month, but I have no data for some months (<em>n</em> = 97). </p>

<p>I did a cross-correlation in R between these 2 times series, and used the <code>na.exclude</code> function for the biological variable to account for the missing values. I then fit a lagged regression, again accounting for the missing values. </p>

<p>However, I want to know:</p>

<ol>
<li><p>Could I do a cross-correlation by deleting these missing values (no data months) for the biological variable, which will leave me with unequal lengths? R still does a cross-correlation, but I am unsure if the results are valid.</p></li>
<li><p>Could I delete the corresponding months in the env. variable to have equal length time series, or is accounting for missing values the correct approach?</p></li>
</ol>

<p>Thanks.</p>
"
"NaN","NaN","109018","<p>I have n individuals with k possible financial products where they can put a % of their incomes. 
I need to predict for each individual the percentage of income that they allocate in each product.</p>

<p>Is there a way to model concurrently all the k percentages, exitimating also the correlation matrix?</p>

<p>In another work I used a Multivariate probit in Stata (Multivariate probit regression using simulated maximum likelihood, L.Cappellari 2004): it's the same thing that I'd like to have now, but in that case the Y have to be 0 or 1, now I have Y that is a rate between 0 and 1.</p>

<p>Suggestions?</p>
"
"0.0419590679148345","0.0412744171706498","109234","<p>I applied the DW test to my regression model in R and I got a DW test statistic of 1.78 and a p-value of 2.2e-16 = 0.  </p>

<p>Does this mean there is no autocorrelation between the residuals because the stat is close to 2 with a small p-value or does it mean although the stat is close to 2 the p-value is small and thus we reject the null hypothesis of there existing no autocorrelation?</p>
"
"NaN","NaN","110597","<p>My question is simple: is there a function in <code>R</code> which estimates the linear regresion model in a similar fashion as <code>lm</code>, but only using the means, variances, and covariance (correlations), i.e. the sufficient statistics? I am looking for a function to which I can input these statistics (plus sample size) and it returns regression coefficients and tests.</p>
"
"0.118678165819385","0.102148970948111","111287","<p>For a simulation study I have to generate data according to a trivariate mediation model (Baron &amp; Kennys causal step approach). Assuming $X$, $M$, and $Y$ are continuous variables of sample size $n$, the following regression equations constitute the data generating process: </p>

<p>$M = \alpha + aX + \epsilon$</p>

<p>$Y = \alpha + cX + bM + \epsilon$</p>

<p>Additionally, $X$ is not normally distributed, but is sampled from a distribution with skewness. I chose log-normal distribution in my current R implementation (see below). Sample size is also highly variable, starting at $n = 15$ and going up to $n = 15.000$.    </p>

<p>Here is my admittedly really primitive and inefficient code: </p>

<p>Sample from <code>rlnorm()</code> until a random sample with desired skewness is found (for clarification: <code>tol</code> refers to a tolerance parameter set to $.01$). </p>

<pre><code>library(moments)
s &lt;- 1 # desired skewness
tol &lt;- .01
n &lt;- 20
a &lt;- b &lt;- c &lt;- .59
skewness &lt;- sqrt(log(-1+2^(1/3)/(2+s^2+sqrt(4*s^2+s^4))^(1/3)+(2+s^2+sqrt(4*s^2+s^4))^(1/3)/2^(1/3)))

repeat{X &lt;- rlnorm(n=n, meanlog=0, sdlog=skewness)
    if (skewness(X) &gt; (skewness - tol) &amp;&amp; skewness(X) &lt; (skewness + tol)) {break}
} 
</code></pre>

<p>Then calculate $M$ and $Y$ from $X$ in a similar fashion and according to the regression equations above (where $a$, $b$ and $c$ are the correlations between the variables):</p>

<pre><code>repeat{
    M &lt;- a * X + rnorm(n)
    if (cor(X, M) &gt; (a - tol) &amp;&amp; cor(X, M) &lt; (a + tol)) {break}
}

repeat{
    Y &lt;- c * X + b * M + rnorm(n)
    if (cor(Y, X) &gt; (c - tol) &amp;&amp; cor(Y, X) &lt; (c + tol) &amp;&amp;
        cor(Y, M) &gt; (b - tol) &amp;&amp; cor(Y, M) &lt; (b + tol)) {break}
}
</code></pre>

<p>Again, this code is woefully ineffective and is only applicable for very low $n$, as the computation time is pretty low for small samples. Once higher sample sizes are reached (about $n=200$), it stops working altogether.  </p>

<p>So here are my actual questions:</p>

<ul>
<li>Is there a way to generate intercorrelated data ($X$, $M$ and $Y$) when $X$ isn't normally distributed and therefore <code>mvrnorm</code> from the <code>MASS</code> package can't be used? Also the correlation between the variables has to be either an exact value, or within a certain range ($\pm$ tolerance). </li>
<li>Assuming it can be done theoretically (which I am quite sure of), how can it be implemented in <code>R</code>?           </li>
</ul>
"
"0.103843395091962","0.102148970948111","111540","<p>I have two data sets whose structure is like this:</p>

<p><strong>DATA SET 1:</strong> </p>

<pre><code>        month_year  sales
 [1,]  ""Jan 2000""  ""30000""
 [2,]  ""Feb 2000""  ""12364""
 [3,]  ""Mar 2000""  ""37485""
 [4,]  ""Apr 2000""  ""2000""
 [5,]  ""Jun 2000""  ""7573""
          .     .      .
          .     .      .
</code></pre>

<p><strong>DATA SET 2:</strong></p>

<pre><code>          month_year    profit
     [1,] ""Jan 2000"" ""84737476""
     [2,] ""Jan 2000"" ""39450334""
     [3,] ""Jan 2000"" ""48384943""
     [4,] ""Feb 2000"" ""12345678""
     [5,] ""Feb 2000"" ""49595340""
     [6,] ""Jan 2001"" ""36769493""
              .     .      .
              .     .      .
</code></pre>

<p>As it can be seen the <strong>first data set has one sales value for each month of each year while in second data set I have <code>n</code>(say 100 but not constant all the time) number of values for each month of each year.</strong></p>

<p>Now I want to develop <strong>a predictive model</strong> which can provide <strong>predictions for profit for future coming months</strong>. What is the best approach to develop such a predictive model? I searched online and found that we can form <strong>a regression</strong> or correlation <strong>fit</strong> model and based on that can do predictions. Which way is better- Regression or correlation? And <strong>how to form a regression fit for such data sets</strong> where one data set has multiple data points for each month of each year while the other has a single data point for each month of each year?</p>

<p><strong>NOTE:</strong> I don't know whether it is the right approach or not but if we can get the regression/correlation value for like the month of Jan,Feb,Mar...and so on and then based on sales value we can do prediction of profit in general. I am new to this and hence I can lack in having certain concepts clear and thus request for some guidance for this issue.  </p>
"
"0.16250677125654","0.159855130326344","112646","<p>I am working on a project whose aim is to analyze the relationship between machine elements and their price. </p>

<p>My data consists of thousands of machine elements, their price, as well as technical and non-technical attributes. Most of the data is categorical</p>

<p>The technical data contains information such as weight, dimensions, material. Each element (identified by unique code) has distinct technical attributes. </p>

<p>The non-technical data contain information such as supplier name, contract type, INCOTERMS, first day of validity of the contract, last day of validity, minimum order quantity, etc. In some cases the prices depend on the order quantity.</p>

<p>One of the problems I am facing is that often each element has a different price depending on the financial data, i.e. element X will cost 100 dollars if Incoterms is A, 200 dollars if Incoterms is B, etc. In other words, there are rows that contain price information on the same element, but one of the columns has a different value and so the price is different.</p>

<p>In other cases the price is 100 if 50-100 elements are ordered, 80 if 100-200 elements are ordered, 60 if 200-500, etc. </p>

<p>I am planning to do some correlations as well as regression. I will probably also try data mining (using Rattle and R). </p>

<p>I need advice on how to treat the (rather) similar observations that each object has in case of correlation, regression and in general for data mining. Should I try to select a single observation per element or do the analysis for â€œall inâ€. I guess the last option wonâ€™t work for correlation at all. </p>

<p>Perhaps including a sample screenshot of the data with clarify my problem. So what is visible in the picture happens quite often: one object, different attributes, different price. I am wondering whether I should (try to) have only one row per object with only one set of attributes (we donâ€™t need to discuss the criteria) or whether I should keep the multiple rows for the same object when running a correlation analysis or a regression.</p>

<p>Thank you in advance. </p>

<p><img src=""http://i.stack.imgur.com/DEOqg.png"" alt=""enter image description here""></p>
"
"0.173269266447717","0.170442012695118","112670","<p>I'm working on a dataset with the following variables:</p>

<pre><code>Y:  a boolean telling whether the subject has experienced a seizure
ID:  the id of the subject
Sess:  the subject's session number (a subject has been observed multiple times)
X3:  numeric measurements of the subject's behavior during the session 
X4:  ""
X6:  ""
</code></pre>

<p>The idea is to see if the behavior measurements(X3,X4,X6) can be used predict the Seizure status (Y) in the population.  If there were just one session per subject I'd model the data with a logistic regression, but since each subject has multiple sessions the observations cannot be assumed independent.</p>

<p>It seems a GEE logistic model makes the most sense for this dataset, but I'm having trouble understanding the results when compared to a regular glm.  When introducing correlation structure to the GEE equation the signs of the coefficients are reversed, and the predicted values are opposite of what they would be with an independent correlation structure.</p>

<pre><code>library(""geepack"")
#regular logistic model
mod0 = glm(Y~X3+X4+X6, family=binomial(""logit""), data=mice)
#gee logistic model, independent correlation structure
mod1 = geeglm(Y~X3+X4+X6, id=ID, family=binomial(""logit""), corstr=""indep"", scale.fix=T, waves=Sess, data=mice)
#gee logistic model, exchangeable correlation structure
mod2 = geeglm(Y~X3+X4+X6, id=ID, family=binomial(""logit""), corstr=""exchangeable"", scale.fix=T, waves=Sess, data=mice)
</code></pre>

<p>As expected, the parameter estimates of the glm model(mod0) and the independent correlation gee model(mod1) are the same.  But when an exchangeable correlation structure(mod2) is introduced, the estimates are completely different and change sign.</p>

<pre><code>&gt; 
&gt; summary(mod1)                    

Call:
geeglm(formula = Y ~ X3 + X4 + X6, family = binomial(""logit""), 
    data = mice, id = ID, waves = Sess, corstr = ""indep"", scale.fix = T)

 Coefficients:
            Estimate Std.err  Wald Pr(&gt;|W|)    
(Intercept)   -1.494   0.459 10.59  0.00114 ** 
X3            -2.377   0.626 14.42  0.00015 ***
X4             1.090   0.398  7.50  0.00619 ** 
X6             1.233   0.403  9.36  0.00222 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Scale is fixed.

Correlation: Structure = independenceNumber of clusters:   28   Maximum cluster size: 4 
&gt; summary(mod2)

Call:
geeglm(formula = Y ~ X3 + X4 + X6, family = binomial(""logit""), 
    data = mice, id = ID, waves = Sess, corstr = ""exchangeable"", 
    scale.fix = T)

 Coefficients:
            Estimate Std.err Wald Pr(&gt;|W|)   
(Intercept)  -0.8928  0.4255 4.40   0.0359 * 
X3            0.1059  0.0383 7.64   0.0057 **
X4           -0.0427  0.0299 2.04   0.1535   
X6           -0.0528  0.0213 6.16   0.0131 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Scale is fixed.

Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha     1.09   0.133
Number of clusters:   28   Maximum cluster size: 4 
</code></pre>

<p>My biggest concern is that the predicted Y's of the two GEE models show opposite trends, i.e. behaviors that would predict a positive seizure status in mod1, predict a negative seizure status in mod2.    </p>

<pre><code>plot(mod1$fitted, mod2$fitted)
</code></pre>

<p><img src=""http://i.stack.imgur.com/nAogH.png"" alt=""scatter plot of fitted values from mod1, mod2""></p>

<p>Also, what's with the estimated correlation parameter of alpha = 1.09 in mod2?  Unless I'm interpreting this wrong, shouldn't this always fall between -1 and 1?</p>

<pre><code>Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha     1.09   0.133
</code></pre>

<p>It seems odd to me that the results are completely flipped depending on whether or not the observations are assumed to have dependence structure.  Can anyone else offer insight on this behavior?</p>

<p>Here is the data:</p>

<pre><code>&gt; dput(mice)
structure(list(Y = c(0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
), ID = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 
5L, 5L, 6L, 6L, 6L, 7L, 7L, 8L, 8L, 9L, 9L, 10L, 10L, 11L, 11L, 
11L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 15L, 
15L, 15L, 15L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 18L, 18L, 18L, 
19L, 19L, 19L, 20L, 20L, 20L, 21L, 21L, 21L, 22L, 22L, 22L, 23L, 
23L, 23L, 23L, 24L, 24L, 24L, 24L, 25L, 25L, 25L, 25L, 26L, 26L, 
26L, 26L, 27L, 27L, 27L, 27L, 28L, 28L, 28L, 28L), Sess = c(2L, 
3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 
4L, 3L, 4L, 3L, 4L, 3L, 4L, 3L, 4L, 1L, 3L, 4L, 1L, 3L, 4L, 1L, 
2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 2L, 
3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 1L, 2L, 3L, 1L, 2L, 
3L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 
4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L), X3 = c(0.511015060794264, 
0.898356533693696, 0.798280430157052, 1.31144372617517, 0.829923189452201, 
0.289089506643144, -0.763028944257538, -0.944459588217789, -1.16474609928919, 
-0.182524267014845, -0.338967193889711, -0.896037509887988, 0.00426073081308205, 
0.0576592603165749, -1.4984737260339, 1.34752684212464, 0.106461438449047, 
-0.108424579472268, -2.85991432039569, -0.230115838261355, -1.54479536845993, 
-1.23693649938367, -1.53704616612456, -1.04825100254239, 0.142768659484482, 
0.28358135516745, -0.236302896321009, 0.708743856986942, -0.507503006972081, 
0.401550711842527, -0.16928449007327, 0.867816722958898, 0.487459858572122, 
1.35172112260613, 0.14742652989871, 0.742155288774287, 0.348552056119878, 
-0.82489952485408, 0.0366834636917457, -0.731010479377091, 0.979544093857171, 
1.4161996712129, 0.661035838980077, 0.600235250313596, -1.10872641912335, 
-0.212101744145196, -0.919575135240643, -0.813993077336991, -0.547068540188791, 
-0.0260198210967738, -0.0962240349391501, -0.251025721625606, 
0.894913664382802, -0.21993004239326, 0.0628839847717805, 1.77763503559622, 
0.718459471596243, 0.984412886705251, 0.77603470471174, 0.486187732642953, 
1.78012655684609, 1.31622243756713, 1.29635178661133, 0.427995111986702, 
0.993748401511881, 0.387623239882247, 0.42006794384777, -0.815889182132972, 
-0.897540332229183, -1.041943103505, 0.379425827374942, -1.00707718576756, 
-0.889182530787803, 0.148432805676879, -0.287928359114935, -0.747152636892815, 
-1.41003790431546, -0.611571256991109, -1.02569548477235, -1.02700056733181, 
-1.45808867127733, -1.47973458605138, 2.23643966561508, 2.69397876103083, 
0.81841473415516, 2.12167589051282, 0.267133799544379, -0.326215175076418, 
-1.08788244901967, -1.18733017947214), X4 = c(0.050598970482242, 
-0.0279694583060402, 0.999225143631274, 0.199872317584803, 0.779316284168575, 
-0.3552692229881, -0.232161792808608, -0.333479851296274, -0.748169603107953, 
-0.57785843363913, -0.480747933235349, -0.740466500603612, -0.618559437949564, 
-0.591541699294345, -0.538855647639331, 0.431376763414175, -0.327931008191724, 
-0.469416282917978, -0.659224551441466, -0.55285236403596, -0.637082867133913, 
-0.780321541069982, -0.40539035027884, -0.54024676972473, -0.185562290173831, 
0.054439450703482, 0.624097793456316, 0.24018937319873, -0.264194638773171, 
-0.389590537012038, -0.42771343162755, -0.738790918078674, -0.122411831542788, 
0.600119921164627, 0.0442597161778152, -0.0955011351192086, -0.521259643827527, 
-0.550050365103255, -0.504566887441653, -0.506571005423286, 0.523650149759566, 
0.341920916685254, -0.396343801985993, -0.366532239883921, -0.739276449002057, 
-0.56054127343218, -0.587601788901296, -0.56798329186843, -0.454937006653748, 
-0.672730639942183, -0.564864467446687, -0.678853515419629, 0.573072971483937, 
0.596973680548765, 0.0403978228634349, 1.93617633381248, 2.54301964691615, 
0.363075891004736, 0.0205658396444095, 0.560923287570261, 1.24212005971229, 
2.32518793880728, 2.69979166871713, 0.626868716830008, 0.219463581391793, 
0.236477261174534, -0.115429539698909, -0.49754151674106, -0.40827433350644, 
-0.0433283703798658, -0.578451015506926, -0.714208713291922, 
-0.802387726290423, -0.836794085697031, -0.471800405613954, -0.668030208971065, 
-0.610945789491312, -0.780838257914176, -0.411360572155088, -0.494388869332376, 
-0.63231547268951, -0.743853022088574, 4.90627675753856, 3.38455016460328, 
0.859445571488139, 2.42212262705776, -0.324759764820016, -0.541581784452693, 
-0.485324968098865, -0.770539730529603), X6 = c(0.0150287583709043, 
-0.151984283645294, -0.347950002037732, 0.379891135882966, 0.129107019894704, 
-0.314047917638528, -0.516381047940779, -0.751192211830495, -0.884460389494645, 
-0.462363867892961, -0.397583161539858, -0.559528880497725, -0.842987555132397, 
-0.922797893301111, -1.01175722882932, 0.32346425626624, -0.610909601293237, 
-0.605155952259822, -1.29840867980623, 0.0793710626694382, -0.806959976634144, 
-0.674523251142452, -0.960113466801064, -0.783836535852452, -0.0665321645536412, 
0.482235339656537, -0.319499220427413, -0.115345965733089, -0.30806448545927, 
0.251747727063608, -0.305013811851957, -0.931916656036151, 0.415032839884745, 
0.337184728843034, 0.0584335852357015, -0.0712185313438638, 0.78632612201797, 
0.490831043388539, 0.8902425262631, 0.160088439571744, 0.90343086944952, 
0.928495373121098, -0.389259569427933, -0.304578433259833, -0.593364448723133, 
-0.411333868741105, -0.882691663964141, -0.91208274239495, -0.708633954450382, 
-0.339396626779965, -0.420927315080057, -0.421383857909298, 0.407474183771483, 
0.629710767351175, -0.438726438495567, 2.40977730548689, 2.47250810430208, 
0.783562677342961, 0.781304150896319, 0.563221804716475, 1.85514126067038, 
1.30723846671955, 1.94869625911545, 0.876751836832149, 0.626629859119409, 
0.067113945916172, 3.54280776301513, 0.0082773667305384, -0.311414481848668, 
-0.732779325538588, -0.594477082903005, -1.0385239418576, -1.04141739541776, 
-0.99472304141247, -0.599659297534257, -0.804801224448196, -1.13096932958525, 
-0.641957537144073, -0.722959119516237, -0.671146043591047, -0.714432955420477, 
-0.766750949574034, 1.20993739830475, 2.79011376379402, 2.64532317075082, 
2.54251033029822, -0.539516582572252, -0.6419726544563, -0.663768795224503, 
-0.644829826467113)), .Names = c(""Y"", ""ID"", ""Sess"", ""X3"", ""X4"", 
""X6""), row.names = c(NA, -90L), class = ""data.frame"")
</code></pre>
"
"0.10277830647413","0.101101261498861","113252","<p>I am so sorry, I am beginner in statistic analysis, I have project using R to analyze the correlation between dependent variables and independents variables. </p>

<p>In this case I have two dependent variables (1. Extrovert, 2. Introvert). 
And the independent variables i have the data from (Call Log-> how long they call everyday, how many they call everyday, SMS log-> how length text in SMS body every day, how many they sent/received sms for each day).</p>

<p>I am so confused how I can do it, please anyone can give me some good references about it. 
I also have some questions such as : </p>

<ol>
<li>I use the different type of variables, independent variables (data type : numeric) but dependent variable (data type is categorical), so it is possible to apply logistic regression and Pearson? </li>
<li>Or any someone will give me some advice the better solution such as another methods for solving this problem. </li>
</ol>

<p>The example of data from dput()</p>

<pre><code>structure(list(sumcallin = c(462L, 998L, 335L, 179L, 34L, 0L, 
0L, 0L, 0L, 0L), caountcallin = c(7L, 5L, 8L, 5L, 1L, 1L, 0L, 
1L, 1L, 1L), sumcallout = c(1068L, 81L, 519L, 393L, 342L, 0L, 
583L, 1902L, 358L, 1017L), countcallout = c(15L, 3L, 10L, 5L, 
6L, 0L, 3L, 3L, 3L, 3L), sumreceived = c(322L, 75L, 20L, 35L, 
8L, 35L, 135L, 103L, 471L, 173L), countreceived = c(15L, 4L, 
2L, 3L, 1L, 2L, 7L, 3L, 18L, 5L), sumsent = c(171L, 31L, 25L, 
23L, 8L, 55L, 87L, 9L, 400L, 258L), countsent = c(10L, 4L, 1L, 
3L, 1L, 3L, 4L, 1L, 13L, 8L), personality = structure(c(2L, 2L, 
2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L), .Label = c(""extro"", ""intro""), class = ""factor"")), .Names = c(""sumcallin"", 
""caountcallin"", ""sumcallout"", ""countcallout"", ""sumreceived"", 
""countreceived"", ""sumsent"", ""countsent"", ""personality""), row.names = c(1L, 
2L, 3L, 4L, 5L, 37L, 38L, 39L, 40L, 41L), class = ""data.frame"")
</code></pre>

<p>Thank you for your help.</p>
"
"0.0938233281301002","0.0922924025252575","113502","<p>I am trying to run a regression using about 80 independent variables. The problem is that the last 20+ coefficients return NA. If I condense the range of data to within 60, I get coefficients for everything just fine. Why am I getting these NAs and how do I resolve it? I need to reproduce this code using all of these variables.</p>

<pre><code>composite &lt;- read.csv(""file.csv"", header = T, stringsAsFactors = FALSE)
composite &lt;- subset(composite, select = -ncol(composite))
composite &lt;- subset(composite, select = -Date)
model1 &lt;- lm(indepvariable ~., data = composite, na.action = na.exclude)
</code></pre>

<p>composite is a data frame with 82 variables.</p>

<p>UPDATE:</p>

<p>What I have done is found a way to create an object that contains only the significantly correlated variables, to narrow the number of independent variables down. </p>

<p>I have a variable now: sigvars, which is the names of an object that sorted a correlation matrix and picked out only the variables with correlation coefficients >0.5 and &lt;-0.5. Here is the code:</p>

<pre><code>sortedcor &lt;- sort(cor(composite)[,1])
regvar = NULL

k = 1
for(i in 1:length(sortedcor)){
  if(sortedcor[i] &gt; .5 | sortedcor[i] &lt; -.5){
    regvar[k] = i
  k = k+1
 }
}
regvar

sigvars &lt;- names(sortedcor[regvar])
</code></pre>

<p>However, it is not working in my lm() function:</p>

<pre><code>model1 &lt;- lm(data.matrix(composite[1]) ~ sigvars, data = composite)
</code></pre>

<p>Error: Error in model.frame.default(formula = data.matrix(composite[1]) ~ sigvars,  : 
  variable lengths differ (found for 'sigvars')</p>
"
"0.0726752374667264","0.0714893875923587","114221","<p>I plotted normal probability plot in R using <code>qqnorm</code> and <code>qqline</code>. I want some help on:</p>

<ol>
<li>How to estimate ""probability"" that a data has normal distribution? (I read in a paper that a probability of 0.10 is required to assume that a data is normally distributed). </li>
<li>Also, how to calculate correlation coefficient for a normal probability plot in R?</li>
<li>Is the normality test valid for nonlinear regression too? (This might be a silly question, excuse me for that!)</li>
</ol>

<p>Thanks!</p>
"
"0.0839181358296689","0.0825488343412996","114441","<p>I'm playing around with the Abalone dataset in R and following along with <a href=""http://scg.sdsu.edu/linear-regression-in-r-abalone-dataset/"" rel=""nofollow"">this article</a>. </p>

<p>The dataset has 8 variables that are taken into account to predict the number of <code>rings</code>. To find the pairwise correlation, the blog post does this:</p>

<pre><code>as.matrix(cor(na.omit(abalone[,-1])))
</code></pre>

<p>and comes to conclusion that the data is heavily correlated. My question is how do they come to this conclusion? What information should I be looking for to come to this conclusion? </p>

<p>Here is the code </p>

<pre><code>&gt; aburl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'
&gt; abnames = c('sex','length','diameter','height','weight.w','weight.s','weight.v','weight.sh','rings')
&gt; abalone = read.table(aburl, header = F , sep = ',', col.names = abnames)
&gt; as.matrix(cor(na.omit(abalone[,-1])))

             length  diameter    height  weight.w  weight.s  weight.v weight.sh     rings
length    1.0000000 0.9868116 0.8275536 0.9252612 0.8979137 0.9030177 0.8977056 0.5567196
diameter  0.9868116 1.0000000 0.8336837 0.9254521 0.8931625 0.8997244 0.9053298 0.5746599
height    0.8275536 0.8336837 1.0000000 0.8192208 0.7749723 0.7983193 0.8173380 0.5574673
weight.w  0.9252612 0.9254521 0.8192208 1.0000000 0.9694055 0.9663751 0.9553554 0.5403897
weight.s  0.8979137 0.8931625 0.7749723 0.9694055 1.0000000 0.9319613 0.8826171 0.4208837
weight.v  0.9030177 0.8997244 0.7983193 0.9663751 0.9319613 1.0000000 0.9076563 0.5038192
weight.sh 0.8977056 0.9053298 0.8173380 0.9553554 0.8826171 0.9076563 1.0000000 0.6275740
rings     0.5567196 0.5746599 0.5574673 0.5403897 0.4208837 0.5038192 0.6275740 1.0000000
&gt; pairs(abalone[,-1]
</code></pre>

<p><img src=""http://i.stack.imgur.com/xD98j.png"" alt=""enter image description here""></p>

<p><strong>Questions</strong></p>

<ul>
<li>What is the problem when the data is heavily correlated?</li>
<li>How can one come to conclusion that the data is heavily correlated by looking at the matrix or by looking at the scatterplot? Is it because the scatterplot shows linear lines in almost every row-column?</li>
</ul>
"
"0.15699645640569","0.154434727891351","114728","<p>I'm trying to run a QAP logistic regression to predict the odds of a tie in a social network (represented as a binary adjacency matrix) given two independent variables (also binary matrices) but am getting opposite results depending on whether I run the analysis in R or UCINET.</p>

<p>All three matrices are rectangular (30 x 75). The 30 rows are people I've interviewed and the 75 columns are the entire population (including the 30 interviewees). All matrices include the person IDs as row and column names.</p>

<p>Running the analysis in R (see code at the bottom of the question), I get the following output:</p>

<pre><code>            Estimate  Exp(b)       Pr(&lt;=b) Pr(&gt;=b) Pr(&gt;=|b|)
(intercept) -5.298525  0.004998961 0.0001  0.9999  0.0001   
indep1       1.797138  6.032358591 0.9693  0.0307  0.2393   
indep2       3.194162 24.389724184 1.0000  0.0000  0.0030   
</code></pre>

<p>But after exporting the variables to .csv files and re-running it in UCINET, I get:</p>

<pre><code>                  1       2       3       4       5       6       7       8       9 
               Coef OddsRat     Sig      SD     Avg     Min     Max   P(ge)   P(le) 
            ------- ------- ------- ------- ------- ------- ------- ------- ------- 
1 Intercept  -3.931   0.020   0.000   0.540  -2.785  -3.931  -1.783       1   0.000 
2    indep1   2.391  10.925   0.003   1.508  -0.239  -5.089  15.437   0.003   0.998 
3    indep2   1.458   4.296   0.000   0.504  -0.026 -15.991   1.458   0.000       1 
</code></pre>

<p>Any ideas why this might be happening?</p>

<p>In case it's important, the (QAP) correlation coefficient between the two independent variables is 0.382</p>

<p>I've only included the 30 interviewees in the matrix rows because they are the only people from whom there might be a tie. The networks and QAP regressions are directed.</p>

<p>Incidentally, if I run the QAP logit using full 75x75 adjacency matrices (all people in the columns also appear as rows), I get the same output in both programs.</p>

<p>I also have a related question... A colleague suggested I could run the analysis using the 75x75 matrices but replace the rows of people I haven't interviewed with NAs. This gives me the same results in R and UCINET. Does this seem like a sensible approach, rather than using rectangular matrices?</p>

<p>Thanks!</p>

<p>Reproducible example in R:</p>

<pre><code>library(sna)

# row and column labels
rowIDs &lt;- c(""1"",  ""2"",  ""3"",  ""5"",  ""9"",  ""16"", ""18"", ""19"", ""26"", ""27"", ""34"", ""35"", ""36"", ""40"", ""46"", ""49"", ""60"", ""64"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""82"", ""85"", ""86"", ""97"", ""100"")
colIDs &lt;- c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""23"", ""26"", ""27"", ""34"", ""35"", ""36"", ""38"", ""40"", ""41"", ""43"", ""45"", ""46"", ""47"", ""49"", ""51"", ""52"", ""53"", ""57"", ""59"", ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", ""66"", ""67"", ""68"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""78"", ""79"", ""81"", ""82"", ""83"", ""84"", ""85"", ""86"", ""87"", ""88"", ""89"", ""91"", ""92"", ""93"", ""94"", ""95"", ""96"", ""97"", ""100"", ""101"")

# create matrices
adj.dep &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0""))

adj.indep1 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

adj.indep2 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

# assign row/column names
rownames(adj.dep) &lt;- rowIDs
colnames(adj.dep) &lt;- colIDs
rownames(adj.indep1) &lt;- rowIDs
colnames(adj.indep1) &lt;- colIDs
rownames(adj.indep2) &lt;- rowIDs
colnames(adj.indep2) &lt;- colIDs

# set up independent variables
g.indeps &lt;- array(dim=c(2, nrow(adj.indep1), ncol(adj.indep1)))
g.indeps[1,,] &lt;- adj.indep1
g.indeps[2,,] &lt;- adj.indep2

# run the analysis
# (warning, this command takes a bit of time to run with 10,000 reps)
nl &lt;- netlogit(adj.dep, g.indeps, reps=10000, nullhyp=""qap"")
# print output
summary(nl)
</code></pre>
"
"0.121125395777877","0.142978775184717","115356","<p>I'm a beginner in statistics and I have to run multilevel logistic regressions. I am confused with the results as they differ from logistic regression with just one level. </p>

<p>I don't know how to interpret the variance and correlation of the random variables. And I wonder how to compute the ICC.</p>

<p>For example : I have a dependent variable about the protection friendship ties give to individuals (1 is for individuals who can rely a lot on their friends, 0 is for the others). There are 50 geographic clusters of respondant and one random variable which is a factor about the social situation of the neighborhood. Upper/middle class is the reference, the other modalities are working class and underprivileged neighborhoods. </p>

<p>I get these results :</p>

<pre><code>&gt; summary(RLM3)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Arp ~ Densite2 + Sexe + Age + Etudes + pcs1 + Enfants + Origine3 +      Sante + Religion + LPO + Sexe * Enfants + Rev + (1 + Strate |  
    Quartier)
   Data: LPE
Weights: PONDERATION
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  3389.9   3538.3  -1669.9   3339.9     2778 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2216 -0.7573 -0.3601  0.8794  2.7833 

Random effects:
 Groups   Name           Variance Std.Dev. Corr       
 Neighb. (Intercept)     0.2021   0.4495              
          Working Cl.    0.2021   0.4495   -1.00      
          Underpriv.     0.2021   0.4495   -1.00  1.00
Number of obs: 2803, groups:  Neigh., 50

Fixed effects:
</code></pre>

<p>The differences with the ""call"" part is due to the fact I translated some words.</p>

<p>I think I understand the relation between the random intercept and the random slope for linear regressions but it is more difficult for logistics ones. I guess that when the correlation is positive, I can conclude that the type of neighborhood (social context) has a positive impact on the protectiveness of friendship ties, and conversely. But how do I quantify that ?</p>

<p>Moreover, I find it odd to get correlation of 1 or -1 and nothing more intermediate.</p>

<p>As for the ICC I am puzzled because I have seen a post about lmer regression that indicates that intraclass correlation can be computed by dividing the variance of the random intercept by the variance of the random intercept, plus the variance the random variables, plus the residuals. </p>

<p>But there are no residuals in the results of a glmer. I have read in a book that ICC must be computed by dividing the random intercept variance by the random intercept variance plus 2.36 (piÂ²/3). But in another book, 2.36 was replaced by the inter-group variance (the first level variance I guess). 
What is the good solution ?</p>

<p>I hope these questions are not too confused.
Thank you for your attention !</p>
"
"0.0839181358296689","0.0825488343412996","117867","<p>I have a basic linear regression model I fitted to a time series. Unfortunately I have to account for autocorrelation and heteroskedasicity in the model and I have done so with the NeweyWest function from the sandwich package in R while analyzing the coefficients. </p>

<p>Now I would like to create prediction intervals using the predict() function (or any other function) while utilizing the NeweyWest matrix/SEs.</p>

<p>As this is the first quesiton I post on here and my experinece in R is very limited here is some information:</p>

<pre><code>LMModel = lm(Return~Sentiment, data=Time Series)
</code></pre>

<p><strong>This is the function I used for my coefficient testing:</strong></p>

<pre><code>coeftest(LMModel , vcov=NeweyWest(LMModel , lag=27, ar.method=""ols""))
</code></pre>

<p><strong>I would like thsi function to use NeweyWest in some way:</strong></p>

<pre><code>predict(LMModel, newdata, interval = ""prediction"", level = 0.95) 
</code></pre>

<p>Thanks a lot in advance!</p>
"
"0.0629386018722517","0.0825488343412996","117910","<p>According to Wikipedia (source of all truth and knowledge...),
<a href=""http://en.wikipedia.org/wiki/Generalized_least_squares#Properties"" rel=""nofollow"">http://en.wikipedia.org/wiki/Generalized_least_squares#Properties</a></p>

<p>a weighted least square regression is equivalent to a standard least square regression, if the variables have been previously ""decorrelated"" using a Cholesky decomposition.</p>

<p>I made up then a very simple example with the function pgls from the package CAPER to test it, where the correlation arises from a phylogeny tree:</p>

<pre><code>tree.mod:
((A:0.2,(B:0.1,C:0.1):0.1):0.1,((E:0.1,F:0.1):0.1,D:0.2):0.1);
</code></pre>

<p>The two approaches are compared here:</p>

<pre><code>library(caper)

## Data
species = c(""A"",""B"",""C"",""D"",""E"",""F"")
gene = c(0.1,0.2,0.3,0.5,0.6,0.7)
pheno = c( 0,0,0,1,1,1)
data=data.frame(species,gene,pheno)

## Phylogeny
tree = read.tree( ""small/tree_small.mod"" )

## GLS regression
cat(""\n     ===&gt; GLS\n"")
cdata   = comparative.data( phy = tree, data = data, names.col = ""species"" )
res = pgls( pheno~gene, cdata )
print(summary(res))

## Cholesky
cat(""\n     ===&gt; Cholesky\n"")
corr = vcv( tree )
cholesky = chol( corr )
invCho = solve( cholesky )
data.gene =  invCho %*% as.vector( data$gene )
data.pheno =  invCho %*% as.vector( data$pheno )
res=lm( data.pheno ~ data.gene )
print(summary(res))
</code></pre>

<p>and yield the outputs:</p>

<pre><code>====&gt; GLS
Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -0.13000    0.27261 -0.4769  0.65834  
gene         1.63333    0.59489  2.7456  0.05161 .


=====&gt;Cholesky
Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  0.02214    0.28551   0.078    0.942  
data.gene    1.29188    0.35006   3.690    0.021 *
</code></pre>

<p>as you can see the results are different...</p>

<p>Does anyone have a clue why?</p>
"
"0.139648219234718","0.148817027473669","118394","<p>I have a dataset in which individuals were assessed at two time points during the study on a cognitive test, as such I was wondering which statistical model would be more appropriate for my data, either linear regression or mixed effects models? </p>

<p>The average length of follow up for my data is 59 months with a standard deviation of 43.03 (range is 0.63-167 months) with 88 (33%) of people having data for only one time point. </p>

<p>For linear regression, the approach I was thinking utilising was taking the delta of the test score between the two time points and regressing that against time (months between test scores). </p>

<p>If I used mixed effects models, the main issue I have is how to handle individuals who have only wave of data? While I know mixed effects models are especially robust in regards to the analysis of unbalanced data, would 33% missingnes cause issues?</p>

<p>Just sample R code highlighting the output using either linear regression or mixed models.</p>

<pre><code>fm1 &lt;- lm(mmse_difference ~ mmse_months_between*ORgrs_apoe, data = dat.wide)
summary(fm1)

Call:
lm(formula = mmse_difference ~ mmse_months_between * ORgrs_apoe, 
    data = newdat)

Residuals:
    Min      1Q  Median      3Q     Max 
-20.960  -3.957   1.854   5.200  12.550 

Coefficients:
                               Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)                    -2.74185    2.20667  -1.243    0.216
mmse_months_between            -0.01768    0.03051  -0.579    0.563
ORgrs_apoe                      0.35163    1.17782   0.299    0.766
mmse_months_between:ORgrs_apoe -0.01973    0.01748  -1.129    0.261

Residual standard error: 7.3 on 170 degrees of freedom
  (88 observations deleted due to missingness)
Multiple R-squared:  0.08481,   Adjusted R-squared:  0.06866 
F-statistic: 5.251 on 3 and 170 DF,  p-value: 0.001725
Num. obs. 174

fm2 &lt;- lme(mmse ~ mmse_months*ORgrs_apoe, random = ~mmse_months|patientid, data = dat.long, method = ""ML"", na.action = na.exclude)
summary(fm2)
Linear mixed-effects model fit by maximum likelihood
 Data: dat.long 
       AIC      BIC    logLik
  2797.467 2829.537 -1390.733

Random effects:
 Formula: ~mmse_months | patientid
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr  
(Intercept) 7.2972822 (Intr)
mmse_months 0.1132399 0.85  
Residual    2.9431616       

Fixed effects: mmse ~ mmse_months * ORgrs_apoe 
                           Value Std.Error  DF   t-value p-value
(Intercept)            24.635821 1.0959420 231 22.479130  0.0000
mmse_months            -0.069918 0.0223198 172 -3.132544  0.0020
ORgrs_apoe             -1.283348 0.6062892 231 -2.116726  0.0354
mmse_months:ORgrs_apoe -0.024952 0.0130561 172 -1.911103  0.0577
 Correlation: 
                       (Intr) mms_mn ORgrs_
mmse_months             0.438              
ORgrs_apoe             -0.882 -0.377       
mmse_months:ORgrs_apoe -0.357 -0.891  0.397

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-3.48643949 -0.31734164  0.07636708  0.26575764  2.49901891 

Number of Observations: 407
Number of Groups: 233 
</code></pre>

<p>Thanks.     </p>
"
"0.0839181358296689","0.0825488343412996","120518","<p>The goal of problem is to predict the weight for missing data .
I have a dataset of categorical type as shown below</p>

<pre><code>ideal_for,occasion,color,outer_material,inner_material,sole_material,closure,tip_shape,weight,other_details,PID,shoe_type
Men,Casual,""Black, Red"",,,,Laced,Round,350 gm,""Cushioned Ankle and Tongue, Textured Sole, Padded Footbed"",SHODR5ZRHRD2RXV7,sneakers
Men,Casual,Brown,,,,Laced,Round,294 gm,""Padded Footbed, Textured Outsole"",SHODQ467KMYXRB2F,sneakers
Men,Casual,Brown,,,,Laced,Round,373 gm,""Padded Footbed, Textured Sole"",SHODSM57F6ZHCCMT,sneakers
Men,Casual,""Navy, Yellow"",,,,Laced,Round,265 gm,""Padded Footbed, Textured Sole, Cushioned Ankle and Tongue"",SHODR3SQEFPF3FTE,sneakers
Men,Casual,Black,,,,Laced,Round,375 gm,""Textured Sole, Padded Footbed, Cushioned Ankle"",SHODS8SP6VDWMJFK,sneakers
Men,Casual,Deep Sea Blue,,,,Laced,Round,368 gm,""Padded Footbed, Textured Sole"",SHODUHJAHQTRE6YH,sneakers
</code></pre>

<p>As you can see we have empty field in data , I replaced this empty field by NA </p>

<p>then I converted this categorical to numerical like as below</p>

<pre><code>ideal_for   occasion    color   outer_material  inner_material  sole_material   closure tip_shape   weight  other_details   PID shoe_type
0   0   0   Black, Red  0   2   0   8   2   350.0   Cushioned Ankle and Tongue, Textured Sole, Padded Footbed   SHODR5ZRHRD2RXV7    76
</code></pre>

<p>eg 0 for men and 1 for women etc</p>

<p>the I applied polynomial regression to predict the weight of missing data,
Is this process correct to predict weight ,I am getting 0.0 correlation score while validating model. what is the best method to predict weight for such categorical dataset</p>
"
"0.118678165819385","0.116741681083556","122039","<p><strong>SOLVED</strong>: an elastic net model, as any other logistic regression model, will not generate more coefficients than input variables. Check Zach's answer to understand how from an (apparent) low number of inputs, more coefficients can be generated. The cause of this question was a code bug, as the users pointed out.</p>

<p>This is a simple question. I've fitted a model with 1334 variables using elastic net to perform feature selection and regularization. I'm now trying to interpret the obtained coefficients in order to find correlations between the input variables and the output. The only problem is that instead of the (expected) 1335 coefficients (intercept+1334), extracting the coefficients through <code>coef(model,s=""lambda.min"")</code> yields around 1390 coefficients. This seems highly counterintuitive and stops me from mapping a single coefficient to a single input variable, so I suppose I'm not understanding some of the insides of the elastic net. Any idea would be very helpful. Thanks in advance.</p>

<p>PS: just in case someone wonders so, I've not included interaction terms nor any synthetic variable, just the original 1334 ones.</p>

<p>PS2: elastic net references:</p>

<ul>
<li>Mathematical paper: <a href=""http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf"" rel=""nofollow"">http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf</a></li>
<li>R package tutorial: <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html</a></li>
</ul>

<p>PS3: about the code used to fit the model:</p>

<p>it is a 250 line script, so unless you specifically need it, I think it'd only clutter the question. Basically, the algorithm takes as an input a data frame of 1393 colums, where the last one is the target variable and the first 1392 are the input variables. So, after separating those into two matrices, input and output, the actual model fitting is done in this call:</p>

<p><code>cv.glmnet(x=input_matrix,y=output_matrix,family=""binomial"",type.measure=""auc"")</code></p>

<p>If you need to, I can actually generate a reproducible file with the data I use and the whole script. </p>
"
"0.121125395777877","0.131063877252658","123012","<p>I'm using <code>glmboost</code> in the mboost package to fit a boosted regression using linear models as the base learner. There are 13200 observations and about 75 variables, and I want to get a measure of the importance of each variable.</p>

<p>At the moment, I'm exploring the following two options:</p>

<ol>
<li><p>The boosted model object returned by <code>glmboost</code> includes information on the selection probabilities of the variables, ie how frequently they are selected by the boosting algorithm.</p></li>
<li><p>I can use <code>stabsel</code>, from the package of the same name, to identify the important variables. This uses a resampling approach to perturb the data, and the output is the frequency with which each variable appears in the resampled models (if I understand correctly).</p></li>
</ol>

<p>The problem is that these two methods are giving radically different results. This is the output from 1:</p>

<pre><code>&gt; summary(php.glmb)

         Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = p_hp ~ ., data = hdata.php.trn)

....

Selection frequencies:
degc238   etc48    etc7   per45   bar25   bar43    etc8   etc60   bar33   bar59
   0.28    0.17    0.07    0.06    0.05    0.05    0.05    0.05    0.04    0.03
  per15   per60   degc1   etc65   bar67 degc209    per5    per23   etc70 
   0.03    0.03    0.02    0.02    0.01    0.01    0.01     0.01    0.01 
</code></pre>

<p>And this is the output from 2:</p>

<pre><code>&gt; stabsel(php.glmb, cutoff=0.75, q=10)
        Stability Selection with unimodality assumption

Selected base-learners:
degc1  per5 per15 per45 per60  etc8 etc60 etc65 etc70 
   20    43    44    52    54    60    72    74    76 

Selection probabilities:
(Intercept)       bar25       bar26       bar28       bar29       bar32       bar42 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      bar43       bar45       bar46       bar49       bar50       bar59       bar60 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      bar62       bar63       bar66       degc2       degc3      degc16     degc109 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
    degc111     degc147     degc154     degc155     degc158     degc181     degc183 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
    degc204     degc205     degc206     degc209     degc229     degc231     degc238 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
    degc255     degc256     degc257     degc260       per21       per24       per34 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      per40       per42       per43       per58       per61       per63        etc5 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
       etc6        etc7       etc11       etc15       etc26       etc30       etc32 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      etc33       etc41       etc45       etc47       etc48       etc59       etc64 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      etc69       bar67       bar33       per23       per60       degc1        per5 
       0.00        0.15        0.18        0.71        0.96        1.00        1.00 
      per15       per45        etc8       etc60       etc65       etc70 
       1.00        1.00        1.00        1.00        1.00        1.00 
</code></pre>

<p>So the important variables are almost completely disjoint: method 1 says <code>degc238</code>, <code>etc48</code>, <code>etc7</code> and <code>per45</code> are the most important (have the highest selection probabilities), while method 2 says <code>etc70</code>, <code>etc65</code>, <code>etc60</code>, <code>etc8</code>, <code>per60</code> and so on. </p>

<p>What can be the reason for this? I should also mention that there's a significant amount of collinearity in this dataset; several predictors have univariate correlations of 90%+ with the response and with each other. Could this be impacting the result?</p>
"
"0.103843395091962","0.116741681083556","124214","<p>I build a prediction modeling using both regression and random forest.</p>

<pre><code>testmodel2&lt;-lm(y~as.matrix(xtest))
summary(testmodel2)

rf2&lt;-randomForest(y~.,data=df,importance=TRUE)
varImpPlot(rf2)
</code></pre>

<p>The regression model result shows that <code>t1, t10 and t11</code> are not significant. However, the <code>varImpPlot</code> show that they are pretty important. On the other side, <code>t3,t5 and t6</code> are significant in terms of P-value in the regression result, but they are not important in the Random forest result. </p>

<p>Is there any reason that linear regression result is different with random forest? Which one should be more reliable? The correlation matrix is also attached for the reference. The result of <code>backward step-wise variable selection</code> is also attached.</p>

<p><img src=""http://i.stack.imgur.com/G0h3k.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/lRnDt.png"" alt=""enter image description here""> </p>

<p><img src=""http://i.stack.imgur.com/8BzXW.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/I3zGF.png"" alt=""enter image description here""></p>
"
"0.118678165819385","0.116741681083556","124700","<p>I am trying to create a linear regression model containing two predictors and 1 response variable. My response variable has a short term pattern, i.e. surge during weekdays and slump during weekends and I suspect this pattern is a result of two things: 
1) A natural trend - people are more active on weekdays and 
2) Partially related to my independent variables which follows a similar pattern.</p>

<p>There is also lagged cross-correlation between predictor and response.</p>

<p>Should I take some steps to normalize the data before running a linear regression? I've been reading about detrending time series, ARIMA, moving averages etc. but am a little lost on the right approach. Attached below are are time series plots of the predictor and response and the lagged cross correlation.</p>

<p><img src=""http://i.stack.imgur.com/NNkB1.jpg"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/cxYFk.jpg"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/zBzmC.jpg"" alt=""enter image description here""></p>
"
"0.169968363404072","0.1847944414668","125414","<p>I have two variables:</p>

<ul>
<li>urban areas</li>
<li>protected areas.</li>
</ul>

<p>My observations are urban areas and protected areas in each year. But these observations are the cumulative ones, so observations in each variable have auto-correlation.</p>

<p>Can I use the general correlation such as yielded by <code>cor()</code> in R to measure the correlation between these two variables? If not, which indicator or method can I use?</p>

<p>I have the scatter plot: the horizontal variable is urban area in a specific year, and the vertical variable is another one in that specific year. And these two variables are increasing as years pass. I can see these two variables present a linear relationship. And my purpose is to find a indicator which can measure this linear relationship. I actually have tested the linear regression: the urban area as independent variable, the protected area as dependent variable, and I put 14 pairs of each year into the regression model, and the coefficients can pass the t-test, and model can pass the t-test, the $R^2$ can reach more than 0.9. </p>

<p>I want to research the relationship between urban development and protected area development. And the scatter plot below is urban and protected area pairs on global scale for 1950-2014 with 5 year intervals (except for 2010 and 2014).</p>

<p>I want to test two questions: First, are these two areas (urban and protected areas) both increasing over the research period? Second, does urbanization (here I mean the development of urban area) cause the development of protected areas?</p>

<p>I want to use some correlation analysis to solve the first question, such as correlation, linear regression or MIC value. However, because my data are time series, I'm not sure it can be used in the calculation of correlation? So I raise this question. In addition, I don't know other methods that could be used to measure strength of linear relationship between two time series. </p>

<p>And for the second question, I want to use Granger causality test to test the causality relationship between these two areas statistically. I know the result of Granger causality can't be sure to determine the causality relationship. And in my opinion, the reasons to improve the development of urban areas or protected areas are both complex, and some of them may be shared. At this level, I simply want to test the causality relationship between these two variables.</p>

<p><img src=""http://i.stack.imgur.com/tHlOm.jpg"" alt=""scatter plot between urban and farm land, the point is a variable pair in a specific year""></p>
"
"0.0951542219543327","0.109201843342674","126356","<p>For linear and parametric regression there are multiple tests where variables and residuals are used by means of performing a linear regression function to test serial correlation of regression errors and homocedasticity of regression errors. </p>

<p>My question is about non linear and non parametric regression for prediction or classification such us SVM, NeuralNets, knn, Recursive Partitioning, Adaptive Regression Spline, etc. </p>

<p>In this regard my questions are:</p>

<ol>
<li><p>As is not linear regression what is the equivalent of OLS assumptions for non linear non parametric regression. Are the consequences of OLS violation in the context of nonlinear and non-parametric regression still valid? </p></li>
<li><p>How could I test or what tests exist for serial correlation of errors for non linear and non parametric regression which are not derived from visual inspection of a graph and by using error residuals only. (something comes in mind like testing for significant acf or pacf on the residual errors - Unsure if this is OK).</p></li>
<li><p>How could I test or what tests exist for homocedasticity for non linear and non parametric regression which are not derived from visual inspection of a graph and by using error residuals only. (something comes too mind like homogenity of distances between the residual errors across time).</p></li>
<li><p>Would it be better to transfor the data into linear by seeking some adequate transformation as to avoid all the non linearity issues mentioned above? </p></li>
</ol>

<p>Thank you</p>
"
"0.151285570837612","0.148817027473669","129337","<h3>The out-of-context short version</h3>

<p>Let $y$ be a random variable with CDF
$$
F(\cdot) \equiv \cases{\theta &amp; y = 0 \\ \theta + (1-\theta) \times \text{CDF}_{\text{log-normal}}(\cdot; \mu, \sigma) &amp; y &gt; 0}
$$</p>

<p>Let's say I wanted to simulate draws of $y$ using the inverse CDF method. Is that possible? This function doesn't exactly have an inverse. Then again there's <a href=""http://stats.stackexchange.com/q/73028/36"">Inverse transformation sampling for mixture distribution of two normal distributions</a> which suggests that there is a known way to apply inverse transformation sampling here.</p>

<p>I'm aware of the two-step method, but I don't know how to apply it to my situation (see below).</p>

<hr>

<h3>The long version with background</h3>

<p>I fitted the following model for a vector-valued response, $y^i = \left( y_1 , \dots , y_K \right)^i$, using MCMC (specifically, Stan):</p>

<p>$$
\theta_k^i \equiv \operatorname{logit}^{-1}\left( \alpha_k x^i \right), \quad \mu_k^i \equiv \beta_k x^i - \frac{ \sigma^2_k }{ 2 } \\
F(\cdot) \equiv \cases{\theta &amp; y = 0 \\ \theta + (1-\theta) \times \text{CDF}_{\text{log-normal}}(\cdot; \mu, \sigma) &amp; y &gt; 0} \\
u_k \equiv F(y_k), \quad z_k \equiv\Phi^{-1}{\left( u_k \right)} \\
z \sim \mathcal{N}(\mathbf{0}, R) \times \prod_k f(y_k) \\
\left( \alpha, \beta, \sigma, R \right) \sim \text{priors}
$$</p>

<p>where $i$ indexes $N$ observations, $R$ is a correlation matrix, and $x$ is a vector of predictors/regressors/features.</p>

<p>That is, my model is a regression model in which the conditional distribution of the response is assumed to be a Gaussian copula with zero-inflated log-normal marginals. I've posted about this model before; it turns out that Song, Li, and Yuan (2009, <a href=""http://onlinelibrary.wiley.com/doi/10.1111/j.1541-0420.2008.01058.x/abstract"" rel=""nofollow"">gated</a>) have developed it and they call it a vector GLM, or VGLM. The following is their specification as close to verbatim as I could get it:
$$
f(\mathbf{y}; \mathbf{\mu}, \mathbf{\varphi}, \Gamma) = c\{ G_1(y_1), \dots, G_m(y_m) | \Gamma \} \prod_{i=1}^m g(y_i; \mu_i, \varphi_i) \\
c(\mathbf{u} | \Gamma) = \left| \Gamma \right|^{-1/2}\exp\left( \frac{1}{2} \mathbf{q}^T \left( I_m - \Gamma^{-1} \right) \mathbf{q} \right) \\
\mathbf{q} = \left( q_1, \dots, q_m \right)^T, \quad q_i = \Phi^{-1}(u_i)
$$
My $F_K$ corresponds to their $G_m$, my $z$ corresponds to their $\mathbf{q}$, and my $R$ corresponds to their $\Gamma$; the details are on page 62 (page 3 of the PDF file) but they're otherwise identical to what I wrote here.</p>

<p>The zero-inflated part roughly follows the specification of Liu and Chan (2010, <a href=""http://www.jstatsoft.org/v35/i11/paper"" rel=""nofollow"">ungated</a>).</p>

<p>Now I would like to simulate data from the estimated parameters, but I'm a little confused as to how to go about it. First I thought I could just simulate $y$ directly (in R code):</p>

<pre><code>for (i in 1:N) {
    for (k in 1:K) {
        Y_hat &lt;- rbinom(1, 1, 1 - theta[i, k])
        if (Y_hat == 1)
            Y_hat &lt;- rlnorm(1, mu[i, k], sigma[k])
    }
}
</code></pre>

<p>which doesn't use $R$ at all. I'd like to try to actually use the correlation matrix I estimated.</p>

<p>My next idea was to take draws of $z$ and then convert them back to $y$. This also seems to coincide with the answers in <a href=""http://stats.stackexchange.com/q/78894/36229"">Generating samples from Copula in R</a> and <a href=""http://stats.stackexchange.com/q/123698/36229"">Bivariate sampling for distribution expressed in Sklar&#39;s copula theorem?</a>. But what the heck is my $F^{-1}$ here? <a href=""http://stats.stackexchange.com/q/73028/36229"">Inverse transformation sampling for mixture distribution of two normal distributions</a> makes it sound like this is possible, but I have no idea how to do it.</p>
"
"0.139162484826546","0.136891755195648","131152","<p>Let say I've ran this linear regression:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt + vs, mtcars)
</code></pre>

<p>I can use <code>anova()</code> to see the amount of variance in the dependent variable accounted for by the two predictors:</p>

<pre><code>anova(lm_mtcars)

Analysis of Variance Table

Response: mpg
          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
wt         1 847.73  847.73 109.7042 2.284e-11 ***
vs         1  54.23   54.23   7.0177   0.01293 *  
Residuals 29 224.09    7.73                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Lets say I now add a random intercept for <code>cyl</code>:</p>

<pre><code>library(lme4)
lmer_mtcars &lt;- lmer(mpg ~ wt + vs + (1 | cyl), mtcars)
summary(lmer_mtcars)

Linear mixed model fit by REML ['lmerMod']
Formula: mpg ~ wt + vs + (1 | cyl)
   Data: mtcars

REML criterion at convergence: 148.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.67088 -0.68589 -0.08363  0.48294  2.16959 

Random effects:
 Groups   Name        Variance Std.Dev.
 cyl      (Intercept) 3.624    1.904   
 Residual             6.784    2.605   
Number of obs: 32, groups:  cyl, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  31.4788     2.6007  12.104
wt           -3.8054     0.6989  -5.445
vs            1.9500     1.4315   1.362

Correlation of Fixed Effects:
   (Intr) wt    
wt -0.846       
vs -0.272  0.006
</code></pre>

<p>The variance accounted for by each fixed effect now drops because the random intercept for <code>cyl</code> is now accounting for some of the variance in <code>mpg</code>:</p>

<pre><code>anova(lmer_mtcars)

Analysis of Variance Table
   Df  Sum Sq Mean Sq F value
wt  1 201.707 201.707 29.7345
vs  1  12.587  12.587  1.8555
</code></pre>

<p>But in <code>lmer_mtcars</code>, how can I tell what proportion of the variance is being accounted for by <code>wt</code>, <code>vs</code> and the random intecept for <code>cyl</code>?</p>
"
"0.0839181358296689","0.0619116257559747","131349","<p>Following Dormann et al 2007 Ecography, I have employed a GLMM approach in R to account for spatial autocorrelation in a binomial regression model (logistic regression) that does not have random terms. Using data from mtcars (just so we all have the same numbers), my code looks as follows:</p>

<pre><code>library(MASS)
data &lt;- mtcars
data$group &lt;- factor(rep(""A"", nrow(data)))
mod1 &lt;- glmmPQL(vs ~ mpg, random=~1|group, correlation=corExp(form=~disp+qsec),
        data=data, family=binomial)
</code></pre>

<p>Question 1: Is this a reasonable way to account for spatial autocorrelation in a binomial model, as Dormann et al suggest?</p>

<p>Question 2: Should/can I demonstrate that there is no longer spatial autocorrelation (SAC) in the error for this model? Can a vario/correlagram be used? How? I have had trouble finding information about how to look for SAC in GLMMs, and the variogram I've managed to create looks very funky -- hard to interpret.</p>
"
"0.0839181358296689","0.0825488343412996","133488","<p>I am trying to run regression on financial data in R. I am new to regression analysis so I am finding it to difficult to interpret certain scenarios. I have the code as follows:</p>

<pre><code>#regression analysis
fit &lt;- lm(fiveMinReturns~RegressionData, data=maindata)
summary(fit) # show results
#correlation
cor(maindata$fiveMinReturns,maindata$RegressionData,use=""everything"")
</code></pre>

<p>My output is: </p>

<pre><code>Call:
lm(formula = fiveMinReturns ~ RegressionData, data = maindata)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.205790 -0.001144 -0.000062  0.001117  0.156418 

Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    6.346e-05  8.785e-06   7.223 5.09e-13 ***
RegressionData 1.597e-07  1.432e-08  11.155  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.004035 on 210912 degrees of freedom
Multiple R-squared:  0.0005896, Adjusted R-squared:  0.0005849 
F-statistic: 124.4 on 1 and 210912 DF,  p-value: &lt; 2.2e-16

cor(maindata$fiveMinReturns,maindata$RegressionData,use=""everything"")
[1] 0.02428219
</code></pre>

<p>p-value is very small that means two variables are tightly coupled, but correlation is small too.
My question is how do I evaluate this situation?
Can we say that this equation will give correct results almost every time?
Which scenario suggests both p-value and correlation both to be really small?
What measures should i take to improve the result? </p>
"
"0.139648219234718","0.148817027473669","134499","<h2>Can anyone explain How I can interpret my result from the below model :</h2>

<p>I am trying to build the linear regression model for finding the transaction behavior of the customer in bank accounts. I have created the below model and done the nvcTest for the same. I need some one help to summary the result of this model.</p>

<pre><code>&gt; str(trans_data)
'data.frame':   8597 obs. of  20 variables:
 $ cust_id          : int  1 1 1 1 1 1 1 1 1 1 ...
 $ acc_type_id      : int  3 3 3 3 3 3 3 3 3 3 ...
 $ loc_id           : int  260 144 362 321 114 331 343 17 345 284 ...
 $ tx_type          : Factor w/ 2 levels ""CR"",""DB"": 1 1 1 2 1 1 1 2 2 2 ...
 $ total_bal        : num  8.36e+08 8.70e+08 9.69e+08 8.93e+08 9.60e+08 ...
 $ age              : int  30 30 30 30 30 30 30 30 30 30 ...
 $ gender           : Factor w/ 2 levels ""F"",""M"": 2 2 2 2 2 2 2 2 2 2 ...
 $ city             : chr  ""New Orleans"" ""New Orleans"" ""New Orleans"" ""New Orleans"" ...
 $ county           : chr  ""Orleans"" ""Orleans"" ""Orleans"" ""Orleans"" ...
 $ state            : chr  ""LA"" ""LA"" ""LA"" ""LA"" ...
 $ category         : Factor w/ 6 levels ""HNI"",""LNI"",""MNI"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ employment_status: Factor w/ 3 levels ""SAL"",""SEL"",""UNE"": 1 1 1 1 1 1 1 1 1 1 ...
 $ marital_status   : Factor w/ 2 levels ""MARRIED"",""UNMARRIED"": 1 1 1 1 1 1 1 1 1 1 ...
 $ branch_num       : int  1 1 1 1 1 1 1 1 1 1 ...
 $ acc_type         : Factor w/ 3 levels ""current"",""savings"",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ acc_sub_type     : Factor w/ 6 levels ""eqty"",""fd"",""gold"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ yyyy             : int  2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...
 $ mm               : int  1 1 1 1 1 1 1 1 1 1 ...
 $ dd               : int  3 4 8 9 11 12 13 17 21 22 ...
 $ tx_amt           : num  78626273 33171055 98937915 75726140 67162109 ...


model1&lt;-lm(formula = tx_amt ~ tx_type + total_bal +   age + gender + category + employment_status + marital_status + acc_type + 
acc_sub_type + yyyy + mm + dd , data=trans_data)

&gt; summary(model1)

Call:
lm(formula = tx_amt ~ tx_type + total_bal + age + gender + category + 
    employment_status + marital_status + acc_type + acc_sub_type + 
    yyyy + mm + dd, data = trans_data)

Residuals:
      Min        1Q    Median        3Q       Max 
-54030777 -24870155    230953  24933123  52907029 

Coefficients: (2 not defined because of singularities)
                          Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)              1.901e+09  1.161e+09   1.638   0.1015  
tx_typeDB               -6.713e+05  6.245e+05  -1.075   0.2824  
total_bal                1.118e-05  3.877e-04   0.029   0.9770  
age                     -7.315e+03  8.957e+04  -0.082   0.9349  
genderM                  2.784e+05  3.258e+06   0.085   0.9319  
categoryLNI             -1.648e+04  3.142e+06  -0.005   0.9958  
categoryMNI             -3.674e+05  2.095e+06  -0.175   0.8608  
categoryNNI              8.522e+05  5.419e+06   0.157   0.8751  
categoryXXX              2.184e+06  2.425e+06   0.901   0.3677  
categoryYYY             -1.949e+06  1.469e+06  -1.327   0.1847  
employment_statusSEL    -1.219e+06  2.755e+06  -0.443   0.6581  
employment_statusUNE     1.606e+06  1.551e+06   1.035   0.3006  
marital_statusUNMARRIED         NA         NA      NA       NA  
acc_typesavings         -1.882e+06  1.192e+06  -1.580   0.1143  
acc_typesecurity         1.909e+05  5.429e+06   0.035   0.9719  
acc_sub_typefd          -1.019e+07  1.288e+07  -0.791   0.4292  
acc_sub_typegold         9.310e+05  1.522e+06   0.612   0.5406  
acc_sub_typemutual_fnd  -1.245e+06  1.041e+06  -1.196   0.2317  
acc_sub_typenormal       1.446e+05  5.311e+06   0.027   0.9783  
acc_sub_typerd                  NA         NA      NA       NA  
yyyy                    -9.185e+05  5.763e+05  -1.594   0.1110  
mm                          -1.904e+05  8.972e+04  -2.123   0.0338 *
dd                       2.334e+04  3.547e+04   0.658   0.5105  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 28850000 on 8576 degrees of freedom
Multiple R-squared:  0.003322,  Adjusted R-squared:  0.0009977 
F-statistic: 1.429 on 20 and 8576 DF,  p-value: 0.09664

library(car)

durbinWatsonTest(model1)
 lag Autocorrelation D-W Statistic p-value
   1   -0.0008975802      2.001674   0.916
 Alternative hypothesis: rho != 0

ncvTest(model1)
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.132947    Df = 1     p = 0.7153958

model4&lt;-lm(formula = tx_amt ~ cust_id + acc_type_id + loc_id + tx_type + total_bal +   age + gender + category + employment_status + marital_status + acc_type + 
acc_sub_type + yyyy + mm + dd , data=trans_data)

 summary(model4)

Call:
lm(formula = tx_amt ~ cust_id + acc_type_id + loc_id + tx_type + 
    total_bal + age + gender + category + employment_status + 
    marital_status + acc_type + acc_sub_type + yyyy + mm + dd, 
    data = trans_data)

Residuals:
      Min        1Q    Median        3Q       Max 
-54109589 -24870576    214972  24958230  52899612 

Coefficients: (4 not defined because of singularities)
                          Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)              1.891e+09  1.161e+09   1.629   0.1033  
cust_id                  1.915e+05  1.822e+05   1.051   0.2933  
acc_type_id             -4.634e+04  2.656e+06  -0.017   0.9861  
loc_id                  -1.941e+03  2.173e+03  -0.893   0.3718  
tx_typeDB               -6.715e+05  6.245e+05  -1.075   0.2824  
total_bal                1.602e-05  3.877e-04   0.041   0.9670  
age                      9.159e+04  9.163e+04   1.000   0.3175  
genderM                 -4.463e+06  4.155e+06  -1.074   0.2828  
categoryLNI             -2.748e+06  3.170e+06  -0.867   0.3860  
categoryMNI             -2.091e+06  2.305e+06  -0.907   0.3644  
categoryNNI             -6.203e+06  6.973e+06  -0.890   0.3737  
categoryXXX             -1.115e+06  2.649e+06  -0.421   0.6738  
categoryYYY             -1.290e+06  1.242e+06  -1.039   0.2989  
employment_statusSEL    -4.898e+06  2.978e+06  -1.645   0.1001  
employment_statusUNE            NA         NA      NA       NA  
marital_statusUNMARRIED         NA         NA      NA       NA  
acc_typesavings         -1.708e+06  1.084e+07  -0.158   0.8747  
acc_typesecurity         1.317e+05  5.429e+06   0.024   0.9807  
acc_sub_typefd          -1.049e+07  1.209e+07  -0.868   0.3856  
acc_sub_typegold         9.841e+05  3.047e+06   0.323   0.7467  
acc_sub_typemutual_fnd  -1.282e+06  2.874e+06  -0.446   0.6557  
acc_sub_typenormal              NA         NA      NA       NA  
acc_sub_typerd                  NA         NA      NA       NA  
yyyy                    -9.124e+05  5.764e+05  -1.583   0.1134  
mm                      -1.905e+05  8.972e+04  -2.124   0.0337 *
dd                       2.302e+04  3.548e+04   0.649   0.5164  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 28850000 on 8575 degrees of freedom
Multiple R-squared:  0.003415,  Adjusted R-squared:  0.0009741 
F-statistic: 1.399 on 21 and 8575 DF,  p-value: 0.1055

ncvTest(model4)
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.08904766    Df = 1     p = 0.7653914
</code></pre>
"
"0.0938233281301002","0.0922924025252575","134837","<p>My data has a binary outcome (attack or not attack), day (20 day in repeated measured design) and some covariates (nestlingâ€™s movement).
The objectives of my experiment are testing the effect of time and other factors and selecting useful variables affecting outcomes.</p>

<p>My data look like below</p>

<pre><code>subject outcome Day nestling.move
   1        A    1      N 
   2        A    1      Y 
   3        A    1      Y 
   4        N    1      Y 
   5        N    1      Y 
   6        N    1      Y 
   7        N    1      Y 
   8        N    1      N 
   9        N    1      N 
   .        .    .      . 
   .        .    .      . 
   1        A    20     N 
   2        A    20     N   
</code></pre>

<p>First of all, I simply transformed outcomes to ratios(attack rate for each day) and test if there is a correlation between attack rates and days by using Spearmanâ€™s rank correlation. But I think it is not a good way to test the effect of time on outcome.</p>

<p>I checked other <a href=""http://stats.stackexchange.com/questions/81246/unable-to-fit-repeated-measures-in-r"">post</a>. and I think I should used an AR1 model with logistic regression since it could be a time-varying processes. However, I don't know how to do this with R or SPSS. </p>

<p>Is this the correct syntax to use in R?</p>

<pre><code>model&lt;- glmmPQL(outcome ~  nestling.move + Day, data=mydata, family=binomial,  random = ~ 1 | subject, correlation = corAR1(form=~Day|subject)) 
</code></pre>
"
"0.111013258946721","0.109201843342674","135847","<p>After you decompose a univariate time series with stl() function in R you are left with the trend, seasonal and random components of the time series. Is it valid to use those components to then model the original timer series with additional other variables?</p>

<p>For example:</p>

<pre><code>&gt; tsData
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2012  22  26  34  33  40  39  39  45  50  58  64  78
2013  51  60  80  80  93 100  96 108 111 119 140 164
2014 103 112 154 135 156 170 146 156 166 176 193 204

&gt; stl(tsData, s.window = ""periodic"")
 Call:
 stl(x = tsData, s.window = ""periodic"")

Components
            seasonal     trend   remainder
Jan 2012 -24.0219753  36.19189   9.8300831
Feb 2012 -20.2516062  37.82808   8.4235219
Mar 2012  -0.4812396  39.46428  -4.9830367
Apr 2012 -10.1034302  41.32047   1.7829612
...
Sep 2014   2.2193527 165.55136  -1.7707170
Oct 2014   7.3239448 169.33893  -0.6628760
Nov 2014  18.4285405 173.12650   1.4449614
Dec 2014  30.5244146 176.84390  -3.3683103
</code></pre>

<p>Now if I wanted to model the time series with a linear model with some other variables is it valid to do so?</p>

<pre><code>lm(index ~ trend + seasonal + s1 + s2, data)
</code></pre>

<p>When I run that model I get an R-squared = .98 which make sense considering that the original time series index is just the sum of trend + season + error. I guess what I'm concerned about is using a linear model with time series data I want to make sure I'm not violating some major rules of linear regression. I figure since I have the seasonal variable I'm essentially controlling for that element and hopefully reducing the auto correlation or am I since the R-squared is so high? Any help is appreciated!</p>
"
"0.0979044918012804","0.0825488343412996","135852","<p>I'm trying to replicate an analysis done in Stata with R that involves calculating the autocorrelation for a particular outcome measured in many different areas. I've already run a linear regression on the data, which produced residuals for the outcome of interest. While I can't post my actual data, here is what it sort of looks like:</p>

<pre><code>year&lt;-c('2003', '2004', '2005','2003', '2004', '2005','2003', '2004', '2005')
location&lt;-c(rep('North', 3), rep('South',3), rep('West',3))
resid &lt;-c(-2.42, -3.563, -2.112, -0.543, 2.391, -1.556, -0.177, 0.983, 1.225)
mydata&lt;-data.frame(location,year, resid)
mydata

  location year resid
1    North 2003  -2.420
2    North 2004  -3.563
3    North 2005  -2.112
4    South 2003  -0.543
5    South 2004   2.391
6    South 2005  -1.556
7     West 2003  -0.177
8     West 2004   0.983
9     West 2005   1.225
</code></pre>

<p>I'm interested in the autocorrelation in the residuals, so I'm running the following:</p>

<pre><code> myacf &lt;-acf(mydata$resid, plot=F)
</code></pre>

<p>I'm not getting the same autocorrelation values as I got with Stata. I'm wondering if I should be specifying somehow that my data aren't a continuous time series from t=1 to t=9, but rather 3 sets of 3 time points that were measured in different locations (so really there can only be lag of 1 or 2, at least in the toy data above). </p>

<p>Also, above, I've made it so every region has 3 years of outcomes, but in my real (more complicated) dataset, some regions might have more rows than others (e.g., North might have 7 rows for 2000-2006, but South might only have 5 because we missed the outcome in 2003 and 2006 so there are no residuals for those 2 years, etc.). Any help would be appreciated in understanding whether acf() is the right way to approach this problem or not, and if so, what I might consider changing.</p>
"
"0.0419590679148345","0.0412744171706498","136071","<p>I am hoping someone can check this code to ensure that I have interpreted the various pieces of PCA correctly. I am trying to figure out a way to identify the leading contributors to the performance of multiple securities. For example, one idea I had was to run a multivariate regression using the securities returns as dependent variables and include things like oil, the dollar, the euro, treasury yields, etc.
E.g.,
SBUX + AAPL + MCD + BAC + TWTR = intercept + oil + dollar + euro + steel + gold + e</p>

<p>I then thought that PCA would probably be better suited for this type of exercise. Here is my code from R. The csv file consists of a matrix of 900 securities and 30 rows (30 daily returns for 900 securities)</p>

<pre><code>FD &lt;- read.csv(""U:/Personal Projects/R/Data Files/FD Securities Jan 2015.csv"")

#Removes columns with any na values
FD1 &lt;- FD[, sapply(FD, function(x) !any(is.na(x)))]

#removes ""zero/constant variance"" columns, which I think are NaNs I couldnt erase using is.nan
FD2 &lt;- FD1[,apply(FD1, 2, var, na.rm=TRUE) != 0]

#Calc PCs using singe value decomposition (prcomp). Should data be a correlation matrix of the variables? I get reasonable looking results both ways, i.e., PC1 explains 30-50% of variance, PC2 ~10%-15%, etc.
FD2.pca &lt;- prcomp(cor(FD2), retx = TRUE, scale = TRUE)
summary(FD2.pca)
plot(FD2.pca)

#These are the 'loadings', i.e., coefficients used for each linear combination?
as.matrix(FD2.pca$rotation[,1])

#I think these ""scores"" are the coefficients of interest, as they incorporate the factor     weightings because the output is pca$rotation * scale (stddev of each factor)
as.matrix(FD2.pca$x[,1]) as.matrix(FD2.pca$x[,2]) as.matrix(FD2.pca$x[,3])     as.matrix(FD2.pca$x[,4])

#Scatterplot of the first two principal components. Not sure if this is right of if $x should be used.
plot(x = FD2.pca$rotation[,1], FD2.pca$rotation[,2], xlab = ""PC1"", ylab = ""PC2"", main=""Principal Component Analysis:"")
</code></pre>
"
"0.103843395091962","0.116741681083556","136925","<p>I have some short grouped time series data. I would like to fit a dynamic multilevel regression model in R, with random coefficients for the mean and first order auto-correlation in each group, and with no cross correlation between the two variance parameters; i.e. this model:</p>

<p>$y_{i,t} - \mu_{i} = \rho_{i} (y_{i,t-1} - \mu_{i}) + \epsilon_{i,t}$</p>

<p>$\mu_{i} = \mu + v_{\mu}$ </p>

<p>$\rho_{i} = \rho + v_{\rho}$ </p>

<p>$\epsilon_{i,t} \sim N(0,\sigma_\epsilon^2), v_{\mu} \sim N(0,\sigma_\mu^2), v_{\rho} \sim N(0,\sigma_\rho^2)$ </p>

<p>The best I can do so far in R is:</p>

<pre><code>library(nlme)
library(dplyr)

#create toy data set
df0 &lt;- Orthodont %&gt;% 
  group_by(Subject) %&gt;% 
  mutate(lag1=lag(distance)) %&gt;% 
  filter(!is.na(lag1))

#multilevel model, mean (not Subject specific mean) centered
m1 &lt;- lme(fixed = distance ~ I(lag1 - mean(distance)), data=df0, 
          random= list(Subject = pdDiag(~ + I(lag1 - mean(distance)))) )
m1
# Linear mixed-effects model fit by REML
#   Data: df0 
#   Log-restricted-likelihood: -164.8976
#   Fixed: distance ~ I(lag1 - mean(distance)) 
#              (Intercept) I(lag1 - mean(distance)) 
#               25.7907622                0.8289975 
# 
# Random effects:
#  Formula: ~+I(lag1 - mean(distance)) | Subject
#  Structure: Diagonal
#          (Intercept) I(lag1 - mean(distance)) Residual
# StdDev: 7.892818e-05                0.3031237 1.675277
# 
# Number of Observations: 81
# Number of Groups: 27 
</code></pre>

<p>This 1) does not estimate ($\mu$) and 2) centres the distance lag ($y_{i,t-1}$) on the population mean ($\mu$) rather than the subject ($i$) specific mean ($\mu_i$) that I desire. </p>
"
"0.0593390829096927","0.0583708405417779","138671","<p>I was doing logistic regression in R on 'Smarket' data set available in the ISLR library.
Since correlation between variables were less, I used all variables in my model and I was getting the following result</p>

<p><img src=""http://i.stack.imgur.com/esOCA.png"" alt=""Logistic Regression Output""></p>

<p>Here p -values are greater then 0.05 for all the variables.</p>

<p>Then I checked the misclassification error and the error was almost zero.</p>

<p><img src=""http://i.stack.imgur.com/tF8iq.png"" alt=""Misclassification Output""></p>

<p>I have tried removing some variables. But still the p values are greater than 0.05 and the misclassification error rates are high for those models.</p>

<p>Can I use this model ??
Is p-value insignificant in logistic regression?</p>
"
"0.0484501583111509","0.0714893875923587","139928","<p>how to simulate data (in R) to generate , sample values
1) variables with specific correlation values for a particular model AND 
2) with predefined regression coefficients? 
3) Can we also set the mean and SD in the same process? 
4) Also how does one simulate the p value/significance of the variable. </p>

<p>This is for imitating existing models for analysis and teaching purposes</p>

<p>Sorry for not being specific : this is for multiple regression, sample values. I would like to specify the mean and SD if possible (apparently not, I can specify only one in order to specify the regression coefficients?)</p>

<p>Thanks for the help.</p>
"
"0.125877203744503","0.123823251511949","140622","<p>In the past I've run separate multiple regression models for <strong>many correlated independent variables</strong> and <strong>one dependent variable</strong>. For this I've been using the R package multtest (<a href=""http://www.bioconductor.org/packages/release/bioc/html/multtest.html"" rel=""nofollow"">http://www.bioconductor.org/packages/release/bioc/html/multtest.html</a>). This allowed me to compute adjusted p-values that took the correlation structure of my matrix of independent variables into account.</p>

<p>Now, I want to do the same thing but for <strong>several dependent variables</strong> as well. Put differently, I have a matrix Y with my dependent variables and a matrix X with my independent variables. From this I want to estimate <strong>X*Y regression models</strong>. Importantly, I want the <strong>adjusted p-values</strong> to account for the correlation structure of both the independent and the dependent variables. I'm looking forward to your suggestions. I've suggested to the authors of multtest before to extend their library to accomodate this case but this hasn't happened yet.</p>

<p><strong>Example</strong> (added):
Let's say I have gene expression data from 10 different tissues. Now I want to know if gene expression is correlated with a 100 different SNPs. This means I'm effectively testing 100*10 = 1000 hypothesis. However, all these hypothesis are not independent of each other. The SNPs might be correlated to each other due to linkage disequilibrium and gene expression might also be correlated accross different tissues, depending on their similarity. Therefore a Bonferroni correction of my p-values for this 1000 statistical tests would be too conservative. I'm looking for a way to <em>derive adjusted p-values that accounts for the above described dependencies within both the independent and the dependent variables</em>. </p>
"
"0.0726752374667264","0.0714893875923587","141119","<p>I have to verify that on two variables, $X_t$ and $Y_t$ hold the followings: $Y_t=\beta \times X_t+\varepsilon_t$ and that $var(Y_t)=\gamma \times X_t^2$. In order to give evidence / support to these hyphotheses I run a nested boostrap regression. Within each bootstrap sample I estimated $beta$ using OLS; I assumed $var(Y_t) \propto (Y_t-\hat{Y_t})^2=\hat{\varepsilon_i}$  and then estimated $\gamma$ from $\hat{\varepsilon_i}=\gamma \times X^2$ on the residuals estimated on the first equation, all within the same bootstrap cycle. Nevertheless I know that on the first regression there is still some autocorrelation of the residuals. I therefore wonder whether bootstrapping the series using block boostrap / maximum entropy boostrap would allow me to get sensible confidence interval estimates of $\beta$ and $\gamma$.</p>
"
"0.118678165819385","0.116741681083556","141701","<p>As you know, one can use regression for inference to learn which variables correlate with a response variable if the input predictors and the response share the same time frame. 
Let's say a predictor variable and a desired response variable are time lagged by X minutes. What statistical numeric techniques can one use to discover those dependencies with the hope of building a predictive model?</p>

<p>For example, let's say you sample every minute 10 variables and a desired target response variable. Let's say you strongly suspect that there may a lagged correlation. Does one just test with brute force correlations lagged by various time offsets to see if there are any?</p>
"
"0.0593390829096927","0.0583708405417779","142316","<p>As a programmer I have used the spdep package successfully for spatial filtering. But would appreciate it if someone could offer a description (preferably with supporting references) of how this concept works. Let's suppose I have a standard linear regression model with an integer response variable and 3 or more predictors comprising real values:</p>

<ol>
<li><p>According to <a href=""http://books.google.de/books/about/Spatial_Statistics_and_Geostatistics.html?id=xSPMCHuchEgC&amp;redir_esc=y"" rel=""nofollow"">Chun and Griffith</a>, a spatial filter would be constructed from missing predictors which are spatially correlated, and which will help to model the autocorrelation of the all observations. How is the autocorrelation among multivariate predictors taken into account?</p></li>
<li><p>How does this approach compare to the spatial Durbin model?</p></li>
<li><p>Does this concept have any relationship with the <a href=""http://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem"" rel=""nofollow"">Perronâ€“Frobenius theorem</a> from linear algebra?</p></li>
</ol>
"
"0.22642822184971","0.229918521344138","142489","<p>I'm analysing PAM fluorescence data from an experimental set-up that I duplicated from an earlier experiment with a missing control. That's why I haven't given the statistics of the experiment much (if any) thought in advance.</p>

<p>The set-up consisted of 8 containers with peat moss (<em>Sphagnum magellanicum</em>), divided over 4 treatments, so that each treatment was performed in duplicate. At regular (weekly) intervals, over the course of 3 months, I performed life PAM fluorescence measurements on a number of capitula (growth tops) in each container to determine a kinetic response curve for each of these capitula.</p>

<p>To minimize intraleaf (in my case, intra<em>capitula</em>) variance, ideally, PAM fluorescence measurements would have been repeated for the same leaf every week in the 3-month time series, but for practical reasons, my AOIs (areas of interest) for the fluorescence meter where located on different capitula every week. This is also my first subquestion: can I consider measurements at different time points in the same container as <em>repeated measures</em>, or would this only be valid if I had been measuring the same AOIs every week? And does this depend on whether I aggregate the measured values of the different AOIs per container before further analysis?</p>

<p>After nightfall, once every week, for 5â€“7 AOIs in each container, I determined a kinetic curve, for which the PAM software performs 20 measurements. The first measurement represents the dark-adapted fluorescence values, after which an actinic light source (at a wavelength that can facilitate photosynthesis) is started for the 19 remaining measurements. From the start of the kinetic curve (the dark adapted $\phi_{PSII}$ values), I determine $F_v/F_m$ and from the end of the curve (the flat part), I determine $\text{mean}(\phi_{PSII})$. $\phi_{PSII}$ and $F_v/F_m$ measure the quantum yield of photosystem II and the max. efficiency of photosystem II, respectively; $F_v/F_m = \phi_{PSII}$ in a dark-adapted state.</p>

<p>I'm interested in building two models, one in which the response (dependent) variable is $\phi_{PSII}$ and one in which it is $F_v/F_m$. The (independent) predictor variables are:</p>

<ul>
<li><code>AOI</code> (factor): a number between 1â€“6; </li>
<li><code>Container</code> (factor): a number between 1â€“8; </li>
<li><code>Treatment</code>: (factor): a number between 1â€“4; and</li>
<li><code>DaysTreated</code> (integer): the number of days since the treatments began.</li>
</ul>

<p>My guess is that I should treat <code>AOI</code> and <code>Container</code> as random effects variables, with <code>AOI</code> nested in <code>Container</code> and <code>Container</code> nested in the fixed effect variable <code>Treatment</code>. <code>DaysTreated</code>, then, would be my continuous predictor (covariate). For $\phi_{PSII}$, I would model this in R like this:</p>

<pre><code>library(nlme)
YII_m1 &lt;- lme(mean_YII ~ DaysTreated * Treatment,
              random = ~1 | Container / AOI,
              method = ""ML"",
              data = fluor_aoi)
# fluor_aoi is a data-frame in which each AOI kinetic curve is
# aggregated into one row, where mean_YII = mean( YII[15:19] )
# and FvFm = YII[1]
</code></pre>

<p>I'm not sure if this is the most parsimious model. To find out, I want to try different models with different fixed effects but all with the same random effects. <code>anova.lme()</code> warned me that comparing between these models is a <a href=""http://stats.stackexchange.com/questions/116770/"">no-go</a> when using the default method (<code>method = ""REML""</code>), which is why I use <code>method = ""ML""</code>.</p>

<pre><code>anova(YII_m1, # ~ DaysTreated * Treatment
      YII_m2, # ~ DaysTreated:Treatment + Treatment
      YII_m3, # ~ DaysTreated:Treatment + DaysTreated
      YII_m4, # ~ DaysTreated:Treatment
      YII_m5, # ~ DaysTreated + Treatment
      YII_m6, # ~ DaysTreated
      YII_m7  # ~ Treatment
     )

       Model df       AIC       BIC   logLik   Test  L.Ratio p-value
YII_m1     1 11 -2390.337 -2340.578 1206.168                        
YII_m2     2 11 -2390.337 -2340.578 1206.168                        
YII_m3     3  8 -2390.347 -2354.158 1203.173 2 vs 3  5.99019  0.1121
YII_m4     4  8 -2390.347 -2354.158 1203.173                        
YII_m5     5  8 -2366.481 -2330.293 1191.241                        
YII_m6     6  5 -2363.842 -2341.224 1186.921 5 vs 6  8.63915  0.0345
YII_m7     7  7 -2264.868 -2233.203 1139.434 6 vs 7 94.97389  &lt;.0001
</code></pre>

<p>I would have liked it if the <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">best fit</a> was model 2 with the fixed effects formula <code>~ DaysTreated:Treatment + Treatment</code>, because my expectation at the onset of my experiment was to see a decline in <em>Sphagnum</em> vitality, but only for some of the treatments and hopefully not in the controls. (The acclimatization period was very long, hoping that any effects on the mosses of the new (greenhouse) environment would have flattened out by the onset of the treatments.)</p>

<p><strong>Edit 2015-May-1:</strong> First I compared only 6 models; model 4 was missing from my initial question. Also, I forgot to factorize treatment, so that instead of model 2, now, different models give the â€˜best fitâ€™.</p>

<p>Anyway, so far (unless you tell me otherwise), I feel I can continue to use model 2, which also best fits the visual observation that 4 of the 8 containers where doing very badly at the end of the experiment while the other 4 seemed to do ok.</p>

<pre><code>anova(YII_m2)
                  numDF denDF  F-value p-value
(Intercept)           1   620 526.9698  &lt;.0001
Treatment             3     4   5.0769  0.0753
DaysTreated:Treatment 4   620  36.4539  &lt;.0001
</code></pre>

<p>An ANCOVA test on model 2 reveals that only the interaction between <code>DaysTreated</code> and <code>Treatment</code> is significant, which makes sense to me, given that the containers started out in roughly the same condition after acclimatization. There was visible difference between containers in the same treatments, but that should have been taken care of by correcting for the random error effect.</p>

<p>Mean $\phi_{PSII}$ plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments:</p>

<p><img src=""http://i.stack.imgur.com/GgGhG.png"" alt=""Mean Y_II plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments.""></p>

<p>Now that I've made an <em>attempt</em> at constructing and testing a somewhat decent model (which I'd love to receive criticism on), I'd like to perform a multiple pairwise comparison to find out which treatments diverge significantly from each other over time, but I have no idea what is the proper way to approach this.</p>

<p>Also, I want to try a linear correlation, but again, I'm clueless as to how. Is there an appropriate way to integrate this in my model or should I try to model a regression per treatment? </p>

<p>Please forgive the ignorance in my approach and my questions. I'm a BSc student whose statistical background mainly consists of a brief entry-level course, followed by a recipe-level R course. RTFM comments are definitely welcome, as long as they include a link to TFM.</p>
"
"0.0979044918012804","0.110065112455066","142693","<p><strong><em>Is the following a reasonable illustration of the OVB problem?</em></strong></p>

<p>We build up fictional data around the regression line:</p>

<p>$$y = 7.2 + 2.3 \, x_1 + 0.1 \, x_2 + 1.5 \, x_3 + 0.013 \, x_4 + eps$$</p>

<p>by using this function:</p>

<pre><code>correlatedValue = function(x, r){
  r2 = r**2
  ve = 1 - r2
  SD = sqrt(ve)
  e  = rnorm(length(x), mean = 0, sd = SD)
  y  = r * x + e
}
</code></pre>

<p>-thank you, @gung for this post:
<a href=""http://stats.stackexchange.com/questions/38856/how-to-generate-correlated-random-numbers-given-means-variances-and-degree-of"">How to generate correlated random numbers (given means, variances and degree of correlation)?</a></p>

<p>And the following function, which generates four variables (<strong><em>x1</em></strong>, <strong><em>x2</em></strong>, <strong><em>x3</em></strong> and <strong><em>x4</em></strong>) as well as noise (<strong><em>eps</em></strong>). <strong><em>x1</em></strong> and <strong><em>x3</em></strong> are sample from normal distributions; <strong><em>x2</em></strong> is extracted from a uniform; and <strong><em>x4</em></strong> from a Poisson.</p>

<pre><code>variables &lt;- function(){
x &lt;- rnorm(1000)
x1 &lt;- 50 + 15 * x
x3 &lt;- 28 + 11 * correlatedValue(x = x, r = 0.6)
x2 &lt;- runif(1000, 0, 100)
x4 &lt;- rpois(1000,50)
eps &lt;- rnorm(1000,5, 7)
y = 7.2 + 2.3 * x1 + 0.001 * x2 + 1.5 * x3 + 0.013 * x4 + eps
dat &lt;- as.data.frame(cbind(y, x1, x2, x3, x4))
c &lt;- as.numeric(coef(lm(y ~ x2 + x3 + x4, dat))[3])
d &lt;- as.numeric(coef(lm(y ~ x1 + x2 + x3 + x4, dat))[4])
c(c,d)
}
</code></pre>

<p><strong><em>x1</em></strong> and <strong><em>x3</em></strong> are highly influential on <strong><em>y</em></strong> and are correlated with each other, setting the values up to observe <strong><em>OVB</em></strong>. <strong><em>x2</em></strong> and <strong><em>x4</em></strong> are less influential.</p>

<p>Here is the plotting of <strong><em>y</em></strong> against <strong><em>x1</em></strong>, <strong><em>x2</em></strong>, <strong><em>x3</em></strong>  and <strong><em>x4</em></strong>, and <strong><em>x1</em></strong> over <strong><em>x3</em></strong> with added regression lines:</p>

<p><img src=""http://i.stack.imgur.com/I4u0S.png"" alt=""enter image description here""></p>

<p>And following is the variance-covariance matrix:</p>

<pre><code>             y           x1           x2         x3          x4
y   1.00000000  0.944410945  0.014421682 0.77571067 -0.01463981
x1  0.94441094  1.000000000 -0.001726526 0.56504020 -0.03562991
x2  0.01442168 -0.001726526  1.000000000 0.03537959  0.02253922
x3  0.77571067  0.565040198  0.035379590 1.00000000  0.02573827
x4 -0.01463981 -0.035629906  0.022539218 0.02573827  1.00000000
</code></pre>

<p>Predictably, the regression including all variables shows similar coefficients to the initial equation:</p>

<pre><code>coef(lm(y~.,dat))[2:5]
         x1          x2          x3          x4 
2.253353226 0.004899445 1.547915198 0.017710038 
</code></pre>

<p>Wrapping up, a quick simulation is carried out to obtain the mean of the <strong><em>x3</em></strong> coefficient in 1,000 simulations <em>WITHOUT</em> including <strong><em>x1</em></strong> (""coef_x3"") and then <em>WITH</em> <strong><em>x1</em></strong> (""coef_x3_full""):</p>

<pre><code>coef_x3 &lt;- NULL
coef_x3_full &lt;- NULL
for (i in 1:1000){
  coef_x3[i] = variables()[1]
  coef_x3_full[i] = variables()[2]
}
mean(coef_x3)
mean(coef_x3_full)
</code></pre>

<p>obtaining a coefficient for <strong><em>x3</em></strong> of <strong>3.383</strong> when <strong><em>x1</em></strong> is excluded versus a coefficient for <strong><em>x3</em></strong> of <strong>1.502</strong> when included. So when <strong><em>x1</em></strong> is included we have an unbiased estimation of the true <strong><em>x3</em></strong> coefficient (<strong><em>1.5</em></strong>), whereas the estimation is biased when we exclude <strong><em>x1</em></strong>.</p>
"
"0.134568391204877","0.154434727891351","144348","<p><strong>Note</strong><br>
I've edited the example to be more intuitive and closer to my real data</p>

<p><strong>Intro</strong><br>
I've got data on customers purchases and with it am trying to predict which customers are more likely to make next purchase at some time in the future. Data consist of customers' features like sex, age etc., and their prior purchase behavior like total spendings and number of orders, one row for every customer. The last two columns are the indicator of wether he have made next purchase or not, and number of days till purchase or till today, in case of no purchase.  </p>

<p><strong>Problem</strong><br>
I am building a Cox regression and then want to predict probability of next purchase for individual observations in, say, 30 days from last purchase.</p>

<p>Reproducible example:</p>

<pre><code>library(survival)
library(rms)
library(pec)
library(ggplot2)

data(cost)

# split into train and test sets
set.seed(1)
ind &lt;- sample(1:nrow(cost), 100)
test.set &lt;- cost[ind, ]
train.set &lt;- cost[-ind, ]
</code></pre>

<p>For Cox regression I use <code>cph</code> from <code>rms</code> package, for prediction - <code>predictSurvProb</code> from <code>pec</code> package as suggested in <a href=""http://stats.stackexchange.com/a/36016/72401"">this</a> discussion.</p>

<pre><code># fit Cox model
fit &lt;- cph(Surv(time, status) ~ ., data = train.set, surv = TRUE)

# predict pobability of event in 30 days
test.set$predicted.probs &lt;- 1 - predictSurvProb(fit, newdata = test.set, times = 1000)[, 1]
</code></pre>

<p>Thus, for every customer we have his probability of making a purchase in 1000 units of time. I want to validate prediction against real data.  </p>

<p><strong>Now to the question:</strong> what is the best/valid way to do it?  </p>

<p>Here's what I've tried:<br>
I expect that valid model would predict higher probabilities for customers who made their purchase earlier so correlation between probabilities and number of days to event' would be negative and strong (e.g. for customer who actualy made next purchase in 2 days, probability of buying in 30 days would be very high).</p>

<pre><code>with(test.set, cor(predicted.probs, time))
# [1] -0.5221604
</code></pre>

<p>Also, probability for those who made purchase (status = 1) would be higher than for those who didn't.</p>

<pre><code>with(test.set, by(predicted.probs, status, mean))
# status: 0
# [1] 0.2371247
# --------------
#   status: 1
# [1] 0.4083586
</code></pre>

<p>And a graph to eyeball my assumptions:</p>

<pre><code>qplot(data = test.set, x = time, y = predicted.probs, color = time)
</code></pre>

<p>Am I correct in my reasoning?</p>
"
"0.209950686014987","0.206524897489036","145849","<p>Iâ€™ve got a question concerning the R package <em>strucchange</em> that I use for testing and dating structural breaks in my PhD thesis.  To be specific, I use the generalized fluctuation test framework with CUSUM/MOSUM and in particular Moving Estimates (<strong>ME</strong>) tests for my analysis. Thus, the following description focuses on the ME test, but in principle is more general to all fluctuation tests.</p>

<p><strong>The problem:</strong> I am testing time series data for structural breaks with the ME test that draws on the function <strong>efp</strong> provided by strucchange. Given the nature of time series data, I want to tackle potential heteroskedasticity and autocorrelation in the data. Strucchange provides some functionality with respect to calculating <em>heteroskedasticity</em> (<strong>HC</strong>) and <em>autocorrelation</em> (<strong>HAC</strong>) consistent covariance matrices,  e.g., the approaches suggested by Newey-West (1987) or Andrews (1991). </p>

<p>However, this functionality in strucchange is limited to the function <strong>gefp</strong> that calculates Generalized Empirical M-Fluctuation Processes that as far as I know does not allow to perform estimates-based tests such as the ME test. Thus, I cannot use <strong>efp</strong> to estimate ME tests (or other tests that are available in this function) using HAC covariance matrices. </p>

<p><strong>The question:</strong> Does anybody know how I could make use of the <strong>efp</strong> function in <em>strucchange</em> for testing and dating structural changes but use HAC covariance matrices to take heteroskedasticity and autocorrelation into account? Maybe there is some way to use the sandwich package for this?</p>

<p><strong>Many thanks for any help!</strong></p>

<p>Here is a minimal working example to show the problem</p>

<pre><code>library(foreign)
library(strucchange)

data(""Nile"")

#using the function efp to perform a moving estimates test
#assuming sperical disturbances
ocus.nile &lt;- efp(Nile ~ 1, type = ""ME"")
sctest(ocus.nile)

#applying the vcov function with the kernHAC option to take heteroskedasticity and autocorrelation does not work, i.e., the option is not used and the result is the same
sctest(ocus.nile, vcov=kernHAC)

#using the function gefp to perform a generalized M-fluctuation process however works with vcov
#assuming spherical disturbances
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm)
sctest(ocus.nile2)

#controlling for heteroskedasticity and autocorrelation using an appropriate covariance matrix changes the result, i.e. works
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm, vcov= kernHAC)
sctest(ocus.nile2)
</code></pre>

<p><strong>Some background</strong></p>

<p>Though probably not necessary, here is some more in-depth background about the problem for the interested reader (and the archive). The formulas are taken from Zeileis et al., 2005, â€Monitoring structural change in dynamic econometric modelsâ€. </p>

<p>The ME test is used to detect structural breaks in the standard linear regression model over time. What it does it in essence partitioning the data and rather than estimating the regression based on the whole sample, it sequentially moves â€œthroughâ€ time in a fixed-width windows containing only a sub-sample of the observations and in each window it estimates the model. These estimates are used to the computation of empirical fluctuation processes that capture fluctuations in regression coefficients and residuals over time. Significant fluctuations of the coefficients are signs of a structural break in the regression. The test statistic of the Moving estimates test is</p>

<p><img src=""http://i.stack.imgur.com/qQ0FY.png"" alt=""Moving estimates test statistic""></p>

<p>where <em>n</em> is the number of observations, <em>h</em> is the bandwith (how many percent of the total number of observations are used for the window), <em>nh</em> is thus the size of the window, Q_(n)=X_(n)^T X_(N)/n, i=[k+t(n-k)], and sigma^2 is an estimate of the variance. The way I understand the above statistic is that it compares the difference between the sub-sample estimate of beta with the whole sample (the window) estimate and how this difference develops over time. A zero difference would indicate a sub-sample estimate that perfectly equals the whole-sample estimate, which would indicate perfect stability of the coefficient. In my understanding, the efp function in strucchange calculates sigma^2 based on the standard OLS residuals u^ i.e., sigma^2=(1/n-k)âˆ‘_(i=1)^n u_i^2 . Thus, in the presence of heteroskedasticity or autocorrelation, the OLS assumption of spherical disturbances will be violated. Thus, ideally, sigma^2 should be estimated based on a HAC covariance matrix to avoid wrong inference.</p>

<p>The question that comes to my mind is whether there is a way to use the ME test based on a HAC estimate. If not, it seems to me that it is limited to spherical disturbances of residuals, which seems to be violated in most applications.</p>
"
"0.0419590679148345","0.0412744171706498","146521","<p>Just for ""train"" with linear regression in <code>R</code> I'm doing a <code>Durbin-Watson test</code> over the residuals of a regression (over stock prices) comparing these with their value at t-1 (lag=1). From my data it's clear that residuals shows a strong autocorrelation. But I understood that from the autoregressive process on the residuals (regressor=1 and R square very close to 1) but I don't understand the <code>Durbin-Watson output</code>, that is:</p>

<pre><code>        Durbin-Watson test

data:  residui[, 1] ~ residui[, 2]
DW = 1.91, p-value = 0.01888
alternative hypothesis: true autocorrelation is greater than 0
</code></pre>
"
"0.167836271659338","0.165097668682599","146919","<p>Please, be kind, as I'm totally noob in stats and R...</p>

<p>I'm the owner of a small restaurant in a commercial center, and I managedd to collect two main dataset, commercial-center (cc) and restaurant (rest).</p>

<p>cc <a href=""https://www.dropbox.com/s/7k6k1rjimrcfqlq/cc.csv?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/7k6k1rjimrcfqlq/cc.csv?dl=0</a> <br/>
DAY 10:00-12.00 12:00-14:00 14:00-16:00 16:00-18:00 18:00-20:00 20:00-22:00 SUB-TOTAL<br/>
01/01/2012  0   825,55  534,85  879,7   964,725 161,975 3366,8</p>

<p>till today</p>

<p>rest <a href=""https://www.dropbox.com/s/rtoqwg64tu4poxs/rest.csv?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/rtoqwg64tu4poxs/rest.csv?dl=0</a> <br/>
is a database of all the restaurant sales from 04 dec 2014<br/>
I have managed to make it in a similar fashion as the above format<br/>
It contains many NA values (periods we were closed)</p>

<p>Also, I gathered other data: <br/>
events <br/>
is a dataset containing holidays and other events (it has to be categorized, as some increase sales, some the opposite)</p>

<p>weather <br/>
a collection of calculated observation for a spot 2 km away from the restaurant, each observation (wind speed, temp, cloudness and rain) have been turned in a value on a scale of 4 values</p>

<p>My objectives are:</p>

<ol>
<li>understand 'rest' seasonalities (daily/weekly, yearly)</li>
<li>understand how weather and other independet variables (not fixed events i.e. Easter or concerts) modify 'rest' and find a coefficient to apply to the model </li>
<li>forecast next days, weeks, months sales movements</li>
<li>verificate the restaurant trend, net of seasonalities and other variables</li>
</ol>

<p>An SD up to 30-40% on model/observed is still a good point to me as long as error distribution is not well spread but with a solid peak on 0</p>

<p>After many testing and try, I have found that the best model to choose is TBATS (BATS is deadly slow to me), expecially for managing multiple seasonality (which is a main point in my study).</p>

<p>The method I was thinking was to:</p>

<ol>
<li>Find indipendent variables coefficient for 'rest' and 'cc' and apply them to the data</li>
<li>Load 'rest' and 'c'c, with modifications at point 1, in an msts object and throw them to TBATS</li>
<li>Find correlations or other kind of link between the seasonalities of 'rest' and 'cc', get coefficients and use them for modeling future 'rest' data</li>
</ol>

<p>Not sure whether I can post multiple questions in here, if not just answer to this please: am I choosing the proper work method, or is anything better out there?</p>

<p>I'm struggling myself with many questions:</p>

<p><ol>
<li>How can I properly describe the data I have in R for using in TBATS, with sampling every 2 hours, starting at 10 and finishing at 22? Should I create a model for each column?</li>
<li>TBATS is not allowing for independent variables (events), is it? How to manage them?</li>
<li>TBATS does not accept NA values? How to manage the closing periods or days (NA) in the 'rest' dataset? Some weeks we were closed on Mondays, some Tuesday, some others we were always opening; we have some afternoon openings.... big mess... Maybe use regressions..?</li>
<li>If I do:</p>

<blockquote>
  <p>export &lt;- data.frame(DAYS=date,tbats.components(cc-SUBTOTAL.msts.tbats),errors=resid(cc-SUBTOTAL.msts.tbats))</li>
  </ol>
  how should I interpretate data? <br/>
  i.e. If I do level+season1+season2+errors=cObserved, I get figures which are slightly different from the ""real"" observed: sd(cObserved/observed)=3% which is fine for my objectives... is it correct?</p>
</blockquote>

<ol start=""5"">
<li>In TBATS, is trend some kind of SMA? at which window?</li>
</ol>
"
"0.111013258946721","0.109201843342674","147530","<p>I'm trying to perform a lagged linear regression on time series data sourced from ~10,000 hospital patients, for the purpose of estimating causal relationships between administration of a drug and a certain physiological response. For example: Do non-steroidal anti inflammatory drugs cause hypertension?</p>

<p>Basically, the linear model I'm trying to fit is like this:</p>

<p><img src=""http://i.stack.imgur.com/qukXs.png"" alt=""AR/cross-correlation model""></p>

<p>This assumes a maximum of 30 lags. $y$ represents hypertension, $x$ is taking the drug, and $h$ is whether the patient is admitted or not (a covariate). </p>

<p><strong>My question is this</strong>: Given a unique time series for <em>each patient</em> (all truncated to the same length of 30 time points), how can I pool all of the time series data together to estimate things like the cross-correlation (e.g., using <code>ccf</code>) and auto-correlation (<code>acf</code>) over the entire data set? If I were just trying to fit a linear model, this can be done relatively easily using something like the <code>plm</code> library, but I haven't been able to find anything similar for single functions.</p>

<p>For reference, here is a very small example of what my data set looks like (note that I only retained 6 of the 30 total time points for each patient, for brevity):</p>

<pre><code>   patient_id            time     nsaid hypertension admission
1           1               1 0.4427955    0.0000000 0.0000000
2           1               2 1.0000000    0.2097246 0.0000000
3           1               3 0.0000000    0.4916697 0.0000000
4           1               4 0.0000000    1.0000000 0.0000000
5           1               5 0.0000000    0.7902754 0.0000000
6           1               6 0.0000000    0.0000000 0.0000000
7           2               1 0.0000000    0.0000000 0.0000000
8           2               2 0.4104132    0.0000000 0.0000000
9           2               3 0.8236088    0.0000000 1.0000000
10          2               4 1.0000000    0.0000000 0.6994038
11          2               5 0.5895868    0.0000000 0.0000000
12          2               6 0.1763912    0.0000000 0.0000000
</code></pre>
"
"NaN","NaN","147593","<p>Consider the following scenario where you use the same data <code>X</code> (the same number of predictors <code>p</code>, same number of observations <code>n</code>) to predict a continuous outcome <code>y</code>, in 2 different regression models (e.g. Linear Regression and Random Forests).
For both models you calculate <code>RMSE</code> and <code>R-squared</code> (assume simple correlation between y and y-hat, squared).</p>

<p>You get:</p>

<pre><code>RMSE1 &lt; RMSE2

R-squared1 &lt; R-squared2
</code></pre>

<p>Could someone explain if this scenario is possible and how it occurs?
A simple simulation in <code>R</code> could also do.</p>
"
"0.0951542219543327","0.0936015800080059","147619","<p>I'm trying to build a model that can predict streamflow for an alpine (snowmelt-fed) watershed using snow albedo (roughly, the energy reflectance of the snow) data. Albedo controls the melt of the snowpack, and higher albedo means slower melt, and vice versa. I have daily time-series data for both the snow albedo and streamflow, for 12 years from 2002-2013. The albedo time-series was obtained by spatially-averaging albedo data (raster files) from NASA's MODIS satellite.</p>

<p>I have tried various methods (simple regression, GLMs, GAMs, decision trees and random forests) to build the flow prediction model, but all of them fail because of the autocorrelated relationship between albedo and flow. Since the albedo is a snowpack property, there is a lag between it and the flow (related to snowmelt).</p>

<p>The Cross correlation function (CCF) between albedo and flow is shown below:</p>

<p><a href=""http://imgur.com/PpW1Kpy"" rel=""nofollow""><img src=""http://i.imgur.com/PpW1Kpy.png"" title=""source: imgur.com"" /></a></p>

<p>I have tried to include albedo lags of various days into the models, but I'm not able to mimic the distributed lag relationship between albedo and flow. I have tried to add precipitation, temperature and other climatic data to the predictors, but they don't seem to help. There are similar lagged and cross-correlation problems between these other predictors and flow.</p>

<p>The albedo, flow, precipitation and air temperature time-series are shown below:</p>

<p><a href=""http://imgur.com/Kb8Ta6q"" rel=""nofollow""><img src=""http://i.imgur.com/Kb8Ta6q.png"" title=""source: imgur.com"" /></a></p>

<p>Is there a statistical or machine learning technique in R that I can explore to build the albedo-streamflow model?</p>

<p>Thank you.</p>
"
"0.0938233281301002","0.0922924025252575","147923","<p>I have a data set with continuous variable and a binary target variable (0 and 1). </p>

<p>I need to discretize the continuous variables (for logistic regression) with respect to the target variable and with the constrained that the frequency of observation in each interval should be balanced. I tried machine learning algorithms like Chi Merge, decision trees. Chi merge gave me intervals with very unbalanced numbers in each interval (an interval with 3 observations and another one with 1000). The decision trees were hard to interpret.</p>

<p>I came to the conclusion that an optimal discretization should maximise the $\chi^2$ statistic between the discretized variable and the target variable and should have intervals containing roughly the same amount of observations. </p>

<p>Is there an algorithm for solving this?</p>

<p>This how it could look like in R (def is the target variable and x the variable to be discretized). I calculated Tschuprow's $T$ to evaluate the ""correlation"" between the transformed and the target variable because $\chi^2$ statistics tends to increase with the number of intervals. I'm not certain if this is the right way.</p>

<p>Is there another way of evaluating if my discretization is optimal other than Tschuprow's $T$ (increases when number of classes decreases)? </p>

<pre><code>chitest &lt;- function(x){
  interv &lt;- cut(x, c(0, 1.6,1.9, 2.3, 2.9, max(x)), include.lowest = TRUE)
  X2 &lt;- chisq.test(df.train$def,as.numeric(interv))$statistic
  #Tschuprow
  Tschup &lt;- sqrt((X2)/(nrow(df.train)*sqrt((6-1)*(2-1))))
  print(list(Chi2=X2,freq=table(interv),def=sum.def,Tschuprow=Tschup))
}
</code></pre>
"
"0.0839181358296689","0.0825488343412996","148529","<p>I am estimating a Weighted Spatial Simultaneous Autoregression Model (<code>spdep::spautolm</code> --> <a href=""http://cran.r-project.org/web/packages/spdep/spdep.pdf"" rel=""nofollow"">Link</a>) in R and I would like to do some residual analysis.</p>

<p>Unfortunately functions such as <code>hatvalues</code>,<code>cooks.distance</code> or <code>plot.lm</code> do not work for <code>spautolm</code> objects. Yet, I would like to calculate leverages and cook distances for my model (see also <a href=""https://stackoverflow.com/questions/29747519/how-to-do-residual-regression-deletion-diagnostics-on-spautolm-sarlm-objects/29756024#29756024"">my post on stackoverflow</a>).</p>

<p>My Model looks like this:</p>

<p>$Y = X^T\beta + \lambda W(Y âˆ’ X^T\beta)+ \epsilon$ with $\epsilon\sim^{iid} N(0,\sigma^2)$</p>

<p>Thus:</p>

<p>$Var[Y]=\Sigma_{SAR} = \sigmaÂ²(I-\lambda W)^{-1}V(I-\lambda W)^{-1}$ with $V=diag[1/n_i]$</p>

<p>$\rho$ is my spatial autoregression parameter and $W$ the matrix that represents spatial dependence.</p>

<p>Obviously, a simple calculation of the hatmatrix $H$ via $H=X(X^TX)^{-1}X^T$ is not valid here due to the weighting and the modeled spatial autocorrelation.</p>

<p><strong>Any ideas how to calculate the leverages and cook's distances by hand in R for this model?</strong></p>
"
"0.0726752374667264","0.0714893875923587","149352","<p>I would be very thankful if somebody could help me and explain the answer in simple terms.</p>

<p>I have y, x, z variables. They are countinuous, no missing values. y is dependent variable, x and z independent. There is significant correlation between them (varies from 0.25 to 0.35).</p>

<p>My hypothesis is that when z is very high, then x and y dependency is linear (x goes bigger, y smaller).
When z is very low, then in the beginning the situation is the same (dependency is linear), but in the end (about last quarter) it changes to quadratic.</p>

<p>So basically there are two different relationships between x and y, but the nature of the relationship depends on z which is again a continuous variable. </p>

<p>How can I test this model? Is it possible doing linear regression or do I need to use something else?</p>
"
"0.125877203744503","0.123823251511949","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.0839181358296689","0.0825488343412996","154043","<p>In polynomial regression, it is recommended to center predictor input variables to break multi colinear relationships of x to x^2.</p>

<p>From Wikipedia: The underlying monomials can be highly correlated ""For example, x and x2 have correlation around 0.97 when x is uniformly distributed on the interval (0, 1). ""</p>

<p>When a variable x is between -1 and 1, x^2 makes the magnitude smaller while when x is outside of that range, x^2 makes x's magnitude larger.</p>

<p>Making the variable into an integer variable could change the behavior.</p>

<p>E.g.</p>

<pre><code>df$x=round((df$x - mean(df$))*100)
</code></pre>

<p>Any opinions on the scale especially in regards to interval [-1,1] vs [-100,100]</p>

<p>It is common to normalize predictors subtracting the mean and dividing by the standard deviation when doing inference analysis but this question pertains to regression prediction.</p>

<p>Asking a similar question in regards to natural log, a variable that has a range (0,1] has a dramatically different transformed value than [1,100].</p>

<pre><code>log(seq(0.1,1,.1)) #mostly negative
log(seq(0.1,1,.1)*100) #rather positive
</code></pre>

<p>If the predictor variable in the case of log happened to be sometimes less than 1 and others greater than 1, that could make the transformation act a little ""wild"". Would it be best to transform the variable to be within (0,1] or [1,] but not both?</p>
"
"0.0419590679148345","0.0412744171706498","156359","<p>If I have a set of data of 9 predictor variables (2 are numerical and 7 are categorical) and one numerical response variable. How can I find the correlation between the different variables and what type of regression can I apply to this in R? My question might be vague, but I don;t know that much about statistics and I have to solve this for a research. I appreciate any help.</p>

<p>Thank you</p>
"
"0.0484501583111509","0.0714893875923587","156780","<p>I am running a logistic regression with 5 continuous independent variables (IV). The problem is that IV4 when taken alone has a positive correlation with outcome (coeff > 0), and when taken with the other variables has a negative correlation (coeff &lt; 0). I evaluated correlation between IV4 and the other variables, and the results are: 
IV4 vs. IV1 (-0.51), IV4 vs. IV2 (-0.48), IV4 vs. IV3 (0.61) and IV4 vs. IV5 (0.73).</p>

<p>I ran other logistic regressions <em>eliminating one at a time all the other variables</em> to look if one of them was responsible for the sign change, and I noticed that when eliminating IV1, the sign of V4 coefficient became positive.</p>

<p>Thus, it seems that IV1 changes the sign of the coefficient of IV4. 
Is there someone who knows what might be the cause and (possibly) the solution?</p>

<p>Practically, do I have to eliminate the IV4 (or IV1) from the model and explain why?</p>

<p>Thanks a lot for answering</p>

<p>Leonardo Frazzoni, MD</p>
"
"0.10277830647413","0.101101261498861","157003","<p>I am using <code>glm (target, formula = target~.,family=binomial)</code> to predict binary outcome.</p>

<p>I have 9 grouped predictors.
I convert them into factors so that I can test them in the regression as dummies.</p>

<p>Additionally, I have initially set the reference group <code>relevel=var_name(var_name, ref = ""ref_group_name"")</code></p>

<p>I run the regression and the result is decent at first glance (good Gini, just a few dummies with high significance).</p>

<p>What I would like to do further is as follows:</p>

<p>1) check Mallows' Cp ( one thing to be checked for overfitting of the model that I know is that Mallows' Cp should be close to the number of dummies entering the model)</p>

<p>2) interactively add/remove dummies in the model</p>

<p>3) check correlation between dummies rather than between the whole variables</p>

<p>For the <strong>first</strong> question I am not sure how to test Mallows' Cp (summary of the <code>glm</code> result does not show it)?</p>

<p>For the <strong>second</strong> question I found package called <code>dummies(dummy.data.frame function)</code> that can easily convert factor variables into dummies so that I can interactively add or remove them (I found that potentially I can do that with package <code>leaps(update function)</code>)
What I am missing is when I create the dummies data frame with <code>dummy.data.frame</code> how can I select which is the <code>reference group</code> when I put the resulting data.frame into the glm function?</p>

<p>For the <strong>third</strong> question: does it now make sense to run Pearson/Spearman correlation on the dummies data.frame (note: initially variables are both numeric and categorical before they were grouped)</p>
"
"0.0938233281301002","0.0922924025252575","157694","<p>I have a formula (not mine):</p>

<pre><code>NY/A = (Passing Yards - Sack Yards) / (Passes Attempted + Times Sacked)
</code></pre>

<p>and this formula correlates with wins at a correlation coefficient of 0.50. </p>

<p>Edit: I have the data for wins and passing yards, sack yards, passes attempted, and times sacked. NY/A stands for net yards per attempt, and I would like to correlate this with wins at a coefficient of at least 0.52. I can put some weight on times sacked or sack yards. </p>

<p>My goal is to increase the correlation to at least 0.52. How can I do this? Is there a regression I can run in R?</p>
"
"0.140839201755668","0.159855130326344","158366","<p>I'm trying to fit a multiple regression model with pairwise deletion in the context of missing data.  <code>lm()</code> uses listwise deletion, which I'd prefer not to use in my case.  I'd also prefer not to use multiple imputation or FIML.  How can I do multiple regression with pairwise deletion in R?</p>

<p>I have tried the <code>mat.regress()</code> function of the <code>psych</code> package, which fits regression models to correlation/covariance matrices (which can be obtained from pairwise deletion), but the regression model does not appear to include an intercept parameter.</p>

<p>Here's what I've tried (small example):</p>

<pre><code>set.seed(33333)
y &lt;- rnorm(1000)
x1 &lt;- y*2 + rnorm(1000, sd=.2)
x2 &lt;- y*5 + rnorm(1000, sd=.5)

y[sample(1:1000, 10)] &lt;- NA
x1[sample(1:1000, 10)] &lt;- NA
x2[sample(1:1000, 10)] &lt;- NA

mydata &lt;- data.frame(y, x1, x2)
covMatrix &lt;- cov(mydata, use=""pairwise.complete.obs"")

#Listwise Deletion
listwiseDeletion &lt;- lm(y ~ x1 + x2, data=mydata)
observations &lt;- length(listwiseDeletion$na.action) #30 rows deleted due to listwise deletion

coef(listwiseDeletion)
(Intercept)          x1          x2 
0.001995527 0.245372245 0.100001989

#Pairwise Deletion --- but missing intercept
pairwiseDeletion &lt;- mat.regress(y=""y"", x=c(""x1"",""x2""), data=covMatrix, n.obs=observations)
pairwiseDeletion$beta
       y
x1 0.1861277
x2 0.1251995

#Pairwise Deletion --- tried to add intercept, but received error when fitting model
mydata$intercept &lt;- 0
covMatrixWithIntercept &lt;- cov(mydata, use=""pairwise.complete.obs"")

pairwiseDeletionWithIntercept &lt;- mat.regress(y=""y"", x=c(""intercept"",""x1"",""x2""), data=covMatrixWithIntercept, n.obs=observations)
Something is seriously wrong the correlation matrix.
In smc, smcs were set to 1.0
Warning messages:
1: In cov2cor(C) :
  diag(.) had 0 or NA entries; non-finite result is doubtful
2: In cor.smooth(R) :
  I am sorry, there is something seriously wrong with the correlation matrix,
cor.smooth failed to  smooth it because some of the eigen values are NA.  
Are you sure you specified the data correctly?
</code></pre>

<p>So, how can I obtain an intercept parameter using <code>mat.regress</code>, or how can I obtain parameter estimates from pairwise deletion using another method or package in R?  I've seen <a href=""https://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re"">matrix calculations</a> to do this, but, ideally, there'd be a package that also outputs regression diagnostics, fit stats, etc.  Also, preferably, the method would be able to fit interaction terms.</p>
"
"0.0726752374667264","0.0714893875923587","160281","<p>Here is what i am doing. I am building a logarithmic model in linear form based on the correlation between two variables shown in the graph!</p>

<p>lm(y~logx,data=logdata) -- i have only one predictor and one response variable<img src=""http://i.stack.imgur.com/6bqBB.png"" alt=""enter image description here""></p>

<p>cor(x,y) = -0.57(bit low for the data i have but they are negatively correlated in general sense and cor should be about atleast -0.80 in normal cases)</p>

<p>Multiple R-square = 35.78%</p>

<p>Data set doesn't contain any null/missing values. But from the graph you can see the concentration of data points is more at the center!</p>

<p>What are the different factors that i should think of in order to improve my model? here is one what i think off -</p>

<ol>
<li>Remove the outliers -- how should i identify outliers?</li>
<li>Should I remove all the data points that are sparsely distributed? -- I am concerned like that is going to effect my model?</li>
</ol>

<p>Do i have to go with regression modeling in this case which is not giving me the best prediction? If not can anyone please suggest me the best algorithm that should be used in this case?</p>
"
"0.126511349842314","0.136891755195648","160316","<p>I have a dataset consisting of about 600 observations. Each observation has around 100 attributes. One of the attributes I want to predict. Since the attribute that I want to predict can only have non-negative integer values, I was looking for ways to predict count data and found that there are various options, such as Poisson regression or negative binomial regression.</p>

<p>For my first try I used negative binomial regression in <code>R</code>:</p>

<pre><code>#First load the data into a dataset
dataset &lt;- test_observations[, c(5:8, 54)]

#Create the model
fm_nbin &lt;- glm.nb(NumberOfIncidents ~ ., data = dataset[10:600, ] )
</code></pre>

<p>I then wanted to see how to predicted values look like:</p>

<pre><code>#Create data to test prediction
newdata &lt;- dataset[1:10, ]

#Do the prediction
predict(fm_nbin, newdata, type=""response"")
</code></pre>

<p>Now the problem is the output looks like this:</p>

<pre><code>     1         2         3         4         5         6         7         8         9        10 
0.2247337 0.2642789 0.2205408 0.2161833 0.1794224 0.2081522 0.2412996 0.2074992 0.2213011 0.2100026 
</code></pre>

<p>The problem with this is that I expected that the predicted values are integers, since that is the whole purpose of using a negative binomial regression. What am I missing here?</p>

<p>Furthermore, I would like to evaluate my predictions in terms of mean squared error and mean absolute error, as well as a correlation coefficient. However, I couldn't find a way to get these easily, without doing all the calculations manually. Is there any built-in function for this?</p>
"
"0.151672986506104","0.159855130326344","160675","<p>I'm interested in determining both the slope regression coefficient and plotting regression lines for autocorrelated time-series datasets of rainfall.  Specifically, I'd like to identify the best approach in R that would allow me to visualize the regression line on the original (undifferenced) time-series plot when I need to difference the data to remove stationarity (i.e, when d>0 in an arima model).</p>

<p>As a start, I'm exploring the use of auto.arima (from the forecast package) and sarima (from the astsa package) which can output regression coefficients in the presence of autocorrelation.</p>

<p>For example:</p>

<ol>
<li><p>Using auto.arima. The 'drift' of -5.009 represents the slope (see <a href=""http://robjhyndman.com/hyndsight/arima-trends/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/arima-trends/</a>) </p>

<pre><code>&gt; min.ar &lt;- auto.arima(dec.yr.mmin$min_prcp)
&gt; summary(min.ar)

Series: dec.yr.mmin$min_prcp 
ARIMA(1,1,0) with drift         

Coefficients:
          ar1    drift
      -0.5138  -5.0089
s.e.   0.2465   5.7986

sigma^2 estimated as 949:  log likelihood=-57.82
AIC=121.64   AICc=124.31   BIC=123.34

Training set error measures:
                     ME     RMSE      MAE       MPE     MAPE      MASE       ACF1
Training set -0.9479987 28.52129 23.83494 -2.484233 16.12547 0.7957998 -0.2617352
</code></pre></li>
<li><p>Using sarima to fit the model and output the slope</p>

<pre><code>  &gt; fit.min &lt;- sarima(dec.yr.mmin$min_prcp, 1,1,0,                       reg=dec.yr.mmin$decade)
initial  value 3.542448 
  iter   2 value 3.488927
  iter   3 value 3.386967
  iter   4 value 3.383464
  iter   5 value 3.382408
  iter   6 value 3.382051
  iter   7 value 3.382024
  iter   8 value 3.382020
  iter   9 value 3.381925
  iter   9 value 3.381925
  iter   9 value 3.381925
  final  value 3.381925 
  converged
  initial  value 3.400729 
  iter   2 value 3.399523
  iter   3 value 3.399490
  iter   4 value 3.399488
  iter   4 value 3.399488
  iter   4 value 3.399488
  final  value 3.399488 
  converged
</code></pre></li>
</ol>

<p>3.Output coefficients</p>

<pre><code>      &gt; fit.min$fit$coef
             ar1       xreg 
      -0.5137696 -0.5009045 
</code></pre>

<ol start=""4"">
<li><p>For comparison, this is the output from an OLS regression which may give an incorrect slope due to autocorrelation.</p>

<pre><code>  &gt; m3 &lt;- lm(dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)
  &gt; summary(m3)

  Call:
  lm(formula = dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)

  Residuals:
      Min      1Q  Median      3Q     Max 
  -45.504  -8.048   1.892  13.650  38.357 

  Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
  (Intercept)        1014.1570   319.9461   3.170  0.00807 **
  dec.yr.mmin$decade   -0.4222     0.1580  -2.672  0.02032 * 
  ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

  Residual standard error: 23.83 on 12 degrees of freedom
  Multiple R-squared:  0.3731,  Adjusted R-squared:  0.3209 
  F-statistic: 7.142 on 1 and 12 DF,  p-value: 0.02032
</code></pre>

<p>`</p></li>
</ol>

<p>The output from sarima identifies the slope coefficient and also intercept when d=0. When differencing is required (e.g., ARIMA (1,1,0) as output above), sarima only outputs the slope. </p>

<p>My question: when d = 1 or more, what approaches in R would allow me to add/visualize the regression line onto the original undifferenced time-series plot. Is it possible to derive the fitted values of the regression line, or derive intercept values from sarima/auto.arima or other package?</p>

<p>Many thanks in advance for your suggestions.</p>
"
"0.0750586625040802","0.073833922020206","161624","<p>I have a system of 2 equations with 16 variables $X={x_{1},x_{2},x_{3},x_{4},x_{5},x_{6},x_{7},x_{8},x_{9},x_{10},x_{11},x_{12},x_{13},x_{14},x_{15},x_{16}}$</p>

<p>$0.2381570 = \frac{\frac{x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}}{\sqrt{3} \cdot \sqrt{5.0625}}-\frac{x_{7},x_{8},x_{9},x_{10}}{\sqrt{3} \cdot \sqrt{2.56}}
 \cdot \frac{x_{11},x_{12},x_{13},x_{14},x_{15},x_{16}}{\sqrt{5.0625} \cdot \sqrt{2.56}}}{1-(\frac{x_{11},x_{12},x_{13},x_{14},x_{15},x_{16}}{\sqrt{5.0625} \cdot \sqrt{2.56}})^{2}}$</p>

<p>$0.2092895 = \frac{\frac{x_{7},x_{8},x_{9},x_{10}}{\sqrt{3} \cdot \sqrt{2.56}}-\frac{x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}}{\sqrt{3} \cdot \sqrt{5.0625}}
 \cdot \frac{x_{11},x_{12},x_{13},x_{14},x_{15},x_{16}}{\sqrt{5.0625} \cdot \sqrt{2.56}}}{1-(\frac{x_{11},x_{12},x_{13},x_{14},x_{15},x_{16}}{\sqrt{5.0625} \cdot \sqrt{2.56}})^{2}}$</p>

<p>I now want to solve the system of equations with the following restriction: </p>

<ul>
<li>$\{x\in R \mid -1 \leq x \leq 1\}$</li>
</ul>

<p>To be more precise: I'm not even sure about this, the x's are Pearson correlations and 0.2381570, 0.2092895 are regression weights. Therefore I think I would prefer solutions that are not 0, thats why I set the startx parameters to 1 in the following R-Code. Maybe the structure of the equations also does not allow any other solutions anyway, but I really would not know how to prove that.</p>

<pre><code>fun &lt;- function(x) { 
    f &lt;- numeric(length(x))                                 
    f[1] &lt;-  ((((x[1]+x[2]+x[3]+x[4]+x[5]+x[6])/(sqrt(3)*sqrt(5.0625)))-
                   ((x[7]+x[8]+x[9]+x[10])/(sqrt(3)*sqrt(2.56)))*
                   ((x[11]+x[12]+x[13]+x[14]+x[15]+x[16])/(sqrt(5.0625)*sqrt(2.56))))/
                  (1-((x[11]+x[12]+x[13]+x[14]+x[15]+x[16])/(sqrt(5.0625)*sqrt(2.56)))^2))-0.238157
    f[2] &lt;-  ((((x[7]+x[8]+x[9]+x[10])/(sqrt(3)*sqrt(2.56)))-
                   ((x[1]+x[2]+x[3]+x[4]+x[5]+x[6])/(sqrt(3)*sqrt(5.0625)))*
                   ((x[11]+x[12]+x[13]+x[14]+x[15]+x[16])/(sqrt(5.0625)*sqrt(2.56))))/
                  (1-((x[11]+x[12]+x[13]+x[14]+x[15]+x[16])/(sqrt(5.0625)*sqrt(2.56)))^2))-0.2092895
    f 
} 
startx &lt;- rep(1,16) # start the answer search here
answers &lt;- as.data.frame(nleqslv(startx,fun))  # answers[""x""] = x,y,z are the solutions closest to startx if there are multiple solutions
</code></pre>

<p>My problem with this: the answers only give solutions for $x_{1}$ and $x_{2}$ and set all other variables to 0. I guess this happens because there are only 2 equations.</p>

<p>An obvious solution would be f.e. X = 0.2. How can I find those? Also the result says ""Jacobian is singular"", what I don't understand.</p>

<p>My ultimate goal would be a vector space of the possible solutions, but then again there are infinitely many because I have more variables than equations?</p>
"
"0.0839181358296689","0.0825488343412996","161797","<p>Say I have some predictors, and I know how they affect some dependent variable:</p>

<pre><code>#predictors
x1&lt;- seq(0,10,0.1)
x2&lt;-runif(101,0,1)
#specify how predictors affect dependent variable y
y&lt;- 15*x1 + 10*x1*x2
#introduce random error
y.err&lt;- rnorm(101,0.01)
y&lt;- y + y.err
</code></pre>

<p>I can then model <code>y</code> as a function of <code>x1</code> and <code>x2</code> like this:</p>

<pre><code>fit&lt;- lm(y ~ x1 + x1*x2)
</code></pre>

<p>which yields this output:</p>

<pre><code>summary(fit)

Call:
lm(formula = y ~ x1 + x1 * x2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.76646 -0.59886 -0.09115  0.70549  2.85311 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.38875    0.41630   0.934    0.353    
x1          14.93601    0.07409 201.585   &lt;2e-16 ***
x2          -0.74815    0.79518  -0.941    0.349    
x1:x2       10.10469    0.13698  73.768   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.025 on 97 degrees of freedom
Multiple R-squared:  0.9997,    Adjusted R-squared:  0.9997 
F-statistic: 1.184e+05 on 3 and 97 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>So, total model $R^2$ is 0.997. </p>

<p>I would like to know what percentage of that $R^2$ value can be attributed to x1, x2, and the interaction of x1 and x2. I am also aware of a previous post on this topic linked <a href=""http://stats.stackexchange.com/questions/60872/how-to-split-r-squared-between-predictor-variables-in-multiple-regression"">here</a>, where the user has an identical question. However, the solution proposed (as I read it) was to run the correlations individually, square them, and they will sum to the full model $R^2$. This is not the case here. </p>
"
"0.0593390829096927","0.0583708405417779","164110","<p>I am running a logistic regression on a data set containing Continuous, Ordinal, Categorical and Dichotomic variables.</p>

<p>I would like to know how to calculate the correlation for all possible combinations (see matrix below - cases marked with an X do not occur in my data set) in order to check for colinearity. I can do this either with SAS or R.</p>

<hr>

<pre><code>             Continuous    Ordinal    Categorical     Dichotomic
Continuous        X           1             2              3
Ordinal                       5             6              7
Categorical                                 8              9
Dichotomic                                                 X
</code></pre>

<p>Case 8 I use <code>proc freq data=data chisq ;</code> to return Cramer's V.</p>

<p>As for the rest I am unsure - is it possible for cases 3, 7 and 9 to consider a dichotomic variable as categorical in two classes in order to compute Cramer's V?</p>
"
"0.111013258946721","0.109201843342674","166986","<p>I have this simple example, where I try to get to the same factor loading as <code>nlme::gls</code></p>

<p>Prepare data</p>

<pre><code>#number of individuals
NN=2
#time
TT=100
set.seed(1234)
#predictors
x=matrix(rnorm(NN*TT,0,0.01),ncol=NN)
#correlated residuals
eps_ucor=matrix(rnorm(NN*TT,0,0.01),ncol=NN)
eps_cor=eps_ucor%*%chol(matrix(c(1,0.8,0.8,1),NN,NN))
#true factor loading
beta=1
#response
y=beta*x+eps_cor
#prepare data.frame; coerce by rows, i.e. y=c(y[1,1],y[1,2],y[2,1],y[2,2],...)
data=data.frame(t=sort(rep(1:TT,NN)),n=rep(1:NN,TT),y=c(t(y)),x=c(t(x)))
</code></pre>

<p>First, let's calculate the factor loading ""manually"" (I will follow <a href=""https://en.wikipedia.org/wiki/Generalized_least_squares"" rel=""nofollow"">https://en.wikipedia.org/wiki/Generalized_least_squares</a>)</p>

<pre><code>#covariance of residuals
cov_eps=cov(eps_cor)
#this is the matrix \Omega in the wikipedia article
#note that I assume that there is no correlation in time!
omega=diag(TT)%x%cov_eps
#calculate inverse
omega_inv=diag(TT)%x%solve(cov_eps)
</code></pre>

<p>We can now calculate the factor loading</p>

<pre><code>beta_gls_1=(t(data$x)%*%omega_inv%*%data$x)^(-1)*t(data$x)%*%omega_inv%*%data$y
beta_gls_1
#         [,1]
#[1,] 1.017732
</code></pre>

<p>Finally, let's calculate the same with <code>nlme::gls</code>
We have to prepare a vector with normalized variances</p>

<pre><code>var_vec=diag(cov_eps)
var_vec=var_vec/var_vec[1]
names(var_vec)=1:2
</code></pre>

<p>Now let's perform the regression</p>

<pre><code>res_gls=gls(model=y~x+0,
    data=data,
    correlation=corSymm(value=cov2cor(cov_eps)[2,1],form=~n|t,fixed=TRUE),
    weights=varIdent(value=numeric(0),form=~1|n,fixed=var_vec[-1]),method=""ML"")
</code></pre>

<p>The gls factor loading is</p>

<pre><code>res_gls$coef
#       x 
#1.015082
</code></pre>

<p>As you can see, there is a difference between <code>beta_gls_1</code> and <code>res_gls$coef</code>. I know, the difference is rather small - but from my experience it usually pays off to get to the bottom of such discrepancies.
My guess is that I have misunderstood how to correctly initialize the fixed correlation and variance structure in <code>gls</code>.
Any pointers to understand the difference highly appreciated!</p>

<p>BTW:</p>

<pre><code>R version 3.0.2 (2013-09-25)
Platform: x86_64-pc-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8     LC_MONETARY=en_US.UTF-8   
 [6] LC_MESSAGES=en_US.UTF-8    LC_PAPER=en_US.UTF-8       LC_NAME=C                  LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] nlme_3.1-113

loaded via a namespace (and not attached):
[1] grid_3.0.2      lattice_0.20-24 tools_3.0.2 
</code></pre>
"
"0.0593390829096927","0.0583708405417779","168182","<p>I am trying to create a model of refrigeration having the energy consumption and the temperature over time. So far, I've tried regression but fitting this data into linear model seems impossible. Another thing that I've tried is cross correlation but it's insignificant (around 0.11 at lag 0). I also clustered the data and for another fridge I was able to state that if the fridge is in 'idle mode' (e.g. not consuming electricity) the temperature goes above certain value. However, for this fridge, this doesn't work as the data seems pretty random. Here is a scatter plot of the data, the bigger the circle, the higher the frequency.
<a href=""http://i.stack.imgur.com/1w5lU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1w5lU.png"" alt=""enter image description here""></a></p>

<p>Any ideas what type of analysis can I use to derive insights from this? I would like to know if there is any correlation between the kW data and the temperature data. A new plot for the full duration that I have:</p>

<p><a href=""http://i.stack.imgur.com/yfZAr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yfZAr.png"" alt=""enter image description here""></a></p>
"
"0.0938233281301002","0.0922924025252575","171151","<p>I am trying to build a multiple regression model using R. I have a number of predictor variables. I have some basic domain knowledge for which I am trying to build the model. To start with, I included a few predictor variables based on domain knowledge and high correlation coefficients with the response variable, while excluding some other predictors due to multicollinearity. I would like to figure out if I should include some interaction terms. But, due to large number of predictors, I am having a hard time trying to figure out which all interaction terms I should include in the model. Based on what I have read on this site about automated model selection (thanks, @gung et. al), I am trying to avoid using it. </p>
"
"0.128010867631825","0.148817027473669","171193","<p>I recently employed multiple quantile regression in my area of research and found some interesting quantile differences across the distribution of Y, but I don't quite understand what they all really mean.  Unlike the traditional methods such as dividing the sample into multiple groups where I have access to the groups' data on various variables which then allows me to make sense of, for example, why the correlation between X and Y is 0 for group 1 and .7 for group 2, I feel like I have no idea where those quantile regression estimates come from, especially when there are more than 2 predictors in the QR model. Another way of putting this is I don't know which specific data points contribute heavily to a given quantile regression estimate and so this makes it very difficult for me to understand what the quantile differences really mean.  </p>

<p>Based on my understanding of QR, it uses all the data points in the full sample but weights the data points that are farther from a quantile of interest less heavily than the data points that are closer to that same quantile of interest, is this correct? If so, as a follow up, can I divide my full sample into 10 groups, e.g., 10th quantile, 20th quantile, 30th quantile group, and then examine how the 10 groups differ on various variables of interest in order to make sense of the 10 quantile regression estimates that I got? I know the subgroups approach is not ideal, which is why I used QR, but if you think this is a terrible idea, please let me know why. And if you know of any other methods  that allow me to have a more fine-grained understanding of my results, please help.  I conducted QR using the <code>quantreg</code> package in R.  </p>
"
"0.133237935355665","0.142978775184717","171483","<p>I am trying to model stock returns with the help of google trends data. As explained in my <a href=""http://stats.stackexchange.com/questions/171212/check-for-normality-is-it-possible-to-combine-variables-to-get-an-overall-view"">first question</a> this data is normalised by google so that it is not normally distributed as User EdM kindly pointed out. Below I attached a <code>qqnorm()</code>plot for the accumulated values of some of my variables for further insight into their distribution.</p>

<p><a href=""http://i.stack.imgur.com/V0AKH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/V0AKH.png"" alt=""enter image description here""></a></p>

<p>Since my data is not normally distributed am I'm still allowed to do an OLS regression and look at performance measures like adjusted R squared and p-values? I saw some posts where it said that those tests are only used for normally distributed data, but that one can still use them due to the central limit theorem. Each variable has approximately 250 data entries in the in-sample data. Is this true? (Sorry, cannot find the posts atm and might be mixing something up)</p>

<p>Additionally, I would be really thankful if you could give me some further insights into model selection. I read a lot about it and as far as I understood it there are no set rules on how to find the best model and that it can get really complex the more variables you have. My problem is that I have more than 100 possible variables and - as far as my knowledge goes - those are simply too many. Hence, I thought I might be able to cut down on the variables by calculating the correlation (spearman) between every independent variable and the dependent variable. I would rank the correlations and start to cut out collinear variables (e.g. variabels that have higher absolute correlation than 0.5 with the variable I am inspecting). I know that I might lose some information by just watching correlation in the first place, but I dont know where else to start. I would do this until I get, lets say, 10 variables with relativ high correlation to the dependent variable and ""low"" collinearity. Then I would like to use stepwise or best subset regression to find the ""best"" model and finally use a rolling window approach or cross validation to approve the model.</p>

<p>As you might have guessed I am fairly new to regression statistics or statistics in general. I would be really thankful for any opinion or hint towards the right direction. I did a lot of research over the past few days, but couldnt simply find the right solution for the problems mentioned above.</p>
"
"0.201875659629796","0.206524897489036","172520","<p>Suppose we have a historical (panel/longitudinal) dataset on the number of buildings in each sub-region (this is a made-up dataset to explain the concept):</p>

<p><a href=""http://i.stack.imgur.com/NLiRD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NLiRD.png"" alt=""Data Structure""></a></p>

<p><a href=""http://i.stack.imgur.com/bFFoI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bFFoI.png"" alt=""Data Format""></a></p>

<ul>
<li>The variable â€œYearâ€ ranges from 1 to 10 and it represents the year that each data point belongs to. </li>
<li>The variable â€œSub-regionâ€ ranges from 1 to 50 and it represents the sub-region the data was collected from.</li>
<li>The variable â€œTypeâ€ ranges from 1 to 2 and it represents the type of each building (say: office car or residential).</li>
<li>The variable â€œGroupâ€ ranges from 1 to 3 and it represents the age group of each building (say: &lt;5years, 5-10 years, >10 years old).</li>
<li>The variable â€œCountâ€ is the dependent variable (Y) and it represents the number of each building group of each type in each sub-region at any specific year.</li>
<li>The variable â€œPopulationâ€ is one of the independent variables (X1) and it represent the population size in each sub-region. Note: it has the same value for each sub-region at any specific year. However it varies from sub-region to sub-region and in each sub-region across years.</li>
<li>The variable â€œCostâ€ is another independent variable (X2) and it represent construction cost in each year. Note: it has the same value for all sub-regions at any specific year. However it varies across years.</li>
</ul>

<p>The idea is to develop a regression model (Y= a + b1*X1 + b2*X2) while taking into account the hierarchical (<strong>multilevel</strong>) nature of the data. </p>

<p>In multilevel models (the classic example of students within classes within schools within districts), each level is defined by a unique set of observations.  For instance, classesâ€™ IDs and attributes vary from one school to another. However, in my example each sub-region has the same number and IDs of building types â€“same thing for groups within types. </p>

<p>My first question is: can I treat this data as multilevel data and accordingly develop a multilevel model with three levels (sub-region, type, and group) while considering the longitudinal nature of the data by adding a random effect (intercept and slope) for years? For instance, in R using <code>nlme</code>, develop a model:</p>

<pre><code>Model &lt;- lme(fixed= log(Count)~Year+log(Population)+Cost, 
             random=list(Sub-region =~year, Type=~1, Group=~1), 
             correlation= corAR1(form=~Year|Sub-region/Type/Group), data=mydata)
</code></pre>

<p>Alternatively,</p>

<pre><code>Model &lt;- lme(fixed= log(Count)~Year+log(Population)+Cost, random=~Year|Sub-region/Type/Group, correlation= corAR1(form=~Year|Sub-region/Type/Group), data=mydata)
</code></pre>

<p>The second question is related to applying this model: suppose we want to estimate / forecast the number of buildings in group 3 of type 1 for sub-region 2 then the model formula should be:</p>

<pre><code>Y|(sub-region=2, type=1, group=3) = [fixed effect: intercept] + 
  [fixed effect: Year]*(Year) + [fixed effect: Population]*(log(Population)) + 
  [fixed effect: Cost] *(Cost) + [random effect: intercept for sub-region 2] + 
  [random effect: Year for sub-region 2] *(Year) + 
  [random effect: intercept of type 1 for sub-region 2] + 
  [random effect: intercept of group 3 of type 1 for sub-region 2] 
= number of buildings in group 3 of type 1 for sub-region 2
</code></pre>

<p>What are the other alternatives? Can I model each type / group (6 different models, estimated separately), each as a multilevel model where data is clustered within sub-regions and the longitudinal nature of the data is considered (especially in this case where the independent variables do not vary across the type / groups (the case when SUR is the same as OLS))?</p>
"
"0.173968166256234","0.180136324713988","172904","<p>I am trying to build a regression model for the forecast of stock market returns. The regression takes about <strong>100 variables</strong> as input and my training data consists of <strong>n=234</strong> weekly data points. I am using a lasso regression for the regression + variable selection.</p>

<p>My problem is that before I can input the data into the regression I need to find the ""optimal lag"" time for it. The variables can be <strong>categorised into 4 groups</strong> and I have got data for the variables up to <strong>113 weeks before the first point in time of my training data</strong>. To calculate the ""optimal"" lag time I tried following approach:</p>

<p>Take every variable for one categorie and apply a rolling window to the training data, which moves forward in quarters e.g. datapoints 1:13,14:26,... This gives <strong>18 seperate</strong> quarters for the training data. For the first quarter calculate the correlation (spearman in this case) between the stock course and every variable in the categorie with a lag of 0. Then take the average of the absoulte correlation of all the variables for this quarter and enter it into a matrix at point [1,1]. Repeat this for every possible lack time up to 113 and then move on to the next quarter. </p>

<p>This results in a <strong>[114,18] matrix</strong> which includes the average absolute correlation for every lag in every quarter. Now I just take the average for every row in the matrix and the row with the highest average correlation should give me the most reliable lag for my data of that category.</p>

<p><strong>Q:</strong> Has anyone seen a similar approach like this in literature before? Is there something I am missing? E.g. would it be smarter to just calculate the correlation over the whole trainingset at once and look for the optimal lag that way? As far as I am concerned that is the most common approach in literature, but it seems to me that it gives you a pretty biased result.</p>

<p>For some more insight you can find my code below:</p>

<pre><code>Correlation.Maximiser = function(Stock, Category){

  Result = matrix(nrow = 114,ncol =  18)

  for(tmp in 1:18){
    Start.Test=1+(tmp-1)*13
    End.Test=13+(tmp-1)*13
    Sample = Stock[Start.Test:End.Test]

    for(i in 0:113){
      int.low=101-i+13*tmp
      int.high=113-i+13*tmp
      NA.omitter = cor(Sample,Category[int.low:int.high,-1], method = ""spearman"")
      NA.omitter[is.na(NA.omitter)] = 0 #some Variables have a lot of 0 values so that it can happen that they are only zero in the quarter we look at which results in an NA
      Result[i+1,tmp]=mean(abs(NA.omitter))
    }
  }  
  return(Result)
}
</code></pre>

<p><strong>EDIT:</strong> I just realised that the comparison in categories makes no real sense, since some variables might have a positive while others might have a negativ correlation to the dependent variable. I tried to compensate that by averaging over the absolute correlation, but I realised that this would not penalise variables that change sign from quarter to quarter and such behavior would be really bad for the regression. In general my question stays the same though. Only now the procedure is applied to every variable on its own.</p>
"
"0.0484501583111509","0.0714893875923587","173828","<p>I have a data set shown below, the 1st, 2nd,3rd column are dependent variable(dv), and 2 independent variables (iv1 &amp; iv2) respectively, I expected the regression coefficient of the ""iv1"" shows a positive value, as there is a positive correlation between dv and iv1. However, The result shows a negative regression coefficient for iv1 (beta_iv1 = -0.55), I am wondering why this happened, I appreciate if anyone can help.</p>

<p>dv    iv1     iv2</p>

<p>1     0.00    7.70<br>
1     2.90    0.00<br>
1     0.00    7.70<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
1     1.50    7.70<br>
1     5.70    0.50<br>
1     7.10    2.30<br>
1     5.70    4.10<br>
1     0.00    4.10<br>
1     4.30    4.10<br>
1     0.00    10.00<br>
1     0.00    4.10<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
0     0.00    9.50<br>
1     0.00    5.90<br>
1     0.00    4.10<br>
1     1.50    5.90<br>
1     5.70    2.30<br>
1     1.50    0.00<br>
0     0.00    10.00<br>
1     5.70    0.00<br>
1     5.70    0.50<br>
1     4.30    2.30<br>
0     0.00    10.00<br>
1     2.90    5.90<br>
1     0.00    5.90<br>
1     0.00    5.90<br>
1     2.90    2.30<br>
1     1.50    2.30<br>
1     2.90    0.50<br>
1     5.70    4.10<br>
1     1.50    0.00<br>
1     0.00    7.70  </p>

<p>I run this using R with package ""logistf"" which overcomes separation problem of logistic regression. The code I run this data set is as below:</p>

<blockquote>
  <p>library(logistf);<br>
      tempT=read.table(fileS);<br>
     fit&lt;-logistf(dv ~ iv1+iv2, data=tempT);</p>
</blockquote>

<p>and the result shows below:  </p>

<pre><code>           coef  se(coef) lower 0.95  upper 0.95     Chisq      p
</code></pre>

<blockquote>
  <p>(Intercept)  9.0086382 5.1741382   1.650380 61.61244068 7.6897111 
  0.005553652<br>
  tempT[, 2]  -0.5509122 0.6567110  -6.013208  1.55280975 0.5490404 0.458710039<br>
  tempT[, 3]  -0.9051062 0.5597601  -6.317335 -0.06328166 4.8315401 0.027943657</p>
</blockquote>

<p>Likelihood ratio test=7.213821 on 2 df, p=0.02713555, n=35</p>
"
"0.125877203744503","0.123823251511949","174057","<p>This is probably an embarrassingly easy question, but where else can I turn to... </p>

<p>I'm trying to put together examples of regression with mixed effects using <code>lmer</code> {lme4}, so that I can present [R] code that automatically downloads toy datasets in Google Drive and run every instance in <a href=""http://stats.stackexchange.com/a/13173/67822"">this blockbuster post</a>. </p>

<p>And starting with the first case (i.e. <code>V1 ~ (1|V2) + V3</code>, where <code>V3</code> is a continuous variable acting as a fixed effect, and <code>V2</code> is <code>Subjects</code>, both trying to account for <code>V1</code>, a continuous DV), I was expecting to retrieve different intercepts for each one of the <code>Subjects</code> and a single slope for all of them. Yet, this was not the case consistently.</p>

<p>I don't want to bore you with the origin or meaning of the datasets below, because I'm sure most of you get the idea without much explaining. So let me show you what I get... If you're so inclined you can just copy and paste in [R]... it should work if you have {lme4} in your Environment:</p>

<h1>Expected Output:</h1>

<pre><code>politeness &lt;- read.csv(""http://www.bodowinter.com/tutorial/politeness_data.csv"")
head(politeness)

  subject   gender scenario  attitude frequency
1      F1      F        1      pol     213.3
2      F1      F        1      inf     204.5
3      F1      F        2      pol     285.1
4      F1      F        2      inf     259.7    


library(lme4)

fit &lt;- lmer(frequency ~ (1|subject) + attitude, data = politeness)

coefficients(fit)
            $subject
               (Intercept) attitudepol
            F1    241.1352   -19.37584
            F2    266.8920   -19.37584
            F3    259.5540   -19.37584
            M3    179.0262   -19.37584
            M4    155.6906   -19.37584
            M7    113.2306   -19.37584
</code></pre>

<h1>Surprising Output:</h1>

<pre><code>library(gsheet)
recall &lt;- read.csv(text = 
    gsheet2text('https://drive.google.com/open?id=1iVDJ_g3MjhxLhyyLHGd4PhYhsYW7Ob0JmaJP8MarWXU',
              format ='csv'))
head(recall)

 Subject Time Emtl_Value Recall_Rate Caffeine_Intake
1     Jim    0   Negative          54              95
2     Jim    0    Neutral          56              86
3     Jim    0   Positive          90             180
4     Jim    1   Negative          26             200

fit &lt;- lmer(Recall_Rate ~ (1|Subject) + Caffeine_Intake, data = recall)

coefficients(fit)
        $Subject
               (Intercept) Caffeine_Intake
        Jason     51.51206        0.013369
        Jim       51.51206        0.013369
        Ron       51.51206        0.013369
        Tina      51.51206        0.013369
        Victor    51.51206        0.013369
</code></pre>

<p>Here is the output of (<code>summary(fit)</code>):</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: Recall_Rate ~ (1 | Subject) + Caffeine_Intake
   Data: recall

REML criterion at convergence: 413.9

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.54125 -0.98422  0.04967  0.81465  1.83317 

Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept)   0.0     0.00   
 Residual             601.2    24.52   
Number of obs: 45, groups:  Subject, 5

Fixed effects:
                Estimate Std. Error t value
(Intercept)     51.51206    5.92408   8.695
Caffeine_Intake  0.01337    0.03792   0.353

Correlation of Fixed Effects:
            (Intr)
Caffen_Intk -0.787
</code></pre>

<h1>Question:</h1>

<p><strong>Why are all the Intercepts for the different subjects the same in the second example? The structure of the datasets and the <code>lmer</code> syntax appear very similar... and the boxplots don't seem to support the result:</strong></p>

<p><a href=""http://i.stack.imgur.com/xXYdS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xXYdS.png"" alt=""enter image description here""></a></p>

<p>Thank you in advance!</p>
"
"0.0726752374667264","0.0714893875923587","174071","<p>I'm using lme4 in R, and I have a model set up that uses a three-level hierarchy for a negative binomial regression.</p>

<p>There is previously a question (<a href=""http://stats.stackexchange.com/questions/166699/how-compute-the-intra-class-correlation-for-a-negative-binomial-mixed-model-in-l"">How compute the Intra-Class Correlation for a Negative Binomial Mixed Model in lme4</a>) that addresses how to calculate ICC for a 2-level negative binomial model, but it's unclear to me how to include the third level.</p>
"
"0.22642822184971","0.237103475136143","174257","<p>I want to do a path analysis with lavaan but encounter a few problems and would appreciate any help.</p>

<p>The structural model looks like this:</p>

<p><a href=""http://i.stack.imgur.com/y8ZZh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y8ZZh.png"" alt=""structural model""></a></p>

<p>The relation between one observed independent (s) and one observed dependent variable (v) is mediated through a latent variable (m) that is defined by two observed indicator variables (x1, x2). This is basically a simplified version of the <a href=""http://lavaan.ugent.be/tutorial/sem.html"" rel=""nofollow"">SEM example</a> in the tutorial on the lavaan project website.</p>

<p>When I enter my code (given further below) into R, I encounter two problems:</p>

<p>(1) The results change when I change the order of the indicator variables.</p>

<p>This model:</p>

<pre><code># measurement model
    m =~ x1 + x2
</code></pre>

<p>returns a different result than this model:</p>

<pre><code># measurement model
    m =~ x2 + x1
</code></pre>

<p>How can that be? Isn't the order of the indicators arbitrary? And if not, how do I know which is the correct order, if my model does not presuppose a specific order?</p>

<p>(2) There are a few warnings that I don't understand: for the first model, no standard errors could be computed; and the second model did not ""converge"" (whatever that means). The warnings are given in context in the full code posted below.</p>

<p>What do I have to do to obtain reliable estimates?</p>

<hr>

<p>Here is the full R output to provide context to my questions.</p>

<pre><code># data

s &lt;- c(2, 5, 4, 4, 4, 8, 2, 9, 1, 1, 3, 3, 2, 3, 2, 5, 5, 7, 4, 7, 8, 4, 10, 10, 2, 4, 0, 2, 4, NA, 1, 5, 2, 6, 3, 5, 0, 5, 3, 6, 4, 9, 4, 9, 4, 5, 6, 1, 8, 0, 6, 9, 1, 5, 1, 6, 2, 5, 0, 5, 6, 2, 4, 10, 3, 4)
v &lt;- c(8, 10, 1, 4, 0, 2, 3, 2, 1, 1, 2, 5, 1, 5, 0, 5, 4, 5, 2, 10, 0, 6, 5, 5, 6, 1, 1, 0, 0, NA, 1, 0, 1, 8, 1, 3, 0, 5, 6, 3, 2, 10, 0, 5, 5, 10, 4, 1, 1, 0, 0, 0, 2, 10, 1, 8, 2, 3, 2, 2, 4, 4, 2, 5, 6, 2)
x1 &lt;- c(2.500000, 3.789474, 1.514563, 5.846868, 4.588235, 5.600000, 5.066667, 11.647059, 2.000000, NA, 4.461538, 18.000000, 1.058824, 9.217391, 27.840000, 15.375000, NA, 6.000000, 9.714286, 12.484848, 16.503497, 20.666667, 3.500000, 4.658824, 4.750000, 4.000000, 2.800000, 14.228571, 11.000000, NA, 2.666667, 3.764706, 4.705882, 13.272727, 2.000000, 18.444444, 17.555556, 14.222222, 2.000000, 4.000000, 8.461538, 19.200000, 13.902439, 13.000000, 3.000000, NA, 7.360000, 1.611374, 1.500000, 3.365854, 22.375000, 10.838710, 2.923077, 3.488372, 5.176471, 37.666667, 1.176471, 7.454545, 36.235294, 6.823529, 2.222222, 6.133333, 11.428571, 42.705882, 28.105263, 18.333333)
x2 &lt;- c(8.125000, 14.273684, 7.339806, 23.387471, 113.058824, 22.200000, 17.466667, 43.647059, 9.230769, NA, 13.538462, 83.555556, 5.058824, 37.391304, 100.000000, 59.250000, NA, 22.470588, 38.428571, 50.787879, 76.223776, 92.888889, 15.375000, 16.235294, 18.875000, 13.647059, 10.133333, 55.885714, 36.428571, NA, 6.933333, 13.294118, 14.117647, 81.818182, 6.117647, 67.777778, 76.333333, 51.888889, 6.428571, 14.200000, 34.000000, 59.680000, 68.634146, 40.500000, 12.250000, NA, 29.760000, 8.909953, 5.400000, NA, 71.125000, 39.741935, 9.846154, 13.116279, 18.823529, 204.000000, 4.588235, 49.090909, 188.470588, 19.647059, 10.222222, 22.933333, 38.285714, 140.235294, 137.526316, 79.000000)
dat &lt;- data.frame(cbind(s, v, x1, x2))

# first model

model &lt;- '
    # measurement model
        m =~ x2 + x1
    # regressions
        m ~ s
        v ~ s + m
    # residual correlations
        x1 ~~ x2
'
fit &lt;- sem(model, data = dat, missing = ""fiml"")

# Warning messages:
# 1: In lav_data_full(data = data, group = group, group.label = group.label,  :
#   lavaan WARNING: some cases are empty and will be removed:
#   30
# 2: In lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats,  :
#   lavaan WARNING: could not compute standard errors!
#   lavaan NOTE: this may be a symptom that the model is not identified.

summary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)

# lavaan (0.5-18) converged normally after 147 iterations
#
#                                                   Used       Total
#   Number of observations                            65          66
#
#   Number of missing patterns                         3
#
#   Estimator                                         ML
#   Minimum Function Test Statistic                0.565
#   Degrees of freedom                                 0
#   Minimum Function Value               0.0043451960201
#
# Model test baseline model:
#
#   Minimum Function Test Statistic              126.904
#   Degrees of freedom                                 6
#   P-value                                        0.000
#
# User model versus baseline model:
#
#   Comparative Fit Index (CFI)                    0.995
#   Tucker-Lewis Index (TLI)                       1.000
#
# Loglikelihood and Information Criteria:
#
#   Loglikelihood user model (H0)               -797.558
#   Loglikelihood unrestricted model (H1)       -797.275
#
#   Number of free parameters                         12
#   Akaike (AIC)                                1619.115
#   Bayesian (BIC)                              1645.208
#   Sample-size adjusted Bayesian (BIC)         1607.435
#
# Root Mean Square Error of Approximation:
#
#   RMSEA                                          0.000
#   90 Percent Confidence Interval          0.000  0.000
#   P-value RMSEA &lt;= 0.05                          1.000
#
# Standardized Root Mean Square Residual:
#
#   SRMR                                           0.027
#
# Parameter estimates:
#
#   Information                                 Observed
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(&gt;|z|)   Std.lv  Std.all
# Latent variables:
#   m =~
#     x2                1.000                              14.272    0.330
#     x1                0.384                               5.482    0.588
#
# Regressions:
#   m ~
#     s                 1.732                               0.121    0.323
#   v ~
#     s                 0.335                               0.335    0.306
#     m                 0.012                               0.171    0.059
#
# Covariances:
#   x2 ~~
#     x1              292.112                             292.112    0.951
#
# Intercepts:
#     x2               35.558                              35.558    0.823
#     x1                7.220                               7.220    0.775
#     v                 1.761                               1.761    0.604
#     m                 0.000                               0.000    0.000
#
# Variances:
#     x2             1663.119                            1663.119    0.891
#     x1               56.783                              56.783    0.654
#     v                 7.591                               7.591    0.892
#     m               182.367                               0.895    0.895
#
# R-Square:
#
#     x2                0.109
#     x1                0.346
#     v                 0.108
#     m                 0.105

model &lt;- '
    # measurement model
        m =~ x1 + x2
    # regressions
        m ~ s
        v ~ s + m
    # residual correlations
        x1 ~~ x2
'
fit &lt;- sem(model, data = dat, missing = ""fiml"")

# Warning messages:
# 1: In lav_data_full(data = data, group = group, group.label = group.label,  :
#   lavaan WARNING: some cases are empty and will be removed:
#   30
# 2: In lavaan::lavaan(model = model, data = dat, missing = ""fiml"", model.type = ""sem"",  :
#   lavaan WARNING: model has NOT converged!

summary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)

# ** WARNING ** lavaan (0.5-18) did NOT converge after 9438 iterations
# ** WARNING ** Estimates below are most likely unreliable
#
#                                                   Used       Total
#   Number of observations                            65          66
#
#   Number of missing patterns                         3
#
#   Estimator                                         ML
#   Minimum Function Test Statistic                   NA
#   Degrees of freedom                                NA
#   P-value                                           NA
#
# Parameter estimates:
#
#   Information                                 Observed
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(&gt;|z|)   Std.lv  Std.all
# Latent variables:
#   m =~
#     x1                1.000                               0.526    0.056
#     x2             1606.326                             845.326   19.343
#
# Regressions:
#   m ~
#     s                -0.001                              -0.001   -0.004
#   v ~
#     s                 0.355                               0.355    0.325
#     m                 0.004                               0.002    0.001
#
# Covariances:
#   x1 ~~
#     x2              -69.375                             -69.375   -0.009
#
# Intercepts:
#     x1               10.099                              10.099    1.083
#     x2               48.281                              48.281    1.105
#     v                 1.761                               1.761    0.604
#     m                 0.000                               0.000    0.000
#
# Variances:
#     x1               86.614                              86.614    0.997
#     x2            -712666.446                            -712666.446 -373.157
#     v                 7.617                               7.617    0.895
#     m                 0.277                               1.000    1.000
#
# R-Square:
#
#     x1                0.003
#     x2                   NA
#     v                 0.105
#     m                 0.000
# Warning message:
# In .local(object, ...) :
#   lavaan WARNING: fit measures not available if model did not converge
</code></pre>

<hr>

<p><em>Note.</em> I have posted the same question to the <a href=""https://groups.google.com/forum/#!forum/lavaan"" rel=""nofollow"">lavaan Google Group</a>, but this is part of my bachelor's thesis, which I have to turn in on Monday, so I'm a bit pressed for time and hope you forgive me for crossposting.</p>
"
"0.113860214858083","0.124447050177862","175770","<p>This question is more of theoretical. I am not sure if this is the right place, but still giving it a try. </p>

<p>I have two variables &mdash; direct cost and indirect cost. When sales persons go for a sales pitch to a customer they know about direct cost that they are going to incur for this service, but they don't know much about indirect cost (they will come to know about it in latter stages). An estimate of indirect cost at this stage will be valuable for sales persons. </p>

<p>I am trying to predict indirect cost as a function of direct cost. I am doing this via a simple linear regression. I plotted scatter plot between direct cost and indirect cost and see a <strong>good linear relationship</strong> between them. I also see that direct cost and indirect cost are <strong>highly corelated</strong> to each other with correlation coefficient as 0.98, so I expected a very good prediction accuracy. But surprisingly, my prediction accuracy is not so good. I have around 200,000 points in my training data and average prediction error on training data is 17 %. Though adjusted R-Square value is 0.97. I am using <code>lm()</code> function from R.       </p>

<p>My question is that in case of simple linear regression, in general, should we expect better prediction accuracy if dependent and independent variables are highly correlated or is it my misconception? If we expect good accuracy, am I missing something here. Please note that I have also tried centering these variables around mean. </p>
"
"0.162825100202661","0.170178781630396","176111","<p>I need to conduct a meta-analysis for a publication, but this is my first meta-analysis and I still donâ€™t feel confident. I will describe the steps I have followed, and hopefully some of you might find errors on my methods, and suggest alternatives.</p>

<p>For this particular analysis, long-term studies should be more important than short-term because a few experiments showed transient effects, hence short-term studies might fail to capture that the effect is not really significant in the long-term. On my dataset, about half the studies have several non-independent measurements taken at different time-points (i.e. several annual measurements). Other experiments, despite having been carried out for several years, show the data already aggregated, with only one row per study with mean and standard deviation. I considered running a multivariate meta-analysis to solve this issue, but it would unbalance the analysis, giving more importance to the experiments with several rows of data (annual measurements) than to the experiments with aggregated data in only one row (pooled across several years). Am I right? This is an example of the dataset:</p>

<ul>
<li>Study 1, Year 1, Effect Size 1 </li>
<li>Study 1, Year 2, Effect Size 2 </li>
<li>Study 1, Year 3, Effect Size 3    </li>
<li>Study 2, Year 1, Effect Size 4    </li>
<li>Study 3, Years 1-4, Effect Size 5</li>
<li>Study 4, Years 1-3, Effect Size 6</li>
</ul>

<p>Alternatively, I decided to try and aggregate the data, so that finally there is only one row per study. I followed these steps:</p>

<p>Calculate effect sites for each row, including those studies with several rows (annual data). In this case, I calculated the log response ratio (ROM):</p>

<pre><code>dat &lt;- escalc (measure=""ROMâ€, n1i=elev.rep, n2i=control.rep, m1i=elev.ANPP.mean, m2i=control.ANPP.mean, sd1i=elev.SD, sd2i=control.SD, data=all)
</code></pre>

<p>Aggregate studies using the function agg {MAd}. I used the Borenstein et al. 2009 method, and correlation=1:</p>

<pre><code>datAgg &lt;- agg(id = id,es = yi,var = vi, cor =1,method = ""BHHR"", data = dat)
</code></pre>

<p>I have now only one row per study. However, since long-term experiments are more important, I have created user-defined weights that take into account the number of replicates and the number of years of each study:</p>

<pre><code>datAgg$weightsTime &lt;- with(datAgg, ((control.rep * elev.rep)/(control.rep + elev.rep)) + ((nyears^2)/(2*nyears)))
</code></pre>

<p>Run the mixed-effects meta-regression with two moderators, using Hedges Estimator (HE) and the Knapp and Hartung approach:</p>

<pre><code>m &lt;- rma.uni(yi, vi, mods= ~ factor(A) * factor(B), method=""HE"", data=datAgg, weights=weightsTime, knha=TRUE)
</code></pre>

<p>Am I doing something wrong? Can this method be improved? So far the results confirm my hypothesis, but of course I might be using a sub-optimal approach. Many thanks</p>
"
"0.0856485887284415","0.101101261498861","177388","<p>I am a beginner and trying to learn new concepts in statistical analysis.</p>

<p>I have some very basic question. With a given data set of individuals I am trying to ascertain as to whether or not they are eligible to be granted for a loan - credit scoring.</p>

<p>My data set has 300 obs of 10 variables which I figured out using:
str(data)
summary(data):</p>

<p><a href=""http://i.stack.imgur.com/2q8Dl.png"" rel=""nofollow"">Detailed Data Desc</a></p>

<p>After applying linear regression on all variables:  </p>

<pre><code>linreg=lm(Rating~.,data=data)  
cor(linreg$fitted.values,data$Rating)
</code></pre>

<p><a href=""http://i.stack.imgur.com/DtbQk.png"" rel=""nofollow"">Linear Regression detailed</a></p>

<p><strong>I understand:</strong>  </p>

<ol>
<li>Having 3 stars - p value means very significant and high and positive Estimates indicats that it has a positive significance and vice versa  </li>
<li>The correlation between fitted values and Rating comes to  0.9867324</li>
</ol>

<p><strong>Questions:</strong>  </p>

<ol>
<li>Does this mean regression predicts correctly 98.67% of the observations  </li>
<li>If everything else are equal then variables like Education and Gender have a positive impact on the rating? Because they have lowest p-values.  </li>
<li>What about Student and Income variables?  </li>
<li>Also does it mean individuals with high Income will have a greater rating, everything else being equal?  </li>
</ol>
"
"0.0593390829096927","0.0583708405417779","178657","<p>I have a data set; sample size is 16, the number of  independent variable is 18 
and one dependent variable . there are correlations between independent variables. I want to conduct Monte Carlo according the linear regression 
relationship of these variables but some of independent variables do not follow  known distributions. in this regard, can anyone suggest a way ?</p>

<p>Thank you in advance</p>

<p>(I read this article <a href=""http://www.iaees.org/publications/journals/ces/articles/2011-1%284%29/a-fitter-use-of-Monte-Carlo-simulations.pdf"" rel=""nofollow"">article</a>, but I have no experience with simulation)</p>
"
"0.0938233281301002","0.0922924025252575","178944","<p><strong>Context</strong></p>

<p>I'm attempting to understand how R's coxph() accepts and handles repeated entries for subjects (or patient/customer if you prefer). Some call this Long format, others call it 'repeated measures'.</p>

<p>See for example the data set that includes the ID column in the Answers section at:</p>

<p><a href=""http://stats.stackexchange.com/questions/143340/best-packages-for-cox-models-with-time-varying-covariates/162621?noredirect=1#comment338858_162621"">Best packages for Cox models with time varying covariates</a></p>

<p><em>Also assume covariates are time-varying throughout and there is exactly one censor (i.e. event) variable, which is binary.</em></p>

<p><strong>Questions</strong></p>

<p>1) In the above link's answer, if ID is not given as a parameter in the call to coxph() should the results be the same as including cluster(ID) as a parameter in coxph()? </p>

<p>I attempted to search for documentation, but the following doesn't seem to clearly address (1):
<a href=""https://stat.ethz.ch/pipermail/r-help//2013-July/357466.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help//2013-July/357466.html</a></p>

<p>2) If the answer to (1) is 'no', then (mathematically) why? It seems cluster() in coxph() seeks correlations between subjects as per subsection 'cluster' on pg. 20 at</p>

<p><a href=""https://cran.r-project.org/web/packages/survival/survival.pdf"" rel=""nofollow"">https://cran.r-project.org/web/packages/survival/survival.pdf</a></p>

<p>3) Vague question: how does coxph() with repeated measures compare to R's frailtypack regression methods? </p>

<p><strong>Addenda</strong></p>

<p>The following hints at using cluster(ID):</p>

<p><a href=""http://stats.stackexchange.com/questions/10051/is-there-a-repeated-measures-aware-version-of-the-logrank-test"">Is there a repeated measures aware version of the logrank test?</a></p>

<p>as does:</p>

<p><a href=""https://stat.ethz.ch/pipermail/r-help//2013-July/357466.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help//2013-July/357466.html</a></p>

<blockquote>
  <p>GEE approach: add ""+ cluster(subject)"" to the model statement in coxph
  Mixed models approach: Add "" + (1|subject)"" to the model statment in coxme.</p>
</blockquote>

<p>Thanks in advance!</p>
"
"0.10277830647413","0.101101261498861","179751","<p>In generating a logistic regression model for a data set of c.8000 observations over c.40 variables (not including interactions) I am trying to figure the relations between the variables. Most variables are binary. 
The purpose is to be able to combine different variables which explains the same variance in the data. 
I have done some web-research of different possibilities but have found their contribution to the process somehow limited. Most of the 2x2 tests points out their is significantly no correlation between any of the pairs. The reason is, as I understand it, that the numbers are relatively big, for instance:</p>

<p><a href=""http://i.stack.imgur.com/leP3C.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/leP3C.png"" alt=""enter image description here""></a></p>

<p>It seems like their is some kind of correlation between both sets though due to the big data sets differences are significant. </p>

<p>Is there any suggestion of how to capture even low value correlations between binary variables? or other ways to understand these relations? </p>

<p>Many thanks.</p>
"
"0.0938233281301002","0.0922924025252575","181489","<p>I have to estimate a number of regressions where a lot of autcorrelation is present. Now, for historical reasons, this autocorrelation is resolved using an iterative Prais-Winsten estimation (a modification of the Cochraneâ€“Orcutt estimation). I have found some R-code which performs this procedure:</p>

<pre><code>    prais.winsten.lm &lt;- function(mod){
    X &lt;- model.matrix(mod)
    y &lt;- model.response(model.frame(mod))
    e &lt;- residuals(mod)
 n &lt;- length(e)
 names &lt;- colnames(X)
 rho &lt;- sum(e[1:(n-1)]*e[2:n])/sum(e^2)
 y &lt;- c(y[1] * (1 - rho^2)^0.5, y[2:n] - rho * y[1:(n-1)])
 X &lt;- rbind(X[1,] * (1 - rho^2)^0.5, X[2:n,] - rho * X[1:(n-1),])
 mod &lt;- lm(y ~ X - 1)
 result &lt;- list()
 result$coefficients &lt;- coef(mod)
     names(result$coefficients) &lt;- names
 summary &lt;- summary(mod, corr = F)
 result$cov &lt;- (summary$sigma^2) * summary$cov.unscaled
     dimnames(result$cov) &lt;- list(names, names)
 result$sigma &lt;- summary$sigma
 result$rho &lt;- rho
 class(result) &lt;- 'prais.winsten'
 result
 }
</code></pre>

<p>Now, this code works fine when all the other regressors are exogeneous. But, in my case, a part of X is endogeneous turning the standard ols regression performed in the above code not usable.</p>

<p>I was thinking about modifying the above code into the following:</p>

<pre><code>  prais.winsten.plm &lt;- function(mod){
  X &lt;- model.matrix(mod,component=""projected"")
  Z &lt;- model.matrix(mod,component=""instruments"")
  y &lt;- model.response(model.frame(mod))
  e &lt;- residuals(mod)
  n &lt;- length(e)
  names &lt;- colnames(X)
  rho &lt;- sum(e[1:(n-1)]*e[2:n])/sum(e^2)
  y &lt;- c(y[1] * (1 - rho^2)^0.5, y[2:n] - rho * y[1:(n-1)])
  X &lt;- rbind(X[1,] * (1 - rho^2)^0.5, X[2:n,] - rho * X[1:(n-1),])
  Z &lt;- rbind(Z[1,] * (1 - rho^2)^0.5, Z[2:n,] - rho * Z[1:(n-1),])
  mod &lt;- ivreg(y ~ X -1|Z)
  result &lt;- list()
  result$coefficients &lt;- coef(mod)
      names(result$coefficients) &lt;- names
  summary &lt;- summary(mod, corr = F)
  cov &lt;- (summary$sigma^2) * summary$cov.unscaled
  result$se&lt;- sqrt(diag(cov))
      dimnames(cov) &lt;- list(names, names)
      result$sigma &lt;- summary$sigma
      result$rho &lt;- rho
  class(result) &lt;- 'prais.winsten'
  result
  }
</code></pre>

<p>I have however no idea if such an approach is correct, especially since I found no similar ways in dealing with this problem.</p>
"
"0.145350474933453","0.142978775184717","181665","<p>My question relates to calculating the uncertainty associated with the estimation of slopes in a varying intercept, varying slope hierarchical model.</p>

<p>I would like to calculate the effect of a treatment in different districts. If I ran a simple linear regression in which there was no pooling at district level, my model would look something like:</p>

<pre><code>    fm1 &lt;- lm(response ~ treatment*district)
</code></pre>

<p>I could then find the effect of treatment in district j by summing the coefficient for treatment with that of the interaction term treatment:districtj, and the standard error for that estimate could be found with:</p>

<pre><code>    sqrt(vcov(fm1)[2,2] + vcov(fm1)[4,4] + 2*vcov(fm1)[2,4])
</code></pre>

<p>I get stuck when moving to a multilevel model with random slopes along the lines of:</p>

<pre><code>    fm2 &lt;- lmer(response ~ treatment + (1 + treatment | district))
</code></pre>

<p>I can again find the effect of treatment in district j easily by summing the coefficient for treatment with the random slope estimate for district j. I would like to account for the uncertainty of both coefficients and I'm not sure how to do that. I can use the arm package to extract the standard errors associated with the slope estimates for each district and I know the standard error of the treatment estimate but I don't know how to estimate their correlation.  </p>

<p>It seems like this should be an easy question to answer but I haven't been able to find it. Hoping someone can point me in the right direction.</p>
"
"0.0839181358296689","0.0825488343412996","182689","<p>My name is Ashley. I'm working on the analyses for my dissertation which involves a meta-analysis of 4 predictors, 1 mediator, and one outcome. So far, I've calculated the meta-analytic correlation matrix between predictors and outcome(s) and harmonic mean of N to run analyses on the model level. 
Out of two major studies published in my area of the social sciences (DeChurch &amp; Mesmer-Magnus, 2010; Joseph et al., 2015), this is all of the information provided for running meta-regression with more than one predictor. No programs are specified and no other matrices are indicated. </p>

<p>I'm having trouble identifying how to run the multivariate metaregression analysis. From what I've found so far, mvmeta package in R is the closest that I've come to identifying a program/package that will produce the estimates that I need. However, I would have to calculate corrected rho for each study/predictor-outcome relationship individually. Also, there is no place to indicate harmonic mean of N. And because of this, I'm skeptical in using this package. </p>

<p>Is anyone aware of another R package, SPSS macro or other statistical software program/package that can handle this type of analysis? Or is mvmeta the best bet?</p>
"
"0.0750586625040802","0.0922924025252575","184341","<p>I am using the Deming function provided by Terry T. on <a href=""http://www.mail-archive.com/r-help@r-project.org/msg85070.html"">this archived r-help thread</a>.  I am comparing two methods, so I have data that look like this:</p>

<pre><code>y  x     stdy   stdx
1  1.2   0.23   0.67
2  1.8   0.05   0.89
4  7.5   1.13   0.44
... ...  ...   ...
</code></pre>

<p>I have done my Deming regression (also called ""total least squares regression"") and I get a slope and intercept. I would like to get a correlation coefficient so I've start calculating the $R^2$. I have manually entered the formula: </p>

<pre><code>R2 &lt;- function(coef,i,x,y,sdty){
    predy    &lt;- (coef*x)+i
    stdyl    &lt;- sum((y-predy)^2)   ### The calculated std like if it was a lm (SSres)
    Reelstdy &lt;- sum(stdy)          ### the real stdy from the data  (SSres real)
    disty    &lt;- sum((y-mean(y))^2) ### SS tot
    R2       &lt;- 1-(stdyl/disty)    ### R2 formula
    R2avecstdyconnu &lt;- 1-(Reelstdy/disty) ### R2 with the known stdy
    return(data.frame(R2, R2avecstdyconnu, stdy, Reelstdy))
}
</code></pre>

<p>This formula works and gives me output.</p>

<ul>
<li>Which of the two $R^2$s makes more sense? (I personally think of both of them as kind of biased.)  </li>
<li>Is there a way to get a correlation coefficient from a total least squared regression?</li>
</ul>

<p>OUTPUT FROM THE DEMING REGRESSION:</p>

<pre><code>Call:
deming(x = Data$DS, y = Data$DM, xstd = Data$SES, ystd = Data$SEM,     dfbeta = T)

               Coef  se(coef)         z            p
Intercept 0.3874572 0.2249302 3.1004680 2.806415e-10
Slope     1.2546922 0.1140142 0.8450883 4.549709e-02

   Scale= 0.7906686 
&gt; 
</code></pre>
"
"0.0938233281301002","0.0922924025252575","186240","<p>Hi I am trying a mediation analysis (using library(""mediation"") in R)</p>

<p>My model has 3 predictors and one mediator (n=455), but I am only interested in predictor 1. There is some collinerarity between predictor 1 and 2 - 0.383444 (Pearson). No collinerarity between predictor 3 and the others. The Mediator is correlated with IV1 and slightly with IV2. Predictors, Mediator and dependent variable are all continuous.</p>

<pre><code>lm(DV ~ IV1 + IV2 + IV3 , data = data)
</code></pre>

<p>Only IV2 is significant, R2 = 0.050</p>

<pre><code>lm(DV ~ Mediator + IV1 + IV2 + IV3 , data = data)
</code></pre>

<p>Mediator and IV2 is significant, R2 = 0.056</p>

<p>I have a much bigger dataset with n = 1200, but unfortunately I don't have Mediator information available for them. If I do a linear regression to predict DV with this dataset, IV1 and IV2 are both highly significant, the standardized beta meaningful.</p>

<ol>
<li><p>With this information can I investigate the mediating effect of the mediator on IV1 with my small dataset with 455 subjects (using the mediate()-Function of the ""mediation""-package in R) , even though the dataset itself is too small to show a significant effect of IV1 on the DV?</p></li>
<li><p>Also, I was wondering whether my mediator might mediate IV2-effect. The correlation between IV1 and the mediator is higher than between IV2 and the mediator though. </p></li>
</ol>

<p>I am thankful for any ideas.</p>
"
"0","0.0412744171706498","188753","<p>I'm attempting to predict vegetation productivity based on climatic and land use variables (the latter are categorical). I found that there is a multicollinearity problem between the predictors (especially land use) as seen from the Variance Inflation Factor (VIF of the Ordinary Least Squares Regression). </p>

<p>Although my knowledge of lasso regression is basic, I assume lasso regression might solve the multicollinearity problem and also select variables that are driving the system. I appreciate an R code for estimating the standardized beta coefficients for the predictors or approaches on how to proceed.</p>

<pre><code>Variable           Coeff.  Std Coeff.  VIF    Std Error    t      P  Value 
Constant          -0.228   0            0      0.086       -2.644  0.008  
Precipitation      &lt;.001   0.151       2.688   &lt;.001        8.541  0.0  
Solar Rad          0.002   0.343       2.836   &lt;.001        18.939 &lt;.001  
Temp              -0.116  -1.604       28.12   0.004       -28.11  0.0  
Water Stress       0.881   0.391       2.352   0.037        23.7   &lt;.001  
Vapor Pressure     0.135   1.382       30.49   0.006        23.259 0.0    
  1               -0.103   -0.109      52.086  0.074       -1.398  0.162    
  2               -0.14    -0.048      6.49    0.079       -1.761  0.078   
  3               -0.11    -0.048      10.007  0.077       -1.42   0.156    
  4               -0.104   -0.234      236.288 0.073       -1.416  0.157    
  5               -0.097   -0.242      285.244 0.073       -1.331  0.183    
  6               -0.104   -0.09       35.067  0.074       -1.406  0.16    
  8               -0.119   -0.261      221.361 0.073       -1.629  0.103 
ELEVATION          &lt;.001   -0.115      3.917   &lt;.001       -5.381  &lt;.001
Condition Number: 59.833 
Mean of Correlation Matrix: 0.221 1st    
Eigenvalue divided by m: 0.328
</code></pre>
"
"0.0726752374667264","0.0714893875923587","190586","<p>I am using cross correlation to demonstrate a potential link between two time series (ext &amp; co). Both series are strongly autocorrelated, so it is difficult to assess the dependence between the two series. For a quick preliminary analysis, the cross correlation shows a clear (somehow delayed) link between the two time series, although it might spurious. <a href=""http://i.stack.imgur.com/eHUnj.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eHUnj.jpg"" alt=""CCF""></a>. Prewhitening seems to be the best option; I will prewhiten my x variable by fitting an ARIMA process and then use the coefficients to filter my variable y. My question is if I should estimate the coefficients of the ARIMA process (for example using <code>auto.arima</code>) using my series x or by using the residuals of the OLS regression of x on y.</p>
"
"0.0419590679148345","0.0412744171706498","190998","<p>I have to do a multiple regression in order to predict the GDP, using some (or all) of the variables that I have (consumption, investment, govt expenditures, disposable income, price index, money stock, population, t bill, unemployment, inflation, and interest).
The problem is that 7 from the 11 variables that I have are highly correlated with the GDP and between each other (r>0.9). </p>

<p>How can I pick variables to predict the GDP? Should I pick only the one with the highest GDP correlation and then try adding the other variables with lower correlation?</p>
"
"0.0726752374667264","0.0714893875923587","191381","<p>I have a quasi-poisson regression model for analysing the correlation between academic prestige and bulic visibility.
I have a set of independent variables- continouse and dummies in this model.
I would like to create scatter plots which describes this model and maby indicate of an interaction effects.
I understand that poisson models are log-linear models, so I tried to plot scatter plot in which the dependent variable is in log transformation.
However,  I don't know which line to choose- should it be a linear regression line or  loess line (I'm using ggplot2 plots).
for example, in the model, the economics department seem to be more visible then the sociology department. However, in the the plots it seem to be the oposite.
<a href=""http://i.stack.imgur.com/T6XMt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/T6XMt.png"" alt=""this is the plot with lm line""></a>
<a href=""http://i.stack.imgur.com/ZowTL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZowTL.png"" alt=""this is the plot with non-method line""></a></p>

<p>How can I make a reliable plot? </p>
"
"0.303022254370866","0.292658210300274","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.0938233281301002","0.0922924025252575","191851","<p>I am building a VAR model to forecast the price of an asset and would like to know whether my method is statistically sound, whether the tests I have included are relevant and if more are needed to ensure a reliable forecast based on my input variables. </p>

<p>Below is my current process to check for Granger causality and forecast the selected VAR model.</p>

<pre><code>require(""forecast"")
require(""vars"")

#Read Data
da=read.table(""VARdata.txt"", header=T)
dac &lt;- c(2,3) # Select variables
x=da[,dac]

plot.ts(x)
summary(x)

#Run Augmented Dickey-Fuller tests to determine stationarity and differences to achieve stationarity.
ndiffs(x[, ""VAR1""], alpha = 0.05, test = c(""adf""))
ndiffs(x[, ""VAR2""], alpha = 0.05, test = c(""adf""))

#Difference to achieve stationarity
d.x1 = diff(x[, ""VAR1""], differences = 2)
d.x2 = diff(x[, ""VAR2""], differences = 2)

dx = cbind(d.x1, d.x2)
plot.ts(dx)

#Lag optimisation
VARselect(dx, lag.max = 10, type = ""both"")

#Vector autoregression with lags set according to results of lag optimisation. 
var = VAR(dx, p=2)

#Test for serial autocorrelation using the Portmanteau test
#Rerun var model with other suggested lags if H0 can be rejected at 0.05
serial.test(var, lags.pt = 10, type = ""PT.asymptotic"")

#ARCH test (Autoregressive conditional heteroscedasdicity)
arch.test(var, lags.multi = 10)

summary(var)

#Granger Causality test
#Does x1 granger cause x2?
grangertest(d.x2 ~ d.x1, order = 2)

#Does x2 granger cause x1?
grangertest(d.x1 ~ d.x2, order = 2)

#Forecasting
prd &lt;- predict(var, n.ahead = 10, ci = 0.95, dumvar = NULL)
print(prd)
plot(prd, ""single"")
</code></pre>

<p>Is this method sound?</p>
"
"0.0726752374667264","0.0714893875923587","192507","<p>BACKGROUND:</p>

<p>I have some animal behaviour data. The time allocated by a group of animals 
to different behaviours per minute was recorded repeatedly until
the end of the experiment. Therefore, I have 4 response variables:</p>

<p>y1 = proportion of time allocated to behaviour 1
y2 = proportion of time allocated to behaviour 2
y3 = proportion of time allocated to behaviour 3
y4 = proportion of time allocated to behaviour 4
NB: The sum of all the y variables is equal to 1. </p>

<p>I also have a number of candidate predictor variables. I wish to establish which 
of these candidate predictor variables have a significant effect on the proportion
of time allocated to the different behaviours.</p>

<p>PROBLEM:</p>

<p>I believe this may be modeled using Dirichlet multivariable regression 
(used for modeling data representing components as a percentage of a total, DirichletReg package in R). However,
there is a issue of temporal correlation ie. the y variables are not independent. For example,
if the proportion of y1 at time_1 is high, then the proportion of y1 at time_2 is also
likely to be high. </p>

<p>QUESTION:</p>

<p>Is it possible incorporate temporal autocorrelation into Dirichlet multivariable regression? If so, which R package
would be suitable? Are there any other possible approaches to this problem? Many thanks in advance.</p>
"
"0.128010867631825","0.137369563821848","192785","<p><strong>Objective</strong></p>

<p>I have a crossed and implicitly nested design and am trying to validate the correct â€˜maximalâ€™ model (including all linear and pairwise interactions of the variables) for use in <code>lmer()</code>.  I intend to use this as the starting point for some kind of backward stepwise regression, possibly making use of the function <code>mixed()</code> in the  <code>{afex}</code> package.</p>

<p><strong>Experimental design</strong></p>

<p>This a linguistics study.  We have 20 <code>Subjects</code>, each speaking 180 utterances, amounting to 3600 observations in total. Each utterance is initiated via prompting, and an associated Response Time is measured. Log Response Time is the dependent variable. </p>

<p><em>Conditions &amp; Blocks</em></p>

<p>The Response Time for the utterances is affected by 3 <code>Conditions</code> (coded 1 to 3). Each <code>Condition</code> is implemented by prompting the <code>Subject</code> to recite 1 of 4 <code>Blocks</code> of utterances (coded 1 to 12).</p>

<p><em>Words &amp; Tones</em></p>

<p>Each <code>Block</code> brings about its associated <code>Condition</code> via 15-utterance repetition of 3 carefully chosen <code>Words</code>.  There are a total of 12 <code>Words</code> used in the experiment (coded 1 to 12). The <code>Words</code> within each <code>Block</code> can also be categorized by <code>Tone</code> (coded 1 to 2).  There are 6 <code>Words</code> per <code>Tone</code>.  </p>

<p><em>Summary</em></p>

<p>Each of the 20 <code>Subjects</code> utter all 12 <code>Blocks</code> of 15 utterances each.  In doing so, they repeatedly utter all 12 <code>Words</code> (15 utterances per <code>Word</code>), and thereby use both <code>Tones</code> (90 utterances per <code>Tone</code>).</p>

<p>I would like to consider <code>Block</code>, <code>Word</code>, and <code>Subject</code> as random effects, and <code>Condition</code> and <code>Tone</code> as fixed.</p>

<p><strong>Proposed Model</strong></p>

<p>I think the model can be written in the following wayâ€¦</p>

<p><code>RT_log ~ Condition*Tone + (Condition*Tone|Subject) + (Condition|Word) + (Tone|Block)</code></p>

<p><strong>Questions</strong></p>

<p><strong>1.</strong> Is this the 'maximal' model (with linear plus pairwise interactions) appropriate for my experimental design?_ </p>

<p><strong>2.</strong> There is correlation between <code>Block</code> and <code>Condition</code> (there are only 4 possible blocks - out of the total 12 - for each <code>Condition</code>).  There is, similarly, correlation between <code>Word</code> and <code>Tone</code>.  Is it 'okay' to leave this correlation in the model? I don't see a good way of removing it.</p>

<p><strong>3.</strong> How will lme4 handle implicit nesting: I.e., the blocks, which are implicitly nested in the 3 conditions (i.e., only 4 blocks are applicable to each of the 3 conditions, even though the blocks are coded from 1 to 12), and the words, which are implicitly nested within the 2 tones (only 6 words are applicable to each tone, even though words are coded from 1 to 12)?</p>

<p><strong>4.</strong> Some <code>Blocks</code> utilize <code>Words</code> of only a single <code>Tone</code>, whereas other <code>Blocks</code> utilize words of both <code>Tones</code>.  Will that cause problems for the <code>(Tone|Block)</code> term in the model? It will only make sense for certain values of Block.</p>

<p><strong>5.</strong> It has been suggested by some that we might need a ""Subject:Word"" grouping (random effect).  Why might we need this grouping?</p>
"
"0.10277830647413","0.101101261498861","194867","<p>At several time points, I sample different individuals from a population (say 60 ind/time point). I assign to each of them one category (either low &lt; middle &lt; high). I then want to model this ordinal dependent variables using several covariates including time (to evaluates temporal changes in proportions/probabilities). </p>

<p>If I had only one time point, I would use ordered logistic regression in R with polyR, vglm... But these time series, I fear to have residual autocorrelation. Can I easily check residual autocorrelation with these models? Should I include the lagged dependent variable as a covariate to remove it? What are the different alternatives to analyse this kind of data in R?</p>

<p>For the moment, I analysed separately each of the categorical variable level with binomial regression and the lagged dependent variable as one of the covariates but hope to find a better solution.</p>
"
"NaN","NaN","194885","<p>Suppose I have following non-periodic time series. Obviously the trend is decreasing and I would like to prove it by some test (with <em>p-value</em>). <strong>I am unable to use classic linear regression due to strong temporal (serial) auto-correlation among values.</strong></p>

<pre><code>library(forecast)
my.ts &lt;- ts(c(10,11,11.5,10,10.1,9,11,10,8,9,9,
               6,5,5,4,3,3,2,1,2,4,4,2,1,1,0.5,1),
            start = 1, end = 27,frequency = 1)
plot(my.ts, col = ""black"", type = ""p"",
     pch = 20, cex = 1.2, ylim = c(0,13))
# line of moving averages 
lines(ma(my.ts,3),col=""red"", lty = 2, lwd = 2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/PtQje.png""><img src=""http://i.stack.imgur.com/PtQje.png"" alt=""enter image description here""></a></p>

<p>What are my options?</p>
"
"0.132686223108569","0.117469050619694","197001","<p>I am trying to follow the procedure offered by <a href=""http://www.jstor.org/stable/2082979?seq=1#page_scan_tab_contents"" rel=""nofollow"">Beck and Katz 1995</a> in a way that I also have a TSCS data with $T=100$ (time dimension) and $N=12$ (unit dimension). My data is not balanced, which means that for some time periods, not all units have observations. </p>

<p>I am using R, and I found a <code>pcse</code> package that does what I need. It calculates panel corrected standard errors which accounts for contemporaneous correlation of errors across units and unit level heteroskedasity of errors. However, the steps I have to take to calculate panel robust standard errors for this type of regression start with the need to correct for serial correlation of errors, if I understand it well. Particularly, that is what is recommended in <code>pcse</code> package documentation:</p>

<p><a href=""http://i.stack.imgur.com/GNowz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/GNowz.png"" alt=""enter image description here""></a></p>

<p>So, I am lost trying to understand what I need to do. My options how I see them:</p>

<ol>
<li>Run simple OLS regression on my pooled panel data. </li>
<li><p>Test for serial correlation of error term using Durbinâ€“Watson test and examining ACF/PACF. In most cases, I will have AR(1) in errors. </p>

<ul>
<li>Either compute clustered standard errors - it should account for the fact that errors should be clustered on the unit variable. After this step, I would get robust standard errors, but I cannot use it in pcse estimation - I don't need the VCV of errors as an input for the <code>pcse</code> function, but the OLS <code>lm</code> object itself.</li>
<li>Or use Cochraneâ€“Orcutt transformation first, and then use transformed  model as an input for pcse estimation. I started doing it, but realized that after CO transformation, error term became serially independent, but had the kurtosis of 20 (normality assumption fails).</li>
</ul></li>
</ol>

<p>So, my options are not so suitable. How do you think I should approach this situation?</p>
"
"0.10277830647413","0.101101261498861","197337","<p>I have monthly temperature averages for a weather station across 100 years.
I am wondering how I should analyze this data in a regression. </p>

<p>The data are set up in the following fashion:</p>

<pre><code>year  month  temp.avg
1900    11      9 
1900    12      6       
1901    01      5 
1901    02      4 
....
2015    12      7 
</code></pre>

<p>My question is: <strong>how do I go about accounting for the time and incorporating it into my model?</strong></p>

<p>Here are my <strong>4 proposed methods:</strong></p>

<ol>
<li><p>Should I add a ""time"" variable which essentially counts my months from 1 through <em>n</em> rows of data? </p>

<ul>
<li>Ex: Using the example above, I would have a ""time"" column of 1,2,3,4,etc.</li>
<li>I essentially lose the actual year &amp; month data, but does this matter? -- can I just add back, for example, ""Dec 1900"" for time 1?</li>
</ul></li>
<li><p>Given a month <em>i</em> in year <em>j</em>, should I create a continuous time by adding 0.0833 (1/12) to each year for each i-1 month? </p>

<ul>
<li><p>Ex. Again using the above example, I would have a ""time"" column consisting of values 1900, 1900.8333, 1900.9167, 1901.0000, 1901.0833, etc...</p></li>
<li><p>The linear regression model for the prior two methods would essentially be: <code>lm(temp.avg ~ time)</code></p></li>
</ul></li>
<li><p>Do I just incorporate year and month (or perhaps more usefully <em>season</em>) in the model together?</p>

<ul>
<li>This would result in: <code>lm(temp.avg ~ year + month)</code></li>
</ul></li>
<li><p>Or is 3 wrong and instead I'd have to create a dummy variable for each month (or season)?</p>

<ul>
<li><code>lm(temp.avg ~ year + jan + feb + mar + apr ...)</code></li>
</ul></li>
</ol>

<p>So <strong>which is correct?</strong> 
I assume perhaps the questions I'm asking would dictate this to some degree. But perhaps someone could describe simply the validity of each method and when to apply each?</p>

<p>Note: I understand that I will have to account for temporal autocorrelation, but I'm wondering how I incorporate time data <em>prior</em> to worrying about that.</p>

<p>I will note that I perform my analyses in <code>R</code>.</p>
"
"0.0726752374667264","0.0714893875923587","197634","<p>What is the correct way to compare correlation between 2 dependent variables in R?</p>

<p>Thanks</p>

<p>Edit:
Here is the edited question and apologize for not asking this correctly before:</p>

<p>I acquired 2 measures from 2 different experiments and I want to know whether these 2 measures are correlated. </p>

<p>The problem is Measure 1 is confounded by some other covariates. So, I went ahead and did multiple regression and found the coefficient of my main effect. Since this is an estimate it has a mean with a deviation.</p>

<p>I could do cor(mean(parameter_estimate, Measure_2)) but I need to know if this correlation is significant. Is this right as I dont incorporate the spread of the estimate (variance)? My guess is the mean may be significant but with the standard error of the estimate, the correlation may become insignificant.</p>

<p>Thank you for your help</p>

<p>Regards</p>
"
"0.16888199063418","0.184584805050515","197710","<p><strong>Experiment:</strong></p>

<p>I have 2 groups and both groups undergo 2 set of evaluations, one with MRI scanner and the other in the lab to test for their behavior. Both these evaluations are known to have statistically significant relationship with age and gender. </p>

<p>Statistical questions:</p>

<p>Whether there is: </p>

<p>1) statistically significant difference between the 2 groups on each evaluation? </p>

<p>2) any relationship between and within the 2 groups between each evaluation?  </p>

<p><strong>Model:</strong></p>

<p>I model the problem as </p>

<p>$\text{MRI_measure} = \beta_{0} + \beta_{1} \text{Age} + \beta_{2} \text{Gender} + \beta_{3} \text{Group} $</p>

<p>$\text{Lab_measure} = \beta_{0}+ \beta_{1}  \text{Age} + \beta_{2}  \text{Gender} +\beta_{3}  \text{Group}$</p>

<p>[Age is continuous and Gender, Group are factors/categorical] </p>

<p>In R: </p>

<pre><code>MRI_model&lt;-lm(cbind(MRI_measure, Lab_measure) ~ age+gender+group, data=data) 
</code></pre>

<p><strong>Result of R:</strong> </p>

<p><code>manova(MRI_model)</code> suggests that yes indeed all the slopes are significantly different than 0 suggesting a relationship between my measures. </p>

<p><strong>Questions</strong> </p>

<p>1) In order to test whether the difference in the MRI_measure is statistically significant between the 2 groups, I use MRI_model$fitted.values for each dependent measure and do a statistical test (either t-test or Wilcox) and claim that the difference is significant. </p>

<p>In the paper I write, multivariate multiple linear regression was performed for the groups while controlling for age and gender. The regressed out MRI_measure was statistically compared to see if the difference is different. </p>

<p>I am assuming that the predicted/fitted.values in model are the regressed out variables. Can I show this and use this result? Is this right? </p>

<p>If no, what is the correct way to statistically compare whether my 2 groups differ in their MRI measure and lab measure when controlled for age and gender. Any R library, literature, possibly a script will be greatly appreciated. </p>

<p>2) I also want to see if there is any relationship between MRI_measure and Lab_measure within the group after they are controlled for age and gender. What is the correct way to do this in R? </p>

<p>Further, I also want to see if there is any significantly different association between the 2 groups for my set of dependent variables. I am thinking of first finding the correlation between 2 dependent variable in each group and test if this correlation is statistically different between the 2 groups? Is this logic right? And if it is, how do I compare the correlation? If not, what is the right way to do this? Any R library, literature, possibly a script will be greatly appreciated. </p>
"
"0.0419590679148345","0.0412744171706498","200198","<p>I'm trying to build a product recommender system. I'm collecting users data from social media like number of mutual friends,age,gender,career for users u1..u50. u1 is target user and I want to apply regression and find correlation coefficient between u1-u2,u1-u3 and so on(and give more weightage to user's rating with higher correlation coefficient). Is it possible to do like this and any ideas how to implement it? </p>
"
"NaN","NaN","202963","<p>I am learning R and how to do regressions in a biological context, so please forgive me.</p>

<p>I am stumped on how to test if a slope parameter is less than a certain number at the alpha = 0.05 level. These are my two correlation(?) coefficients:</p>

<blockquote>
  <p>reg.fit &lt;- lm(gro3 ~ gro2) # fit linear model
  reg.fit</p>
  
  <p>Call:
  lm(formula = gro3 ~ gro2)</p>
  
  <p>Coefficients:</p>
  
  <p>(Intercept),         gro2<br>
     -0.8426,       0.3582    </p>
</blockquote>

<p>I <em>think</em> I should use a t-test to test these somehow. But I'm not sure where to start.</p>
"
"0.106148978486855","0.130521167355216","204145","<p><strong>Background and Problem</strong></p>

<p>I have a question concerning a meta-analysis combining effects from between- and within-subject designs using log-odds ratios (OR) as the metric of interest. I am familiar with conducting meta-analyses and will be undertaking my calculations in R (using the <code>metafor</code> and <code>lme4</code> packages). To provide greater context, the studies in question ask research subjects to make a binary decision with respect to a personal preference across one of two conditions. In some cases, each participant is assigned to a single condition (making only a single binary response); in others, each subject takes part in both conditions (making two binary responses). For now, presume I have the raw data in all cases. The issue I face is how best to calculate an OR that is comparable across design and whether I should take the correlation between conditions into account for the within-subject designs.</p>

<p><strong>My Current Approach</strong></p>

<p>I presently use logistic regression to estimate the OR for between-subject designs. The slope represents the OR and the sampling variance can be calculated by squaring the SE of the slope coefficient. Using this approach produces estimates comparable to equations reported in common texts such The Handbook of Research Synthesis and Meta-Analysis, 2nd Edition (p. 243). I then extend this approach to use a multilevel logistic regression model including a random intercept by subject to estimate the OR for within-subject designs while account for the dependency between conditions. The OR and sampling variance are otherwise calculated in the same fashion. </p>

<p><strong>My Questions:</strong></p>

<p>With this in mind, I would like to ask:</p>

<ol>
<li>Is it reasonable to meta-analytically aggregate OR calculated using standard and multilevel logistic regression?</li>
<li>Would it be better to use standard logistic regression for both designs (ignoring the correlation between conditions for the within-subject designs)?</li>
</ol>
"
"NaN","NaN","204375","<p>This question might be bit naive question. But is it possible to identify which variable are suppressor and which are confounder before running regression just by reviewing the correlation values among independent variables and dependent variable.</p>

<p>Thanks!</p>
"
"0.10277830647413","0.101101261498861","205453","<p>I don't have a working example, because this question is more conceptual.  Let's say I'm running a linear regression using the <code>plm</code> package in R on the relationship between graduating from college and getting lunch-subsidies as a child.  </p>

<p>I have panel data that includes observations from individuals that may be in the same family, so I add family-level fixed effects to allow for arbitrary correlations within the family.  </p>

<p>If I were to add race covariates, however, R would omit them because of multicollinearity in the family level.  </p>

<p>How would I test the hypothesis, then, that blacks have differential effects than whites, but still resolving the issues solved by fixed effects?  </p>

<p><strong>To answer the questions below:</strong></p>

<p>Let's say I have the following regression, attempting to test for the hypothesis that different races and sexes are helped differentially from a free lunch program insofar as it relates to graduating college:</p>

<pre><code>plm(graduate_college ~ free_lunch + black + male + hispanic + data=data, index=c(""mother_id""), model=""within"")
</code></pre>

<p>Mother_ID just tracks siblings from the same mother.  Now, if I want to test the hypothesis that blacks have different effects than whites as it relates to the effect of free lunch on graduating college, how would I test this?  My guess is to remove fixed effects, add clustered standard errors at the mother_id level and add an interaction term for black*free_lunch?</p>
"
"0.10277830647413","0.101101261498861","206039","<p>I am looking at a logistic regression model for predicting hospital acquired infection likelihood (HAI) from predictors of whether germs are found on the  x number of patients (Patient), x number of environmental spots (Env), x number of air samples (Air) or x number of nurses' hands (Hand).</p>

<pre><code>   Month Patient Env Air Hand HAI HAIcat BedOccupancy
      1       4   0   0    1   1    yes            9
      2       2   0   2    0   0     no            9
      3       2   1   0    1   0     no            5
      4       1   2   0    2   2    yes            7
      5       2   3   0    1   1    yes            6
      6       1   2   0    0   1    yes            5
      7       4   0   0    2   1    yes            7
      8       2   0   0    1   3    yes            7
      9       3   2   2    0   1    yes            8
     10       3   0   0    1   1    yes            8
</code></pre>

<p>For example for Month 1, the percentage of HAI would be HAI/BedOccupancy=1/9.
So I'd like to know if bed occupancy or other contamination is significant in predicting HAI. I run a Logistic regression, but it says it's junk. What does a statistician do now?</p>

<pre><code>model&lt;-glm(cbind(MR$HAI,MR$BedOccupancy)~MR$Patient+MR$Env+MR$Air+MR$Hand,family = ""binomial"")
</code></pre>

<p>But I get a bad fit and non-significant correlation:</p>

<pre><code>Call:
glm(formula = cbind(MR$HAI, MR$BedOccupancy) ~ MR$Patient + MR$Env + MR$Air + 
        MR$Hand, family = ""binomial"")

Deviance Residuals: 
       1         2         3         4         5         6         7         8         9        10  
-0.12882  -1.08046  -1.33787   0.01400  -0.10685  -0.02229  -0.04008   1.03688   0.75723  -0.23824  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.30758    1.34049  -0.975    0.329
MR$Patient  -0.22920    0.39350  -0.582    0.560
    MR$Env      -0.02415    0.37672  -0.064    0.949
MR$Air      -0.46851    0.64611  -0.725    0.468
    MR$Hand      0.16054    0.58277   0.275    0.783

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.6594  on 9  degrees of freedom
Residual deviance: 4.6929  on 5  degrees of freedom
AIC: 30.911

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.133237935355665","0.142978775184717","206931","<p>I want to generate 3 dependent(Y's) variables,2 Independent(X's) variables and 3 Moderator(M's) variables(<strong>all are continuous</strong> and lie in the <strong>range 1 to 5</strong>) and each(M's) variable has <strong>300 values</strong>.Following is my correlation structure that I require among the those variables</p>

<pre><code>     Y1     Y2     Y3
X1    +      +      +     
X2    +      -      -
M1    +/-    +      -
M2    -      +      +
M3    +      +      -
</code></pre>

<p>I want to generate my dataset and run <strong>moderated regression</strong> using those variables and keeping correlation direction + and - as mentioned in the matrix?</p>

<p><strong>My Try</strong></p>

<p>I have done the above  using python and R code but the <strong>problem is that all my interaction terms are getting non-significant in each of the 18 models which I think due to the reason that I am not taking into consideration the behaviour of moderators on the relationship between independent and dependent variables in my program</strong></p>

<p>I would have <strong>18 models</strong>(each model would have 1 Depdendent,1 Independent and 1 Moderator variable and 1 interaction term that will be the product of moderator and independent) Moreover I want each Independent,moderator and Interaction term significant. 
Following are 6 models against Y1,similarly 6 for Y2 and 6 for Y3.</p>

<pre><code>Y1=intercept + aX1 +bM1 +cX1M1            // all R.H.S Betas significant
Y1=intercept + aX1 +bM2 +cX1M2            // all R.H.S Betas significant
Y1=intercept + aX1 +bM3 +cX1M3            ...
Y1=intercept + aX2 +bM1 +cX2M1            ...
Y1=intercept + aX2 +bM1 +cX2M1
Y1=intercept + aX2 +bM2 +cX2M2
Y1=intercept + aX2 +bM3 +cX2M3
......
......
......                                     ...
</code></pre>

<p>Any software that could do this simulation by providing following parameters?</p>

<p><strong>1.</strong># of observations</p>

<p><strong>2.</strong># of Idepen variables(also option for scale of values)</p>

<p><strong>3.</strong># of Depen variables(also option for scale of values)</p>

<p><strong>4.</strong># of Moderator variables(also option for scale of values)</p>

<p><strong>5.</strong> Correlation Matrix Structure</p>
"
"0.0593390829096927","0.0583708405417779","207148","<p>I have a OLS model looks like this:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5CDelta%20y_t%3D%5Cbeta_0%2B%5Cbeta_1%5CDelta%20x_t%2B%5Cepsilon_t"" alt=""formula""></p>

<p><a href=""http://i.stack.imgur.com/5FlOO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5FlOO.png"" alt=""enter image description here""></a></p>

<p>However, the residuals have auto-correlation like this:</p>

<p><a href=""http://i.stack.imgur.com/tcCNn.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tcCNn.png"" alt=""enter image description here""></a></p>

<p>It doesn't seem a strong autocorrelation, and the model passes the Engle-Granger cointegration test (package egcm). However, the model fails Durbinâ€“Watson test. Adding an autoregressive term like AR(1) will totally solve this problem, because ARMAX explicitly models the errors. but I am a bit worried about the possibility of spurious regression.</p>

<p>Are my OLS model residuals good enough to conclude cointegration? If not, does ARMAX help in my case?</p>

<p>Thanks in advance.</p>
"
"NaN","NaN","211636","<p>How to assess the performance of the multivariate regression outputs in different scenarios using Pearson's correlation in R ? Is it a straight process to compare the correlation of expected values and observed values ?</p>
"
"0.0750586625040802","0.0922924025252575","212453","<p>In my research project I have to do a regression of the financial risk on the business risk of the year before.
As a reference, I have a paper showing the results for several countries. The paper states that, for the country I am interested in, the coefficient of the regression is negative. their year span :1995-2008</p>

<p>I have to perform the same analysis at region level for the country I am interested in. I find a negative lagged correlation coefficient. However, the coefficient in my regression is positive and significant. My year span:2000-20014.</p>

<p>Should I not have a negative coefficient as in the country-level regression (paper).
Thank you.</p>
"
"0.133237935355665","0.142978775184717","212840","<p>I read Chen et al. <a href=""http://onlinelibrary.wiley.com/doi/10.1002/for.1134/abstract"" rel=""nofollow"">""Forecasting volatility with support vector machine-based GARCH model""</a> (2010) where they implented a recurrent SVM procedure to estimate volatility by a GARCH based model. 
The model is of the form </p>

<p>$y_t = f(y_{t-1}) + u_t \qquad \qquad \ \ \ (1)$ </p>

<p>$u^2_t = g(u^2_{t-1}, w_{t-1}) + w_t \qquad  (2)$ </p>

<p>At first they got estimates for $u_t$ by estimating $(1)$ by a SVM. Then, the following recurrent SVM algorithm was proposed to estimate $(2)$.</p>

<hr>

<p><strong><em>Recurrent SVM Algorithm:</em></strong></p>

<p><strong>Step 1:</strong> Set $i = 1$ and start with all residuals at zero: $w_t^{(1)} = 0 $.</p>

<p><strong>Step 2:</strong> Run an SVM procedure to get the decision function $f^{(i)}$ to the points $\{x_t, y_t\} = \{u_{t - 1}^2, u_t^2 \}$ with all inputs $x_t = \{u_{t - 1}^2, w_{t-1} \}$</p>

<p><strong>Step 3:</strong> Compute the new residuals $w_t^{i+1} = u_t^2 - f^{(i)}$.</p>

<p><strong>Step 4:</strong> Terminate the computaion process if the stopping criterion is satisfied; otherwise, set $i = i + 1$ and go back to Step 2.</p>

<hr>

<p>The proposed stopping critrerion is based a Ljung-Box-Test for the residuals $w_t$. Only if the $p$-values of the test in five consecutive periods are higher than 0.1 the process is stopped. </p>

<p>As real world example the log-returns of the New York Stock Exchange (NYSE) composite stock index for the period from January 8, 2004 to December 31, 2007 was used. The last 60 observations where used as test sample. Hence, the estimation was done with the first 940 observations. In their study, the process converged after 121 interations. <strong>(Question:) However, my implementation in R does not converge. I think I have a misunderstanding of the concept.</strong> Because I think I implemented it exactly as stated. My R code is the following</p>

<pre><code>rm(list = ls())

library(quantmod)
library(e1071)

#Get NYSE data and convert to log returns
id     &lt;- ""^NYA""
data   &lt;- getSymbols(id, source = ""yahoo"", auto.assign = FALSE, 
                     from = ""2004-01-08"", to = ""2007-12-31"")
series &lt;- data[,6]  #Get adjusted closing prices
series &lt;- na.omit(diff(log(series)))*100  #Compute log returns

#Lagged data for analysis
x      &lt;- na.omit(cbind(series, lag(series)))

#Set parameters as in paper
svm_eps   &lt;- 0.05
svm_cost  &lt;- 0.005
sigma     &lt;- 0.02
svm_gamma &lt;- 1/(2*sigma^2)


#SVM to get u_t
svm     &lt;- svm(x = x[,-1], y = x[,1], scale = FALSE,
               type = ""eps-regression"", kernel = ""radial"",
               gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

u    &lt;- svm$residuals  #Extract u_t
n    &lt;- 60  #Size of test set
u_tr &lt;- u[1:(nrow(u) - n)]  #Subset to training set
u_tr &lt;- na.omit(cbind(u_tr, lag(u_tr)))^2  #Final training set


#Recurrent SVM for vola estimation
i       &lt;- 1
p_count &lt;- 0

while(p_count &lt; 5){

  print(i)  #Print number of loops

  #Estimate SVM for u^2
  svmr     &lt;- svm(x = u_tr[,-1], y = u_tr[,1], scale = FALSE,
                  type = ""eps-regression"", kernel = ""radial"",
                  gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

  #Test autocorrelation of residuals to lag 1
  test    &lt;- Box.test(svmr$residuals, lag = 1, type = ""Ljung-Box"")
  p_val   &lt;- test$p.value
  p_count &lt;- ifelse(p_val &gt; 0.1, p_count + 1, 0)

  #Extract residuals for next estimation step
  w        &lt;- svmr$residuals
  w        &lt;- c(0, w[-length(w)])  #lag 1

  u_tr &lt;- cbind(u_tr[,1:2], w)

  i &lt;- i + 1
}
</code></pre>
"
"0.0938233281301002","0.0922924025252575","213947","<p>I'm trying to do meta-regression with a lot of trials (>40 trials with >100 'arms') investigating the efficacy of a procedure (abl) and any 'addon' procedure. Each trial will have 2 or more arms. In some trials, the 'control' arm is no procedure; in others is a 'vanilla' procedure, with the active arm adding in various 'extras'.</p>

<p>I want to see not only if the procedure improves outcomes, but also if various 'extras' to the procedure also influence outcome.</p>

<p>My columns which are predictors in my model start with 'pred_'. If they are to do with the procedure used (abl), they start 'pred_abl_'</p>

<pre><code>&gt; head(af_dat)
        id arm  xi  ni pred_age pred_gender pred_antiarrhythmics pred_abl_pvi_done pred_abl_pvi_type pred_abl_cryo pred_abl_cfea pred_abl_gp pred_abl_lines pred_abl_mapping                      pred_endpoint pred_at_included pred_blanking        pi
1 23094720   A 112 146     56.0        68.0                    0                 1                 1             0             0           0              1                1                             Holter                0             3 0.7671233
2 23094720   B 103 148     54.0        72.0                    1                 0                 0             0             0           0              0                0                             Holter                0             3 0.6959459
3 21539635   A  45  58     57.6        66.0                    0                 1                 1             0             1           0              0                1                            Holter                 1             3 0.7758621
4 21539635   B  14  24     56.4        67.0                    0                 0                 0             0             1           0              0                1                             Holter                1             3 0.5833333
5 21539635   C  27  35     52.2        71.0                    0                 1                 1             0             0           0              0                1                             Holter                1             3 0.7714286
6 24549549   A  50  66     56.3        77.3                    0                 1                 1             0             0           0              0                0 Mobile telemetry (transtelephonic)                1             3 0.7575758
</code></pre>

<p>The column which indicates if they had a procedure at all is pred_abl_pvi_done. The other columns beginning pred_abl_* (of which there are 6 shown above) describe the presence of any 'extras' to the procedure, and <strong>must</strong> be 0 if the pred_abl_pvi_done is 0, by definition.</p>

<p>Initially, I was going to do the meta-regression as follows:</p>

<pre><code>rma(measure='PLO', xi=xi, ni=ni, data=af_dat, mods=~pred_age+pred_gender+pred_antiarrhythmics+pred_abl_pvi_done+pred_abl_pvi_type+pred_abl_cryo+pred_abl_cfea+pred_abl_gp+pred_abl_lines+pred_abl_mapping+pred_endpoint+pred_at_included+pred_blanking)
</code></pre>

<p>However, I'm not sure if this is valid; the pred_abl_* columns will of course be very strongly correlated with pred_abl_pvi_done as they will be 0 if the latter is 0. If the 'abl' procedure (pred_abl_pvi_done=1) is very efficacious, I'm worried the other pred_abl_* columns will also appear correlated even if they add nothing prognostically, as many of the '0's for these columns will merely be indicating no procedure has been done at all.</p>

<p>One way to get around this, I suppose, would be to only look at the 6 procedural columns if pred_abl_pvi_done is 1.</p>

<p>I could therefore do 2 analysies:</p>

<ol>
<li>Multivariate regression of all the studies, excluding the 6 procedural columns. I would then see if a procedure versus no procedure (pred_abl_pvi_done 1 or 0) is significant</li>
<li>Select only cases where pvi_abl_pvi_done = 1 (ie the procedure is done (90% of the trials), and exclude medical treatment), and THEN do a further multivariate regression where I include all the procedural questions.</li>
</ol>

<p>Do you think I need to do these 2 analyses separately due to the correlation?</p>
"
"0.132686223108569","0.130521167355216","215532","<p>I am a clinicians (limited statistical knowledge) who is trying to use mlogit pkg in R to analyze a clinical dataset, running logistic regression on it. I am trying to ascertain if there is any correlation seen in patients with many vars and heart block (var = block (0s and 1s))</p>

<p>I have a dataset with these variables</p>

<pre><code> [1] ""Age""                    ""Sex""                    ""Race""                       ""Obesity""               
 [5] ""CAD""                    ""HTN""                    ""DM""                     ""HLD""                   
 [9] ""CHF""                    ""COPD""                   ""Asthma""                 ""Thyroid.disorder""      
[13] ""Smoking""                ""Illicit.drug.use""       ""Alcohol""                ""INR""                   
[17] ""TB""                     ""AST""                    ""ALT""                    ""Cirrhosis""             
[21] ""Adenosine""              ""Amiodarone""             ""Beta.blocker""           ""CCB""                   
[25] ""Digoxin""                ""TCA""                    ""SSRI""                   ""Antipsychotic""         
[29] ""AV.block""               ""Bundle.branch.block""    ""PAC.PVC""                ""Afib""                  
[33] ""Other.Arrythmia""        ""Nonspecific.ST.wave""    ""Anterioseptal..ST.wave"" ""Anteriolateral.ST.wave""
[37] ""Inferior.ST.wave""       ""Posterior.ST.wave""      ""Axis.deviation""         ""Low.voltage""           
[41] ""Qt.prolongation""        ""Hypertrophy""            ""block""
</code></pre>

<p>Now I have used the mlogit pkg in R</p>

<p>My Code is</p>

<pre><code># Reshaping data
mydata &lt;- mlogit.data(data=mydata, shape=""wide"", choice=""block"")

# Creating Model with all Vars
model &lt;- mlogit(data=mydata, formula=block~0|Age+Sex+Race+Obesity+CAD+HTN+DM+HLD+CHF+COPD+Asthma+Thyroid.disorder+Smoking+Illicit.drug.use+Beta.blocker+CCB+Digoxin+TCA+SSRI+Antipsychotic+Hypertrophy+INR+TB+AST+ALT)
</code></pre>

<p>Generates this error :</p>

<pre><code> Error in solve.default(H, g[!fixed]) : 
 Lapack routine dgesv: system is exactly singular: U[43,43] = 0
</code></pre>

<p>Now if I break the variables into different variables like..</p>

<pre><code>model1 &lt;- mlogit(data=mydata, formula=block~0|Age+Sex+Race+Obesity+CAD+HTN+DM+HLD+CHF+COPD+Asthma+Thyroid.disorder+Smoking)
model2 &lt;- mlogit(data=mydata, formula=block~0|Illicit.drug.use+Beta.blocker+CCB+Digoxin+TCA+SSRI+Antipsychotic+Hypertrophy)
model3 &lt;- mlogit(data=mydata, formula=block~0|INR+TB+AST+ALT)
</code></pre>

<p>IT WORKS without any errors.</p>

<p>But here I would like to know,
how breaking into different models would change my Coefficients?
What should I do to avoid the error and try to incorporate all variables in one model?
How should I interpret my results if break into 3 different models as opposed to one?</p>

<p>Any help is highly appreciated.</p>
"
"0.0484501583111509","0.0714893875923587","217926","<p><em>Will the signs of coef (Estimate) of lm and glm always be the same?</em> <strong>^</strong></p>

<p>According to below toy example, it seems yes. Can you provide a case where they might be different? (If it matters in my real data the outcome is binary, hence used <code>mpg &gt; 20</code>)</p>

<pre><code># dummy data
d &lt;- mtcars

# fit lm, glm, glm_bi
fit_lm &lt;- lm(mpg &gt; 20 ~ cyl + disp, data = d)
fit_glm &lt;- glm(mpg &gt; 20 ~ cyl + disp, data = d)
fit_glm_bi &lt;- glm(mpg &gt; 20 ~ cyl + disp, family = binomial, data = d)

# Signs are always same?
# lm compared to glm
all.equal(sign(coef(fit_lm)),
          sign(coef(fit_glm)))
# output
# [1] TRUE

# lm compared to glm(family = binomial)
all.equal(sign(coef(fit_lm)),
          sign(coef(fit_glm_bi)))

# output
# [1] TRUE
</code></pre>

<p><strong>^</strong> Very much sounds like a dupe, found this similar post: <a href=""http://stats.stackexchange.com/questions/91666/sign-of-coefficients-in-linear-regression-vs-the-sign-of-correlation"">Sign of coefficients in linear regression vs. the sign of correlation</a>. Let me know if this is a dupe.</p>
"
"0.10277830647413","0.101101261498861","219040","<p>UPDATE: I think I was over-complicating my problem and am struggling through a new approach as described here: <a href=""http://stats.stackexchange.com/questions/219288/paired-t-test-as-a-simple-latent-change-score-model"">paired t-test as a simple latent change score model</a></p>

<p>I am still accepting the answer below as it does appear the model is unidentified. </p>

<p>If folks think this is better placed on Stack Overflow, please advise. <code>lavaan</code> questions don't seem to get picked up too frequently there, so I'm starting here. </p>

<p>I am trying to estimate a variation of a change score model as described in the following powerpoint: <a href=""http://davidakenny.net/webinars/powerpoints/SEM/Longitudinal/Change/CSA.ppt"" rel=""nofollow"">http://davidakenny.net/webinars/powerpoints/SEM/Longitudinal/Change/CSA.ppt</a></p>

<p>I don't know the author and am a bit of a novice at latent variable modeling, so I am happy to receive advice on this approach in general...</p>

<p>The path diagram <em>I think</em> I am estimating is as follows: </p>

<p><a href=""http://i.stack.imgur.com/STU7P.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/STU7P.png"" alt=""enter image description here""></a></p>

<p><strong>NOTE 1: I understand that a two-wave follow-up is not preferred for growth-modeling, but this is a limitation of my data.</strong></p>

<p><strong>NOTE 2: I only have two indicator measures instead of three as shown in the presentation. I am thus fixing all of the loadings to 1 in order to accommodate this and ensure the model is identified.</strong></p>

<p>In <code>laavan</code>, I am specifying the model as follows: </p>

<pre><code>LCSM_Raykov &lt;- '
B=~ Y_11 + 1*Y_12 + 1*Y_21 + 1*Y_22
C=~ Y_21 + 1*Y_22
# regressions
C ~ X
# residual correlations
B ~~ X
B ~~ C
Y_11 ~~ Y_12
Y_21 ~~ Y_22
'

LCSM_Raykov_fit &lt;- lavaan(LCSM_Raykov, data=dat)
</code></pre>

<p>However, I am receiving the following error message. </p>

<pre><code>#Error in lav_model_estimate(lavmodel = lavmodel, lavsamplestats = #lavsamplestats,  : 
#  lavaan ERROR: initial model-implied matrix (Sigma) is not positive #definite;
#  check your model and/or starting parameters.
#In addition: Warning message:
#In vnames(FLAT, ""ov.x"", warn = TRUE) :
#  lavaan WARNING: model syntax contains variance/covariance/intercept #formulas
#  involving (an) exogenous variable(s): [X];
#  Please use fixed.x=FALSE or leave them alone
</code></pre>

<p>I am curious as to whether or not this is a problem with my data (which I cannot share) or if it is a problem with my specification. </p>

<p>A simulated set of data which reproduces the error is provided below: </p>

<pre><code>dat &lt;- structure(list(Y_11 = c(420, 354, 308, 415, 373, 354, 342, 434, 
327, 395, 315, 342, 367, 330, 436, 318, 460, 424, 384, 363, 384, 
367, 400, 333, 401, 354, 364, 369, 358, 373, 361, 384, 401, 435, 
368, 433, 360, 388, 373, 383, 349, 371, 400, 315, 342, 411, 360, 
375, 424, 326, 383, 380, 320, 424, 387, 393, 401, 366, 378, 448, 
370, 392, 414, 363, 342, 358, 411, 358, 378, 358, 388, 350, 447, 
371, 385, 361, 308, 357, 367, 371, 389, 397, 415, 370, 393, 327, 
414, 352, 417, 376, 488, 361, 384, 435, 454, 309, 407, 387, 434, 
313, 476, 409, 349, 366, 308, 333, 373, 395, 407, 404, 407, 358, 
363, 337, 354, 408, 337, 460, 378, 378, 394, 354, 352, 376, 407, 
364, 381, 443, 414, 346, 401, 373, 408, 342, 352, 400, 361, 417, 
384, 426, 364, 492, 376, 404, 333, 396, 381, 302, 433, 341, 303, 
358, 404, 337, 368, 433, 338, 407, 366, 363, 377, 338, 344, 442, 
352, 373, 380, 346, 388, 357, 397, 428, 329, 358, 354, 417, 346, 
388, 381, 378, 383, 344, 424, 373, 387, 318, 428, 435, 414, 396, 
396, 371, 348, 345, 344, 414, 378, 392, 378, 366, 460, 320, 342, 
407, 337, 326, 313, 345, 396, 351, 354, 417, 431, 467, 393, 414, 
391, 369, 303, 338, 417, 414, 358, 432, 400, 456, 411, 400, 376, 
296, 360, 327, 417, 338, 321, 388, 420, 409, 396, 400, 407, 338, 
411, 397, 394, 381, 397, 357, 381, 341, 407, 315, 421, 404, 388, 
378, 435, 467, 303, 337, 360, 261, 396, 361, 363, 430, 382, 411, 
370, 357, 404, 375, 400, 400, 381, 313, 421, 412, 341, 394, 385, 
337, 354, 341, 366, 338, 406, 378, 348, 368, 337, 483, 329, 364, 
308, 369, 437, 388, 315, 460, 382, 361, 414, 420, 355, 355, 330, 
395, 373, 321, 396, 373, 395, 373, 357, 421, 370, 379, 394, 375, 
344, 358, 407, 488, 376, 366, 412, 321, 400, 443, 391, 391, 420, 
388, 387, 330, 334, 338, 387, 361, 433, 394, 414, 373, 363, 384, 
346, 326, 417, 327, 373, 403, 375, 431, 303, 371, 381, 402, 378, 
401, 477, 384, 389, 370, 423, 383, 352, 338, 381, 349, 303, 329, 
379, 417, 400, 355, 391, 408, 420), Y_12 = c(459, 364, 358, 409, 
412, 381, 375, 412, 383, 445, 386, 397, 404, 363, 436, 343, 467, 
409, 401, 395, 412, 402, 431, 376, 394, 406, 415, 406, 370, 404, 
404, 415, 431, 422, 397, 428, 375, 406, 361, 412, 404, 372, 422, 
324, 364, 430, 378, 388, 412, 392, 404, 386, 375, 409, 401, 412, 
432, 384, 426, 412, 373, 401, 404, 384, 368, 392, 432, 388, 393, 
404, 386, 364, 452, 366, 412, 404, 346, 401, 412, 340, 404, 409, 
415, 402, 397, 364, 426, 366, 437, 414, 467, 375, 395, 430, 436, 
394, 397, 436, 427, 349, 436, 412, 349, 376, 358, 367, 402, 407, 
397, 412, 400, 363, 376, 376, 412, 445, 392, 420, 388, 400, 470, 
375, 390, 404, 406, 395, 400, 415, 412, 386, 419, 394, 402, 397, 
366, 402, 400, 432, 412, 422, 409, 437, 383, 428, 370, 417, 406, 
346, 412, 378, 375, 385, 412, 375, 404, 432, 349, 420, 393, 409, 
409, 382, 399, 437, 382, 420, 393, 385, 397, 412, 392, 444, 386, 
358, 390, 436, 381, 404, 402, 331, 419, 399, 427, 414, 402, 358, 
409, 430, 392, 428, 384, 404, 401, 409, 364, 404, 395, 404, 397, 
388, 459, 372, 406, 412, 376, 366, 377, 384, 390, 380, 409, 390, 
430, 428, 417, 392, 392, 404, 345, 392, 426, 414, 390, 422, 412, 
404, 420, 400, 422, 352, 394, 350, 424, 370, 383, 400, 412, 426, 
409, 415, 414, 370, 432, 422, 406, 409, 418, 378, 404, 380, 409, 
357, 420, 415, 406, 407, 436, 483, 359, 392, 412, 323, 414, 397, 
370, 431, 401, 445, 409, 380, 402, 399, 420, 409, 401, 358, 417, 
415, 378, 422, 414, 409, 394, 388, 390, 345, 404, 418, 388, 406, 
404, 444, 386, 412, 355, 409, 397, 397, 350, 424, 401, 400, 418, 
422, 406, 406, 360, 412, 417, 404, 397, 400, 426, 407, 390, 402, 
366, 399, 409, 422, 431, 373, 412, 432, 414, 381, 419, 373, 419, 
483, 409, 409, 409, 397, 397, 401, 368, 353, 381, 388, 404, 406, 
426, 376, 372, 404, 381, 366, 426, 358, 388, 404, 401, 436, 316, 
406, 409, 384, 395, 393, 437, 422, 395, 390, 436, 400, 390, 400, 
404, 394, 375, 390, 401, 412, 424, 397, 404, 412, 395), Y_21 = c(412, 
356, 307, 379, 337, 383, 326, 396, 304, 365, 358, 321, 363, 337, 
464, 314, 375, 383, 366, 356, 387, 330, 386, 310, 386, 368, 342, 
330, 316, 400, 347, 397, 396, 394, 316, 389, 368, 360, 297, 351, 
326, 314, 310, 310, 360, 412, 347, 375, 415, 348, 319, 351, 344, 
415, 355, 415, 396, 312, 380, 417, 321, 365, 385, 377, 358, 357, 
432, 417, 375, 330, 368, 330, 433, 325, 379, 390, 298, 344, 338, 
331, 378, 371, 408, 355, 386, 310, 400, 361, 375, 379, 471, 357, 
363, 380, 433, 353, 396, 394, 442, 325, 417, 382, 312, 307, 344, 
373, 325, 380, 386, 410, 382, 312, 347, 321, 368, 387, 355, 426, 
394, 412, 384, 321, 312, 379, 402, 352, 375, 409, 409, 342, 408, 
348, 394, 307, 344, 386, 334, 422, 378, 445, 365, 478, 330, 489, 
338, 392, 286, 380, 421, 331, 380, 360, 433, 316, 316, 405, 312, 
386, 274, 364, 366, 344, 347, 471, 375, 351, 415, 326, 351, 347, 
383, 433, 365, 321, 378, 422, 330, 375, 352, 310, 384, 325, 485, 
368, 387, 336, 446, 392, 459, 402, 389, 384, 368, 340, 330, 417, 
382, 373, 382, 389, 445, 321, 381, 348, 375, 295, 284, 307, 375, 
350, 373, 403, 449, 442, 364, 400, 379, 350, 344, 325, 407, 382, 
304, 404, 337, 443, 415, 375, 387, 307, 358, 316, 437, 316, 297, 
319, 401, 408, 368, 380, 382, 316, 437, 415, 371, 385, 397, 368, 
357, 377, 427, 422, 419, 411, 368, 365, 415, 433, 320, 358, 334, 
314, 415, 338, 361, 377, 387, 412, 314, 361, 452, 341, 400, 362, 
368, 350, 489, 421, 330, 401, 392, 375, 326, 391, 325, 356, 397, 
385, 330, 331, 321, 403, 375, 366, 326, 380, 432, 350, 330, 360, 
379, 347, 380, 412, 336, 336, 347, 440, 360, 338, 364, 360, 371, 
360, 375, 422, 340, 347, 392, 384, 300, 310, 396, 489, 312, 312, 
418, 338, 348, 437, 371, 377, 438, 336, 400, 298, 286, 284, 321, 
336, 432, 413, 392, 381, 370, 384, 386, 316, 407, 331, 352, 380, 
361, 371, 331, 356, 375, 348, 368, 389, 463, 387, 390, 368, 389, 
379, 221, 342, 387, 298, 380, 337, 366, 380, 405, 344, 379, 383, 
412), Y_22 = c(431, 403, 348, 404, 393, 397, 383, 409, 342, 405, 
412, 406, 388, 356, 447, 370, 381, 390, 350, 375, 390, 359, 409, 
354, 401, 406, 378, 390, 371, 406, 369, 408, 427, 397, 355, 406, 
391, 396, 339, 365, 400, 345, 384, 369, 377, 418, 396, 394, 402, 
428, 406, 371, 415, 411, 407, 408, 422, 348, 394, 431, 359, 412, 
402, 437, 400, 386, 427, 405, 398, 357, 432, 363, 422, 353, 400, 
397, 329, 387, 375, 351, 388, 432, 436, 363, 379, 359, 403, 402, 
411, 396, 427, 361, 384, 419, 418, 384, 433, 405, 427, 365, 441, 
422, 339, 372, 379, 382, 398, 377, 409, 418, 393, 345, 370, 404, 
400, 411, 393, 400, 369, 398, 404, 388, 385, 381, 403, 386, 417, 
419, 422, 367, 442, 404, 418, 372, 396, 411, 377, 403, 408, 462, 
405, 432, 353, 466, 380, 400, 418, 415, 421, 383, 417, 394, 398, 
366, 365, 398, 342, 383, 365, 391, 428, 396, 402, 450, 384, 403, 
391, 390, 373, 417, 418, 406, 396, 343, 390, 433, 388, 400, 371, 
345, 448, 404, 432, 393, 384, 380, 398, 402, 402, 415, 391, 402, 
414, 442, 385, 402, 381, 422, 387, 391, 397, 375, 421, 393, 400, 
368, 348, 378, 385, 394, 411, 396, 456, 422, 406, 382, 421, 422, 
360, 412, 419, 391, 353, 442, 385, 464, 408, 389, 394, 324, 395, 
345, 422, 365, 367, 358, 427, 418, 385, 402, 383, 355, 415, 402, 
428, 397, 394, 412, 373, 442, 418, 429, 411, 417, 389, 405, 402, 
405, 324, 400, 422, 352, 415, 378, 407, 448, 400, 405, 497, 422, 
406, 414, 408, 412, 406, 380, 427, 455, 403, 447, 385, 403, 393, 
406, 387, 345, 390, 418, 378, 383, 407, 448, 400, 412, 314, 415, 
408, 412, 336, 379, 428, 377, 418, 394, 415, 415, 356, 419, 385, 
357, 391, 392, 394, 400, 427, 406, 383, 420, 396, 448, 401, 377, 
389, 466, 373, 389, 388, 392, 415, 441, 391, 412, 418, 393, 437, 
329, 340, 345, 339, 378, 415, 432, 425, 406, 379, 418, 391, 386, 
425, 369, 367, 411, 411, 392, 324, 360, 397, 400, 408, 440, 433, 
397, 425, 392, 437, 393, 391, 379, 402, 448, 417, 412, 422, 388, 
406, 412, 396, 402, 402), X = c(4L, 1L, 1L, 16L, 9L, 4L, 1L, 
16L, 16L, 9L, 1L, 25L, 4L, 1L, 25L, 36L, 4L, 1L, 1L, 1L, 9L, 
1L, 4L, 9L, 4L, 25L, 9L, 9L, 4L, 9L, 9L, 36L, 4L, 4L, 25L, 1L, 
4L, 1L, 9L, 16L, 16L, 16L, 1L, 9L, 9L, 4L, 1L, 1L, 4L, 16L, 4L, 
4L, 1L, 9L, 9L, 1L, 9L, 36L, 4L, 4L, 4L, 9L, 9L, 1L, 36L, 4L, 
9L, 4L, 1L, 4L, 9L, 1L, 1L, 1L, 144L, 25L, 9L, 16L, 1L, 4L, 1L, 
4L, 16L, 16L, 9L, 9L, 1L, 16L, 1L, 4L, 25L, 4L, 4L, 1L, 16L, 
4L, 1L, 25L, 36L, 100L, 4L, 16L, 9L, 49L, 1L, 16L, 25L, 9L, 4L, 
1L, 36L, 16L, 1L, 4L, 1L, 4L, 16L, 16L, 9L, 4L, 81L, 9L, 36L, 
1L, 4L, 4L, 4L, 1L, 9L, 81L, 9L, 36L, 1L, 25L, 16L, 36L, 1L, 
4L, 36L, 4L, 1L, 9L, 9L, 4L, 4L, 4L, 100L, 9L, 4L, 16L, 150L, 
9L, 9L, 16L, 4L, 9L, 9L, 9L, 64L, 36L, 49L, 16L, 25L, 1L, 196L, 
4L, 4L, 16L, 9L, 16L, 9L, 9L, 4L, 25L, 16L, 16L, 16L, 1L, 9L, 
25L, 49L, 4L, 16L, 9L, 16L, 64L, 1L, 144L, 9L, 9L, 4L, 9L, 1L, 
9L, 81L, 9L, 4L, 81L, 9L, 9L, 1L, 1L, 1L, 324L, 1L, 49L, 4L, 
25L, 25L, 4L, 16L, 9L, 4L, 9L, 9L, 4L, 1L, 64L, 64L, 4L, 1L, 
9L, 49L, 1L, 4L, 9L, 9L, 4L, 4L, 64L, 9L, 9L, 16L, 4L, 25L, 4L, 
4L, 9L, 81L, 9L, 9L, 25L, 49L, 36L, 9L, 4L, 4L, 100L, 25L, 121L, 
9L, 25L, 1L, 4L, 16L, 4L, 9L, 36L, 144L, 16L, 16L, 16L, 9L, 100L, 
1L, 4L, 1L, 4L, 81L, 1L, 9L, 64L, 4L, 1L, 9L, 144L, 4L, 16L, 
16L, 4L, 1L, 16L, 225L, 1L, 9L, 9L, 9L, 16L, 121L, 4L, 9L, 4L, 
4L, 16L, 9L, 9L, 9L, 144L, 16L, 9L, 4L, 4L, 25L, 4L, 95L, 266L, 
4L, 4L, 4L, 1L, 25L, 9L, 1L, 1L, 4L, 1L, 144L, 4L, 1L, 9L, 1L, 
49L, 1L, 4L, 25L, 49L, 4L, 25L, 4L, 36L, 1L, 1L, 49L, 144L, 16L, 
9L, 100L, 4L, 36L, 100L, 4L, 4L, 9L, 4L, 9L, 9L, 1L, 49L, 9L, 
4L, 4L, 25L, 4L, 9L, 9L, 1L, 49L, 1L, 196L, 16L, 9L, 16L, 1L, 
4L, 1L, 9L, 4L, 1L, 1L, 4L, 75L, 121L, 1L, 9L, 1L, 4L, 49L, 36L, 
4L)), .Names = c(""Y_11"", ""Y_12"", ""Y_21"", ""Y_22"", ""X""), row.names = c(1L, 
3L, 4L, 5L, 6L, 7L, 8L, 10L, 11L, 12L, 15L, 16L, 18L, 20L, 22L, 
23L, 24L, 25L, 26L, 27L, 28L, 31L, 32L, 33L, 34L, 36L, 38L, 39L, 
40L, 41L, 42L, 43L, 45L, 46L, 47L, 48L, 51L, 52L, 53L, 54L, 55L, 
56L, 57L, 59L, 60L, 61L, 62L, 63L, 65L, 67L, 68L, 71L, 72L, 73L, 
74L, 75L, 76L, 77L, 80L, 81L, 82L, 83L, 84L, 85L, 86L, 87L, 88L, 
89L, 90L, 91L, 92L, 93L, 94L, 95L, 96L, 97L, 98L, 99L, 100L, 
101L, 102L, 104L, 105L, 107L, 108L, 109L, 110L, 111L, 112L, 114L, 
116L, 117L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 
128L, 129L, 130L, 131L, 132L, 134L, 135L, 136L, 137L, 138L, 139L, 
140L, 141L, 142L, 143L, 144L, 145L, 147L, 148L, 149L, 150L, 151L, 
153L, 155L, 156L, 157L, 158L, 159L, 161L, 163L, 164L, 165L, 166L, 
167L, 168L, 169L, 172L, 176L, 177L, 178L, 179L, 180L, 181L, 182L, 
183L, 184L, 185L, 187L, 188L, 189L, 190L, 191L, 193L, 194L, 195L, 
196L, 197L, 198L, 199L, 200L, 201L, 203L, 204L, 205L, 206L, 207L, 
208L, 209L, 210L, 212L, 214L, 215L, 216L, 217L, 218L, 219L, 220L, 
221L, 222L, 223L, 224L, 225L, 226L, 227L, 228L, 229L, 230L, 231L, 
232L, 236L, 237L, 238L, 239L, 240L, 241L, 242L, 244L, 245L, 247L, 
248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 
260L, 261L, 262L, 264L, 265L, 266L, 268L, 269L, 270L, 272L, 273L, 
274L, 276L, 279L, 280L, 281L, 282L, 283L, 284L, 285L, 286L, 288L, 
289L, 290L, 291L, 292L, 293L, 294L, 296L, 297L, 298L, 300L, 301L, 
303L, 304L, 305L, 306L, 307L, 308L, 309L, 310L, 311L, 312L, 313L, 
314L, 315L, 316L, 317L, 318L, 319L, 321L, 322L, 323L, 325L, 327L, 
328L, 329L, 330L, 332L, 333L, 334L, 335L, 336L, 337L, 338L, 341L, 
342L, 343L, 345L, 346L, 347L, 348L, 349L, 350L, 351L, 353L, 355L, 
356L, 358L, 359L, 361L, 362L, 363L, 364L, 365L, 366L, 369L, 370L, 
372L, 373L, 374L, 375L, 376L, 378L, 379L, 382L, 383L, 384L, 385L, 
387L, 388L, 389L, 390L, 391L, 393L, 394L, 395L, 396L, 398L, 400L, 
401L, 402L, 405L, 406L, 407L, 408L, 409L, 410L, 411L, 412L, 413L, 
414L, 415L, 416L, 417L, 418L, 419L, 420L, 421L, 422L, 423L, 424L, 
426L, 427L, 429L, 430L, 431L, 432L, 434L, 435L, 436L, 439L, 440L, 
442L, 443L, 445L, 446L, 447L, 448L, 449L, 451L, 452L, 453L, 454L, 
455L, 457L, 458L, 459L, 460L, 461L, 462L, 463L, 464L, 466L, 467L, 
468L, 469L, 470L), class = ""data.frame"", na.action = structure(c(2L, 
9L, 13L, 14L, 17L, 19L, 21L, 29L, 30L, 35L, 37L, 44L, 49L, 50L, 
58L, 64L, 66L, 69L, 70L, 78L, 79L, 103L, 106L, 113L, 115L, 118L, 
133L, 146L, 152L, 154L, 160L, 162L, 170L, 171L, 173L, 174L, 175L, 
186L, 192L, 202L, 211L, 213L, 233L, 234L, 235L, 243L, 246L, 259L, 
263L, 267L, 271L, 275L, 277L, 278L, 287L, 295L, 299L, 302L, 320L, 
324L, 326L, 331L, 339L, 340L, 344L, 352L, 354L, 357L, 360L, 367L, 
368L, 371L, 377L, 380L, 381L, 386L, 392L, 397L, 399L, 403L, 404L, 
425L, 428L, 433L, 437L, 438L, 441L, 444L, 450L, 456L, 465L), .Names = c(""2"", 
""9"", ""13"", ""14"", ""17"", ""19"", ""21"", ""29"", ""30"", ""35"", ""37"", ""44"", 
""49"", ""50"", ""58"", ""64"", ""66"", ""69"", ""70"", ""78"", ""79"", ""103"", 
""106"", ""113"", ""115"", ""118"", ""133"", ""146"", ""152"", ""154"", ""160"", 
""162"", ""170"", ""171"", ""173"", ""174"", ""175"", ""186"", ""192"", ""202"", 
""211"", ""213"", ""233"", ""234"", ""235"", ""243"", ""246"", ""259"", ""263"", 
""267"", ""271"", ""275"", ""277"", ""278"", ""287"", ""295"", ""299"", ""302"", 
""320"", ""324"", ""326"", ""331"", ""339"", ""340"", ""344"", ""352"", ""354"", 
""357"", ""360"", ""367"", ""368"", ""371"", ""377"", ""380"", ""381"", ""386"", 
""392"", ""397"", ""399"", ""403"", ""404"", ""425"", ""428"", ""433"", ""437"", 
""438"", ""441"", ""444"", ""450"", ""456"", ""465""), class = ""omit""))
</code></pre>
"
"0.0839181358296689","0.0825488343412996","219775","<p>I am trying to build a regression model using two time series data in R. There is not much correlation between the two time series, so I am using trend part of both time series(using STL decomposition) for the model. The maximum correlation between these two is about 0.6 at a fixed lag. Is this sufficient to establish that one time series affects the other and build a regression model upon them.</p>
"
"0.178017248729078","0.175112521625334","220001","<p>I'm trying to model a logistic regression in R between two simple variables:</p>

<ul>
<li>Rating: An independent ordered categorical one, ranging from 1 to 99 (1, 2, 3, 4, 5, 99 in particular, 1 is the best)</li>
<li>Result: A dependent binary variable (0-1, not accepted/accepted)</li>
</ul>

<p>The formula I use is </p>

<pre><code>glm(formula = result_dummy ~ best_rating, family = binomial(link = ""logit""), 
    data = cd[1:10000, ])
</code></pre>

<p>result_dummy is a 0/1 numerical variable (original result column was a factor) and scaled_rating is the rating column after use the R <code>scale</code> function.</p>

<p>My thought here was to find a negative correlation (low rating -> more probability to accept) but the more samples I use the more odd results I find:</p>

<pre><code>10 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.6484     0.7413   0.875    0.382
scaled_rating  -5.9403     5.8179  -1.021    0.307
</code></pre>

<hr>

<pre><code>100 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)   -0.09593    0.27492  -0.349  0.72714   
scaled_rating -5.06251    1.76645  -2.866  0.00416 **
</code></pre>

<hr>

<pre><code>1000 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.03539    0.09335  -0.379    0.705    
scaled_rating -6.81964    0.62003 -10.999   &lt;2e-16 ***
</code></pre>

<hr>

<pre><code>10000 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     0.2489     0.0291   8.553   &lt;2e-16 ***
scaled_rating  -7.2319     0.2004 -36.094   &lt;2e-16 ***
</code></pre>

<hr>

<p>Notes:
I know that after the fit I should check residual plot, normality assumptions, etc. etc. but nonetheless I find really strange this behaviour.</p>

<p>I also have similar results using simply the rating column instead of the scaled one.</p>

<p>Edit:
The <code>rating</code> variable is not really an ordinal one, so as pointed out by @Scortchi maybe it would be better to treat it as a categorical one.
I have surely better results and model stability, obviously the model is a simple one and the residual error would be always high (because some variables as not been included in the model).
Indeed, including the frequency table as requested shows that the rating variable IS NOT sufficient for having a clear separation between the result outcome.</p>

<pre><code>          0      1
  1    2881  42564
  2   13878 129292
  3   36839 179500
  4   43511  97148
  5   37330  47002
  6   31801  21228
  7   19096   6034
  99  10008      3
</code></pre>
"
"0.0419590679148345","0.0412744171706498","221819","<p>I have a doubt about use of linear regression.</p>

<ol>
<li>If the correlation between two variables is 0, is there any use of applying linear regression on those variables?</li>
<li>If possible can you explain when we should use regression methods (only when correlation is +-1?).</li>
</ol>
"
"0.0839181358296689","0.0825488343412996","224078","<p>I have three related questions on the package CausalImpact in R. The package can be found <a href=""https://github.com/google/CausalImpact"" rel=""nofollow"">here</a> and a reproducible example is below.</p>

<ol>
<li>Do I basically understand correctly, that the model makes ""1-step ahead""
predictions? I assume it works like a simple lm model that makes
lots of regressions for t+1 with predictors values from t-1 and then looks for the most contributory predictors?</li>
<li>When talking about ""coefficients"", does that mean they are (Pearson)
correlation coefficients?</li>
<li>The function <code>plot(impact$model$bsts.model,""coefficients"")</code> produces
a plot with inclusion probabilities ranging from 0 to 1. Is there
any way to access the actual values in a table? I found
that <code>colMeans(impact$model$bsts.model$coefficients)</code> provides some
values but I'd like to have a confirmation for this.</li>
<li>In my code, I changed <code>bsts.model &lt;- bsts(y ~ x1, ss, niter = 1000)</code> to <code>bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)</code> in order to get values for different variables that I defined before. I did not change the cbind functions for that, as I weren't sure if that was necessary. Now I wonder what to do when I do have a table with -let's say- 200 predictors? Do I have to enter x1 to x200 after <code>bsts(y ~</code> to find the best predictors?</li>
</ol>

<p>R code below:</p>

<pre><code>install.packages(""devtools"")
library(devtools)
devtools::install_github(""google/CausalImpact"")
#Download the tar from the git and then install the package in RStudio.

library(CausalImpact)

set.seed(1)
x1 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = 100)
x2 &lt;- 50 + arima.sim(model = list(ar = 0.899), n = 100)
x3 &lt;- 80 + arima.sim(model = list(ar = 0.799), n = 100)
x4 &lt;- 1.25 * x1 + rnorm(100)
x5 &lt;- 101 + arima.sim(model = list(ar = 0.999), n = 100)
y &lt;- 1.2 * x1 + rnorm(100)
y[71:100] &lt;- y[71:100] + 10
data &lt;- cbind(y, x1)

dim(data)
head(data)
data
matplot(data, type = ""l"")

time.points &lt;- seq.Date(as.Date(""2014-01-01""), by = 1, length.out = 100)
data &lt;- zoo(cbind(y, x1), time.points)
head(data)

pre.period &lt;- as.Date(c(""2014-01-01"", ""2014-03-11""))
post.period &lt;- as.Date(c(""2014-03-12"", ""2014-04-10""))

impact &lt;- CausalImpact(data, pre.period, post.period)
plot(impact)

summary(impact)
summary(impact, ""report"")
impact$summary

post.period &lt;- c(71, 100)
post.period.response &lt;- y[post.period[1] : post.period[2]]
y[post.period[1] : post.period[2]] &lt;- NA

ss &lt;- AddLocalLevel(list(), y)

bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)

impact &lt;- CausalImpact(bsts.model = bsts.model,post.period.response =     post.period.response)

plot(impact)
summary(impact)
summary(impact, ""report"")

plot(impact$model$bsts.model,""coefficients"")
plot(impact$model$bsts.model, ""coef"", inc = .1)
plot(impact$model$bsts.model, ""coef"", inc = .05)

colMeans(impact$model$bsts.model$coefficients)
</code></pre>
"
"0.0938233281301002","0.0922924025252575","224310","<p>This is a two part question concerning linear regression in R. Here is my code and what my residual plot looks like before transformation:</p>

<pre><code>linear_model &lt;- lm(dependent ~ ., data=independent)
residuals &lt;- resid(linear_model)
plot(residuals)
</code></pre>

<p>[I'll add image, as soon as I've got enough reputation to post three links.]</p>

<p>Obviously there is a pattern, although I find it hard to describe. In any case I would think there is evidence for a non-linear relationship. The only function I can think of is $sine$. So I applied the $sine$ function on the dependent variable, played with it a bit and ended up with a shift of $1/2*\pi$ (so the same as cosinus). This what my code and residual plot look like after transformation:</p>

<pre><code>linear_model &lt;- lm(sin((1/2)*pi+dependent) ~ ., data=independent)
residuals &lt;- resid(linear_model)
plot(residuals)
</code></pre>

<p><a href=""http://i.stack.imgur.com/pJ91J.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pJ91J.png"" alt=""enter image description here""></a></p>

<p><strong>Question 1</strong>
Is the application of $sine$ on the dependent variable legitimate?</p>

<p><strong>Question 2</strong>
Although there still is a pattern - maybe even more distinct than before - the residuals are much lower. Does that indicate that the transformation was the right thing to do? And should I try to get rid of the still existing pattern?</p>

<p>Finally, the model's adjusted RÂ² value increased from .13 to .3.</p>

<p><strong>Edit:</strong> As a reaction to the comments, some more information on the dataset. The data has been recorded over a time span of <s>two</s> four weeks, 18 hours each day with intervals between roughly 2-10 minutes. Here is the autocorrelation function estimate of the dependent variable:</p>

<pre><code>acf(dependent)
</code></pre>

<p><a href=""http://i.stack.imgur.com/A0S1D.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/A0S1D.png"" alt=""enter image description here""></a></p>

<p>Apparently, the answer to question 1 is <em>No</em>.</p>
"
"0.145782423805283","0.154434727891351","225283","<p>Iâ€™m analyzing crowdsourced Twitter data, where workers labeled tweets. Within my dataset (N=2,400), I have one IV (call it â€˜dsâ€™) with 2 levels that differentiates which dataset the workers labeled. I have four factors of interest (what workers labeled) -- these are my DVs (let's call them f1, f2, f3, f4). Three of those factors are binomial &lt;0,1>, and one multinomial &lt;0, 1, 2>. Even though the latter can be treated as ordinal, I'm working under the assumption it is nominal. Finally, my datasets are of unequal lengths.</p>

<p>My goal is to analyze the relationship between each of the labeled factors for each level of the IV. More specifically, <strong>I want to tease out the different contributions of each of those factors on each dataset quantitatively, i.e., show amount of variance explained</strong> (e.g., ds1 influenced f1 more than f2, while the inverse for ds2). The end game is to model each factor into a scoring function, which allows me to compute a unified score. Hence, I need to back up the parameter weights for this function.</p>

<p>A snippet of my data frame looks like this:</p>

<pre><code>   f1 f2 f3 f4 ds
1   1  0  1  0  1
2   0  0  0  2  1
3   0  0  1  1  2
4   1  1  0  2  2
</code></pre>

<p>What I initially did was to compute correlations between each factor, and used the strengths of those correlations to back up my scoring function. However, given the many posts and tutorials I've been reading, it seems I need to make use of a mix of logistic and multinomial regression. What I have done so far is run binomial logit (using ?glm with class â€˜binomial') on the first 3 factors, and multinomial regression (using ?nnet) on f4. However, it seems I can only assess one outcome variable at a time.</p>

<p>For f1-f3, I have run the following R code:</p>

<pre><code>fit &lt;- glm(f1 ~ ds, data = xx, family = ""binomial"")
summary(fit)
confint.default(fit)
wald.test(b = coef(fit), Sigma = vcov(fit), Terms = 2)
</code></pre>

<p>For f4:</p>

<pre><code>fit &lt;- multinom(f4 ~ ds, data = xx)
summary(fit)
z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1))*2
</code></pre>

<p>My questions:</p>

<p><strong>1.</strong> Is running such logistic regression analyses appropriate for what I want to do, namely to tease out contributions of each factor? If so, is it meaningful to compare the coefficients of each factor with the other, when computed separately? Or is simply showing a correlation matrix sufficient in my case?</p>

<p><strong>2.</strong> Are there alternative techniques to assess all outcome variables/DVs at once, with respect to each level of my IV? If so, could you please provide me with some pointers (ideally for R)? I'm now looking into hierarchical multinomial marginal (HMM) models... </p>

<p>If something is unclear above, Iâ€™d be happy to clarify.</p>
"
"NaN","NaN","225645","<p>I am trying to perform a logistic regression with the following code </p>

<blockquote>
  <p><code>Y ~ x1+x2+x3,data=data, family=binomial(link=""logit"")</code>. </p>
</blockquote>

<p>However on inspection of both the outcome and predictors i noticed that they are characterized by spatial auto-correlation. My question is, how do I account for the spatial auto-correlation, to get better coefficients? </p>
"
"NaN","NaN","226577","<p>I have a count response across 33 years which I am analyzing using quasipoisson regression.</p>

<pre><code>mdl&lt;- glm(y ~ year, family=""quasipoisson"")
</code></pre>

<p>Now since <code>y</code> is a temporal data, I need to account for temporal autocorrelation at lag(1). I could do this in using <code>gls</code></p>

<pre><code>mdl &lt;- gls(y ~ year,correlation = corAR1(form=~year))
</code></pre>

<p>but in <code>gls</code> how do I specify the error family which is ""quasipoisson""?</p>

<p>Thanks</p>
"
"0.139162484826546","0.136891755195648","228238","<p>I'm having a strange problem running a meta-regression using the function <code>rma.mv()</code> in the 'metafor' package in R.</p>

<p>Since some of my data are from multiple-endpoint studies, I have calculated the variance-covariance matrix so that correlations between outcomes are taken into account. I'm also using random effects at study and treatment level. As far as I'm aware, I have now covered all issues with regard to dependent effect sizes.</p>

<p>The model looks like this:</p>

<pre><code>cov_mod &lt;- rma.mv(Hedges_g, cov, mods = ~ days, random = ~ treatment | study, data = rev)
</code></pre>

<p>When running the code, it gives this error message:</p>

<pre><code>Error in rma.mv(Hedges_g, cov9, mods = ~days, random = ~1 | treatment/study,  : 
  Error during optimization.
In addition: Warning message:
In rma.mv(Hedges_g, cov9, mods = ~days, random = ~1 | treatment/study,  :
  V appears to be not positive definite.
</code></pre>

<p>I have discovered that the problem lies with one particular study (9 effect sizes in total, coming from 3 treatment groups that were each tested at 3 moments in time). When I remove this study from the data set, the code runs without problem.</p>

<p>Thus, apparently this particular study causes the matrix to be 'not positive definite'. I have read that this likely means that ""at least one of [the] variables can be expressed as a linear combination of the others"" (<a href=""http://stats.stackexchange.com/questions/30465/what-does-a-non-positive-definite-covariance-matrix-tell-me-about-my-data"">source</a>).</p>

<p>However, here comes the strange thing: I have replaced all values in the variance-covariance matrix relating to this particular study with random numbers between 0-1 (maintaining the symmetry), and the error message remains unchanged. I am puzzled, because the matrix can no longer be linearly predictable if it contains random numbers.</p>

<p>What could be the issue?</p>
"
"0.126511349842314","0.136891755195648","228641","<p>Consider the following dataset I want to use as the independent variables to conduct linear regression on:</p>

<pre><code>set.seed(42)
sa = runif(10)
sb = runif(10)
sc = sb+sa
sd = sb-sa
df = data.frame(sa,sb,sc,sd)
</code></pre>

<p>Now I want to perform tests for multicollinearity. I'm aware of the <code>ppcor</code> package, which calculates the partial correlation between the variables. In this case:</p>

<pre><code>&gt; pcor(df)
$estimate
            [,1]        [,2]       [,3]       [,4]
[1,]  1.00000000  0.06649968 -0.7325597  0.7706902
[2,]  0.06649968  1.00000000 -0.6304810 -0.6870502
[3,] -0.73255975 -0.63048097  1.0000000  0.1308260
[4,]  0.77069021 -0.68705016  0.1308260  1.0000000
</code></pre>

<p>As far as I know, there is no way of telling that <code>sc</code> and <code>sd</code> are linear combinations of <code>sa</code> and <code>sb</code>, just by looking at the estimates (or the other outputs of <code>pcor</code>, for that matter).</p>

<p>The only method that comes to my mind, is applying linear regression on each of the independent variables like so:</p>

<pre><code>summary(lm(sc~sa+sb,df))

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 1.404e-16  3.915e-17 3.587e+00   0.0089 ** 
sa          1.000e+00  4.716e-17 2.121e+16   &lt;2e-16 ***
sb          1.000e+00  4.135e-17 2.418e+16   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.672e-17 on 7 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 

summary(lm(sd~sa+sb,df))

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 1.404e-16  3.915e-17 3.587e+00   0.0089 ** 
sa          1.000e+00  4.716e-17 2.121e+16   &lt;2e-16 ***
sb          1.000e+00  4.135e-17 2.418e+16   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.672e-17 on 7 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 
</code></pre>

<p>I'm wondering two things: </p>

<ol>
<li><p>Is my approach with linear regression reasonable? I think the downside is, that it can only detect linear correlation. But non-linear correlation shouldn't be a problem with linear regression, right?</p></li>
<li><p>Is there an R function/package that automatically checks for multiple correlation?</p></li>
</ol>
"
"0.0726752374667264","0.0714893875923587","229702","<p>In a follow up to one of the business problem i have discussed here,<br>
for one of the logistic regression model, i have 19 predictor variables of     which around 8 are categorical with multiple categories.   </p>

<p>Instead of including all in the model where one could correlate to another,<br>
i am thinking of using a chi square test of independence at a given p value.<br>
With this i would come to realization if there is any correlation among the     categorical predictors.  </p>

<p>If so i would include the correlated variables one by one in the model<br>
and would used the significant one.<br>
I have researched on this on SE where they have suggested to use chi-<br>
square but unsure if my approach to drop these correlated var is justified.<br>
Please advice. </p>
"
"0.125877203744503","0.123823251511949","229884","<p>I have a cancer classification problem (type A vs type B) on radiological images from which i have generated 756 texture-based predictive features (wavelet transform followed by texture analysis, i.e., features described by Haralick, Amasadun etc) and 8 semantic features based on subjective assessment by expert radiologist. This is entirely for research and publication to show that these predictive features may be useful in this particular problem. I do not intend to deploy the model for practitioners. </p>

<p>I have 107 cases. 60% cases are type A and 40% type B (in keeping with their natural proportions in population). I have done several iterations of model development with varying results. One particular method is giving me an 80% 80% classification accuracy but I am suspicious that my method is not going to stand critical analysis. I am going to outline my method and a few alternatives. I will be grateful if someone can pick if it is flawed. I have used R for this:</p>

<p>Step 1: Split into 71 training and 36 test cases.<br>
Step 2: remove correlated features from training dataset (766 -> 240) using findcorrelation function in R (caret package)<br>
Step 3: rank training data features using Gini index (Corelearn package)<br>
Step 4: Train multivariate logistic regression models on top 10 ranked features using subsets of sizes 3 , 4, 5 ,and 6 in all possible combination (<sup>10</sup>C<sub>3</sub>=252, <sup>10</sup>C<sub>4</sub>=504, <sup>10</sup>C<sub>5</sub>=630). So <strong>total 1386 multivariate logistic regression models were trained</strong> using 10-fold cross-validation and tested on test dataset.<br>
Step 5: Of these I selected a model which gave the best combination of training and test dataset accuracy, i.e., 3 feature model with 80% 80% accuracy.<p></p>

<p>Somehow running 1300 permutations seems quite dodgy to me and seems to have introduced some false discovery. Just want to confirm if this is a valid ML technique or whether I should skip step 4 and only train on top 5 ranked features without running and permutations.</p>

<p>Thanks. <p> PS I experiemented a bit with naive bayes and random forests but get rubbish test set accuracy so dropped them</p>

<p>====================</p>

<h1>UPDATE</h1>

<p>Following discussion with SO members, i have changed the model drastically and thus moved more recent questions regarding model optimisation into a new post <a href=""http://stats.stackexchange.com/questions/232829/lasso-regularised-classification-highly-variable-choice-of-lambda-min-on-repeate"">LASSO regularised classification highly variable choice of lambda.min on repeated cv</a></p>
"
"0.0419590679148345","0.0412744171706498","230372","<p>I was wondering if someone on here could help</p>

<p>I recently ran a Spatial Durbin Regression model in R which came back with two of my three independent variables had significant beta coefficients. A colleague then advised me that I should run a sensitivity test using a negative binomial model to see if I get the same results. However the results are different as all my beta coefficients become significant.</p>

<p>What I am trying to understand is would this likely be due to the incorporation of the spatially lagged independent variable (which has a significant rho value in the spatial Durbin regression)? And does it make sense to use the negative binomial as a 'sensitivity test' as I would think the assumptions would be different, particularly around spatial autocorrelation. </p>
"
"0.118678165819385","0.116741681083556","230532","<p>Trying to get the Bayes Factor for a correlation between two variables in my data, I tried three different functions. All implement the Jeffreysâ€“Zellnerâ€“Siow (JZS) prior, but I get quite different results with the three approaches. Two questions:</p>

<ol>
<li><p>Is this suspicious, or is it reasonable that they produce different values, as the implementations are slightly different?</p></li>
<li><p>Is there a consensus on the best measure to use?  </p></li>
</ol>

<p>My data:</p>

<pre><code>a=rnorm(100,1,2)
b=rnorm(100,.8,1.5)
myData &lt;- data.frame(a=a, b=b)
</code></pre>

<p>I try the <code>jzs_corbf</code> function, described and implemented <a href=""http://www.ncbi.nlm.nih.gov/pubmed/22798023"" rel=""nofollow"">here</a> (<a href=""http://dsquintana.com/post/98962697485/how-to-calculate-a-bayes-factor-for-correlations"" rel=""nofollow"">shorter version</a>)</p>

<pre><code>cor.resu.a_b &lt;- cor.test(myData$a, myData$b, method=c(""pearson""))
cor.resu.a_b$estimate
n = 100
r = cor.resu.a_b$estimate
jzs_corbf(r,n)
[1] 0.08206358
</code></pre>

<p>I also tried the convenience function from the <code>BayesFactor</code> package:</p>

<pre><code>require(BayesFactor)
regressionBF(b ~ a, data = myData, progress=FALSE)

Bayes factor analysis
--------------
[1] a : 0.2181081 Â±0%

Against denominator:
  Intercept only 
---
Bayes factor type: BFlinearModel, JZS
</code></pre>

<p>And I also tried the a function described <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4891395/"" rel=""nofollow"">recently</a> (<a href=""https://osf.io/9d4ip/"" rel=""nofollow"">code</a>)</p>

<pre><code>bf10JeffreysIntegrate(n=100, r=r)

      cor 
0.1297927
</code></pre>

<p>While in this case the differences are only numerical, in my real data I get quite big differences that make it more difficult to decide on an interpretation. </p>

<p><a href=""http://stats.stackexchange.com/questions/184950/calculating-bayes-factor-from-a-correlation-coefficient"">Related</a></p>
"
"0.23404843949959","0.244182744982501","231872","<p>For a better understanding of how r is conducting a logistic regression I created the following test-data (the two predictors and the criterion are binary variables):</p>

<pre><code>   UV1 UV2 AV
1    1   1  1
2    1   1  1
3    1   1  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   0  1
9    0   0  1
10   0   0  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>AV = dependent variable/criterion</p>

<p>UV1 / UV2 = both independant variables/predictors</p>

<p>For measuring the UVs effect on the AV a logistic regression is necessary, as the AV is a binary variable. Hence i used the following code</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata, family = ""binomial"")
</code></pre>

<p>including <strong>""family = ""binomial""""</strong>. Is this correct ( I think so :-))?</p>

<p>Regarding my test-data, I was wondering about the whole model, especially
the estimators and sigificance:</p>

<pre><code>&gt; summary(lrmodel)


Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7344  -0.2944   0.3544   0.7090   1.1774  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.065e-15  8.165e-01   0.000    1.000
UV1         -1.857e+01  2.917e+03  -0.006    0.995
UV2          1.982e+01  2.917e+03   0.007    0.995

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 17.852  on 17  degrees of freedom
AIC: 23.852

Number of Fisher Scoring iterations: 17
</code></pre>

<ol>
<li><p>Why is UV2 not significant. See therefore that for group AV = 1 there are 7 cases with UV2 = 1, and for group AV = 0 there are only 3 cases with UV2 = 1. 
I was expecting that UV2 is a significant discriminator.</p></li>
<li><p>Despite the not-significance of the UVs, the estimators are - in my opinion- very high (e.g. for UV2 = 1.982e+01). How is this possible?</p></li>
<li><p>Why isn't the intercept 0,5?? We have 5 cases with AV = 1 and 5 cases with AV = 0.</p></li>
</ol>

<p>Further: I created UV1 as a predictor I expected not to be significant:  for group AV = 1 there are 5 cases withe UV1 = 1, and for group AV = 0 there are 5 cases withe UV1 = 1 as well.</p>

<p>The whole ""picture"" I gained from the logistic is confusing me...</p>

<p>What was consuming me more:
When I run a ""NOT-logistic"" regression (by omitting <strong>""family = ""binomial""</strong>)</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata,)
</code></pre>

<p>I get the expected results</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7778  -0.1250   0.1111   0.2222   0.5000  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   0.5000     0.1731   2.889  0.01020 * 
UV1          -0.5000     0.2567  -1.948  0.06816 . 
UV2           0.7778     0.2365   3.289  0.00433 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for gaussian family taken to be 0.1797386)

    Null deviance: 5.0000  on 19  degrees of freedom
Residual deviance: 3.0556  on 17  degrees of freedom
AIC: 27.182

Number of Fisher Scoring iterations: 2
</code></pre>

<ol>
<li>UV1 is not significant! :-)</li>
<li>UV2 has an positive effect on AV = 1! :-)</li>
<li>The intercept is 0.5! :-)</li>
</ol>

<p>My overall question: Why isn't logistic regression (including ""family = ""binomial"") producing results as expected, but a ""NOT-logistic"" regression (not including ""family = ""binomial"") does?</p>

<p>Update:
are the observations described above because of the correlation of UV1 and UV 2. Corr = 0.56
After manipulating the UV2's data </p>

<p>AV: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>UV1: 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0</p>

<p>UV2: <strong>0, 0, 0,</strong> 1, 1, 1, 1, <strong>1, 1, 1</strong>, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>(I changed the positions of the three 0s with the three 1s in UV2 to gain a correlation &lt; 0.1 between UV1 and UV2) hence:</p>

<pre><code>UV1 UV2 AV
1    1   0  1
2    1   0  1
3    1   0  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   1  1
9    0   1  1
10   0   1  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>to avoid correlation, my results come closer to my expectations:</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.76465  -0.81583  -0.03095   0.74994   1.58873  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.1248     1.0862  -1.036   0.3004  
UV1           0.1955     1.1393   0.172   0.8637  
UV2           2.2495     1.0566   2.129   0.0333 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 22.396  on 17  degrees of freedom
AIC: 28.396

Number of Fisher Scoring iterations: 4
</code></pre>

<p>But why does the correlation influence the results of the logistic regression and not the results of the ""not-logistic"" regression? </p>
"
"0.0726752374667264","0.0714893875923587","231974","<p>Many Gaussian process packages are available in R. For example there is $\textbf{BACCO}$ that offers some calibration techniques, $\textbf{mlegp}$ and $\textbf{tgp}$ focusing on treed models and parameter estimation and $\textbf{GPML}$ for Gaussian process classification and regression. </p>

<p>The problem with these packages is that the choice of correlation function is restricted. Only some choices are provided for building the correlation function (Gaussian, Matern, etc...).</p>

<p>Does anyone have specific experience on how to can insert my own correlation function and just use the optimization routines available in these packages which are specifically tailored for GP's. Or is there a package which allows me to do that ?</p>
"
"0.0629386018722517","0.0825488343412996","232433","<p>I'm doing a liner regression fit using R. I used <code>lm()</code> to do the regression. Then I standardize my data using <code>scale()</code> and again do the regression on standardize data using <code>lm()</code>.</p>

<p>Surprisingly the regression coefficient of one variable was positive before standardization and after standardization I found it is showing negative coefficient. I checked the correlation between that variable and my predictor. It has positive significant correlation.  </p>

<pre><code>data_bd2=data_2[,c(1:3,5:7)] 
str(data_bd2) 
fit_bd=lm(data_bd2) 
vif(fit_bd) 
summary(fit_bd) 
scale_data_bd2=data.frame(scale(data_bd2))
colnames(scale_data_bd2)=colnames(data_bd2) 
fit_bd_std=lm(scale_data_bd2) 
summary(fit_bd_std) 
</code></pre>

<p>Can you please help me understand why sign of regression coefficient differ before and after standardization?</p>
"
