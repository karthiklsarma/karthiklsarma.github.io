"V1","V2","V3","V4"
"0.134433214484466","0.137360563948689","  4187","<p>Hello
I have two problems that sound like natural candidates for multilevel/mixed models, which I have never used.  The simpler, and one that I hope to try as an introduction, is as follows:
The data looks like many rows of the form </p>

<p><code>x y innergroup outergroup</code></p>

<p>where x is a numeric covariate upon which I want to regress y (another numeric variable), each y belongs to an innergroup, and each innergroup is nested in an outergroup (i.e, all the y in a given innergroup belong to the same outergroup).  Unfortunately, innergroup has a lot of levels (many thousands), and each level has relatively few observations of y, so I thought this sort of model might be appropriate.  My questions are</p>

<ol>
<li><p>How do I write this sort of multilevel formula?</p></li>
<li><p>Once <strong>lmer</strong> fits the model, how does one go about predicting from it?  I have fit some simpler toy examples, but have not found a predict() function.  Most people seem more interested in inference than prediction with this sort of technique.
I have several million rows, so the computations might be an issue, but I can always cut it down as appropriate.  </p></li>
</ol>

<p>I won't need to do the second for some time, but I might as well begin thinking about it and playing around with it.  I have similar data as before, but without x, and y is now a binomial variable of the form $(n,n-k)$.  y also exhibits a lot of overdispersion, even within innergroups.  Most of the $n$ are no more than 2 or 3 (or less), so to derive estimates of the success rates of each $y_i$ I have been using the beta-binomial shrinkage estimator $(\alpha+k_i)/(\alpha+\beta+n_i)$, where $\alpha$ and $\beta$ are estimated by MLE for each innergroup separately.  This is has been somewhat adequate, but data sparsity still plagues me, so I would like to use all the data available.  From one perspective, this problem is easier since there is no covariate, but from the other the binomial nature makes it more difficult.  Does anyone have any high (or low!) level guidance?</p>
"
"0.0896221429896442","0.091573709299126","  6927","<p>I'm doing a simulation study which requires bootstrapping estimates obtained from a generalized linear mixed model (actually, the product of two estimates for fixed effects, one from a GLMM and one from an LMM). To do the study well would require about 1000 simulations with 1000 or 1500 bootstrap replications each time. This takes a significant amount of time on my computer (many days). </p>

<p><code>How can I speed up the computation of these fixed effects?</code></p>

<p>To be more specific, I have subjects who are measured repeatedly in three ways, giving rise to variables X, M, and Y, where X and M are continuous and Y is binary. We have two regression equations 
$$M=\alpha_0+\alpha_1X+\epsilon_1$$
$$Y^*=\beta_0+\beta_1X+\beta_2M+\epsilon_2$$
where Y$^*$ is the underlying latent continuous variable for $Y$ and the errors are not iid.<br>
The statistic we want to bootstrap is $\alpha_1\beta_2$. Thus, each bootstrap replication requires fitting an LMM and a GLMM. My R code is (using lme4)</p>

<pre><code>    stat=function(dat){
        a=fixef(lmer(M~X+(X|person),data=dat))[""X""]
        b=fixef(glmer(Y~X+M+(X+M|person),data=dat,family=""binomial""))[""M""]
        return(a*b)
    }</code></pre>

<p>I realize that I get the same estimate for $\alpha_1$ if I just fit it as a linear model, so that saves some time, but the same trick doesn't work for $\beta_2$.</p>

<p>Do I just need to buy a faster computer? :)</p>
"
"0.100200602007025","0.102382519472325"," 11457","<p>is it possible to do stepwise (direction = both) model selection in nested binary logistic regression in R? I would also appreciate if you can teach me  how to get:</p>

<ul>
<li>Hosmer-Lemeshow statitistic,</li>
<li>Odds ratio of the predictors, </li>
<li>Prediction success of the model.</li>
</ul>

<p>I used lme4 package of R. This is the script I used to get the general model with all the independent variables:</p>

<pre><code>nest.reg &lt;- glmer(decision ~ age + education + children + (1|town), family = binomial, data = fish)
</code></pre>

<p>where:</p>

<ul>
<li>fish -- dataframe</li>
<li>decision -- 1 or 0, whether the respondent exit or stay, respectively.</li>
<li>age, education and children -- independent variables.</li>
<li>town -- random effect (where our respondents are nested)</li>
</ul>

<p>Now my problem is how to get the best model. I know how to do stepwise model selection but only for linear regression. (<code>step( lm(decision ~ age + education + children, data = fish), direction +""both"")</code>). But this could not be used for binary logistic regression right? also when i add <code>(1|town)</code> to the formula to account for the effects of town, I get an error result. </p>

<p>By the way... I'm very much thankful to Manoel Galdino <a href=""http://stackoverflow.com/questions/5906272/step-by-step-procedure-on-how-to-run-nested-logistic-regression-in-r"">who provided me with the script on how to run nested logistic regression</a>. </p>

<p>Thank you very much for your help.</p>
"
"0.0633724250524478","0.0647523908238176"," 11462","<p>I'm trying to find the best model based on AIC using the stepwise (<code>direction = both</code>) model selection in R using the stepAIC in MASS package.</p>

<p>This is the script i used: </p>

<pre><code>stepAIC (glmer(decision ~ as.factor(Age) + as.factor(Educ) + as.factor(Child), family=binomial, data=RShifting), direction=""both"")
</code></pre>

<p>however I got this error result:</p>

<pre><code>Error in lmerFactorList(formula, fr, 0L, 0L) : 
  No random effects terms specified in formula
</code></pre>

<p>I tried to add <code>(1|town)</code> to the formula since town is the random effect (where the respondents are nested) and ran this script):</p>

<pre><code>stepAIC (glmer(decision ~ as.factor(Age) + as.factor(Educ) + as.factor(Child) + (1|town), family=binomial, data=RShifting), direction=""both"")
</code></pre>

<p>The result is this:</p>

<pre><code>Error in x$terms : $ operator not defined for this S4 class
</code></pre>

<p>I hope you could help me figure out how to solve this problem. Thanks a lot.</p>
"
"0.119496190652859","0.122098279065501"," 12319","<p>Background: Iâ€™m analyzing data with mixed-models (lmer in lme4) from an experiment that had RTs and Error Rates as dependent variables. This is a repeated-measures design with approximately 300 measurements for each of the 190 human subjects. The fixed-effects are 1 between-subjects experimental manipulation (dichotomous), 2 within-subjects experimental manipulations (both dichotomous), and 1 subject variable (continuous, centered). My uncorrelated random effects are the participants, and 2 stimulus characteristics.  For the mixed-models, Iâ€™ve coded the experimental manipulations as a -.5/+.5 contrasts so that the parameters are estimates of the experimental effects and the intercept should be the grand mean.</p>

<p>The grand mean produced by the RT model (740 ms) does not match the mean I get if I average all of the individual trials (730 ms). Why does this happen?</p>

<p>A related question: the GLMM (binomial distribution, logistic link function) for error rates produces a parameter estimate  with an associated Z-score that has an absolute value over 2, but when I look at the means (determined the same way as above) to examine this difference they are tiny and almost identical (0.01353835 vs. 0.01354846). What are the values that I can provide that support the reliable parameter estimate? </p>

<p>I have a feeling the discrepancy between my calculated means and the model estimates has something to do with the random factors (perhaps the grouping by subjects), but Iâ€™m not sure exactly what.</p>

<p>If I want to display descriptive statistics along with the table of mixed model estimates, how should these descriptive be determined? Any points to references, examples, etc. will be greatly appreciated.</p>

<p>If this is all just a brain fart on my part, please let me know that too.</p>

<p>Edit: It is probably also important to mention that the amount of trials and types of trials contributed are not the same for every person. The between-subject manipulation changes the proportions of the different trial types presented, and for RTs only correct trials were analyzed. There were, however, very few errors made.</p>
"
"0.0896221429896442","0.091573709299126"," 21024","<p>Plotting a <strong>glm</strong> binomial model is reasonably simple with the <strong>predict</strong> function. I'm having trouble creating a similar plot for a <strong>glmer</strong> model; predict doesn't work:  </p>

<pre><code>id    &lt;- factor(rep(1:20, 3))
age   &lt;- rep(sample(20:50, 20, replace=T), 3)
age   &lt;- age + c(rep(0, 20), rep(3, 20), rep(6, 20))
score &lt;- rbinom(60, 15, 1-age/max(age))
dfx   &lt;- data.frame(id, age, score)

library(lme4)
glmerb  &lt;- glmer(cbind(score, 15-score) ~ age + (1|id), dfx, family=binomial)
ndf     &lt;- expand.grid(age=10:60) #for extensibility, usually also have factors
ndf$fit &lt;- predict(glmerb, ndf, type=""response"")
*Error in UseMethod(""predict"") : no applicable method for 'predict' applied to an object of class ""mer""*
</code></pre>

<ol>
<li>How can I produce the desired plot?</li>
<li>While I'm at it, what other plots would be useful for this kind of model for either diagnostic, presentation or glam purposes?</li>
</ol>
"
"0.110901743841784","0.113316683941681"," 23270","<p>I am modeling some data where I think I have two crossed random effects.  But the data set is not balanced, and I'm not sure what needs to be done to account for it.</p>

<p>My data is a set of events.  An event occurs when a client meets with a provider to perform a task, which is either successful or not.  There are thousands of clients and providers, and each client &amp; provider participates in varying numbers of events (roughly 5 to 500).  Each client and provider has a level of skill, and the chance that the task is successful is a function of the skills of both participants.  There is no overlap between clients and providers.</p>

<p>I am interested in the respective variances of the population of clients and providers, so we can know which source has a bigger effect on the success rate. I also want to know the specific values of the skills among the client and providers we actually have data for, to identify best/worst clients or providers.  </p>

<p>Initially, I want to assume that the probability of success is driven solely by the combined skill levels of the client and provider, with no other fixed effects.  So, assuming that x is a factor for the client and y is a factor for provider, then in R (using package lme4) I have a model specified as:</p>

<pre><code>  glmer( success ~ (1 | x) + (1 | y), family=binomial(), data=events)
</code></pre>

<p>One problem is that clients are not evenly distributed across providers.  Higher skill clients are more likely to be matched up with higher skill providers.  My understanding is that a random effect has to be uncorrelated with any other predictors in the model, but I'm not sure how to account for it.</p>

<p>Also, some clients and providers have very few events (less than 10), while others have many (up to 500), so there's a wide spread in the amount of data we have on each participant.  Ideally this would be reflected in a ""confidence interval"" around each particpants skill estimate (although I think the term confidence interval isn't quite correct here).</p>

<p>Are crossed random effects going to be problematic because of the unbalanced data?  If so, what are some other approaches I should consider?</p>
"
"0.148621510602115","0.15185781720314"," 24844","<p>I am running 3 models on 3 subsets of the same data.  The set up is as follows:</p>

<ol>
<li>Outcome (DV) is binary categorical</li>
<li>Time (IV) is repeated twice (pre and post)</li>
<li>Treatement (IV of interest) is binary categorical</li>
</ol>

<p>I am interested to know if at time 2 treatment has had an effect on outcome.  I used the lme4 package and used the following R code:</p>

<pre><code>tot.null&lt;-lmer(as.factor(outcome)~Time+(1|id), family=binomial(link='logit'),
             data=df.total)
tot.mod&lt;-lmer(as.factor(outcome)~trt*Time+(Time|id), 
             family=binomial(link='logit'), data=df.total)
anova(tot.null,tot.mod)
summary(tot.mod)
</code></pre>

<p><strong>Data head</strong></p>

<pre><code>   id             trt Time outcome
1   1 peer discussion   -1       1
2   2 peer discussion   -1       1
3   3 peer discussion   -1       0
4   4 peer discussion   -1       1
5   5 peer discussion   -1       1
</code></pre>

<p><strong>str of data</strong></p>

<pre><code>&gt; str(df.total)
'data.frame':   872 obs. of  4 variables:
 $ id     : int  1 2 3 4 5 6 7 8 9 10 ...
     $ trt    : Factor w/ 2 levels ""peer discussion"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Time   : num  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...
     $ outcome: num  1 1 1 1 1 1 1 0 1 0 ...
</code></pre>

<p>The problem is I get an error messoge on the <code>tot.mod</code>:</p>

<pre><code>&gt; tot.mod&lt;-glmer(as.factor(outcome)~trt*Time+(Time|id), family=binomial(link='logit'),
               data=df.total)
Warning message:
In mer_finalize(ans) : false convergence (8)
</code></pre>

<p>I think this is the reason the model is significant but none of the predictors are.  look at the inflated SEs.</p>

<p><strong>Comparison to the null model and the summary of full model</strong></p>

<pre><code>&gt; anova(tot.null,tot.mod)
Data: df.total
Models:
tot.null: as.factor(outcome) ~ Time + (1 | id)
tot.mod: as.factor(outcome) ~ trt * Time + (Time | id)
         Df    AIC    BIC  logLik  Chisq Chi Df            Pr(&gt;Chisq)    
tot.null  3 689.54 703.85 -341.77                                        
tot.mod   7 410.67 444.07 -198.34 286.86      4 &lt; 0.00000000000000022 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; summary(tot.mod)
Generalized linear mixed model fit by the Laplace approximation 
Formula: as.factor(outcome) ~ trt2 * Time + (Time | id) 
   Data: df.total 
   AIC   BIC logLik deviance
 410.7 444.1 -198.3    396.7
Random effects:
 Groups Name        Variance Std.Dev. Corr  
 id     (Intercept)  396.46  19.911         
        Time        1441.98  37.973   0.470 
Number of obs: 872, groups: id, 436

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) 10.09866    3.33921   3.024  0.00249 **
trt21        0.01792    5.10796   0.004  0.99720   
Time        -0.93753    5.79560  -0.162  0.87149   
trt21:Time  -0.84882   10.41073  -0.082  0.93502   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
           (Intr) trt21  Time  
trt21      -0.654              
Time        0.558 -0.365       
trt21:Time -0.311  0.473 -0.557
</code></pre>

<p>What's going on?  Why is the model significant but none of the betas?  In OLS I know this is an indicator of multi-colinearity among predictors.  I don't think that's the reason here.  Please help with understanding this problem as well as the error message (I think they may be connected).  What are some things I should check for?</p>

<p>The other two  models from the same data set (<code>split</code> on a different grouping variable) had no apparent problems.</p>

<p>Thank you in advance.</p>

<p><em>Using R 2.14.2, lme4 v. 0.999375-42 on a win 7 machine</em> </p>
"
"0.0633724250524478","0.0647523908238176"," 25997","<p>I have a 2 by 2 design with 12 subjects.  The two factors are within-subjects variables.  For each cell, 24 responses were collected from each subject.  My question is whether I need to aggregate every 24 responses into a mean and then use the mean in lmer as the dependent variable.  This webpage <a href=""http://talklab.psy.gla.ac.uk/simgen/faq.html"" rel=""nofollow"">http://talklab.psy.gla.ac.uk/simgen/faq.html</a> seems to suggest that aggregation is not needed if use mixed effects model, but I doubt that it is true.</p>

<p>Also, if the responses are binary responses, can I just average them and use generalized lmer with binomial family?</p>

<p>Thanks.</p>
"
"0.0776150525706333","0.0793051585718144"," 30460","<p>I am measuring the change in random effect (random intercept) over time by running models such as </p>

<p><code>glmer( bull~p1+p2+p3+p4+p5+(1|school),subset=(yr=2011),data=bull, family=binomial(link=logit))</code> </p>

<p>using different <code>subset=(yr=20??)</code> for each year. In the comments to my <a href=""http://stats.stackexchange.com/questions/30448/identifying-when-fixed-effects-have-changed-in-a-glmm"">other question</a>, it was stated that another way to do this was to use dummy variables for each year, and specify them in the model as random slopes, thus:</p>

<p><code>glmer(bull~p1+p2+p3+p4+p5+(i1+i2+i3+i4+i5+i6-1|school),data=bull, family=binomial(link=logit))</code> </p>

<p>where <code>i1...i6</code> are the indicator variables of the different years. My question is: how is the output of this model to be interpreted in terms of the longitudinal change in random effects variance ?</p>
"
"0.136711905831823","0.152387863551892"," 30803","<p>I will try to explain my data as good as possible.
So we taged 13 different whales with a tag that records time, depth, speed, angle of descent and ascent 25 samples every second.
The normal diving behaviour of these animals is one deep dive of one hour to 1200 meters followed by a series of 3-7 shallow dives of 20 minutes up to 300 m. Because the tag not always stays the same time in each animal, my data is unbalanced and some tag records have one deep dive and 6 shallow dives while other records have 7 deep dives and 26 shallow dives.
I divided each dive in units of 30 seconds. for each unit I have the next data:
whale number, dive number, total number of fluke strokes in the 30 seconds unit of analysis, mean of the sin of the angle during the 30 seconds unit, swim speed, dive type(ascent or descent), dive direction( if it is a descent or an ascent) and time since the start of the dive and finally my variable response which is presence or absence of one type of fluke stroke called stroke type B.</p>

<p>I think there has to be some autocorrelation between each 30 seconds unit of analyis and need to include it in my model but do not know how!</p>

<p>I am interested to know what affects the presence or absence of the type B stroke (which is a binomial variable with 0 and 1)
so I decide to use a  binomial glmm with whale number as a random effect. I included as well dive number within whale as a random effect.</p>

<p>here is the model.</p>

<pre><code>glmm114&lt;-lmer (StrokeB~ Time * Depth+SINP+flukes*Depth+speed+(1|whale_number)+(0+dive_number|whale_number),data=Luciadeepas, family = binomial)
</code></pre>

<p>this is my final model after taking out the non significative variables, the problem is that due to the interaction a problem appears saying</p>

<pre><code>The false convergence warning message (8)
</code></pre>

<p>I looked on internet and it says is a common problem, and some people says that it doesnt make any change in the output while others says that each variable has to be
divided by 100. but when I do this then the variables that become significant doesnt make any sense. </p>
"
"0.0896221429896442","0.091573709299126"," 32580","<p>While running a simulation in R, I noticed that in fitting one particular model, R spits out a warning message, but if I simply change the baseline category in the response variable, it converges without complaining. The results produced do not differ appreciably, except that one value in the random effect correlation matrix is changed. I am curious why this occurs, and whether it suggests something interesting about the data set (the response variable is binary valued, and the explanatory variables are continuous-valued).</p>

<pre><code>&gt; library(lme4)
&gt; fit1.MS.eE &lt;- glmer(label ~ zSpec * zF1 + zF2 + (1 + zSpec + zF1 + zF2 | part), data = MSall.eE, family = binomial())
Warning message:
In mer_finalize(ans) : singular convergence (7)
&gt; MSall.eE$label &lt;- relevel(MSall.eE$label, ""e"")
&gt; fit1.MS.eE &lt;- glmer(label ~ zSpec * zF1 + zF2 + (1 + zSpec + zF1 + zF2 | part), data = MSall.eE, family = binomial())
&gt;(no warning message)
</code></pre>

<p>I have uploaded <a href=""http://www.acsu.buffalo.edu/~lovegren/sample.R"" rel=""nofollow"">the data set</a> referred to in the code for interested persons.</p>
"
"0.129358420951055","0.145392790714993"," 32994","<p>I've been using the <code>MCMCglmm</code> package recently. I am confused by what is referred to in the documentation as R-structure and G-structure. These seem to relate to the random effects - in particular specifying the parameters for the prior distribution on them, but the discussion in the documentation seems to assume that the reader knows what these terms are. For example:</p>

<blockquote>
  <p>optional list of prior specifications having 3 possible elements: R (R-structure) G (G-structure) and B (fixed effects)............ The priors for the variance structures (R and G) are lists with the expected (co)variances (V) and degree of belief parameter (nu) for the inverse-Wishart</p>
</blockquote>

<p>...taken from from <a href=""http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=MCMCglmm%3aMCMCglmm"">here</a>.</p>

<p><strong>EDIT: Please note that I have re-written the rest of the question following the comments from Stephane.</strong></p>

<p>Can anyone shed light on what R-structure and G-structure are, in the context of a simple variance components model where the linear predictor is 
$$\beta_0 + e_{0ij} + u_{0j} $$
with $e_{0ij} \sim N(0,\sigma_{0e}^2)$ and $u_{0j} \sim N(0,\sigma_{0u}^2)$</p>

<p>I made the following example with some data that comes with <code>MCMCglmm</code></p>

<pre><code>&gt; require(MCMCglmm)
&gt; require(lme4)
&gt; data(PlodiaRB)
&gt; prior1 = list(R = list(V = 1, fix=1), G = list(G1 = list(V = 1, nu = 0.002)))
&gt; m1 &lt;- MCMCglmm(Pupated ~1, random = ~FSfamily, family = ""categorical"", 
+ data = PlodiaRB, prior = prior1, verbose = FALSE)
&gt; summary(m1)


 G-structure:  ~FSfamily

         post.mean l-95% CI u-95% CI eff.samp
FSfamily    0.8529   0.2951    1.455      160

 R-structure:  ~units

      post.mean l-95% CI u-95% CI eff.samp
units         1        1        1        0

 Location effects: Pupated ~ 1 

            post.mean l-95% CI u-95% CI eff.samp  pMCMC    
(Intercept)   -1.1630  -1.4558  -0.8119    463.1 &lt;0.001 ***
---

&gt; prior2 = list(R = list(V = 1, nu = 0), G = list(G1 = list(V = 1, nu = 0.002)))
&gt; m2 &lt;- MCMCglmm(Pupated ~1, random = ~FSfamily, family = ""categorical"", 
+ data = PlodiaRB, prior = prior2, verbose = FALSE)
&gt; summary(m2)


 G-structure:  ~FSfamily

         post.mean l-95% CI u-95% CI eff.samp
FSfamily    0.8325   0.3101    1.438    79.25

 R-structure:  ~units

      post.mean l-95% CI u-95% CI eff.samp
units    0.7212  0.04808    2.427    3.125

 Location effects: Pupated ~ 1 

            post.mean l-95% CI u-95% CI eff.samp  pMCMC    
(Intercept)   -1.1042  -1.5191  -0.7078    20.99 &lt;0.001 ***
---

&gt; m2 &lt;- glmer(Pupated ~ 1+ (1|FSfamily), family=""binomial"",data=PlodiaRB)
&gt; summary(m2)
Generalized linear mixed model fit by the Laplace approximation 
Formula: Pupated ~ 1 + (1 | FSfamily) 
   Data: PlodiaRB 
  AIC  BIC logLik deviance
 1020 1029   -508     1016
Random effects:
 Groups   Name        Variance Std.Dev.
 FSfamily (Intercept) 0.56023  0.74849 
Number of obs: 874, groups: FSfamily, 49

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -0.9861     0.1344  -7.336  2.2e-13 ***
</code></pre>

<p>So based on the comments from Stephane I think the G structure is for $\sigma_{0u}^2$. But the comments also say that the R structure is for $\sigma_{0e}^2$ yet this does not seem to appear in the <code>lme4</code> output.</p>

<p>Note that the results from <code>lme4/glmer()</code> are consistent with both examples from MCMC <code>MCMCglmm</code>.</p>

<p>So, is the R structure for $\sigma_{0e}^2$ and why doesn't this appear in the output for <code>lme4/glmer()</code> ?</p>
"
"0.0776150525706333","0.0793051585718144"," 33151","<p>I have a dataset (<code>data.mrsa</code>) about the MRSA prevalence of elderly in long term care facilities (LTCF) with the following information:</p>

<ul>
<li>mrsa_result: MRSA result of recruited elderly (positive VS negative)</li>
<li>age: residents' age</li>
<li>ltcf: UID for each LTCF (we sampled 30 out of 1000 LTCFs)</li>
<li>ltcf_type: type of LTCF (private VS non-private)</li>
</ul>

<p>I have a multi-level logistic regression model like the one below:</p>

<pre><code>fit2 &lt;- glmer(mrsa_result ~ age + (1|ltcf), family=binomial(""logit""),data=data.mrsa)
</code></pre>

<p>I know I am trying to find out the effect of <code>age</code> on the log-odds of <code>mrsa_result</code>, having the <code>ltcf</code> on the 2nd level gives me a wider CI on the lod-odds.</p>

<p>Now I want to add the <code>ltcf_type</code> into the model, I think this should be a fixed effect, as there can only be private and non-private, but <code>itcf_type</code> should be considered as 2nd level data, right? As this describe the type of LTCF, not the type of elderly.</p>

<p>I am puzzled on where should I put the term <code>ltcf_type</code> into my model, I wonder which of the following lines is correct:</p>

<pre><code>fit2a &lt;- glmer(mrsa_result ~ age + ltcf_type + (1|ltcf), family=binomial(""logit""),data=data.mrsa)
fit2b &lt;- glmer(mrsa_result ~ age + (1|ltcf + ltcf_type), family=binomial(""logit""),data=data.mrsa)
</code></pre>

<p>Thanks.</p>
"
"0.118558951157635","0.121140630718605"," 34969","<p>Sorry if I'm missing something very obvious here but I am new to mixed effect modelling. </p>

<p>I am trying to model a binomial presence/absence response as a function of percentages of habitat within the surrounding area. My fixed effect is the percentage of the habitat and my random effect is the site (I mapped 3 different farm sites). </p>

<pre><code>glmmsetaside &lt;- glmer(treat~setas+(1|farm),
       family=binomial,data=territory)
</code></pre>

<p>When <code>verbose=TRUE</code>:</p>

<pre><code>0:     101.32427: 0.333333 -0.0485387 0.138083 
1:     99.797113: 0.000000 -0.0531503 0.148455  
2:     99.797093: 0.000000 -0.0520462 0.148285  
3:     99.797079: 0.000000 -0.0522062 0.147179  
4:     99.797051: 7.27111e-007 -0.0508770 0.145384  
5:     99.797012: 1.45988e-006 -0.0495767 0.141109  
6:     99.797006: 0.000000 -0.0481233 0.136883  
7:     99.797005: 0.000000 -0.0485380 0.138081  
8:     99.797005: 0.000000 -0.0485387 0.138083  
</code></pre>

<p>My output is this:</p>

<pre><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: treat ~ setasidetrans + (1 | farm) 

AIC   BIC logLik deviance
105.8 112.6  -49.9     99.8
Random effects:
 Groups Name        Variance Std.Dev.
farm   (Intercept)  0        0  
Number of obs: 72, groups: farm, 3

Fixed effects:
Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -0.04854    0.44848  -0.108    0.914
setasidetrans  0.13800    1.08539   0.127    0.899

Correlation of Fixed Effects:
            (Intr)
setasidtrns -0.851
</code></pre>

<p>I basically do not understand why my random effect is 0? Is it because the random effect only has 3 levels? I don't see why this would be the case. I have tried it with lots of different models and it always comes out as 0.</p>

<p>It cant be because the random effect doesn't explain any of the variation because I know the habitats are different in the different farms.</p>

<p>Here is an example set of data using <code>dput</code>:</p>

<pre><code>list(territory = c(1, 7, 8, 9, 10, 11, 12, 13, 14, 2, 3, 4, 5, 
6, 15, 21, 22, 23, 24, 25, 26, 27, 28, 16, 17, 18, 19, 20, 29, 
33, 34, 35, 36, 37, 38, 39, 40, 30, 31, 32, 41, 45, 46, 47, 48, 
49, 50, 51, 52, 42, 43, 44, 53, 55, 56, 57, 58, 59, 60, 61, 62, 
54, 63, 65, 66, 67, 68, 69, 70, 71, 72, 64), treat = c(1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0), farm = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3), 
built = c(5.202332763, 1.445026852, 2.613422283, 2.261705833, 
2.168842186, 1.267473928, 0, 0, 0, 9.362387965, 17.55433115, 
4.58020626, 4.739300829, 8.638442377, 0, 1.220760647, 7.979990338, 
13.30789514, 0, 8.685544976, 3.71617163, 0, 0, 6.802926951, 
8.925512803, 8.834006678, 4.687723044, 9.878232478, 8.097800267, 
0, 0, 0, 0, 5.639651095, 9.381654651, 8.801754791, 5.692392532, 
3.865304919, 4.493438554, 4.826277798, 3.650995554, 8.20818417, 
0, 8.169597157, 8.62030666, 8.159474015, 8.608979238, 0, 
8.588288678, 7.185700856, 0, 0, 3.089524893, 3.840381223, 
31.98103158, 5.735501995, 5.297691011, 5.17141191, 6.007539933, 
2.703345394, 4.298077606, 1.469986793, 0, 4.258511595, 0, 
21.07029581, 6.737664009, 14.36176373, 3.056631919, 0, 32.49289428, 
0)
</code></pre>

<p>It goes on with around 10 more columns for different types of habitat (like <code>built</code>, <code>setaside</code> is one of them) with percentages in it.</p>
"
"0.0776150525706333","0.0793051585718144"," 37714","<p>I would like to make a prediction for a (new) subject to have a certain outcome given the historical data and the model:</p>

<pre><code>glm(outcome ~ age + treatment + history, family=binomial, ...) 
</code></pre>

<p>however in the historical data that will be fitted by the model, I have some sort of repeated measurements on some of the subjects (and I don't know if repeated measures is the appropriate term to be used here, hence using lmer etc is doubtful); example:<br></p>

<pre><code>subject_ID    age    treatment    history    outcome
S_1           33      T_1         H_1        0
S_2           27      T_2         H_2        1
S_2           27      T_3         H_2        1
S_3           56      T_1         H_11       0
etc...
</code></pre>

<p>In this example subject_2 (S_2) has two rows because he had simultaneously two different treatments at the same time. could a logistic regression still be used or should cases like subject_2 be removed from the analysis?</p>
"
"0.10976425998969","0.112154430818409"," 37805","<p>I have a GLMM of the form: </p>

<pre><code>lmer(present? ~ factor1 + factor2 + continuous + factor1*continuous + 
                (1 | factor3), family=binomial)
</code></pre>

<p>When I use <code>drop1(model, test=""Chi"")</code>, I get different results than if I use <code>Anova(model, type=""III"")</code> from the car package or <code>summary(model)</code>. These latter two give the same answers. </p>

<p>Using a bunch of fabricated data, I have found that these two methods normally do not differ. They give the same answer for balanced linear models, unbalanced linear models (where unequal n in different groups), and for balanced generalised linear models, but not for balanced generalised linear mixed models. So it appears that only in cases where random factors are included does this discord manifest.</p>

<ul>
<li>Why is there a discrepancy between these two methods?  </li>
<li>When using GLMM should <code>Anova()</code> or <code>drop1()</code> be used?  </li>
<li>The difference between these two is rather slight, at least for my data. Does it even matter which is used?</li>
</ul>
"
"0.0801604816056202","0.102382519472325"," 38195","<p>Is there such a package that provides for zero-inflated negative binomial mixed-effects model estimation in R?</p>

<p>By that I mean:</p>

<ul>
<li><p>Zero-inflation where you can specify the binomial model for zero inflation, like in function zeroinfl in package pscl: <pre>zeroinfl(y~X|Z, dist = ""negbin"")</pre>
where Z is the formula for the zero inflation model;</p></li>
<li><p>Negative binomial distribution for the count part of the model;</p></li>
<li><p>Random effects specified similar to function lmer of package lme4.</p></li>
</ul>

<p>I understand glmmADMB can do all that, except the formula for zero inflation cannot be specified (it is just an intercept, i.e. Z is just 1). But are there any other packages that can do it all?</p>

<p>I will be very thankful for your help!</p>
"
"0.110901743841784","0.129504781647635"," 38463","<p>I've run a MLM using lmer in R on some data that I have. As I'm analysing binary data, I've used a binomial MLM as well. I basically have a simple question about how to report the coefficients from the model.</p>

<p>I've read that the best thing to do with the binomial coefficients is to back-transform them using the invlogit function in order to make the coefficients more human-readable. The issue with this, and this probably seems like something stupid but I want make sure, is how to include the sign of the coefficient when using the invlogit function.</p>

<p>So for example, if I have a coefficient of -2, then I'm not sure whether to transform using invlogit while including the sign or not. It means that:</p>

<pre><code>invlogit(2)
</code></pre>

<p>Gives an answer of:</p>

<pre><code>0.8807971
</code></pre>

<p>However, if we include the sign:</p>

<pre><code>invlogit(-2)
</code></pre>

<p>Gives an answer of:</p>

<pre><code>0.1192029
</code></pre>

<p>So, with that in mind, should I ignore the sign of the coefficient when using invlogit and then re-apply the sign afterwards, or should I keep it in when doing invlogit? I assume the answer is the former option, given that the pattern of my data makes more sense that way, but since this is the first time I've done MLMs of this type with binomial data, I'd like to make 100% certain before submitting a paper that's based on this output!</p>

<p>Thanks</p>
"
"0.126744850104896","0.129504781647635"," 38493","<p><strong>Update</strong>: Since I now know that my problem is called <strong>quasi-complete separation</strong> I updated the question to reflect this (thanks to Aaron).</p>

<hr>

<p>I have a dataset from an experiment in which 29 human participants (factor <code>code</code>) worked on a set of trials and the <code>response</code> was either 1 or 0. In addition, we manipulated the materials so that we had three crossed factors, <code>p.validity</code> (valid versus invalid), <code>type</code> (affirmation versus denial), and <code>counterexamples</code> (few versus many):</p>

<pre><code>d.binom &lt;- read.table(""http://pastebin.com/raw.php?i=0yDpEri8"")
str(d.binom)
## 'data.frame':   464 obs. of  5 variables:
##      $ code           : Factor w/ 29 levels ""A04C"",""A14G"",..: 1 1 1 1 1 1 1 1 1 1 ...
##      $ response       : int  1 1 1 1 0 1 1 1 1 1 ...
##      $ counterexamples: Factor w/ 2 levels ""few"",""many"": 2 2 1 1 2 2 2 2 1 1 ...
##      $ type           : Factor w/ 2 levels ""affirmation"",..: 1 2 1 2 1 2 1 2 1 2 ...
##      $ p.validity     : Factor w/ 2 levels ""invalid"",""valid"": 1 1 2 2 1 1 2 2 1 1 ...
</code></pre>

<p>Overall there is only a small number of 0s: </p>

<pre><code>mean(d.binom$response)
## [1] 0.9504
</code></pre>

<p>One hypothesis is that there is an effect of <code>validity</code>, however, preliminary analysis suggests there might be an effect of <code>counterexamples</code>. As I have dependent data (each participant worked on all trials) I would like to use a GLMM on the data. Unfortunately, <code>counterexamples</code>quasi-completely separate the data (at least for one level):</p>

<pre><code>with(d.binom, table(response, counterexamples))
##         counterexamples
## response few many
##        0   1   22
##        1 231  210
</code></pre>

<p>This is also reflected in the model: </p>

<pre><code>require(lme4)
options(contrasts=c('contr.sum', 'contr.poly'))


m2 &lt;- glmer(response ~ type * p.validity * counterexamples + (1|code), 
            data = d.binom, family = binomial)
summary(m2)
## [output truncated]
## Fixed effects:
##                                      Estimate Std. Error z value Pr(&gt;|z|)
##   (Intercept)                            9.42     831.02    0.01     0.99
##   type1                                 -1.97     831.02    0.00     1.00
##   p.validity1                            1.78     831.02    0.00     1.00
##   counterexamples1                       7.02     831.02    0.01     0.99
##   type1:p.validity1                      1.97     831.02    0.00     1.00
##   type1:counterexamples1                -2.16     831.02    0.00     1.00
##   p.validity1:counterexamples1           2.35     831.02    0.00     1.00
##   type1:p.validity1:counterexamples1     2.16     831.02    0.00     1.00
</code></pre>

<p>The standard errors for the parameters are simply insane. As my final goal is to assess whether or not certain effects are significant, standard errors are not totally unimportant.</p>

<ul>
<li><strong>How can I deal with the quasi complete separation?</strong> What I want is to obtain estimates from which I can judge whether or not a certain effect is significant or not (e.g., using <code>PRmodcomp</code> from package <code>pkrtest</code>, but this is another step not described here).</li>
</ul>

<p>Approaches using other packages are fine as well.</p>
"
"0.0448110714948221","0.045786854649563"," 45655","<p>I am modelling data with a generalized mixed model with binomial error distribution and I am concerned about overdispersion. I know that dispersion parameter can be measured as deviance/df, but for mixed models (= with random effect), the number of degrees of freedom cannot be extracted (I am using the function lmer from R). Is there any way to find out if my data are overdispersed?</p>
"
"0.142294263046161","0.145392790714993"," 46789","<p>I collected data to find whether the presence or absence of vision, sound, and touch during a task affected the successful completion of that task. However, there were no samples collected where all three senses were absent. So the dependent variable is boolean success but I have a question about how to model the independent variables in a logistic regression.</p>

<p>My initial analysis used a single categorical variable with seven levels representing each combination of senses (seven because there were no cases where all three senses were absent).</p>

<pre><code>summary( glmer( Success ~ Condition + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>When I tried to build a model with the Vision, Sound, and Touch as separate variables, the analysis fails. <a href=""http://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q4/004552.html"" rel=""nofollow"" title=""[R-sig-ME] Structural zeros in lme4"">I believe this is because I have empty cells when including the vision*sound*touch interaction</a> because we did not collect results where all senses were absent.</p>

<pre><code>summary( glmer( Success ~ Vision + Sound + Touch + Vision*Sound + Vision*Touch + 
                Sound*Touch + Vision*Sound*Touch + ( 1 | Participant ), 
                family=binomial, data=trials))
</code></pre>

<p>I followed the suggestion linked above to use the <code>interaction</code> function to drop the unused factor (all three senses absent). However, this seems to create a variable that looks like my original single categorical variable.</p>

<pre><code>senses &lt;- interaction( trials$Vision, trials$Sound, trials$Touch, drop=TRUE )
summary( glmer( Success ~ senses + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>As I try to refine this analysis, is there a way to model the senses as separate variables to make the interaction between these variables clearer? That is, to appropriately model the contribution of vision in the <code>vision</code>, <code>vision*sound</code>, <code>vision*touch</code> and <code>vision*sound*touch</code> conditions. From the initial analysis, the <code>vision*sound*touch</code> interaction is the most interesting.</p>
"
"0.0448110714948221","0.045786854649563"," 48485","<p>Suppose I have 10 students, who each attempt to solve 20 math problems.  The problems are scored correct or incorrect (in longdata) and each student's performance can be summarized by an accuracy measure (in subjdata).  Models 1, 2, and 4 below appear to produce different results, but I understand them to be doing the same thing.  Why are they producing different results?  (I included model 3 for reference.)</p>

<pre><code>library(lme4)

set.seed(1)
nsubjs=10
nprobs=20
subjdata = data.frame('subj'=rep(1:nsubjs),'iq'=rep(seq(80,120,10),nsubjs/5))
longdata = subjdata[rep(seq_len(nrow(subjdata)), each=nprobs), ]
longdata$correct = runif(nsubjs*nprobs)&lt;pnorm(longdata$iq/50-1.4)
subjdata$acc = by(longdata$correct,longdata$subj,mean)
model1 = lm(logit(acc)~iq,subjdata)
model2 = glm(acc~iq,subjdata,family=gaussian(link='logit'))
model3 = glm(acc~iq,subjdata,family=binomial(link='logit'))
model4 = lmer(correct~iq+(1|subj),longdata,family=binomial(link='logit'))
</code></pre>
"
"0.176014394654657","0.179847194799054"," 48582","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/48696/generalized-linear-mixed-model-in-r-with-repeated-measures"">Generalized Linear Mixed Model in R with repeated measures</a>  </p>
</blockquote>



<p>I am trying to investigate how four variables (var1=continuous, var2=factor, var3=factor, var4=continuous) influence the number of trials individuals approached (out of total nr of trials --> binomial) across two conditions that differed in food availability (food availability 1 = 42 trials; food availability 8 = 35 trials) (n = 19 individuals). The response variable is binomial as it is the number of trials out of total number of trials. I am using the 'lmer' function of the lme4 package.</p>

<p>I thought the additive model I should run would be with random factor ID:</p>

<pre><code>glmer(cbind(appr_Y,appr_N) ~ Condition+Var1+Var2+Var3+Var4+(1|ID), data=dataset, 
      family=binomial)
</code></pre>

<p>However, the result I get shows that Condition is super significant (p &lt; 2e-16) while the other variables aren't, while exploring the data visually shows no difference in the response variable for Condition and the variables having strong effects.</p>

<p>Below a dummy representing the large data table:</p>

<pre><code>Con ID  Var1  Var2  appr_Y  appr_N  Trial_total
1   1   10      y   14      6       20
1   2   4       y   10      10      20
1   3   5       n   5       15      20
1   4   32      n   18      2       20
1   5   11      y   3       17      20
2   1   10      y   20      5       25
2   2   4       y   10      15      25
2   3   5       n   24      1       25
2   4   32      n   11      14      25  
2   5   11      y   7       18      25
</code></pre>

<p>What am I doing wrong? </p>

<p><strong>update</strong>: I analysed the data with GenStat (which doesn't show AIC values) and the output is totally different. In GenStat it asks for the random factor (here ID) and the denominator (here Trial_total), which is different than putting in Appr_Y, Appr_N. </p>

<p><strong>update2</strong>: The above dataset was just a dummy. I hereby provide the 'summary' of the model and the information about the dataset:</p>

<pre><code>&gt; summary(GLMM1)
Generalized linear mixed model fit by the Laplace approximation 
Formula: cbind(Appr_Y, Appr_N) ~ Condition + Var1 + Var2 + Var3 + Var4 + (1 | ID) 
Data: dataset 
AIC   BIC logLik deviance
102.1 113.5 -44.04    88.08
Random effects:
Groups Name        Variance Std.Dev.
ID     (Intercept) 0.59495  0.77133 
Number of obs: 38, groups: ID, 19

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -2.43536    0.60237  -4.043 5.28e-05 ***
Condition8         1.14942    0.12274   9.365  &lt; 2e-16 ***
Var1               0.04524    0.04002   1.130   0.2583    
Var2Paired        -0.35299    0.47970  -0.736   0.4618    
Var3no             0.55914    0.44095   1.268   0.2048    
Var4               0.11996    0.06282   1.909   0.0562 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) Cndt8- Var1 Var2P Var3no
Cndtn8-strn -0.128                            
Var1        -0.294  0.015                     
Var2unp     -0.474 -0.015 -0.352              
Var3no      -0.178  0.016 -0.310 -0.097       
Var4        -0.664  0.021 -0.078  0.467 -0.134
&gt; str(dataset)
'data.frame':   38 obs. of  9 variables:
$ ID          : Factor w/ 19 levels ""39"",""40"",""41"",..: 1 2 3 4 5 6 7 8 9 10 ...
   $ Appr_Y      : num  3 12 0 7 27 6 12 1 5 17 ...
$ Appr_N      : num  39 30 42 35 15 36 30 41 37 25 ...
    $ Var2        : Factor w/ 2 levels ""paired"",""unpaired"": 2 2 2 2 1 1 2 1 2 1 ...
$ Var1        : num  2 16 19 18 13 11 14 1 8 9 ...
    $ Var3        : Factor w/ 2 levels ""yes"",""no"": 2 2 2 1 2 2 2 1 1 2 ...
$ Var4        : num  2.6 6.87 2.4 1.1 4.32 ...
    $ Condition   : Factor w/ 2 levels ""1"",""8"": 1 1 1 1 1 1 1 1 1 1 ...
$ n           : num  42 42 42 42 42 42 42 42 42 42 ...
</code></pre>

<p>Do I perhaps have to do something with weighing the data as trial nr is not the same across conditions? Or using Appr_Y, total nr of trials instead of Appr_Y, Appr_N ?</p>
"
"0.176014394654657","0.179847194799054"," 48696","<p>I am trying to investigate how four variables (var1=continuous, var2=factor, var3=factor, var4=continuous) influence the number of trials individuals approached (out of total nr of trials --> binomial) across two conditions that differed in food availability (food availability 1 = 42 trials; food availability 8 = 35 trials) (n = 19 individuals). The response variable is binomial as it is the number of trials out of total number of trials. I am using the 'lmer' function of the lme4 package.</p>

<p>I thought the additive model I should run would be with random factor ID:</p>

<pre><code>glmer(cbind(appr_Y,appr_N) ~ Condition+Var1+Var2+Var3+Var4+(1|ID), data=dataset, 
      family=binomial)
</code></pre>

<p>However, the result I get shows that Condition is super significant (p &lt; 2e-16) while the other variables aren't, while exploring the data visually shows no difference in the response variable for Condition and the variables having strong effects.</p>

<p>Below a dummy representing the large data table:</p>

<pre><code>Con ID  Var1  Var2  appr_Y  appr_N  Trial_total
1   1   10      y   14      6       20
1   2   4       y   10      10      20
1   3   5       n   5       15      20
1   4   32      n   18      2       20
1   5   11      y   3       17      20
2   1   10      y   20      5       25
2   2   4       y   10      15      25
2   3   5       n   24      1       25
2   4   32      n   11      14      25  
2   5   11      y   7       18      25
</code></pre>

<p>What am I doing wrong? </p>

<p><strong>update</strong>: I analysed the data with GenStat (which doesn't show AIC values) and the output is totally different. In GenStat it asks for the random factor (here ID) and the denominator (here Trial_total), which is different than putting in Appr_Y, Appr_N. </p>

<p><strong>update2</strong>: The above dataset was just a dummy. I hereby provide the 'summary' of the model and the information about the dataset:</p>

<pre><code>&gt; summary(GLMM1)
Generalized linear mixed model fit by the Laplace approximation 
Formula: cbind(Appr_Y, Appr_N) ~ Condition + Var1 + Var2 + Var3 + Var4 + (1 | ID) 
Data: dataset 
AIC   BIC logLik deviance
102.1 113.5 -44.04    88.08
Random effects:
Groups Name        Variance Std.Dev.
ID     (Intercept) 0.59495  0.77133 
Number of obs: 38, groups: ID, 19

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -2.43536    0.60237  -4.043 5.28e-05 ***
Condition8         1.14942    0.12274   9.365  &lt; 2e-16 ***
Var1               0.04524    0.04002   1.130   0.2583    
Var2Paired        -0.35299    0.47970  -0.736   0.4618    
Var3no             0.55914    0.44095   1.268   0.2048    
Var4               0.11996    0.06282   1.909   0.0562 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) Cndt8- Var1 Var2P Var3no
Cndtn8-strn -0.128                            
Var1        -0.294  0.015                     
Var2unp     -0.474 -0.015 -0.352              
Var3no      -0.178  0.016 -0.310 -0.097       
Var4        -0.664  0.021 -0.078  0.467 -0.134
&gt; str(dataset)
'data.frame':   38 obs. of  9 variables:
$ ID          : Factor w/ 19 levels ""39"",""40"",""41"",..: 1 2 3 4 5 6 7 8 9 10 ...
   $ Appr_Y      : num  3 12 0 7 27 6 12 1 5 17 ...
$ Appr_N      : num  39 30 42 35 15 36 30 41 37 25 ...
    $ Var2        : Factor w/ 2 levels ""paired"",""unpaired"": 2 2 2 2 1 1 2 1 2 1 ...
$ Var1        : num  2 16 19 18 13 11 14 1 8 9 ...
    $ Var3        : Factor w/ 2 levels ""yes"",""no"": 2 2 2 1 2 2 2 1 1 2 ...
$ Var4        : num  2.6 6.87 2.4 1.1 4.32 ...
    $ Condition   : Factor w/ 2 levels ""1"",""8"": 1 1 1 1 1 1 1 1 1 1 ...
$ n           : num  42 42 42 42 42 42 42 42 42 42 ...
</code></pre>

<p>Do I perhaps have to do something with weighing the data as trial nr is not the same across conditions? Or using Appr_Y, total nr of trials instead of Appr_Y, Appr_N ?</p>
"
"0.0801604816056202","0.102382519472325"," 49014","<p>I am trying to predict (binary) memory for pictures based on two continuous fixed effects: memorability and clutter. Using a mixed effects model, I found that both effects predict memory and including both in a model is better than either on their own.</p>

<p>However, memorability and clutter are highly correlated. </p>

<p>1) How do I interpret my result (that including both is better), given that they are correlated? Can they still be independent predictors, yet be correlated?</p>

<p>2) Is this a problem for the model in general? How do I deal with correlated fixed effects?</p>

<hr>

<p>In R:</p>

<pre><code>m_mem &lt;- lmer(memory ~ memorability + (1|subject), data=memDat, family='binomial')
m_clut &lt;- lmer(memory ~ clutter + (1|subject), data=memDat, family='binomial')
m_mem_clut &lt;- lmer(memory ~ memorability+clutter + (1|subject), data=memDat, family='binomial')
</code></pre>

<p><code>anova(m_mem_clut,m_mem)</code> and <code>anova(m_mem_clut,m_clut)</code> are significant.</p>

<p>[Edit: R <code>dput</code> can be found <a href=""http://bit.ly/14ATKAW"" rel=""nofollow"">here</a> ]</p>
"
"0.10976425998969","0.112154430818409"," 50094","<p>In my experiment I assigned subjects to one of the 3 treatments A, B or C. Within each treatment, a single subject was tested with two models sequentially. The test sequence was randomized. The subjects' responses were categorized as reaction (1) or no reaction (0). In sum, my data setup was:</p>

<ul>
<li><strong>Subject</strong>: id of the subjects;</li>
<li><strong>Treatment</strong>: A, B, or C -- between subject variable;</li>
<li><strong>Model</strong>: M or F -- within subject variable;</li>
<li><strong>Sequence</strong>: test sequence, which model was tested first -- between subject variable;</li>
<li><strong>Response</strong>: 0 or 1 -- binary dependent variable;</li>
</ul>

<p>I am interested to see if subjects differed in their response between model types, among the treatments, and if there was any interaction between treatment and model. Here is a GLMM model I tentatively constructed:  </p>

<pre><code>full.glmm &lt;- lmer(Response~Treatment * Model + Sequence + (1|Treatment/Subject), 
                  family=binomial, data=rsp10)
</code></pre>

<p>I am unsure about the random effect statement, i.e., the <code>(1|Treatment/Subject)</code> part. Should any variable go before the ""|"", and is my nesting structure defined correctly (Subject nested within Treatment)? If not, what's the correct structure?</p>
"
"0.0672166072422331","0.091573709299126"," 50726","<p>I'm using the <code>lme4</code> package in R to do some logistic mixed-effects modeling.<br>
My understanding was that sum of each random effects should be zero.</p>

<p>When I make toy linear mixed-models using <code>lmer</code>, the random effects are usually &lt; $10^{-10}$ confirming my belief that the <code>colSums(ranef(model)$groups) ~ 0</code>
But in toy binomial models (and in models of my real binomial data) some of the random effect sum to ~0.9. </p>

<p>Should I be concerned?  How do I interpret this?  </p>

<p>Here is a linear toy example
<code><pre>
toylin&lt;-function(n=30,gn=10,doplot=FALSE){
 require(lme4)
 x=runif(n,0,1000)
 y1=matrix(0,gn,n)
 y2=y1
 for (gx in 1:gn)
 {
   y1[gx,]=2*x*(1+(gx-5.5)/10) + gx-5.5  + rnorm(n,sd=10)
   y2[gx,]=3*x*(1+(gx-5.5)/10) * runif(1,1,10)  + rnorm(n,sd=20)
 }
 c1=y1*0;
 c2=y2*0+1;
 y=c(t(y1[c(1:gn),]),t(y2[c(1:gn),]))
 g=rep(1:gn,each=n,times=2)
 x=rep(x,times=gn*2)
 c=c(c1,c2)
 df=data.frame(list(x=x,y=y,c=factor(c),g=factor(g)))
 (m=lmer(y~x*c + (x*c|g),data=df))
 if (doplot==TRUE)
  {require(lattice)
   df$fit=fitted(m)
   plot1=xyplot(fit ~ x|g,data=df,group=c,pch=19,cex=.1)
   plot2=xyplot(y ~ x|g,data=df,group=c)
   print(plot1+plot2)
  }
 print(colMeans(ranef(m)$g))
 m
}
</code></pre></p>

<p>In this case the colMeans always come out $&lt;10^{-6}$ </p>

<p>Here is a binomial toy example (I would share my actual data, but it is being submitted for publication and I am not sure what the journal policy is on posting beforehand):</p>

<p><pre><code>
toybin&lt;-function(n=100,gn=4,doplot=FALSE){
  require(lme4)<br>
  x=runif(n,-16,16)
  y1=matrix(0,gn,n)
  y2=y1
  for (gx in 1:gn)
  { com=runif(1,1,5)
    ucom=runif(1,1,5)
    y1[gx,]=tanh(x/(com+ucom) + rnorm(1)) > runif(x,-1,1)
    y2[gx,]=tanh(2*(x+2)/com + rnorm(1)) > runif(x,-1,1)
  }
  c1=y1*0;
  c2=y2*0+1;
  y=c(t(y1[c(1:gn),]),t(y2[c(1:gn),]))
  g=rep(1:gn,each=n,times=2)
  x=rep(x,times=gn*2)
  c=c(c1,c2)
  df=data.frame(list(x=x,y=y,c=factor(c),g=factor(g)))
  (m=lmer(y~x*c + (x*c|g),data=df,family=binomial))
  if (doplot==TRUE)
   {require(lattice)
    df$fit=fitted(m)
    print(xyplot(fit ~ x|g,data=df,group=c,pch=19,cex=.1))
   }
  print(colMeans(ranef(m)$g))
  m
}
</pre></code></p>

<p>Now the colMeans sometimes come out above 0.3, and definitely higher, on average than the linear example.</p>
"
"0.149140260907443","0.165086852181216"," 58020","<p>I'm struggling with modeling some experimental data using the <code>lme4</code> package in R, and would appreciate input.</p>

<p>My experimental design is as follows:  subjects entering the experiment answer a screener question, which is used to randomly assign them to a between-subjects condition (variable=<code>rank</code>, which has 2-levels (0/1)). They then complete a distractor task, and make two choices (<code>choice</code> being the within-subjects dependent variable).  At each choice, the stimuli are RANDOMLY ASSIGNED and crossed by two factors (<code>msg</code> has 3-levels (""norm"", ""no norm"" and ""provincial""), and <code>cost</code> has 2-levels (0/1)).   <strong>Because participants were randomly assigned to both <code>cost</code> and <code>msg</code> at two points in time, theoretically they could have the same combination of <code>cost</code> and <code>msg</code> at both points in time.</strong></p>

<p>My life would be easiest if I could use a wonderful package such as <code>ezANOVA</code>, but my data won't allow me to do this because every individual doesn't have EVERY combination of the two within-subjects variables <a href=""http://stats.stackexchange.com/questions/57709/error-in-ezanova-with-balanced-dataset-with-no-missing-data"">see here</a>.  So, I'm in the less-familiar territory of mixed models.</p>

<p>My hypothesis argues that there should be a three-way interaction between <code>rank</code>, <code>msg</code>, and <code>cost</code>.  Thus, a simple version of my model might be:</p>

<pre><code>m1 &lt;- glmer(choice ~ msg*cost*rank + (1|id), data=df, family=""binomial"")
</code></pre>

<p>But, I've also seen it suggested <a href=""http://stats.stackexchange.com/questions/46321/how-to-deal-with-repeated-measurements-in-the-same-condition-of-a-factorial-expe"">on this site</a> that my random effects should be modeled as follows in order to evaluate the interaction between the factor and subjects:</p>

<pre><code>m1 = lmer(choice ~ msg*cost*rank + (rank|id) + (cost|id), data=df)
</code></pre>

<p>The latter model has me straying into unfamilar territory, so I'd appreciate any advice about how to model this data in a way that is (1) simple, but (2) appropriate.</p>

<p>Sample data below:</p>

<pre><code>&gt; dput(df[1:700,2:6])
structure(list(time = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), choice = c(1, 1, 
1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 
1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 
1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 
0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 
0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 
1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 
1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 
1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 
1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 
1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 
1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 
1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 
1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 
1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 
0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 
1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 
0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 
1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 
1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 
0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 
1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 
1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 
1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 
0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 
1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 
1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 
1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 
1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 
1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 
1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 
1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 
0, 1, 0, 1, 0), msg = structure(c(3L, 1L, 1L, 2L, 3L, 1L, 3L, 
3L, 3L, 2L, 2L, 3L, 2L, 3L, 2L, 3L, 1L, 3L, 2L, 3L, 1L, 1L, 3L, 
1L, 2L, 3L, 3L, 2L, 2L, 1L, 1L, 1L, 1L, 3L, 2L, 3L, 3L, 2L, 3L, 
3L, 1L, 3L, 1L, 2L, 2L, 3L, 2L, 3L, 3L, 2L, 3L, 3L, 1L, 2L, 3L, 
3L, 1L, 2L, 3L, 2L, 3L, 3L, 1L, 1L, 3L, 1L, 3L, 2L, 1L, 3L, 2L, 
3L, 3L, 2L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 2L, 2L, 2L, 2L, 2L, 1L, 
3L, 2L, 3L, 2L, 3L, 3L, 3L, 1L, 1L, 3L, 1L, 2L, 2L, 3L, 2L, 3L, 
3L, 2L, 2L, 3L, 2L, 1L, 2L, 1L, 3L, 2L, 2L, 1L, 3L, 3L, 2L, 3L, 
3L, 3L, 2L, 2L, 1L, 2L, 3L, 2L, 2L, 2L, 3L, 3L, 3L, 1L, 2L, 2L, 
1L, 1L, 3L, 1L, 3L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 
2L, 1L, 2L, 1L, 1L, 3L, 3L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 3L, 
3L, 1L, 2L, 3L, 1L, 1L, 3L, 2L, 2L, 3L, 3L, 1L, 1L, 1L, 2L, 1L, 
2L, 3L, 3L, 2L, 1L, 2L, 3L, 1L, 2L, 2L, 1L, 3L, 3L, 1L, 1L, 1L, 
3L, 2L, 3L, 1L, 2L, 2L, 3L, 2L, 1L, 3L, 1L, 2L, 2L, 3L, 3L, 2L, 
1L, 3L, 3L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 3L, 2L, 2L, 1L, 1L, 1L, 
1L, 1L, 3L, 2L, 2L, 1L, 2L, 2L, 3L, 1L, 1L, 2L, 3L, 2L, 3L, 3L, 
3L, 3L, 1L, 3L, 2L, 1L, 2L, 3L, 1L, 1L, 2L, 3L, 3L, 2L, 1L, 1L, 
2L, 1L, 2L, 3L, 3L, 3L, 1L, 2L, 2L, 3L, 1L, 3L, 1L, 3L, 3L, 1L, 
1L, 3L, 1L, 3L, 1L, 1L, 3L, 1L, 1L, 2L, 2L, 3L, 2L, 3L, 2L, 3L, 
1L, 2L, 2L, 1L, 2L, 3L, 3L, 3L, 2L, 3L, 1L, 3L, 1L, 2L, 3L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 3L, 2L, 3L, 2L, 1L, 3L, 3L, 
3L, 1L, 3L, 3L, 1L, 3L, 1L, 3L, 2L, 1L, 3L, 1L, 1L, 2L, 2L, 3L, 
1L, 3L, 3L, 3L, 1L, 3L, 1L, 3L, 2L, 1L, 2L, 1L, 1L, 2L, 3L, 3L, 
3L, 3L, 3L, 2L, 2L, 3L, 2L, 1L, 1L, 2L, 1L, 2L, 3L, 3L, 1L, 1L, 
1L, 3L, 3L, 1L, 1L, 3L, 1L, 2L, 3L, 1L, 1L, 3L, 2L, 1L, 3L, 3L, 
3L, 1L, 3L, 1L, 3L, 1L, 3L, 2L, 3L, 2L, 2L, 1L, 2L, 1L, 3L, 2L, 
2L, 3L, 1L, 2L, 1L, 3L, 3L, 3L, 3L, 2L, 2L, 3L, 3L, 3L, 1L, 1L, 
1L, 1L, 3L, 1L, 3L, 1L, 1L, 1L, 3L, 3L, 2L, 1L, 2L, 3L, 2L, 3L, 
2L, 3L, 1L, 3L, 1L, 3L, 1L, 1L, 3L, 1L, 3L, 2L, 3L, 2L, 1L, 1L, 
1L, 2L, 2L, 1L, 2L, 1L, 3L, 1L, 2L, 2L, 2L, 1L, 1L, 3L, 3L, 1L, 
1L, 3L, 1L, 3L, 3L, 3L, 3L, 2L, 1L, 1L, 3L, 2L, 1L, 3L, 2L, 2L, 
1L, 1L, 3L, 3L, 3L, 1L, 1L, 2L, 1L, 1L, 3L, 1L, 2L, 2L, 3L, 1L, 
2L, 2L, 3L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 3L, 1L, 3L, 
1L, 1L, 3L, 3L, 3L, 2L, 3L, 2L, 2L, 3L, 1L, 2L, 3L, 3L, 2L, 2L, 
1L, 2L, 3L, 2L, 1L, 2L, 3L, 3L, 2L, 2L, 2L, 3L, 1L, 2L, 3L, 2L, 
3L, 3L, 3L, 3L, 1L, 2L, 3L, 2L, 3L, 3L, 3L, 2L, 3L, 1L, 1L, 2L, 
2L, 3L, 3L, 1L, 1L, 3L, 1L, 1L, 1L, 2L, 3L, 2L, 3L, 1L, 2L, 3L, 
2L, 2L, 3L, 3L, 3L, 1L, 3L, 2L, 2L, 1L, 2L, 3L, 1L, 3L, 2L, 3L, 
1L, 2L, 3L, 2L, 1L, 2L, 3L, 3L, 3L, 3L, 2L, 3L, 2L, 3L, 3L, 2L, 
1L, 2L, 3L, 2L, 1L, 2L, 1L, 3L, 1L, 3L, 1L, 1L, 2L, 1L, 3L, 1L, 
2L, 2L, 3L, 2L, 2L, 3L, 3L, 1L, 3L, 1L, 3L, 2L, 1L, 3L, 1L, 2L, 
1L, 3L, 1L, 3L, 1L, 1L, 2L, 1L, 3L, 2L, 3L, 3L, 3L, 1L, 3L, 3L, 
2L, 2L, 1L, 2L, 2L, 2L, 2L, 3L, 1L, 1L, 1L, 2L, 3L, 2L, 3L, 1L, 
3L, 3L, 3L, 1L, 3L, 3L, 2L, 2L, 3L, 2L, 2L, 3L, 2L, 1L, 2L, 3L, 
2L, 2L, 1L, 3L, 2L), .Label = c(""No norm"", ""Norm"", ""Provincial""
), class = ""factor""), cost = structure(c(1L, 2L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 
2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 
2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 
1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 
2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 
1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 
2L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 
1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 
2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 
1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 
2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 
1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 
1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 
1L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 
2L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 
1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 
1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 
2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 
1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 
1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 
2L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 
1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 
1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 
2L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 
1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 
2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 
1L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 
2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 
2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 
1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 
1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 
2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 
1L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 
2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 
1L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 
1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 
2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 
1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L), .Label = c(""0"", ""1""), class = ""factor""), 
    rank = structure(c(1L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 
    2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 
    2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 
    1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
    1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 
    1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
    1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 
    2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 
    2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 
    2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 
    2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 
    2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
    2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 
    2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 
    2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 
    1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 
    1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 
    2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 
    2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 
    1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 
    2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 
    1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 
    1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 
    2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 
    1L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 
    2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 
    2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 
    2L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 
    2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 
    2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 
    2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 
    1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 
    1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 
    1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 
    2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 
    1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 
    1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 
    2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 
    2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 
    1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 
    2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 
    1L), .Label = c(""0"", ""1""), class = ""factor"")), .Names = c(""time"", 
""choice"", ""msg"", ""cost"", ""rank""), row.names = c(""1.1"", 
""2.1"", ""3.1"", ""4.1"", ""5.1"", ""6.1"", ""7.1"", ""8.1"", ""9.1"", ""10.1"", 
""11.1"", ""12.1"", ""13.1"", ""14.1"", ""15.1"", ""16.1"", ""17.1"", ""18.1"", 
""19.1"", ""20.1"", ""21.1"", ""22.1"", ""23.1"", ""24.1"", ""25.1"", ""26.1"", 
""27.1"", ""28.1"", ""29.1"", ""30.1"", ""31.1"", ""32.1"", ""33.1"", ""34.1"", 
""35.1"", ""36.1"", ""37.1"", ""38.1"", ""39.1"", ""40.1"", ""41.1"", ""42.1"", 
""43.1"", ""44.1"", ""45.1"", ""46.1"", ""47.1"", ""48.1"", ""49.1"", ""50.1"", 
""51.1"", ""52.1"", ""53.1"", ""54.1"", ""55.1"", ""56.1"", ""57.1"", ""58.1"", 
""59.1"", ""60.1"", ""61.1"", ""62.1"", ""63.1"", ""64.1"", ""65.1"", ""66.1"", 
""67.1"", ""68.1"", ""69.1"", ""70.1"", ""71.1"", ""72.1"", ""73.1"", ""74.1"", 
""75.1"", ""76.1"", ""77.1"", ""78.1"", ""79.1"", ""80.1"", ""81.1"", ""82.1"", 
""83.1"", ""84.1"", ""85.1"", ""86.1"", ""87.1"", ""88.1"", ""89.1"", ""90.1"", 
""91.1"", ""92.1"", ""93.1"", ""94.1"", ""95.1"", ""96.1"", ""97.1"", ""98.1"", 
""99.1"", ""100.1"", ""101.1"", ""102.1"", ""103.1"", ""104.1"", ""105.1"", 
""106.1"", ""107.1"", ""108.1"", ""109.1"", ""110.1"", ""111.1"", ""112.1"", 
""113.1"", ""114.1"", ""115.1"", ""116.1"", ""117.1"", ""118.1"", ""119.1"", 
""120.1"", ""121.1"", ""122.1"", ""123.1"", ""124.1"", ""125.1"", ""126.1"", 
""127.1"", ""128.1"", ""129.1"", ""130.1"", ""131.1"", ""132.1"", ""133.1"", 
""134.1"", ""135.1"", ""136.1"", ""137.1"", ""138.1"", ""139.1"", ""140.1"", 
""141.1"", ""142.1"", ""143.1"", ""144.1"", ""145.1"", ""146.1"", ""147.1"", 
""148.1"", ""149.1"", ""150.1"", ""151.1"", ""152.1"", ""153.1"", ""154.1"", 
""155.1"", ""156.1"", ""157.1"", ""158.1"", ""159.1"", ""160.1"", ""161.1"", 
""162.1"", ""163.1"", ""164.1"", ""165.1"", ""166.1"", ""167.1"", ""168.1"", 
""169.1"", ""170.1"", ""171.1"", ""172.1"", ""173.1"", ""174.1"", ""175.1"", 
""176.1"", ""177.1"", ""178.1"", ""179.1"", ""180.1"", ""181.1"", ""182.1"", 
""183.1"", ""184.1"", ""185.1"", ""186.1"", ""187.1"", ""188.1"", ""189.1"", 
""190.1"", ""191.1"", ""192.1"", ""193.1"", ""194.1"", ""195.1"", ""196.1"", 
""197.1"", ""198.1"", ""199.1"", ""200.1"", ""201.1"", ""202.1"", ""203.1"", 
""204.1"", ""205.1"", ""206.1"", ""207.1"", ""208.1"", ""209.1"", ""210.1"", 
""211.1"", ""212.1"", ""213.1"", ""214.1"", ""215.1"", ""216.1"", ""217.1"", 
""218.1"", ""219.1"", ""220.1"", ""221.1"", ""222.1"", ""223.1"", ""224.1"", 
""225.1"", ""226.1"", ""227.1"", ""228.1"", ""229.1"", ""230.1"", ""231.1"", 
""232.1"", ""233.1"", ""234.1"", ""235.1"", ""236.1"", ""237.1"", ""238.1"", 
""239.1"", ""240.1"", ""241.1"", ""242.1"", ""243.1"", ""244.1"", ""245.1"", 
""246.1"", ""247.1"", ""248.1"", ""249.1"", ""250.1"", ""251.1"", ""252.1"", 
""253.1"", ""254.1"", ""255.1"", ""256.1"", ""257.1"", ""258.1"", ""259.1"", 
""260.1"", ""261.1"", ""262.1"", ""263.1"", ""264.1"", ""265.1"", ""266.1"", 
""267.1"", ""268.1"", ""269.1"", ""270.1"", ""271.1"", ""272.1"", ""273.1"", 
""274.1"", ""275.1"", ""276.1"", ""277.1"", ""278.1"", ""279.1"", ""280.1"", 
""281.1"", ""282.1"", ""283.1"", ""284.1"", ""285.1"", ""286.1"", ""287.1"", 
""288.1"", ""289.1"", ""290.1"", ""291.1"", ""292.1"", ""293.1"", ""294.1"", 
""295.1"", ""296.1"", ""297.1"", ""298.1"", ""299.1"", ""300.1"", ""301.1"", 
""302.1"", ""303.1"", ""304.1"", ""305.1"", ""306.1"", ""307.1"", ""308.1"", 
""309.1"", ""310.1"", ""311.1"", ""312.1"", ""313.1"", ""314.1"", ""315.1"", 
""316.1"", ""317.1"", ""318.1"", ""319.1"", ""320.1"", ""321.1"", ""322.1"", 
""323.1"", ""324.1"", ""325.1"", ""326.1"", ""327.1"", ""328.1"", ""329.1"", 
""330.1"", ""331.1"", ""332.1"", ""333.1"", ""334.1"", ""335.1"", ""336.1"", 
""337.1"", ""338.1"", ""339.1"", ""340.1"", ""341.1"", ""342.1"", ""343.1"", 
""344.1"", ""345.1"", ""346.1"", ""347.1"", ""348.1"", ""349.1"", ""350.1"", 
""351.1"", ""352.1"", ""353.1"", ""354.1"", ""355.1"", ""356.1"", ""357.1"", 
""358.1"", ""359.1"", ""360.1"", ""361.1"", ""362.1"", ""363.1"", ""364.1"", 
""365.1"", ""366.1"", ""367.1"", ""368.1"", ""369.1"", ""370.1"", ""371.1"", 
""372.1"", ""373.1"", ""374.1"", ""375.1"", ""376.1"", ""377.1"", ""378.1"", 
""379.1"", ""380.1"", ""381.1"", ""382.1"", ""383.1"", ""384.1"", ""385.1"", 
""386.1"", ""387.1"", ""388.1"", ""389.1"", ""390.1"", ""391.1"", ""392.1"", 
""393.1"", ""394.1"", ""395.1"", ""396.1"", ""397.1"", ""398.1"", ""399.1"", 
""400.1"", ""401.1"", ""402.1"", ""403.1"", ""404.1"", ""405.1"", ""406.1"", 
""407.1"", ""408.1"", ""409.1"", ""410.1"", ""411.1"", ""412.1"", ""413.1"", 
""414.1"", ""415.1"", ""416.1"", ""417.1"", ""418.1"", ""419.1"", ""420.1"", 
""421.1"", ""422.1"", ""423.1"", ""424.1"", ""425.1"", ""426.1"", ""427.1"", 
""428.1"", ""429.1"", ""430.1"", ""431.1"", ""432.1"", ""433.1"", ""434.1"", 
""435.1"", ""436.1"", ""437.1"", ""438.1"", ""439.1"", ""440.1"", ""441.1"", 
""442.1"", ""443.1"", ""444.1"", ""445.1"", ""446.1"", ""447.1"", ""448.1"", 
""449.1"", ""450.1"", ""451.1"", ""452.1"", ""453.1"", ""454.1"", ""455.1"", 
""456.1"", ""457.1"", ""458.1"", ""459.1"", ""460.1"", ""461.1"", ""462.1"", 
""463.1"", ""464.1"", ""465.1"", ""466.1"", ""467.1"", ""468.1"", ""469.1"", 
""470.1"", ""471.1"", ""472.1"", ""473.1"", ""474.1"", ""475.1"", ""476.1"", 
""477.1"", ""478.1"", ""479.1"", ""480.1"", ""481.1"", ""482.1"", ""483.1"", 
""484.1"", ""485.1"", ""486.1"", ""487.1"", ""488.1"", ""489.1"", ""490.1"", 
""491.1"", ""492.1"", ""493.1"", ""494.1"", ""495.1"", ""496.1"", ""497.1"", 
""498.1"", ""499.1"", ""500.1"", ""501.1"", ""502.1"", ""503.1"", ""504.1"", 
""505.1"", ""506.1"", ""507.1"", ""508.1"", ""509.1"", ""510.1"", ""511.1"", 
""512.1"", ""513.1"", ""514.1"", ""515.1"", ""516.1"", ""517.1"", ""518.1"", 
""519.1"", ""520.1"", ""521.1"", ""522.1"", ""523.1"", ""524.1"", ""525.1"", 
""526.1"", ""527.1"", ""528.1"", ""529.1"", ""530.1"", ""531.1"", ""532.1"", 
""533.1"", ""534.1"", ""535.1"", ""536.1"", ""537.1"", ""538.1"", ""539.1"", 
""540.1"", ""541.1"", ""542.1"", ""543.1"", ""544.1"", ""545.1"", ""546.1"", 
""547.1"", ""548.1"", ""549.1"", ""550.1"", ""551.1"", ""552.1"", ""553.1"", 
""554.1"", ""555.1"", ""556.1"", ""557.1"", ""558.1"", ""559.1"", ""560.1"", 
""561.1"", ""562.1"", ""563.1"", ""564.1"", ""565.1"", ""566.1"", ""567.1"", 
""568.1"", ""569.1"", ""570.1"", ""571.1"", ""572.1"", ""573.1"", ""574.1"", 
""575.1"", ""576.1"", ""577.1"", ""578.1"", ""579.1"", ""580.1"", ""581.1"", 
""582.1"", ""583.1"", ""584.1"", ""585.1"", ""586.1"", ""587.1"", ""588.1"", 
""589.1"", ""590.1"", ""591.1"", ""592.1"", ""593.1"", ""594.1"", ""595.1"", 
""596.1"", ""597.1"", ""598.1"", ""599.1"", ""600.1"", ""601.1"", ""602.1"", 
""603.1"", ""604.1"", ""605.1"", ""606.1"", ""607.1"", ""608.1"", ""609.1"", 
""610.1"", ""611.1"", ""612.1"", ""613.1"", ""614.1"", ""615.1"", ""616.1"", 
""617.1"", ""618.1"", ""619.1"", ""620.1"", ""621.1"", ""622.1"", ""623.1"", 
""624.1"", ""625.1"", ""626.1"", ""627.1"", ""628.1"", ""629.1"", ""630.1"", 
""631.1"", ""632.1"", ""633.1"", ""634.1"", ""635.1"", ""636.1"", ""637.1"", 
""638.1"", ""639.1"", ""640.1"", ""641.1"", ""642.1"", ""643.1"", ""644.1"", 
""645.1"", ""646.1"", ""647.1"", ""648.1"", ""649.1"", ""650.1"", ""651.1"", 
""652.1"", ""653.1"", ""654.1"", ""655.1"", ""656.1"", ""657.1"", ""658.1"", 
""659.1"", ""660.1"", ""661.1"", ""662.1"", ""663.1"", ""664.1"", ""665.1"", 
""666.1"", ""667.1"", ""668.1"", ""669.1"", ""670.1"", ""671.1"", ""672.1"", 
""673.1"", ""674.1"", ""675.1"", ""676.1"", ""677.1"", ""678.1"", ""679.1"", 
""680.1"", ""681.1"", ""682.1"", ""683.1"", ""684.1"", ""685.1"", ""686.1"", 
""687.1"", ""688.1"", ""689.1"", ""690.1"", ""691.1"", ""692.1"", ""693.1"", 
""694.1"", ""695.1"", ""696.1"", ""697.1"", ""698.1"", ""699.1"", ""700.1""
), class = ""data.frame"")
</code></pre>
"
"0.155230105141267","0.158610317143629"," 58900","<p>I have very recently started learning about Generalised Linear Mixed Models and was using R to explore what difference it makes to treat group membership as either fixed or random effect. In particular, I am looking at the example dataset discussed here:</p>

<p><a href=""http://www.ats.ucla.edu/stat/mult_pkg/glmm.htm"">http://www.ats.ucla.edu/stat/mult_pkg/glmm.htm</a></p>

<p><a href=""http://www.ats.ucla.edu/stat/r/dae/melogit.htm"">http://www.ats.ucla.edu/stat/r/dae/melogit.htm</a></p>

<p>As outlined in this tutorial, the effect of Doctor ID is appreciable and I was expecting the mixed model with a random intercept to give better results. However, comparing AIC values for the two methods suggest that this model is worse:</p>

<pre><code>&gt; require(lme4) ; hdp = read.csv(""http://www.ats.ucla.edu/stat/data/hdp.csv"")
&gt; hdp$DID = factor(hdp$DID) ; hdp$Married = factor(hdp$Married)
&gt; GLM = glm(remission~Age+Married+IL6+DID,data=hdp,family=binomial);summary(GLM)

Call:
glm(formula = remission ~ Age + Married + IL6 + DID, family = binomial, 
data = hdp)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.5265  -0.6278  -0.2272   0.5492   2.7329  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.560e+01  1.219e+03  -0.013    0.990    
Age         -5.869e-02  5.272e-03 -11.133  &lt; 2e-16 ***
Married1     2.688e-01  6.646e-02   4.044 5.26e-05 ***
IL6         -5.550e-02  1.153e-02  -4.815 1.47e-06 ***
DID2         1.805e+01  1.219e+03   0.015    0.988    
DID3         1.932e+01  1.219e+03   0.016    0.987   

[...]

DID405       1.566e+01  1.219e+03   0.013    0.990    
DID405       1.566e+01  1.219e+03   0.013    0.990    
DID406      -2.885e-01  3.929e+03   0.000    1.000    
DID407       2.012e+01  1.219e+03   0.017    0.987    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 10353  on 8524  degrees of freedom
Residual deviance:  6436  on 8115  degrees of freedom
AIC: 7256

Number of Fisher Scoring iterations: 17


&gt; GLMM = glmer(remission~Age+Married+IL6+(1|DID),data=hdp,family=binomial) ; m

Generalized linear mixed model fit by the Laplace approximation 
Formula: remission ~ Age + Married + IL6 + (1 | DID) 
Data: hdp 
AIC  BIC logLik deviance
7743 7778  -3867     7733
Random effects:
Groups Name        Variance Std.Dev.
DID    (Intercept) 3.8401   1.9596  
Number of obs: 8525, groups: DID, 407

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.461438   0.272709   5.359 8.37e-08 ***
Age         -0.055969   0.005038 -11.109  &lt; 2e-16 ***
Married1     0.260065   0.063736   4.080 4.50e-05 ***
IL6         -0.053288   0.011058  -4.819 1.44e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
         (Intr) Age    Marrd1
Age      -0.898              
Married1  0.070 -0.224       
IL6      -0.162  0.012 -0.033


&gt; extractAIC(GLM) ; extractAIC(GLMM)

[1]  410.000 7255.962
[1]    5.000 7743.188
</code></pre>

<p>Thus, my questions are:</p>

<p>(1) Is it appropriate to compare the AIC values provided by the two functions? If so, why does the fixed effect model do better?</p>

<p>(2) What is the best way to identify if fixed or random effects are more important (ie to quantify that the variability due to the doctor is more important than patient characteristics?</p>
"
"0.229077657047781","0.225706438158984"," 59127","<p>Recently, I have done a fairly complex experiment, and I am having trouble coming up with a model that is suitable for the data. I have spent a few days reading about, e.g., when random effects should be nested or crossed, and which variables should be included in a full model. Yet, the literature that is readable for a non-statistician like myself is usually limited to two nesting levels, whereas I (may) have more. The literature that does seem to apply, is too complex for me to understand. I hope you can help me.</p>

<p>My aim is to specify a full model, and start model simplification from there.</p>

<p>The experiment I have done, was as follows:</p>

<ul>
<li>All participants completed three tasks, in a fixed order (Task1, Task2, Task3).</li>
<li>Task 1 consisted of 30 trials, Tasks 2 and 3 both consisted of 10 trials.</li>
<li>Each task was completed three times by each participant (Round1, Round2, Round3).</li>
<li>The manipulation consisted of one factor with three levels (Condition1, Condition2, and Condition3).</li>
<li>The conditions were tied to the rounds, so each participant completed Task 1 in each condition (e.g., 30 trials in Condition1, followed by 30 trials in Condition2, followed by 30 trials in Condition3), before moving on to Task 2.</li>
<li>I used six different stimuli to manipulate these conditions (two for each condition; Stimulus1.1, Stimulus1.2, Stimulus2.1 .. Stimulus3.2)</li>
<li>The dependent variable is binary</li>
</ul>

<p>To complicate matters even more, we used 6 different orders of presenting the stimuli.</p>

<p>I think it's easiest to demonstrate the data structure using an example:</p>

<pre><code>order &lt;- rbind(
c('Stimulus1.1', 'Stimulus2.1', 'Stimulus3.1', 'Stimulus2.2', 'Stimulus3.2', 'Stimulus1.2', 'Stimulus3.1', 'Stimulus1.2', 'Stimulus2.1'),
c('Stimulus1.1', 'Stimulus3.1', 'Stimulus2.1', 'Stimulus3.2', 'Stimulus2.2', 'Stimulus1.2', 'Stimulus2.1', 'Stimulus1.2', 'Stimulus3.1'),
c('Stimulus2.1', 'Stimulus3.1', 'Stimulus1.1', 'Stimulus3.2', 'Stimulus1.2', 'Stimulus2.2', 'Stimulus1.2', 'Stimulus2.1', 'Stimulus3.1'),
c('Stimulus2.1', 'Stimulus1.1', 'Stimulus3.1', 'Stimulus1.2', 'Stimulus3.2', 'Stimulus2.2', 'Stimulus3.2', 'Stimulus2.1', 'Stimulus1.1'),
c('Stimulus3.1', 'Stimulus2.1', 'Stimulus1.1', 'Stimulus2.2', 'Stimulus1.2', 'Stimulus3.2', 'Stimulus1.1', 'Stimulus3.2', 'Stimulus2.1'),
c('Stimulus3.1', 'Stimulus1.1', 'Stimulus2.1', 'Stimulus1.2', 'Stimulus2.2', 'Stimulus3.2', 'Stimulus2.1', 'Stimulus3.2', 'Stimulus1.1'))

test &lt;- expand.grid(trial=1:30, task=c('Task1', 'Task2', 'Task3'), round=c('Round1', 'Round2', 'Round3'), pp=1:12)
test &lt;- test[! (with(test, trial &gt; 10 &amp; task == 'Task2')),]
test &lt;- test[! (with(test, trial &gt; 10 &amp; task == 'Task3')),]
test$taskround &lt;- factor(paste(test$task, test$round, sep=':'))
test$task &lt;- factor(test$task)
test$round &lt;- factor(test$round)
test$stimulus &lt;- factor(unlist(lapply(1:nrow(test), function(x) {order[1 + (test[x, 'pp'] %% 6), as.numeric(test[x,'taskround'])]})))
test$condition &lt;- factor(paste('Condition', substr(as.character(test$stimulus), 9,9), sep=''))
test$response &lt;- factor(rbinom(nrow(test),1, prob=.95))
test$pp &lt;- factor(test$pp)
</code></pre>

<p>The resulting data structure is:</p>

<pre><code>   trial  task  round pp    taskround    stimulus  condition response
1      1 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
2      2 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
3      3 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
4      4 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
5      5 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
6      6 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
7      7 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
8      8 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
9      9 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
10    10 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
</code></pre>

<p>I'm mainly interested in the main effect of Condition.</p>

<p>I am anticipating that the response tendency may differ between tasks and participants. This would translate into a different intercept for each task and participant, correct?</p>

<p>I am also anticipating that the manipulation may affect some participants more than others; and I suspect that different participants may approach different tasks in a different way (e.g., Task 1 may elicit more successes that Task 2, but participant 1 may be more sensitive to this task aspect than participant 2).</p>

<p>My current model is:</p>

<p><code>glmer(response ~ condition + (1 | pp/task/round) + (0 + task + condition | pp), data=test, family=binomial)</code></p>

<p>However, I am not sure if this model is correct. My questions are: </p>

<ol>
<li><p>I am uncertain about the nesting of the random effects in the first clause. I believe <code>task/round</code> is correct, but I am not sure whether I should think of these as being nested under participants, or crossed with participants as follows:
<code>glmer(response ~ condition + (1 | task/round) + (1 | pp) + (0 + task + condition | pp), data=test, family=binomial)</code>.</p></li>
<li><p>I am not sure at all about the <code>(0 + task + condition | pp)</code> random
effects. I get correlations between the task and condition random
effects, and I am not sure whether I would want/need these.</p></li>
<li><p>I removed the intercept here, because it was already included in the
first random effects clause, but I'm not sure about that either.</p></li>
<li><p>Whether this model takes into account any order effects, or whether
I should explicitly model those?</p></li>
<li><p>Whether I should take into account differences between stimuli
(and/or the possibility that different participants react
differently to different stimuli), and how I should model these
(crossed, or nested)?</p></li>
<li><p>I'm worried about fatigue effect (lineair or quadratic) over the
course of the experiment AND within tasks. I would like to include
these effects into the model as well, but again, I'm not sure how to
do that.</p></li>
</ol>

<p>So, in summary, I have lots of questions about analyzing these data. I think I am mainly interested in recommendations on how to approach this dataset, and suggestions for a suitable full model. Any other tips/suggestions are very welcome as well.</p>
"
"0.149140260907443","0.165086852181216"," 59539","<p>I'm new to linear mixed modeling, and have some theory-driven questions that I'm not sure how to analytically resolve.</p>

<p>I am analyzing experimental data with a within-subjects factor (<code>discount</code>). My theory hypothesizes that the effect of this within-subjects factor is contingent upon a between-subjects characteristic of respondents (<code>iipm</code>). Because my data is in long form, and respondents are making 8 choices over time, I model my data as follows:</p>

<pre><code>library(lme4)
m1 &lt;- glmer(chose ~ iipm*discount + product + (1|id) + (1|time), data=long1, 
        family=""binomial"")
</code></pre>

<p>All that I'm trying to do here is fit a simple model that accounts for the dependence between observations for a single subject (<code>1|id</code>), and the potential effect of making several choices in a row (<code>1|time</code>).</p>

<p>However, my theory further specifies that this relationship <strong>should not be affected</strong> by the inclusion of other demographic variables in the model (let's say <code>ideology</code> and <code>partisanship</code>).  So, based on some reading I've done <a href=""http://stats.stackexchange.com/questions/3757/random-effect-slopes-in-linear-mixed-models"">(as well as previous answers on this site)</a>, I fit the following model:</p>

<pre><code>m2 &lt;- glmer(chose ~ iipm*discount + product + (1 |id) + (1|time) + (1|partisanship) +
       (1|ideology) , data=long1, family=""binomial"")
</code></pre>

<p>Because random slopes goes beyond my expertise, I'm just using random intercepts to see what happens when I account for baseline variation amongst individuals attributable to their partisanship and ideology. However, if I <strong>were</strong> to use random slopes to essentially say that the effects of partisanship and ideology vary on an individual basis, even after accounting for baseline variability, I should specify the following model:</p>

<pre><code>m3 &lt;- glmer(chose ~ iipm*discount + product + (1 + partisanship +ideology |id)  +
  (1|time) , data=long1, family=""binomial"")
</code></pre>

<p>To test the hypothesis that this baseline variability <strong><em>doesn't matter</em></strong>, I then run a likelihood ratio test comparing the two models:</p>

<pre><code>library(lmtest)
lrtest(m2,m1) # p=.349
lrtest(m3,m1) # p=.416
</code></pre>

<p>If there's no improvement in fit (p>.05), I (very tentatively) interpret this as support for my hypothesis that <code>demographics</code> and <code>ideology</code> don't matter.</p>

<p>Is this a right way to approach the data, or is there a more sophisticated way to test this hypothesis using multilevel modeling?  Any expertise and advice is greatly appreciated.</p>
"
"0.0448110714948221","0.045786854649563"," 60575","<p>I want to see if there is a significant relationship between the mating (response variable) and several explanatory variables such as individual number (categorical), leg movement (continuous) and the reuse of individuals (categorical). My data looks like this: </p>

<pre><code>Mating  ID  Replication Shaking
1       M1  R1          10
0       M1  R2          14
0       M2  R1          15
0       M2  R2          12
1       M3  R1          14
0       M3  R2          17
1       M4  R1          19
0       M4  R2          22
1       M5  R1          18
0       M5  R2          16
1       M6  R1          17
</code></pre>

<p>1 means mating happened, 0 means did not occur. </p>

<p>I am trying to organize the data to apply a GLMM to the data, but have not  been able to do it. I followed the model proposed by Crawley for binary response with pseudorreplication but it does not work. The script goes like this:</p>

<pre><code>model2 &lt;- lmer(Mating~Replication+(1|ID), family=binomial, method=""PQL"")
</code></pre>

<p>Can somebody tell me what part is wrong and how could I fix it? If you know a better method, that will be really welcomed as well.</p>
"
"0.10976425998969","0.112154430818409"," 63566","<p>I have conducted an experiment with multiple (categorical) conditions per subject, and multiple subject measurements.</p>

<p>My data-frame in short: A subject has one property, <code>is_frisian</code> which is either 0 or 1 depending on the subject. And it is tested for two conditions, <code>person</code> and <code>condition</code>. The measurement variable is <code>error</code>, which is either 0 or 1.</p>

<p>My mixed linear model in R is:</p>

<pre><code>&gt; model &lt;- lmer(error~is_frisian*condition*person+(1|subject_id), data=output)
</code></pre>

<p>However, the residuals plot of this model gives an unexpected (?) result.</p>

<p><img src=""http://i.stack.imgur.com/nz2KY.png"" alt=""Residuals lmer model""></p>

<p>I was taught that this plot should show randomly scattered points, and they should be normal distributed. When plotting the density of the fitted and the residuals, it shows a reasonable normal distribution. The lines you can see in the graph, however, how is this to be explained? And is this okay?</p>

<p>The only thing I could come up with is that the graph has two lines due to the categorical variables. The output variable <code>error</code> is either 0 or 1. But I do not have that much knowledge of the underlying system to confirm this. And then again, the lines also seem to have a low negative slope, is this then perhaps a problem?</p>

<p><strong>UPDATE:</strong></p>

<pre><code>&gt; model &lt;- glmer(error~is_frisian*condition*person + (1|subject_id), data=output, family='binomial')
&gt; binnedplot(fitted(model),resid(model))
</code></pre>

<p>Gives the following result:</p>

<p><img src=""http://i.stack.imgur.com/XMXFx.png"" alt=""binned residual plot""></p>

<p><strong>FINAL EDIT:</strong></p>

<p>The density-plots have been omitted, they have nothing to do with satisfaction of assumptions in this case. For a list of assumptions on logistic regression (when using family=binomial), <a href=""https://www.statisticssolutions.com/academic-solutions/resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/"" rel=""nofollow"">see here at statisticssolutions.com</a></p>
"
"0.110901743841784","0.129504781647635"," 63927","<p>I am struggling to fit a simple logistic regression for one dependent value (group) by one independent qualitative variable (dilat) measured twice independently (rater).</p>

<p>I try many solutions and think according <a href=""http://www.ats.ucla.edu/stat/mult_pkg/whatstat/"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/whatstat/</a> that the solution is a Mixed Effects Logistic Regression.</p>



<pre class=""lang-r prettyprint-override""><code>glmer_dilat&lt;-glmer(group ~ dilat + (1 | rater), data = ex, family = binomial)
summary(glmer_dilat)
</code></pre>



<pre class=""lang-r prettyprint-override""><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: group ~ dilat + (1 | rater) 
   Data: ex 
   AIC   BIC logLik deviance
 105.5 112.5 -49.74    99.48
Random effects:
 Groups Name        Variance Std.Dev.
 rater  (Intercept)  0        0      
Number of obs: 76, groups: rater, 2

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4880   1.736   0.0825 .
dilat        -1.2827     0.5594  -2.293   0.0219 *
</code></pre>

<p>But the result is the same without !</p>

<pre class=""lang-r prettyprint-override""><code>summary(glm(group ~ dilat, data =ex, family = binomial))

glm(formula = group ~ dilat, family = binomial, data = ex)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.552  -0.999  -0.999   1.367   1.367  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4879   1.736   0.0825 .
dilat        -1.2826     0.5594  -2.293   0.0219 *
</code></pre>

<p>What is the solution?</p>

<p>please find my data set here after applying a dput command to it.</p>

<pre class=""lang-r prettyprint-override""><code>structure(list(id = structure(c(38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 23L, 15L, 24L, 25L, 37L, 26L, 38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 22L, 23L, 15L, 24L, 37L, 26L), .Label = c(""1038835"", ""2025267"", ""2053954"", ""3031612"", ""40004760"", ""40014515"", ""40040532"", ""40092413"", ""40101857"", ""40105328"", ""4016213"", ""40187296"", ""40203950"", ""40260642"", ""40269263"", ""40300349"", ""40308059"", ""40327146"", ""40333651"", ""40364468"", ""40435267"", ""40440293"", ""40443920"", ""40485124"", ""40609779"", ""40628741"", ""40662695"", ""5025220"", ""E9701737"", ""M/377313"", ""qsc22913"", ""QSC29371"", ""QSC43884"", ""QSC62220"", ""QSC75555"", ""QSC92652"", ""QSD01289"", ""QSD02237"", ""U/FY0296"" ), class = ""factor""), group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), rater = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), dilat = c(1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), midbrain_atroph = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), quadrigemi_atroph = c(1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), hum_sig = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), flower_sig = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), fp_atroph = c(0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), scp_atroph = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L)), .Names = c(""id"", ""group"", ""rater"", ""dilat"", ""midbrain_atroph"", ""quadrigemi_atroph"", ""hum_sig"", ""flower_sig"", ""fp_atroph"", ""scp_atroph""), class = ""data.frame"", row.names = c(NA, -76L))
</code></pre>
"
"0.0896221429896442","0.091573709299126"," 64352","<p>I'm facing a problem with a binomial <code>glmer</code> model. I want to find if differences in flower presence in pine trees is due to procedence of the tree.
My model is as follows: <code>FlorMas ~ Proc + (1|Blq)</code>.
Proc is a factor with nine levels, one of it (<code>TAMR</code>) presents no flower at all (variable value for all <code>TAMR</code> trees is 0).
This model gives me this output:</p>

<pre><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: FlorMas ~ Proc + (1 | Blq) 
   Data: flower.data 
 AIC   BIC logLik deviance
 593 647.7 -285.5      571
Random effects:
 Groups Name        Variance Std.Dev.
 Blq    (Intercept) 0.18476  0.42983 
Number of obs: 1067, groups: Blq, 8

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -1.7668     0.2958  -5.974 2.32e-09 ***
ProcTAMR     -16.8758  1080.5608  -0.016  0.98754    
ProcARMY      -0.3543     0.3910  -0.906  0.36490    
ProcASPE      -1.4891     0.5260  -2.831  0.00464 ** 
ProcCOCA      -2.4947     0.7619  -3.274  0.00106 ** 
ProcMIMI      -1.2040     0.4930  -2.442  0.01459 *  
ProcORIA      -1.5360     0.5739  -2.676  0.00744 ** 
ProcPLEU      -1.9437     1.0538  -1.845  0.06511 .  
ProcPTOV       0.1693     0.3508   0.483  0.62945    
ProcSCRI       0.5060     0.3346   1.512  0.13050    

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I don't understand that values for <code>TAMR</code> procedence, as if it has all zero values it should be different from the others.
Any help will be appreciated.</p>
"
"0.0896221429896442","0.091573709299126"," 65656","<p>My design is as follows.</p>

<ul>
<li>$y$ is Bernoulli response </li>
<li>$x_1$ is a continuous variable </li>
<li>$x_2$ is a categorical (factor) variable with two levels</li>
</ul>

<p>The experiment is completely within subjects. That is, each subject receives each combination of $x_1$ and $x_2$.</p>

<p>This is a repeated measures logistic regression set-up. The experiment will give two ogives for $p(y=1)$ vs $x_1$, one for level1 and one for level2 of $x_2$. The effect of $x_2$ should be that for level2 compared to level1, the ogive should have a shallower slope and increased intercept.</p>

<p>I am struggling with finding the model using <code>lme4</code>. For example,</p>

<pre><code>glmer(y ~ x1*x2 + (1|subject), family=binomial)
</code></pre>

<p>So far as I understand it, the <code>1|subject</code> part says that <code>subject</code> is a random  effect. But I do not see how to specify that $x_1$ and $x_2$ are repeated measures variables. In the end, I want a model that includes a random effect for subjects, and gives estimated slopes and intercepts for level1 and level2.</p>
"
"0.0776150525706333","0.0793051585718144"," 66496","<p>I am trying to fit a GLMM:</p>

<p>logit$[P(Y_{ij}=1)] = \beta_0 + \beta_1 X_{ij} + \beta_2R_i + \gamma_i$ where $\gamma_i\sim N(0,\sigma^2_\gamma)$, $j=0,1$ and $i=0,1,...,94$. </p>

<p>I first went with the lme4 package in R:</p>

<pre><code>m &lt;-  glmer(PHENO==2 ~ GENO + RACE + (1|GROUP), family=""binomial"",data=iih_data,nAGQ=1 )
</code></pre>

<p>but this uses the Wald z-test by default for hypothesis test for $\beta_1=0$ which is probably not applicable in this case. It was suggested here <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">http://glmm.wikidot.com/faq</a> to use either MCMC or parametric bootstrap to obtain more valid p-values. I rolled my own bootstrap procedure, but it was rather slow. </p>

<p>So I wanted to see if MCMC method would be faster. This led me to the R package MCMCglmm. However, this package is above my understanding of statistics and I was unable to follow the examples on how to set up this analysis using MCMCglmm.</p>

<p>Can you help me set up the above analysis using MCMCglmm to obtain a valid p-value for $\beta_1$? </p>
"
"0.0776150525706333","0.0528701057145429"," 68341","<p>What do the <code>weights</code> argument in <code>glmer</code> refer to? I used sample sizes as <code>weights</code> with <code>glm</code>, but here I am not sure. The variance of sample sizes is quite low, but including it or not in <code>glmer</code> gives me a huge difference. For example, in the dataset below, using only one independent variable, the difference in results is huge (estimate, BIC, p.value). Does anyone have experience using weights in <code>glmer</code> and confirm if it works as expected or if I am doing it right?</p>

<p><a href=""http://comments.gmane.org/gmane.comp.lang.r.lme4.devel/10160"" rel=""nofollow"">A thread on r-sig-mixed models</a> and in github pages, there seems to be an issue with <code>weights</code> argument in <code>glmer</code>, but since my knowledge of mixed-models is only weeks old, I am not able to follow it.</p>

<p>My data:</p>

<pre><code>cv &lt;- structure(list(name = c(""AlfF"", ""AndH"", ""AntH"", ""BerG"", ""BerR"",""FreZ"", ""GerB"",""GerT"", ""GueS"", ""GueV"", ""HanN"", ""HeiW"", ""JakW"",""KarN"", ""KerG"", ""KlaS"", ""ManS"", ""MarS"", ""SilN"", ""TheG"", ""UweP"",""WerT"", ""AlfF"", ""AndH"", ""AntH"", ""BerG"", ""BerR"", ""FreZ"", ""GerB"",""GerT"", ""GueS"", ""GueV"", ""HanN"", ""HeiW"", ""JakW"", ""KarN"", ""KerG"",""KlaS"", ""ManS"", ""MarS"", ""SilN"", ""TheG"", ""UweP"", ""WerT"", ""AlfF"",""AndH"", ""AntH"", ""BerG"", ""BerR"", ""FreZ"", ""GerB"", ""GerT"", ""GueS"",""GueV"", ""HanN"", ""HeiW"", ""JakW"", ""KarN"", ""KerG"", ""KlaS"", ""ManS"",""SilN"", ""TheG"", ""UweP"", ""WerT"", ""AlfF"", ""AndH"", ""AntH"", ""BerG"",""BerR"",""FreZ"",""GerB"", ""GerT"", ""GueS"", ""GueV"", ""HanN"", ""HeiW"",""JakW"", ""KarN"", ""KerG"", ""KlaS"", ""ManS"", ""SilN"", ""TheG"", ""UweP"",""WerT"", ""AlfF"", ""AndH"", ""AntH"", ""BerK"", ""BerR"", ""ChrG"", ""FraR"",""FreZ"", ""GerB"", ""GerB"", ""GerT"", ""GueS"", ""GueV"", ""HanN"", ""HeiW"",""JakW"", ""KlaS"", ""ManS"", ""MarH"", ""PetS"", ""SilN"", ""TheG"", ""UweP"",""WerT"", ""AlfF"", ""AndH"", ""BerK"", ""BerR"", ""ChrG"", ""FraR"", ""FreZ"",""GerB"", ""GerB"", ""GerT"", ""GueV"", ""HanN"", ""HeiW"", ""JakW"", ""KlaS"",""ManS"", ""MarH"", ""PetS"", ""PetW"", ""SilN"", ""SveR"", ""UweP"", ""WerT"",""AlfF"", ""AndH"", ""AntH"", ""BerK"", ""BerR"", ""ChrG"", ""FraR"", ""FreZ"",""GerB"", ""GerB"", ""GerT"", ""GueS"", ""GueV"", ""HanN"", ""HeiW"", ""JakW"",""KlaS"", ""ManS"", ""MarH"", ""MicH"", ""PetS"", ""SilN"", ""SveR"", ""UweP"",""WerT""), prop_yes = c(0, 0.2, 0.6, 0.1, 0, 0, 0.1, 0, 0.3, 0,0, 0, 0, 0.1, 0.8, 0.1, 0.1, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0.3,0.3, 0.5, 0.4, 0.778, 0, 0.05, 0.5, 0, 0, 0.4, 0.2, 0, 0, 0, 0.2, 0.2, 0.3, 0.2, 0.6, 0.2, 0.2, 0.1, 0.1, 0.1, 0, 0.1, 0.3, 0.4, 0.1, 0.111, 0, 0.2, 0.1, 0.2, 0.8, 0, 0.111, 0, 0.1, 0, 0.2, 0.3, 0.1, 0.4, 0.333, 0.2, 0.1, 0.2, 0, 0.2, 0.182, 0, 0.1, 0.364, 0.1, 0.3, 0.375, 0, 0, 0, 0.2, 0, 0.1, 0, 0, 0, 0, 0.1, 0.1, 0, 0.3, 0, 0, 0.3, 0, 0.333, 0, 0, 0.667, 0.2, 0.571, 0.2, 0, 0.2, 0.6, 0.2, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0.2, 0.3, 0, 0.7, 0.3, 0, 0.2, 0.75, 0.2, 0.1, 0.1, 0.4, 0.1, 0.4, 0.3, 0.222, 0.2, 0.1, 0.1, 0.5, 0.2, 0.6, 0, 0, 0.1, 0.167, 0.333, 0, 0.222, 0.4, 0.5, 0, 0.3, 0.1, 0), size = c(10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 19L, 10L, 6L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 9L, 10L, 6L, 10L, 10L, 10L, 10L, 10L, 10L, 9L, 10L, 20L, 10L, 8L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 9L, 10L, 10L, 10L, 10L, 10L, 10L, 9L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 9L, 10L, 10L, 10L, 10L, 10L, 11L, 10L, 10L, 11L, 10L, 10L, 8L, 7L, 10L, 10L, 10L, 9L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 9L, 10L, 10L, 4L, 6L, 10L, 9L, 9L, 10L, 7L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 8L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 9L, 10L, 10L, 10L, 10L, 5L, 10L, 10L, 10L, 10L, 12L, 12L, 10L, 9L, 10L, 10L, 9L, 10L, 10L, 10L), tmean_winter = c(-3.83, -4.31, -3.97, -5.21, -4.6, -4.09, -4.05, -4.09, -4.85, -4.48, -4.77, -6.66, -4.16, -4.68, -4.48, -5.07, -3.83, -4.28, -4.79,  -4.83, -4.09, -4.43, 2.36, 1.47, 2.13, 1.09, 1.93, 2.26, 2.28, 1.98, 1.66, 1.3, 1.69, -1.01, 2.22, 1.89, 2, 1.23, 2.33, 2.1, 1.68, 1.66, 1.95, 1.38, 1.61, 0.86, 1.82, 0.48, 1.45, 1.74, 1.5, 1.78, 1.14, 0.65, 1.17, -1.59, 1.69, 1.55, 1.44, 0.65, 1.59, 1.16, 1.14, 1.23, 0.81, -1.53, -2.61, -1.52, -2.7, -1.77, -1.54, -1.68, -1.32, -2.16, -2.82, -1.95, -4.56, -1.57, -1.77, -1.76, -2.55, -1.51, -1.98, -2.05, -1.97, -2.62, -4.48, -5.25, -4.04, -4.92, -4.59, -5.34, -5.09, -4.12, -4.36, -5.23, -4.94, -4.7, -5.28, -4.55, -7.07, -4.18, -5.17, -4.56, -4.56, -4.74, -4.58, -4.62, -5.08, -5.25, -1.87, -2.67, -2.84, -2.47, -3.11, -2.3, -2.01, -2.05, -2.96, -2.57, -2.75, -2.54, -4.18, -2.07, -3.04, -1.81, -2.39, -2.24, -2.75, -2.75, -2.79, -2.44, -2.85, -0.35, -1.47, -0.43, -1.02, -0.76, -1.23, -1.57, -0.48, -0.65, -1.18, -0.58, -0.92, -1.58, -1.07, -4.05, -0.52, -2.1, -0.36, -0.75, -1.04, -0.67, -1.05, -1.81, -0.61, -1.64)), .Names = c(""name"", ""prop_yes"", ""size"", ""tmean_winter""), row.names = c(NA, -158L), class = ""data.frame"")

glmer(prop_yes~tmean_winter+(1|name), family='binomial', data=cv)
glmer(prop_yes~tmean_winter+(1|name), family='binomial', data=cv, weights=size)
</code></pre>
"
"0.168041518105583","0.171700704935861"," 68786","<p>I measured a binary response for each subject in 5 different conditions. For each subject and condition, I replicated the experiment 36 times. I thus have 36 binary values per condition per subject.</p>

<p>I am trying to build a model for those data. I suppose a logistic regression is what I'm looking for, and I am working with the <code>lmer</code> package. My aim is to check whether the conditions significantly influence the observed values, so I would have two models:</p>

<pre><code>lmH1&lt;-lmer(value~condition, (random effects), data=dataset, family=binomial)
</code></pre>

<p>and</p>

<pre><code>lmH0&lt;-lmer(value~1, (random effects), data=dataset, family=binomial) 
</code></pre>

<p>By looking at the output from <code>anova(lmH0, lmH1)</code>, I would be able to determine the significance of the effect of my condition.</p>

<p>I am just not sure what to specify as random effect; the models I defined so far are:</p>

<pre><code>lmH1 &lt;- lmer( value ~ condition + ( 1 | subject ), data = dataSet, family = binomial )
</code></pre>

<p>and </p>

<pre><code>lmH2 &lt;- lmer( value ~ condition + ( 1 | subject/condition ), data = dataSet, family = binomial )
</code></pre>

<p>However I am not sure about how lmer handles the replicates, so I don't know whether I should include those replicates in my random effects or not. I could modify the proposed models so that the grouping defined by the random effects refers to a specific binary values instead of a group of binaries values. My new models would then be</p>

<pre><code>lmH1a &lt;- lmer( value ~ condition + ( 1 | subject/(condition:replicate) ), data = dataSet, family = binomial )
</code></pre>

<p>and</p>

<pre><code>lmH2a &lt;- lmer( value ~ condition + ( 1 | subject/condition/replicate ), data = dataSet, family = binomial )
</code></pre>

<p>With those models R returns the warning message <code>Number of levels of a grouping factor for the random effects is equal to n, the number of observations</code>. But the model is still computed.</p>

<p>All 4 models return very similar values for the fixed effects and for the random effects that they have in common (e.g. the subject random effects are very similar for all 4 models and the condition within subject random effects are very similar for <code>lmH2</code> and <code>lmH2a</code>).</p>

<p>How can I check which random effect structure is the most appropriate for my design and collected data?</p>
"
"0.10976425998969","0.112154430818409"," 69664","<p>I want to compare two â€‹GLMs with binomial dependent variables. The results are: </p>

<pre><code> m1 &lt;- glm(symptoms ~ 1,         data=data2)
 m2 &lt;- glm(symptoms ~ phq_index, data=data2)
</code></pre>

<p>The model test gives the following results: </p>

<pre><code>â€‹ anova(m1, m2)â€‹
         no AIC    logLik   LR.stat df  Pr(&gt;Chisq)   
 m1      1  4473.9 -2236.0                        
 m2      9  4187.3 -2084.7  302.62  8   &lt; 2.2e-16 ***
</code></pre>

<p>â€‹I am used to comparing these kinds of models using chi-squared values, a chi-squared difference, and a chi-squared difference test. Since all other models in the paper are compared this way, and since I'd like to report them in a table together: why exactly is this model test different from my other model tests in which I get chi-squared values and difference tests? Can I obtain chi-squared values from this test? </p>

<p>Results from other model comparisons (e.g., GLMER), look like this: </p>

<pre><code>    #Df AIC     BIC     logLik  Chisq   Chi     DF diff Pr(&gt;Chisq)
m3  13  11288   11393   -5630.9 392.16          
m4  21  11212   11382   -5584.9 92.02   300.14  8       0.001
</code></pre>
"
"0.110901743841784","0.129504781647635"," 70272","<p>I'm trying to generate an autoplot for mer objects in the same vein as the the <a href=""http://librestats.com/2012/06/11/autoplot-graphical-methods-with-ggplot2/"" rel=""nofollow"">autoplot.lm example</a>.</p>

<p>I can extract the original data frame, the residuals and the linear predictors directly from the returned object...</p>

<pre><code>&gt; random.model &lt;- lmer(a ~ b + c + (1 | d), data = example, family = binomial
&gt; diagnostics  &lt;- cbind(random.model@frame, random.model@eta, random.model@resid)
</code></pre>

<p>...and after reading a suggestion <a href=""http://stats.stackexchange.com/questions/54818/how-to-extract-compute-leverage-and-cooks-distances-for-linear-mixed-effects-mo"">here</a> I can calculate Cooks Distance using the influence.ME package...</p>

<pre><code>&gt; library(influence.ME)
&gt; cooks       &lt;- cooks.distance(influence.ME::influence(random.model))
&gt; diagnostics &lt;- cbind(diagnostics, cooks)
</code></pre>

<p>...and elsewhere (sorry can't find link) found that I could derive the standardised residuals using the HLMdiag package...</p>

<pre><code>&gt; library(HLMdiag)
&gt; stdresid    &lt;- HLMresid(random.model, level = 1, standardize = TRUE)
&gt; diagnostics &lt;- cbind(diagnostics, stdresid)
</code></pre>

<p>But I've hit a problem as the autoplot example calls ggplot2's <code>fortify()</code> function to calculate these and two additional measurements, sigma, the estimate of the residual SD when the corresponding observation is dropped from the model and hat, the diagonal of the hat matrix.</p>

<p>Reading around I thought the <code>hatTrace()</code> function would be one part of the solution, but found  posts on <a href=""http://r.789695.n4.nabble.com/Function-hatTrace-in-package-lme4-td4646071.html"" rel=""nofollow"">R-help</a> indicating that it was removed from the lme4 package.</p>

<p>Can anyone advise on how to calculate sigma and hat vectors?</p>

<p>Or if anyone has canned solutions for diagnostic plots for lme4 that would be useful too.</p>
"
"0.148621510602115","0.138052561093764"," 72882","<p>I'd like your opinion on a very strange behavior that I recently encountered running glmer(). The problem is that when I make the dependent variable into a logical vector, glmer behaves weirdly. My dependent variable is Accuracy, and it is coded in terms of 1 (accurate response) and 0 (wrong response). What puzzles me is that transforming accuracy to a logical vector should work the same way for glmer, as a logical vector is coded in terms of TRUE or FALSE, having also 2 levels. However, glmer gives me different results depending on the transformation of the dependent variable I use. Have you guys encountered this before? Do you know why it happens? Below is sample code so you can replicate the problem yourselves.</p>

<pre><code>#Create fake data
Subject   &lt;- c(rep(""S1"",4), rep(""S2"",4), rep(""S3"",4), rep(""S4"",4))
Item      &lt;- rep(c(""I1"",""I2"",""I3"",""I4""),4)
Factor1   &lt;- c(c(rep(""e1"",2),rep(""e2"",2)), c(""e1"",""e2"",""e2"",""e1""), 
           c(rep(""e2"",2),rep(""e1"",2)), c(""e2"",""e1"",""e1"",""e2""))                  
Accuracy  &lt;- c(1,1,0,0,1,0,1,0,1,0,1,1,1,1,1,1)

#Create data frame and make ""Accuracy"" into a factor with 2 levels
data          &lt;- data.frame(Subject,Item,Factor1, Accuracy)
data$Accuracy &lt;- factor(data$Accuracy)  #Accuracy is a factor w/ 2 levels
#Run glmer
m1 &lt;- glmer(Accuracy ~ Factor1 + (1+Factor1|Subject) + (1+Factor1|Item), family = ""binomial"", data= data)  
summary(m1)
Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)    1.946      1.069   1.820   0.0687 .
Factor1e2     -1.946      1.282  -1.518   0.1290  
---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>That is the output of the first model. Now, look at what happens if I transform data$Accuracy into a logical vector when I run the model:</p>

<pre><code>m2 &lt;- glmer(as.logical(as.numeric(Accuracy)) ~ Factor1 + (1+Factor1|Subject) + (1+Factor1|Item), family = ""binomial"", data= data)  
summary(m2)

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) 2.557e+01  1.259e+05       0        1
Factor1e2   2.223e-06  1.781e+05       0        1
</code></pre>

<p>As you can see, now the coefficient estimates are very different. As I said, this seems very puzzling to me and I'd like yo know if you have some thoughts on why this should be.</p>

<p>Thanks a lot!</p>

<p>--Sol</p>
"
"0.190117275157343","0.194257172471453"," 76980","<p>I'm trying to analyse some data I've recently gotten my hands on, but I'm not entirely sure which model to use. One suggestion has been a Mixed Model, Repeated Measurements ANOVA, but I'm not sure if that such kind of model can answer the questions of interest.</p>

<p><strong>The data</strong>: 
Two individual persons (A and B) have had a lot of different values (V1, V2, V3, ..., Vn) measured four times (At T0, T1, T2 and T3) - The spacing between times differs.</p>

<p>The different values have been grouped into categories (C1, C2, C3, ..., Cn). One value may belong to none, one or multiple categories. Each of the categories have a continuos value (Response_C1,Response_C1, ..., Response_Cn), which is the sum of the measured values belonging to that category. </p>

<p>In addition to this, person B was given a drug at T1.</p>

<p>What I would like to investigate now, is:</p>

<ol>
<li>Is there any observable effect after administering the drug</li>
<li>On which categories did the drug have an effect</li>
<li>If there is an effct on a category, what is the effect size</li>
<li>How does the effect vary over time</li>
<li>If there is an effect, is the effect observed from the drug at T1 still persistant at T3</li>
</ol>

<p>I realise one of the major pitfalls is the lack of both time points and samples, but it would be appreciated if you could suggest any articles/methods for this type of analysis.</p>

<p><strong>What I have tried so far</strong> is just Repeated Measurements ANOVA, using R:</p>

<pre><code>test.aov &lt;- aov(Response_C ~ Category * Timepoint * Treatment + Error(Sample), data=df)
</code></pre>

<p>But I am not sure that the model is correct, neither am I sure that it actually answers my questions, even if I try to model it as a mixed model. </p>

<p>Any help is much appreciated. Please let me know if any additional information is needed</p>

<p><strong>Edit 1:</strong> After doing some more reading, it seems a Generalised Linear Model with a negative binomial distribution (since this kind of data is usually over-dispersed) might be better suited for this kind of data, but I'm still not sure if such a model would answer the questions. Potentially I could fit a model to each individual category, but that would inflate the Type-I error I guess, and so we would need to correct for multiple testing.</p>

<p><strong>Edit 2:</strong> Some more reading, and I thought the <code>lme4</code> R package would be a good way to fit a Linear mixed model to my data, and just do individual comparisons of each category. Here's the model I tried to fit:</p>

<pre><code>lm1 &lt;- lmer(Response ~ Treatment * Timepoint + (1|Subject), data=my_data)
</code></pre>

<p>First off, I'm not sure whether Timepoint should be a factorial or a numerical value. As I mentioned, timepoints are not evenly distributed (To be precise, I have for time 0, 2days, 14 days, 90days), however, the design is balanced. If I enter the Timepoints as a numerical value, I don't get any estimate of what the value is at any given Timepoint, but just some numbers for Correlation of fixed effects, which I can't really use for anything. On the other hand, if I enter the Timepoints as factors, I do get an estimated value for the effect at each timepoint, but I'm not too sure how certain or reliable this value is.</p>
"
"0.134433214484466","0.122098279065501"," 77313","<p>I'd like to match the outputs of lmer (really glmer) with a toy binomial example. I've read the vignettes and believe I understand what's going on.</p>

<p>But apparently I do not. After getting stuck, I fixed the ""truth"" in terms of the random effects and went after estimation of the fixed effects alone. I'm including this code below. To see that it's legit, you can comment out <code>+ Z %*% b.k</code> and it will match the results of a regular glm. I'm hoping to borrow some brainpower to figure out why I'm not able to match lmer's output when the random effects are included.</p>

<pre><code># Setup - hard coding simple data set 
df &lt;- data.frame(x1 = rep(c(1:5), 3), subject = sort(rep(c(1:3), 5)))
df$subject &lt;- factor(df$subject)

# True coefficient values  
beta &lt;- matrix(c(-3.3, 1), ncol = 1) # Intercept and slope, respectively 
u &lt;- matrix(c(-.5, .6, .9), ncol = 1) # random effects for the 3 subjects 

# Design matrices Z (random effects) and X (fixed effects)
Z &lt;- model.matrix(~ 0 + factor(subject), data = df)
X &lt;- model.matrix(~ 1 + x1, data = df)

# Response  
df$y &lt;- c(1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1)
    y &lt;- df$y

### Goal: match estimates from the following lmer output! 
library(lme4)
my.lmer &lt;- lmer( y ~ x1 + (1 | subject), data = df, family = binomial)
summary(my.lmer)
ranef(my.lmer)

### Matching effort STARTS HERE 

beta.k &lt;- matrix(c(-3, 1.5), ncol = 1) # Initial values (close to truth)
b.k &lt;- matrix(c(1.82478, -1.53618, -.5139356), ncol = 1) # lmer's random effects

# Iterative Gauss-Newton algorithm
for (iter in 1:6) {
  lin.pred &lt;- as.numeric(X %*% beta.k +  Z %*% b.k)
  mu.k &lt;- plogis(lin.pred)
  variances &lt;- mu.k * (1 - mu.k)
  W.k &lt;- diag(1/variances)

  y.star &lt;- W.k^(.5) %*% (y - mu.k)
  X.star &lt;- W.k^(.5) %*% (variances * X)
  delta.k &lt;- solve(t(X.star) %*% X.star) %*% t(X.star) %*% y.star

  # Gauss-Newton Update 
  beta.k &lt;- beta.k + delta.k
  cat(iter, ""Fixed Effects: "", beta.k, ""\n"")
}
</code></pre>
"
"0.184760780970256","0.188784037984951"," 80866","<p>After weeks of reading and trying I decided to post my question here because I could not find a convincing solution.
I radio tracked two animals for several months and now I want to find out 1) what influences the activity of the animals and 2) when (hour after sunset) they are showing the highest activity and 3) what influences the travel distance.</p>

<p>For question 1) I created a generalized linear mixed model, with animal as a random factor looking like this:
glmer(cbind(active,inactive)~offspring+season+observation_time+temperature+precipitation+season:temperature+season:precipitation+temperature:precipitation,family=binomial)</p>

<p>offspring and season are factors coded with 0 and 1,observation_time in minutes, temperature is in Â°C and precipitation in mm.</p>

<p>The first lines of the summary() are showing that:</p>

<p>AIC       BIC    logLik  deviance 
 438.3251  460.0690 -209.1625  418.3251 </p>

<p>Random effects:
 Groups Name        Variance Std.Dev.
 Name   (Intercept) 0.06594  0.2568<br>
Number of obs: 65, groups: Name, 2</p>

<p>So my question is: is this model build up correctly? Is there an improvement possible or neccessary? 
It is very difficult to work with this data, because most diagnostic plots which are usually used to evaluate models are different because of the random factor.
I also wanted to boxcox transform the response but one animal showed no activity one day (the activity is zero) and therefore this is not possible.
I try to eliminate variables or interaction terms but in most cases I can only eliminate one interaction term. After that all variables and interaction terms seems to be significant. For variable selection I use differend aproaches (AIC, BIC, AVOVA).</p>

<p>For the 3) question I created a LMM like this: 
lmer(sqrt(distance)~aktivity+season+offspring+observation_time+temperature+precipitation+season:temperature+...(other interaction terms)</p>

<p>The result from summary():</p>

<p>REML criterion at convergence: 513.0257 </p>

<p>Random effects:
 Groups   Name        Variance Std.Dev.
 Name     (Intercept)  11.6     3.405<br>
 Residual             295.8    17.199<br>
Number of obs: 62, groups: Name, 2</p>

<p>Does any of this values tell me something about the goodness of my model?</p>

<p>How can I check if data transformation is neccessary? And if yes, which one?</p>

<p>I'm honest, I'm quite new in this field and all I learnd about R and statistics do not really work with glmm or lmm. Or it's to complicated for me.
I also created a gam without the random factor to check the relationship between the variables and response but I don't know what to do with the results (seems to be no linear relationship between activity and observation_time and rainfall).
How do I fit variables to my model? 
An other idea is to fit the model without the random factor and add the random factor afterwards. Would it be ok to do it like this?</p>

<p>For the second question - at which hour after sunset they are showing the highest activity - I have no idea how the model could be build up...</p>

<p>Sorry for the amount of questions but I'm working for weeks on this and it is very frustrating...
Thanks in advance for all your ideas and help!
Iris </p>
"
"0.129358420951055","0.145392790714993"," 81057","<p>I conducted an experiment which measured a binary response for each subject. The subjects were in 1 of 3 groups. There were two other fixed factors, each of which were continuums (cont1, cont2) ranging from 0 to 10. In other words, for each step in cont1, there was a corresponding 0-10 step continuum (cont2). Cont1 refers to formant frequency of a vowel and cont2 refers to vowels duration. Therefore, there was a total of 121 stimuli repeated 7 times. Each subject provided 847 responses.</p>

<p>I would like to know how the three groups differ in there responses for the two continua. Groups 1 and 2 are control groups (I know more or less how they behave), but I am interested in group 3 and if they perform more like group 1 or group 2.</p>

<p>I have used the following model in R using lme4:</p>

<pre><code>full.mod &lt;- glmer(response ~ cont1+cont2+group+(random effects), data=df, family=binomial)
</code></pre>

<p>I think that I need the interaction between the three fixed effects to answer my research question; however I am not sure. </p>

<pre><code>full.mod.int &lt;- glmer(response ~ cont1*cont2*group+(random effects), data=df, family=binomial)
</code></pre>

<p>Therefore, my first question is whether full.mod or full.mod.int is the best choice. My experience with ANOVA leads me to believe that I need the interactions, however, being that there are 11 levels for each continuum, the output of coefficients is going to be enormous, and this makes me think that something is not right. </p>

<p>With regard to random effects, I know that I should include random intercepts for each subject (1|subject). However, it is not clear to me whether or not I need a random effect for items too. In this case, both cont1 and cont2 are items. I have a column in my data farm called stimuli which gives information like: cont1_0_cont2_0, cont1_0_cont2_1, etc. Assuming this is what I should do, my model now looks like this:</p>

<pre><code>full.mod.int &lt;- glmer(response ~ cont1*cont2*group+(1|subjects)+(1|stimuli), data=df, family=binomial)
</code></pre>

<p>Would there be any benefit to adding random slopes? If so what would they be (cont1,cont2?)?</p>
"
"0.173552533625156","0.177331725532977"," 82379","<p>I want to test the fixed and random effects of some covariates on a discrete variable with non negative values. In exploratory analysis I fitted a null Poisson GLM and an null Poisson GLMM. However, the GLMM underestimated the mean value of the response variable even after inclusion of fixed and/or random covariates. I also tried Bayesian approaches, zero-inflated models and negative binomial distributions but the ""problem"" remains.</p>

<p>Response variable mean: 0.7804<br>
GLM intercept: 0.7803772<br>
GLMM intercept: 0.6595108</p>

<p>Is the estimated intercept of the GLMM an indicative of poor fitting of the model? </p>

<pre><code>summary(banco2$caes)  
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.   
 0.0000  0.0000  0.0000  0.7804  1.0000 12.0000  


mod1 &lt;- glm(caes ~ 1, poisson, banco2)  
Deviance Residuals:  
    Min       1Q   Median       3Q      Max    
-1.2493  -1.2493  -1.2493   0.2381   6.5689  
Coefficients:  
            Estimate Std. Error z value Pr(&gt;|z|)      
(Intercept) -0.24798    0.01078  -23.01   &lt;2e-16 ***  
---  
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1  
(Dispersion parameter for poisson family taken to be 1)  
    Null deviance: 15304  on 11027  degrees of freedom  
Residual deviance: 15304  on 11027  degrees of freedom  
AIC: 27654  
Number of Fisher Scoring iterations: 5  

exp(mod1$coefficients[1])  
(Intercept)  
  0.7803772  


(mod2 &lt;- lmer(caes ~ 1 + (1 | setor), poisson, data = banco2))  
Generalized linear mixed model fit by the Laplace approximation  
Formula: caes ~ 1 + (1 | setor)  
   Data: banco2  
   AIC   BIC logLik deviance  
 13575 13590  -6785    13571  
Random effects:  
 Groups Name        Variance Std.Dev.  
 setor  (Intercept) 0.39817  0.63101  
Number of obs: 11028, groups: setor, 559  
Fixed effects:  
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -0.41626    0.02937  -14.18   &lt;2e-16 ***  

exp(fixef(mod2))  
(Intercept)  
  0.6595108  
</code></pre>

<p>Best regards!</p>
"
"0.0896221429896442","0.0686802819743445"," 82981","<p>I am exploring hierarchical logistic regression, using glmer from the lme4 package. To my understanding, one of the first steps in multilevel modeling is to estimate the degree of clustering of level-1 units within level-2 units, given by the intraclass correlation (to ""justify"" the additional cost of estimating parameters to account for the clustering). When I run a fully unconditional model with glmer</p>

<pre><code>fitMLnull &lt;- glmer(outcome ~ 1 + (1|level2.ID), family=binomial)
</code></pre>

<p>the glmer fit gives me the variance in the intercept for outcome at level-2, but the residual level-1 variance in outcome is nowhere to be found. I read somewhere (can't track it down now) that the residual level-1 variance is <em>not</em> estimated in HGLM (or at least in glmer). Is this true? If so, is there an alternative way to approximate the degree of clustering in the data? If not, how can I access this value to calculate the ICC?</p>
"
"0.118558951157635","0.103834826330233"," 85555","<p>I would like to run a lagged random effects regression.</p>

<p>The data is from an experiment in which participants were assigned to groups of five and participated in an interactive game for 20 rounds.</p>

<p>Participants could exchange something during the experiment, which is the dependent variable.</p>

<p>Now I would like to predict/explain, how much participant received from other participants based on the behaviour of previous rounds.</p>

<p>Since the data is clustered on three levels: subject, group and time (rounds), I am a little bit lost how to correctly formulate the model.</p>

<p>I am currently using the lme4 package in R. 
I transformed the dependent variable to a 0/1 (nothing received/something received) variable, due to high skewness, so I would need to specify a multilevel logistic model.</p>

<p>So far, I specified and ran the following models:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * subject | group), family = binomial)
</code></pre>

<p>and:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * group | subject), family = binomial)
</code></pre>

<p>*predictors are on subject-level.</p>

<p>I get similar (although not the same) estimates for both models, however in model1, z-values are much higher (and therefore p-values much lower).</p>

<p>Can someone help me on that?</p>

<p>What I want to know is; Can previous behaviour (that is behaviour from round x-1 etc.) predict how much a participant received in round x.
But control/acknowledge that participants are clustered in groups and that behaviour is correlated over time (rounds).</p>
"
"0.184760780970256","0.188784037984951"," 87359","<p><strong>Background</strong></p>

<p>I have a large dataset that contains three binary outcomes for individuals belonging to groups. I am interested in jointly modeling these binary outcomes because I have reason to believe they are positively correlated with one another. Most of my data is at the individual level, however I also have some group-level information.</p>

<p>Because of the structure of my data, I am treating this as a 3-level logistic regression. The first level defines the multivariate structure through dummy variables that indicate for each outcome. Therefore level-1 accounts for the within-individual measurements. Level-2 provides the between-individuals variances and level-3 gives the between-group variance.</p>

<p><em>Hypothetical Example:</em></p>

<p>Suppose I have data for students in classrooms. I want to examine whether certain student level characteristics are important predictors for passing three different pre-tests (math, history, and gym). The pre-tests are constructed such that about half of the students should pass each exam (no floor or ceiling effect). Since some students are better than others, I expect whether or not they pass their history exam to be correlated to their probability of passing their math and gym pre-tests. I also expect that students in the same classroom will perform more similarly than students across classrooms.</p>

<p><strong>Here is my attempt at writing out the model</strong></p>

<p>I use $h$ to index level-1, $i$ to index level-2, and $j$ to index level-3. Recall that level-1 corresponds to within-individual, level-2 corresponds to between-individuals, and level-3 corresponds to between-groups. So I have $h$ measures for the $i^\text{th}$ individual in the $j^\text{th}$ group.</p>

<p>Let 
\begin{align}
\alpha_{1ij} &amp;= 1 \text{ if outcome}_1 = 1 \text{ and 0 otherwise} \\
\alpha_{2ij} &amp;= 1 \text{ if outcome}_2 = 1 \text{ and 0 otherwise} \\
\alpha_{3ij} &amp;= 1 \text{ if outcome}_3 = 1 \text{ and 0 otherwise}
\end{align}</p>

<p>\begin{align}
\text{log}\left( \frac{\pi_{hij}}{1 - \pi_{hij}} \right) &amp;= \alpha_{0hij} + \alpha_{1hij}Z_{ij} + \eta_h \\
Z_{ij} &amp;= \beta_{0ij} + X_{ij}\beta_{ij} + U_{j} + \epsilon_i \\
U_{j} &amp;= \gamma_{0j} + X_{j}\gamma_{j}
 + \rho_j
\end{align}</p>

<p>Please leave suggests about this notation; I am not positive that it is correct.</p>

<p><strong>Trying to specify the model in R</strong></p>

<p>I have been using R 3.0.1, but am open to solutions using other standard software (e.g. SAS, Stata, WinBUGs, etc.). Since I am modeling a binary response I am using the <code>glmer</code> function in the <code>lme4</code> package.</p>

<p>My data is in a long format with one row per outcome per individual per group.</p>

<p>One of my current problems is correctly specifying a 3-level model. Given 5 individual-level measures and one group level measure, I have tried to specify the model as:</p>

<pre><code>&gt; glmer(pi ~ outcome1:(x1 + x2 + x3 + x4 + u5) + 
             outcome2:(x1 + x2 + x3 + x4 + u5) +
             outcome3:(x1 + x2 + x3 + x4 + u5) +
             (1 + x1 + x2 + x3 + x4 | individual) +
             (1 + u5 | group), family=binomial, data=pretest)
</code></pre>

<p>but this often produces warnings and does not converge.</p>

<p><strong>My questions</strong></p>

<ol>
<li><p>Does my approach make sense? (i.e. does it make sense to model a multilevel multivariate model by specifying the multivariate structure in the first level?)</p></li>
<li><p>I have difficult with the proper notation and appreciate suggestions for clarifying my notation.</p></li>
<li><p>Am I using the right tools for this problem? Should I be using other packages or software?</p></li>
</ol>

<p>Thanks in advance for your suggestions and advice.</p>
"
"0.141705050316284","0.14479074758769"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.0896221429896442","0.091573709299126"," 87956","<p>I have a repeated-measures experiment where the dependent variable is a percentage, and I have multiple factors as independent variables. I'd like to use <code>glmer</code> from the R package <code>lme4</code> to treat it as a logistic regression problem (by specifying <code>family=binomial</code>) since it seems to accommodate this setup directly.</p>

<p>My data looks like this:</p>

<pre><code> &gt; head(data.xvsy)
   foldnum      featureset noisered pooldur dpoolmode       auc
 1       0         mfcc-ms      nr0       1      mean 0.6760438
 2       1         mfcc-ms      nr0       1      mean 0.6739482
 3       0    melspec-maxp    nr075       1       max 0.8141421
 4       1    melspec-maxp    nr075       1       max 0.7822994
 5       0 chrmpeak-tpor1d    nr075       1       max 0.6547476
 6       1 chrmpeak-tpor1d    nr075       1       max 0.6699825
</code></pre>

<p>and here's the R command that I was hoping would be appropriate:</p>

<pre><code> glmer(auc~1+featureset*noisered*pooldur*dpoolmode+(1|foldnum), data.xvsy, family=binomial)
</code></pre>

<p>The problem with this is that the command complains about my dependent variable not being integers:</p>

<pre><code>In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>and the analysis of this (pilot) data gives weird answers as a result.</p>

<p>I understand why the <code>binomial</code> family expects integers (yes-no counts), but it seems it should be OK to regress percentage data directly. How to do this?</p>
"
"0.0776150525706333","0.0793051585718144"," 88036","<p>I am new to R, and don't see these questions answered anywhere in documentation (though I could be wrong).</p>

<ol>
<li><p>I am using the following nomenclature to run my mixed-effects logistic regression, based on instructions from another site:  </p>

<p><code>output &lt;- glmer(DV ~ IV1 + IV2 + (1 | RE), family = binomial, nAGQ = 10)</code><br>
RE is a factor with several levels.</p>

<p>This works. But I'm wondering why it's necessary to use the <code>(1 | RE)</code> syntax instead of just <code>DV~IV1+IV2 | RE</code>.</p></li>
<li><p>I am running two mixed effects logistic regressions. On one of them I can view the random effects intercepts using <code>ranef()</code>. But I get all 0s when I run ranef on the output of the other one. Both regressions/data are ostensibly the same. What do all 0s for the random effects intercepts mean?</p></li>
</ol>
"
"0.141705050316284","0.14479074758769"," 88313","<p>I have a small data set of counts of bees.</p>

<p>I tried a simple Poisson model without random effects but it was very overdispersed (3.95).
When I fit a GLMM with random effects (using glmer in lme4) it was then underdispersed (0.19). I need to include the random effects because of my set-up.</p>

<p>The data is distributed like this, with lots of zeroes and two large values:</p>

<p><img src=""http://i.stack.imgur.com/ecklZm.jpg"" alt=""histogram of count data""></p>

<p>In addition, most of the zeros occur in one level of the main predictor variable, like this:</p>

<pre><code>    with(SB, table(solbee, treatment) )
</code></pre>

<p><img src=""http://i.stack.imgur.com/qZ3Go.png"" alt=""tabulated counts""></p>

<p>I tried a negative binomial mixed model (glmer.nb in lme4) which was a better fit but still not right, also ZIP and ZINB (using package glmmADMB) but none are a good fit. I tried a hurdle model in glmmADMB but got the following error message:</p>

<p>Model:</p>

<pre><code>hurdle1 &lt;- glmmadmb(solbee ~    treatment + landuse + snh + 
                        (1|site/dayfac),
                        data=subset(SB,solbee&gt;0),
                        family=""truncnbinom1"")
</code></pre>

<p>Error message:</p>

<pre><code>Error in model.frame.default(formula = solbee ~ treatment + landuse +  : 
variable lengths differ (found for 'treatment')
</code></pre>

<p>I think this is because most of the zero counts are in one level of the treatment factor.</p>

<p>Any advice as to what to try next would be very welcome, as would any info on why my data is overdispersed <em>without</em> random effects, and underdispersed <em>with</em> random effects. </p>
"
"0.191075050867171","0.185474014008184"," 88960","<p>this is my first post, so I hope everything is in the right format. I have some problems with glmer and don't know how to fix it, so I hope somebody can help me out with this. I could not find an answer to this anywhere.</p>

<p>My experiment: The experimental setup has eight sites, each site has a central area in which organisms get marked. From the central area, organisms have the choice to go into three different areas which are equipped with traps. The three areas are crossed with sites. The recapture rates in these three areas are very low and the sampling effort (trap days) is very high. Due to external influences, traps got destroyed to different degrees in all the sampling areas. In addition to the marked organisms, unmarked organisms of the same species get caught, too. </p>

<p>The dataset looks something like this:</p>

<pre><code>library(lme4)
data &lt;- structure(list(site = c(""A"", ""A"", ""A"", ""B"", ""B"", ""B"", ""C"", ""C"", ""C"", ""D"", ""D"", ""D"", ""E"", ""E"", ""E"", ""F"", ""F"", ""F"", ""G"", ""G"", ""G"", ""H"", ""H"", ""H""), area = c(""I"", ""II"", ""III"", ""I"", ""II"", ""III"", ""I"", ""II"", ""III"", ""I"", ""II"", ""III"", ""I"", ""II"", ""III"", ""I"", ""II"", ""III"", ""I"", ""II"", ""III"", ""I"", ""II"", ""III""), marked = c(2, 6, 3, 5, 3, 9, 0, 8, 1, 1, 1, 18, 3, 0, 0, 1, 5, 6, 3, 0, 2, 2, 4, 5), unmarked = c(38, 78, 104, 1, 6, 10, 1, 13, 0, 13,7, 85, 7, 1, 0, 9, 4, 36, 3, 4, 3, 10, 20, 29), sampl_effort = c(9300, 9100, 8700, 9900, 9600, 8600, 9800, 9400, 10800, 11600, 11000, 13950, 10300, 9700, 9800, 10450, 10100, 10800, 9600, 9900, 9300, 11800, 11250, 9450)), .Names = c(""site"", ""area"", ""marked"", ""unmarked"", ""sampl_effort""), row.names = c(NA,-24), class = ""data.frame"")
</code></pre>

<p>Now I wanted to fit a glmer. Because the amount of marked organisms may be related to the total amount of organisms, I chose to take a binomial approach, with cbind(marked, unmarked). I use area with the three treatments as explanatory variable, and site as random factor. Because the sampling effort differs between the different areas, I want to include it as an offset. The code looks like this:</p>

<pre><code>mod.glmer1= glmer(cbind(marked,unmarked) ~ area + (1 | site) + offset(sampl_effort), family=binomial, data=data)
</code></pre>

<p>Then, I get the error:</p>

<blockquote>
  <p>Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate</p>
</blockquote>

<p>If I try the glmer without the offset, everything works out fine:</p>

<pre><code>mod.glmer2= glmer(cbind(marked,unmarked) ~ area + (1 | site), family=binomial, data=data)
</code></pre>

<p>Out of interest, I tried a glm without the random factor and with offset:</p>

<pre><code>mod.glm1= glm(cbind(marked,unmarked) ~ area + offset(sampl_effort), family=binomial, data=data)
</code></pre>

<p>and I get the following Warnings:</p>

<blockquote>
  <p>Warning messages:
  1: glm.fit: fitted probabilities numerically 0 or 1 occurred 
  2: glm.fit: fitted probabilities numerically 0 or 1 occurred</p>
</blockquote>

<p>By looking at the fitted values</p>

<pre><code>round(cbind(data[,3:5],fits=fitted(mod.glm1)),8)
</code></pre>

<p>I can see that all the fitted values are 0. 
I now thought that the offset is just too large to get reasonable fitted value. As the sampling effort is arbitrary (instead of days, I could have taken sampling hours, weeks, etc.), I decided to divide the sampling effort by 1000.
By doing this, the glm now works:</p>

<pre><code>mod.glm2= glm(cbind(marked,unmarked) ~ area + offset(sampl_effort/1000), family=binomial, data=data)
</code></pre>

<p>However, for the glmer</p>

<pre><code>mod.glmer3= glmer(cbind(marked,unmarked) ~ area + (1 | site) + offset(sampl_effort/1000), family=binomial, data=data)
</code></pre>

<p>I still get</p>

<blockquote>
  <p>Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate</p>
</blockquote>

<p>So my questions are basically:</p>

<ol>
<li><p>Am I allowed to divide the sampling effort by 1000? In my mind it should lead to the same results, as the relative differences in sampling effort stay the same (and sampling days are a arbitrary measurement). However, I of course tried it with e.g. dividing by 10000 and I get different results. </p></li>
<li><p>How can I include the sampling effort in the glmer? The sampling effort is just too important to keep it out, however, I also need the sites as random factor. Is the offset the right approach and if yes, why doesn't it work.</p></li>
</ol>

<p>P.S.: My session information:</p>

<blockquote>
  <p>R version 3.0.2 (2013-09-25)
  Platform: i386-w64-mingw32/i386 (32-bit)</p>
  
  <p>locale:
  [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252<br>
  [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C<br>
  [5] LC_TIME=English_United States.1252    </p>
  
  <p>attached base packages:
  [1] stats     graphics  grDevices utils     datasets  methods   base     </p>
  
  <p>other attached packages:
  [1] car_2.0-19      lme4_1.0-6      Matrix_1.1-2    lattice_0.20-23</p>
  
  <p>loaded via a namespace (and not attached):
  [1] grid_3.0.2    MASS_7.3-29   minqa_1.2.3   nlme_3.1-111  nnet_7.3-7<br>
  [6] Rcpp_0.11.0   splines_3.0.2 tools_3.0.2</p>
</blockquote>

<p>Thanks in advance!
John</p>
"
"0.179244285979288","0.183147418598252"," 89510","<p>I have a dataset that features a binary outcome, a binary predictor, and an unordered factor with 7 levels, and 120 subjects. Each of the 120 subjects were asked a binary question on seven issues, hence the 7-level unordered factor. Therefore, I have a total of 840 observations. Each of the 840 observations has a binary predictor, and a binary outcome. I want to regress the binary outcome, on the binary predictor of course.</p>

<p>The outcome is slightly complicated. I will use mac vs. PCs as an example. People are asked, ""Do you think macs are better or do you think PCs are better?"" That is recorded as a 1, for preferring macs, and 0 for preferring PCs. Then they are asked ""Is your opinion based on objective truth, or subjective opinion?"" In other words, do you think it is objective (1) or subjective (0). Then the last question is ""In 20 years, will people think macs are better or will people think PCs are better?"" Again, 1 will be coded for macs. A new variable is then created. The variable is 1 if their current opinion matches what they think the future opinion will be. Previous pilot studies found that people tend to believe their stance on issues is what the future will be (...I know, a pretty intuitive finding...people tend to think they are right). The outcome is that variable, whether their current opinion agrees with what they think the future will be. So the point is to see if how people view things as subjective vs. objective, predicts whether they think their vision of the future is in line with their beliefs. The mac vs PC is only an example of an issue. In reality, there are seven issues, ranging from gun control, to abortion, etc., and each of those seven issue ranges in ""objectivity"" given by how the participants answered the objectivity question.</p>

<ul>
<li><code>Participant ID</code>: 120 Factors </li>
<li><code>Objectivity</code>: Binary predictor of 0s and 1s </li>
<li><code>Outcome</code>: Binary outcome of 0s and 1s </li>
<li><code>Issue</code>: 7-factor level variable </li>
</ul>

<p>So I tried something like this:</p>

<pre><code>  # Dummy coding for all participants
contrasts(data$Participant) &lt;- contr.treatment(120)  #$
  # Deviance coding to compare to the overall mean
contrasts(data$Issue)       &lt;- contr.sum(7) 
model &lt;- glmer(Outcome ~ Objectivity*Issue + (Objectivity*Issue|Participant), 
               family=binomial, data=data)
</code></pre>

<p>This gives a bunch of warnings. I essentially want to see if my predictor predicts the outcome, based on the seven issue, while adding a random effect for the participant ID. </p>
"
"0.190117275157343","0.18346510733415"," 89991","<p>Which binnedplot of the glmer should I use to check the model? The residuals against the predicted values without random part(REform=NA) or residuals against the predicted values with random part(REform=NULL)?</p>

<p>I have one binary response variable (y.10) derived from one continuous variable with around 50 to 75% of zeros. I want to model the probability to exceed the limit of 10.
For this example I used only one predictor ""fragments"" which is transformed by  taking the logarithm to get a normal distribution an later a better fit  . All variables are measured in tree regions (region). Within this regions are different plots (plot) and a set of samples were taken from some objects (object).</p>

<p>To inspect the residuals I used binnedplot like discribed in the answer of the question:
<a href=""http://stats.stackexchange.com/questions/63566/unexpected-residuals-plot-of-mixed-linear-model-using-lmer-lme4-package-in-r"">Unexpected residuals plot of mixed linear model using lmer (lme4 package) in R</a>.
To save calculation time with very complex models I modeled at first with 
glm {stats} and based on this results the model with less variables with glmer{lme4}. Doing this I could observe a big difference in the binnedplot of residuals.</p>

<p>To examination the differences I created this example with only one variable. Like you can see in the picture bellow the models of glm and glmer without the random part show a very similar behavior. At the end the random part is not for interest. I need the random part only during model selection.</p>

<p>Which binnedplot of the glmer should I use to check the model? The residuals against the predicted values without random part(REform=NA) or residuals against the predicted values with random part(REform=NULL)?</p>

<p>The code and the resulting picture is given here:</p>

<pre><code>fit.glm=glm(y.10 ~ x.t , data=data, family=""binomial"")
fit.glmer=glmer(y.10 ~ x.t + (1|region) + (1|plot) + (1|object), 
          data=data, family=""binomial"")

y.glm=predict(fit.glm, type =""response"")
y.glmer=predict(fit.glmer,REform=NA,type =""response"") 
y.glmer.ran=predict(fit.glmer,REform=NULL,type =""response"")

par(mfrow=c(2,2))
plot(y.10~x, data=data, type=""n"", main=""Models"")
points(y.glmer.ran~data$x, col=""green"", pch=4, cex=0.5)
    lines(y.glm~data$x, col=""red"")
lines(y.glmer~data$x, col=""darkgreen"")

binnedplot(fitted(fit.glm),resid(fit.glm), main=""Binned residual plot glm"")
binnedplot(y.glmer.ran,resid(fit.glmer), main=""Binned residual plot glmer(REform=NULL)"")
binnedplot(y.glmer,resid(fit.glmer), main=""Binned residual plot glmer(REform=NA)"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/HoW04.jpg"" alt=""Shows in the first first Plot the glm fitted model (red) and glmer fitted models with  (green crosses) and without (dark green line) random part. The other plots show the corresponding residuals.""></p>
"
"0.0672166072422331","0.0686802819743445"," 90392","<p>I have run Generalized linear mixed model with glmer in lme4. I use R version 3.0.1. My dependent variable is binary (correct or wrong). And this is my results:</p>

<pre><code>&gt; glmer16 &lt;- glmer(result ~ (1|item) + (1|speaker) + vowel + sex + cat + dog + exposure + frequency + v00004 + v00024 + v00034 + v00044, data=data1.frame, family=binomial)
&gt; summary(glmer16)
Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: realisation ~ (1 | item) + (1 | speaker) + vowel + sex + cat +      dog + exposure + frequency + v00004 + v00024 + v00034 +      v00044 
   Data: data1.frame 

      AIC       BIC    logLik  deviance 
 881.7026  958.6402 -426.8513  853.7026 

Random effects:
 Groups  Name        Variance Std.Dev.
 speaker (Intercept) 7.0291   2.651   
 item    (Intercept) 0.5084   0.713   
Number of obs: 1800, groups: speaker, 50; item, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 15.52018    5.33634   2.908  0.00363 ** 
vowelhigh    0.16750    0.55907   0.300  0.76449    
vowellow     0.70981    0.63194   1.123  0.26134    
sexmale      1.37080    1.03228   1.328  0.18420    
cat         -0.11460    0.09537  -1.202  0.22953    
dog         -0.05460    0.03633  -1.503  0.13286    
exposure    -0.00404    0.01564  -0.258  0.79613    
frequency   -0.01709    0.15594  -0.110  0.91272    
v00004      -2.83445    0.66039  -4.292 1.77e-05 ***
v00024       0.29687    0.55868   0.531  0.59515    
v00034       0.43899    0.58656   0.748  0.45421    
v00044       0.36663    0.65130   0.563  0.57349    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My questions are: 1) Does v00004 decrease the result of 'wrong' or 'correct'? and 2) what does it mean by significant at the intercept?</p>
"
"0.134433214484466","0.137360563948689"," 90511","<p>My data has a binary response (correct/incorrect), one continuous predictor <code>score</code>, three categorical predictors (<code>race</code>, <code>sex</code>, <code>emotion</code>) and a random intercept for the random factor <code>subj</code>. All predictors are within-subject. One of the categorical factor has 3 levels, the other have two. </p>

<p>I need advice on obtaining ""global"" p-values for each categorical factor (in an ""ANOVA like"" way)</p>

<hr>

<p>Here is how I proceed :</p>

<p>I fitted a binomial GLMM using 'glmer' from the lme4 package (because 'glmmML' doesn't compute on my data and glmmPQL does not provide AIC) and did model selection using <code>drop1</code> repeatedly until no more terms can be dropped. Here is the final model (let's assume it has been validated):</p>

<pre><code>library(lme4)
M5 &lt;- glmer(acc ~ race + sex + emotion + sex:emotion + race:emotion + score +(1|subj), 
        family=binomial, data=subset)
# apparently using family with lmer is deprecated 
drop1(M5, test=""Chisq"")
summary(M5)
</code></pre>

<p><code>drop1</code> gives p-values for the higher level terms only (the two 2-way interactions + <code>score</code>). 
<code>summary</code>gives p-values for every term, but separates the different levels of each categorical factor.</p>

<p>How can I get ""global"" p-values for each factor? I need to report them even if they are not the most relevant or meaningful estimates of signifiance here. How should I proceed? I tried searching on the web and ended up reading about likelihood ratios or the ""Wald test"" but I am not sure if or how this would apply here.</p>

<p>(PS: This is a duplicate from my ""anonymous"" post here that needed editing: <a href=""http://stats.stackexchange.com/questions/90487/binomial-mixed-model-with-categorical-predictors-model-selection-and-getting-p"">Binomial mixed model with categorical predictors: model selection and getting p-values</a> Sorry about that.)</p>
"
"NaN","NaN"," 91184","<p>Suppose I fit a generalized mixed logistic model such like that:</p>

<pre><code>set.seed(2014)
require(lme4)
df&lt;-data.frame(id=rep(1:5, c(8,10,12,14,15)), 
               out=c(rbinom(8,1,0.1), rbinom(10,1,0.3),rbinom(12,1,0.1),rbinom(14,1,0.05),rbinom(15,1,0.1)),
               age=rnorm(59,50,10),
               gender=rbinom(59,1,0.5))
fit&lt;-glmer(out~age+gender+(1|id),data=df,binomial)
df$predicted&lt;-predict(fit,type=""response"")
df$pred.binary&lt;-with(df,ifelse(predicted&gt;=0.5,1,0))
apply(df[,c(2,6)],2,sum)
        out pred.binary 
          9           0 
</code></pre>

<p>What's the best way to predict a binary outcomes by accounting the random effects and with an optimized threshold based on the original data set?</p>
"
"NaN","NaN"," 92307","<p>I have a dataset with 1206 deputies from two different chambers (1998 and 2002, respectively). In addition, there are 18 parties, and some deputies are in both chambers (the ones who were reelected). I am interested in the relation between party discipline (in a scale from 0 to 100) and the probability of geting appointed as party leader (0 or 1), but I am also interested in separate estimates for each party.</p>

<p>A classmate told me that the following mixed effect model would be appropriate, but I don't know what to do with the correlation within subjects: </p>

<pre><code>model &lt;- glmer(leader ~ discipline + (1 + discipline|chamber:party), 
               data, family=binomial)
</code></pre>

<p>What is the structure of these data and which model would be appropriate in this case (using R)? </p>
"
"0.126744850104896","0.129504781647635"," 92366","<p>I am investigating variation in pollinator visitation rate (number of visits per inflorescence) with treatment and time category as fixed factors. Block is a random factor. Following Zuur et al. (2009), I used the number of visits as the response variable with the $\log$(number of inflorescences) as the offset variable in the analysis. A Poisson model was overdispersed, and therefore I opted for a negative binomial model in <code>lme4</code>, as follows:<br>
<code>model1 = glmer.nb(visits ~ treat + timecat + offset(log(infl)) + (1|block))</code></p>

<p>I am specifically interested in differences in visitation rates between treatments. I therefore performed a post hoc test: <code>OPexp1 =  glht(model1,mcp(treat = ""Tukey"")); plot(cld(OPexp1))</code></p>

<p>When I plot these results, I get number of visits on the $y$ axis. But what I want is visitation rate (visits per inflorescence). </p>

<p>Is the post hoc test taking into account the number of inflorescences? Or how do I specify that the post hoc test should be performed using visitation rate? 
I thought that running the model above is analogous to analysing the visitation rate, so are the Tukey's differences between treatments for number of visits also the same for visitation rates?</p>

<p>I assume what is happening is that fitted values are currently expressed as $Î¼ Ã— V$ (where $Î¼ =$ visitation rate and $V =$ number of inflorescences), but how do I specify that they should be expressed as $Î¼$ (visits per inflorescence) only? On p. 240, Zuur et al. (2009) mentions that this is possible, but I have not been able to find an example.</p>

<p>I am quite new to R, and this is my first post here, as I have been unsuccessful in finding an answer elsewhere, so any advice or kick in the right direction would be much appreciated.
Kind regards.</p>
"
"0.179555204315269","0.18346510733415"," 92737","<p>In my data, I have two treatment conditions with repeated measures for each subject. I would like to run a mixed logistic regression separately for each of my two conditions where my binary outcome DV (dependent variable) is regressed on my IV (independent variable) and also have a random slope and intercept fitted for each subject.</p>

<p>So, I run the following:</p>

<pre><code>modelT0 &lt;- glmer(DV ~ IV + (1|subject) + (0 + IV|subject), data = D0, family = binomial)
modelT1 &lt;- glmer(DV ~ IV + (1|subject) + (0 + IV|subject), data = D1, family = binomial)
</code></pre>

<p>In the above, D0 and D1 are data sets restricted to treatment conditions 0 and 1, respectively. What I would like to do is compare the estimated fixed effects coefficient on IV across conditions to see if it significantly changes. To do this, I pool D0 and D1 into a single data set, D, and create a treatment indicator that takes value 0 in D0 and 1 in D1. I then run:</p>

<pre><code>model &lt;- glmer(DV ~ IV + treatment + treatment:IV + (1 + treatment|subject:treatment) 
               + (0 + IV + treatment:IV|subject:treatment), data = D, family = binomial)
</code></pre>

<p>I should be able to look at the fixed effects coefficient on treatment:IV to get my answer, but the issue is that for whatever combination of random effects I seem to specify, the coefficients from the pooled regression are slightly different from the regressions specified by treatment. So for instance, the fixed effect coefficient on treatment:IV plus the one on IV in model is not equal to the coefficient on IV in model1.</p>

<p>Any idea about what I might be doing wrong or how to answer the question I have? Thanks!</p>

<p>EDIT:</p>

<p>As per Henrik's suggestion, I'm copying the random effects output of the models below:</p>

<p>summary(modelT0):</p>

<pre><code>    Random effects:
    Groups    Name        Variance  Std.Dev. 
    subject   (Intercept) 1.412e-07 0.0003758
    subject.1 IV          1.650e+00 1.2844341
</code></pre>

<p>summary(modelT1):</p>

<pre><code>    Random effects:
    Groups    Name        Variance Std.Dev.
    subject   (Intercept) 0.00378  0.06148 
    subject.1 IV          0.26398  0.51379 
</code></pre>

<p>summary(model):</p>

<pre><code>    Random effects:
    Groups              Name         Variance  Std.Dev. Corr 
    subject.treatment   (Intercept)  0.0005554 0.02357       
                        treatment    0.0066042 0.08127  -0.88
    subject.treatment.1 IV           1.6500112 1.28453       
                        IV:treatment 1.0278663 1.01384  -0.93
</code></pre>
"
"0.0633724250524478","0.0647523908238176"," 94619","<p>Stata allows for fixed effects and random effects specification of the logistic regression through the <code>xtlogit fe</code> and <code>xtlogit re</code> commands accordingly. I was wondering what are the equivalent commands for these specifications in R.</p>

<p>The only similar specification I am aware of is the mixed effects logistic regression </p>

<pre><code>&gt; mymixedlogit &lt;- glmer(y ~ x1 + x2 +  x3 + (1 | x4), data = d, family = binomial)
</code></pre>

<p>but I am not sure whether this maps to any of the aforementioned commands.</p>
"
"0.0448110714948221","0.045786854649563"," 95032","<p>I'm looking at the influence of pollen type on whether a flower sets fruit (i.e., yes or no = 1 or 0). Then looking at number of seeds per fruit (1-6 possible).</p>

<p>I was told I should use lmer, however it gave me an error:  </p>

<blockquote>
  <p>calling lmer with 'family' is deprecated; please use glmer() instead</p>
</blockquote>

<p>I've used <code>glmer(FruitYesNo~Pollentype+(1|Plantnumber),family=binomial)</code> for the fruit and <code>glmer(Seednumber ~ Pollentype+(1|Plantnumber), family=poisson)</code> for the seeds. But I've read that glmms won't perform well like that. Can anyone give me any advice on what model I should be using? </p>
"
"NaN","NaN"," 95193","<p>I'm trying to use lme4::glmer to fit a mixed model such like that:</p>

<pre><code>library(lme4)
set.seed(123)
df&lt;-data.frame(id=sample(LETTERS[1:10], 50, T),
               y=rbinom(50, 1, 0.3),
               x1=rbinom(50, 1, 0.5),
               x2=as.integer(rnorm(50, 40, 5)))
df&lt;-df[order(df$id),]

fitm&lt;-glmer(y~x1+x2+(1|id), data=df, binomial)

coef(fitm)
$id
  (Intercept)    x1       x2
A      -1.009 1.239 -0.01631
B      -1.009 1.239 -0.01631
C      -1.009 1.239 -0.01631
D      -1.009 1.239 -0.01631
E      -1.009 1.239 -0.01631
F      -1.009 1.239 -0.01631
G      -1.009 1.239 -0.01631
H      -1.009 1.239 -0.01631
I      -1.009 1.239 -0.01631
J      -1.009 1.239 -0.01631
</code></pre>

<p>I'm wondering why the effects are identical across ids. Why isn't there random effect as expected? Note, the glmer's out is exactly same as using <code>glm</code> here:</p>

<pre><code>fitg&lt;-glm(y~x1+x2, data=df, binomial)
coef(fitg)
(Intercept)          x1          x2 
   -1.00861     1.23882    -0.01631
</code></pre>
"
"0.0801604816056202","0.0819060155778602"," 97834","<p>I ran a mixed model using lme4::glmer for a logistic regression and consistently got these warning messages. I noticed there are still regular results even so, but are they accurate estimates?</p>

<pre><code>    &gt; glmm.ms1&lt;-glmer(as.formula(paste(paste(y[1], x, sep=""~""), mix[1], sep=""+"")),
    +             data=rtf2,control=glmerControl(optimizer=""bobyqa"",
    +             optCtrl=list(maxfun=100000),family=binomial)
Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.8766 (tol = 0.001)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues

&gt; coef(summary(glmm.ms1))
                       Estimate Std. Error    z value  Pr(&gt;|z|)
(Intercept)           1.810e+00  6.558e-01   2.760464 5.772e-03
lepidays             -3.340e+00  2.770e-01 -12.059620 1.726e-33
cldaysbirth          -1.555e+00  5.224e-01  -2.975934 2.921e-03
rotaarm              -2.057e-01  3.209e-01  -0.641102 5.215e-01
cldaysbirth2         -3.072e-01  2.955e-01  -1.039510 2.986e-01
bfh2                 -1.043e+01  1.160e+03  -0.008996 9.928e-01
bfh3                  4.653e-01  4.806e-01   0.968103 3.330e-01
bfh4                  2.547e-01  4.994e-01   0.509966 6.101e-01
bfh5                  3.744e-01  9.926e-01   0.377213 7.060e-01
ruuska               -1.020e-01  5.928e-02  -1.720396 8.536e-02
genderMale           -4.008e-01  2.645e-01  -1.515453 1.297e-01
epiexlbf              6.078e-04  2.796e-03   0.217391 8.279e-01
haz.epi              -7.211e-02  1.373e-01  -0.525039 5.996e-01
cldaysbirth:rotaarm   6.928e-01  4.771e-01   1.452148 1.465e-01
rotaarm:cldaysbirth2  5.181e-01  3.352e-01   1.545527 1.222e-01
Warning messages:
1: In vcov.merMod(object, use.hessian = use.hessian) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite: falling back to var-cov estimated from RX
2: In vcov.merMod(object, correlation = correlation, sigm = sig) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite: falling back to var-cov estimated from RX
</code></pre>

<p>Due to data's sensitivity, I can't post the whole process for generating same messages, but I would like to know how to handle these warnings. I don't think it's suitable to keep my eye blind here.</p>
"
"0.0517433683804222","0.0528701057145429"," 99338","<p>My outcome variable is binomial, and I have 11 independent variables and a time variable. The time variable has different slopes, so I fixed it to <code>time-before</code> and <code>time-after</code>. I used the <code>lme4</code> package (the <code>glmer</code> function).  I have a random intercept and two random slopes. I created my model like this: </p>

<pre><code>m3.glmm &lt;- glmer(y ~ timebefore + timeafter + x1 + x2 +...+ x11 +     
(1+timebefore+timeafter|id),
             data = data, family = binomial (link=""logit""), nAGQ=3)
</code></pre>

<p>When I used this model, I had this error:</p>

<pre><code>Error in updateGlmerDevfun(devfun, glmod$reTrms, nAGQ = nAGQ) : 
  nAGQ &gt; 1 is only available for models with a single, scalar random-effects term
</code></pre>

<p>Anyone have a simple explanation of how to fit (or code) this model?</p>
"
"0.10976425998969","0.112154430818409"," 99660","<p>In a mixed effect model where the intercept is random effect and the slope is fixed effect (see the code below), I understand the output of <code>summary(glmer(...))</code>. But I do not understand <code>coef(glmer(...))</code>; it will output the intercept for each sample. In the example, how are those 100 intercepts estimated? </p>

<pre><code>n &lt;- 100
x &lt;- runif(n,2,6)
a &lt;- -3 
b &lt;- 1.5
s &lt;- 1
N &lt;- 8
id &lt;- 1:n
r  &lt;- rnorm(length(x),0,s) # random factor
p &lt;- function(x,a,b) exp(a+b*x)/(1+exp(a+b*x))
y &lt;- rbinom(length(x), N, prob = p(x,a+r,b))

library(lme4)
model &lt;- glmer(cbind(y,N-y)~x+(1|id),family=binomial)
summary(model)
coef(model) # output here is what I do not understand

# are they estimates for r?
rr &lt;- coef(model)$id[,1]-summary(model)$coefficients[1,1]
plot(r,rr)
</code></pre>

<p>The likelihood of the model, I think, is:
$$
L_{i}=\int_{-\infty}^{\infty}f(y_i|N,a,b,r_{i})g(r_{i}|s)dr_{i}\\
L=\prod_{i=1}^{n} L_{i}
$$</p>

<p>where <em>f</em> is binomial pmf and <em>g</em> is normal pdf. So <em>r</em> should be integrated out. The number of the parameters of this model is 3 (<em>a</em>, <em>b</em>, and <em>s</em>). Although it seems there are 100 intercepts estimated, they are not really considered the parameters of the model, I think. <code>AIC(model)</code> is <code>-2*logLik(model)+3*2</code>. I would like to know what method will give those 100 intercepts.</p>
"
"0.126744850104896","0.113316683941681","100046","<p>Numerically deriving the MLEs of GLMM is difficult and, in practice, I know, we should not use the brute force optimization (e.g., using <code>optim</code> in a simple way). But for my own educational purpose, I want to try it to make sure if I correctly understand the model (see the code below). I found that I always get inconsistent results from the result from <code>glmer</code>). In particular, even if I use the MLEs from <code>glmer</code> as initial values, according to the likelihood function I wrote (<code>negloglik</code>), they are not MLEs (<code>opt1$value</code> is smaller than <code>opt2</code>). I think two potential reasons are: (1) <code>negloglik</code> is not written well so that too much numerical error in it, and (2) the model specification is wrong. For the model specification, the intended model is 
\begin{equation}
L=\prod_{i=1}^{n} \left(\int_{-\infty}^{\infty}f(y_i|N,a,b,r_{i})g(r_{i}|s)dr_{i}\right)
\end{equation}
where <em>f</em> is binomial pmf and <em>g</em> is normal pdf. I am trying to estimate <em>a</em>, <em>b</em>, and <em>s</em>. In particular, I want to know if the model specification is wrong, what the correct specification is.</p>

<pre><code>p &lt;- function(x,a,b) exp(a+b*x)/(1+exp(a+b*x))

a &lt;- -4  # fixed effect (intercept)
b &lt;- 1   # fixed effect (slope)
s &lt;- 1.5 # random effect (intercept)
N &lt;- 8
x &lt;- rep(2:6,each=20)
n &lt;- length(x) 
id &lt;- 1:n
r  &lt;- rnorm(n,0,s) 
y  &lt;- rbinom(n, N, prob = p(x,a+r,b))


negloglik &lt;- function(p,x,y,N){
  a &lt;- p[1]
  b &lt;- p[2]
  s &lt;- p[3]

  Q &lt;- 100  # Inf does not work well
  L_i &lt;- function(r,x,y){
    dbinom(y,size=N,prob=p(x,a+r,b))*dnorm(r,0,s)
  }

  -sum(log(apply(cbind(y,x),1,function(x) integrate(L_i,lower=-Q,upper=Q,x=x[2],y=x[1],rel.tol=1e-14)$value)))
}

library(lme4)
(model &lt;- glmer(cbind(y,N-y)~x+(1|id),family=binomial))

opt0 &lt;- optim(c(fixef(model),sqrt(VarCorr(model)$id[1])),negloglik,x=x,y=y,N=N,control=list(reltol=1e-50,maxit=10000)) 
opt1 &lt;- negloglik(c(fixef(model),sqrt(VarCorr(model)$id[1])),x=x,y=y,N=N)
opt0$value  # negative loglikelihood from optim
opt1        # negative loglikelihood using glmer generated parameters
-logLik(model)==opt1 # but these are substantially different...
</code></pre>

<p><strong>A simpler example</strong></p>

<p>To reduce the possibility of having large numerical error, I created a simpler example. </p>

<pre><code>y &lt;- c(0,3)
N &lt;- c(8,8)
id &lt;- 1:length(y)

negloglik &lt;- function(p,y,N){
  a &lt;- p[1]
  s &lt;- p[2]
  Q &lt;- 100  # Inf does not work well
  L_i &lt;- function(r,y){
    dbinom(y,size=N,prob=exp(a+r)/(1+exp(a+r)))*dnorm(r,0,s)
  }
  -sum(log(sapply(y,function(x) integrate(L_i,lower=-Q,upper=Q,y=x,rel.tol=1e-14)$value)))
}

library(lme4)
(model &lt;- glmer(cbind(y,N-y)~1+(1|id),family=binomial))
MLE.glmer &lt;- c(fixef(model),sqrt(VarCorr(model)$id[1]))
opt0 &lt;- optim(MLE.glmer,negloglik,y=y,N=N,control=list(reltol=1e-50,maxit=10000)) 
MLE.optim &lt;- opt0$par
MLE.glmer # MLEs from glmer
MLE.optim # MLEs from optim

L_i &lt;- function(r,y,N,a,s) dbinom(y,size=N,prob=exp(a+r)/(1+exp(a+r)))*dnorm(r,0,s)

L1 &lt;- integrate(L_i,lower=-100,upper=100,y=y[1],N=N[1],a=MLE.glmer[1],s=MLE.glmer[2],rel.tol=1e-10)$value
L2 &lt;- integrate(L_i,lower=-100,upper=100,y=y[2],N=N[2],a=MLE.glmer[1],s=MLE.glmer[2],rel.tol=1e-10)$value

(log(L1)+log(L2)) # loglikelihood (manual computation)
logLik(model)     # loglikelihood from glmer 
</code></pre>

<hr>

<p>By setting a high value of <code>nAGQ</code> made the MLEs from the two methods equivalent.</p>

<pre><code>glmer(cbind(y,N-y)~1+(1|id),family=binomial,nAGQ=20)
</code></pre>
"
"0.0776150525706333","0.0793051585718144","100060","<p>Maybe it's a basic question, but I'm learning about GLMM using the lme4 package. I'm confused about the way that I can know the significance the overall model using glmer.</p>

<p>First, the random model is:</p>

<pre><code>fit.random &lt;- glmer(VDEP ~ AGE +GENDER +EDUC +V1 +V2 +V3 +(1|STATE), family = binomial(""logit""), data = mydata, nAGQ = 0)
</code></pre>

<p>From Stackoverflow's ask (<a href=""http://stackoverflow.com/questions/23802033/significance-of-the-overall-model-glmer?noredirect=1#comment36684401_23802033"">link</a>) help me with the next null model.</p>

<pre><code>fit.null &lt;- update(fit.random,.~1+(1|STATE))
</code></pre>

<p>Then I can do anova and test the significance of the overall model.</p>

<p>Is correct this conjecture? 
How is the best way to test the significance of the overal model? 
It will be interesting make a pseudo r squared?
There are other important tests (overdispersion, VIF)? (I know that is a very general question, but I only want some ideas to continue learning).</p>

<p>Finally thanks for any help and sorry for the format mistakes.</p>
"
"0.148621510602115","0.124247304984387","100271","<p>I'm trying to analyse a survey study in which I'm interested in the way that individual differences between my participants influence how they respond to my stimuli. The stimuli are pieces of writing randomly selected from a large pool of such, and I'm not currently interested in any fixed effects of these stimuli.</p>

<p>My participants each view 5 pieces of writing and make a binomial choice about each on a single-item DV. I have personality variables from each of the participants that I'm interested in as a predictor of their response. When setting up a mixed-effects model in lmer (with a long dataset) I know that I should include a term for the random effects of the stimuli. My question is whether I should also include a term for the random effects of participant, along with the fixed effects of the personality variable. Because differences between participants on my personality measure are my variable of interest, does it make sense to control for random participant effects?</p>

<p>Also, if I do control for the random effects of participant, what am I conceptually doing here? Partitioning out all the participant-level variance that can't be explained by my personality variable? Controlling for the random effects of participant seems to consistently reduce the predictive power of any personality variable I include in the model, which is why I ask.</p>

<p>Some illustrative lmer syntax for the two options:
Without participant random effects:</p>

<p><code>glmer(Choice ~ Personality + (1| StimID), data=choice.long, family='binomial')</code></p>

<p>With participant random effects:</p>

<p><code>glmer(Choice ~ Personality + (1| StimID) + (1| PartID), data=choice.long, family='binomial')</code></p>

<p>Am I specifying the latter model right?</p>
"
"0.200401204014051","0.184288535050185","103187","<p>I recently ran a pilot of an experiment on Amazon's Mechanical Turk. In the experiment, participants read 5 items, and answered a yes/no question about each one. A between-participants factor was manipulated, which I'll call ""Intention.""</p>

<p>After completing the pilot of the experiment, I attempted to analyze the results in R using the 'lme4' package.</p>

<pre><code>lmer_max    = glmer(formula= Resp ~ 1+Intention + (1|PartCode)+ (1|Item), data=df, family=""binomial"")
lmer_nomain = glmer(formula= Resp ~ 1           + (1|PartCode)+ (1|Item), data=df, family=""binomial"")
</code></pre>

<p>Unfortunately, lme4 seems to dislike the data: using an older version of the package, the model fails to converge, and using a more recent version of the package, the model fit is worse than expected.</p>

<p>Looking at the results, it's apparent that many subject's responded ""yes"" across all 5 vignettes. There's nothing wrong with them doing this within the context of the experiment, but I suspect it's one major source of problems in analyzing the data. One piece of confirmation for this possibility is plotting the lmer model using the <code>effects</code> package (Resp=0: No ; Resp=1: Yes):</p>

<p><img src=""http://i.imgur.com/srhI2zY.png"" alt=""a ceiling effect""></p>

<p>This ceiling effect is unique to how the model is dealing with the random effects: the actual split between Yes's and No's is about Yes=2/3, No=1/3.</p>

<p>My question is basically: how should I deal with this? I have three main options in mind, but am open to others.</p>

<ol>
<li><p><strong>Re-design experiment so that there's more variation within each subject.</strong> Obviously this is ideal, but it's also the riskiest and most difficult to do: the yes/no question participants are being asked is sort of philosophical in nature, and people come in with strong pre-conceptions. The manipulation in question seems to push their responses around, but there's still a lot of across-subject variability, and ultimately it may be difficult to guarantee that no subjects gives the same response on all trials.</p></li>
<li><p><strong>Re-design experiment so that there's FORCED variation within each subject.</strong> Currently, subjects are asked this yes/no question for five items, and yes or no is a perfectly valid response for each. However, I could add a ""catch"" item in which ""no"" is the only plausible response, then remove any subjects who answered ""yes"" on all 6 items. This would guarantee that subjects, at maximum, answered 'yes' only 5 out of 6 times. However, this feels like a slightly odd strategy: instead of having some participants who gave the same response across all items, there will now be an item that has the same response across all participants. Is this a problem? I suppose the difference would be that, right now, about half of subjects had the same response on every item, whereas under this strategy, only 1/6 items would have the same response for every participant.</p></li>
<li><p><strong>Re-do analysis so that there's forced variation within each subject.</strong> This is the laziest possibility, but I thought I'd ask. Is it in any way kosher to introduce a dummy item/trial into the data frame, in which all participants responded ""no,"" simply to remove the difficulties that lmer is having? That is, instead of doing possibility 2, I would simulate possibility 2 with the current data. Or perhaps there's a way of telling lmer not to freak out over subjects who gave the same response across items? </p></li>
</ol>

<p>Any help would be greatly appreciated.</p>
"
"0.100200602007025","0.102382519472325","103340","<p>I have a question about SAS and R. For a research, I used a longitudinal data and I initially used SAS (<code>GLIMMIX</code>) and then I analyzed the data with R (<code>glmer</code>) programming. There are differences between p-values of SAS and R. I expected that regression coefficient and standard error could be different for R and SAS. But there are differences for p value for some variables, which are significant in R, are not significant in SAS. </p>

<p>My R model and SAS model are respectively :</p>

<pre><code>#R
m3.glmm &lt;- glmer(y ~ timebefore + timeafter + x1 + x2 +...+ x11 +      
                     (1+timebefore+timeafter|id), 
                 data=data, family=binomial(link=""logit""), nAGQ=3)

#SAS
proc glimmix data=data METHOD=QUAD(QPOINTS=3) NOCLPRINT ;
  class id x2 x3 x4 x5;
  model y(event='1')=timebefore timeafter x1 x2 x3 x4 x5 
        x6 x7  x8 x9 x10 x11 /solution CL link = logit dist = binary;
  random intercept timebefore timeafter/subject = id GCORR SOLUTION;
run;
</code></pre>

<p>Eg: variable ""x1""(defined as age) was significant (p val= 0.04) in SAS but not in R (p val=0.1). But others were similar. It means that significant variables in SAS are found significant in R, or insignificant variables in SAS are insignificant in R. </p>

<p>Does anybody know about the differences?</p>
"
"0.134433214484466","0.137360563948689","106360","<p>I am running a binomial mixed effects logistic regression in R using <code>glmer</code> for a sociolinguistics project. I was asked to used deviation (effect) coding. From what I gather, in deviation coding the last level in a factor is assigned -1, because this is the level that is never compared to the other levels within that variable. Is it possible to obtain the <code>Estimate</code> (<code>Exp(B)</code> value) for the last level as well by using function <code>relevel</code>? I need to report the estimates for all the levels.</p>

<p>For example, my model has the independent variable called <strong>Orthography</strong> with four levels (<code>s</code>, <code>sh</code>, <code>s1</code>, <code>sh1</code>). The dependent variable is <strong>produced sibilant</strong>. In deviation coding the fourth level (<code>sh1</code>) will not be compared to the other three levels, and estimates will be available for the first three (<code>s</code>, <code>sh</code>, <code>s1</code>). The intercept is the mean of the means of all four levels (<code>s + sh + s1 + sh1 / 4</code>). I am interested in obtaining the estimate for the last level (<code>sh1</code>) as well. Does anyone know how to get that? Do I have to rerun the model by changing levels? If so, does anyone know how to do that? I have been unsuccessful with using function <code>relevel</code> to do this.</p>

<p>I have other terms in my model as well:  </p>

<ul>
<li>following segment, which has two levels (<code>vowel</code>, <code>consonant</code>), </li>
<li>position of sibilant in word (<code>initial</code>, <code>medial</code>, <code>final</code>), </li>
<li>grammatical function (<code>noun</code>, <code>verb</code>, <code>adjectives</code>), and </li>
<li>language of instruction (<code>English</code>, <code>Gujarati</code>).</li>
</ul>

<p>This is the code for my model:</p>

<pre><code>model.final_si = glmer(prod_sib ~ orthography + foll_segment + word_position + 
                                  grammatical_func + language_instruction + 
                                  (1|participant) + (1|item), 
                       family=""binomial"",data=data)
</code></pre>
"
"0.118558951157635","0.121140630718605","107865","<p>I'm investigating environmental effects (wind) on acoustic receiver detection probability for two types of transmitters using a binomial glmer. While my model analysis indicates that there's a significant effect between wind speed and transmitter type, graphical visualisation does not confirm this. If I'm correct, an interaction should demonstrate different regression slopes.</p>

<pre><code>m1 &lt;- glmer(cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth + 
               Receiver.depth + Water.temperature + Wind.speed + Transmitter + 
               Distance + Habitat + Replicate + (1 | Day) + (Distance | SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat + 
               Receiver.depth:Habitat + Wind.speed:Transmitter, data=df, family=binomial(link=logit))
</code></pre>

<p>The model summary is as follows:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth +  
    Receiver.depth + Water.temperature + Wind.speed + Transmitter +  
    Distance + Habitat + Replicate + (1 | Day) + (Distance |  
    SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat +      Receiver.depth:Habitat + Wind.speed:Transmitter
   Data: df

     AIC      BIC   logLik deviance df.resid 
  3941.9   4043.8  -1953.9   3907.9     2943 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-9.4911  0.0000  0.0000  0.5666  1.9143 

Random effects:
 Groups Name        Variance Std.Dev. Corr
 SUR.ID (Intercept)  0.33414 0.5781       
        Distance     0.09469 0.3077   1.00
 Day    (Intercept) 15.96629 3.9958       
Number of obs: 2960, groups:  SUR.ID, 20 Day, 6

Fixed effects:
                                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      3.20222    2.84984   1.124  0.26116    
Transmitter.depth               -0.35015    0.11794  -2.969  0.00299 ** 
Receiver.depth                  -0.57331    0.51919  -1.104  0.26949    
Water.temperature               -0.26595    0.11861  -2.242  0.02495 *  
Wind.speed                       1.31735    1.50457   0.876  0.38127    
TransmitterPT-04                -0.68854    0.08016  -8.590  &lt; 2e-16 ***
Distance                        -0.39547    0.09228  -4.286 1.82e-05 ***
HabitatFinger                   -0.23746    3.57783  -0.066  0.94708    
Replicate2                      -0.21559    0.08009  -2.692  0.00710 ** 
TransmitterPT-04:Distance       -0.27874    0.08426  -3.308  0.00094 ***
Transmitter.depth:HabitatFinger  0.73965    0.28612   2.585  0.00973 ** 
Receiver.depth:HabitatFinger     3.02083    0.74546   4.052 5.07e-05 ***
Wind.speed:TransmitterPT-04     -0.15540    0.06572  -2.364  0.01806 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Trnsm. Rcvr.d Wtr.tm Wnd.sp TrPT-04 Distnc HbttFn Rplct2 TPT-04: Tr.:HF Rc.:HF
Trnsmttr.dp -0.024                                                                               
Recevr.dpth -0.120 -0.267                                                                        
Watr.tmprtr  0.019 -0.159  0.007                                                                 
Wind.speed   0.130  0.073 -0.974  0.040                                                          
TrnsmtPT-04 -0.015  0.027  0.020 -0.018 -0.024                                                   
Distance     0.022 -0.080  0.151 -0.052 -0.141 -0.164                                            
HabitatFngr -0.813  0.010  0.241 -0.025 -0.253  0.009   0.029                                    
Replicate2  -0.067  0.033  0.377 -0.293 -0.394  0.010   0.085  0.103                             
TrnsPT-04:D -0.006  0.043 -0.007 -0.050 -0.003  0.516  -0.373  0.004  0.006                      
Trnsmtt.:HF  0.017 -0.352  0.021  0.055  0.049  0.026  -0.142  0.031 -0.088  0.025               
Rcvr.dpt:HF  0.103  0.189 -0.830  0.051  0.817 -0.036  -0.143 -0.224 -0.385 -0.003  -0.229       
Wnd.:TPT-04 -0.002  0.026 -0.015  0.003 -0.009  0.176  -0.114 -0.002  0.016  0.306  -0.008  0.014
</code></pre>

<p><img src=""http://i.stack.imgur.com/DFxyQ.png"" alt=""enter image description here""></p>

<p>A side question: I noticed a strong negative correlation between the intercept and a dichotome categorical predictor. I wonder if this causes any problems for my data analysis. All the covariates are centered and scaled for numerical stability during modelling.  </p>
"
"0.323677144175493","0.341749552540309","109215","<p>I try to compute the marginal and conditional $R^2$ for a GLMM using a negative binomial distribution by following the procedure recommended by Nakagawa &amp; Schielzeth (2013) . Unfortunately, the supplementary material of their article does not include an example of a negative binomial distribution (see the online version of the article stated below, I also added their code below). 
I fitted my model using the glmmPQL function from the MASS package.</p>

<pre><code>full_model  &lt;- glmmPQL ( Y~ a + b + c,  random = ~ 1 +  A | location  
, family = negative.binomial (1.4 ) ,data= mydata 
</code></pre>

<p>In particular, I do have the following problems:</p>

<ol>
<li><p>First, I need to extract the fixed effect design matrix of my model. However, full_model @X or model.matrix(full_model) does not work. I also tried to set the argument x=TRUE before extracting the matrix. Well, this should not be too tricky, but the following problems are. </p></li>
<li><p>Second, I need to specify the distribution-speciï¬c variance of my model. Examples in the article (see table 2 &amp; and the supplementary R code of the online article) specify this for a binomial and a Poisson distribution. With some deeper statistical knowledge, it should not be difficult to specify this for a negative binomial distribution. </p></li>
<li><p>Finally, I would need to know if glmmPQL uses additive dispersion or to multiplicative dispersion. In the paper, they state: ""we only consider additive dispersion implementation of GLMMs although the formulae that we present below can be easily modiï¬ed for the use with GLMMs that apply to multiplicative dispersion. "" Thus, in case glmmPQL uses multiplicative dispersion, I would need further help to adjust the formula.</p></li>
</ol>

<p>Can anybody help?</p>

<p>Thanks, best 
Philipp</p>

<p>P.S. R-code is welcome.</p>

<p>Nakagawa &amp; Schielzeth (2013) A general and simple method for obtaining R 2 from generalized linear mixed-effects models. Methods in Ecology and Evolution 2013, 4, 133â€“142. doi: 10.1111/j.2041-210x.2012.00261.x</p>

<p>Their R script:</p>

<pre><code>  #A general and simple method for obtaining R2 from generalized linear mixed-effects models
  #Shinichi Nakagawa1,2 and Holger Schielzeth3
  #1 National Centre of Growth and Development, Department of Zoology, University of    Otago, Dunedin, New Zealand
  #2 Department of Behavioral Ecology and Evolutionary Genetics, Max Planck Institute for Ornithology, Seewiesen, Germany
  #3 Department of Evolutionary Biology, Bielefeld University, Bielefeld, Germany
  #Running head: Variance explained by GLMMs
  #Correspondence:
  #S. Nakagawa; Department of Zoology, University of Otago, 340 Great King Street,    Dunedin, 9054, New Zealand
  #Tel:  +64 (0)3 479 5046
  #Fax: +64 (0)3 479 7584
  #e-mail: shinichi.nakagawa@otago.ac.nz 


  ####################################################
  # A. Preparation
  ####################################################
  # Note that data generation appears below the analysis section.
  # You can use the simulated data table from the supplementary files to reproduce exactly the same results as presented in the paper.

  # Set the work directy that is used for rading/saving data tables
  # setwd(""/Users/R2"")

  # load R required packages
  # If this is done for the first time, it might need to first download and install the package
  # install.package(""arm"")
  library(arm)
  # install.package(""lme4"")
  library(lme4)


  ####################################################
  # B. Analysis
  ####################################################

  # 1. Analysis of body size (Gaussian mixed models)
  #---------------------------------------------------

  # Clear memory
  rm(list = ls())

  # Read body length data (Gaussian, available for both sexes)
  Data &lt;- read.csv(""BeetlesBody.csv"")

  # Fit null model without fixed effects (but including all random effects)
  m0 &lt;- lmer(BodyL ~ 1 + (1 | Population) + (1 | Container), data = Data)

  # Fit alternative model including fixed and all random effects
  mF &lt;- lmer(BodyL ~ Sex + Treatment + Condition + (1 | Population) + (1 | Container), data = Data)

  # View model fits for both models
  summary(m0)
  summary(mF)

  # Extraction of fitted value for the alternative model
  # fixef() extracts coefficents for fixed effects
  # mF@X returns fixed effect design matrix
  Fixed &lt;- fixef(mF)[2] * mF@X[, 2] + fixef(mF)[3] * mF@X[, 3] + fixef(mF)[4] * mF@X[, 4]

  # Calculation of the variance in fitted values
  VarF &lt;- var(Fixed)

  # An alternative way for getting the same result
  VarF &lt;- var(as.vector(fixef(mF) %*% t(mF@X)))

  # R2GLMM(m) - marginal R2GLMM
  # Equ. 26, 29 and 30
  # VarCorr() extracts variance components
  # attr(VarCorr(lmer.model),'sc')^2 extracts the residual variance
  VarF/(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + attr(VarCorr(mF), ""sc"")^2)

  # R2GLMM(c) - conditional R2GLMM for full model
  # Equ. XXX, XXX
  (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/(VarF +    VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + (attr(VarCorr(mF), ""sc"")^2))

  # AIC and BIC needs to be calcualted with ML not REML in body size models
  m0ML &lt;- lmer(BodyL ~ 1 + (1 | Population) + (1 | Container), data = Data, REML = FALSE)
  mFML &lt;- lmer(BodyL ~ Sex + Treatment + Condition + (1 | Population) + (1 | Container), data = Data, REML = FALSE)

  # View model fits for both models fitted by ML
  summary(m0ML)
  summary(mFML)


  # 2. Analysis of colour morphs (Binomial mixed models)
  #---------------------------------------------------

  # Clear memory
  rm(list = ls())
  # Read colour morph data (Binary, available for males only)
  Data &lt;- read.csv(""BeetlesMale.csv"")

  # Fit null model without fixed effects (but including all random effects)
  m0 &lt;- lmer(Colour ~ 1 + (1 | Population) + (1 | Container), family = ""binomial"", data = Data)

  # Fit alternative model including fixed and all random effects
  mF &lt;- lmer(Colour ~ Treatment + Condition + (1 | Population) + (1 | Container), family = ""binomial"", data = Data)

  # View model fits for both models
  summary(m0)
  summary(mF)

  # Extraction of fitted value for the alternative model 
  # fixef() extracts coefficents for fixed effects 
  # mF@X returns fixed effect design matrix
  Fixed &lt;- fixef(mF)[2] * mF@X[, 2] + fixef(mF)[3] * mF@X[, 3]

  # Calculation of the variance in fitted values
  VarF &lt;- var(Fixed)

  # An alternative way for getting the same result
  VarF &lt;- var(as.vector(fixef(mF) %*% t(mF@X)))

  # R2GLMM(m) - marginal R2GLMM
  # see Equ. 29 and 30 and Table 2
  VarF/(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + pi^2/3)

  # R2GLMM(c) - conditional R2GLMM for full model
  # Equ. XXX, XXX
  (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/(VarF +     VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + pi^2/3)


  # 3. Analysis of fecundity (Poisson mixed models)
  #---------------------------------------------------

  # Clear memory
  rm(list = ls())

  # Read fecundity data (Poisson, available for females only)
  Data &lt;- read.csv(""BeetlesFemale.csv"")

  # Creating a dummy variable that allows estimating additive dispersion in lmer 
  # This triggers a warning message when fitting the model
  Unit &lt;- factor(1:length(Data$Egg))

  # Fit null model without fixed effects (but including all random effects)
  m0 &lt;- lmer(Egg ~ 1 + (1 | Population) + (1 | Container) + (1 | Unit), family = ""poisson"", data = Data)

  # Fit alternative model including fixed and all random effects
  mF &lt;- lmer(Egg ~ Treatment + Condition + (1 | Population) + (1 | Container) + (1 | Unit), family = ""poisson"", data = Data)

  # View model fits for both models
  summary(m0)
  summary(mF)

  # Extraction of fitted value for the alternative model 
  # fixef() extracts coefficents for fixed effects 
  # mF@X returns fixed effect design matrix
  Fixed &lt;- fixef(mF)[2] * mF@X[, 2] + fixef(mF)[3] * mF@X[, 3]

  # Calculation of the variance in fitted values
  VarF &lt;- var(Fixed)

  # An alternative way for getting the same result
  VarF &lt;- var(as.vector(fixef(mF) %*% t(mF@X)))

  # R2GLMM(m) - marginal R2GLMM 
  # see Equ. 29 and 30 and Table 2 
  # fixef(m0) returns the estimate for the intercept of null model
  VarF/(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + VarCorr(mF)$Unit[1] + log(1 + 1/exp(as.numeric(fixef(m0)))))

  # R2GLMM(c) - conditional R2GLMM for full model
  # Equ. XXX, XXX
  (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/(VarF +    VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + VarCorr(mF)$Unit[1] + log(1 + 
                                                                       1/exp(as.numeric(fixef(m0)))))


  ####################################################
  # C. Data generation
  ####################################################

  # 1. Design matrices 
  #---------------------------------------------------

  # Clear memory
  rm(list = ls())

  # 12 different populations n = 960
  Population &lt;- gl(12, 80, 960)

  # 120 containers (8 individuals in each container)
  Container &lt;- gl(120, 8, 960)

  # Sex of the individuals. Uni-sex within each container (individuals are sorted at the pupa stage)
  Sex &lt;- factor(rep(rep(c(""Female"", ""Male""), each = 8), 60))

  # Condition at the collection site: dry or wet soil (four indiviudal from each condition in each container)
  Condition &lt;- factor(rep(rep(c(""dry"", ""wet""), each = 4), 120))

  # Food treatment at the larval stage: special food ('Exp') or standard food ('Cont')
  Treatment &lt;- factor(rep(c(""Cont"", ""Exp""), 480))

  # Data combined in a dataframe
  Data &lt;- data.frame(Population = Population, Container = Container, Sex = Sex, Condition = Condition, Treatment = Treatment)


  # 2. Gaussian response: body length (both sexes)
  #---------------------------------------------------

  # simulation of the underlying random effects (Population and Container with variance of 1.3 and 0.3, respectively)
  PopulationE &lt;- rnorm(12, 0, sqrt(1.3))
  ContainerE &lt;- rnorm(120, 0, sqrt(0.3))

  # data generation based on fixed effects, random effects and random residuals errors
  Data$BodyL &lt;- 15 - 3 * (as.numeric(Sex) - 1) + 0.4 * (as.numeric(Treatment) - 1) + 0.15 * (as.numeric(Condition) - 1) + PopulationE[Population] + ContainerE[Container] + 
    rnorm(960, 0, sqrt(1.2))

  # save data (to current work directory)
  write.csv(Data, file = ""BeetlesBody.csv"", row.names = F)


  # 3. Binomial response: colour morph (males only)
  #---------------------------------------------------

  # Subset the design matrix (only males express colour morphs)
  DataM &lt;- subset(Data, Sex == ""Male"")

  # simulation of the underlying random effects (Population and Container with variance of 1.2 and 0.2, respectively)
  PopulationE &lt;- rnorm(12, 0, sqrt(1.2))
  ContainerE &lt;- rnorm(120, 0, sqrt(0.2))

  # generation of response values on link scale (!) based on fixed effects and random effects
  ColourLink &lt;- with(DataM, 0.8 * (-1) + 0.8 * (as.numeric(Treatment) - 1) + 0.5 *    (as.numeric(Condition) - 1) + PopulationE[Population] + ContainerE[Container])

  # data generation (on data scale!) based on negative binomial distribution
  DataM$Colour &lt;- rbinom(length(ColourLink), 1, invlogit(ColourLink))

  # save data (to current work directory)
  write.csv(DataM, file = ""BeetlesMale.csv"", row.names = F)


  # 4. Poisson response: fecundity (females only)
  #---------------------------------------------------

  # Subset the design matrix (only females express colour morphs)
  DataF &lt;- Data[Data$Sex == ""Female"", ]

  # random effects
  PopulationE &lt;- rnorm(12, 0, sqrt(0.4))
  ContainerE &lt;- rnorm(120, 0, sqrt(0.05))

  # generation of response values on link scale (!) based on fixed effects, random effects and residual errors
  EggLink &lt;- with(DataF, 1.1 + 0.5 * (as.numeric(Treatment) - 1) + 0.1 *   (as.numeric(Condition) - 1) + PopulationE[Population] + ContainerE[Container] +   rnorm(480, 
                                                                                                                                                         0, sqrt(0.1)))

  # data generation (on data scale!) based on Poisson distribution
  DataF$Egg &lt;- rpois(length(EggLink), exp(EggLink))

  # save data (to current work directory)
  write.csv(DataF, file = ""BeetlesFemale.csv"", row.names = F)
</code></pre>
"
"0.190381143813348","0.12285902336679","111553","<p>I am trying to fit GLMM's to my data using the glmer function available in R's lme4 package. The data is available at: <a href=""https://onedrive.live.com/redir?resid=1B727FC7180E87DF%21118"" rel=""nofollow"">https://onedrive.live.com/redir?resid=1B727FC7180E87DF%21118</a></p>

<p>I keep getting warning messages. Can anybody help me get rid of them.</p>

<p>I am re-posting this from StackOverflow after someone's kind suggestion. That person also suggested that the main of the issue may be low number of virus positive samples n=12. Which I suspected. But I am also wandering if linear separation could be an issue, as all the virus positives occur in the low food group. Can these problems be resolved using GLMMs or should I think of other statistical tests?</p>

<p>Tried fitting the model:</p>

<pre><code>Food_Treatment.glmer &lt;- glmer(Virus_DNA~Food*Treatment+(1|Set),
                              family=binomial,data=data,method = ""ML"")
</code></pre>

<p>to get the warning messages</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
     Model failed to converge with max|grad| = 0.001101 (tol = 0.001, component 3)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
     Model failed to converge: degenerate Hessian with 4 negative eigenvalues
</code></pre>

<p>After running code with more iterations of the model, I still get the same warning messages: </p>

<pre><code>Food_Treatment.glmer &lt;- glmer(Virus_DNA~Food*Treatment+(1|Set),data=data,
                             family=binomial,control=glmerControl(optCtrl=list(maxfun=1e9)))
</code></pre>

<p>I then looked on-line and that people had similar problems and tried the optmizer <code>bobyqa</code>: </p>

<pre><code>Food_Treatment.glmer &lt;- glmer(Virus_DNA~Food*Treatment(1|Set),data=data,family=binomial,
                          control=glmerControl(optimizer=""bobyqa"",optCtrl=list(maxfun=1e9))) 
</code></pre>

<p>I then got the very similar warning messages:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
     Model failed to converge with max|grad| = 0.00393532 (tol = 0.001, component 2)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
     Model failed to converge: degenerate Hessian with 2 negative eigenvalues
</code></pre>

<p>I then thought of simplifying the model and tried no interactions between explanatory variables, with the code:  </p>

<pre><code>Food.Plus.Treatment.glmer&lt;-glmer(Virus_DNA~Food+Treatment(1|Set),family=binomial,
                                 data=data)
</code></pre>

<p>and </p>

<pre><code>Food.Plus.Treatment.glmer&lt;-glmer(Virus_DNA~Food+Treatment(1|Set),family=binomial,
                                 data=data,control=glmerControl(optCtrl=list(maxfun=1e9)))
</code></pre>

<p>Only to get the warning messages :</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
     Model failed to converge with max|grad| = 0.00248016 (tol = 0.001, component 2)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
      Model failed to converge: degenerate Hessian with 1 negative eigenvalues
</code></pre>

<p>So I tried this simplified model with the optimizers <code>bobyqa</code> and <code>Nelder_Mead</code>, as well as the optimzers <code>nlminb</code> and <code>L-BFGS-B</code> from the package <code>optimx</code>.</p>

<p>All but the <code>bobyqa</code> optimizers produce variations on the 2 warning messages. The <code>bobyqa</code> optimizer produces the 1 warning message:</p>

<pre><code>Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
    Model failed to converge with max|grad| = 0.00139574 (tol = 0.001, component 2)
</code></pre>

<p>P.S. This is my first post on here I hope I have provided enough information without being verbose and it is correctly formatted.</p>
"
"0.173552533625156","0.165509610497445","111569","<p>I have an experiment with a design in which subjects answer four items that are of four different types based on two factors (lets call the factors letter: ""a"" X ""b"" and big: ""A"" X ""a"", resulting in four types of questions A, a, B, b). The order of items (called here 1-4) is held constant and each subject answers one item of each type. The types are randomized. A subject can for example get question-type combinations: 1-a, 2-B, 3-b, 4-A; or 1-B, 2-b, 3-a, 4-A; etc.</p>

<p>I am interested in effects of question types, but expect that the random effects may play a role as well. I tried to use the following model:</p>

<pre><code>glmer(answer ~ (1|subject) + (big*letter|item) + big*letter, data = data, family = binomial(link = ""logit""))  
</code></pre>

<p>When I compare this model with one without random slopes:</p>

<pre><code>glmer(answer ~ (1|subject) + (1|item) + big*letter, data = data, family = binomial(link = ""logit""))
</code></pre>

<p>... the first model is not better in any way than the second:</p>

<pre><code>   Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&gt;Chisq)
m2  6 1242.1 1272.1 -615.04   1230.1                        
m1 15 1261.2 1336.2 -615.60   1231.2     0      9          1
</code></pre>

<p>So, my first question is whether the model is specified correctly given the design I have. The second question would be, why is it that including random slopes does not improve the model, even though it is possible to see from the data, that the effect of question type obviously differs between the items.</p>

<p>Edit:
Summary table for m1:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: answer ~ (1 | subject) + (big * letter | item) + big * letter 
   Data: data 

      AIC       BIC    logLik  deviance 
1261.2010 1336.2061 -615.6005 1231.2010 

Random effects:
 Groups  Name               Variance Std.Dev. Corr             
 subject (Intercept)        0.71862  0.8477                    
 item    (Intercept)        0.00000  0.0000                    
         bigTRUE            0.04241  0.2059     NaN            
         letterTRUE         0.10219  0.3197     NaN  1.00      
         bigTRUE:letterTRUE 0.05749  0.2398     NaN -1.00 -1.00
Number of obs: 1097, groups: subject, 275; item, 4

Fixed effects:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)          1.8297     0.1798  10.176  &lt; 2e-16 ***
bigTRUE             -0.9339     0.2413  -3.870 0.000109 ***
letterTRUE          -0.7073     0.2734  -2.587 0.009679 ** 
bigTRUE:letterTRUE   0.7458     0.3159   2.361 0.018212 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) bgTRUE ltTRUE
bigTRUE     -0.683              
letterTRUE  -0.602  0.698       
bgTRUE:TRUE  0.521 -0.786 -0.792
</code></pre>
"
"0.22864940580401","0.173058043883722","113722","<p>I am trying to fit GLMM's to my data using the glmer function available in R's lme4 package. The data is available under the name block1and2 at: <a href=""https://onedrive.live.com/redir?resid=1B727FC7180E87DF%21119"" rel=""nofollow"">https://onedrive.live.com/redir?resid=1B727FC7180E87DF%21119</a></p>

<p>I keep getting error or warning messages. Can anybody help me get rid of them.
There is probably a problem with the low number of Virus positive samples.Can these problems be resolved using GLMMs or should I think of other statistical tests?
N.B. this is a similar problem I had posted about concerning a similar data set and similar analyses (<a href=""http://stats.stackexchange.com/questions/111553/after-trying-various-optimzers-model-simplification-running-more-iterations-wh"">After trying various optimzers, model simplification running more iterations, when fitting GLMMs, R still produces warning messages</a>) </p>

<p>Tried fitting the model:
<code>Line.glmer&lt;-glmer(Virus_DNA~Line+(1|Block/Day_of_Analyses),family=binomial,data=data)</code></p>

<p>Only to get the error message
<code>Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate</code></p>

<p>When running the code, with more iterations:
<code>Line.glmer&lt;-glmer(Virus_DNA~Line+(1|Block/Day_of_Analyses),family=binomial,data=data,control=glmerControl(optCtrl=list(maxfun=1e9)))</code></p>

<p>I get the same message.</p>

<p>When running the code with the optimizer bobyqa:
<code>Line.glmer&lt;-glmer(Virus_DNA~Line+(1|Block/Day_of_Analyses),family=binomial,data=data,control=glmerControl(optimizer=""bobyqa"",optCtrl=list(maxfun=1e9)))</code></p>

<p>I get the warning messages: 
<code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00163126 (tol = 0.001, component 5)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 2 negative eigenvalues</code></p>

<p>When running the code with the optimizer Nelder_Mead:
'Line.Nelder_Mead.glmer&lt;-glmer(Virus_DNA~Line+(1|Block/Day_of_Analyses),family=binomial,data=data,control=glmerControl(optimizer=""Nelder_Mead"",optCtrl=list(maxfun=1e9)))'</p>

<p>I get the error message:
<code>Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate</code></p>

<p>I have also tried the optimizer ""nlminb"" from the package ""optimx"". With the code
<code>Line.nlminb.glmer&lt;-glmer(Virus_DNA~Line+(1|Block/Day_of_Analyses),family=binomial,data=data,                     control=glmerControl(optimizer=""optimx"",optCtrl=list(method=""nlminb"")))</code></p>

<p>Getting the warning messages:
<code>Warning messages:
1: In optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower,  :
  Parameters or bounds appear to have different scalings.
  This can cause poor performance in optimization. 
  It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA.
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00184741 (tol = 0.001, component 5)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 2 negative eigenvalues</code></p>

<p>Similarly with the optimizer ""nlminb"" from the package ""optimx"":
<code>Line.LBFGSB.glmer&lt;-glmer(Virus_DNA~Line+(1|Block/Day_of_Analyses),family=binomial,data=data,
control=glmerControl(optimizer=""optimx"",optCtrl=list(method=""L-BFGS-B"")))</code></p>

<p>Produces the warning messages:
<code>Warning messages:
1: In optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower,  :
  Parameters or bounds appear to have different scalings.
  This can cause poor performance in optimization. 
  It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA.
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00330343 (tol = 0.001, component 5)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?</code></p>
"
"0.185046567345195","0.178571822637922","114040","<p>I was doing some reading about categorical data analysis (e.g. Agresti (2010): Analysis of Ordinal Categorical Data). Additionally to analysing data with an ordinal response with the help of proportional odds cummulative mixed effects models it seems some people suggest e.g. to use the <code>glmer()</code> function from <code>lme4</code> in <code>R</code> to fit a whole range of models.</p>

<p>My data looks as follows:</p>

<ol>
<li>ordinal response: a five point scale with $1&lt;2&lt;3&lt;4&lt;5$ ($1$ being worst, $3$ being neutral, $5$ being best)</li>
<li>one single categorical predictor ""type"" with 5 levels</li>
<li>subjects as random effects term which I would specify in <code>lme4</code> syntax: <code>(type | subjects)</code> with intercept and slope for every subject.</li>
</ol>

<p>Every subject saw $320$ items ($40$ for level $1$, $40$ for level $2$, $40$ for level $3$, $160$ for level $4$) and rated every single one of them. The dataset seems quite nice and as it only contains a single predictor is very good to get
acquainted with categorical data analysis.</p>

<p>However, I was wondering if there are alternative ways of analysing such
data apart from treating the response as a continuous variable and using <code>lmer()</code> or <code>aov()</code>. And if there are any recommendation or advice people can
give here.</p>

<p>Here is an alternative I read about . One could split a dataset with an ordinal response into subsets that each only contain two levels of the ordinal response. There are some questions I have about this. How would you split that dataset into subsets given a $5$ point ordinal response? Based on the ordering of my response the most reasonable choice to me seems: subset $A$ with levels $1$ and $2$, subset $B$ with levels $2$ and $3$, subset $C$ with levels $3$ and $4$, subset $D$ with levels $4$ and $5$. I tried this with the <code>wine</code> data set from the <code>ordinal</code> package. But this will ungraciously fail because of convergence failures, model unidentifiability etc. (I'm using <code>lme4 1.1.8</code>). Here is the wine example:</p>

<pre><code>data(""wine"", package = ""ordinal"") # the wine dataset
library(lme4)
sub &lt;- wine$rating %in% grep(""[1-2]"", wine$rating, value = TRUE)
sub12 &lt;- subset(wine, subset = sub)
sub12$rating &lt;- factor(sub12$rating, ordered = FALSE)
glmer12 &lt;- glmer(rating ~ temp + contact + (temp | judge),
                 data = sub12,
                 family = binomial)

# Substituting (1 | judge) for (temp | judge) won't make it better

# Will give no warning
glm12 &lt;- glm(rating ~ temp + contact,
             data = sub12,
             family = binomial)

sub &lt;- wine$rating %in% grep(""[2-3]"", wine$rating, value = TRUE)
sub23 &lt;- subset(wine, subset = sub)
sub23$rating &lt;- factor(sub23$rating, ordered = FALSE)

glmer23 &lt;- glmer(rating ~ temp + contact + (temp | judge),
                 data = sub23,
                 family = binomial)

sub &lt;- wine$rating %in% grep(""[3-4]"", wine$rating, value = TRUE)
sub34 &lt;- subset(wine, subset = sub)
sub34$rating &lt;- factor(sub34$rating, ordered = FALSE)
glmer34 &lt;- glmer(rating ~ temp + contact + (temp | judge),
                 data = sub34,
                 family = binomial)
</code></pre>

<p>Of course the wine dataset is rather tiny (and we have full- or quasi-separation) and mine won't but how likely am
I to run in similar problem if I analyse ordinal data this way?</p>
"
"0.10976425998969","0.112154430818409","114213","<pre><code>library(lme4)
    out &lt;- glmer(cbind(incidence, size - incidence)
                 ~ period
                 + (1 | herd),
                 data = cbpp,
                 family = binomial,
                 contrasts = list(period = ""contr.sum""))

summary(out)
Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -2.32337    0.22129 -10.499  &lt; 2e-16 ***
period1      0.92498    0.18330   5.046 4.51e-07 ***
period2     -0.06698    0.22845  -0.293    0.769
period3     -0.20326    0.24193  -0.840    0.401
</code></pre>

<p>I was never in a situation where I needed to fit a generalised linear model with effect coding (<code>contr.sum</code> for <code>R</code> users). Can I apply the same interpretation as in the linear model case? In a normal linear model the intercept would be the grand mean and the $\beta$s (parameters for <code>period1</code>, <code>period2</code>, <code>period3</code> and <code>period4 = (Intercept) - period1 - period2 - period3</code> the effects i.e. how the factor levels deviate from the grand mean.</p>

<p>Here is how I think the analogous interpretation for generalised linear models goes. (I will exponentiate all parameters and hence transform the log-odds(-ratios) to odds(-ratios).) The intercept $\exp((\text{Intercept}))$ would then be the overall <strong>odds</strong> of success vs. failure (sticking here to classical binomial terminology) and the $\beta$s the <strong>log-odds-ratios</strong>. And we get the <strong>odds</strong> for e.g. <code>period1</code> by adding $\text{(Intercept)}+\text{period1}$ and then exponentiating: $\exp(\text{(Intercept)}+\text{period1})$. Is the $\text{(Intercept)}$ really the overall/medium <strong>odds</strong> and the $\beta$s <strong>odds-ratios</strong>?</p>
"
"0.155230105141267","0.145392790714993","114350","<p>Thanks to Rijmen et al.(2003), we can fit GRM to the data with <code>lme4::glmer</code>.</p>

<p>I think Rasch model is straightforward, with <code>data.frame</code> with columns like this</p>

<pre><code> response  person  item
 0         1      1
 0         1      2
 1         1      3
 ...
 1         2      1
 0         2      2
</code></pre>

<p>we can fit Rasch model like this</p>

<pre><code> glmer(response ~ -1 + item + (1|person), data=   , family=""binomial"")
</code></pre>

<p>But how about GRM? The data would be like this</p>

<pre><code> response  person  item
 2         1      1
 4         1      2
 3         1      3
 ...
 1         2      1
 4         2      2
 ...
</code></pre>

<p>For a Likert scale (1 to 5). I thought converting the data like this</p>

<pre><code> response  person item  category
 1          1    1       2
 0          1    1       3
 1          1    2       4
 0          1    2       5
</code></pre>

<p>Because for <code>person1</code>, <code>item1</code>, the response is 2, which means that for response 2, it's yes and for response 3, it's no.</p>

<p>The model would be</p>

<pre><code> response ~ item:category + (1|person)
</code></pre>

<p>But I am not quite sure this is the right way to do...</p>

<p><em>Note</em>: person, item, category variables are all factors</p>

<p>According to <a href=""http://www.jstatsoft.org/v39/i12/paper"" rel=""nofollow"">De Boeck et al. (2011)</a>, GRM cannot be fitted with <code>lmer</code>
which is rather in contrast to Rijmen et al(2003).</p>

<p>=== ADDED</p>

<p>Now I think I am pretty sure it will work, at least for GRM with no slope parameter.</p>

<p>Data should be coded like this.</p>

<pre><code>response  person item  category
 0          1    1       1
 1          1    1       2
 1          1    1       3
 1          1    1       4
 1          1    1       5  (which is always true so should be omitted.)
</code></pre>

<p>for 1-5 category(ordinal) answer.</p>

<p>Main benefit of using GLMM for IRT model is you can put other covariates
(person, item, person-item) into the model.</p>

<p>And for GRM, you can set the difference between the ordinal response is the same,
which can't be handled by ordinary GRM function, for example, ltm::grm.
(Oh, I see ordinal::clmm can handle this, but I doubt it can be useful for a model like this)</p>

<pre><code>  response ~ item + (1 + category|person)
</code></pre>

<p>or this</p>

<pre><code>  response ~ item + (-1 + category|item) + (1|person)
</code></pre>

<p>in this case, category is integer and would be better if coded as -2, -1, 0, 1, 2.</p>

<p><strong>References</strong></p>

<p>Rijmen, F., Tuerlinckx, F., De Boeck, P., &amp; Kuppens, P. (2003). A nonlinear mixed model framework for item response theory. Psychological methods, 8(2), 185.</p>

<p>De Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A., Tuerlinckx, F., &amp; Partchev, I. (2011). The estimation of item response models with the lmer function from the lme4 package in R. Journal of Statistical Software, 39(12), 1-28.</p>

<p>====</p>

<p>Here's my source.</p>

<pre><code>library(ltm)
#Science[c(1,3,4,7)]
Sci.df &lt;- Science[c(1,3,4,7)] # Comfort, Work, Future, Benefit
Sci.df$id = 1:nrow(Sci.df)

Sci.long &lt;- reshape(Sci.df, varying=colnames(Sci.df[-5]), 
                v.names=""Response"", timevar=""item"", idvar=c(""id""), direction=""long"")
Sci.long$id &lt;- as.factor(Sci.long$id)
Sci.long$item &lt;- as.factor(Sci.long$item)

library(ordinal)
Sci.long.clmm &lt;- clmm(Response ~ (1|id)+item, data=Sci.long, threshold=""flexible"",     nAGQ=-21)
summary(Sci.long.clmm)

Positive1=as.integer(Sci.long$Response)&lt;=1
    Positive2=as.integer(Sci.long$Response)&lt;=2
Positive3=as.integer(Sci.long$Response)&lt;=3

Sci.long.sep1=Sci.long
Sci.long.sep1$Response=1; Sci.long.sep1$Positive=Positive1

Sci.long.sep2=Sci.long
Sci.long.sep2$Response=2; Sci.long.sep2$Positive=Positive2

Sci.long.sep3=Sci.long
Sci.long.sep3$Response=3; Sci.long.sep3$Positive=Positive3

Sci.long.sep = rbind(Sci.long.sep1, Sci.long.sep2, Sci.long.sep3)

Sci.long.sep$Response=as.factor(Sci.long.sep$Response)

Sci.long.sep.glmm &lt;- glmer(Positive ~ -1 + Response + item + (1|id), data=Sci.long.sep, family=binomial,
                       nAGQ=21, control=glmerControl(optimizer=""optimx"",
                       optCtrl=list(method=""nlminb""), check.conv.grad= .makeCC(""warning"", tol = 1e-4, relTol = NULL) ))
summary(Sci.long.sep.glmm)
</code></pre>

<p>I tried my best to make it same for clmm and glmer... but the log likelihood is different.</p>

<p>logLik = -1730.6 for glmer
logLik = -1633.5 for clmm</p>

<p>and the parameters r not the same but similar.</p>

<p>Does anyone know why the log likehoods are different?</p>
"
"0.2574200074605","0.263025454922346","114895","<p>I am a user more familiar with R, and have been trying to estimate random slopes (selection coefficients) for about 35 individuals over 5 years for four habitat variables. The response variable is whether a location was ""used"" (1) or ""available"" (0) habitat (""use"" below).</p>

<p>I am using a Windows 64-bit computer.</p>

<p>In R version 3.1.0, I use the data and expression below. PS, TH, RS, and HW are fixed effects (standardized, measured distance to habitat types). lme4 V 1.1-7. </p>

<pre><code>str(dat)
'data.frame':   359756 obs. of  7 variables:
 $ use     : num  1 1 1 1 1 1 1 1 1 1 ...
 $ Year    : Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 4 4 4 4 4 4 4 4 3 4 ...
 $ ID      : num  306 306 306 306 306 306 306 306 162 306 ...
 $ PS: num  -0.32 -0.317 -0.317 -0.318 -0.317 ...
 $ TH: num  -0.211 -0.211 -0.211 -0.213 -0.22 ...
 $ RS: num  -0.337 -0.337 -0.337 -0.337 -0.337 ...
 $ HW: num  -0.0258 -0.19 -0.19 -0.19 -0.4561 ...

glmer(use ~  PS + TH + RS + HW +
     (1 + PS + TH + RS + HW |ID/Year),
     family = binomial, data = dat, control=glmerControl(optimizer=""bobyqa""))
</code></pre>

<p>glmer gives me parameter estimates for the fixed effects that make sense to me, and the random slopes (which I interpret as selection coefficients to each habitat type) also make sense when I investigate the data qualitatively. The log-likelihood for the model is -3050.8.</p>

<p>However, most research in animal ecology do not use R because with animal location data, spatial autocorrelation can make the standard errors prone to type I error. While R uses model-based standard errors, empirical (also Huber-white or sandwich) standard errors are preferred. </p>

<p>While R does not currently offer this option (to my knowledge - PLEASE, correct me if I am wrong), SAS does - although I do not have access to SAS, a colleague agreed to let me borrow his computer to determine if the standard errors change significantly when the empirical method is used.</p>

<p>First, we wished to ensure that when using model-based standard errors, SAS would produce similar estimates to R - to be certain that the model is specified the same way in both programs. I don't care if they are exactly the same - just similar.
I tried (SAS V 9.2):</p>

<pre><code>proc glimmix data=dat method=laplace;
   class year id;
   model use =  PS TH RS HW / dist=bin solution ddfm=betwithin;
   random intercept PS TH RS HW / subject = year(id) solution type=UN;
run;title;
</code></pre>

<p>I also tried various other forms, such as adding lines</p>

<pre><code>random intercept / subject = year(id) solution type=UN;
random intercept PS TH RS HW / subject = id solution type=UN;
</code></pre>

<p>I tried without specifying the </p>

<pre><code>solution type = UN,
</code></pre>

<p>or commenting out</p>

<pre><code>ddfm=betwithin;
</code></pre>

<p>No matter how we specify the model (and we have tried many ways), I cannot get the random slopes in SAS to remotely resemble those output from R - even though the fixed effects are similar enough. And when I mean different, I mean that not even the signs are the same. The -2 Log Likelihood in SAS was 71344.94. </p>

<p>I can't upload my full dataset; so I made a toy dataset with only the records from three individuals. SAS gives me output in a few minutes; in R it takes over an hour. Weird. With this toy dataset I'm now getting different estimates for the fixed effects. </p>

<p>My question:
Can anyone shed light on why the random slopes estimates might be so different between R and SAS? Is there anything I can do in R, or SAS, to modify my code so that the calls produce similar results? I'd rather change the code in SAS, since I ""believe"" my R estimates more. </p>

<p>I'm really concerned with these differences and want to get to the bottom of this problem!</p>

<p>My output from a toy dataset that uses only three of the 35 individuals in the full dataset for R and SAS are included as jpegs.</p>

<p><img src=""http://i.stack.imgur.com/ucNnh.jpg"" alt=""R output"">
<img src=""http://i.stack.imgur.com/jUC0K.jpg"" alt=""SAS output 1"">
<img src=""http://i.stack.imgur.com/IfCJm.jpg"" alt=""SAS output 2"">
<img src=""http://i.stack.imgur.com/7XJdA.jpg"" alt=""SAS output 3""></p>

<hr>

<p>EDIT AND UPDATE:</p>

<p>As @JakeWestfall helped discover, the slopes in SAS do not include the fixed effects. When I add the fixed effects, here is the result - comparing R slopes to SAS slopes for one fixed effect, ""PS"", between programs: (Selection coefficient = random slope). Note the increased variation in SAS. </p>

<p><img src=""http://i.stack.imgur.com/JozTd.jpg"" alt=""R vs SAS for PS""></p>
"
"0.155230105141267","0.158610317143629","115356","<p>I'm a beginner in statistics and I have to run multilevel logistic regressions. I am confused with the results as they differ from logistic regression with just one level. </p>

<p>I don't know how to interpret the variance and correlation of the random variables. And I wonder how to compute the ICC.</p>

<p>For example : I have a dependent variable about the protection friendship ties give to individuals (1 is for individuals who can rely a lot on their friends, 0 is for the others). There are 50 geographic clusters of respondant and one random variable which is a factor about the social situation of the neighborhood. Upper/middle class is the reference, the other modalities are working class and underprivileged neighborhoods. </p>

<p>I get these results :</p>

<pre><code>&gt; summary(RLM3)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Arp ~ Densite2 + Sexe + Age + Etudes + pcs1 + Enfants + Origine3 +      Sante + Religion + LPO + Sexe * Enfants + Rev + (1 + Strate |  
    Quartier)
   Data: LPE
Weights: PONDERATION
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  3389.9   3538.3  -1669.9   3339.9     2778 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2216 -0.7573 -0.3601  0.8794  2.7833 

Random effects:
 Groups   Name           Variance Std.Dev. Corr       
 Neighb. (Intercept)     0.2021   0.4495              
          Working Cl.    0.2021   0.4495   -1.00      
          Underpriv.     0.2021   0.4495   -1.00  1.00
Number of obs: 2803, groups:  Neigh., 50

Fixed effects:
</code></pre>

<p>The differences with the ""call"" part is due to the fact I translated some words.</p>

<p>I think I understand the relation between the random intercept and the random slope for linear regressions but it is more difficult for logistics ones. I guess that when the correlation is positive, I can conclude that the type of neighborhood (social context) has a positive impact on the protectiveness of friendship ties, and conversely. But how do I quantify that ?</p>

<p>Moreover, I find it odd to get correlation of 1 or -1 and nothing more intermediate.</p>

<p>As for the ICC I am puzzled because I have seen a post about lmer regression that indicates that intraclass correlation can be computed by dividing the variance of the random intercept by the variance of the random intercept, plus the variance the random variables, plus the residuals. </p>

<p>But there are no residuals in the results of a glmer. I have read in a book that ICC must be computed by dividing the random intercept variance by the random intercept variance plus 2.36 (piÂ²/3). But in another book, 2.36 was replaced by the inter-group variance (the first level variance I guess). 
What is the good solution ?</p>

<p>I hope these questions are not too confused.
Thank you for your attention !</p>
"
"0.0448110714948221","0.045786854649563","116621","<p>Say, I fit a linear or generalised linear model in <code>R</code> with dummy coding (<code>contr.treatment</code> for <code>R</code> users) with a specified reference group:</p>

<pre><code>        library(lme4)
        out1 &lt;- glmer(cbind(incidence, size - incidence)
                      ~ C(period, contr.treatment(4, base=1))
                      + (1 | herd),
                      data = cbpp,
                      family = binomial)
</code></pre>

<p><code>period4</code> is the<code>(Intercept)</code>. And I see that the difference between <code>period1</code> and <code>period4</code> is significant. But what if I'm interested whether there is a difference between say <code>period2</code> and <code>period3</code>. It seems that when people encounter this case they e.g. use <code>glht()</code> from <code>multcomp</code> or <code>TukeysHD</code> to do pairwise comparisons. Couldn't I just refit the model and specify a different reference group? (I would obviously lose any p-value adjustments the aforementioned functions use but otherwise it should be similar.):</p>

<pre><code>        out2 &lt;- glmer(cbind(incidence, size - incidence)
                      ~ C(period, contr.treatment(4, base=2))
                      + (1 | herd),
                      data = cbpp,
                      family = binomial)
</code></pre>
"
"0.16032096321124","0.16381203115572","118172","<p>I fitted a mixed logit model with crossed random effects in <code>lme4_1.1-7::glmer</code> (R version 3.1.1 / OS X 10.9.4 Mavericks).</p>

<p>Had to simplify the maximal random-effect structure justified by the design due to failed convergence; the final model is estimated without any problems:</p>

<pre><code>fitted_1 &lt;- glmer(DV ~ IV1.d*IV2.d + (IV1.d*IV2.d| SubjN) + (1|Items) +  
                       (0+IV1.d|Items) + (0+IV2.d|Items) + (0+IV1.d:IV2.d|Items), 
                  glmerControl(optimizer='bobyqa', optCtrl = list(maxfun=20000)), 
                  data=myPP, family=binomial) 
</code></pre>

<p><code>DV</code> is the binary response variable</p>

<p><code>IV1.d</code> and <code>IV2.d</code> are two within-subjects within-items categorical predictors, two levels each, deviation-contrast coded (values: -.5/.5)</p>

<p>I tried to compute confidence intervals for the beta parameters using profile likelihood via <code>confint.merMod()</code> but the computation seems to be failing.
For all betas, I got values <code>(-Inf Inf)</code> and warning messages of non-monotonic profiles. Reading on [R-sig-ME], this latter issue should mean there is something wonky with the profile. </p>

<p>I tried to simplify the random structure of the model until profile confidence intervals could be computed. Here is the model:</p>

<pre><code>fitted_4 &lt;- glmer(DV ~ IV1.d + IV2.d + IV1.d:IV2.d + (IV1.d + IV2.d| SubjN) +
                       (1|Items) + (0+IV1.d|Items) + (0 +IV2.d|Items), data=myPP, 
                  glmerControl(optimizer='bobyqa', optCtrl = list(maxfun=20000)),
                  family=binomial)
</code></pre>

<ol>
<li><p>I'm not understanding what causes the profile likelihood method to fail for the original <code>fitted_1</code> model but not for <code>fitted_4</code>. </p></li>
<li><p>Is there any other way I could obtain profile CI's for <code>fitted_1</code>?</p></li>
</ol>

<hr>

<pre><code>summary(fitted_1)

##       AIC      BIC   logLik deviance df.resid 
##    1074.0   1168.1   -519.0   1038.0     1362 

##  Scaled residuals: 
##      Min      1Q  Median      3Q     Max 
##  -2.2673 -0.3611 -0.2500 -0.1378  4.5826 

##  Random effects:
##   Groups  Name        Variance  Std.Dev.  Corr             
##   SubjN   (Intercept) 2.424e+00 1.557e+00                  
##           IV1.d       1.990e+00 1.411e+00  0.17            
##           IV2.d       6.065e-01 7.788e-01 -0.97 -0.29      
##           IV1.d:IV2.d 2.172e+00 1.474e+00 -0.19 -0.81  0.39
##   Items   (Intercept) 4.615e-03 6.793e-02                  
##   Items.1 IV1.d       3.233e-13 5.686e-07                  
##   Items.2 IV2.d       9.442e-01 9.717e-01                  
##   Items.3 IV1.d:IV2.d 4.801e-01 6.929e-01                  
##  Number of obs: 1380, groups:  SubjN, 88; Items, 12

##  Fixed effects:
##              Estimate Std. Error z value Pr(&gt;|z|)    
##  (Intercept) -2.40604    0.23196 -10.373   &lt;2e-16 ***
##  IV1.d        0.08249    0.36355   0.227   0.8205    
##  IV2.d        1.11046    0.43579   2.548   0.0108 *  
##  IV1.d:IV2.d  0.16386    0.71246   0.230   0.8181    

##  Correlation of Fixed Effects:
##              (Intr) IV1.d  IV2.d 
##  IV1.d        0.118              
##  IV2.d       -0.434 -0.083       
##  IV1.d:IV2.d -0.090 -0.628  0.064
</code></pre>

<p></p>

<pre><code>confint(fitted_1, method=""profile"", which='beta_')`

##               2.5 % 97.5 %
##   (Intercept)  -Inf    Inf
##   IV1.d        -Inf    Inf
##   IV2.d        -Inf    Inf
##   IV1.d:IV2.d  -Inf    Inf

##  Warning messages:
##  1: In profile.merMod(object, signames = oldNames, ...) :
##    non-monotonic profile
##  2: In profile.merMod(object, signames = oldNames, ...) :
##    non-monotonic profile
##  3: In profile.merMod(object, signames = oldNames, ...) :
##    non-monotonic profile
##  4: In profile.merMod(object, signames = oldNames, ...) :
##    non-monotonic profile
</code></pre>

<p></p>

<pre><code>summary(fitted_4)

##       AIC      BIC   logLik deviance df.resid 
##      1068     1136     -521     1042     1367 

##  Scaled residuals: 
##      Min      1Q  Median      3Q     Max 
##  -2.3575 -0.3555 -0.2522 -0.1613  4.6391 

##  Random effects:
##   Groups  Name        Variance Std.Dev. Corr       
##   SubjN   (Intercept) 2.23144  1.4938              
##           IV1.d       1.53606  1.2394    0.09      
##           IV2.d       0.31120  0.5579   -1.00 -0.18
##   Items   (Intercept) 0.01344  0.1159              
##   Items.1 IV1.d       0.00000  0.0000              
##   Items.2 IV2.d       0.92942  0.9641              
##  Number of obs: 1380, groups:  SubjN, 88; Items, 12

##  Fixed effects:
##              Estimate Std. Error z value Pr(&gt;|z|)    
##  (Intercept) -2.30252    0.21029 -10.949   &lt;2e-16 ***
##  IV1.d        0.17448    0.27729   0.629   0.5292    
##  IV2.d        0.80072    0.36862   2.172   0.0298 *  
##  IV1.d:IV2.d  0.01351    0.41660   0.032   0.9741    

##  Correlation of Fixed Effects:
##              (Intr) IV1.d  IV2.d 
##  IV1.d        0.012              
##  IV2.d       -0.274 -0.010       
##  IV1.d:IV2.d  0.006 -0.255 -0.038
</code></pre>

<p></p>

<pre><code>confint(fitted_4, which='beta_', method='profile')

##                    2.5 %     97.5 %
##  (Intercept) -2.74571641 -1.9052351
##  IV1.d       -0.37989551  0.7320931
##  IV2.d        0.03993436  1.5903197
##  IV1.d:IV2.d -0.80790153  0.8346440
</code></pre>

<hr>

<h2>UPDATE</h2>

<pre><code>## re-compute profiles for both random and fixed effects

pp &lt;- profile(fitted_1)

## 24 warnings with profile(fitted_1) of the types:
## In profile.merMod(fitted_1) : non-monotonic profile
## In optwrap(optimizer, par = start, fn = function(x) dd(mkpar(npar1,  ... :
   # convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded

(c_ci &lt;- confint(pp))

## 2.5 % 97.5 %
## .sig01          0    Inf
## .sig02         -1      1
## .sig03         -1      1
## .sig04         -1      1
## .sig05          0    Inf
## .sig06         -1      1
## .sig07         -1      1
## .sig08          0    Inf
## .sig09         -1      1
## .sig10          0    Inf
## .sig11          0    Inf
## .sig12          0    Inf
## .sig13          0    Inf
## .sig14          0    Inf
## (Intercept)  -Inf    Inf
## IV1.d        -Inf    Inf
## IV2.d        -Inf    Inf
## IV1.d:IV2.d  -Inf    Inf


## plot the profiles (all weird)
    ggplot(as.data.frame(pp),aes(.focal,.zeta))+
    geom_point()+geom_line()+
    facet_wrap(~.par,scale=""free_x"")+
    geom_hline(yintercept=0,colour=""gray"")+
    geom_hline(yintercept=c(-1.96,1.96),linetype=2,
               colour=""gray"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/IUtJx.jpg"" alt=""Plot_profile_fitted1.jpeg""></p>

<pre><code>### setting delta to a smaller value to make the profile stepsize smaller
system.time(pp2 &lt;- profile(fitted_1, delta = 0.1))

## user    system   elapsed 
## 64292.282   135.451 75676.403

## Warning messages:
## 1: In profile.merMod(orig.pp, delta = 0.1) : non-monotonic profile
## 2: display list redraw incomplete
## [...]

c_ci2 &lt;- confint(pp2)
c_ci2

## 2.5 % 97.5 %
## .sig01          0    Inf
## .sig02         -1      1
## .sig03         -1      1
## .sig04         -1      1
## .sig05          0    Inf
## .sig06         -1      1
## .sig07         -1      1
## .sig08          0    Inf
## .sig09         -1      1
## .sig10          0    Inf
## .sig11          0    Inf
## .sig12          0    Inf
## .sig13          0    Inf
## .sig14          0    Inf
## (Intercept)  -Inf    Inf
## IV1.d        -Inf    Inf
## IV2.d        -Inf    Inf
## IV1.d:IV2.d  -Inf    Inf


## plot of profiles (delta = 0.1)

ggplot(as.data.frame(pp2),aes(.focal,.zeta))+
    geom_point()+geom_line()+
    facet_wrap(~.par,scale=""free_x"")+
    geom_hline(yintercept=0,colour=""gray"")+
    geom_hline(yintercept=c(-1.96,1.96),linetype=2,
               colour=""gray"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/olzm2.jpg"" alt=""Plot of profiles delta = 0.1""></p>
"
"0.141705050316284","0.115832598070152","120768","<p>I'm using <code>glmer()</code> with a binomial response variable. My optimal model has two fixed effects (flow and DNA) which in summary() show a non-significant p value but when I remove each fixed effect in turn from the model the likelihood ratio test comparing the two models shows a significant p value. I'm struggling to understand (1) if this is normal, and (2) how to report the results if the explanatory variables ""flow"" and ""DNA"" are important but their p values in the model are well above 0.05?</p>

<p>Optimal model:</p>

<pre><code>a25 &lt;- glmer(Status_qpcr~(1|Root)+Flow+DNA,
             family=binomial, data=spore)
summary(a25)

Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['glmerMod']  
Family: binomial  ( logit ) 
Formula: Status_qpcr ~ (1 | Root) + Flow + DNA   
Data: spore
      AIC      BIC   logLik deviance df.resid 
     72.9     81.0    -32.4     64.9       52 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.9318 -0.8163  0.4435  0.6848  1.6133 

Random effects:  
  Groups Name        Variance Std.Dev.  
  Root   (Intercept) 0.3842   0.6199   
  Number of obs: 56, groups:  Root, 9

Fixed effects:
Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -0.97752    0.79252  -1.233    0.217   
Flow         3.82779    2.27165   1.685    0.092 . 
DNA          0.01616    0.01039   1.556    0.120  
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr) Flow   Flow -0.775        
     DNA    -0.576  0.227
</code></pre>

<p>Likelihood ratio test:</p>

<pre><code>a26 &lt;- update(a25,~.-DNA)
anova(a25,a26)

Data: spore 
Models: 
    a26: Status_qpcr ~ (1 | Root) + Flow 
    a25: Status_qpcr ~ (1 | Root) + Flow + DNA
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
a26  3 74.802 80.878 -34.401   68.802                            
a25  4 72.897 80.998 -32.448   64.897 3.9049      1    0.04815 *

a27 &lt;- update(a25,~.-Flow)
anova(a25,a27)

Data: spore 
Models: 
    a27: Status_qpcr ~ (1 | Root) + DNA 
    a25: Status_qpcr ~ (1 | Root) + Flow + DNA
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
a27  3 78.440 84.723 -36.220   72.440                             
a25  4 72.897 80.998 -32.448   64.897 7.5427      1   0.006025 **
</code></pre>
"
"0.0448110714948221","0.045786854649563","121285","<p>I'm starting with <code>lme4</code> and GLMM. Maybe this question can be basic for experimented researchers, but I'm still learning.</p>

<p>I have a pooled data where every observation is independent for every year. Will it be correct consider time random effect in a GLMM? Or maybe, Should I use other kind of models?</p>

<p>The model is specified as follow:</p>

<pre><code>model &lt;- glmer(response ~ V1 + V2 + (1|TIME), family = binomial, data =data))
</code></pre>

<p>Thanks for any help, references are welcome.</p>
"
"NaN","NaN","121661","<p>I've got two models (all variable are count variables):</p>

<pre><code>frm.ct &lt;- glmer(frm ~ age + education + socialrole +
              offset(log(words)) + (1|subkorpus), family=negative.binomial(1), 
data=daten.alle.kom)

frm.oage &lt;- glmer(frm ~ education + socialrole +
              offset(log(words)) + (1|subkorpus), family=negative.binomial(1), 
data=daten.alle.kom)
</code></pre>

<p>I used this to compare them:</p>

<pre><code>anova(frm.ct, frm.oage)
</code></pre>

<p><img src=""http://i.stack.imgur.com/xb6Jm.png"" alt=""enter image description here""></p>

<p>AIC values tell me that <code>frm.oage</code> is the better model, right? but what do 0.0452 and 0.8315 mean?</p>
"
"0.184760780970256","0.177679094574072","122336","<p>I have a problem with coding of a 2-level categorical predictor variable in R, and subsequently using it as a random slope in lmer().</p>

<p>I can keep the factor as numeric, coded using the treatment coding:</p>

<pre><code>&gt; unique (b$multi)
[1] 0 1
</code></pre>

<p>Running lmer() using a dataset coded in this way yields:</p>

<pre><code>&gt; l1 = glmer(OK ~ multi + (0 + multi|item) + (1|subject)+ (1|item), family=""binomial"", data=b)
&gt; summary(l1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: OK ~ multi + (0 + multi | item) + (1 | subject) + (1 | item)
   Data: b

     AIC      BIC   logLik deviance df.resid 
  4806.5   4838.9  -2398.3   4796.5     4792 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-7.8294 -0.5560 -0.1548  0.5623 14.3342 

Random effects:
 Groups  Name        Variance Std.Dev.
 subject (Intercept) 1.84379  1.3579  
 item    (Intercept) 2.40306  1.5502  
 item.1  multi       0.04145  0.2036  
Number of obs: 4797, groups:  subject, 123; item, 39
[...]
</code></pre>

<p>Above there is only one random slope related to <code>multi</code>. However, something very different happens when I convert the variable into a factor:</p>

<pre><code>&gt; b$multi = as.factor(b$multi)
&gt; levels (b$multi)
[1] ""0"" ""1""
</code></pre>

<p>When I fit a model using <code>multi</code> as a random slope variable:</p>

<blockquote>
  <p>l2 = glmer(OK ~ multi + (0+multi|item) + (1|subject)+ (1|item), family=""binomial"", data=b)
      Warning message:
      In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
        Model failed to converge: degenerate  Hessian with 1 negative eigenvalues</p>
</blockquote>

<p>... the model fails to converge and I get a very different random effects structure:</p>

<pre><code>&gt; summary(l2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: OK ~ multi + (0 + multi | item) + (1 | subject) + (1 | item)
   Data: b

     AIC      BIC   logLik deviance df.resid 
  4807.8   4853.1  -2396.9   4793.8     4790 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-8.3636 -0.5608 -0.1540  0.5627 15.2515 

Random effects:
 Groups  Name        Variance Std.Dev. Corr
 subject (Intercept) 1.8375   1.3555       
 item    (Intercept) 0.9659   0.9828       
 item.1  multi0      1.5973   1.2638       
         multi1      1.0224   1.0111   1.00
Number of obs: 4797, groups:  subject, 123; item, 39
[...]
</code></pre>

<p>The number of parameters in the model clearly change (reflected by the change in AIC, etc.), and I get two random slopes. </p>

<p>My question is which way of coding the categorical variable is better? Intuition tells me that it is the first one, but I have seen recommendations for both ways of coding in various tutorials and classes about running GLMMs in R and this is why it baffles me. Both types of the predictor variable work identically in ordinary regression using lm().</p>
"
"0.100200602007025","0.102382519472325","122419","<p>I have a dataset with count dependent variables such as <code>greetings</code> and ordinal independent variables like <code>education</code> and <code>social</code> etc., but <code>education</code> has some zero values (which are represented by <code>NA</code>). Now I'm doing a variable selection with <code>glmer</code>. my question is, is it necessary to make a new data set without the <code>NA</code>s (which means with fewer observations for the whole dataset) when I compare the models with and without <code>education</code>? because the AIC values are different from the old and new dataset.</p>

<p>old dataset with <code>education</code></p>

<pre><code>model1 &lt;- glmer(greetings ~ education + socialrole + countedmembers + topic +
                 offset(log(words)) + (1|people), family=negative.binomial(1), 
            data=dat.old)####AIC 1119.0
</code></pre>

<p>without <code>education</code></p>

<pre><code>model2 &lt;- glmer(greetings ~ socialrole + countedmembers + topic +
                 offset(log(words)) + (1|people), family=negative.binomial(1), 
            data=dat.old)####AIC 1182.0
</code></pre>

<p>new dataset with <code>education</code>:</p>

<pre><code>model3 &lt;- glmer(greetings ~ education + socialrole + countedmembers + topic +
                 offset(log(words)) + (1|people), family=negative.binomial(1), 
            data=dat.new)###AIC 1119.0
</code></pre>

<p>without <code>education</code>:</p>

<pre><code>model4 &lt;- glmer(greetings ~ socialrole + countedmembers + topic +
                 offset(log(words)) + (1|people), family=negative.binomial(1), 
            data=dat.new)###AIC 1117.3
</code></pre>

<p>thanks for any help!</p>
"
"NaN","NaN","122508","<p>I am trying to replicate a multilevel logistic analysis which uses a dyadic time series data set and R. As for my part, I am using the same data set but Stata.</p>

<p>The original syntax has the following specifications:</p>

<p><code>lmer(depvar ~ indepvar_n (1|country_a)+(1|country_b)+(1|year), data=d, family='binomial'))</code></p>

<p>My Stata syntax currently looks like this:</p>

<p><code>xtmelogit depvar indepvars || country_a:country_b</code></p>

<p>Obviously, somehow I still have to add the <strong>year</strong> variable, but I have troubles in doing so. When excluding years from both equations I produce identical results. However, this is pointless since it is a different analysis.</p>

<p>In which way do I have to modify the Stata syntax to produce the same results which are computed by the R command? I cannot simply add :year after country_b since this is not possible. What am I missing?</p>
"
"0.0896221429896442","0.091573709299126","127298","<p>I am currently using a mixed binomial model with the following specification in a paper I recently submitted (using <code>lme4</code>):</p>

<pre><code>m1&lt;-glmer(y~X1*X2*X3+(1|Subject.ID),data=data,family=""binomial"")
</code></pre>

<p><code>X1</code> and <code>X2</code> are binary predictors</p>

<p>However, a reviewer has asked me to also include a random-effects probit model for the same analysis. What is the difference between a mixed effects model and a random effects model? Am I correct in thinking that a random-effects model would have no fixed effects and would thus be specified by:</p>

<pre><code>m1&lt;-glmer(y~1+(1|Subject.ID)+(1|X1)+(1|X2)+(1|X1:X2),data=data,family=binomial(link=probit))
</code></pre>

<p>If so how do I interpret the output. All I get is the Std.Dev. for each random effect.</p>

<p>Alternatively does it refer to a model where I would have a random slope for each of my predictors like:</p>

<pre><code>m1&lt;-glmer(y~1+(X1|Subject.ID)+(X2|Subject.ID)+(X1:X2|Subject.ID),data=data,family=binomial(link=probit))
</code></pre>
"
"0.261291202242781","0.266980946877876","127479","<p>I'm using a mixed effects model with logistic link function (using lme4 version 1.1-7 in R).  However, I noticed that the estimates of significance for fixed effects change depending on the order of the rows in the dataset.  </p>

<p>That is, if I run a model on a dataset, I get certain estimate for my fixed effect and it has a certain p-value.  I run the model again, and I get the same estimate and p-value.  Now, I shuffle the order of rows (the data is not mixed, just the rows are in a different order).  Running the model a third time, the p-value is very different.</p>

<p>For the data I have, the estimated p-value for the fixed effect can be between p=0.001 and p=0.08.  Obviously, these are crucial differences given conventional significance levels. </p>

<p>I understand that the estimates are just estimated, and there will be differences between values for a number of reasons.  However, the magnitude of the differences for my data seem large to me, and I wouldn't expect the order of my dataframe to have this effect (we discovered this problem by chance when a colleague ran the same model but got different results.  It turned out they had ordered their data frame.).  </p>

<p>Here is the output of my script:
(X and Y are binary variables which are contrast-coded and centred, Group and SubGroup are categorical variables)</p>

<pre><code>&gt; # Fit model
&gt; m1 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; # Shuffle order of rows
&gt; d = d[sample(1:nrow(d)),]
&gt; # Fit model again
&gt; m2 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; summary(m1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5421        
              Y1          0.1847   0.4298   -0.79
 Group        (Intercept) 0.2829   0.5319        
              Y1          0.4640   0.6812   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1325  -8.214   &lt;2e-16 ***
Y1            0.3772     0.2123   1.777   0.0756 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.112 
&gt;
&gt; # -----------------
&gt; summary(m2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5422        
              Y1          0.1846   0.4296   -0.79
 Group        (Intercept) 0.2829   0.5318        
              Y1          0.4641   0.6813   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1166  -9.334  &lt; 2e-16 ***
Y1            0.3773     0.1130   3.339 0.000841 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.074 
</code></pre>

<p>I'm afraid that I can't attach the data due to privacy reasons. </p>

<p>Both models converge.  The difference appears to be in the standard errors, while the differences in coefficient estimates are smaller.  The model fit (AIC etc.) are the same, so maybe there are multiple optimal convergences, and the order of the data pushes the optimiser into different ones.  However, I get slightly different estimates every time I shuffle the data frame (not just two or three unique estimates).  In one case (not shown above), the model did not converge simply because of a shuffling of the rows.</p>

<p>I suspect that the problem lies with the structure of my particular data.  It's reasonably large (nearly 200,000 cases), and has nested random effects.  I have tried centering the data, using contrast coding and feeding starting values to lmer based on a previous fit.  This seems to help somewhat, but I still get reasonably large differences in p-values.  I also tried using different ways of calculating p-values, but I got the same problem.</p>

<p>Below, I've tried to replicate this problem with synthesised data.  The differences here aren't as big as with my real data, but it gives an idea of the problem.</p>

<pre><code>library(lme4)
set.seed(999)

# make a somewhat complex data frame
x = c(rnorm(10000),rnorm(10000,0.1))
x = sample(x)
y = jitter(x,amount=10)
a = rep(1:20,length.out=length(x))
y[a==1] = jitter(y[a==1],amount=3)
y[a==2] = jitter(x[a==2],amount=1)
y[a&gt;3 &amp; a&lt;6] = rnorm(sum(a&gt;3 &amp; a&lt;6))
# convert to binary variables
y = y &gt;0
x = x &gt;0
# make a data frame
d = data.frame(x1=x,y1=y,a1=a)

# run model 
m1 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# shuffle order of rows
d = d[sample(nrow(d)),]

# run model again
m2 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# show output
summary(m1)
summary(m2)
</code></pre>

<p>One solution to this is to run the model multiple times with different row orders, and report the range of p-values.  However, this seems inelegant and potentially quite confusing.</p>

<p>The problem does not affect model comparison estimates (using anova), since these are based on differences in model fit.  The fixed effect coefficient estimates are also reasonably robust.  Therefore, I could just report the effect size, confidence intervals and the p-value from a model comparison with a null model, rather than the p-values from within the main model.</p>

<p>Anyway, has anyone else had this problem?  Any advice on how to proceed?</p>
"
"0.118558951157635","0.121140630718605","128750","<p>I am running a glmer model and I want to determine the total variance. My data is for survival and it is coded as 0 and 1, where 1 represents that the individual survived and 0 represents that the individual died. My data represents offspring from a full factorial cross where some individuals are full sibs or half sibs. </p>

<p>When running a glmer model, and there is no residual variance in the summary output. I have read that the residual variance should be (Ï€^2)/3 for generalized linear mixed models with binomial data and logit link function (Nakagawa, S., Schielzeth, H. 2010. Repeatability for Gaussian and non-Gaussian data: a practical guide for biologists. Biol. Rev. 85:935-956.).</p>

<p>Is this true? Or is there a different way to calculate the residual variance for glmer?</p>

<p>Here is my model and output:</p>

<pre><code>model6 = glmer(X09.Nov~(1|Dam)+(1|Sire)+(1|Sire:Dam), family=binomial, data=data)
summary(model6) 

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation 
      [glmerMod]
 Family: binomial  ( logit )
Formula: X09.Nov ~ (1 | Dam) + (1 | Sire) + (1 | Sire:Dam)
   Data: data

    AIC      BIC   logLik deviance df.resid 
 1274.4   1295.3   -633.2   1266.4     1375 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2747  0.3366  0.3931  0.4664  1.1090 

Random effects:
Groups   Name        Variance  Std.Dev. 
Sire:Dam (Intercept) 3.853e-01 6.207e-01
Sire     (Intercept) 4.181e-02 2.045e-01
Dam      (Intercept) 6.036e-09 7.769e-05
Number of obs: 1379, groups:  Sire:Dam, 49; Sire, 7; Dam, 7
Fixed effects:
            Estimate Std. Error z value     Pr    
(Intercept)   1.6456     0.1419    11.6 &lt;2e-16 *
</code></pre>
"
"0.0896221429896442","0.091573709299126","129439","<p>I am very new to R. I am using R Studio 3.1.2 for windows.</p>

<p>I am trying to run the following analysis on some data about geese behaviour at different sites over time. I have managed to get the model to work fine on my supervisors computer, yet when I try and run the model on either of my computers, it gives me the warning message:</p>

<pre><code>model2Av&lt;-glmer(cbind(Sum.of.vigilant , Sum.of.non.vigilant) ~ site + 
          group.size.start + (1|daysite), family = binomial, data=beforeAv )
</code></pre>

<p>Warning message:</p>

<blockquote>
  <p>In checkConv(attr(opt, ""derivs""), opt\$par, ctrl = control$checkConv, 
  :</p>
  
  <p>Model is nearly unidentifiable: very large eigenvalue</p>
</blockquote>

<ul>
<li>Rescale variables?</li>
</ul>

<p>when I run the code with uid (unique ID number) as a random variable still present the model runs...</p>

<pre><code>model2Av&lt;-glmer(cbind(Sum.of.vigilant , Sum.of.non.vigilant) ~ site + 
          group.size.start + (1|uid) + (1|daysite), family = binomial, data=beforeAv )
</code></pre>

<p>â€‹I cant quite make sense of the google answers to similar problems, but I am sure I am doing something obviously daft!â€‹ It seems odd that the model runs with exctly the same data and script on one computer, and not on another....</p>

<p>Thanks in advance.</p>
"
"0.200401204014051","0.20476503894465","130313","<p>In a logistic Generalized Linear Mixed Model (family = binomial), I don't know how to interpret the random effects variance:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev.
 HOSPITAL (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14
</code></pre>

<p>How do I interpret this numerical result?</p>

<p>I have a sample of renal trasplanted patients in a multicenter study. I was testing if the probability of a patient being treated with a specific antihypertensive treatment is the same among centers. The proportion of patients treated varies greatly between centers, but may be due to differences in basal characteristics of the patients. So I estimated a generalized linear mixed model (logistic), adjusting for the principal features of the patiens.
This are the results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: HTATTO ~ AGE + SEX + BMI + INMUNOTTO + log(SCR) + log(PROTEINUR) + (1 | CENTER) 
   Data: DATOS 

     AIC      BIC   logLik deviance 
1815.888 1867.456 -898.944 1797.888 

Random effects:
 Groups   Name        Variance Std.Dev.
 CENTER (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14

Fixed effects:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)               -1.804469   0.216661  -8.329  &lt; 2e-16 ***
AGE                       -0.007282   0.004773  -1.526  0.12712    
SEXFemale                 -0.127849   0.134732  -0.949  0.34267    
BMI                        0.015358   0.014521   1.058  0.29021    
INMUNOTTOB                 0.031134   0.142988   0.218  0.82763    
INMUNOTTOC                -0.152468   0.317454  -0.480  0.63102    
log(SCR)                   0.001744   0.195482   0.009  0.99288    
log(PROTEINUR)             0.253084   0.088111   2.872  0.00407 ** 
</code></pre>

<p>The quantitative variables are centered.
I know that the among-hospital standard deviation of the intercept is 0.6554, in log-odds scale.
Because the intercept is -1.804469, in log-odds scale, then probability of being treated with the antihypertensive of a man, of average age, with average value in all variables and inmuno treatment A, for an ""average"" center, is 14.1 %.
And now begins the interpretation:  under the assumption that the random effects follow a normal distribution, we would expect approximately 95% of centers to have a value within 2 standard deviations of the mean of zero, so the probability of being treated for the average man will vary between centers with coverage interval of:</p>

<pre><code>exp(-1.804469-2*0.6554)/(1+exp(-1.804469-2*0.6554))

exp(-1.804469+2*0.6554)/(1+exp(-1.804469+2*0.6554))
</code></pre>

<p>Is this correct?</p>

<p>Also, how can I test in glmer if the variability between centers is statistically significant?
I used to work with MIXNO, an excellent software of Donald Hedeker, and there I have an standard error of the estimate variance, that I don't have in glmer.
How can I have the probability of being treated for the ""average"" man in each center, with a confidene interval?</p>

<p>Thanks</p>
"
"0.152155937269623","0.144364264341433","130330","<p>I'm working with the <code>lme4</code> package in <code>R</code>, looking to fit GLMM models. For the proprietary data I'm working with, my dataset can often be quite large. When dealing with bernoulli response variables, I'm in the habit of aggregating my data (see sample code). I'm wondering how this effects the model algorithm. What are the affects of ""lowering the number of observations"" to below the number of random effects? If I can fit the model on non-aggregated data, shouldn't I be able to fit it on the aggregated data? So why am I able to fit <code>model</code>, but not <code>model_agg</code>? I implement a ""hacky"" ""solution"" in the code below - is this a permissible thing to do? </p>

<p>Note: I'm not too worried about the warning message at the end of the code block - this isn't the data I'm working with. I AM interested in understanding the ""number of observations"" error. </p>

<pre><code>library(dplyr)
library(lme4)
hdp &lt;- read.csv('http://www.ats.ucla.edu/stat/data/hdp.csv')
head(hdp)
#   DID CancerStage remission
# 1   1          II         0
# 2   1          II         0
# 3   1          II         0
# 4   1           I         0
# 5   1          II         0
# 6   1           I         0


model &lt;- glmer(remission ~ 1 + CancerStage + (1 + CancerStage | DID), data = hdp, 
           family = 'binomial', control = glmerControl(optimizer = 'bobyqa'))

hdp_agg &lt;- hdp %&gt;% 
    group_by(DID, CancerStage) %&gt;% 
    summarize(y   = sum(remission), 
              num = n())
head(hdp_agg)
# Source: local data frame [6 x 4]
# Groups: DID
# 
#   DID CancerStage y num
# 1   1           I 0   9
# 2   1          II 0  16
# 3   1         III 0   2
# 4   1          IV 0   1
# 5   2           I 6  13
# 6   2          II 4  11


model_agg &lt;- glmer(cbind(y, num - y) ~ 1 + CancerStage + (1 + CancerStage | DID), data = hdp_agg, 
                family = 'binomial', control = glmerControl(optimizer = 'bobyqa'))
# Error: number of observations (=1507) &lt; number of random effects (=1628) for term (1 + CancerStage | DID); 
# the random-effects parameters are probably unidentifiable

hdp_agg2 &lt;- with(hdp_agg, as.tbl(
    data.frame(DID         = rep(unique(DID), each = n_distinct(CancerStage)), 
               CancerStage = rep(unique(CancerStage), times = n_distinct(DID))))) %&gt;% 
    left_join(hdp_agg, by = c('DID', 'CancerStage')) %&gt;%
    mutate(y   = ifelse(is.na(y), 0, y), 
           num = ifelse(is.na(num), 0, num))

model_agg &lt;- glmer(cbind(y, num - y) ~ 1 + CancerStage + (1 + CancerStage | DID), data = hdp_agg2, 
                   family = 'binomial', control = glmerControl(optimizer = 'bobyqa'))
# Warning messages:
# 1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
#   Model failed to converge with max|grad| = 0.00275742 (tol = 0.001, component 1)
# 2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
#   Model is nearly unidentifiable: very large eigenvalue
#  - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio
#  - Rescale variables?
</code></pre>
"
"0.0448110714948221","0.045786854649563","130707","<p>I have a question regarding covariates in a GLMM. My model comprises a condition variable and a covariate. Crucially, my binomially distributed dependent variable can be interpreted only in dependency of the covariate (the data is from a psychophysics study) and I am looking at how the relationship between responses and covariate change with condition. My model looks like this:</p>

<pre><code>glmer(DV ~ covariate*condition + (condition*covariate|subjects), data=data, 
      family=""binomial""(link=""logit""), control=glmerControl(optimizer=""bobyqa""))
</code></pre>

<p>My question is: Is there anything speaking against including a covariate as random effect? </p>
"
"0.167667676667856","0.171318722916682","132677","<p>I am running a glmm with a binomial response variable and a categorical predictor. The random effect is given by the nested design used for the data collection. The data looks like this: </p>

<pre><code>m.gen1$treatment
 [1] sucrose      control      protein      control      no_injection .....
Levels: no_injection control sucrose protein
m.gen1$emergence 
 [1]  1  0  0  1  0  1  1  1  1  1  1  0  0....
&gt; m.gen1$nest
 [1] 1  1  1  2  2  3  3  3  3  4  4  4  .....
Levels: 1 2 3 4 5 6 8 10 11 13 15 16 17 18 20 22 24
</code></pre>

<p>The first model I run looks like this </p>

<p><code>m.glmm.em.&lt;-glmer(emergence~treatment + (1|nest),family=binomial,data=m.gen1)</code></p>

<p>I get two warnings that look like this:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0240654 (tol = 0.001, component 4)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>The model summary shows that one of the treatments has a unusually large standard error, which you can see here:</p>

<pre><code>Fixed effects:
                 Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)         2.565      1.038   2.472   0.0134 *
treatmentcontrol   -1.718      1.246  -1.378   0.1681  
treatmentsucrose   16.863   2048.000   0.008   0.9934  
treatmentprotein   -1.718      1.246  -1.378   0.1681 
</code></pre>

<p>I tried the different optimizers from glmer control and functions from other packages, and I get a similar output. I have run the model using glm ignoring the random effect, and the problem persist. While exploring the data I realized that the treatment with a high Std. error has only successes in the response variable. Just to check whether that could be causing the problem I added a fake data point with a ""failure"" for that treatment and the model runs smoothly, and gives reasonable standard error. You can see that here:</p>

<pre><code>Fixed effects:
                 Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)        3.4090     1.6712   2.040   0.0414 *
treatmentcontrol  -1.8405     1.4290  -1.288   0.1978  
treatmentsucrose  -0.2582     1.6263  -0.159   0.8738  
treatmentprotein  -2.6530     1.5904  -1.668   0.0953 .
</code></pre>

<p>I was wondering if my intuition is right about the lack of failures for that treatment preventing a good estimation, and how can I work around this issue.</p>

<p>Thanks in advance!</p>
"
"0.141705050316284","0.14479074758769","132841","<p>TL;DR: <code>lme4</code> optimization appears to be linear in the number of model parameters by default, and is <em>way</em> slower than an equivalent <code>glm</code> model with dummy variables for groups. Is there anything I can do to speed it up?</p>

<hr>

<p>I'm trying to fit a fairly large hierarchical logit model (~50k rows, 100 columns, 50 groups). Fitting a normal logit model to the data (with dummy variables for group) works fine, but the hierarchical model appears to be getting stuck: the first optimization phase completes fine, but the second goes through a lot of iterations without anything changing and without stopping.</p>

<p><strong>EDIT:</strong> I suspect the problem is mainly that I have so many parameters, because when I try to set <code>maxfn</code> to a lower value it gives a warning:</p>

<pre><code>Warning message:
In commonArgs(par, fn, control, environment()) :
  maxfun &lt; 10 * length(par)^2 is not recommended.
</code></pre>

<p>However, the parameter estimates aren't changing at all over the course of the optimization, so I'm still confused about what to do. When I tried to set <code>maxfn</code> in the optimizer controls (despite the warning), it seemed to hang after finishing the optimization.</p>

<p>Here's some code that reproduces the problem for random data:</p>

<pre><code>library(lme4)

set.seed(1)

SIZE &lt;- 50000
NGRP &lt;- 50
NCOL &lt;- 100

test.case &lt;- data.frame(i=1:SIZE)
test.case[[""grouping""]] &lt;- sample(NGRP, size=SIZE, replace=TRUE, prob=1/(1:NGRP))
test.case[[""y""]] &lt;- sample(c(0, 1), size=SIZE, replace=TRUE, prob=c(0.05, 0.95))

test.formula = y ~ (1 | grouping)

for (i in 1:NCOL) {
    colname &lt;- paste(""col"", i, sep="""")
    test.case[[colname]] &lt;- runif(SIZE)
    test.formula &lt;- update.formula(test.formula, as.formula(paste("". ~ . +"", colname)))
}

print(test.formula)

test.model &lt;- glmer(test.formula, data=test.case, family='binomial', verbose=TRUE)
</code></pre>

<p>This outputs:</p>

<pre><code>start par. =  1 fn =  19900.78 
At return
eval:  15 fn:      19769.402 par:  0.00000
(NM) 20: f = 19769.4 at           0     &lt;other numbers&gt;
(NM) 40: f = 19769.4 at           0     &lt;other numbers&gt;
</code></pre>

<p>I tried setting <code>ncol</code> to other values, and it appears that the number of iterations done is (approximately) 40 per column. Obviously, this becomes a huge pain as I add more columns. Are there tweaks I can make to the optimization algorithm that will reduce the dependence on the number of columns?</p>
"
"0.148621510602115","0.15185781720314","132971","<p>There is something I'm not quite understanding conceptually about the output from generalized linear mixed models. I have read that the target of inference in GLMMs is subject-specific. For example, the accepted answer to <a href=""http://stats.stackexchange.com/questions/17331/what-is-the-difference-between-generalized-estimating-equations-and-glmm"">this</a> question states that in a logistic GLMM the odds-ratios are conditioned on both the fixed and random effects. So, in a GLMM of pupils within classrooms, with random intercepts for classroom (i.e., the ""subject"" in this case), the odds-ratios will differ for each classroom as there will be many random intercepts. So far, this makes sense to me.</p>

<p>What I am confused about is that the typical output from the fixed effects part of such a model reports just one odds-ratio. For example, in the R example I provide below, the odds-ratio for the fixed effect of <code>sex</code> is .662. I have three questions:</p>

<ol>
<li><p><strong>How do I interpret this single fixed effect odds-ratio?</strong><br>
(Is it an odds-ratio ignoring the random effects? Is it an odds-ratio of the average random effect - in which case, isn't it a population average? Is it calculated assuming the random effect variance is zero?) </p></li>
<li><p><strong>Is it possible to calculate a population average odds-ratio using the output from a GLMM?</strong><br>
I know this can be done using a GEE, but what about a GLMM?</p></li>
<li><p><strong>How would I go about calculating the odds-ratio for a particular random effect (a particular classroom, lets say class 7 in the example below)?</strong><br>
Presumably this involves combining the fixed and random effect estimates somehow.</p></li>
</ol>

<p><strong>EDIT 1:</strong>
It seems after doing more reading (for example, this <a href=""http://stats.stackexchange.com/questions/32419/difference-between-generalized-linear-models-generalized-linear-mixed-models-i?lq=1"">post</a>), that since the fixed effect for <code>sex</code> in this example does not have its own random effect (e.g., a random slope), there will be no subject-level interpretation of this parameter. Does this mean that only the intercept term in the model below is subject-specific, while the <code>sex</code> term is a population average?</p>

<pre><code># dummy data:
set.seed(1)
dat &lt;- data.frame(Y         = factor(sample(rep(c(0, 1), 100))),
                  sex       = factor(sample(rep(c(""M"", ""F""), 100))),
                  classroom = factor(sample(rep(paste(""class"", 1:10), 20)))
) 

# model:
library(lme4)
fit &lt;- glmer(Y ~ sex + (1 | classroom), family=binomial, data=dat)

# summary(fit)
exp(fixef(fit))
# (Intercept)   sexM 
#  1.229       0.662 
</code></pre>
"
"0.142294263046161","0.158610317143629","133918","<p>I'm trying to estimate the confidence interval of a psychometric curve (binomial probit GLM), for a population (now only two subjects).</p>

<p>Suppose I've subject ""a"" and subject ""b"", which performs separately a task. The answers are recorded, and plotted against the stimulus level, giving us a sygmoidal curve.</p>

<p>Here is the data, and the code to plot the psychometric curve with its S.E.:</p>

<pre><code># data:
mydata &lt;- structure(list(exp_dur = c(3250L, 2850L, 2250L, 2450L, 3450L, 
3450L, 3050L, 3450L, 2450L, 2650L, 3250L, 2650L, 2850L, 3250L, 
3050L, 2250L, 2450L, 2650L, 2250L, 2850L, 3050L, 2850L, 3050L, 
2850L, 2250L, 2450L, 3450L, 3250L, 3450L, 2850L, 3050L, 2250L, 
2450L, 3050L, 3450L, 2250L, 3050L, 3250L, 2450L, 2650L, 3250L, 
2650L, 2850L, 2450L, 2650L, 3450L, 2250L, 3250L, 2650L, 3250L, 
2450L, 2650L, 2650L, 3450L, 3450L, 3250L, 2450L, 2650L, 2850L, 
3050L, 3050L, 3250L, 2250L, 3050L, 3450L, 2250L, 2450L, 2850L, 
2850L, 2250L, 2250L, 3050L, 2650L, 2650L, 2450L, 3250L, 3050L, 
2850L, 2850L, 3250L, 3250L, 3050L, 2450L, 3050L, 2450L, 3450L, 
3250L, 2250L, 3450L, 2250L, 3450L, 2250L, 3450L, 2850L, 2650L, 
2450L, 2850L, 2650L), response_key_exp_resp = structure(c(2L, 
2L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 
2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 
2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 
2L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 
1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 
1L), .Label = c(""c"", ""l""), class = ""factor""), lcorr = c(1, 1, 
0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 
1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 
0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 
0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 
1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0), subj = structure(c(1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L
), .Label = c(""a"", ""b""), class = ""factor"")), .Names = c(""exp_dur"", 
""response_key_exp_resp"", ""lcorr"", ""subj""), row.names = c(NA, 
98L), class = ""data.frame"")
</code></pre>

<p>For subject ""a"":</p>

<pre><code># subject a:
myfit.a &lt;- glm(lcorr~exp_dur, binomial(probit), mydata, subset=subj==""a"")
suba &lt;- mydata[which(mydata$subj==""a""),]
    sc1a &lt;- with(suba, tapply(lcorr, exp_dur, mean))
    plot(as.numeric(names(sc1a)), sc1a, log=""x"", xlab=""Contrast"", ylab=""P correct"", pch=21, cex=1.5)
    cnt &lt;- seq(2250, 3450, len=2000)
    Long.Pred.a &lt;- predict(myfit.a, newdata=data.frame(exp_dur=cnt), type=""response"", se.fit=T)
    polygon(c(cnt, rev(cnt)), c(Long.Pred.a$fit + Long.Pred.a$se.fit, rev(Long.Pred.a$fit - Long.Pred.a$se.fit)), border=""white"", col=""lightgrey"")
    lines(cnt, Long.Pred.a$fit, lwd=2)
points(as.numeric(names(sc1a)), sc1a, pch=21, cex=1.5)
</code></pre>

<p><img src=""http://i.stack.imgur.com/gZk6y.jpg"" alt=""subject a""></p>

<p>For subject ""b"":</p>

<pre><code># subject b:
myfit.b &lt;- glm(lcorr~exp_dur, binomial(probit), mydata, subset=subj==""b"")
subb &lt;- mydata[which(mydata$subj==""b""),]
    sc1b &lt;- with(subb, tapply(lcorr, exp_dur, mean))
    plot(as.numeric(names(sc1b)), sc1b, log=""x"", xlab=""Contrast"", ylab=""P correct"", pch=19, cex=1.5)
    cnt &lt;- seq(2250, 3450, len=2000)
    Long.Pred.b &lt;- predict(myfit.b, newdata=data.frame(exp_dur=cnt), type=""response"", se.fit=T)
    polygon(c(cnt, rev(cnt)), c(Long.Pred.b$fit + Long.Pred.b$se.fit, rev(Long.Pred.b$fit - Long.Pred.b$se.fit)), border=""white"", col=""lightgrey"")
    lines(cnt, Long.Pred.b$fit, lwd=2, lty=2)
points(as.numeric(names(sc1b)), sc1b, pch=19, cex=1.5)
</code></pre>

<p><img src=""http://i.stack.imgur.com/48GIH.jpg"" alt=""subject b""></p>

<p>Now the psychometric curve of the group (subjects a and b) whith its plotted SE is:</p>

<pre><code># subject a+b
myfit.t &lt;- glm(lcorr~exp_dur*subj, binomial(probit), mydata)
sc1t &lt;- with(mydata, tapply(lcorr, exp_dur, mean))
plot(as.numeric(names(sc1t)), sc1t, log=""x"", xlab=""Contrast"", ylab=""P correct"", cex=1.5)
cnt &lt;- seq(2250, 3450, len=2000)
Long.Pred.t &lt;- predict(myfit.t, newdata=data.frame(exp_dur=cnt), type=""response"", se.fit=T)
polygon(c(cnt, rev(cnt)), c(Long.Pred.b$fit + Long.Pred.b$se.fit, rev(Long.Pred.b$fit - Long.Pred.b$se.fit)), border=""white"", col=""lightgrey"")
lines(cnt, Long.Pred.t$fit, lwd=2, lty=3)
points(as.numeric(names(sc1t)), sc1t, cex=1.5)
</code></pre>

<p><img src=""http://i.stack.imgur.com/aU0AE.jpg"" alt=""subject a+b""></p>

<p>At a visual inspection, I don't think this is the confidence interval of the group. In fact, in the formula used, I did not specified that the data comes from 2 subject, and R interpret like this is a single subject.</p>

<p>So, what is the method to get the S.E. for the group in a GLM analysis?</p>

<p>I'm quite sure that the correct approach to this kind of problem is GLMM (<code>lme4</code> package), but I'm not familiar with it, and it seems to be hard to plot the predict data for <code>glmer</code> models, which would look like this:</p>

<pre><code>mod1 &lt;- glmer(lcorr ~ exp_dur + (1 + exp_dur|subj), family = binomial(link = ""probit""), data=mydata)
</code></pre>

<p>How can I plot the S.E. of the group' psychometric curve?</p>
"
"0.142294263046161","0.145392790714993","135255","<p>I aim to estimate the annual proportion of patients (% of patients) that are smokers in a population whose age and sex must be taken into account. In other words, I want to calculate the adjusted prevalence (%) of smoking each year. I have repeated measurements on the same individuals and want to model the individual as a random effect, which is why I use the lme4 package, more precisely the glmer function. The variable of main interest is ""year"" (period 1996 to 2014), which I need to model as a fixed effect.</p>

<p><strong>Aim:</strong> Obtain adjusted proportions (%) of smokers each year.</p>

<p>Suppose the data set is named ""df"" and the year variable is converted to a factor.</p>

<p>I tried this code (<em>generated with a slightly different data set than the attached one</em>) to fit the model:</p>

<pre><code>&gt; smoke &lt;- glmer(smoker ~ biomarker + year + sex + age + (1 | id), data
&gt; = df, family = binomial, nAGQ = 1)

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -6.201632   0.231582 -26.779  &lt; 2e-16 ***
biomarker        -0.015364   0.008299  -1.851  0.06413 .  
yuar1997           0.648292   0.212400   3.052  0.00227 ** 
yuar1998          -0.586996   0.227217  -2.583  0.00978 ** 
yuar1999          -1.194309   0.216907  -5.506 3.67e-08 ***
yuar2000          -0.999889   0.217536  -4.596 4.30e-06 ***
yuar2001          -0.884453   0.203351  -4.349 1.37e-05 ***
yuar2002          -0.777464   0.199151  -3.904 9.47e-05 ***
yuar2003          -0.961869   0.194723  -4.940 7.83e-07 ***
yuar2004          -1.755470   0.197157  -8.904  &lt; 2e-16 ***
yuar2005          -1.207833   0.189753  -6.365 1.95e-10 ***
yuar2006          -1.072532   0.187504  -5.720 1.07e-08 ***
yuar2007          -1.494477   0.189467  -7.888 3.08e-15 ***
yuar2008          -2.441916   0.191069 -12.780  &lt; 2e-16 ***
yuar2009          -1.881562   0.187321 -10.045  &lt; 2e-16 ***
yuar2010          -2.254924   0.187254 -12.042  &lt; 2e-16 ***
yuar2011          -1.634935   0.184929  -8.841  &lt; 2e-16 ***
yuar2012          -2.405588   0.187349 -12.840  &lt; 2e-16 ***
yuar2013          -2.119775   0.186729 -11.352  &lt; 2e-16 ***
yuar2014          -2.241768   0.210259 -10.662  &lt; 2e-16 ***
sex              -0.071377   0.115975  -0.615  0.53826    
age              -0.012897   0.008011  -1.610  0.10742 
</code></pre>

<p>Using the predict function to obtain probability of being a smoker in 2005:</p>

<pre><code>predict(smoke, data.frame(age=mean(df$age), year=""2005"", sex=mean(df$sex), biomarker=mean(df$biomarker, na.rm=T)), type=""response"", re.form = NA)
</code></pre>

<p>I obtain much too low probabilities of being a smoker a particular year:</p>

<pre><code>0.0002233488
</code></pre>

<p>The same is true when using the lsmeans and effects package. Figures should be around 5â€“15% smokers.</p>

<p><strong>In short</strong>, in the data set I'm aiming to obtain the proportions of smokers during different years, adjusted for differences in age, sex and the biomarker while accounting of repeated measurements.</p>

<p>I'd be extremely grateful for a solution to these problems.</p>
"
"0.118558951157635","0.121140630718605","135840","<p>I have a data set that I expect there to be some variability among individuals; therefore, I chose to include <code>ID</code> as a random effect in the <code>glmer</code> model. However, when I run the model I get the following warning: </p>

<pre><code>model.5 &lt;- glmer(R0A1 ~ Dist_MP + (1|ID), data=secondorder, family=binomial)

Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
</code></pre>

<p>If I remove the random effect then the warning doesn't appear; therefore, I would assume that there is not enough variability among individuals (<code>ID</code>) for a random effect to be needed. Would you remove the random effect and just run a glm model? Also, how does the <code>family=binomial</code> code model the 0's and 1's in a data set? Does it consider 1's as the event? </p>

<pre><code>Summary output from glmer model:

Generalized linear mixed model fit by maximum
  likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: R0A1 ~ Dist_MP + (1 | ID)
   Data: secondorder

     AIC      BIC   logLik deviance df.resid 
 39451.7  39476.5 -19722.8  39445.7    28693 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.0876 -1.0372  0.2758  0.9567  1.7543 

Random effects:
 Groups Name        Variance Std.Dev.
 ID     (Intercept) 0        0       
Number of obs: 28696, groups:  ID, 45

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.679e-01  1.505e-02   11.16   &lt;2e-16 ***
Dist_MP     -1.559e-03  8.771e-05  -17.77   &lt;2e-16 ***
---
Signif. codes:  
0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr)
Dist_MP -0.614
</code></pre>
"
"0.0601203612042152","0.102382519472325","138109","<p>I would like to obtain estimated $\theta$ from glmer.nb function in lme4 package. In my understanding this function fits the model:
$$
Y_{ij}|\boldsymbol{B}_{i}=\boldsymbol{b}_i \overset{ind.}{\sim} NB\Big(mean=\mu,var=\mu + \frac{\mu^2}{\theta}\Big)
$$
where $NB$ refers to the negative binomial distribution and:
$$
\mu = \exp(\boldsymbol{X}_{ij}^T \boldsymbol{\beta} + \boldsymbol{Z}_{ij}^T \boldsymbol{b}_i)
$$
and 
$$
\boldsymbol{B}_i \overset{i.i.d.}{\sim} MVN(mean=\boldsymbol{0},var=\Sigma).
$$
So glmer.nb must be estimating unknown parameters $\boldsymbol{\beta}$, $\Sigma$ and $\theta$ via maximizing its likelihood. The help file of glmer.nb little explains its functionality, and it says ""glmer() for Negative Binomial"". However, the negative binomial is NOT an exponential family when the parameter $\theta$ is unknown. So $\theta$ must be estimated in some other ways that generalized linear mixed effect models (GLMM) do not take, and the estimated $\theta$ must be obtained in some special ways that GLMM do not take. How can I access to the estimate of $\theta$ which should be one of glmer.nb output? </p>
"
"NaN","NaN","141582","<p>I tested whether different version-styles of a loading screen (hourglass vs. progress bar) in different progression patterns (linear, accelerate, decelerate, irregular, binary) affect time estimations within subjects.</p>

<p>By analyzing the data with a binomial linear mixed effects model I have found significant results for the interaction effect of ""versionStyle x DisplayDuration x progressionPattern"" I would like to run a post hoc analysis to test across which condition and Display duration the time estimation was significantly affected by the version Style""</p>

<p>This is the code I used for my analysis:</p>

<pre><code>(data1 &lt;- glmer(Long ~ DisplayDur * Pattern * Proggression+ (1 + DisplayDur + Pattern+ Progression| Subjects), dat=anadat,family=""binomial"",control=glmerControl(optimizer=""bobyqa"")))
</code></pre>

<p>Is there some command that I can use in R to do this?</p>
"
"0.134433214484466","0.137360563948689","143165","<p>I am attempting to build a model for the following dataset:</p>

<p>Level 1 Observations (Product-Level): 89000<br>
Level 2 Observations (""BU_SBU"" Department-Level): 135</p>

<p>Unfortunately I cannot share a sample of my data, since it is confidential.</p>

<p>The dependent variable in the model is a percentage (Delivery Reliability, 0-100%). Fixed effects include roughly 20 variables at level 1 and 5 variables at level 2. The only random effects are the intercepts at level 2. Having run the regression, I have a number of questions regarding the violation of model assumptions which I cannot answer myself:</p>

<ol>
<li><p>Constant variance of residuals: The graphic shows that there appears to be an upper- and lower-bound of the residuals. My guess is that this is due to the limitation of the dependent variable. But do the upper- and lower-bounds shown in the graphic actually indicate a violation of model assumptions? I have also run a GLMER model with a <code>binomial(logit)</code>-link but this did not resolve the issue. The diagnostic plots look almost identical in all three cases.</p></li>
<li><p>Distribution of residuals: Is there a way to compute confidence intervals for residual QQ-plots of LMER models? And is it possible to compute heteroskedasticity-robust standard errors via the lme4-package?</p></li>
<li><p>Normal distribution of level-2 intercepts: The level-2 intercepts do not appear to be Normally distributed. Is this an issue and if so, how can I resolve it?</p></li>
</ol>

<p>I would greatly appreciate, if someone could help me at least with some of these questions. I am currently stuck and was not able to find any resources that provide answers. I am also grateful for recommendations to helpful literature.  </p>

<p><img src=""http://i.stack.imgur.com/WK1NL.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/ZU4x5.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/GUgmq.png"" alt=""enter image description here""></p>

<p><a href=""https://www.dropbox.com/sh/dji9f95kg88i8kg/AACjQCTA4lW86aYJNa8CHaUha?dl=0"" rel=""nofollow"">Dropbox to Diagnostic Plots</a></p>
"
"0.152155937269623","0.155469207752313","143843","<p>Iâ€™m running a logit mixed-effects model on binary data with a 2x2 within-subjects design, with subjects and items as crossed random effects, and the two independent variables deviation-contrast coded.</p>

<p>Here are model specification and summary:</p>

<pre><code>mod1 &lt;- glmer(DV ~ devX1*devX2 + (devX1*devX2|Subject) + (devX1*devX2|Item), 
              data=mydata, family=binomial, glmerControl(optimizer='bobyqa', 
              optCtrl=list(maxfun=400000)))

     AIC      BIC   logLik deviance df.resid 
   628.9    734.3   -290.4    580.9      573 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0527 -0.5025 -0.2217  0.5654  4.0493 

Random effects:
 Groups  Name        Variance Std.Dev. Corr             
 Subject (Intercept) 0.1184   0.3440                    
         devX1       3.5387   1.8812   -0.74            
         devX2       0.2461   0.4961   -0.54  0.06      
         devX1:devX2 4.5912   2.1427    0.32 -0.84  0.07
 Item    (Intercept) 0.5568   0.7462                    
         devX1       0.2693   0.5190    0.48            
         devX2       0.3862   0.6215   -0.31 -0.51      
         devX1:devX2 2.2109   1.4869   -0.57  0.42 -0.31
Number of obs: 597, groups:  Subject, 30; Item, 20

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.47781    0.27602  -5.354 8.60e-08 ***
devX1        2.70622    0.55692   4.859 1.18e-06 ***
devX2        0.08229    0.45801   0.180    0.857    
devX1:devX2 -0.41055    0.99645  -0.412    0.680    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) devX1  devX2 
devX1       -0.498              
devX2       -0.179  0.046       
devX1:devX2 -0.021 -0.266 -0.657
</code></pre>

<p>The model doesÂ converge with full random structure without any problems. (It may be worth mentioning that the binned Pearson residual plot reveals that the model has some issues accounting for y = 0 original data points.)  </p>

<p>I'm encounteringÂ big convergence issues as soon as I include a centered continuous covariate (<code>Age</code>)Â asÂ fixed effect. It does not matter how much I simplify the random structure, the model will not converge.</p>

<pre><code>mod1.age &lt;- glmer(DV ~ devX1*devX2*cAge + (devX1*devX2|Subject) + (devX1*devX2|Item), 
                  data=mydata, family=binomial, glmerControl(optimizer='bobyqa', 
                  optCtrl=list(maxfun=400000)))

     AIC      BIC   logLik deviance df.resid 
   624.4    747.4   -284.2    568.4      569 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.9512 -0.5140 -0.2234  0.5361  5.3189 

Random effects:
 Groups  Name        Variance  Std.Dev.  Corr             
 Subject (Intercept) 1.037e-11 3.220e-06                  
         devX1       2.692e+00 1.641e+00  0.28            
         devX2       3.864e-02 1.966e-01  0.08 -0.94      
         devX1:devX2 4.489e+00 2.119e+00 -0.50 -0.97  0.82
 Item    (Intercept) 5.280e-01 7.267e-01                  
         devX1       2.662e-01 5.159e-01  0.79            
         devX2       3.948e-01 6.284e-01 -0.36 -0.48      
         devX1:devX2 2.906e+00 1.705e+00 -0.59  0.02 -0.22
Number of obs: 597, groups:  Subject, 30; Item, 20

Fixed effects:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -1.3832677  0.0019843  -697.1  &lt; 2e-16 ***
devX1             2.4397103  0.0020486  1190.9  &lt; 2e-16 ***
devX2             0.1386076  0.0019838    69.9  &lt; 2e-16 ***
cAge             -0.0091753  0.0016630    -5.5 3.44e-08 ***
devX1:devX2      -0.3524321  0.0028066  -125.6  &lt; 2e-16 ***
devX1:cAge        0.0150530  0.0019310     7.8 6.41e-15 ***
devX2:cAge        0.0121991  0.0018876     6.5 1.03e-10 ***
devX1:devX2:cAge  0.0005894  0.0019504     0.3    0.763    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
             (Intr) devX1  devX2  cAge   dvX1:X2 dvX1:A dvX2:A
devX1       -0.001                                           
devX2       -0.001  0.001                                    
cAge         0.002 -0.002 -0.002                             
devX1:devX2 -0.002  0.001  0.001 -0.001                      
devX1:cAge  -0.002  0.001  0.001 -0.043  0.002               
devX2:cAge  -0.001  0.001  0.001 -0.040  0.001  -0.017       
dvX1:dvX2:A  0.001 -0.001 -0.001 -0.019 -0.001  -0.020 -0.009
</code></pre>

<p>There is no problem of complete or quasi complete separation between <code>Age</code> and the binary DV. However, there is an almost perfect 1:1 match between <code>Age</code> and <code>Subject</code> (with <code>Subject</code> specified asÂ aÂ random effect in the models). In other words, for most values of <code>Age</code>, there is only one subject corresponding to that value, which makes <code>Age</code> a sort of another version of <code>Subject</code>.</p>

<p>Could this be what is causing severeÂ convergence problems? </p>

<p>If so, would transforming <code>Age</code> into a categorical variable (e.g., with 3 levels) be a suitable solution? I would like to avoid largely arbitrary choices about model specification.</p>

<p>What makes me doubt about this explanation though is that if I remove <code>Subject</code> as random effect, the resulting model still fails to converge.</p>

<pre><code>mod1.age4 &lt;- glmer(DV ~ devX1*devX2*cAge + (devX1*devX2|Item), data=mydata, 
                   family=binomial, glmerControl(optimizer='bobyqa', 
                   optCtrl=list(maxfun=400000)))
</code></pre>
"
"0.155230105141267","0.158610317143629","144815","<p>I'm encountering problems with the results of a <code>glmer</code> model (<code>lme4</code>-package).
Im trying to answer the question, whether a beaver is more likely to be present (<code>Status == 1</code>) or absent (<code>Status == 0</code>) with changing geomorphic and vegetation variables. My model formula looks like this:</p>

<pre><code>model1 &lt;- glmer(Status ~ SlopecatCentered + Canal_width + Distance:Resource_biotopes + 
                         (1 | Location), family=""binomial"", data=Daten12, 
                control=glmerControl(optimizer=""Nelder_Mead""))
</code></pre>

<p>My output looks OK, as far as I can tell, the only peculiar thing being the high estimates of <code>slopecatCentered</code>:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
  ['glmerMod']
Family: binomial  ( logit )
Formula: Status ~ SlopecatCentered + Canal_width + Distance:Resource_biotopes + 
                  (1 | Location)
Data: Datentest
Control: glmerControl(optimizer = ""Nelder_Mead"")

AIC      BIC     logLik    deviance   df.resid 
62.7     77.4    -25.3     50.7       80 

Scaled residuals: 
  Min        1Q    Median        3Q       Max 
-0.095917 -0.003971  0.000000  0.002706  0.079395 

Random effects:
Groups   Name        Variance Std.Dev.
Location (Intercept) 3682     60.68   
Number of obs: 86, groups:  Location, 43

Fixed effects:
                            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -18.5782     7.0847  -2.622 0.008734 ** 
SlopecatCentered             20.4162     5.6060   3.642 0.000271 ***
Canal_width                   0.4763     0.1584   3.007 0.002638 ** 
Distance1:Resource_biotopes   1.0442     0.4717   2.214 0.026861 *  
Distance2:Resource_biotopes   1.0379     0.4662   2.226 0.026010 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) SlpctC Cnl_wd Ds1:R_
SlopctCntrd -0.632                     
Canal_width -0.902  0.698              
Dstnc1:Rsr_ -0.663  0.560  0.458       
Dstnc2:Rsr_ -0.677  0.538  0.461  0.787    
</code></pre>

<p>My qqplot looks weird, though, and so does my residual vs. fitted plot:  </p>

<p><img src=""http://i.stack.imgur.com/8SJjD.jpg"" alt=""qqnorm plot with sjp.glmer(model,...)""></p>

<p><img src=""http://i.stack.imgur.com/wyZp7.jpg"" alt=""fitted vs. residual plot using plot(model)""></p>

<p>edit: I just had a closer look on my data: The <code>SlopecatCentered</code>variable is not a perfect predictor, but my random factor <code>Location</code>is causing this problem. In my raw data set, it denotes 43 different locations. One location has two <code>distance</code> in which most of the variables were measured, so my <code>location</code>variable has 43 * 2 = 86 entrys (in fact, that's the length of the data frame): </p>

<pre><code> &gt;Daten12$Loc
[1] 1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9  10 10 11 11 12 12 13 13 14 14 15 15 16 16 17
[34] 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 29 30 30 31 31 32 32 33 33
[67] 34 34 35 35 36 36 37 37 38 38 39 39 40 40 41 41 42 42 43 43
43 Levels: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ... 43
</code></pre>

<p>I changed that to 1-86 and ran a test model and the plot looked ok (I know that the random effect was futile in that test model, but I wanted to get to the root of the problem).</p>

<p>So apparantly, my raw data frame layout is wrong. But I got samples online to compare, and their layout looks similar, so I just don't know how to fix it.   </p>
"
"0.0896221429896442","0.091573709299126","144904","<p>I have a data set containing various vegetation and geomorphic variables sampled in 3 <code>distances</code> on both <code>sides</code> of 43 drainage ditches (<code>Location</code>). Roughly half of these ditches are occupied by a beaver, the other half is empty. Now I want to run a model with the binomial response variable <code>Status</code> (""beaver == 1"" / ""beaver == 0"")
I'm struggeling with the order and layout of the nested and interaction effects using <code>glmer</code>. So far I've got</p>

<pre><code>fit &lt;- glmer(Status ~ BankslopeScaled + Connectivity + 
                      Canal_width + Distance:Food_crops + 
                      Distance:Edible_trees + 
                (1 | Distance/Side/Location), 
              data, family=binomial(link=""logit"")
</code></pre>

<p>but I'm not sure ifI still have pseudoreplication in my data or whether I correctly applied the formuly in order to estimate the influence of the predictors in every <code>distance</code> on both <code>sides</code> in each <code>Location</code>. </p>

<p>Like, if <code>food_crops</code> in the 3rd <code>distance</code> on the left <code>side</code> is lower than <code>edible_trees</code> in the 2nd <code>distance</code> on the right <code>side</code>, then ...</p>

<p>I kinda feel like there's something wrong with my random effects-term.</p>

<p>My out put looks like this:</p>

<pre><code>summary(fit)

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: binomial  ( logit )
Formula: Status ~ BankslopeScaled + Connectivity + Canal_width + Distance:Food_crops +  
Distance:Edible_trees + (1 | Distance/Side/Location)
Data: Satz

     AIC      BIC   logLik deviance df.resid 
   314.6    360.8   -144.3    288.6      245 

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.18541 -0.71205  0.07243  0.82483  1.75303 

Random effects:
 Groups                   Name        Variance  Std.Dev. 
 Location:(Side:Distance) (Intercept) 2.834e-02 1.683e-01
 Side:Distance            (Intercept) 2.074e-10 1.440e-05
 Distance                 (Intercept) 2.085e-10 1.444e-05
 Number of obs: 258, groups:  Location:(Side:Distance), 258; Side:Distance, 6; Distance, 3

 Fixed effects:
                        Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)            -2.86517    0.79747  -3.593 0.000327 ***
 BankslopeScaled         1.76475    0.62541   2.822 0.004776 ** 
 Connectivity            0.10394    0.02729   3.809 0.000140 ***
 Canal_width             0.19138    0.11089   1.726 0.084364 .  
 Distance1:Food_crops    0.03667    0.09366   0.391 0.695441    
 Distance2:Food_crops    0.10852    0.08996   1.206 0.227694    
 Distance3:Food_crops    0.06303    0.08502   0.741 0.458510    
 Distance1:Edible_trees  0.02273    0.01327   1.712 0.086818 .  
 Distance2:Edible_trees -0.01750    0.02992  -0.585 0.558738    
 Distance3:Edible_trees  0.09769    0.07986   1.223 0.221201    
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 [correlation of fixed effects snipped]    
</code></pre>

<p>A point into the right direction is much appreciated!</p>
"
"0.155691414048724","0.171318722916682","148699","<p>For a current piece of work Iâ€™m trying to model the probability of tree death for beech trees in a woodland in the UK. I have records of whether trees were alive or dead for 3 different census periods along with data on their diameter and growth rate. Each tree has an ID number so it can be identified at each time interval. However, the census intervals vary so that for the time between one survey and another is either 4, 12 or 18 years. Obviously the longer the census period the greater the probability a tree will have died by the time it is next surveyed. <strong>I had problems making a realistic reproducible example so you can find the <a href=""https://github.com/PhilAMartin/Denny_mortality/blob/master/Data/Stack_dead.csv"" rel=""nofollow"">data here</a>.</strong></p>

<p>The variables in the dataset are:</p>

<ol>
<li>ID - Unique ID for tree</li>
<li>Block - the ID for the 20x20m plot in which the tree was located</li>
<li>Dead - Status of tree, either dead (1) or alive (0)</li>
<li>GR - Annual growth rate from previous survey</li>
<li>DBH - diameter of tree at breast height</li>
<li>SL - Length of time between censuses in years</li>
</ol>

<p>Once a tree is recorded as dead it disappears from subsequent surveys.</p>

<p>Ideally I would like to be able to estimate the annual probability of mortality of a tree using information on diameter and growth rate. Having searched around for quite a while I have seen that logistic exposure models appear able to account for differences in census periods by using an altered version of logit link for binomial models as detailed by Ben Bolker <a href=""https://rpubs.com/bbolker/logregexp"" rel=""nofollow"">here</a>. This was originally used by Shaffer to determine the daily probability of bird nest survival where the age (and therefore exposure) of the nest differed. I've not seen it used outside of the context of models of nest survival but it seems like I should be able to use it to model survival/mortality where the exposure differs.</p>

<pre><code>require(MASS)
logexp &lt;- function(exposure = 1)
{
  linkfun &lt;- function(mu) qlogis(mu^(1/exposure))
  ## FIXME: is there some trick we can play here to allow
  ##   evaluation in the context of the 'data' argument?
  linkinv &lt;- function(eta)  plogis(eta)^exposure
  logit_mu_eta &lt;- function(eta) {
    ifelse(abs(eta)&gt;30,.Machine$double.eps,
           exp(eta)/(1+exp(eta))^2)
    ## OR .Call(stats:::C_logit_mu_eta, eta, PACKAGE = ""stats"")
  }
  mu.eta &lt;- function(eta) {       
    exposure * plogis(eta)^(exposure-1) *
      logit_mu_eta(eta)
  }
  valideta &lt;- function(eta) TRUE
  link &lt;- paste(""logexp("", deparse(substitute(exposure)), "")"",
                sep="""")
  structure(list(linkfun = linkfun, linkinv = linkinv,
                 mu.eta = mu.eta, valideta = valideta, 
                 name = link),
            class = ""link-glm"")
}
</code></pre>

<p>At the moment my model looks like this, but I will incorporate more variables as I go along:</p>

<pre><code>require(lme4)
Dead&lt;-read.csv(""Stack_dead.csv"",)


M1&lt;-glmer(Dead~DBH+(1|ID),data=Dead,family=binomial(logexp(Dead$SL))) 
#I use (1|ID) here to account for the repeated measurements of the same individuals
    summary(M1)

plot(Dead$DBH,plogis(predict(M1,re.form=NA)))
</code></pre>

<p><strong>Primarily I want to know</strong>:</p>

<ol>
<li><strong>Does the statistical technique I am using to control for the difference in time between census seem sensible? If it isn't, can you think of a better way to deal with the problem?</strong></li>
<li><strong>If the answer to the first question is yes, is using the inverse logit (plogis) the correct way to get predictions expressed as probabilities?</strong></li>
</ol>

<p>Thanks in advance for any help!</p>
"
"0.201234476647766","0.214962659068617","149732","<p>I'm trying to analyze the data from an experiment I conducted, and could use some guidance in relation to fixed vs. random effects.</p>

<p>The experiment was related to risk-seeking behavior in the context of hypothetical gambles, and implemented a 3 (Response Scale: Control vs. RI vs. ABR) x 3 (Stakes) X 5 (Endowment) factorial design. Response Scale was a between-subjects manipulation, and the levels of Stakes and Endowment were combined factorially to produce 15 different gamble scenarios, all of which were evaluated by each participant (i.e. gamble evaluation was within-subjects). The DV of interest for the particular analysis I'm working on is a binary indicator variable called ""Would.Play"" that describes whether a participant would choose to play the gamble if they were to encounter it in real life.</p>

<p>As a preliminary analysis, I'd like to be able to claim that there were no [or, as the data seem to indicate, <em>were</em>] meaningful differences in Would.Play as a result of random assignment to a particular Response Scale condition (designated by the factor variable ""Response.Scale"", ref=""Control"").</p>

<p>I can obviously do this with a binary logit for each of the 15 gambles (designated by the variable ""Gamble.Num""), but I'd like to avoid issues with multiple testing. My preference, therefore, is to fit a single model that accounts for the heterogeneity in gambles by fitting a separate intercept for each gamble.</p>

<p>I've come across two ways to do this, each of which seems to give different results: Dummy ""Fixed Effects"" modeling in glm() and ""random effects"" modeling in glmer() (see output below).</p>

<p>It seems possible that the difference in the estimated coefficients could be the result of the Dummy ""Fixed Effects"" approach taking Gamble.Num==1 as a reference level, but I don't have a very deep understanding of the math underlying these two techniques. I was hoping someone would be able to give me a quick explanation of (a) why the these two models appear to give different results; and (b) whether one of these approaches is better suited to answering my question of interest: is there a unique effect of Response.Scale on Would.Play, taking heterogeneity in gambles into account?</p>

<p>Below is a quick look at the data I'm using, and the output of the two models:</p>

<pre><code>## Data ##
head(analysis.0.data)
 Local.ID Condition Response.Scale RS.Code Gambles.First Gamble.Num Endowment Stakes
1        8         4             RI       1             0          1      -150     10
2        8         4             RI       1             0          2      -150     50
3        8         4             RI       1             0          3      -150    200
4        8         4             RI       1             0          4       -25     10
5        8         4             RI       1             0          5       -25     50
6        8         4             RI       1             0          6       -25    200
  Would.Play Perc.Risk
1          0         4
2          0         6
3          0         5
4          0         3
5          0         5
6          0         7


## Dummy ""Fixed Effects"" Model ##
summary(glm(Would.Play ~ Response.Scale + factor(Gamble.Num), family=""binomial"",     
data=analysis.0.data))

Call:
glm(formula = Would.Play ~ Response.Scale + factor(Gamble.Num), 
    family = ""binomial"", data = analysis.0.data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7766  -0.7204  -0.4678   0.7006   2.5394  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)          -1.14906    0.21987  -5.226 1.73e-07 ***
Response.ScaleRI     -0.06749    0.12815  -0.527  0.59844    
Response.ScaleABR    -0.91035    0.13843  -6.576 4.82e-11 ***
factor(Gamble.Num)2  -0.94090    0.35886  -2.622  0.00874 ** 
factor(Gamble.Num)3  -1.12416    0.37769  -2.976  0.00292 ** 
factor(Gamble.Num)4   0.31966    0.28379   1.126  0.25999    
factor(Gamble.Num)5  -0.63953    0.33303  -1.920  0.05482 .  
factor(Gamble.Num)6  -0.85860    0.35120  -2.445  0.01449 *  
factor(Gamble.Num)7   1.42100    0.26770   5.308 1.11e-07 ***
factor(Gamble.Num)8   0.35620    0.28268   1.260  0.20765    
factor(Gamble.Num)9  -0.51138    0.32379  -1.579  0.11425    
factor(Gamble.Num)10  2.10754    0.27298   7.720 1.16e-14 ***
factor(Gamble.Num)11  0.28248    0.28496   0.991  0.32154    
factor(Gamble.Num)12 -1.02908    0.36760  -2.799  0.00512 ** 
factor(Gamble.Num)13  2.49612    0.28133   8.873  &lt; 2e-16 ***
factor(Gamble.Num)14  1.72839    0.26867   6.433 1.25e-10 ***
factor(Gamble.Num)15  0.08524    0.29204   0.292  0.77039    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2649.2  on 2249  degrees of freedom
Residual deviance: 2096.4  on 2233  degrees of freedom
AIC: 2130.4

Number of Fisher Scoring iterations: 5


## GLMER ""Random-Effects"" Model##
summary(glmer(Would.Play ~ Response.Scale + (1|Gamble.Num), family=""binomial"", 
data=analysis.0.data))
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
[glmerMod]
 Family: binomial  ( logit )
Formula: Would.Play ~ Response.Scale + (1 | Gamble.Num)
   Data: analysis.0.data

     AIC      BIC   logLik deviance df.resid 
  2169.3   2192.1  -1080.6   2161.3     2246 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.9011 -0.5461 -0.3522  0.5439  4.6708 

Random effects:
 Groups     Name        Variance Std.Dev.
 Gamble.Num (Intercept) 1.291    1.136   
Number of obs: 2250, groups:  Gamble.Num, 15

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -0.90254    0.30722  -2.938  0.00331 ** 
Response.ScaleRI  -0.06682    0.12707  -0.526  0.59897    
Response.ScaleABR -0.90170    0.13727  -6.569 5.07e-11 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Rs.SRI
Rspns.SclRI -0.202       
Rspns.ScABR -0.183  0.456
</code></pre>

<p>Thanks!</p>
"
"0.181101789331478","0.158610317143629","151079","<p>I have a data set of 2430 observations, with a binomial dependent variable, 3 categorical fixed effects and 2 categorical random effects (item and subject). I want to to a mixed effects model using glmer. Here is what I entered into R:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + ``(1|item), data=RprodHSNS, family=""binomial"")`
</code></pre>

<p>I then get the following warnings:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.02081 (tol = 0.001, component 11)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
- Rescale variables?`
</code></pre>

<p>This is what my summary looks like:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
Data: RprodHSNS`


AIC      BIC   logLik deviance df.resid
1400.0   1479.8   -686.0   1372.0     2195 `

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0346 -0.2827 -0.0152  0.2038 20.6578 `

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.475    1.215   
subject (Intercept) 1.900    1.378   
Number of obs: 2209, groups:  item, 54; subject, 45
Fixed effects:`
Estimate Std. Error z value Pr(&gt;|z|)`                             
(Intercept)                -0.61448   42.93639  -0.014 0.988582  
group1                     -1.29254   42.93612  -0.030 0.975984    
context1                    0.09359   42.93587   0.002 0.998261   
context2                   -0.77262    0.22894  -3.375 0.000739***
condition1                  4.99219   46.32672   0.108 0.914186
group1:context1            -0.17781   42.93585  -0.004 0.996696
group1:context2            -0.10551    0.09925  -1.063 0.287741
group1:condition1          -3.07516   46.32653  -0.066 0.947075
context1:condition1        -3.47541   46.32648  -0.075 0.940199
context2:condition1        -0.07293    0.22802  -0.320 0.749087
group1:context1:condition1  2.47882   46.32656   0.054 0.957328
group1:context2:condition1  0.30360    0.09900   3.067 0.002165 **

---

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Correlation of Fixed Effects:
            (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                
context2     0.001  0.000 -0.001                                                              
condition1  -0.297  0.297  0.297  0.000                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001 -0.297                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.000  0.000                                       
grp1:cndtn1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.000                               
cntxt1:cnd1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.001  1.000                        
cntxt2:cnd1  0.000  0.000 -0.001  0.011  0.001  0.000    -0.197 -0.001    -0.001              
grp1:cnt1:1 -0.297  0.297  0.297  0.001  1.000 -0.297    -0.001 -1.000    -1.000  0.001       
grp1:cnt2:1  0.000  0.000  0.001 -0.198  0.000 -0.001     0.252  0.000     0.001 -0.136  0.000
</code></pre>

<p>Extremely high p-values, which does not seem to be possible. </p>

<p>In a previous post I read that one of the problems could be fixed by increasing the amount of iterations by inserting this bit in the command: glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000))</p>

<p>So here's the new command:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + (1|item), data=RprodHSNS, family=""binomial"", glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))
</code></pre>

<p>I get one less warning, but the other one is still there:</p>

<pre><code>&gt; Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.005384 (tol = 0.001, component 7)
</code></pre>

<p>The summary also still looks weird:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
   Data: RprodHSNS
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))`

AIC      BIC   logLik deviance df.resid 
1400.0   1479.8   -686.0   1372.0     2195

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0334 -0.2827 -0.0152  0.2038 20.6610 

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.474    1.214   
subject (Intercept) 1.901    1.379   
Number of obs: 2209, groups:  item, 54; subject, 45

Fixed effects:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.64869   26.29368  -0.025 0.980317    
group1                     -1.25835   26.29352  -0.048 0.961830    
context1                    0.12772   26.29316   0.005 0.996124    
context2                   -0.77265    0.22886  -3.376 0.000735 ***
condition1                  4.97325   22.80050   0.218 0.827335    
group1:context1            -0.21198   26.29303  -0.008 0.993567    
group1:context2            -0.10552    0.09924  -1.063 0.287681    
group1:condition1          -3.05629   22.80004  -0.134 0.893365    
context1:condition1        -3.45656   22.80017  -0.152 0.879500    
context2:condition1        -0.07305    0.22794  -0.320 0.748612    
group1:context1:condition1  2.45996   22.80001   0.108 0.914081    
group1:context2:condition1  0.30347    0.09899   3.066 0.002172 ** 

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                     
context2     0.000  0.000  0.000                                                              
condition1   0.123 -0.123 -0.123 -0.001                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001  0.123                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.001  0.000                                         
grp1:cndtn1 -0.123  0.123  0.123  0.000 -1.000 -0.123    -0.001                               
cntxt1:cnd1 -0.123  0.123  0.123  0.000 -1.000 -0.123     0.000  1.000                        
cntxt2:cnd1  0.000  0.000  0.000  0.011 -0.001  0.000    -0.197  0.001     0.001              
grp1:cnt1:1  0.123 -0.123 -0.123  0.000  1.000  0.123     0.000 -1.000    -1.000 -0.001      
grp1:cnt2:1  0.000 -0.001  0.001 -0.198  0.001 -0.001     0.252 -0.001     0.000 -0.136  0.000
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<p>Does anyone have an idea what I can do to solve this? Or tell me what this warning even means? Please explain in a way that an R-newbie like myself can understand!</p>

<p>Any help is much appreciated!</p>
"
"0.134433214484466","0.137360563948689","151354","<p>While trying to determine power for a Poisson GLMM, I started by checking the probability of rejecting the null for a given parameter when the null is true (parameter is zero). I kept coming up with a rejection rate of approximately $0.1$ where I expected $\alpha = 0.05$. To check if my programming was faulty, I did the same with a binomial GLMM and a Gaussian LMM. Both of these, however, returned the expected percentage of rejections ($5\%$). Thus, I don't think this is a post for Stack Overflow, but maybe others will disagree.</p>

<p>I figured I'd post the code here and see if anyone can tell me why I'm seeing this unexpected result.</p>

<p>First, a function to simulate data with a single independent variable $X$ and the dependent variable $Y$. The data simulate $100$ individuals with $3$ measurements each. $Y$ is defined as a function of the intercept, $bX$, and the group specific intercept ($g$). Though, $b$ is zero, so $X$ doesn't come into play. The function also fits a GLMM to the data and checks/returns whether $b=0$ should be rejected.</p>

<pre><code>simPow.Pois &lt;- function(j=100, i=3, alpha=.05, b=0, tau=1, refRate=.2) {
    # refRate is referent group rate (intercept)

    # g is group level intercept
    g &lt;- rep(rnorm(j, 0, tau), each=i)

    # group identifies the groupings of the individuals
    group &lt;- rep(1:j, each=3)

    # randomly drawn x
    x &lt;- round(runif(i*j,-.5,5.499)) # approx uniform discrete 0-5

    # DV: a function of intercept, b*x, and group intercept
    y &lt;- exp(refRate + b*x + g)
    y &lt;- rpois(i*j, y)

    # fit the model with one of three options
    #ans &lt;- glmer(y ~ x + (1|group), family=poisson)
    ans &lt;- glmmPQL(y ~ x, random=~1|group, family=poisson)
    #ans &lt;- glmmadmb(y ~ x + (1|group), family = ""poisson"",link = ""log"")

    # extract z as [fixed effect] / [SE]
    z &lt;- abs(fixef(ans) / sqrt(diag(vcov(ans))))

    # check if z is too large to believe the null is true
    if(2*(1-pnorm(z['x'])) &lt; alpha) {
        return(1)
    } else { 
        return(0)
    }
}
</code></pre>

<p>The function above returns a <code>1</code> if the null hypothesis is rejected, and a <code>0</code> otherwise.</p>

<p>I then run the following script to do this many times. Unfortunately, this takes a few minutes to run, so I've limited it to 200 iterations. You can expand that if you doubt the result.</p>

<pre><code>require(MASS)
res.pois &lt;- replicate(200,simPow.Pois(b=0))
mean(res.pois)
#[1] 0.105
</code></pre>

<p>I appreciate any thoughts on this. Thank you.</p>
"
"0.0801604816056202","0.102382519472325","151473","<p>How do I interpret main and interaction effects with deviation coding?
This is the output I generated for <code>Code_IS_Condition</code> (3 levels), <code>Code_IS_Language</code> (2 levels), <code>Combined_X</code> (2 levels) and <code>PWI_Condition</code> (2 levels). My dependent variable is <code>Picture naming accuracy</code> and my independent variables are:  </p>

<ol>
<li>Auditory word presented along with the picture (Code_IS_Condition - with three levels), </li>
<li>Language of the auditory word (Code_IS_Langauge - with two levels), </li>
<li>Timing of presentation of the auditory word with respect to the picture (Combined_X - with 2 levels), and </li>
<li>Two types of pictures (PWI_Condition - with 2 levels). </li>
</ol>

<p>I used the following code for effect coding: <code>contrasts(F_YACodeISCondition)=â€²contr.sumâ€²</code>; and the following formula to fit a model:  </p>

<pre><code>test = glmer(PWIAccuracy~CodeISConditionâˆ—CodeISLanguageâˆ—CombinedXâˆ—PWICondition+
                         (1|Subject)+1|PictureName), 
             data=FYA[FYAPicture_Type=='Target',], family=""binomial"")
</code></pre>

<p>I have equal number of participants in each condition. The only between-subject factor is the Combined_X condition. </p>

<pre><code>Fixed effects:
                                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                         -2.072179   0.137586 -15.061  &lt; 2e-16 ***
Code_IS_Condition1                  -0.014436   0.082421  -0.175  0.86096    
Code_IS_Condition2                   0.256573   0.079323   3.235  0.00122 ** 
Code_IS_Language1                    0.017452   0.060233   0.290  0.77201    
Combined_X1                         -0.118204   0.108818  -1.086  0.27737    
PWI_ConditionNonCog                 -0.274938   0.134193  -2.049  0.04048 *  
Code_IS_Condition1:Code_IS_Language1 0.022553   0.082581   0.273  0.78478    
Code_IS_Condition2:Code_IS_Language1 0.018644   0.079629   0.234  0.81488    
Code_IS_Condition1:Combined_X1       0.126012   0.082423   1.529  0.12630    
Code_IS_Condition2:Combined_X1      -0.006376   0.079306  -0.080  0.93592    
Code_IS_Language1:Combined_X1        0.025450   0.058561   0.435  0.66386    
Code_IS_Condition1:PWI_ConditionNonCog 0.002978   0.111587   0.027  0.97871    
Code_IS_Condition2:PWI_ConditionNonCog -0.110211   0.110038  -1.002  0.31655    
Code_IS_Language1:PWI_ConditionNonCog  -0.046570   0.082857  -0.562  0.57408    
Combined_X1:PWI_ConditionNonCog         0.037928   0.079710   0.476  0.63420    
Code_IS_Condition1:Code_IS_Language1:Combined_X1 0.077530   0.082444   0.940  0.34701    
Code_IS_Condition2:Code_IS_Language1:Combined_X1 0.009191   0.079342   0.116  0.90778    
Code_IS_Condition1:Code_IS_Language1:PWI_ConditionNonCog 0.161470   0.111885   1.443  0.14897    
Code_IS_Condition2:Code_IS_Language1:PWI_ConditionNonCog-0.060774   0.110624  -0.549  0.58275    
Code_IS_Condition1:Combined_X1:PWI_ConditionNonCog -0.251237   0.111288  -2.258  0.02398 *  
Code_IS_Condition2:Combined_X1:PWI_ConditionNonCog 0.139801   0.109615   1.275  0.20217    
Code_IS_Language1:Combined_X1:PWI_ConditionNonCog -0.013116   0.079622  -0.165  0.86916    
Code_IS_Condition1:Code_IS_Language1:Combined_X1:PWI_ConditionNonCog -0.189141   0.111285  -1.700  0.08921 .  
Code_IS_Condition2:Code_IS_Language1:Combined_X1:PWI_ConditionNonCog  0.052253   0.109649   0.477  0.63368  
</code></pre>
"
"0.100200602007025","0.0819060155778602","152868","<p>I have an administrative database with hospital readmissions (binomial: yes/no) and a couple of predictors. I've fitted a multilevel model with the function <code>glmer</code> from the package <code>lme4</code> to estimate the effect of these predictors on readmissions. 
The model has two levels: <code>hospital</code> and <code>patient</code>.
When I calculate the predicted probabilities (the chance of a readmission), and afterwards calculate the readmission ratio's for each hospital (by dividing the observed readmissions by the predicted readmissions), all my ratio's are around 1 which can't be correct.</p>

<p>Before I've calculated the predicted probabilities with a normal logistic regression, which gives more plausible ratio's (from 0,64 to 1,5)</p>

<p>This is my code to calculate predicted probabilities for the multilevel model:</p>

<pre><code>database$predprob &lt;- fitted(model1)
</code></pre>

<p>I've also tried this one, but it gives exactly the same predicted probabilities:</p>

<pre><code>database$predprob &lt;- predict(model1, newdata = database, type = ""response"", na.action = na.omit)
</code></pre>

<p>Does anybody know how to calculate predicted probabilities for a multilevel analysis? I suppose there must be another way to calculate it as my calculated ratios (observed/predicted) are all around 1.</p>
"
"0.110901743841784","0.113316683941681","153611","<p>I'm revising a paper on pollination, where the data are binomially distributed (fruit matures or does not). So I used <code>glmer</code> with one random effect (individual plant) and one fixed effect (treatment). A reviewer wants to know whether plant had an effect on fruit set -- but I'm having trouble interpreting the <code>glmer</code> results.</p>

<p>I've read around the web and it seems there can be issues with directly comparing <code>glm</code> and <code>glmer</code> models, so I'm not doing that. I figured the most straightforward way to answer the question would be to compare the random effect variance (1.449, below) to the total variance, or the variance explained by treatment. But how do I calculate these other variances? They don't seem to be included in the output below. I read something about residual variances not being included for binomial <code>glmer</code> -- how do I interpret the relative importance of the random effect?</p>

<pre><code>&gt; summary(exclusionM_stem)
Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: cbind(Fruit_1, Fruit_0) ~ Treatment + (1 | PlantID)

     AIC      BIC   logLik deviance df.resid 
   125.9    131.5    -59.0    117.9       26 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0793 -0.8021 -0.0603  0.6544  1.9216 

Random effects:
 Groups  Name        Variance Std.Dev.
 PlantID (Intercept) 1.449    1.204   
Number of obs: 30, groups:  PlantID, 10

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  -0.5480     0.4623  -1.185   0.2359   
TreatmentD   -1.1838     0.3811  -3.106   0.0019 **
TreatmentN   -0.3555     0.3313  -1.073   0.2832   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
           (Intr) TrtmnD
TreatmentD -0.338       
TreatmentN -0.399  0.509
</code></pre>
"
"0.155230105141267","0.158610317143629","154037","<p>The <code>metafor</code> package in R can be used to fit the random effects model with the exact binomial likelihood as described in Stijnen <em>et al.</em> (2010). The <code>metafor</code> website also shows how to reproduce the examples from that paper (<a href=""http://www.metafor-project.org/doku.php/analyses:stijnen2010"" rel=""nofollow"">link</a>). One of the benefits of using the binomial likelihood instead of normal approximations is that studies with zero events can be properly modelled instead of adding a continuity correction of 0.5 to those studies. </p>

<p>The documentation for the appropriate <code>rma.glmm</code> function says that the continutity correction is added by default, trough the <code>add</code> argument. Here is the problem: Trying to do the analysis without the continuity correction gives an error. From the <code>metafor</code> webpage we can do the analysis from the Stijnen <em>et al</em> (2010) paper that works fine:</p>

<pre><code>library(metafor) # version 1.9-7
dat &lt;- get(data(dat.nielweise2007))
rma.glmm(measure=""PLO"", xi=ci, ni=n2i, data=dat) # add=1/2 by default.
</code></pre>

<p>If instead the continuity correction is removed an error occurs:</p>

<pre><code>rma.glmm(measure=""PLO"", xi=ci, ni=n2i, data=dat, add=0) # add=0 means no correction.

Error in model.frame.default(formula = yi ~ X - 1, drop.unused.levels = TRUE) : 
  variable lengths differ (found for 'X')
In addition: Warning messages:
1: In escalc.default(measure = measure, xi = xi, mi = mi, add = add,  :
  Some yi and/or vi values equal to +-Inf. Recoded to NAs.
2: In rma.glmm(measure = ""PLO"", xi = ci, ni = n2i, data = dat, add = 0) :
  Some yi/vi values are NA.
</code></pre>

<p>I am able to reproduce the analysis by using the <code>lme4</code> package without the correction and get the same estimates:</p>

<pre><code>library(lme4) # version 1.1-7

# nAGQ=7 is what the metafor package uses 
mm &lt;- glmer(cbind(ci, n2i-ci) ~ (1|study), family=binomial(link='logit'), data=dat, nAGQ=7)
summary(mm)
</code></pre>

<p>Why does it not work to remove the continuity correction in the <code>rma.glmm</code> function? I also tried to add a smaller correction (<code>add=0.0000001</code>) and the estimates for tau, I^2 and H^2 changed a bit, but the numbers under the <em>model results</em> heading did not. What is going on? </p>

<p>References:</p>

<p>Stijnen, T., Hamza, T. H., &amp; Ozdemir, P. (2010). Random effects meta-analysis of event outcome in the framework of the generalized linear mixed model with applications in sparse data. Statistics in Medicine, 29(29), 3046â€“3067.</p>
"
"0.10976425998969","0.0934620256820074","155718","<p>I am having trouble interpreting results from a multiple comparison test.  Here is the model I ran:</p>

<pre><code>m1 &lt;-glmer(resp~Grp-1+offset(LArea)+(1|Crk/Meso)+(1|resid),data=df, family=binomial)
</code></pre>

<p>And here are the results from the lsmeans function:</p>

<pre><code>&gt; lsmeans(m1, pairwise ~ Grp, adjustSigma = TRUE, adjust = ""sidak"")
$lsmeans
 Grp      lsmean        SE df asymp.LCL  asymp.UCL
 90U   -0.207353 0.6345219 NA -1.450993  1.0362871
 90I   -2.214079 0.7946987 NA -3.771660 -0.6564978
 96U   -1.605763 0.6551178 NA -2.889770 -0.3217555
 96I   -3.168827 0.7957100 NA -4.728390 -1.6092644
Confidence level used: 0.95 
</code></pre>

<p>OK, everything is great so far.  I can see that the lsmean order is <code>90U>96U>90I>96I</code>.  So now I want to check the observed differences are significant:  </p>

<pre><code>&gt; cld(lsmeans(m1, pairwise ~ Grp, adjustSigma =TRUE, adjust = ""sidak""))
 Grp      lsmean        SE df asymp.LCL  asymp.UCL .group
 96I   -3.168827 0.7957100 NA -4.728390 -1.6092644  1    
 90I   -2.214079 0.7946987 NA -3.771660 -0.6564978  12   
 96U   -1.605763 0.6551178 NA -2.889770 -0.3217555  1    
 90U   -0.207353 0.6345219 NA -1.450993  1.0362871   2   

Confidence level used: 0.95 
P value adjustment: tukey method for comparing a family of 4 estimates 
significance level used: alpha = 0.05
</code></pre>

<p>But now I am confused.  How can <code>90I</code> be grouped with <code>90U</code> while excluding <code>96U</code>?  It is saying <code>-2.2 = -0.2</code> while <code>-2.2!=-1.6</code> and <code>-1.6 != -0.2</code>  If it helps, here is an image with the 95%CI:
<a href=""http://i.imgur.com/IyfLDbv.png"" rel=""nofollow"">Plot of estimated coefficients</a></p>

<p>The explanations I have thought of but find unconvincing are:</p>

<ul>
<li>lsmean ignores the random effects, effectively running a plain-jane pairwise comparison.</li>
<li>This is a perfectly reasonable conclusion.</li>
</ul>

<p>My running hypothesis:</p>

<ul>
<li>I_have_no_idea_what_I_am_doing.jpg</li>
</ul>

<p>And a brief aside: Why does it perform a Tukey adjustment when I ask for a Sidak?</p>
"
"0.119496190652859","0.137360563948689","158539","<p>I'm doing model selection, analysing the effect of a number of variables on the number of shoots browsed by deer, using the number of shoots available as an offset variable. My data distribution is negative binomial.</p>

<p>Following the advices received during a course, I was first fitting a global model using <code>glm.nb</code> and noting the theta value obtained. After that, I was doing my model selection using the package <code>AICcmodavg</code> and <code>glm</code>. I specified the theta value for each model using the value of the first model in <code>glm</code> like this : <code>family=negative.binomial(theta = )</code>. My understanding here is that we specified a similar theta value to be able to compare the models.</p>

<p>So far, so good. But I needed to add a random effect to my model and my models didn't converged with <code>glmer.nb</code>. I thus switched to <code>glmmadmb</code>, where the theta value seems to have a different name, alpha, the negative binomial dispersion parameter. So, my questions:</p>

<p>1-Is alpha really the equivalent of theta ?</p>

<p>2-My models have very different alpha values (from 400 to 0.4000). Is there a range of ""normal"" negative binomial dispersion parameter value ?</p>

<p><em>EDIT: Running again my code this morning removed any values around 400. All alpha values are now similar. I think this was definitely a mistake and I think anyone obtaining very different values should be careful !</em></p>

<p>3-Should I still proceed with specifying a a same alpha values for all my models ? This can be achieve in <code>glmmadmb</code>, in my understanding, by using <code>start= list(log_alpha = )</code>.</p>

<p>Thanks everyone.</p>
"
"0.135110464183741","0.15185781720314","159451","<p>this is my first post, so I will try to make it as clear as possible. Hopefully is not something trivial and I am just stuck.</p>

<p>I am performing meta-analysis using the glmer function of the lme4 package.</p>

<p>My example dataset can be found here  : </p>

<pre><code>data1&lt;-structure(list(Outcome = structure(c(1L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L,  1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""E"",  ""NE""), class = ""factor""), Exposure = structure(c(1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""T"",  ""NT""), class = ""factor""), Study = structure(c(1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""Cullinan1"",  ""Levi"", ""Cullinan2"", ""Colucci"", ""Popov"", ""Ohtsu"", ""Bouche"", ""Lutz"",  ""Koizumi""), class = ""factor"")), .Names = c(""Outcome"", ""Exposure"",  ""Study""), row.names = c(NA, 289L), class = ""data.frame"")
</code></pre>

<p>When I am using glmer with the binomial(logit) family or the gausian(identity) it works fine and produces estimates for the treatment effect. When I am performing the same for the Risk ratio using the binomial(log) family and link I get the following error.</p>

<blockquote>
  <p>Error in summary(glmer(as.numeric(Outcome) - 1 ~ as.numeric(Exposure) +  :    error in evaluating the argument 'object' in selecting a method for function 'summary': Error in summary(glmer(as.numeric(Outcome) - 1 ~ as.numeric(Exposure) +  :    (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate</p>
</blockquote>

<p>The code that I am using is the following for the three families and links.</p>

<p>For the logit.</p>

<pre><code>glmer(Outcome ~ Exposure + (1|Study), data=data1, family =binomial(link = ""logit""))
</code></pre>

<p>For the log (Problematic)</p>

<pre><code>glmer(Outcome ~ Exposure + (1|Study), data=data1, family =binomial(link = ""log""))
</code></pre>

<p>For the identity</p>

<pre><code>glmer(Outcome ~ Exposure + (1|Study), data=data1, family =gaussian(link = ""identity""))
</code></pre>
"
"0.0776150525706333","0.0793051585718144","159735","<p>I have two factors that are fully crossed, the levels of the factor are each coded 0 and 1. I am running a regression testing for one main effect and one interaction. The following is my logistic regression formula:</p>

<pre><code>m1=glmer(y~1+A+A:B+(1|Participants)+(1|Word),data=data, family = ""binomial"")
</code></pre>

<p>I am wondering if this is acceptable (only testing for one main effect and an interaction), and also why I am getting two interaction terms in my output:</p>

<pre><code>Fixed effects:
        Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -0.18740    0.21600  -0.868  0.38561   
A1           0.74546    0.28399   2.625  0.00867 **
A0:B1        0.01537    0.28244   0.054  0.95662   
A1:B1        0.15884    0.28650   0.554  0.57929   
</code></pre>
"
"0.0950586375786717","0.0971285862357264","160622","<p>I have a problem with using <code>apply</code> in <code>lme4</code>.
I have a set of factor independent variables which I have determined their reference before. I have no problem when using them solely in glmer however, when I used them in <code>glmer</code> through the <code>apply</code> command the reference changed.
This is a part of my code.</p>

<p>I have edited the code and provide 30 cases of my data sorry for being in mess. I Preferred to have the data I am working on</p>

<pre><code>            BPdata:

            i4partici1        Edu       BMIC inflatesys
             1           1 Illiterate     Normal          1
             2           1       6-12     Normal          0
             3           2 Illiterate      Obese          0
             4           2       6-12     Normal          1
             5           2 Illiterate      Obese          0
             6           2 Illiterate Overweight          0
             7           3         &lt;6 Overweight          0
             8           3 Illiterate Overweight          1
             9           3 Illiterate     Normal          0
             10          3         &lt;6     Normal          0
             11          4 Illiterate     Normal          1
             12          4       6-12     Normal          0
             13          4 Illiterate Overweight          0
             14          4 Illiterate Overweight          0
             15          5 Illiterate     Normal          0
             16          5         &lt;6 Overweight          1
             17          5 Illiterate Overweight          0
             18          7 Illiterate Overweight          0
             19          7       6-12 Overweight          0
             20          9         &lt;6 Overweight          1
             21         10 Illiterate Overweight          0
             22         11 Illiterate     Normal          0
             23         12       6-12     Normal          0
             24          1 Illiterate      Obese          1
             25          1       6-12     Normal          0
             26          4       6-12     Normal          1
             27          5 Illiterate     Normal          0
             28          5 Illiterate     Normal          1
             29         10 Illiterate      Obese          0
             30          9 Illiterate Overweight          0

   BPdata$BMIC&lt;- relevel(BPdata$BMIC, ref='Normal')
     summary(BPdata$BMIC)

  FINALgl &lt;- function(y,h) {
     gl &lt;- function(x) {
         ml &lt;- glmer(y ~ x  + (1 | i4partici1), 
                     data = BPdata, family = binomial, 
                     control = glmerControl(optimizer = ""bobyqa""), 
                     nAGQ = 1)
         se &lt;- sqrt(diag(vcov(ml)))
         # table of OR estimates with 95% CI
         tab &lt;- cbind(Est = fixef(ml), 
                  LL = fixef(ml) - 1.96 * se, 
                  UL = fixef(ml) + 1.96 * se)
         OR &lt;- round(exp(tab),2)
         as.data.frame(OR)
      }
      return(apply(h,2,gl))
  }

         #running by using apply
         FINALgl(BPdata[,4],BPdata[,c(2:3)])

         #running individualy
         a&lt;-glmer(inflatesys ~ BMIC + (1 | i4partici1), data = BPdata, family = binomial, control = glmerControl(optimizer = ""bobyqa""), nAGQ = 1)
         fixef(a)
</code></pre>

<p>the first argument determined the response and the second determined the explanatory variables.</p>

<p>of course because of small sample size in this example model have some convergence problem but You can see the reference group is different in the two situation.</p>
"
"0.100200602007025","0.102382519472325","160780","<p>having only dim memories of this subject from uni, I'm struggling a bit with fitting a polynomial binomial mixed effects model with two nested random effects.</p>

<p>My data are counts of success and failures (s and f) at a variety of values of x.  There are two random effects, r1 and a nested random effect r2.</p>

<p>I would like to try fitting a polynomial model, up to ninth order and use the AIC values to find the minimum adequate model, but I'm unsure of the syntax.</p>

<p>So I've got three questions really:</p>

<p>Is the code below currently correct?</p>

<p>What is the syntax to fit a glmer polynomial?</p>

<p>Is this whole thing statistical nonsense, and should i try a different approach?</p>

<p>Thanks for your help, code below:</p>

<pre><code>require(lme4)

## SAMPLE SET:
## x fixed effect
## s count success
## f count fail
## r1 random effect 1
## r2 random effect 2

sample.set &lt;- data.frame(x = runif(1000,-100,100),
                         s = round(runif(1000,100,1000),0),
                         f = round(runif(1000,100,10000),0),
                         r1 = c(rep(""A"",250), rep(""B"",250), rep(""C"",250), rep(""D"",250)))

sample.set$r2 &lt;- sapply(sample.set$r1,function(x){if(x == ""A"")                {as.character(round(runif(1,1,5), 0 ))} else
                                                  if(x == ""B"") {as.character(round(runif(1,6,10), 0))} else
                                                  if(x == ""C"") {as.character(round(runif(1,11,15),0))} else
                                                               {as.character(round(runif(1,16,20),0))}})

prop.tab &lt;- cbind(sample.set$s,sample.set$f)

mm.model &lt;- glmer(prop.tab ~ sample.set$x + (sample.set$x | sample.set$r1) + (sample.set$x | sample.set$r2),
                  family = binomial, control = glmerControl(optimizer = ""bobyqa""), nAGQ = 0)
</code></pre>
"
"0.0776150525706333","0.0793051585718144","161581","<p>I want to extract standard deviation of residual from <code>glmer()</code> function in R .</p>

<p>So I wrote :</p>

<pre><code>lmer_obj = glmer(Y ~ X1 + X2 + (1|Subj), data=D, family=binomial)
sigma(lmer_obj)
</code></pre>

<p>I noticed that the last command <code>sigma(lmer_obj)</code> returns always <code>1</code> irrespective of the data That is, whether I used the <code>cbpp</code> data or my own simulated data from multilevel logistic distribution, the residual standard error is always <code>1</code>.</p>

<p>How can I get the residual standard deviation from <code>glmer()</code> function?</p>
"
"0.126744850104896","0.113316683941681","161678","<p>So this is either going to be straight forward or else a huge mess. I am finding that bootstrap CI for my glmer models are really wacky-- in fact, they don't even include the point estimates from the models. Moreover, they completely disagree with both the Wald and profile CIs (which seem reasonable). I also get wacky, although again completely different, CIs if I instead call directly to boot.merMod with my own FUN and then call boot.ci on that output. I am only interested in the fixed effects estimates. Here is an example:</p>

<p>My model estimates the fixed effect of ""scale(rep)"" = 1.569, std. error = 0.261, z-value = 6.02.</p>

<p>Wald estimate from confint returns for this effect (1.058, 2.079) and the profile estimate is (1.326, 2.176).</p>

<p>However, bootstrap with n=500 is (-0.461, 0.794) and with n=1000 is (-0.419, 0.725).</p>

<p>Using instead boot.merMod with FUN=function(fit){return(c(fixef(fit),unlist(VarCorr(fit))))}
and then calling boot.ci with n=500 returns (2.475, 3.660) for Normal and (2.376, 3.577) for Basic.</p>

<p>??? What? Why does it seem that bootstrap is not working at all...</p>

<p>---EDIT---</p>

<p>This glmer is based on 1 participant in a training study, and the random effect structure is (1 + scale(rep) | item)-- note this is the same effect as the one I'm questioning in the CIs.</p>

<p>To make more sense of this, I don't get such an extremely crazy result when looking at a different participant (same model specification, just different measures). For this person, the estimated effect of scale(rep) = 1.358, std. error = 0.336, z-value = 3.45. The Wald CI is (0.585, 2.130) and the bootstrap CI with n=500 is (0.809, 2.599). So, what could be going on here??</p>

<p>A little more about the data for the problematic model/participant: 40 items were probed at 5 different levels of scale(rep), giving 198 observations total (a couple items were probed 4 times only). The measurement is actually a proportion, though, and so it is modeled as a binomial distribution. For example, items 1 and 2 at level 1 of scale(rep) may both be measured as 0.8, but item 1 has a weight of 5 and item 2 a weight of 10, thus modeled as (4 out of 5) and (8 out of 10) binary trials.</p>
"
"0.186306923247712","0.164405141689536","164457","<p>I have seen questions about this on this forum, and I have also asked it myself in a previous post but I still haven't been able to solve my problem. Therefore I am trying again, formulating the question as clearly as I can this time, with as much detailed information as possible. </p>

<p>My data set has a binomial dependent variable, 3 categorical fixed effects and 2 categorical random effects (item and subject). I am using a mixed effects model using glmer. Here is what I entered in R:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + ``(1|item), data=RprodHSNS, family=""binomial"")`
</code></pre>

<p>I get 2 warnings:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.02081 (tol = 0.001, component 11)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
- Rescale variables?`
</code></pre>

<p>My summary looks like this:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
Data: RprodHSNS`


AIC      BIC   logLik deviance df.resid
1400.0   1479.8   -686.0   1372.0     2195 `

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0346 -0.2827 -0.0152  0.2038 20.6578 `

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.475    1.215   
subject (Intercept) 1.900    1.378   
Number of obs: 2209, groups:  item, 54; subject, 45
Fixed effects:`
Estimate Std. Error z value Pr(&gt;|z|)`                             
(Intercept)                -0.61448   42.93639  -0.014 0.988582  
group1                     -1.29254   42.93612  -0.030 0.975984    
context1                    0.09359   42.93587   0.002 0.998261   
context2                   -0.77262    0.22894  -3.375 0.000739***
condition1                  4.99219   46.32672   0.108 0.914186
group1:context1            -0.17781   42.93585  -0.004 0.996696
group1:context2            -0.10551    0.09925  -1.063 0.287741
group1:condition1          -3.07516   46.32653  -0.066 0.947075
context1:condition1        -3.47541   46.32648  -0.075 0.940199
context2:condition1        -0.07293    0.22802  -0.320 0.749087
group1:context1:condition1  2.47882   46.32656   0.054 0.957328
group1:context2:condition1  0.30360    0.09900   3.067 0.002165 **

---

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Correlation of Fixed Effects:
            (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                
context2     0.001  0.000 -0.001                                                              
condition1  -0.297  0.297  0.297  0.000                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001 -0.297                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.000  0.000                                       
grp1:cndtn1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.000                               
cntxt1:cnd1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.001  1.000                        
cntxt2:cnd1  0.000  0.000 -0.001  0.011  0.001  0.000    -0.197 -0.001    -0.001              
grp1:cnt1:1 -0.297  0.297  0.297  0.001  1.000 -0.297    -0.001 -1.000    -1.000  0.001       
grp1:cnt2:1  0.000  0.000  0.001 -0.198  0.000 -0.001     0.252  0.000     0.001 -0.136  0.000
</code></pre>

<p>Extremely high p-values, which does not seem to be possible. </p>

<p>In a previous post I read that one of the problems could be fixed by increasing the amount of iterations by inserting the following in the command: glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000))</p>

<p>So that's what I did:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + (1|item), data=RprodHSNS, family=""binomial"", glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))
</code></pre>

<p>Now, the second warning is gone, but the first one is still there:</p>

<pre><code>&gt; Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.005384 (tol = 0.001, component 7)
</code></pre>

<p>The summary also still looks odd:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
   Data: RprodHSNS
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))`

AIC      BIC   logLik deviance df.resid 
1400.0   1479.8   -686.0   1372.0     2195

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0334 -0.2827 -0.0152  0.2038 20.6610 

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.474    1.214   
subject (Intercept) 1.901    1.379   
Number of obs: 2209, groups:  item, 54; subject, 45

Fixed effects:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.64869   26.29368  -0.025 0.980317    
group1                     -1.25835   26.29352  -0.048 0.961830    
context1                    0.12772   26.29316   0.005 0.996124    
context2                   -0.77265    0.22886  -3.376 0.000735 ***
condition1                  4.97325   22.80050   0.218 0.827335    
group1:context1            -0.21198   26.29303  -0.008 0.993567    
group1:context2            -0.10552    0.09924  -1.063 0.287681    
group1:condition1          -3.05629   22.80004  -0.134 0.893365    
context1:condition1        -3.45656   22.80017  -0.152 0.879500    
context2:condition1        -0.07305    0.22794  -0.320 0.748612    
group1:context1:condition1  2.45996   22.80001   0.108 0.914081    
group1:context2:condition1  0.30347    0.09899   3.066 0.002172 ** 

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                     
context2     0.000  0.000  0.000                                                              
condition1   0.123 -0.123 -0.123 -0.001                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001  0.123                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.001  0.000                                         
grp1:cndtn1 -0.123  0.123  0.123  0.000 -1.000 -0.123    -0.001                               
cntxt1:cnd1 -0.123  0.123  0.123  0.000 -1.000 -0.123     0.000  1.000                        
cntxt2:cnd1  0.000  0.000  0.000  0.011 -0.001  0.000    -0.197  0.001     0.001              
grp1:cnt1:1  0.123 -0.123 -0.123  0.000  1.000  0.123     0.000 -1.000    -1.000 -0.001      
grp1:cnt2:1  0.000 -0.001  0.001 -0.198  0.001 -0.001     0.252 -0.001     0.000 -0.136  0.000
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<p>What I can do to solve this? Or can anyone tell me what this warning even means? (in a way that an R-newbie like myself can understand)</p>
"
"NaN","NaN","167466","<p>I ran a probit regression using the following code:</p>

<pre><code>    m1&lt;-glmer(Success~Name.Origin+(1|Job.ID),family=binomial(link=""probit""))
</code></pre>

<p>However, I am now unsure how to compute the marginal effects and their corresponding standard errors?</p>

<p>Thanks a lot.</p>
"
"0.0316862125262239","0.0647523908238176","167757","<p>I've run a probit regression in R with a random effect and can find no way to get the marginal effects with s.e. and p values.  I have therefore tried to calculate the marginal effects 'by hand' by using the probit scalars and regression coefficients.  However, I do not know how to get p values or standard errors and as far as I have found there is no easy way to do this for a mixed effects probit regression.</p>

<p>My model m1 is</p>

<pre><code> m1&lt;-glmer(Success~Name.Origin+(1|Job.ID),family=binomial(link=""probit""))
</code></pre>
"
"0.0448110714948221","0.045786854649563","167780","<p>I am getting the exact same results for a probit regression and post-hoc tests (simultaneous tests for linear hypotheses) - is this because I have used a dummy variable in the probit model and so it is effectively comparing each factor level to the reference group thus when I run the post-hoc, which is comparing differences between the two groups, that I get the same answers?</p>

<p>This is the model I fitted:</p>

<pre><code> m1&lt;-glmer(Success~Name.Origin+(1|Job.ID),family=binomial(link=""probit""))
</code></pre>

<p>and this is the post hoc test that I did:</p>

<pre><code> summary(glht(m1, lsm(pairwise ~ Name.Origin)))
</code></pre>
"
"0.0776150525706333","0.0793051585718144","168482","<p>I am running a probit regression with a random effect:</p>

<pre><code>m1&lt;-glmer(Binary~Explan+(1|Random),family=binomial(link=""probit""))
</code></pre>

<p>where Explan is a three-level categorical variable. </p>

<p>I want to calculate the mean predicted probabilities for each level of Explan. I tried doing so using this code:</p>

<pre><code>newdata=data.frame(Explan=""First"")
predict(m1,newdata,type=""response"")
</code></pre>

<p>where First is a level of the categorical Explan variable.</p>

<p>However I get the following error message:</p>

<pre><code>Error: (p &lt;- ncol(X)) == ncol(Y) is not TRUE
</code></pre>

<p>Were this a logit model, I would simply strip the model of the intercept and then back-transform the model summary coefficients to get the predicted values that I'm after, but I am unsure of how I would go about this with a mixed-effects probit model. </p>

<p>Any help in extracting the predicted probabilities would be greatly appreciated.</p>
"
"0.173552533625156","0.177331725532977","169549","<p>I am currently writing my master thesis about the effect of an insecticide (clothianidin) on the microflora of bumblebees. I received the bumblebees from an experiment with a nested study design. 16 fields were paired according to land use of the surroundings etc. In each field 2 boxes were placed, which contained 2 hives (colonies) each. </p>

<p>I was trying to determine if the treatment affects the prevalence of certain organisms including ABPV, N.bombi and Snodgrasella as you can see in this data frame:</p>

<pre><code>    structure(list(treatment = structure(c(2L, 2L, 2L, 2L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 
2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L), .Label = c(""Clothianidin"", 
""Control""), class = ""factor""), pair = structure(c(1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 
7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L), .Label = c(""P01"", 
""P02"", ""P03"", ""P04"", ""P05"", ""P10"", ""P11"", ""P12""), class = ""factor""), 
    field = structure(c(6L, 6L, 6L, 6L, 12L, 12L, 12L, 12L, 1L, 
    1L, 1L, 1L, 2L, 2L, 2L, 2L, 10L, 10L, 10L, 10L, 13L, 13L, 
    13L, 13L, 7L, 7L, 7L, 7L, 16L, 16L, 16L, 16L, 8L, 8L, 8L, 
    8L, 9L, 9L, 9L, 9L, 3L, 3L, 3L, 3L, 11L, 11L, 11L, 11L, 4L, 
    4L, 4L, 4L, 5L, 5L, 5L, 5L, 14L, 14L, 14L, 14L, 15L, 15L, 
    15L, 15L), .Label = c(""VR02"", ""VR03"", ""VR04"", ""VR05"", ""VR06"", 
    ""VR07"", ""VR09"", ""VR12"", ""VR13"", ""VR14"", ""VR16"", ""VR17"", ""VR18"", 
    ""VR20"", ""VR21"", ""VR23""), class = ""factor""), box.nested = c(12, 
    11, 12, 11, 23, 24, 23, 24, 1, 1, 2, 2, 4, 3, 3, 4, 20, 20, 
    19, 19, 25, 26, 25, 26, 14, 14, 13, 13, 31, 31, 32, 32, 16, 
    15, 15, 16, 18, 17, 17, 18, 6, 5, 5, 6, 21, 22, 22, 21, 8, 
    8, 7, 7, 9, 9, 10, 10, 28, 27, 28, 27, 29, 30, 30, 29), hive.nested = c(24L, 
    21L, 23L, 22L, 46L, 48L, 45L, 47L, 2L, 1L, 4L, 3L, 8L, 5L, 
    6L, 7L, 40L, 39L, 38L, 37L, 49L, 52L, 50L, 51L, 27L, 28L, 
    26L, 25L, 62L, 61L, 64L, 63L, 31L, 29L, 30L, 32L, 36L, 34L, 
    33L, 35L, 12L, 9L, 10L, 11L, 41L, 43L, 44L, 42L, 15L, 16L, 
    13L, 14L, 17L, 18L, 20L, 19L, 55L, 54L, 56L, 53L, 58L, 59L, 
    60L, 57L), ABPV.detected = structure(c(0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
    1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0), .Dim = c(64L, 1L), .Dimnames = list(NULL, ""ABPV.detected"")), 
    N.bombi.detected = structure(c(0, 0, 0, 0, 0, 0, 0, 0, 1, 
    1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), .Dim = c(64L, 
    1L), .Dimnames = list(NULL, ""N.bombi.detected"")), Snodgrasella.detected = structure(c(0, 
    1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 
    1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1), .Dim = c(64L, 1L), .Dimnames = list(NULL, 
        ""Snodgrasella.detected""))), .Names = c(""treatment"", ""pair"", 
""field"", ""box.nested"", ""hive.nested"", ""ABPV.detected"", ""N.bombi.detected"", 
""Snodgrasella.detected""), class = ""data.frame"", row.names = c(NA, 
-64L))
</code></pre>

<p>I was trying estimate the treatment effect with a model that included pair, field, box and hive as (nested) random effects:</p>

<pre><code>library(lme4)
ABPV.prev &lt;- glmer(ABPV.detected ~ treatment 
                   + (1|pair/field/box.nested/hive.nested)
                   ,data=data.f, 
                   family=binomial)
summary(ABPV.prev)
</code></pre>

<p>The models of ABPV and N. bombi failed to converge, because I have so many zeros. ABPV was only found in one pair and N. bombi was only found in 2 pairs.</p>

<pre><code>Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.100788 (tol = 0.001, component 2)
</code></pre>

<p>The p-values of the models did indicate significant treatment effects, but I guess it's pair or field effects that cause the variation...</p>

<p>In another forum I read that it can be tested whether the failure to converge represents a real problem using this function..</p>

<pre><code>relgrad &lt;- with(ABPV.prev@optinfo$derivs,solve(Hessian,gradient))
max(abs(relgrad))
</code></pre>

<p>... and it does as the p-value is rather large p=0.1 (for ABPV).</p>

<p>I removed some of the random effects and it works and it works e.g. when I remove both hive.nested and box.nested from the model, though increasing the AIC (why?). </p>

<p>I also tried to include pair as a fixed effect: </p>

<pre><code>library(lme4)
ABPV.prev &lt;- glmer(ABPV.detected ~ treatment + pair 
                   + (1|field/box.nested/hive.nested)
                   ,data=data.f, 
                   family=binomial)
</code></pre>

<p>but it produced an error:</p>

<pre><code>Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate
</code></pre>

<p>My questions are </p>

<ol>
<li>How do I judge which random effects to include and which not? </li>
<li>How do I test if it is a pair (or field effect) not a treatment effect that causes the variation?</li>
</ol>

<p>N.B. In the Snodgrasella model it makes absolutely no difference if I exclude, any or all of the random effects. Why is that?</p>

<p>Hope you can help me, thanks!</p>
"
"0.142294263046161","0.145392790714993","174532","<p>I have been working on my PC to analyse my multilevel data. I am now working on a Mac and have run the same model. Some of the output is the same but some is quite different. I can't seem to work out why. Here is the model:</p>

<pre><code>&gt; loss.2 &lt;- glmer.nb(Loss_across.Chain ~ Posn.c*Valence.c + (Valence.c|mood.c/Chain), data = FinalData_forpoisson, control = glmerControl(optimizer = ""bobyqa"", check.conv.grad = .makeCC(""warning"", 0.05)))
</code></pre>

<p>On the PC I got this output: </p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: Negative Binomial(4.9852)  ( log )
Formula: Loss_across.Chain ~ Posn.c * Valence.c + (Valence.c | mood.c/Chain)
   Data: FinalData_forpoisson
Control: ..3

     AIC      BIC   logLik deviance df.resid 
  1894.7   1945.3   -936.4   1872.7      725 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.3882 -0.7225 -0.5190  0.4375  7.1873 

Random effects:
 Groups       Name        Variance  Std.Dev.  Corr
 Chain:mood.c (Intercept) 8.782e-15 9.371e-08     
              Valence.c   9.608e-15 9.802e-08 0.48
 mood.c       (Intercept) 0.000e+00 0.000e+00     
              Valence.c   1.654e-14 1.286e-07  NaN
Number of obs: 736, groups:  Chain:mood.c, 92; mood.c, 2

Fixed effects:
                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -0.19255    0.04794  -4.016 5.92e-05 ***
Posn.c           -0.61011    0.04122 -14.800  &lt; 2e-16 ***
Valence.c        -0.27372    0.09589  -2.855  0.00431 ** 
Posn.c:Valence.c  0.38043    0.08245   4.614 3.95e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Posn.c Vlnc.c
Posn.c       0.491              
Valence.c    0.029 -0.090       
Psn.c:Vlnc. -0.090  0.062  0.491
</code></pre>

<p>On the Mac I got this output:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: Negative Binomial(4.9852)  ( log )
Formula: Loss_across.Chain ~ Posn.c * Valence.c + (Valence.c | mood.c/Chain)
   Data: FinalData_forpoisson
Control: ..3

     AIC      BIC   logLik deviance df.resid 
  1894.7   1945.3   -936.4   1872.7      725 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.3882 -0.7225 -0.5190  0.4375  7.1873 

Random effects:
 Groups       Name        Variance  Std.Dev.  Corr
 Chain:mood.c (Intercept) 1.242e-13 3.524e-07     
              Valence.c   4.724e-13 6.873e-07 0.98
 mood.c       (Intercept) 7.998e-16 2.828e-08     
              Valence.c   3.217e-14 1.793e-07 1.00
Number of obs: 736, groups:  Chain:mood.c, 92; mood.c, 2

Fixed effects:
                   Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)       2.947e-05  4.794e-02   0.001    1.000
Posn.c            7.441e-05  4.122e-02   0.002    0.999
Valence.c        -4.011e-05  9.589e-02   0.000    1.000
Posn.c:Valence.c -6.672e-05  8.245e-02  -0.001    0.999

Correlation of Fixed Effects:
            (Intr) Posn.c Vlnc.c
Posn.c       0.491              
Valence.c    0.029 -0.090       
Psn.c:Vlnc. -0.090  0.062  0.491
</code></pre>

<p>Does anyone know why the output might be different across the two platforms and how I might be able to get them to align?</p>
"
"0.0776150525706333","0.0793051585718144","175191","<p>I've ran an experiment with binary data and subject specific random effects using R's lme4 package:</p>

<pre><code>m8l &lt;- glmer(correct~offset(alpha) + r*rsgn + smoothed + n + (1|subj_id), data=subset(full.sub, plot == ""L""), family=""binomial"")
</code></pre>

<p>I'm trying to use the residuals to check the model fit and eventually test for significances. I thought by design the residual mean would always be close to zero, but <code>mean(resid(m8l))</code> gives me a value of -0.08.</p>

<p>My residual plots are:</p>

<p><img src=""http://i.imgur.com/MriAgLs.png"" alt=""""></p>

<p><img src=""http://i.imgur.com/eGE6stU.png"" alt=""""></p>

<p>The huge amount of negative residuals seems problematic to me. An otherwise identical model on a different subset of the data (plot != ""L"") returned close to perfectly normal iid residuals with a mean of zero, so I wouldn't think the model is completely wrong.</p>

<p>Is there something going on here that could've given me this result? Alternatively, does it matter? My sample size is about 800.</p>
"
"0.155230105141267","0.145392790714993","175773","<p>I have a hypothesis testing model in which I would like to know if environmental variables I collected influences the probability a bacteria on a given amphibian kills a pathogen. </p>

<p>Following Crawley's R intro to stats book on binomial models I do this:</p>

<pre><code>y &lt;- cbind(df$Antipathogen, df$Total_isolated - df$Antipathogen)
</code></pre>

<p>I still follow Crawley, but bring in a glmer model (using help from online) as I want to examine this at the Site level and not transect level. So, I make transect a random effect. I sampled three sites. One site has one transect, the second site has two transects, and the third site was sampled along an altitudinal gradient and has seven transects.</p>

<p>This is my model:</p>

<pre><code>model &lt;- glmer(y ~ Site + Species + sex + BodyCon + Leaf_litter + (1|Transect), 
               data = df, family = binomial)
</code></pre>

<p>I use the Anova function in car to see which terms are significant when they are introduced into the model</p>

<pre><code>Anova(model, type = ""III"", test.statistic = ""Chisq"")
</code></pre>

<p>I get this:</p>

<pre><code>Response: y
              Chisq Df Pr(&gt;Chisq)    
(Intercept) 21.3200  1  3.887e-06 ***
Site        12.0107  2   0.002466 **
Species      0.0617  2   0.969644  
sex          0.2313  2   0.890785    
BodyCon      0.7058  1   0.400851    
Leaf_litter  2.8763  1   0.089890 . 
</code></pre>

<p>I am starting to understand the function lsmeans in lsmeans package to look at pairwise comparisons to figure out how each of my sites differ from one another. </p>

<p>This is where my question comes in:</p>

<p>What is the appropriate approach -- use the full model and apply the lsmeans function to the full model </p>

<pre><code>lsmeans(model, pairwise~Site, adjust = ""tukey"")
</code></pre>

<p>OR -- remove non-significant terms in a step-wise manner following Crawleyâ€™s (2007, pg. 325) Principle of Parsimony to simplify the full model. And use </p>

<pre><code>model2 &lt;- update(model, ~.-Species)
anova(model, model2, test = ""Chisq"")
</code></pre>

<p>To make sure removing the terms is valid.</p>

<p>Then, I can do lsmeans on the final model that now only contains the only significant variable -- site. </p>

<pre><code>lsmeans(model5, pairwise~Site, adjust = ""tukey"")
</code></pre>

<p>Regardless of how I do it I still get a significant p-value for Site, and the pairwise comparisons give similar results. The p-value is only much lower when I do lsmeans on the reduced model. </p>

<p>I don't know what the better approach is, and doing some basic searching I could not find a similar question. </p>
"
"0.100200602007025","0.102382519472325","176294","<p>I have tried to read documentation with no luck. </p>

<p>Let suppose i have a hundred mice and 50 of those mice has a condition X. I am studying the deliveries of the animals and want to know if an incidence of the event Y is more common among the animals with the condition X during the delivery. Some individuals of the animals has only one delivery but some of the animals can have more than one delivery. I want also standardise other factors(nominal and linear). </p>

<p>I have the following variables: 
ID (Same ID can occur on several rows if there is many deliveries), 
Condition.X, 
Color, 
Age, 
Weight, 
Event.Y(0/1)</p>

<p>I am using the following R-code:</p>

<blockquote>
  <p>model &lt;- glmer(Event.Y ~ Condition.X + Color + Age + Weight + (1 | ID), family = binomial)
  nullmodel &lt;- glmer(Event.Y ~  Color + Age + Weight + (1 | ID), family = binomial)
  anova(nullmodel, model)</p>
</blockquote>

<p>If I got it right the method for approximation above is Laplace. </p>

<p>I have also analysed data with SAS and by using RSPL or MMPL methods the significance is better. </p>

<p><strong>My question:</strong> Is it possible to use similar methods to RSPL/MMPL with glmer-function in R? </p>
"
"0.180361083612646","0.184288535050185","177960","<p>I'm trying to assess the effect of showing more impressions on a user. I want to study if users who saw more ads are more likely to make a purchase onsite. To do so I've created a multilevel model. I grouped users into 10 groups averaging their scores (we score users based on a number of factors). So basically I end up having 10 groups (from 0 to 9), where on group 9 I assume to have the best users, and on group 0 the worst.</p>

<pre><code>  picbucket mcuserid impressions mediacostcpm is_buyer gr.impressions gr.mediacostcpm
1         0 1           1        0.460        0       3.632794        2.767509
2         0 2           2        5.000        0       3.632794        2.767509
3         0 3           1        4.590        0       3.632794        2.767509
4         0 4           1        0.590        0       3.632794        2.767509
5         0 5           1        5.000        0       3.632794        2.767509
6         0 6           1        0.315        0       3.632794        2.767509
</code></pre>

<p>I think a multilevel model could be advantageous here because I'm expecting to see different effects on each group. On the best users I'm expecting an additional impression could have a higher impact, whereas on bad users and additional impression may be worthless. It could also be possible the opposite though. So that users which generally higher score will convert even without the need of serving them more impressions, whereas on mid groups additional impressions tend to change their behaviour. </p>

<p>A good model representation could be:</p>

<p>$y_{i} = \alpha_{j[i]} + X_{i}\beta + \epsilon_{i}$</p>

<p>The second level of the model will then be:</p>

<p>$\alpha_{j} = \mu_{\alpha} + \eta_{j}, \text{ with } \eta_{j} \sim N(0, \sigma_{\alpha}^{2})$</p>

<p>On the first level I want to include as a predictor how many impression a user saw. On the second level I want to include the average impressions a user saw within its group and the average media cost for the impressions we served on that user. I've used the package <code>lme4</code> in R to build my model.</p>

<pre><code>glmer(formula = is_buyer ~ impressions + mediacostcpm + (1 + 
    gr.impressions + gr.mediacostcpm | picbucket), data = new.df, 
    family = binomial())
             coef.est coef.se
(Intercept)  -7.42     0.33  
impressions   0.00     0.02  
mediacostcpm  0.03     0.01  

Error terms:
 Groups    Name            Std.Dev. Corr        
 picbucket (Intercept)     7.86                 
           gr.impressions  2.22     -0.99       
           gr.mediacostcpm 0.57     -0.68  0.60 
 Residual                  1.00                 
---
number of obs: 103146, groups: picbucket, 10
AIC = 2755.4, DIC = 2680.5
deviance = 2708.9 
</code></pre>

<p>This is my first experiment with multilevel modeling so I would like to make sure I don't misunderstand the results of my model. </p>

<p>From what I see here, the <code>impressions</code> predictor on the first level is useless. Its coefficient is zero and its standard deviation is very small. This could be due to the fact I'm including a group average on the second level for the impression count (<code>gr.impressions</code>). So, on any group (<code>picbucket</code>), serving more impressions than the average doesn't tell us much about the likelihood of a cookie to convert.</p>

<p>The average media cost on the first level is however an interesting one. Generally, within a group, if I spend a bit more for every impression I should increase the probability of generating conversions. This is probably due to inventory quality. Better inventory costs more, but also has better changes to be viewable inventory.</p>

<p>The coefficients on the group level instead tell you how much they contribute on explaining the group slope. So in this case the average number of impressions at the group level seems to explain quite a significant part of the group slope. </p>

<p>Interestingly, at the group level <code>gr.impressions</code> seem to be a very useful predictor, but at the within group level its usefulness is limited. The opposite applies to the <code>mediacostcpm</code>.</p>

<p>Am I interpreting these results correctly? How can I tell if the model has a good fit? Please note I've used a binomial regression because the dependent variable, <code>is_buyer</code> can take only 0 or 1 (one being the user made a purchase).</p>
"
"0.195326932197706","0.199580272360031","178152","<p>I want to fit diurnal cortisol profiles using linear mixed models, as was done by previous researchers (e.g. <a href=""http://www.sciencedirect.com/science/article/pii/S0306453005000491"" rel=""nofollow"" title=""Estimating between- and within-individual variation in cortisol levels using multilevel models"">Estimating between- and within-individual variation in cortisol levels using multilevel models</a>). </p>

<p>I have 4 samples of each individual for 1 to 3 days and the exact time of taking. I am especially interested in the individual intercepts and slopes of these profiles (i.e. the effect of time), which I NEED for other analyses, as intercept and slope are indicators of different aspects of stress regulation, theoretically.
Because cortisol is not normally distributed most researches use the natural logarithm before estimation. But when I look at the actual distribution, it looks like a poisson distribution (after rounding up). However, after fitting the model I checked for overdispersion, and unfortunately it is there. Thus, a negative binomial distribution might be more adequate.</p>

<p>So for sake of comparison I utilized all three models using the lme4 package.</p>

<p>For the log model i used</p>

<pre><code>lmer(log(cortisol) ~ time + (time|id) + (1|day/id), data=data)
</code></pre>

<p>This model gives me an unconditional RÂ² of 0.48 and a conditional RÂ² of 0.57. Unfortunately, the random effects are perfectly negative correlated, which is probably due to the low variance of random effects. From a statistical perspective a random intercept random slope model seems not appropriate. 
I also tried setting the random effects being independent of each other by using (zeit||id) instead, but this just gives me no variation for the random intercept.</p>

<p>For the second model i used</p>

<pre><code> glmer(round(cortisol, digits=0) ~ time + (time|id) + (1|day/id), data=data, family = poisson(link=log), control=glmerControl(optimizer=""bobyqa"")
</code></pre>

<p>This model gives me a better unconditional RÂ² of 0.56 and a much better conditional RÂ² of 0.94. Also, the correlation between random effects is -.44, which is what I would expect, and would also would like it to be around. However, I have the big problem of overdispersion (or do I???), which can be accounted for by defining an individual-level random effect as is suggested <a href=""http://r.789695.n4.nabble.com/Mixed-effects-model-for-overdispersed-count-data-td3010455.html"" rel=""nofollow"">here</a>. But this will result in the model been similar to first model; almost no variance of random effects and extreme correlation.</p>

<p>And lastly for the negative binomial model is used</p>

<pre><code>glmer.nb(round(cortisol, digits=0) ~ time + (time|id) + (1|day/id), data=data, control=glmerControl(optimizer=""bobyqa"")
</code></pre>

<p>This model gives me the worst unconditional RÂ² of 0.44 and a conditional RÂ² of 0.48. Also, the variance of the random intercept is zero, therefore correlation between random effects is not to be calculated.</p>

<p>So my questions are</p>

<ol>
<li><p>How wrong are the predictions in the second model (poisson distribution) under the consideration of overdispersion? How bad is overdispersion? [for me it is the best model in terms of variance of random effects, expected correlation of random effects and model fit]</p></li>
<li><p>How could I force random effects to have a considerable variance in the first model (and not being totally correlated)?</p></li>
</ol>

<p>Thank you for your help, and please let me know if I can provide additional information. </p>
"
"0.127271857991781","0.141865380426382","178551","<p>I'm using a binomial glmer mixed effects model using and I have two questions. </p>

<ol>
<li>One variable that I have, 'stimulus' has 12 levels. The levels were not randomly selected, so I have used it as a fixed variable in the basic model but R seems not to like it (at least this is my interpretation) given the way the output looks and the amount of time R takes to process it.</li>
</ol>

<p>m0.1 &lt;- glmer(match ~ Listgp + stimulus + (1|Listener), data = PATdata, family = ""binomial"")</p>

<blockquote>
  <p>summary(m0.1)
  Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
  glmerMod]
   Family: binomial  ( logit )
  Formula: match ~ Listgp + stimulus + (1 | Listener)
     Data: PATdata</p>
</blockquote>

<pre><code> AIC      BIC   logLik deviance df.resid 
</code></pre>

<p>5154.3   5259.5  -2562.2   5124.3     8193 </p>

<p>Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-25.0764  -0.2706  -0.1939   0.2472  10.5131 </p>

<p>Random effects:
 Groups   Name        Variance Std.Dev.
 Listener (Intercept) 1.743    1.32<br>
Number of obs: 8208, groups:  Listener, 228</p>

<p>Fixed effects:
              Estimate Std. Error z value Pr(>|z|)<br>
(Intercept)     2.7561     0.2657  10.371  &lt; 2e-16 <strong>*
ListgpTA        0.1741     0.3147   0.553 0.580128<br>
ListgpTQ        0.0810     0.2575   0.315 0.753094<br>
stimulushaaDD  -5.4415     0.2071 -26.272  &lt; 2e-16 <em></strong>
stimulushad    -4.2953     0.1822 -23.569  &lt; 2e-16 <strong></em>
stimulushaDD   -5.4946     0.2086 -26.337  &lt; 2e-16 <em></strong>
stimulushid    -5.1519     0.1994 -25.832  &lt; 2e-16 <strong></em>
stimulushiDD   -0.6708     0.1801  -3.724 0.000196 <em></strong>
stimulushiid   -5.8124     0.2186 -26.593  &lt; 2e-16 <strong></em>
stimulushiiDD  -5.5101     0.2091 -26.353  &lt; 2e-16 <em></strong>
stimulushud    -0.2016     0.1915  -1.053 0.292345<br>
stimulushuDD   -5.6188     0.2123 -26.462  &lt; 2e-16 <strong></em>
stimulushuud   -5.6107     0.2121 -26.450  &lt; 2e-16 *</strong></p>

<h2>stimulushuuDD  -5.3207     0.2038 -26.109  &lt; 2e-16 ***</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:
              (Intr) LstgTA LstgTQ stimulushaaDD stimulushad stimulushaDD
ListgpTA      -0.613<br>
ListgpTQ      -0.755  0.636<br>
stimulushaaDD -0.394 -0.007  0.004<br>
stimulushad   -0.440 -0.006  0.005  0.605<br>
stimulushaDD  -0.392 -0.007  0.003  0.555         0.601<br>
stimulushid   -0.407 -0.007  0.004  0.572         0.624       0.569<br>
stimulushiDD  -0.414  0.000  0.001  0.534         0.606       0.530<br>
stimulushiid  -0.376 -0.006  0.003  0.536         0.578       0.533<br>
stimulushiiDD -0.391 -0.007  0.003  0.554         0.600       0.551<br>
stimulushud   -0.386  0.000  0.000  0.497         0.564       0.493<br>
stimulushuDD  -0.385 -0.007  0.003  0.548         0.592       0.545<br>
stimulushuud  -0.386 -0.007  0.003  0.548         0.593       0.545<br>
stimulushuuDD -0.400 -0.007  0.004  0.564         0.613       0.561<br>
              stimulushid stimulushiDD stimulushiid stimulushiiDD stimulushud
ListgpTA<br>
ListgpTQ<br>
stimulushaaDD<br>
stimulushad<br>
stimulushaDD<br>
stimulushid<br>
stimulushiDD   0.554<br>
stimulushiid   0.549       0.506<br>
stimulushiiDD  0.568       0.529        0.533<br>
stimulushud    0.516       0.569        0.471        0.492<br>
stimulushuDD   0.562       0.521        0.527        0.544         0.484<br>
stimulushuud   0.562       0.522        0.528        0.545         0.485<br>
stimulushuuDD  0.579       0.543        0.542        0.560         0.505<br>
              stimulushuDD stimulushuud
ListgpTA<br>
ListgpTQ<br>
stimulushaaDD<br>
stimulushad<br>
stimulushaDD<br>
stimulushid<br>
stimulushiDD<br>
stimulushiid<br>
stimulushiiDD<br>
stimulushud<br>
stimulushuDD<br>
stimulushuud   0.539<br>
stimulushuuDD  0.554        0.554 </p>

<p>So, my question is, can I consider 'stimulus' as a random effect instead?</p>

<blockquote>
  <p>m0.1 &lt;- glmer(match ~ Listgp + (1|stimulus) + (1|Listener), data = PATdata, family = ""binomial"")
  summary(m0.1)
  Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
  glmerMod]
   Family: binomial  ( logit )
  Formula: match ~ Listgp + (1 | stimulus) + (1 | Listener)
     Data: PATdata</p>
</blockquote>

<pre><code> AIC      BIC   logLik deviance df.resid 
</code></pre>

<p>5218.3   5253.4  -2604.2   5208.3     8203 </p>

<p>Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-21.9276  -0.2804  -0.2059   0.2740   9.4275 </p>

<p>Random effects:
 Groups   Name        Variance Std.Dev.
 Listener (Intercept) 1.676    1.294<br>
 stimulus (Intercept) 4.949    2.225<br>
Number of obs: 8208, groups:  Listener, 228; stimulus, 12</p>

<p>Fixed effects:
            Estimate Std. Error z value Pr(>|z|)<br>
(Intercept)  -1.3754     0.6792  -2.025   0.0429 *
ListgpTA      0.2284     0.3073   0.743   0.4572  </p>

<h2>ListgpTQ      0.1432     0.2513   0.570   0.5687</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:
         (Intr) LstgTA
ListgpTA -0.235<br>
ListgpTQ -0.288  0.636</p>

<blockquote>
  <p></p>
</blockquote>

<p>Appreciating your help,</p>

<p>Shad</p>
"
"0.134433214484466","0.14880727761108","178682","<p>I'm using a binomial glmer mixed effects model using and I have two questions.</p>

<p>One variable that I have, 'stimulus' has 12 levels. The levels were not randomly selected, so I have used it as a fixed variable in the basic model but R seems not to like it (at least this is my interpretation) given the way the output looks and the amount of time R takes to process it.</p>

<pre><code>m0.1 &lt;- glmer(match ~ Listgp + stimulus + (1|Listener), data = PATdata, family = ""binomial"")

summary(m0.1) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod] Family: binomial ( logit ) Formula: match ~ Listgp + stimulus + (1 | Listener) Data: PATdata
 AIC      BIC   logLik deviance df.resid 
5154.3 5259.5 -2562.2 5124.3 8193

Scaled residuals: Min 1Q Median 3Q Max -25.0764 -0.2706 -0.1939 0.2472 10.5131

Random effects: Groups Name Variance Std.Dev. Listener (Intercept) 1.743 1.32
Number of obs: 8208, groups: Listener, 228

Fixed effects: Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) 2.7561 0.2657 10.371 &lt; 2e-16 * ListgpTA 0.1741 0.3147 0.553 0.580128
ListgpTQ 0.0810 0.2575 0.315 0.753094
stimulushaaDD -5.4415 0.2071 -26.272 &lt; 2e-16 stimulushad -4.2953 0.1822 -23.569 &lt; 2e-16 stimulushaDD -5.4946 0.2086 -26.337 &lt; 2e-16 stimulushid -5.1519 0.1994 -25.832 &lt; 2e-16 stimulushiDD -0.6708 0.1801 -3.724 0.000196 stimulushiid -5.8124 0.2186 -26.593 &lt; 2e-16 stimulushiiDD -5.5101 0.2091 -26.353 &lt; 2e-16 stimulushud -0.2016 0.1915 -1.053 0.292345
stimulushuDD -5.6188 0.2123 -26.462 &lt; 2e-16 stimulushuud -5.6107 0.2121 -26.450 &lt; 2e-16 *

stimulushuuDD -5.3207 0.2038 -26.109 &lt; 2e-16 ***

Signif. codes: 0 â€˜â€™ 0.001 â€˜â€™ 0.01 â€˜â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects: (Intr) LstgTA LstgTQ stimulushaaDD stimulushad stimulushaDD ListgpTA -0.613
ListgpTQ -0.755 0.636
stimulushaaDD -0.394 -0.007 0.004
stimulushad -0.440 -0.006 0.005 0.605
stimulushaDD -0.392 -0.007 0.003 0.555 0.601
stimulushid -0.407 -0.007 0.004 0.572 0.624 0.569
stimulushiDD -0.414 0.000 0.001 0.534 0.606 0.530
stimulushiid -0.376 -0.006 0.003 0.536 0.578 0.533
stimulushiiDD -0.391 -0.007 0.003 0.554 0.600 0.551
stimulushud -0.386 0.000 0.000 0.497 0.564 0.493
stimulushuDD -0.385 -0.007 0.003 0.548 0.592 0.545
stimulushuud -0.386 -0.007 0.003 0.548 0.593 0.545
stimulushuuDD -0.400 -0.007 0.004 0.564 0.613 0.561
stimulushid stimulushiDD stimulushiid stimulushiiDD stimulushud ListgpTA
ListgpTQ
stimulushaaDD
stimulushad
stimulushaDD
stimulushid
stimulushiDD 0.554
stimulushiid 0.549 0.506
stimulushiiDD 0.568 0.529 0.533
stimulushud 0.516 0.569 0.471 0.492
stimulushuDD 0.562 0.521 0.527 0.544 0.484
stimulushuud 0.562 0.522 0.528 0.545 0.485
stimulushuuDD 0.579 0.543 0.542 0.560 0.505
stimulushuDD stimulushuud ListgpTA
ListgpTQ
stimulushaaDD
stimulushad
stimulushaDD
stimulushid
stimulushiDD
stimulushiid
stimulushiiDD
stimulushud
stimulushuDD
stimulushuud 0.539
stimulushuuDD 0.554 0.554
</code></pre>

<p>So, my question is, can I consider 'stimulus' as a random effect instead?</p>

<pre><code>m0.1 &lt;- glmer(match ~ Listgp + (1|stimulus) + (1|Listener), data = PATdata, family = ""binomial"") summary(m0.1) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod] Family: binomial ( logit ) Formula: match ~ Listgp + (1 | stimulus) + (1 | Listener) Data: PATdata
 AIC      BIC   logLik deviance df.resid 
5218.3 5253.4 -2604.2 5208.3 8203

Scaled residuals: Min 1Q Median 3Q Max -21.9276 -0.2804 -0.2059 0.2740 9.4275

Random effects: Groups Name Variance Std.Dev. Listener (Intercept) 1.676 1.294
stimulus (Intercept) 4.949 2.225
Number of obs: 8208, groups: Listener, 228; stimulus, 12

Fixed effects: Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.3754 0.6792 -2.025 0.0429 * ListgpTA 0.2284 0.3073 0.743 0.4572

ListgpTQ 0.1432 0.2513 0.570 0.5687

Signif. codes: 0 â€˜â€™ 0.001 â€˜â€™ 0.01 â€˜â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects: (Intr) LstgTA ListgpTA -0.235
ListgpTQ -0.288 0.636
</code></pre>
"
"0.0672166072422331","0.091573709299126","179668","<p>So I get the non-integer #successes in a binomial glm! warning, which has been asked about many times and I understand what it is. My dv is a % measure of accuracy, and these have weights so that they can be modeled using glmer(family=binomial). Sometimes however, people received fractional scores-- so although it is correct to think of  them having 5 trials, they might have earned a 0.9 when the weights say it is 5, because they were awarded 4.5/5 as correct. Hence the non-integer successes.</p>

<p>Of course R only gives you a warning here, the models converge and the results look sensible. If I round up or down (to 4/5 or 5/5) to force the data to have only integer successes, the estimates change only slightly-- so it's clear to me that whatever R does with these non-integer successes, it isn't crazy.</p>

<p>However I'm now super worried about understanding WHAT it does with these non-integer successes, and why the warning is there if the estimates seem fine... do I need to be doing something else to model these data even though the results look valid?</p>
"
"0.190117275157343","0.194257172471453","180344","<p>I am new with R and I am trying to use multilevel modelling for my dataset using the function <code>glmer</code> (for a binomial outcome variable) and <code>lmer</code> for a continuous one.</p>

<p>I have 4 experimental groups (<code>Treatments</code>) and in each group I am measuring the same outcome variables 4 time for each individual, <code>Conc</code> is binomial (0/1) while<code>PosQ</code> is continuous
I am treating the variables as: <code>Time</code> =ordered, <code>Conc</code> =Factor with 2 levels, <code>Treatment</code> = Factor with 4 levels, <code>PosQ</code> = Integer (no decimal values) Here is an example of my datafile</p>

<pre><code>ID   Time   Conc   Treatm    PosQ

1    1     1        1       6   
1    2     1        1       12   
1    3     1        1       14  
1    4     1        1       15 
2    1     0        3       20 
2    2     0        3       12
2    3     0        3       8
2    4     0        3       6
</code></pre>

<p>It is a 3 level repeated measure design and I want to test the effects of <code>Treatment</code> and <code>Time</code> on my outcome variables. 
The variables are nested and not crossed, each individual belong only to 1 of the 4 experimental groups and I made 4 measurement on each individual. 
So from the furthest to the nearest Treatment is nested with <code>ID</code> that is nested with <code>Time</code> (express the repeated measurement)</p>

<p>I want to measure the interaction effect of <code>Time</code> and <code>Treatment</code> (I Expect them to be better at the end of the 4 measurement according to the treatment group they belong to and to the pass of time). 
I am using multilevel model because I want to take into consideration also individual differences into account 
Here are the formulas I was using:</p>

<pre><code>BinomialOutcomeVariable &lt;- glmer(Conc ~ Treatment * Time + (1| Treatm) + (1|Treatm:ID) + (1|Treatm/ID/Time), data = analyses.4, family = binomial(link=""logit""))

ContinuousVariable &lt;- lmer(PosQ ~ Treatm * Sequ + (1| Treatm) + (1|Treatm:ID) + (1|Treatm/ID/Time), data = analyses.4)
</code></pre>

<p>Are those formulas correct? Can be reduced? because when I run the analyses for continuous variable I receive this warning:</p>

<p>1: <code>number of levels of each grouping factor must be &lt; number of observations</code> </p>

<p>2: <code>grouping factors with &lt; 5 sampled levels may give unreliable estimates</code> </p>

<p>3: <code>In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
   Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?</code></p>

<p>Instead if I use this formula I have no problems for only 1 of the continuous variables</p>

<pre><code>model &lt;- lmer(PosQ ~ Treatm * Time + (1| Treatm/ID), data = analyses.4)
</code></pre>

<p>Is the same formula as before? 
Does R understand that <code>Time</code> is nested? 
If I use <code>isNested</code> function it says that they are not nested.</p>

<p>Any help, idea, suggestion is really appreciated.</p>

<p>Thanks in advance</p>
"
"0.184760780970256","0.188784037984951","180904","<p>I have a large data set where I relate the response variable to multiple explanatory variables; since I have different areas I have also included a random factor. 
The response variable is binomial and therefore I use the <code>glmer</code> function from the <code>lme4</code> package. 
The explanatory variables have different scales and to be able to compare the estimates I wanted to standardize the estimates. 
For that I use a <a href=""http://www.stat.columbia.edu/~gelman/research/unpublished/standardizing.pdf"" rel=""nofollow"">standardisation method</a> that has been developed by Gelman (2007), which is available in the <code>arm</code> package. 
Another method would be fine as well, however I use this for a different model, and I would like to use the same method to standardize my data. </p>

<p>However if I use this method, I get different $p$-values: </p>

<pre><code># without standardized data: 
model1 &lt;- glmer(bembryo ~ (s_edlength + s_bplength + s_tide)^2 + (1|Areasite), family=binomial(link = ""logit""), nAGQ = 1, data=data)

Fixed effects:
                      Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)           -1.81791    2.86350  -0.635   0.5255  
s_edlength            12.33513    5.52290   2.233   0.0255 *
s_bplength            -8.77016    4.74700  -1.847   0.0647 .
s_tide                 1.54429    1.38453   1.115   0.2647  
s_edlength:s_bplength -0.01579    0.14525  -0.109   0.9134  
s_edlength:s_tide     -4.77805    2.23256  -2.140   0.0323 *
s_bplength:s_tide      3.47744    1.89254   1.837   0.0661 .   

# With standardized data: 

model.full.stan &lt;- standardize(model1)

Fixed effects:
                          Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 3.1441     0.7192   4.372 1.23e-05 ***
z.s_edlength                5.9579     2.4137   2.468   0.0136 *  
z.s_bplength               -4.0340     2.1221  -1.901   0.0573 .  
z.s_tide                   -1.3594     1.1632  -1.169   0.2425    
z.s_edlength:z.s_bplength  -0.1263     1.2467  -0.101   0.9193    
z.s_edlength:z.s_tide     -10.4140     4.9042  -2.123   0.0337 *  
z.s_bplength:z.s_tide       7.9670     4.3625   1.826   0.0678 . 
</code></pre>

<p>I am not really sure why this is happening. 
I checked if it depends on the standardization method I use.
However, if I just use the function <code>rescale</code> to scale my explanatory variables I also get different $p$-values. 
I do not get different $p$-values when there is only one explanatory variables left, however that is not really helpful. </p>

<p>This same problem occurs when I use a <code>lme</code> function from the <code>nlme</code> package. 
Although for this function the method of Gelman (2007) is not possible, I also get different $p$-values compared to the non-standardized model. </p>

<p>I am not sure why this is happening and I really would like to use standardized estimates, so I would hope that someone has a idea why this is happening. </p>
"
"0.0896221429896442","0.091573709299126","181124","<p>I want to make a GLMM test (in R) to explain the Crop by Temperature and Rainfall from different months. T5=Temperature of May, T6=Temperature of June...etc</p>

<p>I know (or at least what I read) that because my data are counts I should do it with a GLMM and using a Poisson distribution or a negative binomial.
The problem is that I would like to put as random effects: <code>SiteID</code>, <code>Year</code> and <code>Species</code>. </p>

<p>And when I do the analysis I get more than 50 warnings like:
In dpois(y, mu, log = TRUE) : non-integer x = 232.800000</p>

<p>My model is like (I did it with the MASS package): </p>

<pre><code>model &lt;- glm.nb(Crop ~T5+T6+T7+T8+R5+R6+R7+R8+
     (1|SiteID)+(1|Year)+(1|Species),data = my.data)
</code></pre>

<p>I don't know if I'm doing this in a good way or if you have any suggestions about how to run this analysis (I'm a beginner with R and with GLMMs...)</p>

<p>This is the structure of my data (I have more variables than what I mentioned):</p>

<pre><code>str(my.data)
'data.frame':   7352 obs. of  30 variables:
 $ Species: Factor w/ 4 levels ""Betulapendula"",..: 1 1 1 1 1 1 1 1 1 1 ...
     $ SiteID : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Year   : int  1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 ...
     $ Crop   : num  133 2257 347 92 215 ...
 $ T5     : num  10.85 13.09 8.47 8.35 8.4 ...
     $ T6     : num  16 12 11.9 17.2 13.1 ...
 $ T7     : num  16.6 16.3 19 15.5 14.6 ...
     $ T8     : num  14.9 13.8 16.4 16.1 17.4 ...
 $ R5     : num  16 3 28 45 54 34 29 10 29 11 ...
     $ R6     : num  8 47 86 34 55 73 76 26 54 20 ...
 $ R7     : num  45 66 1 31 164 57 75 15 69 89 ...
     $ R8     : num  110 155 60 45 7 53 105 37 46 71 ...
</code></pre>

<p>And head(my.data)</p>

<pre><code>&gt; head(my.data)
         Species SiteID Year Crop      T5     T6     T7     T8 R5 R6  R7  R8
16 Betulapendula      1 1992  133 10.8520 16.000 16.574 14.919 16  8  45 110
17 Betulapendula      1 1993 2257 13.0870 11.987 16.300 13.787  3 47  66 155
18 Betulapendula      1 1994  347  8.4742 11.867 19.016 16.445 28 86   1  60
19 Betulapendula      1 1995   92  8.3484 17.210 15.535 16.081 45 34  31  45
20 Betulapendula      1 1996  215  8.4032 13.067 14.555 17.429 54 55 164   7
21 Betulapendula      1 1997  274  8.0226 16.173 18.755 18.729 34 73  57  53
</code></pre>

<hr>

<p>I tryed both options and didn't work. The first take a lot of time and didn't give any result</p>

<pre><code>library(lme4)
 glmer.nb(Crop ~T5+T6+T7+T8+R5+R6+R7+R8+
     (1|SiteID)+(1|Year)+(1|Species),data = my.data)
</code></pre>

<p>The second option give this error: </p>

<pre><code>Error in mkRespMod(fr, REML = REMLpass) : NA/NaN/Inf in 'y'
</code></pre>

<p>And <code>sum(my.data$crop != round(my.data$crop))</code> is 1023</p>
"
"0.100200602007025","0.102382519472325","181481","<p>I've been looking all around the webs but cant find a conclusive answer.  I have count data for a longitudinal study where subjects were grouped into three treatment groups (A,B,C) and blocked by litter and starting weight category (high, med, low).  I'd like to use a linear mixed model (<code>lme4</code> R package) to ask questions like: which factors are most indicative in differentiating each group of subjects?</p>

<p>I've only found examples using mixed models that use either a continuous response variable or a dichotic (0/1) response. In my case, my response is categorical with three groups.  Is it possible to use mixed models (and more specifically <code>glmer()</code>) with a categorical response of more than two outcomes?  Do I simply specify a binomial family (probit or logit)?</p>
"
"0.0776150525706333","0.0793051585718144","182068","<p>I have data where I'm interested in the effect of treatment on individual decisions: Options 1, 2, &amp; 3. Individuals made multiple decisions (level 1) in groups (level 2).</p>

<p>I want to know the effect of treatment on selecting a particular option over not selecting that option. To do this, I dichotomized each option into 3 variables: Option 1 or not Option 1, Option 2 or not Option 2, Option 3 or not Option 3.</p>

<p>I tried to run a binomial regression using glmer: e.g., <code>glmer(Option1 ~ Predictors + (1|Level_1) + (1|Level_2), family=binomial)</code>, which did not converge.</p>

<p>I ran the same analysis using nnet::multinom: e.g., <code>multinom(Option1 ~ Predictors, random=~ 1| Level_1/Level_2, family=binomial)</code>. This did converge and gave sensible results.</p>

<p>What's going on? Is it justifiable to use multinom instead of glmer on the dichotomized data?</p>

<p>Any insight would be appreciated!</p>
"
"0.0801604816056202","0.0819060155778602","182315","<p>I'm using R to fit a generalized linear model with random effects using a negative binomial distribution. One of the main issues is that to run glmer() with a negative binomial, the dispersion parameter $\theta$ needs to be specified. <a href=""http://stats.stackexchange.com/questions/138109/r-glmer-nb-output-how-to-get-hat-theta"">From what I understand</a>, the glmer.nb() function makes an initial guess for $\theta$ and then runs the glmer() function. This is then iterated until convergence is achieved. This becomes a huge issue when the dataset is large with big number of factors. My question is: is there any other reasonable way of guessing the $\theta$ parameter and running the glmer() function once? For example, if I were to remove all random effects, and run glmer.nb() on the fixed effects alone, would that be a decent guess of $\theta$?</p>
"
"0.135110464183741","0.15185781720314","186825","<p>I am trying to run a mixed model on over-dispersed non-integer data. My data are not counts, but are zero-inflated and over dispersed. The variable is distance (how far a gps point is from a central location) and as such looks like: 0.33, 64.73, 5.2 etc. I have been using a quasi-Poisson distribution as I have read that quasi can handle non-integer data (both Poisson and negative binomial cannot). I am using the <code>glmmPQL</code> function in package MASS as this allows quasi distributions with a random term (the identity of the individual that the gps point comes from).The functions <code>glmm</code> and <code>lmer</code> do not work with a quasi-Poisson distribution. Plotting the residuals indicates a lack of fit of this model.log-transforming the data to try and make it normal before hand also fails (the Shapiro-test for normality is significant). I am unsure how to fix this, as I seemingly have to use a quasi-distribution (link=""log"") because my data is not counts, non-integer and not normal but there is still overdispersion and lack of fit when using this distribution. </p>

<p>My question therefore is: <strong>How to model over-dispersed, non-integer data in a mixed model when quasi-Poisson does not seem to work?</strong>   </p>

<p>My code so far is:</p>

<pre><code>summary(glmmPQL(distance_from_centroid~Chick.Juv.Adult+Summer_winter, 
                random=~1|markingnumber, family=quasipoisson(link=""log""),
                data=centroid_distances))
</code></pre>

<p>Which results in:  </p>

<pre><code>Linear mixed-effects model fit by maximum likelihood
 Data: centroid_distances 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 `Formula: ~1 | markingnumber
        (Intercept) Residual
StdDev:    1.157381 2.136811

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: distance._from_centroid ~ Chick.Juv.Adult + Summer_winter 
                      Value  Std.Error  DF   t-value p-value
(Intercept)       2.0670095 0.09403952 695 21.980221  0.0000
Chick.Juv.AdultC -0.2945360 0.06686399 695 -4.405002  0.0000
Chick.Juv.AdultJ -0.2005831 0.06727181 695 -2.981682  0.0030
Summer_winterW    0.1207721 0.04324588 695  2.792684  0.0054
 Correlation: 
                 (Intr) C.J.AC C.J.AJ
Chick.Juv.AdultC -0.565              
Chick.Juv.AdultJ -0.512  0.736       
Summer_winterW   -0.267  0.134  0.043

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.53759073 -0.48277169 -0.31041612  0.06314122  7.48672836 

Number of Observations: 1009
Number of Groups: 311 
</code></pre>

<p>Which when plotting the residuals gives me:</p>

<p><a href=""http://i.stack.imgur.com/3SKVU.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3SKVU.jpg"" alt=""plot of residuals""></a></p>
"
"0.118558951157635","0.121140630718605","189115","<p>I'm hoping somebody can help with what I think is a relatively simple question, and I think I know the answer but without confirmation it has become something I just can't be certain of. </p>

<p>I have some count data as a response variable and I want to measure how that variable changes with the proportional presence of something.</p>

<p>In more detail, the response variable is counts of the presence of an insect species in a number of sites, so for example a site is sampled 10 times and this species may occur 4 times. </p>

<p>I want to see if this correlates with the proportional presence of a group of plant species in the overall commmunity of plants at these sites. </p>

<p>This means my data looks as follows (this is just an example)</p>

<pre><code>Site, insectCount, NumberOfInsectSamples, ProportionalPlantGroupPresence
1, 5, 10, 0.5
2, 3, 10, 0.3
3, 7, 9, 0.6
4, 0, 9, 0.1
</code></pre>

<p>The data also includes a random effect for location. </p>

<p>I thought of two methods, one would be an linear model (<code>lmer</code>) with the insects converted to a proportion e.g. </p>

<pre><code> lmer.model&lt;-lmer(insectCount/NumberOfInsectSamples~
 ProportionalPlantGroupPresence+(1|Location),data=Data)
</code></pre>

<p>The second would be a binomial GLMM (<code>glmer</code>)
e.g.</p>

<pre><code>glmer.model &lt;- glmer(cbind(insectCount,NumberOfInsectSamples-insectCount)~
 ProportionalPlantGroupPresence+(1|Location),
 data=Data,family=""binomial"")
</code></pre>

<p>I believe the binomial glmer to be the correct method, however they produce fairly different results. I cant seem to find a definitive answer on the net without still feeling slightly uncertain, and wish to make sure I am not making a mistake. </p>

<p>Any help or insight into alternative methods on this would be much appreciated. </p>
"
"0.318696659167993","0.290746817771688","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.118558951157635","0.103834826330233","191869","<p>I have a glmer model that I am performing contrasts on. My model has four factors (emotion, gender, filter, ageGroup). No interactions. I have been using <code>lsmeans(x, spec, contr='pairwise'))</code> (and) <code>lsm</code> within <code>glht</code>) to get lsmeans and to do pairwise contrasts within-factors. I am also interested in comparing the means from one factor against a level from a different factor.</p>

<pre><code>&gt; summary(m7.only_within)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )

 Formula: FaceStimulus.RESP.binary ~ emotion + emotion.Sample + gender +  
    gender.Sample + ageGroup + ageGroup.Sample + filter + filter.Sample + (1 | Subject)

Fixed effects:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -0.823451   0.264442  -3.114  0.00185 ** 
emotiondisgust               1.504896   0.092532  16.264  &lt; 2e-16 ***
emotionfear                  1.555492   0.091832  16.938  &lt; 2e-16 ***
emotion.Sample              -0.106948   0.044364  -2.411  0.01592 *  
gendermale                   0.012718   0.074198   0.171  0.86391    
gender.Sample                0.007021   0.010046   0.699  0.48462    
ageGroupold                  0.364239   0.076379   4.769 1.85e-06 ***
ageGroup.Sample             -0.013519   0.009577  -1.412  0.15808    
filtercropAdaptiveThreshold  0.419095   0.075906   5.521 3.37e-08 ***
filter.Sample                0.070147   0.029658   2.365  0.01802 *  
---
</code></pre>

<p>I understand that lsmeans calculates standard errors using variable variances and covariances from vcov() to do pairwise comparisons. Can I just compare one model variable against another? Like so:</p>

<pre><code>summary(glht(m3.only_within, linfct = c(""filtercropAdaptiveThreshold - ageGroupold = 0"")))

Linear Hypotheses:
                                               Estimate Std. Error z value Pr(&gt;|z|)    
filtercropAdaptiveThreshold - ageGroupold == 0  0.05486    0.10798   0.508        1    
</code></pre>

<p>That contrast is the test of the difference between the betas in the model, not between the lsmeans. Naturally, all my other contrasts compare the lsmeans since I'm using <code>lsm()</code>. Can I just manually take the difference between the lsmeans, and then use the variances and covariance SE's from the model vcov for the two model effects to get the SE for the <code>lsmeans</code> difference? Naturally, those SEs are much lower than the SEs reported from the <code>lsmeans</code> output.</p>

<pre><code>$lsmeans
 ageGroup     lsmean        SE df  asymp.LCL asymp.UCL
 young    0.01043529 0.2466025 NA -0.4728967 0.4937673
 old      0.37467465 0.2474967 NA -0.1104101 0.8597594

$lsmeans
 filter                     lsmean        SE df   asymp.LCL asymp.UCL
 cropDesat37           -0.01699274 0.2462214 NA -0.49957782 0.4655923
 cropAdaptiveThreshold  0.40210268 0.2478032 NA -0.08358268 0.8877880
</code></pre>

<p>So it would be: 
$$
\displaystyle \begin{array}{l}
    \bar{X_1}-\bar{X_2}= lsmean\_of\_cropAdaptiveThreshold - lsmean\_of\_old \\
= 0.40210268 - 0.37467465 = 0.0274\\ \\ 
SE_{\bar{X_1}-\bar{X_2}}= \sqrt{SE_1^2 + SE_2^2 + 2*cov}\\ 
= \sqrt{0.075906^2 + 0.076379^2 + 2*-3.225794e-05}\\ 
= 0.10798
\end{array}
$$</p>

<p>...which is the same SE as the SE in the beta-contrast.</p>

<p>Is it legal to compare cross-factor level lsmeans like this?</p>
"
"0.0633724250524478","0.0647523908238176","194601","<p>I have these three models:</p>

<pre><code>M0&lt;-glm(y==""yes""~x+z+b+c+r+as.factor(w),family=""binomial"")
</code></pre>

<blockquote>
  <p>'logLik' -1147.734 (df=65)</p>
</blockquote>

<pre><code>M1&lt;-glmer(y==""yes""~x+z+b+c+r+(1|w),family=""binomial"")
</code></pre>

<blockquote>
  <p>'logLik' -1206.966 (df=7)</p>
</blockquote>

<pre><code>M2&lt;-glmer(y==""yes""~x+z+b+c+r+(1|w)+(1|z),family=""binomial"")
</code></pre>

<blockquote>
  <p>'logLik' -1206.966 (df=8)</p>
</blockquote>

<p>My questions are:</p>

<ol>
<li>Are the models M1 and M2 nested?</li>
<li>Can I compare the three models? and If I can which is the best model?</li>
</ol>
"
"0.110901743841784","0.129504781647635","198737","<p>I have a dataset with the following variables:</p>

<ul>
<li>proportion of species present, between 0 and 1 (called speciesProp)</li>
<li>a binomial (0,1) presence/absence of the same species (called PA in the model)</li>
<li>year</li>
</ul>

<p>The dataset has many 0s in the proportion and binomial columns. These are actual 0 values collected in the field.</p>

<p>I want to know if the proportion of species is increasing over the year (controlling for random effects)</p>

<p>I logit transformed my proportional data, and then originally I thought of running a linear mixed effects model in lme4 as follows:</p>

<pre><code>m01 &lt;- lmer(speciesPropLOGIT ~ year + (1|referenceID), data = speciesAll)
</code></pre>

<p>But then wondered if the following was more appropriate:
1) a logistic model of the binomial presence / absence data as follows:</p>

<pre><code>model &lt;- glm(PA ~ year , family = binomial(link = ""cloglog""), data = speciesAll)
</code></pre>

<p>followed by the following linear mixed effects model, where the proportion is the response variable and excluding the 0s:</p>

<pre><code>m01 &lt;- lmer(speciesPropLOGIT ~ year + (1|referenceID), data = speciesAll)
</code></pre>

<p>Someone suggested that I think about model multiplication, of the two outputs, the first estimating the proportion of species over year with the 1s and 0s, and the second looking at the positive data over time. I then would like to plot one line of model fit.</p>

<p>When i run the models separately, year comes out as significant in all of them.</p>

<p>I have spent a long time looking for advice on how to do it, but can't seem to find any. </p>

<p>Also - do I need to have family = binomial somewhere if I have logit transformed the proportional data?</p>

<p>Hope you can help?</p>
"
"0.0896221429896442","0.091573709299126","199550","<p>I would like to predict a dichotomous variable Y with five ordinal variables X1, ..., X5.</p>

<p>I have about 50 individuals to do that, and no missing data. But the study is a combination of cross-sectional and longitudinal data: some individuals have answered only once, and some others have answered two or three times at different moments.
I don't think it would be a good idea to treat the predictors as continuous (they only have 3 to 5 possible values), and I don't really want to treat them as nominal because I don't want to lose the order.</p>

<p>So I tried the following model with R (is it okay ?):</p>

<pre><code>X1 &lt;- factor(X1, ordered=TRUE)
# ...
X5 &lt;- factor(X5, ordered=TRUE)

mod &lt;- glmer(Y ~ X1 + X2 + X3 + X4 + X5 + (1|Indiv), family=binomial, data=Data)

summary(mod)
# Fixed effects:
#             Estimate Std. Error z value Pr(&gt;|z|)
# (Intercept) -16.2066   140.9124  -0.115    0.908
# X1.L         14.9098   251.3437   0.059    0.953
# X1.Q          0.5086   227.0290   0.002    0.998
# X1.C          2.5982   200.8045   0.013    0.990
# X1^4        -11.3474   229.4112  -0.049    0.961
# X2.L         18.7472   215.6628   0.087    0.931
# X2.Q          0.9042   248.2168   0.004    0.997
# X2.C        -10.2088   246.0356  -0.041    0.967
# ...
</code></pre>

<p>The output lets me think that R is using polynomials to code the ordinal predictors. I don't know how this works exactly: do you have a book / theoretical reference about that? </p>

<p>More generally, what are the others possibilities (if there are some!) to treat such a dataset? Is it possible to take into account longitudinal data with decision trees or random forests, for example?</p>

<p>Thanks!</p>
"
"0.119496190652859","0.122098279065501","199743","<p>I'd like to analyze data from an signal-detection experiment and I got confused with the different possibilities to model random effects. I'm using glmer with data in long format. </p>

<p>Participants saw 12 pictures first which they had to remember for an recognition test later. In the second phase of the experiment they saw 24 pictures (half of the pictures were presented in the first phase). For each of the pictures they had to decide if they had seen them before or not.</p>

<p>So basically the model goes like that for an random intercept model:</p>

<pre><code>glmer(sayold ~ old + (1|Subject), family = binomial(probit),data=dgl)
</code></pre>

<p>old: the picture was actually presented before or not/
sayold: the participants' response/
Subject: Participants' identifier</p>

<p>In the experiment there were 3 sets of pictures - each participant saw only one set (containing 24 items/pictures).</p>

<p>I'd like to model the items and sets as random effects too. But I am not sure how to do it properly. </p>

<p>If I add </p>

<pre><code>+ (1|item)
</code></pre>

<p>I ignore that the items were nested in sets Ã¡ 24 pictures. Is is correct to do it like this?</p>

<pre><code>+ (1|set/item)
</code></pre>
"
"0.0448110714948221","0.045786854649563","204146","<p>I am running a negative binomial generalised linear mixed model - glmer.nb()from the {lmer} package - to investigate the extent to which elevation (elev) can predict changes in the density of herbaceous plants (herb_den), with site (site) as a random effect to account for heteroscedasticity due to different sampling efforts across elevation:</p>

<pre><code>tmp_glmer.nb &lt;- glmer.nb(herb_den ~ elev + (1|site))
</code></pre>

<p>For basic linear mixed models I can use: </p>

<pre><code>r.squaredGLMM(lmer(tmp_lmer))
</code></pre>

<p>from the {MuMIn} package to generate a pseudo-r-squared value to demonstrate goodness of fit of the model.</p>

<p>Can I do something similar for glmer.nb()?</p>
"
"0.10976425998969","0.112154430818409","206870","<p>By converting and by trying to interpret the parameters of a logistic regression ran in R, I just find them to be overestimated. Therefore I tried to compute them myself but I can not obtain the same values reported by the regression.</p>

<p>I used this web-page for computations:
<a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm</a></p>

<p>Let say we only focus on the LagC parameter:</p>

<p><strong>Logistic Regression</strong></p>

<pre><code>&gt; model &lt;- glmer(RepT2 ~ DistractorC1 + DistractorC2 + LagC + DistractorC1:LagC + DistractorC2:LagC + (LagC | Subject) + (1 | Item),
                data = DF,
                family = binomial(link = ""logit""),
                control = glmerControl(optimizer = ""bobyqa""))
&gt; summary(model)

  Fixed effects:
                    Estimate Std. Error z value Pr(&gt;|z|)    
  (Intercept)       -0.81039    0.22040  -3.677 0.000236 ***
  DistractorC1       0.33129    0.06393   5.182  2.2e-07 ***
  DistractorC2       0.03436    0.10011   0.343 0.731467    
  LagC               2.09567    0.12725  16.469  &lt; 2e-16 ***
  DistractorC1:LagC -0.21654    0.12770  -1.696 0.089932 .  
  DistractorC2:LagC -0.84018    0.20055  -4.189  2.8e-05 ***
</code></pre>

<p>Odds of the parameters:</p>

<pre><code>&gt; show(Odds &lt;- exp(summary(model)$coefficients[,""Estimate""])

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.4446833         1.3927594         1.0349529         8.1308503         0.8052993         0.4316343 
</code></pre>

<p>Probabilities of the parameters:</p>

<pre><code>&gt; show(P &lt;- Odds / (1 + Odds))

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.3078068         0.5820725         0.5085881         0.8904812         0.4460752         0.3014976 
</code></pre>

<p><strong>My Estimations</strong></p>

<pre><code>&gt; Means &lt;- DF %&gt;%
    group_by(Subject, Lag) %&gt;%
    filter(RepT1 == 1) %&gt;%
    summarise(repok = sum(RepT2) / (n())) %&gt;%
    group_by(Lag) %&gt;%
    summarise(Means = mean(repok))

&gt; show(Means)

     Lag     Means
  (fctr)     (dbl)
1   Lag3 0.1972174
2   Lag8 0.5475624
</code></pre>

<p>Odds of the parameter:</p>

<pre><code>&gt; OddsLag3 &lt;- 0.1972174 / (1-0.1972174)
&gt; OddsLag8 &lt;- 0.5475624 / (1-0.5475624)
&gt; OddsLagC &lt;- OddsLag8 / OddsLag3
&gt; show(OddsLagC)

[1] 4.926377
</code></pre>

<p>Probabilities of the parameter:</p>

<pre><code>&gt; show(OddsLag / (1 + OddsLag))

[1] 0.8312628
</code></pre>

<p>We can see that it is close, but not accurate. Does anyone have an explanation?
Note that I compute a mean for each subject and then only a mean for each condition. I also did estimate the parameters without taking into account the subjects, but still, the mismatch was here.</p>

<p><a href=""http://i.stack.imgur.com/YRdPf.png"" rel=""nofollow"">Graphical representation</a></p>
"
"0.179555204315269","0.194257172471453","207395","<p>I have bird nesting data and I am trying to see whether the nest treatment has any significant effects on the survival of the nestling. My original data set is relatively small (n=101). The response variable is binomial (No treatment = 0,  treatment = 1) as is the fixed effect (survived = 1, died = 0). </p>

<p>A copy of my original data set is available <a href=""https://drive.google.com/file/d/0B2vynKP39eZed1pwMFh4ekhGV00/view?usp=sharing"" rel=""nofollow"">here</a>. </p>

<p>I have obtained the following results from my model:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood  (Laplace Approximation)
  ['glmerMod']
Family: binomial  ( logit )
Formula: Survived ~ Treatment + (1 | Nest) + (1 | Year)
Data: Treatment_original
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 4e+05))

  AIC      BIC   logLik deviance df.resid 
109.8    120.2    -50.9    101.8       97 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.9725  0.1557  0.2853  0.3653  1.2021 

Random effects:
Groups Name        Variance Std.Dev.
Nest   (Intercept) 3.2860   1.8127  
Year   (Intercept) 0.5109   0.7148  
Number of obs: 101, groups:  Nest, 39; Year, 7

Fixed effects:
    Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   1.6228     0.7258   2.236   0.0254 *
Treatment     0.9063     0.7676   1.181   0.2377  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
  (Intr)
Treatment -0.152
</code></pre>

<p>To account for the possible influence of small sample size, I produced a bootstrap data set using the following code:</p>

<pre><code>bootstrapdata &lt;- data.frame()
for (i in 1:1000){
  boot &lt;- sample(1:nrow(Treatment_original), replace=TRUE)
  bootdata &lt;- Treatment_original[boot,]
  bootstrapdata &lt;- rbind(bootstrapdata, bootdata)
}
</code></pre>

<p>The bootstrap data set is available <a href=""https://drive.google.com/file/d/0B2vynKP39eZeS3VQVHcwMmw4MkE/view?usp=sharing"" rel=""nofollow"">here</a>.</p>

<p>I then ran the above model on the bootstrap data set, which produced the following results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
  ['glmerMod']
Family: binomial  ( logit )
Formula: Survived ~ Treatment + (1 | Nest) + (1 | Year)
Data: Treatment_bootstrap
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 4e+05))

 AIC      BIC   logLik deviance df.resid 
2957.0   2985.8  -1474.5   2949.0     9996 

Scaled residuals: 
  Min      1Q  Median      3Q     Max 
-3.5915  0.0001  0.0002  0.0026  2.4168 

Random effects:
Groups Name        Variance Std.Dev.
Nest   (Intercept) 511.888  22.625  
Year   (Intercept)   4.251   2.062  
Number of obs: 10000, groups:  Nest, 38; Year, 7

Fixed effects:
    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  16.0123     1.9144   8.364   &lt;2e-16 ***
Treatment     1.5813     0.1465  10.795   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
  (Intr)
Treatment 0.009 
</code></pre>

<p>I would like to know how to interpret the bootstrap model results. Can I now say that the nest treatment had a positive effect on nestling survival?</p>

<p>The original data set showed no significant effect of nest treatment. Should I rather be using these results and adjusting the p value for false discovery rate?</p>

<p>I am unsure as to which results are correct. Should I report both results? What inferences can I make from these results? </p>
"
"0.100200602007025","0.102382519472325","209773","<p>I am estimating a model of the type (logistic regression with random slopes and random intercepts clustered by the variable ID):</p>

<pre><code>formula = result ~ year + (1 | ID) + (year | ID)
</code></pre>

<p>using</p>

<pre><code>glmer(formula, data = data1, family = binomial, control = glmerControl(optimizer = ""bobyqa""), nAGQ = 1)
</code></pre>

<p>However, I am getting results for three random effects:</p>

<pre><code>Random effects:
 Groups Name        Variance  Std.Dev.  Corr 
 ID    (Intercept) 3.645e-01 0.6037203      
 ID.1  (Intercept) 1.228e+00 1.1082860      
        year       3.043e-07 0.0005516 -1.00
</code></pre>

<p>Is there something I am specifying incorrectly?</p>

<p>On the other hand, the fixed effects are correct:</p>

<pre><code>Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 55.328196   8.052619   6.871 6.38e-12 ***
year       -0.027933   0.004008  -6.969 3.20e-12 ***
</code></pre>
"
"0.0633724250524478","0.0647523908238176","210344","<p>I have some whale tourism data that I am trying to model to see which factors significantly affect the number of encounters between whales and tourists. </p>

<p>I have two years worth of data and have converted the data into presence absence data for individual whales during each month. I am using the MuMIN package to dredge my a global model with an optimal random effects structure that I have already determined by comparing AIC. </p>

<p>My global model is: </p>

<pre><code>Par5 &lt;- glmer(PA ~ Hours + Length + Sex
          +Hours*Length + Hours*Sex + Length*Sex 
          + (1|Id) + (1|Year/Month), data=edata, family=binomial)
</code></pre>

<p>Where PA= presence/absence (1 or 0)</p>

<p>Hours = log transformed tour hours</p>

<p>Length= length of the whale</p>

<p>Sex=sex of the whale</p>

<p>Id = whale ID</p>

<p>and I also have year and month </p>

<p>in my table of top ranked models I am finding that my null model is the 4th best ranked model with a delta AIC of 2.33 and I get a warning when I include interactions: 
1  Model failed to converge with max|grad| = 0.0229445
2 Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?</p>

<p>I suspect that something is going very wrong, if someone could shed some light as to what is going very wrong, I'd be very appreciative.</p>

<p>Alicia</p>
"
"0.10976425998969","0.112154430818409","210567","<p>I am struggling to understand the meaning of random effect for the dataset with missing data based on mixed model, I am appreciated if anyone can help. Here is an example.</p>

<p><a href=""http://i.stack.imgur.com/SKFun.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SKFun.png"" alt=""enter image description here""></a></p>

<p>let us say we have 20 subjects, 2 independent variables('var1 &amp; var2'), as shown in the model above, 'i' represents subject, 'j' represents time (a repeated measures design). Finally, I am able to get a random slope for each subject at each time. </p>

<p>The dataset has missing value, actually in some sub-datasets (few subjects at several time), no observations were measured. </p>

<p>My query is that the mixed model still estimate random slopes for these sub-datasets with no trials, how can I interpret this? Intuitively, those random slopes can interpret nothing because there is no observations.</p>

<p>here is the r syntax, observation is binary data type:</p>

<p>output &lt;- glmer(observations ~ variable1 + variable2 + (1|subject) + (0+ variable1|subject)+ (0+variable2|subject), family=binomial, data=dataset)</p>
"
"0.101621958135115","0.103834826330233","212301","<p>I have a huge doubt, which I believe is Basic. I have no difficulty in interpreting the results of our logistic regression model using the ODD ratio, but I do not know what to do when I work with Mixed effects model for longitudinal data.</p>

<p>Below they use the <code>glmer</code> function to estimate a mixed effects logistic regression model with Il6, CRP, and LengthofStay as patient level continuous predictors, CancerStage as a patient level categorical predictor (I, II, III, or IV), Experience as a doctor level continuous predictor, and a random intercept by DID, doctor ID.</p>

<p>The <code>glmer</code> function created 407 groups that refer to the number of doctors.</p>

<p>What would it mean for example the -0.0568 of IL6 and the -2.3370 of CancerStageIV's in the study presented?</p>

#################

<p>m &lt;â€ glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer =  ""bobyqa""),      nAGQ = 10) 
print(m, corr = FALSE) </p>

<h1>Generalized linear mixed model fit by maximum likelihood</h1>

<h2>Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]</h2>

<h2>Family:</h2>

<p>binomial ( logit )  </p>

<h2>Formula:</h2>

<p>remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +<br>
   (1 | DID)  </p>

<p>Data: hdp  </p>

<pre><code>  AIC        BIC    logLik     deviance  df.resid   
 7397        7461    -3690        7379     8516 
</code></pre>

<h2>Random effects:</h2>

<p>Groups Name         Std.Dev.<br>
     DID    (Intercept) 2.01 </p>

<p>Number of obs: 8525, groups: DID, 407  </p>

<h1>Fixed Effects:</h1>

<pre><code>  Intercept    IL6        CRP       CancerStageII  
 â€2.0527     â€0.0568    â€0.0215       â€0.4139 

CancerStageIII   CancerStageIV       LengthofStay      Experience  
 â€1.0035           â€2.3370              â€0.1212          0.1201 
</code></pre>
"
"0.134433214484466","0.137360563948689","213470","<p><strong>Background</strong></p>

<p>Although my data should have a multinomial dependent variable, I have settled for a binary as I could not understand too much of MCMCglmm. The data is a time series cross sectional, so am looking at each individual outcome vis-a-vis the others. I don't have much experience with statistics, so I really need to know if am on the right path of actually coming up with values for a dynamic linear model or way off. Since i need effects from the independent variables.</p>

<p><strong><em>The data sample</em></strong></p>

<p>The data is for students who applied for university courses and were admitted within a period of 3 years. The data mainly has the grades in the subjects done in their pre-entry level exams. Each student can do a maximum of 4, but in the model below <code>NA</code> values are filled with <code>0</code> (Not sure if a correct assumption). </p>

<p><strong>The problem</strong></p>

<ol>
<li>How do I get time varying effects?</li>
<li>How do i extract the effect of time? </li>
<li>what does it imply when time is expressed as a random effect?</li>
</ol>

<p>Every input is highly appreciated, Thank you.</p>

<p>Below is a result from the <code>glmer</code> function with formula</p>

<p><code>glmllb2 &lt;- glmer(logi ~ history + c.r.e + economics + geography + literature + f.art + entrepreneurship + luganda + kiswahili + french + i.r.e + historyc + historycsq + (1 | called), family = binomial(""logit""), control = glmerControl(optimizer = ""bobyqa""), nAGQ = 100, data = data.apriori.llb2)</code></p>

<p>where <code>history+c.r.e + ... + i.r.e</code> are subject grades that predict student admission into a course <code>logi</code> (as a binary) while <code>historyc</code> is a grand mean centered variable for history and <code>historycsq</code> is the squared variable for <code>historyc</code>. <code>called</code> is time in years re-scaled to <code>1,2 and 3</code></p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Adaptive Gauss-Hermite Quadrature, nAGQ = 100) [glmerMod]
 Family: binomial  ( logit )
Formula: logi ~ history + c.r.e + economics + geography + literature +  
    f.art + entrepreneurship + luganda + kiswahili + french +      i.r.e + historyc + historycsq + (1 | called)
   Data: data.apriori.llb2
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  1778.2   1874.8   -875.1   1750.2     7317 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
 -3.843  -0.123  -0.054  -0.025 213.612 

Random effects:
 Groups Name        Variance Std.Dev.
 called (Intercept) 0.09975  0.3158  
Number of obs: 7331, groups:  called, 3

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -13.27747    0.52128 -25.471  &lt; 2e-16 ***
history            0.55942    0.04656  12.016  &lt; 2e-16 ***
c.r.e              0.45941    0.03652  12.580  &lt; 2e-16 ***
economics          0.69835    0.04509  15.489  &lt; 2e-16 ***
geography          0.49442    0.04137  11.950  &lt; 2e-16 ***
literature         0.77936    0.04129  18.877  &lt; 2e-16 ***
f.art              0.50219    0.04387  11.447  &lt; 2e-16 ***
entrepreneurship   0.46377    0.04504  10.297  &lt; 2e-16 ***
luganda            0.49340    0.07643   6.456 1.08e-10 ***
kiswahili          0.52691    0.10498   5.019 5.20e-07 ***
french             0.65225    0.09133   7.142 9.22e-13 ***
i.r.e              0.59269    0.08265   7.171 7.44e-13 ***
historycsq         0.03721    0.01794   2.075    0.038 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) histry c.r.e  ecnmcs ggrphy litrtr f.art  entrpr lugand kiswhl french i.r.e 
history     -0.559                                                                             
c.r.e       -0.608  0.143                                                                      
economics   -0.340 -0.083 -0.002                                                               
geography   -0.569  0.187  0.624 -0.059                                                        
literature  -0.625  0.169  0.540  0.086  0.709                                                 
f.art       -0.614  0.231  0.588  0.118  0.525  0.585                                          
entrprnrshp -0.554  0.191  0.564 -0.031  0.583  0.654  0.596                                   
luganda     -0.305  0.086  0.289  0.065  0.305  0.337  0.283  0.281                            
kiswahili   -0.242  0.158  0.173  0.073  0.161  0.195  0.195  0.186  0.099                     
french      -0.265  0.079  0.314 -0.002  0.257  0.268  0.290  0.305  0.144  0.094              
i.r.e       -0.256  0.038  0.349  0.041  0.253  0.251  0.231  0.242  0.049  0.083  0.137       
historycsq   0.153 -0.253 -0.235 -0.107 -0.275 -0.204 -0.202 -0.231 -0.106 -0.112 -0.119 -0.102
fit warnings:
fixed-effect model matrix is rank deficient so dropping 1 column / coefficient
</code></pre>
"
"0.0672166072422331","0.0686802819743445","217639","<p>Using the glmer() function in the LME4-library in R I computed logistic models, of the form: Y ~ cat1 * cont1 + (1|Subject) where, obviously, Y is the binomial outcome variable (0 or 1), cat1 is a categorial variable (0,1,2) and cont1 is a continuous variable). Then, using confint(model, method = ""boot"") I computed confidence interval on the variables.</p>

<p>Now I would like to plot a graph of the chance P(Y==1), I want to plot P against cont1 for every cat1.</p>

<p>So you'd say: X = B(0) + B(cont1) * cont1 + B(cat1:1) * (cat1==1) + .... + etc
And then: P(Y==1) = 1/(1-exp(-X))</p>

<p>Which does exactly what I expect. But now, I want to incorporate the bootstrapped confidence intervals (so not std. error * 1.96!!) in the graph. I have the numbers, I do not know how to interpret them, what would be the formula for e.g. the 97.5 % line and the 2.5 % line?</p>

<p>Thanks in advance!</p>

<p><strong>EDIT</strong>
Is this the correct way?
Basically taking 10000 samples with replacement, same size as original data, creating the model, computing the predictions, and taking the 97.5th and 2.5th intervals of the predictions.</p>

<pre><code>prediction_pars = expand.grid(cont1= seq(-4,4,.05), cat1= as.factor(c(1,2,3)));

predictions = array(dim = c(10000, dim(prediction_pars)[1]));

for (i in 1:10000){
  new_sample = data[sample(nrow(data), samplesize, replace = T) , ];
  new_model  = glmer (Y ~ cont1 * cat1 + (1|Subject), dat=newdat, family=""binomial"");
  predictions[i , ] = predict(new_model, newdat = new_sample, re.form = NA);
}

hi = lo = array(dim = dim(prediction_pars)[1]);
for (i in (1, dim(prediction_pars)[1])){
  hi[i] = sort(predictions[,1]) [9750];
  lo[i] = sort(predictions[,1]) [ 250];
}
</code></pre>
"
"0.13730692094697","0.140296846784938","218707","<p>I have successfully calculated RÂ²c as a goodness-of-fit measure for GLMM's using the <code>r.squaredGLMM</code> function implemented in <code>R</code>'s <code>MuMIn</code> package, as well as calculating RÂ²GLMM with the step-by-step guide given in Nakagawa &amp; Schielzeth 2013 (A general and simple method for obtaining RÂ² from generalized linear mixed-effects models 4:133-142. doi: 10.1111/j.2041-210x.2012.00261.x). </p>

<p>Successfully anyway, when my models followed a Gaussian or Poisson error structure. I am having trouble, though, to calculate RÂ² for a Gamma-GLMM: </p>

<p>1) <code>r.squaredGLMM</code> apparently does not work for Gamma GLMM's:</p>

<pre><code>Error in r.squaredGLMM.merMod(mod1) : do not know how to calculate variance for this family/link combination`
</code></pre>

<p>2) Nakagawa and Schielzeth give examples on how to calculate RÂ²GLMM for GLMM's with Gaussian, Poisson or Binomial error structures, but not for GLMM's with Gamma error structure. </p>

<p>This following is part of the supplemental R code to the paper:</p>

<pre><code># Fit null model without fixed effects (but including all random effects)
m0 &lt;- lmer(BodyL ~ 1 + (1 | Population) + (1 | Container), data = Data)

# Fit alternative model including fixed and all random effects
mF &lt;- lmer(BodyL ~ Sex + Treatment + Condition + (1 | Population) + (1 |     Container), data = Data)

# View model fits for both models
summary(m0)
summary(mF)

# Extraction of fitted value for the alternative model
# fixef() extracts coefficents for fixed effects
# mF@X returns fixed effect design matrix
Fixed &lt;- fixef(mF)[2] * mF@X[, 2] + fixef(mF)[3] * mF@X[, 3]  + fixef(mF)[4] * mF@X[, 4]

# Calculation of the variance in fitted values
VarF &lt;- var(Fixed)

# An alternative way for getting the same result
VarF &lt;- var(as.vector(fixef(mF) %*% t(mF@X)))

# R2GLMM(m) - marginal R2GLMM
# Equ. 26, 29 and 30
# VarCorr() extracts variance components
# attr(VarCorr(lmer.model),'sc')^2 extracts the residual variance
VarF/(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] +    attr(VarCorr(mF), ""sc"")^2)

# R2GLMM(c) - conditional R2GLMM for full model
# Equ. XXX, XXX
(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/(VarF +  VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + (attr(VarCorr(mF), ""sc"")^2))
</code></pre>

<p>Can anybody give me a solution on how to calculate RÂ²GLMM for a GLMM with Gamma error structure?</p>

<p>The real problem is that I don't know which is the correct specification for the very last part of the very last formula in the above example. This part is really the only one which changes depending on error structure of the model.</p>

<p>For Gaussian it is <code>(attr(VarCorr(mF), ""sc"")^2)</code>, for Binomial it is <code>pi^2/3</code> and for Poisson they use <code>log(1 + 1/exp(as.numeric(fixef(m0))))</code>.</p>

<p>Thanks a lot.</p>
"
"0.118558951157635","0.121140630718605","218970","<p>For a specific analysis I want to calculate the <em>variance partition coefficient</em> (VPC). I am using the following formula:</p>

<pre><code> test &lt;- glmer(SocEenz ~ Herkomst + OuderPersoon + statusscore14 + M_SpoWeek + 
             (1|POSCODN), data = dataScaled, family = binomial)

&gt; summary(test)
     Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
       Family: binomial  ( logit )
       Formula: SocEenz ~ Herkomst + OuderPersoon + statusscore14 + M_SpoWeek +  (1 | POSCODN)
       Data: dataScaled

   AIC      BIC   logLik deviance df.resid 
  43707.5  43757.9 -21847.8  43695.5    32684 

Scaled residuals: 
   Min      1Q  Median      3Q     Max 
 -1.3263 -0.8378 -0.7164  1.1263  2.4788 

  Random effects:
 Groups  Name        Variance Std.Dev.
 POSCODN (Intercept) 0.007148 0.08455 
 Number of obs: 32690, groups:  POSCODN, 173

Fixed effects:
          Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)   -0.56437    0.01798 -31.387  &lt; 2e-16 ***
 Herkomst1      0.49571    0.02980  16.633  &lt; 2e-16 ***
 OuderPersoon1  0.29911    0.02433  12.295  &lt; 2e-16 ***
 statusscore14 -0.09900    0.01353  -7.316 2.56e-13 ***
 M_SpoWeek     -0.08658    0.01225  -7.067 1.58e-12 ***

 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

   Correlation of Fixed Effects:
        (Intr) Hrkms1 OdrPr1 stts14
Herkomst1   -0.436                     
OuderPersn1 -0.553  0.168              
statusscr14 -0.068  0.233  0.013       
M_SpoWeek   -0.044 -0.029  0.138 -0.034
</code></pre>

<p>Because I only have information in the outcome about POSCODN at random effects I am not sure how to calculate the VPC. How can I get an extra row there with information about residuals. For example: </p>

<pre><code>  Random effects:
 Groups  Name        Variance Std.Dev.
 POSCODN (Intercept) 0.007148 0.08455 
 Residual             258.357 16.0735 
 Number of obs: 32690, groups:  POSCODN, 173
</code></pre>

<p>Or do you have other suggestions how to calculate the VPC?</p>

<p>Thanks!</p>
"
"0.148621510602115","0.15185781720314","221412","<p>I want to do a mediation analysis in R on multilevel data where the treatment and mediator are group-level variables while the outcome is recorded at the individual level. The documentation accompanying the <a href=""https://cran.r-project.org/web/packages/mediation/mediation.pdf"" rel=""nofollow"">'mediation' package</a> provides information on how to do that.</p>

<p>Crucially, while my mediator variable is continuous and thus should be estimated with a lm, my outcome variable is binary and thus should be estimated with a glmer (see below). This means the estimated coefficients will lie on different scales. I found that <a href=""http://www.nrhpsych.com/mediation/logmed.html"" rel=""nofollow"">this website</a> provides information on how to make the coefficients comparable across the equations by manual calculations, but am I correct in thinking that there is no way to do this for multilevel analysis with the mediation package? Thus, the results from ""analysis.2"" cannot be interpreted?</p>

<pre><code>&gt; model.m2 &lt;- lm(AttentionForWords ~ NewCondition, data = mediate) 
&gt; model.y2 &lt;- glmer(cbind(PhonemesCorrect,PhonemesIncorrect) ~ 1 + NewCondition + 
           AttentionForWords + (1|Participant), data = data2, family = 'binomial')

&gt; analysis.2 &lt;- mediate(model.m2, model.y2, treat = ""NewCondition"",  
           mediator = ""AttentionForWords"", control.value = ""SilentWithoutNoticing"",  
           treat.value = ""Noticing"")

&gt; summary(analysis.2)

Causal Mediation Analysis 

Quasi-Bayesian Confidence Intervals

Mediator Groups: 

Outcome Groups: Participant 

Output Based on Overall Averages Across Groups 

Estimate 95% CI Lower 95% CI Upper p-value
ACME (control)            0.01195     -0.00505      0.04274    0.21
ACME (treated)            0.01537     -0.00633      0.05191    0.21
ADE (control)             0.10520      0.02421      0.18871    0.01
ADE (treated)             0.10862      0.02575      0.19066    0.01
Total Effect              0.12057      0.03777      0.20497    0.00
Prop. Mediated (control)  0.08130     -0.05277      0.44416    0.21
Prop. Mediated (treated)  0.10946     -0.06537      0.49792    0.21
ACME (average)            0.01366     -0.00584      0.04693    0.21
ADE (average)             0.10691      0.02490      0.18940    0.01
Prop. Mediated (average)  0.09538     -0.05990      0.47435    0.21

Sample Size Used: 863 

Simulations: 1000 
</code></pre>
"
"0.10976425998969","0.112154430818409","221747","<p>Any idea about why the following model specifications should give different fits?</p>

<p>Let's say we have the same data in the following two formats-</p>

<pre><code>&gt; head(data_long)           
ID   correct condition  itemID  
1   1                   A           1  
1   1                   A           2  
1   1                   A           3  
1   0                   A           4  
2   1                   B           1  
2   1                   B           2  
2   0                   B           3  
2   0                   B           4  

&gt; head(data_short)          
ID  sum_correct condition   itemID  
1   3               A           NA   
2   2               B           NA
</code></pre>

<p>The first format is standard and necessary for including by-item random effects, but if we didn't care about including item random effects, one could fit the following model.</p>

<pre><code>&gt; glmer(correct ~ condition + (1|ID), family = binomial, data = data_long)
</code></pre>

<p>Going further, if we don't want to model item random-effects, I've seen examples that use something like the second data format (which only has the sum of correct responses) and the following model (let's say for this example there were only 4 items).</p>

<pre><code>&gt; glmer(cbind(sum_correct, 4 - sum_correct) ~ condition + (1|ID), family = binomial, data = data_short)
</code></pre>

<p>I figured these models should be identical, but in my experience they are not. </p>

<p>Thoughts?</p>
"
"0.158431062631119","0.161880977059544","221856","<p>I compare two diagnostic tests. Therfore, I collected patient data from several studies. The dataframe is similar to this one:</p>

<pre><code>set.seed(10)
data = data.frame( test1 = rbinom(1000, 1, 0.6),
               test2 = rbinom(1000, 1, 0.4),
               reference = rbinom(1000, 1, 0.7),   
               study = sort(paste(""study_"", round(runif(1000, 1, 20),0) ,sep = """")),
               id = 1:1000,
               age = round(rnorm(1000, 60, 10), 0))
</code></pre>

<p>I did a lot of research on how to use hierarchical models for calculating the respective sensitivities and specifities for my tests and tried a lot of variations in the formula of <code>glmer</code>. However, I don't have sufficient  statistical knowledge for interpreting these models. So I don't know if my approach is correct. Therfore I am showing you my latest model.</p>

<p>First, I would like to calculate the logit sensitivity and specifity for each test. 
In a paper by <a href=""http://pubs.rsna.org/doi/suppl/10.1148/radiol.12120509"" rel=""nofollow"" title=""Code is on pages 19f of the appendicecs"">Genders et al.</a> <a href=""http://pubs.rsna.org/doi/suppl/10.1148/radiol.12120509/suppl_file/12120509appendices.pdf"" rel=""nofollow"" title=""Code is on pages 19f"">(appendices)</a> a Stata code to calculate the logit sensitivity and specifity is provided.
I transferred this code to ""R"", but I am not sure if it's correct this way.</p>

<pre><code>m.sen &lt;- glmer(test1 ~ ( 1 | study) + ( 1 | id ), data = subset(data, reference == 1), 
               family = binomial(link = ""logit""), control = glmerControl(optimizer = ""bobyqa""), nAGQ = 1)

require(""useful"")
m.spe &lt;-  glmer(binary.flip(test1) ~ ( 1 | study) + ( 1 | id ), data = subset(data, reference == 0), 
                family = binomial(link = ""logit""), control = glmerControl(optimizer = ""bobyqa""), nAGQ = 1)

logit.sen = fixef(m.sen)  #  0.4079496 
logit.spe = fixef(m.spe)  #  -0.5845133 
</code></pre>

<p>My first question is if it is possible to calculate the logit sensitivity and specifity of a diagnostic test like this.
The next step would be to adjust for different patient characteristics, such as age.</p>

<pre><code>data &lt;- within(data, {age = as.factor(round(age, -1))})
m.sen.age &lt;- glmer(test1 ~ age + ( 1 | study) + ( 1 | id ), data = subset(data, reference == 1),
                   family = binomial(link = ""logit""), control = glmerControl(optimizer = ""bobyqa""), nAGQ = 1)
fix &lt;- fixef(m.sen.age)

# (Intercept)          age30          age40          age50          age60          age70          age80 
#  14.5660762259 -13.4674641465 -14.0319936288 -14.5514773535 -14.1892664080 -14.0713799056 -13.2897828741 
#  age90         age100 
#  -14.1606112199   0.0001916363 
</code></pre>

<p>Now I add the estimates. For example, to determine the logit sensitivity of test1 in patients aged between 55 and 65: <code>sen.50 = fix[1] + fix[5] # 0.3768098</code> 
As an alternative, I thought about further defining the data subset, which produces nearly identically results:</p>

<pre><code>m.sen.age &lt;- glmer(test1 ~ ( 1 | study) + ( 1 | id ), data = subset(data, reference == 1 &amp; age == 60), 
                   family = binomial(link = ""logit""), control = glmerControl(optimizer = ""bobyqa""), nAGQ = 1)
sen.age50 = fixef(m.sen.age) # 0.3768099
</code></pre>
"
"0.205562594952275","0.210038821712942","222949","<p>I am developing GLMM's in order to assess habitat selection (using GLMMs' coeficients to construct Resource selection functions). 
I have (telemetry) data from 5 study areas, and each area has a different number of individuals monitored. </p>

<p>To develop GLMM's, the dependend variable is binary (1-used locations; 0-available locations), and I have a initial set of 14 continuous variables (8 land cover variables; 2 distance variables, to artificial areas and water sources; 4 topographic variables): a buffer was placed around each location and the area of each land cover within that buffer was accounted for; distances were measured from each point to the nearest feature, and topographic variables were obtained using DEM rasters. I tested for correlation using Spearman's Rank, so not all 14 were used in the GLMMs. All variables were transformed using z-score.</p>

<p>As random effect, I used individual ID (In another question (""GLMM: relationship between AIC, R squared and overdispersion?""), it became clear that using study areas as random effect was not useful nor correct).</p>

<p>I constructed a GLMM with 9 variables (not correlated) and a random effect, then used ""dredge()"" function and ""model.avg(dredge)"" to sort models by AIC values. 
This was the result (only models of AICc lower than 2 represented):</p>

<pre><code>[1]Call:
model.avg(object = dredge.m1.1)

Component model call: 
glmer(formula = Used ~ &lt;512 unique rhs&gt;, data = All_SA_Used_RP_Area_z, family = 
     binomial(link = ""logit""))

Component models: 
          df   logLik    AICc  delta weight
123578     8 -4309.94 8635.89   0.00   0.14
1235789    9 -4309.22 8636.44   0.55   0.10
123789     8 -4310.52 8637.04   1.14   0.08
1235678    9 -4309.75 8637.50   1.61   0.06
12378      7 -4311.78 8637.57   1.67   0.06
1234578    9 -4309.79 8637.58   1.69   0.06
</code></pre>

<p>Variables 1 and 2 represent the distance variables; from 3 to 8 land cover variables, and 9 is a topographic variable.
 Weights seem to be very low, even if I average all those models as it seems to be common when delta values are low. Even with this weights, I constructed GLMMs for each of the combinations, and the results were simmilar for all 6 combinations. Here are the results for the first one (GLMM + overdispersion + r-squared):</p>

<pre><code>Random effects:
 Groups    Name        Variance Std.Dev.
 ID.CODE_1 (Intercept) 13.02    3.608   
Number of obs: 32670, groups:  ID.CODE_1, 55

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.54891    0.51174  -1.073 0.283433    
3       -0.22232    0.04059  -5.478 4.31e-08 ***
5       -0.05433    0.02837  -1.915 0.055460 .  
7       -0.13108    0.02825  -4.640 3.49e-06 ***
8       -0.15864    0.08670  -1.830 0.067287 .  
1         0.28438    0.02853   9.968  &lt; 2e-16 ***
2         0.11531    0.03021   3.817 0.000135 ***     
Residual deviance: 0.256           
r.squaredGLMM():
       R2m        R2c 
0.01063077 0.80039950 
</code></pre>

<p>This is what I get from this analysis: </p>

<p>1) Variance and SD of the random effect seems fine (definitely better than the ""0"" I got when using Study Areas as random effect);</p>

<p>2) Estimate values make sense from what I know of the species and the knowledge I have of the study areas;</p>

<p>3) Overdispersion values seem good, and R-squared values don't seem very good (at least when considering only fixed effects) but, as I read in several places, AIC and r-squared are not always in agreement. </p>

<p>4) Weight values seem very low. Does it mean the models are not good?</p>

<p>Then what I did was construct a GLM (""glm()""), so no random effect was used. I used the same set of variables used in [1], and here are the results (only models of AICc lower than 2 represented):</p>

<pre><code>[2] Call:
model.avg(object = dredge.glm_m1.1)

Component model call: 
glm(formula = Used ~ &lt;512 unique rhs&gt;, family = binomial(link = ""logit""), data = 
     All_SA_Used_RP_Area_z)

Component models: 
          df   logLik     AICc   delta weight
12345678   9 -9251.85 18521.70    0.00   0.52
123456789 10 -9251.77 18523.54    1.84   0.21
1345678    8 -9253.84 18523.69    1.99   0.19
</code></pre>

<p>In this case, weight values are higher. </p>

<p>Does this mean that it is better not to use a random effect? (I am not sure I can compare GLMM with GLM results, correct me if I am doing wrong assumptions)</p>
"
"0.149140260907443","0.152387863551892","223008","<p>Good morning all!I am trying to run a binomial gmler model.
My response variable is a binomial variable:  extra pair paternity -->( 1 or 0) I am looking at several continuous variables like weight, tarsus and number of eggs lost. However, I am having problems with my random effects. I would appreciate any help, because I am already 2 days trying to figure out!! Thanks a lot!!!</p>

<p>my data:      </p>

<pre><code>ring_id    nest nest_id number_eggs number_chicks lost_eggs ring_year tarsus weight
1 BD29285 WH00060       6          10            10         0      2016    210   1700
2 BD29286 WH00060       6          10            10         0      2016    200   1510
3 BD29287 WH00060       6          10            10         0      2016    199   1540
4 BD29288 WH00060       6          10            10         0      2016    209   1780
5 BD29289 WH00060       6          10            10         0      2016    199   1670
6 BD29290 WH00060       6          10            10         0      2016    199   1670
  number_epy epy_wpy EPP_nest Epfather
1          0     WPY        0         
2          0     WPY        0         
3          0     WPY        0         
4          0     WPY        0         
5          0     WPY        0         
6          0     WPY        0         
&gt; 
</code></pre>

<p>This is my code</p>

<pre><code>m &lt;- lmer(EPP_nest ~ weight + tarsus + lost_eggs + (1|nest_id) + (1| ring_id) ,family = 'binomial', data=chicks)

summary (m)
</code></pre>

<p>output: </p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod
]
 Family: binomial  ( logit )
Formula: EPP_nest ~ weight + tarsus + lost_eggs + (1 | nest_id) + (1 |      ring_id)
   Data: chicks
Control: structure(list(optimizer = c(""bobyqa"", ""Nelder_Mead""), calc.derivs = TRUE,  
    use.last.params = FALSE, restart_edge = FALSE, boundary.tol = 1e-05,  
    tolPwrss = 1e-07, compDev = TRUE, nAGQ0initStep = TRUE, checkControl = structure(list( 
        check.nobs.vs.rankZ = ""ignore"", check.nobs.vs.nlev = ""stop"",  
        check.nlev.gtreq.5 = ""ignore"", check.nlev.gtr.1 = ""stop"",  
        check.nobs.vs.nRE = ""stop"", check.rankX = ""message+drop.cols"",  
        check.scaleX = ""warning"", check.formula.LHS = ""stop"",  
        check.response.not.const = ""stop""), .Names = c(""check.nobs.vs.rankZ"",  
    ""check.nobs.vs.nlev"", ""check.nlev.gtreq.5"", ""check.nlev.gtr.1"",  
    ""check.nobs.vs.nRE"", ""check.rankX"", ""check.scaleX"", ""check.formula.LHS"",  
    ""check.response.not.const"")), checkConv = structure(list( 
        check.conv.grad = structure(list(action = ""warning"",  
            tol = 0.001, relTol = NULL), .Names = c(""action"",  
        ""tol"", ""relTol"")), check.conv.singular = structure(list( 
            action = ""ignore"", tol = 1e-04), .Names = c(""action"",  
        ""tol"")), check.conv.hess = structure(list(action = ""warning"",  
            tol = 1e-06), .Names = c(""action"", ""tol""))), .Names = c(""check.conv.grad"",  
    ""check.conv.singular"", ""check.conv.hess"")), optCtrl = list()), .Names = c(""optimizer"",  
""calc.derivs"", ""use.last.params"", ""restart_edge"", ""boundary.tol"",  
""tolPwrss"", ""compDev"", ""nAGQ0initStep"", ""checkControl"", ""checkConv"",  
""optCtrl""), class = c(""glmerControl"", ""merControl""))

     AIC      BIC   logLik deviance df.resid 
    56.0     77.5    -22.0     44.0      259 

Scaled residuals: 
      Min        1Q    Median        3Q       Max 
-0.001717 -0.001158 -0.000051  0.020852  0.041496 

Random effects:
 Groups  Name        Variance Std.Dev.
 ring_id (Intercept)    0      0.00   
 nest_id (Intercept) 6613     81.32   
Number of obs: 265, groups:  ring_id, 265; nest_id, 45

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.106e+01  2.222e+01  -0.498  0.61854    
weight      -6.211e-04  7.607e-03  -0.082  0.93493    
tarsus      -6.362e-03  1.157e-01  -0.055  0.95615    
lost_eggs   -6.395e+00  1.777e+00  -3.599  0.00032 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
          (Intr) weight tarsus
weight    -0.252              
tarsus    -0.857 -0.272       
lost_eggs  0.212 -0.170 -0.070
convergence code: 0
Model failed to converge with max|grad| = 0.00997492 (tol = 0.001, component 1)
Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>So I am quite lost.... and I would really apreciate any help!!!!! </p>

<p>Thank you very much!!1
Best, Mara</p>
"
"0.269239595600939","0.253383808278457","223626","<p>In R, I'm wondering how the functions <code>anova()</code> (<code>stats</code> package) and <code>Anova()</code> (<code>car</code> package) differ when being used to compare nested models fit using the <code>glmer()</code> (generalized linear mixed effects model; <code>lme4</code> package) and <code>glm.nb</code> (negative binomial; <code>MASS</code> package) functions. </p>

<p>I've found the two ANOVA functions do not produce the same results for tests of fixed effects in a Poisson mixed model, or a negative binomial fixed effects model (no random effects). Results from both are shown below.</p>

<p><em>My goal</em>: Correctly test the overall significance of a multi-level categorical predictor (fixed; <em>Species</em>). I'm looking for a type III SS-type <em>p</em>-value.</p>

<hr>

<p><em>First</em>: If one fits a <strong>fixed effects</strong> generalized linear model (Poisson here) using <code>glm()</code>, then these two functions <strong>do produce the same results</strong> given the arguments as in the following dummy example:</p>

<pre><code>mod01 &lt;- glm(Count ~ Species + offset(log(Area)), data=data01, family=poisson)

####################
# Anova() function #
####################

library(car)
Anova(mod01, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   255.44  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod01x &lt;- update(mod01, . ~ . - Species)
anova(mod01x, mod01, test=""Chisq"")

# Model 1: Count ~ offset(log(Area))
# Model 2: Count ~ Species + offset(log(Area))

#   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
# 1      1063     1456.4                          
# 2      1055     1201.0  8   255.44 &lt; 2.2e-16 ***

# Test statistics are the SAME (255.44) for the fixed effects model
</code></pre>

<hr>

<p><em>However</em>: For a generalized linear <strong>mixed effects</strong> model (using <code>glmer()</code> with random effect for <em>Group</em>), analogous code <strong>gives a different test statistic across the two functions</strong>:</p>

<pre><code>library(lme4)
mod02 &lt;- glmer(Count ~ 1 + Species + (1 | Group) + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod02, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod02x &lt;- update(mod02, . ~ . - Species)
anova(mod02x, mod02, test=""Chisq"")

# mod02x: Count ~ (1 | Group) + offset(log(Area))
# mod02: Count ~ 1 + Species + (1 | Group) + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod02x  2 1423.9 1433.8 -709.95   1419.9                             
# mod02  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Now the test statistics are DIFFERENT (197.9012 vs. 248.21)

#####################################################################

# Not a matter of type I vs. III SS since whether the fixed or random
# effect is fit first in the model does not affect results:

# List random effect (Group) before fixed (Species):

mod03 &lt;- glmer(Count ~ 1 + (1 | Group) + Species + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod03, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod03x &lt;- update(mod03, . ~ . - Species)
anova(mod03x, mod03, test=""Chisq"")

# mod03x: Count ~ (1 | Group) + offset(log(Area))
# mod03: Count ~ 1 + (1 | Group) + Species + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod03x  2 1423.9 1433.8 -709.95   1419.9                             
# mod03  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Respective test statistics are the same as above case where order of fixed
# and random effects was reversed
</code></pre>

<hr>

<p>Another example of inconsistent test statistics: <strong>Fixed effects negative binomial model</strong>:</p>

<pre><code>library(MASS)
mod04 &lt;- glm.nb(Count ~ Species + offset(log(Area)), data=data01)

####################
# Anova() function #
####################

Anova(mod04, type=3)

# Analysis of Deviance Table (Type III tests)

# Response: Spiders_Tree
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   101.08  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod04x &lt;- update(mod04, . ~ . - Species)
anova(mod04x, mod04)

# Likelihood ratio tests of Negative Binomial Models

# Response: Count
#                            Model     theta Resid. df  2 x log-lik.   Test df LR stat.       Pr(Chi)
# 1           offset(log(Area_M2)) 0.2164382      1063     -1500.688                      
# 2 Species + offset(log(Area_M2)) 0.3488095      1055     -1413.651 1 vs 2  8 87.03677  1.887379e-15 

# Test statistics are also DIFFERENT here (101.08 vs. 87.03677)
</code></pre>

<hr>

<p><em>In summary</em>: The problem:</p>

<ol>
<li>Isn't restricted to only mixed or only fixed effects models</li>
<li>Isn't a matter of type I or III SS, since an example with only one predictor (negative binomial fixed effects model) showed the same problem, and even in the case of more than one predictor (mixed model example), the test is only for the removal of one predictor (<em>Species</em>), so I believe the two types of SS should be equivalent in this case.</li>
</ol>

<p>Could it have to do with the offset? Maybe the functions were written to ""behave well"" with the <code>glm()</code> function, but process others (such as <code>glmer()</code> and <code>glm.nb()</code>) inconsistently? Something else I'm not thinking of?</p>

<hr>

<p>I'm not providing data for my example code above, as I'm assuming someone can comment on the differing theories of each function without a minimal working example. However, if you would like to verify the results really do differ (as shown above), I will add a dummy dataset.</p>
"
"0.118558951157635","0.121140630718605","224372","<p>I have a two-part question that includes issues with generalized linear mixed models and failure to converge. </p>

<p>First, a little bit about my experimental design. I have data where I am trying to test the effects of population and genotype of a tree on the propensity to flower. </p>

<ul>
<li>I went to multiple locations (i.e. populations) to collect cuttings from trees.  </li>
<li>At each tree I took multiple cuttings (each tree is considered a genotype, hence the multiple cuttings from each tree were my genotype replicates). </li>
<li>I then planted all of my cuttings at a single site. Trees were planted in plots of 16 trees, where all 16 trees came from the same population (this is the focus of one of my questions). </li>
</ul>

<p>To analyze the data, I am trying to use the <code>bglmer</code> function in the ""blme"" package of R (as per <a href=""http://stackoverflow.com/questions/25985970/generalised-linear-mixed-model-error-binary-response"">http://stackoverflow.com/questions/25985970/generalised-linear-mixed-model-error-binary-response</a>). </p>

<p>My model contains <code>Population</code>, <code>Genotype</code> nested within <code>Population</code> and <code>Plot</code> as factors. <code>Population</code> and <code>Genotype</code> are treated as random effects and <code>Plot</code> as a fixed effect. </p>

<p>My R code is:</p>

<pre><code>Flower_bayesfull &lt;- bglmer(data=Tam, Flower ~ Plot + (1| Population/GenotypeB), family=binomial, cov.prior=NULL, fixef.prior=normal)
</code></pre>

<p>I am getting warnings about failure to converge with <code>max|grad| = 4.74867</code>.</p>

<p>So my two <strong>questions</strong> are: </p>

<ol>
<li>By including <code>Plot</code> in my analysis, am I properly accounting for my plot design (described above)?</li>
<li>How can I get my model to converge? Or is it impossible, considering my plot structure described above.</li>
</ol>
"
"0.118558951157635","0.121140630718605","225198","<p>I have a question on how to specify a GLMM. I made an experiment with two treatments (control and treated) to test the effect of a water contaminant on reproductive cells of tadpoles. I have data on proportion of oocytes in a histological photo. I took several photos for each animal. My main interest is to test the effect of the treatment on the response variable. <strong>However, animals happen to be in different developmental stages.</strong> I have 5 stages in total. I tried to model these data using a glm including treatment and stage as fixed factor and the photos nested within an animal as random factors. </p>

<p>So first, I found out using: </p>

<pre><code>Ord_plot(datos$Oocitos_Total)
</code></pre>

<p>that the response variable is better modelled by a negative binomial, so I went on and used the <code>lme4::glm.nb</code> like this:</p>

<pre><code>glmer.nb(Oocytes_Total~Treatment+as.factor(Stage)+(1|Animal:Photo), data=datos)
</code></pre>

<p>My question is: should I include the stage as fixed factor or a random factor to which Animal and Photo would be nested within? I'm only interested in the effect of the treatment, I only included the stage to control for this effect. The point is, a given animal will always have the same stage. Is my model above correct?</p>
"
"0.126744850104896","0.113316683941681","226946","<p>Having trouble finding straightforward information this topic.</p>

<p>Basically, I'm trying to use the lme4 package to analyze my data, and the model looks something like (A ~ BCD) + (random effects term 1) + (random effects term 2).</p>

<p>'A' is a yes/no response, which, based on what I've read, indicates that I should use glmer. However, my experiment uses repeated measures - each subject undergoes many trials. It's a psychophysical experiment, so there are many subjects who essentially make yes/no judgements about many, many images. I've read that when there are many trials within a subject, you should use lmer.</p>

<p>What's the best way to go here? Sorry if the info given is too sparse; if anyone thinks they can help me out with this, I'll provide as much info as necessary.</p>

<p><strong>TL;DR: When exactly should one use lmer vs glmer, especially in the context of psychophysical experiments where one subject will undergo many trials with binomial outcomes?</strong></p>

<p>More info/part 2 of question: I initially analyzed my data using ANOVAs in SPSS. The SPSS indicated a highly significant interaction, one that is logical and predicted. When running the same data to modeled in glmer, that interaction in highly insignificant. When running through lmer, it is significant again.</p>

<p>If anyone can help shed some light on whether this makes sense or why it would be so, I'd appreciate it very much.</p>
"
"0.126744850104896","0.113316683941681","229211","<p>I have a logistic mixed model, where I look at the effect of two conditions over  accuracy (% correct) for different subjects. </p>

<pre><code>glmres = glmer(accuracy ~ condition + (1|subject), data = myData, family = binomial) 
</code></pre>

<p>All subjects saw the same 100 stimuli (words), but the experimental condition these words belonged to depended on the subject. So for example, for some subjects word A belonged to condition C1 and word B belonged to condition C2. For some subjects it was the other way around. The words were drawn from a bigger pool so I think I should include them as a random effect. </p>

<p>The problem is, the correspondence between words and conditions could not be (precisely) controlled and it is the case that for <em>most</em> subjects (about 70:30, depending on the word), A belonged to condition C1.
This means that the fixed effect <code>condition</code> and the (potential) random effect <code>(1|word)</code> are not completely independent. </p>

<p>My question is whether it would be incorrect to include two non-independent effects (where one is fixed, one is random). As a result of this dependency (I think) my results change a lot according to whether I include the random effect or not. </p>
"
"0.179244285979288","0.171700704935861","229238","<p>Iâ€™m working with a GLMM using a binomial distribution, using glmer in R. In my experiment birds in a flock visit a grid with patches of two colours (blue and green), but only one colour (blue) contains food. Half the birds visit one condition, and the other half a second condition. Birds take part in 70 trials, but I observe only one individual per trial, and therefore observe each individual in 14 trials. Within a trial, birds can visit several patches, and I also count revisits to the same patch at a later time in the trial (the number of patches visited ranges from 1 to 20, but is 6 or lower in 85 % of the trials). I am interested in the speed with which the birds learn to prefer the correct colour, and whether it is affected by condition and a few other fixed effects. Since the number of blue patches differs between conditions (2 out of 16 or 4 out of 16), an individual visiting patches at random would be expected to have a different proportion of blue patches visited (0.125 in one and 0.250 in the other), and I therefore can't compare the preference for blue (measured as the proportion of visits that were to blue patches) between conditions. So I coded their responses as 1 if the proportion of patches visited was higher than the random expected proportion and 0 if it was equal to or lower than the random expected proportion. In this way I can compare responses in the two conditions. However, I am uncomfortable with the fact that the numbers of patches visited by an individual in each trial varies, and so a score based on only 1 patch visit is not as reliable as one based on 6 visits. I thought to include this as a weight in the model, but am not sure if this is the correct approach. I spoke with a stats advisor who said it might be better to include it as another fixed effect.</p>

<p>My response variable is Success equal to 0 or 1. My fixed effects are the number of trials done (from 1 to 70), the condition in which an individual was engaged (quick or long), the day (1 or 2), and the proportion of patches it joined (varies between 0 and 1). My random effects are its ID and its flock.</p>

<p>So my two options are:</p>

<p>1) Add the total number of patches discovered as a weight:</p>

<pre><code>glmer(Success ~ Trial + Condition + Day + Join.prop + Trial:Condition + Trial:Day + Trial:Join.prop + Condition:Day + Condition:Join.prop + Day:Join.prop + (1|ID) + (1|Flock), weights = Total.disc, data = myData, family = binomial(link = ""logit""), control = glmerControl(optimizer = ""bobyqaâ€))
</code></pre>

<p>2) Add it as a fixed effect. But the potential interactions with this factor would also greatly complicate my model:</p>

<pre><code>glmer(Success ~ Trial + Condition + Day + Join.prop + Total.disc + Trial:Condition + Trial:Day + Trial:Join.prop + Trial:Total.disc + Condition:Day + Condition:Join.prop + Condition:Total.disc + Day:Join.prop + Day:Total.disc + Join.prop:Total.disc + (1|ID) + (1|Flock), data = myData, family = binomial(link = ""logit""), control = glmerControl(optimizer = ""bobyqaâ€))
</code></pre>

<p>I would be very interested to hear thoughts on whether either of these approaches is appropriate, and which seems like the best option. </p>

<p>Thanks in advance! :)</p>
"
"0.174766202492684","0.199580272360031","229260","<p>I have behavioral response data (counts of acts within an hour trial) that I would like to analyze using a GLMM. I'm new to GLMM's but after months of digging through papers and forums, I've concluded this is likely the best approach to analyze my data.</p>

<p>The experiment was an exposure study where small estuarine crabs were exposed to fluoxetine over a 60-day study. We observed their behaviors using ethograms during day and night trials. We also observed their behaviors day and night with a predator added to the tank. There were 4 trials per week (Day -Pred, Day +Pred, Night -Pred, Night +Pred). Our question was whether exposure to the drug ultimately altered their behaviors (being still, mobile, foraging, predator avoidance, etc.) during these trials and if there was an effect of exposure time (i.e., greater effect with longer exposure)?</p>

<p>Because the data are counts within a specified trial period there are observed proportions (i.e., observed behavioral acts / 12 possible observation windows) I believe I should use either a Poisson or a binomial distribution to fit the model. We decided to subset the main dataset into trial types because day/night and +Pred/-Pred aren't treatments per se, so we constructed models for each subset rather than using one over-arching model. </p>

<p>For example we built a model to compare the active behaviors across treatments within the Day+Predator subset: </p>

<p><strong>For reference the variable terms are:</strong>  </p>

<ul>
<li>actsuc= successful active behaviors</li>
<li>actfail= active behavior failures</li>
</ul>

<p><strong>Fixed effects:</strong>  </p>

<ul>
<li>Treatment = 3 levels: Control, 3ng, 30ng fluoxetine; </li>
<li>exposure = 20 days, 40 days, 60 days; </li>
<li>crabsex = Dominant male, subordinate female, subordinate male;</li>
</ul>

<p><strong>Random effects:</strong>  </p>

<ul>
<li>TankID= to account for non-independence of crab within the same tank (3 in each)</li>
<li>Trial = to account for non-independence of multiple trials over exposure study</li>
<li>Crab. = to account for non-independence of multiple observations on a single crab within and across trials </li>
</ul>

<p><strong>Model with interaction terms</strong>  </p>

<pre><code>actfit.glmm.interact = glmer(cbind(actsuc, actfail) ~ Treatment:exposure +  
                             Treatment:crabsex + (1|TankID) + (1|Trial) + (1|Crab.), 
                             family=binomial, data=pday)
</code></pre>

<p><strong>I need help understanding interaction and nested notation using</strong> <code>glmer</code>. <strong>I'm not sure if I've set this up correctly.</strong> I get the following error message:</p>

<pre><code>fixed-effect model matrix is rank deficient so dropping 1 column / coefficient
Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0247253 (tol = 0.001, component 6)
</code></pre>

<p>Further, I cannot use the <code>predict</code> function with this model as it is. See script and resultant error message below:</p>

<pre><code>pday$predict.act &lt;- predict(actfit.glmm.interact, newdata = pday,  re.form=NA, 
                            type=""response"")

Error in X %*% fixef(object) : non-conformable arguments
</code></pre>

<p><strong>I have used the 'full' version</strong> of this model without interactions among fixed factors and was able to get predicted values without any error message:</p>

<pre><code>actfit.glmm.full = glmer(cbind(actsuc, actfail) ~ Treatment + exposure + crabsex + 
                         (1|TankID) + (1|Trial) + (1|Crab.), family=binomial, data=pday)
</code></pre>

<p>Is this more appropriate, to treat the fixed effects without an interaction? I would like to compare behaviors across genders (crabsex) between treatments, but would like to know if the effect changes with exposure period (which I would assume would require an interaction). </p>

<p>I've seen in previous posts that its useful to upload data or a screen shot of the dataframe for reference. Once I figure out how to do that I will upload here. </p>
"
"0.0448110714948221","0.045786854649563","230721","<p>I have the following model  </p>

<pre><code>fit1 &lt;- glmer(Res~FA+FB+FC+(1|fsite), family=binomial(), data=DATA)
</code></pre>

<p>the result of <code>summary()</code> is:  </p>

<pre><code>summary(fit1)
Generalized linear mixed model fit by maximum likelihood 
 (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Res ~ FA + FB + FC + (1 | fsite)
   Data: DATA

     AIC      BIC   logLik deviance df.resid 
   202.3    229.9    -92.1    184.3      150 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.1768 -0.6167 -0.4967  0.6815  2.0132 

Random effects:
 Groups Name        Variance Std.Dev.
 fsite  (Intercept) 0        0       
Number of obs: 159, groups:  fsite, 28

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.55573    0.55830   2.787 0.005327 ** 
FA2         -0.11914    0.37344  -0.319 0.749692    
FB2         -1.38652    0.39967  -3.469 0.000522 ***
FC2         -0.14976    0.61984  -0.242 0.809076    
FC3         -0.06794    0.63171  -0.108 0.914350    
FC4         -1.20114    0.61670  -1.948 0.051452 .  
FC5         -1.44951    0.62817  -2.308 0.021025 *  
FC6         -1.13590    0.65427  -1.736 0.082538 .  
---
Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1

Correlation of Fixed Effects:
        (Intr) fspcs2    FB2    FC2    FC3    FC4    FC5 
FA2     -0.466                                          
FB2     -0.456  0.169                                   
FC2     -0.572 -0.021  0.017                            
FC3     -0.596  0.050  0.036  0.506                     
FC4     -0.582 -0.005  0.020  0.519  0.509              
FC5     -0.558  0.019 -0.038  0.508  0.500  0.511       
FC6     -0.391 -0.101 -0.288  0.485  0.467  0.486  0.492
</code></pre>

<ul>
<li>Why are the variance and Std.Dev of the random effects zero?</li>
<li>How do I check for overdispersion in this model?</li>
<li>What should do if there is overdispersion?</li>
</ul>
"
"0.142294263046161","0.145392790714993","230911","<p>I'm not entirely sure of fitting the model for experiment we've made. The variables and relevant description are as follows:</p>

<ul>
<li>ID - participant ID </li>
<li>Trial - 60 for each participant</li>
<li>Memory - between subject binary factor</li>
<li>State - within subject binary factor  </li>
</ul>

<hr>

<ul>
<li>Correct - whether classification a participant made was correct or not</li>
<li>Rating - the judgement made after each trial on four point Likert scale</li>
</ul>

<p>Procedure brief: each participant (N=60) was randomly assigned to experimental or control group (Memory) and had 120 Trials (60 for State = 0 and 60 for State = 1). Each trial composed of perceptual classification (Correct) and judgment of how easy it was (Rating). The classification problem was randomly selected from two groups each trial (State).</p>

<p>I would like to calculate what impacts the performance (Correct) most - is it memory, state, a specific rating on a scale or any combination of above? I'm not interested in between subject variance, on the oposite, it is a random factor here. Also, it appears that there is bias in responses on Likert scales, so that part of variance should be excluded too. </p>

<p>The way I was thinking to approach this is generalized mixed linear model, but I'm not sure I'm doing it right; there is what I've got so far:</p>

<pre><code>model = glmer(Correct ~ (1|ID/Rating) + Memory * State * Rating, data, family=binomial, 
              control = glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun=100000)))
</code></pre>

<p>Is this approach correct? I'll appreciate your input.</p>

<p>Relevant resources I used: </p>

<ul>
<li><a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">Formulae in R: ANOVA and other models, mixed and fixed</a></li>
<li><a href=""http://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/"" rel=""nofollow"">The Difference Between Crossed and Nested Factors</a> </li>
<li><a href=""http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model"">When is it ok to remove the intercept in a linear regression model?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/225198/nested-random-factor-with-confounding-random-variable"">Nested random factor with confounding (random?) variable</a></li>
</ul>
"
"0.134433214484466","0.137360563948689","231101","<p><strong>Agricultural Experimental design</strong>: a split-split plot in which a field is divided into 3 replications; each replication is divided into 2 to apply different pesticide spraying programs and each spray-plot is again divided into 2 to apply monocropping in one of the subplots and intercropping in the other subplot. </p>

<p><strong>Data collection</strong>: in each plot, 5 plants were sampled to count the insects on the main crop, thereby creating pseudoreplication. The simplest approach is to just take the mean of the counts on these 5 plants to deal with this kind of pseudoreplication. Instead of integers, my new dataset will have now decimal numbers.
Research question: we wish to test whether the spraying program and the cropping system have any effect on insect density on the main crop. </p>

<p><strong>Analyzing approach</strong>: I have COUNT data and furthermore spatial pseudoreplication arising from the split-plot design, so I decide to work with General Mixed Effect Models. I'm sure I will have problems with overdispersion, so instead of poisson distribution (for Count data), I would like to work with negative binomial distribution. </p>

<p><strong>Problem</strong>: most of us will know, Negative Binomial (or even Poisson) distribution don't work for non-integers and my dataset contains now means of the counts, so decimal numbers. If I just ignore the pseudoreplication due to the 5 plants per plot, and run the glmer.nb() function, my degrees of freedom are too high. However, I read that using the appropriate mixed-effect model would remove pseudoreplication. </p>

<p>My <strong>function</strong> looks like this: 
model&lt;-glmer.nb(Mean.Count~Spraying.Program*Cropping.System+(1|Replication/Spraying.Program),data=Incidence)</p>

<p><strong>Questions</strong>: 
(1) How can I deal with the pseudoreplication arising from the sampling of 5 plants/plot?
(2) Is it possible to adapt my formula and still use mixed-effect models on my data, or should I go for another strategy?</p>
"
"0.156838750231877","0.171700704935861","231680","<p>I have a dataset of behavioural observations in an experiment on free ranging breeding birds. I'd like to calculate GLMMs. </p>

<p>The dataset has the following parameters:</p>

<ul>
<li>Behaviour (count data)</li>
<li>Behaviour during Baseline (count data)</li>
<li>Nest</li>
<li>Individual</li>
<li>Treatment Condition</li>
<li>Sex of Individual</li>
<li>Amount of time an individual was seen during experiment</li>
</ul>

<p>I would like to use the time an individual was seen as a model weight, as I want cases in which the individual in question is seen more to have higher influence in the model than cases in which they are only seen very rarely.</p>

<p>When checking my response variable (the counts of the behaviour) it most likely fits a negative binomial distibution:
<a href=""http://i.stack.imgur.com/c5FCd.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/c5FCd.png"" alt=""The qq-plots of my data vs. theoretical distribution""></a></p>

<p>When looking at the data the it has a very high count of zeros (133 out of 240 to be exact)
<a href=""http://i.stack.imgur.com/u43bG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/u43bG.png"" alt=""Sorry about the axes, this is only a quick&#39;n&#39;dirty job""></a></p>

<pre><code>quantile(Data$ACTW)
  0%    25%    50%    75%   100% 
0.00   0.00   0.00  44.75 909.00 
</code></pre>

<p>The problem I face now is that when I want to fit a negative binomial model with nested random effects with <a href=""/questions/tagged/lme4"" class=""post-tag"" title=""show questions tagged &#39;lme4&#39;"" rel=""tag"">lme4</a>, I come across a convergence error.</p>

<p>This is my Model:</p>

<pre><code>ACTW_Model_CP &lt;- glmer.nb(ACTW ~ Condition + Parent + (1|Nest/Individual) + (1|ACTW_BL), 
                          weights = Seen, data = Data, verbose = TRUE)
</code></pre>

<ul>
<li>ACTW is the Behaviour in question</li>
<li>Condition is the treatment condition</li>
<li>Parent is the sex</li>
<li>Nest is the specific nest/the location</li>
<li>Individual is the identifier for the individual</li>
<li>ACTW_BL is the behaviour recorded during baseline observations</li>
<li>Seen is the amount of time the individual was seen (Zero to one)</li>
</ul>

<p>Running the model resulted in:</p>

<pre><code>&gt; ACTW_Model_CP &lt;- glmer.nb(ACTW ~ Condition + Parent + (1|Nest/Individual) + (1|ACTW_BL),
+ data = Data, weights = Seen, verbose = TRUE)
theta.ml: iter 0 'theta = 0.830397'
theta.ml: iter1 theta =-0.687164
theta.ml: iter2 theta =-0.0026382
theta.ml: iter3 theta =0.00521452
theta.ml: iter4 theta =0.0102195
theta.ml: iter5 theta =0.0197524
theta.ml: iter6 theta =0.0373503
theta.ml: iter7 theta =0.0683011
theta.ml: iter8 theta =0.118839
theta.ml: iter9 theta =0.192281
theta.ml: iter10 theta =0.280215
theta.ml: iter11 theta =0.354639
theta.ml: iter12 theta =0.387487
theta.ml: iter13 theta =0.391826
theta.ml: iter14 theta =0.39189
th := est_theta(glmer(..)) = 0.3918897
Error in eval(expr, envir, enclos) : 
  pwrssUpdate did not converge in (maxit) iterations
In addition: Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.017808 (tol = 0.001, component 1)
</code></pre>

<p>I have tried to simplify the model, yet I did not manage to get any models calculated. I also tried both optimisers included in lme4.</p>

<p>My question is as follows: 
Is there a way to model my data in a way that accounts for the amount of time an individual was seen during the experiment?</p>

<p>I'm running lme4 version 1.1-12 in R 3.2.1 under Windows 7 (x86_64-w64-mingw32).</p>
"
"0.134433214484466","0.137360563948689","231980","<p>I am trying to recreate a PROC GLIMMIX command in R using glmer.  Here is a link to the data (from SAS product support GLIMMIX documentation): <a href=""https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_glimmix_a0000001403.htm"" rel=""nofollow"">https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_glimmix_a0000001403.htm</a></p>

<p>The SAS code is</p>

<pre><code> proc glimmix data=multicenter;

      class center group;

      model sideeffect/n = group / solution;

      random intercept / subject=center;

   run;
</code></pre>

<p>The coefficients are:</p>

<pre><code>Intercept: -0.8071

Group A : -0.4896

Group B: 0
</code></pre>

<p>Here is the R command, after swapping 1 and 0 in the <code>sideeffect</code> column to align the defaults in R and SAS:</p>

<pre><code>mt &lt;- glmer(sideeffect/n ~ group + (1|center), data = test, family = binomial, 
    weights=n, control = glmerControl(optimizer = ""bobyqa""), nAGQ = 10)
</code></pre>

<p>The coefficients are:</p>

<pre><code>Intercept: 1.3379

Group B: -0.4966
</code></pre>

<p>The different coefficients are not concerning on their face, since I transformed the data. However, when I compute the associated probabilities, I get from SAS:</p>

<pre><code>P(sideeffect | A) = 0.2147

P(sideeffect | B) = 0.3085
</code></pre>

<p>and from R:</p>

<pre><code>P(sideeffect | A) = 0.2078556

P(sideeffect | B) = 0.3018929
</code></pre>

<p>These estimates have a discrepancy of approximately 2%.  I know that R and SAS use slightly different approaches to the generalized linear models - is this enough to explain this discrepancy?  If not, what should I do to get R to conform to the SAS code?  I have seen an example online using different data where R code exactly replicated the SAS results: <a href=""https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q3/004002.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q3/004002.html</a></p>
"
"0.141705050316284","0.14479074758769","232031","<p>I am analyzing my data using a generalized linear mixed model in R. My design has three categorical variables:  </p>

<ul>
<li><code>language</code> (three levels: English vs French vs Japanese),a between subject effect, that is, three groups of participants</li>
<li><code>function</code> (two levels: radical vs component), a within-subject effect</li>
<li><code>position</code> (five levels: left, right, top, bottom, inside), a within-subject effect</li>
<li>and the dependent variable <code>response</code> is binomial (0 or 1) </li>
</ul>

<p>I model these three categorical variables as fixed effects while <code>subject</code>(the participants) and <code>item</code>(the experimental trail) are random effects. The R code looks like this:</p>

<pre><code>glmm &lt;- glmer(formula=response~proficiency*function*position+
                 (1|item)+(1|subject), 
              family=binomial, data=data1,
          control=glmerControl(optimizer=""bobyqa""))
</code></pre>

<p>My questions are:</p>

<ol>
<li><p>I would like to see the interaction between these three IVs (<code>proficiency*function*position</code>). Is the R code suitable? How could I get the $p$-value of this interaction?</p></li>
<li><p>How do I interpret the results? Since the model sets English, component and bottom as the baseline levels by default, I cannot find the comparison of the baseline. So the question is, how could I do the comparison between different levels? (For example, <code>English.component.bottom</code> vs <code>English.radical.bottom</code>, since English, component and bottom were set as the baseline, I could not find them in the output of the model.) </p></li>
<li><p>After looking though the websites, somebody suggested that by <code>relevel</code> the baseline then can find the results of different levels, but I found that the estimate and the $p$-value change by releveling the baseline.</p></li>
<li><p>Somebody suggested using the <code>glht</code> function, but the output is general linear hypotheses rather than generalized linear mixed model. Does that influence the validity of the results?</p></li>
</ol>
"
"0.118558951157635","0.121140630718605","232450","<p>We are developping a software that run hierarchical linear model in R with the lme4 package. The model we are trying to fit is of the following shape:</p>

<pre><code>&gt; data
ID | Dummy_1 | Dummy_2 | Dummy_3 | Rating 
1  | 0       | 1       | 1       | 14
1  | 1       | 0       | 1       | 15
1  | 0       | 1       | 0       | 11
2  | 1       | 0       | 1       | 15
2  | 1       | 0       | 0       | 12

x = lmer(formula = Rating ~ Dummy_1 + Dummy_2 + Dummy_3 + (1 + Dummy_1 + Dummy_2 + Dummy_3 | ID), data = data)
</code></pre>

<p>It is important to note that this is the shape that the data will take, however, Rating can have very different range depending on the data te user provide.</p>

<p>The example above illustrate a limit case we are trying to deal with which occurs when the dummy variables perfectly predict the independant variable.</p>

<p>Here we can see that for example with <code>intercept = 10</code>, <code>B1 = 2</code>, <code>B2 = 1</code> and <code>B3 = 3</code> we perfectly predict the Rating variable. It implies first that the <code>ID</code> is useless and that we are in a case of <code>complete separation</code>.</p>

<p><strong>Question:</strong> How do you deal with (quasi-)complete separation when the independant variable is not binomial but continuous as it is the case here ? I could only find explanations for logistic regression. Please, ignore the fact that I use a linear regression for discrete-ordinal data and that I treat them as continuous :)</p>

<p>So that you know the warnings I get from R are the following:</p>

<pre><code>1: In optwrap(optimizer, devfun, getStart(start, rho$lower,  ... :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  unable to evaluate scaled gradient
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  Model failed to converge: degenerate  Hessian with 4 negative eigenvalues
</code></pre>
"
"0.101621958135115","0.121140630718605","232648","<p>I am seeking statistical advice on the random-effects structure of a mixed-model. I am using R's lme4 package.</p>

<p>Based on recent papers showing the importance of the random-effects structure  (such as <a href=""http://www.sciencedirect.com/science/article/pii/S0749596X12001180"" rel=""nofollow"">http://www.sciencedirect.com/science/article/pii/S0749596X12001180</a>), I would like to make sure that my random-structure is correct.</p>

<p>More specifically, I have predictors A and B and dependent variable Y. </p>

<p>Predictor A constitutes the experimental manipulation (every subject comes twice to the lab, undergoing treatment 1 or 2), and I have mulitple observations of the dependent variable Y (100 per participant, ""ID""). B in contrast, is a nuisance variable (i.e., Hunger), which is assummed to be constant over the short time of the experiment.</p>

<p>Now I am not sure how to correctly specify the random effects if </p>

<p>a) I am interested (a priori) in the interaction between A and B. 
Would </p>

<pre><code>summary(a&lt;-glmer( Y ~ A * B + (1 + A*B|ID), data= x, family=""binomial""), REML=FALSE)
</code></pre>

<p>or </p>

<pre><code>summary(a&lt;-glmer( Y ~ A * B + (1 + A|ID), data= x, family=""binomial""), REML=FALSE)
</code></pre>

<p>be the correct model? </p>

<p>b) Assuming I am only interested in the main effect of A. B (e.g., hunger, or something that is constant over all experimental seesions, such as age) is considered a nuisance variable. 
Would</p>

<pre><code>summary(a&lt;-glmer( Y ~ A + B + (1 + A + B|ID), data= x, family=""binomial""), REML=FALSE)
</code></pre>

<p>or</p>

<pre><code>  summary(a&lt;-glmer( Y ~ A + B + (1 + A|ID), data= x, family=""binomial""), REML=FALSE)
</code></pre>

<p>be the correct model? Based on the post
<a href=""http://stats.stackexchange.com/questions/176300/mixed-model-analyses-with-interactions-in-the-random-effects-structure"">Mixed Model Analyses with Interactions in the Random Effects Structure</a>
I would suggest the second, but please let me know if I am incorrect. </p>

<p>Thank you, Laura</p>
"
"0.10976425998969","0.112154430818409","233366","<p>I am trying to use <code>lme4::glmer()</code> to fit a binomial GLMM with dependent variable that is not binary, but a continuous variable between zero and one. One can think of this variable as a probability; in fact it <em>is</em> probability as reported by human subjects (in an experiment that I help analyzing). The <code>glmer()</code> yields a model that is clearly off, and very far from the one I get with <code>glm()</code>, so something goes wrong. Why? What can I do? </p>

<hr>

<p><strong>More details</strong></p>

<p>Apparently it is possible to use logistic regression not only for binary DV but also for continuous DV between zero and one. Indeed, when I run </p>

<pre><code>glm(reportedProbability ~ a + b + c, myData, family=""binomial"")
</code></pre>

<p>I get a warning message</p>

<pre class=""lang-none prettyprint-override""><code>Warning message:
In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>but a very reasonable fit (all factors are categorical, so I can easily check whether model predictions are close to the across-subjects-means, and they are). </p>

<p>However, what I actually want to use is</p>

<pre><code>glmer(reportedProbability ~ a + b + c + (1 | subject), myData, family=""binomial"")
</code></pre>

<p>It gives me the identical warning, returns a model, but this model is clearly very much off; the estimates of the fixed effects are very far from the <code>glm()</code> ones and from the across-subject-means. (And I need to include <code>glmerControl(optimizer=""bobyqa"")</code> into the <code>glmer</code> call, otherwise it does not converge at all.)</p>
"
"0.141705050316284","0.14479074758769","234028","<p>I am trying to use a binomial generalized linear mixed model to analyze binary data of an experiment. Just few details on the experiment that could be useful:  </p>

<ul>
<li>The dependent variable is <code>Score</code>: 0 (incorrect); 1 (correct)</li>
<li>The two predictors are <code>CongRec</code>: -1 (Congruent); +1 (Incongruent) and
<code>TempsExp</code>: 6 levels (33ms,50ms,67ms,..117ms), used as a categorical
predictor.</li>
</ul>

<p>For each combination of the levels of the two predictors, we have 40 observations for 29 participants.</p>

<p><strong>The issue</strong>: When I try to fit a model with only the main effects of the predictors (+intercept) and by-subject random slope for both <code>Congruency</code> and <code>ExposureTime</code>, the model fails to converge.</p>

<p>In such cases, I have been told to remove the random effect with the smallest variance. However depending on the way I define the predictor <code>Congruence</code>, i.e., as a categorical predictor with 2 levels (<code>Congruence</code>) or as continuous variable (<code>CongRec</code>), the random effect with the smallest variance differs.</p>

<p>Here is the R command for the (second) model:  </p>

<pre><code>Model4_Categbis = glmer(formula = Score~1+CongRec+TempsExp+(1+CongRec+TempsExp|Sujet), 
                        data=donneestestCN, family=""binomial"", REML=F)
</code></pre>

<p>Here are the random effects for both cases:  </p>

<ol>
<li><p><code>Congruency</code> as categorical predictor  </p>

<p><a href=""http://i.stack.imgur.com/cRLas.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cRLas.png"" alt=""enter image description here""></a></p></li>
<li><p><code>Congruency</code> as continuous predictor  </p>

<p><a href=""http://i.stack.imgur.com/rWRZJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rWRZJ.png"" alt=""enter image description here""></a></p></li>
</ol>

<p>So basically, in the first case, I should remove the random slope for <code>CongRec</code> while, in the second case, I should remove the random intercept by subject. Normally, I think the way I define the predictor <code>Congruence</code> should not have any influence on model main characteristics but, here, it does because of the non-convergence.</p>

<p>So, which random effects should I remove in your opinion and, that being done, which type of variable should I use for Congruency?</p>
"
"0.100200602007025","0.102382519472325","234066","<p>The bird auditory surveys consist of >100 roadside survey routes across Ontario. Bird call count was conducted at 10-20 stations along each survey routes for >10 years. For each station, there is data on the amount of forest harvested within the last 5 years.  An objective is to assess how bird abundance is affected by forest harvesting.</p>

<p>My problem is to how to deal with spatial and temporal autocorrelation and the violation of independence of data. That is, the response variable (abundance of birds) is likely to be spatially and temporally autocorrelated. And, the bird abundance at each station is likely correlated with that at other stations within a route. My approach is to incorporate routes and year as random effects in generalized mixed effects models as shown below (using <code>lme4</code> package). But, I am not sure how well autocorrelation is modeled adequately in this way.</p>

<pre><code>glmer(Abundance ~ Area_harvested + (1 | route) + (1 | Year),
      data = mydata, family = poisson)
</code></pre>

<p>Although I specified Poisson above, negative binomial or zero-inflated models (because there are many zeros; abundance = 0) may be more appropriate.</p>

<p>Could anyone please suggest a proper way to analyze my data? Also, could you please suggest better or proper way to specify random effects given my data?</p>
"
"0.0633724250524478","0.0647523908238176","234498","<p>First, many thanks to the developers of <code>stan_glmer</code>. I have fit a mixed logistic model, <code>value ~ (1 | site_no) + Is.mid + sinWeek + cosWeek</code>. When fitted by <code>glmer</code>, it returns just the variance on <code>site_no</code>. From <code>stan_glmer</code>, I get coefficients (intercept effects) on each site, plus a variable named <code>b[(Intercept) site_no:_NEW_site_no]</code> that is reported by <code>stan_summary</code>.</p>

<p>I am after marginal posterior of the random effects. Is that what the <code>_NEW_</code> coefficient is giving me? i.e., Is this a marginal posterior not conditioning on the site in the <code>new = dataset</code> provided?</p>

<p>Also, how do I find a variance on the random effect, or is the model not specified correctly? The call is below. I welcome any other suggestions as well. Thank you in advance for any help in finding documentation for these <code>stan_glmer</code> outputs.</p>

<pre><code>stan.T.mod &lt;- stan_glmer(form, iter = 50000, warmup = 5000, thin = 1,
family = binomial(link = ""logit""),
prior = student_t(location = 0, df = 1, scale = 10),
prior_intercept = student_t(location = 0, df = 7, scale = 1),
prior_covariance = decov(shape = 10, scale = 1),
chains = 4, seed = 0305991, data = sub.data)
</code></pre>
"
"0.100200602007025","0.102382519472325","234947","<p>I'm looking to run a linear mixed effect model using lme4, where my dependent variable <code>one_syllable_words / total_words_generated</code> is a proportion and my random effect <code>(1 | participant_ID)</code> reflects the longitudinal nature of the design. Independent, fixed effect variables of interest include <code>age</code>, <code>group</code>, <code>timepoint</code>, and interactions between them. </p>

<p>I've come across two main ways to deal with the proportional nature of the DV:  </p>

<ol>
<li><p><strong>Standard logistic regression / binomial GLM</strong>  </p>

<p>In my scenario, I envision the lme4 equation looking like this:  </p>

<pre><code>glmer(one_syllable_words / total_words_generated ~ age + group +
timepoint + age:timepoint + age:group + timepoint:group + (1 |
participant_ID), family = ""binomial"", weights =
total_words_generated, data = mydat)  
</code></pre></li>
<li><p><strong>Beta regression</strong>  </p>

<p>I would apply a transformation to my DV <code>(DV * (n - 1) + .5)/ n)</code> so that it cannot equal 0 or 1. (There are a few instances where it equals zero, no instances where it equals one.)  </p></li>
</ol>

<p>I'm unclear whether logistic regression or beta regression is preferred in this example. My DV isn't a clear-cut case of successes and failures (unless we stretch the definition of ""success""), so I'm worried logistic regression might not be appropriate. However, I'm having trouble getting a firm grasp on beta regression &amp; all it entails. If beta regression is preferred:  </p>

<ol>
<li>Why is it preferred?  </li>
<li>What is it doing ""behind the scenes"" to the data?  </li>
<li>How can it be applied in R?  </li>
</ol>
"
"0.0448110714948221","0.045786854649563","235187","<p>I get that for each interaction term, there should be the individual covariate term specified in model (<a href=""http://stats.stackexchange.com/questions/27724/do-all-interactions-terms-need-their-individual-terms-in-regression-model"">source</a>), otherwise the model is prone to location shifts  - ie. the scale of variables is starting to count - similarly to as when dropping the intercept (please correct me if this understanding is wrong).</p>

<p>My question is then: does it apply for each level of interaction?
Am I right to do:</p>

<pre><code>glmer(Y ~ X1 + X2 + X3 + X1:X2:X3 + (1|X4), data, family = binomial)
</code></pre>

<p>Or should I include the interactions X1:X2, X2:X3 and X1:X3 also?
I intuitively feel, that no, but experimenting with this shows me differences in significance, hence the question.</p>

<p>I'll add, that my hypothesis concerns only the second level interaction.</p>
"
