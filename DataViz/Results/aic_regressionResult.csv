"V1","V2","V3","V4"
"0.0779237207906526","0.0958706236059213","  1432","<p>In answering <a href=""http://stats.stackexchange.com/questions/1412/consequences-of-an-improper-link-function-in-n-alternative-forced-choice-procedur"">this</a> question John Christie suggested that the fit of logistic regression models should be assessed by evaluating the residuals.  I'm familiar with how to interpret residuals in OLS, they are in the same scale as the DV and very clearly the difference between y and the y predicted by the model.  However for logistic regression, in the past I've typically just examined estimates of model fit, e.g. AIC, because I wasn't sure what a residual would mean for a logistic regression.  After looking into R's help files a little bit I see that in R there are five types of glm residuals available, c(""deviance"", ""pearson"", ""working"",""response"", ""partial"").  The help file refers to Davison, A. C. and Snell, E. J. (1991) Residuals and diagnostics. In: Statistical Theory and Modelling. In Honour of Sir David Cox, FRS, eds. Hinkley, D. V., Reid, N. and Snell, E. J., Chapman &amp; Hall, of which I do not have a copy.  Is there a short way to describe how to interpret each of these types?  In a logistic context will sum of squared residuals provide a meaningful measure of model fit or is one better off with an Information Criterion?</p>
"
"0.11525073729837","0.113435651621629","  5304","<p>Dear everyone - I've noticed something strange that I can't explain, can you? In summary: the manual approach to calculating a confidence interval in a logistic regression model, and the R function <code>confint()</code> give different results.</p>

<p>I've been going through Hosmer &amp; Lemeshow's <em>Applied logistic regression</em> (2nd edition).  In the 3rd chapter there is an example of calculating the odds ratio and 95% confidence interval.  Using R, I can easily reproduce the model:</p>

<pre><code>Call:
glm(formula = dataset$CHD ~ as.factor(dataset$dich.age), family = ""binomial"")

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.734  -0.847  -0.847   0.709   1.549  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -0.8408     0.2551  -3.296  0.00098 ***
as.factor(dataset$dich.age)1   2.0935     0.5285   3.961 7.46e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 136.66  on 99  degrees of freedom
Residual deviance: 117.96  on 98  degrees of freedom
AIC: 121.96

Number of Fisher Scoring iterations: 4
</code></pre>

<p>However, when I calculate the confidence intervals of the parameters, I get a different interval to the one given in the text:</p>

<pre><code>&gt; exp(confint(model))
Waiting for profiling to be done...
                                 2.5 %     97.5 %
(Intercept)                  0.2566283  0.7013384
as.factor(dataset$dich.age)1 3.0293727 24.7013080
</code></pre>

<p>Hosmer &amp; Lemeshow suggest the following formula:</p>

<p>$$
e^{[\hat\beta_1\pm z_{1-\alpha/2}\times\hat{\text{SE}}(\hat\beta_1)]}
$$
</p>

<p>and they calculate the confidence interval for <code>as.factor(dataset$dich.age)1</code> to be (2.9, 22.9).</p>

<p>This seems straightforward to do in R:</p>

<pre><code># upper CI for beta
exp(summary(model)$coefficients[2,1]+1.96*summary(model)$coefficients[2,2])
# lower CI for beta
exp(summary(model)$coefficients[2,1]-1.96*summary(model)$coefficients[2,2])
</code></pre>

<p>gives the same answer as the book.</p>

<p>However, any thoughts on why <code>confint()</code> seems to give different results?  I've seen lots of examples of people using <code>confint()</code>.</p>
"
"0.0974046509883157","0.0958706236059213","  5354","<p>I've got some data about airline flights (in a data frame called <code>flights</code>) and I would like to see if the flight time has any effect on the probability of a significantly delayed arrival (meaning 10 or more minutes). I figured I'd use logistic regression, with the flight time as the predictor and whether or not each flight was significantly delayed (a bunch of Bernoullis) as the response. I used the following code...</p>

<pre><code>flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
summary(delay.model)
</code></pre>

<p>...but got the following output.</p>

<pre><code>&gt; flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
&gt; delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  fitted probabilities numerically 0 or 1 occurred
&gt; summary(delay.model)

Call:
glm(formula = BigDelay ~ ArrDelay, family = binomial(link = ""logit""),
    data = flights)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-3.843e-04  -2.107e-08  -2.107e-08   2.107e-08   3.814e-04

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -312.14     170.26  -1.833   0.0668 .
ArrDelay       32.86      17.92   1.833   0.0668 .
---
Signif. codes:  0 Ã¢***Ã¢ 0.001 Ã¢**Ã¢ 0.01 Ã¢*Ã¢ 0.05 Ã¢.Ã¢ 0.1 Ã¢ Ã¢ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.8375e+06  on 2291292  degrees of freedom
Residual deviance: 9.1675e-03  on 2291291  degrees of freedom
AIC: 4.0092

Number of Fisher Scoring iterations: 25
</code></pre>

<p>What does it mean that the algorithm did not converge? I thought it be because the <code>BigDelay</code> values were <code>TRUE</code> and <code>FALSE</code> instead of <code>0</code> and <code>1</code>, but I got the same error after I converted everything. Any ideas?</p>
"
"NaN","NaN","  5647","<p>I am fitting a conditional logistic regression model with 1:4 controls using <code>R</code>. I wish to obtain <code>AIC</code> from the model. How can I extract the appropriate parameters based on the object <code>m</code>?</p>

<pre><code>library(survival)
m&lt;-clogit(cc~exp+ factor1+ factor2 + strata(stratum),data=data1)
</code></pre>
"
"0.13068205256071","0.128623938856882","  7720","<p>I am new to R, ordered logistic regression, and <code>polr</code>.</p>

<p>The ""Examples"" section at the bottom of the help page for <a href=""http://stat.ethz.ch/R-manual/R-patched/library/MASS/html/polr.html"">polr</a> (that fits a logistic or probit regression model to an ordered factor response) shows</p>

<pre><code>options(contrasts = c(""contr.treatment"", ""contr.poly""))
house.plr &lt;- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)
pr &lt;- profile(house.plr)
plot(pr)
pairs(pr)
</code></pre>

<ul>
<li><p>What information does <code>pr</code> contain?  The help page on <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/profile.html"">profile</a> is
generic, and gives no guidance for polr.</p></li>
<li><p>What is <code>plot(pr)</code> showing?  I see six graphs. Each has an X axis that is
numeric, although the label is an indicator variable (looks like an input variable that is an indicator for an ordinal value).  Then the Y axis
is ""tau"" which is completely unexplained.</p></li>
<li><p>What is <code>pairs(pr)</code> showing?  It looks like a plot for each pair of input
variables, but again I see no explanation of the X or Y axes.</p></li>
<li><p>How can one understand if the model gave a good fit?
<code>summary(house.plr)</code> shows Residual Deviance 3479.149 and AIC (Akaike
Information Criterion?) of 3495.149.  Is that good?  In the case those
are only useful as relative measures (i.e. to compare to another model
fit), what is a good absolute measure?  Is the residual deviance approximately chi-squared distributed?  Can one use ""% correctly predicted"" on the original data or some cross-validation?  What is the easiest way to do that?</p></li>
<li><p>How does one apply and interpret <code>anova</code> on this model?  The docs say ""There are methods for the standard model-fitting functions, including predict, summary, vcov, anova.""  However, running <code>anova(house.plr)</code> results in <code>anova is not implemented for a single ""polr"" object</code></p></li>
<li><p>How does one interpret the t values for each coefficient?  Unlike some
model fits, there are no P values here.</p></li>
</ul>

<p>I realize this is a lot of questions, but it makes sense to me to ask as one bundle (""how do I use this thing?"") rather than 7 different questions.  Any information appreciated.</p>
"
"0.107807193135897","0.121267812518166","  8511","<p>Christopher Manning's <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">writeup on logistic regression in R</a> shows a logistic regression in R as follows:</p>

<pre><code>ced.logr &lt;- glm(ced.del ~ cat + follows + factor(class), 
  family=binomial)
</code></pre>

<p>Some output:</p>

<pre><code>&gt; summary(ced.logr)
Call:
glm(formula = ced.del ~ cat + follows + factor(class),
    family = binomial(""logit""))
Deviance Residuals:
Min            1Q    Median       3Q      Max
-3.24384 -1.34325   0.04954  1.01488  6.40094

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -1.31827    0.12221 -10.787 &lt; 2e-16
catd          -0.16931    0.10032  -1.688 0.091459
catm           0.17858    0.08952   1.995 0.046053
catn           0.66672    0.09651   6.908 4.91e-12
catv          -0.76754    0.21844  -3.514 0.000442
followsP       0.95255    0.07400  12.872 &lt; 2e-16
followsV       0.53408    0.05660   9.436 &lt; 2e-16
factor(class)2 1.27045    0.10320  12.310 &lt; 2e-16
factor(class)3 1.04805    0.10355  10.122 &lt; 2e-16
factor(class)4 1.37425    0.10155  13.532 &lt; 2e-16
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 958.66 on 51 degrees of freedom
Residual deviance: 198.63 on 42 degrees of freedom
AIC: 446.10
Number of Fisher Scoring iterations: 4
</code></pre>

<p>He then goes into some detail about how to interpret coefficients, compare different models, and so on.  Quite useful.</p>

<p>However, how much variance does the model account for?  A <a href=""http://www.ats.ucla.edu/stat/stata/output/old/lognoframe.htm"" rel=""nofollow"">Stata page on logistic regression</a> says:</p>

<blockquote>
  <p>Technically, R2 cannot be computed the same way in logistic regression as it is in OLS regression. The pseudo-R2, in logistic regression, is defined as 1 - L1/L0, where L0 represents the log likelihood for the ""constant-only"" model and L1 is the log likelihood for the full model with constant and predictors. </p>
</blockquote>

<p>I understand this at the high level. The constant-only model would be without any of the parameters (only the intercept term).  Log likelihood is a measure of how closely the parameters fit the data.  In fact, Manning sort of hints that the deviance might be -2 log L. Perhaps null deviance is constant-only and residual deviance is -2 log L of the model?  However, I'm not crystal clear on it.</p>

<p>Can someone verify how one actually computes the pseudo-R^2 in R using this example?</p>
"
"0.0435606841869032","0.0428746462856272","  8750","<p>If I have an arima object like <code>a</code>:</p>

<pre><code>set.seed(100)
x1 &lt;- cumsum(runif(100))
x2 &lt;- c(rnorm(25, 20), rep(0, 75))
x3 &lt;- x1 + x2

dummy = c(rep(1, 25), rep(0, 75))

a &lt;- arima(x3, order=c(0, 1, 0), xreg=dummy)
print(a)
</code></pre>

<p>.</p>

<pre><code>Series: x3 
ARIMA(0,1,0)                    

Call: arima(x = x3, order = c(0, 1, 0), xreg = dummy) 

Coefficients:
        dummy
      17.7665
s.e.   1.1434

sigma^2 estimated as 1.307:  log likelihood = -153.74
AIC = 311.48   AICc = 311.6   BIC = 316.67
</code></pre>

<p>How do calculate the R squared of this regression?</p>
"
"0.107807193135897","0.121267812518166","  8864","<p>I'm new to predictive models and I have a problem at hand that I need some advice with. Basically for a clinical application we want to predict the outcome of a rating scale with a model built on top of outcomes of our new measurement device. My dependent variable, a clinical rating scale, is an integer between 0 and 10 (inclusive). Unfortunately I don't have a large sample ($n \approx 100$) and I have a lot features to select from ($p \approx 120$). Also many of these features are correlated. Nearly all of the features are continuous variables. I have a separate sample for validation ($ n \approx 40$). There are several issues I'd like have your advice about: </p>

<ol>
<li>Should I go for regression or tree based methods? </li>
<li>Should I try ensemble learning methods or I'd better stick with a single model? Which methods should I try and why?</li>
<li>If it's better to go for a single model, how should I handle the model selection problem? Should I e.g. limit the number of predictors and go for methods like LEAPS with AIC or should I go for methods like LASSO?</li>
<li>If ensemble methods are suggested, which methods can handle cases with small $n$ and large $p$ better?</li>
<li>Discussing selected/influential features is important for me. Depending on the answers to previous questions, how should I go about it? </li>
</ol>

<p>I have some understanding of regression modeling and model selection problems. I have used the bestglm package in the past. Currently I'm looking at the Caret package as it brings a large number of methods under the same interface. References get technical about the details of the models but so far I didn't find a good one to go over practical issues for problems with small n and big p. I appreciate your suggestions and help.</p>

<p>Thanks,
AlefSin</p>
"
"0.106701449104437","0.105021006302101","  9237","<p>I have several dependent variables that are measures of racial disproportionality; I've calculated them as:</p>

<p>% of events caused by racial minority group / % of events caused by racial majority group</p>

<p>I have a dependent variable for each racial minority group in my sample. I am running longitudinal Generalized Estimating Equations (GEE) on these models, however I am somewhat stumped as to which family is appropriate for these dependent variables. The probability range for my ratios are truncated at 0, as it's not possible to have negative values in my DVs. This makes me question the validity of using a Gaussian family for my models.</p>

<p>The idea behind these variables is that a ratio greater than 1 indicates some level of greater burden of events that a given racial minority is bearing compared to the racial majority, and a ratio less than 1 indicates the opposite.</p>

<ul>
<li>What would be the most appropriate family to use for my GEE regressions?</li>
</ul>

<p>EDIT:</p>

<p>I misspoke about the racial disproportionality measure I was using. The correct formula is:</p>

<p>% events by minority / % of total enrollment that is minority OVER
% events by non-minority / % of total enrollment that is non-minority</p>

<p>Because they are ratios, the number of observations with value less than 1 is comparable to the number of observations greater than 1, with the lower bound being 0 and the upper bound being non-bounded. Looking at the histograms of my response variables, they definitely seem to fit a negative binomial distribution better than the normal. The QIC (GEE adjustment to AIC) confirms this suspicion. My questions now are:</p>

<ul>
<li>Can I trust this evidence to move forward with the negative binomial family?</li>
<li>If so, how do I possibly interpret the exponentiated coefficients from the resulting models? They don't see to be Incidence Rate Ratios, as one would interpret them to be from count variables...</li>
</ul>
"
"0.0889178742536973","0.105021006302101"," 10385","<p>I am trying to fit an ordinal regression model using the <code>logit</code> link function in R using <code>ordinal</code> package; the response variables have five levels.</p>

<p>The number of explanatory variables is much larger than the number of samples ($p \gg n$) </p>

<p>Could any one help me with the following problem:</p>

<ol>
<li>Start with a model that contains only the intercept.</li>
<li>For the current model, explore the improvement in fit by adding additional variables.</li>
<li>Add the baseline for the variables that performed the best (using AIC, deviance, etc.)</li>
<li>Go back to step 2 until the maximal number of variables in the model is reached.</li>
</ol>

<p>Unfortunately, <code>glmnet</code>, cannot handle ordinal regression otherwise it would have been great. Is there a way of reducing the ordinal regression problem to multinomial regression using indicator variables? This would be of great benefit as I could use <code>glmnet</code> for variable selection.</p>

<p>This is sample data (in my case $n \sim 100$, and $p \sim 10000$):</p>

<pre><code>structure(list(resp = structure(c(1L, 1L, 2L, 2L, 2L), .Label = c(""a"", 
""b""), class = c(""ordered"", ""factor"")), x1 = 1:5, x2 = c(0.1, 
0.2, 0.3, 0.4, 0.5), x3 = c(0.01, 0.04, 0.09, 0.16, 0.25), x4 = c(1, 
4, 9, 16, 25), x5 = c(0.001, 0.002, 0.003, 0.004, 0.005), x6 = c(-5, 
-4, -3, -2, -1), x7 = c(-0.5, -0.4, -0.3, -0.2, -0.1), x8 = c(0.25, 
0.16, 0.09, 0.04, 0.01), x9 = c(25, 16, 9, 4, 1), x10 = c(0.0316227766016838, 
0.0447213595499958, 0.0547722557505166, 0.0632455532033676, 0.0707106781186548
)), .Names = c(""resp"", ""x1"", ""x2"", ""x3"", ""x4"", ""x5"", ""x6"", ""x7"", 
""x8"", ""x9"", ""x10""), row.names = c(NA, -5L), class = ""data.frame"")
</code></pre>

<p>Thanks a lot for any help or pointers!</p>
"
"0.119791881513984","0.128623938856882"," 11107","<p>I need to do a logistic regression using R on my data. My response variable (<code>y</code>) is survival at weaning (<code>surv=0</code>; did not <code>surv=1</code>) and I have several independent variables which are binary and categoricals in nature.</p>

<p>I am following some examples on this website <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a> and trying to run some models.</p>

<p>Running the model: </p>

<pre><code>&gt; mysurv2 &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                 as.factor(pmtone), family=binomial(link=""logit""), data=ap)
&gt; summary(mysurv2)

Call:
glm(formula = surv ~ as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
    as.factor(pmtone), family = binomial(link = ""logit""), data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2837  -0.5121  -0.5121  -0.5058   2.0590  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7892.6  on 8791  degrees of freedom
Residual deviance: 7252.8  on 8784  degrees of freedom
  (341 observations deleted due to missingness)
AIC: 7268.8

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Adding the <code>na.action=na.pass</code> at the end of the model gave me an error message. I thought that this would take care NA's in my independent variables.</p>

<pre><code>&gt; mysurv &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                as.factor(pmtone), family=binomial(link=""logit""), data=ap, 
                na.action=na.pass)
Error: NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<p>Since this is my first time to venture into logistic regression, I am wondering whether there is any package in R that would be more suitable?</p>

<p>I am also tryng to understand the regression coefficients. The independent variables used in the model are:</p>

<ol>
<li><p>rectal temperature: </p>

<ul>
<li><code>(PTEM)1</code> = newborns with rectal temp. below 35.4 0C</li>
<li><code>(PTEM)2</code> = newborns with rectal temp. between 35.4 to 36.9 0C</li>
<li><code>(PTEM)3</code> = newborns with rectal temp. above 37.0 0C</li>
</ul></li>
<li><p>shivering:</p>

<ul>
<li><code>(pshiv)1</code> = newborns that were not shivering</li>
<li><code>(pshiv)2</code> = newborns that were shivering</li>
</ul></li>
<li><p>respiration:</p>

<ul>
<li><code>(presp)1</code> = newborns with normal respiration</li>
<li><code>(presp)2</code> = newborns with slight respiration problem</li>
<li><code>(presp)3</code> = newborns with poor respiration</li>
</ul></li>
<li><p>muscle tone:</p>

<ul>
<li><code>(pmtone)1</code> = newborns with normal muscle tone</li>
<li><code>(pmtone)2</code> = newborns with moderate muscle tone</li>
<li><code>(pmtone)1</code> = newborns with poor muscle tone</li>
</ul></li>
</ol>

<p>Looking at the coefficients, I got the following:</p>

<pre><code>                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>In my other analysis, I found that newborns:  </p>

<p>a) with higher rectal temperature<br>
b) do not shiver<br>
c) good respiration and<br>
d) good muscle tone at birth were more likely to survive.  </p>

<p>I am a bit confused with the coefficients I am getting above. I am wondering whether whether I am not interpreting the results correctly or is it something else?</p>
"
"0.0389618603953263","0.0575223741635528"," 11178","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/8661/logistic-regression-in-r-odds-ratio"">Logistic Regression in R (Odds Ratio)</a>  </p>
</blockquote>



<p>I need to do a logistic regression in R. My response variable is <code>surv=0</code>; <code>surv=1</code> and I have about 18 predictor variables.</p>

<p>After reading my model, I got the table of Coefficients below and I need to go through some steps, which I am not familiar with, until I get to the odds ratios.</p>

<p>This is my first time to do a logistic regression in R and your help would be appreciated.</p>

<pre><code>Call:
glm(formula = surv ~ as.factor(tdate) + as.factor(line) + as.factor(wt) + 
    as.factor(crump) + as.factor(pind) + as.factor(pcscore) + 
    as.factor(ptem) + as.factor(pshiv) + as.factor(pincis) + 
    as.factor(presp) + as.factor(pmtone) + as.factor(pscolor) + 
    as.factor(ppscore) + as.factor(pmstain) + as.factor(pbse) + 
    as.factor(psex) + as.factor(pgf), family = binomial(link = ""logit""), 
    data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9772  -0.5896  -0.4419  -0.3154   2.8264  

Coefficients:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.59796    0.27024  -2.213 0.026918 *  
as.factor(tdate)2009-09-08  0.43918    0.19876   2.210 0.027130 *  
as.factor(tdate)2009-09-11  0.27613    0.20289   1.361 0.173514    
as.factor(tdate)2009-09-15  0.58733    0.19232   3.054 0.002259 ** 
as.factor(tdate)2009-09-18  0.52823    0.20605   2.564 0.010360 *  
as.factor(tdate)2009-09-22  0.45661    0.19929   2.291 0.021954 *  
as.factor(tdate)2009-09-25 -0.09189    0.21740  -0.423 0.672526    
as.factor(tdate)2009-09-29 -0.15696    0.28369  -0.553 0.580076    
as.factor(tdate)2010-01-26  1.39260    0.21049   6.616 3.69e-11 ***
as.factor(tdate)2010-01-29  1.67827    0.21099   7.954 1.80e-15 ***
as.factor(tdate)2010-02-02  1.35442    0.21292   6.361 2.00e-10 ***
as.factor(tdate)2010-02-05  1.36856    0.21439   6.383 1.73e-10 ***
as.factor(tdate)2010-02-09  1.18159    0.21951   5.383 7.33e-08 ***
as.factor(tdate)2010-02-12  1.40457    0.22001   6.384 1.73e-10 ***
as.factor(tdate)2010-02-16  1.01063    0.21783   4.639 3.49e-06 ***
as.factor(tdate)2010-02-19  1.54992    0.21535   7.197 6.14e-13 ***
as.factor(tdate)2010-02-23  0.85695    0.33968   2.523 0.011641 *  
as.factor(line)2           -0.26311    0.07257  -3.625 0.000288 ***
as.factor(line)5            0.06766    0.11162   0.606 0.544387    
as.factor(line)6           -0.30409    0.12130  -2.507 0.012176 *  
as.factor(wt)2             -0.33904    0.10708  -3.166 0.001544 ** 
as.factor(wt)3             -0.28976    0.13217  -2.192 0.028359 *  
as.factor(wt)4             -0.50470    0.16264  -3.103 0.001915 ** 
as.factor(wt)5             -0.74870    0.20067  -3.731 0.000191 ***
as.factor(crump)2           0.07537    0.10751   0.701 0.483280    
as.factor(crump)3          -0.14050    0.13217  -1.063 0.287768    
as.factor(crump)4          -0.20131    0.16689  -1.206 0.227724    
as.factor(crump)5          -0.23963    0.20778  -1.153 0.248803    
as.factor(pind)2           -0.29893    0.10752  -2.780 0.005434 ** 
as.factor(pind)3           -0.40828    0.12436  -3.283 0.001027 ** 
as.factor(pind)4           -0.73021    0.14947  -4.885 1.03e-06 ***
as.factor(pind)5           -0.68878    0.17650  -3.902 9.52e-05 ***
as.factor(pcscore)2        -0.52667    0.13606  -3.871 0.000108 ***
as.factor(ptem)2           -0.72600    0.08964  -8.099 5.52e-16 ***
as.factor(ptem)3           -0.79145    0.10503  -7.536 4.86e-14 ***
as.factor(ptem)4           -0.89956    0.10331  -8.707  &lt; 2e-16 ***
as.factor(ptem)5           -0.90181    0.10721  -8.412  &lt; 2e-16 ***
as.factor(pshiv)2           0.25236    0.07713   3.272 0.001068 ** 
as.factor(pincis)2          0.02327    0.07216   0.323 0.747041    
as.factor(presp)2           0.43746    0.11598   3.772 0.000162 ***
as.factor(pmtone)2          0.34515    0.11178   3.088 0.002016 ** 
as.factor(pscolor)2         0.53469    0.26851   1.991 0.046443 *  
as.factor(ppscore)2         0.25664    0.08751   2.933 0.003361 ** 
as.factor(pmstain)2        -0.48619    0.84408  -0.576 0.564611    
as.factor(pbse)2           -0.28248    0.07335  -3.851 0.000117 ***
as.factor(psex)2           -0.18240    0.06385  -2.857 0.004280 ** 
as.factor(pgf)12            0.10329    0.14314   0.722 0.470554    
as.factor(pgf)21           -0.06481    0.10772  -0.602 0.547388    
as.factor(pgf)22            0.39584    0.12740   3.107 0.001890 ** 
as.factor(pgf)31            0.18820    0.10082   1.867 0.061936 .  
as.factor(pgf)32            0.39662    0.13963   2.841 0.004504 ** 
as.factor(pgf)41            0.09178    0.10413   0.881 0.378106    
as.factor(pgf)42            0.21056    0.14906   1.413 0.157787    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7812.9  on 8714  degrees of freedom
Residual deviance: 6797.4  on 8662  degrees of freedom
  (418 observations deleted due to missingness)
AIC: 6903.4

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.162989155760853","0.160422236979937"," 18909","<p>I have an ordinal variable related to an outcome that is comprised of many levels and IÂ´d like to collapse the number of ordinal values as much as possible. </p>

<pre><code>&gt; require(ipred)
&gt; require(party)
&gt; data(GBSG2)
&gt; head(GBSG2)
  horTh age menostat tsize tgrade pnodes progrec estrec time cens
1    no  70     Post    21     II      3      48     66 1814    1
2   yes  56     Post    12     II      7      61     77 2018    1
3   yes  58     Post    35     II      9      52    271  712    1
4   yes  59     Post    17     II      4      60     29 1807    1
5    no  73     Post    35     II      1      26     65  772    1
6    no  32      Pre    57    III     24       0     13  448    1
&gt; table(GBSG2$tgrade)

  I  II III 
 81 444 161 
&gt; ctree(Surv(time,cens)~tgrade,data=GBSG2) -&gt; mn
&gt; plot(mn)
</code></pre>

<p><img src=""http://i.stack.imgur.com/WYIUd.png"" alt=""enter image description here""></p>

<p>Would it be correct to claim that <code>tgrade</code> here could be collapsed into two instead of three values?</p>

<p>edit:</p>

<p>Running the usual parametric analysis I get:</p>

<pre><code>&gt;     anova(i1,i2)
Analysis of Deviance Table
 Cox model: response is  Surv(time, cens)
 Model 1: ~ tgrade
 Model 2: ~ tgrade == ""I""
   loglik  Chisq Df P(&gt;|Chi|)  
1 -1776.0                      
2 -1778.1 4.3049  1     0.038 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt;     anova(i1,i3)
Analysis of Deviance Table
 Cox model: response is  Surv(time, cens)
 Model 1: ~ tgrade
 Model 2: ~ tgrade != ""III""
  loglik  Chisq Df P(&gt;|Chi|)    
1  -1776                        
2  -1784 16.033  1 6.225e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; anova(i2,i3)
Analysis of Deviance Table
 Cox model: response is  Surv(time, cens)
 Model 1: ~ tgrade == ""I""
 Model 2: ~ tgrade != ""III""
   loglik  Chisq Df P(&gt;|Chi|)    
1 -1778.1                        
2 -1784.0 11.728  0 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; extractAIC(i1)
[1]    2.000 3555.975
&gt; extractAIC(i2)
[1]    1.00 3558.28
&gt;   extractAIC(i3)
[1]    1.000 3570.008  
</code></pre>

<p>Hence the i1 model provides a better fit than i2 and i3, and i2 fits significantly better than i3. So now all three categories are warranted with respect to survival, which is at odds with the ctree approach. Can anyone explain this? Is this due to the conditional nature of ctree instead of the semiparametric nature of cox regression?</p>
"
"0.123975880619305","0.122023382522994"," 19772","<p>Can someone please tell me how to have R estimate the break point in a piecewise linear model (as a fixed or random parameter), when I also need to estimate other random effects? </p>

<p>I've included a toy example below that fits a hockey stick / broken stick regression with random slope variances and a random y-intercept variance for a break point of 4. I want to estimate the break point instead of specifying it. It could be a random effect (preferable) or a fixed effect.</p>

<pre><code>library(lme4)
str(sleepstudy)

#Basis functions
bp = 4
b1 &lt;- function(x, bp) ifelse(x &lt; bp, bp - x, 0)
b2 &lt;- function(x, bp) ifelse(x &lt; bp, 0, x - bp)

#Mixed effects model with break point = 4
(mod &lt;- lmer(Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject), data = sleepstudy))

#Plot with break point = 4
xyplot(
        Reaction ~ Days | Subject, sleepstudy, aspect = ""xy"",
        layout = c(6,3), type = c(""g"", ""p"", ""r""),
        xlab = ""Days of sleep deprivation"",
        ylab = ""Average reaction time (ms)"",
        panel = function(x,y) {
        panel.points(x,y)
        panel.lmline(x,y)
        pred &lt;- predict(lm(y ~ b1(x, bp) + b2(x, bp)), newdata = data.frame(x = 0:9))
            panel.lines(0:9, pred, lwd=1, lty=2, col=""red"")
        }
    )
</code></pre>

<p>Output:</p>

<pre><code>Linear mixed model fit by REML 
Formula: Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject) 
   Data: sleepstudy 
  AIC  BIC logLik deviance REMLdev
 1751 1783 -865.6     1744    1731
Random effects:
 Groups   Name         Variance Std.Dev. Corr          
 Subject  (Intercept)  1709.489 41.3460                
          b1(Days, bp)   90.238  9.4994  -0.797        
          b2(Days, bp)   59.348  7.7038   0.118 -0.008 
 Residual               563.030 23.7283                
Number of obs: 180, groups: Subject, 18

Fixed effects:
             Estimate Std. Error t value
(Intercept)   289.725     10.350  27.994
b1(Days, bp)   -8.781      2.721  -3.227
b2(Days, bp)   11.710      2.184   5.362

Correlation of Fixed Effects:
            (Intr) b1(D,b
b1(Days,bp) -0.761       
b2(Days,bp) -0.054  0.181
</code></pre>

<p><img src=""http://i.stack.imgur.com/HnAfg.jpg"" alt=""Broken stick regression fit to each individual""></p>
"
"0.0871213683738064","0.0857492925712544"," 21067","<p>It's been a while since I've thought about or used a robust logistic regression model. However, I ran a few logits yesterday and realized that my probability curve was being affected by some 'extreme' values, and particularly low ones. However, when I went to run a robust logit model, I got the same results as I did in my logit model.</p>

<p>Under what circumstances should a robust logit produce different results from a traditional logit model? (in terms of coefficients)</p>

<p>R Code:</p>

<pre><code>&gt; library(Design)
&gt; ddist&lt;- datadist(dlmydat)
&gt; options(datadist='ddist')
&gt; me = lrm(factor(dlstatus) ~ dlour_bid, data=dlmydat)
&gt; me

Logistic Regression Model

lrm(formula = factor(dlstatus) ~ dlour_bid, data = dlmydat)


Frequencies of Responses
  1   2 
906 154 

       Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy      Gamma      Tau-a         R2      Brier 
      1060      3e-05     170.11          1          0       0.81      0.619      0.621      0.154      0.263      0.105 

          Coef      S.E.      Wald Z P
Intercept -5.233549 0.3731235 -14.03 0
dlour_bid  0.005367 0.0004925  10.90 0

&gt; library(car)
&gt; dlmod = glm(factor(dlstatus) ~ dlour_bid, data=dlmydat, family=binomial(link=""logit""))
&gt; summary(dlmod)

Call:
glm(formula = factor(dlstatus) ~ dlour_bid, family = binomial(link = ""logit""), 
    data = dlmydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2345  -0.5687  -0.3059  -0.1739   2.6999  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.2335492  0.3731235  -14.03   &lt;2e-16 ***
dlour_bid    0.0053667  0.0004925   10.90   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 878.61  on 1059  degrees of freedom
Residual deviance: 708.50  on 1058  degrees of freedom
AIC: 712.5

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.0616041103633697","0.0606339062590832"," 22902","<p>When comparing results obtained with different models in R, what should I look for to select the best one?</p>

<p>If I use for example the following 4 models applied to the same presence/absence sample taken from a species dataset, with the same variables:</p>

<ul>
<li><p>Generalized linear model</p></li>
<li><p>Generalized additive models Classification</p></li>
<li><p>Regression Tree</p></li>
<li><p>Artificial Neural Networks</p></li>
</ul>

<p>Should I compare all methods by AIC, Kappa, or cross-validation?</p>

<p>Will I ever be certain of selecting the best model?</p>

<p>What happens if I compare those 4 models prediction with a Bayes factor? Can I compare them?</p>
"
"0.179605302026775","0.176776695296637"," 24452","<p>I hope you all don't mind this question, but I need help interpreting output for a linear mixed effects model output I've been trying to learn to do in R. I am new to longitudinal data analysis and linear mixed effects regression. I have a model I fitted with weeks as the time predictor, and score on an employment course as my outcome. I modeled score with weeks (time) and several fixed effects, sex and race. My model includes random effects. I need help understanding what the variance and correlation means. The output is the following:</p>

<pre><code>Random effects  
Group   Name    Variance  
EmpId intercept 680.236  
weeks           13.562  
Residual 774.256  
</code></pre>

<p>The correlaton is .231.</p>

<p>I can interpret the correlation as there is a a positive relationship between weeks and score but I want to be able to say it in terms of ""23% of ..."".</p>

<p>I really appreciate the help. </p>

<hr>

<p>Thanks ""guest"" and Macro for replying. Sorry, for not replying, I was out at a conference and Iâ€™m now catching up. 
Here is the output and the context. </p>

<p>Here is the summary for the LMER model I ran. </p>

<pre><code>&gt;summary(LMER.EduA)  
Linear mixed model fit by maximum likelihood  
Formula: Score ~ Weeks + (1 + Weeks | EmpID)   
   Data: emp.LMER4 

  AIC     BIC   logLik   deviance   REMLdev   
 1815     1834  -732.6     1693    1685

Random effects:    
 Groups   Name       Variance Std.Dev. Corr  
 EmpID   (Intercept)  680.236  26.08133        
          Weeks         13.562 3.682662  0.231   
 Residual             774.256  27.82546        
Number of obs: 174, groups: EmpID, 18


Fixed effects:    
            Estimate Std. Error  t value  
(Intercept)  261.171      6.23     37.25    
Weeks          11.151      1.780    6.93

Correlation of Fixed Effects:  
     (Intr)  
Days -0.101
</code></pre>

<p>I donâ€™t understand how to interpret the variance and residual for the random effects and explain it to someone else. I also donâ€™t know how to interpret the correlation, other than it is positive which indicates that those with higher intercepts have higher slopes and those with those with lower intercepts have lower slopes but I donâ€™t know how to explain the correlation in terms of 23% of . . . . (I donâ€™t know how to finish the sentence or even if it makes sense to do so). This is a different type analysis for us as we (me) are trying to move into longitudinal analyses. </p>

<p>I hope this helps.</p>

<p>Thanks for your help so far. </p>

<p>Zeda</p>
"
"0.0871213683738064","0.0857492925712544"," 25611","<p>I have a dataset with 9 continuous independent variables. I'm trying to select amongst these variables to fit a model to a single percentage (dependent) variable, <code>Score</code>. Unfortunately, I know there will be serious collinearity between several of the variables.</p>

<p>I've tried using the <code>stepAIC()</code> function in R for variable selection, but that method, oddly, seems sensitive to the order in which the variables are listed in the equation...</p>

<p>Here's my R code (because it's percentage data, I use a logit transformation for Score):</p>

<pre><code>library(MASS)
library(car)

data.tst = read.table(""data.txt"",header=T)
data.lm = lm(logit(Score) ~ Var1 + Var2 + Var3 + Var4 + Var5 + Var6 + Var7 +
             Var8 + Var9, data = data.tst)

step = stepAIC(data.lm, direction=""both"")
summary(step)
</code></pre>

<p>For some reason, I found that the variables listed at the beginning of the equation end up being selected by the <code>stepAIC()</code> function, and the outcome can be manipulated by listing, e.g., <code>Var9</code> first (following the tilde).</p>

<p>What is a more effective (and less controversial) way of fitting a model here? I'm not actually dead-set on using linear regression: the only thing I want is to be able to understand which of the 9 variables is truly driving the variation in the <code>Score</code> variable. Preferably, this would be some method that takes the strong potential for collinearity in these 9 variables into account.</p>
"
"0.0616041103633697","0.0606339062590832"," 25817","<p>Is it possible to calculate AIC or BIC values for lasso regression models and other regularized models where parameters are only partially entering the equation.  How does one determine the degrees of freedom?</p>

<p>I'm using R to fit lasso regression models with the <code>glmnet()</code> function from the <code>glmnet</code> package, and I'd like to know how to calculate AIC and BIC values for a model.  In this way I might compare the values with models fit without regularization.  Is this possible to do?</p>
"
"0.0779237207906526","0.076696498884737"," 25839","<p>First off, I'll say I am a biologist and new to the statistics side of things so excuse my ignorance</p>

<p>I have a data set that consists of a binary outcome and then a bunch of trinary explanatory variables that looks something like this:</p>

<pre><code>head()
 Category block21_hap1 block21_hap2 block21_hap3 block21_check
1        1            1            1            0             2
2        1            2            0            0             2
3        1            1            0            1             2
4        1            1            0            1             2
5        1            1            1            0             2
6        1            1            1            0             2
</code></pre>

<p>A quick summary of the data</p>

<pre><code>summary()
Category block21_hap1 block21_hap2 block21_hap3 block21_check
 1:718    0:293        0:777        0:1026       2:1467       
 0:749    1:709        1:577        1: 390                    
          2:465        2:113        2:  51  
</code></pre>

<p>and another summary grouped by outcome levels</p>

<pre><code>by(hap.ped.final, hap.ped.final$Category, summary)
hap.ped.final$Category: 1
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:146        0:374        0:518        2:718        
 1:336        1:286        1:174                     
 2:236        2: 58        2: 26                     
---------------------------------------------------------------------------- 
hap.ped.final$Category: 0
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:147        0:403        0:508        2:749        
 1:373        1:291        1:216                     
 2:229        2: 55        2: 25          
</code></pre>

<p>So I am trying to run logistic regression on this data. When I do this:</p>

<pre><code>fit = glm(Category~ block21_hap1 + block21_hap2 + block21_hap3, data = hap.ped.final ,family = ""binomial"")
summary(fit)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.301  -1.177   1.059   1.177   1.200  

Coefficients: (1 not defined because of singularities)
                             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)                 -0.039221   0.280110  -0.140    0.889
hap.ped.final$block21_hap11  0.123555   0.183087   0.675    0.500
hap.ped.final$block21_hap12  0.009111   0.295069   0.031    0.975
hap.ped.final$block21_hap21 -0.084334   0.183087  -0.461    0.645
hap.ped.final$block21_hap22 -0.013889   0.337468  -0.041    0.967
hap.ped.final$block21_hap31  0.201113   0.183087   1.098    0.272
hap.ped.final$block21_hap32        NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2033  on 1466  degrees of freedom
Residual deviance: 2028  on 1461  degrees of freedom
AIC: 2040

Number of Fisher Scoring iterations: 3
</code></pre>

<p>So I don't really know what a singularity is or what's going wrong here that is throwing up NA's as a result of my analysis. Is it my data, or what I'm doing to it.
I tried googling the warning (or whatever you might call it) and I got some pages talking about collinearity and multilinearity, which I do not understand at all. 
Again, sorry for lack of knowledge here. I wish I had done more maths in undergrad. </p>
"
"0.0871213683738064","0.0857492925712544"," 26288","<p>I'm trying to understand how to interpret log odds ratios in logistic regression. Let's say I have the following output:</p>

<pre><code>&gt; mod1 = glm(factor(won) ~ bid, data=mydat, family=binomial(link=""logit""))
&gt; summary(mod1)

Call:
glm(formula = factor(won) ~ bid, family = binomial(link = ""logit""), 
    data = mydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5464  -0.6990  -0.6392  -0.5321   2.0124  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -2.133e+00  1.947e-02 -109.53   &lt;2e-16 ***
bid          2.494e-03  5.058e-05   49.32   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 83081  on 80337  degrees of freedom
Residual deviance: 80645  on 80336  degrees of freedom
AIC: 80649

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So my equation would look like:
$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-2.13 + 0.002\times(\text{bid})]\right)}$$</p>

<p>From here I calculated probabilities from all bid levels. 
<img src=""http://i.stack.imgur.com/5mLa9.png"" alt=""enter image description here""></p>

<p>I have been using this graph to say that at a 1000 bid, the probability of winning is x. At any given bid level, the probability of winning is x.</p>

<p>I have a feeling that my interpretation is wrong because I'm not considering that these are log-odds. How should I really be interpreting this plot/these results?</p>
"
"0.116885581185979","0.115044748327106"," 26831","<p>Still on running logistic regression models and would like to ask a few questions around it.</p>

<p><strong>Question 1</strong>:
Is there a simple way of getting the p-values of each independent factor in a logistic regression model. For example, I am running this model:</p>

<pre><code>mymod3 &lt;- as.formula(surv~as.factor(tdate)+as.factor(sline)+as.factor(pgrp)
                                          +as.factor(weight5)+as.factor(backfat5)
                                          +as.factor(srect2)+as.factor(bcs)
                                          +as.factor(agit)+as.factor(uscore)
                                          +as.factor(loco)+as.factor(teat2)
                                          +as.factor(uscoref)+as.factor(colos)
                                          +as.factor(tb5)+as.factor(nerve)
                                          +as.factor(feed5)+as.factor(fos)
                                          +as.factor(gest3)+as.factor(int3)
                                          +as.factor(psex)+as.factor(bwt5)
                                          +as.factor(presp2)+as.factor(mtone2)
                                          +as.factor(pscolor)+as.factor(pmstain)
                                          +as.factor(pshiv)+as.factor(ppscore)
                                          +as.factor(pincis)+as.factor(prectem5)
                                          +as.factor(pcon12)+as.factor(crum5)
                                          +as.factor(pindx5))

sofNoMis3 &lt;- apf[which(complete.cases(apf[,all.vars(mymod3)])),]
FulMod3 &lt;- glm(mymod3,family=binomial(link=""logit""),data=sofNoMis3)
summary(FulMod3)
</code></pre>

<p>I am using this to look at the significant level of each factor:</p>

<pre><code>anova(FulMod3,test=""Chisq"")
</code></pre>

<p>and got this:</p>

<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: surv

Terms added sequentially (first to last)


                    Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                 7791     7096.2              
as.factor(tdate)    15    50.71      7776     7045.4 9.215e-06 ***
as.factor(sline)     1    13.90      7775     7031.5 0.0001924 ***
as.factor(pgrp)      3     8.83      7772     7022.7 0.0316335 *  
as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    
as.factor(bcs)       3     6.46      7760     7005.1 0.0910745 .  
as.factor(agit)      2    13.44      7758     6991.6 0.0012075 ** 
as.factor(uscore)    2     2.16      7756     6989.5 0.3401845    
as.factor(loco)      2     1.58      7754     6987.9 0.4530983    
as.factor(teat2)     2    25.45      7752     6962.4 2.980e-06 ***
as.factor(uscoref)   2     0.48      7750     6962.0 0.7861675    
as.factor(colos)     1     1.06      7749     6960.9 0.3034592    
as.factor(tb5)       4    49.22      7745     6911.7 5.265e-10 ***
as.factor(nerve)     2     0.99      7743     6910.7 0.6105452    
as.factor(feed5)     4    11.79      7739     6898.9 0.0190170 *  
as.factor(fos)       1    47.10      7738     6851.8 6.732e-12 ***
as.factor(gest3)     2    22.60      7736     6829.2 1.235e-05 ***
as.factor(int3)      2     6.61      7734     6822.6 0.0367298 *  
as.factor(psex)      1     9.50      7733     6813.1 0.0020493 ** 
as.factor(bwt5)      4   348.42      7729     6464.7 &lt; 2.2e-16 ***
as.factor(presp2)    1   106.23      7728     6358.4 &lt; 2.2e-16 ***
as.factor(mtone2)    1    34.13      7727     6324.3 5.146e-09 ***
as.factor(pscolor)   1    12.57      7726     6311.7 0.0003928 ***
as.factor(pmstain)   1     0.30      7725     6311.4 0.5845095    
as.factor(pshiv)     1    32.29      7724     6279.2 1.328e-08 ***
as.factor(ppscore)   1    16.71      7723     6262.4 4.351e-05 ***
as.factor(pincis)    1     0.02      7722     6262.4 0.8892848    
as.factor(prectem5)  4   126.06      7718     6136.4 &lt; 2.2e-16 ***
as.factor(pcon12)    1    17.88      7717     6118.5 2.350e-05 ***
as.factor(crum5)     4    15.25      7713     6103.2 0.0042137 ** 
as.factor(pindx5)    4    25.58      7709     6077.6 3.838e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>but it does not always agree with the final model after applying backward elimination:</p>

<p>Example: </p>

<p>these three factors were not significant above but they still appeared in the final model below</p>

<pre><code>as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    

step(FulMod3,direction=""backward"",trace=FALSE)
</code></pre>

<p>which gives:</p>

<pre><code>Call:  glm(formula = surv ~ as.factor(tdate) + as.factor(pgrp) + as.factor(weight5) + 
    as.factor(backfat5) + as.factor(srect2) + as.factor(agit) + 
    as.factor(uscore) + as.factor(teat2) + as.factor(uscoref) + 
    as.factor(fos) + as.factor(gest3) + as.factor(int3) + as.factor(psex) + 
    as.factor(bwt5) + as.factor(presp2) + as.factor(mtone2) + 
    as.factor(pscolor) + as.factor(pshiv) + as.factor(ppscore) + 
    as.factor(prectem5) + as.factor(pcon12) + as.factor(pindx5), 
    family = binomial(link = ""logit""), data = sofNoMis3)

Coefficients:
               (Intercept)  as.factor(tdate)2009-09-11  as.factor(tdate)2009-09-15  as.factor(tdate)2009-09-18  as.factor(tdate)2009-09-22  
                   1.34799                     0.18414                    -0.19490                    -0.15552                    -0.16822  
as.factor(tdate)2009-09-25  as.factor(tdate)2009-09-29  as.factor(tdate)2010-01-26  as.factor(tdate)2010-01-29  as.factor(tdate)2010-02-02  
                   0.60046                     0.80784                    -1.03442                    -1.30562                    -1.01486  
as.factor(tdate)2010-02-05  as.factor(tdate)2010-02-09  as.factor(tdate)2010-02-12  as.factor(tdate)2010-02-16  as.factor(tdate)2010-02-19  
                  -1.04438                    -0.89311                    -1.06260                    -0.79833                    -1.09651  
as.factor(tdate)2010-02-23            as.factor(pgrp)2            as.factor(pgrp)3            as.factor(pgrp)4         as.factor(weight5)2  
                  -0.55411                     0.12659                    -0.04727                     0.21817                    -0.22592  
       as.factor(weight5)3         as.factor(weight5)4         as.factor(weight5)5        as.factor(backfat5)2        as.factor(backfat5)3  
                  -0.10143                    -0.31562                    -0.37656                    -0.19883                    -0.01188  
      as.factor(backfat5)4        as.factor(backfat5)5          as.factor(srect2)2            as.factor(agit)2            as.factor(agit)3  
                   0.08293                    -0.17116                    -0.18201                    -0.49145                    -0.36659  
        as.factor(uscore)2          as.factor(uscore)3           as.factor(teat2)2           as.factor(teat2)3         as.factor(uscoref)2  
                  -0.12265                     0.15334                     0.16575                     0.21520                     0.24166  
       as.factor(uscoref)3             as.factor(fos)2           as.factor(gest3)2           as.factor(gest3)3            as.factor(int3)2  
                  -0.24363                    -0.29506                     0.09747                     0.81894                    -0.25595  
          as.factor(int3)3            as.factor(psex)2            as.factor(bwt5)2            as.factor(bwt5)3            as.factor(bwt5)4  
                  -1.21086                     0.20025                     0.30753                     0.29614                     0.56753  
          as.factor(bwt5)5          as.factor(presp2)2          as.factor(mtone2)2         as.factor(pscolor)2           as.factor(pshiv)2  
                   0.86479                    -0.29270                    -0.40912                    -0.72782                    -0.33848  
       as.factor(ppscore)2        as.factor(prectem5)2        as.factor(prectem5)3        as.factor(prectem5)4        as.factor(prectem5)5  
                  -0.25958                     0.73842                     0.77476                     0.92158                     0.96269  
        as.factor(pcon12)2          as.factor(pindx5)2          as.factor(pindx5)3          as.factor(pindx5)4          as.factor(pindx5)5  
                   0.38119                     0.43199                     0.44496                     0.73458                     0.59771  

Degrees of Freedom: 7791 Total (i.e. Null);  7732 Residual
Null Deviance:      7096 
Residual Deviance: 6102         AIC: 6222
</code></pre>

<p><strong>Question 2</strong>:</p>

<p>I would like to calculate the standard errors of the odds ratio of each factor level </p>

<pre><code>exp(NewMod3$coefficients)  #Odds ratios
</code></pre>

<p><strong>Question 3:</strong></p>

<p>Lastly, to tell whether the levels of each factor are significantly different or not </p>

<pre><code>               (Intercept) as.factor(tdate)2009-09-11 as.factor(tdate)2009-09-15 as.factor(tdate)2009-09-18 as.factor(tdate)2009-09-22 
                 3.8496863                  1.2021883                  0.8229141                  0.8559688                  0.8451676 
as.factor(tdate)2009-09-25 as.factor(tdate)2009-09-29 as.factor(tdate)2010-01-26 as.factor(tdate)2010-01-29 as.factor(tdate)2010-02-02 
                 1.8229563                  2.2430525                  0.3554327                  0.2710041                  0.3624544 
as.factor(tdate)2010-02-05 as.factor(tdate)2010-02-09 as.factor(tdate)2010-02-12 as.factor(tdate)2010-02-16 as.factor(tdate)2010-02-19 
                 0.3519109                  0.4093819                  0.3455567                  0.4500787                  0.3340336 
as.factor(tdate)2010-02-23           as.factor(pgrp)2           as.factor(pgrp)3           as.factor(pgrp)4        as.factor(weight5)2 
                 0.5745817                  1.1349500                  0.9538339                  1.2437928                  0.7977835 
       as.factor(weight5)3        as.factor(weight5)4        as.factor(weight5)5       as.factor(backfat5)2       as.factor(backfat5)3 
                 0.9035410                  0.7293337                  0.6862173                  0.8196866                  0.9881871 
      as.factor(backfat5)4       as.factor(backfat5)5         as.factor(srect2)2           as.factor(agit)2           as.factor(agit)3 
                 1.0864697                  0.8426844                  0.8335940                  0.6117399                  0.6930936 
        as.factor(uscore)2         as.factor(uscore)3          as.factor(teat2)2          as.factor(teat2)3        as.factor(uscoref)2 
                 0.8845715                  1.1657233                  1.1802836                  1.2401126                  1.2733576 
       as.factor(uscoref)3            as.factor(fos)2          as.factor(gest3)2          as.factor(gest3)3           as.factor(int3)2 
                 0.7837753                  0.7444886                  1.1023798                  2.2681046                  0.7741829 
          as.factor(int3)3           as.factor(psex)2           as.factor(bwt5)2           as.factor(bwt5)3           as.factor(bwt5)4 
                 0.2979401                  1.2217088                  1.3600609                  1.3446543                  1.7639063 
          as.factor(bwt5)5         as.factor(presp2)2         as.factor(mtone2)2        as.factor(pscolor)2          as.factor(pshiv)2 
                 2.3745019                  0.7462454                  0.6642372                  0.4829602                  0.7128545 
       as.factor(ppscore)2       as.factor(prectem5)2       as.factor(prectem5)3       as.factor(prectem5)4       as.factor(prectem5)5 
                 0.7713779                  2.0926314                  2.1700692                  2.5132469                  2.6187261 
        as.factor(pcon12)2         as.factor(pindx5)2         as.factor(pindx5)3         as.factor(pindx5)4         as.factor(pindx5)5 
                 1.4640265                  1.5403203                  1.5604231                  2.0845978                  1.8179532 
</code></pre>

<p>Example:</p>

<p>I would like to have a table like this:</p>

<pre><code>Factor           levels  Odds ratio

Parity group      (1)    1.00Â±standard error   a
                   2     1.50Â±standard errror  b
                  3-4    1.17Â±standard error   c
                   &gt;5    1.19Â±standard error   c
</code></pre>

<p>I would really appreciate your help on these 3 areas.</p>

<p>Baz</p>
"
"0.0616041103633697","0.0606339062590832"," 28472","<p>A Regression with ARIMA errors is given by the following formula (saw on Hyndman et al, 1998):</p>

<p>$Y_t = b_0 + b_1 X_{1,t} + \dots + b_k X_{k,t} + N_t$</p>

<p>where $N_t$ is modeled as an ARIMA process.</p>

<p>If we have that the model for $N_t$ is ARIMA$(0,0,0)$, then $N_t = e_t$, and $Y_t$ is modeled by an ordinary regression.</p>

<p>Suppose the following data:</p>

<pre><code>a &lt;- structure(c(29305, 9900, 9802, 17743, 49300, 17700, 24100, 11000, 
10625, 23644, 38011, 16404, 14900, 16300, 18700, 11814, 13934, 
12124, 18097, 30026, 3600, 15700, 12300, 14600), .Tsp = c(2010.25, 
2012.16666666667, 12), class = ""ts"")
b &lt;- structure(c(1.108528016, 1.136920872, 1.100239002, 1.057191265, 
1.044200511, 1.102063834, 1.083847756, 1.068585841, 1.084879628, 
1.232979511, 1.168894672, 1.257302058, 1.264967051, 1.234793782, 
1.306452369, 1.252644047, 1.178593218, 1.124432965, 1.132878661, 
1.189926986, 1.17249669, 1.176285957, 1.176552, 1.179178082), .Tsp = 
c(2010.25, 2012.16666666667, 12), class = ""ts"")
</code></pre>

<p>If I model it using <code>auto.arima</code> function, I have:</p>

<pre><code>auto.arima(a, xreg=b)
Series: a 
ARIMA(0,0,0) with zero mean     

Coefficients:
              b
      15639.266
s.e.   1773.186

sigma^2 estimated as 101878176:  log likelihood=-255.33
AIC=514.65   AICc=515.22   BIC=517.01

lm(a~b)

Call:
lm(formula = a ~ b)

Coefficients:
(Intercept)            b  
      48638       -26143  
</code></pre>

<p>Coefficients from the models differ. Shouldn't they be the same? What am I missing?</p>
"
"0.209091284097135","0.205798302171011"," 32040","<p>I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>

<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)
&gt; summary(fix1)

Call:
lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5835 -0.3543 -0.0024  0.3944  4.7294 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
Group1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***
Group2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***
Group3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***
Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** 
Group2:Year  0.0032369  0.0031098   1.041  0.29802    
Group3:Year  0.0006081  0.0032666   0.186  0.85235    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.7926 on 2981 degrees of freedom
Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 
F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>

<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>

<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)
&gt; summary(mix1a)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 4727 4775  -2356     4671    4711
Random effects:
 Groups     Name        Variance Std.Dev.
 Individual (Intercept) 0.39357  0.62735 
 Residual               0.24532  0.49530 
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.1010868   45.90
Group2       4.8094268  0.1158095   41.53
Group3       4.5607287  0.1216522   37.49
Group1:Year -0.0084165  0.0016963   -4.96
Group2:Year  0.0032369  0.0019433    1.67
Group3:Year  0.0006081  0.0020414    0.30

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.252  0.000  0.000              
Group2:Year  0.000 -0.252  0.000  0.000       
Group3:Year  0.000  0.000 -0.252  0.000  0.000
</code></pre>

<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>

<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 2941 3001  -1461     2885    2921
Random effects:
 Groups     Name        Variance  Std.Dev. Corr   
 Individual (Intercept) 0.1054790 0.324775        
            Year        0.0017447 0.041769 -0.246 
 Residual               0.1223920 0.349846        
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.0541746   85.64
Group2       4.8094268  0.0620648   77.49
Group3       4.5607287  0.0651960   69.95
Group1:Year -0.0084165  0.0065557   -1.28
Group2:Year  0.0032369  0.0075105    0.43
Group3:Year  0.0006081  0.0078894    0.08

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.285  0.000  0.000              
Group2:Year  0.000 -0.285  0.000  0.000       
Group3:Year  0.000  0.000 -0.285  0.000  0.000
</code></pre>

<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>

<p>How is this possible? I would expect that the random effect will ""eat"" the unexplained variability and increase ""sureness"" of the estimate!</p>

<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>

<h2>Edit</h2>

<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>

<pre><code>indivSlope = c()
for (indiv in 1:103) {
    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])
    indivSlope[indiv] = coef(mod1)['Year']
}

indivGroup = unique(mydata[,c(""Individual"", ""Group"")])[,""Group""]


anova1 = lm(indivSlope ~ 0 + indivGroup)
summary(anova1)

Call:
lm(formula = indivSlope ~ 0 + indivGroup)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.176288 -0.016502  0.004692  0.020316  0.153086 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
indivGroup1 -0.0084165  0.0065555  -1.284    0.202
indivGroup2  0.0032369  0.0075103   0.431    0.667
indivGroup3  0.0006081  0.0078892   0.077    0.939

Residual standard error: 0.04248 on 100 degrees of freedom
Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 
F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 
</code></pre>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>
"
"0.0616041103633697","0.0606339062590832"," 32657","<p>I was playing with the <a href=""http://cran.r-project.org/web/packages/TSA/index.html"" rel=""nofollow"">TSA</a> package in R and wanted to test the <code>arimax</code> function to the solution provided in Pankratz's <em>Forecasting with Dynamic Regression Models</em>, chapter 8. The savings rate and the function seems to provide similar results as the ones in the book except for the IO weights which are quite different. I bet there is a transformation that I might be missing.</p>

<p>Any help on understanding why IO coefficients are so different would be appreciated...</p>

<p>the solution states </p>

<pre><code>AO @ t=82,43,89
LS @ t=99
IO @ t=62,55
</code></pre>

<p>with Parameters estimates</p>

<pre><code>C = 6.1635
w82 = 2.3346
w99 = -1.5114
w43 = 1.1378
w62 = 1.4574
w55 = -1.4915
w89 = -1.0702
AR1 = 0.7976
MA2 = -0.3762
</code></pre>

<p>To fit the model in R, I used
(<code>saving</code> is the data)</p>

<pre><code>arimax(saving, order = c(1,0,2), fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA), io=c(55,62), 
       xreg=data.frame(AO82=1*(seq(saving)==82),
                       AO43=1*(seq(saving)==43),
                       AO89=1*(seq(saving)==89),
                       LS99=1*(seq(saving)&gt;=99)),
       method='ML')
</code></pre>

<p>The savings rate data is (100 points)</p>

<p>4.9
5.2
5.7
5.7
6.2
6.7
6.9
7.1
6.6
7
6.9
6.4
6.6
6.4
7
7.3
6
6.3
4.8
5.3
5.4
4.7
4.9
4.4
5.1
5.3
6
5.9
5.9
5.6
5.3
4.5
4.7
4.6
4.3
5
5.2
6.2
5.8
6.7
5.7
6.1
7.2
6.5
6.1
6.3
6.4
7
7.6
7.2
7.5
7.8
7.2
7.5
5.6
5.7
4.9
5.1
6.2
6
6.1
7.5
7.8
8
8
8.1
7.6
7.1
6.6
5.6
5.9
6.6
6.8
7.8
7.9
8.7
7.7
7.3
6.7
7.5
6.4
9.7
7.5
7.1
6.4
6
5.7
5
4.2
5.1
5.4
5.1
5.3
5
4.8
4.7
5
5.4
4.3
3.5</p>

<p>here it is my output</p>

<pre><code>&gt; arimax(saving, order = c(1,0,2),fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA),io=c(55,62),xreg=data.frame(AO82=1*(seq(saving)==82),
+ AO43=1*(seq(saving)==43),AO89=1*(seq(saving)==89),LS99=1*(seq(saving)&gt;=99)),method='ML')

Call:
arimax(x = saving, order = c(1, 0, 2), xreg = data.frame(AO82 = 1 * (seq(saving) == 
    82), AO43 = 1 * (seq(saving) == 43), AO89 = 1 * (seq(saving) == 
    89), LS99 = 1 * (seq(saving) &gt;= 99)), fixed = c(NA, 0, NA, NA, NA, NA, 
    NA, NA, NA, NA), method = ""ML"", io = c(55, 62))

Coefficients:
         ar1  ma1     ma2  intercept    AO82    AO43     AO89     LS99    IO-55   IO-62
      0.7918    0  0.3406     6.0628  2.3800  1.1297  -1.0466  -1.4885  -0.5958  0.5517
s.e.  0.0674    0  0.1060     0.3209  0.3969  0.3780   0.3835   0.5150   0.4044  0.3772

sigma^2 estimated as 0.2611:  log likelihood = -75.57,  aic = 169.14
</code></pre>
"
"0.107807193135897","0.121267812518166"," 32735","<p>As a financial institution, we often run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  Recently we are building another model in which I believe we have regression with autocorrelated errors.The residuals from linear model have <code>lm(object)</code> has clearly a AR(1) structure, as evident from ACF and PACF.  I took two different approaches, the first one was obviously to fit the model using Generalized least squares <code>gls()</code> in R. My expectation was that the residuals from gls(object) would be a white noise (independent errors).  But the residuals from <code>gls(object)</code> still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing that I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (the residuals are white noise). I really want to use <code>gls()</code> in <code>nlme</code> package so that coding will be lot simpler and easier. What would be the approach I should take here? Am I supposed to use REML? or is my expectation of non-correlated residuals (white noise) from gls() object wrong?</p>

<pre><code>gls.bk_ai &lt;- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, 
                 correlation=corARMA(p=1), method='ML',  data  = fit.cap01A)

gls2.bk_ai  &lt;- update(gls.bk_ai, correlation = corARMA(p=2))

gls3.bk_ai &lt;- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai &lt;- update(gls.bk_ai, correlation = NULL)

anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  
     ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise
</code></pre>

<p>Is there something wrong with what I am doing???????</p>
"
"0.123975880619305","0.13558153613666"," 34319","<p>Let's say I have the following logistic regression models:</p>

<pre><code> df=data.frame(income=c(5,5,3,3,6,5),
                  won=c(0,0,1,1,1,0),
                  age=c(18,18,23,50,19,39),
                  home=c(0,0,1,0,0,1))

&gt; md1 = glm(factor(won) ~ income + age + home, 
+           data=df, family=binomial(link=""logit""))
&gt; md2 = glm(factor(won) ~ factor(income) + factor(age) + factor(home), 
+           data=df, family=binomial(link=""logit""))
&gt; summary(md1)

Call:
glm(formula = factor(won) ~ income + age + home, family = binomial(link = ""logit""), 
    data = df)

Deviance Residuals: 
      1        2        3        4        5        6  
-1.0845  -1.0845   0.8017   0.4901   1.7298  -0.8017  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  4.784832   6.326264   0.756    0.449
income      -1.027049   1.056031  -0.973    0.331
age          0.007102   0.097759   0.073    0.942
home        -0.896802   2.252894  -0.398    0.691

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.3178  on 5  degrees of freedom
Residual deviance: 6.8700  on 2  degrees of freedom
AIC: 14.87

Number of Fisher Scoring iterations: 4

&gt; summary(md2)

Call:
glm(formula = factor(won) ~ factor(income) + factor(age) + factor(home), 
    family = binomial(link = ""logit""), data = df)

Deviance Residuals: 
         1           2           3           4           5           6  
-6.547e-06  -6.547e-06   6.547e-06   6.547e-06   6.547e-06  -6.547e-06  

Coefficients: (3 not defined because of singularities)
                  Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)      2.457e+01  1.310e+05       0        1
factor(income)5 -4.913e+01  1.605e+05       0        1
factor(income)6 -2.573e-30  1.853e+05       0        1
factor(age)19           NA         NA      NA       NA
factor(age)23   -1.383e-30  1.853e+05       0        1
factor(age)39   -3.479e-14  1.605e+05       0        1
factor(age)50           NA         NA      NA       NA
factor(home)1           NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.3178e+00  on 5  degrees of freedom
Residual deviance: 2.5720e-10  on 1  degrees of freedom
AIC: 10
</code></pre>

<p>So depending on the mode of the predictors, R produced different outputs. For factors, R splits out the coefficients into separate categories for the levels, but not for the model with numeric predictors. I'm wondering about a couple things.</p>

<ol>
<li><p>Is it ever useful to have the response categories expressed as individual rows?</p></li>
<li><p>To express the general regression equation, how does one go from a model with the categories expressed in an individual equation to an equation with a single B_i. So, for example, if gender has two coefficients, 3.5 for Male and 2.3 for Female, how does one use that in an equation such that (besides converting them into numeric values):</p></li>
</ol>

<p>Y = B0 + B1 (Gender)</p>
"
"0.13970499065216","0.137504774554232"," 35590","<p>I was unable to figure out how to perform linear regression in R in for a repeated measure design. In a <a href=""http://stackoverflow.com/questions/12182373/plot-of-a-linear-regression-with-interactions"">previous question</a> (still unanswered) it was suggested to me to not use <code>lm</code> but rather to use mixed models. I used <code>lm</code> in the following way:  </p>

<pre><code>lm.velocity_vs_Velocity_response &lt;- lm(Velocity_response~Velocity*Subject, data=mydata)
</code></pre>

<p>(more details on the dataset can be found at the link above)</p>

<p>However I was not able to find on the internet any example with R code showing how to perform a linear regression analysis.</p>

<p>What I want is on one hand a plot of the data with the line fitting the data, and on the other hand the $R^2$ value along with the p-value for the test of significance for the model.</p>

<p>Is there anyone who can provide some suggestions? Any R code example could be of great help.</p>

<hr>

<p><strong>Edit</strong><br>
According to the suggestion I received so far, the solution to my analyze my data in order to understand if there is a linear relation between the two variables Velocity_response (deriving from the questionnaire) and Velocity (deriving from the performance) should be this:</p>

<pre><code>library(nlme)
summary(lme(Velocity_response ~ Velocity*Subject, data=scrd, random= ~1|Subject))
</code></pre>

<p>The result of summary gives this:</p>

<pre><code>    &gt; summary(lme(Velocity_response ~ Velocity*Subject, data=scrd, random= ~1|Subject))
    Linear mixed-effects model fit by REML
     Data: scrd 
           AIC      BIC   logLik
      104.2542 126.1603 -30.1271

    Random effects:
     Formula: ~1 | Subject
            (Intercept) Residual
    StdDev:    2.833804 2.125353

Fixed effects: Velocity_response ~ Velocity * Subject 
                              Value Std.Error DF    t-value p-value
(Intercept)               -26.99558  25.82249 20 -1.0454288  0.3083
Velocity                   24.52675  19.28159 20  1.2720292  0.2180
SubjectSubject10           21.69377  27.18904  0  0.7978865     NaN
SubjectSubject11           11.31468  33.51749  0  0.3375754     NaN
SubjectSubject13           52.45966  53.96342  0  0.9721337     NaN
SubjectSubject2           -14.90571  34.16940  0 -0.4362299     NaN
SubjectSubject3            26.65853  29.41574  0  0.9062674     NaN
SubjectSubject6            37.28252  50.06033  0  0.7447517     NaN
SubjectSubject7            12.66581  26.58159  0  0.4764880     NaN
SubjectSubject8            14.28029  31.88142  0  0.4479188     NaN
SubjectSubject9             5.65504  34.54357  0  0.1637076     NaN
Velocity:SubjectSubject10 -11.89464  21.07070 20 -0.5645111  0.5787
Velocity:SubjectSubject11  -5.22544  27.68192 20 -0.1887672  0.8522
Velocity:SubjectSubject13 -41.06777  44.43318 20 -0.9242591  0.3664
Velocity:SubjectSubject2   11.53397  25.41780 20  0.4537754  0.6549
Velocity:SubjectSubject3  -19.47392  23.26966 20 -0.8368804  0.4125
Velocity:SubjectSubject6  -29.60138  41.47500 20 -0.7137162  0.4836
Velocity:SubjectSubject7   -6.85539  19.92271 20 -0.3440992  0.7344
Velocity:SubjectSubject8  -12.51390  22.54724 20 -0.5550080  0.5850
Velocity:SubjectSubject9   -2.22888  27.49938 20 -0.0810519  0.9362
 Correlation: 
                          (Intr) Velcty SbjS10 SbjS11 SbjS13 SbjcS2 SbjcS3 SbjcS6 SbjcS7 SbjcS8 SbjcS9 V:SS10 V:SS11 V:SS13 Vl:SS2 Vl:SS3
Velocity                  -0.993                                                                                                         
SubjectSubject10          -0.950  0.943                                                                                                  
SubjectSubject11          -0.770  0.765  0.732                                                                                           
SubjectSubject13          -0.479  0.475  0.454  0.369                                                                                    
SubjectSubject2           -0.756  0.751  0.718  0.582  0.362                                                                             
SubjectSubject3           -0.878  0.872  0.834  0.676  0.420  0.663                                                                      
SubjectSubject6           -0.516  0.512  0.490  0.397  0.247  0.390  0.453                                                               
SubjectSubject7           -0.971  0.965  0.923  0.748  0.465  0.734  0.853  0.501                                                        
SubjectSubject8           -0.810  0.804  0.769  0.624  0.388  0.612  0.711  0.418  0.787                                                 
SubjectSubject9           -0.748  0.742  0.710  0.576  0.358  0.565  0.656  0.386  0.726  0.605                                          
Velocity:SubjectSubject10  0.909 -0.915 -0.981 -0.700 -0.435 -0.687 -0.798 -0.469 -0.883 -0.736 -0.679                                   
Velocity:SubjectSubject11  0.692 -0.697 -0.657 -0.986 -0.331 -0.523 -0.607 -0.357 -0.672 -0.560 -0.517  0.637                            
Velocity:SubjectSubject13  0.431 -0.434 -0.409 -0.332 -0.996 -0.326 -0.378 -0.222 -0.419 -0.349 -0.322  0.397  0.302                     
Velocity:SubjectSubject2   0.753 -0.759 -0.715 -0.580 -0.360 -0.992 -0.661 -0.389 -0.732 -0.610 -0.563  0.694  0.528  0.329              
Velocity:SubjectSubject3   0.823 -0.829 -0.782 -0.634 -0.394 -0.622 -0.984 -0.424 -0.799 -0.667 -0.615  0.758  0.577  0.360  0.629       
Velocity:SubjectSubject6   0.462 -0.465 -0.438 -0.356 -0.221 -0.349 -0.405 -0.995 -0.449 -0.374 -0.345  0.425  0.324  0.202  0.353  0.385
Velocity:SubjectSubject7   0.961 -0.968 -0.913 -0.740 -0.460 -0.726 -0.844 -0.496 -0.986 -0.778 -0.718  0.886  0.674  0.420  0.734  0.802
Velocity:SubjectSubject8   0.849 -0.855 -0.807 -0.654 -0.406 -0.642 -0.746 -0.438 -0.825 -0.988 -0.635  0.783  0.596  0.371  0.649  0.709
Velocity:SubjectSubject9   0.696 -0.701 -0.661 -0.536 -0.333 -0.526 -0.611 -0.359 -0.676 -0.564 -0.990  0.642  0.488  0.304  0.532  0.581
                          Vl:SS6 Vl:SS7 Vl:SS8
Velocity                                      
SubjectSubject10                              
SubjectSubject11                              
SubjectSubject13                              
SubjectSubject2                               
SubjectSubject3                               
SubjectSubject6                               
SubjectSubject7                               
SubjectSubject8                               
SubjectSubject9                               
Velocity:SubjectSubject10                     
Velocity:SubjectSubject11                     
Velocity:SubjectSubject13                     
Velocity:SubjectSubject2                      
Velocity:SubjectSubject3                      
Velocity:SubjectSubject6                      
Velocity:SubjectSubject7   0.450              
Velocity:SubjectSubject8   0.398  0.828       
Velocity:SubjectSubject9   0.326  0.679  0.600

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-1.47194581 -0.46509026 -0.05537193  0.39069634  1.89436646 

Number of Observations: 40
Number of Groups: 10 
Warning message:
In pt(q, df, lower.tail, log.p) : NaNs produced
&gt; 
</code></pre>

<p>Now, I do not understand where I can get the R^2 and the corresponding p-values indicating me wether there is a linear relationship between the two variables or not,
nor I have understood how my data can be plotted with the line fitting the regression.</p>

<p>Can anyone be so kind to enlighten me? I really need your help guys...</p>
"
"0.137750978465894","0.13558153613666"," 38491","<p>If we have a spatial autoregressive process, we can estimate a model to control for the autoregression with a spatial lag,
$$y=\rho W y + X\beta + \epsilon$$
Where $\rho$ is the strength of the spatial correlation, and $W$ is a matrix of spatial weights. The <code>spdep</code> package for R contains the <code>lagsarlm</code> command which is designed to estimate precisely this model. The package contains methods for creating the weights. But there seems to be some discrepancy between the model fit between <code>lagsarlm()</code> and <code>lm()</code> fitted to what should be a similar model.</p>

<p>As an example, consider the example given with <code>?lagsarlm</code> in R. </p>

<pre><code>library(spdep)
data(oldcol)
COL.lag &lt;- lagsarlm(CRIME ~ INC + HOVAL, data=COL.OLD,
                nb2listw(COL.nb, style=""W""), method=""eigen"", quiet=TRUE)
summary(COL.lag)
Residuals:
      Min        1Q    Median        3Q       Max 
-37.68585  -5.35636   0.05421   6.02013  23.20555 

Type: lag 
Coefficients: (asymptotic standard errors) 
             Estimate Std. Error z value  Pr(&gt;|z|)
(Intercept) 45.079251   7.177347  6.2808 3.369e-10
INC         -1.031616   0.305143 -3.3808 0.0007229
HOVAL       -0.265926   0.088499 -3.0049 0.0026570

Rho: 0.43102, LR test value: 9.9736, p-value: 0.001588
Asymptotic standard error: 0.11768
    z-value: 3.6626, p-value: 0.00024962
Wald statistic: 13.415, p-value: 0.00024962
</code></pre>

<p>We can estimate what (I think) should be the same model by computing the actual spatial lag variable,</p>

<pre><code>crime.lag &lt;- lag.listw(nb2listw(COL.nb, style=""W""), COL.OLD$CRIME)
linearlag &lt;- lm(CRIME ~ crime.lag + INC + HOVAL, data=COL.OLD)
Residuals:
    Min      1Q  Median      3Q     Max 
-38.644  -6.103   0.266   6.563  21.610 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 38.18099    9.21531   4.143 0.000149 ***
crime.lag    0.55733    0.15029   3.709 0.000570 ***
INC         -0.86584    0.35541  -2.436 0.018864 *  
HOVAL       -0.26358    0.09136  -2.885 0.005986 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 10.12 on 45 degrees of freedom
Multiple R-squared: 0.6572, Adjusted R-squared: 0.6343 
F-statistic: 28.75 on 3 and 45 DF,  p-value: 1.543e-10 
</code></pre>

<p>The two models, which I think should be identical, are in fact significantly different from each other in every parameter and in model fit (with the <code>linearlag</code> model providing significantly lower AIC). Are there reasons why this should be? Why should I just not use the second model and abandon the special methods?</p>
"
"0.0308020551816849","0.0606339062590832"," 38940","<p>I'm running Cox proportional hazards regression in R, and would like to test the option of categorizing one of my continuous variables to factor (I'm aware of the loss of data issue, just checking). </p>

<p>Another thing that I'd like to check is putting the difference between 2 cont. variables inside the regression instead of putting them both.
[It's actually testing if the pulse pressure, e.g, the difference between the systolic and diastolic blood pressure is more significant than each of them separately]</p>

<p>My question is: What is the best way to compare between the different variations of the regressions (lets assume that I'll use step() in each of the attempts). No missing values in the dataframe whatsoever.</p>

<p>I'm pretty confused between AIC, R2 (of coxph) and concordance of coxph. 
Can anyone clear things up for me? Is there any other option of comparing between different models on the same data?</p>

<p>Thanks!</p>
"
"0.157060280430173","0.154586735600211"," 40499","<p>When using the <code>step.plr()</code> function in the <a href=""http://cran.r-project.org/web/packages/stepPlr/index.html"" rel=""nofollow"">stepPlr</a> package, if my predictors are factors, do I need to encode my predictors as dummy variables manually before passing it to the function? I do know that I can specify ""level"", but how  the ""level"" parameter works is confusing to me. 
My understanding is that I need to tell <code>step.plr()</code> explicitly which factors should be encoded as dummy variables and thus leaving one factor out intentionally. </p>

<p>Let's consider a simple example. Suppose I have 1 categorical predicator with 4 levels and binary response. Normally, if I use <code>glm()</code> to fit a logistic regression model, <code>glm()</code> would automatically convert the categorical predicator into 3 dummy variables. Now in <code>stepPlr()</code>, do I specify the ""level"" parameter for that predictor with 4 levels or 3 levels? The ""Help"" section is vague, and says: </p>

<blockquote>
  <p>If the j-th column of x is discrete, level[[ j ]] is the set of levels for the categorical factor.</p>
</blockquote>

<p>Does it mean I should tell <code>step.plr()</code> about all 4 levels, or I should make an intelligent decision myself and tell <code>step.plr()</code> to use only 3 levels? </p>

<p>==============UPDATE (16 Oct 2012)=============</p>

<p>The following example will demonstrate what is the problem with <code>step.plr()</code>'s automatic dummy variable encoding. It is a slight modification of the code in the function's help section. 
     set.seed(100)</p>

<pre><code>n &lt;- 100
p &lt;- 3
z &lt;- matrix(sample(seq(3),n*p,replace=TRUE),nrow=n)
x &lt;- data.frame(x1=factor(z[ ,1]),x2=factor(z[ ,2]),
                x3=factor(sample(seq(3), n, replace=TRUE, prob=c(0.2, 0.5, 0.3))),
                x4=factor(sample(seq(3), n, replace=TRUE, prob=c(0.1, 0.3, 0.6))))
y &lt;- sample(c(0,1),n,replace=TRUE)
fit &lt;- step.plr(x,y, cp=""aic"")
summary(fit)
</code></pre>

<p>And here's an excerpt of the result:</p>

<pre><code>Call:
plr(x = ix0, y = y, weights = weights, offset.subset = offset.subset, 
    offset.coefficients = offset.coefficients, lambda = lambda, 
    cp = cp)

Coefficients:
      Estimate Std.Error z value Pr(&gt;|z|)
Intercept  0.91386   5.04780   0.181    0.856
x4.1       1.33787   4.61089   0.290    0.772
x4.2      -1.70462   4.91240  -0.347    0.729
x4.3       0.36675   3.18857   0.115    0.908
x3.1:x4.1  7.04901  14.35112   0.491    0.623
x3.1:x4.2 -5.50973  15.53674  -0.355    0.723
x3.1:x4.3 -0.50012   7.95651  -0.063    0.950
</code></pre>

<p>You can see that all levels, that is, (1,2,3), are used to fit the model. But normally you only need two dummy variables to encode a predictor with 3 levels.
On the other hand, if you use <code>glm()</code>: </p>

<pre><code>glm(y~.^2, data=x, family=binomial)
</code></pre>

<p>you will get the correct dummy variable encoding.</p>
"
"0.200379147259755","0.197223372913885"," 40670","<p>I am familiar with linear regression models but the random section of linear mixed models just melts my mind. I did find an excellent guide that could have helped me but the languageR package is not compatible with newer versions of lme4 so I've been unable to implement it in my work.</p>

<p>For me the fixed effects are very understandable (below lactation and a higher yr2 value both contribute to a higher weight but the lactation effect is more consistent which results in a higher t-value).</p>

<p>The first problem is to understand what I am actually putting in. To a certain extent I understand that <code>(1|P$grupp)</code> means that the mixed model add to the base line (intercept) while <code>(P$grupp|P$lweek)</code> mean that belong to a group is expected to affect the average weight increase (or decrease) while <code>P$lweek</code> adds to the baseline value. But why does all tutorials seem to favor write ups like <code>(1+P$fgrupp|P$lweek)</code> rather than <code>(P$grupp|P$lweek)</code>?</p>

<p>Now on to the actual output (see below for full output). I've used the following models (sorry for the Swenglish but the sample is the weight of cows <code>P$vikt</code> is the weight at certain time points and <code>P$lweek</code> is the time since a calf was born, <code>P$fgrupp</code> is a factor telling if the cow belongs to feed group 1,2 or 3):</p>

<pre><code>Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 | P$fgrupp)            #$
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 + P$fgrupp | P$lweek)
</code></pre>

<p>Where I understand it as the first one being rather useless (essentially it tells us that the average weight of the cows isn't affected of which feed group it belongs too). This is reflected by fgrupp having variance 0 in the first formula below. The second is more interesting as the <code>P$fgrupp|P$lweek</code> as I understand it should show if different feed groups affect the weight increase of cows as function of the time. But I am really not competent enough to understand the input. I understand that variance somehow mean that belonging to group2 or 3 explain some of the variation in the growth curves but I really don't understand how to interpret this.</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr        
 P$lweek  (Intercept)   13.068  3.6149                                     #$
          P$fgrupp2     77.230  8.7881  1.000                              #$
          P$fgrupp3     81.188  9.0104  1.000 1.000                        #$
 Residual             4031.831 63.4967              
Number of obs: 1048, groups: P$lweek, 84
</code></pre>

<p><strong>Full output</strong></p>

<pre><code>#First model#
Linear mixed model fit by REML 
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 | P$fgrupp)            #$
   AIC   BIC logLik deviance REMLdev
 11703 11732  -5845    11698   11691
Random effects:
 Groups   Name        Variance Std.Dev.
 P$fgrupp (Intercept)    0.0    0.000                                      #$
 Residual             4139.9   64.342  
Number of obs: 1048, groups: P$fgrupp, 3                                   #$

Fixed effects:
            Estimate Std. Error t value
(Intercept)  509.593      4.683  108.82
P$lweek        1.028      0.105    9.79                                    #$
P$laktation   22.789      1.454   15.67                                    #$
P$yr2         35.294      4.093    8.62                                    #$

Correlation of Fixed Effects:
            (Intr) P$lwek P$lktt                                           #$
P$lweek     -0.560                                                         #$
P$laktation -0.636  0.030                                                  #$
P$yr2       -0.240 -0.034 -0.141                                           #$


#Second model#

Linear mixed model fit by REML 
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 + P$fgrupp | P$lweek)  #$
       AIC   BIC logLik deviance REMLdev
     11707 11761  -5842    11693   11685
    Random effects:
     Groups   Name        Variance Std.Dev. Corr        
     P$lweek  (Intercept)   13.068  3.6149                                     #$
              P$fgrupp2     77.230  8.7881  1.000                              #$
              P$fgrupp3     81.188  9.0104  1.000 1.000                        #$
     Residual             4031.831 63.4967              
    Number of obs: 1048, groups: P$lweek, 84                                   #$

Fixed effects:
            Estimate Std. Error t value
(Intercept) 508.2291     5.1770   98.17
P$lweek       1.0662     0.1192    8.94                                    #$
P$laktation  22.6525     1.4459   15.67                                    #$
P$yr2        35.6343     4.0848    8.72                                    #$

Correlation of Fixed Effects:
            (Intr) P$lwek P$lktt                                           #$$
P$lweek     -0.627                                                         #$
P$laktation -0.570  0.025                                                  #$
P$yr2       -0.224 -0.018 -0.136                                           #$
</code></pre>
"
"0.0974046509883157","0.0958706236059213"," 43315","<p>The model that I created in R is:</p>

<blockquote>
  <p>fit &lt;- lm(hired ~ educ + exper + sex, data=data)</p>
</blockquote>

<p>what I am unsure of is how to fit to model to predict probability of interest where p = pr(hiring = 1).</p>

<p>Any help would be appreciated thanks,
Clay </p>

<p><strong>Edit:</strong>
This is the computer output for what I have computed so far. I am unsure if this is even a step in the right direction to find the answer to this question.</p>

<p>What I am trying to do is, Fit a logistic regression model to predict the probability of being hired using years of education, years of experience and sex of job applicants.</p>

<pre><code> &gt; test&lt;-glm(hired ~ educ + exper + sex, data=data, family=binomial(link=""logit""))
 &gt; summary(test)

 Call:
 glm(formula = hired ~ educ + exper + sex, family = binomial(link = ""logit""), 
     data = data)

 Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -1.4380  -0.4573  -0.1009   0.1294   2.1804  

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
 (Intercept) -14.2483     6.0805  -2.343   0.0191 *
 educ          1.1549     0.6023   1.917   0.0552 .
 exper         0.9098     0.4293   2.119   0.0341 *
 sex           5.6037     2.6028   2.153   0.0313 *
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

 (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 35.165  on 27  degrees of freedom
 Residual deviance: 14.735  on 24  degrees of freedom
 AIC: 22.735

 Number of Fisher Scoring iterations: 7
</code></pre>
"
"0.0435606841869032","0.0428746462856272"," 43733","<p>The R documentation for either does not shed much light. All that I can get from <a href=""https://stat.ethz.ch/pipermail/r-help/2007-December/147855.html"">this link</a> is that using either one should be fine. What I do not get is why they are not equal.</p>

<p>Fact: The stepwise regression function in R, <code>step()</code> uses <code>extractAIC()</code>.</p>

<p>Interestingly, running a <code>lm()</code> model and a <code>glm()</code> 'null' model (only the intercept) on the 'mtcars' data set of R gives different results for <code>AIC</code> and <code>extractAIC()</code>.</p>

<pre><code>&gt; null.glm = glm(mtcars$mpg~1)
&gt; null.lm = lm(mtcars$mpg~1)

&gt; AIC(null.glm)
[1] 208.7555
&gt; AIC(null.lm)
[1] 208.7555
&gt; extractAIC(null.glm)
[1]   1.0000 208.7555
&gt; extractAIC(null.lm)
[1]   1.0000 115.9434
</code></pre>

<p>It is weird, given that both the models above are the same, and <code>AIC()</code> gives the same results for both.</p>

<p>Can anyone throw some light on the issue?</p>
"
"0.0974046509883157","0.0958706236059213"," 44502","<p>I have obtained some strange results for the following data when conducting Poisson regression in R.</p>

<pre><code>&gt; RHT1b
   f x1 y1 y2
1 f1  0 35  1
2 f2  2 70  4
3 f3  0  5  1
4 f4  9 37  4
5 f5  0  3  0

&gt; summary(amod2b)

Call:
glm(formula = x1 ~ y2, family = poisson, data = RHT1b)

Deviance Residuals: 
       1         2         3         4         5  
-0.00008  -1.71860  -0.00008   1.36550   0.00000  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -26.61    9977.85  -0.003    0.998
y2              7.08    2494.46   0.003    0.998

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 24.9766  on 4  degrees of freedom
Residual deviance:  4.8182  on 3  degrees of freedom
AIC: 15.485

Number of Fisher Scoring iterations: 18
</code></pre>

<p>I had an issue with the data earlier where x1 row 4 read ""8"", and row 5 read ""1"". When I ran the regression the results seemed reliable and were nearly significant. After correcting the data to how it is shown above and updating the model, I have results which literally do not make sense to me. </p>

<p>My question is, why has this happened? Is is possible the value of 1 changing to 0 exceeded some sort of threshold of zero counts that the Poisson model can't handle? Would a negative binomial model be more appropriate?</p>
"
"0.0754493182241785","0.0742610657232506"," 46117","<p>I used a stepwise multiple regression to generate a model and I am trying to appropriately report the results by indicating the (additional) variance explained by each included factor. I'm not sure how to extract that information, however.</p>

<p>Basically my code thus far is simply:</p>

<pre><code>init &lt;- lm(dep ~ fac1 + fac2 + fac3 + fac4,data=data)
final &lt;- stepAIC(init, direction=""both"")
</code></pre>

<p>or </p>

<pre><code>final &lt;- step(init, direction=""both"")
</code></pre>

<p>In the case of my data, the final model is <code>final ~ fac1 + fac2 + fac3</code></p>

<p>At this point, I want to know how much variance the most important factor explains, how much additional variance is explained by adding a second factor and so on. I can get the coefficients, but I'm not sure how to extract the R/R^2 for each factor.</p>

<p>Where is that reported?</p>

<p>Thanks in advance.</p>
"
"0.11525073729837","0.113435651621629"," 46312","<p>I'm working in R, using glm.nb (of the MASS package) to model count data with a negative binomial regression model.  I'd like to get the standardized (<em>beta</em>) coefficients from the model, but am given the unstandardized (<em>b</em> ""Estimate"") coefficients.</p>

<p>The R documentation does not seem to show of a way to retrieve the standardized beta weights easily for a negative bionomial regression model.</p>

<p>The R script is something like:</p>

<pre><code>library(""MASS"")
nb = glm.nb(responseCountVar ~ predictor1 + predictor2 + 
    predictor3 + predictor4 + predictor5 + predictor6 + 
    predictor7 + predictor8 + predictor9 + predictor10 + 
    predictor11 + predictor12 + predictor13 + predictor14 + 
    predictor15 + predictor16 + predictor17 + predictor18 + 
    predictor19 + predictor20 + predictor21,
    data=myData, control=glm.control(maxit=125))
summary(nb)
</code></pre>

<p>and the output of the above is:</p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-5.1462  -1.0080  -0.4247   0.2277   3.4336  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -3.059e+00  3.782e-01  -8.088 6.05e-16 ***
predictor1    -2.447e+00  4.930e-01  -4.965 6.88e-07 ***
predictor2    -1.004e+00  1.313e-01  -7.650 2.00e-14 ***
predictor3     1.158e+00  1.440e-01   8.047 8.46e-16 ***
predictor4     1.334e+00  7.034e-02  18.970  &lt; 2e-16 ***
predictor5     9.862e-01  2.006e-01   4.915 8.87e-07 ***
predictor6     1.166e+00  2.378e+00   0.490  0.62392    
predictor7    -1.057e-01  1.494e-01  -0.707  0.47936    
predictor8     4.051e-01  7.318e-02   5.536 3.10e-08 ***
predictor9    -3.320e-01  1.132e-01  -2.933  0.00336 ** 
predictor10    3.761e-01  1.561e-01   2.409  0.01600 *  
predictor11    8.660e-02  4.332e-02   1.999  0.04557 *  
predictor12   -1.583e-01  2.044e-01  -0.774  0.43872    
predictor13    6.404e-02  3.972e-03  16.122  &lt; 2e-16 ***
predictor14    4.264e-03  2.297e-04  18.563  &lt; 2e-16 ***
predictor15    3.279e-03  5.697e-04   5.755 8.68e-09 ***
predictor16    3.487e-03  3.447e-03   1.012  0.31177    
predictor17    1.534e-04  1.647e-04   0.931  0.35182    
predictor18   -7.606e-05  9.021e-05  -0.843  0.39917    
predictor19    2.536e-04  1.733e-05  14.633  &lt; 2e-16 ***
predictor20    2.997e-02  4.977e-03   6.021 1.73e-09 ***
predictor21    2.756e+01  3.508e+00   7.856 3.98e-15 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for Negative Binomial(0.9232) family taken to be 1)

    Null deviance: 5631.1  on 1835  degrees of freedom
Residual deviance: 2120.7  on 1814  degrees of freedom

                                AIC: 19268    
Number of Fisher Scoring iterations: 1    
                              Theta: 0.9232 
                          Std. Err.: 0.0282 
                 2 x log-likelihood: -19221.9910
</code></pre>

<p><strong>My question is</strong>:  Is there a way to get the beta weights, or do I need to try to convert my unstandardized b coefficients to standardized beta coefficients (if so, how would I do that)?</p>
"
"0.189876620282201","0.186886250399101"," 47258","<p>I have following data stored in a file. I am applying 'glm' in R to find linear regression equation to best predict the 'output'. </p>

<pre><code>&gt; tmpData
   logOfOutput randomSample multiplied part1 part2 randNormalMean100Std20 output
1    0.0000000           33         11     1    19               89.65387      1
2    0.6931472           76         24     2    18              128.23471      2
3    1.0986123           12         39     3    17              103.70930      3
4    1.3862944           68         56     4    16               99.12617      4
5    1.6094379           50         75     5    15               95.68173      5
6    1.7917595            7         96     6    14              129.27551      6
7    1.9459101           70        119     7    13              104.59333      7
8    2.0794415           55        144     8    12              102.15247      8
9    2.1972246           20        171     9    11               72.43795      9
10   2.3025851           24        200    10    10               80.63634     10
11   2.3978953           32        231     9    11              105.03423     11
12   2.4849067           97        264     8    12               78.10613     12
13   2.5649494           28        299     7    13              107.95286     13
14   2.6390573           99        336     6    14               80.07396     14
15   2.7080502           66        375     5    15              102.01156     15
16   2.7725887           95        416     4    16              119.07361     16
17   2.8332133           42        459     3    17               64.19354     17
18   2.8903718           53        504     2    18              106.23402     18
19   2.9444390           85        551     1    19              151.07976     19
20   2.9957323           48        600     0    20               82.78324     20
</code></pre>

<p>I am using the following code to perform the same </p>

<pre><code>fn = ""delnowSample.txt""
tmpData = read.table(fn, header = TRUE,  sep= ""\t"" , blank.lines.skip = TRUE)
cnames = colnames(tmpData)
(fmla &lt;- as.formula(paste(cnames[length(cnames)], "" ~ "", paste(cnames[1:(length(cnames)-1)],collapse= ""+"")))  )
model &lt;- try(glm(formula = fmla, family=binomial(), na.action=na.omit, data=tmpData));
summary(model) 
</code></pre>

<p>The output that I get is as follow: </p>

<pre><code>&gt; summary(model)

Call:
glm(formula = as.formula(paste(dep, "" ~ "", paste(xn, collapse = ""+""))), 
    family = gaussian(), na.action = na.omit)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.37926  -0.11242  -0.03441   0.16087   0.28200  

Coefficients: (1 not defined because of singularities)
                                            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                                0.2638036  0.3078536   0.857  0.40592    
unlist(tmpData[""logOfOutput""])             0.9202273  0.2727884   3.373  0.00455 ** 
unlist(tmpData[""randomSample""])            0.0026201  0.0018177   1.441  0.17145    
unlist(tmpData[""multiplied""])              0.0288073  0.0012359  23.308 1.34e-12 ***
unlist(tmpData[""part1""])                   0.2106002  0.0403442   5.220  0.00013 ***
unlist(tmpData[""part2""])                          NA         NA      NA       NA    
unlist(tmpData[""randNormalMean100Std20""]) -0.0006214  0.0024922  -0.249  0.80673    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for gaussian family taken to be 0.04403284)

    Null deviance: 665.00000  on 19  degrees of freedom
Residual deviance:   0.61646  on 14  degrees of freedom
AIC: 1.1676

Number of Fisher Scoring iterations: 2
</code></pre>

<p>To a large extent it is predicting the Pr(z) correctly as we can see the probabilities of random variable are not significant. The R-square is also high (1-residual.deviance/null.deviance), close to 1.  </p>

<p>Question 1:
In the above data 'part1+part2' is equal to output variable. Is 'glm' not able to identify such type of relations? </p>

<p>Question 2: 
Why the degree of freedom of  null and residual deviance are different? </p>

<p>Question 3:
I need to convert the output variable into categorical variable (i.e. Everything &lt;=10 is 'no' and more than this is 'yes'). What is the best way to call 'glm', when the response variable is 'categorical'. I tried converting 'no' to '0' and 'yes' to 1, and called glm as follows:</p>

<pre><code> model &lt;- try(glm(formula = as.formula(paste(dep, "" ~ "", paste(xn, collapse= ""+""))), family=binomial(), na.action=na.omit));
</code></pre>

<p>I am getting warning message with this code. Also, I am not sure if this is the correct way to call categorical variable. </p>

<p>Edit:</p>

<p>I have the following categorical data:</p>

<pre><code>&gt; tmpData
           x1  x2 x3  y1
1  0.16294456   1  1  no
2  0.80494934   2  2  no
3  0.28962222   1  3  no
4  0.07177347   2  4  no
5  0.54830544   1  5  no
6  0.67655327   2  6  no
7  0.45189608   1  7  no
8  0.82412502   2  8  no
9  0.09076793   1  9  no
10 0.12221227   2 10  no
11 0.56751754 111 11 yes
12 0.04970992 222 12 yes
13 0.56162037 111 13 yes
14 0.96617891 222 14 yes
15 0.50994534 112 15 yes
16 0.70093692 212 16 yes
17 0.02034940 212 17 yes
18 0.78356903 121 18 yes
19 0.58439662 213 19 yes
20 0.31729282 212 20 yes
</code></pre>

<p>And the following code:</p>

<pre><code>  fn = ""delnowSample.txt""
  tmpData = read.table(fn, header = TRUE,  sep= ""\t"" , blank.lines.skip = TRUE)
  tmpData
  model &lt;- glm(formula = 'y1~x1+x2+x3', family=binomial(), na.action=na.omit, data=tmpData)
  summary(model) 
</code></pre>

<p>This one doesn't seem to be working?? </p>
"
"0.0754493182241785","0.0742610657232506"," 47795","<p>I am currently carrying out an investigation to find if certain factors such as playing home or away or position of a footballer affects overall pass completion using logistic regression. I am using R to compute my data. In my current section in which I am trying to analyse uses the data of every player to convey a general conclusion to whether or not the position of a player affects the successfulness of pass completion. </p>

<p>so far I have computed:</p>

<pre><code>test.logit &lt;- glm( cbind(Total.Successful.Passes.All,Total.Unsuccessful.Passes.All) ~
                   as.factor(Position.Id), data=passes.data, family = ""binomial"")

summary(test.logit)
</code></pre>

<p>and my output was:</p>

<pre><code>Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    

(Intercept)              0.28482    0.01256   22.67   &lt;2e-16 

as.factor(Position.Id)2  0.99768    0.01438   69.38   &lt;2e-16 

as.factor(Position.Id)4  1.06679    0.01398   76.29   &lt;2e-16 

as.factor(Position.Id)6  0.68090    0.01652   41.23   &lt;2e-16 

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 32638  on 10269  degrees of freedom
Residual deviance: 26499  on 10266  degrees of freedom

AIC: 60422

Number of Fisher Scoring iterations: 4
</code></pre>

<p>the intercept is goalkeepers,position.Id 2 is for a defender, 4 = midfielder and 6 = striker</p>

<p>Is this a good set of results to come to a conclusion? and with the large deviances?</p>
"
"0.328924846080606","0.323744603499602"," 48040","<p>I'm trying to test the significance of the ""component"" effect in a multivariate regression model. I'm not sure what is the right way. Using R, I have tried a way with <code>lm()</code> and another way with <code>gls()</code>, and they don't yield compatible results. </p>

<p><strong>Please note that this is not a question about which methodology is the right one to use to analyze my data. By the way these are simulated data. My question is about the understanding in mathematical terms of the R procedures I use.</strong></p>

<p>The dataset:</p>

<pre><code>&gt; str(dat)
'data.frame':   31 obs. of  5 variables:
 $ group: Factor w/ 5 levels ""1"",""5"",""2"",""3"",..: 1 1 1 1 1 1 1 1 3 3 ...
     $ id   : Factor w/ 8 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 1 3 ...
 $ x    : num  2.5 3 3 4 1.2 3.8 3.9 4 2.5 2.9 ...
     $ y    : num  2.6 3.8 3.9 3.8 1.6 5.2 1.3 3.6 4 3.2 ...
 $ z    : num  3.1 3.6 4.9 3.8 2.1 6 2.1 2.9 4.2 2.9 ...
&gt; head(dat,10)
   group id   x   y   z
1      1  1 2.5 2.6 3.1
2      1  2 3.0 3.8 3.6
3      1  3 3.0 3.9 4.9
4      1  4 4.0 3.8 3.8
5      1  5 1.2 1.6 2.1
6      1  6 3.8 5.2 6.0
7      1  7 3.9 1.3 2.1
8      1  8 4.0 3.6 2.9
9      2  1 2.5 4.0 4.2
10     2  3 2.9 3.2 2.9
</code></pre>

<p>I convert this dataset into ""long format"" for graphics (and later for <code>gls()</code>):</p>

<pre><code>dat$subject &lt;- dat$group : dat$id
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

xyplot(value ~ component | group, data=dat.long, 
    pch=16, 
    strip = strip.custom(strip.names=TRUE,var.name=""group"" ), layout=c(5,1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/14KtA.png"" alt=""enter image description here""></p>

<p>Each individual of each group has $3$ repeated measures $x$,$y$,$z$ (I should join the points in the graphic to see the repeated measures).</p>

<p>I want to fit a MANOVA model using group as factor and $(x,y,z)$ is the multivariate response:
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right), \quad i=1,\ldots,5
$$
(of course we could use the default R parameterization $\mu_{ik}=\mu_{1k} + \alpha_{ik}$ by considering <code>group1</code>as the ""intercept"" for each response but I prefer ""my"" parameterization).</p>

<p>This model is fitted as follows using <code>lm()</code>:</p>

<pre><code>###  multivariate least-squares fitting  ###
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )
</code></pre>

<p>I think the model can also  be fitted with <code>gls()</code> as follows (but with a different fitting procedure) :</p>

<pre><code>### generalized least-squares fitting  ###
library(nlme)
gfit &lt;- gls(value ~ group*component, data=dat.long, correlation=corSymm(form= ~ 1 | subject))
</code></pre>

<p>Recall that <code>subject = group:id</code> is the identifier of the individuals. The <code>correlation=corSymm(form= ~ 1 | subject)</code> argument means that the responses $x$, $y$, $z$ for each individual are correlated. Here <code>corSymm</code> means a general, ""unrestricted"",  covariance structure (termed as ""unstructured"" in SAS language).</p>

<p>To check that <code>mfit</code> and <code>gfit</code> are equivalent, we can check for instance that we can deduce the estimated parameters of <code>mfit</code> from the estimated parameters of <code>gfit</code>and vice-versa (so the ""mean"" parameters have exactly the same fitted values):</p>

<pre><code>&gt; coef(mfit)
                  x          y          z
(Intercept)  3.1750  3.2250000  3.5625000
group5      -0.9500 -0.4750000  0.1125000
group2      -1.0750 -0.5678571 -0.2339286
group3      -0.7875 -0.1000000  0.1875000
group4      -0.3750  0.4000000 -0.0125000
&gt; coef(gfit)
      (Intercept)            group5            group2            group3 
        3.1750000        -0.9500000        -1.0750000        -0.7875000 
           group4        componenty        componentz group5:componenty 
       -0.3750000         0.0500000         0.3875000         0.4750000 
group2:componenty group3:componenty group4:componenty group5:componentz 
        0.5071429         0.6875000         0.7750000         1.0625000 
group2:componentz group3:componentz group4:componentz 
        0.8410714         0.9750000         0.3625000 
</code></pre>

<p>Now I want to test the ""component effect"". Rigorously speaking, writing the model as 
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right),
$$
I want to test the hypothesis $\boxed{H_0\colon \{\mu_{1i}=\mu_{2i}=\mu_{3i} \quad \forall i=1,2,3,4,5 \}}$.</p>

<p>Below are my attempts, one attempt with <code>gfit</code> and two attempts with <code>mfit()</code>:</p>

<pre><code>###########################################
## testing significance of the component ##
###########################################

&gt; ### with gfit  ###
&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
&gt; 
&gt; ### with mfit ###
&gt; library(car)
&gt; 
&gt; # first attempt : 
&gt; idata &lt;- data.frame(component=c(""x"",""y"",""z""))
&gt; ( av.ok &lt;- Anova(mfit, idata=idata, idesign=~component, type=""III"") )

Type III Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.84396  140.625      1     26 5.449e-12 ***
group            4   0.10369    0.752      4     26    0.5658    
component        1   0.04913    0.646      2     25    0.5328    
group:component  4   0.22360    0.818      8     52    0.5901    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; 
&gt; # second attempt :
&gt; linearHypothesis(mfit, ""(Intercept) = 0"", idata=idata, idesign=~component, iterms=""component"")

 Response transformation matrix:
  component1 component2
x          1          0
y          0          1
z         -1         -1

Sum of squares and products for the hypothesis:
           component1 component2
component1    1.20125    1.04625
component2    1.04625    0.91125

Sum of squares and products for error:
           component1 component2
component1   31.46179   14.67696
component2   14.67696   21.42304

Multivariate Tests: 
                 Df test stat  approx F num Df den Df  Pr(&gt;F)
Pillai            1 0.0491253 0.6457903      2     25 0.53277
Wilks             1 0.9508747 0.6457903      2     25 0.53277
Hotelling-Lawley  1 0.0516632 0.6457903      2     25 0.53277
Roy               1 0.0516632 0.6457903      2     25 0.53277
</code></pre>

<p>With <code>anova(gfit)</code> the component is significant, but not with my two attempts using <code>mfit</code> and the <code>car</code> package. </p>

<p>I know that <code>gls()</code> use a different fitting method than <code>lm()</code> but this is surely not the cause of the difference. </p>

<p>So my questions are :</p>

<ul>
<li>did I do something wrong ?</li>
<li>which method tests my $H_0$ hypothesis ?</li>
<li>what is the $H_0$ hypothesis of the other methods ?</li>
</ul>

<p>And I have an auxiliary question: how to get $\hat\Sigma$ with <code>mfit</code> and <code>gfit</code> ?</p>

<h2>Update 1</h2>

<p>Below is a reproducible example which simulates the dataset. 
Now I think I understand : both ANOVA methods are correct (the first one with <code>anova(gfit)</code> and the second one with <code>Anova(mfit, ...)</code>, <strong>and they yield very close results when using the type II sum of squares in <code>Anova(mfit, ...)</code></strong>.  For the above example: </p>

<pre><code>&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>is very close to </p>

<pre><code>&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>

<p>Below is the reproducible code with the data sampler (I simulate uncorrelated repeated measures but it suffices to include a covariance matrix in the <code>rmvnorm()</code> function to simulate correlated repeated measures) :</p>

<pre><code>library(mvtnorm)
library(nlme)
library(car)

# set data parameters 
I &lt;- 5 # number of groups
J &lt;- 16 # number of individuals per group
dat &lt;- data.frame(
    group = gl(I,J),
    id = gl(J,1,I*J),
    x=NA, 
    y=NA, 
    z=NA
)
Mu &lt;- c(1:I) # group means of components (assuming E(x)=E(y)=E(z) in each group)

# simulates data: 
for(i in 1:I){
    which.group.i &lt;- which(dat$group==i)
    dat[which.group.i,c(""x"",""y"",""z"")] &lt;- round(rmvnorm(n=J, mean=rep(Mu[i],3)), 1)
}

dat$subject &lt;- droplevels( dat$group : dat$id )
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

# multivariate least-squares fitting 
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )

# gls fitting
dat.long$order.xyz &lt;- as.numeric(dat.long$component)
gfit &lt;- gls(value ~ group*component , data=dat.long, correlation=corSymm(form=  ~ order.xyz | subject)) 

# compares ANOVA : 
anova(gfit)
idata &lt;- data.frame(component=c(""x"",""y"",""z""))
Anova(mfit, idata=idata, idesign=~component, type=""II"")
Anova(mfit, idata=idata, idesign=~component, type=""III"")
</code></pre>

<p>So now I wonder which type of sum of squares is the more appropriate one for my real study... but this is another question</p>

<h2>Update 2</h2>

<p>About my question <em>""how to get $\hat\Sigma$""</em>, here is the answer for <code>gls()</code>:</p>

<pre><code>&gt; getVarCov(gfit)
Marginal variance covariance matrix
        [,1]    [,2]    [,3]
[1,] 0.92909 0.47739 0.24628
[2,] 0.47739 0.92909 0.53369
[3,] 0.24628 0.53369 0.92909
  Standard Deviations: 0.96389 0.96389 0.96389 
</code></pre>

<p>That shows that <strong><code>mfit</code>and <code>gfit</code> were not equivalent models</strong>: <code>gfit</code>assumes the same variance for the three components.</p>

<p>In order to fit a fully unrestricted covariance matrix for the repeated measures, we have to type:</p>

<pre><code>gfit2 &lt;- gls(value ~ group*component , data=dat.long, 
    correlation=corSymm(form=  ~ 1 | subject), 
    weights=varIdent(form = ~1 | component))

&gt; summary(gfit2)
Generalized least squares fit by REML
  Model: value ~ group * component 
  Data: dat.long 
       AIC      BIC    logLik
  264.0077 313.4986 -111.0038

Correlation Structure: General
 Formula: ~1 | subject 
 Parameter estimate(s):
 Correlation: 
  1     2    
2 0.529      
3 0.300 0.616
Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | component 
 Parameter estimates:
       x        y        z 
1.000000 1.253534 1.169335 

....

Residual standard error: 0.8523997 
</code></pre>

<p>But yet I don't understand the extracted covariance matrix given by <code>getVarCov()</code> (but this is not important since we get this matrix with <code>summary(gfit2)</code>): </p>

<pre><code>   &gt; getVarCov(gfit2)
    Error in t(S * sqrt(vars)) : 
      dims [product 9] do not match the length of object [0]
    &gt; getVarCov(gfit2, individual=""1:1"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 0.72659 0.48164 0.25500
    [2,] 0.48164 1.14170 0.65562
    [3,] 0.25500 0.65562 0.99349
      Standard Deviations: 0.8524 1.0685 0.99674 
    &gt; getVarCov(gfit2, individual=""1:2"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 1.14170 0.56319 0.27337
    [2,] 0.56319 0.99349 0.52302
    [3,] 0.27337 0.52302 0.72659
      Standard Deviations: 1.0685 0.99674 0.8524 
</code></pre>

<p>Unfortunately, the <code>anova(gfit2)</code> table is not as close to <code>Anova(mfit, ..., type=""II"")</code> as <code>anova(gfit)</code>:</p>

<pre><code>&gt; anova(gfit2)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 498.1744  &lt;.0001
group               4   1.0514  0.3864
component           2  13.1801  &lt;.0001
group:component     8   0.8310  0.5780

&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>
"
"0.0616041103633697","0.0606339062590832"," 48811","<p>For count data that I have collected, I use Poisson regression to build models. I do this using the <code>glm</code> function in R, where I use <code>family = ""poisson""</code>. To evaluate possible models (I have several predictors) I use the AIC. So far so good. Now I want to perform cross-validation. I already succeeded in doing this using the <code>cv.glm</code> function from the <code>boot</code> package. From <a href=""http://stat.ethz.ch/R-manual/R-patched/library/boot/html/cv.glm.html"">the documentation</a> of <code>cv.glm</code> I see that e.g. for binomial data you need to use a specific cost function to get a meaningful prediction error. However, I have no idea yet what cost function is appropriate for <code>family = poisson</code>, and an extensive Google search did not yield any specific results. My question is anybody has some light to shed on which cost function is appropriate for <code>cv.glm</code> in case of poisson glm's.</p>
"
"0.123208220726739","0.121267812518166"," 51152","<p>I've been trying to use the fastbw function from the rms package in R to perform logistic regression with backward selection, with p-values as exclusion criterion (I am well aware of the arguments against using p-values for this as opposed to e.g. AIC). However, the results are not in agreement with what I would get if I perform the backward selection manually, as fastbw often drops more factors in comparison. The results also seem to depend on the number of factors considered, even with the option </p>

<pre><code>type=""individual"".
</code></pre>

<p>I created some simple example data in order to prove my point, which give the following result:</p>

<pre><code>&gt; fastbw(lrm(y~x1+x2+x3+x4),rule=""p"",type=""individual"")

 Deleted Chi-Sq d.f. P      Residual d.f. P      AIC  
 x3      0.37   1    0.5412 0.37     1    0.5412 -1.63
 x1      1.82   1    0.1771 2.20     2    0.3336 -1.80
 x4      2.58   1    0.1082 4.78     3    0.1889 -1.22
 x2      3.56   1    0.0591 8.34     4    0.0799  0.34

[...]

Factors in Final Model

None
</code></pre>

<p>I.e., x2 is dropped as the last of the factors considered, resulting in a model without factors. However, if I consider x2 only, I get the following result. </p>

<pre><code>&gt; fastbw(lrm(y~x2),rule=""p"",type=""individual"")

No Factors Deleted

Factors in Final Model

[1] x2
</code></pre>

<p>The same is true if I do the backward selection manually, as x2 considered separately has a p-value of 0.045. What might cause this behavior? Since x2 is the last remaining variable in the backward selection, the results shouldn't depend on associations with other model covariates.</p>
"
"NaN","NaN"," 52423","<p>I am trying to perform stepwise regression for variable selection in R. 
In matlab, the <code>stepwisefit function</code> is able to work in <code>n &lt; p</code> problems. Trying to use <code>step()</code> for such problems i get the error message <code>AIC is -infinity for this model, so 'step' cannot proceed</code>. Is there a way to modify parameters so i can use <code>step</code> function?</p>
"
"0.0653410262803548","0.0857492925712544"," 55940","<p>I have a regression model that looks like the following</p>

<pre><code>glm.nb(formula = y ~ Gender + Age + x1 + x2 + x3, data = df)
</code></pre>

<p>In my problem, there are 20 possible choices of variables for <code>x1</code>, 20 possible choices for <code>x2</code>, and 20 possible choices for <code>x3</code>.  <code>Gender</code> and <code>Age</code> must be in the model.  This leaves me with 20*20*20 = 8,000 possible regressions.  I was able to create a program that ran all of these regressions and deliver me the lowest AIC, but I was wondering if there was a library that already does this.</p>

<p>I do not consider what I will find to be the ""best"" model in any statistical manner, but I do find this exercise useful for exploring my data.</p>

<p>I have already attempted using bestglm and leaps.  I do not believe these programs allow for specifying the choice of variable from multiple bucket of variables.</p>
"
"0.0974046509883157","0.0958706236059213"," 56440","<p>So I'm working with logistic regression models in R. Though I'm still new to statistics I feel like I got a bit of an understanding for regression models by now, but there's still something that bothers me:</p>

<p>Looking at the linked picture, you see the summary R prints for an example model I created. The model is trying to predict, if an email in the dataset will be refound or not (binary variable <code>isRefound</code>) and the dataset contains two variables closely related to <code>isRefound</code> , namely <code>next24</code> and <code>next7days</code> - these are also binary and tell if a mail will be clicked in the next 24hrs / next 7 days from the current point in the logs.</p>

<p>The high p-value should indicate, that the impact this variable has on the model prediction is pretty random, isn't it? 
Based on this I don't understand why the precision of the models predictions drops below 10% when these two variables are left out of the calculation formula. If these variables show such a low significance, why does removing them from the model have such a big impact?</p>

<p>Best regards and thanks in advance,
Rickyfox</p>

<p><img src=""http://i.stack.imgur.com/oiCrN.png"" alt=""enter image description here""></p>

<hr>

<h2>EDIT:</h2>

<p>First I removed only next24, which should yield a low impact because it's coef is pretty small. As expected, little changed - not gonna upload a pic for that. </p>

<p>Removing next7days tho had a big impact on the model: AIC 200k up, precision down to 16% and recall down to 73%</p>

<p><img src=""http://i.stack.imgur.com/583nx.png"" alt=""enter image description here""></p>
"
"0.179605302026775","0.176776695296637"," 57031","<p>I have data from a survey experiment in which respondents were randomly assigned to one of four groups:</p>

<pre><code>&gt; summary(df$Group)
       Control     Treatment1     Treatment2     Treatment3 
            59             63             62             66 
</code></pre>

<p>While the three treatment groups do vary slightly in the stimulus applied, the main distinction that I care about is between the control and treatment groups. So I defined a dummy variable <code>Control</code>:</p>

<pre><code>&gt; summary(df$Control)
     TRUE FALSE 
       59   191 
</code></pre>

<p>In the survey, respondents were asked (among other things) to choose which of two things they preferred: </p>

<pre><code>&gt; summary(df$Prefer)
      A   B  NA's 
    152  93   5 
</code></pre>

<p>Then, after receiving some stimulus as determined by their treatment group (and none if they were in the control group), respondents were asked to choose between the same two things:</p>

<pre><code>&gt; summary(df$Choice)
  A    B 
149  101 
</code></pre>

<p>I want to know if the being in one of the three treatment groups had an effect on the choice that respondents made in this last question. My hypothesis is that respondents who received a treatment are more likely to choose <code>A</code> than <code>B</code>.     </p>

<p>Given that I am working with categorical data, I have decided to use a logit regression (feel free to chime in if you think that's incorrect). Since respondents were randomly assigned, I am under the impression that I shouldn't necessarily need to control for other variables (e.g. demographics), so I have left those out for this question. My first model was simply the following:</p>

<pre><code>&gt; x0 &lt;- glm(Product ~ Control + Prefer, data=df, family=binomial(link=""logit""))
&gt; summary(x0)

Call:
glm(formula = Choice ~ Control + Prefer, family = binomial(link = ""logit""), 
    data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8366  -0.5850  -0.5850   0.7663   1.9235  

Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           1.4819     0.3829   3.871 0.000109 ***
ControlFALSE         -0.4068     0.3760  -1.082 0.279224    
PreferA              -2.7538     0.3269  -8.424  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 328.95  on 244  degrees of freedom
Residual deviance: 239.69  on 242  degrees of freedom
  (5 observations deleted due to missingness)
AIC: 245.69

Number of Fisher Scoring iterations: 4
</code></pre>

<p>I am under the impression that the intercept being statistically significant is not something that holds interpretable meaning. I thought perhaps that I should include an interaction term as follows:</p>

<pre><code>&gt; x1 &lt;- glm(Choice ~ Control + Prefer + Control:Prefer, data=df, family=binomial(link=""logit""))
&gt; summary(x1)

Call:
glm(formula = Product ~ Control + Prefer + Control:Prefer, family = binomial(link = ""logit""), 
    data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.5211  -0.6424  -0.5003   0.8519   2.0688  

Coefficients:
                                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                         3.135      1.021   3.070  0.00214 ** 
ControlFALSE                       -2.309      1.054  -2.190  0.02853 *  
PreferA                            -5.150      1.152  -4.472 7.75e-06 ***
ControlFALSE:PreferA                2.850      1.204   2.367  0.01795 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 328.95  on 244  degrees of freedom
Residual deviance: 231.27  on 241  degrees of freedom
  (5 observations deleted due to missingness)
AIC: 239.27

Number of Fisher Scoring iterations: 5
</code></pre>

<p>Now the respondents status as in a treatment group has the expected effect. Was this a valid set of steps? How can I interpret the interaction term <code>ControlFALSE:PreferA</code>? Are the other coefficients still the log odds?</p>
"
"0.13068205256071","0.128623938856882"," 60760","<p>let <code>m</code> be my matrix of data</p>

<pre><code>      x_i y_i
 [1,] 0.0   0
 [2,] 0.0   0
 [3,] 0.0   0
 [4,] 0.0   0
 [5,] 0.1   0
 [6,] 0.2   0
 [7,] 0.3   0
 [8,] 0.4   0
 [9,] 0.5   0
[10,] 0.6   0
[11,] 0.0   1
[12,] 0.0   1
[13,] 0.0   1
[14,] 0.9   1
[15,] 1.0   1
</code></pre>

<p>My aim is to study the logistic regression <code>y~x</code>, where the covariate <code>x</code> has observations <code>m[,1]</code> and similarly for <code>y</code>.
Please note that we have no complete separation in the data <em>but</em> the ""anomalous"" entries in rows <code>m[11,], m[12,]</code> and <code>m[13,]</code> all correspond to observations with <code>x_i=0</code>.</p>

<p>I expect <code>glm</code> to diverge as the likelihood function reaches no maximum in the ray  $k\beta$, for $k\rightarrow \infty$ and $\beta=(-0.7,1)$. </p>

<p>Using <code>glm</code> with 1 iteration I get the output </p>

<pre><code>  Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.2552     0.7648  -1.641    0.101
x             1.6671     1.7961   0.928    0.353

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.275  on 13  degrees of freedom
AIC: 22.275

Number of Fisher Scoring iterations: 1
</code></pre>

<p>with an error message (the algorithm does not converge). 
Moreover, with the default number of iterations <code>(=25)</code> the output is</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.1257     0.7552  -1.491    0.136
x             1.4990     1.6486   0.909    0.363

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.246  on 13  degrees of freedom
AIC: 22.246

Number of Fisher Scoring iterations: 4
</code></pre>

<p>and no error warning. </p>

<p>I see a contradiction; even in presence of 1 iteration the algorithm does not converge but the output is ""finite"" (I have not explicitly computed the inverse of the Hessian of the likelihood function, unfortunately). Moreover, with 25 iterations the warning message disappears and the output is still finite.</p>

<p>What do you think about this situation?
 Is it possible that <code>glm</code> stops automatically after the first iteration?
Thank you, Avitus</p>
"
"0.146215163817907","0.154982604969517"," 61869","<p>I am trying to replicate a path analysis SEM model using Lavaan in R, and was very confused about the results that it gave regarding the model fit statistics. </p>

<p><strong>The code is as follows:</strong> </p>

<pre><code>#Import Package
library(lavaan)

#Input Correlation Matrix
sigma &lt;- matrix(c(1.00, -0.03,  0.39, -0.05, -0.08,
                 -0.03,  1.00,  0.07, -0.23, -0.16,
                  0.39,  0.07,  1.00, -0.13, -0.29,
                 -0.05, -0.23, -0.13,  1.00,  0.34,
                 -0.08, -0.16 ,-0.29,  0.34,  1.00), nr=5, byrow=TRUE)
rownames(sigma) &lt;-c(""Exercise"", ""Hardiness"", ""Fitness"", ""Stress"", ""Illness"")
colnames(sigma) &lt;-c(""Exercise"", ""Hardiness"", ""Fitness"", ""Stress"", ""Illness"")

#Create Covariance Matrix
sdevs &lt;-c(66.5, 3.8, 18.4, 6.7, 624.8)
covmax &lt;- cor2cov(sigma, sdevs)
as.matrix(covmax)

#Specify Model 
mymodel&lt;-'Illness ~ Exercise + Fitness
Illness ~ Hardiness + Stress
Fitness ~ Exercise + Hardiness 
Stress ~ Exercise + Hardiness + Fitness 
Exercise ~~ Exercise 
Hardiness ~~ Hardiness 
Exercise ~~ Hardiness'

#Fit the model with the covariance matrix
N = 363
fit.path &lt;-sem(mymodel,sample.cov=covmax, sample.nobs=N, fixed.x=FALSE)

#Summary of the model fit
summary(fit.path, fit.measures = TRUE)
</code></pre>

<p><strong>And the output I get is as follows:</strong> </p>

<pre><code> lavaan (0.5-12) converged normally after  93 iterations

 Number of observations                         37300

 Estimator                                         ML
 Minimum Function Test Statistic                0.000
 Degrees of freedom                                 0
 P-value (Chi-square)                           1.000

 Model test baseline model:

 Minimum Function Test Statistic            16594.387
 Degrees of freedom                                10
 P-value                                        0.000

 Full model versus baseline model:

 Comparative Fit Index (CFI)                    1.000
 Tucker-Lewis Index (TLI)                       1.000

 Loglikelihood and Information Criteria:

 Loglikelihood user model (H0)             -882379.005
 Loglikelihood unrestricted model (H1)     -882379.005

 Number of free parameters                         15
 Akaike (AIC)                              1764788.009
 Bayesian (BIC)                            1764915.910
 Sample-size adjusted Bayesian (BIC)       1764868.240

 Root Mean Square Error of Approximation:

 RMSEA                                          0.000
 90 Percent Confidence Interval          0.000  0.000
 P-value RMSEA &lt;= 0.05                          1.000

 Standardized Root Mean Square Residual:

 SRMR                                           0.000

 Parameter estimates:

 Information                                 Expected
 Standard Errors                             Standard

                Estimate  Std.err  Z-value  P(&gt;|z|)
 Regressions:
 Illness ~
 Exercise          0.318    0.048    6.640    0.000
 Fitness          -8.835    0.174  -50.737    0.000
 Hardiness       -12.146    0.793  -15.321    0.000
 Stress           27.125    0.451   60.079    0.000
 Fitness ~
 Exercise          0.109    0.001   82.602    0.000
 Hardiness         0.396    0.023   17.211    0.000
 Stress ~
 Exercise         -0.001    0.001   -2.614    0.009
 Hardiness        -0.393    0.009  -44.332    0.000
 Fitness          -0.040    0.002  -19.953    0.000

 Covariances:
 Exercise ~~
 Hardiness        -7.581    1.309   -5.791    0.000

 Variances:
 Exercise       4422.131   32.381
 Hardiness        14.440    0.106
 Illness       318744.406 2334.012
 Fitness         284.796    2.085
 Stress           41.921    0.307
</code></pre>

<p><strong>These are my questions:</strong>  </p>

<ul>
<li>Why does the chi-squared say that there are no degrees of freedom? </li>
<li>Why are the p-values exactly 1? Why is the CFI and TLI exactly 1? </li>
<li><p>Why is the RMSEA 0?</p></li>
<li><p>What would I need to do to simulate a more realistic model that doesn't appear artificially ""perfect""? </p></li>
<li>Does it have to do with the model specification? </li>
</ul>
"
"0.169040284260494","0.176776695296637"," 63222","<p>How do I get p-values using the <code>multinom</code> function of <code>nnet</code> package in <code>R</code>?</p>

<p>I have a dataset which consists of â€œPathology scoresâ€ (Absent, Mild, Severe) as outcome variable, and two main effects: Age (two factors: twenty / thirty days) and Treatment Group (four factors: infected without ATB; infected + ATB1; infected + ATB2; infected + ATB3).</p>

<p>First I tried to fit an ordinal regression model, which seems more appropriate given the characteristics of my dependent variable (ordinal). However, the assumption of odds proportionality was severely violated (graphically), which prompted me to use a multinomial model instead, using the <code>nnet</code> package.  </p>

<p>First I chose the outcome level that I need to use as baseline category: </p>

<pre><code>Data$Path &lt;- relevel(Data$Path, ref = ""Absent"")
</code></pre>

<p>Then, I needed to set baseline categories for the independent variables:</p>

<pre><code>Data$Age &lt;- relevel(Data$Age, ref = ""Twenty"")
Data$Treat &lt;- relevel(Data$Treat, ref=""infected without ATB"") 
</code></pre>

<p>The model:</p>

<pre><code>test &lt;- multinom(Path ~ Treat + Age, data = Data) 
# weights:  18 (10 variable) 
initial value 128.537638 
iter 10 value 80.623608 
final  value 80.619911 
converged
</code></pre>

<p>The output:</p>

<pre><code>Coefficients:
         (Intercept)   infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate   -2.238106   -1.1738540      -1.709608       -1.599301        2.684677
Severe     -1.544361   -0.8696531      -2.991307       -1.506709        1.810771

Std. Errors:
         (Intercept)    infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate   0.7880046    0.8430368       0.7731359       0.7718480        0.8150993
Severe     0.6110903    0.7574311       1.1486203       0.7504781        0.6607360

Residual Deviance: 161.2398
AIC: 181.2398
</code></pre>

<p>For a while, I could not find a way to get the $p$-values for the model and estimates when using <code>nnet:multinom</code>. Yesterday I came across a post where the author put forward a similar issue regarding estimation of $p$-values for coefficients (<a href=""http://stats.stackexchange.com/questions/9715/how-to-set-up-and-estimate-a-multinomial-logit-model-in-r"">How to set up and estimate a multinomial logit model in R?</a>). There, one blogger suggested that getting $p$-values from the <code>summary</code> result of <code>multinom</code> is pretty easy, by first getting the $t$values as follows: </p>

<pre><code>pt(abs(summary1$coefficients / summary1$standard.errors), df=nrow(Data)-10, lower=FALSE) 

         (Intercept)   infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate 0.002670340   0.08325396      0.014506395     0.02025858       0.0006587898
Severe   0.006433581   0.12665278      0.005216581     0.02352202       0.0035612114
</code></pre>

<p>According to Peter Dalgard, ""There's at least a factor of 2 missing for a two-tailed $p$-value. It is usually a mistake to use the $t$-distribution for what is really a $z$-statistic; for aggregated data, it can be a very bad mistake.""
According to Brian Ripley, ""it is also a mistake to use Wald tests for <code>multinom</code> fits, since they suffer from the same (potentially severe) problems as binomial fits. 
Use profile-likelihood confidence intervals (for which the package does provide software), or if you must test, likelihood-ratio tests (ditto).""</p>

<p>I just need to be able to derive reliable $p$-values.</p>
"
"0.0616041103633697","0.0606339062590832"," 63494","<p>In <code>R</code> I have a categorical variable that I performed logistic regression on and got the following result:</p>

<pre><code>glm(formula = mortality ~ SMOKE, family = binomial, data = c.data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.2155  -0.2155  -0.2155  -0.1860   2.8515  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -4.0483     0.3189 -12.694   &lt;2e-16 ***
SMOKEN        0.2968     0.3559   0.834    0.404    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 492.45  on 2369  degrees of freedom
Residual deviance: 491.72  on 2368  degrees of freedom
AIC: 495.72

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Is the value for the intercept the same as <code>SMOKEY</code> (has a history of smoking)?</p>
"
"0.123208220726739","0.121267812518166"," 63927","<p>I am struggling to fit a simple logistic regression for one dependent value (group) by one independent qualitative variable (dilat) measured twice independently (rater).</p>

<p>I try many solutions and think according <a href=""http://www.ats.ucla.edu/stat/mult_pkg/whatstat/"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/whatstat/</a> that the solution is a Mixed Effects Logistic Regression.</p>



<pre class=""lang-r prettyprint-override""><code>glmer_dilat&lt;-glmer(group ~ dilat + (1 | rater), data = ex, family = binomial)
summary(glmer_dilat)
</code></pre>



<pre class=""lang-r prettyprint-override""><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: group ~ dilat + (1 | rater) 
   Data: ex 
   AIC   BIC logLik deviance
 105.5 112.5 -49.74    99.48
Random effects:
 Groups Name        Variance Std.Dev.
 rater  (Intercept)  0        0      
Number of obs: 76, groups: rater, 2

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4880   1.736   0.0825 .
dilat        -1.2827     0.5594  -2.293   0.0219 *
</code></pre>

<p>But the result is the same without !</p>

<pre class=""lang-r prettyprint-override""><code>summary(glm(group ~ dilat, data =ex, family = binomial))

glm(formula = group ~ dilat, family = binomial, data = ex)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.552  -0.999  -0.999   1.367   1.367  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4879   1.736   0.0825 .
dilat        -1.2826     0.5594  -2.293   0.0219 *
</code></pre>

<p>What is the solution?</p>

<p>please find my data set here after applying a dput command to it.</p>

<pre class=""lang-r prettyprint-override""><code>structure(list(id = structure(c(38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 23L, 15L, 24L, 25L, 37L, 26L, 38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 22L, 23L, 15L, 24L, 37L, 26L), .Label = c(""1038835"", ""2025267"", ""2053954"", ""3031612"", ""40004760"", ""40014515"", ""40040532"", ""40092413"", ""40101857"", ""40105328"", ""4016213"", ""40187296"", ""40203950"", ""40260642"", ""40269263"", ""40300349"", ""40308059"", ""40327146"", ""40333651"", ""40364468"", ""40435267"", ""40440293"", ""40443920"", ""40485124"", ""40609779"", ""40628741"", ""40662695"", ""5025220"", ""E9701737"", ""M/377313"", ""qsc22913"", ""QSC29371"", ""QSC43884"", ""QSC62220"", ""QSC75555"", ""QSC92652"", ""QSD01289"", ""QSD02237"", ""U/FY0296"" ), class = ""factor""), group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), rater = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), dilat = c(1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), midbrain_atroph = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), quadrigemi_atroph = c(1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), hum_sig = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), flower_sig = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), fp_atroph = c(0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), scp_atroph = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L)), .Names = c(""id"", ""group"", ""rater"", ""dilat"", ""midbrain_atroph"", ""quadrigemi_atroph"", ""hum_sig"", ""flower_sig"", ""fp_atroph"", ""scp_atroph""), class = ""data.frame"", row.names = c(NA, -76L))
</code></pre>
"
"0.157060280430173","0.154586735600211"," 64535","<p>My ecological question is: ""What are the trends in percent coral cover by island and depth across the state of Hawaii from 1999 to 2012?""  </p>

<p>I am trying to analyze this hierarchical data set using R with 10 transects at each depth, 2 depths per site, and site nested in island.</p>

<p>Data structure:</p>

<pre><code>Fixed effects:
 Island: Hawaii, Maui, Molokai, Kahoolawe, Oahu, Kauai.
 DepthCat: S = Shallow, D = Deep.
 WYear: 0-13. It was suggested that I use this factor as a covariate for years.

Random effects:
 Site: 34 sites across the 6 islands with 2 depths per site.
 Transect: 10 permanent transects at each depth.
 Year: 1999 â€“ 2012 (14 years)

Dependent variable: PercentCover
</code></pre>

<p>Currently, I am using the <code>lmer</code> function in the <code>lmerTest</code> package and this is the model that I've constructed.</p>

<pre><code>fit1 &lt;- lmer(PercentCover ~ WYear*Island*DepthCat +
             (1+WYear|Island/Site/DepthCat/Transect) + (1|Year), data=Benthic)
</code></pre>

<p>Unfortunately, the data are spotty (i.e., missing data in multiple years for a number of sites) so the model returns <code>[1] ""Asymptotic covariance matrix A is not positive!""</code>, even using arcsin transformed data. I can still run the summary statistics to get results, but I don't feel comfortable with the error message. Perhaps I have not structured the model correctly in terms of organizing the nested factors, but the number of observations for each of the levels in the summary stats seems correct. I tried different and simpler iterations of the model such as:</p>

<pre><code> fit1 &lt;- lmer(PercentCover ~ WYear + Island + DepthCat + (1+WYear|Transect/Site) + 
              (1|Year), data=Benthic)
</code></pre>

<p>which works, but doesn't give me the interaction information and returns a larger AIC suggesting that the model does not fit the data as well.</p>

<p>To deal with all of the missing data, I tried another approach by using the regression slope of percent cover over time as the dependent variable for each site X depth combination.</p>

<pre><code>Data structure:

Fixed effects:
 Island: Hawaii, Maui, Molokai, Kahoolawe, Oahu, Kauai.
 DepthCat: S = Shallow, D = Deep.

Random effects:
 Site: 34 sites across the 6 islands with 2 depths per site.
 Transect: 10 permanent transects at each depth.

Dependent variable: Trend
</code></pre>

<p>I used the following model, but the summary results did not make much sense, even after transforming the data.</p>

<pre><code> fit1&lt;-lmer(Trend ~ Island*DepthCat + (1| Island/Site/DepthCat/Transect), data=Benthic)
</code></pre>

<p>Any suggestions on improving my analytical approach would be appreciated.</p>
"
"0.11525073729837","0.113435651621629"," 64788","<p>I performed multivariate logistic regression with the dependent variable <code>Y</code> being death at a nursing home within a certain period of entry and got the following results (note if the variables starts in <code>A</code> it is a continuous value while those starting in <code>B</code> are categorical):</p>

<pre><code>Call:
glm(Y ~ A1 + B2 + B3 + B4 + B5 + A6 + A7 + A8 + A9, data=mydata, family=binomial)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.0728  -0.2167  -0.1588  -0.1193   3.7788  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  20.048631  6.036637   3.321 0.000896 ***
A1           0.051167   0.016942   3.020 0.002527 ** 
B2          -0.664940   0.304299  -2.185 0.028878 *  
B3          -2.825281   0.633072  -4.463 8.09e-06 ***
B4          -2.547931   0.957784  -2.660 0.007809 ** 
B5          -2.862460   1.385118  -2.067 0.038774 *  
A6          -0.129808   0.041286  -3.144 0.001666 ** 
A7           0.020016   0.009456   2.117 0.034276 *  
A8          -0.707924   0.253396  -2.794 0.005210 ** 
A9           0.003453   0.001549   2.229 0.025837 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 485.10  on 2206  degrees of freedom
Residual deviance: 417.28  on 2197  degrees of freedom
AIC: 437.28

Number of Fisher Scoring iterations: 7

 (Intercept)           A1           B2           B3           B4           B5           A6           A7           A8           A9 
5.093426e+08 1.052499e+00 5.143045e-01 5.929197e-02 7.824340e-02 5.712806e-02 8.782641e-01 1.020218e+00 4.926657e-01 1.003459e+00 

                   2.5 %       97.5 %
(Intercept) 3.703525e+03 7.004944e+13
A1          1.018123e+00 1.088035e+00
B2          2.832698e-01 9.337710e-01
B3          1.714448e-02 2.050537e-01
B4          1.197238e-02 5.113460e-01
B5          3.782990e-03 8.627079e-01
A6          8.099945e-01 9.522876e-01
A7          1.001484e+00 1.039302e+00
A8          2.998207e-01 8.095488e-01
A9          1.000416e+00 1.006510e+00
</code></pre>

<p>As you can see, all of the variables are ""significant"" in that their p values are below the usual threshold of 0.05. However looking at the coefficients, I'm not quite sure what to make of these results. It seems that although these variables contribute to the model, looking at the odds ratios, they don't seem to really seem to have much predictive power. Of note, when I calculated the AUC, I got approximately 0.8. </p>

<p>Can I say that this model is better at predicting against mortality (e.g. predicting that seniors will live past the prescribed period) compared to predicting for mortality?</p>
"
"0.0754493182241785","0.0742610657232506"," 65258","<p>Consider the Challenger-Disaster:</p>

<pre><code>Temp &lt;- c(66,67,68,70,72,75,76,79,53,58,70,75,67,67,69,70,73,76,78,81,57,63,70)
Fail &lt;- factor(c(0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1))
shuttle &lt;- data.frame(Temp, Fail)
colnames(shuttle) &lt;- c(""Temp"", ""Fail"")
</code></pre>

<p>Now I can fit a logistic model which will explain the ""Fail"" of O-ring seals by Temperature:</p>

<pre><code>fit &lt;- glm(Fail~Temp,data=shuttle, family=binomial); fit
</code></pre>

<p>The R output looks like this:</p>

<pre><code> Call:  glm(formula = Ausfall ~ Temp, family = binomial, data =
 shuttle)

 Coefficients: (Intercept)         Temp  
     15.0429      -0.2322  

 Degrees of Freedom: 22 Total (i.e. Null);  21 Residual Null Deviance:  
 28.27  Residual Deviance: 20.32    AIC: 24.32
</code></pre>

<h3>Questions</h3>

<ul>
<li><strong>In general, how do you predict probabilities for specific data in logistic regressions using R?</strong></li>
<li><strong>Or specifically, what is the command to calculate the probability of a ""Fail"" if temperature is at 37Â°?</strong> (which it was in the night before the Challenger disaster).</li>
</ul>

<p>I thought it would be something like this:</p>

<pre><code>predict(fit, Temp=37)
</code></pre>

<p>but it won't give me ""0.9984243"" (which I calculated myself with:  </p>

<pre><code>exp(15.0429 + (37*(-0.2322))) / 1+ exp(15.0429 + (37*(-0.2322)))
</code></pre>

<p>The method <code>predict</code> returns a matrix of numbers that makes no sense to me.</p>
"
"0.0754493182241785","0.0742610657232506"," 65285","<p>I have a time series data set to which I am trying to fit a Hidden Markov Model (HMM) in order to estimate the number of latent states in the data.  My pseudo code for doing this is the following:</p>

<pre><code>for( i in 2 : max_number_of_states ){ 
    ...
    calculate HMM with i states
    ...
    optimal_number_of_states = ""model with smallest BIC""
    ...
}
</code></pre>

<p>Now, in the usual regression models the BIC tends to favor the most parsimonious models but in the case of the HMM I am not sure that is what it is doing.  Does anyone actually know what kind of HMM's the BIC criterion tends toward?  I also am able to obtain the AIC and likelihood value as well.  Since I am trying to infer the true total number of states, is one of these criteria ""better"" than the other for this purpose?</p>
"
"0.157462484111592","0.154982604969517"," 67243","<p>This is a fairly complex question so I will attempt to ask it in a fairly basic manner. </p>

<p>I have data on the abundance of 99 different species of estuarine macroinvertebrate species and the sediment mud content (0 - 100 %) in which each observation was obtained. I have a total of 1402 observations for each species (i.e. a massive dataset). </p>

<p>Here is a subset of the raw data for one species to give you an idea of the data I'm working with (if I had 10 reputation points I'd upload a plot of real raw data):</p>

<pre><code>Abundance: 10,14,10,3,3,3,3,4,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,6,0,0,0,0,12,0,0,0,34,0,0
Mud %:     0.9,4,2,10,13,14,6,5,5,7,22,27,34,37,47,58,54,70,54,80,90,65,56,7,8,34,67,54,32,1,57,45,49,4,78,65,45,35
</code></pre>

<p>The primary aim of my research is to determine an ""optimum mud % range"" (e.g. 15 - 45 %) and ""distribution mud % range"" (e.g. 0 - 80 %) for each of the 99 invertebrate species.</p>

<p>As you can see the abundance data for the above species contains a significant number of zero values. Although this significantly skews any sort of model that I run on the data (i.e. GLM, GAM), even if I model the non-zero data only, the model for certain species does not fit the data at all well.</p>

<p>So, my question is: what would be the best, most robust way to determine an ""optimum"" and ""distribution"" mud range for each species, given that responses vary significantly between species? </p>

<hr>

<p>Just to clarify - the above data is a hypothetical example to give you an idea of how messy the abundance (that is count) data can be for a given species.</p>

<p>Regarding the poisson regression approach: I'm considering conducting a two-step GLM or GAM approach for each species; Step (1) uses logistic regression to model the ""probability of presence""  for a given species over the mud gradient - using presence/absence data. This obviously takes into account the zero counts; and Step (2) models the ""maximum abundance"" over the mud gradient - using presence only count data. This step gives me an idea of the species typical response to mud where they DO occur. What are your thoughts on this approach?</p>

<p>I have R code for both steps for one particular species. Heres the code:</p>

<pre><code>     ## BINARY

aa1&lt;-glm(Bin~Mud,dist=binomial,data=Antho)
xmin &lt;- ceiling(min(Antho$Mud))
    xmax &lt;- floor(max(Antho$Mud))
Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
pred.dat &lt;- data.frame(Mudnew)
names(pred.dat) &lt;- ""Mud""
pred.aa1 &lt;- data.frame(predict.glm(aa1, pred.dat, se.fit=TRUE, type=""response""))
pred.aa1.comb &lt;- data.frame(pred.dat, pred.aa1)
names(pred.aa1.comb)
plot(fit ~ Mud, data=pred.aa1.comb, type=""l"", lwd=2, col=1, ylab=""Probability of presence"", xlab=""Mud content (%)"", ylim=c(0,1))


## Maximum abundance

 aa2&lt;-glm(Maxabund~Mud,family=Gamma,data=antho)
 xmin &lt;- ceiling(min(antho$Mud))
     xmax &lt;- floor(max(antho$Mud))
 Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
 pred.dat &lt;- data.frame(Mudnew)
 names(pred.dat) &lt;- ""Mud""
 pred.aa2 &lt;- data.frame(predict.glm(aa2, pred.dat, se.fit=TRUE, type=""response""))
 pred.aa2.comb &lt;- data.frame(pred.dat, pred.aa2)
 names(pred.aa2.comb)
 plot(fit ~ Mud, data=pred.aa2.comb, type=""l"", lwd=2, col=1, ylab=""Maximum abundance per 0.0132 m2"", xlab=""Mud content (%)"")
 AIC(aa2)
</code></pre>

<p>My question is: for step (2); will the model code need to be altered (i.e. family=) depending on the shape of each species abundance data, if so, would I just need to inspect a scatter plot of the raw presence only abundance data to confirm the use of a certain function? and how would the code be written for a certain species exhibiting a certain response/functional form? </p>
"
"0.0871213683738064","0.0857492925712544"," 68087","<p>I have a question about performing stepwise regression. I realize that there are issues with using stepwise methods, but I have about 30 or so predictors and have constructed an <code>lm</code> object.</p>

<pre><code>m1 &lt;- lm(LEADSforester ~ . , data=dat)
m2 &lt;- lm(LEADSforester ~ 1 , data=dat)
step(m1, m2, direction = ""backward"")
</code></pre>

<p>However, when I run the following line of code, I get an error message.</p>

<pre><code>backBIC &lt;- step(m1, direction=""backward"", data=dat)

Error in step(m1, direction = ""backward"") : 
  AIC is -infinity for this model, so 'step' cannot proceed
</code></pre>

<p>The same problem occurs when I run the following:</p>

<pre><code>m1 = lm(LEADS ~ ED + Fa + Pu + New + Gr + Vol + Dur + Boun + Visit + views + Nw + 
                Uniq + sits, data=dat)
step(m1, direction=""backward"")

Error in step(m1, direction = ""backward"") : 
  AIC is -infinity for this model, so 'step' cannot proceed
</code></pre>

<p>What am I doing wrong?</p>
"
"NaN","NaN"," 70209","<p>I have the following data:</p>

<pre><code>t       mean
147     1.4
143     3
137.5       1.8
133     1.9
129.5       1.8
124.5       2.5
115.5       1.9
107     2.5
102.5       6.3
98.5        6.5
94.5        5
89      5.5
81      4.8
73      9.3
</code></pre>

<p>To me, the slope looks more exponential than linear when plotted as a scatterplot. I've been using the following code in R:</p>

<pre><code>data&lt;-read.csv(""regression.csv"")
attach(data)
plot(t,mean)
data.lm&lt;-lm(mean~t,data=data)
summary(data.lm)
data.exp&lt;-lm(log(mean) ~ log(t) ,data=data)
summary (data.exp)
AIC(data.lm, k=2)
AIC(data.exp, k=2)
</code></pre>

<p>data.exp, the exponential regression, has a much lower p-value and a much lower AIC score than data.lm, the linear model: 6.869e-05 vs. 0.000194, and 11.4641 vs. 52.22926.</p>

<p>But (how) can I demonstrate that the data fits an exponential line better than a straight line? Is the use of AIC legitimate here? Sorry to ask such as simple question but I've looked online and haven't found an answer.</p>

<p>Thank you!</p>
"
"0.116161824498409","0.114332390095006"," 70227","<p>I want to model the infection rates in bees based on weather conditions. The weather variables are rolling means for different time periods and durations. Dependent data is infection levels gathered in March and the independent variables are the weather aggregates (e.g. from 30 day period from Jan1-Jan30, 90 day period from Dec1-Feb28), a few thousands of them and highly correlated.</p>

<p>PCA techniques did not work since the infections are not so strongly related to weather. I have also tried Bayesian Model Averaging and Boosted Regression Trees, since variables could be selected based on variable importance they calculate.</p>

<p>But since, my data is longitudinal and my apiaries have a fixed location, I think mixed-models are a good choice. Is there a way to do variable selection based on mixed-models?</p>

<p>What I have done now is to<br>
1. run <code>glmer</code> for each of the independent variables separately,<br>
2. remove those variable whose p-values for fixed-effect estimates are below 0.05 (not sure if this is a right thing - if the estimate for a variable is not significant, that variable being the only one in the model, it is right to drop that variable, is it?)<br>
3. from the variables that are left over, test for correlation between the variables<br>
4. remove the variables that are highly correlated, giving preference to the variable that has the lowest AIC.  </p>

<p>Or should I at this stage, not worry about p-values of Intercepts and only focus on AIC (or BIC)? since some of the variables have high p-values but AICs lower than than those with low p-values. </p>

<p>I have tried reading up a lot, and there is no one fool-proof solution for variable selection, but would like to know if there is anything inherently wrong with my method. As I am not a statistician, equations often look like beautiful Arabic calligraphy and there lies my dead-end.</p>
"
"0.0616041103633697","0.0606339062590832"," 70482","<p>I've got some ordinal variables b and a and a categorized variable c. I would like to fit a multinomial logit regression from the library car.
I tried to ignore the ordinal scale.
 I have the following data:</p>

<pre><code> a&lt;-c( 3, 4,   4,   4,   3, 4,   3, 3, 4,   2, 2, 4,   3, 3, 3, 1,   3, 2, 2, 3, 3, 1,   3, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 1,   2, 2, 2, 2, 3, 2, 3, 4,   4,   3, 3, 2, 2, 3, 3, 3, 2, 1,   1,   1,   1,   1,   1,   2, 3, 4,   3, 3, 4,   3, 4,   3, 2, 3, 3, 3, 3, 3, 4,   3, 4,   3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 1,   2, 2, 1,   1,   4,   3, 3, 2, 2, 2, 2, 2, 2, 3, 4,   4,   4,   3, 3, 3, 3, 3, 4,   4,   3, 3, 2, 3, 3, 3, 3, 4,   3, 4,   2, 2, 3, 3, 3, 2, 2, 3, 2, 4,   2, 2, 2, 2, 2, 1,   2, 2, 1,   1,   3, 4,   3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 1)


 c&lt;-c(5 ,3 ,4 ,3 ,4 ,4 ,2 ,2 ,3 ,4 ,2 ,5, 3, 5, 4 ,3 ,2 ,4 ,4 ,4, 4 ,4, 4, 2 ,3, 4 ,2 ,3 ,3 ,3 ,4 ,3 ,3 ,2 ,2 ,3 ,3 ,3 ,3 ,4 ,2 ,4 ,3, 3, 3, 4, 4, 3, 3 ,2 ,3 ,3 ,3 ,3, 4 ,4, 4, 3, 2, 2 ,4 ,4 ,3 ,3 ,2 ,2 ,1 ,2 ,2 ,2 ,1 ,2 ,5 ,2 ,3 ,3 ,2, 4 ,3 ,1 ,2 ,3 ,2 ,3 ,3 ,3 ,3 ,3 ,3 ,2 ,2 ,2 ,2 ,3 ,2 ,4 ,3, 3 ,2 ,3, 2, 4, 3, 3, 3 ,3 ,4 ,2 ,2 ,4 ,3 ,3 ,3 ,3 ,3 ,2 ,3, 3 ,3, 3, 4 ,4 ,4 ,1 ,3 ,3 ,3 ,4 ,4 ,4 ,3 ,2 ,4 ,4 ,2 ,4 ,4 ,4 ,4 ,2 ,3 ,3, 2, 2 ,3 ,2 ,3 ,4 ,5 ,2, 3 ,3 ,2 ,3 ,2 ,2 ,3 ,2 ,2 ,4 ,4 ,3 ,3 ,2, 4 ,4 ,2 ,4 ,3 ,4, 4, 3 ,2 ,3)
b&lt;-c(3 ,2, 2, 2, 3, 2, 3, 3, 4, 1, 2, 2, 4, 2, 3, 1, 3, 1, 2, 4, 2, 1, 3, 2, 2, 2, 1, 3, 3, 3, 2, 2, 2, 2, 1, 3, 1, 3, 2, 3, 1 ,3 ,3 ,2, 2, 3, 1, 3, 2, 2, 2, 2, 2, 2, 3, 4, 3, 3, 2, 1, 4, 3 ,3 ,2 ,2, 1, 2, 2, 2, 2, 1, 2, 5, 3, 3, 4, 3, 4, 1, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2 ,4 ,2 ,3 ,2 ,2 ,2 ,4 ,2 ,2 ,2 ,2 ,2 ,2, 1, 5 ,4 ,3 ,2, 2 ,2 ,2 ,2 ,4 ,2 ,2 ,4 ,3 ,3 ,1 ,2 ,2 ,2 ,2 ,2 ,2, 2, 2, 2, 2, 2 ,2 ,2 ,2 ,2 ,2 ,2, 2, 2, 2 ,2 ,2 ,2 ,2 ,5 ,4 ,3 ,2 ,1 ,1 ,1 ,4 ,3 ,2 ,2 ,3 ,3 ,3 ,2 ,2 ,2 ,2 ,2, 3, 2 ,2 ,2 ,2 ,2 ,1)
</code></pre>

<p>now I ignored the ordinal scale and treated them as factors to fit the multinomial logit regression </p>

<pre><code>require(car)
a&lt;-as.factor(a)    
b&lt;-as.factor(b)
c&lt;-as.factor(c)
multinom(formula = a ~ b + c)

Call:
multinom(formula = a ~ b + c)

Coefficients:
  (Intercept)        b2       b3       b4        b5         c2         c3         c4        c5
2   0.3410779  1.009797 41.80056 45.22081 -13.02923 -0.5229982  0.9216514  0.2170273 -18.03928
3  -1.4697131  2.698228 44.91938 47.04268 -16.24570 -0.7341395  0.7088424  1.2495310  20.70641
4 -46.0095393 33.603384 75.13911 79.00502  56.91264 -7.4198320 13.0220759 14.2526951  33.85774

Std. Errors:
  (Intercept)        b2        b3        b4           b5           c2        c3        c4           c5
2   1.2654428 0.6530052 0.4659520 0.5495402          NaN 1.337075e+00 1.4180126 1.4993079 8.028986e-16
3   1.6649206 0.9361438 0.5123106 0.5879588 2.446562e-15 1.640462e+00 1.7003411 1.7558418 8.601766e-01
4   0.3399454 0.4767032 0.3699569 0.4144527 3.321501e-11 6.973173e-08 0.6549144 0.6953767 8.601766e-01

Residual Deviance: 328.1614
AIC: 382.1614  
</code></pre>

<p>I think I found the mistake....the column b5 is empty for a1 and a2. </p>

<pre><code>table(b,c,a)
, , a = 1

   c
b    1  2  3  4  5
  1  0  3  2  2  0
  2  1  7  1  0  0
  3  0  0  0  0  0
  4  0  0  0  0  0
  5  0  0  0  0  0

, , a = 2

   c
b    1  2  3  4  5
  1  1  5  2  2  0
  2  1 12 21  4  0
  3  0  1  6  1  0
  4  0  2  1  1  0
  5  0  0  0  0  0
</code></pre>

<p>But do you know how to solve this problem?</p>
"
"0.11525073729837","0.113435651621629"," 73567","<p>I have a logistic regression model with several variables and one of those variables (called x3 in my example below) is not significant. However, x3 should remain in the model because it is scientifically important.</p>

<p>Now, x3 is continuous and I want to create a plot of the predicted probability vs x3. Even though x3 is not statistically significant, it has an effect on my outcome and therefore it has an effect on the predicted probability. This means that I can see from the graph, that the probability changes with increasing x3. However, how should I interpret the graph and the change in the predicted probability, given that x3 is indeed not statistically significant?</p>

<p>Below is a simulated data in R set to illustrate my question. The graph also contains a 95% confidence interval for the predicted probability (dashed lines):</p>

<pre><code>&gt; set.seed(314)
&gt; n &lt;- 300
&gt; x1 &lt;- rbinom(n,1,0.5)
&gt; x2 &lt;- rbinom(n,1,0.5)
&gt; x3 &lt;- rexp(n)
&gt; logit &lt;- 0.5+0.9*x1-0.5*x2
&gt; prob &lt;- exp(logit)/(1+exp(logit))
&gt; y &lt;- rbinom(n,1,prob)
&gt; 
&gt; model &lt;- glm(y~x1+x2+x3, family=""binomial"")
&gt; summary(model)

Call:
glm(formula = y ~ x1 + x2 + x3, family = ""binomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0394  -1.1254   0.5604   0.8554   1.4457  

Coefficients:
            Estimate Std. Error z value Pr(    &gt;|z|)    
(Intercept)   1.1402     0.2638   4.323 1.54e-05 ***
x1            0.8256     0.2653   3.112  0.00186 ** 
x2           -1.1338     0.2658  -4.266 1.99e-05 ***
x3           -0.1478     0.1249  -1.183  0.23681    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 373.05  on 299  degrees of freedom
Residual deviance: 341.21  on 296  degrees of freedom
AIC: 349.21

Number of Fisher Scoring iterations: 3

&gt; 
&gt; dat &lt;- data.frame(x1=1, x2=1, x3=seq(0,5,0.1))
&gt; preds &lt;- predict(model, dat,type = ""link"", se.fit = TRUE )
&gt; critval &lt;- 1.96
&gt; upr &lt;- preds$fit + (critval * preds$se.fit)
&gt; lwr &lt;- preds$fit - (critval * preds$se.fit)
&gt; fit &lt;- preds$fit
    &gt; 
    &gt; fit2 &lt;- mod$family$linkinv(fit)
    &gt; upr2 &lt;- mod$family$linkinv(upr)
    &gt; lwr2 &lt;- mod$family$linkinv(lwr)
    &gt; 
    &gt; plot(dat$x3, fit2, lwd=2, type=""l"", main=""Predicted Probability"", ylab=""Probability"", xlab=""x3"", ylim=c(0,1.00))
&gt; lines(dat$x3, upr2, lty=2)
    &gt; lines(dat$x3, lwr2, lty=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ljW7W.png"" alt=""enter image description here""></p>

<p>Thanks!</p>

<p>Emilia</p>
"
"0","0.0428746462856272"," 74304","<p>What goodness of fit tests are usually used for quantile regression? Ideally I need something similar to F-test in linear regression, but something like AIC in logistic regression will suite as well. I use quantreg R package, but found only some Khmaladze test in there. To be fair I hardly understand what is does.</p>
"
"0.0871213683738064","0.0857492925712544"," 74678","<p>I am comparing multiple published equation forms, refit with independent data.  I'm trying to be true to the original authors' methods as much as possible. Therefore, I have 3 linear equations (fit in R using lm()), two of which use transformed Y-variables, and one equation fit using nonlinear regression (fit in R using the gnls() function).</p>

<p>In all instances cases I'm weighting the residual variance structure using the inverse of one of the predictors to account for observed heteroskedasticity.</p>

<p>I have been evaluating the models using R2, and RMSE- using back-transformed data for the two models with transformations.</p>

<p>I've calculated RMSE ""by hand"" using the following equation:</p>

<pre><code> RMSE&lt;-sqrt(sum(residuals(Equation)^2)/length(residuals(Equation))-2))
</code></pre>

<p>Should I use similar code to calculate RMSE for the linear and nonlinear regression models?  Is the metric still a valid statistic for comparison, or am I missing some important assumption?  </p>

<p>Edited: I initially stated that I was also comparing models using AIC; I later recalled that AIC would not be appropriate if the Y-variables were transformed because the models would be estimating different things.</p>
"
"0.0754493182241785","0.0742610657232506"," 78284","<p>I loaded the data to R and fitted it using GLM function.</p>

<blockquote>
  <p>fit.glm = glm(y~ aX+bZ+cW)</p>
</blockquote>

<p>Then, I found the cuttinf-point using ""segmented"" tool of R.</p>

<blockquote>
  <p>o&lt;-segmented(fit.glm,seg.Z=~X,psi=10)</p>
</blockquote>

<p>Now I have the cut-point and two different slope of X. </p>

<blockquote>
  <p>Call: segmented.glm(obj = fit.glm1, seg.Z = ~PTH, psi = 10)</p>
  
  <p>Meaningful coefficients of the linear terms: (Intercept)          X   Z       W</p>
  
  <p>19.43840           2.29574           0.08701           8.75784      </p>
  
  <p>Estimated Break-Point(s) psi1.PTH : 8.5 </p>
  
  <p>Degrees of Freedom: 324 Total (i.e. Null);  314 Residual Null
  Deviance:     17320  Residual Deviance: 7645      AIC: 1971</p>
</blockquote>

<p>However, I'm trying to get estimated y value of two linear regression models using the result of 'segmented'.</p>

<p>Is it possible using R, or I have to substitute Z and W with mean values of Z &amp; W, and calculate the y value myself?</p>
"
"0.0616041103633697","0.0606339062590832"," 82346","<p>I just did a binary linear regression in R with a dataset that has 100000 lines. The output of the regression is, that almost every parameter is highly significant. I wouldn't expect that when I look at the <a href=""http://picpaste.com/yGsn701X.png"" rel=""nofollow"">boxplots</a>. Did I do something wrong in my code or can that be right?</p>

<pre><code>Call:
glm(formula = damage ~ dist_gerst + dist_gew + dist_hunt + dist_kart + 
    dist_mais + dist_raps + dist_road1 + dist_road2 + dist_road3 + 
    dist_road4 + dist_roada + dist_rog + dist_rmr + dist_ruben + 
    dist_sg + dist_wald + dist_hecke + dist_weize + dist_wg + 
    dist_bra, family = binomial(logit), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.3963  -0.0024   0.2446   0.4947   5.1474  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -9.7073574  0.5951280 -16.311  &lt; 2e-16 ***
dist_gerst   0.0355374  0.0053699   6.618 3.65e-11 ***
dist_gew     0.0033584  0.0012147   2.765 0.005698 ** 
dist_hunt    0.0531545  0.0017567  30.259  &lt; 2e-16 ***
dist_kart    0.0472300  0.0022333  21.148  &lt; 2e-16 ***
dist_mais    0.0578135  0.0031780  18.192  &lt; 2e-16 ***
dist_raps    0.0470257  0.0021689  21.682  &lt; 2e-16 ***
dist_road1  -0.0135003  0.0023328  -5.787 7.15e-09 ***
dist_road2   0.0304884  0.0016027  19.023  &lt; 2e-16 ***
dist_road3  -0.0003806  0.0011631  -0.327 0.743505    
dist_road4  -0.0515227  0.0048316 -10.664  &lt; 2e-16 ***
dist_roada  -0.0186244  0.0050640  -3.678 0.000235 ***
dist_rog    -0.0403263  0.0031825 -12.671  &lt; 2e-16 ***
dist_rmr    -0.1133255  0.0045872 -24.705  &lt; 2e-16 ***
dist_ruben   0.1168154  0.0032703  35.721  &lt; 2e-16 ***
dist_sg     -0.0450639  0.0020300 -22.199  &lt; 2e-16 ***
dist_wald    0.1127090  0.0035169  32.047  &lt; 2e-16 ***
dist_hecke   0.1065537  0.0028434  37.474  &lt; 2e-16 ***
dist_weize  -0.1496686  0.0038303 -39.075  &lt; 2e-16 ***
dist_wg     -0.0937316  0.0051061 -18.357  &lt; 2e-16 ***
dist_bra    -0.0599667  0.0023916 -25.074  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 64559  on 53510  degrees of freedom
Residual deviance: 32357  on 53490  degrees of freedom
  (46231 observations deleted due to missingness)
AIC: 32399

Number of Fisher Scoring iterations: 9
</code></pre>
"
"0.0871213683738064","0.0643119694284408"," 83133","<p>So, I have a homework assignment in which I'm being asked to compare the fit of two similar models by comparing their $R^2$ and AIC.  Both models were run in R, one using the <code>lm</code> command (for OLS) and the other the <code>glm</code> command; the former yielded an adjusted $R^2$ of 0.82, and the second model, with the same DV and covariates, produced an AIC of 365.96.</p>

<p>The only difference between the models is the ""g"" in the <code>lm</code>/<code>glm</code> command. The coefficients and standard errors they returned are virtually identical.</p>

<p>How does one compare $R^2$ and AIC? How can I tell which regression model fits the data better?</p>
"
"0.0987863462557455","0.113435651621629"," 86952","<p>I'm trying to regress some simple pooled data. My data has 60 observations and three columns: Weight, Height, and Sex (female=1, male=0).</p>

<p>If I regress thus, Weight ~ Height + Sex, my model is fairly satisfactory, but the residuals are not homoscedastic (green errors are male, blue female):</p>

<p><img src=""http://i1267.photobucket.com/albums/jj541/nbahmanyar/Rplot_zps69001b34.png"" alt=""plot""></p>

<p>I tried regressing on the log of Weight and/or Height, but that didn't do much. What should I do to make the residuals homescedastic and/or make my model more accurate? Any help would be appreciated.</p>

<p><strong>Edit</strong></p>

<p>Doing a generalized regression model gives the following.</p>

<pre><code>Generalized least squares fit by REML
  Model: Weight ~ h + s 
  Data: P149 
       AIC      BIC    logLik
  514.2221 524.4374 -252.1111

Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | Sex 
 Parameter estimates:
        0         1 
1.0000000 0.6685307 

Coefficients:
                 Value Std.Error   t-value p-value
(Intercept)  27.197499  51.88129  0.524226  0.6022
h             1.852382   0.75634  2.449128  0.0174
s           -25.284478   5.53300 -4.569755  0.0000

 Correlation: 
  (Intr) h     
h -0.997       
s -0.524  0.466

Standardized residuals:
       Min         Q1        Med         Q3        Max 
-1.6655243 -0.6879858 -0.1839396  0.5628971  3.9857544 

Residual standard error: 22.13369 
Degrees of freedom: 60 total; 57 residual
</code></pre>

<p>With this s. residual plot:</p>

<p><img src=""http://i1267.photobucket.com/albums/jj541/nbahmanyar/Rplot1_zps5ee264a0.png"" alt=""""></p>

<p>Could someone please explain how precisely this model is different from a standard multiple regression model? Thanks.</p>
"
"NaN","NaN"," 87345","<p>I have tried calculating the AIC of a linear regression in R but without using the <code>AIC</code> function, like this:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ drat, mtcars)

nrow(mtcars)*(log((sum(lm_mtcars$residuals^2)/nrow(mtcars))))+(length(lm_mtcars$coefficients)*2)
[1] 97.98786
</code></pre>

<p>However, <code>AIC</code> gives a different value:</p>

<pre><code>AIC(lm_mtcars)
[1] 190.7999
</code></pre>

<p>Could somebody tell me what I'm doing wrong?</p>
"
"0.0974046509883157","0.0958706236059213"," 87530","<p>I have fitted a Poisson regression to my claim frequency.</p>

<p>I have obtained the following result:</p>

<pre><code>              Estimate   Std. Error  z value   Pr(&gt;|z|) 
(Intercept)  -19.95861   1139.33678   -0.018     0.9860 
make          -0.10534      0.04116   -2.559     0.0105 *
agevehO        0.05983      0.08580    0.697     0.4856
area1         20.68177   1139.33677    0.018     0.9855 
area2         20.85866   1139.33677    0.018     0.9854 
area3         20.76927   1139.33676    0.018     0.9855  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Null deviance:        657.49 on 583 degrees of freedom
Residual deviance: 150.62 on 578 degrees of freedom
AIC: 1096.
</code></pre>

<p>My predictors area i have four levels (area 1,2,3,4) and agveh two (old and new), however for make i have four levels (make 1,2,3,4), how come it is not showing the three levels?? i am confused on how to interprete this result? Also my deviance table showed the following :</p>

<pre><code>             DF   DEVIANCERESID DF RESIDDEV PR(&gt;CHI)
make         1        13.61    582   643.88 0.0002251 ***
ageveh       1         9.80    581   634.08 0.0017460 ** 
area         3       483.47    578   150.62 &lt; 2.2e-16 ***
</code></pre>

<p>Bases on this can i conclude that make ageveh and area are statistically significant in explaining my claim frequency? Thanks</p>
"
"0.144474445059126","0.14219911474863"," 87608","<p>I have run a few tests/methods on my data and am getting contradictory results.</p>

<p>I have a linear model saying:
reg1 = lm(weight = height + age + gender (categorical) + several other variables). </p>

<p>If I model each term linearly i.e. no squared or interaction term, and run vif(reg1), 4 variables are >15. If I delete the variable with the highest vif number and re-run it the gifs change and now only 2 variables are >15. I repeat this until I'm left with 20 variables (out of 30) below 10. If I use stepwise directly on reg1 then it does not delete the 'highest vic' factor. <strong>I don't understand how it tells me 'what' is linearly dependant on 'what variable' and how (and I cannot seem to find this information despite googling for ages).</strong> </p>

<p>Furthermore, when I look at the residual plots, most appear horizontal except a few which are upside down u curved (none of these have high vifs). Does this means a transformation is needed? (I removed outliers, leverage points etc - but now there seem to be more!)</p>

<p>reg2 = lm(weight = (height + age + gender (categorical) + several other variables)^2). </p>

<p>If I run vif on this all of the terms are >500! </p>

<p>What else I have tried (without cutting any variables): 
(1) The errors seem correlated when i run diagnostics and check with Durbin Waston statistics indicating the model is not linear... however...
(2) Box Cox gives lambda = 1 so no transformation is needed.
(3) LASSO gives the lowest mallows cp on the full 30 variable model (i.e. least squares)
(4) Ridge regression gives lambda = 0 which did surprise me. </p>

<p>I'm getting really confused about this data. <strong>To determine a suitable model for weight should I be looking just at linear terms or linear and interaction terms (remember there are 25 variables so there are 30^2 interaction terms)?</strong> </p>

<p>When I check which ones are significant in reg2 only 12 predictors and 6 interaction terms seem significant (AIC is lowest with this combination after I run step). <strong>Should I just use this 'new model with deleted variables/interaction terms' and do all my tests e.g. stepwise method, LASSO etc or do I do it on the entire model?</strong> </p>

<p>I'm getting quite lost in terms of making sense of steps to find a suitable model for weight using the variables. </p>

<p><strong>My final question is once I have the model - how do i test/prove its the best/a decent model?</strong> </p>

<p>Any help would really be appreciated. </p>
"
"0.131340404599205","0.14219911474863"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.0435606841869032","0.0428746462856272"," 88722","<p>I am building a regression model of time series data in R, where my primary interest is the coefficients of the independent variables. The data exhibit strong seasonality with a trend.</p>

<p><img src=""http://i.stack.imgur.com/GYxaU.png"" alt=""Original data""></p>

<p>The model looks good, with four of the six regressors significant:
<img src=""http://i.stack.imgur.com/ZmoSd.png"" alt=""Model""></p>

<p>Here are the OLS residuals:
<img src=""http://i.stack.imgur.com/EIybo.png"" alt=""Residuals""></p>

<p>I used auto.arima to select the sARIMA structure, and it returns the model (0,1,1)(1,1,0)[12].</p>

<pre><code>fit.ar &lt;- auto.arima(at.ts, xreg = xreg1, stepwise=FALSE, approximation=FALSE)
summary(fit.ar)

Series: at.ts 
ARIMA(0,1,1)(1,1,0)[12]                    

Coefficients:
          ma1    sar1      v1       v2      v3       v4         v5
      -0.7058  0.3974  0.0342  -0.0160  0.0349  -0.0042  -113.4196
s.e.   0.1298  0.2043  0.0239   0.0567  0.0555   0.0333   117.1205

sigma^2 estimated as 3.86e+10:  log likelihood=-458.13
AIC=932.26   AICc=936.05   BIC=947.06

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 7906.896 147920.3 103060.4 0.1590107 3.048322 0.1150526
</code></pre>

<p>My question is this: based on the parameter estimates and s.e. of the regressors, I believe that none of them are significant - is this correct, and if so, what does it imply if my goal is to interpret the relative importance of these predictors as opposed to forecasting?</p>

<p>Any other advice relative to the process of building this model is welcome and appreciated.</p>

<p>Here are the ACF and PACF for the residuals:</p>

<p><img src=""http://i.stack.imgur.com/a3Gvy.png"" alt=""ACF-PACF""></p>

<pre><code>&gt; durbinWatsonTest(mod.ols, max.lag=12)
 lag Autocorrelation D-W Statistic p-value
   1     0.120522674     1.6705144   0.106
   2     0.212723044     1.4816530   0.024
   3     0.159828108     1.5814771   0.114
   4     0.031083831     1.8352377   0.744
   5     0.081081308     1.6787808   0.418
   6    -0.024202465     1.8587561   0.954
   7    -0.008399949     1.7720761   0.944
   8     0.040751905     1.6022835   0.512
   9     0.129788310     1.4214391   0.178
  10    -0.015442379     1.6611922   0.822
  11     0.004506292     1.6133994   0.770
  12     0.376037337     0.7191359   0.000
 Alternative hypothesis: rho[lag] != 0
</code></pre>
"
"0.106701449104437","0.105021006302101"," 89474","<p>I ran this ordinal logistic regression in R:</p>

<pre><code>mtcars_ordinal &lt;- polr(as.factor(carb) ~ mpg, mtcars)
</code></pre>

<p>I got this summary of the model:</p>

<pre><code>summary(mtcars_ordinal)

Re-fitting to get Hessian

Call:
polr(formula = as.factor(carb) ~ mpg, data = mtcars)

Coefficients:
      Value Std. Error t value
mpg -0.2335    0.06855  -3.406

Intercepts:
    Value   Std. Error t value
1|2 -6.4706  1.6443    -3.9352
2|3 -4.4158  1.3634    -3.2388
3|4 -3.8508  1.3087    -2.9425
4|6 -1.2829  1.3254    -0.9679
6|8 -0.5544  1.5018    -0.3692

Residual Deviance: 81.36633 
AIC: 93.36633 
</code></pre>

<p>I can get the log odds of the coefficient for <code>mpg</code> like this:</p>

<pre><code>exp(coef(mtcars_ordinal))
 mpg 
0.7917679 
</code></pre>

<p>And the the log odds of the thresholds like:</p>

<pre><code>exp(mtcars_ordinal$zeta)

       1|2         2|3         3|4         4|6         6|8 
0.001548286 0.012084834 0.021262900 0.277242397 0.574406353 
</code></pre>

<p>Could someone tell me if my interpretation of this model is correct:</p>

<blockquote>
  <p>As <code>mpg</code> increases by one unit, the odds of moving from category 1 of <code>carb</code> into any of the other 5 categories, decreases by -0.23. If the log odds crosses the threshold of 0.0015, then the predicted value for a car will be category 2 of <code>carb</code>.  If the log odds crosses the threshold of 0.0121, then the predicted value for a car will be category 3 of <code>carb</code>, and so on.</p>
</blockquote>
"
"0.137750978465894","0.13558153613666"," 89692","<p>My data has 3 major inputs: <code>BLDDAY</code> (a factor), <code>BLDMNT</code> (a factor), and <code>D_BLD_SER</code> (days as an integer variable).  The output is whether input variable has any impact on failure.  My model is: <code>model = glm(FAILED~BLDDAY+BLDMNT+D_BLD_SER, family=""binomial"", data=data_list)</code>.  (I used <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">UCLA's statistics help site's guide to logistic regression in R</a> to build this model.)  </p>

<p>Output: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3282  -0.9123  -0.8128   1.4056   2.1053  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -0.7672583  0.1317132  -5.825 5.70e-09 ***
BLDDAYMonday    -0.1545646  0.0839380  -1.841  0.06556 .  
BLDDAYSaturday  -0.1257976  0.2028259  -0.620  0.53511    
BLDDAYSunday    -0.1183008  0.1868713  -0.633  0.52669    
BLDDAYThursday  -0.2007452  0.0772653  -2.598  0.00937 ** 
BLDDAYTuesday    0.0480453  0.0758603   0.633  0.52651    
BLDDAYWednesday -0.0358585  0.0760027  -0.472  0.63707    
BLDMNTAug        0.3009445  0.1405545   2.141  0.03226 *  
BLDMNTDec        0.5562170  0.1338467   4.156 3.24e-05 ***
BLDMNTFeb        0.3334978  0.2133475   1.563  0.11801    
BLDMNTJan        0.4076504  0.2277978   1.790  0.07353 .  
BLDMNTJul        0.1306585  0.1415302   0.923  0.35591    
BLDMNTJun       -0.0357361  0.1428105  -0.250  0.80241    
BLDMNTMar        0.4570491  0.1949815   2.344  0.01907 *  
BLDMNTMay       -0.2292620  0.1614577  -1.420  0.15562    
BLDMNTNov        0.3060012  0.1334034   2.294  0.02180 *  
BLDMNTOct        0.2390501  0.1341877   1.781  0.07484 .  
BLDMNTSep        0.2481405  0.1384901   1.792  0.07317 .  
D_BLD_SER       -0.0020960  0.0003367  -6.225 4.82e-10 ***

(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 10288  on 8182  degrees of freedom
Residual deviance: 10154  on 8164  degrees of freedom
AIC: 10192
Number of Fisher Scoring iterations: 4
</code></pre>

<p>The ANOVA table is the following:</p>

<pre><code>anova(model, test=""Chisq"")
Analysis of Deviance Table
Model: binomial, link: logit
Response: FAILED
Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                       8182      10288              
BLDDAY     6   20.392      8176      10268  0.002357 ** 
BLDMNT    11   70.662      8165      10197 9.142e-11 ***
D_BLD_SER  1   43.797      8164      10154 3.642e-11 ***
</code></pre>

<p>My questions are:</p>

<ol>
<li><p>Although the p-values for all three components are less than 0.05, which can be considered as significant, the deviance reduced due to each component is less than 1% of the total deviance. <strong>Normally the interpretation of output like this is input parameter affects output and it's better to use this parameter then using noting.</strong> But does it really make sense of taking this parameter as significant input?</p></li>
<li><p>The p-values for <code>BLDDAY</code> and <code>BLDMNT</code> given by <code>anova()</code> is the overall p-value,  which is significant, but <code>summary()</code> gives detailed impact of each factor level. If I consider the p-values for each factor overall <code>BLDDAY</code> is significant but individually only <code>BLDDAYThursday</code> is significant. I am bit confused not as whether to consider <code>BLDDAY</code> as significant input, or Thursday only, or Thursday &amp; Friday both.</p></li>
</ol>
"
"0.138323750077661","0.148522131446501"," 89714","<p>I have ran a negative binomial regression. I'm guessing the use of a negative binomial regression is not ideal given my design, but I'm hoping I can 'get away with it', as it seems to be working fairly well. </p>

<p>Here's my design:</p>

<ul>
<li><p>Count the the total number of geese in a flock</p></li>
<li><p>Count the number of these that are vigilant.  </p></li>
<li><p>Then divide number vigilant/number in    flock, to get a percentage
that are vigilant  </p></li>
<li>Predictor variables are    flock size (count),    water height in
metres, date, subspecific identity of the    flock and
the part of my study site that    the flock were in (ssp.zone)</li>
</ul>

<p>Below is a plot of my response variable (<code>alert.integer</code>)</p>

<p><img src=""http://i.stack.imgur.com/RbEJU.jpg"" alt=""enter image description here""></p>

<p>Below is a summary of the model</p>

<pre><code>Call:
glm.nb(formula = alert.integer ~ ssp * water.height + ssp * ssp.zone + 
    ssp * count + ssp * I(as.Date(datetime)), data = dataScanSampling_sub, 
    init.theta = 1.168496598, link = log)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.0035  -0.9556  -0.3382   0.3886   2.0942  

Coefficients:
                                                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                                        3.971e+02  6.239e+01   6.365 1.95e-10 ***
sspLight-bellied Brent Goose                      -4.369e+02  7.294e+01  -5.990 2.10e-09 ***
water.height                                       2.978e-01  8.719e-02   3.415 0.000637 ***
ssp.zonelight zone                                -1.214e+00  2.103e-01  -5.775 7.70e-09 ***
count                                             -5.226e-03  8.506e-04  -6.143 8.09e-10 ***
I(as.Date(datetime))                              -2.461e-02  3.904e-03  -6.303 2.93e-10 ***
sspLight-bellied Brent Goose:water.height          2.743e-03  1.015e-01   0.027 0.978438    
sspLight-bellied Brent Goose:ssp.zonelight zone    1.344e+00  2.635e-01   5.102 3.36e-07 ***
sspLight-bellied Brent Goose:count                 4.174e-03  8.766e-04   4.761 1.93e-06 ***
sspLight-bellied Brent Goose:I(as.Date(datetime))  2.726e-02  4.564e-03   5.973 2.33e-09 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for Negative Binomial(1.1685) family taken to be 1)

    Null deviance: 558.21  on 367  degrees of freedom
Residual deviance: 433.87  on 358  degrees of freedom
AIC: 3236.4

Number of Fisher Scoring iterations: 1


              Theta:  1.1685 
          Std. Err.:  0.0894 

 2 x log-likelihood:  -3214.4300 
</code></pre>

<p>And here are residuals from model:</p>

<p><img src=""http://i.stack.imgur.com/hnBEW.jpg"" alt=""enter image description here""></p>

<p>So taking into account the description of my design, the plot of the response variable (<code>alert.integer</code>), the residuals plot and the output from <code>summary</code>(), can I get away with a negative binomial regression model here? </p>
"
"0.0754493182241785","0.0742610657232506"," 89886","<p>I'm trying to produce a linear regression model, but I only have 25 observations and 34 predictors.</p>

<p>I'm trying feature selection,</p>

<pre><code>library(MASS)
full.m &lt;- lm(fmla, data=mydata)
fsel.m &lt;- step(full.m, direction = ""both"")
</code></pre>

<p>but I get this error,</p>

<pre><code>Error in step(full.m, direction = ""both"") : 
  AIC is -infinity for this model, so 'step' cannot proceed
</code></pre>

<p>Well, then I tried PCA and factor analysis but I get this weird image from PCA.</p>

<pre><code>r &lt;- prcomp(formula=pca.fmla, mydata, scale=FALSE)
</code></pre>

<p><img src=""http://i.stack.imgur.com/gg45b.png"" alt=""enter image description here""></p>

<p>In factor analysis,</p>

<pre><code>f &lt;- factanal(x=pca.fmla, mydata, factors=2)
</code></pre>

<p>I get,</p>

<pre><code>Error in solve.default(cv) : 
  system is computationally singular: reciprocal condition number = 1.09137e-18
</code></pre>

<p>Any help please?</p>

<p>EDIT: I can answer this.</p>
"
"0.195030550545354","0.201099916634961"," 90503","<p>I have zero inflated response variable I am trying to predict. I am facing few issues applying different regression models that should correct for this.</p>

<p>This is my 10,000 obs dataframe</p>

<pre><code>    e_weight       left_size         right_size        time_diff        
 Min.   :0.000   Min.   :  1.000   Min.   :  1.000   Min.   :      737  
 1st Qu.:0.000   1st Qu.:  1.000   1st Qu.:  1.000   1st Qu.:  4669275  
 Median :0.000   Median :  3.000   Median :  3.000   Median : 12263474  
 Mean   :0.022   Mean   :  6.194   Mean   :  5.469   Mean   : 21000288  
 3rd Qu.:0.000   3rd Qu.:  5.000   3rd Qu.:  5.000   3rd Qu.: 25420278  
 Max.   :3.000   Max.   :792.000   Max.   :792.000   Max.   :155291532
</code></pre>

<p>Here the frequency count for my 3 variables
<img src=""http://i.stack.imgur.com/1yIvx.jpg"" alt=""enter image description here"">
Indeed I have a problem with zeros...</p>

<p>I tried respectively a Zero-Inflated Negative Binomial Regression and a Zero-inflated Poisson Regression</p>

<pre><code>library(pscl)
m1 &lt;- zeroinfl(e_weight ~ left_size*right_size | time_diff, data = s)
summary(m1)

# Call:
# zeroinfl(formula = e_weight ~ left_size * right_size | time_diff, data = s)
#
# Pearson residuals:
#     Min      1Q  Median      3Q     Max 
# -1.4286 -0.1460 -0.1449 -0.1444 19.6054 
#
# Count model coefficients (poisson with log link):
#                        Estimate Std. Error z value Pr(&gt;|z|)    
#  (Intercept)          -3.8826386  0.0696970 -55.707  &lt; 2e-16 ***
#  left_size             0.0022261  0.0006195   3.594 0.000326 ***
#  right_size            0.0033622         NA      NA       NA    
#  left_size:right_size  0.0001715         NA      NA       NA    
# 
# Zero-inflation model coefficients (binomial with logit link):
#               Estimate Std. Error  z value Pr(&gt;|z|)    
# (Intercept)  1.753e+01  6.011e+00    2.916  0.00354 ** 
#  time_diff   -3.342e-04  1.059e-06 -315.773  &lt; 2e-16 ***
#  ---
#  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
#
# Number of iterations in BFGS optimization: 28 
#  Log-likelihood: -1053 on 6 Df
#  Warning message:
#  In sqrt(diag(object$vcov)) : NaNs produced
</code></pre>

<p>and </p>

<pre><code>library(MASS)
m2 &lt;- glm.nb(e_weight ~ left_size*right_size + time_diff, data = s) 
</code></pre>

<p>which gives</p>

<pre><code>There were 22 warnings (use warnings() to see them)
warnings()
Warning messages:
1: glm.fit: algorithm did not converge
...
21: glm.fit: algorithm did not converge
22: In glm.nb(e_weight ~ left_size * right_size + time_diff,  ... :
  alternation limit reached
</code></pre>

<p>If  I ask a summary for the second model </p>

<pre><code>summary(m2)

# Call:
# glm.nb(formula = e_weight ~ left_size * right_size + time_diff, 
#     data = s, init.theta = 0.1372733321, link = log)
#
# Deviance Residuals: 
#     Min       1Q   Median       3Q      Max  
# -3.4645  -0.2331  -0.1885  -0.1266   2.7669  
# 
# Coefficients:
#                        Estimate Std. Error z value Pr(&gt;|z|)    
# (Intercept)          -3.239e+00  1.090e-01 -29.699  &lt; 2e-16 ***
# left_size            -4.462e-03  1.835e-03  -2.431 0.015047 *  
# right_size           -7.144e-03  2.118e-03  -3.374 0.000742 ***
# time_diff            -6.013e-08  8.584e-09  -7.005 2.48e-12 ***
# left_size:right_size  4.691e-03  2.749e-04  17.068  &lt; 2e-16 ***
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
#
# (Dispersion parameter for Negative Binomial(0.1374) family taken to be 1)
# 
#     Null deviance: 1106.5  on 9999  degrees of freedom
# Residual deviance:  958.5  on 9995  degrees of freedom
# AIC: 1967.2
# 
# Number of Fisher Scoring iterations: 12
# 
# 
#              Theta:  0.1373 
#          Std. Err.:  0.0223 
# Warning while fitting theta: alternation limit reached 
#
#
# 2 x log-likelihood:  -1955.2260
</code></pre>

<p>Also both models have very low p-values for heteroskedasticity </p>

<pre><code>bptest(m1)
# 
#   studentized Breusch-Pagan test
#
# data:  m1
# BP = 244.832, df = 3, p-value &lt; 2.2e-16
#
bptest(m2)
# 
#   studentized Breusch-Pagan test
#
# data:  m2
# BP = 277.2589, df = 4, p-value &lt; 2.2e-16
</code></pre>

<p>How should I approach this regression. Would make sense to simply add 1 to all my dataframe before running any regression?</p>
"
"0.0616041103633697","0.0606339062590832"," 90532","<p>I work at a software company, and we are trying to build a statistical model to predict the defect density (the number of defects that exist in a certain software release that is delivered to a customer divided by the number of person months that it took to develop the software release). </p>

<p>We have 15 independent variables. The dependent variable seems to follow a Poisson distribution, but it is continuous. A large part of the data is in the neighborhood of 0. We have 284 observations.</p>

<p>I tried to run a regression tree. With <code>rpart</code> I got only a root. With <code>ctree</code> I arrived at a more detailed tree. Do you know what the reason for this might be? How can I compare them?</p>

<p>I also tried GLM with quasi-Poisson. I am not sure that this appropriate because the dependent variable is continuous. Do you think that this model is appropriate for use? If so, how can I estimate it (there is no AIC in the R output)?
Other suggestions are also welcome.
Thanks!</p>
"
"NaN","NaN"," 90947","<p><code>FullModel&lt;- (lm(Fubar~.-Foo-Bar,data=BarFoo))
NullModel&lt;-(lm(Fubar~1))
step(NullModel,scope=formula(FullModel),direction=""forward"",k=log(nrow(BarFoo)))</code></p>

<p>When doing the above forward stepwise regression, the forward steps halt before certain variables are added in. does this mean that they do not improve the AIC score or does it mean that the inputs are in error?</p>
"
"0.144474445059126","0.14219911474863"," 93392","<p>First of all, sorry i am new about this and any helps are really welcome.</p>

<p>I am reading a reaserch paper where the authors report: <em>Stepwise forward regression (Zar 1996) was used to select the most informative variables, which were included in a multiple (linear) regression model. A 5% significance level was chosen as a threshold for the inclusion of the model variables.</em></p>

<p>with a private email the first author told me that the variable selection was performed using stepAIC of MASS library using direction ""forward"" and they considered only for the final model the variables with a significance level of &lt; 5%.</p>

<p>using junk data i tried to rewrite the analysis in order to understand the procedure</p>

<pre><code>state.x77
st = as.data.frame(state.x77) str(st) colnames(st)[4] = ""Life.Exp""
colnames(st)[6] = ""HS.Grad"" st[,9] = st$Population * 1000 / st$Area colnames(st)[9] = ""Density""
str(st) model1 = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + + HS.Grad + Frost + Area + Density, data=st)
model1.stepAIC &lt;- stepAIC(model1, direction=c(""both""))
summary(model1.stepAIC)


Call:
lm(formula = Life.Exp ~ Population + Murder + HS.Grad + Frost, 
    data = st)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.47095 -0.53464 -0.03701  0.57621  1.50683 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  7.103e+01  9.529e-01  74.542  &lt; 2e-16 ***
Population   5.014e-05  2.512e-05   1.996  0.05201 .  
Murder      -3.001e-01  3.661e-02  -8.199 1.77e-10 ***
HS.Grad      4.658e-02  1.483e-02   3.142  0.00297 ** 
Frost       -5.943e-03  2.421e-03  -2.455  0.01802 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7197 on 45 degrees of freedom
Multiple R-squared:  0.736,     Adjusted R-squared:  0.7126 
F-statistic: 31.37 on 4 and 45 DF,  p-value: 1.696e-12
</code></pre>

<p>followint the protocol of the paper the final model is </p>

<pre><code>Life.Exp ~ Murder + HS.Grad + Frost (final model) 
</code></pre>

<p>because Population is > 0.05.</p>

<p>I wish to know if this final model approach is correct, and then:</p>

<pre><code>fmodel = lm(Life.Exp ~ Murder + HS.Grad + Frost, data=st)
summary(fmodel)

Call:
lm(formula = Life.Exp ~ Murder + HS.Grad + Frost, data = st)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.5015 -0.5391  0.1014  0.5921  1.2268 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 71.036379   0.983262  72.246  &lt; 2e-16 ***
Murder      -0.283065   0.036731  -7.706 8.04e-10 ***
HS.Grad      0.049949   0.015201   3.286  0.00195 ** 
Frost       -0.006912   0.002447  -2.824  0.00699 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7427 on 46 degrees of freedom
Multiple R-squared:  0.7127,    Adjusted R-squared:  0.6939 
F-statistic: 38.03 on 3 and 46 DF,  p-value: 1.634e-12
</code></pre>
"
"0.13068205256071","0.128623938856882"," 93417","<p>This question is a prolongation of this question: <a href=""http://stats.stackexchange.com/questions/11096/how-to-interpret-coefficients-in-a-poisson-regression"">How to interpret coefficients in a Poisson regression?</a></p>

<p>If we follow the (almost) exact same routine, but we add correlation between the variablese treatment and improved (just for the sake of my question, which is interpreting the output), we get:</p>

<pre><code>treatment     &lt;- factor(rep(c(1, 2), c(43, 41)), 
                        levels = c(1, 2),
                        labels = c(""placebo"", ""treated""))
improved      &lt;- factor(rep(c(1, 2, 3, 1, 2, 3), c(29, 7, 7, 13, 7, 21)),
                        levels = c(1, 2, 3),
                        labels = c(""none"", ""some"", ""marked""))    
numberofdrugs &lt;- rpois(84, 10) + 1    
healthvalue   &lt;- rpois(84, 5)   
y             &lt;- data.frame(healthvalue, numberofdrugs, treatment, improved)
test          &lt;- glm(healthvalue~numberofdrugs+treatment+improved + treatment:improved, y, family=poisson)
summary(test)
</code></pre>

<p>Note the $\textbf{ treatment:improved}$ term I added inside the glm function. </p>

<p>Now, we get the following output:</p>

<pre><code>    Call:
glm(formula = healthvalue ~ numberofdrugs + treatment + improved + 
    treatment:improved, family = poisson, data = y)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.9261  -0.8733  -0.0296   0.5473   2.3358  

Coefficients:
                                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      1.553051   0.184229   8.430   &lt;2e-16 ***
numberofdrugs                    0.004298   0.014242   0.302   0.7628    
treatmenttreated                 0.007399   0.149440   0.050   0.9605    
improvedsome                     0.358897   0.164891   2.177   0.0295 *  
improvedmarked                  -0.178360   0.203756  -0.875   0.3814    
treatmenttreated:improvedsome   -0.330336   0.265310  -1.245   0.2131    
treatmenttreated:improvedmarked  0.050617   0.260203   0.195   0.8458    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 97.805  on 83  degrees of freedom
Residual deviance: 89.276  on 77  degrees of freedom
AIC: 383.29

Number of Fisher Scoring iterations: 5
</code></pre>

<p>If we ignore what seems to be insignificant coefficients, I can ask my question:</p>

<p>I understand that, as in the original post, treatment=placebo and improved=none is the base level for those variables, and thus are set to zero. My question is, why does it not exist any interaction terms with the base lavels for treatment=placebo and improved=none?</p>

<p>I thought setting the base levels to zero was just a construct, and in my mind there should still exist correlation between them...(?)</p>
"
"0.157060280430173","0.154586735600211"," 94581","<p>I have a ordinal dependendent variable, easiness, that ranges from 1 (not easy) to 5 (very easy).  Increases in the values of the independent factors are associated with an increased easiness rating.</p>

<p>Two of my independent variables (<code>condA</code> and <code>condB</code>) are categorical, each with 2 levels, and 2 (<code>abilityA</code>, <code>abilityB</code>) are continuous.</p>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/ordinal/index.html"">ordinal</a> package in R, where it uses what I believe to be</p>

<p>$$\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y &gt; g)} = \beta_{0_g} - (\beta_{1} X_{1} + \dots + \beta_{p} X_{p}) \quad(g = 1, \ldots, k-1)$$<br>
(from @caracal's answer <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">here</a>)</p>

<p>I've been learning this independently and would appreciate any help possible as I'm still struggling with it.  In addition to the tutorials accompanying the ordinal package, I've also found the following to be helpful: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">Interpretation of ordinal logistic regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">Negative coefficient in ordered logistic regression</a></li>
</ul>

<p>But I'm trying to interpret the results, and put the different resources together and am getting stuck. </p>

<ol>
<li><p>I've read many different explanations, both abstract and applied, but am still having a hard time wrapping my mind around what it means to say: </p>

<blockquote>
  <p>With a 1 unit increase in condB (i.e., changing from one level to the next of the categorical predictor), the predicted odds of observing Y = 5 versus Y = 1 to 4 (as well as the predicted odds of observed Y = 4 versus Y = 1 to 3) change by a factor of exp(beta) which, for diagram, is exp(0.457) = 1.58. </p>
</blockquote>

<p>a. Is this different for the categorical vs. continuous independent variables?<br>
b. Part of my difficulty may be with the cumulative odds idea and those comparisons. ... Is it fair to say that going from condA = absent (reference level) to condA = present is 1.58 times more likely to be rated at a higher level of easiness?  I'm pretty sure that is NOT correct, but I'm not sure how to correctly state it.</p></li>
</ol>

<p>Graphically,<br>
1. Implementing the code in <a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">this post</a>, I'm confused as to why the resulting 'probability' values are so large.<br>
2. The graph of p (Y = g) in <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">this post</a> makes the most sense to me ... with an interpretation of the probability of observing a particular category of Y at a particular value of X.  The reason I am trying to get the graph in the first place is to get a better understanding of the results overall.</p>

<p>Here's the output from my model:</p>

<pre><code>m1c2 &lt;- clmm (easiness ~ condA + condB + abilityA + abilityB + (1|content) + (1|ID), 
              data = d, na.action = na.omit)
summary(m1c2)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: 
easiness ~ illus2 + dx2 + abilEM_obli + valueEM_obli + (1 | content) +  (1 | ID)
data:    d

link  threshold nobs logLik  AIC    niter     max.grad
logit flexible  366  -468.44 956.88 729(3615) 4.36e-04
cond.H 
4.5e+01

Random effects:
 Groups  Name        Variance Std.Dev.
 ID      (Intercept) 2.90     1.70    
 content  (Intercept) 0.24     0.49    
Number of groups:  ID 92,  content 4 

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
condA              0.681      0.213    3.20   0.0014 ** 
condB              0.457      0.211    2.17   0.0303 *  
abilityA           1.148      0.255    4.51  6.5e-06 ***
abilityB           0.577      0.247    2.34   0.0195 *  

Threshold coefficients:
    Estimate Std. Error z value
1|2   -3.500      0.438   -7.99
2|3   -1.545      0.378   -4.08
3|4    0.193      0.366    0.53
4|5    2.121      0.385    5.50
</code></pre>
"
"0.0889178742536973","0.105021006302101"," 95190","<p>I am conducting a <strong>meta-analysis</strong> from a large number of studies. In each study are <strong>compared weights of two groups</strong> (fishes with and without internal parasite). I am interested if the <strong>weight can explain the presence/absence of a parasite</strong>. From forest plot it seems to be clear that in each continent (and for world as a whole)  there is a <strong>clear preference towards bigger fishes</strong>.</p>

<p>I have studies from all over the world, that means <strong>the term ""bigger"" fish is a relative</strong>. The bigger fish in Africa could be very small in comparison with smallest fish in Australia. <strong>Because of this fact I have used random-effect model</strong>. It should be a good choice according to some textbooks.</p>

<p><strong>Is random-effect model appropriate for my situation?</strong></p>

<p>Could you please help me with <strong>interpretation of model output</strong>? There are some things I do not understand:</p>

<p>P-value from model results is highly significant. So it is highly probable that the weight has something to do with parasite infestation. However, the Test for Heterogeneity is also significant. Does it means that <strong>my model do not meet the assumption for normal distribution of residuals?</strong> Similarly like a in the case of some simple regression?</p>

<p><strong>And what about tau^2, tau, I^2 and H^2?
Is any of them similar to R-squared?</strong></p>

<p>Please give me some guidance (in layman terms if possible).</p>

<pre><code>library(metafor)
mod_weight &lt;- rma(yi, vi, data = dat_weight); summary(mod_weight)

Random-Effects Model (k = 62; tau^2 estimator: REML)

  logLik  deviance       AIC       BIC      AICc  
-65.0051  130.0103  134.0103  138.2320  134.2172  

tau^2 (estimated amount of total heterogeneity): 0.3618 (SE = 0.0808)
tau (square root of estimated tau^2 value):      0.6015
I^2 (total heterogeneity / total variability):   86.67%
H^2 (total variability / sampling variability):  7.50

Test for Heterogeneity: 
Q(df = 61) = 395.7163, p-val &lt; .0001

Model Results:

estimate       se     zval     pval    ci.lb    ci.ub          
  0.8007   0.0853   9.3830   &lt;.0001   0.6334   0.9679      *** 
</code></pre>

<p><strong><em>P.S. the fish-parasite research is a made-up story :)</em></strong></p>
"
"0.184812331090109","0.18190171877725"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.138323750077661","0.136145287159293"," 95386","<p>What I have is a medical data set with several variables, all 0-1 variables. I want to make inference about them with logistic regression. I have a few problems:</p>

<ol>
<li><p>I have location variables for the disease. I was advised by my statistic advisor to put them in bins as follows: If it was solely in the right part of the organ then I would mark 1 in the column for right and similarily for left. However if it were in both places I marked in neither of the left and right column but marked one in column both. Using this approach I get error in R, numeric 0 1 error when I use glm in R and I think it is due to how these variables are constructed. Shouldn't I rather have just left and right variables and when we have the disease in both sides I should mark in left and right column and skip the both column and maybe introduce interaction term between left and right (that I would at least do in a linear model).</p></li>
<li><p>Using glm (family binomial for logistic regression) in R I was thinking how to find the best model describing some variable. I started with one usual approach with finding univarietly which variables had p-value less than $0.1$ in Fischer exact test. Then I included those variables in my model and started to delete them after which had the highest p-value. In most medical reasearches I have read when applying multivariate regression I see the usage of p-value $0.05$ but I have a feeling that it might be because of lack of understanding of the subject. When I ranked the model according to AIC and explored the best model I usually got variable with p-value around $0.1$. Which approach is preferably, is it justifyable to just cut of at p-value $0.05$ or should use AIC as an estimator of the best multivariate model? AIC does punish for extra variables and so it shouldnt give one too many variables.</p></li>
</ol>
"
"0.0435606841869032","0.0428746462856272"," 95795","<p>from what I have studied in the data mining course (please correct me if I'm wrong) - in logistic regression, when the response variable is binary, then from the ROC curve we can determine the threshold.</p>

<p>Now I'm trying to apply the logistic regression for an ordinal categorical response variable with  more than two categories (4).
I used the function <code>polr</code> in r:</p>

<pre><code>&gt; polr1&lt;-polr(Category~Division+ST.Density,data=Versions.data)
&gt; summary(polr1)

Re-fitting to get Hessian

Call:
polr(formula = Category ~ Division + ST.Density, data = Versions.data)

Coefficients:
               Value Std. Error t value
DivisionAP   -0.8237     0.5195  -1.586
DivisionAT   -0.8989     0.5060  -1.776
DivisionBC   -1.5395     0.5712  -2.695
DivisionCA   -1.8102     0.5240  -3.455
DivisionEM   -0.5580     0.4607  -1.211
DivisionNA   -1.7568     0.4704  -3.734
ST.Density    0.3444     0.0750   4.592

Intercepts:
    Value   Std. Error t value
1|2 -1.3581  0.4387    -3.0957
2|3 -0.5624  0.4328    -1.2994
3|4  1.2661  0.4390     2.8839

Residual Deviance: 707.8457 
AIC: 727.8457  
</code></pre>

<p>How should I interpret the Intercepts?
and how can I determine the threshold for each group?</p>

<p>Thanks</p>
"
"0.106701449104437","0.105021006302101"," 95891","<p>I'm running a logistic regression model where anecdotally I expected age to be a very large factor. If you see from the charts I made in Excel before running the model through R, this is how the support lines up by age:</p>

<p><img src=""http://i.stack.imgur.com/oEVZQ.jpg"" alt=""enter image description here""></p>

<p>Looks pretty significant.</p>

<p>Though when I run the model, as you can see below, age is the <em>only</em> thing that's not significant -- which was very surprising:</p>

<pre><code>&gt; attach(mydata) 
&gt; 
&gt; # Define variables 
&gt; 
&gt; Y &lt;- cbind(support)
&gt; X &lt;- cbind(sex, region, age, supportscore1, supportscore2, county)
&gt;
&gt; # Logit model coefficients 
&gt; 
&gt; logit &lt;- glm(Y ~ X, family=binomial (link = ""logit""), na.action = na.exclude) 
&gt; 
&gt; summary(logit) 

Call:
glm(formula = Y ~ X, family = binomial(link = ""logit""), na.action = na.exclude)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1019  -0.7609   0.5231   0.7101   2.3965  

Coefficients:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            4.013446   0.440962   9.102  &lt; 2e-16 ***
Xsex                  -0.229256   0.104859  -2.186 0.028792 *  
Xregion               -1.103308   0.091497 -12.058  &lt; 2e-16 ***
Xage                   0.004569   0.003209   1.424 0.154512    
Xsupportscore1        -0.019262   0.005732  -3.360 0.000778 ***
Xsupportscore2         0.019810   0.005264   3.764 0.000168 ***
Xcounty               -0.047581   0.011161  -4.263 2.02e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2871.5  on 2072  degrees of freedom
Residual deviance: 2245.5  on 2066  degrees of freedom
  (66 observations deleted due to missingness)
AIC: 2259.5

Number of Fisher Scoring iterations: 4
</code></pre>

<p>My only guess on this is that the previous support scores (both 0-100 numerical values) I'm using may have already taken age into account, and the model doesn't want to count it twice. Though, to compare, region and county are just two different ways of cutting up the geography -- and those both seem significant.</p>

<p>Can somebody let me know what you would think if your model told you that age wasn't significant when in clearly is? Trying to figure out if there's a way of thinking about it that I'm missing or if something in my code is wrong.</p>

<p>Thanks!</p>

<p>--
<strong>EDIT</strong></p>

<p>Pairs plot added to show correlation (despite some factors being categorical):</p>

<pre><code>pairs(~sex + region +  age + supportscore1 + supportscore2 + county, data=mydata)
</code></pre>

<p><img src=""http://i.stack.imgur.com/N2IG4.jpg"" alt=""enter image description here""></p>
"
"0.254000254000381","0.242647058823529"," 96010","<p>I am struggling to fit alternative count models into my data. I guess my problem is just too many zeros.</p>

<p>This is my data</p>

<pre><code>&gt; summary(smpl)
    response        predict1          predict2        
 Min.   :0.000   Min.   :   1.00   Min.   :    22005  
 1st Qu.:0.000   1st Qu.:   3.00   1st Qu.:  4669705  
 Median :0.000   Median :   8.00   Median : 12540318  
 Mean   :0.017   Mean   :  23.27   Mean   : 20382574  
 3rd Qu.:0.000   3rd Qu.:  20.00   3rd Qu.: 25468156  
 Max.   :3.000   Max.   :1584.00   Max.   :145348049

&gt; table(smpl$response)
  0   1   2   3 
987  10   2   1 
</code></pre>

<p>I tried three regressions: basic Poisson, negative binomial and zero-inflated but the only formula returning coefficients without warnings is the Poisson:</p>

<pre><code>&gt; summary(glm(response ~ ., data = smpl, family = poisson))

Call:
glm(formula = response ~ ., family = poisson, data = smpl)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3871  -0.2214  -0.1722  -0.1148   4.7861  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.472e+00  3.521e-01  -9.862  &lt; 2e-16 ***
predict1     3.229e-03  7.271e-04   4.442 8.93e-06 ***
predict2    -6.258e-08  3.060e-08  -2.045   0.0409 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 150.67  on 999  degrees of freedom
Residual deviance: 135.84  on 997  degrees of freedom
AIC: 170.06

Number of Fisher Scoring iterations: 8
</code></pre>

<p>The negative binomial returns a warnings on both the convergence and the alternation limit</p>

<pre><code>summary(glm.nb(response ~ ., data = smpl))

Call:
glm.nb(formula = response ~ ., data = smpl, init.theta = 0.04901296596, 
    link = log)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.28844  -0.17677  -0.14542  -0.09808   2.38314  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.899e+00  4.587e-01  -8.499  &lt; 2e-16 ***
predict1     1.226e-02  2.144e-03   5.720 1.06e-08 ***
predict2    -5.982e-08  3.407e-08  -1.756   0.0791 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for Negative Binomial(0.049) family taken to be 1)

    Null deviance: 69.927  on 999  degrees of freedom
Residual deviance: 55.940  on 997  degrees of freedom
AIC: 152.37

Number of Fisher Scoring iterations: 1


              Theta:  0.0490 
          Std. Err.:  0.0251 
Warning while fitting theta: alternation limit reached 

 2 x log-likelihood:  -144.3700 
Warning messages:
1: glm.fit: algorithm did not converge 
2: In glm.nb(response ~ ., data = smpl) : alternation limit reached
</code></pre>

<p>and the zero-inflated (from the <code>pscl</code> package) doesn't return anything at all</p>

<pre><code>&gt; summary(zeroinfl(response ~ ., data = smpl, dist = ""negbin""))

Call:
zeroinfl(formula = response ~ ., data = smpl, dist = ""negbin"")

Pearson residuals:
     Min       1Q   Median       3Q      Max 
-0.45252 -0.08817 -0.05515 -0.04210 19.56118 

Count model coefficients (negbin with log link):
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.477e+00         NA      NA       NA
predict1     2.678e-03         NA      NA       NA
predict2    -1.160e-07         NA      NA       NA
Log(theta)  -1.241e+00         NA      NA       NA

Zero-inflation model coefficients (binomial with logit link):
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  4.869e+00         NA      NA       NA
predict1    -1.329e-01         NA      NA       NA
predict2    -1.346e-07         NA      NA       NA
Error in if (getOption(""show.signif.stars"") &amp; any(rbind(x$coefficients$count,  : 
  missing value where TRUE/FALSE needed
</code></pre>

<p>Then my questions are: </p>

<ol>
<li>Is there anything I can do in terms of ""formula tweaking"" with the negative binomial (to avoid the warnings) and with the zero-inflated (to get the coefficients)?</li>
<li>Looking only at the results above (thus including problems with convergence and alternation limit) should I select the negative binomial model since it seems, looking at the AIC, to fit better than the Poisson in my data?</li>
</ol>
"
"0.0987863462557455","0.113435651621629"," 96236","<p>I am following an example <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">here</a> on using Logistic Regression in R. However, I need some help interpreting the results. They do go over some of the interpretations in the above link, but I need more help with understanding a goodness of fit for Logistic Regression and the output that I am given.</p>

<p>For convenience, here is the summary given in the example:</p>

<pre><code>## Call:
## glm(formula = admit ~ gre + gpa + rank, family = ""binomial"", 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.627  -0.866  -0.639   1.149   2.079  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.98998    1.13995   -3.50  0.00047 ***
## gre          0.00226    0.00109    2.07  0.03847 *  
## gpa          0.80404    0.33182    2.42  0.01539 *  
## rank2       -0.67544    0.31649   -2.13  0.03283 *  
## rank3       -1.34020    0.34531   -3.88  0.00010 ***
## rank4       -1.55146    0.41783   -3.71  0.00020 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 458.52  on 394  degrees of freedom
## AIC: 470.5
## 
## Number of Fisher Scoring iterations: 4
</code></pre>

<ol>
<li>How well did Logistic Regression fit here?</li>
<li>What exactly are the Deviance Residuals? I believe they are the average residuals per quartile. How do I determine if they are bad/good/statistically significant?</li>
<li>What exactly is the <code>z-value</code> here? Is it the normalized standard deviation from the mean of the Estimate assuming a mean of 0? </li>
<li>What exactly are Signif. codes?</li>
</ol>

<p>Any help is greatly appreciated! You do not have to answer them all!</p>
"
"0.0974046509883157","0.0958706236059213","102695","<p>I'm using R to run some logistic regression. My variables were continuous, but I used cut to bucket the data. Some particular buckets for these variables always result in dependent variable being equal to 1. As expcted, the coefficient estimate for this bucket is very high, but the p-value is also high. There are about ~90 observations in either these buckets, and around 800 total observations, so I don't think it's a problem of sample size. Also, this variable should not be related to other variables, which would naturally reduce their p-values.</p>

<p>Are there any other plausible explanations for the high p-value?</p>

<p>Example:</p>

<pre><code>myData &lt;- read.csv(""application.csv"", header = TRUE)
myData$FICO &lt;- cut(myData$FICO, c(0, 660, 680, 700, 720, 740, 780, Inf), right = FALSE)
myData$CLTV &lt;- cut(myData$CLTV, c(0, 70, 80, 90, 95, 100, 125, Inf), right = FALSE)
fit &lt;- glm(Denied ~ CLTV + FICO, data = myData, family=binomial())
</code></pre>

<p>Results are something like this:</p>

<pre><code>Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.53831  -0.77944  -0.62487   0.00027   2.09771  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -1.33630    0.23250  -5.747 9.06e-09 ***
CLTV(70,80]     -0.54961    0.34864  -1.576 0.114930    
CLTV(80,90]     -0.51413    0.31230  -1.646 0.099715 .  
CLTV(90,95]     -0.74648    0.37221  -2.006 0.044904 *  
CLTV(95,100]     0.38370    0.37709   1.018 0.308906    
CLTV(100,125]   -0.01554    0.25187  -0.062 0.950792    
CLTV(125,Inf]   18.49557  443.55550   0.042 0.966739    
FICO[0,660)     19.64884 3956.18034   0.005 0.996037    
FICO[660,680)    1.77008    0.47653   3.715 0.000204 ***
FICO[680,700)    0.98575    0.30859   3.194 0.001402 ** 
FICO[700,720)    1.31767    0.27166   4.850 1.23e-06 ***
FICO[720,740)    0.62720    0.29819   2.103 0.035434 *  
FICO[740,780)    0.31605    0.23369   1.352 0.176236    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1037.43  on 810  degrees of freedom
Residual deviance:  803.88  on 798  degrees of freedom
AIC: 829.88

Number of Fisher Scoring iterations: 16
</code></pre>

<p>FICO in the range [0, 660) and CLTV in the range (125, Inf] indeed always results in Denial = 1, so their coefficients are very large, but why are they also ""insignificant""?</p>
"
"0.0974046509883157","0.0958706236059213","107865","<p>I'm investigating environmental effects (wind) on acoustic receiver detection probability for two types of transmitters using a binomial glmer. While my model analysis indicates that there's a significant effect between wind speed and transmitter type, graphical visualisation does not confirm this. If I'm correct, an interaction should demonstrate different regression slopes.</p>

<pre><code>m1 &lt;- glmer(cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth + 
               Receiver.depth + Water.temperature + Wind.speed + Transmitter + 
               Distance + Habitat + Replicate + (1 | Day) + (Distance | SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat + 
               Receiver.depth:Habitat + Wind.speed:Transmitter, data=df, family=binomial(link=logit))
</code></pre>

<p>The model summary is as follows:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth +  
    Receiver.depth + Water.temperature + Wind.speed + Transmitter +  
    Distance + Habitat + Replicate + (1 | Day) + (Distance |  
    SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat +      Receiver.depth:Habitat + Wind.speed:Transmitter
   Data: df

     AIC      BIC   logLik deviance df.resid 
  3941.9   4043.8  -1953.9   3907.9     2943 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-9.4911  0.0000  0.0000  0.5666  1.9143 

Random effects:
 Groups Name        Variance Std.Dev. Corr
 SUR.ID (Intercept)  0.33414 0.5781       
        Distance     0.09469 0.3077   1.00
 Day    (Intercept) 15.96629 3.9958       
Number of obs: 2960, groups:  SUR.ID, 20 Day, 6

Fixed effects:
                                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      3.20222    2.84984   1.124  0.26116    
Transmitter.depth               -0.35015    0.11794  -2.969  0.00299 ** 
Receiver.depth                  -0.57331    0.51919  -1.104  0.26949    
Water.temperature               -0.26595    0.11861  -2.242  0.02495 *  
Wind.speed                       1.31735    1.50457   0.876  0.38127    
TransmitterPT-04                -0.68854    0.08016  -8.590  &lt; 2e-16 ***
Distance                        -0.39547    0.09228  -4.286 1.82e-05 ***
HabitatFinger                   -0.23746    3.57783  -0.066  0.94708    
Replicate2                      -0.21559    0.08009  -2.692  0.00710 ** 
TransmitterPT-04:Distance       -0.27874    0.08426  -3.308  0.00094 ***
Transmitter.depth:HabitatFinger  0.73965    0.28612   2.585  0.00973 ** 
Receiver.depth:HabitatFinger     3.02083    0.74546   4.052 5.07e-05 ***
Wind.speed:TransmitterPT-04     -0.15540    0.06572  -2.364  0.01806 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Trnsm. Rcvr.d Wtr.tm Wnd.sp TrPT-04 Distnc HbttFn Rplct2 TPT-04: Tr.:HF Rc.:HF
Trnsmttr.dp -0.024                                                                               
Recevr.dpth -0.120 -0.267                                                                        
Watr.tmprtr  0.019 -0.159  0.007                                                                 
Wind.speed   0.130  0.073 -0.974  0.040                                                          
TrnsmtPT-04 -0.015  0.027  0.020 -0.018 -0.024                                                   
Distance     0.022 -0.080  0.151 -0.052 -0.141 -0.164                                            
HabitatFngr -0.813  0.010  0.241 -0.025 -0.253  0.009   0.029                                    
Replicate2  -0.067  0.033  0.377 -0.293 -0.394  0.010   0.085  0.103                             
TrnsPT-04:D -0.006  0.043 -0.007 -0.050 -0.003  0.516  -0.373  0.004  0.006                      
Trnsmtt.:HF  0.017 -0.352  0.021  0.055  0.049  0.026  -0.142  0.031 -0.088  0.025               
Rcvr.dpt:HF  0.103  0.189 -0.830  0.051  0.817 -0.036  -0.143 -0.224 -0.385 -0.003  -0.229       
Wnd.:TPT-04 -0.002  0.026 -0.015  0.003 -0.009  0.176  -0.114 -0.002  0.016  0.306  -0.008  0.014
</code></pre>

<p><img src=""http://i.stack.imgur.com/DFxyQ.png"" alt=""enter image description here""></p>

<p>A side question: I noticed a strong negative correlation between the intercept and a dichotome categorical predictor. I wonder if this causes any problems for my data analysis. All the covariates are centered and scaled for numerical stability during modelling.  </p>
"
"0.150898636448357","0.148522131446501","108374","<p>I have a monthly time series with an intervention and I would like to quantify the effect of this intervention on the outcome. I realize the series is rather short and the effect is not yet concluded.</p>

<p><strong>The Data</strong></p>

<pre><code>  cds&lt;- structure(c(2580L, 2263L, 3679L, 3461L, 3645L, 3716L, 3955L, 
    3362L, 2637L, 2524L, 2084L, 2031L, 2256L, 2401L, 3253L, 2881L, 
    2555L, 2585L, 3015L, 2608L, 3676L, 5763L, 4626L, 3848L, 4523L, 
    4186L, 4070L, 4000L, 3498L), .Dim = c(29L, 1L), .Dimnames = list(
        NULL, ""CD""), .Tsp = c(2012, 2014.33333333333, 12), class = ""ts"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/lNOEk.jpg"" alt=""enter image description here""></p>

<p><strong>The methodology</strong></p>

<p>1) The pre-intervention series (up until October 2013) was used with the <code>auto.arima</code> function. The model suggested was ARIMA(1,0,0) with non-zero mean. The ACF plot looked good.</p>

<pre><code>pre&lt;-window(cds,start = c(2012,01), end=c(2013,09))

mod.pre&lt;-auto.arima(log(pre))

Coefficients:
         ar1  intercept
      0.5821     7.9652
s.e.  0.1763     0.0810

sigma^2 estimated as 0.02709:  log likelihood=7.89
AIC=-9.77   AICc=-8.36   BIC=-6.64
</code></pre>

<p>2) Given the plot of the full series, the pulse response was chosen below, with T = Oct 2013,</p>

<p><img src=""http://i.stack.imgur.com/YU3nB.jpg"" alt=""enter image description here""></p>

<p>which according to cryer and chan can be fit as follows with the arimax function:</p>

<pre><code>   mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
            xtransf=data.frame(Oct13=1*(seq(cds)==22)),
            transfer=list(c(1,1))
          )

    mod.arimax


Series: log(cds) 
ARIMA(1,0,0) with non-zero mean 

Coefficients:
         ar1  intercept  Oct13-AR1  Oct13-MA0  Oct13-MA1
      0.7619     8.0345    -0.4429     0.4261     0.3567
s.e.  0.1206     0.1090     0.3993     0.1340     0.1557

sigma^2 estimated as 0.02289:  log likelihood=12.71
AIC=-15.42   AICc=-11.61   BIC=-7.22
</code></pre>

<p>The residuals from this appeared OK:</p>

<p><img src=""http://i.stack.imgur.com/wvdXD.jpg"" alt=""enter image description here""></p>

<p>The plot of fitted and actuals:</p>

<pre><code>plot(fitted(mod.arimax),col=""red"", type=""b"")
lines(window(log(cds),start=c(2012,02)),type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/kJ1pj.jpg"" alt=""enter image description here""></p>

<p><strong>The Questions</strong></p>

<p>1) Is this methodology correct for intervention analysis?</p>

<p>2) Can I look at estimate/SE for the components of the transfer function and say that the effect of the intervention was significant?</p>

<p>3) How can one visualize the transfer function effect (plot it?)</p>

<p>4) Is there a way to estimate how much the intervention increased the output after 'x' months? I guess for this (and maybe #3) I am asking how to work with an equation of the model - if this were simple linear regression with dummy variables (for example) I could run scenarios with and without the intervention and measure the impact - but I am just unsure how to work this this type of model.</p>

<p><strong>ADD</strong></p>

<p>Per request, here are the residuals from the two parametrizations.</p>

<p>First from the fit:</p>

<pre><code>fit &lt;- arimax(log(cds), order = c(1,0,0), 
              xtransf = data.frame(Oct13a = 1*(seq_along(cds)==22), Oct13b = 1*(seq_along(cds)==22)),
              transfer = list(c(0,0), c(1,0)))

plot(resid(fit), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/sqMZN.jpg"" alt=""enter image description here""></p>

<p>Then, from this fit</p>

<pre><code>mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
                   xtransf=data.frame(Oct13=1*(seq(cds)==22)),
                   transfer=list(c(1,1))
)

mod.arimax
plot(resid(mod.arimax), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/DjAyu.jpg"" alt=""enter image description here""></p>
"
"0.0616041103633697","0.0606339062590832","108524","<p>Here is example to workout:</p>

<pre><code>require(lasso2)
data(Prostate)
fit = lm(lpsa~.,data=Prostate)
summary(fit)

lam = seq(0,10000,len=5000)
require(MASS)
ridgefits = lm.ridge(lpsa~.,data=Prostate,lam=lam)

# plot traces for each predictor
plot(range(lam), range(ridgefits$coef),type=""n"")
    clrs &lt;- c(""red"", ""blue"", ""green"", ""pink"", ""green1"", ""tan"", ""yellow"", ""gray40"")
    for(i in 1:nrow(ridgefits$coef)){
   lines(lam,ridgefits$coef[i,], col = clrs[i])
       }
    legend(6500, 0.7, rownames(ridgefits$coef), col = clrs,  lwd = 2)
</code></pre>

<p>Next I would like to generate graph something like this - 
<img src=""http://i.stack.imgur.com/3tLxq.jpg"" alt=""enter image description here""></p>

<p>I think I see nice discussion and Matlab script in this <a href=""http://stats.stackexchange.com/questions/8309/how-to-calculate-regularization-parameter-in-ridge-regression-given-degrees-of-f"">Q/A</a>. Is there anyway we can do same in R ? Once we get this how can we get AIC /BIC and use that in decision making in selection of $\lambda$ ?</p>

<p><strong>Edits:</strong> based on following suggestions in answer:</p>

<pre><code>    require(rms)
    ridgefits = ols(lpsa~lcavol+lweight+age+lbph+svi+lcp+gleason+pgg45,
       method=""qr"", data=Prostate,
    se.fit = TRUE, x=TRUE, y=TRUE)
    p &lt;- pentrace(ridgefits, seq(.2,1,by=.05))
    effective.df(ridgefits,p)

     out &lt;- p$results.all
   out
   penalty       df      aic      bic    aic.c
1     0.00 8.000000 87.15916 66.56148 85.52280
2     0.20 7.994347 87.16845 66.58532 85.53437
3     0.25 7.992884 87.17026 66.59089 85.53677
4     0.30 7.991400 87.17186 66.59631 85.53898
5     0.35 7.989898 87.17327 66.60159 85.54099
6     0.40 7.988376 87.17448 66.60671 85.54282
7     0.45 7.986836 87.17549 66.61170 85.54445
8     0.50 7.985277 87.17632 66.61654 85.54591
9     0.55 7.983700 87.17696 66.62124 85.54719
10    0.60 7.982104 87.17741 66.62580 85.54829
11    0.65 7.980491 87.17768 66.63022 85.54921
12    0.70 7.978860 87.17777 66.63451 85.54995
13    0.75 7.977212 87.17768 66.63867 85.55053
14    0.80 7.975546 87.17742 66.64269 85.55094
15    0.85 7.973864 87.17698 66.64658 85.55118
16    0.90 7.972165 87.17637 66.65035 85.55125
17    0.95 7.970449 87.17559 66.65399 85.55117
18    1.00 7.968717 87.17464 66.65750 85.55092

    plot(out$penalty, out$df)
    plot( out$df, out$penalty, type = ""l"", col = ""red"")
</code></pre>

<p>I am close know but not still could not plot the figure exactly like above. </p>

<p><img src=""http://i.stack.imgur.com/4f5so.jpg"" alt=""enter image description here""></p>
"
"0","0.0428746462856272","108904","<p>I did stepwise regression with my multiple regression model and using AIC as a measure of fit with the <code>step</code> function in R. Afterwards some variables that the stepwise regression did not eliminate was not significant (> 0.05 p-value). Does this mean i have to take out those variables with large p-values or what is a normal procedure?  </p>
"
"0.194809301976631","0.18215418485125","109835","<p>While working on a big data set made of 10-minutes-points of information - i.e. <code>144</code> points per day, <code>1008</code> per week and <code>52560</code> per year - I encountered a few problem in R. The information concerns electricity load on a source substation during the year.</p>

<h3>Multiple seasonality :</h3>

<p>The data set clearly shows multiple seasonalities, which are daily, weekly and yearly. From <a href=""http://stats.stackexchange.com/questions/47729/two-seasonal-periods-in-arima-using-r"">there</a> I understood that R doesn't handle multiple seasonality within the ARIMA modeling functions.  I would really like to work with ARIMA models though, because my previous work is based on ARIMA models and I know approximatively how to translate a model into an equation.  </p>

<h3>Long seasonality :</h3>

<p>Each of the seasonalities is of high value, with the shortest one being the daily seasonality at 144. Unfortunately from the SARIMA general equation which is<br>
$\phi(B)\Phi(B^s)W_t = \theta(B)\Theta(B^s)Z_t$<br>
I guessed that the maximum lag for a given model <code>SARIMA(p,d,q)(P,D,Q)144</code> is<br>
$max((p+P*144), (q+Q*144))$</p>

<p>I would really like to try and fit models with values of P and/or Q greater than 1, but R doesn't allow me since the <code>maximum supported lag = 350</code>. To do so I found <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">this link</a> which is really interesting and led to new functions in the forecast package by M. Hyndman, called <code>fourier</code> and <code>fourierf</code> which you can find <a href=""http://www.inside-r.org/packages/cran/forecast/docs/fourier"" rel=""nofollow"">here</a>. But since I am not a specialist in forecasting nor in statistics, I have some difficulties understanding how I can make this work.  </p>

<hr>

<p>The thing is I thing this whole fourier regressors package could help me a lot. From what I understood I could use it to simulate the long-seasonality of my data set, maybe use it to simulate multiple seasonality, and even more it could allow me to introduce exogenous variables - which are the <code>temperature</code> and (<code>public holiday + sundays</code>).<br>
I also tried doing some regression following <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">this example</a> but I couldn't make it work because :</p>

<pre><code>Error in forecast.Arima(bestfit, xreg = fourierf(gas, K = 12, h = 1008)) : 
Number of regressors does not match fitted model
</code></pre>

<p>I really hope somebody can help me get a better understanding of these functions. Thanks.</p>

<p><strong>Edit :</strong> So I tried my best with the fourier example given <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a> but couldn't figure out how it handles the fitting. Here is the code (I copy-pasted M. Hyndman one and adapted to my data set - unsuccessfully) :</p>

<pre><code>n &lt;- 50000
m &lt;- 144
y &lt;- read.table(""auch.txt"", skip=1)
fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}

library(forecast)
fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008)))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m), fourier(n+1:(14*m),4,1008))))
</code></pre>

<p>So I wanted to ""force"" the model to be a <code>SARIMA(2,1,5)(1,2,8)[144]</code> but when I type <code>arimod</code>this is the result of the Arima fitting :</p>

<pre><code>&gt; fit  
Series: y[1:n, 1] , 
ARIMA(2,1,5)                  

sigma^2 estimated as 696895:  log likelihood=-407290.2  
AIC=814628.3   AICc=814628.3   BIC=814840
</code></pre>

<p>It doesn't even take into consideration the seasonal part of the model, and I don't know much about the range the AIC values can take, but it seems way too high to be a good fitting model right there. I think it all comes down to my misunderstanding of the use of Fourier terms as regressors, but I can't figure out why.</p>

<p><strong>Edit 2 :</strong> Also I can't seem to be able to add another exogenous variable to the Arima function. I need to use <code>temperature</code> - probably as a lead - to fit the <code>SARIMAX</code> model, but as soon as I write this :</p>

<pre><code>fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008), tmp[1:n]))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m),fourier(n+1:(14*m),4,1008), tmp[n+1:(14*m)])))
</code></pre>

<p>Nothing is plotted besides the initial data set. There is no forecast while without <code>tmp</code> as an <code>xreg</code> I still get some results.</p>
"
"0.0435606841869032","0.0428746462856272","110148","<p>I have run a <code>multinomial logistic regression</code> test for the interaction between species of deer, days a camera trap was in the field and type of reaction. </p>

<p>The model with the best AIC value was: </p>

<pre><code>Coefficients:
   (Intercept) speciesmuntjac   speciesroe speciessika        days
r   -0.7471023      0.6263753 -0.005967869 -0.74253017 -0.05189515
sr   0.6909319      0.5552278 -0.355611180 -0.01622306 -0.03178001

Residual Deviance: 971.6464 
AIC: 991.6464 
</code></pre>

<p>But I also got the following </p>

<pre><code>    (Intercept) speciesmuntjac speciesroe speciessika       days
r  0.0071741793     0.17599897  0.9865350   0.2095687 0.08669276
sr 0.0001402185     0.08536257  0.1331651   0.9574258 0.08829020
</code></pre>

<p>I'm just wondering if anyone knows why this is considered the best model when neither days nor deer react (r) or strongly react (sr) higher than chance?</p>
"
"0.0616041103633697","0.0606339062590832","110155","<p>I have run a multinomial logistic regression test for the interaction between species of deer, days a camera trap was in the field and type of reaction.</p>

<p>The model with the best AIC value was:</p>

<pre><code>Coefficients:
   (Intercept) speciesmuntjac   speciesroe speciessika        days
r   -0.7471023      0.6263753 -0.005967869 -0.74253017 -0.05189515
sr   0.6909319      0.5552278 -0.355611180 -0.01622306 -0.03178001

Residual Deviance: 971.6464 
AIC: 991.6464 
</code></pre>

<p>But I also got the following</p>

<pre><code>    (Intercept) speciesmuntjac speciesroe speciessika       days
r  0.0071741793     0.17599897  0.9865350   0.2095687 0.08669276
sr 0.0001402185     0.08536257  0.1331651   0.9574258 0.08829020
</code></pre>

<p>I'm just wondering if anyone would know what the interaction between species and days is in relation to r (reaction) and sr (strong reaction)? I know I can't reject the null hypothesis that there is no effect of species or days on r or sr but beyond that I'm lost!</p>
"
"0.0754493182241785","0.0742610657232506","110569","<p>I have a data set based on 14 field sites, the dependent variable I am investigating is count data recorded at each site (on 3-5 visits).  The independent variables (I have around 45) are fixed at each site (apart from wind speed and temperature).  An example data set, illustrating this is below:</p>

<pre><code>            Trees    water   butterflycount 
Site 1      6        8       3

Site 1      6        8       12

Site 2      3        3       8

Site 2      3        3       0
</code></pre>

<p>What is the best way of preparing this kind of  data for a multiple regression analysis in R?  Would data1.glm = glm(butterflycount ~ 1, data = data1, family = poisson) and the working through adding the lowest AIC valued variables to the equation work?  Any help would be greatly appreciated.</p>
"
"0.0616041103633697","0.0606339062590832","111541","<p>Could anyone provide me the details of how to determine the lag order of the  distributed lags for an $\text{ADL}(p,q)$ model in Matlab or another statistical package (and very much preferably in combination with the autoregression lags)? </p>

<p>Full working examples with model selection criteria ($\text{AIC}$ and $\text{BIC}$) seem to be available on the Matlab website for $\text{VAR}$ models, $\text{ARMA}$ models etc. but I can't find one for the $\text{ADL}(p,q)$ model. I would not have a clue how to rewrite those models to $\text{ADL}(p,q)$ myself but I have a vague feeling that such a thing would be possible.</p>

<p>In the end I want to automate this analysis by first checking the lag orders $p$, and $q$ and then using these numbers automatically to create the regressions out of this. So basically I'm looking for a fully working example. (I want to skip part of adding and deleting regressors by hand as much as possible to get a quick idea of the distributed lags of several assets).</p>
"
"0.200379147259755","0.214373231428136","112241","<p><strong>Summary:</strong> Is there any statistical theory to support the use of the $t$-distribution (with degrees of freedom based on the residual deviance) for tests of logistic regression coefficients, rather than the standard normal distribution?</p>

<hr>

<p>Some time ago I discovered that when fitting a logistic regression model in SAS PROC GLIMMIX, under the default settings, the logistic regression coefficients are tested using a $t$ distribution rather than the standard normal distribution.$^1$ That is, GLIMMIX reports a column with the ratio $\hat{\beta}_1/\sqrt{\text{var}(\hat{\beta}_1)}$ (which I will call $z$ in the rest of this question), but also reports a ""degrees of freedom"" column, as well as a $p$-value based on assuming a $t$ distribution for $z$ with degrees of freedom based on the residual deviance -- that is, degrees of freedom = total number of observations minus number of parameters. At the bottom of this question I provide some code and output in R and SAS for demonstration and comparison.$^2$</p>

<p>This confused me, since I thought that for generalized linear models such as logistic regression, there was no statistical theory to support the use of the $t$-distribution in this case. Instead I thought what we knew about this case was that</p>

<ul>
<li>$z$ is ""approximately"" normally distributed;</li>
<li>this approximation might be poor for small sample sizes;</li>
<li>nevertheless it <em>cannot</em> be assumed that $z$ has a $t$ distribution like we can assume in the case of normal regression.</li>
</ul>

<p>Now, on an intuitive level, it does seem reasonable to me that if $z$ is approximately normally distributed, it might in fact have some distribution that is basically ""$t$-like"", even if it is not exactly $t$. So the use of the $t$ distribution here does not seem crazy. But what I want to know is the following:</p>

<ol>
<li>Is there in fact statistical theory showing that $z$ really does follow a $t$ distribution in the case of logistic regression and/or other generalized linear models?</li>
<li>If there is no such theory, are there at least papers out there showing that assuming a $t$ distribution in this way works as well as, or maybe even better than, assuming a normal distribution?</li>
</ol>

<p>More generally, is there any actual support for what GLIMMIX is doing here other than the intuition that it is probably basically sensible?</p>

<p>R code:</p>

<pre><code>summary(glm(y ~ x, data=dat, family=binomial))
</code></pre>

<p>R output:</p>

<pre><code>Call:
glm(formula = y ~ x, family = binomial, data = dat)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.352  -1.243   1.025   1.068   1.156  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.22800    0.06725   3.390 0.000698 ***
x           -0.17966    0.10841  -1.657 0.097462 .  
---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1235.6  on 899  degrees of freedom
Residual deviance: 1232.9  on 898  degrees of freedom
AIC: 1236.9

Number of Fisher Scoring iterations: 4
</code></pre>

<p>SAS code:</p>

<pre><code>proc glimmix data=logitDat;
    model y(event='1') = x / dist=binomial solution;
run;
</code></pre>

<p>SAS output (edited/abbreviated):</p>

<pre><code>The GLIMMIX Procedure

               Fit Statistics

-2 Log Likelihood            1232.87
AIC  (smaller is better)     1236.87
AICC (smaller is better)     1236.88
BIC  (smaller is better)     1246.47
CAIC (smaller is better)     1248.47
HQIC (smaller is better)     1240.54
Pearson Chi-Square            900.08
Pearson Chi-Square / DF         1.00


                       Parameter Estimates

                         Standard
Effect       Estimate       Error       DF    t Value    Pr &gt; |t|

Intercept      0.2280     0.06725      898       3.39      0.0007
x             -0.1797      0.1084      898      -1.66      0.0978
</code></pre>

<p>$^1$Actually I first noticed this about <em>mixed-effects</em> logistic regression models in PROC GLIMMIX, and later discovered that GLIMMIX also does this with ""vanilla"" logistic regression.</p>

<p>$^2$I do understand that in the example shown below, with 900 observations, the distinction here probably makes no practical difference. That is not really my point. This is just data that I quickly made up and chose 900 because it is a handsome number. However I do wonder a little about the practical differences with small sample sizes, e.g. $n$ &lt; 30.</p>
"
"0.163352565700887","0.171498585142509","112380","<p>I have a problem to build and to explain the linear multiple regression.</p>

<p>I have a data set called <code>Cars93</code> with 26 variables (numeric and not numeric) and 93 observations. This data set you can find in the <code>MASS</code> R package. I want to build a linear regression model for predicting the price of a car. Then I have to do a variable selection (forward and backward stepwise) using AIC and BIC in R.  My knowledge in R is too little thatÂ´s why I have some problems. I really hope you can help me! </p>

<p>1) The data set has some missing values </p>

<p>I just solved this problem like this:</p>

<pre><code> Cars93 [! Complete.cases (Cars93)] 
 Cars93new &lt;- na.omit (Cars93) 
 Cars93 = Cars93new 
</code></pre>

<p>I think some informations are going lost. Is there another solution to eliminate the missing values? </p>

<p>2) Some variables from the dataset are not numeric
I tried to convert these values into numerical values like this:</p>

<pre><code>Cars93 $ airbags = factor (Cars93 $ airbags, labels = c (2,1,0)) 
Cars93 $ airbags 
  [1] 0 2 1 2 1 1 1 1 1 1 2 0 1 2 0 1 2 2 1 0 1 1 1 1 0 2 0 0 0 1 1 1 1 0 1 1 2 2 
[39] 0 0 0 0 1 1 2 2 2 0 0 1 1 2 1 0 0 1 1 1 1 0 1 1 0 0 0 2 0 2 1 1 0 0 1 0 1 1 
[77] 1 0 0 0 1 2 
Levels: 2 1 0 
</code></pre>

<p>I did the same with other not numeric variables.</p>

<p>Afterwards I tried to build a linear model regression with all variables:</p>

<pre><code>Modell=lm(Price~Horsepower+EngineSize+MPG.city+MPG.highway+Rev.per.mile+Man.trans.avail+Fuel.tank.capacity+Passengers+Length+Wheelbase+Width+Turn.circle+Weight+Rear.seat.room+Luggage.room+Origin+AirBags+Type+Cylinders+Weight+PRM)
summary(Modell)
</code></pre>

<p>But the output does make any sense:</p>

<pre><code>Call:
lm(formula = Price ~ Horsepower + EngineSize + MPG.city + MPG.highway + 
    Rev.per.mile + Man.trans.avail + Fuel.tank.capacity + Passengers + 
    Length + Wheelbase + Width + Turn.circle + Weight + Rear.seat.room + 
    Luggage.room + Origin + AirBags + Type + Cylinders + Weight + 
    RPM)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.4893 -2.3664 -0.0062  2.1180 18.1112 

Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        81.335018  37.993697   2.141 0.036826 *  
Horsepower          0.123535   0.049355   2.503 0.015372 *  
EngineSize         -0.615828   3.047223  -0.202 0.840602    
MPG.city           -0.392888   0.470385  -0.835 0.407259    
MPG.highway         0.013646   0.428978   0.032 0.974740    
Rev.per.mile        0.001498   0.002511   0.597 0.553206    
Man.trans.availYes -1.600967   2.480497  -0.645 0.521387    
Fuel.tank.capacity  0.462731   0.572169   0.809 0.422219    
Passengers          0.615593   1.823089   0.338 0.736925    
Length              0.074875   0.130511   0.574 0.568547    
Wheelbase           0.740146   0.343760   2.153 0.035796 *  
Width              -1.745792   0.571082  -3.057 0.003473 ** 
Turn.circle        -0.695287   0.415708  -1.673 0.100203    
Weight             -0.004068   0.006255  -0.650 0.518171    
Rear.seat.room      0.101150   0.420050   0.241 0.810619    
Luggage.room        0.176183   0.367199   0.480 0.633306    
Originnon-USA       1.881047   1.762845   1.067 0.290696    
AirBagsDriver only -3.294049   1.888346  -1.744 0.086777 .  
AirBagsNone        -8.535307   2.289737  -3.728 0.000464 ***
TypeLarge          -1.692122   3.999146  -0.423 0.673887    
TypeMidsize         2.684947   2.639047   1.017 0.313504    
TypeSmall           1.913341   2.896592   0.661 0.511710    
TypeSporty          4.686129   3.268426   1.434 0.157407    
Cylinders4         -3.126727   4.554852  -0.686 0.495360    
Cylinders5         -4.732933   7.498898  -0.631 0.530605    
Cylinders6          0.224795   5.695793   0.039 0.968664    
Cylinders8          4.020677   7.255406   0.554 0.581755    
RPM                -0.002778   0.002450  -1.134 0.261805    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 5.009 on 54 degrees of freedom
  (11 observations deleted due to missingness)
Multiple R-squared:  0.8313,    Adjusted R-squared:  0.747 
F-statistic: 9.859 on 27 and 54 DF,  p-value: 1.014e-12
</code></pre>

<p>I have 4 times ""Cylinders"" and ""Type"" and twice ""AirBag"" in the summary. I dont know why... And only 4 variables are significant in the model. Can somebody tell me, where is the mistake in my model?</p>

<p>I also would like to know how to test other assumptions in R for multiple linear model.</p>
"
"0.131340404599205","0.14219911474863","115154","<p>Relatively new to stats. I use linear regression  and get R^2, which is quite low.</p>

<p><strong>MODEL 1</strong></p>

<pre><code>    lmoutar=lm(formula = ts_y ~ ts_y_lag + ts_x)
</code></pre>

<p>So switched to arima with external regressor. Using ""auto.arima"", I formulate arimax model</p>

<p><strong>MODEL 2</strong></p>

<pre><code>    fitarima &lt;- auto.arima(ts_y, xreg=ts_x)
    arimaout&lt;-arima(ts_y,order=c(2,0,5),xreg=ts_x)
</code></pre>

<p>How can I compare the explanability of AR model with arima model. From the thread <a href=""http://stats.stackexchange.com/questions/8750/how-can-i-calculate-the-r-squared-of-a-regression-with-arima-errors-using-r"">How can I calculate the R-squared of a regression with arima errors using R?</a>, I understand R^2 is not an option for ARIMA.</p>

<p>From the thread <a href=""http://stats.stackexchange.com/questions/11850/model-comparison-between-an-arima-model-and-a-regression-model"">Model comparison between an ARIMA model and a regression model</a>, AIC/BIC is not the right criteria and MSE from forcast/predict can be possible criteria for comparison across AR and ARIMA model. Is MSE the best option for model comparison, if so how would I generate MSE for AR and ARIMA?</p>

<p>I tried to compare the above ar and arima model using anova, but I get following error message</p>

<pre><code>anova.lm(lmoutar,arimaout)
   Warning message:
    In anova.lmlist(object, ...) :
            models with response â€˜""NULL""â€™ removed because response differs from model 1
</code></pre>

<p>What does this error message mean? </p>

<p><strong><em>EDIT</em></strong></p>

<p>Thanks for the response so far and insight that AR is nested within ARIMA. How would one answer this question, if I rephrase  as ""How to compare AR, ARIMA and General Linear Models?"". The first model I listed has AR(1) and independent variable; it is a general linear model. So how would I compare a GLM versus ARIMAX model? Any thing else besides MSE that I could use to judge between GLM and ARIMAX</p>
"
"0.144474445059126","0.14219911474863","115188","<p>I am trying to look at whether 2 variables (one dichotomous categorical and one continuous) predict the occurrence of a dichotomous categorical dependent variable.</p>

<pre><code>dependent variable is LENIpos - 0 = no event, 1 = event
predictor variables are Hip.Prox.Femur - 0 = no hip fracture, 1 = hip fracture
                and     age (continuous)
</code></pre>

<p>Both predictor variables have significant p values in separate chi square test and Mann Whitney U test respectively.</p>

<p>When I run a logistic regression <code>glm(LENIpos ~ age + Hip.Prox.Femur, family = ""binomial)</code>, the variables come out as not significant. (1)</p>

<p>However, when I run the logistic regression with interactions <code>glm(LENIpos ~ age * Hip.Prox.Femur...)</code> (2), they are no both significant.  How is this to be interpreted?</p>

<p>Example R outputs:</p>

<p>(1)</p>

<pre><code>Call: glm(formula = LENIpos ~ age + Hip.Prox.Fem, family = ""binomial"", 
    data = dvt)

Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -0.9346  -0.7826  -0.4952  -0.3374   2.1897  

Coefficients:
                         Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)              -3.46888    1.00693  -3.445 0.000571 ***
age                       0.02122    0.01519   1.397 0.162535  
Hip.Prox.Femhip fracture  0.72410    0.57790   1.253 0.210212    

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145.23  on 151  degrees of freedom
Residual deviance: 135.48  on 149  degrees of freedom
AIC: 141.48

Number of Fisher Scoring iterations: 5
</code></pre>

<p>(2)</p>

<pre><code>glm(formula = LENIpos ~ age * Hip.Prox.Fem, family = ""binomial"", 
    data = dvt)

Deviance Residuals: 
        Min       1Q   Median       3Q      Max  
    -1.0364  -0.7815  -0.5373  -0.1761   2.3443  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)                  -5.89984    1.98289  -2.975  0.00293 **
age                           0.05851    0.02818   2.076  0.03788 * 
Hip.Prox.Femhip fracture      5.04990    2.46269   2.051  0.04031 * 
age:Hip.Prox.Femhip fracture -0.06058    0.03339  -1.814  0.06965 . 


(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145.23  on 151  degrees of freedom
Residual deviance: 131.82  on 148  degrees of freedom
AIC: 139.82

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.0435606841869032","0.0428746462856272","115343","<p>I have done a linear regression in R, using glm function. The calculated intercept says 0.98, but when I plot it, it does not seem to hit the estimated intercept on Y axis. Its far below. Here are my data and function: </p>

<pre><code>event = c(2.2, 6.4, 3.4, 10.2, 4.45, 2.65, 8.25, 4.65, 3, 6.5, 5.25, 
8.65, 7.25, 6.4, 7.75, 7.45)

c(230208, 813178, 316617, 1531919, 576869, 270148, 1090947, 562643, 
439885, 745741, 666454, 1078175, 924429, 784333, 1091289, 948062)

fit=glm(event~size)

Call:  glm(formula = chr.co.count.wt ~ size)

Coefficients:
(Intercept)         size  
  9.783e-01    6.528e-06  

Degrees of Freedom: 15 Total (i.e. Null);  14 Residual
Null Deviance:      83.08 
Residual Deviance: 2.849    AIC: 23.8




plot(size,events,col=""blue"",pch=16,xlab=""size"",ylab=""events"",ylim=c(0,12),frame.plot=FALSE,xlim=c(0,2000000),axes = F)

axis(side = 1,at = c(0,0.5e6,1e6,1.5e6),labels =  c(0,0.5e6,1e6,1.5e6))
axis(side = 2,at = seq(from = 0,to = 12,by = 0.5),labels = seq(from = 0,to = 12,by = 0.5))
abline(fit.wt)
</code></pre>

<p><img src=""http://i.stack.imgur.com/iqDK2.png"" alt=""enter image description here""></p>

<p>Why is this discrepancy ? Am i missing something here ? I have also checked the std. err which is 0.27, still higher than what is being observed on plot. </p>

<p>Thank you.</p>
"
"0.138323750077661","0.148522131446501","115356","<p>I'm a beginner in statistics and I have to run multilevel logistic regressions. I am confused with the results as they differ from logistic regression with just one level. </p>

<p>I don't know how to interpret the variance and correlation of the random variables. And I wonder how to compute the ICC.</p>

<p>For example : I have a dependent variable about the protection friendship ties give to individuals (1 is for individuals who can rely a lot on their friends, 0 is for the others). There are 50 geographic clusters of respondant and one random variable which is a factor about the social situation of the neighborhood. Upper/middle class is the reference, the other modalities are working class and underprivileged neighborhoods. </p>

<p>I get these results :</p>

<pre><code>&gt; summary(RLM3)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Arp ~ Densite2 + Sexe + Age + Etudes + pcs1 + Enfants + Origine3 +      Sante + Religion + LPO + Sexe * Enfants + Rev + (1 + Strate |  
    Quartier)
   Data: LPE
Weights: PONDERATION
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  3389.9   3538.3  -1669.9   3339.9     2778 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2216 -0.7573 -0.3601  0.8794  2.7833 

Random effects:
 Groups   Name           Variance Std.Dev. Corr       
 Neighb. (Intercept)     0.2021   0.4495              
          Working Cl.    0.2021   0.4495   -1.00      
          Underpriv.     0.2021   0.4495   -1.00  1.00
Number of obs: 2803, groups:  Neigh., 50

Fixed effects:
</code></pre>

<p>The differences with the ""call"" part is due to the fact I translated some words.</p>

<p>I think I understand the relation between the random intercept and the random slope for linear regressions but it is more difficult for logistics ones. I guess that when the correlation is positive, I can conclude that the type of neighborhood (social context) has a positive impact on the protectiveness of friendship ties, and conversely. But how do I quantify that ?</p>

<p>Moreover, I find it odd to get correlation of 1 or -1 and nothing more intermediate.</p>

<p>As for the ICC I am puzzled because I have seen a post about lmer regression that indicates that intraclass correlation can be computed by dividing the variance of the random intercept by the variance of the random intercept, plus the variance the random variables, plus the residuals. </p>

<p>But there are no residuals in the results of a glmer. I have read in a book that ICC must be computed by dividing the random intercept variance by the random intercept variance plus 2.36 (piÂ²/3). But in another book, 2.36 was replaced by the inter-group variance (the first level variance I guess). 
What is the good solution ?</p>

<p>I hope these questions are not too confused.
Thank you for your attention !</p>
"
"0.195619323358134","0.210042012604201","116007","<p>I have a fairly simple dataset looking at the relationship between the first nesting date of a bird in a given year (Date) and the birds overall fledgling production from that year (Fledge; count data from 0-3 fledglings). I want to determine the optimal laying date for this species (i.e. the date where fledgling production is highest); however, I have been struggling to work out which statistical analysis is most appropriate for my data.</p>

<p>Most birds produce no fledglings in a year, so there are many zero values in the data. With this in mind, I thought that a zero inflated poisson regression might be most appropriate. To test this in R, I fitted a regular glm with poisson distribution (model1 below) and a zero inflated poisson model using zeroinfl() from the pscl library (model2 below). I then compared the two using vuong test statistic (output below).</p>

<pre><code>model1&lt;-glm(Fledge~Date,data=OPT,family=""poisson"")
model2&lt;-zeroinfl(Fledge~Date,data=OPT,dist=""poisson"")
vuong(model1,model2)

Vuong Non-Nested Hypothesis Test-Statistic: 4.25169 
(test-statistic is asymptotically distributed N(0,1) under the
null that the models are indistinguishible)
in this case:
model1 &gt; model2, with p-value 1.0608e-05
</code></pre>

<p>According to the vuong output, a regular non-zero inflated poisson regression is most appropriate. This wasn't that surprising as, from my understanding, the zeros in my data are 'true zeroes' i.e. they are legitimate data points and not due to sampling technique or design. Next I tested for overdispersion by fitting a glm with a quasipoisson distribution to check the dispersion parameter (model3).</p>

<pre><code>model3&lt;-glm(Fledge~Date,data=OPT,family=""quasipoisson"")
summary(model3)

Call:
glm(formula = Fledge ~ Date, family = ""quasipoisson"", data = OPT)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7877  -0.6258  -0.5578  -0.4504   3.3648  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept) -0.15109    0.54874  -0.275  0.78315   
Date        -0.03288    0.01133  -2.901  0.00385 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 1.296628)

Null deviance: 455.36  on 642  degrees of freedom
Residual deviance: 443.40  on 641  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 6
</code></pre>

<p>As you can see, the output showed a dispersion parameter close to one (1.297), and a null deviance (455.36) very close to the residual deviance (443.40). So I concluded that overdispersion was not a big problem, and a poisson, rather than negative binomial, distribution was required.</p>

<p>After doing all this, I was fairly confident that my regular poisson regression (model1) was best for my data. HOWEVER, when I plotted the model outputs of the zero inflated and non-zero inflated data, the non-zero inflated model output didn't appear to fit my data at all while the zero inflated model fitted much better.</p>

<pre><code>ggplot(OPT,aes(x=Date,y=Fledge))+
geom_point()+
theme_bw()+
geom_line(data=cbind(OPT,optpred=predict(model1)),aes(y=optpred),size=1,colour=""red"")+
geom_line(data=cbind(OPT,optpred2=predict(model2)),aes(y=optpred2),size=1,colour=""blue)
</code></pre>

<p><img src=""http://i.stack.imgur.com/GFp5F.jpg"" alt=""enter image description here""></p>

<p>As you can see, the non-zero inflated model (red line) doesn't seem to fit the data at all, as it is predicting fledgling production less than 0 on all dates (obviously not biologically possible)! Conversely, the zero inflated model (blue line) seems to fit the data very well. So I have two questions to ask;</p>

<ol>
<li><p>Were the methods I used to test zero inflation (and overdispersion) appropriate and interpreted correctly?</p></li>
<li><p>If so, is my application of a poisson regression appropriate? Or are there other more appropriate options for my data outside of the two I've tried here?</p></li>
</ol>

<p>I've included a dput() of my data below to allow for replication.</p>

<p>Thanks for the help!</p>

<pre><code>structure(list(Date = c(45L, 40L, 42L, 41L, 39L, 34L, 40L, 44L, 
36L, 32L, 33L, 89L, 58L, 50L, 46L, 56L, 48L, 69L, 64L, 56L, 61L, 
58L, 66L, 63L, 57L, 58L, 60L, 65L, 31L, 48L, 42L, 41L, 46L, 38L, 
59L, 41L, 65L, 41L, 34L, 41L, 36L, 60L, 42L, 39L, 43L, 46L, 47L, 
38L, 38L, 71L, 65L, 51L, 42L, 37L, 51L, 41L, 65L, 59L, 44L, 50L, 
51L, 47L, 40L, 53L, 56L, 62L, 50L, 46L, 51L, 55L, 50L, 46L, 45L, 
39L, 36L, 52L, 50L, 73L, 42L, 38L, 51L, 49L, 43L, 45L, 44L, 76L, 
68L, 65L, 70L, 56L, 40L, 45L, 49L, 52L, 66L, 80L, 45L, 42L, 44L, 
37L, 48L, 43L, 53L, 31L, 47L, 49L, 44L, 46L, 54L, 55L, 48L, 53L, 
55L, 72L, 54L, 45L, 83L, 59L, 48L, 47L, 52L, 72L, 51L, 70L, 48L, 
44L, 42L, 38L, 48L, 43L, 45L, 39L, 45L, 42L, 64L, 46L, 56L, 34L, 
50L, 48L, 47L, 47L, 60L, 50L, 61L, 40L, 72L, 63L, 55L, 66L, 69L, 
66L, 61L, 60L, 60L, 40L, 70L, 45L, 40L, 41L, 42L, 71L, 54L, 45L, 
52L, 48L, 40L, 39L, 49L, 42L, 43L, 53L, 38L, 53L, 52L, 68L, 61L, 
62L, 87L, 41L, 45L, 37L, 44L, 45L, 43L, 72L, 39L, 56L, 34L, 74L, 
62L, 46L, 43L, 47L, 35L, 54L, 61L, 44L, 49L, 54L, 61L, 37L, 51L, 
48L, 52L, 48L, 48L, 44L, 45L, 44L, 45L, 68L, 61L, 87L, 51L, 52L, 
50L, 50L, 56L, 55L, 56L, 57L, 65L, 41L, 63L, 76L, 52L, 62L, 50L, 
50L, 54L, 63L, 48L, 54L, 46L, 57L, 54L, 52L, 45L, 41L, 54L, 74L, 
69L, 68L, 51L, 60L, 54L, 44L, 67L, 52L, 49L, 43L, 41L, 44L, 49L, 
46L, 43L, 46L, 49L, 46L, 47L, 54L, 55L, 67L, 52L, 55L, 52L, 49L, 
50L, 51L, 57L, 48L, 34L, 54L, 49L, 47L, 71L, 62L, 43L, 45L, 45L, 
49L, 58L, 57L, 55L, 54L, 52L, 51L, 41L, 54L, 70L, 52L, 53L, 53L, 
50L, 71L, 56L, 48L, 33L, 43L, 41L, 68L, 42L, 38L, 39L, 46L, 55L, 
64L, 62L, 56L, 69L, 44L, 49L, 54L, 86L, 46L, 46L, 50L, 44L, 45L, 
55L, 55L, 52L, 49L, 49L, 56L, 41L, 34L, 50L, 62L, 39L, 41L, 56L, 
42L, 40L, 43L, 44L, 45L, 43L, 48L, 41L, 45L, 62L, 49L, 47L, 49L, 
63L, 69L, 46L, 53L, 49L, 59L, 54L, 33L, 46L, 44L, 49L, 36L, 41L, 
33L, 66L, 56L, 67L, 43L, 66L, 31L, 51L, 59L, 57L, 51L, 39L, 44L, 
31L, 40L, 39L, 42L, 27L, 43L, 42L, 78L, 60L, 70L, 64L, 67L, 66L, 
67L, 66L, 62L, 58L, 51L, 50L, 60L, 38L, 45L, 34L, 69L, 38L, 45L, 
39L, 44L, 39L, 44L, 43L, 46L, 37L, 59L, 74L, 59L, 39L, 43L, 40L, 
38L, 45L, 45L, 42L, 36L, 33L, 51L, 64L, 52L, 40L, 89L, 49L, 37L, 
51L, 70L, 65L, 71L, 62L, 61L, 68L, 59L, 54L, 75L, 57L, 55L, 58L, 
52L, 58L, 45L, 50L, 41L, 64L, 49L, 50L, 67L, 54L, 43L, 49L, 54L, 
55L, 53L, 53L, 59L, 47L, 47L, 48L, 45L, 50L, 39L, 48L, 51L, 54L, 
44L, 43L, 56L, 51L, 38L, 71L, 62L, 56L, 65L, 69L, 68L, 52L, 47L, 
47L, 47L, 80L, 51L, 48L, 36L, 32L, 39L, 45L, 31L, 43L, 57L, 65L, 
60L, 62L, 36L, 53L, 64L, 57L, 43L, 71L, 66L, 63L, 49L, 39L, 49L, 
43L, 32L, 47L, 44L, 35L, 35L, 41L, 54L, 50L, 44L, 44L, 48L, 50L, 
41L, 40L, 46L, 48L, 38L, 43L, 54L, 52L, 36L, 62L, 72L, 47L, 66L, 
50L, 51L, 50L, 56L, 47L, 67L, 50L, 35L, 40L, 43L, 42L, 31L, 35L, 
43L, 46L, 45L, 46L, 39L, 40L, 40L, 39L, 36L, 45L, 43L, 44L, 44L, 
44L, 38L, 49L, 52L, 49L, 43L, 42L, 47L, 56L, 51L, 51L, 51L, 59L, 
64L, 46L, 40L, 75L, 65L, 51L, 91L, 56L, 83L, 56L, 57L, 58L, 51L, 
50L, 56L, 40L, 69L, 54L, 45L, 35L, 41L, 48L, 60L, 54L, 39L, 39L, 
31L, 92L, 39L, 66L, 56L, 48L, 44L, 40L, 42L, 47L, 51L, 47L, 45L, 
49L, 69L, 48L, 42L, 58L, 56L, 58L, 61L, 42L, 36L, 47L, 52L, 45L, 
54L, 55L, 62L, 48L, 44L, 54L, 51L, 46L, 44L, 50L, 37L, 33L, 40L, 
57L, 54L, 64L, 59L, 69L, 46L, 40L, 51L, 53L, 79L, 60L), Fledge = c(1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 3L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 
0L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 2L, 0L, 0L, 0L, 0L, 
0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 1L, 2L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 
0L, 1L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 3L, 0L, 0L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
2L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 2L, 1L, 2L, 0L, 0L, 0L, 1L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 3L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 2L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 
0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 2L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
2L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 2L, 0L, 0L, 0L, 1L, 0L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 1L, 1L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L)), .Names = c(""Date"", ""Fledge""), class = ""data.frame"", row.names = c(NA, 
    -643L))
</code></pre>
"
"0.275583939363875","0.277859465113106","116562","<p>I recently started transitioning from JMP to R and to get started, I've been trying to reproduce some of my old JMP results in R. However, when I run a multiple regression with one continuous variable (income) and one categorical variable (condition) predicting a continuous variable (psc), the results from the 2 programs differ.</p>

<p>Here's my JMP model and results:
<img src=""http://i.stack.imgur.com/ZQnL8.png"" alt=""JMP model""></p>

<p><img src=""http://i.stack.imgur.com/Rrmyv.png"" alt=""JMP results""></p>

<p>And here's my R code and results:</p>

<pre><code>&gt; library(plyr)

&gt; # load data files
&gt; online &lt;- read.csv('r_online.csv')
&gt; paper &lt;- read.csv('r_paper.csv')

&gt; # define conditions for online data
&gt; online$condition &lt;- NA
&gt; levels(online$condition) &lt;- c('wc','fd')

&gt; online[!is.na(online$Ntrl1), 'condition'] &lt;- 'wc'
&gt; online[!is.na(online$Ntrl3), 'condition'] &lt;- 'fd'

&gt; online$condition &lt;- factor(online$condition)

&gt; # merge online and paper data
&gt; mydata &lt;- rbind.fill(online, paper)

&gt; # exclude dropped data
&gt; mydata &lt;- subset(mydata, Class &lt; 5)

&gt; # calcualte psc
&gt; psc &lt;- ((8-mydata$PSF1r)+(8-mydata$PSF2r)+mydata$PSF3+(8-mydata$PSF4r)+(8-mydata$PSF5r)+mydata$PSF6)/6
&gt; mydata$psc &lt;- psc

&gt; # save income and condition as values
&gt; income &lt;- mydata$Income
&gt; condition &lt;-mydata$condition

&gt; # psc by income and condition
&gt; psc.income.regress &lt;- lm(psc ~ income * condition)
&gt; summary(psc.income.regress)

Call:
lm(formula = psc ~ income * condition)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.7275 -0.3585  0.0731  0.5122  1.1602 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         3.89116    0.50804   7.659 1.96e-09 ***
income              0.13393    0.07494   1.787   0.0813 .  
conditionwc        -1.53409    0.69323  -2.213   0.0325 *  
income:conditionwc  0.21807    0.10291   2.119   0.0402 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7742 on 41 degrees of freedom
  (4 observations deleted due to missingness)
Multiple R-squared:  0.4149,    Adjusted R-squared:  0.3721 
F-statistic:  9.69 on 3 and 41 DF,  p-value: 5.854e-05
</code></pre>

<p>So, R-squared, R-squared Adjusted, Overall F, Overall p, and p and t for the interaction are the same in both R and JMP, but p and t for the main effects and all of the Estimates are different.</p>

<p>I did some reading and found that this occurs because JMP calculates Type-III sums of squares, while R calculates Type-I SS. So far, though, I haven't been able to figure out how to get R to calculate Type-III SS in the same way as JMP.</p>

<p>One site said that I could get Type-III SS by changing the last part of my R code to this:</p>

<pre><code>&gt; ### alternative method suggested for getting type-III SS ###
&gt; options(contrasts=c(""contr.sum"",""contr.poly""))
&gt; psc.income.regress &lt;- lm(psc ~ income * condition)
&gt; drop1(psc.income.regress,~.,test=""F"")

Single term deletions

Model:
psc ~ income * condition
                 Df Sum of Sq    RSS      AIC F value    Pr(&gt;F)    
&lt;none&gt;                        24.576 -19.2208                      
income            1   13.3659 37.941  -1.6778 22.2985 2.729e-05 ***
condition         1    2.9354 27.511 -16.1434  4.8972   0.03253 *  
income:condition  1    2.6917 27.267 -16.5438  4.4906   0.04018 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1


&gt; summary(psc.income.regress)

Call:
lm(formula = psc ~ income * condition)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.7275 -0.3585  0.0731  0.5122  1.1602 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        3.12412    0.34662   9.013 2.82e-11 ***
income             0.24297    0.05145   4.722 2.73e-05 ***
condition1         0.76704    0.34662   2.213   0.0325 *  
income:condition1 -0.10904    0.05145  -2.119   0.0402 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7742 on 41 degrees of freedom
  (4 observations deleted due to missingness)
Multiple R-squared:  0.4149,    Adjusted R-squared:  0.3721 
F-statistic:  9.69 on 3 and 41 DF,  p-value: 5.854e-05
</code></pre>

<p>Now all the estimates for income, interaction, and the intercept are the same in R as in JMP, but condition is still different.</p>

<p>Another person suggested that I recode my conditions as numeric contrasts (instead of having them as factors) and center everything, so I changed the end of my code to this:</p>

<pre><code>&gt; ### 2nd alternative method: change condition to numeric contrast and center variables ###
&gt; condition_c &lt;- ifelse(condition == 'fd', +.5, -.5)
&gt; condition_c &lt;- scale(condition_c, scale=F,center=T)
&gt; income_c &lt;- scale(income,scale=F,center=T)
&gt; psc.income.regress &lt;- lm(psc ~ income_c * condition_c)
&gt; summary(psc.income.regress)

Call:
lm(formula = psc ~ income_c * condition_c)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.7275 -0.3585  0.0731  0.5122  1.1602 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)           4.67891    0.11588  40.379  &lt; 2e-16 ***
income_c              0.22739    0.05241   4.338 9.13e-05 ***
condition_c           0.14813    0.23615   0.627   0.5340    
income_c:condition_c -0.21807    0.10291  -2.119   0.0402 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7742 on 41 degrees of freedom
  (4 observations deleted due to missingness)
Multiple R-squared:  0.4149,    Adjusted R-squared:  0.3721 
F-statistic:  9.69 on 3 and 41 DF,  p-value: 5.854e-05
</code></pre>

<p>Doing this, the p and t values for the interaction and condition become the same as in JMP, but now income and all the estimates are different.</p>

<p>I've tried to be as thorough as I can in trying finding an answer on my own, but I've run out of ideas so any help would be immensely appreciated. All relevant R files can be found here: <a href=""https://www.dropbox.com/s/eoup5im2iko1ro6/R.zip?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/eoup5im2iko1ro6/R.zip?dl=0</a></p>
"
"0.0711342994029579","0.0875175052517506","116825","<p>I'm exploring linear regressions in R and Python, and usually get the same results but this is an instance I do not. </p>

<p>I added the sum of <code>Agriculture</code> and <code>Education</code> to the <code>swiss</code> dataset as an additional explanatory variable, with <code>Fertility</code> as the regressor.</p>

<p>R gives me an <code>NA</code> for the $\beta$ value of <code>z</code>, but Python gives me a numeric value for <code>z</code> and a warning about a very small eigenvalue. Is there a way to make Python and statmodels explicitly tell me that <code>z</code> adds no information to the regressor?</p>

<p>Additionally, I originally did this analysis in an iPython notebook, where there is no need to do an explicit <code>print</code> of the regression summary results <code>reg_results</code>, and when the <code>print</code> command is omitted there is no warning about the low eigenvalues which makes it more difficult to know that <code>z</code> is worthless.</p>

<p>R code:</p>

<pre><code>data(swiss)
swiss$z &lt;- swiss$Agriculture + swiss$Education
formula &lt;- 'Fertility ~ .'
print(lm(formula, data=swiss))
</code></pre>

<p>R output:</p>

<pre><code>Call:
lm(formula = formula, data = swiss)

Coefficients:
     (Intercept)       Agriculture       Examination         Education
         66.9152           -0.1721           -0.2580           -0.8709
        Catholic  Infant.Mortality                 z
          0.1041            1.0770                NA
</code></pre>

<p>Python Code:</p>

<pre><code>import statsmodels.formula.api as sm
import pandas.rpy.common as com

swiss = com.load_data('swiss')

# get rid of periods in column names
swiss.columns = [_.replace('.', '_') for _ in swiss.columns]

# add clearly duplicative data
swiss['z'] = swiss['Agriculture'] + swiss['Education']

y = 'Fertility'
x = ""+"".join(swiss.columns - [y])
formula = '%s ~ %s' % (y, x)
reg_results = sm.ols(formula, data=swiss).fit().summary()
print(reg_results)
</code></pre>

<p>Python output:</p>

<pre><code>                            OLS Regression Results
==============================================================================
Dep. Variable:              Fertility   R-squared:                       0.707
Model:                            OLS   Adj. R-squared:                  0.671
Method:                 Least Squares   F-statistic:                     19.76
Date:                Thu, 25 Sep 2014   Prob (F-statistic):           5.59e-10
Time:                        22:55:42   Log-Likelihood:                -156.04
No. Observations:                  47   AIC:                             324.1
Df Residuals:                      41   BIC:                             335.2
Df Model:                           5
====================================================================================
                       coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------------
Intercept           66.9152     10.706      6.250      0.000        45.294    88.536
Agriculture          0.1756      0.062      2.852      0.007         0.051     0.300
Catholic             0.1041      0.035      2.953      0.005         0.033     0.175
Education           -0.5233      0.115     -4.536      0.000        -0.756    -0.290
Examination         -0.2580      0.254     -1.016      0.315        -0.771     0.255
Infant_Mortality     1.0770      0.382      2.822      0.007         0.306     1.848
z                   -0.3477      0.073     -4.760      0.000        -0.495    -0.200
==============================================================================
Omnibus:                        0.058   Durbin-Watson:                   1.454
Prob(Omnibus):                  0.971   Jarque-Bera (JB):                0.155
Skew:                          -0.077   Prob(JB):                        0.925
Kurtosis:                       2.764   Cond. No.                     1.11e+08
==============================================================================

Warnings:
[1] The smallest eigenvalue is 3.87e-11. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
</code></pre>

<p>```</p>
"
"0.0924061655450546","0.0909508593886249","117450","<p>I have a confusing situation where I have strongly conflicting results from two ways of analyzing my simple data. I measure two binary variables from each participant, AestheticOnly and ChoiceVA. I want to know if AestheticOnly depends on ChoiceVA and whether this relation is different in two different experiments. Here is my participant count data:</p>

<pre><code>Experiment 1
                 AestheticOnly
                 0   1  All
ChoiceVA A      35   6   41
         V      20  13   33
         All    55  19   74

Experiment 2
                 AestheticOnly
                 0   1  All
ChoiceVA A      12  10   22
         V      31  11   42
         All    43  21   64
</code></pre>

<p>I run a logistic regression where AestheticOnly is modelled by ChoiceVA, Experiment, and the interaction:</p>

<pre><code>&gt; mod &lt;- glm( AestheticOnly ~ ChoiceVA*Experiment, data = d, family=binomial)
&gt; summary(mod)

Call:
glm(formula = AestheticOnly ~ ChoiceVA * Experiment, family = binomial, 
    data = d)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1010  -0.7793  -0.5625   1.2557   1.9605  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -3.3449     0.9820  -3.406 0.000659 ***
ChoiceVAV              3.5194     1.2630   2.787 0.005327 ** 
Experiment             1.5813     0.6153   2.570 0.010170 *  
ChoiceVAV:Experiment  -2.1866     0.7929  -2.758 0.005820 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 166.16  on 137  degrees of freedom
Residual deviance: 157.01  on 134  degrees of freedom
AIC: 165.01

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Apparently all factors are significant. But, this just doesn't make sense to me. For example, looking at the main effect of experiment should be equivalent to performing a Fisher's Exact test comparing 55 and 19 with 43 and 21 (bottom lines of each table). This is obviously not significant (p=.452). So why does the regression model give such a different result? Any help much appreciated.</p>
"
"0.138323750077661","0.136145287159293","117593","<p>This is a really simple problem I am having, yet for the life of me I can't find a solution searching around. In theory I can simply recode the data, but that is an extreme solution I would rather not use if I don't have to. </p>

<p>I am simply trying to do a logistic regression with an ordered factor as my predictor. For a toy data set, consider:</p>

<pre><code>  radiation leukemia other total
1         0       13   378   391
2       1-9        5   200   205
3     10-49        5   151   156
4     50-99        3    47    50
5   100-199        4    31    35
6       200       18    33    51
</code></pre>

<p>I want to execute the following:</p>

<pre><code>glm(cbind(leukemia,other)~radiation,data=leuk,family=binomial(""logit""))
</code></pre>

<p>That is, leukemia are the ""successes"" and other are the ""failures"". Basically, trying to predict dose-response relationship between radiation and the proportional mortality rates for leukemia. However, this model is oversaturated:</p>

<pre><code>Call:  glm(formula = cbind(leukemia, other) ~ radiation, family = binomial(""logit""), 
    data = leuk)

Coefficients:
     (Intercept)      radiation1-9    radiation10-49  radiation100-199  
         -3.3699           -0.3189           -0.0379            1.3223  
    radiation200    radiation50-99  
          2.7638            0.6184  

Degrees of Freedom: 5 Total (i.e. Null);  0 Residual
Null Deviance:      54.35 
Residual Deviance: -3.331e-15   AIC: 33.67
</code></pre>

<p>I don't want each level of radiation as a factor to be its own predictor variable; that makes no sense, especially when you only have a small number of data points (note, this isn't actually the real data I am using, this is just a toy example that is similar). In any case, how do I force R to simply consider the factor radiation as a single variable with multiple levels? For example, if I do the following:</p>

<pre><code>x&lt;-c(0,1,2,3,4,5)
glm(cbind(leukemia,other)~x,data=leuk,family=binomial(""logit""))

Call:  glm(formula = cbind(leukemia, other) ~ x, family = binomial(""logit""), 
    data = leuk)

Coefficients:
(Intercept)            x  
    -3.9116       0.5731  

Degrees of Freedom: 5 Total (i.e. Null);  4 Residual
Null Deviance:      54.35 
Residual Deviance: 10.18        AIC: 35.84
</code></pre>

<p>This is more in line with what I want. But I am nervous about using that x variable in the regression for fear of changing the interpretation of the results. Similarly, I'd prefer to avoid an irritating system of dummy variables. </p>

<p>How do I go about doing this? Or is there a better workaround altogether for studying this type of relationship that I am not considering?</p>
"
"0.125748863706964","0.123768442872084","117631","<p>I measure two binary responses from each participant (ChoiceVA = V or A, AestheticOnly = 0 or 1). There are two experiments (between-participant). I want to test the following hypotheses:</p>

<p>AestheticOnly depends on Experiment (main effect)
AestheticOnly depends on ChoiceVA (main effect)
The way AestheticOnly depends on Experiment depends on ChoiceVA (interaction)</p>

<p>Here is my data. The first number in each cell is the proportion of participants scoring 1 for AestheticOnly, and the second number is the n for participants in that cell.</p>

<pre><code>                         ChoiceVA               
                        A       V     All

Experiment  1      0.1463  0.3939  0.2568
                       41      33      74

            2      0.4545  0.2619  0.3281
                       22      42      64

            All    0.2540  0.3200  0.2899
                       63      75     138
</code></pre>

<p>Just from looking at the data it is pretty obvious that neither main effect is significant (e.g. for ChoiceVA, bottom row, .25 of 63 participants is not significantly different from .32 of 75 participants). In my naivity I thought perhaps I could test these hypotheses with a straightforward binary logistic regression:</p>

<pre><code>&gt; mod &lt;- glm( AestheticOnly ~ Experiment+ChoiceVA+Experiment*ChoiceVA, data = d, family=binomial )
&gt; summary(mod)

Call:
glm(formula = AestheticOnly ~ Experiment + ChoiceVA + Experiment *
    ChoiceVA, family = binomial, data = d)

Deviance Residuals:
    Min       1Q   Median       3Q      Max 
-1.1010  -0.7793  -0.5625   1.2557   1.9605 

Coefficients:
                      Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)            -1.7636     0.4419  -3.991 6.57e-05 ***
Experiment2             1.5813     0.6153   2.570  0.01017 * 
ChoiceVAV               1.3328     0.5676   2.348  0.01887 * 
Experiment2:ChoiceVAV  -2.1866     0.7929  -2.758  0.00582 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 166.16  on 137  degrees of freedom
Residual deviance: 157.01  on 134  degrees of freedom
AIC: 165.01

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Clearly, the main effects are not being tested here in the way I hoped. I believe that this model, in testing main effects, rather than testing e.g. ChoiceVA=A against ChoiceVA=V across both levels of Experiment, is confining itself to that comparison only when Experiment=1. Can a model be constructed that instead tests the main effects in the way I would like?</p>

<p>This is related to a previous question (<a href=""http://stats.stackexchange.com/questions/117450/logistic-regression-gives-very-different-result-to-fishers-exact-test-why"">Logistic regression gives very different result to Fisher&#39;s exact test - why?</a>), but when I asked it I understand this even worse than I do now and consequently the question was so unclear that I need to start again.</p>
"
"0.144978720397083","0.154586735600211","118394","<p>I have a dataset in which individuals were assessed at two time points during the study on a cognitive test, as such I was wondering which statistical model would be more appropriate for my data, either linear regression or mixed effects models? </p>

<p>The average length of follow up for my data is 59 months with a standard deviation of 43.03 (range is 0.63-167 months) with 88 (33%) of people having data for only one time point. </p>

<p>For linear regression, the approach I was thinking utilising was taking the delta of the test score between the two time points and regressing that against time (months between test scores). </p>

<p>If I used mixed effects models, the main issue I have is how to handle individuals who have only wave of data? While I know mixed effects models are especially robust in regards to the analysis of unbalanced data, would 33% missingnes cause issues?</p>

<p>Just sample R code highlighting the output using either linear regression or mixed models.</p>

<pre><code>fm1 &lt;- lm(mmse_difference ~ mmse_months_between*ORgrs_apoe, data = dat.wide)
summary(fm1)

Call:
lm(formula = mmse_difference ~ mmse_months_between * ORgrs_apoe, 
    data = newdat)

Residuals:
    Min      1Q  Median      3Q     Max 
-20.960  -3.957   1.854   5.200  12.550 

Coefficients:
                               Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)                    -2.74185    2.20667  -1.243    0.216
mmse_months_between            -0.01768    0.03051  -0.579    0.563
ORgrs_apoe                      0.35163    1.17782   0.299    0.766
mmse_months_between:ORgrs_apoe -0.01973    0.01748  -1.129    0.261

Residual standard error: 7.3 on 170 degrees of freedom
  (88 observations deleted due to missingness)
Multiple R-squared:  0.08481,   Adjusted R-squared:  0.06866 
F-statistic: 5.251 on 3 and 170 DF,  p-value: 0.001725
Num. obs. 174

fm2 &lt;- lme(mmse ~ mmse_months*ORgrs_apoe, random = ~mmse_months|patientid, data = dat.long, method = ""ML"", na.action = na.exclude)
summary(fm2)
Linear mixed-effects model fit by maximum likelihood
 Data: dat.long 
       AIC      BIC    logLik
  2797.467 2829.537 -1390.733

Random effects:
 Formula: ~mmse_months | patientid
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr  
(Intercept) 7.2972822 (Intr)
mmse_months 0.1132399 0.85  
Residual    2.9431616       

Fixed effects: mmse ~ mmse_months * ORgrs_apoe 
                           Value Std.Error  DF   t-value p-value
(Intercept)            24.635821 1.0959420 231 22.479130  0.0000
mmse_months            -0.069918 0.0223198 172 -3.132544  0.0020
ORgrs_apoe             -1.283348 0.6062892 231 -2.116726  0.0354
mmse_months:ORgrs_apoe -0.024952 0.0130561 172 -1.911103  0.0577
 Correlation: 
                       (Intr) mms_mn ORgrs_
mmse_months             0.438              
ORgrs_apoe             -0.882 -0.377       
mmse_months:ORgrs_apoe -0.357 -0.891  0.397

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-3.48643949 -0.31734164  0.07636708  0.26575764  2.49901891 

Number of Observations: 407
Number of Groups: 233 
</code></pre>

<p>Thanks.     </p>
"
"0.195030550545354","0.155395390127015","120549","<p>It is a basic question but I could not find clear answer on my reading. I am trying to find independent predictors of Infant.Mortality in data frame 'swiss' in R. </p>

<pre><code>&gt; head(swiss)
             Fertility Agriculture Examination Education Catholic Infant.Mortality
Courtelary        80.2        17.0          15        12     9.96             22.2
Delemont          83.1        45.1           6         9    84.84             22.2
Franches-Mnt      92.5        39.7           5         5    93.40             20.2
Moutier           85.8        36.5          12         7    33.77             20.3
Neuveville        76.9        43.5          17        15     5.16             20.6
Porrentruy        76.1        35.3           9         7    90.57             26.6
</code></pre>

<p>Following are the results using lm and I find only Fertility to be a significant predictor: </p>

<pre><code>&gt; fit = lm(Infant.Mortality~., data=swiss)
&gt; summary(fit)

Call:
lm(formula = Infant.Mortality ~ ., data = swiss)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.2512 -1.2860  0.1821  1.6914  6.0937 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  8.667e+00  5.435e+00   1.595  0.11850
Fertility    1.510e-01  5.351e-02   2.822  0.00734    #  &lt;&lt;&lt;&lt; NOTE P VALUE HERE
Agriculture -1.175e-02  2.812e-02  -0.418  0.67827
Examination  3.695e-02  9.607e-02   0.385  0.70250
Education    6.099e-02  8.484e-02   0.719  0.47631
Catholic     6.711e-05  1.454e-02   0.005  0.99634

Residual standard error: 2.683 on 41 degrees of freedom
Multiple R-squared:  0.2439,    Adjusted R-squared:  0.1517 
F-statistic: 2.645 on 5 and 41 DF,  p-value: 0.03665
</code></pre>

<p>Following are the graphs:</p>

<pre><code>plot(fit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/lopHb.png"" alt=""enter image description here""></p>

<p>On performing stepwise regression, following are the results: </p>

<pre><code>&gt; step &lt;- stepAIC(fit, direction=""both""); 
Start:  AIC=98.34
Infant.Mortality ~ Fertility + Agriculture + Examination + Education + 
    Catholic

              Df Sum of Sq    RSS     AIC
- Catholic     1     0.000 295.07  96.341
- Examination  1     1.065 296.13  96.511
- Agriculture  1     1.256 296.32  96.541
- Education    1     3.719 298.79  96.930
&lt;none&gt;                     295.07  98.341
- Fertility    1    57.295 352.36 104.682

Step:  AIC=96.34
Infant.Mortality ~ Fertility + Agriculture + Examination + Education

              Df Sum of Sq    RSS     AIC
- Examination  1     1.320 296.39  94.551
- Agriculture  1     1.395 296.46  94.563
- Education    1     5.774 300.84  95.252
&lt;none&gt;                     295.07  96.341
+ Catholic     1     0.000 295.07  98.341
- Fertility    1    72.609 367.68 104.681

Step:  AIC=94.55
Infant.Mortality ~ Fertility + Agriculture + Education

              Df Sum of Sq    RSS     AIC
- Agriculture  1     4.250 300.64  93.220
- Education    1     6.875 303.26  93.629
&lt;none&gt;                     296.39  94.551
+ Examination  1     1.320 295.07  96.341
+ Catholic     1     0.255 296.13  96.511
- Fertility    1    79.804 376.19 103.758

Step:  AIC=93.22
Infant.Mortality ~ Fertility + Education

              Df Sum of Sq    RSS     AIC
&lt;none&gt;                     300.64  93.220
- Education    1    21.902 322.54  94.525
+ Agriculture  1     4.250 296.39  94.551
+ Examination  1     4.175 296.46  94.563
+ Catholic     1     2.318 298.32  94.857
- Fertility    1    85.769 386.41 103.017
&gt; 
&gt; 
&gt; step$anova
Stepwise Model Path 
Analysis of Deviance Table

Initial Model:
Infant.Mortality ~ Fertility + Agriculture + Examination + Education + 
    Catholic

Final Model:
Infant.Mortality ~ Fertility + Education


           Step Df     Deviance Resid. Df Resid. Dev      AIC
1                                      41   295.0662 98.34145
2    - Catholic  1 0.0001533995        42   295.0663 96.34147
3 - Examination  1 1.3199421028        43   296.3863 94.55125
4 - Agriculture  1 4.2499886025        44   300.6363 93.22041
&gt; 
&gt; 
</code></pre>

<p>Summary shows Education also has trend towards significant association: </p>

<pre><code>summary(step)

Call:
lm(formula = Infant.Mortality ~ Fertility + Education, data = swiss)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.6927 -1.4049  0.2218  1.7751  6.1685 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  8.63758    3.33524   2.590 0.012973
Fertility    0.14615    0.04125   3.543 0.000951
Education    0.09595    0.05359   1.790 0.080273

Residual standard error: 2.614 on 44 degrees of freedom
Multiple R-squared:  0.2296,    Adjusted R-squared:  0.1946 
F-statistic: 6.558 on 2 and 44 DF,  p-value: 0.003215
</code></pre>

<p>What do I conclude? Is Education an important predictor or not?</p>

<p>Also, do the graphs using plot(fit) add any significant information?</p>

<p>Thanks for your help.</p>

<hr>

<p>Edit: 
I ran shapiro test on all columns and found 2 are not normally distributed: </p>

<pre><code>Fertility : P= 0.3449466 (Normally distributed) 
Agriculture : P= 0.1930223 (Normally distributed) 
Examination : P= 0.2562701 (Normally distributed) 
Education : P= 1.31202e-07 (--- NOT Normally distributed! ---) 
Catholic : P= 1.20461e-07 (--- NOT Normally distributed! ---) 
Infant.Mortality : P= 0.4978056 (Normally distributed) 
</code></pre>

<p>Does that make a difference? </p>
"
"0.242654137442641","0.231369055685281","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.179605302026775","0.166378066161541","122336","<p>I have a problem with coding of a 2-level categorical predictor variable in R, and subsequently using it as a random slope in lmer().</p>

<p>I can keep the factor as numeric, coded using the treatment coding:</p>

<pre><code>&gt; unique (b$multi)
[1] 0 1
</code></pre>

<p>Running lmer() using a dataset coded in this way yields:</p>

<pre><code>&gt; l1 = glmer(OK ~ multi + (0 + multi|item) + (1|subject)+ (1|item), family=""binomial"", data=b)
&gt; summary(l1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: OK ~ multi + (0 + multi | item) + (1 | subject) + (1 | item)
   Data: b

     AIC      BIC   logLik deviance df.resid 
  4806.5   4838.9  -2398.3   4796.5     4792 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-7.8294 -0.5560 -0.1548  0.5623 14.3342 

Random effects:
 Groups  Name        Variance Std.Dev.
 subject (Intercept) 1.84379  1.3579  
 item    (Intercept) 2.40306  1.5502  
 item.1  multi       0.04145  0.2036  
Number of obs: 4797, groups:  subject, 123; item, 39
[...]
</code></pre>

<p>Above there is only one random slope related to <code>multi</code>. However, something very different happens when I convert the variable into a factor:</p>

<pre><code>&gt; b$multi = as.factor(b$multi)
&gt; levels (b$multi)
[1] ""0"" ""1""
</code></pre>

<p>When I fit a model using <code>multi</code> as a random slope variable:</p>

<blockquote>
  <p>l2 = glmer(OK ~ multi + (0+multi|item) + (1|subject)+ (1|item), family=""binomial"", data=b)
      Warning message:
      In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
        Model failed to converge: degenerate  Hessian with 1 negative eigenvalues</p>
</blockquote>

<p>... the model fails to converge and I get a very different random effects structure:</p>

<pre><code>&gt; summary(l2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: OK ~ multi + (0 + multi | item) + (1 | subject) + (1 | item)
   Data: b

     AIC      BIC   logLik deviance df.resid 
  4807.8   4853.1  -2396.9   4793.8     4790 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-8.3636 -0.5608 -0.1540  0.5627 15.2515 

Random effects:
 Groups  Name        Variance Std.Dev. Corr
 subject (Intercept) 1.8375   1.3555       
 item    (Intercept) 0.9659   0.9828       
 item.1  multi0      1.5973   1.2638       
         multi1      1.0224   1.0111   1.00
Number of obs: 4797, groups:  subject, 123; item, 39
[...]
</code></pre>

<p>The number of parameters in the model clearly change (reflected by the change in AIC, etc.), and I get two random slopes. </p>

<p>My question is which way of coding the categorical variable is better? Intuition tells me that it is the first one, but I have seen recommendations for both ways of coding in various tutorials and classes about running GLMMs in R and this is why it baffles me. Both types of the predictor variable work identically in ordinary regression using lm().</p>
"
"0.0974046509883157","0.076696498884737","122704","<p>I have used auto.arima to fit a time series model (a linear regression with ARIMA errors, as described <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">on Rob Hyndman's site</a> )  When finished - the output reports that the best model has a (5,1,0) with drift structure - and reports back values of information criteria as </p>

<p>AIC:  2989.2
AICC:  2989.3
BIC: 3261.2</p>

<p>When I use Arima to fit a model with a (1,1,1) with drift structure - the output reports back noticeably lower IC's of</p>

<p>AIC:  2510.3
AICC:  2510.4
BIC:  2759</p>

<p>I can force auto.arima to consider the (1,1,1) with drift model (using the start.p and start.q parameters), and when I do that, and set ""trace=TRUE"" - I do see that the (1,1,1) with drift model is considered, but rejected, by auto.arima.  It still reports back the (5,1,0) with drift model as the best result.</p>

<p>Are there circumstances when auto.arima uses other criteria to choose between models?</p>

<p>Edited to add (in response to request)</p>

<p>Data for this example can be found at <a href=""https://drive.google.com/file/d/0B6afOuS0y79aenBMeFYyWVNwUUU/view?usp=sharing"" rel=""nofollow"">this Google spreadsheet</a></p>

<p>and R code to reproduce the example is</p>

<pre><code>repro = read.csv(""mindata.csv"")
reprots = ts(repro, start=1, frequency=24)
fitauto = auto.arima(reprots[,""lnwocone""],
xreg=cbind(fourier(reprots[,""lnwocone""], K=11),
reprots[,c(""temp"",""sqt"",""humidity"",""windspeed"",""mist"",""rain"")]),
start.p=1, start.q=1, trace=TRUE, seasonal=FALSE)
fitdirect &lt;- Arima(reprots[,""lnwocone""], order=c(1,1,1), seasonal=c(0,0,0),
xreg=cbind(fourier(reprots[,""lnwocone""], K=11),
reprots[,c(""temp"",""sqt"",""humidity"",""windspeed"",""mist"",""rain"")]), include.drift=TRUE)
summary(fitauto)
summary(fitdirect)
</code></pre>

<p>Apologies if the Google docs data - inline code is not the best way to provide the example.  I think I have seen in the past guidelines on the best way to do this - but could not locate those guidelines in searching this morning.</p>
"
"0.0974046509883157","0.0958706236059213","123511","<p>EDIT: SOLVED <em>The problem seems to have been an explanatory variable that was a factor. If it is made binary numeric insted, the values of BIC and AIC is calculated alright. However, the analyses give the same parameter estimates such as z-values, variances, p-values. I leave the original post as a reference.</em></p>

<p><strong>Original post:</strong></p>

<p>I have some problems calculating AIC or BIC values for a structural equation model (SEM) fitted with Lavaan. (I fail to find anything when searching the web for the error message or parts of it.)</p>

<p>My model:</p>

<pre><code>model&lt;- '
        dependant ~ var1 + var2 + var3
        var1 ~ var2
        var3 ~ var1 + var2
        '
semla&lt;-sem(model, data=dat, missing=""listwise""
summary(semla)
</code></pre>

<p>With the result:</p>

<pre><code>lavaan (0.5-16) converged normally after  38 iterations

                                              Used       Total
Number of observations                            47          59

Estimator                                         ML
Minimum Function Test Statistic                0.000
Degrees of freedom                                 0
P-value (Chi-square)                           0.000

Parameter estimates:

Information                                 Expected
Standard Errors                             Standard

               Estimate  Std.err  Z-value  P(&gt;|z|)
Regressions:
dependant ~
 var1            0.121    0.071    1.687    0.092
 var2            0.015    0.006    2.750    0.006
 var3            1.721    2.134    0.807    0.420
var1 ~
 var2           -0.018    0.011   -1.663    0.096
var3 ~
 var1           -0.003    0.005   -0.685    0.493
 var2            0.000    0.000    0.450    0.653

Variances:
 dependant       0.049    0.010
 var1            0.205    0.042
 var3            0.000    0.000
</code></pre>

<p>Howvere, when i do <code>AIC(semla)</code> or <code>BIC(semla)</code>i get this error message:</p>

<pre><code>Error in fitMeasures(object, c(""logl"", ""npar"", ""ntotal"")) : 
object 'logl.H0' not found` 
</code></pre>

<p>Unfortunately I am not able to figure out what is wrong. As I have tried to simplify my model in several steps and the error persists it seems to be something fundamental. As I mentioned before, I'm not able to get any information by googling for the error message or part of it.</p>

<p>Any help to solve the matter would be much appreciated.</p>

<p>(I have also posted this question on <em>Stack overflow</em>, but realised that <em>Cross validated</em> might be a more suitable place to ask, please feel free to express your opinions on the matter. <a href=""http://stackoverflow.com/q/26844510/3489824"">http://stackoverflow.com/q/26844510/3489824</a>)</p>
"
"0.0754493182241785","0.0742610657232506","123568","<p>I need to create a Multiple Linear regression model on those data explaining max03 T9 T12 T15 Ne9 Ne12 Ne15 Vx9 Vx12 Vx15 maxO3v</p>

<p>!My data <a href=""http://i.stack.imgur.com/mGU6I.png"" rel=""nofollow"">1</a></p>

<p>My first intuition was to make a backward selection : </p>

<pre><code>attach(ozone)
res &lt;- lm(maxO3~T9+T12+T15+Ne9+Ne12+Ne15+Vx9+Vx12+Vx15+maxO3v)
shapiro.test(res$residuals)
</code></pre>

<p>data:  res$residuals</p>

<p>W = 0.9682, p-value = 0.008945</p>

<p>But the first full model return non-normal residuals.</p>

<p>Is it okay to continue doing a backward selection (AIC criterion)?</p>

<p>I don't think it [non-normal residuals] has an impact on that sort of selection, but I can't find a definite answer to that question.</p>

<p>If I keep doing the backward selection process</p>

<pre><code>[...]
res &lt;- lm(maxO3~T12+Ne9+Vx15+maxO3v)
drop1(res)

summary(res)
shapiro.test(res$residuals)
</code></pre>

<p>Shapiro-Wilk normality test</p>

<p>data:  res$residuals
W = 0.9622, p-value = 0.002946</p>

<p>My residuals aren't normal at the end ...</p>
"
"0.0987863462557455","0.113435651621629","124616","<p>I am testing the logistic regression classifier in R. I created some test data like this:</p>

<pre><code>x=runif(10000)
y=runif(10000)
df=data.frame(x,y,as.factor(x-y&gt;0))
</code></pre>

<p>basically I am sampling the 2D unit square [0,1] and classifying a point belonging to class A or B depending on which side of y=x it lies.</p>

<p>I generated a scatter plot of the data like below:</p>

<pre><code>names(df) = c(""feature1"", ""feature2"", ""class"")
levels=levels(df[[3]])
obs1=as.matrix(subset(df,class==levels[[1]])[,1:2])
obs2=as.matrix(subset(df,class==levels[[2]])[,1:2])
# make scatter plot
dev.new()
plot(obs1[,1],obs1[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=0,col=colors[[1]])
points(obs2[,1],obs2[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=1,col=colors[[2]])
</code></pre>

<p>it gives me below graph:</p>

<p><img src=""http://i.stack.imgur.com/5zN4y.png"" alt=""scatter plot""></p>

<p>Now I tried running LR (logistic regression) on this data using code below:</p>

<pre><code>model=glm(class~.,family=""binomial"",data=df)
summary(model) # prints summary
</code></pre>

<p>here are the results:</p>

<pre><code>Call:
glm(formula = class ~ ., family = ""binomial"", data = df)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.11832   0.00000   0.00000   0.00000   0.08847  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  5.765e-01  1.923e+01   0.030    0.976
feature1     9.761e+04  8.981e+04   1.087    0.277
feature2    -9.761e+04  8.981e+04  -1.087    0.277

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.3863e+04  on 9999  degrees of freedom
Residual deviance: 2.9418e-02  on 9997  degrees of freedom
AIC: 6.0294

Number of Fisher Scoring iterations: 25
</code></pre>

<p>I also get these warning messages:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>If I try plotting the ROC curve using a varying threshold, I get following graph (AUC=1 which is good):
<img src=""http://i.stack.imgur.com/xbyPX.png"" alt=""enter image description here""></p>

<p><strong>Could someone please explain why the algorithm does not converge and coefficient estimates are not statistically significant (high std. error in coeff estimates)?</strong></p>

<p>I also compared to LDA:</p>

<pre><code>lda_classifier=lda(class~., data=df)
</code></pre>

<p>gives:</p>

<pre><code>Call:
lda(class ~ ., data = df)

Prior probabilities of groups:
 FALSE   TRUE 
0.5007 0.4993 

Group means:
       feature1  feature2
FALSE 0.3346288 0.6676169
TRUE  0.6710111 0.3380432

Coefficients of linear discriminants:
               LD1
**feature1  4.280490
feature2 -4.196388**
</code></pre>
"
"0.0616041103633697","0.0606339062590832","126976","<p>I am playing a data without any background information. First, I try multiple linear regression. The model fits well, since the $r^2$ is larger than 90%. I deleted several variables by AIC. The fits improves a little bit. I am interesting what are possible directions that I should look into, if the model fits well at the very beginning.</p>

<p>The target of the analysis is find the model that make the most accurate prediction.</p>

<p>What I can come up with:</p>

<ol>
<li><p>Try other models like regression tree, random forest, SVM or whatever data mining models. The prediction may or may not become better.</p></li>
<li><p>Perform cross validation.</p></li>
<li><p>Check overfitting.</p></li>
<li><p>Diagnostic residual.</p></li>
</ol>
"
"0.0754493182241785","0.0742610657232506","128704","<p>We have 2 correlated variables and a lot of binomial factors (around 200),
here illustrated with just $f1$ and $f2$:</p>

<pre><code>x &lt;- rnorm(100)
y &lt;- rnorm(100)
f1 &lt;- rbinom(100, 1, 0.5)
f2 &lt;- rbinom(100, 1, 0.5)
</code></pre>

<p>Which gives four possible groups: A $(f1=1,f2=1)$, B $(f1=0,f2=1)$, C $(f1=1,f2=0)$, and D $(f1=0,f2=0)$.</p>

<p>We then run the model</p>

<pre><code>&gt; glm(y ~ x * f1 + x * f2)

Call:
glm(formula = y ~ x * f1 + x * f2)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.72028  -0.58501   0.03167   0.60097   1.86332  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -0.03188    0.17388  -0.183   0.8549  
x            0.08105    0.20540   0.395   0.6940  
f1           0.26823    0.19309   1.389   0.1681  
f2          -0.34568    0.19488  -1.774   0.0793 .
x:f1         0.10301    0.20183   0.510   0.6110  
x:f2        -0.25875    0.20828  -1.242   0.2172  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for gaussian family taken to be 0.8906953)

    Null deviance: 88.754  on 99  degrees of freedom
Residual deviance: 83.725  on 94  degrees of freedom
AIC: 280.02

Number of Fisher Scoring iterations: 2
</code></pre>

<p>We can simplify this output and make a regression ($y = a + b \times x$) for each group by doing:</p>

<p>$a_A = (-0.03188) + (0.26823) + (-0.34568)=-0.10932806$
$b_A = (0.08105) + (0.10301) + (-0.25875)=-0.07468630$</p>

<p>$a_B = (-0.03188) + (-0.34568)=-0.37755949$
$b_B = (0.08105) + (-0.25875)=-0.17769345$</p>

<p>And the same for the C and D groups. My question is: How do I calculate a standard deviation or confidence intervals for the individual group slopes. Is it additive like the estimates or is it the means or something else?
Thank you for any help.</p>
"
"0.13068205256071","0.128623938856882","128754","<p>I am using R to run some negative binomial regression models. 
For model 1 I have the number of network in-degrees as the dependent variable, and Twitter followers, friends and number of Twitter statuses as independent variables. Model 2 has the number of network out-degrees as the dependent and number of Twitter followers, friends and number of Twitter statuses as independent variables.</p>

<p>the first model</p>

<pre><code>summary(m1 &lt;- glm.nb(INdegrees ~ Followers + Friends + Statuses, data = list_indegrees))
</code></pre>

<p>converges just fine. But when I run the second model i get the warning message:
<code>glm.fit: algorithm did not converge</code>:</p>

<pre><code>summary(m2 &lt;- glm.nb(OUTdegrees ~ Followers + Friends + Statuses, data = list_outdegrees))

Call:
glm.nb(formula = OUTdegrees ~ Friends + Statuses + Followers, 
data = list_outdegrees, init.theta = 0.7029123173, link = log)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-7.1442  -1.0481  -0.7852  -0.2878  14.6127  

Coefficients:
         Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 4.149e+00  1.022e-02 406.058  &lt; 2e-16 ***
Friends     1.834e-06  4.214e-07   4.353 1.35e-05 ***
Statuses    1.600e-07  1.208e-07   1.325    0.185    
Followers   6.440e-07  8.629e-09  74.639  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for Negative Binomial(0.7029) family taken to be 1)

Null deviance: 22689  on 17244  degrees of freedom
Residual deviance: 20537  on 17241  degrees of freedom
AIC: 179693

Number of Fisher Scoring iterations: 1


          Theta:  0.70291 
      Std. Err.:  0.00653 

2 x log-likelihood:  -179683.26300 
Warning message:
glm.fit: algorithm did not converge 
</code></pre>

<p>Also, when I take out <code>Followers</code> from the model â€“ it converges, but since <code>Followers</code> is an important predictor it wouldn't make any sense to take it out.</p>

<p>This is the first time I'm running negative binomial regression and don't really know what's the deal with the convergence.</p>
"
"0.123208220726739","0.121267812518166","131331","<p>This is the first time I am posting a question, so please excuse any etiquette violations and poorly worded questions!</p>

<p>I am working on the analysis for a chapter of my thesis. I am examining the behavioural response of an animal to a visual stimulus, and trying to determine which of eight explanatory variables (and their two-way interactions) affect this response. I recorded the response on an ordinal scale of 0 (no response), 1 (attention to but no avoidance of stimulus) or 2 (escape response to stimulus). I am leaning towards collapsing categories and using logistic regression where a 1 is an escape response, and 0 is anything else because logistic regression seems much easier to interpret. </p>

<p>I have 794 observations. I am including observer and location (because field sites differed) as random effects, although I am unsure this is a good approach. </p>

<p>I am having trouble with model selection. I ran all possible subsets using the dredge function in packing 'MuMIn'. I thought I was avoiding data dredging by </p>

<ul>
<li>including main effects which were selected because I thought they would have an effect (rather than all conceivable variables)</li>
<li>including only the two-way interactions of interest (R will not run if the global model includes all possible two-way interactions because of the huge number of terms/models)</li>
</ul>

<p>I've come to realise that the second point may be problematic because it leads to an unbalanced model set as in Burnham and Anderson (2002). </p>

<blockquote>
  <p>Page 169: When assessing the relative importance of variables using sums of the AIC    weights, it is important to achieve a balance in the number of models that contain each variable j.</p>
</blockquote>

<p>My questions are</p>

<ol>
<li><p>Is it possible to have a balanced model set without it being considered data dredging? If so, how? </p></li>
<li><p>Is my approach at all reasonable? If not, are there other avenues I should explore? I started with Hosmer&amp;Lemeshow purposeful forward selection, as advocated by my supervisor, but I had some issues with this which I can elaborate on if necessary. </p></li>
</ol>
"
"0.137750978465894","0.13558153613666","135792","<p>I'm interested in building a set of candidate models in R for an analysis using logistic regression. Once I build the set of candidate models and evaluate their fit to the data using AICc (<code>aicc = dredge(results, eval=TRUE, rank=""AICc"")</code>), I would like to use k-fold cross fold validation to evaluate the predictability of the final model chosen from the analysis. I have a few questions associated to k-fold cross validation: </p>

<ol>
<li><p>I assume you use your entire data set for initially building your candidate set of models. For example, say I have 20,000 data values, wouldn't I first build my candidate set of models based on the entire 20,000 data values? Then do use AIC to rank the models and select the most parsimonious model?</p></li>
<li><p>After you select the final model (or model averaged model), would you then conduct a k-fold cross validation to evaluate the predictability of the model? </p></li>
<li><p>What is the easiest way to code a k-fold cross-validation in R? </p></li>
<li><p>Does the k-fold cross validation code break up your entire data set (e.g., 20,000 data values) into training and validation sets automatically? Or do you have to subset the data manually? </p></li>
</ol>
"
"0.110200782772715","0.13558153613666","135967","<p>I am a beginner in R. I am doing logistic regression using around 80 independent variables using <code>glm</code> function in R. The dependent variable is <code>churn</code> which says whether a customer churned or not. I want to know how to identify the right combination of variables to get a good predictive logistic regression model in R.  I also want to know how to identify the same for making good decision tree in R ( I am using the <code>ctree</code> function from the <code>party</code> package).
So far, I had used <code>drop1</code> function  and  <code>anova(LogMdl, test=""Chisq"")</code> where <code>LogMdl</code> is my logistic regression model to drop unwanted variables in the predictive model.  But maximum accuracy I was able to achieve was only 60%. </p>

<p>Also I am not sure if I am using the <code>drop1</code> and <code>anova</code> functions correctly. I dropped the variables with lowest AIC using <code>drop1</code> function.  Using <code>anova</code> function, I dropped variables with p value > 0.05</p>

<p>Kindly help me how to identify the right set of variables for both logistic regression and decision tree models to increase my model's predictive accuracy to close to 90% or more than that if possible.   </p>

<pre><code>library(party)
setwd(""D:/CIS/Project work"")
CellData &lt;- read.csv(""Cell2Cell_SPSS_Data - Orig.csv"")
trainData &lt;- subset(CellData,calibrat==""1"")
testData &lt;- subset(CellData,calibrat==""0"") # validation or test data set
LogMdl = glm(formula=churn ~ revenue  + mou    + recchrge+ directas+ 
               overage + roam    + changem +
               changer  +dropvce + blckvce + unansvce+ 
               custcare+ threeway+ mourec  +
               outcalls +incalls + peakvce + opeakvce+ 
               dropblk + callfwdv+ callwait+
               months  + uniqsubs+ actvsubs+  phones  + models  +
               eqpdays  +customer+ age1    + age2    + 
               children+ credita + creditaa+
               creditb  +creditc + creditde+ creditgy+ creditz + 
               prizmrur+ prizmub +
               prizmtwn +refurb  + webcap  + truck   + 
               rv      + occprof + occcler +
               occcrft  +occstud + occhmkr + occret  + 
               occself + ownrent + marryun +
               marryyes +marryno + mailord + mailres + 
               mailflag+ travel  + pcown   +
               creditcd +retcalls+ retaccpt+ newcelly+ newcelln+ 
               refer   + incmiss +
               income   +mcycle  + creditad+ setprcm + setprc  + retcall, 
               data=trainData, family=binomial(link=""logit""),
               control = list(maxit = 50))
ProbMdl = predict(LogMdl, testData, type = ""response"")
testData$churndep = rep(0,31047)  # replacing all churndep with zero
testData$churndep[ProbMdl&gt;0.5] = 1   # converting records with prob &gt; 0.5 as churned
table(testData$churndep,testData$churn)  # comparing predicted and actual churn
mean(testData$churndep!=testData$churn)    # prints the error %
</code></pre>

<p>Link for documentation of variables: <a href=""https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/</a></p>

<p>Link for Dataset (.csv file) : 
<a href=""https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/</a></p>

<p>I could not produce the output of <code>dput</code> since the data size is more than 5 MB. So I have zipped the file and placed in the above link. </p>

<p>Description of important variables:
* <code>churn</code> is the variable that says whether a customer churned or not.....
* <code>churndep</code> is the variable that needs to be predicted in the test data (validation data) and has to be compared with the <code>churn</code> variable which is already populated with actual churn.
For both churn and churndep, value of 1 means churned and 0 means not churned.</p>
"
"0.0974046509883157","0.0958706236059213","136040","<p>I'm implementing a logistic regression model in R and I have 80 variables to chose from. I need to automatize the process of variable selection of the model so I'm using the step function.</p>

<p>I've no problem using the function or finding the model, but when I look at the final model I find that some of the variables chosen by the step function are not significant (I look at this using the summary function and looking at the fourth column in $coef, this is the Wald Test). This is a problem because I need all the variables included in the model to be significant.</p>

<p>Is there any function or any way to get the best model based on AIC or BIC methods but that also consider that all the coefficients must be significant?
Thanks</p>
"
"NaN","NaN","136137","<p>I am performing a GLM on count data (insurance claims) and I wish to compare Overdispersed Poisson Regression (ODP) against Negative Binomial regression. I would know whether there is a practical index (AIC, logLik) that in standard R could support me in fitting which one to use. I am selecting significant predictors with backard deletion (using anova(fittedModel, test=""Chisq"") type III tests). Theferore it is not assumed the final model within each distribution family to have the same predictor sets.</p>
"
"0.157462484111592","0.166052791038768","139528","<p>When modelling continuous proportions (e.g. proportional vegetation cover at survey quadrats, or proportion of time engaged in an activity), logistic regression is considered inappropriate (e.g. <a href=""http://www.esajournals.org/doi/full/10.1890/10-0340.1"" rel=""nofollow"">Warton &amp; Hui (2011) The arcsine is asinine: the analysis of proportions in ecology</a>). Rather, OLS regression after logit-transforming the proportions, or perhaps beta regression, are more appropriate.</p>

<p>Under what conditions do the coefficient estimates of logit-linear regression and logistic regression differ when using R's <code>lm</code> and <code>glm</code>?</p>

<p>Take the following simulated dataset, where we can assume that <code>p</code> are our raw data (i.e. continuous proportions, rather than representing ${n_{successes}\over n_{trials}}$):</p>

<pre><code>set.seed(1)
x &lt;- rnorm(1000)
a &lt;- runif(1)
b &lt;- runif(1)
logit.p &lt;- a + b*x + rnorm(1000, 0, 0.2)
p &lt;- plogis(logit.p)

plot(p ~ x, ylim=c(0, 1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/AzWOX.png"" alt=""enter image description here""></p>

<p>Fitting a logit-linear model, we obtain:</p>

<pre><code>summary(lm(logit.p ~ x))
## 
## Call:
## lm(formula = logit.p ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.64702 -0.13747 -0.00345  0.15077  0.73148 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.868148   0.006579   131.9   &lt;2e-16 ***
## x           0.967129   0.006360   152.1   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## Residual standard error: 0.208 on 998 degrees of freedom
## Multiple R-squared:  0.9586, Adjusted R-squared:  0.9586 
## F-statistic: 2.312e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Logistic regression yields:</p>

<pre><code>summary(glm(p ~ x, family=binomial))
## 
## Call:
## glm(formula = p ~ x, family = binomial)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.32099  -0.05475   0.00066   0.05948   0.36307  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.86242    0.07684   11.22   &lt;2e-16 ***
## x            0.96128    0.08395   11.45   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 176.1082  on 999  degrees of freedom
## Residual deviance:   7.9899  on 998  degrees of freedom
## AIC: 701.71
## 
## Number of Fisher Scoring iterations: 5
## 
## Warning message:
## In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>Will the logistic regression coefficient estimates always be unbiased with respect to the logit-linear model's estimates?</p>
"
"0.0435606841869032","0.0428746462856272","140509","<p>I used logistic regression and found that my model fits well: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6434  -1.4623   0.8704   0.9013   1.0066  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   0.41595    0.02115   19.67   &lt;2e-16 ***
init_att_cnt  0.02115    0.00146   14.48   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)

Null deviance: 154956  on 122239  degrees of freedom
Residual deviance: 154746  on 122238  degrees of freedom
AIC: 154750
</code></pre>

<p>The chi-squared test is hightly statisticaly significant: <code>p = 9.642755e-48</code>. I decided to check the Nagelkerke $R^2$ statistic, </p>

<pre><code>R2 &lt;- R2/(1-exp((-mylogit$null.deviance)/n))
</code></pre>

<p>but it was $R^2 = 0.001350927$. This is unbelievable, why is $R^2$ so small, if my model fits well?</p>
"
"0.0987863462557455","0.113435651621629","141603","<p>So I'm playing around with logistic regression in R, using the mtcars dataset, and I decide to create a logistic regression model on the 'am' parameter (that is manual or automatic transmission for those of you familiar with the mtcars-dataset).</p>

<pre><code>Call:
glm(formula = am ~ mpg + qsec + wt, family = binomial, data = mtcars)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-4.484e-05  -2.100e-08  -2.100e-08   2.100e-08   5.163e-05  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    924.89  883764.07   0.001    0.999
mpg             20.65   18004.32   0.001    0.999
qsec           -55.75   32172.52  -0.002    0.999
wt            -111.33  103183.48  -0.001    0.999

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 4.3230e+01  on 31  degrees of freedom
Residual deviance: 6.2903e-09  on 28  degrees of freedom
AIC: 8

Number of Fisher Scoring iterations: 25
</code></pre>

<p>Now, at first sight this looks like a terrible regression, right? The standard errors are HUGE, the z-values are all close to zero and the corresponding probabilities are all close to one. HOWEVER, the residual deviance is extremely small! </p>

<p>I decide to check how well the model does as a classification model by running:</p>

<pre><code>pred &lt;- predict(logit_fit, data.frame(qsec = mtcars$qsec, wt = mtcars$wt, mpg = mtcars$mpg), type = ""response"") # Make a prediction of the probabilities on our data
mtcars$pred_r &lt;- round(pred, 0) # Round probabilities to closest 0 or 1
table(mtcars$am, mtcars$pred_r) # Check if results of classification is any good.
</code></pre>

<p>Indeed, the model perfectly predicts the data:</p>

<pre><code>     0  1
  0 19  0
  1  0 13
</code></pre>

<p>Have I completely misunderstood how to interpret model data? Am I overfitting massively or what's going on here? What's going on?</p>
"
"0.107807193135897","0.121267812518166","141630","<p>For a regression example, I constructed some artificial data and ran ols,</p>

<pre><code>import pandas as pd
n = 100
df = pd.DataFrame()
np.random.seed(1)
df['x1'] = np.random.randn(n)
df['x2'] = np.random.randn(n)
df['x3'] = np.random.randn(n)
df['x4'] = np.random.randn(n)
df['y'] = 10 + -100*df['x1'] +  75*df['x3'] + np.random.randn(n)
import statsmodels.formula.api as smf
results = smf.ols('y ~ x1 + x2 + x3 + x4 ', data=df).fit()
print results.summary()
</code></pre>

<p>The result</p>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       1.000
Model:                            OLS   Adj. R-squared:                  1.000
Method:                 Least Squares   F-statistic:                 3.397e+05
Date:                Fri, 13 Mar 2015   Prob (F-statistic):          2.00e-196
Time:                        16:04:29   Log-Likelihood:                -140.87
No. Observations:                 100   AIC:                             291.7
Df Residuals:                      95   BIC:                             304.8
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept     10.0787      0.103     97.783      0.000         9.874    10.283
x1          -100.0711      0.116   -864.165      0.000      -100.301   -99.841
x2            -0.1041      0.112     -0.931      0.354        -0.326     0.118
x3            75.0392      0.103    726.037      0.000        74.834    75.244
x4            -0.2711      0.096     -2.824      0.006        -0.462    -0.080
==============================================================================
Omnibus:                        1.621   Durbin-Watson:                   1.709
Prob(Omnibus):                  0.445   Jarque-Bera (JB):                1.283
Skew:                          -0.050   Prob(JB):                        0.526
Kurtosis:                       2.454   Cond. No.                         1.38
==============================================================================
</code></pre>

<p>The result suggests the 4th variable is significant when in fact, it is not (artificial y did not include it). </p>

<p>But when I use R to generate artificial data the same way</p>

<pre><code>set.seed(1)
n = 100
x1 = rnorm(n)
x2 = rnorm(n)
x3 = rnorm(n)
x4 = rnorm(n)
y = 10 -100*x1 + 75*x3 +  rnorm(n)
dat = data.frame(cbind(x1,x2,x3,x4,y))
write.csv(dat, '/tmp/out.csv')
</code></pre>

<p>and load this data, run ols()</p>

<pre><code>import pandas as pd
import statsmodels.formula.api as smf
df = pd.read_csv('/tmp/out.csv')
results = smf.ols('y ~ x1 + x2 + x3 + x4 ', data=df).fit()
print results.summary()
</code></pre>

<p>I get </p>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       1.000
Model:                            OLS   Adj. R-squared:                  1.000
Method:                 Least Squares   F-statistic:                 2.502e+05
Date:                Fri, 13 Mar 2015   Prob (F-statistic):          4.03e-190
Time:                        16:07:36   Log-Likelihood:                -154.89
No. Observations:                 100   AIC:                             319.8
Df Residuals:                      95   BIC:                             332.8
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept      9.9430      0.118     84.263      0.000         9.709    10.177
x1           -99.8067      0.131   -762.203      0.000      -100.067   -99.547
x2             0.1696      0.123      1.380      0.171        -0.074     0.414
x3            75.0337      0.114    655.983      0.000        74.807    75.261
x4             0.0427      0.119      0.357      0.722        -0.195     0.280
==============================================================================
Omnibus:                        2.555   Durbin-Watson:                   1.988
Prob(Omnibus):                  0.279   Jarque-Bera (JB):                2.051
Skew:                           0.339   Prob(JB):                        0.359
Kurtosis:                       3.180   Cond. No.                         1.26
==============================================================================
</code></pre>

<p>Now x2 and x4 are not significant, as they should (not) be. What I am curious about is what is np.random.randn doing that is so different from R's random numbers? I tried scipy.stats.norm.rvs, also numpy.random.normal, the result is the same (bad x4). </p>

<p>Any suggestions? </p>

<p>Thanks,</p>
"
"0.163352565700887","0.171498585142509","141820","<p>I want to find which soil variables better explain plant productivity, using a database that contains information for about 100 forests plots across Europe.
These plots have only one species per plot, but overall there are 4 different species in the dataset. These plots also have different climate conditions (temperature, precipitation,...). My final goal is finding out which combination of the more than 20 different soil variables better explain plant productivity. However, both climate and species may confound the analysis because both affect plant growth (some species grow more than others, and plants grown in warmer climates may grow more). I am only interested in plant growth due to soil characteristics, so I need to get rid of the species and climate effects on plant productivity that may confound the analysis. According to what I have read I could just include all variables in the model: soil, climate and species (factor of 4 levels), like this:</p>

<pre><code>fit &lt;- lm(scale(IVMean)~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+
                        scale(EXCHCA)+scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+
                        scale(EXCHNA)+scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+
                        scale(S_SO4)+scale(N_NH4)+scale(BS)+scale(CN)+scale(Temp)+
                        scale(Precip)+scale(Rad)+scale(PET)+species)
</code></pre>

<p>IVMean = mean stem volume increment (productivity). Note climate variables (temperature, precipitation, radiation and potential evapotranspiration -PET-) and species at the end, and the standardisation of all variables with <code>scale()</code>.</p>

<p>After this, I could run a stepwise regression analysis to preliminarily find which variables are the most important explaining plant productivity.</p>

<pre><code>library(MASS)
step &lt;- stepAIC(fit, direction=""backward"")
step$anova # display results
</code></pre>

<p>Which renders the following best minimal model:</p>

<pre><code>Final Model:
scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
    scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species

&gt; model &lt;- lm(scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
+               scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species, 
+             data = icp)
&gt; summary(model)

Call:
lm(formula = scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + 
    scale(EXCHMG) + scale(EXCHMN) + scale(BS) + scale(Temp) + 
    scale(PET) + species, data = icp)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.13836 -0.41522 -0.02816  0.35094  1.65587 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -0.37587    0.16967  -2.215 0.030897 *  
scale(PHCACL2)      0.58776    0.20617   2.851 0.006128 ** 
scale(EXCHCA)      -0.38061    0.19025  -2.001 0.050381 .  
scale(EXCHMG)      -0.37374    0.14686  -2.545 0.013769 *  
scale(EXCHMN)       0.13102    0.09970   1.314 0.194241    
scale(BS)           0.39502    0.19428   2.033 0.046871 *  
scale(Temp)         1.34654    0.32033   4.204 9.74e-05 ***
scale(PET)         -0.62177    0.29749  -2.090 0.041250 *  
speciesoak         -1.24553    0.34788  -3.580 0.000726 ***
speciespicea_abies  1.38679    0.25031   5.540 8.79e-07 ***
speciesscots_pine   0.02627    0.25960   0.101 0.919769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.6411 on 55 degrees of freedom
Multiple R-squared:  0.6522,    Adjusted R-squared:  0.5889 
F-statistic: 10.31 on 10 and 55 DF,  p-value: 1.602e-09
</code></pre>

<p>The final model includes 5 soil variables, 2 out of 4 climate variables, and species. So far so good?</p>

<p>However, this seems to be not good enough for my supervisor. Rather, he asked me to do an analysis of the residuals to â€œget rid of climate and species effectsâ€! To be honest, I have no idea what he is talking about, and I was afraid to ask because he sounded like something I should know since my childhood. Perhaps he meant I should study which SOIL variables can explain the residuals of productivity ~ climate * species? Please, help me find out which type of analysis of the residuals would make sense to focus on soil effects eliminating climate and species effects.</p>

<p>This is the only thing I can think of:  </p>

<pre><code># Study the importance of confounding effects:
confounding     &lt;- IVMean ~ (Temp + Precip + PET + Rad) * species 
confounding.res &lt;- residuals(confounding)
lm(confounding.res ~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+scale(EXCHCA)+
                    scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+scale(EXCHNA)+
                    scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+scale(S_SO4)+
                    scale(N_NH4)+scale(BS)+scale(CN))
</code></pre>

<p>This way maybe I could study which soil variables explain what climate and species effects could not explain? I donâ€™t know if it makes any sense. I am open to suggestions and alternatives. </p>
"
"0.238715927658058","0.234956386753948","142489","<p>I'm analysing PAM fluorescence data from an experimental set-up that I duplicated from an earlier experiment with a missing control. That's why I haven't given the statistics of the experiment much (if any) thought in advance.</p>

<p>The set-up consisted of 8 containers with peat moss (<em>Sphagnum magellanicum</em>), divided over 4 treatments, so that each treatment was performed in duplicate. At regular (weekly) intervals, over the course of 3 months, I performed life PAM fluorescence measurements on a number of capitula (growth tops) in each container to determine a kinetic response curve for each of these capitula.</p>

<p>To minimize intraleaf (in my case, intra<em>capitula</em>) variance, ideally, PAM fluorescence measurements would have been repeated for the same leaf every week in the 3-month time series, but for practical reasons, my AOIs (areas of interest) for the fluorescence meter where located on different capitula every week. This is also my first subquestion: can I consider measurements at different time points in the same container as <em>repeated measures</em>, or would this only be valid if I had been measuring the same AOIs every week? And does this depend on whether I aggregate the measured values of the different AOIs per container before further analysis?</p>

<p>After nightfall, once every week, for 5â€“7 AOIs in each container, I determined a kinetic curve, for which the PAM software performs 20 measurements. The first measurement represents the dark-adapted fluorescence values, after which an actinic light source (at a wavelength that can facilitate photosynthesis) is started for the 19 remaining measurements. From the start of the kinetic curve (the dark adapted $\phi_{PSII}$ values), I determine $F_v/F_m$ and from the end of the curve (the flat part), I determine $\text{mean}(\phi_{PSII})$. $\phi_{PSII}$ and $F_v/F_m$ measure the quantum yield of photosystem II and the max. efficiency of photosystem II, respectively; $F_v/F_m = \phi_{PSII}$ in a dark-adapted state.</p>

<p>I'm interested in building two models, one in which the response (dependent) variable is $\phi_{PSII}$ and one in which it is $F_v/F_m$. The (independent) predictor variables are:</p>

<ul>
<li><code>AOI</code> (factor): a number between 1â€“6; </li>
<li><code>Container</code> (factor): a number between 1â€“8; </li>
<li><code>Treatment</code>: (factor): a number between 1â€“4; and</li>
<li><code>DaysTreated</code> (integer): the number of days since the treatments began.</li>
</ul>

<p>My guess is that I should treat <code>AOI</code> and <code>Container</code> as random effects variables, with <code>AOI</code> nested in <code>Container</code> and <code>Container</code> nested in the fixed effect variable <code>Treatment</code>. <code>DaysTreated</code>, then, would be my continuous predictor (covariate). For $\phi_{PSII}$, I would model this in R like this:</p>

<pre><code>library(nlme)
YII_m1 &lt;- lme(mean_YII ~ DaysTreated * Treatment,
              random = ~1 | Container / AOI,
              method = ""ML"",
              data = fluor_aoi)
# fluor_aoi is a data-frame in which each AOI kinetic curve is
# aggregated into one row, where mean_YII = mean( YII[15:19] )
# and FvFm = YII[1]
</code></pre>

<p>I'm not sure if this is the most parsimious model. To find out, I want to try different models with different fixed effects but all with the same random effects. <code>anova.lme()</code> warned me that comparing between these models is a <a href=""http://stats.stackexchange.com/questions/116770/"">no-go</a> when using the default method (<code>method = ""REML""</code>), which is why I use <code>method = ""ML""</code>.</p>

<pre><code>anova(YII_m1, # ~ DaysTreated * Treatment
      YII_m2, # ~ DaysTreated:Treatment + Treatment
      YII_m3, # ~ DaysTreated:Treatment + DaysTreated
      YII_m4, # ~ DaysTreated:Treatment
      YII_m5, # ~ DaysTreated + Treatment
      YII_m6, # ~ DaysTreated
      YII_m7  # ~ Treatment
     )

       Model df       AIC       BIC   logLik   Test  L.Ratio p-value
YII_m1     1 11 -2390.337 -2340.578 1206.168                        
YII_m2     2 11 -2390.337 -2340.578 1206.168                        
YII_m3     3  8 -2390.347 -2354.158 1203.173 2 vs 3  5.99019  0.1121
YII_m4     4  8 -2390.347 -2354.158 1203.173                        
YII_m5     5  8 -2366.481 -2330.293 1191.241                        
YII_m6     6  5 -2363.842 -2341.224 1186.921 5 vs 6  8.63915  0.0345
YII_m7     7  7 -2264.868 -2233.203 1139.434 6 vs 7 94.97389  &lt;.0001
</code></pre>

<p>I would have liked it if the <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">best fit</a> was model 2 with the fixed effects formula <code>~ DaysTreated:Treatment + Treatment</code>, because my expectation at the onset of my experiment was to see a decline in <em>Sphagnum</em> vitality, but only for some of the treatments and hopefully not in the controls. (The acclimatization period was very long, hoping that any effects on the mosses of the new (greenhouse) environment would have flattened out by the onset of the treatments.)</p>

<p><strong>Edit 2015-May-1:</strong> First I compared only 6 models; model 4 was missing from my initial question. Also, I forgot to factorize treatment, so that instead of model 2, now, different models give the â€˜best fitâ€™.</p>

<p>Anyway, so far (unless you tell me otherwise), I feel I can continue to use model 2, which also best fits the visual observation that 4 of the 8 containers where doing very badly at the end of the experiment while the other 4 seemed to do ok.</p>

<pre><code>anova(YII_m2)
                  numDF denDF  F-value p-value
(Intercept)           1   620 526.9698  &lt;.0001
Treatment             3     4   5.0769  0.0753
DaysTreated:Treatment 4   620  36.4539  &lt;.0001
</code></pre>

<p>An ANCOVA test on model 2 reveals that only the interaction between <code>DaysTreated</code> and <code>Treatment</code> is significant, which makes sense to me, given that the containers started out in roughly the same condition after acclimatization. There was visible difference between containers in the same treatments, but that should have been taken care of by correcting for the random error effect.</p>

<p>Mean $\phi_{PSII}$ plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments:</p>

<p><img src=""http://i.stack.imgur.com/GgGhG.png"" alt=""Mean Y_II plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments.""></p>

<p>Now that I've made an <em>attempt</em> at constructing and testing a somewhat decent model (which I'd love to receive criticism on), I'd like to perform a multiple pairwise comparison to find out which treatments diverge significantly from each other over time, but I have no idea what is the proper way to approach this.</p>

<p>Also, I want to try a linear correlation, but again, I'm clueless as to how. Is there an appropriate way to integrate this in my model or should I try to model a regression per treatment? </p>

<p>Please forgive the ignorance in my approach and my questions. I'm a BSc student whose statistical background mainly consists of a brief entry-level course, followed by a recipe-level R course. RTFM comments are definitely welcome, as long as they include a link to TFM.</p>
"
"0.0435606841869032","0.0428746462856272","144603","<p>I have built a logistic regression where the outcome variable is being cured after receiving treatment (<code>Cure</code> vs. <code>No Cure</code>). All patients in this study received treatment. I am interested in seeing if having diabetes is associated with this outcome. </p>

<p>In R my logistic regression output looks as follows: </p>

<pre><code>Call:
glm(formula = Cure ~ Diabetes, family = binomial(link = ""logit""), data = All_patients)
...
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   1.2735     0.1306   9.749   &lt;2e-16 ***
Diabetes     -0.5597     0.2813  -1.990   0.0466 *  
...
    Null deviance: 456.55  on 415  degrees of freedom
Residual deviance: 452.75  on 414  degrees of freedom
  (2 observations deleted due to missingness)
AIC: 456.75
</code></pre>

<p>However, the confidence interval for the odds ratio <strong>includes 1</strong>:</p>

<pre><code>                   OR     2.5 %   97.5 %
(Intercept) 3.5733333 2.7822031 4.646366
Diabetes    0.5713619 0.3316513 1.003167
</code></pre>

<p>When I do a chi-squared test on these data I get the following:</p>

<pre><code>data:  check
X-squared = 3.4397, df = 1, p-value = 0.06365
</code></pre>

<p>If you'd like to calculate it on your own the distribution of diabetes in the cured and uncured groups are as follows:</p>

<pre><code>Diabetic cure rate:      49 /  73 (67%)
Non-diabetic cure rate: 268 / 343 (78%)
</code></pre>

<p>My question is: Why don't the p-values and the confidence interval including 1 agree? </p>
"
"0.0889178742536973","0.105021006302101","148808","<p>I'm trying to evaluate the value of an object, depending on his characteristics. In order to do this, I'm building the following regression model <code>price ~ .</code>, using similar objects and for each variable I got min 20 observations</p>

<p>I encountered following problem: none of the regression models worked for all my data, so I decided to use all of the followings methods:</p>

<pre><code>model.lm &lt;- lm(price ~ .)
model.lmLog &lt;- lm(log(price) ~ .)
model.ltsReg &lt;- ltsReg(price ~ .)
model.ltsRegLog &lt;- lts(log(price) ~ .)
model.lmrob &lt;- lmrob(price ~ .)
model.lmrobLog &lt;- lmrob(log(price) ~ .)
model.lmRob &lt;- lmRob(price ~ .)
model.lmRobLog &lt;- lmRob(log(price) ~ .)
model.glm &lt;- glm(price ~ .)
model.glmLog &lt;- glm(price ~ ., family=gaussian(link=""log""))
</code></pre>

<p>My question is: how can I decide which of this models fits best for the current data, without plotting the results?</p>

<p>As far as I know, the <code>r-squared</code> aren't trusty, because I the data is corrupted, so will be the <code>r-squared</code>.</p>

<p>Any ideas?</p>

<p>Thank you!</p>

<p><strong>[UPDATE]</strong></p>

<p>what do you think about using <code>BIC</code> or <code>AIC</code> and choosing the one with the lowest value?</p>

<p>what do you think about choosing the variables for the regression upon the analysis of <code>anova</code>?</p>

<p>I have 17 variables from which 10 are dummy variables, is that a problem?</p>
"
"0.0754493182241785","0.0742610657232506","151463","<p>Generally, coeficients and their p values are focussed upon while assessing the regression output. However, there are other things mentioned. How can we analyze the output of glm without the coefficients: </p>

<pre><code>&gt; summary(mod)

Call:
glm(formula = outvar ~ ., family = binomial, data = mydf)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3537  -0.8172  -0.6462   1.2131   2.2757  

Coefficients:
....
....


(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8973.8  on 7760  degrees of freedom
Residual deviance: 8526.6  on 7752  degrees of freedom
AIC: 8544.6

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Does above part of glm output can be used to comment anything about the regression performed without coefficients being available?</p>
"
"0.264969297566249","0.25374774349557","151657","<p>I am running X-13 SEATS on r for monthly data in six years of observations and I think I got a (sufficiently) reasonable fit for the ARIMA model, but the output also shows me that my original series does not have significant seasonality, as it follows:</p>

<pre><code> Call:
seas(x = data_r[, 1], transform.function = ""log"", regression.aictest = NULL, 
    outlier = NULL, arima.model = ""(0 1 1)(1 1 0)"")

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
AR-Seasonal-12     -0.6194     0.1110  -5.581 2.39e-08 ***
MA-Nonseasonal-01   0.6220     0.1093   5.690 1.27e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 773.4, BIC: 778.4  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 20.04   Shapiro (normality): 0.9754
    &gt; 
                qs p-val
    qsori        0     1
    qsorievadj   0     1
    qsrsd        0     1
    qssadj       0     1
    qssadjevadj  0     1
    qsirr        0     1
    qsirrevadj   0     1
</code></pre>

<p>(Still, there is also the fact that the irregular component seems to dominate the SI ratio for some specific months in some years. So maybe there is some dummy variable in the pre-adjustment that I am missing (right?)) </p>

<p>But when I run a regression on Stata for yearly and monthly dummies on the original series -- assuming the seasonality is deterministic --, I cannot reject with an F test that they are all equal to zero. What does this show me? That my ARIMA fit is not correct?</p>

<p>Also, if someone could point me out the difference in interpretation that you should have when running a regression on seasonal dummies and deseasonalizing data with a X-13 SEATS, it would be also very helpful. Maybe that is what I am missing here.</p>

<p>Edit: is it by any chance a common practice, in some particular situations (when you are deseasonalizing a set of series), still deseasonalize a given series even if that series does not show significant seasonality?</p>

<p>Edit2: Adding the results of the automatic adjustment:</p>

<pre><code>Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
Constant            59.1761    38.0551   1.555  0.11994    
Easter[15]        -903.6151   341.1891  -2.648  0.00809 ** 
MA-Nonseasonal-01    0.4974     0.1138   4.370 1.24e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)  Obs.: 60  Transform: none
AICc: 925.6, BIC: 933.2  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.):  21.9   Shapiro (normality): 0.9498 *

            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1 
</code></pre>

<p>I also, I get the following error for the monthplot function with the automatic adjustment: </p>

<pre><code>Error in `[.default`(x$data, , ""seasonal"") : subscript out of bounds
</code></pre>

<p>Following this result from the automatic adjustment, the use of the dummy for easter, with the original specification, does not change that much the first output:</p>

<pre><code>Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
Easter[15]        -0.08307    0.02690  -3.088  0.00202 ** 
AR-Seasonal-12    -0.63353    0.10816  -5.858  4.7e-09 ***
MA-Nonseasonal-01  0.50391    0.12075   4.173  3.0e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 767.9, BIC: 774.3  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 29.37   Shapiro (normality): 0.9721  
            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1
</code></pre>

<p>Most recent observation: Now I Think I am fairly sure that there is no significant seasonality in this series, but I would be thankful if someone could show me other problems that I might not be considering. Still, I would like a possible canonical/scholarly answer on why I can reject the null hypothesis for the whole set of seasonal dummies being zero (though I had a small result for the F test with my data, ~4, but I still reject the null) and still get a reasonable ARIMA fit with which I cannot reject no seasonality in my original data. Does that have something to do with the difference of the adjustment with ARIMA models and deterministic seasonality? An intuitive answer on this difference would be of some help.</p>
"
"0.0871213683738064","0.0857492925712544","152012","<p>This might fit better here than on stackoverflow, I guess.</p>

<p>I was <a href=""http://stackoverflow.com/questions/30139874/r-dynamic-linear-regression-with-dynlm-package-how-to-predict"">trying to build a dynamic regression model with the dynlm</a> package, but it did not work out. After reading <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">this</a> by Hyndman, I now switched to an ARMAX model:</p>

<pre><code>y_t = a_1*x1_t + a_2*x2_t + ... + a_k*xk_t + n_t
</code></pre>

<p>where the error term follows an ARMA model</p>

<pre><code>n_t ~ ARMA(p,q)
</code></pre>

<p>So far I am using the function <code>auto.arima(y, xreg=cbind(x1, ..., xk))</code> from the <code>forecast</code>package, which is doing the job!</p>

<p>As a benchmark I am running a pure multiple regression with <code>lm()</code>, where I make use of the <code>step()</code> function to kick out non relevant variables (about 100 variables, from which 96 are dummies) to optimize the model according to <code>AIC</code>.</p>

<p>The in-sample forecasting for both models is more or less equal. As the ARMAX model always includes <strong>all</strong> independent variables <code>(x1, ..., xk)</code>, I am pretty sure that, if I could apply the <code>step()</code> function on it, I would achieve a further improvement here.</p>

<p>The problem is that the <code>step()</code> function does not work on <code>auto.arima()</code>?!</p>

<p>Do you have any suggestions how I could still do this? Or would I need a totally new approach?</p>

<p>(I have not provide a reproducible example, as this is a rather general question of which methods/functions/packages to use. If the question is not clear enough, please tell me and I will try to provide one)</p>
"
"0.0616041103633697","0.0606339062590832","152268","<p>Can someone explain to me how you would know which variables to remove? And how do you know if something is accurate or not? Because when I plot an ROC curve the specificity/sensitivity curve, it looks like a square for a random forest.</p>

<p>If I use 2 different models, I assume I would have to use the same variables if I did stepAIC(logistic regression model) and then use those exact same variables for the random forest? </p>

<p>I have 3 unbalanced datasets that come from the same sample and I have to make something of it and I am not too sure where to start. Thanks!</p>
"
"0.0779237207906526","0.076696498884737","152394","<p>I am using R's flexsurvreg function (in the flexsurv package) to fit a AFT model to my data. </p>

<p>This is the line of code that fits the model to the data:</p>

<pre><code>TestModel &lt;- flexsurvreg(Surv(time,death) ~ param1 + param2 + param3 + param4 + param5 + param6 + param7 + param8 + param9 + param10 + param11 + param12 + param13, data = DataTest, dist = ""weibull"")  
</code></pre>

<p>Once the model fits, this is a summary of the results:</p>

<pre><code>Estimates: 
        data mean     est        L95%       U95%       se         exp(est)   L95%       U95%     
shape      NA         9.99e-01         NA         NA         NA         NA         NA         NA
scale      NA         2.20e+02         NA         NA         NA         NA         NA         NA
param1     1.32e-01   2.51e-01         NA         NA         NA   1.29e+00         NA         NA
param2     1.61e-01  -1.54e-02         NA         NA         NA   9.85e-01         NA         NA
param3     1.89e-01  -4.68e-02         NA         NA         NA   9.54e-01         NA         NA
param4     1.76e-01  -2.25e-02         NA         NA         NA   9.78e-01         NA         NA
param5     1.87e-01  -5.35e-02         NA         NA         NA   9.48e-01         NA         NA
param6     7.56e-01  -2.74e-01         NA         NA         NA   7.60e-01         NA         NA
param7     2.28e-01   3.23e-02         NA         NA         NA   1.03e+00         NA         NA
param8     1.58e-01  -1.69e-02         NA         NA         NA   9.83e-01         NA         NA
param9     4.32e-01  -1.89e-02         NA         NA         NA   9.81e-01         NA         NA
param10    1.30e+02  -1.01e-03         NA         NA         NA   9.99e-01         NA         NA
param11    2.26e+01  -4.08e-03         NA         NA         NA   9.96e-01         NA         NA
param12    5.54e+02  -2.84e-04         NA         NA         NA   1.00e+00         NA         NA
param13    9.57e+01  -4.69e-03         NA         NA         NA   9.95e-01         NA         NA

N = 40320,  Events: 32154,  Censored: 8166
Total time at risk: 2584693
Log-likelihood = -171611.5, df = 15
AIC = 343253.1
</code></pre>

<p>I want to measure how the covariates affect the survival time. The estimates provide an understanding of this. Also, as I read <a href=""http://stats.stackexchange.com/questions/6026/how-do-i-interpret-expb-in-cox-regression"">here</a>, $exp(est)$ provides an estimate of how the hazard changes with change in 1 unit of a covariate by keeping the other covariates fixed. Is there a way I can calculate p-values for these covariates?</p>

<p>I have fitted a Weibull distribution to my dataset.</p>
"
"0.13068205256071","0.128623938856882","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.162989155760853","0.160422236979937","153802","<p>I have a large data set with repeated measurements of same blood value (co) (2 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement.</p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to right and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>At first I assumed random intercepts among patients. I constructed a null model and model with time as independent.</p>

<pre><code>fit0&lt;-(lmer(lgco~(1|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(1|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (1 | id)
fit1: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit0  3 200.44 213.16 -97.219   194.44                             
fit1  4 189.62 206.59 -90.811   181.62 12.815      1  0.0003438 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Ok, so I have an empty model and model with independent variable.
<img src=""http://i.stack.imgur.com/SFIFL.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/8RbgF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/phYJ1.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/G4HNH.png"" alt=""enter image description here""></p>

<p>Adding covariate time in my model improves it significantly and also the graphical explanation is clear.</p>

<p>Fixed slopes, however, are not reasonable in my data, so I should use random slopes.</p>

<pre><code>fit0&lt;-(lmer(lgco~(time|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(time|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (time| id)
fit1: lgco ~ time + (time | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
fit0  5 190.15 211.36 -90.076   180.15                            
fit1  6 182.06 207.51 -85.029   170.06 10.094      1   0.001487 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>At this point I dont understand my model equations. Graphical outputs for fit0 and fit1 are as follows:
<img src=""http://i.stack.imgur.com/E4w5D.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/XyeTJ.png"" alt=""enter image description here""></p>

<p>For the fit1 the model equation is:
<img src=""http://i.stack.imgur.com/Z6WLa.png"" alt=""enter image description here""></p>

<p>Why the lines in fit0 have non-zero slopes? What are they and what is the equation in that case? Also I dont understand how should I clarify the change in model fit? In the case of only random intercepts I can state that ""adding fixed factor <em>beta1</em> to model improves it significantly"". What would be the equal statement in the case of random slopes?</p>
"
"0.174242736747613","0.171498585142509","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"0.11525073729837","0.113435651621629","157597","<p>I am trying to understand how to interpret log-linear models for contingency tables, fitted by way of Poisson GLMs. </p>

<p>Consider this example from CAR (Fox and Weisberg, 2011, p. 252). </p>

<pre><code>require(car)
data(AMSsurvey)
(tab.sex.citizen &lt;- xtabs(count ~ sex + citizen, data=AMSsurvey))
</code></pre>

<p>Yielding:</p>

<pre><code>        citizen
sex      Non-US  US
  Female    260 202
  Male      501 467
</code></pre>

<p>Then we fit the model of (mutual) independence: </p>

<pre><code>AMS2 &lt;- as.data.frame(tab.sex.citizen)
(phd.mod.indep &lt;- glm(Freq ~ sex + citizen, family=poisson, data=AMS2))
pchisq(2.57, df=1, lower.tail=FALSE)
</code></pre>

<p>Outputting: </p>

<pre><code>&gt; (phd.mod.indep &lt;- glm(Freq ~ sex + citizen, family=poisson, data=AMS2))

Call:  glm(formula = Freq ~ sex + citizen, family = poisson, data = AMS2)

Coefficients:
(Intercept)      sexMale    citizenUS  
     5.5048       0.7397      -0.1288  

Degrees of Freedom: 3 Total (i.e. Null);  1 Residual
Null Deviance:      191.5 
Residual Deviance: 2.572    AIC: 39.16
&gt; pchisq(2.57, df=1, lower.tail=FALSE)
[1] 0.1089077
</code></pre>

<p>The p value is close to 0.1 indicating weak evidence to reject independence. However, let us <strong>assume</strong> that we have sufficient evidence to reject the NULL (i.e. for our purposes, the 0.10 p value is indicative of an association between the two variables). </p>

<p><strong>Question</strong>: How, then, do we interpret this loglinear model? </p>

<p>(Do we fit the saturated model (i.e. <code>update(phd.mod.indep, . ~ . + sex:citizen)</code>)? Do we interpret the estimated regression coefficients? In CAR they stop at this point, because of weak evidence for rejecting the NULL, but I'm interested in understanding the mechanics of the interpretation of this simple log-linear model <em>as if</em> the ""interaction"" were significant...)</p>
"
"0.0779237207906526","0.0958706236059213","160545","<p>I recently ran two tests in R - one using glm() and one using lm() with the goal being to test the relationship between a binary response and binary predictor.  I ran glm() first and got an estimate of -0.68 for the predictor coefficient which I thought was pretty good.  P&lt;.05 and AIC of 653.  </p>

<p>When I ran lm() however I got an estimate of -.14, a multiple r-squared of .008, P&lt;.05.  </p>

<p>My understanding is that linear regression is usually a poor choice for a categorical response compared with logistic regression, but when is this not the case? I noticed in this post <a href=""http://statisticalhorizons.com/linear-vs-logistic"" rel=""nofollow"">http://statisticalhorizons.com/linear-vs-logistic</a> that the author states there's middle ground where it does make sense to use linear regression.  Are there any common rules (or rules of thumb you personally use) that determine when to try out linear regression on a categorical response?  Do any of these differ from the author's cases?</p>
"
"0.157462484111592","0.154982604969517","160675","<p>I'm interested in determining both the slope regression coefficient and plotting regression lines for autocorrelated time-series datasets of rainfall.  Specifically, I'd like to identify the best approach in R that would allow me to visualize the regression line on the original (undifferenced) time-series plot when I need to difference the data to remove stationarity (i.e, when d>0 in an arima model).</p>

<p>As a start, I'm exploring the use of auto.arima (from the forecast package) and sarima (from the astsa package) which can output regression coefficients in the presence of autocorrelation.</p>

<p>For example:</p>

<ol>
<li><p>Using auto.arima. The 'drift' of -5.009 represents the slope (see <a href=""http://robjhyndman.com/hyndsight/arima-trends/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/arima-trends/</a>) </p>

<pre><code>&gt; min.ar &lt;- auto.arima(dec.yr.mmin$min_prcp)
&gt; summary(min.ar)

Series: dec.yr.mmin$min_prcp 
ARIMA(1,1,0) with drift         

Coefficients:
          ar1    drift
      -0.5138  -5.0089
s.e.   0.2465   5.7986

sigma^2 estimated as 949:  log likelihood=-57.82
AIC=121.64   AICc=124.31   BIC=123.34

Training set error measures:
                     ME     RMSE      MAE       MPE     MAPE      MASE       ACF1
Training set -0.9479987 28.52129 23.83494 -2.484233 16.12547 0.7957998 -0.2617352
</code></pre></li>
<li><p>Using sarima to fit the model and output the slope</p>

<pre><code>  &gt; fit.min &lt;- sarima(dec.yr.mmin$min_prcp, 1,1,0,                       reg=dec.yr.mmin$decade)
initial  value 3.542448 
  iter   2 value 3.488927
  iter   3 value 3.386967
  iter   4 value 3.383464
  iter   5 value 3.382408
  iter   6 value 3.382051
  iter   7 value 3.382024
  iter   8 value 3.382020
  iter   9 value 3.381925
  iter   9 value 3.381925
  iter   9 value 3.381925
  final  value 3.381925 
  converged
  initial  value 3.400729 
  iter   2 value 3.399523
  iter   3 value 3.399490
  iter   4 value 3.399488
  iter   4 value 3.399488
  iter   4 value 3.399488
  final  value 3.399488 
  converged
</code></pre></li>
</ol>

<p>3.Output coefficients</p>

<pre><code>      &gt; fit.min$fit$coef
             ar1       xreg 
      -0.5137696 -0.5009045 
</code></pre>

<ol start=""4"">
<li><p>For comparison, this is the output from an OLS regression which may give an incorrect slope due to autocorrelation.</p>

<pre><code>  &gt; m3 &lt;- lm(dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)
  &gt; summary(m3)

  Call:
  lm(formula = dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)

  Residuals:
      Min      1Q  Median      3Q     Max 
  -45.504  -8.048   1.892  13.650  38.357 

  Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
  (Intercept)        1014.1570   319.9461   3.170  0.00807 **
  dec.yr.mmin$decade   -0.4222     0.1580  -2.672  0.02032 * 
  ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

  Residual standard error: 23.83 on 12 degrees of freedom
  Multiple R-squared:  0.3731,  Adjusted R-squared:  0.3209 
  F-statistic: 7.142 on 1 and 12 DF,  p-value: 0.02032
</code></pre>

<p>`</p></li>
</ol>

<p>The output from sarima identifies the slope coefficient and also intercept when d=0. When differencing is required (e.g., ARIMA (1,1,0) as output above), sarima only outputs the slope. </p>

<p>My question: when d = 1 or more, what approaches in R would allow me to add/visualize the regression line onto the original undifferenced time-series plot. Is it possible to derive the fitted values of the regression line, or derive intercept values from sarima/auto.arima or other package?</p>

<p>Many thanks in advance for your suggestions.</p>
"
"0.244135217806734","0.240290328416727","161941","<p>Something I rather vaguely asked a few months back, saw the tumbleweed roll by (actually hacked some hardware in the time, to get a few answers) before the question was deleted, so I'll try again on a slightly more specific note, as there will be a cleaner / quicker way to get the answer.</p>

<p>If I know a regression equation derives a particular answer from a given set of variables and coefficients, but I don't know the values for at least one set, how can I model this in R / get R to have a guess at deriving the unknowns.    </p>

<p>I ask as I was curious as to the maths embedded in number of Bioelectrical Impedance Analysis (BIA) devices I had around (A mix of Salter and Withings).
A quick Google revealed <a href=""http://pubs.sciepub.com/ijcn/2/1/1/"" rel=""nofollow"">dozens of published regression formula</a>, <a href=""http://ajcn.nutrition.org/content/64/3/436S.full.pdf"" rel=""nofollow"">to estimate: Total body Water / Fat / Lean Mass</a>, so was interested in which had made their way into the devices eg.</p>

<blockquote>
  <p>Lukaski &amp; Bolonchuk's (1988):</p>
  
  <p><strong>TBW</strong> = (0.372 * HeightCM * HeightCM /
  Resistance)  + (0.142 * massKg) - (0.069 * ageYears) + (3.05 * isMale)</p>
</blockquote>

<p>...</p>

<blockquote>
  <p>Matias et al. (2015): </p>
  
  <p><strong>TBW</strong> = 0.286 + (0.195 * HeightCM * HeightCM /
  Resistance) + (0.385 * massKg) + (5.086 * isMale)</p>
</blockquote>

<p>Anyway the devices show the values of all but any Resistance (R) and possibly imaginary Rectance (X) values used (Withings), so thought I'd pester / learn a bit of R to see if there was an alternative to an afternoon without socks, but with a multimeter, a few bits of wire and a tweaked iPad.</p>

<p><strong>FYI:</strong> My original five min play, in April, with a <strong>Salter 9141 WH3R</strong> suggested that:-</p>

<blockquote>
  <p><strong>BodyFat %</strong>  ~= x + (0.1 * AgeYears) + (0.4 * MassKg) - (8 * isMale) + y</p>
  
  <p><strong>TBW %</strong>  ~= x - (0.2 * AgeYears) - (0.27 * MassKg) + (7.58 * isMale) + y</p>
</blockquote>

<p>where x:  is some combination of: </p>

<p><strong>a * HeightCM^2 [+/] b * Resistance + c * HeightCM</strong></p>

<p>and with a <strong>Withings WS-50</strong> that:-</p>

<blockquote>
  <p><strong>BodyFat %</strong>  ~= X + (0.12 * AgeYears) + (0.29 * BodyMassKg) - (16.64 * isMale) + (5 * isJapanese) + z</p>
</blockquote>

<p>where X:  is some combination of: </p>

<p><strong>a * HeightCM^2 [+/] b * Resistance + c * HeightCM + d * Reactance</strong></p>

<p>So is there a way that I a can ger <strong>R</strong> to have a guess at solving these unknowns, or must I stick with a: screwdriver, multimeter, tweaked iPad and / or logging proxy (mitProxy / HoneyProxy).</p>

<pre><code># Quick play to try to identify a few of the constants used by the Salter 9141 WH3R body fat scale
# sex ( 1 = male, 0 = female)
sex &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
ageYrs &lt;- c(33, 33, 43, 43, 53, 53, 33, 43, 43, 53, 33, 43, 53, 33, 33, 43, 43, 53, 53, 33, 43, 43, 53, 33, 43, 53)
heightCM &lt;- c(191, 191, 191, 191, 191, 191, 181, 181, 181, 181, 171, 171, 171, 191, 191, 191, 191, 191, 191, 181, 181, 181, 181, 171, 171, 171)
heightCM2 &lt;- heightCM * heightCM
massKg &lt;- c(81.6, 83, 81.6, 83, 81.6, 83, 81.6, 81.6, 83, 81.6, 81.6, 81.6, 81.6, 81.6, 83, 81.6, 83, 81.6, 83, 81.6, 81.6, 83, 81.6, 81.6, 81.6, 81.6)
bodyWaterPct &lt;- c(62, 61.6, 59.9, 59.6, 57.9, 57.5, 58, 55.9, 55.5, 53.9, 53.2, 51.2, 49.2, 54.3, 54, 52.3, 52, 50.3, 50, 50.4, 48.4, 47.9, 46.4, 45.7, 43.6, 41.6)
bodyFatPct &lt;- c(17.1, 17.7, 18.1, 18.7, 19.2, 19.8, 22.6, 23.7, 24.2, 24.7, 29.1, 30.1, 31.1, 25.2, 25.7, 26.2, 26.7, 27.2, 27.7, 30.5, 31.6, 32.3, 32.9, 37, 38.1, 39.1)

bodyFat.Salter = data.frame(sex, ageYrs, heightCM, heightCM2, massKg, bodyWaterPct, bodyFatPct)
bodyFat.Salter
summary(bodyFat.Salter)
fitBodyFat1 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + massKg, data=bodyFat.Salter)
fitBodyFat2 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM2 + massKg, data=bodyFat.Salter)
fitBodyFat3 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Salter)
fitBodyWater1 &lt;- lm ( bodyWaterPct ~ sex + ageYrs + heightCM + massKg, data=bodyFat.Salter)
fitBodyWater2 &lt;- lm ( bodyWaterPct ~ sex + ageYrs + heightCM2 + massKg, data=bodyFat.Salter)
fitBodyWater3 &lt;- lm ( bodyWaterPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Salter)
summary(fitBodyFat1)
summary(fitBodyFat2)
summary(fitBodyFat3)
summary(fitBodyWater1)
summary(fitBodyWater2)
summary(fitBodyWater3)

library(glmulti)
fitBodyFatG1 &lt;- glm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Salter)
test.model1 &lt;- glmulti(fitBodyFatG1, level = 1, crit=""aicc"")
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>&gt; summary(fitBodyFat1)

Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM + massKg, data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.42607 -0.20940  0.08889  0.15588  0.27051 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 101.747418   6.105948  16.664 1.39e-13 ***
sex          -8.007692   0.092545 -86.528  &lt; 2e-16 ***
ageYrs        0.105000   0.005899  17.801 3.80e-14 ***
heightCM     -0.591111   0.006422 -92.051  &lt; 2e-16 ***
massKg        0.400794   0.079446   5.045 5.39e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2359 on 21 degrees of freedom
Multiple R-squared:  0.9988,    Adjusted R-squared:  0.9986 
F-statistic:  4442 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyFat2)

Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM2 + massKg, 
    data = bodyFat.Salter)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.5345 -0.3141  0.1192  0.2024  0.3494 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  4.843e+01  8.409e+00   5.759 1.02e-05 ***
sex         -8.008e+00  1.238e-01 -64.696  &lt; 2e-16 ***
ageYrs       1.050e-01  7.889e-03  13.310 1.05e-11 ***
heightCM2   -1.626e-03  2.365e-05 -68.758  &lt; 2e-16 ***
massKg       3.972e-01  1.063e-01   3.738  0.00121 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.3156 on 21 degrees of freedom
Multiple R-squared:  0.9979,    Adjusted R-squared:  0.9975 
F-statistic:  2481 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyFat3)

Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + 
    massKg, data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.12607 -0.02949 -0.01111  0.03162  0.17393 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 249.776918   9.660823   25.86  &lt; 2e-16 ***
sex          -8.007692   0.026174 -305.94  &lt; 2e-16 ***
ageYrs        0.105000   0.001668   62.94  &lt; 2e-16 ***
heightCM     -2.225111   0.104938  -21.20 3.53e-15 ***
heightCM2     0.004500   0.000289   15.57 1.20e-12 ***
massKg        0.400794   0.022469   17.84 9.48e-14 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.06673 on 20 degrees of freedom
Multiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 
F-statistic: 4.448e+04 on 5 and 20 DF,  p-value: &lt; 2.2e-16

...

Call:
lm(formula = bodyWaterPct ~ sex + ageYrs + heightCM + massKg, 
    data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.19309 -0.12797 -0.06759  0.18355  0.31346 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.639382   4.922117   0.130  0.89788    
sex          7.576923   0.074602 101.565  &lt; 2e-16 ***
ageYrs      -0.202500   0.004755 -42.587  &lt; 2e-16 ***
heightCM     0.432037   0.005177  83.460  &lt; 2e-16 ***
massKg      -0.269841   0.064043  -4.213  0.00039 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.1902 on 21 degrees of freedom
Multiple R-squared:  0.999, Adjusted R-squared:  0.9988 
F-statistic:  5087 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyWater2)

Call:
lm(formula = bodyWaterPct ~ sex + ageYrs + heightCM2 + massKg, 
    data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.25119 -0.16896 -0.09268  0.25881  0.39269 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.960e+01  6.631e+00   5.972  6.3e-06 ***
sex          7.577e+00  9.760e-02  77.635  &lt; 2e-16 ***
ageYrs      -2.025e-01  6.221e-03 -32.553  &lt; 2e-16 ***
heightCM2    1.188e-03  1.865e-05  63.728  &lt; 2e-16 ***
massKg      -2.670e-01  8.378e-02  -3.187  0.00443 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2488 on 21 degrees of freedom
Multiple R-squared:  0.9982,    Adjusted R-squared:  0.9979 
F-statistic:  2970 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyWater3)

Call:
lm(formula = bodyWaterPct ~ sex + ageYrs + heightCM + heightCM2 + 
    massKg, data = bodyFat.Salter)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.078205 -0.027991 -0.002778  0.038408  0.069017 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.200e+02  6.685e+00  -17.95 8.46e-14 ***
sex          7.577e+00  1.811e-02  418.32  &lt; 2e-16 ***
ageYrs      -2.025e-01  1.154e-03 -175.41  &lt; 2e-16 ***
heightCM     1.763e+00  7.262e-02   24.28 2.58e-16 ***
heightCM2   -3.667e-03  2.000e-04  -18.34 5.63e-14 ***
massKg      -2.698e-01  1.555e-02  -17.35 1.59e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.04618 on 20 degrees of freedom
Multiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 
F-statistic: 6.911e+04 on 5 and 20 DF,  p-value: &lt; 2.2e-16

...

Initialization...
TASK: Exhaustive screening of candidate set.
Fitting...

After 50 models:
Best model: bodyFatPct~1+sex+ageYrs+heightCM+heightCM2+massKg
Crit= -53.5831033999792
Mean crit= 117.663945821469
Completed.
</code></pre>

<p><strong>Withings</strong></p>

<pre><code>...
fitBodyFat1 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + massKg, data=bodyFat.Withings)
fitBodyFat2 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM2 + massKg, data=bodyFat.Withings)
fitBodyFat3 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Withings)
summary(fitBodyFat1)
summary(fitBodyFat2)
summary(fitBodyFat3)

library(glmulti)
fitBodyFatG1 &lt;- glm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Withings)
test.model1 &lt;- glmulti(fitBodyFatG1, level = 1, crit=""aicc"")
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM + massKg, data = bodyFat.Withings)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.3348 -0.5042  0.0597  0.5361  6.5767 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 120.39668    9.97149  12.074  &lt; 2e-16 ***
sex         -16.64221    0.19636 -84.754  &lt; 2e-16 ***
ageYrs        0.12039    0.01490   8.082 2.78e-13 ***
heightCM     -0.61641    0.01784 -34.552  &lt; 2e-16 ***
massKg        0.29065    0.11652   2.494   0.0138 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.9957 on 139 degrees of freedom
Multiple R-squared:  0.9848,    Adjusted R-squared:  0.9844 
F-statistic:  2251 on 4 and 139 DF,  p-value: &lt; 2.2e-16

...

Initialization...
TASK: Exhaustive screening of candidate set.
Fitting...

After 50 models:
Best model: bodyFatPct~1+sex+ageYrs+heightCM+massKg
Crit= 414.926000298605
Mean crit= 752.216409496029
Completed.
</code></pre>
"
"0.106701449104437","0.105021006302101","162174","<p>I have performed a Poisson Regression in R, but I got strange results that I cannot find an answer for.
My data is like this:</p>

<pre><code> Aspect_16    Nr_Pereti
1   E         49
2   ENE       73
3   ESE       29
4   N         84
5   NE        77
6   NNE       99
7   NNW       77
8   NW        92
9   S         19
10  SE        20
11  SSE       9
12  SSW       17
13  SW        23
14  W         39
15  WNW       56
16  WSW       25
</code></pre>

<p>The Nr_Pereti variable are counts for each level in the 'Aspect_16' column.
The model formula and results are:</p>

<p>summary(model_nr_exp)</p>

<pre><code>Call:
glm(formula = tab_gen_exp$Nr_Pereti ~ tab_gen_exp$Aspect_16, 
    family = poisson)

Deviance Residuals: 
 [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0

Coefficients:
                         Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                3.8918     0.1429  27.243  &lt; 2e-16 ***
tab_gen_exp$Aspect_16ENE   0.3986     0.1847   2.159 0.030886 *  
tab_gen_exp$Aspect_16ESE  -0.5245     0.2343  -2.239 0.025169 *  
tab_gen_exp$Aspect_16N     0.5390     0.1798   2.998 0.002714 ** 
tab_gen_exp$Aspect_16NE    0.4520     0.1827   2.473 0.013386 *  
tab_gen_exp$Aspect_16NNE   0.7033     0.1747   4.026 5.66e-05 ***
tab_gen_exp$Aspect_16NNW   0.4520     0.1827   2.473 0.013386 *  
tab_gen_exp$Aspect_16NW    0.6300     0.1769   3.562 0.000368 ***
tab_gen_exp$Aspect_16S    -0.9474     0.2703  -3.505 0.000456 ***
tab_gen_exp$Aspect_16SE   -0.8961     0.2653  -3.377 0.000733 ***
tab_gen_exp$Aspect_16SSE  -1.6946     0.3627  -4.673 2.97e-06 ***
tab_gen_exp$Aspect_16SSW  -1.0586     0.2815  -3.761 0.000169 ***
tab_gen_exp$Aspect_16SW   -0.7563     0.2528  -2.992 0.002769 ** 
tab_gen_exp$Aspect_16W    -0.2283     0.2146  -1.064 0.287468    
tab_gen_exp$Aspect_16WNW   0.1335     0.1956   0.683 0.494845    
tab_gen_exp$Aspect_16WSW  -0.6729     0.2458  -2.738 0.006182 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 2.9506e+02  on 15  degrees of freedom
Residual deviance: 2.1316e-14  on  0  degrees of freedom
AIC: 120.29

Number of Fisher Scoring iterations: 3
</code></pre>

<p>I don't have any Deviance Residuals for the model, and when I try to plot the model, it gives me this error:</p>

<pre><code>Error in qqnorm.default(rs, main = main, ylab = ylab23, ylim = ylim, ...) : 
  y is empty or has only NAs
In addition: Warning messages:
1: not plotting observations with leverage one:
  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 
2: In min(x) : no non-missing arguments to min; returning Inf
3: In max(x) : no non-missing arguments to max; returning -Inf
</code></pre>

<p>What did I do wrong here? Thanks.</p>
"
"0.106701449104437","0.105021006302101","163922","<p>I am trying to find any evidence of warming in monthly times series data of water temperature over a 21-year period that is serially correlated. Essentially I am looking to determine a global trend, like what can be done with OLS regression with data that is from independent observations. I am at a crossroads in trying to determine whether a seasonal ARIMA model or a linear mixed model with a trend component as detailed by Crawley on page 799 of ""The R Book"" (2nd ed.) is the most appropriate method to use. I therefore explored both techniques, but got very contradicting answers!</p>

<p>ARIMA modelling gave me a seasonal ARIMA of form (2,0,2)(0,0,1)[12], indicating that no differencing is required and therefore that the series is stationary with NO trend.</p>

<p>However, the linear mixed affects modelling, comparing two models with and without a trend component using ANOVA and maximum likelihood indicated a highly significant trend (R notation):</p>

<pre><code>model2: ave ~ sin(time * 2 * pi) + cos(time * 2 * pi) + (1 | factor(yr))

model1: ave ~ index + sin(time * 2 * pi) + cos(time * 2 * pi) + (1 | factor(yr))

ANOVA(model2,model1)

      Df  AIC     BIC     logLik deviance Chisq Chi Df Pr(&gt;Chisq)   
model2 5 346.82   364.49    -168.41   336.82                           
model1 6 338.54   359.74    -163.27   326.54 10.28      1   0.001345 **
</code></pre>

<p>How can this be? What am I missing? Is it about assuming whether the trend is a parametric form (appropriate for linear mixed model) or whatever weird shape (appropriate for ARIMA)? If so how do I go about choosing which approach to adopt?</p>

<p>Thank you kindly for any advice.</p>
"
"0.0616041103633697","0.0606339062590832","164101","<p>I used the multiple regression models to derive the outcomes after using AIC pairwise comparison and deleted the outliers, high leverage points. </p>

<p>And it seems good, the adjusted R^2 acheived 0.9543, see below:</p>

<pre><code>Call:
lm(formula = P ~ V + EF + W + H, data = PS)

Residuals:
Min      1Q     Median      3Q     Max 
-22.448 -14.576   2.949  12.524  26.034 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -2.186e+03  5.209e+02  -4.196 0.000201 ***
V           1.511e-01  1.033e-02  14.633 9.96e-16 ***
EF           1.183e+01  3.239e+00   3.653 0.000917 ***
W           -1.135e+00  4.205e-01  -2.698 0.011028 *  
H            3.192e+01  8.482e+00   3.763 0.000678 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 15.79 on 32 degrees of freedom
(13 observations deleted due to missingness)
Multiple R-squared:  0.9593, Adjusted R-squared:  0.9543 
F-statistic: 188.8 on 4 and 32 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>However, when I was inspecting the residuals, they are fairly high.
With the R^2 higher than 0.95. I expect the residuals should be around 5% but actually not. It's kind of weird. See below for the (actual - fitted)/actual.</p>

<pre><code>tidy((PS$P-fitted(fita2))/PS$P)
         x
1   0.01778472
2  -0.19525412
3  -0.01824948
4  -0.24418585
5  -0.06475068
6   0.65211477
7   0.58158990
8  -0.14876657
9  -0.27050744
10  0.15220738
11  0.14239352
12  0.02274694
13  0.10920921
14  0.04696290
15 -0.07793881
16 -0.32173830
17 -0.60332883
18 -1.47499192
19 -0.70576325
20 -0.04088402
21 -1.11266825
22 -1.13704286
23 -0.72987082
24 -0.50573858
25  0.38938329
26  0.57790490
27  0.21233140
28 -0.42890622
29  0.27630390
</code></pre>

<p>Plot of Residuals vs Fitted:</p>

<p><a href=""http://i.stack.imgur.com/c0KpM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/c0KpM.png"" alt=""enter image description here""></a></p>

<p>Normal Q-Q plot:</p>

<p><a href=""http://i.stack.imgur.com/R5jBB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/R5jBB.png"" alt=""enter image description here""></a></p>
"
"0.162989155760853","0.148963505767084","164228","<p>GLM (family=binomial) is foucusd on when the response is dichotomous(yes/no, male/female, etc..). I'm wondering how to judge if the model we built is good eough? As we know, in OLS regression some criterion like R^2 and adjusted R^2 can tell us how much variations are explained but not for GLM. See example I performed:</p>

<pre><code>    &gt; summary(fit.full)
    Call:
    glm(formula = ynaffair ~ gender + age + yearsmarried + children + 
    +religiousness + education + occupation + rating, family = binomial(), 
    data = Affairs)

    Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
    -1.6575  -0.7459  -0.5714  -0.2552   2.5099  

    Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)    0.71792    0.96165   0.747 0.455336    
    gendermale     0.28665    0.23973   1.196 0.231811    
    age           -0.04494    0.01831  -2.454 0.014142 *  
    yearsmarried   0.09686    0.03236   2.993 0.002758 ** 
    childrenyes    0.37088    0.29466   1.259 0.208147    
    religiousness -0.32230    0.09003  -3.580 0.000344 ***
    education      0.01795    0.05088   0.353 0.724329    
    occupation     0.03210    0.07194   0.446 0.655444    
    rating2       -0.02312    0.58177  -0.040 0.968303    
    rating3       -0.84532    0.57619  -1.467 0.142354    
    rating4       -1.13916    0.55740  -2.044 0.040981 *  
    rating5       -1.61050    0.56649  -2.843 0.004470 ** 
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 675.38  on 600  degrees of freedom
    Residual deviance: 608.22  on 589  degrees of freedom
    AIC: 632.22
</code></pre>

<p>After removed the insignificant variables, the reduced model look like below,although the AIC decreasd, we still do not know if this is the model with the lowest AIC we can achieved:</p>

<pre><code>    &gt; summary(fit.reduced)
    Call:
    glm(formula = ynaffair ~ age + yearsmarried + religiousness + 
        +rating, family = binomial(), data = Affairs)

    Deviance Residuals: 
    Min        1Q      Median      3Q      Max  
   -1.5117  -0.7541  -0.5722  -0.2592   2.4123  

    Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)    1.10220    0.71849   1.534 0.125014    
    age           -0.03588    0.01740  -2.062 0.039224 *  
    yearsmarried   0.10113    0.02933   3.448 0.000565 ***
    religiousness -0.32571    0.08971  -3.631 0.000282 ***
    rating2        0.11848    0.57258   0.207 0.836068    
    rating3       -0.70168    0.56671  -1.238 0.215658    
    rating4       -0.96190    0.54230  -1.774 0.076109 .  
    rating5       -1.49502    0.55550  -2.691 0.007118 ** 
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 675.38  on 600  degrees of freedom
    Residual deviance: 613.63  on 593  degrees of freedom
    AIC: 629.63
</code></pre>

<p>And we perform the ANOVA, suggesting that the reduced model with
four predictors fits as well as the full model:</p>

<pre><code>    &gt; anova(fit.reduced, fit.full, test=""Chisq"")
    Analysis of Deviance Table

    Model 1: ynaffair ~ age + yearsmarried + religiousness + +rating
    Model 2: ynaffair ~ gender + age + yearsmarried + children + 
             +religiousness + education + occupation + rating
    Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
     1       593     613.63                     
     2       589     608.22  4   5.4124   0.2475
</code></pre>
"
"0.0754493182241785","0.0742610657232506","164434","<p>Sometimes during modeling we are faced with non-normal data.  One of the presumptions of linear regression is the response data being normal.</p>

<p><em>I wish I found a better example data set to illustrate this.  The cars data doesn't need this proposed method...</em></p>

<p>I never seen this done in a book nor in a class so most likely this is not a good idea. I am curious what others think of this method.  Is there a documented approach that works similar to this?</p>

<p>One method to make continuous response data normal is to take the rank converted to a percentile and bound it between 0.001 and 0.999. (Winsoring it.) Lastly converted it to a zscore.</p>

<pre><code>cars2=cars;
cars2$dist=qnorm(pmin(0.999,pmax(0.001,rank(cars2$dist)/length(rank(cars2$dist)))))
</code></pre>

<p>Then with the original data, making a mapping variable for the values 1:999</p>

<pre><code>map=(quantile(cars$dist,probs = seq(0.001,0.999,0.001)))
</code></pre>

<ul>
<li>skewness(cars dist) is 0.7824835 </li>
<li>skewness(cars2 dist) is 0.378929</li>
<li>rmse(cars dist,fit) is 15.06886    </li>
<li>rmse(cars dist,fit2) is 14.80842</li>
</ul>

<p>So the skew is reduced and performs slightly better.
On large data sets this method almost assuredly gets rid of the skew in the response.</p>

<p>Here is plainly the code:</p>

<pre><code>require(datasets)

hist(cars$speed)
    hist(cars$dist)
m=lm(dist~speed,cars)
fit=predict(m)
skewness(cars$dist)
summary(m)

cars2=cars;
cars2$dist=qnorm(pmin(0.999,pmax(0.001,rank(cars2$dist)/length(rank(cars2$dist)))))
    map=(quantile(cars$dist,probs = seq(0.001,0.999,0.001)))
hist(cars2$dist)
    skewness(cars2$dist)
length(map)
hist(map)
m3=lm(dist~speed,cars2); 
m3=stepAIC(m3,trace=F)
summary(m3)
data=round(pnorm(predict(m3))*1000)
range(data)
fit2=map[data]
plot(cars$dist,fit2,col=""blue"")
    points(cars$dist,fit,col=""red"")

rmse=function(x,y,k=0){
  return( sqrt(sum((x-y)^2)/(length(x)-k)));
}
rmse(cars$dist,fit)
    rmse(cars$dist,fit2)
</code></pre>

<p>So how crazy of an idea is this?
Has this approach been documented/studied before? Where?</p>

<p>Thank you for you commentary. :)</p>
"
"0.0889178742536973","0.105021006302101","164541","<p>I am attempting to do a logistic regression bootstrap with R. The problem is I get high SE's. I'm not sure what to do about this or what it means. Does it mean that bootstrap does not work well for my particular data? Here is my code:</p>

<pre><code>get.coeffic = function(data, indices){
  data    = data[indices,]
  mylogit = glm(F~B+D, data=data, family=""binomial"")
  return(mylogit$coefficients)
}

Call:
boot(data = Pres, statistic = logit.bootstrap, R = 1000)

Bootstrap Statistics :
       original      bias    std. error
t1* -10.8609610 -23.0604501  338.048398
t2*   0.2078474   0.4351766    6.387781
</code></pre>

<p>I also want to know that after bootstrapping, how would this help with my final regression model? That is, how do I find what regression coefficient do I use in my final model?</p>

<pre><code>&gt; fit &lt;- glm(F ~ B + D , data = President, family = ""binomial"")
&gt; summary(fit)
Call:
glm(formula = F ~ B + D, family = ""binomial"", data = President)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7699  -0.5073   0.1791   0.8147   1.2836  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -14.57829    8.98809  -1.622   0.1048  
B             0.15034    0.14433   1.042   0.2976  
D             0.13385    0.08052   1.662   0.0965 .
- --
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 23.508  on 16  degrees of freedom
Residual deviance: 14.893  on 14  degrees of freedom
AIC: 20.893

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.13068205256071","0.128623938856882","167363","<p>I have no training in Bayesian data analysis, so I can't wrap my head around how to start solving the following problem and am hoping you can help:</p>

<p>I am using linear regression to forecast the net scores (home - visitor) of (American) pro-football games from differences in team-strength scores (home - visitor). Those strength scores fall on a 0-100 scale, and they represent the percent chance that the team in question would beat another team selected at random from the 31 others in the league. The differences between those strength scores and the net game scores are both normally distributed.</p>

<p>Right now, I am using team-strength scores that are fixed for the entire season in a mixed-effects model that also includes random intercepts for each team as the home team. The strength scores are fixed because they come from a preseason survey. I would like to see if I can make the predictions more accurate by using Bayesian updating to allow that team-strength score to vary over the course of the season, as we learn more about how teams are performing relative to preseason expectations.</p>

<p>The single piece of information that strikes me as most useful in that regard is the cumulative sum of each team's prediction errors --- in other words, the cumulative sum of the differences between the team's predicted game performance (based on the preseason strength scores and where each game is played) and its actual game performance. </p>

<p>How might I go about doing that? In R, I have gotten as far as computing those cumulative errors, which turn out to be normally distributed for the season with a mean of ~0 and sd of ~50. I have tinkered with algebraic ways to adjust the strength scores as a function of that cumulative error. The forecasts based on those algebraic adjustments are slightly more accurate, but the approach seems clunky, and I'd like to use this problem as an opportunity to learn about Bayesian updating if I can. Any suggestions on how to do that in the context of this problem --- and, ideally, in R --- would be much appreciated.</p>
"
"0.0974046509883157","0.0958706236059213","169291","<p>I have a logistic regression model below, predicting a dichotomous variable <em>type</em> from a single continuous predictor <em>fatigue</em>. Using the coefficients below I can obtain the increase in the odds of a positive <em>type</em> from a 1 unit increase in fatigue.</p>

<p>Also I believe by forming the model expression</p>

<pre><code>logit(type) = 0.3134 - 91.1171 * fatigue 
</code></pre>

<p>I can obtain the odds of a positive <em>type</em> for a given value of <em>fatigue</em> by plugging it in, say for a value <em>fatigue</em> = 1.</p>

<p><strong>However</strong>, what I want to do is to obtain the odds of a positive <em>type</em> for a range of <em>fatigue</em> values, i.e. <strong>&lt;= 0</strong>. Is this possible?</p>

<pre><code>## Call:
## glm(formula = type ~ fatigue, family = binomial(), data = myData)

## Deviance Residuals:
## Min 1Q Median 3Q Max
## -1.6703 -1.3104 0.8369 1.0049 1.4695
##
## Coefficients:
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) 0.3134 0.1496 2.095 0.0362 *
## fatigue -91.1171 36.3785 -2.505 0.0123 *
## ---
## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
## Null deviance: 282.84 on 210 degrees of freedom
## Residual deviance: 276.03 on 209 degrees of freedom
## AIC: 280.03
##
## Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.11525073729837","0.113435651621629","169438","<p>As we all know, there are 2 methods to evaluate the logistic regression model and 
they are testing very different things</p>

<ol>
<li><p>Predictive power:</p>

<p>Get a statistic that measures how well you can predict the dependent variable 
based on the independent variables. The well-know Pseudo R^2 are McFadden 
(1974) and Cox and Snell (1989).</p></li>
<li><p>Goodness-of-fit statistics</p>

<p>The test is telling whether you could do even better by making the model more 
complicated, which is actually testing whether there are any non-linearities or 
interactions.</p>

<p>I implemented both tests on my model, which added quadratic and interaction<br>
already: </p>

<pre><code>&gt;summary(spec_q2)

Call:
glm(formula = result ~ Top + Right + Left + Bottom + I(Top^2) + 
 I(Left^2) + I(Bottom^2) + Top:Right + Top:Bottom + Right:Left, 
 family = binomial())

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.955431   8.838584   0.108   0.9139    
Top          0.311891   0.189793   1.643   0.1003    
Right       -1.015460   0.502736  -2.020   0.0434 *  
Left        -0.962143   0.431534  -2.230   0.0258 *  
Bottom       0.198631   0.157242   1.263   0.2065    
I(Top^2)    -0.003213   0.002114  -1.520   0.1285    
I(Left^2)   -0.054258   0.008768  -6.188 6.09e-10 ***
I(Bottom^2)  0.003725   0.001782   2.091   0.0366 *  
Top:Right    0.012290   0.007540   1.630   0.1031    
Top:Bottom   0.004536   0.002880   1.575   0.1153    
Right:Left  -0.044283   0.015983  -2.771   0.0056 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 3350.3  on 2799  degrees of freedom
Residual deviance: 1984.6  on 2789  degrees of freedom
AIC: 2006.6
</code></pre></li>
</ol>

<p>and the predicted power is as below, the MaFadden is 0.4004, and the value between 0.2~0.4 should be taken to present very good fit of the model(Louviere et al (2000), Domenich and McFadden (1975))                                                :</p>

<pre><code> &gt; PseudoR2(spec_q2)
    McFadden     Adj.McFadden        Cox.Snell       Nagelkerke McKelvey.Zavoina           Effron            Count        Adj.Count 
   0.4076315        0.4004680        0.3859918        0.5531859        0.6144487        0.4616466        0.8489286        0.4712500 
         AIC    Corrected.AIC 
2006.6179010     2006.7125925 
</code></pre>

<p>and the goodness-of-fit statistics:</p>

<pre><code> &gt; hoslem.test(result,phat,g=8)

     Hosmer and Lemeshow goodness of fit (GOF) test

  data:  result, phat
  X-squared = 2800, df = 6, p-value &lt; 2.2e-16
</code></pre>

<p>As my understanding, GOF is actually testing the following null and alternative hypothesis:</p>

<pre><code>  H0: The models does not need interaction and non-linearity
  H1: The models needs interaction and non-linearity
</code></pre>

<p>Since my models added interaction, non-linearity already and the p-value shows H0 should be rejected, so I came to the conclusion that my model needs interaction, non-linearity indeed. Hope my interpretation is correct and thanks for any advise in advance, thanks. </p>
"
"0.0754493182241785","0.0742610657232506","171879","<p>I have the R output for the logistic regression model. It seems that only the intercept and psa are statistically significant. Does that mean I should remove sorbets_psa and cinko from my model and create a new model as new.model = glm(status ~ psa,family = binomial(link =""probit""))</p>

<pre><code>Call:
glm(formula = status ~ psa + serbest_psa + cinko, family = binomial(link =""probit""), data = data)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.3285  -0.6773  -0.6261  -0.5604   1.9500  

Coefficients:
      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.9697009  0.2409856  -4.024 5.72e-05 ***
psa          0.0444376  0.0094368   4.709 2.49e-06 ***
serbest_psa -0.0440718  0.0250486  -1.759   0.0785 .  
cinko       -0.0006923  0.0016984  -0.408   0.6835    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 534.27  on 477  degrees of freedom
Residual deviance: 477.07  on 474  degrees of freedom
AIC: 485.07

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.151347073206507","0.148963505767084","172943","<p>I'm trying to understand the output of <code>glm</code> when a categorical variable has more than 2 categories.</p>

<p>I'm analysing if age affects death. Age is a categorical variable with 4 categories</p>

<p>I use the following code in R:</p>

<pre><code>mydata &lt;- read.delim(""Data.txt"", header = TRUE)
mydata$Agecod &lt;- factor(mydata$Agecod)
mylogit &lt;- glm(Death ~ Agecod, data = mydata, family = ""binomial"")
summary(mylogit)
</code></pre>

<p>Obtaining the following output: </p>

<pre><code>Call:
glm(formula = Death ~ Agecod, family = ""binomial"", data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4006  -0.8047  -0.8047   1.2435   2.0963  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.5108     0.7303   0.699   0.4843  
Agecod2      -0.6650     0.7715  -0.862   0.3887  
Agecod3      -1.4722     0.7658  -1.922   0.0546 .
Agecod4      -2.5903     1.0468  -2.474   0.0133 *

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 237.32  on 184  degrees of freedom
Residual deviance: 223.73  on 181  degrees of freedom
  (1 observation deleted due to missingness)
AIC: 231.73

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Since I have p-values for <code>Agecod2</code>, <code>Agecod3</code> and <code>Agecod4</code> and only <code>Agecod4</code> has a significant p-value my questions are:</p>

<ol>
<li>Is really <code>Age</code> associated with death?</li>
<li>Is only the 4th age category associated with death?</li>
<li>What happens with the first category since I don't have its p-value?</li>
</ol>

<p>Update:</p>

<p>Since Antoni Parellada says â€œIt seems as though you have proven that old age is a good predictor of deathâ€ and Gung points â€œYou cannot tell from your output if Age is associated with deathâ€ Iâ€™m still confused.</p>

<p>I understand that â€œInterceptâ€ is representing Agecod1 and is the â€œreference levelâ€. According to Gung â€œThe Estimates for the rest are the differences between the indicated level and the reference level. The associated p-values are for the tests of the indicated level vs. the reference level in isolation.â€ </p>

<p>My question now is: </p>

<p>Since Agecod4 p-value (0.0133) is significantly different from Agecod1 (reference lelvel) it doesnâ€™t mean that age is associated with death?</p>

<p>I have also tried to perform a nested test with the following command:</p>

<pre><code>anova(mylogit, test=""LRT"")
</code></pre>

<p>Obtaining:</p>

<pre><code>       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   
NULL                     184     237.32            
Agecod  3   13.583       181     223.73 0.003531 *
</code></pre>

<p>Does it mean that Age is definitively associated with death?</p>

<p>Update2:</p>

<p>I have solved my problem using binary logistic regression in SPSS. The output is the same than â€œmylogitâ€ but with SPSS I obtain a global p-value for the overall variable Agecod which is 0.008.</p>

<p>I donâ€™t know if is possible to obtain this â€œglobal p-valueâ€ with R, but since I know that I can use SPSS is not a big problem for me.</p>
"
"0.0711342994029579","0.105021006302101","173629","<p>When applying the ""urca"" package function <code>ur.df</code>, like </p>

<pre><code>summary(ur.df(data$col1, type = c(""none""), lags = 12, selectlags = c(""AIC"")))
</code></pre>

<p>I get following result:</p>

<pre><code>############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
      Min        1Q    Median        3Q       Max 
-12928366  -2888728   1284718   4218373   7179531 

Coefficients:
                 Estimate    Std. Error  t value  Pr(&gt;|t|)   
(Intercept)  5.391984e+07  1.638362e+07  3.29108 0.0043123 **
z.lag.1     -2.438154e+00  7.557134e-01 -3.22629 0.0049588 **
tt           6.579260e+05  2.730453e+05  2.40959 0.0275861 * 
z.diff.lag1  1.712004e+00  6.595980e-01  2.59553 0.0188537 * 
z.diff.lag2  1.402824e+00  6.379412e-01  2.19899 0.0420083 * 
z.diff.lag3  1.321555e+00  5.294537e-01  2.49607 0.0231329 * 
z.diff.lag4  1.099430e+00  4.720412e-01  2.32910 0.0324428 * 
z.diff.lag5  8.132753e-01  4.181477e-01  1.94495 0.0685140 . 
z.diff.lag6  1.797331e-01  3.654326e-01  0.49184 0.6291254   
z.diff.lag7  5.890640e-01  2.939590e-01  2.00390 0.0612825 . 
z.diff.lag8  3.919041e-01  2.794371e-01  1.40248 0.1787705   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6708593 on 17 degrees of freedom
Multiple R-squared:  0.7237276, Adjusted R-squared:  0.5613144 
F-statistic: 4.253547 on 10 and 17 DF,  p-value: 0.003348755


Value of test-statistic is: -3.2263 3.9622 5.2635 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -4.15 -3.50 -3.18
phi2  7.02  5.13  4.31
phi3  9.31  6.73  5.61
</code></pre>

<p>Now the question:</p>

<ol>
<li>I do understand that ""-3.2263"" is the critical value (t-value)</li>
<li><strong>There is a unit root</strong> with trend since -3.2263 > -3.18 (tau3@10pct)
This means the time-series is <strong>non-stationary</strong> at a 10% significance level.</li>
<li>But, what is the meaning of ""p-value: 0.003348755""? Should I list this value in a table summarizing my unit root test results or rather mark the 0.1 significance level (*10%)?</li>
</ol>

<p>The <a href=""http://www.inside-r.org/packages/cran/urca/docs/ur.df"" rel=""nofollow"">documentation</a> says that critical values are based on Hamilton (1994) and Dickey and Fuller (1981)"". </p>
"
"0.131340404599205","0.129271922498755","173996","<p>I'm using R (package lmer) to run linear mixed model My study looks at allergy levels of skin patches from patients and readings (repeated 5 times) are measured over 4 time points.</p>

<p>I need to determine if the allergy level for skin patch changes over time
(e.g., if allergy level from skin patch 1 for patient 1 at time 0 is different from allergy level for skin patch 1 for patient 1 at time 1 etc.) I do not want to see the difference between skin patch 1 and skin patch 2. Using package lmer:  </p>

<pre><code>model &lt;- lmer(allergy_level ~ time +(time|patient/patch))
</code></pre>

<p><strong>Results from this model indicate that time is not significant - the average patient allergy level for individual skin patches does not change over time</strong> (see below for output). However, <strong>I need to be able to tell if there is a significant difference for individual patches for individual patients over time</strong>.</p>

<p>If I run individual regression models for each skin patch for each patient, this will result in a large number of models as I have There are 16 skin patches per patient. (10 patients in total) 5 readings are taken at each of the 4 time points. I thought linear mixed models would be an appropriate method to answer my question (I need to be able to tell if there is a significant difference for individual patches for individual patients over time). </p>

<p>Output:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr             
 ID:patch (Intercept) 17.4109  4.1726                    
          time1        2.7109  1.6465   -0.30            
          time2        3.0082  1.7344   -0.26  0.60      
          time3        5.7643  2.4009   -0.35  0.15  0.54
 patch    (Intercept) 19.1576  4.3769                    
          time1        0.2103  0.4586   -0.56            
          time2        0.4372  0.6612   -0.94  0.48      
          time3        0.5895  0.7678   -0.48  0.96  0.49
 Residual              4.9467  2.2241                    
Number of obs: 2956, groups:  ID:patch, 149; patch, 16

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.44763    1.15028   5.605
time1       -0.01907    0.21237  -0.090
time2       -0.03172    0.24759  -0.128
time3       -0.01124    0.29940  -0.038

model1: AllergyLevel ~ 1 + (1 + time | patch/ID)
model2: AllergyLevel ~ time + (1 + time | patch/ID)
         Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
model11 22 14281 14413 -7118.5    14237                         
model12 25 14287 14437 -7118.4    14237 0.0208      3     0.9992
</code></pre>

<p>I have extracted the random coefficients from model 1:</p>

<pre><code>ranef(model1)

`ID:patch`
      (Intercept)       time1        time2        time3
1:11    5.9845070  0.34088535  0.431998708  1.590906238
1:12    5.1236456 -0.03178611 -0.149784278 -0.116150278
1:13    6.3746877 -0.76853294 -0.550037715  0.842518786
   :
   :
</code></pre>
"
"0.110200782772715","0.13558153613666","174252","<p>I have built some ""regular"" and robust regression models, using the standard lm function as well as rlm and lmrob.  While I know that there is some discussion about using stepwise regression, I have used the stepAIC function to prune my variable set.  After I've gotten a reduced set of variables using stepAIC, I've then run some robust regressions.</p>

<p>The <strong>cvTools</strong> package allows me to use cross validation to compare the performance of my various models.</p>

<p>I obviously would like to run lasso regression (ideally using the <strong>glmnet</strong> package) on my dataset.</p>

<p>My question is whether or not there is an already built package/functionality that will allow me to use cross validiation to compare the lasso regression model with the other models.</p>

<p>If there is not, then my initial thought had been to go back to first principles and manually code K-fold cross validation for lasso regression.  However, I am now wondering if this is theoretically a good idea.  Each time I run a fold in my manual CV, I would run cv.glmnet on the training set.  Each training set would most likely result in a different lambda.min and lambda.1se.  </p>

<p>My question is:  is it technically proper CV to determine the overall CV error by averaging the error on each fold given that the lambda chosen for each fold will be producing a different lasso result?</p>

<p>Here is some sample code that I have used to create leave-one-out CV on the dataset to evaluate the lasso regression.  I have computed my cross validation error  on each fold using lambda.1se and labmda.min that arise for that fold.</p>

<pre><code>lassocv&lt;-function() {

len&lt;-length(drx$DR)

errmin&lt;-0
err1se&lt;-0
print(len)

for (i in 1:len) {
    gmer&lt;-data.matrix(drx[-i,])
    yxer&lt;-yx[-i]
    lfit&lt;-cv.glmnet(gmer, yxer)
    newr&lt;-data.matrix(drx[i,])
    pmin&lt;-predict(lfit,newx=newr,s=lfit$lambda.min)[[1]]
	    p1se&lt;-predict(lfit,newx=newr,s=lfit$lambda.1se)[[1]]
    errmin&lt;-errmin+abs(pmin-yx[i])
    err1se&lt;-err1se+abs(p1se-yx[i])
}
print(errmin/len)
print(err1se/len)

}
</code></pre>

<p>However, I get different CV results.  The two results that are returned for my dataset are 21.94867 and 23.74074.</p>
"
"0.235071195647559","0.246296091535945","174257","<p>I want to do a path analysis with lavaan but encounter a few problems and would appreciate any help.</p>

<p>The structural model looks like this:</p>

<p><a href=""http://i.stack.imgur.com/y8ZZh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y8ZZh.png"" alt=""structural model""></a></p>

<p>The relation between one observed independent (s) and one observed dependent variable (v) is mediated through a latent variable (m) that is defined by two observed indicator variables (x1, x2). This is basically a simplified version of the <a href=""http://lavaan.ugent.be/tutorial/sem.html"" rel=""nofollow"">SEM example</a> in the tutorial on the lavaan project website.</p>

<p>When I enter my code (given further below) into R, I encounter two problems:</p>

<p>(1) The results change when I change the order of the indicator variables.</p>

<p>This model:</p>

<pre><code># measurement model
    m =~ x1 + x2
</code></pre>

<p>returns a different result than this model:</p>

<pre><code># measurement model
    m =~ x2 + x1
</code></pre>

<p>How can that be? Isn't the order of the indicators arbitrary? And if not, how do I know which is the correct order, if my model does not presuppose a specific order?</p>

<p>(2) There are a few warnings that I don't understand: for the first model, no standard errors could be computed; and the second model did not ""converge"" (whatever that means). The warnings are given in context in the full code posted below.</p>

<p>What do I have to do to obtain reliable estimates?</p>

<hr>

<p>Here is the full R output to provide context to my questions.</p>

<pre><code># data

s &lt;- c(2, 5, 4, 4, 4, 8, 2, 9, 1, 1, 3, 3, 2, 3, 2, 5, 5, 7, 4, 7, 8, 4, 10, 10, 2, 4, 0, 2, 4, NA, 1, 5, 2, 6, 3, 5, 0, 5, 3, 6, 4, 9, 4, 9, 4, 5, 6, 1, 8, 0, 6, 9, 1, 5, 1, 6, 2, 5, 0, 5, 6, 2, 4, 10, 3, 4)
v &lt;- c(8, 10, 1, 4, 0, 2, 3, 2, 1, 1, 2, 5, 1, 5, 0, 5, 4, 5, 2, 10, 0, 6, 5, 5, 6, 1, 1, 0, 0, NA, 1, 0, 1, 8, 1, 3, 0, 5, 6, 3, 2, 10, 0, 5, 5, 10, 4, 1, 1, 0, 0, 0, 2, 10, 1, 8, 2, 3, 2, 2, 4, 4, 2, 5, 6, 2)
x1 &lt;- c(2.500000, 3.789474, 1.514563, 5.846868, 4.588235, 5.600000, 5.066667, 11.647059, 2.000000, NA, 4.461538, 18.000000, 1.058824, 9.217391, 27.840000, 15.375000, NA, 6.000000, 9.714286, 12.484848, 16.503497, 20.666667, 3.500000, 4.658824, 4.750000, 4.000000, 2.800000, 14.228571, 11.000000, NA, 2.666667, 3.764706, 4.705882, 13.272727, 2.000000, 18.444444, 17.555556, 14.222222, 2.000000, 4.000000, 8.461538, 19.200000, 13.902439, 13.000000, 3.000000, NA, 7.360000, 1.611374, 1.500000, 3.365854, 22.375000, 10.838710, 2.923077, 3.488372, 5.176471, 37.666667, 1.176471, 7.454545, 36.235294, 6.823529, 2.222222, 6.133333, 11.428571, 42.705882, 28.105263, 18.333333)
x2 &lt;- c(8.125000, 14.273684, 7.339806, 23.387471, 113.058824, 22.200000, 17.466667, 43.647059, 9.230769, NA, 13.538462, 83.555556, 5.058824, 37.391304, 100.000000, 59.250000, NA, 22.470588, 38.428571, 50.787879, 76.223776, 92.888889, 15.375000, 16.235294, 18.875000, 13.647059, 10.133333, 55.885714, 36.428571, NA, 6.933333, 13.294118, 14.117647, 81.818182, 6.117647, 67.777778, 76.333333, 51.888889, 6.428571, 14.200000, 34.000000, 59.680000, 68.634146, 40.500000, 12.250000, NA, 29.760000, 8.909953, 5.400000, NA, 71.125000, 39.741935, 9.846154, 13.116279, 18.823529, 204.000000, 4.588235, 49.090909, 188.470588, 19.647059, 10.222222, 22.933333, 38.285714, 140.235294, 137.526316, 79.000000)
dat &lt;- data.frame(cbind(s, v, x1, x2))

# first model

model &lt;- '
    # measurement model
        m =~ x2 + x1
    # regressions
        m ~ s
        v ~ s + m
    # residual correlations
        x1 ~~ x2
'
fit &lt;- sem(model, data = dat, missing = ""fiml"")

# Warning messages:
# 1: In lav_data_full(data = data, group = group, group.label = group.label,  :
#   lavaan WARNING: some cases are empty and will be removed:
#   30
# 2: In lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats,  :
#   lavaan WARNING: could not compute standard errors!
#   lavaan NOTE: this may be a symptom that the model is not identified.

summary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)

# lavaan (0.5-18) converged normally after 147 iterations
#
#                                                   Used       Total
#   Number of observations                            65          66
#
#   Number of missing patterns                         3
#
#   Estimator                                         ML
#   Minimum Function Test Statistic                0.565
#   Degrees of freedom                                 0
#   Minimum Function Value               0.0043451960201
#
# Model test baseline model:
#
#   Minimum Function Test Statistic              126.904
#   Degrees of freedom                                 6
#   P-value                                        0.000
#
# User model versus baseline model:
#
#   Comparative Fit Index (CFI)                    0.995
#   Tucker-Lewis Index (TLI)                       1.000
#
# Loglikelihood and Information Criteria:
#
#   Loglikelihood user model (H0)               -797.558
#   Loglikelihood unrestricted model (H1)       -797.275
#
#   Number of free parameters                         12
#   Akaike (AIC)                                1619.115
#   Bayesian (BIC)                              1645.208
#   Sample-size adjusted Bayesian (BIC)         1607.435
#
# Root Mean Square Error of Approximation:
#
#   RMSEA                                          0.000
#   90 Percent Confidence Interval          0.000  0.000
#   P-value RMSEA &lt;= 0.05                          1.000
#
# Standardized Root Mean Square Residual:
#
#   SRMR                                           0.027
#
# Parameter estimates:
#
#   Information                                 Observed
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(&gt;|z|)   Std.lv  Std.all
# Latent variables:
#   m =~
#     x2                1.000                              14.272    0.330
#     x1                0.384                               5.482    0.588
#
# Regressions:
#   m ~
#     s                 1.732                               0.121    0.323
#   v ~
#     s                 0.335                               0.335    0.306
#     m                 0.012                               0.171    0.059
#
# Covariances:
#   x2 ~~
#     x1              292.112                             292.112    0.951
#
# Intercepts:
#     x2               35.558                              35.558    0.823
#     x1                7.220                               7.220    0.775
#     v                 1.761                               1.761    0.604
#     m                 0.000                               0.000    0.000
#
# Variances:
#     x2             1663.119                            1663.119    0.891
#     x1               56.783                              56.783    0.654
#     v                 7.591                               7.591    0.892
#     m               182.367                               0.895    0.895
#
# R-Square:
#
#     x2                0.109
#     x1                0.346
#     v                 0.108
#     m                 0.105

model &lt;- '
    # measurement model
        m =~ x1 + x2
    # regressions
        m ~ s
        v ~ s + m
    # residual correlations
        x1 ~~ x2
'
fit &lt;- sem(model, data = dat, missing = ""fiml"")

# Warning messages:
# 1: In lav_data_full(data = data, group = group, group.label = group.label,  :
#   lavaan WARNING: some cases are empty and will be removed:
#   30
# 2: In lavaan::lavaan(model = model, data = dat, missing = ""fiml"", model.type = ""sem"",  :
#   lavaan WARNING: model has NOT converged!

summary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)

# ** WARNING ** lavaan (0.5-18) did NOT converge after 9438 iterations
# ** WARNING ** Estimates below are most likely unreliable
#
#                                                   Used       Total
#   Number of observations                            65          66
#
#   Number of missing patterns                         3
#
#   Estimator                                         ML
#   Minimum Function Test Statistic                   NA
#   Degrees of freedom                                NA
#   P-value                                           NA
#
# Parameter estimates:
#
#   Information                                 Observed
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(&gt;|z|)   Std.lv  Std.all
# Latent variables:
#   m =~
#     x1                1.000                               0.526    0.056
#     x2             1606.326                             845.326   19.343
#
# Regressions:
#   m ~
#     s                -0.001                              -0.001   -0.004
#   v ~
#     s                 0.355                               0.355    0.325
#     m                 0.004                               0.002    0.001
#
# Covariances:
#   x1 ~~
#     x2              -69.375                             -69.375   -0.009
#
# Intercepts:
#     x1               10.099                              10.099    1.083
#     x2               48.281                              48.281    1.105
#     v                 1.761                               1.761    0.604
#     m                 0.000                               0.000    0.000
#
# Variances:
#     x1               86.614                              86.614    0.997
#     x2            -712666.446                            -712666.446 -373.157
#     v                 7.617                               7.617    0.895
#     m                 0.277                               1.000    1.000
#
# R-Square:
#
#     x1                0.003
#     x2                   NA
#     v                 0.105
#     m                 0.000
# Warning message:
# In .local(object, ...) :
#   lavaan WARNING: fit measures not available if model did not converge
</code></pre>

<hr>

<p><em>Note.</em> I have posted the same question to the <a href=""https://groups.google.com/forum/#!forum/lavaan"" rel=""nofollow"">lavaan Google Group</a>, but this is part of my bachelor's thesis, which I have to turn in on Monday, so I'm a bit pressed for time and hope you forgive me for crossposting.</p>
"
"0.0435606841869032","0.0428746462856272","174989","<p>Please bear with me, I am very new to R.</p>

<p>My question is regarding the use of the <code>improveProb</code> function in the <code>Hmisc</code> package. I have two logistic models, the only difference being that the second model contains my novel marker of interest. I am trying to calculate NRI and IDI.</p>

<p>I have the PredRisks for both models - PredRisk1 and PredRisk2, and my outcome is disease 0/1. How do I define this in R in order to run</p>

<p><code>improveProb(x1, x2, y)</code>?</p>

<hr>

<p>The data are the same for both models. We are looking at ways to validate our findings. We have performed k-fold cross-validation (MSE=0.08) and bootstrapping with optimism (AUC original = 0.826 After correction =0.791) to check for overfitting. Is this appropriate? The LRT was significant for both logistic regression models, but I need to check this. Also, the AIC for model 2 is lower than model 1. Thanks again for your expert knowledge :)</p>
"
"0.0871213683738064","0.0857492925712544","175203","<p>I'm reading <a href=""https://onlinecourses.science.psu.edu/stat504/node/177"" rel=""nofollow"">this tutorial</a> to understand how to interpret the coefficients of an ordinal logistic regression which assumes the proportional odds. </p>

<p>They use a dataset about a cheese tasting experiment. Subjects were randomly assigned to taste one of four different cheeses (A,B,C,D). Response categories are 1 = strong dislike to 9 = excellent taste.</p>

<pre><code>m1=polr(response~cheese,weights=N,data=dati)
summary(m1)

Re-fitting to get Hessian

Call:
polr(formula = response ~ cheese, data = dati, weights = N)

Coefficients:
         Value Std. Error t value
cheeseB -3.352     0.4287  -7.819
cheeseC -1.710     0.3715  -4.603
cheeseD  1.613     0.3805   4.238

Intercepts:
    Value    Std. Error t value 
1|2  -5.4674   0.5236   -10.4413
2|3  -4.4122   0.4278   -10.3148
3|4  -3.3126   0.3700    -8.9522
4|5  -2.2440   0.3267    -6.8680
5|6  -0.9078   0.2833    -3.2037
6|7   0.0443   0.2646     0.1673
7|8   1.5459   0.3017     5.1244
8|9   3.1058   0.4057     7.6547

Residual Deviance: 711.3479 
AIC: 733.3479 
</code></pre>

<p>The tutorial's author writes:</p>

<blockquote>
  <p>we see that the implied ordering of cheeses in terms of quality is D >
  A > C > B. Furthermore, D is significantly better preferred than A,
  but A is not significantly better than C.</p>
</blockquote>

<p>Is this correct? 
I do agree that cheese B and C are significantly worse than A, and that D is significantly better than A, but I don't understand why cheese A should not be significantly better than C, as the author claims.</p>

<p>This are instead my conclusions:<br>
Since $\beta_B \neq 0$ and $\beta_B &lt; 0$, then $B&lt;A$.<br>
Since $\beta_C \neq 0$ and $\beta_C &lt; 0$, then $C&lt;A$.<br>
Since $\beta_D \neq 0$ and $\beta_D &gt; 0$, then $D&gt;A$.<br>
So, $D&gt;A&gt;B$ and $D&gt;A&gt;B$.
But since $\beta_B &lt; \beta_C$, then $D&gt;A&gt;B&gt;C$.<br>
So, I would say instead that cheese A is significantly better than C.</p>
"
"0.157060280430173","0.154586735600211","175767","<p>Logistic regression models the relationship between a set of independent variables and the probability that a case is a member of one of the categories of the dependent variable. If the probability is greater than 0.5, the case is classified in the modeled category.  If the probability is less than 0.50, the case is classified in the other category. The problem is that when I run the model with my dataset, the probabilities are far from 0.5, in fact it never gets to that value.</p>

<p>Here is part of My dataset:</p>

<pre><code>  sum_profit   direction   profit_cl1
   10           up          0.00
   0            Not_up     -0.03
  -5            Not_up      0.04
  -5            Not_up     -0.04
</code></pre>

<p>I want to find a relationship between the price of oil and the stock price of a Colombian oil company. So the variable 'sum_profit' is the sum of the change in the stock price in the next ten minutes. The variable 'profit_cl1' shows me the net change in the oil price in the last 10 minutes. </p>

<p>So what I want to know is that if the oil price changes in the last 10 minutes how would I expect the stock price direction to be in the following 10 minutes (Up or Down).</p>

<p>The problem is that my probabilities once I run the logistic regression are far from 0.5 even though the model is significant </p>

<pre><code>    glm.fit=glm(formula = direction ~ profit_cl1, family = binomial, data = datos)

    Deviance Residuals: 
      Min       1Q   Median       3Q      Max  
    -0.6786  -0.6786  -0.6131  -0.6131   1.8783  

    Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)     -1.57612    0.01618 -97.394   &lt;2e-16 ***
    profit_cl1       0.22485    0.02288   9.829   &lt;2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 48530  on 50309  degrees of freedom
    Residual deviance: 48434  on 50308  degrees of freedom
    AIC: 48438

    Number of Fisher Scoring iterations: 4 
</code></pre>

<p>The code to get the probabilities:</p>

<pre><code>   log.probs=predict(glm.fit, type=""response"")
   mean(log.probs)=0.1873 
</code></pre>

<p>the 0.1873 is very far from 0.5.</p>

<p>Sorry but I did not know where else to look for help! I appreciate any suggestion!</p>
"
"0.0889178742536973","0.105021006302101","176084","<p>I want to predict the impact of oil price over a Colombian oil company's stock price. I plan to use a multinomial regression for this with a categorical variable (Up, Down or Neutral given the direction of the stock price). Here is part of my dataset:</p>

<pre><code>Minute  ecopet  profit  sum_profit   direccion  cl1_chg   sum_cl1    direccion_cl1
571     2160     0       10           Up         -0.03     0.00      Down
572     2160     0        0           Neutral     0.07    -0.03      Down
573     2160     0       -5           Down       -0.08     0.04      Up
574     2160     0       -5           Down       -0.07    -0.04      Down
575     2160     5       -5           Down       -0.08    -0.11      Down
576     2165     0       -25          Down        0.00    -0.19      Down
577     2165     0       -25          Down       -0.05    -0.19      Down
578     2165     0       -15          Down       -0.17    -0.24      Down
579     2165     5       -15          Down       -0.06    -0.41      Down
580     2170     0       -20          Down        0.03    -0.47      Down
581     2170    -10       0           Neutral     0.04    -0.44      Down
</code></pre>

<p>My dependent variable is 'direccion'. But as you can see it has 3 response classes.the code I am using in R for the multinomial regression is:</p>

<pre><code>glm.fit=multinomial(direccion~direccion_cl1, data=datos)
</code></pre>

<p>I am working with intraday information and plan to predict what happens when the oil moves up/ down (in the previous 10 minutes) and how it impacts the stock price in the next 10 minutes.</p>

<p>The problem is that once I run the regression, what I get for glm.fit does not include the Coefficients for the level ""Down"". Would you know why is that? I get this:</p>

<pre><code> Call:
 multinom(formula = direccion ~ direccion_cl1, data = datos)

 Coefficients:
          (Intercept) direccion_cl1Up     
 Neutral   1.0505813       0.1955194 
 Up       -0.2513035       0.3936570 

 Residual Deviance: 90752.54 
 AIC: 90764.54 
</code></pre>

<p>Additional to this, when I use the function predict to see how well my model works I get this error message:</p>

<pre><code> log.probs=predict(glm.fit, ""probs"")
 Error in eval(expr, envir, enclos) : object 'direccion_cl1' not found
</code></pre>

<p>Thanks a lot!</p>
"
"0.0871213683738064","0.0857492925712544","177650","<p>I have a binary response variable and a categorical predictor variable. If I test for associations between the 2 variables using chi-square test , it turns out to be significant. However, if I do a logistic regression with the same set of variables, the predictor is not significant. Why does this happen?</p>

<pre><code>  table(Data1$pred,Data1$target)

                            0    1
  Level1                    1    0
  Level2                    4    0
  Level3                   98    1
  Level4                 2056   22
  Level5                    1    0
  Level6                    2    0
  Level7                  311    0
  Level8                    6    1
  Level9                  131    7
  Level10                  49    2

  chisq.test(table(Data1$pred,Data1$target))

  Pearson's Chi-squared test

  data:  tabletable(Data1$pred,Data1$target)
  X-squared = 34.2614, df = 9, p-value = 8.037e-05
</code></pre>

<p>Logistic Regression on the same</p>

<pre><code>  logit.glm &lt;- glm(as.factor(target) ~ pred,                  
               data=Data1, family=binomial(link=""logit"")
  summary(logit.glm)
  Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.5553  -0.1459  -0.1459  -0.1459   3.0315  

  Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)
  (Intercept)   -2.057e+01  1.773e+04  -0.001    0.999
  Data1Level2   -6.313e-06  1.982e+04   0.000    1.000
  Data1Level3    1.598e+01  1.773e+04   0.001    0.999
  Data1Level4    1.603e+01  1.773e+04   0.001    0.999
  Data1Level5   -6.312e-06  2.507e+04   0.000    1.000
  Data1Level6   -6.312e-06  2.172e+04   0.000    1.000
  Data1Level7   -6.312e-06  1.776e+04   0.000    1.000
  Data1Level8    1.877e+01  1.773e+04   0.001    0.999
  Data1Level9    1.764e+01  1.773e+04   0.001    0.999
  Data1Level10   1.737e+01  1.773e+04   0.001    0.999

  (Dispersion parameter for binomial family taken to be 1)

   Null deviance: 356.09  on 2691  degrees of freedom
   Residual deviance: 333.06  on 2682  degrees of freedom
   AIC: 353.06

   Number of Fisher Scoring iterations: 19
</code></pre>
"
"0.0616041103633697","0.0606339062590832","177903","<p>I fitted an ordinal logistic regression but I'm unable to interpret the coefficients. Can anyone assist in this regard? Here is the output generated: </p>

<pre><code>Call:
polr(formula = factor(grade) ~ factor(Month) + Day, data = myData, 
    Hess = TRUE)

Coefficients:
                  Value Std. Error t value
factor(Month)4 1.405114    0.51547  2.7259
Day            0.007672    0.01944  0.3947

Intercepts:
    Value   Std. Error t value
1|2 -0.6785  0.7019    -0.9667
2|3  1.6767  0.7162     2.3412

Residual Deviance: 333.602 
AIC: 341.602 
</code></pre>

<p>The grade is factored: </p>

<p>1 = good<br>
2 = very good<br>
3 = excellent</p>

<p>Month is factored:</p>

<p>3 = March<br>
4 = April</p>

<p>The grade is the response while month and day are my explanatory variables.</p>
"
"0.0974046509883157","0.0958706236059213","177921","<p>I have this data plotted as a scatter plot in Excel:  </p>

<p><a href=""http://i.stack.imgur.com/G45wZ.jpg""><img src=""http://i.stack.imgur.com/G45wZ.jpg"" alt=""enter image description here""></a></p>

<p>I had done a regression in Excel, and the p value was 2.14E-05 while the R- value was 0.32. I was told the R value was too low compared to the significance of the p value, and was told to control for the dispersion of the data by running it through R with GLM with quasipoisson error.</p>

<p>This gave me</p>

<pre><code>glm(formula = encno ~ temp, family = quasipoisson(link = log), 
    data = encnotemp)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-6.008  -2.431  -1.021   1.353   9.441  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 2.005807   0.174628  11.486  &lt; 2e-16 ***
temp        0.029065   0.006528   4.453 1.53e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 10.19898)

    Null deviance: 1807.4  on 171  degrees of freedom
Residual deviance: 1620.1  on 170  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 5
</code></pre>

<p>How do I analyse this output? </p>

<p>The problem is that the scatterplot data is too dispersed, and I would like to make a scatterplot from the quasipoisson GLM output that shows less dispersed (more fitted) data points. Will this be possible?</p>
"
"0.179883113951559","0.186886250399101","177960","<p>I'm trying to assess the effect of showing more impressions on a user. I want to study if users who saw more ads are more likely to make a purchase onsite. To do so I've created a multilevel model. I grouped users into 10 groups averaging their scores (we score users based on a number of factors). So basically I end up having 10 groups (from 0 to 9), where on group 9 I assume to have the best users, and on group 0 the worst.</p>

<pre><code>  picbucket mcuserid impressions mediacostcpm is_buyer gr.impressions gr.mediacostcpm
1         0 1           1        0.460        0       3.632794        2.767509
2         0 2           2        5.000        0       3.632794        2.767509
3         0 3           1        4.590        0       3.632794        2.767509
4         0 4           1        0.590        0       3.632794        2.767509
5         0 5           1        5.000        0       3.632794        2.767509
6         0 6           1        0.315        0       3.632794        2.767509
</code></pre>

<p>I think a multilevel model could be advantageous here because I'm expecting to see different effects on each group. On the best users I'm expecting an additional impression could have a higher impact, whereas on bad users and additional impression may be worthless. It could also be possible the opposite though. So that users which generally higher score will convert even without the need of serving them more impressions, whereas on mid groups additional impressions tend to change their behaviour. </p>

<p>A good model representation could be:</p>

<p>$y_{i} = \alpha_{j[i]} + X_{i}\beta + \epsilon_{i}$</p>

<p>The second level of the model will then be:</p>

<p>$\alpha_{j} = \mu_{\alpha} + \eta_{j}, \text{ with } \eta_{j} \sim N(0, \sigma_{\alpha}^{2})$</p>

<p>On the first level I want to include as a predictor how many impression a user saw. On the second level I want to include the average impressions a user saw within its group and the average media cost for the impressions we served on that user. I've used the package <code>lme4</code> in R to build my model.</p>

<pre><code>glmer(formula = is_buyer ~ impressions + mediacostcpm + (1 + 
    gr.impressions + gr.mediacostcpm | picbucket), data = new.df, 
    family = binomial())
             coef.est coef.se
(Intercept)  -7.42     0.33  
impressions   0.00     0.02  
mediacostcpm  0.03     0.01  

Error terms:
 Groups    Name            Std.Dev. Corr        
 picbucket (Intercept)     7.86                 
           gr.impressions  2.22     -0.99       
           gr.mediacostcpm 0.57     -0.68  0.60 
 Residual                  1.00                 
---
number of obs: 103146, groups: picbucket, 10
AIC = 2755.4, DIC = 2680.5
deviance = 2708.9 
</code></pre>

<p>This is my first experiment with multilevel modeling so I would like to make sure I don't misunderstand the results of my model. </p>

<p>From what I see here, the <code>impressions</code> predictor on the first level is useless. Its coefficient is zero and its standard deviation is very small. This could be due to the fact I'm including a group average on the second level for the impression count (<code>gr.impressions</code>). So, on any group (<code>picbucket</code>), serving more impressions than the average doesn't tell us much about the likelihood of a cookie to convert.</p>

<p>The average media cost on the first level is however an interesting one. Generally, within a group, if I spend a bit more for every impression I should increase the probability of generating conversions. This is probably due to inventory quality. Better inventory costs more, but also has better changes to be viewable inventory.</p>

<p>The coefficients on the group level instead tell you how much they contribute on explaining the group slope. So in this case the average number of impressions at the group level seems to explain quite a significant part of the group slope. </p>

<p>Interestingly, at the group level <code>gr.impressions</code> seem to be a very useful predictor, but at the within group level its usefulness is limited. The opposite applies to the <code>mediacostcpm</code>.</p>

<p>Am I interpreting these results correctly? How can I tell if the model has a good fit? Please note I've used a binomial regression because the dependent variable, <code>is_buyer</code> can take only 0 or 1 (one being the user made a purchase).</p>
"
"0.0974046509883157","0.0958706236059213","178102","<p>I have this data plotted as a scatter plot in Excel</p>

<p><img src=""http://i.stack.imgur.com/8xm8M.jpg"" alt=""enter image description here""></p>

<p>I had done a regression in Excel, and the p value was 2.14E-05 while the 
R- value was 0.32. I was told the R value was too low compared to the significance of the p value, and was told to control for the dispersion of the data by running it through R with GLM with quasipoisson error. </p>

<p>This gave me </p>

<pre><code>glm(formula = encno ~ temp, family = quasipoisson(link = log), 
    data = encnotemp)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-6.008  -2.431  -1.021   1.353   9.441  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 2.005807   0.174628  11.486  &lt; 2e-16 ***
temp        0.029065   0.006528   4.453 1.53e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 10.19898)

    Null deviance: 1807.4  on 171  degrees of freedom
Residual deviance: 1620.1  on 170  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 5
</code></pre>

<p>How do I analyse this output? And is it possible for me to make a scatter plot from this GLM output, that increases the fit of the data points and help sort out the outliers?</p>
"
"0.0871213683738064","0.0857492925712544","178492","<p>One can perform a logit regression in R using such code:</p>

<pre><code>&gt; library(MASS)
&gt; data(menarche)
&gt; glm.out = glm(cbind(Menarche, Total-Menarche) ~ Age,
+                                              family=binomial(logit), data=menarche)
&gt; coefficients(glm.out)
(Intercept)         Age 
 -21.226395    1.631968
</code></pre>

<p>It looks like the optimization algorithm has converged - there is information about steps number of the fisher scoring algorithm:</p>

<pre><code>Call:
glm(formula = cbind(Menarche, Total - Menarche) ~ Age, family = binomial(logit), 
    data = menarche)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0363  -0.9953  -0.4900   0.7780   1.3675  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -21.22639    0.77068  -27.54   &lt;2e-16 ***
Age           1.63197    0.05895   27.68   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 3693.884  on 24  degrees of freedom
Residual deviance:   26.703  on 23  degrees of freedom
AIC: 114.76

Number of Fisher Scoring iterations: 4
</code></pre>

<p>I am curious about what optim algorithm it is? Is it Newton-Raphson algorithm (second order gradient descent)? Can I set some parameters to use Cauchy algorithm (first order gradient descent)?</p>
"
"0.131340404599205","0.129271922498755","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"0.107807193135897","0.121267812518166","180285","<p>I'm pretty new to the concepts of stationarity/cointegration. I am using the ""urca"" package in ""Rstudio"" to run my tests.</p>

<p>I have been trying to run cointegration tests, but the frustrating thing is that I haven't been able to find two series that are non-stationary, even when I try using examples cited by cointegration tutorials. My $p$-value is always too big such that I have to reject the null straight away. However, if I look at the $t$-values and compare them to the critical values, they seem to suggest otherwise. </p>

<p>Should I then ignore the $p$-value in the ADF test?
Here are my test results. My two price series are <code>XLE US Equity</code> and <code>CO1 Comdty</code> (Brent 1st futures) from 01/01/2010 - today (5/11/2015).</p>

<p>Any help/elaboration will be very much appreciated, thank you!</p>

<pre><code>&gt; testXLE&lt;-ur.df(XLE,type=""drift"",selectlags=""AIC"")
&gt; summary(testXLE)

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression drift 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.3948  -2.5809   0.6846   2.7908  10.1940 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  6.58864    3.43524   1.918   0.0596 .
z.lag.1     -0.08584    0.04533  -1.894   0.0628 .
z.diff.lag   0.05529    0.12544   0.441   0.6609  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 4.162 on 64 degrees of freedom
Multiple R-squared:  0.05337,   Adjusted R-squared:  0.02379 
F-statistic: 1.804 on 2 and 64 DF,  p-value: 0.1729


Value of test-statistic is: -1.8936 1.8395 

Critical values for test statistics: 
      1pct  5pct 10pct
tau2 -3.51 -2.89 -2.58
phi1  6.70  4.71  3.86
</code></pre>

<p>My interpretation of the results:</p>

<blockquote>
  <ul>
  <li>according to p-value (0.1729>0.05) do not reject null; series is stationary   </li>
  <li>t-value = (-1.8936>-2.89) --> do not reject null hypothesis; series is not stationary  </li>
  <li>t-value = (1.8395&lt;4.71) --> do not reject a0=0 --> there is no drift</li>
  </ul>
</blockquote>

<p>Conclusion: The series is non-stationary: Random Walk with no drift.</p>
"
"0.157462484111592","0.166052791038768","180447","<p>I have some data on patients presenting to emergency departments after sustaining self-inflicted gunshot injuries, stored in a data frame (""SIGSW,"" which is ~16,000 observations of 47 variables) in R. I want to create a model that helps a physician predict, using several objective covariates, the ""pretest probability"" of the self-shooting being a suicide attempt, or a negligent discharge. The covariates are largely categorical variables, but a few are continuous or binary. My outcome, suicide attempt or not, is coded as a binary/indicator variable, ""SI,"" so I believe a binary logistic regression to be the appropriate tool.  </p>

<p>In order to construct my model, I intended to individually regress SI on each covariate, and use the p-value from the likelihood ratio test for each model to inform which covariates should be considered for the backward model selection. </p>

<p>For each model, SI~SEX, SI~AGE, etc, I receive the following error:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: algorithm did not converge
</code></pre>

<p>A little Googling revealed that I perhaps need to increase the number of iterations to allow convergence. I did this with the following:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW, control = list(maxit = 50))

Call:  glm(formula = SI ~ SEX, family = binomial, data = SIGSW, control = list(maxit = 50))

Coefficients:
(Intercept)          SEX  
 -3.157e+01   -2.249e-13  

Degrees of Freedom: 15986 Total (i.e. Null);  15985 Residual
Null Deviance:      0 
Residual Deviance: 7.1e-12  AIC: 4
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>This warning message, after a little Googling, suggests a ""perfect separation,"" which, as I understand it, means that my predictor is ""too good."" Seeing as how this happens with all of the predictors, I'm somewhat skeptical that they're all ""too good."" Am I doing something wrong? </p>

<p>Edit: In light of the answers, here is a sample of the data (I only selected a few of the variables for space concerns):</p>

<pre><code>   SIGSW.AGENYR_C SIGSW.SEX SIGSW.RACE_C SIGSW.SI
1              19      Male        White        0
2              13      Male        Other        0
3              18      Male   Not Stated        0
4              15      Male        White        0
5              23      Male        White        0
6              11      Male        Black        0
7              16      Male   Not Stated        1
8              21      Male   Not Stated        0
9              14      Male        White        0
10             41      Male        White        0
</code></pre>

<p>And here is the crosstabulation of SEX and SI, showing that SI is coded as an indicator variable, and that there are both men and women with SI, so sex is not a perfect predictor. </p>

<pre><code>  &gt;table(SIGSW$SEX, SIGSW$SI)        
              0     1
  Unknown     1     3
  Male    11729  2121
  Female   1676   457
</code></pre>

<p>Does the small cell size represent a problem?</p>
"
"0.137750978465894","0.122023382522994","180521","<p>I have a time series that includes some rare extreme values. We are talking about daily data, in total 1461 observations and 11 extreme values. I adjusted those 11 values with a multiple regression. Now I am using the <code>tbats()</code> on the original time series and the adjusted one. </p>

<pre><code>accuracy(original)
&gt;                   ME    RMSE      MAE MPE MAPE      MASE          ACF1
&gt;Training set 10.23539 4202.19 2921.593 NaN  Inf 0.6777689 -0.0003493096
accuracy(adjusted)
&gt;                   ME    RMSE      MAE MPE MAPE      MASE          ACF1
&gt;Training set 43.35625 3803.618 2787.39 NaN  Inf 0.6827622 -0.004749092

#original AIC
&gt;35101.43
#adjusted AIC
&gt;34798.24
</code></pre>

<p>How can I see if the model improves due to the adjustment or not? Since I reduced those 11 extreme values, I can't just compare MAE, RMSE or AIC. MASE is the only measure that should work?</p>

<p>I could divide MAE, RMSE and AIC by the mean of the respective time series.</p>

<pre><code># original
0.4962245 # MAE/mean(original)
0.7137304 # RMSE/mean(original)
5.96188 # AIC/mean(original)

# adjusted
0.4862567 # MAE/mean(adjusted)
0.6635364 # RMSE/mean(adjusted)
6.07051 # AIC/mean(adjusted)
</code></pre>

<p>Is that a legitimate way to compare the results?</p>

<p>Here are the <code>pacf</code>-diagrams of both models:</p>

<p><strong>original</strong>:</p>

<p><a href=""http://i.stack.imgur.com/nFARp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nFARp.png"" alt=""original""></a></p>

<p><strong>adjusted</strong>:</p>

<p><a href=""http://i.stack.imgur.com/YXIGF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YXIGF.png"" alt=""adjusted""></a></p>

<p><strong>Update:</strong></p>

<p>I just realized that when i use the <code>accuracy()</code> function of the <code>forecast</code> package with a <code>tbats()</code> based on a <code>msts()</code> object the resulting MASE is using an in-sample naive forecast for scaling. I guess that is not optimal? It should be better to use an in-sample naive seasonal forecast with the longest season of the <code>msts()</code> object.</p>

<pre><code>MASE(original) # scaled with a in-sample naive seasonal forecast (365)
&gt; 0.6339

MASE(adjusted) # scaled with a in-sample naive seasonal forecast (365)
&gt; 0.6287
</code></pre>
"
"0.151347073206507","0.160422236979937","180854","<p>I am trying to test for cointegration between two series that based on qualitative reasoning, should be cointegrated. They are the prices of XLE ETF (<code>XLE US equity</code>) and 1st futures of Brent (<code>CO1 Comdty</code>). However, the results that I arrive at using two different methods both show that there exists no cointegration between the two series - not sure if my execution or the interpretation of the data is wrong? </p>

<p>(Both XLE and Brent 1st Futures have been tested for non-stationarity using ADF test from ""urca"" package)</p>

<p><strong>1st test - Engle Granger 2-step test:</strong><br>
In doing this, I referenced <em>Using R to Test Pairs of Securities for Cointegration</em> by Paul Teetor</p>

<p><strong>(1)</strong> Conducting Spread</p>

<pre><code>&gt; M&lt;-lm(XLE~Brent+0,data=XLE.Brent)
&gt; beta&lt;-coef(M)[1]
&gt; spread&lt;-XLE.Brent$XLE-beta*XLE.Brent$Brent
&gt; 
&gt; summary(M)

Call:
lm(formula = XLE ~ Brent + 0, data = XLE.Brent)

Residuals:
   Min      1Q  Median      3Q     Max 
-20.363  -9.543  -2.909  13.294  36.269 

Coefficients:
     Estimate Std. Error t value Pr(&gt;|t|)    
&gt;Brent  0.74962    0.02004    37.4   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 16.37 on 68 degrees of freedom
Multiple R-squared:  0.9536,    Adjusted R-squared:  0.953 
F-statistic:  1399 on 1 and 68 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>(2)</strong> Testing the stationarity of the spread using ADF test (from package ""urca""):</p>

<pre><code>&gt; spread.ADF&lt;-ur.df(spread,type=""none"",selectlags=""AIC"")
&gt; summary(spread.ADF)

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test #  
</code></pre>

#########################################

<pre><code>Test regression none 


Call:
lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)

Residuals:
   Min      1Q  Median      3Q     Max 
 -6.1449 -2.2523  0.5559  2.9194  8.4567 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
z.lag.1    -0.0003928  0.0266919  -0.015    0.988
z.diff.lag  0.1207084  0.1278700   0.944    0.349

Residual standard error: 3.443 on 65 degrees of freedom
Multiple R-squared:  0.01395,   Adjusted R-squared:  -0.01639 
F-statistic: 0.4596 on 2 and 65 DF,  p-value: 0.6335


Value of test-statistic is: -0.0147 

Critical values for test statistics: 
    1pct  5pct 10pct
tau1 -2.6 -1.95 -1.61
</code></pre>

<p>My interpretation: since $t$-value = <code>-0.0147</code> is bigger than <code>-1.61</code>, do not reject null. Spread is not stationary. Hence no cointegration between XLE and Brent.</p>

<p><strong>Second Test: Johansen Test</strong></p>

<pre><code>&gt; XLE.brent.coint&lt;-ca.jo(data.frame(XLE,Brent),type=""trace"",ecdet=""trend"",K=2,spec=""longrun"")
&gt; summary(XLE.brent.coint)
&gt;
&gt;###################### 
&gt;# Johansen-Procedure # 
&gt;###################### 
&gt;
&gt;Test type: trace statistic , with linear trend in cointegration 
&gt;
&gt;Eigenvalues (lambda):
&gt;[1] 8.179514e-02 6.025284e-02 2.775558e-17
&gt;
&gt;Values of teststatistic and critical values of test:
&gt;
&gt;        test 10pct  5pct  1pct
&gt;r &lt;= 1 | 4.16 10.49 12.25 16.26
&gt;r = 0  | 9.88 22.76 25.32 30.45
&gt;
&gt;Eigenvectors, normalised to first column:
&gt;(These are the cointegration relations)
&gt;
&gt;          XLE.l2   Brent.l2   trend.l2
&gt;XLE.l2   1.000000  1.0000000  1.0000000
&gt;Brent.l2 1.467806 -0.4346323  0.1610563
&gt;trend.l2 1.896366 -0.4903454 -0.8891875
&gt;
&gt;Weights W:
&gt;(This is the loading matrix)
&gt;
&gt;            XLE.l2    Brent.l2      trend.l2
&gt;XLE.d   -0.01629102 -0.13534537 -4.695795e-17
&gt;Brent.d -0.03819241 -0.03886418  5.127543e-17
</code></pre>

<p>My interpretation: Since $t$-value for <code>r=0: 9.88&lt;22.76</code>, do not reject null. Hence <code>r=0</code>, there exists no cointegration between <strong>XLE and Brent</strong>.</p>

<p>Additionally, I have carried out cointegration tests (both methods) on <strong>US 10 year and 2 year yields</strong>, and the results on both tell me that the series are not co-integrated, which does not make sense intuitively. Something must be wrong with the way I'm doing the tests!</p>

<p><a href=""http://i.stack.imgur.com/eApO4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eApO4.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/sf2HV.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sf2HV.jpg"" alt=""enter image description here""></a></p>
"
"0.107807193135897","0.106109335953396","182286","<p>I am doing a regression analysis for an ordinal response variable with 5 explanatory variables. I will be using the <code>polr()</code> or <code>lrm()</code> functions to do the ordinal logistic regression. For my non-ordinal response variables (e.g., count and binary data), I have been using glmulti for model selection, but this doesn't seem to be compatible with the <code>polr()</code> and <code>lrm()</code> R functions. I've also tried <code>stepAIC()</code>, <code>step()</code> and <code>leap()</code> functions without any luck. The summary of the <code>polr()</code> regression shows an AIC score.</p>

<pre><code>&gt; model1 &lt;- polr(x ~ Age + Gender + StudentType + StudentYear + RacialGroup,
+ data = question8a, Hess =TRUE)
&gt; summary(model1)
Call:
polr(formula = x ~ Age + Gender + StudentType + 
    StudentYear + RacialGroup, data = question8a, Hess = TRUE)

Coefficients:
                                   Value Std. Error  t value
Age                             -0.16691    0.04925 -3.38872
GenderWoman                      0.05514    0.24655  0.22366
StudentTypeUndergraduatestudent -1.36414    0.50748 -2.68807
StudentYear2ndyear              -0.02042    0.29600 -0.06899
StudentYear3rdyear              -0.05997    0.38253 -0.15676
StudentYear4+years               0.89921    0.66430  1.35363
StudentYear4thyear               0.25324    0.42433  0.59680
RacialGroupNon-Indigenous       -2.13460    0.42163 -5.06268

Intercepts:
    Value   Std. Error t value
1|2 -9.9335  1.5283    -6.4999
2|3 -8.3051  1.4752    -5.6298
3|4 -7.2498  1.4567    -4.9770
4|5 -4.8720  1.4240    -3.4214

Residual Deviance: 657.086 
AIC: 681.086 
</code></pre>

<p>I tried to follow this suggestion: <a href=""http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R"" rel=""nofollow"">http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R</a>, but wasn't able to get it to work. </p>

<p>Has anyone been able to get this to work? Or do I need to compare the 2^5 = 32 model AIC scores by hand? </p>
"
"0.174544979362881","0.18190171877725","183320","<p>I have the following dataframe on which I did logistic regression with response as outcome. There are some good predictors in these variables so I expected significant variables.</p>

<pre><code>structure(list(response = c(0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 
    0L, 0L, 1L, 0L, 1L, 0L), HIST1H3F_rna = c(1.09861228866811, 0.693147180559945, 
    2.07944154167984, 1.09861228866811, 1.79175946922805, 0, 0, 0, 
    2.39789527279837, 1.38629436111989, 1.6094379124341, 1.6094379124341, 
    0.693147180559945, 1.79175946922805, 0), NCF1_rna = c(2.77258872223978, 
    3.09104245335832, 2.63905732961526, 2.19722457733622, 2.30258509299405, 
    2.56494935746154, 3.09104245335832, 3.98898404656427, 2.56494935746154, 
    4.06044301054642, 3.87120101090789, 2.07944154167984, 3.49650756146648, 
    3.17805383034795, 3.95124371858143), WDR66_rna = c(5.06890420222023, 
    4.49980967033027, 5.11799381241676, 3.40119738166216, 3.25809653802148, 
    4.02535169073515, 5.8348107370626, 5.89440283426485, 3.87120101090789, 
    5.67675380226828, 5.35185813347607, 4.15888308335967, 6.23441072571837, 
    5.91889385427315, 3.68887945411394), PTH2R_rna = c(0.693147180559945, 
    5.08759633523238, 0.693147180559945, 1.09861228866811, 0, 6.01126717440416, 
    6.56526497003536, 5.18178355029209, 0, 4.36944785246702, 2.19722457733622, 
    1.09861228866811, 3.49650756146648, 1.38629436111989, 5.93753620508243
    ), HAVCR2_rna = c(4.48863636973214, 3.40119738166216, 3.09104245335832, 
    2.94443897916644, 3.2188758248682, 3.76120011569356, 3.95124371858143, 
    2.83321334405622, 2.07944154167984, 4.36944785246702, 3.58351893845611, 
    1.94591014905531, 4.23410650459726, 3.43398720448515, 2.56494935746154
    ), CD200R1_rna = c(2.484906649788, 2.94443897916644, 0.693147180559945, 
    1.94591014905531, 0.693147180559945, 2.89037175789616, 2.56494935746154, 
    1.6094379124341, 1.6094379124341, 1.94591014905531, 2.19722457733622, 
    0.693147180559945, 4.26267987704132, 1.6094379124341, 0.693147180559945
    )), .Names = c(""response"", ""HIST1H3F_rna"", ""NCF1_rna"", ""WDR66_rna"", 
    ""PTH2R_rna"", ""HAVCR2_rna"", ""CD200R1_rna""), row.names = c(NA, 
    -15L), class = ""data.frame"")
</code></pre>

<p>However, running the following lines and getting a summary of the model I find that all variables have a p-value of 1 and the standard errors seem so high. What's going on here?</p>

<pre><code>fullmod &lt;- glm(response ~ ., data=final_model,family='binomial')
summary(fullmod)
Call:
glm(formula = response ~ ., family = ""binomial"", data = final_model)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-6.515e-06  -2.404e-06  -2.110e-08   2.110e-08   7.470e-06  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   1.460e+02  5.598e+05       0        1
HIST1H3F_rna  2.135e+01  5.145e+05       0        1
NCF1_rna     -4.133e+01  3.388e+05       0        1
WDR66_rna     1.296e+01  6.739e+05       0        1
PTH2R_rna     1.975e+00  3.775e+05       0        1
HAVCR2_rna   -2.477e+01  1.191e+06       0        1
CD200R1_rna  -1.420e+01  1.315e+06       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.0190e+01  on 14  degrees of freedom
Residual deviance: 2.2042e-10  on  8  degrees of freedom
AIC: 14

Number of Fisher Scoring iterations: 25
</code></pre>

<hr>

<p>In response to your comments I'll show the feature selection step (and the complete dataframe I'm working with below that).  </p>

<pre><code># forward  feature selection 
library('boot')
z = c()
nullmod &lt;- glm(response ~ 1, data=final_model, family='binomial') ## â€˜emptyâ€™ 
fullmod &lt;- glm(response ~ ., data=final_model, family='binomial') ## Full model
first = T
for(x in 1:ncol(final_model)){
  stepmod &lt;- step(nullmod, scope=list(lower=formula(nullmod), upper=formula(fullmod)),
                  direction=""forward"", data=final_model, steps=x, trace=F)
  cv.err  &lt;- cv.glm(data=final_model, glmfit=stepmod, K=nrow(final_model))$delta[1]
  if (first == T){
    first=F
    final_features &lt;- stepmod
  }else{
    if (cv.err &lt; min(z)){ final_features &lt;- stepmod }
  }
  z[x] &lt;- cv.err
  print(paste(x,cv.err))
  print(colnames(final_features$model))
}

plot(z, main='Forward Feature Selection GLM Final Model', 
     xlab='Number of Steps', ylab='LOOCV-error', col='red', type='l')
points(z)
colnames(final_features$model)
summary(final_features)

structure(list(response = c(0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 
0L, 1L, 0L, 1L, 1L, 1L), HIST1H3F_rna = c(1.09861228866811, 2.07944154167984, 
1.09861228866811, 1.79175946922805, 0, 0, 0, 2.39789527279837, 
1.38629436111989, 1.6094379124341, 1.6094379124341, 0.693147180559945, 
2.19722457733622, 2.39789527279837, 2.89037175789616), NCF1_rna = c(2.77258872223978, 
2.63905732961526, 2.19722457733622, 2.30258509299405, 2.56494935746154, 
3.09104245335832, 3.98898404656427, 2.56494935746154, 4.06044301054642, 
3.87120101090789, 2.07944154167984, 3.49650756146648, 2.07944154167984, 
2.07944154167984, 1.09861228866811), WDR66_rna = c(5.06890420222023, 
5.11799381241676, 3.40119738166216, 3.25809653802148, 4.02535169073515, 
5.8348107370626, 5.89440283426485, 3.87120101090789, 5.67675380226828, 
5.35185813347607, 4.15888308335967, 6.23441072571837, 4.0943445622221, 
4.21950770517611, 3.95124371858143), PTH2R_rna = c(0.693147180559945, 
0.693147180559945, 1.09861228866811, 0, 6.01126717440416, 6.56526497003536, 
5.18178355029209, 0, 4.36944785246702, 2.19722457733622, 1.09861228866811, 
3.49650756146648, 0, 0.693147180559945, 1.38629436111989), 
HAVCR2_rna = c(4.48863636973214, 
3.09104245335832, 2.94443897916644, 3.2188758248682, 3.76120011569356, 
3.95124371858143, 2.83321334405622, 2.07944154167984, 4.36944785246702, 
3.58351893845611, 1.94591014905531, 4.23410650459726, 1.38629436111989, 
1.09861228866811, 1.38629436111989), CD200R1_rna = c(2.484906649788, 
0.693147180559945, 1.94591014905531, 0.693147180559945, 2.89037175789616, 
2.56494935746154, 1.6094379124341, 1.6094379124341, 1.94591014905531, 
2.19722457733622, 0.693147180559945, 4.26267987704132, 1.94591014905531, 
0, 0.693147180559945), GDF7 = c(0.2232, -0.7281, 0.0655, -0.7919, 
0.175, 0.0891, 0.4396, -0.2774, -0.4079, 0.4069, 0.3057, 0.7371, 
-0.4978, -0.5096, -0.0827), HS1BP3 = c(0.2232, -0.7281, 0.0655, 
-0.7919, 0.175, 0.0891, 0.4396, -0.2774, -0.4079, 0.4069, 0.3057, 
0.7371, -0.4978, -0.5096, -0.0827), NKAIN3 = c(0.4072, 0.3216, 
-0.5466, -0.1588, 0.4515, 0.2849, 0.1675, 0.0847, 0.6601, 0.6331, 
-0.135, 1.3532, -0.503, -0.1241, 0.2061), UG0898H09 = c(0.4072, 
0.3216, -0.5466, -0.1588, 0.4515, 0.2849, 0.1675, 0.0847, 0.6601, 
0.6331, -0.135, 1.3532, -0.503, -0.1241, 0.2061), C15orf41 = c(0.122, 
-0.7519, -1.1267, -0.7882, -0.1117, -0.5105, -0.3905, -0.6834, 
-0.5944, 0.0714, -0.8134, -0.0115, -1.1112, -1.1488, -0.4878), 
    FAM98B = c(-0.1871, -0.7519, -1.1267, -0.7882, -0.1117, -0.5105, 
    -0.3905, -0.6834, -0.5944, 0.0714, -0.8134, -0.0115, -1.1112, 
    -1.1488, -0.4878), SPRED1 = c(-0.1871, -0.7519, -1.1267, 
    -0.7882, -0.1117, -0.5105, -0.3905, -0.6834, -0.5944, 0.0714, 
    -0.8134, -0.0115, -1.1112, -1.1488, -0.4878), MPDZ_ex = c(1, 
    0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0), TPR_ex = c(0, 
    0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), BUB1B_ex = c(0, 
    0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), APC_ex = c(0, 
    0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), ATM_ex = c(0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0), DYNC1LI1_ex = c(0, 
    0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0), TTK_ex = c(0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), PSMG2_ex = c(1, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), NegRegMitosis = c(1, 
    0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0), brca1ness = c(0.037719, 
    0.900878, 0.013261, 0.900878, 0.659963, 0.005629, 9.8e-05, 
    0.996336, 0.910072, 0.850776, 0.000613, 0.104428, 0.978114, 
    0.938767, 0.041696), Methylation = c(0L, 0L, 0L, 1L, 1L, 
    1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L), LinoleicAcid_Metab = structure(c(2L, 
    2L, 2L, 2L, 1L, 3L, 2L, 2L, 1L, 5L, 2L, 5L, 1L, 2L, 2L), .Label = c(""CYP2E1_high"", 
    ""CYP2E1_med"", ""high"", ""low"", ""PLA2G2A_high""), class = ""factor""), 
    Neuro_lr = structure(c(2L, 2L, 1L, 1L, 3L, 3L, 3L, 1L, 3L, 
    1L, 1L, 3L, 3L, 1L, 1L), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor""), 
    NOX_signalling = structure(c(2L, 2L, 2L, 2L, 1L, 2L, 1L, 
    2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L), .Label = c(""high"", ""low""
    ), class = ""factor"")), .Names = c(""response"", ""HIST1H3F_rna"", 
""NCF1_rna"", ""WDR66_rna"", ""PTH2R_rna"", ""HAVCR2_rna"", ""CD200R1_rna"", 
""GDF7"", ""HS1BP3"", ""NKAIN3"", ""UG0898H09"", ""C15orf41"", ""FAM98B"", 
""SPRED1"", ""MPDZ_ex"", ""TPR_ex"", ""BUB1B_ex"", ""APC_ex"", ""ATM_ex"", 
""DYNC1LI1_ex"", ""TTK_ex"", ""PSMG2_ex"", ""NegRegMitosis"", ""brca1ness"", 
""Methylation"", ""LinoleicAcid_Metab"", ""Neuro_lr"", ""NOX_signalling""
), row.names = c(NA, -15L), class = ""data.frame"")
</code></pre>

<p>Summary now gives the following:</p>

<pre><code>Call:
glm(formula = response ~ NegRegMitosis, family = ""binomial"", 
    data = final_model)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-3.971e-06  -3.971e-06   3.971e-06   3.971e-06   3.971e-06  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)       25.57   76367.61       0        1
NegRegMitosis    -51.13  111790.71       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.0728e+01  on 14  degrees of freedom
Residual deviance: 2.3655e-10  on 13  degrees of freedom
AIC: 4

Number of Fisher Scoring iterations: 24
</code></pre>

<p>Again even in a single predictor model, my p-value is 1. The predictor in this case is equal to the response, so it should predict perfectly. Then why is my pvalue 1?</p>
"
"0.162989155760853","0.160422236979937","183699","<p>I encountered a strange phenomenon when calculating pseudo R2 for logistic models when using aggregated files: the results are simply too good to be true. An example (but as far as I can see, every aggregated file offers similar problems):</p>

<pre><code> library(pscl)
 cuse &lt;- read.table(""http://data.princeton.edu/wws509/datasets/cuse.dat"",
               header=TRUE)

 head(cuse)
 cuse.fit &lt;- glm( cbind(using, notUsing) ~ age + education + wantsMore, 
             family = binomial, data=cuse)

 summary(cuse.fit)
 pR2(cuse.fit)     
</code></pre>

<p>The results are:</p>

<pre><code>&gt; summary(cuse.fit)

Call:
glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, 
family = binomial, data = cuse)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.5148  -0.9376   0.2408   0.9822   1.7333  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.8082     0.1590  -5.083 3.71e-07 ***
age25-29       0.3894     0.1759   2.214  0.02681 *  
age30-39       0.9086     0.1646   5.519 3.40e-08 ***
age40-49       1.1892     0.2144   5.546 2.92e-08 ***
educationlow  -0.3250     0.1240  -2.620  0.00879 ** 
wantsMoreyes  -0.8330     0.1175  -7.091 1.33e-12 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 165.772  on 15  degrees of freedom
Residual deviance:  29.917  on 10  degrees of freedom
AIC: 113.43

Number of Fisher Scoring iterations: 4

&gt; pR2(cuse.fit)
         llh      llhNull           G2     McFadden         r2ML 
 -50.7125647 -118.6401419  135.8551544    0.5725514    0.9997947 
       r2CU 
  0.9997950 
</code></pre>

<p>The last three outcomes from pscl function pR2 present McFadden's pseudo r-squared,  Maximum likelihood pseudo r-squared (Cox &amp; Snell) and Cragg and Uhler's or Nagelkerke's pseudo r-squared. The calculation seems to be flawless, but the outcomes close to 1 seem to good to be true.</p>

<p>Using weight instead of cbind:</p>

<pre><code>cuse2 = rbind(cuse,cuse)
cuse2$using.contraceptive=1
    cuse2$using.contraceptive[1:nrow(cuse)]=0
cuse2$freq = cuse2$notUsing
cuse2$freq[1:nrow(cuse)] = cuse2$using[1:nrow(cuse)]
cuse.fit2 = glm(using.contraceptive ~ age + education + wantsMore,
            weight=freq, family = binomial, data = cuse2)
summary(cuse.fit2)
round(pR2(cuse.fit2),5)
</code></pre>

<p>produces different logistic regression coefficients, and slightly different pseudo R2's for r2ml and r2CU and a large difference for McFadden R2:</p>

<pre><code>&gt; round(pR2(cuse.fit2),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.98567 
        r2CU 
     0.98567 
</code></pre>

<p>Full expansion results in very different estimates from pR2:</p>

<pre><code> cuse3 = rbind(cuse[rep(1:nrow(cuse), cuse[[""notUsing""]]), ],
          cuse[rep(1:nrow(cuse), cuse[[""using""]]), ])
 cuse3$using.contraceptive=1
     cuse3$using.contraceptive[1:sum(cuse$notUsing)]=0
 summary(cuse3)
 cuse.fit3 = glm(using.contraceptive ~ age + education + wantsMore,
            family = binomial, data = cuse3)
 summary(cuse.fit3)
 round(pR2(cuse.fit3),5)

 &gt; round(pR2(cuse.fit3),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.08106 
        r2CU 
     0.11376 
</code></pre>

<p>This indicates a logistic model which explains very little, which is a little bit more believable than the near perfect results from the aggregated files. Is there a more correct, and preferably more consistent, way to calculate the pseudo R2's? </p>
"
"0.0779237207906526","0.0958706236059213","185800","<p>I try to find a model using logistic regression. More precisely, what I did so far, is using stepwise regression and subset selection (although I know, it is often a bad idea) to find the ""best"" model. Clearly, depending on the information criteria I used, I got different results. </p>

<p>Now, I found an interesting example on page 250 in the book <a href=""http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"" rel=""nofollow"">""An Introduction to Statistical Learning""</a>. They chose among the models of different sizes using cross-validation, that is they make predictions for each model and compute the test errors. Eventually, the compute the cross validation error and choose the model corresponding to the minimal average cross-validation error. </p>

<p>However, the function <code>regsubsets</code> of the R package ""leaps"" is only working for linear models. How can I implement this for logistic regression or glm models in general? </p>

<p>My idea was, to just estimate the models within a cross-validation using the <code>step</code> function of the ""stats"" package and then kind of take the average number of features (which is determined by minimum AIC, for example). Is this a legitimate approach?</p>
"
"0.157060280430173","0.154586735600211","186728","<p>I am using the great <code>{caret}</code> package to run a lot of models, however I would like to analyse the model as one usually does having run that model in its own right, i.e. not within caret.</p>

<p>I am using the mboost package, starting with the <code>glmboost</code> function. If you run this model there are then functions within the mboost package that can be applied directly to the output of that function. however, these same functions do not work on the output of <code>train</code> from caret.
<code>train</code> is essentially the wrapper function which allows you to optimise the parameters for the chosen model, glmboost in my case.</p>

<p>Here is some dummy code if anybody wants to play with it. Its a boosted tree regression model, first using the <code>glmboost</code> function directly from the mboost package, then the same thing through the caret package (with some extra parameters to optimise over):</p>

<pre><code>## ============================================================== ##
##  Create a simple model using glmboost that runs through caret  ##
## ============================================================== ##

## install as necessary!
library(mboost)
library(caret)
## Use multicore if you can!
library(doMC)
registerDoMC(4)

## ============= ##
##  Create data  ##
## ============= ##

## Let's say we are predicting a numeric value, based on the predictors
## 70 observations of 10 variables, assuming they are chronologically order (a time-series)

set.seed(666)                                                # the devil's seed
myData &lt;- as.data.frame(matrix(rnorm(70*15, 2, .4), 70, 10)) #10 columns of random numbers
names(myData) &lt;- c(""to.predict"", paste0(""var_"", seq(1, 9)))
# Have a ganders
str(myData)                             

## Create model output using the mboost package directly
glm_mboost &lt;- glmboost(to.predict ~ .,  # predict against all variables
                       myData,          # supply our data
                       control = boost_control(mstop = 200)
                       )

## This is what I'd like to do with the output from the caret package!
plot(glm_mboost)
cvr &lt;- cvrisk(glm_mboost)
plot(cvr)

## ========================================== ##
##  Set parameters for train() - using caret  ##
## ========================================== ##

## glmboost takes 'mstop' and 'prune' as inputs
myGrid &lt;- expand.grid(mstop = seq(20, 250, 50),
                      prune = ""AIC""    #this isn't actually required by the mboost package!
                      )
myControl &lt;- trainControl(method = ""timeslice"", # take consequetive portions of the time-series
                          fixedWindow = TRUE, # If this is TRUE, we get the error
                          horizon = 1,
                          initialWindow = 20) # ~1 months of trading days
## fixedWindow = TRUE  --&gt; 

## =============== ##
##  Run the model  ##
## =============== ##

glm_caret &lt;- train(to.predict ~ ., data = myData,
                method = ""glmboost"",
                #metric = ""MyGauss"",
                trControl = myControl,
                tuneGrid = myGrid
                ##verbose = FALSE)
                )

## Maybe this will give you some idea about how to extract it
str(glm_caret)

## This is the best I can do, but the first plot doesn't come out right
x &lt;- glm_caret$finalModel
plot(x)
cvr1 &lt;- cvrisk(x)
plot(cvr1)
</code></pre>

<p>An idea I have is to simply use the optimal output given by caret to run the <code>glmboost</code> function once, with the provided parameters, but as I am going through many models, I'd rather save the computing time!</p>
"
"0.150898636448357","0.148522131446501","186845","<p>I created some data using the following code:</p>

<pre><code>set.seed(1221)
x &lt;- runif(500)
y &lt;- runif(500,0,2)
z &lt;- rep(0,500)
z[-0.8*x + y - 0.75 &gt; 0] &lt;- 1
plot(x,y,col=as.factor(z))
</code></pre>

<p>This produces the following plot</p>

<p><a href=""http://i.stack.imgur.com/ycWdr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ycWdr.png"" alt=""enter image description here""></a></p>

<p>The data is linearly separable. Then, I applied the glm function to create a logistic regression model.</p>

<pre><code>df &lt;- data.frame(class = z, x = x, y = y)
model &lt;- glm(z ~ x + y, family = binomial, data = df)
</code></pre>

<p>This produces the following output:</p>

<pre><code>summary(model)
Call:
glm(formula = z ~ x + y, family = binomial, data = df)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.127e-04  -2.000e-08  -2.000e-08   2.000e-08   7.699e-04  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    -1062      52666   -0.02    0.984
x              -1163      57197   -0.02    0.984
y               1433      70408    0.02    0.984

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.8274e+02  on 499  degrees of freedom
Residual deviance: 1.3345e-06  on 497  degrees of freedom
AIC: 6

Number of Fisher Scoring iterations: 25
</code></pre>

<p>The result surprised me, first because the parameter estimates are huge, and second because I was expecting such estimates to be close to the original decision boundary function, i.e. <code>-0.8x + y - 0.75 = 0</code>.</p>

<p>I then used the <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">glmnet</a> package to see if I could solve this issue. This package creates a penalised logistic regression model in order to deal with the large values in the parameter estimates. The code I used is the following:</p>

<pre><code>library(glmnet)
cvfit &lt;- cv.glmnet(as.matrix(df[,-1]), as.factor(df$class), family =   ""binomial"", type.measure = ""class"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/vH4AV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vH4AV.png"" alt=""enter image description here""></a></p>

<p>And the coefficients for the optimal penalty strength are:</p>

<pre><code>coef(cvfit, s = ""lambda.min"")
3 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept) -84.01446
x           -91.40983
y           113.18736
</code></pre>

<p>Such coefficients are smaller than the ones obtained with the <code>glm</code> function. Still they are not the same as the decision boundary function. </p>

<p>Does anybody know why this is happening? Any help is greatly appreciated.</p>
"
"0.195030550545354","0.191959011333372","187100","<p>I have a certain knowledge in stochastic processes (specially analysis of nonstationary signals), but in addition to be a beginner in R, I have never worked with regression models before.
Well, I have some doubts on understanding the outcome of the function summary() in R, when using with the results of a glm model fitted to my data. Well, suppose I used the following command to fit a generalized linear model to my data:**</p>

<pre><code>glm_model &lt;- glm(Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
</code></pre>

<p>Then I use summary(glm_model) to obtain the following:</p>

<pre><code>Call: 
glm(formula = Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-7.4583  -0.8985   0.1628   1.0670   6.0673  
Coefficients:

Estimate Std. Error t value Pr(&gt;|t|)    

(Intercept)        8.522e+00  6.553e-02 130.041  &lt; 2e-16 ***

Input1            -3.819e-04  3.021e-05 -12.642  &lt; 2e-16 ***

Input2            -2.557e-04  2.518e-05 -10.156  &lt; 2e-16 ***

Input3            -3.202e-02  1.102e-02  -2.906  0.00367 ** 

Input4            -1.268e-01  7.608e-02  -1.666  0.09570 .  

Input1:Input2      1.525e-08  2.521e-09   6.051 1.53e-09 ***


Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 2.487504)
    Null deviance: 18544  on 5959  degrees of freedom
Residual deviance: 14811  on 5954  degrees of freedom
  (1708 observations deleted due to missingness)
AIC: 22353
Number of Fisher Scoring iterations: 2
</code></pre>

<p>From a estimation theory perspective, I understand that ""estimate"" and ""Std. Error"" are the estimates and the standard deviation of the unknown parameters (beta1, beta2,...) of my model. However, there are some things I do not understand:</p>

<p>1) How can I assess how good my fit is from the output of <code>summary()</code>? We could not use only the information of the standard deviation of the parameter estimators to assess the goodness-of-fit. I would expect to have access to the sampling distribution of a given parameter estimator to know the % of estimates within +- 1std, +-0.5std or any +-x*std, for example. Other option would be knowing the theoretical distribution of the parameter estimator, so as to try to calculate its Cramer Rao Lower Bound and compare with the calculated std.</p>

<p>2) What does the t value (or Pr(>|t|) ) have to do with the goodness-of-fit? Since I am not familiar with regression models, I do not know the connection between the student t distribution and the estimation of the model parameters. What does it mean? Is the parameter estimator of the glm model distributed according to the student t pdf (like the sample estimator for small samples of an unknown population)? What conclusions should I take from Pr(>|t|)?</p>

<p>3) Do we have a more general form of assessing the goodness-of-fit, like a measure of the variability of the data my model can capture, maybe a table of critical values for such a measure given a certain significance level?** </p>

<p>4) When fitting a glm model, do we need to specify a significance level? If yes, why such an information is not provided by the summary function?</p>

<p>5) The summary function outputs some measures based on information theory, like AIC: 22353. Can we define an optimal reference value for AIC? What is a good AIC value? My intuition is that we could not do so, like other information theory measures (mutual information, entropym,...)</p>

<p>Thank you for your help!</p>
"
"0.123208220726739","0.121267812518166","189188","<p>If I create a linear model in R, I get a p-value for the whole model. When I create a logistic regression model, I don't. Why is this?</p>

<p><strong>Linear Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-x+rnorm(100)
summary(lm(y~x))

 Call: lm(formula = y ~ x)

 Residuals:
      Min       1Q   Median       3Q      Max 
 -2.46237 -0.52810 -0.04574  0.48878  2.81002 

 Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)     (Intercept) -0.02318    0.09394  -0.247    0.806     x            1.10130    0.09421  11.690   &lt;2e-16***
 --- Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Residual standard error: 0.9374 on 98 degrees of freedom Multiple
 R-squared:  0.5824,    Adjusted R-squared:  0.5781  F-statistic: 136.7 on
 1 and 98 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Logistic Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-factor(c(rep(""ONE"",50),rep(""TWO"",50)))
summary(glm(y~x,family = ""binomial""))

 Call: glm(formula = y ~ x, family = ""binomial"")

 Deviance Residuals: 
      Min        1Q    Median        3Q       Max  
 -1.20658  -1.18093  -0.00499   1.17444   1.21414  

 Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|) (Intercept)  3.857e-05  .000e-01   0.000    1.000 x           -3.924e-02  2.055e-01  -0.191    0.849

 (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom Residual deviance: 138.59  on 98  degrees of freedom AIC: 142.59

 Number of Fisher Scoring iterations: 3
</code></pre>
"
"0.311911491972989","0.295835585956533","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.157060280430173","0.142695448246348","191891","<p>I'm hoping someone can help clarify a few things for me.</p>

<p>I ran some relatively simple logistic regressions in r and am having trouble with interpretation.  I'm interested in the effects of elevation and a species diversity index on the presence/absence of a disease in individual animals.</p>

<p>I ran a simple model of: <code>Result~Elevation+Diversity</code> which gave this result</p>

<pre><code>Call:
glm(formula = Test_Result ~ Elevation + Simpsons_Diversity, family = binomial, 
    data = XXXXXX)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8141  -0.6984  -0.5317  -0.4143   2.3337  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -2.118e+00  1.594e-01 -13.289  &lt; 2e-16 
Elevation           1.316e-04  2.247e-05   5.855 4.76e-09 
Simpsons_Diversity -9.907e-01  2.725e-01  -3.635 0.000278 

    Null deviance: 3015.2  on 3299  degrees of freedom
Residual deviance: 2923.6  on 3297  degrees of freedom
AIC: 2929.6
</code></pre>

<p>I have a strong suspicion that diversity decreases with increasing elevation which I have confirmed although the relationship isn't quite as strong as I thought. When I run a model with an interaction term <code>elevation*diversity</code> I get:</p>

<pre><code>Call:
glm(formula = Test_Result ~ Elevation_1000 + Simpsons_Diversity_100 + 
    Elevation_1000 * Simpsons_Diversity_100, family = binomial, 
    data = XXXXXXX)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7908  -0.6959  -0.5437  -0.3963   2.4215  

Coefficients:
                                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                           -2.014422   0.179507 -11.222  &lt; 2e-16 
Elevation_1000                         0.112466   0.027433   4.100 4.14e-05 
Simpsons_Diversity_100                -0.015851   0.005780  -2.743   0.0061  
Elevation_1000:Simpsons_Diversity_100  0.001408   0.001200   1.173   0.2406   

    Null deviance: 3015.2  on 3299  degrees of freedom
Residual deviance: 2922.2  on 3296  degrees of freedom
AIC: 2930.2

Number of Fisher Scoring iterations: 5
</code></pre>

<p>Showing that adding the interaction term doesn't really help the fit of the model (AIC = 2930) and the interaction term itself is not significant (p-value=0.24).</p>

<p>Am I on the right track so far?</p>

<p>If I am, I understand how to convert coefficients to odds ratios and interpret those.  My main question is can I plot the predicted probabilities for a combination of elevation and diversity where each variable is allowed to vary? Or is this essentially plotting the interaction?  </p>

<p>I was able to create a dataframe where I varied elevation and diversity and I used my simple non-interaction model to obtain predicted probabilities using the PREDICT fuction) for those combinations, but I want to make sure that I am doing things correctly.  I've attached the plot of predicted probs for different levels of diversity. </p>

<p><a href=""http://i.stack.imgur.com/NINBE.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NINBE.gif"" alt=""Elevation vs. Predicted Probabilities for various levels of diversity)""></a></p>
"
"0.162989155760853","0.160422236979937","191916","<p>I have taken plenty of time to try and help myself, but I keep reaching dead ends. </p>

<p>I have a dataset consisting of body measurements collected from a bird species, and the sex of each bird (known by molecular means). I built a logistic regression model (using the AIC information criterion) to assess which measurements explain better the sex of the birds. My ultimate goal is to have an equation which could be used by others under field conditions to predict reliably the sex of the birds by taking as few body measurements as possible. </p>

<p>My final model includes four independent variables, namely ""Culmen"", ""Head-bill"", ""Tarsus length"", and ""Wing length"" (all continuous). I wish my model was a little more parsimonious, but all the variables seem to be important according to AIC criterion. Because the model produced should be used as prediction tool, I decided validate it using a leave-one-out cross validation approach. In my learning process, I first tried to complete the analyses (cross-validation and plotting) by including only one explanatory variable, namely ""Culmen"". </p>

<p>The output of the cross validation (package ""boot"" in R) yields two values (deltas), which are the cross-validated prediction errors where the first number is the raw leave-one-out, or lieu cross-validation result, and the second one is a bias-corrected version of it. </p>

<pre><code>model.full &lt;- glm(Sex ~ Culmen, data = my.data, family = binomial)
summary(model.full.1)

cv.glm(my.data, model.full, K=114)

$call
cv.glm(data = my.data, glmfit = model.full, K = 114)

$K
[1] 114

$delta
[1] 0.05941851 0.05937288
</code></pre>

<p>Q1. Could anyone expalin what do these two values represent and how to interpret them?    </p>

<p>Following is the code as presented by Dr. Markus MÃ¼ller (Calimo) in a similar, albeit not identical, post (<a href=""http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r"">http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r</a>) which I tried to tweak to meet my data:</p>

<pre><code>library(pROC)
data(my.data)
k &lt;- 114    # Number of observations or rows in dataset
n &lt;- dim(my.data)[1]
indices &lt;- sample(rep(1:k, ceiling(n/k))[1:n])

all.response &lt;- all.predictor &lt;- aucs &lt;- c()
for (i in 1:k) {
test = my.data[indices==i,]
learn = my.data[indices!=i,]
model &lt;- glm(Sex ~ Culmen, data = learn, family=binomial)
model.pred &lt;- predict(model, newdata=test)
aucs &lt;- c(aucs, roc(test$Sex, model.pred)$auc)
all.response &lt;- c(all.response, test$outcome)
all.predictor &lt;- c(all.predictor, model.pred)
}

Error in roc.default(test$Sex, model.pred) : No case observation.

roc(all.response, all.predictor)

Error in roc.default(all.response, all.predictor) : No valid data provided.

mean(aucs)
</code></pre>

<p>Q2. What's the reason for the first error message? I guess the second error is associated with the first one, and that it will be solved once I find a solution to the first one.</p>

<p>I will appreciate very much any help!!</p>

<p>Luciano </p>
"
"0.158475266494213","0.155979437026444","192319","<p>I have the following dataset:</p>

<ul>
<li>100 indiviuals  (id)</li>
<li>Each individual with 4 measurements at 4 timepoints time1,time2,time3 and
time4</li>
<li>Outcome is a categorical or ordered variable 1,2,3,4.</li>
</ul>

<p>I simulated a data based on probabilities to change from one level to the other (simulation below). Here is an example of the first 6 individuals:</p>

<pre><code>head(df0)
  id time1 time2 time3 time4
1  1     2     2     1     2
2  2     2     2     2     2
3  3     3     1     1     1
4  4     2     2     1     1
5  5     1     1     1     1
6  6     2     1     1     1
</code></pre>

<p>Individual 1 changes from 2 to 2 to 1 and ends to 2. With 4 times and 4 categories there are 4^4 = 256 combinations.</p>

<p>I have the following questions:</p>

<ol>
<li>Is there a standard plot to display this kind of data?
I could use a mosaic plot or plot probability against time (computed from a proportional odds model) but these plots don't consider repeated measurements. Parallele coordinate plot is not appropriate due to low number of levels (overplotting).</li>
<li>Is there a regression method to analize this kind of data (preferably in R)?
Again, the problem is that the measurements are matched.</li>
</ol>

<p>I try to analze the data. I computed the frequency of the various combinations to see which combination has the highest frequency:</p>

<pre><code>library(""sqldf"")
df1 &lt;- sqldf(paste(""SELECT time1, time2,time3,time4, COUNT(*) n""
            ,"" FROM df0""
            ,"" GROUP BY  time1, time2,time3,time4""
            ,"" ORDER BY COUNT(*) DESC""
            )
      )
nrow(df1)
[1] 34
head(df1)
  time1 time2 time3 time4  n
1     2     2     2     2 20
2     1     1     1     1  8
3     2     1     1     1  7
4     3     3     3     3  6
5     2     2     2     1  4
6     2     3     3     3  4
</code></pre>

<p>There are totally 34 combination (of 254 possible combination). the combination with the highest frequency is 2-2-2-2. Interesting would be also to know the inital distribution:</p>

<pre><code>table(df0$time1)
 1  2  3  4
12 51 28  9
</code></pre>

<p>I also tried to plot this kind of informations:</p>

<pre><code>vcol &lt;- c(paste(""indianred"",1:4,sep=""""),paste(""royalblue"",1:4, sep=""""))

plot(1, type=""n"", xlab="""", ylab="""", xlim=c(0,4), ylim=c(1, 4.5), axes=FALSE)
for (i in 1:8)  {
    lines(1:4,df1[i,1:4]+0.02*i
          , lwd=df1[i,5]*2
          , col=vcol[i] )
}
text(0,1:4,1:4)
text(0.5,1:4, labels=table(df0$time1))
axis(1,at=1:4,labels=c(""T1"",""T2"",""T3"",""T4""))
abline(h=1:4, lty = 2)
abline(v=1:4, lty = 2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/nXXkO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nXXkO.jpg"" alt=""enter image description here""></a></p>

<p>The width of the lines are proportionally to the counts of the specific combination. The drawback of this plot is that similar informations are splitted. For example, 2-2-2-2- is similar to 2-2-3-2, 2-1-2-2 and so on.</p>

<p>Any idea how to analyze this kind of data. Is there a multivariate methode like principal component, biplot, clusters which reduces a multidemensional dataset to a lower number of dimension?</p>

<p>What follows is the simulated data set:</p>

<pre><code># Number of individuals
n &lt;- 100
# Distribution at time 1
df0 &lt;- data.frame(id = 1:100
              , time1 = sample(rep(1:4, each=25)
                          ,n
                          , prob=rep(c(0.1,0.5,0.3,0.1), each=25)
                          , replace=TRUE)
              )

# Probablities:
p0 &lt;- 0.8     # No change
p1 &lt;- 0.15    # Change to next level
p2 &lt;- 0.04    # Jumps one level, e.g. 2 to 4
p3 &lt;- 0.01    # Jumps two levels, e.g 4 to 1

# Vectors
pp1 &lt;- c(p0,p1,p2,p3)
pp2 &lt;- c(p1,p0,p1,p2)
pp3 &lt;- c(p2,p1,p0,p1)
pp4 &lt;- c(p3,p2,p1,p0)

# Initialize time points
df0$time2 &lt;- NA
df0$time3 &lt;- NA
df0$time4 &lt;- NA

# Compute random levels given change probablities
for (i in 1:nrow(df0) ) {
      df0$time2[i] &lt;- ifelse(df0$time1[i] == 1,sample(c(1:4),1,prob=pp1), df0$time2[i])
      df0$time2[i] &lt;- ifelse(df0$time1[i] == 2,sample(c(1:4),1,prob=pp2), df0$time2[i])
      df0$time2[i] &lt;- ifelse(df0$time1[i] == 3,sample(c(1:4),1,prob=pp3), df0$time2[i])
      df0$time2[i] &lt;- ifelse(df0$time1[i] == 4,sample(c(1:4),1,prob=pp4), df0$time2[i])

      df0$time3[i] &lt;- ifelse(df0$time2[i] == 1,sample(c(1:4),1,prob=pp1), df0$time3[i])
      df0$time3[i] &lt;- ifelse(df0$time2[i] == 2,sample(c(1:4),1,prob=pp2), df0$time3[i])
      df0$time3[i] &lt;- ifelse(df0$time2[i] == 3,sample(c(1:4),1,prob=pp3), df0$time3[i])
      df0$time3[i] &lt;- ifelse(df0$time2[i] == 4,sample(c(1:4),1,prob=pp4), df0$time3[i])

      df0$time4[i] &lt;- ifelse(df0$time3[i] == 1,sample(rep(1:4),1,prob=pp1), df0$time4[i])
      df0$time4[i] &lt;- ifelse(df0$time3[i] == 2,sample(rep(1:4),1,prob=pp2), df0$time4[i])
      df0$time4[i] &lt;- ifelse(df0$time3[i] == 3,sample(rep(1:4),1,prob=pp3), df0$time4[i])
      df0$time4[i] &lt;- ifelse(df0$time3[i] == 4,sample(rep(1:4),1,prob=pp4), df0$time4[i])
}
</code></pre>
"
"0.0653410262803548","0.0857492925712544","194140","<p>I've been using stepAIC to narrow down my logistic regression model.  However, I get the following warning when I run my model:</p>

<p>glm.fit: fitted probabilities numerically 0 or 1 occurred</p>

<p>I know this means I have complete or quasi-complete separation in my data.  On examination of my data, I see the quasi-complete separation and think that it's meaningful.  Reading online, I see recommendations to use a Firth penalized regression (logistf) or exact logistic regression (elrm); but neither of these will work with stepAIC.  I've also tried bayesglm but I still get the same warning. </p>

<p>How should I select a model when my data has complete separation?  How would I do this in R?  Is my mistake in my stats or in my understanding of using the packages in R?  Any help would be much appreciated!</p>
"
"NaN","NaN","196734","<p>I have a panel data set with binary dependent variable of 20,000 observations and 11 independent variables.  I ran a logistic regression with fixed effects and the model returns maximum log likelihood value of <code>-7417.845</code> and AIC equals <code>Inf</code>. I am not sure why here the AIC value goes to infinity?</p>

<p>I am using R package ""<a href=""https://cran.r-project.org/web/packages/glmmML/glmmML.pdf"" rel=""nofollow"">glmmML</a>"".</p>
"
"NaN","NaN","197745","<p>I'm searching for a best subset selection algorithm for ridge regression in R. There is a wide range of algoritms for an ordinary least squares fit. There also exists a function like <code>stepAIC</code> for the ordinary least squares fit. </p>

<p>Does anybody have experience with ridge regression in R?</p>
"
"0.144474445059126","0.14219911474863","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.0435606841869032","0.0428746462856272","199284","<p>I have a large dataset with 4000 variables and 20000 observations. Most of the variables are a variety of moving averages since I am trying to create a predictive model. I'd like to use logistic regression with the best of these 4000 variables. I know this is an absurd amount and I am looking to trim it down to less than ten or so but am unsure of the best way to do it. </p>

<p>I'm aware that using stepAIC or the like will take forever and I need something that can select variables based on the p value or any other criteria.</p>

<p>Any help is appreciated.</p>
"
"NaN","NaN","199970","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>There are a few things I'm confused by here:</p>

<p>1) What is going on with the X1:term + term:X5 terms? What do they mean in the context of glm()?</p>

<p>2) There does not seem to be an intercept term in the output under <code>Coefficients</code>. Could this be for any other reason than there simply not being an intercept term?</p>

<p>3) The AIC for the model is 50000. How should I interpret this? Can I interpret this without more models to compare to? If it is not useful, what else should I be looking for instead?</p>
"
"0.0889178742536973","0.105021006302101","200031","<p>I have very easy question that I'm hoping someone can assist me with:</p>

<p>I ran an example logistic regression using this R code:</p>

<pre><code>     hours &lt;- c(0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5)
        pass &lt;- c(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1)
        data &lt;- data.frame(hours, pass)
        mylogit &lt;- glm(pass ~ hours, data = data, family = ""binomial"") #Activates the logistic regression model
        summary(mylogit) #Summary of the model

    Call:
    glm(formula = pass ~ hours, family = ""binomial"", data = data)

    Deviance Residuals: 
         Min        1Q    Median        3Q       Max  
    -1.70557  -0.57357  -0.04654   0.45470   1.82008  

    Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)  
    (Intercept)  -4.0777     1.7610  -2.316   0.0206 *
    hours         1.5046     0.6287   2.393   0.0167 *
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

        Null deviance: 27.726  on 19  degrees of freedom
    Residual deviance: 16.060  on 18  degrees of freedom
    AIC: 20.06

    Number of Fisher Scoring iterations: 5

    round(exp(cbind(OR = coef(mylogit), confint(mylogit))),3)

               OR 2.5 % 97.5 %
   (Intercept) 0.017 0.000  0.281
    hours       4.503 1.698 23.223
</code></pre>

<p>I know that by taking the exponent of the log-odds/coefficients for hours the odds of passing increase by a factor of 4.503 for a one-unit change in hours.  However, given that the explanatory variable (hours) is continuous, what is considered a 'one-unit change' i.e. going from 1 to 2 hours as one unit? or from 1.75 to 1.76 hours as one unit?  Also, is this interpretation of one-unit the same for regular OLS regression as well? I'm seeking to better understand the rules R applies to creating its regression coefficients.  </p>
"
"0.0653410262803548","0.0857492925712544","200182","<p>I am wondering how I can present the results of nonparametric regression. I performed the nonparametric tests using R, and R package 'np'.</p>

<p>The commands used for this are</p>

<blockquote>
  <p>freq &lt;- npreg(Respno ~ Colony + Localden + Agg.prop, regtype = ""ll"",bwmethod = ""cv.aic"",gradients = TRUE, data = resp)</p>
  
  <p>summary(freq2)</p>
  
  <p>npsigtest(freq2)</p>
</blockquote>

<p>Using the last command, 'npsigtest', I get results like this</p>

<blockquote>
  <p>npsigtest(freq)</p>
  
  <p>Kernel Regression Significance Test
  Type I Test with IID Bootstrap (399 replications, Pivot = TRUE, joint = FALSE)</p>
  
  <p>Explanatory variables tested for significance:</p>
  
  <p>Colony (1), Localden (2), Agg.prop (3)</p>
  
  <p>Colony Localden Agg.prop</p>
  
  <p>Bandwidth(s): 21.88052 5956578 0.3183519</p>
  
  <p>Individual Significance Tests</p>
  
  <p>P Value: </p>
  
  <p>Colony 0.0025063 ** </p>
  
  <p>Localden &lt; 2.22e-16 *** </p>
  
  <p>Agg.prop 0.0802005 .</p>
</blockquote>

<p>How do I present this data in a scientific paper? For the simple linear regression results, I included the n, df, t and P. </p>

<p>Thank you, any advice would be greatly appreciated!!</p>
"
"0.0754493182241785","0.0742610657232506","201462","<p>I'm fitting a logistic regression model with <code>patient_group</code> (0,1) as response variable and the explanatory variable being an interaction between two SNPs. When running summary for the model, the alert 'Coefficients: (1 not defined because of singularities)' is shown, and I guess it is due to the fact that the combination AACT has 0 observations. </p>

<p>My question is whether the statistics are still valid, or is there a better way to analyse this kind of data? (The SNPs are located close to each other and are most likely strongly linked.)</p>

<pre><code>&gt; table(data$SNP1, data$SNP2)    
     CC CT
  TT 27  9
  AT 83 14
  AA 47  0
&gt; model &lt;- glm(patient_group ~ SNP1 * SNP2, data=data, family=""binomial"")
&gt; summary(model)
Call:
glm(formula = patient_group ~ SNP1 * SNP2, family = ""binomial"", 
data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2735  -0.9072  -0.7679   1.4742   1.8365  

Coefficients: (1 not defined because of singularities)
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)    -1.4816     0.4954  -2.991  0.00279 **
SNP1AT          0.8065     0.5471   1.474  0.14048   
SNP1AA          0.4112     0.5978   0.688  0.49158   
SNP2CT          1.7047     0.8339   2.044  0.04093 * 
SNP1AT:SNP2CT  -2.3289     1.0833  -2.150  0.03157 * 
SNP1AA:SNP2CT       NA         NA      NA       NA   

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 218.19  on 179  degrees of freedom
Residual deviance: 212.31  on 175  degrees of freedom
(26 observations deleted due to missingness)
AIC: 222.31

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.13068205256071","0.128623938856882","205123","<p>To idetifying the important activity performed from users who have been converted in last N days. So, I have tried GLM, Rpart and Random forest models which can give me the impoprtant activities (in terms of Data Sciece its highly significant variables). Now If I want to extract the influencer counts for each important activity. i.e. count 5 for viewed_product activity means every user who converts into customer performs five product views. </p>

<p>I have tried my GLM with response variable as IsConverted and rest of the variables are frequencies of all activity performed at user level.  </p>

<pre><code>Call: 
glm(formula = regression_input2$IsConverted ~ ., family = binomial(), 
    data = regression_input2)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2285  -0.8820  -0.8245   1.3572   1.6479  

Coefficients: (1 not defined because of singularities)
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -0.904264   0.068017 -13.295  &lt; 2e-16 ***
view_product         0.029021   0.007867   3.689 0.000225 ***
view_collection     -0.034973   0.054757  -0.639 0.523018    
view_brand           0.047889   0.020289   2.360 0.018258 *  
search_category     -0.028920   0.032899  -0.879 0.379384    
search               0.172942   0.053855   3.211 0.001322 ** 
remov_product       -0.178905   0.151888  -1.178 0.238845    
payment                0.321474   1.054034   0.305 0.760371    
like_product          0.047789   0.035914   1.331 0.183305        
checkout_unsuccessful        NA         NA      NA       NA    
checkout_successful    0.397584   0.973795   0.408 0.683066    
added_product          0.179261   0.097749   1.834 0.066671 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2174.3  on 1668  degrees of freedom
Residual deviance: 2086.3  on 1657  degrees of freedom
AIC: 2110.3

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Apart from these, I have also tried Random forest and Rpart models. 
Here is the output of variable importance of Random forest.</p>

<pre><code>                      IncNodePurity
view_product           51.6447716
view_collec            16.9695232
view_brands            31.8345159
search_category        20.6999952
search                 18.0962766
remov_product_cart     6.6511766
payment                2.2859159
like_product          14.4360793
checkout_unsuccessful  0.2139582
checkout_successful    2.6284091
added_product          14.7047717
</code></pre>

<p>So, with the above utilities how can I get that counts of influencer activity which affects user's convergence. </p>
"
"0.107807193135897","0.0909508593886249","205390","<p>I have the following problem. I am trying to use the poisson regression on my data, which look like this:</p>

<pre><code>Year   Resistance.proportion
1990            0.367
1991            0.678
1992            0.786
</code></pre>

<p>I am using the following code in R to do the modeling:</p>

<pre><code>&gt; model &lt;- glm(Resistance.proportion ~ Year, data=sulfodf_for_poisson_r, family = poisson(link=log))
</code></pre>

<p>After executing the command, I get the following message in R:</p>

<pre><code>Warning messages:
1: In dpois(y, mu, log = TRUE) : non-integer x = 0.362069
2: In dpois(y, mu, log = TRUE) : non-integer x = 0.375000
3: In dpois(y, mu, log = TRUE) : non-integer x = 0.723684
4: In dpois(y, mu, log = TRUE) : non-integer x = 0.458333
5: In dpois(y, mu, log = TRUE) : non-integer x = 0.595238
6: In dpois(y, mu, log = TRUE) : non-integer x = 0.666667
7: In dpois(y, mu, log = TRUE) : non-integer x = 0.875000
8: In dpois(y, mu, log = TRUE) : non-integer x = 0.583333
9: In dpois(y, mu, log = TRUE) : non-integer x = 0.321429
10: In dpois(y, mu, log = TRUE) : non-integer x = 0.933333
</code></pre>

<p>Is the problem here really that the numbers are floats and have to be integers or is there something more subtle? Furthermore, if I summarize the model, I get an AIC that is infinite, which is also a bad thing as I understood.</p>

<p>I have looked through several questions as <a href=""http://stats.stackexchange.com/questions/26346/fitting-a-poisson-glm-in-r-issues-with-rates-vs-counts"">Fitting a Poisson GLM in R - issues with rates vs. counts</a>, but they do not provide an helpful answer to me.</p>

<p>So my questions are: </p>

<p>1.)Why am I getting these errors and how can I resolve them?</p>

<p>2.) Once the model is correct, what is a good way to graphically represent it?</p>
"
"0.106701449104437","0.105021006302101","206039","<p>I am looking at a logistic regression model for predicting hospital acquired infection likelihood (HAI) from predictors of whether germs are found on the  x number of patients (Patient), x number of environmental spots (Env), x number of air samples (Air) or x number of nurses' hands (Hand).</p>

<pre><code>   Month Patient Env Air Hand HAI HAIcat BedOccupancy
      1       4   0   0    1   1    yes            9
      2       2   0   2    0   0     no            9
      3       2   1   0    1   0     no            5
      4       1   2   0    2   2    yes            7
      5       2   3   0    1   1    yes            6
      6       1   2   0    0   1    yes            5
      7       4   0   0    2   1    yes            7
      8       2   0   0    1   3    yes            7
      9       3   2   2    0   1    yes            8
     10       3   0   0    1   1    yes            8
</code></pre>

<p>For example for Month 1, the percentage of HAI would be HAI/BedOccupancy=1/9.
So I'd like to know if bed occupancy or other contamination is significant in predicting HAI. I run a Logistic regression, but it says it's junk. What does a statistician do now?</p>

<pre><code>model&lt;-glm(cbind(MR$HAI,MR$BedOccupancy)~MR$Patient+MR$Env+MR$Air+MR$Hand,family = ""binomial"")
</code></pre>

<p>But I get a bad fit and non-significant correlation:</p>

<pre><code>Call:
glm(formula = cbind(MR$HAI, MR$BedOccupancy) ~ MR$Patient + MR$Env + MR$Air + 
        MR$Hand, family = ""binomial"")

Deviance Residuals: 
       1         2         3         4         5         6         7         8         9        10  
-0.12882  -1.08046  -1.33787   0.01400  -0.10685  -0.02229  -0.04008   1.03688   0.75723  -0.23824  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.30758    1.34049  -0.975    0.329
MR$Patient  -0.22920    0.39350  -0.582    0.560
    MR$Env      -0.02415    0.37672  -0.064    0.949
MR$Air      -0.46851    0.64611  -0.725    0.468
    MR$Hand      0.16054    0.58277   0.275    0.783

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.6594  on 9  degrees of freedom
Residual deviance: 4.6929  on 5  degrees of freedom
AIC: 30.911

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.0823219552131213","0.113435651621629","206058","<p>Using R, I can only find tools for performing L1 and/or L2 regularized linear regression (lars, glmnet) and tools for constrained linear regression (quadprog , or lsei {limSolve} , where the inequality and equality constraints can be only given in the form Ax = b , Gx &lt;= h).</p>

<p>It seems inutitive for me that the possibility of combining both should be required very oft when solving specific regression problems, but so far I havent been able to do it. </p>

<p>Instead of providing information on my specific set of constraints and algebraic system, IÂ´d be interested to know if this is a problem I can actually solve using the above mentioned packages? Are there any packages at all in R built for both parameter regularization and specific parameter penalties? </p>

<p>Update: For better understanding: I am not trying to combine different regularization methods (like in elastic net), nor trying to combine different parameter constraints. My goal is to combine regularization with specific coefficient constraints, so for example:</p>

<p>Find the most sparse solution (penalizing absolute values through LASSO) of a linear regression y = bx which satisfy the coefficient constraints bA &lt; h for some given matrix A and threshold h. </p>

<p>min($\parallel$ $\beta$x-y$\parallel_2^2$ + $\lambda$ $\mid$ $\beta$ $\mid_1$ ), s.t   $\beta$ A $\leqslant$ h</p>
"
"0.0974046509883157","0.0958706236059213","206702","<p>I have some data to fit a logistic regression, although the data seems quite good, the resulted fit does not look as expected.</p>

<blockquote>
<pre><code>  paramValue      normality
1  3.69             0
2  1.16             0
3  6.12             1
4  2.78             1
5  1.45             1
6  3.56             0
</code></pre>
</blockquote>

<pre><code>mylogit &lt;- glm(normality ~paramValue,  family = binomial(link=""logit""))
summary(mylogit)
</code></pre>

<blockquote>
<pre><code>Call:
glm(formula = normality ~ paramValue, family = binomial(link = ""logit""))

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.44994  -0.73312   0.08151   0.63377   1.41140  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.9945     0.9531  -2.093   0.0364 *
paramValue    1.2582     0.5655   2.225   0.0261 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 29.065  on 20  degrees of freedom
Residual deviance: 19.746  on 19  degrees of freedom
AIC: 23.746

Number of Fisher Scoring iterations: 5
</code></pre>
</blockquote>

<pre><code>    plot(paramValue,normality)

    x &lt;- seq(-1, 6, 0.1)

curve(predict(mylogit,data.frame(paramValue=x),type=""response""),add=TRUE, col=""red"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/1f0DY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1f0DY.png"" alt=""enter image description here""></a></p>

<p>Did I do something wrong? Is there any way to force the regression to cross the origin?</p>
"
"0.0435606841869032","0.0428746462856272","206735","<p>I've created an example table (just in order to create a function) with:</p>

<pre><code>ex&lt;-data.frame(b=c(rep('A',50),rep('B',30), rep('C',20)), 
fl=round(runif(100,0,1),0),r=runif(100,0,0.5))
ex2&lt;-cbind(ex,model.matrix(~b-1,ex))
lineal&lt;-ex2$bB+ex2$bA*ex$fl+ex$fl
ex$clase&lt;-round(1/(1+exp(-lineal)),0)
</code></pre>

<p>Then I run a logistic regression model (MASS library)</p>

<pre><code>fm&lt;-as.formula(clase~b+fl+r)
modT&lt;-glm(clase~1, family=binomial, data = ex)
modT&lt;-stepAIC(modT, scope = fm, family=binomial, data =ex, k = 4)
summary(modT)
</code></pre>

<p>As you can see coefficients are not significant, but I've created the class using them. So I don't understand why this is happening.</p>

<p><a href=""http://i.stack.imgur.com/yR4jV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yR4jV.png"" alt=""enter image description here""></a></p>
"
"0.144978720397083","0.154586735600211","207608","<p>I'd like to run a probit regression on the ""B1_df"" data frame with 3 categorical outcome variables (rank 1,2 or 3). I cannot use glm because there are 3 outcome variables.  I would like to be able to tie out the results from polr() and mlogit().  I am getting reasonable results from polr() but strange results from mlogit() I believe due to my data frame construction.</p>

<p>Basically I have 3 machine B1, B2 and B3 and each have 5 runs that are ranked 1 to 3 and I am using probit to tell me which machine has the highest probability of returning the highest rank. </p>

<pre><code>First with polr():

require(ggplot2)
require(MASS)
require(mlogit)

machine = c(rep(""B1"",5), rep(""B2"",5),rep(""B3"",5))
rank = c(rep(3,5), rep(2,5),rep(1,5))
#rank = c(c(3,3,3,3,1), rep(2,5),rep(1,5)) # see *** comment below
dat = data.frame(machine = machine, rank = rank)
dat$B1 =  c(rep(1,5), rep(0,5),rep(0,5))
    dat$B2 =  c(rep(0,5), rep(1,5),rep(0,5))
dat$B3 =  c(rep(0,5), rep(0,5),rep(1,5))
B1_df = dat[,1:3]
B1_df
b1=polr(formula = as.factor(rank)~ as.factor(B1), data= B1,  Hess = FALSE, model = TRUE,method = c(""probit""))
b1

   machine rank B1
1       B1    3  1
2       B1    3  1
3       B1    3  1
4       B1    3  1
5       B1    3  1
6       B2    2  0
7       B2    2  0
8       B2    2  0
9       B2    2  0
10      B2    2  0
11      B3    1  0
12      B3    1  0
13      B3    1  0
14      B3    1  0
15      B3    1  0
&gt; b1=polr(formula = as.factor(rank)~ as.factor(B1), data= B1,  Hess = FALSE, model = TRUE,method = c(""probit""))
&gt; b1
Call:
polr(formula = as.factor(rank) ~ as.factor(B1), data = B1, Hess = FALSE, 
    model = TRUE, method = c(""probit""))

Coefficients:
as.factor(B1)1 
      8.599074 

Intercepts:
         1|2          2|3 
0.0002318407 4.4165977032 

Residual Deviance: 13.86319 
AIC: 19.86319 
</code></pre>

<p>Question:  Does the coef of 8.5 indicate that by setting B1 =1 the z-score would increase by 8.5 giving a higher probability of getting a higher rank? I was thinking that it did but then I uncommented this line:</p>

<pre><code>rank = c(c(3,3,3,3,1), rep(2,5),rep(1,5))
</code></pre>

<p>so now B1 does not have all 3's it has 4 3's and one 1. I was expecting the coef, intercepts, deviance to change but they don't.  Please uncomment ** and run. Any idea why no change?</p>

<p>Now I'd like to try to get those same results in mlogit:</p>

<pre><code>B1_df2 = mlogit.data(B1_df, shape = ""wide"", choice =""rank"", id.var= ""B1"") #configure the data frame with mlogit.data
B1_df2
summary(mlogit(rank ~  0 | B1 ,data=B1_df2, Probit=TRUE))  # call mlogit 

&gt; summary(mlogit(rank ~  0 | B1 ,data=B1_df2, Probit=TRUE))  # call mlogit

Call:
mlogit(formula = rank ~ 0 | B1, data = B1_df2, Probit = TRUE, 
    method = ""nr"", print.level = 0)

Frequencies of alternatives:
       1        2        3 
0.533333 0.400000 0.066667 

nr method
16 iterations, 0h:0m:0s 
g'(-H)^-1g = 7.84E-07 
gradient close to zero 

Coefficients :
                 Estimate  Std. Error t-value Pr(&gt;|t|)
2:(intercept) -8.9248e-17  6.3246e-01  0.0000   1.0000
3:(intercept) -1.6669e+01  1.8625e+03 -0.0089   0.9929
2:B1          -1.0986e+00  1.3166e+00 -0.8345   0.4040
3:B1           1.5570e+01  1.8625e+03  0.0084   0.9933

Log-Likelihood: -11.683
McFadden R^2:  0.11726 
Likelihood ratio test : chisq = 3.1037 (p.value = 0.21186)
</code></pre>

<p>You can see the mlogit coefs are nowhere near the 8.5 and there seem to be duplicates.</p>

<p>For mlogit() I am looking at page 22 here for the pure multinomial model:</p>

<p><a href=""https://cran.r-project.org/web/packages/mlogit/mlogit.pdf"" rel=""nofollow"">https://cran.r-project.org/web/packages/mlogit/mlogit.pdf</a></p>

<p>Any idea how to properly set up these models to get them to tie?</p>
"
"0.0435606841869032","0.0428746462856272","209374","<p>I'm using the <code>multinom</code> package in R to run a multinomial logistic regression model. My dependent variable has 3 levels and as the output, I'm getting the probability for each of the level.</p>

<p>Currently, I have the VIF, AIC, p-values and confusion matrix in the model.</p>

<p>I have the following questions:</p>

<ol>
<li><p>I want a single output based on the probabilities. How do I decide a ""cut-off"" for deciding the ""best event""?</p></li>
<li><p>Does it make sense to get an ROC curve here? If yes, then how do I get one?</p></li>
<li><p>What are the things I should look at for the validation of the model?</p></li>
</ol>
"
"0.162989155760853","0.160422236979937","209766","<p>In order to explain the distribution of a species (LO), I have run a glm (family=binary, link=logit) in R 3.1.2. I have presence absence data for my species, and a number of dataset describing the climate and landscape as explanatory values. My dataset looks like this in R:</p>

<pre><code>'data.frame':   72920 obs. of  17 variables:
 $ LO : int  1 1 1 1 1 1 1 1 1 1 ...
 $ MAG: int  0 0 0 0 0 0 0 0 1 0 ...
 $ PCR: int  0 1 0 0 0 0 0 0 0 0 ...
 $ WAT: int  0 0 0 0 0 0 0 0 0 0 ...
 $ SVE: int  0 0 0 0 0 0 0 0 0 0 ...
 $ ARA: int  0 0 1 0 0 0 0 1 0 0 ...
 $ GRA: int  0 0 0 1 0 1 1 0 0 1 ...
 $ FOR: int  0 0 0 0 0 0 0 0 0 0 ...
 $ MHF: num  32 39.2 29.2 36 39.2 ...
 $ B13: int  100 99 112 114 104 109 107 105 106 113 ...
 $ B12: int  420 421 474 485 438 454 435 427 427 472 ...
 $ MTP: int  43 81 13 4 3 3 2 21 98 2 ...
 $ BI6: int  72 72 74 70 74 68 69 70 70 84 ...
 $ BI4: int  4821 4886 4947 4997 4859 4971 4909 5009 5173 4901 ...
 $ ALT: num  18.2 20.1 132.8 166.5 54.8 ...
 $ BI1: num  18.6 18.6 18.5 18.2 18.6 ...
 $ URB: int  1 0 0 0 1 0 0 0 0 0 ...
</code></pre>

<p>LO is my species, and 1 symbolizes presence, while 0 is absence of the species.</p>

<p>I type the following into R, and get a model output:</p>

<pre><code>glm(formula = LO ~ MAG + PCR + WAT + FOR + SVE + ARA + GRA + 
    MHF + B13 + B12 + MTP + BI4 + ALT + BI1 + URB, family = binomial(""logit""), 
    data = data)
</code></pre>

<p>At first glance, my resulting model look okay, with significant interactions are found. But problematically, I found it actually predicts where my species is not likely to be found. What I want is of course a model describing where I am likely to find my species. I hope someone can tell me what has gone wrong in my model, making it predict absence instead of presence.</p>

<p>I have worked with my data in arcGIS, and when using the regression results to look at the predicted distribution, I find that the regression predicts that my species will be where it is actually absent, while being absent from all the geographic regions where it is actually found. This is why I know that absence, not presence, is being described.</p>

<p>The output of my model looks like this:</p>

<pre><code>&gt; model2&lt;-glm(LO~MAG+PCR+WAT+FOR+SVE+ARA+GRA+MHF+B13+B12+MTP+BI4+ALT+BI1+URB, data=data, family=binomial(""logit""))
&gt; summary(model2)

Call:
glm(formula = LO ~ MAG + PCR + WAT + FOR + SVE + ARA + GRA + 
    MHF + B13 + B12 + MTP + BI4 + ALT + BI1 + URB, family = binomial(""logit""), 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.7059  -0.4574   0.2278   0.5829   2.9743  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -4.960e+00  1.279e-01 -38.786  &lt; 2e-16 ***

MAG          4.116e-01  5.463e-02   7.534 4.92e-14 ***
PCR          2.129e-01  5.317e-02   4.004 6.24e-05 ***
WAT         -1.245e+00  6.946e-02 -17.920  &lt; 2e-16 ***
FOR         -4.349e-01  4.514e-02  -9.635  &lt; 2e-16 ***
SVE         -1.305e+00  6.734e-02 -19.376  &lt; 2e-16 ***
ARA          9.591e-01  4.787e-02  20.038  &lt; 2e-16 ***
GRA         -7.206e-01  5.232e-02 -13.773  &lt; 2e-16 ***
MHF         -1.181e-02  1.165e-03 -10.137  &lt; 2e-16 ***
B13          1.058e-02  8.541e-04  12.383  &lt; 2e-16 ***
B12         -6.434e-04  1.160e-04  -5.547 2.91e-08 ***
MTP         -2.785e-03  5.401e-04  -5.156 2.52e-07 ***
BI4          3.914e-04  1.229e-05  31.849  &lt; 2e-16 ***
ALT          2.641e-03  2.982e-05  88.581  &lt; 2e-16 ***
BI1          2.611e-01  2.355e-03 110.867  &lt; 2e-16 ***
URB          5.909e-01  7.080e-02   8.346  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 98797  on 72919  degrees of freedom
Residual deviance: 54626  on 72904  degrees of freedom
AIC: 54658

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.106701449104437","0.105021006302101","212301","<p>I have a huge doubt, which I believe is Basic. I have no difficulty in interpreting the results of our logistic regression model using the ODD ratio, but I do not know what to do when I work with Mixed effects model for longitudinal data.</p>

<p>Below they use the <code>glmer</code> function to estimate a mixed effects logistic regression model with Il6, CRP, and LengthofStay as patient level continuous predictors, CancerStage as a patient level categorical predictor (I, II, III, or IV), Experience as a doctor level continuous predictor, and a random intercept by DID, doctor ID.</p>

<p>The <code>glmer</code> function created 407 groups that refer to the number of doctors.</p>

<p>What would it mean for example the -0.0568 of IL6 and the -2.3370 of CancerStageIV's in the study presented?</p>

#################

<p>m &lt;â€ glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer =  ""bobyqa""),      nAGQ = 10) 
print(m, corr = FALSE) </p>

<h1>Generalized linear mixed model fit by maximum likelihood</h1>

<h2>Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]</h2>

<h2>Family:</h2>

<p>binomial ( logit )  </p>

<h2>Formula:</h2>

<p>remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +<br>
   (1 | DID)  </p>

<p>Data: hdp  </p>

<pre><code>  AIC        BIC    logLik     deviance  df.resid   
 7397        7461    -3690        7379     8516 
</code></pre>

<h2>Random effects:</h2>

<p>Groups Name         Std.Dev.<br>
     DID    (Intercept) 2.01 </p>

<p>Number of obs: 8525, groups: DID, 407  </p>

<h1>Fixed Effects:</h1>

<pre><code>  Intercept    IL6        CRP       CancerStageII  
 â€2.0527     â€0.0568    â€0.0215       â€0.4139 

CancerStageIII   CancerStageIV       LengthofStay      Experience  
 â€1.0035           â€2.3370              â€0.1212          0.1201 
</code></pre>
"
"0.144474445059126","0.14219911474863","212611","<p>I have ran these two Logistic Regression models (below) on some small data and I am able to interpret the output - significance and direction - of the regressors, but I do not know for sure how to interpret all the data which is supposed to tell me everything related to <strong>effect (size) etc</strong>. I did select my predictors properly by adding one each time and checking whether the model was still significant (which yielded the same result as an automatic stepAIC from the MASS package) and I also did some diagnostic checks (outliertest, VIF-score).</p>

<p>What (I think) I got from the models is:</p>

<ul>
<li><strong>R2</strong>: model1 only explains 4.8% of all variation and model2 6.6%, so no predictive power?</li>
<li><strong>C</strong>: model1 does not have acceptable discrimination, neither does model2 (&lt;0.7)</li>
</ul>

<p>Is there other <strong>important information that I am ignorant of</strong>? It seems that these models do <strong>not have much 'power'</strong> (according to <strong>R2</strong> and <strong>C</strong>), but how are they then <strong>still significant</strong> (there is also very significant behaviour (***) for regressors)?</p>

<p>*PS: Sorry if am missing obvious things - I do not have that strong of a statistical background. I am also finding it a hard time searching for all the parameters and metrics since they are often denoted by a one letter name (e.g. C, g) - which is not easy to search for if you do not know what you are looking for... So that's why I came to CrossValidated!</p>

<p>I have found <a href=""http://stats.stackexchange.com/questions/104485/logistic-regression-evaluation-metrics"">this question</a>, but it does not really have an answer since it's maybe way too vague? If someone else has a reading suggestion for my problem, that's also welcomed!*</p>

<h2>First model: Agentivity ~ Period + Genre</h2>

<pre><code>(from lrm)      
                     Model Likelihood      Discrimination    Rank Discrim.    
                       Ratio Test            Indexes           Indexes       
Obs          700    LR chi2      25.55    R2       0.048    C       0.602    
 strong      403    d.f.             4    g        0.440    Dxy     0.204    
 weak        297    Pr(&gt; chi2) &lt;0.0001    gr       1.553    gamma   0.240    
max |deriv| 3e-14                         gp       0.105    tau-a   0.100    
                                          Brier    0.236                     

(from glm)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3171  -0.9825  -0.8094   1.1882   1.6090  

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 954.29  on 699  degrees of freedom
Residual deviance: 928.74  on 695  degrees of freedom
AIC: 938.74

Number of Fisher Scoring iterations: 4
</code></pre>

<h2>Second model: Type ~ Period</h2>

<pre><code>(from lrm)
                      Model Likelihood      Discrimination    Rank Discrim.    
                         Ratio Test           Indexes           Indexes       
Obs           872    LR chi2      36.70    R2       0.066    C       0.637    
 mediopassive 701    d.f.             2    g        0.552    Dxy     0.275    
 passive      171    Pr(&gt; chi2) &lt;0.0001    gr       1.736    gamma   0.401    
max |deriv| 9e-10                          gp       0.087    tau-a   0.087    
                                           Brier    0.151                     

(for glm)
Deviance Residuals: 
 Min       1Q   Median       3Q      Max  
-0.8645  -0.6109  -0.4960  -0.4960   2.0767  

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 863.19  on 871  degrees of freedom
Residual deviance: 826.49  on 869  degrees of freedom
AIC: 832.49

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.137750978465894","0.13558153613666","212903","<p>I have the data <a href=""https://docs.google.com/spreadsheets/d/1lEzUt0QdFCp1ho-iWd4HzEIZoo8IyAM8YP2gu-K7BQo/edit?usp=sharing"" rel=""nofollow"">here</a>.But When I tried to build the logistic regression model using glm function its shows NA in TotalVisits. I have found similar question on stack overflow but that is answered for linear model.  </p>

<pre><code> str(quality)
'data.frame':   131 obs. of  14 variables:
 $ MemberID            : int  1 2 3 4 5 6 7 8 9 10 ...
 $ InpatientDays       : int  0 1 0 0 8 2 16 2 2 4 ...
 $ ERVisits            : int  0 1 0 1 2 0 1 0 1 2 ...
 $ OfficeVisits        : int  18 6 5 19 19 9 8 8 4 0 ...
 $ Narcotics           : int  1 1 3 0 3 2 1 0 3 2 ...
 $ DaysSinceLastERVisit: num  731 411 731 158 449 ...
 $ Pain                : int  10 0 10 34 10 6 4 5 5 2 ...
 $ TotalVisits         : int  18 8 5 20 29 11 25 10 7 6 ...
 $ ProviderCount       : int  21 27 16 14 24 40 19 11 28 21 ...
 $ MedicalClaims       : int  93 19 27 59 51 53 40 28 20 17 ...
 $ ClaimLines          : int  222 115 148 242 204 156 261 87 98 66 ...
 $ StartedOnCombination: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ AcuteDrugGapSmall   : int  0 1 5 0 0 4 0 0 0 0 ...
 $ PoorCare            : int  0 0 0 0 0 1 0 0 1 0 ...



table(is.na(quality))
FALSE 
1834
</code></pre>

<p>My data does not contain any NA values.</p>

<pre><code>set.seed(100)
split &lt;- sample.split(quality$PoorCare, SplitRatio = .5)
train &lt;-subset(quality, split ==TRUE)
test &lt;- subset(quality, split ==FALSE)
</code></pre>

<p>Building the model using all variable </p>

<pre><code>log.Quality &lt;- glm(PoorCare ~ ., data = train, family = 'binomial')

summary(log.Quality)      
Call:
glm(formula = PoorCare ~ ., family = ""binomial"", data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5679  -0.6384  -0.3604  -0.1154   2.1298  

Coefficients: (1 not defined because of singularities)
                          Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)              -3.583178   1.807020  -1.983   0.0474 *
MemberID                 -0.008742   0.010988  -0.796   0.4263  
InpatientDays            -0.106578   0.095632  -1.114   0.2651  
ERVisits                  0.275225   0.310364   0.887   0.3752  
OfficeVisits              0.126433   0.066140   1.912   0.0559 .
Narcotics                 0.190862   0.106890   1.786   0.0742 .
DaysSinceLastERVisit     -0.001221   0.002026  -0.603   0.5467  
Pain                     -0.020104   0.023057  -0.872   0.3832  
TotalVisits                     NA         NA      NA       NA  
ProviderCount             0.046297   0.040637   1.139   0.2546  
MedicalClaims             0.025123   0.030564   0.822   0.4111  
ClaimLines               -0.010384   0.012746  -0.815   0.4152  
StartedOnCombinationTRUE  2.205058   1.724923   1.278   0.2011  
AcuteDrugGapSmall         0.217813   0.139890   1.557   0.1195  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 72.549  on 64  degrees of freedom
Residual deviance: 49.213  on 52  degrees of freedom
AIC: 75.213

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Can anyone provide me a good explanation why this is happening ? </p>
"
"0.11525073729837","0.113435651621629","214682","<p>I am trying to understand the basic difference between stepwise and backward regression in R using the step function. 
For stepwise regression I used the following command </p>

<pre><code>  step(lm(mpg~wt+drat+disp+qsec,data=mtcars),direction=""both"")
</code></pre>

<p>I got the below output for the above code.</p>

<p><a href=""http://i.stack.imgur.com/dpR8s.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dpR8s.png"" alt=""forward""></a></p>

<p>For backward variable selection I used the following command </p>

<pre><code> step(lm(mpg~wt+drat+disp+qsec,data=mtcars),direction=""backward"")
</code></pre>

<p>And I got the below output for backward</p>

<p><a href=""http://i.stack.imgur.com/7hZj9.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7hZj9.jpg"" alt=""backward""></a></p>

<p>As much as I have understood, when no parameter is specified, stepwise selection acts as backward unless the parameter ""upper"" and ""lower"" are specified in R. Yet in the output of stepwise selection, there is a +disp that is added in the 2nd step. What is the function trying to achieve by adding the +disp again in the stepwise selection? Why is R adding the +disp in the 2nd step whereas the results are the same (AIC values and model selection values) as the backward selection. How is R exactly working in the stepwise selection? </p>

<p>I really want to understand how this function is working in R. 
Thanks in advance for the help! </p>
"
"0.0754493182241785","0.0742610657232506","214892","<p>I'm trying to construct a univariate prediction model using logistic regression in order to predict credit default likelihood from overdue level in telecommunication companies:</p>

<p><a href=""https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg"" rel=""nofollow"">https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg</a></p>

<p>For this, I used the function glm and found two problematic ranks:
        RANK_OVERDUE between S/. 3,000 and S/. 5,000 &amp; RANK_OVERDUE More than S/. 5,000.</p>

<p>which have p-values of 0.946 and 0.473:</p>

<pre><code>Call:
glm(formula = impago ~ MONTO_VENCIDO_DOC_IMPAGOS, family = binomial, 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.1355  -0.0569  -0.0569  -0.0569   3.5855  

Coefficients:
                                                  Estimate
(Intercept)                                       -6.42627
RANK_OVERDUE&lt;S/. 0 - S/. 500]         0.69763
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000]   1.73952
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000] -10.13980
RANK_OVERDUE&lt;S/. 500 - 1,500]         1.13854
RANK_OVERDUEMÃ¡s de S/. 5,000          0.71916
</code></pre>

<p></p>

<pre><code>                                                 Pr(&gt;|z|)    
(Intercept)                                       &lt; 2e-16 ***
RANK_OVERDUE&lt;S/. 0 - S/. 500]       1.78e-15 ***
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000] 2.51e-05 ***
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000]    0.946    
RANK_OVERDUE&lt;S/. 500 - 1,500]       1.23e-06 ***
RANK_OVERDUEMÃ¡s de S/. 5,000           0.473    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 9683.9  on 345828  degrees of freedom
Residual deviance: 9603.5  on 345823  degrees of freedom
AIC: 9615.5

Number of Fisher Scoring iterations: 15
</code></pre>

<p>I would need to know what options I have on order to deal with this situation. Should these ranks be included in the model? I tried to join them into one (overdue over S/. 3,000) but when applying again the model, it continued to be not significant (I obtained a p-value of 0.919).</p>
"
"0.0435606841869032","0.0428746462856272","216119","<p>I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket.</p>

<pre><code>  logitshots &lt;- glm(df$shot_made_flag ~ df$shot_distance, family = binomial(link=""logit""))
Call:  glm(formula = df$shot_made_flag ~ df$shot_distance, family = binomial(link = ""logit""))

Coefficients:
 (Intercept)  df$shot_distance  
      0.3681           -0.0441  

Degrees of Freedom: 25696 Total (i.e. Null);  25695 Residual
Null Deviance:      35330 
Residual Deviance: 34290    AIC: 34300
</code></pre>

<p>As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. </p>

<p>To do so I have done it this way, but I get a positive effect, so I am not sure about it. </p>

<pre><code>(exp(coef(logitshots))/(1+exp(coef(logitshots))))
(Intercept) df$shot_distance 
   0.5909933        0.4889768 
</code></pre>

<p>So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?)  </p>

<p>I'd really appreciate any interesting insight and help! :)</p>
"
"0.184812331090109","0.18190171877725","216122","<p>As far as I know, the difference between logistic model and fractional response model (frm) is that the dependent variable (Y) in which frm is [0,1], but logistic is {0, 1}. Further, frm uses the quasi-likelihood estimator to determine its parameters. </p>

<p>Normally, we can use <code>glm</code> to obtain the logistic models by <code>glm(y ~ x1+x2, data = dat, family = binomial(logit))</code>. </p>

<p>For frm, we change <code>family = binomial(logit)</code> to <code>family = quasibinomial(logit)</code>.  </p>

<p>I noticed we can also use <code>family = binomial(logit)</code> to obtain frm's parameter since it gives the same estimated values. See the following example</p>

<pre><code>library(foreign)
mydata &lt;- read.dta(""k401.dta"")


glm.bin &lt;- glm(prate ~ mrate + age + sole + totemp, data = mydata
,family = binomial('logit'))
summary(glm.bin)
</code></pre>

<p>return,</p>

<pre><code>Call:
glm(formula = prate ~ mrate + age + sole + totemp, family = binomial(""logit""), 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1214  -0.1979   0.2059   0.4486   0.9146  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.074e+00  8.869e-02  12.110  &lt; 2e-16 ***
mrate        5.734e-01  9.011e-02   6.364 1.97e-10 ***
age          3.089e-02  5.832e-03   5.297 1.17e-07 ***
sole         3.636e-01  9.491e-02   3.831 0.000128 ***
totemp      -5.780e-06  2.207e-06  -2.619 0.008814 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1166.6  on 4733  degrees of freedom
Residual deviance: 1023.7  on 4729  degrees of freedom
AIC: 1997.6

Number of Fisher Scoring iterations: 6
</code></pre>

<p>And for <code>family = quasibinomial('logit')</code>,</p>

<pre><code>glm.quasi &lt;- glm(prate ~ mrate + age + sole + totemp, data = mydata
,family = quasibinomial('logit'))
summary(glm.quasi)
</code></pre>

<p>return,</p>

<pre><code>Call:
glm(formula = prate ~ mrate + age + sole + totemp, family = quasibinomial(""logit""), 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1214  -0.1979   0.2059   0.4486   0.9146  

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.074e+00  4.788e-02  22.435  &lt; 2e-16 ***
mrate        5.734e-01  4.864e-02  11.789  &lt; 2e-16 ***
age          3.089e-02  3.148e-03   9.814  &lt; 2e-16 ***
sole         3.636e-01  5.123e-02   7.097 1.46e-12 ***
totemp      -5.780e-06  1.191e-06  -4.852 1.26e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasibinomial family taken to be 0.2913876)

    Null deviance: 1166.6  on 4733  degrees of freedom
Residual deviance: 1023.7  on 4729  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 6
</code></pre>

<p>The estimated Beta from both <code>family</code> are the same, but the difference is the SE values.  However, to obtain the correct SE, we have to use <code>library(sandwich)</code> as in this <a href=""http://stackoverflow.com/questions/37584715/fractional-response-regression-in-r"">post</a>.</p>

<p>Now, my questions:</p>

<ol>
<li>What is the difference between these two codes?</li>
<li>Is frm about to obtain robust SE?</li>
</ol>

<p>If my understanding is not correct, please give some suggestions.</p>
"
"0.0889178742536973","0.105021006302101","219684","<p>I am trying to create a logistic regression model to predict whether a customer given a loan will be a bad or a good customer: bad meaning missing a certain amount of payments and good meaning frequent enough and in time with payments. For the purpose of the model I have coded Bad as 1 and Good as 0 and tried different combinations with the variables. </p>

<p>One of the models I have built has an AIC of 5383.7 and Gini coefficient of 0.416733. This is the result after I play around with the threshold:</p>

<pre><code>     FALSE TRUE
  0  3327  638
  1   165   95
</code></pre>

<p>So the model guessed that 165 customers would be good, but they are bad, but also put 638 good customers into the bad customers group.</p>

<p>The second model I built has an AIC of 5734.6 (350.9 higher), but its Gini is 0.4190394 and is slightly better at predicting the bad customers:</p>

<pre><code>     FALSE TRUE
  0  3537  673
  1   177  105
</code></pre>

<p>[UPDATE] Okay. After checking a few things - It turns out that one of the variables has missing values and the model excludes the observations that have them by default. Hence the difference in observations in my models. I know about multiple imputation, but I don't really feel alright with it. My question is should I impute the missing data or should I exclude it from the data set so I can compare models with different number of variables?</p>
"
"0.11525073729837","0.113435651621629","219828","<p>I am doing logistic regression in R on a binary dependent variable with only one independent variable. I found the odd ratio as 0.99 for an outcomes. This can be shown in following. Odds ratio is defined as, $ratio_{odds}(H) = \frac{P(X=H)}{1-P(X=H)}$. As given earlier $ratio_{odds} (H) = 0.99$ which implies that $P(X=H) = 0.497$ which is close to 50% probability. This implies that the probability for having a H cases or non H cases 50% under the given condition of independent variable. This does not seem realistic from the data as only ~20% are found as H cases. Please give clarifications and proper explanations of this kind of cases in logistic regression.</p>

<p>I am hereby adding the results of my model output:</p>

<pre><code>M1 &lt;- glm(H~X, data=data, family=binomial())
summary(M1)

Call:
glm(formula = H ~ X, family = binomial(), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8563   0.6310   0.6790   0.7039   0.7608  

Coefficients:
                Estimate      Std. Error      z value     Pr(&gt;|z|)    
(Intercept)    1.6416666      0.2290133      7.168      7.59e-13 ***
   X          -0.0014039      0.0009466     -1.483      0.138    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1101.1  on 1070  degrees of freedom
Residual deviance: 1098.9  on 1069  degrees of freedom
  (667 observations deleted due to missingness)
AIC: 1102.9

Number of Fisher Scoring iterations: 4


exp(cbind(OR=coef(M1), confint(M1)))
Waiting for profiling to be done...
                                      OR           2.5 %       97.5 %
(Intercept)                    5.1637680       3.3204509     8.155564
     X                         0.9985971       0.9967357     1.000445
</code></pre>

<p>I have 1738 total dataset, of which H is a dependent binomial variable. There are 19.95% fall in (H=0) category and remaining are in (H=1) category. Further this binomial dependent variable compare with the covariate X whose minimum value is 82.23, mean value is 223.8 and maximum value is 391.6. The 667 missing values correspond to the covariate X i.e 667 data for X is missing in the dataset out of 1738 data.</p>
"
"0.174242736747613","0.171498585142509","220868","<p>The goal of this regression is to determine whether the amount of leaf disk that an insect consumed varied by what tree the leaf material came from. I'll acknowledge upfront that my coding is rarely pretty/efficient, but hopefully it works (usually).</p>

<ul>
<li>Variables:

<ul>
<li>Response: pctrans; the percent of a 7 mm diameter leaf disk that was consumed.  Values have been transformed to fit (0,1).</li>
<li>Explanatory: tree; a categorical (factor) variable of six tree types.</li>
</ul></li>
</ul>

<p>When I use betareg(), which as I understand it, is best suited to data of this sort, I get no significance:</p>

<pre><code>model.beta &lt;- betareg(pctrans ~ tree, data=BT.data, link=""logit"")
modelnull.beta &lt;- betareg(pctrans ~ tree, data=BT.data, link=""logit"")
lrtest(model1.beta, modelnull.beta)
</code></pre>

<p>Results:</p>

<pre><code>Call:
betareg(formula = pctrans ~ tree, data = BT.data, link = ""logit"")

Standardized weighted residuals 2:
    Min      1Q  Median      3Q     Max 
-2.7716 -0.5800  0.0472  0.5351  3.5109 

Coefficients (mean model with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.111504   0.069191 -16.064  &lt; 2e-16 ***
treeBC3F3   -0.050940   0.095889  -0.531  0.59525    
treeD54     -0.279927   0.096470  -2.902  0.00371 ** 
treeD58     -0.034000   0.095716  -0.355  0.72242    
treeEllis1  -0.006764   0.095175  -0.071  0.94334    
treeQing     0.785992   0.094003   8.361  &lt; 2e-16 ***

Phi coefficients (precision model with identity link):
      Estimate Std. Error z value Pr(&gt;|z|)    
(phi)   3.5549     0.1352   26.29   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Type of estimator: ML (maximum likelihood)
Log-likelihood: 529.8 on 7 Df
Pseudo R-squared: 0.1105
Number of iterations: 20 (BFGS) + 2 (Fisher scoring) 

Likelihood ratio test

Model 1: pctrans ~ tree
Model 2: pctrans ~ 1
  #Df LogLik Df  Chisq Pr(&gt;Chisq)    
1   7 529.82                         
2   2 460.70 -5 138.25  &lt; 2.2e-16 ***
</code></pre>

<p>As I've been told, since the model is significantly worse than the null, no comparisons can be made between treatment means.</p>

<p>HOWEVER...
If I run the same model using glm the model is significantly better than the null.</p>

<pre><code>beta.glm &lt;- glm(pctrans ~ tree, data=BT.data, family=quasibinomial)
</code></pre>

<p>Results:</p>

<pre><code>Call:
glm(formula = pctrans ~ tree, family = quasibinomial, data = BT.data)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.94474  -0.38492  -0.08785   0.22725   1.80291  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.22601    0.07643 -16.042  &lt; 2e-16 ***
treeBC3F3    0.06826    0.10660   0.640  0.52205    
treeD54     -0.33864    0.11312  -2.994  0.00281 ** 
treeD58     -0.19878    0.11062  -1.797  0.07260 .  
treeEllis1  -0.07763    0.10808  -0.718  0.47276    
treeQing     0.88596    0.09978   8.879  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasibinomial family taken to be 0.2069603)

    Null deviance: 307.54  on 1240  degrees of freedom
Residual deviance: 267.59  on 1235  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 4

Analysis of Deviance Table
Model: quasibinomial, link: logit
Response: pctrans
Terms added sequentially (first to last)

     Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                  1240     307.54              
tree  5   39.951      1235     267.59 &lt; 2.2e-16 ***
</code></pre>

<p>Where do I go from here?</p>
"
"0.106701449104437","0.105021006302101","221011","<p>I have two monthly time series: </p>

<ul>
<li>one for house prices expressed in annual change growth rates: $\left( \text{ln}(X_t) - \text{ln}(X_{t-12})\right) - \left( \text{ln}(X_{t-1}) - \text{ln}(X_{t-1-12})\right)$;</li>
<li>the other simply in growth rates: $\text{ln}(X_t) - \text{ln}(X_{t-1})$. </li>
</ul>

<p>Here is the data: </p>

<pre><code>House Prices = [1]  0.009189829  0.022612618  0.003952796 -0.015179184  0.001903336 -0.028779902  0.025668239 -0.011237850
  [9]  0.014782630 -0.018844480 -0.023547458  0.020613233  0.029281069 -0.010539781  0.006707366  0.023693144
 [17] -0.002632498  0.148738752 -0.154539337  0.013908319 -0.002294980  0.013274177  0.010043605 -0.007862785
 [25] -0.018297295 -0.003167249  0.022984841  0.001666694 -0.001310199 -0.131548705  0.114723242 -0.003431495
 [33]  0.000953231 -0.010096108 -0.009434595 -0.037774255  0.030877947 -0.011245971 -0.018800312 -0.012805013
 [41]  0.001326392 -0.012034079 -0.045279346 -0.017308170  0.002490863 -0.007340975  0.005052948 -0.024053201
 [49] -0.004190424 -0.028607790  0.004678486  0.026626293 -0.015166864  0.006988983  0.038257855  0.020798177
 [57]  0.008175391  0.021294030 -0.013331432  0.030969145  0.017065249 -0.002672683  0.019435476 -0.037047871
 [65]  0.001844432  0.007663458  0.034406137 -0.049379845 -0.012527106 -0.012859680  0.012954488 -0.015463951
 [73] -0.025509006  0.006318645  0.012977464  0.019940525 -0.025592828  0.020774198 -0.033613414  0.018338077
 [81]  0.001765807  0.009236604 -0.041413104  0.030227358  0.017180849  0.012593360 -0.039001526 -0.004994992
 [89]  0.037766071 -0.043167230 -0.016613786  0.023199890 -0.016214873 -0.012282560  0.065978520 -0.031465767
 [97]  0.006355108 -0.000449523 -0.005810647  0.016823517 -0.021988463  0.026178014  0.007654339 -0.008356379
[105]  0.013273736  0.031645473 -0.046408064  0.022334664  0.008517194 -0.014892335  0.019147342  0.007955040
[113]  0.014122506 -0.035722162  0.018174284  0.021410306 -0.038943797 -0.014517888  0.032750195  0.022506553
[121] -0.003870785  0.130924075 -0.057934974 -0.174228244  0.016937619  0.010647759  0.015691962 -0.033174094
[129]  0.038263205  0.003456250 -0.013422897

B = [1] -0.0223848461  0.0102749646  0.0913403867 -0.0758207770 -0.0053898407 -0.0204047336  0.0050358986
  [8]  0.0195335195 -0.0200303353 -0.0045390828  0.0056380761 -0.0004492945  0.0040043649  0.0012918928
 [15] -0.0104850394  0.0047110190  0.0049805985 -0.0046957178  0.0095002549  0.0202597343 -0.0183526932
 [22]  0.0237185217 -0.0137022065  0.0133787918 -0.0212629487  0.0070512978  0.0959447868 -0.0801519036
 [29] -0.0362526334 -0.0000278572  0.0269014993  0.0009862920 -0.0329868357  0.0283667004 -0.0135186142
 [36] -0.0004975495  0.0053822189  0.0108219907 -0.0078419784  0.0418340658 -0.0316367599 -0.0092324801
 [43] -0.0192830637  0.0336003682  0.0021479539 -0.0146426306  0.0003717930  0.0216259502 -0.0323127786
 [50]  0.0033077606 -0.0123735085 -0.0014757035  0.0266339779 -0.0228959378  0.0002848944  0.0133572802
 [57] -0.0093035312 -0.0034350607  0.0052349772  0.0115210916 -0.0122443122  0.0435497970 -0.0100099291
 [64]  0.0267252321 -0.0654005679  0.0088385287 -0.0089122237  0.0155299273 -0.0027394997 -0.0126183268
 [71]  0.0090999709  0.0017039487 -0.0144843611  0.0269128625  0.0042663583  0.0220574344 -0.0523831016
 [78] -0.0059331639  0.0171559908  0.0125030653  0.0151902738  0.0471484001 -0.0477394702  0.0888317354
 [85] -0.1044700154  0.0234134906 -0.0215966718  0.0157974035  0.0970094980 -0.1049559862 -0.0290578406
 [92]  0.0617653831 -0.0132202439  0.0022117274  0.0091225692  0.0424813190 -0.0614889434  0.0163745828
 [99] -0.0112793057  0.0666179349 -0.0352838073 -0.0259179501  0.0269557599  0.0127882202 -0.0430512536
[106]  0.0862308560 -0.0633012329  0.0596481270  0.0900367605 -0.0303162498 -0.0153738373 -0.0442218848
[113] -0.0116158350 -0.0531058308  0.2036373944  0.1598602057 -0.3837940703 -0.0069112146 -0.0192015196
[120]  0.0110269191 -0.0351135484  0.0439917033  0.0522746614  0.0036354828 -0.0414276671 -0.0361649669
[127]  0.0080753079  0.0352684982 -0.0282391428 -0.0141622744  0.0045799464
</code></pre>

<p>I am studying if <code>B</code> has an effect on <code>House prices</code>. For this reason first a take a simple liner regression between the two and I get a negative and significant estimate at the 95% confidence interval: (-0.0004189 *). </p>

<p>Wanting to reach a step forward I undertake a Granger causality test as following:</p>

<p>I) Determine the optimal number of lags using the AIC/BIC test using:</p>

<pre><code>select.lags&lt;-function(x,y,max.lag=20) {
  y&lt;-as.numeric(y)
  y.lag&lt;-embed(y,max.lag+1)[,-1,drop=FALSE]
  x.lag&lt;-embed(x,max.lag+1)[,-1,drop=FALSE]

  t&lt;-tail(seq_along(y),nrow(y.lag))

  ms=lapply(1:max.lag,function(i) lm(y[t]~y.lag[,1:i]+x.lag[,1:i]))

  pvals&lt;-mapply(function(i) anova(ms[[i]],ms[[i-1]])[2,""Pr(&gt;F)""],max.lag:2)
  ind&lt;-which(pvals&lt;0.05)[1]
  ftest&lt;-ifelse(is.na(ind),1,max.lag-ind+1)

  aic&lt;-as.numeric(lapply(ms,AIC))
  bic&lt;-as.numeric(lapply(ms,BIC))
  structure(list(ic=cbind(aic=aic,bic=bic),pvals=pvals,
                 selection=list(aic=which.min(aic),bic=which.min(bic),ftest=ftest)))
}

s&lt;-select.lags(Topic.15,House.Prices,20)
t(s$selection)
plot.ts(s$ic)
</code></pre>

<p>As a result I get:     </p>

<pre><code>aic bic ftest
14  12  13   
</code></pre>

<p>Here is when it comes the first doubt: why are they giving me different results? Nevertheless, when I do the Granger causality test for both directions using these numbers as possible lags I get in all high significant results (***) only in the direction that <code>B</code> is causing <code>House prices</code> movements:</p>

<pre><code>lmtest::grangertest(Topic.15,House.Prices,12)
lmtest::grangertest(House.Prices,Topic.15,12)
</code></pre>

<p>I do not seem to see the direction of the cause, is it possitive or negative (an increase in <code>B</code> produces an increase or a drop in <code>House prices</code> at time $t+1$?).<br>
Another question, is the conclusion valid that changes in <code>B</code> produce changes in <code>House prices</code>? What are the weakness in this line of argument?</p>
"
"0.154010275908424","0.18190171877725","221510","<p>I'm new to logistic regression analysis, and was unable to find an answer elsewhere in Cross Validated or Stack Overflow. </p>

<p>Consider a standard logistic regression analysis of a binary outcome (admission to college) based on continuous covariates gre score and high school gpa, and ordinal categorical rank prestige of the undergraduate institution (data from the nice UCLA stats dept. logistic regression in R tutorial: <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a>)</p>

<pre><code>&gt; admissions.data &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; admissions.data$rank &lt;- as.factor(admissions.data$rank)
&gt; summary(admissions.data)
     admit             gre             gpa        rank
 Min.   :0.0000   Min.   :220.0   Min.   :2.260   1: 61
 1st Qu.:0.0000   1st Qu.:520.0   1st Qu.:3.130   2:151
 Median :0.0000   Median :580.0   Median :3.395   3:121
 Mean   :0.3175   Mean   :587.7   Mean   :3.390   4: 67
 3rd Qu.:1.0000   3rd Qu.:660.0   3rd Qu.:3.670
 Max.   :1.0000   Max.   :800.0   Max.   :4.000

&gt; fit1 &lt;- glm(admit ~ gre + gpa + rank, data = admissions.data, family=""binomial"")
&gt; summary(fit1)

Call:
glm(formula = admit ~ gre + gpa + rank, family = ""binomial"",
    data = admissions.data)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6268  -0.8662  -0.6388   1.1490   2.0790

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *
gpa          0.804038   0.331819   2.423 0.015388 *
rank2       -0.675443   0.316490  -2.134 0.032829 *
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4

# Odds Ratios
&gt; exp(coef(fit1))
(Intercept)         gre         gpa       rank2       rank3       rank4
  0.0185001   1.0022670   2.2345448   0.5089310   0.2617923   0.2119375

# 95% confidence intervals
&gt; exp(confint(fit1))
Waiting for profiling to be done...
                  2.5 %    97.5 %
(Intercept) 0.001889165 0.1665354
gre         1.000137602 1.0044457
gpa         1.173858216 4.3238349
rank2       0.272289674 0.9448343
rank3       0.131641717 0.5115181
rank4       0.090715546 0.4706961
</code></pre>

<p>My questions are:</p>

<p>1) In R, is there a straight-forward way to determine ORs with 95% CIs for specific values of the covariates? E.g., based on this model, what are the odds of college acceptance for students applying to a rank 2 schools with a gpa of 3 and a gre score of 750, compared with a student applying to a rank 3 school with the same gpa and gre score? I could calculate ORs by hand given the model coefficient estimates and these specific covariate values, but am unsure how to correctly propagate SEs to calculate 95% CIs.</p>

<p>2) Would this particular example be considered a case-control study design, and therefore odds ratios could be estimated, but not predictions? (See: <a href=""http://stats.stackexchange.com/questions/69561/case-control-study-and-logistic-regression"">Case-control study and Logistic regression</a>)</p>
"
"0.0987863462557455","0.113435651621629","223379","<p>I'm fitting an <code>arima</code>(1,0,0) model using the <code>forecast</code> package in R on the <code>usconsumption</code> dataset. However, when I mimic the same fit using <code>lm</code>, I get different coefficients. My understanding is that they should be the same (in fact, they give the same coefficients if I model an <code>arima</code>(0,0,0) and <code>lm</code> with only the external regressor, which is related to this post: <a href=""http://stats.stackexchange.com/questions/28472/regression-with-arima0-0-0-errors-different-from-linear-regression"">Regression with ARIMA(0,0,0) errors different from linear regression</a>). </p>

<p>Is this because <code>arima</code> and <code>lm</code> use different techniques to calculate coefficients? If so, can someone explain the difference?  </p>

<p>Below is my code.</p>

<pre><code>&gt; library(forecast)
&gt; library(fpp)
&gt; 
&gt; #load data
&gt; data(""usconsumption"")
&gt; 
&gt; #create equivalent data frame from time-series
&gt; lagpad &lt;- function(x, k=1) {
+   c(rep(NA, k), x)[1 : length(x)] 
+ }
&gt; usconsumpdf &lt;- as.data.frame(usconsumption)
&gt; usconsumpdf$consumptionLag1 &lt;- lagpad(usconsumpdf$consumption)
&gt; 
&gt; #create arima model
&gt; arima(usconsumption[,1], xreg=usconsumption[,2], order=c(1,0,0))

Call:
arima(x = usconsumption[, 1], order = c(1, 0, 0), xreg = usconsumption[, 2])

Coefficients:
         ar1  intercept  usconsumption[, 2]
      0.2139     0.5867              0.2292
s.e.  0.0928     0.0755              0.0605

sigma^2 estimated as 0.3776:  log likelihood = -152.87,  aic = 313.74
&gt; 
&gt; #create lm model
&gt; lm(consumption~consumptionLag1+income, data=usconsumpdf)

Call:
lm(formula = consumption ~ consumptionLag1 + income, data = usconsumpdf)

Coefficients:
    (Intercept)  consumptionLag1           income  
         0.3779           0.2456           0.2614  
</code></pre>
"
"0.131340404599205","0.14219911474863","223582","<p>I am trying to tie the odds ratio from a 2x2 cross classification table to the intercepts of a logistic regression on those 2 variables. I have a cross classification table that produces 2 odds ratios and the results of a logistic regression of PLACE3 ~ VIOL should produce intecepts should match the odds ratio of the contingency table. i.e. Odds ratio = exp(intercepts)  BUT the POLR package is not producing the correct intercepts.</p>

<p>Here is the data.  In the logistic regression PLACE3 is the outcome and VIOl is the independent variable.   You can see the PLACE3 vs. VIOL contingency table below and the logistic regression of PLACE3 ~ VIOL.  The odds ratios in the contingency table 1.79 and 3.1 are correct but the polr function seems off. Any thoughts on why  exp(summary(m)$zeta) does not produce 1.79 and 3.1?</p>

<p>For reference this is from Lemeshow's Applied Logisitic Regression book page 274.</p>

<pre><code>library(data.table)
aps &lt;- fread('http://www.umass.edu/statdata/statdata/data/aps.dat')
colnames(aps) = c(""ID"",""PLACE"",""PLACE3"",""AGE"",""RACE"",""GENDER"",""NEURO"",""EMOT"",""DANGER"",""ELOPE"",""LOS"",""BEHAV"",""CUSTD"",
                    ""VIOL"")
head(aps)
</code></pre>

<p>Here is  a cross classification table of PLACE3 vs. VIOl variables</p>

<pre><code>table(aps$PLACE3,aps$VIOL) 
      0   1
  0  80 179
  1  26 104
  2  15 104
</code></pre>

<p>using PLACE3 = 0 as the reference the 2 odds ratios from the contingency table are </p>

<pre><code>(104*80)/(179*26)  #1.79
(104*80)/(179*15)  #3.10
</code></pre>

<p>These odds ratios should be the same as exponentiating the slope coefficients  from 
a logistic model  PLACE3 ~ VIOL which is below</p>

<pre><code>aps$constant = rep(1,dim(aps)[1])
m &lt;- polr(as.factor(PLACE3) ~ constant + as.factor(VIOL), data = aps, Hess=TRUE,model=TRUE,method = c(""logistic""))
summary(m)

&gt; summary(m)
Call:
polr(formula = as.factor(PLACE3) ~ constant + as.factor(VIOL), 
    data = aps, Hess = TRUE, model = TRUE, method = c(""logistic""))

Coefficients:
                  Value Std. Error t value
as.factor(VIOL)1 0.8454     0.2112   4.003

Intercepts:
    Value  Std. Error t value
0|1 0.6869 0.1884     3.6464 
1|2 1.8608 0.2032     9.1557 

Residual Deviance: 1031.75 
AIC: 1037.75 
</code></pre>

<p>But you can see the exponentiation of the zeta vector is not 1.79 and 3.10</p>

<pre><code>exp(summary(m)$zeta)

&gt; exp(summary(m)$zeta)
     0|1      1|2 
1.987495 6.429049 
</code></pre>
"
"0.13068205256071","0.128623938856882","225697","<p>Let me give a simple example,</p>

<pre><code>set.seed(100)
disease = sample(c(0,1),100,replace = TRUE)
snp1 = sample(c(""AA"",""AB"",""BB""),100,replace = TRUE)
snp2 = sample(c(""XX"",""XY"",""YY""),100,replace = TRUE)

summary(glm(disease~snp1*snp2, family = binomial))
</code></pre>

<p>output1</p>

<pre><code>Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.55176  -0.94003  -0.00649   0.90052   1.53535  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   -8.109e-01  6.009e-01  -1.349   0.1772  
snp1AB         5.232e-01  9.718e-01   0.538   0.5903  
snp1BB         1.504e+00  8.580e-01   1.753   0.0796 .
snp2XY         4.074e-16  8.498e-01   0.000   1.0000  
snp2YY         1.504e+00  9.280e-01   1.621   0.1051  
snp1AB:snp2XY  1.135e+00  1.335e+00   0.850   0.3952  
snp1BB:snp2XY  1.542e-01  1.254e+00   0.123   0.9022  
snp1AB:snp2YY -1.216e+00  1.333e+00  -0.912   0.3616  
snp1BB:snp2YY -2.785e+00  1.244e+00  -2.239   0.0252 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom
Residual deviance: 127.71  on 91  degrees of freedom
AIC: 145.71
</code></pre>

<p>Output2</p>

<pre><code>snp12 = interaction(snp1,snp2)
summary(glm(disease~snp12, family = binomial))


Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.55176  -0.94003  -0.00649   0.90052   1.53535  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -8.109e-01  6.009e-01  -1.349   0.1772  
snp12AB.XX   5.232e-01  9.718e-01   0.538   0.5903  
snp12BB.XX   1.504e+00  8.580e-01   1.753   0.0796 .
snp12AA.XY  -3.990e-16  8.498e-01   0.000   1.0000  
snp12AB.XY   1.658e+00  9.150e-01   1.812   0.0700 .
snp12BB.XY   1.658e+00  9.150e-01   1.812   0.0700 .
snp12AA.YY   1.504e+00  9.280e-01   1.621   0.1051  
snp12AB.YY   8.109e-01  8.333e-01   0.973   0.3305  
snp12BB.YY   2.231e-01  8.199e-01   0.272   0.7855  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom
Residual deviance: 127.71  on 91  degrees of freedom
AIC: 145.71

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So here I did a logistic regression for interaction between, lets say 2 mutations (each with 3 categories). Like shown above I can do it in 2 ways. My questions are,</p>

<ol>
<li>Are both output1 and output2 same ? </li>
<li>If same, which one is more appropriate?</li>
<li>How to interpret the coefficients (and odds ratios) in each case?</li>
</ol>
"
"NaN","NaN","225908","<p>What is a goodness of fit measure for quantile regression? I'm not too familiar with the theory but in R the AIC and log-likelihood are given in <code>quantreg</code> package. So I can use that to do variable selection.</p>

<p>But if I want to compare two fitting methods, eg whether <code>quantreg</code> does better than <code>quantregForest</code>, what shall I do? Maybe trimmed residual mean square?</p>
"
"0.0435606841869032","0.0428746462856272","226849","<p>How can I find the function 'petest' in R? </p>

<p>A little background, I want to compare two different #regressions, 
y=x1+x2
logy=logx1+logx2</p>

<p>It is clear to me that I cannot use the BIC or AIC values, because the outcome variable is different. I searched in verbeek 2008 (A guide to modern Econometrics) and he suggests the PE test to compare linear and loglinear models.</p>

<p>I searched on this website <a href=""http://artax.karlin.mff.cuni.cz/r-help/library/lmtest/html/petest.html"" rel=""nofollow"">http://artax.karlin.mff.cuni.cz/r-help/library/lmtest/html/petest.html</a> and there I could find that the PE test exist in R, however, I cannot find it. I have the package lmtest, and 'petest' is supposed to be there, but when I try to use it, it says: Error: could not find function ""petest"".</p>

<p>Also, I have looked for more explanation, like examples, or videos that show how to apply this PE model, but I did not find any. Any help on this topic is highly appreciated!</p>
"
"0.157060280430173","0.154586735600211","228316","<p>I want to predict a binary response variable <code>y</code> using logistic regression. <code>x1</code> to <code>x4</code> are the log  of continuous variables and <code>x5</code> to <code>x7</code> are binary variables. </p>

<pre><code>Call:
glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + 
    x6 + x7, family = binomial(), data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.6604  -0.5712   0.4691   0.6242   2.4095  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -2.84633    0.31609  -9.005  &lt; 2e-16 ***
x1             0.14196    0.04828   2.940  0.00328 ** 
x2             4.05937    0.22702  17.881  &lt; 2e-16 ***
x3            -0.83492    0.08330 -10.023  &lt; 2e-16 ***
x4             0.05679    0.02109   2.693  0.00709 ** 
x5             0.08741    0.18955   0.461  0.64467    
x6            -2.21632    0.53202  -4.166  3.1e-05 ***
x7             0.25282    0.15716   1.609  0.10769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1749.5  on 1329  degrees of freedom
Residual deviance: 1110.5  on 1322  degrees of freedom
AIC: 1126.5

Number of Fisher Scoring iterations: 5
</code></pre>

<p>The output of the GLM shows that most of my variables are significant for my model, but the various goodness of fit test I have done:</p>

<pre><code>anova &lt;- anova(model, test = ""Chisq"")   # Anova
1 - pchisq(sum(anova$Deviance, na.rm = TRUE),df = 7) # Null Model vs Most Complex Model
1 - pchisq(model$null.deviance - model$deviance, 
           df = (model$df.null - model$df.residual )) # Null Deviance - Residual Deviance ~ X^2
hoslem.test(model$y, model$fitted.values, g = 8)     # Homer Lemeshow test
pR2(model)                                            # Pseudo-R^2
</code></pre>

<p>tell me that there is a lack of evidence to support my model.</p>

<p>More over, I have a bimodal deviance plot. I suspect the bimodal distribution is caused by the sparsity of my binary variables.
 <a href=""http://i.stack.imgur.com/J27fL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/J27fL.png"" alt=""enter image description here""></a></p>

<p>So I calculated the absolute error <code>abs(y - y_hat)</code>, and obtained the following:</p>

<ul>
<li>77% of my absolute errors were in [0;0.25], which I think is very good!</li>
</ul>

<p>On the following plot, Y=1 is red, and Y=0 is green. This model is better at predicting when Y will be 1 than 0.</p>

<p><a href=""http://i.stack.imgur.com/ZEGuv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZEGuv.png"" alt=""enter image description here""></a></p>

<p>My question is thus the following:</p>

<p>The goodness of fit tests all assume that my null hypothesis follows a Chi square distribution of some sort. Is it correct to conclude that based on my absolute error, my model's prediction is OK, it's just that it doesn't follow a Chi square distribution and thus perform poorly with these tests? </p>
"
"0.11525073729837","0.113435651621629","228985","<p>I have, as the title suggests, two heavily skewed, overdispersed histograms. The data ranges from 0 minutes to 85334 minutes. 90% of the data is below 15 minutes, and takes the form of a positive-skewed exponential/power distribution. Then, there's just a huge tail. There are two groups with similar data structuresâ€”one for <strong>Conversation A</strong> and <strong>Conversations B</strong>. </p>

<p>I'm solid enough with basic statistics to know that comparing the means, STD, p-values, etc. is pretty useless, but I'm not good enough to know <em>how</em> I can compare these two, or what metrics I can compare with one another to see if being in <strong>A</strong> or <strong>B</strong> has any significant effect on the data. I've done some research, and it looks like <em>negative binomial regression</em> fittings will suit my purposes best.</p>

<p>I'm using the <code>MASS</code> package in R, w/ the calls
<code>glm.nb(conversation$A_times ~ 1)</code>:</p>

<pre><code>Coefficients:
(Intercept)
    5.624

Degrees of Freedom: 1674 (i.e. Null); 1674 Residual
Null Deviance:      1850
Residual Deviance:  1850    AIC: 17130
</code></pre>

<p>and <code>glm.nb(conversation$B_times ~ 1)</code>:</p>

<pre><code>Coefficients:
(Intercept)
    4.768

Degrees of Freedom: 1072 (i.e. Null); 1072 Residual
Null Deviance:      1234
Residual Deviance:  1234    AIC: 12390
</code></pre>

<p>Now, I imagine that the goal here is to compare two coefficients (or sets thereof) for significant differences, but I'm not actually sure what to do with this info. What are some directions I can take to learn more and really figure out what I'm doing? </p>
"
"0.0616041103633697","0.0606339062590832","229709","<p>I have several slightly related variables measured in two instruments on the same sample at different time points. I'm trying to know how well the differences between the two instruments can be explained by other variables and the time point.</p>

<p>1.- I'd like to know if my method is adequate: I have chosen to perform a multivariate regression (with lm or glm) that includes a ""day"" as a polinomial variable and the average of both instruments (ex. AVGvar2) and differences between them (ex. DIFvar2) as linear variables. And I perform a StepAIC on the fit. All this for each variable (so that I eventually can say <strong>what variables significantly influde in observed differences</strong> for each variable).</p>

<pre><code>fit &lt;- glm(var1dif~var1+AVGvar2+DIFvar2+AVGvar3+DIFvar3+AVGvar4+DIFvar4+AVGvar5+DIFvar5+day+I(day^2)+I(day^3)+I(day^4)+I(day^5)+I(day^5); step1 &lt;-stepAIC(fit,direction=""both"")
fit2 &lt;- lm(var1dif~var1+AVGvar2+DIFvar2+AVGvar3+DIFvar3+AVGvar4+DIFvar4+AVGvar5+DIFvar5+poly(day,6); step2 &lt;-stepAIC(fit2,direction=""both"")
</code></pre>

<p>(the same would go for DIFvar2, DIFvar3, DIFvar4 and DIFvar5)
Any correction, advice or further step?</p>

<p>2.- When I compare summary(step) and summary(step2) the output (estimates, std error, coefficients) is the same for the variables. The results for variable day differ when the linear model has orthogonal or raw polinomials. Which one is better for my regression?</p>

<p>NOTE: I understand stepwise is frowned upon, but I think for this retrospective analysis it is decent and cost-effective enough.</p>

<p>Thank you.</p>
"
"0.11525073729837","0.113435651621629","229905","<p>I have read <a href=""http://stats.stackexchange.com/questions/145870/how-to-fit-an-exponential-equation-of-the-form-y-a-becx-to-data"">this article</a> and those linked to it, but I am still having difficulties fitting a function of this form to data I have using the nls function in R. Invariably, I fail to get convergence regardless of what starting values I choose for <code>A</code> through <code>C</code>.</p>

<p>Here is a plot of all six of the relationships I would ultimately like to fit:
<a href=""http://i.stack.imgur.com/JKmq3.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JKmq3.jpg"" alt=""enter image description here""></a></p>

<p>While <code>VEG.STM3</code> and <code>PROP.RIPE</code> probably will conform to a curve of this shape fairly well, the others probably won't, and that's fine. I still have a justifiable reason for trying it.</p>

<p>My questions are as follows:</p>

<ol>
<li>What are the parameters <code>A</code>, <code>B</code>, and <code>C</code> doing, mathematically, in a function of this form? <code>A</code> appears to be the Y asymptote, <code>B</code> seems to be a scalar of some kind, and <code>C</code> seems to control the rate of decay, but, beyond that, I can't seem to figure out what a reasonable range of values for each parameter should be, especially from one set of data to the next.</li>
<li>Is there a way to ""linearize"" this problem so that <code>lm</code> can be used in place of <code>nls</code>? My understanding is that this function is equivalent to <code>ln(Y) ~ ln(A) + ln(B) + CX</code>, but I don't understand how to fit that equation any more than I understand how to fit this current curve. </li>
<li>Is there any way to get <code>nls</code> to be less fussy and more robust to lousy starting guesses?</li>
<li>Once I get <code>nls</code> to run successfully, how can I compare the fit of the model to one fit by simple linear regression? Would use of AICc be appropriate in that case? </li>
</ol>

<p>Here is some data to work with:</p>

<pre><code>PROP.RIPE = c(0.37, 0.223, 0.223, 0.224, 0.388, 0.413, 0.406, 0.422, 0.554, 
0.453, 0.569, 0.511, 0.13, 0.166, 0.16, 0.216, 0.297, 0.344, 
0.339, 0.292, 0.601, 0.535, 0.65, 0.535, 0.269, 0.238, 0.334, 
0.272, 0.523, 0.358, 0.449, 0.393, 0.458, 0.426, 0.576, 0.468, 
0.581, 0.579, 0.527, 0.568, 0.348, 0.313, 0.317, 0.267, 0.623, 
0.527, 0.646, 0.589, 0.488, 0.444, 0.498, 0.449, 0.109, 0.103, 
0.171, 0.153, 0.505, 0.343, 0.345, 0.213, 0.029, 0.011, 0.071, 
0.013, 0.697, 0.604, 0.624, 0.639, 0.386, 0.508, 0.38, 0.471, 
0.618, 0.488, 0.513, 0.485, 0.602, 0.597, 0.625, 0.495, 0.318, 
0.457, 0.423, 0.547, 0.88, 0.949, 0.912, 0.771, 0.628, 0.635, 
0.486, 0.567, 0.621, 0.549, 0.698, 0.709, 0.541, 0.563, 0.789, 
0.692, 0.525, 0.395, 0.449, 0.597, 0.57, 0.487, 0.556, 0.546, 
0.495, 0.617, 0.754, 0.71, 0.585, 0.719, 0.508, 0.536, 0.592, 
0.472, 0.481, 0.658, 0.937, 0.853, 0.981, 0.887)

NODES0 = c(124L, 362L, 198L, 343L, 152L, 193L, 98L, 167L, 148L, 284L, 
113L, 137L, 227L, 323L, 156L, 362L, 166L, 327L, 137L, 312L, 166L, 
350L, 97L, 222L, 182L, 456L, 143L, 277L, 172L, 272L, 110L, 184L, 
138L, 288L, 102L, 124L, 236L, 280L, 159L, 127L, 104L, 176L, 93L, 
167L, 178L, 400L, 126L, 248L, 189L, 336L, 181L, 304L, 245L, 283L, 
151L, 327L, 116L, 179L, 144L, 177L, 397L, 642L, 322L, 443L, 125L, 
249L, 100L, 144L, 56L, 22L, 23L, 17L, 252L, 387L, 184L, 308L, 
115L, 267L, 82L, 157L, 223L, 226L, 79L, 73L, 101L, 139L, 104L, 
60L, 200L, 164L, 66L, 49L, 173L, 204L, 64L, 107L, 435L, 215L, 
51L, 129L, 392L, 550L, 174L, 178L, 276L, 204L, 98L, 74L, 421L, 
303L, 126L, 150L, 168L, 195L, 77L, 75L, 72L, 142L, 59L, 47L, 
391L, 479L, 109L, 111L)

NODE.SUCCESS = c(1.05, 0.79, 0.86, 0.69, 0.85, 0.8, 0.77, 0.84, 0.53, 0.88, 
0.88, 0.79, 0.85, 0.88, 1, 1.03, 0.95, 0.77, 0.92, 0.73, 0.98, 
0.92, 0.97, 0.99, 0.88, 0.82, 0.84, 0.78, 0.63, 0.54, 0.54, 0.47, 
1.09, 0.88, 0.95, 0.99, 0.96, 1.13, 0.91, 1.01, 0.81, 0.89, 0.99, 
0.85, 0.95, 0.65, 0.87, 0.73, 0.64, 0.82, 0.82, 0.75, 0.93, 1.06, 
0.94, 0.89, 0.65, 0.53, 0.6, 0.62, 0.91, 0.89, 0.93, 1.13, 0.81, 
0.63, 0.75, 0.67, 0.93, 0.82, 0.7, 0.88, 0.8, 0.96, 0.9, 0.94, 
0.58, 0.66, 0.65, 0.63, 0.66, 0.62, 0.66, 0.77, 0.51, 0.6, 0.47, 
0.9, 0.69, 0.73, 0.59, 0.63, 0.97, 0.93, 0.95, 0.91, 0.81, 0.92, 
0.88, 1.1, 0.56, 0.57, 0.44, 0.51, 0.85, 0.83, 0.96, 0.85, 1, 
0.97, 0.94, 0.95, 0.61, 0.71, 0.73, 0.8, 1.06, 0.96, 0.9, 0.91, 
0.45, 0.41, 1.3, 0.55)
</code></pre>
"
"0.0308020551816849","0.0606339062590832","230797","<p>I am new to R coding and was hoping someone could help. Am trying to create a regression line where the dependent variable is a proportion (I only have the proportion, not the denominator and numerator). With it being a proportion a linear regression line isn't appropriate as it needs to sit between 0-1, I think a sigmoid shape would be best. So far I have had limited success with the Loess function, however ideally I want to be able to gain the coefficients and AIC from the regression. So far the best shape I have been able to obtain is using the binomial family in ggplot2, but I don't think this is an appropriate distribution.</p>

<p>I have attached my code, am wondering if anyone could suggest an improvement.</p>

<pre><code> c &lt;- ggplot(dat, aes(y=ITN_Coverage, x=Study_Date))
c + stat_smooth(method =""glm"",  method.args = list(family=""binomial""), size=0.5, col = ""black"") + geom_point(aes(color = Country)) + 
labs(title = ""Scatter plot: Insecticide treated net coverage against year"", x= ""Study date"", y= ""ITN coverage"")
</code></pre>
"
"0.151347073206507","0.160422236979937","231059","<p>So first of all I did some research on this forum, and I know <a href=""http://stats.stackexchange.com/questions/140991/comparing-difference-between-two-polynomial-regression-models-in-r"">extremely similar</a>  questions have been asked but they usually haven't been answered properly or sometimes the answer are simply not detailed enough for me to understand. So this time my question is : I have two sets of data, on each, I do a polynomial regression like so :</p>

<pre><code>Ratio&lt;-(mydata2[,c(2)])
Time_in_days&lt;-(mydata2[,c(1)])
fit3IRC &lt;- lm( Ratio~(poly(Time_in_days,2)) )
</code></pre>

<p>The polynomial regressions plots are:</p>

<p><a href=""http://i.stack.imgur.com/T7r3i.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/T7r3i.png"" alt=""enter image description here""></a></p>

<p>The coefficients are :</p>

<pre><code>&gt; as.vector(coef(fit3CN))
[1] -0.9751726 -4.0876782  0.6860041
&gt; as.vector(coef(fit3IRC))
[1] -1.1446297 -5.4449486  0.5883757 
</code></pre>

<p>And now I want to know, if there is a way to use an R function to do a test that would tell me whether or not there is a statistical significance in the difference between the two polynomials regression knowing that the relevant interval of days is [1,100].</p>

<p>From what I understood I can not apply directly the anova test because the values come from two different sets of data nor the AIC, which is used to compare model/true data.</p>

<p>I tried to follow the instructions given by @Roland in the related question but I probably misunderstood something when looking at my results :</p>

<p>Here is what I did : </p>

<p>I combined both my datasets into one.</p>

<p><code>f</code> is the variable factor that @Roland talked about. I put 1s for the first set and 0s for the other one.</p>

<pre><code>y&lt;-(mydata2[,c(2)])
x&lt;-(mydata2[,c(1)])
f&lt;-(mydata2[,c(3)])

plot(x,y, xlim=c(1,nrow(mydata2)),type='p')

fit3ANOVA &lt;- lm( y~(poly(x,2)) )

fit3ANOVACN &lt;- lm( y~f*(poly(x,2)) )
</code></pre>

<p>My data looks like this now :</p>

<p><a href=""http://i.stack.imgur.com/dNpMQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dNpMQ.png"" alt=""enter image description here""></a></p>

<p>The red one is <code>fit3ANOVA</code> which is still working but I have a problem with the blue one <code>fit3ANOVACN</code> the model has weird results. I don't know if the fit model is correct, I do not understand what @Roland meant exactly.</p>

<p>Considering @DeltaIV solution I suppose that in that case :
<a href=""http://i.stack.imgur.com/HLLp9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HLLp9.png"" alt=""enter image description here""></a>
The models are significantly different even though they overlap. Am I right to assume so ?</p>
"
"0.261459615486428","0.257341883884529","231872","<p>For a better understanding of how r is conducting a logistic regression I created the following test-data (the two predictors and the criterion are binary variables):</p>

<pre><code>   UV1 UV2 AV
1    1   1  1
2    1   1  1
3    1   1  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   0  1
9    0   0  1
10   0   0  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>AV = dependent variable/criterion</p>

<p>UV1 / UV2 = both independant variables/predictors</p>

<p>For measuring the UVs effect on the AV a logistic regression is necessary, as the AV is a binary variable. Hence i used the following code</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata, family = ""binomial"")
</code></pre>

<p>including <strong>""family = ""binomial""""</strong>. Is this correct ( I think so :-))?</p>

<p>Regarding my test-data, I was wondering about the whole model, especially
the estimators and sigificance:</p>

<pre><code>&gt; summary(lrmodel)


Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7344  -0.2944   0.3544   0.7090   1.1774  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.065e-15  8.165e-01   0.000    1.000
UV1         -1.857e+01  2.917e+03  -0.006    0.995
UV2          1.982e+01  2.917e+03   0.007    0.995

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 17.852  on 17  degrees of freedom
AIC: 23.852

Number of Fisher Scoring iterations: 17
</code></pre>

<ol>
<li><p>Why is UV2 not significant. See therefore that for group AV = 1 there are 7 cases with UV2 = 1, and for group AV = 0 there are only 3 cases with UV2 = 1. 
I was expecting that UV2 is a significant discriminator.</p></li>
<li><p>Despite the not-significance of the UVs, the estimators are - in my opinion- very high (e.g. for UV2 = 1.982e+01). How is this possible?</p></li>
<li><p>Why isn't the intercept 0,5?? We have 5 cases with AV = 1 and 5 cases with AV = 0.</p></li>
</ol>

<p>Further: I created UV1 as a predictor I expected not to be significant:  for group AV = 1 there are 5 cases withe UV1 = 1, and for group AV = 0 there are 5 cases withe UV1 = 1 as well.</p>

<p>The whole ""picture"" I gained from the logistic is confusing me...</p>

<p>What was consuming me more:
When I run a ""NOT-logistic"" regression (by omitting <strong>""family = ""binomial""</strong>)</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata,)
</code></pre>

<p>I get the expected results</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7778  -0.1250   0.1111   0.2222   0.5000  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   0.5000     0.1731   2.889  0.01020 * 
UV1          -0.5000     0.2567  -1.948  0.06816 . 
UV2           0.7778     0.2365   3.289  0.00433 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for gaussian family taken to be 0.1797386)

    Null deviance: 5.0000  on 19  degrees of freedom
Residual deviance: 3.0556  on 17  degrees of freedom
AIC: 27.182

Number of Fisher Scoring iterations: 2
</code></pre>

<ol>
<li>UV1 is not significant! :-)</li>
<li>UV2 has an positive effect on AV = 1! :-)</li>
<li>The intercept is 0.5! :-)</li>
</ol>

<p>My overall question: Why isn't logistic regression (including ""family = ""binomial"") producing results as expected, but a ""NOT-logistic"" regression (not including ""family = ""binomial"") does?</p>

<p>Update:
are the observations described above because of the correlation of UV1 and UV 2. Corr = 0.56
After manipulating the UV2's data </p>

<p>AV: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>UV1: 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0</p>

<p>UV2: <strong>0, 0, 0,</strong> 1, 1, 1, 1, <strong>1, 1, 1</strong>, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>(I changed the positions of the three 0s with the three 1s in UV2 to gain a correlation &lt; 0.1 between UV1 and UV2) hence:</p>

<pre><code>UV1 UV2 AV
1    1   0  1
2    1   0  1
3    1   0  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   1  1
9    0   1  1
10   0   1  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>to avoid correlation, my results come closer to my expectations:</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.76465  -0.81583  -0.03095   0.74994   1.58873  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.1248     1.0862  -1.036   0.3004  
UV1           0.1955     1.1393   0.172   0.8637  
UV2           2.2495     1.0566   2.129   0.0333 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 22.396  on 17  degrees of freedom
AIC: 28.396

Number of Fisher Scoring iterations: 4
</code></pre>

<p>But why does the correlation influence the results of the logistic regression and not the results of the ""not-logistic"" regression? </p>
"
"0.0435606841869032","0.0428746462856272","232825","<p>I have three regression binomial models in R (GLMM) for which I have the following anova table.</p>

<p><a href=""http://i.stack.imgur.com/iA3qR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iA3qR.png"" alt=""anovaII""></a></p>

<p>Now, which is the model of the best fit based on the metrics in the table? Is it model 2 since it has the lowest BIC value? </p>

<p>I read somewhere that the best fitting model should have a lower residual deviance, AIC and BIC values than other models. The p-value threshold here is p&lt;0.5.  </p>

<p>In addition, how can I calculate the Chisq,Chi Df and p-value of model2 in R since these are not provided in the first row? </p>
"
"0.0616041103633697","0.0606339062590832","233063","<p>I have created a logistic regression in R and would like to use the trained model to create an predict function (lets say in Excel).  How can I convert the coefficients into a predict equation?</p>

<pre><code>glm(formula = is_bad ~ is_rent + dti + bc_util + open_acc +    pub_rec_bankruptcies + 
chargeoff_within_12_mths, family = binomial, data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8659  -0.5413  -0.4874  -0.4322   2.4289  

Coefficients:
                            Estimate Std. Error  z value Pr(&gt;|z|)    
(Intercept)              -2.9020574  0.0270641 -107.229  &lt; 2e-16 ***
is_rentTRUE               0.3105513  0.0128643   24.141  &lt; 2e-16 ***
dti                       0.0241821  0.0008331   29.025  &lt; 2e-16 ***
bc_util                   0.0044706  0.0002561   17.458  &lt; 2e-16 ***
open_acc                  0.0030552  0.0012694    2.407   0.0161 *  
pub_rec_bankruptcies      0.1117733  0.0163319    6.844 7.71e-12 ***
chargeoff_within_12_mths -0.0268015  0.0564621   -0.475   0.6350    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 173006  on 233017  degrees of freedom
Residual deviance: 170914  on 233011  degrees of freedom
(2613 observations deleted due to missingness)
AIC: 170928

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.152462394654161","0.160779923571102","234077","<p>I'm trying to analyze some count data for a few species ('Tetab' indicates the species in the below code). Consulting with a friend who is much more stats literate than I, he suggested analyzing the data with a Poisson regression, and then utilizing confidence intervals to determine which treatments resulted in significantly different count responses. This worked fine for the other two species I analyzed, but I'm getting the error listed in the title. Comparing the code, everything's the same among the different species' analyses, so I'm assuming it has something to do with the data - also because this species is the only one where the zero-inflated poisson regression can't be run. The total count data for the other two species is 33 and 47, but only 22 for Tetab. Could this be related to the error? Is there any workaround for this? The data is heterogenous for variances, so I can't utilize Kruskal-Wallis or multiple comparisons.</p>

<pre><code>&gt; Tetab.pglm &lt;- glm(Count ~ Treatment, data = spond.spp.list[['Tetab']], family = poisson)
&gt; Tetab.zpglm &lt;- zeroinfl(Count ~ Treatment, data = spond.spp.list[['Tetab']], dist = ""poisson"")
Error in solve.default(as.matrix(fit$hessian)) : 
  system is computationally singular: reciprocal condition number = 1.63511e-19

&gt; summary(Tetab.pglm)

Call:
glm(formula = Count ~ Treatment, family = poisson, data = spond.spp.list[[""Tetab""]])

Deviance Residuals: 
 Min        1Q    Median        3Q       Max  
-1.35873  -0.00006  -0.00006   0.07899   2.36154  

Coefficients:
          Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -2.030e+01  4.311e+03  -0.005    0.996
Treatment2   2.004e+01  4.311e+03   0.005    0.996
Treatment3   9.922e-09  6.096e+03   0.000    1.000
Treatment4   2.022e+01  4.311e+03   0.005    0.996

(Dispersion parameter for poisson family taken to be 1)

Null deviance: 54.484  on 51  degrees of freedom
Residual deviance: 23.804  on 48  degrees of freedom
AIC: 68.297

Number of Fisher Scoring iterations: 18

&gt; exp(coef(Tetab.zpglm))
Error in coef(Tetab.zpglm) : object 'Tetab.zpglm' not found
&gt; exp(coef(Tetab.pglm))
 (Intercept)   Treatment2   Treatment3   Treatment4 
1.522998e-09 5.050767e+08 1.000000e+00 6.060920e+08 
&gt; exp(confint(Tetab.zpglm))
Error in confint(Tetab.zpglm) : object 'Tetab.zpglm' not found
&gt; exp(confint(Tetab.pglm))
Waiting for profiling to be done...
Error: no valid set of coefficients has been found: please supply starting values
In addition: Warning messages:
1: glm.fit: fitted rates numerically 0 occurred 
2: glm.fit: fitted rates numerically 0 occurred 
3: glm.fit: fitted rates numerically 0 occurred 
4: glm.fit: fitted rates numerically 0 occurred 
5: glm.fit: fitted rates numerically 0 occurred 
6: glm.fit: fitted rates numerically 0 occurred 
7: glm.fit: fitted rates numerically 0 occurred 
8: glm.fit: fitted rates numerically 0 occurred 
9: glm.fit: fitted rates numerically 0 occurred 
</code></pre>

<p>Thanks for any help you can provide!
Max</p>

<p>Here's the data set:</p>

<pre><code>Species Date    Site    Treatment   Count
Tetab   20160602    2   1   0
Tetab   20160602    2   2   1
Tetab   20160602    2   3   0
Tetab   20160602    2   4   1
Tetab   20160606    1   1   0
Tetab   20160606    1   2   1
Tetab   20160606    1   3   0
Tetab   20160606    1   4   0
Tetab   20160606    2   1   0
Tetab   20160606    2   2   1
Tetab   20160606    2   3   0
Tetab   20160606    2   4   0
Tetab   20160607    2   1   0
Tetab   20160607    2   2   0
Tetab   20160607    2   3   0
Tetab   20160607    2   4   1
Tetab   20160609    1   1   0
Tetab   20160609    1   2   0
Tetab   20160609    1   3   0
Tetab   20160609    1   4   2
Tetab   20160609    2   1   0
Tetab   20160609    2   2   0
Tetab   20160609    2   3   0
Tetab   20160609    2   4   1
Tetab   20160610    1   1   0
Tetab   20160610    1   2   1
Tetab   20160610    1   3   0
Tetab   20160610    1   4   0
Tetab   20160610    2   1   0
Tetab   20160610    2   2   1
Tetab   20160610    2   3   0
Tetab   20160610    2   4   0
Tetab   20160620    1   1   0
Tetab   20160620    1   2   1
Tetab   20160620    1   3   0
Tetab   20160620    1   4   1
Tetab   20160620    2   1   0
Tetab   20160620    2   2   1
Tetab   20160620    2   3   0
Tetab   20160620    2   4   4
Tetab   20160622    1   1   0
Tetab   20160622    1   2   0
Tetab   20160622    1   3   0
Tetab   20160622    1   4   1
Tetab   20160622    2   1   0
Tetab   20160622    2   2   2
Tetab   20160622    2   3   0
Tetab   20160622    2   4   1
Tetab   20160624    2   1   0
Tetab   20160624    2   2   1
Tetab   20160624    2   3   0
Tetab   20160624    2   4   0
</code></pre>
"
