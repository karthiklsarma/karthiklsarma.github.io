"V1","V2","V3","V4"
"NaN","NaN","  5090","<p>The R package <a href=""http://cran.r-project.org/web/packages/dlm/index.html"" rel=""nofollow"">dlm</a> implements filtering and smoothing (<code>dlmFilter</code> and <code>dlmSmooth</code>) for models with regression effects, but forecasting is not (yet) available for these models:</p>

<pre><code>mod &lt;- dlmModSeas(4)+dlmModReg(cbind(rnorm(100),rnorm(100)))
fi &lt;- dlmFilter(rnorm(100),mod)
f &lt;- dlmForecast(fi,nAhead=12)
Error in dlmForecast(fi, nAhead = 12): 
  dlmForecast only works with constant models
</code></pre>

<p>How can I do this in R?</p>

<p>Thanks for your help!</p>
"
"0.101015254455221","0.104828483672192"," 13069","<p>I am very interested about the potential of statistical analysis for simulation/forecasting/function estimation, etc. </p>

<p>However, I don't know much about it and my mathematical knowledge is still quite limited -- I am a junior undergraduate student in software engineering. </p>

<p>I am looking for a book that would get me started on certain things which I keep reading about: linear regression and other kinds of regression, bayesian methods, monte carlo methods, machine learning, etc.
I also want to get started with R so if there was a book that combined both, that would be awesome. </p>

<p>Preferably, I would like the book to explain things conceptually and not in too much technical details -- I would like statistics to be very intuitive to me, because I understand there are very many risky pitfalls in statistics. </p>

<p>I am off course willing to read more books to improve my understanding of topics which I deem valuable.</p>
"
"0.484452141651805","0.459023246837373"," 29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"0.142857142857143","0.14824986333222"," 32657","<p>I was playing with the <a href=""http://cran.r-project.org/web/packages/TSA/index.html"" rel=""nofollow"">TSA</a> package in R and wanted to test the <code>arimax</code> function to the solution provided in Pankratz's <em>Forecasting with Dynamic Regression Models</em>, chapter 8. The savings rate and the function seems to provide similar results as the ones in the book except for the IO weights which are quite different. I bet there is a transformation that I might be missing.</p>

<p>Any help on understanding why IO coefficients are so different would be appreciated...</p>

<p>the solution states </p>

<pre><code>AO @ t=82,43,89
LS @ t=99
IO @ t=62,55
</code></pre>

<p>with Parameters estimates</p>

<pre><code>C = 6.1635
w82 = 2.3346
w99 = -1.5114
w43 = 1.1378
w62 = 1.4574
w55 = -1.4915
w89 = -1.0702
AR1 = 0.7976
MA2 = -0.3762
</code></pre>

<p>To fit the model in R, I used
(<code>saving</code> is the data)</p>

<pre><code>arimax(saving, order = c(1,0,2), fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA), io=c(55,62), 
       xreg=data.frame(AO82=1*(seq(saving)==82),
                       AO43=1*(seq(saving)==43),
                       AO89=1*(seq(saving)==89),
                       LS99=1*(seq(saving)&gt;=99)),
       method='ML')
</code></pre>

<p>The savings rate data is (100 points)</p>

<p>4.9
5.2
5.7
5.7
6.2
6.7
6.9
7.1
6.6
7
6.9
6.4
6.6
6.4
7
7.3
6
6.3
4.8
5.3
5.4
4.7
4.9
4.4
5.1
5.3
6
5.9
5.9
5.6
5.3
4.5
4.7
4.6
4.3
5
5.2
6.2
5.8
6.7
5.7
6.1
7.2
6.5
6.1
6.3
6.4
7
7.6
7.2
7.5
7.8
7.2
7.5
5.6
5.7
4.9
5.1
6.2
6
6.1
7.5
7.8
8
8
8.1
7.6
7.1
6.6
5.6
5.9
6.6
6.8
7.8
7.9
8.7
7.7
7.3
6.7
7.5
6.4
9.7
7.5
7.1
6.4
6
5.7
5
4.2
5.1
5.4
5.1
5.3
5
4.8
4.7
5
5.4
4.3
3.5</p>

<p>here it is my output</p>

<pre><code>&gt; arimax(saving, order = c(1,0,2),fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA),io=c(55,62),xreg=data.frame(AO82=1*(seq(saving)==82),
+ AO43=1*(seq(saving)==43),AO89=1*(seq(saving)==89),LS99=1*(seq(saving)&gt;=99)),method='ML')

Call:
arimax(x = saving, order = c(1, 0, 2), xreg = data.frame(AO82 = 1 * (seq(saving) == 
    82), AO43 = 1 * (seq(saving) == 43), AO89 = 1 * (seq(saving) == 
    89), LS99 = 1 * (seq(saving) &gt;= 99)), fixed = c(NA, 0, NA, NA, NA, NA, 
    NA, NA, NA, NA), method = ""ML"", io = c(55, 62))

Coefficients:
         ar1  ma1     ma2  intercept    AO82    AO43     AO89     LS99    IO-55   IO-62
      0.7918    0  0.3406     6.0628  2.3800  1.1297  -1.0466  -1.4885  -0.5958  0.5517
s.e.  0.0674    0  0.1060     0.3209  0.3969  0.3780   0.3835   0.5150   0.4044  0.3772

sigma^2 estimated as 0.2611:  log likelihood = -75.57,  aic = 169.14
</code></pre>
"
"0.428571428571429","0.395332968885921"," 35489","<p>I have real daily market data which I'm looking at to create a model for forecasting. The model that I created (below) used autoregressive terms within a linear regression.</p>

<p>I was sharing this with a colleague and he said ""autoregressive variables are correlated with the other variables in multiple linear setting which creates multicollinarity problem, creating unreliable result.""</p>

<p>So I'm turning to the group for help. Here is the data and the analysis that I performed in R.</p>

<pre><code>#Read in Data
MarketData = read.table('http://sharp-waterfall-3397.herokuapp.com/MarketCategories6.txt', header=TRUE,na.strings = ""NA"", sep="","")
MarketData$Month &lt;- as.factor(MarketData$Month)
MarketData$Weekday &lt;- as.factor(MarketData$Weekday)

str(MarketData)
</code></pre>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/PERregress/index.html"" rel=""nofollow"">PERregress</a> library to help with the autoregression using the <code>back()</code> function and to help with the residual diagnostics:</p>

<pre><code>library(PERregress)
descStat(MarketData)
</code></pre>

<p>Subsetting the data for model building and prediction purposes:</p>

<pre><code>Total = MarketData
MarketData = MarketData[1:268,]
attach(MarketData)
</code></pre>

<p>Here is a regression with everything that I can think of. Note you can have higher autoregressive terms but this will start to mask events since R will ignore the first several rows. Also just an FYI for some reason the residual analysis is breaking which I liked to look for points with undue leverage.</p>

<pre><code>#Market1Category1 Regression for the markets with everything that I can think of it
Market1Category1Output=lm(Market1Category1 ~ Trend+Month2+Month3+Month4+
                          Month5+Month6+Month7+Month8+Month9+Monday+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday2+Holiday3+Holiday4+
                          Event1+Event2+Event3+Event4+Event5+Event6+Event7+
                          Event8+Event9+Event10+Event11+Event12+Event13+
                          Event14+Event15+Event16+Event17+Event18+Event19+
                          Event20+Event21+Event22+Event23+Event24+Event25+
                          Event26+Event27+Event28+
                          back(Market1Category1)+back(Market1Category1, 2))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is the final equation. I'd like to say that I reduced the variables using partial f-test but I couldn't find an easy way to do this so if you know a function please let me know. Basically I looked at the change in adjusted $R^2$.</p>

<pre><code>#Final regression equation 
Market1Category1Output=lm(Market1Category1 ~ Month5+Month6+Month7+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday3+Event2+Event7+Event10+
                          Event13+Event16+Event25+Event28+
                          back(Market1Category1)+back(Market1Category1, 6))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is a plot of the actuals in green vs the predictions in blue but there's a problem:</p>

<pre><code>plot(Time, Market1Category1, col='green')
points(Time, predict(Market1Category1Output, MarketData), col='blue', pch=20)
</code></pre>

<p>The issue is that predict will use the data values instead of it's predicted values for the autoregressive terms. In order to make it use predicted terms I created this loop. If you know a better way let me know.</p>

<pre><code>dataSet2 &lt;- Total
dataSet2[8:length(dataSet2$Time),""Market1Category1""] &lt;- NA
    for (i in (1:(length(dataSet2$Time)-7))) {
  dataSet2[6+i+1,""Market1Category1""] &lt;- 1
  dataSet2[6+i+1,""Market1Category1""] &lt;- predict(Market1Category1Output, 
                                                dataSet2[0:6+i+1,])[6+1] 
}
</code></pre>

<p>Here is the plot again with the results in blue using the predicted results for the autoregressive terms (with the exception of the first 7 since the model needs those to <code>predict</code>):</p>

<pre><code>plot(Total$Time, Total$Market1Category1, col='green')
points(dataSet2$Time, dataSet2$Market1Category1, col='blue', pch=20)
</code></pre>

<p>So here are my questions in order of importance:</p>

<ol>
<li>Does using autoregressive and linear terms violate any fundamental assumptions?</li>
<li>What issues can this cause and what analysis/steps should I do take to avoid these problems?</li>
<li>Is there a better approach to modeling this timeseries?</li>
<li>Is there a more efficient approach?</li>
<li>Given the residuals what steps would you take?</li>
</ol>

<p>Finally two questions which is just causing me more work than possibly necessary:</p>

<ol>
<li>As you can see instead of using the factors for weekday and month I'm using separate conditional variables. I'm doing this because if I use the factor and a level turns out to be insignificant (e.g., Monday for days of the week). I can't remove it. Perhaps there's a way?</li>
<li>Is there a quick way to run a partial F-statistic to understand whether removing a variable makes sense?</li>
</ol>
"
"NaN","NaN"," 51116","<p>I'm trying to adopt a solution for long-term electricity annual prices forecasting (depending on past electricity prices, past oil prices, past consumption data, etc.)</p>

<p>I'm considering some solutions:</p>

<ul>
<li>off-the-shelf specific software: Alyuda, Aleasoft, ...</li>
<li>already-built modules of known software (Excel, MatLab, R): neuroXL for Excel, modules for Simulink, ...</li>
<li>code from scratch (Python, R) by using known models and Machine Learning: scikit-learn, Weka, regression, ...</li>
</ul>

<p>I would like to find more solutions to test and, if possible, some experiences using them.</p>

<p>Thank you very much in advance.</p>
"
"0.174963553055941","0.181568259800641"," 60200","<p>I am beginner in forecasting, especially forecasting with R and I am really willing to improve my knowledge. </p>

<p>Recently, I started practicing electricity consumption time series forecasting. </p>

<p>The first barrier I faced is the choice of out of sample data for assessing the forecast accuracy of the forecast model i will be using (regression with ARIMA errors). </p>

<p>I have data for 147 months and I want to forecast the next 24 months, for the period June 2013 to January 2015. Furthermore, I have read in <a href=""http://stats.stackexchange.com/users/159/rob-hyndman"">@RobHyndman</a>'s online text book  </p>

<p>HynÂ­dÂ­man, R.J. and AthanaÂ­sopouÂ­los, G. (2013),<br>
<em>Forecasting: principles and practice</em>,<br>
(accessed 28 May 2013),  <a href=""http://otexts.com/fpp/2/5/"" rel=""nofollow"">section 2.5</a><br>
under '<strong>Training and test sets</strong>', that:</p>

<blockquote>
  <p>size of the test set is typÂ­iÂ­cally about 20% of the total samÂ­ple, although this value depends on how long the samÂ­ple is and how far ahead you want to foreÂ­cast</p>
</blockquote>

<p>If I divide the dataset with 20% of it being the out-of-sample data, the forecast model applied to the in-sample data is not quite accurate, since I guess it fails to capture the recent trend, (which began in the middle of the last year), of decreased electricity consumption due to a significantly raised electricity tariff. </p>

<p>What do I do about this?</p>

<p>Can you possibly give me instruction on what would be considered appropriate size of the out of sample data. I also tried with 7 months of out-of-sample data, but I am afraid that there might be an overfitting issue. Is that right?</p>
"
"0.101015254455221","0.104828483672192"," 63883","<p>Is there a method to find the right distance function in non-parametric regression?
I use some time series to learn forecasting. Series are nonlinear and non-gaussian.
I can get the right dimension and delay. I can find the right bandwidth with the hdrcde library.
I have no problem with kernel functions.<br>
My problem is with distances. I use Euclidian, Cosine and Correlation weighting functions.
These are the kernel which give good results, but one time, Euclidian is good, then after adding some data, one to 5-7 generally, Euclidian give nothing, even with very little change in statistics.
So, my question is if there is a method to choose the right distance. I would like to have advices on that point and on articles that will help solve this problem. What package in R could eventually help?</p>

<p>Thank you. </p>
"
"0.267261241912424","0.237728655525098"," 69371","<p>I have a set of predictors in a linear regression, as well as three control variables. The issue here is that one of my variables of interest is only statistically significant if the control variables are included in the final model. However, the control variables themselves are not statistically significant.</p>

<p>Here is how the multicollinearity of all my variables look like (including control variables):</p>

<pre><code> &gt; vif(lm(return ~ EQ + EFF + SIZE + MOM + MSCR + UMP, data = as.data.frame(port.df)))
       EQ      EFF     SIZE      MOM     MSCR      UMP 
 3.687171 3.481672 2.781901 1.064312 1.438596 1.003408

 &gt; vif(lm(return ~ EQ + MOM + MSCR, data = as.data.frame(port.df)))
       EQ      MOM     MSCR 
 1.359992 1.048142 1.412658 
</code></pre>

<p>My variables of interest are <strong>EQ, MOM and MSCR</strong>, and the control variables are <em>EFF, SIZE and UMP</em>. EQ is only significant if the three control var are included, and becomes insignificant when they are not:</p>

<ul>
<li><p>Here are the coefficients (1rst row) and t-stats (2nd row) when control variables are included (notice that EQ is statistically significant)</p>

<pre><code>       intercept           EQ          EFF        SIZE         MOM       MSCR          UMP
[1,] 0.005206246 -0.006310531 0.0001229055 0.004125551 0.007738259 0.00473377 5.838596e-06
[2,] 1.866628909 -1.746583234 0.0388823612 1.178460997 2.145062820 2.08131100 1.994863e-01
</code></pre></li>
<li><p>Now, here is the result of the regression when the control variables are excluded (notice that EQ is NOT statistically significant anymore)</p>

<pre><code>       intercept           EQ         MOM       MSCR
[1,] 0.007313402 -0.002111833 0.007128606 0.00668364
[2,] 2.652662996 -0.595391117 2.036985378 2.80177366
</code></pre></li>
</ul>

<p>The problem is that when I include my control variables, all my variables of interest are significant, but my control variables are not.</p>

<p>Which variables should I include in my final model? How should I structure my final model then, given the fact that the model will be used for forecasting?</p>

<p>Thank you,</p>
"
"0.225876975726313","0.234403615469248"," 82153","<p>I have a multivariate time series dataset including interacting biological and environmental variables (plus possibly some exogenous variables). Beside seasonality, there is no clear long-term trend in the data. My purpose is to see which variables are related to each other. Forecasting is not really looked for. </p>

<p>Being new to time-series analysis, I read several references. As far as I understand, Vector Autoregressive (VAR) model would be appropriate, but I donâ€™t feel comfortable with seasonality and most examples I found concerned economics field (as often with time series analysisâ€¦) without seasonality.</p>

<p>What should I do with my seasonal data?
I considered deseasonalizing them â€“ for example in R, I would use <code>decompose</code> and then use the <code>$trend + $rand</code> values to obtain a signal which appears pretty stationary (as judged per <code>acf</code>).
Results of the VAR model are confusing me (a 1-lag model is selected while I would have intuitively expected more, and only coefficients for autoregression â€“ and not for regression with other lagged variables - are significant). 
Am I doing anything wrong, or should I conclude that my variables are not (linearly) related / my model is not the good one (subsidiary question: is there a non-linear equivalent to VAR?).</p>

<p>[Alternatively, I read I could probably use dummy seasonal variables, though I canâ€™t figure out exactly how to implement it].</p>

<p>Step-by-step suggestions would be very appreciated, since details for experienced users might actually be informative to me (and R code snippets or links towards concrete examples are very welcome, of course). Thank you.</p>
"
"0.101015254455221","0.104828483672192"," 88722","<p>I am building a regression model of time series data in R, where my primary interest is the coefficients of the independent variables. The data exhibit strong seasonality with a trend.</p>

<p><img src=""http://i.stack.imgur.com/GYxaU.png"" alt=""Original data""></p>

<p>The model looks good, with four of the six regressors significant:
<img src=""http://i.stack.imgur.com/ZmoSd.png"" alt=""Model""></p>

<p>Here are the OLS residuals:
<img src=""http://i.stack.imgur.com/EIybo.png"" alt=""Residuals""></p>

<p>I used auto.arima to select the sARIMA structure, and it returns the model (0,1,1)(1,1,0)[12].</p>

<pre><code>fit.ar &lt;- auto.arima(at.ts, xreg = xreg1, stepwise=FALSE, approximation=FALSE)
summary(fit.ar)

Series: at.ts 
ARIMA(0,1,1)(1,1,0)[12]                    

Coefficients:
          ma1    sar1      v1       v2      v3       v4         v5
      -0.7058  0.3974  0.0342  -0.0160  0.0349  -0.0042  -113.4196
s.e.   0.1298  0.2043  0.0239   0.0567  0.0555   0.0333   117.1205

sigma^2 estimated as 3.86e+10:  log likelihood=-458.13
AIC=932.26   AICc=936.05   BIC=947.06

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 7906.896 147920.3 103060.4 0.1590107 3.048322 0.1150526
</code></pre>

<p>My question is this: based on the parameter estimates and s.e. of the regressors, I believe that none of them are significant - is this correct, and if so, what does it imply if my goal is to interpret the relative importance of these predictors as opposed to forecasting?</p>

<p>Any other advice relative to the process of building this model is welcome and appreciated.</p>

<p>Here are the ACF and PACF for the residuals:</p>

<p><img src=""http://i.stack.imgur.com/a3Gvy.png"" alt=""ACF-PACF""></p>

<pre><code>&gt; durbinWatsonTest(mod.ols, max.lag=12)
 lag Autocorrelation D-W Statistic p-value
   1     0.120522674     1.6705144   0.106
   2     0.212723044     1.4816530   0.024
   3     0.159828108     1.5814771   0.114
   4     0.031083831     1.8352377   0.744
   5     0.081081308     1.6787808   0.418
   6    -0.024202465     1.8587561   0.954
   7    -0.008399949     1.7720761   0.944
   8     0.040751905     1.6022835   0.512
   9     0.129788310     1.4214391   0.178
  10    -0.015442379     1.6611922   0.822
  11     0.004506292     1.6133994   0.770
  12     0.376037337     0.7191359   0.000
 Alternative hypothesis: rho[lag] != 0
</code></pre>
"
"0.142857142857143","0.14824986333222"," 94406","<p>I am working on a project, and I am totally new to statistics. I have sales data for last two years at week level, along with other variables like temperature, holiday (TRUE/FALSE), where holiday are nominal variables. I have to do forecasting for the next 52 weeks. I have the following questions:</p>

<ol>
<li>Can I use time series regression model where sales would be dependent, and temperature and<br>
holiday would be independent variables?</li>
<li>How to decide which independent variable would have more impact on the sales? </li>
<li>Can we do forecasting using nominal variables? Will dummy coding work?</li>
<li>Can we do it in R/SPSS?</li>
</ol>

<p>I would appreciate any kind of help. Thanks in advance.</p>
"
"0.378807204207079","0.419313934688767","109835","<p>While working on a big data set made of 10-minutes-points of information - i.e. <code>144</code> points per day, <code>1008</code> per week and <code>52560</code> per year - I encountered a few problem in R. The information concerns electricity load on a source substation during the year.</p>

<h3>Multiple seasonality :</h3>

<p>The data set clearly shows multiple seasonalities, which are daily, weekly and yearly. From <a href=""http://stats.stackexchange.com/questions/47729/two-seasonal-periods-in-arima-using-r"">there</a> I understood that R doesn't handle multiple seasonality within the ARIMA modeling functions.  I would really like to work with ARIMA models though, because my previous work is based on ARIMA models and I know approximatively how to translate a model into an equation.  </p>

<h3>Long seasonality :</h3>

<p>Each of the seasonalities is of high value, with the shortest one being the daily seasonality at 144. Unfortunately from the SARIMA general equation which is<br>
$\phi(B)\Phi(B^s)W_t = \theta(B)\Theta(B^s)Z_t$<br>
I guessed that the maximum lag for a given model <code>SARIMA(p,d,q)(P,D,Q)144</code> is<br>
$max((p+P*144), (q+Q*144))$</p>

<p>I would really like to try and fit models with values of P and/or Q greater than 1, but R doesn't allow me since the <code>maximum supported lag = 350</code>. To do so I found <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">this link</a> which is really interesting and led to new functions in the forecast package by M. Hyndman, called <code>fourier</code> and <code>fourierf</code> which you can find <a href=""http://www.inside-r.org/packages/cran/forecast/docs/fourier"" rel=""nofollow"">here</a>. But since I am not a specialist in forecasting nor in statistics, I have some difficulties understanding how I can make this work.  </p>

<hr>

<p>The thing is I thing this whole fourier regressors package could help me a lot. From what I understood I could use it to simulate the long-seasonality of my data set, maybe use it to simulate multiple seasonality, and even more it could allow me to introduce exogenous variables - which are the <code>temperature</code> and (<code>public holiday + sundays</code>).<br>
I also tried doing some regression following <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">this example</a> but I couldn't make it work because :</p>

<pre><code>Error in forecast.Arima(bestfit, xreg = fourierf(gas, K = 12, h = 1008)) : 
Number of regressors does not match fitted model
</code></pre>

<p>I really hope somebody can help me get a better understanding of these functions. Thanks.</p>

<p><strong>Edit :</strong> So I tried my best with the fourier example given <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a> but couldn't figure out how it handles the fitting. Here is the code (I copy-pasted M. Hyndman one and adapted to my data set - unsuccessfully) :</p>

<pre><code>n &lt;- 50000
m &lt;- 144
y &lt;- read.table(""auch.txt"", skip=1)
fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}

library(forecast)
fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008)))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m), fourier(n+1:(14*m),4,1008))))
</code></pre>

<p>So I wanted to ""force"" the model to be a <code>SARIMA(2,1,5)(1,2,8)[144]</code> but when I type <code>arimod</code>this is the result of the Arima fitting :</p>

<pre><code>&gt; fit  
Series: y[1:n, 1] , 
ARIMA(2,1,5)                  

sigma^2 estimated as 696895:  log likelihood=-407290.2  
AIC=814628.3   AICc=814628.3   BIC=814840
</code></pre>

<p>It doesn't even take into consideration the seasonal part of the model, and I don't know much about the range the AIC values can take, but it seems way too high to be a good fitting model right there. I think it all comes down to my misunderstanding of the use of Fourier terms as regressors, but I can't figure out why.</p>

<p><strong>Edit 2 :</strong> Also I can't seem to be able to add another exogenous variable to the Arima function. I need to use <code>temperature</code> - probably as a lead - to fit the <code>SARIMAX</code> model, but as soon as I write this :</p>

<pre><code>fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008), tmp[1:n]))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m),fourier(n+1:(14*m),4,1008), tmp[n+1:(14*m)])))
</code></pre>

<p>Nothing is plotted besides the initial data set. There is no forecast while without <code>tmp</code> as an <code>xreg</code> I still get some results.</p>
"
"0.142857142857143","0.14824986333222","120207","<p>I'm developing a forecasting application whose purpose is to allow an importer to forecast demand for its products from its customer network of distributors. Sales figures are a pretty good proxy for demand, so long as there is adequate inventory to fill the demand. When inventory gets drawn down to zero, though (the situation we're looking to help our customer avoid), we don't know much much we missed the target by. How many sales would the customer have made, had they had sufficient supply? Standard regression-based ML approaches that use Sales as a simple target variable will produce inconsistent estimates of the relationship between time, my descriptive variables, and demand. </p>

<p>Tobit modeling is the most obvious way to approach the problem: <a href=""http://en.wikipedia.org/wiki/Tobit_model"">http://en.wikipedia.org/wiki/Tobit_model</a>. I am wondering about ML adaptations of random forests, GBMS, SVMs, and neural networks that also account for a left-handed censored structure of the data. </p>

<p>In short, how do I apply machine learning tools to left-censored regression data to get consistent estimates of the relationships between my dependent and independent variables? First preference would be for solutions available in R, followed by Python. </p>

<p>Cheers, </p>

<p>Aaron</p>
"
"0.475457311050196","0.493405369684476","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.174963553055941","0.181568259800641","135011","<p>I'm having trouble finding a time series technique to deal with a data set I am working on. It contains multiple subjects and multiple variables, not all of which will likely be part of the time series. It looks something like this:</p>

<pre><code>Subject  Date      T1  T2  V1  V2  V3
A        1/1/2012  1   5   9   13  17
A        2/1/2012  2   6   10  14  18
...
B        1/1/2012  3   7   11  15  19
B        2/1/2012  4   8   12  16  20
...
</code></pre>

<p>Where T1, T2 are likely time series, and V1, V2, and V3 are likely not. I'm sure that this distinction is probably unnecessary, since techniques like Box-Jenkins should detect autoregression in any variable.</p>

<p>Ultimately, I want to be able to do forecasting on other subjects that were probably not used to build this model.</p>

<p>If you know of any R package(s) that can take this on, please let me know. Some example code would also be greatly appreciated. Thank you for any insight you can provide.</p>

<p>Edit: I am looking into dynamic linear regression using the <code>dynlm</code> package, but am having trouble coding it to include the dates and subjects.</p>
"
"0.174963553055941","0.12104550653376","141339","<p>I'm having trouble in taking a direction of my research project. I have independent variables that are commonly used as economic indicators and I want to include variables/indicators that are not commonly used to improve my eventual forecasts. I have 31 independent variables with 607 monthly observations after making it stationary and applying the scale function.(scale was applied cause my variable series are of different units/measures)
   I used the PCA function and got down to 13 components that capture 80% cumulative of the variance.
   Question is now that I have 13 new independent variables and the one dependent variable that is ternary in the sense that in the 607 observations it indicates 1 for peak, 0 for nothing, and -1 as trough, what model is best for forecasting/predicting the next 1 &amp; -1 of my dependent variable series based on my 13 independent principal components?</p>

<p>FYI: I have looked at VAR, Cointegration, Granger Causality, Multiple Linear Regression, but can't really make sense if what I'm using is correct and appropriate for my topic.</p>
"
"0.202030508910442","0.209656967344384","152012","<p>This might fit better here than on stackoverflow, I guess.</p>

<p>I was <a href=""http://stackoverflow.com/questions/30139874/r-dynamic-linear-regression-with-dynlm-package-how-to-predict"">trying to build a dynamic regression model with the dynlm</a> package, but it did not work out. After reading <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">this</a> by Hyndman, I now switched to an ARMAX model:</p>

<pre><code>y_t = a_1*x1_t + a_2*x2_t + ... + a_k*xk_t + n_t
</code></pre>

<p>where the error term follows an ARMA model</p>

<pre><code>n_t ~ ARMA(p,q)
</code></pre>

<p>So far I am using the function <code>auto.arima(y, xreg=cbind(x1, ..., xk))</code> from the <code>forecast</code>package, which is doing the job!</p>

<p>As a benchmark I am running a pure multiple regression with <code>lm()</code>, where I make use of the <code>step()</code> function to kick out non relevant variables (about 100 variables, from which 96 are dummies) to optimize the model according to <code>AIC</code>.</p>

<p>The in-sample forecasting for both models is more or less equal. As the ARMAX model always includes <strong>all</strong> independent variables <code>(x1, ..., xk)</code>, I am pretty sure that, if I could apply the <code>step()</code> function on it, I would achieve a further improvement here.</p>

<p>The problem is that the <code>step()</code> function does not work on <code>auto.arima()</code>?!</p>

<p>Do you have any suggestions how I could still do this? Or would I need a totally new approach?</p>

<p>(I have not provide a reproducible example, as this is a rather general question of which methods/functions/packages to use. If the question is not clear enough, please tell me and I will try to provide one)</p>
"
"0.225876975726313","0.234403615469248","156202","<p>I am working on a project where I am to do the intervention analysis and forecasting based on the time series. The problem is something like:</p>

<p><em>I have a normal time series entries but in between them some known event like natural calamities (storm, tornado) happens. I have the data for that and it affects the normal time series. Now my objective is to forecast the value of time series both in normal mode and also when I have a prediction of storm coming.</em></p>

<p>I have been reading <a href=""http://rads.stackoverflow.com/amzn/click/0471615285"" rel=""nofollow"">Forecasting with dynamic regression</a> chapter 7 about intervention analysis. I am also reading about the transfer function modeling. Can you please help me as in which model is good for this kind of time series analysis? Or may be some link which can guide me as how to do it? I will appreciate a link with some example in R or some examples.</p>

<p>EDIT: I guess I was not correct in description but I know the exact time information of all the previous storm events and I sort of want to find out the effect of storm intervention on the time series and I can forecast more closely if I know that there is a storm happening right now.</p>
"
"0.225876975726313","0.234403615469248","159428","<p>I have a set of data, let's say average weight of employees, captured every month over a period of 5 years (2010 - 2014). I cannot find a seasonality trend in the data over these years. Also, I have found that it is not dependent on any other factors.</p>

<p>I am trying to forecast values for 2015 to get a general sense of this data as it is an important metric in the operations of my business. </p>

<p>I have tried ARIMA, R-regression, Exponential smoothing, Excel forecast to find any seasonality whatsoever. However, my efforts are yet to materialize. </p>

<p>My question is: How do I forecast a variable that has no seasonality?</p>

<p>I have attached my data herewith. </p>

<p><strong>Graphs</strong></p>

<p>Yearly Values for years 2010 - 2014</p>

<p><img src=""http://i.stack.imgur.com/rmoeD.jpg"" alt=""enter image description here""></p>

<p>Value Cumulative over 2010 - 2014</p>

<p><img src=""http://i.stack.imgur.com/iwyh8.jpg"" alt=""enter image description here""></p>

<p>All Values from 2010 - 2014</p>

<p><img src=""http://i.stack.imgur.com/dfcGd.jpg"" alt=""enter image description here""></p>

<p><strong>Auto ARIMA in R</strong></p>

<pre><code># Map 1-based optional input ports to variables
dataset1 &lt;- maml.mapInputPort(1) # class: data.frame
library(forecast)


dates &lt;-  dataset1$Date
values &lt;- dataset1$Weight

dates &lt;-  as.Date(dates, format = '%m/%d/%Y')
values &lt;- as.numeric(values)

train_ts &lt;- ts(values, frequency=12)
fit1 &lt;- auto.arima(train_ts)
train_model &lt;- forecast(fit1, h = 12)
plot(train_model)

# produce forecasting
train_pred &lt;- round(train_model$mean,2)
data.forecast &lt;- as.data.frame(t(train_pred))
#colnames(dataset1.forecast) &lt;- paste(""Forecast"", 1:data$horizon, sep="""")

# Select data.frame to be sent to the output Dataset port
maml.mapOutputPort(""data.forecast"");
</code></pre>

<p><strong>Forecasted Value with Auto ARIMA</strong></p>

<pre><code>Date        Weight
01-01-15    11.77
01-02-15    11.76
01-03-15    11.77
01-04-15    11.76
01-05-15    11.77
01-06-15    11.77
01-07-15    11.76
01-08-15    11.77
01-09-15    11.76
01-10-15    11.77
01-11-15    11.77
01-12-15    11.76
</code></pre>

<p><strong>Data</strong></p>

<pre><code>Date        Weight      Cumulative Weight
01-01-10    11.8800     11.8800
01-02-10    10.4000     22.2800
01-03-10    6.9500      29.2300
01-04-10    15.5000     44.7300
01-05-10    17.0400     61.7700
01-06-10    10.4700     72.2400
01-07-10    12.1400     84.3800
01-08-10    2.5800      86.9600
01-09-10    12.6300     99.5900
01-10-10    11.6800     111.2700
01-11-10    9.0700      120.3400
01-12-10    10.8900     131.2300
01-01-11    1.7500      132.9800
01-02-11    -1.7700     131.2100
01-03-11    5.9300      137.1400
01-04-11    -4.9200     132.2200
01-05-11    4.3900      136.6100
01-06-11    1.5100      138.1200
01-07-11    1.2200      139.3400
01-08-11    10.2900     149.6300
01-09-11    13.0600     162.6900
01-10-11    10.1400     172.8300
01-11-11    8.5250      181.3550
01-12-11    6.4350      187.7900
01-01-12    -5.5100     182.2800
01-02-12    -4.3000     177.9800
01-03-12    2.3200      180.3000
01-04-12    4.0700      184.3700
01-05-12    12.2700     196.6400
01-06-12    14.7400     211.3800
01-07-12    8.4600      219.8400
01-08-12    11.6300     231.4700
01-09-12    -0.1500     231.3200
01-10-12    2.5200      233.8400
01-11-12    6.7400      240.5800
01-12-12    35.6300     276.2100
01-01-13    26.4000     302.6100
01-02-13    26.1300     328.7400
01-03-13    16.2100     344.9500
01-04-13    56.0800     401.0300
01-05-13    32.2300     433.2600
01-06-13    17.5100     450.7700
01-07-13    3.6700      454.4400
01-08-13    7.7700      462.2100
01-09-13    -14.2800    447.9300
01-10-13    1.0800      449.0100
01-11-13    9.4000      458.4100
01-12-13    7.3400      465.7500
01-01-14    6.1400      471.8900
01-02-14    3.8200      475.7100
01-03-14    16.7600     492.4700
01-04-14    0.4900      492.9600
01-05-14    17.9800     510.9400
01-06-14    14.8000     525.7400
01-07-14    12.6400     538.3800
01-08-14    5.7300      544.1100
01-09-14    -2.0900     542.0200
01-10-14    9.1300      551.1500
01-11-14    12.5100     563.6600
01-12-14    -1.3900     562.2700
</code></pre>

<p><strong>Actual Values for 2015</strong></p>

<pre><code>Date        Weight
01-01-15    -18.43
01-02-15    13.94
01-03-15    26.14
01-04-15    24.36
01-05-15    18.37
</code></pre>
"
"0.142857142857143","0.0741249316661101","161614","<p>I want to solve the first exercice of the Multiple Regression Chapter of R. Hyndman's online book on Time Series Forecasting (see <a href=""https://www.otexts.org/fpp/5/8"" rel=""nofollow"">https://www.otexts.org/fpp/5/8</a>). I use <code>R</code> with <code>fpp</code> package as wanted in the exercise.</p>

<p>I am blocked in the following question:
c. Use R to fit a regression model to the logarithms of these sales data with a linear trend, seasonal dummies and a â€œsurfing festivalâ€ dummy variable.</p>

<p>Indeed, I don't know how to make the function <code>tslm</code> work with my dummy vector for the surfing festival. Here is my code.</p>

<pre><code>library(fpp)
log_fancy = log(fancy)
dummy_fest_mat = matrix(0, nrow=84, ncol=1)
for(h in 1:84)
    if(h%%12 == 3)   #this loop builds a vector of length 84 with
        dummy_fest_mat[h,1] = 1   #1 corresponding to each month March
dummy_fest_mat[3,1] = 0 #festival started one year later

dummy_fest = ts(dummy_fest_mat, freq = 12, start=c(1987,1))
fit = tslm(log_fancy ~ trend + season + dummy_fest)
</code></pre>

<p>When I do <code>summary(fit)</code>, I see that the regression coefficients have been well calculated, but when I continue with <code>forecast(fit)</code>
I get the following error : </p>

<pre><code>Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
  variables have not equal length (found for 'factor(dummy_fest)')
In addition: Warning message:
'newdata' had 50 rows but variables found have 84 rows 
</code></pre>

<p>But what is even stranger is that when I do <code>forecast(fit, h=84)</code>, it works!!
I don't know what is happening here, can someone explain me?</p>
"
"0.174963553055941","0.181568259800641","163074","<p>So I've been learning how to forecast over this summer and I've been using Rob Hyndman's book Forecasting: principles and practice.  I've been using R, but my questions aren't about code.  For the data I've been using, I've found that an average forecast of multiple models has produced higher accuracy levels that any sole model by itself.  </p>

<p>Recently I read an blog that talked about averaging forecasting methods and assigning weights to them.  So in my case, lets say I assign 11 different models to my set of data (Arima, ETS, Holt Winters, naive, snaive, and so forth) and I want to average a few of these to get a forecast.  Has anyone had any experience with this or can point me to an article that might give some insight on the best way of going about this?</p>

<p>As of right now, I'm using cross validation and Mean Absolute Error to figure out which models perform best and which perform worst. I can even use this to identify the top k # of models.</p>

<p>I guess my questions are</p>

<p>1) How many models would you suggest selecting? (2,3,4,5,6, etc)</p>

<p>2) Any ideas on weights?  (50% to the best, 25% to the second best, 15% third best, 10% to the 4th best, etc)</p>

<p>3) Are any of these forecasting models redundant and shouldn't be included? 
(Arima, snaive, naive, HW's ""additive"", ETS, HoltWinters exponential smoothing, HoltWinters smoothing w/ trend, HoltWinters w/ trend/seasonality, multiple regression)</p>
"
"0.202030508910442","0.209656967344384","166485","<p>I created for the following data set a multiple regression. Now I would like to forecast the next 20 data points.</p>

<pre><code>&gt; dput(datSel)
structure(list(oenb_dependent = c(1.0227039, -5.0683144, 0.6657713, 
3.3161374, -2.1586704, -0.7833623, -0.2203209, 2.416144, -1.7625406, 
-0.1565037, -7.9803936, 9.4594715, -4.8104584, 8.4827107, -6.1895262, 
1.4288595, 1.4896459, -0.4198522, -5.1583964, 5.2502294, 1.0567102, 
-1.0923342, -1.5852298, 0.6061936, -0.3752335, 2.5008664, -1.3999729, 
2.2802166, -2.1468756, -1.4890328, -0.79254376, 3.21804705, -0.94407886, 
-0.27802316, -0.20753079, -1.12610048, 2.0883735, -0.7424854, 
0.44203729, -1.48905938, 1.39644424, -3.8917377, 11.25665848, 
-9.22884035, 3.26856762, -0.00179541, -2.39664325, 4.00455574, 
-5.60891295, 4.6556348, -4.40536951, 6.64234497, -7.34787319, 
7.56303006, -8.23083674, 4.43247855, 1.31090412), carReg = c(0.73435946, 
0.24001161, 16.90532537, -14.60281976, 6.47603166, -8.35815849, 
3.55576685, 7.10705794, -4.6955223, 10.9623709, 5.5801857, -6.4499936, 
-9.46196502, 9.36289122, -8.52630424, 5.45070994, -4.5346405, 
-2.26716538, 2.56870398, 0.013737, 5.7750101, -27.1060826, 1.08977179, 
4.94934712, 17.55391859, -13.91160577, 10.38981128, -11.81349246, 
-0.0831467, 2.79748237, 1.84865463, -1.98736934, -6.24191695, 
13.33602659, -3.86527871, 0.78720993, 4.73360651, -4.1674034, 
9.37426802, -5.90660464, -0.4915792, -5.84811629, 9.67648643, 
-6.96872719, -7.6535767, 0.24847595, 0.18685263, -2.28766949, 
1.1544631, -3.87636933, -2.4731545, 4.33876671, 1.08836339, 5.64525271, 
1.90743854, -3.94709355, -0.84611324), cpi = c(1.16, -3.26, 0.22, 
-3.51, 0.84, -2.81, -0.34, -4.57, -0.12, -3.95, -1.37, -2.73, 
0.35, -5.38, -4.43, -3.08, 0.74, -3.03, -1.09, -2, 0.35, -1.52, 
1.28, 0.2, -0.25, -4.55, -2.49, -4.24, -0.31, -2.96, -2.24, -0.46, 
-0.06, -2.67, -1.27, -1.4, -0.7, -0.96, -2.18, -2.53, -0.52, 
-1.74, -2.18, -1.4, -0.34, -0.09, -1.65, -1.15, -0.17, -2.01, 
-1.38, -1.24, 0.09, -2.44, -1.92, -2.61, -0.34), primConstTot = c(-0.33334, 
-0.93333, -0.16667, -0.33333, -0.16667, -0.86666, -0.3, -0.4, 
-0.26667, -1.56667, -0.73333, 0.1, -0.23333, -0.26667, -1.5774, 
-0.19284, 0.38568, -2.42423, -0.93663, 0.08265, -0.63361, 0.0551, 
-0.49587, 2.39668, -1.70798, -3.36085, -2.56196, 0.16529, 0, 
-1.84572, -1.3774, -0.49586, -1.70798, -1.90081, -0.55096, -0.77134, 
-0.16529, -0.30303, -0.17066, -0.23853, -0.64401, -1.52657, -1.57426, 
-0.28623, -0.54861, -1.07336, -0.71558, 0.02385, -0.38164, -1.09721, 
0, 0.14311, -0.38164, -1.02566, -0.42934, -0.35779, -0.4532), 
    resProp.Dwell = c(0.8, -4, -3.2, 2.7, -1.6, -1, -2.4, -0.4, 
    -0.8, 1, -12.1, 0.2, -5.2, 3.7, -2.7, -1.7, 1.5, 0.7, -7.9, 
    0.3, 0.3, 1.4, -3.3, -1, -1.6, 1.5, 0.5, 1.5, -1, -2.2, -3.5, 
    0.5, 0.5, -0.9, -0.4, -3.4, 0.9, 0.1, -0.2, -2.8, -0.8, -6.2, 
    11.3, -4.6, 1, 1.1, -1.7, 4.1, -5, 2.3, -2.3, 4.6, -6.3, 
    6.3, -6.9, 0, 2.4), cbre.office.primeYield = c(0, 0, 0.15, 
    0.15, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.2, 0.15, 0.1, 
    0.05, 0.15, 0.3, 0.35, 0.4, 0.3, 0.2, 0, -0.15, -0.85, -1, 
    -0.85, -0.75, -0.1, 0, 0, 0, 0.05, 0.05, 0.05, 0.05, 0, 0, 
    0, 0.2, 0.2, 0.2, 0.2, 0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25, 
    0, 0, 0, 0, 0, 0, 0), cbre.retail.capitalValue = c(-1882.35294, 
    230.76923, -230.76923, -226.41509, -670.78117, -436.13707, 
    -222.22223, 0, -205.91233, -202.16847, 0, -393.5065, -403.91909, 
    -186.30647, -539.81107, -748.11463, -764.70588, -311.47541, 
    -301.42782, -627.09677, -480, 720, 782.6087, 645.96273, 251.42857, 
    1386.66667, -533.33334, -533.33333, -533.33333, 0, 0, -1024.56141, 
    -192.10526, 0, -730, 0, 0, 0, 0, 0, -834.28571, 0, -1450.93168, 
    0, 0, 0, -700.78261, 0, 0, 0, 0, 0, 0, 0, -1452, 0, 0)), .Names = c(""oenb_dependent"", 
""carReg"", ""cpi"", ""primConstTot"", ""resProp.Dwell"", ""cbre.office.primeYield"", 
""cbre.retail.capitalValue""), row.names = c(NA, -57L), class = ""data.frame"")
&gt; 
&gt; fit &lt;- lm(oenb_dependent ~ carReg + cpi + primConstTot + 
+             resProp.Dwell + cbre.office.primeYield + cbre.retail.capitalValue , data = datSel)
&gt; summary(fit) # show results

Call:
lm(formula = oenb_dependent ~ carReg + cpi + primConstTot + resProp.Dwell + 
    cbre.office.primeYield + cbre.retail.capitalValue, data = datSel)

Residuals:
   Min     1Q Median     3Q    Max 
-5.166 -1.447 -0.162  1.448  7.903 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               0.831630   0.492297    1.69    0.097 .  
carReg                    0.085208   0.039600    2.15    0.036 *  
cpi                      -0.349192   0.212044   -1.65    0.106    
primConstTot              0.752772   0.383810    1.96    0.055 .  
resProp.Dwell             0.994356   0.086812   11.45  1.4e-15 ***
cbre.office.primeYield    1.274734   1.212782    1.05    0.298    
cbre.retail.capitalValue  0.000528   0.000643    0.82    0.416    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.24 on 50 degrees of freedom
Multiple R-squared:  0.754, Adjusted R-squared:  0.725 
F-statistic: 25.6 on 6 and 50 DF,  p-value: 1.2e-13
</code></pre>

<p>I tried the following:</p>

<pre><code>vals.multipleRegr &lt;- forecast(fit, h = 20)
Error: could not find function ""forecast""
</code></pre>

<p>However, this does not work as the function forecast cannot be found. I am using the following packages in my code, <code>library(bootstrap)</code>, <code>library(DAAG)</code> and <code>library(relaimpo)</code>. </p>

<p>Any suggestion how to forecasting using multiple regression?</p>

<p>I appreciate your replies!</p>
"
"0.202030508910442","0.157242725508288","166953","<p><strong>Issue</strong>: Cannot forecast sales accurately using quantile regression in R. I am using rq function from ""quantreg"" package which is giving me warning ""Result might have Non unique solutions""</p>

<p><strong>Aim</strong>: I am trying to forecast hourly sales of a store using quantile regression. </p>

<p>Below are the columns in my source table for forecasting.</p>

<ul>
<li><em>transaction_date</em> : sales date (input)</li>
<li><em>hr1 to hr24</em> : column with hourly sales info. (24 columns) (input)</li>
<li><em>totala</em> : total of 24 column hr1 to hr24 (not using currently)</li>
<li><em>location, department, sales_type</em>: forecasting will be done for each location, sales_type and department. (used to select data)</li>
<li><em>f1 to f24 :</em> columns I want to forecast for each hour (24 columns) (output)</li>
</ul>

<p>Packages Used: forecast, quantreg, Metrics</p>

<p><strong>Code</strong>: 
I have extracted date features from transaction_date eg. weekend, week of month and also holidays (1 if it is holiday 0 for regular days).</p>

<pre><code>attach(train_data) 
Y &lt;- cbind(hr) 
X &lt;- cbind(transation_date, Years, Months, Days, WeekDay, WeekofYear, Weekend, WeekofMonth, holidays) 

quantreg.all &lt;- rq(Y ~ X, tau = seq(0.05, 0.95, by = 0.05))
prediction_train &lt;- data.frame(predict(quantreg.all))
</code></pre>

<p>I have 19 models in prediction_train for each tau from 0.05 to 0.95, I select best model based on rmse value and than forecast using that tau.</p>

<pre><code>rmse(actual, predicted)
</code></pre>

<p>transaction_date is Date type, quantreg.all is rqs class and rest are numeric.</p>

<p><strong>Note:</strong> Stores are not open 24 hours, hence many hour columns will be 0 (time when store was close). Currently for most of such hours rq is predicting 0 or some negative values.</p>

<p>Weather  does not have major impact on sales.</p>
"
"0.142857142857143","0.14824986333222","169523","<p>I'm new to forecasting and trying to create a model to forecast one step ahead weekly sales from my company. The variables I've identified for the model are Lag 1 Markdown spend and lag 1 sales, and I've included dummy variables for monthly seasonality and some promotional weekends we run, resulting in the below model:</p>

<pre><code>&gt; summary(fit)

Call:
lm(formula = Sales.RtlT ~ L1.MD + P1 + P2 + P3 + P4 + P5 + P6 + 
    P7 + P8 + P9 + P10 + P11 + L1.Sales, data = lux)

Residuals:
 Min       1Q   Median       3Q      Max 
-10.7028  -2.8850  -0.2948   2.5488  15.3845 

Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.17443    1.86670   2.772 0.006115 ** 
L1.MD        0.16591    0.02353   7.050 3.06e-11 ***
P1           0.21394    1.84689   0.116 0.907902    
P2           0.47421    1.83599   0.258 0.796462    
P3          -1.16762    1.75653  -0.665 0.507013    
P4          -0.77594    1.84712  -0.420 0.674892    
P5           3.36375    1.76142   1.910 0.057650 .  
P6          -1.11759    1.56631  -0.714 0.476384    
P7           1.76297    1.79197   0.984 0.326429    
P8           5.13988    1.66437   3.088 0.002309 ** 
P9           5.39127    1.52419   3.537 0.000506 ***
P10          6.58703    1.61126   4.088 6.37e-05 ***
P11          7.02233    1.60600   4.373 2.00e-05 ***
L1.Sales     0.63513    0.04829  13.152  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 4.795 on 194 degrees of freedom
Multiple R-squared:  0.8115,    Adjusted R-squared:  0.7988 
F-statistic: 64.23 on 13 and 194 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>my weekly sales data isn't stationary, but if I difference it once it is, as shown below:</p>

<p><a href=""http://i.stack.imgur.com/VspqS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VspqS.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/cIN6S.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cIN6S.png"" alt=""enter image description here""></a></p>

<p>However I can't fit lm with the differenced values. Is it preferable to have a stationary time series for a forecast like this, and if so how should I run the regression to account for this? </p>
"
"0.202030508910442","0.209656967344384","178787","<p>Im really new in regression estimation but my problem here is about forecasting confrontation. </p>

<p>This is my model:</p>

<p>$Y_t = \beta_0 + \beta_1 X_t + \epsilon_t$ </p>

<p>My OLS estimation using r function ""lm"" was:</p>

<pre><code>set.seed(123)
data &lt;- matrix(rnorm(50*2),nrow=50)
m &lt;- data.frame(data )


Model1&lt;- lm(X1 ~ X2 -1 , data = m)
&gt; Modelo1$coef
        X2 
-0.0296194 
</code></pre>

<p>My Quantile Regression (Median, $\tau = 0.5$) was:</p>

<pre><code>&gt; ModeloRQ1&lt;-rq(X1 ~ X2 -1, tau = 0.5,method=""br"", data=m) 

&gt; ModeloRQ1$coef
        X2 
-0.1256418 
</code></pre>

<p>The estimation procedure i understand. </p>

<p>But the Forecasting Procedure i dont understand. 
I know that after making the forecast i should compare using RMSFE statistics, for example.</p>

<p>But when i use the ""forecast"" function gives me the same point forecast (same values) when i use ""predict"" function.</p>

<p>I have read some papers which do not detail this procedure. Only say that ""OLS and QR (0.5) forecasts are Confronted against each other"". </p>

<p>How should i do this procedure? Simply by using the function predict/forecasting? this would be a commonly used procedure?</p>
"
"0.142857142857143","0.14824986333222","180217","<p>I'm using time series data containing both trend and seasonality. I also have 2 endogenous predictor variables that I would like to include in my model.</p>

<p>In R I've used the forecast package to develop a dynamic regression model with use of <code>auto.arima()</code> and the <code>xreg</code> argument from the <code>forecast package</code>. I understand this procedure takes a regression and then attempts to fit the residuals with an ARMA Model.</p>

<p>I've also developed what seems to be an appropriate model using the forecasting Module in SPSS by specifying a Seasonal ARIMA model and including my covariates. However, one of the coefficients on one of my endogeneous predictors has a negative sign which makes no sense intuitively. </p>

<p>I've read Dr. Hyndman's article <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">The ARIMAX model muddle</a> and found it to be extremely insightful and useful. However, I have not been able to find any documentation on what type of statistical procedure SPSS uses to fit an ARIMA model with covariates, so I'm not sure how I should interpret the coefficients or how concerned I should be with a flipped sign. Any help clarifying the modelling procedure used by SPSS would be tremendously appreciated. </p>
"
"0.202030508910442","0.209656967344384","184713","<p>I am fairly new to R. I have attempted to read up on time series analysis and have already finished </p>

<ol>
<li>Shumway and Stoffer's <a href=""http://www.stat.pitt.edu/stoffer/tsa3/"" rel=""nofollow"">Time series analysis and its applications 3rd Edition</a>,</li>
<li>Hyndman's excellent <a href=""https://www.otexts.org/fpp"" rel=""nofollow"">Forecasting: principles and practice</a></li>
<li>Avril Coghlan's <a href=""http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html"" rel=""nofollow"">Using R for Time Series Analysis</a></li>
<li>A. Ian McLeod et al <a href=""http://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf"" rel=""nofollow"">Time Series Analysis with R</a></li>
<li>Dr. Marcel Dettling's <a href=""https://stat.ethz.ch/education/semesters/ss2013/atsa/ATSA-Scriptum-SS2013_130218.pdf"" rel=""nofollow"">Applied Time Series Analysis</a></li>
</ol>

<p>Edit: I'm not sure how to handle this but I found a usefull resource outside of Cross Validated. I wanted to include it here in case anyone stumbles upon this question. </p>

<p><a href=""http://isites.harvard.edu/fs/docs/icb.topic79832.files/L06_Program_Evaluation_2/Segmented_Regression.Wagner.2002.pdf"" rel=""nofollow"">Segmented regression analysis of interrupted time series studies in medication use research</a></p>

<p>I have a univariate time series of the number of items consumed (count data) measured daily for 7 years. An intervention was applied to the study population at roughly the middle of the time series. This intervention is not expected to produce an immediate effect and the timing of the onset of effect is essentially unknowable.</p>

<p>Using Hyndman's <code>forecast</code> package I have fitted an ARIMA model to the pre-intervention data using <code>auto.arima()</code>. But I am unsure of how to use this fit to answer whether there has been a statistically significant change in trend and quantify the amount.</p>

<pre><code># for simplification I will aggregate to monthly counts
# I can later generalize any teachings the community supplies
count &lt;- c(2464, 2683, 2426, 2258, 1950, 1548, 1108,  991, 1616, 1809, 1688, 2168, 2226, 2379, 2211, 1925, 1998, 1740, 1305,  924, 1487, 1792, 1485, 1701, 1962, 2896, 2862, 2051, 1776, 1358, 1110,  939, 1446, 1550, 1809, 2370, 2401, 2641, 2301, 1902, 2056, 1798, 1198,  994, 1507, 1604, 1761, 2080, 2069, 2279, 2290, 1758, 1850, 1598, 1032,  916, 1428, 1708, 2067, 2626, 2194, 2046, 1905, 1712, 1672, 1473, 1052,  874, 1358, 1694, 1875, 2220, 2141, 2129, 1920, 1595, 1445, 1308, 1039,  828, 1724, 2045, 1715, 1840)
# for explanatory purposes
# month &lt;- rep(month.name, 7)
# year &lt;- 1999:2005
ts &lt;- ts(count, start(1999, 1))
train_month &lt;- window(ts, start=c(1999,1), end = c(2001,1))
require(forecast)
arima_train &lt;- auto.arima(train_month)
fit_month &lt;- Arima(train_month, order = c(2,0,0), seasonal = c(1,1,0), lambda = 0)
plot(forecast(fit_month, 36)); lines(ts, col=""red"")
</code></pre>

<p>Are there any resources specifically dealing with interrupted time series analysis in R? I have found <a href=""http://epoc.cochrane.org/sites/epoc.cochrane.org/files/uploads/21%20Interrupted%20time%20series%20analyses%202013%2008%2012_1.pdf"" rel=""nofollow"">this</a> dealing with ITS in SPSS but I have not been able to translate this to R. </p>
"
"0.202030508910442","0.157242725508288","188510","<p>I have developed a model (TSM) which is very good at forecasting daily revenue, however it is very black box. The TSM is univariate, whereas the regression models are multivariate. My goal here is to identify the causal factors of revenue for policy recommendations using regression analysis in R. The models that I am using are decision tree, random forest, linear model (some variables), linear model 2 (all variables), xgboost, and lasso regression. All variables are numeric. </p>

<p>Looking at random forest vs linear regression, (both rmse and mae are similar for these two), we see something odd: Looking at the variable importance plot for random forest, variable x is the least important. However, for the regression model, it is highly significant. Furthermore, lasso regression also indicates that the variable is very important. How can this be so? </p>

<p>This variable has a important policy implication to the business, so which model is right - is variable x the most important or is it the least important?  I vaguely recall Breiman writing a paper some time ago discussing something along the lines of this. </p>

<p>The below chart is the out of sample accuracy of the models over a 4 month time period. </p>

<pre><code>&gt; a
    tsm      tree        rf       lm       lm2      xgb      lasso
 0.9715964 0.9854246 0.9904363 0.981333 0.9817757 0.974603  0.997324
</code></pre>

<p>PS, I am sorry I cannot disclose the variables themselves since they are confidential to the company.</p>

<p>Thank you for your assistance!</p>
"
"0.285714285714286","0.29649972666444","188597","<p>I have daily data for 3 years. This sales data is of seasonal nature as business has spikes and downfall by month. Also, sales differ by each day of the week. for example, monday in general in a month tend to have similar pattern.</p>

<p>I have used ARIMA and created a matrix of month dummy variables and day of week dummy variables and have passed that in ARIMA. however i hit the bottom when i couldn't reconvert differenced stationary number forecasts into the actual sales metric. <a href=""http://stats.stackexchange.com/questions/188595/convert-double-differenced-forecast-into-actual-value"">Posted here already</a></p>

<p>I have also tried dummy regression using sales as dependent variable and 11 month dummy variables and 6 day of week dummy variables. i abandoned this as R square was low at 48% and MAPE from the forecasted results was more than 20%</p>

<p>Edit: I have tried auto.arima as well.
My question: What technique can i use for forecasting sales for next 365 days? that will consider this month of the year and day of the week seasonality?</p>
"
"0.225876975726313","0.234403615469248","191851","<p>I am building a VAR model to forecast the price of an asset and would like to know whether my method is statistically sound, whether the tests I have included are relevant and if more are needed to ensure a reliable forecast based on my input variables. </p>

<p>Below is my current process to check for Granger causality and forecast the selected VAR model.</p>

<pre><code>require(""forecast"")
require(""vars"")

#Read Data
da=read.table(""VARdata.txt"", header=T)
dac &lt;- c(2,3) # Select variables
x=da[,dac]

plot.ts(x)
summary(x)

#Run Augmented Dickey-Fuller tests to determine stationarity and differences to achieve stationarity.
ndiffs(x[, ""VAR1""], alpha = 0.05, test = c(""adf""))
ndiffs(x[, ""VAR2""], alpha = 0.05, test = c(""adf""))

#Difference to achieve stationarity
d.x1 = diff(x[, ""VAR1""], differences = 2)
d.x2 = diff(x[, ""VAR2""], differences = 2)

dx = cbind(d.x1, d.x2)
plot.ts(dx)

#Lag optimisation
VARselect(dx, lag.max = 10, type = ""both"")

#Vector autoregression with lags set according to results of lag optimisation. 
var = VAR(dx, p=2)

#Test for serial autocorrelation using the Portmanteau test
#Rerun var model with other suggested lags if H0 can be rejected at 0.05
serial.test(var, lags.pt = 10, type = ""PT.asymptotic"")

#ARCH test (Autoregressive conditional heteroscedasdicity)
arch.test(var, lags.multi = 10)

summary(var)

#Granger Causality test
#Does x1 granger cause x2?
grangertest(d.x2 ~ d.x1, order = 2)

#Does x2 granger cause x1?
grangertest(d.x1 ~ d.x2, order = 2)

#Forecasting
prd &lt;- predict(var, n.ahead = 10, ci = 0.95, dumvar = NULL)
print(prd)
plot(prd, ""single"")
</code></pre>

<p>Is this method sound?</p>
"
"0.349927106111883","0.332875142967841","198315","<p>I am currently getting slightly confused with how a rolling forecast should be setup in R, as in how the data should be organised in order to <em>train</em> and <em>test</em> my model. I feel there is a large gap in my understanding somewhere.</p>

<p>I have seen many examples of forecasting methods, but not many on time-series (using lagged variables) that go into detail, i.e. perform everything manually using <code>predict()</code>. Instead it normally just points to a built in R package. <strong>My question has to do with the training of the model - the alignment of the data for the training.</strong></p>

<p>I know that the regression equation (assuming I am performing a linear regression) looks like this:</p>

<p>$$ y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + \beta_3 x_{t-1} + \beta_4 x_{t-2} + \varepsilon_t $$ </p>

<p>So I have my outcome variable, $y$, being explained by two of its own lagged values, plus two lagged values of a second variable, $x$. Each variable has its own coefficient, all of which my model is estimating.</p>

<p>Let's say I have a <em>data.table</em> (or data.frame), where each row consists of the data aligned according to the equation above. Each row is one day, and represents the given equation. I have five columns, and for this example say 200 rows/days.</p>

<blockquote>
  <p>Day  |  $y_t$  |  $y_{t-1}$  |  $y_{t-2}$  |  $x_{t-1}$  |  $x_{t-2}$</p>
  
  <p>001  | <em>Values</em> --></p>
  
  <p>002  | <em>Values</em> --></p>
  
  <p>003  | <em>Values</em> --></p>
</blockquote>

<p>I use the above equation as the formula to fit my model to obtain estimates for the four coefficients (neglecting $\varepsilon$) using a fixed 40-day time frame.</p>

<pre><code>model &lt;- lm(y ~ y_1 + y_2 + x_1, x_2,            ## my regression formula
            data = input_data[1:40])             ## my data.table 
</code></pre>

<p>Now I want to make a prediction using the fitted <code>model</code>.
I do this by using <code>predict()</code> in <strong>R</strong> as follows:
I take the next (41st) row of data, minus the outcome variable</p>

<pre><code>my_pred &lt;- predict(model, newdata = input_data[41][, outcome := NULL])
</code></pre>

<p>And then calculate my error:</p>

<pre><code>my_error &lt;- input_data[41, outcome] - my_pred
</code></pre>

<p>I then shift everything forward one row, so still a 40-day frame <code>input_data[2:41]</code> updating the coefficients for all variables and predicting the following outcome variable, <code>input_data[42]</code>. This is yielding terrible results for my model, with overall accuracies not much better than a naÃ¯ve forecast, i.e. random guessing.</p>

<p>Should I realign the data for the training segment, so that each row rather represents the data I had on that day? This would mean adding one more column, $x_t$.</p>

<p>Any other suggestions or comments?</p>

<p>Thanks.</p>
"
"0.349927106111883","0.332875142967841","198844","<p>I'm trying to understand how <code>auto.arima</code> with covariates in the xreg parameter works. I'm familiar with regression and I'm starting to work on forecasting.</p>

<p>My understanding of forecasting is that you look for patterns in the past time series and then project those paterns onto the future.  </p>

<p>My uderstanding of regression is that you use predictors to try to generate an output value and minimize the difference between your created value and the real value.  </p>

<p>So how does forecasting <code>auto.arima</code> with <code>xreg</code> work? Do you create a forecast for a timeseries based on past data and regression model based on the input time series and input <code>xreg</code>, and then forecast each data point in the time series and for each forecasted data point use the regression model you built and future <code>xreg</code> values to adjust the forecasted values?</p>

<p>I'm a former physics grad student, so I'm not allergic to math but I'm just looking for a high level overview of the process here to understand how forecasting <code>auto.arima</code> works.  </p>

<p>For example like, </p>

<ul>
<li><p>step 1: build forecast model on input time series, and regression model on input time series and input <code>xreg</code> values</p></li>
<li><p>step 2: forecast model into future one step, and predict value with regression model and future <code>xreg</code> values</p></li>
<li><p>step 3: algorithm combines forecasted value and regression model prediction to get combined value</p></li>
</ul>

<p>This is just a guess at how it works, but it's an example of the kind of high level explanation I'm looking for.</p>

<p>I've included some code below that I've been working on trying to forecast time in to out <code>TiTo</code> for customers at a restaurant with predictor count of customers in the restaurant <code>CustCount</code>.</p>

<pre><code>OV&lt;-zoo(SampleData$TiTo, 
    order.by=SampleData$DateTime)


eDate &lt;- ts(OV, frequency = 24)

Train &lt;-eDate[1:15000]
Test &lt;- eDate[15001:22773]

xregTrain &lt;- SampleData[1:15000,]$CustCount
    xregTest &lt;- SampleData[15001:22773,]$CustCount

Arima.fit &lt;- auto.arima(Train, xreg = xregTrain)

Acast&lt;-forecast(Arima.fit, h=7772, xreg = xregTest)

accuracy(Acast$mean,Test)
</code></pre>
"
"0.285714285714286","0.259437260831385","212840","<p>I read Chen et al. <a href=""http://onlinelibrary.wiley.com/doi/10.1002/for.1134/abstract"" rel=""nofollow"">""Forecasting volatility with support vector machine-based GARCH model""</a> (2010) where they implented a recurrent SVM procedure to estimate volatility by a GARCH based model. 
The model is of the form </p>

<p>$y_t = f(y_{t-1}) + u_t \qquad \qquad \ \ \ (1)$ </p>

<p>$u^2_t = g(u^2_{t-1}, w_{t-1}) + w_t \qquad  (2)$ </p>

<p>At first they got estimates for $u_t$ by estimating $(1)$ by a SVM. Then, the following recurrent SVM algorithm was proposed to estimate $(2)$.</p>

<hr>

<p><strong><em>Recurrent SVM Algorithm:</em></strong></p>

<p><strong>Step 1:</strong> Set $i = 1$ and start with all residuals at zero: $w_t^{(1)} = 0 $.</p>

<p><strong>Step 2:</strong> Run an SVM procedure to get the decision function $f^{(i)}$ to the points $\{x_t, y_t\} = \{u_{t - 1}^2, u_t^2 \}$ with all inputs $x_t = \{u_{t - 1}^2, w_{t-1} \}$</p>

<p><strong>Step 3:</strong> Compute the new residuals $w_t^{i+1} = u_t^2 - f^{(i)}$.</p>

<p><strong>Step 4:</strong> Terminate the computaion process if the stopping criterion is satisfied; otherwise, set $i = i + 1$ and go back to Step 2.</p>

<hr>

<p>The proposed stopping critrerion is based a Ljung-Box-Test for the residuals $w_t$. Only if the $p$-values of the test in five consecutive periods are higher than 0.1 the process is stopped. </p>

<p>As real world example the log-returns of the New York Stock Exchange (NYSE) composite stock index for the period from January 8, 2004 to December 31, 2007 was used. The last 60 observations where used as test sample. Hence, the estimation was done with the first 940 observations. In their study, the process converged after 121 interations. <strong>(Question:) However, my implementation in R does not converge. I think I have a misunderstanding of the concept.</strong> Because I think I implemented it exactly as stated. My R code is the following</p>

<pre><code>rm(list = ls())

library(quantmod)
library(e1071)

#Get NYSE data and convert to log returns
id     &lt;- ""^NYA""
data   &lt;- getSymbols(id, source = ""yahoo"", auto.assign = FALSE, 
                     from = ""2004-01-08"", to = ""2007-12-31"")
series &lt;- data[,6]  #Get adjusted closing prices
series &lt;- na.omit(diff(log(series)))*100  #Compute log returns

#Lagged data for analysis
x      &lt;- na.omit(cbind(series, lag(series)))

#Set parameters as in paper
svm_eps   &lt;- 0.05
svm_cost  &lt;- 0.005
sigma     &lt;- 0.02
svm_gamma &lt;- 1/(2*sigma^2)


#SVM to get u_t
svm     &lt;- svm(x = x[,-1], y = x[,1], scale = FALSE,
               type = ""eps-regression"", kernel = ""radial"",
               gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

u    &lt;- svm$residuals  #Extract u_t
n    &lt;- 60  #Size of test set
u_tr &lt;- u[1:(nrow(u) - n)]  #Subset to training set
u_tr &lt;- na.omit(cbind(u_tr, lag(u_tr)))^2  #Final training set


#Recurrent SVM for vola estimation
i       &lt;- 1
p_count &lt;- 0

while(p_count &lt; 5){

  print(i)  #Print number of loops

  #Estimate SVM for u^2
  svmr     &lt;- svm(x = u_tr[,-1], y = u_tr[,1], scale = FALSE,
                  type = ""eps-regression"", kernel = ""radial"",
                  gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

  #Test autocorrelation of residuals to lag 1
  test    &lt;- Box.test(svmr$residuals, lag = 1, type = ""Ljung-Box"")
  p_val   &lt;- test$p.value
  p_count &lt;- ifelse(p_val &gt; 0.1, p_count + 1, 0)

  #Extract residuals for next estimation step
  w        &lt;- svmr$residuals
  w        &lt;- c(0, w[-length(w)])  #lag 1

  u_tr &lt;- cbind(u_tr[,1:2], w)

  i &lt;- i + 1
}
</code></pre>
"
"0.101015254455221","0.104828483672192","215207","<p>I have a question regarding Dynamic regression linear models.
I wonder if it is possible to implement a MLR model (in R) using 'lm' and creating lagged values of predictors and dependent variables.
For example, considering the linear regression described at <a href=""https://www.otexts.org/fpp/5/1"" rel=""nofollow"">https://www.otexts.org/fpp/5/1</a> 
 (""Forecasting: principles and practice""), can I retrive lagged values for savings, income, etc. and the same output ""score"" variable and build a model like below?</p>

<p>require(dplyr)
log.savings_lag1&lt;-lag(log.savings, 1)
........
score_lag1&lt;-lag(score,1)</p>

<p>fit &lt;- lm(score ~ log.savings + log.income +
log.address + log.employed+ log_savings_lag1,score_lag1, data=creditlog)</p>

<p>If this feasible? There is any particular concern I have to be aware?
Thanks in advance!</p>
"
"0.377964473009227","0.364215679542342","234076","<p>I am looking to do time series forecasting with multiple variables. For example a data frame (df) of 4 different time series might look like this, where each column is its own time series: </p>

<pre><code>    X1 X2 X3 X4
1   4 13  2 81
2  24 91 86 58
3  21 97 39  1    
4   1 56 79 55
5  63  6 91 79
6  66 96 95 81 
</code></pre>

<p>Let's say X1 is 'cost' and the other variables are things like temperature, volume, and #_of_people.</p>

<p>I would like to forecast 'cost' using the other 3 variables. I imagine using something like <strong>Vector Autoregressive Models (VAR) for Multivariate Time Series</strong> can be used to see how each variable impacts the other in each separate time series (one for each variable). </p>

<p>For example, using the <strong>vars</strong> package in r we can run forecasts against the 4 time series and plot the results: </p>

<pre><code>var.2c &lt;- VAR(df, p = 2, type = ""const"")
var.2c.prd &lt;- predict(var.2c, n.ahead = 8, ci = 0.95)
fanchart(var.2c.prd)
</code></pre>

<p><a href=""http://i.stack.imgur.com/lq4VD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lq4VD.png"" alt=""enter image description here""></a></p>

<p>As I understand it, 4 separate regression models were built, one for each variable, where all the other variables were considered for each one. In other words, 'cost' was forecasted, taking into consideration not just the 'cost' trends, but also the impact the other 3 variables (X2, X3, and X3) had on 'cost.'</p>

<p>My question is, say I wanted to take a date in the future, on the forecast of cost, and see what happens to that forecasted value when temperature is increased (X2). I am assuming I can just take the coefficients in the 'cost' regression model that was used to forecast 'cost' using VAR, and use them as you would normally. For example, if the coefficient says the 'cost' will increase by 5 dollars for every one unit increase in temperature (X2), then I could take the forecasted value at the date of interest and add the $5, to say that is what would happen to the forecasted value if X2 were to change. </p>

<p>Are my intuitions correct here or am I missing something? Are there better ways to run 'what-if' analyses on forecasted multivariate time series?</p>
"
"0.142857142857143","0.14824986333222","235422","<p>I am migrating from Weka+Pentaho forecasting and I am trying to get a regression model working in R.</p>

<p>As for my data, it is a time series of network utilization (second column).  How to make sure that I make use of timestamp?</p>

<p>I know that discretization becomes inconsistent at one point but this should not be related to the problem. In Weka I realigned discretization to appear linear. Need to fix this in R if this will cause issues.</p>

<p>Here is two samples from a set of 22k+ records sitting in a list of two columns:</p>

<p><code>2015-12-21 04:11:56          87
 2015-12-21 04:16:56          82
 2015-12-21 04:21:56          76
 2015-12-21 04:26:56          88
 2015-12-21 04:31:56          83</code></p>

<p><code>21999 2016-03-03 23:03:16          59
22000 2016-03-03 23:08:16          51
22001 2016-03-03 23:13:16          58
22002 2016-03-03 23:18:16          42
22003 2016-03-03 23:23:16          56
22004 2016-03-03 23:28:16          53
22005 2016-03-03 23:33:16          61</code>
I suspect I may confuse dimensions here... </p>

<p>I succeeded with SVM and KNN models in Weka, So far, neither is working in R. Below is my attempt to predict with KKNN library which fails.</p>

<p><code>library(kknn)
 fit = train.kknn(utilization~., d)
 predict(fit)
</code></p>

<p>The snippet above fails with the following message:
<code>Error: C stack usage  7969804 is too close to the limit</code></p>

<p>I need some guidelines on R pipeline to get at least a rough model and produce a prediction of N points, and calculate prediction performance metrics.</p>
"
