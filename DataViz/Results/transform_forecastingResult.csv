"V1","V2","V3","V4"
"0.377964473009227","0.377964473009227"," 28737","<p>I have time series as </p>

<pre><code>0.4385487 0.7024281 0.9381081 0.8235792 0.7779642 1.1670665 1.1958634 1.1958634 0.8235792 0.8530141 0.8802216 1.1958634 1.1235897 1.3542734 1.3245534 0.9381081 1.1670665 1.1958634 0.8802216 1.3542734 1.1670665 4.9167998 0.9651803 0.8221709 1.1070461 1.2006974 1.3542734 0.9651803 0.9381081 0.9651803 0.8854192 1.3245534 1.1235897 1.2006974 1.1958634 0.4385487 1.3245534 4.9167998 1.2277843 0.8530141 1.0018480 0.3588158 0.8530141 0.8867365 1.3542734 1.1958634 1.1958634 0.9651803 0.8802216 0.8235792 4.9167998 1.1958634 0.9651803 0.8854192 0.8854192 1.2006974 0.8867365 0.9381081 0.8235792 0.9651803 0.4385487 0.9936722 0.8821301 1.3542734 1.1235897 1.6132899 1.3245534 1.3542734 0.8132233 0.8530141 1.1958634 1.2279813 0.8354292 1.3578511 1.1070461 0.8530141 0.9670581 1.1958634 0.7779642 1.2006974 1.1958634 0.8235792 1.3245534 0.5119648 2.3386331 0.8890464 0.8867365 4.9167998 1.2006974 1.2006974 0.6715839 4.9167998 0.7747481 4.9167998 0.8867365 1.2277843 0.8890464 1.2277843 0.8890464 1.0541099 0.8821301 
</code></pre>

<p>I am using package ""itsmr""-autofit(),""forecast""-auto.arima(),""package""--functions</p>

<ol>
<li><p>Autoregressive model</p>

<pre><code>&gt; ar(t)

Call:
    ar(x = t)

    Order selected 0  sigma^2 estimated as  0.9222 
</code></pre></li>
<li><p>ARMA model</p>

<pre><code>&gt; autofit(t)
    $phi
    [1] 0

    $theta
    [1] 0

    $sigma2
    [1] 0.9130698

    $aicc
    [1] 279.4807

    $se.phi
    [1] 0

    $se.theta
    [1] 0
</code></pre></li>
<li><p>ARIMA model</p>

<pre><code>    &gt; auto.arima(t)
    Series: t 
    ARIMA(0,0,0) with non-zero mean 

    Coefficients:
          intercept
             1.2623
    s.e.     0.0951

    sigma^2 estimated as 0.9131:  log likelihood=-138.72
    AIC=281.44   AICc=281.56   BIC=286.67
</code></pre>

<p>The auto.arima function automatically differences time series: we don't have to worry about transformation.</p>

<pre><code>&gt; auto.arima(AirPassengers)
Series: AirPassengers 
ARIMA(0,1,1)(0,1,0)[12]                    

Coefficients:
          ma1
      -0.3184
s.e.   0.0877

sigma^2 estimated as 137.3:  log likelihood=-508.32
AIC=1020.64   AICc=1020.73   BIC=1026.39`
</code></pre></li>
</ol>

<p>Which model should I select to get p,q values &amp; for forecasting purpose?</p>
"
"0.267261241912424","0.267261241912424"," 32657","<p>I was playing with the <a href=""http://cran.r-project.org/web/packages/TSA/index.html"" rel=""nofollow"">TSA</a> package in R and wanted to test the <code>arimax</code> function to the solution provided in Pankratz's <em>Forecasting with Dynamic Regression Models</em>, chapter 8. The savings rate and the function seems to provide similar results as the ones in the book except for the IO weights which are quite different. I bet there is a transformation that I might be missing.</p>

<p>Any help on understanding why IO coefficients are so different would be appreciated...</p>

<p>the solution states </p>

<pre><code>AO @ t=82,43,89
LS @ t=99
IO @ t=62,55
</code></pre>

<p>with Parameters estimates</p>

<pre><code>C = 6.1635
w82 = 2.3346
w99 = -1.5114
w43 = 1.1378
w62 = 1.4574
w55 = -1.4915
w89 = -1.0702
AR1 = 0.7976
MA2 = -0.3762
</code></pre>

<p>To fit the model in R, I used
(<code>saving</code> is the data)</p>

<pre><code>arimax(saving, order = c(1,0,2), fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA), io=c(55,62), 
       xreg=data.frame(AO82=1*(seq(saving)==82),
                       AO43=1*(seq(saving)==43),
                       AO89=1*(seq(saving)==89),
                       LS99=1*(seq(saving)&gt;=99)),
       method='ML')
</code></pre>

<p>The savings rate data is (100 points)</p>

<p>4.9
5.2
5.7
5.7
6.2
6.7
6.9
7.1
6.6
7
6.9
6.4
6.6
6.4
7
7.3
6
6.3
4.8
5.3
5.4
4.7
4.9
4.4
5.1
5.3
6
5.9
5.9
5.6
5.3
4.5
4.7
4.6
4.3
5
5.2
6.2
5.8
6.7
5.7
6.1
7.2
6.5
6.1
6.3
6.4
7
7.6
7.2
7.5
7.8
7.2
7.5
5.6
5.7
4.9
5.1
6.2
6
6.1
7.5
7.8
8
8
8.1
7.6
7.1
6.6
5.6
5.9
6.6
6.8
7.8
7.9
8.7
7.7
7.3
6.7
7.5
6.4
9.7
7.5
7.1
6.4
6
5.7
5
4.2
5.1
5.4
5.1
5.3
5
4.8
4.7
5
5.4
4.3
3.5</p>

<p>here it is my output</p>

<pre><code>&gt; arimax(saving, order = c(1,0,2),fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA),io=c(55,62),xreg=data.frame(AO82=1*(seq(saving)==82),
+ AO43=1*(seq(saving)==43),AO89=1*(seq(saving)==89),LS99=1*(seq(saving)&gt;=99)),method='ML')

Call:
arimax(x = saving, order = c(1, 0, 2), xreg = data.frame(AO82 = 1 * (seq(saving) == 
    82), AO43 = 1 * (seq(saving) == 43), AO89 = 1 * (seq(saving) == 
    89), LS99 = 1 * (seq(saving) &gt;= 99)), fixed = c(NA, 0, NA, NA, NA, NA, 
    NA, NA, NA, NA), method = ""ML"", io = c(55, 62))

Coefficients:
         ar1  ma1     ma2  intercept    AO82    AO43     AO89     LS99    IO-55   IO-62
      0.7918    0  0.3406     6.0628  2.3800  1.1297  -1.0466  -1.4885  -0.5958  0.5517
s.e.  0.0674    0  0.1060     0.3209  0.3969  0.3780   0.3835   0.5150   0.4044  0.3772

sigma^2 estimated as 0.2611:  log likelihood = -75.57,  aic = 169.14
</code></pre>
"
"0.56980288229819","0.56980288229819","116145","<p>I have downloaded the daily stock Adjusted Close price of one stock from sep 2011 to till date. As per my study plan, I have plotted some basic plots to understand the daily stock Adjusted closing price.</p>

<p>Here is the xyplot of the stock closing price by date and the code used to plot(My x axis not visible).</p>

<pre><code>Stock_T=stocks[which(symbol=='Stock_T'),]
xyplot(Adj.Close~Date,type='l',data=Stock_T,main='Adj.Close Price of the Stock_T')
</code></pre>

<p><img src=""http://i.stack.imgur.com/Ivlmk.png"" alt=""Timeseries plot of the raw data- Adjusted Closing price of the Stock""></p>

<p>By seeing this plot, the closing price was stable for period but had sudden huge increase in the stock price, it might had some other indicator which caused this much change in the stock price. Now my objective is to learn some ARIMA modeling concepts using this stock prices and try to do some forecasting of the stock price for few weeks. </p>

<p>As I have basic knowledge in ARIMA modeling, and I learned in the books that we should have stationary series before applying the ARIMA Model.</p>

<p>So, now I have plotted the ACF and PACF of the above raw data timeseries.</p>

<pre><code>acf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/rpy8S.png"" alt=""Raw data ACF Plot""></p>

<pre><code>pacf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/QKQge.png"" alt=""Raw data PACF plot""></p>

<p>From the above ACF and PACF plot, the series is not stationary and have huge autocorrelation (please correct me if am wrong), by differencing the series we will have stationary series (please correct me if am wrong). Here is the below plot.</p>

<pre><code>Stock_T_d1=diff(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/sD9Tj.png"" alt=""First difference of the raw series""></p>

<p>Here the differencing series and its ACF AND PACF plots. ACF plot shows that there is no auto correlation and the series is stationary (please correct me if I am wrong) but I am unable to interpret the PACF plots, can someone explain it to me?  </p>

<p><img src=""http://i.stack.imgur.com/cAoVf.png"" alt=""ACF plot of Difference series""></p>

<p><img src=""http://i.stack.imgur.com/U10g8.png"" alt=""PACF plot of Difference series""></p>

<p>The above difference series shows some unequal variance in the series and so I am taking log transformation before differencing and its ACF and PACF.</p>

<pre><code>Stock_T_logd1=diff(log(Stock_T$Adj.Close))
</code></pre>

<p><img src=""http://i.stack.imgur.com/MWovq.png"" alt=""Difference Logged series ""></p>

<p><img src=""http://i.stack.imgur.com/oYd2d.png"" alt=""ACF of Difference logged Series""></p>

<p><img src=""http://i.stack.imgur.com/HQxZw.png"" alt=""PACF of Difference logged Series""></p>

<p>Now I will try to ask my questions.</p>

<ol>
<li>Should we have stationary series before we apply ARIMA?</li>
<li>Could you please explain me the ACF and PACF of the original series, and what we should do if we have this kind of series?</li>
<li>Could you please explain me the ACF and PACF of the difference series, and what will be the next step?</li>
<li>Could you please explain me the ACF and PACF of the difference logged series, and what will be the next step?</li>
<li>Should we use difference series or difference logged series?</li>
<li>What will be the ARIMA orders of this series?</li>
<li>Is there any R code to find the ARIMA order automatically of the original series?</li>
</ol>
"
"0.327326835353989","0.327326835353989","122482","<p>I want to create a code which tests absolutely everything for time-series forecasting accuracy.</p>

<p>The current tests that I do are:</p>

<p><code>bptest()</code> - tests against heteroskedasticity (to test if series should be transformed)</p>

<p><code>gqtest()</code> - tests against heteroskedasticity (to test if series should be transformed)</p>

<p><code>RMSE</code> - of in-sample <code>sqrt(mean((data-fitted values)^2))</code></p>

<p><code>MAPE</code> - of in-sample <code>mean(abs(100*(data-fitted values)/data))</code></p>

<p><code>MAPE</code> - of out-sample (removing the last 4 data points and running model) <code>mean(abs(100*(last 4 data points-the 4 forecasted points)/last 4 data points))</code></p>

<p><code>MAPE</code>- of forced out-sample (removing the last 4 data points and running model keeping the same parameters as the model of the full data)</p>

<p><code>ACF and PACF</code> - of residuals <code>data-fitted values</code></p>

<p>Here is a link that will show you my code so you can see what i have done so far and where/what i can add. I dont want to miss anything so as many suggestions as possible please! If you could explain them a little too that would be fab. </p>

<p><a href=""http://bit.ly/10iUY5Z"" rel=""nofollow"">http://bit.ly/10iUY5Z</a></p>
"
"0.718381116519239","0.760638829255665","123576","<p>I am trying to test the effect on the heat flux between indoors and outdoors before and after removing insulation.</p>

<p>Briefly, I have 26 sensors on a wall, measuring heat flow between indoors and outdoors over a number of days. The wall was part of a real world experimental setup so that the insulation on the wall was removed halfway through the experiment. Â What I care about is to have a measure of the effect of the removal of the insulation (I am not interested in any form of forecasting). Â I am exploring the use of a SARIMA/ARIMAX models with one regressor because, aside from the removal of the insulation, the heat flow between indoors and outdoors was affected by daily cyclical and random environmental effects (heating on or off, daily temperature changes, wind, etc).  Here I will present that data and analysis of one sensor.  My data has been collected hourly, and I have transformed the variable â€˜insulatedâ€™ â€˜not insulatedâ€™ as a factor of 0s and 1s as indicator.</p>

<pre><code>heat.flux = c(8.677048,6.558642,5.920314,5.583614,5.373176,5.253928,4.938272,7.358305,9.743266,10.46577,11.06201,10.90067,11.49691,13.15236,12.10017,10.60606,10.45875,10.03788,9.588945,9.287318,8.578844,8.024691,10.26936,11.8757,10.20623,8.634961,8.305275,8.101852,8.12991,7.947531,7.814254,10.40264,13.08221,14.3729,14.94809,15.08838,15.20763,15.75477,14.57632,12.79461,11.97391,10.97082,10.33249,9.701178,9.715208,9.083895,10.63412,12.07912,9.736251,7.638889,6.453423,5.983446,5.499439,5.099607,4.70679,6.972503,9.259259,9.981762,10.24832,10.17116,10.27637,10.27637,9.546857,7.568743,7.168911,6.867284,6.705948,6.916386,8.319304,8.424523,11.41274,13.52413,11.70034,9.532828,8.957632,9.07688,9.694164,9.301347,9.048822,12.28255,14.95511,15.22868,15.24972,15.12346,15.08838,15.17256,13.68547,12.18434,12.1633,12.13524,11.81257,11.58109,11.44781,11.27946,13.87486,15.92312,14.07828,11.90376,10.46577,9.518799,8.978676,8.803311,8.684063,11.65123,14.39394,15.69865,16.61756,16.828,16.83502,16.16863,14.23962,12.19837,12.09315,11.5881,11.20932,10.50786,10.59203,10.64815,13.51712,15.71268,13.92396,12.10718,12.2615,11.65123,11.05499,10.31846,9.834456,12.9349,15.41807,15.78283,15.8179,16.11953,15.95118,15.63552,13.1243,11.22334,10.21324,8.705107,7.526655,6.15881,5.30303,5.597643,8.599888,11.17424,9.631033,8.038721,7.638889,7.203984,7.161897,6.76908,6.888328,9.518799,12.40881,13.21549,14.28872,14.43603,14.8078,14.81481,13.60129,12.59119,11.86167,11.91779,11.73541,12.04405,11.51796,11.74242,13.7486,15.85999,14.84989,12.63328,10.68322,9.343434,8.592873,8.333333,8.445567,10.97783,13.82576,15.12346,16.58249,17.61364,18.30808,19.10774,17.97138,16.62458,15.867,16.07744,15.63552,16.0073,15.42508,15.01122,17.10157,18.94641,22.44669,18.94641,16.01431,14.55527,13.88889,12.77357,11.66526,12.46493,15.41807,16.75786,17.27694,17.03143,16.84905,16.828,16.02834,16.35802,16.04237,15.03928,14.00112,14.1344,13.86785,13.99411,15.30584,18.20286,19.49355,16.16162,14.05022,12.05107,12.27553,13.01207,12.5491,13.72054,16.91218,18.62374,18.79209,20.80527,19.50758,20.18799,20.63692,18.49747,17.25589,17.38215,18.40629,18.60269,19.12177,18.66582,21.09989,24.45286,26.71156,23.54798,20.01964,17.98541,14.83586,14.31678,15.15152,15.30584,17.95735,19.71801,20.30724,20.19501,20.2862,20.1459,20.10382,18.20988,16.54742,15.22868,13.96605,12.71044,11.61616,10.71829,12.12121,14.77273,14.04321,12.44388,10.94978,10.2413,9.708193,9.638047,9.322391,11.27245,14.24663,14.77273,14.75168,14.92705,15.47419,15.48822,14.73765,13.68547,12.65432,12.35269,12.34568,12.32464,12.7385,12.84371,14.16947,17.34007,17.09456,15.0954,13.40488,11.70735,10.8165,10.64815,12.01599,13.55219,16.7298,17.45932,17.61364,19.58474,20.02666,19.79517,19.38833,17.32604,16.11953,15.62851,15.01122,14.70258,14.5693,14.35887,16.28086,18.69388,18.92536,16.56846,15.97222,13.34877,12.81566,12.04405,13.23653,14.1835,16.75786,17.55752,17.98541,18.85522,18.8482,19.02357,18.96044,17.31201,15.42508,14.38692,13.57323,12.36672,12.03002,11.41274,13.15236,15.88103,14.66049,12.8858,11.67228,11.03395,9.399551,8.375421,8.073793,10.6271,13.57323,13.61532,14.31678,14.73765,15.08838,15.62149,16.6807,15.28479,14.07127,13.14534,12.61223,12.57015,12.02301,12.17031,14.33782,18.83418,20.45455,18.67985,18.40629,16.51235,14.45006,14.61841,15.20763,15.57941,18.06958,19.88636,20.51066,21.633,23.24635,24.28451,24.70539,24.19332,22.81145,21.97671,21.58389,21.3945,21.21212,20.89646,21.1069,23.86364)

insulation = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
</code></pre>

<p>First off, the time series plot of the heat flux is this (the red line is when the insulation is removed):</p>

<p><img src=""http://i.stack.imgur.com/SYSQj.jpg"" alt=""Time series plot of heat flux""></p>

<p>Than this are the ACF and PACF plots of the same data:</p>

<p><img src=""http://i.stack.imgur.com/7keT7.jpg"" alt=""ACF and PACF of the data""></p>

<p>For my data, an <code>stl()</code> decomposition, run as <code>stl(ts(heat.flux, frequency = 24), 'period')</code></p>

<p>shows a strong â€˜seasonalâ€™ (i.e daily) component and a trend in the series. Â </p>

<p><img src=""http://i.stack.imgur.com/CUsta.jpg"" alt=""STL of the data""></p>

<p>Firs off I am trying to determine the best parameters for a SARIMA or ARIMAX model so that I can get an estimation of the effect removing the insulation. Despite the fact I can produce the ACF and PACF plots there is no way I can figure out the proper orders, so I load the library <code>forecast</code> and I run:</p>

<pre><code>library(forecast)
auto.arima(ts(heat.flux, frequency = 24), xreg = insulation, max.p = 10, max.q = 10, max.P = 10, max.Q = 10, stationary = F)Â 
</code></pre>

<p>The reason why I do not specify a stationary model is because of the trend I see with <code>stl()</code> and because I assume an effect of removing the insulation.</p>

<p>from <code>auto.arima()</code> I get:</p>

<pre><code>Series: ts(heat.flux, frequency = 24) 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept  carp.hour$interv
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449            4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075            0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=840.55   AICc=841.03   BIC=876.11
</code></pre>

<p>If I try to use the <code>TSA</code> package and use <code>arimax()</code> with those orders I get basically the same stuff:</p>

<pre><code>library(TSA)
arimax(ts(heat.flux, frequency = 24), xreg = insulation, order = c(2,0,2), seasonal = list(order = c(1,0,1), frequency = 24))
Series: x 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=838.55   AICc=839.03   BIC=874.11
</code></pre>

<p>And all is apparently well (Irrespective of the function I choose I get an estimate of the effect of the removal of the insulation and a se with it with is what I want). Â Unfortunately, when I test the fit of this model with the function <code>sarima()</code> from the <code>astsa</code>package I get significant Ljung-Box p-values for all my sensors and for all the lags:</p>

<pre><code>library(astsa)
sarima(ts(heat.flux, frequency = 24), p = 2, d = 0, q = 2, P =1, D = 0, Q = 1, S = 24, xreg = insulation)
$fit

Call:
stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, 
Q), period = S), xreg = xreg, optim.control = list(trace = trc, REPORT = 1, 
reltol = tol))

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood = -411.28,  aic = 840.55
</code></pre>

<p>but the plot that comes with is shows that at every single lag the Ljung-Box statistics is significant:</p>

<p><img src=""http://i.stack.imgur.com/ZMGKN.jpg"" alt=""SARIMA""></p>

<p>What is going on?  To sum it up:</p>

<ol>
<li>which of these models is the most correct to estimate the effect of insulation?</li>
<li>why are the Ljung-Box p-values all significant?  I would have though that the ARIMA/ARIMAX/SARIMA would have sorted that issue</li>
<li>If the orders calculated by <code>auto.arima()</code> are the problem, how could I find them in a different way (which is computationally feasible and does not take days).</li>
</ol>

<p>Finally, two notes.  I also have collected variables such as internal and external temperatures, windspeed, etc, but I would have though that integrating these in the model would be superfluous given the fact it is already an ARIMA model to start with.  Second, I am not at all wedded to this kind of analysis, but I am aware that a straightforward linear model would not be acceptable given the autocorrelation between the data points.</p>
"
"0.327326835353989","0.327326835353989","143129","<p>My data set in R contains the values like</p>

<pre><code>a   b  c
15  30  15
10  40  19
19  10  41
40  25  27 
</code></pre>

<p>I have used the formula <code>forecast(sample$a,h=5)</code> after invoking <code>require(forecast)</code>. Got the negative forecasting values. </p>

<pre><code>Point Forecast       Lo 80     Hi 80      Lo 95     Hi 95
5       39.99787    1.036967  78.95877  -19.58769  99.58343
6       39.99787  -22.550980 102.54672  -55.66234 135.65808
7       39.99787  -47.693434 127.68917  -94.11441 174.11015
8       39.99787  -76.830999 156.82674 -138.67647 218.67221
9       39.99787 -111.825136 191.82088 -192.19538 272.19112
</code></pre>

<p>To avoid this I have used log values for forecasting and as a result I got the new values like   </p>

<pre><code>forecast(log(sample$a),h=5)

  Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95
5       3.688805 3.007466 4.370144 2.646787 4.730823
6       3.688805 2.725294 4.652316 2.215243 5.162367
7       3.688805 2.508770 4.868840 1.884097 5.493513
8       3.688805 2.326229 5.051381 1.604926 5.772684
9       3.688805 2.165407 5.212203 1.358969 6.018641 
</code></pre>

<p>If I look at the forecasted values before and after the log transformation, lot of difference is there. Can someone assist me which one is appropriate between these to forecasting values? Which values should I consider as a forecasted values for the variable 'a'?</p>
"
"0.566946709513841","0.566946709513841","145251","<p>Sorry in advance if this is too basic of a question - I've been struggling with this data set for almost a month and feel like I'm going in circles, and the more I Google the more confused I get.</p>

<p>I have a time series of hourly activity levels (mean of 7 persons) for a period of about 2 months (1704 observations). There is obviously a strong ""seasonal"" component (freq=24) to this time series, with activity showing regular fluctuations between night and day. I am ultimately hoping to compare my activity time series to three other time series of environmental variables, to see how weather, temperature, etc affect people's activity on an hourly scale, following the methods in <a href=""http://cid.oxfordjournals.org/content/early/2012/05/21/cid.cis509.full"" rel=""nofollow"">this paper</a>. I'm not planning on doing forecasting, just wanting to know if these explanatory variables are affecting activity, and if so, how.</p>

<p>The paper linked above did their analysis in a few steps, if I understand correctly:</p>

<ol>
<li>Use stl to assess trend and seasonality.</li>
<li>Fit time series to ARIMA model.</li>
<li>Transform data into series of independent, identically distributed random variables</li>
<li>Choose best-fitting model by AIC</li>
<li>Use residuals for cross-correlating variables.</li>
</ol>

<p>Okay. Here are my questions:</p>

<ol>
<li><p>I can do step 1, but don't know how to relate that to step 2. Am I using the remainder from stl analysis for ARIMA modeling? If not, what's the point of step 1?</p></li>
<li><p>I understand how to choose some candidate models for ARIMA based on ACF, PACF, and auto.arima. But I can't get past the diagnostics. My Ljung-Box values are ALWAYS significant for ALL lags. Okay, so that means my residuals are correlated (I think). And since I want to use the residuals for cross-correlation, I assume that's bad. But no matter which models I try (I've tried maybe 6-10, is that enough?) I can't get good Ljung-Box p-values. The best fitting ARIMA so far (by AIC) is (1,0,2)x(1,1,2)24.</p></li>
</ol>

<p>Does this mean my time series doesn't fit an ARIMA model? How can I get iid residuals if I can't even get it to fit a model? Arrrghh.</p>

<p>So to be more succinct, my main question is: why do I always have these significant Ljung-Box values, and what can I do to fit a better model to get iid residuals?</p>

<p>Subsample of data (full set <a href=""https://www.dropbox.com/s/lhd9zu0x8r4o8pe/fitbit%20data.txt?dl=0"" rel=""nofollow"">here</a>):</p>

<pre><code>[1] 24 16 40 48 50 38 24  4  4  5  3  6  4  4  4  3 12 63 55 42 56 20 10 26 45 47 66 64 59
[30] 54 24  5  6  2  4  3  6 10  6  2 13 39 26 17 24 13 19 26 17 32 54 68 58 39 20  0  3  2
[59]  8  2  4  1  5 11  5 60 57 54 40 40 53 74 40 42 57 46 46 26  9  8  4  6 14  8  5  3  2
[88]  7 19 47 53 43 53 51 55 64 48 64 57 56 52 34 22  8  5  6  4  6  3  4  7  6 27 40 48 41
[117] 43 51 50 44 56 64 68 46 49 35 16  2 14  3  7  3 13  3  3  2 14 49 62 42 41 57 52 63 32
[146] 54 59 60 68 24 12  2  2  2  2  7  6  5  9 10 26 53 50 59 28 45 47 44 48 55 59 77 86 33
[175] 18 16 10  6  9  9 14  7  9  7  9 46 57 41 33 32 34 29 39 39 27 26  4 10  9  6  6  2  4
[204]  1  2  2  4  4 17 50 47 24 27 34 26 38 20  6 20 15 25  8  2  2  3  6  4  3  3  4  4  2
[233] 18 41 63 52 37 32 32 28 48 20  6 10  9  7  5 10  4  3  4  7  4  3  4 10  8 56 47 50 27
[262] 30 22 38 38 28 33 24 18 12 14  2 10  4 21  4  5  6  4  4 20 41 46 16  8 20 24 21 16 27
[291] 10  6 14  5  6  6 12  2 10  7  6  2  2  3 16 47 56 43 30 35 32 41 20 20 11 34 16  6 10
[320]  2  5 10  3 11  6  5  7  5 14 50 30 26 19 16 10  5 12 12 22 16 16 10  4  5  4  4  8 14
[349]  4  6  4  5 21 47 28 15  8 12 18 18 16 10  5  8 12  3  6  4  5 12 11  8  2  4  6 10 25
[378] 42 20 15  8 18 10 10  6 18 12  4  7  6  6  4  8 14  3 10 11  5 10  9 26 54 41 36 44  9
[407]  4  5  3  8 12 16 11 12 13 26  5 13 13  1  1  5 18  7 39 64 64 65 44 34 42 63 62 54 26
[436] 30 34 25 15  7  1  0  2  1  0  9 13 10 33 65 59 48 44 60 65 44 55 65 67 76 85 63 48  8
[465]  2  0  3  1  1  1  8 12 19 72 67 42 46 70 54 37 41 66 62 54 80 52 22  3  2  2  1  1  5
[494]  2  2  5 37 48 32 29 27 25 21  2 17  3 24  2  7  1  1  4  7  8  7  4  3  6  2  4 26 28
[523] 15  6  2  4  1 12  4  2  4 14 11  2  5  1 13 16 10  5 14  1  2  3 13 24 29 20 12  8  4
[552]  8  1 11  8 10  6  4  6  1  6  8  4  7 18 17 12  3 18 50 25 27 20 14 14  9 14 14 15  5
[581]  8  3  4  3  3 11 12 12  4 19 25  8 33 53 61 49 50 34 38 45 76 65 72 53 84 65 51 19  4
[610]  2 11  7  5  3  6  3 38 85 83 72 58 77 78 63 73 64 56 22  3 10 13 10  2  1  1  0  8  6
[639]  5  2 34 54 56 54 14  5 17 18 21  3 14 14  6  4  1  2  4 10  7  3  3  4 12 17 54 68 49
[668] 51 38 11 29 17  1  2  4  8  9  6  4  3 14  0  1 10  8  4  3  3 25 31  9  9 10  6  8  9
[697]  4 11  4  6  3  9  0  2  4  1 10 20 11  2  8  4 28 35 40 34 36 19 19 15 23 14  6  4  2
[726]  6  5  4  2  4  4  2  8 13 17  4 44 30 23 22 11  5 10 12  6  8 11  1 12 10  1  2  0  6
[755]  6  3  4  9  1  9 13 41  8  6  9 13 28  7  2  8  7  2  3  6  1  2  5  4  4  4  2  5  9
[784]  9 28 53 40 28  6  8  1  7  2 13 20  7  3  8  4  2  2  6  3  5 16  8  2 14 16 41 20 22
[813]  7  8 10 24 23 24 19 14  5  1  1  2  9  0  6  2 15  8  4  5 26 28  9  9 16 30 11 12  7
</code></pre>

<p>ACF/PACF after taking 24th difference: </p>

<p><img src=""http://i.stack.imgur.com/1SWHy.png"" alt=""ACF/PACF of time series after taking 24th difference""></p>

<p>Diagnostics of SARIMA(1,0,2)x(1,1,2)24 model (best model by AIC and as suggested by auto.arima):</p>

<p><img src=""http://i.stack.imgur.com/Tp70f.png"" alt=""enter image description here""></p>
"
"0.327326835353989","0.327326835353989","163371","<p>I made a time series decomposition with tbats. There is weekly and yearly seasonality in the data (and maybe also monthly - not really important for the question)</p>

<pre><code>x &lt;- msts(data, start=c(2005,1,1), seasonal.period=c(7,30.4,365.25))
fit &lt;- tbats(x, use.box.cox=FALSE)
plot(fit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/pxr6g.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pxr6g.png"" alt=""enter image description here""></a></p>

<p>As far as i understand the tbats result is an ""additive decomposition"", right?</p>

<p>I can now access the different parts of the tbats decomposition:</p>

<pre><code>level &lt;- as.numeric(tbats.components(fit)[,'level'])
season1 &lt;- as.numeric(tbats.components(fit)[,'season1'])
season2 &lt;- as.numeric(tbats.components(fit)[,'season2'])
season3 &lt;- as.numeric(tbats.components(fit)[,'season3'])
</code></pre>

<p>Since i suppressed 'Box-Cox transformation' earlier, level is roughly the same then trend according to a post from Rob J Hyndman  in the comments section here: <a href=""http://robjhyndman.com/hyndsight/dailydata/"" rel=""nofollow""> here</a></p>

<p>In order to get the remainder part of the decomposition is it a legit way to just subtract all the parts from the original data?</p>

<pre><code>remainder &lt;- data - level - season1 - season2 - season3
</code></pre>

<p>What do i get when i use this:</p>

<pre><code>y &lt;- resid(fit)
</code></pre>

<p>I am a bit confused right now about the right way to do it... Many thanks for all your input!</p>

<p><strong>Update:</strong></p>

<pre><code>resid(fit)
fit$errors
</code></pre>

<p>Those two are both the same. I guess these values are related to the tbats method. I am doubting that i can take them as the ""remainder"" of the decomposition? Is there a way to extract the remainder out of the tbats method? In his paper '<em>Forecasting time series with complex seasonal patterns using exponential smoothing</em>' Rob J Hyndman shows remainder graphs for the tbats method so that's why i think it is possible to get in R as well.
Anyone any thoughts about that?</p>
"
"0.188982236504614","0.188982236504614","167944","<p>I have the following time series of <em>count data</em>:</p>

<pre><code>x &lt;- ts(c(21337, 56994, 95497, 138829, 146346, 157182, 128136,
          104615, 103659, 102082, 109968, 113945, 118067, 93867, 54930))
</code></pre>

<p>To which I have associated the following model</p>

<pre><code>&gt; library(forecast)
...
&gt; ets(x)
ETS(A,N,N) 

Call:
 ets(y = x) 

  Smoothing parameters:
    alpha = 0.9999 

  Initial states:
    l = 105466.6663 

  sigma:  32125.45

     AIC     AICc      BIC 
355.9429 356.9429 357.3590 
</code></pre>

<p>Which gives me negative prediction boundaries at 95% confidence:</p>

<pre><code>&gt; forecast(ets(x), level = .95)
   Point Forecast       Lo 95    Hi 95
16       54933.94   -8030.795 117898.7
17       54933.94  -34107.138 143975.0
18       54933.94  -54116.824 163984.7
...
</code></pre>

<p>Since we're dealing with count data, I've decided to hide the negative values from my final plot:</p>

<pre><code>plot(forecast(ets(x), level = .95), ylim = c(0, 260e3))
</code></pre>

<p><a href=""http://i.stack.imgur.com/Vp2wN.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Vp2wN.png"" alt=""plot""></a></p>

<p><strong>My questions are:</strong></p>

<ol>
<li><strong>How many Statistics professors have I just aggravated with that procedure?</strong></li>
<li><strong>How could I get away with such a model without having to resort to transforming my data (I'm trying to avoid the back-and-forth of log-transformation)?</strong></li>
</ol>

<p>Related questions:</p>

<ul>
<li><a href=""http://stats.stackexchange.com/q/92443/27433"">Can a mathematically sound prediction interval have a negative lower bound?</a></li>
<li><a href=""http://stats.stackexchange.com/q/143129/27433"">Getting Negative Forecasting Values</a></li>
</ul>
"
"0.188982236504614","0.188982236504614","184938","<pre><code>cola=read.csv(""/Users/ericali/Desktop/monthly sales of tasty cola.csv"",header=TRUE)
plot(cola,main=""Monthly Sales of Tasty Cola"", xlab=""Months"", ylab=""Amount"")
cola=ts(cola[1:36,2], start=c(1,1), frequency=12)
class(cola)

plot.ts(cola,main=""Monthly Sales of Tasty Cola"", xlab=""Months"", ylab=""Amount"")
plot(stl(cola,""periodic""))
</code></pre>

<p><a href=""http://i.stack.imgur.com/0LnD0.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0LnD0.jpg"" alt=""The following is my decomposition plot. I really need help with how to read it!""></a></p>

<p>Describe the data set, aim of modeling (e.g., forecasting).
I assume this is not a forecasting model... but not sure what it is.</p>

<p>Plot the time series, analyze (transformation needed, seasonality, trend, stationarity, etc.). 
I don't know how to plot and analyze ACF and PACF to preliminary and identify the model. Do I need to use Box-Cox Transformation? And how to see if it is seasonality, trend or stationarity?</p>
"
"0","0.188982236504614","203422","<p>I'm trying to forecast an auto.arima model like the code below.  The time series I'm forecasting needs to have a transformation applied to it.  I was wondering if I also need to apply the same transformation to the xreg predictors, like I have done in the example below?</p>

<p>Code:</p>

<pre><code>series&lt;-BoxCox(OldSeries,0.27)
Train &lt;-series[1:700]
Validation &lt;- series[701:1000]


Xreg&lt;-BoxCox(series2[1:1000],0.27)

##Predictor
xregTrain &lt;- Xreg[1:700]
xregVal &lt;- Xreg[701:1000]


fit &lt;- auto.arima(Train, xreg = xregTrain)

Fcast&lt;-forecast(fit, h=300, xreg = xregVal)
</code></pre>
"
"0.566946709513841","0.566946709513841","213192","<p>I have half-hourly electricity data of several homes for a duration of one month. Also, I have ambient temperature at same sampling rate. Now, I need to make half-hourly forecasts using historical electricity data and forecasted temperature for coming day.</p>

<p>Currently, I am facing a problem with seasonality, i.e., in some homes I observe (via visual inspection) day-wise seasonality and in some other homes no pattern is found. <a href=""http://stats.stackexchange.com/a/212797/60072"">Stephan</a> suggested to check for weekly seasonality as well, but visually I do not find any. So, I thought to try models with different forced seasonalities to find the prediction accuracy. I can think of two options:</p>

<ol>
<li>Model_1 with daily seasonality (frequency = 48 observations)</li>
<li>Model_2 with weekly and daily seasonality (48, 7*48)</li>
</ol>

<p>Keeping the above approach in mind, I am facing the following issue:</p>

<ol>
<li>For Model_1, I can use <code>auto.arima()</code> with <code>xreg</code> option to specify an extra <code>temperature</code> predictor. However, this only works for a single seasonality.</li>
<li>For Model_2, I tried to use the <code>tbats()</code> forecasting function, which models multiple seasonalities, but does not allow extra predictor variables. </li>
</ol>

<p>Is there a forecasting function which allows <em>both</em> multiple predictors and multiple seasonalities?</p>

<p>Here is the electricity data of one month. </p>

<pre><code>    data &lt;- structure(c(1642.8, 1467.1, 165.57, 1630.99, 1618.65, 1629.29, 
        1598.93, 1839.9, 1604.52, 1606.73, 1473.82, 1669.17, 1698.9, 
        2111.21, 2056.41, 3671.29, 2808.01, 1336.15, 794.11, 1212.15, 
        377.36, 888.54, 174.58, 218.54, 420.76, 389.58, 397.77, 395.31, 
        359.11, 364.8, 376.13, 389.37, 929.5, 1702.38, 519.65, 2452.28, 
        1354.45, 1842.96, 725.41, 661.11, 528.44, 733.4, 429.51, 310.47, 
        279.72, 407.83, 1791.1, 1754.53, 1536.73, 1608.37, 1432.23, 1401.72, 
        1582.14, 1558.75, 1536.24, 1745.59, 1375.61, 1556.71, 1671.12, 
        1206.77, 1391.84, 876.23, 1617.3, 1638.99, 1833.61, 1591.42, 
        1455, 183.87, 177.55, 184.36, 332.99, 352.95, 425.1, 945.67, 
        342.3, 348.45, 227.18, 382.15, 268.91, 335.88, 326.94, 233.23, 
        169.71, 179.51, 195.3, 207.23, 1681.9, 1493.32, 941.52, 980.36, 
        924.31, 379.02, 1229.89, 1590.21, 1250.92, 1149.24, 1124.04, 
        993.78, 883.98, 860.69, 934.17, 969.31, 1049.55, 1104.94, 904.3, 
        1220.23, 1183.9, 891.26, 825.33, 787.77, 1060.93, 1029.1, 982.25, 
        193.13, 182.65, 181.75, 167.68, 165.89, 291.02, 300.2, 418.4, 
        297.66, 231.89, 305.66, 701.18, 338.5, 337.24, 332.11, 332.66, 
        187.8, 179.03, 130.22, 177.73, 172.24, 173.45, 334.23, 810.53, 
        359.41, 330.23, 333.29, 568.85, 2462.46, 1660.32, 1156.13, 1136.2, 
        1189.07, 832.73, 181.91, 185.93, 1076.77, 672.78, 1376.71, 1020.17, 
        382.18, 1160.74, 791.36, 1569.4, 817.78, 850.71, 747.21, 826.12, 
        1306.46, 506.23, 140.05, 132.6, 304.19, 308.14, 406.13, 290.73, 
        188.9, 165.59, 174.56, 145.97, 151.83, 142.29, 443.58, 799.9, 
        279.36, 223.88, 221.03, 291.26, 374.97, 431.36, 598.98, 625.82, 
        1052.02, 2036.83, 1230.03, 1429.81, 1099.34, 1646.03, 1668.56, 
        1631.79, 1604.04, 2849.49, 2998.63, 2476.96, 1601.04, 1216.42, 
        2004.2, 1868.51, 1961.91, 1813.35, 1500.22, 1276.94, 1369.29, 
        632.43, 238.15, 488.76, 467.6, 330.27, 144.67, 153.36, 924.12, 
        1348.18, 799.01, 524.3, 420.5, 264.34, 283.86, 198.95, 206.52, 
        217.33, 356.16, 207.83, 197.6, 194.03, 193.01, 249.85, 271.22, 
        244.25, 442.5, 660.49, 245.66, 356.13, 443.32, 336.15, 849.74, 
        1709.21, 1542.09, 1315.69, 2628.6, 2261.8, 1576.42, 1776.48, 
        1239.64, 1401.12, 1106.17, 1378.55, 1315.57, 1141, 1642.63, 2484.13, 
        1968.94, 3059.42, 1317.32, 905.05, 484.42, 486.86, 96.79, 183.45, 
        173.94, 342, 255.96, 320.54, 106.19, 147.88, 150.77, 176.17, 
        344.75, 371.73, 309.46, 237.86, 187.32, 202.61, 292.5, 248.28, 
        259.3, 283.67, 365.09, 230.47, 326.15, 350.92, 335.42, 419.39, 
        345.31, 1093.22, 1392.87, 1298.11, 919.16, 1654.53, 1045.99, 
        558.42, 437.29, 857.9, 758.34, 1220.04, 1390.62, 956.74, 909.93, 
        584.67, 409.87, 387.3, 387.93, 1276.28, 871.06, 413.8, 313.2, 
        199.5, 330.21, 210.9, 358.28, 352.13, 233.62, 259.18, 123.57, 
        255.58, 411.78, 427.65, 318.95, 298.5, 283.23, 279.85, 200.45, 
        205.97, 254.24, 307.98, 1090.53, 289.71, 215.14, 286.63, 328.55, 
        288.96, 1281.19, 1354.8, 1302.05, 1254.46, 261.95, 270.09, 243.28, 
        696.61, 314.27, 241.73, 245.68, 157.74, 222.55, 294.36, 185.46, 
        203.49, 182.14, 246.69, 178.26, 397.5, 330.2, 212.02, 248.72, 
        265.48, 249.37, 130.59, 248.97, 279.94, 319.07, 358.5, 278.98, 
        251.92, 304.66, 455.05, 365.95, 340.93, 287.51, 264.82, 260.18, 
        34.35, 35.11, 184.09, 247.18, 160.9, 139.27, 284.96, 296.31, 
        252.3, 342.65, 353.03, 380.52, 346.19, 350.06, 218.52, 133.94, 
        173.7, 128.26, 167.8, 112.77, 147.8, 129, 170.54, 89.88, 243.08, 
        97.61, 190.31, 193.94, 268.17, 233.5, 205.27, 92.29, 167.43, 
        168.34, 151.99, 193.84, 379.1, 318.69, 327.28, 487.39, 414.01, 
        336.06, 278.02, 168.05, 155.6, 236.4, 264.94, 296.05, 326.46, 
        357.43, 356.31, 340.29, 319.81, 312.79, 341.53, 317.36, 309.62, 
        440.6, 285.5, 282.06, 288.99, 334.48, 196.54, 144.24, 218.55, 
        173.64, 242.29, 251.78, 186.81, 184.36, 141.62, 208.91, 157.53, 
        154.03, 139.44, 137.66, 256.75, 1202.05, 177.36, 177.93, 72.83, 
        252.9, 231.35, 1090.39, 442.91, 363.12, 248.96, 478.75, 249.64, 
        297.29, 227.28, 365.82, 879.7, 488.93, 184.79, 138.13, 151.77, 
        123.18, 175.76, 251.84, 208.06, 126.68, 246.3, 307.34, 319.79, 
        324.3, 379.6, 309.53, 253.17, 221.91, 228.42, 150.24, 148.59, 
        118.79, 86.89, 140.51, 200.43, 212.15, 276.14, 441.81, 125.77, 
        152.42, 329.28, 269.21, 177.35, 1106.29, 128.92, 96.35, 63.53, 
        520.62, 940.25, 1014.34, 314.99, 390.2, 330.1, 377.04, 341.35, 
        342.79, 241.79, 249.9, 391.92, 292.68, 105.02, 179.99, 118.53, 
        154.17, 90.53, 206.7, 345.33, 244.75, 291.68, 820.57, 1777.84, 
        1805.83, 1753.73, 1416.7, 279.2, 262.82, 1345.88, 467.98, 1136.66, 
        170.02, 159.96, 1478.8, 1414.12, 1347.93, 1505.59, 1341.69, 445.53, 
        277.59, 1609.61, 1476.45, 244.08, 192.57, 213.55, 439.02, 112.86, 
        128.54, 376.09, 251.15, 116.27, 254.82, 302.56, 304.6, 198.5, 
        240.05, 219.35, 70.3, 190.96, 211.95, 328.53, 714.62, 3176.56, 
        2604.09, 191.65, 145.25, 93.93, 83.6, 81.13, 146.12, 331.75, 
        250.24, 1144.53, 1616.47, 1008.7, 316.65, 311.46, 1152.99, 1504.86, 
        1543.21, 1081.54, 1428.07, 1358.23, 1349.75, 190.23, 2398.92, 
        2196.11, 1466.94, 2249.77, 2150.2, 2542.25, 618.03, 453.22, 880.99, 
        1497.86, 440.96, 161.85, 324.88, 434.11, 316.33, 444.66, 359.14, 
        277.41, 1237.28, 761.41, 183.53, 309.44, 213.48, 121.64, 346.7, 
        149.86, 2060.39, 1102.13, 347.97, 600.24, 912.6, 590.77, 1805.76, 
        1673.93, 1573.91, 505.74, 446.76, 1033.41, 1668.68, 1293.9, 383.81, 
        1419.99, 1349.4, 711.55, 218.63, 182, 401.93, 1876, 1486.34, 
        1543.11, 2313.8, 478.57, 615.19, 542.68, 971.98, 531.11, 766.21, 
        489.76, 344.47, 319.86, 321.26, 311.41, 288.67, 310.67, 305.15, 
        419.33, 422.84, 950.08, 2188.88, 3454.92, 1989.54, 590.33, 327.05, 
        354.78, 578.41, 1583.29, 2016.66, 1481.03, 293.21, 1864.84, 399.65, 
        366.76, 357.7, 2074.97, 1626.86, 1133, 1624.61, 1506.93, 628.4, 
        1405.68, 217.8, 1223.11, 1356.97, 1171.72, 1182.86, 1642.11, 
        2289.02, 814.39, 595.76, 542.78, 1596.41, 884.97, 235.25, 1540.68, 
        781.95, 115.71, 1204.98, 718.66, 452.09, 305.58, 444.67, 356.76, 
        182.54, 674.47, 153.8, 862.25, 1322.88, 323.33, 1659.64, 496.72, 
        304.74, 246.6, 327.12, 239.31, 246.72, 225.72, 234.7, 324.07, 
        304.27, 171.86, 97.64, 242.69, 295, 324.53, 513.81, 1100.65, 
        1151.77, 231.56, 189.88, 786.3, 1164.87, 676.09, 882.82, 1496.3, 
        1027.91, 872.92, 809.1, 840.31, 1302.18, 2055.87, 677.74, 934.66, 
        263.91, 186.68, 248.5, 214.62, 371.54, 298, 294.52, 304.86, 1295.77, 
        942.5, 305, 265.78, 255.89, 255.63, 151.54, 108.16, 116.81, 100.19, 
        224.75, 84.11, 1143.89, 262.15, 784.21, 1728.29, 1506.79, 434.94, 
        374.29, 265.43, 560.74, 1651.49, 1063.07, 1054.69, 1298.4, 1261.59, 
        1132.75, 692.82, 660.57, 198.25, 97.81, 1258.67, 833.64, 796.35, 
        868.76, 999.86, 2240.02, 885.72, 1317.52, 1267.18, 167.93, 133.22, 
        364.44, 267.17, 406.13, 412.52, 1036.04, 779.34, 655.43, 1901.2, 
        270.18, 266.31, 284.21, 288.66, 135.38, 176.11, 154.86, 160.21, 
        146.28, 163.72, 139.75, 278.12, 253.51, 319.62, 396.39, 1662.69, 
        1577.09, 1059.71, 241.64, 407.54, 290.49, 846.17, 1325.31, 1418.23, 
        1432.5, 1412.6, 1015.87, 1619.88, 1426.58, 1333.32, 1963.35, 
        1638.11, 1081.89, 285.56, 1084.58, 2038.77, 1022.39, 1145.92, 
        513.87, 107.7, 176.19, 143.77, 374.3, 373.99, 221.76, 148.3, 
        331.01, 2323.12, 1502.09, 347.17, 296.7, 306.06, 313.03, 221.69, 
        295.35, 301.95, 250.92, 231.54, 140.6, 717.63, 863.5, 402.15, 
        1337.78, 1575.44, 1738.49, 1675.57, 1617.66, 1365.58, 242.8, 
        286.29, 712.34, 1559.59, 1600.34, 3447.88, 3432.89, 3337.23, 
        1472.21, 1323.76, 1265.31, 1221.65, 1312.63, 2016.09, 2972.13, 
        1451.67, 735.67, 130.13, 379.87, 162.21, 226.48, 417.18, 357.51, 
        346.37, 200.89, 190.15, 276.05, 942.74, 1471.99, 1047.53, 1240.06, 
        742.62, 169.34, 144.28, 220.58, 165.23, 344.8, 227.47, 254.64, 
        742.37, 1809.01, 436.11, 1692.87, 1697.74, 1474.91, 1635.68, 
        1664.2, 1489.85, 1316.4, 1364.07, 1510.02, 1497.89, 1709.17, 
        2846.71, 2736.08, 1015.43, 1017.08, 1195.21, 751.18, 523.89, 
        199.69, 2148.71, 1151.39, 1182.84, 788.91, 259.31, 146.29, 141.09, 
        299.38, 349.53, 404.08, 449.9, 391.77, 251.89, 222.25, 281.04, 
        565.4, 371.24, 219.75, 324.45, 227, 157.67, 212.48, 201.69, 140.84, 
        220.67, 187.64, 399.79, 157.92, 275.49, 326.99, 1340.51, 1578.68, 
        1599.41, 1667.74, 1129.07, 1312.55, 1393.09, 1368.69, 1130.85, 
        968.71, 1130.2, 1223.1, 1124.5, 1077.09, 1052.42, 1255.31, 918.21, 
        1263.93, 706.21, 3080.29, 1620.18, 1122.48, 750.06, 262.89, 110.02, 
        236.83, 413, 227.53, 355, 277.51, 258.03, 368.44, 656.68, 1808.17, 
        1493.41, 1272.21, 330.67, 1674.18, 719.45, 1089.68, 784.01, 275.73, 
        312.69, 345.11, 761.8, 1020.11, 259.16, 345.98, 270.23, 580.15, 
        1303.68, 1659.7, 1732.94, 204.9, 373.58, 373.36, 1381.52, 1437.74, 
        1262.78, 1264.6, 1184.54, 1175.12, 857.09, 1428.34, 841.92, 232.47, 
        223.22, 473.24, 382.7, 189.84, 1737.48, 1689.34, 378.48, 872.56, 
        180.12, 363.03, 301.38, 412.57, 401.17, 387.35, 417.63, 300.61, 
        376.82, 284.31, 232.31, 269.96, 188.41, 203.79, 134.88, 193.66, 
        57.86, 89.71, 167.66, 60.84, 197.03, 703.66, 1638.7, 1467.03, 
        347.22, 1397.23, 1511.92, 1362.25, 1397.18, 1106.19, 826.5, 1033.04, 
        1039.46, 584.98, 706.35, 548.07, 373.39, 681.6, 1231.28, 288.94, 
        649.36, 79.28, 209.23, 290.75, 304.12, 132.57, 91.2, 355.37, 
        197.45, 343.17, 339.2, 284.65, 229, 234.73, 322.36, 323.43, 295.1, 
        197.03, 308.17, 223.88, 235.08, 225.16, 172.83, 236.26, 135.17, 
        394.89, 479.2, 315.34, 280.9, 282.36, 204.78, 367.24, 1712.29, 
        1521, 1686.09, 960.19, 1019.12, 1062.77, 851.88, 1369.98, 689.2, 
        580.5, 751.74, 547.67, 556.64, 493.85, 404.15, 428.07, 716.2, 
        1442.05, 1045.81, 1497.07, 567.59, 155.07, 537.72, 446.03, 282.57, 
        642.88, 409.37, 338.91, 173.89, 358.63, 195.42, 209.95, 186.44, 
        152.34, 105.51, 132.26, 82.14, 122.88, 149.38, 211.42, 350.17, 
        429.72, 336.4, 982.21, 1436.25, 1726.87, 1830.42, 1282.05, 1293.4, 
        1121.01, 946.2, 707.1, 154.53, 767.56, 607.78, 448, 288.23, 270.32, 
        223.93, 166.22, 262.45, 223.74, 159.31, 210.44, 257.94, 183.99, 
        151.38, 206.11, 193.43, 388.95, 577.98, 304.64, 285.13, 256.59, 
        420.26, 289.34, 356.02, 358.08, 325.22, 275.95, 164.46, 213.23, 
        142.99, 221.66, 270.61, 206.56, 213.68, 254.33, 250.15, 267.99, 
        403.95, 671.2, 1574.11, 396.34, 477.88, 631.08, 618.25, 1366.88, 
        298.19, 287.05, 290.38, 332.44, 235.9, 229.79, 831.6, 1320.86, 
        477.31, 944.84, 547.33, 411.21, 705.43, 873.49, 572.81, 585.36, 
        1229.69, 701.01, 653.49, 74.81, 162.47, 179.54, 330.27, 544.51, 
        332.41, 296.66, 130.66, 1055.61, 556.79, 265.43, 383.44, 398.22, 
        362.66, 223.99, 130.35, 193.67, 217.68, 273.3, 247.84, 161.66, 
        320.08, 322.52, 274.61, 811.44, 353.85, 323.41, 383.61, 389.5
        ), .Dim = c(1248L, 1L), .Dimnames = list(NULL, ""power""))
</code></pre>

<p>Temperature predictor values are:</p>

<pre><code>temp&lt;-structure(c(31, 31, 31, 31, 30, 29, 28, 27.5, 27, 26, 26, 26, 
26, 26, 26, 28, 29, 29, 30, 31, 32, 33, 33, 34, 34, 35, 36, 36, 
36.5, 37, 38, 38, 39, 39, 39, 38, 37, 36, 36, 35, 34, 33, 33, 
33, 32.5, 32, 32, 31, 30, 26, 24, 24, 24, 24, 24, 24, 25, 25, 
25, 25, 26, 26, 26, 27, 28.3, 29.7, 31, 32, 33, 33, 34, 34, 34, 
35, 35, 35, 36, 37, 38, 39, 39, 39, 39, 38.5, 38, 37, 36, 35, 
34, 34, 33, 32.5, 32, 30, 30, 30, 28, 28, 27, 25, 25, 24, 24, 
24, 24, 24, 25, 25, 25, 25, 25, 27, 28, 30, 31, 33, 34, 37, 37, 
38, 38, 39, 39.3, 39.7, 40, 40, 40, 40, 41, 40, 40, 38, 38, 36, 
35, 34, 33, 31, 31, 30, 31, 30, 30, 29, 28, 28, 28, 27, 27, 28, 
27, 24, 24, 24, 23, 23, 23, 24, 24, 26, 27, 28, 29.7, 31.3, 33, 
34, 35.3, 36.7, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 
39, 38, 37, 36, 34, 33, 32, 32, 30, 28.5, 27, 27, 27, 27, 27, 
26, 26, 26, 25.3, 24.7, 24, 24, 24, 24, 23, 23, 24.5, 26, 28, 
29.7, 31.3, 33, 34.5, 36, 37, 38, 39, 40, 40, 40.3, 40.7, 41, 
41, 41, 41, 41, 41, 41, 39.5, 38, 37, 37, 35, 34, 34, 33, 33, 
33, 32, 31, 31, 30, 30, 30, 29, 29, 28.5, 28, 28, 28, 29, 29, 
28.5, 28, 29, 29, 30, 31, 31, 32.5, 34, 35, 37, 39.5, 42, 42, 
42, 42, 42, 43, 44, 45, 45, 44, 43, 42, 42, 41, 39, 36.5, 34, 
34, 33, 33, 33, 33, 33, 33, 32, 32, 32, 32, 31.5, 31, 30, 30, 
30, 29, 29, 28, 27, 28, 29, 30, 31, 32.5, 34, 34, 35, 37, 40, 
41, 41, 42, 42, 42.5, 43, 43, 43, 42, 42, 43, 42, 42, 41, 40, 
39, 37.5, 36, 35, 35, 34, 34, 33, 32, 32, 32, 32, 31, 31, 31, 
31, 31, 28, 28, 28, 28, 28, 28.5, 29, 30, 30.5, 31, 32, 33.5, 
35, 36, 37, 38, 39, 38, 39, 40, 41, 42, 42, 43, 42, 42, 42, 41, 
41, 40, 39, 38, 37, 36, 35, 34, 34, 33, 32, 32, 31, 30.5, 30, 
30, 30, 30, 29, 29, 29, 28, 27, 27, 27, 27, 28.5, 30, 31.5, 33, 
34, 35, 36, 37, 38, 38, 39, 40, 41, 42, 42, 42, 42, 42, 42, 42, 
42, 42, 42, 42, 40, 40, 39, 37, 37, 36.5, 36, 35, 34, 34, 33, 
33, 32, 32, 32, 32, 32, 32, 31, 31, 30, 30, 30, 30, 31, 31, 31, 
32, 33.5, 35, 35.5, 36, 37, 37, 38, 39, 40, 41, 41, 42, 42, 42, 
43, 43, 42, 41, 41, 40, 38, 36, 35, 34, 34, 35, 36, 36, 35, 35, 
33, 33, 32, 31, 31, 30, 30, 29, 29, 29, 29, 29, 31, 31.5, 32, 
31, 30, 31, 32, 33, 34, 34, 35, 36, 36, 37, 37, 38, 38, 38, 39, 
39, 39, 39, 39, 39, 39, 38, 37, 37, 37, 37, 35, 33, 31, 31, 31, 
30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 28, 28, 27, 27, 26, 29, 
29, 30, 31, 32, 32, 33, 33, 33, 34, 34, 35, 35, 37, 37, 37, 37, 
37, 38, 38, 38, 38, 37, 37, 36, 35, 34.5, 34, 34, 33, 33, 32, 
32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 30, 30, 29.5, 29, 
29, 30, 30, 31, 32, 32, 33, 33.5, 34, 34, 34, 30.7, 27.3, 24, 
24, 24, 24.7, 25.3, 26, 27, 28, 29, 29, 29, 28, 28, 27, 27, 26, 
25, 25, 25, 24.5, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 
23, 23, 23, 24, 24, 24, 25, 25, 26, 28, 29, 30, 31, 31, 32, 33, 
34, 35, 35, 36, 37, 37, 37, 37, 37, 37, 37, 35, 34, 33, 33, 33, 
32, 31.7, 31.3, 31, 31, 31, 31, 30, 30, 30, 30, 28, 28, 27, 27, 
26, 26, 25, 25, 25, 25, 25, 26, 27, 28, 29.7, 31.3, 33, 34, 34, 
35, 36, 37, 36, 36, 35, 35, 34, 32, 32, 33, 31.3, 29.7, 28, 28, 
29, 29, 29, 29, 28, 27.5, 27, 26.5, 26, 26, 26, 26, 26, 26, 25, 
25, 25, 25, 25, 24, 24, 23, 23, 23, 24.5, 26, 27, 28, 29, 30, 
31, 32, 32, 32, 33, 34, 35, 36, 36, 36, 36, 36, 36, 36, 37, 37, 
37, 36, 36, 34, 34, 33, 32, 32, 31, 31, 30, 28, 28, 28, 28, 27, 
27, 27, 27, 27, 27, 27, 27, 26.5, 26, 26, 26, 26, 27, 28.8, 30.5, 
32.2, 34, 34, 34, 35, 36, 37, 38, 38, 38, 39, 40, 39, 39, 39, 
40, 40, 40, 39, 39, 38, 37, 35, 35, 34, 34, 34, 33, 32, 32, 32, 
32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 33, 34, 
35, 37, 37, 37.3, 37.7, 38, 39, 40, 43, 43, 43, 44, 44, 43, 43, 
43, 43, 43, 43, 42, 41, 40, 39, 38, 37, 36, 35, 35, 34, 34, 34, 
34, 34, 33, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 34, 
35, 36, 37, 38, 38, 40, 40.5, 41, 42, 42, 42, 42, 41, 40, 40, 
39, 39, 37, 32, 27, 26, 26, 27, 28, 27, 27, 27, 26, 25, 25, 25, 
25, 25, 25, 25, 25, 25, 25, 25, 24, 23, 23, 22.5, 22, 22, 23, 
24, 25, 26.3, 27.7, 29, 30, 32, 33, 34, 36, 37, 37, 39, 40, 40.5, 
41, 42, 42, 42, 43, 43, 42, 41.5, 41, 39, 38, 35, 35, 35, 34, 
33, 33, 33, 33, 32, 32, 31, 31, 30, 30, 30, 29, 29, 28, 28, 28, 
28.5, 29, 29, 29, 30, 32, 32, 34, 35, 37, 38, 39, 40, 41, 42, 
42, 43, 44, 44, 44, 45, 45, 45, 44, 43, 43, 42, 40, 38.5, 37, 
35, 35, 34, 34, 33, 34, 33, 33, 33, 33, 32, 32, 31, 32, 31, 30, 
29, 29, 29, 29, 30, 30, 31, 32.7, 34.3, 36, 37, 38, 39, 40, 41, 
41, 42, 42, 43, 44, 44, 44, 44, 44, 43.5, 43, 43, 42, 42, 40, 
38, 38, 37, 36, 36, 36, 36, 35, 35, 34, 34, 34, 34, 32, 32, 31.5, 
31, 31, 30, 30, 30, 30, 31, 33, 34, 35, 37, 38, 39, 40, 40, 42, 
42, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 42, 40, 40, 
40, 39, 38, 38, 38, 37, 37, 36, 36, 35.5, 35, 34.5, 34, 33, 33, 
32, 32, 31, 30, 30, 30, 31, 32, 33.5, 35, 36.5, 38, 38, 39, 40, 
41, 41, 41, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 
42, 40.5, 39, 38, 38, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 
35, 35, 34, 34, 34, 32, 32, 31, 31, 33, 34, 35, 35, 36, 38, 39, 
40, 41, 42, 42.5, 43, 43, 44, 44, 45, 45, 45, 45, 45, 45, 45, 
44, 43.5, 43, 43, 42, 41, 40, 40, 39, 39, 38, 38, 37, 36, 36, 
35, 34, 34, 34, 32, 32, 32, 32, 32, 32, 32, 34, 35, 35, 37, 38, 
38, 39, 39, 40, 41, 42, 43, 44, 44, 44, 45, 45, 46, 46, 46, 45, 
45, 45, 44, 42, 40, 38, 37, 37, 37, 37, 37, 36, 35), .Dim = c(1248L, 
1L), .Dimnames = list(NULL, ""temperature""))
</code></pre>

<p>Forecasted temperature values are:</p>

<pre><code>forecast_temp &lt;- structure(c(35, 34, 34, 33, 33, 30, 29.7, 29.3, 29, 28, 28, 27, 
27.3, 27.7, 28, 29, 30.5, 32, 33.5, 35, 36, 36, 37, 39, 39.5, 
40, 40, 41, 42, 42, 43, 43, 43, 44, 44, 44, 43, 42, 40, 39, 39, 
38, 37, 37, 36, 35, 35, 34), .Dim = c(48L, 1L), .Dimnames = list(
    NULL, ""temperature""))
</code></pre>

<h3><em>UPDATE-1</em></h3>

<p>From @Stephan's suggestion, I followed the approach mentioned by Prof. Rob at <a href=""http://robjhyndman.com/hyndsight/dailydata/"" rel=""nofollow"">link1</a>, <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">link2</a> as</p>

<pre><code>library(forecast)
tsob &lt;- ts(train_power,frequency = 48) #training electriciy data at daily frequency
tsob_weekly &lt;- fourier(ts(train_power,frequency = 7*48),K=3) #training data at weekly frequency
tempob &lt;- ts(train_temperature,frequency = 48) #Temperature, another predictor variable
fit &lt;- auto.arima(tsob,xreg=cbind(tsob_weekly,tempob),seasonal=FALSE)
tempob_forecast &lt;- ts(test_temperature,frequency = 48) #forecasted temperauture values
forecast_val &lt;- forecast(fit,xreg=tempob_forecast,h=48*5) #forecast for coming 5 days
</code></pre>

<p>As evident from the code, I have used Fourier transformation suggested at above links to show weekly seasonal affect. Up to <code>auto.arima()</code>, it works properly but at forecast function an error is thrown as:</p>

<pre><code>Error in forecast.Arima(fit, xreg = tempob_forecast, h = 48 * 5) : 
  Number of regressors does not match fitted model
</code></pre>

<p>The error is clear, i.e., I do not provide the forecasted regressor values of <code>tsob_weekly</code>, which I don't have. How should I handle this issue? Prof. Rob has used the same value of regressor for both the <code>auto.arima</code> and <code>forecast</code> functions.</p>
"
"0.188982236504614","0.188982236504614","217474","<p>Currently I'm using the ARIMA provided in R, the training series is a seasonal time series, with some values close to zero in each period, and I find that when the training series have a descending trend, then in the result of the forecast, there will be some negative values.</p>

<p>But the time series should be positive on every timestamps, is there a way to add constraints in ARIMA so as to prevent forecasting negative values?</p>

<p>If adding constraints is not possible, is there a way to transform the predicted results so as to make them all positive and still capture the tendency?</p>
"
"0.188982236504614","0.188982236504614","220119","<p>I have come across a sales forecasting problem. It involves using Fourier transformation. But I have trouble understanding the output.</p>

<p>what are the variables and values in the output (fplot1)? how do we interpret them? here just dates are used to create a series, sales data is not used at all.</p>

<p>code for reproduction</p>

<pre><code>r1 = seq(as.Date('2013-01-01'), as.Date('2015-09-17'), by = ""day"")

fplot1 = as.data.frame(fourier(ts(seq_along(r1), frequency = 365), 5))

par(mfrow=c(2,3))

p1 = ts.plot(c(fplot1$S1-365, fplot1$C1-365))
p2 = ts.plot(c(fplot1$S2-365, fplot1$C2-365))
p3 = ts.plot(c(fplot1$S3-365, fplot1$C3-365))
p4 = ts.plot(c(fplot1$S4-365, fplot1$C4-365))
p5 = ts.plot(c(fplot1$S5-365, fplot1$C4-365))

head(fplot1))

      S1-365    C1-365     S2-365    C2-365     S3-365    C3-365     S4-365    C4-365    S5-365    C5-365
1 0.01721336 0.9998518 0.03442161 0.9994074 0.05161967 0.9986668 0.06880243 0.9976303 0.0859648 0.9962982
2 0.03442161 0.9994074 0.06880243 0.9976303 0.10310170 0.9946708 0.13727877 0.9905325 0.1712931 0.9852201
3 0.05161967 0.9986668 0.10310170 0.9946708 0.15430882 0.9880227 0.20510450 0.9787401 0.2553533 0.9668478
4 0.06880243 0.9976303 0.13727877 0.9905325 0.20510450 0.9787401 0.27195816 0.9623091 0.3375229 0.9413173
5 0.08596480 0.9962982 0.17129314 0.9852201 0.25535330 0.9668478 0.33752290 0.9413173 0.4171936 0.9088176
6 0.10310170 0.9946708 0.20510450 0.9787401 0.30492122 0.9523776 0.40148799 0.9158643 0.4937756 0.8695894
</code></pre>

<p>any help would be appreciated!!</p>

<p>plots
<a href=""http://i.stack.imgur.com/8EMO9.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8EMO9.jpg"" alt=""enter image description here""></a></p>
"
"0.428571428571429","0.428571428571429","223888","<p>I am interested in forecasting with a vector error correction model (VECM). I am facing a problem of not being able to transform a cointegrated series into a VECM model using the stationary series. </p>

<p>In multivariate forecast like VAR or VECM it is important to see which of the two models to use for forecasting. To decide whether to use a VAR or a VECM:</p>

<ul>
<li>First, we do a cointegration test using the <code>ca.jo</code> function from ""urca"" package in R. </li>
<li>If we find that there is no cointegrating vector suggested by the Johansen procedure, then we can run a VAR model. But if we find evidence of cointegration then we have to use a VECM model in order to incorporate the error correction coefficients in the model. </li>
<li>To test if there is cointegration in the series we use Johansen test on the data <strong>in levels</strong>, i.e. in non-stationary form. But after we find evidence of cointegration we have to incorporate as many cointegrating vectors in the VECM as the number suggested by the Johansen test. But then this time the VECM should have been run on stationary series having made them <strong>differenced</strong>. </li>
<li>But in R I am not getting the option as to how to make a VECM model differenced and then forecast it. R manuals are suggesting that we should use the function <code>vec2var</code> to convert a VECM to a VAR model and then forecast the VAR model thus obtained. </li>
<li>But the VAR model thus obtained from the VECM is <strong>at levels</strong> and <em>not</em> at <strong>differenced</strong> form. Hence, inference from this may be biased. </li>
</ul>

<p>I just want to run a VECM in <strong>differenced</strong> series (not <strong>in levels</strong>) and also to include the error correction term. Please help me with this. </p>
"
"0.462910049886276","0.462910049886276","230538","<p>I am using the rugarch package in R to forecast volatility using 5 minute data. The package has an a conditional variance model exclusively written for high frequency data, the Multiplicative Component Garch, I know that. 
The thing is that I invested a whole month to to find tick data and transform them to 5 minute intervals which I then seasonally adjusted using techniques from the extant bibliography. 
I am trying to manipulate the package even though It is not efficient for 5 minute data to extract the conditional variance forecasts. My data span the period from 2015-01-01 17:00:00 to 2015-12-31 17:00. (I am willing to reduce my sample to find meaningful results).</p>

<p>This is how my data look like with five fivemin_returns[,2] being the standardised (deseasonalized) returns and <a href=""https://drive.google.com/file/d/0B9Gcg5pCwYo6OERFMjg3SjNlaVU/view?usp=sharing"" rel=""nofollow"">click this to download my data</a></p>

<pre><code>head(fivemin_returns, 3)

                           DPRICE STD_DPRICE
2015-01-01 17:00:00  0.000000e+00  0.0000000
2015-01-01 17:05:00  9.797714e-05  0.2324451
2015-01-01 17:10:00  2.027022e-04  0.9845100

tail(fivemin_returns, 3)
                           DPRICE STD_DPRICE
2015-12-31 16:50:00 -0.0010186764 -6.0913134
2015-12-31 16:55:00 -0.0006841370 -3.9376338
2015-12-31 17:00:00  0.0002481227  1.1458867
</code></pre>

<p>So I want to create several conditional variance models and extract the forecasted sigma values. My methodology draws heavily from this article <a href=""http://unstarched.net/2013/01/07/does-anything-not-beat-the-garch11/"" rel=""nofollow"">Does anything NOT beat the GARCH(1,1)?</a>. I fit a MA(1) process to my returns without the conditional mean.</p>

<pre><code>library(xts)
library(rugarch)
model = c('sGARCH', 'gjrGARCH', 'eGARCH', 'apARCH', 'csGARCH', 'fGARCH', 'fGARCH', 'fGARCH')
submodel = c(NA, NA, NA, NA, NA, 'AVGARCH', 'NGARCH', 'NAGARCH')
spec1 = vector(mode = 'list', length = 8)
for (i in 1:8) spec1[[i]] = ugarchspec(mean.model = list(armaOrder = c(0, 1), include.mean = FALSE), variance.model = list(model = model[i], submodel = if (i &gt; 5) submodel[i] else NULL))
spec2 = vector(mode = 'list', length = 8)
for (i in 1:8) spec2[[i]] = ugarchspec(mean.model = list(armaOrder = c(0, 1), include.mean = FALSE), variance.model = list(model = model[i], submodel = if (i &gt; 5) submodel[i] else NULL), distribution = 'sstd')
spec3 = vector(mode = 'list', length = 8)
for (i in 1:8) spec3[[i]] = ugarchspec(mean.model = list(armaOrder = c(0, 1), include.mean = FALSE), variance.model = list(model = model[i], submodel = if (i &gt; 5) submodel[i] else NULL), distribution = 'nig')
spec4 = vector(mode = 'list', length = 8)
for (i in 1:8) spec4[[i]] = ugarchspec(mean.model = list(armaOrder = c(0, 1), include.mean = FALSE), variance.model = list(model = model[i], submodel = if (i &gt; 5) submodel[i] else NULL), distribution = 'jsu')
spec = c(spec1, spec2, spec3, spec4)
cluster = makePSOCKcluster(15)
clusterExport(cluster, c('spec', 'fivemin_returns'))
clusterEvalQ(cluster, library(rugarch))
# Out of sample estimation
n = length(spec)
fitlist = vector(mode = 'list', length = n)
for (i in 1:n) {
    tmp = ugarchroll(spec[[i]], fivemin_returns[,2], n.ahead = 1, forecast.length = 8640, refit.every = 50, refit.window = 'moving', windows.size = 1500, solver = 'hybrid', calculate.VaR = FALSE, cluster = cluster, keep.coef = FALSE)
    if (!is.null(tmp@model$noncidx)) {
        tmp = resume(tmp, solver = 'solnp', fit.control = list(scale = 1), solver.control = list(tol = 1e-07, delta = 1e-06), cluster = cluster)
        if (!is.null(tmp@model$noncidx))
            fitlist[[i]] = NA
    } else {
        fitlist[[i]] = as.data.frame(tmp, which = 'density')
    }
}
</code></pre>

<p>I tried to run the code but it is impossible due to the amount of data and the wrong formulation of the <code>ugarchroll(spec[[i]], R, n.ahead = 1, forecast.length = 8640, refit.every = 50, refit.window = 'moving', windows.size = 1500, solver = 'hybrid', calculate.VaR = FALSE, cluster = cluster, keep.coef = FALSE)</code> argument I reckon. I am not really sure about the </p>

<pre><code>refit.every = 50, refit.window = 'moving', windows.size = 1500
</code></pre>

<p>How can I make it work? Decrease the sample size to 4 months maybe and try to forecast the last 15 days?</p>

<p>Edit 1: decreasing the sample and forecasting the condiational variance for the last 15 days is not the most practical thing to do as foreacting evaluation techniques with 15 (aggregated) observations is not a sensible thing to do. That is why I see at least one year of tick data in other papers and makes me think how much computing power have these scientists at their disposal? With my mere I3 processor and 4 gb ram I can't do anything.</p>
"
