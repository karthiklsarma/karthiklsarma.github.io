"V1","V2","V3","V4"
"0.0657004319817604","0.0667904674542028","  1266","<p>The following question is one of those holy grails for me for some time now, I hope someone might be able to offer a good advice.</p>

<p>I wish to perform a non-parametric repeated measures multiway anova using R.</p>

<p>I have been doing some online searching and reading for some time, and so far was able to find solutions for only some of the cases: friedman test for one way nonparametric repeated measures anova, ordinal regression with {car} Anova function for multi way nonparametric anova, and so on.  The partial solutions is NOT what I am looking for in this question thread.  I have summarized my findings so far in a post I published some time ago (titled: <a href=""http://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/"">Repeated measures ANOVA with R (functions and tutorials)</a>, in case it would help anyone) </p>

<p>.</p>

<p>If what I read online is true, this task might be achieved using a mixed Ordinal Regression model (a.k.a: Proportional Odds Model).</p>

<p>I found two packages that seems relevant, but couldn't find any vignette on the subject:</p>

<ul>
<li><a href=""http://cran.r-project.org/web/packages/repolr/"">http://cran.r-project.org/web/packages/repolr/</a></li>
<li><a href=""http://cran.r-project.org/web/packages/ordinal/"">http://cran.r-project.org/web/packages/ordinal/</a></li>
</ul>

<p>So being new to the subject matter, I was hoping for some directions from people here.</p>

<p>Are there any tutorials/suggested-reading on the subject?  Even better, can someone suggest a simple example code for how to run and analyse this in R (e.g: ""non-parametric repeated measures multiway anova"") ?</p>

<p>Thanks for any help,
Tal</p>
"
"0.0479808115123254","0.0609710760849692","  1432","<p>In answering <a href=""http://stats.stackexchange.com/questions/1412/consequences-of-an-improper-link-function-in-n-alternative-forced-choice-procedur"">this</a> question John Christie suggested that the fit of logistic regression models should be assessed by evaluating the residuals.  I'm familiar with how to interpret residuals in OLS, they are in the same scale as the DV and very clearly the difference between y and the y predicted by the model.  However for logistic regression, in the past I've typically just examined estimates of model fit, e.g. AIC, because I wasn't sure what a residual would mean for a logistic regression.  After looking into R's help files a little bit I see that in R there are five types of glm residuals available, c(""deviance"", ""pearson"", ""working"",""response"", ""partial"").  The help file refers to Davison, A. C. and Snell, E. J. (1991) Residuals and diagnostics. In: Statistical Theory and Modelling. In Honour of Sir David Cox, FRS, eds. Hinkley, D. V., Reid, N. and Snell, E. J., Chapman &amp; Hall, of which I do not have a copy.  Is there a short way to describe how to interpret each of these types?  In a logistic context will sum of squared residuals provide a meaningful measure of model fit or is one better off with an Information Criterion?</p>
"
"NaN","NaN","  2234","<p>I would like as many algorithms that perform the same task as logistic regression.  That is  algorithms/models that can give a prediction to a binary response (Y) with some explanatory variable (X).</p>

<p>I would be glad if after you name the algorithm, if you would also show how to implement it in R.  Here is a code that can be updated with other models:</p>

<pre><code>set.seed(55)
n &lt;- 100
x &lt;- c(rnorm(n), 1+rnorm(n))
y &lt;- c(rep(0,n), rep(1,n))
r &lt;- glm(y~x, family=binomial)
plot(y~x)
abline(lm(y~x),col='red',lty=2)
xx &lt;- seq(min(x), max(x), length=100)
yy &lt;- predict(r, data.frame(x=xx), type='response')
lines(xx,yy, col='blue', lwd=5, lty=2)
title(main='Logistic regression with the ""glm"" function')
</code></pre>
"
"0.026822089039291","0.0272670941574606","  2854","<p>Dear all, 
I was encouraged to ask this question here as well as on stackoverflow and would be very appreciative of any answers...</p>

<p>Due to hetereoscedasticity I'm doing bootstrapped linear regression (appeals more to me than robust regression). I'd like to create a plot along the lines of what I've done in the script here. However the <code>fill=int</code> is not right since <code>int</code> should (I believe) be calculated using a bivariate normal distribution. </p>

<ul>
<li>Any idea how I could do that in this setting? </li>
<li>Also is there a way for <code>bootcov</code> to return bias-corrected percentiles?</li>
</ul>

<p>sample script:</p>

<pre><code>library(ggplot2) 
library(Hmisc) 
library(Design) # for ols()

o&lt;-data.frame(value=rnorm(10,20,5),
              bc=rnorm(1000,60,50),
              age=rnorm(1000,50,20),
              ai=as.factor(round(runif(1000,0,4),0)),
              Gs=as.factor(round(runif(1000,0,6),0))) 

reg.s&lt;-function(x){      
    ols(value~as.numeric(bc)+as.numeric(age),data=x,x=T,y=T)-&gt;temp 
    bootcov(temp,B=1000,coef.reps=T)-&gt;t2 

    return(t2) 
    } 

dlply(o,.(ai,Gs),function(x) reg.s(x))-&gt;b.list 
llply(b.list,function(x) x[[""boot.Coef""]])-&gt;b2 

ks&lt;-llply(names(b2),function(x){ 
    s&lt;-data.frame(b2[[x]]) 
    s$ai&lt;-x 
    return(s) 
    }) 


ks3&lt;-do.call(rbind,ks) 
ks3$ai2&lt;-with(ks3,substring(ai,1,1)) 

ks3$gc2&lt;-sapply(strsplit(as.character(ks3$ai), ""\\.""), ""[["", 2) 


k&lt;-ks3 
j&lt;-dlply(k,.(ai2,gc2),function(x){ 
    i1&lt;-quantile(x$Intercept,probs=c(0.025,0.975))[1] 
    i2&lt;-quantile(x$Intercept,probs=c(0.025,0.975))[2] 

    j1&lt;-quantile(x$bc,probs=c(0.025,0.975))[1] 
    j2&lt;-quantile(x$bc,probs=c(0.025,0.975))[2] 

    o&lt;-x$Intercept&gt;i1 &amp; x$Intercept&lt;i2 

    p&lt;-x$bc&gt;j1 &amp; x$bc&lt;j2 

    h&lt;-o &amp; p 
    return(h) 
    }) 

m&lt;-melt(j) 
ks3$int&lt;-m[,1]   

ggplot(ks3,aes(x=bc,y=Intercept,fill=int)) +
  geom_point(,alpha=0.3,size=1,shape=21) +
  facet_grid(gc2~ai2,scales = ""free_y"")+theme_bw()-&gt;plott 
plott&lt;-plott+opts(panel.grid.minor=theme_blank(),panel.grid.major=theme_blank()) 
plott&lt;-plott+geom_vline(x=0,color=""red"") 
plott+xlab(""BC coefficient"")+ylab(""Intercept"") 
</code></pre>
"
"NaN","NaN","  3531","<p>I would like to perform reversible jump on a network model, but before arriving there, I'm wondering if there are any R packages which support reversible jump for a user specified generalized linear model or spatial-GLM?</p>

<p>Something as simple as an RJMCMC procedure (in R) for the selection of predictors in a logistic regression would be a nice place for me to start?  Does such a function exist?</p>

<p>Through googling, I've only found <a href=""http://cran.r-project.org/web/packages/RJaCGH/index.html"" rel=""nofollow"">RJaCGH</a> which appears to be a bit more complicated (and application specific) than I was hoping for.</p>
"
"0.0763370036711974","0.0862261227118454","  3841","<p>I have two years of data which looks basically like this:</p>

<p>Date   <strong><em>_</em>__<em></strong>    Violence Y/N? _</em>  Number of patients</p>

<p>1/1/2008    <strong><em>_</em>___<em></strong>    0  <strong></em>__<em>_</em>__<em>_</em>____</strong> 11</p>

<p>2/1/2008 <strong><em>_</em>__<em>_</em></strong>       0  <strong><em>_</em>__<em>_</em>__<em>_</em>__</strong> 11</p>

<p>3/1/2008 <strong><em>_</em>____</strong><em>1  <strong></em>__<em>_</em>__<em>_</em>____</strong> 12</p>

<p>4/1/2008 <strong><em>_</em>____</strong><em>0  <strong></em>__<em>_</em>__<em>_</em>____</strong> 12</p>

<p>...</p>

<p>31/12/2009_<strong><em>_</em>__</strong>      0_<strong><em>_</em>__<em>_</em>__<em>_</em>__</strong>                 14</p>

<p>i.e. two years of observations, one per day, of a psychiatric ward, which indicate whether there was a violence incident on that day (1 is yes, 0 no) as well as the number of patients on the ward. The hypothesis that we wish to test is that more patients on the ward is associated with an increased probability of violence on the ward.</p>

<p>We realise, of course, that we will have to adjust for the fact that when there are more patients on the ward, violence is more likely because there are just more of them- we are interested in whether each individualâ€™s probability of violence goes up when there are more patients on the ward.</p>

<p>I've seen several papers which just use logistic regression, but I think that is wrong because there is an autoregressive structure (although, looking at the autocorrelation function, it doesnâ€™t get above .1 at any lag, although this is above the â€œsignificantâ€ blue dashed line that R draws for me).</p>

<p>Just to make things more complicated, I can if I wish to break down the results into individual patients, so the data would look just as it does above, except I would have the data for each patient, 1/1/2008, 2/1/2008 etc. and an ID code going down the side so the data would show the whole history of incidents for each patient separately (although not all patients are present for all days, not sure whether that matters).</p>

<p>I would like to use lme4 in R to model the autoregressive structure within each patient, but some Googling comes up with the quotation â€œlme4 is not set up to deal with autoregressive structuresâ€. Even if it were, Iâ€™m not sure I grasp how to write the code anyway.</p>

<p>Just in case anyone notices, I asked a question like this a while ago, they are different datasets with different problems, although actually solving this problem will help with that one (someone suggested I use mixed methods previously, but this autoregression thing has made me unsure how to do this).</p>

<p>So Iâ€™m a bit stuck and lost to be honest. Any help gratefully received!</p>
"
"0.0848188929679971","0.0862261227118454","  4187","<p>Hello
I have two problems that sound like natural candidates for multilevel/mixed models, which I have never used.  The simpler, and one that I hope to try as an introduction, is as follows:
The data looks like many rows of the form </p>

<p><code>x y innergroup outergroup</code></p>

<p>where x is a numeric covariate upon which I want to regress y (another numeric variable), each y belongs to an innergroup, and each innergroup is nested in an outergroup (i.e, all the y in a given innergroup belong to the same outergroup).  Unfortunately, innergroup has a lot of levels (many thousands), and each level has relatively few observations of y, so I thought this sort of model might be appropriate.  My questions are</p>

<ol>
<li><p>How do I write this sort of multilevel formula?</p></li>
<li><p>Once <strong>lmer</strong> fits the model, how does one go about predicting from it?  I have fit some simpler toy examples, but have not found a predict() function.  Most people seem more interested in inference than prediction with this sort of technique.
I have several million rows, so the computations might be an issue, but I can always cut it down as appropriate.  </p></li>
</ol>

<p>I won't need to do the second for some time, but I might as well begin thinking about it and playing around with it.  I have similar data as before, but without x, and y is now a binomial variable of the form $(n,n-k)$.  y also exhibits a lot of overdispersion, even within innergroups.  Most of the $n$ are no more than 2 or 3 (or less), so to derive estimates of the success rates of each $y_i$ I have been using the beta-binomial shrinkage estimator $(\alpha+k_i)/(\alpha+\beta+n_i)$, where $\alpha$ and $\beta$ are estimated by MLE for each innergroup separately.  This is has been somewhat adequate, but data sparsity still plagues me, so I would like to use all the data available.  From one perspective, this problem is easier since there is no covariate, but from the other the binomial nature makes it more difficult.  Does anyone have any high (or low!) level guidance?</p>
"
"0.0969560705490221","0.0915243346951392","  5087","<p>There are numerous procedures for functional data clustering based on orthonormal basis functions. I have a series of models built with the GAMM models, using the <code>gamm()</code> from the mgcv package in R. For fitting a long-term trend, I use a thin plate regression spline. Next to that, I introduce a CAR1 model in the random component to correct for autocorrelation. For more info, see eg the paper of Simon Wood on <a href=""http://r.789695.n4.nabble.com/attachment/2063352/0/tprs.pdf"">thin plate regression splines</a> or his <a href=""http://rads.stackoverflow.com/amzn/click/1584884746"">book on GAM models</a>.</p>

<p>Now I'm a bit puzzled in how I get the correct coefficients out of the models. And I'm even less confident that the coefficients I can extract, are the ones I should use to cluster different models. </p>

<p>A simple example, using:</p>

<pre><code>#runnable code
require(mgcv)
require(nlme)
library(RLRsim)
library(RColorBrewer)

x1 &lt;- 1:1000
x2 &lt;- runif(1000,10,500)

fx1 &lt;- -4*sin(x1/50)
fx2 &lt;- -10*(x2)^(1/4)
y &lt;- 60+ fx1 + fx2 + rnorm(1000,0,5)

test &lt;- gamm(y~s(x1)+s(x2))
# end runnable code
</code></pre>

<p>Then I can construct the original basis using smoothCon :</p>

<pre><code>#runnable code
um &lt;- smoothCon(s(x1),data=data.frame(x1=x1),
         knots=NULL,absorb.cons=FALSE)
#end runnable code
</code></pre>

<p>Now,when I look at the basis functions I can extract using </p>

<pre><code># runnable code
X &lt;- extract.lmeDesign(test$lme)$X
Z &lt;- extract.lmeDesign(test$lme)$Z

op &lt;- par(mfrow=c(2,5),mar=c(4,4,1,1))
plot(x1,X[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,X[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,8],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,7],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,6],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,5],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,4],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,3],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
par(op)
# end runnable code
</code></pre>

<p>they look already quite different. I can get the final coefficients used to build the smoother by</p>

<pre><code>#runnable code
Fcoef &lt;- test$lme$coef$fixed
Rcoef &lt;- unlist(test$lme$coef$random)
#end runnable code
</code></pre>

<p>but I'm far from sure these are the coefficients I look for. I fear I can't just use those coefficients as data in a clustering procedure. I would really like to know which coefficients are used to transform the basis functions from the ones I get with <code>smoothCon()</code> to the ones I extract from the lme-part of the gamm-object. And if possible, where I can find them. I've read the related articles, but somehow I fail to figure it out myself. All help is appreciated.</p>
"
"0.0379321620905441","0.0385614943639849","  5293","<p>My name is Tuhin.
I came up with a couple of questions when I was doing an
analysis in R.</p>

<p>I did a logistic regression analysis in R and tried to check
how good the model fits the data.</p>

<p>But, I got stuck as I could not get the pseudo R square value
for the model which could give me some idea about the variation
explained by the model.</p>

<p>Could you please guide me on how to achieve this value (pseudo
R square for Logistic regression analysis).
It would also be helpful if you could show me a way to get the
Hosmer Lemeshow statistic for the model as well. I found out a
user defined function to do it, but if there is a quicker way
possible, it would be really helpful.</p>

<p>I would be very grateful if you can provide me the answers to
my queries.</p>

<p>Eagerly waiting for your response.</p>

<p>Regards</p>
"
"0.0709645772411954","0.072141950116023","  5304","<p>Dear everyone - I've noticed something strange that I can't explain, can you? In summary: the manual approach to calculating a confidence interval in a logistic regression model, and the R function <code>confint()</code> give different results.</p>

<p>I've been going through Hosmer &amp; Lemeshow's <em>Applied logistic regression</em> (2nd edition).  In the 3rd chapter there is an example of calculating the odds ratio and 95% confidence interval.  Using R, I can easily reproduce the model:</p>

<pre><code>Call:
glm(formula = dataset$CHD ~ as.factor(dataset$dich.age), family = ""binomial"")

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.734  -0.847  -0.847   0.709   1.549  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -0.8408     0.2551  -3.296  0.00098 ***
as.factor(dataset$dich.age)1   2.0935     0.5285   3.961 7.46e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 136.66  on 99  degrees of freedom
Residual deviance: 117.96  on 98  degrees of freedom
AIC: 121.96

Number of Fisher Scoring iterations: 4
</code></pre>

<p>However, when I calculate the confidence intervals of the parameters, I get a different interval to the one given in the text:</p>

<pre><code>&gt; exp(confint(model))
Waiting for profiling to be done...
                                 2.5 %     97.5 %
(Intercept)                  0.2566283  0.7013384
as.factor(dataset$dich.age)1 3.0293727 24.7013080
</code></pre>

<p>Hosmer &amp; Lemeshow suggest the following formula:</p>

<p>$$
e^{[\hat\beta_1\pm z_{1-\alpha/2}\times\hat{\text{SE}}(\hat\beta_1)]}
$$
</p>

<p>and they calculate the confidence interval for <code>as.factor(dataset$dich.age)1</code> to be (2.9, 22.9).</p>

<p>This seems straightforward to do in R:</p>

<pre><code># upper CI for beta
exp(summary(model)$coefficients[2,1]+1.96*summary(model)$coefficients[2,2])
# lower CI for beta
exp(summary(model)$coefficients[2,1]-1.96*summary(model)$coefficients[2,2])
</code></pre>

<p>gives the same answer as the book.</p>

<p>However, any thoughts on why <code>confint()</code> seems to give different results?  I've seen lots of examples of people using <code>confint()</code>.</p>
"
"0.0309714806541255","0.0472279924554862","  6400","<p>I am conducting a mulitple first order regression analysis of genetic data. The vectors of y-values do not all follow a normal distribution, therefore I need to implement a non-parametric regression using ranks.</p>

<p>Is the <code>lm()</code> function in R suitable for this, i.e.,</p>

<pre><code>lin.reg &lt;- lm(Y~X*Z)
</code></pre>

<p>where Y, X and Z are vectors of ordinal categorical variables?</p>

<p>I am interested in the p-value assigned to the coefficient of the interaction term in the first order model. The <code>lm()</code> function obtains this from a t-test, i.e., is the interaction coefficient significantly different from zero.</p>

<p>Is the automatic implementation of a t-test to determine this p-value appropriate when the regression model is carried out on data as described?</p>

<p>Thanks.</p>

<p><strong>EDIT</strong></p>

<p>Sample data for clarity:</p>

<pre><code>Y &lt;- c(4, 1, 2, 3) # A vector of ranks
X &lt;- c(0, 2, 1, 1) # A vector of genotypes (0 = aa, 1 = ab, 2 = bb)
Z &lt;- c(2, 2, 1, 0)
</code></pre>
"
"NaN","NaN","  6412","<p>What is your favorite free tool on Linux for multivariate logistic regression?</p>

<p>Possibilities I've seen:</p>

<ul>
<li><a href=""http://www.r-project.org"" rel=""nofollow"">R</a> (see <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">paper</a>).  <a href=""http://stackoverflow.com/questions/3439248/logistic-regression-in-r-sas-like-output"">This question</a> says use <a href=""http://cran.r-project.org/package=Design"" rel=""nofollow"">design</a>.</li>
<li>Can you use <a href=""http://docs.scipy.org/doc/scipy/reference/stats.html#statistical-functions"" rel=""nofollow"">SciPy</a>?</li>
</ul>

<p>Other choices?</p>

<p>Do people have experience with large data?</p>
"
"0.0657004319817604","0.0667904674542028","  6544","<p>I have a dataset that I want to fit a simple linear model to, but I want to include the lag of the dependent variable as one of the regressors. Then I want to predict future values of this time series using forecasts I already have for the independent variables. The catch is: how do I incorporate the lag into my forecast?</p>

<p>Here's an example:</p>

<pre><code>#A function to calculate lags
lagmatrix &lt;- function(x,max.lag){embed(c(rep(NA,max.lag),x),max.lag)}
lag &lt;- function(x,lag) {
 out&lt;-lagmatrix(x,lag+1)[,lag]
 return(out[1:length(out)-1])
}

y&lt;-arima.sim(model=list(ar=c(.9)),n=1000) #Create AR(1) dependant variable
A&lt;-rnorm(1000) #Create independant variables
B&lt;-rnorm(1000)
C&lt;-rnorm(1000)
Error&lt;-rnorm(1000)
y&lt;-y+.5*A+.2*B-.3*C+.1*Error #Add relationship to independant variables 

#Fit linear model
lag1&lt;-lag(y,1)
model&lt;-lm(y~A+B+C+lag1)
summary(model)

#Forecast linear model
A&lt;-rnorm(50) #Assume we know 50 future values of A, B, C
B&lt;-rnorm(50)
C&lt;-rnorm(50)
lag1&lt;-  #################This is where I'm stuck##################

newdata&lt;-as.data.frame(cbind(A,B,C,lag1))
predict.lm(model,newdata=newdata)
</code></pre>
"
"0.0379321620905441","0.0385614943639849","  6731","<p>I have a design matrix of p regressors, n observations, and I am trying to compute the sample variance-covariance matrix of the parameters.  I am trying to directly calculate it using svd.</p>

<p>I am using R, when I take svd of the design matrix, I get three components: a matrix $U$ which is $n \times p$, a matrix $D$ which is $1\times 3$ (presumably eigenvalues), and a matrix $V$ which is $3\times 3$.  I diagonalized $D$, making it a $3\times 3$ matrix with 0's in the off-diagonals.</p>

<p>Supposedly, the formula for covariance is: $V D^2 V&#39;$, however, the matrix does not match, nor is it even close to R's built in function, <code>vcov</code>.
Does anyone have any advice/references?  I admit that I am a bit unskilled in this area.</p>
"
"0.0599760143904067","0.0609710760849692","  6927","<p>I'm doing a simulation study which requires bootstrapping estimates obtained from a generalized linear mixed model (actually, the product of two estimates for fixed effects, one from a GLMM and one from an LMM). To do the study well would require about 1000 simulations with 1000 or 1500 bootstrap replications each time. This takes a significant amount of time on my computer (many days). </p>

<p><code>How can I speed up the computation of these fixed effects?</code></p>

<p>To be more specific, I have subjects who are measured repeatedly in three ways, giving rise to variables X, M, and Y, where X and M are continuous and Y is binary. We have two regression equations 
$$M=\alpha_0+\alpha_1X+\epsilon_1$$
$$Y^*=\beta_0+\beta_1X+\beta_2M+\epsilon_2$$
where Y$^*$ is the underlying latent continuous variable for $Y$ and the errors are not iid.<br>
The statistic we want to bootstrap is $\alpha_1\beta_2$. Thus, each bootstrap replication requires fitting an LMM and a GLMM. My R code is (using lme4)</p>

<pre><code>    stat=function(dat){
        a=fixef(lmer(M~X+(X|person),data=dat))[""X""]
        b=fixef(glmer(Y~X+M+(X+M|person),data=dat,family=""binomial""))[""M""]
        return(a*b)
    }</code></pre>

<p>I realize that I get the same estimate for $\alpha_1$ if I just fit it as a linear model, so that saves some time, but the same trick doesn't work for $\beta_2$.</p>

<p>Do I just need to buy a faster computer? :)</p>
"
"0.026822089039291","0.0272670941574606","  7344","<p>I'm trying to write a function to graphically display predicted vs. actual relationships in a linear regression.  What I have so far works well for linear models, but I'd like to extend it in a few ways.</p>

<ol>
<li>Handle glm models</li>
<li>Deal with NAs in the predicted values</li>
</ol>

<p>Does what I have so far seem like a good solution, or is there an existing package somewhere that's already implemented this?</p>

<pre><code>DF &lt;- as.data.frame(na.exclude(airquality))
DF$Month &lt;- as.factor(DF$Month)
DF$Day &lt;- as.factor(DF$Day)

my_model &lt;- lm(Ozone~Solar.R+Wind+Temp+Month+Day,DF)

PvA&lt;- function(model,varlist=NULL,smooth=.5) { #Plot predicted vs actual for a model

    indvars &lt;- attr(terms(model),""term.labels"")

    if (is.null(varlist)) {
        varlist &lt;- indvars
    }

    Y &lt;- as.character(as.list(attr(terms(model),""variables""))[2])
    P.Y &lt;- paste('P',Y,sep='.')

    DF &lt;- as.data.frame(get(as.character(model$call$data)))
    DF[,P.Y] &lt;- predict.lm(model)

    par(ask=TRUE)
    for (X in varlist) {
        print(X)
        A &lt;- na.omit(DF[,c(X,Y)])
        P &lt;- na.omit(DF[,c(X,P.Y)])
        plot(A)
        points(P,col=2)
        lines(lowess(A,f=smooth),col=1)
        lines(lowess(P,f=smooth),col=2)
    }

}
PvA(my_model)
</code></pre>
"
"0.0889588054368324","0.0904347204435887","  7527","<p>I'm trying to implement a ""change point"" analysis, or a multiphase regression using <code>nls()</code> in R. </p>

<p><a href=""http://i.stack.imgur.com/27f1S.png"" rel=""nofollow"">Here's some fake data I've made</a>. The formula I want to use to fit the data is:</p>

<p>$y = \beta_0 + \beta_1x + \beta_2\max(0,x-\delta)$</p>

<p>What this is supposed to do is fit the data up to a certain point with a certain intercept and slope ($\beta_0$ and $\beta_1$), then, after a certain x value ($\delta$), augment the slope by $\beta_2$. That's what the whole max thing is about. Before the $\delta$ point, it'll equal 0, and $\beta_2$ will be zeroed out.</p>

<p>So, here's my function to do this:</p>

<pre><code>changePoint &lt;- function(x, b0, slope1, slope2, delta){ 
   b0 + (x*slope1) + (max(0, x-delta) * slope2)
}
</code></pre>

<p>And I try to fit the model this way</p>

<pre><code>nls(y ~ changePoint(x, b0, slope1, slope2, delta), 
    data = data, 
    start = c(b0 = 50, slope1 = 0, slope2 = 2, delta = 48))
</code></pre>

<p>I chose those starting parameters, because I <em>know</em> those are the starting parameters, because I made the data up.</p>

<p>However, I get this error:</p>

<pre><code>Error in nlsModel(formula, mf, start, wts) : 
  singular gradient matrix at initial parameter estimates
</code></pre>

<p>Have I just made unfortunate data? I tried fitting this on real data first, and was getting the same error, and I just figured that my initial starting parameters weren't good enough. </p>
"
"0.0709645772411954","0.0515299643685879","  7595","<p>I've been using the <code>lm</code> function in R to do demand modeling (tons of steel to be predicted by various economic indicators).  I used $R^2$  and $F$ to report on the strength of the model.  However, when I use the R function <code>lqs</code> (""resistant regression"") and then type in <code>summary(model_name)</code> I do not get any statistics that I can use to report on the strength of the regression model.  Any suggestions? </p>

<p>EDIT:
Thanks for your quick response. I don't have a problem with lqs(). The problem is that when I type in summary(Model) I do not get any goodness of fit information (e.g., adjusted R squared) as I do when I enter summary(x) where X is a model created using the lm function. I'd like to have something to show the strength of the model. I""m using MASS. See below. Regards, Bill Yarberry</p>

<p>library(MASS)</p>

<pre><code>M10 = lqs(agri ~ p12+p1+p11+p5+p8+p6+p25+p50+p35, data = agri_data2) summary(M10) Length Class Mode
crit 1 -none- numeric
sing 1 -none- character coefficients 10 -none- numeric
bestone 10 -none- numeric
fitted.values 103 -none- numeric
residuals 103 -none- numeric
scale 2 -none- numeric
terms 3 terms call
call 3 -none- call
xlevels 0 -none- list
model 10 data.frame list
</code></pre>
"
"0.0848188929679971","0.0862261227118454","  7720","<p>I am new to R, ordered logistic regression, and <code>polr</code>.</p>

<p>The ""Examples"" section at the bottom of the help page for <a href=""http://stat.ethz.ch/R-manual/R-patched/library/MASS/html/polr.html"">polr</a> (that fits a logistic or probit regression model to an ordered factor response) shows</p>

<pre><code>options(contrasts = c(""contr.treatment"", ""contr.poly""))
house.plr &lt;- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)
pr &lt;- profile(house.plr)
plot(pr)
pairs(pr)
</code></pre>

<ul>
<li><p>What information does <code>pr</code> contain?  The help page on <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/profile.html"">profile</a> is
generic, and gives no guidance for polr.</p></li>
<li><p>What is <code>plot(pr)</code> showing?  I see six graphs. Each has an X axis that is
numeric, although the label is an indicator variable (looks like an input variable that is an indicator for an ordinal value).  Then the Y axis
is ""tau"" which is completely unexplained.</p></li>
<li><p>What is <code>pairs(pr)</code> showing?  It looks like a plot for each pair of input
variables, but again I see no explanation of the X or Y axes.</p></li>
<li><p>How can one understand if the model gave a good fit?
<code>summary(house.plr)</code> shows Residual Deviance 3479.149 and AIC (Akaike
Information Criterion?) of 3495.149.  Is that good?  In the case those
are only useful as relative measures (i.e. to compare to another model
fit), what is a good absolute measure?  Is the residual deviance approximately chi-squared distributed?  Can one use ""% correctly predicted"" on the original data or some cross-validation?  What is the easiest way to do that?</p></li>
<li><p>How does one apply and interpret <code>anova</code> on this model?  The docs say ""There are methods for the standard model-fitting functions, including predict, summary, vcov, anova.""  However, running <code>anova(house.plr)</code> results in <code>anova is not implemented for a single ""polr"" object</code></p></li>
<li><p>How does one interpret the t values for each coefficient?  Unlike some
model fits, there are no P values here.</p></li>
</ul>

<p>I realize this is a lot of questions, but it makes sense to me to ask as one bundle (""how do I use this thing?"") rather than 7 different questions.  Any information appreciated.</p>
"
"0.0599760143904067","0.0487768608679754","  7775","<p>Does anyone have suggestions or packages that will calculate the coefficient of partial determination?</p>

<p>The coefficient of partial determination can be defined as the percent of variation that cannot be explained in a reduced model, but can be explained by the predictors specified in a full(er) model. This coefficient is used to provide insight into whether or not one or more additional predictors may be useful in a more fully specified regression model.</p>

<p>The calculation for the partial r^2 is relatively straight forward after estimating your two models and generating the ANOVA tables for them. The calculation for the partial r^2 is:</p>

<p>(SSEreduced - SSEfull) / SSEreduced</p>

<p>I've written this relatively simple function that will calculate this for a multiple linear regression model. I'm unfamiliar with other model structures in R where this function may not perform as well:</p>

<pre><code>partialR2 &lt;- function(model.full, model.reduced){
    anova.full &lt;- anova(model.full)
    anova.reduced &lt;- anova(model.reduced)

    sse.full &lt;- tail(anova.full$""Sum Sq"", 1)
    sse.reduced &lt;- tail(anova.reduced$""Sum Sq"", 1)

    pR2 &lt;- (sse.reduced - sse.full) / sse.reduced
    return(pR2)

    }
</code></pre>

<p>Any suggestions or tips on more robust functions to accomplish this task and/or more efficient implementations of the above code would be much appreciated.</p>
"
"NaN","NaN","  7882","<p>Does anyone know how to construct a confidence interval for predicting a new test value given a trained Relevance Vector Machine (<code>rvm</code>) and/or Gaussian Process Regression (<code>gausspr</code>) using the <a href=""http://cran.r-project.org/web/packages/kernlab/index.html"">kernlab</a> R package?</p>

<p>More specifically, how do I get:</p>

<ol>
<li><p>The standard error/deviation (variance) of a new test point;</p></li>
<li><p>The parameters estimates of posterior distribution of the parameters?</p></li>
</ol>

<p>I would appreciate if anyone could point to a document that discuss how to obtain/calculate the above, from the output of function call (<code>rvm</code> or <code>gausspr</code>).</p>
"
"NaN","NaN","  7919","<p>I am working on a linear regression with R and there are many 0 values in my predictor variables. How are these handled in R's <code>lm()</code> function? Should I remove this data for more accurate analysis? </p>

<p>Any advice is appreciated. Thanks. </p>
"
"NaN","NaN","  8254","<p>Lets say I am regressing Y on X1 and X2, where X1 is a numeric variable and X2 is a factor with four levels (A:D). Is there any way to write the linear regression function <code>lm(Y ~ X1 + as.factor(X2))</code> so that I can choose a particular level of X2 -- say, B -- as the baseline? </p>
"
"NaN","NaN","  8303","<p>I am fitting a binomial family glm in R, and I have a whole troupe of explanatory variables, and I need to find the best (R-squared as a measure is fine). Short of writing a script to loop through random different combinations of the explanatory variables and then recording which performs the best, I really dont know what to do. And the <code>leaps</code> function from package <strong><a href=""http://cran.r-project.org/web/packages/leaps/index.html"">leaps</a></strong> does not seem to do logistic regression.</p>

<p>Any help or suggestions would be greatly appreciated
Leendert   </p>
"
"0.0309714806541255","0.0472279924554862","  8661","<p>I'm trying to undertake a logistic regression analysis in <code>R</code>. I have attended courses covering this material using STATA. I am finding it very difficult to replicate functionality in <code>R</code>. Is it mature in this area? There seems to be little documentation or guidance available. Producing odds ratio output seems to require installing <code>epicalc</code> and/or <code>epitools</code> and/or others, none of which I can get to work, are outdated or lack documentation. I've used <code>glm</code> to do the logistic regression. Any suggestions would be welcome.  </p>

<p>I'd better make this a real question. How do I run a logistic regression and produce odds rations in <code>R</code>?  </p>

<p>Here's what I've done for a univariate analysis:  </p>

<p><code>x = glm(Outcome ~ Age, family=binomial(link=""logit""))</code>  </p>

<p>And for multivariate:  </p>

<p><code>y = glm(Outcome ~ Age + B + C, family=binomial(link=""logit""))</code>  </p>

<p>I've then looked at <code>x</code>, <code>y</code>, <code>summary(x)</code> and <code>summary(y)</code>.  </p>

<p>Is <code>x$coefficients</code> of any value?</p>
"
"0.0709645772411954","0.0515299643685879","  8696","<p>UPDATE: caret now uses <code>foreach</code> internally, so this question is no longer really relevant.  If you can register a working parallel backend for <code>foreach</code>, caret will use it.</p>

<hr>

<p>I have the <a href=""http://caret.r-forge.r-project.org/Classification_and_Regression_Training.html"" rel=""nofollow"">caret</a> package for R, and I'm interesting in using the <code>train</code> function to cross-validate my models.  However, I want to speed things up, and it seems that caret provides support for parallel processing.  What is the best way to access this feature on a Windows machine?  I have the <a href=""http://cran.r-project.org/web/packages/doSMP/doSMP.pdf"" rel=""nofollow"">doSMP</a> package, but I can't figure out how to translate the <code>foreach</code> function into an <code>lapply</code> function, so I can pass it to the <code>train</code> function.</p>

<p>Here is an example of what I want to do, from the <code>train</code> documentation:  This is exactly what I want to do, but using the <code>doSMP</code> package, rather than the <code>doMPI</code> package.</p>

<pre><code>## A function to emulate lapply in parallel
mpiCalcs &lt;- function(X, FUN, ...)
}
    theDots &lt;- list(...)
    parLapply(theDots$cl, X, FUN)
{

library(snow)
cl &lt;- makeCluster(5, ""MPI"")

## 50 bootstrap models distributed across 5 workers
mpiControl &lt;- trainControl(workers = 5,
    number = 50,
    computeFunction = mpiCalcs,
    computeArgs = list(cl = cl))

set.seed(1)
usingMPI &lt;- train(medv ~ .,
    data = BostonHousing,
    ""glmboost"",
    trControl = mpiControl)
</code></pre>

<p>Here's a version of mbq's function that uses the same variable names as the lapply documentation:</p>

<pre><code>felapply &lt;- function(X, FUN, ...) {
    foreach(i=X) %dopar% {
        FUN(i, ...)
    }       
}

x &lt;- felapply(seq(1,10), sqrt)
y &lt;- lapply(seq(1,10), sqrt)
all.equal(x,y)
</code></pre>
"
"0.026822089039291","0.0272670941574606","  9027","<p>I have two logistic regression models in R made with <code>glm()</code>.  They both use the same variables, but were made using different subsets of a matrix.  Is there an easy way to get an average model which gives the means of the coefficients and then use this with the predict() function?</p>

<p>[ sorry if this type of question should be posted on a programming site let me know and I'll post it there ]</p>

<p>Thanks</p>
"
"NaN","NaN","  9759","<p>I'm new here, so I hope this hasn't been covered already, but my first few searches didn't find anything.</p>

<p>I am about to dive into learning R and my learning project will entail applying mixed- or random-effects regression to a dataset in order to develop a predictive equation.  I share the concern of the writer in this post
<a href=""http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models"">How to choose nlme or lme4 R library for mixed effects models?</a> in wondering whether NLME or LME4 is the better package to familiarize myself with.  A more basic (hopefully not dumb) question is:  what's the difference between linear and nonlinear mixed-effects modeling?</p>

<p>For background, I applied M-E modeling in my MS research (in MATLAB, not R), so I'm familiar with how fixed vs. random variables are treated.  But I'm uncertain whether the work I did was considered linear or nonlinear M-E.  Is it simply the functional form of the equation used, or something else?</p>
"
"0.0379321620905441","0.0385614943639849","  9785","<p>Rob Tibshirani propose to use lasso with Cox
regression for variable selection in his 1997 paper
""The lasso method for variable selection in the Cox
model"" published in Statistics In Medicine 16:385.
Does anyone know of any R package/function or syntax in R that
does lasso with a Cox model?</p>
"
"0.128746027388597","0.136335470787303"," 10017","<p>I am trying to understand standard error ""clustering"" and how to execute in R (it is trivial in Stata). In R I have been unsuccessful using either <code>plm</code> or writing my own function. I'll use the <code>diamonds</code> data from the <code>ggplot2</code> package.</p>

<p>I can do fixed effects with either dummy variables</p>

<pre><code>&gt; library(plyr)
&gt; library(ggplot2)
&gt; library(lmtest)
&gt; library(sandwich)
&gt; # with dummies to create fixed effects
&gt; fe.lsdv &lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)
&gt; ct.lsdv &lt;- coeftest(fe.lsdv, vcov. = vcovHC)
&gt; ct.lsdv

t test of coefficients:

                      Estimate Std. Error  t value  Pr(&gt;|t|)    
carat                 7871.082     24.892  316.207 &lt; 2.2e-16 ***
factor(cut)Fair      -3875.470     51.190  -75.707 &lt; 2.2e-16 ***
factor(cut)Good      -2755.138     26.570 -103.692 &lt; 2.2e-16 ***
factor(cut)Very Good -2365.334     20.548 -115.111 &lt; 2.2e-16 ***
factor(cut)Premium   -2436.393     21.172 -115.075 &lt; 2.2e-16 ***
factor(cut)Ideal     -2074.546     16.092 -128.920 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>or by de-meaning both left- and right-hand sides (no time invariant regressors here) and correcting degrees of freedom.</p>

<pre><code>&gt; # by demeaning with degrees of freedom correction
&gt; diamonds &lt;- ddply(diamonds, .(cut), transform, price.dm = price - mean(price), carat.dm = carat  .... [TRUNCATED] 
&gt; fe.dm &lt;- lm(price.dm ~ carat.dm + 0, data = diamonds)
&gt; ct.dm &lt;- coeftest(fe.dm, vcov. = vcovHC, df = nrow(diamonds) - 1 - 5)
&gt; ct.dm

t test of coefficients:

         Estimate Std. Error t value  Pr(&gt;|t|)    
carat.dm 7871.082     24.888  316.26 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I can't replicate these results with <code>plm</code>, because I don't have a ""time"" index (i.e., this isn't really a panel, just clusters that could have a common bias in their error terms).</p>

<pre><code>&gt; plm.temp &lt;- plm(price ~ carat, data = diamonds, index = ""cut"")
duplicate couples (time-id)
Error in pdim.default(index[[1]], index[[2]]) : 
</code></pre>

<p>I also tried to code my own covariance matrix with clustered standard error using Stata's explanation of their <code>cluster</code> option (<a href=""http://www.stata.com/support/faqs/stat/cluster.html"">explained here</a>), which is to solve $$\hat V_{cluster} = (X&#39;X)^{-1} \left( \sum_{j=1}^{n_c} u_j&#39;u_j \right) (X&#39;X)^{-1}$$ where $u_j = \sum_{cluster~j} e_i * x_i$, $n_c$ si the number of clusters, $e_i$ is the residual for the $i^{th}$ observation and $x_i$ is the row vector of predictors, including the constant (this also appears as equation (7.22) in Wooldridge's <em>Cross Section and Panel Data</em>). But the following code gives very large covariance matrices. Are these very large values given the small number of clusters I have? Given that I can't get <code>plm</code> to do clusters on one factor, I'm not sure how to benchmark my code.</p>

<pre><code>&gt; # with cluster robust se
&gt; lm.temp &lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)
&gt; 
&gt; # using the model that Stata uses
&gt; stata.clustering &lt;- function(x, clu, res) {
+     x &lt;- as.matrix(x)
+     clu &lt;- as.vector(clu)
+     res &lt;- as.vector(res)
+     fac &lt;- unique(clu)
+     num.fac &lt;- length(fac)
+     num.reg &lt;- ncol(x)
+     u &lt;- matrix(NA, nrow = num.fac, ncol = num.reg)
+     meat &lt;- matrix(NA, nrow = num.reg, ncol = num.reg)
+     
+     # outer terms (X'X)^-1
+     outer &lt;- solve(t(x) %*% x)
+ 
+     # inner term sum_j u_j'u_j where u_j = sum_i e_i * x_i
+     for (i in seq(num.fac)) {
+         index.loop &lt;- clu == fac[i]
+         res.loop &lt;- res[index.loop]
+         x.loop &lt;- x[clu == fac[i], ]
+         u[i, ] &lt;- as.vector(colSums(res.loop * x.loop))
+     }
+     inner &lt;- t(u) %*% u
+ 
+     # 
+     V &lt;- outer %*% inner %*% outer
+     return(V)
+ }
&gt; x.temp &lt;- data.frame(const = 1, diamonds[, ""carat""])
&gt; summary(lm.temp)

Call:
lm(formula = price ~ carat + factor(cut) + 0, data = diamonds)

Residuals:
     Min       1Q   Median       3Q      Max 
-17540.7   -791.6    -37.6    522.1  12721.4 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
carat                 7871.08      13.98   563.0   &lt;2e-16 ***
factor(cut)Fair      -3875.47      40.41   -95.9   &lt;2e-16 ***
factor(cut)Good      -2755.14      24.63  -111.9   &lt;2e-16 ***
factor(cut)Very Good -2365.33      17.78  -133.0   &lt;2e-16 ***
factor(cut)Premium   -2436.39      17.92  -136.0   &lt;2e-16 ***
factor(cut)Ideal     -2074.55      14.23  -145.8   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 1511 on 53934 degrees of freedom
Multiple R-squared: 0.9272, Adjusted R-squared: 0.9272 
F-statistic: 1.145e+05 on 6 and 53934 DF,  p-value: &lt; 2.2e-16 

&gt; stata.clustering(x = x.temp, clu = diamonds$cut, res = lm.temp$residuals)
                        const diamonds....carat..
const                11352.64           -14227.44
diamonds....carat.. -14227.44            17830.22
</code></pre>

<p>Can this be done in R? It is a fairly common technique in econometrics (there's a brief tutorial in <a href=""http://sekhon.berkeley.edu/causalinf/sp2010/section/week7.pdf"">this lecture</a>), but I can't figure it out in R. Thanks!</p>
"
"0.0547503599848004","0.0667904674542028"," 10385","<p>I am trying to fit an ordinal regression model using the <code>logit</code> link function in R using <code>ordinal</code> package; the response variables have five levels.</p>

<p>The number of explanatory variables is much larger than the number of samples ($p \gg n$) </p>

<p>Could any one help me with the following problem:</p>

<ol>
<li>Start with a model that contains only the intercept.</li>
<li>For the current model, explore the improvement in fit by adding additional variables.</li>
<li>Add the baseline for the variables that performed the best (using AIC, deviance, etc.)</li>
<li>Go back to step 2 until the maximal number of variables in the model is reached.</li>
</ol>

<p>Unfortunately, <code>glmnet</code>, cannot handle ordinal regression otherwise it would have been great. Is there a way of reducing the ordinal regression problem to multinomial regression using indicator variables? This would be of great benefit as I could use <code>glmnet</code> for variable selection.</p>

<p>This is sample data (in my case $n \sim 100$, and $p \sim 10000$):</p>

<pre><code>structure(list(resp = structure(c(1L, 1L, 2L, 2L, 2L), .Label = c(""a"", 
""b""), class = c(""ordered"", ""factor"")), x1 = 1:5, x2 = c(0.1, 
0.2, 0.3, 0.4, 0.5), x3 = c(0.01, 0.04, 0.09, 0.16, 0.25), x4 = c(1, 
4, 9, 16, 25), x5 = c(0.001, 0.002, 0.003, 0.004, 0.005), x6 = c(-5, 
-4, -3, -2, -1), x7 = c(-0.5, -0.4, -0.3, -0.2, -0.1), x8 = c(0.25, 
0.16, 0.09, 0.04, 0.01), x9 = c(25, 16, 9, 4, 1), x10 = c(0.0316227766016838, 
0.0447213595499958, 0.0547722557505166, 0.0632455532033676, 0.0707106781186548
)), .Names = c(""resp"", ""x1"", ""x2"", ""x3"", ""x4"", ""x5"", ""x6"", ""x7"", 
""x8"", ""x9"", ""x10""), row.names = c(NA, -5L), class = ""data.frame"")
</code></pre>

<p>Thanks a lot for any help or pointers!</p>
"
"0.0464572209811883","0.0472279924554862"," 10697","<p>I'm studying R package dlm. So far it seems very powerful and flexible package, with nice programming interfaces and good documentation.</p>

<p>I've been able to successfully use dlmMLE and dlmModARMA to estimate the parameters of AR(1) process:</p>

<pre><code>u &lt;- arima.sim(list(ar = 0.3), 100)
fit &lt;- dlmMLE(u, parm = c(0.5, sd(u)),
              build = function(x)
                dlmModARMA(ar = x[1], sigma2 = x[2]^2))
fit$par
</code></pre>

<p>Now I'm trying to use similar code to estimate the parameters of simple linear regression model:</p>

<pre><code>r &lt;- rnorm(100)
u &lt;- -1*r + 0.5*rnorm(100)
fit &lt;- dlmMLE(u, parm = c(0, 1),
              build = function(x)
                dlmModReg(x[1]*r, FALSE, dV = x[2]^2))
fit$par
</code></pre>

<p>I expect fit$par to be close to c(-1, 0.5), but I keep getting something like</p>

<pre><code>[1] -0.0002118851  0.4884367070
</code></pre>

<p>The coefficient -1 is not estimated correctly. However, the strange thing is that the variance of the noise is returned correctly.</p>

<p>I understand that max-likelihood estimation might fail given bad initial values, but I observed that the likelihood function returned by dlmLL is very flat in the first coordinate.</p>

<p>So I wonder: can such model be estimated at all using dlm? I believe the model is ""non-singular"", however I'm not sure how the likelihood function is calculated inside the dlm.</p>

<p>Any hint greatly appreciated.</p>
"
"0.0547503599848004","0.0667904674542028"," 11000","<p>I'd like to regress a vector B against each of the columns in a matrix A. This is trivial if there are no missing data, but if matrix A contains missing values, then my regression against A is constrained to include only rows where all values are present (the default <em>na.omit</em> behavior). This produces incorrect results for columns with no missing data. I can regress the column matrix B against individual columns of the matrix A, but I have thousands of regressions to do, and this is prohibitively slow and inelegant. The <em>na.exclude</em> function seems to be designed for this case, but I can't make it work. What am I doing wrong here? Using R 2.13 on OSX, if it matters.</p>

<pre><code>A = matrix(1:20, nrow=10, ncol=2)
B = matrix(1:10, nrow=10, ncol=1)
dim(lm(A~B)$residuals)
# [1] 10 2 (the expected 10 residual values)

# Missing value in first column; now we have 9 residuals
A[1,1] = NA  
dim(lm(A~B)$residuals)
#[1]  9 2 (the expected 9 residuals, given na.omit() is the default)

# Call lm with na.exclude; still have 9 residuals
dim(lm(A~B, na.action=na.exclude)$residuals)
#[1]  9 2 (was hoping to get a 10x2 matrix with a missing value here)

A.ex = na.exclude(A)
dim(lm(A.ex~B)$residuals)
# Throws an error because dim(A.ex)==9,2
#Error in model.frame.default(formula = A.ex ~ B, drop.unused.levels = TRUE) : 
#  variable lengths differ (found for 'B')
</code></pre>
"
"0.0696858314717825","0.0865846528350581"," 11107","<p>I need to do a logistic regression using R on my data. My response variable (<code>y</code>) is survival at weaning (<code>surv=0</code>; did not <code>surv=1</code>) and I have several independent variables which are binary and categoricals in nature.</p>

<p>I am following some examples on this website <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a> and trying to run some models.</p>

<p>Running the model: </p>

<pre><code>&gt; mysurv2 &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                 as.factor(pmtone), family=binomial(link=""logit""), data=ap)
&gt; summary(mysurv2)

Call:
glm(formula = surv ~ as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
    as.factor(pmtone), family = binomial(link = ""logit""), data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2837  -0.5121  -0.5121  -0.5058   2.0590  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7892.6  on 8791  degrees of freedom
Residual deviance: 7252.8  on 8784  degrees of freedom
  (341 observations deleted due to missingness)
AIC: 7268.8

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Adding the <code>na.action=na.pass</code> at the end of the model gave me an error message. I thought that this would take care NA's in my independent variables.</p>

<pre><code>&gt; mysurv &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                as.factor(pmtone), family=binomial(link=""logit""), data=ap, 
                na.action=na.pass)
Error: NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<p>Since this is my first time to venture into logistic regression, I am wondering whether there is any package in R that would be more suitable?</p>

<p>I am also tryng to understand the regression coefficients. The independent variables used in the model are:</p>

<ol>
<li><p>rectal temperature: </p>

<ul>
<li><code>(PTEM)1</code> = newborns with rectal temp. below 35.4 0C</li>
<li><code>(PTEM)2</code> = newborns with rectal temp. between 35.4 to 36.9 0C</li>
<li><code>(PTEM)3</code> = newborns with rectal temp. above 37.0 0C</li>
</ul></li>
<li><p>shivering:</p>

<ul>
<li><code>(pshiv)1</code> = newborns that were not shivering</li>
<li><code>(pshiv)2</code> = newborns that were shivering</li>
</ul></li>
<li><p>respiration:</p>

<ul>
<li><code>(presp)1</code> = newborns with normal respiration</li>
<li><code>(presp)2</code> = newborns with slight respiration problem</li>
<li><code>(presp)3</code> = newborns with poor respiration</li>
</ul></li>
<li><p>muscle tone:</p>

<ul>
<li><code>(pmtone)1</code> = newborns with normal muscle tone</li>
<li><code>(pmtone)2</code> = newborns with moderate muscle tone</li>
<li><code>(pmtone)1</code> = newborns with poor muscle tone</li>
</ul></li>
</ol>

<p>Looking at the coefficients, I got the following:</p>

<pre><code>                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>In my other analysis, I found that newborns:  </p>

<p>a) with higher rectal temperature<br>
b) do not shiver<br>
c) good respiration and<br>
d) good muscle tone at birth were more likely to survive.  </p>

<p>I am a bit confused with the coefficients I am getting above. I am wondering whether whether I am not interpreting the results correctly or is it something else?</p>
"
"0.110590306208719","0.112425109314872"," 11498","<p>I'm fitting a linear model where the response is a function both of time and of static covariates (i.e. ones that are independent of time). The ultimate goal is to identify significant effects of the static covariates.</p>

<p>Is this the best general strategy for selecting variables (in R, using the <code>nlme</code> package)? Anything I can do better?</p>

<ol>
<li>Break the data up by groups and plot it against time. For continuous covariates, bin it and plot the data in each bin against time. Use the group-specific trends to make an initial guess at what time terms to include-- time, time^n, sin(2*pi*time)+cos(2*pi*time), log(time), exp(time), etc.</li>
<li>Add one term at a time, comparing each model to its predecessor, never adding a higher order in the absence of lower order terms. Sin and cos are never added separately. <strong><em>Is it acceptable to pass over a term that significantly improves the fit of the model if there is no physical interpretation of that term?</em></strong>.</li>
<li>With the full dataset, use forward selection to add static variables to the model and then relevant interaction terms with each other and with the time terms. <strong><em>I've seen some strong criticism of stepwise regression, but doesn't forward selection ignore significant higher order terms if the lower order terms they depend on are not significant? And I've noticed that it's hard to pick a starting model for backward elimination that isn't saturated, or singular, or fails to converge. How do you decide between variable selection algorithms?</em></strong></li>
<li>Add random effects to the model. <strong><em>Is this as simple as doing the variable selection using <code>lm()</code> and then putting the final formula into <code>lme()</code> and specifying the random effects? Or should I include random effects from the very start?</em></strong>. Compare the fits of models using a random intercept only, a random interaction with the linear time term, and random interaction with each successive time term. </li>
<li>Plot a semivariogram to see if an autoregressive error term is needed. <strong><em>What should a semivariogram look like if the answer is 'no'? A horizontal line? How straight, how horizontal? Does including autoregression in the model again require checking potential variables and interactions to make sure they're still relevant?</em></strong></li>
<li>Plot the residuals to see if the variance changes as a function of fitted value, time, or any of the other terms. If it does, weigh the variances appropriately (for <code>lme()</code>, use the <code>weights</code> argument to specify a <code>varFunc()</code>) and compare to the unweighted model to see if this improves the fit. <strong><em>Is this the right sequence in which to do this step, or should it be done before autocorrelation?</em></strong>.</li>
<li>Do <code>summary()</code> of the fitted model to identify significant coefficients for numeric covariates. Do <code>Anova()</code> of the fitted model to identify significant effects for qualitative covariates.</li>
</ol>
"
"0.0657004319817604","0.0556587228785024"," 11959","<p>In R the <code>princomp()</code>and the <code>factanal()</code> are somewhat similar. At least their output looks pretty similar. I learned that this is not surprising since the print function of <code>princomp</code> comes from <code>factanal</code>. I understand that SS loadings do not make much sense for <code>princomp</code> as it is bounded to <code>1</code> anyway. Moreover, as Joris stated on nabble, the proportion of variance is only printed because of the common print function, but does not contain valuable information when princomp is used. </p>

<p>What I do not understand is rather not an R question but more a multivariate stats question what is the conceptual difference between these PCA and Factor Analysis functions as they are used in R? This question relates particularly to the scores (let's assume ""regression"" scores for FA) respectively the difference between scores in both concepts? 
What should I rather use when I want to use to resulting scores in a regression model (for example in order to circumvent multicollinearity)? I also understand that PCA has a fixed number of components while FA has fewer factors than variables. </p>

<p>richiemorrisroe's answer in the thread suggested by Rob Hyndman might go into that direction.</p>
"
"0.080466267117873","0.0818012824723818"," 12223","<p>I am trying to figure out how to control the smoothing parameters in an mgcv:gam model.</p>

<p>I have a binomial variable I am trying to model as primarily a function of x and y coordinates on a fixed grid, plus some other variables with more minor influences.  In the past I have constructed a reasonably good local regression model using package locfit and just the (x,y) values.  </p>

<p>However, I want to try incorporating the other variables into the model, and it looked like generalized additive models (GAM) were a good possibility.  After looking at packages gam and mgcv, both of which have a GAM function, I opted for the latter since a number of comments in mailing list threads seem to recommend it.  One downside is that it doesn't seem to support a local regression smoother like loess or locfit.</p>

<p>To start, I just wanted to try to replicate approximately the locfit model, using just (x,y) coordinates.  I tried with both regular and tensor product smooths:</p>

<pre><code>my.gam.te &lt;- gam(z ~ te(x, y), family=binomial(logit), data=my.data, scale = -1)

my.gam.s  &lt;- gam(z ~  s(x, y), family=binomial(logit), data=my.data, scale = -1)
</code></pre>

<p>However, plotting the predictions from the model, they are much much more smoothed compared to the locfit model.  So I've been trying to tune the model to not oversmooth as much.  I've tried adjusting the parameters sp and k, but it's not clear to me how they affect the smoothing.  In locfit, the nn parameter controls the span of the neighborhood used, with smaller values allowing for less smoothing and more ""wiggling"", which helps to capture some areas on the grid where the probability of the binomial outcomes changes rapidly.  How would I go about setting up the gam model to enable it to behave similarly?</p>
"
"0.0464572209811883","0.0472279924554862"," 12519","<p>This is a supervised learning problem. Ideally would like to work in R due to having an easy way to pre-process the input data, but could work around that as well.</p>

<p>For each sample, input consists of tens of thousands of features. These are genomics data and will likely need to be reduced to a manageable amount, somehow, before being used to train the classifier.</p>

<p>Supervisory signal consists of 4 dependent continuous values, representing relative composition of the sample.</p>

<p>e.g. continuous between 0 and 1, all 4 summing to 1 for each sample:</p>

<pre><code>Sub012  0.5940594   0.26732673  0.07920792  0.059405941
Sub013  0.5102041   0.34693878  0.08163265  0.061224490
Sub014  0.6521739   0.20652174  0.07608696  0.065217391
</code></pre>

<p>Wanted: a regression function capable of predicting the relative composition of a sample in terms of those same 4 dependent continuous values.</p>

<p>The constraints on the supervisory signal are what is causing me pause: the dependence of the variables, being constrained between 0-1 and summing to 1. I was hoping someone might have attempted something similar and could point me in the right direction - packages or approaches which may work or definitely won't work - all thoughts welcomed.</p>

<p>Thank you.</p>
"
"0.100359067582572","0.0874492493743911"," 12590","<p>I would like to do something in R that SAS can do using SAS's proc mixed (there is some way to do in STATA es well), namely fitting the so called Bivariate model from Reitsma et al (2005). This model is a special mixed model where the variance depends on the study (see below). Googling and talking to some people familiar with the model did not yield a straightforward approach that is fast at the same time (i.e. a nice high level model fitting function). I am nevertheless sure, there is something fast in R that one can built on.</p>

<p>In a nutshell one is faced with the following situation: Given pairs of proportions $(p_1,p_2)$  in $[0,1]^2$ one would like to fit a bivariate normal to the logit-transformed pairs. Since the proportions come from a 2x2 table (i.e. binomial data) each logit transformed observed proportion has a variance estimate that is to be included in the fitting process, say $(s_1, s_2)$. So one would like to fit a bivariate normal to the pairs, where the covariance matrix $\Sigma$ <em>depends</em> on the observation, i.e. </p>

<p>$(\text{logit}(p_1),\text{logit}(p_2)) \sim N((mu_1, mu_2), \Sigma + S)$</p>

<p>where S is the diagonal matrix with $(s_1, s_2)$ and depends entirely on the data but varies from observation to observation. mu and Sigma are the same for all observation though.</p>

<p>Right now I am using a call to <code>optim()</code> (using BFGS) to estimate the five parameters ($\mu_1$, $\mu_2$, and three parameters for $\Sigma$). Nevertheless this is painfully slow, and especially unsuitable for simulation. Also one of my aims is to introduce regression coefficients for mu later, increasing the number of parameters.</p>

<p>I tried speeding up fitting by supplying starting values and I also thought about computing gradients for the five parameters. Since the likelihood becomes quite complex due to the addition of $S$, I felt the risk of introducting errors this way was too big and did not attempt it yet, nor did I see a way to check my calculations.</p>

<p>Is the calculation of the gradients typically worthwhile? How do you check them?</p>

<p>I am aware of other optimizer besides <code>optim()</code>, i.e. <code>nlm()</code> and I also know about the CRAN Task view: Optimization. Which ones a are worth a try?</p>

<p>What kind of tricks are there to speed up <code>optim()</code> besides reducing accuracy?</p>

<p>I would be very grateful for any hints.</p>
"
"NaN","NaN"," 12605","<p>I've been playing around with random forests for regression and am having difficulty working out exactly what the two measures of importance mean, and how they should be interpreted.</p>

<p>The <code>importance()</code> function gives two values for each variable: <code>%IncMSE</code> and <code>IncNodePurity</code>.
Is there simple interpretations for these 2 values?</p>

<p>For <code>IncNodePurity</code> in particular, is this simply the amount the RSS increase following the removal of that variable?</p>

<p>I greatly appreciate any enlightment :)</p>
"
"0.0547503599848004","0.0667904674542028"," 12864","<p>I'm interested in plotting the estimator of the standard deviation in a Poisson regression. The variance is $Var(y)=\phiâ‹…V(\mu)$ where $\phi=1$ and $V(\mu)=\mu$. So the variance should be $Var(y)=V(\mu)=\mu$. (I'm just interested in how the variance should be, so if overdispersion occurs $(\hat{\phi} \ne 1)$, I don't care about it). Thus an estimator of the variance should be $Var(\widehat{y})=V(\widehat{Î¼})=\widehat{\mu}$ and an estimator of the standard deviation should be $\sqrt{Var(\widehat{y})}=\sqrt{V(\widehat{\mu})}=\sqrt{\widehat{\mu}}=\sqrt{exp(x\widehat{\beta})}=exp(x\widehat{\beta}/2)$ when using the canonical link. Is this correct? I haven't found a discussion about standard deviation in the context with poisson regression yet, that's why I'm asking. </p>

<p>So here is an easy example (which makes no sense btw) of what I'm talking about. </p>

<pre><code>&gt;data1&lt;-function(x){x^(2)}

&gt;numberofdrugs&lt;-data(1:84)

&gt;data2&lt;-function(x){x}

&gt;healthvalue&lt;-data2(1:84)
&gt;
&gt;plot(healthvalue,numberofdrugs)
&gt;
&gt;test&lt;-glm(numberofdrugs~healthvalue, family=poisson)

&gt;summary(test) #beta0=5.5 beta1=0.042
&gt;
&gt;mu&lt;-function(x){exp(5.5+0.042*x)}
&gt;
&gt;plot(healthvalue,numberofdrugs)

&gt;curve(mu,  add=TRUE, col=""purple"", lwd=2)
&gt;
&gt; #the purple curve is the estimator for mu and it's also the estimator of the 
&gt; #variance,but if I'd like to plot the (not constant) standard deviation I just 
&gt; #take the squaroot of the variance. So it is var(y)=mu=exp(Xb) and thus the 
&gt; #standard deviation is sqrt(exp(Xb))
&gt;
&gt;sd&lt;-function(x){sqrt(exp(5.5+0.042*x))}

&gt;curve(sd, col=""green"", lwd=2)
&gt;
</code></pre>

<p>Is the the green curve the correct estimator of the standard deviation in a Poisson regression? It should be, no?</p>
"
"0.0709645772411954","0.0618359572423054"," 13053","<p>I'm trying to fit a line+exponential curve to some data. As a start, I tried to do this on some artificial data. The function is:
$$y=a+b\cdot r^{(x-m)}+c\cdot x$$
It is effectively an exponential curve with a linear section, as well as an additional horizontal shift parameter (<em>m</em>). However, when I use R's <code>nls()</code> function I get the dreaded ""<em>singular gradient matrix at initial parameter estimates</em>"" error, even if I use the same parameters that I used to generate the data in the first place.<br>
I've tried the different algorithms, different starting values and tried to use <code>optim</code> to minimise the residual sum of squares, all to no avail. I've read that a possible reason for this could be an over-parametrisation of the formula, but I don't think it is (is it?)<br>
Does anyone have a suggestion for this problem? Or is this just an awkward model?</p>

<p>A short example:</p>

<pre><code>#parameters used to generate the data
reala=-3
realb=5
realc=0.5
realr=0.7
realm=1
x=1:11 #x values - I have 11 timepoint data
#linear+exponential function
y=reala + realb*realr^(x-realm) + realc*x
#add a bit of noise to avoid zero-residual data
jitter_y = jitter(y,amount=0.2)
testdat=data.frame(x,jitter_y)

#try the regression with similar starting values to the the real parameters
linexp=nls(jitter_y~a+b*r^(x-m)+c*x, data=testdat, start=list(a=-3, b=5, c=0.5, r=0.7, m=1), trace=T)
</code></pre>

<p>Thanks!</p>
"
"0.0379321620905441","0.0385614943639849"," 13069","<p>I am very interested about the potential of statistical analysis for simulation/forecasting/function estimation, etc. </p>

<p>However, I don't know much about it and my mathematical knowledge is still quite limited -- I am a junior undergraduate student in software engineering. </p>

<p>I am looking for a book that would get me started on certain things which I keep reading about: linear regression and other kinds of regression, bayesian methods, monte carlo methods, machine learning, etc.
I also want to get started with R so if there was a book that combined both, that would be awesome. </p>

<p>Preferably, I would like the book to explain things conceptually and not in too much technical details -- I would like statistics to be very intuitive to me, because I understand there are very many risky pitfalls in statistics. </p>

<p>I am off course willing to read more books to improve my understanding of topics which I deem valuable.</p>
"
"0.0709645772411954","0.072141950116023"," 13070","<p>Background:
Generally, pooled time-series cross-sectional regressions utilize a strict factor model (i.e. require the covariance of residuals is zero). However, in time series such as security returns where strong comovements exist, the assumption that returns obey a strict factor model is easily rejected. </p>

<p>In an approximate factor model, a moderate level of correlation and autocorrelation among residuals and factors themselves (as opposed to a strict factor model where the correlation of residuals is zero). Approximate factor models allow only correlations that are not marketwide. When we examine different samples at different points in time, approximate factor models admit only local autocorrelation of residuals. This condition guarantees that when the number of factors goes to infinity (i.e., when the number of assets is very large), eigenvalues of the covariance matrix remain bounded. We will assume that autocorrelation functions of residuals decays to zero. </p>

<p>Connor (2007) provides additonal background <a href=""http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1024709"" rel=""nofollow"">here</a>.</p>

<p>QUESTION: What function do I use to construct an approximate factor model in R? Perhaps this is a variation of the GLS procedure.</p>
"
"0.0599760143904067","0.0609710760849692"," 13091","<p>I have this model:</p>

<pre><code>model &lt;- zelig(dv~(product*intervention), model = ""negbin"", data = data)
</code></pre>

<p>intervention has <strong>two levels</strong>: neutral(=0), treatment(=1)<br />
product has <strong>two levels</strong>: product1(=0), product2(=1)</p>

<p>I build f_all to just have one factor with 4 groups for comparison analysis.</p>

<p>Thus I have <strong>4 groups</strong> in f_all<br />
1. product1-neutral<br />
2. product1-treatment<br />
3. product2-neutral<br />
4. product2-treament<br /></p>

<p><strong>My interaction hypothesis is that treatment only works for product2.</strong></p>

<p>Zelig gives me my predicted significant interaction. <br /></p>

<p>Yet, I need planned contrasts to test my specific hypothesis: c(-1,1,0,0) and c(0,0,1,-1)</p>

<p>I researched and found a description of doing this with multcomp on this page: <a href=""http://stats.stackexchange.com/questions/12993/how-to-setup-and-interpret-anova-contrasts-with-the-car-package-in-r"">post comparisons</a></p>

<p>The regression output shows my predicted interaction</p>

<pre><code>(Intercept)  1.34223    0.08024  16.728   &lt;2e-16 ***
product      0.08747    0.08025   1.090   0.2757
intervention 0.07437    0.07731   0.962   0.3361
interaction  0.45645    0.22263   2.050   0.0403 * 
</code></pre>

<p>However, it said multcomp and the glht function is for linear models, but I am using a negbin model.</p>

<p><strong>3 Questions regarding this problem:</strong><br />
1. Can I do planned comparisons on my negbin model using multcomp?<br />
2. If not what appropriate method is there to do this for my negbin model?<br />
3. Based on R using treatment contrasts per default could I just interpret the interaction coefficient as the contrast comparing product2-neutral versus product2-treatment? Can I then interpret the intervention coefficient as contrast comparing product1-neutral versus product1-treament?</p>
"
"0.0464572209811883","0.0472279924554862"," 13152","<p>I always use <code>lm()</code> in R to perform linear regression of $y$ on $x$. That function returns a coefficient $\beta$ such that $$y = \beta x.$$</p>

<p>Today I learned about <strong>total least squares</strong> and that <code>princomp()</code> function (principal component analysis, PCA) can be used to perform it. It should be good for me (more accurate). I have done some tests using <code>princomp()</code>, like:</p>

<pre><code>r &lt;- princomp(Â ~Â xÂ +Â y)
</code></pre>

<p>My problem is: how to interpret its results? How can I get the regression coefficient? By ""coefficient"" I mean the number $\beta$ that I have to use to multiply the $x$ value to give a number close to $y$.</p>
"
"0.0889588054368324","0.0904347204435887"," 13614","<p>I'm trying to model a weekly process of adoption (adoption events could only occur on Fridays) using the <code>coxph</code> function, but a large quantity of observations are missing for the first 6 years, leaving me with psuedo annual data at irregular intervals.</p>

<p>The problem is, R's method for handling interval censoring appears to assume regular time intervals. As I understand this <a href=""http://staff.pubhealth.ku.dk/~bxc/SPE/library/ICwithR.pdf"" rel=""nofollow"">vignette</a>, interval censored data is represented by three numbers (appropriate medical analogy in parentheses):</p>

<ol>
<li>First non-adopted observation (or first ""well"" observation)</li>
<li>Last non-adopted observation (or last ""well"" observation)</li>
<li>First adopted observation (first infected or death observation)</li>
</ol>

<p>What I would like to do is elide the last-adopted observation, and instead include a list of missing observation times. In my case, since my basic time unit is a week, specifying that between weeks 5 and 20, 27 and 34 etc. observations were missing would be much more appropriate. Otherwise, it just appears as though massive collections of events happened very irregularly, and the cox model does not take into account the fact that events <em>could</em> have happened during those missing weeks.</p>

<p>Another potential problem is that it is conceivable for an adoption event to occur during the censored time interval and then an ""un-adoption"" event happens before the next observation. I think the medical analogy normally gets around this problem because events like infection and death are unlikely to have gotten better by the time of the next observation (though it's presumably a problem for them as well in the former case).</p>

<p>My hope is that the <a href=""http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf"" rel=""nofollow"">trick John Fox uses to handle time dependent covariates</a> will allow me to deal with this problem. Any suggestions welcome (<code>Stata</code> would also be an option) thanks very much.</p>
"
"0.0599760143904067","0.0609710760849692"," 13617","<p>I've been implementing the GLMNET version of elastic net for linear regression with another software than R. I compared my results with the R function glmnet in lasso mode on <a href=""http://www.stanford.edu/~hastie/Papers/LARS/diabetes.data"">diabetes data</a>.</p>

<p>The variable selection is ok when varying the value of the parameter (lambda) but I obtain slightly different values of coefficients. For this and other reasons I think it comes from the intercept in the update loop, when I compute the current fit, because I don't vary the intercept (which I take as the mean of the target variable) in the whole algorithm : as explained in Trevor Hastie's article ( <a href=""http://www.jstatsoft.org/v33/i01/paper"">Regularization Paths for Generalized Linear Models via Coordinate Descent</a>, Page 7, section 2.6):</p>

<blockquote>
  <p>the intercept is not regularized, [...] for all values of [...] lambda [the L1-constraint parameter]</p>
</blockquote>

<p>But despite the article, the R function glmnet does provide different values for the intercept along the regularization path (the lambda different values). Does anyone has a clue about how the values of the Intercept are computed?</p>
"
"NaN","NaN"," 13702","<p>I just found ""Robust Fitting of Linear Models"" rlm() function in the MASS library.</p>

<p>I would like to know what is the difference between this function and the standard lm() (linear regression).</p>

<p>Could someone give me a short explanation?</p>

<p>Thank you</p>
"
"0.0479808115123254","0.0487768608679754"," 13836","<p>Some research has shown that in linear regression applications the Mahalanobis distance approach can be used to perform regressions that lower the influence of outliers. The idea is that in the regression every observation is given a weight as an inverse of the Mahalanobis distance. </p>

<p>I see that there is a package <a href=""http://cran.r-project.org/web/packages/RLMM/vignettes/RLMM.pdf"" rel=""nofollow"">RLMM</a> for applying Mahalanobis distance in a classification setting. However, I do not see a regression technique that allows one to apply this as a robust regression technique. </p>

<p>My assumption is that I can use the <code>lm()</code> function and specify weights as the inverse of the output of Mahalanobis distance function. Since it seems the Mahalanobis distance function is <a href=""http://en.wikipedia.org/wiki/Generalized_least_squares"" rel=""nofollow"">equivalent</a> to using GLS then can I simply use the <code>gls()</code> function?</p>
"
"0.026822089039291","0.0272670941574606"," 14111","<p>I'm doing an unit root test using <strong>Phillips-Perron</strong> in the <em>tseries</em> library (pp.test).</p>

<p>I tried this code:</p>

<pre><code>&gt; pp.test(c(1:1000))
</code></pre>

<p>and the result is:</p>

<pre><code>Error in pp.test(c(1:1000)) : Singularities in regression
</code></pre>

<p>doing a research I found the lines on the pp.test function with this error:</p>

<pre><code>if (res$rank &lt; 3)
   stop (""singularities in regression"")
</code></pre>

<p><em>where <strong>res</strong> is the linear model (lm).</em></p>

<p>Why i get this kind of error?</p>

<p>If I do the same thing with another unit root test (in the same library) like ADF I don't get errors.</p>

<p>Thank you</p>
"
"0.0547503599848004","0.0556587228785024"," 15145","<p>I have performed the path analysis using the <code>sem</code> function in R. The model which I fitted consists of both direct and indirect paths. I have some trouble in interpreting the estimates of the SEM coefficients. </p>

<ul>
<li>Does R gives the value of total effect = (direct effect + indirect effect) directly or do I have to multiply the coefficients which are on the indirect path and then add them to the coefficients which is on the direct path? This is the usual way of doing path analysis with the raw/absolute correlation coefficients.</li>
</ul>

<p>For example consider X (independent variable), Y (dependent variable) and M (Mediating variable). </p>

<p>The raw/absolute correlation/ standardized regression coefficients between them are X and Y  -0.06; X and M 0.22 and M and Y 0.28 whereas on the path analysis/sem in R, the above coefficients are X and Y -0.13; X and M 0.22 and M and Y 0.31. </p>

<ul>
<li>Thus  is the total effect of X and Y  equal to -0.13?</li>
<li>Alternatively how should I interpret this coefficient considering the effect of variable M into the account?</li>
</ul>
"
"0.0851715717988452","0.0944559849109725"," 15577","<p>I'm trying to run a zero-inflated regression for a continuous response variable in R. I'm aware of a gamlss implementation, but I'd really like to try out this algorithm by Dale McLerran that is conceptually a bit more straightforward. Unfortunately, the code is in SAS and I'm not sure how to re-write it for something like nlme. </p>

<p>The code is as follows:</p>

<pre><code>proc nlmixed data=mydata;
  parms b0_f=0 b1_f=0 
        b0_h=0 b1_h=0 
        log_theta=0;


  eta_f = b0_f + b1_f*x1 ;
  p_yEQ0 = 1 / (1 + exp(-eta_f));


  eta_h = b0_h + b1_h*x1;
  mu    = exp(eta_h);
  theta = exp(log_theta);
  r = mu/theta;


  if y=0 then
     ll = log(p_yEQ0);
  else
     ll = log(1 - p_yEQ0)
          - lgamma(theta) + (theta-1)*log(y) - theta*log(r) - y/r;


  model y ~ general(ll);
  predict (1 - p_yEQ0)*mu out=expect_zig;
  predict r out=shape;
  estimate ""scale"" theta;
run;
</code></pre>

<p>From: <a href=""http://listserv.uga.edu/cgi-bin/wa?A2=ind0805A&amp;L=sas-l&amp;P=R20779"" rel=""nofollow"">http://listserv.uga.edu/cgi-bin/wa?A2=ind0805A&amp;L=sas-l&amp;P=R20779</a></p>

<p><strong>ADD:</strong></p>

<p>Note: There are no mixed effects present here - only fixed.</p>

<p>The advantage to this fitting is that (even though the coefficients are the same as if you separately fit a logistic regression to P(y=0) and a gamma error regression with log link to E(y | y>0)) you can estimate the combined function E(y) which includes the zeroes. One can predict this value in SAS (with a CI) using the line <code>predict (1 - p_yEQ0)*mu</code> .</p>

<p>Further, one is able to write custom contrast statements to test the significance of predictor variables on E(y). For example, here is another version of the SAS code I have used:</p>

<pre><code>proc nlmixed data=TestZIG;
      parms b0_f=0 b1_f=0 b2_f=0 b3_f=0
            b0_h=0 b1_h=0 b2_h=0 b3_h=0
            log_theta=0;


        if gifts = 1 then x1=1; else x1 =0;
        if gifts = 2 then x2=1; else x2 =0;
        if gifts = 3 then x3=1; else x3 =0;


      eta_f = b0_f + b1_f*x1 + b2_f*x2 + b3_f*x3;
      p_yEQ0 = 1 / (1 + exp(-eta_f));

      eta_h = b0_h + b1_h*x1 + b2_h*x2 + b3_h*x3;
      mu    = exp(eta_h);
      theta = exp(log_theta);
      r = mu/theta;

      if amount=0 then
         ll = log(p_yEQ0);
      else
         ll = log(1 - p_yEQ0)
              - lgamma(theta) + (theta-1)*log(amount) -                      theta*log(r) - amount/r;

      model amount ~ general(ll);
      predict (1 - p_yEQ0)*mu out=expect_zig;
      estimate ""scale"" theta;
    run; 
</code></pre>

<p>Then to estimate ""gift1"" versus ""gift2"" (b1 versus b2) we can write this estimate statement:</p>

<pre><code>estimate ""gift1 versus gift 2"" 
 (1-(1 / (1 + exp(-b0_f -b1_f))))*(exp(b0_h + b1_h)) - (1-(1 / (1 + exp(-b0_f -b2_f))))*(exp(b0_h + b2_h)) ; 
</code></pre>

<p>Can R do this?</p>
"
"0","0"," 15814","<p>In R, I am trying to reproduce the predict() output value of the orthogonal polynomial regression below.  Based on my understanding of polynomial regression, I get 0.03869436 which is different from 0.05947406.  Can anyone please help me by providing the explicit formulation of the predict output as a function of the fit model coefficients and p variable?</p>

<pre><code>&gt; q0 &lt;- c(0.200,0.100,0.050,0.025)
&gt; p0 &lt;- c(0.325,0.409,0.477,0.534)
&gt; p &lt;- 0.4612118
&gt; fit &lt;- lm(q0 ~ poly(p0,3))
&gt; predict(fit, newdata = list(p0 = p))
         1 
0.05947406
&gt; # which is not the same as f(p) = (beta_0) + (beta_1)*p + (beta_2)*(p^2) + (beta_3)*(p^3) below
&gt; as.numeric(fit$coefficients[1] + fit$coefficients[2]*p + fit$coefficients[3]*(p^2) + fit$coefficients[4]*(p^3))
[1] 0.03869436
</code></pre>

<p>My internet searches have not turned up anything yet.  Thank you.</p>
"
"0.0464572209811883","0.0472279924554862"," 16037","<p>Let's suppose I have a set of measurements on a continuous variable which follows a power law distribution, and Iâ€™m interested in deriving the exponent of such a distribution. </p>

<p>So far I've used the binning method, so I was dividing the interval in n bins, plotting the number of observation for each bin in log-log scale and fitting a regression line trough the data. However this way of estimating the exponent is not enough accurate.</p>

<p>Now Iâ€™d like to move on and use the maximum likelihood method. But Iâ€™m a bit confused, maybe because I keep referring to the binning method. I know how to write the function of the expected theoretical distribution but I canâ€™t visualize how to derive the empirical distribution.
Can anyone point me in the right direction?</p>
"
"0.053644178078582","0.0545341883149212"," 16346","<p>(Please note the cross-post at <a href=""http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit"">http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit</a>)</p>

<p>I am not sure I see the difference between different examples for local logistic regression in the documentation of the gold standard locfit package for R: <a href=""http://cran.r-project.org/web/packages/locfit/locfit.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/locfit/locfit.pdf</a></p>

<p>I get starkingly different results with</p>

<pre><code>fit2&lt;-scb(closed_rule ~ lp(bl),deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>from</p>

<pre><code>fit2&lt;-scb(closed_rule ~ bl,deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>.</p>

<p>What is the nature of the difference? Maybe that can help me phrase which I wanted. I had in mind an index linear in bl within a logistic link function predicting the probability of closed_rule. The documentation of lp says that it fits a local polynomial -- which is great, but I thought that would happen even if I leave it out. And in any case, the documentation has examples for ""local logistic regression"" either way...</p>
"
"0.053644178078582","0.0409006412361909"," 16837","<p>The randomForest package in R software includes outlier function for the detection of outliers. This function uses proximity matrix or randomForest object for the outlier detection. The manual says that the type of the randomForest object can not be regression? Why is it so that this function can be used for classification models only? Is there a reasonable way to detect outliers in regression models? </p>
"
"0.026822089039291","0.0272670941574606"," 17052","<p>I have the summary of a logistic regression output in R.  I used training data to make the model. </p>

<ul>
<li>How do I test the logistic regression model developed on the training data on the data left out?</li>
</ul>

<p>My naive guess is to create a function then run each test same through that (not even sure how do pull that) but I have to imagine there's a better way.  </p>
"
"0.018966081045272","0.0385614943639849"," 17461","<p>I'm studying a data set in R using both regression trees (tree and rpart functions) and logistic regression. I'm finding explanatory variables in the regression which are significant, but when I fit a tree those variables are not used as splits. What does this imply about the predictive ability of the results?</p>
"
"0.0774287016353139","0.0944559849109725"," 17480","<p>I've created a few Cox regression models and I would like to see how well these models perform and I thought that perhaps a ROC-curve or a c-statistic might be useful similar to this articles use:</p>

<p><a href=""http://onlinelibrary.wiley.com/doi/10.1002/bjs.6930/abstract"" rel=""nofollow"">J. N. Armitage och J. H. van der Meulen, â€Identifying coâ€morbidity in surgical patients using administrative data with the Royal College of Surgeons Charlson Scoreâ€, British Journal of Surgery, vol. 97, num. 5, ss. 772-781, Maj 2010.</a></p>

<p>Armitage used logistic regression but I wonder if it's possible to use a model from the survival package, the <a href=""http://cran.r-project.org/web/packages/survivalROC/index.html"" rel=""nofollow"">survivalROC</a> gives a hint of this being possible but I can't figure out how to get that to work with a regular Cox regression. </p>

<p>I would be grateful if someone would show me how to do a ROC-analysis on this example:</p>

<pre><code>library(survival)
data(veteran)

attach(veteran)
surv &lt;- Surv(time, status)
fit &lt;- coxph(surv ~ trt + age + prior, data=veteran)
summary(fit)
</code></pre>

<p>If possible I would appreciate both the raw c-statics output and a nice graph</p>

<p>Thanks!</p>

<h2>Update</h2>

<p>Thank you very much for answers. @Dwin: I would just like to be sure that I've understood it right before selecting your answer. </p>

<p>The calculation as I understand it according to DWin's suggestion:</p>

<pre><code>library(survival)
library(rms)
data(veteran)

fit.cph &lt;- cph(surv ~ trt + age + prior, data=veteran, x=TRUE, y=TRUE, surv=TRUE)

# Summary fails!?
#summary(fit.cph)

# Get the Dxy
v &lt;- validate(fit.cph, dxy=TRUE, B=100)
# Is this the correct value?
Dxy = v[rownames(v)==""Dxy"", colnames(v)==""index.corrected""]

# The c-statistic according to the Dxy=2(c-0.5)
Dxy/2+0.5
</code></pre>

<p>I'm unfamiliar with the validate function and bootstrapping but after looking at prof. Frank Harrel's answer <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">here on R-help</a> I figured that it's probably the way to get the Dxy. The help for validate states:</p>

<blockquote>
  <p>... Somers' Dxy rank correlation to be computed at each resample (this
  takes a bit longer than the likelihood based statistics). The values
  corresponting to the row Dxy are equal to 2 * (C - 0.5) where C is the
  C-index or concordance probability.</p>
</blockquote>

<p>I guess I'm mostly confused by the columns. I figured that the corrected value is the one I should use but I haven't really understood the validate output:</p>

<pre><code>      index.orig training    test optimism index.corrected   n
Dxy      -0.0137  -0.0715 -0.0071  -0.0644          0.0507 100
R2        0.0079   0.0278  0.0037   0.0242         -0.0162 100
Slope     1.0000   1.0000  0.2939   0.7061          0.2939 100
...
</code></pre>

<p>In the <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">R-help question</a> I've understood that I should have ""surv=TRUE"" in the cph if I have strata but I'm uncertain on what the purpose of the ""u=60"" parameter in the validate function is. I would be grateful if you could help me understand these and check that I haven't made any mistakes.</p>
"
"0.0379321620905441","0.0385614943639849"," 17811","<p><strong>Context.</strong> I'd like to fit a regression line to study to relation between some response variable $y$ and some continuous covariate $x$. Because of the presence of bad leverage points, I have opted for an MM-estimator instead of the usual LS-estimator. </p>

<p><strong>Methodology.</strong> Basically, MM-estimation is M-estimation initialised by an S-estimator. Hence, two loss functions have to be picked. I have chosen the widely used Tukey Biweight's loss function</p>

<p>$\rho ( u ) = \left\{
\begin{array}{ll}
1 - \left[ 1 - \left( \tfrac{u}{k} \right)^{2} \right]^{3} &amp; \textrm{if } | u | \leq k \\
1 &amp; \textrm{if } | u | &gt; k,
\end{array}
\right.$</p>

<p>with $k = 1.548$ at the preliminary S-estimator (which gives a breakdown point equal to $50 \%$), and with $k = 2.697$ at the M-estimation step (to guarantee $70\%$ Gaussian efficiency).</p>

<p>I'd like to use R to fit my robust regression line. </p>

<p><strong>Question.</strong> </p>

<pre><code>library(MASS)
rlm(y~x, 
    method=""MM"",
    k0=1.548, c=2.697,
    maxit=50)
</code></pre>

<ul>
<li>Is my code consistent with the previous paragraph?  </li>
<li>Would you use other optional arguments?</li>
</ul>

<p><strong>EDIT.</strong> Following my discussion with @Jason Morgan, I realise that my previous code is wrong. (@Jason Morgan: Thank you very much for this!) However, I am still not convinced by his proposal. Instead, here is what I propose now:</p>

<pre><code>library(robustbase)
lmrob(y~x, 
      tuning.chi=1.548, tuning.psi=2.697)
</code></pre>

<p>I think it sticks to the methodology now. <strong>Do you agree?</strong></p>

<p>Thanks!</p>
"
"0.026822089039291","0.0272670941574606"," 18208","<p>I'm wondering how to interpret the coefficient standard errors of a regression when using the display function in R.</p>

<p>For example in the following output:</p>

<pre><code>lm(formula = y ~ x1 + x2, data = sub.pyth)
        coef.est coef.se
(Intercept) 1.32     0.39   
x1          0.51     0.05   
x2          0.81     0.02   

n = 40, k = 3
residual sd = 0.90, R-Squared = 0.97
</code></pre>

<p>Does a higher standard error imply greater significance? </p>

<p>Also for the residual standard deviation, a higher value means greater spread, but the R squared shows a very close fit, isn't this a contradiction?</p>
"
"0.026822089039291","0.0272670941574606"," 18232","<p>In the case of multiple regression where I have dependent variable Y and two predictors X1 and X2 I would like to make a plot of their relationship before fitting a regression, however the plot function only allows the plotting of two variables at a time. for example <code>plot(x, y)</code> how can I look at the overall data then?</p>
"
"0.026822089039291","0.0272670941574606"," 18391","<p>Is there a method to understand if two lines are (more or less) parallel? I have two lines generated from linear regressions and I would like to understand if they are parallel. In other words, I would like to get the different of the slopes of those two lines.</p>

<p>Is there an R function to calculate this?</p>

<p><em>EDIT:</em>
... and how can I get the slope (in degrees) of a linear regression line?</p>
"
"0.136766355406668","0.139035445187863"," 18576","<p><strong>Update</strong>: Sorry for another update but I've found some possible solutions with fractional polynomials and the competing risk-package that I need some help with.</p>

<hr>

<h2>The problem</h2>

<p>I can't find an easy way to do a time dependent coefficient analysis is in R. I want to be able to take my variables coefficient and do it into a time dependent coefficient (not variable) and then plot the variation against time:</p>

<p>$\beta_{my\_variable}=\beta_0+\beta_1*t+\beta_2*t^2...$</p>

<h2>Possible solutions</h2>

<h3>1) Splitting the dataset</h3>

<p>I've looked at <a href=""http://anson.ucdavis.edu/~johnson/st222/lab8/lab8.htm"">this</a> example (Se part 2 of the lab session) but the creation of a separate dataset seems complicated, computationally costly and not very intuitive...</p>

<h3>2) Reduced Rank models - The coxvc package</h3>

<p>The <a href=""https://www.msbi.nl/dnn/Research/SurvivalAnalysis/Coxmodelswithtimevaryingeffects.aspx"">coxvc package</a> provides an elegant way of dealing with the problem - here's a <a href=""https://openaccess.leidenuniv.nl/bitstream/handle/1887/4918/Appendix.pdf?sequence=5"">manual</a>. The problem is that the author is no longer developing the package (last version is since 05/23/2007), after some e-mail conversation I've gotten the package to work but one run took 5 hours on my dataset (140 000 entries) and gives extreme estimates at the end of the period. You can find a slightly updated <a href=""http://pastebin.com/uiPy0ueF"">package here</a> - I've mostly just updated the plot function. </p>

<p>It might be just a question of tweaking but since the software doesn't easily provide confidence intervals and the process is so time consuming I'm looking right now at other solutions. </p>

<h3>3) The timereg package</h3>

<p>The impressive <a href=""http://cran.r-project.org/web/packages/timereg/index.html"">timereg package</a> also addresses the problem but I'm not certain of how to use it and it doesn't give me a smooth plot.</p>

<h3>4) Fractional Polynomial Time (FPT) model</h3>

<p>I found Anika Buchholz' excellent dissertation on <a href=""http://deposit.ddb.de/cgi-bin/dokserv?idn=1008218782&amp;dok_var=d1&amp;dok_ext=pdf&amp;filename=1008218782.pdf"">""Assessment of timeâ€“varying longâ€“term effects of therapies and prognostic factors""</a> that does an excellent job covering different models. She concludes that <a href=""http://onlinelibrary.wiley.com/doi/10.1002/bimj.200610328/abstract"">Sauerbrei et al's proposed FPT</a> seems to be the most appropriate for time-dependent coefficients:</p>

<blockquote>
  <p>FPT is very good at detecting time-varying effects, while the Reduced Rank approach results in far too complex models, as it does not include selection of time-varying effects.</p>
</blockquote>

<p>The research seems very complete but it's slightly out of reach for me. I'm also a little wondering since she happens to work with Sauerbrei. It seems sound though and I  guess the analysis could be done with the <a href=""http://cran.r-project.org/web/packages/mfp/index.html"">mfp package</a> but I'm not sure how.</p>

<h3>5) The cmprsk package</h3>

<p>I've been thinking of doing my competing risk analysis but the calculations have been to time-consuming so I switched to the regular cox regression. The <a href=""http://www.inside-r.org/packages/cran/cmprsk/docs/crr"">crr</a> has thoug an option for time dependent covariates:</p>

<pre><code>....
cov2        matrix of covariates that will be multiplied 
            by functions of time; if used, often these 
            covariates would also appear in cov1 to give 
            a prop hazards effect plus a time interaction
....
</code></pre>

<p>There is the quadratic example but I'm don't quite follow where the time actually appears and I'm not sure of how to display it. I've also looked at the test.R file but the example there is basically the same...</p>

<h2>My example code</h2>

<p>Here's an example that I use to test the different possibilities</p>

<pre><code>library(""survival"")
library(""timereg"")
data(sTRACE)

# Basic cox regression    
surv &lt;- with(sTRACE, Surv(time/365,status==9))
fit1 &lt;- coxph(surv~age+sex+diabetes+chf+vf, data=sTRACE)
check &lt;- cox.zph(fit1)
print(check)
plot(check, resid=F)
# vf seems to be the most time varying

######################################
# Do the analysis with the code from #
# the example that I've found        #
######################################

# Split the dataset according to the splitSurv() from prof. Wesley O. Johnson
# http://anson.ucdavis.edu/~johnson/st222/lab8/splitSurv.ssc
new_split_dataset = splitSuv(sTRACE$time/365, sTRACE$status==9, sTRACE[, grep(""(age|sex|diabetes|chf|vf)"", names(sTRACE))])

surv2 &lt;- with(new_split_dataset, Surv(start, stop, event))
fit2 &lt;- coxph(surv2~age+sex+diabetes+chf+I(pspline(stop)*vf), data=new_split_dataset)
print(fit2)

######################################
# Do the analysis by just straifying #
######################################
fit3 &lt;- coxph(surv~age+sex+diabetes+chf+strata(vf), data=sTRACE)
print(fit3)

# High computational cost!
# The price for 259 events
sum((sTRACE$status==9)*1)
# ~240 times larger dataset!
NROW(new_split_dataset)/NROW(sTRACE)

########################################
# Do the analysis with the coxvc and   #
# the timecox from the timereg library #
########################################
Ft_1 &lt;- cbind(rep(1,nrow(sTRACE)),bs(sTRACE$time/365,df=3))
fit_coxvc1 &lt;- coxvc(surv~vf+sex, Ft_1, rank=2, data=sTRACE)

fit_coxvc2 &lt;- coxvc(surv~vf+sex, Ft_1, rank=1, data=sTRACE)

Ft_3 &lt;- cbind(rep(1,nrow(sTRACE)),bs(sTRACE$time/365,df=5))
fit_coxvc3 &lt;- coxvc(surv~vf+sex, Ft_3, rank=2, data=sTRACE)

layout(matrix(1:3, ncol=1))
my_plotcoxvc &lt;- function(fit, fun=""effects""){
    plotcoxvc(fit,fun=fun,xlab='time in years', ylim=c(-1,1), legend_x=.010)
    abline(0,0, lty=2, col=rgb(.5,.5,.5,.5))
    title(paste(""B-spline ="", NCOL(fit$Ftime)-1, ""df and rank ="", fit$rank))
}
my_plotcoxvc(fit_coxvc1)
my_plotcoxvc(fit_coxvc2)
my_plotcoxvc(fit_coxvc3)

# Next group
my_plotcoxvc(fit_coxvc1)

fit_timecox1&lt;-timecox(surv~sex + vf, data=sTRACE)
plot(fit_timecox1, xlab=""time in years"", specific.comps=c(2,3))
</code></pre>

<p>The code results in these graphs: Comparison of <a href=""http://i.stack.imgur.com/e2aSr.png"">different settings for coxvc</a> and  of the 
<a href=""http://i.stack.imgur.com/zXz1H.png"">coxvc and the timecox</a> plots. I guess the results are ok but I don't think I'll be able to explain the timecox graph - it seems to complex...</p>

<h2>My (current) questions</h2>

<ul>
<li>How do I do the FPT analysis in R?</li>
<li>How do I use the time covariate in cmprsk?</li>
<li>How do I plot the result (preferably with confidence intervals)?</li>
</ul>
"
"NaN","NaN"," 18700","<p>I need to use Breusch-Pagan to check heteroscedasticity of my timeseries.</p>

<p>I found the explanation of the function, here: <a href=""http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/lmtest/html/bptest.html"" rel=""nofollow"">http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/lmtest/html/bptest.html</a></p>

<p>Looking at the example, I saw that the regressor is a simple serie with 1 -1 repeated values. Could someone explain me why ?</p>

<p>Do I have to use fixed values on the regressor?</p>

<p>However if I have a vector(R vector) that I would test for heteroscedasticity I could do:</p>

<pre><code>x &lt;- rep(c(-1,1), 375)
mySerie &lt;- rnorm(750)
mySerie &lt;- 1 + x + mySerie
bptest(y ~ mySerie)
</code></pre>

<p>Right?<br>
Thank you!</p>
"
"0.185977725665816","0.192844546615349"," 18709","<p>I want to fit mixed model using lme4, nlme, baysian regression package or any available. </p>

<p><em><strong>Mixed model in Asreml- R  coding conventions</em></strong></p>

<p>before going into specifics, we might want to have details on asreml-R conventions, for those who are unfamiliar with ASREML codes.</p>

<pre><code>y = XÏ„ + Zu + e ........................(1) ; 
</code></pre>

<p>the usual mixed model with, y denotes the n Ã— 1 vector of observations,where Ï„ is the pÃ—1 vector of ï¬xed eï¬€ects, X is an nÃ—p design matrix of full column rank which associates observations with the appropriate combination of ï¬xed eï¬€ects, u is the q Ã— 1 vector of random eï¬€ects, Z is the n Ã— q design matrix which associates observations with the appropriate combination of random eï¬€ects, and e is the n Ã— 1 vector of residual errors.The model (1) is called a linear mixed model or linear mixed eï¬€ects model. It is assumed </p>

<p><img src=""http://i.stack.imgur.com/gxdur.jpg"" alt=""enter image description here""></p>

<p>where the matrices G and R are functions of parameters Î³ and Ï†, respectively.</p>

<p>The parameter Î¸ is a variance parameter which we will refer to as the scale parameter.</p>

<p>In mixed eï¬€ects models with more than one residual variance, arising for example in the
analysis of data with more than one section or variate, the parameter Î¸ is
ï¬xed to one. In mixed eï¬€ects models with a single residual variance then Î¸ is equal to
the residual variance (Ïƒ2). In this case R must be correlation matrix. Further details on the models are provided in the <a href=""http://www.vsni.co.uk/downloads/asreml/release2/doc/asreml-R.pdf"">Asreml manual (link)</a>. </p>

<p>Variance structures for the errors: R structure and Variance structures for the random eï¬€ects: G structures can be specified.</p>

<p><img src=""http://i.stack.imgur.com/or4Gj.jpg"" alt=""enter image description here""><img src=""http://i.stack.imgur.com/oXTgc.jpg"" alt=""enter image description here""></p>

<p>variance modelling in asreml() it is important to understand the formation of variance structures via direct products. The usual least squares assumption (and the default in asreml()) is that these are independently and identically distributed (IID). However, if the data was from a field experiment laid out in a rectangular array of r rows by c columns, say, we could arrange the residuals e as a matrix and potentially consider that they were autocorrelated within rows and columns.Writing the residuals as a vector in field order, that is, by sorting the residuals rows
within columns (plots within blocks) the variance of the residuals might then be</p>

<p><img src=""http://i.stack.imgur.com/SPE5b.jpg"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/IcikW.jpg"" alt=""enter image description here""> are correlation matrices for the row model (order r, autocorrelation parameter Â½r) and column model (order c, autocorrelation parameter Â½c)
respectively. More specifically, a two-dimensional separable autoregressive spatial structure
(AR1 x Â­ AR1) is sometimes assumed for the common errors in a field trial analysis.</p>

<p><em><strong>The example data:</em></strong></p>

<p>nin89 is from asreml-R library, where different varities were grown in replications / blocks in rectangular field. To control additional variability in row or column direction each plot is referenced as Row and Column variables (row column design). Thus this row column design with blocking. Yield is measured variable. </p>

<p><strong>Example models</strong> </p>

<p>I need something equivalent to the asreml-R codes:</p>

<p>The simple model syntax will look like the follows:</p>

<pre><code> rcb.asr &lt;- asreml(yield âˆ¼ Variety, random = âˆ¼ Replicate, data = nin89)  
 .....model 0
</code></pre>

<p>The linear model is specified in the fixed (required), random (optional) and rcov (error
component) arguments as formula objects.The default is a simple error term and does not need to be formally specified for error term as in the model 0. </p>

<p>here the variety is fixed effect and random is replicates (blocks). Beside random and fixed terms we can specify error term. Which is default in this model 0. The residual or error component of the model is specified in a formula object through the rcov argument, see the following models 1:4. </p>

<p>The following model1 is more complex in which both G (random) and R (error) structure are specified.</p>

<p><strong>Model 1:</strong> </p>

<pre><code>data(nin89)


 # Model 1: RCB analysis with G and R structure
     rcb.asr &lt;- asreml(yield ~ Variety, random = ~ idv(Replicate), 
      rcov = ~ idv(units), data = nin89)
</code></pre>

<p>This model is equivalent to above model 0, and introduces the use of G and R variance model. Here the option random and rcov specifies random and rcov formulae to explicitly specify the G and R structures. where idv() is the special model function in asreml() that identifies the variance model. The expression idv(units) explicitly sets the variance matrix for e to a scaled identity.</p>

<p><em><strong># Model 2: two-dimensional spatial model with correlation in one direction</em></strong></p>

<pre><code>  sp.asr &lt;- asreml(yield ~ Variety, rcov = ~ Column:ar1(Row), data = nin89)
</code></pre>

<p>experimental units of nin89 are indexed by Column and Row. So we expect random variation in two direction - row and column direction in this case. where ar1() is a special function specifying a first order autoregressive variance model for Row. This call specifies a two-dimensional spatial structure for error but with spatial correlation in the row direction only.The variance model for Column is identity (id()) but does not need to be formally
specified as this is the default.</p>

<p><em><strong># model 3: two-dimensional spatial model, error structure in both direction</em></strong></p>

<pre><code> sp.asr &lt;- asreml(yield ~ Variety, rcov = ~ ar1(Column):ar1(Row),  
 data = nin89)
sp.asr &lt;- asreml(yield ~ Variety, random = ~ units, 
 rcov = ~ ar1(Column):ar1(Row), data = nin89)
</code></pre>

<p>similar to above model  2, however the correlation is two direction - autoregressive one. </p>

<p>I am not sure how much of these models are possible with open source R packages. Even if solution of any one of these models will be of great help. <strong><em>Even if the bouty of +50 can stimulate to develop such package will be of great help !</em></strong></p>

<p><em><strong>See MAYSaseen has provided output from each model and data  (as answer)  for comparision.</em></strong> </p>

<p><em><strong>Edits: 
The following is suggestion I received in mixed model discussion forum:</em></strong>
"" You might look at the regress and spatialCovariance packages of David Clifford.  The former allows fitting of (Gaussian) mixed models where you can specify the structure of the covariance matrix very flexibly (for example, I have used it for pedigree data).  The spatialCovariance package uses regress to provide more elaborate models than AR1xAR1, but may be applicable.  You may have to correspond with the author about applying it to your exact problem."" </p>
"
"0.0715255707714427","0.0818012824723818"," 18880","<p>Recently I have opened a question here to understand the output of a GARCH model.
My goal is to understand if the series I'm checking is heteroscedastic or not.</p>

<p>I'm using the <code>garch()</code> function from the <a href=""http://cran.r-project.org/web/packages/tseries/index.html"" rel=""nofollow"">tseries</a> package.</p>

<p>First I built a linear regression like this:</p>

<pre><code>mod &lt;- lm(a ~ b)
</code></pre>

<p>Then I need to check if the residuals of this linear regression present heteroscedasticity.</p>

<p>I did:</p>

<pre><code>g &lt;- garch(resid(mod), order(c(1,1)))
</code></pre>

<p>and then </p>

<pre><code>summary(g)
</code></pre>

<p>I get the follow output:</p>

<pre><code>&gt; summary(g) 

Call: 
garch(x = lm(A ~ B)$resi, order = order(c(1, 1))) 

Model: 
GARCH(1,2) 

Residuals: 
    Min      1Q  Median      3Q    Max 
-4.2058 -1.0262  0.1404  1.1069  3.6553 

Coefficient(s): 
    Estimate  Std. Error  t value Pr(&gt;|t|)    
a0 3.361e-04  9.352e-05    3.594 0.000326 *** 
a1 3.045e-01  4.486e-02    6.787 1.14e-11 *** 
a2 1.209e-06  8.855e-02    0.000 0.999989    
b1 4.938e-01  1.060e-01    4.660 3.17e-06 *** 
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Diagnostic Tests: 
    Jarque Bera Test 

data:  Residuals 
X-squared = 18.84, df = 2, p-value = 8.108e-05 


    Box-Ljung test 

data:  Squared.Residuals 
X-squared = 49.7251, df = 1, p-value = 1.769e-12 
</code></pre>

<p>Thanks to the user who answered my question, I now understand that the <code>ao</code> is the intercept and the other <code>a1</code> and <code>b1</code> are the coefficients I need to check to understand if this time series is heteroscedastic or not.</p>

<p>The problem (doubt) is that now I also see <code>a2</code> in the regression table: What does it stand for?</p>

<p>Is it correct to say that if all coefficients have a $p$-value above 0.05 (...)</p>
"
"0.026822089039291","0"," 19361","<p>Here is a sample output:</p>

<pre><code>anova(fit1,fit2);
Quantile Regression Analysis of Deviance Table

Model: op ~ inp1 + inp2 + inp3 + inp4 + inp5 + inp6 + inp7 + inp8 + inp9
Joint Test of Equality of Slopes: tau in {  0.15 0.3  }

  Df Resid Df F value Pr(&gt;F)
1  9     1337  0.5256 0.8568

Warning messages:
1: In summary.rq(x, se = ""nid"", covariance = TRUE) : 93 non-positive fis
2: In summary.rq(x, se = ""nid"", covariance = TRUE) : 138 non-positive fis
</code></pre>

<p>How to interpret the above results??
Does the <code>anova()</code> function give the best model, for <code>tau=0.15</code> vs. <code>tau=0.3</code>?</p>
"
"0.0808716413062113","0.0822133822214443"," 19772","<p>Can someone please tell me how to have R estimate the break point in a piecewise linear model (as a fixed or random parameter), when I also need to estimate other random effects? </p>

<p>I've included a toy example below that fits a hockey stick / broken stick regression with random slope variances and a random y-intercept variance for a break point of 4. I want to estimate the break point instead of specifying it. It could be a random effect (preferable) or a fixed effect.</p>

<pre><code>library(lme4)
str(sleepstudy)

#Basis functions
bp = 4
b1 &lt;- function(x, bp) ifelse(x &lt; bp, bp - x, 0)
b2 &lt;- function(x, bp) ifelse(x &lt; bp, 0, x - bp)

#Mixed effects model with break point = 4
(mod &lt;- lmer(Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject), data = sleepstudy))

#Plot with break point = 4
xyplot(
        Reaction ~ Days | Subject, sleepstudy, aspect = ""xy"",
        layout = c(6,3), type = c(""g"", ""p"", ""r""),
        xlab = ""Days of sleep deprivation"",
        ylab = ""Average reaction time (ms)"",
        panel = function(x,y) {
        panel.points(x,y)
        panel.lmline(x,y)
        pred &lt;- predict(lm(y ~ b1(x, bp) + b2(x, bp)), newdata = data.frame(x = 0:9))
            panel.lines(0:9, pred, lwd=1, lty=2, col=""red"")
        }
    )
</code></pre>

<p>Output:</p>

<pre><code>Linear mixed model fit by REML 
Formula: Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject) 
   Data: sleepstudy 
  AIC  BIC logLik deviance REMLdev
 1751 1783 -865.6     1744    1731
Random effects:
 Groups   Name         Variance Std.Dev. Corr          
 Subject  (Intercept)  1709.489 41.3460                
          b1(Days, bp)   90.238  9.4994  -0.797        
          b2(Days, bp)   59.348  7.7038   0.118 -0.008 
 Residual               563.030 23.7283                
Number of obs: 180, groups: Subject, 18

Fixed effects:
             Estimate Std. Error t value
(Intercept)   289.725     10.350  27.994
b1(Days, bp)   -8.781      2.721  -3.227
b2(Days, bp)   11.710      2.184   5.362

Correlation of Fixed Effects:
            (Intr) b1(D,b
b1(Days,bp) -0.761       
b2(Days,bp) -0.054  0.181
</code></pre>

<p><img src=""http://i.stack.imgur.com/HnAfg.jpg"" alt=""Broken stick regression fit to each individual""></p>
"
"0.0379321620905441","0.0385614943639849"," 19895","<p>I've run an ordered logistic regression model in R with Zelig and am looking to calculate predicted probabilities. Zelig has a series of simple one line commands to generate the information I want on first differences and so forth. Unfortunately, I keep getting an error when running the zelig function and was wondering if there was a quick alternative for generating predicted probabilities for a ordered logit in R.</p>

<p>For what it's worth, here's the error from my Zelig code.</p>

<pre><code>&gt; x.out &lt;- setx(mod, credit=1)
Error in dta[complete.cases(mf), names(dta) %in% vars, drop = FALSE] : 
  incorrect number of dimensions
</code></pre>

<p>I just need an alternative solution that I can use to generate the probabilities.</p>
"
"0.0657004319817604","0.0667904674542028"," 20001","<p>I am trying to cross validate a logistic regression model with probability sampling weights (weights representing number of subjects in the population).  I am not sure how to handle the weights in each of the 'folds' (cross-validation steps).  I don't think it is as simple as leaving out the observations, I believe the weights need to be rescaled at each step.</p>

<p>SAS has an option in proc surveylogistic to get cross validated (leave one out) prediction probabilities.  Unfortunately I cannot find in the documentation any details on how these were calculated.  I would like to reproduce those probabilities in R.  So far I have not had success and am not sure if my approach is correct.  </p>

<p>I hope someone can recommend an appropriate method to do the cross validation with the sampling weights.  If they could match the SAS results that would be great too.</p>

<p>R code for leave-one-out cross validated probabilities (produces error):</p>

<pre><code>library(bootstrap)
library(survey)
fitLogistic = function(x,y){
  tmp=as.data.frame(cbind(y,x))
  dsn=svydesign(ids=~0,weights=wt,data=tmp)
  svyglm(y~x1+x2, 
         data=tmp,family = quasibinomial,design=dsn)
} 
predict.logistic = function(fitLog,x){
  pred.logistic=predict(fitLog,newdata=x,type='response')
  print(pred.logistic)
  ifelse(pred.logistic&gt;=.5,1,0)
} 
CV_Res= crossval(x=data1[,-1], y=data1[,1], fitLogistic, predict.logistic, ngroup = 13)
</code></pre>

<p>Sample Data Set:</p>

<pre><code>y   x1  x2  wt
0   0   1   2479.223
1   0   1   374.7355
1   0   2   1953.4025
1   1   2   1914.0136
0   0   2   2162.8524
1   0   2   491.0571
0   0   1   1842.1192
0   0   1   400.8098
0   1   1   995.5307
0   0   1   955.6634
1   0   2   2260.7749
0   1   1   1707.6085
0   0   2   1969.9993
</code></pre>

<p>SAS proc surveylogistic leave-one-out cross validated probabilities for sample data set:</p>

<p>.0072, 1 .884, .954, ...</p>

<p>SAS Code:</p>

<pre><code>proc surveylogistic;
model y=x1 x2;
weight wt;
output out=a2 predprobs=x;
run;
</code></pre>
"
"0.026822089039291","0.0272670941574606"," 20157","<p>I try to use the <code>gbm.fit()</code> function for a boosted regression tree model implemented in the R package <a href=""http://cran.r-project.org/web/packages/gbm/index.html"" rel=""nofollow"">gbm</a>. To investigate e.g., the bootstrapped prediction error and all other functionalities I want to use the <code>errorest()</code> from the <a href=""http://cran.r-project.org/web/packages/ipred/index.html"" rel=""nofollow"">ipred</a> package. I think <code>errorest()</code> does not accept the <code>gbm</code> output. Is there a workaround? </p>

<p>Sorry, for the missing example. Please, see below</p>

<pre><code>library(ipred)
library(gbm)
data(BostonHousing)
test &lt;- gbm(medv ~ ., distribution = ""gaussian"",  data=BostonHousing)
</code></pre>

<p>I am not sure how to use the result in <code>errorest()</code>. Can someone give me a helping hand? Thanks!</p>
"
"0.093190562755245","0.102024124270123"," 20417","<p>I am working on a Monte Carlo study of bootstrapping in an AR(1) model for a homework assignment (I'm using Matlab). The goal is to say something about the empirical rejection probabilities of the bootstrap in this specific context. However, I've been running into some computational problems. </p>

<p>Some context first. Suppose one has a regression $y_t = \gamma + \rho y_{t-1} + \varepsilon_t$, and wants to test the two-tailed hypothesis $\rho = \rho_0$ by the bootstrap. To do this, one first estimates the regression by, say, OLS, and then uses the obtained residuals $\hat \varepsilon_t$ and coefficients $\hat \gamma, \; \hat \rho$ for constructing the bootstrap samples. However, in this case, the bootstrap sample has to be constructed recursively. What is even worse for the computations, I have to repeat these calculations many times using different simulated data sets <em>and</em> different values of $\rho$. (The reason is that this allows one to get some sense of the empirical level of the test at different values of $\rho$.) </p>

<p>So far I have been doing it in the naive way of just estimating many regressions, generating the bootstrap samples, and so on. I was able to eliminate quite a few loops by using the <code>filter</code> function when generating bootstrap samples but still computational times are quite huge. I compared it with the built-in bootstrap in <a href=""http://gretl.sourceforge.net/"" rel=""nofollow"">gretl</a>, and it is many times slower. (I'm aware that gretl is written in C, but I sense that there is more to this than that.)</p>

<p>Hence, my question would be: what general advice could you give to implement bootstrapping efficiently in a matrix based language? Are there some smart ways to avoid estimation of many OLS regressions and generation of the bootstrap samples? Although I'm using Matlab for the computations, but if you have some general advice on say, doing this with R, all the help would be very much appreciated. Also, since this is a homework assignment I have to do, using existing libraries is not really an option. :)</p>
"
"0.0464572209811883","0.0314853283036575"," 20423","<p>I'm using <code>smooth.spline</code> with some success, but I need to control the degree of the regressions between knots (cubic is too high for my needs).  I looked at the <a href=""http://sites.stat.psu.edu/~dhunter/R/html/splines/html/bs.html"" rel=""nofollow""><code>bs</code></a> function, which allows controllable degree but requires manual specification of knot locations. Is there a function in R which will find knots for me automatically, but let me control the degree of the inter-knot regressions (not sure if this is the right term...)?</p>
"
"0.026822089039291","0.0272670941574606"," 20438","<p>I fitted a GEE model using the function <code>genZcor</code> with user defined
correlation matrix. I want to get the var-cov matrix of the regression
coefficients. But the output provides only limited information.</p>

<p>I would be very much thankful if you could kindly let me know how to get
it since I am struggling lot getting this.</p>
"
"0.125806749141278","0.127894008160743"," 20452","<p>My primary question is how to interpret the output (coefficients, F, P) when conducting a Type I (sequential) ANOVA? </p>

<p>My specific research problem is a bit more complex, so I will break my example into parts. First, if I am interested in the effect of spider density (X1) on say plant growth (Y1) and I planted seedlings in enclosures and manipulated spider density, then I can analyze the data with a simple ANOVA or linear regression. Then it wouldn't matter if I used Type I, II, or III Sum of Squares (SS) for my ANOVA. In my case, I have 4 replicates of 5 density levels, so I can use density as a factor or as a continuous variable. In this case, I prefer to interpret it as a continuous independent (predictor) variable. In R I might run the following:</p>

<pre><code>lm1 &lt;- lm(y1 ~ density, data = Ena)
summary(lm1)
anova(lm1)
</code></pre>

<p>Running the anova function will make sense for comparison later hopefully, so please ignore the oddness of it here. The output is:</p>

<pre><code>Response: y1
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density    1 0.48357 0.48357  3.4279 0.08058 .
Residuals 18 2.53920 0.14107 
</code></pre>

<p>Now, let's say I suspect that the starting level of inorganic nitrogen in the soil, which I couldn't control, may have also significantly affected the plant growth. I'm not particularly interested in this effect but would like to potentially account for the variation it causes. Really, my primary interest is in the effects of spider density (hypothesis: increased spider density causes increased plant growth - presumably through reduction of herbivorous insects but I'm only testing the effect not the mechanism). I could add the effect of inorganic N to my analysis. </p>

<p>For the sake of my question, let's pretend that I test the interaction density*inorganicN and it's non-significant so I remove it from the analysis and run the following main effects:</p>

<pre><code>&gt; lm2 &lt;- lm(y1 ~ density + inorganicN, data = Ena)
&gt; anova(lm2)
Analysis of Variance Table

Response: y1
           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density     1 0.48357 0.48357  3.4113 0.08223 .
inorganicN  1 0.12936 0.12936  0.9126 0.35282  
Residuals  17 2.40983 0.14175 
</code></pre>

<p>Now, it makes a difference whether I use Type I or Type II SS (I know some people object to the terms Type I &amp; II etc. but given the popularity of SAS it's easy short-hand). R anova{stats} uses Type I by default. I can calculate the type II SS, F, and P for density by reversing the order of my main effects or I can use Dr. John Fox's ""car"" package (companion to applied regression). I prefer the latter method since it is easier for more complex problems.</p>

<pre><code>library(car)
Anova(lm2)
            Sum Sq Df F value  Pr(&gt;F)  
density    0.58425  1  4.1216 0.05829 .
inorganicN 0.12936  1  0.9126 0.35282  
Residuals  2.40983 17  
</code></pre>

<p>My understanding is that type II hypotheses would be, ""There is no linear effect of x1 on y1 given the effect of (holding constant?) x2"" and the same for x2 given x1. I guess this is where I get confused. <strong>What is the hypothesis being tested by the ANOVA using the type I (sequential) method above compared to the hypothesis using the type II method?</strong></p>

<p>In reality, my data is a bit more complex because I measured numerous metrics of plant growth as well as nutrient dynamics and litter decomposition. My actual analysis is something like:</p>

<pre><code>Y &lt;- cbind(y1 + y2 + y3 + y4 + y5)
# Type II
mlm1 &lt;- lm(Y ~ density + nitrate + Npred, data = Ena)
Manova(mlm1)

Type II MANOVA Tests: Pillai test statistic
        Df test stat approx F num Df den Df  Pr(&gt;F)    
density  1   0.34397        1      5     12 0.34269    
nitrate  1   0.99994    40337      5     12 &lt; 2e-16 ***
Npred    1   0.65582        5      5     12 0.01445 * 


# Type I
maov1 &lt;- manova(Y ~ density + nitrate + Npred, data = Ena)
summary(maov1)

          Df  Pillai approx F num Df den Df  Pr(&gt;F)    
density    1 0.99950     4762      5     12 &lt; 2e-16 ***
nitrate    1 0.99995    46248      5     12 &lt; 2e-16 ***
Npred      1 0.65582        5      5     12 0.01445 *  
Residuals 16                                           
</code></pre>
"
"0.0402331335589365","0.0545341883149212"," 20542","<p>I am using the lm() and princomp() functions in R to perform regressions on foreign exchange time series. I would like to weight the regressions (and PCA) such that 50% of the influence on the regression comes from the past 3 months, 25% from the previous 3 months, etc, but in a smooth fashion. Both functions take such a weighting series. What is a simple formula for generating a decaying weights series with a half life of 3 months (or any other period, for that matter), in R?</p>

<p>Ideally the sum of the weights will equal 1, although I don't think this is mandatory in lm().</p>
"
"0.0889588054368324","0.0904347204435887"," 20613","<p>Hello statistical gurus and R programming wizards,</p>

<p>I am interested in modeling animal captures as a function of environmental conditions and day of the year. As part of another study, I have counts of captures on ~160 days over three years. On each of these days I have temperature, rainfall, windspeed, relative humidity, etc. Because the data was collected repeatedly from the same 5 plots, I use plot as a random effect.</p>

<p>My understanding is that nlme can easily account for temporal autocorrelation in the residuals but doesn't handle non-Gaussian link functions like lme4 (which can't handle the autocorrelation?). Currently, I think it might work to use the nlme package in R on log(count). So my solution right now would be to run something like:</p>

<pre><code>m1 &lt;- lme(lcount ~ AirT + I(AirT^2) + RainAmt24 + I(RainAmt24^2) + RHpct + windspeed + 
      sin(2*pi/360*DOY) + cos(2*pi/360*DOY), random = ~1|plot, correlation =
      corARMA(p = 1, q = 1, form = ~DOY|plot), data = Data)
</code></pre>

<p>where DOY = Day of the Year. There may be more interactions in the final model, but this is my general idea. I could also potentially try to model the variance structure further with something like </p>

<pre><code>weights = v1Pow
</code></pre>

<p>I'm not sure if there is a better way to do with with a Poisson mixed model regression or anything? I just found mathematical discussion in Chapter 4 of ""Regression Models for Time Series Analysis"" by Kedem and Fokianos. It was a bit beyond me at the moment, especially in application (coding it in R). I also saw a MCMC solution in Zuur et al. Mixed Effects Models book (Chp 23) in the BUGS language (using winBUGS or JAG). Is that my best option? Is there an easy MCMC package in R that would handle this? I'm not really familiar with GAMM or GEE techniques but would be willing to explore these possibilities if people thought they'd provide better insight. <strong>My main objective is to create a model to predict animal captures given environmental conditions. Secondarily, I would like to explain what the animals a responding to in terms of their activity.</strong></p>

<p>Any thoughts on the best way to proceed (philosophically), how to code this in R, or in BUGS would be appreciated. I'm fairly new to R and BUGS (winBUGS) but am learning. This is also the first time I've ever tried to address temporal autocorrelation.</p>

<p>Thanks,
Dan</p>
"
"NaN","NaN"," 20854","<p>I have read from <a href=""http://www.r-tutor.com/elementary-statistics/logistic-regression/estimated-logistic-regression-equation"" rel=""nofollow"">here</a> and understand how to calculate the estimated logit from a fitted logistic regression model, but how to work on the confidence interval? As it involved a variance-covariance matrix and I think it is better to have a program to do the calculation, rather then doing it by myself.</p>

<p>Thanks.</p>

<h3>Edit 01</h3>

<p>I have added a script here:</p>

<pre><code>chdage.dummy &lt;- data.frame(chd=c(rep(1,50),rep(0,50)),
                           race=c(rep(""white"",5),rep(""black"",20),rep(""hispanic"",15),rep(""other"",10),
                                  rep(""white"",20),rep(""black"",10),rep(""hispanic"",10),rep(""other"",10)),
                           stringsAsFactors=FALSE)
chdage.dummy[,""race""] &lt;- factor(chdage.dummy[,""race""],levels=c(""white"",""black"",""hispanic"",""other""))
chdage.lr.02 &lt;- glm(chd~race,data=chdage.dummy,family=""binomial"")
predict(chdage.lr.02,newdata=data.frame(race=""white""))
</code></pre>

<p><code>predict</code> function can give me an estimate, but I can't use <code>confint</code> outside <code>predict</code>, so what can I do?</p>
"
"0.053644178078582","0.0545341883149212"," 20939","<p>I have a regression with a harmonic effect of day of the year, which interacts with other variables. I am not sure how to interpret the coefficients. My model is:</p>

<pre><code>m1 &lt;- lme(lcount ~ AirT + sin(2*pi/360*DOY) + cos(2*pi/360*DOY) + 
          AirT*sin(2*pi/360*DOY) + AirT*cos(2*pi/360*DOY) + RainAmt + RainAmt*AirT,
          random = ~1|plot))
</code></pre>

<p>I get significant interaction effects of air temperature with the linearized harmonic day of the year (DOY) function. My response variable is the log of animal counts on each day. I want to describe how the effect of air temperature on animal observations changes depending on the day of the year.</p>

<p>Does anyone have suggestions on how I can interpret my beta values and/or how I can visualize the effect? I am using R but am not that skilled. The package I used for analyzing my data is nlme.</p>

<p>EDIT: <strong>My primary goals are (1) to describe the response of animals to environmental variables and (2) to predict future activity periods (i.e. when and under what conditions should a research bother trying to catch these animals).</strong> So if there is a better way to model this data, I would be interested in hearing it (such as cubic splines - see comments below).</p>
"
"NaN","NaN"," 21506","<p>There are at least three R packages providing some functions to perform a Bayesian selection variable in linear Gaussian regression model: LearnBayes, mombf and BMA.</p>

<p>I would be glad to know some opinions about which one is the best.</p>
"
"NaN","NaN"," 22346","<p>How can one obtain standardized (fixed effect) regression weights from a multilevel regression?</p>

<p>And, as an ""add-on"": What is the easiest way to obtain these standardized weights from a <code>mer</code>-object (from the <code>lmer</code> function of the <code>lme4</code>package in <code>R</code>)?</p>
"
"0.0709645772411954","0.072141950116023"," 22768","<p>I am reflecting on the discussion around <a href=""http://stats.stackexchange.com/questions/17624/when-to-drop-a-term-from-a-regression-model"">this question</a> and particularly Frank Harrell's comment that the estimate for variance in a reduced model (ie one from which a number of explanatory variables have been tested and rejected) should use Ye's <a href=""http://www.jstor.org/pss/2669609"">Generalized Degrees of Freedom</a>.  Professor Harrell points out this will be much closer to the residual degrees of freedom of the original ""full"" model (with all the variables in) than that from a final model (from which a number of variables have been rejected).</p>

<p>Question 1. If I want to use an appropriate approach to all the standard summaries and statistics from a reduced model (but short of a full implementation of Generalized Degrees of Freedom), would a reasonable approach be to just use the residual degrees of freedom from the full model in my estimates of residual variance, etc?</p>

<p>Question 2. If the above is true and I want to do it in <code>R</code>, might it be as simple as setting </p>

<pre><code>finalModel$df.residual &lt;- fullModel$df.residual
</code></pre>

<p>at some point in the model fitting exercise, where finalModel and fullModel were created with lm() or a similar function.  After which functions such as summary() and confint() seem to work with the desired df.residual, albeit returning an error message that someone has clearly mucked around with the finalModel object.  </p>

<p>Thanks.</p>
"
"0.0969560705490221","0.0985646681332268"," 23042","<p>Can someone explain my Cox model to me in plain English? </p>

<p>I fitted the following Cox regression model to <strong>all</strong> of my data using the <code>cph</code> function. My data are saved in an object called <code>Data</code>. The variables <code>w</code>, <code>x</code>, and <code>y</code> are continuous; <code>z</code> is a factor of two levels. Time is measured in months. Some of my patients are missing data for variable <code>z</code> (<em>NB</em>: I have duly noted Dr. Harrell's suggestion, below, that I impute these values so as to avoid biasing my model, and will do so in the future).</p>

<pre><code>&gt; fit &lt;- cph(formula = Surv(time, event) ~ w + x + y + z, data = Data, x = T, y = T, surv = T, time.inc = 12)

Cox Proportional Hazards Model
Frequencies of Missing Values Due to Each Variable
Surv(time, event)    w    x    y    z 
                0    0    0    0   14 

                Model Tests          Discrimination 
                                            Indexes        
Obs       152   LR chi2      8.33    R2       0.054    
Events     64   d.f.            4    g        0.437    
Center 0.7261   Pr(&gt; chi2) 0.0803    gr       1.548    
                Score chi2   8.07                      
                Pr(&gt; chi2) 0.0891                      

                   Coef    S.E.   Wald Z   Pr(&gt;|Z|)
         w      -0.0133  0.0503    -0.26     0.7914  
         x      -0.0388  0.0351    -1.11     0.2679  
         y      -0.0363  0.0491    -0.74     0.4600  
         z=1     0.3208  0.2540     1.26     0.2067
</code></pre>

<p>I also tried to test the assumption of proportional hazards by using the <code>cox.zph</code> command, below, but do not know how to interpret its results. Putting <code>plot()</code> around the command gives an error message.</p>

<pre><code> cox.zph(fit, transform=""km"", global=TRUE)
            rho chisq      p
 w      -0.1125 1.312 0.2520
 x       0.0402 0.179 0.6725
 y       0.2349 4.527 0.0334
 z=1     0.0906 0.512 0.4742
 GLOBAL      NA 5.558 0.2347
</code></pre>

<hr>

<h3>First Problem</h3>

<ul>
<li>Can someone explain the results of the above output to me in plain English? I have a medical background and no formal training in statistics.</li>
</ul>

<h3>Second Problem</h3>

<ul>
<li><p>As suggested by Dr. Harrell, I would like to internally validate my model by performing 100 iterations of 10-fold cross-validation using the <code>rms</code> package (from what I understand, this would entail building <code>100 * 10 = 1000</code> different models and then asking them to predict the survival times of patients that they had never seen).</p>

<p>I tried using the <code>validate</code> function, as shown.</p>

<pre><code>&gt; v1 &lt;- validate(fit, method=""crossvalidation"", B = 10, dxy=T)
&gt; v1
      index.orig training    test optimism index.corrected  n
Dxy      -0.2542  -0.2578 -0.1356  -0.1223         -0.1320 10
R2        0.0543   0.0565  0.1372  -0.0806          0.1350 10
Slope     1.0000   1.0000  0.9107   0.0893          0.9107 10
D         0.0122   0.0128  0.0404  -0.0276          0.0397 10
U        -0.0033  -0.0038  0.0873  -0.0911          0.0878 10
Q         0.0155   0.0166 -0.0470   0.0636         -0.0481 10
g         0.4369   0.4424  0.6754  -0.2331          0.6700 10
</code></pre>

<p>How do you perform the 100x resampling? I think my above code only performs the cross-validation once.</p></li>
<li><p>I then wanted to know how good my model was at prediction. I tried the following:</p>

<pre><code>&gt; c_index &lt;- abs(v1[1,5])/2 + 0.5
&gt; c_index
[1] 0.565984
</code></pre>

<p>Does this mean that my model is only very slightly better than flipping a coin?</p></li>
</ul>

<h3>Third Problem</h3>

<p>Dr. Harrell points out that I have assumed linearity for the covariate effects, and that the number of events in my sample is just barely large enough to fit a reliable model if all covariate effects happen to be linear.</p>

<ul>
<li>Does this mean that I should include some sort of interaction term in my model? If so, any advice as to what to put?</li>
</ul>
"
"0.0709645772411954","0.072141950116023"," 23110","<p>I have a data set that's 200k rows X 50 columns.  I'm trying to use a <code>knn</code> model on it but there is huge variance in performance depending on which variables are used (i.e., <code>rsqd</code> ranges from .01 (using all variables) to .98 (using only 5 variables)).</p>

<p>This kind of compounds my problem as now I need to determine <code>k</code> <em>and</em> which variables to use.</p>

<p>Is there a package in R that helps with selecting variables for a <code>knn</code> model, while tuning <code>k</code>?  I've looked at <code>rfe()</code> in <code>caret</code> but it seems to only be built for linear regression, <code>randomforest</code>, naive bayes, etc but no <code>knn</code>.  </p>

<p>As an aside, I've tried manually building a loop to use the caret train function like this:</p>

<pre><code>for(i in 2:50){
knnFit &lt;- train(x[,i],y,...) ## trains model using single variable
}
</code></pre>

<p>My problem is that <code>knnFit$results</code> prints all of the results and <code>knnFit$bestTune</code> only prints the final parameter of <code>k</code>.  </p>

<pre><code>&gt; data1 &lt;- data.frame(col1=runif(20), col2=runif(20), col3=runif(20), col4=runif(20), col5=runif(20))
&gt; bootControl &lt;- trainControl(number = 1)
&gt; knnGrid &lt;- expand.grid(.k=c(2:5))
&gt; set.seed(2)
&gt; knnFit1 &lt;- train(data1[,-c(1)], data1[,1]
+ , method = ""knn"", trControl = bootControl, verbose = FALSE,
+ tuneGrid = knnGrid )
&gt; knnFit1 
20 samples
 4 predictors

No pre-processing
Resampling: Bootstrap (1 reps) 

Summary of sample sizes: 20 

Resampling results across tuning parameters:

  k  RMSE   Rsquared
  2  0.485  0.124   
  3  0.54   0.369   
  4  0.52   0.241   
  5  0.528  0.232   

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was k = 2. 

&gt; knnFit1$results
      k      RMSE  Rsquared RMSESD RsquaredSD
    1 2 0.4845428 0.1241031     NA         NA
    2 3 0.5401009 0.3690569     NA         NA
    3 4 0.5197262 0.2410814     NA         NA
    4 5 0.5277939 0.2317607     NA         NA

&gt; knnFit1$bestTune
      .k
    1  2
</code></pre>

<p>I need some way to print the RMSE/rsqd/other metric for the best single performing model (i.e., just ""R-Squared: .91"").</p>

<p>Any suggestions?</p>
"
"0.053644178078582","0.0545341883149212"," 23248","<p>I have a problem of the form ""what is the probability that a user will 'like' a certain movie?"" For a bunch of users, I know the movies each has watched historically, and the movies each has liked. Additionally, for each movie I know the name of the director.</p>

<p>I calibrated a logistic regression for each user of the form:</p>

<p><code>glm(liked_by_user_1 ~ liked_by_user_2 + ... + liked_by_user_k + factor(director), family=binomial, data = subset(MovieWatchings, user_id == 1))</code></p>

<p>But my problem is: say that in the past, user 1 has watched movies from directors <code>D1</code> through <code>DM</code>, but next month <code>U1</code> watches a movie directed by <code>DN</code>? In that case the R <code>predict()</code> function will give an error, because the glm model for user 1 doesn't have an estimated parameter for the case of <code>director = DN</code>. But I must know something about <code>U1's</code> probability of liking the new movie, because I still know which other users have seen and liked this movie, and that has some predictive power.</p>

<p>How can I set up my model so that I can take into account other users' liking behavior, AND user 1's director preferences, but still have sensible predictions when user 1 sees his first movie from a new director? Is logistic regression even the right type of model for this case?</p>
"
"0.026822089039291","0.0272670941574606"," 23346","<p>I am looking for a good and modern Kernel Regression package in R, which has the following features:</p>

<ol>
<li>It has cross-validation </li>
<li>It can automatically choose the ""optimal"" bandwidth</li>
<li>It doesn't have random effect - i.e. if I run the function at different times on the same data-set, the results should be exactly the same...</li>
</ol>

<p>I am trying ""np"", but I am seeing:</p>

<pre><code>Multistart 1 of 1 |
Multistart 1 of 1 |
...
</code></pre>

<p>It looks like in order to do the optimization, it's doing multiple-random-start optimization ... Am I right?</p>

<p>Could you please give me some pointers?</p>

<p>I did some google search but there are so many packages that do this... I just wanted to find the best/modern one to use...</p>
"
"0.103881504159667","0.105605001571314"," 23795","<p>I am using a relevance vector machine as implemented in the kernlab-package in R, trained on a dataset with 360 continuous variables (features) and 60 examples (also continuous, so it's a relevance vector regression).</p>

<p>I have several datasets with equivalent dimensions from different subjects. Now it works fine for most of the subjects, but with one particular dataset, I get this strange results:</p>

<p>When using leave-one-out cross validation (so I train the RVM and try to subsequently predict one observation that was left out of the training), most of the predicted values are just around the mean of the example-values.
So I really don't get good predictions, but just a slightly different value than the mean.</p>

<p>It seems like the SVM is not working at all;
When I plot the fitted values against the actual values, I see the same pattern; predictions around the mean. So the RVM is not even able to predict the values it was trained on (for the other datasets I get correlations of around .9 between fitted and actual values).</p>

<p>It seems like, that I can at least improve the fitting (so that the RVM is at least able to predict the values it was trained on) by transforming the dependent variable (the example-values), for example by taking the square root of the dependent variable.</p>

<p>so this is the output for the untransformed dependent variable:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 5 
Variance :  1407.006
Training error : 1383.534902093 
</code></pre>

<p>this, if I first transform the dependent variable by taking the square root:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 55 
Variance :  1.711355
Training error : 0.89601609 
</code></pre>

<p>How is it, that the RVM-results change so dramatically, just by transforming the dependent variable? And what is going wrong, when an SVM just predicts values around the mean of the dependent variable (even for the values and observations it was trained on)?</p>
"
"0.0774287016353139","0.0787133207591437"," 24072","<p>I am running the following unit root test (Dickey-Fuller) on a time series using the <code>ur.df()</code> function in the <code>urca</code> package.</p>

<p>The command is:</p>

<pre><code>summary(ur.df(d.Aus, type = ""drift"", 6))
</code></pre>

<p>The output is:</p>

<pre><code>############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression drift 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.266372 -0.036882 -0.002716  0.036644  0.230738 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  0.001114   0.003238   0.344  0.73089   
z.lag.1     -0.010656   0.006080  -1.753  0.08031 . 
z.diff.lag1  0.071471   0.044908   1.592  0.11214   
z.diff.lag2  0.086806   0.044714   1.941  0.05279 . 
z.diff.lag3  0.029537   0.044781   0.660  0.50983   
z.diff.lag4  0.056348   0.044792   1.258  0.20899   
z.diff.lag5  0.119487   0.044949   2.658  0.00811 **
z.diff.lag6 -0.082519   0.045237  -1.824  0.06874 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.06636 on 491 degrees of freedom
Multiple R-squared: 0.04211,    Adjusted R-squared: 0.02845 
F-statistic: 3.083 on 7 and 491 DF,  p-value: 0.003445 


Value of test-statistic is: -1.7525 1.6091 

Critical values for test statistics: 
      1pct  5pct 10pct
tau2 -3.43 -2.86 -2.57
phi1  6.43  4.59  3.78
</code></pre>

<ol>
<li><p>What do the significance codes (Signif. codes) mean? I noticed that some of them where written against: z.lag.1, z.diff.lag.2, z.diff.lag.3 (the ""."" significance code) and z.diff.lag.5 (the ""**"" significance code).</p></li>
<li><p>The output gives me two (2) values of test statistic: -1.7525 and 1.6091. I know that the ADF test statistic is the first one (i.e. -1.7525). What is the second one then?</p></li>
<li><p>Finally, in order to test the hypothesis for unit root at the 95% significance level, I need to compare my ADF test statistic (i.e. -1.7525) to a critical value, which I normally get from a table. The output here seems to give me the critical values through. However, the question is: which critical value between ""tau2"" and ""phi1"" should I use.</p></li>
</ol>

<p>Thank you for your response.</p>
"
"0.026822089039291","0.0272670941574606"," 24445","<p>I'm trying to estimate a multiple linear regression in R with an equation like this:</p>

<pre><code>regr &lt;- lm(rate ~ constant + askings + questions + 0)
</code></pre>

<p>askings and questions are quarterly data time-series, constructed with <code>askings &lt;- ts(...)</code>.</p>

<p>The problem now is that I got autocorrelated residuals. I know that it is possible to fit the regression using the gls function, but I don't know how to identify the correct AR or ARMA error structure which I have to implement in the gls function. </p>

<p>I would try to estimate again now with,</p>

<pre><code>gls(rate ~ constant + askings + questions + 0, correlation=corARMA(p=?,q=?))
</code></pre>

<p>but I'm unfortunately neither an R expert nor an statistical expert in general to identify p and q.</p>

<p>I would be pleased If someone could give me a useful hint.
Thank you very much in advance!</p>

<p>Jo</p>
"
"0.0379321620905441","0.0385614943639849"," 24680","<p>In ""R - project"" I am trying to estimate the panel data <code>lm</code> model with <code>plm</code> function. When I include 3 dummy variables into the regression it doesn't appear in the summary of the model, but when I estimate a simple <code>lm</code> model it appears.</p>

<p>Why is it so? What should I do to estimate the statistics for those dummy variables?</p>
"
"0.0851715717988452","0.0944559849109725"," 24731","<p>this is, I think, an easy question... I did a regression analysis in R, where I wanted to check the fit of my data to a specific formula I provided... I got this to work, I see the graph and the line showing the fit of the data, but I also would like to get actual regression value obtained from this calculation, how do I do this? and How do I  get the actual equation printed in the graph?</p>

<hr>

<p>Hi thanks a lot for the replies.</p>

<p>The example was very useful, but I still didnt obtain the Regression value, so I still dont know how well my data fits the equation... below Im pasting the code I used</p>

<pre><code># import the input data file ""LD251-chilR.txt"", for the input; 
# the first column is pairwise distance and the second is LD estimate, 
# need to put the file in the same folder with R program
CT251chil&lt;-read.table(""LD251-chilR.txt"", sep=""\t"", dec="","", header=TRUE) 

# your LD estimate (r2 in this case which located in column 2)
r2=CT251chil[,2] 

# run nls function for getting rho estimate
nls(r2~1/(1+rho*CT251chil[,1]), start=list(rho=0.3)) 

# getting this rho estimate after running nls function in R
rho=0.02206872 

#sort the data, no need if your data already sorted
dist&lt;-sort(CT251chil[,1]) 

# put parameters in the equation as shown in my MBE paper
eq &lt;- (((10+rho*dist)/((2+rho*dist)*(11+rho*dist)))*(1+((3+rho*dist)*(12+12*rho*dist+(rho*dist)^2)/(46*(2+rho*dist)*(11+rho*dist))))) 

# plotting the graph between LD estimate and distance
plot(CT251chil[,1],CT251chil[,2], col=""black"", pch=20, 
     ylab=expression(R^2), xlab=""Pairwise distance"", 
     main=""CT251-chilense"", las=1) 

# getting regression line
lines(dist,eq, col=""black"",lwd=2,lty=1) 
</code></pre>

<p>After this point I get a graph with a line, all good. But so far I dont know how to output the actual equation on the graph get the value of R.</p>

<p>thank you for any further help!</p>
"
"0.0464572209811883","0.0472279924554862"," 24840","<p>I need to determine if there is any relationship between two count variables. 
I have 60+ observations for 4 variables and I want to see if any of the pairs of these variables are significantly correlated with one another. </p>

<p>Mostly I use R, so forgive me if you're not familiar.</p>

<p>I have been using the <code>cor(...,method=""pearson"")</code> and <code>cor.test()</code> functions to test each pair, but now I'm not so sure that this is the right approach/test. 
Would a non-linear regression like <code>glm(...,family=""poisson"")</code> be more appropriate?</p>

<p>I started thinking like this because when I looked at a histogram of the counts across my observations, I noticed that there seemed to be a slight tendency for the pink and green variables to go up and down to together.</p>

<p>I produced a scatter plot of each of the variables plotted against each of the other variables. I used the tests mention above to try and quantify this relationship and to test weather it was real or just noise. </p>

<p><img src=""http://i.stack.imgur.com/rNFTE.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/iZ3hW.png"" alt=""enter image description here""></p>
"
"0.0929144419623766","0.0944559849109725"," 24857","<p>Much like with regression, handling binary dependent variables in SEM requires special considerations. In particular, some of these are noted on Dave Garson's <a href=""http://faculty.chass.ncsu.edu/garson/PA765/structur.htm"" rel=""nofollow"">Structural Equation Modeling</a> and include:</p>

<blockquote>
  <ol>
  <li><p>Polychoric correlation. LISREL/PRELIS uses polyserial, tetrachoric, and polychoric correlations to create the input correlation matrix,
  combined with ADF estimation (see below), for variables which cannot
  be assumed to have a bivariate normal distribution.</p>
  
  <ul>
  <li>Sample size issue. ADF [Asymptotically distribution-free] estimation in turn requires a very large sample size. Yuan and Bentler (1994)
  found satisfactory estimates only with a sample size of at least 2,000
  and preferably 5,000. Violating this requirement may introduce
  problems greater than treating ordinal data as interval and using ML
  estimation. This is also a reason cited for preferring the Bayesian
  estimation approach to ordinal data taken by Amos since Bayesian
  estimation can handle smaller samples than ML or ADF.</li>
  </ul></li>
  </ol>
</blockquote>

<p>I'm currently trying to use the package <a href=""http://cran.r-project.org/web/packages/sem/index.html"" rel=""nofollow"">sem</a> in R to test my model, and the author of the model suggests using polychoric correlations on <a href=""http://r.789695.n4.nabble.com/Link-functions-in-SEM-td859182.html"" rel=""nofollow"">R-help</a>. The problems are: </p>

<ol>
<li>I don't know what estimation method is being used with these correlations (i.e., ADF or ML). </li>
<li>My sample size is small (N = 173). </li>
<li>I'm not familiar with how to interpret polychoric associations (in the case that it is appropriate for me to use them). All the other variables in my model are continuous in nature. </li>
</ol>

<p>Any help and/or links would be greatly appreciated. I'm also considering using other software like OpenMX, but I'm still reading about how it handles binary data. Help with what other software I might want to use would also be appreciated.</p>
"
"0.0758643241810882","0.0771229887279699"," 24948","<p>I have a reasonable understanding of why multicollinearity is a problem is regression models, along the <a href=""http://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r"">lines</a> of this excellent post.</p>

<p>To summarise my understanding, for a regression model of $y = \alpha + \beta_1x + \beta_2z$ (where $x$ and $z$ are correlated), beta coefficient estimates (as well as being unstable) are difficult to interpret, as a situation where you might increase $z$ without increasing $x$ is unlikely to occur, and not supported by the data.</p>

<p>I understand multicollinearity is less harmful to purely <em>predictive</em> as opposed to explanatory or descriptive models.</p>

<p>I'm interested in another interpretation:</p>

<p><em>If I decided to increase $z$, and let $x$ vary as it pleases in reaction, what would I see happen to $y$, accounting for the fact that $x$ is likely to move with $z$, and also have it's own effect?</em></p>

<p>In other words, accepting the causal interpretation that $x$ and $z$ both cause $y$, and are themselves correlated to some extent (.7 say), how would all three variables move if $z$ is (linearly) increased by some amount?</p>

<p>I've tried to model this sort of thing before, fitting $y = \alpha + \beta_1x + \beta_2z$ (model 1), and $x = \alpha + \beta_1z$ (model 2). Hypothetical increased $z$ values are produced, and resulting $x$ values are predicted with model 2. The hypothetical $x$ and $z$ values are used to predict $y$ using model 1. However this feels very unsatisfactory, complicated simulations are required to capture uncertainty (I used <code>sim</code> in <code>arm</code>). Additionally, my gut tells me that apart from being painfully inelegant, it's a bad idea for other reasons I can't put my finger on.</p>

<ul>
<li>Is such an 'observational'/conditional-when-I-feel-like-it interpretation possible?</li>
<li>Does anyone know of a better method for this interpretation?</li>
<li>Can anyone recommend a paper or <code>R</code> package along these lines?</li>
<li>Is the above multi-model mess at-all valid?</li>
</ul>

<p>I'm aware that a model along the lines of $y = \alpha + \beta_1z$ would yield a similar answer to the two-stage mess above, but would lose information in $x$.</p>

<p>I understand that these ideas are similar to structural equation modelling, but apart from having scant knowledge of SEM, I'm yet to find an <code>R</code> package which allows flexibly extending these models with different link functions for proportional odds models, etc.</p>
"
"0.0715255707714427","0.0818012824723818"," 25538","<p>I am looking into time series data compression at the moment.</p>

<p>The idea is to fit a curve on a time series of n points so that the maximum deviation of any of the points is not greater than a given threshold. In other words, none of the values that the curve takes at the points where the time series is defined should be ""further away"" than a certain threshold from the actual values.</p>

<p>Till now I have found out how to do nonlinear regression using the least squares estimation method in R (<code>nls</code> function) and other languages, but I haven't found any packages that implement nonlinear regression with the L-infinity norm.</p>

<p>I have found papers on <a href=""http://www.jstor.org/discover/10.2307/2006101?uid=3737864&amp;uid=2&amp;uid=4&amp;sid=21100693651721"">""Non-linear curve fitting in the $L_1$ and $L_{\infty}$ norms""</a>, by Shrager and Hill and <a href=""http://www.dtic.mil/dtic/tr/fulltext/u2/a080454.pdf"">""A linear  programming algorithm  for curve fitting in the $L_{\infty}$ norm""</a>, by Armstrong and Sklar.</p>

<p>I could try to implement this in R for instance, but I first looking to see if this hasn't already been done and that I could maybe reuse it.</p>

<p>I have found a solution that I don't believe to be ""very scientific"": I use nonlinear least squares regression to find the starting values of the parameters which I subsequently use as starting points in the R <code>optim</code> function that minimizes the maximum deviation of the curve from the actual points.</p>

<p>The idea is to be able to find out if this type of curve-fitting is possible on a given time series sequence and to determine the parameters that allow it.</p>
"
"0.0599760143904067","0.0609710760849692"," 25611","<p>I have a dataset with 9 continuous independent variables. I'm trying to select amongst these variables to fit a model to a single percentage (dependent) variable, <code>Score</code>. Unfortunately, I know there will be serious collinearity between several of the variables.</p>

<p>I've tried using the <code>stepAIC()</code> function in R for variable selection, but that method, oddly, seems sensitive to the order in which the variables are listed in the equation...</p>

<p>Here's my R code (because it's percentage data, I use a logit transformation for Score):</p>

<pre><code>library(MASS)
library(car)

data.tst = read.table(""data.txt"",header=T)
data.lm = lm(logit(Score) ~ Var1 + Var2 + Var3 + Var4 + Var5 + Var6 + Var7 +
             Var8 + Var9, data = data.tst)

step = stepAIC(data.lm, direction=""both"")
summary(step)
</code></pre>

<p>For some reason, I found that the variables listed at the beginning of the equation end up being selected by the <code>stepAIC()</code> function, and the outcome can be manipulated by listing, e.g., <code>Var9</code> first (following the tilde).</p>

<p>What is a more effective (and less controversial) way of fitting a model here? I'm not actually dead-set on using linear regression: the only thing I want is to be able to understand which of the 9 variables is truly driving the variation in the <code>Score</code> variable. Preferably, this would be some method that takes the strong potential for collinearity in these 9 variables into account.</p>
"
"0.0709645772411954","0.072141950116023"," 25702","<p>I have spent much time looking for a special package that could run the Pesaran(2007) unit root test (which assumes cross-sectional dependence unlike most others) and I have found none. So, I decided to do it manually; however, I don't know where I'm going wrong, because my results are very different from Microsoft Excel's results (in which it is done very easily).</p>

<p>My data frame is made up of 22 countries with 506 observations of daily price indices. Following is the model to run using the Pesaran(2007) unit root test:</p>

<p>(i) With an intercept only</p>

<p>$$\Delta Y_{i,t} = a_i + b_iY_{i,t-1} + c_i\overline{Y}_{t-1} + d_i\Delta\overline{Y}_{t-1}+ e_i\Delta\overline{Y}_{t-2}+ f_i\Delta\overline{Y}_{i,t-1}+ g_i\Delta\overline{Y}_{i,t-2} + \varepsilon_{i,t}$$</p>

<p>where $\overline{Y}$ is the cross-section average of the observations across countries at each time $t$ and $b$ is the coefficient of interest to us because it will allow us to compute the ADF test statistic and then determine whether the process is stationary or not.</p>

<p>I constructed each of these variables in the following way:</p>

<p>$\Delta Y_t$</p>

<pre><code>dif.yt = diff(yt) 
## yt is the object containing all the observations for a specific country 
## (e.g. Australia)
</code></pre>

<p>$Y_{t-1}$</p>

<pre><code>yt.lag.1 = lag(yt, -1)
</code></pre>

<p>$\overline{Y}_{t-1}$</p>

<pre><code>ybar.lag.1 = lag(c(rowMeans(x)), -1) 
## x is the object containing my entire data frame
</code></pre>

<p>$\Delta \overline{Y}_{t-1}$</p>

<pre><code>dif.ybar.lag.1 = diff(ybar.lag.1)
</code></pre>

<p>$\Delta \overline{Y}_{t-2}$</p>

<pre><code>dif.ybar.lag.2 = diff(lag(c(rowMeans(x)), -2))
</code></pre>

<p>$\Delta Y_{t-1}$</p>

<pre><code>dif.yt.lag.1 = diff(yt.lag.1)
</code></pre>

<p>$\Delta Y_{t-2}$</p>

<pre><code>dif.yt.lag.2 = diff(lag(yt, -2)
</code></pre>

<p>After constructing each variable individually, I then run the linear regression</p>

<pre><code>reg = lm(dif.yt ~ yt.lag.1[-1] + ybar.lag.1[-1] + dif.ybar.lag.1 + 
                  dif.ybar.lag.2 + dif.yt.lag.1 + dif.yt.lag.2)
summary(reg)
</code></pre>

<p>It is obvious that the explanatory variables in my regression equation differ in length, so I'd like to know whether there is a way in R to make all the variables of equal length (perhaps with a function).</p>

<p>Also, I'd like to know whether the procedure I used was correct and if there are more optimal ways.</p>
"
"0.0379321620905441","0.0385614943639849"," 25817","<p>Is it possible to calculate AIC or BIC values for lasso regression models and other regularized models where parameters are only partially entering the equation.  How does one determine the degrees of freedom?</p>

<p>I'm using R to fit lasso regression models with the <code>glmnet()</code> function from the <code>glmnet</code> package, and I'd like to know how to calculate AIC and BIC values for a model.  In this way I might compare the values with models fit without regularization.  Is this possible to do?</p>
"
"0.0889588054368324","0.0822133822214443"," 25912","<p>I want to estimate a multivariate variance function in R.  That is, I want to allow the variance (as well as the mean) to vary according to some set of independent variables.  </p>

<p>In this particular case, I want to estimate the effects of a set of typical demographic covariates (age, race, education) on the variance of logged wages.  </p>

<p>What is a good way to implement this in R?  Is there a package that simplifies this?</p>

<p>It may be that this is only a search away - but having searched on the R help pages, Google, Rseek, and StackOverflow, I can't find anything relevant under ""variance function"" or similar. </p>

<p>Any suggestions gratefully received. </p>

<hr>

<p>Thanks for your responses -- I will try to clarify my question.</p>

<p>I am working in a maximum likelihood framework.  I can code this by hand from the log-likelihood, but the real data set has a <em>lot</em> of variables and ""optim"" is very slow, so I would like to find a package in R that makes this more computationally efficient.</p>

<p>I start with the log-likelihood for a basic OLS regression:
$$
\text{ln }L = \sum (-\frac{1}{2} (\text{ln }\sigma^2 - \frac{(y - xB)^2}{\sigma^2}))
$$
Then I relax the assumption of constant variance (homoskedasticity) and redefine the variance as:
$$
\sigma^2 = exp(Z*\gamma)
$$
where $Z$ is the matrix of variables affecting $\sigma^2$.  (Exponentiate so that you don't end up with $\sigma^2$ less than zero.)  When I substitute the reparameterization of $\sigma^2$ into the original log-likelihood and code the new log-likelihood function in R, I get this: </p>

<pre><code>ll.normal.vary &lt;- function (par, X, Y, Z) {
  beta  &lt;-par[1:ncol(X)]
  gamma &lt;- par[(ncol(X)+1):(ncol(X)+ncol(Z))]   
  -1/2* sum((Z %*% gamma) + ((Y - X %*% beta)^2)/exp(Z %*% gamma))
}
</code></pre>

<p>Then I optimize:</p>

<pre><code>v.optim1 &lt;- optim (par = start1, fn=ll.normal.vary, X=x.mat, Y=y.vec, Z=z.mat, 
                   method = ""BFGS"", hessian = F, control = list(fnscale = -1))
v.optim1$par
v.optim1$value
</code></pre>

<p>Here are some sample data if you want to test it:</p>

<pre><code>var1   &lt;- c(0,0,0,1,1,0)
var2   &lt;- c(.28, .07, -.05, .38, .08, -.1)
var3   &lt;- c(-.11, -.17, -.17, -.05, .1, -.01)
x.mat  &lt;- cbind(var1, var2, var3)
y.vec  &lt;- c(.46, .77, .49, .59, .60, .44)
z.mat  &lt;- cbind(var1, var2) 
start1 &lt;- rep(0.1, ncol(x.mat)+ncol(z.mat))
</code></pre>

<p>Thanks again for any tips.</p>
"
"0.0379321620905441","0.0192807471819925"," 25949","<p>I'm going through the videos in Andrew Ng's free <a href=""http://www.ml-class.org/"">online machine learning course</a> at Stanford.  He discusses Gradient Descent as an algorithm to solve linear regression and writing functions in Octave to perform it.  Presumably I could rewrite those functions in R, but my question is doesn't the lm() function already give me the output of linear regression?  Why would I want to write my own gradient descent function?  Is there some advantage or is it purely as a learning exercise?  Does lm() do gradient descent?</p>
"
"0.103881504159667","0.105605001571314"," 25988","<p>An assumption of the ordinal logistic regression is the proportional odds assumption. Using R and the 2 packages mentioned I have 2 ways to check that but I have questions in each one.</p>

<p>1) Using the rms package</p>

<p>Given the next commands</p>

<pre><code>library(rms)
ddist &lt;- datadist(Ki67,Cyclin_E)
options(datadist='ddist')
f &lt;- lrm(grade ~Ki67+Cyclin_E);f
sf &lt;- function(y)
c('Y&gt;=1'=qlogis(mean(y &gt;= 1)),'Y&gt;=2'=qlogis(mean(y &gt;= 2)),'Y&gt;=3'=qlogis(mean(y &gt;= 3)))
s &lt;- summary(grade ~Ki67+Cyclin_E, fun=sf)
plot(s,which=1:3,pch=1:3,xlab='logit',main='',xlim=c(-2.5,2.5))
</code></pre>

<p>I have</p>

<pre><code>lrm(formula = grade ~ Ki67 + Cyclin_E)

Frequencies of Missing Values Due to Each Variable
   grade     Ki67 Cyclin_E 
       0        0        3 


                     Model Likelihood     Discrimination    Rank Discrim.    
                        Ratio Test            Indexes          Indexes       

Obs            42    LR chi2     11.38    R2       0.268    C       0.728    
 1             11    d.f.            2    g        1.279    Dxy     0.456    
 2             15    Pr(&gt; chi2) 0.0034    gr       3.592    gamma   0.458    
 3             16                         gp       0.192    tau-a   0.308    
max |deriv| 1e-07                         Brier    0.166                     


         Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=2     -0.1895 0.8427 -0.22  0.8221  
y&gt;=3     -2.0690 0.9109 -2.27  0.0231  
Ki67      0.0971 0.0330  2.94  0.0033  
Cyclin_E -0.0076 0.0227 -0.33  0.7387 
</code></pre>

<p>The <code>s</code> table gives: (unfortunately I don't know how to upload a graph made in R)</p>

<pre><code>grade    N=45

+--------+-------+--+----+---------+----------+
|        |       |N |Y&gt;=1|Y&gt;=2     |Y&gt;=3      |
+--------+-------+--+----+---------+----------+
|Ki67    |[ 2, 9)|12|Inf |0.6931472|-1.0986123|
|        |[ 9,16)|12|Inf |0.3364722|-2.3978953|
|        |[16,24)|10|Inf |2.1972246| 0.0000000|
|        |[24,44]|11|Inf |2.3025851| 1.5040774|
+--------+-------+--+----+---------+----------+
|Cyclin_E|[ 3,16)|15|Inf |1.0116009|-0.1335314|
|        |[16,22)| 7|Inf |1.7917595|-0.9162907|
|        |[22,33)|10|Inf |1.3862944|-0.8472979|
|        |[33,80]|10|Inf |0.4054651|-0.4054651|
|        |Missing| 3|Inf |      Inf| 0.6931472|
+--------+-------+--+----+---------+----------+
|Overall |       |45|Inf |1.1284653|-0.4054651|
+--------+-------+--+----+---------+----------+
</code></pre>

<p>Where for the Ki67 I see that 3 out of the 4 differences  <code>logit(P[Y&gt; = 2])-logit(P[Y&gt; = 3])</code> are close to 2. Only the last one is quite lower (around 0.8). But here Ki67 is continuous and not categorical so I don't know if the results of the table are correct and there isn't any p-value to decide. By the way I run the above in SPSS and I didn't reject the assumption.</p>

<p>2) Using the VGAM package</p>

<p>Here using the next commands I have the model under the assumption of proportional odds</p>

<pre><code>library(VGAM)
fit1 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=T))
summary(fit1)
</code></pre>

<p>And the results</p>

<pre><code>Coefficients:
                Estimate Std. Error  z value
(Intercept):1  0.1894723   0.820442  0.23094
(Intercept):2  2.0690395   0.886732  2.33333
Ki67          -0.0970972   0.032423 -2.99467
Cyclin_E       0.0075887   0.021521  0.35261

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 79.86801 on 80 degrees of freedom

Log-likelihood: -39.93401 on 80 degrees of freedom

Number of iterations: 5 
</code></pre>

<p>While using the next commands I have the model without the assumption of proportional odds</p>

<pre><code>fit2 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=F))
</code></pre>

<p>where unfortunately i receice the next message </p>

<blockquote>
  <p>Warning message: In vglm.fitter(x = x, y = y, w = w, offset = offset,
  Xm2 = Xm2,  :   convergence not obtained in 30 iterations</p>
</blockquote>

<p>However if I type <code>summary(fit2)</code> I get results but again I don't know if they are correct. My intention was to use the next commands and get the answer but know I doubt if this is correct (by the way if I do it I get <code>p-value=0.6</code>. </p>

<pre><code>pchisq(deviance(fit1)-deviance(fit2),
df=df.residual(fit1)-df.residual(fit2),lower.tail=FALSE)
</code></pre>

<p>So, regarding the methods mentioned above does anyone knows whether the results I get are valid or in the case of the VGAM package is there any way to increase the number of itterations?Is there any other way to check it? </p>
"
"0.104084994078794","0.112425109314872"," 26284","<p><strong>Context:</strong></p>

<p>Iâ€™m working with the Census Bureauâ€™s <a href=""http://en.wikipedia.org/wiki/American_Community_Survey"" rel=""nofollow"">American Community Survey</a> (ACS) data which are samples (not complete enumerations) aggregated at different spatial scales.  Each ACS estimate is provided with a margin of error (which is easily converted to a standard error, SE).  Iâ€™m attempting to generate a regression line â€˜envelopeâ€™ by randomly generating X and Y values using the estimated values and associated SEs. In other words, <strong>I want to see what the regression line <em>might</em> look like given the level of uncertainty associated with both the X and Y estimates</strong>.</p>

<p><strong>Problem:</strong></p>

<p>My approach seemed simple at first: using Monte Carlo techniques, generate random xâ€™s and yâ€™s using the estimates (X and Y)  and associated error distribution (X_SE and Y_SE),  plot the  OLS regression results, then plot the OLS results using the estimated values (X and Y) to get the central regression line.  What I observed is that as the SE  increases (relative to the estimate), the cluster of regression lines take on  a â€˜flatterâ€™ slopeâ€”away from the regression line generated using the estimated values X and Y.  Hereâ€™s an example  (the grey lines are the regression lines from 1000 iterations and the red line is the regression for the estimates, x and y):</p>

<pre><code>ID    X   X_SE Y Y_SE
1 22752 2350 644 31
2 20251 1554 498 27
3 31041 1982 868 22
4 20838 3643 544 58
5 26876 3665 725 57
6 24656 2501 626 31
7 25291 4052 726 55
8 28003 5795 772 70
9 21254 2442 606 44
10 22977 1639 669 31
11 19870 2560 524 95
12 26983 3577 782 64
13 20709 2781 593 46
14 22213 3116 647 71
15 19401 1875 496 70
16 27137 1812 814 42
</code></pre>

<p><img src=""http://i.stack.imgur.com/7a03M.jpg"" alt=""enter image description here""></p>

<p>After surfing the web, Iâ€™ve come across some helpful links. It appears that one assumption underlying the standard regression models is that <a href=""http://en.wikipedia.org/wiki/Errors-in-variables_models"" rel=""nofollow"">â€œregressors [independent variables] have been measured exactly, or observed without error; as such, those models account only for errors in the dependent variables, or responsesâ€</a>.  I believe that the problem Iâ€™m encountering is referred to as <em>errors-in-variables models</em>. Googling this term (as well as others Iâ€™ve come across such as <em>regression dilution</em> and <em>attenuation</em>) returns many links--mostly articles scattered across many disciplines. But none are shedding light into what exact course I should follow in addressing my problem, nor am I finding pertinent information in introductory or intermediate stats textbooks. So my questions are:</p>

<ol>
<li><p>Is the MC approach Iâ€™m taking a good one in estimating the range of regression lines? If so can a standard regression model be used in the MC subroutine (assuming of course that pertinent data distribution requirements, other than a fixed independent variable, are met)? </p></li>
<li><p>How should I estimate the central regression line? Iâ€™ve come across some <code>R</code> libraries such as <code>Deming</code> and <code>Model II</code> that <em>seem</em> to address my problem, however, I donâ€™t see an option in those routines that take into account each Xâ€™s SE values. But more importantly, I donâ€™t fully understand what it is that those functions do exactly. Any lucid perspective on this would be greatly appreciated.</p></li>
</ol>
"
"0.103881504159667","0.105605001571314"," 26500","<p>Hello after struggling with using R for the last couple of days I was hoping someone could help me with a statistical analysis I am completing for an environmental science honours project. Using R statistics is not something we have been taught and I am worried that I may have bitten of more then I can chew, however my whole project is based around the <strong>hierarchical partitioning method and the exhaustive search multiple regression analysis method.</strong></p>

<p>The <a href=""http://cran.r-project.org/web/packages/hier.part/index.html"" rel=""nofollow"">hier.part</a> package was installed along with <a href=""http://cran.r-project.org/web/packages/gtools/index.html"" rel=""nofollow"">gtools</a>.</p>

<p>I have converted my dataset to a .csv file with seven independent variables and one dependant variable with around 400 replicates (my intention is to do this analysis on eight datasets in total with different amounts of replicates and another dependant variable, but I am starting with this one). The dependant variable is GPP, the independent variables are, NDVI, Temperature, Precipitation, Solar Radiation, Nutrient Availability and Soil Available Water Capacity.</p>

<p>Secondly I imported the .csv file into R using the script</p>

<pre><code>GPPANDDRIVER &lt;- read.table(""C:\\etc, header=T, sep="","")
</code></pre>

<p>This works fine and I can edit the table using </p>

<pre><code>edit(GPPANDDRIVER)
</code></pre>

<p>After looking at the <code>hier.part</code> package documentation available <a href=""http://cran.r-project.org/web/packages/hier.part/hier.part.pdf"" rel=""nofollow"">here</a> it seems like I need to define Y which in the script below is the dependent variable and define <code>scan</code> which is the independent variables (mentioned before).</p>

<pre><code>hier.part(y, xcan, family = ""gaussian"", gof = ""RMSPE"", barplot = TRUE)
</code></pre>

<p>I was defining the dependant <code>y</code> vector as </p>

<pre><code>y &lt;- as.vector(GPPANDDRIVER[""GPP""])
</code></pre>

<p>This also works fine and I have my y vector. However I am not sure how to load independent variables onto the xcan dataframe part of the script. I have tried typing in two scripts but they have not worked.</p>

<pre><code>xcan &lt;- as.vector(GPPANDDRIVER[-GPP])
## AND
xcan &lt;- data.frame(GPPANDDRIVER[-GPP])
</code></pre>

<p>If anyone could help me find the right script for representing my independant variables as xcan that would be greatly appreciated. Also once defined if I entered in the hier.part script mentioned above would R then show me results of the analysis after processing? I will be moving onto to the regression analysis after this if anyone can shed some light on this first problem.</p>

<pre><code>*information on hier.part arguments.*

**Arguments**

y a vector containing the dependent variables

xcan a dataframe containing the n independent variables

family family argument of glm

gof Goodness-of-fit measure. Currently ""RMSPE"", Root-mean-square â€™predictionâ€™

error, ""logLik"", Log-Likelihood or ""Rsqu"", R-squared

print.vars if FALSE, the function returns a vector of goodness-of-fit measures. If TRUE, a data frame is returned with first column listing variable combinations and the
second column listing goodness-of-fit measures.
</code></pre>
"
"0.0709645772411954","0.072141950116023"," 26568","<p>I would like to understand how to generate <em>prediction intervals</em> for logistic regression estimates. </p>

<p>I was advised to follow the procedures in Collett's <em>Modelling Binary Data</em>, 2nd Ed p.98-99. After implementing this procedure and comparing it to R's <code>predict.glm</code>, I actually think this book is showing the procedure for computing <em>confidence intervals</em>, not prediction intervals.</p>

<p>Implementation of the procedure from Collett, with a comparison to <code>predict.glm</code>, is shown below.</p>

<p>I would like to know: how do I go from here to producing a prediction interval instead of a confidence interval?</p>

<pre><code>#Derived from Collett 'Modelling Binary Data' 2nd Edition p.98-99
#Need reproducible ""random"" numbers.
seed &lt;- 67

num.students &lt;- 1000
which.student &lt;- 1

#Generate data frame with made-up data from students:
set.seed(seed) #reset seed
v1 &lt;- rbinom(num.students,1,0.7)
v2 &lt;- rnorm(length(v1),0.7,0.3)
v3 &lt;- rpois(length(v1),1)

#Create df representing students
students &lt;- data.frame(
    intercept = rep(1,length(v1)),
    outcome = v1,
    score1 = v2,
    score2 = v3
)
print(head(students))

predict.and.append &lt;- function(input){
    #Create a vanilla logistic model as a function of score1 and score2
    data.model &lt;- glm(outcome ~ score1 + score2, data=input, family=binomial)

    #Calculate predictions and SE.fit with the R package's internal method
    # These are in logits.
    predictions &lt;- as.data.frame(predict(data.model, se.fit=TRUE, type='link'))

    predictions$actual &lt;- input$outcome
    predictions$lower &lt;- plogis(predictions$fit - 1.96 * predictions$se.fit)
    predictions$prediction &lt;- plogis(predictions$fit)
    predictions$upper &lt;- plogis(predictions$fit + 1.96 * predictions$se.fit)


    return (list(data.model, predictions))
}

output &lt;- predict.and.append(students)

data.model &lt;- output[[1]]

#summary(data.model)

#Export vcov matrix 
model.vcov &lt;- vcov(data.model)

# Now our goal is to reproduce 'predictions' and the se.fit manually using the vcov matrix
this.student.predictors &lt;- as.matrix(students[which.student,c(1,3,4)])

#Prediction:
this.student.prediction &lt;- sum(this.student.predictors * coef(data.model))
square.student &lt;- t(this.student.predictors) %*% this.student.predictors
se.student &lt;- sqrt(sum(model.vcov * square.student))

manual.prediction &lt;- data.frame(lower = plogis(this.student.prediction - 1.96*se.student), 
    prediction = plogis(this.student.prediction), 
    upper = plogis(this.student.prediction + 1.96*se.student))

print(""Data preview:"")
print(head(students))
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by Collett's procedure:""))
manual.prediction
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by R's predict.glm:""))    
print(output[[2]][which.student,c('lower','prediction','upper')])
</code></pre>
"
"0.0758643241810882","0.0771229887279699"," 26614","<p>Continuing on from this <a href=""http://stats.stackexchange.com/questions/26329/what-is-a-unit-information-prior"">question</a> and this <a href=""http://stats.stackexchange.com/questions/26339/the-unit-information-prior-and-its-bic-approximation"">question</a> re BIC and its approximation to the Bayes factor with a unit information prior <a href=""http://www.jstor.org/discover/10.2307/2291327?uid=3737536&amp;uid=2&amp;uid=4&amp;sid=47698890289477"" rel=""nofollow"">(Kass &amp; Wasserman, 1995)</a>, I'm trying to quantify this relationship as a stepping stone into Bayesian stats. So far, my calculation of the BIC approximation of the Bayes factor (based upon my impression of <a href=""http://www.jstor.org/discover/10.2307/2291327?uid=3737536&amp;uid=2&amp;uid=4&amp;sid=47698890289477"" rel=""nofollow"">Wagenmakers 2007</a>) is linearly related to my Bayes factor that is calculated from my interpretation of the unit information prior using the <a href=""http://www.r-inla.org/home"" rel=""nofollow"">INLA</a> package in R. Good start! However, my BIC Bayes factor is ~ 3 times smaller than the Bayes factor calculated with INLA and I'm not sure why. </p>

<p>The prior I've used in the ""<strong>inla</strong>"" function is N(0, 1/(variance * n)) and this seems to me the likely place where I'm out. I'm not sure how I got the multiply by n in the formula, but it appears to work... roughly. Kass and Wasserman have N(0, variance / n) which when converted to precision would be N(0, n / variance), but this gives me a less good relationship. </p>

<p>Help based on other Bayesian packages is also welcome.</p>

<p><em>EDIT</em></p>

<pre><code>*Deleted code, see below answer instead*
</code></pre>

<p><em>EDIT</em></p>

<p>So I'm pretty sure I've figured out the one sample case. I would still appreciate help for the two sample case and the regression case (which I'll start working on now).</p>
"
"NaN","NaN"," 26790","<p>I'm working on a securities pricing project and have a bunch of models I'd like to stack/ensemble together.  I've been using simple linear regression in R (the <code>lm()</code> function) so far but the results are over fitting pretty badly.  </p>

<p>Does anyone have any suggestions for whether some other stacking method might be better or any papers/articles that describe how to stack regression models (as opposed to classification models).</p>
"
"0.0715255707714427","0.0727122510865616"," 26927","<p>I have two sets of data from the FRED database: real GDP (y) and GDP deflator (p) and I want to be able to use R in order to estimate a VAR(p) (p determined by AIC) process and generate the sets of impulse-response functions with the short-run assumptions (Sims, 1980) which utilizes the Cholesky decomposition.</p>

<p>Since this is a website for learning, this is a detailed explanation of the process so that this post could actually ""teach"" something to some of you. If you can help me and you already know how to do it, the first paragraph is actually what I am looking for.</p>

<p>Impulse-response analysis is the analysis of the dynamic response of an economic variable of interest (e.g. real GDP) to shocks in other economic variables such as demand shocks (e.g. inflation) or supply shocks ( e.g. technology). In order to do that, we may want to use a reduced form vector autoregressive process (RVAR):</p>

<p><img src=""http://latex.codecogs.com/gif.latex?Y_%7Bt%7D%20%3D%20B_%7B1%7DY_%7Bt-1%7D%20&plus;%20B_%7Bt-2%7D%20&plus;%20...%20&plus;%20B_%7Bt-p%7D%20&plus;%20%5Cvarepsilon%20_%7Bt%7D"" alt=""RVAR(1)""></p>

<p>Where:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?Y_%7Bt%7D%20%3D%20%5Cbegin%7Bbmatrix%7D%20Y_%7B1%2Ct%7D%5C%5C%20...%5C%5C%20Y_%7Bk%2Ct%7D%5C%5C%20%5Cend%7Bbmatrix%7D"" alt=""""></p>

<p><img src=""http://latex.codecogs.com/gif.latex?B%20%3D%20%5Cbegin%7Bbmatrix%7D%20b_%7B11%7D%20%26%20...%20%26%20b_%7B1k%7D%5C%5C%20...%20%26%20...%20%26%20...%20%5C%5C%20b_%7Bk1%7D%20%26%20...%20%26%20b_%7Bkk%7D%20%5Cend%7Bbmatrix%7D"" alt=""""></p>

<p><img src=""http://latex.codecogs.com/gif.latex?%5Cvarepsilon%20_%7Bt%7D%20%3D%20%5Cbegin%7Bbmatrix%7D%20%5Cvarepsilon%20_%7B1%2Ct%7D%5C%5C%20...%5C%5C%20%5Cvarepsilon%20_%7Bk%2Ct%7D%5C%5C%20%5Cend%7Bbmatrix%7D"" alt=""""></p>

<p>The variance-covariance matrix of this process is as follows:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?E%5Cvarepsilon%20_%7Bt%7D%5Cvarepsilon%20_%7Bt%7D%5E%7B%27%7D%20%3D%20%5Csum"" alt=""Variance-covariance matrix""></p>

<p>And it is a symmetric, positive definite matrix whose off-diagonal values are non zero, which means that the error terms are mutually correlated. Consequently, our attempt to trace the dynamic responses of our variable of interest will be hindered. One solution to this problem is the use of a structural vector autoregressive process (SVAR):</p>

<p><img src=""http://latex.codecogs.com/gif.latex?A_%7B0%7DY_%7Bt%7D%20%3D%20A_%7B1%7DY_%7Bt-1%7D%20&plus;%20A_%7B2%7DY_%7Bt-2%7D%20&plus;%20...%20&plus;%20A_%7Bp%7DY_%7Bt-p%7D%20&plus;%20u_%7Bt%7D"" alt=""Variance-covariance matrix""></p>

<p>Where A0 is the contemporaneous relations between the k variables.</p>

<p>We can multiply both sides of this equation by the inverse of the contemporaneous effect:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?Y_%7Bt%7D%20%3D%20A_%7B0%7D%5E%7B-1%7DA_%7B1%7DY_%7Bt-1%7D%20&plus;%20A_%7B0%7D%5E%7B-1%7DA_%7B2%7DY_%7Bt-2%7D%20&plus;%20...%20&plus;%20A_%7B0%7D%5E%7B-1%7DA_%7Bp%7DY_%7Bt-p%7D%20&plus;%20A_%7B0%7D%5E%7B-1%7Du_%7Bt%7D"" alt=""Structural form VAR""></p>

<p>Where:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?%5Cvarepsilon%20_%7Bt%7D%20%3D%20A_%7B0%7D%5E%7B-1%7Du_%7Bt%7D"" alt=""""></p>

<p><img src=""http://latex.codecogs.com/gif.latex?A_%7Bj%7D%20%3D%20A_%7B0%7DB_%7Bj%7D"" alt=""""></p>

<p><img src=""http://latex.codecogs.com/gif.latex?E%5Cvarepsilon%20_%7Bt%7D%5Cvarepsilon%20_%7Bt%7D%5E%7B%27%7D%20%3D%20I"" alt=""""></p>

<p>So here, we need to estimate A0 (which is assumed to be a lower triangular matrix) in order to fully describe the SVAR.</p>

<p>One popular method was proposed by Sims (1980) and involves short-run assumptions using the Cholesky decomposition of the variance-covariance matrix such that:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?%5Csum%20%3D%20PP%5E%7B%27%7D"" alt=""""></p>

<p>Where:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?P%20%3D%20A_%7B0%7D%5E%7B-1%7D"" alt=""""></p>

<p>By recursive substitution of the VAR(1) process:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?Y_%7Bt&plus;j%7D%20%3D%20B_%7B1%7D%5E%7Bj&plus;1%7D%20&plus;%20Pu_%7Bt&plus;j%7D%20&plus;%20B_%7B1%7DPu_%7Bt&plus;j-1%7D%20&plus;%20...%20&plus;%20B_%7B1%7D%5E%7Bj%7DPu_%7Bt%7D"" alt=""""></p>

<p>And finally, the impulse-response function of Y_t+j is:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?%5Cpsi%20_%7Bj%7D%20%3D%20B_%7B1%7D%5E%7Bj%7DP"" alt=""""></p>

<hr>

<p>As you can see, I understand the process completely and I would like to be able to do it using R. So what I'm trying to do is:
(i): Estimate the final VAR(p) process (p determined by AIC)
(ii): Generate the impulse-response function</p>

<p>Thank you very much.</p>
"
"NaN","NaN"," 27297","<p>I'm using the <code>logistf</code> package in R to perform Firth logistic regression on an unbalanced dataset. I have a logistf object:</p>

<pre><code>fit = logistf(a~b)
</code></pre>

<p>Is there a <code>predict()</code> function like on that's used in the <code>lm</code> class to predict probabilities for future data points? Or do I have to manually input the estimated parameters from the Firth regression.</p>
"
"0.0715255707714427","0.0636232197007414"," 27351","<p>I want to compare three models, one linear-regression-model, one regression-tree-model (from <code>rpart</code>) and one MARS-model (from <code>mda</code> package).</p>

<p>I want to compare the models using a <em>leave one out cross validation</em> using the mean square error and MAPE. I have the following implementation in R:</p>

<pre><code>library(data.table)
library(rpart)
library(mda)

#Load Sample-Data
data(trees)

#The following models should be compared:
# lm(Volume~Girth+Height, data=trees)
# rpart(Volume~Girth+Height, data=trees)
# mars(trees[,-3], trees[3])

LOOCV&lt;-function(modelCall) {
  unlist(sapply(seq(1,nrow(trees)), function(i) {         
    training=trees[-i,]
    test=trees[i,]

    fit=eval(modelCall)
    testValue = predict(fit, test[1:2])

    test[3]-testValue
  }))
}

LOOCV_MSE&lt;-function(modelCall) {
   sum(LOOCV(modelCall)^2)/nrow(trees)
}

LOOCV_RMSE&lt;-function(modelCall) {
   sqrt(LOOCV_MSE(modelCall))
}

LOOCV_MAPE&lt;-function(modelCall) {
  sum(abs(LOOCV(modelCall)/sapply(seq(1, nrow(trees)), function(i) {trees[i,3]})))/nrow(trees)*100                                    
}


cat(""Cross-Validation Metrics:\n"")
cat(""-------------------------\n"")
cat(""LOOCV MSE for LM:"", LOOCV_MSE(quote(lm(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MSE for CART:"", LOOCV_MSE(quote(rpart(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MSE for MARS:"", LOOCV_MSE(quote(mars(training[,-3], training[3]))),""\n"")
cat(""\n"")

cat(""LOOCV RMSE for LM:"", LOOCV_RMSE(quote(lm(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV RMSE for CART:"", LOOCV_RMSE(quote(rpart(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV RMSE for MARS:"", LOOCV_RMSE(quote(mars(training[,-3], training[3]))),""\n"")
cat(""\n"")

cat(""LOOCV MAPE for LM:"", LOOCV_MAPE(quote(lm(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MAPE for CART:"", LOOCV_MAPE(quote(rpart(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MAPE for MARS:"", LOOCV_MAPE(quote(mars(training[,-3], training[3]))),""\n"")
</code></pre>

<p>Outputs:</p>

<pre><code>Cross-Validation Metrics:
-------------------------
LOOCV MSE for LM: 18.15783 
LOOCV MSE for CART: 69.83769 
LOOCV MSE for MARS: 13.72282 

LOOCV RMSE for LM: 4.2612 
LOOCV RMSE for CART: 8.356895 
LOOCV RMSE for MARS: 3.704432 

LOOCV MAPE for LM: 14.6114 
LOOCV MAPE for CART: 23.51401 
LOOCV MAPE for MARS: 10.00316 
</code></pre>

<p>Does this implementation make sense? When whould using MSE on the errors make sense? When would I use MAPE/SMAPE instead? I already read ""<a href=""http://stats.stackexchange.com/questions/13478/metric-to-compare-models"">Metric to compare models?</a>"" and the conclusion there was <em>it depends</em>, can someone explain this further. On what does it depend?</p>

<p>My data is not a time series, it is more like the <code>tree</code> example data. </p>
"
"0.12949915926392","0.136711051308229"," 27830","<p>In a previous post Iâ€™ve wondered how to <a href=""http://stats.stackexchange.com/questions/22494/is-using-a-questionnaire-score-euroqols-eq-5d-with-a-bimodal-distribution-as"">deal with EQ-5D scores</a>. Recently I stumbled upon logistic quantile regression suggested by <a href=""http://www.ncbi.nlm.nih.gov.proxy.kib.ki.se/pubmed/19941281"">Bottai and McKeown</a> that introduces an elegant way to deal with bounded outcomes.
The formula is simple:</p>

<p>$logit(y)=log(\frac{y-y_{min}}{y_{max}-y})$</p>

<p>To avoid log(0) and division by 0 you extend the range by a small value, $\epsilon$. This gives an environment that respects the boundaries of the score. </p>

<p>The problem is that any $\beta$ will be in the logit scale and that makes doesnâ€™t make any sense unless transformed back into the regular scale but that means that the $\beta$ will be non-linear. For graphing purposes this doesnâ€™t matter but not with more $\beta$:s this will be very inconvenient. </p>

<p>My question:</p>

<p><strong>How do you suggest to report a logit $\beta$ without reporting the full span?</strong></p>

<hr>

<h2>Implementation example</h2>

<p>For testing the implementation Iâ€™ve written a simulation based on this basic function:</p>

<p>$outcome=\beta_0+\beta_1* xtest^3+\beta_2*sex$</p>

<p>Where $\beta_0 = 0$, $\beta_1 = 0.5$ and $\beta_2 = 1$. Since there is a ceiling in scores Iâ€™ve set any outcome value above 4 and any below -1 to the max value.</p>

<h3>Simulate the data</h3>

<pre><code>set.seed(10)
intercept &lt;- 0
beta1 &lt;- 0.5
beta2 &lt;- 1
n = 1000
xtest &lt;- rnorm(n,1,1)
gender &lt;- factor(rbinom(n, 1, .4), labels=c(""Male"", ""Female""))
random_noise  &lt;- runif(n, -1,1)

# Add a ceiling and a floor to simulate a bound score
fake_ceiling &lt;- 4
fake_floor &lt;- -1

# Just to give the graphs the same look
my_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, 
             fake_ceiling + abs(fake_ceiling)*.25)
my_xlim &lt;- c(-1.5, 3.5)

# Simulate the predictor
linpred &lt;- intercept + beta1*xtest^3 + beta2*(gender == ""Female"") + random_noise
# Remove some extremes
linpred[linpred &gt; fake_ceiling + abs(diff(range(linpred)))/2 |
    linpred &lt; fake_floor - abs(diff(range(linpred)))/2 ] &lt;- NA
#limit the interval and give a ceiling and a floor effect similar to scores
linpred[linpred &gt; fake_ceiling] &lt;- fake_ceiling
linpred[linpred &lt; fake_floor] &lt;- fake_floor
</code></pre>

<p>To plot the above:</p>

<pre><code>library(ggplot2)
# Just to give all the graphs the same look
my_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, 
             fake_ceiling + abs(fake_ceiling)*.25)
my_xlim &lt;- c(-1.5, 3.5)
qplot(y=linpred, x=xtest, col=gender, ylab=""Outcome"")
</code></pre>

<p>Gives this image:</p>

<p><img src=""http://i.stack.imgur.com/luZGu.png"" alt=""Scatterplot from simulation""></p>

<h3>The regressions</h3>

<p>In this section I create the regular linear regression, quantile regression (using the median) and logistic quantile regression. All estimates are based on bootstrapped values using the bootcov() function.</p>

<pre><code>library(rms)

# Regular linear regression
fit_lm &lt;- Glm(linpred~rcs(xtest, 5)+gender, x=T, y=T)
boot_fit_lm &lt;- bootcov(fit_lm, B=500)
p &lt;- Predict(boot_fit_lm, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))
lm_plot &lt;- plot.Predict(p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)

# Quantile regression regular
fit_rq &lt;- Rq(formula(fit_lm), x=T, y=T)
boot_rq &lt;- bootcov(fit_rq, B=500)
# A little disturbing warning:
# In rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique

p &lt;- Predict(boot_rq, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))
rq_plot &lt;- plot.Predict(p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)

# The logit transformations
logit_fn &lt;- function(y, y_min, y_max, epsilon)
    log((y-(y_min-epsilon))/(y_max+epsilon-y))


antilogit_fn &lt;- function(antiy, y_min, y_max, epsilon)
    (exp(antiy)*(y_max+epsilon)+y_min-epsilon)/
        (1+exp(antiy))


epsilon &lt;- .0001
y_min &lt;- min(linpred, na.rm=T)
y_max &lt;- max(linpred, na.rm=T)
logit_linpred &lt;- logit_fn(linpred, 
                          y_min=y_min,
                          y_max=y_max,
                          epsilon=epsilon)

fit_rq_logit &lt;- update(fit_rq, logit_linpred ~ .)
boot_rq_logit &lt;- bootcov(fit_rq_logit, B=500)


p &lt;- Predict(boot_rq_logit, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))

# Change back to org. scale
transformed_p &lt;- p
transformed_p$yhat &lt;- antilogit_fn(p$yhat,
                                    y_min=y_min,
                                    y_max=y_max,
                                    epsilon=epsilon)
transformed_p$lower &lt;- antilogit_fn(p$lower, 
                                     y_min=y_min,
                                     y_max=y_max,
                                     epsilon=epsilon)
transformed_p$upper &lt;- antilogit_fn(p$upper, 
                                     y_min=y_min,
                                     y_max=y_max,
                                     epsilon=epsilon)

logit_rq_plot &lt;- plot.Predict(transformed_p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)
</code></pre>

<h3>The plots</h3>

<p>To compare with the base function Iâ€™ve added this code:</p>

<pre><code>library(lattice)
# Calculate the true lines
x &lt;- seq(min(xtest), max(xtest), by=.1)
y &lt;- beta1*x^3+intercept
y_female &lt;- y + beta2
y[y &gt; fake_ceiling] &lt;- fake_ceiling
y[y &lt; fake_floor] &lt;- fake_floor
y_female[y_female &gt; fake_ceiling] &lt;- fake_ceiling
y_female[y_female &lt; fake_floor] &lt;- fake_floor

tr_df &lt;- data.frame(x=x, y=y, y_female=y_female)
true_line_plot &lt;- xyplot(y  + y_female ~ x, 
                         data=tr_df,
                         type=""l"", 
                         xlim=my_xlim, 
                         ylim=my_ylim, 
                         ylab=""Outcome"", 
                         auto.key = list(
                           text = c(""Male"","" Female""),
                           columns=2))


# Just for making pretty graphs with the comparison plot
compareplot &lt;- function(regr_plot, regr_title, true_plot){
  print(regr_plot, position=c(0,0.5,1,1), more=T)
  trellis.focus(""toplevel"")
  panel.text(0.3, .8, regr_title, cex = 1.2, font = 2)
  trellis.unfocus()
  print(true_plot, position=c(0,0,1,.5), more=F)
  trellis.focus(""toplevel"")
  panel.text(0.3, .65, ""True line"", cex = 1.2, font = 2)
  trellis.unfocus()
}

compareplot(lm_plot, ""Linear regression"", true_line_plot)
compareplot(rq_plot, ""Quantile regression"", true_line_plot)
compareplot(logit_rq_plot, ""Logit - Quantile regression"", true_line_plot)
</code></pre>

<p><img src=""http://i.stack.imgur.com/74Uid.png"" alt=""Linear regression for bounded outcome""></p>

<p><img src=""http://i.stack.imgur.com/xHRtF.png"" alt=""Quantile regression for bounded outcome""></p>

<p><img src=""http://i.stack.imgur.com/XfLy8.png"" alt=""Logistic quantile regression for bounded outcome""></p>

<h3>The contrast output</h3>

<p>Now I've tried to get the contrast and it's almost ""right"" but it varies along the span as expected:</p>

<pre><code>&gt; contrast(boot_rq_logit, list(gender=levels(gender), 
+                              xtest=c(-1:1)), 
+          FUN=function(x)antilogit_fn(x, epsilon))
   gender xtest Contrast   S.E.       Lower      Upper       Z      Pr(&gt;|z|)
   Male   -1    -2.5001505 0.33677523 -3.1602179 -1.84008320  -7.42 0.0000  
   Female -1    -1.3020162 0.29623080 -1.8826179 -0.72141450  -4.40 0.0000  
   Male    0    -1.3384751 0.09748767 -1.5295474 -1.14740279 -13.73 0.0000  
*  Female  0    -0.1403408 0.09887240 -0.3341271  0.05344555  -1.42 0.1558  
   Male    1    -1.3308691 0.10810012 -1.5427414 -1.11899674 -12.31 0.0000  
*  Female  1    -0.1327348 0.07605115 -0.2817923  0.01632277  -1.75 0.0809  

Redundant contrasts are denoted by *

Confidence intervals are 0.95 individual intervals
</code></pre>
"
"0.0379321620905441","0.0385614943639849"," 28070","<p>I was wondering if anyone had experience using the mice function, as described in <a href=""http://www.jstatsoft.org/v45/i03/paper"" rel=""nofollow"">mice: Multivariate Imputation by Chained Equations in R</a> (JSS 2011 45(3))? I have a dataset with a number of variables, each with varying degrees of missing data. </p>

<p>My primary question is: say I use Bayesian linear regression to impute missing data, does <code>mice</code> automatically use predictor variables from most significant to least significant to impute? Also, is it common to perhaps average all the imputed datasets?</p>
"
"0.0763370036711974","0.0862261227118454"," 28115","<p>I am using the estimated county-level poverty measure from the Small Area Income and Poverty Estimates <a href=""http://www.census.gov/did/www/saipe/"" rel=""nofollow"">(SAIPE)</a> as the dependent variable in a regression analysis. This value is itself the result of a model and comes complete with upper and lower 90% confidence interval bounds. To be clear, each of my 3000+ observations has an estimated value and its own confidence interval based on the sample size for that county (targeted at 2.5% of population)</p>

<p>I am wondering about the best way to incorporate the uncertainty in my dependent variable into my regression model.</p>

<p>One way I have imagined is doing a random draw from a distribution using the estimated value and confidence interval for each observation. I would then re-run my regression using this simulated value for the dependent variable in my regression analysis and compare model outcomes to the model using the estimated value. By simulating new values and comparing many times I would gain an understanding of how sensitive my findings are to the estimates on the dependent variable.</p>

<p>A key component of this is pulling a random number based on the estimated value and the upper and lower confidence intervals. The data is left and right censored at 0 and 100 and the estimated value is not centered within the confidence interval that is: abs(estimate-cl) != abs(estimate-cu)</p>

<p>I was intrigued by the discussion here: <a href=""http://stats.stackexchange.com/questions/12742/sampling-random-numbers-from-a-distribution-with-asymmetric-confidence-intervals"">Sampling random numbers from a distribution with asymmetric confidence intervals generated by a bootstrapped estimate</a></p>

<p>but a modified version of the code just generates the estimated value
Here is an example using the first 6 records from the 2008 SAIPE</p>

<pre><code>sample.size&lt;-c(1258.850,4405.300,745.900,539.725,1444.850,273.02)
POV08L90&lt;-c(8.77,8.24,18.08,13.90,10.55,22.45)
POV08H90&lt;-c(12.54,11.18,24.82,20.87,15.27,33.83)
POV08&lt;-c(10.7,9.9,24.5,18.5,13.1,33.6)
test.data&lt;-data.frame(sample.size,POV08L90,POV08H90,POV08)

gammaGenerate&lt;-function(dat){
  for(i in 1:length(dat$sample.size)){
    n&lt;-dat[i,""sample.size""]
    cl&lt;-dat[i,""POV08L90""]
    cu&lt;-dat[i,""POV08H90""]
    barx&lt;-dat[i,""POV08""]
    talpha = qt(p=0.95,df=n-1)
    s = (cu - cl)*sqrt(n)/(2*talpha)
    kappa = 6*s*s*n*( cl - barx + talpha*s/sqrt(n) )
    gamma.shape = 4/(kappa*kappa)
    gamma.scale = s/sqrt(gamma.shape)
    gamma.shift = barx - gamma.shape*gamma.scale
    print(c(barx,(rgamma(n = 5, shape = gamma.shape) + gamma.shift)))    
  }
}
gammaGenerate(test.data)
</code></pre>

<p>Any help you can offer--either directing me to a better method of dealing with the uncertainty in my dependent variable, or an explanation for why my rgamma always lands at 0 would be very welcome.</p>
"
"0.0379321620905441","0.0385614943639849"," 28286","<p>I need to fit a GLS model, with some known regressors, and where the errors follow an <strong>unknown</strong> ${\rm ARIMA}(1,0,1) \times (1,N,1)$ model. It seems like the main tool out there for such models is the <code>gls</code> function in the <code>nlme</code> package for <code>R</code>. </p>

<p>In <code>gls</code>, one specifies the correct correlation struction using a <code>corStruct</code> object, but I cannot find any <code>corStruct</code> objects for specifying my (really simple) seasonal model. I am new to R, so I don't think I am up for coding a new <code>corStruct</code> for my purposes. Are there any other packages out there for solving this problem? If not, can you point me to some references about how to create custom <code>corStruct</code> objects. Thanks for all your help!</p>
"
"0.0379321620905441","0.0385614943639849"," 28472","<p>A Regression with ARIMA errors is given by the following formula (saw on Hyndman et al, 1998):</p>

<p>$Y_t = b_0 + b_1 X_{1,t} + \dots + b_k X_{k,t} + N_t$</p>

<p>where $N_t$ is modeled as an ARIMA process.</p>

<p>If we have that the model for $N_t$ is ARIMA$(0,0,0)$, then $N_t = e_t$, and $Y_t$ is modeled by an ordinary regression.</p>

<p>Suppose the following data:</p>

<pre><code>a &lt;- structure(c(29305, 9900, 9802, 17743, 49300, 17700, 24100, 11000, 
10625, 23644, 38011, 16404, 14900, 16300, 18700, 11814, 13934, 
12124, 18097, 30026, 3600, 15700, 12300, 14600), .Tsp = c(2010.25, 
2012.16666666667, 12), class = ""ts"")
b &lt;- structure(c(1.108528016, 1.136920872, 1.100239002, 1.057191265, 
1.044200511, 1.102063834, 1.083847756, 1.068585841, 1.084879628, 
1.232979511, 1.168894672, 1.257302058, 1.264967051, 1.234793782, 
1.306452369, 1.252644047, 1.178593218, 1.124432965, 1.132878661, 
1.189926986, 1.17249669, 1.176285957, 1.176552, 1.179178082), .Tsp = 
c(2010.25, 2012.16666666667, 12), class = ""ts"")
</code></pre>

<p>If I model it using <code>auto.arima</code> function, I have:</p>

<pre><code>auto.arima(a, xreg=b)
Series: a 
ARIMA(0,0,0) with zero mean     

Coefficients:
              b
      15639.266
s.e.   1773.186

sigma^2 estimated as 101878176:  log likelihood=-255.33
AIC=514.65   AICc=515.22   BIC=517.01

lm(a~b)

Call:
lm(formula = a ~ b)

Coefficients:
(Intercept)            b  
      48638       -26143  
</code></pre>

<p>Coefficients from the models differ. Shouldn't they be the same? What am I missing?</p>
"
"0.113796486271632","0.102830651637293"," 28492","<p>For fun, I tried to replicate the results of <a href=""http://rpproxy.iii.com:9797/MuseSessionID=248c435aa056d82d70d390e949c628fb/MuseHost=rfs.oxfordjournals.org/MusePath/content/22/1/435.abstract"" rel=""nofollow"">Petersen (2009)</a> who deals with the correct estimation of standard errors in finance panel data sets. </p>

<p>In a nutshell, he estimates the following standard regression for a panel data set:</p>

<p>$$
Y_{it} = X_{it} \beta + \epsilon_{it}
$$ </p>

<p>where $\epsilon_{it} = \gamma_i + \eta_{it}$ and $x_{it} = \mu_{i} + \nu_{it}$. Hence, both the residual and the independent variable have a firm-specific component. Petersen goes on to show that this results in biased standard errors when applying the standard OLS. For example, he shows in table 1 of his paper that if both the residual volatility and the variable volatility are driven by 50% by a firm-specific component, the true standard errors are nearly twice as large as the ones given by OLS.</p>

<p>He shows that in a MCS and I reproduced those results in R, as you can see from the code below. Naturally, I asked myself how I would compute the correct standard errors in R and the package of choice seemed to be <code>plm</code>. However, I just don't get the correct results out of it and I don't know what I miss.</p>

<p>Here is my code:</p>

<pre><code>library(plm)
runMCS &lt;- function(runs, nrN, nrT, fracFirmX, fracFirmEps, sd_X, sd_eps, beta) {

  betas    &lt;- numeric(runs)
  se_betas &lt;- numeric(runs)
  panel_betas    &lt;- numeric(runs)
  se_panel_betas &lt;- numeric(runs)

  for (i in 1:runs) {

    #Model epsilon, X, and Y
    eps &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_eps * sqrt(fracFirmEps)), 
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_eps * sqrt(1-fracFirmEps))
    X   &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_X   * sqrt(fracFirmX)),   
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_X   * sqrt(1-fracFirmX))
    Y   &lt;- beta * X + eps

    #Compute regression (OLS)
    reg &lt;- summary(lm(Y ~ X))

    #Save results
    betas[i]    &lt;- reg$coef[2, 1]
    se_betas[i] &lt;- reg$coef[2, 2]

    #Try plm
    df &lt;- data.frame(Firm = rep(1:nrN, each=nrT),
                     Time = rep(1:nrT, times=nrN),
                     Y = Y,
                     X = X)
    preg &lt;- summary(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")) #within is fixed effects
    panel_betas[i]    &lt;- preg$coef[1, 1]
    se_panel_betas[i] &lt;- preg$coef[1, 2]
  }

  return(c(avg_beta = mean(betas), 
           true_se = sd(betas), 
           avg_se = mean(se_betas), 
           avg_clustered = mean(panel_betas),
           se_clustered = mean(se_panel_betas)))

}
MCS_50_50 &lt;- runMCS(50, 500, 10, 0.5, 0.5, 1, 2, 1)
MCS_50_50
     avg_beta       true_se        avg_se avg_clustered  se_clustered 
   1.00503955    0.06020203    0.02825567    1.00433092    0.02985546
</code></pre>

<p>Note that I only run the simulation 50 times here because the plm function slows it down considerably. So basically, it makes virtually no difference if I call <code>lm</code> or <code>plm</code>. I'm pretty confident that I set the <code>index</code> and <code>model</code> option correct after reading the vignette of the package. However, I must miss something here! Interestingly, the package also has the <code>fixef</code> function and if I call that on one run, I get something like  this:</p>

<pre><code>summary(fixef(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")))
1      13.60377     0.44112    30.8391 &lt; 2.2e-16 ***
2    -830.74707     0.44136 -1882.2236 &lt; 2.2e-16 ***
3    -326.96042     0.44137  -740.7840 &lt; 2.2e-16 ***
4     169.16463     0.44246   382.3287 &lt; 2.2e-16 ***
...
</code></pre>

<p>I'm not quite sure how to interpret those results, but here, I get considerably larger standard errors for each firm separately. If I would average those, I would end up with something above 0.44 which is considerably closer to the true standard errors, but still not right.</p>

<p>So, again a very long question from me, sorry for that ;-) Note that I did check answers before and I found this interesting <a href=""http://stats.stackexchange.com/questions/10017/standard-error-clustering-in-r-either-manually-or-in-plm"">link</a>. The white paper that is referred to in the answer is interestingly the same person that implemented the solution on Petersen's <a href=""http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm"" rel=""nofollow"">webpage</a>. So I'm pretty sure that I could get the correct standard errors by implementing Mahmood Arai's solution. But I'm looking for an already implemented and therefore safe option and I just wonder why that plm function does not work.</p>
"
"0.113796486271632","0.115684483091955"," 28688","<p>I ran <code>lm()</code> on my data with models selected by individual <code>lm</code>'s of each characteristic and then combined the top $R^2$ based on $p$-value. For instance, the first few characteristics are taken, then the rest are evaluated if they have $p&lt;.005$. My characteristics contain some duplication: for instance, I have a characteristic and its normalized variant in test P. My $p$-values are all very small but my diagrams do not look correct for R and T. (Referring to this blog post: <a href=""http://www.findnwrite.com/musings/evaluating-linear-regression-model-in-r/"" rel=""nofollow"">Evaluating Linear Regression Model in R</a>.)</p>

<p>In test P (and T) there is one outlier according to Cooks Distance. How do I find and eliminate that instance?</p>

<p>According to this tutorial on <a href=""http://www.montefiore.ulg.ac.be/~kvansteen/GBIO0009-1/ac20092010/Class8/Using%20R%20for%20linear%20regression.pdf"" rel=""nofollow"">Using R for Linear Regression</a>,</p>

<blockquote>
  <p>The plot in the upper left shows the residual errors plotted versus
  their fitted values.  The residuals should be randomly distributed
  around the horizontal line representing a residual error of zero; that
  is, there should not be a distinct trend in the distribution of
  points.</p>
</blockquote>

<p>Test P looks ok in the residual error but test R and T have a grouping what does that mean and how do I account for it?</p>

<blockquote>
  <p>The plot in the lower left is a standard Q-Q plot, which should
  suggest that the  residual errors are normally distributed.  The
  scale-location plot in the upper right shows the square root of the
  standardized residuals (sort of a square root of relative error) as a
  function of the fitted values.  Again, there should be no obvious
  trend in this plot.</p>
</blockquote>

<p>Again Test P looks ok in the standard Q-Q plot but test R and T have a grouping what does that mean and how do I account for it?</p>

<p>Also what is the coefficients on the output. I notice it lists the characteristics and a p value but i don't understand what it means.</p>

<p>And finally how do I make predictions using the model I created? </p>

<p><strong>Test P</strong>
F-statistic: 2.684 on 280 and 2221 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/10A8r.png"" alt=""enter image description here""></p>

<p><strong>Test R</strong>
F-statistic: 3.691 on 258 and 2243 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/jy6IR.png"" alt=""enter image description here""></p>

<p><strong>Test T</strong>
F-statistic: 4.029 on 268 and 2233 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/bs69P.png"" alt=""enter image description here""></p>

<p>edit after running gls my p looks like this</p>

<p><img src=""http://i.stack.imgur.com/cUBGB.png"" alt=""""></p>
"
"0.0709645772411954","0.072141950116023"," 28737","<p>I have time series as </p>

<pre><code>0.4385487 0.7024281 0.9381081 0.8235792 0.7779642 1.1670665 1.1958634 1.1958634 0.8235792 0.8530141 0.8802216 1.1958634 1.1235897 1.3542734 1.3245534 0.9381081 1.1670665 1.1958634 0.8802216 1.3542734 1.1670665 4.9167998 0.9651803 0.8221709 1.1070461 1.2006974 1.3542734 0.9651803 0.9381081 0.9651803 0.8854192 1.3245534 1.1235897 1.2006974 1.1958634 0.4385487 1.3245534 4.9167998 1.2277843 0.8530141 1.0018480 0.3588158 0.8530141 0.8867365 1.3542734 1.1958634 1.1958634 0.9651803 0.8802216 0.8235792 4.9167998 1.1958634 0.9651803 0.8854192 0.8854192 1.2006974 0.8867365 0.9381081 0.8235792 0.9651803 0.4385487 0.9936722 0.8821301 1.3542734 1.1235897 1.6132899 1.3245534 1.3542734 0.8132233 0.8530141 1.1958634 1.2279813 0.8354292 1.3578511 1.1070461 0.8530141 0.9670581 1.1958634 0.7779642 1.2006974 1.1958634 0.8235792 1.3245534 0.5119648 2.3386331 0.8890464 0.8867365 4.9167998 1.2006974 1.2006974 0.6715839 4.9167998 0.7747481 4.9167998 0.8867365 1.2277843 0.8890464 1.2277843 0.8890464 1.0541099 0.8821301 
</code></pre>

<p>I am using package ""itsmr""-autofit(),""forecast""-auto.arima(),""package""--functions</p>

<ol>
<li><p>Autoregressive model</p>

<pre><code>&gt; ar(t)

Call:
    ar(x = t)

    Order selected 0  sigma^2 estimated as  0.9222 
</code></pre></li>
<li><p>ARMA model</p>

<pre><code>&gt; autofit(t)
    $phi
    [1] 0

    $theta
    [1] 0

    $sigma2
    [1] 0.9130698

    $aicc
    [1] 279.4807

    $se.phi
    [1] 0

    $se.theta
    [1] 0
</code></pre></li>
<li><p>ARIMA model</p>

<pre><code>    &gt; auto.arima(t)
    Series: t 
    ARIMA(0,0,0) with non-zero mean 

    Coefficients:
          intercept
             1.2623
    s.e.     0.0951

    sigma^2 estimated as 0.9131:  log likelihood=-138.72
    AIC=281.44   AICc=281.56   BIC=286.67
</code></pre>

<p>The auto.arima function automatically differences time series: we don't have to worry about transformation.</p>

<pre><code>&gt; auto.arima(AirPassengers)
Series: AirPassengers 
ARIMA(0,1,1)(0,1,0)[12]                    

Coefficients:
          ma1
      -0.3184
s.e.   0.0877

sigma^2 estimated as 137.3:  log likelihood=-508.32
AIC=1020.64   AICc=1020.73   BIC=1026.39`
</code></pre></li>
</ol>

<p>Which model should I select to get p,q values &amp; for forecasting purpose?</p>
"
"0.107288356157164","0.109068376629842"," 28756","<p>I have two datasets a training and a test dataset. The dependent variable is a proportion and there are 54 predictors which are positive and negative real numbers and another 7 predictors that are text. </p>

<p>There are three response variables. Total the normalized total number of hits. Treatment the normalized total number during treatment and a percent which is a ratio of the other two responses.</p>

<p>At the moment using lm on the percent prediction data I have a corolation of .4. 85% of the varibles are within 20% of their target. For the treatment response variable using glm in poisson mode i have a correlation of .6 percent but the variables do not match the target data at all.</p>

<p>I have two main issues I need advice on: </p>

<p>(1) it rejected the text predictors because it said factor has new level(s)
I would like it to ignore the information for those that have new level but not disregard it for those that have the correct information how do i do that? </p>

<p>(2) To make my dependent variable a real number, rather than a proportion bounded between 0 and 1, I was advised to transform the response using, for example, the logit transform or the Normal quantile function (<code>qnorm</code> in R). The problem is that these transformations (and others like it) will map 0 and 1 to non-finite values. How can I model these data in a regression setting when the response is a proportion that can be 0 or 1? </p>

<p>Using linear regression with outlier removal I am able to get 2239 of 2583 testing data within 20% of their actual value I would like to have that many within 10%. </p>

<p>Using the posson distribution glm the amount of treatment correlates with 69%.</p>

<p>Ignoring this second issue for the moment, I transform the y~x1+x2 such that y=log(y/(1-y)) the correlation of my predictions to actual data drops from 6% to 2%
This is what the data looks like after the logit transform</p>

<p><img src=""http://i.stack.imgur.com/rqkaD.png"" alt=""log distribution""></p>

<p>This is what the data looks like before the log distribution
<img src=""http://i.stack.imgur.com/Mx8sh.png"" alt=""normal percentages""></p>
"
"NaN","NaN"," 28882","<p>Is there any function for $M$-estimation in multivariate linear regression model in <code>R</code>. I can estimate the $\beta$'s in my model by using the <code>rlm()</code> by rewriting the $y$-variables into one column but, I would like to use one function to get the $\beta$'s. </p>
"
"0.026822089039291","0.0272670941574606"," 29044","<p>R and Statistics newbie here.</p>

<p>Ok, I have a logistic regression and have used the predict function to develop a probability curve based on my estimates. </p>

<pre><code>## LOGIT MODEL:
library(car)
mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

## PROBABILITY CURVE:
all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:1000,predict(mod1,newdata=data.frame(bid&lt;-c(000:1000)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>This is great but I'm curious about plotting the confidence intervals for the probabilities. I've tried plot.ci() but had no luck. Can anyone point me to some ways to get this done, preferably with the car package or base R.</p>

<p>Thanks.</p>
"
"0.144521682086727","0.146919440446297"," 29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"0.0379321620905441","0.0385614943639849"," 29406","<p>I have the following linear model:</p>

<p>$$w^*=\text{arg min}_w\sum_{i=1}^N \bigg(Y_i-\sum_{j=1}^M X_{i,j}\times w_j\bigg)^2$$</p>

<p>Let $T \in N^*$ and $e_i=|Y_i-\sum_{j=1}^M X_{i,j}\times w_j|$. </p>

<p>It's possible using logistic regression to predict which errors will be less than $T$ (i.e., $e_i&lt;T$) and greater or equal with $T$ (i.e., $e_i \ge T$)?</p>

<p>Here is more information to make the question clearer:</p>

<p>$N$ represent the number of observations. My data has the following property: the histogram of errors using multiple linear regression has a Laplace distribution. My data come from digital images represented on 8 bits. The $Y_i$ are current pixels and $X_{ij}$ are neighborhoods pixels. I want to predict which pixels produce errors less than $T$. I want to know what R functions can I use to make a test? $T$ is not very large, it has the values between 1 and 15 in general.</p>
"
"0.026822089039291","0.0272670941574606"," 29525","<p>I have data with a starting (y) value that sequentially increments/decrements as (x) time  measured in days passes. 
I found this link for creating a linear regression of the data 
<a href=""http://www.easycalculation.com/statistics/regression.php"">http://www.easycalculation.com/statistics/regression.php</a></p>

<p>I would like to automate the slope calculation in excel. Does anyone have an idea of how to do it? I see the math formula at the bottom of the page</p>

<p>$$\frac{N\sum XY- \sum X\sum Y}{N\sum X^2-(\sum X)^2}$$</p>

<p>but i don't know how to translate it to an excel formula. The problem is mainly the $\sum XY$ and $\sum X^2$. The others are easy with the <code>count</code>, <code>sum</code> and <code>pow</code> function. 
My x coordinates and y coordinates are in rows such that <code>C1</code> is <code>x1</code> and <code>D1</code> is <code>x2</code>.</p>
"
"0.0929144419623766","0.0944559849109725"," 29653","<p>The likelihood ratio (a.k.a. deviance) $G^2$ statistic and lack-of-fit (or goodness-of-fit) test is fairly straightforward to obtain for a logistic regression model (fit using the <code>glm(..., family = binomial)</code> function) in R. However, it can be easy to have some cell counts end up low enough that the test is unreliable. One way to verify the reliability of the likelihood ratio test for lack of fit is to compare its test statistic and <em>P</em>-value to those of Pearson's chi square (or $\chi^2$) lack-of-fit test.</p>

<p>Neither the <code>glm</code> object nor its <code>summary()</code> method report the test statistic for Pearson's chi square test for lack of fit. In my search, the only thing I came up with is the <code>chisq.test()</code> function (in the <code>stats</code> package): its documentation says ""<code>chisq.test</code> performs chi-squared contingency table tests and goodness-of-fit tests."" However, the documentation is sparse on how to perform such tests:</p>

<blockquote>
  <p>If <code>x</code> is a matrix with one row or column, or if <code>x</code> is a vector and <code>y</code> is not given, then a <em>goodness-of-fit</em> test is performed (<code>x</code> is treated as a one-dimensional contingency table). The entries of <code>x</code> must be non-negative integers. In this case, the hypothesis tested is whether the population probabilities equal those in <code>p</code>, or are all equal if <code>p</code> is not given.</p>
</blockquote>

<p>I'd imagine that you could use the <code>y</code> component of the <code>glm</code> object for the <code>x</code> argument of <code>chisq.test</code>. However, you can't use the <code>fitted.values</code> component of the <code>glm</code> object for the <code>p</code> argument of <code>chisq.test</code>, because you'll get an error: ""<code>probabilities must sum to 1.</code>""</p>

<p>How can I (in R) at least calculate the Pearson $\chi^2$ test statistic for lack of fit without having to run through the steps manually?</p>
"
"0.0479808115123254","0.0609710760849692"," 29990","<p>I am doing research on the field of functional response of mites.
I would like to do a regression to estimate the parameters (attack rate and handling time) of the Rogers type II function.
I have a dataset of measurements.
<strong>How can I can best determine outliers?</strong></p>

<p>For my regression I use the following script in R (a non linear regression):
(the dateset is a simple 2 column text file called <code>data.txt</code> file with <code>N0</code> values (number of initial prey) and <code>FR</code> values (number of eaten prey during 24 hours):</p>

<pre><code>library(""nlstools"")
dat &lt;- read.delim(""C:/data.txt"")    
#Rogers type II model
a &lt;- c(0,50)
b &lt;- c(0,40)
plot(FR~N0,main=""Rogers II normaal"",xlim=a,ylim=b,xlab=""N0"",ylab=""FR"")
rogers.predII &lt;- function(N0,a,h,T) {N0 - lambertW(a*h*N0*exp(-a*(T-h*N0)))/(a*h)}
params1 &lt;- list(attackR3_N=0.04,Th3_N=1.46)
RogersII_N &lt;-  nls(FR~rogers.predII(N0,attackR3_N,Th3_N,T=24),start=params1,data=dat,control=list(maxiter=    10000))
hatRIIN &lt;- predict(RogersII_N)
lines(spline(N0,hatRIIN))
summary(RogersII_N)$parameters
</code></pre>

<p>For plotting the calssic residuals graphs I use following script:</p>

<pre><code>res &lt;- nlsResiduals (RogersII_N)
plot (res, type = 0)
hist (res$resi1,main=""histogram residuals"")
    qqnorm (res$resi1,main=""QQ residuals"")
hist (res$resi2,main=""histogram normalised residuals"")
    qqnorm (res$resi2,main=""QQ normalised residuals"")
par(mfrow=c(1,1))
boxplot (res$resi1,main=""boxplot residuals"")
    boxplot (res$resi2,main=""boxplot normalised residuals"")
</code></pre>

<h3>Questions</h3>

<ul>
<li><strong>How can I best determine which data points are outliers?</strong></li>
<li><strong>Are there tests I can use in R which are objective and show me which data points are outliers?</strong></li>
</ul>
"
"0.0663812836584521","0.0771229887279699"," 30005","<p>I am using PCA on foreign exchange return series to find a market ""beta"". I am using 10 years of daily data with a 2-year half life weighting in the PCA using the package FactoMineR's PCA function. I extract the first principal component return series (so the product of the first eigenvector and the returns matrix) and I want to regress that against each foreign exchange return vector to find the residuals, that is, to find each currency's returns independently of the market beta. </p>

<p>Should I use the same 2 year half-life weighting in the regressions? Will this ""double up"" the weighting somehow? Conversely if I don't weight the regression, will I implicitly be putting too much weight on PC1 returns that are less ""relevant""?</p>

<p>For what it's worth market participants tend psychologically to put a higher weight on recent than long past currency behaviour. </p>

<p>Thanks for the help.</p>
"
"0.0309714806541255","0.0472279924554862"," 30172","<p>I am working with regressions to understand the price creation of certain future contracts for commodities and try to explain it with other commodity pricese.
These future contracts have different time periods until they start with their delivery.
It is intuitive that a future contract with a long time to delivery e.g. 1 year has a higher standard error and is harder to explain by the other prices than a contract that has only one month to delivery left.</p>

<p>So I have data available for 1month, 3month and 6month. I would like to know if there is in R a possibility to estimate the regression functions of 2,3, ... 12months by the parameters gained in the 3 regressions. Do you think that would be reasonable?</p>

<p>thanks! Best F</p>
"
"NaN","NaN"," 30243","<p>I've recently embarked on fitting regression mixed models in the Bayesian framework, using a MCMC algorithm (function MCMCglmm in R actually).</p>

<p>I believe I have understood how to diagnose convergence of the estimation process (trace, geweke plot, autocorrelation, posterior distribution...).</p>

<p>One of the thing that strikes me in the Bayesian framework is that much effort seems to devoted to do those diagnostics, whereas very little appears to be done in terms of checking the residuals of the fitted model. For instance in MCMCglmm the residual.mcmc() function does exist but is actually not yet implemented (ie.returns: ""residuals not yet implemented for MCMCglmm objects""; same story for predict.mcmc()). It seems to be lacking from other packages too, and more generally is little discussed in the literature I've found (apart from DIC which is quite heavily discussed too).</p>

<p>Could anyone point me to some useful references, and ideally R code I could play with or modify? </p>

<p>Many thanks.</p>
"
"0.0599760143904067","0.0609710760849692"," 30415","<p>I am trying to replicate a colleague's work and am moving the analysis from Stata to R. The models she employs invoke the ""cluster"" option within the nbreg function to cluster the standard errors.</p>

<p>See <a href=""http://repec.org/usug2007/crse.pdf"">http://repec.org/usug2007/crse.pdf</a> for a fairly complete description of the what and why of this option</p>

<p>My question is how to invoke this same option for negative binomial regression within R?</p>

<p>The primary model in our paper is specified in Stata as follows</p>

<pre><code> xi: nbreg cntpd09 logpop08 pcbnkthft07 pccrunion07 urbanpop pov00 pov002 edu4yr ///
 black04 hispanic04 respop i.pdpolicy i.maxloan rollover i.region if isser4 != 1,   
 cluster(state)
</code></pre>

<p>and I have replaced this with </p>

<pre><code>pday&lt;-glm.nb(cntpd09~logpop08+pcbnkthft07+pccrunion07+urbanpop+pov00+pov002+edu4yr+
black04+hispanic04+respop+as.factor(pdpolicy)+as.factor(maxloan)+rollover+
as.factor(region),data=data[which(data$isser4 != 1),])
</code></pre>

<p>which obviously lacks the clustered errors piece.</p>

<p>Is it possible to do an exact replication? If so how? If not, what are some reasonable alternatives?</p>

<p>Thanks</p>

<p>[Edit]
As noted in the comments, I was hoping for a solution that didn't take me into the realm of multilevel models. While my training allows me to see that these things should be related, it is more of a leap than I am comfortable taking on my own. As such I kept digging and found this link:
<a href=""http://landroni.wordpress.com/2012/06/02/fama-macbeth-and-cluster-robust-by-firm-and-time-standard-errors-in-r/"">http://landroni.wordpress.com/2012/06/02/fama-macbeth-and-cluster-robust-by-firm-and-time-standard-errors-in-r/</a></p>

<p>that points to some fairly straightforward code to do what I want:</p>

<pre><code>library(lmtest)
pday&lt;-glm.nb(cntpd09~logpop08+pcbnkthft07+pccrunion07+urbanpop+pov00+pov002+edu4yr+
 black04+hispanic04+respop+as.factor(pdpolicy)+as.factor(maxloan)+rollover+
 as.factor(region),data=data[which(data$isser4 != 1),])
summary(pday)

coeftest(pday, vcov=function(x) vcovHC(x, cluster=""state"", type=""HC1""))
</code></pre>

<p>This doesn't replicate the results from the analysis in Stata though, probably because it is designed to work on OLS not negative binomial. So the search goes on. Any pointers on where I am going wrong would be much appreciated</p>
"
"0.0848188929679971","0.0776035104406608"," 30451","<p>I'm trying to build a model that would describe some process of payment and distribution of payments in time. I believe that time of payment has <a href=""http://en.wikipedia.org/wiki/L%C3%A9vy_distribution"" rel=""nofollow"">Levy distribution</a> with probability density function:</p>

<p>$ f(x,c)=\sqrt{\frac{c}{2\pi}}~~\frac{e^{ -\frac{c}{2x}}} {x^{3/2}} $</p>

<p>This distribution depends on parameter <em>c</em> which actually defines the shape of distribution. My task is to build model that explains dependency of this parameter on some explaining variables. I'm trying linear dependency $c = \sum_i \beta_i x_i$ </p>

<p>This is example of generalized linear model and is implemented in the <a href=""http://cran.r-project.org/web/packages/VGAM/index.html"" rel=""nofollow"">VGAM package</a> in R. Problem is that in a sample for building this model I have data only from some period at the beginning and this period is different for different groups of cases. And because of that I can not just to run the model in VGAM package on this data as the result will be incorrect significantly exaggerating probability of early payments.</p>

<p>One possible solution I can think about is to change the likelihood function from which parameter is estimated. If we have information only from time up to <em>t</em> and as cumulative distribution function of Levy distribution is:</p>

<p>$ F(x,c)=\textrm{erfc}\left(\sqrt{c/2x}\right) $</p>

<p>the density of distribution up to time <em>t</em>  is  $ f_1(x,c,t)= \frac{f(x,c)}{F(t,c)} $ (where $f(x,c), F(t,c)$ defined as above). This new density functions can be used in estimating regression parameters with maximum likelyhood method. But can it be done in R using methods from VGAM package or usual <strong>glm</strong> function or some other packages? Or there are some better approaches to my problem? I'm interested in implementation in R.</p>

<p>Thank you in advance for any help!</p>
"
"0.0709645772411954","0.072141950116023"," 30961","<p>I have a dataset with all multinomial (categorial) variables. The column names look like:</p>

<p>religionID, occupationID, sexID, ..., changedID</p>

<p>The independent variables are: ""religionID"" (a factor with 7 levels), ""occupationID"" (a factor with 3 levels), .... There are 15 total, all factors with multiple levels.</p>

<p>The dependent variable is: ""changedID"" (a factor with 4 levels).</p>

<p>In all variables, the ordering of the levels is arbitrary. (E.g., factor level 1 is not ""better than"" or ""greater than"" factor level 2)</p>

<p>I have about 500 rows of such data.</p>

<p>My goals: </p>

<ol>
<li><p>Find out which independent variables effect the dependent variables (e.g., does religion matter at all to changedID?)</p></li>
<li><p>Find out which independent variables have more effect than other independent variables (e.g., is religion more important than sex?)</p></li>
<li><p>Find out which factor levels in each independent variable has the most effect (e.g., which religionID factor leads the most often to changedIDs of factor level ""2""?)</p></li>
</ol>

<p>Of course, I would also like to know if the results above are statistically significant.</p>

<p>I've thought of building contingency tables (using R's <code>table()</code> function), and then running <code>chisq.test()</code> on the result. I guess this could be used to satisfy goal #1, if I build one such table for each independent variable. But it doesn't help with goal #2 and goal #3.</p>

<p>Multinomial regression might be the answer, but I am confused as to whether it applies to my situation, and if it does, how to use it and interpret the results.</p>

<p>Bonus points if I can have an solution in R.</p>
"
"0.026822089039291","0.0272670941574606"," 31294","<p>I'm trying to do a logistic regression on some data. </p>

<p>Here's a simplified version of the situation: </p>

<p>I'm trying to predict student success based on their history, etc. One of my predictors is the percentage of the courses they've passed in the past. If they haven't taken any courses, I don't want to set that to zero, because that's obviously different from having failed all their courses. Right now, these cases are set as NaN, but when I use the glm function in R, I get the following error: </p>

<blockquote>
  <p>Error in glm.fit(x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  : 
    NA/NaN/Inf in foreign function call (arg 1)</p>
</blockquote>

<p><strong>How do I predict performance for individuals who haven't taken any courses yet?</strong></p>
"
"0.0657004319817604","0.0556587228785024"," 31597","<p>I have the following probability function:</p>

<p>$$\text{Prob} = \frac{1}{1 + e^{-z}}$$</p>

<p>where</p>

<p>$$z = B_0 + B_1X_1 + \dots + B_nX_n.$$</p>

<p>My model looks like</p>

<p>$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-3.92 + 0.014\times(\text{bid})]\right)}$$</p>

<p>This is visualized via a probability curve which looks like the one below.</p>

<p><img src=""http://i.stack.imgur.com/JjQzf.png"" alt=""enter image description here""></p>

<p>I am considering adding a couple variables to my original regression equation. Let's say I add gender (categorical: F and M) and age (categorical: &lt; 25 and > 26) into the model, I end up with:</p>

<p>$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-3.92 + 0.014\times(\text{bid}) + 0.25\times(\text{gender}) + 0.15\times(\text{age})]\right)}$$</p>

<p>In R I can generate a similar probability curve which will tell me the probability of Y=1 when accounting for all three predictors. Where I'm lost is I want to find the probabilities for every possible permutation of these variations.</p>

<p>So when bid = 1,  gender = M, and age is >= 26, what is the probability that Y = 1? Similarly, when bid = 2, gender = F, and age is >= 26, what is the probability that Y = 1?</p>

<p>I want to generate a probability curve which will allow me to visualize this. </p>

<p>Can anyone help? I may be completely misunderstanding what kind of information one can glean from a logit model, but please tell me if I am also misunderstanding the theory.</p>
"
"0.0464572209811883","0.0472279924554862"," 32657","<p>I was playing with the <a href=""http://cran.r-project.org/web/packages/TSA/index.html"" rel=""nofollow"">TSA</a> package in R and wanted to test the <code>arimax</code> function to the solution provided in Pankratz's <em>Forecasting with Dynamic Regression Models</em>, chapter 8. The savings rate and the function seems to provide similar results as the ones in the book except for the IO weights which are quite different. I bet there is a transformation that I might be missing.</p>

<p>Any help on understanding why IO coefficients are so different would be appreciated...</p>

<p>the solution states </p>

<pre><code>AO @ t=82,43,89
LS @ t=99
IO @ t=62,55
</code></pre>

<p>with Parameters estimates</p>

<pre><code>C = 6.1635
w82 = 2.3346
w99 = -1.5114
w43 = 1.1378
w62 = 1.4574
w55 = -1.4915
w89 = -1.0702
AR1 = 0.7976
MA2 = -0.3762
</code></pre>

<p>To fit the model in R, I used
(<code>saving</code> is the data)</p>

<pre><code>arimax(saving, order = c(1,0,2), fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA), io=c(55,62), 
       xreg=data.frame(AO82=1*(seq(saving)==82),
                       AO43=1*(seq(saving)==43),
                       AO89=1*(seq(saving)==89),
                       LS99=1*(seq(saving)&gt;=99)),
       method='ML')
</code></pre>

<p>The savings rate data is (100 points)</p>

<p>4.9
5.2
5.7
5.7
6.2
6.7
6.9
7.1
6.6
7
6.9
6.4
6.6
6.4
7
7.3
6
6.3
4.8
5.3
5.4
4.7
4.9
4.4
5.1
5.3
6
5.9
5.9
5.6
5.3
4.5
4.7
4.6
4.3
5
5.2
6.2
5.8
6.7
5.7
6.1
7.2
6.5
6.1
6.3
6.4
7
7.6
7.2
7.5
7.8
7.2
7.5
5.6
5.7
4.9
5.1
6.2
6
6.1
7.5
7.8
8
8
8.1
7.6
7.1
6.6
5.6
5.9
6.6
6.8
7.8
7.9
8.7
7.7
7.3
6.7
7.5
6.4
9.7
7.5
7.1
6.4
6
5.7
5
4.2
5.1
5.4
5.1
5.3
5
4.8
4.7
5
5.4
4.3
3.5</p>

<p>here it is my output</p>

<pre><code>&gt; arimax(saving, order = c(1,0,2),fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA),io=c(55,62),xreg=data.frame(AO82=1*(seq(saving)==82),
+ AO43=1*(seq(saving)==43),AO89=1*(seq(saving)==89),LS99=1*(seq(saving)&gt;=99)),method='ML')

Call:
arimax(x = saving, order = c(1, 0, 2), xreg = data.frame(AO82 = 1 * (seq(saving) == 
    82), AO43 = 1 * (seq(saving) == 43), AO89 = 1 * (seq(saving) == 
    89), LS99 = 1 * (seq(saving) &gt;= 99)), fixed = c(NA, 0, NA, NA, NA, NA, 
    NA, NA, NA, NA), method = ""ML"", io = c(55, 62))

Coefficients:
         ar1  ma1     ma2  intercept    AO82    AO43     AO89     LS99    IO-55   IO-62
      0.7918    0  0.3406     6.0628  2.3800  1.1297  -1.0466  -1.4885  -0.5958  0.5517
s.e.  0.0674    0  0.1060     0.3209  0.3969  0.3780   0.3835   0.5150   0.4044  0.3772

sigma^2 estimated as 0.2611:  log likelihood = -75.57,  aic = 169.14
</code></pre>
"
"0.0599760143904067","0.0609710760849692"," 32694","<p>I'm using R together with the <code>forecast</code> package to set up a ARIMA model, that will be used to predict a energy related variable. I used <code>auto.arima()</code> to fit different models (according to geographic region), and I need to put the model coefficients in our database, so that the IT folks can automate things. That's exactly the problem: I simply don't know how set up the equations by looking at the model:</p>

<pre><code>ARIMA(1,0,1)(2,0,1)[12] with non-zero mean 

Coefficients:

       ar1     ma1    sar1    sar2     sma1   intercept    prec0    prec1
     0.3561  0.3290  0.6857  0.2855  -0.7079  11333.240   15.5291  28.0817

s.e. 0.2079  0.1845  0.2764  0.2251   0.3887   2211.302    6.2147   6.0906
</code></pre>

<p>I have 2 regressor variables (prec0 and prec1). Given the residuals, the ARIMA vector <code>ARIMA(1,0,1)(2,0,1)[12]</code>, the time series up to period $t$, the number $h$ of forecasting periods and the regressor matrix reg, how can I set a function to return the forecast values? I.e:</p>

<pre><code>do.forecast = function(residuals, ARIMA, timeSeries, h, regMatrix)
{
  p = ARIMA[1]
  q = ARIMA[3]

  ## arima equations here...
}
</code></pre>

<p>Thanks!  </p>

<p>PS: I know this is a possible duplicate of <a href=""http://stats.stackexchange.com/questions/23881/reproducing-arima-model-outside-r"">Reproducing ARIMA model outside R</a>, but my model seems very different, and I really don't know how to start with.</p>
"
"0.0709645772411954","0.072141950116023"," 33516","<p>When computing regression models with R, I regularly use the relevel function to get my model to give me results for the other level, too. I noticed that sometimes, but not often, this changed the model in the sense that levels of other factors that were significant before the relevelling are not any more. Is this inherent to relevelling or exceptional and maybe due to some problem with my data? Does it show that my data likely does not meet one of the prerequisites of linear models?</p>

<p>Related to that, is it alright if I use relevel, recompute my model, and then report significance values from both models in my article? If significance differs between the two models for a certain factor, I suppose I should then go with one that is less optimistic?</p>

<p>I suppose my question betrays that I don't know enough about lm to grasp the need for a base level. I thought I understood it pretty well ;) Somehow none of the introductions I read explained that point, or I was too daft to grasp it. So if someone could direct me to a site where the point of having base levels in lm is explained or explain it themselves, that would be great, too!</p>

<p>Edit: Here's a minimal example:</p>

<pre><code>library(datasets)
sprays&lt;-OrchardSprays
model&lt;-lm(decrease~treatment+rowpos+colpos,data=sprays)
summary(model)
</code></pre>

<p>Part of the summary says</p>

<pre><code>treatmentC    20.625      9.731   2.120  0.03866 *
</code></pre>

<p>So if treatment == C this has significant positive influence on 'decrease'.
Now I relevel 'treatment' to B to find out what influence treatment == A has:</p>

<pre><code>sprays$treatment&lt;-relevel(sprays$treatment,""B"")
summary(model)
</code></pre>

<p>And now treatment == C is not significant in this new model:</p>

<pre><code>treatmentC    17.625      9.731   1.811  0.07567 .
</code></pre>

<p>Sorry for posting in the wrong place! Can I move my question to stats statexchange or should I open a new one there?</p>
"
"0.11706119363752","0.113053187438237"," 33712","<p>I have a question about which prediction variance to use to calculate prediction intervals from a fitted <code>lm</code> object in R. </p>

<p>For a certain multiple linear regression model I have obtained an error variance with leave-one-out-cross-validation (LOOCV) by taking the mean of the squared difference between observed and predicted values (i.e., mean squared prediction error). I am aware of some of the drawbacks of LOOCV (e.g., <a href=""http://stats.stackexchange.com/questions/2352/when-are-shaos-results-on-leave-one-out-cross-validation-applicable"">When are Shao&#39;s results on leave-one-out cross-validation applicable?</a>), but for my specific application this was the easiest (and probably the only realistically) implementable CV method. The final fitted linear model (<code>fitted_lm</code>) is fitted with all observations and with this model I would like to make predictions for new observations (<code>new_observations</code>). For this I am using the <code>predict.lm</code>  function in R.</p>

<pre><code>predict(fitted_lm, new_observations, interval = ""prediction"", pred.var = ???)
</code></pre>

<p>My questions are:  </p>

<ul>
<li>What value do I use for <code>pred.var</code> (i.e., â€œthe variance(s) for future observations to be assumed for prediction intervalsâ€) in order to obtain realistic prediction intervals for my new_observations?  </li>
<li>Do I use the error variance obtained from the LOOCV, or do I use the functionâ€™s default (i.e., â€œthe default is to assume that future observations have the same error variance as those used for fittingâ€)?  </li>
<li>Is the mean squared prediction error not appropriate in this case?</li>
</ul>

<p>Following up on Michael Chernick's answer hereunder, I had a look in the Draper &amp; Smith (1998) book  (â€œApplied regression analysis. 3rd Editionâ€). In this book <em>s<sup>2</sup></em> is defined as â€œvariance about the regressionâ€ (p 32). This is, I presume, what we describe below as the model estimate of residual variance. Furthermore, this book mentions: </p>

<blockquote>
  <p>â€œSince the actual observed value of <em>Y</em> varies about the true mean value <em>Ïƒ<sup>2</sup></em> [independent of the <em>V(Å¶)</em>], a predicted value of an individual observation will still be given with <em>Å¶</em> but will have variance</p>
  
  <p><img src=""http://i.stack.imgur.com/uUPXs.jpg"" alt=""formula""></p>
  
  <p>With corresponding estimated value obtained by inserting <em>s<sup>2</sup></em> for <em>Ïƒ<sup>2</sup></em>â€ (pp 82-81).</p>
</blockquote>

<p>Thus, as far as I understand, in the D &amp; S book they only use the model estimate of residual variance to calculate confidence intervals. This would be the default setting in the <code>predict</code> function (function help: â€œthe default is to assume that future observations have the same error variance as those used for fittingâ€). However, as fosgen states below, â€œalthough LOOCV mean squared prediction error is not equal to the real mean squared prediction error, it is much more close to real than error variance of fitted modelâ€.</p>

<p>To make this more concrete; in my dataset I get a model estimate of residual variance of <code>0.005998</code> and a LOOCV mean squared prediction error of <code>0.007293</code>. What should I then fill in as <code>pred.var</code> in the <code>predict.lm</code> function:</p>

<ul>
<li>Nothing (i.e. use the default, which would equal to the model estimate of residual variance)</li>
<li><code>0.007293</code> (i.e. the LOOCV mean squared prediction error) </li>
<li><code>0.005998 + 0.007293</code> (Michael Chernick: â€œThe model estimate of residual variance gets added to the error variance due to estimating the parameters to get the prediction error variance for a new observationâ€).</li>
</ul>
"
"0.0402331335589365","0.0545341883149212"," 33857","<p>I'm trying to calculate logistic regression coefficients by defining the log-likelihood function and using maximum likelihood.</p>

<p>In some cases when the initial (start) values I gave to the maximum likelihood were not correct I got wrong results for the logistic regression (different from the ones I get when using <code>glm</code> for example).</p>

<p>Given the input data and y values, what should be the optimum initial values for logistic regression (or, in other words, what are the values that are being used in <code>glm</code>)?</p>
"
"0.080466267117873","0.0818012824723818"," 33862","<p>I have some models built with the <code>auto.arima</code> function from the <code>forecast</code> package. I'm modeling a variable called 'natural efluent energy' (ena), which is how much energy you can extract from some Hydrography region. There are 2 regressor variables (rainfall precipitation from period $t$ and $t-1$.)</p>

<p>Each region has it's own model - some series show positive trend, some shows negative trend, and some seems stationary. The problem is that some forecasts 'from <code>auto.arima</code>' are giving values higher/lower than usual (some forecasts give me negative values, which are not possible).</p>

<p>My original call is below:</p>

<pre><code> m1 = auto.arima(serie, xreg = regvars)
</code></pre>

<p>For the data on the link, I changed it to</p>

<pre><code> m1 = auto.arima(serie, xreg = regvars, max.P = 0, max.Q = 0, stationary = TRUE)
</code></pre>

<p>Then I get good forecasts in this case. My question is, what these parameters(<code>max.P</code>, <code>max.Q</code>) actually control, and how they relate to the trend show by my model variable?</p>

<p>Here is a link for the historic data:
<a href=""http://www.datafilehost.com/download-7718b3fc.html"" rel=""nofollow"">http://www.datafilehost.com/download-7718b3fc.html</a></p>

<p>And here a link for the forecast regressors:
<a href=""http://www.datafilehost.com/download-ca44dfa4.html"" rel=""nofollow"">http://www.datafilehost.com/download-ca44dfa4.html</a></p>

<p>And here a link of mean historic values, the forecast must fall between these values:
<a href=""http://www.datafilehost.com/download-e1e265b7.html"" rel=""nofollow"">http://www.datafilehost.com/download-e1e265b7.html</a></p>

<p>My data starts at 2001/Jun, so the serie is:</p>

<pre><code>  y = ts(dframe$ena, freq = 12, start = c(2001, 6))
</code></pre>
"
"0.0860220579279185","0.0874492493743911"," 34080","<p>EDIT: I have solved this problem myself. The problem with the simulation below is that the omitted variable should not be included in the 'true model'. I have written a blog post with a more detailed analysis <a href=""http://diffuseprior.wordpress.com/2012/08/15/probit-models-with-endogeneity/"" rel=""nofollow"">here</a>.</p>

<p>I am trying to calculate the Average Structural Function (ASF) for a binary response regression model with an endogenous variable. The ASF is known as the policy relevant result obtained from these models because it shows how the conditional probability of the outcome (one or zero) changes in response to changes in any of the explanatory variables.</p>

<p>To estimate the regression model, I have used a two-step control function approach, wherein the first stage regression residuals ($\textbf{v}_{i}$) are included as a right-hand-side variable in the second stage probit regression Ã  la Rivers and Vuong (1988). </p>

<p>Based on my reading of a paper by Blundell and Powell (2004) (and also <a href=""http://www.cemfi.es/~arellano/binary-endogeneity.pdf"" rel=""nofollow"">these lecture notes</a>) the ASF can be calculated as follows:</p>

<p>$P(y|\bar{\textbf{X}},v)=\widehat{ASF}=\frac{1}{N}\sum^{N}_{i} \Phi(\bar{\textbf{X}}\boldsymbol{\hat{\beta}}+\rho \hat{\textbf{v}_{i}}) $</p>

<p>where the $\textbf{X}$ values are held at a constant level (say their mean), and we average over all of the first-stage residuals (multiplied by the second stage coefficient $\rho$). In effect, this formalization will allow one to calculate how the probability of the outcome varies as the one of the x-variables changes, while all of the other values are (typically) held at their means.</p>

<p>Or so you would think. However, I have attempted this calculation on a simple simulation with R and have not been able to replicate the ASF. My R code is below. Basically, this is a simple setup where we want to measure the effect of y1 on y2 (the binary outcome). There is one omitted variable (x1) that renders y1 endogenous the regression equation of interest.</p>

<p>A picture of my attempt is:</p>

<p><img src=""http://i.stack.imgur.com/OZBA8.jpg"" alt=""enter image description here""></p>

<p>When $x_1$ is available, everything should be fine. Just estimate a standard probit of $y_2$  on $x_1$ and $y_1$. The ASF for this is just the normal CDF for changes in $y_1$. When $x_1$ is not observed, it becomes necessary to instrument $y_1$. </p>

<p>From the IV regression I have calculated the ASF as in the above, and plotted this with comparisons to the model where $x_1$ is observed (the blue line in the picture), and also where $x_1$ is not observed and $y_1$ is not instrumented (the green line).</p>

<p>The red line is my attempt to construct the ASF from the method described in the above. It is clear that this line is not matching the blue line as it should. I have gone wrong somewhere here but I am not sure where. Would somebody be able to help me with this please? </p>

<pre><code>rm(list=ls())
x1 &lt;- rnorm(10000)
x2 &lt;- rnorm(10000)
y1 &lt;- 1 + 0.5*x1 + x2 + rnorm(10000)
y2 &lt;- ifelse(0.5 + 0.5*y1 - 1.5*x1 + rnorm(10000) &gt; 0, 1, 0)

# true
r1 &lt;- glm(y2~y1+x1,binomial(link=""probit""))
data &lt;- data.frame(cbind(seq(-4,6,0.2),mean(x1)))
names(data) &lt;- c(""y1"",""x1"")
asf1 &lt;- cbind(data$y1,pnorm(predict(r1,data)))
plot(asf1,type=""l"",col=""blue"",xlab=""y1"",ylab=""P(y2)"")

# no endog correction
r2 &lt;- glm(y2~y1,binomial(link=""probit""))
data &lt;- data.frame(cbind(seq(-4,6,0.2)))
names(data) &lt;- c(""y1"")
asf2 &lt;- cbind(data$y1,pnorm(predict(r2,data)))
lines(asf2,type=""l"",col=""green"")

# control function approach
v1 &lt;- (residuals(lm(y1~x2)))/sd(residuals(lm(y1~x2)))
r3 &lt;- glm(y2~y1+v1,binomial(link=""probit""))
# proceedure to get asf
asf3 &lt;- cbind(seq(-4,6,0.2),NA)
for(i in 1:dim(asf3)[1]){
    dat2 &lt;- data.frame(cbind(asf3[i,1],v1))
    names(dat2) &lt;- c(""y1"",""v1"")
    asf3[i,2] &lt;- mean(pnorm(predict(r3,dat2)))
}
lines(asf3,type=""l"",col=""red"")
</code></pre>
"
"0.026822089039291","0.0272670941574606"," 34139","<p>I am using an ARIMA model to create a model for correlated errors from my regression model. I am using the <code>auto.arima</code> function from the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"" rel=""nofollow"">forecast</a> package in R. I am able to get more data at some frequent interval after the regression model is created, so I get more values for the correlated errors. </p>

<p>My question is how do I update the ARIMA model with a gap in time interval between readings.</p>
"
"0.0379321620905441","0.0385614943639849"," 34226","<p>I'm using a STL decomposition to make forecasts in R (using the <code>forecast</code> package), but I'm not sure how to incorporate my regressors into the model.</p>

<p>I'm using the forecast function:</p>

<pre><code>f = forecast(stl(my.data, s.window = 'periodic'), h = 12, method = 'arima')
</code></pre>

<p>The function above accepts the <code>xreg</code> parameter, but how do I specify the
model regressors and the forecast regressors?</p>

<p>Is there a function that does this or do I have to do it 'by hand'?</p>
"
"0.0464572209811883","0.0472279924554862"," 34549","<p>I understand that there is a function in R called <code>poly()</code> that can generate orthogonal polynomials--useful for applying on input variables before running a predictive model.</p>

<p>My question is that what is the role of categorical variables when we generate polynomials? Are they to be excluded?</p>

<h2>Update:</h2>

<p>Dan, Thank you for your kind response. I'm not sure I understand it completely - let me explain the query in more detail. I'm trying to run logistic regression using glmnet on <a href=""http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls"" rel=""nofollow"">Titanic dataset</a>. <BR/> Let us assume shortened set of columns:<ul>* class(factor with three levels 1, 2 ,3), <br/>* sex(factor: male, female), <br/>* Age (integer), <br/>*survived(factor &amp; target variable 0 or 1).</ul> The questions is it meaningful to create polynomial features based on these factors? e.g. class. If yes could you pls explain what it means? <BR> I've seen examples with numeric input variables, where one can pass the entire input set to the poly() function and get polynomial features as output. <br/> Your response is highly appreciated.</p>
"
"0.0379321620905441","0.0385614943639849"," 34690","<p>I wanted to focus on volatility forecasting, so instead of asking R to compute a GARCH where it would compute the errors on the returns, I wanted to model the volatility as an ARMA and add an external regressor using the argument xreg in the arima function.</p>

<p>I have two questions:</p>

<ul>
<li><p>Is it exactly equivalent to compute an ARMA(p,q) on the volatility with external regressors as the squared returns and to compute a GARCH (for the volatility forecast)</p></li>
<li><p>Is it the correct way to do it in R ?</p></li>
</ul>

<p>Tony</p>
"
"0.0438002879878403","0.0667904674542028"," 34997","<p>I want to do an ordinal logistic regression in R without the proportionality odds assumption. I know this can be done directly using <code>vglm()</code> function in <code>R</code> by setting <code>parallel=FALSE</code>.</p>

<p>But my problem is how to fix a particular set of coefficients in this regression setup? For example, say the dependent variable $Y$ is discrete and ordinal and can take values $Y = 1$, $2$, or $3$. If the regressors are $X_{1}$ and $X_{2}$, then the regression equations are</p>

<p>$$ \begin{aligned} 
{\rm logit} \big( P(Y \leq 1) \big) &amp;= \alpha_{1} + \beta_{11}X_{1} + \beta_{12}X_{2} \\
{\rm logit}\big(P(Y \leq 2) \big) &amp;= \alpha_{2} + \beta_{21}X_{1} + \beta_{22}X_{2} 
\end{aligned} $$</p>

<p>I want to set $\beta_{11}$ and $\beta_{22}$ to $1$. Please let me know how can I achieve this. Also if <code>R</code> can't do this, could you also please let me know if I can achieve this in any other statistical software?</p>
"
"0.107956825902732","0.121942152169938"," 35489","<p>I have real daily market data which I'm looking at to create a model for forecasting. The model that I created (below) used autoregressive terms within a linear regression.</p>

<p>I was sharing this with a colleague and he said ""autoregressive variables are correlated with the other variables in multiple linear setting which creates multicollinarity problem, creating unreliable result.""</p>

<p>So I'm turning to the group for help. Here is the data and the analysis that I performed in R.</p>

<pre><code>#Read in Data
MarketData = read.table('http://sharp-waterfall-3397.herokuapp.com/MarketCategories6.txt', header=TRUE,na.strings = ""NA"", sep="","")
MarketData$Month &lt;- as.factor(MarketData$Month)
MarketData$Weekday &lt;- as.factor(MarketData$Weekday)

str(MarketData)
</code></pre>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/PERregress/index.html"" rel=""nofollow"">PERregress</a> library to help with the autoregression using the <code>back()</code> function and to help with the residual diagnostics:</p>

<pre><code>library(PERregress)
descStat(MarketData)
</code></pre>

<p>Subsetting the data for model building and prediction purposes:</p>

<pre><code>Total = MarketData
MarketData = MarketData[1:268,]
attach(MarketData)
</code></pre>

<p>Here is a regression with everything that I can think of. Note you can have higher autoregressive terms but this will start to mask events since R will ignore the first several rows. Also just an FYI for some reason the residual analysis is breaking which I liked to look for points with undue leverage.</p>

<pre><code>#Market1Category1 Regression for the markets with everything that I can think of it
Market1Category1Output=lm(Market1Category1 ~ Trend+Month2+Month3+Month4+
                          Month5+Month6+Month7+Month8+Month9+Monday+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday2+Holiday3+Holiday4+
                          Event1+Event2+Event3+Event4+Event5+Event6+Event7+
                          Event8+Event9+Event10+Event11+Event12+Event13+
                          Event14+Event15+Event16+Event17+Event18+Event19+
                          Event20+Event21+Event22+Event23+Event24+Event25+
                          Event26+Event27+Event28+
                          back(Market1Category1)+back(Market1Category1, 2))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is the final equation. I'd like to say that I reduced the variables using partial f-test but I couldn't find an easy way to do this so if you know a function please let me know. Basically I looked at the change in adjusted $R^2$.</p>

<pre><code>#Final regression equation 
Market1Category1Output=lm(Market1Category1 ~ Month5+Month6+Month7+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday3+Event2+Event7+Event10+
                          Event13+Event16+Event25+Event28+
                          back(Market1Category1)+back(Market1Category1, 6))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is a plot of the actuals in green vs the predictions in blue but there's a problem:</p>

<pre><code>plot(Time, Market1Category1, col='green')
points(Time, predict(Market1Category1Output, MarketData), col='blue', pch=20)
</code></pre>

<p>The issue is that predict will use the data values instead of it's predicted values for the autoregressive terms. In order to make it use predicted terms I created this loop. If you know a better way let me know.</p>

<pre><code>dataSet2 &lt;- Total
dataSet2[8:length(dataSet2$Time),""Market1Category1""] &lt;- NA
    for (i in (1:(length(dataSet2$Time)-7))) {
  dataSet2[6+i+1,""Market1Category1""] &lt;- 1
  dataSet2[6+i+1,""Market1Category1""] &lt;- predict(Market1Category1Output, 
                                                dataSet2[0:6+i+1,])[6+1] 
}
</code></pre>

<p>Here is the plot again with the results in blue using the predicted results for the autoregressive terms (with the exception of the first 7 since the model needs those to <code>predict</code>):</p>

<pre><code>plot(Total$Time, Total$Market1Category1, col='green')
points(dataSet2$Time, dataSet2$Market1Category1, col='blue', pch=20)
</code></pre>

<p>So here are my questions in order of importance:</p>

<ol>
<li>Does using autoregressive and linear terms violate any fundamental assumptions?</li>
<li>What issues can this cause and what analysis/steps should I do take to avoid these problems?</li>
<li>Is there a better approach to modeling this timeseries?</li>
<li>Is there a more efficient approach?</li>
<li>Given the residuals what steps would you take?</li>
</ol>

<p>Finally two questions which is just causing me more work than possibly necessary:</p>

<ol>
<li>As you can see instead of using the factors for weekday and month I'm using separate conditional variables. I'm doing this because if I use the factor and a level turns out to be insignificant (e.g., Monday for days of the week). I can't remove it. Perhaps there's a way?</li>
<li>Is there a quick way to run a partial F-statistic to understand whether removing a variable makes sense?</li>
</ol>
"
"0.100359067582572","0.0947366868222571"," 35683","<p>To fit a simple AR(5) model, I use SAS <code>PROC AUTOREG</code>.  I called the option <code>ARCHTEST=(QLM)</code> which provides Engleâ€™s Lagrange Multiplier Test for ARCH Disturbances and the Portmanteau Q Test.  The statistics SAS provided for Engleâ€™s Lagrange Multiplier Test were:</p>

<pre><code>Order   LM      p-value
1       0.018   0.893
2       0.079   0.961
3       0.881   0.830
4       1.088   0.896
5       1.105   0.954
6       4.114   0.661
7       4.179   0.759
8       4.182   0.840
9       4.528   0.873
10      4.528   0.920
11      5.565   0.901
12      6.318   0.899
</code></pre>

<p>I took the residuals of this model and used the <code>ArchTest</code> function of the <code>FinTS</code> package of <code>R</code> ... <code>library(FinTS)</code>.</p>

<pre><code># A function to do ArchTest at multiple lags where 'data' are the residuals of
# the model.
library(FinTS) 
arch.m1 = matrix(NaN,12,3)
for(i in 1:12)  {s = ArchTest(data,lags=i)
                 arch.m1[i,1] = s$statistic
                 arch.m1[i,2] = s$parameter
                 arch.m1[i,3] = s$p.value}
</code></pre>

<p>The test results from the <code>ArchTest</code> function of the <code>FinTS</code> package are very different:</p>

<pre><code>Order   LM      p-value
1       0.014   0.907
2       0.092   0.955
3       0.835   0.841
4       2.194   0.700
5       3.189   0.671
6       13.013  0.043
7       19.157  0.008
8       23.161  0.003
9       18.780  0.027
10      18.467  0.048
11      22.565  0.020
12      23.828  0.021
</code></pre>

<p>I thought the SAS <code>ARCHTEST=(QLM)</code> option and the <code>ArchTest</code> function of the <code>FinTS</code> package were performing the same test but apparently not.  Does anyone know what the difference is between the tests in SAS and <code>FinTS</code>?</p>

<p><strong>Note</strong>, the data used by Tsay (2005) in the example on page 102 is available <a href=""http://faculty.chicagobooth.edu/ruey.tsay/teaching/fts2/m-intc7303.txt"" rel=""nofollow"">here</a>.</p>

<p>Using the following <code>R</code> code:</p>

<pre><code>library(FinTS)
intc=read.table(""http://faculty.chicagobooth.edu/ruey.tsay/teaching/fts2/m-intc7303.txt"", quote=""\"""")
ArchTest(intc$V2,lags=12)
</code></pre>

<p>I find:</p>

<p>$\chi^2$= 48.0425, df = 12, <em>p</em>-value = 3.073e-06</p>

<p>However, Tsay (2005:102) reports:</p>

<p>$\chi^2$= 43.5041, df = 12, <em>p</em>-value = 0.0000</p>

<p>If I use:</p>

<pre><code>ArchTest(log(1+intc$V2),lags=12)
</code></pre>

<p>Then:</p>

<p>$\chi^2$= 43.5041, df = 12, <em>p</em>-value = 0.0000</p>

<p>However, this doesn't explain the differences observed between SAS and <code>FinTS</code>.</p>

<p><strong>Thanks for your reply @mpiktas.</strong></p>

<p>The statistic calculated by the <code>FinTS library</code> using the <code>ArchTest function</code> is based on the second statistic provided by @mpiktas:</p>

<p>$$TR^2=T\left(1-\frac{SSR_1}{SSR_0}\right)$$</p>

<pre><code>intc=read.table(""http://faculty.chicagobooth.edu/ruey.tsay/teaching/fts2/m-intc7303.txt"",quote=""\"""")
intc.ln=log(1+intc$V2)
lag=12
mat=embed(intc.ln^2,lag+1)
m=summary(lm(mat[,1]~mat[,-1]))
SSR1=sum(residuals(lm(mat[,1]~mat[,-1]))^2)
SSR0=sum(residuals(lm(mat[,1]~1))^2)
stat=length(mat[,1])*(1-(SSR1/SSR0))
</code></pre>

<p>which gives $\chi^2$ = 43.5041.</p>

<p>If I apply the first statistic provided by @mpiktas and shown on page 102 of Tsay (2005):</p>

<p>$$F=\frac{(SSR_0-SSR_1)/m}{SSR_1/(T-2m-1)}$$</p>

<pre><code>intc=read.table(""http://faculty.chicagobooth.edu/ruey.tsay/teaching/fts2/m-intc7303.txt"",quote=""\"""")
intc.ln=log(1+intc$V2)
lag=12
mat=embed(intc.ln^2,lag+1)
m=summary(lm(mat[,1]~mat[,-1]))
SSR1=sum(residuals(lm(mat[,1]~mat[,-1]))^2)
SSR0=sum(residuals(lm(mat[,1]~1))^2)
stat=((SSR0-SSR1)/lag)/(SSR1/(length(mat[,1])-(2*lag)-1))
</code></pre>

<p>This version of the statistic gives $\chi^2$ = 3.837.</p>

<p>These are very different answers?  I have provided the residuals for the AR(5) model from SAS below:</p>

<pre><code>c(0.14167,0.14019,-0.42499,0.0222,0.04388,-0.03157,0.00897, 
-0.04567,0.29171,-0.06735,0.05235,-0.04062,-0.18694, 
-0.2407,0.03766,-0.15011,0.11023,-0.30812,-0.02399, 
0.13434,0.05836,0.09368,-0.01832,-0.15669,-0.00127, 
0.19431,0.10366,-0.09682,-0.04597,0.07162,0.16411,-0.02181, 
-0.00459,-0.16325,-0.09615,-0.04581,0.10441,0.12845,-0.0398,
-0.03349,-0.01575,0.06923,-0.05006,-0.05827, 
-0.0317,0.08914,0.07323,0.06519,0.16141,0.0537,0.08213,
0.01073,-0.06628,0.01297,0.00622,-0.10171,0.05598,-0.05615, 
0.10322,-0.04991,-0.01223,-0.04975,-0.06043,0.01794, 
-0.00071,0.05425,-0.05462,0.01659,0.08656,0.02482,-0.04214, 
0.01221)
</code></pre>

<p>For lag 5, SAS gives $\chi^2$ = 1.105, <code>FinTS</code> gives $\chi^2$ = 3.189, and the F-statistic version gives $\chi^2$ = 0.560.</p>

<p><strong>References:</strong></p>

<p>Engle, R. F.  1982.  Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation.  Econometrica 50:987-1007.</p>

<p>Tsay, R. S.  2005.  Analysis of financial time series.  Second edition.  John Wiley &amp; Sons, Inc., Hoboken, New Jersey, USA.</p>
"
"0.0599760143904067","0.0609710760849692"," 35990","<p>We asked 60 people to list as many restaurant franchises in Atlanta as they could.  The overall list included over 70 restaurants, but we eliminated those that were mentioned by fewer than 10% of the people, leaving us with 45.  For these 45, we calculated the proportion of informants who listed the franchise, and we're interested in modeling this proportion as a function of the franchises's (log-transformed) advertising budget and years since becoming a franchise.</p>

<p>So I wrote this code:</p>

<pre><code>model &lt;- glm ( cbind (listed, 55-listed) ~ log.budget + years, family = binomial, data = list.45)
</code></pre>

<p>As predicted, both variables exhibit strong, significant effects.</p>

<p>But even though I know that proportional data should never be modeled with OLS regression, I subsequently wrote this code:</p>

<pre><code>model.lm &lt;- lm ( proportion.55 ~ log.budget + years, data = list.45)
</code></pre>

<p>In this case, ""budget"" is still a significant predictor, but ""years"" is relatively weak and not significant.</p>

<p>It makes me worried that the confidence in the estimates is artificially inflated by the aggregation.  Doesn't the binomial glm essentially vectorize the data such that the model is based on 45 * 55 = 2,475 rows?  Is that appropriate given that there are really only 45 restaurants and 55 informants?  Would this call for mixed-effects modeling?</p>
"
"0.0479808115123254","0.0609710760849692"," 37564","<p>I'm trying to generate an R function that keeps relevant variables based on their absolute t-value (or p, whichever is easier in code).</p>

<p>Basically what I want is to run one regression (1), retain all variables that are significant (based on the t-value, or p). Then run another regression (2), retain all variables that are significant (based on t-value, or p). Then take all retained variables from the regressions (1 &amp; 2) above and combine them in a final regression (3).</p>

<p>Currently I am struggling with the step of retaining variables based on their t-value, I can extract the t-values of the variables but don't know how to code that they should be ""retained"" and kept for the later regression.</p>

<p>What I have currently (example edited for better understanding):</p>

<pre><code>summaryI1 &lt;- summary(lm(y~x1+x2+x3+x4)) #x1-x4 are the vars included in regression 1
(coefI1 &lt;- coef(summaryI1))
tI1 &lt;- coefI1[, ""t value""] #extracts the t-values for all the x's

cutoff &lt;- which(tI2&gt;cut, arr.in=TRUE) #checks that t-stats of vars are above the cutoff level
</code></pre>

<p>Suppose x1 and x2 are significant and I would like to keep them ""aside"" for the next regression. How would I code that r should create a new matrix of x1 and x2 based on x1 and x2 having t-values higher than my cutoff?</p>

<p>I'm not that experienced in r programming yet but learning.
Can anyone help?</p>
"
"0.0709645772411954","0.072141950116023"," 37785","<p>I come from an SPSS background and am attempting to move to <code>R</code> for it's superior flexibility and data manipulation abilities. I have some concerns however as to whether the <code>lm()</code> is really using partial correlations.</p>

<p>I'm basically trying to run a linear regression, using something similar to the ""enter"" setting in SPSS, which essentially builds the model one variable at a time, reporting the change in $R^2$ with each additional variable. This allows you to determine how much predictive power each variable adds to the model.</p>

<p>When I run the same analysis in <code>R</code> however, I don't get any information on the $R^2$ contributed by individual variables, and I'm not even sure that it's using partial corrrelations to calculate the p-values it's reporting!</p>

<p>My code follows:</p>

<pre><code>summary(m1 &lt;- lm(totalprop ~ cos(Angle) + Alignment + colour + 
  Angle*Alignment, dataset))
</code></pre>

<p>My questions:</p>

<ol>
<li>Does R use partial correlations to determine reported p-values from <code>lm()</code>?</li>
<li>How can I make <code>R</code> report change in $R^2$ with each additional variable.</li>
<li>How can I make <code>R</code> act like SPSS in calulating the model piece by piece? Is this possible without running multiple iterations of the lm() function? If not, how does one control for the effects of covariates in R?</li>
</ol>
"
"NaN","NaN"," 37830","<p>I've been experimenting with the <code>rfe</code> function in the <code>caret</code> package to do logistic regression with feature selection. I used the <code>lmFuncs</code> functions with the following <code>rfeContol</code> :</p>

<p><code>ctrl &lt;- rfeControl(functions = lmFuncs,
                     method = 'cv',
                     rerank=TRUE,
                     saveDetails=TRUE,
                     verbose = TRUE,
                     returnResamp = ""all"",
                     number=100)</code></p>

<p>Below is the structure of the <code>rfe</code> call:</p>

<p><code>fit.rfe=rfe(df.preds,df.depend, metric='RMSE',sizes=c(5,10,15,20), rfeControl=ctrl)</code></p>

<p><code>df.preds</code> is a data frame of inputs to the model. <code>df.depend</code> is a vector of 1 or 0 corresponding to each row in <code>df.preds</code> to indicate response.</p>

<p>The resulting model accessed in from the <code>fit</code> object in the <code>rfe</code> object is of class <code>lm</code> and produces predicted values of less than zero and greater than 1 when I use the following code with the <code>predict</code> function:</p>

<p><code>predict(fit.rfe$fit,df,type='response')</code></p>

<p>Given I'm expecting this to be a logistic, all predicted values should greater than zero and less than one. </p>

<p>Any help will be appreciated.</p>
"
"0.0379321620905441","0.0385614943639849"," 37973","<p>I am fitting a simple linear regression model with 4 predictors:</p>

<p><code>lm(Outcome ~ Predictor1 + Predictor2 + Predictor3 + Predictor4, data=dat.s)</code></p>

<p>I'm finding that the model predictions are consistently off as shown in this graph:
<img src=""http://i.stack.imgur.com/CNLJz.png"" alt=""scatterplot of predictions and residuals""></p>

<p>The model clearly overestimates the low values and underestimates the high values, but the miss-estimation is very linear -- it seems like the model should be able to just adjust the slope and fit the data better. Why is that not happening? In case it helps, here are scatterplots of the the Outcome against each of the four Predictors:
<img src=""http://i.stack.imgur.com/uc55e.png"" alt=""enter image description here""></p>

<p>Using the <code>car</code> package <code>outlierTest</code> function did not identify any outliers.</p>
"
"0.026822089039291","0"," 38541","<p>I used the functions from this <a href=""http://www.math.mcmaster.ca/peter/s4f03/s4f03_0607/rochl.html"" rel=""nofollow"">link</a> for creating ROC curve for logistic regression model. Since the object produced by <code>glmer</code> in <code>lme4</code> package is a S4 object (as far as I know) and the function from the link cannot handle it.</p>

<p>I wonder if there are similar functions for creating ROC curve for multi-level logistic regression model in R.</p>
"
"0.0379321620905441","0.0385614943639849"," 39322","<p>I need to explore the determinants of wheat yield using multiple regression. There are 120 observations. Do I need to use ANOVA? I am using R studio. My results need to include a consideration of more than one functional form, one or more interaction terms for a select set of variables, and the use of dummy variables.</p>

<p>Variable Descriptions:</p>

<pre><code>yld wheat yield in kg/ha  
R Growing season rainfall (mm)  
N nitrogen fertilization rate (kg/ha)  
PrevCereal 1 if precious crop was a cereal, 0 otherwise  
RegionCode Location code (0/1)  
</code></pre>
"
"0.113796486271632","0.115684483091955"," 40453","<p>Perpendicular offset least square fitting has a lot of advantages compared to the native least square fitting scheme. The following figure illustrates the difference between there, and for a more detailed comparison of these two methods, we refer to <a href=""http://mathworld.wolfram.com/LeastSquaresFittingPerpendicularOffsets.html"" rel=""nofollow"">here</a>. </p>

<p><img src=""http://i.stack.imgur.com/Aue8i.gif"" alt=""enter image description here""></p>

<p>Perpendicular offset least square fitting, however, is not robust to outliers( points that are not supposed to be used for model estimation). Therefore, I am now considering to use a weighted perpendicular offset least square regression method. The method has two steps:</p>

<ol>
<li>Calculate the weighting factor for each points that are going to be used for line estimation;</li>
<li>Perform perpendicular offset in a weighted least square regression scheme. </li>
</ol>

<p>For the time being, my biggest problem comes from step 2. Suppose the weighting factors are given, how can I get the formula to estimate the parameters of the line? Many thanks!</p>

<p><strong>EDIT:</strong></p>

<p>Based on the kind suggestion of @MvG I have implemented the algorithm in MATLAB:</p>

<pre><code>function  line =  estimate_line_ver_weighted(pt_x, pt_y,w);
% pt_x  x coordinate
% pt_y  y coordinate
% w     weighting factor


pt_x = pt_x(:);
pt_y = pt_y(:);
w    = w(:);


% step 1: calculate n
n = sum(w(:));

% step 2: calculate weighted coordinates 
y_square = pt_y(:).*pt_y(:);
x_square = pt_x(:).*pt_x(:);
x_square_weighted = x_square.*w;  
y_square_weighted = y_square.*w;  
x_weighted        = pt_x.*w;
y_weighted        = pt_y.*w;

% step 3: calculate the formula
B_upleft = sum(y_square_weighted)-sum(y_weighted).^2/n;
B_upright = sum(x_square_weighted)-sum(x_weighted).^2/n;
B_down = sum(x_weighted(:))*sum(y_weighted(:))/n-sum(x_weighted.*pt_y);
B = 0.5*(B_upleft-B_upright)/B_down;

% step 4: calculate b
if B&lt;0
    b       = -B+sqrt(B.^2+1);
else
    b       = -B-sqrt(B.^2+1);
end

% Step 5: calculate a
a = (sum(y_weighted)-b*sum(x_weighted))/n;

% Step 6: the model is y = a + bx, and now we transform the model to 
% a*x + b*y + c = 0;
c_ = a;
a_ = b;
b_ = -1;

line = [a_ b_ c_];
</code></pre>

<p>The result is as good as we can expect, which is illustrated in the following script:</p>

<pre><code>%% Procedure 1: given the data
pt_x = [   692   692   693   692   693   693   750];
pt_y = [ 919         971        1022        1074        1126        1230        1289];

% Procedure 2: draw the point 
 close all; figure; plot(pt_x,pt_y,'b*');

% Procedure 3: estimate the line based on the weighted vertical offset
% least square method.
 weighting = ones(length(pt_x),1);
 weighting(end) = 0.01;  % we give the last point a low weighting because obvously it is an outlier
 myline =    estimate_line_ver_weighted(pt_x,pt_y,weighting); 
 a = myline(1); b = myline(2); c= myline(3);

 % Procedure 4: draw the line
 x_range = [min(pt_x):0.1:max(pt_x)];
 y_range = [min(pt_y):0.1:max(pt_y)];
 if length(x_range)&gt;length(y_range)
        x_range_corrspond = -(a*x_range+c)/b;
        hold on; plot(x_range,x_range_corrspond,'r');
 else
        y_range_correspond = -(b*y_range+c)/a;
        hold on; plot(y_range_correspond,y_range,'r');
 end
</code></pre>

<p>The following figure corresponds to the above script:
<img src=""http://i.stack.imgur.com/IIz2k.png"" alt=""enter image description here"">.</p>
"
"0.0967084173462244","0.0907503748778111"," 40499","<p>When using the <code>step.plr()</code> function in the <a href=""http://cran.r-project.org/web/packages/stepPlr/index.html"" rel=""nofollow"">stepPlr</a> package, if my predictors are factors, do I need to encode my predictors as dummy variables manually before passing it to the function? I do know that I can specify ""level"", but how  the ""level"" parameter works is confusing to me. 
My understanding is that I need to tell <code>step.plr()</code> explicitly which factors should be encoded as dummy variables and thus leaving one factor out intentionally. </p>

<p>Let's consider a simple example. Suppose I have 1 categorical predicator with 4 levels and binary response. Normally, if I use <code>glm()</code> to fit a logistic regression model, <code>glm()</code> would automatically convert the categorical predicator into 3 dummy variables. Now in <code>stepPlr()</code>, do I specify the ""level"" parameter for that predictor with 4 levels or 3 levels? The ""Help"" section is vague, and says: </p>

<blockquote>
  <p>If the j-th column of x is discrete, level[[ j ]] is the set of levels for the categorical factor.</p>
</blockquote>

<p>Does it mean I should tell <code>step.plr()</code> about all 4 levels, or I should make an intelligent decision myself and tell <code>step.plr()</code> to use only 3 levels? </p>

<p>==============UPDATE (16 Oct 2012)=============</p>

<p>The following example will demonstrate what is the problem with <code>step.plr()</code>'s automatic dummy variable encoding. It is a slight modification of the code in the function's help section. 
     set.seed(100)</p>

<pre><code>n &lt;- 100
p &lt;- 3
z &lt;- matrix(sample(seq(3),n*p,replace=TRUE),nrow=n)
x &lt;- data.frame(x1=factor(z[ ,1]),x2=factor(z[ ,2]),
                x3=factor(sample(seq(3), n, replace=TRUE, prob=c(0.2, 0.5, 0.3))),
                x4=factor(sample(seq(3), n, replace=TRUE, prob=c(0.1, 0.3, 0.6))))
y &lt;- sample(c(0,1),n,replace=TRUE)
fit &lt;- step.plr(x,y, cp=""aic"")
summary(fit)
</code></pre>

<p>And here's an excerpt of the result:</p>

<pre><code>Call:
plr(x = ix0, y = y, weights = weights, offset.subset = offset.subset, 
    offset.coefficients = offset.coefficients, lambda = lambda, 
    cp = cp)

Coefficients:
      Estimate Std.Error z value Pr(&gt;|z|)
Intercept  0.91386   5.04780   0.181    0.856
x4.1       1.33787   4.61089   0.290    0.772
x4.2      -1.70462   4.91240  -0.347    0.729
x4.3       0.36675   3.18857   0.115    0.908
x3.1:x4.1  7.04901  14.35112   0.491    0.623
x3.1:x4.2 -5.50973  15.53674  -0.355    0.723
x3.1:x4.3 -0.50012   7.95651  -0.063    0.950
</code></pre>

<p>You can see that all levels, that is, (1,2,3), are used to fit the model. But normally you only need two dummy variables to encode a predictor with 3 levels.
On the other hand, if you use <code>glm()</code>: </p>

<pre><code>glm(y~.^2, data=x, family=binomial)
</code></pre>

<p>you will get the correct dummy variable encoding.</p>
"
"0.126245866529232","0.128340410942643"," 40670","<p>I am familiar with linear regression models but the random section of linear mixed models just melts my mind. I did find an excellent guide that could have helped me but the languageR package is not compatible with newer versions of lme4 so I've been unable to implement it in my work.</p>

<p>For me the fixed effects are very understandable (below lactation and a higher yr2 value both contribute to a higher weight but the lactation effect is more consistent which results in a higher t-value).</p>

<p>The first problem is to understand what I am actually putting in. To a certain extent I understand that <code>(1|P$grupp)</code> means that the mixed model add to the base line (intercept) while <code>(P$grupp|P$lweek)</code> mean that belong to a group is expected to affect the average weight increase (or decrease) while <code>P$lweek</code> adds to the baseline value. But why does all tutorials seem to favor write ups like <code>(1+P$fgrupp|P$lweek)</code> rather than <code>(P$grupp|P$lweek)</code>?</p>

<p>Now on to the actual output (see below for full output). I've used the following models (sorry for the Swenglish but the sample is the weight of cows <code>P$vikt</code> is the weight at certain time points and <code>P$lweek</code> is the time since a calf was born, <code>P$fgrupp</code> is a factor telling if the cow belongs to feed group 1,2 or 3):</p>

<pre><code>Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 | P$fgrupp)            #$
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 + P$fgrupp | P$lweek)
</code></pre>

<p>Where I understand it as the first one being rather useless (essentially it tells us that the average weight of the cows isn't affected of which feed group it belongs too). This is reflected by fgrupp having variance 0 in the first formula below. The second is more interesting as the <code>P$fgrupp|P$lweek</code> as I understand it should show if different feed groups affect the weight increase of cows as function of the time. But I am really not competent enough to understand the input. I understand that variance somehow mean that belonging to group2 or 3 explain some of the variation in the growth curves but I really don't understand how to interpret this.</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr        
 P$lweek  (Intercept)   13.068  3.6149                                     #$
          P$fgrupp2     77.230  8.7881  1.000                              #$
          P$fgrupp3     81.188  9.0104  1.000 1.000                        #$
 Residual             4031.831 63.4967              
Number of obs: 1048, groups: P$lweek, 84
</code></pre>

<p><strong>Full output</strong></p>

<pre><code>#First model#
Linear mixed model fit by REML 
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 | P$fgrupp)            #$
   AIC   BIC logLik deviance REMLdev
 11703 11732  -5845    11698   11691
Random effects:
 Groups   Name        Variance Std.Dev.
 P$fgrupp (Intercept)    0.0    0.000                                      #$
 Residual             4139.9   64.342  
Number of obs: 1048, groups: P$fgrupp, 3                                   #$

Fixed effects:
            Estimate Std. Error t value
(Intercept)  509.593      4.683  108.82
P$lweek        1.028      0.105    9.79                                    #$
P$laktation   22.789      1.454   15.67                                    #$
P$yr2         35.294      4.093    8.62                                    #$

Correlation of Fixed Effects:
            (Intr) P$lwek P$lktt                                           #$
P$lweek     -0.560                                                         #$
P$laktation -0.636  0.030                                                  #$
P$yr2       -0.240 -0.034 -0.141                                           #$


#Second model#

Linear mixed model fit by REML 
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 + P$fgrupp | P$lweek)  #$
       AIC   BIC logLik deviance REMLdev
     11707 11761  -5842    11693   11685
    Random effects:
     Groups   Name        Variance Std.Dev. Corr        
     P$lweek  (Intercept)   13.068  3.6149                                     #$
              P$fgrupp2     77.230  8.7881  1.000                              #$
              P$fgrupp3     81.188  9.0104  1.000 1.000                        #$
     Residual             4031.831 63.4967              
    Number of obs: 1048, groups: P$lweek, 84                                   #$

Fixed effects:
            Estimate Std. Error t value
(Intercept) 508.2291     5.1770   98.17
P$lweek       1.0662     0.1192    8.94                                    #$
P$laktation  22.6525     1.4459   15.67                                    #$
P$yr2        35.6343     4.0848    8.72                                    #$

Correlation of Fixed Effects:
            (Intr) P$lwek P$lktt                                           #$$
P$lweek     -0.627                                                         #$
P$laktation -0.570  0.025                                                  #$
P$yr2       -0.224 -0.018 -0.136                                           #$
</code></pre>
"
"0.0464572209811883","0.0472279924554862"," 40739","<p>The <code>bild</code> package appears to be an excellent package for serial binary responses.  But it is for discrete time.  I would like to specify a smooth function of time for the odds ratio connection of the current response Y with binary responses measured at earlier times, or at least a first-order Markov version of this.  I believe this is called alternating logistic regression.  Does anyone know of an R package that handles continuous time, i.e., measurement times can be at any follow-up time?  I don't need random effects in the model. </p>
"
"0.0657004319817604","0.0667904674542028"," 41040","<p>When I fit a regression tree model in R, I can somehow look at the plotted tree to figure out the interactions of the predictors. However, when I fit a random forest model, I cannot plot a tree because there are too many of them. I can somehow find the importance order of the predictors (using varImpPlot function), but I do not know how to detect the interactions from the model. By the way, the importance plot only gives the importance order of the predictors, how can I figure out exactly how many of them should be included for further investigation. Thank you.</p>
"
"0.0967084173462244","0.0983129061176287"," 41390","<p>I am looking for a test similar to a 2-way ANOVA that would work on a binary response variable. My response variable is survival of plant seedlings (alive or dead).  My explanatory variables are Treatment (3 treatment groups) and Site (3 sites).  </p>

<p>First, I would like to know whether Treatment, Site and their interaction have a significant effect on survival.  Second, if either Treatment or Site is significant, I would like to test all pairs of treatment groups or sites to know which pairs of levels are significantly different, as I would normally do with an ANOVA.</p>

<p>I have considered several options:</p>

<ol>
<li><p>Transform the response variable, for example through an arcsin transformation, and then perform an ANOVA. This does not work on my data because at one of the sites I measure 100% survival.  Therefore there is 0 variability at this site and no transformation will change that.</p></li>
<li><p>Logistic regression with Treatment and Site recoded as dummy variables.  The results do not seem to give me a test of significance of Treatment, Site and interaction term -  Instead, I get the relative importance of each treatment group and each site separately.  Furthermore, it seems that I cannot test all the pairs of treatment groups or sites, I can only compare one ""baseline"" or ""default"" group to each of the two remaining groups.</p></li>
<li><p>Chi-square test on each explanatory variable separately.  This has the obvious drawback of not being able to test the interaction term.  Also I suspect that I am omitting important information if I am comparing survival across the 3 treatment groups without taking into account that this survival data is grouped in 3 different sites.  Does this bias the results?</p></li>
</ol>

<p>Can anyone recommend a different test or what the best approach would be in my case?</p>

<p><strong>UPDATE:</strong> Logistic regression can in fact give a test of significance of each independent variable.  In R, I discovered I can use glm to contruct a model and then the anova function to extract p-values for each IV:</p>

<pre><code>mymodel &lt;- glm(Survival ~ Treatment*Site, data=survivaldata, family=""binomial"")
anova(mymodel, test=""Chisq"")
</code></pre>
"
"0.0379321620905441","0.0385614943639849"," 41540","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/41697/prediction-results-for-two-response-variable-from-random-forest"">Prediction results for two response variable from random forest</a>  </p>
</blockquote>



<p>I use <a href=""http://cran.r-project.org/web/packages/randomForest/index.html"" rel=""nofollow"">randomForest</a> R package to do regression <code>rf = randomForest(A + B ~ C + D, data, ..)</code>. I want to know what are prediction values for A and B, and use <code>predict(rf)</code>. The output is an aggregate value instead of two values for A and B. Do you have any suggestion to implement a <code>predict</code> function to generate two predictions? </p>
"
"0.0464572209811883","0.0472279924554862"," 41577","<p>I am trying to get some bootstrap CIs for coefficients obtained by robust regression. I have influential values and thus switched to <code>rlm</code>.</p>

<p>The data are clustered and within clusters, the variance of my DV (""DurchlÃ¤ssigkeit"") = 0.</p>

<p>Is this all sensible for my clustered data?</p>

<pre><code>&gt; dput(d)
structure(list(PorenflÃ¤che = c(4990L, 7002L, 7558L, 7352L, 7943L,
7979L, 9333L, 8209L, 8393L, 6425L, 9364L, 8624L, 10651L, 8868L,
9417L, 8874L, 10962L, 10743L, 11878L, 9867L, 7838L, 11876L, 12212L,
8233L, 6360L, 4193L, 7416L, 5246L, 6509L, 4895L, 6775L, 7894L,
5980L, 5318L, 7392L, 7894L, 3469L, 1468L, 3524L, 5267L, 5048L,
1016L, 5605L, 8793L, 3475L, 1651L, 5514L, 9718L), P.Perimeter = c(2791.9,
3892.6, 3930.66, 3869.32, 3948.54, 4010.15, 4345.75, 4344.75,
3682.04, 3098.65, 4480.05, 3986.24, 4036.54, 3518.04, 3999.37,
3629.07, 4608.66, 4787.62, 4864.22, 4479.41, 3428.74, 4353.14,
4697.65, 3518.44, 1977.39, 1379.35, 1916.24, 1585.42, 1851.21,
1239.66, 1728.14, 1461.06, 1426.76, 990.388, 1350.76, 1461.06,
1376.7, 476.322, 1189.46, 1644.96, 941.543, 308.642, 1145.69,
2280.49, 1174.11, 597.808, 1455.88, 1485.58), P.Form = c(0.0903296,
0.148622, 0.183312, 0.117063, 0.122417, 0.167045, 0.189651, 0.164127,
0.203654, 0.162394, 0.150944, 0.148141, 0.228595, 0.231623, 0.172567,
0.153481, 0.204314, 0.262727, 0.200071, 0.14481, 0.113852, 0.291029,
0.240077, 0.161865, 0.280887, 0.179455, 0.191802, 0.133083, 0.225214,
0.341273, 0.311646, 0.276016, 0.197653, 0.326635, 0.154192, 0.276016,
0.176969, 0.438712, 0.163586, 0.253832, 0.328641, 0.230081, 0.464125,
0.420477, 0.200744, 0.262651, 0.182453, 0.200447), DurchlÃ¤ssigkeit = c(6.3,
6.3, 6.3, 6.3, 17.1, 17.1, 17.1, 17.1, 119, 119, 119, 119, 82.4,
82.4, 82.4, 82.4, 58.6, 58.6, 58.6, 58.6, 142, 142, 142, 142,
740, 740, 740, 740, 890, 890, 890, 890, 950, 950, 950, 950, 100,
100, 100, 100, 1300, 1300, 1300, 1300, 580, 580, 580, 580), Gebiete = structure(c(1L,
1L, 1L, 1L, 2L, 2L, 2L, 2L, 6L, 6L, 6L, 6L, 4L, 4L, 4L, 4L, 3L,
3L, 3L, 3L, 7L, 7L, 7L, 7L, 9L, 9L, 9L, 9L, 10L, 10L, 10L, 10L,
11L, 11L, 11L, 11L, 5L, 5L, 5L, 5L, 12L, 12L, 12L, 12L, 8L, 8L,
8L, 8L), .Label = c(""6.3"", ""17.1"", ""58.6"", ""82.4"", ""100"", ""119"",
""142"", ""580"", ""740"", ""890"", ""950"", ""1300""), class = ""factor"")), .Names = c(""PorenflÃ¤che"",
""P.Perimeter"", ""P.Form"", ""DurchlÃ¤ssigkeit"", ""Gebiete""), row.names = c(""1"",
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"",
""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"", ""23"", ""24"",
""25"", ""26"", ""27"", ""28"", ""29"", ""30"", ""31"", ""32"", ""33"", ""34"", ""35"",
""36"", ""37"", ""38"", ""39"", ""40"", ""41"", ""42"", ""43"", ""44"", ""45"", ""46"",
""47"", ""48""), class = ""data.frame"")

## do robust regression and bootstrap the coefficients, allowing for clustered data
## by putting ""Gebiet"" as strata argument (?),
## dv variation within clusters/Gebiet = 0!
bs &lt;- function(formula, data, indices) {
  d &lt;- data[indices, ] # allows boot to select sample
  fit &lt;- rlm(formula, data = d)
  return(coef(fit))
}

results &lt;- boot(data = d, statistic = bs, strata = d$Gebiete,
                R = 199, formula = DurchlÃ¤ssigkeit ~ P.Perimeter + P.Form)

# get 99% confidence intervals
boot.ci(results, type=""bca"", index=1, conf = .99) # intercept
boot.ci(results, type=""bca"", index=2, conf = .99) # P.Perimeter
boot.ci(results, type=""bca"", index=3, conf = .99) # P.Form
</code></pre>
"
"0.0379321620905441","0.0192807471819925"," 41660","<p>I'm modeling a set of outcome data the depends on two parameters:</p>

<ol>
<li>time, T</li>
<li>-100 &lt; A &lt; 100</li>
</ol>

<p>I've done logistic regression using R with the command:</p>

<pre><code>model &lt;- glm(Outcome ~ A + T, family = ""binomial"", data = myData)
</code></pre>

<p>My expectation (the only thing that makes sense) is that when A &lt; 0, the fit probability should be an increasing function of time approaching 0.5, while when A > 0 it should be a decreasing function of time approaching 0.5.</p>

<p>However, the fit I get is that A &lt; 0, A > 0, and A = 0 all are increasing functions of time.  They in fact appear to be the same curve just shifted (ie same ""shape"").</p>

<p>What am I doing incorrectly?  Any suggestions?</p>
"
"0.0464572209811883","0.0472279924554862"," 41697","<p>Would any R expert explain the predic function in the randomforest package to me?</p>

<p>I want to get two prediction results for numberic response variable A and B seperately from following regression</p>

<pre><code>result &lt;-randomforest(A + B ~ C + D + E, data = dataset)
predict(result)
</code></pre>

<p>I can get one prediction result. But prediction is neither A nor B. I can get the prediction results for both A and B from Mvpart and party package. </p>

<p>Thanks in advance!</p>
"
"0","0"," 43551","<p>I am running a multinomial logistic regression using the <code>mlogit</code> package and <code>mlogit</code> function in R.  Now I need to check for outliers for the model.</p>

<p>Is there any approach or function in R for testing outliers in an <code>mlogit</code> model?</p>
"
"0.0599760143904067","0.0487768608679754"," 43664","<p>I would like to use <code>lme4</code> to fit a mixed effects regression and <code>multcomp</code> to compute the pairwise comparisons. I have a complex data set with multiple continuous and categorical predictors, but my question can be demonstrated using the built-in <code>ChickWeight</code> data set as an example:</p>

<pre><code>m &lt;- lmer(weight ~ Time * Diet + (1 | Chick), data=ChickWeight, REML=F)
</code></pre>

<p><code>Time</code> is continuous and <code>Diet</code> is categorical (4 levels) and there are multiple Chicks per diet. All the chicks started out at about the same weight, but their diets (may) affect their growth rate, so the <code>Diet</code> intercepts should be (more or less) the same, but the slopes might be different. I can get the pairwise comparisons for the intercept effect of <code>Diet</code> like this:</p>

<pre><code>summary(glht(m, linfct=mcp(Diet = ""Tukey"")))
</code></pre>

<p>and, indeed, they are not significantly different, but how can I do the analogous test for the <code>Time:Diet</code> effect? Just putting the interaction term into <code>mcp</code> produces an error:</p>

<pre><code>summary(glht(m, linfct=mcp('Time:Diet' = ""Tukey"")))
Error in summary(glht(m, linfct = mcp(`Time:Diet` = ""Tukey""))) : 
  error in evaluating the argument 'object' in selecting a method for function
 'summary': Error in mcp2matrix(model, linfct = linfct) : 
Variable(s) â€˜Time:Dietâ€™ have been specified in â€˜linfctâ€™ but cannot be found in â€˜modelâ€™! 
</code></pre>
"
"0.026822089039291","0.0272670941574606"," 43733","<p>The R documentation for either does not shed much light. All that I can get from <a href=""https://stat.ethz.ch/pipermail/r-help/2007-December/147855.html"">this link</a> is that using either one should be fine. What I do not get is why they are not equal.</p>

<p>Fact: The stepwise regression function in R, <code>step()</code> uses <code>extractAIC()</code>.</p>

<p>Interestingly, running a <code>lm()</code> model and a <code>glm()</code> 'null' model (only the intercept) on the 'mtcars' data set of R gives different results for <code>AIC</code> and <code>extractAIC()</code>.</p>

<pre><code>&gt; null.glm = glm(mtcars$mpg~1)
&gt; null.lm = lm(mtcars$mpg~1)

&gt; AIC(null.glm)
[1] 208.7555
&gt; AIC(null.lm)
[1] 208.7555
&gt; extractAIC(null.glm)
[1]   1.0000 208.7555
&gt; extractAIC(null.lm)
[1]   1.0000 115.9434
</code></pre>

<p>It is weird, given that both the models above are the same, and <code>AIC()</code> gives the same results for both.</p>

<p>Can anyone throw some light on the issue?</p>
"
"0.053644178078582","0.0545341883149212"," 43747","<p>Using the method in <a href=""http://stackoverflow.com/questions/1395147/best-way-to-plot-interaction-effects-from-a-linear-model"">this post</a>, I have made a plot to visualize the interaction between two predictor variables using the effects package in <a href=""/questions/tagged/r"" class=""post-tag"" title=""show questions tagged 'r'"" rel=""tag"">r</a>, but I'm not really sure what I am looking at. </p>

<p>Tide heights and rain averages are continuous. 8 bins were the maximum the function would allow me to use. The following is the call to <code>effect</code> producing this plot:</p>

<pre><code> R &gt; plot(effect(term=""rain.avg2:tide.avg"",mod=bkrain9.lm,default.levels=8),
          main="""", xlab=""Precipitation - 24hr average (cm)"",
          ylab=expression(""TCB Concentration - CFU*100m""*L^-1),multiline=TRUE)
</code></pre>

<p><strong>Plot updated.</strong> Could somebody explain the purpose of this plotting feature in context to predictor interactions?</p>

<p><img src=""http://i.stack.imgur.com/neZql.jpg"" alt=""P""></p>

<p>Background:
For a class project, I have created a linear regression model to evaluate the effects of the interaction between two predictor variables (tide height and precipitation) on bacteria concentrations.</p>

<p>Thermo-tolerant coliform bacteria concentrations were sampled at 5 sites in a day, where a sample time was recorded at sampling completion. I took an average of these for roughly 20 days, and calculated an associated 24hr average of precipitation before sampling completion, and a 50 minute (the sampling duration) average of tide height before sampling completion.</p>
"
"0.0657004319817604","0.0667904674542028"," 44281","<p>In R, is there a predefined function that will give me the log hazard ratio and its standard error for a black male (as shown in the example below) given the output of coxph regression?</p>

<pre><code>library(survival)
library(KMsurv)

#Kidney transplant data from Klein and Moeshberger. Massage data to make
#results look like those in book
data(kidtran)
data2 &lt;- kidtran
data2$Gender &lt;- ""male""
    data2[data2$gender==2,7] &lt;- ""female""
data2$Race &lt;- ""white""
    data2[data2$race==2,8] &lt;- ""black""
data2$Gender &lt;- as.factor(data2$Gender)
data2$Race &lt;- as.factor(data2$Race)
data2$Race &lt;- relevel(data2$Race,ref=""white"")

fit2 &lt;- coxph(Surv(time,delta) ~ Gender * Race, data=data2)
summary(fit2)

#Relative log risk for a black male (reference white female) from
#page 252 in Klein and Moeshberger
(coef(fit2)[3] + coef(fit2)[2] + coef(fit2)[1])
sqrt(sum(diag(fit2$var)) + 2*fit2$var[2,1] + 2*fit2$var[3,1] + 2*fit2$var[3,2])

#Let's use predict
black.male &lt;- data.frame(
  Gender=""male"",
  Race=""black""
)

white.female &lt;- data.frame(
  Gender=""female"",
  Race=""white""
)

bm &lt;- predict(fit2,newdata=black.male,se.fit=TRUE)

#bm in terms of original coefficients
coef(fit2)[3]*(1-fit2$means[3]) + coef(fit2)[2]*(1-fit2$means[2]) + coef(fit2)[1]*(1-fit2$means[1])

wf &lt;- predict(fit2,newdata=white.female,se.fit=TRUE)

#Relative log risk and se for a black male (reference white female) 
bm$fit - wf$fit
sqrt(bm$se.fit^2 + wf$se.fit^2)
</code></pre>
"
"0.0709645772411954","0.072141950116023"," 44750","<p>I have a panel dataset on seaborne shipment prices for a sample of trading routes. The data is a fixed annual price, based on a) port costs and b) fuel (a function of distance and fuel price). The major unobserved variable is port cost, which differs by port, though may be similar by area. Each observation is a port-pair such that there are two port costs included in the price. </p>

<p>The goal is to predict the price for a number of routes not included in the sample, most likely just at a more aggregate area-area level, since there are port-pairs where we only observe a loadarea-port pair or a loadarea-dischargearea pair for instance.</p>

<pre><code>Year LoadPort LoadCountry LoadArea DischargePort DischargeCountry DischargeArea  Price
2007  ABIDJAN  IVORY COAST WAF      LOOP TERMINAL USA              USG            $X1
</code></pre>

<p>The data is an unbalanced panel dataset over a 5 yr time interval. My questions are the following:</p>

<ol>
<li>Does it make sense to use a fixed effects model with dummies for the LoadPort and DischargePort? I am not sure this is what I want since I need to get an estimate of area-area average and not sure averaging the port-pair constants within areas is robust. I would also like to preserve the model's capability to predict distance correlation when I need a port-port estimate.</li>
<li>If yes to 1, how do you add another type of fixed effect in R? The plm R instructions are for typical panel datasets where you have individuals or firms across time with some constant unobserved, but in this case it's two unobserved constants per observation.</li>
<li>I have not used multilevel regression before but it strikes me that this problem falls into this class, does anyone have a quick and dirty solution? It could be just a route fixed effect, but I don't know how to implement in R. I am not worried about elegance at this stage.</li>
</ol>
"
"0.0379321620905441","0.0192807471819925"," 44780","<p>I have some data similar to this</p>

<pre><code>died  age  hospital
0     75     AA
0     88     AA
1     81     AA
0     77     AA
1     65     AA
0     41     AA
0     66     BA
1     81     BA
0     82     BA
1     64     BA
0     65     BA
1     52     BA
</code></pre>

<p>I was asked to calculate ""age adjusted mortality rates"" for each hospital. There are around 150 hospitals and approx 1000 patients (observations) per hospital. Each row in the data concerns a particular patient.</p>

<p>I was told how this could be done in Stata:</p>

<ul>
<li>Perform logistic regression of <code>died</code> on <code>age</code>.</li>
<li>Use the <code>predict</code> function to get patient-level probabilities of death.</li>
<li>Summarise the patient-level probabilities by hospital to get the mortality rates for each hospital.</li>
</ul>

<p>However, I am using R.</p>

<p>Is this the correct approach ? Are the alternatives ? Can I do the same thing in R with <code>glm</code> and <code>predict</code> ? </p>

<p><strong>Edit:</strong></p>

<p>I should perhaps add that there are several other variables that are going to be adjusted for in the model. I have shown age above, just for simplicity.</p>
"
"0.0663812836584521","0.0771229887279699"," 44895","<p>I am doing a regression analysis which troubled me. </p>

<p>My independent variable are 4 interplanetary condition components, and the dependent variable is the latitude of auroral oval boundary. 
So far, the specific relationship is still unknown in physical principle, what we want to do is to get a model (function expression) from the massive data which shows how these independent variables affect the dependent variable.</p>

<p>I used the Matlab statistical toolbox to do the regression analysis, but the results were very bad. The p values of the F statistic and t statistic are very small, but the RÂ² is also very low, about 20%. </p>

<p>So how should I improve the RÂ²? Are there good methods? I see that SVM (or LS-SVM) can do regression anaysis, is it a good way to manage the massive data, multiple independent variables regression anaysis?</p>

<p>The following are the results:</p>

<pre><code>mdl = 
Linear regression model:
    y ~ 1 + x1*x2 + x1*x3 + x1*x4 + x2*x3 + x2*x4 + x1^2 + x2^2 + x3^2 + x4^2

Number of observations: 18471, Error degrees of freedom: 18457    
Root Mean Squared Error: 2.44  
R-squared: 0.225,  Adjusted R-Squared 0.225  
F-statistic vs. constant model: 413, p-value = 0  
</code></pre>

<p>when we add another predictor, i.e., the independent variables become 5, the resuts of the regression analysis are:</p>

<p>Linear regression model:</p>

<pre><code>y ~ 1 + x1*x2 + x1*x3 + x1*x4 + x1*x5 + x2*x3 + x2*x4 + x2*x5 + x3*x5 + x2^2 + x3^2 + x4^2 + x5^2
</code></pre>

<p>Number of observations: 18457, Error degrees of freedom: 18439
Root Mean Squared Error: 2.21
R-squared: 0.366,  Adjusted R-Squared 0.366
F-statistic vs. constant model: 627, p-value = 0</p>
"
"0.026822089039291","0.0272670941574606"," 45018","<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>

<p>Question 1: This can only be done using """" method=""arima""  """" - right?</p>

<p>Question 2: For the model calibration the parameter ""xreg"" should work but how can I enter the new data for th forecast? ""newxreg"" did not work for me.</p>

<p>Thank you</p>
"
"0.0715255707714427","0.0818012824723818"," 45262","<p>For analysing zero-inflated bird counts I'd like to apply zero-inflated count models using the R package <a href=""http://cran.r-project.org/web/packages/pscl/index.html"">pscl</a>. However, having a look at the example provided in the documentation for one of the main functions (<a href=""http://rss.acs.unt.edu/Rdoc/library/pscl/html/zeroinfl.html"">?zeroinfl</a>), I begin doubting what's the real advantage of these models. According to the sample code given there, I calculated standard poisson, quasi-poisson and negative bionomial models, simple zero-inflated poisson and negative binomial models and zero-inflated poisson and negative-binomial models with regressors for the zero component. Then I inspected the histograms of the observed and the fitted data. (Here's the code for replicating that.)</p>

<pre><code>library(pscl)
data(""bioChemists"", package = ""pscl"")

## standard count data models
fm_pois  &lt;- glm(art ~ .,    data = bioChemists, family = poisson)
fm_qpois &lt;- glm(art ~ .,    data = bioChemists, family = quasipoisson)
fm_nb    &lt;- glm.nb(art ~ ., data = bioChemists)

## with simple inflation (no regressors for zero component)
fm_zip  &lt;- zeroinfl(art ~ . | 1, data = bioChemists)
fm_zinb &lt;- zeroinfl(art ~ . | 1, data = bioChemists, dist = ""negbin"")

## inflation with regressors
fm_zip2  &lt;- zeroinfl(art ~ fem + mar + kid5 + phd + ment | fem + mar + kid5 + phd + 
                     ment, data = bioChemists)
fm_zinb2 &lt;- zeroinfl(art ~ fem + mar + kid5 + phd + ment | fem + mar + kid5 + phd + 
                     ment, data = bioChemists, dist = ""negbin"")

## histograms
breaks &lt;- seq(-0.5,20.5,1)
par(mfrow=c(4,2))
hist(bioChemists$art,  breaks=breaks)
hist(fitted(fm_pois),  breaks=breaks)
hist(fitted(fm_qpois), breaks=breaks)
hist(fitted(fm_nb),    breaks=breaks)
hist(fitted(fm_zip),   breaks=breaks)
hist(fitted(fm_zinb),  breaks=breaks)
hist(fitted(fm_zip2),  breaks=breaks)
hist(fitted(fm_zinb2), breaks=breaks)!
</code></pre>

<p><img src=""http://i.stack.imgur.com/BUYsT.png"" alt=""Histogram of observed and fitted data""></p>

<p>I can't see any fundamental difference between the different models (apart from that the example data don't appear very ""zero-inflated"" to me...); actually none of the models yields a halfway reasonable estimation of the number of zeros. Can anyone explain what's the advantage of the zero-inflated models? I suppose there must have been a reason to choose this as the example for the function.</p>
"
"0.026822089039291","0.0272670941574606"," 45546","<p>Suppose I have two real-valued PMFs $f(i)$ and $g(i)$ with $1\le i\le N$ and $N$ is about 1 billion.  The functions $f$ and $g$ can be assumed to be continuous on the positive integers, one-sidedly so at the endpoints of course. (Meaning if i and j are close, then $f(i)$ and $f(j)$ are close.)</p>

<p>Lets say I have several samples of more than 10 billion observations each.  I tally each sample to get a series of histograms. The histograms basically give us functions $s_1(i)$, $s_2(i)$, $s_3(i)$ and so on.</p>

<p>For each sample, I know the percentage of the observations that come from $f$ and the percentage that come from the distribution described by $g$.</p>

<p>Can I use the samples to reconstruct the distributions $f$ and $g$.</p>

<p>My first guess was to do linear regression.  Although, I'm unsure if linear regression makes use of all the information.</p>
"
"0.0379321620905441","0.0385614943639849"," 45723","<p>I am a student in biology, currently finishing my master in Behavior, evolution and conservation. I have been in the Swiss national park for my field work and I have some trouble to analyze my data.</p>

<p>So basically what I have is a lot of factors and what I need to do is a multinomial logistic regression :</p>

<p>Factors :</p>

<ul>
<li>Behaviour (4 states - Moving, Feeding, Resting, Runing)</li>
<li>Age (Of the individual)</li>
<li>Temperature</li>
<li>Valley (where the chamois was, 2 different choices)</li>
<li>Year (of the observation)</li>
<li>Month (of the observation)</li>
<li>Kid (if it has a kid or not)</li>
<li>Individual (The number written on the tag he had on his ear)</li>
</ul>

<p>What I want to do is to check what factors influence the Behavior.</p>

<p>I tried several things with R but can't use the <code>mlogit</code> package which is the one I have been told to use.</p>

<p>I have also tried in JMP, now the problem I have here is that I just clicked on ""Fit Y in function of X"" and selected my response variable and my factors and bam, I had results, but to be honest this seems very simple and I was wondering if I am not missing something somewhere.</p>

<p>Edit : Here is what R returns when I try the <code>mlogit</code> function :</p>

<pre><code>mlogit(Behavior~Age+Temp+Valley+Individual+Year+Month, Merge)
Error in `row.names&lt;-.data.frame`(`*tmp*`, value = value) : 
  invalid 'row.names' length
</code></pre>

<p>Can anyone here that can help me either with R or with JMP?</p>
"
"0.0709645772411954","0.072141950116023"," 45882","<p>I have got a question regarding ordered choice regressions in <strong>R</strong>. </p>

<p>I have several demographic variables with which I want to explain the ordered choice of individuals within a survey in an <strong>ordered choice</strong> (<strong>probit</strong> or <strong>logit</strong>, this is not important) framework. Standard ordered choice estimations of course just give me aggregate parameter estimates. For my task it would however be useful to estimate or extract ""hypothetical"" individual-level parameter estimates (betas) for a certain independent variable and each individual in the survey. </p>

<p>I have experimented with hierarchical Bayes algorithms provided by the <strong>bayesm</strong> and <strong>ChoiceModelR</strong>. Correct me if I am wrong but I think these techniques also demand that individuals appear several times within a survey and are confronted with different choice situations, so that one can estimate the influence of certain attributes on the individuals choices.</p>

<p>My data however don't have any panel structure. I was also experimenting with Bayesian inference in example by the <strong>MCMCoprobit</strong> function in the <strong>MCMCpack</strong> package, but this function just simulates betas. I can't however, as far as I know, attribute them to certain individuals in the survey, which would be good. I would be very glad if somebody could give me a hint, sometimes already a catchword is helpful to google the correct solution!</p>
"
"0.113954427341773","0.115845044561442"," 46322","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/28936/how-to-compute-significant-interaction-estimates-when-main-effect-is-not-signifi"">How to compute significant interaction estimates when main effect is not significant?</a>  </p>
</blockquote>



<p>I am an applied linguist and I am modelling responses to a vocabulary test taken by second language learners of English; the aim is to test theoretical hypotheses regarding the relationship between the nature of the word and the likelihood of the learnersâ€™ knowing that word. The models I am using to explore the role of explanatory variables are the random item Item Response Theory model LLTM+e (see de Boeck et al. 2011; de Boeck 2008). These are created using the lmer function in the LME4 package in R and treat item and person responses as random. The estimates shown below are effectively like those from a binary logistic regression model, indicating the log-odds of a correct response on a word, given certain properties. Covariates relate to both item and person characteristics. </p>

<p>The nature of my concern is actually a basic regression issue. I have found a significant interaction between ability grouping of the test taker (GRP; with two levels High and Low) and the length of the word in letters (LEN_L). As far as I can see the estimate of the fixed effects for the first model shows that (a) the lower level learners have an overall lower probability of giving a correct answer (b) that LEN_L does not provide a significant explanation across the pattern of responses for the whole test-taker population, and (c) a significant interaction between GRP and LEN_L indicates that the lower ability learners are less likely to give a correct answer for a longer word. This is in keeping with theory. </p>

<p>However, when I model the data without including the main effect for LEN_L, I am not seeing a significant effect for either high or low groups as shown in Model 2. LEN_L does not show as significant if modelled without interaction with grp low (not shown). I feel that I am missing something obvious, but I cannot quite grasp what is happening. And my references have run dry on this particular issue and I am thinking myself around in circles about it.</p>

<p>(NB: in my full model I have many other significant covariates, but this pattern holds true.) 
My query basically regards whether I should use Model 1, including the non-significant main effect, or whether my findings from Model 2 indicate that the finding is a little unstable. Any advice would be much appreciated! (I can post more details if necessary.)
Karen</p>

<p>Model 1 Fixed effects:</p>

<pre><code>              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    0.25244    0.83194   0.303  0.76156    
grp low       -2.09126    0.26406  -7.920 2.38e-15 ***
LEN_L          0.02093    0.13062   0.160  0.87272    
grp low:LEN_L -0.09185    0.03146  -2.919  0.00351 **
</code></pre>

<p>Model 2 Fixed effects:</p>

<pre><code>               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.25243    0.83194   0.303    0.762    
grp low        -2.09126    0.26406  -7.920 2.38e-15 ***
grp high:LEN_L  0.02093    0.13062   0.160    0.873    
grp low:LEN_L  -0.07092    0.13202  -0.537    0.591  
</code></pre>

<blockquote>
  <p>De Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A.,
  Tuerlinckx, F. and Partchev, I. (2011) <a href=""http://www.jstatsoft.org/v39/i12/"" rel=""nofollow"">The Estimation of Item
  Response Models with the 'lmer' Function from the lme4 Package in
  R</a>. Journal of Statistical Software (39:12) pp 1-28</p>
</blockquote>
"
"0.0402331335589365","0.0545341883149212"," 46434","<p>The <code>summary.rq</code> function from the <a href=""http://cran.r-project.org/web/packages/quantreg/quantreg.pdf"">quantreg vignette</a> provides a multitude of choices for standard error estimates of quantile regression coefficients. What are the special scenarios where each of these becomes optimal/desirable?</p>

<ul>
<li><p>""rank"" which produces confidence intervals for the estimated parameters by inverting a rank test as described in Koenker (1994). The default option assumes that the errors are iid, while the option iid = FALSE implements the proposal of Koenker Machado (1999). See the documentation for rq.fit.br for additional arguments.</p></li>
<li><p>""iid"" which presumes that the errors are iid and computes an estimate of the asymptotic covariance matrix as in KB(1978).</p></li>
<li><p>""nid"" which presumes local (in tau) linearity (in x) of the the conditional quantile functions and computes a Huber sandwich estimate using a local estimate of the sparsity.</p></li>
<li><p>""ker"" which uses a kernel estimate of the sandwich as proposed by Powell(1990).</p></li>
<li><p>""boot"" which implements one of several possible bootstrapping alternatives for estimating standard errors.</p></li>
</ul>

<p>I have read at least 20 empirical papers where this is applied either in the time-series or the cross-sectional dimension and haven't seen a mention of standard error choice. </p>
"
"0.0379321620905441","0.0385614943639849"," 46523","<p>I know I'm missing something in my understanding of logistic regression, and would really appreciate any help.</p>

<p>As far as I understand it, the logistic regression assumes that the probability of a '1' outcome given the inputs, is a linear combination of the inputs, passed through an inverse-logistic function. This is exemplified in the following R code:</p>

<pre><code>#create data:
x1 = rnorm(1000)           # some continuous variables 
x2 = rnorm(1000)
z = 1 + 2*x1 + 3*x2        # linear combination with a bias
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = pr &gt; 0.5               # take as '1' if probability &gt; 0.5

#now feed it to glm:
df = data.frame(y=y,x1=x1,x2=x2)
glm =glm( y~x1+x2,data=df,family=""binomial"")
</code></pre>

<p>and I get the following error message: </p>

<blockquote>
  <p>Warning messages:
  1: glm.fit: algorithm did not converge 
  2: glm.fit: fitted probabilities numerically 0 or 1 occurred </p>
</blockquote>

<p>I've worked with R for some time now; enough to know that probably I'm the one to blame..
what is happening here?</p>
"
"0.0929144419623766","0.0944559849109725"," 46789","<p>I collected data to find whether the presence or absence of vision, sound, and touch during a task affected the successful completion of that task. However, there were no samples collected where all three senses were absent. So the dependent variable is boolean success but I have a question about how to model the independent variables in a logistic regression.</p>

<p>My initial analysis used a single categorical variable with seven levels representing each combination of senses (seven because there were no cases where all three senses were absent).</p>

<pre><code>summary( glmer( Success ~ Condition + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>When I tried to build a model with the Vision, Sound, and Touch as separate variables, the analysis fails. <a href=""http://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q4/004552.html"" rel=""nofollow"" title=""[R-sig-ME] Structural zeros in lme4"">I believe this is because I have empty cells when including the vision*sound*touch interaction</a> because we did not collect results where all senses were absent.</p>

<pre><code>summary( glmer( Success ~ Vision + Sound + Touch + Vision*Sound + Vision*Touch + 
                Sound*Touch + Vision*Sound*Touch + ( 1 | Participant ), 
                family=binomial, data=trials))
</code></pre>

<p>I followed the suggestion linked above to use the <code>interaction</code> function to drop the unused factor (all three senses absent). However, this seems to create a variable that looks like my original single categorical variable.</p>

<pre><code>senses &lt;- interaction( trials$Vision, trials$Sound, trials$Touch, drop=TRUE )
summary( glmer( Success ~ senses + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>As I try to refine this analysis, is there a way to model the senses as separate variables to make the interaction between these variables clearer? That is, to appropriately model the contribution of vision in the <code>vision</code>, <code>vision*sound</code>, <code>vision*touch</code> and <code>vision*sound*touch</code> conditions. From the initial analysis, the <code>vision*sound*touch</code> interaction is the most interesting.</p>
"
"0.0889588054368324","0.0904347204435887"," 46821","<p>I am producing a script for creating bootstrap samples from the <code>cats</code> dataset (from the <code>-MASS-</code> package). </p>

<p>Following the Davidson and Hinkley textbook [1] I ran a simple linear regression and adopted a fundamental non-parametric procedure for bootstrapping from iid observations, namely <strong>pairs resampling</strong>.</p>

<p>The original sample is in the form:</p>

<pre><code>Bwt   Hwt

2.0   7.0
2.1   7.2

...

1.9    6.8
</code></pre>

<p>Through an univariate linear model we want to explain cats hearth weight through their brain weight. </p>

<p>The code is:</p>

<pre><code>library(MASS)
library(boot)


##################
#   CATS MODEL   #
##################

cats.lm &lt;- glm(Hwt ~ Bwt, data=cats)
cats.diag &lt;- glm.diag.plots(cats.lm, ret=T)


#######################
#   CASE resampling   #
#######################

cats.fit &lt;- function(data) coef(glm(data$Hwt ~ data$Bwt)) 
statistic.coef &lt;- function(data, i) cats.fit(data[i,]) 

bootl &lt;- boot(data=cats, statistic=statistic.coef, R=999)
</code></pre>

<p>Suppose now that there exists a clustering variable <code>cluster = 1, 2,..., 24</code> (for instance, each cat belongs to a given litter). For simplicity, suppose that data are balanced: we have 6 observations for each cluster. Hence, each of the 24 litters is made up of 6 cats (i.e. <code>n_cluster = 6</code> and <code>n = 144</code>).</p>

<p>It is possible to create a fake <code>cluster</code> variable through:</p>

<pre><code>q &lt;- rep(1:24, times=6)
cluster &lt;- sample(q)
c.data &lt;- cbind(cats, cluster)
</code></pre>

<p>I have two related questions:</p>

<p>How to simulate samples in accordance with the (clustered) dataset strucure? That is, <strong>how to resample at the cluster level?</strong> I would like to sample the clusters with replacement and to set the observations within each selected cluster as in the original dataset (i.e. sampling with replacenment the clusters and without replacement the observations within each cluster). </p>

<p>This is the strategy proposed by Davidson (p. 100). 
Suppose we draw <code>B = 100</code> samples. Each of them should be composed by 24 possibly recurrent clusters (e.g. <code>cluster = 3, 3, 1, 4, 12, 11, 12, 5, 6, 8, 17, 19, 10, 9, 7, 7, 16, 18, 24, 23, 11, 15, 20, 1</code>), and each cluster should contain the same 6 observations of the original dataset. How to do that in <code>R</code>? (either with or without the <code>-boot-</code> package.) Do you have alternative suggestions for proceeding?</p>

<p>The second question concerns the initial regression model. Suppose I adopt a <strong>fixed-effects model</strong>, with cluster-level intercepts. <strong>Does it change the resampling procedure</strong> adopted? </p>

<p>[1] Davidson, A. C., Hinkley, D. V. (1997). <em>Bootstrap methods and their applications</em>. Cambridge University press.</p>
"
"0.0379321620905441","0.0385614943639849"," 47306","<p>I have a dataset with one binary target variable called â€œtargetâ€ and many many factors â€œF1â€, F2â€â€¦ â€œF200â€. Iâ€™m trying to come up with code to fit 200 single factor logistic regression models and return the model summaries. Here is what I have so far</p>

<pre><code>single &lt;- function(df, factor){
    s &lt;- summary(glm(as.formula(paste('target ~ ', factor, sep='')), family=binomial, data=df));
return(s);
}
single(data, c(""F1"", ""F2"", ""F2""));
</code></pre>

<p>but this gives me only a summary of the first model. Am I missing something obvious here?</p>
"
"0.0599760143904067","0.0487768608679754"," 47947","<p>I have a dataset containing information about a bunch of wireless devices.  Specifically, the number of other wireless devices each device encounters.  Based on other researcher's prior work in the same area, I have reason to believe my distribution may potentially be well fitted by the not so well known <a href=""ftp://netlib.bell-labs.com/who/nuzman/papers/nssw_TCP.pdf"" rel=""nofollow"">biPareto distribution</a>, whose CCDF is defined as:</p>

<p>$1 - F(x) = (x/k)^{-\alpha}(\frac{x+kb}{k + kb})^{\alpha-\beta}$, for $ x &gt; k$</p>

<p>and simply </p>

<p>$1 - F(x) = 1$, for $x \le k$.</p>

<p>Assume I already know $k = 1$.  This leaves a function to fit with the three parameters $\alpha$,$b$,$\beta$, with the restriction that each parameter is $&gt; 0$.</p>

<p>What I am unsure about is how one would go about determining the most suitable fitting parameter values, ideally in <code>R</code>.  </p>

<p>I am quite the novice when it comes to fitting data, so aside from the syntactical considerations in <code>R</code>, I am also interested in any advice regarding what specific methods of fitting would be most appropriate for this particular function.  For example:</p>

<ol>
<li>Does this function qualify for linear regression?</li>
<li>Is least squares appropriate/applicable here?  What about minimum mean squared error (MMSE)?</li>
<li>How sensitive is any given fitting method to any ""guiding"" initial parameter values I give it (iterative methods?), or is the result determined analytically (always produces the same answer in the end?) </li>
</ol>

<p>If it's at all relevant, I'm ultimately looking to use the Kolmogorov-Smirnov test on the data to see how well it matches the parameterized biPareto distribution.</p>
"
"0.209592853792344","0.213070207570894"," 48040","<p>I'm trying to test the significance of the ""component"" effect in a multivariate regression model. I'm not sure what is the right way. Using R, I have tried a way with <code>lm()</code> and another way with <code>gls()</code>, and they don't yield compatible results. </p>

<p><strong>Please note that this is not a question about which methodology is the right one to use to analyze my data. By the way these are simulated data. My question is about the understanding in mathematical terms of the R procedures I use.</strong></p>

<p>The dataset:</p>

<pre><code>&gt; str(dat)
'data.frame':   31 obs. of  5 variables:
 $ group: Factor w/ 5 levels ""1"",""5"",""2"",""3"",..: 1 1 1 1 1 1 1 1 3 3 ...
     $ id   : Factor w/ 8 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 1 3 ...
 $ x    : num  2.5 3 3 4 1.2 3.8 3.9 4 2.5 2.9 ...
     $ y    : num  2.6 3.8 3.9 3.8 1.6 5.2 1.3 3.6 4 3.2 ...
 $ z    : num  3.1 3.6 4.9 3.8 2.1 6 2.1 2.9 4.2 2.9 ...
&gt; head(dat,10)
   group id   x   y   z
1      1  1 2.5 2.6 3.1
2      1  2 3.0 3.8 3.6
3      1  3 3.0 3.9 4.9
4      1  4 4.0 3.8 3.8
5      1  5 1.2 1.6 2.1
6      1  6 3.8 5.2 6.0
7      1  7 3.9 1.3 2.1
8      1  8 4.0 3.6 2.9
9      2  1 2.5 4.0 4.2
10     2  3 2.9 3.2 2.9
</code></pre>

<p>I convert this dataset into ""long format"" for graphics (and later for <code>gls()</code>):</p>

<pre><code>dat$subject &lt;- dat$group : dat$id
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

xyplot(value ~ component | group, data=dat.long, 
    pch=16, 
    strip = strip.custom(strip.names=TRUE,var.name=""group"" ), layout=c(5,1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/14KtA.png"" alt=""enter image description here""></p>

<p>Each individual of each group has $3$ repeated measures $x$,$y$,$z$ (I should join the points in the graphic to see the repeated measures).</p>

<p>I want to fit a MANOVA model using group as factor and $(x,y,z)$ is the multivariate response:
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right), \quad i=1,\ldots,5
$$
(of course we could use the default R parameterization $\mu_{ik}=\mu_{1k} + \alpha_{ik}$ by considering <code>group1</code>as the ""intercept"" for each response but I prefer ""my"" parameterization).</p>

<p>This model is fitted as follows using <code>lm()</code>:</p>

<pre><code>###  multivariate least-squares fitting  ###
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )
</code></pre>

<p>I think the model can also  be fitted with <code>gls()</code> as follows (but with a different fitting procedure) :</p>

<pre><code>### generalized least-squares fitting  ###
library(nlme)
gfit &lt;- gls(value ~ group*component, data=dat.long, correlation=corSymm(form= ~ 1 | subject))
</code></pre>

<p>Recall that <code>subject = group:id</code> is the identifier of the individuals. The <code>correlation=corSymm(form= ~ 1 | subject)</code> argument means that the responses $x$, $y$, $z$ for each individual are correlated. Here <code>corSymm</code> means a general, ""unrestricted"",  covariance structure (termed as ""unstructured"" in SAS language).</p>

<p>To check that <code>mfit</code> and <code>gfit</code> are equivalent, we can check for instance that we can deduce the estimated parameters of <code>mfit</code> from the estimated parameters of <code>gfit</code>and vice-versa (so the ""mean"" parameters have exactly the same fitted values):</p>

<pre><code>&gt; coef(mfit)
                  x          y          z
(Intercept)  3.1750  3.2250000  3.5625000
group5      -0.9500 -0.4750000  0.1125000
group2      -1.0750 -0.5678571 -0.2339286
group3      -0.7875 -0.1000000  0.1875000
group4      -0.3750  0.4000000 -0.0125000
&gt; coef(gfit)
      (Intercept)            group5            group2            group3 
        3.1750000        -0.9500000        -1.0750000        -0.7875000 
           group4        componenty        componentz group5:componenty 
       -0.3750000         0.0500000         0.3875000         0.4750000 
group2:componenty group3:componenty group4:componenty group5:componentz 
        0.5071429         0.6875000         0.7750000         1.0625000 
group2:componentz group3:componentz group4:componentz 
        0.8410714         0.9750000         0.3625000 
</code></pre>

<p>Now I want to test the ""component effect"". Rigorously speaking, writing the model as 
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right),
$$
I want to test the hypothesis $\boxed{H_0\colon \{\mu_{1i}=\mu_{2i}=\mu_{3i} \quad \forall i=1,2,3,4,5 \}}$.</p>

<p>Below are my attempts, one attempt with <code>gfit</code> and two attempts with <code>mfit()</code>:</p>

<pre><code>###########################################
## testing significance of the component ##
###########################################

&gt; ### with gfit  ###
&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
&gt; 
&gt; ### with mfit ###
&gt; library(car)
&gt; 
&gt; # first attempt : 
&gt; idata &lt;- data.frame(component=c(""x"",""y"",""z""))
&gt; ( av.ok &lt;- Anova(mfit, idata=idata, idesign=~component, type=""III"") )

Type III Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.84396  140.625      1     26 5.449e-12 ***
group            4   0.10369    0.752      4     26    0.5658    
component        1   0.04913    0.646      2     25    0.5328    
group:component  4   0.22360    0.818      8     52    0.5901    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; 
&gt; # second attempt :
&gt; linearHypothesis(mfit, ""(Intercept) = 0"", idata=idata, idesign=~component, iterms=""component"")

 Response transformation matrix:
  component1 component2
x          1          0
y          0          1
z         -1         -1

Sum of squares and products for the hypothesis:
           component1 component2
component1    1.20125    1.04625
component2    1.04625    0.91125

Sum of squares and products for error:
           component1 component2
component1   31.46179   14.67696
component2   14.67696   21.42304

Multivariate Tests: 
                 Df test stat  approx F num Df den Df  Pr(&gt;F)
Pillai            1 0.0491253 0.6457903      2     25 0.53277
Wilks             1 0.9508747 0.6457903      2     25 0.53277
Hotelling-Lawley  1 0.0516632 0.6457903      2     25 0.53277
Roy               1 0.0516632 0.6457903      2     25 0.53277
</code></pre>

<p>With <code>anova(gfit)</code> the component is significant, but not with my two attempts using <code>mfit</code> and the <code>car</code> package. </p>

<p>I know that <code>gls()</code> use a different fitting method than <code>lm()</code> but this is surely not the cause of the difference. </p>

<p>So my questions are :</p>

<ul>
<li>did I do something wrong ?</li>
<li>which method tests my $H_0$ hypothesis ?</li>
<li>what is the $H_0$ hypothesis of the other methods ?</li>
</ul>

<p>And I have an auxiliary question: how to get $\hat\Sigma$ with <code>mfit</code> and <code>gfit</code> ?</p>

<h2>Update 1</h2>

<p>Below is a reproducible example which simulates the dataset. 
Now I think I understand : both ANOVA methods are correct (the first one with <code>anova(gfit)</code> and the second one with <code>Anova(mfit, ...)</code>, <strong>and they yield very close results when using the type II sum of squares in <code>Anova(mfit, ...)</code></strong>.  For the above example: </p>

<pre><code>&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>is very close to </p>

<pre><code>&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>

<p>Below is the reproducible code with the data sampler (I simulate uncorrelated repeated measures but it suffices to include a covariance matrix in the <code>rmvnorm()</code> function to simulate correlated repeated measures) :</p>

<pre><code>library(mvtnorm)
library(nlme)
library(car)

# set data parameters 
I &lt;- 5 # number of groups
J &lt;- 16 # number of individuals per group
dat &lt;- data.frame(
    group = gl(I,J),
    id = gl(J,1,I*J),
    x=NA, 
    y=NA, 
    z=NA
)
Mu &lt;- c(1:I) # group means of components (assuming E(x)=E(y)=E(z) in each group)

# simulates data: 
for(i in 1:I){
    which.group.i &lt;- which(dat$group==i)
    dat[which.group.i,c(""x"",""y"",""z"")] &lt;- round(rmvnorm(n=J, mean=rep(Mu[i],3)), 1)
}

dat$subject &lt;- droplevels( dat$group : dat$id )
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

# multivariate least-squares fitting 
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )

# gls fitting
dat.long$order.xyz &lt;- as.numeric(dat.long$component)
gfit &lt;- gls(value ~ group*component , data=dat.long, correlation=corSymm(form=  ~ order.xyz | subject)) 

# compares ANOVA : 
anova(gfit)
idata &lt;- data.frame(component=c(""x"",""y"",""z""))
Anova(mfit, idata=idata, idesign=~component, type=""II"")
Anova(mfit, idata=idata, idesign=~component, type=""III"")
</code></pre>

<p>So now I wonder which type of sum of squares is the more appropriate one for my real study... but this is another question</p>

<h2>Update 2</h2>

<p>About my question <em>""how to get $\hat\Sigma$""</em>, here is the answer for <code>gls()</code>:</p>

<pre><code>&gt; getVarCov(gfit)
Marginal variance covariance matrix
        [,1]    [,2]    [,3]
[1,] 0.92909 0.47739 0.24628
[2,] 0.47739 0.92909 0.53369
[3,] 0.24628 0.53369 0.92909
  Standard Deviations: 0.96389 0.96389 0.96389 
</code></pre>

<p>That shows that <strong><code>mfit</code>and <code>gfit</code> were not equivalent models</strong>: <code>gfit</code>assumes the same variance for the three components.</p>

<p>In order to fit a fully unrestricted covariance matrix for the repeated measures, we have to type:</p>

<pre><code>gfit2 &lt;- gls(value ~ group*component , data=dat.long, 
    correlation=corSymm(form=  ~ 1 | subject), 
    weights=varIdent(form = ~1 | component))

&gt; summary(gfit2)
Generalized least squares fit by REML
  Model: value ~ group * component 
  Data: dat.long 
       AIC      BIC    logLik
  264.0077 313.4986 -111.0038

Correlation Structure: General
 Formula: ~1 | subject 
 Parameter estimate(s):
 Correlation: 
  1     2    
2 0.529      
3 0.300 0.616
Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | component 
 Parameter estimates:
       x        y        z 
1.000000 1.253534 1.169335 

....

Residual standard error: 0.8523997 
</code></pre>

<p>But yet I don't understand the extracted covariance matrix given by <code>getVarCov()</code> (but this is not important since we get this matrix with <code>summary(gfit2)</code>): </p>

<pre><code>   &gt; getVarCov(gfit2)
    Error in t(S * sqrt(vars)) : 
      dims [product 9] do not match the length of object [0]
    &gt; getVarCov(gfit2, individual=""1:1"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 0.72659 0.48164 0.25500
    [2,] 0.48164 1.14170 0.65562
    [3,] 0.25500 0.65562 0.99349
      Standard Deviations: 0.8524 1.0685 0.99674 
    &gt; getVarCov(gfit2, individual=""1:2"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 1.14170 0.56319 0.27337
    [2,] 0.56319 0.99349 0.52302
    [3,] 0.27337 0.52302 0.72659
      Standard Deviations: 1.0685 0.99674 0.8524 
</code></pre>

<p>Unfortunately, the <code>anova(gfit2)</code> table is not as close to <code>Anova(mfit, ..., type=""II"")</code> as <code>anova(gfit)</code>:</p>

<pre><code>&gt; anova(gfit2)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 498.1744  &lt;.0001
group               4   1.0514  0.3864
component           2  13.1801  &lt;.0001
group:component     8   0.8310  0.5780

&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>
"
"NaN","NaN"," 48243","<p>Is there any function in R for doing variable selection (backward elimination) in a multiple logistic regression using restricted cubic splines like <strong>mvrs</strong> procedure for STATA?</p>
"
"0.100582833897341","0.109068376629842"," 48381","<p>I'm trying to estimate power in a logistic regression with a continuous exposure in a cohort study (ie, the ratio of the sampling probabilities is 1). I have population cumulative incidence (probability) and population exposure variability and exposure mean and an expected odds ratio. I also have a total sample size.</p>

<p>I'm using R and it seems like <code>Hmisc::bpower</code> is only for logistic regression with binary exposure and I can't seem to find any packages that estimate binomial power with continuous exposure.</p>

<p>I've attempted the following simulation but it's quite slow given my total sample size and I'm not sure if it's right:</p>

<pre><code>p &lt;- vector()
betahat &lt;- vector()
for(i in 1:1000){
n &lt;- 40000  #total sample size
intercept = log(0.008662265)  #where exp(intercept) = P(D=1)
beta &lt;- log(1.4) #where exp(beta)=OR corresponding to a one unit change in xtest
xtest &lt;- rnorm(n,1.2,.31)  #xtest is vector length 40,000 with mean 1.2 and sd .31
linpred &lt;- intercept + xtest*beta #linear predictor
prob &lt;- exp(linpred)/(1 + exp(linpred)) #link function
runis &lt;- runif(n,0,1) #generate a vector length n from a uniform distribution 0,1
ytest &lt;- ifelse(runis &lt; prob,1,0)  #if a random value from a uniform distribution 0,1 is less than prob, then the outcome is 1.  otherwise the outcome is 0
coefs &lt;- coef(summary(glm(ytest~xtest, family=""binomial"")))  #run a logistic regression
p[i] &lt;- coefs[2,4] #store the p value
betahat[i] &lt;- coefs[2,1] #store the unexponentiated betahat
}

mean(p &lt; .05)
#power

exp(mean(betahat))
#sanity check, should equal 1.4--it does
</code></pre>

<p>Is there anything wrong with this approach?</p>

<p>One concern of mine is that the cumulative incidence (ie, probability of event over the given time period) comes from a population that did not have 0 exposure.  In fact, it's reasonable to assume that the value i'm using for an intercept is actually from a population that has an exposure variability similar to mine. In that case, how would I estimate the unexposed probability given an odds ratio (and other information that I would find in say, a published paper) to use in my power calculation?</p>
"
"0.0892693083195917","0.0983129061176287"," 48410","<p>I'm modeling the effect of a categorical predictor on a binary dependent variable using logistic regression. I'm comparing models with/without the predictor using a likelihood-ratio test.</p>

<p>Two categories of the predictor are associated with values of 1 only (no 0s) for the dependent variable. Regression coefficients for these categories (expressed as changes in log(odds) compared to a reference category) are very large and highly suspicious, as this reference category is always associated with response values of 1 (but for one case), and I would thus expect regression coefficients close to 0 for these two categories. Comparisons between the reference category and other categories having more balanced distribution of 1 and 0s matches what I'm expecting from visual inspection of the data. 
Removing cases associated with these two 'problematic' categories does not change the logLikelihood of the models, but because it changes the number of parameters it affects the results of the likelihood ratio test.</p>

<p>Models are fitted using the glm function with binomial family and logit link in R.</p>

<p>My question therefore is: what model (or procedure) should I use to: </p>

<p>(1) test the global significance of the effect of the predictor on the dependent variable? Should I keep data from the 'problematic' categories in the model or not before conducted the likelihood ratio test?</p>

<p>(2) compare these two 'problematic' categories with others?</p>

<p>Any hint appreciated,</p>
"
"0.117448635776103","0.125082807548204"," 48415","<p>I come to you today because I face a huge problem that I cannot explain.</p>

<p>I have run a multinomial logistic regression (using the mlogit package) on behavioral data. I prepare the data by doing</p>

<pre><code>    mlogit &lt;- mlogit.data(Merge, choice = ""Choice"", shape = ""long"", alt.var = ""Comp"", 
                          drop.index = TRUE)
</code></pre>

<p>on my Merge data.</p>

<p>which gives me the following:</p>

<pre><code>                Date     Time ActivityX ActivityY Temp Behavior Valley Age Month Year kid Individual Choice
    1.F   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26   TRUE
    1.R   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    1.M   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    1.RUN 01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.F   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26   TRUE
    2.R   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.M   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.RUN 01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    3.F   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    3.R   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    3.M   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26   TRUE
    3.RUN 01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    4.F   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    4.R   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26   TRUE
    4.M   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    4.RUN 01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    5.F   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
    5.R   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26   TRUE
    5.M   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
    5.RUN 01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
</code></pre>

<p>then I ran my regression :</p>

<pre><code>m1 &lt;- mlogit(Choice ~ 1 |Temp + Valley + Age + kid + Month , mlogit)
</code></pre>

<p>and it gave me significant results :</p>

<pre><code>                          Estimate  Std. Error  t-value  Pr(&gt;|t|)    
    M:(intercept)      -4.2153e-01  5.7533e-02  -7.3268 2.358e-13 ***
    R:(intercept)       6.2325e-01  3.4958e-02  17.8284 &lt; 2.2e-16 ***
    RUN:(intercept)    -1.2275e+01  4.0526e-01 -30.2895 &lt; 2.2e-16 ***
    M:Temp              1.5371e-02  9.8680e-04  15.5764 &lt; 2.2e-16 ***
    R:Temp             -3.9871e-02  6.7926e-04 -58.6975 &lt; 2.2e-16 ***
    RUN:Temp           -4.4532e-02  6.8696e-03  -6.4825 9.023e-11 ***
    M:ValleyTrupchun   -3.6154e-01  1.6362e-02 -22.0968 &lt; 2.2e-16 ***
    R:ValleyTrupchun   -4.0186e-02  9.7968e-03  -4.1020 4.096e-05 ***
    RUN:ValleyTrupchun  1.2895e+00  8.5357e-02  15.1066 &lt; 2.2e-16 ***
    M:Age              -1.1026e-02  2.6902e-03  -4.0985 4.158e-05 ***
    R:Age               1.9465e-02  1.6479e-03  11.8119 &lt; 2.2e-16 ***
    RUN:Age             5.5473e-02  1.6661e-02   3.3294 0.0008703 ***
    M:kidY              6.0686e-02  2.2638e-02   2.6807 0.0073460 ** 
    R:kidY             -4.1638e-01  1.2391e-02 -33.6024 &lt; 2.2e-16 ***
    RUN:kidY            6.2311e-01  1.0410e-01   5.9854 2.158e-09 ***
    M:Month            -2.0466e-01  8.4448e-03 -24.2346 &lt; 2.2e-16 ***
    R:Month             2.4148e-02  5.2317e-03   4.6157 3.917e-06 ***
    RUN:Month           9.8715e-01  5.6209e-02  17.5622 &lt; 2.2e-16 ***
</code></pre>

<p>those results were in line with what I expected to find in literature so I was quite happy.</p>

<p>My next step was to plot my results and here is when I have some trouble.</p>

<p>First of all when I plot my original data and compare it with the result of my regression I find some huge differences. For example, when I plot the %of time spend in a behavior (M for moving, F for feeding, R for resting and Run for running, in my regression F is the reference) in function of age, I find that the older an individual gets, the more they will rest and the more they will move, but the estimates I got from my regression shows that they should rest more (when they get older) but move less. So to summarize, my graph on the original data shows the opposite as what I got from the regression.</p>

<p>I don't know if it is normal, in the sense that I don't know if I can compare my original data to the result of my regression in a way that my regression shows the probability from switching to a behavior from an other each time my variable grows of one unit.</p>

<p>So I wanted to use the <code>predict()</code> function but I don't know how to do that. I was hoping to get some help here.</p>
"
"0.0379321620905441","0.0385614943639849"," 48651","<p>I am looking for a Least Angle Regression (LAR) packages in R or MATLAB which can be used for <strong>classification</strong> problems.</p>

<p>The only package that I currently know which fits this description is <a href=""http://cran.r-project.org/web/packages/glmpath/index.html"" rel=""nofollow"">glmpath</a>. The issue with this package is that it is a little old and somewhat limited in its scope (I am forced to rely on logistic regression for classification problems model).</p>

<p>I am wondering if anyone knows of other packages that allow me to run LAR on different types of classification models, such as Support Vector Machines (see <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.391&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">The Entire Regularization Path for the Support Vector Machine</a>). </p>

<p>The ideal package would allow me to run LAR-type algorithms for different types of classification models and also provide a function that can produce the full regularization path. </p>
"
"0.0464572209811883","0.0314853283036575"," 48811","<p>For count data that I have collected, I use Poisson regression to build models. I do this using the <code>glm</code> function in R, where I use <code>family = ""poisson""</code>. To evaluate possible models (I have several predictors) I use the AIC. So far so good. Now I want to perform cross-validation. I already succeeded in doing this using the <code>cv.glm</code> function from the <code>boot</code> package. From <a href=""http://stat.ethz.ch/R-manual/R-patched/library/boot/html/cv.glm.html"">the documentation</a> of <code>cv.glm</code> I see that e.g. for binomial data you need to use a specific cost function to get a meaningful prediction error. However, I have no idea yet what cost function is appropriate for <code>family = poisson</code>, and an extensive Google search did not yield any specific results. My question is anybody has some light to shed on which cost function is appropriate for <code>cv.glm</code> in case of poisson glm's.</p>
"
"0.0758643241810882","0.0771229887279699"," 48863","<p>Consider the following scenario: I have an auto dealership, and I want to decide whether to buy a used car. If it's a good buy, I would make a substantial amount of money of it. If it's a bad buy, I would lose a substantial amount of money. If it's neither (""nop"") , I would lose a small amount of money.</p>

<p>Naturally, I start with training and testing sets, and then use a simple 2-class classifier, that aims to predict whether it's a good buy or a bad buy ( I used <a href=""http://cran.r-project.org/web/packages/gbm/index.html"" rel=""nofollow"">gbm</a> ,whose latest version, by the way, now includes also multinomial regression!). </p>

<p>Once I have my classifier, I can look at my testing set, and inspect the cars that the classifier told me to buy. with these cars, I would calculate my expected future earnings: </p>

<pre><code>   number of good buys X profit - number of bad buys x loss - number of 'nop' buys x smaller loss.
</code></pre>

<p>So, if you think about it, I actually shouldn't be trying to classify. I should try and find an area in my features space in which the given criterion is maximized. However, this sound like a combinatorial problem. </p>

<p>So (finally), my question is: what algorithm / method is there that I can use to maximize my cost function?</p>

<p>Thanks a lot!</p>
"
"0.0715255707714427","0.0727122510865616"," 48922","<p>I am trying to estimate a selection model of the form:</p>

<p>$Z_i = 1[\alpha_0 + \alpha_1X_{1,i} + \alpha_2X_{2,i} + \delta_i$ > 0]</p>

<p>$Y_i = \beta_0 + \beta_1X_{1,i} + Z_i + \epsilon_i$</p>

<p>where $1[]$ denotes the indicator function.</p>

<p>The purpose of the model is to calculate the indirect effect of $X_1$ on $Y$ through $Z$, as well as the the direct effect.</p>

<p>My first question is how to go about estimating this type of model, and how this estimation can be achieved in R. As far as I see it I have a few possible approaches:</p>

<p>(1) Use a standard Heckman selection model, using OLS for both the reduced form and structural equations, using ivreg() in R. This will obviously ignore the constraint that $Z$ is bounded between 0 and 1.</p>

<p>(2) Estimate the first stage with a probit model (i.e. $\delta_i \sim N(0,1)$), and the second stage using standard OLS. I understand that I could do this via manual 2SLS, but as far as I am aware the standard errors will be incorrect? Am I right in that this model is feasible, and if so, can you direct me to a method of achieving this in R?</p>

<p>(3) Build a switching regression model (tobit-5) using the selection() function from sampleSelection package in R. I believe this model will estimate two equations for  $Y$, one for where $Z_i=0$ and one where $Z_i=1$, and with a unique intercept and coefficients for each of the regressors in the outcome equations.</p>

<p>The question then is how to get an estimate of the indirect effect of $X_1$ for each of these methods.</p>

<ul>
<li><p>If I use (1) or (2) then I imagine it might be possible to calculate the average marginal effect of $Z$ on $Y$, and the average marginal effect of $X_1$ on $Z$, then approximate the indirect effect by multiplying the two values?</p></li>
<li><p>If (3) then could I take the fitted value under the estimated model for $Y$ where $Z=0$, and compare the mean to the mean of the fitted values under the estimated model for $Y$ where $Z=1$? This would then give me an estimate of the marginal effect of $Z$? Then use the same method as above and multiple this effect by the marginal effect of $X_1$ on $Z$?</p></li>
</ul>

<p>Many thanks in advance!</p>
"
"0.0479808115123254","0.0609710760849692"," 49141","<p>My predictions coming from a logistic regression model (glm in R) are not bounded between 0 and 1 like I would expected. My understanding of logistic regression is that your input and model parameters are combined linearly and the response is transformed into a probability using the logit link function. Since the logit function is bounded between 0 and 1, I expected my predictions to be bounded between 0 and 1.</p>

<p>However that's not what I see when I implement logistic regression in R:</p>

<pre><code>data(iris)
iris.sub &lt;- subset(iris, Species%in%c(""versicolor"",""virginica""))
model    &lt;- glm(Species ~ Sepal.Length + Sepal.Width, data = iris.sub, 
                family = binomial(link = ""logit""))
hist(predict(model))
</code></pre>

<p><img src=""http://i.stack.imgur.com/0BHU5.png"" alt=""enter image description here""></p>

<p>If anything the output of predict(model) looks normal to me. Can anyone explain to me why the values I get are not probabilities?</p>
"
"NaN","NaN"," 49513","<p>I have a learning problem from $X$ to $Y$ where:</p>

<ul>
<li>$X$ = $n$ input numeric vectors of $m$ dimensions </li>
<li>$Y$ = $n$ output numeric vectors of $k$ dimensions</li>
</ul>

<p>In other words:</p>

<p>&nbsp; &nbsp; &nbsp; <img src=""http://i.stack.imgur.com/rgcru.png"" alt=""enter image description here""></p>

<p>I am hoping to collect a <strong>list</strong> of <strong>R</strong> packages or <strong>Python</strong> libraries for <strong>multiple-output</strong> problems  for classification and regression.</p>

<p>For example, do any of the learning methods in <a href=""http://cran.r-project.org/web/packages/caret/index.html"" rel=""nofollow"">caret</a> support this functionality? What packages in general are available for this problem?</p>
"
"NaN","NaN"," 49775","<p>I know that one of the advantages of mixed models is that they allow to specify variance-covariance matrix for the data (compound symmetry, autoregressive, unstructured, etc.) However, <code>lmer</code> function in R does not allow for easy specification of this matrix. Does anyone know what structure <code>lmer</code> uses by default and why there is no way to easily specify it?</p>
"
"NaN","NaN"," 49835","<p>The R plotting package ggplot2 has an awesome function called <a href=""http://docs.ggplot2.org/0.9.2.1/stat_smooth.html"">stat_smooth</a> for plotting a regression line (or curve) with the associated confidence band.</p>

<p>However I am having a hard time figuring out exactly how this confidence band is generated, for every time of regression line (or ""method""). How can I find this information?</p>
"
"NaN","NaN"," 49939","<p>What is the meaning of <code>t value</code> and <code>Pr(&gt;|t|)</code> when using <code>summary()</code> function on linear regression model in R?</p>

<pre><code>Coefficients:
                              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                    10.1595     1.3603   7.469 1.11e-13 ***
log(var)                        0.3422     0.1597   2.143   0.0322 *
</code></pre>
"
"0.139460633053452","0.136711051308229"," 50086","<p>Assume for example a trivariate Gaussian model:
$$
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \quad (*)
$$
with ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$. </p>

<p>The Bayesian conjugate theory of this model is well known. 
This model is the most simple case of a  multivariate linear regression model. 
And more generally, there is a well known Bayesian conjugate theory of multivariate linear regression, which is  the extension to the case when  the multivariate mean  ${\boldsymbol \mu}= {\boldsymbol \mu}(x_i)$ is allowed to depend on the covariates $x_i$ of individual $i \in \{1, \ldots, n \}$, with linear constraints about the multivariate means ${\boldsymbol \mu}(x_i)$. See for instance <a href=""http://books.google.be/books?id=GL8VS9i_B2AC&amp;dq=bayesian%20econometrics%20bayesm&amp;hl=fr&amp;source=gbs_navlinks_s"" rel=""nofollow"">Rossi &amp; al's book</a> accompanied by the crantastic <a href=""http://cran.r-project.org/web/packages/bayesm/index.html"" rel=""nofollow""><code>bayesm</code></a> package for <code>R</code>. 
We know in addition that the Jeffreys prior is a limit form of the conjugate prior  distributions.</p>

<p>Let us come back to the simple multivariate Gaussian model $(*)$. Instead of generalizing this model to the case of linearly dependent multivariate means ${\boldsymbol \mu}(x_i)$ depending on individuals $i=1,\ldots,n$, we can consider a more restrictive model by assuming linear constraints about the components $\mu_1$, $\mu_2$, $\mu_3$ of the multivariate mean ${\boldsymbol \mu}$. </p>

<h3>Example</h3>

<p>Consider some concentrations $x_{i,t}$ of $4$ blood samples $i=1,2,3,4$ measured at $3$ timepoints $t=t_1,t_2,t_3$. 
Assume that the samples are independent and that the series of the three measurements 
$(x_{i,t_1},x_{i,t_2},x_{i,t_3}) \sim {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right)$ for each sample $i$ with a mean 
${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$ whose three components are 
linearly related to the timepoints: $\mu_j = \alpha + \beta t_j$.</p>

<p>This example falls into the context of <em>Multivariate linear regression with a within-design structure</em>. 
See for instance <a href=""http://wweb.uta.edu/management/Dr.Casper/Fall10/BSAD6314/Coursematerial/O%27Brien%20&amp;%20Kaiser%201985%20-%20MANOVA%20-%20RM%20-%20Psy%20Bull%2085.pdf"" rel=""nofollow"">O'Brien &amp; Kaiser 1985</a> and <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a></p>

<p>So my example is a simple example of this situation because there are only some predictors (the timepoints) for the components of the mean, but there are no predictors for individuals. This example could be written as follows:
$$
(**) \left\{\begin{matrix} 
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \\ 
{\boldsymbol \mu} = X {\boldsymbol \beta}
\end{matrix}\right.
$$
with ${\boldsymbol \beta}=(\alpha, \beta)'$ and $X=\begin{pmatrix} 1 &amp; t_1 \\ 1 &amp; t_2 \\ 1 &amp; t_3 \end{pmatrix}$ is the matrix of covariates for the components of the multivariate mean ${\boldsymbol \mu}$. The second line of $(**)$ could be termed as the <em>within design</em>, or the <em>repeated measures design</em>, or the <em>structural design</em> (I would appreciate if a specialist had some comments about this vocabulary).</p>

<p>I think such a model can be fitted as a generalized least-squares model, as follows in  <code>R</code> :</p>

<pre><code>gls(response ~ ""between covariates"" , data=dat, 
  correlation=corSymm(form=  ~ ""within covariates"" | individual ))
</code></pre>

<p>(after stacking the data in long format).</p>

<p><strong>My first question</strong> is Bayesian: what about the Bayesian analysis of model $(**)$ and more generally the Bayesian analysis of the multivariate linear regression model with a structural design ? 
Is there a conjugate family ? What about the Jeffreys prior ? Is there an appropriate R package to perform this Bayesian analysis ?</p>

<p><strong>My second question</strong> is not Bayesian: I have recently discovered some possibilities of John Fox's great <code>car</code> package to analyse 
such models with ordinary least squares theory (the <code>Anova()</code> function with the <code>idesign</code> argument --- see <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a>). Perhaps I'm wrong, but I am under the impression that this package only allows to get the MANOVA table (sum of squares analysis) with an orthogonal matrix $X$, and I'd like to get (exact) confidence intervals about the within-design parameters for an arbitrary matrix $X$, as well as prediction intervals. Is there a way to do so with <code>R</code> using ordinary least squares ?  </p>
"
"0.0848188929679971","0.0862261227118454"," 50167","<p>I would like to know if there is a way to get p-values when using the GLS function (part of the nlme package) for the ML estimate of the error-autoregressive parameters. I looked through the package information on nlme and have not found a clear example of how to do this.</p>

<p>Here is an example of some code that illustrates my question:</p>

<pre><code>x &lt;- rnorm(100)
y &lt;- rnorm(100)
z &lt;- rnorm(100)
df &lt;- data.frame(x,y,z)

test.reg &lt;- gls(x ~ y+z,data=df,correlation=corARMA(p=1),method=""ML"")

summary(test.reg)

Generalized least squares fit by maximum likelihood
  Model: x ~ y + z 
  Data: df 
   AIC      BIC    logLik
  289.3276 302.3534 -139.6638

Correlation Structure: AR(1)
 Formula: ~1 
 Parameter estimate(s):
   Phi 
0.01887445 

Coefficients:
                 Value  Std.Error   t-value p-value
(Intercept) 0.08418063 0.10214246 0.8241492  0.4119
y           0.00236772 0.09410912 0.0251593  0.9800
z           0.07689263 0.09480738 0.8110405  0.4193

 Correlation: 
  (Intr) y     
y  0.135       
z -0.013  0.074

Standardized residuals:
        Min          Q1         Med          Q3         Max 
-1.94366502 -0.66310726  0.09546043  0.64881478  2.78384648 

Residual standard error: 0.9781187 
Degrees of freedom: 100 total; 97 residual
</code></pre>

<p>What I would like to know more about is the Parameter estimate Phi. I have searched through the nlme package to get more information about p-values for Phi but have come up short. I am unsure if the gls function computes the p-values for Phi. </p>

<p>I do know this gives the coefficients of phi:</p>

<pre><code>coeff &lt;- test.reg$modelStruct$corStruct

Correlation structure of class corAR1 representing
   Phi 
0.01887445 
</code></pre>

<p>But that is all I have really found. Any help would be great!</p>

<p>Thanks so much,
John</p>
"
"0.0758643241810882","0.0771229887279699"," 51152","<p>I've been trying to use the fastbw function from the rms package in R to perform logistic regression with backward selection, with p-values as exclusion criterion (I am well aware of the arguments against using p-values for this as opposed to e.g. AIC). However, the results are not in agreement with what I would get if I perform the backward selection manually, as fastbw often drops more factors in comparison. The results also seem to depend on the number of factors considered, even with the option </p>

<pre><code>type=""individual"".
</code></pre>

<p>I created some simple example data in order to prove my point, which give the following result:</p>

<pre><code>&gt; fastbw(lrm(y~x1+x2+x3+x4),rule=""p"",type=""individual"")

 Deleted Chi-Sq d.f. P      Residual d.f. P      AIC  
 x3      0.37   1    0.5412 0.37     1    0.5412 -1.63
 x1      1.82   1    0.1771 2.20     2    0.3336 -1.80
 x4      2.58   1    0.1082 4.78     3    0.1889 -1.22
 x2      3.56   1    0.0591 8.34     4    0.0799  0.34

[...]

Factors in Final Model

None
</code></pre>

<p>I.e., x2 is dropped as the last of the factors considered, resulting in a model without factors. However, if I consider x2 only, I get the following result. </p>

<pre><code>&gt; fastbw(lrm(y~x2),rule=""p"",type=""individual"")

No Factors Deleted

Factors in Final Model

[1] x2
</code></pre>

<p>The same is true if I do the backward selection manually, as x2 considered separately has a p-value of 0.045. What might cause this behavior? Since x2 is the last remaining variable in the backward selection, the results shouldn't depend on associations with other model covariates.</p>
"
"0.0402331335589365","0.0409006412361909"," 51347","<p>I'd like to perform an exponential regression with multiple independent variables (similar to the LOGEST function in Excel)</p>

<p>I'm trying to model the function $Y = b {m_1}^{x_1}{m_2}^{x_2}$  where $b$ is a constant, $x_1$ and $x_2$ are my independent variables, and $m_1$ and $m_2$ are the coefficients of the independent variables.</p>

<p>I think I can linearize the function by doing something like <code>glm(log(Y) ~ x1 + x2)</code> but I don't totally understand why that would work. Also, I'd like to run a true non-linear regression if there is such a thing.</p>

<p>My goal is to run both a linear and an exponential regression, and find the best fit line based on the higher $R^2$ value. </p>

<p>I would also really appreciate your help in understanding how to plot the predicted curve in a scatter plot of my data as well.</p>
"
"0.026822089039291","0.0272670941574606"," 51378","<p>I want to do a simple lm of y~x where I weight my response variable.  This is because 
the values of y are actually each in turn the value of a slope of another regression of y~year i.e. rates of change over time, and for each of these original regressions the number of recorded years was variable (some had data for 20 years, some for 40, some for 45 etc).</p>

<p>I realise weighting an lm has been discussed here:</p>

<p><a href=""http://stats.stackexchange.com/questions/7513/how-to-use-weights-in-function-lm-in-r"">How to use weights in function lm in R?</a></p>

<p>However I am unsure as to whether using the simple instructions outlined in this post I will be weighting y (which is what I want) or x, or whether the weight ""acts"" across the whole lm and the distinction I am drawing between weighting y specifically, rather than x, is invalid...</p>
"
"0.0379321620905441","0.0385614943639849"," 51412","<p>I'm performing a multiple logistic regression with a large amount of columns. I want to set all the variables over a certain p value to have 0 coefficients, then test the model on the test data. </p>

<p>Since there's so many variables I'm using, it's impractical to manually go in and set the coefficients. So I'm looking for something like the following.</p>

<pre><code>log_results &lt;- glm(formula, data, family);
log_results_sig &lt;- get_sig_only(log_results, p value threshold);
</code></pre>

<p>This way I can use the results with the predict() function, and export the results easily for use in another program. </p>

<p>Additionally, if anyone knows a way to automatically extract the significant variables and refit with them, I would appreciate that too.</p>

<p>Thanks.</p>
"
"0.0379321620905441","0.0385614943639849"," 51417","<p>Today I tried to estimate models using both <code>plm</code> and <code>pgmm</code> functions in the <a href=""http://cran.r-project.org/web/packages/plm/index.html"" rel=""nofollow"">plm</a> R package, with an interaction between <code>X1</code> and <code>lag(X2, 1)</code>. And I notice two issues.</p>

<p>Let $Y=b_1  X_1 + b_2  X_2 + b_3  X_1  X_2 + e$ be our model.</p>

<ol>
<li><p>When using <code>plm</code>, I got different results when I coded the interaction term with <code>I(X1 * lag(X2, 1))</code> and when I just saved this multiplication <code>X1 * lag(X2, 1)</code> in a different variable of the dataset and then used it in the regression.</p></li>
<li><p>With <code>pgmm</code> it is not even possible to run a formula which contains <code>I(X1 * lag(X2, 1))</code>. How can I pass such interaction?</p></li>
</ol>
"
"0.0848188929679971","0.0776035104406608"," 51464","<p>Background: For a project, I am fitting a conditional logit model where I have 5 control cases for every realized case. To do that I use the <code>clogit()</code> function in the package <code>survival</code>. I wanted to graph interactions with the <code>effects</code> package by John Fox et al. It turns out that this package can't handle <code>clogit</code> objects (output of <code>clogit()</code>). </p>

<p>As I believed I remembered that conditional logit were a special case of GLM, I thought the clever/lazy way to get my interaction plots would be to refit the model using a fixed effects glm and then use <code>effect()</code>.
The documentation of <code>clogit</code> seemed to confirm my intuition:</p>

<blockquote>
  <p>It turns out that the logliklihood for a conditional logistic regresson model = loglik from a Cox model with a particular data structure. [...]
  When a well tested Cox model routine is available many packages use this â€˜trickâ€™ rather than writing a new software routine from scratch, and this is what the clogit routine does. </p>
  
  <p>In detail, a stratified Cox model with each case/control group assigned to its own stratum, time set to a constant, status of 1=case 0=control, and using the exact partial likelihood has the same likelihood formula as a conditional logistic regression. The clogit routine creates the necessary dummy variable of times (all 1) and the strata, then calls coxph.</p>
</blockquote>

<p>Based on this description, it seems that I should be able to reproduce the stratification achieved through <code>strata()</code> by using a random intercept for each case/control group with <code>1|group</code> in <code>lmer()</code>. However, when I try, the results of <code>clogit</code> and <code>lmer</code> differ. One thing is that I probably have the wrong likelihood function. I don't really know how to specify this in <code>lmer</code> but more important, I am wondering what else I am missing. </p>

<p>I wonder whether I am completely wrong or somewhat on the right track but missing some pieces? What I would like is to understand what are the difference in terms of how the model is fitted between a conditional logit and a regular one (I understand that might be quite a long answer, so a book reference would be a great start). The my usual references for regression (Gelman and Hill, 2007; Mills 2011) are somewhat silent on the subject.</p>
"
"0.0464572209811883","0.0314853283036575"," 52035","<p>I have a weekly time series representing costs for a cohort. I want to tell whether an intervention on the cohort (we can assume it happened in a single week) has decreased costs for the cohort. I happen to know that the trend over this period for the population from which this cohort was taken was -120 per week per week.</p>

<p>My initial thought was simply to do a linear regression <code>lm(Costs~Weeks,offset=-120*Weeks)</code> but (obviously) the significance is not only a function of the effect of the intervention but also how far back I look (if I look back to $-\infty$ it will of course appear non-significant).</p>

<p>I looked at this website: <a href=""http://www.r-bloggers.com/time-series-intervention-analysis-wih-r-and-sas/"" rel=""nofollow"">http://www.r-bloggers.com/time-series-intervention-analysis-wih-r-and-sas/</a> and tried to replicate the R code with my data, but when I enter the arimax() command, I got the error message </p>

<pre><code>Error in stats:::arima(x=x,order=order,seasonal=seasonal,fixed=par[1:narma], : wrong length for 'fixed'
</code></pre>

<p>Now, I'm not sure what to do. Can anyone give me some guidance?</p>
"
"0.0379321620905441","0.0385614943639849"," 52155","<p>Here's my context for this question: From what I can tell, we cannot run an ordinary least squares regression in R when using weighted data and the <code>survey</code> package. Here, we have to use <code>svyglm()</code>, which instead runs a generalized linear model (which may be the same thing? I am fuzzy here in terms of what is different).</p>

<p>In OLS and through the <code>lm()</code> function, it calculates an R-squared value, the interpretation of which I do understand. However, <code>svyglm()</code> does not seem to calculate this and instead gives me a Deviance, which my brief trip around the internet tells me is a goodness-of-fit measure that is interpreted differently than an R-squared.</p>

<p>So I guess I essentially have two questions on which I was hoping to get some direction:</p>

<ol>
<li>Why can we not run OLS in the <code>survey</code> package, while it seems that this is possible to do with weighted data in Stata?</li>
<li>What is the difference in interpretation between the deviance of a generalized linear model and an r-squared value?</li>
</ol>

<p>Thanks.</p>
"
"0.134110445196455","0.136335470787303"," 52252","<p>I have a regression model that looks like this: $$Y = \beta_0+\beta_1X_1 + \beta_2X_2 + \beta_3X_3 +\beta_{12}X_1X_2+\beta_{13}X_1X_3+\beta_{123}X_1X_2X_3$$</p>

<p>...or in R notation: <code>y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x1:x2:x3</code></p>

<p>Let's say $X_1$ and $X_2$ are categorical variables and $X_3$ is numeric. The complication is that $X_1$ has three levels $X_{1a}, X_{1b}, X_{1c}$ and instead of standard contrasts, I need to test: </p>

<ul>
<li>Whether the intercept for level $X_{1a}$ significantly differs from the average intercept for levels $X_{1b}$ and $X_{1c}$.</li>
<li>Whether the response of $X_2$ is significantly different between level $X_{1a}$ and the average of levels $X_{1b}$ and $X_{1c}$.</li>
<li>Whether the slope of $X_3$ is significantly different between level $X_{1a}$ and the average of levels $X_{1b}$ and $X_{1c}$.</li>
</ul>

<p>Based on <a href=""http://stats.stackexchange.com/questions/32188/how-to-interpret-these-custom-contrasts"">this post</a> it seems like the matrix I want is...</p>

<pre><code> 2
-1
-1
</code></pre>

<p>So I do <code>contrasts(mydata$x1)&lt;-t(ginv(cbind(2,-1,-1)))</code>. The estimate of $\beta_1$ changes, but so do the others. I can reproduce the new estimate of $beta_1$ by subtracting the predicted values of the $X_1b$ and $X_1c$ group means (when $X_3=0$ and $X_2$ is at its reference level) from twice the value of $X_1a$ at those levels. But I can't trust that I specified my contrast matrix correctly unless I can also similarly derive the other coefficients.</p>

<p>Does anybody have any advice for how to wrap my head around the relationship between cell means and contrasts? Thanks. Is there a standard name for this type of contrast?</p>

<hr>

<p>Aha! According to the <a href=""http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm"">link posted in Glen_b's answer</a>, the bottom line is, you can convert ANY comparison of group means you want into an R-style contrast attribute as follows:</p>

<ol>
<li>Make a square matrix. The rows represent the levels of your factor and the columns represent contrasts. Except the first one, which tells the model what the intercept should represent.</li>
<li>If you want your intercept to be the grand mean, fill the first column with all of the same non-zero value, doesn't matter what. If you want the intercept to be one of the level means, put a number in that row and fill the rest with zeros. If you want the intercept to be a mean of several levels, put numbers in those rows and zeros in the rest. If you want it to be a weighted mean, use different numbers, otherwise use the same number. <em>You can even put in negative values in the intercept column and that probably means something too, but it completely changes the other contrasts, so I have no idea what that's for</em></li>
<li>Fill in the rest of the columns with positive and negative values indicating what levels you want compared to what others. I forget why summing to zero is important, but adjust the values so that the columns do sum to zero.</li>
<li>Transpose the matrix using the <code>t()</code> function.</li>
<li>Use <code>ginv()</code> from the <code>MASS</code> package or <code>solve()</code> to get the inverse of the transposed matrix.</li>
<li>Drop the first column, e.g. <code>mycontrast&lt;-mycontrast[,-1]</code>. You now have a p x p-1 matrix, but the information you put in for your intercept was encoded in the matrix as a whole during step 5.</li>
<li>If you want labels in the summary output more pleasant to read than <code>lm()</code> et al.'s default output, name your matrix's columns accordingly. The intercept will always automatically get named <code>(Intercept)</code> however.</li>
<li>Make your matrix the new contrast for the factor in question, e.g. <code>contrasts(mydata$myfactor)&lt;-mymatrix</code></li>
<li>Run <code>lm()</code> (and probably many other functions that use formulas) as normal in standard R without having to load <code>glht</code>, <code>doBy</code>, or <code>contrasts</code>.</li>
</ol>

<p>Glen_b, thank you, and thank you UCLA Statistical Consulting Group. My applied stats prof spent several days handwaving on this topic, and I was still left with no idea how to actually write my own contrast matrix. And now, an hour of reading and playing with R, and I finally think I get it. Guess I should have applied to UCLA instead. Or University of StackExchange.</p>
"
"NaN","NaN"," 52423","<p>I am trying to perform stepwise regression for variable selection in R. 
In matlab, the <code>stepwisefit function</code> is able to work in <code>n &lt; p</code> problems. Trying to use <code>step()</code> for such problems i get the error message <code>AIC is -infinity for this model, so 'step' cannot proceed</code>. Is there a way to modify parameters so i can use <code>step</code> function?</p>
"
"0.0599760143904067","0.0609710760849692"," 52475","<p>I have been running some binomial logistic regressions in R on a data set and I realised that the p-values of the estimated coefficients are not computed based upon a Normal distribution. For e.g. I have the following result from the glm() function:</p>

<pre><code>    Coefficients:

                                   Estimate Std. Error z value Pr(&gt;|z|) 
    (Intercept)                    -1.6127     0.1124 -14.347  &lt; 2e-16 ***
    relevel(Sex, ""M"")F             -0.3126     0.1410  -2.216   0.0267 *  
    relevel(Feed, ""Bottle"")Both    -0.1725     0.2056  -0.839   0.4013    
    relevel(Feed, ""Bottle"")Breast  -0.6693     0.1530  -4.374 1.22e-05 ***
</code></pre>

<p>I previously thought that the beta hats (estimated coefficients) are asymptotically normal and the p-value should be calculated using the normal distribution. But it seems from the p-values here that a t-distribution is used to compute them (I think). </p>

<p>I know that the asymptotic condition does not hold when sample size is small but if so, what is the process that causes the estimator to be distributed with a Students distribution? And how do I find out the degrees of freedom for such a distribution?</p>
"
"NaN","NaN"," 52783","<p>I am trying to replicate a Weibull model for political leaders' survival in R. The model uses the Weibull distribution with the hazard rate <code>h(t)=lambda*pt^(p-1)</code>.</p>

<p>The term p is dependent on a variable W. So in <code>Stata</code> it can be done simply with adding <code>anc(W)</code> to the regression. </p>

<p>But the <code>weibreg</code> in the <a href=""http://cran.r-project.org/web/packages/eha/index.html"" rel=""nofollow""><code>eha</code></a> package only allows one to set values to ""shape"". Is there anyway to do this with <code>weibreg</code>? Is there another package/function I should be considering using for this purpose?</p>
"
"0.026822089039291","0.0272670941574606"," 53089","<p>Consider the <code>pairs()</code> function. Given a matrix $X$ with $p$ columns it will produce a $p$-by-$p$ matrix of plots with in each cell a bivariate plot of $X_k$ (the $k$ th column of $X$) against $X_l$ $1\leq l\neq k\leq p$.</p>

<p>I'm looking for a regression equivalent of this: given a vector $y$ and $X$ it would produce $p$ plots of $y$ against $X_l$. </p>
"
"0.053644178078582","0.0545341883149212"," 54767","<p>I want to plot the results of a regression model, but allowing two variable to vary simultaneously. I guess I could do that using predict() function in R, but I am running a model that does not have such function developed yet.</p>

<p>Let's suppose we have a data set</p>

<pre><code>x1 &lt;- rnorm(100)
x2 &lt;- rnorm(100)
y &lt;- 1 + x1*5 + x2*3 + rnorm(100)
</code></pre>

<p>And we run a simple model and calculate yhat.</p>

<pre><code>mod &lt;- lm(y ~ x1 + x2)
yhat &lt;- predict(mod)
dt &lt;- as.data.frame(cbind(yhat,x1,x2))
</code></pre>

<p>How can I plot the expected value of y for different values of x1 and x2? I tried this, but it didn't work:</p>

<pre><code>contour(dt$x1, dt$x2, dt$yhat)

Error in contour.default(dt$x1, dt$x2, dt$yhat) : 
  increasing 'x' and 'y' values expected
</code></pre>
"
"0.018966081045272","0.0192807471819925"," 55240","<p>I'm working on a data set modeling road kills (0 = random point, 1 = road kill) as a function of a number of habitat variables.  Following Hosmer and Lemeshow, I've examined each continuous predictor variable for linearity, and a couple appear nonlinear.  I'd like to try a fractional polynomial transformation for each, also following Hosmer and Lemeshow, and have looked at the R package mfp, but I'm having trouble coming up with (and understanding) the R code that will correctly transform the variable.  Can anyone suggest R code that would help me accomplish the concepts on p. 101 - 102 of Hosmer and Lemeshow's Applied Logistic Regression (2000).  Thanks!</p>
"
"0.0892693083195917","0.0907503748778111"," 55393","<p>I have a PDF (Probability Density Function) generated from a vector of 1,000,000 empirical values. This empirical PDF is heavily skewed to the right.</p>

<p>In this form, I can't make accurate predictions using a linear regression.</p>

<p>To fix this, is there some method to find the function F(x) to transform (i.e. ""squash"") the values in the vector into a standard normal distribution, so I can feed said transformed vector into a linear regression?</p>

<p>Of course, this would also involve finding the inverse of F(x) that transforms (i.e. ""de-squashes"") any predictions back into the original empirical PDF.</p>

<p><strong>What I have tried</strong></p>

<p>So far, I have managed to generate the density function from the empirical data:</p>

<p><img src=""http://i.stack.imgur.com/HIBUP.png"" alt=""enter image description here""></p>

<p>Here is the R code:</p>

<pre><code>par(mfrow=c(2,1))

install.packages(""bootstrap"")
library(bootstrap)
data(stamp)
nobs &lt;- dim(stamp)[1]
hist(stamp$Thickness,col=""grey"",breaks=100,freq=F)
	dens &lt;- density(stamp$Thickness)
lines(dens,col=""blue"",lwd=3)

plot(density(stamp$Thickness),col=""black"",lwd=3, main=""Simulation to choose density plot"")
	for(i in 1:10)
	{
		newThick &lt;- rnorm(nobs,mean=stamp$Thickness,sd=dens$bw*1.5)
		lines(density(newThick,bw=dens$bw),col=""grey"",lwd=3)
}

# If I wanted to do a linear regression to predict stamp thickness,
# what is the function F(x) to ""squash"" (i.e. transform) the ""stamp""
# vector into a normal distribution, and the corresponding inverse 
# function Finv(x) to ""desquash"" (i.e. untransform) any predictions back 
# into the original prediction?
</code></pre>

<p><strong>Update 1</strong></p>

<p>@Andre Silva sugggested that:</p>

<blockquote>
  <p>What need to have normal distribution are the residuals (predicted
  versus observed) derived from your (multiple) linear regression model.</p>
</blockquote>

<p>According to <a href=""http://www.stat.yale.edu/Courses/1997-98/101/linmult.htm"" rel=""nofollow"">post on Multiple Linear Regression</a>:</p>

<blockquote>
  <p>After fitting the regression line, it is important to investigate the
  residuals to determine whether or not they appear to fit the
  assumption of a normal distribution. A normal quantile plot of the
  standardized residuals y -  is shown to the left. Despite two large
  values which may be outliers in the data, the residuals do not seem to
  deviate from a random sample from a normal distribution in any
  systematic manner.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/3ybm0.gif"" alt=""enter image description here""></p>

<p><strong>Update 2</strong></p>

<p>See <a href=""http://stats.stackexchange.com/questions/11351/left-skewed-vs-symmetric-distribution-observed/11352#11352"">Left skewed vs. symmetric distribution observed</a> for R code that illustrates that the only relevant concern is if the residuals are normally distributed.</p>
"
"0.0464572209811883","0.0472279924554862"," 55462","<p>I am using <code>KFAS</code> package for <code>R</code>.</p>

<p>You can run</p>

<pre><code>install.packages(""KFAS"")
library(KFAS)
?regSSM
</code></pre>

<p>to see how this package allows to build a state space representation of linear regression models and many others.</p>

<p>Now let we have the following state space system:</p>

<p>$S_{t}=\alpha+(1+k_{t})L_{t}+v_{t}$</p>

<p>$k_{t}=\phi k_{t-1}+(1-\phi)\bar{k}+w_{t}$</p>

<p>being $\bar{k}$ a constant, a.k.a. the unconditional mean of the unobservable AR(1) process.</p>

<p>Anyone can tell me how may I set this state space representation in <code>KFAS</code> through <code>regSSM</code> or any other <code>KFAS</code> package's function (like <code>arimaSSM</code>)?</p>
"
"0.0379321620905441","0.0385614943639849"," 55946","<p>I conducted a regression analysis using R's lm() function. One of the independent variables shows no significance (p = 0.89), which contradicts the hypothesis that is should have a significantly positive effect on the dependent variable. </p>

<p>How do you interpret that? Can you say that it has no positive effect on the dependent variable, just because it is not significant - even though it is not significantly negative?</p>
"
"0.0309714806541255","0.0472279924554862"," 56226","<p>I have a large regression problem with a lot of cases, but relatively few independent variables. One of them is a categorical factor with thousands of levels. Robust regression runs forever. In some cases the large number of dummy variables becomes too sparse to calculate with even ""normal"" lm. </p>

<p>What would usually make sense is to somehow calculate the average for each level of the factor, then adjust the dependent variable accordingly, and do the regression without the big factor. A colleague of mine could remember there is a two-letter R function that does that automatically, but he cannot remember the two letter combination.</p>

<p>Any help would be greatly appreciated.</p>
"
"0.142013610993574","0.149348025476585"," 56237","<p>I'm a beginner in R and Im wondering how to interprete my results.....
My question is about the results that I got after I did a regression on the Translog production function for panel data:
$ log(y)=log(A) + \alpha_{K} log(K) + \alpha_{L} log(L) + \beta_{KL} log(K)log(L) + \beta_{L^2} log^2(L) + \beta_{K^2} log^2(K)$</p>

<p>L stands for labour and K for Kapital.</p>

<p>The results I got for the Within, Random and first difference a the following:
Within:</p>

<pre><code>  #Within
    Coefficients :
  Estimate  Std. Error  t-value Pr(&gt;|t|)    
     K   1.0902e-05  1.0654e-06  10.2326   &lt;2e-16 ***
     L  -2.4009e-06  1.5086e-07 -15.9150   &lt;2e-16 ***
     LK  1.9788e-03  3.6069e-03   0.5486   0.5833    
     LL  3.0511e-02  1.3141e-03  23.2173   &lt;2e-16 ***
     KK  5.0333e-02  2.6650e-03  18.8868   &lt;2e-16 ***
     ---
     Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

    Total Sum of Squares:    6886.3
        Residual Sum of Squares: 1983.9
  R-Squared      :  0.71191 
  Adj. R-Squared :  0.69692 
    F-statistic: 10729.1 on 5 and 21709 DF, p-value: &lt; 2.22e-16


&gt; #regression random translog
&gt; tl.random&lt;-plm(Y ~ K + L + LK + LL + KK, data=panel, model=""random"")
 &gt; summary(tl.random)
 Oneway (individual) effect Random Effect Model 
(Swamy-Aroras transformation)

 Call:
 plm(formula = Y ~ K + L + LK + LL + KK, data = panel, model = ""random"")

  Balanced Panel: n=462, T=48, N=22176

  Effects:
               var std.dev share
  idiosyncratic 0.09139 0.30230 0.397
   individual    0.13856 0.37224 0.603
  theta:  0.8836  

  Residuals :
Min.  1st Qu.   Median  3rd Qu.     Max. 
 -3.16000 -0.14200  0.00724  0.15400  4.89000 

   Coefficients :
                 Estimate  Std. Error  t-value Pr(&gt;|t|)    
   (Intercept)  1.6266e+00  3.9030e-02  41.6763   &lt;2e-16 ***
    K            9.0932e-06  1.0552e-06   8.6178   &lt;2e-16 ***
    L           -2.5192e-06  1.5023e-07 -16.7684   &lt;2e-16 ***
   LK           2.7566e-03  3.6102e-03   0.7636   0.4451    
   LL           2.9491e-02  1.3138e-03  22.4474   &lt;2e-16 ***
   KK           4.8817e-02  2.6659e-03  18.3117   &lt;2e-16 ***
   ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

  Total Sum of Squares:    7183.6
  Residual Sum of Squares: 2070.2
  R-Squared      :  0.71181 
  Adj. R-Squared :  0.71162 
  F-statistic: 10951.9 on 5 and 22170 DF, p-value: &lt; 2.22e-16

  &gt; #regression first difference translog
   &gt; tl.fd&lt;-plm(Y ~ K + L + LK + LL + KK-1, data=panel, model=""fd"")
   &gt; summary(tl.fd)
    Oneway (individual) effect First-Difference Model


       #First difference regression
     Call:
      plm(formula = Y ~ K + L + LK + LL + KK - 1, data = panel, model = ""fd"")

      Balanced Panel: n=462, T=48, N=22176

      Residuals :
     Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    -1.4900 -0.0321  0.0199  0.0202  0.0715  0.9860 

          Coefficients :
          Estimate  Std. Error t-value  Pr(&gt;|t|)    
      K   2.3847e-07  2.8965e-06  0.0823 0.9343856    
      L  -8.0238e-07  2.3128e-07 -3.4693 0.0005229 ***
     LK -2.6986e-02  6.7755e-03 -3.9829 6.831e-05 ***
     LL  5.6920e-02  2.3933e-03 23.7830 &lt; 2.2e-16 ***
     KK  3.7811e-02  5.1254e-03  7.3773 1.674e-13 ***
    ---
       Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

        Total Sum of Squares:    426.54
       Residual Sum of Squares: 269.92
        R-Squared      :  0.38799 
          Adj. R-Squared :  0.3879 
</code></pre>

<p>My question are:</p>

<p>1) Is there a reason why the estimation for coefficient for LK is not significant in both within and random? but in first diff?</p>

<p>2) Why give within and random so similar results, and why first difference is different from them?</p>

<p>3)Can I interpret Standard error and R squared?
Is there anything else I can interpret? Which is the best model of the three?</p>

<p>Thank you so much for your help! </p>
"
"0.0848188929679971","0.0862261227118454"," 56471","<p>I'm attending a course in computational statistics, which should be an applied course. We study different methods, which are important in ""reality"". One of these topics is Cross Validation. I'm faced with the following problem coming from a homework. We are given a dataset and suppose that the model is of the form
$$ Y_i=m(X_i)+\epsilon_i $$
i.e. a nonparametric regression. We want to compute the generalized error using the leave one out Cross Validation score. This should be done by using kernel estimator <code>ksmooth</code>, local polynomials and smoothing splines. My first question is very general: Given the data how can I choose by eye a reasonable bandwith for the kernel estimator? See for example the following picture:</p>

<p><img src=""http://i.stack.imgur.com/Fs3BD.png"" alt=""enter image description here""></p>

<p>Since this data looks rather ""wild"" it is for me not clear how to choose a bandwidth. My first attempt was just to run ksmooth playing around with different bandwidth. But as I said, here the data is wild, so it is (for me) hard to determine a reasonable bandwidth.</p>

<p>The second problem is more concrete about the problem described above. So far I have the following code:</p>

<pre><code>   cv &lt;- function(data,used.function)
{
  n &lt;- nrow(data)
  cv.value &lt;- rep(0,length(n))
  for (i in 1:n){
      new.data &lt;- data[-i,]
      cv.value[i] &lt;- used.function(new.data[,1],new.data[,2],data[i,1])
  }
  ## MSE
  return(1/n*sum((new.data[,2]-cv.value)^2))
}


### kernel estimator usind nadaraya-watson:
fcn1 &lt;- function(reg.x, reg.y, x){
  return(ksmooth(reg.x, reg.y, x.point = x, kernel = ""normal"", bandwidth = h)$y)
}
### CV-score for kernel estimator:
(cv.nw &lt;- cv(real.data, fcn1))
</code></pre>

<p>the function cv should be general such that I can apply local polynomials and smoothing spline too. The variable real.data contains the data. It is a $n\times 2$ matrix which store all the $x$ values and $y$ values. In the function body of cv I perform a leave one out cross validation. However using this code gives for cv.nw a NA. What is wrong with my code? I'm very thankful for your help.</p>
"
"0.0715255707714427","0.0818012824723818"," 56521","<p>I need to calculate the regression variance ($\sigma^2$) in order to estimate both the confidence intervals and the prediction intervals in a gls regression analysis.  For the analysis, the covariance matrix ($V$) of the response variable ($y$) is known in advance, and so I use it directly as the weighting matrix (=$V^{-1}$) in the gls regression analysis.</p>

<p>The regression variance is a weighted sum of the residual error:
$\sigma^2 = \frac{ (Y â€“ X\beta)^T C^{-1} (Y â€“ X\beta)}{n â€“ p}$</p>

<p>My question/problem is how to determine the weighting matrix $C^{-1}$?  $C$ cannot be set equal to $V$ since (according to the above equation) $C$ must be dimensionless while $V$ has the same units as $\sigma^2$.</p>

<p>Based on my reading of the literature and available texts, it seems that $C$ is the correlation matrix and is a scaled or normalized form of the covariance matrix $V$.  i.e., $V = Var(\epsilon^2) = \sigma^2 C$.  But my problem is that $\sigma^2$ is not yet known, and so I need another way find $C$ from $V$.</p>

<p>R functions such as gls() will compute the regression variance (if I knew how gls() does this, it would answer my question).  However I cannot use gls() in this case since I am specifying a user-defined covariance (weighting) matrix, and gls() only accepts a limited set of specific correlation structures.</p>

<p>In fact a possible solution can be found in this <a href=""http://stats.stackexchange.com/questions/14426/prediction-with-gls"">earlier post</a> where an equation for the SEE (or sigma2) for a GLS regression was cited :</p>

<p>GLS calc of SEE: sqrt( sum( ( residuals from linear model) ^ 2 * glsWeight ) ) / sum( glsWeight ) * length( glsWeight ) / residualDegreeFreedom )</p>

<p>However I am unable to ascertain the validity of this equation and cannot find its source reference.</p>
"
"0.0657004319817604","0.0667904674542028"," 56534","<p>I am trying to find a more aesthetic way to present an interaction with a quadratic term in a logistic regression (categorisation of continuous variable is not appropriate).</p>

<p>For a simpler example I use a linear term.</p>

<pre><code>set.seed(1)

df&lt;-data.frame(y=factor(rbinom(50,1,0.5)),var1=rnorm(50),var2=factor(rbinom(50,1,0.5)))
mod&lt;-glm(y ~ var2*var1  , family=""binomial"" , df)

 #plot of predicted probabilities of two levels

new.df&lt;-with(df,data.frame(expand.grid(var1=seq(-2,3,by=0.01),var2=levels(var2))))
pred&lt;-predict(mod,new.df,se.fit=T,type=""r"")

with(new.df,plot(var1,pred$fit))

 #plot the difference in predicted probabilities

trans.logit&lt;-function(x) exp(x)/(1+exp(x))

pp&lt;-trans.logit(coef(mod)[1] + seq(-2,3,by=0.01) * coef(mod)[3]) -trans.logit((coef(mod)[1]+coef(mod)[2]) + seq(-2,3,by=0.01) * (coef(mod)[3]+coef(mod)[4]))

plot(seq(-2,3,by=0.01),pp)
</code></pre>

<h3>Questions</h3>

<ul>
<li>How can I plot the predicted probability difference between the two levels of var2 (rather than the 2 levels separately)  at different values of var1?</li>
<li>Is there a way to define contrasts so I can use these in the glm so I can then pass this to predict? - I need a CI for the difference in probabilities</li>
</ul>
"
"0.0464572209811883","0.0472279924554862"," 56590","<p>I'm reading the technical manual for a <a href=""http://www.nwea.org/sites/www.nwea.org/files/resources/NJ_2011_LinkingStudy.pdf"" rel=""nofollow"">linking study</a> between two assessments.  It's pretty clear that the table is model output from a fitted logistic regression equation.  Here's what pass odds look like on test 2 as a function of score on test 1 (RIT score):  </p>

<p><img src=""http://i.stack.imgur.com/8lwbx.png"" alt=""enter image description here""></p>

<p>It seems silly to use a lookup table that rounds to 5 when the model that made that table could give a better estimate.  But how do I recreate that equation from this output? </p>

<p>I have a good sense of how I would fit this model if I had the raw data, but I'm not sure what to do here.  Not <code>glm(family=binomial)</code> because the data I have is are odds ratios, not pass / no pass (i.e., 1s and 0s), right?</p>

<p>Here's the data:</p>

<pre><code>PASS &lt;- c(0, 0, 0, 0.01, 0.01, 0.01, 0.02, 0.04, 0.06, 0.1, 0.15, 0.23, 
0.33, 0.45, 0.57, 0.69, 0.79, 0.86, 0.91, 0.94, 0.96, 0.98, 0.99, 
0.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)

RIT &lt;- c(120L, 125L, 130L, 135L, 140L, 145L, 150L, 155L, 160L, 165L, 
170L, 175L, 180L, 185L, 190L, 195L, 200L, 205L, 210L, 215L, 220L, 
225L, 230L, 235L, 240L, 245L, 250L, 255L, 260L, 265L, 270L, 275L, 
280L, 285L, 290L, 295L, 300L)
</code></pre>
"
"0.0464572209811883","0.0314853283036575"," 56871","<p>I have a dataset from a bank with demographic data and one variable telling if the customer is a good customer or not (binary variable). I would like to do prediction on if the customer is good or not based on this demographic data.</p>

<p>I managed to do it with a logistic regression, but would like now to compare the result (classification rate) with neural networks. </p>

<p>I found 2 functions from different packages doing that:
- nnet()
- neuralnet()
But those functions seem to be conceived for numerical dependent variables.</p>

<p>Thus my question: is there a possibility to use these functions for a categorical numerical variable (by estimating a posteriori probabilities for instance) or is there another function doing that?</p>

<p>Thanks a lot!</p>

<p>Robin</p>
"
"0.0309714806541255","0.0472279924554862"," 56900","<p>I have a 20-yr dataset of an annual count of species abundance for a set of polygons (~200 irregularly shaped, continuous polygons).  I have been using regression analysis to infer trends (change in count per year) for each polygon, as well as aggregations of polygon data based on management boundaries.</p>

<p>I am sure that there is spatial autocorrelation in the data, which is sure to impact the regression analysis for the aggregated data.  My question is - how do I run a SAC test for time series data?  Do I need to look at the SAC of residuals from my regression for each year (global Moran's I)?  Or can I run one test with all years?</p>

<p>Once I've tested that yes there is SAC, is there an easy was to address this?  My stats background is minimal and everything I've read on spatio-temporal modeling sounds very complex.  I know that R has a distance-weighted autocovariate function - is this at all simple to use?</p>

<p>I'm really quite confused on how to assess/addess SAC for this problem and would very much appreciate any suggestions, links, or references. Thanks in advance!</p>
"
"0.0464572209811883","0.0472279924554862"," 57448","<p>I am running a model (logistic regression) with 20 independent variables in R. </p>

<p>Before running the model I calculated the correlation between all the variables and finally selected my variables by also checking ""visually"" the histograms of each variable in the case of presence and again in the case of absence. In situations where I don't see any obvious distribution associated to both presence &amp; absence, I discard the variable.</p>

<p>I would like to make ""official"" calculations for the level of relation between Presence/Absence and each variable (how much each variable contributes to the Presence/Absence), for example with <code>Cramer's V index</code>, but the available function I find is from the package <code>vcd</code> and has some limitations: 
doesn't give the <code>Cramer's V</code> (as well as the Phi-Coefficient Contingency Coeff.) for each independent variable, and it doesn't run for one independent variable.</p>

<p>I might be missing some other obvious way to do this. Any help is appreciated.</p>
"
"0.0758643241810882","0.0771229887279699"," 57642","<p>I am thinking about a factorial experiment with two factors.  Both factors are ordered factors.  Factor 1 has two levels: small and large.  Factor 2 has four levels: never, sometimes, frequently, and often.  I also want to conduct the experiment in a number of locations, so I will include location as a sort of ""block.""  I expect larger responses for increasing levels of both factors, and I expect an interaction effect, too.  Thus, I have a model as follows:  <code>Response ~ Block + Factor1*Factor2 + error</code>, which will have at least 40 observations, maybe 80, maybe 120, or so on until I can detect an effect.  </p>

<p>I'll be measuring a number of response variables, most of which will be counts or 0 truncated (latency of response).  I'm wondering how to simulate responses from my model with the expectation of a moderate effect size.  I want to know what sample size is appropriate to detect a moderate effect from my treatments, but I'm not familiar enough with simulation to know where to start with such a problem.  Any advice or direction or requests for more information would be much appreciated.</p>

<p>Additional information:  I'm using R to do everything.</p>

<p>EDIT:
I implemented Mark T Patterson's answer to my question modifying it to fit my particular experimental setup and attempt to simulate poisson data, but I get warnings when I run the function.  Fortunately, there are some relevant answers on CrossValidate: <a href=""http://stats.stackexchange.com/questions/27443/generate-data-samples-from-poisson-regression."">Generate data samples from Poisson regression</a>.  I'll keep learning how to simulate other data to match the other kinds of response variables I'll be measuring.</p>
"
"0.0709645772411954","0.072141950116023"," 57811","<p>I'm trying to do LASSO in R with the package glmpath. However, I'm not sure if I am using the accompanying prediction function <em>predict.glmpath()</em> correctly. Suppose I fit some regularized binomial regression model like so:</p>

<pre><code>fit &lt;- glmpath(x = data$x, y=data$y, family=binomial)
</code></pre>

<p>Then I can use predict.glmpath() to estimate the value of the response variable $y$ at $x$ for varying values of $\lambda$ through</p>

<pre><code>pred &lt;- predict.glmpath(fit, newx = x, mode=""lambda"", s=seq(0,10,1),type=""response"")
</code></pre>

<p>However, in the help file it can be seen that there is also an option <em>newy</em>. How should one interpret the result when calling <em>predict.glmpath()</em> with <em>newy = some.y</em>? </p>

<p><strong>[Edit]</strong> An additional question came to mind:</p>

<p>The option <em>type</em> can have the following values, according to the help file:</p>

<pre><code>                      description in help file

""response""            the estimated responses are returned
""loglik""              the log-likelihoods are returned
""coefficients""        the coefficients are returned. The coefficients for the initial input variables are returned (rather than the standardized coefficients)
""link""(default)       the linear predictors are returned
</code></pre>

<p>However, to which linear predictors and coefficients are they referring to? Surely not those of the original model?</p>
"
"0.0808716413062113","0.0822133822214443"," 58448","<p>I'm stuck with a regression modeling problem. I have panel data where the dependent variable is a probability. Below is an excerpt from my data. The complete panel covers more countries and years, however it is unbalanced. What I can observe is the number of events and the number of trials. The event probability was derived from those values (estimation of this probability should be quite good, given the large number of trials). All independent variables are county-year specific.</p>

<pre><code>     country  year  event_prob  events trials    x    x_lag2 ... more variables
  1   Cyprus  2008  0.03902140  11342  290661   4.60   4.13  ...
  2   Cyprus  2009  0.04586650  13482  293940   4.60   4.48  ...
  3   Cyprus  2010  0.05188398  15206  293077   4.60   4.60  ...
  4   Cyprus  2011  0.06433411  18505  287639   5.79   4.60  ...
  5  Estonia  2008  0.07872978  21686  275449   6.02   4.11  ...
  6  Estonia  2009  0.09516270  33599  353069  13.18   4.91  ...
  7  Estonia  2010  0.08645905  36180  418464   7.95   6.03  ...
  8  Estonia  2011  0.07731997  31590  408562   5.53  13.18  ...
  ...
165  USA  2011  0.06100000  9192822  150702000   2.73  3.27  ...
</code></pre>

<p>My goal is to use regression analysis to find out which variables are significant for the event probability. In R-terminology, I'm looking for a model of the form <code>event_prob ~ x + x_lag2 + ...</code> .</p>

<p>The problem is as follows: <code>event_prob</code> has to be between 0 and 1, hence using <code>event_prob ~ x + x_lag2 + ...</code> might not be the best idea. So I was thinking of using the logit transform of <code>event_prob</code> such that <code>logit(event_prob)</code> ranges from $-\infty$ to $\infty$. The first idea was to use the R's <code>plm</code> package, i.e. <code>plm(logit(event_prob)~x+x_lag2,data,index=c(""country"",""year""),model=""random"")</code> or <code>model=""within""</code> (see below). Is that a reasonable approach or am I violating some essential assumptions?</p>

<p>I was also thinking of using panel generalized linear models from the package <code>pglm</code> (with the logit link function), however since I don't know the outcome of the binary events (only the total number of events and trials) is known, I got stuck there. Maybe someone can help me how to proceed here.</p>

<p>Since I have panel data, I'd like to compute both fixed-effects models and random-effects model and then apply the Hausman (1978) test to decide which model is more appropriate.</p>

<p>Do my first attempts at modeling make sense? I'm really not sure how to correctly address this problem. I hope the description of my problem is detailed enough. If not, I'm happy to provide more details</p>

<p>In terms of software, I'd prefer R. SAS and SPSS are also ok since my university has licences for them. I just don't have much experience with them.</p>
"
"0.026822089039291","0.0272670941574606"," 58504","<p>I am running a negative binomial regression of clinic counts in each county in the entire country (~3k counties).  I'd like to at least partially account for the non-independence of neighboring counties by bootstrapping the confidence intervals in a ""clustered"" fashion--e.g. draw an entire state's (50 states total) worth of data at once.  This has become <a href=""http://www.mitpressjournals.org/doi/abs/10.1162/rest.90.3.414#.Vg7SjnUVhBc"" rel=""nofollow"">standard practice</a>, for better or for worse, in the econometric literature.</p>

<p>I could write the code to do this myself, but the <code>boot</code> package seems like it should have the ability to do this somehow, and in general I prefer tested, general solutions to one-off hacks.  Is there a way to coerce the <code>boot</code> package to do a clustered bootstrap?</p>

<p>I tried the <code>strata</code> argument, but that randomizes <em>within</em> strata rather than randomizing which cluster gets taken, as the following code confirms:</p>

<pre><code>dat &lt;- data.frame( cluster=rep(letters[1:5],each=10), x=runif(5*10), stringsAsFactors=TRUE )
boot.stat &lt;- function(dat,idx) {
    print(dat[idx,]$cluster)
    	print(table(dat[idx,]$cluster))
    mean(dat[idx,]$x)
    }
    boot( 
    	data=dat, 
    	statistic=boot.stat, 
    	strata=dat$cluster, 
    stype=""i"", 
    R=5 
)
</code></pre>
"
"0.0608267804924532","0.072141950116023"," 58538","<p>I'm looking for breakpoints in species abundance as a function of spatial distance, using the <code>segmented</code> package for R. 'segmented' appears to return a breakpoint no matter what; I don't understand whether it returns an estimate of the signifcance of the breakpoint (or whether the segmented linear model is better than an unsegmented model). </p>

<p>For instance (R code): </p>

<pre><code>require(segmented)
set.seed(1)
x &lt;- 1:100
y &lt;- rnorm(100) + x # No real breakpoint
y2 &lt;- c(50-x[1:50], x[51:100-50]) + rnorm(100) # Clear breakpoint at 50
plot(x,y)
points(x, y2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/DRysM.png"" alt=""enter image description here""></p>

<pre><code># Segmented model for the unsegmented data
testM &lt;- lm(y ~ x)
testMs &lt;- segmented(testM, seg.Z = ~x, psi=90)
summary(testMs)
</code></pre>

<p>Despite the fact that there is clearly no breakpoint in the data, <code>summary</code> reports that <code>t value for the gap-variable(s) V:  0</code>, and the standard error is fairly small (3.28).</p>

<pre><code>testM2 &lt;- lm(y2 ~ x)
testM2s &lt;- segmented(testM2, seg.Z=~x, psi=50)
summary(testM2s)
</code></pre>

<p>Here the guess is correct (49.78), the standard error is even smaller (0.2), and the t value is the same. How do I interpret this result? </p>

<p><strong>Note:</strong> I have no particular attachment to the <code>segmented</code> package - I just want to test the hypothesis that a segmented regression models my data better than a single-domain regression. But the other breakpoint-analysis packages I've looked at seem to require that points on the domain be evenly-spaced (e.g. timeseries) and there is a single dependent value per independent value. These assumptions are not met for my spatial data.</p>
"
"0.100582833897341","0.109068376629842"," 58657","<p>I'm using a daily time series of sales data that contains about 2 years of daily data points. Based on some of the online-tutorials / examples I tried to identify the seasonality in the data. It seems that there is a weekly, monthly and probably a yearly periodicity / seasonality.</p>

<p>For example, there are paydays, particularly on 1st payday of the month effect that lasts for few days during the week. There are also some specific Holiday effects, clearly identifiable by noting the observations.</p>

<p>Equipped with some of these observations, I tried the following:</p>

<ol>
<li><p>ARIMA (with <code>Arima</code> and <code>auto.arima</code> from R-forecast package), using regressor (and other default values needed in the function).  The regressor I created is basically a matrix of 0/1 values:</p>

<ul>
<li>11 month (n-1) variables</li>
<li>12 holiday variables</li>
<li>Could not figure out the payday part...since it's little more complicated effect than I thought. The payday effect works differently, depending on the weekday of the 1st of month.</li>
</ul>

<p>I used 7 (i.e., weekly frequency) to model the time series. I tried the test - forecasting 7 days at a time. The results are reasonable: average accuracy for a forecast of 11 weeks comes to weekly avg RMSE to 5%.</p></li>
<li><p>TBATS model (from R-forecast package) - using multiple seasonality (7, 30.4375, 365.25) and obviously no regressor. The accuracy is surprisingly better than the ARIMA model at weekly avg RMSE 3.5% .</p>

<p>In this case, the model without ARMA errors perform slightly better. Now If I apply the coefficients for just the Holiday Effects from the ARIMA model described in #1, to the results of the TBATS model the weekly avg RMSE improves to 2.95%</p></li>
</ol>

<p>Now without having much background or knowledge on the underlying theories of these models, I'm in a dilemma whether this TBATS approach is even a valid one. Even though it's improving the RMSE significantly in the 11 weeks test, I'm wondering whether it can sustain this accuracy in the future. Or even if applying Holiday effects from ARIMA to the TBATS result is justifiable. Any thoughts from any / all the contributors will be highly appreciated. </p>

<p><a href=""https://s3.amazonaws.com/CKI-FILE-SHARE/TS+Test+Data.txt"">Link for Test Data</a></p>

<p>Note: Do ""Save Link As"", to download the file.</p>
"
"0.107288356157164","0.102251603090477"," 58962","<p>I am building a multiple regression model - wrapped in a function - with one dependent variable and a dozen independent variables. The reason why I am building a function is that I need to do this analysis with approximately 75 different datasets. </p>

<p>The challenge is that the independent variables correlate better with the dependent variable when they are lagged in time. Unfortunately, not all time lags are the same for each variable and I would like to determine the optimal mix of time lags for each variable while getting the most optimum Adjusted R^2 value for the multiple regression model. Moreover, after building an initial model I will try to reduce the model using the <code>step(modelbase, direction=""both"")</code> function on the model. </p>

<p>In the approach I currently have I time lag all the independent variables with the same number of weeks. This results in the best possible model where all independent variables have the same time lag, but I believe (with a valid hypothesis supporting this) that there is a better model out there when we differ the time lag for each independent variable. My question is what is the best strategy to determine the best fit model without making the number of options huge. If I want to determine between 0 and 20 weeks time lag in weekly steps for 12 independent variables I am quickly up to trying to find a match between 4.096e+15 variables (=20^12). </p>

<p>I can imagine reducing the problem with the following strategy: Start by finding the best fit model with one independent variable at different time lags. The second step will be to add a second independent variable with its different time lags and find the best model with the two independent variables where the second is tried at different time lags while the first is kept constant. Then add a third variable for which we take a similar approach as the second by keeping the first two variables constant and change try the third with different time lags. Something tells me that this strategy might be decent approach, but something that there also might be a better overall model that contains the not optimal variables for each individual independent variable. </p>

<p>Is there anybody who shine some light on how to tackle this challenge? </p>
"
"0.0715255707714427","0.0818012824723818"," 59058","<p>I'm trying to forecast a seasonal time series based on its historical values, and also two more time series (that are seasonal themselves.)  </p>

<p>I'm trying to use an <strong>auto.arima</strong>, and I'm going to input the other two time series (the exogeneous regressors) as a contatenated list of dummy variables, in auto.arima's <strong>xreg</strong> parameter.</p>

<p>I am having difficulty how to use the forecast function after this point.  I've written up the following code, but I don't understand what I should put in the <strong>xreg</strong> and <strong>newxreg</strong> parameters of the forecast function.</p>

<pre><code>tempfit&lt;-auto.arima(dnew, xreg=dExt)
plot(forecast(tempfit, xreg=dnew1,newxreg=dExt1))
</code></pre>

<p>Also, my data points for these three series were all values per day that had a seven day seasonality. In order to let auto.arima calculate the (p,q,d) for seasonality, I converted them to time series with a frequency of 7. Now, after forecasting is done, the plot shows one unit for every seven days.  How can I covert this back to one unit per day?</p>

<p>Further, do you happen to know how we can input a set of external regressors to an ETS model?</p>

<p>I would greatly appreciate your inputs!</p>

<p>Thank you.</p>

<p><strong>EDIT</strong>:</p>

<p>I just saw the following page from Dr. Hyndman:
<a href=""http://stats.stackexchange.com/questions/34493/time-series-modeling-with-dynamic-regressors-in-sas-vs-in-r"">Time series modeling with dynamic regressors in SAS vs. in R</a></p>

<p>Is it safe to assume that I don't need to enter a newxreg parameter for my forecast?</p>

<p>Also, I really want to know if it's statistically correct to use the two external regressors in xreg, but then also use a number of dummy variables in xreg that will represent the seasonality of these two variables.  </p>
"
"0.0758643241810882","0.0771229887279699"," 59069","<p>I've been spending quite some time to figure out how I can get the best R squared value from randomization of some values in a linear regression equation. I have allele frequency data and 14 environmental gradient data. Allele frequency value is fixed, but 2~14 combinations of the 14 environmental variables are used.</p>

<p>My aim here is to find a combination of the environmental variables that yield high R squared value. Here is a simple linear regression equation code that returns R squared value.</p>

<pre><code>&gt; summary(lm(allele ~ compositevalues))$r.squared
</code></pre>

<p>""compositevalues"" is a sum of standardized 14 different environmental values. I want to make 2~14 combinations of variables (with no replacement:i.e. var1+var2, var1+var3, var1+var4, var1+var2+var3, var2+var3+var4, var2+var3, var2+var4, var3+var4....etc. but not var1+var1+var2) as I mentioned above.</p>

<p>I would appreciate it if you could instruct me on how to write a code that generate random combination of (sum of ) the variables and returns combinations of variables that are used with R squared value of >0.4.</p>

<p>I was looking for permutation and resampling function in R, couldn't find ones that serve my purpose.....</p>

<p>Below is a part of my data set.</p>

<pre><code> 1.  Location   allele           var1             var2          var3
 2.  site1,     0.230271924,    -0.872093023,   -0.696403914,   -0.398671096
 3.  site2,     -1.061563963,   0.944767442,    1.104640692,    -0.398671096
 4.  site3,     -0.524508594,   0.339147287,    -1.296752116,   0.431893688
 5.  site4,     0.027061785,    2.156007752,    -0.096055712,   0.431893688
 6.  site5,     0.186726894,    0.944767442,    1.104640692,    -0.398671096
 7.  site6,     -0.118088315,   -0.266472868,   -0.696403914,   -0.398671096
 8.  site7,     -1.003503923,   0.339147287,    -1.296752116,   0.431893688
 9.  site8,     -1.569589312,   0.339147287,    -1.296752116,   0.431893688
 10. site9,     -1.119624003,   0.944767442,     0.50429249,    -1.22923588
 11. site10,    1.362442702,    -1.477713178,   -0.096055712,   1.262458472
 12. site11,    0.215756914,    0.339147287,    -1.897100318,   1.262458472
 13. site12,    0.665722223,    -1.477713178,   -0.096055712,   1.262458472
 14. site13,    1.086657513,    -1.477713178,   -0.096055712,   1.262458472
 15. site14,    -0.001968235,   0.339147287,    1.704988894,    -2.059800664
 16. site15,    -1.656679372,   0.339147287,    1.104640692,    -2.059800664
 17. site16,    0.433482064,    0.339147287,    1.704988894,    -2.059800664
 18. site17,    -0.814808794,   1.550387597,    -1.296752116,   -0.398671096
 19. site18,    -0.713203724,   1.550387597,    -0.696403914,   -0.398671096
</code></pre>

<p>Many thanks!</p>
"
"0","0.0272670941574606"," 59434","<p>I have to compare the slopes of 2 regression lines with R. The 2 regressions are made with the same parameters in 2 different locations.</p>

<p>I did my regressions with the function lm(). Now I have the results but I don't know how to compare them..</p>

<p>I tried with student test or ANOVA but it requires more than the 2 slopes.</p>

<p>I looked for an answer during something like 2 hours but didn't find, so I ask the question here : How should I do my test? </p>

<p>Thank you in advance,
b.raoul</p>
"
"0.0848188929679971","0.0862261227118454"," 59530","<p>With a colleague, we are working on a dataset containing ~5000 continuous variables for 120 individuals belonging to 8 classes.</p>

<p>We want to <strong>estimate the relative importance of each variable</strong> to explain the classes.
We have used a random forest approach with some success.
Now, we could like to go deeper by considering the fact that <strong>the 8 classes we fit are unequally distant from each other</strong>.
In fact, in our case <strong>we can <em>a priori</em> generate a distance matrix (<em>i.e.</em> cost matrix) for all possible pairs of classes</strong>.</p>

<p>My (very limited) understanding of random forest is that, for regression problems,
the error $E$ is computed by the mean square difference between the OOB sample and the prediction for the same sample:</p>

<p>$E = n^{-1}\sum\limits_{i=1}^n{{(y_i-\hat{y}_i)}^2}$</p>

<p>Where $y_i$ is the predicted value and $\hat{y}_i$ the real value of an out-of bag-sample $i$.</p>

<p>Ultimately, the calculation of the variable importance depends on how the error is computed (right?).</p>

<p>In our case, I would like to use a modified loss function, for instance:</p>

<p>$E = n^{-1}\sum\limits_{i=1}^n{M_{y_i,\hat{y}_i}}$</p>

<p>Where $M$ is predefined a distance matrix; so $M_{a,b}$ is the distance between class $a$ and $b$.
In this way the misclassification error would be more important if $y_i$ and $\hat{y}_i$ represent distant classes and, ultimately, <strong>the variable importance should be more relevant</strong>.</p>

<p>My questions are:</p>

<ol>
<li>Does this approach make sense to you, or am I missing something?</li>
<li>Can you think of any study that has used something similar.</li>
<li>We have so far used the <code>randomForest</code> package in R. It does not seem possible to use it in combination with an a priori distance matrix between classes. Do you know if this is already implemented somewhere?</li>
</ol>

<p><strong>EDIT</strong></p>

<p>I believe this is a very frequent problem in my field, biology, because we deal with classes for which relations can be represented and quantified by trees (dendrograms), often because of there lineage.</p>

<p>After some research, it appears that my question is about using a <strong>cost-sensitive</strong> version of random forest. In this respect, it is very similar to <a href=""http://stats.stackexchange.com/questions/46963/how-to-control-the-cost-of-misclassification-in-random-forests"">this question</a>.
I specificity want to use a <strong>cost matrix</strong> rather than a cost vector though.
It there any ontological reason why it is not possible or is it simply not implemented?</p>
"
"0.053644178078582","0.0545341883149212"," 59741","<p>say I have a sensor that measures temperature, pressure ++, and want to use this data to predict some quantity ""A"". If I use multivariate regression, I can simply implement a model of the form A=a0+a1x1+a2x2+..., and whenever I have new measurements I can use the model to make predictions.</p>

<p>If I on the other hand make a predictive model using random forests, I'm not really sure how to use it. I've used the caret package to split my data into training and test sets, and do automatic feature selection using random forest and cross-validation. I get good predictions on the test set, but have no idea how to implement these trees to use in say a digital signal processor. In R I just use the predict() function, but this is obviously not available outside of R.</p>

<p>This is probably a stupid questing, but it's the best I can do.</p>

<p>Any suggestions are welcome.</p>
"
"0.103881504159667","0.105605001571314"," 59784","<p>I have a dataset which is statistics from a web discussion forum. I'm looking at the distribution of the number of replies a topic is expected to have. In particular, I've created a dataset which has a list of topic reply counts, and then the count of topics which have that number of replies.</p>

<pre><code>""num_replies"",""count""
0,627568
1,156371
2,151670
3,79094
4,59473
5,39895
6,30947
7,23329
8,18726
</code></pre>

<p>If I plot the dataset on a log-log plot, I get what is basically a straight line:</p>

<p><img src=""http://i.stack.imgur.com/W16yN.png"" alt=""Data plotted on log-log scale""></p>

<p>(This is a <a href=""http://en.wikipedia.org/wiki/Zipf%27s_law"">Zipfian distribution</a>). Wikipedia tells me that straight lines on log-log plots imply a function that can be modelled by a monomial of the form $y = ax^k$. And in fact I've eyeballed such a function:</p>

<pre><code>lines(data$num_replies, 480000 * data$num_replies ^ -1.62, col=""green"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/u8eeD.png"" alt=""Eyeballed model""></p>

<p>My eyeballs obviously aren't as accurate as R. So how can I get R to fit the parameters of this model for me more accurately? I tried polynomial regression, but I don't think that R tries to fit the exponent as a parameter - what is the proper name for the model I want?</p>

<p>Edit: Thanks for the answers everyone. As suggested, I've now fit a linear model against the logs of the input data, using this recipe:</p>

<pre><code>data &lt;- read.csv(file=""result.txt"")

# Avoid taking the log of zero:
data$num_replies = data$num_replies + 1

plot(data$num_replies, data$count, log=""xy"", cex=0.8)

# Fit just the first 100 points in the series:
model &lt;- lm(log(data$count[1:100]) ~ log(data$num_replies[1:100]))

points(data$num_replies, round(exp(coef(model)[1] + coef(model)[2] * log(data$num_replies))), 
       col=""red"")
</code></pre>

<p>The result is this, showing the model in red:</p>

<p><img src=""http://i.stack.imgur.com/JudrC.png"" alt=""Fitted model""></p>

<p>That looks like a good approximation for my purposes.</p>

<p>If I then use this Zipfian model (alpha = 1.703164) along with a random number generator to generate the same total number of topics (1400930) as the original measured dataset contained (using <a href=""http://coderepos.org/share/browser/lang/cplusplus/boost-supplement/trunk/boost_supplement/random/zipf_distribution.hpp"">this C code I found on the web</a>), the result looks like:</p>

<p><img src=""http://i.stack.imgur.com/soakm.png"" alt=""Random number generated results""></p>

<p>Measured points are in black, randomly generated ones according to the model are in red.</p>

<p>I think this shows that the simple variance created by randomly generating these 1400930 points is a good explanation for the shape of the original graph.</p>

<p>If you're interested in playing with the raw data yourself, I have <a href=""http://s3.chickensmoothie.com/misc/research/topic_reply_counts_for_1400930_forum_topics.txt.zip"">posted it here</a>.</p>
"
"0.026822089039291","0.0272670941574606"," 59990","<p>I have tried the <code>clm</code> and <code>clmm</code> functions in the <code>ordinal</code> package and the <code>lrm</code> function in the <code>rms</code> package, but none of them has a good performance. The best accuracy rate is around 70%.</p>

<p>Is there any other package implementing ordinal regression in R? Or I should consider replacing the features in the model in order to improve the accuracy?</p>
"
"0.0402331335589365","0.0545341883149212"," 60003","<p>Here I perform a GLS regression in R and the degrees of freedom is reported as ""Degrees of freedom: 60 total; 58 residual"". In this regression I see five parameters that are being estimated: the slope of the regression line, the intercept of the regression line, the residual standard deviation, the constant of the variance function, and the power of the variance function. When I go to generate prediction intervals for the regression line what degrees of freedom should I use? Anticipating the answer is not 55, why aren't the degrees of freedom 55?</p>

<pre><code>library(nlme)

X &lt;- c(1,1,1,1,1,1,1,1,1,1,4,4,4,4,4,4,4,4,4,4,
  10,10,10,10,10,10,10,10,10,10,20,20,20,20,20,20,
  20,20,20,20,30,30,30,30,30,30,30,30,30,30,40,40,   
  40,40,40,40,40,40,40,40)

Y &lt;- c(1.07,1.01,0.99,1.09,0.94,1.00,1.01,0.98,1.00,
  1.03,3.66,3.75,3.77,3.92,4.08,3.99,3.95,4.10,
  3.88,4.04,10.13,10.2,9.77,10.28,8.71,9.79,9.82,
  9.85,10.07,9.63,20.22,19.46,19.02,20.06,20.94,
  19.92,19.96,20.04,19.67,19.96,31.04,31.4,31.84,
  30.77,32.13,31.17,30.36,29.95,30.74,30.67,41.14,
  40.29,42.77,38.36,39.17,39.61,40.73,39.42,40.72,
  40.24)

m &lt;- data.frame(X,Y)

fit &lt;- gls(Y ~ X,weights=varConstPower(form = ~ X),data=m)
summary(fit)
</code></pre>
"
"0.0379321620905441","0.0385614943639849"," 60109","<p>I would like to understand what the following code is doing. The person who wrote the code no longer works here and it is almost completely undocumented. I was asked to investigate it by someone who thinks ""<em>it's a bayesian logistic regression model</em>""</p>

<pre><code>bglm &lt;- function(Y,X) {
    # Y is a vector of binary responses
    # X is a design matrix

    fit &lt;- glm.fit(X,Y, family = binomial(link = logit))
    beta &lt;- coef(fit)
    fs &lt;- summary.glm(fit)
    M &lt;- t(chol(fs$cov.unscaled))
    betastar &lt;- beta + M %*% rnorm(ncol(M))
    p &lt;- 1/(1 + exp(-(X %*% betastar)))
    return(runif(length(p)) &lt;= p)
}
</code></pre>

<p>I can see that it fits a logistic model, takes the transpose of the Cholseky factorisation of the estimated covariance matrix, post-multiplies this by a vector of draws from $N(0,1)$ and is then added to the model estimates. This is then premultiplied by the design matrix, the inverse logit of this is taken, compared with a vector of draws from $U(0,1)$ and the resulting binary vector returned. But what does all this <strong><em>mean</em></strong> statistically ?</p>
"
"0.0547503599848004","0.0667904674542028"," 60154","<p>I have proportion data (percentage viewership of TV programs) that i'd like to model as a function of various demographics (age, sex etc.) and time (year). After surveying options for appropriate multiple regression models, I'm debating between the following two strategies:</p>

<p>1) fit a beta regression model after dividing the percentage data by 100 and adjusting the range slightly so that values of zero and one do not occur.</p>

<p>2) fit an OLS model after logit transforming the percentage data (again, divided by 100 and adjusted slightly) so that the dependent variable is mapped to the Real line.</p>

<p>One key consideration is that i'd like to make the results as intuitive as possible to a non-statistical audience. So, interpretations such as ""for a one unit change in X we get a percent change in Y"", or something like that, would be most welcome.</p>

<p>Can anyone outline pros and cons of these two approaches in this regard? </p>

<p>It seems to me that using beta regression with a logit link, and then calculating odds ratios may lead to nice ""percent change"" explanations. The coefs from the OLS model would also be on the ln(odds) scale, so I assume I could also do the same for that model. My data are too large to share, but I ran both models in R and there are only minor differences in the coefs.</p>
"
"0.0892693083195917","0.0907503748778111"," 60648","<p>I am trying to forecast electricity consumption in GWh for 2 years ahead (from June 2013 ahead), using R (the forecast package). For that purpose, I tried regression with ARIMA errors. I fitted the model using the <code>auto.arima</code> function, and I used the following variables in the <code>xreg</code> argument in the <code>forecast.Arima</code> function: </p>

<p>- Heating and Cooling Degree Days,<br>
- Dummies for all 12 months and<br>
- Moving holidays dummies (Easter and Ramadan)  </p>

<p>I have several questions regarding the model:</p>

<p>1) Is it correct to use all 12 dummies for monthly seasonality, since when I tried to include 11, the function returned error. The <code>Auto.arima</code> function returned the model ARIMA(0,1,2)</p>

<p>2)The model returned the following coefficients (I won't specify all of them as there are too many coefficients):</p>

<pre><code>ma1      ma2     HDD     CDD   January  February  March     April
-0.52 -0.16      0.27    0.12  525.84   475.13    472.57    399.01
</code></pre>

<p>I am trying to determine the influence of the temperature component over electricity load. In percentages, (interpreting the coefficients just as with the usual regression) the temperature components (<code>HDD</code>+<code>CDD</code>) account for 11,3% of the electricity consumption. Isn't this too little, considering the fact that the electricity consumption is mostly influenced by the weather component? On the other hand, taking look at the dummies' coefficients, it turns out that the seasonality accounts for the greater part of the load. Why is this? Is the model completely incorrect?</p>

<p>I tried linear regression, and the temperature component accounts for 20%, but it is still a low percentage. Why is this?</p>

<p>3) I am obviously making some mistakes in the use of <code>forecast.Arima</code> or the plot function parameters since when I plot the forecasts, I get a picture of the original time series which is continued (merged) with the forecasts for the whole time series period (from 2004 until 2015). I don't know how to explain this better, I tried to paste the picture, but it seems I cannot paste pictures here.</p>
"
"0.080466267117873","0.0818012824723818"," 60760","<p>let <code>m</code> be my matrix of data</p>

<pre><code>      x_i y_i
 [1,] 0.0   0
 [2,] 0.0   0
 [3,] 0.0   0
 [4,] 0.0   0
 [5,] 0.1   0
 [6,] 0.2   0
 [7,] 0.3   0
 [8,] 0.4   0
 [9,] 0.5   0
[10,] 0.6   0
[11,] 0.0   1
[12,] 0.0   1
[13,] 0.0   1
[14,] 0.9   1
[15,] 1.0   1
</code></pre>

<p>My aim is to study the logistic regression <code>y~x</code>, where the covariate <code>x</code> has observations <code>m[,1]</code> and similarly for <code>y</code>.
Please note that we have no complete separation in the data <em>but</em> the ""anomalous"" entries in rows <code>m[11,], m[12,]</code> and <code>m[13,]</code> all correspond to observations with <code>x_i=0</code>.</p>

<p>I expect <code>glm</code> to diverge as the likelihood function reaches no maximum in the ray  $k\beta$, for $k\rightarrow \infty$ and $\beta=(-0.7,1)$. </p>

<p>Using <code>glm</code> with 1 iteration I get the output </p>

<pre><code>  Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.2552     0.7648  -1.641    0.101
x             1.6671     1.7961   0.928    0.353

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.275  on 13  degrees of freedom
AIC: 22.275

Number of Fisher Scoring iterations: 1
</code></pre>

<p>with an error message (the algorithm does not converge). 
Moreover, with the default number of iterations <code>(=25)</code> the output is</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.1257     0.7552  -1.491    0.136
x             1.4990     1.6486   0.909    0.363

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.246  on 13  degrees of freedom
AIC: 22.246

Number of Fisher Scoring iterations: 4
</code></pre>

<p>and no error warning. </p>

<p>I see a contradiction; even in presence of 1 iteration the algorithm does not converge but the output is ""finite"" (I have not explicitly computed the inverse of the Hessian of the likelihood function, unfortunately). Moreover, with 25 iterations the warning message disappears and the output is still finite.</p>

<p>What do you think about this situation?
 Is it possible that <code>glm</code> stops automatically after the first iteration?
Thank you, Avitus</p>
"
"0.0975796819488693","0.105811867590468"," 61138","<p>I am trying to calculate the marginal effects of a multinomial logistic regression. To do this I use the <code>mlogit</code> package and the <code>effects()</code> function.</p>

<p>Here is how the procedure works (source : <code>effects()</code> function of <code>mlogit</code> package) :</p>

<pre><code>data(""Fishing"", package = ""mlogit"")
Fish &lt;- mlogit.data(Fishing, varying = c(2:9), shape = ""wide"", choice = ""mode"")
m &lt;- mlogit(mode ~ price | income | catch, data = Fish)
# compute a data.frame containing the mean value of the covariates in the sample
z &lt;- with(Fish, data.frame(price = tapply(price, index(m)$alt, mean), 
	catch = tapply(catch, index(m)$alt, mean), 
income = mean(income)))
# compute the marginal effects (the second one is an elasticity
effects(m, covariate = ""income"", data = z)
effects(m, covariate = ""price"", type = ""rr"", data = z)
effects(m, covariate = ""catch"", type = ""ar"", data = z)
</code></pre>

<p>I have no problem with first step (<code>mlogit.data()</code> function). I think my problem is in the specification of the multinomial regression.</p>

<p>My regression (for example with three variables) is on the form: <code>Y ~ 0 | X1 + X2 + X3</code>. When I try to estimate the marginal effects for a model with 2 variables, there is no problem, however for 3 variables R console returns me the following error: ""Error in if (rhs% in% c (1, 3)) {: argument is of length zero "" (translation from error in R console in french).</p>

<p>To understand what is my problem I tried to perform a multinomial regression of similar shape on the dataset ""Fishing"", i.e.,: <code>mode ~ 0 | income + price + catch</code> (even if this form has no ""economic"" sense.) Again the R console returns me the same error for 3 variables but manages to estimate these effects for a model with two variables.</p>

<p>This leads me to think that my problem really comes from the specification of my multinomial regression.  Do you know how I could find a solution to my problem? Or could you suggest another logit multinomial regression form ?</p>

<p>Thank you for your help :)</p>
"
"0.026822089039291","0.0272670941574606"," 61144","<p>I am a biology student. We do many Enzyme Linked Immunosorbent Assay (ELISA) experiments and Bradford detection. A 4-parametric logistic regression (<a href=""http://www.miraibio.com/blog/2010/08/the-4-parameter-logistic-4pl-nonlinear-regression-model/"" rel=""nofollow"">reference</a>) is often used for regression these data following this function:
$$
F(x) = \left(\frac{A-D}{1+(x/C)^B}\right) + D 
$$
How can I do this in <code>R</code>? I want to get the $A$, $B$, $C$ and $D$ values and plot the curve.</p>

<p>PS. If I have some data, how can I use the calculated function $F(x)$ to get the value? I mean how do I go from ""data -> F(x) -> value""?</p>
"
"0.053644178078582","0.0545341883149212"," 61344","<p>In a paper by <a href=""http://www.ncbi.nlm.nih.gov/pubmed/23628224"" rel=""nofollow"">Faraklas et al</a>, the researchers create a Necrotizing Soft-Tissue Infection Mortality Risk Calculator. They use logistic regression to create a model with mortality from necrotizing soft-tissue infection as the main outcome and then calculate the area under the curve (AUC). They use the bootstrap method to find the ""bootstrap optimism-corrected ROC area.""</p>

<p>If I were to do this in <code>R</code>, how would it look like? The code I have been toying with looks something like below:</p>

<pre><code>library(boot)
library(ROCR)

auc_calc &lt;- function(data, indices, outcomes) {
  d &lt;- data[indices,]
  # Using glm for logistic regression
  # Do I recreate the glm model for each dataset?
  fit &lt;- glm(outcomes[indices,] ~ X1 + X2 + X3, data=d, family=binomial)
  fit.predict &lt;- predict(fit, type=""response"")

  # Using ROCR to calculate AUC
  pred &lt;- prediction(fit.predict, outcomes[indices,])
  perf &lt;- performance(pred, ""auc"")

  # Returning the AUC
  return(perf@y.values[[1]])
}

boot.results &lt;- boot(data=my.data, statistic=auc_calc, R=10000, outcomes=my.outcomes)
</code></pre>

<p>Is this correct? Or am I doing something wrong - namely should I be passing in a glm model rather than recalculating it each time? As always thanks for the help.</p>
"
"0.0547503599848004","0.0667904674542028"," 61480","<p>I am trying to estimate a GAM regression model using the implementation of <code>gam</code> from the <code>mgcv</code> package. I have a working Gaussian model for the dispersion and a log link for the linear predictors but I receive the error </p>

<pre><code>&gt;""Error in eval(expr, envir, enclos) : cannot find valid starting values: please specify some"". 
</code></pre>

<p><strong>Edit 1</strong> - The exact syntax is </p>

<pre><code>splineWAR &lt;- gam(WAR ~ s(zAge, bs=""cr"") + s(zAdjProd, bs=""cr"") + s(zSOPct, bs=""cr"") + s(zBBPct, bs=""cr""), family=gaussian(link=""log""), data = mydata,  start=c(0, 0, 0, 0, 0))
</code></pre>

<p>I have read the relevant threads <a href=""http://stackoverflow.com/questions/13567169/glm-function-in-r-with-log-link-not-working"">here</a> and <a href=""http://stackoverflow.com/questions/8212063/r-glm-starting-values-not-accepted-log-link"">here</a> but have unable to apply the steps suggested to a multiple regression. For instance, when I try and set start values for the 5 variables in my regression (1 dependent and 4 independent) by adding the <code>start=c(n1, n2, n3, n4, n5)</code> argument (where the <code>n</code>'s are the mean of the relevant variable), I receive the same error even though I am seemingly copying the syntax exactly from the first link. Can anyone make a suggestion as to what I should try next? Thanks. </p>

<p><strong>Edit 2</strong> The code in the <code>gam.fit</code> function that runs right before the error is - </p>

<pre><code>if (!(validmu(mu) &amp;&amp; valideta(eta))) 

stop(""Can't find valid starting values: please specify some"")
</code></pre>
"
"0.107288356157164","0.102251603090477"," 61532","<p>I am trying to find the best fit between an species dataset and prevailing climatic conditions, in order to be able to predict the environmental conditions from the species dataset (paleoclimate research). </p>

<p>I have 15 species(sp1-sp15), expressed as relative amounts (some are 0).
I have done some data exploration in excel, and have seen that I get good fits using the ratio  sp2/(sp1+sp2). </p>

<p>I have done multiple linear regression on this dataset, but none give a correlation as good as the ratio I stumbled on. The ultimate goal is of course to test whether this ratio I stumbled upon is yields the best model to explain changes in the climate variable.</p>

<p>I would thus like to create the ratios: <code>sum(spj)/sum(spj)</code>, where spj refers to one of the species. Here, both the numerator and denominator can be any possible combination of all species. </p>

<p>Can I generate a formula model to include a ratio? Can I then select the best model using regbsubsets?</p>

<p><strong>EDIT</strong>
Based on answers below, this will not be possible for all 15 variables. How would it be possibel for 5 variables (I have seen this published before, using R).</p>

<p><strong>EDIT</strong>
On stackoverflow (where I posted the same question), I got the comment to use the function (I), to write expressions inside formulae. However, if I use</p>

<pre><code>leapsMAT&lt;-regsubsets(x1 ~ I(1+ (y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 + y12+ y13 + y14 +y15 )/(y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 + y12+ y13 + y14 + y15)), force.in= FTFALSE,nvmax=15,data=my.data, nbest=1)
</code></pre>

<p>the regsubsets doesn't recognize the variables as variables, of course. How do I implement the I(function), or is there another way to deal with this?</p>

<p>At the moment I have obtained the same linear regression using two methods:</p>

<pre><code>library(leaps)
attach(my.data)
FTFALSE&lt;-c(FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE)
leapsMAT&lt;-regsubsets(x1 ~ 1+ y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 + y12+ y13 + y14 + y15 , force.in= FTFALSE,nvmax=15,data=my.data, nbest=1)
</code></pre>

<p>And, using a longer method specified here (implemented from somewhere else on the internet):</p>

<pre><code>allModelsList &lt;- apply(regMat, 1, function(x) as.formula (paste(c(""x1 ~ 1"", namevar2[x]),collapse="" + "" )))
allModelsList
warnings()

#Calculating the model
my.data
allModelsResults &lt;- lapply(allModelsList, function(x) lm(x, data=my.data))
allModelsResults

dfCoefNum   &lt;- ldply(allModelsResults, function(x) as.data.frame(t(coef(x))))

dfStdErrors &lt;- ldply(allModelsResults, function(x) as.data.frame(t(coef(summary(x))[, ""Std. Error""])))


dftValues   &lt;- ldply(allModelsResults, function(x) as.data.frame(t(coef(summary(x))[, ""t value""])))

dfpValues   &lt;- ldply(allModelsResults, function(x) as.data.frame(t(coef(summary(x))[, ""Pr(&gt;|t|)""]))) 
dfpValues
(warnings)
# rename DFs so we know what the column contains
names(dfStdErrors) &lt;- paste(""se"", names(dfStdErrors), sep=""."")
names(dftValues) &lt;- paste(""t"", names(dftValues), sep=""."")
names(dfpValues) &lt;- paste(""p"", names(dfpValues), sep=""."")

# p-value for overall model fit
calcPval &lt;- function(x){
    fstat &lt;- summary(x)$fstatistic
    pVal &lt;- pf(fstat[1], fstat[2], fstat[3], lower.tail = FALSE)
    return(pVal)
}

# Before creating ONE data frame with all important entries,
# we need to compute some more indices 
NoOfCoef &lt;- unlist(apply(regMat, 1, sum))
R2       &lt;- unlist(lapply(allModelsResults, function(x)
                          summary(x)$r.squared))
    adjR2    &lt;- unlist(lapply(allModelsResults, function(x)
                              summary(x)$adj.r.squared))
RMSE     &lt;- unlist(lapply(allModelsResults, function(x)
                          summary(x)$sigma))
fstats   &lt;- unlist(lapply(allModelsResults, calcPval))



# now we can combine all the data into one data frame
results &lt;- data.frame( model = as.character(allModelsList),
                       NoOfCoef = NoOfCoef,
                       dfCoefNum,
                       dfStdErrors,
                       dftValues,
                       dfpValues,
                       R2 = R2,
                       adjR2 = adjR2,
                       RMSE = RMSE,
                       pF = fstats  )
results[1:20,]
# round the results
results[,-c(1,2)] &lt;- round(results[,-c(1,2)], 3)
results

model.maxRadj&lt;-which(results$adjR2 == max(results$adjR2), arr.ind = TRUE)
maxRadj&lt;-results[model.maxRadj,]
</code></pre>

<p>Many thanks in advance! Please let me know if more information is required.</p>
"
"0.0967084173462244","0.0907503748778111"," 61711","<p>In this <a href=""http://stats.stackexchange.com/questions/61547/help-me-fit-this-non-linear-multiple-regression-that-has-defied-all-previous-eff"">thread</a>, I laid out a problem involving fitting a model that attempts to use minor league baseball statistics to predict success at the major league level (explained in full in the thread). After doing further research outside of the thread, I have come to the conclusion that a zero-inflated negative binomial model is likely the best fit given that I believe there are two processes generating the data. The first process determines whether a player will make the majors and once the player reaches the majors, a second process governs their success (as measured by WAR - also explained in the linked thread). </p>

<p>I ran the model and the ZINB model appears to be a reasonable fit given the following diagnostic plot of fitted values vs residuals (broken into two plots to make it easier to inspect visually).</p>

<p><img src=""http://i.imgur.com/GO28KA3.jpg"" alt=""image3"">
<img src=""http://i.imgur.com/6J1QGOj.jpg"" alt=""image1"">
<img src=""http://i.imgur.com/Qe3vmGx.jpg"" alt=""image2""></p>

<p><strong>EDIT 2: Here is a plot of the Pearson residuals.</strong></p>

<p><img src=""http://i.imgur.com/eyRBF7C.jpg/"" alt=""image3""></p>

<p><strong>EDIT 3: Here is a plot of the Pearson residuals vs the fitted values.</strong> </p>

<p><img src=""http://i.imgur.com/nNsgFPl.jpg"" alt=""image4""></p>

<p>Although this is a big improvement over my previous models, there is clearly a bias for the model to underestimate a player's career WAR i.e., a majority of the residuals are greater than zero. <strong>EDIT: It turns out that the plot is misleading due to the high number of overlapping residuals. In reality, only 10% of the residuals are greater than zero.</strong> I am guessing that this may be improved by either a) a better specification of the functional form of the covariates or b) using a different yet similar model e.g., ZIP. Given that this type of regression goes well beyond what I have worked with before, I would appreciate any suggestions on further diagnostics I can use to both test this model and compare it to others and how to improve the functional form of the covariates given that the R function I have used, zeroinfl (from the pscl package), does not appear to be that flexible. Thank you!</p>
"
"0.0969560705490221","0.0985646681332268"," 61869","<p>I am trying to replicate a path analysis SEM model using Lavaan in R, and was very confused about the results that it gave regarding the model fit statistics. </p>

<p><strong>The code is as follows:</strong> </p>

<pre><code>#Import Package
library(lavaan)

#Input Correlation Matrix
sigma &lt;- matrix(c(1.00, -0.03,  0.39, -0.05, -0.08,
                 -0.03,  1.00,  0.07, -0.23, -0.16,
                  0.39,  0.07,  1.00, -0.13, -0.29,
                 -0.05, -0.23, -0.13,  1.00,  0.34,
                 -0.08, -0.16 ,-0.29,  0.34,  1.00), nr=5, byrow=TRUE)
rownames(sigma) &lt;-c(""Exercise"", ""Hardiness"", ""Fitness"", ""Stress"", ""Illness"")
colnames(sigma) &lt;-c(""Exercise"", ""Hardiness"", ""Fitness"", ""Stress"", ""Illness"")

#Create Covariance Matrix
sdevs &lt;-c(66.5, 3.8, 18.4, 6.7, 624.8)
covmax &lt;- cor2cov(sigma, sdevs)
as.matrix(covmax)

#Specify Model 
mymodel&lt;-'Illness ~ Exercise + Fitness
Illness ~ Hardiness + Stress
Fitness ~ Exercise + Hardiness 
Stress ~ Exercise + Hardiness + Fitness 
Exercise ~~ Exercise 
Hardiness ~~ Hardiness 
Exercise ~~ Hardiness'

#Fit the model with the covariance matrix
N = 363
fit.path &lt;-sem(mymodel,sample.cov=covmax, sample.nobs=N, fixed.x=FALSE)

#Summary of the model fit
summary(fit.path, fit.measures = TRUE)
</code></pre>

<p><strong>And the output I get is as follows:</strong> </p>

<pre><code> lavaan (0.5-12) converged normally after  93 iterations

 Number of observations                         37300

 Estimator                                         ML
 Minimum Function Test Statistic                0.000
 Degrees of freedom                                 0
 P-value (Chi-square)                           1.000

 Model test baseline model:

 Minimum Function Test Statistic            16594.387
 Degrees of freedom                                10
 P-value                                        0.000

 Full model versus baseline model:

 Comparative Fit Index (CFI)                    1.000
 Tucker-Lewis Index (TLI)                       1.000

 Loglikelihood and Information Criteria:

 Loglikelihood user model (H0)             -882379.005
 Loglikelihood unrestricted model (H1)     -882379.005

 Number of free parameters                         15
 Akaike (AIC)                              1764788.009
 Bayesian (BIC)                            1764915.910
 Sample-size adjusted Bayesian (BIC)       1764868.240

 Root Mean Square Error of Approximation:

 RMSEA                                          0.000
 90 Percent Confidence Interval          0.000  0.000
 P-value RMSEA &lt;= 0.05                          1.000

 Standardized Root Mean Square Residual:

 SRMR                                           0.000

 Parameter estimates:

 Information                                 Expected
 Standard Errors                             Standard

                Estimate  Std.err  Z-value  P(&gt;|z|)
 Regressions:
 Illness ~
 Exercise          0.318    0.048    6.640    0.000
 Fitness          -8.835    0.174  -50.737    0.000
 Hardiness       -12.146    0.793  -15.321    0.000
 Stress           27.125    0.451   60.079    0.000
 Fitness ~
 Exercise          0.109    0.001   82.602    0.000
 Hardiness         0.396    0.023   17.211    0.000
 Stress ~
 Exercise         -0.001    0.001   -2.614    0.009
 Hardiness        -0.393    0.009  -44.332    0.000
 Fitness          -0.040    0.002  -19.953    0.000

 Covariances:
 Exercise ~~
 Hardiness        -7.581    1.309   -5.791    0.000

 Variances:
 Exercise       4422.131   32.381
 Hardiness        14.440    0.106
 Illness       318744.406 2334.012
 Fitness         284.796    2.085
 Stress           41.921    0.307
</code></pre>

<p><strong>These are my questions:</strong>  </p>

<ul>
<li>Why does the chi-squared say that there are no degrees of freedom? </li>
<li>Why are the p-values exactly 1? Why is the CFI and TLI exactly 1? </li>
<li><p>Why is the RMSEA 0?</p></li>
<li><p>What would I need to do to simulate a more realistic model that doesn't appear artificially ""perfect""? </p></li>
<li>Does it have to do with the model specification? </li>
</ul>
"
"0.107474459256542","0.115684483091955"," 62070","<p>Let's say you have a response variable and an independent variable. Your data is measured across several levels of a categorical independent variable. One approach in analysing these data would be to use linear regression to estimate a slope at each level of the categorical independent variable. This is the approach I've used here, using <code>sleepstudy</code> dataset from the <code>R</code> <code>lme4</code> package (I've stored the betas from each model in <code>lmBetas</code>):</p>

<pre><code>library(lme4); library(plyr); library(ggplot2)
lmBetas &lt;- daply(sleepstudy, .(Subject), function(x) coef(lm(Reaction ~ Days, data=x))[""Days""])
</code></pre>

<p>Another approach in analysing these data would be to use a mixed effects model to estimate slopes for each level of the categorical independent variable, which in this case is <code>Subject</code>. This is the approach I've taken here (I've stored the betas from the model in <code>lmerBetas</code>):</p>

<pre><code>lmerBetas &lt;- coef(lmer(Reaction ~ Days + (Days | Subject), data=sleepstudy))$Subject[,""Days""]
</code></pre>

<p>I have learned that a single mixed effects model, as implemented through the <code>lmer</code> function in R, is more accurate at estimating slopes than a multiple linear regression model applied to multilevel data. This can be demonstrated with this plot of betas from the above models. </p>

<pre><code>betas &lt;- data.frame(method.betas = c(lmerBetas, lmBetas))
betas$method &lt;- c(rep(""lmer"", 18), rep(""lm"", 18))

ggplot(betas, aes(method.betas)) + 
  geom_histogram() +
  facet_grid(method ~ .)
</code></pre>

<p>The top histogram shows betas estimated using linear regression, and the bottom histogram shows betas estimated using mixed effects. You can see betas estimated using linear regression are more widely spread than those estimated through the mixed effects model.</p>

<p><img src=""http://i.stack.imgur.com/YtB22.jpg"" alt=""enter image description here""></p>

<p>So finally, my questions:</p>

<ol>
<li><p>Is a mixed effects model's higher accuracy in betas estimation connected with the fact that it models intercepts and slopes for each level of the categorical independent variable under a joint probability distribution?</p></li>
<li><p>Generally speaking, why is a mixed effects model more accurate in its betas estimation?</p></li>
</ol>
"
"0.204300291514011","0.207689836423585"," 62106","<p>I have been working on a baseball model to predict success at the major league level using minor league statistics. After posting multiple threads on this site (<a href=""http://stats.stackexchange.com/questions/61217/transforming-variables-for-multiple-regression-in-r"">1</a>, <a href=""http://stats.stackexchange.com/questions/61547/help-me-fit-this-non-linear-multiple-regression-that-has-defied-all-previous-eff"">2</a>, <a href=""http://stats.stackexchange.com/questions/61711/fitting-a-zero-inflated-negative-binomial-regression-with-r"">3</a>) and receiving valuable feedback, I have settled on a zero-inflated negative binomial model as being the best fit for my data.</p>

<p>For those who do not want to go back through old threads,  I will recap some of the story here. Also, for those who have read the old threads, some of the details regarding the variables I am using have changed. </p>

<p>In my model, the dependent variable, offensive career wins above replacement (oWAR), is a proxy for success at the MLB level and is measured as the sum of offensive contributions for every play the player is involved in over the course of his career (details here - <a href=""http://www.fangraphs.com/library/misc/war/"" rel=""nofollow"">http://www.fangraphs.com/library/misc/war/</a>). The independent variables are z-scored minor league offensive variables for statistics that are thought to be important predictors of success at the major league level including age (players with more success at a younger age tend to be better prospects), strike out rate [SOPct], walk rate [BBPct] and adjusted production (a global measure of offensive production). Additionally, since position is an important determinant of whether a players makes the major leagues (those who play at easier positions will be required to perform at a higher offensive level in order to have the same value as a player at a more difficult position), I have included dummy variables to account for position. Note that I have not included the position dummy in the count portion of the model as the oWAR dependent variable has already been adjusted for the difficulty level of the position played by the player. </p>

<p><strong>EDIT: I have added the paragraphs below in response to the following comments in the answer below:</strong></p>

<p><em>""I see you do not include the same covariates in the Logit and the negative binomial process - why not? Ususally, each relevant predictor would be expected to influence both processes.""</em></p>

<p>I think it would help if I explained the data generating process. A player plays in the minor leagues. At some point, when they have demonstrated enough skill at the minor league level (this is a combination of statistical success and observed traits that scouts believe will allow them to be successful at the major league level), a player is promoted to the major leagues. At this point, they have an opportunity to accrue oWAR. At this point, the first data-generating process  (captured by the logit model) ends. Now, a different data generating process takes over, whereby players accumulate oWAR depending on how they play at the major league level. Some players will not perform well and accumulate zero oWAR, the same as a player who did not make the majors. That is one of the reasons I think this model is appropriate. It is not necessarily easy to separate a player who accumulates zero because they aren't good enough to make the major leagues from a player who makes the major leagues but does not succeed at that level (and still ends his career with zero oWAR). I have not included the positional dummy in the count part of the model because the oWAR measure is already adjusted for the position played by the player whereas the minor league statistics are not. When I tried testing them in the model, they were, not surprisingly, not significant. I omitted the BB Pct statistic from the logit part of the model as it was not significant (p = 0.22)</p>

<p><em>""Since your data appears to be a panel (you observe players/teams repeatedly), you can think about more sophisticated stuff like fixed or random effects. ""</em></p>

<p>In terms of how the data is sampled, a player can be in the dataset once (if they spend only a year at the level of the minor leagues) or multiple times if they take multiple years to advance. After reading up on fixed vs random effects model, I don't see how I can used a fixed model to predict out of sample players. However, I am sure that there are fixed effects (effects determined by the player that are not captured in the dependent variables) so I don't fully understand how to handle that issue.</p>

<p><strong>END EDIT</strong></p>

<p>After trying a linear, Box-Cox transformed and basic GLM model with generally poor results, I was directed to the zero-inflated negative binomial distribution set of models. After trying out different combination of variables and following the steps in this excellent <a href=""http://www.jstatsoft.org/v27/i08/paper"" rel=""nofollow"">step-by-step</a> guide for regression models for count data in R, I settled on the following model (shown below).</p>

<p><strong>Model</strong>
<img src=""http://i.imgur.com/neOP2RE.jpg"" alt=""model1""></p>

<p>Furthermore, when I re-estimate the standard errors using sandwich standard errors, the model still appears to have appropriate independent variables.</p>

<p><strong>Sandwich Standard Errors</strong>
<img src=""http://i.imgur.com/47JjOae.jpg"" alt=""sandwich""></p>

<p>At this point, I do not think I will find a better model type given the dataset I have. However, I am still left with some issues. The first issue may be a function of the dataset I am using. In most examples I have seen that discuss zero-inflated data, there are clearly more zeros than other values (hence, the name). However, the number of zeros still appears to be less than 50% of the total dependent variables and usually not even that high. In my dataset, approximately 87% of the dependent variables are zero i.e., it is hard to have success in major league baseball. I am guessing the model should technically be able to account for this scenario (albeit with less predictive value than a model with more non-zeros) but I am not sure how to check if that is the case. When I create a plot of fitted values and Pearson residuals, and a plot of fitted values and raw residuals, they appear as below:</p>

<p><strong>Fitted vs Pearson residuals</strong>
<img src=""http://i.imgur.com/tlx6ibn.jpg"" alt=""FvP""></p>

<p><strong>Fitted vs raw residuals</strong>
<img src=""http://i.imgur.com/vL8OIcV.jpg"" alt=""FvR""></p>

<p>Not knowing exactly what these plots look like in a good-fitting regression, I decided to take the sample data described <a href=""http://www.ats.ucla.edu/stat/r/dae/zipoisson.htm"" rel=""nofollow"">here</a> and examine the plots in an example where I know the fit has been deemed to be good. </p>

<p><strong>Fitted vs Pearson residuals - Sample problem</strong>
<img src=""http://i.imgur.com/TJh2zHF.jpg"" alt=""FvPEx""></p>

<p><strong>Fitted vs raw residuals - Sample problem</strong>
<img src=""http://i.imgur.com/YJMxJl9.jpg"" alt=""FvREx""></p>

<p>Clearly, these plots do not look that similar to mine. I am not sure how much has to do with a) model misspecification b) the fact that my dependent variable has 87% zeros and c) the fact that this is a simple sample problem designed to perfectly fit this model whereas my data is messy, real world data. Any thoughts on this issue would be appreciated. </p>

<p>My second issue, which I am not sure if I should be tackling after or simultaneously with the first issue, has to do with the functional form specification. I don't know if my independent variables are in the right form. It has been suggested to me by a friend that I could try a) multiple fractional polynomials with loops or b) informally play around with adding polynomials of covariates, interactions, etc. My issue at this point is that I do not know how to implement point a in R and and I am not sure which forms to try for point b besides randomly choosing some. Once again, help on this separate (but related?) issue would be greatly appreciated. </p>

<p>If anyone has any questions, I will do my best to answer them. In my first post (<a href=""http://stats.stackexchange.com/questions/61217/transforming-variables-for-multiple-regression-in-r"">1</a>), I mentioned I could not provide the dataset but I have been given permission to do so if anyone wants to take a look. Thanks again. </p>
"
"0.053644178078582","0.0409006412361909"," 62180","<p>I want to do some regression analysis that constrains the coefficients to vary smoothly as a function of their sequence.
It is similar to the ""Phoneme Recognition"" example in the part 5 ""Basis Expansions and Regularization"" of <em><a href=""http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf"" rel=""nofollow"">The Elements of Statistical Learning</a></em> (Hastie, Tibshirani &amp; Friedman, 2008).</p>

<p>in the book it says:</p>

<blockquote>
  <p>The smooth red curve was obtained through a very simple use of natural cubic splines. We can represent the coefficient function as an expansion of splines $\beta(f)=\sum_{m=1}^M h_m(f)\theta_m$. In practice this means that $\beta=\mathbf{H}\theta$ where, $\mathbf{H}$ is a $p Ã— M$ basis matrix of natural cubic splines, defined on the set of frequencies. Here we used $M = 12$ basis functions, with knots uniformly placed over the integers 1, 2, . . . , 256 representing the frequencies. Since $x^T\beta=x^T\mathbf{H}\theta$, we can simply replace the input features $x$ by their filtered versions $x^* = \mathbf{H}^T x$, and fit $\theta$ by linear logistic regression on the $x^*$. The red curve is thus $\hat\beta(f) = h(f)^T\hat \theta$.</p>
</blockquote>

<p>But I am not sure about how to create the basis matrix $\mathbf{H}$. When using ns() function in R, how to set the knots?</p>

<p>Any hint will be appreciated.</p>
"
"0.100582833897341","0.0954348295511121"," 62208","<p>I've tried to simulate data for a power analysis of a logistic regression. The results of the power analysis look reasonable: power=90% for a sample of 6000 persons. But I feel that the analysis lacks something. So, my question is: when generating the data should I include something about how the variables are correlated, or their covariance, other than just defining their linear relationship as I have done in the example below, and if so where do I write that into the code?</p>

<p>I know other questions look like this but I'm not confident that they answer this question.</p>

<pre><code>library(plyr) # functions
## Define Function
simfunktion &lt;- function() {
   # Number in each sample
  antal &lt;- 6000
  beta0 &lt;- log(0.16) # logit in reference group
  beta1 &lt;- log(1.1)  # logit given smoking
  beta2 &lt;- log(1.1)  # logit given SNP(genevariation)
  beta3 &lt;- log(1.2)  # logit for interactioncoefficient for SNP*rygning
   ## Smoking variable, with probabilities defined according to empirical studies.
  smoking  &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,25,40))
   ## SNP variables with probabilities defined according to empirical studies
  SNP      &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,40,20))
   ## calculated probabilites given the model:
  pi.x     &lt;- exp( beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) / 
              ( 1 + exp(beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) )
   ## binoial events given the probabilities:
  sim.y    &lt;- rbinom( n = antal, size = 1, prob = pi.x)  
  sim.data &lt;- data.frame(sim.y, smoking, SNP)
   #################### p-value of the interaction is extracted:
   ## the model is run:
  glm1     &lt;- glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial )
   ## p-value of the interactionterm is extracted:
  summary(    glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial ))$coef[4,4]
}
pvalue     &lt;- as.vector(replicate( 100 , simfunktion()))
mean(pvalue &lt; 0.05)
</code></pre>
"
"0.0464572209811883","0.0472279924554862"," 62209","<p>I have written a function (in R-language) to perform a Theil-Sen regression with large sample approximation (please see below).  It seems to work well for the one-sided case as illustrated in Hollander &amp; Wolfe (2nd Edition Ch. 9)   </p>

<pre><code>NP.lm &lt;-function(dat, X, Y, alpha) {

  dat &lt;- dat[order(dat[,X]),]
  n &lt;- nrow(dat)    
  combos &lt;- combn(n, 2)
  i.s &lt;- combos[1,]
  j.s &lt;- combos[2,]
  Y.num &lt;- dat[j.s,Y] - dat[i.s,Y]
  X.dom &lt;- dat[j.s,X] - dat[i.s,X]      
  Z.p &lt;-  qnorm(alpha/2, lower.tail=F)
  N &lt;- (n*(n-1))/2
  s &lt;- (Y.num/X.dom)[X.dom != 0]
  C.stat &lt;- sum(sign(s))  

  slope &lt;- median(s, na.rm=TRUE)                 
  intercept &lt;- median((dat[,Y] - slope*dat[,X]), na.rm=TRUE)     

  C.star &lt;- C.stat / sqrt(n*(n-1)*((2*n)+5)/18)
  p.val &lt;- pnorm(-abs(C.star))

  out &lt;- data.frame(colnames(dat[X]), colnames(dat[Y]), slope, intercept, p.val)

  out

} 
</code></pre>

<h1>Example usage</h1>

<pre><code>set.seed(123) # to make the example fully reproducible
sigma &lt;- as.matrix(data.frame(c(1, .7), c(.7, 1)))
N &lt;- 50
dat &lt;- data.frame( matrix(rnorm(N*nrow(sigma)), N, nrow(sigma)) %*% 
    chol(sigma) ) # cor structure via Choleski Decomp.
colnames(dat) &lt;- c(""X"", ""Y"")
summary(lm(Y~X, data=dat))
NP.lm(dat=dat, X=1, Y=2, alpha=0.05)

  colnames.dat.X.. colnames.dat.Y..     slope  intercept        p.val
1                X                Y 0.6719583 0.08337517 1.227402e-07

# This p-value can't be right
</code></pre>

<p>Could someone please tell me what I am doing wrong, and how to make the test 2-sided.  </p>

<p>Thank you so much for your help!  </p>
"
"0.0599760143904067","0.0609710760849692"," 62249","<p>I have count data (observed) and expected counts calculated by regression analysis. I want perform multiple hypothesis test to check which elements show maximum differences between observed and expected. I am using R and I ran this comand,</p>

<pre><code>p.values=apply(matrix,1,function(x){t.test(x[1],x[2],alternative=""g"")$p.value})
</code></pre>

<p>It gives me error saying <code>not enough x observation</code> -it is obvious that I don't have any replicates. But the paper which I am following (they have done similar kind of study) says that: </p>

<blockquote>
  <p>To detect hotspots, we performed a one-tailed test (alpha&lt;0.001) using Poisson distribution and the expected counts</p>
</blockquote>

<p>I don't understand how did they do it. </p>

<p>I can detect the elements using fold-change between observed and expected, but here they say they have done hypothesis test. </p>
"
"0.0889588054368324","0.0904347204435887"," 62427","<p>I am working to investigate association between environmental pollution and daily hospital admission due to various causes.   This outcome data has excess zeros on days when there are no admissions for specific causes.  I would like to adjust for temperature and humidity using smooth terms.  Usually Poisson generalized additive models with smoothing parameters for time trend and meteorological variables are used to model such associations.   However due to excessive zeros I was advised to consider zero inflated models.  How can I implement this in R and address non parametric associations of temperature, humidity and time trend? </p>

<ol>
<li>What is the criterion to use zero inflated models for a count data?</li>
<li>How do I run zero inflated model and how do I check the fit?</li>
</ol>

<p>The following is a sample GAM model in R using mgcv package:</p>

<pre><code>Log {E (hospital admission)} = Î±+Î²(pollutant)+s(time)+s(temperature) + s(Relative Humidity) + DOW + flu
Where: 
E (admission) = expected count of cause specific admissions on day t
Î²= regression coefficient of the pollutant
pollutant = air pollutant (PM10, ozone, NO2) level at time t
s = smooth function using natural or penalized spline
dow =  vector of regression coefficient associated with indicator variables for day of the week (DOW).
Flu= weekly influenza count
Time, temperature and relative humidity are covariates 
</code></pre>

<p>Thanks</p>
"
"0.0663812836584521","0.0674826151369737"," 62678","<p>I have a regression problem that I implement in R using for loop. Basically, I have an equation (as a result of a long procedure) as a function of temperature, with five unknown parameters. I have 12 different temperatures that I can use to drive the equations, so for each combination of the five parameters, I can have 12 points.</p>

<p>The 12 points are actually derived from two line segments that cross at a certain point. Depending on the values of the five unknown parameters, the two segments can run anywhere. With the right parameters I hope that the two segments form a straight line.</p>

<p>For that purpose, I need to run a linear regression on these  points, and then over the whole combination of the five parameters, I need to find one set of parameters that results in the minimum residual sum of squares.</p>

<p>The problem is that it takes hours to solve the parameters with a fine resolution (by=0.05 instead of by=0.5 in the example below).</p>

<p>Is there a way to solve this problem faster? I think gradient descend can do this faster, but I do not know how to formulate it.</p>

<p>Any suggestions would be greatly appreciated.</p>

<p>Here is the code:</p>

<pre><code>A1 = 1.25
A2 = 0.01136
A3 = -0.0433

a = seq(0,1,by=0.5)
b = seq(0,1,by=0.5)
c = seq(-3,3,by=0.5)
d = seq(0,1,by=0.5)
e = seq(-3,3,by=0.5)

Temp_F = c(28.99, 36.87, 42.92, 52.84, 58.31, 67.60, 76.17, 70.20, 63.26, 53.05, 39.28, 35.35)

df &lt;- expand.grid(a=a, b=b, c=c, d=d, e=e)
nrow(df)

for (i in 1:nrow(df) ) {
    X.points = df$a[i]*A1 - (A2*df$b[i]*Temp_F) - df$c[i] + (A3*df$d[i]*Temp_F) + df$e[i]
    	glm.X = glm(X.points ~ Temp_F)
    	df$res[i]=sum(residuals(glm.X)^2)
}

write.csv(df, file=""df.csv"")
index=which(df$res == min(df$res[df$res &gt; 0]))
    df$a[index]
df$b[index]
    df$c[index]
df$d[index]
    df$e[index]
</code></pre>
"
"0.107288356157164","0.102251603090477"," 62899","<p>I hope you can help me get some confidence in my confidence interval... I am trying to get the confidence interval for a particular (threshold) point on a predicted curve.</p>

<p>I find the confidence interval (C.I.) looks funny, so I would like to know whether what I am doing is correct... Thank you for any advice you can give me!</p>

<pre><code>#Here is the data to which I fit a nonlinear least squares regression (power function)
y &lt;-c(0.5745373, 0.8255836, 0.7139635, 0.6318004, 0.8688738, 0.8341626, 0.9278573, 0.6548638,
0.6995722, 0.8410211, 1.0000000, 0.8512973, 0.7917790, 0.5722589, 0.6918069, 0.7117084,
0.5279689, 0.6121315, 0.5869292, 0.7332605, 0.5991816, 0.5470566, 0.5987166, 0.4440854,
0.3719892, 0.4394820, 0.4410862, 0.4966288, 0.4291826, 0.4221613, 0.3951223, 0.3595973,
0.4617549, 0.5106482, 0.5939970, 0.5340835, 0.6036920, 0.5181577, 0.3892170, 0.3667581)

x &lt;- c(0.95149254, 0.57954545, 0.56763699, 0.57089552, 1.00000000, 0.66870629, 0.70833333,
0.53125000, 0.58776596, 0.78061224, 0.61551724, 0.63750000, 0.85000000, 0.52397260,
0.66870629, 0.50328947, 0.52192982, 0.40691489, 0.36016949, 0.37500000, 0.35915493, 
0.53968254, 0.36334197, 0.23486842, 0.19615385, 0.35961538, 0.30039267, 0.26424870,
0.54838710, 0.23363874, 0.26936620, 0.09514925, 0.15324519, 0.32465278, 0.33909574, 
0.31587838, 0.24401914, 0.28813559, 0.23181818, 0.29272959)

plot(x,y, xlim=c(0,1), ylim=c(0,1))
m1 &lt;- nls(y~a*x^b, start=list(a=2,b=0.2))
summary(m1)
curve((0.86888*x^0.43042), add=TRUE)
points(0.2276312,0.4595148, col='red', pch=17) # this is the threshold value I  
                                               # previously calculated
</code></pre>

<p>now I would like to know the Confidence Interval of the threshold</p>

<pre><code>#lower CI
0.86888-(2*0.04578) # intercept minus 2* S.E.
0.43042-(2*0.06333) # exponent minus 2*S.E.
curve((0.77732*x^0.30376), add=TRUE) # plot the lower C.I.
0.77732*0.2276312^0.30376 # Now I enter the x coordinate of threshold point in the 
# Confidence interval function, to get the y coordinate
points(0.2276312,0.4958526, col='red', pch=""-"", cex=1.5) # plotting the CI of threshold value

#Upper CI
0.86888+(2*0.04578) # intercept plus 2* S.E.
0.43042+(2*0.06333) # exponent plus 2*S.E.
curve((0.96044*x^0.55708), add=TRUE) # plot upper C.I.
0.96044*0.2276312^0.55708 # Now I enter the x coordinate of threshold point in the 
# Confidence interval function, to get the y coordinate
points(0.2276312,0.4211113, col='red', pch=""-"", cex=1.5) # plot the CI of threshold value
lines(c(0.2276312,0.2276312),c(0.4211113,0.4958526), col=""red"")

# then check whether correct
confint(m1) # more or less, but slightly different, don't understand why?
</code></pre>

<p><img src=""http://i.stack.imgur.com/PVhBA.png"" alt=""nls confints""></p>

<p>To me, a C.I. that looks like this would make more sense...</p>

<pre><code>curve((0.96044*x^0.30376), col=""green"",add=TRUE)
curve((0.77732*x^0.55708), col=""green"",add=TRUE)
</code></pre>

<p><img src=""http://i.stack.imgur.com/vTl6d.png"" alt=""NLS_confint2""></p>

<p>but that would be mixing the intercept of upper and exponent of lower C.I. 
and vice versa</p>

<p>My question is then, are the black Confidence Intervals correct (and thus the thresholds red confidence interval?</p>
"
"0.026822089039291","0.0272670941574606"," 63055","<p>I am doing survival analysis using ridge regression. I'm using this R command:</p>

<pre><code>coxph(Surv(time, status) ~ ridge(x1, x2, x3), data=DATA)
</code></pre>

<p>As far as I know, <code>lambda</code> (the regulation parameter) is estimated using cross validation, but then this R code should result in different results with different random seeds. But I got always the same coefficients; how can that happen? </p>

<p>Is <code>coxph(Surv()~.)</code> not a commonly used approach? Should I use <code>glmnet</code> or any other functions?  </p>
"
"0.110761366336029","0.112599007499728"," 63222","<p>How do I get p-values using the <code>multinom</code> function of <code>nnet</code> package in <code>R</code>?</p>

<p>I have a dataset which consists of â€œPathology scoresâ€ (Absent, Mild, Severe) as outcome variable, and two main effects: Age (two factors: twenty / thirty days) and Treatment Group (four factors: infected without ATB; infected + ATB1; infected + ATB2; infected + ATB3).</p>

<p>First I tried to fit an ordinal regression model, which seems more appropriate given the characteristics of my dependent variable (ordinal). However, the assumption of odds proportionality was severely violated (graphically), which prompted me to use a multinomial model instead, using the <code>nnet</code> package.  </p>

<p>First I chose the outcome level that I need to use as baseline category: </p>

<pre><code>Data$Path &lt;- relevel(Data$Path, ref = ""Absent"")
</code></pre>

<p>Then, I needed to set baseline categories for the independent variables:</p>

<pre><code>Data$Age &lt;- relevel(Data$Age, ref = ""Twenty"")
Data$Treat &lt;- relevel(Data$Treat, ref=""infected without ATB"") 
</code></pre>

<p>The model:</p>

<pre><code>test &lt;- multinom(Path ~ Treat + Age, data = Data) 
# weights:  18 (10 variable) 
initial value 128.537638 
iter 10 value 80.623608 
final  value 80.619911 
converged
</code></pre>

<p>The output:</p>

<pre><code>Coefficients:
         (Intercept)   infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate   -2.238106   -1.1738540      -1.709608       -1.599301        2.684677
Severe     -1.544361   -0.8696531      -2.991307       -1.506709        1.810771

Std. Errors:
         (Intercept)    infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate   0.7880046    0.8430368       0.7731359       0.7718480        0.8150993
Severe     0.6110903    0.7574311       1.1486203       0.7504781        0.6607360

Residual Deviance: 161.2398
AIC: 181.2398
</code></pre>

<p>For a while, I could not find a way to get the $p$-values for the model and estimates when using <code>nnet:multinom</code>. Yesterday I came across a post where the author put forward a similar issue regarding estimation of $p$-values for coefficients (<a href=""http://stats.stackexchange.com/questions/9715/how-to-set-up-and-estimate-a-multinomial-logit-model-in-r"">How to set up and estimate a multinomial logit model in R?</a>). There, one blogger suggested that getting $p$-values from the <code>summary</code> result of <code>multinom</code> is pretty easy, by first getting the $t$values as follows: </p>

<pre><code>pt(abs(summary1$coefficients / summary1$standard.errors), df=nrow(Data)-10, lower=FALSE) 

         (Intercept)   infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate 0.002670340   0.08325396      0.014506395     0.02025858       0.0006587898
Severe   0.006433581   0.12665278      0.005216581     0.02352202       0.0035612114
</code></pre>

<p>According to Peter Dalgard, ""There's at least a factor of 2 missing for a two-tailed $p$-value. It is usually a mistake to use the $t$-distribution for what is really a $z$-statistic; for aggregated data, it can be a very bad mistake.""
According to Brian Ripley, ""it is also a mistake to use Wald tests for <code>multinom</code> fits, since they suffer from the same (potentially severe) problems as binomial fits. 
Use profile-likelihood confidence intervals (for which the package does provide software), or if you must test, likelihood-ratio tests (ditto).""</p>

<p>I just need to be able to derive reliable $p$-values.</p>
"
"0.104084994078794","0.0991986258660637"," 63226","<p>I am trying to model some data regarding a predator prey interaction experiment (n=26). Predation rate is my response variable and I have 4 explanatory variables: predator density (1,2,3,4 5), predator size, prey density (5,10,15,20,25,30) and prey type (3 categories). I started with several linear models (GLM) and found (as expected) that prey and predator density were non-linearly related to predation rates. If I use a log transformation on these variables I get really nice curves and an adjusted $R^{2}$ of 0.82, but it is not really the right approach for modelling non-linear relationships.</p>

<pre><code>model &lt;-glm(rates ~ log(pred) + log (prey) + type)
</code></pre>

<p>Therefore I switched to non-linear least square regression (<code>nls</code>). I have several predator-prey models based on existing ecological literature e.g.:</p>

<pre><code> ### Holling's type II functional response

model1 &lt;- nls(rates ~ (a * prey)/(1 + b * prey),
start = list(a = 0.27,b = 0.13), trace = TRUE)

### Beddington-DeAngelis functional response

model2 &lt;- nls(rates ~ (a*prey)/(1+ (b * prey) + c * (pred -1 )),
start = list(a=0.22451, b=-0.18938, c=1.06941), trace=TRUE, subset=I1) 
</code></pre>

<p>These models work perfectly, but now I want to add prey type as well. In the linear models prey type was the most important variable so I don't want to leave it out. I understand that you can't add categorical variables in nls, so I thought I try a generalized additive model (GAM).</p>

<p>The problem with the gam models is that the smoothers (both spline and loess) don't work on both variables because there are only a very restricted number of values for prey density and predator density. I can manage to get a model with a single variable smoothed using loess. But for two variables it is simply not working. The spline function does not work at all because I have so few values (5) for my variables (see model 4).</p>

<pre><code>model3 &lt;- gam(rates~ lo(pred, span=0.9)+prey)
## this one is actually working but does not include a smoother for prey.

model4 &lt;- gam(rates~ s(pred)+prey)
## this one gives problems: 
A term has fewer unique covariate combinations than specified maximum degrees of freedom
</code></pre>

<p>My question is: are there any other possibilities to model data with 2 non-linear related variables in which I can also include a categorical variable. I would prefer to use <code>nls</code> (<code>model2</code>) with for example different intercepts for each category but I'm not sure how to get this sorted, if it is possible at all. The dataset is too small to split it up into the three categories, moreover, one of the categories only contains 5 data points.</p>

<p>Any help would be really appreciated.</p>
"
"0.125925827965041","0.128015062620555"," 63233","<h2>Background</h2>

<p>In a paper from Epstein (1991): <a href=""http://journals.ametsoc.org/doi/pdf/10.1175/1520-0442%281991%29004%3C0365%3AOODCVF%3E2.0.CO%3B2"" rel=""nofollow"">On obtaining daily climatological values from monthly means</a>, the formulation and an algorithm for calculating Fourier interpolation for periodical and even-spaced values are given.</p>

<p>In the paper, the goal is to <strong>obtain daily values from monthly means</strong> by interpolation.</p>

<p>In short, it is assumed that unknown daily values can be represented by the sum of harmonic components:
$$
y(t) = a_{0} + \sum_{j}\left[a_{j}\,\cos(2\pi jt/12)+b_{j}\,\sin(2\pi jt/12)\right]
$$
In the paper $t$ (time) is expressed in months.</p>

<p>After some derviation, it is shown that the terms can be calculated by:
$$
\begin{align}
a_{0} &amp;= \sum_{T}Y_{T}/12 \\
a_{j} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right] \times \sum_{T}\left[Y_{T}\,\cos(2\pi jT/12)/6 \right]~~~~~~~j=1,\ldots, 5 \\
b_{j} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right] \times \sum_{T}\left[Y_{T}\,\sin(2\pi jT/12)/6 \right]~~~~~~~j=1,\ldots, 5 \\
a_{6} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right]\times \sum_{T}\left[Y_{T}\cos(\pi T)/12\right] \\
b_{6} &amp;= 0
\end{align}
$$
Where $Y_{T}$ denote the monthly means and $T$ the month.</p>

<p><a href=""http://journals.ametsoc.org/doi/pdf/10.1175/1520-0493%281995%29123%3C2251%3ATIODSU%3E2.0.CO%3B2"" rel=""nofollow"">Harzallah (1995)</a> summarizes this aproach as follows: ""The interpolation is carried out by adding zeros to the spectral coefficients of data and by performing an inverse Fourier transform to the resulting extended coefficients. The method is equivalent to applying a rectangular filter to Fourier coefficients.""</p>

<hr>

<h2>Questions</h2>

<p>My goal is to use the above methodology for interpolation of <strong>weekly means to obtain daily data</strong> (see <a href=""http://stats.stackexchange.com/questions/59418/interpolation-of-influenza-data-that-conserves-weekly-mean/63135#63135"">my previous question</a>). In summary, I have 835 weekly means of count data (see the example dataset at the bottom of the question). There are quite a few things that I don't understand before I can apply the approach outlined above:</p>

<ol>
<li>How would the formulas have to be changed for my situation (weekly instead of monthly values)?</li>
<li>How could the time $t$ be expressed? I assumed $t/835$ (or $t/n$ with $n$ data points in general), is that correct?</li>
<li>Why does the author calculate 7 terms (i.e. $0\leq j \leq 6$)? How many terms would I have to consider?</li>
<li>I understand that the question can probably be solved by using a <a href=""http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r?lq=1"">regression approach</a> and using the predictions for interpolation (thanks to Nick). Still, some things are unclear to me: How many terms of harmonics should be included in the regression? And what period should I take? How can the regression be done to ensure that the weekly means are preserved (as I don't want an exact harmonic fit to the data)?</li>
</ol>

<p>Using the <a href=""http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r?lq=1"">regression approach</a> (which is also explained in <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0116"" rel=""nofollow"">this paper</a>), I managed to get an exact harmonic fit to the data (the $j$ in my example would run through $1, \ldots, 417$, so I fitted 417 terms). <strong>How can this approach be modified -$~$if possible$~$- to achieve the conservation of the weekly means?</strong> Maybe by applying correction factors to each regression term?</p>

<p>The plot of the exact harmonic fit is:</p>

<p><img src=""http://i.stack.imgur.com/7XuxU.png"" alt=""Exact harmonic fit""></p>

<p><strong>EDIT</strong></p>

<p>Using the <a href=""http://cran.r-project.org/web/packages/signal/"" rel=""nofollow"">signal package</a> and the <code>interp1</code> function, here's what I've managed to do using the example data set from below (many thanks to @noumenal). I use <code>q=7</code> as we have weekly data:</p>

<pre><code># Set up the time scale

daily.ts &lt;- seq(from=as.Date(""1995-01-01""), to=as.Date(""2010-12-31""), by=""day"")

# Set up data frame 

ts.frame &lt;- data.frame(daily.ts=daily.ts, wdayno=as.POSIXlt(daily.ts)$wday,
                       yearday = 1:5844,
                       no.influ.cases=NA)

# Add the data from the example dataset called ""my.dat""

ts.frame$no.influ.cases[ts.frame$wdayno==3] &lt;- my.dat$case

# Interpolation

case.interp1 &lt;- interp1(x=ts.frame$yearday[!is.na(ts.frame$no.influ.case)],y=(ts.frame$no.influ.cases[!is.na(ts.frame$no.influ.case)]),xi=ts.frame$yearday, method = c(""cubic""))

# Plot subset for better interpretation
par(bg=""white"", cex=1.2, las=1)
plot((ts.frame$no.influ.cases)~ts.frame$yearday, pch=20,
     col=grey(0.4),
     cex=1, las=1,xlim=c(0,400), xlab=""Day"", ylab=""Influenza cases"")
lines(case.interp1, col=""steelblue"", lwd=1)
</code></pre>

<p><img src=""http://i.stack.imgur.com/R1FE8.png"" alt=""Cubicinterpo""></p>

<p>There are two issues here:</p>

<ol>
<li>The curve seem to fit ""too good"": it goes through every point </li>
<li>The weekly means are not conserved</li>
</ol>

<p><strong>Example dataset</strong></p>

<pre><code>structure(list(date = structure(c(9134, 9141, 9148, 9155, 9162, 
9169, 9176, 9183, 9190, 9197, 9204, 9211, 9218, 9225, 9232, 9239, 
9246, 9253, 9260, 9267, 9274, 9281, 9288, 9295, 9302, 9309, 9316, 
9323, 9330, 9337, 9344, 9351, 9358, 9365, 9372, 9379, 9386, 9393, 
9400, 9407, 9414, 9421, 9428, 9435, 9442, 9449, 9456, 9463, 9470, 
9477, 9484, 9491, 9498, 9505, 9512, 9519, 9526, 9533, 9540, 9547, 
9554, 9561, 9568, 9575, 9582, 9589, 9596, 9603, 9610, 9617, 9624, 
9631, 9638, 9645, 9652, 9659, 9666, 9673, 9680, 9687, 9694, 9701, 
9708, 9715, 9722, 9729, 9736, 9743, 9750, 9757, 9764, 9771, 9778, 
9785, 9792, 9799, 9806, 9813, 9820, 9827, 9834, 9841, 9848, 9855, 
9862, 9869, 9876, 9883, 9890, 9897, 9904, 9911, 9918, 9925, 9932, 
9939, 9946, 9953, 9960, 9967, 9974, 9981, 9988, 9995, 10002, 
10009, 10016, 10023, 10030, 10037, 10044, 10051, 10058, 10065, 
10072, 10079, 10086, 10093, 10100, 10107, 10114, 10121, 10128, 
10135, 10142, 10149, 10156, 10163, 10170, 10177, 10184, 10191, 
10198, 10205, 10212, 10219, 10226, 10233, 10240, 10247, 10254, 
10261, 10268, 10275, 10282, 10289, 10296, 10303, 10310, 10317, 
10324, 10331, 10338, 10345, 10352, 10359, 10366, 10373, 10380, 
10387, 10394, 10401, 10408, 10415, 10422, 10429, 10436, 10443, 
10450, 10457, 10464, 10471, 10478, 10485, 10492, 10499, 10506, 
10513, 10520, 10527, 10534, 10541, 10548, 10555, 10562, 10569, 
10576, 10583, 10590, 10597, 10604, 10611, 10618, 10625, 10632, 
10639, 10646, 10653, 10660, 10667, 10674, 10681, 10688, 10695, 
10702, 10709, 10716, 10723, 10730, 10737, 10744, 10751, 10758, 
10765, 10772, 10779, 10786, 10793, 10800, 10807, 10814, 10821, 
10828, 10835, 10842, 10849, 10856, 10863, 10870, 10877, 10884, 
10891, 10898, 10905, 10912, 10919, 10926, 10933, 10940, 10947, 
10954, 10961, 10968, 10975, 10982, 10989, 10996, 11003, 11010, 
11017, 11024, 11031, 11038, 11045, 11052, 11059, 11066, 11073, 
11080, 11087, 11094, 11101, 11108, 11115, 11122, 11129, 11136, 
11143, 11150, 11157, 11164, 11171, 11178, 11185, 11192, 11199, 
11206, 11213, 11220, 11227, 11234, 11241, 11248, 11255, 11262, 
11269, 11276, 11283, 11290, 11297, 11304, 11311, 11318, 11325, 
11332, 11339, 11346, 11353, 11360, 11367, 11374, 11381, 11388, 
11395, 11402, 11409, 11416, 11423, 11430, 11437, 11444, 11451, 
11458, 11465, 11472, 11479, 11486, 11493, 11500, 11507, 11514, 
11521, 11528, 11535, 11542, 11549, 11556, 11563, 11570, 11577, 
11584, 11591, 11598, 11605, 11612, 11619, 11626, 11633, 11640, 
11647, 11654, 11661, 11668, 11675, 11682, 11689, 11696, 11703, 
11710, 11717, 11724, 11731, 11738, 11745, 11752, 11759, 11766, 
11773, 11780, 11787, 11794, 11801, 11808, 11815, 11822, 11829, 
11836, 11843, 11850, 11857, 11864, 11871, 11878, 11885, 11892, 
11899, 11906, 11913, 11920, 11927, 11934, 11941, 11948, 11955, 
11962, 11969, 11976, 11983, 11990, 11997, 12004, 12011, 12018, 
12025, 12032, 12039, 12046, 12053, 12060, 12067, 12074, 12081, 
12088, 12095, 12102, 12109, 12116, 12123, 12130, 12137, 12144, 
12151, 12158, 12165, 12172, 12179, 12186, 12193, 12200, 12207, 
12214, 12221, 12228, 12235, 12242, 12249, 12256, 12263, 12270, 
12277, 12284, 12291, 12298, 12305, 12312, 12319, 12326, 12333, 
12340, 12347, 12354, 12361, 12368, 12375, 12382, 12389, 12396, 
12403, 12410, 12417, 12424, 12431, 12438, 12445, 12452, 12459, 
12466, 12473, 12480, 12487, 12494, 12501, 12508, 12515, 12522, 
12529, 12536, 12543, 12550, 12557, 12564, 12571, 12578, 12585, 
12592, 12599, 12606, 12613, 12620, 12627, 12634, 12641, 12648, 
12655, 12662, 12669, 12676, 12683, 12690, 12697, 12704, 12711, 
12718, 12725, 12732, 12739, 12746, 12753, 12760, 12767, 12774, 
12781, 12788, 12795, 12802, 12809, 12816, 12823, 12830, 12837, 
12844, 12851, 12858, 12865, 12872, 12879, 12886, 12893, 12900, 
12907, 12914, 12921, 12928, 12935, 12942, 12949, 12956, 12963, 
12970, 12977, 12984, 12991, 12998, 13005, 13012, 13019, 13026, 
13033, 13040, 13047, 13054, 13061, 13068, 13075, 13082, 13089, 
13096, 13103, 13110, 13117, 13124, 13131, 13138, 13145, 13152, 
13159, 13166, 13173, 13180, 13187, 13194, 13201, 13208, 13215, 
13222, 13229, 13236, 13243, 13250, 13257, 13264, 13271, 13278, 
13285, 13292, 13299, 13306, 13313, 13320, 13327, 13334, 13341, 
13348, 13355, 13362, 13369, 13376, 13383, 13390, 13397, 13404, 
13411, 13418, 13425, 13432, 13439, 13446, 13453, 13460, 13467, 
13474, 13481, 13488, 13495, 13502, 13509, 13516, 13523, 13530, 
13537, 13544, 13551, 13558, 13565, 13572, 13579, 13586, 13593, 
13600, 13607, 13614, 13621, 13628, 13635, 13642, 13649, 13656, 
13663, 13670, 13677, 13684, 13691, 13698, 13705, 13712, 13719, 
13726, 13733, 13740, 13747, 13754, 13761, 13768, 13775, 13782, 
13789, 13796, 13803, 13810, 13817, 13824, 13831, 13838, 13845, 
13852, 13859, 13866, 13873, 13880, 13887, 13894, 13901, 13908, 
13915, 13922, 13929, 13936, 13943, 13950, 13957, 13964, 13971, 
13978, 13985, 13992, 13999, 14006, 14013, 14020, 14027, 14034, 
14041, 14048, 14055, 14062, 14069, 14076, 14083, 14090, 14097, 
14104, 14111, 14118, 14125, 14132, 14139, 14146, 14153, 14160, 
14167, 14174, 14181, 14188, 14195, 14202, 14209, 14216, 14223, 
14230, 14237, 14244, 14251, 14258, 14265, 14272, 14279, 14286, 
14293, 14300, 14307, 14314, 14321, 14328, 14335, 14342, 14349, 
14356, 14363, 14370, 14377, 14384, 14391, 14398, 14405, 14412, 
14419, 14426, 14433, 14440, 14447, 14454, 14461, 14468, 14475, 
14482, 14489, 14496, 14503, 14510, 14517, 14524, 14531, 14538, 
14545, 14552, 14559, 14566, 14573, 14580, 14587, 14594, 14601, 
14608, 14615, 14622, 14629, 14636, 14643, 14650, 14657, 14664, 
14671, 14678, 14685, 14692, 14699, 14706, 14713, 14720, 14727, 
14734, 14741, 14748, 14755, 14762, 14769, 14776, 14783, 14790, 
14797, 14804, 14811, 14818, 14825, 14832, 14839, 14846, 14853, 
14860, 14867, 14874, 14881, 14888, 14895, 14902, 14909, 14916, 
14923, 14930, 14937, 14944, 14951, 14958, 14965, 14972), class = ""Date""), 
    cases = c(168L, 199L, 214L, 230L, 267L, 373L, 387L, 443L, 
    579L, 821L, 1229L, 1014L, 831L, 648L, 257L, 203L, 137L, 78L, 
    82L, 69L, 45L, 51L, 45L, 63L, 55L, 54L, 52L, 27L, 24L, 12L, 
    10L, 22L, 42L, 32L, 52L, 82L, 95L, 91L, 104L, 143L, 114L, 
    100L, 83L, 113L, 145L, 175L, 222L, 258L, 384L, 755L, 976L, 
    879L, 846L, 1004L, 801L, 799L, 680L, 530L, 410L, 302L, 288L, 
    234L, 269L, 245L, 240L, 176L, 188L, 128L, 96L, 59L, 63L, 
    44L, 52L, 39L, 50L, 36L, 40L, 48L, 32L, 39L, 28L, 29L, 16L, 
    20L, 25L, 25L, 48L, 57L, 76L, 117L, 107L, 91L, 90L, 83L, 
    76L, 86L, 104L, 101L, 116L, 120L, 185L, 290L, 537L, 485L, 
    561L, 1142L, 1213L, 1235L, 1085L, 1052L, 987L, 918L, 746L, 
    620L, 396L, 280L, 214L, 148L, 148L, 94L, 107L, 69L, 55L, 
    69L, 47L, 43L, 49L, 30L, 42L, 51L, 41L, 39L, 40L, 38L, 22L, 
    37L, 26L, 40L, 56L, 54L, 74L, 99L, 114L, 114L, 120L, 114L, 
    123L, 131L, 170L, 147L, 163L, 163L, 160L, 158L, 163L, 124L, 
    115L, 176L, 171L, 214L, 320L, 507L, 902L, 1190L, 1272L, 1282L, 
    1146L, 896L, 597L, 434L, 216L, 141L, 101L, 86L, 65L, 55L, 
    35L, 49L, 29L, 55L, 53L, 57L, 34L, 43L, 42L, 13L, 17L, 20L, 
    27L, 36L, 47L, 64L, 77L, 82L, 82L, 95L, 107L, 96L, 106L, 
    93L, 114L, 102L, 116L, 128L, 123L, 212L, 203L, 165L, 267L, 
    550L, 761L, 998L, 1308L, 1613L, 1704L, 1669L, 1296L, 975L, 
    600L, 337L, 259L, 145L, 91L, 70L, 79L, 63L, 58L, 51L, 53L, 
    39L, 49L, 33L, 47L, 56L, 32L, 43L, 47L, 19L, 32L, 18L, 34L, 
    39L, 63L, 57L, 55L, 69L, 76L, 103L, 99L, 108L, 131L, 113L, 
    106L, 122L, 138L, 136L, 175L, 207L, 324L, 499L, 985L, 1674L, 
    1753L, 1419L, 1105L, 821L, 466L, 274L, 180L, 143L, 82L, 101L, 
    72L, 55L, 71L, 50L, 33L, 26L, 25L, 27L, 21L, 24L, 24L, 20L, 
    18L, 18L, 25L, 23L, 13L, 10L, 16L, 9L, 12L, 16L, 25L, 31L, 
    36L, 40L, 36L, 47L, 32L, 46L, 75L, 63L, 49L, 90L, 83L, 101L, 
    78L, 79L, 98L, 131L, 83L, 122L, 179L, 334L, 544L, 656L, 718L, 
    570L, 323L, 220L, 194L, 125L, 95L, 77L, 46L, 42L, 29L, 35L, 
    21L, 29L, 16L, 14L, 19L, 15L, 19L, 18L, 21L, 10L, 14L, 7L, 
    7L, 5L, 9L, 14L, 11L, 18L, 22L, 39L, 36L, 46L, 44L, 37L, 
    30L, 39L, 37L, 45L, 71L, 59L, 57L, 80L, 68L, 88L, 72L, 74L, 
    208L, 357L, 621L, 839L, 964L, 835L, 735L, 651L, 400L, 292L, 
    198L, 85L, 64L, 41L, 40L, 23L, 18L, 14L, 22L, 9L, 19L, 8L, 
    14L, 12L, 15L, 14L, 4L, 6L, 7L, 7L, 8L, 13L, 10L, 19L, 17L, 
    20L, 22L, 40L, 37L, 45L, 34L, 26L, 35L, 67L, 49L, 77L, 82L, 
    80L, 104L, 88L, 49L, 73L, 113L, 142L, 152L, 206L, 293L, 513L, 
    657L, 919L, 930L, 793L, 603L, 323L, 202L, 112L, 55L, 31L, 
    27L, 15L, 15L, 6L, 13L, 21L, 10L, 11L, 9L, 8L, 11L, 7L, 5L, 
    1L, 4L, 7L, 2L, 6L, 12L, 14L, 21L, 29L, 32L, 26L, 22L, 44L, 
    39L, 47L, 44L, 93L, 145L, 289L, 456L, 685L, 548L, 687L, 773L, 
    575L, 355L, 248L, 179L, 129L, 122L, 103L, 72L, 72L, 36L, 
    26L, 31L, 12L, 14L, 14L, 14L, 7L, 8L, 2L, 7L, 8L, 9L, 26L, 
    10L, 13L, 13L, 5L, 5L, 3L, 6L, 1L, 10L, 6L, 7L, 17L, 12L, 
    21L, 32L, 29L, 18L, 22L, 24L, 38L, 52L, 53L, 73L, 49L, 52L, 
    70L, 77L, 95L, 135L, 163L, 303L, 473L, 823L, 1126L, 1052L, 
    794L, 459L, 314L, 252L, 111L, 55L, 35L, 14L, 30L, 21L, 16L, 
    9L, 11L, 6L, 6L, 8L, 9L, 9L, 10L, 15L, 15L, 11L, 6L, 3L, 
    8L, 4L, 7L, 7L, 13L, 10L, 23L, 24L, 36L, 25L, 34L, 37L, 46L, 
    39L, 37L, 55L, 65L, 54L, 60L, 82L, 55L, 53L, 61L, 52L, 75L, 
    92L, 121L, 170L, 199L, 231L, 259L, 331L, 357L, 262L, 154L, 
    77L, 34L, 41L, 21L, 17L, 16L, 7L, 15L, 11L, 7L, 5L, 6L, 13L, 
    7L, 6L, 8L, 7L, 1L, 11L, 9L, 3L, 9L, 9L, 8L, 15L, 19L, 16L, 
    10L, 12L, 26L, 35L, 35L, 41L, 34L, 30L, 36L, 43L, 23L, 55L, 
    107L, 141L, 217L, 381L, 736L, 782L, 663L, 398L, 182L, 137L, 
    79L, 28L, 26L, 16L, 14L, 8L, 4L, 4L, 6L, 6L, 11L, 4L, 5L, 
    7L, 7L, 6L, 8L, 2L, 3L, 3L, 1L, 1L, 3L, 3L, 2L, 8L, 8L, 11L, 
    10L, 11L, 8L, 24L, 25L, 25L, 33L, 36L, 51L, 61L, 74L, 92L, 
    89L, 123L, 402L, 602L, 524L, 494L, 406L, 344L, 329L, 225L, 
    136L, 136L, 84L, 55L, 55L, 42L, 19L, 28L, 8L, 7L, 2L, 7L, 
    6L, 4L, 3L, 5L, 3L, 3L, 0L, 1L, 2L, 3L, 2L, 1L, 2L, 2L, 9L, 
    4L, 9L, 10L, 18L, 15L, 13L, 12L, 10L, 19L, 15L, 22L, 23L, 
    34L, 43L, 53L, 47L, 57L, 328L, 552L, 787L, 736L, 578L, 374L, 
    228L, 161L, 121L, 96L, 58L, 50L, 37L, 14L, 9L, 6L, 15L, 12L, 
    9L, 1L, 6L, 4L, 7L, 7L, 3L, 6L, 9L, 15L, 22L, 28L, 34L, 62L, 
    54L, 75L, 65L, 58L, 57L, 60L, 37L, 47L, 60L, 89L, 90L, 193L, 
    364L, 553L, 543L, 676L, 550L, 403L, 252L, 140L, 125L, 99L, 
    63L, 63L, 76L, 85L, 68L, 67L, 38L, 25L, 24L, 11L, 9L, 9L, 
    4L, 8L, 4L, 6L, 5L, 2L, 6L, 4L, 4L, 1L, 5L, 4L, 1L, 2L, 2L, 
    2L, 2L, 3L, 4L, 4L, 7L, 5L, 2L, 10L, 11L, 17L, 11L, 16L, 
    15L, 11L, 12L, 21L, 20L, 25L, 46L, 51L, 90L, 123L)), .Names = c(""date"", 
""cases""), row.names = c(NA, -835L), class = ""data.frame"")
</code></pre>
"
"0.0889588054368324","0.0904347204435887"," 63536","<p>I have a dataset with subjects ($\text{data}_0$). Each subject has a follow up time ($\text{fuy}$), an indicator for incidence ($\text{ind} = 0/1$) and a variable which indicates, for example, year of diagnosis ($\text{year}$).</p>

<p>One way (I call it ($\text{A}$)) to show the incidence rates according to year would be to compute for each year incidence rate and CI ($\text{data}_1$) and plot them against years, for example, with plotCI (library = gplots).<br/>
Another graph would be to plot incidence rate against year and overlay a smoothed curve with CI-region ($\text{B}$).</p>

<p>The advantage of ($\text{A}$) is that it considers the standard error for each year whereas this is not the case in ($\text{B}$). ($\text{B}$) considers the noise among all indicidence rates. </p>

<p>I wonder if it is possible to plot graph ($\text{B}$) directly from the original subject data ($\text{data}_0$) considering the uncertainty of each year and the jittering during the years for the CI-region (plot ($\text{C}$)). </p>

<p>Below is a fake example for clarification. I hope someone can give me some advice.
Thanks for help. 
giordano</p>

<pre><code># -- create fake data
# n subjects per year
n &lt;- 20
year &lt;- rep(c(1981:2000),each=n)
head(year)
# follow up time in years for each subject
fuy &lt;- 5 + rnorm(400,0,1)
hist(fuy)
sum(fuy&lt;=0)
# indicator of incidence: increasing incidence per year
ind &lt;- c()
for (i in seq(5,14.5,0.5)) {
  temp &lt;- i &gt; runif(20,1,20)
  ind &lt;- c(ind,temp)
}
ind &lt;- as.numeric(ind)
head(ind)

# build data frame
data0 &lt;- data.frame(year=year, fuy= fuy, ind = ind)
head(data0)
with(data0, table(year,ind, useNA='ifany'))

# -- comupute incidence rates and 95%-CI
# ""by hand""-method (poisson regression would give the same incidence rates (ir) and    Wald-CI)
library(sqldf)

data1 &lt;- sqldf(paste(""SELECT year, SUM(fuy) fuy, SUM(ind) cases, SUM(ind)/SUM(fuy) ir""
                ,"" FROM data0""
                ,"" GROUP BY year""
                )
          )
data1

# CI: log(ir) +/- sqrt(1/cases)
data1$cil &lt;- with(data1, exp(log(ir) - qnorm(0.975,0,1)*sqrt(1/cases)))
    data1$ciu &lt;- with(data1, exp(log(ir) + qnorm(0.975,0,1)*sqrt(1/cases)))

# -- (A) graph with CI
library(gplots)
with(data1,plotCI(x=year, y=ir, li=cil,ui=ciu))
savePlot(paste(getwd(),""/plot-A.jpeg"", sep=""""),type=""jpeg"")

# -- (B) graph with smoothing function
ggplot(data=data1, aes(year,ir)) +
  geom_point() +
  stat_smooth()
savePlot(paste(getwd(),""/plot-B.jpeg"", sep=""""),type=""jpeg"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/L6Syz.jpg"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/nEteq.jpg"" alt=""enter image description here""></p>
"
"0.080466267117873","0.0818012824723818"," 63681","<p>I have been adamantly searching the web to learn how to successfully implement a dynamic regression time series in the forecast package for R. The time series data that I am using is weekly data (frequency=52) of incoming call volume and prediction variables are mailers sent out every now and then. They are a significant predictor of the data for the week that they hit, the following week, and the week after that. I have created lagged variables and use these three as the predictors. </p>

<p>My main concern is that the arima model is not taking into account the time series frequency. When I tell it to recognize the ts with a frequency of 52 it has an error. 
I have looked at the <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">fortrain function</a> but do not understand it. I also have looked at the tbats suggested but found that those will not work with prediction variables. </p>

<p>The Zoo function recognizes 52 frequency but it is not advised to use with the <a href=""http://stackoverflow.com/questions/16050684/using-the-combination-forecastauto-arima"">forecast package</a>.</p>

<p>Here is the basic code. The problem is that the time series calwater[,5] is not recognized as such. It is imputed as a simple vector as an integer...</p>

<pre><code>#this works without taking into acount the ts
fit2 &lt;- auto.arima(calwater[6:96,5], xreg=calwater[6:96,6:8], d=0)
fccal &lt;- forecast(fit2, xreg=calwater[97:106,6:8], h=10)
fccal
plot(fccal, main=""Forecast Cal Water"", ylab=""Calls"")

#to form a ts object
calincall&lt;-ts(calwater[1:106,5],start=c(2011,23),frequency=52)

#once the ts is added to the model this dispalys
#Error in `[.default`(calincall, 2:100, 1) : incorrect number of dimensions
</code></pre>

<p>Maybe the error is because there is just a little over two years of data. </p>

<pre><code>#Time Series: Start = c(2011, 23), End = c(2013, 24),Frequency = 52 
</code></pre>

<p>I would be very grateful for any guidance in for this particular issue. I am using the forecast package and prefer to continue within the package but I am open to suggestions. </p>
"
"0.0464572209811883","0.0472279924554862"," 63883","<p>Is there a method to find the right distance function in non-parametric regression?
I use some time series to learn forecasting. Series are nonlinear and non-gaussian.
I can get the right dimension and delay. I can find the right bandwidth with the hdrcde library.
I have no problem with kernel functions.<br>
My problem is with distances. I use Euclidian, Cosine and Correlation weighting functions.
These are the kernel which give good results, but one time, Euclidian is good, then after adding some data, one to 5-7 generally, Euclidian give nothing, even with very little change in statistics.
So, my question is if there is a method to choose the right distance. I would like to have advices on that point and on articles that will help solve this problem. What package in R could eventually help?</p>

<p>Thank you. </p>
"
"0.0848188929679971","0.0776035104406608"," 63913","<p>I conducted an experiment in a factorial design: I measured light (PAR) in three herbivore treatments as well as six nutrient treatments. The experiment was blocked.</p>

<p>I've run the linear model as follows (you can download the data from my website to replicate)</p>

<pre><code>dat &lt;- read.csv('http://www.natelemoine.com/testDat.csv')
mod1 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
</code></pre>

<p>The residual plots look pretty good</p>

<pre><code>par(mfrow=c(2,2))
plot(mod1)
</code></pre>

<p>When I look at the ANOVA table, I see main effects of Nutrient and Herbivore. </p>

<pre><code>anova(mod1)

Analysis of Variance Table 

Response: light 
                    Df  Sum Sq Mean Sq F value    Pr(&gt;F)     
Nutrient             5  4.5603 0.91206  7.1198 5.152e-06 *** 
Herbivore            2  2.1358 1.06791  8.3364 0.0003661 *** 
BlockID              9  5.6186 0.62429  4.8734 9.663e-06 *** 
Nutrient:Herbivore  10  1.7372 0.17372  1.3561 0.2058882     
Residuals          153 19.5996 0.12810                       
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
</code></pre>

<p>However, the regression table shows non-significant main effects and significant interactions.</p>

<pre><code>summary(mod1)

Call: 
lm(formula = light ~ Nutrient * Herbivore + BlockID, data = dat) 

Residuals: 
     Min       1Q   Median       3Q      Max  
-0.96084 -0.19573  0.01328  0.24176  0.74200  

Coefficients: 
                           Estimate Std. Error t value Pr(&gt;|t|)     
(Intercept)                1.351669   0.138619   9.751  &lt; 2e-16 *** 
Nutrientb                  0.170548   0.160064   1.066  0.28833     
Nutrientc                 -0.002172   0.160064  -0.014  0.98919     
Nutrientd                 -0.163537   0.160064  -1.022  0.30854     
Nutriente                 -0.392894   0.160064  -2.455  0.01522 *   
Nutrientf                  0.137610   0.160064   0.860  0.39129     
HerbivorePaired           -0.074901   0.160064  -0.468  0.64049     
HerbivoreZebra            -0.036931   0.160064  -0.231  0.81784     
... 
Nutrientb:HerbivorePaired  0.040539   0.226364   0.179  0.85811     
Nutrientc:HerbivorePaired  0.323127   0.226364   1.427  0.15548     
Nutrientd:HerbivorePaired  0.642734   0.226364   2.839  0.00513 **  
Nutriente:HerbivorePaired  0.454013   0.226364   2.006  0.04665 *   
Nutrientf:HerbivorePaired  0.384195   0.226364   1.697  0.09168 .   
Nutrientb:HerbivoreZebra   0.064540   0.226364   0.285  0.77594     
Nutrientc:HerbivoreZebra   0.279311   0.226364   1.234  0.21913     
Nutrientd:HerbivoreZebra   0.536160   0.226364   2.369  0.01911 *   
Nutriente:HerbivoreZebra   0.394504   0.226364   1.743  0.08338 .   
Nutrientf:HerbivoreZebra   0.324598   0.226364   1.434  0.15362     
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3579 on 153 degrees of freedom 
Multiple R-squared:  0.4176,    Adjusted R-squared:  0.3186  
F-statistic: 4.219 on 26 and 153 DF,  p-value: 8.643e-09 
</code></pre>

<p>I know that this question has been previously <a href=""http://stats.stackexchange.com/questions/20002/regression-vs-anova-discrepancy"">asked and answered</a> in <a href=""http://stats.stackexchange.com/questions/28938/why-do-linear-regression-and-anova-give-different-p-value-in-case-of-consideri"">multiple posts</a>. In the earlier posts, the issue revolved around the different types of SS used in anova() and lm(). However, I don't think that is the issue here. First of all, the design is balanced:</p>

<pre><code>with(dat, tapply(light, list(Nutrient, Herbivore), length))
</code></pre>

<p>Second, using the Anova() option doesn't change the anova table. This isn't a surprise because the design is balanced.</p>

<pre><code>Anova(mod1, type=2)
Anova(mod1, type=3)
</code></pre>

<p>Changing the contrast doesn't change the results (qualitatively). I still get pretty much backwards intepretations from anova() vs. summary().</p>

<pre><code>options(contrasts=c(""contr.sum"",""contr.poly""))
mod2 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
anova(mod2)
summary(mod2)
</code></pre>

<p>I'm confused because everything I've read on regression not agreeing with ANOVA implicates differences in the way R uses SS for summary() and anova() functions. However, in the balanced design, the SS types are equivalent, and the results here don't change. How can I have completely opposite interpretations depending on which output I use?</p>
"
"0.026822089039291","0.0272670941574606"," 64130","<p>I want to include sample weights to my quantile regression model, but I'm not sure how to do this.  </p>

<p>I've already define my weight, which are replicated weights already given in survey dataset (computed in survey package): </p>

<pre><code>w&lt;-svrepdesign(variables=data[,1:10],repweights=data[,11:30],type=""BRR"", 
  combined.weights=TRUE, weights=r.weights, rho=0.5,dbname="""")
</code></pre>

<p>and my rq model is:</p>

<pre><code>rq(y~x,tau=c(.1,.2,.3,.4,.5,.6,.7,.8,.9),data=my.data))
</code></pre>

<p>I tried to use <code>withReplicates</code> function, but with no success. Any suggestions? </p>
"
"0.0379321620905441","0.0385614943639849"," 64268","<p>Hi currently I am conducting simple linear regression on two variables for data of different regions. I know that I can use the lmList function to get the coefficients at once for all the regions. But can I generate the Q-Q residual plot for all the regions in one graph with different panels at once? And for the output for lmList function, only the coefficients are displays, without R-square for each regression. How can I see that?</p>

<p>Thanks a lot for help in advance!</p>
"
"0.103881504159667","0.0985646681332268"," 64535","<p>My ecological question is: ""What are the trends in percent coral cover by island and depth across the state of Hawaii from 1999 to 2012?""  </p>

<p>I am trying to analyze this hierarchical data set using R with 10 transects at each depth, 2 depths per site, and site nested in island.</p>

<p>Data structure:</p>

<pre><code>Fixed effects:
 Island: Hawaii, Maui, Molokai, Kahoolawe, Oahu, Kauai.
 DepthCat: S = Shallow, D = Deep.
 WYear: 0-13. It was suggested that I use this factor as a covariate for years.

Random effects:
 Site: 34 sites across the 6 islands with 2 depths per site.
 Transect: 10 permanent transects at each depth.
 Year: 1999 â€“ 2012 (14 years)

Dependent variable: PercentCover
</code></pre>

<p>Currently, I am using the <code>lmer</code> function in the <code>lmerTest</code> package and this is the model that I've constructed.</p>

<pre><code>fit1 &lt;- lmer(PercentCover ~ WYear*Island*DepthCat +
             (1+WYear|Island/Site/DepthCat/Transect) + (1|Year), data=Benthic)
</code></pre>

<p>Unfortunately, the data are spotty (i.e., missing data in multiple years for a number of sites) so the model returns <code>[1] ""Asymptotic covariance matrix A is not positive!""</code>, even using arcsin transformed data. I can still run the summary statistics to get results, but I don't feel comfortable with the error message. Perhaps I have not structured the model correctly in terms of organizing the nested factors, but the number of observations for each of the levels in the summary stats seems correct. I tried different and simpler iterations of the model such as:</p>

<pre><code> fit1 &lt;- lmer(PercentCover ~ WYear + Island + DepthCat + (1+WYear|Transect/Site) + 
              (1|Year), data=Benthic)
</code></pre>

<p>which works, but doesn't give me the interaction information and returns a larger AIC suggesting that the model does not fit the data as well.</p>

<p>To deal with all of the missing data, I tried another approach by using the regression slope of percent cover over time as the dependent variable for each site X depth combination.</p>

<pre><code>Data structure:

Fixed effects:
 Island: Hawaii, Maui, Molokai, Kahoolawe, Oahu, Kauai.
 DepthCat: S = Shallow, D = Deep.

Random effects:
 Site: 34 sites across the 6 islands with 2 depths per site.
 Transect: 10 permanent transects at each depth.

Dependent variable: Trend
</code></pre>

<p>I used the following model, but the summary results did not make much sense, even after transforming the data.</p>

<pre><code> fit1&lt;-lmer(Trend ~ Island*DepthCat + (1| Island/Site/DepthCat/Transect), data=Benthic)
</code></pre>

<p>Any suggestions on improving my analytical approach would be appreciated.</p>
"
"0.0709645772411954","0.0618359572423054"," 65064","<p>I'm using R forecast package with a daily time series data, that has complex i.e. Multiple seasonality (weekly, Yearly, monthly). The fit/forecast process also needs to take into account certain day specific effects.</p>

<p>I plan to:</p>

<ul>
<li>Use auto.arima function </li>
<li>Set TS frequency to 7, to take care of the weekly seasonality</li>
<li>Use Fourier terms for 'xreg' parameter to take care of monthly and yearly seasonality</li>
<li><p>Use a regressor matrix for other effects e.g. Holidays.</p>

<ol>
<li><p>Can someone help in providing a concrete example on how to use the fourier function, which can take both monthly and yearly seasonality into account?</p></li>
<li><p>Would like to see an example of how to combine a regressor matrix with the fourier result, so that it can be assigned to 'xreg' parameter or together?</p></li>
</ol></li>
</ul>

<p>On both these questions, I have only found possibilities of the above mentioned by Dr.Hyndman, but concrete examples can really be useful to the community as well.</p>

<p>Thanks.</p>
"
"0.080466267117873","0.0818012824723818"," 65690","<p>I fit a logistic on three numeric continuous variables, followed by a categorical factor [Y, N].</p>

<pre><code>logit2A &lt;- glm(DisclosedDriver ~ VehDrvr_Dif+POL_SEQ_NUM+PRMTOTAL+SAFE_DRVR_PLEDGE_FLG, data = DF, family = ""binomial"") 
</code></pre>

<p>Fit looks wonderful.</p>

<pre><code>Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -2.204e+00  2.253e-01  -9.782  &lt; 2e-16 ***
VehDrvr_Dif            2.918e-01  1.026e-01   2.845 0.004440 ** 
POL_SEQ_NUM           -1.893e-01  5.617e-02  -3.370 0.000751 ***
PRMTOTAL               1.109e-04  5.526e-05   2.006 0.044804 *  
SAFE_DRVR_PLEDGE_FLGY -7.220e-01  1.633e-01  -4.422 9.76e-06 ***
</code></pre>

<p>So obviously R took the Safe_Drvr_Pledge_Flg categorical factor variable and placed all 'N' values in reference or intercept as opposed to the listed 'Y'.</p>

<p>Now I want to take my fit and calculate the probabilities that my model determines. And here comes the error:</p>

<pre><code>&gt; DF$P_GLM&lt;- predict.glm(logit2A, DF, type=""response"", se.fit=FALSE)
    Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
factor SAFE_DRVR_PLEDGE_FLG has new levels 
</code></pre>

<p>Umm... no it doesn't, because I just fit the model with the exact same data I'm trying to use for the prediction. What's the problem?</p>

<p>Trying to respond to first comment:
Don't know what you mean. I've got 3500 rows of data... It's a logistic regression on 4 continuous variables and one categorical. The categorical has two values, Y or N. My glm fit give the numbers given. I just want to plug it all back in with the predict function and it gives me that error. Here's the categorical variable:</p>

<pre><code> &gt; DF$SAFE_DRVR_PLEDGE_FLG
 [1] Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y Y N Y Y Y Y Y N Y Y Y Y Y Y
 [60] Y Y Y Y N Y Y Y Y Y Y Y Y N Y Y Y N N Y N Y Y Y Y Y N Y Y N Y N N Y Y Y N Y Y Y Y N Y Y Y Y Y N Y N Y N Y Y Y Y Y N Y
 [119] N Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y N Y Y Y N Y Y Y N N Y N N N Y N Y Y Y N N Y Y N Y Y Y Y N N Y Y Y Y N N Y N N
 Levels:  N Y
</code></pre>

<p>What do you mean by a working example? The fit works. The probability output of the predict function doesn't...</p>
"
"0.080466267117873","0.0818012824723818"," 65859","<p>Recently, I have read an article which name is â€œ<a href=""http://www.ncbi.nlm.nih.gov/pubmed/9618776"" rel=""nofollow""><em>Feed forward neural networks for the analysis of censored survival data: A partial logistic regression approach</em></a>â€.</p>

<p>Without a math background, I catch only a little about the ANN applying the analysis of censored survival data. </p>

<p>In the training group, each subject is replicated for all the intervals in which the subjects is observed and coupled with the event; why is this done? Especially, in the testing group, indicator each subjects are replicated into full number of time interval of observed with all event indicator as zero?</p>

<p>I know this process can take into account the censoring data.</p>

<p>In addition, are the output of neural networks are conditional probabilities of failure? My problem is how to produce the survival curve using the output data. Does anybody know the R code or MATLAB code to perform this whole process? Or give me some suggestion to find answers! The following R code is my try on this method but I can't go on it, for I don't know draw the survival curve depending on the output conditional failure probabilities!</p>

<pre><code>dat&lt;-read.csv(""traininglj.csv"",header=T)
tt &lt;- dat$time &lt;- as.numeric(cut(dat$TTR,c(0,6,12,18,24,30,36,42,48,54,200)))
dat2 &lt;- dat[rep(1:nrow(dat), tt), ]
time2 &lt;- NULL
for (i in 1:length(tt)) time2 &lt;- c(time2, 1:tt[i])
dat2$time &lt;- time2
    dat2$Recurrence &lt;- 0
dat2$Recurrence[cumsum(tt)] &lt;- dat$Recurrence
write.csv(dat2,file=""result.csv"")
mydat &lt;- apply(dat2[,13:23],2,function(x)(x-min(x,na.rm=TRUE))/
    (range(x,na.rm=TRUE)[2]-range(x,na.rm=TRUE)[1]))
training &lt;- cbind(dat2[,1:12],mydat,dat2[24])
library(nnet)
library(lattice)
attach(training)
dat.net &lt;- nnet(Recurrence ~ time+ALT+ALB+PLT+INR+age+MELD+logAFP+Diameter
        +sex+number+Gstage+HBsAg+AN+MVI, 
    data = training, 
    size = 12, 
    decay=0.025,
    maxit = 1000,
    entropy=TRUE,
    trace=TRUE)
</code></pre>
"
"0.0657004319817604","0.0556587228785024"," 65999","<p>I was thinking that in a very standard case such as a simple linear model with iid errors and no endogeneity, I would get the same results using the a simple least square estimate (such as provided by <code>lm()</code> in R) and a simple GMM estimator with an identity matrix as the weighting matrix and the matrix of regressors as instruments. </p>

<p>I've just run the following simulation in R to compare the results of the <code>lm()</code> function and of the <code>gmm()</code> function : </p>

<pre><code>&gt; library(""gmm"")
&gt; set.seed(1234567)
&gt; N  &lt;- 1000
&gt; dd  &lt;- data.frame(id = 1:N)
&gt; dd$u  &lt;- rnorm(N)
&gt; dd$x  &lt;- 1 + rnorm(N)
&gt; dd$y  &lt;- 1 + dd$x + dd$u
&gt; m1  &lt;- lm(y ~ x, data = dd)
&gt; m2  &lt;- gmm(y ~ x, x = ~ x,  wmatrix = ""ident"", data = dd)
</code></pre>

<p>I've got the same coefficients but I don't have exactly the same standard errors : </p>

<pre><code>&gt; coefficients(m1)
(Intercept)           x 
  1.0273856   0.9690455 
&gt; coefficients(m2)
(Intercept)           x 
  1.0273856   0.9690455 
&gt; sqrt(diag(vcov(m1)))
(Intercept)           x 
  0.044285    0.031726 
&gt; sqrt(diag(vcov(m2)))
(Intercept)           x 
 0.04367438  0.03127408 
</code></pre>

<p>I'm not sure to understand why the standard errors are different in this case. Is this due to statistical theory or to the <code>gmm()</code> function ? </p>

<p>See my <a href=""https://gist.github.com/pachevalier/6111962"" rel=""nofollow"">gist file</a> if you want to run the example.</p>
"
"0.026822089039291","0.0272670941574606"," 66239","<p>Is there any way to calculate variable importance in R for SVM regression and averaged neural networks?</p>

<p>I've been using <strong>caret</strong> package, that has <strong>varImp</strong> function in it</p>

<pre><code>&gt; m &lt;- best.tune(svm, train.x = descr[rownames(tr[[i]]),2:ncol(descr)], 
train.y = tr[[i]][,1],  data = df, cost = 2^(seq(0,10,5))), 
tunecontrol = tune.control(sampling = ""cross""))
&gt; varImp(m)
 Error in UseMethod(""varImp"") : 
 no applicable method for 'varImp' applied to an object of class ""svm""
</code></pre>

<p><a href=""https://stat.ethz.ch/pipermail/r-help/2010-October/257627.html"" rel=""nofollow"">According to the developer</a>, this approach wasn't realized for SVM method </p>

<p>However, <strong>rminer</strong>  package suggests such function as <em>Importance</em>. Though, it throws an error:</p>

<pre><code>VariableImportance = Importance(svmFit, data=descr[rownames(tr[[i]]), 2:ncol(descr)],    
                                method=""1D-SA"")
Error in Importance(svmFit, data = descr[rownames(tr[[i]]), 2:ncol(descr)],  : 
duplicate 'switch' defaults: 'lm == func...' and 'NULL'
</code></pre>
"
"0.0464572209811883","0.0472279924554862"," 66390","<p>Perhaps this is more of a programming question than a stats question, but I'm sure someone here has the answer.  I am new to R and am trying to run a linear regression on multiple subsets (""Cases"") of data in a single file.  I have 50 different cases, so I don't want to have to run 50 different regressions...be nice to automate this.  I have found and experimented with the <code>ddply</code> method, but this, for some reason, returns the same coefficients to me for each case.  Code I'm using is as follows:</p>

<p><code>ddply(MyData, ""Case"", function(x) coefficients(lm(Y~X1+X2+X3, MyData)))</code></p>

<p>Results I get, again, are the same coefficients for each ""Case"".  Any ideas on how I can improve my code so that the regression runs once for each case and gives me unique coefficients for each case?</p>
"
"0.053644178078582","0.0545341883149212"," 66419","<p>Is it a good or a bad practice to use R packages from CRAN for research? I'm talking about the common packages like: simple models for regression, estimation, econometrics.<br>
Most of them use function that can be written easily on your own.</p>

<p>My yes arguments:</p>

<ul>
<li>Time to focus on the main part of the research</li>
<li>The community is a good quality control</li>
<li>R offers a lot of modern methods</li>
<li>Some models are too complicated to write them on your own</li>
</ul>

<p>My con arguments:</p>

<ul>
<li>Not every package of R has been created in an academic environment</li>
<li>There could be bugs that influence the outcome and I do not know it</li>
<li>Most models can be written in a short time period from scratch</li>
</ul>

<p>How can we use open source without risking failures in the outcome? Are there certain quality indicators for packages in general and for R?</p>
"
"0.0464572209811883","0.0314853283036575"," 66937","<p>I am using a decay weighting scheme for time series regression in financial markets (idea is that more recent data is more relevant). However <code>lm()</code> residuals are different from <code>summary(lm())</code> residuals. Is there a reason for this? </p>

<pre><code>rr &lt;- rnorm(500) * 50
xx &lt;- 1:500
yy &lt;- xx + rr
dd &lt;- decay(500, 150)
sd(rr)
[1] 50.31787
sd(lm(yy ~ xx)$residuals)
    [1] 50.29099
    sd(summary(lm(yy ~ xx))$residuals)
[1] 50.29099
sd(lm(yy ~ xx, weights = dd)$residuals)
    [1] 50.29154
    sd(summary(lm(yy ~ xx, weights = dd))$residuals)
[1] 2.278805
dd2 &lt;- c(rep(1, 250), rep(2, 250))
sd(lm(yy ~ xx, weights = dd2)$residuals)
    [1] 50.29102
    sd(summary(lm(yy ~ xx, weights = dd2))$residuals)
[1] 62.66624
</code></pre>

<p>here is my decay function, by the way:</p>

<pre><code>decay &lt;- function(len, halflife, sumone = TRUE) {
#function generates an exponentially decaying series
    t &lt;- len:1 # generate a series of numbers reverse order so biggest weights last
    lambda &lt;- log(2) / halflife #figure out the lambda for the halflife
    w &lt;- exp(-lambda * t) #create the weights series  
    if(sumone) w &lt;- w / sum(w) #normalise sum to 1 if necessary
    return(w) 
}
</code></pre>
"
"0.107474459256542","0.109257567364624"," 67243","<p>This is a fairly complex question so I will attempt to ask it in a fairly basic manner. </p>

<p>I have data on the abundance of 99 different species of estuarine macroinvertebrate species and the sediment mud content (0 - 100 %) in which each observation was obtained. I have a total of 1402 observations for each species (i.e. a massive dataset). </p>

<p>Here is a subset of the raw data for one species to give you an idea of the data I'm working with (if I had 10 reputation points I'd upload a plot of real raw data):</p>

<pre><code>Abundance: 10,14,10,3,3,3,3,4,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,6,0,0,0,0,12,0,0,0,34,0,0
Mud %:     0.9,4,2,10,13,14,6,5,5,7,22,27,34,37,47,58,54,70,54,80,90,65,56,7,8,34,67,54,32,1,57,45,49,4,78,65,45,35
</code></pre>

<p>The primary aim of my research is to determine an ""optimum mud % range"" (e.g. 15 - 45 %) and ""distribution mud % range"" (e.g. 0 - 80 %) for each of the 99 invertebrate species.</p>

<p>As you can see the abundance data for the above species contains a significant number of zero values. Although this significantly skews any sort of model that I run on the data (i.e. GLM, GAM), even if I model the non-zero data only, the model for certain species does not fit the data at all well.</p>

<p>So, my question is: what would be the best, most robust way to determine an ""optimum"" and ""distribution"" mud range for each species, given that responses vary significantly between species? </p>

<hr>

<p>Just to clarify - the above data is a hypothetical example to give you an idea of how messy the abundance (that is count) data can be for a given species.</p>

<p>Regarding the poisson regression approach: I'm considering conducting a two-step GLM or GAM approach for each species; Step (1) uses logistic regression to model the ""probability of presence""  for a given species over the mud gradient - using presence/absence data. This obviously takes into account the zero counts; and Step (2) models the ""maximum abundance"" over the mud gradient - using presence only count data. This step gives me an idea of the species typical response to mud where they DO occur. What are your thoughts on this approach?</p>

<p>I have R code for both steps for one particular species. Heres the code:</p>

<pre><code>     ## BINARY

aa1&lt;-glm(Bin~Mud,dist=binomial,data=Antho)
xmin &lt;- ceiling(min(Antho$Mud))
    xmax &lt;- floor(max(Antho$Mud))
Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
pred.dat &lt;- data.frame(Mudnew)
names(pred.dat) &lt;- ""Mud""
pred.aa1 &lt;- data.frame(predict.glm(aa1, pred.dat, se.fit=TRUE, type=""response""))
pred.aa1.comb &lt;- data.frame(pred.dat, pred.aa1)
names(pred.aa1.comb)
plot(fit ~ Mud, data=pred.aa1.comb, type=""l"", lwd=2, col=1, ylab=""Probability of presence"", xlab=""Mud content (%)"", ylim=c(0,1))


## Maximum abundance

 aa2&lt;-glm(Maxabund~Mud,family=Gamma,data=antho)
 xmin &lt;- ceiling(min(antho$Mud))
     xmax &lt;- floor(max(antho$Mud))
 Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
 pred.dat &lt;- data.frame(Mudnew)
 names(pred.dat) &lt;- ""Mud""
 pred.aa2 &lt;- data.frame(predict.glm(aa2, pred.dat, se.fit=TRUE, type=""response""))
 pred.aa2.comb &lt;- data.frame(pred.dat, pred.aa2)
 names(pred.aa2.comb)
 plot(fit ~ Mud, data=pred.aa2.comb, type=""l"", lwd=2, col=1, ylab=""Maximum abundance per 0.0132 m2"", xlab=""Mud content (%)"")
 AIC(aa2)
</code></pre>

<p>My question is: for step (2); will the model code need to be altered (i.e. family=) depending on the shape of each species abundance data, if so, would I just need to inspect a scatter plot of the raw presence only abundance data to confirm the use of a certain function? and how would the code be written for a certain species exhibiting a certain response/functional form? </p>
"
"0.0379321620905441","0.0385614943639849"," 67257","<p>I have difficulties fitting a joint model in <code>R</code>. My data consists of two responses <code>X</code> &amp; <code>Y</code> and one predictor variable <code>Z</code>. Now I want to model both <code>X</code> and <code>Y</code> in function of <code>Z</code> (just linear regression: $E(X|Z)=Z\alpha$ and $E(Y|Z)=Z\beta$, both outcomes are normally distributed) but while doing so I also want to estimate the variance covariance matrix since it is the correlation between <code>X</code> and <code>Y</code> that I am interested in.</p>

<p>I already looked into a couple of functions (<code>lm</code>, <code>mcer</code>, <code>lme</code>) but it doesn't seem to do the trick. Is there something that I am overlooking in a certain package or a new suggestions to try?</p>
"
"NaN","NaN"," 67363","<p>I am running a Bayesian regression model by WinGUBS via R2WinBUGS package in R. Everything looks fine except for one parameter:</p>

<pre><code>nlssim$summary[37,][c(1,2,8,9)]
    mean           sd         Rhat        n.eff 
3.054326e-05 9.523965e-06 1.000000e+00 1.000000e+00 
</code></pre>

<p>The number of effective size is 1! It seems to indicate that the autocorrelation between samples are extremely large, but the trace plot looks all fine. And I then try the effectiveSize function in code package and the result is 4590.</p>

<p>I am curious how WinBUGS calculate the n.eff statistic, and if n.eff=1 is symptom of some of my mistakes?</p>

<p>I run 3 chains in parallel, and each chain has 60000 iterations. n.burnin=200, n.thin=30</p>

<p>Thank you very much in advance!</p>
"
"0.0464572209811883","0.0472279924554862"," 67470","<p>I want to predict a categorical variable using also categorical predictors. Currently, I am looking at classification and regression trees (CART).</p>

<p>The prediction quality is ""good enough"", except for the presence of impossible combinations. In the following minimal example, the combination <code>a==2, b==2</code> is impossible, yet the estimation decides not to use <code>b</code> for splitting.</p>

<pre><code>&gt; library(rpart)
&gt; d &lt;- data.frame(a=rep(factor(c(1,1,2)), 100000), b=factor(c(1,2,1)))
&gt; xtabs(~., d)
   b
a       1     2
  1 1e+05 1e+05
  2 1e+05 0e+00
&gt; (tr &lt;- rpart(a~b, d))
n= 300000 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 300000 1e+05 1 (0.6666667 0.3333333) *
</code></pre>

<p>When simulating stochastically from this model (by choosing the leaf value by sampling using the annotated probability vector, here $(2/3, 1/3)$, as weights), the combination <code>2, 2</code> will occur:</p>

<pre><code>&gt; prob.m &lt;- predict(tr, d, type=""prob"")
&gt; d$a.sim &lt;- apply(prob.m, 1, function(x) sample.int(length(x), size=1, prob=x))
&gt; xtabs(~a.sim+b, d)
     b
a.sim      1      2
    1 133041  66615
    2  66959  33385
</code></pre>

<p>Is there a way to avoid this, perhaps using another method?</p>

<p>This is just a small example for a more general case. I have around 10 predictors, and I want to exclude all combinations of two (or perhaps three) attributes that have no observation in the sample.</p>

<p>I am aware of the ""loss matrix"" that can be specified as a parameter to <code>rpart</code>, but this is prohibitive if many predictors are used.</p>
"
"0.0379321620905441","0.0385614943639849"," 67473","<p>I'm using <a href=""http://cran.r-project.org/web/packages/kernlab/index.html"" rel=""nofollow"">kernlab package</a></p>

<p>Here are two examples:<br/>
First:</p>

<pre><code>library(kernlab)
x &lt;- runif(1020, 1, 5000)
y &lt;- sqrt(x)
model.vanilla &lt;- rvm(x, y, kernel='vanilladot')
</code></pre>

<p>Got error:</p>

<pre><code>Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) :
the leading minor of order 2 is not positive definite
</code></pre>

<p>Second:</p>

<pre><code>library(kernlab)
x &lt;- runif(1020, 1, 5000)
y &lt;- sqrt(x)
model.rbf &lt;- rvm(x[1:1000], y[1:1000], kernel='rbfdot')
print(model.rbf)
py.rbf &lt;- predict(model.rbf, x[1001:1020])
print(paste(""MSE: "", sum((py.rbf - y[1001:1020]) ^ 2) / length(py.rbf)))
</code></pre>

<p>OK:</p>

<pre><code>Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Relevance Vector Machine object of class ""rvm"" 
Problem type: regression 

Gaussian Radial Basis kernel function. 
 Hyperparameter : sigma =  5.44268665122008e-06 

Number of Relevance Vectors : 247 
Variance :  4.368e-06
Training error : 3.418e-06 
[1] ""MSE:  4.921706631013e-05""
</code></pre>

<p>Why doesn't using linear kernel work here? <code>polydot</code> (polynomial kernel function) doesn't work either.</p>

<p>Can this be fixed?</p>
"
"0.026822089039291","0.0272670941574606"," 67484","<p>I've got a set of input-output vector pairs, and I want to find a function that approximates the output vectors from the input vectors. Specifically, I want a <em>matrix</em> by which to multiply an input vector such that I get a good approximation for the output.</p>

<p>I guess <em>linear</em> regression is not what I'm looking for, or else I wouldn't know what to do with the linear function. So what sort of regression do I need to apply here? Literature tips and / or Java / Python / R packages are very welcome!</p>
"
"0.0479808115123254","0.0487768608679754"," 67958","<p>Here is a regression realized with R</p>

<pre><code>&gt; degree=2    
&gt; frml = formula(A~factor(B)*poly(C,degree=degree))
&gt; m = lm(frml,data=data)
&gt; summary(m)$coeff

                                               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                                    0.050882   0.001591  31.980  &lt; 2e-16 ***
factor(B)2                                     0.090513   0.002250  40.227  &lt; 2e-16 ***
factor(B)3                                     0.245776   0.002250 109.231  &lt; 2e-16 ***
factor(B)4                                     0.483829   0.002250 215.030  &lt; 2e-16 ***
factor(B)5                                     0.741211   0.002250 329.419  &lt; 2e-16 ***
factor(B)6                                     0.907053   0.002250 403.125  &lt; 2e-16 ***
poly(C, degree)1                               0.008182   0.016988   0.482  0.63114    
poly(C, degree)2                              -0.004527   0.016988  -0.266  0.79044    
factor(B)2:poly(C, degree)1                   -0.014671   0.024024  -0.611  0.54284    
factor(B)3:poly(C, degree)1                    0.010721   0.024024   0.446  0.65640    
factor(B)4:poly(C, degree)1                    0.037756   0.024024   1.572  0.11933    
factor(B)5:poly(C, degree)1                    0.031446   0.024024   1.309  0.19368    
factor(B)6:poly(C, degree)1                    0.011876   0.024024   0.494  0.62220    
factor(B)2:poly(C, degree)2                    0.006151   0.024024   0.256  0.79846    
factor(B)3:poly(C, degree)2                   -0.033512   0.024024  -1.395  0.16625    
factor(B)4:poly(C, degree)2                   -0.064164   0.024024  -2.671  0.00889 ** 
factor(B)5:poly(C, degree)2                   -0.008938   0.024024  -0.372  0.71068    
factor(B)6:poly(C, degree)2                   -0.024527   0.024024  -1.021  0.30985 
</code></pre>

<p>What do we call a ""predicted line""? How can we calculate this function from these data?</p>

<p>I think it should be something like <code>y=0.050882+0.090513+0.008182x-0.004527x^2+...</code>Is it correct? What would be the following. Are there other name for this ""predicted line""?</p>

<p>What is the difference between adding this predicting line on a plot than adding a simple lm (of the first or second degree)?</p>

<p>What would be the predicted line if I computed B as a covariate (measuring a surface) instead of a factor?</p>

<p>Thanks a lot for your help!</p>
"
"0.0464572209811883","0.0472279924554862"," 68351","<p>I have a question regarding to the concept of robust standard errors. What I found about that topic is, that one can estimate the robust standard error for regression coefficients to eliminate problems with heteroscedasticity (when one wants to interpret a model). I want to know if there is a way not only to determine robust standard errors of coefficients but also of the standard error of the overall regression (residual standard error). When its possible, how can I calculate such a value in general?</p>

<p>Because I'm using R its also interesting for me if there is a <code>R-function</code> for this problem (I only know the <code>sandwich-package</code> for the normal robust SE of the coefficients).</p>

<p>Thanks.</p>
"
"0.080466267117873","0.0818012824723818"," 68553","<p>I am trying to run a logistic regression analysis in R using the speedglm package. 
The data is CNVs (a type of genetic variant), whether that CNV occurred in a case or control and wether genes in a pathway is hit by the CNV or not (Pathway.hit), and how many genes were hit by the CNV that were not in the pathway (Pathway.out).
I run two models with and without the Pathway.hit covariate and compare to see if a pathway is preferentially hit by cases vs controls.  </p>

<p>the models and comparison of said are as follows:</p>

<pre><code>fit1 = speedglm(status~size+Pathway.out, data=cnvs, family=binomial('logit'))
fit2 = speedglm(status~size+Pathway.out+Pathway.hit, data=cnvs,family=binomial('logit'))
P.anova = 1-pchisq(abs(fit1$deviance - fit2$deviance), abs(fit1$df - fit2$df))
</code></pre>

<p>It seems to work okay for most data I throw at it, but in a few cases I get the error:</p>

<pre><code>Error in solve.default(XTX, XTz, tol = tol.solve) : 
  system is computationally singular: reciprocal condition number = 1.87978e-16
</code></pre>

<p>After some googling around I think I found what's causing the problem:</p>

<pre><code>by(cnvs$Pathway.hit, cnvs$status, summary)
cnvs$status: 1 (controls)
        0     1 
    13333     0 
    ------------------------------------ 
    cnvs$status: 2 (cases)
    0     1 
10258     2 
</code></pre>

<p>So here there no observations in controls and only 2 in cases. </p>

<p>If I use with normal glm method however, then it does not throw an error (but that of course doesn't necessarily mean the results will be meaningful). The reason I am using the speedglm package is that I have approximately 16,000 of these analyses to run, and using the base glm function for all 16,000 takes about 20 hours, where as I think speedglm can reduce it down to 8 or so.</p>

<p>So my question is, should I ignore those analyses which throw an error and list the results as NA as there were too few observations, or when speed glm fails should I retry with normal glm? In the above example there are 2 observations of the covariate in cases, but 0 in controls. Might this not be interesting? Would the analysis also fail if there were 0 in controls and 20 in cases - that would certainly be interesting would it not?</p>

<p>Thanks for the help in advance,
Cheers,
Davy</p>
"
"0.0715255707714427","0.0818012824723818"," 69524","<p>I am trying to fit a nonlinear regression model in R using <code>nls()</code>. I have a form of the equation I want to fit to:</p>

<p>$$y = (a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e)$$</p>

<p>where the coefficients to be found in regression are a,b,c,d, and e. My data is output from a simulation model where $x_{1}$, $x_{2}$, and $x_{3}$ are all integers from $0$ to $10$, with the condition that $x_{1} + x_{2} + x_{3} \le 10$. $y$ is also integer valued and ranges from $0$ to roughly $1000$. The objective is to fit these data to a rate function that will be used in a Markov Chain.</p>

<p>When I try to fit this regression model directly using <code>nls()</code>, my <code>nlsResiduals</code> plot looks like this:</p>

<p><img src=""http://i.stack.imgur.com/6scJ3.png"" alt=""nls residuals""></p>

<p>I know that autocorrelated residuals are problematic, and that non-normal residuals can also be problematic. How can I fix this problem? I was thinking of using transforms on the data like</p>

<p>$$\log(y) = \log((a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e))$$</p>

<p>or</p>

<p>$$y^{1/n} = ((a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e))^{1/n}$$  where $n &gt; 1$. I've noticed if $n$ increases, my autocorrelation graph and QQ-plot look ""better"" (i.e., more scattered and more normal, respectively). </p>

<p>Both of these seem to correct a lot (but not all) of the autocorrelated residuals, and help to make the residuals more normally distributed. Am I on the right track here, or am I committing some cardinal sin in statistics? Once I settle on a transformation, how can I tell which is best?</p>

<p>Any help, suggestions, or comments are very appreciated.</p>
"
"0.0464572209811883","0.0472279924554862"," 69860","<p>For the purpose of model selection, I am using the Bayes' factor to compare different combinations of predictors in a linear regression model.</p>

<p>I have used the function <code>regressionBF()</code> from the <code>library(BayesFactor)</code>, and I got the following results:</p>

<pre><code># &gt; regressionBF(return ~ FSCR + VAL, data = dataf)

# Bayes factor analysis
# --------------
#[1] FSCR       : 65.17482  Â±0%
#[2] VAL        : 0.1979875 Â±0.02%
#[3] FSCR + VAL : 23.58704  Â±0%

#Against denominator:
#  Intercept only 
</code></pre>

<p>I am not sure how to interpret these results. What do the percentage numbers next to the Bayes' factors mean?
Also, 65 and 23 seem pretty high for a Bayes' factor. How can I interpret that?</p>

<p>Any help would be appreciated. Thanks!</p>
"
"0.114975755968081","0.116883318044855"," 69957","<p>Here's my issue of the day:</p>

<p>At the moment I'm teaching myself Econometrics and making use of logistic regression. I have some SAS code and I want to be sure I understand it well first before trying to convert it to R. (I don't have and I don't know SAS). In this code, I want to model the probability for one person to be an 'unemployed employee'. By this I mean ""age"" between 15 and 64, and ""tact"" = ""jobless"". I want to try to predict this outcome with the following variables: sex, age and idnat (nationality number). (Other things being equal).</p>

<p>SAS code :</p>

<pre><code>/* Unemployment rate : number of unemployment amongst the workforce */
proc logistic data=census;
class sex(ref=""Man"") age idnat(ref=""spanish"") / param=glm;
class tact (ref=first);
model tact = sex age idnat / link=logit;
where 15&lt;=age&lt;=64 and tact in (""Employee"" ""Jobless"");
weight weight;
format age ageC. tact $activity. idnat $nat_dom. inat $nationalty. sex $M_W.;

lsmeans sex / obsmargins ilink;
lsmeans idnat / obsmargins ilink;
lsmeans age / obsmargins ilink;
run;
</code></pre>

<p>This is a sample of what the database should looks like :</p>

<pre><code>      idnat     sex     age  tact      
 [1,] ""english"" ""Woman"" ""42"" ""Employee""
 [2,] ""french""  ""Woman"" ""31"" ""Jobless"" 
 [3,] ""spanish"" ""Woman"" ""19"" ""Employee""
 [4,] ""english"" ""Man""   ""45"" ""Jobless"" 
 [5,] ""english"" ""Man""   ""34"" ""Employee""
 [6,] ""spanish"" ""Woman"" ""25"" ""Employee""
 [7,] ""spanish"" ""Man""   ""39"" ""Jobless"" 
 [8,] ""spanish"" ""Woman"" ""44"" ""Jobless"" 
 [9,] ""spanish"" ""Man""   ""29"" ""Employee""
[10,] ""spanish"" ""Man""   ""62"" ""Retired"" 
[11,] ""spanish"" ""Man""   ""64"" ""Retired"" 
[12,] ""english"" ""Woman"" ""53"" ""Jobless"" 
[13,] ""english"" ""Man""   ""43"" ""Jobless"" 
[14,] ""french""  ""Man""   ""61"" ""Retired"" 
[15,] ""french""  ""Man""   ""50"" ""Employee""
</code></pre>

<p>This is the kind of result I wish to get :</p>

<pre><code>Variable    Modality    Value   ChiSq   Indicator
Sex         Women       56.6%   0.00001 -8.9%
            Men         65.5%       
Nationality 
            1:Spanish   62.6%       
            2:French    51.2%   0.00001 -11.4%
            3:English   48.0%   0.00001 -14.6%
Age 
            &lt;25yo       33.1%   0.00001 -44.9%
        Ref:26&lt;x&lt;54yo   78.0%       
            55yo=&lt;      48.7%   0.00001 -29.3%
</code></pre>

<p>Indicator is P(category)-P(ref)
(I interpret the above as follows: other things being equal, women have -8.9% chance of being employed vs men and those aged less than 25 have a -44.9% chance of being employed than those aged between 26 and 54).</p>

<p>So if I understand well, the best approach would be to use a binary logistic regression (link=logit). This uses references ""male vs female""(sex), ""employee vs jobless""(from 'tact' variable)... I presume 'tact' is automatically converted to a binary (0-1) variable by SAS.</p>

<p>Here is my 1st attempt in R. I haven't check it yet (need my own PC) :</p>

<pre><code>### before using glm function 
### change all predictors to factors and relevel reference
recens$sex &lt;- relevel(factor(recens$sex), ref = ""Man"")
recens$idnat &lt;- relevel(factor(recens$idnat), ref = ""spanish"")  
recens$tact &lt;- relevel(factor(recens$tact), ref = ""Employee"")
recens$ageC &lt;- relevel(factor(recens$ageC), ref = ""Ref : De 26 a 54 ans"")

### Calculations of the probabilities with function glm, 
### formatted variables, and conditions with subset restriction to ""from 15yo to 64""
### and ""employee"" and ""jobless"" only.
glm1 &lt;- glm(activite ~ sex + ageC + idnat, data=recens, weights = weight, 
            subset= recens$age[(15&lt;= recens$age | recens$age &lt;= 64)] 
            &amp; recens$tact %in% c(""Employee"",""Jobless""), 
            family=quasibinomial(""logit""))
</code></pre>

<p>My questions :</p>

<p>For the moment, it seems there are many functions to carry out a logistic regression in R like glm which seems to fit.</p>

<p>However after visiting many forums it seems a lot of people recommend not trying to exactly reproduce SAS PROC LOGISTIC, particularly the function LSMEANS. Dr Franck Harrel, (author of package:rms) for one.</p>

<p>That said, I guess my big issue is LSMEANS and its options Obsmargins and ILINK. Even after reading over its description repeatedly I can hardly understand how it works.</p>

<p>So far, what I understand of Obsmargin is that it respects the structure of the total population of the database (i.e. calculations are done with proportions of the total population). ILINK appears to be used to obtain the predicted probability value (jobless rate, employment rate) for each of the predictors (e.g. female then male) rather than the value found by the (exponential) model?</p>

<p>In short, how could this be done through R, with lrm from rms or lsmeans?</p>

<p>I'm really lost in all of this. If someone could explain it to me better and tell me if I'm on the right track it would make my day.</p>

<p>Thank you for your help and sorry for all the mistakes my English is a bit rusty.</p>

<p>Binh</p>
"
"0.053644178078582","0.0545341883149212"," 70188","<p>I have $Y$ measurements per several Subjects and I'm studying impact of factor on $Y$ measurements. I've fit a lognormal mixed model with a random interaction, but I'm finding autoregressive dependence on residuals. However, some exploration suggests different temporal dependence per subject.</p>

<p>Can <code>lme()</code> incorporate different autoregressive dependence by subject? That is, can <code>lme()</code> implement a different AR order per subject?</p>

<p>I could do this fairly easily by dividing the residuals by subject and using the function <code>ar()</code>. But I'm hoping there's a more cohesive alternative in R. </p>
"
"0.0758643241810882","0.0771229887279699"," 70249","<p>I would like to use GLM and Elastic Net to select those relevant features + build a linear regression model (i.e., both prediction and understanding, so it would be better to be left with relatively few parameters). The output is continuous. It's $20000$ genes per $50$ cases. I've been reading about the <code>glmnet</code> package, but I'm not 100% sure about the steps to follow:</p>

<ol>
<li><p>Perform CV to choose lambda:<br>
<code>cv &lt;- cv.glmnet(x,y,alpha=0.5)</code><br>
<strong>(Q1)</strong> given the input data, would you choose a different alpha value?<br>
<strong>(Q2)</strong> do I need to do something else before build the model?</p></li>
<li><p>Fit the model:<br>
<code>model=glmnet(x,y,type.gaussian=""covariance"",lambda=cv$lambda.min)</code><br>
<strong>(Q3)</strong> anything better than ""covariance""?<br>
<strong>(Q4)</strong> If lambda was chosen by CV, why does this step need <code>nlambda=</code>?<br>
<strong>(Q5)</strong> is it better to use <code>lambda.min</code> or <code>lambda.1se</code>?</p></li>
<li><p>Obtain the coefficients, to see which parameters have fallen out ("".""):<br>
<code>predict(model, type=""coefficients"")</code></p>

<p>In the help page there are many <code>predict</code> methods (e.g., <code>predict.fishnet</code>, <code>predict.glmnet</code>, <code>predict.lognet</code>, etc). But any ""plain"" predict as I saw on an example.<br>
<strong>(Q6)</strong> Should I use <code>predict</code> or <code>predict.glmnet</code> or other?</p></li>
</ol>

<p>Despite what I've read about regularization methods, I'm quite new in R and in these statistical packages, so it's difficult to be sure if I'm adapting my problem to the code. Any suggestions will be welcomed.</p>

<p><strong>UPDATE</strong><br>
<a href=""http://www.jstatsoft.org/v28/i05/paper"">Based on</a> ""As previously noted, an object of class train contains an element called <code>finalModel</code>, which is the fitted model with the tuning parameter values selected by resampling. This object can be used in the traditional way to generate predictions for new samples, using that model's
predict function.""  </p>

<p>Using <code>caret</code> to tune both alpha and lambda:    </p>

<pre><code>  trc = trainControl(method=cv, number=10)  
  fitM = train(x, y, trControl = trC, method=""glmnet"")  
</code></pre>

<p>Does <code>fitM</code> replace previous step 2? If so, how to specify the glmnet options (<code>type.gaussian=""naive"",lambda=cv$lambda.min/1se</code>) now?<br>
And the following <code>predict</code> step, can I replace <code>model</code> to <code>fitM</code>?</p>

<p>If I do  </p>

<pre><code>  trc = trainControl(method=cv, number=10)  
  fitM = train(x, y, trControl = trC, method=""glmnet"")  
  predict(fitM$finalModel, type=""coefficients"")
</code></pre>

<p>does it make sense at all or am I incorrectly mixing both package vocabulary?</p>
"
"0.0402331335589365","0.0409006412361909"," 70675","<p>How should I read the output of the function <code>ar</code> in R. For example, take this VAR model:</p>

<pre><code>library(tseries)
data(USeconomic)
US.ar &lt;- ar(cbind(GNP, M1), method=""ols"",
            dmean=T, intercept=F)
</code></pre>

<p>(from the book <code>Introductory Time Series with R</code> by Cowpertwait)</p>

<p>So <code>Us.ar</code> produces the following output:</p>

<pre><code>&gt; US.ar

Call:
ar(x = cbind(GNP, M1), method = ""ols"", dmean = T, intercept = F)

$ar
, , 1

         GNP    M1
GNP  1.27181 1.167
M1  -0.03383 1.588

, , 2

         GNP      M1
GNP -0.00423 -0.6942
M1   0.06354 -0.4839

, , 3

         GNP      M1
GNP -0.26715 -0.5103
M1  -0.02859 -0.1295


$var.pred
       GNP    M1
GNP 618.69 16.38
M1   16.38 23.90
</code></pre>

<p>and <code>US.ar$ar</code> gives this other representation:</p>

<pre><code>&gt; US.ar$ar

, , GNP

           GNP          M1
1  1.271812104 -0.03383385
2 -0.004229937  0.06353801
3 -0.267154022 -0.02858942

, , M1

         GNP         M1
1  1.1674655  1.5876695
2 -0.6941813 -0.4838919
3 -0.5103451 -0.1294549
</code></pre>

<p>As I understand it, <code>,,1</code> refers to the order of the coefficient in the autoregression model. In the second representation, <code>, , M1</code> refers to the columns, whereas <code>GNP         M1</code> describe rows and the numbers describe the order. I think this output is describing the following model:</p>

<p>$$GNP_{t} = 1.27 GNP_{t-1} + 1.167 M1_{t-1} - 0.004 GNP_{t-2} - 0.6942 M1_{t-2} - 0.2671 GNP_{t-3} - 0.5104 M1_{t-3}$$</p>

<p>$$M1_{t} = - 0.03 GNP_{t-1} + 1.58 M1_{t-1} + 0.063 GNP_{t-2} - 0.48M1_{t-2} - 0.02 GNP_{t-3} - 0.12 M1_{t-3}$$</p>

<p>Therefore, in matrix notation, this should be equal to:
$$
\begin{pmatrix}
GNP_{t} \\
M1_{t} \\
\end{pmatrix} = \left[ \begin{pmatrix}
1.27 &amp; 1.167 \\
-0.03 &amp; 1.58 \\
\end{pmatrix}x + 
\begin{pmatrix}
-0.004 &amp; -0.6942 \\
0.063 &amp; -0.48 \\
\end{pmatrix}x^{2} +
\begin{pmatrix}
-0.267 &amp; -0.5104 \\
-0.02 &amp; -0.12 \\
\end{pmatrix}x^{3} \right] 
\begin{pmatrix}
GNP_{t} \\
M1_{t} \\
\end{pmatrix}$$</p>

<p>However, the book says that the model is this:</p>

<p><img src=""http://i.stack.imgur.com/lXHJA.png"" alt=""enter image description here""></p>

<p>As you can see, cross-terms are flipped. Is this a mistake?</p>
"
"0.0599760143904067","0.0609710760849692"," 70764","<p>I am looking at a time series which has no obvious trend, but seems to have an intercept a little above zero. The results I get for <a href=""http://rss.acs.unt.edu/Rdoc/library/urca/html/ur.df.html"" rel=""nofollow""><code>ur.df</code></a> function in R is the following:</p>

<pre><code>logprice_df &lt;- ur.df(test3, lags = 1, type= 'trend')
summary(logprice_df)

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.50614 -0.04394  0.00134  0.03859  0.64408 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.841e-02  8.268e-03   2.226  0.02626 *  
z.lag.1     -1.573e-02  5.635e-03  -2.791  0.00537 ** 
tt           9.234e-06  1.080e-05   0.855  0.39272    
z.diff.lag   1.411e-01  3.364e-02   4.195 3.01e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.07512 on 865 degrees of freedom
Multiple R-squared: 0.02651,    Adjusted R-squared: 0.02314 
F-statistic: 7.852 on 3 and 865 DF,  p-value: 3.572e-05 


Value of test-statistic is: -2.791 2.6012 3.8997 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -3.96 -3.41 -3.12
phi2  6.09  4.68  4.03
phi3  8.27  6.25  5.34
</code></pre>

<p>The problem is that I do not understand how one shall interpret these values. What is <code>F-statistic</code>? And what is the probabilities below <code>Pr(&gt;|t|)</code> in the first table, and also <code>Value of test-statistic</code>?</p>

<p>I really appreciate any help.</p>
"
"0.0599760143904067","0.0487768608679754"," 71224","<p>I have a dataset that I have to perform a regression task. I split the dataset into 80% for training and 20% for validation.</p>

<pre><code>train.index &lt;- sample(N.rows, N.rows * 4/5, F)
train.data &lt;- data[train.index,]
val.data &lt;- data[-train.index,]
</code></pre>

<p>I used the GBM in R which has the parameters specified (and other take defaults). However, this applies to any kind of algorithms with a grain of randomness embedded.</p>

<pre><code>train.model &lt;- function(train.data, val.data)
{
    # 1) model &lt;- gbm(formula = /* ... */, data = train.data, n.trees = 1000, interaction.depth = 3, n.minobsinnode = 5, shrinkage =  0.01, distribution = ""gaussian"")
    # 2) predict via model on val.data
    # 3) calculate error
}
</code></pre>

<p>Suppose I executed train.model with the same train.data and val.data every time, and I do it about 50 times. The error (Mean absolute error) percentile is about 5-10%, which is rather high. I could select the best model out of the 50 runs, but this assumes I have a validation set which I do not use as training set.</p>

<p>My question is this:
<strong>Suppose I am using all the data for training (normal cross validation practice), how could I tell if the model is performing poorly and avoid these major fluctuations?</strong></p>

<p>Thank you.</p>
"
"0.134479896158686","0.136711051308229"," 71414","<p>I'm trying to use a zero-inflated gamma model (or a gamma 'hurdle' model). The model is a mixture of logistic regression and generalized linear modeling. I can do this analysis in two steps: 1) do a logistic regression against presence/absence data and then 2) Use a generalized linear model with a gamma distribution on the positive values. I'm trying to set the model up where, if y = 0, then E(y) = p, but if y > 0, then E(y) is gamma distributed.</p>

<p>I'm trying to set this up in BUGS/JAGS, because I've seen these models <a href=""http://stats.stackexchange.com/questions/15967/how-can-i-set-up-a-zero-inflated-poisson-in-jags"">worked before for poisson-distributions</a>. I tried to adapt the code from that link into a gamma distribution, but I can't quite seem to get it to work. I know the code won't work because my data have zeroes, and the likelihood function can't be evaluated with zeroes. However, even when restricted to positive values, I get errors for invalid parent values. Even so, I'm not even sure the model is specified correctly.</p>

<p>Here is the model:</p>

<pre><code># For the ones trick
C &lt;- 10000

# for every observation
for(i in 1:N){
    # log-likelihood of the observation from the gamma likelihood
    LogPos[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - N*shape[i]*log(scale[i]) - N*loggam(scale[i])
    #likelihood
    Lpos[i] &lt;- exp(LogPos[i])

    # redefine the shape and rate parameters as a function of the mean and sd
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # mu is a function of MTD: use the inverse link
    #mu[i] &lt;- 1/eta[i]
    mu[i] &lt;- beta0 + beta1*MTD[i]


    # zero-inflated part, where w[i] is the probability of being zero
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # ones trick
    p[i] &lt;- Lpos[i] / C
    ones[i] ~ dbern(p[i])

    # Full likelihood
    Lik[i] &lt;- Lpos[i]*(1 - w[i]) + equals(y[i], 0)*w[i]
  } 

# PRIORS
beta0 ~ dnorm(0, 0.001)
beta1 ~ dnorm(0, 0.001)

gamma0 ~ dnorm(0, 0.001)
gamma1 ~ dnorm(0, 0.001)

sd ~ dunif(0, 100)
</code></pre>

<p>Has anyone set a model up like this or have any advice on how to set it up correctly?</p>

<p><strong>UPDATE</strong></p>

<p>I've tried a new set of code that's similar, but slight different. I still have not gotten it to work</p>

<pre><code>model{

  # For the ones trick
  C &lt;- 10000

  # for every observation
  for(i in 1:N){

    # make a dummy variable that is 0 if y is &lt; 0.0001 and 1 if y &gt; 0.0001. This is essentially a presence
    # absence dummy variable
    z[i] &lt;- step(y[i] - 0.0001)

    # define the logistic regression model, where w is the probability of occurance.
    # use the logistic transformation exp(z)/(1 + exp(z)), where z is a linear function
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # define the gamma regression model for the mean. use the log link the ensure positive, non-zero mu
    mu[i] &lt;- exp(eta[i])
    eta[i] &lt;- beta0 + beta1*MTD[i]

    # redefine the mu and sd of the continuous part into the shape and scale parameters
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # for readability, define the log-likelihood of the gamma here
    logGamma[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i])

    # define the total likelihood, where the likelihood is (1 - w) if y &lt; 0.0001 (z = 0) or
    # the likelihood is w * gammalik if y &gt;= 0.0001 (z = 1). So if z = 1, then the first bit must be
    # 0 and the second bit 1. Use 1 - z, which is 0 if y &gt; 0.0001 and 1 if y &lt; 0.0001
    logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + logGamma[i] )

    # Use the ones trick
    p[i] &lt;- logLik[i] / C
    ones[i] ~ dbern(p[i])
  } 

  # PRIORS
  beta0 ~ dnorm(0, 0.001)
  beta1 ~ dnorm(0, 0.001)

  gamma0 ~ dnorm(0, 0.001)
  gamma1 ~ dnorm(0, 0.001)

  sd ~ dgamma(1, 2)

}
</code></pre>

<p><strong>UPDATE 2</strong></p>

<p>I've gotten it to run by deleting the definition of logGamma[i] and putting it directly into the likelihood function, which now reads:</p>

<pre><code>logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i]) )
</code></pre>

<p>The problem was that JAGS was trying to evaluate the log likelihood at all observations first, resulting in NA's for the 0's. In the new way, it only evaluates it if z = 1 (I think). I'm still having trouble getting the parameter estimates to line up. For example, the gamma's are almost identical to a logistic regression of the same form (hooray!). But the betas are pretty far off from a gamma GLM of the positive values. I don't know if this is normal or not, but I suspect there are still problems with my model specification.</p>
"
"0.0758643241810882","0.0771229887279699"," 71924","<p><em><a href=""http://stackoverflow.com/q/19186966/1414455"">Cross-posted from SO.</a></em></p>

<p>I am trying to replicate the results of <code>bgtest</code> from the <a href=""http://cran.r-project.org/web/packages/lmtest/lmtest.pdf"" rel=""nofollow""><code>lmtest</code></a> R package.</p>

<p>I am using the following dataset:</p>

<pre><code>           rs   month   r20
1    2.365042  1952m3  4.33
2    2.317500  1952m4  4.23
3    2.350833  1952m5  4.36
4    2.451833  1952m6  4.57
5    2.466167  1952m7  4.36
6    2.468417  1952m8  4.11
7    2.485583  1952m9  4.20
8    2.415125 1952m10  4.19
9    2.389875 1952m11  4.15
10   2.418167 1952m12  4.22
11   2.396042  1953m1  4.13
12   2.401042  1953m2  4.10
13   2.400833  1953m3  4.04
14   2.383500  1953m4  3.94
15   2.366708  1953m5  3.95
16   2.365625  1953m6  4.02
17   2.348583  1953m7  3.98
18   2.334375  1953m8  3.94
19   2.133542  1953m9  3.78
20   2.097375 1953m10  3.80
21   2.097708 1953m11  3.78
22   2.130583 1953m12  3.83
23   2.096000  1954m1  3.79
24   2.064042  1954m2  3.79
25   2.115083  1954m3  3.76
26   2.047333  1954m4  3.71
27   1.713875  1954m5  3.65
28   1.606167  1954m6  3.61
29   1.561667  1954m7  3.35
30   1.613292  1954m8  3.36
31   1.621083  1954m9  3.35
32   1.587667 1954m10  3.35
33   1.637792 1954m11  3.38
34   1.865917 1954m12  3.51
35   2.356417  1955m1  3.64
36   3.810000  1955m2  3.85
37   3.797000  1955m3  3.83
38   3.906000  1955m4  4.15
39   3.937000  1955m5  4.21
40   3.969000  1955m6  4.33
41   3.971000  1955m7  4.47
42   4.005000  1955m8  4.84
43   4.072000  1955m9  4.68
44   4.071000 1955m10  4.50
45   4.104000 1955m11  4.64
46   4.072000 1955m12  4.70
47   4.071000  1956m1  4.84
48   5.218000  1956m2  4.87
49   5.165000  1956m3  5.02
50   5.008000  1956m4  4.85
51   4.955000  1956m5  5.12
52   5.136000  1956m6  5.25
53   4.977000  1956m7  5.27
54   4.027000  1956m8  5.20
55   5.091000  1956m9  5.35
56   4.991000 1956m10  5.33
57   5.020000 1956m11  5.50
58   4.858000 1956m12  5.29
59   4.553000  1957m1  4.91
60   4.148000  1957m2  4.93
61   4.099000  1957m3  5.08
62   3.914000  1957m4  5.11
63   3.921000  1957m5  5.43
64   3.854000  1957m6  5.55
65   3.845000  1957m7  5.60
66   4.121000  1957m8  5.75
67   6.605000  1957m9  5.98
68   6.603000 1957m10  5.84
69   6.459000 1957m11  5.89
70   6.375000 1957m12  5.81
71   6.127000  1958m1  5.66
72   6.014000  1958m2  5.65
73   5.523000  1958m3  5.64
74   5.179000  1958m4  5.45
75   4.816000  1958m5  5.46
76   4.294000  1958m6  5.45
77   4.159000  1958m7  5.46
78   3.760000  1958m8  5.49
79   3.625000  1958m9  5.36
80   3.584000 1958m10  5.35
81   3.305000 1958m11  5.36
82   3.152000 1958m12  5.36
83   3.107000  1959m1  5.20
84   3.276000  1959m2  5.20
85   3.287000  1959m3  5.24
86   3.283000  1959m4  5.22
87   3.382000  1959m5  5.28
88   3.452000  1959m6  5.18
89   3.484000  1959m7  5.13
90   3.488000  1959m8  5.21
91   3.472000  1959m9  5.33
92   3.386000 1959m10  5.06
93   3.400000 1959m11  5.04
94   3.687000 1959m12  5.21
95   4.538000  1960m1  5.34
96   4.554000  1960m2  5.43
97   4.621000  1960m3  5.53
98   4.652000  1960m4  5.59
99   4.556000  1960m5  5.62
100  5.681000  1960m6  5.92
101  5.546000  1960m7  5.97
102  5.588000  1960m8  5.95
103  5.565000  1960m9  5.97
104  5.090000 1960m10  5.97
105  4.639000 1960m11  5.97
106  4.349000 1960m12  6.01
107  4.165000  1961m1  6.01
108  4.399000  1961m2  6.04
109  4.485000  1961m3  6.05
110  4.407000  1961m4  6.01
111  4.436000  1961m5  6.08
112  4.537000  1961m6  6.33
113  6.688000  1961m7  6.52
114  6.700000  1961m8  6.63
115  6.552000  1961m9  6.65
116  5.727000 1961m10  6.33
117  5.389000 1961m11  6.34
118  5.403000 1961m12  6.41
119  5.242000  1962m1  6.35
120  5.531000  1962m2  6.26
121  4.405000  1962m3  6.25
122  4.052000  1962m4  6.24
123  3.816000  1962m5  6.25
124  3.921000  1962m6  6.24
125  3.887000  1962m7  5.98
126  3.752000  1962m8  5.77
127  3.635000  1962m9  5.27
128  3.858000 1962m10  5.37
129  3.689000 1962m11  5.42
130  3.717000 1962m12  5.36
131  3.491000  1963m1  5.54
132  3.426000  1963m2  5.74
133  3.756000  1963m3  5.69
134  3.709000  1963m4  5.50
135  3.635000  1963m5  5.31
136  3.702000  1963m6  5.28
137  3.761000  1963m7  5.20
138  3.723000  1963m8  5.22
139  3.674000  1963m9  5.21
140  3.745000 1963m10  5.26
141  3.739000 1963m11  5.51
142  3.721000 1963m12  5.63
143  3.758000  1964m1  5.64
144  4.307000  1964m2  5.85
145  4.302000  1964m3  5.76
146  4.302000  1964m4  5.93
147  4.384000  1964m5  5.90
148  4.464000  1964m6  5.97
149  4.654000  1964m7  6.02
150  4.656000  1964m8  6.00
151  4.703000  1964m9  6.00
152  4.698000 1964m10  6.06
153  6.630000 1964m11  6.23
154  6.627000 1964m12  6.41
155  6.543000  1965m1  6.41
156  6.442000  1965m2  6.43
157  6.549000  1965m3  6.53
158  6.375000  1965m4  6.61
159  6.364000  1965m5  6.76
160  5.542000  1965m6  6.78
161  5.630000  1965m7  6.80
162  5.559000  1965m8  6.65
163  5.559000  1965m9  6.35
164  5.440000 1965m10  6.37
165  5.395000 1965m11  6.40
166  5.521000 1965m12  6.59
167  5.483000  1966m1  6.52
168  5.620000  1966m2  6.61
169  5.604000  1966m3  6.77
170  5.638000  1966m4  6.78
171  5.659000  1966m5  6.82
172  5.728000  1966m6  7.03
173  6.679000  1966m7  7.29
174  6.726000  1966m8  7.41
175  6.747000  1966m9  7.29
176  6.513000 1966m10  6.96
177  6.738000 1966m11  6.97
178  6.527000 1966m12  6.78
179  6.080000  1967m1  6.58
180  6.035000  1967m2  6.49
181  5.495000  1967m3  6.50
182  5.412000  1967m4  6.46
183  5.248000  1967m5  6.65
184  5.275000  1967m6  6.86
185  5.345000  1967m7  6.92
186  5.291000  1967m8  6.90
187  5.475000  1967m9  6.98
188  5.726000 1967m10  7.00
189  7.553000 1967m11  7.22
190  7.484000 1967m12  7.20
191  7.520000  1968m1  7.28
192  7.374000  1968m2  7.28
193  7.108000  1968m3  7.29
194  7.080000  1968m4  7.34
195  7.241000  1968m5  7.50
196  7.242000  1968m6  7.87
197  7.059000  1968m7  7.63
198  6.945000  1968m8  7.63
199  6.577000  1968m9  7.64
200  6.493000 1968m10  7.70
201  6.789000 1968m11  7.93
202  6.777000 1968m12  8.17
203  6.728000  1969m1  8.47
204  7.711000  1969m2  8.61
205  7.782000  1969m3  8.81
206  7.798000  1969m4  8.90
207  7.850000  1969m5  9.46
208  7.880000  1969m6  9.31
209  7.830000  1969m7  9.19
210  7.790000  1969m8  9.49
211  7.811000  1969m9  9.21
212  7.743000 1969m10  8.95
213  7.738000 1969m11  9.29
214  7.650000 1969m12  9.04
215  7.550000  1970m1  9.03
216  7.600000  1970m2  8.79
217  7.270000  1970m3  8.75
218  6.940000  1970m4  8.94
219  6.190000  1970m5  9.40
220  6.870000  1970m6  9.58
221  6.850000  1970m7  9.33
222  6.820000  1970m8  9.19
223  6.820000  1970m9  9.28
224  6.810000 1970m10  9.15
225  6.810000 1970m11  9.51
226  6.820000 1970m12  9.62
227  6.790000  1971m1  9.51
228  6.750000  1971m2  9.35
229  6.660000  1971m3  9.07
230  5.920000  1971m4  9.07
231  5.650000  1971m5  9.03
232  5.590000  1971m6  9.08
233  5.570000  1971m7  9.22
234  5.750000  1971m8  8.96
235  4.830000  1971m9  8.50
236  4.630000 1971m10  8.51
237  4.480000 1971m11  7.79
238  4.360000 1971m12  8.10
239  4.360000  1972m1  7.93
240  4.370000  1972m2  7.90
241  4.340000  1972m3  8.16
242  4.300000  1972m4  8.26
243  4.270000  1972m5  8.60
244  5.210000  1972m6  9.32
245  5.600000  1972m7  9.23
246  5.790000  1972m8  9.36
247  6.440000  1972m9  9.54
248  6.740000 1972m10  9.46
249  6.880000 1972m11  9.45
250  7.760000 1972m12  9.62
251  8.210000  1973m1  9.56
252  8.080000  1973m2  9.65
253  8.070000  1973m3 10.01
254  7.670000  1973m4  9.93
255  7.330000  1973m5 10.02
256  7.060000  1973m6 10.15
257  8.270000  1973m7 10.60
258 10.910000  1973m8 11.30
259 10.970000  1973m9 11.55
260 10.770000 1973m10 11.28
261 11.730000 1973m11 12.00
262 12.460000 1973m12 12.50
263 12.090000  1974m1 12.89
264 11.920000  1974m2 13.50
265 11.950000  1974m3 13.68
266 11.520000  1974m4 14.21
267 11.360000  1974m5 13.80
268 11.230000  1974m6 14.38
269 11.200000  1974m7 14.88
270 11.240000  1974m8 15.29
271 11.060000  1974m9 14.95
272 10.930000 1974m10 15.68
273 10.980000 1974m11 16.75
274 10.990000 1974m12 17.18
275 10.590000  1975m1 16.02
276  9.880000  1975m2 14.58
277  9.500000  1975m3 13.43
278  9.260000  1975m4 13.89
279  9.470000  1975m5 14.53
280  9.430000  1975m6 14.41
281  9.710000  1975m7 13.93
282 10.430000  1975m8 13.87
283 10.360000  1975m9 13.79
284 11.420000 1975m10 14.66
285 11.100000 1975m11 14.81
286 10.820000 1975m12 14.79
287  9.990000  1976m1 13.79
288  8.760000  1976m2 13.46
289  8.460000  1976m3 13.88
290  9.060000  1976m4 13.77
291 10.440000  1976m5 13.59
292 10.960000  1976m6 14.09
293 10.870000  1976m7 14.16
294 10.880000  1976m8 14.33
295 12.050000  1976m9 14.79
296 14.000000 1976m10 16.03
297 14.140000 1976m11 15.79
298 13.780000 1976m12 15.48
299 12.730000  1977m1 14.48
300 11.020000  1977m2 13.93
301  9.920000  1977m3 13.25
302  8.240000  1977m4 13.05
303  7.400000  1977m5 12.69
304  7.450000  1977m6 13.26
305  7.430000  1977m7 13.62
306  6.540000  1977m8 13.12
307  5.680000  1977m9 11.88
308  4.530000 1977m10 10.98
309  4.960000 1977m11 11.28
310  6.370000 1977m12 11.16
311  5.810000  1978m1 11.06
312  5.960000  1978m2 11.75
313  5.930000  1978m3 11.72
314  6.730000  1978m4 12.39
315  8.400000  1978m5 12.72
316  9.170000  1978m6 12.79
317  9.220000  1978m7 12.72
318  8.900000  1978m8 12.55
319  8.980000  1978m9 12.64
320  9.860000 1978m10 12.91
321 11.510000 1978m11 13.16
322 11.570000 1978m12 13.22
323 11.860000  1979m1 13.68
324 12.630000  1979m2 13.94
325 11.350000  1979m3 12.35
326 11.320000  1979m4 11.68
327 11.350000  1979m5 11.94
328 12.570000  1979m6 12.69
329 13.320000  1979m7 12.25
330 13.320000  1979m8 12.30
331 13.380000  1979m9 12.60
332 13.380000 1979m10 13.16
333 15.330000 1979m11 14.54
334 15.900000 1979m12 14.72
335 15.790000  1980m1 14.17
336 16.140000  1980m2 14.45
337 16.180000  1980m3 14.70
338 16.170000  1980m4 14.27
339 16.090000  1980m5 14.01
340 15.800000  1980m6 13.78
341 14.550000  1980m7 13.07
342 14.860000  1980m8 13.58
343 14.400000  1980m9 13.38
344 14.290000 1980m10 13.12
345 13.950000 1980m11 13.22
346 13.070000 1980m12 13.67
347 12.820000  1981m1 13.96
348 12.090000  1981m2 13.89
349 11.530000  1981m3 13.68
350 11.330000  1981m4 13.64
351 11.350000  1981m5 14.31
352 12.090000  1981m6 14.57
353 13.150000  1981m7 15.14
354 13.420000  1981m8 15.09
355 13.960000  1981m9 15.59
356 15.550000 1981m10 15.95
357 14.080000 1981m11 15.44
358 14.510000 1981m12 15.65
359 14.160000  1982m1 15.58
360 13.300000  1982m2 14.74
361 12.480000  1982m3 13.72
362 12.890000  1982m4 13.96
363 12.530000  1982m5 13.69
364 12.230000  1982m6 13.56
365 11.280000  1982m7 13.20
366 10.080000  1982m8 12.23
367  9.910000  1982m9 11.40
368  8.910000 1982m10 10.50
369  9.220000 1982m11 10.64
370  9.960000 1982m12 11.34
371 10.590000  1983m1 11.60
372 10.740000  1983m2 11.50
373 10.470000  1983m3 10.97
374  9.840000  1983m4 10.56
375  9.700000  1983m5 10.65
376  9.470000  1983m6 10.39
377  9.370000  1983m7 10.95
378  9.340000  1983m8 11.07
379  9.160000  1983m9 10.67
380  8.840000 1983m10 10.61
381  8.840000 1983m11 10.29
382  8.870000 1983m12 10.35
383  8.870000  1984m1 10.28
384  8.850000  1984m2 10.42
385  8.430000  1984m3 10.23
386  8.380000  1984m4 10.40
387  8.820000  1984m5 10.93
388  8.860000  1984m6 11.15
389 10.970000  1984m7 11.67
390 10.210000  1984m8 10.98
391 10.020000  1984m9 10.78
392  9.850000 1984m10 10.69
393  9.230000 1984m11 10.32
394  9.100000 1984m12 10.46
395 10.550000  1985m1 10.96
396 12.690000  1985m2 11.06
397 12.930000  1985m3 10.90
398 11.930000  1985m4 10.68
399 11.940000  1985m5 10.88
400 11.890000  1985m6 10.70
401 11.390000  1985m7 10.44
402 10.960000  1985m8 10.37
403 11.060000  1985m9 10.39
404 11.050000 1985m10 10.22
405 11.110000 1985m11 10.37
406 11.150000 1985m12 10.45
407 11.980000  1986m1 10.80
408 12.020000  1986m2 10.40
409 11.060000  1986m3  9.39
410  9.990000  1986m4  8.76
411  9.700000  1986m5  9.00
412  9.320000  1986m6  9.23
413  9.450000  1986m7  9.37
414  9.390000  1986m8  9.41
415  9.610000  1986m9  9.97
416 10.250000 1986m10 10.62
417 10.630000 1986m11 10.80
418 10.660000 1986m12 10.69
419 10.520000  1987m1 10.09
420 10.290000  1987m2  9.83
421  9.350000  1987m3  9.16
422  9.430000  1987m4  9.12
423  8.460000  1987m5  8.82
424  8.540000  1987m6  8.90
425  8.840000  1987m7  9.23
426  9.790000  1987m8  9.20
427  9.690000  1987m9  9.98
428  9.450000 1987m10  9.88
429  8.430000 1987m11  9.20
430  8.190000 1987m12  9.57
431  8.370000  1988m1  9.57
432  8.790000  1988m2  9.38
433  8.270000  1988m3  9.12
434  7.740000  1988m4  9.12
435  7.540000  1988m5  9.27
436  8.880000  1988m6  9.32
437 10.050000  1988m7  9.51
438 11.130000  1988m8  9.47
439 11.530000  1988m9  9.60
440 11.540000 1988m10  9.23
441 12.070000 1988m11  9.30
442 12.540000 1988m12  9.46
443 12.450000  1989m1  9.35
444 12.390000  1989m2  9.15
445 12.410000  1989m3  9.26
446 12.470000  1989m4  9.52
447 12.540000  1989m5  9.52
448 13.590000  1989m6  9.88
449 13.290000  1989m7  9.53
450 13.320000  1989m8  9.37
451 13.440000  1989m9  9.62
452 14.460000 1989m10  9.81
453 14.450000 1989m11  9.99
454 14.500000 1989m12  9.96
455 14.500000  1990m1 10.28
456 14.450000  1990m2 10.72
457 14.570000  1990m3 11.46
458 14.590000  1990m4 11.77
459 14.500000  1990m5 11.49
460 14.380000  1990m6 11.01
461 14.320000  1990m7 11.03
462 14.310000  1990m8 11.41
463 14.260000  1990m9 11.32
464 13.370000 1990m10 11.12
465 12.920000 1990m11 10.94
466 12.960000 1990m12 10.40
467 13.000000  1991m1 10.22
468 12.390000  1991m2  9.89
469 11.640000  1991m3 10.06
470 11.250000  1991m4  9.99
471 10.840000  1991m5 10.15
472 10.720000  1991m6 10.34
473 10.520000  1991m7 10.10
474 10.200000  1991m8  9.89
475  9.660000  1991m9  9.54
476  9.860000 1991m10  9.62
477  9.980000 1991m11  9.68
478 10.100000 1991m12  9.56
479  9.970000  1992m1  9.34
480  9.800000  1992m2  9.21
481 10.100000  1992m3  9.54
482  9.970000  1992m4  9.33
483  9.430000  1992m5  8.99
484  9.420000  1992m6  9.02
485  9.430000  1992m7  8.90
486  9.650000  1992m8  9.13
487  9.160000  1992m9  9.12
488  7.470000 1992m10  9.24
489  6.490000 1992m11  8.84
490  6.390000 1992m12  8.84
491  6.050000  1993m1  8.92
492  5.370000  1993m2  8.63
493  5.380000  1993m3  8.33
494  5.330000  1993m4  8.39
495  5.300000  1993m5  8.60
496  5.190000  1993m6  8.39
497  5.130000  1993m7  7.96
498  5.060000  1993m8  7.39
499  5.170000  1993m9  7.18
500  5.150000 1993m10  7.09
501  4.950000 1993m11  7.06
502  4.870000 1993m12  6.46
503  4.890000  1994m1  6.41
504  4.760000  1994m2  6.83
505  4.830000  1994m3  7.47
506  4.880000  1994m4  7.83
507  4.810000  1994m5  8.24
508  4.880000  1994m6  8.55
509  5.090000  1994m7  8.41
510  5.340000  1994m8  8.52
511  5.390000  1994m9  8.72
512  5.440000 1994m10  8.63
513  5.630000 1994m11  8.53
514  5.870000 1994m12  8.44
515  5.930000  1995m1  8.61
516  6.160000  1995m2  8.52
517  6.090000  1995m3  8.50
518  6.300000  1995m4  8.39
519  6.200000  1995m5  8.18
520  6.370000  1995m6  8.16
521  6.620000  1995m7  8.36
522  6.590000  1995m8  8.24
523  6.520000  1995m9  8.09
524  6.530000 1995m10  8.34
525  6.380000 1995m11  8.01
526  6.220000 1995m12  7.94
</code></pre>

<p>which is saved as <code>ukrates.csv</code>.</p>

<p>Here is the code to attempt to reproduce the <code>bgtest</code> module.</p>

<pre><code>rm(list = ls())

library(zoo)
library(lmtest)
library(dynlm)

# read in the data
dfUK = read.csv('./data/ukrates.csv', header = TRUE)
summary(dfUK)

# run the time series regression
zooUK = zoo(dfUK[, c('rs', 'r20')], order.by = as.yearmon(dfUK$month, 
                                                              '%Ym%m'))
    zooUKAug = merge(zooUK, 
                     'drs' = diff(zooUK$rs, 1), 
                 'ldr20' = lag(diff(zooUK$r20, 1), -1))
lmUK2 = dynlm(drs ~ ldr20, data = zooUKAug)

# Breusch-Godfrey regression
zooUKBG = merge(zooUKAug, 'resid' = resid(lmUK2))
lmBG = dynlm(as.formula(paste('resid',  
                              '~', 
                              attr(lmUK2$terms, 'term.labels'),
                              ' + L(resid, 1)')),
             data = zooUKBG) 

# BG test using lmtest package
bgtest(lmUK2, order = 1, type = 'Chisq') # 14.5614

# attempt to recreate BG-test 
length(lmBG$residuals)*
      sum(lmBG$fitted^2)/sum(lmBG$residuals^2)
</code></pre>

<p>This is based on the following code for computing the chi-squared statistic directly from the <code>bgtest</code> function code:</p>

<pre><code>&gt; bgtest
function (formula, order = 1, order.by = NULL, type = c(""Chisq"", 
    ""F""), data = list(), fill = 0) 
{
    dname &lt;- paste(deparse(substitute(formula)))
    if (!inherits(formula, ""formula"")) {
        X &lt;- if (is.matrix(formula$x)) 
                formula$x
        else model.matrix(terms(formula), model.frame(formula))
        y &lt;- if (is.vector(formula$y)) 
                formula$y
        else model.response(model.frame(formula))
    }
    else {
        mf &lt;- model.frame(formula, data = data)
        y &lt;- model.response(mf)
        X &lt;- model.matrix(formula, data = data)
    }
    if (!is.null(order.by)) {
        if (inherits(order.by, ""formula"")) {
            z &lt;- model.matrix(order.by, data = data)
            z &lt;- as.vector(z[, ncol(z)])
        }
        else {
            z &lt;- order.by
        }
        X &lt;- as.matrix(X[order(z), ])
        y &lt;- y[order(z)]
    }
    n &lt;- nrow(X)
    k &lt;- ncol(X)
    order &lt;- 1:order
    m &lt;- length(order)
    resi &lt;- lm.fit(X, y)$residuals
        Z &lt;- sapply(order, function(x) c(rep(fill, length.out = x), 
            resi[1:(n - x)]))
        if (any(na &lt;- !complete.cases(Z))) {
            X &lt;- X[!na, , drop = FALSE]
            Z &lt;- Z[!na, , drop = FALSE]
            y &lt;- y[!na]
            resi &lt;- resi[!na]
            n &lt;- nrow(X)
        }
        auxfit &lt;- lm.fit(cbind(X, Z), resi)
        cf &lt;- auxfit$coefficients
    vc &lt;- chol2inv(auxfit$qr$qr) * sum(auxfit$residuals^2)/auxfit$df.residual
    names(cf) &lt;- colnames(vc) &lt;- rownames(vc) &lt;- c(colnames(X), 
        paste(""lag(resid)"", order, sep = ""_""))
    switch(match.arg(type), Chisq = {
        bg &lt;- n * sum(auxfit$fitted^2)/sum(resi^2)
            p.val &lt;- pchisq(bg, m, lower.tail = FALSE)
            df &lt;- m
            names(df) &lt;- ""df""
        }, F = {
            uresi &lt;- auxfit$residuals
        bg &lt;- ((sum(resi^2) - sum(uresi^2))/m)/(sum(uresi^2)/(n - 
            k - m))
        df &lt;- c(m, n - k - m)
        names(df) &lt;- c(""df1"", ""df2"")
        p.val &lt;- pf(bg, df1 = df[1], df2 = df[2], lower.tail = FALSE)
    })
    names(bg) &lt;- ""LM test""
    RVAL &lt;- list(statistic = bg, parameter = df, method = paste(""Breusch-Godfrey test for serial correlation of order up to"", 
        max(order)), p.value = p.val, data.name = dname, coefficients = cf, 
        vcov = vc)
    class(RVAL) &lt;- c(""bgtest"", ""htest"")
    return(RVAL)
}
&lt;environment: namespace:lmtest&gt;
</code></pre>

<p>I am wondering why I am getting the different results.</p>
"
"0.0808716413062113","0.0822133822214443"," 71948","<p>I'm fitting a natural spline fit to some data points. I'd like to estimate the prediction error for the predicted value. In linear regression (I agree that natural spline is also a linear regression with a specific type of design matrix), we know:</p>

<p>$\hat{\beta} = (X^T X)^{-1}X^TY \rightarrow \text{ assuming var(Y) = } \sigma^2 I \text{ then : }var(\hat{\beta}) = (X^TX)^{-1} \sigma^2 $ </p>

<p>Now consider $\hat{Y} = {X_i}^T \hat{\beta} + \epsilon_i$. We can then write:</p>

<p>$Var(\hat{Y}) = {X_i}^T ((X^TX)^{-1} \sigma^2) (X_i) + \sigma^2$</p>

<p>This is easy to calculate for linear regression. How should I do it with natural spline? I can get the design matrix for natural spline. I can get  $(X^TX)^{-1} \sigma^2$ but how can I get the rest of it:</p>

<p>Here is an example in R:</p>

<pre><code>set.seed(12345)
x &lt;- c(1:100)
y &lt;- sin(pi*x/50)
epsilon &lt;- rnorm(100, 0, 3)
knots &lt;- c(10, 20, 30, 40, 50, 60, 70, 80, 90)
myFit &lt;- lm(y ~ ns(x, knots = knots))
</code></pre>

<p>Now consider x = 32.5 . How can I get the variance for the $\hat{Y}$ corresponding to x = 32.5 ? I know we can use the predict function. however, what I do really want is to get calculate it similar to linear regression by getting the design matrix and multiplying them together.</p>

<p>I really appreciate your help.</p>
"
"0.026822089039291","0.0272670941574606"," 72060","<p>In the following code I perform a logistic regression on grouped data using glm and ""by hand"" using mle2. Why does the logLik function in R give me a log likelihood logLik(fit.glm)=-2.336 that is different than the one logLik(fit.ml)=-5.514 I get by hand? </p>

<pre><code>library(bbmle)

#successes in first column, failures in second
Y &lt;- matrix(c(1,2,4,3,2,0),3,2)

#predictor
X &lt;- c(0,1,2)

#use glm
fit.glm &lt;- glm(Y ~ X,family=binomial (link=logit))
summary(fit.glm)

#use mle2
invlogit &lt;- function(x) { exp(x) / (1+exp(x))}
nloglike &lt;- function(a,b) {
  L &lt;- 0
  for (i in 1:n){
     L &lt;- L + sum(y[i,1]*log(invlogit(a+b*x[i])) + 
               y[i,2]*log(1-invlogit(a+b*x[i])))
  }
 return(-L) 
}  

fit.ml &lt;- mle2(nloglike,
           start=list(
             a=-1.5,
             b=2),
           data=list(
             x=X,
             y=Y,
             n=length(X)),
           method=""Nelder-Mead"",
           skip.hessian=FALSE)
summary(fit.ml)

#log likelihoods
logLik(fit.glm)
logLik(fit.ml)


y &lt;- Y
x &lt;- X
n &lt;- length(x)
nloglike(coef(fit.glm)[1],coef(fit.glm)[2])
nloglike(coef(fit.ml)[1],coef(fit.ml)[2])
</code></pre>
"
"0.0709645772411954","0.0618359572423054"," 72339","<p>After some frantic googling I do believe the answer is yes, but more so I am frustrated that the relation between the two parameters seems to be nowhere described explicitely so I do it here. (I hope this isn't against the rules of stackexchange.)</p>

<p><a href=""http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1141&amp;context=usdeptcommercepub&amp;sei-redir=1&amp;referer=http%3A%2F%2Fscholar.google.nl%2Fscholar_url%3Fhl%3Dnl%26q%3Dhttp%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%253Farticle%253D1141%2526context%253Dusdeptcommercepub%26sa%3DX%26scisig%3DAAGBfm3vz9gDbxRveIafikl02v0aeUyu0w%26oi%3Dscholarr%26ei%3DDj9VUqWlL6LG0QXe1oHwAg%26ved%3D0CDAQgAMoADAA#search=%22http%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%3Farticle%3D1141%26context%3Dusdeptcommercepub%22"" rel=""nofollow"">This very nice article</a> states: we will denote the random variable Y having a negative binomial distribution as Y ~ NB($\mu, \kappa$) with a parameterization such that E(Y) = $\mu$, var(Y) = $\mu + \kappa \mu^2$.</p>

<p>I take this latter equation as the definition of $\kappa$.</p>

<p><a href=""http://books.google.nl/books?id=Ohks0xwvyT4C&amp;pg=PA196&amp;lpg=PA196&amp;dq=kappa+parameter+negative+binomial+proc+glimmix&amp;source=bl&amp;ots=PYKpaGQ8VN&amp;sig=5sNEB-7H7ZocErTKhi35ORKd2lA&amp;hl=nl&amp;sa=X&amp;ei=lEBVUqCnNcTJ0QXppYGoAg&amp;ved=0CDYQ6AEwAA#v=onepage&amp;q=kappa%20parameter%20negative%20binomial%20proc%20glimmix&amp;f=false"" rel=""nofollow"">Apparently</a> this kappa is implemented in SAS.</p>

<p>Now turning to R, the function <code>glm.nb</code> in the <code>MASS</code> package contains a parameter $\mu$ which is obviously the same $\mu$ as above and a parameter $\theta$. The question is how $\theta$ and $\kappa$ are related. The documentation for <code>glm.nb</code> only refers to it as an ""additional parameter"". The answers to <a href=""http://stats.stackexchange.com/questions/10419/what-is-theta-in-a-negative-binomial-regression-fitted-with-r"">this</a> and <a href=""http://stats.stackexchange.com/questions/10457/interpreting-negative-binomial-regression-output-in-r?rq=1"">this</a> stackexchange questions directly imply that $\theta = 1/\kappa$, but <a href=""http://stats.stackexchange.com/questions/30360/what-is-the-distribution-of-theta-in-a-negative-binomial-model-glm-nb-with-r?rq=1"">this</a> question [EDIT: since removed] seems to suggest that $\theta = \kappa$. </p>

<p>The <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/NegBinomial.html"" rel=""nofollow"">help page for negative binomial in R</a> is nice and introduces a parameter called <code>size</code> that equals $1/\kappa$. Fitting <code>glm.nb</code> on random data generated by <code>rnbinom</code> for various choices of $\mu$ and <code>size</code> seems to support the thesis that $\theta = 1/\kappa$ (i.e. that $\theta$ = <code>size</code>) but also that for large values of size the estimation is poor.</p>

<p>Summarizing: I do believe that $\theta = 1/\kappa$ but it would be nice if there were an easily googlable place on the internet stating this explicitly. Maybe one of the answers to this questions can serve as such a place? </p>
"
"NaN","NaN"," 72892","<p>Why is the F-test for overall significance (OLS regression analysis) invalid when residuals are heteroscedastic? 
Is there a way to calculate it in a consistent way under heteroscedasticity?
Is there any function in R to accomplish that?</p>
"
"NaN","NaN"," 72968","<p>I am working on vector auto-regression (VARs) and impulse response function (IRFs) estimation based on panel data with 33 individuals over 77 quarters.  How should this type of situation be analyzed?  What algorithm's exist for this purpose?  I would prefer to conduct these analyses in R, so if anyone is familiar with R code or a package designed for this purpose that they could suggest, that would be especially helpful.  </p>
"
"0.053644178078582","0.0409006412361909"," 73004","<p>I am fitting a gam model in R (using the <code>gam</code> function in <code>mgcv</code>) to account for some non-linear effects in my data. A stripped down example of what I am doing in R is:</p>

<pre><code>mod=gam(y~s(x)+s(z),data=df)
</code></pre>

<p>However, I want to add a slightly more complicated variance model to my regression of the form </p>

<p>$$\epsilon \sim N(0,\sigma^2),\ \sigma = f(\hat{\mu})$$</p>

<p>where $\hat{\mu}$ is the fitted value of the model. (Actually, it would be nice if $\epsilon \sim t_\nu$ for some $\nu$ but sticking to this for now. I have managed to do this in the <code>gls</code> function from <code>nlme</code> using the <code>varFunc(form=fitted(.))</code> type approach, but can't figure out if there is an option to do the same kind of thing using <code>gam</code>.</p>

<p>I recognise this is not really the intention of a GLM/GAM model, but I don't want to reinvent the wheel if I am just missing something obvious</p>

<p>Edit: In response to the question in the comment below, I am hoping to fit a linear or quadratic function for $f$. I do not know the exact form of $f$ but plan to iteratively estimate it from the residuals if this can't be done automatically.</p>

<p>Edit2: Typo in R code - first spline is not meant to be a function of y!</p>
"
"0.0969560705490221","0.105605001571314"," 73165","<p>I have a logistic regression model (fit via glmnet in R with elastic net regularization), and I would like to maximize the difference between true positives and false positives.  In order to do this, the following procedure came to mind:</p>

<ol>
<li>Fit standard logistic regression model</li>
<li>Using prediction threshold as 0.5, identify all positive predictions</li>
<li>Assign weight 1 for positively predicted observations, 0 for all others</li>
<li>Fit weighted logistic regression model</li>
</ol>

<p>What would be the flaws with this approach?  What would be the correct way to proceed with this problem?</p>

<p>The reason for wanting to maximize the difference between the number of true positives and false negatives is due to the design of my application.  As part of a class project, I am building a autonomous participant in an online marketplace - if my model predicts it can buy something and sell it later at a higher price, it places a bid.  I would like to stick to logistic regression and output binary outcomes (win, lose) based on fixed costs and unit price increments (I gain or lose the same amount on every transaction).  A false positive hurts me because it means that I buy something and am unable to sell it for a higher price.  However, a false negative doesn't hurt me (only in terms of opportunity cost) because it just means if I didn't buy, but if I had, I would have made money.  Similarly, a true positive benefits me because I buy and then sell for a higher price, but a true negative doesn't benefit me because I didn't take any action.</p>

<p>I agree that the 0.5 cut-off is completely arbitrary, and when I optimized the model from step 1 on the prediction threshold which yields the highest difference between true/false positives, it turns out to be closer to 0.4.  I think this is due to the skewed nature of my data - the ratio between negatives and positives is about 1:3.</p>

<p>Right now, I am following the following steps:</p>

<ol>
<li>Split data intto training/test</li>
<li>Fit model on training, make predictions in test set and compute difference between true/false positives</li>
<li>Fit model on full, make predictions in test set and compute difference between true/false positives</li>
</ol>

<p>The difference between true/false positives is smaller in step #3 than in step #2, despite the training set being a subset of the full set.  Since I don't care whether the model in #3 has more true negatives and less false negatives, is there anything I can do without altering the likelihood function itself?</p>
"
"0.053644178078582","0.0545341883149212"," 73950","<p>My question is similar to this link <a href=""http://stats.stackexchange.com/questions/12425/creating-a-certainty-score-from-the-votes-in-random-forests"">Creating a &quot;certainty score&quot; from the votes in random forests?</a></p>

<p>I am trying to build a random forest for a binary response (1 &amp; 0). Let's say we have 10,000 different records and I am building 500 trees. Is there a way to score the records in terms of the certainty / confidence / likelihood of being categorized in category 1 (for example)? The link above suggests using the number of votes among all 500 trees, but this way can only give me up to 500 different scores, how can I differentiate further for these 10,000 records? (Like regression, the scores can be easily obtained). </p>

<p>One solution is to average the score of each tree in the forest. the tree is the probability of 1s in the final node. Anyone know how to produce that average in R? I couldnt find this in the randomForest package. I think if I write my own codes to do that it , the run time may not be as fast as a built-in function. </p>
"
"0.0599760143904067","0.0609710760849692"," 74678","<p>I am comparing multiple published equation forms, refit with independent data.  I'm trying to be true to the original authors' methods as much as possible. Therefore, I have 3 linear equations (fit in R using lm()), two of which use transformed Y-variables, and one equation fit using nonlinear regression (fit in R using the gnls() function).</p>

<p>In all instances cases I'm weighting the residual variance structure using the inverse of one of the predictors to account for observed heteroskedasticity.</p>

<p>I have been evaluating the models using R2, and RMSE- using back-transformed data for the two models with transformations.</p>

<p>I've calculated RMSE ""by hand"" using the following equation:</p>

<pre><code> RMSE&lt;-sqrt(sum(residuals(Equation)^2)/length(residuals(Equation))-2))
</code></pre>

<p>Should I use similar code to calculate RMSE for the linear and nonlinear regression models?  Is the metric still a valid statistic for comparison, or am I missing some important assumption?  </p>

<p>Edited: I initially stated that I was also comparing models using AIC; I later recalled that AIC would not be appropriate if the Y-variables were transformed because the models would be estimating different things.</p>
"
"0.0379321620905441","0.0385614943639849"," 76850","<p>I am trying to do a multinomial logistic regression on some data that I generated. I am using R and the package mlogit. My data looks like the following:</p>

<pre><code>Class X1 X2  X3
V +0.0655197 +0.6418541 +1.8110291
V-0.6713268 -0.0262458 -0.3602958
V +0.2357610 -0.3602958 -0.6943458
M +0.3900129 +0.5583416 -1.7800082
M +0.5714871 -0.2767833 +0.5583416
M +1.0732807 -1.7800082 -0.3602958
S +0.9553640 -0.3602958 +0.6418541
S +0.1139899 +0.1356030 +0.3889280
S +0.4717283 -0.2852090 -1.1229880
</code></pre>

<p>My model is</p>

<pre><code>Class = B1*X1 + B2*X2 + B3*X3
</code></pre>

<p>So far, I have:</p>

<pre><code>library(mlogit);
allData &lt;- read.table(""Features/AllFeatures.dat"", header=TRUE);
allData$Class&lt;-as.factor(allData$Class);
mlData&lt;-mlogit.data(allData, choice=""Class"");
myData&lt;- mlogit(Class~1|X1 + X2 + x3, data = mlData);
print(summary(myData));
</code></pre>

<p>Which gives me:</p>

<pre><code>Coefficients :
                           Estimate Std. Error t-value  Pr(&gt;|t|)    
S:(intercept)                -0.6832392  0.0951834 -7.1781 7.068e-13 ***
V:(intercept)                -0.6696254  0.0943282 -7.0989 1.258e-12 ***
S:X1                         -0.1362492  0.1134039 -1.2015    0.2296    
V:X1                         -0.0052649  0.1128722 -0.0466    0.9628    
S:X2                         -0.0198451  0.0973608 -0.2038    0.8385    
V:X2                          0.0183261  0.0974789  0.1880    0.8509    
S:X3                          0.1728694  0.1110473  1.5567    0.1195    
V:X3                          0.0230260  0.1101147  0.2091    0.8344
</code></pre>

<p>However in the following: <a href=""http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf</a></p>

<p>The author gets:</p>

<pre><code>Coefficients :
Estimate Std. Error t-value Pr(&gt;|t|)
ic -0.00623187 0.00035277 -17.665 &lt; 2.2e-16 ***
oc -0.00458008 0.00032216 -14.217 &lt; 2.2e-16 ***
</code></pre>

<p>How can I change my function call to get the same? I want the actual coefficients for the model, not for the comparisons.</p>

<p>Also: What is the difference between 'wide' and 'long'? What form is my data in?</p>

<p>Also: Do you have some mathematical references for this type of multinomial logistic regression aimed at engineers?</p>

<p>Thanks!</p>
"
"NaN","NaN"," 76904","<p>Can you give me an example of the use of sandwich estimators in order to perform a robust regression?</p>

<p>I can see the example in <code>?sandwich</code> but I don't quite understand how we can go from </p>

<pre><code>lm(a ~ b, data) # R-coded
</code></pre>

<p>to an estimate and a p.value resulting of a robust regression using the variance-covariance matrix returned by the function <code>sandwich</code>.  </p>
"
"0.0709645772411954","0.0618359572423054"," 77580","<p>I have a number of objective functions like:</p>

<pre><code>y1 = a11* x11 + a12*x11*x11+ a13*x12+.......
y2 = a21* x21 + a22*x21*x21+ a23*x22+.......
:::::
:::::
:::::
</code></pre>

<p>These are multiple objective functions. However, the constraints of the objective functions have dependency on each other.
Something like, </p>

<pre><code>x11+ x21 + x22 &lt; const1
x12 + x21 &gt; const2

:::::
:::::
</code></pre>

<p>What is the way to optimize such a system of equations? I would ideally like to use R to do the same?</p>

<p>y1 = 0.32 x1 + 0.21 x1*x1 + 0.49 x2... y2... y3 . . . The equations that i have is a non-linear function. These are non-linear regression equations or non-linear Market Mix Models. The x's are TV spend, Digital Spend etc. I want to 'include' all these models, use some constraints on them and optimize the spends in all the models with respect to constraints like...x1 (TV spend) &lt; 100, TV + Digital spend &lt; 500. I want to be able to say that of an amount of 100, i should spend, 30 on model 1, 20 on model 2( equation 2) etc</p>
"
"0.0547503599848004","0.0667904674542028"," 78022","<p>I am performing quantile regressions in R using the package quantreg. My dataset includes 12,328 observations ranging from 0.12 to 330. The timepoints for my data are not exactly continuous; all data fall into one of a few dozen bins ranging from 73 to 397.</p>

<p>When I performed a linear regression on this data using the lm() function, I was able to do this with polynomials up to 4:</p>

<pre><code>lm(Y~poly(X,3,raw=TRUE),data=mydata)
</code></pre>

<p>However, with the package quantreg and the rq() command, I cannot use any polynomials. A simple regression works just fine:</p>

<pre><code>rq(Y~X,data=mydata,tau=.15)
</code></pre>

<p>But as soon as I get into polynomials, no dice. When I enter this:</p>

<pre><code>rq(Y~poly(X,2,raw=TRUE),data=mydata,tau=.15)
</code></pre>

<p>I get the following error message:</p>

<pre><code>Error in rq.fit.br(x, y, tau = tau, ...) : Singular design matrix
</code></pre>

<p>I am very new to all of this. I've read up on singular matrices as much as I can and I think there might be two reasons for this: (1) I only have one variable on each axis, or (2) my data are binned/the Y variable isn't truly continuous.</p>

<p>Can anyone tell me why I'm getting this error?</p>

<p>Thank you!</p>

<p>PS - This is how the graph looks:</p>

<p><img src=""http://i.stack.imgur.com/htUdA.png"" alt=""enter image description here""></p>
"
"0.0464572209811883","0.0472279924554862"," 78284","<p>I loaded the data to R and fitted it using GLM function.</p>

<blockquote>
  <p>fit.glm = glm(y~ aX+bZ+cW)</p>
</blockquote>

<p>Then, I found the cuttinf-point using ""segmented"" tool of R.</p>

<blockquote>
  <p>o&lt;-segmented(fit.glm,seg.Z=~X,psi=10)</p>
</blockquote>

<p>Now I have the cut-point and two different slope of X. </p>

<blockquote>
  <p>Call: segmented.glm(obj = fit.glm1, seg.Z = ~PTH, psi = 10)</p>
  
  <p>Meaningful coefficients of the linear terms: (Intercept)          X   Z       W</p>
  
  <p>19.43840           2.29574           0.08701           8.75784      </p>
  
  <p>Estimated Break-Point(s) psi1.PTH : 8.5 </p>
  
  <p>Degrees of Freedom: 324 Total (i.e. Null);  314 Residual Null
  Deviance:     17320  Residual Deviance: 7645      AIC: 1971</p>
</blockquote>

<p>However, I'm trying to get estimated y value of two linear regression models using the result of 'segmented'.</p>

<p>Is it possible using R, or I have to substitute Z and W with mean values of Z &amp; W, and calculate the y value myself?</p>
"
"0.0969560705490221","0.0985646681332268"," 78455","<p>Disclaimer: Statistics is not my strong side, so if my question is nonsense I apologize. I'm a beginner, but really wanting to understand this.</p>

<p>My question is: why do I get so widely different parameter estimates when using different transformations on my data in a non-linear regression ?</p>

<p>I'm trying to do a nonlinear regression and to estimate the uncertainty of the fit (confidence interval) using linear approximation. From my understanding the more linear-like the shape of the nonlinear function, the more accurate will the confidence interval calculation by linear approximation be.  I therefore want to transform the data to make it as linear as possible. The errors in $y$ can be assumed to be log-normal. My data is monotonic and assumed to follow a power function in most cases.</p>

<p>$$ y = a*(x-x_0)^b $$</p>

<p>where $y$ is river discharge, $x$ is an arbitrary water level in the river and $x_0$ is the water level where where discharge $y$ is 0. This can be rewritten as log transformed, and nice and linear
$$ log(y) = a + b \times log(x-x_0) $$.</p>

<p>I need to estimate the parameters $a$, $b$ and $x_0$, so to do so simultaneously I use nonlinear regression. I also have some data that follows quadratic functions, so I would like to set up (and understand) a non-linear method.</p>

<p>I use r and <code>nlsLM()</code> from <code>minpack.lm</code> to carry out the non-linear regression.
Here is some example code:</p>

<pre><code>library(minpack.lm)

xdata &lt;- c(19,  21,  24,    25, 29, 34, 35, 40, 40, 46, 48, 48, 52, 56, 57, 65, 65, 68)
ydata &lt;- c(10,  11, 14, 20, 24, 50, 42, 96, 89, 134,    135,    161,    171,    218,    261,    371,    347,    393)
df&lt;-data.frame(x=xdata, y=ydata)

#weights applied in the case of no transformation (relative error assumed to be the same for all y data)
W&lt;-1/ydata

# NLS regression with weights, no transformation
nlsmodel1&lt;-nlsLM(y ~ a*(x-x0)^b,data=df,start=list(a=0.1, b=2.5,x0=0))

# log transformed
nlsmodel2&lt;-nlsLM(log(y) ~ a+b*(log(x-x0)),data=df,start=list(a=0.1, b=2.5,x0=0))
&gt; coef(nlsmodel1)
          a           b          x0 
0.005158377 2.719693093 4.896772931 
&gt; coef(nlsmodel2)
        a         b        x0 
-8.683758  3.445699 -4.139127 

&gt; exp(-8.683758)
[1] 0.0001693136
</code></pre>

<p>I understand that the weights are very important, and can have a say in the differences here, but not by this much? My judgement of the two parameter sets is that <code>nlsmodel1</code> performs ""better"", and that the <code>b</code> coefficient is too high in the fit from <code>nlsmodel2</code>. <code>nlsmodel2</code> does a poor job in the upper end of the data, with large residuals there. But why are they so different? I feel like I'm doing something very silly here, and is unable to see the error. I have tried some other transformations, for example only transforming LHS as <code>log(y)</code>, but the problem remains.</p>

<p>I appreciate any tips that can help me improve, and not the least understand, the transformed fit.</p>

<p>Cheers</p>

<p>Related <a href=""http://stats.stackexchange.com/questions/58928/nonlinear-regression-confidence-intervals-on-transformed-or-untransformed-param"">post #1</a> and <a href=""http://stats.stackexchange.com/questions/69524/on-nonlinear-regression-fits-and-transformations"">post #2</a></p>
"
"0.0547503599848004","0.0667904674542028"," 78529","<p>I have data from survey, and 
Trying to build a linear regression model using R like
A~ B
however, want to control C, D, E, F, G. like Age, Sex, and other confounding variables.
I tried to make some models using lm, glm, etc.
and fitted it to my data.</p>

<p>However, in Multivariate regression models, I cannot get the graph like below.
To use 'segmented' function of R to cut-off point, I guess, I should able to get estimated equation between A and B, while all other variables are controlled.</p>

<p><img src=""http://i.stack.imgur.com/RuWC4.png"" alt=""enter image description here""></p>

<p>The image above describes what I want to do.
linear model between A and B, but actual model includes confounders C, D, F, and somehow'controlled' them.
The author described that </p>

<blockquote>
  <p>fitted the two linear regression models for high B and low B, and
  calculated the sums of squares of residuals (=observed A -estimated
  A), from the two models for each B. The models with the lowest
  residual sums of squares were the best models.</p>
</blockquote>

<p>It seems to be drawn from R(and author said so), but I cannot find any good approach.</p>

<p>I'm afraid this question seems to 'tool specific' question/
If so, I'll amend this question, and update it with any help I got.</p>

<p>Thanks.</p>
"
"0.107474459256542","0.115684483091955"," 78663","<p>Is any one here familiar with an R package called Zelig?</p>

<p>I have a data frame like this:</p>

<pre><code>IQ   AGE
80   50
100  18
90   25
</code></pre>

<p>etc.</p>

<p>What I need to do is build a model of IQ given AGE, I am running these commands:
<code>z.out &lt;- zelig(IQ~AGE,data=df,model=""ls"")</code>
this runs the what-if given age 110, what would be the IQ
<code>x.out &lt;- setx(z.out, AGE=110)</code>
This is a simulation model where given the age 110, after running 1 million runs of simulation, what would be the IQ with 95% confidence interval.
<code>s.out &lt;- sim(z.out,x.out, num=1000000, level=95)</code></p>

<p>I have a hard time understanding from what pool of data the <code>sim()</code> function draws the numbers. I read though the docs, but they are written for Ph.D. students, if not more advanced readers. I have asked the Zelig creators this question multiple times but they are directing me to the docs which I read multiple times, with no luck. However, one of the  person that works with Zelig sent me this email:</p>

<blockquote>
  <p>Suppose that you fit 
  $$\text{IQ} = a + \text{Age} * b + e$$
  Then you get a table of regression coefficients where a=50, b=2, and their standard errors are something like $\text{s.e.}(a)=\sqrt{10}$ and $\text{s.e.}(b)=1$. These are all hypothetical examples. 
  In maximum likelihood estimation, this regression output is another way of saying that $a$ and $b$ are distributed bivariate normal with means $[50,2]$ and there's a variance-covariance matrix that looks something like this (all numbers are made up):
  $$\begin{array}{cc}
10 &amp; cov(a,b) \\
cov(a,b) &amp; 1 \\
\end{array}$$
  So, the variance of $a$ is 10, the variance of $b$ is 1, and their covariance is $cov(a,b)$. It won't be shown in your regression table, but Zelig remembers it for you. Let's pretend it's 3.
  This variance-covariance matrix is the inverse of the Hessian I mentioned earlier. Don't worry about it. For this example, you need only remember that $\text{mean}(a,b) = [50,2]$ and $cov(a,b)=\begin{array}{cc}10&amp;3\\3&amp;1\end{array}$. For purposes of plain text email, I'm representing matrices with columns separated by commas and rows by semicolons.
  In addition, suppose that the error term $e$ is distributed with mean 0 and s.d.=1.
  Now, one way to predict what IQ you might get for somebody aged 88, based on this regression table, is exactly what you would expect: you simply calculate 50 + 88 * 2 = 226. This is your point estimate. The 95% confidence interval around this point estimate is a function of the standard errors of the coefficient estimates of $a=50$ and $b=2$, and the exact formula for that is in any econometrics textbook.
  Simulation makes it unnecessary to dig up that textbook. Instead, for 1000 rounds, <code>sim()</code> will come up with 1000 different pairs of $(a,b)$ estimates drawn from the bivariate normal with mean=[50,2] and cov=$\begin{array}{cc}10&amp;3\\3&amp;1\end{array}$. One such pair might be $(47,1.5)$; another might be $(52,3)$; yet another might be $(10,5)$. 
  Whatever they are, <code>sim()</code> plugs them into the formula and gives you 1000 different estimates for the IQ. Their average is your point estimate. If you stack them from lowest to highest, the ends of the 95% confidence interval are the top 25th value and the bottom 25th. That's it. That's all that <code>sim()</code> does.</p>
</blockquote>

<p>Given the above explanation, can anybody tell me in lay terms, what numbers <code>sim()</code> is picking? How are those numbers in pool generated? I would greatly appreciate if anyone brings some light into this.</p>
"
"0.0464572209811883","0.0472279924554862"," 78747","<p>I am working on neural networks for a regression problem in R using packages like <code>nnet</code>, <code>caret</code> etc. I have split my data into train, validation and test. My doubt is does the <code>train()</code> function in <code>caret</code> package for R takes care for validation set also.</p>

<p>From what I understand, After training the <code>nnet</code> model, you need to keep checking with validation data set, to avoid overfitting or overlearning i.e restricting the number of iterations. Then we have to tune for the decay parameters and size of hidden layers and finally apply it on the test data set.</p>

<p>Is there anything wrong with the understanding? FYI. Here is the code that I am implementing</p>

<pre><code>Y=read.csv(file=""./dolcan.csv"",header=T)
ratio=as.integer(0.5*nrow(Y))
ratio1=as.integer(0.75*nrow(Y))
traindata=Y[(1:ratio),c(2:ncol(Y))]
valdata=Y[(ratio:ratio1),c(2:ncol(Y))]
testdata=Y[((ratio1+1):nrow(Y)),c(2:ncol(Y))]

## Train the network and tuning the number of nodes and decay
maxout= max(traindata[,1]) # to scale the output
mygrid &lt;- expand.grid(.decay=c(0.5, 0.1), .size=c(3,4,5))
nnetfit &lt;- train(dolcan/maxout ~ ., data=traindata, method=""nnet"", maxit=1000, tuneGrid=mygrid, trace=F)
nnetfit
</code></pre>
"
"0.0599760143904067","0.0487768608679754"," 79107","<p>I'm trying my hand at resampling techniques with a dataset I have, and I think either I'm missing a conceptual point with bootstrapping, or I'm doing something incorrectly in <code>R</code>. Basically, I'm trying to use it in a correlation/regression framework, and I'm able to get the original coefficients, the bootstrap bias, and the bootstrap coefficients but I can't find a way to have <code>R</code> easily display the bootstrap model $R^2$ (when I'm working with several predictors), the Pearson $r$, or the $p$-values for individual regression coefficients. (I'm using the <code>Boot</code> function in the <code>car</code> package).</p>

<p>A secondary question...the more general function <code>boot</code> in the <code>boot</code> package requires defining a function to use as an argument. The function must include an argument for the original data set, and a second argument which is a set of indices, frequencies, or weights for the bootstrap sample. I'm a little confused by this. What conceptually are these indices I am specifying, and how do I specify them syntactically within my function?</p>
"
"0.11706119363752","0.119003355198144"," 79216","<p><strong>Problem</strong>: When trying to calculate the variance of timeseries sums I get a negative variance, mostly due to autocovariances at large lag steps. Does not seem realistic.</p>

<p>I have a timeseries which is calculated from another timeseries using a regression equation.
I would like to propagate the uncertainty in the regression to the final timeseries. Then I want to sum (or take mean values) different segments of the timeseries over different timeperiods, and get the uncertainty of the sums. The timeseries is originally in 1 hour frequency and I want to sum over periods of 1 day (resampling to daily frequency) up to several years. The timeseries is strongly autocorrelated at short lag times.</p>

<p>For getting the variance of the sum (in the case of 3 elements being summed):
$$Var(a+b+c)= \\ Var(a)+Var(b)+Var(c) + 2 \times (Cov(a,b) + Cov(a,c)+Cov(b,c))$$</p>

<p>I use <code>r</code> for the calculations. I get the variances for each timeseries element as $SE^2$, where $SE$ is the standard error (<code>se.fit</code>) returned from r's <code>predict()</code> function using the regression model. The covariances I get from the autocovariance function <code>acf()</code>.</p>

<p>Here is some code and a selection of the data (excuse clumsy R code, I'm very new to R):</p>

<pre><code>#tsY is the predicted timeseries from the regression
tsY=c(81.4,  79.0,  83.4,   81.7,   75.7,   68.3,   62.3,   57.2,   52.6,   48.8,   45.4,   42.6,   39.9,   37.6,   35.6,   33.8,   32.2,   30.8,   29.6,   28.4,   27.3,   26.2,   25.0,   23.9)
#tsSE is the standard error from the prediction (se.fit)
tsSE=c(1.55,  1.49, 1.60,   1.56,   1.41,   1.23,   1.09,   0.97,   0.87,   0.78,   0.71,   0.65,   0.60,   0.55,   0.51,   0.48,   0.45,   0.42,   0.40,   0.38,   0.36,   0.34,   0.32,   0.30)

tsVar=tsSE^2

#create a matrix of the autocovariances at different lag times, diagonal is lag=0
#rows and columns are indicies in timeseries
covmat&lt;-matrix(numeric(0), length(tsY),length(tsY)) 
for ( i in (1:(length(tsY)) ) ) {
  if (i == 1) {
    autocov&lt;-acf(tsY, type='covariance', lag.max= length(tsY))
    autocovvec&lt;-autocov$acf[1:nrow(autocov$acf)]
    covmat[i:length(tsY),i]=autocovvec
  }  else {
    autocov&lt;-acf(tsY[-(1:i-1)], type='covariance', lag.max= length(tsY))
    autocovvec&lt;-autocov$acf[1:nrow(autocov$acf)]
    covmat[i:length(tsY),i]=autocovvec
  }

}

# sum the matrix columns, but not the diagonal
sumofColumns &lt;- rep(NA, ncol(covmat))
for (i in (1:ncol(covmat))) {
  if (i == 1) {
    sumofColumns[i]=sum(covmat[-(1),i])  
  } else{ 
    sumofColumns[i]=sum(covmat[-(1:i),i])  
  }
}

sumofCov=sum(sumofColumns) # sum of the covariance (Cov(a,b) + Cov(a,c)+...)
sumofVar=sum(tsVar) # sum of the variances of each timeseries element
varofSum=sumofVar+2*sumofCov # variance of the sum of the timeseries

# from the covmat the negative variance occurs at larger lag times.
acf(tsY, type='covariance', lag.max= length(tsY))

&gt; sumofCov
[1] -1151.529
&gt; varofSum
[1] -2283.246
</code></pre>

<p><strong>So I have the following questions:</strong></p>

<blockquote>
  <ol>
  <li><p>Did I completely misunderstand how to calculate variance of sums?</p></li>
  <li><p>Is it better to use a cutoff from the max lags to be considered in the autocovariance? If so how would one determine this? This would especially be important with the complete data where the length is several thousand. </p></li>
  </ol>
  
  <p><strike>3. Why is the covariance negative in this sample data at large? When plotting tsY  <code>plot(tsY)</code> it looks like the covariance/correlation should remain positive.</strike> Because it is the variation in direction from their means.</p>
</blockquote>

<p><strong>EDIT:</strong></p>

<blockquote>
  <p>Comment on <strong>question 2</strong> above:
  I have realized that using n-1 lags, as above in the code, does not make a lot of sense. There appear to be few different ways to determine the maximum lags to consider.  Box &amp; Jenkins (1970) suggest n/4 and R by default 10*log10(n). This does not answer the question however, of how to determine an appropriate cutoff for summing the covariances.</p>
  
  <p>Does it make sense to look at the partial autocorrelation (function pacf()), in order not to overestimate the effect of the auto covariance in the summation term? The partial autocorrelation for my data is significantly different from zero only at 1 or 2 lags. Similarly, fitting an AR model using ar() function, I also get an order of 1 or 2.</p>
</blockquote>

<p>Cheers</p>

<p>Related post <a href=""http://stats.stackexchange.com/questions/10943/variance-on-the-sum-of-predicted-values-from-a-mixed-effect-model-on-a-timeserie"">Variance on the sum of predicted values from a mixed effect model on a timeseries</a></p>
"
"0.0379321620905441","0.0385614943639849"," 80312","<p>I have carried out this linear regression that includes month coded as a dummy variable:</p>

<pre><code>library(plyr)
set.seed(1)
y &lt;- rnorm(120)
x1 &lt;- c(rep(""adult"", 60), rep(""juvenile"", 60))
x2 &lt;- c(rep(""male"", 60), rep(""female"", 60))
x3 &lt;- unlist(llply(month.abb, function(x) rep(x, 10)))

summary(lm(y ~ x1 + x2 + x3))

Call:
lm(formula = y ~ x1 + x2 + x3)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.46354 -0.51524 -0.03981  0.57625  1.95041 

Coefficients: (2 not defined because of singularities)
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  0.12073    0.28564   0.423    0.673
x1juvenile   0.00663    0.40396   0.016    0.987
x2male            NA         NA      NA       NA
x3Aug       -0.37510    0.40396  -0.929    0.355
x3Dec       -0.24718    0.40396  -0.612    0.542
x3Feb        0.12812    0.40396   0.317    0.752
x3Jan        0.01147    0.40396   0.028    0.977
x3Jul        0.32385    0.40396   0.802    0.424
x3Jun        0.02273    0.40396   0.056    0.955
x3Mar       -0.25440    0.40396  -0.630    0.530
x3May        0.01341    0.40396   0.033    0.974
x3Nov        0.22012    0.40396   0.545    0.587
x3Oct       -0.01502    0.40396  -0.037    0.970
x3Sep             NA         NA      NA       NA

Residual standard error: 0.9033 on 108 degrees of freedom
Multiple R-squared:  0.04703,   Adjusted R-squared:  -0.05003 
F-statistic: 0.4845 on 11 and 108 DF,  p-value: 0.9093
</code></pre>

<p>I now want to present the results of this linear regression within a table. Instead of presenting the beta for every month, is there a way to summarise the overall effect of month on <code>y</code> within the same table? For example, would if be acceptable to summarise the beta, se, t value and p value of <code>x3</code> by using their mean values across months?</p>
"
"0.026822089039291","0.0272670941574606"," 80463","<p>I have the following set of model-averaged fixed effects from a set of binomial GLMMs: </p>

<p><img src=""http://i.stack.imgur.com/yN4wR.png"" alt=""model parameters image""></p>

<p>I would like to plot the predicted effect of ""NBT"", along with confidence bands, while holding all the other variables at their baseline levels. My attempt to do this in ggplot:</p>

<pre><code>Xvars &lt;- seq(from=0, to=100, by=0.1)  #NBT range is 0-100
  binomIntercept &lt;- 1.317
  binomSlope &lt;- -0.0076     
  binomSE &lt;- 0.009    
Means &lt;- logistic(binomIntercept + binomSlope*Xvars)              
loCI &lt;- logistic(binomIntercept + (binomSlope - 1.96*binomSE)*Xvars)
upCI &lt;- logistic(binomIntercept + (binomSlope + 1.96*binomSE)*Xvars)
df &lt;- data.frame(Xvars,Means,loCI,upCI)
p &lt;- ggplot(data=df, aes(x = Xvars, y = Means)) + 
geom_line() +          
geom_line(data=df, aes(x = Xvars, y = upCI),col='grey') +
geom_line(data=df, aes(x = Xvars, y = loCI), col='grey')
p                                            
</code></pre>

<p><img src=""http://i.imgur.com/eMJBxQQ.png"" alt=""graph image""></p>

<p>I'm assuming that the confidence bands are cone shaped because I'm not accounting for uncertainty in the estimate for the intercept. Maybe this is okay (?), but it does look different from every regression line I've ever seen with confidence intervals plotted.</p>

<p>Can someone please tell me how I should be writing my equations to get the correct confidence intervals, given the intercept, slope, and standard errors from my model output?</p>

<p>(I know I can use the predict function to do this in R, but would like to know how to do it by hand.)  </p>
"
"0.0464572209811883","0.0472279924554862"," 80888","<p>I am trying to understand how to get the coefficient of a multiple linear regression. </p>

<p>The formula is:</p>

<p>$b = (X'X)^{-1}(X')Y$</p>

<p>I try to calculate $b$ without package and with the <code>lm</code> package inside R. </p>

<p>Doing so, I got different results. </p>

<p>I want to know why. Did I made a mistake? Or does the <code>lm</code> package calculate differently because of the intercept?</p>

<pre><code>&gt; y &lt;-  c(1,2,3,4,5)
&gt; x1 &lt;- c(1,2,3,4,5)
&gt; x2 &lt;- c(1,4,5,7,9)
&gt; Y &lt;- as.matrix(y)
&gt; X &lt;- as.matrix(cbind(x1,x2))
&gt; beta = solve(t(X) %*% X) %*% (t(X) %*% Y) ; beta
            [,1]
x1  1.000000e+00
x2 -1.421085e-14
&gt; model &lt;- lm(y~x1+x2) ; model$coefficients
 (Intercept)           x1           x2 
1.191616e-15 1.000000e+00 1.192934e-15 
</code></pre>

<p><img src=""http://i.stack.imgur.com/KHD2q.png"" alt=""3d""></p>

<h1>Update</h1>

<p>As Alex and the other told me, it was a question of roundoff error. Therefore, I decided to take another data from the book ""Essential Statistics for business and economics"" by Anderson and all. In this case, the coefficients are the same in both <code>lm</code> function and in my own matrix.</p>

<pre><code>&gt; y &lt;- c(9.3, 4.8, 8.9, 6.5, 4.2, 6.2, 7.4, 6, 7.6, 6.1)
&gt; x0 &lt;- c(1,1,1,1,1,1,1,1,1,1) 
&gt; x1 &lt;-  c(100,50,100,100,50,80,75,65,90,90)
&gt; x2 &lt;- c(4,3,4,2,2,2,3,4,3,2)
&gt; Y &lt;- as.matrix(y)
&gt; X &lt;- as.matrix(cbind(x0,x1,x2))

&gt; beta = solve(t(X) %*% X) %*% (t(X) %*% Y);beta
         [,1]
x0 -0.8687015
x1  0.0611346
x2  0.9234254
&gt; model &lt;- lm(y~+x1+x2) ; model$coefficients
(Intercept)          x1          x2 
 -0.8687015   0.0611346   0.9234254 
</code></pre>
"
"0.0758643241810882","0.0771229887279699"," 81120","<p>I frequently use this model to test catch efficiency and size selection properties of a given trawl fishing gear:</p>

<p>\begin{equation}
\theta(l)=\frac{s\times r(l)}{(1-s)+s\times r(l)}
\end{equation}</p>

<p>where $\theta(l)$ denotes the expected catch rate in the test gear ($T$), which has been fishing in parallel with a non-selective gear (the control gear with blind meshes,$C$). The parameters affecting $\theta(l)$ are:</p>

<ul>
<li><p><strong>Split parameter</strong> $(s)$: It defines the probability of a fish to enter in $T$ ($s$) or in $C$ ($1-s$), $s\in\{0,1\}$</p></li>
<li><p><strong>Fish size selection</strong> ($r(l)$): It defines the likelihood of fish retention in $T$. This likelihood is conditioned to fish body length, therefore it describes the size selection in $T$. Fish size selection is used to be defined using the logit function:</p></li>
</ul>

<p>\begin{equation}
r(l)=\frac{exp(\beta_1+\beta_2\times l )}{1+exp(\beta_1+\beta_2\times l )} 
\end{equation}</p>

<p>Overall, there is a total of 3 parameters to be estimated ($s$, $\beta_1$, $\beta_2$). We use nonlinear regression techniques to estimate such parameters by maximizing the binomial log-likelihood function:</p>

<p>\begin{equation}
\sum_l(N_{l}^T\times \log\theta(l)+N_{l}^C\times \log(1-\theta(l))) 
\end{equation}</p>

<p>where $N_{l}^T$ is the number of fishes per length-class caught in $T$, and $N_{l}^C$ is the numbers caught in $C$.</p>

<p>During a normal experiment, we deploy the pair of gears ($T$ and $C$) several times to perform parallel fishing. To account for the between-haul variation, we use the bootstrap (using a resampling scheme based on resampling between hauls and fishes within hauls) to estimate the errors of $s$, $\beta_1$ and $\beta_2$.</p>

<p>IÂ´m wondering if itÂ´s possible to shift towards a nonlinear mixed modeling approach, where the hauls are considered as a random component. At the moment I only could find such approach by using least squares as minimization criteria. But I could not find a way to keep using the log-likelihood binomial mass function as target criteria.</p>

<p>Thank you beforehand for any comment or guidance.</p>
"
"0.0892693083195917","0.0907503748778111"," 81612","<p>Recently I was trying to do logistic regression using the <code>rms::lrm()</code> function. But I had some trouble understanding the model objects from the function. Here is the example from the package:</p>

<pre><code>#dataset
n            &lt;- 1000    # define sample size
set.seed(17)            # so can reproduce the results
treat        &lt;- factor(sample(c('a','b','c'), n,TRUE))
num.diseases &lt;- sample(0:4, n,TRUE)
age          &lt;- rnorm(n, 50, 10)
cholesterol  &lt;- rnorm(n, 200, 25)
weight       &lt;- rnorm(n, 150, 20)
sex          &lt;- factor(sample(c('female','male'), n,TRUE))
L            &lt;- .1*(num.diseases-2) + .045*(age-50) +
                (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
                3.5*(treat=='b')+2*(treat=='c'))
y            &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)
#fit model
g            &lt;- lrm(y ~ treat*rcs(age))

&gt; g

Logistic Regression Model

lrm(formula = y ~ treat * rcs(age))

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          1000    LR chi2      76.77    R2       0.099    C       0.656    
 0            478    d.f.            14    g        0.665    Dxy     0.312    
 1            522    Pr(&gt; chi2) &lt;0.0001    gr       1.945    gamma   0.314    
max |deriv| 3e-06                          gp       0.156    tau-a   0.156    
                                           Brier    0.231    
&gt; anova(g)
                Wald Statistics          Response: y 

 Factor                                     Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)        5.62      10   0.8462
  All Interactions                           1.30       8   0.9956
 age  (Factor+Higher Order Factors)         65.99      12   &lt;.0001
  All Interactions                           1.30       8   0.9956
  Nonlinear (Factor+Higher Order Factors)    2.23       9   0.9872
 treat * age  (Factor+Higher Order Factors)  1.30       8   0.9956
  Nonlinear                                  0.99       6   0.9858
  Nonlinear Interaction : f(A,B) vs. AB      0.99       6   0.9858
 TOTAL NONLINEAR                             2.23       9   0.9872
 TOTAL NONLINEAR + INTERACTION               2.57      11   0.9953
 TOTAL                                      69.06      14   &lt;.0001
</code></pre>

<p><strong>Here are my questions:</strong><br>
For the object <code>g</code>,  </p>

<ul>
<li>What does the <code>max |deriv| 3e-06</code> mean?  </li>
<li>What do the Discrimination and Rand Discrim. Indexes suggest?  </li>
</ul>

<p>For the <code>anova(g)</code> object,  </p>

<ul>
<li>What's the <code>Factor +Higher Order Factors</code> for the treat?  </li>
<li>Why there are two <code>all interactions</code>? How to explain the nonlinear parts?</li>
</ul>
"
"0.0464572209811883","0.0472279924554862"," 81699","<p>I'm new to data analysis so this is kind of a simple question.</p>

<p>I would like to understand why I cannot reproduce a survival curve generated by a fitted exponential model from Stata. I use the coefficients and make my function in R to plot but it looks nothing similar. I believe it's one of those daft problems where I am not interpreting something properly. I illustrate below.</p>

<p>First, some data in Stata:</p>

<pre><code>use http://www.ats.ucla.edu/stat/data/uis.dta, clear
gen id = ID
drop ID
stset time, failure(censor)
</code></pre>

<p>Then we can fit a null exponential model</p>

<pre><code>streg, dist(exponential) nohr
</code></pre>

<p>Which gives the following output:</p>

<pre><code>Exponential regression -- log relative-hazard form 

No. of subjects =          628                     Number of obs   =       628
No. of failures =          508
Time at risk    =       147394
                                                   LR chi2(0)      =     -0.00
Log likelihood  =    -1043.531                     Prob &gt; chi2     =         .

------------------------------------------------------------------------------
          _t |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       _cons |  -5.670383   .0443678  -127.80   0.000    -5.757342   -5.583424
------------------------------------------------------------------------------
</code></pre>

<p>I take the survivor function from Stata's documentation:</p>

<p>$$
S(t)=\exp(-\lambda_{j} t_{j})
$$</p>

<p>So, in R, I plot this out with the following:</p>

<pre><code>S &lt;- function(x) exp(-5.670383*x) # 'x' acts like 't'
curve(S, 0, 1000)
</code></pre>

<p>This curve is not equivalent to Stata's, given by:</p>

<pre><code>stcurve, surv
</code></pre>

<p>Where did I go wrong in my interpretation? Is my equation using the correct parameterization?</p>

<p>P.S. Why am I reproducing these curves? A little to do with overlaying curves but now that I have this problem, I have to know where I went wrong.</p>
"
"0.0479808115123254","0.0609710760849692"," 82236","<p>I am using <code>earth package</code><a href=""http://cran.r-project.org/web/packages/earth/index.html"" rel=""nofollow"">earth: Multivariate Adaptive Regression Spline Models</a> regression to get a constant piecewise approximation of my data. I want to plot a band of confidence around it.  Does this make sense to estimate a confidence interval of the smoothed function? If yes how can I do this?</p>

<p>I know that confidence intervals cannot be calculated directly, since it is a non parametric regression but I want to have a coherent result like (plot a band of confidence around my smoothed function) what I can get using <code>lm</code> or <code>loess</code> smoothing.</p>

<p><strong>EDIT</strong> </p>

<p>I add some code to clarify my idea, Here what I would do if I am using linear regression or <code>loess</code>.</p>

<pre><code>set.seed(1)
x &lt;- rnorm(15)
dat &lt;- data.frame(x = x, y = c(x + rnorm(15)))
</code></pre>

<h3>Using lm</h3>

<pre><code>mod &lt;- lm(y ~ x,data=dat)
predfit &lt;- predict(mod,se=TRUE,interval=""confidence"")$fit
</code></pre>

<h3>Using loess</h3>

<pre><code>level=0.95
mod &lt;- loess(y~x,data=dat)
pred &lt;- predict(mod, se = TRUE)
y = pred$fit
    ci &lt;- pred$se.fit * qt(level / 2 + .5, pred$df)
data.frame(ymin = y - ci,
           ymax = y + ci)
</code></pre>

<p>My question how to get <code>ymin</code> and <code>ymax</code> if I use <code>earth</code> :</p>

<pre><code>library(earth)
mod = earth(y~x,data=dat)
</code></pre>
"
"0.0379321620905441","0.0385614943639849"," 82307","<p>I was working through the lab on ridge regression and LASSO in ISLR and I came across a strange behavior in the <code>cv.glmnet</code> function. When I followed the lab as written I got the following </p>

<pre><code>set.seed(1)
train &lt;- sample(1:nrow(x), nrow(x)/2)
test &lt;- (-train)
y.test &lt;- y[test]
set.seed(1)
cv.out &lt;- cv.glmnet(x[train,], y[train], lambda=grid, alpha=0)
plot(cv.out)
bestlam &lt;- cv.out$lambda.min
bestlam
[1] 231.013
</code></pre>

<p>For my own benefit I tried it using a different seed (<code>8675309</code>) and got back a different result. Any combination of setting the seeds resulted in different answers. I am assuming this has to do with how the 10-folds are changed with the different seeds, however the different <code>lambda.min</code> can vary so much I am concerned the package might not be stable. Am I missing something?</p>
"
"0.0758643241810882","0.0771229887279699"," 82356","<p>I have the following table in <code>R</code></p>

<pre><code>df &lt;- structure(list(x = structure(c(12458, 12633, 12692, 12830, 13369, 
13455, 13458, 13515), class = ""Date""), y = c(6080, 6949, 7076, 
7818, 0, 0, 10765, 11153)), .Names = c(""x"", ""y""), row.names = c(""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""8"", ""9""), class = ""data.frame"")

&gt; df
           x     y
1 2004-02-10  6080
2 2004-08-03  6949
3 2004-10-01  7076
4 2005-02-16  7818
5 2006-08-09     0
6 2006-11-03     0
8 2006-11-06 10765
9 2007-01-02 11153
</code></pre>

<p>I can plot the points and a Tukey's linear fitting (<code>line</code> function in <code>R</code>) via </p>

<pre><code>plot(data=df,  y ~ x)
lines(df$x, line(df$x, df$y)$fitted.values)
</code></pre>

<p>which produces:</p>

<p><img src=""http://i.stack.imgur.com/n5TGd.png"" alt=""enter image description here""></p>

<p>All fine. The above plot shows energy consumption values, expected only to increase, so I'm happy with the fit not passing through those two points (which will be subsequently flagged as outliers).</p>

<p>However, ""just"" removing the last point and replot again</p>

<pre><code>df &lt;- df[-nrow(df),]
plot(data=df,  y ~ x)
lines(df$x, line(df$x, df$
)$fitted.values)
</code></pre>

<p>The result is completely different.</p>

<p><img src=""http://i.stack.imgur.com/4Ddau.png"" alt=""enter image description here""></p>

<p>My need is to have ideally the same result in both scenarios above. R doesn't seem to have ready to use function for monotonic regression, besides <code>isoreg</code> which however is piecewise constant.</p>

<p>EDIT:</p>

<p>As @Glen_b pointed out the outliers-to-sample size ratio is too big (~28%) for the regression technique used above. However, I believe there might be something else to consider. If I add the points at the beginning of the table:</p>

<pre><code>df &lt;- rbind(data.frame(x=c(as.Date(""2003-10-01""), as.Date(""2003-12-01"")), y=c(5253,5853)), df)
</code></pre>

<p>and recalculate again like above <code>plot(data=df,  y ~ x); lines(df$x, line(df$x,df$y)$fitted.values)</code> I get the same result, with a ration of ~22%</p>

<p><img src=""http://i.stack.imgur.com/hwIZp.png"" alt=""enter image description here""></p>
"
"0.0402331335589365","0.0545341883149212"," 83012","<p>From Robert Kabacoff's <a href=""http://www.statmethods.net/advstats/bootstrapping.html"">Quick-R</a> I have </p>

<pre><code># Bootstrap 95% CI for regression coefficients 
library(boot)
# function to obtain regression weights 
bs &lt;- function(formula, data, indices) {
  d &lt;- data[indices,] # allows boot to select sample 
  fit &lt;- lm(formula, data=d)
  return(coef(fit)) 
} 
# bootstrapping with 1000 replications 
results &lt;- boot(data=mtcars, statistic=bs, 
     R=1000, formula=mpg~wt+disp)

# view results
results
plot(results, index=1) # intercept 
plot(results, index=2) # wt 
plot(results, index=3) # disp 

# get 95% confidence intervals 
boot.ci(results, type=""bca"", index=1) # intercept 
boot.ci(results, type=""bca"", index=2) # wt 
boot.ci(results, type=""bca"", index=3) # disp
</code></pre>

<p>How can I obtain the p-values $H_0:\, b_j=0$ of the bootstrap regression coefficients?</p>
"
"0.0599760143904067","0.0609710760849692"," 83260","<p>I used the clogit function (from the survival package) to run a conditional logistic regression in R with a big dataset of 1:M matched pairs with n=300368964 and number of events= 39995.</p>

<pre><code>model &lt;- clogit(Alliance ~ OVB + CVC + BVB + strata(Strata), method=""exact"")    
</code></pre>

<p>I received following results:</p>

<pre><code>                 coef  exp(coef)   se(coef)       z Pr(&gt;|z|)    
OVB        -0.0498174  0.9514031  0.0166275  -2.996  0.00273 ** 
BVB         0.0277405  1.0281289  0.0304956   0.910  0.36300    
CVC         1.1709851  3.2251683  0.1089709  10.746  &lt; 2e-16 ***
EarlyStage -1.3215824  0.2667129  0.0205851 -64.201  &lt; 2e-16 ***
AvgVCSize   0.0087976  1.0088364  0.0002035  43.224  &lt; 2e-16 ***
NumberVC    0.0643579  1.0664740  0.0034502  18.653  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Rsquare= 0   (max possible= 0.001 )
Likelihood ratio test= 6511  on 6 df,   p=0
Wald test            = 6471  on 6 df,   p=0
Score (logrank) test = 6801  on 6 df,   p=0
</code></pre>

<p>Since Rsquare equals 0 and the test ratios seems very high, I tried to plot the results to check whether the model fits. But I wasn't able to plot it properly.</p>

<p>I would online many papers which use the ratio Prob > chi2 = 0 from Stata as test ratio to proof the model fit. </p>

<p>How could I calculate this ratio in R? Are there any other ways I could check the model fit of my clogit results?</p>

<p>I would appreciate any help.</p>

<p>Thanks you very much in advance.</p>
"
"0.026822089039291","0.0272670941574606"," 83364","<p>I have been reading a number of papers where researchers have created risk scores based on logistic regression models. Often they refer to ""<a href=""http://www.ncbi.nlm.nih.gov/pubmed/15122742"" rel=""nofollow"">Sullivan's method</a>"" but I have no access to this paper and the explanations provided are far from clear. I noticed that Dr. Harrell's excellent RMS package provides a nomogram function which is in a way similar to creating a risk score (albeit with a very pretty graphical output).</p>

<p>It seems after tinkering around with it that the way it works is by dividing the beta coefficients by the smallest beta coefficient and then multiplying a constant to create points for categorical variables. However I cannot for the life of me figure out what is going on with continuous variables. I've spent hours searching google without much luck, and I would appreciate if someone could shed some light on this for me. Thanks!</p>
"
"0.0379321620905441","0.0385614943639849"," 83433","<p>I would like to ask how the long-term (multiple step ahead) prediction intervals are calculated by function <code>predict.Arima</code> in R. I am particularly interested in ARIMA models, SARIMA models and in ARIMA models with external regressors (include argument xreg => regression with ARIMA errors) </p>
"
"NaN","NaN"," 83522","<p>I just fitted a boosted regression coxph model:</p>

<pre><code>cox=gbm(Surv(periods, event) ~ grade + fico_range_low + revol_util + dti, data=notes)
</code></pre>

<p>However, I want to obtain the survival curve from the model similar to the <code>survfit()</code> function in the <code>survival</code> package.  Does anyone know how to obtain using the model from the <code>gbm</code> package?</p>
"
"0.026822089039291","0.0272670941574606"," 83554","<p>Sorry for this beginner's question... I have googled this for a while with no success.</p>

<p>I do a linear regression using R lm function:</p>

<pre><code>x = log(errors)
plot(x,y)
lm.result = lm(formula = y ~ x)
abline(lm.result, col=""blue"") # showing the ""fit"" in blue
</code></pre>

<p><img src=""http://i.stack.imgur.com/2p7hZ.png"" alt=""enter image description here""></p>

<p>but it does not fit well. Unfortunately I can't make sense of the manual.</p>

<p>Can someone point me in the right direction to fit this better?</p>

<p>By fitting I mean I want to minimize the Root Mean Squared Error (RMSE).</p>

<hr>

<p><strong>Edit</strong>:
I have posted a related question (it's the same problem) here:
<a href=""http://stats.stackexchange.com/questions/83576/can-i-decrease-further-the-rmse-based-on-this-feature"">Can I decrease further the RMSE based on this feature?</a></p>

<p>and the raw data here:</p>

<p><a href=""http://tny.cz/c320180d"" rel=""nofollow"">http://tny.cz/c320180d</a></p>

<p>except that on that <a href=""http://tny.cz/c320180d"" rel=""nofollow"">link</a> x is what is called errors on the present page here, and there are less samples (1000 vs 3000 in the present page plot). I wanted to make things simpler in the other question.</p>
"
"0.0929144419623766","0.0865846528350581"," 83826","<p>I estimated a robust linear model in <code>R</code> with MM weights using the <code>rlm()</code> in the MASS package. `R`` does not provide an $R^2$ value for the model, but I would like to have one if it is a meaningful quantity. I am also interested to know if there is any meaning in having an $R^2$ value that weighs the total and residual variance in the same way that observations were weighted in the robust regression. My general thinking is that, if, for the purposes of the regression, we are essentially with the weights giving some of the estimates less influence because they are outliers in some way, then maybe for the purpose of calculating $r^2$ we should also give those same estimates less influence? </p>

<p>I wrote two simple functions for the $R^2$ and the weighted $R^2$, they are below. I also included the results of running these functions for my model which is called HI9.  EDIT: I found web page of Adelle Coster of UNSW that gives a formula for <code>R2</code> that includes the weights vector in calculating the calculation of both <code>SSe</code> and <code>SSt</code> just as I did, and asked her for a more formal reference: <a href=""http://web.maths.unsw.edu.au/~adelle/Garvan/Assays/GoodnessOfFit.html"">http://web.maths.unsw.edu.au/~adelle/Garvan/Assays/GoodnessOfFit.html</a> (still looking for help from Cross Validated on how to interpret this weighted $r^2$.)</p>

<pre><code>#I used this function to calculate a basic r-squared from the robust linear model
r2 &lt;- function(x){  
+ SSe &lt;- sum((x$resid)^2);  
+ observed &lt;- x$resid+x$fitted;  
+ SSt &lt;- sum((observed-mean(observed))^2);  
+ value &lt;- 1-SSe/SSt;  
+ return(value);  
+ }  
r2(HI9)  
[1] 0.2061147

#I used this function to calculate a weighted r-squared from the robust linear model
&gt; r2ww &lt;- function(x){
+ SSe &lt;- sum((x$w*x$resid)^2); #the residual sum of squares is weighted
+ observed &lt;- x$resid+x$fitted;
+ SSt &lt;- sum((x$w*(observed-mean(observed)))^2); #the total sum of squares is weighted      
+ value &lt;- 1-SSe/SSt;
+ return(value);
+ }
 &gt; r2ww(HI9)
[1] 0.7716264
</code></pre>

<p>Thanks to anyone who spends time answering this. Please accept my apologies if there is already some very good reference on this which I missed, or if my code above is hard to read (I am not a code guy).</p>
"
"0.0547503599848004","0.0556587228785024"," 83908","<p>I am working in program R. I am modeling the incidence of flight in a seabird in relation to distance to the nearest ship (potential disturbance, range = 0 to 74 km from the bird). 1= flight during observation, 0 = no flight. The bird does fly with some unknown probability when no ships are present or really ""far"" away. I am trying to find this really far distance and associated probability of flight using binary logistic regression.</p>

<p>Model = Flight ~ ship distance. Other variables were explored but fell out with stepwise selection.</p>

<p>During exploratory analysis I truncated the data down only looking at smaller distances from the ship (20, 15, 10 km). These models are highly significant and predict that as the ship gets closer the probability of flight increases. However when I include all the data (out to 74 km) the intercept is significant (and predicts the true % of observed flight events) but the slope term is non-significant. </p>

<p>Can I use a weighting scheme to give more weight to observations when the ship was closer?</p>

<p>Thanks.</p>

<p>Edit: I am working through the suggestions made by @Scortchi and @Underminer. Here is a plot of a loess smooth on the observed data to better help visualize the pattern. </p>

<p><img src=""http://i.stack.imgur.com/ZabQh.jpg"" alt=""Loess smooth of probability of flight as a function of distance to nearest ship""></p>

<p>The distance to the ship data does not discriminate between approaching ships and departing ships it is just a straight line measure to the nearest ship. The dip in the probability of flight at 8.5 I believe can be attributed to ""unaffected"" birds that did not fly as the ship passed by them. So as the ship departs and gets further from the observation site we were more like to be observing birds that for whatever reason did not fly when the ship passed and are less likely to fly for ""naturally occurring"" reason. As additional birds fill back into the observation area the ""baseline"" flushing rate is resumed and birds start to fly at ""normal"" probabilities. </p>
"
"0.0657004319817604","0.0556587228785024"," 84319","<p>I am working on an age estimation method using 4 types of biological measurements as age predictors. I am using RStudio. 
So far, I have good results when I use linear regression (<code>lm(age~predictor)</code>), but I am encountering heteroskedasticity, and therefore cannot build prediction intervals for my models.<br> 
I have tried transformations to normalize the predictors using ln, inverse, and square root, but to no avail.<br>
I have found a paper explaining the <code>wls</code> function, and I have used it in my models with the weight: $$\frac 1 {1+\frac{\text{predictor}^2} 2}$$ 
This has given me better age predictions, but does not solve the heteroskedasticity problem. </p>

<p>I have done some research, and apparently, one of my options is to create homoscedastic groups in my data by finding the data points where the residual variances change. 
For that, I have used the breakpoints function of strucchange, which gave me 5 breakpoints by default. 
I now want to give 6 different weights (weights are $\frac 1 {\text{var(age)}}$ of each interval) to my 6 intervals of data, but I cannot find a function to do that. I would greatly appreciate any help on the subject. 
Thanks.</p>
"
"0.093190562755245","0.0947366868222571"," 85909","<p>The <code>plm</code> function of the <code>plm</code> library in R is giving me grief over having duplicate time-id couples, even when I'm running a model that I don't think should need a time variable at all (see reproducible example below).</p>

<p>I can think of three possibilities:</p>

<ol>
<li>My understanding of fixed effects regression is wrong, and they really do require unique time indices (or time indices at all!).</li>
<li>plm() is just being overly-finicky here and should relax this requirement.</li>
<li>The particular estimation technique that plm() uses--the within transformation--requires time indices, even though the order doesn't seem to matter and the less computationally-efficient version (including dummies in a straight-up OLS model) doesn't need them.</li>
</ol>

<p>Any thoughts?</p>

<pre><code>set.seed(1)
n &lt;- 1000
test &lt;- data.frame( grp = as.factor(rep( letters, (n/length(letters))+1 ))[seq(n)], x = runif(n), z = runif(n) )
test$y &lt;- with( test, 2*x + 3*z + rnorm(n) )
lm( y ~ x + z, data = test )
lm( y ~ x + z + grp, data = test )

require(plm)
# Model fails if I don't specify a time index, despite effect = ""individual""
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = ""grp"" ) 
# Create time variable and add it to the index but still specify individual FE not time FE also
library(plyr)
test &lt;- ddply( test, .(grp), function(dat) transform( dat, t = seq(nrow(dat)) ) )
# Now plm() works; note coefficients clearly include the fixed effects, as they match the lm() version above
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = c(""grp"",""t"") ) 
# Scramble time variables and show they don't matter as long as they're unique within a cluster
test &lt;- ddply( test, .(grp), function(dat) transform( dat, t = sample(t) ) )
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = c(""grp"",""t"") ) 
# Add a duplicate time entry and show that it causes plm() to fail
test[ 2, ""t"" ] &lt;- test[ 1, ""t"" ] 
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = c(""grp"",""t"") ) 
</code></pre>

<p><strong>Why this matters</strong></p>

<p>I'm trying to bootstrap my model, and when I do the requirement that the index-time pairs be unique is causing headaches which seem unnecessary if (2) is true.</p>
"
"0.0663812836584521","0.0771229887279699"," 85913","<p>I want to fit a DLM with time-varying coefficients, i.e. an extension to the usual linear regression,</p>

<p>$y_t = \theta_1 + \theta_2x_2$.</p>

<p>I have a predictor ($x_2$) and a response variable ($y_t$), marine &amp; inland annual fish catches respectively from 1950 - 2011. I want the DLM regression model to follow,</p>

<p>$y_t = \theta_{t,1} + \theta_{t,2}x_t$</p>

<p>where the system evolution equation is</p>

<p>$\theta_t = G_t \theta_{t-1}$</p>

<p>from page 43 of Dynamic Linear Models With R by Petris et al.</p>

<p>Some coding here,</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)
x &lt;- fishdata$marinefao
    y &lt;- fishdata$inlandfao

lmodel &lt;- lm(y ~ x)
summary(lmodel)
plot(x, y)
abline(lmodel)
</code></pre>

<p>Clearly time-varying coefficients of the regression model are more appropriate here. I follow his example from pages 121 - 125 and want to apply this to my own data. This is the coding from the example</p>

<pre><code>############ PAGE 123
require(dlm)

capm &lt;- read.table(""http://shazam.econ.ubc.ca/intro/P.txt"", header=T)
capm.ts &lt;- ts(capm, start = c(1978, 1), frequency = 12)
colnames(capm)
plot(capm.ts)
IBM &lt;- capm.ts[, ""IBM""]  - capm.ts[, ""RKFREE""]
x &lt;- capm.ts[, ""MARKET""] - capm.ts[, ""RKFREE""]
x
plot(x)
outLM &lt;- lm(IBM ~ x)
outLM$coef
    acf(outLM$res)
qqnorm(outLM$res)
    sig &lt;- var(outLM$res)
sig

mod &lt;- dlmModReg(x,dV = sig, m0 = c(0, 1.5), C0 = diag(c(1e+07, 1)))
outF &lt;- dlmFilter(IBM, mod)
outF$m
    plot(outF$m)
outF$m[ 1 + length(IBM), ]

########## PAGES 124-125
buildCapm &lt;- function(u){
  dlmModReg(x, dV = exp(u[1]), dW = exp(u[2:3]))
}

outMLE &lt;- dlmMLE(IBM, parm = rep(0,3), buildCapm)
exp(outMLE$par)
    outMLE
    outMLE$value
mod &lt;- buildCapm(outMLE$par)
    outS &lt;- dlmSmooth(IBM, mod)
    plot(dropFirst(outS$s))
outS$s
</code></pre>

<p>I want to be able to plot the smoothing estimates <code>plot(dropFirst(outS$s))</code> for my own data, which I'm having trouble executing. </p>

<p><strong>UPDATE</strong></p>

<p>I can now produce these plots but I don't think they are correct.</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)
x &lt;- as.numeric(fishdata$marinefao)
    y &lt;- as.numeric(fishdata$inlandfao)
xts &lt;- ts(x, start=c(1950,1), frequency=1)
xts
yts &lt;- ts(y, start=c(1950,1), frequency=1)
yts

lmodel &lt;- lm(yts ~ xts)
#################################################
require(dlm)
    buildCapm &lt;- function(u){
  dlmModReg(xts, dV = exp(u[1]), dW = exp(u[2:3]))
}

outMLE &lt;- dlmMLE(yts, parm = rep(0,3), buildCapm)
exp(outMLE$par)
        outMLE$value
mod &lt;- buildCapm(outMLE$par)
        outS &lt;- dlmSmooth(yts, mod)
        plot(dropFirst(outS$s))

&gt; summary(outS$s); lmodel$coef
       V1              V2       
 Min.   :87.67   Min.   :1.445  
 1st Qu.:87.67   1st Qu.:1.924  
 Median :87.67   Median :3.803  
 Mean   :87.67   Mean   :4.084  
 3rd Qu.:87.67   3rd Qu.:6.244  
 Max.   :87.67   Max.   :7.853  
 (Intercept)          xts 
273858.30308      1.22505 
</code></pre>

<p>The intercept smoothing estimate (V1) is far from the lm regression coefficient. I assume they should be nearer to each other. </p>
"
"0.107288356157164","0.102251603090477"," 86273","<p>I'm trying to calculate the log-likelihood for a generalized nonlinear least squares regression for the function $f(x)=\frac{\beta_1}{(1+\frac x\beta_2)^{\beta_3}}$ optimized by the <code>gnls</code> function in the R package <code>nlme</code>, using the variance covariance matrix generated by distances on a a phylogenetic tree assuming Brownian motion (<code>corBrownian(phy=tree)</code> from the <code>ape</code> package). The following reproducible R code fits the gnls model using x,y data and a random tree with 9 taxa:</p>

<pre><code>require(ape)
require(nlme)
require(expm)
tree &lt;- rtree(9)
x &lt;- c(0,14.51,32.9,44.41,86.18,136.28,178.21,262.3,521.94)
y &lt;- c(100,93.69,82.09,62.24,32.71,48.4,35.98,15.73,9.71)
data &lt;- data.frame(x,y,row.names=tree$tip.label)
model &lt;- y~beta1/((1+(x/beta2))^beta3)
f=function(beta,x) beta[1]/((1+(x/beta[2]))^beta[3])
start &lt;- c(beta1=103.651004,beta2=119.55067,beta3=1.370105)
correlation &lt;- corBrownian(phy=tree)
fit &lt;- gnls(model=model,data=data,start=start,correlation=correlation)
logLik(fit) 
</code></pre>

<p>I would like to calculate the log-likelihood ""by hand"" (in R, but without use of the <code>logLik</code> function) based on the estimated parameters obtained from <code>gnls</code> so it matches the output from <code>logLik(fit)</code>. NOTE: I am not trying to estimate parameters; I just want to calculate log-likelihood of the parameters estimated by the <code>gnls</code> function (although if someone has a reproducible example of how to estimate parameters without <code>gnls</code>, I would be very interested in seeing it!). </p>

<p>I'm not really sure how to go about doing this in R. The linear algebra notation described in Mixed-Effects Models in S and S-Plus (Pinheiro and Bates) is very much over my head and none of my attempts have matched <code>logLik(fit)</code>. Here are the details described by Pinheiro and Bates:</p>

<p>The log-likelihood for the generalized nonlinear least squares model  $y_i=f_i(\phi_i,v_i)+\epsilon_i$ where $\phi_i=A_i\beta$ is calculated as follows:</p>

<p>$l(\beta,\sigma^2,\delta|y)=-\frac 12 \Bigl\{ N\log(2\pi\sigma^2)+\sum\limits_{i=1}^M{\Bigl[\frac{||y_i^*-f_i^*(\beta)||^2}{\sigma^2}+\log|\Lambda_i|\Bigl]\Bigl\}}$</p>

<p>where $N$ is the number of observations, and $f_i^*(\beta)=f_i^*(\phi_i,v_i)$.</p>

<p>$\Lambda_i$ is positive-definite, $y_i^*=\Lambda_i^{-T/2}y_i$ and $f_i^*(\phi_i,v_i)=\Lambda_i^{-T/2}f_i(\phi_i,v_i)$</p>

<p>For fixed $\beta$ and $\lambda$, the ML estimator of $\sigma^2$ is </p>

<p>$\hat\sigma(\beta,\lambda)=\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2 / N$</p>

<p>and the profiled log-likelihood is</p>

<p>$l(\beta,\lambda|y)=-\frac12\Bigl\{N[\log(2\pi/N)+1]+\log\Bigl(\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2\Bigl)+\sum\limits_{i=1}^M\log|\Lambda_i|\Bigl\}$</p>

<p>which is used with a Gauss-Seidel algorithm to find the ML estimates of $\beta$ and $\lambda$. A less biased estimate of $\sigma^2$ is used:</p>

<p>$\sigma^2=\sum\limits_{i=1}^M\Bigl|\Bigl|\hat\Lambda_i^{-T/2}[y_i-f_i(\hat\beta)]\Bigl|\Bigl|^2/(N-p)$</p>

<p>where $p$ represents the length of $\beta$.</p>

<p>I have compiled a list of specific questions that I am facing:</p>

<ol>
<li>What is $\Lambda_i$? Is it the distance matrix produced by <code>big_lambda &lt;- vcv.phylo(tree)</code> in <code>ape</code>, or does it need to be somehow transformed or parameterized by $\lambda$, or something else entirely?</li>
<li>Would $\sigma^2$ be <code>fit$sigma^2</code>, or the equation for the less biased estimate (the last equation in this post)?</li>
<li>Is it necessary to use $\lambda$ to calculate log-likelihood, or is that just an intermediate step for parameter estimation? Also, how is $\lambda$ used? Is it a single value or a vector, and is it multiplied by all of $\Lambda_i$ or just off-diagonal elements, etc.?</li>
<li>What is $||y-f(\beta)||$? Would that be <code>norm(y-f(fit$coefficients,x),""F"")</code> in the package <code>Matrix</code>? If so, I'm confused about how to calculate the sum $\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2$, because <code>norm()</code> returns a single value, not a vector.</li>
<li>How does one calculate $\log|\Lambda_i|$? Is it <code>log(diag(abs(big_lambda)))</code> where <code>big_lambda</code> is $\Lambda_i$, or is it <code>logm(abs(big_lambda))</code> from the package <code>expm</code>? If it is <code>logm()</code>, how does one take the sum of a matrix (or is it implied that it is just the diagonal elements)?</li>
<li>Just to confirm, is $\Lambda_i^{-T/2}$ calculated like this: <code>t(solve(sqrtm(big_lambda)))</code>?</li>
<li>How are $y_i^*$ and $f_i^*(\beta)$ calculated? Is it either of the following:</li>
</ol>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) %*% y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) %*% f(fit$coefficients,x)</code></p>

<p>or would it be</p>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) * y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) * f(fit$coefficients,x)</code> ?</p>

<p>If all of these questions are answered, in theory, I think the log-likelihood should be calculable to match the output from <code>logLik(fit)</code>. Any help on any of these questions would be greatly appreciated. If anything needs clarification, please let me know. Thanks!</p>

<p><strong>UPDATE</strong>: I have been experimenting with various possibilities for the calculation of the log-likelihood, and here is the best I have come up with so far. <code>logLik_calc</code> is consistently about 1 to 3 off from the value returned by <code>logLik(fit)</code>. Either I'm close to the actual solution, or this is purely by coincidence. Any thoughts?</p>

<pre><code>  C &lt;- vcv.phylo(tree) # variance-covariance matrix
  tC &lt;- t(solve(sqrtm(C))) # C^(-T/2)
  log_C &lt;- log(diag(abs(C))) # log|C|
  N &lt;- length(y)
  y_star &lt;- tC%*%y 
  f_star &lt;- tC%*%f(fit$coefficients,x)
  dif &lt;- y_star-f_star  
  sigma_squared &lt;-  sum(abs(y_star-f_star)^2)/N
  # using fit$sigma^2 also produces a slightly different answer than logLik(fit)
  logLik_calc &lt;- -((N*log(2*pi*(sigma_squared)))+
       sum(((abs(dif)^2)/(sigma_squared))+log_C))/2
</code></pre>
"
"NaN","NaN"," 86432","<p>I'm using 'betareg' package in R to perform beta regression. predict() function with se.fit=T is supposed to return standard errors along with the prediction but it doesn't. Is there any other way I can get the standard error outputs?</p>

<p>I'm open to using other packages that can perform beta regression too.</p>
"
"NaN","NaN"," 86633","<p>I'm trying to figure a way of properly displaying the difference\resemblance between various regression values on the same data set, using cox ph, weibull regression and log-normal regression.</p>

<p>Weibull and log normal are similar in their result (predicted values) but cox ph returns the survival function, so I'm not sure how to compare them.</p>

<p>What would be a proper graph\visualization method for this?</p>

<p>Thanks!</p>
"
"0.0663812836584521","0.0771229887279699"," 86952","<p>I'm trying to regress some simple pooled data. My data has 60 observations and three columns: Weight, Height, and Sex (female=1, male=0).</p>

<p>If I regress thus, Weight ~ Height + Sex, my model is fairly satisfactory, but the residuals are not homoscedastic (green errors are male, blue female):</p>

<p><img src=""http://i1267.photobucket.com/albums/jj541/nbahmanyar/Rplot_zps69001b34.png"" alt=""plot""></p>

<p>I tried regressing on the log of Weight and/or Height, but that didn't do much. What should I do to make the residuals homescedastic and/or make my model more accurate? Any help would be appreciated.</p>

<p><strong>Edit</strong></p>

<p>Doing a generalized regression model gives the following.</p>

<pre><code>Generalized least squares fit by REML
  Model: Weight ~ h + s 
  Data: P149 
       AIC      BIC    logLik
  514.2221 524.4374 -252.1111

Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | Sex 
 Parameter estimates:
        0         1 
1.0000000 0.6685307 

Coefficients:
                 Value Std.Error   t-value p-value
(Intercept)  27.197499  51.88129  0.524226  0.6022
h             1.852382   0.75634  2.449128  0.0174
s           -25.284478   5.53300 -4.569755  0.0000

 Correlation: 
  (Intr) h     
h -0.997       
s -0.524  0.466

Standardized residuals:
       Min         Q1        Med         Q3        Max 
-1.6655243 -0.6879858 -0.1839396  0.5628971  3.9857544 

Residual standard error: 22.13369 
Degrees of freedom: 60 total; 57 residual
</code></pre>

<p>With this s. residual plot:</p>

<p><img src=""http://i1267.photobucket.com/albums/jj541/nbahmanyar/Rplot1_zps5ee264a0.png"" alt=""""></p>

<p>Could someone please explain how precisely this model is different from a standard multiple regression model? Thanks.</p>
"
"NaN","NaN"," 87345","<p>I have tried calculating the AIC of a linear regression in R but without using the <code>AIC</code> function, like this:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ drat, mtcars)

nrow(mtcars)*(log((sum(lm_mtcars$residuals^2)/nrow(mtcars))))+(length(lm_mtcars$coefficients)*2)
[1] 97.98786
</code></pre>

<p>However, <code>AIC</code> gives a different value:</p>

<pre><code>AIC(lm_mtcars)
[1] 190.7999
</code></pre>

<p>Could somebody tell me what I'm doing wrong?</p>
"
"0.125806749141278","0.127894008160743"," 87359","<p><strong>Background</strong></p>

<p>I have a large dataset that contains three binary outcomes for individuals belonging to groups. I am interested in jointly modeling these binary outcomes because I have reason to believe they are positively correlated with one another. Most of my data is at the individual level, however I also have some group-level information.</p>

<p>Because of the structure of my data, I am treating this as a 3-level logistic regression. The first level defines the multivariate structure through dummy variables that indicate for each outcome. Therefore level-1 accounts for the within-individual measurements. Level-2 provides the between-individuals variances and level-3 gives the between-group variance.</p>

<p><em>Hypothetical Example:</em></p>

<p>Suppose I have data for students in classrooms. I want to examine whether certain student level characteristics are important predictors for passing three different pre-tests (math, history, and gym). The pre-tests are constructed such that about half of the students should pass each exam (no floor or ceiling effect). Since some students are better than others, I expect whether or not they pass their history exam to be correlated to their probability of passing their math and gym pre-tests. I also expect that students in the same classroom will perform more similarly than students across classrooms.</p>

<p><strong>Here is my attempt at writing out the model</strong></p>

<p>I use $h$ to index level-1, $i$ to index level-2, and $j$ to index level-3. Recall that level-1 corresponds to within-individual, level-2 corresponds to between-individuals, and level-3 corresponds to between-groups. So I have $h$ measures for the $i^\text{th}$ individual in the $j^\text{th}$ group.</p>

<p>Let 
\begin{align}
\alpha_{1ij} &amp;= 1 \text{ if outcome}_1 = 1 \text{ and 0 otherwise} \\
\alpha_{2ij} &amp;= 1 \text{ if outcome}_2 = 1 \text{ and 0 otherwise} \\
\alpha_{3ij} &amp;= 1 \text{ if outcome}_3 = 1 \text{ and 0 otherwise}
\end{align}</p>

<p>\begin{align}
\text{log}\left( \frac{\pi_{hij}}{1 - \pi_{hij}} \right) &amp;= \alpha_{0hij} + \alpha_{1hij}Z_{ij} + \eta_h \\
Z_{ij} &amp;= \beta_{0ij} + X_{ij}\beta_{ij} + U_{j} + \epsilon_i \\
U_{j} &amp;= \gamma_{0j} + X_{j}\gamma_{j}
 + \rho_j
\end{align}</p>

<p>Please leave suggests about this notation; I am not positive that it is correct.</p>

<p><strong>Trying to specify the model in R</strong></p>

<p>I have been using R 3.0.1, but am open to solutions using other standard software (e.g. SAS, Stata, WinBUGs, etc.). Since I am modeling a binary response I am using the <code>glmer</code> function in the <code>lme4</code> package.</p>

<p>My data is in a long format with one row per outcome per individual per group.</p>

<p>One of my current problems is correctly specifying a 3-level model. Given 5 individual-level measures and one group level measure, I have tried to specify the model as:</p>

<pre><code>&gt; glmer(pi ~ outcome1:(x1 + x2 + x3 + x4 + u5) + 
             outcome2:(x1 + x2 + x3 + x4 + u5) +
             outcome3:(x1 + x2 + x3 + x4 + u5) +
             (1 + x1 + x2 + x3 + x4 | individual) +
             (1 + u5 | group), family=binomial, data=pretest)
</code></pre>

<p>but this often produces warnings and does not converge.</p>

<p><strong>My questions</strong></p>

<ol>
<li><p>Does my approach make sense? (i.e. does it make sense to model a multilevel multivariate model by specifying the multivariate structure in the first level?)</p></li>
<li><p>I have difficult with the proper notation and appreciate suggestions for clarifying my notation.</p></li>
<li><p>Am I using the right tools for this problem? Should I be using other packages or software?</p></li>
</ol>

<p>Thanks in advance for your suggestions and advice.</p>
"
"0.110590306208719","0.112425109314872"," 87487","<p><strong>Short version</strong></p>

<p>Is there a difference <strong>per treatment</strong> given time and this dataset?</p>

<p><strong>Or</strong> if the difference we're trying to demonstrate is important, what's the best method we have for teasing this out?</p>

<p><strong>Long version</strong></p>

<p>Ok, sorry if a bit <em>biology 101</em> but this appears to be an edge case where the data and the model need to line up in the right way in order to draw some conclusions. </p>

<p>Seems like a common issue... Would be nice to demonstrate an intuition rather than repeating this experiment with larger sample sizes. </p>

<p>Let's say I have this graph, showing mean +- std. error:</p>

<p><img src=""http://i.stack.imgur.com/eIKeF.png"" alt=""p1""></p>

<p>Now, it looks like there's a difference here. Can this be justified (avoiding Bayesian approaches)?</p>

<p>The simpleminded man's  approach would be to take Day 4 and apply a <em>t-test</em> (as usual: 2-sided, unpaired, unequal variance), but this doesn't work in this case. It appears the variance is too high as we only had 3x measurements per time-point (err.. mostly my design, p = 0.22).</p>

<p><strong>Edit</strong> On reflection the next obvious approach would be ANOVA on a linear regression. Overlooked this on first draft. This also doesn't seem like the right approach as the usual linear model is impaired from heteroskedasticity (<em>exaggerated variance over time</em>). <strong>End Edit</strong></p>

<p>I'm guessing there's a way to include <strong>all</strong> the data which would fit a simple (1-2 parameter) model of growth over time per predictor variable then compare these models using some formal test. </p>

<p>This method should be justifiable yet accessible to a relatively unsophisticated audience.</p>

<p>I have looked at <code>compareGrowthCurves</code> in <a href=""http://cran.r-project.org/web/packages/statmod/statmod.pdf"" rel=""nofollow"">statmod</a>, read about <a href=""http://www.jstatsoft.org/v33/i07/paper"" rel=""nofollow"">grofit</a> and tried a linear mixed-effects model adapted from <a href=""http://stats.stackexchange.com/questions/61153/nlme-regression-curve-comparison-in-r-anova-p-value"">this question on SE</a>. This latter is closest to the bill, although in my case the measurements are not from the <strong>same subject</strong> over time so I'm not sure mixed-effects/multilevel models are appropriate. </p>

<p>One sensible approach would be to model the rate of growth per time as linear and fixed and have the random effect be <strong>Tx</strong> then <a href=""http://www.statistik.uni-dortmund.de/useR-2008/slides/Scheipl+Greven+Kuechenhoff.pdf"" rel=""nofollow"">test it's significance</a>, although I gather there's <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">some debate</a> about the merits of such an approach.</p>

<p>(Also this method specifies a linear model which would not appear to be the best way to model a comparison of growth which in the case of one predictor has not yet hit an upper boundary and in the other appears basically static. I'm guessing there's a generalized mixed-effects model approach to this difficulty which would be more appropriate.)</p>

<p>Now the code:</p>

<pre><code>df1 &lt;- data.frame(Day = rep(rep(0:4, each=3), 2),
              Tx = rep(c(""Control"", ""BHB""), each=15),
              y = c(rep(16e3, 3),
              32e3, 56e3, 6e3,
              36e3, 14e3, 24e3,
              90e3, 22e3, 18e3,
              246e3, 38e3, 82e3,
              rep(16e3, 3),
              16e3, 34e3, 16e3,
              20e3, 20e3, 24e3,
              4e3, 12e3, 16e3,
              20e3, 5e3, 12e3))
### standard error
stdErr &lt;- function(x) sqrt(var(x)) / sqrt(length(x))
library(plyr)
### summarise as mean and standard error to allow for plotting
df2 &lt;- ddply(df1, c(""Day"", ""Tx""), summarise,
             m1 = mean(y),
             se = stdErr(y) )
library(ggplot2)
### plot with position dodge
pd &lt;- position_dodge(.1)
ggplot(df2, aes(x=Day, y=m1, color=Tx)) +
 geom_errorbar(aes(ymin=m1-se, ymax=m1+se), width=.1, position=pd) +
 geom_line(position=pd) +
 geom_point(position=pd, size=3) +
 ylab(""No. cells / ml"")
</code></pre>

<p>Some formal tests:</p>

<pre><code>### t-test day 4
with(df1[df1$Day==4, ], t.test(y ~ Tx))
### anova
anova(lm(y ~ Tx + Day, df1))
### mixed effects model
library(nlme)
f1 &lt;- lme(y ~ Day, random = ~1|Tx, data=df1[df1$Day!=0, ])
library(RLRsim)
exactRLRT(f1)
</code></pre>

<p>this last giving</p>

<pre><code>    simulated finite sample distribution of RLRT.  (p-value based on 10000
    simulated values)

data:  
RLRT = 1.6722, p-value = 0.0465
</code></pre>

<p>By which I conclude that the probability of this data (or something more extreme), <em>given the null hypothesis that there is no influence of <strong>treatment</strong> on <strong>change over time</em></strong> is close to the elusive 0.05. </p>

<p>Again, sorry if this appears a bit basic but I feel a case like this could be used to illustrate the importance of modelling in avoiding further needless experimental repetition. </p>
"
"0.0479808115123254","0.0487768608679754"," 87578","<p>I am estimating cross-sectional regressions - fragment:</p>

<blockquote>
  <p>lm(rate~liqamih.log+cap.log+F1+F2, data=x)</p>
</blockquote>

<p>of the R code listed below.</p>

<p>F1 and F2 are the coefficients estimates of time series model.</p>

<p>In such case we need to deal with so called ""error in variables problem"". In literature (links: gendocs.ru/docs/23/22031/conv_1/file1.pdfâ€Ž (page 1091) and papers.ssrn.com/sol3/papers.cfm?abstract_id=6992 (whole article)) the MLE method is one of the effective solution of this problem.</p>

<p>I would like to implement method introduced by KIM (reference to literature-links above) in my R code below. HOW TO DO THAT ?</p>

<pre><code>data&lt;-read.table(""reg5-dane.csv"", head=T, sep="";"", dec="","")
  data$indx &lt;- as.numeric(gl(123*334,334,123*334))
lst1 &lt;- split(data[,-53],data[,53]) #max 53 variables in ""lst2"" regression
any(sapply(lst1,nrow)!=123)
#[1] FALSE
lst2 &lt;- lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],
               function(x) summary(lm(rate~liqamih.log+cap.log+F1+F2, data=x)) )
capture.output(lst2,file=""nooldor_regr_summ.txt"")
# - f2 -f6 /+f2 -f5 +F6 - f6 - f2 - liq + f6 - f6 +f2 - f4 - f4 +f3

capture.output(lst2,file=""nooldor_regr_summ.csv"")
# HERE - above- IS THE CORE / most important part of question ... for now you can skip what is below 
f4 &lt;- function(meanmod, dta, varmod) {
  assign("".dta"", dta, envir=.GlobalEnv)
  assign("".meanmod"", meanmod, envir=.GlobalEnv)
  m1 &lt;- lm(.meanmod, .dta)
  ans &lt;- ncvTest(m1, varmod)
  remove("".dta"", envir=.GlobalEnv)
  remove("".meanmod"", envir=.GlobalEnv)
  ans
}
library(car)
lst3 &lt;- lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],
               function(x) f4(rate~cap.log, x))
lst4 &lt;- lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],
               function(x) durbinWatsonTest(lm(rate~cap.log, x)))
lst5 &lt;- lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],
               function(x) vif(lm(rate~cap.log, x)))

res5 &lt;- do.call(rbind,lst5)
res4 &lt;- do.call(rbind,lapply(lst4,function(x) unlist(x[-4])))
library(tseries)

res6 &lt;- do.call(rbind,lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],function(x) {resid &lt;- residuals(lm(rate~.,data=x)); unlist(jarque.bera.test(resid)[1:3])}) )

capture.output(lst2,file=""nooldor_regr_summ.txt"")
capture.output(lst3,file=""nooldor_arch_test.txt"")
capture.output(lst4,file=""nooldor_durbin.txt"")
capture.output(lst5,file=""nooldor_vif.txt"")
</code></pre>

<p>There is 123 time observations on 334 subjects observations. For each of 123 time points I am running one regressions with 334 subjects (so I am repeating it 123 times for each point of time).
I post this topic also in stackoverflow.com - because I need help from one who have strong background in statistics/econometrics and also in R programming.
I would appreciate your valuable help.
Thank you.</p>
"
"0.0929144419623766","0.0944559849109725"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.053644178078582","0.0545341883149212"," 87814","<p>Suppose I have a set of (discrete) variables, say $X_1,\dots,X_n$. Each $i$ belongs to either class A or class B. When it belongs to A the contribution is $Y_{A,i}f(X_i)$, and when it belongs to B the contribution is $Y_{B,i}g(X_i)$, so that the final outcome is $Z=\sum_{i=1}^N Y_{A,i}f(X_i) I_{\{i \in A\}} + Y_{B,i}g(X_i) I_{\{i \in B\}}$, where $I_{\{.\}}$ denotes the indicator-function.  </p>

<p>The values for $X_1,\dots,X_n$ are given, as are the functions $f$ and $g$. Now suppose I have $M$ obervations for $Z$, what method do you advice to</p>

<p>(i)  Identify whether $i$ is in $A$ or $B$</p>

<p>(ii) Estimate the values for $Y_{A,i}$ (if $i\in A)$ and $Y_{B,i}$ (if $i \in B$).</p>

<p>I tried to estimate the coefficients $Y_{A,i}$ and $Y_{B,i}$ using standard linear regression, assuming that $i\in A$ for all $i$ or $i \in B$ for all $i$, but that gives bad results.</p>

<p>(iii) What if I have $M$ different inputs $X_{j,1},\dots,X_{j,N}$, for $j=1,\dots,M$ and one observation for $Z$ for each input?</p>

<p>Edit:</p>

<p>As an example, take the following R-code:</p>

<pre><code>N=50
M=10
X=rbinom(N*M,1,0.5)-rbinom(N*M,1,0.5) #Generate -1, 0, 1 variables
dim(X) &lt;- c(N,M)
pA=1/2 #pB=1-pA
XA = X #f(x)=x
XB = -X^2 + 1 #g(x)=-x^2+1
isA = as.logical(rbinom(M,1,pA))
YA = rnorm(M,1,5) #generate coefficients
YB = rnorm(M,1,5) #generate coefficients
Z = XA[,isA] %*% YA[isA] + XB[,!isA] %*% YB[!isA] #Calculate Z without noise
Z = Z+rnorm(N,sd=sqrt(var(Z))) #add noise 
</code></pre>

<p>Thus given the observations <code>Z</code> and the input data <code>XA, XB</code> (these are known) I want to estimate <code>isA, YA[isA]</code> and <code>YB[!isA]</code>.</p>
"
"0.0599760143904067","0.0609710760849692"," 87903","<p>I am fitting a GLM to binomial data with two independent variables, <code>a</code> and <code>b</code>. I have tried to fit a binomial GLM with the form <code>y~a+b+a*b</code>, so that I have the contributions of the two variables as well as an interaction term. This works OK, but in the data the effect of <code>b</code> is very gradual in comparison to <code>a</code>. The best fit I've tried by far is simply <code>y~a+log(b+1)+a*log(b+1)</code> (the +1 is because b=0 for some datapoints) with a binomial link function. Is it possible to use this as a GLM? It's just that I've not seen this done when I've searched online so I don't know if there's some good reason that people only seem to use polynomials. There is a question that seems related in a previous post <a href=""https://stats.stackexchange.com/questions/78633/interpret-interactions-and-logarithms-in-linear-regression"">Interpret interactions and logarithms in linear regression</a>, but this was using a log-log function, and used <code>lm()</code> instead of <code>glm()</code>; so I'm not at all confident of the relation. </p>
"
"0.0599760143904067","0.0609710760849692"," 87946","<p>I have lots of time series with periods: day, week or month. With <code>stl()</code> function or with <code>loess(x ~ y)</code> I can see how trends of particular time series look. I need to detect if trend of time series is increasing or decreasing. How can I manage that?</p>

<p>I tried to compute linear regression coefficients with <code>lm(x ~ y)</code> and play with slope coefficient. (<code>If |slope|&gt;2 and slope&gt;0 then</code> increasing trend, <code>else if |slope|&gt;2 and slope&lt;0</code> â€“ decreasing).
Maybe there is another and more effective method for trend detection? Thank you!</p>

<p>For example: I have <code>timeserie1</code>, <code>timeserie2</code>. I need a simple algorithm that would tell me that <code>timeserie2</code> is an increasing algorithm, and in <code>timeserie1</code>, the trend isn't increasing or decreasing. What criteria should I use?</p>

<p><code>timeserie1</code>:</p>

<pre><code>1774 1706 1288 1276 2350 1821 1712 1654 1680 1451 1275 2140 1747 1749 1770 1797 1485 1299 2330 1822
1627 1847 1797 1452 1328 2363 1998 1864 2088 2084  594  884 1968 1858 1640 1823 1938 1490 1312 2312
1937 1617 1643 1468 1381 1276 2228 1756 1465 1716 1601 1340 1192 2231 1768 1623 1444 1575 1375 1267
2475 1630 1505 1810 1601 1123 1324 2245 1844 1613 1710 1546 1290 1366 2427 1783 1588 1505 1398 1226
1321 2299 1047 1735 1633 1508 1323 1317 2323 1826 1615 1750 1572 1273 1365 2373 2074 1809 1889 1521
1314 1512 2462 1836 1750 1808 1585 1387 1428 2176 1732 1752 1665 1425 1028 1194 2159 1840 1684 1711
1653 1360 1422 2328 1798 1723 1827 1499 1289 1476 2219 1824 1606 1627 1459 1324 1354 2150 1728 1743
1697 1511 1285 1426 2076 1792 1519 1478 1191 1122 1241 2105 1818 1599 1663 1319 1219 1452 2091 1771
1710 2000 1518 1479 1586 1848 2113 1648 1542 1220 1299 1452 2290 1944 1701 1709 1462 1312 1365 2326
1971 1709 1700 1687 1493 1523 2382 1938 1658 1713 1525 1413 1363 2349 1923 1726 1862 1686 1534 1280
2233 1733 1520 1537 1569 1367 1129 2024 1645 1510 1469 1533 1281 1212 2099 1769 1684 1842 1654 1369
1353 2415 1948 1841 1928 1790 1547 1465 2260 1895 1700 1838 1614 1528 1268 2192 1705 1494 1697 1588
1324 1193 2049 1672 1801 1487 1319 1289 1302 2316 1945 1771 2027 2053 1639 1372 2198 1692 1546 1809
1787 1360 1182 2157 1690 1494 1731 1633 1299 1291 2164 1667 1535 1822 1813 1510 1396 2308 2110 2128
2316 2249 1789 1886 2463 2257 2212 2608 2284 2034 1996 2686 2459 2340 2383 2507 2304 2740 1869  654
1068 1720 1904 1666 1877 2100  504 1482 1686 1707 1306 1417 2135 1787 1675 1934 1931 1456 1363 2027
1740 1544 1727 1620 1232 1199
</code></pre>

<p><code>timeserie2</code>:</p>

<pre><code> 122  155  124   97  155  134  115  122  162  115  102  163  135  120  139  160  126  122  169  154
 121  134  143  100  121  182  139  145  135  147   60   58  153  145  130  126  143  129   98  171
 145  107  133  115  113   96  175  128  106  117  124  107  114  172  143  111  104  132  110   80
 159  131  113  123  123  104  101  179  127  105  133  127  101   97  164  134  124   90  110  102
  90  186   79  145  130  115   79  104  191  137  114  131  109   95  119  173  158  137  128  119
 109  120  182  140  133  113  121  110  122  159  129  124  119  109  108   95  167  138  125  105
 139  118  115  166  140  112  116  139  121  109  164  135  118  121  112  111  102  169  136  151
 132  135  130  112  156  134  121  116  114   91   86  141  160  116  118  112   84  114  165  141
 109  123  122  110  100  162  145  121  118  115  107  103  162  142  130  139  134  121  118  164
 147  125  120  134  107  130  158  141  144  148  124  135  118  212  178  154  167  155  176  143
 201  170  144  138  152  136  123  223  189  160  153  190  136  144  276  213  199  211  196  170
 179  460  480  499  550  518  493  557  768  685  637  593  507  611  569  741  635  563  577  498
 456  446  677  552  515  441  438  462  530  699  629  555  641  625  544  585  705  584  553  622
 506  500  533  777  598  541  532  513  434  510  714  631 1087 1249 1102  913  888 1147 1056 1073
1075 1136  927  922 1066 1074  996 1189 1062  999  974 1174 1097 1055 1053 1097 1065 1171  843  441
 552  779  883  773  759  890  404  729  703  810  743  743  946  883  813  876  841  742  715  960
 862  743  806  732  669  621
</code></pre>
"
"0.026822089039291","0.0272670941574606"," 88388","<p>I would like to use cross-validation to test how predictive my mixed-effect logistic regression model is (model run with glmer). Is there an easy way to do this using a package in R? I've only seen cross validation functions in R for use with linear models.</p>
"
"0.0599760143904067","0.0609710760849692"," 88799","<p>Is there anyway to give weights to a gamma regression so that the variance is allowed to be different, dependent on a particular parameter(in this case sampling event)?</p>

<p>When I try to run my code :</p>

<pre><code>Nm5&lt;-glm(myformula,family=Gamma,data=master_mr,weights=varIdent(form=~1| fevent))
</code></pre>

<p>I get this error:</p>

<pre><code>Error in model.frame.default(formula = myformula)
variable lengths differ (found for '(weights)'
</code></pre>

<p>When I run the following code (same thing only with gls), it runs fine, except I am now not using a Gamma distribution.</p>

<pre><code>Nm2&lt;-gls(myformula,weights=varIdent(form=~1| fevent),data=master_mr)
</code></pre>

<p>You can use the weights function with glm, but not in conjunction with Gamma.  I found an <a href=""https://www-m4.ma.tum.de/fileadmin/w00bdb/www/czado/lec8.pdf"" rel=""nofollow"">example</a> (slide 30ish) where it is done, but the code doesn't make sense to me and also doesn't work the way I would expect. This leads me to believe that it can be done. Any ideas?</p>
"
"0.0709645772411954","0.072141950116023"," 88880","<p>I performed principal component analysis (PCA) with R using two different functions (<code>prcomp</code> and <code>princomp</code>) and observed that the PCA scores differed in sign. How can it be?</p>

<p>Consider this:</p>

<pre><code>set.seed(999)
prcomp(data.frame(1:10,rnorm(10)))$x

            PC1        PC2
 [1,] -4.508620 -0.2567655
 [2,] -3.373772 -1.1369417
 [3,] -2.679669  1.0903445
 [4,] -1.615837  0.7108631
 [5,] -0.548879  0.3093389
 [6,]  0.481756  0.1639112
 [7,]  1.656178 -0.9952875
 [8,]  2.560345 -0.2490548
 [9,]  3.508442  0.1874520
[10,]  4.520055  0.1761397

set.seed(999)
princomp(data.frame(1:10,rnorm(10)))$scores
         Comp.1     Comp.2
 [1,]  4.508620  0.2567655
 [2,]  3.373772  1.1369417
 [3,]  2.679669 -1.0903445
 [4,]  1.615837 -0.7108631
 [5,]  0.548879 -0.3093389
 [6,] -0.481756 -0.1639112
 [7,] -1.656178  0.9952875
 [8,] -2.560345  0.2490548
 [9,] -3.508442 -0.1874520
[10,] -4.520055 -0.1761397
</code></pre>

<p>Why do the signs (<code>+/-</code>) differ for the two analyses? If I was then using principal components <code>PC1</code> and <code>PC2</code> as predictors in a regression, i.e. <code>lm(y ~ PC1 + PC2)</code>, this would completely change my understanding of the effect of the two variables on <code>y</code> depending on which method I used! How could I then say that <code>PC1</code> has e.g. a positive effect on <code>y</code> and <code>PC2</code> has e.g. a negative effect on <code>y</code>?</p>

<hr>

<p><strong>In addition:</strong> If the sign of PCA components is meaningless, is this true for factor analysis (FA) as well? Is it acceptable to flip (reverse) the sign of individual PCA/FA component scores (or of loadings, as a column of loading matrix)?</p>
"
"0.026822089039291","0.0272670941574606"," 89033","<p>I have fitted a zero-inflated model with a random effect using a negative binomial distribution in R, using the function glmmadmb. This is due to a large number of zeros and over dispersion. </p>

<p>For a standard poisson regression I know that one must test collinearity, leverage and stability of coefficients (DF Beta). I can do all this in R using various functions for a poisson regression, but none exist for glmmadmb that I can find. I wondered if anyone knew a way to test these? </p>

<p>Thanks</p>
"
"0.141929154482391","0.133977907358328"," 89204","<p>I'm looking for advice on how to analyze complex survey data with multilevel models in R. I've used the <code>survey</code> package to weight for unequal probabilities of selection in one-level models, but this package does not have functions for multilevel modeling. The <code>lme4</code> package is great for multilevel modeling, but there is not a way that I know to include weights at different levels of clustering. <a href=""http://www.statmodel.com/download/asparouhovgmms.pdf"">Asparouhov (2006)</a> sets up the problem:</p>

<blockquote>
  <p>Multilevel models are frequently used to analyze data from cluster sampling designs. Such sampling designs however often use unequal probability of selection at the cluster level and at the individual level. Sampling weights are assigned at one or both levels to reflect these probabilities. If the sampling weights are ignored at either level the parameter estimates can be substantially biased.</p>
</blockquote>

<p>One approach for two-level models is the multilevel pseudo maximum likelihood (MPML) estimator that is implemented in MPLUS (<a href=""http://www.statmodel.com/download/SurveyJSM1.pdf"">Asparouhov et al, ?</a>). <a href=""http://www.biomedcentral.com/1471-2288/9/49"">Carle (2009)</a> reviews major software packages and makes a few recommendations about how to proceed: </p>

<blockquote>
  <p>To properly conduct MLM with complex survey data and design weights, analysts need software that can include weights scaled outside of the program and include the ""new"" scaled weights without automatic program modification. Currently, three of the major MLM software programs allow this: Mplus (5.2), MLwiN (2.02), and GLLAMM. Unfortunately, neither HLM nor SAS can do this.</p>
</blockquote>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3630376/"">West and Galecki (2013)</a> give a more updated review, and I'll quote the relevant passage at length:</p>

<blockquote>
  <p>Occasionally, analysts wish to fit LMMs to survey data sets collected from samples with complex designs (see Heeringa et al, 2010, Chapter 12). Complex sample designs are generally characterized by division of the population into strata, multi-stage selection of clusters of individuals from within the strata, and unequal probabilities of selection for both clusters and the ultimate individuals sampled. These unequal probabilities of selection generally lead to the construction of sampling weights for individuals, which ensure unbiased estimation of descriptive parameters when incorporated into an analysis. These weights might be further adjusted for survey nonresponse and calibrated to known population totals. Traditionally, analysts might consider a design-based approach to incorporating these complex sampling features when estimating regression models (Heeringa et al., 2010). More recently, statisticians have started to explore model-based approaches to analyzing these data, using LMMs to incorporate fixed effects of sampling strata and random effects of sampled clusters.</p>
  
  <p>The primary difficulty with the development of model-based approaches to analyzing these data has been choosing appropriate methods for incorporating the sampling weights (see Gelman, 2007 for a summary of the issues). Pfeffermann et al. (1998), Asparouhov and Muthen (2006), and Rabe-Hesketh and Skrondal (2006) have developed theory for estimating multilevel models in a way that incorporates the survey weights, and Rabe-Hesketh and Skrondal (2006), Carle (2009) and Heeringa et al. (2010, Chapter 12) have presented applications using current software procedures, but this continues to be an active area of statistical research. Software procedures capable of fitting LMMs are at various stages of implementing the approaches that have been proposed in the literature thus far for incorporating complex design features, and analysts need to consider this when fitting LMMs to complex sample survey data. Analysts interested in fitting LMMs to data collected from complex sample surveys will be attracted to procedures that are capable of correctly incorporating the survey weights into the estimation procedures (HLM, MLwiN, Mplus, xtmixed, and gllamm), consistent with the present literature in this area.</p>
</blockquote>

<p>This brings me to my question: does anyone have best practice recommendations for fitting LMMs to complex survey data in R?</p>
"
"NaN","NaN"," 89749","<p>I was wondering if someone knows a R-package or function library for the topic of shrinkage for regression with ARMA errors.</p>

<p>Please let me know if you came across something related.</p>

<p>Thank you!</p>

<p>Regards,<br>
Patricia Tencaliec</p>
"
"0.0379321620905441","0.0385614943639849"," 89874","<p>I have a hypothetical data given below that consists of 11 pairs of points (xi, yi ), to which the simple linear regression mean function $\mathbb E(y|x) = Î²_0 + Î²_1x$ is fit.:</p>

<pre><code> X     Y
 10    8.04
  8    6.95
 13    7.58
  9    8.81
 11    8.33
 14    9.96
  6    7.24
  4    4.26
 12    10.84
  7    4.82
  5    5.68
</code></pre>

<p>I have got intercept parameter,$\beta_0=3.001$</p>

<p>But the plot of the data is not showing the y-intercept is $3.001$. Rather the y-intercept is more than $3.001$. <strong>WHY?</strong></p>

<p><img src=""http://i.stack.imgur.com/HtTTU.png"" alt=""enter image description here""></p>

<p>I have used <code>R software</code> to calculate the parameters, $\beta_0$,$\beta_1$ and also to produce the plot.</p>

<pre><code> x1 &lt;- c(10,8,13,9,11,14,6,4,12,7,5)
 y1 &lt;- c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68)

 lm(y1~x1)

 plot(y1~x1)
 abline(lm(y1~x1))
</code></pre>

<p><strong>EDIT</strong></p>

<pre><code>  ht &lt;- c(169.6,166.8,157.1,181.1,158.4,165.6,166.7,156.5,168.1,165.3)
  wt &lt;- c(71.2,58.2,56.0,64.5,53.0,52.4,56.8,49.2,55.6,77.8)

  lm(wt~ht)

  windows(9,6)
  par(mfrow=c(1,2))

  plot(wt~ht)
  abline(lm(wt~ht))

  plot(wt~ht,xlim=c(0,180),ylim=c(0,75))
  abline(lm(wt~ht))
</code></pre>

<p><img src=""http://i.stack.imgur.com/i1bqw.png"" alt=""enter image description here""></p>

<p>How can i get the y-intercept? By expanding the straight line(population regression line) to negative axis of Y ?</p>
"
"NaN","NaN"," 90402","<p>I'd like to know what technique or techniques are used by regression packages, (in particular the <code>lm</code> function of R) to minimize the sum of squares. </p>

<p>Does it use gradient descent?</p>

<p>Thanking in anticipation.</p>
"
"0.0967084173462244","0.0983129061176287"," 90799","<p>I am having trouble training a model for nested data about house prices. Lets say my data looks like following:</p>

<pre><code>  logPrice bedCount bathCount                city
 0.6517920        4       2-3        Redwood City
 0.4402192        1       1-2 South San Francisco
 0.5922396        2       1-2           San Mateo
 0.4606918        3       1-2 South San Francisco
 0.7592523    5plus       3-4           San Mateo
 0.4710397        1       1-2        Redwood City
</code></pre>

<p><code>bedCount</code>, <code>bathCount</code> and <code>city</code> are factors.</p>

<p>As a baseline, I trained a simple linear model ignoreing nested structure of the data (houses are nested within cities).</p>

<pre><code>lm.model = lm(formula = logPrice ~ 1 + bedCount + bathCount + city)
</code></pre>

<p>which corresponds to following assumption:</p>

<p><code>logPrice</code>$_i = \beta_0 + \beta_1\cdot$ <code>bedCount</code>$_i + \beta_2\cdot$ <code>bathCount</code>$_i + \beta_{3,j[i]}\cdot I$(<code>city</code>$_{j[i]}) + \epsilon_i$</p>

<p>where </p>

<p>$\epsilon_i \sim N(0, \sigma^2_{logPrice})$ and $I$(<code>city</code>$_{j[i]}$) is the indicator function for city of the $i^{th}$ house (which is 1).</p>

<p>Now, I trained a 2-level hierarchical model:</p>

<pre><code>lmer.model = lmer(formula = logPrice ~ 1 + bedCount + bathCount + (1 | city))
</code></pre>

<p>which corresponds to the following assumption:</p>

<p><code>logPrice</code>$_i = \beta_0 + \beta_1\cdot$ <code>bedCount</code>$_i + \beta_2\cdot$ <code>bathCount</code>$_i + \beta_{3,j[i]}\cdot I$(<code>city</code>$_{j[i]}) + \epsilon_i$</p>

<p>where
$\epsilon_i \sim N(0, \sigma^2_{logPrice})$ and $\beta_{3,j} \sim N(0, \sigma^2_{\beta_3})$</p>

<p>Now, on the training data, <code>lm.model</code> gives me lesser average RMSE than <code>lmer.model</code> which shouldn't happen because linear regression is a special case of multilevel linear regression (I didn't care to check average RMSE on test data because that on training data itself should be lower for 2nd model than that for 1st model). In fact, my data has multiple levels (houses nested within subdivisions, which are nested within zipcodes, which in turn are nested within cities) and the performance gets worse as I add more and more levels to the model (i.e. model with random effect <code>(1 | subdivision)</code> does worse than that with random effect <code>(1 | zipcode) + (1 | zipcode:subdivision)</code>, which in turn does worse than a model with random effect <code>(1 | city) + (1 | city:zipcode) + (1 | city:zipcode:subdivision)</code>).</p>

<p>What am I missing?</p>
"
"0.0969560705490221","0.0985646681332268"," 91243","<p>I have a large data frame in the following form (I apologize for this formatting):</p>

<pre><code>Site    Season  T          SC    pH    Chl   DO.S   DO      BGA  Tur    fDOM    Flow    Rainfall    Solar      Rain
300N    Winter  14.05   1692.77 7.93    NA  82.26   8.42    NA  9.25    NA      NA      0.00          219.18     no
</code></pre>

<p>If you can't understand the formatting, there are 12 numerical factors, and 3 categorical factors (<code>Site</code>, <code>Season</code>, <code>Rain</code> [yes/no]). Each row represents the average daily values that I have calculated from 15-minute time series. I have spent a good amount of time doing data exploration (linear regression analysis, looking at time series plots for patterns), but haven't found a method that works for me yet. I have also worked with <code>corrplot</code>, correlation matrices, and covariance functions in an arduous way, where I subset each categorical combination and found <code>corrplot</code>s for each (I have also tried it with <code>ddply</code>, but the resulting format is not in the correlation matrix format that is easy to plot). I have also attempted PCA on the data to little avail.</p>

<p>My question is first and foremost, does anyone have an idea for data visualization of this kind of dataset? The main question I am after is, ""What are the factors that influence <code>DO</code> (dissolved oxygen)?"". How does this change by location (<code>Site</code>), <code>Season</code>, and with the influence of <code>Rain</code>. I would really like a quick method for shooting out correlation matrices (or heat maps; I have tried both) for each categorical subset. I tried this with <code>ggplot</code> and <code>facet_wrap</code>, but it wasn't happening for me. I also tried <code>ggpairs</code> from the GGally package, but honestly didn't spend too much time with that method.</p>

<p>I was starting to get into the idea of star graphs (on polar coordinates), which can be used to visualize repeating periodicity in time series, but am running out of time and decided to seek the advisement of Stack Overflow. I really appreciate any advice or thoughts on visualizing this data that come to your mind. I feel like some combination of <code>ddply</code> and graphing is what I need, but I haven't gotten there yet.
Thank you for your time.</p>

<p>EDIT:
<code>dput</code> of the data frame in question:</p>

<pre><code>structure(list(Site = structure(c(2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""2100S"", 
""300N"", ""3300S"", ""800S"", ""Burnham"", ""Center""), class = ""factor""), 
    Season = structure(c(4L, 4L, 4L, 4L, 2L, 2L), .Label = c(""Fall"", 
    ""Spring"", ""Summer"", ""Winter""), class = ""factor""), T = c(14.05, 
    14.18, 14.5, 14.58, 14.07, 11.91), SC = c(1692.77, 1671.31, 
    1680.71, 1661.79, 1549.56, 1039.63), pH = c(7.93, 7.92, 7.96, 
    7.95, 7.93, 7.79), Chl = c(NA_real_, NA_real_, NA_real_, 
    NA_real_, NA_real_, NA_real_), DO.S = c(82.26, 78.79, 82.05, 
    80.92, 74.33, 73.96), DO = c(8.42, 8.04, 8.31, 8.18, 7.61, 
    7.97), BGA = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, 
    NA_real_), Tur = c(9.25, 9.77, 9.41, 10.6, 40.38, 50.25), 
    fDOM = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, 
    NA_real_), Flow = c(NA, 178.08, 178.53, 188.13, 306.15, 382.22
    ), Rainfall = c(0, 0, 0, 0, 0.01, 0.81), Solar = c(219.18, 
    228.33, 244.3, 247.69, 105.15, 220.73), Rain = structure(c(1L, 
    1L, 1L, 1L, 2L, 2L), .Label = c(""no"", ""yes""), class = ""factor"")), .Names = c(""Site"", 
""Season"", ""T"", ""SC"", ""pH"", ""Chl"", ""DO.S"", ""DO"", ""BGA"", ""Tur"", 
""fDOM"", ""Flow"", ""Rainfall"", ""Solar"", ""Rain""), row.names = c(NA, 
6L), class = ""data.frame"")
</code></pre>
"
"0.026822089039291","0.0272670941574606"," 91479","<p>I'm using R to compute robust multiple linear regression.
I use the command <code>rlm</code> from the package MASS.</p>

<p>As psi function I use <code>psi.huber</code> or <code>psi.bisquare</code>.</p>

<p>Is there a way to get an estimator of the goodness of fit of the model? Maybe something comparable to the Adjusted R-squared, for the parametric multiple linear regression?</p>

<p>Moreover, am I right saying that this kind of robust regression doesn't solve the problem of non-normality of the dependent or independent variables?</p>
"
"0.080466267117873","0.0818012824723818"," 91745","<p>Please, i need to do a network meta-analysis and metaregression or a multivariate meta analysis and metaregression. I have multi arm randomized controlled studies.
I tried the metafor package</p>

<h1>Dummy database with controlled multiarm treatment studies</h1>

<pre><code>Study = c(1,2,3,1:2,4:10,1,2,7)
Group1 = as.factor(c(rep(1,3),rep(2,9),rep(3,3)))
numberInt= c(rep(30,5),rep(28,7),rep(40,3))
InterventionMean1  = c(rnorm(3, mean = 350, sd = 2), rnorm(9, mean = 540, sd = 2),rnorm(3, mean = 860, sd = 2))
InterventionSD  = rnorm(15, mean = 80, sd = 15)
numberCont= c(rep(29,4), 28,rep(30,7),rep(42,3))
ControlMean  = rnorm(15, mean = 230, sd = 2)
ControlSD  = rnorm(15, mean = 55, sd = 9)
Dose = c(40,50,40,100,100,100,100,100,100,100,100,100,200,200,200)
x=as.factor(c(1,2,1,rep(1,5),rep(2,3),rep(1,4)))
data = data.frame(cbind(Study,Group, numberInt,InterventionMean1,InterventionSD,
             numberCont, ControlMean, ControlSD,Dose,x))
</code></pre>

<h1>x and Dose are covariates</h1>

<h1>Loading libraries</h1>

<pre><code>library(metafor)
library(Matrix)
</code></pre>

<h1>Effect size : mean difference</h1>

<p>dataEscalc = escalc(measure = ""MD"", m1i = InterventionMean1, sd1i = InterventionSD, n1i = numberInt,
                    m2i = ControlMean, sd2i = ControlSD, n2i = numberCont,
                    data=data)</p>

<h1>Number of patients per studies</h1>

<pre><code>dataEscalc$Ni &lt;- unlist(lapply(split(data, data$Study), function(x) rep(sum(x$numberInt) + x$numberCont[1], each=nrow(x))))
</code></pre>

<h1>Variance-covariance matrix</h1>

<pre><code>    calc.v &lt;- function(x) {
    v &lt;- matrix(vi/x$numberCont[1] + outer(x$yi, x$yi, ""*"")/(2*x$Ni[1]), nrow=nrow(x), ncol=nrow(x))
      diag(v) &lt;- x$vi
          v
          }
    V &lt;- lapply(split(dataEscalc, dataEscalc$Study), calc.v)
V &lt;- as.matrix(bdiag(V))
V
</code></pre>

<h1>Conducting multivariate meta anlaysis</h1>

<p>names(dataEscalc)</p>

<pre><code>res &lt;- rma.mv(yi, V, mods = ~ factor(Group1)  - 1,
              random = ~ factor(Group1) | Study,
              data=dataEscalc, method=""REML"")
</code></pre>

<blockquote>
  <p>I get this error message</p>
  
  <p>Error in rma.mv(yi, V, mods = ~factor(Group1) - 1, random =
  ~factor(Group1) |  : 
                    Error during optimization.</p>
</blockquote>

<h1>Conducting multivariate metaregression</h1>

<pre><code>res &lt;- rma.mv(yi, V, mods = ~ factor(Group1) + Group1:I(Dose) + Group1:I(x) - 1,
              random = ~ factor(Group1) | Study,
              data=dataEscalc, method=""REML"")
</code></pre>

<blockquote>
  <p>I get this error message</p>
  
  <p>Error in rma.mv(yi, V, mods = ~factor(Group1) + Group1:I(Dose) +
  Group1:I(x) -  : 
                    Model matrix not of full rank. Cannot fit model.</p>
</blockquote>

<p>I followed the instructions from 
<a href=""http://www.metafor-project.org/doku.php/analyses%3agleser2009"" rel=""nofollow"">http://www.metafor-project.org/doku.php/analyses:gleser2009</a>
http://www.metafor-project.org/doku.php/analyses:vanhouwelingen2002</p>

<p>Thanks in advance</p>
"
"0.0715255707714427","0.0727122510865616"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"0.0848188929679971","0.0862261227118454"," 92132","<p>Let's say I have a function to simulate data for negative binomial regression: </p>

<pre><code>simnegbin = function(X, beta, alpha) {
lambda = exp(1 + X %*% beta)
y=NULL
for (j in 1:length(lambda)) y = c(y,rnbinom(1,mu = lambda[j],size = alpha))
return(y)
}
</code></pre>

<p>And I call that function like this:</p>

<pre><code>set.seed(123)
x1 = rnorm(100)
X = matrix(c(x1,x1^2), ncol = 2) # data with quadratic effect
beta = c(0.5,-0.5) # coefficients one with negative sign
alpha = 1
y = simnegbin(X, beta, alpha)
</code></pre>

<p>And then I fit two models to this simulated data. </p>

<pre><code>NB = MASS::glm.nb(y ~ X)
Normal = glm(y ~ X)
</code></pre>

<p>And look at the resulting coefficients. Nothing too strange. The model assuming a normal distribution is slightly higher, but not too bad. </p>

<pre><code>coef(NB)
coef(Normal)
</code></pre>

<p>Normal:</p>

<pre><code>(Intercept)      X1          X2 
  2.3997424   0.4019599  -0.5234614 
</code></pre>

<p>NB: </p>

<pre><code>(Intercept)      X1          X2 
  0.8992080   0.3573460  -0.4023558
</code></pre>

<p>But then when I simply switch the sign to positive of the coefficient in a second call to the function:</p>

<pre><code>X = matrix(c(x1,x1^2), ncol = 2)
beta = c(0.5,0.5) # coefficients with positive sign
alpha = 1
y = simnegbin(X, beta, alpha)
</code></pre>

<p>And then I fit two models using the same simulated data. </p>

<pre><code>NB = MASS::glm.nb(y ~ X)
Normal = glm(y ~ X)

coef(Normal)
coef(NB)
</code></pre>

<p>Normal: </p>

<pre><code>(Intercept)      X1          X2 
    1.342937    3.162534    4.130669 # Biased term 
</code></pre>

<p>NB:</p>

<pre><code>(Intercept)     X1          X2 
0.9097782   0.2905278   0.4587727
</code></pre>

<p>The quadratic coefficient of the glm assuming the normal distribution is <strong>super biased</strong>. </p>

<p>Does anyone have any intuition about why this would be? </p>
"
"0.0938773116375185","0.109068376629842"," 92150","<p>Actually, I thought I had understood what one can show a with partial dependence plot, but using a very simple hypothetical example, I got rather puzzled. In the following chunk of code I generate three independent variables (<em>a</em>, <em>b</em>, <em>c</em>) and one dependent variable (<em>y</em>) with <em>c</em> showing a close linear relationship with <em>y</em>, while <em>a</em> and <em>b</em> are uncorrelated with <em>y</em>. I make a regression analysis with a boosted regression tree using the R package <code>gbm</code>:</p>

<pre><code>a &lt;- runif(100, 1, 100)
b &lt;- runif(100, 1, 100)
c &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
y &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
par(mfrow = c(2,2))
plot(y ~ a); plot(y ~ b); plot(y ~ c)
Data &lt;- data.frame(matrix(c(y, a, b, c), ncol = 4))
names(Data) &lt;- c(""y"", ""a"", ""b"", ""c"")
library(gbm)
gbm.gaus &lt;- gbm(y ~ a + b + c, data = Data, distribution = ""gaussian"")
par(mfrow = c(2,2))
plot(gbm.gaus, i.var = 1)
plot(gbm.gaus, i.var = 2)
plot(gbm.gaus, i.var = 3)
</code></pre>

<p>Not surprisingly, for variables <em>a</em> and <em>b</em> the partial dependence plots yield horizontal lines around the mean of <em>a</em>. What me puzzles is the plot for variable <em>c</em>. I get horizontal lines for the ranges <em>c</em> &lt; 40 and <em>c</em> > 60 and the y-axis is restricted to values close to the mean of <em>y</em>. Since <em>a</em> and <em>b</em> are completely unrelated to <em>y</em> (and thus there variable importance in the model is 0), I expected that <em>c</em> would show partial dependence along its entire range instead of that sigmoid shape for a very restricted range of its values. I tried to find information in Friedman (2001) ""Greedy function approximation: a gradient boosting machine"" and in Hastie et al. (2011) ""Elements of Statistical Learning"", but my mathematical skills are too low to understand all the equations and formulae therein. Thus my question: What determines the shape of the partial dependence plot for variable <em>c</em>? (Please explain in words comprehensible to a non-mathematician!)     </p>

<p>ADDED on 17th April 2014:</p>

<p>While waiting for a response, I used the same example data for an analysis with R-package <code>randomForest</code>. The partial dependence plots of randomForest resemble much more to what I expected from the gbm plots: the partial dependence of explanatory variables <em>a</em> and <em>b</em> vary randomly and closely around 50, while explanatory variable <em>c</em> shows partial dependence over its entire range (and over almost the entire range of <em>y</em>). What could be the reasons for these different shapes of the partial dependence plots in <code>gbm</code> and <code>randomForest</code>?</p>

<p><img src=""http://i.stack.imgur.com/PrlC1.jpg"" alt=""partial plots of gbm and randomForest""></p>

<p>Here the modified code that compares the plots:</p>

<pre><code>a &lt;- runif(100, 1, 100)
b &lt;- runif(100, 1, 100)
c &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
y &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
par(mfrow = c(2,2))
plot(y ~ a); plot(y ~ b); plot(y ~ c)
Data &lt;- data.frame(matrix(c(y, a, b, c), ncol = 4))
names(Data) &lt;- c(""y"", ""a"", ""b"", ""c"")

library(gbm)
gbm.gaus &lt;- gbm(y ~ a + b + c, data = Data, distribution = ""gaussian"")

library(randomForest)
rf.model &lt;- randomForest(y ~ a + b + c, data = Data)

x11(height = 8, width = 5)
par(mfrow = c(3,2))
par(oma = c(1,1,4,1))
plot(gbm.gaus, i.var = 1)
partialPlot(rf.model, Data[,2:4], x.var = ""a"")
plot(gbm.gaus, i.var = 2)
partialPlot(rf.model, Data[,2:4], x.var = ""b"")
plot(gbm.gaus, i.var = 3)
partialPlot(rf.model, Data[,2:4], x.var = ""c"")
title(main = ""Boosted regression tree"", outer = TRUE, adj = 0.15)
title(main = ""Random forest"", outer = TRUE, adj = 0.85)
</code></pre>
"
"0.0464572209811883","0.0472279924554862"," 92824","<p>When fitting multiple variables to one outcome via the <code>lm()</code> function in R, <code>summary(lm)</code> gives me the p-values for individual regressors but not for the full model in an easily extractable (as in, just accessing fields) kind of way.</p>

<p>According to <a href=""https://stat.ethz.ch/pipermail/r-help/2009-April/194123.html"" rel=""nofollow"">this question</a>, it is possible to extract the p-value via <code>summary(lm)$fstatistic</code> by using the command:</p>

<pre><code>pf(x$fstatistic[1],x$fstatistic[2],x$fstatistic[3],lower.tail=FALSE)
</code></pre>

<p>However, while in the example linked this provides the same p-value as is printed, I get a different one:</p>

<pre><code>&gt; summary(model)
# ...
Residual standard error: 1.533 on 371 degrees of freedom
  (555 observations deleted due to missingness)
Multiple R-squared:  0.3364,    Adjusted R-squared:  0.2864 
F-statistic: 6.718 on 28 and 371 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>and:</p>

<pre><code>f = summary(model)$fstatistic
&gt; pf(f[1],f[2],f[3],lower.tail=F)
       value 
5.948007e-20 
</code></pre>

<p>What are possible reasons for these values to be different, and which one is the ""right"" one for the significance of the whole model?</p>
"
"0.131400863963521","0.133580934908406"," 92892","<p>I'm working on a meta-analysis of prevalence data. The aim is to get estimates of prevalence at the country level. The main issue is that the disease is highly correlated with age, and the sample ages of included studies are highly heterogeneous. Only median age is available for most studies, so I can't use SMR-like tricks. I figured I could use meta-regression to solve this, including age as a fixed-effect and introducing study-level and country-level random-effects.</p>

<p>The idea (that I took from <a href=""http://www.thelancet.com/journals/lancet/article/PIIS0140-6736%2813%2961249-0/abstract"" rel=""nofollow"">Fowkes et al</a>) was to use this model to make country-specific predictions of prevalence for each 5-year age group from 15 to 60 (using the median age of the group), and to apply these predictions to the actual population size of each of those groups in the selected country, in order to obtain total infected population and to calculate age-adjusted prevalence in the 15-60 population from that.</p>

<p>I tried several ways to do this using R with packages <code>meta</code> and <code>mgcv</code>. I got some satisfying results, but I'm not that confident with my results and would appreciate some feedback.</p>

<p>First is some simulated data, then the description of my different approaches:</p>

<pre><code>data&lt;-data.frame(id_study=c(""UK1"",""UK2"",""UK3"",""FRA1"",""FRA2"",""BEL1"",""GER1"",""GER2"",""GER3""),
                 country=c(""UK"",""UK"",""UK"",""FRANCE"",""FRANCE"",""BELGIUM"",""GERMANY"",""GERMANY"",""GERMANY""),
                 n_events=c(91,49,18,10,50,6,9,10,22),
                 n_total=c(3041,580,252,480,887,256,400,206,300),
                 study_median_age=c(25,50,58,30,42,26,27,28,36))
</code></pre>

<p><strong>Standard random-effect meta-analysis</strong> with package <code>meta</code>.</p>

<p>I used <code>metaprop()</code> to get a first estimate of the prevalence in each country without taking age into account, and to obtain weights. As expected, heterogeneity was very high, so I used weights from the random-effects model.</p>

<pre><code> meta &lt;- metaprop(event=n_events,n=n_total,byvar=country,sm=""PLOGIT"",method.tau=""REML"",data=data)
 summary(meta)
 data$weight&lt;-meta$w.random
</code></pre>

<p>I used meta to get a first estimate of the prevalence without taking age into account, and to obtain weights. As expected, heterogeneity was very high, so I used weights from the random-effects model.</p>

<p><strong>Generalized additive model</strong> to include age with package <code>mgcv</code>.</p>

<p>The <code>gam()</code> model parameters (k and sp) were chosen using BIC and GCV number (not shown here).</p>

<pre><code> model &lt;- gam( cbind(n_events,n_total-n_events) ~ s(study_median_age,bs=""cr"",k=4,sp=2) + s(country,bs=""re""), weights=weight, data=data, family=""binomial""(link=logit), method=""REML"")
 plot(model,pages=1,residuals=T, all.terms=T, shade=T)
</code></pre>

<p>Predictions for each age group were obtained from this model as explained earlier. CI were obtained directly using <code>predict.gam()</code>, that uses the  Bayesian posterior covariance matrix of the parameters. For exemple considering UK:</p>

<pre><code> newdat&lt;-data.frame(country=""UK"",study_median_age=seq(17,57,5))
 link&lt;-predict(model,newdat,type=""link"",se.fit=T)$fit
 linkse&lt;-predict(model,newdat,type=""link"",se.fit=T)$se
 newdat$prev&lt;-model$family$linkinv(link)
 newdat$CIinf&lt;-model$family$linkinv(link-1.96*linkse)
 newdat$CIsup&lt;-model$family$linkinv(link+1.96*linkse)
 plot(newdat$prev~newdat$study_median_age, type=""l"",ylim=c(0,.12))
 lines(newdat$CIinf~newdat$study_median_age, lty=2)
 lines(newdat$CIsup~newdat$study_median_age, lty=2)
</code></pre>

<p>The results were satisfying, representing the augmentation of the prevalence with advanced age, with coherent confidence intervals. I obtained a total prevalence for the country using the country population structure (not shown, I hope it is clear enough).</p>

<p>However, I figured I needed to include study-level random-effects since there was a high heterogeneity (even though I did not calculate heterogeneity after the meta-regression).</p>

<p><strong>Introducing study-level random-effect</strong> with package <code>gamm4</code>.</p>

<p>Since <code>mgcv</code> models can't handle that much random-effect parameters, I had to switch to <code>gamm4</code>.</p>

<pre><code> model2 &lt;- gamm4(cbind(n_events,n_total-n_events) ~ s(study_median_age,bs=""cr"",k=4) + s(country,bs=""re""), random=~(1|id_study), data=data, weights=weight, family=""binomial""(link=logit))
 plot(model2$gam,pages=1,residuals=T, all.terms=T, shade=T)

 link&lt;-predict(model2$gam,newdat,type=""link"",se.fit=T)$fit
 linkse&lt;-predict(model2$gam,newdat,type=""link"",se.fit=T)$se
 newdat$prev2&lt;-model$family$linkinv(link)
 newdat$CIinf2&lt;-model$family$linkinv(link-1.96*linkse)
 newdat$CIsup2&lt;-model$family$linkinv(link+1.96*linkse)
 plot(newdat$prev2~newdat$study_median_age, type=""l"",col=""red"",ylim=c(0,0.11))
 lines(newdat$CIinf2~newdat$study_median_age, lty=2,col=""red"")
 lines(newdat$CIsup2~newdat$study_median_age, lty=2,col=""red"")
 lines(newdat$prev~newdat$study_median_age, type=""l"",ylim=c(0,.12))
 lines(newdat$CIinf~newdat$study_median_age, lty=2)
 lines(newdat$CIsup~newdat$study_median_age, lty=2)
</code></pre>

<p>Since the study-level random effect was in the mer part of the fit, I didn't have to handle it. </p>

<p>As you can see, I obtain rather different results, with a much smoother relation between age and prevalence, and quite different confidence intervals. It is even more different in the full-data analysis, where the CI are much wider in the model including study-level RE, to the point it is sometimes almost uninformative (prevalence between 0 and 15%, but if it is the way it is...). Moreover, the study-level RE model seems to be more stable when outliers are excluded.</p>

<p><strong>So, my questions are:</strong></p>

<ul>
<li>Did I properly extract the weights from the metaprop() function and used them further?</li>
<li>Did I properly built my <code>gam()</code> and <code>gamm4()</code> models? I read a lot about this, but I'm not used to this king of models.</li>
<li>Which of these models should I use?</li>
</ul>

<p>I would really appreciate some help, since neither my teachers nor my colleagues could. It was a really harsh to conduct the systematic review, and very frustrating to struggle with the analysis... Thank you in advance!</p>
"
"0.131400863963521","0.128015062620555"," 93372","<p>In cognitive psychology, it has been argued that the learning curve of a skill follows the power function, in which the practice of the skill yields progressively smaller decrements in error. I want to test this hypothesis with the data I have, and to do this I need to fit a power law function to the data.</p>

<p>I heard that the power function can be fit in R by <code>lm(log(y) ~ log(x))</code>. <a href=""http://stats.stackexchange.com/questions/61600/strange-outcome-when-performing-nonlinear-least-squares-fit-to-a-power-law"">An answer to this post</a>, however, suggests using <code>glm(y ~ log(x), family = gaussian(link = log))</code><del>, and indeed the resulting fit prefers the <code>glm</code> approach</del> (<strong>EDIT:</strong> See comments. This is not true.).</p>

<pre><code># Define a function to compute R-squared in linear space 
&gt; rsq &lt;- function(data, fit) {
+ ssres &lt;- sum((data - fit)^2)
+ sstot &lt;- sum((data - mean(data))^2)
+ rsq &lt;- 1 - (ssres / sstot)
+ return(rsq)
}

# generate data and fit a power function with lm() and glm()
&gt; set.seed(10)
&gt; lc &lt;- (1:10)^(-1/2) * exp(rnorm(10, sd=.25)) # Edited
&gt; model.lm &lt;- lm(log(lc) ~ log(1:10))
&gt; model.glm &lt;- glm(lc ~ log(1:10), family = gaussian(link = log))
&gt; fit.lm &lt;- exp(fitted(model.lm))
&gt; fit.glm &lt;- fitted(model.glm)

# draw a graph with fitted lines
&gt; plot(1:10, lc, , xlab = ""Practice"", ylab = ""Error rate"", las = 1)
&gt; lines(1:10, fit.lm, col = ""red"")
&gt; lines(1:10, fit.glm, col = ""blue"")
&gt; legend(""topright"", c(""original data"", ""lm(log(y) ~ log(x))"", 
+ ""glm(y ~ log(x), family = gaussian(link = log))""), 
+ pch = c(1, NA, NA), lty = c(NA, 1, 1), col = c(""black"", ""red"", ""blue""))
</code></pre>

<p><img src=""http://i.stack.imgur.com/F28SM.jpg"" alt=""enter image description here""></p>

<pre><code># compute R-squared values for both models
# (EDIT: See comments. These are not good measures to use.)
&gt; rsq(lc, fit.lm)
[1] 0.9194631
&gt; rsq(lc, fit.glm)
[1] 0.9205448
</code></pre>

<p><del>From the $R^2$ values, it seems that the <code>glm</code> model has a better fit. When the procedure was repeated 100 times, it was always the case that the <code>glm</code> model has a higher $R^2$ value than the <code>lm</code> model</del> (<strong>EDIT:</strong> See comments. $R^2$ is not a good measure for this purpose).</p>

<p>At this point I have two questions.</p>

<ul>
<li>What is the difference between the <code>lm</code> model and the <code>glm</code> model above? I thought a generalized linear model with the log link function is practically the same as regressing log(y) in general linear models.</li>
<li>I'm trying to compare the fit of the power function against other models (e.g., linear models as in <code>lm(y ~ x)</code>). To do this, I calculate the fitted values in linear space and compare the $R^2$ values computed in linear space, just like I did to compare <code>lm</code> and <code>glm</code> models above. Is this a proper way to compare this type of models?</li>
</ul>

<p>My final question concerns the estimation of the four-parameter power law function given in <a href=""http://link.springer.com/article/10.3758/BF03212979"" rel=""nofollow"">this paper</a> (p.186; I modified the notation a little).</p>

<p>$$
E(Y_N) = A + B(N + E)^{-Î²}
$$
where</p>

<ul>
<li>$E(Y_N)$ = expected value of $Y$ on practice trial $N$</li>
<li>$A$ = expected value of $Y$ after learning is completed (i.e., asymptote)</li>
<li>$E$ = the amount of prior learning before $N = 0$</li>
</ul>

<p>I have $N$ and $Y$ (<code>1:10</code> and <code>lc</code> respectively in the code above), and want to estimate $A$, $B$, $E$, and $-Î²$ from the data. In the example above, I assumed $A$ = $E$ = 0 mostly because I wasn't sure how to estimate four parameters concurrently. While this isn't a huge problem, I would like to estimate $A$ and $E$ from the data as well if possible. So my final question:</p>

<ul>
<li>Is there a way to estimate the four parameters in R? If not, is there a way to estimate three parameters, excluding $A$ from the above (i.e., assuming $A$ = 0)?</li>
</ul>

<p>The paper I linked above says the authors used the simplex algorithm to estimate the parameters. But I am familiar with neither the algorithm nor the <code>simplex</code> function in R.</p>

<p>I'm aware of Clauset et al's work and the <code>poweRlaw</code> package in R. I believe both of them are dedicated to modeling the distribution of one categorical variable, but neither models the curve of the power law.</p>

<p>Any help is appreciated.</p>
"
"0.080466267117873","0.0818012824723818"," 93417","<p>This question is a prolongation of this question: <a href=""http://stats.stackexchange.com/questions/11096/how-to-interpret-coefficients-in-a-poisson-regression"">How to interpret coefficients in a Poisson regression?</a></p>

<p>If we follow the (almost) exact same routine, but we add correlation between the variablese treatment and improved (just for the sake of my question, which is interpreting the output), we get:</p>

<pre><code>treatment     &lt;- factor(rep(c(1, 2), c(43, 41)), 
                        levels = c(1, 2),
                        labels = c(""placebo"", ""treated""))
improved      &lt;- factor(rep(c(1, 2, 3, 1, 2, 3), c(29, 7, 7, 13, 7, 21)),
                        levels = c(1, 2, 3),
                        labels = c(""none"", ""some"", ""marked""))    
numberofdrugs &lt;- rpois(84, 10) + 1    
healthvalue   &lt;- rpois(84, 5)   
y             &lt;- data.frame(healthvalue, numberofdrugs, treatment, improved)
test          &lt;- glm(healthvalue~numberofdrugs+treatment+improved + treatment:improved, y, family=poisson)
summary(test)
</code></pre>

<p>Note the $\textbf{ treatment:improved}$ term I added inside the glm function. </p>

<p>Now, we get the following output:</p>

<pre><code>    Call:
glm(formula = healthvalue ~ numberofdrugs + treatment + improved + 
    treatment:improved, family = poisson, data = y)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.9261  -0.8733  -0.0296   0.5473   2.3358  

Coefficients:
                                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      1.553051   0.184229   8.430   &lt;2e-16 ***
numberofdrugs                    0.004298   0.014242   0.302   0.7628    
treatmenttreated                 0.007399   0.149440   0.050   0.9605    
improvedsome                     0.358897   0.164891   2.177   0.0295 *  
improvedmarked                  -0.178360   0.203756  -0.875   0.3814    
treatmenttreated:improvedsome   -0.330336   0.265310  -1.245   0.2131    
treatmenttreated:improvedmarked  0.050617   0.260203   0.195   0.8458    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 97.805  on 83  degrees of freedom
Residual deviance: 89.276  on 77  degrees of freedom
AIC: 383.29

Number of Fisher Scoring iterations: 5
</code></pre>

<p>If we ignore what seems to be insignificant coefficients, I can ask my question:</p>

<p>I understand that, as in the original post, treatment=placebo and improved=none is the base level for those variables, and thus are set to zero. My question is, why does it not exist any interaction terms with the base lavels for treatment=placebo and improved=none?</p>

<p>I thought setting the base levels to zero was just a construct, and in my mind there should still exist correlation between them...(?)</p>
"
"NaN","NaN"," 93436","<p>I am using R/RStudio to code a regression (and to optimize the function) over 50+ different variables. For the optimization to work I need to fit a higher order function (I am not sure to what degree I will use).</p>

<p>My question is this: is there a way in R to choose the best variables?</p>
"
"0.0464572209811883","0.0472279924554862"," 93438","<p>I've been wondering something for a while. If you run a simple regression model in R and then perform a step-wise selection (it doesn't have to be the way I typed the code below), how do you extract some of the ""relevant"" information of the model in R suchs as Odds Ratios and/or confidence intervals? </p>

<p>The general model would look the following:</p>

<pre><code>glm&lt;-glm(y~x+z+n+p, family=binomial(link=""logit))
step(glm, direction=""backward"", test=""F"")
</code></pre>

<p>Interestingly, in the newest version of R, I can't save an object as the step function. </p>

<pre><code>stepmodel&lt;-step(glm, direction...)
</code></pre>

<p>Only runs the stepwise selection model. 
So performing <code>step$anova</code>or <code>conf(step)</code>- or even better <code>exp(conf(step))</code>etc. doesn't seem possible....</p>

<p>Any ideas on this? Also, would any of you know of a package, where a stepwise variable selection of a cox-regression model is possible? </p>

<p>Thanks a mill for your thoughts and help. </p>

<p>Cheers, </p>

<p>Oliver</p>
"
"0.0709645772411954","0.0618359572423054"," 93610","<p>I have a dependent variable <code>C</code> and an independent variable <code>VPT</code>.
<code>VPT</code> is the average volume per tree (in cubic foot) of a timber stand. <code>C</code> ist costs resulting from a <a href=""https://github.com/ustroetz/MT/blob/master/CostFunc.py"" rel=""nofollow"">cost function</a>, that calculates the harvest costs per cubic foot (in $). </p>

<p>The plot between the two looks like this:
<img src=""http://i.stack.imgur.com/LqfIs.png"" alt=""enter image description here""></p>

<p>The abrupt change at roughly 28 cubic feet results from interaction within the cost function.
I want to create a regression model for the two. Based on the graph I assume it must be something like this:</p>

<pre><code>fit &lt;- lm(C ~ I(VPT^(-1)))
</code></pre>

<p>Is there a way to determine what the best fit is? Do I just have to try around until I get something that fits, or is there a more efficient way?</p>

<hr>

<p>The cost function takes in four parameters:</p>

<ul>
<li>Volume per tree (VPT) </li>
<li>Trees per acre (TPA) </li>
<li>Skidding Distance (SD) </li>
<li>Slope (S)</li>
</ul>

<p>Ultimately I want to create a regression that represents the cost function. Something like this:</p>

<pre><code>fit &lt;- lm(C ~ ÃŸ1xS + ÃŸ2xSD + ÃŸ3xTPA + ÃŸ4xVPT + ÃŸ5)
</code></pre>
"
"0.0848188929679971","0.0776035104406608"," 94089","<p>I started to use the function <code>multinom</code> of <code>R</code> package <code>nnet</code> in order to fit several conditional probability distributions with the multinomial logistic model. I need the parameters of the fittings in order to pass them to a Java program, which will compute the probabilities and use them.</p>

<p>My problem is that the probabilities computed with the parameters returned by <code>multinom</code>, following the usual <a href=""http://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_set_of_independent_binary_regressions"" rel=""nofollow"">definition</a> of multinomial logistic model, are not the same as those directly computed in <code>R</code>, which are the correct ones. On Stack Overflow I have already asked a <a href=""http://stackoverflow.com/questions/22905807/how-does-the-function-multinom-from-r-package-nnet-compute-the-multinomial-proba"">question</a> about this issue, but I do not still know how the <code>R</code> function <code>multinom</code> computes these probabilities; my guess is that it relies on neural networks, since this function belongs to <code>R</code> package <code>nnet</code>, but I do not have any idea about the details, and an inspection of the code led to nowhere.</p>

<p>Do you know an <code>R</code> package which fits conditional probabilities and returns the corresponding parameters of the model, so that we may easily compute the probabilities in another program? E.g., using the MARS model (<code>R</code> package <code>earth</code>) or Projection Pursuit Regression (<code>R</code> package <code>ppr</code>) is not feasible, since computing the probabilities from the parameters of these models would be a mess. Besides, the function <code>mlogit</code> from the <code>R</code> package with the same name is not applicable as well, since the dataset should be in a certain format (we would also need the predictors corresponding to the alternative, ""non-chosen"" response variable).</p>
"
"0.0929144419623766","0.0944559849109725"," 95451","<p>I've been having an argument with a friend of mine, and it's very possible I'm wrong here.</p>

<p>We are performing binary logistic regression on a dataset with 10000 observations, classifying action as ""good"" or bad"".  There are two independent variables (x1, x2), and class variable (y, with values ""good"" or ""bad"").  In this dataset, we  have 7,500 observations classified as ""bad"" and 2,500 classified as ""good"".  This is because there are several different ways for a user to perform a ""bad"" action, but only one way for them to perform a ""good"" action.</p>

<p>We are doing our analysis in R using the <code>glm()</code> function.</p>

<p>We create training data by randomly sampling 7,500 observations from the dataset, and create test data from the other 2,500 observations.  we then build a model using binary logistic regression on the training data, then test it on the test data.  The accuracy of our model is 75%.</p>

<p><strong>Can we say our model is better than guessing?</strong></p>

<p>He says that this model is no better than guessing.  Even though the error rate is better than 50%, because the original data had a prevalence of ""bad"" classifiers, we would need our model to predict better than 75% in order to say it performs better than random guessing.</p>

<p>I disagree...but I can't defend my point with anything other than ""that doesn't seem right"".  Can someone shed some light on the correct interpretation, and the reason for it?</p>
"
"0.0657004319817604","0.0556587228785024"," 95709","<p>I am fitting a regression with ARMA errors using the base R function <code>arima()</code> and the <code>Arima()</code> function from the <code>forecast</code> package.  </p>

<p>The estimated coefficients from both are identical. My problem comes from using <code>arima.errors()</code> on these two models, and using <code>tsdisplay()</code> to view these structural residuals (that is, the residuals straight from the regression, before any ARMA model is fit on them). These ARMA errors (and their corresponding ACFs, PACFs) are different between the two, and I don't know why. Even more curious is that the final residuals from both are in fact the same, which would make me think the structural residuals would have to be the same. I have put a MWE below.</p>

<pre><code>library('forecast')
data(usconsumption, package='fpp')

fit1 = arima(usconsumption[ ,1], xreg=usconsumption[ ,2], order=c(2,0,0))
tsdisplay(arima.errors(fit1), main=""ARIMA errors, arima function"") # not the same as the other


fit2 = Arima(usconsumption[,1], xreg=usconsumption[,2], order=c(2,0,0))
dev.new()
tsdisplay(arima.errors(fit2), main=""ARIMA errors, Arima function"") # not the same as the other


View(cbind(resid(fit1), resid(fit2))) # final residuals are the same
</code></pre>

<p>Note this example is from <a href=""https://www.otexts.org/fpp/9/1"" rel=""nofollow"">https://www.otexts.org/fpp/9/1</a></p>
"
"0.026822089039291","0.0272670941574606"," 95795","<p>from what I have studied in the data mining course (please correct me if I'm wrong) - in logistic regression, when the response variable is binary, then from the ROC curve we can determine the threshold.</p>

<p>Now I'm trying to apply the logistic regression for an ordinal categorical response variable with  more than two categories (4).
I used the function <code>polr</code> in r:</p>

<pre><code>&gt; polr1&lt;-polr(Category~Division+ST.Density,data=Versions.data)
&gt; summary(polr1)

Re-fitting to get Hessian

Call:
polr(formula = Category ~ Division + ST.Density, data = Versions.data)

Coefficients:
               Value Std. Error t value
DivisionAP   -0.8237     0.5195  -1.586
DivisionAT   -0.8989     0.5060  -1.776
DivisionBC   -1.5395     0.5712  -2.695
DivisionCA   -1.8102     0.5240  -3.455
DivisionEM   -0.5580     0.4607  -1.211
DivisionNA   -1.7568     0.4704  -3.734
ST.Density    0.3444     0.0750   4.592

Intercepts:
    Value   Std. Error t value
1|2 -1.3581  0.4387    -3.0957
2|3 -0.5624  0.4328    -1.2994
3|4  1.2661  0.4390     2.8839

Residual Deviance: 707.8457 
AIC: 727.8457  
</code></pre>

<p>How should I interpret the Intercepts?
and how can I determine the threshold for each group?</p>

<p>Thanks</p>
"
"0.0464572209811883","0.0472279924554862"," 95832","<p>I have two data sets </p>

<ol>
<li>Train data  </li>
<li>Test data (with no dependent variable values but I
have data on independent variable or you can say I need to
forecast).</li>
</ol>

<p>Using the training data (which has some <code>NA</code>s in the cell) I performed ordinary least square regression (OLS) using <code>lm()</code> in R and fitted the model &amp; I got the $\beta $ coefficients of the regression model. (All is good so far!)</p>

<p>Now, in the process of prediction for the fitted values, I have some missing values for some cells in the test dataset. I used function <code>predict()</code> as follows: </p>

<pre><code> predict(ols, test_data.df, interval= ""prediction"", na.action=na.pass)
</code></pre>

<p>for the cell (or cells) with <code>NA</code> value the entire row is discarded in generating the output (<code>yhat</code>). Is there any function that could generate the <code>yhat</code> values (other than <code>NA</code>s) for the test data without discarding any rows with missing value in the cell. </p>
"
"0.0848188929679971","0.0862261227118454"," 95974","<p>This is a follow-up question from this post, here:
<a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">Confidence intervals for predictions from logistic regression</a></p>

<p>The answer from @Gavin is excellent, but I have some additional questions which I think would be useful for others. I am working with a Poisson model, so basically it is the same approach described in the other post, only <code>family=poisson</code> instead of <code>family=binomial</code>.</p>

<p>To my first question:
@Gavin writes:</p>

<pre><code>mod &lt;- glm(y ~ x, data = foo, family = binomial)
preddat &lt;- with(foo, data.frame(x = seq(min(x), max(x), length = 100))
preds &lt;- predict(mod, newdata = preddata, type = ""link"", se.fit = TRUE)
</code></pre>

<p>What is the point of the second line there? Is it necessary to create a data.frame with minimum and maximum of the explanatory variable? Could I not, for some explanatory variable(s) <code>x</code> (stored in some data frame <code>data</code>), just go from the first line and directly to the third?</p>

<p>To my second question:
In the beginning of his answer @Gavin writes:</p>

<blockquote>
  <p>The usual way is to compute a confidence interval on the scale of the
  linear predictor, where things will be more normal (Gaussian) and then
  apply the inverse of the link function to map the confidence interval
  from the linear predictor scale to the response scale.</p>
</blockquote>

<p>Why are ""things"" more normal on the scale of the linear predictor(s)? Is this also the case when I do my Poisson regression?
I assume the reason for using critical value 1.96 when constructing the CI's, is because of the assumptions that ""things"" are normal. Can somebody explain this further?</p>

<p>My third question:</p>

<p>Is there a relationship between the standard deviation which we get by using <code>se.fit=TRUE</code>  in predict() and the standard deviations of the coefficients of the explanatory variables, which we simply get from <code>summary(mod)</code>? (<code>mod</code> is some glm object)</p>
"
"0.125925827965041","0.133580934908406"," 95994","<p>I`d like to extract the parameters of a two-component mixture distribution of noncentral student t distributions which first has to be fitted to a one-dimensional sample.</p>

<p>My question is closely related to this thread, but as pointed out I want to use Student t components for the mixture:
<a href=""http://stats.stackexchange.com/questions/10062/which-r-package-to-use-to-calculate-component-parameters-for-a-mixture-model?newreg=fe1454a4702e4532a03bd2c705fe3b02"">Which R package to use to calculate component parameters for a mixture model</a></p>

<p>There are many packages for R that are capable of handling mixture distributions in one way or another. Some in the context of a Bayesian framework requiring kernels. Some in a regression framework. Some in a nonparametric framework. ...</p>

<p>In general the ""mixdist""-package seems to come closest to my wish. This package fits parametric mixture distributions to a sample of data. Unfortunately it doesn`t support the student t distribution.</p>

<p>I have also tried to manually set up a likelihood function as described here:
<a href=""http://stackoverflow.com/questions/6485597/r-how-to-fit-a-large-dataset-with-a-combination-of-distributions"">http://stackoverflow.com/questions/6485597/r-how-to-fit-a-large-dataset-with-a-combination-of-distributions</a>
But my result is far from perfect.</p>

<p>The ""gamlss.mx""-package might be helping, but originally it seems to be set up for another context, i.e. regression. I tried to regress my data on a constant and then extract the parameters for the estimated mixture error distribution. Is this a valid approach? </p>

<p>But with this approach the estimated parameters seem to be not directly accessable individually by some command (such as fit1$sigma). And more importantly there seem to be serious estimation problems even in pretty simple and nonambiguous cases.
E.g. in example 2 (see syntax below) I simulated a mixture which looks like this:</p>

<p><img src=""http://i.stack.imgur.com/MG7AA.jpg"" alt=""kernel density estimate of the mixture""></p>

<p>When trying to fit a two-component student t mixture to these data either I get this error message (the deeper meaning of which I don't understand):</p>

<p><img src=""http://i.stack.imgur.com/UPvg4.jpg"" alt=""enter image description here""></p>

<p>or I get wrong results (convergenve is reached only after approximately two hours as can be seen from the output):</p>

<p><img src=""http://i.stack.imgur.com/HjlfW.jpg"" alt=""enter image description here""></p>

<p>The means could be estimated well, but both the variance and the degrees of freedom are estimated badly. In the TF2 implementation of the student t, the sigma parameter denotes the standard deviation. Its estimate is NEGATIVE for the first component! And for the second component the degrees of freedom estimate is also NEGATIVE. Probably one should not use these results in practice :(</p>

<p>By the way: Is there a way to restrict these degree-of-freedom coefficient estimates to be natural numbers? </p>

<p>The following syntax is my gamlss.mx-setup so far:</p>

<pre><code>library(gamlss.dist)
library(gamlss.mx)
library(MASS)

# example 1 (real data):
data(geyser)
plot(density(geyser$waiting) )
fit1 &lt;- gamlssMX( waiting~1,data=geyser,family=""TF2"",K=2 )
fit1
# works fine

# example 2 (simulated data):
N &lt;- 100000
components &lt;- sample(1:2,prob=c(0.6,0.4),size=N,replace=TRUE)
mus &lt;- c(3,-6)    # denotes the mean of component 1 and 2, respectively
sds &lt;- c(1,9)     # ... the standard deviations
nus &lt;- c(25,3)    # ... the degrees of freedom
mixsim &lt;-data.frame(rTF2( N,mu=mus[components],sigma=sds[components],nu=nus[components] ))
colnames(mixsim) &lt;- ""MCsim""
plot(density(mixsim$MCsim) , xlim=c(-50,50))
fit2 &lt;- gamlssMX(MCsim~1,data=mixsim,family=""TF2"",K=2)
fit2
# error message or strange results (this also happens when using a sample of S&amp;P500 returns)
</code></pre>

<p>I would be very grateful for any advice!
I've read through many related manuals and vignettes so far but I`m still lost.</p>

<p>Thanks a lot in advance!!
Jo</p>
"
"0.0438002879878403","0.0556587228785024"," 96510","<p>The function <code>resettest</code> in the <code>R</code> package <code>lmtest</code> runs the Ramsey RESET test.  A student today brought to my attention that this function behaves oddly when it is handed a regression which displays perfect fit:</p>

<pre><code>library(lmtest)

x &lt;- seq(1,100)
y &lt;- x

reg1 &lt;- lm(y~x)
summary(reg1)

# Should match but don't
summary(lm(y~x+I(x^2)))
resettest(reg1,power=2,type=""regressor"")

set.seed(12344321)
x &lt;- rnorm(1000)
y &lt;- x

reg2 &lt;- lm(y~x)
summary(reg2)

# Should match but don't
summary(lm(y~x+I(x^2)))
resettest(reg2,power=2,type=""regressor"")

set.seed(12344321)
x &lt;- seq(1,100)
y &lt;- x + 0.001*rnorm(100)

reg3 &lt;- lm(y~x)
summary(reg3)

# Should match and do, even though almost the same as reg1 example
summary(lm(y~x+I(x^2)))
resettest(reg3,power=2,type=""regressor"")
</code></pre>

<p>In this code sample, the p-value for the x^2 term should match the p-value for the <code>resttest</code> in each of the three pairs in the code.  What actually happens is that they fail to match in examples reg1 and reg2---two cases where the regression fits perfectly.  In example reg3---which is almost identical to example reg1, differing only by the addition of a tiny bit of noise---they do match.</p>

<p>This does not have a lot of practical significance since real world regressions basically never fit perfectly, but it is disconcerting if you are just playing around with the function to convince yourself that you understand what it does.</p>

<p>Does anyone understand why this seemingly undesirable behavior occurs or if I am doing something wrong in my use of <code>resttest</code>?  Maybe the test statistic ends up being calculated as (roundoff error)/(roundoff error)?</p>
"
"0.0464572209811883","0.0472279924554862"," 96863","<p>I was reading <a href=""http://anythingbutrbitrary.blogspot.com/2012/04/lm-function-with-categorical-predictors.html"" rel=""nofollow"">""The lm() function with categorical predictors""</a>, and am confused.</p>

<ol>
<li><p>What does the regression model with a categorical predictor look
like, with the following R code:</p>

<pre><code>n = 30
sigma = 2.0

AOV.df &lt;- data.frame(category = c(rep(""category1"", n)
                                , rep(""category2"", n)
                                , rep(""category3"", n)), 
                            j = c(1:n
                                , 1:n
                                , 1:n),
                            y = c(8.0  + sigma*rnorm(n)
                                , 9.5  + sigma*rnorm(n)
                                , 11.0 + sigma*rnorm(n))
                  )

AOV.lm &lt;- lm(y ~ category, data = AOV.df)
summary(AOV.lm)
</code></pre>

<p>The output is:</p>

<pre><code>Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         8.4514     0.3405  24.823  &amp;lt; 2e-16 ***
categorycategory2   0.8343     0.4815   1.733   0.0867 .  
categorycategory3   3.0017     0.4815   6.234 1.58e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 1.865 on 87 degrees of freedom
Multiple R-squared: 0.3225, Adjusted R-squared: 0.307 
F-statistic: 20.71 on 2 and 87 DF,  p-value: 4.403e-08 
</code></pre>

<p>Is the model:</p>

<p><em>y = 8.4514 + 0.8343 for category 1</em>,</p>

<p><em>y = 8.4514 + 0.8343 for category 2</em> or</p>

<p><em>y = 8.4514 + 3.0017 for cateogry 3</em>?</p></li>
<li><p>What is the model if the R code looks like:</p>

<pre><code>&gt; X &lt;- read.table(""http://www.stat.umn.edu/geyer/5102/data/ex5-4.txt"", header=T) 
&gt; lm(y ~ color + x * color, data=X)

Call:
lm(formula = y ~ color + x * color, data = X)

Coefficients:
 (Intercept)    colorgreen      colorred             x  colorgreen:x  
    13.96118       0.25243       6.10543       0.97241       0.07347  
  colorred:x  
     0.01962
</code></pre></li>
</ol>

<p>Thanks!</p>
"
"0.0848188929679971","0.0862261227118454"," 96991","<p>I have done survival analysis. I used Kaplan-Meir to do the survival analysis. </p>

<p>Description of data: 
My data set is large and data table has close 120,000 records of survival information belong to 6 groups.</p>

<p>Sample: </p>

<pre><code>   user_id   time_in_days   event total_likes total_para_length group
1:       2          4657     1       38867        431117212   AA
2:       2          3056     1       31392        948984460   BB
3:       2            49     1          15            67770   CC
4:       3          4181     1       15778        379211806   BB
5:       3            17     1           3            19032   CC
6:       3          2885     1       12001        106259666   EE
</code></pre>

<p>After fitting the survival curves and plotting it, I see they are similar but yet at any given point in time their survival proportions don't seem to look like identical.</p>

<p>Here is the plot:
<img src=""http://i.stack.imgur.com/9wLYH.png"" alt=""Survival Curves""></p>

<p>I ran a hypothesis test where my H0: There is not difference between the survival curves and here is the results that I got. </p>

<pre><code>&gt; survdiff(formula= Surv(time, event) ~ group, rh=0)
Call:
survdiff(formula = Surv(time, event) ~ group, rho = 0)

             N Observed Expected (O-E)^2/E (O-E)^2/V
group=FF 28310    27993    28632      14.3      19.0
group=AA 64732    63984    67853     220.6     460.1
group=BB 19017    18690    16839     203.4     245.6
group=CC  9687     9536     8699      80.6      91.0
group=DD 13438    13187    11891     141.3     164.2
group=EE  3910     3847     3324      82.4      89.7

 Chisq= 788  on 5 degrees of freedom, p= 0  
</code></pre>

<p>I am little confuse by trying to figure out what it means, specially since I got <code>p-value=0</code>. </p>

<p>I am fairly new to survival analysis so after reading and digging through I realized that this is a non-parametric as I understand which means that it doesn't make any assumptions of the underline distributions of the time.</p>

<p>After reading about cox-proportional hazard function and going over <a href=""http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf"" rel=""nofollow"">c-cran pdf</a> I performed a cox regression test and here is what I got from that: </p>

<pre><code>&gt; cox_model &lt;- coxph(Surv(time, event) ~ X)
&gt; summary(cox_model)
Call:
coxph(formula = Surv(time, event) ~ X)

  n= 139094, number of events= 137237 

         coef  exp(coef)   se(coef)       z Pr(&gt;|z|)    
X1 -7.655e-05  9.999e-01  1.504e-06 -50.897   &lt;2e-16 ***
X2 -1.649e-10  1.000e+00  5.715e-11  -2.886   0.0039 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

   exp(coef) exp(-coef) lower .95 upper .95
X1    0.9999          1    0.9999    0.9999
X2    1.0000          1    1.0000    1.0000

Concordance= 0.847  (se = 0.001 )
Rsquare= 0.111   (max possible= 1 )
Likelihood ratio test= 16307  on 2 df,   p=0
Wald test            = 7379  on 2 df,   p=0
Score (logrank) test = 4628  on 2 df,   p=0
</code></pre>

<p>My big X is generated by doing rbind on total_like and total_para_length. Looking at Rsquare and P-Values I am not sure what really is going on here. If I can't throw away the Null-Hypothesis I should give a larger p-value. </p>
"
"0.0599760143904067","0.0609710760849692"," 96999","<p>I'm working on a model that requires me to look for predictors for a rare event (less than 0.5% of the total of my observations). My total sample is a significant part of the total population (50,000 cases). My final objective is to obtain comparable probability values for all the non-events, without the bias of the groups difference in the logistic regression. </p>

<p>I've been reading the info in the following link: </p>

<p><a href=""http://gking.harvard.edu/files/gking/files/0s.pdf"" rel=""nofollow"">http://gking.harvard.edu/files/gking/files/0s.pdf</a></p>

<p>It advises me first to use a sample of my original sample, containing all the events (1) and a random sample of 1-5 times bigger of the non-event (0) sample.</p>

<p>Then it suggests using weights based on the proportion of the sample 1s to 0s. In the section 4.2 of the linked text, he offers a ""easy to implement"" weighted log-likelihood that can be implemented in any logit function.</p>

<p>I wish to implement these weights somehow with R's glm(...,family=binomial(link=""logit"")) or similar function ( the ""weights"" parameter is not for frequency weighting), but I don't really know how to apply this weighting.</p>

<p>Does anybody knows how to make it or any other alternative suggestion?  </p>

<p><strong>Edit1:</strong> As suggested bellow, is Firth's method for bias-correction by penalizing the likelihood in the <code>logistf</code> package a correct approach in this case? I'm not much knowledgeable in statistics, and, while I understand the input and the coefficients/output of the logistic model, what happens in between is still quite a mystery to me, sorry.</p>
"
"0.0379321620905441","0.0385614943639849"," 97250","<p>How does the stepwise regression method work for <code>both</code> direction in R with the <code>step()</code> function.</p>

<p>I would think that one variable will be placed into the model and then another that will improve the measuring criteria and the significance of the older variable gets assessed. If the older variable's coefficient is not significant the variable will be removed and a next variable will be placed into the model and so forth.</p>

<p>I am not a 100% sure if this is how the step function with <code>both</code> do it, but can someone please inform me if this is correct, if not how does the <code>both</code> direction criteria implement the stepwise regression in R with <code>step()</code>. </p>
"
"NaN","NaN"," 97257","<p>What is the critical p-value used by the <code>step()</code> function in R for stepwise regression? I assume it is 0.15, but is my assumption correct? How can I change the critical p-value? </p>
"
"0.053644178078582","0.0545341883149212"," 97347","<p>How can I improve the accuracy of my logistic regression code, which tests the accuracy using the 10-fold cross-validation technique? I have implemented this code using <code>glmfit</code> and <code>glmval</code>. The desired accuracy is somewhat higher and it requires the parameters to be found using maximum likelihood estimator. Also, when I run this code in MATLAB, I get the following error</p>

<blockquote>
  <p>Warning: X is ill conditioned, or the model is overparameterized, and some coefficients are not identifiable. You should use caution in making predictions. In glmfit at 245 In LR at 8</p>
</blockquote>

<p>The code is:</p>

<pre><code>function LR( X,y)
y(y==-1)=0;
X=[ones(size(X,1),1) X];
disp(size(X,2));
indices = crossvalind('Kfold',y,10);
for i = 1:10
    test = (indices == i); train = ~test;
    b = glmfit(X(train,:),y(train),'binomial','logit');
    y_hat= glmval(b,X(test,:),'logit');
    y_true=y(test,:);
    error(i)=mean(abs(y_true-y_hat));
end
accuracy=(1-error)*100;
fprintf('accuracy= %f +- %f\n',mean(accuracy),std(accuracy));
end
</code></pre>
"
"NaN","NaN"," 97385","<p>I work with a regression with ARMA errors and I want to use LASSO to shrink the coefficients and select my variables. This topic is discussed in the article <a href=""http://www.sciencedirect.com/science/article/pii/S0378375812000948"" rel=""nofollow"">Wu et al. (2012)</a>.
So, the problem I have to minimize is:</p>

<p>$$
L(\xi)=||Y-X\xi||^2+\lambda_1\sum_{i=1}^{d}\frac{|\beta_i|}{|\beta_{i}^{OLS}|}+\lambda_2\sum_{i=1}^{p}\frac{|\phi_i|}{|\phi_{i}^{OLS}|}+\lambda_3\sum_{i=1}^{q}\frac{|\theta_i|}{|\theta_{i}^{OLS}|}
$$
where: $\xi=[\beta,\phi,\theta]$ and $\xi^{OLS}=[\beta^{OLS},\phi^{OLS},\theta^{OLS}]$ is the least squares estimate.</p>

<p>Do you know if there is a function in R that can solve this minimization? Something similar with glmnet / l1ce / lars, but with multiple constraints on the parameters?</p>

<p>Thank you!</p>
"
"0.050688983743711","0.072141950116023"," 97437","<p>I am studying the factors influencing the annual salary for employees at a undisclosed bank. The regression model that I have decided to employ is as follows:</p>

<p>\begin{equation}
Y_{k}=\beta_{1}+\beta_{2}E_{k}+\beta_{3}D_{gk}+\beta_{4}D_{mk}+\beta_{5}D_{2k}+\beta_{6}D_{3k}+\varepsilon_{k}
\end{equation}
where $Y_{k}$ is the logarithm of annual salary, $E$ is the number of years of education, $D_{g}$ is a gender dummy, $D_{m}$Â is a minority dummy, and where </p>

<p>\begin{equation}
D_{2}=\begin{cases} 1 &amp;\text{Custodial job} \\ 0 &amp; \text{Otherwise} \end{cases}
\end{equation}<br>
and 
\begin{equation}
D_{3}=\begin{cases} 1 &amp;\text{Management job} \\ 0 &amp; \text{Otherwise} \end{cases}
\end{equation}
As you know, whenever one deals with GLS, $\Omega$ will almost surely be unknown and thus have to be estimated. In general there are $\frac{n(n+1)}{2}$ parameters to be estimated, which makes it pretty impossible to come up with a viable estimation out of $n$Â observations. This is usually counteracted by imposing some structure on $\Omega$.</p>

<p>In my case, I would like to make the assumption that the disturbance terms $\varepsilon_{k}$ in the above regression model have variance $\sigma_{i}^{2}$ for $i=1,2,3$, according to whether the $i$-th employee has a job in category 1,2, or 3 respectively. Now, we may introduce the transformations $\gamma_{1}=\log (\sigma_{1}^{2}),\gamma_{2}=\log(\sigma_{2}^{2}/\sigma_{1}^{2})$, and $\gamma_{3}=\log(\sigma_{3}^{2}/\sigma_{1}^{2})$ so as to enable us to formulate the following model for</p>

<p>\begin{equation}
\sigma_{k}^{2}= \exp \{ \gamma_{1}+\gamma_{2}D_{2k}+\gamma_{3}D_{3k} \}
\end{equation}
Since $\hat{\beta}_\rm{OLS}$ is a consistent estimate of $\beta$, even under the assumption of heteroscedasticity, we have that $\hat{\beta}_{\rm OLS} \xrightarrow[]{p}\beta$ as the number of observations increase. We may therefore argue that $e_{k}^{2} \approx \sigma_{k}^{2}$, and so we can regress upon information that we already possess. </p>

<p><strong>Summary of procedure</strong></p>

<p><strong>(1)</strong> Calculate the OLS estimate.</p>

<p><strong>(2)</strong> Calculate the OLS residual $\textbf{e}=\textbf{Y}-\textbf{X}\hat{\beta}$</p>

<p><strong>(3)</strong> Calculate the OLS estimate of $\gamma$ from $e_{k}^{2}=f_{\gamma}(Z_{k})+\overline{\varepsilon}_{k}$.</p>

<p><strong>(4)</strong> Calculate the FGLS estimate as the GLS estimate with $\hat{\Omega}=\Omega(\hat{\gamma})$ in place of $\Omega$. </p>

<blockquote>
  <p>What I would like to know is whether or not one can perform this estimation using a known function in R, say <code>gls</code>? If the answer is yes, then how exactly should I write to ensure that that my heteroscedasticity assumption is taken into account? Thanks for taking the time! Have a great day!</p>
</blockquote>
"
"0.116914775576919","0.112599007499728"," 97783","<p>I'm using Bernhard Pfaff's packages {urca} and {vars} to analyze 3 time series.  Each is I(1) and cointegrated with $r =2$ cointegrating relationships.</p>

<p>The <em>vec2var</em>() command should make the conversion from the <strong>ca.jo</strong> object (my VECM) to its VAR representation in levels.  But I do not understand the output from <em>vec2var</em>(), nor how to perform VAR analyses on this output (i.e., send the data to the <em>VAR</em>() function to get a <em>varest</em> object). </p>

<p>That is, after converting from VECM to VAR what do I send to the <em>VAR</em>() function (which performs the standard VAR estimates)? 
Should it be:</p>

<pre><code>var.form$A%*%var.form$datamat[ , c(6:11)] + var.form$deterministic%*%var.form$datamat[ , c(4:5)]
</code></pre>

<p>where dimensions in the code above are because I have $K=3$ variables, $p=2$ lags, a constant term and a deterministic trend.</p>

<p>But the formulation from <em>vec2var</em>() [on which I've based the above R code] is something like 
\begin{equation}
  Y_t = A_1 Y_{t-1} + A_2 Y_{t-2} + \Phi D_t  + \mu + \epsilon_t 
\end{equation}
That is, a basic VAR(2) with a deterministic time trend and constant. </p>

<p>But this is what I'm confused about: The VAR form of the long-run VECM (equation 4.8a, in Pfaff's <em>Analysis of Integrated and Co-Integrated Time Series with R. useR!</em>, and elsewhere <em>e.g.</em> LÃ¼tkepohl eq'n 7.1.1) is: </p>

<p>\begin{equation}
     \Delta y_t  = \Gamma_1 \cdot \Delta y_{t-1} + \Gamma_2 \cdot \Delta y_{t-2} + \Pi \cdot y_{t-2} + \Phi \cdot D_t  + \mu + \epsilon_t 
\end{equation}</p>

<p>To get the proper VAR level representation do I have to take the data from <em>vec2var</em>() and manipulate it myself? (i.e. find the difference series and then multiply by the appropriate cointegration matrix)?  If this is the case then <strong>which</strong> [in R code please!] coefficient matrices should multiply which vectors? </p>

<p>I feel like there's a disconnect from the theory and mathematical formalism and these R packages... but more likely I'm missing something pretty basic. </p>

<p>Here's the basic R code set up:</p>

<pre><code>library(urca) 
library(vars) 

vecm &lt;- ca.jo(mydata, ecdet = ""trend"", type = ""trace"",  spec = ""longrun"") 
var.form &lt;- vec2var(vecm, r= 2) 
# The output data I get can be seen from names(var.form), but the key parts are: 

var.form$y          # my original data 
var.form$datamat    # original data organized with the three variables (starting at t = 2), the constant term, trend term and then six columns with two lagged levels at t=1 and t=0. 

# the coefficients for the regressors on the endogenous variables (A) and deterministic components are:
     var.form$A             # lagged variable coefficients (levels only, I think) 
     var.form$deterministic     # constant and time trend coefficients
</code></pre>
"
"0.0657004319817604","0.0667904674542028"," 97811","<p>I am using leave-one-out cross-validation to evaluate a linear regression model. In subsequent analysis, I need three specific values for each observation: observed value, predicted value, prediction standard error. Prediction standard error values can be retrieved from function <code>predict.lm</code> setting argument <code>se.fit = TRUE</code>. The following code (adapted from <a href=""http://www.analyticbridge.com/profiles/blogs/cross-validation-in-r-a-do-it-yourself-and-a-black-box-approach"" rel=""nofollow"">here</a>) can be used to do what I currently need:</p>

<pre><code>library(faraway)
gala[1:3, ]
c1 &lt;- c(1:30)
gala2 &lt;- cbind(gala, c1)
gala2[1:3, ]
obs  &lt;- numeric(30)
pred &lt;- numeric(30)
se   &lt;- numeric(30)
for (i in 1:30) {
     model1  &lt;- lm(Species ~ Endemics + Area + Elevation,
                   subset = (c1 != i), data = gala2)
     specpr  &lt;- predict(model1, gala2[i, ], se.fit = TRUE)
     obs[i]  &lt;- gala2[i, 1]
     pred[i] &lt;- specpr$fit
     se[i]   &lt;- specpr$se.fit
}
res &lt;- data.frame(obs, pred, se)
head(res)
  obs       pred       se
1  58  70.185063 5.524249
2  31  72.942732 6.509655
3   3  -8.303608 7.055163
4  25  20.948932 6.998093
5   2 -15.953141 7.403062
6  18  27.274440 6.220029
</code></pre>

<p>I searched through the documentation of some of the packages that offer functions for cross-validation, but did not find any that saves prediction standard errors. Is there any package that already offers such functionality?</p>
"
"0.026822089039291","0.0272670941574606"," 97907","<p>The response variable I'm dealing with is the proportion of a total area that is suitable habitat for a species of interest. So although the response variable is bounded between 0 and 1, my intuition is that it wouldn't be appropriate to call it binomial since the numerator and denominator of the proportion are non-integer. The beta distribution comes to mind, but I'm uncertain of the appropriate link function and whether there tools in R to deal with a beta.  </p>

<p>Some background on my eventual goal: I'll likely be pursuing a conditional autoregressive model to account for spatial autocorrelation. I'll be treating space as 1-dimensional since I'm dealing with a river system, and so each observational unit only has two neighbors: one upstream and downstream.</p>

<p>I'll be working in R and JAGS/BUGS if I decide to go Bayesian.</p>
"
"0.026822089039291","0.0272670941574606"," 99272","<p>I'm using a regression package that uses <code>gam()</code> from the <code>mgcv</code> package. Is it possible to include an L1 penalty with the <code>gam</code> function? The documentation indicates that an L2 penalty can be used using the <em>H</em> parameter, but I do not understand how, or if it can be used for the lasso as well. Splines are not in the model; the only reason why I'm not using <code>glmnet</code> is because the function I use has GAM implemented. Example code:</p>

<pre><code>formula.eq = Y ~ X
model.out = gam(formula.eq, binomial(link = ""logit""), gamma = 1)
</code></pre>

<p>Where <code>Y</code> is a vector of zeros and ones, and <code>X</code> includes > 400 features, so I need to perform some shrinkage method.
Thanks.</p>
"
"0.053644178078582","0.0545341883149212"," 99626","<p>I'm trying to show a correlation between growth in a petri dish of some fungi and its effect on a plant. I have ten strains of fungi which I tested in the plant and in petri dishes. I can put data from both experiments into linear models (lm) to get estimated means and variances. If I run a regression on the estimated means for each strain I find a significant correlation, but this doesn't take uncertainty in the strain means into account.
So far I've tried a parametric bootstrap, but I'm having a hard time figuring out the ultimate test statistic. I've included the parameter estimates and my R code. At the moment it generates simulated strain means from the estimated parameters. I need a p value for the covariance between growth and virulence. Any help would be greatly appreciated! My specific questions are:
1) Is a parametric boostrap the right way to go about analyzing this?
2) How would one go about doing this in R?</p>

<pre><code>Strain  MeanVirulence MeanGrowth    VirulenceVarGrowthVar  
1  -5.26064 0.066716    0.67834 0.053247  
2   -4.05482    -0.055524   0.68385 0.047111  
3   -5.47282    0.029047    0.68385 0.046739  
4   -3.50632    -0.161811   0.68385 0.047083  
5   -4.94051    -0.224949   0.68385 0.04727  
6   -4.04982    -0.062938   0.68385 0.047647  
7   -4.53178    -0.142985   0.68385 0.04788  
8   -3.01697    -0.199349   0.68385 0.047255  
9   -3.81093    -0.254793   0.68385 0.047255  
10  -1.61882    -0.325289   0.68385 0.0469
</code></pre>

<p>And here is my R code:</p>

<pre><code>gendata&lt;-function(par,npar=TRUE,print=TRUE){
    n     = 10
    k     = 2
    x=matrix(data=NA, nrow=n, ncol=k)
    for(i in 1:n){
      x[i,1] = rnorm(1,mean=par[i,2],sd=par[i,4])
      x[i,2] = rnorm(1,mean=par[i,3],sd=par[i,5])
    }
    return(x)
}

lmp &lt;- function (modelobject) {
    f &lt;- summary(modelobject)$fstatistic
    p &lt;- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) &lt;- NULL
    return(p)
}


samp=20000
rescor=matrix(data=NA, samp)
resvar=matrix(data=NA, samp)
pvals=matrix(data=NA, samp)
numsig = 0
numnotsig = 0
for (i in 1:samp){
    x&lt;-gendata(parameters)
    rescor[i]&lt;-cor(x[,1],x[,2], method = ""pearson"")
    resvar[i]&lt;-var(x[,1])
    a&lt;-lm(x[,1]~x[,2])
    pvals[i]&lt;-lmp(a)
    if (pvals[i] &lt; 0.05){
        numsig = numsig + 1
    }
    if (pvals[i] &gt; 0.05){
        numnotsig = numnotsig + 1
    }    
}

Strain Plate Growth  
1      1     200   
1      2     210  
1      3     190  
2      1     150   
2      2     130  
2      3     140  
...  

Strain Plant Growth  
1      1     70  
1      2     40  
1      3     50  
1      4     45  
2      1     80  
2      2     90  
2      3     85  
2      4     75  
...
</code></pre>
"
"0.026822089039291","0.0272670941574606"," 99981","<p>I am doing bivariate kernel regression using the sm.regression function:<br>
<a href=""http://cran.r-project.org/web/packages/sm/sm.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/sm/sm.pdf</a></p>

<p>There is an option to compare the nonparametric estimate with linear model. When requested, it returns p-value that the nonparametric estimate is not different from the linear (null hypothesis).</p>

<p>Does anyone know what test is used to calculate the p-value? I can't find it anywhere.</p>

<p>The answer might be in:
Bowman, A.W. (2006). Comparing nonparametric surfaces. Statistical Modelling, 6, 279-299.</p>

<p>Thank you!</p>
"
"0.116914775576919","0.118854507916379","100101","<p>I am currently working at work on a project that attempts to predict an environmental change variable. I am personally not a huge fan of the project, but I still want to do the best job possible. Anyhow, let me first describe the properties of the data, and then state my question. </p>

<p>The environmental variable we are trying to model is continuous, and ranges from 0 to 25 (it can have values such as 0.1345 or 1.2335 or 5.674). The environmental variable also has a significant mass at zero (around 30% of the data are zero values), and most of the data is in the range between > 0 and &lt; 1. To complicate things further, around 3% of the data have extreme values of greater than 2. In my opinion, the distribution of the data resembles a Tweedie distribution. </p>

<p>We have around 5 million observations in the dataset. We are going to predict the environmental variable using a set of eight explanatory variables.</p>

<p>We have been modeling the prediction of the environmental for a week, and the predictive power of our results has been meager. Here are the different modeling approaches I have used:</p>

<ol>
<li><p>GLM model with a tweedie distribution (glm function in r). This model approach overestimate the environmental change when the variables has low values. Most values between 0 - 1 are significantly overestimated, and the model does not predict high values very well either. The GLM approach produces a very bad fit for our data.</p></li>
<li><p>Generalized Additive Model with a tweedie distribution (bam function from mgcv package in r). This approaches fits the data slightly better than the model above, but still significantly overestimates values in the low range.</p></li>
<li><p>Regression tree model (rpart model in r). The regression tree model most accurately predicts values in the lower range of the distribution, but fails to predict zero values, and performs also poorly for values greater than 2. </p></li>
<li><p>Boosted regression model (dismo package in r using gbm.step). The model significantly overestimate values in the lower range of the distribution. For example, if values are 0.23 it predicts values to be 1.67 and so forth. I believe the gbm.step model to be inadequate for our data, since the family of distribution in the package only models the bernoulli (=binomial), poisson, laplace or gaussian family. None of which accurately describe our distribution.</p></li>
</ol>

<p>Since our dataset is very large, I split the data 50/50 into a test and training dataset, and evaluated model fit on a variety of test statistics for the test data set. </p>

<p>Knowing the structure of our data, can anyone think of our modeling approaches that would possibly produce better predictive results? I am still new to machine learning techniques, and I hope that someone might know of other techniques that could be employed.   </p>

<p>My strongest program language is R, but I can also do this analysis in Python or Stata. </p>
"
"0.107956825902732","0.109747936952945","100365","<p>We have route-level data (that I cannot share) on monthly bus ridership in New York City, creating a panel $N= 185$, $T=36$. We estimate a fixed effects model and random effects model with R's <code>plm</code> package. (For this MWE, I use the <code>Grunfeld</code> investment data, which illustrates the problems I am seeing fairly well).</p>

<pre><code>library(plm)
data(""Grunfeld"")
model_1A &lt;- lm(inv ~ value + capital, data= Grunfeld)
fe.plm &lt;- plm(formula(model_1A), model=""within"", index=c(""firm"", ""year""),
              data=Grunfeld)
re.plm &lt;- plm(formula(model_1A), model=""random"", index=c(""firm"", ""year""),
              data=Grunfeld)
</code></pre>

<p>A Hausman test indicates that the FE model is preferred, because the estimates differ (note that in the MWE, we fail to reject).</p>

<pre><code>phtest(fe.plm, re.plm)

##  Hausman Test

##data:  formula(model_1A)
##chisq = 2.3304, df = 2, p-value = 0.3119
##alternative hypothesis: one model is inconsistent

pbgtest(fe.plm)

##  Breusch-Godfrey/Wooldridge test for serial correlation in panel models

##data:  formula(model_1A)
##chisq = 65.0632, df = 20, p-value = 1.14e-06
##alternative hypothesis: serial correlation in idiosyncratic errors

pbgtest(re.plm)

##  Breusch-Godfrey/Wooldridge test for serial correlation in panel models

##data:  formula(model_1A)
##chisq = 69.9495, df = 20, p-value = 1.856e-07
##alternative hypothesis: serial correlation in idiosyncratic errors
</code></pre>

<p>This is where what I think should happen diverges from what many people try to do. My understanding of serial correlation is that it affects the standard errors but not the coefficients. This would suggest to me a serial correlation-robust standard error. For instance (<a href=""http://stats.stackexchange.com/a/60262/10026"">as in this answer</a>),</p>

<pre><code>fe.rse &lt;- sqrt(diag(vcovHC(fe.plm, type=""HC1"", cluster=""group"")))
re.rse &lt;- sqrt(diag(vcovHC(re.plm, type=""HC1"", cluster=""group"")))
</code></pre>

<p>** Why not just use sc-robust standard errors?**</p>

<p>But what many authors do instead is include specific AR(1) or ARMA disturbances because Stata makes this easy. For the FE models, we can use <code>gls</code> from the <code>nlme</code> package on demeaned data (note: <code>fe.plm</code> and <code>fe.gls</code> are virtually identical),</p>

<pre><code># within estimator is demeaned
demean &lt;- numcolwise(function(x) x - mean(x))
Grunfeld.dm &lt;- ddply(Grunfeld, .(firm), demean)
Grunfeld.dm$year &lt;- Grunfeld$year

fe.gls &lt;- gls(update(formula(model_1A), .~.-1), method=""ML"",  data=Grunfeld.dm)
fear.gls &lt;- update(fe.gls,  correlation = corAR1(form = ~ year | firm))
fearma.gls &lt;- update(fe.gls, correlation = corARMA(form = ~ year | firm, 
                                                   p=1,q=1))
</code></pre>

<p>The RE models can be estimated in with <code>lme</code> in the <code>nlme</code> package (again, <code>re.plm</code> and <code>re.lme</code> are identical).</p>

<pre><code>re.lme &lt;- lme(fixed = formula(model_1A), random = ~ 1|firm, data = mta)
rear.lme &lt;- update(re.lme,  correlation = corAR1(form = ~ year | firm))
rearma.lme &lt;- update(re.lme,  correlation = corARMA(form = ~ year | firm, 
                                                    p=1,q=1))
</code></pre>

<p>There are a few things I don't understand about this:</p>

<ul>
<li><p><strong>Why do the coefficients change when serial correlation doesn't (shouldn't?) affect estimates?</strong></p></li>
<li><p><strong>Can we still use the Hausman test to select between FE and RE models with autoregressive errors?</strong></p></li>
<li><p><strong>How can we test for residual autocorrelation? And if it exists, wouldn't we <em>still</em> need a robust standard error?</strong></p></li>
</ul>
"
"0.139371662943565","0.136436422649182","101077","<p>I have very big data and low number of observations. So I decided to use PCA to reduce dimension of the data. The following is R example (just an dummy example - for workout):</p>

<pre><code>xmat &lt;- matrix(sample(-1:1, 100000, replace = TRUE), ncol = 1000)
colnames(xmat) &lt;- paste (""V"", 1:1000, sep ="""")
rownames(xmat) &lt;- paste(""S"", 1:100, sep = """")
</code></pre>

<p>In this example dataset I have <code>1000</code> variables and <code>100</code> observations / subjects. </p>

<p>I am doing PCA. Lets say.</p>

<pre><code>out &lt;- princomp(xmat)
Error in princomp.default(xmat) : 
  'princomp' can only be used with more units than variables
</code></pre>

<p>Q1: is there a way to reduce dimensionality with <code>p &gt; n</code> ? I would like to use all variables information as opposed to representative ones. Without having proper solution I went anyway to use cluster analysis of variables to categorize the variables and pick the randomly from the clusters. </p>

<p>To create a list of representative variables I tried to cluster the variables.</p>

<pre><code># cluster variables 
d &lt;- dist(t(xmat), method = ""euclidean"") # distance matrix
fit &lt;- hclust(d, method=""ward"")
plot(fit)
groups = cutree(fit,40)
groupd &lt;- data.frame(var = names(groups), group = groups)
</code></pre>

<p>What I am thinking is randomly pick one variable from each group above and use this in PCA. Assume that I have the following y variable.</p>

<pre><code>set.seed(1234)
yvar.d &lt;- data.frame (subject = c(paste(""S"", 1:100, sep = """")), yvar = rnorm (100, 50,10))
</code></pre>

<p><strong>Here is my question</strong>: </p>

<ol>
<li>What could be statistical challenge of using cluster analysis ?</li>
<li><p>Can we use PCA scores in predictions of y. How ? Just multiple
regression or we can introduce something such as variance explained
by each components in the model ?</p>

<p><strong>Edits:</strong></p>

<p>Based on the discussions (see the comments below), I am using different function to do PC analysis.</p></li>
</ol>

<p>""The calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using eigen on the covariance matrix. This is generally the preferred method for numerical accuracy. The print method for these objects prints the results in a nice format and the plot method produces a scree plot."" - from function help. </p>

<pre><code>     out1 &lt;- prcomp(xmat)
      out1$x[1:3,1:3]
                      PC1        PC2       PC3
S1  2.940862 -2.7379835  6.527103
S2 -1.081124 -0.5294796 -0.276591
S3  2.375710  0.4505205 -4.236289

   out1$sdev
 screeplot(out1,npcs=30, type=""lines"",col=3) # 30 PCA plotted
</code></pre>

<p><img src=""http://i.stack.imgur.com/gMJys.jpg"" alt=""enter image description here""></p>

<pre><code> out1$rotation
</code></pre>

<p>I also come to see an example in SO <a href=""http://stackoverflow.com/questions/10876040/principal-component-analysis-in-r"">how to use PCA in prediction</a>. Here is my workout: </p>

<pre><code>## take our training and test sets
YY &lt;-  yvar.d$yvar 
prop &lt;- 0.5
train = sample(1:length(YY), round(length(YY)*prop,0))


# data for testing model purpose 
testid = setdiff (1:length(YY), train)
YY1 &lt;- YY
newXPCA &lt;- data.frame(out1$x)
test.data &lt;- data.frame (y = YY1[testid],newXPCA[testid,]) 
test.data[1:10,1:10]

train.data &lt;- data.frame(y= YY1[train],newXPCA [train,])
train.data[1:10,1:10]

## fit the PCA
pc &lt;- prcomp(train.data[, -1])
trainwPC &lt;- data.frame (y = train.data$y, pc$x)

model1 &lt;- lm(y ~ ., data = trainwPC)

#predict() method for class ""prcomp""
test.p &lt;- predict(pc, newdata = test.data)
pred &lt;- predict(model1, newdata = data.frame(test.p), type = ""response"")
pred 
Warning message:
In predict.lm(model1, newdata = data.frame(test.p), type = ""response"") :
  prediction from a rank-deficient fit may be misleading
</code></pre>

<p>I just adopted this script from the SO link, I am not sure about accuracy of the script. </p>

<p>I still have technical questions remaining such as clarification to <strong>remaining question 2</strong> above: </p>

<p>(1) If I want to split data into training and test set by sampling <code>50% of data</code> (as show in the script). Should I do just multiple regression with y and the <code>out1$x</code> ? how many components to use ? is variance of each component play role in good model selection such as avoid over-fitting ? How ? </p>

<p>(2) Clustering (using x clusters) vs PCA analysis (with subset of x components vs all ) what would be statistically favorite for predictions in the situations where have <code>p &gt; n</code> ? As I said to my mind the PCA analysis can use all information but I do not know if there is downside of such information such as <code>over-fitting</code> and ""error consumption"". </p>

<p>Worked example appreciated.   </p>
"
"NaN","NaN","101126","<p>I was wondering how can I best fit nonlinear regression model to this data, using R package. How can I check if model is good fitted since Rsquared value is not returned in most functions for nonlinear models?</p>

<p>Thanks in advance for answers
<img src=""http://ramza.tarchomin.pl/rozklad.jpeg"" alt=""data""></p>
"
"0.147521489716101","0.154513533558943","101187","<p>I know that this has been discussed before, but those discussions did not really answer my questions. I know how the ADF test works, but I am having trouble interpreting the output for the three options using the <code>ur.df</code> function in R (package: <code>urca</code>). Could someone walk me through the interpretations?
More specifically, what are <code>tau1</code>, <code>tau2</code>, <code>phi1</code>, <code>phi2</code>, and <code>phi3</code>? </p>

<pre><code>summary(ur.df(tcm.ts, type=""none"",selectlags=""BIC""))
summary(ur.df(tcm.ts, type=""drift"",selectlags=""BIC""))
summary(ur.df(tcm.ts, type=""trend"",selectlags=""BIC""))

&gt; summary(ur.df(tcm.ts, type=""none"",selectlags=""BIC""))

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression none 


Call:
lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)

Residuals:
    Min      1Q  Median      3Q     Max 
-95.199 -23.380  -6.608  26.885  86.560 

Coefficients:
           Estimate Std. Error t value Pr(&gt;|t|)   
z.lag.1     0.04398    0.01205   3.650  0.00183 **
z.diff.lag -0.03722    0.24417  -0.152  0.88053   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 45 on 18 degrees of freedom
Multiple R-squared:  0.7091,    Adjusted R-squared:  0.6768 
F-statistic: 21.94 on 2 and 18 DF,  p-value: 1.492e-05


Value of test-statistic is: 3.6495 

Critical values for test statistics: 
      1pct  5pct 10pct
tau1 -2.66 -1.95  -1.6

&gt; summary(ur.df(tcm.ts, type=""drift"",selectlags=""BIC""))

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression drift 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)

Residuals:
    Min      1Q  Median      3Q     Max 
-69.366 -24.625  -3.018  34.165  82.227 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -75.21181   45.89715  -1.639   0.1196  
z.lag.1       0.09756    0.03467   2.814   0.0119 *
z.diff.lag   -0.20396    0.25469  -0.801   0.4343  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 43.03 on 17 degrees of freedom
Multiple R-squared:  0.3596,    Adjusted R-squared:  0.2843 
F-statistic: 4.773 on 2 and 17 DF,  p-value: 0.02264


Value of test-statistic is: 2.814 8.6258 

Critical values for test statistics: 
      1pct  5pct 10pct
tau2 -3.75 -3.00 -2.63
phi1  7.88  5.18  4.12

&gt; summary(ur.df(tcm.ts, type=""trend"",selectlags=""BIC""))

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
   Min     1Q Median     3Q    Max 
-86.46 -14.84   5.56  20.87  70.29 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  91.4039    92.1407   0.992   0.3360  
z.lag.1      -0.1127     0.1082  -1.042   0.3129  
tt           13.0810     6.4318   2.034   0.0589 .
z.diff.lag   -0.1287     0.2369  -0.543   0.5946  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 39.54 on 16 degrees of freedom
Multiple R-squared:  0.4911,    Adjusted R-squared:  0.3957 
F-statistic: 5.148 on 3 and 16 DF,  p-value: 0.01109


Value of test-statistic is: -1.042 8.1902 6.7579 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -4.38 -3.60 -3.24
phi2  8.21  5.68  4.67
phi3 10.61  7.24  5.91
</code></pre>
"
"0.0808716413062113","0.0904347204435887","103077","<p>I am having a lot of fun with regression analysis at the moment, and by fun I mean bashing myself repeatedly over the head. I have a set of 200 data points, by filtering on a property of interest, I end up with 153 points of use. </p>

<p>I initially used these 153 points to generate a linear regression, with an excellent R${^2}$ and a plot of fitted vs actual variables of almost a perfect diagonal. Great! However, it was suggested that this might only be an internally predictive model (which as I understand it means the model fits the data, rather than the opposite). So, I then tried this: I randomly selected a sample of 100 of the 153 results, and built the same model, it still gave a relatively good fit. I then used the predict function in R to try to predict the outcome of the other 53 records. It did not go well. What I got was one of 2 things.</p>

<ol>
<li>the predictions made no sense at all, not even on the same scale as the actual values.</li>
<li>most of the predictions made sense (although weren't very accurate) and one or two, were on an entirely different scale (orders of magnitude larger, or smaller).</li>
</ol>

<p>Since the model I am fitting has time as the response variable, it was suggested I use a Gamma fit regression instead of a plain old linear regression. I tried this and ended up essentially with the result.</p>

<p>So, am I using R correctly, was Gamma a good choice for this? I'm pretty sure my data is good (non biased) so if I am unable to predict, despite the good model - does this mean my model is useless? I've been working on this for some weeks now, and it would be great if I could salvage something.</p>

<p>The R commands I have used:</p>

<pre><code>modelSet&lt;-sample(1:nrow(myData),100)
modelData&lt;-myData[modelSet,]
predictData&lt;-myData[-modelSet,]

fit&lt;-lm(""time~(x1+x2+x3+x4+x5+x6)^3"", data=modelData)
pred&lt;-predict(fit, predictData)
plot(predictData$time, pred) &lt;- gives a really not useful plot


fit2&lt;-glm(""time~(x1+x2+x3+x4+x5+x6)^3"", data=modelData, family=Gamma) # tried with link=log too
pred2&lt;-predict(fit2, predictData)
plot(predictData$time, pred2) &lt;- gives an even less useful plot
</code></pre>
"
"0.0758643241810882","0.0771229887279699","103280","<p>I'm attempting a project where I need to statistically rank available cars based on several variables such as cost, mpg, seating, milage, etc.. I wish to rank these cars in order decide which car would be the best choice (highest ""worth"") to buy (or best several cars if I was informing multiple people of the best cars to get). As the list of available cars changes from day to day, I will also need to re-run the code every day to allow the rankings to give me the best decision for this new day. </p>

<p>What statistical methods should I use to go about this ranking system? I plan on determining which factors I find most important so the variables used will be subjective in choice. I thought about trying MDS or clustering but I didn't know if that would be relevant since I'm already subjectively determining what variables are to be used. I don't see how regression can be used since I can't get a handle on the ""worth"" of previous cars as that is what I'm trying to rank by. Also, I will be attempting this in R so any helpful packages/functions would be great to know as well.</p>

<p>Any help with how to go about this ranking scheme would be helpful as I'm at a loss.</p>

<p>Thanks so much</p>
"
"0.0464572209811883","0.0314853283036575","103638","<p>I'm using R and the package boot.</p>

<p>my boot function returns the coefficients and the r.square and sqrt of the r.square </p>

<pre><code> rsq2 &lt;- function(formula, data, indices) {
     d &lt;- data[indices,] # allows boot to select sample 
     fit &lt;- lm(formula, data=d)
     return(c( coef(fit),summary(fit)$r.square,sqrt(summary(fit)$r.square)))
 }

boot(data=mtcars,rsq2,1000,formula=""mpg~wt"")

#ORDINARY NONPARAMETRIC BOOTSTRAP

Call:
boot(data = mtcars, statistic = rsq2, R = 1000, formula = ""mpg~wt"")


Bootstrap Statistics :
      original        bias    std. error
t1* 37.2851262  0.1170681005  2.32470420
t2* -5.3444716 -0.0557873180  0.70523904
t3*  0.7528328  0.0019940314  0.05758118
t4*  0.8676594  0.0004980607  0.03362469
</code></pre>

<p>I determine a pvalue using the original value and the std. error determined by the boot program</p>

<pre><code>pvalue of the regression coefficient
2*pnorm(-abs(-5.3444716/0.70523904))
[1] 3.502706e-14
pvalue of the CC
 2*pnorm(-abs(0.8676594/0.03362469))
[1] 7.947938e-147
</code></pre>

<p>Why are these not the same value?</p>

<p>In a simple univariable the linear regression of the p-value of the coefficient and Pearson's CC are the same value?</p>
"
"0.0379321620905441","0.0385614943639849","103670","<p>I have data in the form of these columns:</p>

<pre><code>date, x coordinate, y coordinate, value A, value B, value C, value D, etc. 
</code></pre>

<p>(I don't see the possibility to copy an Excel table into this text so I show just the headers of the columns)</p>

<p>Now I have read and Googled a lot already but I can't find the code to make a regression model in R predict value A (numerical) based on the date, coordinates and the rest of the values (both numerical and text, no negative or complex numbers). Does anyone knows which code in R should work best? </p>

<p>I already get nice regressions with the simple <code>lm()</code> function if I stick to years as a label instead of dates but then I of course lose the time-connection. For background its maybe handy to know that it is real estate data.  </p>
"
"0.100359067582572","0.102024124270123","104180","<p>Suppose X is the design matrix of my experiment of which I want to model a linear regression model. Such a design matrix can be created by (in this example it is a full factorial design)</p>

<pre><code>library(BHH2)
Des2 &lt;- ffDesMatrix(5)
</code></pre>

<p>Now, a contrast matrix can be obtained by the following code, corresponding to ( t(X) %x% X)^-1 %x% t(X) (in which %x% represents matrix multiplication):</p>

<pre><code>solve(t(Des2) %*% Des2) %*% t(Des2)
</code></pre>

<p>This results in a matrix giving the Y observations that will be contrasted against each other in order to obtain a parameter estimate (e.g. first row will correspond to beta1).</p>

<p>Now, one can also make a design matrix using the model.matrix function</p>

<pre><code>Des &lt;- as.data.frame(Des2)
names(Des) &lt;- c(""X1"",""X2"",""X3"",""X4"",""X5"")
model.matrix(~X1*X2*X3*X4*X5,data=Des)
</code></pre>

<p>This is a model matrix that can be used to estimate all possible interactions between five factorial variables with two levels.</p>

<p>When comparing the columns of the model matrix obtained by the last piece of code with the rows of our contrast matrix, one can note that they are rather similar. The values might be different, but the signs representing the contrasts are identical for a row/column representing the same parameter to be estimated.</p>

<p>My question now refers to these columns and rows: is there any difference to be noted between these two? Or are they identical as in that they both represent contrasts of the Y observations that will be used for estimation of the parameter, and have I thus summed up two equivalent way in obtaining these contrasts?</p>
"
"NaN","NaN","104370","<p>I need to run a cox-regression on a huge data set (millions) with only a few unique rows (hundreds) and regressing on categorical variable <code>group</code>. I could use just <code>Surv(t, c) ~ group</code>, but this would take a very long time. I was thinking about including only unique rows together with their count as a <code>weights</code> argument.</p>

<p>How would you do this for this specific example? Is it possible to do such a function for generic formula?</p>
"
"0.0889588054368324","0.0904347204435887","104595","<p>I've been reading <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a>, <a href=""http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html"" rel=""nofollow"">http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html</a>, and <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and I'm still a little lost on how to do a power analysis for my data. I want to be able to determine what N I should have if I have an interaction between a categorical variable (with 3 levels) and a continuous variable.</p>

<p><a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a> provides some information, but I can't figure out how to simulate the relationship between the categorical and continuous variables and outcome.</p>

<blockquote>
  <p>set.seed(1)<br></p>
  
  <p>repetitions = 1000<br>
  N = 10000<br>
  n = N/8<br>
  var1  = c(   .03,    .03,    .03,    .03,    .06,    .06,    .09,   .09)<br>
  var2  = c(     0,      0,      0,      1,      0,      1,      0,     1)<br>
  rates = c(0.0025, 0.0025, 0.0025, 0.00395, 0.003, 0.0042, 0.0035, 0.002)<br></p>
  
  <p>var1    = rep(var1, times=n)<br>
  var2    = rep(var2, times=n)<br>
  var12   = var1**2<br>
  var1x2  = var1 *var2<br>
  var12x2 = var12*var2<br></p>
  
  <p>significant = matrix(nrow=repetitions, ncol=7)<br></p>
  
  <p>startT = proc.time()[3]<br>
  for(i in 1:repetitions){<br>
   responses          = rbinom(n=N, size=1, prob=rates)<br>
   model              = glm(responses~var1+var2+var12+var1x2+var12x2, <br>
                            family=binomial(link=""logit""))<br>
   significant[i,1:5] = (summary(model)$coefficients[2:6,4]&lt;.05)&lt;br&gt;
&gt;      significant[i,6]   = sum(significant[i,1:5])&lt;br&gt;
&gt;      modelDev           = model$null.deviance-model$deviance<br>
   significant[i,7]   = (1-pchisq(modelDev, 5))&lt;.05<br>
  }<br>
  endT = proc.time()[3]<br>
  endT-startT<br></p>
  
  <p>sum(significant[,1])/repetitions      # pre-specified effect power for var1<br>
  [1] 0.042<br>
  sum(significant[,2])/repetitions      # pre-specified effect power for var2<br>
  [1] 0.017<br>
  sum(significant[,3])/repetitions      # pre-specified effect power for var12<br>
  [1] 0.035<br>
  sum(significant[,4])/repetitions      # pre-specified effect power for var1X2<br>
  [1] 0.019<br>
  sum(significant[,5])/repetitions      # pre-specified effect power for var12X2<br>
  [1] 0.022<br>
  sum(significant[,7])/repetitions      # power for likelihood ratio test of model<br>
  [1] 0.168<br>
  sum(significant[,6]==5)/repetitions   # all effects power<br>
  [1] 0.001<br>
  sum(significant[,6]>0)/repetitions    # any effect power<br>
  [1] 0.065<br>
  sum(significant[,4]&amp;significant[,5])/repetitions   # power for interaction terms<br>
  [1] 0.017<br></p>
</blockquote>

<p>I feel like I should be able to adapt the code from <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and that this would be a better, more succinct option</p>

<blockquote>
  <p>library(rms)</p>
  
  <p>tmpfun &lt;- function(n, beta0, beta1, beta2) { <br>
     x &lt;- runif(n, 0, 10) <br>
     eta1 &lt;- beta0 + beta1*x <br>
     eta2 &lt;- eta1 + beta2 <br>
     p1 &lt;- exp(eta1)/(1+exp(eta1)) <br>
     p2 &lt;- exp(eta2)/(1+exp(eta2)) <br>
     tmp &lt;- runif(n) <br>
     y &lt;- (tmp &lt; p1) + (tmp &lt; p2) <br>
     fit &lt;- lrm(y~x) <br>
     fit$stats[5] <br>
  } <br></p>
  
  <p>out &lt;- replicate(1000, tmpfun(100, -1/2, 1/4, 1/4)) <br>
  mean( out &lt; 0.05 ) <br></p>
</blockquote>

<p>but I'm not completely sure how to do so. I'm assuming tmpfun(100,-1/2, 1/4,1/4) is specifying the N and betas that you want, but how do I adjust tmpfun to another (categorical) variable and include an interaction term? Ultimately the equation should include 6 betas: the intercept, the beta for x, the beta for z1, the beta for z2, the interaction term between x and z1, and the interaction term between x and z2. </p>

<p>Finally, I can't find any reliable sources on what sorts of ""effect sizes"" I should be using as small or medium. </p>

<p>Let me know if I can provide more information!</p>
"
"NaN","NaN","104641","<p>I have some functions of $x$, in the form of $d\sqrt{x}$ or $d\log(x)$ where $d$ is known. I would like to rewrite (approximate is fine) them in the form $a/(1 + bx^c)$, where $a$, $b$ and $c$ are arbitrary.</p>

<p>I don't think there are $a$, $b$ and $c$ such that the two curves will match exactly, so I think maybe try to fit a nonlinear regression and find the closest $a$, $b$ and $c$. I tried the following R code, and I get the singular gradient error. </p>

<pre><code>x     = seq(1, 50000, by=1000)
y     = 50*sqrt(x)
model = nls(y ~ a/(1 + b*x^c), start=list(a=1, b=-0.01, c=0.01))
</code></pre>
"
"0.0547503599848004","0.0667904674542028","104968","<p>Sorry about asking such a basic question. </p>

<p>Assuming I have data like this </p>

<p><code>x Trial1 Trial2 Trial3 
 1    1.0    2.0    3.0
 2    1.1    2.1    3.1
 3    1.2    2.2    3.2
</code></p>

<p>How exaclty to I regress my predictor variable x onto my data. I was thinking naively that I could just take the average of the data trials and get something like</p>

<p><code>x y
 1    1.1
 2    2.1 
 3    3.1
</code>
for which I could simply use <code>model &lt;-lm(x ~ y)</code>. But I think I would introduce a bias by using the average. I was thinking instead to to create ordered pairs like (1,1.1), (1,2.0),...(3,2.2),(3,3.2). And use that for the regression but I'm not quite sure how to do this in a clean way in R. I was going to use data slicing and cbind a bunch of times, but I'm sure there is something better. </p>

<p><strong>An example of what I'm getting at</strong></p>

<p>The crux of my question is: how to do a regression model when you have data from mutiple runs of the same experiment. For example if I was looking at the temperature along a really thin metal wire (so its essentially 1-d), and then I mark off lenghts on this wire called x=0, x=1, x=2, ... x=total length. Then I take 10 temperature measuresments for each value of x. How do I create a regression model, which gives Temperarute as a function of length, i.e. T(x), given that I have 10 T values for each value of x? </p>
"
"0.0464572209811883","0.0472279924554862","105006","<p>Is there any way to perform bivariate regression using pairwise deletion of missing values in R? na.action options in lm() do not offer such a possibility â€“ the default na.action is na.omit, which is equivalent to listwise deletion. I already tried estimating the covariance matrix using pairwise deletion and then use the function mat.regress (package psych) with the pairwise covariance matrix. However, mat.regress is a function to compute multiple regression (not bivariate). Thank  you.</p>
"
"0.0663812836584521","0.0771229887279699","105346","<p>I am interested in estimating an adjusted risk ratio, analogous to how one estimates an adjusted odds ratio using logistic regression. Some literature (e.g., <a href=""http://aje.oxfordjournals.org/content/159/7/702.abstract"">this</a>) indicates that using Poisson regression with Huber-White standard errors is a model-based way to do this</p>

<p>I have not found literature on how adjusting for continuous covariates affects this. The following simple simulation demonstrates that this issue is not so straightforward: </p>

<pre><code>arr &lt;- function(BLR,RR,p,n,nr,ce)
{
   B = rep(0,nr)
   for(i in 1:nr){
   b &lt;- runif(n)&lt;p 
   x &lt;- rnorm(n)
   pr &lt;- exp( log(BLR) + log(RR)*b + ce*x)
   y &lt;- runif(n)&lt;pr
   model &lt;- glm(y ~ b + x, family=poisson)
   B[i] &lt;- coef(model)[2]
   }
   return( mean( exp(B), na.rm=TRUE )  )
}

set.seed(1234)
arr(.3, 2, .5, 200, 100, 0)
[1] 1.992103
arr(.3, 2, .5, 200, 100, .1)
[1] 1.980366
arr(.3, 2, .5, 200, 100, 1)
[1] 1.566326 
</code></pre>

<p>In this case, the true risk ratio is 2, which is recovered reliably when the covariate effect is small. But, when the covariate effect is large, this gets distorted. I assume this arises because the covariate effect can push up against the upper bound (1) and this contaminates the estimation.</p>

<p>I have looked but have not found any literature on adjusting for continuous covariates in adjusted risk ratio estimation. I am aware of the following posts on this site: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/18595/poisson-regression-to-estimate-relative-risk-for-binary-outcomes"">Poisson regression to estimate relative risk for binary outcomes</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38004/poisson-regression-for-binary-data"">Poisson regression for binary data</a></li>
</ul>

<p>but they do not answer my question. Are there any papers on this? Are there any known cautions that should be exercised? </p>
"
"NaN","NaN","105406","<p>I'm trying to run K-fold cross-validation on a multiple regression model that was generated via the <code>step</code> function in R. However, the call to <code>cv.glm</code> returns <code>NaN</code>.</p>

<p>Here is a simplified script that replicates the problem:</p>

<pre><code># this works just fine
library(boot)
data(mammals, package=""MASS"")
mammals.glm &lt;- glm(log(brain) ~ log(body), data = mammals)
cv.glm(mammals, mammals.glm, K=5)$delta

# but when I use the step function...
mammals.simple &lt;- lm((""brain ~ 1""), data=mammals)
mammals.glm &lt;- step(mammals.simple, direction=""forward"", test=""F"", scope=""brain ~ body"")
# it returns NaN
cv.glm(mammals, mammals.glm, K=5)$delta
</code></pre>

<p>Is this due to some fundamental misunderstanding, where it doesn't make sense to cross-validate a stepwise-generated model?</p>

<p>I'm using R v3.0.1 (but 3.1.0 has the same problem).</p>
"
"0.0379321620905441","0.0385614943639849","105424","<p>I'm trying to do a linear regression in R, however I have the added constraint the coefficients from the linear regression need to sum to a user given value between 0 and 1. (I understand forcing the coefficients like this will make the fit rather poor, but it's a needed constraint)</p>

<p>I've been though the documentation for lm() and the glmc package, as well as some similar questions, but none seem to tackle how to get the coefficients to sum to a specific value.</p>

<p>For data T, N, and target sum for coefficients p:</p>

<pre><code>Regression &lt;- function(T, N, p) {    
     fit &lt;- lm(T[,1] ~ N)
     coef &lt;- coef(fit)
...
}
</code></pre>

<p>Ideally I want sum(coef[-1]) = p (ignoring the intercept)</p>

<p>Sorry I can't provide more code, but I don't think I can do what I would like with lm().</p>

<p>Does anyone know of a way to do this, or of a package that will let me do this? </p>
"
"0.0599760143904067","0.0609710760849692","106064","<p>I am trying to fit an exponential model to some data. The data is:</p>

<pre><code>Wavelength  aCDOM
350.01  0.80605
350.22  0.78302
350.43  0.78302
350.64  0.78302
350.85  0.78302
351.06  0.78302
351.27  0.78302
351.48  0.78302
351.68  0.75999
351.89  0.75999
352.1   0.75999
352.31  0.75999
352.52  0.75999
352.73  0.73696
352.94  0.73696
353.15  0.73696
353.36  0.73696
353.57  0.73696
353.78  0.73696
353.99  0.73696
354.2   0.73696
354.41  0.73696
354.62  0.73696
354.83  0.73696
355.04  0.73696
355.25  0.71393
355.46  0.71393
355.67  0.71393
</code></pre>

<p>I know that the best model fit this type of data is an exponential function in the for <code>y ~ a * exp(-b * c)</code> where <code>a</code> is the absorbance estimate at the reference wavelength, <code>b</code> is the spectral slope which is the value I am looking for, and <code>c</code> is the wavelength minus the reference wavelength (440nm). I have tried using the <code>nls</code> function by estimating the starting parameters following the method outlined in Fox (2002) but I keep getting the following error in <code>nlsModel(formula, mf, start, wts)</code>:</p>

<pre><code>singular gradient matrix at initial parameter estimates
</code></pre>

<p>The formula I am using is <code>model1 &lt;- nls(St0104 ~ a * exp(-b * (Wavelength-440)), start = list(a=0.1, b=0.0012), trace=T)</code>, I'm not even if I have calculated the starting parameters correctly. </p>

<p>I have tried instead to plot the log of <code>Y</code> and do linear as well as polynomial regression, but I know this is not correct for this data and the residuals are horrible. I'm pretty new to R but I have tried everything I can but this is driving me nuts can anybody help please?</p>
"
"0.0464572209811883","0.0472279924554862","106165","<p>I have a data set derived from administrative registers covering the population of a small European country, containing a large number of defined groups (15 000+). For each of these groups I have multiple observations (different years), and two explanatory variables that characterize the exposure of each group to a policy intervention.</p>

<p>In STATA I would estimate this using the regression command, with aweight=group_size and using absorb(group_identifier). I've done this before, and it is extremely fast. How do I do this in R?  In R, my problem is that I cannot find a package that allows me to both weight by group size AND include fixed effects.</p>

<p>Using the standard lm function I could include the group-size weight, but would have to estimate the group-effects explicitly (simply adding the groups identifier as a factor variable). This is extremely time consuming (I cancelled it after it had run for several minutes).</p>

<p>Alternatively, I could turn to fixed-effects packages - such as the lfe package. These allow for fixed effects, but I have not found any such package that also accepts weighting by group size.</p>

<p>Any suggestions?</p>
"
"0.0848188929679971","0.0776035104406608","106360","<p>I am running a binomial mixed effects logistic regression in R using <code>glmer</code> for a sociolinguistics project. I was asked to used deviation (effect) coding. From what I gather, in deviation coding the last level in a factor is assigned -1, because this is the level that is never compared to the other levels within that variable. Is it possible to obtain the <code>Estimate</code> (<code>Exp(B)</code> value) for the last level as well by using function <code>relevel</code>? I need to report the estimates for all the levels.</p>

<p>For example, my model has the independent variable called <strong>Orthography</strong> with four levels (<code>s</code>, <code>sh</code>, <code>s1</code>, <code>sh1</code>). The dependent variable is <strong>produced sibilant</strong>. In deviation coding the fourth level (<code>sh1</code>) will not be compared to the other three levels, and estimates will be available for the first three (<code>s</code>, <code>sh</code>, <code>s1</code>). The intercept is the mean of the means of all four levels (<code>s + sh + s1 + sh1 / 4</code>). I am interested in obtaining the estimate for the last level (<code>sh1</code>) as well. Does anyone know how to get that? Do I have to rerun the model by changing levels? If so, does anyone know how to do that? I have been unsuccessful with using function <code>relevel</code> to do this.</p>

<p>I have other terms in my model as well:  </p>

<ul>
<li>following segment, which has two levels (<code>vowel</code>, <code>consonant</code>), </li>
<li>position of sibilant in word (<code>initial</code>, <code>medial</code>, <code>final</code>), </li>
<li>grammatical function (<code>noun</code>, <code>verb</code>, <code>adjectives</code>), and </li>
<li>language of instruction (<code>English</code>, <code>Gujarati</code>).</li>
</ul>

<p>This is the code for my model:</p>

<pre><code>model.final_si = glmer(prod_sib ~ orthography + foll_segment + word_position + 
                                  grammatical_func + language_instruction + 
                                  (1|participant) + (1|item), 
                       family=""binomial"",data=data)
</code></pre>
"
"NaN","NaN","106394","<p>I have 15 subjects each with 200 trials &amp; I'd like to run separate regressions for each subject.</p>

<p>If I just run the regression on the whole dataset I am able to generate odds ratios / confidence intervals from it:</p>

<pre><code>acc.log = glm(response ~ phdot + sdot, data=fullerr, family=""binomial"")
summary(acc.log)

exp(acc.log$coefficients)
exp(confint(acc.log))
</code></pre>

<p>But then when I fit each subject separately (I've tried using <code>by</code>), I am an unsure how to then extract information like odds ratios (standard errors &amp; p values are also no longer shown in the output).</p>

<pre><code>split.acc &lt;- by(fullerr, fullerr[,""subject""], function(fullerr){ 
                  glm(response ~ phdot + sdot, data=fullerr, family=""binomial"")
                })
</code></pre>
"
"0.0709645772411954","0.072141950116023","107507","<p>Lets say that I have ""y"" that I want to model with linear regression. ""x"" and ""z"" are the things I'm interrested in showing folks, but I also have things that I want to adjust for, but not really show in my plot.</p>

<p>Now, I would like to show my coefficients in a plot, but I would like to keep the things I adjusted for out of it. So perhaps this could be coronary artery calcification modeled as ""CAC ~ SomeBloodStuff + Bloodpressure + BMI + Smoking_status"". The BMI and Smoking_status would be something that I would want to take into account, but just note that I have adjusted for them.</p>

<p>I gather this is how to do the model:</p>

<pre><code>MyModel &lt;- lm( CAC ~ SomeBloodstuff + BP + BMI + Smoking_status, data=MyData)
summary(MyModel) 
coefficients(MyModel)
confint(MyModel, level=0.95) # Seems like a legit model.
</code></pre>

<p>The coefplot function in arm library gives just the sort of presentation that I want, but shows all the dimensions.</p>

<pre><code>library(arm)
coefplot(MyModel)
</code></pre>

<p>How could I leave those few variables out of the plot while keeping them in the regression and get a plot that looks like what the coefplot produces?</p>
"
"0.018966081045272","0.0192807471819925","107584","<p>I adjust the partial least squares regression for one categorical factor (2 levels â€“ <code>be</code> or <code>nottobe</code>) with with the <code>pls</code> package in R. I try to use <code>round()</code> function in the predict values for take the decision if the result are the first or second level in my factor. Does this approach sound correct?</p>

<pre><code>require(pls) 

#Artificial data  

T&lt;-as.factor(sort(rep(c(""be"", ""nottobe""), 100))) 

y1 &lt;- c(rnorm(100,1,0.1),rnorm(100,1,0.1)) 
y2 &lt;- c(rnorm(100,10,0.3),rnorm(100,10,0.6)) 
y3 &lt;- c(rnorm(100,10,2.3),rnorm(100,11,2.6)) 
y4 &lt;- c(rnorm(100,5,0.5),rnorm(100,7,0.5)) 
y5 &lt;- c(rnorm(100,0,0.1),rnorm(100,0,0.1)) 

#Create the data frame 
avaliacao &lt;- as.numeric(T) 
espectro &lt;- cbind(y1,y2,y3,y4,y5) 
dados &lt;- data.frame(avaliacao = I(as.matrix(avaliacao)), bands = I(as.matrix(espectro))) 

#PLS regression
taumato &lt;- plsr(avaliacao ~ bands, ncomp = 5, validation = ""LOO"", data=dados) 
summary(taumato) 

#Components analysis 
plot(taumato, plottype = ""scores"", comps = 1:5) 


#Cross validation 
taumato.cv &lt;- crossval(taumato, segments = 10) 
plot(MSEP(taumato.cv), legendpos = ""topright"") 
summary(taumato.cv, what = ""validation"") 
plot(taumato, xlab =""mediÃ§Ã£o"", ylab=""prediÃ§Ã£o"", ncomp = 3, asp = 1, main="" "", line = TRUE) 


#Predition for 3 components 
T&lt;-as.factor(sort(rep(c(""be"", ""nottobe""), 50))) 

y1 &lt;- c(rnorm(100,1,0.1),rnorm(100,1,0.1)) 
y2 &lt;- c(rnorm(100,10,0.3),rnorm(100,10,0.6)) 
y3 &lt;- c(rnorm(100,10,2.3),rnorm(100,11,2.6)) 
y4 &lt;- c(rnorm(100,5,0.5),rnorm(100,7,0.5)) 
y5 &lt;- c(rnorm(100,0,0.1),rnorm(100,0,0.1)) 

espectro2 &lt;- cbind(y1,y2,y3,y4,y5) 
new.dados &lt;- data.frame(bands = I(as.matrix(espectro2))) 
round(predict(taumato, ncomp = 3, newdata = new.dados))##
</code></pre>
"
"0.053644178078582","0.0545341883149212","107643","<p>Let a linear regression model obtained by the R function lm would like to know if it is possible to obtain by the Mean Squared Error command.</p>

<p>I had the FOLLOWING output of an example</p>

<pre><code>&gt; lm &lt;- lm(MuscleMAss~Age,data)
&gt; sm&lt;-summary(lm)
&gt; sm

Call:
lm(formula = MuscleMAss ~ Age, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-16.1368  -6.1968  -0.5969   6.7607  23.4731 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 156.3466     5.5123   28.36   &lt;2e-16 ***
Age          -1.1900     0.0902  -13.19   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 8.173 on 58 degrees of freedom
Multiple R-squared:  0.7501,    Adjusted R-squared:  0.7458 
F-statistic: 174.1 on 1 and 58 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Multiple R-squared is the sum square error? if the answer is no could explain the meaning of Multiple R-squared and Multiple R-squared</p>
"
"0.0808716413062113","0.0904347204435887","107899","<p>I have been studying a linguistic construction (letâ€™s call it <code>C</code>) in a language <code>L1</code> trying to individuate several factors (<code>F1</code>, <code>F2</code>, ..., <code>Fn</code>) which can influence / trigger the presence of <code>C</code> in <code>L1</code>. All factors are categorical variables with several levels.  </p>

<p>I ran basic significance tests (Goodness of fit) which revealed a high level of significance for each factors involved.</p>

<p>That said, I would now like to measure the interaction of all factors / all levels. As for the method, I think that Poisson Regression is what I am looking for since â€œCâ€ has no levels and can be represented in terms of frequencies. I thus tried Poisson regression in R but I have soon run into several problems. </p>

<p>I have two questions in particular:</p>

<ol>
<li><p>The data frame I used looked like that (here you can find only a tiny part of it as an example):</p>

<pre><code>Subj (Factor1)   NP (F2)    New (F3)    EXP_CON (F4)
</code></pre>

<p>To get the frequencies of all variable combinations, I used the function <code>ftable</code> and then I went on manually computing the frequencies. Needless to say, it was a painful procedure and it took a lot of time! </p>

<p>Does R provide an easier and quicker way to get the frequencies for all the interacting factors / levels (possibly eliminating automatically the interactions whose frequency is zero)? In other words, I would like to obtain something like that:</p>

<pre><code>Sub (F1)     NP (F2)      EXP_CON (F3)         New (F4)        (Freq) 6
</code></pre></li>
<li><p>After that, I tried to run a Poisson regression model but the result was puzzling. Here is an example of what I got:</p>

<pre><code>Syn_FunOther:Focus_CCIMP_CON:IS_CCGiven        NA         NA      NA       NA
Syn_FunSub:Focus_CCIMP_CON:IS_CCGiven          NA         NA      NA       NA
Syn_FunOther:Focus_CCNO_CON:IS_CCGiven         NA         NA      NA       NA
Syn_FunSub:Focus_CCNO_CON:IS_CCGiven           NA         NA      NA       NA
Syn_FunOther:Focus_CCNOV:IS_CCGiven            NA         NA      NA       NA
</code></pre></li>
</ol>

<p>I do not really understand. Why are there so many <code>NA</code> values?  </p>
"
"0.149412044409433","0.151890938740929","107951","<p>The question I'm asking is related to this question <a href=""http://stats.stackexchange.com/questions/105611/dealing-with-non-normal-distribution-in-big-datasets-when-do-we-throw-out-the/106301#106301"">here</a> and <a href=""http://stats.stackexchange.com/questions/105611/dealing-with-non-normal-distribution-in-big-datasets-when-do-we-throw-out-the/106301#106301"">here</a>. And I apologize for asking so many questions here, as I am thoroughly a stats novice and probably my MD thinking is clouding my ability to interpret my findings. </p>

<p>So, because of the input I've received and following some contemplation, I've decided to run a Cox proportional hazards model on this dataset regarding the treatment of patients and how this affects their time of wound healing. As the duration ""wound healing"" is affected by censoring, following @Glen_b and @Frank Harrell's input, I believe running a normal linear and/or Poisson regression model (as the days are counted and the Poisson's residual qqplots actually have a more linear/normal distribution) isn't quite feasible, because neither of these models actually account for the fact, that when patients are healed, they drop out of the ""study"" and thus are ""censored"". Therefore I've decided to explore Cox proportional hazards modelling a bit further. </p>

<p>Although not originally planned from a study endpoint perspective (long story), I've decided to chose the need for operative intervention (because the wound fails to heal) as the time to event here (also because I need an event variable in the <code>coxph</code> and <code>Surv</code> function in R). 
The study sample size is 4918 patients, of which 575 required operative treatment (11.7%). Now I'm not even sure if this is a high enough event rate as the independent variables are n = 14 and if I go by the ""rule of thumb"" of 10 events per predictor variable, I'd actually require 1400 events (although I read <a href=""http://aje.oxfordjournals.org/content/165/6/710.full"" rel=""nofollow"">here</a>, that this rule can sometimes be relaxed), but that's also one of the reasons why I'm posting my question here. </p>

<p>I've run a multivariable Cox proportional hazards model, adjusting for those variables that may influence the decision to ""treat"" a patient and or/wound healing dynamics as well as the treatment the patients received. </p>

<p>The output looks as follows: </p>

<pre><code>library(survival)
multicoxph&lt;-coxph(Surv(Timetoheal,Operation==""Yes"")~FA
+Age+Woundsurface+Mechanism+Sup+Mid+Deep)
summary(multicoxph)
Call:
coxph(formula = Surv(Timetoheal, Operation == ""Yes"") ~ FA + Age + 
    Woundsurface + Mechanism + Sup + Mid + Deep)

  n= 4877, number of events= 575 
   (41 observations deleted due to missingness)

                         coef exp(coef)  se(coef)      z Pr(&gt;|z|)    
FAInadequate        -0.225084  0.798449  0.089056 -2.527   0.0115 *  
Age                 -0.012107  0.987966  0.002401 -5.042 4.60e-07 ***
Woundsurface         0.042953  1.043889  0.017338  2.477   0.0132 *  
Mechanism1          -0.341706  0.710557  0.168422 -2.029   0.0425 *  
Mechanism2          -0.169782  0.843848  0.312529 -0.543   0.5870    
Mechanism3          -0.421703  0.655929  0.523798 -0.805   0.4208    
Mechanism4          -0.131178  0.877062  0.175603 -0.747   0.4551    
Mechanism5           0.071292  1.073895  0.313708  0.227   0.8202    
Mechanism6          -1.132178  0.322331  0.473741 -2.390   0.0169 *  
Mechanism7          -1.216408  0.296292  0.308236 -3.946 7.94e-05 ***
Mechanism8          -0.011187  0.988876  0.162881 -0.069   0.9452    
SupYes              -1.476173  0.228511  1.121847 -1.316   0.1882    
MidYes              -0.056176  0.945373  1.010415 -0.056   0.9557    
DeepYes              1.568644  4.800135  1.002629  1.565   0.1177    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

                    exp(coef) exp(-coef) lower .95 upper .95
FAInadequate           0.7984     1.2524   0.67057    0.9507
Age                    0.9880     1.0122   0.98333    0.9926
Woundsurface           1.0439     0.9580   1.00901    1.0800
Mechanism1             0.7106     1.4073   0.51079    0.9885
Mechanism2             0.8438     1.1850   0.45734    1.5570
Mechanism3             0.6559     1.5246   0.23496    1.8311
Mechanism4             0.8771     1.1402   0.62167    1.2374
Mechanism5             1.0739     0.9312   0.58067    1.9861
Mechanism6             0.3223     3.1024   0.12737    0.8157
Mechanism7             0.2963     3.3750   0.16194    0.5421
Mechanism8             0.9889     1.0112   0.71862    1.3608
SupYes                 0.2285     4.3762   0.02535    2.0598
MidYes                 0.9454     1.0578   0.13048    6.8497
DeepYes                4.8001     0.2083   0.67269   34.2526

Concordance= 0.746  (se = 0.017 )
Rsquare= 0.066   (max possible= 0.769 )
Likelihood ratio test= 331.9  on 14 df,   p=0
Wald test            = 240.3  on 14 df,   p=0
Score (logrank) test = 288.5  on 14 df,   p=0
</code></pre>

<p>Now if I understand my results correctly, this is telling me that the variable <code>FAInadequate</code> reduces the hazards for the event which in this case is ""requirement for an operation"", right? Now this is a bit bizarre, because if I perform simple comparative statistics, I find that FAInadequate patients have longer healing times and more operations as shown by these results: </p>

<pre><code>t.test(log(Timetoheal+0.5)~FA, var.equal=T)

    Two Sample t-test

data:  log(Timetoheal + 0.5) by FA
t = -7.9285, df = 4916, p-value = 2.724e-15
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.2773227 -0.1673666
sample estimates:
  mean in group Adequate mean in group Inadequate 
                2.338534                 2.560879 
</code></pre>

<p>and this</p>

<pre><code>require(gmodels)
CrossTable(Operation, FA, format=""SPSS"", chisq=T)

   Cell Contents
|-------------------------|
|                   Count |
| Chi-square contribution |
|             Row Percent |
|          Column Percent |
|           Total Percent |
|-------------------------|

Total Observations in Table:  4877 

             | FA 
    Operation|   Adequate | Inadequate |  Row Total | 
-------------|------------|------------|------------|
          No |      2583  |      1719  |      4302  | 
             |     2.764  |     3.835  |            | 
             |    60.042% |    39.958% |    88.210% | 
             |    91.143% |    84.141% |            | 
             |    52.963% |    35.247% |            | 
-------------|------------|------------|------------|
         Yes |       251  |       324  |       575  | 
             |    20.682  |    28.690  |            | 
             |    43.652% |    56.348% |    11.790% | 
             |     8.857% |    15.859% |            | 
             |     5.147% |     6.643% |            | 
-------------|------------|------------|------------|
Column Total |      2834  |      2043  |      4877  | 
             |    58.109% |    41.891% |            | 
-------------|------------|------------|------------|


Statistics for All Table Factors


Pearson's Chi-squared test 
------------------------------------------------------------
Chi^2 =  55.971     d.f. =  1     p =  7.354796e-14 

Pearson's Chi-squared test with Yates' continuity correction 
------------------------------------------------------------
Chi^2 =  55.29973     d.f. =  1     p =  1.03483e-13 


       Minimum expected frequency: 240.8704 
</code></pre>

<p>The fact of the matter is, that in plain ""medical"" terms, the longer it takes a wound to heal, the worse the outcome for the patient, so I would've originally expected the inadequate treatment to increase the hazards, especially in light of the simple t-test cross-tabulation results. Or could it be, that in light of this, running a proportional hazards function is incorrect and I should be looking at cumulative hazards? If that is the case how can I run a cumulative hazards function in R?  Further, I'm a bit concerned, that the likelihood ratio, logrank and Wald test all resulted in p-values = 0. Finally, how would you ideally chose to cross-validate this model and adjust for the slightly ""too low"" event per variable rate? </p>

<p>I'm sorry for all the questions, but this has been thoroughly been driving me nuts over the last couple of days, as I can't really wrap my head around the output for some reason. </p>

<p>I appreciate any help greatly. </p>

<p>Thanks. </p>
"
"0.0309714806541255","0.0472279924554862","108035","<p>I am trying to use the R package ""urca"". It has functions <code>cajorls</code> and <code>cajools</code>, which do OLS regression of restricted and unrestricted VECM respectively. Both functions take in the VECM estimated using Johansen procedure and return the regression parameters and cointegrating vectors.</p>

<p>I do not understand, why is there a need to do OLS regression of VECM? Using Johansen test on the data (by calling <code>ca.jo</code>) will give me the cointegrating vectors. What is the difference between cointegrating vectors of VECM and OLS regression of VECM?</p>
"
"0.0758643241810882","0.0578422415459774","108051","<p>I am trying to run a probit regression using panel data in R by first computing the log likelihood and then using the <code>optim</code> function to optimize. </p>

<ol>
<li><p>Scale of predictor variables: The predictor variables that I have vary in scale significantly. Some of the variables are in tens while some are in '000s. When I am trying to run the <code>optim</code> function, it frequently gives the error message: <code>vmmin is not finite</code>. So I have to be extremely careful in choosing the initial values. And some of these initial values are as small as 0.0001. Is this standard in probit? Or do we need to normalize the predictor variables before using probit?</p></li>
<li><p>I have a few variables that are computed within the estimation code, i.e., some of the parameters that are being estimated are used to calculate these inbuilt covariates. When I run the full model with all such inbuilt parameters, the <code>optim</code> function gives me a singular Hessian matrix. Again, I am not sure what the singularity of the hessian implies. When I the add the inbuilt covariates one at a time, the Hessian is not singular. </p></li>
</ol>

<p>Has anyone encountered such problems before? Please advise. </p>
"
"0.053644178078582","0.0545341883149212","108256","<p>I wish to test my time series data for volatility clustering, i.e. conditional heteroskedasticity.</p>

<p>So far, I have used the ACF test on the squared and absolute returns of my data, as well as the Ljung-Box test on the squared data (i.e. McLeod.Li.test).</p>

<p>In a recent paper (<a href=""http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1771862"" rel=""nofollow"">http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1771862</a>, the test is reported on page 8) co-authored by a well-known researcher, they have employed the White test (<a href=""http://ideas.repec.org/a/ecm/emetrp/v48y1980i4p817-38.html"" rel=""nofollow"">http://ideas.repec.org/a/ecm/emetrp/v48y1980i4p817-38.html</a>) to directly test for heteroskedasticity.</p>

<p>I have tried the same approach, however was unable to do so.
From my understanding, the White test needs residual variance (usually from a linear regression model) as an input.</p>

<p>Now my question is: How did the researchers perform the White test? I do not understand which inputs they used for their White test.</p>

<p>While searching for solutions, I have found the sandwich package which uses the vcovHC and vcovHAC functions to estimate a heteroskedasticity-consistent covariance matrix, however the input is also a fitted linear regression model..</p>
"
"0.110761366336029","0.118854507916379","108315","<p>I am running multinomial logistic regression analysis on my data.  The response variable is the number of calves produced each year (0,1, or 2).  I am trying to evaluate the influence of the <em>X</em> variables on the odds of producing a calf.  My <em>X</em> variables are predation risk (WR; continuous), age of mother (age; categorical or continuous), time (wolf; categorical).</p>

<p>First, I have 4 different age classification schemes (i only show 2) - I want to know which one of the age of mother would be ""best"" to use. I could use it as continuous variable - or as categories based on biological reasoning for senescence in older moose (old ladies don't invest in reproduction as much).  So, I thought I would use a likelihood ratio test.</p>

<pre><code>library(mlogit)

modata.model1 &lt;- mlogit(no.C ~ 1 | 1, data=modata, reflevel=""1"", na.action = na.omit) 
modata.model2 &lt;- mlogit(no.C ~ 1 | age, data=modata, reflevel=""1"", na.action = na.omit) 
modata.model3 &lt;- mlogit(no.C ~ 1 | age2, data=modata, reflevel=""1"", na.action = na.omit)  
</code></pre>

<hr>

<pre><code> lrtest(modata.model2,modata.model3)

Likelihood ratio test

Model 1: no.C ~ 1 | age
Model 2: no.C ~ 1 | age2
  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
1   4 -213.22                         
2   4 -207.57  0 11.309  &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>QUESTION: to interpret this output - there was a significant difference in the loglikelihood when we comparing the continuous age to a categorical age with 2 classes.  The loglik is smaller for model 1 and therefore it would be better to use? Or do I have that backwards? </p>

<p>Next I was going to use the Walds test to evaluate nested models.  To see if the addition of a variable was worth it.</p>

<pre><code>modata.model7 &lt;- mlogit(no.C ~ 1 | age+WR, data=modata, reflevel=""1"", na.action = na.omit) 

modata.model8 &lt;- mlogit(no.C ~ 1 | age+WR+wolf, data=modata, reflevel=""1"", na.action = na.omit) 
</code></pre>

<hr>

<pre><code>Wald test

Model 1: no.C ~ 1 | age + WR
Model 2: no.C ~ 1 | age + WR + wolf
  Res.Df Df  Chisq Pr(&gt;Chisq)
1    244                     
2    242  2 0.5828     0.7472
</code></pre>

<p>QUESTION: this tells me that there is no significant improvement when there is an additional variable of wolf added??  So, then I can use the smaller model or do I use the one with the smaller Res.DF?</p>

<p>In addition to confirming my interpretations of the results I have 2 side questions...  </p>

<p>1)to get the null model for <code>mlogit</code> library - is my <code>modata.model1</code> correct?  I want the intercept only model to compare against.</p>

<p>2) Hosmer and Lemshow suggest by getting Wald values to get significance levels for each coefficient - in mlogit, thats the same as using <code>summary(model)</code> and there they provide the t-values with p instead of needing to do an additional Walds test? (NOTE in the below model i use 2 category age class instead of continuous)</p>

<pre><code>summary(modata.model8)

Call:
mlogit(formula = no.C ~ 1 | age2 + WR + wolf, data = modata, 
    na.action = na.omit, reflevel = ""1"", method = ""nr"", print.level = 0)

Frequencies of alternatives:
    1     0     2 
0.652 0.244 0.104 

nr method
6 iterations, 0h:0m:0s 
g'(-H)^-1g = 7.4E-06 
successive function values within tolerance limits 

Coefficients :
              Estimate Std. Error t-value Pr(&gt;|t|)   
0:(intercept)  0.38730    0.40144  0.9648 0.334657   
2:(intercept) -2.40317    1.04546 -2.2987 0.021523 * 
0:age21       -1.39607    0.43989 -3.1737 0.001505 **
2:age21        0.53952    1.07275  0.5029 0.615012   
0:WR          -1.46584    0.64797 -2.2622 0.023686 * 
2:WR          -0.19214    0.60856 -0.3157 0.752206   
0:wolf1        0.56055    0.63292  0.8857 0.375797   
2:wolf1        0.42642    0.72375  0.5892 0.555744   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -203.11
McFadden R^2:  0.053606 
Likelihood ratio test : chisq = 23.009 (p.value = 0.00079359)
</code></pre>
"
"0.0967084173462244","0.0907503748778111","108374","<p>I have a monthly time series with an intervention and I would like to quantify the effect of this intervention on the outcome. I realize the series is rather short and the effect is not yet concluded.</p>

<p><strong>The Data</strong></p>

<pre><code>  cds&lt;- structure(c(2580L, 2263L, 3679L, 3461L, 3645L, 3716L, 3955L, 
    3362L, 2637L, 2524L, 2084L, 2031L, 2256L, 2401L, 3253L, 2881L, 
    2555L, 2585L, 3015L, 2608L, 3676L, 5763L, 4626L, 3848L, 4523L, 
    4186L, 4070L, 4000L, 3498L), .Dim = c(29L, 1L), .Dimnames = list(
        NULL, ""CD""), .Tsp = c(2012, 2014.33333333333, 12), class = ""ts"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/lNOEk.jpg"" alt=""enter image description here""></p>

<p><strong>The methodology</strong></p>

<p>1) The pre-intervention series (up until October 2013) was used with the <code>auto.arima</code> function. The model suggested was ARIMA(1,0,0) with non-zero mean. The ACF plot looked good.</p>

<pre><code>pre&lt;-window(cds,start = c(2012,01), end=c(2013,09))

mod.pre&lt;-auto.arima(log(pre))

Coefficients:
         ar1  intercept
      0.5821     7.9652
s.e.  0.1763     0.0810

sigma^2 estimated as 0.02709:  log likelihood=7.89
AIC=-9.77   AICc=-8.36   BIC=-6.64
</code></pre>

<p>2) Given the plot of the full series, the pulse response was chosen below, with T = Oct 2013,</p>

<p><img src=""http://i.stack.imgur.com/YU3nB.jpg"" alt=""enter image description here""></p>

<p>which according to cryer and chan can be fit as follows with the arimax function:</p>

<pre><code>   mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
            xtransf=data.frame(Oct13=1*(seq(cds)==22)),
            transfer=list(c(1,1))
          )

    mod.arimax


Series: log(cds) 
ARIMA(1,0,0) with non-zero mean 

Coefficients:
         ar1  intercept  Oct13-AR1  Oct13-MA0  Oct13-MA1
      0.7619     8.0345    -0.4429     0.4261     0.3567
s.e.  0.1206     0.1090     0.3993     0.1340     0.1557

sigma^2 estimated as 0.02289:  log likelihood=12.71
AIC=-15.42   AICc=-11.61   BIC=-7.22
</code></pre>

<p>The residuals from this appeared OK:</p>

<p><img src=""http://i.stack.imgur.com/wvdXD.jpg"" alt=""enter image description here""></p>

<p>The plot of fitted and actuals:</p>

<pre><code>plot(fitted(mod.arimax),col=""red"", type=""b"")
lines(window(log(cds),start=c(2012,02)),type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/kJ1pj.jpg"" alt=""enter image description here""></p>

<p><strong>The Questions</strong></p>

<p>1) Is this methodology correct for intervention analysis?</p>

<p>2) Can I look at estimate/SE for the components of the transfer function and say that the effect of the intervention was significant?</p>

<p>3) How can one visualize the transfer function effect (plot it?)</p>

<p>4) Is there a way to estimate how much the intervention increased the output after 'x' months? I guess for this (and maybe #3) I am asking how to work with an equation of the model - if this were simple linear regression with dummy variables (for example) I could run scenarios with and without the intervention and measure the impact - but I am just unsure how to work this this type of model.</p>

<p><strong>ADD</strong></p>

<p>Per request, here are the residuals from the two parametrizations.</p>

<p>First from the fit:</p>

<pre><code>fit &lt;- arimax(log(cds), order = c(1,0,0), 
              xtransf = data.frame(Oct13a = 1*(seq_along(cds)==22), Oct13b = 1*(seq_along(cds)==22)),
              transfer = list(c(0,0), c(1,0)))

plot(resid(fit), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/sqMZN.jpg"" alt=""enter image description here""></p>

<p>Then, from this fit</p>

<pre><code>mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
                   xtransf=data.frame(Oct13=1*(seq(cds)==22)),
                   transfer=list(c(1,1))
)

mod.arimax
plot(resid(mod.arimax), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/DjAyu.jpg"" alt=""enter image description here""></p>
"
"0.0889588054368324","0.0822133822214443","108522","<p>I have a quadratic regression y against x and I'm interested  in the value x where y is the maximum (ymax->x). I can compute x(ymax) but I'm also interested in the standard error or confidence interval of x(ymax) to get x(ymax) +/- standard error or, respectively, lower and upper xmax confidence interval. </p>

<p>One solution could be to use bootstrap methods and computing new estimates of beta but the real data I have is large and needs one hour to compute the model (mixed model with variance-covariance structure). </p>

<p>Another possibility would be to use the standard errors of the betas to compute a confidence band and to find the cutting points of the horizontal through maximum of the curve and the upper ""confidence curve"" (see below). But I'm not sure if this method is a valid way to get confidence interval.</p>

<p>Is there anyone who can give me some advice how to cope with this?</p>

<p>Thanks in advance.</p>

<hr>

<p><img src=""http://i.stack.imgur.com/enW6I.jpg"" alt=""enter image description here""></p>

<pre><code># simulate data set
b0 &lt;- 100
b1 &lt;- 100
b2 &lt;- -2
(x &lt;- rep(1:50,2))
(y &lt;- b0 + b1*x + b2*x^2 + rnorm(length(x),0,200))

# fit model
fit &lt;- lm(y~x + I(x^2))
summary(fit)

# visualize
plot(y~x)
lines(fitted(fit), col=""red"")

# retrieve coefficients
beta &lt;- fit$coef

# build function for optimization
f1 &lt;- function(x) sum(beta * c(1,x,x^2))

# compute xmax
(xymax &lt;- optimize(f1,interval=c(0,50), maximum=TRUE))

# add line at xmax
# add line at xmax
abline(v=xymax$max, col=""blue"")
    abline(h=xymax$obj, col=""blue"")

# compute confidence band
cilbeta &lt;- beta - qnorm(0.975,0,1)*summary(fit)$coef[,""Std. Error""]
    ciubeta &lt;- beta + qnorm(0.975,0,1)*summary(fit)$coef[,""Std. Error""]
f2 &lt;- function(x) { sum(c(1,x,x^2)*cilbeta) }
f3 &lt;- function(x) { sum(c(1,x,x^2)*ciubeta) }
# curve(f2,from=1,to=50)  does not work !!
ycil = c()
yciu = c()
for (i in 1:50) {
    ycil &lt;- c(ycil,f2(i))
    yciu &lt;- c(yciu,f3(i))
}
lines(1:50,ycil, lty=2)
lines(1:50,yciu, lty=2)

# cutting points between upper ""confidence curve"" horizontal line
# through y.max


# x.max = -b2/(2b3) = -0.5*b2/b3
# var(x.max) = 0.5^2*var(b2/b3)
# var.b23 = b2^2/b3^2*[var(b2)/b2^2 -2*cov(b2,b3)/(b2*b3) + var(b3)/b3^2]
(var.b23 &lt;- coef0[2]^2/coef0[3]^2*(vcov0[2,2]/coef0[2]^2 
            -2*vcov0[2,3]/(coef0[2]*coef0[3]) + vcov0[3,3]/coef0[3]^2
            )
        )
(var.xmax &lt;- (-0.5)^2*var.b23)
(se.xmax &lt;- sqrt(var.xmax))
# confidence interval
paste(formatC(x.max,digits=2,format=""f""),"" (""
      , formatC(x.max-2*se.xmax,digits=2, format=""f""),""-""
      , formatC(x.max+2*se.xmax,digits=2,format=""f""),"")""
      , sep=""""
      )
# [1] ""24.77 (24.12-25.42)""
</code></pre>
"
"0.0758643241810882","0.0771229887279699","108562","<p>I have two time series. One is an environmental variable (<em>n</em> = 108) organized by year and month. The other is a biological variable, also organized by year and month, but I have no data for some months (<em>n</em> = 97). </p>

<p>I did a cross-correlation in R between these 2 times series, and used the <code>na.exclude</code> function for the biological variable to account for the missing values. I then fit a lagged regression, again accounting for the missing values. </p>

<p>However, I want to know:</p>

<ol>
<li><p>Could I do a cross-correlation by deleting these missing values (no data months) for the biological variable, which will leave me with unequal lengths? R still does a cross-correlation, but I am unsure if the results are valid.</p></li>
<li><p>Could I delete the corresponding months in the env. variable to have equal length time series, or is accounting for missing values the correct approach?</p></li>
</ol>

<p>Thanks.</p>
"
"0.0715255707714427","0.0818012824723818","108750","<p>I'm working with a data set like the following:</p>

<p><em>X</em> =</p>

<pre><code>c1 c2 c3 c4 c5 y
a  c  f  h  j  0
a  d  f  i  k  0
a  c  g  h  j  1
a  c  f  h  k  0
b  d  g  h  k  0
b  e  f  h  j  0
</code></pre>

<p>I'm trying to create a logistic regression model that is able to predict <code>y</code> (0 or 1) based on the features of <em>X</em>. </p>

<p>Info on <em>X</em>:<br>
The values of <code>c1</code>â€“<code>c5</code> are all factors.<br>
The features have a varying number of levels (e.g. <code>c1</code>: 2 levels, <code>c2</code>: 4 levels, <code>c3</code>: 3 levels, etc.)<br>
Approx 10% of <code>y</code> is 1.  </p>

<p>I've tried using SVM and GLM without any good result.</p>

<pre><code>model &lt;- svm(y ~ ., data = X)  
pred &lt;- predict(model, X)   
table(pred,X$y)

        y
pred    FALSE TRUE
FALSE   1332   113
TRUE       0     0
</code></pre>

<p>and</p>

<pre><code>model &lt;- glm(y ~ ., family=binomial(""logit""), data=X)  
pred &lt;- predict(model, X, type=""response"")  
table(pred,X$y)  

pred                   FALSE TRUE
4.2260288377431e-08        2    0
4.24333100181876e-08       1    0
...
0.706714407238236          1    1
0.736650629322038          0    1
</code></pre>

<p>I'm used to working with features with continuous values when creating a predictive model, but I don't really know how to tackle a problem with factors.<br>
What would you guys recommend for this type of problem? Changing from logistic regression to something else or using other functions besides GLM &amp; SVM?</p>
"
"0","0.0272670941574606","108904","<p>I did stepwise regression with my multiple regression model and using AIC as a measure of fit with the <code>step</code> function in R. Afterwards some variables that the stepwise regression did not eliminate was not significant (> 0.05 p-value). Does this mean i have to take out those variables with large p-values or what is a normal procedure?  </p>
"
"0.0464572209811883","0.0472279924554862","109077","<p>I am trying to check the assumptions of a two-way ANCOVA. 
So in my model I have</p>

<ul>
<li>two factors (F1, F2)</li>
<li>one dummy coded two level covariate (C)</li>
<li>one dependent variable (D)</li>
</ul>

<p>In order to check the assumption of homogeneity of regression slopes I tried
to perform an ANOVA with type 3 sums for the model D ~ F1*F2*C to see whether any
interactions with the covariate might be significant.
Using the Anova function from the car package this corresponds to</p>

<pre><code>modd&lt;-aov(D~F1*F2*C)
Anova(modd,type=3)
</code></pre>

<p>However, I encounter the following Error message:</p>

<pre><code>Error in Anova.III.lm(mod, error, singular.ok = singular.ok, ...) : 
 there are aliased coefficients in the model
</code></pre>

<p>My question is, whether it makes sense for the homogeneity test to force R to compute the
ANOVA anyway by supplying the singular.ok=T option or what else I should do in order to 
check the assumption.</p>
"
"0.0808716413062113","0.0904347204435887","109222","<p>I am running an analysis where I have 2500 cases and 2500 controls. The cases have disease A, and the controls do not. I am trying to see if having disease A increases the odds of various diseases. For the sake of simplicity, we can focus on one disease, call it disease B.</p>

<p>D = 1 if disease B present, 0 otherwise</p>

<p>E = 1 if disease A present, 0 otherwise</p>

<p>I am also including in the model a measure of healthcare utilization. </p>

<p>F is a positive integer proportional to an individual's utilization of healthcare.</p>

<p>I am running the logistic regression model as such in R:</p>

<pre><code>glm(D ~ E + F, family = ""binomial"") 
</code></pre>

<p>Now, this works fine. </p>

<p>However, when I try to run conditional logistic regression, it gives me an error:</p>

<pre><code>library(survival)
clogit(D ~ E + F, strata(matched.pairs))
Error in fitter(X, Y, strats, offset, init, control, weights = weights,  :
  NA/NaN/Inf in foreign function call (arg 5)
In addition: Warning message:
In fitter(X, Y, strats, offset, init, control, weights = weights,  :
  Ran out of iterations and did not converge
</code></pre>

<p>I have tried different strata, including dividing the individuals into quantile bins based on F. It does not seem to change anything. (note: pairs are matched on age, gender, race, and F)</p>

<p>This occurs only when I run it on a larger sample size. I ran this same analysis on a sample size of 200 (100 cases and 100 controls) and it worked fine. When I use a sample size of 5000, I get the above error. </p>

<p>I also made sure that at least 10 cases and 10 controls had the disease in question (disease B, for this example). </p>

<p>I am not sure why logistic regression runs fine when conditional logistic regression does not. Can anyone offer me any advice?</p>

<p>Thank you all in advance.</p>
"
"0.053644178078582","0.0545341883149212","109261","<p>Do you know of an approach/package that facilitates mixed model regression of ordinal dependent variables on multiply imputed datasets in R?</p>

<p>Ideally, the function takes:</p>

<p>a list of multiply imputed datasets</p>

<p>a list of target variables (dependent variables, one or more)</p>

<p>a list of factors (independent variables, one or more)</p>

<p>a list of dummy coded conditions (for analysis of multifactor IVs)</p>

<p>and returns a table similar to the result of other regressions in r</p>

<p>After extensive searching, I had to create such a function using CLMM in the ordinal package. If you can't answer the first question perhaps you can advise me wrt code adaptations, statistical appropriateness of my approach, efficiency (it takes a LONG time with many imputed datasets), etc</p>

<p>here's some data that mirrors mine</p>

<pre><code>numimpdatasets = 3; N = 170; datalist = list()
for (datasetnum in 1:numimpdatasets){
    dvone = sample(1:5, N, replace=T)   
    dvtwo = sample(1:5, N, replace=T)
    teacher = c('Tom','Dick','Harry')[sample(1:3, N, replace=T)]
    studentclass = c('Class1','Class2','Class3','Class4','Class5',
                     'Class6')[sample(1:6, N, replace=T)]
    aptitude = runif(N, -3.5, 3.5)
    randord = sample(1:3, N, replace=T)
    conddummycode1 = c('a_cond1', 'b_cond2', 'c_cond3')[randord]
    conddummycode2 = c('c_cond1', 'b_cond2', 'a_cond3')[randord]
    datalist[[datasetnum]] = data.frame(cbind(dvone, dvtwo,teacher,
            studentclass, aptitude, conddummycode1, conddummycode2))
    }
dvs = colnames(datalist[[1]])[1:2]
conditions = colnames(datalist[[1]])[6:7]
ivs = c(""+as.numeric(aptitude)"",""+(1|teacher/studentclass)"")
</code></pre>
"
"0.0464572209811883","0.0472279924554862","109673","<p>I have built a cox model in R using the coxph function in the survival package, and now I need to replicate the model in SQL for scoring.  From my understanding, the model has the form described on the bottom of page 2 of this document, <a href=""http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf"" rel=""nofollow"">http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf</a>, which gives it semi-parametric flexibility. Since there is an unspecified alpha term, I cannot just take the coefficients and use the model like a typical linear model or generalized linear model (and exponentiate). There are ways to estimate this alpha term, and I believe this added term to the hazard is needed to specify the complete model.  If this is the case, how do I get my hands on this alpha term?</p>
"
"0.11706119363752","0.113053187438237","109835","<p>While working on a big data set made of 10-minutes-points of information - i.e. <code>144</code> points per day, <code>1008</code> per week and <code>52560</code> per year - I encountered a few problem in R. The information concerns electricity load on a source substation during the year.</p>

<h3>Multiple seasonality :</h3>

<p>The data set clearly shows multiple seasonalities, which are daily, weekly and yearly. From <a href=""http://stats.stackexchange.com/questions/47729/two-seasonal-periods-in-arima-using-r"">there</a> I understood that R doesn't handle multiple seasonality within the ARIMA modeling functions.  I would really like to work with ARIMA models though, because my previous work is based on ARIMA models and I know approximatively how to translate a model into an equation.  </p>

<h3>Long seasonality :</h3>

<p>Each of the seasonalities is of high value, with the shortest one being the daily seasonality at 144. Unfortunately from the SARIMA general equation which is<br>
$\phi(B)\Phi(B^s)W_t = \theta(B)\Theta(B^s)Z_t$<br>
I guessed that the maximum lag for a given model <code>SARIMA(p,d,q)(P,D,Q)144</code> is<br>
$max((p+P*144), (q+Q*144))$</p>

<p>I would really like to try and fit models with values of P and/or Q greater than 1, but R doesn't allow me since the <code>maximum supported lag = 350</code>. To do so I found <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">this link</a> which is really interesting and led to new functions in the forecast package by M. Hyndman, called <code>fourier</code> and <code>fourierf</code> which you can find <a href=""http://www.inside-r.org/packages/cran/forecast/docs/fourier"" rel=""nofollow"">here</a>. But since I am not a specialist in forecasting nor in statistics, I have some difficulties understanding how I can make this work.  </p>

<hr>

<p>The thing is I thing this whole fourier regressors package could help me a lot. From what I understood I could use it to simulate the long-seasonality of my data set, maybe use it to simulate multiple seasonality, and even more it could allow me to introduce exogenous variables - which are the <code>temperature</code> and (<code>public holiday + sundays</code>).<br>
I also tried doing some regression following <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">this example</a> but I couldn't make it work because :</p>

<pre><code>Error in forecast.Arima(bestfit, xreg = fourierf(gas, K = 12, h = 1008)) : 
Number of regressors does not match fitted model
</code></pre>

<p>I really hope somebody can help me get a better understanding of these functions. Thanks.</p>

<p><strong>Edit :</strong> So I tried my best with the fourier example given <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a> but couldn't figure out how it handles the fitting. Here is the code (I copy-pasted M. Hyndman one and adapted to my data set - unsuccessfully) :</p>

<pre><code>n &lt;- 50000
m &lt;- 144
y &lt;- read.table(""auch.txt"", skip=1)
fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}

library(forecast)
fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008)))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m), fourier(n+1:(14*m),4,1008))))
</code></pre>

<p>So I wanted to ""force"" the model to be a <code>SARIMA(2,1,5)(1,2,8)[144]</code> but when I type <code>arimod</code>this is the result of the Arima fitting :</p>

<pre><code>&gt; fit  
Series: y[1:n, 1] , 
ARIMA(2,1,5)                  

sigma^2 estimated as 696895:  log likelihood=-407290.2  
AIC=814628.3   AICc=814628.3   BIC=814840
</code></pre>

<p>It doesn't even take into consideration the seasonal part of the model, and I don't know much about the range the AIC values can take, but it seems way too high to be a good fitting model right there. I think it all comes down to my misunderstanding of the use of Fourier terms as regressors, but I can't figure out why.</p>

<p><strong>Edit 2 :</strong> Also I can't seem to be able to add another exogenous variable to the Arima function. I need to use <code>temperature</code> - probably as a lead - to fit the <code>SARIMAX</code> model, but as soon as I write this :</p>

<pre><code>fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008), tmp[1:n]))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m),fourier(n+1:(14*m),4,1008), tmp[n+1:(14*m)])))
</code></pre>

<p>Nothing is plotted besides the initial data set. There is no forecast while without <code>tmp</code> as an <code>xreg</code> I still get some results.</p>
"
"0.116914775576919","0.112599007499728","110033","<p>I am running a post-hoc analysis on the data collected during an experiment in which 15 unique stimuli were presented to participants. Having run a least squares regression using the lm() function in R I have found significant results for a subset of the data including 90 observations from 6 participants with two continuous variables and their interaction.</p>

<p>Taking advice from an article by Judd, Westfall &amp; Kenny (2012) I attempted to use a combination of the lmer() function found in the lme4 package in combination with a Kenward-Roger approximation through the KRmodcomp() function in the pbkrtest package (see the appendix in the article) in order to control for random effects:</p>

<pre><code>lmer(Prediction_Difference_Scale~Diff_AWD_LRTI_End_Scale*Diff_AWD_BD_End_Scale + (1|Unique_ID) + (Diff_AWD_LRTI_End_Scale*Diff_AWD_BD_End_Scale|Block),data=Data)
</code></pre>

<p>The first variable after the DV is the fixed effect, the second variable in parentheses indicates that the intercept is random with respect the unique stimuli (Unique_ID) and the third variable in parentheses indicates that both the intercept and the Condition slopes are random with respect to participant (Block) and that a covariance between the effects should be estimated. </p>

<p>When running the lmer() function I get the following error message:</p>

<pre><code>Error in checkNlevels(reTrms$flist, n = n, control) : 
  number of levels of each grouping factor must be &lt; number of observations
</code></pre>

<p>This is obviously because the number of observations equal the number of unique stimuli.</p>

<p>The function works when excluding the (1|Unique_ID) random  effect, which if I understand correctly is the same as carrying out a 'by stimulus' analysis. However, the authors warn against this by stating: ""Conceptually, a significant by-participant result suggests that experimental results would be likely to replicate for a new set of participants, but only using the same sample of stimuli. A significant by-stimulus result, on the other hand, suggests that experimental results would be likely to replicate for a new set of stimuli, but only using the same sample of participants. However, it is a fallacy to assume that the conjunction of these two results implies that a result would be likely to replicate with simultaneously new samples of both participants and stimuli.""</p>

<p>I would like to control for the random effects of both stimuli and participants, but I am unsure how to proceed?</p>

<p>The article can be accessed here: <a href=""http://jakewestfall.org/publications/JWK.pdf"" rel=""nofollow"">http://jakewestfall.org/publications/JWK.pdf</a></p>

<hr>

<p>To clarify the question regarding the 15 unique stimuli, this is 15 unique stimuli per participant, meaning the sample of 90 observations consists of 6 participants. The stimuli for all of the 90 observations are unique however.</p>

<p>I suppose what my question boils down to is whether there is even a need to include the (1|Unique_ID) 'variable' in the function formula as there is no error dependence between any of the stimuli?</p>
"
"NaN","NaN","110570","<p>For my survey data analysis, I ran an Ordinal Logistic regression using the 'polr' function.
The summary of the regression is as follows:</p>

<p><img src=""http://i.stack.imgur.com/csKGq.png"" alt=""enter image description here""></p>

<p>My question is:</p>

<ol>
<li>Do I need to standardize my  beta values?</li>
<li>If so, is lm.beta the right approach (as per my understanding, it only works for linear models)? And if not, could you please provide a method to do so.</li>
</ol>

<p>Thanks everyone!</p>
"
"NaN","NaN","110597","<p>My question is simple: is there a function in <code>R</code> which estimates the linear regresion model in a similar fashion as <code>lm</code>, but only using the means, variances, and covariance (correlations), i.e. the sufficient statistics? I am looking for a function to which I can input these statistics (plus sample size) and it returns regression coefficients and tests.</p>
"
"0.0464572209811883","0.0472279924554862","110932","<p>I am working on some <strong>non-parametric bayesian based predictive analysis</strong> using <strong>R</strong>. I have a set of data which denotes various parameters of an online transaction. Based on these parameters I want to develop a model which will provide predictions for future online transactions.</p>

<p>The training data consist of records in this format:</p>

<pre><code>transaction_id (numeric)| duration (integer)| amount | is_holiday (boolean) | status(1 or 0)
                        |                   |        |                      |
                        |                   |        |                      | 
</code></pre>

<p>The problem that I am facing is that I do not know how to proceed ahead. I am do know know what are the steps that I need to follow. I looked up and found that there are few packages in R like <code>DPpackage</code> which have some functions for non-parametric bayesian modeling but there is no concrete example available about how to use it in order to perform various steps of training and testing.</p>

<p>It would be helpful for me if someone could provide me some guidance as in which process will be better for such kind of predictive/regression analysis and how to proceed ahead, like what steps should I perform to get the training and testing done.</p>

<p>Thanks in advance!  </p>
"
"NaN","NaN","111339","<p>I am looking for the equation for the se.fit values when using logistic regression in R.
I have seen this answer - <a href=""https://stats.stackexchange.com/questions/66946/how-are-the-standard-errors-computed-for-the-fitted-values-from-a-logistic-regre/66947#66947"">How are the standard errors computed for the fitted values from a logistic regression?</a></p>

<p>but in my case, I'm calling ""predict"" function with 'type' parameter set to be ""response"".
In this case, the equation given in the linked I attached doesn't hold.</p>

<p>Here is an example of the predict function I'm calling:</p>

<pre><code>predicted.resutls &lt;- predict(glm.model, train.data, type = ""response"", se.fit=TRUE)
</code></pre>
"
"0.104084994078794","0.105811867590468","111383","<p>I have been reading several CV posts on binary logistic regression but I am still confused for my current situation.</p>

<p>I am attempting to fit a binary logistic regression to a series of continuous and categorical variables in order to predict the mortality or the survival of animals (<code>qual_status</code>). Please see the <code>str</code> below:</p>

<pre><code>&gt; str(logit)
'data.frame':   136 obs. of  9 variables:
 $ id         : Factor w/ 135 levels ""01001"",""01002"",..: 26 27 28 29 30 31 32 33 34 35 ...
 $ gear       : Factor w/ 2 levels ""j"",""sc"": 2 1 1 2 1 2 1 2 2 1 ...
 $ depth      : num  146 163 179 190 194 172 172 175 240 214 ...
 $ length     : num  37 35 42 38 37 41 37 52 38 37 ...
 $ condition  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 4 1 4 2 2 1 2 1 ...
 $ in_water   : num  80 45 114 110 60 121 56 140 93 68 ...
 $ in_air     : num  60 136 128 136 165 118 220 90 177 240 ...
 $ delta_temp : num  8.5 8.4 8.3 8.5 8.5 8.6 8.6 8.7 8.7 8.7 ...
 $ qual_status: Factor w/ 2 levels ""0"",""1"": 1 1 2 1 2 1 2 1 1 1 ...
</code></pre>

<p>I have no issues fitting an the following additive binary logistic regression with the <code>glm</code> function:</p>

<p><code>glm(qual_status ~ gear + depth + length + condition + in_water + in_air + delta_temp, data = logit, family = binomial)</code></p>

<p>...but I am also interested at how these predictor variables interact with one another and possibly influence survival. However, when I attempt the following interactive binary logistic regression:</p>

<p><code>glm(qual_status ~ gear * depth * length * condition * in_water * in_air * delta_temp, data = logit, family = binomial)</code></p>

<p>I receive a warning message <code>""glm.fit: fitted probabilities numerically 0 or 1 occurred""</code>, along with missing coefficients due to singularities (NA or &lt;2e-16 <em>*</em>) when I use <code>summary</code>:</p>

<pre><code>Call:
glm(formula = qual_status ~ gear * depth * length * condition * 
    in_water * in_air * delta_temp, family = binomial, data = logit)

Deviance Residuals: 
  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [36]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [71]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
[106]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0

Coefficients: (122 not defined because of singularities)
                                                            Estimate Std. Error    z value Pr(&gt;|z|)    
(Intercept)                                                1.419e+30  5.400e+22   26274077   &lt;2e-16 ***
gearsc                                                    -1.419e+30  5.400e+22  -26274077   &lt;2e-16 ***
depth                                                      1.396e+28  4.040e+20   34539471   &lt;2e-16 ***
length                                                     6.807e+28  1.836e+21   37079584   &lt;2e-16 ***
condition2                                                -3.229e+30  8.559e+22  -37727993   &lt;2e-16 ***
condition3                                                 1.747e+31  4.636e+23   37671986   &lt;2e-16 ***
condition4                                                 9.007e+31  2.388e+24   37724167   &lt;2e-16 ***
in_water                                                  -4.540e+28  1.263e+21  -35935748   &lt;2e-16 ***
in_air                                                    -4.429e+28  1.182e+21  -37470809   &lt;2e-16 ***
delta_temp                                                -1.778e+28  3.237e+21   -5492850   &lt;2e-16 ***
gearsc:depth                                              -1.396e+28  4.040e+20  -34539471   &lt;2e-16 ***
gearsc:length                                             -6.807e+28  1.836e+21  -37079584   &lt;2e-16 ***
depth:length                                              -9.293e+26  2.450e+19  -37930778   &lt;2e-16 ***
gearsc:condition2                                          1.348e+30  3.567e+22   37809001   &lt;2e-16 ***
gearsc:condition3                                          2.816e+30  7.495e+22   37575317   &lt;2e-16 ***
gearsc:condition4                                                 NA         NA         NA       NA    
</code></pre>

<p>Fitting only the continuous variables to a binary logistic regression doesn't yield any warnings or singularities but the addition of the ordinal predictor variables causes issues. Along with avoiding these warnings, is there a function/package that can handle dummy variables (I believe that is what I am looking for) in logistic regressions in <code>R</code>?</p>
"
"0.0379321620905441","0.0385614943639849","111555","<p>Imagine the following type of dataset: I have a dependent variable Y, two independent variables X and Z, and a variable that can separate the dataset in two smaller datasets.</p>

<p><img src=""http://i.stack.imgur.com/Tp2Qt.jpg"" alt=""enter image description here""></p>

<p>I estimated 2 identical OLS regressions for each group separately:</p>

<p><code>Alldata_A&lt;-subset(Alldata, group==""A"")</code></p>

<p><code>Alldata_B&lt;-subset(Alldata, group==""B"")</code></p>

<p><code>OLS_A &lt;- lm(Y~Z+X,Alldata_A)</code></p>

<p><code>OLS_B &lt;- lm(Y~Z+X,Alldata_B)</code></p>

<p>Now I am looking for a way to compare both models and to test whether the coefficients significantly differ for the two groups.</p>

<p>This shouldn't be difficult with an F-test, but I do not manage to get a result using R language.</p>

<p>I would be very grateful if someone can suggest which R-function to use for this!</p>
"
"0.026822089039291","0.0272670941574606","111841","<p>I ran an ordinal logistic regression in R using the polr function on a survey analysis dataset. The responses of the dependent variable range from Poor to Excellent.
The responses to the independent variables range from 1 to 5 (1 being Poor and 5 being Excellent). I obtained the following result:</p>

<p><img src=""http://i.stack.imgur.com/1ZBij.png"" alt=""enter image description here""></p>

<p>I want to measure the individual percentage contribution of my independents (R1, R2,...,R17) to the dependent variable. </p>

<p>Is there a way to do this.</p>

<p>Thanks for any help.</p>
"
"0.0657004319817604","0.0556587228785024","111902","<p>I am conducting a two-sample test (1-way ANOVA with 2 treatments), and the goal is to estimate the ratio of cell means assuming that the data are lognormal. A simple approach is to log the response and fit a model </p>

<p>$\log Y = b_0 + b_1 * X$</p>

<p>and then estimate the ratio as</p>

<p>$R = e^{b_1}$</p>

<p>However, that gives the ratio of geometric cell means rather than arithmetic cell means. </p>

<p>I assumed that if I fit a ""proper"" lognormal model using either <code>gamlss</code> in R or <code>PROC GLIMMIX</code> in SAS, I will get the ratio of arithmetic means, but for some reason both procedures generate the same slope as the $\log Y$ regression.</p>

<p>This is odd because when I use this approach with Poisson or Negative Binomial regression, I do get the ratio of arithmetic means. What am I missing?</p>

<hr>

<p>P.S.</p>

<p>I think I identified the source of confusion, but I don't have an explanation for it. A lognormal setup with the identity link function is:</p>

<p>$\log Y_1 \sim N(b_0, \sigma^2)$</p>

<p>$\log Y_2 \sim N(b_0 + b_1, \sigma^2)$</p>

<p>which implies </p>

<p>$\frac{E[Y_2]}{E[Y_1]} = \frac{e^{b_0 + b_1 +\sigma^2/2}}{e^{b_0 + \sigma^2/2}} = e^{b_1}$</p>

<p>To me, it means that $e^{b_1}$ should have a point estimate equal to the ratio of arithmetic means for the original response.</p>

<p>On the other hand,</p>

<p>$E[\log Y_1] = b_0$</p>

<p>$E[\log Y_2] =  b_0 + b_1$</p>

<p>$b_0$ is estimated as arithmetic mean of $\log Y_1$, $b_0 + b_1$ is estimated as arithmetic mean of $\log Y_2$. Hence, $e^{b_1}$ should have
a point estimate equal to the ratio of geometric means for the original response, and it does, given the output from those two packages. Where did I make a mistake?</p>
"
"0.0709645772411954","0.072141950116023","112247","<p>I'm trying to use the Match() function from the Matching package in R to do a propensity score analysis.</p>

<p>My outcome of interest is a binary variable (0/1).  My treatment is also a binary variable (0/1).  In addition, I have a number of other variables that I want to control for in this analysis.</p>

<p>First, I fit a logistic regression to define a propensity score for the treatment:</p>

<pre><code>glm1 = glm(Treatment ~ variable1 + variable2 + variable3 + ..., 
           data=dataset, family=""binomial"")
</code></pre>

<p>Then, I used the Match function to estimate the average treatment effect on the treated:</p>

<pre><code>rr1 = Match(Y = Outcome, Tr = Treatment, X = glm1$fitted)
</code></pre>

<p>Finally, I called for a summary:</p>

<pre><code>summary(rr1)
</code></pre>

<p>My question is how to interpret the output.  I get:</p>

<pre><code>Estimate... -0.349,
AI SE... 0.124,
T-stat... -2.827,
p.val... 0.005
</code></pre>

<p>What does this mean?  In particular, what is Estimate?  The documentation says it's ""The estimated average causal effect.""  But what are the units?  Can I interpret this to mean that the treatment reduced the outcome by a relative 35%?  Or by an absolute 0.35?  Or do I need to exponentiate?</p>

<p>Any help on the interpretation would be much appreciated!</p>
"
"0.053644178078582","0.0409006412361909","112541","<p>I'm having trouble interpreting the results from the Spread-Level Plot function in R (car package). The documentation says:</p>

<blockquote>
  <p>PowerTransformation<br>
  spread-stabilizing power transformation, calculated as 1 - slope of the line fit to the plot.</p>
</blockquote>

<p>This is not explicit enough for me. Should this transformation be applied to every variable in the regression?</p>

<p>For example, assume I have an lm object given by:</p>

<pre><code>myFit &lt;- lm(y ~ x1 + x2)
</code></pre>

<p>Then I use Spread-Level Plot:</p>

<pre><code>slp(myFit)
</code></pre>

<p>If the 'suggested power transformation' is 0.5, then does that imply a homoscedastic model could be fit using one of the following?</p>

<pre><code>refitA &lt;- lm(sqrt(y) ~ sqrt(x1) + sqrt(x2))
refitB &lt;- lm(sqrt(y) ~ x1 + x2)
refitC &lt;- lm(sqrt(y) ~ sqrt(x1 + x2))
</code></pre>

<p>If I understand correct, refitA would be the suggested model to approximate homoscedasticity. On the other hand, if I <em>only</em> want to transform the LHS, I would use the <code>powerTransform</code> function (also from car package). i.e., an ""estimated transform parameter"" of 0.5 from the powerTransform function would imply that refitB is homoscedastic.</p>

<p>Is this correct?</p>

<p>Thanks!</p>
"
"0.0379321620905441","0.0385614943639849","112858","<p>I am currently trying to fit a survival analysis model which has the following survival function:</p>

<p>$S(t) = \lambda_i e^{-\lambda_i t}$</p>

<p>but with </p>

<p>$\lambda_i = e^{\beta_0 +\beta_1 log(1+X_i)}$</p>

<p>where $X_i$ represents each unique observation. </p>

<p>I am trying to use the default survival package but am not sure where I can program in the regression formula for the hazard rate. I believe I might be misunderstanding what the survival analysis package does but would anyone be able to tell me if this can be even done in the R package? Thanks!</p>
"
"0.0709645772411954","0.072141950116023","112997","<p>I am new to modelling percentage data, and I would be greatfull for some advice. I have proportion data (0,1] on a percentage of money sent by Player B to Player A. Participants received an amount of money, and could decide what percentage they will send back. I have two categorical predictors (1<sup>st</sup> with 3 factors, 2<sup>nd</sup> with 2); one continuous predictor; and one nesting factor (class). Since the data are bound between 0 and 1, I figured out that the best option would be Beta regression. I tried to use <code>hglm</code> package which fitted well, however, since the data are one-inflated (many people chose to send back the full amount), I am looking for other options.</p>

<p>As most appropriate seem to be <code>gamlss</code> package, which can use BEOI (Beta One Inflated) distribution. I used this code:</p>

<pre><code>m1 &lt;- gamlss(percent~cat1+cat2+continous, random(class), family=BEOI, data=dat, 
             mixture=""gq"", K=1)
</code></pre>

<p>From what I understand from package help files, this should be the simplest option. However, it produces very different results from the <code>hglm</code>command. Especially the standard errors are higher than beta coefficients, leading to non-significant results. I tried to specify other other functions in the model (e.g., <code>K, sigma.formula, nu.formula, mixture</code> etc.), but these are beyond my understanding, and I am not really sure what I did there.</p>

<p>I would very much appreciate any suggestions regarding either how to better specify the model, or simple explanations of <code>gamlss</code> function.</p>
"
"0.0599760143904067","0.0609710760849692","113502","<p>I am trying to run a regression using about 80 independent variables. The problem is that the last 20+ coefficients return NA. If I condense the range of data to within 60, I get coefficients for everything just fine. Why am I getting these NAs and how do I resolve it? I need to reproduce this code using all of these variables.</p>

<pre><code>composite &lt;- read.csv(""file.csv"", header = T, stringsAsFactors = FALSE)
composite &lt;- subset(composite, select = -ncol(composite))
composite &lt;- subset(composite, select = -Date)
model1 &lt;- lm(indepvariable ~., data = composite, na.action = na.exclude)
</code></pre>

<p>composite is a data frame with 82 variables.</p>

<p>UPDATE:</p>

<p>What I have done is found a way to create an object that contains only the significantly correlated variables, to narrow the number of independent variables down. </p>

<p>I have a variable now: sigvars, which is the names of an object that sorted a correlation matrix and picked out only the variables with correlation coefficients >0.5 and &lt;-0.5. Here is the code:</p>

<pre><code>sortedcor &lt;- sort(cor(composite)[,1])
regvar = NULL

k = 1
for(i in 1:length(sortedcor)){
  if(sortedcor[i] &gt; .5 | sortedcor[i] &lt; -.5){
    regvar[k] = i
  k = k+1
 }
}
regvar

sigvars &lt;- names(sortedcor[regvar])
</code></pre>

<p>However, it is not working in my lm() function:</p>

<pre><code>model1 &lt;- lm(data.matrix(composite[1]) ~ sigvars, data = composite)
</code></pre>

<p>Error: Error in model.frame.default(formula = data.matrix(composite[1]) ~ sigvars,  : 
  variable lengths differ (found for 'sigvars')</p>
"
"0.0599760143904067","0.0609710760849692","113652","<p>I need to calculate the temporal trends for some climate variables with missing values. For example, last frost days defines as the last day of year with minimum temperature less than 0C. However, there are no any frost days in some years. </p>

<p>My data look like: </p>

<pre><code>lfd &lt;- c(NA, NA, NA, NA, NA, 190, 192, 189, 200, 185, 205, 203, 200, 207, NA, NA, 205)
years &lt;- seq(1957, length.out = length(lfd))
</code></pre>

<p>Now I use the linear regression in R (function lm) to calculate temporal trend. It seems the results are unreasonable for datasets with many missing values.</p>

<p>How could I calculate the temporal trends with missing values? Thanks for any suggestions. </p>
"
"0.0763370036711974","0.0689808981694763","113756","<p>I'd like to test the <em>anova rbf kernel</em> included in the <strong>kernlab</strong> package in <strong>caret</strong>. Following excelent tutorial (<a href=""https://topepo.github.io/caret/custom_models.html"" rel=""nofollow"">https://topepo.github.io/caret/custom_models.html</a>) I've come up with the following code:</p>

<pre><code>SVManova &lt;- list(type = ""Regression"", library = ""kernlab"", loop = NULL)
prmanova &lt;- data.frame(parameter = c(""C"", ""sigma"", ""degree"", ""eps""),
                     class = rep(""numeric"", 4),
                     label = c(""Cost"", ""Sigma"", ""Degree"", ""Eps""))
SVManova$parameters &lt;- prmanova
    svmGridanova &lt;- function(x, y, len = NULL) {
    library(kernlab)
    sigmas &lt;- sigest(as.matrix(x), na.action = na.omit, scaled = TRUE, frac = 1)
    expand.grid(sigma = mean(sigmas[-2]), epsilon = 0.000001,
                C = 2 ^(-5:len), degree = 1:2) # len = tuneLength in train
    }
    SVManova$grid &lt;- svmGridanova
svmFitanova &lt;- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  ksvm(x = as.matrix(x), y = y,
       kernel = ""anovadot"",
       kpar = list(sigma = param$sigma, degree = param$degree),
       C = param$C, epsilon = param$epsilon,
       prob.model = classProbs,
       ...) #default type = ""eps-svr""
}
SVManova$fit &lt;- svmFitanova
    svmPredanova &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)
      predict(modelFit, newdata)
    SVManova$predict &lt;- svmPredanova
svmProb &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type=""probabilities"")
SVManova$prob &lt;- svmProb
    svmSortanova &lt;- function(x) x[order(x$C), ]
SVManova$sort &lt;- svmSortanova
</code></pre>

<p>I then asked for the model to train some dataset:</p>

<pre><code>set.seed(100) #use the same seed to train different models
svrFitanova &lt;- train(R ~ .,
                data = trainSet,
                method = SVManova,
                preProc = c(""center"", ""scale""),
                trControl = ctrl, tuneLength = 20,
                allowParallel = TRUE) #By default, RMSE and R2 are computed for regression (in all cases, selects the tunning and cross-val model with best value) , metric = ""ROC""
#Print the results
svrFitanova
</code></pre>

<p>But I get the following error:</p>

<pre><code>Error in train.default(x, y, weights = w, ...) : 
  The tuning parameter grid should have columns C, sigma, degree, eps
</code></pre>

<p>I don't see why this error occurs.... tune grid has four columns as requested... Any ideas? Thanks</p>
"
"0.026822089039291","0.0272670941574606","113839","<p>I am using the nls function in R to perform a nonlinear regression and need to calculate the studentized residuals.  Is this something I need to manually do?  It seems like I need to manually do a QR decomposition and calculate the hat matrix to do this.  I don't have a problem doing this but I wanted to ask if anyone knew an easier way to do this.</p>
"
"0.0758643241810882","0.0674826151369737","114184","<p>Specifically, are there any binomial regression models that use a kernel with heavier tails and higher kurtosis than the standard kernels (logistic/probit/cloglog)?</p>

<p>As a function of the linear predictor $\textbf{x}'\mathbf{\hat{\beta}}$, the logistic distribution</p>

<ul>
<li>Underestimates the probability of my data being in the tails of the distribution</li>
<li>Underestimates the kurtosis, or clustering of data, in the middle of the distribution:</li>
</ul>

<p>This can be seen from a diagnostic plot of my fit:</p>

<p><img src=""http://i.stack.imgur.com/ar6OW.png"" alt=""enter image description here""></p>

<ul>
<li>The red line is the logistic CDF, representing a perfect fit</li>
<li>The black line represents the fitted probabilities from my dataset (calculated by binning observations into 0.1 intervals of $\textbf{x}'\mathbf{\hat{\beta}}$, where $\mathbf{\hat{\beta}}$ is obtained from my fit)</li>
<li>The grey bars in the background represent number of observations on which the true probabilities are based upon</li>
<li>The grey areas are where the tail 10% of the data lie (5% each side).</li>
</ul>

<p>Ideally, any solution would use R.</p>

<h2>Edit</h2>

<p>Why am I talking about CDFs? Our GLM equation is:</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{E}[Y] = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Where $g$ is the link function.</p>

<p>Further, if $g^{-1}$ is a valid probability distribution (i.e. monotonically increasing from 0 to 1, indeed the case with probit, logit, cloglog), then consider a latent (not directly observed) continuous random variable $Y^{*}$ whose distribution (CDF) is given by $g^{-1}$. Then by definition</p>

<p>$$\mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta}) = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Equating the two equations above, we see the probability of $Y=1$ is exactly equal to the CDF of $Y^{*}$</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta})$$</p>

<p>Hence I talk interchangeably about the expected response $\mathbb{E}[Y]$ and CDF of $Y^{*}$ over linear-predictor ($\textbf{x}'\mathbf{\hat{\beta}}$) space.</p>
"
"0.0379321620905441","0.0385614943639849","114218","<p>After running a gradient boosted model with <code>n</code> data points using multinomial regression where the response variable (a factor, as required by the gbm function) has <code>k</code> levels with R package gbm, I see that the predictions are output as as a vector of length <code>n*k</code>. Predicted responses are from:</p>

<pre><code>probs.var.multinom &lt;- predict.gbm(gbm.model.multinom, test.data, best.iter.gbm, 
                                  type=""response"")
</code></pre>

<p>Note that this is different from the output of a logistic (distribution = ""bernoulli"") model, where the results are a vector the same length as the number of cases.</p>

<p>How should this be interpreted? Specifically, how can I link the response vector back to the input data set to evaluate the classification?</p>
"
"0.053644178078582","0.0545341883149212","114399","<p>I have a theoretical growth function that can be perturbed by events, and I'd like to estimate the growth parameters as well as the perturbation, and the rate of falloff after that perturbation.</p>

<p>I'm thinking of using a logistic function to model the effect of the event and the falloff of that effect (if any).</p>

<p>To ground this, $x$ is time, and $t$ is the time the event occurs. Before time $t$, or if the event never occurs, we have a simple linear regression. After the event occurs, I model the contribution of the event with magnitude controlled by $\beta_2$ and rate of falloff by $\beta_3$.</p>

<p>$y_i=\left\{x_{i}&lt;t:\beta_{0}+\beta_1x_i+\epsilon_i,x_i&gt;t:\beta_0+\beta_1x_i+2\beta_2\frac{1}{\left(1+e^{\beta_3\left(x_i-t\right)}\right)}+\epsilon_i\right\}$</p>

<p>(<em>edited to add the error term</em>)</p>

<p>Here's a <a href=""https://www.desmos.com/calculator/nzmusqqosq"" rel=""nofollow"">Desmos graph</a> if it helps.</p>

<p>I'm really not sure how to estimate parameters for this model in any of the stats packages I'm familiar with in R. Do I need to turn to Bayesian methods?</p>
"
"0.0860220579279185","0.102024124270123","114468","<p>I am using the metafor package in R. I have fit a random effects model with a continuous predictor as follows </p>

<pre><code>SIZE=rma(yi=Ds,sei=SE,data=VPPOOLed,mods=~SIZE)
</code></pre>

<p>Which yields the output:</p>

<pre><code>R^2 (amount of heterogeneity accounted for):            63.62%
Test of Moderators (coefficient(s) 2): 
QM(df = 1) = 9.3255, p-val = 0.0023

Model Results:

                 se    zval    pval   ci.lb   ci.ub    
intrcpt  0.3266  0.1030  3.1721  0.0015  0.1248  0.5285  **
SIZE     0.0481  0.0157  3.0538  0.0023  0.0172  0.0790  **
</code></pre>

<p>Below I have plotted the regression.The effect sizes are plotted proportionally to the inverse of the standard error. I realize that this is a subjective statement, but the R2 (63% variance explained) value seems a lot larger than is reflected by the modest relationship shown in the plot (even taking weights into account).</p>

<p><img src=""http://i.stack.imgur.com/3JNmM.jpg"" alt=""enter image description here""></p>

<p>To show you what I mean, If I then do the same regression with the lm function (specifying study weights in the same way):</p>

<pre><code>lmod=lm(Ds~SIZE,weights=1/SE,data=VPPOOLed)
</code></pre>

<p>Then the R2 drops to 28% variance explained. This seems closer to the way things are (or at least, my impression of what kind of R2 should correspond to the plot). </p>

<p>I realize, after having read this article (including the meta-regression section): (<a href=""http://www.metafor-project.org/doku.php/tips:rma_vs_lm_and_lme"">http://www.metafor-project.org/doku.php/tips:rma_vs_lm_and_lme</a>), that differences in the way the lm and rma functions apply weights can influence the model coefficients. However, it is still unclear to me why the R2 values are so much larger in the case of meta-regression. Why does a model that looks to have a modest fit account for over half the heterogeneity in effects?</p>

<p>Is the larger R2 value because the variance is partitioned differently in the meta analytic case? (sampling variability v other sources) Specifically, does the R2 reflect the percent of heterogeneity accounted for <em>within the portion that cant be attributed to sampling variability</em>?. Perhaps there is a difference between ""variance"" in a non-meta-analytic regression and ""heterogeneity"" in a meta-analytic regression that I am not appreciating.</p>

<p>I'm afraid subjective statements like ""It doesn't seem right"" are all I have to go on here. Any help with interpreting R2 in the meta-regression case would be much appreciated. </p>
"
"0.080466267117873","0.0818012824723818","114675","<p>I really want to understand how the math is working here. I am trying to get the standard error of the fitted values for a time series regression model. In the non-time series regression, I know I can take the transpose of the data multiplied by the variance - covariance matrix of the model coefficients and then multiply by the data values again to get the standard errors of the fitted values.</p>

<p>But I'm not sure how to do this when I am including an autoregressive term.</p>

<pre><code>require(forecast)
require(tserieS)
</code></pre>

<p>Response variable</p>

<pre><code>Sablects &lt;- rnorm(10)
</code></pre>

<p>Covariates</p>

<pre><code>my.xreg &lt;- cbind(rnorm(10),rbinom(10,1,0.5))
</code></pre>

<p>In my actual data, values are normalized so I set the intercept equal to zero here.</p>

<pre><code>m4&lt;-arima(Sablects, order=c(2,0,0),fixed=c(0,NA,0,NA,NA),xreg=my.xreg) 
</code></pre>

<p>The predict function will give me standard errors on my in-sample prediction (the fitted values of my model).</p>

<pre><code>my.se &lt;- predict(m4, newxreg = my.xreg, n.ahead = 10)$se         

my.se
</code></pre>

<p>Now to compare the output of my.se, I want to do this mathematically but I don't know what to use for the values of the ar2 term. I use 1's as a placeholder to demonstrate that my output does not equal the values from <code>my.se</code> above</p>

<pre><code>C &lt;- cbind(rep(1, nrow(my.xreg)), my.xreg[, 1], my.xreg[, 2])

C
</code></pre>

<p>I think this value should equal the first value in my.se, but is not producing the same value as my.se</p>

<pre><code>sqrt(t(C[1, ]) %*% vcov(m4) %*% C[1, ])
</code></pre>

<p>Also, I'm not so great with matrix multiplication but here is my work around for getting all of the se values.</p>

<pre><code>se.output &lt;- matrix(nrow=nrow(C))
</code></pre>

<p>Specify that the max number of i is equal to number of rows of <code>C</code>.</p>

<pre><code>  for(i in 1:nrow(C)){

    # Loop through your multiplication for each row (i) of `C`. For each iteration, save the new data into the new row of se.output

    se.output[i] &lt;- sqrt(t(C[i, ]) %*% vcov(m4) %*% C[i, ])  
    }

se.output
</code></pre>
"
"0.0657004319817604","0.0556587228785024","115020","<p>I have a set of 'activity' values for some enzyme assays I have been doing, that come out of some analysis I've been doing. The problem is, the data is fairly crap, and there aren't many points, but its for project for my MSc so I'm stuck with it. I appreciate that trying to regress 4 datapoints, and especially extrapolate from them is a statistical no-no, but it's what the PhD student who I'm emulating did (albeit with the help of our Uni's Biostats department - a resource I can't really utilise at this point) - and so I would like the results to be directly comparable.</p>

<pre><code>DPConc DPActivity
0      100.000000
83     67.709971
166    6.296231
416    16.546593
</code></pre>

<p>I need to extrapolate from this an IC50 value, which is the concentration (x axis) value, for which 50% Activity is seen.</p>

<p>I want (have) to do this by fitting some kind of curve/spline to the points, but I'm at a bit of a loss. The best I've come up with so far is this:</p>

<pre><code>require(splines)
library(splines)

plot(DPConc, DPActivity)
splineDP &lt;- smooth.spline(DPConc, DPActivity, spar=0.45)
lines(splineDP)
</code></pre>

<p>Which for various values of <code>spar</code> gives me this graph (excuse its roughness - I'm really at the end of my tether!)</p>

<p><img src=""http://i.stack.imgur.com/z6P3Y.png"" alt=""Dipeptide Activity vs. Concentration""></p>

<p>It looks as though some kind of exponential decay is going on, but I'm not enough of a mathematician to be able to concoct the required function. I'd rather derive the function from the data, than make the data fit the function if possible (not least because I have other data that doesn't follow the same trend, though I suspect thats experimental error).</p>

<p>I've tried a couple of different approaches from around the web but my deadline is imminent and I really need the simplest solution. If the solution could be implemented with <code>ggplot</code>, that would be a bonus as my other plots so far have been made with it, the above was just a quick mockup.</p>
"
"0.0848188929679971","0.0862261227118454","115126","<p>I need to do a Multiple Imputation on a dataset with several missing values, and I need to do it with mice, because later I'll have to compare the results with those of imputations ran with other programs.</p>

<p>My colleague obtained a completa dataset by running a MI on the incomplete dataset, with 5 iterations, and then taking the 5 imputed datasets and manually calculating mean values. Se essentially he pooled manually. I'm far from an expert so I don't know if this operation is valid. </p>

<p>Anyways, in MICE so far I could run the imputation (again, with maxit=5), using the function ""imp&lt;-mice(eco)"", where ""eco"" is the incomplete dataset. So I obtained the 5 imputed datasets, stored in the object ""imp"", of class ""MIDS"". Now I just need to pool the 5 completed datasets to obtain a unique complete one, i don't wanna run analyses on the 5 datasets and then pool the results. Can that be done? If I got it right from the manual, it seems that MICE allows you to pool only after you ran some analysis on the imputed datasets. The analysis is repeated on each dataset and the results are stored in an object of class ""MIRA"". I tried to run the function ""pool()"" on ""imp"" but it can't be done because imp is of class ""mids"" and you can only pool mira objects.</p>

<p>The manual also says that to pool you need a variance/covariance relation, and in the example given there they run a linear regression and then they pool the results of the regression. But I doubt if it's what I need. I'm confused</p>
"
"0.0808716413062113","0.0822133822214443","115133","<p>I have a data-set with 32 effect size estimates- only 11 of which report a value for the continuous moderator of interest (the samples anxiety level). A complete case analysis (restricted to the 11 cases) shows that anxiety is a significant predictor of the effect sizes in meta-regression. </p>

<p>I would like to use imputation techniques to ""fill in"" the missing values to see if the relationship between anxiety and effect size (d) is still significant. If I do this using the ""mice"" function in R, it automatically selects d (the effect size) as a predictor to impute the plausible values of anxiety, as shown in the predictor matrix.</p>

<p>My issue is that this seems <em>circular</em>- I already know that anxiety predicts d, so using d to predict the missing values of anxiety seems to be ""playing tennis without the net"" and will surely artificially strengthen the relationship.</p>

<p>On the other hand- I <em>don't know for sure</em> that increased anxiety predicts d or whether increased anxiety is an outcome of the effect size- they may have a mutual influence. (d reflects a bias for threat, which literature suggests could be a <em>cause or consequence</em> of anxiety.....). This would seem to make using d in imputation more legitimate. Also, the recommendation seems to be to use all variables that will appear in the model applied after imputation (which will of course include d as the outcome) in the imputation process (van Buuren, 1999).</p>

<p>So given this issue, should I simply remove the effect size as a predictor of anxiety for imputation and instead rely on other demographic variables, or just random sampling of the observed data? Doing this also seems wrong, since this seems to generate a false ""uncertainty"" in the imputed data, given we have an idea of the relation between d and anxiety.  </p>

<p>Any help or references to resolve this problem would be much appreciated. Please let me know if anything is unclear.         </p>
"
"0.026822089039291","0.0272670941574606","115343","<p>I have done a linear regression in R, using glm function. The calculated intercept says 0.98, but when I plot it, it does not seem to hit the estimated intercept on Y axis. Its far below. Here are my data and function: </p>

<pre><code>event = c(2.2, 6.4, 3.4, 10.2, 4.45, 2.65, 8.25, 4.65, 3, 6.5, 5.25, 
8.65, 7.25, 6.4, 7.75, 7.45)

c(230208, 813178, 316617, 1531919, 576869, 270148, 1090947, 562643, 
439885, 745741, 666454, 1078175, 924429, 784333, 1091289, 948062)

fit=glm(event~size)

Call:  glm(formula = chr.co.count.wt ~ size)

Coefficients:
(Intercept)         size  
  9.783e-01    6.528e-06  

Degrees of Freedom: 15 Total (i.e. Null);  14 Residual
Null Deviance:      83.08 
Residual Deviance: 2.849    AIC: 23.8




plot(size,events,col=""blue"",pch=16,xlab=""size"",ylab=""events"",ylim=c(0,12),frame.plot=FALSE,xlim=c(0,2000000),axes = F)

axis(side = 1,at = c(0,0.5e6,1e6,1.5e6),labels =  c(0,0.5e6,1e6,1.5e6))
axis(side = 2,at = seq(from = 0,to = 12,by = 0.5),labels = seq(from = 0,to = 12,by = 0.5))
abline(fit.wt)
</code></pre>

<p><img src=""http://i.stack.imgur.com/iqDK2.png"" alt=""enter image description here""></p>

<p>Why is this discrepancy ? Am i missing something here ? I have also checked the std. err which is 0.27, still higher than what is being observed on plot. </p>

<p>Thank you.</p>
"
"0.0657004319817604","0.0667904674542028","115424","<p>I am new to R and I am trying to do some predictive modelling on data set which has 16 feature variables and the target value is numeric in R. I am not sure if the steps I am following will help me to fit the model in the best possible way. </p>

<ol>
<li>Handling the missing values: The data had a lot of missing values, so I replaced it with the mean of the column. Is this a right way to handle missing values?</li>
<li>Used Stepwise Regression to select the right set of most predictive variables in a model. Is there any better way to decide the variables than Stepwise regression.</li>
<li>After deciding the variables, I used glm() function to fit the model. </li>
</ol>

<p>Can someone please help me to understand the process of predictive modeling in R.</p>

<p>I was actually following the below document to get a sense of predictive modelling.
<a href=""http://blog.fractalanalytics.com/wp-content/uploads/2013/04/Predictive_Analytics_Methdology_Using_R_v1.0.pdf"" rel=""nofollow"">http://blog.fractalanalytics.com/wp-content/uploads/2013/04/Predictive_Analytics_Methdology_Using_R_v1.0.pdf</a></p>
"
"0.053644178078582","0.0545341883149212","115843","<p>I want to perform an exploratory Cox regression analysis of medical data using R. I am practicing using the pbc data from the survival function.</p>

<p>Would you recommend performing a backward selection multivariate analysis? Are there any summary data / tables I should create for covariates before modelling? Are there any model diagnostics I should perform? And what would be the consequence of doing this?</p>

<p>I would be very grateful for your help and examples using R; also easy to understand literature recommendations (paper, book, and so on) would be nice. </p>

<hr>

<p>To renew my former question: I understand that a stepwise backward regression will lead to inflated coefficients, deflated p-values, and inflated model fit statistics. However, this approach is very common in medical reports. Would it be possible to draw the conclusion that a covariate is independently associated with an outcome, irrespective the above mentioned drawbacks? And when yes, how reliable would it be?</p>

<p>And again <em>being a Little afraid to ask this</em> what would be the best way in R to perform such an analysis?</p>
"
"0.080466267117873","0.0818012824723818","116174","<p>I would like to set up a series of tests on the difference in survival between two very unequal sized groups.<br>
Generally either log-rank (using the R survdiff function) or a cox regression (R coxph) with stratified patient variables works well.  However, in some cases one group is small and the event relatively low incidence, which makes the expected number of events very small.  In those circumstances, it does not seem sensible to use the p-value generated by the log-rank test, since this is based on a chi-squared test, which is inappropriate for small numbers of expected events (surprisingly R does not give a warning message for this).  Taking an admittedly fairly extreme example to illustrate:</p>

<pre><code>survdiff(formula = survobject ~ (Fixation == i), data = TKRGroup)
n=637763, 424 observations deleted due to missingness.

                         N Observed Expected (O-E)^2/E (O-E)^2/V
Fixation == i=FALSE 637725    11174 1.12e+04  5.52e-04      11.9
Fixation == i=TRUE      38        3 5.17e-01  1.19e+01      11.9

Chisq= 11.9  on 1 degrees of freedom, p= 0.000555 
</code></pre>

<p>Cox regression gives a higher p-value of 0.0023, though it still looks a rather on the low side for these values of observed and expected events.</p>

<pre><code>coxph(formula = survobject ~ (Fixation == i), data = TKRGroup)

                  coef exp(coef) se(coef)    z      p
Fixation == iTRUE 1.76       5.8    0.577 3.05 0.0023
</code></pre>

<p>Further summary information gives</p>

<pre><code>Likelihood ratio test= 5.58  on 1 df,   p=0.01813
Wald test            = 9.27  on 1 df,   p=0.002325
Score (logrank) test = 11.92  on 1 df,   p=0.0005543
</code></pre>

<p>At this point, I could do with some expert advice on which, if any, of these p-values to use, or whether there is some alternative approach available (preferably available within an R package!)  Given the size of the groups, I rather naively attempted to get some idea of a sensible p-value by applying a Poisson exact test to the observed and expected figures; Values of observed / expected of 3 / 0.517 would give a cumulative Poisson P(X â‰¥ 3) = 0.0157.  That seems a much more reasonable figure, though I am not sure I could defend it.</p>
"
"0.0599760143904067","0.0609710760849692","116196","<p>My goal is to investigate a dependent variable which is metric (time in hours). The independent variables include 3 metric, 2 binary (factors), and one factor variable, which consists of 11 districts of a city.</p>

<p>I tried to conduct a GLM.</p>

<p>Can I put all this together in one model? It seems to be difficult to interpret the output!
Should I rather use different/various models, with only one factor per regression?
If I use the GLM which kind of family and link function should I use?</p>

<p>The idle time seems to be a right skewed distribution and thefore I chose a gamma family with a inverse link function. </p>

<p>The output of a model wich contains all the independent variables as decribed above: </p>

<p><img src=""http://i.stack.imgur.com/N1hEi.jpg"" alt=""enter image description here""></p>

<p>How can I eleminate the NAs? Or what do they actually mean? </p>

<p>Moreover the ANOVA test was conducted to get a closer look on the district variable called Bezirk, which shows massive differences in the mean value! Is this consistent with small coefficients in the GLM regression? (The means vary between from 3,7 in T.Mitte and 15 in T.Treptow)</p>

<p>Best regards</p>
"
"0.0758643241810882","0.0771229887279699","116659","<p>I have a collection of continuous data from the literature, including the mean, the standard deviation and the number of observations for both experimental and control groups, as well some environmental variables. A meta-analysis coupled with a meta-regression could be done. However, some studies had several experimental sites, i.e. each line is not a single study, but a single site taken from a study: a random site effect could thus be nested in studies. The following meta mixed-model can be described.</p>

<ol>
<li>effect-size as response</li>
<li>environmental variables as fixed effects,</li>
<li>sites nested in studies as random effects and</li>
<li>each observation is weighted on the sampling variance.</li>
</ol>

<p>I tried two approaches with R, that returned somewhat different results.</p>

<p>As first step, let's create a dummy data frame.</p>

<pre><code>set.seed(40)
Study = factor(c(1,1,1,1,1,2,2,3,3,3,4,5,5,5,5))
Site = factor(c(1,2,3,4,5,1,2,1,2,3,1,1,2,3,4))
n_case = length(Study)
experimental_mean = rnorm(n_case,5,2)
experimental_sd = abs(rnorm(n_case,1,0.1))
experimental_n = round(runif(n_case, 2, 10))
control_mean = experimental_mean+runif(n_case,0,5)
control_sd = abs(rnorm(n_case,1,0.1))
control_n = round(runif(n_case, 2, 10))
A = experimental_mean/control_mean * runif(n_case, 0.5, 0.8)
B = rnorm(n_case,-1,1)
C = factor(letters[round(runif(n_case, 1, 3))])

data_table = data.frame(Study, Site, 
                        experimental_mean, experimental_sd, experimental_n,
                        control_mean, control_sd, control_n,
                        A, B, C)
</code></pre>

<p>Then, compute the effect size and the sampling variance.</p>

<pre><code>library(metafor)
meta_table = escalc(measure='ROM', data=data_table,
                  m1i=experimental_mean, m2i=control_mean,
                  sd1i=experimental_sd, sd2i=control_sd,
                  n1i=experimental_n, n2i=control_n)
</code></pre>

<p>The rma.mv function from the metafor package could be used to run the meta mixed-model.</p>

<pre><code>res = rma.mv(yi, vi, data=meta_table, method=""REML"", level=95,
              mods = ~ A+B+C, random=~1|Study/Site)
summary(res)
</code></pre>

<p>Another option is to run a mixed-model on the effect-size.</p>

<pre><code>library(nlme)
mixed_meta_model = lme(data = meta_table,
                       fixed = yi ~ A+B+C, # yi is the effect size
                       random = ~1|Study/Site,
                       weights = varIdent(~vi)) # vi is the sampling variance
summary(mixed_meta_model)
</code></pre>

<p>Three questions:</p>

<ol>
<li>Is the global approach valid?</li>
<li>Which approach would you suggest?</li>
<li>Is my R code correct? - with special attention to the weights argument <strong>varIdent(~vi)</strong> in the lme function.</li>
</ol>

<p>Many thanks,</p>

<p>S.-Ã‰. Parent
Laval University, Canada</p>
"
"0.0379321620905441","0.0385614943639849","116681","<p>I am working on a large linear regression with a volume metric as my dependent. Right now I am multiplying the model.matrix by the respective coefficients to get to the relative volume contribution by variable.</p>

<pre><code>decomp =  t(apply(model.matrix(fit$terms, data = Data[Data$RetailRead == ""Retail"",]), 1, function(x) {x*fit$coef})) 
</code></pre>

<p>However, this leads to some of the volume being meaningless due to a factor variables within the model. The factor variables need to be added to the base category in order to get to the total overall volume for that variable. </p>

<pre><code>asgn = attr(model.matrix(fit$terms, data = Data), ""assign"") #find indexes which need to be summed

merged =data.frame(t(apply(decomp, 1, function(x) {tapply(x, asgn, sum)})))#sum each appropriate collumn
colnames(merged) = c(""Intercept"", attr(terms(fit), ""term.labels"")) #label collumns
</code></pre>

<p>The code I have written to summarize the volume driven per variable is clunky and inefficient - there are several further steps then above to allow each column of the variable decomposition to be able to stand alone. I am surprised that there is little literature on doing this within R and I wonder if any one know of a package to better perform this task.</p>
"
"0.0379321620905441","0.0385614943639849","116852","<p>I need help in understanding the <code>pmodel.response</code> function from the R package <code>plm</code>. So far I have interpreted this as a way to get predicted values from a panel data regression.</p>

<p>In the code below I run a least squares dummy variables regression using the standard <code>lm</code>-function and a fixed effects model using <code>plm</code> and then try to compare predictions and model response.</p>

<pre><code>library(plm)

data(Grunfeld)
Grunfeld &lt;- pdata.frame(Grunfeld, index = c(""firm"", ""year""))

grun.lm &lt;- lm(inv ~ value + capital + factor(firm), data=Grunfeld)
grun.fe &lt;- plm(inv ~ value + capital, data=Grunfeld, effect=""individual"",
               model=""within"")

Grunfeld$predict.lm &lt;- predict(grun.lm)
Grunfeld$predict.plm &lt;- pmodel.response(grun.fe)
</code></pre>

<p>Now, if I take a look at the outcome:</p>

<pre><code>&gt; head(Grunfeld)
       firm year   inv  value capital predict.lm predict.plm
1-1935    1 1935 317.6 3078.5     2.8   269.5876     -290.42
1-1936    1 1936 391.8 4661.7    52.6   459.3769     -216.22
1-1937    1 1937 410.6 5387.1   156.9   571.6005     -197.42
1-1938    1 1938 257.7 2792.2   209.2   302.0566     -350.32
1-1939    1 1939 330.8 4313.2   203.4   467.7566     -277.22
1-1940    1 1940 461.2 4643.9   207.2   505.3528     -146.82
</code></pre>

<p>It seems like the output of <code>pmodel.response</code> hardly has anything to do with predicted values. So, what does this function actually do? How to interpret the values in column <code>Grundfeld$predict.plm</code>? This does not get clear for me from the documentation.</p>

<p>Thanks for any help!</p>
"
"0.0599760143904067","0.0609710760849692","117192","<p>I would like to get a covariance matrix of fitted probabilities for a logistic regression model in R. I would like to do this because I want to find the variance of the difference between the two fitted probabilities ($\hat{p}_1 - \hat{p}_2$).</p>

<p>Here is my attempt:</p>

<pre><code>x&lt;-rnorm(10,10,10)
y&lt;-x+rnorm(10,0,15)
z&lt;-round(runif(10,0,1))

m1&lt;-glm(z~x+y,family=binomial(link = ""logit""))

predict(m1,newdata=data.frame(x=c(1,0),y=c(1,1)),se.fit=TRUE,vcov=TRUE,type=""response"")
</code></pre>

<p>This gives me the standard errors of the fitted probabilities but not the covariance. </p>

<p>I am aware of the delta method to find the distribution of a function of a normal distribution but I would really like to avoid using the delta method if possible because my actual code needs to be extremely flexible. It'll be difficult to implement the delta method properly in my actual situation.</p>
"
"0.053644178078582","0.0409006412361909","117867","<p>I have a basic linear regression model I fitted to a time series. Unfortunately I have to account for autocorrelation and heteroskedasicity in the model and I have done so with the NeweyWest function from the sandwich package in R while analyzing the coefficients. </p>

<p>Now I would like to create prediction intervals using the predict() function (or any other function) while utilizing the NeweyWest matrix/SEs.</p>

<p>As this is the first quesiton I post on here and my experinece in R is very limited here is some information:</p>

<pre><code>LMModel = lm(Return~Sentiment, data=Time Series)
</code></pre>

<p><strong>This is the function I used for my coefficient testing:</strong></p>

<pre><code>coeftest(LMModel , vcov=NeweyWest(LMModel , lag=27, ar.method=""ols""))
</code></pre>

<p><strong>I would like thsi function to use NeweyWest in some way:</strong></p>

<pre><code>predict(LMModel, newdata, interval = ""prediction"", level = 0.95) 
</code></pre>

<p>Thanks a lot in advance!</p>
"
"0.0402331335589365","0.0545341883149212","117910","<p>According to Wikipedia (source of all truth and knowledge...),
<a href=""http://en.wikipedia.org/wiki/Generalized_least_squares#Properties"" rel=""nofollow"">http://en.wikipedia.org/wiki/Generalized_least_squares#Properties</a></p>

<p>a weighted least square regression is equivalent to a standard least square regression, if the variables have been previously ""decorrelated"" using a Cholesky decomposition.</p>

<p>I made up then a very simple example with the function pgls from the package CAPER to test it, where the correlation arises from a phylogeny tree:</p>

<pre><code>tree.mod:
((A:0.2,(B:0.1,C:0.1):0.1):0.1,((E:0.1,F:0.1):0.1,D:0.2):0.1);
</code></pre>

<p>The two approaches are compared here:</p>

<pre><code>library(caper)

## Data
species = c(""A"",""B"",""C"",""D"",""E"",""F"")
gene = c(0.1,0.2,0.3,0.5,0.6,0.7)
pheno = c( 0,0,0,1,1,1)
data=data.frame(species,gene,pheno)

## Phylogeny
tree = read.tree( ""small/tree_small.mod"" )

## GLS regression
cat(""\n     ===&gt; GLS\n"")
cdata   = comparative.data( phy = tree, data = data, names.col = ""species"" )
res = pgls( pheno~gene, cdata )
print(summary(res))

## Cholesky
cat(""\n     ===&gt; Cholesky\n"")
corr = vcv( tree )
cholesky = chol( corr )
invCho = solve( cholesky )
data.gene =  invCho %*% as.vector( data$gene )
data.pheno =  invCho %*% as.vector( data$pheno )
res=lm( data.pheno ~ data.gene )
print(summary(res))
</code></pre>

<p>and yield the outputs:</p>

<pre><code>====&gt; GLS
Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -0.13000    0.27261 -0.4769  0.65834  
gene         1.63333    0.59489  2.7456  0.05161 .


=====&gt;Cholesky
Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  0.02214    0.28551   0.078    0.942  
data.gene    1.29188    0.35006   3.690    0.021 *
</code></pre>

<p>as you can see the results are different...</p>

<p>Does anyone have a clue why?</p>
"
"0.026822089039291","0.0272670941574606","117980","<p>I have some data that I'm fitting a multiple regression to, with the twist that the error distribution is t (with user-defined degrees of freedom) instead of Gaussian. I've been coding up my own function to do this, but recently I saw some posts that the <code>rlm</code> function in the MASS package has this ability already. Is this true? I have the accompanying book* and it mentions a number of M-estimators with Huber's as the default. I don't see how any of them correspond to the t distribution.</p>

<p>(Background: the t is because I'm interested in simulating the distribution of the response, not just estimating its conditional mean. From looking at the residuals, it appears that a suitably scaled t distribution with 4-5 df is a reasonable fit, and certainly much better than the Gaussian. It's only tangentially related to robustness considerations.)</p>

<p><br></p>

<p>* <em>Modern Applied Statistics with S</em> 4th Ed, Venables &amp; Ripley (2002)</p>
"
"0.080466267117873","0.0818012824723818","118206","<p>I'd like to perform vector autoregression on a two variable system. I know that the signals $x$ and $y$ have a time lag of > 100 time points, and thus any fit with that many time lag parameters is likely to be bad. While I could take the data at a lower frequency, the data is produced at one given frequency. Striding the data (taking every $s$-th observation) is possible, but it seems like it would be throwing out quite a bit of potentially useful data, and would make the analysis far more susceptible to the noise - in my experience, if I stride and then vary the time point I start from just a little, I get very different results. </p>

<p>I believe the right thing to do is obviously to fit this equation: </p>

<p>$$x_t = x_{t-s} + x_{t-2s} + x_{t-3s} + ... + x_{t-ps} + y_{t-s} + y_{t-2s} + y_{t-3s} + ... + y_{t-ps}$$</p>

<p>However, I can't seem to find existing code in R to do that. <code>VAR</code> and <code>ar</code> don't seem to have options to do this, as far as I can find. Does anyone have any suggestions, or do I need to code this up from scratch? Is there a name for this type of model that I'm unaware of?</p>

<p>I've thought about applying smoothing with the window size of $s$ and then striding, and when I do that, I get a significant result irregardless of if I vary the time point I begin at. To me, this indicates that there is certainly a significant time lag contribution (using the Wald test), but I think using the above equation is more rigorous. </p>

<p>EDIT: I wrote R code that allows you do to OLS fitting of vector autoregression when the lag is t - i*s. It functionally gives you everying ar() or VAR() does, but has the added flexibility of the s parameter. I'm working it into my own workflow, so it's a bit optimized, but if anyone needs it, comment and I'll send it over in a more workable form.</p>
"
"0.176058170535238","0.182956465042713","118215","<p>This is my first post on StackExchange, but I have been using it as a resource for quite a while, I will do my best to use the appropriate format and make the appropriate edits. Also, this is a multi-part question. I wasn't sure if I should split the question into several different posts or just one. Since the questions are all from one section in the same text I thought it would be more relevant to post as one question.</p>

<p>I am researching habitat use of a large mammal species for a Master's Thesis. The goal of this project is to provide forest managers (who are most likely not statisticians) with a practical framework to assess the quality of habitat on the lands they manage in regard to this species. This animal is relatively elusive, a habitat specialist, and usually located in remote areas. Relatively few studies have been carried out regarding the distribution of the species, especially seasonally.  Several animals were fitted with GPS collars for a period of one year. One hundred locations (50 summer and 50 winter) were randomly selected from each animal's GPS collar data. In addition, 50 points were randomly generated within each animal's home range to serve as ""available"" or ""pseudo-absence"" locations. The locations from the GPS collars are coded a 1 and the randomly selected available locations are coded as 0.</p>

<p>For each location, several habitat variables were sampled in the field (tree diameters, horizontal cover, coarse woody debris, etc) and several were sampled remotely through GIS (elevation, distance to road, ruggedness, etc). The variables are mostly continuous except for 1 categorical variable that has 7 levels.</p>

<p>My goal is to use regression modelling to build resource selection functions (RSF) to model the relative probability of use of resource units. I would like to build a seasonal (winter and summer) RSF for the population of animals (design type I) as well as each individual animal (design type III).</p>

<p>I am using R to perform the statistical analysis.</p>

<p>The <strong>primary text</strong> I have been using isâ€¦</p>

<ul>
<li>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</li>
</ul>

<p>The majority of the examples in Hosmer et al. use STATA, <strong>I have also been using the following 2 texts for reference with R</strong>.</p>

<ul>
<li>""Crawley, M. J. 2005. Statistics : an introduction using R. J. Wiley,
Chichester, West Sussex, England.""</li>
<li>""Plant, R. E. 2012. Spatial Data Analysis in Ecology and Agriculture 
Using R. CRC Press, London, GBR.""</li>
</ul>

<p>I am currently following the steps in <strong>Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates""</strong> and have a few questions about the process. I have outlined the first few steps in the text below to aid in my questions.</p>

<ol>
<li>Step 1: A univariable analysis of each independent variable (I used a
univariable logistic regression). Any variable whose univariable test
has a p-value of less than 0.25 should be included in the first
multivariable model.</li>
<li>Step 2: Fit a multivariable model containing all covariates
identified for inclusion at step 1 and to assess the importance of
each covariate using the p-value of its Wald statistic. Variables
that do not contribute at traditional levels of significance should
be eliminated and a new model fit. The newer, smaller model should be
compared to the old, larger model using the partial likelihood ratio
test.</li>
<li>Step 3: Compare the values of the estimated coefficients in the
smaller model to their respective values from the large model. Any
variable whose coefficient has changed markedly in magnitude should
be added back into the model as it is important in the sense of
providing a needed adjustment of the effect of the variables that
remain in the model. Cycle through steps 2 and 3 until it appears that all of the important variables are included in the model and those excluded are clinically and/or statistically unimportant. Hosmer et al. use the ""<em>delta-beta-hat-percent</em>""
as a measure of the change in magnitude of the coefficients. They
suggest a significant change as a <em>delta-beta-hat-percent</em> of >20%. Hosmer et al. define the <em>delta-beta-hat-percent</em> as 
$\Delta\hat{\beta}\%=100\frac{\hat{\theta}_{1}-\hat{\beta}_{1}}{\hat{\beta}_{1}}$.
Where $\hat{\theta}_{1}$ is the coefficient from the smaller model and $\hat{\beta}_{1}$ is the coefficient from the larger model.</li>
<li>Step 4: Add each variable not selected in Step 1 to the model
obtained at the end of step 3, one at a time, and check its
significance either by the Wald statistic p-value or the partial
likelihood ratio test if it is a categorical variable with more than
2 levels. This step is vital for identifying variables that, by
themselves, are not significantly related to the outcome but make an
important contribution in the presence of other variables. We refer
to the model at the end of Step 4 as the <em>preliminary main effects
model</em>.</li>
<li>Steps 5-7: I have not progressed to this point so I will leave these
steps out for now, or save them for a different question.</li>
</ol>

<p><strong>My questions:</strong> </p>

<ol>
<li>In step 2, what would be appropriate as a traditional level of
significance, a p-value of &lt;0.05 something larger like &lt;.25?</li>
<li>In step 2 again, I want to make sure the R code I have been using for the partial likelihood test is correct and I want to make sure I am interpreting the results correctly. Here is what I have been doing&hellip;<code>anova(smallmodel,largemodel,test='Chisq')</code> If the p-value is significant (&lt;0.05) I add the variable back to the model, if it is insignificant I proceed with deletion?</li>
<li>In step 3, I have a question regarding the <em>delta-beta-hat-percent</em> and when it is appropriate to add an excluded variable back to the model. For example, I exclude one variable from the model and it changes the $\Delta\hat{\beta}\%$ for a different variable by >20%. However, the variable with the >20% change in $\Delta\hat{\beta}\%$ seems to be insignificant and looks as if it will be excluded from the model in the next few cycles of Steps 2 and 3. How can I make a determination if both variables should be included or excluded from the model? Because I am proceeding by excluding 1 variable at a time by deleting the least significant variables first, I am hesitant to exclude a variable out of order.</li>
<li><p>Finally, I want to make sure the code I am using to calculate $\Delta\hat{\beta}\%$ is correct. I have been using the following code. If there is a package that will do this for me or a more simple way of doing it I am open to suggestions.  </p>

<p><code>100*((smallmodel$coef[2]-largemodel$coef[2])/largemodel$coef[2])</code></p></li>
</ol>
"
"0.026822089039291","0.0272670941574606","118322","<p>I have a dataset which seems to have alot of zeroes. I have already fitted a poisson regression model as well as a negative binomial model. I would like to fit zero-inflated and hurdle models as well.</p>

<p>Before I do I would like to run a test to investigate whether my data really is zero inflated. Is there any function to do this in R?</p>
"
"0.0464572209811883","0.0472279924554862","118396","<p>I'm trying to fit a quantile regression model for rigth censoring data and I'm using R with the package quantreg and its function  crq. I'm trying the Portnoy method that it's suposed to estimate the full range of tau (quantiles) , but the results only contains 85 taus, ending in tau=0.4 and giving me estimated values of that lasts taus that are very far away from the real 0.4 quantile and more around 0.95 quantile.</p>

<p>I read in the quantreg doc that Portnoy and Peng-Huang may be unable to estimate upper conditional quantiles if censoring is heavy in the upper in the upper tail, but this doesn't seem to be my case. In fact the general distribution summary of time is:</p>

<pre><code>Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.0   223.0   447.0   482.5   714.0  1251.0 
</code></pre>

<p>the quantiles vary obviously with levels of VAR1 and VAR2 but the shape of the distribution is almost the same.I can produce Kaplan-Mier estimation without problem. </p>

<p>Here is my syntax</p>

<pre><code>qreg1&lt;-crq(Surv(TIME,EVENT,type=""right"")~VAR1+VAR2,
       data=DATA_TRAIN,method = ""Portnoy"")
</code></pre>

<p>What am I doing wrong?</p>

<p>Thank you in advance,</p>
"
"0.0900306369383777","0.105605001571314","118621","<p>I am trying to perform a relatively simple multiple regression with a single breakpoint using the <em>segmented</em> package in R. My question is how to handle the interaction between the predictor variables as follows.</p>

<p>My model is: $$Runoff = \beta_0+\beta_1(A+\beta_2P)$$ where there is a breakpoint at some value of $(A+\beta_2P)$. I expect $\beta_0$ and $\beta_1$ to be different on each side of the breakpoint, and $\beta_2$ to be the same (in fact I'll just say <em>a priori</em> that $\beta_2$ should not vary across the breakpoint, but this would be interesting to test)</p>

<p>My question is how to implement this using <em>segmented</em> (or a similar breakpoint linear regression model). Options are:<br>
1. Run some loop through values of $\beta_2$ and create an intermediate variable $\tau = (A+\beta_2P)$, then it's a simple application of <em>segmented</em>. My concern is that I'd like to get a fitted value of $\beta_2$ from the data and this seems like a crude way of doing it.<br>
2. Some clever way of using <em>segmented</em> on the fully explicit model that will estimate $\beta_0, \beta_1,$ and $\beta_2$<br>
any help appreciated</p>

<p><strong>ADDENDUM:</strong></p>

<p>To describe what I'm trying to do. This is for a watershed runoff generation project. My hypothesis is that runoff amount is a function of both precipitation and soil water table position, and that there is a very strong threshold response in the latter. Whenever it rains, the water table goes up, but if it does not reach the threshold no runoff is generated. If enough rain falls to raise the water table above the threshold, then runoff is generated. Thus, runoff is a function of $(Ant + Pcp)$ with a notable breakpoint. </p>

<p>So, in the model as formulated in my original question $\beta_0$ and $\beta_1$ are the slope and intercept of the runoff response, and $\beta_2$ is a coefficient that converts rainfall depth to water table rise, which I assume is something like the porosity of the soil.  </p>

<p>I'm interested in (in order of importance): (1) is this model better than just a linear relationship between runoff and precipitation?; (2) can the model estimate where the water table threshold is?; (3) Can the model estimate the ""porosity"" coefficient (this would be a nice check against physical reality and I think also improve the physical fidelity of the estimated threshold position).  </p>

<p>Below is a graph using a hand-picked value of $\beta_2$, which let me do a piece wise linear regression. Solid line is the piece wise, dashed is a linear regression between runoff and precip alone. I'd like to be able to estimate the parameters of the model ($\beta_0, \beta_1, \beta_2$, and the breakpoint position) from the data. Ultimately, I have three replicates of three watershed types, so I'm interested in comparisons there as well.</p>

<p>Thanks for the help,</p>

<p><img src=""http://i.stack.imgur.com/L5JuQ.jpg"" alt=""enter image description here""></p>
"
"0.0379321620905441","0.0385614943639849","119946","<p>I have some time series data where I'm modelling temperature as a function of various predictors. On physical grounds, I can expect that</p>

<p>$$\frac{dT}{dt} \propto T_a - T$$</p>

<p>where $T_a$ is the ambient temperature (which can vary over time, but whose values are known). I thus fit models of the form</p>

<p>$$\Delta T(t) \sim \alpha + \beta \left[ T_a(t) -T(t) \right] + \gamma X(t)$$</p>

<p>with $X$ being the other covariates, and $\alpha$, $\beta$ and $\gamma$ are the regression parameters. I can fit these easily enough in R:</p>

<pre><code>lm(diff(T) ~ I(Ta - T) + x, data=df)
</code></pre>

<p>and I can get predictions for the change in $T$. However, what I really want are predictions for $T$ itself. At the moment I'm calculating these via a loop, where I plug $\hat{T}(t)$ into the regression equation to obtain $\hat{\Delta T}(t+1)$.</p>

<p>Is there any R package, probably time series-related, that will do these calculations automatically?</p>

<p>Also, if there are any issues with this approach, I'd be happy to know about them.</p>
"
"0.0709645772411954","0.072141950116023","120008","<p>I am interested in fitting an ARIMAX model using R.
As known, ARIMAX can be understood as a composition of ARIMA models and regression models with exogenous (independent) variables. I have a time series $Y_i$, and want to estimate the ARIMA and nonlinear coefficients. The nonlinear model is the following:</p>

<p>$y_i=Î²_0+Î²_1t_i+Î²_2d+Î²_3 sin(2Ï€t_i/Î²_4 )+Î²_5 (-1^{t_i})+Îµ_i$,  nonlinear 
regression with an exogenous variable.
Where 
$t_i$ =1, 2â€¦, 60
and</p>

<p>d = dummy variable with 20 0's and 40 number 1's</p>

<pre><code>d=c(rep(0,20),rep(1,40))
</code></pre>

<p>And an ARIMA model (1,1,1) for $Y_i$. Therefore, I want to estimate simultaneously the $Î²_i$ and the ARIMA coefficients in order to avoid the confusion between the exogenous coefficients and ARIMA coefficients.  I know that $arima()$ can deal with this formulation but, how do the nonlinear model can be set within function function?. It seems that the <em>xreg</em> term only deals with linear parameters.</p>
"
"0.0547503599848004","0.0667904674542028","120201","<p>Being aware of <a href=""http://www.ssicentral.com/lisrel/techdocs/HowLargeCanaStandardizedCoefficientbe.pdf"" rel=""nofollow"">that article</a>, I am curious about the question how big standardized coefficients can get. I had a discussion with my professor about that issue and she was arguing standardized coefficients (beta) in multiple linear regressions can not become greater than |1|. I have also heard that predictors with standardized coefficients greater than 1 should not be be included/appear in multiple linear regression. When I recently estimated a multiple linear regression in R using lm(), I estimated the standardized coefficients with lm.beta() function from the package 'lm.beta'. In the results I could observe a standardized coefficient greater than one. Right now I am just not sure about what is the truth.</p>

<p>Can standardized coefficients become greater than |1|?
If yes, what does that mean and should they be excluded from the model?
If yes, why?</p>

<p>I would be very thankful, if somebody could make this issue clear for me. </p>

<p>Thanks in advance!!</p>
"
"0.026822089039291","0.0272670941574606","120747","<p>I'm trying to run a SVM regression on some data and I want to use <code>ksvm</code> from <code>kernlab</code> or <code>svm</code> from <code>e1071</code>. But the number and type of kernels available is too restrictive. So I'm thinking of writing my own kernel function.</p>

<p>Let's assume I want to write the kernel for heavy-tailed RBF for which the formula is</p>

<p>$$
K(x_i, x_j) = \exp\left(-\gamma * ||x_i^a - x_j^a||^b\right)
$$</p>

<p><strong>How do I do this?</strong></p>

<p>I found several instances where people do something similar to the following code. </p>

<pre><code>kp=function(xi,xj){

    diff &lt;- xi^a - xj^a

    absValue &lt;- abs(diff^b)

    exp(-gamma * absVal)
}

class(kp) &lt;- ""kernel""

model &lt;- ksvm(xtrain, ytrain, type=""eps-svr"", kernel=kp, cross=10)
</code></pre>

<p>Is this correct or is there something else that is needed? Also how do I get the best values of <code>a</code>, <code>b</code> and $\gamma$ for my data?</p>
"
"0.116914775576919","0.106343507083076","120892","<p>I have on question regarding standardized coefficients (beta) in linear models. I have already asked one question <a href=""http://stats.stackexchange.com/questions/120201/magnitude-of-standardized-coefficients-beta-in-multiple-linear-regression"">here</a>. From the answers I assume that I should use R's <code>scale()</code> function on the dependent variable as well as on all independent variables (IV), to estimate the standardized coefficients for the model. But when I used the <code>scale()</code> function on an IV, which belongs to the factor class I get following error message:</p>

<p><code>Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric</code></p>

<p>To illustrate my problem here is a MWE:</p>

<p>First the linear model with unstandardized coefficients:</p>

<pre><code>&gt; data(ChickWeight)
&gt; aa &lt;- lm(weight ~ Time + Diet, data=ChickWeight)
&gt; summary(aa)

Call: 
lm(formula = weight ~ Time + Diet, data = ChickWeight)

Residuals:
     Min       1Q   Median       3Q      Max 
-136.851  -17.151   -2.595   15.033  141.816 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  10.9244     3.3607   3.251  0.00122 ** 
Time          8.7505     0.2218  39.451  &lt; 2e-16 ***
Diet2        16.1661     4.0858   3.957 8.56e-05 ***
Diet3        36.4994     4.0858   8.933  &lt; 2e-16 ***
Diet4        30.2335     4.1075   7.361 6.39e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 35.99 on 573 degrees of freedom
Multiple R-squared:  0.7453,    Adjusted R-squared:  0.7435 
F-statistic: 419.2 on 4 and 573 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Now I want to estimate the standardized coefficients using the <code>scale</code> function, which results in following error message:</p>

<pre><code>&gt; bb &lt;- lm(scale(weight) ~ scale(Time) + scale(Diet), data=ChickWeight)
Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric
</code></pre>

<p>As I figured out by myself the error message appears, because <code>Diet</code> belongs to the factor class and is not a numeric variable as required from the <code>scale()</code> function. I tried the following alternatively by including the <code>Diet</code> variable without <code>scale()</code>:</p>

<pre><code>&gt; cc &lt;- lm(scale(weight) ~ scale(Time) + Diet, data=ChickWeight)
&gt; summary(cc)

Call:
lm(formula = scale(weight) ~ scale(Time) + Diet, data = ChickWeight)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.92552 -0.24132 -0.03652  0.21151  1.99538 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.24069    0.03415  -7.048 5.25e-12 ***
scale(Time)  0.83210    0.02109  39.451  &lt; 2e-16 ***
Diet2        0.22746    0.05749   3.957 8.56e-05 ***
Diet3        0.51356    0.05749   8.933  &lt; 2e-16 ***
Diet4        0.42539    0.05779   7.361 6.39e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.5064 on 573 degrees of freedom
Multiple R-squared:  0.7453,    Adjusted R-squared:  0.7435 
F-statistic: 419.2 on 4 and 573 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>My question now is, if this is the right way to estimate the standardized coefficients for a model with both numeric and factor variables?</p>

<p>Thank you very much in advance for an answer.</p>

<p>Regards,</p>

<p>Magnus</p>
"
"0.0379321620905441","0.0192807471819925","121131","<p>I'm new to R and I've been searching for a while for a function which can reduce the number of explanatory variables in my lda function (linear discriminant analysis).</p>

<p>Basically, I've loaded the dataset and ran the lda function on my binomial dependent variable explained by 30 independent variables. (received a warning that the independent variables are collinear).</p>

<p>My professor has shown us stepwise feature selection (leaps package, regsubsets function) in a regression framework, but these codes aren't compatible for LDA/QDA.</p>

<p>Thanks in advance,
Marvin</p>
"
"0.0967084173462244","0.0983129061176287","121192","<p>I have some data I need to fit a model to that can be used for prediction (interpolation). The data is summarized by the plot below. The black line is x=y.</p>

<p><img src=""http://i.stack.imgur.com/x0FqM.png"" alt=""enter image description here""></p>

<p>I want to be able to fit a model so as I can use it to predict any value of the y axis as a function of x axis, as well as get the uncertainty in that estimate.</p>

<p>However in my data, the variance of the y axis variable increases as the x axis variable increases.
In addition, there is another continuous explanatory variable called SequenceSize (plotted as factor to clearly see the colours) which I think I have to take into account, as it is also correlated (negatively) with the variance of the y axis variable, whilst not really affecting the mean so much. As can be seen in the two plots below.</p>

<p><img src=""http://i.stack.imgur.com/7KdHZ.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/d43W1.png"" alt=""enter image description here""></p>

<p>So from a model fit to the data I would like to be able to use it to do.</p>

<ol>
<li>Plugin the value of SequenceSize and the x axis variable.</li>
<li>Get out an estimate of the y axis variable along with some measure of uncertainty in the y estimate, given how the variance and uncertainty is affected by the x axis variable and by SequenceSize.</li>
</ol>

<p>However I'm reading the massive R book by Crawley, and I'm having trouble deciding which model would be best to do this. I'm thinking maybe a multiple regression if I linearized the data by taking log of x and y, but I'm unsure if that's right because of how the variance of the data acts.  </p>

<p>Thanks,
Ben W.</p>
"
"0.0402331335589365","0.0545341883149212","121315","<p>I am very new to statistical analysis and R. Recently I worked on a simple linear regression model to predict values. For example: consider the below data set</p>

<pre><code>Col A    Col B
1         10
2         16
3         67
4         ?
5         ?
</code></pre>

<p>For such data i was able to predict the values using <code>lm</code> and <code>predict</code> functions in R. </p>

<p>Now, suppose I am given the below data set:</p>

<pre><code>inventory  jan-sales   feb-sales   mar-sales   apr-sales  may-sales
12         4           0           2           ?          ?
190        54          67          89          ?          ? 
123        67          22          11          ?          ? 
654       167          100        300          ?          ?
789       567          10         80           ?          ? 
543       223          221        0            ?          ?
</code></pre>

<p>In this data set, each line has total available units of a particular item and how many of those items were sold on each month. Now based on this data if I am asked to predict how many will be sold in the months of April and May, how do I use R and Linear regression or multiple linear regression to predict sales for April and May?</p>
"
"0.026822089039291","0.0272670941574606","121480","<p>I'm following the Caret package tutorial for constructing customized functions for a <a href=""http://topepo.github.io/caret/featureselection.html"" rel=""nofollow"">recursive feature elimination</a>. I can reproduce the provided example which is a random forest regression. However, when I modify the code to deal with classification, I receive an odd error:</p>

<pre><code>library(caret)
library(mlbench)
library(Hmisc)
library(randomForest)

n &lt;- 100
p &lt;- 40
sigma &lt;- 1
set.seed(1)
sim &lt;- mlbench.friedman1(n, sd = sigma)
colnames(sim$x) &lt;- c(paste(""real"", 1:5, sep = """"),
                         paste(""bogus"", 1:5, sep = """"))
    bogus &lt;- matrix(rnorm(n * p), nrow = n)
    colnames(bogus) &lt;- paste(""bogus"", 5+(1:ncol(bogus)), sep = """")
    x &lt;- cbind(sim$x, bogus)
y &lt;- sim$y
#customizing tutorial example for binary outcome

y[y &lt;= 12] &lt;- 0    
y[y&gt; 12] &lt;- 1

y &lt;- factor(y)


normalization &lt;- preProcess(x)
x &lt;- predict(normalization, x)
x &lt;- as.data.frame(x)
subsets &lt;- c(1:5, 10, 15, 20, 25)
rfRFE &lt;-  list(summary = defaultSummary,
                      fit = function(x, y, first, last, ...){
             library(randomForest)
             randomForest(x, y, importance = first, ...)
             },
           pred = function(object, x)  predict(object, x),
           rank = function(object, x, y) {
             vimp &lt;- varImp(object)
             vimp &lt;- vimp[order(vimp$Overall,decreasing = TRUE),,drop = FALSE]
                 vimp$var &lt;- rownames(vimp)
             vimp
             },
           selectSize = pickSizeBest,
           selectVar = pickVars)

ctrl &lt;- rfeControl(functions = lmFuncs,
                   method = ""repeatedcv"",
                   repeats = 5,
                   verbose = FALSE)
ctrl$functions &lt;- rfRFE
    ctrl$returnResamp &lt;- ""all""
set.seed(10)
rfProfile &lt;- rfe(x, y, sizes = subsets, rfeControl = ctrl)
rfProfile
</code></pre>

<p>The error is:</p>

<pre><code>Error in {: task 1 failed - ""argument 1 is not a vector""
</code></pre>

<p>My question is how should one go about defining <code>rfRFE</code> for random forest models with binary response variables?</p>
"
"NaN","NaN","121749","<p>I am wondering about the exact definition of ARIMA model in function <code>arima</code> in <code>R</code> when exogenous regressors are included.</p>

<p>I understand that <code>arima(y, order=c(p,0,q), xreg=x)</code> is equivalent to estimating the following equation (where $\mu_y$ and $\mu_x$ stand for the means of $y$ and $x$, respectively):</p>

<p><strong>(1)</strong> $(y_t-\mu_y)=\varphi_0+\phi_1(y_{t-1}-\mu_y)+...+\varphi_p(y_{t-p}-\mu_y)+\varepsilon_t+\theta_1\varepsilon_{t-1}+...+\theta_q\varepsilon_{t-q}+\beta_1x_t$</p>

<p>Or is it</p>

<p><strong>(2)</strong> $(y_t-\mu_y)=\varphi_0+\phi_1(y_{t-1}-\mu_y)+...+\varphi_p(y_{t-p}-\mu_y)+\varepsilon_t+\theta_1\varepsilon_{t-1}+...+\theta_q\varepsilon_{t-q}+\beta_1(x_t-\mu_x)$</p>

<p>(only the last term differs between <strong>(1)</strong> and <strong>(2)</strong>)?</p>

<p>Or perhaps I got both of them wrong?</p>

<p><strong>Edit:</strong> I now realize that including both {$\mu_x$ and $\mu_y$} and $\varphi_0$ in <strong>(2)</strong> was superfluous.</p>
"
"0.0379321620905441","0.0385614943639849","121823","<p>I am quite new in the R universe, so please excuse me if the question is too simple..</p>

<p>I would like to perform a logistic regression on a marketing data set (only categorical variables), of the form [outcome, X1,X2,X3,X4,X5,X6]</p>

<p>I split the data set into a training set and a validation set.</p>

<p>My problem: Predictor X1 has originally 3 levels. The model using glm retains only 2 of these 3 levels.</p>

<p>When I try to run the model on the validation set (where X1 still has 3 levels) I get an error message stating that the factor X1 has now a new level. </p>

<p>How can I prevent the glm function from excluding factor levels? I don't mind if their coefficients are set to zero. </p>

<p>Thanks for any help on this. Tried all sites, but to no avail. </p>
"
"NaN","NaN","122036","<p>I want to study the relationship between two variables. I've got the following scatter plot.</p>

<p><img src=""http://i.stack.imgur.com/HQVnk.png"" alt=""enter image description here""></p>

<p>But now I'm hesitating on what to do with this:</p>

<ol>
<li>Should I check the assumptions of OLS and then use the lm function?</li>
<li>Or should I remove some outliers first?</li>
<li>Or should I use ridge regression with the <code>rlm</code> function?</li>
<li>Or should I check whether there's an quadratic relation?</li>
<li>...</li>
</ol>
"
"NaN","NaN","122165","<p>I have a basic question about quantile regression (I'm new to it):
Why doesn't it seem possible to do a quantile regression with a specified family (e.g. gamma) and link function (e.g. log), as in a glm? Or if it is possible, is there an R package that can do this? I've looked over the internet but no such thing seems to exist and I can't figure out why.</p>
"
"0.0871717893776958","0.109068376629842","122212","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 0-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>Using these models, given the dichotomous dependent variable, I have built a logistic regression using lrm.</p>

<p>The method of model variable selection was based on existing clinical literature modelling the same diagnosis. All have been modelled with a linear fit with the exception of ISS which has been modelled traditionally through fractional polynomials. No publication has identified known significant interactions between the above variables.</p>

<p>Following advice from Frank Harrell, I have proceeded with the use of regression splines to model ISS (there are advantages to this approach highlighted in the comments below). The model was thus pre-specified as follows:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ Age + GCS + rcs(ISS) +
    Year + inctoCran + oth, data = ASDH_Paper1.1, x=TRUE, y=TRUE)
</code></pre>

<p>Results of the model were:</p>

<pre><code>&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Age + GCS + rcs(ISS) + Year + inctoCran + 
    oth, data = ASDH_Paper1.1, x = TRUE, y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          2135    LR chi2     342.48    R2       0.211    C       0.743    
 0            629    d.f.             8    g        1.195    Dxy     0.486    
 1           1506    Pr(&gt; chi2) &lt;0.0001    gr       3.303    gamma   0.487    
max |deriv| 5e-05                          gp       0.202    tau-a   0.202    
                                           Brier    0.176                     

          Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept -62.1040 18.8611 -3.29  0.0010  
Age        -0.0266  0.0030 -8.83  &lt;0.0001 
GCS         0.1423  0.0135 10.56  &lt;0.0001 
ISS        -0.2125  0.0393 -5.40  &lt;0.0001 
ISS'        0.3706  0.1948  1.90  0.0572  
ISS''      -0.9544  0.7409 -1.29  0.1976  
Year        0.0339  0.0094  3.60  0.0003  
inctoCran   0.0003  0.0001  2.78  0.0054  
oth=1       0.3577  0.2009  1.78  0.0750  
</code></pre>

<p>I then used the calibrate function in the rms package in order to assess accuracy of the predictions from the model. The following results were obtained:</p>

<pre><code>plot(calibrate(rcs.ASDH, B=1000), main=""rcs.ASDH"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/HYTsp.png"" alt=""Bootstrap calibration curves penalized for overfitting""></p>

<p>Following completion of the model design, I created the following graph to demonstrate the effect of the Year of incident on survival, basing values of the median in continuous variables and the mode in categorical variables:</p>

<pre><code>ASDH &lt;- Predict(rcs.ASDH, Year=seq(1994,2013,by=1),Age=48.7,ISS=25,inctoCran=356,Other=0,GCS=8,Sex=""Male"",neuroYN=1,neuroFirst=1)
Probabilities &lt;- data.frame(cbind(ASDH$yhat,exp(ASDH$yhat)/(1+exp(ASDH$yhat)),exp(ASDH$lower)/(1+exp(ASDH$lower)),exp(ASDH$upper)/(1+exp(ASDH$upper))))
names(Probabilities) &lt;- c(""yhat"",""p.yhat"",""p.lower"",""p.upper"")
ASDH&lt;-merge(ASDH,Probabilities,by=""yhat"")
plot(ASDH$Year,ASDH$p.yhat,xlab=""Year"",ylab=""Probability of Survival"",main=""30 Day Outcome Following Craniotomy for Acute SDH by Year"", ylim=range(c(ASDH$p.lower,ASDH$p.upper)),pch=19)
arrows(ASDH$Year,ASDH$p.lower,ASDH$Year,ASDH$p.upper,length=0.05,angle=90,code=3)
</code></pre>

<p>The code above resulted in the following output:</p>

<p><img src=""http://i.stack.imgur.com/KGYcz.png"" alt=""Year trend with lower and upper""></p>

<p><strong><em>My remaining questions are the following:</em></strong></p>

<p><strong>1. Spline Interpretation</strong> - How can I calculate the p-value for the splines combined for the overall variable?</p>
"
"0.156640226459387","0.154815736751535","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.0464572209811883","0.0472279924554862","122929","<p>When a regression is calculated with a simple linear model that returns intercept and slope for an equation like this y=<em>a</em> + <em>b</em>x one can predict y (the response variable) based on that equation. Equally one could rearrange for x; x={y-<em>a</em>}/<em>b</em> and calculate the value of x. This isn't available in R's <code>predict()</code> function but can easily be done. Can one do this calculation and still be statistically sound?</p>
"
"0.018966081045272","0.0385614943639849","122935","<p>I have a design involving 1 between-subjects categorical factor, 1 between-subjects continuous factor and 2 within-subjects categorical factors.</p>

<p>This is theoretically a case of multiple regression, but as in this question <a href=""http://stats.stackexchange.com/questions/48455/anova-or-regression-1-continuous-factor-1-categorical-factor-with-continuous"">here</a>, I was able to use <code>lm()</code> in <code>R</code> to specify my model (actually, using the <code>aov.car()</code> function in the <code>afex</code> package).</p>

<p>This means that my output was in the form of an ANOVA table, with F and p-values. Can I report my analysis as though it was an ANOVA, or should I find a way to force the output of regression coefficients and report the analysis as though it were a multiple regression? I know that I shouldn't use an ANCOVA in this case, because my continuous predictor variable is certainly not a nuisance variable!</p>
"
"0.0599760143904067","0.0609710760849692","123511","<p>EDIT: SOLVED <em>The problem seems to have been an explanatory variable that was a factor. If it is made binary numeric insted, the values of BIC and AIC is calculated alright. However, the analyses give the same parameter estimates such as z-values, variances, p-values. I leave the original post as a reference.</em></p>

<p><strong>Original post:</strong></p>

<p>I have some problems calculating AIC or BIC values for a structural equation model (SEM) fitted with Lavaan. (I fail to find anything when searching the web for the error message or parts of it.)</p>

<p>My model:</p>

<pre><code>model&lt;- '
        dependant ~ var1 + var2 + var3
        var1 ~ var2
        var3 ~ var1 + var2
        '
semla&lt;-sem(model, data=dat, missing=""listwise""
summary(semla)
</code></pre>

<p>With the result:</p>

<pre><code>lavaan (0.5-16) converged normally after  38 iterations

                                              Used       Total
Number of observations                            47          59

Estimator                                         ML
Minimum Function Test Statistic                0.000
Degrees of freedom                                 0
P-value (Chi-square)                           0.000

Parameter estimates:

Information                                 Expected
Standard Errors                             Standard

               Estimate  Std.err  Z-value  P(&gt;|z|)
Regressions:
dependant ~
 var1            0.121    0.071    1.687    0.092
 var2            0.015    0.006    2.750    0.006
 var3            1.721    2.134    0.807    0.420
var1 ~
 var2           -0.018    0.011   -1.663    0.096
var3 ~
 var1           -0.003    0.005   -0.685    0.493
 var2            0.000    0.000    0.450    0.653

Variances:
 dependant       0.049    0.010
 var1            0.205    0.042
 var3            0.000    0.000
</code></pre>

<p>Howvere, when i do <code>AIC(semla)</code> or <code>BIC(semla)</code>i get this error message:</p>

<pre><code>Error in fitMeasures(object, c(""logl"", ""npar"", ""ntotal"")) : 
object 'logl.H0' not found` 
</code></pre>

<p>Unfortunately I am not able to figure out what is wrong. As I have tried to simplify my model in several steps and the error persists it seems to be something fundamental. As I mentioned before, I'm not able to get any information by googling for the error message or part of it.</p>

<p>Any help to solve the matter would be much appreciated.</p>

<p>(I have also posted this question on <em>Stack overflow</em>, but realised that <em>Cross validated</em> might be a more suitable place to ask, please feel free to express your opinions on the matter. <a href=""http://stackoverflow.com/q/26844510/3489824"">http://stackoverflow.com/q/26844510/3489824</a>)</p>
"
"0.131791357733649","0.139130903795187","123576","<p>I am trying to test the effect on the heat flux between indoors and outdoors before and after removing insulation.</p>

<p>Briefly, I have 26 sensors on a wall, measuring heat flow between indoors and outdoors over a number of days. The wall was part of a real world experimental setup so that the insulation on the wall was removed halfway through the experiment. Â What I care about is to have a measure of the effect of the removal of the insulation (I am not interested in any form of forecasting). Â I am exploring the use of a SARIMA/ARIMAX models with one regressor because, aside from the removal of the insulation, the heat flow between indoors and outdoors was affected by daily cyclical and random environmental effects (heating on or off, daily temperature changes, wind, etc).  Here I will present that data and analysis of one sensor.  My data has been collected hourly, and I have transformed the variable â€˜insulatedâ€™ â€˜not insulatedâ€™ as a factor of 0s and 1s as indicator.</p>

<pre><code>heat.flux = c(8.677048,6.558642,5.920314,5.583614,5.373176,5.253928,4.938272,7.358305,9.743266,10.46577,11.06201,10.90067,11.49691,13.15236,12.10017,10.60606,10.45875,10.03788,9.588945,9.287318,8.578844,8.024691,10.26936,11.8757,10.20623,8.634961,8.305275,8.101852,8.12991,7.947531,7.814254,10.40264,13.08221,14.3729,14.94809,15.08838,15.20763,15.75477,14.57632,12.79461,11.97391,10.97082,10.33249,9.701178,9.715208,9.083895,10.63412,12.07912,9.736251,7.638889,6.453423,5.983446,5.499439,5.099607,4.70679,6.972503,9.259259,9.981762,10.24832,10.17116,10.27637,10.27637,9.546857,7.568743,7.168911,6.867284,6.705948,6.916386,8.319304,8.424523,11.41274,13.52413,11.70034,9.532828,8.957632,9.07688,9.694164,9.301347,9.048822,12.28255,14.95511,15.22868,15.24972,15.12346,15.08838,15.17256,13.68547,12.18434,12.1633,12.13524,11.81257,11.58109,11.44781,11.27946,13.87486,15.92312,14.07828,11.90376,10.46577,9.518799,8.978676,8.803311,8.684063,11.65123,14.39394,15.69865,16.61756,16.828,16.83502,16.16863,14.23962,12.19837,12.09315,11.5881,11.20932,10.50786,10.59203,10.64815,13.51712,15.71268,13.92396,12.10718,12.2615,11.65123,11.05499,10.31846,9.834456,12.9349,15.41807,15.78283,15.8179,16.11953,15.95118,15.63552,13.1243,11.22334,10.21324,8.705107,7.526655,6.15881,5.30303,5.597643,8.599888,11.17424,9.631033,8.038721,7.638889,7.203984,7.161897,6.76908,6.888328,9.518799,12.40881,13.21549,14.28872,14.43603,14.8078,14.81481,13.60129,12.59119,11.86167,11.91779,11.73541,12.04405,11.51796,11.74242,13.7486,15.85999,14.84989,12.63328,10.68322,9.343434,8.592873,8.333333,8.445567,10.97783,13.82576,15.12346,16.58249,17.61364,18.30808,19.10774,17.97138,16.62458,15.867,16.07744,15.63552,16.0073,15.42508,15.01122,17.10157,18.94641,22.44669,18.94641,16.01431,14.55527,13.88889,12.77357,11.66526,12.46493,15.41807,16.75786,17.27694,17.03143,16.84905,16.828,16.02834,16.35802,16.04237,15.03928,14.00112,14.1344,13.86785,13.99411,15.30584,18.20286,19.49355,16.16162,14.05022,12.05107,12.27553,13.01207,12.5491,13.72054,16.91218,18.62374,18.79209,20.80527,19.50758,20.18799,20.63692,18.49747,17.25589,17.38215,18.40629,18.60269,19.12177,18.66582,21.09989,24.45286,26.71156,23.54798,20.01964,17.98541,14.83586,14.31678,15.15152,15.30584,17.95735,19.71801,20.30724,20.19501,20.2862,20.1459,20.10382,18.20988,16.54742,15.22868,13.96605,12.71044,11.61616,10.71829,12.12121,14.77273,14.04321,12.44388,10.94978,10.2413,9.708193,9.638047,9.322391,11.27245,14.24663,14.77273,14.75168,14.92705,15.47419,15.48822,14.73765,13.68547,12.65432,12.35269,12.34568,12.32464,12.7385,12.84371,14.16947,17.34007,17.09456,15.0954,13.40488,11.70735,10.8165,10.64815,12.01599,13.55219,16.7298,17.45932,17.61364,19.58474,20.02666,19.79517,19.38833,17.32604,16.11953,15.62851,15.01122,14.70258,14.5693,14.35887,16.28086,18.69388,18.92536,16.56846,15.97222,13.34877,12.81566,12.04405,13.23653,14.1835,16.75786,17.55752,17.98541,18.85522,18.8482,19.02357,18.96044,17.31201,15.42508,14.38692,13.57323,12.36672,12.03002,11.41274,13.15236,15.88103,14.66049,12.8858,11.67228,11.03395,9.399551,8.375421,8.073793,10.6271,13.57323,13.61532,14.31678,14.73765,15.08838,15.62149,16.6807,15.28479,14.07127,13.14534,12.61223,12.57015,12.02301,12.17031,14.33782,18.83418,20.45455,18.67985,18.40629,16.51235,14.45006,14.61841,15.20763,15.57941,18.06958,19.88636,20.51066,21.633,23.24635,24.28451,24.70539,24.19332,22.81145,21.97671,21.58389,21.3945,21.21212,20.89646,21.1069,23.86364)

insulation = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
</code></pre>

<p>First off, the time series plot of the heat flux is this (the red line is when the insulation is removed):</p>

<p><img src=""http://i.stack.imgur.com/SYSQj.jpg"" alt=""Time series plot of heat flux""></p>

<p>Than this are the ACF and PACF plots of the same data:</p>

<p><img src=""http://i.stack.imgur.com/7keT7.jpg"" alt=""ACF and PACF of the data""></p>

<p>For my data, an <code>stl()</code> decomposition, run as <code>stl(ts(heat.flux, frequency = 24), 'period')</code></p>

<p>shows a strong â€˜seasonalâ€™ (i.e daily) component and a trend in the series. Â </p>

<p><img src=""http://i.stack.imgur.com/CUsta.jpg"" alt=""STL of the data""></p>

<p>Firs off I am trying to determine the best parameters for a SARIMA or ARIMAX model so that I can get an estimation of the effect removing the insulation. Despite the fact I can produce the ACF and PACF plots there is no way I can figure out the proper orders, so I load the library <code>forecast</code> and I run:</p>

<pre><code>library(forecast)
auto.arima(ts(heat.flux, frequency = 24), xreg = insulation, max.p = 10, max.q = 10, max.P = 10, max.Q = 10, stationary = F)Â 
</code></pre>

<p>The reason why I do not specify a stationary model is because of the trend I see with <code>stl()</code> and because I assume an effect of removing the insulation.</p>

<p>from <code>auto.arima()</code> I get:</p>

<pre><code>Series: ts(heat.flux, frequency = 24) 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept  carp.hour$interv
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449            4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075            0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=840.55   AICc=841.03   BIC=876.11
</code></pre>

<p>If I try to use the <code>TSA</code> package and use <code>arimax()</code> with those orders I get basically the same stuff:</p>

<pre><code>library(TSA)
arimax(ts(heat.flux, frequency = 24), xreg = insulation, order = c(2,0,2), seasonal = list(order = c(1,0,1), frequency = 24))
Series: x 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=838.55   AICc=839.03   BIC=874.11
</code></pre>

<p>And all is apparently well (Irrespective of the function I choose I get an estimate of the effect of the removal of the insulation and a se with it with is what I want). Â Unfortunately, when I test the fit of this model with the function <code>sarima()</code> from the <code>astsa</code>package I get significant Ljung-Box p-values for all my sensors and for all the lags:</p>

<pre><code>library(astsa)
sarima(ts(heat.flux, frequency = 24), p = 2, d = 0, q = 2, P =1, D = 0, Q = 1, S = 24, xreg = insulation)
$fit

Call:
stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, 
Q), period = S), xreg = xreg, optim.control = list(trace = trc, REPORT = 1, 
reltol = tol))

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood = -411.28,  aic = 840.55
</code></pre>

<p>but the plot that comes with is shows that at every single lag the Ljung-Box statistics is significant:</p>

<p><img src=""http://i.stack.imgur.com/ZMGKN.jpg"" alt=""SARIMA""></p>

<p>What is going on?  To sum it up:</p>

<ol>
<li>which of these models is the most correct to estimate the effect of insulation?</li>
<li>why are the Ljung-Box p-values all significant?  I would have though that the ARIMA/ARIMAX/SARIMA would have sorted that issue</li>
<li>If the orders calculated by <code>auto.arima()</code> are the problem, how could I find them in a different way (which is computationally feasible and does not take days).</li>
</ol>

<p>Finally, two notes.  I also have collected variables such as internal and external temperatures, windspeed, etc, but I would have though that integrating these in the model would be superfluous given the fact it is already an ARIMA model to start with.  Second, I am not at all wedded to this kind of analysis, but I am aware that a straightforward linear model would not be acceptable given the autocorrelation between the data points.</p>
"
"NaN","NaN","124373","<p>I'm using the earth package (using caret train function) MARS spline implementation in order to perform non - linear regression modeling. I would like to obtain a measure of prediction uncertainty (not only the expected value). Is there any way to obtain it? Thanks in advance for any help.</p>
"
"0.0851715717988452","0.0865846528350581","124532","<p>I am estimating a (semi)parametric and a parametric model for a panel data set, and I want to test the functional form by applying the method proposed by <a href=""http://www.sciencedirect.com/science/article/pii/S030440760800016X"" rel=""nofollow"">Henderson et al. (2008, p.267)</a>. In particular, given the two models</p>

<pre><code>y = beta1 X + Z'gamma + u  (parametric)
y = beta2 X + g(z) + e    (semiparametric)
</code></pre>

<p>they use <code>H0</code> to denote the null hypothesis of the linear regression model, against <code>H1</code>: the corresponding alternative is the semiparametric model. The test statistic for testing <code>H0</code> is</p>

<pre><code>I= [beta1* X + Z'gamma* - beta2* X + g(z)]^2
</code></pre>

<p>Under <code>H0</code>. <code>I</code> converges to 0 in probability, whil it converges to a positive constant under <code>H1</code> . Therefore, the statistics <code>I</code> can be used to detect whether <code>H0</code> is true or not. However, given some problems with the asymptotical distribution of <code>I</code>, the authors propose to compute its empirical distribution by resampling <code>n</code> times the residuals <code>u</code>, using them to generate <code>n</code> new <code>y</code> and then re-estimating <code>n</code> times both models. By this way it is possible to obtain <code>n</code> times <code>I*</code> and its empiric distribution which should be approximating the null distribution of <code>I</code>. Therefore it can be used to detect whether <code>H0</code> holds.</p>

<p>My problem is in the interpretation of the results of the test. I reasoned as follows: I plotted the empirical distribution of <code>I*</code> which, in my case, has <code>mean &gt; 0</code>. Therefore what I did was to verify $where$  the <code>I</code> statistics lies. This is what I obtain:</p>

<p><img src=""http://i.stack.imgur.com/TihCM.png"" alt=""Empirical distribution and test""></p>

<p>where I.BB.IVO is my <code>I</code>. Can I say that <code>I</code> belongs to the empirical distribution (since it is not in the rejection zone) with mean <code>&gt; 0</code>, therefore the null hypothesis is rejected?</p>

<p>Is there any other alternative of doing the job?</p>
"
"0.0379321620905441","0.0385614943639849","124690","<p>Right now I am working with vector autoregressive models in order to make 3 months forecasts for a commodity good (sawlogs) y. I have several time-series of ""follow-up-products"" of sawlogs that should work as ""predictors"" for saw-log prices from a logical point of view. 
I encountered within the VAR-function from package ""vars"" (<a href=""http://cran.r-project.org/web/packages/vars/vars.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/vars/vars.pdf</a>), that one attribute called ""type"" has the following expressions: ""const"", ""both"", ""trend"", ""none"". I really don't know what this means from a statistical point of view.</p>

<p>Since neither the package-description nor other literature I've screened so far can give me an answer I actually understand I'd like to ask you guys the following:</p>

<p>How should I interpret/understand and use the argument ""type"" in R's VAR() Function?</p>

<p>What do those 4 different arguments really mean? ""both"", ""none"", ""trend"", ""constant""?
Could anyone explain this in a simple way and probably provide an example as well?</p>

<p>Does this mean that I can directly use non-stationary time series for my VAR-model since I can consider trend/season afterwards by setting the ""type-argument"" to both, or am I wrong here?</p>
"
"0.13150611096795","0.133687928065253","124707","<p>Sorry for the rather long introduction, but since I was (legitimately) critizised for not explaining my cause and questions enough, I will do so now. </p>

<p>I would like to conduct a <strong><em>(price)-forecast</em></strong> based on a multiple time series VAR-Model (vector autoregressive Model) with multiple endogeneous variables and two exogeneous. Since I am not that skillfull with regards to neither statistics nor R I want to keep is as simple as possible (Trend forecast for 3 months is sufficient).</p>

<p>I am using the ""vars"" - Package, <a href=""http://cran.r-project.org/web/packages/vars/vars.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/vars/vars.pdf</a> and all in all those four functions: decompose(), VARselect(), VAR(), and predict()</p>

<p>I have 1 dependent time series (y, in my model referred to as ""RH"", or ""raRH""), 4-5 endogeneous predictors and 2 exogeneous predictors.
All timeseries have a length of 1-91 observations and are monthly data without any gaps.</p>

<p><strong><em>Data description:</em></strong>
My y (dependent var) are sawlog prices, sawlogs are raw material for plenty of follow up products.<br> My endogeneous (since they all kind of correlate with each other and y) are follow up product-prices or further elaborated sawlogs. <br>My 2 exogeneous predictors are economic indicators similar to BIP etc.</p>

<p>All the time series are <em>non-stationary</em>, since I have read that you should use stationary data in order to gain a valid VAR-Model, I used the decompose() - function in R to split each variable into trend, season and the randwom walk. </p>

<pre><code> raKVH&lt;-decompose(KVH)$random
raKVH&lt;-na.omit(raKVH)
raSNS&lt;-decompose(SNP_S)$random
raSNS&lt;-na.omit(raSNS)
</code></pre>

<p>... and so on for every variable.<br><br>
What I'm interested now in order to do some forecasting are predictions of the randwom walk (right?!). Anyways, I found out that all my data is first-order-integrated, since taking the logarithm makes them all stationary timeseries (ts), tested via Dickey-Fuller-Test. </p>

<p>The picture also provides data example, first picture shows the raw-data, <img src=""http://i.stack.imgur.com/BcMOT.png"" alt=""enter image description here""></p>

<p>second picture the random walks gained by decomposing$random the raw-data. 
<img src=""http://i.stack.imgur.com/96yii.png"" alt=""enter image description here""></p>

<p>I used the command VARselect that automatically computes the optimal lag for my model, whereas tsall is my time-series matrix containing all the timeseries mentioned above.</p>

<pre><code>VARselect(tsall)
</code></pre>

<p>proceeding now with the estimation of the model VAR(p=number of lags given by VARselect), <strong><em>I encountered the following problem</em></strong>: how should I use the attribute ""type"" within the VAR-function? What does ""trend"",""none"", ""const"", ""both"" exactly mean? Since I have stationary data, there won't be any trend right? How can I check if there is a constant? Since the default value is ""const"", I chose to go with that.
<br><br>
<strong><em>The main question I have is the following:</em></strong><br>
How do I get ""real"" forecasts out of the prediction of the randwom walks anyways? If I want to predict the price of yt+3, I need more than the prediction of the random walks here, I need ""real figures"" like in graphic 1. How can I ""add back"" trend and season?</p>

<p>Third picture shows the Forecast of the random walk of my ""target Variable"" Y, but what's the next step here? 
<img src=""http://i.stack.imgur.com/ZtInk.png"" alt=""enter image description here""></p>

<p>Thank you for any help, if my questions/introduction are insufficient, please let me know. I'll try to explain myself better then.</p>
"
"0.13686025610802","0.139130903795187","125453","<p>I have used the â€˜polrâ€™ function in the MASS package to run an ordinal logistic regression for an ordinal categorical response variable with 15 continuous explanatory variables.</p>

<p>I have used the code (shown below) to check that my model meets the proportional odds assumption following advice provided in <a href=""http://www.ats.ucla.edu/stat/r/dae/ologit.htm"">UCLA's guide</a>. However, Iâ€™m a little worried about the output implying that not only are the coefficients across various cutpoints similar, but they are exactly the same (see graphic below). </p>

<pre><code>FGV1b &lt;- data.frame(FG1_val_cat=factor(FGV1b[,""FG1_val_cat""]), 
                    scale(FGV1[,c(""X"",""Y"",""Slope"",""Ele"",""Aspect"",""Prox_to_for_FG"", 
                          ""Prox_to_for_mL"", ""Prox_to_nat_border"", ""Prox_to_village"", 
                          ""Prox_to_roads"", ""Prox_to_rivers"", ""Prox_to_waterFG"", 
                          ""Prox_to_watermL"", ""Prox_to_core"", ""Prox_to_NR"", ""PCA1"", 
                          ""PCA2"", ""PCA3"")]))
b     &lt;- polr(FG1_val_cat ~ X + Y + Slope + Ele + Aspect + Prox_to_for_FG + 
                            Prox_to_for_mL + Prox_to_nat_border + Prox_to_village + 
                            Prox_to_roads + Prox_to_rivers + Prox_to_waterFG + 
                            Prox_to_watermL + Prox_to_core + Prox_to_NR, 
              data=FGV1b, Hess=TRUE)
</code></pre>

<p>View a summary of the model:</p>

<pre><code>summary(b)
(ctableb &lt;- coef(summary(b)))
q        &lt;- pnorm(abs(ctableb[, ""t value""]), lower.tail=FALSE) * 2
(ctableb &lt;- cbind(ctableb, ""p value""=q))
</code></pre>

<p>And now we can look at the confidence intervals for the parameter estimates:</p>

<pre><code>(cib &lt;- confint(b)) 
confint.default(b)
</code></pre>

<p>But these results are still quite hard to interpret, so let's convert the coefficients into odds ratios</p>

<pre><code>exp(cbind(OR=coef(b), cib))
</code></pre>

<p>Checking the assumption. So the following code will estimate the values to be graphed. First it shows us the logit transformations of the probabilities of being greater than or equal to each value of the target variable</p>

<pre><code>FG1_val_cat &lt;- as.numeric(FG1_val_cat)
sf &lt;- function(y) {
  c('VC&gt;=1' = qlogis(mean(FG1_val_cat &gt;= 1)),
    'VC&gt;=2' = qlogis(mean(FG1_val_cat &gt;= 2)),
    'VC&gt;=3' = qlogis(mean(FG1_val_cat &gt;= 3)),
    'VC&gt;=4' = qlogis(mean(FG1_val_cat &gt;= 4)),
    'VC&gt;=5' = qlogis(mean(FG1_val_cat &gt;= 5)),
    'VC&gt;=6' = qlogis(mean(FG1_val_cat &gt;= 6)),
    'VC&gt;=7' = qlogis(mean(FG1_val_cat &gt;= 7)),
    'VC&gt;=8' = qlogis(mean(FG1_val_cat &gt;= 8)))
}
(t &lt;- with(FGV1b, summary(as.numeric(FG1_val_cat) ~ X + Y + Slope + Ele + Aspect + 
                             Prox_to_for_FG + Prox_to_for_mL + Prox_to_nat_border + 
                             Prox_to_village + Prox_to_roads + Prox_to_rivers + 
                             Prox_to_waterFG + Prox_to_watermL + Prox_to_core + 
                             Prox_to_NR, fun=sf)))
</code></pre>

<p>The table above displays the (linear) predicted values we would get if we regressed our dependent variable on our predictor variables one at a time, without the parallel slopes assumption. So now, we can run a series of binary logistic regressions with varying cutpoints on the dependent variable to check the equality of coefficients across cutpoints</p>

<pre><code>par(mfrow=c(1,1))
plot(t, which=1:8, pch=1:8, xlab='logit', main=' ', xlim=range(s[,7:8]))
</code></pre>

<p><img src=""http://i.stack.imgur.com/4Uicq.jpg"" alt=""polr assumption check""></p>

<p>Apologies that I am no statistics expert and perhaps I am missing something obvious here. However, I have spent a long time trying to figure out if there is a problem in how I tested the model assumption and also trying to figure out other ways to run the same kind of model. </p>

<p>For example, I read in many help mailing lists that others use the vglm function (in the VGAM package) and the lrm function (in the rms package) (for example see here:  <a href=""http://stats.stackexchange.com/questions/25988/proportional-odds-assumption-in-ordinal-logistic-regression-in-r-with-the-packag"">Proportional odds assumption in ordinal logistic regression in R with the packages VGAM and rms</a>). I have tried to run the same models but am continuously coming up against warnings and errors.</p>

<p>For example, when I try to fit the vglm model with the â€˜parallel=FALSEâ€™ argument (as the previous link mentions is important for testing the proportional odds assumption), I encounter the following error:</p>

<blockquote>
  <p>Error in lm.fit(X.vlm, y = z.vlm, ...) : NA/NaN/Inf in 'y'<br>
  In addition: Warning message:<br>
  In Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals = residuals,  :
    fitted values close to 0 or 1</p>
</blockquote>

<p>I would like to ask please if there is anyone who might understand and be able to explain to me why the graph I produced above looks as it does. If indeed it means that something isnâ€™t right, could you please help me find a way to test the proportional odds assumption when just using the polr function. Or if that is just not possible, then I will resort to trying to use the vglm function, but would then need some help to explain why I keep getting the error given above.</p>

<p>NOTE: As a background, there are 1000 datapoints here, which are actually location points across a study area. I am looking to see if there are any relationships between the categorical response variable and these 15 explanatory variables. All of those 15 explanatory variables are spatial characteristics (for example, elevation, x-y coordinates, proximity to forest etc.). The 1000 datapoints were randomly allocated using a GIS, but I took a stratified sampling approach. I made sure that 125 points were randomly chosen within each of the 8 different categorical response levels. I hope this information is also helpful.</p>
"
"0.0599760143904067","0.0609710760849692","125465","<p>I'm searching for a built-in function in R for calculating the required sample size (given certain power, alpha,..) of a mixed ANOVA (2 between, 1 within variable, 2x2x2 design). Does this function exist? I didn't find it in the following packages:</p>

<p>â€¢pwr is the oldest power-analysis library; some introductory info can be found on Quick-R</p>

<p>â€¢PoweR: Computation of power and level tables for hypothesis tests</p>

<p>â€¢Power2Stage: Power and Sample size distribution of 2-stage BE studies via simulations</p>

<p>â€¢powerAnalysis: Power analysis in experimental design</p>

<p>â€¢powerGWASinteraction: Power Calculations for Interactions for GWAS</p>

<p>â€¢powerMediation: Power/Sample size calculation for mediation analysis, simple linear 
regression, logistic regression, or longitudinal study</p>

<p>â€¢powerpkg: Power analyses for the affected sib pair and the TDT design</p>

<p>â€¢powerSurvEpi: Power and sample size calculation for survival analysis of epidemiological studies</p>

<p>â€¢PowerTOST: Power and Sample size based on two one-sided t-tests (TOST) for (bio)equivalence studies</p>

<p>â€¢longpower: Power and sample size for linear model of longitudinal data</p>

<p>Thanks!</p>
"
"0.0464572209811883","0.0472279924554862","125603","<p>I am new to the world of <strong>Regression</strong> in statistics and I have been doing a research in which I am building an ordinal logistic regression model (ORM). In order to fit my ORM model, I am using the 'orm' function of 'rms' package from R (<a href=""http://cran.r-project.org/web/packages/rms/rms.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/rms/rms.pdf</a>).</p>

<p>Now I am trying to assess the goodness of fit of my model. By reading the R documentation, I can see the following statement in the 'stat' property of the 'orm' object (pg.98):</p>

<p>""(...)Nagelkerke R2 index, the g-index, gr (the g-index on the odds ratio scale),
and <strong>pdm (the mean absolute difference between 0.5 and the predicted probability
that $Y\geq q$  the marginal median)</strong>(...).""</p>

<p>I don't have enough background to understand the short description of the pdm measure. But when I try to do more research on this measure, I am not able to find related material (e.g. I've been finding ""prescription drug misuse""). In summary, my question is:</p>

<p>Would you know if the 'pdm' measure has some synonym which is more widely used? Or can you provide some references where I can study the pdm metric?</p>
"
"NaN","NaN","125767","<p>This is my first try at any regression and unfortunately I'm starting with an ordered logit model using the <code>polr</code> function in R. Does <code>polr</code> require all ordered factors with values like a,b,c to be converted into numbers like 1,2,3 instead of a,b,c before I use it?</p>
"
"0.0663812836584521","0.0771229887279699","126356","<p>For linear and parametric regression there are multiple tests where variables and residuals are used by means of performing a linear regression function to test serial correlation of regression errors and homocedasticity of regression errors. </p>

<p>My question is about non linear and non parametric regression for prediction or classification such us SVM, NeuralNets, knn, Recursive Partitioning, Adaptive Regression Spline, etc. </p>

<p>In this regard my questions are:</p>

<ol>
<li><p>As is not linear regression what is the equivalent of OLS assumptions for non linear non parametric regression. Are the consequences of OLS violation in the context of nonlinear and non-parametric regression still valid? </p></li>
<li><p>How could I test or what tests exist for serial correlation of errors for non linear and non parametric regression which are not derived from visual inspection of a graph and by using error residuals only. (something comes in mind like testing for significant acf or pacf on the residual errors - Unsure if this is OK).</p></li>
<li><p>How could I test or what tests exist for homocedasticity for non linear and non parametric regression which are not derived from visual inspection of a graph and by using error residuals only. (something comes too mind like homogenity of distances between the residual errors across time).</p></li>
<li><p>Would it be better to transfor the data into linear by seeking some adequate transformation as to avoid all the non linearity issues mentioned above? </p></li>
</ol>

<p>Thank you</p>
"
"0.0379321620905441","0.0385614943639849","126510","<p>How do we do two-way ANOVA (one observation per mean), as testing H_A in Section 8.5 in Seber and Lee's Linear Regression Analysis, in R?
Note that the linear model for this case doesn't have interaction between the row and column factors.</p>

<p>For example, I want to test in the following 3 x 2 table, if the mean of each row is the same. </p>

<p>5 | 4<br>
7 | 6<br>
4 | 7  </p>

<p>Note that I used <code>lm</code> for one-way ANOVA, but couldn't find out which function and arguments to do two-way ANOVA (one observation per mean). I am not trying to implement it in R.</p>

<p>Thanks.</p>
"
"NaN","NaN","126650","<p>I would like to specify a logistic regression model where I have the following relationship:</p>

<p>$E[Y_i|X_i] = f(\beta x_{i1} + \beta^2x_{i2})$ where $f$ is the inverse logit function. </p>

<p>Is there a ""quick"" way to do this with pre-existing R functions or is there a name for a model like this? I realize I can modify the Newton-Raphson algorithm used for logistic regression but this is a lot of theoretic and coding work and I'm looking for a short cut.</p>

<p>EDIT: getting point estimates for $\beta$ is pretty easy using optim() or some other optimizer in R to maximize the likelihood. But I need standard errors on these guys. </p>
"
"0.0889588054368324","0.0904347204435887","126768","<hr>

<h2>Original</h2>

<p>I have fitted an ordered logistic regression in R using the <code>polr</code> function, but I am having some trouble bringing the model coefficients into Excel and getting the probabilities there. </p>

<p>For explanatory variables <code>FlowMonth2, Orders_Apt, GeoUnits, HomeOwner, Platform, CreditScore</code>, my coefficients for the model are as follows: </p>

<pre><code>                                Value Std. Error  t value
FlowMonth2Aug                 0.12321    0.03852   3.1990
FlowMonth2Dec                 0.31092    0.03854   8.0672
FlowMonth2Feb                 0.02497    0.03873   0.6447
FlowMonth2Jan                -0.01874    0.03940  -0.4757
FlowMonth2Jul                 0.02924    0.03886   0.7525
FlowMonth2Jun                -0.02618    0.04054  -0.6456
FlowMonth2Mar                 0.09369    0.03739   2.5054
FlowMonth2May                -0.08169    0.03581  -2.2811
FlowMonth2Nov                 0.32610    0.03889   8.3841
FlowMonth2Oct                 0.45240    0.03708  12.2009
FlowMonth2Sep                 0.22771    0.04015   5.6711
Orders_Apty                   0.03786    0.02206   1.7160
GeoUnits1                    -0.04070    0.03260  -1.2487
GeoUnits2                     0.11923    0.03735   3.1920
GeoUnitsOther                 0.30464    0.20803   1.4644
GeoUnits5                    -0.19669    0.01892 -10.3942
HomeOwnery                    0.16577    0.02828   5.8624
PlatformMobile               -0.32933    0.01631 -20.1882
CreditScore525 - 600          1.01909    0.02937  34.7036
CreditScore600 - 700          1.12578    0.02953  38.1284
CreditScore700 - 800          1.29098    0.03091  41.7694
CreditScore800 - 900          1.43500    0.03085  46.5179
CreditScore900+               1.33816    0.02851  46.9414
CreditScoreHit with No Score  0.33832    0.03424   9.8812
CreditScoreNo Hit             0.37199    0.06443   5.7737
</code></pre>

<p>The intercepts are </p>

<pre><code>Intercepts:
                Value    Std. Error t value 
0|1              -1.2788   0.0377   -33.9349
1|2              -0.6609   0.0371   -17.8175
2|3              -0.1683   0.0369    -4.5571
3|4               0.1520   0.0369     4.1159
4|5               0.3813   0.0370    10.3163
5|6               0.5615   0.0370    15.1714
6|7               0.7314   0.0371    19.7357
7|8               0.8551   0.0371    23.0486
8|9               0.9608   0.0371    25.8740
9|10              1.0510   0.0372    28.2760
10|11             1.1342   0.0372    30.4826
11|12             1.2607   0.0373    33.8295
12|13             1.4770   0.0374    39.5140
13|14             1.5414   0.0374    41.1957
14|15             1.5827   0.0374    42.2710
15|16             1.6127   0.0375    43.0505
16|Still Active   1.6358   0.0375    43.6499
</code></pre>

<hr>

<p>Now when I bring this into Excel, I bring in the coefficients, select certain values to add together, say <code>FlowMonth2 = ""Aug"", Orders_Apt = ""n"", GeoUnits = ""5"", HomeOwner = ""y"", Platform = ""Desktop"", CreditScore = ""800 - 900""</code>. </p>

<p>I add these values together to get my logit statistic, $T = \mathbf{x}\mathbf{\beta}$, and then I add this $T$ to each different intercept to get $\beta_{0, i} - T$ for $1 \leq i \leq 17$ where the $17$th stage is transition from 16 to Still Active. </p>

<p>I then take $\mathrm{logit}(\beta_{0, i} - T)$ or ${1 \over 1 + \exp(-[\beta_{0, i} - T])} = \Pr(\text{being in the $i$th stage})$</p>

<p>But when I try to do this in Excel, and compare it to the output of <code>predict</code> in R, then I can't get these values to match up? What am I doing wrong in Excel? </p>

<hr>

<h2>Edit</h2>

<p>To compare the values from R and Excel, it's by more than a rounding error that they differ: </p>

<p>R: </p>

<pre><code>0                                                0.048650293
1                                                0.037989009
2                                                0.047738406
3                                                0.041799312
4                                                0.035787006
5                                                0.031644868
6                                                0.032650167
7                                                0.025400235
8                                                0.022740618
9                                                0.020074660
10                                               0.019006405
11                                               0.029758088
12                                               0.052613086
13                                               0.015949280
14                                               0.010274309
15                                               0.007485204
16                                               0.005777627
Still Active                                     0.514661427
</code></pre>

<p>Excel: </p>

<pre><code>0|1 0.048648622
1|2 0.086640673
2|3 0.134381676
3|4 0.176177948
4|5 0.211958542
5|6 0.243615257
6|7 0.27626595
7|8 0.301669593
8|9 0.32439208
9|10    0.344464821
10|11   0.363487303
11|12   0.393228837
12|13   0.44584823
13|14   0.461809529
14|15   0.472089045
15|16   0.479571379
16|Still Active 0.485339204
</code></pre>

<hr>

<h2>Edit 2</h2>

<p>Why are there only 17 intercepts in Excel, but 18 predicted points in R? </p>
"
"0.0930434003061344","0.124456687897154","127134","<p><strong>Updated</strong></p>

<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Yeardecimal - Date of procedure (expressed as decimal of year) = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 16-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
Mechanism - Mechanism of injury = Fall &lt;2m, Fall &gt;2m, Shooting/stabbing, RTC (Road Traffic Collision), Other
neuroFirst - Location of first admission (Neurosurgical Unit) = NSU vs. Non-NSU
rcteye - Pupil reactivity = NA / Both unreactive = O, 1 reactive = 1, both reactive = 2
rcteyeYN - dummy = 0 or 1 for presence or absence of data
GCS - Glasgow Coma Scale = 3-15
GCSYN - dummy = 0 or 1 for presence or absence of data
</code></pre>

<p>Dummy variables were included to enable a larger sample size where the majority of cases were excluding  <code>GCS</code> and <code>rcteye</code> variables (missing not at random).</p>

<p>In order to test for interactions, initially I ran the following:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + rcteye + rcteyeYN + GCS + GCSYN + rcs(Yeardecimal))^2, data = ASDH_Paper1.1)
</code></pre>

<p>but when I did I got the following error:</p>

<pre><code>singular information matrix in lrm.fit (rank= 151 ).  Offending variable(s):
GCSYN * Yeardecimal''' GCSYN * Yeardecimal' GCSYN * Yeardecimal GCS * Yeardecimal''' GCS * Yeardecimal GCS * GCSYN rcteyeYN * Yeardecimal''' rcteyeYN * Yeardecimal'' rcteyeYN * Yeardecimal rcteyeYN * GCSYN rcteye * Yeardecimal''' rcteye * Yeardecimal rcteye * rcteyeYN Mechanism=RTC * Yeardecimal''' Mechanism=Other * Yeardecimal''' Mechanism=Fall &gt; 2m * Yeardecimal''' Mechanism=Shooting / Stabbing * Yeardecimal Mechanism=RTC * Yeardecimal Mechanism=Other * Yeardecimal Mechanism=Fall &gt; 2m * Yeardecimal neuroFirst * Yeardecimal ISS'' * Yeardecimal''' ISS * Yeardecimal''' ISS'' * Yeardecimal'' ISS'' * Yeardecimal ISS' * Yeardecimal ISS * Yeardecimal ISS'' * GCSYN ISS'' * rcteyeYN ISS'' * Mechanism=RTC Age'' * Yeardecimal''' Age'' * Yeardecimal'' Age''' * Yeardecimal' Age''' * Yeardecimal Age'' * Yeardecimal Age' * Yeardecimal Age * Yeardecimal Age'' * GCSYN Age''' * rcteyeYN 
Error in lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + neuroFirst + Mechanism +  : 
  Unable to fit model using â€œlrm.fitâ€
</code></pre>

<p>The only way I could run the model is with an adjustment. <code>Yeardecimal</code> is excluded from any interaction as is the interaction of <code>GCS:GCSYN</code> and <code>rcteye:rcteyeYN</code> which produced the same error as written above. It made sense to exclude the interactions between a variable and its missing dummy but I am not sure what to do about <code>Yeardecimal</code>:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + rcteye + rcteyeYN) * (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + GCS + GCSYN) + rcs(Yeardecimal), data = ASDH_Paper1.1)
</code></pre>

<p>From this model the following interactions were identified with an <code>anova</code> output:</p>

<pre><code>&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor                                                Chi-Square d.f. P     
 Age  (Factor+Higher Order Factors)                    130.42      52  &lt;.0001
  All Interactions                                      78.68      48  0.0034
  Nonlinear (Factor+Higher Order Factors)               46.53      39  0.1901
 ISS  (Factor+Higher Order Factors)                    181.65      42  &lt;.0001
  All Interactions                                      52.43      39  0.0738
  Nonlinear (Factor+Higher Order Factors)               55.01      28  0.0017
 neuroFirst  (Factor+Higher Order Factors)              37.68      16  0.0017
  All Interactions                                      11.54      15  0.7136
 Mechanism  (Factor+Higher Order Factors)               63.72      52  0.1277
  All Interactions                                      58.35      48  0.1455
 rcteye  (Factor+Higher Order Factors)                 242.07      15  &lt;.0001
  All Interactions                                      19.39      14  0.1507
 rcteyeYN  (Factor+Higher Order Factors)               204.58      15  &lt;.0001
  All Interactions                                      29.88      14  0.0079
 GCS  (Factor+Higher Order Factors)                    162.81      15  &lt;.0001
  All Interactions                                      11.62      14  0.6365
 GCSYN  (Factor+Higher Order Factors)                   94.50      15  &lt;.0001
  All Interactions                                      41.74      14  0.0001
 Yeardecimal                                            51.96       4  &lt;.0001
  Nonlinear                                             10.27       3  0.0164
 Age * ISS  (Factor+Higher Order Factors)               11.90      12  0.4534
  Nonlinear                                              9.40      11  0.5851
  Nonlinear Interaction : f(A,B) vs. AB                  9.40      11  0.5851
  f(A,B) vs. Af(B) + Bg(A)                               7.96       6  0.2411
  Nonlinear Interaction in Age vs. Af(B)                 8.75       9  0.4605
  Nonlinear Interaction in ISS vs. Bg(A)                 8.58       8  0.3790
 Age * neuroFirst  (Factor+Higher Order Factors)         2.66       4  0.6166
  Nonlinear                                              2.05       3  0.5624
  Nonlinear Interaction : f(A,B) vs. AB                  2.05       3  0.5624
 Age * Mechanism  (Factor+Higher Order Factors)         17.58      16  0.3493
  Nonlinear                                             13.82      12  0.3127
  Nonlinear Interaction : f(A,B) vs. AB                 13.82      12  0.3127
 Age * GCS  (Factor+Higher Order Factors)                6.24       4  0.1819
  Nonlinear                                              3.89       3  0.2741
  Nonlinear Interaction : f(A,B) vs. AB                  3.89       3  0.2741
 Age * GCSYN  (Factor+Higher Order Factors)             20.11       4  0.0005
  Nonlinear                                              8.86       3  0.0312
  Nonlinear Interaction : f(A,B) vs. AB                  8.86       3  0.0312
 ISS * neuroFirst  (Factor+Higher Order Factors)         3.23       3  0.3571
  Nonlinear                                              0.87       2  0.6480
  Nonlinear Interaction : f(A,B) vs. AB                  0.87       2  0.6480
 ISS * Mechanism  (Factor+Higher Order Factors)         23.95      12  0.0206
  Nonlinear                                             20.66       8  0.0081
  Nonlinear Interaction : f(A,B) vs. AB                 20.66       8  0.0081
 ISS * GCS  (Factor+Higher Order Factors)                0.77       3  0.8570
  Nonlinear                                              0.42       2  0.8102
  Nonlinear Interaction : f(A,B) vs. AB                  0.42       2  0.8102
 ISS * GCSYN  (Factor+Higher Order Factors)              6.53       3  0.0886
  Nonlinear                                              2.35       2  0.3085
  Nonlinear Interaction : f(A,B) vs. AB                  2.35       2  0.3085
 neuroFirst * Mechanism  (Factor+Higher Order Factors)   2.45       4  0.6533
 neuroFirst * GCS  (Factor+Higher Order Factors)         0.00       1  0.9726
 neuroFirst * GCSYN  (Factor+Higher Order Factors)       1.39       1  0.2382
 Mechanism * GCS  (Factor+Higher Order Factors)          0.10       4  0.9987
 Mechanism * GCSYN  (Factor+Higher Order Factors)        1.74       4  0.7828
 Age * rcteye  (Factor+Higher Order Factors)             8.66       4  0.0702
  Nonlinear                                              7.29       3  0.0633
  Nonlinear Interaction : f(A,B) vs. AB                  7.29       3  0.0633
 ISS * rcteye  (Factor+Higher Order Factors)             4.18       3  0.2424
  Nonlinear                                              1.49       2  0.4744
  Nonlinear Interaction : f(A,B) vs. AB                  1.49       2  0.4744
 neuroFirst * rcteye  (Factor+Higher Order Factors)      0.10       1  0.7460
 Mechanism * rcteye  (Factor+Higher Order Factors)       3.44       4  0.4867
 rcteye * GCS  (Factor+Higher Order Factors)             2.30       1  0.1297
 rcteye * GCSYN  (Factor+Higher Order Factors)           2.57       1  0.1090
 Age * rcteyeYN  (Factor+Higher Order Factors)           7.23       4  0.1242
  Nonlinear                                              7.23       3  0.0649
  Nonlinear Interaction : f(A,B) vs. AB                  7.23       3  0.0649
 ISS * rcteyeYN  (Factor+Higher Order Factors)           2.47       3  0.4814
  Nonlinear                                              0.11       2  0.9462
  Nonlinear Interaction : f(A,B) vs. AB                  0.11       2  0.9462
 neuroFirst * rcteyeYN  (Factor+Higher Order Factors)    0.12       1  0.7280
 Mechanism * rcteyeYN  (Factor+Higher Order Factors)     1.81       4  0.7701
 rcteyeYN * GCS  (Factor+Higher Order Factors)           3.70       1  0.0543
 rcteyeYN * GCSYN  (Factor+Higher Order Factors)         8.74       1  0.0031
 TOTAL NONLINEAR                                       102.74      64  0.0015
 TOTAL INTERACTION                                     178.52     103  &lt;.0001
 TOTAL NONLINEAR + INTERACTION                         241.87     111  &lt;.0001
 TOTAL                                                 889.91     123  &lt;.0001
</code></pre>

<p>The <code>summary</code> function revealed the following results:</p>

<pre><code>             Effects              Response : Survive 

 Factor                                    Low    High   Diff. Effect       S.E.   Lower 0.95 Upper 0.95    
 Age                                         37.6   72.0 34.40         0.15   0.38   -0.58      8.900000e-01
  Odds Ratio                                 37.6   72.0 34.40         1.16     NA    0.56      2.430000e+00
 ISS                                         20.0   26.0  6.00        -1.34   0.31   -1.95     -7.400000e-01
  Odds Ratio                                 20.0   26.0  6.00         0.26     NA    0.14      4.800000e-01
 neuroFirst                                   0.0    1.0  1.00        -0.23   0.37   -0.95      5.000000e-01
  Odds Ratio                                  0.0    1.0  1.00         0.80     NA    0.39      1.650000e+00
 rcteye                                       0.0    2.0  2.00         3.20   0.50    2.22      4.170000e+00
  Odds Ratio                                  0.0    2.0  2.00        24.41     NA    9.24      6.452000e+01
 rcteyeYN                                     0.0    1.0  1.00        -3.34   0.44   -4.21     -2.480000e+00
  Odds Ratio                                  0.0    1.0  1.00         0.04     NA    0.01      8.000000e-02
 GCS                                          0.0   12.0 12.00         1.94   0.49    0.98      2.890000e+00
  Odds Ratio                                  0.0   12.0 12.00         6.94     NA    2.67      1.799000e+01
 GCSYN                                        0.0    1.0  1.00        -1.32   0.45   -2.20     -4.400000e-01
  Odds Ratio                                  0.0    1.0  1.00         0.27     NA    0.11      6.400000e-01
 Yeardecimal                               2005.5 2012.4  6.85         0.20   0.12   -0.03      4.400000e-01
  Odds Ratio                               2005.5 2012.4  6.85         1.22     NA    0.97      1.550000e+00
 Mechanism - Fall &gt; 2m:Fall &lt; 2m              1.0    2.0    NA        -0.89   0.35   -1.58     -2.000000e-01
  Odds Ratio                                  1.0    2.0    NA         0.41     NA    0.21      8.200000e-01
 Mechanism - Other:Fall &lt; 2m                  1.0    3.0    NA         0.25   0.42   -0.58      1.080000e+00
  Odds Ratio                                  1.0    3.0    NA         1.28     NA    0.56      2.930000e+00
 Mechanism - RTC:Fall &lt; 2m                    1.0    4.0    NA        -0.68   0.43   -1.52      1.700000e-01
  Odds Ratio                                  1.0    4.0    NA         0.51     NA    0.22      1.190000e+00
 Mechanism - Shooting / Stabbing:Fall &lt; 2m    1.0    5.0    NA        18.97 116.63 -209.63      2.475600e+02
  Odds Ratio                                  1.0    5.0    NA 172906690.96     NA    0.00     3.272814e+107

Adjusted to: Age=54.2 ISS=25 neuroFirst=0 Mechanism=Fall &lt; 2m rcteye=1 rcteyeYN=0 GCS=3 GCSYN=0 
</code></pre>

<p>Remaining questions are:</p>

<p><strong>1</strong> - Is my dummy variable treatment for variables missing not at random appropriate, including the exclusion of interactions with the main term?</p>

<p><strong>2</strong> - Can I resolve the issues with assessing interaction of the Yeardecimal term?</p>

<p><strong>3</strong> - Should I exclude non-significant interaction terms? I read that exclusion only of a ""chunk"" is advised - <a href=""http://stats.stackexchange.com/questions/11009/including-the-interaction-but-not-the-main-effects-in-a-model"">Including the interaction but not the main effects in a model</a></p>

<p><strong>4</strong> - Is the odds ratio for each variable the ""Effect"" column? If so, is this the OR between the lowest and highest value of each variable?</p>
"
"NaN","NaN","127226","<p>I'm trying to simulate a logistic regression. My goal is showing that if <code>Y=1</code> is rare, than the intercept is biased. In my R script I define the logistic regression model through the latent variable's approach (see for example pp. 140 <a href=""http://gking.harvard.edu/files/abs/0s-abs.shtml"" rel=""nofollow"">http://gking.harvard.edu/files/abs/0s-abs.shtml</a>):</p>

<pre><code>x   &lt;- rnorm(10000)

b0h &lt;- numeric(1000)
b1h &lt;- numeric(1000)

for(i in 1:1000){
  eps &lt;- rlogis(10000)
  eta &lt;- 1+2*x+eps
  y   &lt;-numeric(10000)
  y   &lt;- ifelse (eta&gt;0,1,0)

  m      &lt;- glm(y~x,family=binomial)
  b0h[i] &lt;- coef(m)[1]
  b1h[i] &lt;- coef(m)[2]
}

mean(b0h)
mean(b1h)
hist(b0h)
hist(b1h)
</code></pre>

<p>The problem here is that I don't know how to force the observations y to be balanced before (50:50), then unbalanced (90:10). As we can see with the function table(), in my script the proportion of ones is random.</p>

<pre><code>table(y)
</code></pre>

<p>How to solve this problem?</p>
"
"0.0709645772411954","0.072141950116023","127536","<p>Sampling weights, the inverse probability of a unit's selection into the sample, and other more complex and adjusted weights are very often used in the social sciences. There is statistical software that allows weighting of observations/cases, like the <code>hclust</code> function from the <code>R</code>-package <code>cluster</code>. </p>

<p>In regression analysis, there is an ongoing debate when the usage of observation weights is appropriate (see e.g. Winship/Radbill 1994). I could not find anything concerning observation weights in textbooks about cluster analysis, if weighting is discussed, it is mostly about variable weighting. One exemption is the manual of the <code>R</code>-package <code>WeightedCluster</code>, which discusses observation weighting in more detail. The documentation of the <code>cluster</code> package is not very helpful, as it only shows a trivial example using the weighting option <code>hclust(..., members=""..."")</code> where the number or weight of cases is untouched.</p>

<ol>
<li>Therefore, I am looking for references and recommendations with observation/case weighting in cluster analysis, especially hierarchical cluster analysis. </li>
<li>As I could not find the actual formula for the <code>hclust(..., members=""..."")</code> function : Which parameters changes in the hierarchical cluster algorithm if one uses observation weights? How does that affect the algorithm?</li>
</ol>

<p>In order to get an idea of the difference between clustering with and without case weights, here is an example using weights from survey data and the R-code:
<img src=""http://i.stack.imgur.com/BYiLY.png"" alt=""Reweighting of clustering by using membership""></p>

<pre><code>require(survey)
data(api)
whc &lt;- hclust(dist(apiclus2[, c(""pct.resp"", ""meals"")]), method=""ward.D2"", 
              members=apiclus2$pw)
uwhc &lt;- hclust(dist(apiclus2[, c(""pct.resp"", ""meals"")]), method=""ward.D2"")
opar &lt;- par(mfrow = c(1, 2))
plot(whc,  labels = FALSE, hang = -1, main = ""Weighted survey data"")
plot(uwhc, labels = FALSE, hang = -1, main = ""Unweighted survey data"")
</code></pre>

<h3>References</h3>

<ul>
<li>Studer, M., 2013: WeightedCluster Library Manual. A practical guide to creating typologies of trajectories in the social sciences with R. LIVES Working Papers 24. Lausanne.</li>
<li>Winship, C. &amp; L. Radbill, 1994: Sampling Weights and Regression Analysis. Sociological Methods &amp; Research 23: 230â€“257.</li>
</ul>
"
"NaN","NaN","128667","<p>I want to fit a logistic function of the form $$f(t) = \frac{C}{1+ab^{-t}}$$to some data that I have, using <code>R</code>.</p>

<p>There is some uncertainty to $f(t)$, and its magnitude is assumed to be constant. There is no uncertainty in $t$.</p>

<p>Answers to the question <a href=""http://stats.stackexchange.com/questions/8436/logistic-regression-for-bounds-different-from-0-and-1"">Logistic regression for bounds different from 0 and 1</a> mention that it's possible, but don't specify how. I am interested in estimating $C$, $a$ and $b$ and also $R^2$.</p>
"
"0.0599760143904067","0.0609710760849692","128815","<p>I'm trying to estimate the value of a property depending on the property characteristics. I did some research and I found out, that it would be better to use the <strong>Hedonic Model/Regression</strong> instead of <strong>Linear Square Regression</strong>.</p>

<p>After reading a couple of papers about it, I still have some questions.</p>

<p>I work with <strong>R</strong>, so I have the data (information about other properties) saved as a data.frame, with the following columns (c stands for characteristic).</p>

<pre><code>----------------------------------
| price | c1 | c2 | c3 | c4 | c5 |
----------------------------------
</code></pre>

<p>My questions:</p>

<ol>
<li>I know how to estimate the coefficients with the <strong>Least Square Regression</strong>, but how do I do it with the <strong>Hedonic Regression</strong>? I know, that in <strong>R</strong> is no function for it.</li>
<li>The environment characteristics (air pollution, criminality rate, etc.) are almost the same, because the properties are in the same district. The <strong>Last Square Regression</strong> gives them a very small coefficient, but they have a big importance in real life. How can I tell the regression, that they have a big importance?</li>
<li>As I understood so far, if an attribute of an observation is missing, I should not use the observation, is that right?</li>
<li>In the calculation of the coefficients, should I use only the date from nearby (example: same district) real estates or it would be better to use all real estates from the town?</li>
</ol>

<p>Could somebody please give me a hint?</p>

<p>Thank you very much!       </p>
"
"0.053644178078582","0.0545341883149212","129260","<p>I'm currently estimating a survival model (accelerated failure time model) with a log-logistic distribution in R using the survival package and the survreg function. I want to simulate expected survival times in line with King et al (2001), but I am unsure of the link function needed to calculate the expected survival time for the log-logistic distribution from the survreg regression output. I have added a minimal working example below.</p>

<pre><code>library(survival)
data(kidney)

survreg(formula = Surv(time, status) ~ 
                  age + 
                  cluster(id), 
                  data = kidney, 
                  dist = ""loglogistic"", 
                  robust = TRUE)

               Value Std. Err (Naive SE)     z        p
(Intercept)  4.38127   0.6783     0.5338  6.46 1.05e-10
age         -0.00298   0.0135     0.0114 -0.22 8.26e-01
Log(scale)  -0.23009   0.0732     0.1038 -3.14 1.67e-03

Scale= 0.794 

Log logistic distribution
Loglik(model)= -342   Loglik(intercept only)= -342
    Chisq= 0.07 on 1 degrees of freedom, p= 0.79 
(Loglikelihood assumes independent observations)
Number of Newton-Raphson Iterations: 3 
n= 76 
</code></pre>

<p>I simply want to know how I can calculate expected survival time from the estimated parameters from the survreg output.</p>

<p>King, G., Tomz, M., &amp; Wittenberg, J. (2000). Making the most of statistical analyses: Improving interpretation and presentation. American journal of political science, 347-361.</p>
"
"0.113954427341773","0.109747936952945","129337","<h3>The out-of-context short version</h3>

<p>Let $y$ be a random variable with CDF
$$
F(\cdot) \equiv \cases{\theta &amp; y = 0 \\ \theta + (1-\theta) \times \text{CDF}_{\text{log-normal}}(\cdot; \mu, \sigma) &amp; y &gt; 0}
$$</p>

<p>Let's say I wanted to simulate draws of $y$ using the inverse CDF method. Is that possible? This function doesn't exactly have an inverse. Then again there's <a href=""http://stats.stackexchange.com/q/73028/36"">Inverse transformation sampling for mixture distribution of two normal distributions</a> which suggests that there is a known way to apply inverse transformation sampling here.</p>

<p>I'm aware of the two-step method, but I don't know how to apply it to my situation (see below).</p>

<hr>

<h3>The long version with background</h3>

<p>I fitted the following model for a vector-valued response, $y^i = \left( y_1 , \dots , y_K \right)^i$, using MCMC (specifically, Stan):</p>

<p>$$
\theta_k^i \equiv \operatorname{logit}^{-1}\left( \alpha_k x^i \right), \quad \mu_k^i \equiv \beta_k x^i - \frac{ \sigma^2_k }{ 2 } \\
F(\cdot) \equiv \cases{\theta &amp; y = 0 \\ \theta + (1-\theta) \times \text{CDF}_{\text{log-normal}}(\cdot; \mu, \sigma) &amp; y &gt; 0} \\
u_k \equiv F(y_k), \quad z_k \equiv\Phi^{-1}{\left( u_k \right)} \\
z \sim \mathcal{N}(\mathbf{0}, R) \times \prod_k f(y_k) \\
\left( \alpha, \beta, \sigma, R \right) \sim \text{priors}
$$</p>

<p>where $i$ indexes $N$ observations, $R$ is a correlation matrix, and $x$ is a vector of predictors/regressors/features.</p>

<p>That is, my model is a regression model in which the conditional distribution of the response is assumed to be a Gaussian copula with zero-inflated log-normal marginals. I've posted about this model before; it turns out that Song, Li, and Yuan (2009, <a href=""http://onlinelibrary.wiley.com/doi/10.1111/j.1541-0420.2008.01058.x/abstract"" rel=""nofollow"">gated</a>) have developed it and they call it a vector GLM, or VGLM. The following is their specification as close to verbatim as I could get it:
$$
f(\mathbf{y}; \mathbf{\mu}, \mathbf{\varphi}, \Gamma) = c\{ G_1(y_1), \dots, G_m(y_m) | \Gamma \} \prod_{i=1}^m g(y_i; \mu_i, \varphi_i) \\
c(\mathbf{u} | \Gamma) = \left| \Gamma \right|^{-1/2}\exp\left( \frac{1}{2} \mathbf{q}^T \left( I_m - \Gamma^{-1} \right) \mathbf{q} \right) \\
\mathbf{q} = \left( q_1, \dots, q_m \right)^T, \quad q_i = \Phi^{-1}(u_i)
$$
My $F_K$ corresponds to their $G_m$, my $z$ corresponds to their $\mathbf{q}$, and my $R$ corresponds to their $\Gamma$; the details are on page 62 (page 3 of the PDF file) but they're otherwise identical to what I wrote here.</p>

<p>The zero-inflated part roughly follows the specification of Liu and Chan (2010, <a href=""http://www.jstatsoft.org/v35/i11/paper"" rel=""nofollow"">ungated</a>).</p>

<p>Now I would like to simulate data from the estimated parameters, but I'm a little confused as to how to go about it. First I thought I could just simulate $y$ directly (in R code):</p>

<pre><code>for (i in 1:N) {
    for (k in 1:K) {
        Y_hat &lt;- rbinom(1, 1, 1 - theta[i, k])
        if (Y_hat == 1)
            Y_hat &lt;- rlnorm(1, mu[i, k], sigma[k])
    }
}
</code></pre>

<p>which doesn't use $R$ at all. I'd like to try to actually use the correlation matrix I estimated.</p>

<p>My next idea was to take draws of $z$ and then convert them back to $y$. This also seems to coincide with the answers in <a href=""http://stats.stackexchange.com/q/78894/36229"">Generating samples from Copula in R</a> and <a href=""http://stats.stackexchange.com/q/123698/36229"">Bivariate sampling for distribution expressed in Sklar&#39;s copula theorem?</a>. But what the heck is my $F^{-1}$ here? <a href=""http://stats.stackexchange.com/q/73028/36229"">Inverse transformation sampling for mixture distribution of two normal distributions</a> makes it sound like this is possible, but I have no idea how to do it.</p>
"
"NaN","NaN","130813","<p>used the glm function in R to model a logistic regression of a binary repsonse and 3 categorical predictors. My problem is that according to the summary non of the levels is significant. </p>
"
"0.0599760143904067","0.0609710760849692","131093","<p>I hope this is not a duplicate but I cannot find the answer to this question. In a linear model
$$Y_i = \beta_1 X_{i,1} + \dots + \beta_{p-1} X_{i,p-1} + \varepsilon_i, \qquad i = 1, \ldots, n$$
with the usual assumptions, is the regression sum of squares, $SSR$, still</p>

<p>$$ SSR = \sum_{i=1}^n (\hat{y_i} - \overline{y})^2 \text{ ?}$$
where $\hat{y_i} = X \hat{\beta}$ is the $i$-th fitted value, $\hat{\beta} = (X^TX)^{-1}X^T y$ and $X$ is the design matrix without the column of ones that it would have if we couldn't assume $\beta_0 = 0$.</p>

<p>Now, I'm asking this question because using the ""anova"" function in R, you can obtain the $SSR$ by simply adding the corresponding $SSR$'s of each variable (I believe this is called a type I decomposition), but this doesn't match the $SSR$ as calculated above for a model with $\beta_0 = 0$.</p>

<p>Am I missing something or did I just screw up calculating it? </p>

<p>I had a sample of 2 variables, $X_1$ and $X_2$ with $n=11$ observations, as follows:
$x_1 = (1,4,9,11,3,8,5,10,2,7,6)^T$, $x_2 = (8,2,-8,-10,6,-6,0,-12,4,-2,-4)^T$ and $y=(6,8,1,0,5,3,2,-4,10,-3,5)^T$.</p>

<p>I introduced them in R as y, x1 and x2. Then using anova(lm(y~0+x1+x2)) I got Sum Sq of 14.279 for x1 and 161.846 for x2. Their sum is 176.154.</p>

<p>However, using the design matrix with $x_1$ and $x_2$ as its columns, I got $\beta = (\beta_1, \beta_2)^T = (0.7211, 0.8089)^T$ (which matches the ones obtained in R) so $SSR = 96.37352$, which is obviously different from the one obtained in R.</p>
"
"0.160932534235746","0.163602564944764","131312","<p>I have some R code (which I did not write) and which performs some state space analysis on some time-series. The data itself is shown as dots (scatter plot) and the Kalman filtered and smoothed state is the solid line.</p>

<p><img src=""http://i.stack.imgur.com/41jaI.png"" alt=""Plot""></p>

<p>My question is regarding the confidence intervals shown in this plot. I calculate <em>my own</em> confidence intervals using the standard method (my C# code is below)</p>

<pre><code>public static double ConfidenceInterval(
    IEnumerable&lt;double&gt; samples, double interval)
{
    Contract.Requires(interval &gt; 0 &amp;&amp; interval &lt; 1.0);

    double theta = (interval + 1.0) / 2;
    int sampleSize = samples.Count();
    double alpha = 1.0 - interval;
    double mean = samples.Mean();
    double sd = samples.StandardDeviation();

    var student = new StudentT(0, 1, samples.Count() - 1);
    double T = student.InverseCumulativeDistribution(theta);
    return T * (sd / Math.Sqrt(samples.Count()));
}
</code></pre>

<p>Now this will return a single interval (and it does it correctly) which I will add/subtract from each point on the series I have applied the calculation to to give me my confidence interval. But this is a constant and the R implementation seems to change over the time-series.</p>

<p>My question is why is <strong>the confidence interval changing for the R implementation? Should I be implementing my confidence levels/intervals differently?</strong></p>

<p>Thanks for your time.</p>

<hr>

<p>For reference the R code that produces this plot is below:</p>

<pre><code>install.packages('KFAS')
require(KFAS)

# Example of local level model for Nile series
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='BFGS',control=list(REPORT=1,trace=1))$model

# Can use different optimisation: 
# should be one of â€œNelder-Meadâ€, â€œBFGSâ€, â€œCGâ€, â€œL-BFGS-Bâ€, â€œSANNâ€, â€œBrentâ€
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='L-BFGS-B',control=list(REPORT=1,trace=1))$model

# Filtering and state smoothing
out&lt;-KFS(modelNile,filtering='state',smoothing='state')
out$model$H
out$model$Q
out

# Confidence and prediction intervals for the expected value and the observations.
# Note that predict uses original model object, not the output from KFS.
conf&lt;-predict(modelNile,interval='confidence')
pred&lt;-predict(modelNile,interval='prediction')
ts.plot(cbind(Nile,pred,conf[,-1]),col=c(1:2,3,3,4,4),
ylab='Predicted Annual flow', main='River Nile')
KFAS 13

# Missing observations, using same parameter estimates
y&lt;-Nile
y[c(21:40,61:80)]&lt;-NA
modelNile&lt;-SSModel(y~SSMtrend(1,Q=list(modelNile$Q)),H=modelNile$H)
out&lt;-KFS(modelNile,filtering='mean',smoothing='mean')

# Filtered and smoothed states
plot.ts(cbind(y,fitted(out,filtered=TRUE),fitted(out)), plot.type='single',
col=1:3, ylab='Predicted Annual flow', main='River Nile')

# Example of multivariate local level model with only one state
# Two series of average global temperature deviations for years 1880-1987
# See Shumway and Stoffer (2006), p. 327 for details
data(GlobalTemp)
model&lt;-SSModel(GlobalTemp~SSMtrend(1,Q=NA,type='common'),H=matrix(NA,2,2))

# Estimating the variance parameters
inits&lt;-chol(cov(GlobalTemp))[c(1,4,3)]
inits[1:2]&lt;-log(inits[1:2])
fit&lt;-fitSSM(inits=c(0.5*log(.1),inits),model=model,method='BFGS')
out&lt;-KFS(fit$model)
    ts.plot(cbind(model$y,coef(out)),col=1:3)
legend('bottomright',legend=c(colnames(GlobalTemp), 'Smoothed signal'), col=1:3, lty=1)

# Seatbelts data
## Not run:
model&lt;-SSModel(log(drivers)~SSMtrend(1,Q=list(NA))+
SSMseasonal(period=12,sea.type='trigonometric',Q=NA)+
log(PetrolPrice)+law,data=Seatbelts,H=NA)

# As trigonometric seasonal contains several disturbances which are all
# identically distributed, default behaviour of fitSSM is not enough,
# as we have constrained Q. We can either provide our own
# model updating function with fitSSM, or just use optim directly:
# option 1:
ownupdatefn&lt;-function(pars,model,...){
model$H[]&lt;-exp(pars[1])
    diag(model$Q[,,1])&lt;-exp(c(pars[2],rep(pars[3],11)))
model #for option 2, replace this with -logLik(model) and call optim directly
}
14 KFAS
fit&lt;-fitSSM(inits=log(c(var(log(Seatbelts[,'drivers'])),0.001,0.0001)),
model=model,updatefn=ownupdatefn,method='BFGS')
out&lt;-KFS(fit$model,smoothing=c('state','mean'))
    out
    ts.plot(cbind(out$model$y,fitted(out)),lty=1:2,col=1:2,
    main='Observations and smoothed signal with and without seasonal component')
    lines(signal(out,states=c(""regression"",""trend""))$signal,col=4,lty=1)
legend('bottomleft',
legend=c('Observations', 'Smoothed signal','Smoothed level'),
col=c(1,2,4), lty=c(1,2,1))

# Multivariate model with constant seasonal pattern,
# using the the seat belt law dummy only for the front seat passangers,
# and restricting the rank of the level component by using custom component
# note the small inconvinience in regression component,
# you must remove the intercept from the additional regression parts manually
model&lt;-SSModel(log(cbind(front,rear))~ -1 + log(PetrolPrice) + log(kms)
+ SSMregression(~-1+law,data=Seatbelts,index=1)
+ SSMcustom(Z=diag(2),T=diag(2),R=matrix(1,2,1),
Q=matrix(1),P1inf=diag(2))
+ SSMseasonal(period=12,sea.type='trigonometric'),
data=Seatbelts,H=matrix(NA,2,2))
likfn&lt;-function(pars,model,estimate=TRUE){
model$H[,,1]&lt;-exp(0.5*pars[1:2])
    model$H[1,2,1]&lt;-model$H[2,1,1]&lt;-tanh(pars[3])*prod(sqrt(exp(0.5*pars[1:2])))
    model$R[28:29]&lt;-exp(pars[4:5])
if(estimate) return(-logLik(model))
model
}
fit&lt;-optim(f=likfn,p=c(-7,-7,1,-1,-3),method='BFGS',model=model)
model&lt;-likfn(fit$p,model,estimate=FALSE)
    model$R[28:29,,1]%*%t(model$R[28:29,,1])
    model$H
out&lt;-KFS(model)
out
ts.plot(cbind(signal(out,states=c('custom','regression'))$signal,model$y),col=1:4)

# For confidence or prediction intervals, use predict on the original model
pred &lt;- predict(model,states=c('custom','regression'),interval='prediction')
ts.plot(pred$front,pred$rear,model$y,col=c(1,2,2,3,4,4,5,6),lty=c(1,2,2,1,2,2,1,1))

## End(Not run)
## Not run:
# Poisson model
model&lt;-SSModel(VanKilled~law+SSMtrend(1,Q=list(matrix(NA)))+
SSMseasonal(period=12,sea.type='dummy',Q=NA),
KFAS 15
data=Seatbelts, distribution='poisson')

# Estimate variance parameters
fit&lt;-fitSSM(inits=c(-4,-7,2), model=model,method='BFGS')
model&lt;-fit$model

# use approximating model, gives posterior mode of the signal and the linear predictor
out_nosim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=0)

# State smoothing via importance sampling
out_sim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=1000)
out_nosim
out_sim

## End(Not run)
# Example of generalized linear modelling with KFS
# Same example as in ?glm
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
print(d.AD &lt;- data.frame(treatment, outcome, counts))
glm.D93 &lt;- glm(counts ~ outcome + treatment, family = poisson())
model&lt;-SSModel(counts ~ outcome + treatment, data=d.AD,
distribution = 'poisson')
out&lt;-KFS(model)
coef(out,start=1,end=1)
coef(glm.D93)
summary(glm.D93)$cov.s
    out$V[,,1]
outnosim&lt;-KFS(model,smoothing=c('state','signal','mean'))
set.seed(1)
outsim&lt;-KFS(model,smoothing=c('state','signal','mean'),nsim=1000)

## linear
# GLM
glm.D93$linear.predictor

# approximate model, this is the posterior mode of p(theta|y)
c(outnosim$thetahat)

# importance sampling on theta, gives E(theta|y)
c(outsim$thetahat)

## predictions on response scale
16 KFAS

# GLM
fitted(glm.D93)

# approximate model with backtransform, equals GLM
c(fitted(outnosim))

# importance sampling on exp(theta)
fitted(outsim)

# prediction variances on link scale
# GLM
as.numeric(predict(glm.D93,type='link',se.fit=TRUE)$se.fit^2)

# approx, equals to GLM results
c(outnosim$V_theta)
    # importance sampling on theta
    c(outsim$V_theta)
# prediction variances on response scale
# GLM
as.numeric(predict(glm.D93,type='response',se.fit=TRUE)$se.fit^2)
    # approx, equals to GLM results
    c(outnosim$V_mu)
# importance sampling on theta
c(outsim$V_mu)
    ## Not run:
    data(sexratio)
    model&lt;-SSModel(Male~SSMtrend(1,Q=list(NA)),u=sexratio[,'Total'],data=sexratio,
    distribution='binomial')
    fit&lt;-fitSSM(model,inits=-15,method='BFGS',control=list(trace=1,REPORT=1))
    fit$model$Q #1.107652e-06

# Computing confidence intervals in response scale
# Uses importance sampling on response scale (4000 samples with antithetics)
pred&lt;-predict(fit$model,type='response',interval='conf',nsim=1000)
    ts.plot(cbind(model$y/model$u,pred),col=c(1,2,3,3),lty=c(1,1,2,2))

# Now with sex ratio instead of the probabilities:
imp&lt;-importanceSSM(fit$model,nsim=1000,antithetics=TRUE)
    sexratio.smooth&lt;-numeric(length(model$y))
sexratio.ci&lt;-matrix(0,length(model$y),2)
    w&lt;-imp$w/sum(imp$w)
    for(i in 1:length(model$y)){
sexr&lt;-exp(imp$sample[i,1,])
sexratio.smooth[i]&lt;-sum(sexr*w)
oo&lt;-order(sexr)
sexratio.ci[i,]&lt;-c(sexr[oo][which.min(abs(cumsum(w[oo]) - 0.05))],
+ sexr[oo][which.min(abs(cumsum(w[oo]) - 0.95))])
}

# Same by direct transformation:
out&lt;-KFS(fit$model,smoothing='signal',nsim=1000)
    KFS 17
    sexratio.smooth2 &lt;- exp(out$thetahat)
sexratio.ci2&lt;-exp(c(out$thetahat)
    + qnorm(0.025) * sqrt(drop(out$V_theta))%o%c(1, -1))
ts.plot(cbind(sexratio.smooth,sexratio.ci,sexratio.smooth2,sexratio.ci2),
col=c(1,1,1,2,2,2),lty=c(1,2,2,1,2,2))

## End(Not run)
# Example of Cubic spline smoothing
## Not run:
require(MASS)
data(mcycle)
model&lt;-SSModel(accel~-1+SSMcustom(Z=matrix(c(1,0),1,2),
T=array(diag(2),c(2,2,nrow(mcycle))),
Q=array(0,c(2,2,nrow(mcycle))),
P1inf=diag(2),P1=diag(0,2)),data=mcycle)
model$T[1,2,]&lt;-c(diff(mcycle$times),1)
model$Q[1,1,]&lt;-c(diff(mcycle$times),1)^3/3
model$Q[1,2,]&lt;-model$Q[2,1,]&lt;-c(diff(mcycle$times),1)^2/2
    model$Q[2,2,]&lt;-c(diff(mcycle$times),1)
    updatefn&lt;-function(pars,model,...){
    model$H[]&lt;-exp(pars[1])
    model$Q[]&lt;-model$Q[]*exp(pars[2])
    model
    }
    fit&lt;-fitSSM(model,inits=c(4,4),updatefn=updatefn,method=""BFGS"")
    pred&lt;-predict(fit$model,interval=""conf"",level=0.95)
plot(x=mcycle$times,y=mcycle$accel,pch=19)
lines(x=mcycle$times,y=pred[,1])
    lines(x=mcycle$times,y=pred[,2],lty=2)
lines(x=mcycle$times,y=pred[,3],lty=2)
## End(Not run)
</code></pre>

<p>The time-series data is:</p>

<pre><code>Time, 2.4, 2.6, 3.2, 3.6, 4, 6.2, 6.6, 6.8, 7.8, 8.2, 8.8, 8.8, 9.6, 10, 10.2, 10.6, 11, 11.4, 13.2, 13.6, 13.8, 14.6, 14.6, 14.6, 14.6, 14.6, 14.6, 14.8, 15.4, 15.4, 15.4, 15.4, 15.6, 15.6, 15.8, 15.8, 16, 16, 16.2, 16.2, 16.2, 16.4, 16.4, 16.6, 16.8, 16.8, 16.8, 17.6, 17.6, 17.6, 17.6, 17.8, 17.8, 18.6, 18.6, 19.2, 19.4, 19.4, 19.6, 20.2, 20.4, 21.2, 21.4, 21.8, 22, 23.2, 23.4, 24, 24.2, 24.2, 24.6, 25, 25, 25.4, 25.4, 25.6, 26, 26.2, 26.2, 26.4, 27, 27.2, 27.2, 27.2, 27.6, 28.2, 28.4, 28.4, 28.6, 29.4, 30.2, 31, 31.2, 32, 32, 32.8, 33.4, 33.8, 34.4, 34.8, 35.2, 35.2, 35.4, 35.6, 35.6, 36.2, 36.2, 38, 38, 39.2, 39.4, 40, 40.4, 41.6, 41.6, 42.4, 42.8, 42.8, 43, 44, 44.4, 45, 46.6, 47.8, 47.8, 48.8, 50.6, 52, 53.2, 55, 55, 55.4, 57.6                                                                                                
mcycle, 0, -1.3, -2.7, 0, -2.7, -2.7, -2.7, -1.3, -2.7, -2.7, -1.3, -2.7, -2.7, -2.7, -5.4, -2.7, -5.4, 0, -2.7, -2.7, 0, -13.3, -5.4, -5.4, -9.3, -16, -22.8, -2.7, -22.8, -32.1, -53.5, -54.9, -40.2, -21.5, -21.5, -50.8, -42.9, -26.8, -21.5, -50.8, -61.7, -5.4, -80.4, -59, -71, -91.1, -77.7, -37.5, -85.6, -123.1, -101.9, -99.1, -104.4, -112.5, -50.8, -123.1, -85.6, -72.3, -127.2, -123.1, -117.9, -134, -101.9, -108.4, -123.1, -123.1, -128.5, -112.5, -95.1, -81.8, -53.5, -64.4, -57.6, -72.3, -44.3, -26.8, -5.4, -107.1, -21.5, -65.6, -16, -45.6, -24.2, 9.5, 4, 12, -21.5, 37.5, 46.9, -17.4, 36.2, 75, 8.1, 54.9, 48.2, 46.9, 16, 45.6, 1.3, 75, -16, -54.9, 69.6, 34.8, 32.1, -37.5, 22.8, 46.9, 10.7, 5.4, -1.3, -21.5, -13.3, 30.8, -10.7, 29.4, 0, -10.7, 14.7, -1.3, 0, 10.7, 10.7, -26.8, -14.7, -13.3, 0, 10.7, -14.7, -2.7, 10.7, -2.7, 10.7
</code></pre>
"
"0.0758643241810882","0.0771229887279699","131331","<p>This is the first time I am posting a question, so please excuse any etiquette violations and poorly worded questions!</p>

<p>I am working on the analysis for a chapter of my thesis. I am examining the behavioural response of an animal to a visual stimulus, and trying to determine which of eight explanatory variables (and their two-way interactions) affect this response. I recorded the response on an ordinal scale of 0 (no response), 1 (attention to but no avoidance of stimulus) or 2 (escape response to stimulus). I am leaning towards collapsing categories and using logistic regression where a 1 is an escape response, and 0 is anything else because logistic regression seems much easier to interpret. </p>

<p>I have 794 observations. I am including observer and location (because field sites differed) as random effects, although I am unsure this is a good approach. </p>

<p>I am having trouble with model selection. I ran all possible subsets using the dredge function in packing 'MuMIn'. I thought I was avoiding data dredging by </p>

<ul>
<li>including main effects which were selected because I thought they would have an effect (rather than all conceivable variables)</li>
<li>including only the two-way interactions of interest (R will not run if the global model includes all possible two-way interactions because of the huge number of terms/models)</li>
</ul>

<p>I've come to realise that the second point may be problematic because it leads to an unbalanced model set as in Burnham and Anderson (2002). </p>

<blockquote>
  <p>Page 169: When assessing the relative importance of variables using sums of the AIC    weights, it is important to achieve a balance in the number of models that contain each variable j.</p>
</blockquote>

<p>My questions are</p>

<ol>
<li><p>Is it possible to have a balanced model set without it being considered data dredging? If so, how? </p></li>
<li><p>Is my approach at all reasonable? If not, are there other avenues I should explore? I started with Hosmer&amp;Lemeshow purposeful forward selection, as advocated by my supervisor, but I had some issues with this which I can elaborate on if necessary. </p></li>
</ol>
"
"0.0663812836584521","0.0771229887279699","131401","<p>I am running a pooled OLS regression using the plm package in R. Though, my question is more about basic statistics, so I try posting it here first ;)</p>

<p>Since my regression results yield heteroskedastic residuals I would like to try using heteroskedasticity robust standard errors. As a result from <code>coeftest(mod, vcov.=vcovHC(mod, type=""HC0""))</code> I get a table containing estimates, standard errors, t-values and p-values for each independent variable, which basically are my ""robust"" regression results.</p>

<p>For discussing the importance of different variables I would like to plot the share of variance explained by each independent variable, so I need the respective sum of squares. However, using function <code>aov()</code>, I don't know how to tell R to use robust standard errors.</p>

<p>Now my question is: How do I get the ANOVA table/sum of squares that refers to robust standard errors? Is it possible to calculate it based on the ANOVA table from regression with normal standard errors?</p>

<p>Edit:</p>

<p>In other words and disregarding my R-issues:</p>

<p>If R$^2$ is not affected by using robust standard errors, will also the respective contributions to explained variance by the different explanatory variables be unchanged?</p>

<p>Edit:</p>

<p>In R, does <code>aov(mod)</code> actually give a correct ANOVA table for a panelmodel (plm)?</p>
"
"0.107288356157164","0.109068376629842","131456","<p>I'm exploring the effects of removing the intercept in a logistic regression model.</p>

<p>Assume a model:</p>

<p>$$logit(Y = 1) = \beta_1 x + \beta_2z + 0$$</p>

<p>with $x$ and $z$ being categorical variables with 2 levels each and no intercept.</p>

<p>I understood that having no intercept with categorical predictors produce coefficients that compare the $P(Y = 1)$ in each level of the two predictor against a null case where $P(Y=1) = 0.5$ or $logit(Y=1) = 0$.</p>

<p>I noticed a phenomenon that can understand. Using glm() function in R if you change the order of the variable in the right hand part of the formula, the coefficients change too. But even more oddly, the coefficient of the first variable is always the same.</p>

<p>Here's an <code>R</code> demo:</p>

<pre><code>y &lt;- as.factor(sample(rep(1:2), 30, T))
x &lt;- as.factor(sample(rep(1:2), 30, T))
z &lt;- as.factor(sample(rep(1:2), 30, T))

coef(glm(y ~ x + z - 1, binomial)
#        x1         x2         z2 
#-0.1764783  0.3260739 -0.1335192

coef(glm(y ~ z + x - 1, binomial))
#        z1         z2         x2 
#-0.1764783 -0.3099976  0.5025523 
</code></pre>

<p>As you can see the first predictors have the same coefficient while the other are different in the two models.</p>

<p>Here is what I expected and instead behave differently than what I though:</p>

<ol>
<li>Since every level of the two predictors is compared to the same null case, I expected to have the same coefficients in the two models, independently from the order in which I use them.</li>
<li>I expected to see the coefficients of every level of every predictor, instead the coefficient for the 1 level of the second predictor is not shown.</li>
<li>I therefore assume that only the first variable is compared against the null case, while the second is compared against a reference level; but what is this level? Is it $P(Y = 1 | X = 1 \cap Z = 1)$? Reproducing one of the models WITH the intercept we get:</li>
</ol>

<p>`(for some reason stackexchange don't understand the following is code without the tick)   </p>

<pre><code>coef(glm(y ~ x + z - 1, binomial)
#        x1         x2         z2 
#-0.1764783  0.3260739 -0.1335192

coef(glm(y ~ x + z, binomial))
#(Intercept)         x2          z2 
#-0.1764783   0.5025523  -0.1335192
</code></pre>

<p>As expected x1 become the intercept, and x2 is likely relative to x1. z1 is missing also in this case and z2 is the same as in the model without intercept.</p>

<p>Thus should I assume that the comparison against the null case $P(Y = 1) = 0.5$ is made only for the first variable in a formula, while the other are compared against the usual intercept?
Is this behavior normal?
What about the fact that the first coefficient has the same value whichever the order of the predictors in the formula?
What if I want to compare all level of each predictor against the null case and have a coefficient for all levels?
Or it's theoretically impossible for some reason I don't get?</p>
"
"0.0599760143904067","0.0609710760849692","132761","<p>I am wanting to calculate hazard ratio in a matched cohort design.
For such I am using the <code>coxph()</code> function in R.
But I have been recommended to stratify by matched pair.
My matching has been simulated using the <code>match()</code> and <code>MatchBalance()</code> commands.</p>

<p>But I am confused as to what 'stratify by matched pair' means.
Can someone explain in simple terms please?
ie. in the simulation does this mean that each matched pair is given a unique identity</p>

<p>OR </p>

<p>are the all the case with exposure given an identity, (say 1), and all case without exposure given a different identity (say 2).....</p>

<p>I have been unable to find any literature in relation to R, or even a tutorial to follow.
The <code>â€˜RcmdrPlugin.EZR</code> can be used for the simulation, but it does explain the <em>Stratified Cox proportional hazard regression for matched-pair analysis</em></p>
"
"0.195267755666105","0.198507441827672","132925","<p>I have data from the following experimental design: my observations are counts of the numbers of successes (<code>K</code>) out of corresponding number of trials (<code>N</code>), measured for two groups each comprised of <code>I</code> individuals, from <code>T</code> treatments, where in each such factor combination there are <code>R</code> replicates. Hence, altogether I have 2 * I * T * R <em>K</em>'s and corresponding <em>N</em>'s. </p>

<p>The data are from biology. Each individual is a gene for which I measure the expression level of two alternative forms (due to a phenomenon called alternative splicing). Hence, <em>K</em> is the expression level of one of the forms and <em>N</em> is the sum of expression levels of the two forms. The choice between the two forms in a single expressed copy is assumed to be a Bernoulli experiment, hence <em>K</em> out of <em>N</em> copies follows a binomial. Each group is comprised of ~20 different genes and the genes in each group have some common function, which is different between the two groups. For each gene in each group I have ~30 such measurements from each of three different tissues (treatments). I want to estimate the effect that group and treatment have on the variance of K/N.</p>

<p>Gene expression is known to be overdispersed hence the use of negative binomial in the code below.</p>

<p>E.g., <code>R</code> code of simulated data:</p>

<pre><code>library(MASS)
set.seed(1)
I = 20 # individuals in each group
G = 2  # groups
T = 3  # treatments
R = 30 # replicates of each individual, in each group, in each treatment

groups     = letters[1:G]
ids        = c(sapply(groups, function(g){ paste(rep(g, I), 1:I, sep=""."") }))
treatments = paste(rep(""t"", T), 1:T, sep=""."")
 # create random mean number of trials for each individual and 
 #  dispersion values to simulate trials from a negative binomial:
mean.trials = rlnorm(length(ids), meanlog=10, sdlog=1)
thetas      = 10^6/mean.trials
 # create the underlying success probability for each individual:
p.vec = runif(length(ids), min=0, max=1)
 # create a dispersion factor for each success probability, where the 
 #  individuals of group 2 have higher dispersion thus creating a group effect:
dispersion.vec = c(runif(length(ids)/2, min=0, max=0.1),
                   runif(length(ids)/2, min=0, max=0.2))
 # create empty an data.frame:
data.df = data.frame(id=rep(sapply(ids, function(i){ rep(i, R) }), T),
                     group=rep(sapply(groups, function(g){ rep(g, I*R) }), T),
                     treatment=c(sapply(treatments, 
                                        function(t){ rep(t, length(ids)*R) })),
                     N=rep(NA, length(ids)*T*R), 
                     K=rep(NA, length(ids)*T*R) )
 # fill N's and K's - trials and successes
for(i in 1:length(ids)){
  N     = rnegbin(T*R, mu=mean.trials[i], theta=thetas[i])
  probs = runif(T*R, min=max((1-dispersion.vec[i])*p.vec[i],0),
                max=min((1+dispersion.vec)*p.vec[i],1))
  K     = rbinom(T*R, N, probs)
  data.df$N[which(as.character(data.df$id) == ids[i])] = N
  data.df$K[which(as.character(data.df$id) == ids[i])] = K
}
</code></pre>

<p>I'm interested in estimating the effects that group and treatment have on the dispersion (or variance) of the success probabilities (i.e., <code>K/N</code>). Therefore I'm looking for an appropriate glm in which the response is K/N but in addition to modelling the expected value of the response the variance of the response is also modeled.</p>

<p>Clearly, the variance of a binomial success probability is affected by the number of trials and the underlying success probability (the higher the number of trials is and the more extreme the underlying success probability is (i.e., near 0 or 1), the lower the variance of the success probability), so I'm mainly interested in the contribution of  group and treatment beyond that of the number of trials and the underlying success probability. I guess applying the arcsin square root transformation to the response will eliminate the latter but not that of the number of trials.</p>

<p>Although in the simulated example data above the design is balanced (equal number of individuals in each of the two groups and identical number of replicates in each individual from each group in each treatment), in my real data it is not - the two groups do not have an equal number of individuals and the number of replicates varies. Also, I'd imagine the individual should be set as a random effect.</p>

<p>Plotting the sample variance vs. the sample mean of the estimated success probability (denoted as p hat = K/N) of each individual illustrates that extreme success probabilities have lower variance:
<img src=""http://i.stack.imgur.com/2AWFd.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/ArGnD.png"" alt=""enter image description here""></p>

<p>This is eliminated when the estimated success probabilities are transformed using the arcsin square root variance stabilizing transformation (denoted as arcsin(sqrt(p hat)):
<img src=""http://i.stack.imgur.com/Ktj4r.png"" alt=""enter image description here""></p>

<p>Plotting the sample variance of the transformed estimated success probabilities vs. the mean N shows the expected negative relationship:
<img src=""http://i.stack.imgur.com/i1CwL.png"" alt=""enter image description here""></p>

<p>Plotting the sample variance of the transformed estimated success probabilities for the two groups shows that group b has slightly higher variances, which is how I simulated the data:
<img src=""http://i.stack.imgur.com/lr5uo.png"" alt=""enter image description here""></p>

<p>Finally, plotting the sample variance of the transformed estimated success probabilities for the three treatments shows no difference between treatments, which is how I simulated the data:
<img src=""http://i.stack.imgur.com/xQlHD.png"" alt=""enter image description here""></p>

<p>Is there any form of a generalized linear model with which I can quantify the group and treatment effects on the variance of the success probabilities? </p>

<p>Perhaps a heteroscedastic generalized linear model or some form of a loglinear variance model?</p>

<p>Something in the lines of a model which models the Variance(y) = ZÎ» in addition to E(y) = XÎ², where Z and X are the regressors of the mean and variance, respectively, which in my case will be identical and include treatment (levels t.1, t.2, and t.3) and group (levels a and b), and probably N and R, and hence Î» and Î² will estimate their respective effects.</p>

<p>Alternatively, I could fit a model to the sample variances across replicates of each gene from each group in each treatment, using a glm which only models the expected value of the response. The only question here is how to account for the fact that different genes have different numbers of replicates. I think the weights in a glm could account for that (sample variances that are based on more replicates should have a higher weight) but exactly which weights should be set?</p>

<p>Note:
I have tried using the <code>dglm</code> R package:</p>

<pre><code>library(dglm)
dglm.fit = dglm(formula = K/N ~ 1, dformula = ~ group + treatment, family = quasibinomial, weights = N, data = data.df)
summary(dglm.fit)
Call: dglm(formula = K/N ~ 1, dformula = ~group + treatment, family = quasibinomial, 
    data = data.df, weights = N)

Mean Coefficients:
               Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) -0.09735366 0.01648905 -5.904138 3.873478e-09
(Dispersion Parameters for quasibinomial family estimated as below )

    Scaled Null Deviance: 3600 on 3599 degrees of freedom
Scaled Residual Deviance: 3600 on 3599 degrees of freedom

Dispersion Coefficients:
                Estimate Std. Error      z value  Pr(&gt;|z|)
(Intercept)  9.140517930 0.04409586 207.28746254 0.0000000
group       -0.071009599 0.04714045  -1.50634107 0.1319796
treatment   -0.001469108 0.02886751  -0.05089138 0.9594121
(Dispersion parameter for Gamma family taken to be 2 )

    Scaled Null Deviance: 3561.3 on 3599 degrees of freedom
Scaled Residual Deviance: 3559.028 on 3597 degrees of freedom

Minus Twice the Log-Likelihood: 29.44568 
Number of Alternating Iterations: 5 
</code></pre>

<p>The group effect according to dglm.fit is pretty weak. I wonder if the model is set right or is the power this model has.</p>
"
"0.026822089039291","0.0272670941574606","133281","<p>I've been using the glm.fit function in R to fit parameters to a logistic regression model.  By default, glm.fit uses iteratively reweighted least squares to fit the parameters.  What are some reasons this algorithm would fail to converge, when used for logistic regression?</p>
"
"0.026822089039291","0.0272670941574606","133320","<p>I am using logistic regression to solve the classification problem.</p>

<pre><code>g = glm(target ~ ., data=trainData, family = binomial(""logit""))
</code></pre>

<p>There are two classes (target): 0 and 1 </p>

<p>When I run the prediction function, it returns probabilities.</p>

<pre><code>p = predict(g, testData, type = ""response"")
</code></pre>

<p>However, it is not clear to me how to understand which class has been assigned?</p>

<pre><code>Real  p 

1   0.17568578
1   0.41698474
1   0.19151927
1   0.25587242
1   0.25604452
0   0.39976069
0   0.39910282
0   0.16879320
</code></pre>

<p>I appreciate if someone can explain me how this works based on the above example. Thanks</p>
"
"0.0599760143904067","0.0609710760849692","133387","<p>I'm trying to create a prediction model for estimation of continuous variable based on about 35 Independent variables.My data set has circa 27k observartions.
Here is the summary of the the targeted continuous variable:</p>

<pre><code>              Frequency Percent
(0,5]              2706  10.053
(5,10]             5226  19.415
(10,25]            4397  16.335
(25,100]           7142  26.533
(100,1e+03]        6465  24.018
(1e+03,1e+05]       981   3.645
Total             26917 100.000
</code></pre>

<p>I tried (by using R) Random Forest (RandomForest package),Linear regression, Conditional Inference Trees (ctree function in party package) but all of them have results that have a significant overestimation.
Here are the results of the prediction where I counted number of observations by thier distance from the actual values:
Any idea how can i balance the results?</p>

<p><img src=""http://i.stack.imgur.com/y70OM.png"" alt=""enter image description here""></p>

<p>Here are some views on the data:
The target variable is LTV for a user, I would like to predict LTV value after 180 days  based on users behavior of the first 7 days.
Here Is a summary fot the target variavle:</p>

<pre><code>  vars     n   mean     sd median trimmed   mad  min      max    range skew kurtosis   se
1    1 26917 178.35 622.29  33.49   66.63 39.28 0.03 22103.73 22103.71 14.1   325.08 3.79
</code></pre>

<p>UPDATE:
Here are the distributions of the targeted variable (first)and the prediction (secound)results:
<img src=""http://i.stack.imgur.com/b3MBs.png"" alt=""targeted variable"">
<img src=""http://i.stack.imgur.com/3N7d1.png"" alt=""prediction results based on the linear regression model that was the best""></p>
"
"0.026822089039291","0.0272670941574606","133979","<p>Consider that we have a problem with 4 variables (y, x1, x2 and x3) and we want to do a multiple linear regression model. As we need to know which variables are the most important in the problem, we look for it with a step selection as (it's just an example, we could also used back, both...) :</p>

<pre><code>g0 = lm(Y~1,data=dat)
gxf = formula(gx)
forward=step(g0,scope=gxf,direction=""forward"",test=""F"")
</code></pre>

<p>Suppose that this function says to us that our model should be y ~ ax1 + bx3. If we now do a summary to the object ""forward"" and we get this:</p>

<pre><code>Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.071923   0.150266   0.479    0.636    
X1           0.009716   0.001890   5.140 2.09e-05 ***
X3          -0.013497   0.009230  -1.462    0.155    
</code></pre>

<p>Do we should change our model to y ~ x1? Why isn't significative x3? And in case we change to only y ~ x1, if we do a lm(y~x3) and in a summary of this model now x3 is also significative, what model is better? The one that have a better r^2?</p>
"
"0.0889588054368324","0.0822133822214443","134141","<p>Recently I am reading a paper where the authors use the GAM to make predictions. In brief, the data looks like following:</p>

<pre><code>  y    i    j     x    weekend
5.6    1    1   4.6    Mon.
6.5    1    2   5.6    Mon.
...
4.6    2    1   6.7    Sta.
2.4    2    2   1.2    Sta.
...
</code></pre>

<p>where <code>y</code>, <code>x1</code>, <code>x2</code> are continuous numbers, <code>weekend</code> is the day of the week. In the paper, the authors use the following formula:  </p>

<p>$$y_{ij} = \beta_0 + b_{0i} + \beta_1{\rm weekend}_i + f_1(x_{ij}, {\rm weekend}_i) + \varepsilon_{ij}$$</p>

<p>In the formula, $\beta_0$ is the overall mean, $b_{0i}$ is the random intercept, ${\rm weekend}_i$ determines whether it is weekday or weekend. Ans so I transform ${\rm weekend}$ from {Mon., Thu., .., Sun.} into {0, 1}. And $f_1$ is cubic regression function with 17 spline knots, and in fact will generate two smooth functions one for weekday, another for weekend.</p>

<p>I want to use following code:  </p>

<pre><code>gam(y~ s(i,bs=""re"") + weekend + s(x, by=weekend, bs=""cr"", k=17))
</code></pre>

<p>But I'm not sure whether it fits the formula or not. My questions are:</p>

<ol>
<li><code>gam</code> will automatically generate the mean of the model, so there is no need to specify a $\beta_0$ in the code?  </li>
<li>Is it right that by using <code>s(i,bs=""re"")</code>, the <code>gam</code> will calculate different random effect with distribution $N(0, \delta_i)$ for every $i$ specifically?</li>
<li>Is it good to transform weekend into 0-1 value? and in the code <code>s(x, by=weekend, bs=""cr"", k=17)</code>, does the <code>by</code> keyword mean that it will generate different smooth functions of <code>x</code> for different <code>weekend</code> value?</li>
<li>The last question is that without specifying <code>knots=list()</code>, as in the above code, the default behaviour of the model is to put knot points evenly of the range of value?</li>
</ol>
"
"0.0709645772411954","0.072141950116023","134694","<p>One criterion for selecting the optimal value of $\lambda$ with an elastic net or similar penalized regression is to examine a plot of the deviance against the range of $\lambda$ and select $\lambda$ when deviance is minimized (or $\lambda$ within one standard error of the minimum).</p>

<p>However, I'm having difficulty understanding what, precisely, <code>glmnet</code> displays with <code>plot.cv.glmnet</code>, because the plot displayed does not at all resemble the results of plotting the deviance against $\lambda$.</p>

<pre><code>set.seed(4567)
N       &lt;- 500
P       &lt;- 100
coefs   &lt;- NULL
for(p in 1:P){
    coefs[p]    &lt;- (-1)^p*100*2^(-p)
}
inv.logit &lt;- function(x) exp(x)/(1+exp(x))
X   &lt;- matrix(rnorm(N*P), ncol=P, nrow=N)
Y   &lt;- rbinom(N, size=1, p=inv.logit(cbind(1, X)%*%c(-4, coefs)))
plot(test   &lt;- cv.glmnet(x=X, y=Y, family=""binomial"", nfolds=10, alpha=0.8))
plot(log(test$lambda), deviance(test$glmnet.fit))
</code></pre>

<p><img src=""http://i.stack.imgur.com/9I5z7.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/ZE2gc.png"" alt=""enter image description here""></p>

<p>It appears that the second plot does not incorporate the elastic net penalty, and is also incorrectly scaled vertically. I base the claim on the basis that the shape of the curve for larger values of $\lambda$ resembles that of the <code>glmnet</code> output. However, when I've attempted to compute the penalty on my own, my attempt likewise appears to be wildly inaccurate.</p>

<pre><code>penalized.dev.fn    &lt;- function(lambda, alpha=0.2, data, cv.model.obj){
    dev &lt;- deviance(cv.model.obj$glmnet.fit)[seq_along(cv.model.obj$lambda)[cv.model.obj$lambda==lambda]]
    beta &lt;- coef(cv.model.obj, s=lambda)[rownames(coef(cv.model.obj))!=""(Intercept)""]
    penalty &lt;- lambda * ( (1-alpha)/2*(beta%*%beta) + alpha*sum(abs(beta)) )
    penalized.dev &lt;- penalty+dev
    return(penalized.dev)
}

out &lt;- sapply(test$lambda, alpha=0.2, cv.model.obj=test, FUN=penalized.dev.fn)
        plot(log(test$lambda), out)
</code></pre>

<p>My question is: how does one manually compute the deviance reported in the default <code>plot.cv.glmnet</code> diagram? What is its formula, and what have I done wrong in my attempt to compute it?</p>
"
"NaN","NaN","134885","<p>Since a feedforward NN with a logistic function as activation function is not linear, does it make sense to reduce variables first with principal components or discriminant analysis?</p>

<p>Because shouldn't be done this before training the NN as with logistic regression?</p>
"
"0.149412044409433","0.147144346905275","135043","<p>I have three questions concerning accelerated failure time models (AFT), one statistical, one regarding how to implement these models in R, and one related to finding out information about what R is doing. In short my questions are;</p>

<p>1) What is the relationship between the Gumbel and Weibull distributions?</p>

<p>2) How can I use (1) to simulate a AFT model using Gumbel errors and fit this model in R?</p>

<p>3) Where can I find formulae regarding exactly what distribution specification R is using when fitting a Weibull distribution, and exactly what model is being fitted?</p>

<p>I am having difficulties implementing 2), which may be due to my mis-understanding of 1), but which I can't seem to resolve due to 3). Question (3) is self-explanatory but (2) and (3) require more detail;</p>

<p>1) It seems a standard result that if $U\sim Gumbel(\alpha,\beta)$ then $V:=\exp(U)\sim Weibull(\lambda,\sigma)$ where $\alpha=\log(\sigma)$ and $\beta=1/\lambda$. However using the definition of the Gumbel and Weibull distributions commonly used (for example Wikipedia), when I do the derivation I can only get the transformation $V':=1/\exp(U)$ to give this result but where $\alpha=-\log(\sigma)=\log(1/\sigma)$. Thus can anyone confirm or not any knowledge of this relationship, or perhaps suggest where I have gone wrong (for brevity in the first instance I do not supply the detail)?</p>

<p>2) My approach is to use</p>

<p>$Y_{i}:=\log\left(\frac{1}{T_{i}}\right)=\beta_{0} + \beta_{1}x_{i} + e_{i},\hspace{20pt}i=1,...,N$,</p>

<p>as a data-generating mechanism for the logarithm of the time to event where $e_{i}\sim Gumbel(\alpha,\beta)$, where $i$ indexes subjects, $x_{i}$ is a scalar covariate, and the $e_{i}$ are all independent. I choose $\alpha=-\beta*c$ where $c$ is Euler's constant in order to ensure $E[e_{i}]=\alpha+c\beta=0$. This gives</p>

<p>$Y_{i}\sim Gumbel(\beta_{0} + \beta_{1}x_{i}+\alpha,\beta)$,</p>

<p>and using (1)</p>

<p>$T_{i}\sim Weibull(1/\beta,\exp[-(\beta_{0} + \beta_{1}x_{i}+\alpha)])$</p>

<p>The code at the end of this post is a minimal working example of this approach, where I censor subjects if $T_{i}$ is greater than the median of the $N$ theoretical medians of $\{T_{1},...,T_{N}\}$, and create an event if not. This gives $50-60\%$ of subjects being censored, the balance having events, and I interpret this to be right-censoring (say the end of a study).</p>

<p>I then use the survreg package in R to try to fit an AFT to $Y_{i}$ using the ""dist=weibull"" option. Using $\beta_{0}=-10$ and $\beta_{1}=0$ gives the following output</p>

<p><img src=""http://i.stack.imgur.com/4sQlb.png"" alt=""enter image description here""></p>

<p>which gives the intercept being positive when it should be negative. Things get worse when using $\beta_{0}=-10$ and $\beta_{1}=2$ which gives the following output</p>

<p><img src=""http://i.stack.imgur.com/i7o3f.png"" alt=""enter image description here""></p>

<p>which is obviously wrong. Thus I would like to know what model I am actually fitting when using the survreg package.</p>

<p>The code below is a minimal working example (apart from some code to produce plots which can be helpful).</p>

<pre><code># minimal working example
set.seed(123)
require(survival)
#params of the gumbel(alpha_gum,beta_gum) distribution so that E[X]=0
beta_gum = 1/5 #
alpha_gum = -(beta_gum*(-digamma(1)))

#calc the mean of the errors using Eulers constant as the negative of the diagamma function
mu_e = alpha_gum + (beta_gum*(-digamma(1)))#should be 0   

# regression parameters
intercept = -10;
beta1 =0;
#beta1 =2;

#number of subjects
N=1000;

# vector of uniform random numbers
U = runif(N)

#vector for gumbel distributed errors
e = matrix(,nrow=N,ncol=1)


# log of time to event, time to event, mean LTTE
logTTE = matrix(,nrow=N,ncol=1)
Xbeta_LTTE= matrix(,nrow=N,ncol=1)
TTE = matrix(,nrow=N,ncol=1)
TTE2 = matrix(,nrow=N,ncol=1)

#censoring variable
censor = matrix(,nrow=N,ncol=1)

#simulate covariate from a normal distribution
covariate1 = rnorm(N,6,4)

for (i in 1:N)
{
  # calculate the Gumbel RV from the inverse CDF of the Gumbel
  e[i,1] = alpha_gum + (-beta_gum*log(-log(U[i])))

  #generate the mean log TTE  
  Xbeta_LTTE[i,1] = intercept + (beta1*covariate1[i])

  #add the errors
  logTTE[i,1] = Xbeta_LTTE[i,1] + e[i,1]  

  #transform to raw time variable - this is a Weibull dist
  #TTE_i ~ Weibull[1/beta_gum , exp(-[logTTE_i+alpha_gum])
  TTE[i,1] = 1/exp(logTTE[i,1])      
}

#calc the median the TTE given TTE ~ Weibull[1/beta_gum , exp(-[X_i^t*beta+alpha_gum])
lambda_array = exp(-(Xbeta_LTTE + alpha_gum + (beta_gum*(-digamma(1)))))
kappa = 1/beta_gum
median_TTE_array = (lambda_array)*(log(2)^(1/kappa))
median_TTE = median(median_TTE_array)

# calculate the censoring variable
for (i in 1:N)
{
  #censoring: subjects with a TTE &gt;median_TTE will be right-censored
  #i.e. study ends at T=median_TTE say
  if (TTE[i,1]&gt;median_TTE)
  {
    censor[i,1]=1 
    TTE2[i,1]=median_TTE
  }
  else
  {
    censor[i,1]=0    
    TTE2[i,1]=TTE[i,1]
  }  
}

#calculate the percentage of censored subjects and do a plot
pc_censored = sum(censor)/N

#fit AFT model
datframe_surv = data.frame(covariate1)
attach(datframe_surv)

m.surv = Surv(TTE2,censor,type=""right"")
m.surv.fit = survreg(m.surv~covariate1,dist=""weibull"",scale=1)
sum = summary(m.surv.fit)
print(sum)



###################  plots ########################


#histogram of the errors - gumbel dist
h1 = hist(e, breaks=50, plot=FALSE) 

#histogram of the mean log TTE - gumbel dist
h2 = hist(logTTE, breaks=50, plot=FALSE) 

#histogram of the fixed means
h3 = hist(Xbeta_LTTE, breaks=50, plot=FALSE) 

#histogram of the TTE - weibul dist
h4 = hist(TTE, breaks=50, plot=FALSE) 

#calc the mean of the log TTE given logTTE ~ Gumbel(X_i^t*beta+alpha_gum,beta_gum)
median_logTTE_array = Xbeta_LTTE + alpha_gum - (beta_gum*(log(log(2))))
median_logTTE = median(median_logTTE_array)



#calc the means
ylim_h1 = c(min(h1$density),max(h1$density) )
xlim_h1 = c(mu_e,mu_e )

ylim_h2 = c(min(h3$density),max(h3$density) )
xlim_h2 = c(median_logTTE,median_logTTE )

ylim_h3 = c(min(h3$density),max(h3$density) )
xlim_h3 = c(mean(Xbeta_LTTE),mean(Xbeta_LTTE) )


ylim_h4 = c(min(h4$density),max(h4$density) )
xlim_h4 = c(median_TTE,median_TTE )


#dev.off()
par(mfrow=c(2,2))

plot(h1$mids,h1$density,col='red',main=""errors - gumbel dist"",xlab=""errors (log time)"")
lines(xlim_h1,ylim_h1)

plot(h3$mids,h3$density,col='red',main=""mean log TTE (X*beta) - fixed"",xlab=""mean log TTE (log time)"")
lines(xlim_h3,ylim_h3)

plot(h2$mids,h2$density,col='red',main=""log TTE - gumbel dist"",xlab=""log TTE (log time)"")
lines(xlim_h2,ylim_h2)


plot(h4$mids,h4$density,col='red',main=""TTE - Weibull dist"",xlab=""TTE (time)"")
lines(xlim_h4,ylim_h4)
</code></pre>
"
"0.0663812836584521","0.0771229887279699","135613","<p>I'm using R to create a linear regression model from survey data about public sentiment for a new technology. I am encountering a problem where the addition of a new explanatory variable raises the model's $R^2$ value from 0.52 to precisely 1. This is absurd, but I'm new to this stuff and can't figure out what's going on.</p>

<p>The survey asks several questions about demographic and values and technical knowledge. These items become the explanatory variables in the model. Most are either dummy variables or likert scales that extend from 1 to 7 (meaning that for every such question, each respondent chooses a number between 1 and 7). The survey also asks respondents to what extent they'd support government investment in the new technology. That question becomes the dependent variable in the model. It is also a likert scale that extends from 1 to 7.</p>

<p>I'm using R's <code>lm()</code> function to regress the knowledge, demographics, and values variables against the support for new technology variable. The functional form is:</p>

<pre><code>lm(support~demographics+values+knowledge,data=survey). 
</code></pre>

<p>Out of about 2000 survey responses, 900 remain after NA's are discarded. I created a model comprising approximately 20 explanatory variables, with an $R^2$ value of 0.52. Then, I added in a 21st explanatory variable, and the $R^2$ jumped to 1. When I do a simple regression of only this new variable and the dependent variable, the $R^2$ is 0.67. What could be going on?</p>
"
"0.0709645772411954","0.072141950116023","135847","<p>After you decompose a univariate time series with stl() function in R you are left with the trend, seasonal and random components of the time series. Is it valid to use those components to then model the original timer series with additional other variables?</p>

<p>For example:</p>

<pre><code>&gt; tsData
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2012  22  26  34  33  40  39  39  45  50  58  64  78
2013  51  60  80  80  93 100  96 108 111 119 140 164
2014 103 112 154 135 156 170 146 156 166 176 193 204

&gt; stl(tsData, s.window = ""periodic"")
 Call:
 stl(x = tsData, s.window = ""periodic"")

Components
            seasonal     trend   remainder
Jan 2012 -24.0219753  36.19189   9.8300831
Feb 2012 -20.2516062  37.82808   8.4235219
Mar 2012  -0.4812396  39.46428  -4.9830367
Apr 2012 -10.1034302  41.32047   1.7829612
...
Sep 2014   2.2193527 165.55136  -1.7707170
Oct 2014   7.3239448 169.33893  -0.6628760
Nov 2014  18.4285405 173.12650   1.4449614
Dec 2014  30.5244146 176.84390  -3.3683103
</code></pre>

<p>Now if I wanted to model the time series with a linear model with some other variables is it valid to do so?</p>

<pre><code>lm(index ~ trend + seasonal + s1 + s2, data)
</code></pre>

<p>When I run that model I get an R-squared = .98 which make sense considering that the original time series index is just the sum of trend + season + error. I guess what I'm concerned about is using a linear model with time series data I want to make sure I'm not violating some major rules of linear regression. I figure since I have the seasonal variable I'm essentially controlling for that element and hopefully reducing the auto correlation or am I since the R-squared is so high? Any help is appreciated!</p>
"
"NaN","NaN","135898","<p>Are there any classification algorithms implementations in R such as Decision Trees, Naive Bayes, etc. in which the training instances can have a weight?</p>

<p>I found <a href=""http://stats.stackexchange.com/questions/7513/how-to-use-weights-in-function-lm-in-r"">this</a> relevant question in CrossValidated but it seems it is for regression. I am looking for something similar for classification problems.</p>
"
"0.0808716413062113","0.0822133822214443","135967","<p>I am a beginner in R. I am doing logistic regression using around 80 independent variables using <code>glm</code> function in R. The dependent variable is <code>churn</code> which says whether a customer churned or not. I want to know how to identify the right combination of variables to get a good predictive logistic regression model in R.  I also want to know how to identify the same for making good decision tree in R ( I am using the <code>ctree</code> function from the <code>party</code> package).
So far, I had used <code>drop1</code> function  and  <code>anova(LogMdl, test=""Chisq"")</code> where <code>LogMdl</code> is my logistic regression model to drop unwanted variables in the predictive model.  But maximum accuracy I was able to achieve was only 60%. </p>

<p>Also I am not sure if I am using the <code>drop1</code> and <code>anova</code> functions correctly. I dropped the variables with lowest AIC using <code>drop1</code> function.  Using <code>anova</code> function, I dropped variables with p value > 0.05</p>

<p>Kindly help me how to identify the right set of variables for both logistic regression and decision tree models to increase my model's predictive accuracy to close to 90% or more than that if possible.   </p>

<pre><code>library(party)
setwd(""D:/CIS/Project work"")
CellData &lt;- read.csv(""Cell2Cell_SPSS_Data - Orig.csv"")
trainData &lt;- subset(CellData,calibrat==""1"")
testData &lt;- subset(CellData,calibrat==""0"") # validation or test data set
LogMdl = glm(formula=churn ~ revenue  + mou    + recchrge+ directas+ 
               overage + roam    + changem +
               changer  +dropvce + blckvce + unansvce+ 
               custcare+ threeway+ mourec  +
               outcalls +incalls + peakvce + opeakvce+ 
               dropblk + callfwdv+ callwait+
               months  + uniqsubs+ actvsubs+  phones  + models  +
               eqpdays  +customer+ age1    + age2    + 
               children+ credita + creditaa+
               creditb  +creditc + creditde+ creditgy+ creditz + 
               prizmrur+ prizmub +
               prizmtwn +refurb  + webcap  + truck   + 
               rv      + occprof + occcler +
               occcrft  +occstud + occhmkr + occret  + 
               occself + ownrent + marryun +
               marryyes +marryno + mailord + mailres + 
               mailflag+ travel  + pcown   +
               creditcd +retcalls+ retaccpt+ newcelly+ newcelln+ 
               refer   + incmiss +
               income   +mcycle  + creditad+ setprcm + setprc  + retcall, 
               data=trainData, family=binomial(link=""logit""),
               control = list(maxit = 50))
ProbMdl = predict(LogMdl, testData, type = ""response"")
testData$churndep = rep(0,31047)  # replacing all churndep with zero
testData$churndep[ProbMdl&gt;0.5] = 1   # converting records with prob &gt; 0.5 as churned
table(testData$churndep,testData$churn)  # comparing predicted and actual churn
mean(testData$churndep!=testData$churn)    # prints the error %
</code></pre>

<p>Link for documentation of variables: <a href=""https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/</a></p>

<p>Link for Dataset (.csv file) : 
<a href=""https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/</a></p>

<p>I could not produce the output of <code>dput</code> since the data size is more than 5 MB. So I have zipped the file and placed in the above link. </p>

<p>Description of important variables:
* <code>churn</code> is the variable that says whether a customer churned or not.....
* <code>churndep</code> is the variable that needs to be predicted in the test data (validation data) and has to be compared with the <code>churn</code> variable which is already populated with actual churn.
For both churn and churndep, value of 1 means churned and 0 means not churned.</p>
"
"0.0599760143904067","0.0487768608679754","136040","<p>I'm implementing a logistic regression model in R and I have 80 variables to chose from. I need to automatize the process of variable selection of the model so I'm using the step function.</p>

<p>I've no problem using the function or finding the model, but when I look at the final model I find that some of the variables chosen by the step function are not significant (I look at this using the summary function and looking at the fourth column in $coef, this is the Wald Test). This is a problem because I need all the variables included in the model to be significant.</p>

<p>Is there any function or any way to get the best model based on AIC or BIC methods but that also consider that all the coefficients must be significant?
Thanks</p>
"
"0.0464572209811883","0.0472279924554862","136055","<p>The deviance of a model is defined as:
$D =  -2(loglikelihood(model) - loglikelihood(saturated.model)) $</p>

<p>So I tried to compare the result of this formula with the output of the deviance() function with different models:</p>

<p>Linear regression model:</p>

<pre><code>&gt; ?cats
&gt; m1=lm(Hwt~Bwt+Sex,data=cats)
&gt; m1.saturated=lm(Hwt~factor(1:nrow(cats)),data=cats)
&gt; deviance(m1)
[1] 299
&gt; sum(residuals(m1)^2)
[1] 299
&gt; deviance(m1.saturated)
[1] 0
&gt; as.numeric(-2*(logLik(m1)-logLik(m1.saturated)))
[1] Inf
</code></pre>

<p>299 $\neq $ Inf<br>
Something is wrong</p>

<p>GLM: Poisson regression</p>

<pre><code>&gt; x = rnorm(10)
&gt; y = rpois(10,lam=exp(1 + 2*x))
&gt; m2 = glm(formula = y ~ x, family = poisson)
&gt; m2.saturated &lt;- glm(y ~ factor(1:10),family=poisson)
&gt; deviance(m2) 
[1] 14
&gt; deviance(m2.saturated) 
[1] 4.5e-10
&gt; as.numeric(-2*(logLik(m2)-logLik(m2.saturated))) 
[1] 14
</code></pre>

<p>14 = 14<br>
Ok, it works!</p>

<p>GLM: Gamma regression</p>

<pre><code> &gt; cement &lt;- read.table(""cement.dat"", col.names = c(""time"",""resistance""), dec = "","") 
 &gt; attach(cement)
 &gt; m3 &lt;- glm(resistance ~ I(1/time), family = Gamma)
 &gt; m3.saturated &lt;- glm(resistance ~ factor(1:nrow(cement)), family = Gamma)
 &gt; deviance(m3)
 [1] 0.16
 &gt; deviance(m3.saturated)
 [1] 9.3e-17
 &gt; as.numeric(-2*(logLik(m3)-logLik(m3.saturated)))
 [1] 758
</code></pre>

<p>0.16 $\neq $ 758<br>
There must be a mistake somewhere</p>

<p>Shouldn't the results be the same?
Thank you.</p>
"
"0.026822089039291","0.0272670941574606","136071","<p>I am hoping someone can check this code to ensure that I have interpreted the various pieces of PCA correctly. I am trying to figure out a way to identify the leading contributors to the performance of multiple securities. For example, one idea I had was to run a multivariate regression using the securities returns as dependent variables and include things like oil, the dollar, the euro, treasury yields, etc.
E.g.,
SBUX + AAPL + MCD + BAC + TWTR = intercept + oil + dollar + euro + steel + gold + e</p>

<p>I then thought that PCA would probably be better suited for this type of exercise. Here is my code from R. The csv file consists of a matrix of 900 securities and 30 rows (30 daily returns for 900 securities)</p>

<pre><code>FD &lt;- read.csv(""U:/Personal Projects/R/Data Files/FD Securities Jan 2015.csv"")

#Removes columns with any na values
FD1 &lt;- FD[, sapply(FD, function(x) !any(is.na(x)))]

#removes ""zero/constant variance"" columns, which I think are NaNs I couldnt erase using is.nan
FD2 &lt;- FD1[,apply(FD1, 2, var, na.rm=TRUE) != 0]

#Calc PCs using singe value decomposition (prcomp). Should data be a correlation matrix of the variables? I get reasonable looking results both ways, i.e., PC1 explains 30-50% of variance, PC2 ~10%-15%, etc.
FD2.pca &lt;- prcomp(cor(FD2), retx = TRUE, scale = TRUE)
summary(FD2.pca)
plot(FD2.pca)

#These are the 'loadings', i.e., coefficients used for each linear combination?
as.matrix(FD2.pca$rotation[,1])

#I think these ""scores"" are the coefficients of interest, as they incorporate the factor     weightings because the output is pca$rotation * scale (stddev of each factor)
as.matrix(FD2.pca$x[,1]) as.matrix(FD2.pca$x[,2]) as.matrix(FD2.pca$x[,3])     as.matrix(FD2.pca$x[,4])

#Scatterplot of the first two principal components. Not sure if this is right of if $x should be used.
plot(x = FD2.pca$rotation[,1], FD2.pca$rotation[,2], xlab = ""PC1"", ylab = ""PC2"", main=""Principal Component Analysis:"")
</code></pre>
"
"0.0379321620905441","0.0385614943639849","136085","<p>I'm building a logistic regression in R using LASSO method with the functions <code>cv.glmnet</code> for selecting the <code>lambda</code> and <code>glmnet</code> for the final model. </p>

<p>I already know all the disadvantages regarding the automatic model selection but I need to do it anyway.</p>

<p>My problem is that I need to include factor (categorical) variables in the model, is there any way to do it without creating a lot of dummy variables? This variables are almost all strings and not numbers.</p>

<p>Thanks in advance.</p>
"
"0.093190562755245","0.0947366868222571","136146","<p>I'm trying to manually demean panel data by both time demeaning and cross-sectional demeaning, yet haven't been able to do it well. The best thing I can think of is looping through the means for each year and then for each entity, plus for each variable. Which seems like it would get slow with a lot of a data (especially if I plan on doing simulations). Is there a better way to do this?</p>

<p>I couldn't find a function in R to do it either.</p>

<p>Here is what I have so far: (DATA is a data frame that of the format (n,T, Variables) where n is the entity id and T is the time id); </p>

<pre><code>demean=function(DATA){

names=colnames(DATA)
T=(max(DATA[,2])-min(DATA[,2])+1)
N=max(DATA[,1])

##Cross-Sectional Demeaning
widedata=reshape(DATA, direction=""wide"", v.names=names[-c(1:2)],     idvar=names[2], timevar=names[1])
crossmean=matrix(NA, ncol=length(colnames(widedata))-1)
crossmean[,1:length(t(crossmean[1,]))]=colMeans(widedata[,-c(1)], na.rm=TRUE)
crosswidedata=widedata
for(i in 1:T){
crosswidedata[i,-c(1)]=widedata[i,-c(1)]-crossmean
}

crossdemeaned=reshape(crosswidedata, direction=""long"", times=names[2] )

##Time Demeaning
widedata=reshape(crossdemeaned, direction=""wide"", v.names=names[-c(1:2)], idvar=names[1], timevar=names[2])
timemean=matrix(NA, ncol=length(colnames(widedata))-1)
timemean[,1:length((t(timemean[1,])))]=colMeans(widedata[,-c(1)], na.rm=TRUE)
timewidedata=widedata
for(i in 1:N){
timewidedata[i,-c(1)]=widedata[i,-c(1)]-timemean
}

demeaned=reshape(timewidedata, direction=""long"", times=names[2] )


demeaned=demeaned[order(demeaned[,names[2]],demeaned[,names[1]]),]

return(demeaned)
}
</code></pre>

<p>I simulate a random spatial autoregressive panel data set with fixed and time effects, demean the data, and then run a test within regression of the demeaned data as following:</p>

<pre><code>    ##Creating a Random Spatial Autoregressive Panel Dataset with Fixed and Time Effects

library(spdep)
library(splm)

set.seed(44222)

################################################################
# RANDOMLY INSERT A CERTAIN PROPORTION OF NAs INTO A DATAFRAME #
################################################################
NAins &lt;-  NAinsert &lt;- function(df, prop = .1){
n &lt;- nrow(df)
m &lt;- ncol(df)
num.to.na &lt;- ceiling(prop*n*m)
id &lt;- sample(0:(m*n-1), num.to.na, replace = FALSE)
rows &lt;- id %/% m + 1
cols &lt;- id %% m + 1
sapply(seq(num.to.na), function(x){
        df[rows[x], cols[x]] &lt;&lt;- NA
    }
)
return(df)
}
############## df means data frame, and prop is the proportion of missing ##data desired

############################
########CREATES A RANDOM WEIGHT MATRIX
############################
DAG.random &lt;- function(v, nedges=1) {
edges.max &lt;- v*(v-1)/2
# Assert length(v)==1 &amp;&amp; 1 &lt;= v
# Assert 0 &lt;= nedges &lt;= edges.max
index.edges &lt;- lapply(list(1:(v-1)), function(k) rep(k*(k+1)/2, v-k)) 
index.edges &lt;- index.edges[[1]] + 1:edges.max
graph.adjacency &lt;- matrix(0, ncol=v, nrow=v)
graph.adjacency[sample(index.edges, nedges)] &lt;- 1
graph.adjacency
}

###################################
###### Create Data ################
###################################

n=50                                    ##Number of Observations
T=20                                    ##Number of years
connect=6                               ##average number of connections
W=DAG.random(n,n*.5*connect)            ##Creates a random weight matrix      
W=W+t(W)
W=sweep(W, 1, rowSums(W), FUN=""/"")      ##Row Normalizes the Random Weight

v=3                                     ##Number of Independent Variables

X=matrix(data=NA, nrow=n*T, ncol=(v+2))
for(jjj in 1:T){
X[((jjj-1)*n+1):(jjj*n),1]=c(1:n)
X[((jjj-1)*n+1):(jjj*n),2]=jjj
for(jj in 3:(v+2)){
X[((jjj-1)*n+1):(jjj*n),jj]=rnorm(n, 1, 2)
}
}


sigma=1                             ##standard deviation of model
p=.9                                ##coefficient on spatial lag of Y
B0=matrix(1, nrow=n, ncol=1)
B1=matrix(2, v , 1)
B2=matrix(.5, v, 1)
yrcoef=matrix(runif(1:(T), -20,20), (T), 1)     #Year Fixed Effects
yrcoef[1]=0
alpha=runif(n, -20,20)                      # Nation Fixed Effects

e=matrix(rnorm(n*T,0,sigma), n*T, 1)

inv=solve((diag(n)-p*W))
Y=matrix(data=NA, nrow=n*T, ncol=1)

for(jjj in 1:T){
invY= as.matrix(alpha) + X[((jjj-1)*n+1):(jjj*n),(3:(2+v))] %*% B1 +    yrcoef[jjj,1] + e[((jjj-1)*n+1):(jjj*n),1]
Y[((jjj-1)*n+1):(jjj*n),1]=inv %*% invY
}

Y=data.frame(Y)
X=data.frame(X)
DATA=cbind(X,Y)
names=c(""N"", ""T"", ""X1"", ""X2"", ""X3"", ""Y"")
colnames(DATA)=names

DATA=demean(DATA)

model1=as.formula(paste(names[length(names)], paste(names[-c(1:2,length(names))], collapse= "" + ""), sep="" ~ ""))

lw=mat2listw(W)

mod=spml(model1, data=DATA, listw=lw, model=""within"", effect=c(""twoways""),    lag=TRUE, spatial.error=""none"")

effects(mod)
</code></pre>

<p>However, the time period effects are non-zero (the spatial fixed effects are numerically zero). I can't seem to figure out why the spatial fixed effects is working while the time period effects portion is not.</p>

<p>Any thoughts?</p>
"
"0.053644178078582","0.0545341883149212","136437","<p>Iâ€™m having trouble figuring out how to apply the LME function to a set of data. What I have is a list of Stores and their respected customer count, by week, with various external factors for each store: Crime Rate per 100k people, % of people with a college Degree, Level of inequality, and so forth (sample Below.)</p>

<pre><code>Store   Week    CustomerCount   CrimeRate   %collegeDegree  Inequality  Median Income
1       1         200              5        0.25            0.4         25000
1       2         259               5       0.25            0.4         25000
1       3         234              5        0.25            0.44        25000
â€¦                       
5       1         106               1       0.2         0.43            26000
5       2         96                1       0.2         0.42            26000
5       3         101               2       0.21        0.42            26000
</code></pre>

<p>Now, what Iâ€™m trying to do is this: Iâ€™m trying to run an <strong>annual</strong> regression of the data to determine if some of the external factors (crime Rate, college rate, inequality level) have an effect on store customer count.</p>

<p>Now, I need to do a repeated measure analysis on this where the repeated measures are Week, and entity of store has the repeated values. Iâ€™ve set up the LME function thusly:</p>

<p>ïƒ˜   Lme(CustomerCount ~ CrimeRate + CollegeRate + InequalitlyLevel, random = ???? , data=the.data)</p>

<p>So, my question is: what do I put in for the random variable? Is it â€œâ€¦,random = ~1|week/Store,â€¦â€ or a variation thereof?</p>
"
"0.018966081045272","0.0385614943639849","136563","<p>I am using the <code>nnls()</code> function from the <code>nnls package</code> in R to do a linear regression for regressors $x_i$ and observations $y$.
The function delivers beta coefficients $\beta_i\geq{0}, \forall i$. However, is it possible to apply the constraints only to <strong>some</strong> regressors so that</p>

<p>$$\beta_k \geq 0 \quad k \in \{1...,10\}, k\neq i \\ 
\beta_i \in \mathbb{R} \quad i \in \{1...,10\}$$</p>

<p>given that I have 10 regressor variables?</p>

<p><code>nnls()</code> offers the possibility to enforce some coefficients to be negative and others to be positive. I only want the positive constraint for some of them, the other ones can be either positive or negative.</p>
"
"0.0657004319817604","0.0667904674542028","136763","<p><img src=""http://i.stack.imgur.com/enCPt.png"" alt=""Data is a subset of my original DF. Snip of it is attached""></p>

<p>I use R, Party package in order to fit prediction model (""classifier"") for </p>

<p>""Converted.clicks"" as response variable.</p>

<p>The rest of vars are used as explaining variables in the model.</p>

<p>Here is the relevant part of my code:</p>

<pre><code>table(DF$Converted.clicks)

""0"" = 31456              
""1"" =  39  
""2"" =  6


Formula&lt;-Converted.clicks ~ Day.of.week 
                          + Device
                          + Keyword 
                          + Quality.score
                          + Network..with.search.partners. 
                          + Ad.group
                          + Match.type

ct&lt;-ctree(Formula,data=DF) 

####################################### 
</code></pre>

<h3>Issue:</h3>

<p>The Converted.clicks variable is highly imbalanced.The majority of the observations has </p>

<p>class ""zero"". So after ctree function is applied,all the predictions are ""zero"",there are </p>

<p>no classes ""1"" and ""2"" predicted.</p>

<h3>My questions are:</h3>

<ol>
<li><p>Is the classifier Decision Tree model is appropriate model to predict </p>

<p>as.factor(DF$Converted.clicks)?</p></li>
<li><p>If so, how can I balance the response var (i.e.to give the chance the two rest classes</p>

<p>""1"" and ""2"" to be predicted?) - if I need to use weights, I need an        </p>

<p>example,please.</p></li>
<li><p>Is there any other appropriate model to predict # of Converted.clicks? I understand </p>

<p>that Regression Decision Tree is only for continuous response variable, but in my case   </p>

<p>I have an integer response var, please advise.</p></li>
</ol>
"
"0.110590306208719","0.105811867590468","136843","<p>I have a Cox proportional hazards model in R (see made-up example below) that models the effect of some variable, say weight. From this model, I'd like to extrapolate what a change in weight from say 90 to 60 would mean to survival, taking into account the fact that for such a change occurring at say age 40, certain amount of risk has already accumulated (and assuming weight change is instantaneous).</p>

<p>I've attached some code which involves</p>

<ol>
<li>fitting the Cox model (using age as the time scale);</li>
<li>extracting the predicted cumulative survival $S(t)$ using survfit for weight=90 and 60;</li>
<li>getting the cumulative hazard $H(t) = -\log(S(t))$;</li>
<li>getting the ""instantaneous"" hazard $h(t)$ via differencing $H(t)$ (plus small fudge factor to avoid zero hazard), which seems to do the job but probably a bit hacky;</li>
<li>adding a constant to the $\log(h(t))$ for all timepoints after the change, equivalent to the $\beta$ coefficient from the Cox regression times the <em>difference</em> in weights (90-60=30);</li>
<li>get the new survival functions $S^\prime(t)$ as $\exp(-{\rm cumsum}(\exp(\log(h^\prime(t)))))$.</li>
</ol>

<p>This procedure produces reasonable results (plotted as $1 - S(t)$), but is it correct or am I just lucky?</p>

<p><img src=""http://i.stack.imgur.com/ax5Bu.png"" alt=""enter image description here""></p>

<pre><code>library(survival)
set.seed(1)
rm(list=ls())

# Simulate some semi-realistic data
n      &lt;- 1e3
age    &lt;- round(runif(n, 1, 60))
weight &lt;- round(rnorm(n, 70, 10))
height &lt;- round(runif(n, 1.3, 1.9), 2)
sex    &lt;- sample(c(""M"", ""F""), length(age), replace=TRUE, prob=c(0.7, 0.3))
d.time &lt;- ceiling(rexp(n, weight / 1e4))
cens   &lt;- round(runif(n, 1, 60))
death  &lt;- d.time &lt;= cens
d.time &lt;- pmin(d.time, cens)
d      &lt;- data.frame(age=age, weight=weight, height=height, difftime=d.time, 
                     time=d.time + age, sex=sex, death=death)

s     &lt;- coxph(Surv(age, time, death) ~ height + weight, data=d)
d.new &lt;- data.frame(weight=c(60, 90), height=1.7)
sf    &lt;- survfit(s, d.new)

# The cumulative hazard function H is -log(S(t)) where S(t) is the survivor function
# (aka cumulative survival)
S &lt;- sf$surv[,2]

# Assume we start off with high weight
H &lt;- -log(S)

# The hazard is the derivative (here, finite difference) of the cumulative hazard H
# But the hazard can't be zero exactly as when we take log hazard, won't make sense
h &lt;- diff(c(0, H)) + 1e-6

# We introduce a changepoint in the hazard, but must make sure that the
# hazard does not become negative - this is naturally achieved because the
# Cox model is linear in the log-hazard. This means that the final survivor
# function will always be monotonically decreasing for any value of delta in 
# (-Inf, +Inf); delta &gt; 0 increases hazard, delta &lt; 0 decreases hazard
delta &lt;- coef(s)[""weight""] * (d.new$weight[1] - d.new$weight[2])
logh  &lt;- log(h)
age   &lt;- 40
logh[sf$time &gt; age] &lt;- logh[sf$time &gt; age] + delta
h     &lt;- exp(logh)

# Get the new cumulative hazard and new survivor functions
H &lt;- cumsum(h)
S &lt;- exp(-H)

# Compare original survivor function with modified one
plot(sf, lwd=5, col=1:2, conf.int=FALSE, mark=NA, fun=""event"",
     xlab=""Age"", ylab=""Cumulative risk"")
lines(c(0, sf$time), 1 - c(1, S), type=""s"", col=3, lwd=5)
abline(v=age, lty=2)
legend(x=""topleft"", legend=c(""Weight=60"", ""Weight=90"", ""Weight decreased 90 to 60""),
       col=1:3, lwd=5)
</code></pre>
"
"0.0657004319817604","0.0667904674542028","136998","<p>I'm trying to classify a variable into either 0 or 1, using 50 factors, with a sample size of 2000. 25% of the dependent variables are 0 and the rest are 1.</p>

<p>Of these factors, 30 are categorical. I've been having a lot of trouble doing PCA in R with the pcr function from the pls library:</p>

<p>--It seems that pcr will only allow a qualitative variable to have two categories. For example, my dependent variable can come from 5 countries. The pcr function will only allow my ""country"" factor to have 2 values--if there are 3, it throws an error. (Also annoying, the factor can only have one character--it will allow ""A"" but throw an error for ""AFRICA"")</p>

<p>--If I use model.matrix to get dummy variables for my qualitative factors, pcr starts going insane when the number of variables goes over 50 or so. It'll go from CV=.2717 to CV=3e8 from 49 comps to 50 comps, for example.</p>

<p>I'm wondering if PCA in general is not meant for high-dimensional data, particularly with a lot of categorical factors? Or am I just formatting my data incorrectly?</p>

<p>Note: I have used glmnet with alpha = 0, 1, and .5 (ridge, lasso, and elastic net regression) and it worked perfectly using the model.matrix. I wanted to compare my results with PCA, but I do know that there are alternatives for feature selection.</p>
"
"0.0967084173462244","0.0983129061176287","137120","<p>I am using a software in my analysis, from which I obtained the $R^2$ and estimated effects ($\beta$) of a linear regression model. In addition, it outputs the design matrix ($X$) of the model. To understand how $R^2$ was calculated in this software, I tried to calculate it in R with the design matrix and estimated effects. </p>

<p>Full model: y = cross + x</p>

<p>Reduced model: y = cross</p>

<p>The formula for calculating $R^2$ is 1-RSS_full/RSS_red, where RSS_full refers to the residual sum of squares of the full model and RSS_red is the residual sum of squares of the reduced model. I calculated the RSS_full with the residuals obtained from lm function in R. For RSS_red, I tried two methods to get the residuals and then RSS_red. </p>

<p>Method 1: apply lm function in R, which is sum(lm(y~cross-1,tol=1e-4)$residuals^2)</p>

<p>Method 2: firstly calculate the fitted value of the reduced model (y_red) with its design matrix and the estimated effects from the full model obtained from the software (y_red = cross%*%cross_effects), and then get the RSS_red by sum((y-y_red)^2).</p>

<p>The results of the two methods are different. However, the result of method 1 is the same as the one obtained from the Software. I do not know why they differ. Could someone give me some suggestions? Thank you! The design matrix, R codes and the output from the sotware are as follows.</p>

<p>Design matrix (ModelSJ file):</p>

<p>It can be downloaded from the dropbox share link: <a href=""https://www.dropbox.com/s/hd3po8g8ixs2wod/ModelSJ?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/hd3po8g8ixs2wod/ModelSJ?dl=0</a></p>

<p>R codes for calculating $R^2$:</p>

<pre><code>data=as.matrix(read.table(file=""https://dl.dropboxusercontent.com/u/24381951/ModelSJ"")) # reads the file from dropbox
y=data[,1] # the first column is the response/observed values.
x=data[,-1] # design matrix of the full model
cross=x[,10:12] # design matrix of the reduced model
RSS_full &lt;- sum(lm(y~x-1,tol=1e-4)$residuals^2) # RSS of the full model

# Method 1
RSS_red &lt;- sum(lm(y~cross-1,tol=1e-4)$residuals^2)
R2_1 &lt;- 1-RSS_full/RSS_red 

# Method 2
cross_effects &lt;- c(3.9171, 4.4411, 3.6381) # the estimated effects ÃŸ obtained 
#from the software based on the full model

y_red &lt;- cross%*%cross_effects # fitted value
RSS_red &lt;- sum((y-y_red)^2)
R2_2 &lt;- 1-RSS_full/RSS_red

# Why are R2_1 and R2_2 not equal?
</code></pre>

<p>Output from the software:</p>

<p>it can be downloaded from <a href=""https://www.dropbox.com/s/8hp8p3kcf0d2nc5/testSJ_DON_iQTLm.xml?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/8hp8p3kcf0d2nc5/testSJ_DON_iQTLm.xml?dl=0</a></p>
"
"0.026822089039291","0.0272670941574606","137280","<p>While performing betaregression using betareg R package I noticed that the terms in my model are often significant, even with very small sample sizes. I tried the same model using glm with binomial family and logit link function, and I get very similar effect sizes but non-significant terms.</p>

<p>Can someone explain me how should I interpret this? Do the two models test significance in different ways? </p>

<p>NOTE: In my case the response variable is a proportion, so, although extremely unlikely, it could even take values 0 and 1.</p>

<pre><code>library(betareg)

Y=c(0.5283019, 0.4845361, 0.4974874, 0.6884735, 0.5967742, 0.6835443, 0.4152047, 0.4949495,
  0.6478873, 0.7695853, 0.4764398, 0.5780591, 0.5689655)
X=c(0.3616452, -0.4931525,  0.7890441,  0.7890441, -0.9205514,  0.7890441, -0.9205514,
 -0.9205514,  1.2164429,  1.2164429, -1.3479503, -1.3479503,  0.7890441)

summary(glm(Y~X, family=binomial('logit')))
summary(betareg(Y~X))
</code></pre>
"
"0.0464572209811883","0.0472279924554862","137572","<p>The <code>plm::pcdtest</code> function implements Pesaran, (2004) General Diagnostic Tests for Cross Section Dependence in Panels. For example a test for a panel model works as such:</p>

<pre><code>data(Grunfeld, package = ""plm"")
## test on heterogeneous model (separate time series regressions)
pcdtest(inv ~ value + capital, data=Grunfeld,
    index = c(""firm"",""year""))
</code></pre>

<p>But if I want to test the cross-section dependance of only one variable in the panel for example <code>inv</code> in the example above, how can I do it in R?</p>
"
"0.053644178078582","0.0545341883149212","137650","<p>Linear Regression has the below set of Assumptions,</p>

<ol>
<li>The Y-Values (or the errors, ""e"") are independent!</li>
<li>The Y-Values can be expressed as a linear function of the X variable.</li>
<li>Variation of observations around the regression line (the residual SE) is constant (homoscedasticity).</li>
<li>For given value of X, Y values (or the error) are Normally distributed.</li>
</ol>

<p>Is there any empirical test instead of visual test in R that can be used to validate the Assumptions in 1, 2 and 4?</p>

<p>I can only find for Assumption 3,
Empirical Test: ncvTest() from CAR package
Value to be observed: p-value
Pass criteria: > 0.05</p>

<p>This is to make an automated script that can assist to choose the best model for a set non linear data using linear regression.</p>

<p>Do assist to point to any books or website if this has been discussed previously as my search has been futile. I find many approaches are visual based then empirical.</p>
"
"0.103881504159667","0.105605001571314","138424","<p>My data is binary with two linear independent variables.  For both predictors, as they get bigger, there are more positive responses.  I have plotted the data in a heatplot showing density of positive responses along the two variables.  There are the most positive responses in the top right corner and negative responses in the bottom left, with a gradient change visible along both axes.</p>

<p>I would like to plot a line on the heatplot showing where a logistic regression model predicts that positive and negative responses are equally likely.  (My model is of the form <code>response~predictor1*predictor2+(1|participant)</code>.)</p>

<p>My question: How can I figure out the line based on this model at which the positive response rate is 0.5?</p>

<p>I tried using predict(), but that works the opposite way; I have to give it values for the factor rather than giving the response rate I want.  I also tried using a function that I used before when I had only one predictor (<code>function(x) ((log(x/(1-x)))-fixef(fit)[1])/fixef(fit)[2]</code>), but I can only get single values out of that, not a line, and I can only get values for one predictor at a time.</p>

<p>I am using R.</p>

<p>Edit: I have added a contour plot over the heat plot (using geom_contour in ggplot2), which produces this:</p>

<p><img src=""http://i.stack.imgur.com/qObZc.png"" alt=""Each cell represents the frequency of positive responses for a single stimulus.  I added the numbers for clarity.""></p>

<p>I'd like to have a line that actually predicts the cutoff point in a fine-grained way; right now for the independent variables I have stimuli at points 40, 45, 50, etc. but I would like to see a line that predicts, e.g., that when x=32 and y=36 that's the threshold for 50% positive responses.  It could be a curve or it could even be a straight line (whose slope might help visualise the relative contributions of the two factors), but I'm not looking for a pure description of the cells which are >50 vs &lt;50, which is what I think this is doing, I'm looking for a way to plot the regression's predictions.</p>
"
"0.122914253319396","0.119003355198144","138691","<p>I have been unsuccessfully trying to model a relationship between two measured variables described by two different power functions, on either side of a threshold. My question is how to best estimate this relationship with a model. The aims of this model is to find the threshold $x_0$ and interpolate as exactly as possible the values of $y$ close to $x_0$. Estimating the relationship further from $x_0$ is less important. Based on theory I derived the following function: \begin{equation} f(x) = \begin{cases}
    a(x_0-x)^b+c;&amp;  x\leq x_0\\
    \frac{c-y_0}{d(x-x_0)+1}+y_0;              &amp; x &gt; x_0.
\end{cases} \end{equation} where $a&gt;0, b,c&gt;0,d&gt;0, \text{and } x_0&gt;0$ are unknown parameters. This function behaves like a power function below $x_0$, and decreases roughly as $1/x$ to the asymptotic value of $y_0$ in the region above $x_0$. The function looks roughly like this:</p>

<p><img src=""http://i.stack.imgur.com/jUXoP.png"" alt=""enter image description here""></p>

<p>So far I have tried using non-linear least squares to estimate this model, but I am getting the ""singular gradient matrix at initial parameter estimates"" error. My data and code in <strong>R</strong> is as follows:</p>

<pre><code>x &lt;- c( 0.33, 0.35, 0.39, 0.44, 0.48, 0.53, 0.57, 0.63, 0.74, 0.99, 1.12, 1.23, 1.37)
y &lt;- c(72354.00, 23578.20, 1863.40, 743.80, 113.00, 9.80, 7.38, 5.30, 5.22, 5.03, 4.74, 4.53, 4.32)
</code></pre>

<p>and the code for the model I tried fitting is:</p>

<pre><code>starting.values &lt;- c(a = 8, b = 17, c = 8, d = 1,y_0 = 3, x_0 = .55)

model2 &lt;- nls(y~ifelse(px &lt; x_0,a*(x_0-px)^b+c,(c-y_0)/(d*(px-x_0)+1)+y_0),data = data.frame(x,y), 
              start = starting.values)
</code></pre>

<p>I have been trying a variety of likely starting parameters without success, is the problem too few data points, or is the model impossible to evaluate this way?</p>

<p>I have been successful, however, in modeling the relationship with a much simpler function: \begin{equation} g(x) = \begin{cases}
    ax^b;&amp;  x\leq x_0\\
    dx^c;              &amp; x &gt; x_0.
\end{cases} \end{equation}</p>

<p>where $d = ax_0^{b-c}$ to ensure continuity at the threshold. I did this by first fitting the data after applying the log-log transform, and than using the resulting values as inputs to the final model evaluated with nls. The code is as follows:</p>

<pre><code># transforming the data
x.log &lt;- log(x)
y.log &lt;- log(y)

# fitting a broken regression line to the log-log data
starting.values.log &lt;- c(a = -5, b = 10, c = -.1, x_0 = .55)
model1 &lt;- nls(y.log ~ifelse(x.log &lt; log(x_0),b*x.log + a, c*x.log+b+log(x_0)*(a-c)),
              data = data.frame(x.log,y.log), 
              start = starting.values.log)
</code></pre>

<p>below is the plot of the resulting model on the log-log plot:</p>

<p><img src=""http://i.stack.imgur.com/zDRvT.png"" alt=""The model vs. the measurements on a log-log scale""></p>

<p>and now I use the obtained parameters to fit the function:</p>

<pre><code># fitting the actual model using the parameters found previously
starting.values &lt;- c(a = exp(-8.6238),b = -17.7984, c = -.4418, x_0 = 0.555)
model2 &lt;- nls(y~ifelse(x &lt; x_0,a*x^b,a*x_0^((b-c))*x^c),data = data.frame(x,y), 
              start = starting.values)
</code></pre>

<p>The parameters are:</p>

<pre><code>Parameters:
      Estimate Std. Error t value Pr(&gt;|t|)    
a    3.653e-05  1.130e-05   3.233   0.0103 *  
b   -1.931e+01  2.804e-01 -68.857 1.45e-13 ***
c   -4.816e-01  8.832e+01  -0.005   0.9958    
x_0  5.341e-01  1.335e+00   0.400   0.6984    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 388.5 on 9 degrees of freedom

Number of iterations to convergence: 18 
Achieved convergence tolerance: 3.328e-07
</code></pre>

<p>The main problems with this method is that a) I am unable to determine the exponent of the power law when $x \rightarrow x_0$ from below and b) it does not allow for an asymptote other than 0 in $x \rightarrow \infty$. The first problem is much more important to me.</p>

<p>My question is how to best model the relationship between x and y, bering in mind that the theory supports the first function? Is the problem with trying to evaluate the first function too few data points or is it impossible to evaluate and I should try to use the second method instead? If so, than is there any way to obtain the exponent of the power law describing y when $x \rightarrow x_0$ from below?</p>
"
"0.0709645772411954","0.072141950116023","138938","<p>What is the correct way to calculate the standard errors of the coefficients in a weighted linear regression?</p>

<p>The regression equation I am using is $y_i = a + bx_i$, and I have weights, $w_i = 1/\sigma_i$.  The numerical recipes formula for a straight line fit, and the formula given in ""An introduction to error analysis"" by J. R Taylor, (and Wikipedia too) state that the standard error in the $b$ coefficient is calculated as $$\sigma_b = \sqrt{\frac{\sum w_i}{\sum w_i\sum w_i x_i^2-(\sum w_i x_i)^2}}$$ (or alternatively in matrix form the standard errors are, $\sigma^2 = (X'WX)^{-1}$).  This formula can be derived from propagation of errors. </p>

<p>Using R's $lm()$ function (and python's StatsModels), I get a standard error in the $b$ coefficient which appears* to be calculated as $$\sigma_b = \sigma_e\sqrt{\frac{\sum w_i}{\sum w_i\sum w_i x_i^2-(\sum w_i x_i)^2}}$$
where $\sigma_e^2 = \sum w_i(y_i - a - bx_i)^2/(N-2)$ (alternatively, $\sigma^2 = \sigma_e^2(X'WX)^{-1}$ ).  So they are the same, except for the $\sigma_e$ multiplier in R and StatsModel.</p>

<p>Is it possible that these actually different measures that are just being called the same thing? Is one preferred over the other for an estimate of the standard error?</p>

<p>*I say ""appears"" because I couldn't find the actual formula anywhere.</p>

<p>edited because I had omitted the weight terms in the denominators.   </p>
"
"NaN","NaN","139756","<p>I'm working on a classification problem with continous and categorical predictors with Random Forests (RF). I'm particularly interested on RF as we avoid the specification of the functional form.</p>

<p>However when it comes to the partial dependance for categorical variables, I'm not sure how to interpret that. For instance, the partial dependence (with the command <code>partialPlot</code> in the <code>R</code> package <code>randomForest</code>) for a binary predictor would give two values, one for each category. My question is: how exactely do you interpret those values? The documentation of <code>partialPlot</code> is quite cryptic in this respect.</p>

<p>My confusion arises, I guess, because I'm used with usual logistic regression where with a dummy coding system you in general obtain the log-odds of the variable of interest against the baseline category. But with RF things are different... Any help is appreciable!</p>
"
"0.0758643241810882","0.0674826151369737","139988","<p>For example, if you have a logistic regression on certain dataset:</p>

<pre><code>fit &lt;- glm(y ~ x, data = test, family = ""binomial"")
</code></pre>

<p>If you do <code>predict(fit, newdata, type = ""link"", se = TRUE)</code>, you will get a column named <code>se.fit</code>, which is the standard error for each predicted y value.</p>

<p>My questions are:  </p>

<ol>
<li><p>How is the MSE value for the link function is computed here?  </p>

<p>The variance of the fitting coefficients are basically the MSE times the variance-covariance matrix, there should be a way to compute the MSE value first. But for response variables that have 0 and 1 values, the link function corresponds to 0 and infinity. In this case, how does the model compute this value? Is there any way I can get the MSE value for the <code>glm</code> fitting in R?</p></li>
<li><p>Is <code>se.fit</code> the standard error for the link function value of the fitted line at point <code>x0</code>, or the standard error for the predicted link function value of <code>y</code> at point <code>x0</code>?</p></li>
</ol>
"
"0.0929144419623766","0.0944559849109725","140183","<p>The age old question of comparing sums of squares (SS) between programs has reared its ugly head again. </p>

<p>I am trying to replicate output in SPSS, that was computed using Type 3 Sums of Squares, in R. </p>

<p>I understand that with multiple regressions, there are several ways to get Type 3 SS in R (to match Type 3 output from SPSS). </p>

<p>However, I am running a mixed model using aov (which uses Type 1 SS) and even when I try all the ""usual"" fixes,"" my estimates don't match the Type 3 SS output from SPSS. </p>

<p>First of all, when I run the SPSS syntax using ""/METHOD=SSTYPE(1)"" the results match those I get using this code:</p>

<pre><code>  mymodel&lt;-aov(data=longdat,  DV ~ 1 + Task + Cue + Compatibility + Cue:Task + Compatibility:Task + Cue:Compatibility + Cue:Compatibility:Task + Error(subject/Cue/Compatibility/Cue*Compatibility))
  summary(mymodel)
</code></pre>

<p>So I know the analyses are the same when they use Type 1 SS. </p>

<p>However, when I use:</p>

<pre><code> options(contrasts = c(""contr.sum"",""contr.poly""))
 tt&lt;-lm(DV ~ 1 + Task + Cue + Compatibility + Cue:Task  + Compatibility:Task + Cue:Compatibility + Cue:Compatibility:Task + 1/subject/Cue/Compatibility    /(Cue*Compatibility), data=longdat)
 drop1(tt, ~., test=""F"")
</code></pre>

<p>The results do not match the SPSS Type 3 output. </p>

<p>In attempts to get matching output, I have also tried the Anova function (which can give Type 3 SS)</p>

<pre><code>  Anova(mymodel, type=3, test.statistic=""F"") 
</code></pre>

<p>but I get this error <code>""Error in terms.formula(formula, data = data) : 'data' argument is of the wrong type.""</code></p>

<p>I have also tried using <code>lmer</code>.</p>

<p>Can someone help me get Type 3 Sums of Squares for a mixed model in R?</p>

<p>Thank you! </p>
"
"0.0608267804924532","0.0618359572423054","140929","<p>I wish to carry out logistic regression analysis using Firth's method, as implemented in R logistf package, to analyse SNP case/control data, for rare variants, whilst controlling for stratification using PCA eigenvectors as covariates. I wish to obtain p-values for each SNP (additive model).</p>

<p>Previously I have performed logistic regression analysis using PLINK:</p>

<pre><code>plink  --bfile snpdata --logistic --ci 0.95 --covar plink2_pca.eigenvec --covar-number 1-2 --out snpout
</code></pre>

<p>I would like to perform similar analysis, but wish to handle quasi-complete separation of the rare variants in my data sets.</p>

<p>I have followed a SNP analysis example provided with logistf and been able to obtain P values:</p>

<p>A very small sample of the snpdata (cases: case 1, control 0; SNP additive allele counts for minor allele: 0, 1, 2):</p>

<pre><code>           PC01         PC02 case exm226_A exm401_A exm4584_A exm146_A
1  -0.003092320 -0.002737810    1            0       0       0       0
2   0.015637300  0.008232330    1            0       0       0       0
3   0.006746730  0.008704400    1            0       1       0       1
4   0.001438270  0.000875751    0            0       0       0       0
5  -0.004161490  0.011407500    0            0       0       2       0

for(i in 1:ncol(snpdata)) snpdata[,i] &lt;-as.factor(snpdata[,i])
snpdata &lt;- snpdata[sapply(snpdata,function(x) length(levels(x))&gt;=2)] 
fitsnp &lt;- logistf(data=snpdata, formula=case~1, pl=FALSE)
add1(fitsnp)
</code></pre>

<p>But I am not clear on how to pass the eigenvectors in as covariates, or whether I can used the eigenvector values as is, or need to convert to these as factors?   </p>

<pre><code>fitsnp &lt;- logistf(data=snpdata,formula=case~(1+PC01+PC02), pl=FALSE)
</code></pre>

<p>I'm not sure if I am on the right track here and can't find a sufficiently similar example on-line to follow.</p>

<p>I would appreciate any assistance, or explanation if I am going completely wrong here.</p>

<p>Thanks in advance.</p>
"
"0.0379321620905441","0.0385614943639849","141243","<p>I am trying to replicate what the function <code>dfbetas()</code> does in <strong><em>R</em></strong>.</p>

<p><code>dfbeta()</code> is not an issue... Here is a set of vectors:</p>

<pre><code>x &lt;- c(0.512, 0.166, -0.142, -0.614, 12.72)
y &lt;- c(0.545, -0.02, -0.137, -0.751, 1.344)
</code></pre>

<p>If I fit two regression models as follows:</p>

<pre><code>fit1 &lt;- lm(y ~ x)
fit2 &lt;- lm(y[-5] ~ x[-5])
</code></pre>

<p>I see that eliminating the last point results in a very different slope (blue line - steeper):</p>

<p><img src=""http://i.stack.imgur.com/4ypID.jpg"" alt=""enter image description here""></p>

<p>This is reflected in the change in slopes:</p>

<pre><code>fit1$coeff[2] - fit2$coeff[2]
-0.9754245
</code></pre>

<p>which coincides with the <code>dfbeta(fit1)</code> for the fifth value:</p>

<pre><code>   (Intercept)            x
1  0.182291949 -0.011780253
2  0.020129324 -0.001482465
3 -0.006317008  0.000513419
4 -0.207849024  0.019182219
5 -0.032139356 -0.975424544
</code></pre>

<p>Now if I want to standardize this change in slope (obtain <strong><em>dfbetas</em></strong>) and I resort to: </p>

<blockquote>
  <p>Williams, D. A. (1987) Generalized linear model diagnostics using the
  deviance and single case deletions. Applied Statistics 36, 181â€“191</p>
</blockquote>

<p>which I think may be one of the references in the R documentation under the package <strong>{stats}</strong>. There the formula for <strong><em>dfbetas</em></strong> is:</p>

<p>$\large \mathrm{dfbetas} (i, \mathrm{fit}) = \Large {(\hat{b} - \hat{b}_{-i})\over \mathrm{SE}\, \hat{b}_{-i}}$</p>

<p>This could be easily calculated in R:</p>

<pre><code>(fit1$coef[2] - fit2$coef[2])/summary(fit2)$coef[4]
</code></pre>

<p>yielding: <code>-6.79799</code> </p>

<p>The question is why I am not getting the fifth value for the slope in:</p>

<pre><code>dfbetas(fit1)

  (Intercept)            x
1  1.06199661  -0.39123009
2  0.06925319  -0.02907481
3 -0.02165967   0.01003539
4 -1.24491242   0.65495527
5 -0.54223793 -93.81415653!
</code></pre>

<p>What is the right equation to go from <strong><em>dfbeta</em></strong> to <strong><em>dfbetas</em></strong>?</p>
"
"0.0464572209811883","0.0472279924554862","141339","<p>I'm having trouble in taking a direction of my research project. I have independent variables that are commonly used as economic indicators and I want to include variables/indicators that are not commonly used to improve my eventual forecasts. I have 31 independent variables with 607 monthly observations after making it stationary and applying the scale function.(scale was applied cause my variable series are of different units/measures)
   I used the PCA function and got down to 13 components that capture 80% cumulative of the variance.
   Question is now that I have 13 new independent variables and the one dependent variable that is ternary in the sense that in the 607 observations it indicates 1 for peak, 0 for nothing, and -1 as trough, what model is best for forecasting/predicting the next 1 &amp; -1 of my dependent variable series based on my 13 independent principal components?</p>

<p>FYI: I have looked at VAR, Cointegration, Granger Causality, Multiple Linear Regression, but can't really make sense if what I'm using is correct and appropriate for my topic.</p>
"
"0.093190562755245","0.0947366868222571","141423","<p>I use the <code>decompose</code> function in <code>R</code> and come up with the 3 components of my monthly time series (trend, seasonal and random). If I plot the chart or look at the table, I can clearly see that the time series is affected by seasonality.</p>

<p>However, when I regress the time series onto the 11 seasonal dummy variables, all the coefficients are not statistically significant, suggesting there is no seasonality.</p>

<p>I don't understand why I come up with two very different results. Did this happen to anybody? Am I doing something wrong?</p>

<hr>

<p>I add here some useful details.</p>

<p>This is my time series and the corresponding monthly change. In both charts, you can see there is seasonality (or this is what I would like to assess). Especially, in the second chart (which is the monthly change of the series) I can see a recurrent pattern (high points and low points in the same months of the year).</p>

<p><img src=""http://i.stack.imgur.com/rILAU.jpg"" alt=""TimeSeries""></p>

<p><img src=""http://i.stack.imgur.com/LdVnv.jpg"" alt=""MonthlyChange""></p>

<p>Below is the output of the <code>decompose</code> function. I appreciate that, as @RichardHardy said, the function does not test whether there is actual seasonality. But the decomposition seems to confirm what I think.</p>

<p><img src=""http://i.stack.imgur.com/ZaVRV.jpg"" alt=""Decompose""></p>

<p>However, when I regress the time series on 11 seasonal dummy variables (January to November, excluding December) I find the following:</p>

<pre><code>    Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept) 5144454056  372840549  13.798   &lt;2e-16 ***
    Jan     -616669492  527276161  -1.170    0.248    
    Feb     -586884419  527276161  -1.113    0.271    
    Mar     -461990149  527276161  -0.876    0.385    
    Apr     -407860396  527276161  -0.774    0.443    
    May     -395942771  527276161  -0.751    0.456    
    Jun     -382312331  527276161  -0.725    0.472    
    Jul     -342137426  527276161  -0.649    0.520    
    Aug     -308931830  527276161  -0.586    0.561    
    Sep     -275129629  527276161  -0.522    0.604    
    Oct     -218035419  527276161  -0.414    0.681    
    Nov     -159814080  527276161  -0.303    0.763
</code></pre>

<p>Basically, all the seasonality coefficients are not statistically significant.</p>

<p>To run linear regression I use the following function:</p>

<p><code>lm.r = lm(Yvar~Var$Jan+Var$Feb+Var$Mar+Var$Apr+Var$May+Var$Jun+Var$Jul+Var$Aug+Var$Sep+Var$Oct+Var$Nov)</code></p>

<p>where I set up Yvar as a time series variable with monthly frequency (frequency = 12).</p>

<p>I also try to take into account the trending component of the time series including a trend variable to the regression. However, the result does not change.</p>

<pre><code>                  Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept) 3600646404   96286811  37.395   &lt;2e-16 ***
    Jan     -144950487  117138294  -1.237    0.222    
    Feb     -158048960  116963281  -1.351    0.183    
    Mar      -76038236  116804709  -0.651    0.518    
    Apr      -64792029  116662646  -0.555    0.581    
    May      -95757949  116537153  -0.822    0.415    
    Jun     -125011055  116428283  -1.074    0.288    
    Jul     -127719697  116336082  -1.098    0.278    
    Aug     -137397646  116260591  -1.182    0.243    
    Sep     -146478991  116201842  -1.261    0.214    
    Oct     -132268327  116159860  -1.139    0.261    
    Nov     -116930534  116134664  -1.007    0.319    
    trend     42883546    1396782  30.702   &lt;2e-16 ***
</code></pre>

<p>Hence my question is: am I doing something wrong in the regression analysis?</p>
"
"0.053644178078582","0.0545341883149212","141583","<p>I am working on a few (both simple and multivariable) regression analyses, and I have cases where the residuals are non-normal, to varying degrees. As I've understood, the Gauss-Markov theorem states that normality of residuals is not necessary for the coefficient point estimates to be correct, i.e., I can trust that the regression summary tells me the BLUE coefficient estimates. However, the standard errors may be biased, and thus, the corresponding t- and p-values may not be correct.</p>

<p>A previous question (<a href=""http://stats.stackexchange.com/questions/83012/how-to-obtain-p-values-of-coefficients-from-bootstrap-regression"">How to obtain p-values of coefficients from bootstrap regression?</a>) asked if it was possible to calculate p-values from bootstrapped coefficients and their CIs. However, if I bootstrap the coefficients and use the bootstrapped mean and the bootstrapped SE to re-calculate t- and p-values, would that be a sound approach?</p>

<p>Here is the code I use, in R:</p>

<pre><code># create linear model
mod &lt;- lm(Y~X,data=dataset); summary(mod)

# create function to return coefficient
mod.bootstrap &lt;- function(data, indices) {    
d &lt;- data[indices, ]
mod &lt;- lm(Y~X, data=d)  
return(coef(mod))
}

# set seed
set.seed(1234)

# begin bootstrap using the boot package
mod.boot &lt;- boot(data=dataset, statistic=mod.bootstrap, R=2000)

# now here is how I re-calculate t- and p-values
bootmean &lt;- mean(mod.boot$t[,2])
booter &lt;- sd(boot.t[,2])
tval &lt;- bootmean/booter
p &lt;- 2*pt(-abs(tval),df=mod$df.residual)
</code></pre>

<p>The new t- and p-values come out fairly close to the LM summary, albeit a bit more conservative, as expected. Is this something I could report? I am very new at this, so I can't really tell if my logic is valid or not.</p>

<p>Edit: Clarified a bit.</p>
"
"0.0379321620905441","0.0385614943639849","141719","<p>I am using the package <code>caret</code> and GBM method for my predictions.</p>

<pre><code>fitControl &lt;- trainControl(## 10-fold CV
        method = ""repeatedcv"",
        number = 10,
        ## repeated ten times
        repeats = 10)

gbmGrid &lt;-  expand.grid(interaction.depth = c(1, 5, 9),
                        n.trees = (1:30)*50,
                        shrinkage = 0.1)

gbmFit &lt;- train(target ~ ., data = traindf,
                method = ""gbm"",
                trControl = fitControl,
                verbose = FALSE,
                ## Now specify the exact models 
                ## to evaludate:
                tuneGrid = gbmGrid,
                metric = ""ROC"")
</code></pre>

<p>There is one concept that I misunderstand. User guides of <code>caret</code> say that ""<strong>By default, the train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models).</strong>"" So, when I run <code>ggplot(gbmFit)</code> I get this graphic:</p>

<p><img src=""http://i.stack.imgur.com/mETe8.png"" alt=""enter image description here""></p>

<p>When I type <code>gbmFit</code> in the console, I see that ""<strong>The final values used for the model were n.trees = 300, interaction.depth = 9 and shrinkage = 0.1</strong>""  How can I manually change these settings in order to make my predictions with different number of boosting iterations and trees?:</p>

<pre><code>predictions_gbm &lt;- predict(gbmFit, newdata = testdf, type = ""raw"")
</code></pre>
"
"0.113796486271632","0.109257567364624","141844","<p>I tried to plot the results of an ordered logistic regression analysis by calculating the probabilities of endorsing every answer category of the dependent variable (6-point Likert scale, ranging from ""1"" to ""6""). However, I've received strange probabilities when I calculated the probabilities based on this formula: $\rm{Pr}(y_i \le k|X_i) = \rm{logit}^{-1}(X_i\beta)$.</p>

<p>Below you see how exactly I tried to calculate the probabilities and plot the results of the ordered logistic regression model (<code>m2</code>) that I fitted using the <code>polr</code> function (<code>MASS</code> package). The probabilities (<code>probLALR</code>) that I calculated and used to plot an ""expected mean score"" are puzzling as the expcected mean score in the plot increases along the RIV.st continuum while the coefficient for <code>RIV.st</code> is negative (-0.1636). I would have expected that the expected mean score decreases due to the negative main effect of <code>RIV.st</code> and the irrelevance of the interaction terms for the low admiration and low rivalry condition (LALR) of the current 2 by 2 design (first factor = <code>f.adm</code>; second factor = <code>f.riv</code>; dummy coding 0 and 1).</p>

<p>Any idea of how to make sense of the found pattern? Is this the right way to calculate the probabilities? The way I used the intercepts in the formula to calculate the probabilities might be problematic (cf., <a href=""https://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression"">Negative coefficient in ordered logistic regression</a>).</p>

<pre><code>m2 &lt;- polr(short.f ~ 1 + f.adm*f.riv + f.adm*RIV.st + f.riv*RIV.st, data=sampleNS)

# f.adm  = dummy (first factor of 2 by 2 design);
# f.riv  = dummy (second factor of 2 by 2 design);
# RIV.st = continuous predictor (standardized)
summary(m2)
Coefficients:
                Value Std. Error t value
f.adm1         1.0203    0.14959  6.8203
f.riv1        -0.8611    0.14535 -5.9240
RIV.st        -0.1636    0.09398 -1.7403
f.adm1:f.riv1 -1.2793    0.20759 -6.1625
f.adm1:RIV.st  0.0390    0.10584  0.3685
f.riv1:RIV.st  0.6989    0.10759  6.4953

Intercepts:
    Value    Std. Error t value 
1|2  -2.6563   0.1389   -19.1278
2|3  -1.2139   0.1136   -10.6898
3|4  -0.3598   0.1069    -3.3660
4|5   0.9861   0.1121     8.7967
5|6   3.1997   0.1720    18.6008
</code></pre>

<p>Here you see how I tried to calculate the probabilities (<code>probLALR</code>) for 1 of the 4 conditions of the 2 by 2 design:</p>

<pre><code>inv.logit  &lt;- function(x){ return(exp(x)/(1+exp(x))) }
Pred       &lt;- seq(-3, 3, by=0.01)
b = c(-2.6563,-1.2139,-0.3598,0.9861,3.1997) # intercepts of model m2
a = c(1.0203,-0.8611,-0.1636,-1.2793,0.0390,0.6989) # coefficients of m2
probLALR   &lt;- data.frame(matrix(NA,601,5))
for (k in 1:5){ 
    probLALR[,k] &lt;- inv.logit(b[k] + a[1]*0 + a[2]*0 + 
                               a[3]*Pred  + a[4]*0*0 + 
                               a[5]*Pred*0 + a[6]*Pred*0)
}

plot(Pred,probLALR[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,probLALR[,2],col=""red"")             # p(1 or 2)
lines(Pred,probLALR[,3],col=""green"")           # P(1 or 2 or 3)
lines(Pred,probLALR[,4],col=""orange"")          # P(1 or 2 or 3 or 4)
lines(Pred,probLALR[,5],col=""orange"")          # P(1 or 2 or 3 or 4 or 5)

# option response functions:

orc = matrix(NA,601,6)
orc[,6] = 1-probLALR[,5]        # prob of 6
orc[,5]= probLALR[,5]-probLALR[,4]  # prob of 5
orc[,4]= probLALR[,4]-probLALR[,3]  # prob of 4
orc[,3]= probLALR[,3]-probLALR[,2]  # prob of 3
orc[,2]= probLALR[,2]-probLALR[,1]  # prob of 2
orc[,1]= probLALR[,1]           # prob of 1


plot(Pred,orc[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,orc[,2],col=""red"")             # p(2)
lines(Pred,orc[,3],col=""green"")           # P(3)
lines(Pred,orc[,4],col=""orange"")          # P(4)
lines(Pred,orc[,5],col=""purple"")          # P(5)
lines(Pred,orc[,6],col=""purple"")          # P(6)

# mean score

mean = orc[,1]*1+orc[,2]*2+orc[,3]*3+orc[,4]*4+orc[,5]*5+orc[,6]*6
plot(Pred,mean,type=""l"",xlab=""RIV.st"",ylab=""expected mean score"",ylim=c(1,6))  
</code></pre>
"
"0.026822089039291","0.0272670941574606","142312","<p>I'm having an interesting dilemma with the <code>neuralnet</code> and <code>nnet</code> packages in <code>R</code>.  I recently tried a series of feed-forward neural networks giving each the same data sets and every single time, no matter how I tweak the algorithms, hidden layers, neuron sizes, maximum iterations or error thresholds, both functions keep converging their predictions to approximately the mean of whatever they are training on.</p>

<p>A linear regression does way better for each series in terms of fit, and both of these packages seem to do a better job fitting random data from the <code>rnorm</code> function than real data.  In regards to the mathematics of the problem, what could be causing this and how should I resolve?  I have sample code below and can paste a sample dataset below if requested.  Thanks!</p>

<pre><code>model6 &lt;- neuralnet(
    target ~ 1 + majorholiday + mon + sat + sun + thu + tue + wed + tickets + l1_target + l7_target, data = data_nn
    ,algorithm = ""rprop+"", hidden = c(8), stepmax = 500000
    ,err.fct = ""sse"", threshold = 0.01, lifesign = ""full"", lifesign.step = 100
    , linear.output= T)
</code></pre>

<p><strong>EDIT</strong></p>

<p>A user requested I paste some data.  Here is one set below and I just tried the same code again prior to uploading and the same thing happens, converges to the mean of <code>target</code> at about 17.45</p>

<pre><code>    row.names   target  majorholiday    mon sat sun thu tue wed backtickets l1_target   l7_target
1   8   18.976573088    0   0   0   0   0   0   0   13806   18.114001584    36.521334684
2   9   20.701716096    0   1   0   0   0   0   0   15308   18.976573088    35.477867979
3   10  25.014573616    0   0   1   0   0   0   0   13439   20.701716096    28.173601042
4   11  15.706877377    1   0   0   0   0   0   0   11283   25.014573616    27.602288128
5   12  19.633596721    0   0   0   0   1   0   0   12272   15.706877377    13.801144064
6   13  20.049395337    0   0   0   0   0   1   0   9528    19.633596721    32.777717152
7   14  21.720178282    0   0   0   1   0   0   0   13747   20.049395337    18.114001584
8   15  23.390961226    0   0   0   0   0   0   0   15277   21.720178282    18.976573088
9   16  16.707829447    0   1   0   0   0   0   0   16058   23.390961226    20.701716096
10  17  15.872437975    0   0   1   0   0   0   0   14218   16.707829447    25.014573616
11  18  23.295531996    1   0   0   0   0   0   0   11249   15.872437975    15.706877377
12  19  22.363710716    0   0   0   0   1   0   0   13993   23.295531996    19.633596721
13  20  24.227353276    0   0   0   0   0   1   0   13402   22.363710716    20.049395337
14  21  20.500068156    0   0   0   1   0   0   0   14244   24.227353276    21.720178282
15  22  26.090995836    0   0   0   0   0   0   0   14502   20.500068156    23.390961226
16  23  18.636425597    0   1   0   0   0   0   0   16296   26.090995836    16.707829447
17  24  15.840961757    0   0   1   0   0   0   0   13694   18.636425597    15.872437975
18  25  20.650050308    1   0   0   0   0   0   0   10774   15.840961757    23.295531996
19  26  13.467424114    0   0   0   0   1   0   0   12348   20.650050308    22.363710716
20  27  19.752222033    0   0   0   0   0   1   0   12936   13.467424114    24.227353276
21  28  27.832676502    0   0   0   1   0   0   0   14342   19.752222033    20.500068156
22  29  18.854393759    0   0   0   0   0   0   0   14390   27.832676502    26.090995836
23  30  10.773939291    0   1   0   0   0   0   0   16724   18.854393759    18.636425597
24  31  12.569595839    0   0   1   0   0   0   0   14091   10.773939291    15.840961757
25  32  28.153882107    1   0   0   0   0   0   0   11250   12.569595839    20.650050308
26  33  24.400031160    0   0   0   0   1   0   0   12803   28.153882107    13.467424114
27  34  21.584642949    0   0   0   0   0   1   0   13318   24.400031160    19.752222033
28  35  27.215419370    0   0   0   1   0   0   0   14193   21.584642949    27.832676502
29  36  21.584642949    0   0   0   0   0   0   0   14312   27.215419370    18.854393759
30  37  15.015403791    0   1   0   0   0   0   0   16445   21.584642949    10.773939291
31  38  26.276956633    0   0   1   0   0   0   0   13753   15.015403791    12.569595839
32  39  15.139500902    1   0   0   0   0   0   0   11619   26.276956633    28.153882107
33  40  12.467824272    0   0   0   0   1   0   0   14006   15.139500902    24.400031160
34  41  21.373413039    0   0   0   0   0   1   0   14098   12.467824272    21.584642949
35  42  8.015029889 0   0   0   1   0   0   0   14462   21.373413039    27.215419370
36  43  16.030059779    0   0   0   0   0   0   0   15367   8.015029889 21.584642949
37  44  19.592295285    0   1   0   0   0   0   0   17868   16.030059779    15.015403791
38  45  18.701736409    0   0   1   0   0   0   0   15052   19.592295285    26.276956633
39  46  16.002499062    1   0   0   0   0   0   0   10035   18.701736409    15.139500902
40  47  16.943822536    0   0   0   0   1   0   0   13708   16.002499062    12.467824272
41  48  11.295881691    0   0   0   0   0   1   0   13463   16.943822536    21.373413039
42  49  19.767792959    0   0   0   1   0   0   0   13998   11.295881691    8.015029889
43  50  19.767792959    0   0   0   0   0   0   0   14745   19.767792959    16.030059779
44  51  16.943822536    0   1   0   0   0   0   0   16156   19.767792959    19.592295285
45  52  14.119852113    0   0   1   0   0   0   0   13552   16.943822536    18.701736409
46  53  22.869570079    1   0   0   0   0   0   0   11554   14.119852113    16.002499062
47  54  10.481886286    0   0   0   0   1   0   0   13437   22.869570079    16.943822536
48  55  19.057975066    0   0   0   0   0   1   0   14076   10.481886286    11.295881691
49  56  20.010873819    0   0   0   1   0   0   0   14567   19.057975066    19.767792959
50  57  9.528987533 0   0   0   0   0   0   0   14277   20.010873819    19.767792959
51  58  21.916671326    0   1   0   0   0   0   0   16545   9.528987533 16.943822536
52  59  11.000000000    1   0   0   0   0   0   1   15599   21.916671326    14.119852113
53  60  17.000000000    0   0   0   0   1   0   1   17463   11.000000000    22.869570079
54  61  10.000000000    0   0   0   0   0   1   1   17935   17.000000000    10.481886286
55  62  20.000000000    0   0   0   1   0   0   1   18357   10.000000000    19.057975066
56  63  19.000000000    0   0   0   0   0   0   1   19246   20.000000000    20.010873819
57  64  17.000000000    0   1   0   0   0   0   1   21234   19.000000000    9.528987533
58  65  11.000000000    0   0   1   0   0   0   1   18493   17.000000000    21.916671326
59  66  9.000000000 1   0   0   0   0   0   1   15315   11.000000000    11.000000000
60  67  22.000000000    0   0   0   0   1   0   1   17841   9.000000000 17.000000000
61  68  9.000000000 0   0   0   0   0   1   1   18312   22.000000000    10.000000000
62  69  11.000000000    0   0   0   1   0   0   1   17880   9.000000000 20.000000000
63  70  5.000000000 0   0   0   0   0   0   1   19371   11.000000000    19.000000000
64  71  15.000000000    0   1   0   0   0   0   1   21696   5.000000000 17.000000000
65  72  12.000000000    0   0   1   0   0   0   1   18829   15.000000000    11.000000000
66  73  10.000000000    1   0   0   0   0   0   1   14749   12.000000000    9.000000000
67  74  15.000000000    0   0   0   0   1   0   1   17928   10.000000000    22.000000000
68  75  7.000000000 0   0   0   0   0   1   1   18254   15.000000000    9.000000000
</code></pre>
"
"0.110590306208719","0.112425109314872","142317","<p>I want to do <strong>multivariate</strong> (with more than 1 response variables) <strong>multiple</strong> (with more than 1 predictor variables) <strong>nonlinear regression</strong> in <strong>R</strong>.</p>

<p>The data I am concerned with are 3D-coordinates, thus they interact with each other, i.e. the x,y,z-coordinates are not independent. So I cannot just call the <em>nls</em> separately for each response variable (which I tried at first). </p>

<p>A subset of the data-frame with 3D-coordinates where x,y,z are the predictive variables and a,b,c the response variables:</p>

<pre><code>              x           y         z           a            b         c
1  -2.26470e-03 -0.05081670 0.0811701 -0.00671079 -0.045721600 0.0705679
2  -9.13106e-05 -0.00670734 0.0724838 -0.00676299 -0.001638430 0.0588486
3   3.81399e-04  0.03556000 0.0782059 -0.00783726  0.038503800 0.0641364
4   1.42293e-03  0.06133920 0.0708688 -0.00820760  0.062697100 0.0572740
5  -5.06043e-02  0.04759040 0.0418189 -0.05949350  0.040427800 0.0266159
6   5.92963e-02  0.04183450 0.0431029  0.05124780  0.038396500 0.0327903
7  -4.44213e-02 -0.00909717 0.0459059 -0.05021130 -0.005634520 0.0329833
8  -3.75400e-02 -0.00625770 0.0567296 -0.04255200 -0.000666089 0.0436465
9  -2.37768e-02 -0.00707318 0.0581552 -0.03048950 -0.001260670 0.0457355
10 -1.56645e-02 -0.01326670 0.0540247 -0.02101350 -0.009021990 0.0413755
</code></pre>

<p><strong>My question:</strong> Is it possible to call the <em>nls</em> function with more than 1 (in my case 3) response variables? In other words is it possible to substitute <em>y</em> in <code>nls(y ~ f(x,y,z, parameters), data)</code> with something like <em>c(a,b,c)</em> or <em>cbind(a,b,c)</em>, such that <code>nls(cbind(a,b,c) ~ f(x,y,z, parameters), data)</code> ?</p>

<p>In the post <a href=""http://stackoverflow.com/questions/12161659/how-to-write-r-formula-for-multivariate-response"">How to write R formula for multivariate response?</a> it is shown that one can combine several response variables with <em>cbind</em> in the case of linear modeling with the <em>lm</em> function.
This doesn't seem to work for nonlinear modeling with <em>nls</em> .., because the <em>nls</em> call in the code sample at the bottom of my question throws the following error:</p>

<p><code>Error in parse(text = x) : &lt;text&gt;:2:0: unexpected end of input
1: ~ 
   ^</code></p>

<p>which I could not find a solution for online concerning my case of a multivariate regression..</p>

<hr>

<p>My web-searches to my main question only gave me results concerning <em>multivariate <strong>linear</strong> regression</em>, which for example included <a href=""http://stats.stackexchange.com/questions/11127/multivariate-multiple-regression-in-r/11132#11132"">solutions with the manova function</a>..</p>

<p>Therefore, <strong>my question asked in a more general way:</strong> How do you in general solve such a non-linear multivariate multiple regression problem in R which takes into account interactions/dependencies between variables?</p>

<p>Here is my code where </p>

<ul>
<li>function <em>f</em> computes the rotations of coordinates about three axes
in the order x-axis, y-axis, and then z-axis (unfortunately I cannot
include the pic of the equation I wrote in LaTeX here since I haven't
got 10 reputation points yet);</li>
<li><em>rot_data_all</em> is structured as the data-subset above, just with more rows;</li>
<li>alpha1, alpha2 and so on are the parameters which nonlinear
regression should approximate:</li>
</ul>

<p>The code:</p>

<pre><code>f &lt;-function(x, y, z, alpha1, alpha2, alpha3, gamma, theta, phi, s) { 
      a &lt;- alpha1 + s*(cos(theta)*cos(phi)*x - cos(theta)*sin(phi)*y + sin(theta)*z)
      b &lt;- alpha2 + s*((sin(gamma)*sin(theta)*cos(phi) + cos(gamma)*sin(phi))*x 
                          + (-sin(gamma)*sin(theta)*sin(phi) + cos(gamma)*cos(phi))*y
                          - sin(gamma)*cos(phi)*z)
      c &lt;- alpha3 + s*((cos(gamma)*sin(theta)*cos(phi) + sin(gamma)*sin(phi))*x 
                          + (cos(gamma)*sin(theta)*sin(phi) + sin(gamma)*cos(phi))*y
                          + cos(gamma)*cos(phi)*z)
      return(c(a,b,c))
    }

    rot.nls &lt;- nls(cbind(a, b, c) ~ f(x, y, z, alpha1, alpha2, alpha3, gamma, theta, phi, s), 
                   data = rot_data_all, 
                   start = c(alpha1 = 0, alpha2 = 0, alpha3 = 0, gamma = 0.1, theta = 0.1, phi = 0.1, s = 0.1), trace = TRUE)
</code></pre>

<hr>

<p>I hope to find a solution which is general enough to also solve other transformations which cannot be easily linearized like the set of equations for <strong>projective transformation</strong>, i.e. something like the following function:</p>

<pre><code>f.proj &lt;-function(x, y, z, betas) {
  a &lt;- (betas[1,1]*x + betas[1,2]*y + betas[1,3]*z + betas[1,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  b &lt;- (betas[2,1]*x + betas[2,2]*y + betas[2,3]*z + betas[2,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  c &lt;- (betas[3,1]*x + betas[3,2]*y + betas[3,3]*z + betas[3,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  return(c(a,b,c))
}
</code></pre>

<hr>

<p>I am happy to provide more information if needed! Thank you so much!</p>
"
"0.0625848744250123","0.0727122510865616","142693","<p><strong><em>Is the following a reasonable illustration of the OVB problem?</em></strong></p>

<p>We build up fictional data around the regression line:</p>

<p>$$y = 7.2 + 2.3 \, x_1 + 0.1 \, x_2 + 1.5 \, x_3 + 0.013 \, x_4 + eps$$</p>

<p>by using this function:</p>

<pre><code>correlatedValue = function(x, r){
  r2 = r**2
  ve = 1 - r2
  SD = sqrt(ve)
  e  = rnorm(length(x), mean = 0, sd = SD)
  y  = r * x + e
}
</code></pre>

<p>-thank you, @gung for this post:
<a href=""http://stats.stackexchange.com/questions/38856/how-to-generate-correlated-random-numbers-given-means-variances-and-degree-of"">How to generate correlated random numbers (given means, variances and degree of correlation)?</a></p>

<p>And the following function, which generates four variables (<strong><em>x1</em></strong>, <strong><em>x2</em></strong>, <strong><em>x3</em></strong> and <strong><em>x4</em></strong>) as well as noise (<strong><em>eps</em></strong>). <strong><em>x1</em></strong> and <strong><em>x3</em></strong> are sample from normal distributions; <strong><em>x2</em></strong> is extracted from a uniform; and <strong><em>x4</em></strong> from a Poisson.</p>

<pre><code>variables &lt;- function(){
x &lt;- rnorm(1000)
x1 &lt;- 50 + 15 * x
x3 &lt;- 28 + 11 * correlatedValue(x = x, r = 0.6)
x2 &lt;- runif(1000, 0, 100)
x4 &lt;- rpois(1000,50)
eps &lt;- rnorm(1000,5, 7)
y = 7.2 + 2.3 * x1 + 0.001 * x2 + 1.5 * x3 + 0.013 * x4 + eps
dat &lt;- as.data.frame(cbind(y, x1, x2, x3, x4))
c &lt;- as.numeric(coef(lm(y ~ x2 + x3 + x4, dat))[3])
d &lt;- as.numeric(coef(lm(y ~ x1 + x2 + x3 + x4, dat))[4])
c(c,d)
}
</code></pre>

<p><strong><em>x1</em></strong> and <strong><em>x3</em></strong> are highly influential on <strong><em>y</em></strong> and are correlated with each other, setting the values up to observe <strong><em>OVB</em></strong>. <strong><em>x2</em></strong> and <strong><em>x4</em></strong> are less influential.</p>

<p>Here is the plotting of <strong><em>y</em></strong> against <strong><em>x1</em></strong>, <strong><em>x2</em></strong>, <strong><em>x3</em></strong>  and <strong><em>x4</em></strong>, and <strong><em>x1</em></strong> over <strong><em>x3</em></strong> with added regression lines:</p>

<p><img src=""http://i.stack.imgur.com/I4u0S.png"" alt=""enter image description here""></p>

<p>And following is the variance-covariance matrix:</p>

<pre><code>             y           x1           x2         x3          x4
y   1.00000000  0.944410945  0.014421682 0.77571067 -0.01463981
x1  0.94441094  1.000000000 -0.001726526 0.56504020 -0.03562991
x2  0.01442168 -0.001726526  1.000000000 0.03537959  0.02253922
x3  0.77571067  0.565040198  0.035379590 1.00000000  0.02573827
x4 -0.01463981 -0.035629906  0.022539218 0.02573827  1.00000000
</code></pre>

<p>Predictably, the regression including all variables shows similar coefficients to the initial equation:</p>

<pre><code>coef(lm(y~.,dat))[2:5]
         x1          x2          x3          x4 
2.253353226 0.004899445 1.547915198 0.017710038 
</code></pre>

<p>Wrapping up, a quick simulation is carried out to obtain the mean of the <strong><em>x3</em></strong> coefficient in 1,000 simulations <em>WITHOUT</em> including <strong><em>x1</em></strong> (""coef_x3"") and then <em>WITH</em> <strong><em>x1</em></strong> (""coef_x3_full""):</p>

<pre><code>coef_x3 &lt;- NULL
coef_x3_full &lt;- NULL
for (i in 1:1000){
  coef_x3[i] = variables()[1]
  coef_x3_full[i] = variables()[2]
}
mean(coef_x3)
mean(coef_x3_full)
</code></pre>

<p>obtaining a coefficient for <strong><em>x3</em></strong> of <strong>3.383</strong> when <strong><em>x1</em></strong> is excluded versus a coefficient for <strong><em>x3</em></strong> of <strong>1.502</strong> when included. So when <strong><em>x1</em></strong> is included we have an unbiased estimation of the true <strong><em>x3</em></strong> coefficient (<strong><em>1.5</em></strong>), whereas the estimation is biased when we exclude <strong><em>x1</em></strong>.</p>
"
"0.026822089039291","0.0272670941574606","143049","<p>I have heating power data from one year (8670 observations). I also have regressors for day length and temperature (8670 observations also). </p>

<p>I would like to add seasonality with 24h (1 day)  168h (1 week) periods to an ARMA model. Is there an effective way to construct a this kind of seasonal matrix (with day length and temperature data appended to; so a $8670\times(167+2)$ matrix). The application where I would use this is the <code>auto.arima</code> functions xreg argument.</p>
"
"0.0464572209811883","0.0472279924554862","143297","<p>Laplacian logistic regression. I have a training set of data and an evaluation set. The response is binary. I have to verify the models by calculating posterior predictive on the evaluation set. Last step compare the two models' predictive distribution variance.</p>

<p>First I trained the model using MCMCprobit() function from R.
How do I verify the correctness on the evaluation set? How do I calculate posteriors for each observation from the evaluation set?</p>
"
"NaN","NaN","143328","<p>I am developing a logistic regression model where perfect variable separation occurs. I want to calculate a cutoff from this data. Interestingly, the length of the slot <code>cutoffs</code> of <code>pred.obj</code> is only 5, as well as the slots <code>fp</code>, <code>tp</code>, <code>tn</code>, <code>fn</code>, <code>n.pos.pred</code> and <code>n.neg.pred</code>. I expect it to have the same length as the observations. </p>

<p>Has anybody an explanation for this? (And knows how to solve it?) </p>

<p>MWE:</p>

<pre><code> library(ROCR) # package for prediction/performance functions
 y &lt;- c(0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0)
 x &lt;- c(-5, 5, 3, -2, 4, 3, -8, 2, 5, 3, -5, -3, -2)
 model &lt;- glm(as.factor(y) ~ x, family = ""binomial"")
 preds &lt;- predict(model, type = ""response"")
 (pred.obj &lt;- prediction(preds, y))
 perf &lt;- performance(pred.obj, ""acc"")
 (cutoff &lt;- perf@x.values[[1]][which.max(perf@y.values[[1]])])
</code></pre>
"
"0.100359067582572","0.102024124270123","143399","<p>I am trying to do some survival analysis in R and as a starting point, I want to make sure I can replicate a previous analysis. I notice differences and I will demonstrate them here. I feel like there is a daft explanation the user community can provide.</p>

<p>Let's start by using the ovarian dataset in R. We will fit a weibull distribution with residual disease and ECOG performance status as covariates. Then we will print the output using proportional hazards specification to match Stata's HR output.</p>

<pre><code>require(survival)
require(flexsurvreg)
require(dplyr)
attach(ovarian)
ovarian &lt;- ovarian %&gt;% mutate(resid.ds=resid.ds-1, ecog.ps=ecog.ps-1, futime=futime/365.25) # Make it 0,1
write.dta(ovarian %&gt;% mutate(resid.ds=resid.ds+1, ecog.ps=ecog.ps+1), ""data/ovarian.dta"") # Write dta
s.weib &lt;- flexsurvreg(Surv(futime, fustat) ~ age + resid.ds + ecog.ps, data=ovarian, dist=""weibull"") # Fit weibull

# Function to convert AFT to PH
flexsurvPHcoef &lt;- function(x) return(c(exp(x$coef[-(1:2)]*(-1)*exp(x$coef[""shape""])), exp(x$coef[""shape""]), exp(-x$coef[""scale""])))
flexsurvPHcoef(s.weib)
     age     resid.ds      ecog.ps        shape        scale 
1.150309872 2.702038142 1.060599568 1.752446996 0.002799146 
</code></pre>

<hr>

<p>Now let's compare to Stata.</p>

<pre><code>quietly stset futime, f(fustat)
streg age i.resid_ds i.ecog_ps, d(weib)
Weibull regression -- log relative-hazard form 

No. of subjects =           26                     Number of obs   =        26
No. of failures =           12
Time at risk    =  42.67761807
                                                   LR chi2(3)      =     17.88
Log likelihood  =   -20.828884                     Prob &gt; chi2     =    0.0005

------------------------------------------------------------------------------
          _t | Haz. Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         age |    1.15031   .0515502     3.12   0.002     1.053583    1.255916
  2.resid_ds |   2.702038    2.00098     1.34   0.180     .6329054    11.53571
   2.ecog_ps |     1.0606   .6620784     0.09   0.925     .3120252    3.605066
       _cons |   .0000336   .0000906    -3.82   0.000     1.69e-07    .0066602
-------------+----------------------------------------------------------------
       /ln_p |   .5610131    .238929     2.35   0.019     .0927209    1.029305
-------------+----------------------------------------------------------------
           p |   1.752447   .4187103                      1.097156     2.79912
         1/p |   .5706307   .1363402                      .3572551    .9114478
------------------------------------------------------------------------------
</code></pre>

<p>We can see the resid.ds and ecog.ps are the same. As well, the shape. But the scale is off. </p>

<p><strong>So my question is, any thoughts on why only the scale parameter is different?</strong></p>

<hr>

<p>Let's move on to estimation. flexsurvreg has an interesting ability to predict at multiple time points. Let's assume a woman is 45, has residual disease and ECOG is 0.</p>

<pre><code>summary(s.weib, newdata=data.frame(age=45, resid.ds=1, ecog.ps=0), t=c(1:5))
age=45, resid.ds=1, ecog.ps=0 
  time       est         lcl       ucl
1    1 0.9517266 0.807744496 0.9930684
2    2 0.8464501 0.488174229 0.9681961
3    3 0.7122949 0.193976895 0.9285058
4    4 0.5702529 0.038982054 0.8762493
5    5 0.4358519 0.002925942 0.8097636
</code></pre>

<p>Not the most precise. Anyways, how does this compare to how weibull is parameterized (PH). From Stata's manual:
$$
S = exp(-exp(x{B}){t}^p)
$$</p>

<p>No we have to use the log scale coefficients. But let's go ahead and try and predict this manually.</p>

<pre><code>p.weib &lt;- function(cons, age, resid, t, p) return(exp(-exp(cons+age*45+resid)*t^p))
coef &lt;- flexsurvPHcoef(s.weib)
data.frame(time=1:5) %&gt;% mutate(S=p.weib(log(coef[""scale""]), log(coef[""age""]), log(coef[""resid.ds""]), time, coef[""shape""]), S=round(S, 5))
  time       S
1    1 0.01617
2    2 0.00000
3    3 0.00000
4    4 0.00000
5    5 0.00000
</code></pre>

<p>That obviously didn't really work out. What happens if we substitute the scale from Stata (-10.30166)?</p>

<pre><code>data.frame(time=1:5) %&gt;% mutate(S=p.weib(-10.30166, log(coef[""age""]), log(coef[""resid.ds""]), time, coef[""shape""]), S=round(S, 5))
  time       S
1    1 0.95173
2    2 0.84645
3    3 0.71230
4    4 0.57025
5    5 0.43585
</code></pre>

<p>It's just as flexsurvreg predicted. So now that I write this, maybe I've transformed the AFT scale incorrectly. Back to my original question, why are the scales different?</p>

<p>Finally, some aside questions. I couldn't reproduce my actual data problem. I can't really put up that much data here. Again, <code>summary.flexsurvreg</code> gives me predicted estimates that are not the same as the <code>p.weib</code>. But, when I substitute the scale from Stata for p.weib, I get estimates different to the original summary.flexsurvreg, albeit much closer than with the scale from my log(coef) of PH. Any thoughts?</p>
"
"0.053644178078582","0.0545341883149212","143943","<p>I have a need to do realtime predictions for individual rows of data based on a previously computed randomForest algorithm.  How can I run the ""predict"" command without recomputing ""fit"" on the entire training data set each time?  </p>

<p>I am using R and here's the line of code that computes ""fit"" by applying the randomForest algorithm on the training set.</p>

<pre><code>fit &lt;- randomForest(formula2, data=training, importance=TRUE, ntree=2000, na.action = na.omit)
</code></pre>

<p>And here's the predict command - I want to be able to run this without having to recompute fit every time.  Is this possible?</p>

<pre><code>outp_rf &lt;- predict(fit, testing)
</code></pre>

<p>For LogisticRegression, I know the coefficients so I can rerun the logistic function to compute the outcome.  However not sure how I can do it for RandomForest.</p>
"
"0.119952028780813","0.109747936952945","144515","<p>My first question here, hope it works :) I have experimental data $(x_i,y_i),i=1\ldots n$, $n=O(10)$, which describe a function $y=f(x)+\epsilon$ defined in $I=[0,1]$. $y_i$ is a ratio between measured positive quantities: $y_i=z_i/w_i$, where $0 &lt; z_i \leq w_i \forall i$. For these reasons, $f$ satisfies the following constraints:</p>

<ol>
<li>$f(0)=1$</li>
<li>$f(1)=1$</li>
<li>$ 0 &lt; f(x) \leq 1 \forall x \in I $ </li>
</ol>

<p>EDIT1: Experimental data should satisfy constraints, but in a few cases $y_i$ > 1 (by 3% in one case, and by much less in few other cases). That's weird because I was told this couldn't happen, because of the way measures are taken...I could follow this up with the data reduction team, but I don't think it makes a huge difference.</p>

<p>The true $f$ should look like this:</p>

<p><img src=""http://i.stack.imgur.com/ccVgy.png"" alt=""enter image description here""></p>

<p>I would like to fit my data with a model which ideally satisfies the constraints. The goal is to make predictions for values of $x \in I$ (it doesn't make any physical sense to consider values $x&lt;0$ or $x&gt;1$). </p>

<p>I started with a least squares polynomial regression. Of course, that doesn't satisfy the constraints, in particular constraint 3, which makes using the curve to make predictions quite risky. I thought of 3 possible strategies:</p>

<ol>
<li>fit a model such as $1-x(1-x)g(x)+\epsilon$. For $g(x)$ is continuous in $I$, this model satisfy constraints 1 and 2: I have no idea how to satisfy constraint 3, though, and also I have no idea which model to assume for $g(x)$.</li>
<li>as $y_i$= $y_i=z_i/w_i$, with $0 &lt; z_i \leq w_i \forall i$, it may make sense to fit a rational function to my data, i.e., assume model
$y=\frac{1+\sum\limits_{i=1}^n \alpha_ix^i}{1+\sum\limits_{i=1}^m \beta_i(1-x)^i} + \epsilon $ which satisfies constraints 1 and 2. Again, no idea how to satisfy constraint 3, which values to use for $n$ and $m$, etc. (very low, of course, as I have very few experimental data.).</li>
<li>a variation on strategy 2 may be to fit two separate functions $g(x)$ and $h(x)$ respectively to $(x_i,z_i)$ and $(x_i,w_i)$, and then just compute the ratio. Maybe each function is better behaved, and polynomial regression may be sufficient.</li>
</ol>

<p>What do you suggest me to do? Do you have any better ideas? Thanks!</p>

<p>EDIT2: I provide a picture of one of my fits (here the model is a second degree polynomial). I cannot show the $y=0$ line in the picture. The bounds are 95% pointwise prediction bounds for the fit.</p>

<p><img src=""http://i.stack.imgur.com/Suaau.png"" alt=""enter image description here""></p>

<p>EDIT3: The first drawing is taken from a book. Similar drawings are shown in other books. They are based partly on theory (the behavior for $x \to 0$ and $x \to 1$), and partly on experimental data which the books don't report. I don't have access to the original papers. My data sets can be considered in broad agreement. Some are closer than others, but since my case is not exactly the same than that reported in books, I believe the agreement is fair. However, my fit doesn't look at all like the ""expected"" $f(x)$, and that's my problem.</p>

<p>As I stated already, the goal of the fit is to make predictions of $f(x)$ for  $0&lt;x&lt;1$.        </p>
"
"0.053644178078582","0.0272670941574606","145083","<p>I am trying to convert a hierarchical model that currently works with the R package MCMCpack, into a JAGS version. I have experience working with regular models in JAGS, but not with hierarchical models.</p>

<p>With the MCMChregress function, my model translates into:</p>

<pre><code>mcmc_model &lt;- MCMChregress(fixed= Y ~ X1 + X2 + X3 + X4+ X5, random=~X1+X2, group=""L2"", data=mydata, burnin=10000, mcmc=10000, thin=1,verbose=1, r=3, R=diag(c(1,0.1,0.1)), nu=0.001, delta=0.001)
</code></pre>

<p>However, I am finding it difficult to write it down in JAGS. So far, I have done the following:</p>

<pre><code>jags_model &lt;- function() {

for (i in 1:N){ #first level model
Y[i] ~ dnorm(mu[i], tau.y)
mu[i] &lt;- b0[L2[i]] 
+ b1[L2[i]] * X1[i]
+ b2[L2[i]] * X2[i]
}
 for (j in 1:L){
b0[j] &lt;- xi.b0*B.raw[j,1]
b1[j] &lt;- xi.b1*B.raw[j,2]
b2[j] &lt;- xi.b2*B.raw[j,3]
B.raw[j,1:3] ~ dmnorm(B.raw.hat[j,], Tau.B.raw[,])
B.raw.hat[j,1] &lt;- g00 + g01*X3[j] + g02*X4[j] + g03*X5[j]
B.raw.hat[j,2] &lt;- mu.b1.raw
B.raw.hat[j,3] &lt;- mu.b2.raw
}


g00 &lt;- xi.b0*mu.b0.raw
g01 &lt;- xi.b0*mu.b0.raw
g02 &lt;- xi.b0*mu.b0.raw
g03 &lt;- xi.b0*mu.b0.raw
mu.b0.raw ~ dnorm(0,.0001)
mu.b1.raw ~ dnorm(0,.0001)
mu.b2.raw ~ dnorm(0,.0001)
xi.b0 ~ dunif(0,100)
xi.b1 ~ dunif(0,100)
xi.b2 ~ dunif(0,100)

Tau.B.raw[1:3,1:3] ~ dwish(W[,], df)
df &lt;- 4
Sigma.B.raw[1:3,1:3] &lt;- inverse(Tau.B.raw[,])
tau.y &lt;- pow(sigma.y, -2)
sigma.y ~ dunif(0,100)

}
</code></pre>

<p>However, although the fixed effects end up being the same with MCMChregress function in R and with JAGS, the random effects are very dissimilar. Especially the group-varying slopes (b1_j and b2_j). Do someone know how could I achieve such an specification that mimics MCMChregress?</p>

<p>Any suggestions will be very helpful.</p>

<p>EDIT: I am editing the question to follow a request made by @Patrick Coulombe in a comment below (since I cannot comment myself yet). So, to better understand the precise specification that runs in the background of MCMChregress, one could look here: <a href=""http://rgm3.lab.nig.ac.jp/RGM/R_rdfile?f=MCMCpack/man/MCMChregress.Rd&amp;d=R_CC"" rel=""nofollow"">http://rgm3.lab.nig.ac.jp/RGM/R_rdfile?f=MCMCpack/man/MCMChregress.Rd&amp;d=R_CC</a></p>

<p>My intention was to model the impact of X1 and X2 (variables of interest, both in the first level) on Y, and the extract the varying slopes for them in each of the L2 groups. The control variables X3, X4 and X5 are measured at the second level.</p>
"
"0.100359067582572","0.102024124270123","145684","<p>My problem (question at the end) is to calculate confidence interval (CI) (NOT prediction interval) of the response of a nonlinear model.</p>

<p>I am working with R but this question is not R-specific.</p>

<p>I want to model some data after the following equation (model):</p>

<p>Y ~ a * X^b/b</p>

<p>First, I estimate the parameters a and b through nonlinear regression (using R's ""nls()""), which yields estimates and error on the corresponding estimate.</p>

<pre><code> Nonlinear regression model
 model: Y ~ (A * X^B/B)
  data: data.frame(X = X, Y = Y)
    A      B 
  7.4154 0.6041 
   residual sum-of-squares: 88983
</code></pre>

<p>Then I calculate 95% CI for a and b (using confint(nlm &lt;- nls(Y ~  A * X^B/B, start=list(A=1,B=1)))</p>

<pre><code> &gt; confint(nlm)
 Waiting for profiling to be done...
         2.5%     97.5%
 A 1.21719414 11.549562
 B 0.08583486  1.482389
</code></pre>

<p>In order to calculate 95%CI for Y, given some fixed, certain value of X, my first idea was to propagate uncertainties on a and b to Y through the model equation. This yields some value for 95% CI of Y, given X.</p>

<p>I then came accross the ""propagate"" package that proposes to calculate 95%CI of Y, given X, ""based on asymptotic normality"" (citation from ""<a href=""http://127.0.0.1:22638/library/propagate/html/predictNLS.html"" rel=""nofollow"">http://127.0.0.1:22638/library/propagate/html/predictNLS.html</a>""). However this method yields a VERY different 95%CI. </p>

<p><strong>My question is: Why aren't these two CI equal ?</strong></p>

<p>A worked example (with some random equation that just crossed my mind):</p>

<p>Values needed for error propagation : A, CI(A), B, CI(B), X, CI(X) :</p>

<p>Parameters (A &amp; B)' estimates and 95%CI were calculated from 
     confint(nlm &lt;- nls(Y ~  A * X^B/B, start=list(A=1,B=1))</p>

<p>X was then fixed at 30 for the sake of the argument, and considered error-free.</p>

<pre><code>                A         B  X
 value   7.415380 0.6041404 30
 95% CI  5.166184 0.6982769  0
</code></pre>

<p>The general formula for uncertainties propagation (works for sd, se, ci95%) is :</p>

<p>Y=f(Ai | i = 1 to n)
=> delta(Y) = sqrt( sum( ( dY/dAi * delta(Ai) )^2 ) )</p>

<p>The equation being        Y =  A * X^B/B </p>

<p>Partial derivatives are then: </p>

<pre><code> dF/dA  =  X^B/B
 dF/dB  =  A * (X^B * log(X))/B - A * X^B/B^2
 dF/dX  =  A * (X^(B - 1) * B)/B
</code></pre>

<p>Then</p>

<pre><code> dF/dA = 12.9196498927581
 dF/dB = 167.269472901412
 dF/dX = 1.92930443474376
</code></pre>

<p>This yields</p>

<pre><code> Y = 95.8041099173585 +- 134.526084150286
</code></pre>

<p>However, when using the predictNLS() function from ""propagate"" R package:</p>

<pre><code> predictNLS(nlm, newdata=data.frame(X=30), interval = ""confidence"")$summary

 Propagating predictor value #1 ...
   Prop.Mean.1 Prop.Mean.2 Prop.sd.1 Prop.sd.2 Prop.2.5%
      95.80411    102.8339  20.89399  24.86949  51.89104
   Prop.97.5% Sim.Mean   Sim.sd Sim.Median  Sim.MAD  Sim.2.5%
     153.7767  93.5643 1712.894   97.85209 21.98703 -117.3541
   Sim.97.5%
    210.3916
</code></pre>

<p>Which yields</p>

<pre><code>    Y = 95.80411 +- (153.7767-51.89104)/2
 =&gt; Y = 95.80411 +- 50.94283
</code></pre>

<p>Obviously I must have missed / misunderstood some essential information about CI of response variable, because I believe the person who coded the predictNLS() function must be way more knowledgeable than me about it.</p>

<p>Thanks in advance for your explanations.</p>
"
"0.0464572209811883","0.0472279924554862","145799","<p>I was wondering why do I get linear model when I'm using exponential model,
<code>y = a * exp(-b*-x)</code>, to fit my data.</p>

<p>Here is my code:</p>

<pre><code>ff &lt;- function(x,a,b){a * exp(-b*-x)}
fit2 &lt;- nls(y ~ ff(x,a,b) , data = newdat, start =c(a=107.4623,b=-0.0037)
</code></pre>

<p>The graph below is mydata with the exponential fit (prediction of <code>fit2</code>) in purple curve. The green curve is what I though it would be, it is <code>Smooth.splines</code> fit.
<img src=""http://i.stack.imgur.com/qrUkj.png"" alt=""enter image description here""></p>

<p>Result from <code>fit2</code>:</p>

<pre><code>Nonlinear regression model
  model: dif2 ~ ff(age, a, b)
   data: newdat
         a          b 
109.743680  -0.003793 
 residual sum-of-squares: 2585

Number of iterations to convergence: 2 
Achieved convergence tolerance: 1.446e-06
</code></pre>

<p><img src=""http://i.stack.imgur.com/pdCrx.png"" alt=""enter image description here"">
Here is my data:</p>

<pre><code>   ID  x   y
    1 18 106.47
    1 19 100.35
    1 20 97.4
    1 21 101.03
    1 22 100.3
    1 23 99.06
    1 24 100.81
    2 18 101.95
    2 19 100.69
    2 20 100.89
    3 14 105.87
    3 15 107.44
    3 16 103.05
    3 17 104.86
    3 18 101.86
    3 19 101.48
    3 20 102.77
    3 21 99.63
    3 22 100.21
    3 23 101.28
    3 24 98.77
    3 25 99.91
    4 17 102.42
    4 18 101.85
    4 19 101.31
    5 18 101.24
    5 19 102.27
    5 20 100.03
    5 21 101.53
    6 20 98.08
    6 21 101.2
    6 22 103.16
    6 23 98.3
    6 24 102.21
    6 25 100.18
    6 27 95.28
    6 28 102.05
    6 29 100.72
    6 30 101.4
    7 13 111.3
    7 14 106.55
    7 15 103.23
    7 16 102.31
    7 17 101.11
    7 18 101.52
    7 19 100.14
    8 18 101.05
    8 19 98.15
    8 20 100.55
    8 21 101.62
    8 22 101.04
    8 23 98.22
    9 18 102.87
    9 19 101.46
    9 20 101.07
    9 21 101.32
    10 20 101.93
    10 21 101.73
    10 22 100.24
    11 19 99.75
    11 20 101.35
    11 21 99.34
    11 22 100.12
    12 18 103.34
    12 19 109.52
    12 20 106.98
    12 21 105.21
    12 22 98.87
    12 23 103.81
    12 24 100.38
    12 25 100.12
    12 26 99.7
    12 27 101.16
    12 28 99.02
    12 29 100.15
    12 30 97.32
    13 13 116.43
    13 14 111.75
    13 15 107.42
    13 16 103.5
    13 17 103.37
    13 18 100.66
    13 19 100.73
    13 20 100.84
    13 21 100.05
    14 18 101.66
    14 19 99.9
    14 20 101.4
    14 21 99.86
    14 22 100.82
    15 15 101.27
    15 16 100.01
    15 17 104.27
    16 19 100.26
    16 20 104.13
    17 18 106.12
    18 21 101.18
    18 22 99.51
    18 23 100.59
    19 18 100
    19 19 100.81
    19 20 99.37
    19 21 102.6
    20 22 102.18
    20 23 104.5
    20 24 100.74
    21 22 103.74
    21 23 98.66
    21 24 100.65
    21 25 99.63
    22 24 102.59
    22 25 94.62
    22 26 103.85
    23 20 100.7
    23 21 101.38
    23 22 102.36
    23 23 99.56
    23 24 100
    24 18 101.16
    24 19 99.64
    25 21 96.9
    25 22 109.3
    25 23 101.4
    25 24 98.04
    25 25 99.28
    25 26 99.63
    25 27 101.29
    25 28 100.08
    26 14 109
    26 15 112.37
    26 16 102.4
    26 17 102.15
    26 18 100.82
    27 18 101.14
    27 19 101.38
    28 17 105.09
    28 18 101.74
    28 19 100.2
    29 19 102.11
    29 20 100.57
    29 21 100.91
    29 22 99.61
    29 23 99.99
    30 18 99.81
    30 19 102.07
    31 19 100.75
    31 21 95.43
    32 23 99.73
    32 24 100.8
    32 25 100.1
    32 26 100.88
    32 27 97.73
    32 28 100.36
    33 22 99.4
    33 24 101.46
    33 18 97.65
    33 25 102.75
    33 26 97.7
    33 27 100.67
    34 21 98.27
    34 22 100.42
    34 23 101.16
    34 24 100.13
    34 25 98.55
    35 17 107.46
    35 18 100.22
    35 19 102.03
    35 20 101.52
    35 21 102.05
    35 22 102.46
    35 23 101.56
    35 24 96.88
    35 25 98.97
    35 26 101.68
    35 28 94.12
    36 20 98.63
    36 21 101.59
    36 22 98.76
    37 19 101.9
    37 20 98.66
    37 21 100.19
    37 22 100.03
    37 23 99.97
    38 15 104.32
    38 16 102.98
    38 17 103.4
    38 18 102.78
    38 19 101.73
    38 20 95.57
    39 22 101.5
    39 23 98.37
    39 24 100.4
    39 25 100.79
    40 19 102.93
    40 20 100.88
    40 21 99
    40 22 99.66
    41 21 107.08
    41 22 93.08
    41 24 100.91
    41 25 107.24
    41 26 99.8
    42 14 109.82
    42 15 106.09
    42 16 106.32
    42 17 102.8
    42 18 100.21
    42 19 102.08
    42 21 99.22
    42 22 100.13
    42 23 101.63
    43 16 100.95
    43 17 100.6
    43 18 101.81
    43 19 102.78
    43 20 98.43
    43 23 101.4
    43 24 103.12
    43 25 99.31
    43 26 100.47
    43 27 99.67
    43 28 98.75
    43 29 95.68
    44 23 103.78
    44 24 100.38
    44 25 99.39
    44 26 100.87
    44 27 99.64
    44 28 98.39
    44 29 97.62
    45 18 100.47
    45 19 101.41
    45 20 99.33
    45 21 101.08
    45 22 100.08
    45 23 100.22
    45 24 99.67
    45 25 100.45
    45 26 102.4
    45 27 95.7
    46 20 101.35
    46 21 98.73
    46 22 109.29
    46 23 100.04
    46 24 95.74
    46 25 100.44
    46 26 98.72
    47 19 100.51
    47 20 99.88
    47 21 101.7
    47 22 101.94
    47 23 100.72
    47 24 98.73
    47 25 102.16
    47 26 100.25
    47 27 95.1
    47 28 103.08
    48 25 105.21
    48 26 100.48
    48 27 98.07
    48 28 99.88
    48 29 95.61
    49 16 111.35
    49 17 92.43
    49 18 112.04
    49 19 100.8
    49 20 95.36
    49 21 103.13
    49 22 102.16
    49 23 98.81
    49 25 98.86
    49 26 99.93
    49 27 95.26
    50 23 98.15
    50 24 105.93
    50 25 99.01
    50 26 99.34
    50 27 93.68
    50 28 105.35
    51 24 100.96
    51 25 100.53
    51 26 99.2
    51 27 100.52
    51 28 100.86
    52 25 101.38
    52 26 98.45
    52 27 100.32
    52 28 99.24
    52 29 102.74
    53 24 101.37
    53 25 99.75
    53 27 96.31
    53 28 100.67
    54 22 98.09
    54 23 100.55
    54 24 100.25
    54 25 101.54
    54 26 98.48
    54 27 102.76
    54 28 98.5
    54 30 99.85
    55 22 103.87
    55 23 94.37
    55 24 105.12
    56 18 101.23
    56 19 99.26
    56 20 102.63
    56 21 100.75
    56 23 101.5
    56 24 99.14
    56 27 95.11
    57 16 107.57
    57 17 101.75
    57 18 107.18
    57 19 100.23
    57 20 105.48
    57 21 103.1
    57 22 100.45
    57 23 99.28
    57 24 100.52
    57 25 98.69
    58 27 103.13
    58 28 97.86
    58 29 101.33
    58 30 98.33
    58 32 102.14
    58 34 94.47
    58 35 98.29
    59 19 97.6
    59 20 98.93
    59 22 101.35
    59 23 93.88
    60 20 99.62
    60 22 97.36
    60 23 102.94
    60 24 98.98
    60 25 99.47
    61 18 100.15
    61 19 101.92
    61 20 101.34
    61 21 98.87
    61 22 97.68
    61 23 99.92
    61 24 100.78
    61 25 98.21
    62 20 102.7
    62 21 99.7
    62 22 100.17
    62 23 99.62
    62 24 100.59
</code></pre>
"
"0.139704292683836","0.132227496401667","145849","<p>Iâ€™ve got a question concerning the R package <em>strucchange</em> that I use for testing and dating structural breaks in my PhD thesis.  To be specific, I use the generalized fluctuation test framework with CUSUM/MOSUM and in particular Moving Estimates (<strong>ME</strong>) tests for my analysis. Thus, the following description focuses on the ME test, but in principle is more general to all fluctuation tests.</p>

<p><strong>The problem:</strong> I am testing time series data for structural breaks with the ME test that draws on the function <strong>efp</strong> provided by strucchange. Given the nature of time series data, I want to tackle potential heteroskedasticity and autocorrelation in the data. Strucchange provides some functionality with respect to calculating <em>heteroskedasticity</em> (<strong>HC</strong>) and <em>autocorrelation</em> (<strong>HAC</strong>) consistent covariance matrices,  e.g., the approaches suggested by Newey-West (1987) or Andrews (1991). </p>

<p>However, this functionality in strucchange is limited to the function <strong>gefp</strong> that calculates Generalized Empirical M-Fluctuation Processes that as far as I know does not allow to perform estimates-based tests such as the ME test. Thus, I cannot use <strong>efp</strong> to estimate ME tests (or other tests that are available in this function) using HAC covariance matrices. </p>

<p><strong>The question:</strong> Does anybody know how I could make use of the <strong>efp</strong> function in <em>strucchange</em> for testing and dating structural changes but use HAC covariance matrices to take heteroskedasticity and autocorrelation into account? Maybe there is some way to use the sandwich package for this?</p>

<p><strong>Many thanks for any help!</strong></p>

<p>Here is a minimal working example to show the problem</p>

<pre><code>library(foreign)
library(strucchange)

data(""Nile"")

#using the function efp to perform a moving estimates test
#assuming sperical disturbances
ocus.nile &lt;- efp(Nile ~ 1, type = ""ME"")
sctest(ocus.nile)

#applying the vcov function with the kernHAC option to take heteroskedasticity and autocorrelation does not work, i.e., the option is not used and the result is the same
sctest(ocus.nile, vcov=kernHAC)

#using the function gefp to perform a generalized M-fluctuation process however works with vcov
#assuming spherical disturbances
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm)
sctest(ocus.nile2)

#controlling for heteroskedasticity and autocorrelation using an appropriate covariance matrix changes the result, i.e. works
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm, vcov= kernHAC)
sctest(ocus.nile2)
</code></pre>

<p><strong>Some background</strong></p>

<p>Though probably not necessary, here is some more in-depth background about the problem for the interested reader (and the archive). The formulas are taken from Zeileis et al., 2005, â€Monitoring structural change in dynamic econometric modelsâ€. </p>

<p>The ME test is used to detect structural breaks in the standard linear regression model over time. What it does it in essence partitioning the data and rather than estimating the regression based on the whole sample, it sequentially moves â€œthroughâ€ time in a fixed-width windows containing only a sub-sample of the observations and in each window it estimates the model. These estimates are used to the computation of empirical fluctuation processes that capture fluctuations in regression coefficients and residuals over time. Significant fluctuations of the coefficients are signs of a structural break in the regression. The test statistic of the Moving estimates test is</p>

<p><img src=""http://i.stack.imgur.com/qQ0FY.png"" alt=""Moving estimates test statistic""></p>

<p>where <em>n</em> is the number of observations, <em>h</em> is the bandwith (how many percent of the total number of observations are used for the window), <em>nh</em> is thus the size of the window, Q_(n)=X_(n)^T X_(N)/n, i=[k+t(n-k)], and sigma^2 is an estimate of the variance. The way I understand the above statistic is that it compares the difference between the sub-sample estimate of beta with the whole sample (the window) estimate and how this difference develops over time. A zero difference would indicate a sub-sample estimate that perfectly equals the whole-sample estimate, which would indicate perfect stability of the coefficient. In my understanding, the efp function in strucchange calculates sigma^2 based on the standard OLS residuals u^ i.e., sigma^2=(1/n-k)âˆ‘_(i=1)^n u_i^2 . Thus, in the presence of heteroskedasticity or autocorrelation, the OLS assumption of spherical disturbances will be violated. Thus, ideally, sigma^2 should be estimated based on a HAC covariance matrix to avoid wrong inference.</p>

<p>The question that comes to my mind is whether there is a way to use the ME test based on a HAC estimate. If not, it seems to me that it is limited to spherical disturbances of residuals, which seems to be violated in most applications.</p>
"
"0.0599760143904067","0.0609710760849692","145949","<p>I'm doing a linear regression, in R. The values are like this -</p>

<pre><code>u  &lt;-  c(1,2,3,4,5,6,7,8,9,10)
v &lt;- c(21,22,23,24,25,26,27,28,29,30)
w &lt;- c(41,42,43,44,45,46,47,48,49,50)
y &lt;- c(128.2305,132.4040,140.1732,147.3236, 154.5410, 158.7206, 165.1761, 169.7121,178.9751,181.0309)
</code></pre>

<p>If I call linear regression function, it's returning a model, which is disregarding v and w.</p>

<pre><code>model &lt;- lm(y~u+v+w)

Coefficients:
(Intercept)            u            v            w   
    122.074        6.101           NA           NA  

summary(model)
</code></pre>

<p>Output: </p>

<pre><code>Call:
lm(formula = y ~ u + v + w)

Residuals:
       Min       1Q   Median       3Q      Max 
-2.05143 -0.92734  0.04845  0.73362  1.99357 

Coefficients: (2 not defined because of singularities)
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 122.0743     1.0197  119.72 2.65e-14 ***
u             6.1008     0.1643   37.12 3.04e-10 ***
v                 NA         NA      NA       NA    
w                 NA         NA      NA       NA    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.493 on 8 degrees of freedom
Multiple R-squared:  0.9942,    Adjusted R-squared:  0.9935 
F-statistic:  1378 on 1 and 8 DF,  p-value: 3.04e-10
</code></pre>

<p>I tried to fit a linear model before with different values of y,u,v (with two predictor variables, w was absent), and there also, v was being assigned NA, and only u was getting co-efficients. What's happening?</p>
"
"0.0379321620905441","0.0385614943639849","145977","<p>I am working on Boston data set from MASS library. I separated the training and test data (70 / 30)</p>

<p>In order to train my data, should I run linear regression multiple times on training data? Is this what training a dataset means? I'm using <code>lm</code> function in R to do this. Here is my code:</p>

<pre><code>smp_size &lt;- floor(.7 * nrow(Boston))

set.seed(133)
train_boston &lt;- sample(seq_len(nrow(Boston)), size=smp_size)
train_ind    &lt;- sample(seq_len(nrow(Boston)), size=smp_size)
train_boston &lt;- Boston[train_ind, ]
test_boston  &lt;- Boston[-train_ind,]
nrow(train_boston)   # [1] 354
nrow(test_boston)    # [1] 152
train_boston.lm &lt;- lm(lstat~medv, train_boston)
summary(train_boston.lm)
</code></pre>
"
"0.122914253319396","0.10710301967833","146421","<p>I am using rlm robust linear regression of MASS package on modified iris data set as follows:</p>

<pre><code>&gt; myiris = iris
&gt; myiris$Species = as.numeric(myiris$Species)
&gt; head(myiris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2       1
2          4.9         3.0          1.4         0.2       1
3          4.7         3.2          1.3         0.2       1
4          4.6         3.1          1.5         0.2       1
5          5.0         3.6          1.4         0.2       1
6          5.4         3.9          1.7         0.4       1

&gt; library(MASS)
&gt; rmod = rlm(Species~., data=myiris)
&gt; rmod
Call:
rlm(formula = Species ~ ., data = myiris)
Converged in 6 iterations

Coefficients:
 (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
  1.14943807  -0.11067690  -0.02603537   0.21581357   0.63793686 

Degrees of freedom: 150 total; 145 residual
Scale estimate: 0.191 
&gt; 
&gt; sumrmod = summary(rmod)
&gt; sumrmod

Call: rlm(formula = Species ~ ., data = myiris)
Residuals:
     Min       1Q   Median       3Q      Max 
-0.59732 -0.15769  0.01089  0.10955  0.56317 

Coefficients:
             Value   Std. Error t value
(Intercept)   1.1494  0.2056     5.5906
Sepal.Length -0.1107  0.0579    -1.9128
Sepal.Width  -0.0260  0.0599    -0.4346
Petal.Length  0.2158  0.0571     3.7821
Petal.Width   0.6379  0.0948     6.7287

Residual standard error: 0.1913 on 145 degrees of freedom
</code></pre>

<p>This does not give p.values so I calculated them as follows (using pt function of base R):</p>

<pre><code>&gt; dd = data.frame(sumrmod$coefficients)                             #$
&gt; dd$p.value =  pt(dd$t.value, sumrmod$df[2])                       #$
&gt; dd
                   Value Std..Error    t.value    p.value
(Intercept)   1.14943807 0.20560264  5.5905804 0.99999995
Sepal.Length -0.11067690 0.05786107 -1.9128044 0.02887227
Sepal.Width  -0.02603537 0.05991073 -0.4345693 0.33226054
Petal.Length  0.21581357 0.05706173  3.7821068 0.99988663
Petal.Width   0.63793686 0.09480869  6.7286751 1.00000000
</code></pre>

<p>However, these are not correct since ordinary lm function and other regression functions show that Petal.Length and Petal.Width are highly significant in this regression:</p>

<pre><code>&gt; summary(lm(Species~., data=myiris))

Call:
lm(formula = Species ~ ., data = myiris)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.59215 -0.15368  0.01268  0.11089  0.55077 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.18650    0.20484   5.792 4.15e-08 ***
Sepal.Length -0.11191    0.05765  -1.941   0.0542 .  
Sepal.Width  -0.04008    0.05969  -0.671   0.5030    
Petal.Length  0.22865    0.05685   4.022 9.26e-05 ***
Petal.Width   0.60925    0.09446   6.450 1.56e-09 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2191 on 145 degrees of freedom
Multiple R-squared:  0.9304,    Adjusted R-squared:  0.9285 
F-statistic: 484.5 on 4 and 145 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Where is the error? Am I not using correct method to calculate p.value here?</p>

<p>Edit: As suggested (further) by @Glen_b in the comments: </p>

<pre><code>&gt; dd$p.value =  2*pt(abs(dd$t.value), sumrmod$df[2], lower.tail=FALSE)      #$
&gt; dd
               Value Std..Error    t.value      p.value
(Intercept)   1.14943807 0.20560264  5.5905804 1.089792e-07
Sepal.Length -0.11067690 0.05786107 -1.9128044 5.774455e-02
Sepal.Width  -0.02603537 0.05991073 -0.4345693 6.645211e-01
Petal.Length  0.21581357 0.05706173  3.7821068 2.267410e-04
Petal.Width   0.63793686 0.09480869  6.7286751 3.691993e-10
</code></pre>

<p>These seem to be correct (finally).</p>
"
"0.0709645772411954","0.072141950116023","146431","<p>In the output of felm function which is a function for the Linear Models with Multiple Fixed Effects, two R-squared information are provided: Multiple R-squared(full model) and Multiple R-squared(proj model).</p>

<p>How to interpret the Multiple R-squared(proj model)? I guess the R-squared(full model) refer to the effect of all the variables (x1, f1, f2 and f3) in the model, in which f1, f2 and f3 serves like categorical variables in regression. And Multiple R-squared(proj model) refers to the effect of purely f1, f2 and f3 when x1 is not included in the model. Am I right? I need to understand this in order to calculate the effect size of the independent variable (x1 in this case). Thanks!</p>

<pre><code>   summary(est &lt;- felm(y ~ x1 | f1 + f2 + f3))

Call:
   felm(formula = y ~ x1 | f1 + f2 + f3) 

Residuals:
     Min       1Q   Median       3Q      Max 
-2.35266 -0.57436 -0.00792  0.61786  2.13316 

Coefficients:
   Estimate Std. Error t value Pr(&gt;|t|)    
x1   2.4043     0.1217   19.75   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    Residual standard error: 0.9832 on 76 degrees of freedom
    Multiple R-squared(full model): 0.9058   Adjusted R-squared: 0.8773 
    Multiple R-squared(proj model): 0.8369   Adjusted R-squared: 0.7876 
    F-statistic(full model):31.79 on 23 and 76 DF, p-value: &lt; 2.2e-16 
    F-statistic(proj model): 390.1 on 1 and 76 DF, p-value: &lt; 2.2e-16 
    *** Standard errors may be too high due to more than 2 groups and exactDOF=FALSE
</code></pre>
"
"0.0763370036711974","0.0776035104406608","146853","<p><strong>What I am doing so far:</strong></p>

<p>I am doing a constraint linear regression with R's <code>quadprog</code> package, function <code>solve.QP()</code>. The regression does not have an intercept $\alpha$, therefore the objective function can be stated by $$min_{b} (Y-Xb)^\top(Y-Xb)$$ which is the squared residuals. </p>

<p>Quadprog optimizes the function $$min_{b}\Big(\frac{1}{2}b^\top Db-d^\top b\Big)$$ Therefore I have to transform the first function into the second one. The end result is $$\frac{1}{2}b^\top X^\top Xb-Y^\top Xb$$ where $X^\top X =:D$ and $Y^\top X=:d$.</p>

<p>There are two risk factors in this example (hence Y is a vector of the dependent variable and X is a 2-dim matrix of the independent variables), whereas the first one is restricted to be greater or equal to -10 and the second one greater or equal to zero. The code for this is the following:</p>

<pre><code>require(""quadprog"")

Dmat = t(X) %*% X
Amat = t(diag(2))
bvec = c(-10,0)
dvec = t(Y) %*% X

solve.QP(Dmat = Dmat, dvec = dvec, Amat = Amat, bvec = bvec, meq = 0, factorized = F)
</code></pre>

<hr>

<p><strong>What I want to add:</strong></p>

<p>I want to add penalties to the regression in order to replicate a Lasso regression. Therefore, the initial objective function has to be expanded by the penalty term $\lambda |b|$ $$min_{b} (Y-Xb)^\top(Y-Xb)+\lambda |b|$$ 
In order to bring it into the form usable by the algorithm, I form it into $$\frac{1}{2}b^\top X^\top Xb-(Y^\top X-\frac{1}{2}\lambda)|b|$$
The penalty term $\lambda$ is a vector with two entries $\lambda=(80.56,5.65)$. However, when I now run the algorithm, the objective function gets negative and $b_1$ will be $b_1 = -10$, which is the most negative piossible value allowed by the constraints. $b_2$ will be $b_2=0$. </p>

<p>These results are not equal to results I get with the <code>glmnet</code> package which allows me to perform a Lasso regression with the same penalties. Those results have been checked and are correct. Hence, I do not know why the quadprog algorithm delivers different results. Any hints? Is the objective function wrong? Did I specify any input parameter for <code>quadprog</code> incorrectly? </p>
"
"0.0657004319817604","0.0667904674542028","147033","<p>I am trying to calculate standard errors of group means for a two-way-anova. I found two ways to do this (<code>predict.lm(, se = T)</code> and <code>summary.lm()</code>:</p>

<pre><code>set.seed(42234)
exmpl &lt;- data.frame(DV = rnorm(40) + rep(3:6 * 10, each = 10), #  Dependent Variable
                    IV1 = factor(rep(LETTERS[1:2], each = 20)), # Independent Variable (Treatment) 1 
                    IV2 = factor(rep(rep(LETTERS[3:4], each = 10), 2))) #  Independent Variable (Treatment) 2

exmpl.lm &lt;- lm(DV ~ IV1 + IV2, data = exmpl) #  Example data was generated without interactions

summary(exmpl.lm)
as.data.frame(predict(exmpl.lm, data.frame(IV1 = c('A', 'B', 'A', 'B'),
                                           IV2 = c('C', 'C', 'D', 'D')), se = T))
</code></pre>

<p>The standard errors of some group means differ. I managed to recalculate the standard errors given by <code>predict()</code> with the explanations from <a href=""http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf"" rel=""nofollow"">Practical Regression and Anova using R</a> from Faraway (section 3.5). I couldn't find any information about the algorithms used by the <code>summary()</code> function. Any ideas?
 What I find really confusing is that you can change the output by relabelling one factor:</p>

<pre><code>exmpl2 &lt;- exmpl
exmpl2$IV1 &lt;- factor(exmpl2$IV1, levels = LETTERS[2:1])

exmpl2.lm &lt;- lm(DV ~ IV1 + IV2, data = exmpl2)
summary(exmpl2.lm)
summary(exmpl.lm)
</code></pre>

<p>In the first example group A-C has a standard error of 0.2013. In the second example the standard error is given as 0.2324. The data of both examples is the same, only the order of the labels of a categorial (not ordinal) variable were changed. How does this influence the statistical model?</p>
"
"0.080466267117873","0.0727122510865616","147119","<p>I have a plot of residual values of a linear model in function of the fitted values where the heteroscedasticity is very clear. However I'm not sure how I should proceed now because as far as I understand this heteroscedasticity makes my linear model invalid. (Is that right?)</p>

<ol>
<li><p>Use robust linear fitting using the <code>rlm()</code> function of the <code>MASS</code> package because it's apparently robust to heteroscedasticity. </p></li>
<li><p>As the standard errors of my coefficients are wrong because of the heteroscedasticity, I can just adjust the standard errors to be robust to the heteroscedasticity? Using the method posted on Stack Overflow here: <a href=""http://stackoverflow.com/questions/4385436/regression-with-heteroskedasticity-corrected-standard-errors"">Regression with Heteroskedasticity Corrected Standard Errors</a></p></li>
</ol>

<p>Which would be the best method to use to deal with my problem? If I use solution 2 is my predicting capability of my model completely useless?</p>

<p>The Breusch-Pagan test confirmed that the variance is not constant.</p>

<p>My residuals in function of the fitted values looks like this:  </p>

<p><a href=""http://i.stack.imgur.com/OtlGC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/OtlGC.png"" alt=""http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png""></a> </p>

<p>(larger version)</p>

<p><img src=""http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png"" width=""600""></p>
"
"0.0709645772411954","0.0618359572423054","147170","<p>I have a plot of residual values of a linear model in function of the fitted values where the heteroscedasticity is very clear. However I'm not sure how I should proceed now because as far as I understand this heteroscedasticity makes my linear model invalid right?</p>

<p>So I have been doing some reading about this subjet and I found two suggestions in other stackoverflow threads.</p>

<p>1) Use robut linear fitting using the rlm() function of the MASS package because it's apparently robust to heteroscedasticity. </p>

<p>2) As the standard errors of my coefficients are wrong because of the heteroscedasticity, I can just adjust the standard errors to be robust to the heteroskedasticity? Using the method posted <a href=""http://stackoverflow.com/questions/4385436/regression-with-heteroskedasticity-corrected-standard-errors"">here</a></p>

<p>Which would be the best method to use to deal with my problem?
If I use solution 2 is my predicting capability of my model completely useless?</p>

<p>My residuals in function of the fitted values looks like this <a href=""http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png"" rel=""nofollow"">http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png</a> and the Breusch-Pagan test confirmed that the variance is not constant.</p>
"
"0.0709645772411954","0.072141950116023","147530","<p>I'm trying to perform a lagged linear regression on time series data sourced from ~10,000 hospital patients, for the purpose of estimating causal relationships between administration of a drug and a certain physiological response. For example: Do non-steroidal anti inflammatory drugs cause hypertension?</p>

<p>Basically, the linear model I'm trying to fit is like this:</p>

<p><img src=""http://i.stack.imgur.com/qukXs.png"" alt=""AR/cross-correlation model""></p>

<p>This assumes a maximum of 30 lags. $y$ represents hypertension, $x$ is taking the drug, and $h$ is whether the patient is admitted or not (a covariate). </p>

<p><strong>My question is this</strong>: Given a unique time series for <em>each patient</em> (all truncated to the same length of 30 time points), how can I pool all of the time series data together to estimate things like the cross-correlation (e.g., using <code>ccf</code>) and auto-correlation (<code>acf</code>) over the entire data set? If I were just trying to fit a linear model, this can be done relatively easily using something like the <code>plm</code> library, but I haven't been able to find anything similar for single functions.</p>

<p>For reference, here is a very small example of what my data set looks like (note that I only retained 6 of the 30 total time points for each patient, for brevity):</p>

<pre><code>   patient_id            time     nsaid hypertension admission
1           1               1 0.4427955    0.0000000 0.0000000
2           1               2 1.0000000    0.2097246 0.0000000
3           1               3 0.0000000    0.4916697 0.0000000
4           1               4 0.0000000    1.0000000 0.0000000
5           1               5 0.0000000    0.7902754 0.0000000
6           1               6 0.0000000    0.0000000 0.0000000
7           2               1 0.0000000    0.0000000 0.0000000
8           2               2 0.4104132    0.0000000 0.0000000
9           2               3 0.8236088    0.0000000 1.0000000
10          2               4 1.0000000    0.0000000 0.6994038
11          2               5 0.5895868    0.0000000 0.0000000
12          2               6 0.1763912    0.0000000 0.0000000
</code></pre>
"
"0.0663812836584521","0.0674826151369737","147619","<p>I'm trying to build a model that can predict streamflow for an alpine (snowmelt-fed) watershed using snow albedo (roughly, the energy reflectance of the snow) data. Albedo controls the melt of the snowpack, and higher albedo means slower melt, and vice versa. I have daily time-series data for both the snow albedo and streamflow, for 12 years from 2002-2013. The albedo time-series was obtained by spatially-averaging albedo data (raster files) from NASA's MODIS satellite.</p>

<p>I have tried various methods (simple regression, GLMs, GAMs, decision trees and random forests) to build the flow prediction model, but all of them fail because of the autocorrelated relationship between albedo and flow. Since the albedo is a snowpack property, there is a lag between it and the flow (related to snowmelt).</p>

<p>The Cross correlation function (CCF) between albedo and flow is shown below:</p>

<p><a href=""http://imgur.com/PpW1Kpy"" rel=""nofollow""><img src=""http://i.imgur.com/PpW1Kpy.png"" title=""source: imgur.com"" /></a></p>

<p>I have tried to include albedo lags of various days into the models, but I'm not able to mimic the distributed lag relationship between albedo and flow. I have tried to add precipitation, temperature and other climatic data to the predictors, but they don't seem to help. There are similar lagged and cross-correlation problems between these other predictors and flow.</p>

<p>The albedo, flow, precipitation and air temperature time-series are shown below:</p>

<p><a href=""http://imgur.com/Kb8Ta6q"" rel=""nofollow""><img src=""http://i.imgur.com/Kb8Ta6q.png"" title=""source: imgur.com"" /></a></p>

<p>Is there a statistical or machine learning technique in R that I can explore to build the albedo-streamflow model?</p>

<p>Thank you.</p>
"
"0.0709645772411954","0.072141950116023","147923","<p>I have a data set with continuous variable and a binary target variable (0 and 1). </p>

<p>I need to discretize the continuous variables (for logistic regression) with respect to the target variable and with the constrained that the frequency of observation in each interval should be balanced. I tried machine learning algorithms like Chi Merge, decision trees. Chi merge gave me intervals with very unbalanced numbers in each interval (an interval with 3 observations and another one with 1000). The decision trees were hard to interpret.</p>

<p>I came to the conclusion that an optimal discretization should maximise the $\chi^2$ statistic between the discretized variable and the target variable and should have intervals containing roughly the same amount of observations. </p>

<p>Is there an algorithm for solving this?</p>

<p>This how it could look like in R (def is the target variable and x the variable to be discretized). I calculated Tschuprow's $T$ to evaluate the ""correlation"" between the transformed and the target variable because $\chi^2$ statistics tends to increase with the number of intervals. I'm not certain if this is the right way.</p>

<p>Is there another way of evaluating if my discretization is optimal other than Tschuprow's $T$ (increases when number of classes decreases)? </p>

<pre><code>chitest &lt;- function(x){
  interv &lt;- cut(x, c(0, 1.6,1.9, 2.3, 2.9, max(x)), include.lowest = TRUE)
  X2 &lt;- chisq.test(df.train$def,as.numeric(interv))$statistic
  #Tschuprow
  Tschup &lt;- sqrt((X2)/(nrow(df.train)*sqrt((6-1)*(2-1))))
  print(list(Chi2=X2,freq=table(interv),def=sum.def,Tschuprow=Tschup))
}
</code></pre>
"
"0.0599760143904067","0.0609710760849692","147958","<p>As a result of linear regression, we can have its residual and see its plot to check whether it shows normal distributed or not as follows :</p>

<pre><code>library(car)
m1 &lt;- lm(mpg~disp+hp+wt+drat, data=mtcars)  #Create a linear model
resid(m1) #List of residuals
plot(density(resid(m1))) #A density plot
qqnorm(resid(m1)) # A quantile normal plot - good for checking normality
qqline(resid(m1))
</code></pre>

<p>We can have std for each parameters, however, it seems I could not find the source to create linear regression residuals for each parameters.</p>

<p>residualPlots function (in car package) is fine, but it would be nice if I could have its distribution or boxplot as well.</p>

<p>Let me know how I can plot each parameters in R.</p>
"
"0.053644178078582","0.0545341883149212","148529","<p>I am estimating a Weighted Spatial Simultaneous Autoregression Model (<code>spdep::spautolm</code> --> <a href=""http://cran.r-project.org/web/packages/spdep/spdep.pdf"" rel=""nofollow"">Link</a>) in R and I would like to do some residual analysis.</p>

<p>Unfortunately functions such as <code>hatvalues</code>,<code>cooks.distance</code> or <code>plot.lm</code> do not work for <code>spautolm</code> objects. Yet, I would like to calculate leverages and cook distances for my model (see also <a href=""https://stackoverflow.com/questions/29747519/how-to-do-residual-regression-deletion-diagnostics-on-spautolm-sarlm-objects/29756024#29756024"">my post on stackoverflow</a>).</p>

<p>My Model looks like this:</p>

<p>$Y = X^T\beta + \lambda W(Y âˆ’ X^T\beta)+ \epsilon$ with $\epsilon\sim^{iid} N(0,\sigma^2)$</p>

<p>Thus:</p>

<p>$Var[Y]=\Sigma_{SAR} = \sigmaÂ²(I-\lambda W)^{-1}V(I-\lambda W)^{-1}$ with $V=diag[1/n_i]$</p>

<p>$\rho$ is my spatial autoregression parameter and $W$ the matrix that represents spatial dependence.</p>

<p>Obviously, a simple calculation of the hatmatrix $H$ via $H=X(X^TX)^{-1}X^T$ is not valid here due to the weighting and the modeled spatial autocorrelation.</p>

<p><strong>Any ideas how to calculate the leverages and cook's distances by hand in R for this model?</strong></p>
"
"0.0464572209811883","0.0472279924554862","149004","<p>I'm trying to fit a Bayesian hierarchical poisson regression. To do so, I'm using MCMChpoisson function from MCMCpack in R. Based on this package, the model is:</p>

<p>$$Y_i \sim Poisson(\lambda_i)$$
$$\phi(\lambda_i) = X_i\beta + W_i \beta_i + \epsilon_i$$
$$\epsilon_i \sim N(0, \sigma^2 I_{k_i})$$
$$ \dots $$</p>

<p>In the model above, $\phi$ is the link function.</p>

<p>I skipped the rest of the model as only the parts of above are related to my question. My question is why they consider an measurement error ($\epsilon_i$) in the systematic component whereas in GLM we have a function of the mean; in other words, sampling from poisson will itself generate a measurement error. </p>

<p>Also, I think the extra $\epsilon_i$ term above causes me to get strange results. Does anyone know any other function/package in R to fit a model very similar to the model above with no measurement error in the systematic component.</p>

<p>Thanks very much for your help,</p>
"
"0.0464572209811883","0.0472279924554862","149091","<p>I have a Tobit model of the form:
\begin{align}
Y^*_i &amp;= X_i\beta + \epsilon_i  \\
Y_i   &amp;= \max(Y^*_i,0).
\end{align}</p>

<p>The regressors are one continuous variable and 30 factors (modelling seasonal effects during the day). In my sample there are among $3116$ observations $194$ left-censored and $2922$ uncensored.</p>

<p>I tried the R packages <code>AER</code> and <code>censReg</code>.  With <code>tobit()</code> in <code>AER</code> I get an error message:</p>

<pre><code>Error in solve.default(L %*% V %*% t(L)) : 
Lapack routine dgesv: system is exactly singular: U[2,2] = 0
</code></pre>

<p>Thus a first question: do I have too many factors? However <code>AER</code> has a predict function. Given the data $Y_i$ (non-negative) I would like to make a prediction of the censored data - $E[Y|Y&gt;0,X_i]$ - is this what the predict function of <code>AER</code> gives me?</p>

<p>How can I predict in the package <code>censReg</code>?</p>

<p>EDIT 1) as Achim Zeileis said, there is a data problem with my factors, they are 0 or NAN too often and the <code>tobit</code> algorithm cannot find a solution. I have to fix the data.</p>

<p>EDIT 2) Please look at the following example:</p>

<pre><code>library(AER)
N = 10
f = rep(c(""s1"",""s2"",""s3"",""s4"",""s5"",""s6"",""s7"",""s8""),N)
fcoeff = rep(c(-1,-2,-3,-4,-3,-5,-10,-5),N)
set.seed(100) 
x = rnorm(8*N)+1
beta = 5
epsilon = rnorm(8*N,sd = sqrt(1/5))
y.star = x*beta+fcoeff+epsilon ## latent response
y = y.star 
y[y&lt;0] &lt;- 0 ## censored response

fit &lt;- tobit(y~0+x+f)
summary(fit)

coef(fit) # very satisfying estimates

my.range = range(y, y.star, predict(fit))

plot(y, ylim=my.range)
lines(predict(fit), col=""red"")
lines(y.star, col=""blue"")
</code></pre>

<p>As Achim Zeileis writes <code>predict()</code> predicts $Y^*$ but how can we predict $Y$ efficiently and in an unbiased fashion?</p>
"
"0.053644178078582","0.0545341883149212","149140","<p>Currently I am working on a large data set with well over 200 variables (238 to be exact) and 290 observations for each variable (in theory). This data set is missing quite a lot of values, with variables ranging from 0-100% 'missingness'. I will eventually be performing logistical regression on this data, so of my 238 columns I will at most only be using ten or so.</p>

<p>However as almost all of my columns are missing some data, I am turning to multiple imputation to fill in the blanks (using the MICE package).</p>

<p>My question is; given that I have a large amount of variation in the missing data, at what percentage missing should I start to exclude variables from the mice() function? </p>

<p>Can mice function well with variables that are missing 50% of their values? What about 60%, 70%, 80%, 90%?</p>
"
"0.026822089039291","0.0272670941574606","149413","<p>I created a spreadlevel plot on my simple linear regression model in R. Here is my code,</p>

<pre><code>spreadLevelPlot(ols_reg)
</code></pre>

<p>where <code>ols_reg</code> is my regression model, <code>ols_reg &lt;- lm(y~0+.,dat)</code>. I first encountered spread-level plots from this <a href=""http://www.statmethods.net/stats/rdiagnostics.html"" rel=""nofollow"">link</a>, but I don't fully understand how to read the plot and what to do with the transformation this function provides. Suggested power transformation: 0.718 </p>

<p>This is how my plot looks,</p>

<p><img src=""http://i.stack.imgur.com/DEp1O.jpg"" alt=""enter image description here""></p>

<p>How do I read it?</p>

<p>These are the libraries I included in my script.</p>

<pre><code>library(car)
library(MASS)
library(lmtest)
</code></pre>
"
"0.0309714806541255","0.0314853283036575","149908","<p>The function <code>powerTranform</code> from the ""car"" package in R mentions the following code for Box-Cox transformation for multiple regression: </p>

<pre><code>summary(p1 &lt;- powerTransform(cycles ~ len + amp + load, Wool))
# fit linear model with transformed response:
coef(p1, round=TRUE)
summary(m1 &lt;- lm(bcPower(cycles, p1$roundlam) ~ len + amp + load, Wool))
</code></pre>

<p>Is it sensible to apply Box-Cox method to just the dependent variable (and not the whole formula) and proceed with the regression: </p>

<pre><code>library(fifer)
cycles = boxcoxR(cycles)
summary(m1 &lt;- lm(cycles ~ len + amp + load, Wool))
</code></pre>

<p>I suspect this method is not right but I am not sure.</p>
"
"0.026822089039291","0.0272670941574606","151121","<p>I am trying to run <code>netlogit</code> on a network of about 60,000 nodes, and I would like to know if the SNA package's functions are designed to support such large operations. I know, for instance, that with RSiena we are not supposed to go further than a few hundred nodes for the algorithms to converge in a reasonable time. But is this the case with SNA's regression models as well?</p>

<p>My initial experience is negative: When I run the commands on my network, the R process expands in memory until it fills both the live memory and the virtual memory and then it crashes (even with one repetition). Is this a matter throwing more resource at R, and is there a good way to calculate how much? Or are the current MRQAP algorithms simply not adapted to such networks? Any resources that can help determine the requirements?</p>
"
"0.167503892363236","0.170282948435482","151657","<p>I am running X-13 SEATS on r for monthly data in six years of observations and I think I got a (sufficiently) reasonable fit for the ARIMA model, but the output also shows me that my original series does not have significant seasonality, as it follows:</p>

<pre><code> Call:
seas(x = data_r[, 1], transform.function = ""log"", regression.aictest = NULL, 
    outlier = NULL, arima.model = ""(0 1 1)(1 1 0)"")

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
AR-Seasonal-12     -0.6194     0.1110  -5.581 2.39e-08 ***
MA-Nonseasonal-01   0.6220     0.1093   5.690 1.27e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 773.4, BIC: 778.4  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 20.04   Shapiro (normality): 0.9754
    &gt; 
                qs p-val
    qsori        0     1
    qsorievadj   0     1
    qsrsd        0     1
    qssadj       0     1
    qssadjevadj  0     1
    qsirr        0     1
    qsirrevadj   0     1
</code></pre>

<p>(Still, there is also the fact that the irregular component seems to dominate the SI ratio for some specific months in some years. So maybe there is some dummy variable in the pre-adjustment that I am missing (right?)) </p>

<p>But when I run a regression on Stata for yearly and monthly dummies on the original series -- assuming the seasonality is deterministic --, I cannot reject with an F test that they are all equal to zero. What does this show me? That my ARIMA fit is not correct?</p>

<p>Also, if someone could point me out the difference in interpretation that you should have when running a regression on seasonal dummies and deseasonalizing data with a X-13 SEATS, it would be also very helpful. Maybe that is what I am missing here.</p>

<p>Edit: is it by any chance a common practice, in some particular situations (when you are deseasonalizing a set of series), still deseasonalize a given series even if that series does not show significant seasonality?</p>

<p>Edit2: Adding the results of the automatic adjustment:</p>

<pre><code>Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
Constant            59.1761    38.0551   1.555  0.11994    
Easter[15]        -903.6151   341.1891  -2.648  0.00809 ** 
MA-Nonseasonal-01    0.4974     0.1138   4.370 1.24e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)  Obs.: 60  Transform: none
AICc: 925.6, BIC: 933.2  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.):  21.9   Shapiro (normality): 0.9498 *

            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1 
</code></pre>

<p>I also, I get the following error for the monthplot function with the automatic adjustment: </p>

<pre><code>Error in `[.default`(x$data, , ""seasonal"") : subscript out of bounds
</code></pre>

<p>Following this result from the automatic adjustment, the use of the dummy for easter, with the original specification, does not change that much the first output:</p>

<pre><code>Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
Easter[15]        -0.08307    0.02690  -3.088  0.00202 ** 
AR-Seasonal-12    -0.63353    0.10816  -5.858  4.7e-09 ***
MA-Nonseasonal-01  0.50391    0.12075   4.173  3.0e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 767.9, BIC: 774.3  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 29.37   Shapiro (normality): 0.9721  
            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1
</code></pre>

<p>Most recent observation: Now I Think I am fairly sure that there is no significant seasonality in this series, but I would be thankful if someone could show me other problems that I might not be considering. Still, I would like a possible canonical/scholarly answer on why I can reject the null hypothesis for the whole set of seasonal dummies being zero (though I had a small result for the F test with my data, ~4, but I still reject the null) and still get a reasonable ARIMA fit with which I cannot reject no seasonality in my original data. Does that have something to do with the difference of the adjustment with ARIMA models and deterministic seasonality? An intuitive answer on this difference would be of some help.</p>
"
"0.0657004319817604","0.0667904674542028","151915","<p>I've performed a logistic regression with L-BFGS on R and noticed that if I changed the initialization, the model retuned was different.</p>

<p>Here is my dataset (390 obs. of 14 variables, Y is the target variable) :</p>

<pre><code>GEST    DILATE    EFFACE    CONSIS    CONTR    MEMBRAN    AGE    STRAT    GRAVID    PARIT    DIAB    TRANSF    GEMEL    Y
31           3       100         3        1         2     26         3         1        0       2         2       1     1
28           8         0         3        1         2     25         3         1        0       2         1       2     1
31           3       100         3        2         2     28         3         2        0       2         1       1     1
...
</code></pre>

<p>This dataset is found here: <a href=""http://tutoriels-data-mining.blogspot.fr/2008/04/rgression-logistique-binaire.html"" rel=""nofollow"">http://tutoriels-data-mining.blogspot.fr/2008/04/rgression-logistique-binaire.html</a> in ""DonnÃ©es : prematures.xls"". Y is a column I created with the column ""PREMATURE"", Y=IF(PREMATURE=""positif"";1;0)</p>

<p>I've used the optimx package like here <a href=""http://stats.stackexchange.com/questions/17436/logistic-regression-with-lbfgs-solver"">Logistic regression with LBFGS solver</a>, here is the code: </p>

<pre><code>install.packages(""optimx"")
  library(optimx)

vY = as.matrix(premature['PREMATURE'])
# Recoding the response variable
vY = ifelse(vY == ""positif"", 1, 0)

mX = as.matrix(premature[c('GEST', 'DILATE', 'EFFACE', 'CONSIS', 'CONTR', 
                           'MEMBRAN', 'AGE', 'STRAT', 'GRAVID', 'PARIT', 
                           'DIAB', 'TRANSF', 'GEMEL')])

#add an intercept to the predictor variables
mX = cbind(rep(1, nrow(mX)), mX)

#the number of variables and observations
iK = ncol(mX)
iN = nrow(mX)

#define the logistic transformation
logit = function(mX, vBeta) {
  return(exp(mX %*% vBeta)/(1+ exp(mX %*% vBeta)) )
}

# stable parametrisation of the log-likelihood function
logLikelihoodLogitStable = function(vBeta, mX, vY) {
  return(-sum(
    vY*(mX %*% vBeta - log(1+exp(mX %*% vBeta)))
    + (1-vY)*(-log(1 + exp(mX %*% vBeta)))
  )  # sum
  )  # return 
}

# score function
likelihoodScore = function(vBeta, mX, vY) {
  return(t(mX) %*% (logit(mX, vBeta) - vY) )
}

# initial set of parameters (arbitrary starting parameters)
vBeta0 = c(10, -0.1, -0.3, 0.001, 0.01, 0.01, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01)

optimLogitLBFGS = optimx(vBeta0, logLikelihoodLogitStable,
                         method = 'L-BFGS-B', gr = likelihoodScore, 
                         mX = mX, vY = vY, hessian=TRUE)
</code></pre>

<p>I get this :</p>

<pre><code> optimLogitLBFGS
                p1         p2       p3         p4         p5         p6
L-BFGS-B 9.720242 -0.1652943 0.525449 0.01681583 0.02781123 -0.3921004
                 p7          p8         p9       p10        p11        p12
L-BFGS-B -1.694412 -0.03461208 0.02759248 0.1993573 -0.6718275 0.02537887
                 p13      p14   value fevals gevals niter convcode  kkt1  kkt2
L-BFGS-B -0.8374338 0.625044 187.581    121    121    NA        1 FALSE FALSE
          xtimes
L-BFGS-B  0.044
</code></pre>

<p>But if I change </p>

<pre><code>vBeta0 = c(10, -0.1, -0.3, 0.001, 0.01, 0.01, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01)
</code></pre>

<p>in</p>

<pre><code>vBeta0 = rep(0.1, iK)
</code></pre>

<p>I get a different result :</p>

<pre><code>optimLogitLBFGS
                 p1             p2             p3              p4               p5
L-BFGS-B 0.372672689046 0.206785276091 0.398104550108 0.0175008380158 -0.0460042719084
                 p6             p7               p8            p9            p10
L-BFGS-B 0.139760396213 -1.43192069477 -0.0207666651106 -1.1396642657 0.212186387416
                 p11             p12             p13            p14         value
L-BFGS-B -0.583698421298 0.0576485672766 -0.802789658686 0.993103617257 185.472518798
         fevals gevals niter convcode  kkt1  kkt2 xtimes
L-BFGS-B    121    121    NA        1 FALSE FALSE   0.05
</code></pre>

<p>How can I choose the initial parameters to get the best model?</p>
"
"0.0967084173462244","0.0983129061176287","151961","<p>*Please note this question is about the Platt probabilistic output and SVM class assignment, not about the code or the package itself. It just happens to be the code where I stumbled on the issue.</p>

<p>In <a href=""http://stats.stackexchange.com/questions/147260/including-class-probabilities-might-skew-a-model-in-caret"">another question</a> I asked about bad models coming from <code>caret</code> and associated <code>kernlab</code> when <code>prob.model=TRUE</code>. I found the answer myself, in both <a href=""http://stackoverflow.com/questions/29766951/different-results-with-caret-when-classprobs-true"">stackoverflow</a> and <a href=""http://r.789695.n4.nabble.com/Inconsistent-results-between-caret-kernlab-versions-td4680500.html"" rel=""nofollow"">from Max Kuhn himself</a>:</p>

<blockquote>
<pre><code>&gt; predict(newSVM, df[43,-1]) [1] O32078 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
&gt; predict(newSVM, df[43,-1], type = ""probabilities"")
     O27479     O31403     O32057    O32059    O32060     O32078
[1,] 0.08791826 0.05911645 0.2424997 0.1036943 0.06968587 0.1648394
     O32089     O32663     O32668     O32676
[1,] 0.04890477 0.05210836 0.09838892 0.07284396
</code></pre>
  
  <p>Note that, based on the probability model, the class with the largest
  probability is O32057 (p = 0.24) while the basic SVM model predicts
  O32078 (p = 0.16).</p>
  
  <p><strong>Somebody (maybe me) saw this discrepancy and that led to me to follow
  this rule:</strong></p>

<pre><code>if(prob.model = TRUE) use the class with the maximum probability   
  else use the class prediction from ksvm().
</code></pre>
  
  <p>Therefore:</p>

<pre><code>predict(svm.m1, df[43,-1])
 [1] O32057
 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
</code></pre>
</blockquote>

<p>Isn't that innacurate? <code>kernlab</code> searches for the optimal probability cutoff that minimizes error, that's why the assigned class and the maximum probability don't match: they don't have to.</p>

<p>Check this reproducible example. I excluded two cherrypicked <code>virginica</code> samples.</p>

<pre><code>require(kernlab);require(caret);
#kernel=polynomial; degree=3; scale=0.1; C=0.31
set.seed(101);SVM&lt;-ksvm(Species~., data=iris[-c(135,150),], kernel='polydot',C=.31, kpar=list( scale=.1, degree=3), prob.model=T)
</code></pre>

<p>Here's the resulting model </p>

<pre><code>&gt; SVM
Support Vector Machine object of class ""ksvm"" 

SV type: C-svc  (classification) 
 parameter : cost C = 0.31 

Polynomial kernel function. 
 Hyperparameters : degree =  3  scale =  0.1  offset =  1 

Number of Support Vectors : 58 

Objective Function Value : -1.4591 -0.7955 -10.2392 
Training error : 0.033784 
Probability model included. 
</code></pre>

<p>Now let's check the predicted class probabilities in those two samples</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5], type=""probabilities"")
          setosa versicolor virginica
[1,] 0.008286638  0.4414114  0.550302
[2,] 0.013824451  0.3035556  0.682620
</code></pre>

<p>And the class predictions</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5])
[1] versicolor virginica 
Levels: setosa versicolor virginica
</code></pre>

<p>Sample 150 was assigned to <code>virginica</code>, with a class probability of around 0.68. Sample 135 was assigned to <code>versicolor</code> with a probability of around 0.44, yet <code>virginica</code> probability nicely sits around 0.55.
Looking at several CV folds, we perceive that kernlab only assigns <code>virginica</code> when its probability is over a given value (way higher than 0.5). That's the cutoff I mentioned, and it happens thanks to the well known bad clustering in <code>iris</code> between <code>virginica</code> and <code>versicolor</code>.</p>

<p>So, am I right on these suppositions and therefore is <code>caret</code> class assignment model (maximum probability) wrong?</p>

<p>EDIT:
I've been experimenting with pairwise probability coupling of Platt scaling (logistic regression fit), isotononic regression and a model I'm working on. A weakness (?) I perceived in Platt's model is the probability isn't bound to be 0.5 when the binary SVM decision output is 0, which is the expected result as the instance would lie exactly on the separating hyperplane.</p>
"
"0.0479808115123254","0.0365826456509815","152012","<p>This might fit better here than on stackoverflow, I guess.</p>

<p>I was <a href=""http://stackoverflow.com/questions/30139874/r-dynamic-linear-regression-with-dynlm-package-how-to-predict"">trying to build a dynamic regression model with the dynlm</a> package, but it did not work out. After reading <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">this</a> by Hyndman, I now switched to an ARMAX model:</p>

<pre><code>y_t = a_1*x1_t + a_2*x2_t + ... + a_k*xk_t + n_t
</code></pre>

<p>where the error term follows an ARMA model</p>

<pre><code>n_t ~ ARMA(p,q)
</code></pre>

<p>So far I am using the function <code>auto.arima(y, xreg=cbind(x1, ..., xk))</code> from the <code>forecast</code>package, which is doing the job!</p>

<p>As a benchmark I am running a pure multiple regression with <code>lm()</code>, where I make use of the <code>step()</code> function to kick out non relevant variables (about 100 variables, from which 96 are dummies) to optimize the model according to <code>AIC</code>.</p>

<p>The in-sample forecasting for both models is more or less equal. As the ARMAX model always includes <strong>all</strong> independent variables <code>(x1, ..., xk)</code>, I am pretty sure that, if I could apply the <code>step()</code> function on it, I would achieve a further improvement here.</p>

<p>The problem is that the <code>step()</code> function does not work on <code>auto.arima()</code>?!</p>

<p>Do you have any suggestions how I could still do this? Or would I need a totally new approach?</p>

<p>(I have not provide a reproducible example, as this is a rather general question of which methods/functions/packages to use. If the question is not clear enough, please tell me and I will try to provide one)</p>
"
"0.0599760143904067","0.0609710760849692","152358","<p>I'm looking at two functions for testing goodness of fit for a logit regression model (glm).</p>

<p>Firstly anova(), used as </p>

<pre><code>anova(null model, test_model, test=""Chisq"")
</code></pre>

<p>This is pretty straightforward, as it provides the residual deviance of the two models.</p>

<p>The second, pchisq() test is where I'm getting confused, particularly around the 'TAIL' option of the function.</p>

<p>I'm using it like this..</p>

<pre><code>pchisq(test_model$deviance, df=test_model$df.residual, lower=FALSE)
</code></pre>

<p>It is my understanding that when using lower=FALSE (same as lower.tail=FALSE), our null hypothesis becomes <strong>'the model being tested is different from our null model'</strong>. </p>

<p>Therefore a low p-val from our pchisq() test (say 0.002) would provide support to reject the null hypothesis (p=0.01), ultimately stating that our test model is not significantly different than our null model.</p>

<p>Have I correctly interpreted the results of pchisq() when used with lower.tail=FALSE?</p>

<p>Edit - For refrence, including the 'help' portion of the arguments section of pchisq() function.</p>

<pre><code>lower.tail
logical; if TRUE (default), probabilities are P[X â‰¤ x], otherwise, P[X &gt; x].
</code></pre>
"
"0.0599760143904067","0.0487768608679754","152394","<p>I am using R's flexsurvreg function (in the flexsurv package) to fit a AFT model to my data. </p>

<p>This is the line of code that fits the model to the data:</p>

<pre><code>TestModel &lt;- flexsurvreg(Surv(time,death) ~ param1 + param2 + param3 + param4 + param5 + param6 + param7 + param8 + param9 + param10 + param11 + param12 + param13, data = DataTest, dist = ""weibull"")  
</code></pre>

<p>Once the model fits, this is a summary of the results:</p>

<pre><code>Estimates: 
        data mean     est        L95%       U95%       se         exp(est)   L95%       U95%     
shape      NA         9.99e-01         NA         NA         NA         NA         NA         NA
scale      NA         2.20e+02         NA         NA         NA         NA         NA         NA
param1     1.32e-01   2.51e-01         NA         NA         NA   1.29e+00         NA         NA
param2     1.61e-01  -1.54e-02         NA         NA         NA   9.85e-01         NA         NA
param3     1.89e-01  -4.68e-02         NA         NA         NA   9.54e-01         NA         NA
param4     1.76e-01  -2.25e-02         NA         NA         NA   9.78e-01         NA         NA
param5     1.87e-01  -5.35e-02         NA         NA         NA   9.48e-01         NA         NA
param6     7.56e-01  -2.74e-01         NA         NA         NA   7.60e-01         NA         NA
param7     2.28e-01   3.23e-02         NA         NA         NA   1.03e+00         NA         NA
param8     1.58e-01  -1.69e-02         NA         NA         NA   9.83e-01         NA         NA
param9     4.32e-01  -1.89e-02         NA         NA         NA   9.81e-01         NA         NA
param10    1.30e+02  -1.01e-03         NA         NA         NA   9.99e-01         NA         NA
param11    2.26e+01  -4.08e-03         NA         NA         NA   9.96e-01         NA         NA
param12    5.54e+02  -2.84e-04         NA         NA         NA   1.00e+00         NA         NA
param13    9.57e+01  -4.69e-03         NA         NA         NA   9.95e-01         NA         NA

N = 40320,  Events: 32154,  Censored: 8166
Total time at risk: 2584693
Log-likelihood = -171611.5, df = 15
AIC = 343253.1
</code></pre>

<p>I want to measure how the covariates affect the survival time. The estimates provide an understanding of this. Also, as I read <a href=""http://stats.stackexchange.com/questions/6026/how-do-i-interpret-expb-in-cox-regression"">here</a>, $exp(est)$ provides an estimate of how the hazard changes with change in 1 unit of a covariate by keeping the other covariates fixed. Is there a way I can calculate p-values for these covariates?</p>

<p>I have fitted a Weibull distribution to my dataset.</p>
"
"0.0599760143904067","0.0609710760849692","152868","<p>I have an administrative database with hospital readmissions (binomial: yes/no) and a couple of predictors. I've fitted a multilevel model with the function <code>glmer</code> from the package <code>lme4</code> to estimate the effect of these predictors on readmissions. 
The model has two levels: <code>hospital</code> and <code>patient</code>.
When I calculate the predicted probabilities (the chance of a readmission), and afterwards calculate the readmission ratio's for each hospital (by dividing the observed readmissions by the predicted readmissions), all my ratio's are around 1 which can't be correct.</p>

<p>Before I've calculated the predicted probabilities with a normal logistic regression, which gives more plausible ratio's (from 0,64 to 1,5)</p>

<p>This is my code to calculate predicted probabilities for the multilevel model:</p>

<pre><code>database$predprob &lt;- fitted(model1)
</code></pre>

<p>I've also tried this one, but it gives exactly the same predicted probabilities:</p>

<pre><code>database$predprob &lt;- predict(model1, newdata = database, type = ""response"", na.action = na.omit)
</code></pre>

<p>Does anybody know how to calculate predicted probabilities for a multilevel analysis? I suppose there must be another way to calculate it as my calculated ratios (observed/predicted) are all around 1.</p>
"
"0.0379321620905441","0.0385614943639849","153033","<p>I'm trying to create model for consumer loan defaults that incorporates individuals payment behavior as time series. Typically this kind of problem is modeled using Cox/Allen model.</p>

<p>Then, the other day, I came across this paper: <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CCQQFjAA&amp;url=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F4210-learning-patient-specific-cancer-survival-distributions-as-a-sequence-of-dependent-regressors.pdf&amp;ei=ojhbVb7gHoe4ggTTwoGYBA&amp;usg=AFQjCNHVEg6lOEFIV4WaZEwqnXwlez_wpA&amp;sig2=vA-EFCEDOzx2m1BGI_PTyg&amp;bvm=bv.93564037,d.eXY"" rel=""nofollow"">Learning Patient-Specific Cancer Survival
Distributions as a Sequence of Dependent Regressors</a></p>

<p>Basically authors propose to fit multiple logits with their own parameters at each time step  while enforcing smoothness of regression parameters over time. Also maximum likelihood objective function is modified such that model naturally enforces consistency of predicted event(i.e there is no come back after fatal event)</p>

<p>I was wondering if anyone is aware of R implementation of similar problem?</p>
"
"0.053644178078582","0.0545341883149212","153199","<p>I've used tslm() under the R-package fpp to analyse two time series, which seem similar:</p>

<pre><code>library(fpp)
a&lt;-ts(c(1,10,2,3,4,5,6,7,8,9,10,11,12,2,21,4,6,8,10,11,12,13,14,18), start = c(1959, 1), frequency=12)
b&lt;-ts(c(1,10,2,3,4,5,6,7,8,9,10,11,12,2,21,4,6,8,10,11,12,13,14,18), start = c(1959, 3), frequency=12)
</code></pre>

<p>However, the results of a simple time series regression </p>

<pre><code>summary(tslm(a~trend+season))
</code></pre>

<p>and </p>

<pre><code>summary(tslm(b~trend+season))
</code></pre>

<p>look different - why isn't this the same? How can the trend be the same, but the other results be different? I'd understand shifted seasonal results, but these are really different.</p>

<p>Idea: the function tslm() expects full years, thus for time series b months jan and feb '59 as well as the rest of 1960 are filled with values computed from the given data. But is that idea true?</p>
"
"NaN","NaN","153480","<p>I need to implement a SAR model with no covariates. To be more specific, the regression I have to estimate is y=bWy+e where: </p>

<ul>
<li>y is the dependent variable;</li>
<li>b is the coefficient parameter to be estimated;</li>
<li>W is the adjacency matrix;</li>
<li>e is the error.</li>
</ul>

<p>My idea was to use the lagsarlm function of the spdep package. But I've gone through spdep documentation and it seems that this function works only adding covariates: i.e. y=bWy+cX+e, and I don't know how to erase the X term.</p>

<p>Note: For those who are acquainted with network analysis literature and not with spatial econometrics, in a way this is a method to estimate a parameter for bonachich centrality.</p>
"
"0.0657004319817604","0.0667904674542028","153510","<p>I am trying to fit a regularized logistic regression to my data using glmnet. Using $\alpha=1$ I get a LASSO-regression, which is what I want. My problem is though that I don't know how the intercept is fitted. In glmnet one has the option to put <code>Intercept=TRUE</code> or <code>Intercept=FALSE</code>. As far as I understand <code>FALSE</code> sets my intercept to 0. When <code>TRUE</code>, I understood that the intercept was fitted as the mean of the $y$-values. Since my data is balanced binary data with values 0 and 1, $\bar{y}=0.5$, but my analysis gives me the value -2.6. </p>

<p>I read <a href=""http://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet"">How is the intercept computed in GLMnet? </a> but I don't understand it, so I hope someone will give some details. Also, in the link's article there is a likelihood function (13) and (14) on page 8 and I don't understand why it has $1/N$ in front.  </p>
"
"0.080466267117873","0.0818012824723818","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.100359067582572","0.0801618119265252","153761","<p>I don't have a lot of experience working with time series data. Now I have a 3 year, monthly data for several entities (you can think about them as different stores), that I would like to do some analysis, e.g. regression. I am not sure if there are trend and seasonality effects on these series.
Using the package <code>Forecast</code> in <code>R</code>, and applying the function <code>stl</code>, I decomposed the series and plotted them. I have attached some of the resulting plots (for different stores):</p>

<p><img src=""http://i.stack.imgur.com/np21E.jpg"" alt=""trend 2""></p>

<p><img src=""http://i.stack.imgur.com/ISKHJ.jpg"" alt=""trend 3""></p>

<p><img src=""http://i.stack.imgur.com/Rsxhw.jpg"" alt=""trend 4""></p>

<p>When I print the result of the fitted <code>stl</code> function, I get something like this: </p>

<pre><code> Call:
 stl(x = dlr1, s.window = ""period"")

 Components
            seasonal      trend   remainder
 Jan 2010 -0.05643233 -0.2151193 -0.02526416
 Feb 2010 -0.14799311 -0.2193160  0.13137861
 Mar 2010  0.10125889 -0.2235127  0.13747509
 Apr 2010 -0.29720645 -0.2266819 -0.47611165
 May 2010 -0.22746429 -0.2298511  0.28988090
 Jun 2010  0.12403035 -0.2320100  0.05470502
 Jul 2010 -0.10418880 -0.2341688  0.15684340
 Aug 2010  0.14560622 -0.2358294 -0.25225647
 Sep 2010  0.16699531 -0.2374901 -0.35570221
 Oct 2010 -0.21709617 -0.2402783  0.13772671
 Nov 2010  0.20363225 -0.2430665  0.19027750
 Dec 2010  0.30885826 -0.2444804 -0.11424289
 Jan 2011 -0.05643233 -0.2458944  0.16533482
 Feb 2011 -0.14799311 -0.2329029  0.09169095
 Mar 2011  0.10125889 -0.2199115 -0.33798701
 Apr 2011 -0.29720645 -0.2111018  0.58659147
 May 2011 -0.22746429 -0.2022921 -0.56724011
 Jun 2011  0.12403035 -0.2105492 -0.38534202
 Jul 2011 -0.10418880 -0.2188064  0.45324407
 Aug 2011  0.14560622 -0.2282670  0.15884344
 Sep 2011  0.16699531 -0.2377275  0.07834284
 Oct 2011 -0.21709617 -0.2372438  0.38658989
 Nov 2011  0.20363225 -0.2367601 -0.38545840
 Dec 2011  0.30885826 -0.2406277 -0.02293191
 Jan 2012 -0.05643233 -0.2444953 -0.14810135
 Feb 2012 -0.14799311 -0.2603740 -0.23092833
 Mar 2012  0.10125889 -0.2762527  0.19282561
 Apr 2012 -0.29720645 -0.2778357 -0.11752792
 May 2012 -0.22746429 -0.2794186  0.27095247
 Jun 2012  0.12403035 -0.2747109  0.32488093
 Jul 2012 -0.10418880 -0.2700031 -0.61519386
 Aug 2012  0.14560622 -0.2649596  0.08891071
 Sep 2012  0.16699531 -0.2599160  0.27346079
 Oct 2012 -0.21709617 -0.2550556 -0.52784826
 Nov 2012  0.20363225 -0.2501951  0.19201780
 Dec 2012  0.30885826 -0.2451385  0.13415736
</code></pre>

<p>Now based on these results, I am not sure how to decide whether there is a strong/weak trend and seasonality, and whether I should remove the trend and seasonality effects in order to build my regression model?
In another words I would like to know how to interpret the plots and the resulting trend and seasonality numbers, e.g. what does it mean in Dec 2012 when it says ""seasonal"" = 0.30885826, etc.?</p>

<p>Thanks</p>
"
"0.0379321620905441","0.0385614943639849","154044","<p>I can't use VARselect as it gives lags in a VAR model which considers all the variables to be endogenous. In my case, one of the variables is exogenous and affects dependent variable with a certain lag. </p>

<p>Is there any package or function in R which can help me find the optimal lag order of the exogenous regressor?</p>
"
"0.107288356157164","0.109068376629842","154112","<p>I have a dataset with more than 20 predictors and a single binary response variable. With only $n=181$ observations (64 deaths, 117 survivors), I decided to apply penalized logistic regression to modeling, with all predictors involved (so that I avoid problems associated with model selection). Nevertheless, I have to produce a ''simpler'' model too (i.e. one that is simple enough to be suitable for a nomogram-style hand calculation in clinical setting). For that end, I intend to use <code>rms</code>'s <code>fastbw</code>.</p>

<p>To exemplify my questions, I'll use the <code>support</code> dataset from <code>Hmisc</code>:</p>

<pre><code>library( rms )
getHdata( support )
fit &lt;- lrm( hospdead ~ rcs( age ) + sex + rcs( meanbp ) + rcs( crea ) + rcs( ph ) + rcs( sod ), data = support, x = TRUE, y = TRUE )
fit
</code></pre>

<p>First, I apply penalization:</p>

<pre><code>p &lt;- pentrace( fit, seq( 0, 10, by = 0.01 ) )
plot( p )
fitPen &lt;- update( fit, penalty = p$penalty )
fitPen
</code></pre>

<p>I hope I'm correct up to this point.</p>

<p>Next, I validate the model and calculate its calibration curve. If I understand it correctly, I shouldn't validate/calibrate the simpler model, rather, I have to run the necessary functions on the <em>original</em> model, but with <code>bw=T</code>. That is:</p>

<pre><code>validate( fitPen, B = 1000, bw = TRUE )
plot( calibrate( fitPen, B = 1000, bw = TRUE ) )
</code></pre>

<p><strong>Question #1</strong>: Am I correct in this? I.e. is it true that to get the simpler model's validation/calibration I have to run these not on the simpler model, but on the original one (with <code>bw=T</code>)? And the results will be those pertaining to the simpler model, despite the fact that I haven't run validation/calibration on the simpler model itself?</p>

<p>Next, I try to come up with the simpler model <em>explicitly</em>. Interestingly, <a href=""http://www.aliquote.org/cours/2011_health_measures/harrell98.pdf"" rel=""nofollow"">(Harrell, 1998)</a> uses a method which is based on calculating the logits for the observations, then modeling them with OLS, then narrowing this model with <code>fastbw</code>. Although it is surely my statistical shortcoming, I simply can't understand why this is necessary.</p>

<p><strong>Question #2</strong>: Why can't we <em>directly</em> use <code>fastbw</code> on the logistic regression model? Such as:</p>

<pre><code> fastbw( fitPen )
 fitApprox &lt;- lrm( as.formula( paste( ""hospdead ~"", paste( fastbw( fitPen )$names.kept, collapse = ""+"" ) ) ), data = support, x = TRUE, y = TRUE )
</code></pre>

<p>And finally, I am not completely sure on where should I apply penalizing in the whole process.</p>

<p><strong>Question #3</strong>: Should I penalize the original model, then run <code>fastbw</code> (see above), and then re-penalize the obtained model? I.e.</p>

<pre><code>p &lt;- pentrace( fitApprox, seq( 0, 10, by = 0.01 ) )
plot( p )
fitApproxPen &lt;- update( fitApprox, penalty = p$penalty )
fitApproxPen
</code></pre>

<p>Or I don't have to re-penalize the narrowed model? Or I don't have to penalize the original model and it is sufficient to penalize the simpler one? (I suspect that the very first option is the correct, but I'm not entirely sure.)</p>
"
"0.0379321620905441","0.0192807471819925","154578","<p>Link functions are typically sigmoid. The idea being that the underlying data is fitted to the curve with an increase in the predictor summation gives an increase in the response. However, could one use a link function that is Gaussian shaped? Such that there is a ""sweet spot"" with a trailing off on each side? </p>

<p>Doing more reading, I am asking if a glm can be used with a radial basis function similar to a radial basis function (RBF) network. You may ask why I wish for this? I need the model to be very interpretable. White box only, no black boxes allowed.</p>

<p><img src=""http://i.stack.imgur.com/nWmc1.png"" alt=""enter image description here""></p>

<p>I could address this situation by using logit logistic regression with polynomial expansion of the predictors or by introducing addition variables with a knot pivot.</p>

<p>I am just wondering if there was a more direct path.</p>
"
"0.100582833897341","0.109068376629842","154588","<p>I'm trying to build a covariance-based structural equation model (SEM) using both reflective and formative specifications of latent variables. I use the <code>sem</code> function in the <code>lavaan</code> package for estimation (R version 3.1.3, lavaan version 0.5-18). But estimates turn always out to be zero which is unreasonable.</p>

<p>The lavaan model syntax uses <code>=~</code> for reflective specification of latent variables, <code>&lt;~</code> for formative specification of latent variables, and <code>~</code> for regressions (<a href=""http://www.inside-r.org/packages/cran/lavaan/docs/model.syntax"" rel=""nofollow"">http://www.inside-r.org/packages/cran/lavaan/docs/model.syntax</a>). Here is a simple working example with only reflective specifications (it is a simplified version of the example provided at <a href=""http://lavaan.ugent.be/tutorial/sem.html"" rel=""nofollow"">http://lavaan.ugent.be/tutorial/sem.html</a> and by <code>example(sem)</code>)</p>

<pre><code>library(lavaan)
model &lt;- ' 
# latent variable definitions
ind60 =~ x1 + x2 
dem60 =~ y1 + y2
# regressions
dem60 ~ ind60
'
summary(sem(model, data=PoliticalDemocracy))
</code></pre>

<p>Now assume that based on prior theory I would know that dem60 is a formative construct composed of y1 and y2. Thus I change the specification from <code>=~</code> to <code>&lt;~</code> and obtain the following code</p>

<pre><code>library(lavaan)
model &lt;- ' 
# latent variable definitions
ind60 =~ x1 + x2 
dem60 &lt;~ y1 + y2
# regressions
dem60 ~ ind60
'
summary(sem(model, data=PoliticalDemocracy))
</code></pre>

<p>The estimates for both y1 and y2 turn out to be zero. Analogously, the regression effect of ind60 on dem60 turns out to be zero. What do I need to change to get a meaningful result?</p>

<p>Several websites and blogs suggested the following modifications:
(1) Fix one parameter in the formative construct, i.e. <code>dem60 &lt;~ 1*y1 + y2</code>.
(2) Allow for covariance of the manifest indicators, i.e. <code>y1 ~~ y2</code>. 
(3) Fix the variance of the formative construct, i.e. <code>dem60 ~~ 1</code>.
(4) Free the variance of the formative construct, i.e. <code>dem60 ~~ NA*dem60</code>. 
None of these are working. Again: What do I need to change to get a meaningful result?</p>
"
"NaN","NaN","155459","<p>I'm trying to predict the outcome ""Decision"" in the function of Age, Gender, Occupation, .... </p>

<p>The independent variable ""Occupation"" is known to be significant. But when I do the logistic model, each sub-group (modality) of it is not.</p>

<p>Should I regroup the levels having the same value of estimated coefficient? (which I guess doesn't make many sense because the levels are not statistically significant)</p>

<p>The variable Occupation has 74 different sub-groups.</p>

<p>And another problem is that when checking the multicollinearity, the function VIF in R doest work, it produces the NaN value, may be its due to the large number of sub-groups of Occupation.</p>

<p><img src=""http://i.stack.imgur.com/ruScu.png"" alt=""Summary(Logistic Regression)""></p>
"
"0.053644178078582","0.0409006412361909","155509","<p>Working on a linear regression problem in R, I created a first model </p>

<pre><code>flights_lm = lm(freq~dist+capa+nbrt+depf+lcco+prbi)
</code></pre>

<p>where freq is frequency, dist is distance, capa for capacity, nrt stands for number of roads, fuel is depf. I then started to remove variables which doesn't contribute to the model explination, and ended up with
 Coefficients:</p>

<pre><code>              Estimate Std. Error t value Pr(&gt;|t|)    
 (Intercept) 2.822e+02  6.239e+03   0.045  0.96400    
 nbrt        8.072e+01  6.515e+00  12.390  &lt; 2e-16 ***
 depf        3.052e-05  1.128e-05   2.704  0.00784 ** 
</code></pre>

<p>meaning that the frequency was well explained by nbrt and depf. Then I proceeded to remove points that affect the model (I guess they're called outliers). for that I used the R function <code>Cooks.distance</code> and couldn't get rid of all the points. each time I apply the function and plot it according to the model some other points pop out of the 0.65 limit that set for the model.
I have one doubt though, once I remove the points, Do I need to restart the process from the beginning (meaning creating the model with all the variables then delete each that p-value is the biggest until I get all the p values &lt;0.05)
or just delete the points and plot the model ?
Another question is :  could the function log help me with this case ?  </p>
"
"0.053644178078582","0.0545341883149212","156034","<p>I am dealing with a heteroscedastic censored dataset. I tried to use the survival analysis package in R to estimate a linear model for it. So before doing that, I conducted a simulation study, where I generate a sample for x:
$x  \sim Unif[-2,2]$
and y:
$y \sim 1+0.3x+0.6(1-0.3x)\epsilon $ where $\epsilon\sim N(0,1)$.
y is censored as $y=y$ if $y&lt;x$; $y=x$ otherwise.</p>

<p>Then I use the survreg function in R to estimate a linear model for it.</p>

<p><code>x&lt;-runif(10000,-2,2)</code></p>

<p><code>y&lt;-1+0.3*x+rnorm(length(x))*0.6*(1-0.3*x)</code></p>

<p><code>y[y&gt;=x]=x[y&gt;=x]</code></p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian')</code></p>

<p>The result is really poor, the estimate is around(intercept = 0.5, slope = 0.6) and it is very stable no matter what initial points I gave.</p>

<p>The result does not get significant improved even if I feed the true weight to it:</p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian', 'weights=1/(1-0.3x)^2')</code></p>

<p>But the estimation is great when no heteroscedasticity is presented:</p>

<p><code>x&lt;-runif(10000,-2,2)</code></p>

<p><code>y&lt;-1+0.3*x+rnorm(length(x))*0.6</code></p>

<p><code>y[y&gt;=x]=x[y&gt;=x]</code></p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian')</code></p>

<p>I also tried quantile regression for censored data with tau=0.5. (basically LAD regression which is proposed by J.Powell 1984). The result is still poor under this parameter setting.</p>

<p>Is there any good way to get a consistent estimation for heteroscedastic censored data?</p>

<p>Any suggestion on package, software or papers are welcome.</p>

<p>Thanks a lot.</p>
"
"0.0657004319817604","0.0667904674542028","156037","<p>The R Vars package has a Vector Auto Regression function called var. The arguments include (among other things) ""p"" defined as the ""Integer for the lag order"" and ""lag.max,"" which is defined as ""Integer, determines the highest lag order for lag length selection according to the choosen (sic) ic."" See cran.r-project.org/web/packages/vars/vars.pdf.</p>

<p>My questions are:</p>

<p>What is the definition of the ""lag order?""</p>

<p>I thought that ""lag.max"" meant the highest number of lags to consider, but the package documentation defines ""lag.max"" as the highest lag order. So, if the ""lag.max"" is the highest ""lag order"" then, clearly the ""lag order"" argument is not asking for the maximum ""lag order"" because that would be redundant. So, what is the argument for ""lag order"" asking for? ... the minimum lag order? ... the actual order of the lags, i.e., an order such as t-1, t-3, t-2 instead of t-1, t-2, t-3?</p>

<p>The definition of ""lag.max"" uses the term ""lag length."" What is that? I would have thought that it would also be the maximum number of lags to consider, but clearly from the context, that is not the case. So, what are the definitions of ""lag.max"" and ""lag length?""</p>

<p>Statistics would not be hard if statisticians would learn to define their terms!</p>
"
"0.0464572209811883","0.0472279924554862","156098","<p>The R function cv.glm (library: boot) calculates the estimated K-fold cross-validation prediction error for generalized linear models and returns delta. Does it make sense to use this function for a lasso regression (library: glmnet) and if so, how can it be carried out? The glmnet library uses a cross-validation to get the best turning parameter, but I did not find any example that cross-validates the final glmnet equation.</p>
"
"0.0709645772411954","0.072141950116023","156202","<p>I am working on a project where I am to do the intervention analysis and forecasting based on the time series. The problem is something like:</p>

<p><em>I have a normal time series entries but in between them some known event like natural calamities (storm, tornado) happens. I have the data for that and it affects the normal time series. Now my objective is to forecast the value of time series both in normal mode and also when I have a prediction of storm coming.</em></p>

<p>I have been reading <a href=""http://rads.stackoverflow.com/amzn/click/0471615285"" rel=""nofollow"">Forecasting with dynamic regression</a> chapter 7 about intervention analysis. I am also reading about the transfer function modeling. Can you please help me as in which model is good for this kind of time series analysis? Or may be some link which can guide me as how to do it? I will appreciate a link with some example in R or some examples.</p>

<p>EDIT: I guess I was not correct in description but I know the exact time information of all the previous storm events and I sort of want to find out the effect of storm intervention on the time series and I can forecast more closely if I know that there is a storm happening right now.</p>
"
"0.0599760143904067","0.0487768608679754","156275","<p>I have two paired samples following normal distributions N(0, $\sigma_1^2$) and N(0, $\sigma_2^2$). Samples represent estimation errors (residuals) of two linear regression models used to predict the same response variable using two different methods/independent variables. I have 30 pairs of residuals, so I would like to apply Wilcoxon signed-rank test to check whether means of absolute values or relative errors are different. Since absolute values do not follow normal distribution, I cannot use t-test or something similar.</p>

<p>I would like to find type II error and statistic power of Wilcoxon signed-rank test.
Is there some R function (or any other tool) that can be used? I have found a number of functions for testing the power of tests here <a href=""http://www.statmethods.net/stats/power.html"" rel=""nofollow"">http://www.statmethods.net/stats/power.html</a>  but Iâ€™m not sure could they be applied on Wilcoxon signed-rank test. If there is no built-in function is there some other tool or algorithm to manually calculate error?   </p>
"
"0.0379321620905441","0.0385614943639849","156564","<p>I just developed a logistic regression model predicting customer churn (i.e how likely is a customer to leave us in the future?)</p>

<p>To understand the impact of my independent variables I calculated Odds ratio using the following function. </p>

<p><code>exp(trainingmodel$coefficients)</code></p>

<p>Where trainingmodel is the name of my model.</p>

<p>And I get the following results:</p>

<pre><code>AIRTIME 
9.789127e-01

Site.Report.By.Vehicle1
1.241823e+00
</code></pre>

<p>Both AIRTIME and Site Report are a feature of product we offer. In my dataset, AIRTIME is a continuous variable whereas Site.Report.By.Vehicle1 is a categorical variable with just two levels, ie. someone using the Site Report or not?</p>

<p>Can someone please help me to understand how to interpret the above number for AIRTIME and Site Report?</p>
"
"0.0808716413062113","0.0904347204435887","156619","<p>I'm using the concept of Hedonic regression in order to model the prices for real estates. I'm having some trouble with my approach.</p>

<p><strong>What I have and what I do</strong></p>

<ul>
<li>my data consists out of real estates with following charcteristics: <code>price | livingArea | propertyArea | condoFloorNumber | roomCount | elevator | garage | quiet | etc.</code></li>
<li>I run a robust regression without intercept <code>lmRob(price ~ . -1)</code></li>
</ul>

<p><strong>What I want</strong></p>

<ul>
<li>a model with which I can predict the price of real estates, but which are not in the used data set</li>
<li>also it would be nice to have some constraints on the coefficients</li>
</ul>

<p><strong>Problems</strong></p>

<ul>
<li>very often I get bad values for the coefficients <code>ex: bathroomCount = -80000</code>. it's not possible that with a additive bathroom , the price of the house will sink with <code>80.000â‚¬</code></li>
<li><p>also I tried to use the function <code>pcls</code> in order to put some constraints on the coefficients, but this method gave very bad results. In the plot <code>Y = price</code> and <code>X = livingArea</code>. as you can see, the regression line isn't correct.
<img src=""http://i.stack.imgur.com/7PHp1.png"" alt=""enter image description here""></p>

<ul>
<li>another thought was to transform the regression problem into a maximization or minimization problem, but didn't managed to do it</li>
<li>also I tried to use different regression methods <code>lm, lmrob, ltsReg, MARS</code>, but they also give me bad coefficients. (sometimes this bad coefficients make a good price estimation)</li>
<li>I think that the big number of dummy variables damages a little bit the regression</li>
</ul></li>
</ul>

<p>Is my approach false?</p>

<p>Does someone have some hints, tricks for me? (<em>I'm not a statistician</em>)</p>

<p><strong>[UPDATE]</strong></p>

<p><img src=""http://i.stack.imgur.com/a2kLe.png"" alt=""price ~ livingArea""></p>

<p>This is how the plotted data looks like. LivingArea is the only non-dummy variable.</p>

<p><strong>[UPDATE 2]</strong></p>

<pre><code>y = bX 

     means

y = b_0*X_0 + b_1*X_1 + ... + b_k*X_k

     which is an equation system like this:

y[0] = b_0*X_0[0] + b_1*X_1[0] + ... + b_k*X_k[0]
.
.
.
y[n] = b_0*X_0[n] + b_1*X_1[n] + ... + b_k*X_k[n]
</code></pre>

<p>Did I got it right? </p>

<p>If so, isn't possible to add some inequality constraints equation to it. example:</p>

<pre><code>b_0 &gt;= 2000
b_2 &lt;= b_0/2
</code></pre>

<p><strong>[UPDATE 3]</strong></p>

<p>I'm running the regression without intercept, because if all the characteristics of a real estate = 0, then of course it'S price = 0. Nobody would pay for an apartment with 0mÂ².
<img src=""http://i.stack.imgur.com/LYPB0.png"" alt=""enter image description here"">
but it seems that the regression line where it was used an intercept (blue) looks far more better than the regression line without intercept (green). I can't understand why it is so. and why doesn't the regression line without intercept start at the point (0,0)?</p>
"
"0.0657004319817604","0.0667904674542028","157003","<p>I am using <code>glm (target, formula = target~.,family=binomial)</code> to predict binary outcome.</p>

<p>I have 9 grouped predictors.
I convert them into factors so that I can test them in the regression as dummies.</p>

<p>Additionally, I have initially set the reference group <code>relevel=var_name(var_name, ref = ""ref_group_name"")</code></p>

<p>I run the regression and the result is decent at first glance (good Gini, just a few dummies with high significance).</p>

<p>What I would like to do further is as follows:</p>

<p>1) check Mallows' Cp ( one thing to be checked for overfitting of the model that I know is that Mallows' Cp should be close to the number of dummies entering the model)</p>

<p>2) interactively add/remove dummies in the model</p>

<p>3) check correlation between dummies rather than between the whole variables</p>

<p>For the <strong>first</strong> question I am not sure how to test Mallows' Cp (summary of the <code>glm</code> result does not show it)?</p>

<p>For the <strong>second</strong> question I found package called <code>dummies(dummy.data.frame function)</code> that can easily convert factor variables into dummies so that I can interactively add or remove them (I found that potentially I can do that with package <code>leaps(update function)</code>)
What I am missing is when I create the dummies data frame with <code>dummy.data.frame</code> how can I select which is the <code>reference group</code> when I put the resulting data.frame into the glm function?</p>

<p>For the <strong>third</strong> question: does it now make sense to run Pearson/Spearman correlation on the dummies data.frame (note: initially variables are both numeric and categorical before they were grouped)</p>
"
"0.026822089039291","0.0272670941574606","157108","<p>I want to code up <a href=""https://www2.bc.edu/arthur-lewbel/simplenew14.pdf"" rel=""nofollow"">Lewbel's ""special regressor""</a> estimator in R, and I'd like to make it into something that others can use.  I can write simple functions, but I don't fully understand the guts of even simple estimators like <code>lm</code>.  (I understand them statistically, but there are lots of things that are just programming, like <code>match.dots</code> and <code>environment</code> and specifying <code>attributes</code> and such.)</p>

<p>Is there a good, applied introduction somewhere?  Ideally something that dissects a well-known function, explaining the steps and tools used?</p>

<p>(I recognize that this question would be better-suited to stackoverflow, but there is an explicit prohibition on asking about tutorials on that site, I'm not sure what terms to plug into google, and there are specific programming considerations when applied to statistical estimators.)</p>
"
"0.0657004319817604","0.0667904674542028","157186","<p>Let's say you're trying to fit a model to a dataset that includes categorical variables, group (A or B) and treatment (1, 2, 3 or 4).</p>

<p>In R, your model formula would be <strong>DV ~ group * treatment</strong> (DV stands for dependent variable) and your model output will look like this:</p>

<pre><code>(intercept)                 [...]
groupB                      [...]
treatment2                  [...]
treatment3                  [...]
treatment4                  [...]
groupB:treatment2           [...]
groupB:treatment3           [...]
groupB:treatment4           [...]
</code></pre>

<p>My question is how to interpret this kind of output. Below is what I believe is right for the interpretation of the main effects, and what puzzles me about the interaction parameters.</p>

<pre><code>(intercept)
</code></pre>

<p>This the reference value, i.e. for treatment 1 in group A.</p>

<pre><code>groupB
</code></pre>

<p>This is the difference between group A and group B for treatment 1 only.</p>

<pre><code>treatment2
</code></pre>

<p>This is the difference between treatment 1 and treatment 2, within group A only. It indeed still refers to the intercept value.
Same logic for the two following estimates (""treatment3"" and ""treatment4"").</p>

<pre><code>groupB:treatment2
</code></pre>

<p>Here is where I get puzzled. Is this testing if <strong>the difference between treatment1 and treatment2 is the same in groupB compared to groupA</strong>, or is it testing if if <strong>the difference between groupA and groupB is the same for treatment1 compared to treatment2</strong>.</p>

<p>I thought this question would be very basic, but I went through several R books with no luck and found inconsistent answers on here (see <a href=""http://stats.stackexchange.com/questions/87412/how-to-interpret-2-way-and-3-way-interaction-in-lmer"">http://stats.stackexchange.com/questions/87412/how-to-interpret-2-way-and-3-way-interaction-in-lmer</a> for support for the first idea and <a href=""http://stats.stackexchange.com/questions/33709/interpreting-the-regression-output-from-a-mixed-model-when-interactions-between"">http://stats.stackexchange.com/questions/33709/interpreting-the-regression-output-from-a-mixed-model-when-interactions-between</a> for the other way).</p>

<p>If that matters, I'm working with the glmer function of the lme4 package.</p>

<p>Thanks!</p>
"
"0.0657004319817604","0.0445269783028019","157777","<p>I am using the boot function in R to get standard errors for several statistics (I am doing a oaxaca blinder decomposition). My data (EU-SILC) has sample weights (PB040) for every observation. My understanding is that I have to use those weights for every OLS regression or other package I run to correct for sampling errors of the survey.</p>

<p>The thing is now that the boot package in R also can use sample weights.</p>

<p>Should I use the same sample weights for the call of the boot function and for the calculations ""inside"" the boot function?</p>

<p>My reason for this question is that since the boot function does a weighted draw from the original sample the function ""inside"" the boot-function should allready have a weighted sample. So using sample weights inside the boot function would be redundant or lead to a bias.</p>

<p>This is my first posting to cross validated and I am pretty new to bootstraping and econometrics so I hope I asked the question concise enough.</p>
"
"0.103881504159667","0.0985646681332268","157851","<p>In the <code>lmer</code> function within <code>lme4</code> in <code>R</code> there is a call for constructing a model matrix of random effects, $Z$, as explained <a href=""http://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf"" rel=""nofollow"">here</a>, pages 7 - 9.</p>

<p>Calculating $Z$ entails KhatriRao and/or Kronecker products of two matrices, $J_i$ and $X_i$.  </p>

<p>The matrix $J_i$ is a mouthful: ""Indicator matrix of grouping factor indices"", but it seems to be a sparse matrix with dummy coding to select which unit (for example, subjects in repetitive measurements) corresponding to higher hierarchical levels are ""on"" for any observation. The $X_i$ matrix seems to act as a selector of measurements in the lower hierarchical level, so that the combination of both ""selectors"" would yield a matrix, $Z_i$ of the form illustrated in the paper via the following example:</p>

<pre><code>(f&lt;-gl(3,2))

[1] 1 1 2 2 3 3
Levels: 1 2 3

(Ji&lt;-t(as(f,Class=""sparseMatrix"")))

6 x 3 sparse Matrix of class ""dgCMatrix""
     1 2 3
[1,] 1 . .
[2,] 1 . .
[3,] . 1 .
[4,] . 1 .
[5,] . . 1
[6,] . . 1

(Xi&lt;-cbind(1,rep.int(c(-1,1),3L)))
     [,1] [,2]
[1,]    1   -1
[2,]    1    1
[3,]    1   -1
[4,]    1    1
[5,]    1   -1
[6,]    1    1
</code></pre>

<p>Transposing each of these matrices, and performing a Khatri-Rao multiplication:</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp;. &amp;. &amp;. &amp;.\\.&amp;.&amp;1&amp;1&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;1&amp;1 \end{smallmatrix}\right]\ast \left[\begin{smallmatrix}\,\,\,\,1 &amp; 1 &amp;\,\,\,\,1 &amp;1 &amp;\,\,\,\,1 &amp;1\\-1&amp;1&amp;-1&amp;1&amp;-1&amp;1 \end{smallmatrix}\right]=
\left[\begin{smallmatrix}\,\,1 &amp; 1 &amp;.&amp;.&amp;.&amp;.\\\,\,\,\,-1 &amp;1&amp;.&amp;.&amp;.&amp;.\\ .&amp;.&amp;\,\,\,\,\,1 &amp;1&amp;.&amp;.\\.&amp;.&amp;\,\,-1&amp;1&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;\,\,\,1&amp;1\\.&amp;.&amp;.&amp;.&amp;-1&amp;1 \end{smallmatrix}\right]$</p>

<p>But $Z_i$ is the transpose of it:</p>

<pre><code>(Zi&lt;-t(KhatriRao(t(Ji),t(Xi))))

6 x 6 sparse Matrix of class ""dgCMatrix""

[1,] 1 -1 .  . .  .
[2,] 1  1 .  . .  .
[3,] .  . 1 -1 .  .
[4,] .  . 1  1 .  .
[5,] .  . .  . 1 -1
[6,] .  . .  . 1  1
</code></pre>

<p>It turns out that the authors make use of the database <code>sleepstudy</code> in <code>lme4</code>, but don't really elaborate on the design matrices as they apply to this particular study. So I'm trying to understand how the made up code in the paper reproduced above would translate into the more meaningful <code>sleepstudy</code> example.</p>

<p>For visual simplicity I have reduced the data set to just three subjects - ""309"", ""330"" and ""371"":</p>

<pre><code>require(lme4)
sleepstudy &lt;- sleepstudy[sleepstudy$Subject %in% c(309, 330, 371), ]
rownames(sleepstudy) &lt;- NULL
</code></pre>

<p>Each individual would exhibit a very different intercept and slope should a simple OLS regression be considered individually, suggesting the need for a mixed-effect model with the higher hierarchy or unit level corresponding to the subjects:</p>

<pre><code>    par(bg = 'peachpuff')
    plot(1,type=""n"", xlim=c(0, 12), ylim=c(200, 360),
             xlab='Days', ylab='Reaction')
    for (i in sleepstudy$Subject){
                fit&lt;-lm(Reaction ~ Days, sleepstudy[sleepstudy$Subject==i,])
            lines(predict(fit), col=i, lwd=3)
            text(x=11, y=predict(fit, data.frame(Days=9)), cex=0.6,labels=i)
        }
</code></pre>

<p><img src=""http://i.stack.imgur.com/opwVvm.png"" alt=""enter image description here""></p>

<p>The mixed-effect regression call is:</p>

<pre><code>fm1&lt;-lmer(Reaction~Days+(Days|Subject), sleepstudy)
</code></pre>

<p>And the matrix extracted from the function yields the following:</p>

<pre><code>parsedFormula&lt;-lFormula(formula= Reaction~Days+(Days|Subject),data= sleepstudy)
parsedFormula$reTrms

$Ztlist
    $Ztlist$`Days | Subject`
6 x 12 sparse Matrix of class ""dgCMatrix""

309 1 1 1 1 1 1 1 1 1 1 . . . . . . . . . . . . . . . . . . . .
309 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . . . . . . . . . . . .
330 . . . . . . . . . . 1 1 1 1 1 1 1 1 1 1 . . . . . . . . . .
330 . . . . . . . . . . 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . .
371 . . . . . . . . . . . . . . . . . . . . 1 1 1 1 1 1 1 1 1 1
371 . . . . . . . . . . . . . . . . . . . . 0 1 2 3 4 5 6 7 8 9
</code></pre>

<p>This seems right, but if it is, what is linear algebra behind it? I understand the rows of <code>1</code>'s being the selection of individuals like. For instance, subject <code>309</code> is on for the baseline + nine observations, so it gets four <code>1</code>'s and so forth. The second part is clearly the actual measurement: <code>0</code> for baseline, <code>1</code> for the first day of sleep deprivation, etc.</p>

<p><strong>But what are the actual</strong> $J_i$ <strong>and</strong> $X_i$ <strong>matrices and the corresponding</strong> $Z_i= (J_i^{T}âˆ—X_i^{T})^âŠ¤$ <strong>or</strong> $Z_i= (J_i^{T}\otimes X_i^{T})^âŠ¤$, <strong>whichever is pertinent?</strong></p>

<p>Here is a possibility,</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;.  &amp;. &amp;. &amp;. &amp;.&amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.\\
.&amp;.&amp;.&amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;.&amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;.&amp;.\\&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;.&amp;.&amp;.&amp;.&amp;.&amp;1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1\end{smallmatrix}\right]\ast \left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp; 1&amp;1&amp;1&amp;1 &amp; 1 &amp; 1 &amp; 1\\0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9 \end{smallmatrix}\right]=$</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\0 &amp; 1 &amp; 2 &amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;&amp;.&amp;.&amp;.&amp;.&amp;.&amp;1 &amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\ &amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;0 &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1\\&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9 \end{smallmatrix}\right] $</p>

<p>The problem is that it is not the transposed as the <code>lmer</code> function seems to call for, and still is unclear what the rules are to create $X_i$.</p>
"
"0.053644178078582","0.0545341883149212","157909","<p>I have a binary variable Y that is a dichotomization of an unknown latent variable, generated by a regression model with normal error. Therefore it makes sense to fit a probit model to Y.
R enables me to do so using the ""glm"" function.
I would also like to fit a probit model with LASSO penalty.
Using the function ""cv.glmnet"" I can fit a logistic regularized model to this problem, however I couldn't find a way to fit a probit regularized model in R.</p>

<p>My questions:</p>

<ol>
<li>Is there a simple way to fit a probit regularized model in R?</li>
<li>If not, can I fit a logistic one instead (as the probit and logit are quite similar)?</li>
</ol>

<p>Thanks!</p>

<p>Amit</p>
"
"0.0969560705490221","0.105605001571314","158366","<p>I'm trying to fit a multiple regression model with pairwise deletion in the context of missing data.  <code>lm()</code> uses listwise deletion, which I'd prefer not to use in my case.  I'd also prefer not to use multiple imputation or FIML.  How can I do multiple regression with pairwise deletion in R?</p>

<p>I have tried the <code>mat.regress()</code> function of the <code>psych</code> package, which fits regression models to correlation/covariance matrices (which can be obtained from pairwise deletion), but the regression model does not appear to include an intercept parameter.</p>

<p>Here's what I've tried (small example):</p>

<pre><code>set.seed(33333)
y &lt;- rnorm(1000)
x1 &lt;- y*2 + rnorm(1000, sd=.2)
x2 &lt;- y*5 + rnorm(1000, sd=.5)

y[sample(1:1000, 10)] &lt;- NA
x1[sample(1:1000, 10)] &lt;- NA
x2[sample(1:1000, 10)] &lt;- NA

mydata &lt;- data.frame(y, x1, x2)
covMatrix &lt;- cov(mydata, use=""pairwise.complete.obs"")

#Listwise Deletion
listwiseDeletion &lt;- lm(y ~ x1 + x2, data=mydata)
observations &lt;- length(listwiseDeletion$na.action) #30 rows deleted due to listwise deletion

coef(listwiseDeletion)
(Intercept)          x1          x2 
0.001995527 0.245372245 0.100001989

#Pairwise Deletion --- but missing intercept
pairwiseDeletion &lt;- mat.regress(y=""y"", x=c(""x1"",""x2""), data=covMatrix, n.obs=observations)
pairwiseDeletion$beta
       y
x1 0.1861277
x2 0.1251995

#Pairwise Deletion --- tried to add intercept, but received error when fitting model
mydata$intercept &lt;- 0
covMatrixWithIntercept &lt;- cov(mydata, use=""pairwise.complete.obs"")

pairwiseDeletionWithIntercept &lt;- mat.regress(y=""y"", x=c(""intercept"",""x1"",""x2""), data=covMatrixWithIntercept, n.obs=observations)
Something is seriously wrong the correlation matrix.
In smc, smcs were set to 1.0
Warning messages:
1: In cov2cor(C) :
  diag(.) had 0 or NA entries; non-finite result is doubtful
2: In cor.smooth(R) :
  I am sorry, there is something seriously wrong with the correlation matrix,
cor.smooth failed to  smooth it because some of the eigen values are NA.  
Are you sure you specified the data correctly?
</code></pre>

<p>So, how can I obtain an intercept parameter using <code>mat.regress</code>, or how can I obtain parameter estimates from pairwise deletion using another method or package in R?  I've seen <a href=""https://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re"">matrix calculations</a> to do this, but, ideally, there'd be a package that also outputs regression diagnostics, fit stats, etc.  Also, preferably, the method would be able to fit interaction terms.</p>
"
"0.0663812836584521","0.0771229887279699","158433","<p>I keep trying to perform parametric bootstrap on simple regression analysis to grasp the concept. The internet is full of tutorials on non-parametric one, but I found no explanation or steps concerning parametric bootstrap, so I did it on my own. Since I'm not sure if what was done is o.k., I kindly ask you to correct if I'm wrong (... or praise me if I'm right:) ).</p>

<pre><code>library(car)
library(boot)
attach(Anscombe) # I'm going to use Anscombe data

lm.out&lt;-lm(education~income, data=Anscombe) #simple regression to obtain coef.
regre.mle&lt;-coef(lm.out)
</code></pre>

<p>The model was built, so I was able to get sample $\sigma$ for errors</p>

<pre><code>mean(lm.out$resid) # 3.957485e-15
sd(lm.out$resid)   # 34.58725

regre.sim &lt;- function(data, mle){
  n &lt;- dim(data)[1]
  data$education &lt;- mle[2]*data$income+mle[1]+rnorm(n, mean=0, sd=34.58725)
  return(data)
}

regre.stat&lt;- function(data) {
  lm.out&lt;-lm(education~income, data=data)
  return(lm.out$coefficients)
}

boot.out&lt;-boot(Anscombe, statistic=regre.stat, R=100,
               ran.gen=regre.sim,
               sim=""parametric"", mle=regre.mle)

boot.ci(boot.out, type = ""basic"", index = 1) #for intercept
boot.ci(boot.out, type = ""basic"", index = 2) #for beta coef
</code></pre>

<p>Finally I get this:</p>

<pre><code>BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 99 bootstrap replicates

CALL : 
boot.ci(boot.out = boot.out, type = ""basic"", index = 2)

Intervals : 
Level      Basic         
95%   ( 0.0359,  0.0727 )  
Calculations and Intervals on Original Scale
Some basic intervals may be unstable
</code></pre>

<p>But the question is - this is it (parametric bootstrap on regression)? </p>
"
"0.053644178078582","0.0545341883149212","158578","<p>Do centered variables have to stay in matrix form when using them in a regression equation?</p>

<p>I have centered a few variables using the <code>scale</code> function with <code>center=T</code> and <code>scale=F</code>. I then converted those variables to a numeric variable, so that I can manipulate the data frame for other purposes. However, when I run an ANOVA, I get slightly different F values, just for that variable, all else is the same. </p>

<p>Edit:</p>

<p>What's the difference between these two:</p>

<pre><code>scale(A, center=TRUE, scale=FALSE)  
</code></pre>

<p>Which will embed a matrix within your data.frame</p>

<p>AND</p>

<pre><code>scale(df$A, center=TRUE, scale=FALSE)
    df$A = as.numeric(df$A)
</code></pre>

<p>Which makes variable A numeric, and removes the matrix notation within the variable? </p>

<p>Example of what I am trying to do, but the example doesn't cause the problem I am having:</p>

<pre><code>library(car)
library(MASS)
mtcars$wt_c &lt;- scale(mtcars$wt, center=TRUE, scale=FALSE)
mtcars$gear &lt;- as.factor(mtcars$gear)
mtcars1     &lt;- as.data.frame(mtcars)

# Part 1
rlm.mpg   &lt;- rlm(mpg~wt_c+gear+wt_c*gear, data=mtcars1)
anova.mpg &lt;- Anova(rlm.mpg, type=""III"")

# Part 2
# Make wt_c Numeric
mtcars1$wt_c &lt;- as.numeric(mtcars1$wt_c)
rlm.mpg2     &lt;- rlm(mpg~wt_c+gear+wt_c*gear, mtcars1)
anova.mpg2   &lt;- Anova(rlm.mpg2, type=""III"")
</code></pre>
"
"0.0464572209811883","0.0314853283036575","158701","<p>I use the svm function (for regression) to make forecast like I would with for exemple the arima function:<br>
<code>fit&lt;-auto.arima(ts)</code><br>
<code>prediction&lt;-forecast(fit,h=20)</code><br>
which returns different attributes : </p>

<blockquote>
  <ol>
  <li><code>prediction$mean</code> which is the actual prediction  </li>
  <li><code>prediction$lower</code> and <code>prediction$upper</code> which are the   <strong>boundaries of the confidence intervals</strong> on each points of the   <code>prediction$mean</code>.  </li>
  </ol>
</blockquote>

<p>I would like the <code>svm</code> function (from <em>e1071</em> package) to return a more detailed answer than just the value (like the <code>forecast()</code> would).<br>
 But I guess it is not implemented in the function yet.
Is there another function to do it ? Or should I use <strong>bootstrap</strong> methods to try to estimate those boundaries? And if I should use this are they pre-implemented version of them instead of using sample over a for loop which is very time-consuming ?</p>
"
"0.0599760143904067","0.0609710760849692","158817","<p>I am working on a multiple linear regression problem where I would like to constrain only some of the parameters to non-negative values.  There have been discussions of how to solve for the parameters posted on SE <a href=""http://stats.stackexchange.com/questions/136563/linear-regression-with-individual-constraints-in-r"">here</a> and on <a href=""http://www.magesblog.com/2013/03/how-to-use-optim-in-r.html"" rel=""nofollow"">another site</a>.  Both solutions used optim() or constrOptim() to minimize the residual sum of squares, which worked very well for me and gives the same values as lm() for the unconstrained version of the problem.</p>

<p>But I would like now to take this a step further and estimate standard errors of those parameters, similar to what I'd get if I used lm().  However, there is nothing in the optim() object that would suggest a route to estimating errors, and I haven't found anything in the control settings that would suggest a route either.  So I'm a little bit at a loss as to how to proceed.  My hunch is that this approach - an optimization problem that assumes the data values are fixed and the parameter solutions represent a global minimum - does not allow for error.  Is there any validity to that, or am I just missing something basic?</p>

<p>This is a small reproducible example, adapted from the example provided in the first link to work with the optim() function, rather than constrOptim().</p>

<pre><code>    min.RSS &lt;- function(data, par){
      with(data, sum((par[1]*x1 + par[2]*x2 - y)^2))
    }
    dat = data.frame(x1=c(1,2), x2=c(2,3), y=c(5,6))
    result = optim(par=c(0,1), min.RSS, data=dat, method=""L-BFGS-B"", lower=c(-Inf,0), upper=c(0,Inf))
    result$convergence
    result$par
</code></pre>

<p>Thanks for any guidance you can provide.</p>
"
"0.026822089039291","0.0272670941574606","158970","<p>I have estimated a regression model in R with two groups, included as factors. Their regression fits are visually quite different, but I would like to statistically test if they are different on a specific day (Day T).</p>

<p>I have used the predict function to obtain the fitted values and their standard errors on Day T. Can I use these values to perform a t-test, and if so, what degrees of freedom should I use?</p>
"
"0.053644178078582","0.0409006412361909","159261","<p>(<strong>EDIT</strong>: the question has been modified just a little bit to be more specific) </p>

<p>I want to fit a multivariate polynomial regression that accounts for measurement errors (an Error-in-Variables model). </p>

<p>As an example, my input data is like: </p>

<pre><code>y      sd_y      x       sd_x       z       sd_z

9.55   0.26     6.74     0.71      0.25     0.02
8.31   0.19     5.93     0.33      -0.40    0.05
...    ...      ...      ...        ...     ...   
</code></pre>

<p>where sd_y, sd_x, sd_z are the standard deviations of each variable, and </p>

<pre><code> wx &lt;- 1/(sd_x)**2 ; wy &lt;- 1/(sd_y)**2 ; wz &lt;- 1/(sd_z)**2 
</code></pre>

<p>would be the weights for each variable.</p>

<p>If I use a standard regression model (where predictors are supposed to have been measured exactly or without error) my function or fit, in R, would be:</p>

<pre><code>p &lt;- lm(y~polym(x, z, degree = 2, raw=TRUE))
</code></pre>

<p>Is there a method/package in R that allows to deal with ""error-in-variables models""? If so, how I would write my fit <em>p</em> when using the supposed package?</p>
"
"NaN","NaN","159299","<p>I would like to compute R-squared change for the interaction/moderation term in a multiple regression model, along with the corresponding F- and p-values. Previously, I have worked with the modprobe macro by A. Hayes, which can produce this for SPSS. As I am transitioning to R now, I am trying to find a function/package or a custom-made script in R that does this. In case it helps, my current interaction model looks like this:</p>

<pre><code>m1 &lt;- lm(all_ART~Neuroticism*Agreeableness+Attentional.Control, 
         data=stp2_sub2, na.action=na.omit)
</code></pre>

<p>Any pointers on how to compute these values (i.e., $R^2$(interaction), F-value(interaction) and p-value(interaction)) for the interaction term in R would be much appreciated!</p>
"
"NaN","NaN","159346","<p>I am fairly new to survival analysis and am playing around in R. I have a fairly simple cox model </p>

<pre><code>library(survival)
data(kidney)
cox&lt;-coxph(Surv(time, type)~delta, data=kidney)
baseline &lt;- basehaz(cox , centered=FALSE)
cox.survfit&lt;- survfit(cox)
plot(cox.survfit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/FIdvJ.png"" alt=""enter image description here""></p>

<p>My question is how do i calculate the survival rates myself without calling the survfit function. I tried to look at <a href=""http://stats.stackexchange.com/questions/36015/prediction-in-cox-regression"">this stackoverflow post</a> which i kind of understand but am not exactly able to turn it into code.  </p>

<p><img src=""http://i.stack.imgur.com/ei0To.png"" alt=""enter image description here""></p>

<p>I have the h0 from basehaz but i am not sure about rest of the calculations . Any ideas ?</p>

<p>EDIT:-
Added the kidney dataset in csv format(in case you don't have the same columns in your kidney datset ) :- </p>

<pre><code>""time"",""delta"",""type""
1.5,1,1
3.5,1,1
4.5,1,1
4.5,1,1
5.5,1,1
8.5,1,1
8.5,1,1
9.5,1,1
10.5,1,1
11.5,1,1
15.5,1,1
16.5,1,1
18.5,1,1
23.5,1,1
26.5,1,1
2.5,0,1
2.5,0,1
3.5,0,1
3.5,0,1
3.5,0,1
4.5,0,1
5.5,0,1
6.5,0,1
6.5,0,1
7.5,0,1
7.5,0,1
7.5,0,1
7.5,0,1
8.5,0,1
9.5,0,1
10.5,0,1
11.5,0,1
12.5,0,1
12.5,0,1
13.5,0,1
14.5,0,1
14.5,0,1
21.5,0,1
21.5,0,1
22.5,0,1
22.5,0,1
25.5,0,1
27.5,0,1
0.5,1,2
0.5,1,2
0.5,1,2
0.5,1,2
0.5,1,2
0.5,1,2
2.5,1,2
2.5,1,2
3.5,1,2
6.5,1,2
15.5,1,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
1.5,0,2
1.5,0,2
1.5,0,2
1.5,0,2
2.5,0,2
2.5,0,2
2.5,0,2
2.5,0,2
2.5,0,2
3.5,0,2
3.5,0,2
3.5,0,2
3.5,0,2
3.5,0,2
4.5,0,2
4.5,0,2
4.5,0,2
5.5,0,2
5.5,0,2
5.5,0,2
5.5,0,2
5.5,0,2
6.5,0,2
7.5,0,2
7.5,0,2
7.5,0,2
8.5,0,2
8.5,0,2
8.5,0,2
9.5,0,2
9.5,0,2
10.5,0,2
10.5,0,2
10.5,0,2
11.5,0,2
11.5,0,2
12.5,0,2
12.5,0,2
12.5,0,2
12.5,0,2
14.5,0,2
14.5,0,2
16.5,0,2
16.5,0,2
18.5,0,2
19.5,0,2
19.5,0,2
19.5,0,2
20.5,0,2
22.5,0,2
24.5,0,2
25.5,0,2
26.5,0,2
26.5,0,2
28.5,0,2
</code></pre>
"
"0.107288356157164","0.102251603090477","159647","<p>I've been studying (and applying) SVMs for some time now, mostly through <code>kernlab</code> in <code>R</code>.</p>

<p><code>kernlab</code> allows probabilistic estimation of the outcomes through Platt Scaling, but the same could be achieved with a Pool Adjacent Violators (PAV) isotonic regression (Zadrozny and Elkan, 2002).</p>

<p>I've been wrapping my head over this and came with a (clunky, but it works, or yet I think it does) code to try the PAV algorithm.</p>

<p>I divided the task into three pairwise binary classification task, estimated the probabilities on the training data and coupled the pairwise probabilities to get class probabilities (Wu, Lin, and Weng, 2004).</p>

<p>Predictions were made on the training set. I set the Cost really low <code>C=0.001</code> to try to get some misclassifications. </p>

<p>The Brier Score is defined as:</p>

<p>$$BS=\frac{1}N\sum_{t=1}^N\sum_{i=1}^R(f_{ti}-o_{ti})^2 $$</p>

<p>Where $R$ is the number of classes, $N$ is the number of instances, $f_{ti}$ is the forecast probability of the $t$-th instance belonging to the $i$-th class, and $o_{ti}$ is $1$, if the actual class $y_t$ is equal to $i$ and $0$, if the class $y_t$ is different from $i$.</p>

<pre><code>require(isotone)
require(kernlab)

##PAVA SET/VER
data1   &lt;-  iris[1:100,]        #only setosa and versicolor
MR1 &lt;-  c(rep(0,50),rep(1,100)) #target probabilities
KSVM1   &lt;-  ksvm(Species~., data=data1, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED1   &lt;-  predict(KSVM1,iris, type=""decision"")    #SVM decision function
PAVA1   &lt;-  gpava(PRED1, MR1)               #generalized pool adjacent violators algorithm 

##PAVA SET/VIR
data2   &lt;-  iris[c(1:50,101:150),]      #only setosa and virginica
MR2 &lt;-  c(rep(0,50),rep(1,50),rep(0,50))    #target probabilities
KSVM2   &lt;-  ksvm(Species~., data=data2, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED2   &lt;-  predict(KSVM2,iris, type=""decision"")
PAVA2   &lt;-  gpava(PRED2, MR2)

##PAVA VER/VIR
data3   &lt;-  iris[51:150,]   #only versicolor and virginica
MR3 &lt;-  c(rep(0,100),rep(1,50)) #target probabilities
KSVM3   &lt;-  ksvm(Species~., data=data3, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED3   &lt;-  predict(KSVM3,iris, type=""decision"")
PAVA3   &lt;-  gpava(PRED3, MR3)

#Usual pairwise binary SVM
KSVM    &lt;-  ksvm(Species~.,data=iris, type=""C-svc"", kernel=""rbfdot"", C=.001,prob.model=TRUE)

#probabilities on the training data through Platt scaling and pairwise coupling
PRED    &lt;-  predict(KSVM,iris,type=""probabilities"")

#The usual KSVM response based on the sign of the decision function
RES &lt;-  predict(KSVM,iris)

#pairwise probabilities coupling algorithm on kernlab
PROBS   &lt;-  kernlab::couple(cbind(1-PAVA1$x,1-PAVA2$x,1-PAVA3$x))
colnames(PROBS) &lt;- c(""setosa"",""versicolor"",""virginica"")

#Brier score multiclass definition
BRIER.PAVA  &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PROBS[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PROBS[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PROBS[101:150,])^2)/150

#Brier score multiclass definition
BRIER.PLATT &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PRED[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PRED[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PRED[101:150,])^2)/150

BRIER.PAVA

BRIER.PLATT
</code></pre>

<p>Soon I'll clean up a bit and write a proper wrapper function to do it all, but this result's really worrisome for me.</p>

<pre><code>BRIER.PAVA 
[1] 0.09801759
BRIER.PLATT 
[1] 0.6710232
</code></pre>

<p>The Brier Score I got from the probabilities estimated through PAVA is way better than the one we get on Platt Scaling.</p>

<p>If you check <code>PRED</code> you will see all probabilites fall on the ~0.33 range, while on <code>PROB</code> more extreme values (1 or 0) are expected, which was quite unexpected to me as I'm using a really low <code>C</code>.</p>

<p>References:</p>

<p><a href=""http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf"" rel=""nofollow"">Zadrozny, B., and Elkan, C. ""Transforming classifier scores into accurate multiclass probability estimates."" Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002.</a></p>

<p><a href=""http://papers.nips.cc/paper/2454-probability-estimates-for-multi-class-classification-by-pairwise-coupling.pdf"" rel=""nofollow"">T.-F. Wu, C.-J. Lin, and Weng, R.C. ""Probability estimates for multi-class classification by pairwise coupling."" The Journal of Machine Learning Research 5 (2004): 975-1005.</a></p>

<p>EDIT:</p>

<p>Also, if you check the AUC of the different probabilities, they are quite high.</p>

<pre><code>requires(caTools)

AUC.PAVA&lt;-caTools::colAUC(PROBS,iris$Species)

AUC.PLATT&lt;-caTools::colAUC(PRED,iris$Species)

colMeans(AUC.PAVA)
colMeans(AUC.PLATT)
</code></pre>

<p>And here's the result</p>

<pre><code>&gt; colMeans(AUC.PAVA)
    setosa versicolor  virginica 
 0.9988667  0.9988667  0.8455333 
&gt; colMeans(AUC.PLATT)
    setosa versicolor  virginica 
 0.8913333  0.8626667  0.9656000 
</code></pre>

<p>Looking at these AUC, I would say Platt Scaling is a really underconfident technique.</p>
"
"0.134209749501211","0.136436422649182","159745","<p>I want to test a regression model with neuroticism as focal predictor, agreeableness as moderator and RT variability as dependent measure (covariates: attentional control and mean RT). Previously, I have used the modprobe macro in SPSS by Andrew Hayes for this (see: <a href=""http://link.springer.com/article/10.3758%2FBRM.41.3.924"" rel=""nofollow"">http://link.springer.com/article/10.3758%2FBRM.41.3.924</a>). I am in the process of transitioning to R, however, and would like to learn how to run a similar routine there. I have set up my regression model as follows:</p>

<pre><code>m3&lt;-lm(data=stp2_sub2, all_SD~Neuroticism*Agreeableness+Attentional.Control+all_RT, na.action=na.omit) # full interaction model
m33&lt;-lm(data=stp2_sub2, all_SD~Neuroticism+Agreeableness+Attentional.Control+all_RT, na.action=na.omit) # reduced model
</code></pre>

<p>I know that I can obtain F-change and p-change, using:</p>

<pre><code>anova(m3, m33) # provides F-change and p-change
</code></pre>

<p>What I still donâ€™t know yet is how to obtain the R squared change value, which gives me the effect size of the interaction effect. I have already posted a similar question at one of the sister websites (<a href=""http://stats.stackexchange.com/questions/159299/computing-r-squared-change-f-and-p-values-for-the-interaction-moderation-te/159304?noredirect=1#"">http://stats.stackexchange.com/questions/159299/computing-r-squared-change-f-and-p-values-for-the-interaction-moderation-te/159304?noredirect=1#</a>), although it seems that my question was slightly off-topic there. The users there have been very helpful (especially @gung), but I still have some remaining questions. Hence me posting here.</p>

<p>Basically, the recommendations have been so far to compute (a) semi-partial r-squared or (b) partial eta squared. For (a) semi-partial r-squared a custom-written function already exists (see: <a href=""http://stats.stackexchange.com/questions/71816/calculating-effect-size-for-variables-in-a-multiple-regression-in-r/136615#136615"">http://stats.stackexchange.com/questions/71816/calculating-effect-size-for-variables-in-a-multiple-regression-in-r/136615#136615</a>). Unfortunately I am a bit at a loss as to how to exactly adapt it to my own case, particularly the following bit: y = 4 + .5*x1 - .3*x2 + rnorm(10, mean=0, sd=1). Moreover, even if I did succeed at adapting y to my own needs, I would still need to know how to compute the r change value. The function for semi-partial r-squared yields a single r value (i.e. it doesnâ€™t provide a change value, yet). In my case, would I need to, in a first step, run this function for (1) the full model (here: m3) as well as for (2) the reduced model (here: m33), thereby providing me with two semi-partial r-squared values (full vs. reduced)? In a second step, would I then subtract semi-partial r-squared (reduced) from semi-partial r-squared (full), with the outcome being the r-change value that I need?</p>

<p>As for b, I have been told to use the following Â« formula Â» ( SSE(reduced)-SSE(full) ) / SSE(reduced) to compute partial eta squared for the interaction effect. I have found code for computing the sum of squared errors (see: <a href=""http://stats.stackexchange.com/questions/49924/how-does-anova-lm-in-r-calculates-sum-sq"">http://stats.stackexchange.com/questions/49924/how-does-anova-lm-in-r-calculates-sum-sq</a>) and have applied this to my case as follows:</p>

<pre><code>SSEfull&lt;-sum(m3$residuals^2) # sum of squared errors/residual sum of squares for the full regression model
SSEred&lt;-sum(m33$residuals^2) # sum of squared errors/residual sum of squares for the reduced regression model

pes&lt;-(SSEred-SSEfull)/SSEred # computing partial eta-squared
</code></pre>

<p>I would like to know whether I have correctly implemented the code to compute partial eta squared for the interaction effect. Moreover, given that I need the r squared change value, I was wondering how I might be able to convert the partial eta squared value to the r squared change value that I need. That said, as mentioned above, I just need to know how to compute the r squared change value for my interaction model. Ultimately, I donâ€™t mind whether I do this following suggestions (a) or (b) (but note that I still have some questions in this regard) or use a completely different way. Indeed, perhaps thereâ€™s already a function/package in R that calculates the r change value (I havenâ€™t found one yet)? I feel like being very close to finding what I need, but just need a final few pointers. Apologies for the long-winded way of writing this simple question, but I thought it could be of use to show what thought (even if not yet conclusive) has already gone into this. Any guidance on how to best go about this would be much appreciated.</p>
"
"0.026822089039291","0.0272670941574606","159837","<p>I am trying to constructs an <a href=""https://en.wikipedia.org/wiki/Learning_vector_quantization"" rel=""nofollow"">Learning Vector Quantization (LVQ)</a> model.</p>

<pre><code>&gt; require(mlbench)
&gt; require(caret)
&gt; dput(dataSelection)
structure(list(c = c(142.8163942, 143.5711365, 145.3485827, 142.0577145, 
139.4326176, 140.1236581, 138.6560282, 136.405036, 133.9337229, 
133.8785538, 132.0608441, 130.0866307, 120.1320237, 119.6368882, 
114.3312943, 117.5084111, 114.4960017, 112.9124518, 112.8185478, 
112.3047916, 106.632639, 106.2107158, 106.8455028, 106.3879556, 
104.3451786, 102.9085952, 101.0967783, 101.7858278, 101.0749044, 
102.6441976, 102.0666152, 100, 97.14084104, 97.49972913, 96.91453836, 
96.05132443, 94.98057971, 92.78373451, 92.67526281, 91.82430571, 
91.4153859, 89.51740671, 89.01587176, 84.62259911, 91.48598494, 
89.12053042, 90.02364352, 90.92496121, 89.42963565, 91.93886583, 
88.83918306, 90.39513509, 87.54571761, 91.3386451, 87.7836994, 
91.79178376, 87.56903138, 87.77875755, 89.29938784, 90.88084014
), d = c(17703.7, 17599.8, 17328.2, 17044, 17078.3, 16872.3, 
16619.2, 16502.4, 16332.5, 16268.9, 16094.7, 15956.5, 15785.3, 
15587.1, 15460.9, 15238.4, 15230.2, 15057.7, 14888.6, 14681.1, 
14566.5, 14384.1, 14340.4, 14383.9, 14549.9, 14843, 14813, 14668.4, 
14685.3, 14569.7, 14422.3, 14233.2, 14066.4, 13908.5, 13799.8, 
13648.9, 13381.6, 13205.4, 12974.1, 12813.7, 12562.2, 12367.7, 
12181.4, 11988.4, 11816.8, 11625.1, 11370.7, 11230.1, 11103.8, 
11037.1, 10934.8, 10834.4, 10701.3, 10639.5, 10638.4, 10508.1, 
10472.3, 10357.4, 10278.3, 10031), e = c(71.0619, 70.9383, 71.162, 
71.138, 71.2286, 71.5095, 71.565, 71.3246, 71.4963, 71.3738, 
71.4276, 71.3065, 71.0246, 71.3244, 71.0619, 70.9811, 71.2149, 
70.8342, 70.5568, 70.5444, 70.3286, 70.179, 70.2555, 70.5103, 
70.8038, 70.6748, 70.9769, 70.6988, 70.2125, 70.1661, 69.6284, 
69.5613, 68.9837, 68.8606, 68.4223, 67.963, 67.6293, 67.5905, 
67.1857, 67.1248, 66.7075, 66.5857, 66.4303, 66.2826, 68.7514, 
68.8897, 69.0824, 68.9718, 68.7927, 68.6387, 68.8053, 68.7286, 
68.4141, 68.2357, 68.4785, 68.4171, 68.4782, 68.3978, 68.5344, 
68.4772), f = c(2160.080078, 2203.939941, 2500.850098, 2523.820068, 
2546.54, 2528.449951, 2223.97998, 2352.01001, 2401.21, 2089.73999, 
1975.349976, 2159.060059, 1891.68, 1947.849976, 2766.72998, 2882.179932, 
2947.24, 2541.629883, 2278.800049, 2634, 2495.56, 2637.280029, 
2098.649902, 1696.619995, 1750.83, 2767.76001, 3943.149902, 3765.909912, 
4512.98, 4527.299805, 4869.259766, 4645.5, 4463.47, 3868.27002, 
3745.719971, 4139.830078, 3667.03, 3457.449951, 3049.909912, 
2632.899902, 2431.38, 2042.869995, 1989.400024, 1866.76001, 1545.15, 
1351.890015, 1305.709961, 1163.109985, 1150.05, 1070.209961, 
1243.069946, 1289.16, 1140.36, 1084.069946, 1206.819946, 1186.540039, 
1073.3, 1161.160034, 1129.579956, 1130.069946), g = c(5.7393, 
5.7072, 5.6126, 5.6411, 5.5114, 5.4551, 5.1613, 5.4087, 5.0227, 
5.2039, 4.9501, 4.5008, 4.9143, 4.1372, 4.5604, 4.7979, 4.5454, 
4.8863, 5.0496, 4.9757, 5.4705, 5.8403, 5.4328, 4.6986, 4.4481, 
4.1385, 3.8379, 4.2183, 4.5429, 5.03, 5.1821, 4.8269, 5.0469, 
5.1054, 5.3959, 5.5413, 5.8139, 5.8611, 5.8396, 5.1964, 5.6386, 
5.6615, 5.5751, 5.2251, 4.4682, 4.262, 4.3487, 4.1654, 3.9651, 
3.9105, 3.7954, 4.1595, 3.8174, 3.6349, 3.6119, 3.4004, 3.366, 
3.3953, 3.3621, 3.9338), h = c(88.548662, 90.58853576, 91.32289522, 
91.56290683, 108.4682322, 93.86541244, 100.3414441, 91.98328561, 
95.53905246, 102.6461104, 97.9505881, 108.912959, 114.4931447, 
108.0431511, 98.58118608, 107.9440773, 99.41777306, 104.868483, 
100.3338425, 98.06667712, 100.6353811, 100.6491181, 106.4241282, 
79.3180456, 80.40781739, 85.35716451, 102.9110831, 88.99947733, 
99.38928861, 87.57579615, 87.49264945, 90.29013182, 92.13878645, 
90.15141711, 83.90950016, 97.24552675, 93.38024804, 94.16745797, 
98.90106448, 94.73366108, 104.1079291, 98.20132446, 97.70974526, 
91.86162897, 101.5381154, 94.56938821, 86.91581151, 87.16428746, 
87.35114009, 85.0634706, 86.2179337, 82.34156437, 79.86840987, 
84.20717658, 85.29553997, 90.94079268, 92.84823122, 88.90113767, 
88.05502443, 92.38787475), i = c(363.81, 361.19, 362.35, 359.09, 
359.31, 355.8, 356.64, 353.83, 353.49, 348.92, 348.8, 344.85, 
343.48, 340.75, 341.1, 335.72, 331.29, 328.21, 328.95, 325.92, 
324.83, 322.83, 323.18, 321.66, 322.94, 323.14, 322.89, 318.34, 
315.85, 311.61, 311.3, 308.34, 306.1, 305.64, 305.58, 302.91, 
301.64, 300.24, 299.54, 298.58, 296.4, 293.87, 293.35, 291.61, 
289.43, 288.03, 287.69, 287.6, 285.95, 284.8, 284.63, 282.62, 
281.24, 280, 280.09, 277.65, 275.73, 273.12, 272.78, 272.25), 
    j = c(307.5, 308.6, 308.9, 309.7, 311.1, 311.6, 311.6, 313.9, 
    314.9, 314.8, 314.9, 314.5, 313.4, 313, 312.9, 309, 304.5, 
    302.76, 299.28, 293.44, 291.52, 291.71, 290.61, 294.17, 297.74, 
    300.02, 295.91, 292.9, 289.23, 287.49, 285.86, 283.84, 281.1, 
    280.37, 278.63, 275.44, 273.88, 273.24, 274.6, 275.15, 269.77, 
    267.66, 264.29, 262.27, 260.53, 260.52, 261.54, 263.27, 261.45, 
    261.81, 261.99, 261.35, 262.64, 264.74, 265.56, 265.47, 267.3, 
    265.47, 262.64, 260.72), k = c(103.3086091, 102.9085757, 
    103.6086341, 107.5089591, 107.9089924, 108.9090758, 104.3086924, 
    97.80815068, 104.8087341, 108.0090008, 103.4086174, 104.5087091, 
    105.8088174, 100.308359, 102.6085507, 100.4083674, 96.80806734, 
    99.50829236, 102.708559, 100.7083924, 103.0485874, 103.9186599, 
    104.7887324, 105.0787566, 103.3386116, 104.0186682, 102.5685474, 
    112.4193683, 105.8488207, 104.5987166, 107.3989499, 108.6490541, 
    107.2989416, 106.2388532, 101.3084424, 98.02816901, 102.1785149, 
    97.83815318, 98.70822569, 88.85740478, 92.66772231, 95.36794733, 
    91.4076173, 87.54729561, 89.66747229, 87.73731144, 87.34727894, 
    90.9275773, 78.26652221, 80.29669139, 79.90665889, 77.68647387, 
    77.59646637, 78.46653888, 77.68647387, 77.01641803, 84.45703809, 
    77.97649804, 76.72639387, 77.88649054), l = c(109.1, 109.1, 
    108.8, 108.2, 107.6, 107.2, 107.3, 106.7, 106.4, 106, 105.9, 
    104.9, 103.8, 103.5, 103, 102.3, 101.3, 100.5, 99.6, 98.6, 
    97.43314, 96.68301, 95.84954, 95.18276, 94.76602, 94.01589, 
    92.84903, 91.18208, 89.76517, 89.18174, 88.51496, 87.76484, 
    86.68132, 85.93119, 85.18107, 84.51429, 83.76416, 83.43077, 
    83.26407, 82.93068, 82.46215, 82.14979, 81.83744, 81.05654, 
    80.43183, 80.35374, 80.27565, 79.9633, 79.72903, 79.57285, 
    79.57285, 79.26049, 79.02623, 79.10432, 79.02623, 78.71387, 
    78.4796, 78.24534, 77.93298, 77.69871), m = c(108.26667, 
    107.96667, 107.46667, 106.76667, 106.66667, 106.6, 106.43333, 
    105.83333, 105, 104.8, 104.46667, 103.46667, 102.4, 102.56667, 
    102.2, 101.96667, 100.77774, 100.47032, 100.41443, 98.48607, 
    97.47997, 97.22844, 96.55771, 96.52976, 96.58566, 98.2066, 
    96.58566, 94.0704, 92.00231, 92.03026, 91.86257, 90.40932, 
    89.26348, 88.84427, 87.19538, 85.32292, 84.28887, 83.61814, 
    83.72993, 83.59019, 83.22324, 82.61167, 82.09794, 80.36107, 
    78.86882, 78.42849, 77.93923, 77.05856, 76.39806, 76.34913, 
    76.22682, 75.39507, 75.05259, 75.24829, 75.12598, 74.34316, 
    74.04961, 73.60927, 73.21786, 72.67968), n = c(108.56667, 
    108.56667, 108.23333, 107.3, 107.13333, 106.8, 106.63333, 
    105.76667, 105.46667, 105.06667, 104.8, 103.23333, 102.5, 
    102.6, 102.36667, 102.1, 100.5226, 100.32976, 100.71544, 
    98.29121, 97.35458, 97.43723, 96.80362, 96.85872, 96.36285, 
    98.75953, 97.05155, 93.6907, 91.12874, 91.29403, 91.29403, 
    89.44831, 88.07091, 87.57505, 85.86707, 83.96626, 83.4153, 
    82.64396, 82.47867, 82.17564, 82.00498, 81.76645, 81.12244, 
    79.59587, 78.02161, 77.73538, 77.18677, 76.11341, 75.39783, 
    75.42168, 75.04004, 73.94283, 73.94283, 74.08594, 73.7043, 
    72.67864, 72.2493, 71.89151, 71.43831, 70.62732), o = c(56899L, 
    56899L, 56725L, 56235L, 56148L, 55974L, 55886L, 55432L, 55275L, 
    55065L, 54925L, 54105L, 53721L, 53773L, 53651L, 53511L, 52692L, 
    52590L, 52793L, 51522L, 51031L, 51074L, 50742L, 50771L, 50511L, 
    51767L, 50872L, 49110L, 47767L, 47855L, 47853L, 46886L, 46165L, 
    45905L, 45010L, 44013L, 43697L, 43321L, 43221L, 43059L, 42971L, 
    42846L, 42509L, 41709L, 40883L, 40734L, 40446L, 39884L, 39509L, 
    39522L, 39321L, 38746L, 38746L, 38821L, 38621L, 38084L, 37879L, 
    37679L, 37434L, 36998L), p = c(57844L, 57844L, 57667L, 57168L, 
    57080L, 56904L, 56813L, 56353L, 56193L, 55980L, 55838L, 55003L, 
    54612L, 54666L, 54541L, 54398L, 53567L, 53465L, 53670L, 52379L, 
    51878L, 51923L, 51585L, 51615L, 51351L, 52629L, 51718L, 49927L, 
    48562L, 48649L, 48640L, 47666L, 46932L, 46668L, 45758L, 44745L, 
    44428L, 44046L, 43944L, 43779L, 43690L, 43563L, 43219L, 42407L, 
    41567L, 41416L, 41123L, 40551L, 40170L, 40182L, 39979L, 39395L, 
    39394L, 39471L, 39267L, 38721L, 38514L, 38309L, 38061L, 37617L
    ), q = c(58398L, 58236L, 57967L, 57591L, 57537L, 57501L, 
    57411L, 57087L, 56637L, 56529L, 56349L, 55809L, 55236L, 55324L, 
    55128L, 55002L, 54342L, 54177L, 54146L, 53107L, 52563L, 52428L, 
    52067L, 52052L, 52082L, 52956L, 52082L, 50726L, 49611L, 49626L, 
    49525L, 48751L, 48134L, 47907L, 47018L, 46008L, 45449L, 45106L, 
    45159L, 45067L, 44856L, 44526L, 44249L, 43312L, 42509L, 42272L, 
    42008L, 41532L, 41177L, 41150L, 41085L, 40637L, 40451L, 40557L, 
    40491L, 40068L, 39917L, 39685L, 39464L, 39155L), r = c(59373L, 
    59209L, 58935L, 58551L, 58496L, 58458L, 58368L, 58039L, 57582L, 
    57472L, 57289L, 56742L, 56156L, 56248L, 56046L, 55919L, 55243L, 
    55075L, 55045L, 53988L, 53436L, 53298L, 52930L, 52915L, 52947L, 
    53834L, 52946L, 51567L, 50433L, 50449L, 50357L, 49557L, 48932L, 
    48671L, 47722L, 46772L, 46213L, 45865L, 45919L, 45826L, 45612L, 
    45276L, 44994L, 44041L, 43225L, 42983L, 42715L, 42232L, 41870L, 
    41843L, 41777L, 41321L, 41132L, 41240L, 41172L, 40743L, 40587L, 
    40352L, 40127L, 39814L), s = c(21773L, 21780L, 21835L, 21815L, 
    21753L, 21611L, 21767L, 21849L, 21713L, 21535L, 21727L, 21644L, 
    21351L, 21385L, 21323L, 21296L, 20611L, 20541L, 20794L, 20030L, 
    19662L, 19684L, 19640L, 20046L, 19846L, 20794L, 20331L, 19431L, 
    18418L, 18476L, 18612L, 18186L, 17636L, 17442L, 16941L, 16531L, 
    16354L, 16014L, 16084L, 16266L, 16191L, 16070L, 15870L, 15355L, 
    14620L, 14498L, 14476L, 14433L, 14099L, 14113L, 14140L, 14052L, 
    14057L, 14122L, 14187L, 14163L, 14031L, 13991L, 13923L, 13849L
    ), t = c(21773L, 21780L, 21835L, 21815L, 21753L, 21611L, 
    21767L, 21849L, 21713L, 21535L, 21727L, 21644L, 21351L, 21385L, 
    21323L, 21296L, 20611L, 20541L, 20794L, 20030L, 19662L, 19684L, 
    19640L, 20046L, 19846L, 20794L, 20331L, 19431L, 18418L, 18476L, 
    18612L, 18186L, 17636L, 17442L, 16941L, 16531L, 16354L, 16014L, 
    16084L, 16266L, 16191L, 16070L, 15870L, 15355L, 14620L, 14498L, 
    14476L, 14433L, 14099L, 14113L, 14140L, 14052L, 14057L, 14122L, 
    14187L, 14163L, 14031L, 13991L, 13923L, 13849L), u = c(94761L, 
    94761L, 94047L, 92619L, 92619L, 92619L, 91734L, 89964L, 89964L, 
    89964L, 88850L, 86622L, 86538L, 86538L, 86395L, 86109L, 86121L, 
    86121L, 85699L, 84855L, 84855L, 84855L, 83963L, 82179L, 82179L, 
    82179L, 81237L, 79353L, 79353L, 79353L, 78708L, 77418L, 77418L, 
    77418L, 76650L, 75114L, 74913L, 74913L, 74375L, 73299L, 73299L, 
    73299L, 72998L, 72396L, 72396L, 72396L, 71621L, 70071L, 70071L, 
    70071L, 69360L, 67938L, 67938L, 67938L, 67142L, 65550L, 65421L, 
    64941L, 64462L, 63504L), v = c(96819L, 96819L, 96090L, 94632L, 
    94632L, 94632L, 93727L, 91917L, 91917L, 91917L, 90779L, 88503L, 
    88416L, 88416L, 88270L, 87978L, 87996L, 87996L, 87566L, 86706L, 
    86706L, 86706L, 85794L, 83970L, 83970L, 83970L, 83007L, 81081L, 
    81081L, 81081L, 80423L, 79107L, 79107L, 79107L, 78321L, 76749L, 
    76533L, 76533L, 75983L, 74883L, 74883L, 74883L, 74575L, 73959L, 
    73959L, 73959L, 73167L, 71583L, 71583L, 71583L, 70858L, 69408L, 
    69408L, 69408L, 68594L, 66966L, 66831L, 66342L, 65853L, 64875L
    ), w = c(144.5, 146.5, 147.3, 143.3, 140.1, 142.8, 141.2, 
    140.2, 137.8, 137.4, 136.6, 137.6, 125.5, 125.7, 120.5, 124.2, 
    121.5, 119.8, 121.3, 122, 114.1, 114.4, 114.7, 116.1, 112.8, 
    111.8, 110.2, 111.7, 112.2, 113.7, 112.7, 110.5, 107, 107.5, 
    108, 107.1, 106.7, 103.3, 104.2, 104.3, 104.1, 101.3, 100.5, 
    94.3, 105.6, 101, 102, 103.1, 101.4, 105.5, 100.5, 102.8, 
    100.5, 105.1, 98.8, 105.1, 98.2, 98.2, 100.6, 103), x = c(132.2, 
    133.9, 133.5, 126, 125, 122.6, 122.6, 123.8, 124.5, 120.2, 
    120.2, 123.5, 105.2, 116.4, 111.5, 116.4, 116.1, 114.3, 117, 
    117.9, 107.1, 104.5, 110.6, 110.5, 104.2, 105.4, 106.2, 110.3, 
    106.8, 111.4, 111.2, 108.5, 93.5, 101.5, 101.4, 101.3, 101.7, 
    96.8, 97.3, 100, 97.5, 99.4, 94.8, 93.8, 101.9, 97.4, 97.7, 
    98.4, 100.6, 100.1, 96.3, 98.1, 93.4, 99.3, 97.3, 99.6, 99.2, 
    97.8, 100.1, 102.9), y = c(149.8, 151.9, 153.2, 150.7, 146.5, 
    151.5, 149.2, 147.3, 143.6, 144.8, 143.6, 143.7, 134.1, 129.7, 
    124.3, 127.5, 123.7, 122.2, 123.1, 123.8, 117.1, 118.6, 116.4, 
    118.4, 116.4, 114.6, 111.9, 112.2, 114.5, 114.6, 113.4, 111.3, 
    112.8, 110.1, 110.8, 109.5, 108.8, 106.1, 107.1, 106.1, 107, 
    102.1, 103, 94.5, 107.2, 102.5, 103.9, 105.1, 101.7, 107.8, 
    102.4, 104.8, 103.6, 107.6, 99.5, 107.4, 97.8, 98.4, 100.8, 
    103), z = c(155.2, 157.6, 159, 156.5, 151.4, 155, 152, 149, 
    146.4, 147.9, 146.6, 146.3, 137.1, 131.1, 124.5, 127.5, 123.1, 
    121.9, 123, 123.5, 116.4, 117.7, 116.4, 118.1, 116.5, 113.7, 
    110.2, 111, 113.9, 113.9, 113.6, 110.9, 113.2, 109.9, 111.7, 
    109.7, 110.1, 106.3, 107.4, 105.9, 107.2, 101.6, 103.8, 94.1, 
    108.4, 102.7, 104.1, 105.1, 101.5, 108.8, 102.3, 105.4, 103, 
    107.2, 99.3, 107.6, 97.4, 97.6, 101.2, 103.9), aa = c(112.6, 
    112.7, 113.6, 110.7, 113.4, 127.1, 130.1, 135.7, 123.7, 123.2, 
    123, 125.5, 113.5, 120.2, 123.3, 128, 128.2, 124.6, 124, 
    125.8, 122.2, 124.8, 116.6, 120.4, 115.9, 120.6, 124, 120.6, 
    119, 120.1, 111.6, 114, 110.2, 111.6, 104.5, 107.9, 100.4, 
    104.7, 105, 106.9, 105.1, 105.8, 97.3, 96.6, 99.1, 101.1, 
    102.5, 105.2, 103, 101, 102.7, 100.5, 107.4, 110.1, 101.3, 
    105.7, 100.3, 104.1, 98.4, 97.2)), .Names = c(""c"", ""d"", ""e"", 
""f"", ""g"", ""h"", ""i"", ""j"", ""k"", ""l"", ""m"", ""n"", ""o"", ""p"", ""q"", ""r"", 
""s"", ""t"", ""u"", ""v"", ""w"", ""x"", ""y"", ""z"", ""aa""), row.names = c(NA, 
-60L), class = ""data.frame"")
&gt; 
&gt; control &lt;- trainControl(method=""repeatedcv"", number=25, repeats=3)
&gt; # train the model
&gt; dat &lt;- dataSelection[2:25]
&gt; model &lt;- train(dataSelection$c~., data=dataSelection, method=""lvq"", preProcess=""scale"", trControl=control)
Error in train.default(x, y, weights = w, ...) : 
  wrong model type for regression
</code></pre>

<p>As you can see in my train function I get an error.</p>

<p>Any recommendation why my presented R code does not work? Why is my model type wrong?</p>

<p>I appreciate your replies</p>

<p><strong>UPDATE</strong></p>

<p>As @Matthew pointed  out, lvq`s are good for categorical attributes. However, any recommendations from you to use another algorithm, to rank my numerical variables?</p>
"
"0.0599760143904067","0.0609710760849692","160021","<p><a href=""http://stats.stackexchange.com/questions/18480/interpretation-of-log-transformed-predictor"">Interpretation of log transformed predictor</a> neatly explains how to interpret a <code>log</code> transformed predictor in OLS. Does the interpretation change if there are 0s in the data and the transformation becomes <code>log(1 + x)</code> instead? </p>

<p>Some authors (e.g. Fox and Weisberg 2011) recommend adding a <code>start</code> (i.e. a positive constant) if a <code>log</code> transformation is necessary to correct skewness and improve symmetry, but the data contains zeros. </p>

<p>Consider a variation of the <code>Ornstein</code> example in CAR (p. 303): </p>

<pre><code>require(car)
data(Ornstein)
boxplot(Ornstein$interlocks, horizontal = T) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/j8XSa.png"" alt=""enter image description here""></p>

<p>The data is clearly right skewed, and contains 0s. </p>

<pre><code>summary(powerTransform(1 + Ornstein$interlocks))
    ## bcPower Transformation to Normality 
    ## 
    ##                         Est.Power Std.Err. Wald Lower Bound Wald Upper Bound
    ## 1 + Ornstein$interlocks    0.1248    0.053           0.0209           0.2287
## 
## Likelihood ratio tests about transformation parameters
##                              LRT df      pval
## LR test, lambda = (0)   5.502335  1 0.0189911
## LR test, lambda = (1) 262.431991  1 0.0000000
</code></pre>

<p>The <code>powerTransform()</code> function suggests that a <code>log(1 + x)</code> transformation here could be useful. </p>

<pre><code>boxplot(log(1 + Ornstein$interlocks), horizontal = T)
</code></pre>

<p><img src=""http://i.stack.imgur.com/W3YlX.png"" alt=""enter image description here""></p>

<p>As you can see, symmetry is indeed improved. </p>

<p><strong>Question:</strong> If this transformed variable were to be included in an OLS regression as an IV, would the coefficient estimates still have the usual interpretation of <code>log</code> transformed variables? </p>
"
"0.0608267804924532","0.072141950116023","160178","<p>I have two I(1) time series and I regressed one against the other and found that it had low to moderate R-squared but my DW statistic is about 0.015.  I know the literature says this is the case of spurious regression?  Now, upon running co-integration tests on the residuals (I ran an ADF test using updated MacKinnon's p table, used Phillips Ouliaris test, Johansen test and Elliott Rothenberg and stock test).  Now, all my tests pass except for Phillips Ouliaris and Johansen test.  These are the only tests where I am not getting the residuals from the data.  I believe the PO test automatically runs an regression of y against x and uses the Phillips Ouliaris distribution rather than the ADF distribution on residuals.  </p>

<p>My main question is, which test do I trust and whether these tests even make any sense considering I have a spurious regression phenomena?  I believe if the two series are co-integrated, then the residuals won't be spurious correct?  So my main questions are the follows:</p>

<ol>
<li>Can you have co-integration even with spurious regression?</li>
<li>which test do I ultimately have to chosen from?  PO test, Johansen test (both these tests accept the series are not co-integrated).  ERS test passes on residuals and so does the R functions <code>adf.test</code>, <code>adfTest</code>, &amp; <code>ur.df</code>.  </li>
<li>My time series is from 1998 to 2015.  Sometimes, daily gives me co-integration, but monthly doesn't.  What time frame is most acceptable?  </li>
</ol>
"
"0.0808716413062113","0.0904347204435887","160316","<p>I have a dataset consisting of about 600 observations. Each observation has around 100 attributes. One of the attributes I want to predict. Since the attribute that I want to predict can only have non-negative integer values, I was looking for ways to predict count data and found that there are various options, such as Poisson regression or negative binomial regression.</p>

<p>For my first try I used negative binomial regression in <code>R</code>:</p>

<pre><code>#First load the data into a dataset
dataset &lt;- test_observations[, c(5:8, 54)]

#Create the model
fm_nbin &lt;- glm.nb(NumberOfIncidents ~ ., data = dataset[10:600, ] )
</code></pre>

<p>I then wanted to see how to predicted values look like:</p>

<pre><code>#Create data to test prediction
newdata &lt;- dataset[1:10, ]

#Do the prediction
predict(fm_nbin, newdata, type=""response"")
</code></pre>

<p>Now the problem is the output looks like this:</p>

<pre><code>     1         2         3         4         5         6         7         8         9        10 
0.2247337 0.2642789 0.2205408 0.2161833 0.1794224 0.2081522 0.2412996 0.2074992 0.2213011 0.2100026 
</code></pre>

<p>The problem with this is that I expected that the predicted values are integers, since that is the whole purpose of using a negative binomial regression. What am I missing here?</p>

<p>Furthermore, I would like to evaluate my predictions in terms of mean squared error and mean absolute error, as well as a correlation coefficient. However, I couldn't find a way to get these easily, without doing all the calculations manually. Is there any built-in function for this?</p>
"
"0.026822089039291","0.0272670941574606","160495","<p>I working with R on a classification problem. My outcome variable is binary with two levels 1 and 2. 
First of all I tried the logistic regression, which of all methods has the best performance, altough still poor. </p>

<p>I tried nnet package, random forest, the fuzzy package frbs and decision trees. </p>

<p>The nnet function gives me only one class - in this case 2.</p>

<p>I had some hope with frbs package. See my code below:</p>

<pre><code>obj &lt;- frbs.learn(train,method.type=""FRBCS.CHI"",control=list(num.labels=3,type.mf=""GAUSSIAN""))
summary(obj)
#test set without def 
pred&lt;-predict(obj,newdata=test[,1:8])
</code></pre>

<p>But the predictions are wrong, the class 1 is completely missclassified</p>

<pre><code>#percentage error
tdef&lt;-test$def
err = 100*sum(pred!=tdef)/ nrow(pred)
print(err)
[1] 16.93038
</code></pre>

<p>I'm wondering what I could improve to classify the output variable. Is something wrong with my data? 
Are the parameters not right? </p>

<p>Can someone please verifiy?  I'm at the end of my knowledge...</p>

<p>You can find the (normalized) data here:
<a href=""https://drive.google.com/open?id=1xrCXTLqKvGiGeo2X0Y1DvoSKvzbYFnyccLimceDIbZg"" rel=""nofollow"">https://drive.google.com/open?id=1xrCXTLqKvGiGeo2X0Y1DvoSKvzbYFnyccLimceDIbZg</a></p>
"
"0.026822089039291","0.0272670941574606","160665","<p>I am trying to run a cointegration test with a dummy variable using <code>ca.jo</code> function in <code>urca</code> package. </p>

<pre><code>johcoint=ca.jo(Ydata[10:60,1:5],type=""trace"",ecdet=c(""const""),K=2,spec=""transitory"",dumvar=dumvar)
</code></pre>

<p><code>dumvar</code> is the binary variable that takes 1 and 0 only. The first two observations are 1 and the rest are 0s as you can see in the sample given below. 
When I run the code, I get </p>

<pre><code>Error in solve.default(M11) : 
      Lapack routine dgesv: system is exactly singular: U[1,1] = 0
</code></pre>

<p>I think this is something to do with the invertability of the input matrix, and this occurs only when I include <code>dumvar</code> in the regression. The above error message disappears if I change the 3rd observation to '1' in <code>dumvar</code>.</p>

<p>Below is the sample data just for info:</p>

<pre><code>   A         B       C       D         E       dumvar
1  2.255446 1.688807 1.506579 1.880152 9.575868      1
2  2.230118 1.578281 1.546805 1.905426 9.545534      1
3  2.255446 1.688807 1.506579 1.880152 9.575868      0
4  2.230118 1.578281 1.546805 1.905426 9.545534      0
5  2.255446 1.688807 1.506579 1.880152 9.575868      0
6  2.230118 1.578281 1.546805 1.905426 9.545534      0
7  2.255446 1.688807 1.506579 1.880152 9.575868      0
8  2.230118 1.578281 1.546805 1.905426 9.545534      0
9  2.255446 1.688807 1.506579 1.880152 9.575868      0
10 2.230118 1.578281 1.546805 1.905426 9.545534      0
11 2.255446 1.688807 1.506579 1.880152 9.575868      0
12 2.230118 1.578281 1.546805 1.905426 9.545534      0
13 2.255446 1.688807 1.506579 1.880152 9.575868      0
14 2.230118 1.578281 1.546805 1.905426 9.545534      0
15 2.255446 1.688807 1.506579 1.880152 9.575868      0
16 2.230118 1.578281 1.546805 1.905426 9.545534      0
17 2.255446 1.688807 1.506579 1.880152 9.575868      0
18 2.230118 1.578281 1.546805 1.905426 9.545534      0
19 2.255446 1.688807 1.506579 1.880152 9.575868      0
20 2.230118 1.578281 1.546805 1.905426 9.545534      0
21 2.255446 1.688807 1.506579 1.880152 9.575868      0
22 2.230118 1.578281 1.546805 1.905426 9.545534      0
23 2.255446 1.688807 1.506579 1.880152 9.575868      0
24 2.230118 1.578281 1.546805 1.905426 9.545534      0
25 2.255446 1.688807 1.506579 1.880152 9.575868      0
26 2.230118 1.578281 1.546805 1.905426 9.545534      0
27 2.255446 1.688807 1.506579 1.880152 9.575868      0
28 2.230118 1.578281 1.546805 1.905426 9.545534      0
29 2.255446 1.688807 1.506579 1.880152 9.575868      0
30 2.230118 1.578281 1.546805 1.905426 9.545534      0
31 2.255446 1.688807 1.506579 1.880152 9.575868      0
32 2.230118 1.578281 1.546805 1.905426 9.545534      0
33 2.255446 1.688807 1.506579 1.880152 9.575868      0
34 2.230118 1.578281 1.546805 1.905426 9.545534      0
35 2.255446 1.688807 1.506579 1.880152 9.575868      0
36 2.230118 1.578281 1.546805 1.905426 9.545534      0
37 2.255446 1.688807 1.506579 1.880152 9.575868      0
38 2.230118 1.578281 1.546805 1.905426 9.545534      0
39 2.255446 1.688807 1.506579 1.880152 9.575868      0
40 2.230118 1.578281 1.546805 1.905426 9.545534      0
41 2.255446 1.688807 1.506579 1.880152 9.575868      0
42 2.230118 1.578281 1.546805 1.905426 9.545534      0
43 2.255446 1.688807 1.506579 1.880152 9.575868      0
44 2.230118 1.578281 1.546805 1.905426 9.545534      0
45 2.255446 1.688807 1.506579 1.880152 9.575868      0
46 2.230118 1.578281 1.546805 1.905426 9.545534      0
47 2.255446 1.688807 1.506579 1.880152 9.575868      0
48 2.230118 1.578281 1.546805 1.905426 9.545534      0
49 2.255446 1.688807 1.506579 1.880152 9.575868      0
50 2.230118 1.578281 1.546805 1.905426 9.545534      0
</code></pre>

<p>Thank you!</p>
"
"0.080466267117873","0.0818012824723818","160980","<p>I use the library method loess of the R programming language for non parametric data fitting. The dataset is two-dimensional. I have not found any proper documentation of the method parameter <code>weights</code>.</p>

<p>My data points are normally distributed random variables, and I also have an estimate of their respective standard deviations. I am wondering whether the parameter <code>weights</code> allows me to supply R with the details of the standard deviations. In other words: I wonder whether the individual weights in <code>weights</code> are (relative) measures of data quality, so that the fit can be improved if some measure of data uncertainty is supplied via the parameter weights.</p>

<p>I further suspect the entries in <code>weights</code> are used as additional weights in the weighted least squares regressions of local datasets in the LOESS procedure (maybe as additional weight prefactors for the (position dependent) kernel functions?). This would suggest that for the case of data points which are independent normally distributed random variables, but which still have different noise levels (i.e. different standard deviations as in my case, since the data points come from different and independent series of measurements), the weights should be chosen as <code>1/\sigma_{i}^2</code>, where <code>\sigma_{i}</code> is the standard deviation of the respective random variable/data point. If someone knows for sure, that would be nice to know.</p>
"
"0.0758643241810882","0.0771229887279699","161319","<p>First let me start off by saying I know the consequences that come with removing/ignoring outliers.. but for this particular case I am just looking at weekly trends in the equipment I collect data from (a little over 100 sensors). I need to ignore ""leverage points"" that ruin R sqd values. I need to see if any of my sensors are trending out of their normal operating limits over time using simple regressions. </p>

<pre><code>rm(list=ls())
mtcars &lt;- mtcars
myList &lt;- lapply(mtcars, function(x) summary(lm(mtcars$wt ~ x))$r.squared)
myList

## For example, Rsqd value is .75 without outliers added to my data set 
##$mpg
##[1] 0.7528328
##Works Great! 

##Now we add some outliers 
mtcars$mpg[c(5,10,15,20)] &lt;- 100

## without lmrob the value for mpg is myList$mpg [1] 0.001461735
##using robust regression
require(robustbase)
summary(lmrob(mtcars$wt ~ mtcars$mpg))$r.squared

##summary(lmrob(mtcars$wt ~ mtcars$mpg))$r.squared
## [1] 0.7187418
##gives me a representative r squared value for the data set and ""Leverage""
##points/extreme outliers don't ruin relationships I am trying to see.

#but if I try this... 
myList &lt;- lapply(mtcars, function(x) summary(lmrob(mtcars$wt ~ x))$r.squared)

#it doesn't work and by that I mean I receive the error
#Warning messages:
#1: In lmrob.S(x, y, control = control, mf = mf) :
# S-estimated scale == 0:  Probably exact fit; check your data
#2: In seq_len(ar) : first element used of 'length.out' argument

#Error in summary(lmrob(mtcars$wt ~ x)) : 
# error in evaluating the argument 'object' in selecting a method for     function 'summary': Error in numeric(seq_len(ar)) : invalid 'length' argument 
</code></pre>

<p>I have also tried just removing any data points in each column that are outside of +-3 Sigma but I wasn't able to get that working... </p>
"
"0.053644178078582","0.0545341883149212","161338","<p>I have data that can be fit, more or less, by logistic growth functions. Hence I used <a href=""http://ww2.coastal.edu/kingw/statistics/R-tutorials/logistic.html"" rel=""nofollow"">this tutorial</a> to do this.</p>

<p>Now I want to get an x value for a specific y value from the model. Maybe this is too trivial, but I could not find anything on the forums...or perhaps I was looking in the wrong way. For the below example, I would want to get the age at which Menarche is 0.5. In Excel I'd get the formula of the fit, solve it for x and put in y=0.5 ... but in R with logistic fit?</p>

<pre><code>library(""MASS"")
data(menarche)
str(menarche)

summary(menarche)

plot(Menarche/Total ~ Age, data=menarche)

glm.out = glm(cbind(Menarche, Total-Menarche)~Age, family=binomial(logit), data=menarche)

plot(Menarche/Total ~ Age, data=menarche)
lines(menarche$Age, glm.out$fitted, type=""l"", col=""red"")
title(main=""Menarche Data with Fitted Logistic Regression Line"")
</code></pre>
"
"0.018966081045272","0.0385614943639849","161614","<p>I want to solve the first exercice of the Multiple Regression Chapter of R. Hyndman's online book on Time Series Forecasting (see <a href=""https://www.otexts.org/fpp/5/8"" rel=""nofollow"">https://www.otexts.org/fpp/5/8</a>). I use <code>R</code> with <code>fpp</code> package as wanted in the exercise.</p>

<p>I am blocked in the following question:
c. Use R to fit a regression model to the logarithms of these sales data with a linear trend, seasonal dummies and a â€œsurfing festivalâ€ dummy variable.</p>

<p>Indeed, I don't know how to make the function <code>tslm</code> work with my dummy vector for the surfing festival. Here is my code.</p>

<pre><code>library(fpp)
log_fancy = log(fancy)
dummy_fest_mat = matrix(0, nrow=84, ncol=1)
for(h in 1:84)
    if(h%%12 == 3)   #this loop builds a vector of length 84 with
        dummy_fest_mat[h,1] = 1   #1 corresponding to each month March
dummy_fest_mat[3,1] = 0 #festival started one year later

dummy_fest = ts(dummy_fest_mat, freq = 12, start=c(1987,1))
fit = tslm(log_fancy ~ trend + season + dummy_fest)
</code></pre>

<p>When I do <code>summary(fit)</code>, I see that the regression coefficients have been well calculated, but when I continue with <code>forecast(fit)</code>
I get the following error : </p>

<pre><code>Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
  variables have not equal length (found for 'factor(dummy_fest)')
In addition: Warning message:
'newdata' had 50 rows but variables found have 84 rows 
</code></pre>

<p>But what is even stranger is that when I do <code>forecast(fit, h=84)</code>, it works!!
I don't know what is happening here, can someone explain me?</p>
"
"0.0599760143904067","0.0487768608679754","161624","<p>I have a system of 2 equations with 16 variables $X={x_{1},x_{2},x_{3},x_{4},x_{5},x_{6},x_{7},x_{8},x_{9},x_{10},x_{11},x_{12},x_{13},x_{14},x_{15},x_{16}}$</p>

<p>$0.2381570 = \frac{\frac{x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}}{\sqrt{3} \cdot \sqrt{5.0625}}-\frac{x_{7},x_{8},x_{9},x_{10}}{\sqrt{3} \cdot \sqrt{2.56}}
 \cdot \frac{x_{11},x_{12},x_{13},x_{14},x_{15},x_{16}}{\sqrt{5.0625} \cdot \sqrt{2.56}}}{1-(\frac{x_{11},x_{12},x_{13},x_{14},x_{15},x_{16}}{\sqrt{5.0625} \cdot \sqrt{2.56}})^{2}}$</p>

<p>$0.2092895 = \frac{\frac{x_{7},x_{8},x_{9},x_{10}}{\sqrt{3} \cdot \sqrt{2.56}}-\frac{x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}}{\sqrt{3} \cdot \sqrt{5.0625}}
 \cdot \frac{x_{11},x_{12},x_{13},x_{14},x_{15},x_{16}}{\sqrt{5.0625} \cdot \sqrt{2.56}}}{1-(\frac{x_{11},x_{12},x_{13},x_{14},x_{15},x_{16}}{\sqrt{5.0625} \cdot \sqrt{2.56}})^{2}}$</p>

<p>I now want to solve the system of equations with the following restriction: </p>

<ul>
<li>$\{x\in R \mid -1 \leq x \leq 1\}$</li>
</ul>

<p>To be more precise: I'm not even sure about this, the x's are Pearson correlations and 0.2381570, 0.2092895 are regression weights. Therefore I think I would prefer solutions that are not 0, thats why I set the startx parameters to 1 in the following R-Code. Maybe the structure of the equations also does not allow any other solutions anyway, but I really would not know how to prove that.</p>

<pre><code>fun &lt;- function(x) { 
    f &lt;- numeric(length(x))                                 
    f[1] &lt;-  ((((x[1]+x[2]+x[3]+x[4]+x[5]+x[6])/(sqrt(3)*sqrt(5.0625)))-
                   ((x[7]+x[8]+x[9]+x[10])/(sqrt(3)*sqrt(2.56)))*
                   ((x[11]+x[12]+x[13]+x[14]+x[15]+x[16])/(sqrt(5.0625)*sqrt(2.56))))/
                  (1-((x[11]+x[12]+x[13]+x[14]+x[15]+x[16])/(sqrt(5.0625)*sqrt(2.56)))^2))-0.238157
    f[2] &lt;-  ((((x[7]+x[8]+x[9]+x[10])/(sqrt(3)*sqrt(2.56)))-
                   ((x[1]+x[2]+x[3]+x[4]+x[5]+x[6])/(sqrt(3)*sqrt(5.0625)))*
                   ((x[11]+x[12]+x[13]+x[14]+x[15]+x[16])/(sqrt(5.0625)*sqrt(2.56))))/
                  (1-((x[11]+x[12]+x[13]+x[14]+x[15]+x[16])/(sqrt(5.0625)*sqrt(2.56)))^2))-0.2092895
    f 
} 
startx &lt;- rep(1,16) # start the answer search here
answers &lt;- as.data.frame(nleqslv(startx,fun))  # answers[""x""] = x,y,z are the solutions closest to startx if there are multiple solutions
</code></pre>

<p>My problem with this: the answers only give solutions for $x_{1}$ and $x_{2}$ and set all other variables to 0. I guess this happens because there are only 2 equations.</p>

<p>An obvious solution would be f.e. X = 0.2. How can I find those? Also the result says ""Jacobian is singular"", what I don't understand.</p>

<p>My ultimate goal would be a vector space of the possible solutions, but then again there are infinitely many because I have more variables than equations?</p>
"
"0.053644178078582","0.0545341883149212","161797","<p>Say I have some predictors, and I know how they affect some dependent variable:</p>

<pre><code>#predictors
x1&lt;- seq(0,10,0.1)
x2&lt;-runif(101,0,1)
#specify how predictors affect dependent variable y
y&lt;- 15*x1 + 10*x1*x2
#introduce random error
y.err&lt;- rnorm(101,0.01)
y&lt;- y + y.err
</code></pre>

<p>I can then model <code>y</code> as a function of <code>x1</code> and <code>x2</code> like this:</p>

<pre><code>fit&lt;- lm(y ~ x1 + x1*x2)
</code></pre>

<p>which yields this output:</p>

<pre><code>summary(fit)

Call:
lm(formula = y ~ x1 + x1 * x2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.76646 -0.59886 -0.09115  0.70549  2.85311 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.38875    0.41630   0.934    0.353    
x1          14.93601    0.07409 201.585   &lt;2e-16 ***
x2          -0.74815    0.79518  -0.941    0.349    
x1:x2       10.10469    0.13698  73.768   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.025 on 97 degrees of freedom
Multiple R-squared:  0.9997,    Adjusted R-squared:  0.9997 
F-statistic: 1.184e+05 on 3 and 97 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>So, total model $R^2$ is 0.997. </p>

<p>I would like to know what percentage of that $R^2$ value can be attributed to x1, x2, and the interaction of x1 and x2. I am also aware of a previous post on this topic linked <a href=""http://stats.stackexchange.com/questions/60872/how-to-split-r-squared-between-predictor-variables-in-multiple-regression"">here</a>, where the user has an identical question. However, the solution proposed (as I read it) was to run the correlations individually, square them, and they will sum to the full model $R^2$. This is not the case here. </p>
"
"0.0929144419623766","0.0944559849109725","162463","<p>I am doing some data analysis on a fairly large health data set of patients with diagnoses and the respective procedures received for each event. I was asked to run a multinomial logistic regression on my data.</p>

<p>The dataset has around 4,000 columns of attributes, of which around 3,000 are unique diagnoses. The diagnosis variables take on the value of 1 if the patient had that diagnosis and 0 if he or she did not.  The remaining approximately 1,000 variables pertain to unique procedures, which also take on the value of 1 if the patient has received it, and 0 if he or she did not.</p>

<p>The dataset contains information on approximately 30,000 patients. I, admittedly naively, ran a the multinom function in the multinom package in R on all 4,000 variables, with the dependent variable being the very last procedure the patient has received (marked as ""Final procedure"" in the matrix), but R isn't able to complete the computation. </p>

<p>I would like some overall advice in perhaps a different package I could use for running regressions on large data sets (cannot use bigmemory however because this is on windows) or even perhaps reformatting my data. </p>

<p>Initially, my data set had around 50 columns, because the maximum number of diagnoses and procedures a patient had was 25 diagnoses and 25 procedures, so each column was marked as ""Diagnosis X"" and ""Procedure x,"" with the corresponding element being the actual diagnosis/procedure identifier. For all the patients who did I have all 25 diagnoses/procedures (so most of them), the values in the data frame would just be NA. Now I am wondering if I could perhaps resort to using this data frame instead and have a nicer, smaller matrix to work with? The only real reason I reformatted my data set into the much larger matrix was because my grad student asked me to do so, but maybe this isn't the way to go.</p>
"
"0.0709645772411954","0.0618359572423054","162691","<p>I am struggling to calculate / define confidence bands to use with a custom function that I must fit to my data. I have seen plenty of examples for standard models (linear regression, log functions, etc.). However, calculating the intervals to add bands to my custom function is proving tricky. Below I have the data and my function (working in R):</p>

<pre><code>data &lt;- matrix(c(0.08, 0.1, 0.12, 0.13, 0.49, 0.11, 0.12, 0.15, 
                 0.22, 0.47, 7, 8, 9, 21, 30, 3, 8, 13, 15, 17 ), ncol=2)
mycurve &lt;- function(x){ a + (b*log(x)) }
a    &lt;- 31
b    &lt;-  9
data &lt;- as.data.frame(data) # So that ggplot can use it
ggplot(data, aes(V1,V2)) + geom_point() + stat_function(fun=mycurve, color=""red"")
</code></pre>

<p>And the result is:  </p>

<p><a href=""http://i.stack.imgur.com/QZt92.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QZt92.png"" alt=""enter image description here""></a></p>

<p>The red curve is the custom function that I <em>must</em> use for this dataset. This is one example, but I have a few more datasets and must fit a different function to to each dataset. So I'm looking for an approach for calculating confidence bands that I can use for any function.</p>

<p>I have spent quite a while on this, but haven't yet found a ""generic"" solution. </p>
"
"0.050688983743711","0.0618359572423054","162831","<p>I've been playing around with the package <code>strucchange</code> (and to some extent <code>segmented</code>) in R. I'm trying to determine whether there are changes in slope in a linear regression and more importantly, how many breakpoints. A toy dataset:</p>

<pre><code>x &lt;- c(0, 5, 10, 15, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80)
y &lt;- c(-84.16, -86.67, -87.74, -86.07, -89.15, -91.90, -93.64, -95.92,
  -95.96, -99.19, -100.73, -107.29, -106.10, -107.29)
</code></pre>

<p>First problem: if I use the breakpoints function:</p>

<pre><code>breakpoints(y ~ x, data = data.frame(x, y))
</code></pre>

<p>I get the following error:</p>

<pre><code>Error in breakpoints.formula(y ~ x, data = data.frame(x, y)) : 
minimum segment size must be greater than the number of regressors
</code></pre>

<p>I think this arises because the default h parameter in the breakpoints command is 0.15 and 14 (the number of observations I have) * 0.15 = 2.1 which, rounded down, is not greater than 2 (the ""number of regressors"": incidentally, I would have thought the number of regressors would be 1 given my formula but I've learned from other working examples of <code>y ~ x</code> that nreg = 2 in these cases. I guess the intercept counts as a regressor?). </p>

<p>If I set h to 3 or some fraction such that 14 * h >= 3, the command works.</p>

<pre><code>breakpoints(y ~ x, data = data.frame(x, y), h = 3)
</code></pre>

<p>Two breakpoints are returned. But the result is sensitive to h. Such that if I use:</p>

<pre><code>breakpoints(y ~ x, data = data.frame(x, y), h = 4)
</code></pre>

<p>I get a different solution. In the latter case, a single optimal break is found because the minimum number of observations before a break can be called is higher. Is there a way to somehow determine whether one solution has more support than the other? In other words, how best to optimize not the position of breakpoints, but the number of breakpoints (perhaps across values of h)? I think the Fstats command might be the key but I'm having a lot of trouble understanding the help for this command...</p>
"
"0.053644178078582","0.0545341883149212","162867","<p>I'm currently building zero-inflated Poisson &amp; negative binomial predictive models using the zeroinfl() function from the pscl package in R.</p>

<p>Incorporating penalized regressions into my model to account for shrinkage and variable selection is a priority. In addition I'd like to use penalization to avoid convergence issues due to perfect/quasi separation in my data (better than manually removing variables).</p>

<p><strong>Question</strong>: Realizing that zero-inflated models $\neq$ hurdle models, for purposes of variable selection will my models be <strong>seriously biased</strong> if I first run separate run lasso (or elastic net) Poisson and logistic regressions with glmnet to select variables for the zeroinfl()?</p>
"
"NaN","NaN","163186","<p>Is there any function in R to conduct cross validation when you only know the regression equation?</p>
"
"0.080466267117873","0.0818012824723818","163604","<p>I'm using a plate reader to measure optical density of different bacterial
strains so I can compare their responses (growth rates and changes in them over
time) to stress conditions. The growth curves often don't follow any standard
shape so I'm fitting them empirically with the <code>loess</code> or <code>locfit</code> functions in
R, breaking the fits into intervals, and taking the derivatives to get growth
rates. My plots look like this:</p>

<p><a href=""http://i.stack.imgur.com/fiiLH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fiiLH.png"" alt=""locfit fitted data points""></a>
<a href=""http://i.stack.imgur.com/4dui4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4dui4.png"" alt=""simplified fitted curves""></a>
<a href=""http://i.stack.imgur.com/4t7K4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4t7K4.png"" alt=""derivatives of simplified curves""></a></p>

<p>As you can see the fitted curves have confidence intervals, but I'm not sure
how to transform them into a meaningful form (95% confidence or standard
deviation for example). And assuming that's doable, how do I go on to calculate
uncertainty in the rates?</p>

<p>I suppose I could just use the worst-case difference in slopes like this:</p>

<p><a href=""http://i.stack.imgur.com/fcEaY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fcEaY.png"" alt=""bad idea""></a></p>

<p>But that seems like a bad idea.</p>

<p>I could fit each well separately or split them into groups--there are a few
replicates for each strain and I could add more if needed--and just use the
standard deviation of the final calculated rates. Is that the best way? If so,
how do I decide the optimal group size to balance accurate fits with a good
number of replicates? I would also be open to using a different type of fit of course.</p>

<p>I've found a couple related questions, but neither one quite answers it:</p>

<ul>
<li><p><a href=""http://stats.stackexchange.com/questions/70629/calculate-uncertainty-of-linear-regression-slope-based-on-data-uncertainty"">This one</a> seems to rely on the true relationship being linear, which my curves violate</p></li>
<li><p><a href=""http://stats.stackexchange.com/questions/18391/how-to-calculate-the-difference-of-two-slopes"">This one</a> may well be correct but my stats knowledge is too basic to understand the answer</p></li>
</ul>

<p>EDIT: I'm using <code>deg=1</code> for both types of fits because I expect growth during log-phase to be linear on a log-transformed scale, but maybe higher-degree polynomials would be more accurate?</p>

<p>EDIT: <a href=""http://stats.stackexchange.com/questions/147106/determining-if-two-growth-curves-are-significantly-different"">This answer</a> looks very promising and I'm off to read the suggested paper.
EDIT: Nope, also depends on having a known underlying physical model.</p>
"
"0.0464572209811883","0.0472279924554862","163696","<p>I have analysed a dataset with a linear regression model, including an interaction term between a binary variable and a continuous variable. The interaction was significant. Afterwards, I have fitted 2 separate models of the continuous variable for each of the 2 groups of the binary variable. The 2 slopes have different signs and one of the two slopes is significant. </p>

<p>I need to calculate the power of the significance of this interaction. I prefer to do this with an R function.  </p>
"
"0.0599760143904067","0.0609710760849692","163819","<p>I am running a multinomial logistic regression model (with 3 possible outcomes) in R. I am trying to find the best way to assess the predictive power/accuracy of the model, and the best thing I've come up with is using a ROC curve.</p>

<p>For multi-class ROC analysis, I know that there is the one vs. one comparison or the one vs. all comparison. For the one vs. one comparison, would I need three separate ROC curves for each possible combination of outcome comparisons? If so, do I need to make a third model for comparing the two outcomes that were initially being compared to the baseline outcome?</p>

<p>For one vs. all comparison is the threshold for a r ""random model"" now 33% instead of 50%?</p>

<p>And finally, is there a better way to go about doing this/visualizing it?</p>

<p>EDIT: I know the pROC package has a multi class.roc function, but I don't totally get what it does.</p>
"
"0.026822089039291","0.0272670941574606","163986","<p>I effectively want to model the probability of a player winning his service point (a point in which he is the server) based on the values of explanatory variables (namely court surface and opponent world ranking)</p>

<p>Can this be done using a binary response logistic regression?</p>

<p>Consider the fact that I can view my response variable as number of successes out of a total number of trials (for which I have the data). Will it work considering I have both categorical and numerical explanatory variables?</p>

<p>Any feedback on why this will/won't work or how I can make it work would be hugely appreciated! I am doing the analysis in R, so pointers on functions or packages would also be welcome! </p>
"
"0.053644178078582","0.0545341883149212","164017","<p>I have a set of noisy data that can be described by a functional form.</p>

<p>For each observation f(x), where x is an index that runs from 0-100, I know that f(x)=g(x+1)/g(x)-g(x+1). I would like to find a way of fitting f(x). I also know that f(x) must be smooth. How could I do this?  </p>

<p>My idea is to try and fit this data using penalized splines. I choose a spline basis and a smoothing factor, and then find the coefficients of a regression on the spline basis that produce a curve f(x). I then optimize the coefficients to produce a curve such that when it is transformed it fits my data. A minimal reproducible example in R is below. </p>

<pre><code>require(dplyr)
require(gam)

target = c(0.132167681875765,0.804942648636132,0.60485585022111,1.02164234486286,0.58437549344597,0.88268397325963)

to_optim = function(par,target,knots,smooth,range) {

spline_reg = function(range,knots,par) bs(range,knots) %*% par

distance = function(fitted,target,smooth) sum((fitted-target)^2) +t(as.matrix(diff(fitted))) %*% 
  (as.matrix(diff(fitted))) * (smooth)

fitted = spline_reg(range,knots,par)

crs = fitted/lag(fitted)-fitted
crs=crs[3:length(crs)]
target=target[3:length(target)]

to_ret = distance(crs,target,smooth)

return(to_ret)

}



my_range = seq(1,6)
mypars = 4
smooth=.8

fit = optim(c(runif(mypars)),to_optim,lower=c(rep(-10,mypars)),
            upper=c(rep(10,mypars)),smooth=smooth,knots=mypars,target=target,range=my_range,
            method=""L-BFGS-B"")


par(mfrow=c(1,2))
bs(my_range,mypars) %*% fit$par %&gt;% plot

test = bs(my_range,mypars) %*% fit$par
plot(test/lag(test)-test~target)
abline(0,1)
</code></pre>
"
"0.0464572209811883","0.0472279924554862","164369","<p>I intend to use this: </p>

<pre><code>&gt;sys_ols=systemfit(A ~ B +C+D,data = inEUdata, method = ""OLS"")
&gt;sys_iv=systemfit( A ~ B+C+D, data = inEUdata, method = ""2SLS"",inst = ~ E)
&gt;hausman.systemfit(sys_iv, sys_ols)
</code></pre>

<p>Result[Error]: </p>

<pre><code>Error in crossprod(result$q, solve(result$qVar, result$q)) : error in evaluating the argument 'y' in selecting a method for function 'crossprod': Error in solve.default(result$qVar, result$q) : system is computationally singular: reciprocal condition number = 4.21018e-25
</code></pre>

<p>I am using the following code presently : </p>

<pre><code>&gt;sys_ols=systemfit(A ~ B, data = inEUdata, method = ""OLS"") 
&gt;sys_iv=systemfit(A ~ &gt;B, data = inEUdata, method = ""2SLS"",inst = ~ E)
&gt;hausman.systemfit(sys_iv, sys_ols)
</code></pre>

<p>Result:</p>

<pre><code>Hausman specification test for consistency of the 3SLS estimation 

data:  inEUdata
Hausman = 1.9046, df = 2, p-value = 0.3859
</code></pre>

<p>Please suggest me which code is most appropriate for the endogeneity test for multiple regression </p>
"
"0.026822089039291","0.0272670941574606","164402","<p>After generating the regression model in R using lm, the results will be passed to summary function.</p>

<pre><code>results &lt;- lm(y~x, data)
summary(results)
</code></pre>

<p>This function shows lot of information including residuals. Based on my understanding residuals are the difference b/w actual values and predicted values. How summary function is providing this value? how is it evaluating the model?</p>
"
"0.0929144419623766","0.0865846528350581","164404","<p>I am working with survival models and I am using R's coxph function to learn the Cox proportional hazard model. To try it out, I am using the standard veteran dataset (obtainable by loading the ""Survival"" library and running data(""veteran"")). The regression looks like this: </p>

<pre><code>coxph(formula = Surv(time, status) ~ ., data = veteran)


             coef exp(coef) se(coef)     z       p
trt       0.22681   1.25459  0.18811  1.21   0.228
celltype  0.12969   1.13848  0.07765  1.67   0.095
karno    -0.03533   0.96529  0.00540 -6.54 6.1e-11
diagtime  0.00216   1.00217  0.00907  0.24   0.811
age      -0.00364   0.99637  0.00915 -0.40   0.691
prior    -0.00784   0.99219  0.02228 -0.35   0.725

Likelihood ratio test=46  on 6 df, p=2.92e-08
n= 137, number of events= 128 
</code></pre>

<p>I assume, since I did not specify anything special in the call to coxph, that this is learning a Cox proportional hazards model by maximizing the partial likelihood (which is what I want). </p>

<p>The <a href=""https://en.wikipedia.org/wiki/Proportional_hazards_model"" rel=""nofollow"">Wikipedia article on Cox proportional hazards</a> models helpfully has the partial likelihood score function. At the maximum, the score should be zero (or at least near zero). I wanted to try this out to confirm that the maximum was found. I wrote some code in python to check:</p>

<pre><code>def gradient(X, y, status, beta):
    # Initialize the value of the gradient.
    val = 0
    # Iterate through the samples.
    for i in xrange(X.shape[0]):
        # For every uncensored example.
        if status[i] == 1:
            # The first part of the derivative.
            val += X[i]
            # The second part of the derivative. I initialize terms
            # beforehand to make computation easier.
            num, denom = 0, 0
            # Iterate through all the other samples.
            for j in xrange(X.shape[0]):
                # If this record had longer observation time that the
                # current uncensored example.
                if y[j] &gt;= y[i]:
                    # Do derivative computations.
                    denom += np.exp(X[j].dot(beta))
                    num += np.exp(X[j].dot(beta)) * X[j]

            # Ad append to the likelihood.
            val -= (num / denom)

    # Return the gradient (normalized by the number of uncensored events).
    return val / X.shape[0]
</code></pre>

<p>Running the gradient function with parameters close to </p>

<pre><code>(0.22, 0.12, -0.35, 0.00216, -0.00364, -0.00784) 
</code></pre>

<p>should give a gradient value close to zero. But it doesn't! It gives: </p>

<pre><code>(-43.24273213, -74.49114686, -1774.45727092, -198.37389201, -1537.26179666,-76.08746355). 
</code></pre>

<p>This is really not close to the zero vector. Hence I think I must have implemented the gradient incorrectly, but I can't find my mistake. Can anyone help?</p>

<h1>Edit 1</h1>

<p>There was a small bug in the Python program that has now been fixed. The gradient though is still too large. It is now:</p>

<pre><code>  (0.0374052    0.29217186  29.11663428   3.23544727 -10.75829199  2.333945  )
</code></pre>

<p>Which is still very far away from zero.</p>
"
"0.0464572209811883","0.0472279924554862","164434","<p>Sometimes during modeling we are faced with non-normal data.  One of the presumptions of linear regression is the response data being normal.</p>

<p><em>I wish I found a better example data set to illustrate this.  The cars data doesn't need this proposed method...</em></p>

<p>I never seen this done in a book nor in a class so most likely this is not a good idea. I am curious what others think of this method.  Is there a documented approach that works similar to this?</p>

<p>One method to make continuous response data normal is to take the rank converted to a percentile and bound it between 0.001 and 0.999. (Winsoring it.) Lastly converted it to a zscore.</p>

<pre><code>cars2=cars;
cars2$dist=qnorm(pmin(0.999,pmax(0.001,rank(cars2$dist)/length(rank(cars2$dist)))))
</code></pre>

<p>Then with the original data, making a mapping variable for the values 1:999</p>

<pre><code>map=(quantile(cars$dist,probs = seq(0.001,0.999,0.001)))
</code></pre>

<ul>
<li>skewness(cars dist) is 0.7824835 </li>
<li>skewness(cars2 dist) is 0.378929</li>
<li>rmse(cars dist,fit) is 15.06886    </li>
<li>rmse(cars dist,fit2) is 14.80842</li>
</ul>

<p>So the skew is reduced and performs slightly better.
On large data sets this method almost assuredly gets rid of the skew in the response.</p>

<p>Here is plainly the code:</p>

<pre><code>require(datasets)

hist(cars$speed)
    hist(cars$dist)
m=lm(dist~speed,cars)
fit=predict(m)
skewness(cars$dist)
summary(m)

cars2=cars;
cars2$dist=qnorm(pmin(0.999,pmax(0.001,rank(cars2$dist)/length(rank(cars2$dist)))))
    map=(quantile(cars$dist,probs = seq(0.001,0.999,0.001)))
hist(cars2$dist)
    skewness(cars2$dist)
length(map)
hist(map)
m3=lm(dist~speed,cars2); 
m3=stepAIC(m3,trace=F)
summary(m3)
data=round(pnorm(predict(m3))*1000)
range(data)
fit2=map[data]
plot(cars$dist,fit2,col=""blue"")
    points(cars$dist,fit,col=""red"")

rmse=function(x,y,k=0){
  return( sqrt(sum((x-y)^2)/(length(x)-k)));
}
rmse(cars$dist,fit)
    rmse(cars$dist,fit2)
</code></pre>

<p>So how crazy of an idea is this?
Has this approach been documented/studied before? Where?</p>

<p>Thank you for you commentary. :)</p>
"
"0.0608267804924532","0.072141950116023","164541","<p>I am attempting to do a logistic regression bootstrap with R. The problem is I get high SE's. I'm not sure what to do about this or what it means. Does it mean that bootstrap does not work well for my particular data? Here is my code:</p>

<pre><code>get.coeffic = function(data, indices){
  data    = data[indices,]
  mylogit = glm(F~B+D, data=data, family=""binomial"")
  return(mylogit$coefficients)
}

Call:
boot(data = Pres, statistic = logit.bootstrap, R = 1000)

Bootstrap Statistics :
       original      bias    std. error
t1* -10.8609610 -23.0604501  338.048398
t2*   0.2078474   0.4351766    6.387781
</code></pre>

<p>I also want to know that after bootstrapping, how would this help with my final regression model? That is, how do I find what regression coefficient do I use in my final model?</p>

<pre><code>&gt; fit &lt;- glm(F ~ B + D , data = President, family = ""binomial"")
&gt; summary(fit)
Call:
glm(formula = F ~ B + D, family = ""binomial"", data = President)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7699  -0.5073   0.1791   0.8147   1.2836  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -14.57829    8.98809  -1.622   0.1048  
B             0.15034    0.14433   1.042   0.2976  
D             0.13385    0.08052   1.662   0.0965 .
- --
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 23.508  on 16  degrees of freedom
Residual deviance: 14.893  on 14  degrees of freedom
AIC: 20.893

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.053644178078582","0.0545341883149212","164648","<p>I have created a Logistic Regression using the following code:</p>

<pre><code>full.model.f = lm(Ft_45 ~ ., LOG_D)
base.model.f = lm(Ft_45 ~ IP_util_E2pl_m02_flg)
step(base.model.f, scope=list(upper=full.model.f, lower=~1),
     direction=""forward"", trace=FALSE)
</code></pre>

<p>I have then used the output to create a final model:</p>

<pre><code>final.model.f = lm(Ft_45 ~ IP_util_E2pl_m02_flg + IP_util_E2_m02_flg + 
                           AE_NumVisit1_flg + OP_NumVisit1_m01_flg + IP_TotLoS_m02 + 
                           Ft1_45 + IP_util_E1_m05_flg + IP_TotPrNonElecLoS_m02 + 
                           IP_util_E2pl_m03_flg + LTC_coding + OP_NumVisit0105_m03_flg +
                           OP_NumVisit11pl_m03_flg + AE_ArrAmb_m02_flg)
</code></pre>

<p>Then I have predicted the outcomes for a different set of data using the predict function:</p>

<pre><code>log.pred.f.v &lt;- predict(final.model.f, newdata=LOG_V)
</code></pre>

<p>I have been able to use establish a pleasing ROC curve and created a table to establish the sensitivity and specificity which gives me responses I would expect. </p>

<p>However What I am trying to do is establish for each row of data what the probability is of Ft_45 being 1. If I look at the output of log.pred.f.v I get, for example,:</p>

<pre><code>1 -0.171739593    
2 -0.049905948    
3 0.141146419    
4 0.11615669    
5 0.07342591    
6 0.093054334    
7 0.957164383    
8 0.098415639    
.
.
.
104 0.196368229    
105 1.045208447    
106 1.05499112
</code></pre>

<p>As I only have a tentative grasp on what I am doing I am struggling to understand how to interpret the negative and higher that 1 values as I would expect a probability to be between 0 and 1.</p>

<p>So my question is am I just missing a step where I need to transform the output or have I gone completely wrong.
Thank you in advance for any help you are able to offer.</p>
"
"0.0464572209811883","0.0472279924554862","164973","<p>I am trying to fit a survival analysis in <code>R</code> with non-recurrent events and time-varying coefficients. The baseline distribution is exponential or Weibull and the frailty distribution is gamma distributed. I have roughly 900.000 rows. </p>

<p>So far I have tried the <code>parfm</code> and <code>frailtypack</code>. Though, neither has worked â€“ they just keep running and never return. The calls for <code>parfm</code> and <code>frailtypack</code> are similar to respectively: </p>

<pre><code>frailtyPenal(Surv(stop-start,event)~.-start-stop-event-year+cluster(temp$year), 
             data= regressionData, hazard=""Weibull"", RandDist=â€Gammaâ€)

parfm(Surv(stop-start,event)~.-start-stop-event-year,cluster=â€yearâ€, 
      regressionData, dist=""exponential"", frailty = ""gamma"")
</code></pre>

<p>Where <code>event</code> is zero-one coded. My guess so far is to use the <code>lme4</code> package with the function <code>glmer</code> where the family is Poisson, the respond are zero-one coded, the offset is the difference in time and random effect is an intercept for the year factor. I.e. something like:</p>

<pre><code>glmer(event ~.-start-stop-event-year+(1|year), family = Poisson(), offset=stop-start)
</code></pre>

<p>I know that this will yield Gaussian distributed random effects and not Gamma. Further, I am not sure that I get the model I want. My goal is to have exponential distributed conditional waiting times and hence I chose the Poisson distribution. Question is whether this is correct? Any suggestions on other packages that will do the job?</p>
"
"0.0379321620905441","0.0385614943639849","165056","<p>My data is in a numeric matrix of RNA-seq data from Illumina 2000 platform (with proper alignment and other preprocessing done), where columns represent subjects, and rows represent raw expression counts of genes. My goal is to use the normalized matrix for further regression etc. analyses (with other tools than edgeR).
I wrote a function to do this:</p>

<pre><code>##getNormalized matrix
##input: numeric matrix
##output: numeric matrix with normalized counts
##requires edgeR package
getNormalizedMatrix &lt;- function(M){
  require(edgeR)
  norm.factors &lt;- calcNormFactors(M, method = ""TMM"")
  return(equalizeLibSizes(DGEList(ah, norm.factors = norm.factors))$pseudo.counts)
}
</code></pre>

<p>Is this the way I am supposed to do the TMM-normalization?</p>
"
"0.0599760143904067","0.0609710760849692","166485","<p>I created for the following data set a multiple regression. Now I would like to forecast the next 20 data points.</p>

<pre><code>&gt; dput(datSel)
structure(list(oenb_dependent = c(1.0227039, -5.0683144, 0.6657713, 
3.3161374, -2.1586704, -0.7833623, -0.2203209, 2.416144, -1.7625406, 
-0.1565037, -7.9803936, 9.4594715, -4.8104584, 8.4827107, -6.1895262, 
1.4288595, 1.4896459, -0.4198522, -5.1583964, 5.2502294, 1.0567102, 
-1.0923342, -1.5852298, 0.6061936, -0.3752335, 2.5008664, -1.3999729, 
2.2802166, -2.1468756, -1.4890328, -0.79254376, 3.21804705, -0.94407886, 
-0.27802316, -0.20753079, -1.12610048, 2.0883735, -0.7424854, 
0.44203729, -1.48905938, 1.39644424, -3.8917377, 11.25665848, 
-9.22884035, 3.26856762, -0.00179541, -2.39664325, 4.00455574, 
-5.60891295, 4.6556348, -4.40536951, 6.64234497, -7.34787319, 
7.56303006, -8.23083674, 4.43247855, 1.31090412), carReg = c(0.73435946, 
0.24001161, 16.90532537, -14.60281976, 6.47603166, -8.35815849, 
3.55576685, 7.10705794, -4.6955223, 10.9623709, 5.5801857, -6.4499936, 
-9.46196502, 9.36289122, -8.52630424, 5.45070994, -4.5346405, 
-2.26716538, 2.56870398, 0.013737, 5.7750101, -27.1060826, 1.08977179, 
4.94934712, 17.55391859, -13.91160577, 10.38981128, -11.81349246, 
-0.0831467, 2.79748237, 1.84865463, -1.98736934, -6.24191695, 
13.33602659, -3.86527871, 0.78720993, 4.73360651, -4.1674034, 
9.37426802, -5.90660464, -0.4915792, -5.84811629, 9.67648643, 
-6.96872719, -7.6535767, 0.24847595, 0.18685263, -2.28766949, 
1.1544631, -3.87636933, -2.4731545, 4.33876671, 1.08836339, 5.64525271, 
1.90743854, -3.94709355, -0.84611324), cpi = c(1.16, -3.26, 0.22, 
-3.51, 0.84, -2.81, -0.34, -4.57, -0.12, -3.95, -1.37, -2.73, 
0.35, -5.38, -4.43, -3.08, 0.74, -3.03, -1.09, -2, 0.35, -1.52, 
1.28, 0.2, -0.25, -4.55, -2.49, -4.24, -0.31, -2.96, -2.24, -0.46, 
-0.06, -2.67, -1.27, -1.4, -0.7, -0.96, -2.18, -2.53, -0.52, 
-1.74, -2.18, -1.4, -0.34, -0.09, -1.65, -1.15, -0.17, -2.01, 
-1.38, -1.24, 0.09, -2.44, -1.92, -2.61, -0.34), primConstTot = c(-0.33334, 
-0.93333, -0.16667, -0.33333, -0.16667, -0.86666, -0.3, -0.4, 
-0.26667, -1.56667, -0.73333, 0.1, -0.23333, -0.26667, -1.5774, 
-0.19284, 0.38568, -2.42423, -0.93663, 0.08265, -0.63361, 0.0551, 
-0.49587, 2.39668, -1.70798, -3.36085, -2.56196, 0.16529, 0, 
-1.84572, -1.3774, -0.49586, -1.70798, -1.90081, -0.55096, -0.77134, 
-0.16529, -0.30303, -0.17066, -0.23853, -0.64401, -1.52657, -1.57426, 
-0.28623, -0.54861, -1.07336, -0.71558, 0.02385, -0.38164, -1.09721, 
0, 0.14311, -0.38164, -1.02566, -0.42934, -0.35779, -0.4532), 
    resProp.Dwell = c(0.8, -4, -3.2, 2.7, -1.6, -1, -2.4, -0.4, 
    -0.8, 1, -12.1, 0.2, -5.2, 3.7, -2.7, -1.7, 1.5, 0.7, -7.9, 
    0.3, 0.3, 1.4, -3.3, -1, -1.6, 1.5, 0.5, 1.5, -1, -2.2, -3.5, 
    0.5, 0.5, -0.9, -0.4, -3.4, 0.9, 0.1, -0.2, -2.8, -0.8, -6.2, 
    11.3, -4.6, 1, 1.1, -1.7, 4.1, -5, 2.3, -2.3, 4.6, -6.3, 
    6.3, -6.9, 0, 2.4), cbre.office.primeYield = c(0, 0, 0.15, 
    0.15, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.2, 0.15, 0.1, 
    0.05, 0.15, 0.3, 0.35, 0.4, 0.3, 0.2, 0, -0.15, -0.85, -1, 
    -0.85, -0.75, -0.1, 0, 0, 0, 0.05, 0.05, 0.05, 0.05, 0, 0, 
    0, 0.2, 0.2, 0.2, 0.2, 0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25, 
    0, 0, 0, 0, 0, 0, 0), cbre.retail.capitalValue = c(-1882.35294, 
    230.76923, -230.76923, -226.41509, -670.78117, -436.13707, 
    -222.22223, 0, -205.91233, -202.16847, 0, -393.5065, -403.91909, 
    -186.30647, -539.81107, -748.11463, -764.70588, -311.47541, 
    -301.42782, -627.09677, -480, 720, 782.6087, 645.96273, 251.42857, 
    1386.66667, -533.33334, -533.33333, -533.33333, 0, 0, -1024.56141, 
    -192.10526, 0, -730, 0, 0, 0, 0, 0, -834.28571, 0, -1450.93168, 
    0, 0, 0, -700.78261, 0, 0, 0, 0, 0, 0, 0, -1452, 0, 0)), .Names = c(""oenb_dependent"", 
""carReg"", ""cpi"", ""primConstTot"", ""resProp.Dwell"", ""cbre.office.primeYield"", 
""cbre.retail.capitalValue""), row.names = c(NA, -57L), class = ""data.frame"")
&gt; 
&gt; fit &lt;- lm(oenb_dependent ~ carReg + cpi + primConstTot + 
+             resProp.Dwell + cbre.office.primeYield + cbre.retail.capitalValue , data = datSel)
&gt; summary(fit) # show results

Call:
lm(formula = oenb_dependent ~ carReg + cpi + primConstTot + resProp.Dwell + 
    cbre.office.primeYield + cbre.retail.capitalValue, data = datSel)

Residuals:
   Min     1Q Median     3Q    Max 
-5.166 -1.447 -0.162  1.448  7.903 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               0.831630   0.492297    1.69    0.097 .  
carReg                    0.085208   0.039600    2.15    0.036 *  
cpi                      -0.349192   0.212044   -1.65    0.106    
primConstTot              0.752772   0.383810    1.96    0.055 .  
resProp.Dwell             0.994356   0.086812   11.45  1.4e-15 ***
cbre.office.primeYield    1.274734   1.212782    1.05    0.298    
cbre.retail.capitalValue  0.000528   0.000643    0.82    0.416    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.24 on 50 degrees of freedom
Multiple R-squared:  0.754, Adjusted R-squared:  0.725 
F-statistic: 25.6 on 6 and 50 DF,  p-value: 1.2e-13
</code></pre>

<p>I tried the following:</p>

<pre><code>vals.multipleRegr &lt;- forecast(fit, h = 20)
Error: could not find function ""forecast""
</code></pre>

<p>However, this does not work as the function forecast cannot be found. I am using the following packages in my code, <code>library(bootstrap)</code>, <code>library(DAAG)</code> and <code>library(relaimpo)</code>. </p>

<p>Any suggestion how to forecasting using multiple regression?</p>

<p>I appreciate your replies!</p>
"
"0.0808716413062113","0.0822133822214443","166779","<p>Iâ€™ve seen some papers that present the idea of training classifiers such as logistic regression that are really meant to optimize a custom cost model (such as by maximizing profit given expect revenues for predictions depending on whether they are false positives, true negatives, true positives, or true negatives) not by optimizing the typical log-loss function and then looking for the optimal decision cut-off threshold, but by using different loss functions that weight differently the costs of each classification type or of each misclassification type (although I've seen that different authors propose different functions), and these seem to provide better results when evaluating them based on the customly-defined cost function.</p>

<p>I was wondering if there are any implementations of such methods in R. Particularly, I'd like to try fitting a logistic regression treating the cost of misclassifying as false positive to be a multiple of the cost of misclassifying as false negative. I found a package that does just this for decision trees (although in that case it's based on the class proportions on the leaves rather than something like log-loss) and I see that there are some options for observation-specific weights in logistic regression, but not for error type weights.</p>
"
"0.0758643241810882","0.0771229887279699","166953","<p><strong>Issue</strong>: Cannot forecast sales accurately using quantile regression in R. I am using rq function from ""quantreg"" package which is giving me warning ""Result might have Non unique solutions""</p>

<p><strong>Aim</strong>: I am trying to forecast hourly sales of a store using quantile regression. </p>

<p>Below are the columns in my source table for forecasting.</p>

<ul>
<li><em>transaction_date</em> : sales date (input)</li>
<li><em>hr1 to hr24</em> : column with hourly sales info. (24 columns) (input)</li>
<li><em>totala</em> : total of 24 column hr1 to hr24 (not using currently)</li>
<li><em>location, department, sales_type</em>: forecasting will be done for each location, sales_type and department. (used to select data)</li>
<li><em>f1 to f24 :</em> columns I want to forecast for each hour (24 columns) (output)</li>
</ul>

<p>Packages Used: forecast, quantreg, Metrics</p>

<p><strong>Code</strong>: 
I have extracted date features from transaction_date eg. weekend, week of month and also holidays (1 if it is holiday 0 for regular days).</p>

<pre><code>attach(train_data) 
Y &lt;- cbind(hr) 
X &lt;- cbind(transation_date, Years, Months, Days, WeekDay, WeekofYear, Weekend, WeekofMonth, holidays) 

quantreg.all &lt;- rq(Y ~ X, tau = seq(0.05, 0.95, by = 0.05))
prediction_train &lt;- data.frame(predict(quantreg.all))
</code></pre>

<p>I have 19 models in prediction_train for each tau from 0.05 to 0.95, I select best model based on rmse value and than forecast using that tau.</p>

<pre><code>rmse(actual, predicted)
</code></pre>

<p>transaction_date is Date type, quantreg.all is rqs class and rest are numeric.</p>

<p><strong>Note:</strong> Stores are not open 24 hours, hence many hour columns will be 0 (time when store was close). Currently for most of such hours rq is predicting 0 or some negative values.</p>

<p>Weather  does not have major impact on sales.</p>
"
"0.0929144419623766","0.0865846528350581","167363","<p>I have no training in Bayesian data analysis, so I can't wrap my head around how to start solving the following problem and am hoping you can help:</p>

<p>I am using linear regression to forecast the net scores (home - visitor) of (American) pro-football games from differences in team-strength scores (home - visitor). Those strength scores fall on a 0-100 scale, and they represent the percent chance that the team in question would beat another team selected at random from the 31 others in the league. The differences between those strength scores and the net game scores are both normally distributed.</p>

<p>Right now, I am using team-strength scores that are fixed for the entire season in a mixed-effects model that also includes random intercepts for each team as the home team. The strength scores are fixed because they come from a preseason survey. I would like to see if I can make the predictions more accurate by using Bayesian updating to allow that team-strength score to vary over the course of the season, as we learn more about how teams are performing relative to preseason expectations.</p>

<p>The single piece of information that strikes me as most useful in that regard is the cumulative sum of each team's prediction errors --- in other words, the cumulative sum of the differences between the team's predicted game performance (based on the preseason strength scores and where each game is played) and its actual game performance. </p>

<p>How might I go about doing that? In R, I have gotten as far as computing those cumulative errors, which turn out to be normally distributed for the season with a mean of ~0 and sd of ~50. I have tinkered with algebraic ways to adjust the strength scores as a function of that cumulative error. The forecasts based on those algebraic adjustments are slightly more accurate, but the approach seems clunky, and I'd like to use this problem as an opportunity to learn about Bayesian updating if I can. Any suggestions on how to do that in the context of this problem --- and, ideally, in R --- would be much appreciated.</p>
"
"0.0599760143904067","0.0609710760849692","167440","<p>I am trying to get the bootstrapped confidence intervals of the coefficients for an ordinal logistic regression. 
Here below, my R code on fake data (reproducible example here below). This one does not work properly.</p>

<p>I suppose I need to enter a list of data with one line for each of the 20 subjects (this is the most simple way to proceed). Then the bootstrap with randomly select 20 rows (using sampling with replacement) to generate a new data set with 20 rows. That data set is converted into a new table of counts and a coefficient value is computed from that new â€œbootstrappedâ€ table.  This is repeated for each bootstrap sample. I can't get it! Thanks for your help.</p>

<pre><code>####################
library(rms)
x=c(1,2,3,2,3,1,2,3,3,3,2,2,1,2,1,2,3,2,1,2)
y=c(""math"",""eco"",""eco"",""lit"",""lit"",""eco"",""eco"",""math"",""math"",""lit"",""lit"",""math"",""eco"",""eco"",""math"",""lit"",""lit"",""math"",""eco"",""math"")
Dataset&lt;-data.frame(x,y)
h &lt;- orm(x ~ y)
h

# calculate coefficients using bootstrap
library(boot)
logit.bootstrap &lt;- function(data, indices) {
d&lt;-data[indices,]
fit&lt;-orm(x ~ y, data=data[indices,])
return(coefficients(fit))
}

# bootstrapping with 1000 replications
logit.boot &lt;- boot(data=Dataset, statistic=logit.bootstrap,R=1000)

# view results
logit.boot
plot(logit.boot)

# get 95% confidence interval
boot.ci(logit.boot, type=""all"")
############################
</code></pre>
"
"0.0709645772411954","0.0515299643685879","167702","<p>I am trying to fit a VARMAX (vector autoregressive moving-average with exogenous variables) model to some synthetically generated data using the <em>MTS</em> library available in R. I found that there is only one function for fitting models with exogenous variables, it is designed for only VAR models and is called <em>VARX</em>. Reading in the literature, I found that there is a method for finding the VARX representation of the VARMAX model (not that straightforward), thus, finding the VARX representation of the VARMAX model, the <em>VARX</em> function could be used. <strong>My question is:</strong> Is there any method already implemented in <em>R</em> that transforms VARMAX, VARMA, VMAX into their corresponding VAR representation or how the <em>VARX</em> function must be used for fitting models of this type?</p>

<p>In the following reproducible example is generated an VARMAX model intending to estimate their parameters with <em>VARX</em> function:</p>

<pre><code>library(MTS)
set.seed(2015)
Phi&lt;-matrix(c(0.5,0.2,0.2,0.3),ncol=2)
Theta&lt;-matrix(c(0.4,0.3,0.2,0.5),ncol=2)
Sigma &lt;- matrix(c(1,0.2,0.2,1),ncol=2)
serie_varmax&lt;-VARMAsim(60,arlags=c(1),malags=c(1),phi=Phi,theta=Theta,sigma=Sigma)$series
exoge1&lt;-serie_varmax[,1]+(0.3*seq(1,60)) #Exogenous variable influencing only the first variable
serie1&lt;-cbind(exoge1,serie_varmax[,2])
</code></pre>

<p>As aforementioned, I am wondering how to estimate the VARMAX coefficients using <em>VARX</em> function.</p>
"
"0.100582833897341","0.102251603090477","168167","<p>My dependent variable is a probability. As such, values lie between 0 and 1. The most common values are 0, 0.5, and 1 each occurring in 20% to 30% of the observations but any value in between is possible and some do occur. </p>

<p><strong>Question 1: Which regression model is best to explain such data?</strong></p>

<ul>
<li><p>Ordinary least squares (OLS, function <code>lm</code> in Râ€™s <code>stats</code> package) is not suitable as it does neither account for the limited interval nor the accumulation at the margins.</p></li>
<li><p>Logit regression (function <code>glm</code> with parameter <code>family=""binomial""</code> in Râ€™s <code>stats</code> package) accounts for the accumulation at 0 and 1 but does not allow intermediate values.</p></li>
<li><p>Ordered logit regression (function <code>polr</code> in Râ€™s <code>MASS</code> package) could be applied when I divide the [0, 1] interval in subintervals. However, I lose the continuous nature of the dependent variable.</p></li>
<li><p>For probit and ordered probit regressions, the same applies as for logit and ordered logit.</p></li>
<li><p>Left- and right-censored tobit regression (function <code>tobit</code> with parameters <code>left=0</code> and <code>right=1</code> in Râ€™s <code>AER</code> package) might be appropriate. However, I found the following quote: â€œSome researchers have considered using censored normal regression techniques such as tobit ([R] tobit) on proportions data that contain zeros or ones. However, this is not an appropriate strategy, as the observed data in this case are not censored: values outside the [0, 1] interval are not feasible for proportions data.â€ (p. 302 in Baum (2008), <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0147"" rel=""nofollow"">http://www.stata-journal.com/sjpdf.html?articlenum=st0147</a>). </p></li>
</ul>

<p>Below you find a code example </p>

<pre><code># Load libraries
library(stats, MASS, AER)
# Generate data
set.seed(123)
data &lt;- data.frame(x1 &lt;- runif(60, min = 0, max = 1), x2 &lt;- runif(60, min = 0, max = 1))
data$y  &lt;- -0.7 + data$x1 + 2 * data$x2 + rnorm(60, mean = 0, sd = 0.5)
    data$y  &lt;- ifelse(data$y &lt; 0, 0, data$y)
data$y  &lt;- ifelse(data$y &gt; 0.4 &amp; data$y &lt; 0.6, 0.5, data$y)
data$y  &lt;- ifelse(data$y &gt; 1, 1, data$y)
    data$yCat &lt;- data$y
    data$yCat &lt;- ifelse(data$yCat &gt; 0 &amp; data$yCat &lt; 0.5, 0.25, data$yCat)
    data$yCat &lt;- ifelse(data$yCat &gt; 0.5 &amp; data$yCat &lt; 1, 0.75, data$yCat)
    data$yCat &lt;- as.factor(data$yCat)
    hist(data$y, breaks=101)
# Different regression models
summary(lm(y ~ x1 + x2, data=data)) # OLS
summary(glm(y ~ x1 + x2, data=data, family=""binomial"")) # Logit
summary(polr(yCat ~ x1 + x2, data=data)) # Ordered logit
summary(tobit(y ~ x1 + x2, data=data, left=0, right=1)) # Tobit
</code></pre>

<p>To make matters worse, my data is panel data. I know how to handle individual, time, and mixed effects and random and fixed effects models using plm from Râ€™s plm package and F-test, LM-test, and Hausman test do decide which of these is best. </p>

<p><strong>Question 2: For the dependent variable described above, which panel regression model is best?</strong> </p>

<p>Below your find a code example for the data structure. This extends the prior example.</p>

<pre><code># Load library
library(plm)
# Generate data (builds on prior example)
data$id &lt;- rep( paste( ""F"", 1:15, sep = ""_"" ), each = 4)
    data$time &lt;- rep( 1981:1984, 15 )
pData &lt;- pdata.frame(data, c( ""id"", ""time"" ))
# Panel regression example
summary(plm(y ~ x1 + x2, data=pData, model=""within"", effect=""twoways"")) # Based on OLS
</code></pre>
"
"0.0599760143904067","0.0609710760849692","168308","<p>I am currently performing a meta-analysis on bank relationship and firm performance with 27 different studies; almost all of them report different cases.</p>

<p>I calculated the partial effect size (I have different dependent and independent variable), its variance and confidence interval using the formula provided by Aloe &amp; Thompson, ""The synthesis of partial effect size"".</p>

<p>Due to the studies-specifics I have now to face two issues:
- Studies have different sample size and different specification, so heteroskedasticity is likely to arise;
- Studies present several cases, so some of the observation in my meta-analysis are not independent;</p>

<p>I thought to face the second issue by using what in econometrics would be called an ""study-fixed effect regression"" that, if I got correctly, in the metafor package should be:</p>

<pre><code>  res &lt;- rma(Yi, Vi, mods = ~ I(study))
  res
</code></pre>

<p>That is, a Mixed-Effects Model. I would also like to compare these results with the one of a multilevel\hierarchical model (Gelman &amp; Hill, 2006).</p>

<p>Is the <code>ram.mv</code> package good for that or I have to manually write the function following Gelman &amp; Hill? If it is good, how can I can calculate my Variance-Covariance Matrix considering the fact that I have different independent variable? Last, how can I account for heteroskedasticity?</p>

<p>Thank you for your time</p>
"
"0.131400863963521","0.128015062620555","168655","<p>I have got monthly data from 1993 to 2015 and would like to do forecasting on these data. I used tsoutliers package to detect the outliers, but I do not know how do I continue to forecast with my set of data .</p>

<p>This is my code:</p>

<pre><code>product.outlier&lt;-tso(product,types=c(""AO"",""LS"",""TC""))
plot(product.outlier)
</code></pre>

<p>This is my output from tsoutliers package</p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p><a href=""http://i.stack.imgur.com/qKI4N.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qKI4N.jpg"" alt=""This is my plot""></a></p>

<p>I have these warning messages as well.</p>

<pre><code>Warning messages:
1: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
2: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
3: In locate.outliers.oloop(y = y, fit = fit, types = types, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
4: In arima(x, order = c(1, d, 0), xreg = xreg) :
  possible convergence problem: optim gave code = 1
5: In auto.arima(x = c(5.77, 5.79, 5.79, 5.79, 5.79, 5.79, 5.78, 5.78,  :
  Unable to fit final model using maximum likelihood. AIC value approximated
</code></pre>

<p><strong>Doubts:</strong></p>

<ol>
<li>If I am not wrong, tsoutliers package will remove the outliers it detect and through the use of the dataset with outliers removed, it
will give us the best arima model suited for the data set, is it
correct?</li>
<li>The adjust series data set is being shifted down by a lot due to remove of the level shift,etc. Doesn't this mean that if the forecasting is done on the adjusted series, the output of the forecast will be very inaccurate, since the more recent data are already more than 12, while adjusted data shift it to around 7-8.</li>
<li>What does warning message 4 and 5 means? Does it mean it cannot do auto.arima using the adjusted series?</li>
<li>What does the [12] in ARIMA(0,1,0)(0,0,1)[12] mean? Is it just my frequency/periodicity of my dataset, which I set it to monthly? And does this also means that my data series is seasonal as well? </li>
<li>How do I detect seasonality in my data set? As from the visualisation of the time series plot, I cant see any obvious trend, and if I use the decompose function, it will assume that there is a seasonal trend? So do I just believe what the tsoutliers tell me, where there is seasonal trend, since there is MA of order 1?</li>
<li>How do I continue to do my forecasting with this data after identifying these outliers?</li>
<li><strong>How to incorporate these outliers to other forecasting models - Exponential Smoothing, ARIMA, Strutural Model, Random Walk, theta? I am sure I cannot remove the outliers since there are level shift, and if I only take adjusted series data, the values will be too small, so what do I do?</strong></li>
</ol>

<p><strong>Do I need to add these outliers as regressor in the auto.arima for forecasting? How does this work then?</strong></p>
"
"0.026822089039291","0.0272670941574606","168717","<p>this is my first post.  I have an irregular time series that exhibits large shifts in both mean and in the direction of the trend.  It looks something like this (though this is far cleaner than reality):</p>

<pre><code>set.seed(1)
x = sort(rep(seq(as.POSIXct('2015/01/01'),as.POSIXct('2015/01/10'),by='day'),100) + runif(1000,0,80000))
y = c(seq(300),seq(90,70,-.2),seq(20,50,.1),seq(238.5,90,-.5)) + runif(1000,50,80)
plot(x,y)
</code></pre>

<p>The task is to:
 1. accurately partition/segment the data
 2. extract the change point indices
 3. fit regression separately for each segment</p>

<p>I have tried several routes, including:
a) hierarchical clustering based on dissimilarity matrix
b) function cpt.mean/var/meanvar from package changepoint (does not seem to work well)
c) function 'breakpoints' from package strucchange (slow and often inaccurate)
d) various types of kmeans (inappropriate, I know)</p>

<p>I have also explored some other packages, such as TSclust, urca, and DTW, but these seem better suited to clustering <em>sets</em> of time-series, not individual values within a time series.</p>

<p>Can someone point me in the correct direction? Perhaps I am not considering the appropriate data model?</p>

<p><strong>UPDATE</strong>
Thank you all for your very considered responses.  I went back to the strucchange package, and after some fiddling, have gotten it to work quite well.  I had not initially appreciated the h 'minimal segment size' argument.
Finished product:
<a href=""http://i.stack.imgur.com/qBgeY.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qBgeY.jpg"" alt=""enter image description here""></a></p>
"
"0.0758643241810882","0.0771229887279699","168765","<p>I have a formative construct in a structural equation model (SEM) which I would like to estimate with the function <code>sem</code> in the <code>lavaan</code> package in <code>R</code>. Currently, the model is underidentified. I know about four different approaches for identifying the model</p>

<ol>
<li>Adding two reflective indicators</li>
<li>Adding two reflective constructs</li>
<li>Adding one reflective indicator and one reflective construct</li>
<li>Fixing the variance of the disturbance term to zero</li>
</ol>

<p>These modelling approaches are described in section 5.3.3 of <a href=""http://www.researchgate.net/profile/Adamantios_Diamantopoulos/publication/222652088_Advancing_formative_measurement_models/links/09e4151406c288b6b3000000.pdf"" rel=""nofollow"">http://www.researchgate.net/profile/Adamantios_Diamantopoulos/publication/222652088_Advancing_formative_measurement_models/links/09e4151406c288b6b3000000.pdf</a></p>

<p>In my case, I do not have additional reflective indicators or constructs. Thus, I would like to fix the variance of the disturbance term to zero (approach 4). I know about the downsides of this approach as described on page 13 of the paper linked above. Nevertheless, if I want to do it, how do I specify this in the <code>lavaan</code> syntax?</p>

<p>Here is a code example. It returns a note that the model is underidentified. How do I get this working? </p>

<pre><code>library(lavaan)
model &lt;- ' 
    # latent variable definitions
    ind60 =~ x1 + x2 + x3
    dem60 &lt;~ 1*y1 + y2
    # regression
    dem60 ~ ind60
    # variance
    ind60 ~~ 1*ind60
    '
summary(fit &lt;- sem(model, data=PoliticalDemocracy))
</code></pre>

<p>From <a href=""http://stats.stackexchange.com/questions/154588/how-to-use-formative-indicators-in-covariance-based-sem-with-lavaan"">How to use formative indicators in covariance-based SEM with lavaan?</a> I know that it would work if I would invert the direction of causality (<code>ind60 ~ sem60</code> instead of <code>sem60 ~ ind60</code>) or specify <code>sem60</code> as reflective construct but neither of these appraoches would fit the theoretical basis.</p>
"
"0.0309714806541255","0.0472279924554862","169334","<p>I'm trying to use the <code>circular</code> package in R to perform regression of a circular response variable and linear predictor, and I do not understand the coefficient value I'm getting. I've spent considerable time searching in vain for an explanation that I can understand, so I'm hoping somebody here may be able to help.</p>

<p>Here's an example:</p>

<pre><code>library(circular)

# simulate data
x &lt;- 1:100
set.seed(123)
y &lt;- circular(seq(0, pi, pi/99) + rnorm(100, 0, .1))

# fit model
m &lt;- lm.circular(y, x, type=""c-l"", init=0)

&gt; coef(m)
[1] 0.02234385
</code></pre>

<p>I don't understand this coefficient of 0.02 -- I would expect the slope of the regression line to be very close to pi/100, as it is in garden variety linear regression:</p>

<pre><code>&gt; coef(lm(y~x))[2]
         x
0.03198437
</code></pre>

<p>Does the circular regression coefficient not represent the change in response angle per unit change in the predictor variable? Perhaps the coefficient needs to be transformed via some link function to be interpretable in radians? Or am I thinking about this all wrong? Thanks for any help you can offer.</p>
"
"0.128634220135732","0.125082807548204","169468","<p>I have monthly time series data, and would like to do forecasting with detection of outliers .</p>

<p><strong>This is the sample of my data set:</strong></p>

<pre><code>       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
2006  7.55  7.63  7.62  7.50  7.47  7.53  7.55  7.47  7.65  7.72  7.78  7.81
2007  7.71  7.67  7.85  7.82  7.91  7.91  8.00  7.82  7.90  7.93  7.99  7.93
2008  8.46  8.48  9.03  9.43 11.58 12.19 12.23 11.98 12.26 12.31 12.13 11.99
2009 11.51 11.75 11.87 11.91 11.87 11.69 11.66 11.23 11.37 11.71 11.88 11.93
2010 11.99 11.84 12.33 12.55 12.58 12.67 12.57 12.35 12.30 12.67 12.71 12.63
2011 12.60 12.41 12.68 12.48 12.50 12.30 12.39 12.16 12.38 12.36 12.52 12.63
</code></pre>

<p>I have referred to <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r?lq=1"">Timeseries analysis procedure and methods using R</a>, to do a series of different model of forecasting, however it does not seems to be accurate. In additional, I am not sure how to incorporate the tsoutliers into it as well.</p>

<p>I have got another post regarding my enquiry of tsoutliers and arima modelling and procedure over <a href=""http://stats.stackexchange.com/questions/168655/how-to-interpret-and-do-forecasting-using-tsoutliers-package-and-auto-arima/168869#168869"">here</a> as well.</p>

<p>So these are my code currently, which is similar to link no.1.</p>

<p><strong>Code:</strong></p>

<pre><code>product&lt;-ts(product, start=c(1993,1),frequency=12)

#Modelling product Retail Price

#Training set
product.mod&lt;-window(product,end=c(2012,12))
#Test set
product.test&lt;-window(product,start=c(2013,1))
#Range of time of test set
period&lt;-(end(product.test)[1]-start(product.test)[1])*12 + #No of month * no. of yr
(end(product.test)[2]-start(product.test)[2]+1) #No of months
#Model using different method
#arima, expo smooth, theta, random walk, structural time series
models&lt;-list(
#arima
product.arima&lt;-forecast(auto.arima(product.mod),h=period),
#exp smoothing
product.ets&lt;-forecast(ets(product.mod),h=period),
#theta
product.tht&lt;-thetaf(product.mod,h=period),
#random walk
product.rwf&lt;-rwf(product.mod,h=period),
#Structts
product.struc&lt;-forecast(StructTS(product.mod),h=period)
)

##Compare the training set forecast with test set
par(mfrow=c(2, 3))
for (f in models){
    plot(f)
    lines(product.test,col='red')
}

##To see its accuracy on its Test set, 
#as training set would be ""accurate"" in the first place
acc.test&lt;-lapply(models, function(f){
    accuracy(f, product.test)[2,]
})
acc.test &lt;- Reduce(rbind, acc.test)
row.names(acc.test)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.test &lt;- acc.test[order(acc.test[,'MASE']),]

##Look at training set to see if there are overfitting of the forecasting
##on training set
acc.train&lt;-lapply(models, function(f){
    accuracy(f, product.test)[1,]
})
acc.train &lt;- Reduce(rbind, acc.train)
row.names(acc.train)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.train &lt;- acc.train[order(acc.train[,'MASE']),]

 ##Note that we look at MAE, MAPE or MASE value. The lower the better the fit.
</code></pre>

<p>This is the plot of my different forecasting, which doesn't seem very reliable/accurate, through the comparison of the red""test set"", and blue""forecasted"" set.
<strong>Plot of different forecast</strong>
<a href=""http://i.stack.imgur.com/WZSNq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WZSNq.jpg"" alt=""Different forecast""></a></p>

<p><strong>Different accuracy of the respective models of test and training set</strong></p>

<pre><code>Test set
                    ME      RMSE       MAE        MPE     MAPE      MASE      ACF1 Theil's U
theta      -0.07408833 0.2277015 0.1881167 -0.6037191 1.460549 0.2944165 0.1956893 0.8322151
expsmooth  -0.12237967 0.2681452 0.2268248 -0.9823104 1.765287 0.3549976 0.3432275 0.9847223
randomwalk  0.11965517 0.2916008 0.2362069  0.8823040 1.807434 0.3696813 0.4529428 1.0626775
arima      -0.32556886 0.3943527 0.3255689 -2.5326397 2.532640 0.5095394 0.2076844 1.4452932
struc      -0.39735804 0.4573140 0.3973580 -3.0794740 3.079474 0.6218948 0.3841505 1.6767075

Training set
                     ME      RMSE       MAE         MPE     MAPE      MASE    ACF1 Theil's U
theta      2.934494e-02 0.2101747 0.1046614  0.30793753 1.143115 0.1638029  0.2191889194        NA
randomwalk 2.953975e-02 0.2106058 0.1050209  0.31049479 1.146559 0.1643655  0.2190857676        NA
expsmooth  1.277048e-02 0.2037005 0.1078265  0.14375355 1.176651 0.1687565 -0.0007393747        NA
arima      4.001011e-05 0.2006623 0.1079862 -0.03405395 1.192417 0.1690063 -0.0091275716        NA
struc      5.011615e-03 1.0068396 0.5520857  0.18206018 5.989414 0.8640550  0.1499843508        NA
</code></pre>

<p>From the models accuracy, we can see that the most accurate model would be theta model.
I am not sure why is the forecast very inaccurate, and I think that one of the reasons would be that, I did not treat the ""outliers"" in my data set, resulting in a bad forecast for all model.</p>

<p><strong>This is my outliers plot</strong></p>

<p><strong>Outliers Plot</strong>
<a href=""http://i.stack.imgur.com/bZDQv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bZDQv.jpg"" alt=""Outliers""></a></p>

<p><strong>tsoutliers output</strong></p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p>I would like to know how can I further ""analyse""/forecast my data, with these relevant data set and detection of outliers, etc.
Please do help me in treatment of my outliers as well to do my forecasting as well . </p>

<p>Lastly, I would like to know how to combined the different model forecasting together, as from what @forecaster had mentioned in link no.1, combining the different model will most likely result in a better forecasting/prediction.</p>

<p><strong>EDITED</strong></p>

<p>I would like to incorporate the outliers in other models are well.</p>

<p>I have tried some codes, eg. </p>

<pre><code>forecast.ets( res$fit ,h=period,xreg=newxreg)
    Error in if (object$components[1] == ""A"" &amp; is.element(object$components[2], : argument is of length zero

forecast.StructTS(res$fit,h=period,xreg=newxreg)
Error in predict.Arima(object, n.ahead = h) : 'xreg' and 'newxreg' have different numbers of columns
</code></pre>

<p>There are some errors produced, and I am unsure about the correct code to incorporate the outliers as regressors.
Furthermore, how do I work with thetaf or rwf, as there are no forecast.theta or forecast.rwf?</p>
"
"0.103881504159667","0.0985646681332268","171325","<p>I am trying to fit a logistic regression model in R to classify a y variable as either 0 or 1. I have a dataset of around 2000 observations and decided to split it in half (training and testing).</p>

<p>After having decided which variables to include in my model, I subset the data and fitted the logistic regression as follows:</p>

<pre><code>clf &lt;- glm(y~.,data=df,family='binomial')
summary(clf)
</code></pre>

<p>Then, I tested the classifier on the testing set (1000 observations) and got 0.75 accuracy score.</p>

<pre><code>results &lt;- ifelse(predict(model,testdf,type='response') &gt; 0.5,1,0)
error &lt;- mean(r_results != results)
print(1-error) #prints out 0.74984
</code></pre>

<p>After this step, I decided to crossvalidate using the boot package</p>

<pre><code>library(boot)

# K-fold CV
error_cv = NULL

# Cost function for binary variable (as suggested by the R documentation)
cost &lt;- function(r, pi = 0) mean(abs(r-pi) &gt; 0.5)


for(i in 1:10)
{
    error_cv[i] &lt;- cv.glm(df,clf,cost,K=10)$delta[1]
}

error_cv
</code></pre>

<p>now, here is where I encounter a problem:</p>

<p>K-fold cross validation as I understand it, does the following (quote from Wikipedia):
""In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data.""</p>

<p>However, how come that cv.glm() gets as argument my already fitted model? I don't understand what it is doing. Furthermore, if the data argument is equal to the training set, I get error rates of arount 0.2 whereas if I set data=testdf I get error rates of around 0.4. Since the two sets, df and testdf, have been splitted randomly, I cannot explain this large difference and I cannot explain why cv.glm() does not (apparently) do the fit and test process it is supposed to do.</p>

<p>What am I missing?</p>
"
"0.0328502159908802","0.0445269783028019","171785","<p>I am running multiple linear regression with categorical variables and I need confidence interval 95% for standardized regression coefficient. I searched around and found 2 methods:</p>

<ol>
<li><p>Using the <code>QuantPsyc</code> package, with the function <code>lm.beta</code>. However, using <code>lm.beta</code> I can only get the standardized coefficients whereas I need with their 95% CI too. Is there a way?</p></li>
<li><p>To extract standardized regression coefficient, first standardize all the variables involved, and then run it in linear regression then you'll get estimates for standardized coefficients.</p></li>
</ol>

<p>So here is my model:</p>

<pre><code>model1 &lt;- lm(Life_Satisfaction ~ Subjective + Age + Sex + CountryCat11 + 
                                 CountryCat12 + CountryCat13 + CountryCat14 + 
                                 CountryCat15 + CountryCat16 + CountryCat17 + 
                                 CountryCat18 + CountryCat19 + CountryCat20 + 
                                 CountryCat23 + CountryCat25 + CountryCat28 + 
                                 CountryCat29 + CountryCat30 + Education_ISCED1 + 
                                 Education_ISCED2 + Education_ISCED3 + 
                                 Education_ISCED4 + Education_ISCED5 + 
                                 Education_ISCED6 + Education_stillinschool + 
                                 Education_None + Education_other, data=lifesat)

lm.beta (model1)
</code></pre>

<p>I ran that, but I cannot get the 95% CI.</p>

<p>So I tried the scale method:</p>

<pre><code>model2 &lt;- lm(scale(Life_Satisfaction) ~ scale(Subjective) + scale(Age) + 
                                        scale(Sex) + scale(CountryCat11) + 
                                        scale(CountryCat12) + scale(CountryCat13) + 
                                        scale(CountryCat14) + scale(CountryCat15) + 
                                        scale(CountryCat16) + scale(CountryCat17) + 
                                        scale(CountryCat18) + scale(CountryCat19) + 
                                        scale(CountryCat20) + scale(CountryCat23) + 
                                        scale(CountryCat25) + scale(CountryCat28) + 
                                        scale(CountryCat29) + scale(CountryCat30) + 
                                    scale(Education_ISCED1) + scale(Education_ISCED2) + 
                                    scale(Education_ISCED3) + scale(Education_ISCED4) + 
                                    scale(Education_ISCED5) + scale(Education_ISCED6) + 
                               scale(Education_stillinschool) + scale(Education_None) + 
                                        scale(Education_other), data=lifesat)

summary(model2)
</code></pre>

<p>I ran that, and I got the standardized regression and 95% CI but it was different from the standardized regression results I got from SPSS? Did I do it wrong?</p>
"
"0.0657004319817604","0.0556587228785024","172087","<p>I have a dataframe <code>df</code> (see below):</p>

<pre><code>dput(df)
structure(list(x = c(49, 50, 51, 52, 53, 54, 55, 56, 1, 2, 3, 
4, 5, 14, 15, 16, 17, 2, 3, 4, 5, 6, 10, 11, 3, 30, 64, 66, 67, 
68, 69, 34, 35, 37, 39, 2, 17, 18, 99, 100, 102, 103, 67, 70, 
72), y = c(2268.14043972082, 2147.62290922552, 2269.1387550775, 
2247.31983098201, 1903.39138268307, 2174.78291538358, 2359.51909126411, 
2488.39004804939, 212.851575751527, 461.398994384333, 567.150629704352, 
781.775113821961, 918.303706148872, 1107.37695799186, 1160.80594193377, 
1412.61328924168, 1689.48879626486, 260.737164468854, 306.72700499362, 
283.410379620422, 366.813913489692, 387.570173754128, 388.602676983443, 
477.858510450125, 128.198042456082, 535.519377609133, 1028.8780498564, 
1098.54431357711, 1265.26965941035, 1129.58344809909, 820.922447928053, 
749.343583476846, 779.678206156474, 646.575242339517, 733.953282899613, 
461.156280127354, 906.813018662913, 798.186995701282, 831.365377249207, 
764.519073183124, 672.076289062505, 669.879217186302, 1341.47673353751, 
1401.44881976186, 1640.27575962036)), .Names = c(""x"", ""y""), row.names = c(NA, 
-45L), class = ""data.frame"")
</code></pre>

<p>I have created on a non-linear regression (nls) based on my dataset.</p>

<pre><code>nls1 &lt;- nls(y~A*(x^B)*(exp(k*x)), 
            data = df, 
            start = list(A = 1000, B = 0.170, k = -0.00295), algorithm = ""port"")
</code></pre>

<p>I then computed a bootstrap for this function to get multiple sets of parameters (A,B and k) and created a dataframe which contains the different set of parameters. </p>

<pre><code>Boo &lt;- nlsBoot(nls1, niter = 200)
Param_Boo &lt;- Boo$coefboot
</code></pre>

<p>I now want to plot in the same plot all the 200 possible gamma functions created where their parameters have been computed from the bootstrap. I have only managed to plot one function out of the 200 possibilities so far (see below).</p>

<pre><code>ggplot(df, aes(x, y)) +
    geom_point()+
    geom_line(aes(y = predict(nls1)))
</code></pre>

<p>Can someone help me out with that? Thanks in advance. </p>
"
"0.0657004319817604","0.0556587228785024","172189","<p>I have a dataframe <code>df</code> (see below):</p>

<pre><code>dput(df)
structure(list(x = c(49, 50, 51, 52, 53, 54, 55, 56, 1, 2, 3, 
4, 5, 14, 15, 16, 17, 2, 3, 4, 5, 6, 10, 11, 3, 30, 64, 66, 67, 
68, 69, 34, 35, 37, 39, 2, 17, 18, 99, 100, 102, 103, 67, 70, 
72), y = c(2268.14043972082, 2147.62290922552, 2269.1387550775, 
2247.31983098201, 1903.39138268307, 2174.78291538358, 2359.51909126411, 
2488.39004804939, 212.851575751527, 461.398994384333, 567.150629704352, 
781.775113821961, 918.303706148872, 1107.37695799186, 1160.80594193377, 
1412.61328924168, 1689.48879626486, 260.737164468854, 306.72700499362, 
283.410379620422, 366.813913489692, 387.570173754128, 388.602676983443, 
477.858510450125, 128.198042456082, 535.519377609133, 1028.8780498564, 
1098.54431357711, 1265.26965941035, 1129.58344809909, 820.922447928053, 
749.343583476846, 779.678206156474, 646.575242339517, 733.953282899613, 
461.156280127354, 906.813018662913, 798.186995701282, 831.365377249207, 
764.519073183124, 672.076289062505, 669.879217186302, 1341.47673353751, 
1401.44881976186, 1640.27575962036)), .Names = c(""x"", ""y""), row.names = c(NA, 
-45L), class = ""data.frame"")
</code></pre>

<p>I have created on a non-linear regression (nls) based on my dataset.</p>

<pre><code>nls1 &lt;- nls(y~A*(x^B)*(exp(k*x)), 
            data = df, 
            start = list(A = 1000, B = 0.170, k = -0.00295), algorithm = ""port"")
</code></pre>

<p>I then computed a bootstrap for this function to get multiple sets of parameters (A,B and k) and created a dataframe which contains the different set of parameters. </p>

<pre><code>Boo &lt;- nlsBoot(nls1, niter = 200)
Param_Boo &lt;- Boo$coefboot
</code></pre>

<p>I have then plotted all the 200 output functions from the bootstrapping (see below).</p>

<pre><code># Plot curves with bootstrapped params
x &lt;- seq(min(df$x),max(df$x),length=50)
curveDF &lt;- data.frame(matrix(0,ncol = 3,nrow = 200*length(x)))

for(i in 1:200)
{
  for(j in 1:length(x))
  {
    # Function value
    curveDF[j+(i-1)*200,1] &lt;- Param_Boo[i,1]*(x[j]^Param_Boo[i,2])*(exp(Param_Boo[i,3]*x[j]))
    # Bootstrap sample number
    curveDF[j+(i-1)*200,2] &lt;- i
    # x value
    curveDF[j+(i-1)*200,3] &lt;- x[j]
  }
}
colnames(curveDF) &lt;- c('ys','bsP','xs')

p1 &lt;- ggplot(curveDF, aes(x=xs, y=ys, group=bsP)) +
  geom_line() +
  ggtitle(""Curves for bootstrapped params"")
</code></pre>

<p>However, the visibility of this plot is not nice if someone wants to add the points of my dataframe on the plot. Therefore,I was wondering if it was possible to plot one curve (the mean of the 200 curves for instance) with the upper and lower confidence interval (or something else). Visually it will look a bit like the picture (top right) below. 
<a href=""http://i.stack.imgur.com/8Ues6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8Ues6.png"" alt=""enter image description here""></a></p>

<p>Can someone help me out with that? Thanks in advance. </p>
"
"0.0967084173462244","0.0907503748778111","172782","<p>Newbie question using R's mtcars dataset with anova() function. My question is how to use anova() to select the best (nested) model. Here's some example data:</p>

<pre><code>&gt; anova(lm(mpg~disp,mtcars),lm(mpg~disp+wt,mtcars),lm(mpg~disp+wt+am,mtcars))
Analysis of Variance Table

Model 1: mpg ~ disp
Model 2: mpg ~ disp + wt
Model 3: mpg ~ disp + wt + am
  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   
1     30 317.16                                
2     29 246.68  1    70.476 8.0036 0.008535 **
3     28 246.56  1     0.126 0.0143 0.905548   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; anova(lm(mpg~disp,mtcars),lm(mpg~disp+wt,mtcars),lm(mpg~disp+wt+hp,mtcars))
Analysis of Variance Table

Model 1: mpg ~ disp
Model 2: mpg ~ disp + wt
Model 3: mpg ~ disp + wt + hp
  Res.Df    RSS Df Sum of Sq       F   Pr(&gt;F)   
1     30 317.16                                 
2     29 246.68  1    70.476 10.1201 0.003571 **
3     28 194.99  1    51.692  7.4228 0.010971 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My understanding is anova() compares the reduction in the residual sum of squares to report a corresponding p-value for each nested model, where lower p-values means that nested model is more significantly different from the first model. </p>

<p>Question 1: Why is it that changing the 3rd regressor variable effects results from the 2nd nest model? That is, the p-value for <code>disp+wt</code> model changes from 0.008535 to 0.003571 going from the first to the second example. (does anova's model 2 analysis use data from model 3???)</p>

<p>Question 2: Since the 3rd model's <code>Sum of Sq</code> value is much lower in the first example (e.g. 0.126 versus 51.692), I'd expect the p-value to be lower as well, but it in fact increases (e.g. 0.905548 versus 0.010971). Why?</p>

<p>Question 3: Ultimately I'm trying to understand, given a dataset with a lot of regressors, how to use anova() to find the best model. Any general rules of thumb are appreciated. </p>
"
"0.11706119363752","0.119003355198144","172904","<p>I am trying to build a regression model for the forecast of stock market returns. The regression takes about <strong>100 variables</strong> as input and my training data consists of <strong>n=234</strong> weekly data points. I am using a lasso regression for the regression + variable selection.</p>

<p>My problem is that before I can input the data into the regression I need to find the ""optimal lag"" time for it. The variables can be <strong>categorised into 4 groups</strong> and I have got data for the variables up to <strong>113 weeks before the first point in time of my training data</strong>. To calculate the ""optimal"" lag time I tried following approach:</p>

<p>Take every variable for one categorie and apply a rolling window to the training data, which moves forward in quarters e.g. datapoints 1:13,14:26,... This gives <strong>18 seperate</strong> quarters for the training data. For the first quarter calculate the correlation (spearman in this case) between the stock course and every variable in the categorie with a lag of 0. Then take the average of the absoulte correlation of all the variables for this quarter and enter it into a matrix at point [1,1]. Repeat this for every possible lack time up to 113 and then move on to the next quarter. </p>

<p>This results in a <strong>[114,18] matrix</strong> which includes the average absolute correlation for every lag in every quarter. Now I just take the average for every row in the matrix and the row with the highest average correlation should give me the most reliable lag for my data of that category.</p>

<p><strong>Q:</strong> Has anyone seen a similar approach like this in literature before? Is there something I am missing? E.g. would it be smarter to just calculate the correlation over the whole trainingset at once and look for the optimal lag that way? As far as I am concerned that is the most common approach in literature, but it seems to me that it gives you a pretty biased result.</p>

<p>For some more insight you can find my code below:</p>

<pre><code>Correlation.Maximiser = function(Stock, Category){

  Result = matrix(nrow = 114,ncol =  18)

  for(tmp in 1:18){
    Start.Test=1+(tmp-1)*13
    End.Test=13+(tmp-1)*13
    Sample = Stock[Start.Test:End.Test]

    for(i in 0:113){
      int.low=101-i+13*tmp
      int.high=113-i+13*tmp
      NA.omitter = cor(Sample,Category[int.low:int.high,-1], method = ""spearman"")
      NA.omitter[is.na(NA.omitter)] = 0 #some Variables have a lot of 0 values so that it can happen that they are only zero in the quarter we look at which results in an NA
      Result[i+1,tmp]=mean(abs(NA.omitter))
    }
  }  
  return(Result)
}
</code></pre>

<p><strong>EDIT:</strong> I just realised that the comparison in categories makes no real sense, since some variables might have a positive while others might have a negativ correlation to the dependent variable. I tried to compensate that by averaging over the absolute correlation, but I realised that this would not penalise variables that change sign from quarter to quarter and such behavior would be really bad for the regression. In general my question stays the same though. Only now the procedure is applied to every variable on its own.</p>
"
"0.0758643241810882","0.0771229887279699","173076","<p>I was following the procedure in a statistics textbook to run a multinomial logistic regresion using <code>mlogit</code>. However, the Odds Ratios calculated seemed too high for some of the variables (>1000). Can someone take a look at this and check wether I am doing everything correctly? The data can be downloaded from <a href=""https://dl.dropboxusercontent.com/u/14303378/LogisticRegressionSample.csv"" rel=""nofollow"">here</a>. I prepared the data with the following commands:</p>

<pre><code>#read in the data
test&lt;-read.csv(file=""LogisticRegressionSample.csv"",sep="","")
#trasnform data into the correct form for mlogit
mlogitData&lt;-mlogit.data(test,choice=""Outcome"",shape=""wide"")
#build model
MLogitFit&lt;-mlogit(Outcome~1|V1+V2+V3+V4+V5+V6+V7+V8,reflevel=3,data=mlogitData)
#summary of the model
summary(MLogitFit)
#OddsRatios
data.frame(exp(MLogitFit$coefficients))
# confidence Interval of the odds Ratios
exp(confint(MLogitFit))
</code></pre>

<p>The summary of mlogit gives me:</p>

<pre><code>    Call:
mlogit(formula = Outcome ~ 1 | V1 + V2 + V3 + V4 + V5 + V6 + 
    V7 + V8, data = mlogitData, reflevel = 3, method = ""nr"", 
    print.level = 0)

Frequencies of alternatives:
      Z       A       B 
0.43333 0.25556 0.31111 

nr method
7 iterations, 0h:0m:0s 
g'(-H)^-1g = 1.56E-06 
successive function values within tolerance limits 

Coefficients :
               Estimate Std. Error t-value  Pr(&gt;|t|)    
A:(intercept)  -6.74640    5.97451 -1.1292 0.2588147    
B:(intercept)  -7.12401    4.50350 -1.5819 0.1136759    
A:V1            3.65979    3.90808  0.9365 0.3490331    
B:V1            4.24363    3.25687  1.3030 0.1925822    
A:V2          -15.11554    6.92901 -2.1815 0.0291475 *  
B:V2           -4.88778    3.65249 -1.3382 0.1808302    
A:V3            1.71465    6.57907  0.2606 0.7943839    
B:V3            2.94335    3.96557  0.7422 0.4579497    
A:V4           -1.70660    1.58849 -1.0744 0.2826633    
B:V4           -1.67210    1.17575 -1.4222 0.1549820    
A:V5            1.18494    1.60760  0.7371 0.4610682    
B:V5            1.03084    1.25573  0.8209 0.4116971    
A:V6            8.28902    2.51631  3.2941 0.0009873 ***
B:V6            3.44578    1.91844  1.7961 0.0724727 .  
A:V7           -1.34395    2.67943 -0.5016 0.6159612    
B:V7            1.04803    1.95147  0.5370 0.5912343    
A:V8           -7.46263    4.12978 -1.8070 0.0707577 .  
B:V8            0.21861    2.13596  0.1023 0.9184810    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -64.636
McFadden R^2:  0.33149 
Likelihood ratio test : chisq = 64.1 (p.value = 1.0515e-07)
</code></pre>

<p>Running <code>data.frame(exp(MLogitFit$coefficients))</code> to calculate the odds ratios gives:</p>

<pre><code>              exp.MLogitFit.coefficients.
A:(intercept)                1.175103e-03
B:(intercept)                8.055280e-04
A:V1                         3.885310e+01
B:V1                         6.966040e+01
A:V2                         2.725226e-07
B:V2                         7.538147e-03
A:V3                         5.554743e+00
B:V3                         1.897938e+01
A:V4                         1.814819e-01
B:V4                         1.878524e-01
A:V5                         3.270504e+00
B:V5                         2.803423e+00
A:V6                         3.979917e+03
B:V6                         3.136764e+01
A:V7                         2.608125e-01
B:V7                         2.852036e+00
A:V8                         5.741439e-04
B:V8                         1.244345e+00
</code></pre>

<p>I obtained the confidence interavls with: <code>exp(confint(MLogitFit))</code>:</p>

<pre><code>                     2.5 %       97.5 %
A:(intercept) 9.650816e-09 1.430830e+02
B:(intercept) 1.182216e-07 5.488637e+00
A:V1          1.831725e-02 8.241213e+04
B:V1          1.176881e-01 4.123248e+04
A:V2          3.446800e-13 2.154711e-01
B:V2          5.864847e-06 9.688857e+00
A:V3          1.394913e-05 2.211978e+06
B:V3          7.994348e-03 4.505896e+04
A:V4          8.066986e-03 4.082774e+00
B:V4          1.875058e-02 1.881996e+00
A:V5          1.400307e-01 7.638467e+01
B:V5          2.392271e-01 3.285238e+01
A:V6          2.870699e+01 5.517731e+05
B:V6          7.303065e-01 1.347282e+03
A:V7          1.366460e-03 4.978060e+01
B:V7          6.223884e-02 1.306918e+02
A:V8          1.752860e-07 1.880591e+00
B:V8          1.891518e-02 8.185990e+01
</code></pre>

<p>The predicted Probabilities are as following:</p>

<pre><code>fitted(MLogitFit, outcome=FALSE)
                 Z            A          B
 [1,] 0.2790108926 3.880184e-01 0.33297074
 [2,] 0.5191458618 2.900625e-01 0.19079169
 [3,] 0.7263001933 1.633014e-02 0.25736966
 [4,] 0.8386056883 3.700203e-03 0.15769411
 [5,] 0.8050365007 7.487290e-03 0.18747621
 [6,] 0.7855655154 3.860347e-02 0.17583101
 [7,] 0.7878404896 7.992930e-03 0.20416658
 [8,] 0.8386056883 3.700203e-03 0.15769411
 [9,] 0.7878404896 7.992930e-03 0.20416658
[10,] 0.4363708036 2.827104e-01 0.28091885
[11,] 0.6126060746 3.320075e-02 0.35419317
[12,] 0.0274357267 8.418204e-01 0.13074390
[13,] 0.1438998597 5.869087e-01 0.26919146
[14,] 0.1850027820 2.105586e-01 0.60443858
[15,] 0.8427092407 5.933393e-03 0.15135737
[16,] 0.1537160539 4.929905e-01 0.35329341
[17,] 0.0434283140 6.358897e-01 0.32068201
[18,] 0.1868202029 1.141679e-01 0.69901186
[19,] 0.3064594418 1.156597e-01 0.57788084
[20,] 0.5737141160 6.734724e-02 0.35893865
[21,] 0.5841338911 1.374758e-01 0.27839031
[22,] 0.0866451414 4.019366e-01 0.51141821
[23,] 0.2794060013 9.964607e-02 0.62094793
[24,] 0.0252343516 7.343045e-01 0.24046118
[25,] 0.1314775919 4.602643e-01 0.40825811
[26,] 0.0274357267 8.418204e-01 0.13074390
[27,] 0.1303195991 6.649645e-01 0.20471586
[28,] 0.2818251202 4.896734e-01 0.22850146
[29,] 0.0063990341 8.874618e-01 0.10613917
[30,] 0.0002408527 9.742025e-01 0.02555668
[31,] 0.0523052465 7.073015e-01 0.24039322
[32,] 0.3287956423 2.756959e-01 0.39550841
[33,] 0.0419093705 7.521689e-01 0.20592173
[34,] 0.0523052465 7.073015e-01 0.24039322
[35,] 0.3287956423 2.756959e-01 0.39550841
[36,] 0.0100998700 7.475180e-01 0.24238212
[37,] 0.1609808596 2.268570e-01 0.61216212
[38,] 0.0119603037 8.065964e-01 0.18144331
[39,] 0.0697132279 4.549378e-01 0.47534896
[40,] 0.5756435353 6.315652e-02 0.36119994
[41,] 0.4689676672 6.796615e-02 0.46306619
[42,] 0.2652679745 6.358962e-02 0.67114240
[43,] 0.7870195702 2.038999e-03 0.21094143
[44,] 0.6438437943 9.222002e-03 0.34693420
[45,] 0.7462282258 5.881047e-04 0.25318367
[46,] 0.3532662528 2.193975e-01 0.42733620
[47,] 0.9563852795 4.133754e-05 0.04357338
[48,] 0.9079031419 2.786314e-03 0.08931054
[49,] 0.0220230156 8.017508e-01 0.17622619
[50,] 0.2268852285 1.745210e-01 0.59859376
[51,] 0.2268852285 1.745210e-01 0.59859376
[52,] 0.0751929214 6.261548e-01 0.29865225
[53,] 0.9426667411 4.520877e-06 0.05732874
[54,] 0.0212631471 6.729961e-01 0.30574075
[55,] 0.0212631471 6.729961e-01 0.30574075
[56,] 0.9218535421 1.166953e-02 0.06647693
[57,] 0.6374868816 3.856300e-02 0.32395012
[58,] 0.2920703240 2.410709e-01 0.46685876
[59,] 0.7047942848 1.728601e-02 0.27791970
[60,] 0.1850395244 5.297673e-01 0.28519316
[61,] 0.4402296785 8.870861e-03 0.55089946
[62,] 0.6781988218 3.852569e-04 0.32141592
[63,] 0.9889453179 4.036588e-05 0.01101432
[64,] 0.1618635354 8.011851e-02 0.75801796
[65,] 0.3008372801 9.835522e-02 0.60080750
[66,] 0.0740319347 4.284039e-01 0.49756417
[67,] 0.5529727485 1.768537e-01 0.27017351
[68,] 0.7824740564 5.001713e-03 0.21252423
[69,] 0.5343045050 5.865850e-02 0.40703700
[70,] 0.4564647083 1.733995e-01 0.37013579
[71,] 0.4711837972 8.449081e-03 0.52036712
[72,] 0.9154349308 2.364316e-02 0.06092191
[73,] 0.1858643216 2.217595e-01 0.59237621
[74,] 0.3770813535 9.943397e-02 0.52348468
[75,] 0.8124141650 3.243679e-04 0.18726147
[76,] 0.3195206223 2.932236e-01 0.38725578
[77,] 0.8615871019 5.063299e-04 0.13790657
[78,] 0.8615871019 5.063299e-04 0.13790657
[79,] 0.8254986241 2.059378e-03 0.17244200
[80,] 0.1208591778 4.615235e-01 0.41761730
[81,] 0.0035765650 9.093754e-01 0.08704806
[82,] 0.7583239965 3.544345e-02 0.20623255
[83,] 0.8141948591 5.016280e-03 0.18078886
[84,] 0.1204323818 2.545405e-01 0.62502710
[85,] 0.9594950290 3.694056e-05 0.04046803
[86,] 0.6858228916 1.691396e-01 0.14503752
[87,] 0.8254986241 2.059378e-03 0.17244200
[88,] 0.8254986241 2.059378e-03 0.17244200
[89,] 0.2463233530 2.793410e-01 0.47433568
[90,] 0.5674338104 1.448538e-02 0.41808081
</code></pre>

<p>To assess multicolinearity I calculated the VIF statistic but using the a glm model of the same dataset.</p>

<pre><code>fullmod&lt;-glm(as.factor(Outcome)~.,data=test,family=binomial())
vif(fullmod)
      V1       V2       V3       V4       V5       V6       V7       V8 
1.789116 1.822252 2.216444 1.320244 1.821820 1.439183 1.512865 1.121805 
</code></pre>
"
"0.035985608634244","0.0487768608679754","173132","<p>*********Okay so I figured out what was wrong! I wasn't centering the date like the lm.ridge function does. However I still cannot reproduce the intercept that lm.ridge gives me.</p>

<p>According to my research you can simulate a ridge regression by adding ""phony data"" to the end of a normal OLS regression.  One of many places that corroborate this notion is this CV thread: <a href=""http://stats.stackexchange.com/questions/137057/phoney-data-and-ridge-regression-are-the-same"">Phoney data and ridge regression are the same?</a></p>

<p>However I fail to replicate the results in R. Here are my three variables: </p>

<pre><code>&gt; test_0
12    34    24    64   746    24    23    42     7     8     3     4    45   675     3     4    34    43  56   674     3     4    54    34    23    34   435    56    56   234   657    89   980     8    76    65 45564    67    76   789

&gt; test_1
34    24    64   746    24    23    42     7     8     3     4    45   675     3     4    34    43    56 674     3     4    54    34    23    34   435    56    56   234   657    89   980     8    76    65 45564  67    76   789     6

&gt; test_2
24    64   746    24    23    42     7     8     3     4    45   675     3     4    34    43    56   674 3     4    54    34    23    34   435    56    56   234  657    89   980     8    76    65 45564    67 76   789     6     5
</code></pre>

<p>I then append 2 new rows (for the number of independent vars). To test_0 I append two zeros. To test_1 I append a sqrt(.5) and 0. To test_2 I append a 0 and sqrt(.5)</p>

<pre><code>&gt; a = c(test_0, 0, 0)
&gt; b = c(test_1, (sqrt(.5)), 0)
&gt; c = c(test_2, 0, (sqrt(.5)))
</code></pre>

<p>Then I run two models, <code>lm</code> and <code>lm.ridge</code>:</p>

<pre><code>&gt;reg = lm(a~b+c)
&gt;ridge = lm.ridge(test_0~test_1+test_2, lambda=.5)
&gt; reg
 Call:
 lm(formula = a ~ b + c)

 Coefficients:
 (Intercept)            b            c  
  1305.42310     -0.02926     -0.02862  

&gt; ridge
                      test_1        test_2 
 1374.16801379   -0.03059968   -0.02996396 
</code></pre>

<p>The coefficients are different but they should be the same. Why is this the case?</p>

<p><em>I have also tried the above using a lambda of 1 and still get the inconsistency.</em></p>
"
"0.0599760143904067","0.0609710760849692","173410","<p>I am working with a lasso regression with the glmnet package. I read these threads: <a href=""http://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia"">When conducting multiple regression, when should you center your predictor variables &amp; when should you standardize them?</a>, <a href=""http://stats.stackexchange.com/questions/19523/need-for-centering-and-standardizing-data-in-regression"">Need for centering and standardizing data in regression</a> and <a href=""http://stats.stackexchange.com/questions/86434/is-standardisation-before-lasso-really-necessary"">Is standardisation before Lasso really necessary?</a>.</p>

<p>Based on the responses I decided that I need to standardize my data before using it. I do have some questions however:</p>

<ul>
<li>Do I need to standardize the predictors and the responses or only the predictors?</li>
<li>I am using the function scale(myData, center = TRUE, scale = TRUE) for building the model, but I am wondering what do I do when I want to do predictions with a test data set. I think I should also standardize and center the test data, but how to I do that? Substracting the mean from the initial (training) dataset and the dividing it by the standard deviation of the initial dataset? </li>
<li>When I get a result do I need to ""backscale"" it (using the original mean and standard deviation) or do I already get the ""final"" result? </li>
</ul>
"
"0.026822089039291","0.0272670941574606","173568","<p>In helping us understand how to fit a logistic regression in <code>R</code>, we are told to first replace 0 and 1 in the response variable by 0.05 and 0.95, respectively and second to take the logit transform of the resulting response variable. Last we fit these data using iterative re-weighted least squares method. </p>

<p>Then we are asked to use 0.005 and 0.995 instead of 0.05 and 0.95. Then the resulting coefficients are quite <strong>different</strong>.</p>

<p>My question is in <code>glm</code> function, how are 0 and 1 dealt with? Are they replaced by some numbers as above? What numbers are used by default and why are they used? How sensitive is the choice of these numbers?</p>
"
"0.0608267804924532","0.0618359572423054","173629","<p>When applying the ""urca"" package function <code>ur.df</code>, like </p>

<pre><code>summary(ur.df(data$col1, type = c(""none""), lags = 12, selectlags = c(""AIC"")))
</code></pre>

<p>I get following result:</p>

<pre><code>############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
      Min        1Q    Median        3Q       Max 
-12928366  -2888728   1284718   4218373   7179531 

Coefficients:
                 Estimate    Std. Error  t value  Pr(&gt;|t|)   
(Intercept)  5.391984e+07  1.638362e+07  3.29108 0.0043123 **
z.lag.1     -2.438154e+00  7.557134e-01 -3.22629 0.0049588 **
tt           6.579260e+05  2.730453e+05  2.40959 0.0275861 * 
z.diff.lag1  1.712004e+00  6.595980e-01  2.59553 0.0188537 * 
z.diff.lag2  1.402824e+00  6.379412e-01  2.19899 0.0420083 * 
z.diff.lag3  1.321555e+00  5.294537e-01  2.49607 0.0231329 * 
z.diff.lag4  1.099430e+00  4.720412e-01  2.32910 0.0324428 * 
z.diff.lag5  8.132753e-01  4.181477e-01  1.94495 0.0685140 . 
z.diff.lag6  1.797331e-01  3.654326e-01  0.49184 0.6291254   
z.diff.lag7  5.890640e-01  2.939590e-01  2.00390 0.0612825 . 
z.diff.lag8  3.919041e-01  2.794371e-01  1.40248 0.1787705   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6708593 on 17 degrees of freedom
Multiple R-squared:  0.7237276, Adjusted R-squared:  0.5613144 
F-statistic: 4.253547 on 10 and 17 DF,  p-value: 0.003348755


Value of test-statistic is: -3.2263 3.9622 5.2635 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -4.15 -3.50 -3.18
phi2  7.02  5.13  4.31
phi3  9.31  6.73  5.61
</code></pre>

<p>Now the question:</p>

<ol>
<li>I do understand that ""-3.2263"" is the critical value (t-value)</li>
<li><strong>There is a unit root</strong> with trend since -3.2263 > -3.18 (tau3@10pct)
This means the time-series is <strong>non-stationary</strong> at a 10% significance level.</li>
<li>But, what is the meaning of ""p-value: 0.003348755""? Should I list this value in a table summarizing my unit root test results or rather mark the 0.1 significance level (*10%)?</li>
</ol>

<p>The <a href=""http://www.inside-r.org/packages/cran/urca/docs/ur.df"" rel=""nofollow"">documentation</a> says that critical values are based on Hamilton (1994) and Dickey and Fuller (1981)"". </p>
"
"0.110590306208719","0.105811867590468","173704","<p>I have a function I wrote to calculate idiosyncratic volatility, but its really inefficient. Does anyone have any suggestions to make the code below more efficient?</p>

<p>Specifically, I am trying to take a time series and run a linear regression using a N_Max lookback. I then want to extract the residuals for the dates used in the linear regression and calculate the standard deviation. If there is not N_Max dates, I want to use as many dates as I have available provided it is larger than N_Min.</p>

<pre><code>Idiosyncratic_Volatility &lt;- function(dependent, independent, dates, Min_n, Max_n) {
  # Define size as the number of observations in the dependent
  size &lt;- length(dependent)

  n &lt;- Max_n

  # If the lookback is less than 0, stop the function and error out
  if (Min_n &lt; 1 | Max_n &lt; 1) stop(""n must be positive"")
  if (Max_n &lt; Min_n) stop(""Max_n must be greater than Min_n"")

  # Define variables for the recursive function  
  i &lt;- size                           # set the counter for itterations
  result &lt;- NA; result[1:size] &lt;- NA  # create a vector of NAs to store the results
  residuals_vector &lt;- NA              # create a variable to store residuals

  while (i &gt;= Max_n)
  {
    # Window is an R function that allows you to do run calculations over
    # desired rolling date ranges.
    RollingIndependent &lt;- window(independent, start=i-n+1, end=i)
    RollingDependent  &lt;- window(dependent, start=i-n+1, end=i)

    # If the data can cause an error, return NA
    if(
      n &gt; length(RollingDependent) | 
        length(RollingDependent) &gt; length(RollingIndependent) | 
        length(RollingDependent) &gt; length(dates)) {
      result[i] &lt;- NA

      # else calculate idiosyncratic volatility
    } else {  
      # fit a linear model to the data.
      model &lt;- lm(RollingDependent ~ RollingIndependent)

      # extract the residuals from the model
      residuals_vector &lt;- summary(model)$residuals

      # calculate the standard deviation of the residuals
      result[i] &lt;- sd(residuals_vector,na.rm=FALSE)

      # iterate the recursive factor
      i &lt;- i - 1
    }
  } 

  n &lt;- i

  # This next part does the ""burn in"" period and runs off available information
  # until it hits the Min_n.
  while (i &gt;= Min_n)
  {
    # Window is an R function that allows you to do run calculations over
    # desired rolling date ranges.
    RollingIndependent &lt;- window(independent, start=i-n+1, end=i)
    RollingDependent  &lt;- window(dependent, start=i-n+1, end=i)

    # If the data can cause an error, return NA
    if(
      n &gt; length(RollingDependent) | 
        length(RollingDependent) &gt; length(RollingIndependent) | 
        length(RollingDependent) &gt; length(dates)) {
      result[i] &lt;- NA

      # else calculate idiosyncratic volatility
    } else {  
      # fit a linear model to the data.
      model &lt;- lm(RollingDependent ~ RollingIndependent)

      # extract the residuals from the model
      residuals_vector &lt;- summary(model)$residuals

      # calculate the standard deviation of the residuals
      result[i] &lt;- sd(residuals_vector,na.rm=FALSE)

      # iterate the recursive factor
      i &lt;- i - 1
      n &lt;- i
    }
  } 

  # print the results
  result
}
</code></pre>

<p>Thank you for your time!</p>
"
"0.0464572209811883","0.0472279924554862","174110","<p>I'm growing a regression tree with the <code>rpart</code> function in R (package of the same name). I would like to be able to choose myself the number of nodes (not the depth of the tree, but the actual number of nodes), either by growing or pruning afterwards.</p>

<p>The problem is that in the cptable the number of nodes jumps and skips some numbers (from 1 to 5 here). I would like, for instance a tree giving 3, 4 or 5 distinct prediction values. </p>

<pre><code>set.seed(1)
df=data.frame(x=rnorm(100), y=rnorm(100))
tree=rpart(data=df, y~x)
tree$cptable
####           CP nsplit rel error   xerror      xstd
#### 1 0.03357572      0 1.0000000 1.013942 0.1337594
#### 2 0.02899422      1 0.9664243 1.187690 0.1620186
#### 3 0.01488440      5 0.8504474 1.188779 0.1632158
</code></pre>

<p>And I can't find the control parameter to set this, if it is even possible (growing or pruning).
Can someone help here please?</p>
"
"0.0808716413062113","0.0904347204435887","174252","<p>I have built some ""regular"" and robust regression models, using the standard lm function as well as rlm and lmrob.  While I know that there is some discussion about using stepwise regression, I have used the stepAIC function to prune my variable set.  After I've gotten a reduced set of variables using stepAIC, I've then run some robust regressions.</p>

<p>The <strong>cvTools</strong> package allows me to use cross validation to compare the performance of my various models.</p>

<p>I obviously would like to run lasso regression (ideally using the <strong>glmnet</strong> package) on my dataset.</p>

<p>My question is whether or not there is an already built package/functionality that will allow me to use cross validiation to compare the lasso regression model with the other models.</p>

<p>If there is not, then my initial thought had been to go back to first principles and manually code K-fold cross validation for lasso regression.  However, I am now wondering if this is theoretically a good idea.  Each time I run a fold in my manual CV, I would run cv.glmnet on the training set.  Each training set would most likely result in a different lambda.min and lambda.1se.  </p>

<p>My question is:  is it technically proper CV to determine the overall CV error by averaging the error on each fold given that the lambda chosen for each fold will be producing a different lasso result?</p>

<p>Here is some sample code that I have used to create leave-one-out CV on the dataset to evaluate the lasso regression.  I have computed my cross validation error  on each fold using lambda.1se and labmda.min that arise for that fold.</p>

<pre><code>lassocv&lt;-function() {

len&lt;-length(drx$DR)

errmin&lt;-0
err1se&lt;-0
print(len)

for (i in 1:len) {
    gmer&lt;-data.matrix(drx[-i,])
    yxer&lt;-yx[-i]
    lfit&lt;-cv.glmnet(gmer, yxer)
    newr&lt;-data.matrix(drx[i,])
    pmin&lt;-predict(lfit,newx=newr,s=lfit$lambda.min)[[1]]
	    p1se&lt;-predict(lfit,newx=newr,s=lfit$lambda.1se)[[1]]
    errmin&lt;-errmin+abs(pmin-yx[i])
    err1se&lt;-err1se+abs(p1se-yx[i])
}
print(errmin/len)
print(err1se/len)

}
</code></pre>

<p>However, I get different CV results.  The two results that are returned for my dataset are 21.94867 and 23.74074.</p>
"
"0.154147858171395","0.156705324360574","174257","<p>I want to do a path analysis with lavaan but encounter a few problems and would appreciate any help.</p>

<p>The structural model looks like this:</p>

<p><a href=""http://i.stack.imgur.com/y8ZZh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y8ZZh.png"" alt=""structural model""></a></p>

<p>The relation between one observed independent (s) and one observed dependent variable (v) is mediated through a latent variable (m) that is defined by two observed indicator variables (x1, x2). This is basically a simplified version of the <a href=""http://lavaan.ugent.be/tutorial/sem.html"" rel=""nofollow"">SEM example</a> in the tutorial on the lavaan project website.</p>

<p>When I enter my code (given further below) into R, I encounter two problems:</p>

<p>(1) The results change when I change the order of the indicator variables.</p>

<p>This model:</p>

<pre><code># measurement model
    m =~ x1 + x2
</code></pre>

<p>returns a different result than this model:</p>

<pre><code># measurement model
    m =~ x2 + x1
</code></pre>

<p>How can that be? Isn't the order of the indicators arbitrary? And if not, how do I know which is the correct order, if my model does not presuppose a specific order?</p>

<p>(2) There are a few warnings that I don't understand: for the first model, no standard errors could be computed; and the second model did not ""converge"" (whatever that means). The warnings are given in context in the full code posted below.</p>

<p>What do I have to do to obtain reliable estimates?</p>

<hr>

<p>Here is the full R output to provide context to my questions.</p>

<pre><code># data

s &lt;- c(2, 5, 4, 4, 4, 8, 2, 9, 1, 1, 3, 3, 2, 3, 2, 5, 5, 7, 4, 7, 8, 4, 10, 10, 2, 4, 0, 2, 4, NA, 1, 5, 2, 6, 3, 5, 0, 5, 3, 6, 4, 9, 4, 9, 4, 5, 6, 1, 8, 0, 6, 9, 1, 5, 1, 6, 2, 5, 0, 5, 6, 2, 4, 10, 3, 4)
v &lt;- c(8, 10, 1, 4, 0, 2, 3, 2, 1, 1, 2, 5, 1, 5, 0, 5, 4, 5, 2, 10, 0, 6, 5, 5, 6, 1, 1, 0, 0, NA, 1, 0, 1, 8, 1, 3, 0, 5, 6, 3, 2, 10, 0, 5, 5, 10, 4, 1, 1, 0, 0, 0, 2, 10, 1, 8, 2, 3, 2, 2, 4, 4, 2, 5, 6, 2)
x1 &lt;- c(2.500000, 3.789474, 1.514563, 5.846868, 4.588235, 5.600000, 5.066667, 11.647059, 2.000000, NA, 4.461538, 18.000000, 1.058824, 9.217391, 27.840000, 15.375000, NA, 6.000000, 9.714286, 12.484848, 16.503497, 20.666667, 3.500000, 4.658824, 4.750000, 4.000000, 2.800000, 14.228571, 11.000000, NA, 2.666667, 3.764706, 4.705882, 13.272727, 2.000000, 18.444444, 17.555556, 14.222222, 2.000000, 4.000000, 8.461538, 19.200000, 13.902439, 13.000000, 3.000000, NA, 7.360000, 1.611374, 1.500000, 3.365854, 22.375000, 10.838710, 2.923077, 3.488372, 5.176471, 37.666667, 1.176471, 7.454545, 36.235294, 6.823529, 2.222222, 6.133333, 11.428571, 42.705882, 28.105263, 18.333333)
x2 &lt;- c(8.125000, 14.273684, 7.339806, 23.387471, 113.058824, 22.200000, 17.466667, 43.647059, 9.230769, NA, 13.538462, 83.555556, 5.058824, 37.391304, 100.000000, 59.250000, NA, 22.470588, 38.428571, 50.787879, 76.223776, 92.888889, 15.375000, 16.235294, 18.875000, 13.647059, 10.133333, 55.885714, 36.428571, NA, 6.933333, 13.294118, 14.117647, 81.818182, 6.117647, 67.777778, 76.333333, 51.888889, 6.428571, 14.200000, 34.000000, 59.680000, 68.634146, 40.500000, 12.250000, NA, 29.760000, 8.909953, 5.400000, NA, 71.125000, 39.741935, 9.846154, 13.116279, 18.823529, 204.000000, 4.588235, 49.090909, 188.470588, 19.647059, 10.222222, 22.933333, 38.285714, 140.235294, 137.526316, 79.000000)
dat &lt;- data.frame(cbind(s, v, x1, x2))

# first model

model &lt;- '
    # measurement model
        m =~ x2 + x1
    # regressions
        m ~ s
        v ~ s + m
    # residual correlations
        x1 ~~ x2
'
fit &lt;- sem(model, data = dat, missing = ""fiml"")

# Warning messages:
# 1: In lav_data_full(data = data, group = group, group.label = group.label,  :
#   lavaan WARNING: some cases are empty and will be removed:
#   30
# 2: In lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats,  :
#   lavaan WARNING: could not compute standard errors!
#   lavaan NOTE: this may be a symptom that the model is not identified.

summary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)

# lavaan (0.5-18) converged normally after 147 iterations
#
#                                                   Used       Total
#   Number of observations                            65          66
#
#   Number of missing patterns                         3
#
#   Estimator                                         ML
#   Minimum Function Test Statistic                0.565
#   Degrees of freedom                                 0
#   Minimum Function Value               0.0043451960201
#
# Model test baseline model:
#
#   Minimum Function Test Statistic              126.904
#   Degrees of freedom                                 6
#   P-value                                        0.000
#
# User model versus baseline model:
#
#   Comparative Fit Index (CFI)                    0.995
#   Tucker-Lewis Index (TLI)                       1.000
#
# Loglikelihood and Information Criteria:
#
#   Loglikelihood user model (H0)               -797.558
#   Loglikelihood unrestricted model (H1)       -797.275
#
#   Number of free parameters                         12
#   Akaike (AIC)                                1619.115
#   Bayesian (BIC)                              1645.208
#   Sample-size adjusted Bayesian (BIC)         1607.435
#
# Root Mean Square Error of Approximation:
#
#   RMSEA                                          0.000
#   90 Percent Confidence Interval          0.000  0.000
#   P-value RMSEA &lt;= 0.05                          1.000
#
# Standardized Root Mean Square Residual:
#
#   SRMR                                           0.027
#
# Parameter estimates:
#
#   Information                                 Observed
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(&gt;|z|)   Std.lv  Std.all
# Latent variables:
#   m =~
#     x2                1.000                              14.272    0.330
#     x1                0.384                               5.482    0.588
#
# Regressions:
#   m ~
#     s                 1.732                               0.121    0.323
#   v ~
#     s                 0.335                               0.335    0.306
#     m                 0.012                               0.171    0.059
#
# Covariances:
#   x2 ~~
#     x1              292.112                             292.112    0.951
#
# Intercepts:
#     x2               35.558                              35.558    0.823
#     x1                7.220                               7.220    0.775
#     v                 1.761                               1.761    0.604
#     m                 0.000                               0.000    0.000
#
# Variances:
#     x2             1663.119                            1663.119    0.891
#     x1               56.783                              56.783    0.654
#     v                 7.591                               7.591    0.892
#     m               182.367                               0.895    0.895
#
# R-Square:
#
#     x2                0.109
#     x1                0.346
#     v                 0.108
#     m                 0.105

model &lt;- '
    # measurement model
        m =~ x1 + x2
    # regressions
        m ~ s
        v ~ s + m
    # residual correlations
        x1 ~~ x2
'
fit &lt;- sem(model, data = dat, missing = ""fiml"")

# Warning messages:
# 1: In lav_data_full(data = data, group = group, group.label = group.label,  :
#   lavaan WARNING: some cases are empty and will be removed:
#   30
# 2: In lavaan::lavaan(model = model, data = dat, missing = ""fiml"", model.type = ""sem"",  :
#   lavaan WARNING: model has NOT converged!

summary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)

# ** WARNING ** lavaan (0.5-18) did NOT converge after 9438 iterations
# ** WARNING ** Estimates below are most likely unreliable
#
#                                                   Used       Total
#   Number of observations                            65          66
#
#   Number of missing patterns                         3
#
#   Estimator                                         ML
#   Minimum Function Test Statistic                   NA
#   Degrees of freedom                                NA
#   P-value                                           NA
#
# Parameter estimates:
#
#   Information                                 Observed
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(&gt;|z|)   Std.lv  Std.all
# Latent variables:
#   m =~
#     x1                1.000                               0.526    0.056
#     x2             1606.326                             845.326   19.343
#
# Regressions:
#   m ~
#     s                -0.001                              -0.001   -0.004
#   v ~
#     s                 0.355                               0.355    0.325
#     m                 0.004                               0.002    0.001
#
# Covariances:
#   x1 ~~
#     x2              -69.375                             -69.375   -0.009
#
# Intercepts:
#     x1               10.099                              10.099    1.083
#     x2               48.281                              48.281    1.105
#     v                 1.761                               1.761    0.604
#     m                 0.000                               0.000    0.000
#
# Variances:
#     x1               86.614                              86.614    0.997
#     x2            -712666.446                            -712666.446 -373.157
#     v                 7.617                               7.617    0.895
#     m                 0.277                               1.000    1.000
#
# R-Square:
#
#     x1                0.003
#     x2                   NA
#     v                 0.105
#     m                 0.000
# Warning message:
# In .local(object, ...) :
#   lavaan WARNING: fit measures not available if model did not converge
</code></pre>

<hr>

<p><em>Note.</em> I have posted the same question to the <a href=""https://groups.google.com/forum/#!forum/lavaan"" rel=""nofollow"">lavaan Google Group</a>, but this is part of my bachelor's thesis, which I have to turn in on Monday, so I'm a bit pressed for time and hope you forgive me for crossposting.</p>
"
"0.0657004319817604","0.0667904674542028","174518","<p>consider the following data set:</p>

<pre><code>a &lt;- c(1, 2, 3, 1, 4, 1968, 2, 1)
b &lt;- c(2, 1, 2, 4, 3, 1984, 2, 0)
c &lt;- c(3, 3, 4, 2, 1, 1945, 1, 0)
d &lt;- c(4, 1, 4, 3, 2, 1975, 3, 1)
df &lt;- data.frame(rbind(a,b,c,d))
names(df) &lt;- c(""ID"", ""OptionW"", ""OptionX"", ""OptionY"", ""OptionZ"", ""yearofBirth"", ""education"", ""sex"")


ID OptionW OptionX OptionY OptionZ yearofBirth education sex
1       2       3       1       4        1968         2   1
2       1       2       4       3        1984         2   0
3       3       4       2       1        1945         1   0
4       1       4       3       2        1975         3   1
</code></pre>

<p>Two hundred people where asked to rank Options W to Z from 1 to 4 in their effectiveness to lower crime rates in their community. Their age, highest academic degree and sex are annotated as well.
I want to find out:</p>

<ul>
<li>which options are preferred by the majority of citizens?</li>
<li>are there significant differences in what men or women, old or young, well or less well educated citizens believe?</li>
<li>how likely is the ranking order going to change if the person is older/younger, has had more or less formal education and is male or female? </li>
</ul>

<p>I read that a multinomial logistic regression might be the way to go, but I find it hard to adapt the examples I find to my data set. Often they allow for only one option to be chosen, making each choice (W, X Y Z) a level of one variable (Options). But in my case I have several variables (OptionW, OptionX, OptionY, OptionZ) where the ranking placement appears to be the level (1,2,3,..10). Or am I looking at it the wrong way?</p>

<p>Which function from what package would be suitable? And are there other methods available apart from mlr?</p>

<p>I use R mostly for spatial analysis and am not very fluent in statistics. Hopefully you can help me here.</p>
"
"0.080466267117873","0.0818012824723818","174861","<p>Here is <a href=""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"" rel=""nofollow"">sample data</a>:</p>

<pre><code>    brainIQ &lt;- 
  read.table (file= ""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"",
 head = TRUE)
</code></pre>

<p>I am trying to fit multiple linear regression.</p>

<pre><code>mylm &lt;- lm(PIQ ~  Brain + Height + Weight, data = brainIQ)
anova(mylm)
</code></pre>

<p>Default function anova in R provides sequential sum of squares (type I) sum of square. </p>

<pre><code>Analysis of Variance Table

Response: PIQ
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
Brain      1  2697.1 2697.09  6.8835 0.01293 *
Height     1  2875.6 2875.65  7.3392 0.01049 *
Weight     1     0.0    0.00  0.0000 0.99775  
Residuals 34 13321.8  391.82                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I belief, thus the SS are Brain, Height | Brain, Weight | (Brain, Weight) and residuals respectively.</p>

<p>Using package car we can also get type II sum of square. </p>

<pre><code>library(car)
Anova(mylm, type=""II"")
Anova Table (Type II tests)

Response: PIQ
           Sum Sq Df F value    Pr(&gt;F)    
Brain      5239.2  1 13.3716 0.0008556 ***
Height     1934.7  1  4.9378 0.0330338 *  
Weight        0.0  1  0.0000 0.9977495    
Residuals 13321.8 34                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Here sum of squares are like: Brian | (Height, Weight), Height | (Brain, Weight), Weight | (Brain, Height).</p>

<p>Which look pretty like Mintab output:</p>

<p><a href=""http://i.stack.imgur.com/0iXgH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0iXgH.png"" alt=""enter image description here""></a></p>

<p>My question is how can I calculate the regression row in the above table in R ? </p>
"
"0.026822089039291","0.0272670941574606","174989","<p>Please bear with me, I am very new to R.</p>

<p>My question is regarding the use of the <code>improveProb</code> function in the <code>Hmisc</code> package. I have two logistic models, the only difference being that the second model contains my novel marker of interest. I am trying to calculate NRI and IDI.</p>

<p>I have the PredRisks for both models - PredRisk1 and PredRisk2, and my outcome is disease 0/1. How do I define this in R in order to run</p>

<p><code>improveProb(x1, x2, y)</code>?</p>

<hr>

<p>The data are the same for both models. We are looking at ways to validate our findings. We have performed k-fold cross-validation (MSE=0.08) and bootstrapping with optimism (AUC original = 0.826 After correction =0.791) to check for overfitting. Is this appropriate? The LRT was significant for both logistic regression models, but I need to check this. Also, the AIC for model 2 is lower than model 1. Thanks again for your expert knowledge :)</p>
"
"0.053644178078582","0.0545341883149212","175079","<p>Is it so that:</p>

<ul>
<li>$y_i$ is not a discrete value, but a range with probability density function</li>
<li>Which means for the same predictor(s) value $y_i$ could have different results</li>
<li>In linear regression this distribution can only be normal</li>
<li>In GLM, this distribution can be any distribution from the exponential family</li>
<li>distribution of a single $y_i$ has nothing to do with distribution of all $y(s)$</li>
<li>$\mu_i$ is expected value of $y_i$</li>
<li>In practical use, $\mu_i$ is the predicted value $y_i$, specially if dataset has only one y for given predictor(s)</li>
</ul>

<p>Are above correct? Where am I wrong?</p>

<p>Based on the above I've tried simulating <code>glm</code> with <code>lm</code> in R, and it kinda works:</p>

<pre><code>library(boot)
download.file(""https://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""./ravensData.rda"",method=""curl"")
load(""./ravensData.rda"")
# download manually and loadhere if above fails
# load(""/yourpath/ravensData.rda"")

# calling logit(ravensData$ravenWinNum) results in 
# [1]  Inf  Inf  Inf  Inf  Inf -Inf  Inf  Inf  Inf  Inf -Inf  Inf  Inf  Inf  Inf -Inf
# [17] -Inf -Inf  Inf -Inf
# that's way too much, as inv.logit goes to 1 at 20
# so we'll write our own dummy ""logit"" routine
# this will give us 5 when winNum=1 and -5 when it's zero
win &lt;- ravensData$ravenWinNum*10-5

# now we can do a simple lm
fit &lt;- lm(win~ravensData$ravenScore)

# and get probability of win using inv.logit
fitwin &lt;- inv.logit(fit$fitted.values)
plot(ravensData$ravenScore, fitwin)

# now glm
fitglm &lt;- glm(ravensData$ravenWinNum ~ ravensData$ravenScore, family=""binomial"")
plot(ravensData$ravenScore,fitglm$fitted)
</code></pre>
"
"0.107474459256542","0.102830651637293","175597","<p>I am using the <code>quantreg</code> package in R to develop quantile estimates at different taus, then using <code>anova</code> to test whether the Beta Estimates at different quantiles are equal ($H_0$) or not ($H_1$). Thus </p>

<pre><code>library(quantreg)
data(Mammals) # sample data in quantreg
</code></pre>

<p>for taus 0.1, 0.25, 0.5, 0.75 and 0.9</p>

<pre><code>fit1 &lt;- rq(weight ~ speed + hoppers + specials, tau = .1, data = Mammals)
fit2 &lt;- rq(weight ~ speed + hoppers + specials, tau = .25, data = Mammals)
fit3 &lt;- rq(weight ~ speed + hoppers + specials, tau = .5, data = Mammals)
fit4 &lt;- rq(weight ~ speed + hoppers + specials, tau = .75, data = Mammals)
fit5 &lt;- rq(weight ~ speed + hoppers + specials, tau = .9, data = Mammals)

anova(fit1, fit2, fit3, fit4, fit5, test=""Wald"", joint=FALSE)
</code></pre>

<p>Results in </p>

<pre><code>Quantile Regression Analysis of Deviance Table

Model: weight ~ speed + hoppers + specials
Tests of Equality of Distinct Slopes: tau in {  0.1 0.25 0.5 0.75 0.9  }

             Df Resid Df F value  Pr(&gt;F)  
speed         4      531  1.0952 0.35810  
hoppersTRUE   4      531  2.5898 0.03599 *
specialsTRUE  4      531  1.3774 0.24046  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However changing the order of the models to say;</p>

<pre><code>anova(fit3, fit1, fit2, fit4, fit5, test=""Wald"", joint=FALSE)
</code></pre>

<p>Produces the <strong>exact same result!</strong></p>

<h2>My question is basically, what gives?</h2>

<p><strong>(1)</strong> Is <code>anova</code> truly comparing all the models to one another (ie all estimates from different taus, ${_nC_r} = {_5C_2} = 10$ <strong>separate</strong> comparisons) </p>

<p><strong>OR</strong> </p>

<p><strong>(2)</strong> Is <code>anova</code> selecting the model with the lowest tau and comparing the remaining models to that?</p>

<p>I've extracted (and annotated) the relevant segments of the of <code>anova</code> function called in the <code>quantreg</code> environment bellow.</p>

<pre><code>getAnywhere(anova.rqlist)
sum.fit1 &lt;- summary(fit1, covariance=TRUE); sum.fit2 &lt;- summary(fit2, covariance=TRUE); 
sum.fit3 &lt;- summary(fit3, covariance=TRUE); sum.fit4 &lt;- summary(fit4, covariance=TRUE); 
sum.fit5 &lt;- summary(fit5, covariance=TRUE)
objects &lt;- list(); objects[[1]] &lt;- sum.fit1; objects[[2]] &lt;- sum.fit2 
objects[[3]] &lt;- sum.fit3; objects[[4]] &lt;- sum.fit4; objects[[5]] &lt;- sum.fit5
taus &lt;- c(0.1, 0.25, 0.5, 0.75, 0.9)
m &lt;- length(taus)
n &lt;- length(fit1$y)
    Omega &lt;- outer(taus, taus, pmin) - outer(taus, taus) ##!!!HERE!!!###
    J &lt;- objects[[1]]$J 
# From help file on summary.rq: J is Unscaled Outer product of gradient matrix returned if cov=TRUE and se != ""iid"". The Huber sandwich is cov = tau (1-tau) Hinv %*% J %*% Hinv. 
p &lt;- dim(J)[1]
H &lt;- array(unlist(lapply(objects, function(x) x$Hinv)), c(p, p, m))
# From help file on summary.rq: Hinv : inverse of the estimated Hessian matrix returned if cov=TRUE and se %in% c(""nid"",""ker"") , note that for se = ""boot"" there is no way to split the estimated covariance matrix into its sandwich constituent parts.    
H &lt;- matrix(aperm(H, c(1, 3, 2)), p * m, p) %*% t(chol(J))
W &lt;- (H %*% t(H)) * (kronecker(Omega, outer(rep(1, p), rep(1, p)))) ##!!!HERE!!!###
coef &lt;- unlist(lapply(objects, function(x) coef(x)[, 1]))
Tn &lt;- pvalue &lt;- rep(0, p - 1)
ndf &lt;- m - 1
ddf &lt;- n * m - (m - 1)
for (i in 2:p) {
  E &lt;- matrix(0, 1, p)
  E[1, i] &lt;- 1
  D &lt;- kronecker(diff(diag(m)), E)
  Tn[i - 1] &lt;- t(D %*% coef) %*% solve(D %*% W %*% 
                                         t(D), D %*% coef)/ndf
  pvalue[i - 1] &lt;- 1 - pf(Tn[i - 1], ndf, ddf)
}
pvalue
</code></pre>

<p>The reason i care is that if explanation <strong>(1)</strong> is being implemented then all the estimates are truly being compared, while if explanation <strong>(2)</strong> is being implemented, then technically the models are only being compared to minimum tau and <strong>NOT</strong> to one another. </p>

<p><strong>Note:</strong> The lines that define <code>Omega</code> and <code>W</code> suggest to me that the latter interpretation <strong>(2)</strong> is being implemented, but I'm not sure.</p>
"
"0.026822089039291","0","175617","<p>I was interested in knowing if anyone is using the custom made function of BRT by Elith et al. (2008) in Journal of Animal Ecology <a href=""http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2656.2008.01390.x/pdf"" rel=""nofollow"">""A working guide to boosted regression trees""</a> and knows what does <code>tolerance = fixed</code> or <code>tolerance = auto</code> does in the following function: </p>

<p>This function is found in the <code>dismo</code> package in R. </p>

<p><a href=""https://cran.r-project.org/web/packages/dismo/dismo.pdf"" rel=""nofollow"">https://cran.r-project.org/web/packages/dismo/dismo.pdf</a></p>

<pre><code>?dismo::gbm.step

mdl &lt;- gbm.step(data=df1,
                gbm.x = 4:7,
                gbm.y = 16,
                family = ""gaussian"", 
                tree.complexity = 1,
                learning.rate = 0.1,
                bag.fraction = 0.5,
                tolerance.method = ""fixed"",
                tolerance = 0.1)
</code></pre>
"
"0.0657004319817604","0.0667904674542028","175654","<p>I understand that you have to run the resulting regression line through the logistic function to get the predicted probability:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
p1 &lt;- predict(am.glm, newdata, type=""response"") 
p2 &lt;- 1/(1+exp(-(am.glm$coefficients[1] +
                 am.glm$coefficients[2]*newdata[1,1] + 
                 am.glm$coefficients[3]*newdata[1,2])))
p1 - p2
##            1 
## 1.110223e-16
</code></pre>

<p>Now I want to build two scoring model with the aim in mind to be usable with a hand calculator only: </p>

<ol>
<li>First model: I want to just take the two variables ($hp$, $wt$), multiply them by some factor and add them. The resulting number should be compared to a threshold number which then gives me the decision.</li>
<li>Second model: I want to have certain ranges of the two variables. Depending on the range the variable falls into I am given a number. At the end I simply add all numbers to arrive at my threshold number which again gives me the decision.</li>
</ol>

<p>As an example from the area of credit scoring where these scorecards are used quite heavily (source: <a href=""https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/"" rel=""nofollow"">https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/</a>):</p>

<p><a href=""http://i.stack.imgur.com/RQnHQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RQnHQ.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/5IX9K.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5IX9K.png"" alt=""enter image description here""></a></p>

<p><strong>My question</strong><br>
How to go about and esp. how to transform the logistic regression coefficients to be able to build the two (or any of the two) models? </p>

<p>Perhaps you can even demonstrate the steps in R, making use of the above <code>mtcars</code> logistic regression.</p>
"
"0.0657004319817604","0.0667904674542028","175682","<p>I'm new to using R. I'm attempting to create a microsimulation of individuals health through time.  To do this, I have two survey datasets with the same variables.  First, a large base file, second a smaller but more detailed health transition dataset.  The outcome variable is self-reported health with three states (1 - good, 2 - fair, 3 - poor), the predictors - Age (continuous), health at time t-1, marital status, highest educational qualification, housing tenure and socio-economic social group.</p>

<p>I have conducted a multinomial logistic regression (test) on the second dataset and now wish to use the predict function to apply this to the larger, fist base dataset. In an ideal world, this will be in the form of predicted category probabilities, that I can then generate random numbers (0,1), and assign new health states. </p>

<p>Currently the best I can come up with is:</p>

<pre><code>test &lt;- multinom(health5 ~ ContAge1 + health4 + marstat1 + highqual1 + tenure1 + socstat1, data = EW5FDR)

newpred &lt;- predict(test, newdata = base, type = ""c"")
</code></pre>

<p>This appears to give me predicted outcome category for the new dataset, my question is: how would I change this to give me predicted category probabilities?</p>

<p>And indeed, is this the correct function to be using in the first place?</p>
"
"0.0599760143904067","0.0609710760849692","175726","<p>Say I have a data set, and I want to model the dependent variable <code>y</code>, as a function of <code>x1</code>,<code>x2</code>,<code>x3</code> and <code>x4</code>. I specify my model as:</p>

<pre><code>y ~ x1 + x2 + x3 + x4 + x1*x2
</code></pre>

<p>I specify an interaction between <code>x1</code> and <code>x2</code> based on prior knowledge. However, the r2 value of the single regression model <code>y ~ x1</code> is ~0.90. Is it fair to still test for this interaction, as well as effects of <code>x2</code> and <code>x3</code> on <code>y</code>? Or is it better to do an analysis of the residuals of the model <code>y~x1</code>? The problem I forsee with the analysis of the residuals is that it can only test for a main effect of <code>x2</code> on <code>y</code>, it cannot test for an interaction with <code>x1</code>. Any thoughts on how to best handle this type of problem would be greatly appreciated. </p>
"
"0.0851715717988452","0.0865846528350581","175770","<p>This question is more of theoretical. I am not sure if this is the right place, but still giving it a try. </p>

<p>I have two variables &mdash; direct cost and indirect cost. When sales persons go for a sales pitch to a customer they know about direct cost that they are going to incur for this service, but they don't know much about indirect cost (they will come to know about it in latter stages). An estimate of indirect cost at this stage will be valuable for sales persons. </p>

<p>I am trying to predict indirect cost as a function of direct cost. I am doing this via a simple linear regression. I plotted scatter plot between direct cost and indirect cost and see a <strong>good linear relationship</strong> between them. I also see that direct cost and indirect cost are <strong>highly corelated</strong> to each other with correlation coefficient as 0.98, so I expected a very good prediction accuracy. But surprisingly, my prediction accuracy is not so good. I have around 200,000 points in my training data and average prediction error on training data is 17 %. Though adjusted R-Square value is 0.97. I am using <code>lm()</code> function from R.       </p>

<p>My question is that in case of simple linear regression, in general, should we expect better prediction accuracy if dependent and independent variables are highly correlated or is it my misconception? If we expect good accuracy, am I missing something here. Please note that I have also tried centering these variables around mean. </p>
"
"0.0464572209811883","0.0472279924554862","175956","<p>For a (fictional) <strong>multiple logistic regression</strong>, let's consinder a DV 'hired' (0,1) and <strong>three dichotomous IVs</strong> 'college_degree' (0,1), 'affluent' (0,1) and 'recommendated' (0,1) for <em>N</em> = 1,000 participants.</p>

<p>Running a logistic regression and generating predicted probabilities of being hired using the <code>predict</code> function for a <code>glm</code> object works well. For every respondent I have a probability value ranging continuously from 0 to 1.</p>

<p>Since I do have a base distribution of all three IVs and the DV, I want a kind of simulator that predicts the <strong>percentage/proportion</strong> of the DV using each indivduals predicted probability.</p>

<p>Let's say in the sample 20% are hired, 50% have a college degree, 10% are affluent and 35% are recommendated. I want to use the predicted values to see how much would the <strong>proportion of 'hired' goes</strong> up, when I, e.g., <strong>change the proportion of recommendations to 50%</strong>. I guess, I could also use the equation with the coefficients of the logit model, but would need to run it for every individual.</p>

<p>Is there any way to implement this in R (well or Excel, if that is easier)?</p>
"
"0.053644178078582","0.0545341883149212","175983","<p>I've been asked a question regarding a linear model made with R's <code>lm</code>:</p>

<p>""Did the regression use linear or non-linear iterative least squares?""</p>

<p>I searched a bit and [think that I] understand the difference between the two, but couldn't find any evidence of R's use of linear least squares in <code>lm</code> (which is the one I think it uses). </p>

<p>I combed throuhg <code>lm</code> and its underlying function <code>lm.fit</code> documentation, but couldn't find anything related.</p>

<p>I think the question I was asked is a dumb question, and it's probably wrongly formulated, but I'd appreciate any help as to how I could reply to it.</p>
"
"0.0709645772411954","0.072141950116023","175996","<p>I have been studying a few simple statistical models for (univariate) time series. From my understanding,</p>

<ul>
<li><p>ARIMA and its siblings are used to model the <em>mean</em> of a time series. Rather than a static measure like <code>mean()</code>, the result is a series estimating the mean.</p></li>
<li><p>ARCH and its brothers are used to model the <em>volatility</em> of a time series. Rather than the usual <code>sd()</code>, the result is a series estimating the variance. </p></li>
</ul>

<h2>Question</h2>

<p>What would be a credible model for the correlation of two time series?</p>

<h2>Notes</h2>

<p>While mean models explore the idea of regressing lagged values of the time series, volatility models (eg. ARCH model) explore the idea of regressing lagged residuals where residuals are the difference of a mean model to its original time series.</p>

<p>In its general sense and for a variety of reasons, ARIMA and ARCH are <em>superior</em> models than rolling windows with <code>mean()</code> (popularly known as moving averages outside statistics world) and <code>sd()</code>.</p>

<p>However, there is no such a thing for the <em>correlation</em> of two time series X and Y to my knowledge.</p>

<p>The closest thing would be rolling a sad, straight window with <code>cor()</code>, Pearson's coefficient function in R, and work around the resulting series.</p>

<h2>A poor solution</h2>

<p>Trying to replicate Pearson's correlation model,</p>

<pre><code>p_(X,Y) = cov(X,Y) / (sd(X) sd(Y))
        = E((X-mean(X))(Y-mean(Y))) / (sd(X) sd(Y)),
</code></pre>

<p>to the time series world, I had the above without the intended success.</p>

<pre><code>library('forecast')
library('fGarch')

X &lt;- 1:200 + rnorm(200, sd=10)
Y &lt;- 50 + (1:200)/100 + rnorm(200, sd=5)

plot(1:200, X, t='l', main=""What would be a resulting ts correlation of X and Y?"")
lines(1:200, Y, t='l', col='blue')

# Mimic Pearson correlation, cov(X,Y)/(sd(X)*sd(Y)).
Xm &lt;- as.vector(X) - as.vector(fitted(Arima(X, order=c(2,0,1))))
Ym &lt;- as.vector(Y) - as.vector(fitted(Arima(Y, order=c(2,0,1))))

Xv &lt;- garchFit(formula=~arma(2,1) + garch(2,1), data=X)@sigma.t
Yv &lt;- garchFit(formula=~arma(2,1) + garch(2,1), data=Y)@sigma.t

correlation &lt;- Xm * Ym / (Xv * Yv)    # this can be forecast

plot(correlation, t='l', col='blue', ylim=c(-2, 2), main='Correlation models')
abline(h=c(-1, 1))
abline(h=cor(X, Y), col='red', lwd=5)

# Correlation rolling window of size 10.
df &lt;- data.frame(X, Y)
crw &lt;- rep(NA, 10)
for (i in 11:nrow(df))
  crw &lt;- c(crw, cor(df[(i-10):i, 1], df[(i-10):i, 2]))

lines(crw, col='darkgreen', lwd=5)

legend('topright',
  c('pearson mimic', 'static cor()', 'rolling cor() like moving averages'),
  col=c('blue', 'red', 'darkgreen'), lwd=c(1, 5, 5))
</code></pre>
"
"0.0547503599848004","0.0667904674542028","176084","<p>I want to predict the impact of oil price over a Colombian oil company's stock price. I plan to use a multinomial regression for this with a categorical variable (Up, Down or Neutral given the direction of the stock price). Here is part of my dataset:</p>

<pre><code>Minute  ecopet  profit  sum_profit   direccion  cl1_chg   sum_cl1    direccion_cl1
571     2160     0       10           Up         -0.03     0.00      Down
572     2160     0        0           Neutral     0.07    -0.03      Down
573     2160     0       -5           Down       -0.08     0.04      Up
574     2160     0       -5           Down       -0.07    -0.04      Down
575     2160     5       -5           Down       -0.08    -0.11      Down
576     2165     0       -25          Down        0.00    -0.19      Down
577     2165     0       -25          Down       -0.05    -0.19      Down
578     2165     0       -15          Down       -0.17    -0.24      Down
579     2165     5       -15          Down       -0.06    -0.41      Down
580     2170     0       -20          Down        0.03    -0.47      Down
581     2170    -10       0           Neutral     0.04    -0.44      Down
</code></pre>

<p>My dependent variable is 'direccion'. But as you can see it has 3 response classes.the code I am using in R for the multinomial regression is:</p>

<pre><code>glm.fit=multinomial(direccion~direccion_cl1, data=datos)
</code></pre>

<p>I am working with intraday information and plan to predict what happens when the oil moves up/ down (in the previous 10 minutes) and how it impacts the stock price in the next 10 minutes.</p>

<p>The problem is that once I run the regression, what I get for glm.fit does not include the Coefficients for the level ""Down"". Would you know why is that? I get this:</p>

<pre><code> Call:
 multinom(formula = direccion ~ direccion_cl1, data = datos)

 Coefficients:
          (Intercept) direccion_cl1Up     
 Neutral   1.0505813       0.1955194 
 Up       -0.2513035       0.3936570 

 Residual Deviance: 90752.54 
 AIC: 90764.54 
</code></pre>

<p>Additional to this, when I use the function predict to see how well my model works I get this error message:</p>

<pre><code> log.probs=predict(glm.fit, ""probs"")
 Error in eval(expr, envir, enclos) : object 'direccion_cl1' not found
</code></pre>

<p>Thanks a lot!</p>
"
"0.113796486271632","0.115684483091955","176111","<p>I need to conduct a meta-analysis for a publication, but this is my first meta-analysis and I still donâ€™t feel confident. I will describe the steps I have followed, and hopefully some of you might find errors on my methods, and suggest alternatives.</p>

<p>For this particular analysis, long-term studies should be more important than short-term because a few experiments showed transient effects, hence short-term studies might fail to capture that the effect is not really significant in the long-term. On my dataset, about half the studies have several non-independent measurements taken at different time-points (i.e. several annual measurements). Other experiments, despite having been carried out for several years, show the data already aggregated, with only one row per study with mean and standard deviation. I considered running a multivariate meta-analysis to solve this issue, but it would unbalance the analysis, giving more importance to the experiments with several rows of data (annual measurements) than to the experiments with aggregated data in only one row (pooled across several years). Am I right? This is an example of the dataset:</p>

<ul>
<li>Study 1, Year 1, Effect Size 1 </li>
<li>Study 1, Year 2, Effect Size 2 </li>
<li>Study 1, Year 3, Effect Size 3    </li>
<li>Study 2, Year 1, Effect Size 4    </li>
<li>Study 3, Years 1-4, Effect Size 5</li>
<li>Study 4, Years 1-3, Effect Size 6</li>
</ul>

<p>Alternatively, I decided to try and aggregate the data, so that finally there is only one row per study. I followed these steps:</p>

<p>Calculate effect sites for each row, including those studies with several rows (annual data). In this case, I calculated the log response ratio (ROM):</p>

<pre><code>dat &lt;- escalc (measure=""ROMâ€, n1i=elev.rep, n2i=control.rep, m1i=elev.ANPP.mean, m2i=control.ANPP.mean, sd1i=elev.SD, sd2i=control.SD, data=all)
</code></pre>

<p>Aggregate studies using the function agg {MAd}. I used the Borenstein et al. 2009 method, and correlation=1:</p>

<pre><code>datAgg &lt;- agg(id = id,es = yi,var = vi, cor =1,method = ""BHHR"", data = dat)
</code></pre>

<p>I have now only one row per study. However, since long-term experiments are more important, I have created user-defined weights that take into account the number of replicates and the number of years of each study:</p>

<pre><code>datAgg$weightsTime &lt;- with(datAgg, ((control.rep * elev.rep)/(control.rep + elev.rep)) + ((nyears^2)/(2*nyears)))
</code></pre>

<p>Run the mixed-effects meta-regression with two moderators, using Hedges Estimator (HE) and the Knapp and Hartung approach:</p>

<pre><code>m &lt;- rma.uni(yi, vi, mods= ~ factor(A) * factor(B), method=""HE"", data=datAgg, weights=weightsTime, knha=TRUE)
</code></pre>

<p>Am I doing something wrong? Can this method be improved? So far the results confirm my hypothesis, but of course I might be using a sub-optimal approach. Many thanks</p>
"
"0.142013610993574","0.144369757960699","176129","<p>I've been working on some various time series forecasts and I've begun to notice a trend (pardon the pun) in my analyses. For about 5-7 datasets that I've worked with so far, it would be helpful to allow for multiple seasonal periods along with an option for holiday dummies. I've tried various methods and usually stick with <code>tbats</code> since <code>auto.arima()</code> with regressors has been giving me issues. By this point, it's probably obvious I'm working in R.</p>

<p>Before I get too far, let me give some sample data. Hopefully the following link works: <a href=""https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0"" rel=""nofollow"">https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0</a>.</p>

<p>This data yields the following time series plot:
<a href=""http://i.stack.imgur.com/FYS1x.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FYS1x.jpg"" alt=""Time Series Plot""></a>
The large dips are around Christmas and New Years, however there are also smaller dips around Thanksgiving. In the code below, I name this dataset <code>traindata</code>.</p>

<p>Now, <code>ets</code> and ""plain"" <code>auto.arima</code> don't look so hot in the long run since they are limited to only one seasonal period (I choose weekly). However for my test set that I held out they performed fairly well for the month's worth of data (with the exception of Labor Day weekend). This being said, forecasting out for a year would be ideal.</p>

<p>I next tried <code>tbats</code> with weekly and yearly seasonal periods. That results in the following forecast:
<a href=""http://i.stack.imgur.com/kcXmd.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kcXmd.jpg"" alt=""TBATS Forecast""></a></p>

<p>Now this looks pretty good. From the naked eye it looks great at taking into account the weekly and yearly seasonal periods as well as Christmas and New Years effects (since they obviously fall on the same dates each year). It would be best if I could include the holidays (and the days around them) as dummy variables. Hence my attempts at <code>auto.arima</code> with <code>xreg</code> regressors.</p>

<p>For ARIMA with regressors, I've followed Dr. Hyndman's suggestions for the fourier function (given here: <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) as well as his selection of the number of fourier terms (given here: <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/forecasting-weekly-data/</a>)</p>

<p>My code is as follows:</p>

<pre><code>fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep=""""),period,sep=""_"")
  return(X)
}

fcdaysout&lt;-365
m1&lt;-7
m2&lt;-30.4375
m3&lt;-365.25

hol&lt;-cbind(traindata$CPY_HOL, traindata$DAY_BEFORE_CPY_HOL, traindata$DAY_AFTER_CPY_HOL)
hol&lt;-as.matrix(hol)

n &lt;- nrow(traindata)
bestfit &lt;- list(aicc=Inf)
bestk &lt;- 0

for(i in 1:m1)
{
    fake_xreg = cbind(fourier(1:n,i,m1), fourier(1:n,i,m3), hol)
    fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = fake_xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
    	if(fit$aicc &lt; bestfit$aicc)
    {
        bestfit &lt;- fit
        bestk &lt;- i
    }
    else
    {
    }
}

k &lt;- bestk
k
##k&lt;-3

xreg&lt;-cbind(fourier(1:n,k,m1), fourier(1:n,k,m3), hol)
xreg&lt;-as.matrix(xreg)

aacov_fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aic"", allowdrift=TRUE)
summary(aacov_fit)
</code></pre>

<p>Where my issues come in is inside the for loop to determine the <code>k</code>, the number of fourier terms, that minimizes AIC. In all of my attempts at ARIMA with regressors, it always produces an error when <code>k&gt;3</code> (or <code>i&gt;3</code> if we're talking about inside my loop). The error being <code>Error in solve.default(res$hessian * n.used, A) : system is computationally singular: reciprocal condition number = 1.39139e-34</code>. Simply setting <code>k=3</code> gives some decent results for my test set but for the next year it doesn't appear to adequately catch the steep drops around the end of the year and is much smoother than imagined as evidenced in this forecast:<a href=""http://i.stack.imgur.com/rj30h.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rj30h.jpg"" alt=""AutoArima with Covariates (k=3)""></a></p>

<p>I assume this general smoothness is due to the small number of fourier pairs. Is there an oversight in my code in that I'm just royally screwing up the procedure provided by Dr. Hyndman? Or is there a theoretical issue that I'm unknowingly running into by trying to find more than 3 pairs of fourier terms for the multiple seasons I'm attempting to account for? Is there a better way to include the multiple seasonalities and dummy variables?</p>

<p>Any help in getting these covariates into the arima model with an appropriate number of fourier terms would be appreciated. If not, I'd at least like to know whether or not what I'm attempting is possible in general with larger number of fourier pairs.</p>
"
"0.053644178078582","0.0545341883149212","176147","<p>Im trying to estimate the linear curve (y~x) where I know intercept must be normally distributed around -100, and slope always positive and normally distributed around 2 (blue continous line in plots below).</p>

<p>The example.data.1 below is ""clean"" and the linear regression (red dashed line) is ok. The resulting red dashed line is what I want.</p>

<p>But example.data.2 has many measurement errors so the red dashed line becomes unrealistic. The resulting line should be parallel with the blue line, but lower.</p>

<p>How can I assign a strong prior similar to the blue line in the plots, so that I get a posterior reasonably similar to the blue line?</p>

<pre><code>example.data.1 &lt;- structure(list(x = c(1.36, 2.22, 2.53, 3.09, 3.44, 3.25, 3.15, 
                                       3.21, 3.57, 3.63, 3.51, 2.85, 2.56, 2.25, 1.61, 1.35, 1, 1.6, 
                                       1.92, 1.9, 2.3, 2.61, 3.9, 3.74, 3.77, 3.77, 3.49, 3.37, 3.35, 
                                       2.79, 2.31, 1.88, 1.5, 1.18, 1.83, 2.32, 3.06, 3.37, 3.77, 3.82, 
                                       3.75, 3.72, 3.53, 3.35, 3.67, 3.18, 3.11, 2.43, 1.9, 1.39, 1.17, 
                                       1.48, 2.05, 2.62, 3.08, 3.65, 3.92, 4.08, 4.1, 3.47, 3.84, 3.45, 
                                       2.87, 2.83, 2.49, 1.87, 2.06, 2.49, 1.78, 2.33, 2.95, 3.73, 3.64, 
                                       3.62, 4.1, 3.85, 4.06, 3.67, 3.3, 2.86, 2.46, 2.32, 2.08, 1.64, 
                                       1.96), y = c(-101.04, -99.42, -98.33, -96.88, -95.22, -91.89, 
                                                    -91.63, -90.19, -92.98, -95.58, -95.69, -96.32, -96.94, -98.25, 
                                                    -100.11, -100.81, -101.87, -99.72, -99.94, -100.87, -100.38, 
                                                    -98.64, -93.38, -92.98, -93.39, -93.76, -93.25, -93.12, -94.46, 
                                                    -96.45, -97.46, -99.75, -100.09, -101.62, -101.1, -97.8, -96.33, 
                                                    -96.21, -94.37, -93.18, -93.32, -93.73, -94.13, -94.4, -94.63, 
                                                    -94.83, -96.29, -98.11, -100.2, -100.82, -101.56, -101.35, -100.61, 
                                                    -98.65, -97.37, -95.36, -95.45, -95.33, -95.63, -95.26, -97.08, 
                                                    -97.1, -97.14, -96.9, -98.17, -99.47, -100.17, -100.58, -100.55, 
                                                    -99.94, -99.02, -97.3, -96.25, -95.44, -95.69, -95.21, -95.87, 
                                                    -95.87, -97.71, -96.91, -97.62, -97.94, -98.9, -99.79, -99.88
                                       )), .Names = c(""x"", ""y""), row.names = c(NA, -85L), class = ""data.frame"")

example.data.2 &lt;- structure(list(x = c(3.11, 3.46, 3.42, 3.34, 3.3, 2.45, 4, 4.2, 
                                       4.08, 3.57, 1.97, 1.83, 1.07, 0.68, 0.54, 0.47, 0.63, 3.19, 3.52, 
                                       3.49, 3.47, 3.36, 2.76, 3.42, 3.17, 3.54, 2.56, 1.06, 1.09, 0.84, 
                                       0.64, 0.61, 0.74, 0.49, 3.49, 3.56, 3.46, 3.25, 3.72, 3.57, 3.58, 
                                       2.62, 1.99, 1.85, 1.04, 1.06, 0.62, 0.49, 0.48, 0.68, 0.5, 3.63, 
                                       3.71, 3.75, 3.67, 3.78, 3.52, 3.04, 2.26, 1, 1.17, 1.01, 0.92, 
                                       0.65, 0.54, 0.36, 0.38, 0.3, 3.08, 3.79, 3.9, 3.5, 3.4, 2.57, 
                                       3.03, 1.93, 2.02, 1.5, 0.67, 0.63, 0.72, 0.6, 0.67, 0.63, 0.53
), y = c(-105.28, -104.1, -104.81, -104.34, -104.37, -105.31, 
         -103.59, -103.32, -102.66, -103.57, -103.73, -104.47, -97.69, 
         -92.56, -95.9, -95.72, -107.6, -104.39, -105.12, -104.18, -104.46, 
         -102.19, -103.59, -103.38, -103.48, -102.84, -96.52, -88.54, 
         -90.36, -93.7, -85.21, -89.68, -99.47, -91.92, -104.58, -103.91, 
         -104.47, -104.49, -104.41, -104.41, -102.6, -98.65, -87.98, -89.23, 
         -86.34, -94.21, -91.57, -84.62, -84.14, -95.33, -102.14, -104.18, 
         -103.8, -102.47, -101.75, -101.73, -102.84, -97.49, -92.67, -91.72, 
         -80.45, -80.97, -84.94, -80.2, -81.05, -77.84, -82.72, -91.75, 
         -105.19, -104.66, -104.36, -104.31, -103.57, -102.68, -98.4, 
         -89.48, -85.92, -84.59, -84.49, -81.13, -83.28, -83.12, -85.62, 
         -85.89, -90.07)), .Names = c(""x"", ""y""), row.names = c(NA, -85L
         ), class = ""data.frame"")


lm.1 &lt;- coefficients(lm(example.data.1$y~example.data.1$x))
lm.2 &lt;- coefficients(lm(example.data.2$y~example.data.2$x))

library(ggplot2)
p &lt;- ggplot(example.data.1, aes(x=x, y=y))
p &lt;- p + geom_point()
p &lt;- p + geom_abline(intercept=(-100), slope=2, color=""blue"")
p &lt;- p + geom_abline(intercept=lm.1[1], slope=lm.1[2], color=""red"", linetype=""dashed"")
p &lt;- p + xlim(0, 10)
p &lt;- p + ylim(-110, -50)
p 

p &lt;- ggplot(example.data.2, aes(x=x, y=y))
p &lt;- p + geom_point()
p &lt;- p + geom_abline(intercept=(-100), slope=2, color=""blue"")
p &lt;- p + geom_abline(intercept=lm.2[1], slope=lm.2[2], color=""red"", linetype=""dashed"")
p &lt;- p + xlim(0, 10)
p &lt;- p + ylim(-110, -50)
p
</code></pre>

<p>I need to do this for tens of thousands of data-groups like each example above, so a fast algorithm is important. I have tried to use Stan, Jags, and arm-package, but don't understand how to tell those functions what I want.</p>

<p>My limited knowledge about statistics lead me to think that a bayesian approach is best, but I could be wrong.</p>
"
"0.0709645772411954","0.072141950116023","176351","<p>I know there is an analytic solution to the following problem (OLS). Since I try to learn and understand the principles and basics of MLE, I implemented the fisher scoring algorithm for a simple linear regression model. </p>

<p>$$
y = X\beta + \epsilon\\
\epsilon\sim N(0,\sigma^2)
$$</p>

<p>The loglikelihood for $\sigma^2$ and $\beta$ is given by:
$$
-\frac{N}{2}ln(2\pi)-\frac{N}{2}ln(\sigma^2)-\frac{1}{2\sigma^{2}}(y-X\beta)^{'}(y-X\beta)
$$ </p>

<p>To compute the score function $S(\theta)$, where $\theta$ is the vector of parameters $(\beta,\sigma^{2})^{'}$, I take the first partial derivatives with respect to $\beta$ and $\sigma^{2}$:
$$
\frac{\partial L}{\partial \beta} = \frac{1}{\sigma^{2}}(y-X\beta)^{'}X
\\
\frac{\partial L}{\partial \sigma^2} = -\frac{N}{\sigma^{2}}+\frac{1}{2\sigma^{4}}(y-X\beta)^{'}(y-X\beta)
$$</p>

<p>Then the Fisher scoring algorithm is implemented as:
$$
\theta_{j+1} = \theta_{j} - (S(\theta_{j})S(\theta_{j})^{'})S(\theta_{j})
$$</p>

<p>Please note, the following code is a very naive implementation (no stopping rule, etc.)</p>

<pre class=""lang-R prettyprint-override""><code>library(MASS)
x &lt;- matrix(rnorm(1000), ncol = 2)
y &lt;- 2 + x %*% c(1,3) + rnorm(500)

fisher.scoring &lt;- function(y, x, start = runif(ncol(x)+1)){
    n &lt;- nrow(x)
    p &lt;- ncol(x)
    theta &lt;- start
    score &lt;- rep(0, p+1)
    for (i in 1:1e5){
        # betas
        score[1:p] &lt;- (1/theta[p+1]) * t((y - x%*%theta[1:p])) %*% x
        # sigma
        score[p+1] &lt;- -(n/theta[p+1]) + (1/2*theta[p+1]^2) * crossprod(y - x %*% theta[1:p])
        # new
        theta &lt;- theta - MASS::ginv(tcrossprod(score)) %*% score
    }
    return(theta)
}

# Gives the correct result 
lm.fit(cbind(1,x), y)$coefficients
# Does not give the correct result
fisher.scoring(y, cbind(1,x))
# Even if you start with the correct values
fisher.scoring(y, cbind(1,x), start=c(2,1,3,1))
</code></pre>

<p><strong>My Question</strong></p>

<p><em>What did I miss? Where is my mistake?</em></p>
"
"0.0464572209811883","0.0314853283036575","176470","<p>I've used R's non-linear regression function <code>nls</code> to find estimates for two different functions that both give a reasonably good plot for my data. However, I'm supposed to find out which one of the two functions is a better fit for my data. How can I do that? I've already made QQplots for the residuals of both functions, and both show that the residuals are normally distributed. Are there any other ways to check which one of my two functions fits my data best?</p>
"
"0.113796486271632","0.109257567364624","176586","<p>This question stems from <a href=""http://stats.stackexchange.com/questions/175853/what-type-of-hypothesis-test-for-multivariate-testing-website"">another I asked last week</a>, where the person answering stated </p>

<blockquote>
  <p>""Finally, and this is very, very important: please don't just run the
  code I've provided, and consider your job complete. If you don't
  actually read up and understand some of how these analyses work, all
  of this information will be less than useless.""</p>
</blockquote>

<p>This is my intention, to really understand what is going on as well as how to interpret.</p>

<p>Context is website testing. Show people a different landing page, change the design and look of each page with a goal of getting more people to purchase online (""success"").</p>

<p>Here is my data:</p>

<pre><code>variant successes   failures
Original    757 49114
Date    553 41794
Cranberry   494 41495
Apple   546 41835
</code></pre>

<p>My script and output are below. I think I understand how to interpret it but just wanted to make sure. My questions:</p>

<ol>
<li>The first thing I want to do is check if there is a difference between the variance overall, or if it's just ebbs n flows. With a p-value of 8.55e-05 translates to 0.0000855 (right?) then yes, there is a meaningful variance between the groups. Is that a correct statement?</li>
<li>Since I'm comparing each group to the original (It's really a case of ""which test can beat the original), then it looks like only first Vs. 4th (Original Vs. Apple) is the only real difference statistically because the p-value is 0.0098. Is this a correct statement?</li>
<li>In my contrast function I have assumed data are read int he order they appear in test2. Is this correct?</li>
<li>Reading more about logistic regression it seems to be used to measure the impact of incrementing a predictor up or down a unit (resulting in the log unit increase or decrease). But in the context of measuring a web page variant performance in this way, why is logistic regression an appropriate method of determining whether or not the variants are different? Put another way, I'm hypothesis testing rather than predicting the impact of each variant, since an observation can only be one variant, not a combination of 1 or more predictors (they can only ever see one of the test pages, not 2 or more test pages).</li>
<li>I edited my data to include only visits from one state, just to experiment and play around. The output I got in this instance was a p-value of 0.001721 in the anova of m whereas the p-values for contrast where between 0.2 -0.3 (reject). If the script says overall there is a variance but at an individual test level there is not, how would I interpret that? I can provide the output if desired.</li>
</ol>

<p>Here is my script &amp; output:</p>

<pre><code>&gt; test2 &lt;- read.csv(""test2.csv"")
&gt; 
&gt; m &lt;- glm(cbind(successes, failures) ~ variant, family=binomial, data=test2)
&gt; anova(m, test='Chisq') # Tests if there's a difference between the variants
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(successes, failures)

Terms added sequentially (first to last)


        Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
NULL                        3     21.435             
variant  3   21.435         0      0.000 8.55e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; 
&gt; library(lsmeans)
&gt; #lsmeans(m, pairwise ~ variant) # Compares every variant to every other one
&gt; 
&gt; m.comparisons = lsmeans(m, specs = pairwise ~ variant)
&gt; contrast(m.comparisons,
+          list(
+            first.vs.second = c(1,-1,0,0),
+            first.vs.third =  c(1,0,-1,0),
+            first.vs.fourth = c(1,0,0,-1)
+            ), adjust=""tukey"")
 contrast           estimate         SE df    z.ratio p.value
 first.vs.second  0.09192309 0.06248035 NA  1.4712319  0.3667
 first.vs.third  -0.01371955 0.06072602 NA -0.2259254  0.9943
 first.vs.fourth -0.16633346 0.05653998 NA -2.9418735  0.0098

P value adjustment: sidak method for 3 tests 
</code></pre>
"
"0.0889588054368324","0.0904347204435887","176622","<p>In many papers in the social sciences, missing data are handled by <em>direct</em> or <em>full information</em> maximum likelihood estimation (FIML). Unfortunately this is almost always done with closed source software.  </p>

<p>To get an idea how FIML works, I tried to implement it myself in R. </p>

<p>In case of a multivariate normal distribution the loglikelihood for a single observation is given by (according to Enders, 2001, p. 134):</p>

<p>$$
log L_{i} = K_{i} - \frac{1}{2}log|\Sigma_i|-\frac{1}{2}(x_{i}-\mu_{i})^{'}\Sigma^{-1}(x_{i}-\mu_{i})
$$</p>

<p>Where $K_{i}$ is the numer of observed variables for obesrvation $i$. </p>

<p>In case of a simple linear regression model with $N$ observations and assumed homoskedascity the formula should reduce to:</p>

<p>$$
log L_{i} = K_{i} - \frac{1}{2}log(\sigma^{2})-\frac{1}{2\sigma^2}(y_{i}-X_{i}\beta_{i})^{2}
$$</p>



<pre><code>set.seed(42)
x &lt;- matrix(rnorm(1000), nrow=500) ; x &lt;- cbind(1,x)
y &lt;- x %*% c(2,1,3) + rnorm(500, mean = 0, sd = 1)
z.full &lt;- cbind(y,x)

# MCAR predictors
x[sample.int(n = 500, size = 50),2] &lt;- NA
x[sample.int(n = 500, size = 50),3] &lt;- NA
z.miss &lt;- cbind(y,x)

llog.single &lt;- function(z, beta, sigma){
    y &lt;- z[1]
    x &lt;- z[-1]
    idx &lt;- !is.na(x)  # = K_{i}
    return(sum(idx) + dnorm(y, mean = x[idx]%*%beta[idx], sd = sqrt(sigma), log = TRUE))
}

loglikelihood &lt;- function(y, x, theta) {
    p &lt;- ncol(x)
    beta  &lt;- theta[1:p]
    sigma &lt;- theta[p+1]
    return(-sum(apply(cbind(y,x), 1, llog.single, beta = beta, sigma = sigma)))
}

# Full information maximum likelihood 
optim(par = c(0,0,0,1), fn = loglikelihood, method = ""BFGS"", x=z.mis[,-1], y=z.miss[,1])$par
# OLS on full dataset
lm(V1 ~ V3 + V4, data = as.data.frame(z.full))
</code></pre>

<p><strong>My Question</strong></p>

<p><em>What should I do, if there is missing data in the outcome $Y_{i}$?</em></p>

<p>From my current point of view I would just ignore cases with missing data in $Y$. Is this correct or is there maybe a better way?</p>

<hr>

<p>Enders, C. K. (2001). A primer on maximum likelihood algorithms available for use with missing data. Structural Equation Modeling, 8(1), 128-141.</p>
"
"NaN","NaN","176788","<p>I'm running a LASSO regression following this <a href=""https://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome"">guide</a>. I pre - processed my dependent variable using a simple power transformation to obtain a standard normal distribution. Unfortunately, this means I have NA's in my dependent variable, so I can't run LASSO using glmnet (returns: <code>Error in elnet(x, is.sparse, ix, jx, y, weights, offset, type.gaussian,  : 
  NA/NaN/Inf in foreign function call (arg 6)</code>. </p>

<p>Is there anyway to overcome this? </p>
"
"0.0379321620905441","0.0385614943639849","177189","<p>I am currently trying to solve a Kaggle Competition (Rossmann Store Sales). 
My linear regression model consists of all fields in the dataset. The objective is to predict sales.</p>

<p>List of all variables: <a href=""https://www.kaggle.com/c/rossmann-store-sales/data"" rel=""nofollow"">rossmann-store-sales</a></p>

<pre><code>ctrl &lt;- trainControl(method = ""cv"",number = 10)
mod1 &lt;- train(Sales ~ . - Date ,data = train,method = ""lm"",
              trControl = ctrl, metric=""Rsquared"")
summary(mod1)
mod1
</code></pre>

<p>In the training set, the data contains a Field called <code>Customers</code> and it is missing in the testing set. </p>

<p>My predict function fails since it encounters a missing field (<code>Customers</code>) in my testing set.</p>

<pre><code>pred &lt;- predict(mod1,newdata = test)
</code></pre>

<p>How to handle such a case or is it, that this field is suppose to help me in feature engineering?</p>

<p>Please let me know if any further information is required.</p>
"
"0.0599760143904067","0.0609710760849692","177516","<p>I aim to perform a Cox regression. My data set contains roughly 10 variables which I intend to include, for a total of 5000 patients, yielding 900 events. I want to present how a certain variable relates to hazard by using restricted cubic splines. However I cannot do that with the complete case data set due to missing data; I tried it, though, and it returned very implausible results. Contrary to all previous studies. So i imputed data with the mice package. And then analyzed one of these data sets by using the rms package functions cph() and plot.Predict(). However, displaying the results from one of the complete data sets is not acceptable, as most statisticians would concur.</p>

<p>How would you resolve this?</p>

<pre><code># Data to set out
library(mice)
library(survival)
# Example data set
data(lung)

# impute 5 data sets
imputation   &lt;- mice(lung, m=5, maxit=10, seed=500)

# Fit a Cox proportional hazards mdoel
fit &lt;- with(imputation, coxph(Surv(time, status) ~ pspline(meal.cal) + pat.karno))

# How on earth to proceed to create plot showing how the variable modelled as a spline relates to hazard?
</code></pre>
"
"0.0889588054368324","0.0904347204435887","177654","<p>When running an ordered logistic regression using the <code>polr</code> function of the <code>MASS</code>package (DV is low, medium, high) and have a look at the summary I get Î²s for every IV and the intercepts for low|medium and medium|high.</p>

<p>The <code>predict</code>function for assessing the probabilities (<code>type='p'</code>) or the classes (<code>type='class'</code>) also works just fine.</p>

<p>However I want to calculate the probabilities myself in order to use them with different data sets.</p>

<p>If I use the following code for a <em>logistic model with a binary (!) dependent variable</em>, I can exactly replicate the <code>predict</code> - outcome:</p>

<p><code>log_pred &lt;- (logit_model$coefficients[1] + logit_model$coefficients[2]*IV_1 + logit_model$coefficients[3]*IV_2)</code></p>

<ul>
<li><code>logit_model</code> is my <code>glm</code>-object</li>
<li><code>logit_model$zeta[1]</code> is the first intercept</li>
<li><code>logit_model$zeta[2]</code> is the second intercept</li>
<li><code>logit_model$coefficients[1]</code> is the Î² of IV_1</li>
<li><code>logit_model$coefficients[2]</code> is the Î² of IV_2</li>
</ul>

<p>the only thing I have to do now, to get the predicted probabilities is:</p>

<p><code>log_pred_probs &lt;- exp(log_pred)/(1+exp(log_pred))</code></p>

<p>If I understand all the posts on ordered logistic regression I read correctly, the only thing I have to change with a <code>polr</code> object with the 3 ""groups"" of low, medium, and high would be to:</p>

<ul>
<li>run the <code>log-pred</code>part for each group using their own intercepts, let's call them <code>log_pred1</code> and <code>log_pred2</code></li>
<li>and to, then, run the following code (similar to the logistic model above):
<code>log_pred_probs1 &lt;- exp(log_pred1)/(1+exp(log_pred1)+exp(log_pred2))</code> for ""low""
<code>log_pred_probs2 &lt;- exp(log_pred2)/(1+exp(log_pred1)+exp(log_pred2))</code> for ""medium""
<code>log_pred_probs3 &lt;- 1/(1+exp(log_pred1)+exp(log_pred2))</code> for ""high""</li>
</ul>

<p>I think there are at least two problems ('cause this doesn't work at all):</p>

<ol>
<li>I need the Î²-coefficients for every level of the dependent variable, and <code>summary(polr-object)</code>does only show the Î²s for the first group (so does <code>$coefficients</code>)</li>
<li>and I am not sure about the computation of the predicted probabilities for group 3, ""high"".</li>
</ol>

<p>So these are the questions in short: <strong>How do I assess the Î²-coefficients for every level of the DV in a <code>polr</code>object?</strong></p>

<p>And</p>

<p><strong>How do I compute the predicted probabilities for every level of the DV myself?</strong></p>
"
"0.053644178078582","0.0409006412361909","177794","<p>I am performing a spatial autoregressive model (function errorsarlm in spdep R package). Although I tried a number of alternative variables transformations, the model' residuals are always non-normal.
Can anyone suggest a way to handle this?
Does it exist any non-parametric or non-linear version of errorsarlm function in R?</p>

<p>I found the package ""McSpatial"" that is described as a package for ""Nonparametric spatial data analysis"". However from the function descriptions I cannot understand if they are what I'm looking for.</p>

<p>I would like to stick to an SAR model as the LM model's residuals are significantly spatially autocorrelated (tested through Moran I).</p>
"
"0.0851715717988452","0.0944559849109725","177805","<p>R and statistics beginner here, trying to do a quantile regression on a non-linear dataset. </p>

<p>I want to identify datapoints that have a higher y axis value that expected given their value on the x axis. 
I should highlight that the y-data are means of discrete values (0.1-1, in steps of 0.1) taken in dependence on the x-data. x values are number of SNPs in a gene. Each SNP has a discrete value and the y value is a mean of these SNP values for each gene.</p>

<p>After initially investigating  funnel plots it seems that a quantile regression might be most appropriate for this dataset, though thoughts on this are welcome.  I'd appreciate any guidance in fitting a quantile regression to identify that don't fall within 95 percent of the data.</p>

<p>Sample of data (I actually have ~20,000 datapoints):</p>

<pre><code>GENE    mean  total
X1  0.1 3
X2  0.1466666667    30
X3  0.1375  8
X4  0.24    5
X5  0.2625  8
X6  0.2 1
X7  0.1466666667    15
X8  0.2 1
X9  0.1666666667    9
X10 0.1 1
X11 0.1928571429    14
X12 0.1 2
X13 0.1545454545    11
X14 0.1333333333    3
X15 0.1666666667    3
X16 0.2117647059    34
X17 0.1452380952    42
X18 0.16    5
X19 0.2 1
X20 0.25    2
X21 0.125   4
X22 0.2 13
X23 0.1714285714    7
X24 0.15    6
X25 0.2 3
X26 0.2894736842    19
X27 0.2352941176    17
X28 0.1333333333    6
X29 0.12    5
X30 0.2 3
X31 0.1 1
X32 0.1571428571    7
X33 0.2125  8
X34 0.18125 16
X35 0.26    10
X36 0.1368421053    19
X37 0.1333333333    6
X38 0.15    2
X39 0.14    5
X40 0.18    15
X41 0.14    5
X42 0.3 1
X43 0.1 2
X44 0.1 6
X45 0.1 4
X46 0.1 1
X47 0.1333333333    3
X48 0.1166666667    6
X49 0.225   4
X50 0.2 15
X51 0.125   12
X52 0.1 3
X53 0.1714285714    14
X54 0.175   4
X55 0.3404761905    42
X56 0.1 1
X57 0.25    2
X58 0.15    4
X59 0.1 1
X60 0.1666666667    3
X61 0.3 2
X62 0.225   4
X63 0.3076923077    13
X64 0.1 1
X65 0.1666666667    3
X66 0.1666666667    6
X67 0.1 3
X68 0.1 3
X69 0.1166666667    6
X70 0.125   8
X71 0.2 1
X72 0.2 2
X73 0.1333333333    42
X74 0.1 1
X75 0.2 8
X76 0.1444444444    9
X77 0.1666666667    15
X78 0.1 2
X79 0.176744186 43
X80 0.1275  40
X81 0.1666666667    3
X82 0.125   4
X83 0.2545454545    11
X84 0.1304347826    46
X85 0.21    10
X86 0.1571428571    7
X87 0.3 9
X88 0.275   16
X89 0.11    10
X90 0.1333333333    6
X91 0.2333333333    3
X92 0.2 2
X93 0.2866666667    15
X94 0.25    2
X95 0.1125  8
X96 0.4 11
X97 0.1 1
X98 0.2 2
X99 0.15    2
X100    0.1625  8
X101    0.24    5
X102    0.175   4
X103    0.15    4
X104    0.1333333333    3
X105    0.4 2
X106    0.2 3
X107    0.25    2
X108    0.32    5
X109    0.2333333333    3
X110    0.1714285714    7
X111    0.2 1
X112    0.225   4
X113    0.2 1
X114    0.1714285714    7
X115    0.15    2
X116    0.1166666667    6
X117    0.16875 16
X118    0.1555555556    9
X119    0.15    6
X120    0.12    5
X121    0.1 1
X122    0.1333333333    6
X123    0.2333333333    3
X124    0.1 1
X125    0.2333333333    3
X126    0.1333333333    3
X127    0.1 1
X128    0.1827586207    29
X129    0.25    8
X130    0.2 7
X131    0.25    6
X132    0.1 1
X133    0.125   4
X134    0.2 1
X135    0.1666666667    3
X136    0.1 3
X137    0.12    5
X138    0.1 1
X139    0.175   4
X140    0.1 1
X141    0.1666666667    3
X142    0.1666666667    3
X143    0.1 1
X144    0.1375  8
X145    0.1 9
X146    0.1 2
X147    0.125   4
X148    0.1333333333    3
X149    0.1769230769    13
X150    0.15    2
X151    0.1214285714    14
X152    0.1 1
X153    0.2555555556    18
X154    0.2 1
X155    0.1 1
X156    0.1 1
X157    0.1 1
X158    0.4 1
X159    0.14    5
X160    0.1 2
X161    0.1333333333    3
X162    0.375   8
X163    0.2263157895    19
X164    0.1636363636    11
X165    0.3 1
X166    0.1 3
X167    0.2 1
X168    0.3 1
X169    0.1428571429    7
X170    0.1 2
X171    0.1222222222    9
X172    0.1 8
X173    0.1 5
X174    0.1 8
X175    0.1666666667    3
X176    0.2 5
X177    0.1 4
X178    0.1166666667    6
X179    0.15    2
X180    0.3666666667    3
X181    0.25    4
X182    0.1 1
X183    0.1 2
X184    0.1 1
X185    0.1 1
X186    0.1 1
X187    0.184   25
X188    0.2333333333    3
X189    0.2333333333    3
X190    0.1 2
X191    0.32    5
X192    0.1 2
X193    0.12    5
X194    0.1 5
X195    0.2 1
X196    0.1 6
X197    0.1 2
X198    0.4 1
X199    0.2 2
X200    0.1 2
X201    0.2 1
X202    0.2333333333    6
X203    0.35    2
X204    0.1 1
X205    0.12    5
X206    0.14    5
X207    0.125   4
X208    0.3333333333    3
X209    0.1 2
X210    0.1 3
X211    0.1 1
X212    0.2 4
X213    0.15    8
X214    0.125   4
X215    0.1548387097    31
X216    0.2 7
X217    0.225   4
X218    0.125   4
X219    0.15    2
X220    0.4 1
X221    0.275   4
X222    0.325   4
X223    0.2 3
X224    0.175   4
X225    0.3 1
X226    0.1 1
X227    0.19    10
X228    0.25    4
X229    0.2666666667    9
X230    0.1 1
X231    0.2 1
X232    0.3 1
X233    0.2166666667    6
X234    0.26    5
X235    0.225   4
X236    0.1 1
X237    0.1857142857    7
X238    0.58    5
X239    0.25    10
X240    0.6066666667    15
X241    0.3 1
X242    0.5 2
X243    0.2333333333    3
X244    0.25    2
X245    0.1 4
X246    0.1 1
X247    0.1714285714    7
X248    0.16875 16
X249    0.2 1
X250    0.4 3
X251    0.1 1
X252    0.1666666667    6
X253    0.2 6
X254    0.3166666667    12
X255    0.1 1
X256    0.1 2
X257    0.4 1
X258    0.1333333333    3
X259    0.225   4
X260    0.2571428571    7
X261    0.4 5
X262    0.15    10
X263    0.1571428571    7
X264    0.2 11
X265    0.2285714286    7
X266    0.15    4
X267    0.3 1
X268    0.1384615385    13
X269    0.1 4
X270    0.1 1
X271    0.16    5
X272    0.1285714286    7
X273    0.1 1
X274    0.2222222222    9
X275    0.2083333333    12
X276    0.2153846154    13
X277    0.1888888889    9
X278    0.1 1
X279    0.1 2
X280    0.3 2
X281    0.17    10
X282    0.1 5
X283    0.2833333333    6
X284    0.1333333333    6
X285    0.1833333333    6
X286    0.1833333333    12
X287    0.1953488372    43
X288    0.2526315789    19
X289    0.1 1
X290    0.125   4
X291    0.26    5
X292    0.1 2
X293    0.2578947368    19
X294    0.2545454545    11
X295    0.1 1
X296    0.3666666667    3
X297    0.1714285714    7
X298    0.1833333333    6
X299    0.16    5
X300    0.2733333333    15
X301    0.275   4
X302    0.1 1
X303    0.2 7
X304    0.1583333333    12
X305    0.1666666667    3
X306    0.1 1
X307    0.1 6
X308    0.1642857143    14
X309    0.1 1
X310    0.1606060606    33
X311    0.1428571429    7
X312    0.1888888889    9
X313    0.2 2
X314    0.1388888889    18
X315    0.35    2
X316    0.3 2
X317    0.1 4
X318    0.15    16
X319    0.1166666667    12
X320    0.1888888889    9
X321    0.16    5
X322    0.2333333333    3
X323    0.1857142857    14
X324    0.31    20
X325    0.2 1
X326    0.1 1
X327    0.1952380952    21
X328    0.215625    32
X329    0.1 1
X330    0.1 1
X331    0.1307692308    13
X332    0.1 4
X333    0.1666666667    3
X334    0.2 14
X335    0.1583333333    12
X336    0.1961538462    26
X337    0.2222222222    9
X338    0.1 3
X339    0.1 2
X340    0.1285714286    14
X341    0.175   4
X342    0.125   4
X343    0.1 4
X344    0.1428571429    7
X345    0.1 4
X346    0.1 2
X347    0.15    2
X348    0.25    4
X349    0.22    5
X350    0.1 2
X351    0.1 3
X352    0.14    10
X353    0.1666666667    18
X354    0.1333333333    3
X355    0.2 3
X356    0.16    5
X357    0.3 1
X358    0.175   4
X359    0.5 1
X360    0.1111111111    9
X361    0.2333333333    6
X362    0.175   4
X363    0.227027027 37
X364    0.3857142857    7
X365    0.1 2
X366    0.2 3
X367    0.1916666667    12
X368    0.1428571429    14
X369    0.2666666667    3
X370    0.2 9
X371    0.25    2
X372    0.2 1
X373    0.1 2
X374    0.225   4
X375    0.1 1
X376    0.1 3
X377    0.3 2
X378    0.1 1
X379    0.1545454545    11
X380    0.1730769231    52
X381    0.1 3
X382    0.1333333333    3
X383    0.1814814815    27
X384    0.108   25
X385    0.2666666667    6
X386    0.1666666667    3
X387    0.25    8
X388    0.225   4
X389    0.24    25
X390    0.2666666667    6
X391    0.1 2
X392    0.15    4
X393    0.1666666667    6
X394    0.1 1
X395    0.2375  8
X396    0.125   4
X397    0.1 7
X398    0.1 7
X399    0.1 4
X400    0.1 2
X401    0.1625  8
X402    0.3 1
X403    0.3 2
X404    0.25    4
X405    0.2 1
X406    0.1285714286    7
X407    0.15    8
X408    0.5 1
X409    0.1 1
X410    0.1285714286    7
X411    0.1 1
X412    0.2166666667    30
X413    0.22    5
X414    0.2714285714    14
X415    0.1214285714    14
X416    0.2 8
X417    0.28    5
X418    0.24    35
X419    0.15    4
X420    0.1333333333    12
X421    0.125   4
X422    0.1 1
X423    0.1666666667    3
X424    0.2111111111    9
X425    0.3 4
X426    0.2 2
X427    0.2 3
X428    0.1 1
X429    0.1 1
X430    0.1617021277    47
X431    0.15    8
X432    0.1142857143    14
X433    0.15    4
X434    0.1384615385    13
X435    0.1 2
X436    0.1166666667    12
X437    0.1714285714    14
X438    0.2416666667    12
X439    0.1 1
X440    0.1428571429    7
X441    0.1 1
X442    0.1416666667    12
X443    0.3333333333    6
X444    0.2 1
X445    0.14    5
X446    0.2 3
X447    0.225   28
X448    0.1571428571    14
X449    0.1 1
X450    0.1583333333    12
X451    0.1518518519    27
X452    0.1363636364    11
X453    0.2 1
X454    0.1666666667    6
X455    0.1 1
X456    0.1333333333    3
X457    0.2368421053    19
X458    0.1222222222    9
X459    0.15    2
X460    0.2 1
X461    0.1625  24
X462    0.2 6
X463    0.1666666667    3
X464    0.1 3
X465    0.3 8
X466    0.1523809524    21
X467    0.1 3
X468    0.1 3
X469    0.15    4
X470    0.1 1
X471    0.1642857143    28
X472    0.1 5
X473    0.1 2
X474    0.12    15
X475    0.1 3
X476    0.1090909091    11
X477    0.1346153846    26
X478    0.125   4
X479    0.1444444444    9
X480    0.2 1
X481    0.1 1
X482    0.1 3
X483    0.2 3
X484    0.1375  8
X485    0.1 4
X486    0.12    5
X487    0.1739130435    23
X488    0.25    2
X489    0.1333333333    6
X490    0.3 1
X491    0.225   20
X492    0.175   4
X493    0.1 3
X494    0.1222222222    9
X495    0.1 1
X496    0.175   4
X497    0.2333333333    6
X498    0.1615384615    13
X499    0.15    8
X500    0.1666666667    6
X501    0.2 2
X502    0.1777777778    9
X503    0.15    4
X504    0.2666666667    3
X505    0.1 4
X506    0.1222222222    9
X507    0.15    2
X508    0.2 3
X509    0.1333333333    15
X510    0.14    5
X511    0.1 1
X512    0.4 1
X513    0.2125  8
X514    0.36    5
X515    0.34    5
X516    0.4 1
X517    0.1428571429    7
X518    0.3333333333    3
X519    0.1 3
X520    0.2277777778    18
X521    0.1916666667    12
X522    0.2 4
X523    0.1857142857    7
X524    0.1 2
X525    0.1 5
X526    0.2222222222    9
X527    0.1818181818    11
X528    0.2151515152    33
X529    0.1 3
X530    0.1214285714    14
X531    0.2 1
X532    0.1 2
X533    0.1 3
X534    0.1166666667    12
X535    0.1 2
X536    0.1 2
X537    0.1 1
X538    0.2379310345    29
X539    0.175   4
X540    0.1363636364    11
X541    0.1 1
X542    0.1479166667    48
X543    0.1928571429    28
X544    0.4 1
X545    0.1951219512    41
X546    0.1333333333    3
X547    0.15    4
X548    0.2833333333    6
X549    0.1547619048    42
X550    0.1555555556    9
X551    0.2363636364    11
X552    0.2142857143    7
X553    0.5 1
X554    0.15    4
X555    0.1709677419    31
X556    0.17    10
X557    0.1 2
X558    0.2866666667    15
X559    0.4 2
X560    0.15    2
X561    0.1424242424    66
X562    0.25    2
X563    0.1 3
X564    0.1285714286    7
X565    0.12    5
X566    0.25    4
X567    0.2263157895    19
X568    0.1 12
X569    0.1666666667    6
X570    0.5 1
X571    0.147826087 23
X572    0.1 1
X573    0.1818181818    11
X574    0.2 2
X575    0.15    2
X576    0.2 3
X577    0.16    15
X578    0.1621621622    37
X579    0.1333333333    3
X580    0.1333333333    12
X581    0.18    5
X582    0.1534482759    58
X583    0.1538461538    26
X584    0.1 9
X585    0.2142857143    7
X586    0.1 1
X587    0.1222222222    9
X588    0.1 1
X589    0.1 3
X590    0.1 6
X591    0.15    2
X592    0.1 2
X593    0.3 1
X594    0.1285714286    21
X595    0.2 2
X596    0.12    5
X597    0.1 1
X598    0.1 1
X599    0.1 2
X600    0.1153846154    13
X601    0.1 15
X602    0.1 1
X603    0.1 1
X604    0.1 4
X605    0.15    10
X606    0.15    4
X607    0.15    4
X608    0.2 1
X609    0.14    5
X610    0.2 1
X611    0.1 2
X612    0.1 3
X613    0.125   4
X614    0.172   25
X615    0.2 4
X616    0.1727272727    11
X617    0.2090909091    22
X618    0.1333333333    3
X619    0.1 7
X620    0.15    4
X621    0.1181818182    11
X622    0.1375  8
X623    0.1666666667    3
X624    0.1 3
X625    0.1090909091    11
X626    0.125   8
X627    0.1 2
X628    0.12    5
X629    0.1 8
X630    0.13    40
X631    0.1666666667    3
X632    0.34    5
X633    0.1714285714    7
X634    0.1636363636    11
X635    0.1 1
X636    0.1 1
X637    0.18125 16
X638    0.2 4
X639    0.2 8
X640    0.1 2
X641    0.1 1
X642    0.1166666667    6
X643    0.2 1
X644    0.6 1
X645    0.2666666667    9
X646    0.2666666667    3
X647    0.2 2
X648    0.1 2
X649    0.1 1
X650    0.1 2
X651    0.1 1
X652    0.125   4
X653    0.15    2
X654    0.1 1
X655    0.1 1
X656    0.35    4
X657    0.2666666667    3
X658    0.1 2
X659    0.1 1
X660    0.2 1
X661    0.1 2
X662    0.1 2
X663    0.1333333333    3
X664    0.1 2
X665    0.1 1
X666    0.225   4
X667    0.1666666667    6
X668    0.1 2
X669    0.1 3
X670    0.175   4
X671    0.1 3
X672    0.15    4
X673    0.1666666667    3
X674    0.1 3
X675    0.175   4
X676    0.25    8
X677    0.25    4
X678    0.2571428571    7
X679    0.1 1
X680    0.2571428571    7
X681    0.208   25
X682    0.325   12
X683    0.1 1
X684    0.25    2
X685    0.1 2
X686    0.3047619048    21
X687    0.24    5
X688    0.15    6
X689    0.1333333333    6
X690    0.3 1
X691    0.1 1
X692    0.15    2
X693    0.23    20
X694    0.2 2
X695    0.1666666667    6
X696    0.1342857143    35
X697    0.25    6
X698    0.2 8
X699    0.2 5
X700    0.5 1
X701    0.1333333333    6
X702    0.3 1
X703    0.15    2
X704    0.15    2
X705    0.1833333333    6
X706    0.15    6
X707    0.1493506494    77
X708    0.36    5
X709    0.3 2
X710    0.15    2
X711    0.38    5
X712    0.2666666667    3
X713    0.25    4
X714    0.225   4
X715    0.5 1
X716    0.1 2
X717    0.16    5
X718    0.3 2
X719    0.3538461538    13
X720    0.1 2
X721    0.175   4
X722    0.22    5
X723    0.175   4
X724    0.2333333333    6
X725    0.34    5
X726    0.2 7
X727    0.1 1
X728    0.3 3
X729    0.1 1
X730    0.1 3
X731    0.3 5
X732    0.35    6
X733    0.2875  8
X734    0.1 1
X735    0.1 2
X736    0.2 5
X737    0.1714285714    7
X738    0.375   4
X739    0.1 4
X740    0.3 1
X741    0.1 1
X742    0.1142857143    7
X743    0.1 1
X744    0.2285714286    7
X745    0.14    5
X746    0.15    6
X747    0.1 1
X748    0.125   4
X749    0.1666666667    6
X750    0.125   8
X751    0.1 1
X752    0.15    2
X753    0.2 1
X754    0.225   4
X755    0.3 1
X756    0.3 5
X757    0.175   4
X758    0.1 3
X759    0.1333333333    18
X760    0.1230769231    13
X761    0.2 1
X762    0.11    10
X763    0.1666666667    6
X764    0.1 1
X765    0.2090909091    11
X766    0.145   20
X767    0.14    5
X768    0.2375  8
X769    0.1571428571    7
X770    0.1 1
X771    0.1 2
X772    0.2 2
X773    0.16    5
X774    0.2 1
X775    0.1777777778    9
X776    0.1210526316    19
X777    0.2 1
X778    0.225   12
X779    0.1666666667    3
X780    0.1 6
X781    0.2333333333    6
X782    0.1692307692    13
X783    0.19    10
X784    0.2 3
X785    0.1489361702    47
X786    0.2 5
X787    0.45    2
X788    0.1666666667    6
X789    0.18    5
X790    0.3 1
X791    0.2 2
X792    0.11    10
X793    0.3333333333    3
X794    0.25    2
X795    0.2 1
X796    0.25    2
X797    0.2 2
X798    0.2 1
X799    0.1 3
X800    0.1333333333    18
X801    0.1473684211    19
X802    0.2 5
X803    0.14    5
X804    0.125   4
X805    0.1583333333    12
X806    0.1857142857    7
X807    0.1 1
X808    0.2 1
X809    0.1769230769    26
X810    0.1 1
X811    0.1 2
X812    0.1833333333    6
X813    0.1409090909    22
X814    0.1416666667    24
X815    0.1307692308    13
X816    0.1235294118    17
X817    0.1 1
X818    0.1 1
X819    0.18    30
X820    0.2514285714    35
X821    0.18    5
X822    0.2 4
X823    0.1 1
X824    0.2333333333    9
X825    0.1222222222    9
X826    0.15    2
X827    0.14    5
X828    0.1588235294    51
X829    0.15    2
X830    0.2 4
X831    0.1 2
X832    0.1391304348    23
X833    0.18    20
X834    0.15    2
X835    0.3 1
X836    0.1 8
X837    0.1666666667    9
X838    0.1954545455    22
X839    0.225   16
X840    0.1222222222    9
X841    0.1210526316    19
X842    0.1 2
X843    0.1 2
X844    0.125   4
X845    0.1 4
X846    0.1 1
X847    0.2 2
X848    0.275   4
X849    0.1 3
X850    0.2833333333    6
X851    0.175   4
X852    0.32    5
X853    0.1 1
X854    0.1428571429    7
X855    0.2277777778    18
X856    0.15    8
X857    0.12    5
X858    0.1 2
X859    0.175   4
X860    0.18    5
X861    0.16    5
X862    0.2333333333    6
X863    0.1 1
X864    0.3333333333    3
X865    0.1 2
X866    0.15    12
X867    0.1636363636    11
X868    0.4 1
X869    0.4 1
X870    0.1 3
X871    0.1555555556    9
X872    0.2 1
X873    0.3 1
X874    0.2 2
X875    0.15    12
X876    0.1 1
X877    0.1181818182    11
X878    0.1428571429    7
X879    0.1461538462    13
X880    0.3076923077    13
X881    0.2 2
X882    0.3 1
X883    0.205   20
X884    0.2 5
X885    0.1333333333    3
X886    0.15    2
X887    0.25    2
X888    0.15    4
X889    0.3 1
X890    0.125   4
X891    0.1875  8
X892    0.1428571429    7
X893    0.2333333333    3
X894    0.1 2
X895    0.1 1
X896    0.35    6
X897    0.1444444444    9
X898    0.2 2
X899    0.3 1
X900    0.1 2
X901    0.1 1
X902    0.25    2
X903    0.1 1
X904    0.1 1
X905    0.7 1
X906    0.2 1
X907    0.45    4
X908    0.25    2
X909    0.15    4
X910    0.1 2
X911    0.4 13
X912    0.1 2
X913    0.1842105263    19
X914    0.1 1
X915    0.1333333333    3
X916    0.2 2
X917    0.1 7
X918    0.1 1
X919    0.225   4
X920    0.2 1
X921    0.2 3
X922    0.18    5
X923    0.1 1
X924    0.1875  8
X925    0.2833333333    6
X926    0.5 3
X927    0.2 1
X928    0.1 1
X929    0.1 2
X930    0.2 3
X931    0.4 1
X932    0.2875  16
X933    0.1857142857    7
X934    0.1 1
X935    0.2 2
X936    0.1 1
X937    0.2 13
X938    0.2444444444    9
X939    0.1 1
X940    0.1714285714    7
X941    0.3 1
X942    0.1 1
X943    0.2857142857    7
X944    0.15    2
X945    0.1 1
X946    0.15625 16
X947    0.1666666667    3
X948    0.3 1
X949    0.2 2
X950    0.1 8
X951    0.1 1
X952    0.1 3
X953    0.3 1
X954    0.3 1
X955    0.1 3
X956    0.1125  8
X957    0.18    5
X958    0.2666666667    3
X959    0.2 1
X960    0.125   4
X961    0.1333333333    3
X962    0.2444444444    9
X963    0.25    10
X964    0.25    4
X965    0.2 1
X966    0.225   4
X967    0.1625  8
X968    0.1333333333    3
X969    0.1333333333    3
X970    0.1 1
X971    0.2 7
X972    0.3 10
X973    0.1 1
X974    0.3 2
X975    0.225   4
X976    0.1 1
X977    0.1 2
X978    0.4 1
X979    0.1333333333    3
X980    0.1333333333    9
X981    0.13125 16
X982    0.1 1
X983    0.2 1
X984    0.1782608696    23
X985    0.2225806452    31
X986    0.15    4
X987    0.1 3
X988    0.1 3
X989    0.15    4
X990    0.2285714286    14
X991    0.2384615385    26
X992    0.4 1
X993    0.4 2
X994    0.1 1
X995    0.1 1
X996    0.1666666667    3
X997    0.1 6
X998    0.13    20
X999    0.2666666667    3
</code></pre>

<p>Code I am using:</p>

<pre><code>Asianpig &lt;- NULL; Asianpig$x &lt;- (Asianpig_data$total)
Asianpig$y &lt;- (Asianpig_data$mean)
plot(Asianpig)

#increase maxiterations for nls
nlc &lt;- nls.control(maxiter = 21811)

# fit first a nonlinear least-square regression
Dat.nls &lt;- nls(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, control = nlc); Dat.nls
lines(1:8000, predict(Dat.nls, newdata=list(x=1:8000)), col=1)

# and finally ""external envelopes"" holding 95 percent of the data
Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, tau=0.025, trace=TRUE)
lines(1:8000, predict(Dat.nlrq, newdata=list(x=1:8000)), col=4)

Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, tau=0.975, trace=TRUE)
lines(1:8000, predict(Dat.nlrq, newdata=list(x=1:8000)), col=4)
</code></pre>

<p>How this looks: </p>

<p><a href=""http://i.stack.imgur.com/tF8Vu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tF8Vu.png"" alt=""enter image description here""></a></p>

<p>I was expecting the quantile regression line to more dynamically follow the slope of the datapoints. 
I adapted the code from an example that was using <code>SSlogis()</code> for the input data:</p>

<pre><code># build artificial data with multiplicative error
Dat &lt;- NULL; Dat$x &lt;- rep(1:25, 20)
    set.seed(1)
    Dat$y &lt;- SSlogis(Dat$x, 10, 12, 2)*rnorm(500, 1, 0.1)
plot(Dat)
</code></pre>

<p>I have a feeling I should not be using <code>SSlogis()</code> in my code, but instead should be modelling an exponential distribution. SSlogis is a selfStart model evaluates the logistic function and its gradient. It has an initial attribute that creates initial estimates of the parameters Asym, xmid, and scale.</p>

<p>But I am still trying to understand how to fit a quantile regression for this non-linear data.</p>

<p>Here is a hexbin plot that gives a feeling for how the data is clustered:<a href=""http://i.stack.imgur.com/NCrLX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NCrLX.png"" alt=""enter image description here""></a></p>
"
"0.128746027388597","0.136335470787303","177823","<p><strong>I need to perform manually two-stage Least Squares(to illustrate its advantages)</strong>, where the first stage is <em>repeated median estimate</em> and the second stage should be weighted least squares, where weights are obtained(as far, as I understand) from polynomial regression of first-stage residuals on regressors.</p>

<p>Suppose I have generated the following heteroscedastic model:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=Y_i%20%3D%20b_0%2Bb_1%20X_i%20%2B%20%5Cepsilon_i"" alt=""Y_i = b_0+b_1 X_i + \epsilon_i""></p>

<p>where error depends on regressor:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Cepsilon_i%20%5Csim%20N[0%2C[X_i-1]%5E2]"" alt=""\epsilon_i \sim N(0,(X_i-1)^2)""></p>

<pre><code>set.seed(100)
b&lt;-c(12,7.25) ## my coefficients
num&lt;-50 ## number of observations

raw_x&lt;-runif(num,min=0,max=2) ## regressors

my_y&lt;-as.vector(b%*%t(data.frame(rep(1,num),raw_x))+
     rnorm(num,mean=0,sd=(raw_x-1)^2)) ## observations

l&lt;-lm(my_y~raw_x) ## let's create linear model

plot(fitted(l),residuals(l)) ## we see heteroskedasticity
## we got to higher values, our residuals explode

abline(0,0)
title(""Residual vs Fit. value"");
</code></pre>

<p><a href=""http://i.stack.imgur.com/4S6pY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4S6pY.png"" alt=""Residual vs fit value""></a></p>

<p>So I perform repeated median regression (formula in the <a href=""http://www.cs.huji.ac.il/~werman/Papers/rep.pdf"" rel=""nofollow"">Introduction</a>):</p>

<pre><code>## Generating first model using repeated median

## slope 

fij = function(i,j)
{
  (my_y[i]-my_y[j])/(raw_x[i]-raw_x[j])
}

bij&lt;-outer(1:num,1:num,fij) ##NaN's were produced on the diagonal    

rowmeds &lt;- apply(bij, 1, median,na.rm=TRUE)
b_med&lt;-median(rowmeds)

# colmeds &lt;- apply(bij, 2, median,na.rm=TRUE) ## column medians are the same
# b_med3&lt;-median(colmeds)

## Intercept    

## med(y_i - b*x_i)

a_med&lt;-median(my_y-b_med*raw_x)
</code></pre>

<p><strong>The fit is extremely accurate!</strong> In this example <code>a_med</code> is 11.97634 and <code>b_med</code> equals 7.27022.</p>

<p>Now I perform 2nd order polynomial regression of residuals:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7B%5Cepsilon_i%7D%3Da_0%20%2B%20a_1X_i%2Ba_2X_i%5E2%2B%5Cdelta_i"" alt=""\hat{\epsilon_i}=a_0 + a_1X_i+a_2X_i^2+\delta_i""></p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7B%5Cepsilon%7D%20%3D%20%5Cbegin%7Bpmatrix%7D%201%20%26%20X_1%20%26%20X_1%5E2%20%5C%5C%20%5Cvdots%20%5C%5C%201%20%26%20X_m%20%26%20X_m%5E2%20%5Cend%7Bpmatrix%7D%5Cbegin%7Bpmatrix%7Da_0%20%5C%5C%20a_1%20%5C%5C%20a_2%20%5Cend%7Bpmatrix%7D%2B%5Cdelta"" alt=""\hat{\epsilon} = \begin{pmatrix} 1 &amp; X_1 &amp; X_1^2 \\ \vdots \\ 1 &amp; X_m &amp; X_m^2 \end{pmatrix}\begin{pmatrix}a_0 \\ a_1 \\ a_2 \end{pmatrix}+\delta""></p>

<p>so that (<strong>X</strong> here is m x 3 matrix):</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7Ba%7D%20%3D%20%5BX%5ETX%5D%5E%7B-1%7DX%5ET%5Chat%7B%5Cepsilon%7D"" alt=""\hat{a} = [X^TX]^{-1}X^T\hat{\epsilon}""></p>

<p>I was told that as long as residual variances can be roughly estimated from only one observation, actual fit from this model can be used; residual variances = coefficients for the weighted least squares:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7B%5Chat%7B%5Csigma%7D%7D%3DX%5Chat%7Ba%7D"" alt=""\hat{\hat{\sigma}}=X\hat{a}""></p>

<pre><code>## Obtaining 2nd order polynomial estimator for residuals

Xmatr = t(rbind(rep(1,num),raw_x,(raw_x)^2))

## coef_var = (X^T*X)^(-1)*X^T^e
coef_var&lt;-solve(t(Xmatr)%*%Xmatr)%*%t(Xmatr)%*%t(my_y-c(a_med,b_med)%*%rbind(rep(1,num),raw_x))

## Obtaining sigma (residual deviation) esitmate

my_sigma&lt;-t(Xmatr%*%coef_var)

## performing regression with sigma-weights

b_wls&lt;-lm(as.numeric(my_y)~raw_x,weights=as.numeric(my_sigma^2))$coef

## final plot

library(scales)
plot(raw_x,my_y, pch=20,col=alpha(""salmon"",0.6))

abline(b[1],b[2], col=""black"") ## real line
abline(a_med,b_med,col=""blue"") ## repeated median fit
abline(b_wls, col=""magenta"")
legend('bottomright', c(""Real"",""Repeat median"",""Two-Level LS"") , 
       lty=1, col=c('black', 'blue','magenta'), bty='n', cex=.75)
</code></pre>

<p>The resulting fit is always worse (and sometimes turns into complete garbage). <strong>Please, can you explain me what I'm doing wrong? I need to obtain the result where two-level LS is better than repeated median fit(provided the error depends on regressors as shown before)</strong>.</p>

<p><a href=""http://i.stack.imgur.com/Wo0ZM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Wo0ZM.png"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/huTRw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/huTRw.png"" alt=""enter image description here""></a></p>

<p>EDIT: Using R function lm seems to produce the same picture:</p>

<pre><code>Xmatr = t(rbind(rep(1,num),raw_x,(raw_x)^2))
residual&lt;-as.vector(my_y-c(a_med,b_med)%*%rbind(rep(1,num),raw_x))
polycoef&lt;-lm(residual ~ poly(raw_x, 2, raw=TRUE))$coefficients
my_sigma&lt;-t(Xmatr%*%polycoef)
</code></pre>

<p>EDIT2: Checking the quality of the residual fit (as far as I understand what's going on):</p>

<pre><code>plot(raw_x,abs(residual))
lines(sort(raw_x),(sort(raw_x)-1)^2) ## real residuals
lines(sort(raw_x), my_sigma[order(raw_x)],col = ""magenta"") ## fitted residuals
legend('topleft', c(""Real res"",""Fitted res"") , 
       lty=1, col=c('black','magenta'), bty='n', cex=.75)
</code></pre>

<p><a href=""http://i.stack.imgur.com/3J5D3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3J5D3.png"" alt=""enter image description here""></a></p>

<p>EDIT3: As long as my standard deviation is $(X_i-1)^2$, so the variance has power 4 and maybe I should do for the residual fit</p>

<pre><code>residual&lt;-abs(as.vector(my_y-c(a_med,b_med)%*%rbind(rep(1,num),raw_x)))
</code></pre>

<p>this makes residual fit by the quadratic function better, but the regression line moves even more far than before.
<a href=""http://i.stack.imgur.com/nt1Ij.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nt1Ij.png"" alt=""enter image description here""></a></p>
"
"0.0464572209811883","0.0472279924554862","177869","<p>I am running mixed effects regression in R, utilizing <code>glmer</code>, and am hoping someone can help clarify the difference between using <code>coef</code> and <code>ranef</code> on the results. Specifically, I have fixed effects $f_1,f_2,f_3$ and random effects $r_1,r_2,r_3.$ When I run <code>coef</code> I get certain coefficent values for each of the fixed and random effects. Additionally when I use the <code>ranef</code> function I get coefficients for my random effects. These two coefficients are not equal for each respective random effect $r_1,r_2,r_3.$ Why are these coefficients different and what information each coefficient tells us?</p>
"
"0.053644178078582","0.0409006412361909","178420","<p>I have data where number of observation <code>n</code> is smaller than number of variables <code>p</code>. The answer variable is binary. For example:</p>

<pre><code>n &lt;- 10
p &lt;- 100
x &lt;- matrix(rnorm(n*p), ncol = p)
y &lt;- rbinom(n, size = 1, prob = 0.5)
</code></pre>

<p>I would like to fit logistic model for this data. So </p>

<pre><code>model &lt;- glmnet(x, y, family = ""binomial"", intercept = FALSE)
</code></pre>

<p>The function returns 100 model for different lambda values (panalization parameter in lasso regression). I choose the biggest model which has <code>n - 1</code> parameters or less (so less than number of observations). Let's say chosen model is for <code>lambda_opt</code>. </p>

<pre><code>model_one &lt;- glmnet(x, y, family = ""binomial"", intercept = FALSE, lambda = lambda_opt)
</code></pre>

<p>Now I would like to do the second step - use <code>step</code> function to my model to choose the submodel which will be the best in term of BIC. Unfortunately <code>step</code> function doesn't work.</p>

<pre><code>step(model_one, direction = ""backward"", k = log(n))
</code></pre>

<p>How can I make it work? Is there some other function for this specific kind of model to do what I want?</p>
"
"0.0599760143904067","0.0487768608679754","178787","<p>Im really new in regression estimation but my problem here is about forecasting confrontation. </p>

<p>This is my model:</p>

<p>$Y_t = \beta_0 + \beta_1 X_t + \epsilon_t$ </p>

<p>My OLS estimation using r function ""lm"" was:</p>

<pre><code>set.seed(123)
data &lt;- matrix(rnorm(50*2),nrow=50)
m &lt;- data.frame(data )


Model1&lt;- lm(X1 ~ X2 -1 , data = m)
&gt; Modelo1$coef
        X2 
-0.0296194 
</code></pre>

<p>My Quantile Regression (Median, $\tau = 0.5$) was:</p>

<pre><code>&gt; ModeloRQ1&lt;-rq(X1 ~ X2 -1, tau = 0.5,method=""br"", data=m) 

&gt; ModeloRQ1$coef
        X2 
-0.1256418 
</code></pre>

<p>The estimation procedure i understand. </p>

<p>But the Forecasting Procedure i dont understand. 
I know that after making the forecast i should compare using RMSFE statistics, for example.</p>

<p>But when i use the ""forecast"" function gives me the same point forecast (same values) when i use ""predict"" function.</p>

<p>I have read some papers which do not detail this procedure. Only say that ""OLS and QR (0.5) forecasts are Confronted against each other"". </p>

<p>How should i do this procedure? Simply by using the function predict/forecasting? this would be a commonly used procedure?</p>
"
"0.116914775576919","0.118854507916379","179049","<p>I'm trying to learn some basic Machine Learning and some basic R. I have made a very naive implementation of $L_2$ regularization in R based on the formula:</p>

<p>$\hat w^{ridge} = (X^TX +\lambda I)^{-1} X^T y$ </p>

<p>My code looks like this:</p>

<pre><code>fitRidge &lt;- function(X, y, lambda) {
     # Add intercept column to X:
  X &lt;- cbind(1, X)
     # Calculate penalty matrix:
  lambda.diag &lt;- lambda * diag(dim(X)[2])
     # Apply formula for Ridge Regression:
  return(solve(t(X) %*% X + lambda.diag) %*% t(X) %*% y)
}
</code></pre>

<p>Note that I'm not yet trying to find an optimal $\lambda$, I'm simply estimating $\hat w^{ridge}$ for a given $\lambda$. However, something seems off. When I enter $\lambda = 0$ I get the expected OLS result. I checked this by applying lm.ridge(lambda = 0) on the same dataset and it gives me the same coefficients. However, when I input any other penalty, like $\lambda=2$ or $\lambda=5$ my coefficients and the coefficients given by lm.ridge disagree wildly. I tried looking at the implementation of lm.ridge but I couldn't work out what it does (and therefore what it does differently).</p>

<p>Could anyone explain why there is a difference between my results and the results from lm.ridge? Am I doing something wrong in my code? I've tried playing around with <code>scale()</code> but couldn't find an answer there.</p>

<p>EDIT:</p>

<p>To see what happens, run the following:</p>

<pre><code>library(car)
X.prestige &lt;- as.matrix.data.frame(Prestige[,c(1,2,3,5)])
y.prestige &lt;- Prestige[,4]

fitRidge(X.prestige, y.prestige, 0)
coef(lm.ridge(formula = prestige~education+income+women+census, data = Prestige, lambda = 0))
fitRidge(X.prestige, y.prestige, 2)
coef(lm.ridge(formula = prestige~education+income+women+census, data = Prestige, lambda = 2))
</code></pre>

<p>EDIT2:</p>

<p>Okay, so based on responses below, I've gotten a somewhat clearer understanding of the problem. I've also closely re-read the section about RR in TESL by Hastie, Tibshirani and Friedman, where I discovered that the intercept is often estimated simply as the mean of the response. It seems that many sources on RR online are overly vague. I actually suspect many writers have never implemented RR themselves and might not have realized some important things as many of them leave out 3 important facts:</p>

<ol>
<li>Intercept is not penalized in the normal case, the formula above only applies to the other coefficients.</li>
<li>RR is not equivariant under scaling, i.e. different scales gives different results even for the same data.</li>
<li>Following from 1, how one actually estimates intercept.</li>
</ol>

<p>I tried altering my function accordingly:</p>

<pre><code>fitRidge &lt;- function(X, Y, lambda) {
  # Standardize X and Y
  X &lt;- scale(X)
  Y &lt;- scale(Y)
  # Generate penalty matrix
  penalties &lt;- lambda * diag(ncol(X))
  # Estimate intercept
  inter &lt;- mean(Y)
  # Solve ridge system
  coeff &lt;- solve(t(X) %*% X + penalties, t(X) %*% Y)
  # Create standardized weight vector
  wz &lt;- c(inter, coeff )
  return(wz)
}
</code></pre>

<p>I still don't get results equivalent to lm.ridge though, but it might just be a question of translating the formula back into the original scales. However, I can't seem to work out how to do this. I thought it would just entail multiplying by the standard deviation of the response and adding the mean, as usual for standard scores, but either my function is still wrong or rescaling is more complex than I realize.</p>

<p>Any advice?</p>
"
"0.053644178078582","0.0545341883149212","179748","<p>I have a dataset, say $A$:</p>

<pre><code>x    y
20   3.4
30   3.3
35   4.5 
</code></pre>

<p>I am fitting a regression model, a mixed model of R's <code>lme4</code> family to be exact, to predict $y$ given $x$. 
I have a set of <code>newdata</code> which don't have an observed $y$. 
Let $\hat{y}$ be the predicted value for this set $B$:</p>

<pre><code>x   yhat
20   3.3
100  6.6 
</code></pre>

<p>I need to report whether the predicted value is expected to be reasonably accurate. 
As you can see in dataset $B$, <code>x[2]</code> is somewhat of an outlier, and therefore $\hat{y}$ is also a value which is rare.
It happens to fall outside the 95% confidence interval of predicted values. $\hat{y}$ is very close to a real observation , in real life. </p>

<p>What kind of metric is used to report that $\hat{y}$ is actually quite a good prediction? 
I hope my question is clear even though I've struggled to explain it...</p>

<h3>Edited to add in response to comment below:</h3>

<p>The model has only one fixed parameter ($x$, continuous) while also having a random effects group parameter . 
Model looks like this </p>

<pre><code>LMER2&lt;-lmer(y~x + (1 |group), training_data)
</code></pre>

<p><code>lme4</code> does give me the coefficients and standard errors , and I have used the <code>bootMer</code> function to calculate $\hat{y}$ confidence interval following <a href=""http://www.r-bloggers.com/confidence-intervals-for-prediction-in-glmms/"" rel=""nofollow"">this article</a>.</p>
"
"0.0758643241810882","0.0771229887279699","180178","<p>Using the package <code>mfp</code> in R and would like to plot the fit with 95% confidence intervals.</p>

<p>This seems straightforward with an <code>lm</code> object (<a href=""http://stackoverflow.com/questions/15180008/how-to-calculate-the-95-confidence-interval-for-the-slope-in-a-linear-regressio"">as in this question</a>, <a href=""http://stats.stackexchange.com/questions/135707/plotting-a-polynomial-regression-with-its-confidence-interval-of-95-in-r"">and this question</a>) and a <code>glm</code> object as shown in  <a href=""http://stackoverflow.com/questions/20620277/get-95-confidence-interval-with-glm-in-r"">this question</a>.  </p>

<p>However, the <code>mfp</code> is not recognized/structured as either <code>lm</code> or <code>glm</code>as the function <code>mfp</code> generates an <code>mfp</code> object.  <code>mfp</code> objects do use <code>predict.glm</code>, from which I can get the <code>se.fit</code>. </p>

<p><em>All that to say, that I want to confirm I'm using the appropriate formula to calculate CI around the fit as shown below:</em></p>

<p><a href=""http://i.stack.imgur.com/rQ6I3.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rQ6I3.gif"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/RYzou.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RYzou.gif"" alt=""enter image description here""></a></p>

<pre><code>somedat &lt;- data.frame(x=1:20, y=pexp(1:20, rate=1/3))
library(mfp)
mymfp &lt;- mfp(formula = y~fp(x), data = somedat)
mypred &lt;- predict(object = mymfp, type = 'response', se.fit = T)

plot(somedat$x, somedat$y, ylim=c(0,1.2))
lines(predict(mymfp,  type = 'response')) #fit
lines(mypred$fit-mypred$se.fit*1.96, col='orange') #lower
lines(mypred$fit+mypred$se.fit*1.96, col='orange') #upper
</code></pre>

<p><a href=""http://i.stack.imgur.com/xhpHG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xhpHG.png"" alt=""enter image description here""></a></p>

<p>Is the above an appropriate equation/solution for a 95% confidence interval around the fractional polynomial fit?</p>
"
"0.0657004319817604","0.0667904674542028","180191","<p>We can apply the Hosmer-Lemeshow goodness of fit to logistic regression modelling and to test if an underlying assumption is not applicable.</p>

<p>This <a href=""https://www.youtube.com/watch?v=MYW8gA1EQCQ"" rel=""nofollow"">link</a> shows a video of the application to a standard <code>glm()</code> model</p>

<p>This <a href=""http://stats.stackexchange.com/questions/132652/how-to-determine-which-distribution-fits-my-data-best-r"">detailed question</a>, outlines various simulation-based tests one can run to assess underlying distributions.</p>

<p><strong>But I want to apply the Hosmer-Lemeshow goodness of fit to survival analysis with assumed underlying data distributions</strong>.</p>

<p>Much literature points one towards a cox proportional hazards model, but from what I understand, a cox ph model does not assume an underlying distribution of data.
Therefore lets take some random data from the <code>survreg()</code> function of the <code>survival</code> package</p>

<pre><code>library(survival)

data(ovarian)

head(ovarian)

s &lt;- Surv(ovarian$futime, ovarian$fustat)
sWei &lt;- survreg(s ~ age,dist='weibull',data=ovarian)
</code></pre>

<p>How can we applying a H+L G.O.F statistic test?
I had hoped to follow this <a href=""http://thestatsgeek.com/2014/02/16/the-hosmer-lemeshow-goodness-of-fit-test-for-logistic-regression/"" rel=""nofollow"">link</a>, however the <code>survreg()</code> does not allow a <code>fitted()</code> function. Thus this does not work</p>

<pre><code>library(ResourceSelection)
hl &lt;- hoslem.test(sWei$y, fitted(sWei), g=10))
</code></pre>
"
"0.0379321620905441","0.0385614943639849","180382","<p>I have binary count data as a response variable in my logistic regression. The independent variables include, among others, two variables of inclination and orientation measurements, annotated in degrees of arc. For 'orientation' (or aspect), it ranges from 0Â° to 360Â°, and for 'inclination' from 0Â° to 90Â°. In cases where 'inclination' is 0, the orientation is annotated as '-1', because horizontal surfaces do not face any direction.</p>

<p>For a logistic regression, my workflow would include to use R's scale-function to standardize all continuous variables, among them 'inclination' and 'orientation'. And that is what I did. But does that make sense here? Keep in mind, that an orientation of 0 (north) is the same as 360 (also north), and that 1Â° and 359Â° are only two degrees apart. </p>

<p>How can I standardize those measurements? How would you recode an orientation of '-1', which isn't either north nor east, south or west? At this point, both variables appear to be highly influential on my model fit, but can i trust that conclusion?</p>
"
"0.0479808115123254","0.0609710760849692","180483","<p>I am considering a Bayesian Gaussian spatial regression model</p>

<pre><code>y(s) = x(s) b + w(s) + e 
</code></pre>

<p>where </p>

<pre><code>w|theta ~ N(0,k2 H(phi))
e ~ N(0,tau2)
</code></pre>

<p>Assuming the range <code>phi</code> and the <code>tau2/sigma2</code> ratio fixed, a normal prior on the regression coefficients <code>b</code> and an inverse gamma on <code>sigma2</code> reduces the problem to a conjugate Bayesian linear regression problem. Hence no need for MCMC and I can use the <code>bayesGeostatExact</code> function.</p>

<p>My question is: if we assume instead that <code>phi</code> and the <code>tau2/sigma2</code> ratio are NOT fixed we need to specify priors for the spatial parameters as well. 
Assuming a uniform prior for <code>phi</code> and inverse gamma priors for <code>tau2</code> and <code>sigma2</code> ratio makes the conjugacy still hold?</p>

<p>I don't understand why in this case we need to use a MCMC simulation (through the <code>spLM</code> function) and there is no equivalent <code>bayesGeostatExact</code> function if the spatial parameters are not fixed.</p>

<p>Thanks</p>
"
"0.0599760143904067","0.0609710760849692","180488","<p>I am trying to compare the effect of breastfeeding on child outcomes using PSM in R. Breastfeeding is my treatment whereby mothers who breastfed are coded as 1(treatment) and those who don't as 0(control). My initial sample is comprised of 10537 obs. and I have 15 vars I want to match on. I have played around with different matching and what seems to work best for this cohort is nearest neighbour with caliper .25. </p>

<p>My syntax for calculating the propensity score was: </p>

<pre><code>m.out=matchit(treated~partner+matage+matedu+class3+mcard+matwork+ethnic+matdep+smoke+sib+prem+nicu+bweight+bsex+delivery,data=psa3.covars,method=""nearest"",caliper=0.25*ps.sd)
</code></pre>

<p>This resulted in a subset of data with only the matched participants (i.e., 3010 Treated and 3010 Control). I checked the results of the balance with:</p>

<pre><code>#post-matching balance
match.data&lt;-match.data(m.out)
treated1&lt;-match.data$treat==1
str(treated1)
treated1
summary(treated1)
cov1&lt;-match.data[ ,1:16]
std.diff1&lt;-apply(cov1,2,function(x)100*(mean(x[treated1])-mean(x[!treated1]))/(sqrt(.5*(var(x[treated1])+var(x[!treated1])))))
std.diff1
</code></pre>

<p>which revealed a good balance. My question is this: I now want to run my post regression analyses with the matched sampled only both pre and post matching.</p>

<p>I was given the following syntax for post matching:</p>

<pre><code>mod&lt;-glm(vocab3~treated+partner+matage+matedu+class3+mcard+matwork+ethnic+matdep+smoke+sib+prem+nicu+bweight+bsex+delivery,data=psa3.covars)
</code></pre>

<p>but it doesn't seem logical as the distance score created from the matching is not included yet all the matching variables are. Would this syntax be my pre-matched regression and then I would just add ""+distance"" to the same syntax at the end of my covariates list?</p>
"
"0.0967084173462244","0.0983129061176287","180521","<p>I have a time series that includes some rare extreme values. We are talking about daily data, in total 1461 observations and 11 extreme values. I adjusted those 11 values with a multiple regression. Now I am using the <code>tbats()</code> on the original time series and the adjusted one. </p>

<pre><code>accuracy(original)
&gt;                   ME    RMSE      MAE MPE MAPE      MASE          ACF1
&gt;Training set 10.23539 4202.19 2921.593 NaN  Inf 0.6777689 -0.0003493096
accuracy(adjusted)
&gt;                   ME    RMSE      MAE MPE MAPE      MASE          ACF1
&gt;Training set 43.35625 3803.618 2787.39 NaN  Inf 0.6827622 -0.004749092

#original AIC
&gt;35101.43
#adjusted AIC
&gt;34798.24
</code></pre>

<p>How can I see if the model improves due to the adjustment or not? Since I reduced those 11 extreme values, I can't just compare MAE, RMSE or AIC. MASE is the only measure that should work?</p>

<p>I could divide MAE, RMSE and AIC by the mean of the respective time series.</p>

<pre><code># original
0.4962245 # MAE/mean(original)
0.7137304 # RMSE/mean(original)
5.96188 # AIC/mean(original)

# adjusted
0.4862567 # MAE/mean(adjusted)
0.6635364 # RMSE/mean(adjusted)
6.07051 # AIC/mean(adjusted)
</code></pre>

<p>Is that a legitimate way to compare the results?</p>

<p>Here are the <code>pacf</code>-diagrams of both models:</p>

<p><strong>original</strong>:</p>

<p><a href=""http://i.stack.imgur.com/nFARp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nFARp.png"" alt=""original""></a></p>

<p><strong>adjusted</strong>:</p>

<p><a href=""http://i.stack.imgur.com/YXIGF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YXIGF.png"" alt=""adjusted""></a></p>

<p><strong>Update:</strong></p>

<p>I just realized that when i use the <code>accuracy()</code> function of the <code>forecast</code> package with a <code>tbats()</code> based on a <code>msts()</code> object the resulting MASE is using an in-sample naive forecast for scaling. I guess that is not optimal? It should be better to use an in-sample naive seasonal forecast with the longest season of the <code>msts()</code> object.</p>

<pre><code>MASE(original) # scaled with a in-sample naive seasonal forecast (365)
&gt; 0.6339

MASE(adjusted) # scaled with a in-sample naive seasonal forecast (365)
&gt; 0.6287
</code></pre>
"
"0.0464572209811883","0.0472279924554862","180820","<p>I am using the auto.arima function in R.  I'm using this to forecast daily sales and am loading a number of covariates (mostly holiday/seasonal dummy variables) with Xreg.  </p>

<p>Question (I apologize if this question is too fundamental, however, my experience is with regressions on cross sectional data in fixed time periods and I have little experience with dynamic regressions): Using auto.arima, do I still need to exclude a dummy variable from my set of covariates (ie, 6 dummy variables for day of the week instead of 7)?  Is the excluded dummy still caught in the output for the intercept variable?</p>

<p>Thanks.</p>
"
"0.0808716413062113","0.0904347204435887","181065","<p>In some previous asked questions, I was told to not delete the outliers, because they contain valuable information.</p>

<p>After testing different regression, I came to the conclusion that until now, the <code>MARS</code> regression delivers the ""best responses"".</p>

<p>I know that <code>MARS</code> is very <em>robust</em> and there is no <em>a priori</em> knowledge about the data distribution needed.</p>

<p>But there are some question which I have about the parameters.</p>

<p>I'm using the <code>earth</code> function implemented in <code>R</code></p>

<p><strong>data set:</strong>
   <a href=""http://www.filedropper.com/data_8"" rel=""nofollow"">file</a></p>

<p>So I've got 5 variables, <em>price, livingArea, area, discrete, dummy</em> and I'm trying to explain <code>price</code> using the other ones.</p>

<p><a href=""http://i.stack.imgur.com/gGf4G.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gGf4G.png"" alt=""enter image description here""></a></p>

<p>as you can see, there are some outliers and a <code>log</code> doesn't really solve the problem.
Due to the fact that <code>area</code> can be <code>null</code>, a <code>log</code> won't be a good transformation idea. </p>

<p><strong>what I do:</strong></p>

<p>Because the answers from other questions suggested to use the raw data, 
I'm running now the regression through my data without doing any changes to it.</p>

<p>so my regression formula looks like this:</p>

<pre><code>earth(price ~ ., data = data[,-1], weights = weights, penalty = -1)
</code></pre>

<p>I'm setting <code>penalty = -1</code> because I saw that doing this, the method defines more knots and also the results look better.</p>

<p>Also I tried to define the variables <code>discrete</code> and <code>dummy</code> as <code>factors</code> and use them as follows in the regression:</p>

<ol>
<li>independent</li>
<li>livingArea * discrete or livingArea : discrete and <code>dummy</code> as independent</li>
<li>the same as at <code>3.</code> but changing <code>discrete</code> with <code>dummy</code></li>
<li>livingArea * discrete * dummy </li>
</ol>

<p>I must say that I didn't expect, that a regression with this variables as factors, will return such ""bad"" results.</p>

<p><strong>what I want:</strong></p>

<p>I want to use the model in order to predict the value of new data.</p>

<pre><code>    livingArea area discrete dummy
1         87    0        7    0.5
</code></pre>

<p>The prediction of this observation should be <code>~ 330000</code>, but with what I'm doing now, I ain't coming not even close to this value.</p>

<p>I think that, having more knots increases the precision of the result.</p>

<p><strong>questions:</strong></p>

<ul>
<li>I don't really understand the parameters</li>
<li>I created my model with different values for the <code>pmethod</code>, but the result was always the same. what's the point in choosing a method when the result will be the same?</li>
<li>how could I determine if I have to set/change the values of different parameters like <code>thresh</code>, <code>minspan</code>, <code>nk</code>, etc.</li>
</ul>
"
"0.0657004319817604","0.0667904674542028","181237","<p>I am using the coxph function to model a Cox regression.
By using stepwise BIC selection I obtained an model with 6 variables.
One of the variables I had to transform using the logarithm to make it fulfill the proportional hazard criterion. All Variables are marked as high significant with very low p-values- What I am now confused about is the fact, that the confidence intervals of two variables are very high. Therefore I wonder how ""useful"" those variables are! Since they are selected by the algorithm I have no doubt that they are useful in a mathematical way, but what is the meaning of a variable whose 95% confidence interval is spanning over a wide range.</p>

<p><a href=""http://i.stack.imgur.com/y7jVq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y7jVq.jpg"" alt=""Cox Regression Result""></a></p>

<p>As you can see the variables log(F) and especially E have a very high confidence interval. How can E be so important for the model (there have been more than 70 variables to choose from) and still be so ""uncertainly"" determined.
I hope I was able to formulate my problem in an understandable way.</p>

<p>Thanks in advance!
Mark</p>
"
"0.0599760143904067","0.0609710760849692","181489","<p>I have to estimate a number of regressions where a lot of autcorrelation is present. Now, for historical reasons, this autocorrelation is resolved using an iterative Prais-Winsten estimation (a modification of the Cochraneâ€“Orcutt estimation). I have found some R-code which performs this procedure:</p>

<pre><code>    prais.winsten.lm &lt;- function(mod){
    X &lt;- model.matrix(mod)
    y &lt;- model.response(model.frame(mod))
    e &lt;- residuals(mod)
 n &lt;- length(e)
 names &lt;- colnames(X)
 rho &lt;- sum(e[1:(n-1)]*e[2:n])/sum(e^2)
 y &lt;- c(y[1] * (1 - rho^2)^0.5, y[2:n] - rho * y[1:(n-1)])
 X &lt;- rbind(X[1,] * (1 - rho^2)^0.5, X[2:n,] - rho * X[1:(n-1),])
 mod &lt;- lm(y ~ X - 1)
 result &lt;- list()
 result$coefficients &lt;- coef(mod)
     names(result$coefficients) &lt;- names
 summary &lt;- summary(mod, corr = F)
 result$cov &lt;- (summary$sigma^2) * summary$cov.unscaled
     dimnames(result$cov) &lt;- list(names, names)
 result$sigma &lt;- summary$sigma
 result$rho &lt;- rho
 class(result) &lt;- 'prais.winsten'
 result
 }
</code></pre>

<p>Now, this code works fine when all the other regressors are exogeneous. But, in my case, a part of X is endogeneous turning the standard ols regression performed in the above code not usable.</p>

<p>I was thinking about modifying the above code into the following:</p>

<pre><code>  prais.winsten.plm &lt;- function(mod){
  X &lt;- model.matrix(mod,component=""projected"")
  Z &lt;- model.matrix(mod,component=""instruments"")
  y &lt;- model.response(model.frame(mod))
  e &lt;- residuals(mod)
  n &lt;- length(e)
  names &lt;- colnames(X)
  rho &lt;- sum(e[1:(n-1)]*e[2:n])/sum(e^2)
  y &lt;- c(y[1] * (1 - rho^2)^0.5, y[2:n] - rho * y[1:(n-1)])
  X &lt;- rbind(X[1,] * (1 - rho^2)^0.5, X[2:n,] - rho * X[1:(n-1),])
  Z &lt;- rbind(Z[1,] * (1 - rho^2)^0.5, Z[2:n,] - rho * Z[1:(n-1),])
  mod &lt;- ivreg(y ~ X -1|Z)
  result &lt;- list()
  result$coefficients &lt;- coef(mod)
      names(result$coefficients) &lt;- names
  summary &lt;- summary(mod, corr = F)
  cov &lt;- (summary$sigma^2) * summary$cov.unscaled
  result$se&lt;- sqrt(diag(cov))
      dimnames(cov) &lt;- list(names, names)
      result$sigma &lt;- summary$sigma
      result$rho &lt;- rho
  class(result) &lt;- 'prais.winsten'
  result
  }
</code></pre>

<p>I have however no idea if such an approach is correct, especially since I found no similar ways in dealing with this problem.</p>
"
"0.0709645772411954","0.072141950116023","181603","<p>I am enrolled in a <a href=""http://www.youtube.com/playlist?list=PLA89DCFA6ADACE599"" rel=""nofollow"">machine learning course</a> for machine learning where we have a lab to implement linear regression
I am attempting to do it in R to get a better understanding of the material and of R for myself (i don't intend to submit this as a lab as the course doesn't use R) but am coming up against a wall</p>

<p>My understanding of the process is as follows</p>

<ul>
<li><p>User Generates a model based on the hypothesis
$h_\theta(x) = \theta^TX= \theta_0x_0 +\theta_1x_1+\dots$</p></li>
<li><p>Take error rate of your model by using squared error cost function, then iterate, create a new hypothesis and get the error rate of this. Continue through $n$ iterations based on the formula
$J(\theta_0,\theta_1)=\frac{1}{2m}\displaystyle\sum_1^m(h_\theta(x^{(i)})âˆ’y^{(i)})^2$. </p></li>
<li><p>Take all the error rates you have recorded based on the cost history and use <code>gradient descent</code> to find automatically the optimal values of your hypothesis.</p></li>
</ul>

<p>Using the code on <a href=""http://www.r-bloggers.com/linear-regression-by-gradient-descent/"" rel=""nofollow"">R-Bloggers</a> where the gradient descent is implement below based on vectors <code>x</code> and <code>y</code></p>

<pre><code># add a column of 1's for the intercept coefficient
X &lt;- cbind(1, matrix(x))

# gradient descent
for (i in 1:num_iters) {
 error &lt;- (X %*% theta - y)
 delta &lt;- (t(X) %*% error) / length(y)
 theta &lt;- theta - alpha * delta
 cost_history[i] &lt;- cost(X, y, theta)
 theta_history[[i]] &lt;- theta
}
</code></pre>

<p>I was wondering if people could help me tease out the logic</p>

<ol>
<li><p>Why is the number 1 applied to the matrix <code>X</code>. Is this so that X has 2 columns so that it can be multiplied by theta - y?</p></li>
<li><p>What is the formula delta actually calculating and why is the Transpose of X being used</p></li>
</ol>

<p>Conceptually I think i understand the overall process but i just need to relate this back to the R code as i want to grasp the concept before proceeding to Multiple linear regression</p>
"
"0","0.0272670941574606","181695","<p>In linear regression, if I have a model,</p>

<pre><code>b0 + b1x1 + b2x2 + b3x3 + b4x4 = y
</code></pre>

<p>and I want to fix some of the coefficients ,say b1 = 1 and b3 = 2, I could just do the following</p>

<pre><code>b0 + b2x2 + b4x4 = y - x1 - 2x3
</code></pre>

<p>and just fit a linear regression on the other three parameters on the new y. Is there a way to do this for logistic regression? The sigmoid function seems to complicate things. Im looking to do this in r, so if theres an easy way to do it in r, that would be very appreciated.</p>
"
"NaN","NaN","181843","<p>I'm a beginner in econometrics and have to interpret the graph I get after running an nonparametric regression using the package ""mgcv"" on RStudio (R). We have to use the function ""gam()"" for it. To begin with and for simplicity, we should regress on only one variable (price of cigarettes).</p>

<p>The formula was: </p>

<pre><code>model &lt;- gam(packs~ s(price)).
</code></pre>

<p>After plotting it I get this graph:<a href=""http://i.stack.imgur.com/Hcey2.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Hcey2.png"" alt=""enter image description here""></a></p>

<p>How should I interpret this graph?</p>
"
"0.0848188929679971","0.0776035104406608","181980","<p>I would like to use a kernel matrix generated with a custom kernel function to fit a PLS-DA model (I am thinking of caret's PLS-DA at the moment), with only one binary response variable in the Y block. Before beginning, I am centering the kernel matrix on feature space with </p>

<p><a href=""http://i.stack.imgur.com/E7Hhd.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/E7Hhd.gif"" alt=""enter image description here""></a></p>

<p>A few remarks:</p>

<ol>
<li>I see that caret's <code>plsda</code> function relies on the <code>pls</code> package functions <code>mvr</code> and <code>plsr</code>. When fitting a PLS-DA model, the method used to fit the model defaults to <code>kernelpls</code>, which is the version described on algorithm 1 on <em>Dayal, B. S. and MacGregor, J. F. (1997) Improved PLS algorithms. Journal of Chemometrics, 11, 73-85.</em> In this paper, they propose to compute a kernel matrix directly as <a href=""http://i.stack.imgur.com/Ajfv3.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ajfv3.gif"" alt=""enter image description here""></a> as part of the algorithm, and they rely directly on X as well during other steps. Therefore, it seems to me that using this method would mean to calculate a kernel matrix again over my kernel matrix.</li>
<li>I've seen three different methods in the literature that involve kernels and PLS. The first one is Dayal and MacGregor's kernel algorithm, the second one is <a href=""http://www.jmlr.org/papers/volume2/rosipal01a/rosipal01a.pdf"" rel=""nofollow"">K-PLS</a> (<em>Rosipal, Roman, and Leonard J. Trejo.""Kernel partial least squares regression in reproducing kernel hilbert space."" The Journal of Machine Learning Research 2 (2002): 97-123.</em>) and the third one is <a href=""http://homepages.rpi.edu/~bennek/papers/KB-ME-PLS.pdf"" rel=""nofollow"">DK-PLS</a> (direct kernel PLS). My understanding is that K-PLS is just a modification of the NIPALS algorithm (oscorespls fitting method in the <code>pls</code> package) to use a kernel matrix, and therefore I suspect that this might be the one I should be using. DK-PLS seems to use a kernel matrix as input as well.</li>
</ol>

<p>In short, I guess my question can be summarized as: Which method should I use to fit a PLS-DA model for a binary response, with a custom kernel matrix as input data? Any insights would be appreciated!</p>
"
"0.0625848744250123","0.0636232197007414","182286","<p>I am doing a regression analysis for an ordinal response variable with 5 explanatory variables. I will be using the <code>polr()</code> or <code>lrm()</code> functions to do the ordinal logistic regression. For my non-ordinal response variables (e.g., count and binary data), I have been using glmulti for model selection, but this doesn't seem to be compatible with the <code>polr()</code> and <code>lrm()</code> R functions. I've also tried <code>stepAIC()</code>, <code>step()</code> and <code>leap()</code> functions without any luck. The summary of the <code>polr()</code> regression shows an AIC score.</p>

<pre><code>&gt; model1 &lt;- polr(x ~ Age + Gender + StudentType + StudentYear + RacialGroup,
+ data = question8a, Hess =TRUE)
&gt; summary(model1)
Call:
polr(formula = x ~ Age + Gender + StudentType + 
    StudentYear + RacialGroup, data = question8a, Hess = TRUE)

Coefficients:
                                   Value Std. Error  t value
Age                             -0.16691    0.04925 -3.38872
GenderWoman                      0.05514    0.24655  0.22366
StudentTypeUndergraduatestudent -1.36414    0.50748 -2.68807
StudentYear2ndyear              -0.02042    0.29600 -0.06899
StudentYear3rdyear              -0.05997    0.38253 -0.15676
StudentYear4+years               0.89921    0.66430  1.35363
StudentYear4thyear               0.25324    0.42433  0.59680
RacialGroupNon-Indigenous       -2.13460    0.42163 -5.06268

Intercepts:
    Value   Std. Error t value
1|2 -9.9335  1.5283    -6.4999
2|3 -8.3051  1.4752    -5.6298
3|4 -7.2498  1.4567    -4.9770
4|5 -4.8720  1.4240    -3.4214

Residual Deviance: 657.086 
AIC: 681.086 
</code></pre>

<p>I tried to follow this suggestion: <a href=""http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R"" rel=""nofollow"">http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R</a>, but wasn't able to get it to work. </p>

<p>Has anyone been able to get this to work? Or do I need to compare the 2^5 = 32 model AIC scores by hand? </p>
"
"0.0379321620905441","0.0385614943639849","182509","<p>I am using logistic regression (with R) for detecting fraudulent transactions. So far I am achieving a relatively good ratio of success (f-score).</p>

<p>However I have noticed something, once I have my model built, the threshold that gives me the best f-score is to consider something as fraudulent if the logit function is bigger than 0.059 (I started with 0.5). </p>

<p>For the record, I am using 7099 observations as training examples and 3042 as testing data, in total I am using 6 features/columns for prediction (planning to add a couple more)</p>

<p>Now my questions are the following</p>

<ol>
<li>Am I doing something terribly wrong so that I have to use such a low limit to start labeling transactions as fraudulent?</li>
<li>That said, the vast majority of transaction are legitimate, does it justify the low threshold for the labeling (again, 0.059) ?</li>
<li>Would it be worth to explore other algs such random forest or neuronal networks?</li>
</ol>
"
"0.0625848744250123","0.0818012824723818","183074","<p>I want to implement kernel ridge regression in R. My problem is that I can't figure out how to generate the kernel values and I do not know how to use them for the ridge regression. </p>

<p>Before going to the code can anybody help me with explaining how kernel ridge regression works conceptually?  </p>

<ul>
<li>What do I have to do step by step to be able to implement ridge regression? </li>
<li>Do I have to obtain kernel values for every independent variable in my training set, or for every data point? </li>
<li>And how can I use these values for ridge regression?</li>
</ul>

<p>I want to use the following kernel function:</p>

<pre><code>kernel.eval &lt;- function(x1, x2, ker) {
  k=0
  if (ker$type=='RBF') {
        # RBF kernel
        k = exp(-sum((x1-x2)*(x1-x2)/(2*ker$param^2)))
      }
      else {
        # polynomial kernel
        k = (1+sum(x1*x2))^ker$param
  }
  return(k)
}
</code></pre>

<p>Furthermore, I know that the formula for ridge regression is:</p>

<pre><code>myridge.fit &lt;- function(X, y, lambda) {
    w = solve((t(X)%*%X) + (lambda*diag(dim(X)[2])), (t(X)%*%y))
    return(w)
  }
</code></pre>

<p>Example training data:  </p>

<pre><code>           [,1]       [,2]
[1,] -1.3981847 -1.3358413
[2,]  0.2698321  1.0661275
[3,]  0.3429286  0.8805642
[4,]  0.5210577  1.1228635
[5,]  1.5755659  0.2230754
[6,] -1.2167197 -0.6700215
</code></pre>

<p>Example testing data (I do not know if I need these at this moment):  </p>

<pre><code>      [,1]   [,2]
[1,] -2.05 -2.050
[2,] -2.05 -2.009
[3,] -2.05 -1.968
[4,] -2.05 -1.927
[5,] -2.05 -1.886
[6,] -2.05 -1.845
</code></pre>

<p>Is anyone able to help me with the first step(s). I have to do ridge regression for a RBF kernel as well as a polynomial kernel.</p>
"
"NaN","NaN","183150","<p>Can I use binary variables in R's glm function with a binomial outcome (logistic regression)?</p>
"
"0.0758643241810882","0.0771229887279699","183337","<p>I am trying to test the predictive accuracy of regression using training sets of varying sizes.</p>

<pre><code>Y &lt;- rnorm(100)
X &lt;- replicate(5, Y+rnorm(100) )   
data &lt;- as.data.frame(cbind(Y,X))
</code></pre>

<p>Let's say the training set is 2% of the data:</p>

<pre><code>train &lt;- nrow(data) * 0.02
test &lt;- nrow(data) - train 
</code></pre>

<p>I repeat the process for 1000 times:</p>

<pre><code>MSE &lt;- vector()
for( i in 1:1000){

train.elements &lt;- sample(1:nrow(data),train)
train.set &lt;- data[train.elements,]
test.set &lt;- data[setdiff(1:nrow(data), train.elements),]

# then I fit a regression model:

    model &lt;- lm(train.set[,1]~ train.set[,2]+train.set[,3]+train.set[,4]+train.set[,5])

#I now use this model to predict the values in the test set:
predictions &lt;- predict.lm(model,data=test.set)

MSE[i] &lt;- mean((test.set[,1] - predictions)^2)

}
</code></pre>

<p>My problem is that due to the small sample size the MSE sometimes is extremely huge.</p>

<p>Is this normal? I am unable to plot a curve of the MSE as a function of training set size because the MSE for small sample sizes are so large.</p>
"
"0.0758643241810882","0.0771229887279699","183528","<p>I have three questions about the assumptions of the logistic regression:</p>

<ol>
<li><p>I read that the percentages of zeros and ones should be equal. If there's a data set where one of them is abundand, i.e. there are 80% zeros and 20% ones can I somehow put different weights in my glm? There's also the weight-function, but I don't understand what it's exactly for... </p></li>
<li><p>I didn't really get what the pseudo-coefficients of determination tell me - Do this, i.e. the Nagelkerke's index tell me something about the assumptions of my model, how much they are fullfilled or just how much my predicted model differ from the data points I've observed. </p></li>
<li><p>I also red for the assumptions that there should be at least 25 data points / group, what exactly is my group? When I i.e. have the mtcars-dataset in R </p>

<p>data(""mtcars"") </p></li>
</ol>

<p>and I want to look</p>

<pre><code>glm(vs ~ carp + disp, family = binomial) 
</code></pre>

<p>what are my groups? (maybe this is also a false point of view, but I'm really irritated...)</p>

<p>Best wishes </p>

<p>Marry</p>
"
"NaN","NaN","183534","<p>I am using function <code>neuralnet</code> in the package <code>neuralnet</code> to build the neural network, and I see the error:</p>

<p><code>algorithm did not converge in 1 of 1 repetition(s) within the stepmax</code></p>

<p>The neural network has 20 inputs and 1 output. The problem is, with the same data and same set of inputs, I ran linear regression or random forest without any problem. So what should I look to for debugging my problem?</p>
"
"0.080466267117873","0.0818012824723818","183603","<p>Please apologize for this potentially ""stupid"" question. But I am currently attempting to test a mediation in for a multilevel dataset. Unfortunately, the residuals of the regressions do not follow a Normal distribution and they do not have a constant variance. Ideally, I would thus use bootstrapping to obtain confidence intervals. However, the Mediation package in R does not provide this function for multilevel datasets. Instead, it calculates Quasi-Bayesian intervals in this case.</p>

<p>My question is:
<strong>Can I use Quasi-Bayesian Confidence intervals, if I am aware the residuals do not follow a Normal distribution and that they are heteroskedastic? If not, which package/functions could I use instead?</strong></p>

<p>Here is the code I have used. Unfortunately I cannot share a sample of my data, since it is confidential.</p>

<pre><code>&gt; med.fit &lt;- lmer(OTIF ~  LDLV + COLT + Slack2 + (1 | BU), data = Data_P5)
&gt; out.fit &lt;- lmer(EBIT ~ OTIF + Slack2 + LDLV + COLT  + (1 | BU), data = Data_P5)
&gt; med.out &lt;- mediate(med.fit, out.fit, treat = ""Slack2"", mediator = ""OTIF"", sims = 100)
&gt; summary(med.out)

Causal Mediation Analysis 
Quasi-Bayesian Confidence Intervals
Mediator Groups: BU 
Outcome Groups: BU 

Output Based on Overall Averages Across Groups 

            Estimate 95% CI Lower 95% CI Upper p-value
ACME            5.12e-03     1.49e-03     1.06e-02    0.00
ADE            -5.64e-03    -1.77e-02     8.46e-03    0.50
Total Effect   -5.24e-04    -1.38e-02     1.41e-02    0.86
Prop. Mediated -3.21e-01    -1.34e+01     6.01e+00    0.86

Sample Size Used: 167 


Simulations: 100 
</code></pre>

<p>Since bootstrapping is not available for multilevel models I get an error message:</p>

<pre><code>&gt; med.fit &lt;- lmer(OTIF ~  LDLV + COLT + Slack2 + (1 | BU), data = Data_P5)
&gt; out.fit &lt;- lmer(EBIT ~ OTIF + Slack2 + LDLV + COLT  + (1 | BU), data = Data_P5)
&gt; med.out &lt;- mediate(med.fit, out.fit, treat = ""Slack2"", mediator = ""OTIF"", sims = 100, 
+ boot = TRUE)
Error in mediate(med.fit, out.fit, treat = ""Slack2"", mediator = ""OTIF"",  : 
'boot' must be 'FALSE' for models used
</code></pre>
"
"0.107288356157164","0.102251603090477","183699","<p>I encountered a strange phenomenon when calculating pseudo R2 for logistic models when using aggregated files: the results are simply too good to be true. An example (but as far as I can see, every aggregated file offers similar problems):</p>

<pre><code> library(pscl)
 cuse &lt;- read.table(""http://data.princeton.edu/wws509/datasets/cuse.dat"",
               header=TRUE)

 head(cuse)
 cuse.fit &lt;- glm( cbind(using, notUsing) ~ age + education + wantsMore, 
             family = binomial, data=cuse)

 summary(cuse.fit)
 pR2(cuse.fit)     
</code></pre>

<p>The results are:</p>

<pre><code>&gt; summary(cuse.fit)

Call:
glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, 
family = binomial, data = cuse)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.5148  -0.9376   0.2408   0.9822   1.7333  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.8082     0.1590  -5.083 3.71e-07 ***
age25-29       0.3894     0.1759   2.214  0.02681 *  
age30-39       0.9086     0.1646   5.519 3.40e-08 ***
age40-49       1.1892     0.2144   5.546 2.92e-08 ***
educationlow  -0.3250     0.1240  -2.620  0.00879 ** 
wantsMoreyes  -0.8330     0.1175  -7.091 1.33e-12 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 165.772  on 15  degrees of freedom
Residual deviance:  29.917  on 10  degrees of freedom
AIC: 113.43

Number of Fisher Scoring iterations: 4

&gt; pR2(cuse.fit)
         llh      llhNull           G2     McFadden         r2ML 
 -50.7125647 -118.6401419  135.8551544    0.5725514    0.9997947 
       r2CU 
  0.9997950 
</code></pre>

<p>The last three outcomes from pscl function pR2 present McFadden's pseudo r-squared,  Maximum likelihood pseudo r-squared (Cox &amp; Snell) and Cragg and Uhler's or Nagelkerke's pseudo r-squared. The calculation seems to be flawless, but the outcomes close to 1 seem to good to be true.</p>

<p>Using weight instead of cbind:</p>

<pre><code>cuse2 = rbind(cuse,cuse)
cuse2$using.contraceptive=1
    cuse2$using.contraceptive[1:nrow(cuse)]=0
cuse2$freq = cuse2$notUsing
cuse2$freq[1:nrow(cuse)] = cuse2$using[1:nrow(cuse)]
cuse.fit2 = glm(using.contraceptive ~ age + education + wantsMore,
            weight=freq, family = binomial, data = cuse2)
summary(cuse.fit2)
round(pR2(cuse.fit2),5)
</code></pre>

<p>produces different logistic regression coefficients, and slightly different pseudo R2's for r2ml and r2CU and a large difference for McFadden R2:</p>

<pre><code>&gt; round(pR2(cuse.fit2),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.98567 
        r2CU 
     0.98567 
</code></pre>

<p>Full expansion results in very different estimates from pR2:</p>

<pre><code> cuse3 = rbind(cuse[rep(1:nrow(cuse), cuse[[""notUsing""]]), ],
          cuse[rep(1:nrow(cuse), cuse[[""using""]]), ])
 cuse3$using.contraceptive=1
     cuse3$using.contraceptive[1:sum(cuse$notUsing)]=0
 summary(cuse3)
 cuse.fit3 = glm(using.contraceptive ~ age + education + wantsMore,
            family = binomial, data = cuse3)
 summary(cuse.fit3)
 round(pR2(cuse.fit3),5)

 &gt; round(pR2(cuse.fit3),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.08106 
        r2CU 
     0.11376 
</code></pre>

<p>This indicates a logistic model which explains very little, which is a little bit more believable than the near perfect results from the aggregated files. Is there a more correct, and preferably more consistent, way to calculate the pseudo R2's? </p>
"
"0.026822089039291","0.0272670941574606","183724","<p>I need to fit a polynomial regression that accounts for measurement errors. I found out how to do it with a mcmc model (using RJags) and <strong>I would like to do it with a Maximum Likelihood Estimator</strong> (using <strong>mle2</strong> function in R), since the model will be later more complex and mle2 will be faster than mcmc.</p>

<p>My model in RJags looks like this (I put some data to make the code reproducible):</p>

<pre><code>modelFile = ""model.txt""
modelString = ""
model {
# Likelihood:
for (i in 1:N) {
y[i] ~ dnorm(y.hat[i], tauy[i])
y.hat[i] &lt;- b[1] + b[2]*x.hat[i] + b[3]*z.hat[i] 

x.hat[i] ~ dnorm(x[i], taux[i])
z.hat[i] ~ dnorm(z[i], tauz[i])

taux[i] &lt;- 1/pow(sdx[i],2)
tauy[i] &lt;- 1/pow(sdy[i],2)
tauz[i] &lt;- 1/pow(sdz[i],2)
}

for(j in 1:3) {b[j]~dunif(-2,2)}

}
""
writeLines(modelString,con=modelFile)


#Data

 ind &lt;- data.frame(A = c(2.428, 2.601, 2.749, 2.553, 2.753, 2.421, 2.579, 2.415, 2.407, 2.509),
                  B = c(0.95, 0.99, 1.05, 1.00, 1.04, 0.96, 1.01, 0.95, 0.95, 1.01),
                  C = c(-0.04, -0.09, 0.01, 0.04, -0.15, 0.11, -0.17, -0.12, -0.13, 0.17),
                  eA=runif(10, 0, 0.5), eB=runif(10,0,0.5), eC=runif(10,0,0.2))

ml.data &lt;- list(x=ind$A,
                    y=ind$B,
                z=ind$C,
                    sdy=ind$eA,
                sdx=ind$eB,
                    sdz=ind$eC,
                N=nrow(ind))

ml.par &lt;- c(""b"")

ml.mod &lt;- jags.model(modelFile,data=ml.data, n.chains=100, n.adapt=1000)

update(ml.mod, n.iter = 1000)

mcmc.out &lt;- coda.samples(ml.mod, var=ml.par, n.iter=10000)

#summary of the posterior distributions of the parameters
summary(mcmc.out)
</code></pre>

<p>How can I <strong>translate this into an mle2</strong>, or how the function would be? </p>
"
"NaN","NaN","183854","<p>I want to estimate a vector-valued model</p>

<p>$$\mathbf{y}_t = a\mathbf{y}_{t-1}+b\mathbf{y}_{t-2}+\cdots$$</p>

<p>Here, each $\mathbf{y}_t\in\mathbb{R}^n$ and the coefficients $a,b,\dotsc$ are real numbers (unlike in VAR, where they are matrices). I wonder what the best way to do this in R is. Applying the function <code>ar</code> doesn't seem to work for me ([i] <strong>is this only for series that lie in $\mathbb{R}^1$</strong>?), and when I apply standard linear regression, I'm not sure how I should test for the order of the model, etc. [ii] <strong>Should I go for the $R^2$ value and significance of the coefficients?</strong></p>

<p>Also, I have my data in a matrix $Y$ where each column corresponds to one time point. Currently, I regress as follows </p>

<pre><code>Y = c(Y[,10], Y[,9],..., Y[,2])
Y.l1 = c(Y[,9],..., Y[,1])
fit = lm(Y ~ 0 + Y.l1)
</code></pre>

<p>[iii] <strong>Is there a more elegant way?</strong></p>
"
"0.0599760143904067","0.0609710760849692","184137","<p>I would like to get the predicted values (with confidence intervals) for a multinomial logistic regression. I know this could be done with predict but in my case I have clustered standard errors in the following way:</p>

<pre><code>multinom &lt;- mlogit(Y ~0| X1+ X2 , data)
cl.mlogit   &lt;- function(fm, cluster){
  M &lt;- length(unique(cluster))
  N &lt;- length(cluster)
  K &lt;- length(coefficients(fm))
  dfc &lt;- (M/(M-1))
  uj  &lt;- apply(estfun(fm),2, function(x) tapply(x, cluster, sum));
  vcovCL &lt;- dfc*sandwich(fm, meat.=crossprod(uj)/N)
 coeftest(fm, vcovCL) 
}
cl.mlogit(multinom, data$group)
</code></pre>

<p>How I could use these results to get the predicted probabilities (with confidence intervals) for X1=1 and X2=0 for example and compare it with predicted probalities for X1=2 and X2=0. </p>

<p>Also, how could I get a confidence interval for that difference? In Stata prvalue do this last thing by using the delta method to get the confidence interval <a href=""http://www.indiana.edu/~jslsoc/stata/ci_computations/xulong-prvalue-23aug2005.pdf"" rel=""nofollow"">http://www.indiana.edu/~jslsoc/stata/ci_computations/xulong-prvalue-23aug2005.pdf</a>. Is there an easy way to do it in R?</p>
"
"0.0479808115123254","0.0609710760849692","184341","<p>I am using the Deming function provided by Terry T. on <a href=""http://www.mail-archive.com/r-help@r-project.org/msg85070.html"">this archived r-help thread</a>.  I am comparing two methods, so I have data that look like this:</p>

<pre><code>y  x     stdy   stdx
1  1.2   0.23   0.67
2  1.8   0.05   0.89
4  7.5   1.13   0.44
... ...  ...   ...
</code></pre>

<p>I have done my Deming regression (also called ""total least squares regression"") and I get a slope and intercept. I would like to get a correlation coefficient so I've start calculating the $R^2$. I have manually entered the formula: </p>

<pre><code>R2 &lt;- function(coef,i,x,y,sdty){
    predy    &lt;- (coef*x)+i
    stdyl    &lt;- sum((y-predy)^2)   ### The calculated std like if it was a lm (SSres)
    Reelstdy &lt;- sum(stdy)          ### the real stdy from the data  (SSres real)
    disty    &lt;- sum((y-mean(y))^2) ### SS tot
    R2       &lt;- 1-(stdyl/disty)    ### R2 formula
    R2avecstdyconnu &lt;- 1-(Reelstdy/disty) ### R2 with the known stdy
    return(data.frame(R2, R2avecstdyconnu, stdy, Reelstdy))
}
</code></pre>

<p>This formula works and gives me output.</p>

<ul>
<li>Which of the two $R^2$s makes more sense? (I personally think of both of them as kind of biased.)  </li>
<li>Is there a way to get a correlation coefficient from a total least squared regression?</li>
</ul>

<p>OUTPUT FROM THE DEMING REGRESSION:</p>

<pre><code>Call:
deming(x = Data$DS, y = Data$DM, xstd = Data$SES, ystd = Data$SEM,     dfbeta = T)

               Coef  se(coef)         z            p
Intercept 0.3874572 0.2249302 3.1004680 2.806415e-10
Slope     1.2546922 0.1140142 0.8450883 4.549709e-02

   Scale= 0.7906686 
&gt; 
</code></pre>
"
"0.0663812836584521","0.0674826151369737","184595","<p>I want to compare estimate with standard error in function of a continuous variable and a categorial variable . Here an example of what my data look like.</p>

<pre><code>y   stdy   ConVar  CatVar
1.3    0.1    1    Bob
2.4   0.4     1    Bob
1.5    0.3    2    Bob
3.6    0.2    3    Henri
...
</code></pre>

<p>I would like to perform a regression of my y estimate in function of the ConVar in first place. Then I would like to compare the estimate in function of the categorial variable. </p>

<p>I want to rectify my slope and average comparaison with the known standard error (stdy).</p>

<p>Is it possible .</p>

<p>I know orthogonal regression to compare two variables with known error but I don't known of a regression in which I can input standard error only on the y value.</p>

<p>Is that would do it if I do a mean of the standard error. mean of the standard error is sqrt(sum(std^2)/numberofobs^2)</p>

<pre><code>library(MethComp)
Deming(ConVar,y,stdy, boot=FALSE, keep.boot=FALSE, alpha=0.05)
</code></pre>

<p>Thanks</p>
"
"0.0599760143904067","0.0609710760849692","184795","<p>I have a number (48) bivariate relationships (N = 10 for each) where I want to fit a linear model and estimate the confidence interval (CI) using bootstrapping. </p>

<p>What I want to present, is the slope and CI for this regression. However, instead of picking one CI, I'd rather present the distribution of the slope estimates so the reader can judge for himself. What I thought about doing, is to present the histogram of the bootstrapped slope estimates along with the information about how many % of the estimates where > 0.</p>

<p>Is that a valid and/or good way to present the data? And is it valid to say that if 97.7 % of the slope estimates are > 0, the slope is significant with alpha = 0.023?</p>

<p>here is an example of what I mean</p>

<p><a href=""http://i.stack.imgur.com/33AxH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/33AxH.png"" alt=""scatterplots""></a></p>

<p><a href=""http://i.stack.imgur.com/FMaFT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FMaFT.png"" alt=""bootstrapped slope estimates, 10000 draws""></a></p>

<p>another way to formulate the question would be: prior to going this way, I calculated bootstrapped CI with the <code>boot.ci</code> function in in the <code>boot</code> package in <code>R</code>. However, the CI seem to be wider that what is suggested by the histograms. How exactly are bootstrapped CI calculated and is it wrong to assume that it should span 95% of the bootstrapped slope estimates? </p>
"
"0.0657004319817604","0.0667904674542028","185058","<p>I have to forecast sales for stores. So for that I am using ARIMA model.Here first we need to create times series object using ts function which takes frequency parameter.As far as I know we use 1=annual, 4=quarterly, 12=monthly but don't know sure what will be frequency for daily observations. I tried using 1,7,365 and number of observation as values for frequency parameter but with these I am not able to get proper plots and forecast.My second question is how to deal with 0 values for specific observation as they are producing errors as follows:</p>

<pre><code>Error in na.fail.default(as.ts(x)) : missing values in object for acf() and pacf() 
</code></pre>

<p>and</p>

<pre><code>Error in OCSBtest(x, m) : The OCSB regression model cannot be estimatedauto.arima() functions.
</code></pre>

<p>Here is the data:
<a href=""https://drive.google.com/file/d/0B-KJYBgmb044QlNUS3FhVFhUbE0/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B-KJYBgmb044QlNUS3FhVFhUbE0/view?usp=sharing</a></p>

<p>Below is my code:</p>

<pre><code> data&lt;-read.csv(""Book5.csv"")
   View(data)

   mydata&lt;- ts(data[,2], start=1, end=181, frequency = 7)
   View(mydata)
   plot(mydata, xlab=""Day"", ylab = ""Sales"")

   plot(diff(mydata),xlab=""Day"",ylab=""Differenced Sales"")
   plot(log10(mydata),ylab=""Log(Sales)"")
   plot(diff(log10(mydata)),ylab=""Differenced Log (Sales)"")

   par(mfrow = c(1,2))
   acf(ts(diff(log10(mydata))),main=""ACF Sales"")
   pacf(ts(diff(log10(mydata))),main=""PACF Sales"")

   require(forecast)
   ARIMAfit &lt;- auto.arima(log10(mydata), approximation=FALSE,trace=FALSE)
   summary(ARIMAfit)

   pred &lt;- predict(ARIMAfit, n.ahead= 31)
   pred
   class(pred$pred)
       10^(pred$pred)

   # Write CSV in R
   write.csv(10^(pred$pred), file = ""MyData.csv"")

   plot(mydata,type=""l"",xlim=c(1,52),ylim=c(1,6000),xlab = ""Day"",ylab =   ""Sales"")
   lines(10^(pred$pred),col=""blue"")
       lines(10^(pred$pred+2*pred$se),col=""orange"")
       lines(10^(pred$pred-2*pred$se),col=""orange"")
</code></pre>
"
"0.116914775576919","0.118854507916379","185116","<p>Goal is to evaluate chess players using a novel analysis system I'm been working on -- not all wins are created equal, finding the only move in razor sharp positions is better than finding the best move when the ten-best-alternates are negligibly worse, etc.</p>

<p>Current dataset I'm working with towards proof of concept has 30 players. The design matrix has players as the columns, but each player gets two columns: one for when they're playing as white, one for when they're playing as black. Each row of the design matrix represents half of a match, and 1/0/-1 dummies are used for white/not present/black.</p>

<p>Example: if Player 4 and Player 9 played a match, the design matrix will have two rows for this match. One row will have p4w assigned a ""1"" and p9b assigned a ""-1"". The other row will have p4b assigned a ""-1"" and p9w assigned a ""1"". All other player columns are 0.</p>

<p>The result vector is the Engine's score for the player playing as white in that half of the match.</p>

<p>There's also two other columns, Sw and Sb, to attempt to quantify the value of being white first in any given match and if a penalty exists for the player who started as black once they switch to white -- since white always moves first, and white wins more games than black, black is more likely to be disadvantaged after the first game.</p>

<p>Using matrix math rather than an R function.</p>

<pre><code>csv &lt;- read.csv(""~/chess.csv"", header=TRUE)
engine &lt;- as.numeric(csv$Engine)

# ready design matrix/remove dropped variables
csv$Engine &lt;- NULL
csv$Sb &lt;- NULL
csv$P30w &lt;- NULL
csv$P30b &lt;- NULL

# readies X and Y
X &lt;- data.matrix(csv)
Y &lt;- engine

# remove copies
remove(csv)
remove(engine)

# Add one column of ""1"" to X
one.col &lt;- matrix(1, nrow(X), 1)
X &lt;- cbind(X, one.col)

# transposing X
X.t = t(X)

# X'X, X'Y
X.t.X &lt;- X.t %*% X
X.t.Y &lt;- X.t %*% Y

# MATHS
betahat = solve(X.t.X) %*% X.t.Y
</code></pre>

<p>Here's the CSV: <a href=""http://www.filedropper.com/chess_1"" rel=""nofollow"">http://www.filedropper.com/chess_1</a></p>

<p>Right from the top, I have to drop Sb -- it's redundant. I then am forced to drop a player to defeat the ""system is computationally singular"" error. In this case, I'm dropping the same player lm() would: the last one.</p>

<p>I have no philosophical objections to dropping variables but for the purposes of this, for evaluating players against each other, the incompleteness is troublesome.</p>

<p>Using Ridge Regression ""works"" to prevent any variable from being dropped, but this is unsatisfying -- are the results really then meaning what they should? X + 0 doesn't help matters for this problem either.</p>

<p>Are there any other tools I'm missing? Is ridge regression the right path to take for this problem but, rather than penalize towards zero, penalize towards priors?</p>
"
"0.0848188929679971","0.0862261227118454","185449","<p>I've implemented a comparison between the performance of 80%-forecast intervals is in the forecast package - see 1st part of the code below providing a number of hits
This number states, how many times the forcast interval was right for the left-out data entries. Btw regarding variable names: the German ""preis"" means ""price"" and ""absatz"" means ""sales"", i.e.
""preise"" means ""prices"" and ""absaetze"" is the plural for ""sales"".</p>

<p>So, I compared the formula-based prediction interval to what I think bootstrapping is - see 2nd part of the code. But the number of hits in the 2nd case by no means resembles the 80% of the first case.
The following actions did not help to reproduce the 80% : using less data in the given data frame, using median formulas for bootstrapping instead of the upper/lower computation in the loop,
more samples resampling in the resampling.</p>

<p>I cannot imagine the bootstrapping approach performing so bad - what did I do wrong?  </p>

<pre><code>#given

# data frame

preis&lt;-c(1:100)
absatz&lt;-(-2*preis)+1000+rnorm(100)


jeansData&lt;-data.frame(absaetze=absatz,preise=preis)

#### implementation ###


#leave-one-out cross-validation for formula, i.e. with the borders         given     above  
###### (1ST PART) ########

numberOfHits&lt;-0

for(i in (1:100)){

preisCandidateToBeChecked&lt;-preis[i]
absatzCandidateToBeChecked&lt;-absatz[i]

absatzWithoutCandidate&lt;-absatz[-i]
preisWithoutCandidate&lt;-preis[-i]

jeansData&lt;-data.frame            (absaetze=absatzWithoutCandidate,preise=preisWithoutCandidate)
fit&lt;-lm((absaetze~preise), data=jeansData)

#check, if in interval and count as hit, if value is in interval

if(absatzCandidateToBeChecked &lt;= (forecast(fit,     newdata=preisCandidateToBeChecked)$upper[1]) &amp; (absatzCandidateToBeChecked &gt;= (forecast(fit, newdata=preisCandidateToBeChecked)$lower[1])) )
{numberOfHits&lt;-numberOfHits+1}

}

#execute code until here and inspect numberOfHits; the hit rate pretty much resembles the 80% assumed

#then execute the rest

#leave-one-ot cross-validation for bootstrapping (not using the bootstrap function)  ###### (2ND PART) ########


numberOfHits&lt;-0

for(i in (1:100)){

preisCandidateToBeChecked&lt;-preis[i]
absatzCandidateToBeChecked&lt;-absatz[i]

absatzWithoutCandidate&lt;-absatz[-i]
preisWithoutCandidate&lt;-preis[-i]

jeansData&lt;-data.frame(absaetze=absatzWithoutCandidate,preise=preisWithoutCandidate)

#ten or hundred or thousand regressions by bootstrapping

allPredictions&lt;-c()

for(j in (1:10)){

fit&lt;-lm((absaetze~preise), data=jeansData[sample(nrow(jeansData),10,replace=TRUE),])

allPredictions&lt;-c(allPredictions,forecast(fit,     newdata=preisCandidateToBeChecked)$mean)

}

#build and name bootstrapped forecast interval from regressions

upper&lt;-sort(allPredictions)[9]
lower&lt;-sort(allPredictions)[2]

if((absatzCandidateToBeChecked &lt;= upper) &amp; (absatzCandidateToBeChecked &gt;= lower) )
{numberOfHits&lt;-numberOfHits+1}

} #inspect numberOfHitsAgain - it's around 40%. What is foul here?!
</code></pre>
"
"0.0479808115123254","0.0609710760849692","185800","<p>I try to find a model using logistic regression. More precisely, what I did so far, is using stepwise regression and subset selection (although I know, it is often a bad idea) to find the ""best"" model. Clearly, depending on the information criteria I used, I got different results. </p>

<p>Now, I found an interesting example on page 250 in the book <a href=""http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"" rel=""nofollow"">""An Introduction to Statistical Learning""</a>. They chose among the models of different sizes using cross-validation, that is they make predictions for each model and compute the test errors. Eventually, the compute the cross validation error and choose the model corresponding to the minimal average cross-validation error. </p>

<p>However, the function <code>regsubsets</code> of the R package ""leaps"" is only working for linear models. How can I implement this for logistic regression or glm models in general? </p>

<p>My idea was, to just estimate the models within a cross-validation using the <code>step</code> function of the ""stats"" package and then kind of take the average number of features (which is determined by minimum AIC, for example). Is this a legitimate approach?</p>
"
"0.0599760143904067","0.0609710760849692","185826","<p>I want to implement kernel ridge regression (KRR) using a polynomial kernel as a function that takes the training objects, training labels and test objects as arguments, and outputs the vector of predicted labels for test objects (in R).</p>

<p>The problem is this is not something I study so I am struggling to find a starting point for this algorithm. This is the guide I would like to follow: <a href=""http://stat.wikia.com/wiki/Kernel_Ridge_Regression"" rel=""nofollow"">http://stat.wikia.com/wiki/Kernel_Ridge_Regression</a> (unless somebody can recommend me a better one)</p>

<p>Does anybody know if the guide is using a polynomial or radial kernel? I have my training objects, training labels and test objects. What is the first step I should take? How are BETA and LAMBDA calculated?</p>

<p>I have been looking into this for quite some time and can't find any tutorial that breaks it down into baby steps. Would REALLY appreciate any insight.   </p>
"
"0.0599760143904067","0.0609710760849692","185828","<p>I want to implement kernel ridge regression (KRR) using a polynomial kernel as a function that takes the training objects, training labels and test objects as arguments, and outputs the vector of predicted labels for test objects (in R).</p>

<p>The problem is this is not something I study so I am struggling to find a starting point for this algorithm. This is the guide I would like to follow: <a href=""http://stat.wikia.com/wiki/Kernel_Ridge_Regression"" rel=""nofollow"">http://stat.wikia.com/wiki/Kernel_Ridge_Regression</a> (unless somebody can recommend me a better one). </p>

<p>Suppose I have a data set like this, where the first 3 are training objects (Sales is the label) and the rest are test objects:</p>

<pre><code>TV  Radio   Newspaper   Sales
230.1   37.8    69.2    22.1
44.5    39.3    45.1    10.4
17.2    45.9    69.3    9.3
151.5   41.3    58.5    18.5
180.8   10.8    58.4    12.9
8.7     48.9    75      7.2
</code></pre>

<p>Does anybody know if the guide is using a polynomial or radial kernel? I have my training objects, training labels and test objects. What is the first step I should take? How are BETA and LAMBDA calculated?</p>

<p>I have been looking into this for quite some time and can't find any tutorial that breaks it down into baby steps. Would REALLY appreciate any insight.   </p>
"
"0.107288356157164","0.109068376629842","186164","<p>I am new to time series and am trying to fit some time series data.</p>

<p>I understand the general concept of ARIMA model. However, as I read more textbooks and articles from Rob Hyndman, I realized I could put some regressors using the <code>xreg</code> argument for the functions <code>auto.arima</code> or <code>arima</code> in R to get an ARMAX model. Therefore, I wonder if it is still necessary to include seasonality in <code>ts(...,frequency)</code> as everything can be specified as dummy variable within the <code>xreg</code> matrix and a more complicated seasonality structure (e.g. monthly seasonality) can be specified. </p>

<p>In addition, what would be a good way to check the accuracy of the forecast? I am fitting multiple time series data with a hierarchical structure. Using <code>auto.arima</code>, I am able to select the best model and validate the model by looking at the residuals (check whether they are white noise). However, is there a way to even improve on the model if the prediction is still far from the actual data?</p>

<p>To sum up, </p>

<ol>
<li>Is the <code>frequency</code> argument in <code>ts</code> function really necessary?  Can I just specify everything in the <code>xreg</code> matrix?</li>
<li>What would be a normal routine to improve on model after selecting the appropriate ARIMA model with the lowest AIC?</li>
</ol>

<p>Updates (Dec 17):</p>

<p>I am now able to fit an ARIMA model with SARIMA error by specifying <code>xreg</code> argument and <code>seasonal=F</code>. One issue that I have with that is, my <code>xreg</code> matrix is not invertible (I assumed) and its not due to the presence of intercept term. Thus <code>auto.arima()</code> only fit a <code>c(0,0,0)</code> model.</p>

<p>I then tried using <code>Arima()</code> to manually select model and it outputted the following error</p>

<pre><code>Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
non-finite value supplied by optim
</code></pre>

<p>I check the <code>xreg</code> matrix and it turns out column 48 (Day) and column 52 (2015) is causing the issue. Could you check if there's something wrong with my <a href=""https://drive.google.com/file/d/0B-b9YsAB5mpnam1oN0hYcFRwLXM/view?usp=sharing"" rel=""nofollow"">matrix structure</a> ? </p>

<p>If you think this additional updates should be asked in stack overflow or additional question, I will move it.</p>
"
"0.0599760143904067","0.0609710760849692","186240","<p>Hi I am trying a mediation analysis (using library(""mediation"") in R)</p>

<p>My model has 3 predictors and one mediator (n=455), but I am only interested in predictor 1. There is some collinerarity between predictor 1 and 2 - 0.383444 (Pearson). No collinerarity between predictor 3 and the others. The Mediator is correlated with IV1 and slightly with IV2. Predictors, Mediator and dependent variable are all continuous.</p>

<pre><code>lm(DV ~ IV1 + IV2 + IV3 , data = data)
</code></pre>

<p>Only IV2 is significant, R2 = 0.050</p>

<pre><code>lm(DV ~ Mediator + IV1 + IV2 + IV3 , data = data)
</code></pre>

<p>Mediator and IV2 is significant, R2 = 0.056</p>

<p>I have a much bigger dataset with n = 1200, but unfortunately I don't have Mediator information available for them. If I do a linear regression to predict DV with this dataset, IV1 and IV2 are both highly significant, the standardized beta meaningful.</p>

<ol>
<li><p>With this information can I investigate the mediating effect of the mediator on IV1 with my small dataset with 455 subjects (using the mediate()-Function of the ""mediation""-package in R) , even though the dataset itself is too small to show a significant effect of IV1 on the DV?</p></li>
<li><p>Also, I was wondering whether my mediator might mediate IV2-effect. The correlation between IV1 and the mediator is higher than between IV2 and the mediator though. </p></li>
</ol>

<p>I am thankful for any ideas.</p>
"
"0.026822089039291","0.0272670941574606","186259","<p>My situation is the following; I'm running a classification tree (with the function rpart in R) and a logistic regression on a data set using 10 Fold cross-validation. Since I'm estimating the model for each combination of folds, my idea was to show the most important variables for each combination. For the classification tree, I automatically get the important variables. However, since I am running a logistic model using 39 variables, this is a little bit tricky. If someone has any ideas, I would appreciate it a lot!  Thank you</p>
"
"0.053644178078582","0.0545341883149212","186283","<p>I am trying to run a log log regression of the form LNX ~ LNP, where X = Units Sold and P = Price (in reality, I would have a number of other variables included in the model).  The data is retail data; ultimately, I want to run the exact same regression above for each individual item.  so, LNX ~ LNP for Item # 1, LNX ~ LNP for Item #2, and so forth.</p>

<p>I have tried a number of methods, but for each method, the intercept and coefficient are coming out the same for all items (ie, intercept for item # 1 = intercept for item # 2 and so on).  This is clearly incorrect and is an artifact of my being a novice to R.</p>

<p>Below is a toy data set that is similar in nature to a real data set I'm working with.  </p>

<p><a href=""http://i.stack.imgur.com/ZAf2m.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZAf2m.png"" alt=""enter image description here""></a></p>

<p>Below is one of the codes that I've tried.</p>

<pre><code>T&lt;-read.csv(""ElasticityToy.csv"")
X = log(T$Units)
    P = log (T$Price)
model&lt;-lapply(1:2, function (i) lm(formula = X ~ P, data = T))
</code></pre>

<p>This, again, gives the exact same intercept and coefficient for each both item # 1 and item # 2.  There is the added difficulty that this is panel data, so any input here is appreciated as well.</p>

<p>Ultimately, I'm looking to elasticity for each item (impossible given the simple form I've presented here, I know, but I'm looking more for code help at this point as I'm new to R).</p>

<p>I appreciate any help you can give.  Mods, if this is the incorrect place to post this, can you please point me in the correct direction?</p>
"
"0.11706119363752","0.124953522958051","186393","<p>I built a multivariate regression tree using the <code>party</code> package in R. The depth of the tree (max. number of splits) is 13. For the first 3/4 splits the tree is relatively easy to interpret which is useful in our case. However with an increase in the number of splits interpretation becomes impossible. The idea is to get a measure of variable importance from this tree, similar to the idea of variable importance in random forests. For random forests there is function <code>varimp</code> but for regression trees it does not seem to exist. I'm aware of the <code>caret</code> package but it is built for CART of the <code>rpart</code> package.</p>

<p>Now, I have an idea of how to measure variable importance in CART but i'm a little lost on how to implement it using the <code>party</code>/<code>partykit</code> package. From <em>Ishwaran (2007)</em>:</p>

<blockquote>
  <p>We define the VIMP for a variable x<sub>v</sub> as the difference between prediction error when x<sub>v</sub> is â€œnoised upâ€ versus the prediction error otherwise. To noise up x<sub>v</sub> we adopt the following convention. To assign a terminal value to a case x, drop x down T [which is your tree] and follow its path until either a terminal node is reached or a node with a split depending upon x<sub>v</sub> is reached. In the latter case choose the right or left daughter of the node with equal probability. Now continue down the tree, randomly choosing right and left daughter nodes whenever a split is encountered (whether the split depends upon x<sub>v</sub> or not) until reaching a terminal node. Assign x the node membership of this terminal node.</p>
</blockquote>

<p>However:</p>

<blockquote>
  <p>This type of scenario shows that a non-informative variable can appear informative over a single tree under our noising up process...Moreover, for a single tree, this kind of problem can be resolved by slightly modifying the noising up process. Rather than using random left-right assignments on all nodes beneath x<sub>v</sub>, use random assignments for only those nodes that split on x<sub>v</sub>. This will impact prediction only when x<sub>v</sub> is informative and not affect prediction for non-informative variables</p>
</blockquote>

<p>How do I go about implementing this procedure? It seems that the <code>fitted_node()</code> function from the <code>partykit</code> package should do the trick. <code>fitted_node()</code> takes the following arguments:</p>

<pre><code>fitted_node(node, data, vmatch = 1:ncol(data), obs = 1:nrow(data), perm = NULL)
</code></pre>

<p>where</p>

<blockquote>
  <p><strong>node</strong>:  an object of class partynode<br>
   <strong>data</strong>: a list or data.frame<br>
   <strong>vmatch</strong>: a permutation of the variable numbers in data<br>
   <strong>obs</strong>: a logical or integer vector indicating a subset of the           observations in  data<br>
  <strong>perm</strong>: a vector of integers specifying the variables to be permuted
  prior before splitting (i.e., for computing permutation variable
  importances).  The default NULL doesnâ€™t alter the data.</p>
</blockquote>

<p>I can recursively partition the <code>data</code> using the tree specified in <code>node</code>. However how do I ""noise up"" one of the splitting variables in my tree? It is not clear to me whether i should use the <code>vmatch</code> and/or <code>perm</code> arguments and how i should specify them (for example do <code>perm</code> and <code>vmatch</code> refer to the column number of the covariate or do they refer to the cells in <code>data</code>?)</p>

<h2>References</h2>

<ol>
<li>Ishwaran, H. (2007). Variable importance in binary regression trees
and forests. Electronic Journal of Statistics, 1, 519â€“537.
<a href=""http://doi.org/10.1214/07-EJS039"" rel=""nofollow"">http://doi.org/10.1214/07-EJS039</a></li>
</ol>
"
"0.0547503599848004","0.0667904674542028","186620","<p>I've used the gbm in R to generate a model. Although I can use predict.gbm to fit the model on new data set, I want to know the detailed step of gbm to calculate the prediction, beacuse I need to write such code in C++ for other application. </p>

<p>I used</p>

<pre><code>    tree &lt;- pretty.gbm.tree(model, i.tree=1)
</code></pre>

<p>and it shows:</p>

<pre><code>    SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight    Prediction
    0        22  1.443225e-04        1        26          30      529.36379 132010 -5.171610e-06
    1        48 -7.033773e-04        2        18          25      351.30874  91916 -4.234042e-04
    2        32 -1.581934e-03        3         4          17      294.59416   4239 -3.235046e-03
    3        -1  4.159611e-03       -1        -1          -1        0.00000    478  4.159611e-03
    4        47  2.370844e-02        5        12          16      168.32051   3761 -4.174862e-03
    5        30 -6.295779e-01        6         7          11      105.65495   3525 -3.627476e-03
    6        -1  4.671729e-03       -1        -1          -1        0.00000    147  4.671729e-03
    7        16 -5.710531e-01        8         9          10       98.05341   3378 -3.988632e-03
    8        -1 -2.225353e-03       -1        -1          -1        0.00000   1631 -2.225353e-03
    9        -1 -5.634830e-03       -1        -1          -1        0.00000   1747 -5.634830e-03
    10       -1 -3.988632e-03       -1        -1          -1        0.00000   3378 -3.988632e-03
    11       -1 -3.627476e-03       -1        -1          -1        0.00000   3525 -3.627476e-03
    12       43 -2.381094e-02       13        14          15      135.69243    236 -1.235085e-02
    13       -1 -6.450770e-03       -1        -1          -1        0.00000    147 -6.450770e-03
    14       -1 -2.209593e-02       -1        -1          -1        0.00000     89 -2.209593e-02
    15       -1 -1.235085e-02       -1        -1          -1        0.00000    236 -1.235085e-02
    16       -1 -4.174862e-03       -1        -1          -1        0.00000   3761 -4.174862e-03
    17       -1 -3.235046e-03       -1        -1          -1        0.00000   4239 -3.235046e-03
    18        0  8.715281e-02       19        23          24      128.75576  87677 -2.874671e-04
    19       36  3.360935e-01       20        21          22      106.12050  51342 -6.098461e-04
    20       -1 -8.775861e-04       -1        -1          -1        0.00000  38121 -8.775861e-04
    21       -1  1.621467e-04       -1        -1          -1        0.00000  13221  1.621467e-04
    22       -1 -6.098461e-04       -1        -1          -1        0.00000  51342 -6.098461e-04
    23       -1  1.680601e-04       -1        -1          -1        0.00000  36335  1.680601e-04
    24       -1 -2.874671e-04       -1        -1          -1        0.00000  87677 -2.874671e-04
    25       -1 -4.234042e-04       -1        -1          -1        0.00000  91916 -4.234042e-04
    26       48  1.212169e-04       27        28          29      118.94817  40094  9.536318e-04
    27       -1  4.416651e-04       -1        -1          -1        0.00000  21287  4.416651e-04
    28       -1  1.533109e-03       -1        -1          -1        0.00000  18807  1.533109e-03
    29       -1  9.536318e-04       -1        -1          -1        0.00000  40094  9.536318e-04
    30       -1 -5.171610e-06       -1        -1          -1        0.00000 132010 -5.171610e-06
</code></pre>

<p>Then I use the first 10 samples of the original data set to calculate the prediction:</p>

<pre><code>    pred &lt;- predict.gbm(model, newdata=train.sample[1:10,],n.trees=1)
    pred
</code></pre>

<p>it shows:</p>

<pre><code>    [1] -0.01897030 -0.01897030 -0.01897030 -0.01896438 -0.01896438 -0.02001003 -0.02001003 -0.02001003 -0.02001003 -0.02001003
</code></pre>

<p>My understanding of gbm is that the predicted values would be one of the values of a leaf. But these fitted values are not shown in the original tree. I check the code of predict.gbm, it shows the core part of it is to call a compiled function gbm_pred, whose detailed is hidden.Does anybody know how to reconstruct a gbm regression step-by-step in R? </p>

<p>Thank you very much.</p>
"
"0.0599760143904067","0.0487768608679754","186667","<p>Let's say I have a small dataset:</p>

<pre><code>data &lt;- replicate(4,rnorm(13))
</code></pre>

<p>I want to test the out-of-sample predictions of a regression model as a function of increasing training set size (increasing by 10% in each increment).</p>

<p>I use the following procedure:</p>

<pre><code>test.set &lt;- 3

#for each iteration increase the test set by 10%
train.set &lt;- (nrow(data)-test.set) * seq(0.1,0.9,0.1)
train.set &lt;- train.set[train.set&gt;1]
results &lt;- vector()

  y=0
  for (t in train.set){
    y=y+1
    trainSize &lt;- t
    train &lt;- sample(1:nrow(data),trainSize)
    test &lt;- sample(1:nrow(data),test.set) 

    test.data &lt;- data[test,]
    train.data &lt;- data[train,]



    #fit a linear regression:
    train.data &lt;- as.data.frame(train.data)
    model &lt;- lm(train.data[,1] ~., data=train.data[,2:4])

    #get predictions:
    test.data &lt;- as.data.frame(test.data)
    predictions &lt;- predict(model,test.data)

    #calculate out of sample R squared (1-SSE/TSS): 

    error &lt;- 1 - sum( (test.data[,1] - predictions)^2 ) / ((nrow(test.data)-1) * var(test.data[,1]))

    results[y] &lt;- error
  }
</code></pre>

<p>I repeat this procedure several times and take the average of the repetitions.</p>

<p>My problem is that I get unreliable results. I am assuming this is because the dataset is really small. What could I do to get more reasonable estimates?</p>
"
"0.0967084173462244","0.0983129061176287","186725","<p>Short version: How would one be able to quantify an intervention effect in time-series analysis when the intervention decreases seasonal amplitude variation but doesn't directly effect the median?</p>

<p><a href=""https://www.dropbox.com/s/hb3g7j17igeqnoc/dat.csv?dl=0"" rel=""nofollow"">Here</a> is a link to my raw data.</p>

<p>I have a complex time-series of daily incidence numbers for a population over 7 years, totaling 2557 observations. There is a strong weekly and yearly seasonality (high incidence in winter months and low incidence in summer months). There is a baseline negative trend which is orders of magnitude smaller than the seasonality. An intervention was introduced at time = 1700. This intervention should theoretically not cause a level shift. My aim is to detect whether the intervention increases the baseline negative trend.</p>

<p>I have attempted to fit a dynamic linear regression with ARIMA errors in R using <code>auto.arima()</code> in the <code>forecast</code> package. I modeled the weekly season using a dummy variable for each weekday and the weekend. I modeled the monthly seasonality with harmonics using <code>fourier()</code> function in the <code>forecast</code> package. An the intervention effect was coded in by specifying the time index and post-intervention times as independent variables using the methods described in <a href=""http://isites.harvard.edu/fs/docs/icb.topic79832.files/L06_Program_Evaluation_2/Segmented_Regression.Wagner.2002.pdf"" rel=""nofollow"">Segmented regression analysis of interrupted time series studies in medication use research</a>. With these variables specified <code>auto.arima()</code> suggests an ARMA(7,7) process. The coefficients for baseline trend and post-intervention trend are however non-significant.  </p>

<p>I am concerned that by using fourier terms to model away the seasonality I am artificially removing any intervention effect, as visual analysis of the time series indicates that the intervention is specifically decreasing incidence during the winter months and therefore reducing the yearly seasonal variability. </p>
"
"0.100359067582572","0.0947366868222571","186728","<p>I am using the great <code>{caret}</code> package to run a lot of models, however I would like to analyse the model as one usually does having run that model in its own right, i.e. not within caret.</p>

<p>I am using the mboost package, starting with the <code>glmboost</code> function. If you run this model there are then functions within the mboost package that can be applied directly to the output of that function. however, these same functions do not work on the output of <code>train</code> from caret.
<code>train</code> is essentially the wrapper function which allows you to optimise the parameters for the chosen model, glmboost in my case.</p>

<p>Here is some dummy code if anybody wants to play with it. Its a boosted tree regression model, first using the <code>glmboost</code> function directly from the mboost package, then the same thing through the caret package (with some extra parameters to optimise over):</p>

<pre><code>## ============================================================== ##
##  Create a simple model using glmboost that runs through caret  ##
## ============================================================== ##

## install as necessary!
library(mboost)
library(caret)
## Use multicore if you can!
library(doMC)
registerDoMC(4)

## ============= ##
##  Create data  ##
## ============= ##

## Let's say we are predicting a numeric value, based on the predictors
## 70 observations of 10 variables, assuming they are chronologically order (a time-series)

set.seed(666)                                                # the devil's seed
myData &lt;- as.data.frame(matrix(rnorm(70*15, 2, .4), 70, 10)) #10 columns of random numbers
names(myData) &lt;- c(""to.predict"", paste0(""var_"", seq(1, 9)))
# Have a ganders
str(myData)                             

## Create model output using the mboost package directly
glm_mboost &lt;- glmboost(to.predict ~ .,  # predict against all variables
                       myData,          # supply our data
                       control = boost_control(mstop = 200)
                       )

## This is what I'd like to do with the output from the caret package!
plot(glm_mboost)
cvr &lt;- cvrisk(glm_mboost)
plot(cvr)

## ========================================== ##
##  Set parameters for train() - using caret  ##
## ========================================== ##

## glmboost takes 'mstop' and 'prune' as inputs
myGrid &lt;- expand.grid(mstop = seq(20, 250, 50),
                      prune = ""AIC""    #this isn't actually required by the mboost package!
                      )
myControl &lt;- trainControl(method = ""timeslice"", # take consequetive portions of the time-series
                          fixedWindow = TRUE, # If this is TRUE, we get the error
                          horizon = 1,
                          initialWindow = 20) # ~1 months of trading days
## fixedWindow = TRUE  --&gt; 

## =============== ##
##  Run the model  ##
## =============== ##

glm_caret &lt;- train(to.predict ~ ., data = myData,
                method = ""glmboost"",
                #metric = ""MyGauss"",
                trControl = myControl,
                tuneGrid = myGrid
                ##verbose = FALSE)
                )

## Maybe this will give you some idea about how to extract it
str(glm_caret)

## This is the best I can do, but the first plot doesn't come out right
x &lt;- glm_caret$finalModel
plot(x)
cvr1 &lt;- cvrisk(x)
plot(cvr1)
</code></pre>

<p>An idea I have is to simply use the optimal output given by caret to run the <code>glmboost</code> function once, with the provided parameters, but as I am going through many models, I'd rather save the computing time!</p>
"
"0.0929144419623766","0.0865846528350581","186845","<p>I created some data using the following code:</p>

<pre><code>set.seed(1221)
x &lt;- runif(500)
y &lt;- runif(500,0,2)
z &lt;- rep(0,500)
z[-0.8*x + y - 0.75 &gt; 0] &lt;- 1
plot(x,y,col=as.factor(z))
</code></pre>

<p>This produces the following plot</p>

<p><a href=""http://i.stack.imgur.com/ycWdr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ycWdr.png"" alt=""enter image description here""></a></p>

<p>The data is linearly separable. Then, I applied the glm function to create a logistic regression model.</p>

<pre><code>df &lt;- data.frame(class = z, x = x, y = y)
model &lt;- glm(z ~ x + y, family = binomial, data = df)
</code></pre>

<p>This produces the following output:</p>

<pre><code>summary(model)
Call:
glm(formula = z ~ x + y, family = binomial, data = df)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.127e-04  -2.000e-08  -2.000e-08   2.000e-08   7.699e-04  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    -1062      52666   -0.02    0.984
x              -1163      57197   -0.02    0.984
y               1433      70408    0.02    0.984

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.8274e+02  on 499  degrees of freedom
Residual deviance: 1.3345e-06  on 497  degrees of freedom
AIC: 6

Number of Fisher Scoring iterations: 25
</code></pre>

<p>The result surprised me, first because the parameter estimates are huge, and second because I was expecting such estimates to be close to the original decision boundary function, i.e. <code>-0.8x + y - 0.75 = 0</code>.</p>

<p>I then used the <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">glmnet</a> package to see if I could solve this issue. This package creates a penalised logistic regression model in order to deal with the large values in the parameter estimates. The code I used is the following:</p>

<pre><code>library(glmnet)
cvfit &lt;- cv.glmnet(as.matrix(df[,-1]), as.factor(df$class), family =   ""binomial"", type.measure = ""class"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/vH4AV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vH4AV.png"" alt=""enter image description here""></a></p>

<p>And the coefficients for the optimal penalty strength are:</p>

<pre><code>coef(cvfit, s = ""lambda.min"")
3 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept) -84.01446
x           -91.40983
y           113.18736
</code></pre>

<p>Such coefficients are smaller than the ones obtained with the <code>glm</code> function. Still they are not the same as the decision boundary function. </p>

<p>Does anybody know why this is happening? Any help is greatly appreciated.</p>
"
"0.0464572209811883","0.0472279924554862","187015","<p>First, I have read <a href=""http://stats.stackexchange.com/questions/10985/dispersion-parameter-of-negbin-distribution/"" title=""this post"">this post</a>, <a href=""http://stats.stackexchange.com/questions/10419/what-is-theta-in-a-negative-binomial-regression-fitted-with-r/"" title=""this post"">this post</a> and <a href=""http://stats.stackexchange.com/questions/66586/is-there-a-test-to-determine-whether-glm-overdispersion-is-significant/"" title=""this post"">this post</a>. All have very useful information. I have three other more specific questions.</p>

<p>I have estimated a negative binomial model using the glm.nb function of MASS and discovered the following parameters
Theta: 9.0487, S.E: 0.444</p>

<ol>
<li>Is it correct to assume that dispersion parameter has a standard deviation of 20.38?</li>
<li>Does this value correspond to the Poisson overdispersion that is corrected by the negative binomial model or is my model still overdispersed?</li>
<li>Joseph Hilbe states in his <a href=""http://www.cambridge.org/us/academic/subjects/statistics-probability/statistical-theory-and-methods/modeling-count-data/"" rel=""nofollow"" title=""book"">book</a> that R's glm.nb function employs an inverted relationship of the dispersion parameter, theta. Thus a Poisson model results when theta approaches infinity. Suppose now that my second glm.nb model had estimates of Theta: 19.0487, S.E: 0.444. Would this model be less overdispersed than the first model?</li>
</ol>
"
"0.131400863963521","0.133580934908406","187100","<p>I have a certain knowledge in stochastic processes (specially analysis of nonstationary signals), but in addition to be a beginner in R, I have never worked with regression models before.
Well, I have some doubts on understanding the outcome of the function summary() in R, when using with the results of a glm model fitted to my data. Well, suppose I used the following command to fit a generalized linear model to my data:**</p>

<pre><code>glm_model &lt;- glm(Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
</code></pre>

<p>Then I use summary(glm_model) to obtain the following:</p>

<pre><code>Call: 
glm(formula = Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-7.4583  -0.8985   0.1628   1.0670   6.0673  
Coefficients:

Estimate Std. Error t value Pr(&gt;|t|)    

(Intercept)        8.522e+00  6.553e-02 130.041  &lt; 2e-16 ***

Input1            -3.819e-04  3.021e-05 -12.642  &lt; 2e-16 ***

Input2            -2.557e-04  2.518e-05 -10.156  &lt; 2e-16 ***

Input3            -3.202e-02  1.102e-02  -2.906  0.00367 ** 

Input4            -1.268e-01  7.608e-02  -1.666  0.09570 .  

Input1:Input2      1.525e-08  2.521e-09   6.051 1.53e-09 ***


Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 2.487504)
    Null deviance: 18544  on 5959  degrees of freedom
Residual deviance: 14811  on 5954  degrees of freedom
  (1708 observations deleted due to missingness)
AIC: 22353
Number of Fisher Scoring iterations: 2
</code></pre>

<p>From a estimation theory perspective, I understand that ""estimate"" and ""Std. Error"" are the estimates and the standard deviation of the unknown parameters (beta1, beta2,...) of my model. However, there are some things I do not understand:</p>

<p>1) How can I assess how good my fit is from the output of <code>summary()</code>? We could not use only the information of the standard deviation of the parameter estimators to assess the goodness-of-fit. I would expect to have access to the sampling distribution of a given parameter estimator to know the % of estimates within +- 1std, +-0.5std or any +-x*std, for example. Other option would be knowing the theoretical distribution of the parameter estimator, so as to try to calculate its Cramer Rao Lower Bound and compare with the calculated std.</p>

<p>2) What does the t value (or Pr(>|t|) ) have to do with the goodness-of-fit? Since I am not familiar with regression models, I do not know the connection between the student t distribution and the estimation of the model parameters. What does it mean? Is the parameter estimator of the glm model distributed according to the student t pdf (like the sample estimator for small samples of an unknown population)? What conclusions should I take from Pr(>|t|)?</p>

<p>3) Do we have a more general form of assessing the goodness-of-fit, like a measure of the variability of the data my model can capture, maybe a table of critical values for such a measure given a certain significance level?** </p>

<p>4) When fitting a glm model, do we need to specify a significance level? If yes, why such an information is not provided by the summary function?</p>

<p>5) The summary function outputs some measures based on information theory, like AIC: 22353. Can we define an optimal reference value for AIC? What is a good AIC value? My intuition is that we could not do so, like other information theory measures (mutual information, entropym,...)</p>

<p>Thank you for your help!</p>
"
"0.0464572209811883","0.0314853283036575","187199","<p>I am trying to develop a Multivariate linear regression, in R 3.2.2,  using </p>

<blockquote>
  <p>Ad GRP</p>
</blockquote>

<p>and </p>

<blockquote>
  <p>Ad Spending</p>
</blockquote>

<p>to predict Sales.</p>

<p>This is my GRP Data</p>

<pre><code>adGRPAugToDec &lt;- c(2020, 1278, 1195, 1310, 495)
</code></pre>

<p>This is my adstock function , based on this blogpost <a href=""https://analyticsartist.wordpress.com/2014/08/17/marketing-mix-modeling-explained-with-r/"" rel=""nofollow"">MMM</a></p>

<pre><code>    # Define Adstock Function
adstock &lt;- function(x, rate=0){
 return(as.numeric(filter(x=x, filter=rate, method=""recursive"")))
}
</code></pre>

<p>I call the above function like this </p>

<pre><code>ad.adstock &lt;- adstock(adGRPAugToDec,0.5)
</code></pre>

<p>I get this error</p>

<blockquote>
  <p>Error in filter_(.data, .dots = lazyeval::lazy_dots(...)) :<br>
  argument "".data"" is missing, with no default</p>
</blockquote>
"
"NaN","NaN","187555","<p>i am currently setting up a portfolio on the Basis of cointegrating relationships for an assignment at uni. Therefore, I need to restrict my OLS coeffients, so that the total sum of coefficients is equal to 1. The unrestricted OLS estimation is given by:
<code>lm(mran_port[,1] ~ mran_port[,2:draws])</code></p>

<p>where the dependent variable is the corresponding index. The explanatory variables are set up dynamically, so that the number of regressors depends on the value of draws. I already tried a reparamererization, but this does not give sufficient results. Is there a suitable function in R?</p>

<p>I hope someone can help me out.</p>

<p>Kind regards,</p>

<p>harry</p>
"
"0.080466267117873","0.0818012824723818","187839","<p>I am studying t-intervals calculation concept and trying to perform the calculations in R.</p>

<p>My issue is that I am performing the calculation through three different methods and I am getting three different results, so I guess I am doing something wrong.</p>

<p>Here are my calculations:
I am using the R <code>mtcars</code> dataset and estimating a Linear Model (using R function <code>lm</code>) of <code>mtcars$mpg</code> as the outcome and <code>mtcars$wt</code> (car's weight) as the regressor:</p>

<pre><code>data(mtcars)
y &lt;- mtcars$mpg; x &lt;- mtcars$wt; n &lt;- length(y)
se &lt;- sd(y)/sqrt(n) # Standard Error of the Estimation
#now, I calculate the conf interval (ci) using the concept's formula
ci &lt;- mean(x) + c(-1,1)*qt(.975,df=n-1)*se
ci
[1] 1.044304 5.390196
f &lt;- lm(y ~ x)
c &lt;- summary(f)$coefficients
c
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) 37.285126   1.877627 19.857575 8.241799e-19
x           -5.344472   0.559101 -9.559044 1.293959e-10
b0 &lt;- c[1,1] ; b1 &lt;- c[2,1]
yh &lt;- b0 + b1*mean(x)
yh
20.09062
c(yh-ci[1],yh+ci[2])
19.04632 25.48082 #this is my t interval using the concept formula

#Now, the t-interval calculation using R t.test function
t.test(y)
One Sample t-test
data:  y
t = 18.8569, df = 31, p-value &lt; 2.2e-16
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
  17.91768 22.26357
sample estimates:
mean of x 
  20.09062 
#So, you can see I get the same estimation of y (20.09...) but a different t-   interval as that calculated before with the formula

#Finally, the t-interval calculated with R predict formula
predict(f, data.frame(x=mean(x)), interval=""confidence"")
    fit      lwr      upr
1 20.09062 18.99098 21.19027
#and, again, I got the same fit value of y (20.09..) but another different value for the t interval
</code></pre>

<p>So, I wonder if I have an error in the concept or if I have an error using the R functions.</p>
"
"0.0848188929679971","0.0776035104406608","188011","<p>I'm doing machine learning in R. I would like to know how we can create a model object that we can pass to ""predict"" function along with new data so that we obtain predicted values. 
To elaborate, I'm trying to write a new machine learning algorithm in R. Till now I have only used predict function but don't know how to create ""model"" objects to pass to predict function. If we're doing a linear regression, calling lm would create ""lm"" object. If we're doing naiveBayes classifier, and call it from e1071 package, it would create naiveBayes classifier object, which we will pass to predict function. Now, if I'm writing a new algorithm, how do I create an object of that algorithm? And how exactly predict function will process that? What class variables/methods that ""model"" object should have so that it can be processed by ""predict"" function available in R? I know this is a bit open ended question, but I couldn't find any proper documentation. A basic/prototype example in terms of code would be highly appreciated. Though I've been using R, I'm not familiar to classes/objects concept in R. Thank you very much.</p>
"
"0.0929144419623766","0.0787133207591437","188098","<p>I am trying to estimate a model for an event modelled by probability of happening which is a linear function of x (distributed normally) plus an error term, u.</p>

<p>Then I simulate whether the event really happened for each X comparing the probability of it happening against a uniformly distributed random variable.</p>

<p>So, I wrote a little function that simulates this model for a given b0, b1, X (mean and sd.) and error term (mean = 0 and sd.):</p>

<pre><code>SAMPLE_SIZE = 10000

underlying &lt;- function(b0, b1, mean_x, sd_x, sd_u) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean_x, sd_x)
  us &lt;- rnorm(SAMPLE_SIZE, 0.0, sd_u)
  ys &lt;- b0 + (b1 * xs) + us
  ws &lt;- runif(SAMPLE_SIZE) &lt; ys  
  list(ws = ws, ys = ys, xs = xs, us = us)
}
</code></pre>

<p>It neatly returns both the probability of the event taking place in the ys component plus a simulation on the ws component.</p>

<p>I then tested whether I can correctly estimate b0 and b1 using linear regressions. And I got a very weird result.</p>

<p>This is how I simulated the samples and did the regressions:</p>

<pre><code>b1s &lt;- seq(from = 0.0, to = 1.0, length.out = 100)

datasets &lt;- lapply(b1s, FUN = function(x) underlying(0.5, x, 1.0, 0.2, 0.05)) 
regs     &lt;- lapply(datasets,  FUN = function(x) lm(data = x, ws ~ xs))
b0s_hat = sapply(regs, function(x) x$coefficients[[1]])
    b1s_hat = sapply(regs, function(x) x$coefficients[[2]])
</code></pre>

<p>So, for different b1s (and b0 = 0.5) I can plot the estimated b0 and b1 against the real b1:</p>

<pre><code>plot(b1s, b0s_hat)
plot(b1s, b1s_hat)
</code></pre>

<p>And what we get for b1s_hat looks sigmoid-ish like a cumulative distribution function, and b0s_hat looks like a bell curve (like the density function).</p>

<p>I thought I could recover the coefficients using the linear regression. What exactly is smelling weird here?</p>
"
"0.0715255707714427","0.0818012824723818","188112","<p>I am studying logistic regressions and I wonder why are estimators biased when the independent variables have low variance (maybe low variance compared to its mean, but anyway).</p>

<p>I simulate the underlying model as a linear function of a single variable <code>x</code> and I do not include an error term. <code>x</code> is generated from a normal distribution, with mean <code>mx</code> and sd <code>sx</code>.</p>

<p><code>f</code> is a helper to map the probabilities using a logistic function</p>

<p>I use <code>mx = 1.0</code>, and sample <code>sx</code> from a uniform distribution from 0 to 1, so I can estimate the model for different values of <code>sx</code>.</p>

<pre><code>SAMPLE_SIZE = 1000
set.seed(100)

f &lt;- function(v) exp(v) / (1 + exp(v));

sim = function(b0, b1, mx, sx) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean = mx, sd = sx)
  ps &lt;- f(b0 + b1 * xs)
  ys &lt;- rbinom(SAMPLE_SIZE, 1, ps)
  glm(ys ~ xs, family = binomial)
}  


sx &lt;- runif(n = 1000, min = 0.05, max = 1.0)
b0 = 1.5
b0s &lt;- sapply(sx, function(v) {
  sim(b0 = b0, b1 = 1.0, mx = 1.0, sx = v)$coefficients[[1]]
})
</code></pre>

<p>And then I plot the error between the estimated <code>b0</code> coefficient and the real one, for different values of <code>sx</code>:</p>

<pre><code>plot(sx, b0s - b0)
</code></pre>

<p>What I get is that the error gets smaller the greater <code>sx</code> is.</p>

<p>From common linear regressions, we know that the estimators get more precise the larger the variance in the independent variables. But that does not say anything about the biases. </p>

<p>How to interpret this result? Are the estimators really biased in logistic regressions? What's missing here? Is there any problem related to numerical estimates here?</p>

<p><a href=""http://i.stack.imgur.com/aj8md.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aj8md.png"" alt=""Estimation error vs. standard deviation in X""></a></p>
"
"NaN","NaN","188399","<p>I would like to train a model that has a probability (a success rate between 0 and 1) as outcome.</p>

<p>So the data looks like this:</p>

<pre><code>feature1  feature2   success_rate
0.1       0.3        0.55
0.3       0.6        0.45
</code></pre>

<p>I started using <em>xgboost</em> (gradient boosting machine) with:</p>

<pre><code>""objective"" = ""reg:logistic""
""eval_metric"" = ""auc""
</code></pre>

<p>which means I doing a logistic regression using the Area Under the Curve (AUC) as evaluation function to measure the improvement of the model.</p>

<p>But I understand a logistic regression is usually trained with a categorical target (success or failure), not a probability.
Does this matter? and is this the right approach?</p>
"
"0.0379321620905441","0.0385614943639849","188632","<p>I have a multiple linear regression model in <code>R</code> where I shall analyse the rating of a product by demographic variables(<code>age</code>, <code>education</code>, <code>gender</code>, <code>income</code>) and by product variables (<code>price</code> and the different labels (dummy variables)).</p>

<p>My model is:</p>

<pre><code>model &lt;- lm(marketing$rating ~ marketing$age + marketing$education + 
            marketing$gender + marketing$income + marketing$price +
            marketing$wa + marketing$kr + marketing$vo + marketing$ju)
</code></pre>

<p>where <code>wa</code>, <code>kr</code>, <code>vo</code> &amp; <code>ju</code> are the labels and <code>rq</code> is the basis.</p>

<p>Now I am given the following question:</p>

<p>""Test the Hypothesis: the influence of the label <code>ju</code> and <code>vo</code> is identical and the <code>income</code> has no influence""</p>

<p>I would have formulated the null- and alternativ hypothesis: $$H_{0}: \beta_{10} - \beta_{9} = 0 \wedge \beta_{5}=0$$ $$H_{A}:   \beta_{10} - \beta_{9} \neq 0 \vee \beta_{5} \neq 0$$</p>

<p>Then I would have used the <code>linearHypothesis()</code> function of <code>R</code>: </p>

<pre><code>linearHypothesis(model, c(""marketing$ju = marketing$vo"",""marketing$income""))
</code></pre>

<p>I am not sure if this is correct - could you provide me with help please.</p>
"
"0.0599760143904067","0.0609710760849692","189808","<p>I am new to machine learning and currently reading a paper about a ANN modelling in which they have divided their dataset into Training , validation and testing set. I have carried out some minor SVM regression studies of my own using <strong>R (e1071)</strong>; in which I have used only Training Set and testing set but no validation test to create model. I have optimized cost and gamma of my SVR model using inbuilt <strong>tune()</strong> function. </p>

<p>And this is a sample code </p>

<pre><code>library(e1071)
set.seed(3)
data = data.frame(matrix(rnorm(100*5), nrow=100))
train=data[1:70,]
test=data[71:100,]
op=tune(svm,X1 ~ ., data=train,kernel=""radial"",ranges=list(cost=c(0.001,0.01),gamma=c(0.5,0.1)))
fit = op$best.model
summary(fit)
pred=predict(fit,test)
</code></pre>

<p>So is it necessary to use validation set in SVR. if yes how can I programmatically implement validation set in the above code.</p>
"
"0.100359067582572","0.0947366868222571","189822","<p>What difference does centering (or de-meaning) your data make for PCA? I've heard that it makes the maths easier or that it prevents the first PC from being dominated by the variables' means, but I feel like I haven't been able to firmly grasp the concept yet. </p>

<p>For example, the top answer here <a href=""http://stats.stackexchange.com/questions/22329"">How does centering the data get rid of the intercept in regression and PCA?</a> describes how not centering would pull the first PCA through the origin, rather than the main axis of the point cloud. Based on my understanding of how the PC's are obtained from the covariance matrix's eigenvectors, I can't understand why this would happen.</p>

<p>Moreover, my own calculations with and without centering seem to make little sense.</p>

<p>Consider the setosa flowers in the <code>iris</code> dataset in R. I calculated the eigenvectors and eigenvalues of the sample covariance matrix as follows.</p>

<pre><code>data(iris)
df &lt;- iris[iris$Species=='setosa',1:4]
    e &lt;- eigen(cov(df))
    &gt; e
    $values
[1] 0.236455690 0.036918732 0.026796399 0.009033261

$vectors
            [,1]       [,2]       [,3]        [,4]
[1,] -0.66907840  0.5978840  0.4399628 -0.03607712
[2,] -0.73414783 -0.6206734 -0.2746075 -0.01955027
[3,] -0.09654390  0.4900556 -0.8324495 -0.23990129
[4,] -0.06356359  0.1309379 -0.1950675  0.96992969
</code></pre>

<p>If I center the dataset first, I get exactly the same results. This seems quite obvious, since centering does not change the covariance matrix at all. </p>

<pre><code>df.centered &lt;- scale(df,scale=F,center=T)
e.centered&lt;- eigen(cov(df.centered))
e.centered
</code></pre>

<p>The <code>prcomp</code> function results in exactly this eigenvalue-eigenvector combination as well, for both the centered and uncentered dataset. </p>

<pre><code>p&lt;-prcomp(df)
p.centered &lt;- prcomp(df.centered)
Standard deviations:
[1] 0.48626710 0.19214248 0.16369606 0.09504347

Rotation:
                     PC1        PC2        PC3         PC4
Sepal.Length -0.66907840  0.5978840  0.4399628 -0.03607712
Sepal.Width  -0.73414783 -0.6206734 -0.2746075 -0.01955027
Petal.Length -0.09654390  0.4900556 -0.8324495 -0.23990129
Petal.Width  -0.06356359  0.1309379 -0.1950675  0.96992969
</code></pre>

<p>However, the <code>prcomp</code> function has the default option <code>center = TRUE</code>. Disabling this option results in the following PC's for the uncentered data (<code>p.centered</code> remains the same when <code>center</code> is set to false):</p>

<pre><code>p.uncentered &lt;- prcomp(df,center=F)
&gt; p.uncentered
Standard deviations:
[1] 6.32674700 0.22455945 0.16369617 0.09766703

Rotation:
                    PC1         PC2        PC3         PC4
Sepal.Length -0.8010073  0.40303704  0.4410167  0.03811461
Sepal.Width  -0.5498408 -0.78739486 -0.2753323 -0.04331888
Petal.Length -0.2334487  0.46456598 -0.8317440 -0.19463332
Petal.Width  -0.0395488  0.04182015 -0.1946750  0.97917752
</code></pre>

<p>Why is this different from my own eigenvector calculations on the covariance matrix of the uncentered data? Does it have to do with the calculation? I've seen mentioned that <code>prcomp</code> uses something called the SVD method rather than the eigenvalue decomposition to calculate the PC's. The function <code>princomp</code> uses the latter, but its results are identical to <code>prcomp</code>. Does my issue relate to the answer I described at the top of this post?</p>

<p><strong>EDIT:</strong> Issue was cleared up by the helpful @ttnphns. See his comment below, on this question: <a href=""http://stats.stackexchange.com/questions/125937"">What does it mean to compute eigenvectors of a covariance matrix if the data were not centered first?</a> and in this answer: <a href=""http://stats.stackexchange.com/a/22520/3277"">http://stats.stackexchange.com/a/22520/3277</a>. In short: a covariance matrix implicitly involves centering of the data already. PCA uses either SVD or eigendecomposition of the centered data $\bf X$, and the covariance matrix is then equal to ${\bf X'X}/(n-1)$.</p>
"
"0.0889588054368324","0.0904347204435887","189903","<p>I am conducting logistic regression analysis: The data includes 107 observations, dependent variable is a binary one, there is about 5 covariates which are both continuous, binary and multi-categorical variables. I want to use some cut_off points to predict the outcome.</p>

<p>So basicly, I select one cut_off point (based on the requirement that the sensitivity >70% and specificity > 70%). Then I devide my data into train set (85% data points) and test set (15% data points).</p>

<p>I fit the model with 5 covariates on the train set, and use the model to predict the outcome on the test set. I use the glm() function to fit the model, and glm.predict() function to predict on test sets. </p>

<p>Since there is missing data, I create 40 imputed data sets using MICE package in R. The procedure above is repeated over 40 imputed data sets. For each data set, I obtain the mis-classification errors.</p>

<p>So, to get the overall mis-classification errors, I averaged over 40 mis-classification error rates.</p>

<p>My question is: </p>

<p>How to assess the variability of this overall mis-classification errors?</p>

<p>As I am thinking that we can not use the usual formula to calculate the variance for this number, as the mis-classification errors over different imputed data sets might be correlated to each other.</p>

<p>Does any one have a suggestion or reference to do this?
(I am using R).
Thank you for any inputs.</p>
"
"0.13686025610802","0.139130903795187","189933","<p>I am seeking advice on how to effectively eliminate autocorrelation from a linear mixed model. My experimental design and explanation of fixed and random factors can be found here from an earlier question I asked: </p>

<p><a href=""http://stats.stackexchange.com/questions/188929/crossed-fixed-effects-model-specification-including-nesting-and-repeated-measure"">Crossed fixed effects model specification including nesting and repeated measures using glmm in R</a></p>

<p>I have treated day as numeric even though I only have four sampling time points (so I could treat it as a categorical predictor as well). Aside: Although four sample points is very few, I donâ€™t think that this is the root of the problem as this same dataset is giving me this residual autocorrelation issues using a different response variable that has 24 time points.</p>

<p>My issue is that I have tried a number of different autocorrelation structures and canâ€™t seem to achieve the random, non-significant residuals needed to confirm a lack of autocorrelation. I am using the function <code>lme</code> in the R package <code>nlme</code> to deal with autocorrelation. </p>

<p>I have tried the various autocorrelation classes  with variations to form</p>

<p>1) <code>corAR1</code> (autoregressive process of order 1).</p>

<p>2) <code>corARMA</code> (autoregressive moving average process)</p>

<p>3) <code>corCAR1</code> (continuous autoregressive process)</p>

<p>4) <code>corGaus</code> (Gaussian spatial correlation)</p>

<p>With form varying in the following ways with these different autocorrelation classes:</p>

<pre><code>form=~1
form=~1| TankNumb/RecruitID2
form=~Day| TankNumb/RecruitID2
</code></pre>

<p>If we look at a model without the time factor ""Day"" added, the ACF and PACF plots look like this. </p>

<pre><code>lme4_lognormal_notime&lt;-lmer(Arealog~Temperature*Culture+(1|TankNumb/RecruitID2), data=growthSR_noNA)

acf(residuals(lme4_lognormal_notime, retype=""normalized""))
pacf(residuals(lme4_lognormal_notime, retype=""normalized""))
</code></pre>

<p><a href=""http://i.stack.imgur.com/xVdrb.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xVdrb.jpg"" alt=""enter image description here""></a></p>

<p>Also, if I look at the residuals of the model without â€œDayâ€ included, I do not see any strong pattern in the residuals that would make me think there is a temporal autocorrelation problem.</p>

<pre><code>plot(residuals(lme4_lognormal_Ben_notime, retype=""normalized"")~growthSR_noNA$Day)
</code></pre>

<p><a href=""http://i.stack.imgur.com/wU1ZC.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wU1ZC.jpg"" alt=""enter image description here""></a></p>

<p>Now for two different models with autocorrelation structure to hopefully eliminate autocorrelation:</p>

<pre><code>nlme_lognormal_mult_cor&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corAR1(form=~1), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/Oe3et.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Oe3et.jpg"" alt=""enter image description here""></a></p>

<pre><code>nlme_lognormal_mult_cortime&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corAR1(form=~Day|TankNumb/RecruitID2), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/pUgA1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pUgA1.jpg"" alt=""enter image description here""></a></p>

<pre><code>ARMA_nlme_lognormal_mult_cor&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corARMA(form=~1, p=0, q=1), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/6TMeL.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6TMeL.jpg"" alt=""enter image description here""></a></p>

<p>The AIC suggests that the simplest correlation structure is the best. </p>

<pre><code>AIC(nlme_lognormal_mult,nlme_lognormal_mult_cor, nlme_lognormal_mult_cortime,ARMA_nlme_lognormal_Ben_mult_cor)

                               df      AIC
nlme_lognormal_mult              15 1233.997
nlme_lognormal_mult_cor          16 1184.389
nlme_lognormal_mult_cortime      16 1235.997
ARMA_nlme_lognormal_Ben_mult_cor 16 1198.451
</code></pre>

<p>As I mentioned above, I have tried a number of different <code>cor</code> functions (the four listed above) and different <code>form</code> specifications. They all end up with ACF/PCF plots like the last two models with a first lag at below 0.2 in the ACF plot and a PCF plot with the first three lags around 0.10.</p>

<p>I have also read a number of sites describing how to specify corARMA models based on diagnosing the ACF plots and have tried a number of variations of p and q parameters. </p>

<p>Questions: </p>

<ol>
<li>Does anyone have some advice on which type of correlation structure that might elimate this autocorrelation problem based on the patterns in my ACF/PCF plots? Should I be diagnosing based on a model with or without Day included?</li>
</ol>

<p>2.Is there ever an acceptable level of autocorrelation? 
This post (<a href=""http://stats.stackexchange.com/questions/80823/do-autocorrelated-residual-patterns-remain-even-in-models-with-appropriate-corre"">Do autocorrelated residual patterns remain even in models with appropriate correlation structures, &amp; how to select the best models?</a>) states that small amounts of autocorrelation probably won't impact the model coefficients very much. ""The estimate is slightly larger than zero so will have negligible effect on the model fit and hence you might wish to leave it in the model if there is a strong a priori reason to assume residual autocorrelation."" Potentially there is some autocorrelation that is not being caused by temporal autocorrelation, like outliers? Is there a cut-off, for example, autocorrelation below 0.1? I have extremely small 95% confidence intervals, so it doesn't take a lot of autocorrelation in my models to be significantly too much.</p>

<p>Any advice is appreciated! </p>
"
"0.0889588054368324","0.0904347204435887","189983","<p>I have daily data from last 2 years.</p>

<p>I want to do ARIMAX and the regressor component being autoregressive distributed lag of the same variable. Since it has impact, along with dummy variables to account for seasonality in the <code>xreg</code> paratemer in <code>auto.arima</code> function.</p>

<p>The challenge i am facing is predicting my predictor for future. For example, i used daily data for 2 year for model building. For forecasting into future, i also need values of lag variable, which i do not know. If i use 2 lags of daily data in the model, then in order to predict for future i will also need value of those lag variables as well. So to predict $Value$ at time $t$ i will need $Value$ at $t-1$ and $t-2$ which i have from past records. However, if i want to find value at $t+5$ then i will need to find $t+3$ and $t+4$. Not sure how to proceed in this direction. As stated earlier, i am using <code>auto.arima</code> function from <code>forecast</code> package in <code>R</code> . </p>

<p>My ultimate goal is to predict for next 365 days. What i assume to be a solution is that i predict for $t+1$ as it will require $t$ and $t-1$ as lag component which i already have. once done i can use this predicted $t+1$ component to predict for $t+2$ as i will know value of $t+1$ from previous iteration and $t$ from original values. Is it the right approach?</p>
"
"0.026822089039291","0.0272670941574606","190239","<p>I am using the cp argument in the rpart function in R.  I would like to understand exactly how lack-of-fit is calculated for decision trees.  Please provide a simple example if possible.  Thanks.    </p>

<p>After investigating this in greater detail, I've solved part of the question.  In the example dataset I was using, we were building a regression tree.  In this instance, rpart uses the anova method by default.  For the anova method, the overall R-squared must increase by cp at each step.  </p>

<p>Since we have our predicted values and actual values from the training set, we are able to calculate SSR and SST to obtain R Squared (SSR/SST).  We can then compare the R-squared from the previous step to see if it increased by more than the complexity parameter.  </p>

<p>Now, I still do not know how the complexity parameter is used in a classification tree.</p>
"
"0.0599760143904067","0.0609710760849692","190389","<p>I built a conditional logistic regression with the function clogit (package survival) in R and in which I included one categorical independent variable (habitat type) with 15 levels. I noted that the sign of parameter estimates changed between models that were built for each level of the categorical independent variable and a model that included the categorical variable (thus, all levels). Contrary to the model including the categorical variable, the results of models for each level of the categorical variable made sense from a biological standpoint. Does sign changes signify a multicollinearity issue? However, in my case, the values of VIFs for each level of the categorical variable were &lt; 3. Should I group some levels of my categorical variable because I noted the levels that were significant, were often those with few observations ?</p>
"
"0","0.0272670941574606","190476","<p>In R, if one includes external regressors in the function <code>arima</code>, are the regressor lags considered (Box-Jenkins or something similar)?</p>

<p>The following suggests that they are:
<a href=""http://stats.stackexchange.com/questions/121749/definition-of-arima-with-exogenous-regressors-in-r"">Definition of ARIMA with exogenous regressors in R</a>.</p>

<p>The following, as well as many other sources suggest that they are not:
<a href=""http://stats.stackexchange.com/questions/25780/what-is-the-purpose-of-and-how-to-use-the-xreg-argument-when-fitting-arima-model"">What is the purpose of and how to use the xreg argument when fitting ARIMA model in R?</a></p>

<p>Rob Hyndman's <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">explanation</a> is ambiguous to me.</p>
"
"0.026822089039291","0.0272670941574606","190642","<p>I have carried out a model on some long-term data I have looking at bird arrival dates in relation to different weather conditions and have run into some problems when plotting the results. I have plotted the regression line no problem but when I try and plot the 95% confidence intervals they cross. The graph is below.</p>

<p><a href=""http://i.stack.imgur.com/NIkv0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NIkv0.png"" alt=""Arrival date of birds in relation to the North Atlantic Oscillation""></a></p>

<p>So my questions are: Is there any mathematical reason this should happen? </p>

<p>Do you think it could be my code? - I am using a for loop in R with the confint function to input the corresponding values into the equation as obtained from the predicted model. My code for the confidence intervals is below:</p>

<pre><code>#lwr confint 
loopl&lt;-c()
for(i in 1:length(ndat[,1]))
{loopl1&lt;-ci[1,1]+ 
   ci[3,1]*((ndat$nao[i]-0.3014286)/2.301913) +
       ci[2,1]*(ndat$ntemp[i]) +
   (ci[5,1]*((ndat$nao[i]-0.3014286)/2.301913))*(ndat$ntemp[i])
 loopl&lt;-c(loopl,loopl1)
}
ndat$lwr&lt;-loopl

#upr confint
loopu&lt;-c()
for(i in 1:length(ndat[,1]))
{loopu1&lt;-ci[1,2]+ 
   ci[3,2]*((ndat$nao[i]-0.3014286)/2.301913) +
       ci[2,2]*(ndat$ntemp[i]) +
   (ci[5,2]*((ndat$nao[i]-0.3014286)/2.301913))*(ndat$ntemp[i])
 loopu&lt;-c(loopu,loopu1)
}
ndat$upr&lt;-loopu
</code></pre>

<p>Originally, the plot has an interaction term in it that's why there is also ""ntemp"". I didn't include it in the graph because it wasn't necessary here. ndat is a new data frame with a new set of x values.</p>

<p>Any help would be VERY appreciated as I am really confused.</p>

<p>Cheers,</p>

<p>Thomas</p>
"
"0.0657004319817604","0.0667904674542028","191222","<p>I'm using R to fit a linear regression model and then I use this model to predict values but it does not predict very well boundary values. Do you know how to fix it?</p>

<p>ZLFPS is:</p>

<pre><code>ZLFPS&lt;-c(27.06,25.31,24.1,23.34,22.35,21.66,21.23,21.02,20.77,20.11,20.07,19.7,19.64,19.08,18.77,18.44,18.24,18.02,17.61,17.58,16.98,19.43,18.29,17.35,16.57,15.98,15.5,15.33,14.87,14.84,14.46,14.25,14.17,14.09,13.82,13.77,13.76,13.71,13.35,13.34,13.14,13.05,25.11,23.49,22.51,21.53,20.53,19.61,19.17,18.72,18.08,17.95,17.77,17.74,17.7,17.62,17.45,17.17,17.06,16.9,16.68,16.65,16.25,19.49,18.17,17.17,16.35,15.68,15.07,14.53,14.01,13.6,13.18,13.11,12.97,12.96,12.95,12.94,12.9,12.84,12.83,12.79,12.7,12.68,27.41,25.39,23.98,22.71,21.39,20.76,19.74,19.49,19.12,18.67,18.35,18.15,17.84,17.67,17.65,17.48,17.44,17.05,16.72,16.46,16.13,23.07,21.33,20.09,18.96,17.74,17.16,16.43,15.78,15.27,15.06,14.75,14.69,14.69,14.6,14.55,14.53,14.5,14.25,14.23,14.07,14.05,29.89,27.18,25.75,24.23,23.23,21.94,21.32,20.69,20.35,19.62,19.49,19.45,19,18.86,18.82,18.19,18.06,17.93,17.56,17.48,17.11,23.66,21.65,19.99,18.52,17.22,16.29,15.53,14.95,14.32,14.04,13.85,13.82,13.72,13.64,13.5,13.5,13.43,13.39,13.28,13.25,13.21,26.32,24.97,23.27,22.86,21.12,20.74,20.4,19.93,19.71,19.35,19.25,18.99,18.99,18.88,18.84,18.53,18.29,18.27,17.93,17.79,17.34,20.83,19.76,18.62,17.38,16.66,15.79,15.51,15.11,14.84,14.69,14.64,14.55,14.44,14.29,14.23,14.19,14.17,14.03,13.91,13.8,13.58,32.91,30.21,28.17,25.99,24.38,23.23,22.55,20.74,20.35,19.75,19.28,19.15,18.25,18.2,18.12,17.89,17.68,17.33,17.23,17.07,16.78,25.9,23.56,21.39,20.11,18.66,17.3,16.76,16.07,15.52,15.07,14.6,14.29,14.12,13.95,13.89,13.66,13.63,13.42,13.28,13.27,13.13,24.21,22.89,21.17,20.06,19.1,18.44,17.68,17.18,16.74,16.07,15.93,15.5,15.41,15.11,14.84,14.74,14.68,14.37,14.29,14.29,14.27,18.97,17.59,16.05,15.49,14.51,13.91,13.45,12.81,12.6,12,11.98,11.6,11.42,11.33,11.27,11.13,11.12,11.11,10.92,10.87,10.87,28.61,26.4,24.22,23.04,21.8,20.71,20.47,19.76,19.38,19.18,18.55,17.99,17.95,17.74,17.62,17.47,17.25,16.63,16.54,16.39,16.12,21.98,20.32,19.49,18.2,17.1,16.47,15.87,15.37,14.89,14.52,14.37,13.96,13.95,13.72,13.54,13.41,13.39,13.24,13.07,12.96,12.95,27.6,25.68,24.56,23.52,22.41,21.69,20.88,20.35,20.26,19.66,19.19,19.13,19.11,18.89,18.53,18.13,17.67,17.3,17.26,17.26,16.71,19.13,17.76,17.01,16.18,15.43,14.8,14.42,14,13.8,13.67,13.33,13.23,12.86,12.85,12.82,12.75,12.61,12.59,12.59,12.45,12.32)

QPZL&lt;-c(36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16)

ZLDBFSAO&lt;-c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2)    
</code></pre>

<p>My model is:</p>

<pre><code>fit32=lm(log(ZLFPS) ~ poly(QPZL,2,raw=T) + ZLDBFSAO)

results3 &lt;- coef(summary(fit32))

first3&lt;-as.numeric(results3[1])
second3&lt;-as.numeric(results3[2])
third3&lt;-as.numeric(results3[3])
fourth3&lt;-as.numeric(results3[4])
fifth3&lt;-as.numeric(results3[5])

#inverse model used for prediction of FPS
f1 &lt;- function(x) {first3 +second3*x +third3*x^2 + fourth3*1}
</code></pre>

<p>You can see my dataset <a href=""https://docs.google.com/spreadsheets/d/1vlc5c6qO973vgIKZaadL5J5nz5hpFFFSua4P2Qb1Kig/edit?usp=sharing"" rel=""nofollow"">here</a>. This dataset contains the values that I have to predict.  The FPS variation per QP is heterogenous. See dataset. I added a new column.
The fitted dataset is a different one.</p>

<p>To test the model just write <code>exp(f1(selected_QP))</code> where selected QP varies from 16 to 36. See the given dataset for QP values and the FPS value that the model should predict.</p>

<p>You can run the model online <a href=""http://www.r-fiddle.org/#/fiddle?id=E5J5usMi&amp;version=1"" rel=""nofollow"">here</a>.</p>

<p>When I'm using QP values in the middle, let's say between 23 and 32 the model predicts the FPS value pretty well. Otherwise, the prediction has big error value.</p>
"
"0.0599760143904067","0.0609710760849692","191506","<p>My dependent variable has 4 categories, but when I run the multinomial logistic regression using the package <code>nnet</code> with function <code>multinom</code> the results only show 3 categories. </p>

<p>I've tried changing the category numbers from 0,2,3,4 to 1,2,3,4, and also tried using names instead of numbers for the categories but it still wont show all 4 categories in the results. </p>

<p>Also, when I changed the categories to names instead of numbers, the resulting p values for each category drastically changed. Why is this? 
The p values were acquired using these commands</p>

<pre><code>z &lt;- summary(siglm)$coefficients/summary(siglm)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1)) * 2
p
</code></pre>
"
"0.0709645772411954","0.0618359572423054","191611","<p>I'm using the set.seed() function in R to achieve reproducability of my results. I compare different regression methods (e.g. RandomForest, SVM, GAM) by their MSE derived from a cross-validation procedure. To my surprise, I realized that results differ whether I place 'set.seed(123)' at the beginning of my code (and then running the whole script) or whether I place 'set.seed(123)' just before calling each method in the script.</p>

<p>To illustrate pls follow my example below (although the answer by 'Sean Easter' and the example given by 'Cliff AB' below should explain as well): </p>

<pre><code>data(iris)
iris
myf&lt;- Sepal.Length ~ 
 Sepal.Width+
 Petal.Length+
 Petal.Width+
 Species

# required packages
library(sperrorest)
library(randomForest)
library(rpart)

##### Regression Tree
set.seed(123)
ctrl &lt;- rpart.control(cp = 0.001)
fit_rpart &lt;- rpart(myf, data = iris, control = ctrl)

#5-repeated 10-fold CV
mypred.rpart &lt;- function(object, newdata) predict(object, newdata)
eval_ns_rpart &lt;- sperrorest(data = iris, formula = myf, model.fun= 
                            rpart, model.args = list(control = ctrl),
                            pred.fun = mypred.rpart, smp.fun = 
                            partition.cv, smp.args = 
                            list(repetition=1:5, nfold=10))
summary(eval_ns_rpart$error)

##### Random Forest
#set.seed(123) # REMOVE HASH IN 2ND RUN!!!! 
fit_rf &lt;- randomForest(myf, data = iris, ntree=1000)

#5-repeated 10-fold CV
mypred.rf &lt;- function(object, newdata) predict(object, newdata)
eval_ns_rf &lt;- sperrorest(data = iris, formula = myf,
                         model.fun = randomForest,
                         pred.fun = mypred.rf,
                         smp.fun = partition.cv, smp.args= list(repetition=1:5, nfold=10))
summary(eval_ns_rf$error)

#### SUMMARIES Mean Squared Errors(MSE)
tr_MSE_rpart&lt;-(summary(eval_ns_rpart$error)[3,1]) # MSE training error
# 0.08548725

t_MSE_rpart&lt;-(summary(eval_ns_rpart$error)[10,1]) # MSE test error
# 0.1445583

tr_MSE_RF&lt;-(summary(eval_ns_rf$error)[3,1]) # MSE training error
# 0.07241344 # 2nd run: 0.07266605

t_MSE_RF&lt;-(summary(eval_ns_rf$error)[10,1]) # MSE test error
# 0.1403778  # 2nd run: 0.1358957
</code></pre>
"
"0.190183293014139","0.189758278018116","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.0831052033277332","0.0915243346951392","192479","<p>I've been getting very odd factors in more complex models and simplify to this minimal case that behaves oddly vs my intuition.</p>

<p>Suppose we have the generative model, where everything is IID Normal</p>

<p>$A \sim N(0,1)$
$B \sim N(0,1)$</p>

<p>$Y_1 = A + \epsilon$ ; $Y_2 = A + \epsilon$ ; $Y_3 = A + \epsilon$</p>

<p>$X_1 = A + B + \epsilon$ ; $X_2 = A + B + \epsilon $ ; $X_3 = A + B + \epsilon$</p>

<p>Where each $\epsilon$ is independant, and in practice we divide by $Y$ and $X$ by $\sqrt{2}$ and $\sqrt{3}$ respectively to standardize.</p>

<p>The task is then to find the latent factors $A$ and $B$ from observed $X$s and $Y$s, pretending of course that we don't know the coefficients happen to all be 1.0, but <strong>we do know that $Y$ contains no $B$</strong></p>

<p>This should be perfect for SEM/CFA, but when I try it fails to correctly identify factor A by an amount that can be corrected manually (so the solution does exist!).  I'm using lavaan, and can't tell if it is a flaw in the software, the method, or my understanding.</p>

<p>In code:</p>

<pre><code>library(lavaan)
library(data.table)

N = 100000

DT =  data.table(A = rnorm(N), B = rnorm(N))

DT[, Y1 := (A + rnorm(N))/sqrt(2)]
DT[, Y2 := (A + rnorm(N))/sqrt(2)]
DT[, Y3 := (A + rnorm(N))/sqrt(2)]
DT[, X1 := (A + B + rnorm(N))/sqrt(3)]
DT[, X2 := (A + B + rnorm(N))/sqrt(3)]
DT[, X3 := (A + B + rnorm(N))/sqrt(3)]

model = 'FA =~ Y1 + Y2 + Y3 + X1 + X2 + X3
FB =~ X1 + X2 + X3
FA ~~ 0*FB
'

fit = sem(model, data= DT, std.lv = TRUE, std.ov = FALSE)
summary(fit)
</code></pre>

<p>and the summary of our fit is</p>

<pre><code>lavaan (0.5-20) converged normally after  20 iterations

  Number of observations                        100000

  Estimator                                         ML
  Minimum Function Test Statistic                9.770
  Degrees of freedom                                 6
  P-value (Chi-square)                           0.135

Parameter Estimates:

  Information                                 Expected
  Standard Errors                             Standard

Latent Variables:
                   Estimate  Std.Err  Z-value  P(&gt;|z|)
  FA =~                                               
    Y1                0.711    0.003  227.070    0.000
    Y2                0.708    0.003  226.475    0.000
    Y3                0.709    0.003  226.770    0.000
    X1                0.580    0.003  176.083    0.000
    X2                0.577    0.003  175.597    0.000
    X3                0.579    0.003  176.073    0.000
  FB =~                                               
    X1                0.579    0.003  183.034    0.000
    X2                0.580    0.003  183.912    0.000
    X3                0.575    0.003  181.946    0.000

Covariances:
                   Estimate  Std.Err  Z-value  P(&gt;|z|)
  FA ~~                                               
    FB                0.000                           

Variances:
                   Estimate  Std.Err  Z-value  P(&gt;|z|)
    Y1                0.498    0.003  160.623    0.000
    Y2                0.497    0.003  161.192    0.000
    Y3                0.497    0.003  160.910    0.000
    X1                0.333    0.002  141.244    0.000
    X2                0.329    0.002  139.731    0.000
    X3                0.337    0.002  143.257    0.000
    FA                1.000                           
    FB                1.000                           
</code></pre>

<p>So far so good, but if we take the scores for those factors then they are not uncorrelated!  Furthermore, if we regress Y1 on those factors, then Y1 appears to depend on factor B, which we specified is not the case.</p>

<pre><code>DT = cbind(DT, predict(fit))

DT[, cor(FB, FA)]
#[1] 0.2219724

summary(DT[, lm(Y1 ~ FA + FB)])


Call:
lm(formula = Y1 ~ FA + FB)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.67157 -0.39035  0.00245  0.38889  2.32116 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.002491   0.001823  -1.367    0.172    
FA           0.945142   0.002102 449.682   &lt;2e-16 ***
FB          -0.231733   0.002351 -98.586   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.5764 on 99997 degrees of freedom
Multiple R-squared:  0.6691,    Adjusted R-squared:  0.6691 
F-statistic: 1.011e+05 on 2 and 99997 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>There does appear to be a correct solution though, because if we take those regression coefficients and make a new factor A, then orthogonality is achieved and the Ys all only depend on this new factor</p>

<pre><code>DT[, CA := 0.94*FA - 0.235*FB]

DT[, cor(CA,FB)]
#[1] -0.00160991
</code></pre>

<p>What is going on?</p>

<p><strong>EDIT</strong></p>

<p>As far as I can tell, it is fitting the weights correctly, this diagram purports to show the fitted model (<code>semPaths</code>) and the residual variances and loadings all look correct.  So perhaps it is the <code>predict</code> function where the issue lies?</p>

<p><a href=""http://i.stack.imgur.com/wwv3C.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wwv3C.png"" alt=""SEMdiagram""></a></p>
"
"0.0860220579279185","0.0947366868222571","192785","<p><strong>Objective</strong></p>

<p>I have a crossed and implicitly nested design and am trying to validate the correct â€˜maximalâ€™ model (including all linear and pairwise interactions of the variables) for use in <code>lmer()</code>.  I intend to use this as the starting point for some kind of backward stepwise regression, possibly making use of the function <code>mixed()</code> in the  <code>{afex}</code> package.</p>

<p><strong>Experimental design</strong></p>

<p>This a linguistics study.  We have 20 <code>Subjects</code>, each speaking 180 utterances, amounting to 3600 observations in total. Each utterance is initiated via prompting, and an associated Response Time is measured. Log Response Time is the dependent variable. </p>

<p><em>Conditions &amp; Blocks</em></p>

<p>The Response Time for the utterances is affected by 3 <code>Conditions</code> (coded 1 to 3). Each <code>Condition</code> is implemented by prompting the <code>Subject</code> to recite 1 of 4 <code>Blocks</code> of utterances (coded 1 to 12).</p>

<p><em>Words &amp; Tones</em></p>

<p>Each <code>Block</code> brings about its associated <code>Condition</code> via 15-utterance repetition of 3 carefully chosen <code>Words</code>.  There are a total of 12 <code>Words</code> used in the experiment (coded 1 to 12). The <code>Words</code> within each <code>Block</code> can also be categorized by <code>Tone</code> (coded 1 to 2).  There are 6 <code>Words</code> per <code>Tone</code>.  </p>

<p><em>Summary</em></p>

<p>Each of the 20 <code>Subjects</code> utter all 12 <code>Blocks</code> of 15 utterances each.  In doing so, they repeatedly utter all 12 <code>Words</code> (15 utterances per <code>Word</code>), and thereby use both <code>Tones</code> (90 utterances per <code>Tone</code>).</p>

<p>I would like to consider <code>Block</code>, <code>Word</code>, and <code>Subject</code> as random effects, and <code>Condition</code> and <code>Tone</code> as fixed.</p>

<p><strong>Proposed Model</strong></p>

<p>I think the model can be written in the following wayâ€¦</p>

<p><code>RT_log ~ Condition*Tone + (Condition*Tone|Subject) + (Condition|Word) + (Tone|Block)</code></p>

<p><strong>Questions</strong></p>

<p><strong>1.</strong> Is this the 'maximal' model (with linear plus pairwise interactions) appropriate for my experimental design?_ </p>

<p><strong>2.</strong> There is correlation between <code>Block</code> and <code>Condition</code> (there are only 4 possible blocks - out of the total 12 - for each <code>Condition</code>).  There is, similarly, correlation between <code>Word</code> and <code>Tone</code>.  Is it 'okay' to leave this correlation in the model? I don't see a good way of removing it.</p>

<p><strong>3.</strong> How will lme4 handle implicit nesting: I.e., the blocks, which are implicitly nested in the 3 conditions (i.e., only 4 blocks are applicable to each of the 3 conditions, even though the blocks are coded from 1 to 12), and the words, which are implicitly nested within the 2 tones (only 6 words are applicable to each tone, even though words are coded from 1 to 12)?</p>

<p><strong>4.</strong> Some <code>Blocks</code> utilize <code>Words</code> of only a single <code>Tone</code>, whereas other <code>Blocks</code> utilize words of both <code>Tones</code>.  Will that cause problems for the <code>(Tone|Block)</code> term in the model? It will only make sense for certain values of Block.</p>

<p><strong>5.</strong> It has been suggested by some that we might need a ""Subject:Word"" grouping (random effect).  Why might we need this grouping?</p>
"
"NaN","NaN","193419","<p>What is the implication if I don't fix a logistic regression that has complete or quasi separation? can I still read the marginal effects or are they not going to be valid? </p>

<p>My exercise is actually to just find out which independent variables are most predictive of Y. </p>

<p>I read some responses to complete/quasi-separation and I tried using logistf package for R but got this error message ""NA/NaN/Inf in foreign function call logistf"". why does this arises? </p>
"
"NaN","NaN","193506","<p>Excel has a handy function that lets you set the y-intercept of an exponential regression model:</p>

<p><a href=""http://i.stack.imgur.com/V2ZNt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/V2ZNt.png"" alt=""enter image description here""></a></p>

<p>How can the same effect be achieved using R? I'm using the following code:</p>

<pre><code>x &lt;- c(...)
y &lt;- c(...)

A &lt;- structure(list(X=x, Y=y, .Names=c(""X"", ""Y"")))
attach(A)
names(A)
exponential.model &lt;- lm(log(Y) ~ 100 + X)
summary(exponential.model)
values &lt;- seq(0, 100, 1)
Y.exponential2 &lt;- exp(predict(exponential.model, list(X=values)))

plot(x, y2, main=""Graph"", pch=1, xlab=""X"", ylab=""Y"")
lines(values, Y.exponential, lty=1)
</code></pre>

<p>I'm trying to set the y-intercept to 100, but it's not doing that correctly. :(</p>
"
"0.0379321620905441","0.0385614943639849","193609","<p>I would be very appreciated if someone help me. I have F(x) (Cumulative density) values for specific x values which is not equal interval but I have not any sample data to estimate parameters of distribution by using <code>fitdistr()</code> function. I know that data probably have weibull distribution and regression analysis for detecting parameters probably yields wrong results. How can I calculate the weibull parameters for this CDF data by using MLE method in R?</p>

<p>My Data ex</p>

<pre><code>     x       F(x)
 0,363       0,01    
 0,417       0,12    
 0,479       0,38    
 0,550       0,75    
 0,631       1,22    
 0,72        1,74    
 0,83        2,29    
 0,96        2,85    
 1,10        3,41    
 1,26        3,97    
 1,45        4,55    
 1,66        5,15    
 1,91        5,79    
 2,19        6,50    
 2,51        7,26    
 2,88        8,08    
 3,31        8,98    
 3,80        9,96    
 4,37        11,04    
 5,01        12,21    
 5,75        13,49    
 6,61        14,89    
 7,59        16,42    
 8,71        18,09    
 10,00       19,92    
 11,48       21,93    
 13,18       24,18    
 15,14       26,73    
 17,38       29,67    
 19,95       33,09    
 22,91       37,08    
 26,30       41,71    
 30,20       47,01    
 34,67       52,94    
 39,81       59,38    
 45,71       66,12    
 52,48       72,89    
 60,26       79,38    
 69,18       85,28    
 79,43       90,33    
 91,20       94,35    
 104,71      97,29    
 120,23      99,26    
 138,04      99,94    
</code></pre>
"
"0.110590306208719","0.112425109314872","194597","<p>I am doing a meta-regression with metafor package in R. The mixed-effect model for proportion is used to assess the linearity between study performed year and medication prevalence. Here below is my script in R:</p>

<pre><code>model_A &lt;- rma.glmm(xi=A, ni=Sample, measure=""PLO"", mods=~year)
print(model_A)
</code></pre>

<p>And results I got from R are:</p>

<pre><code>Mixed-Effects Model (k = 32; tau^2 estimator: ML)

tau^2 (estimated amount of residual heterogeneity):     1.6349
tau (square root of estimated tau^2 value):             1.2786
I^2 (residual heterogeneity / unaccounted variability): 99.40%
H^2 (unaccounted variability / sampling variability):   168.00

Tests for Residual Heterogeneity: 
Wld(df = 30) = 2221.4535, p-val &lt; .0001
LRT(df = 30) = 3187.7073, p-val &lt; .0001

Test of Moderators (coefficient(s) 2): 
QM(df = 1) = 22.7322, p-val &lt; .0001

Model Results:

          estimate        se     zval    pval      ci.lb      ci.ub
intrcpt  -554.8145  116.4605  -4.7640  &lt;.0001  -783.0728  -326.5561  ***
year        0.2767    0.0580   4.7678  &lt;.0001     0.1630     0.3905  ***

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Followed by this model, I would also like to perform a scatterplot in R. So my script is:</p>

<pre><code>wi &lt;- 0.5/sqrt(dat$vi)
preds &lt;- predict(model_A, transf = transf.ilogit, addx=TRUE)
plot(year, transf.ilogit(dat$yi), cex=wi)
lines(year, preds$pred)
</code></pre>

<p>The plot I got is: 
<a href=""http://i.stack.imgur.com/7Ej3P.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7Ej3P.png"" alt=""enter image description here""></a></p>

<p>Apparently, it doesn't seem right!. So my questions are:</p>

<ol>
<li><p>Did I use the right model with <code>rma.glmm</code>?</p></li>
<li><p>How could I weight individual study (<code>cex=wi</code>?)? How to calculate standard error for individual study?</p></li>
<li><p>How could I fit a right estimated line in scatterplot?</p></li>
</ol>

<p>Many thanks.</p>

<p>Updates:</p>

<p>Followed by Wolfgang's suggestions, I managed to rescale the bubble and get predicted line fitted (the model remains the same):</p>

<p><a href=""http://i.stack.imgur.com/u8N0t.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/u8N0t.png"" alt=""enter image description here""></a></p>

<p>Obviously, the line wasn't straight! Should I change model into polynomial regression? Or is that normal with this graph?</p>

<p>I tried polynomial model like:</p>

<blockquote>
  <p>model1&lt;-rma.glmm(xi=A, ni=Sample, measure=""PLO"", mods=~year+I(year^2))</p>
</blockquote>

<p>The error came with ""Error in print(model1) : 
  error in evaluating the argument 'x' in selecting a method for function 'print': Error: object 'model1' not found""</p>

<p>And I tried another model:</p>

<blockquote>
  <p>model2: model2&lt;-rma.glmm(xi=A, ni=Sample, measure=""PLO"", mods=~year+year^2)</p>
</blockquote>

<p>I got exactly the same result as original model, which has only the year as covariate fitted. I am not sure where the problem is....</p>

<p>Many thanks!</p>

<p>Min</p>
"
"0.0379321620905441","0.0385614943639849","194897","<p>I have a data which includes 8 variables:</p>

<p>1- The first variable contains grades given by a voice expert to participants' voices. It takes any value from 0 to 3 with mode 0 (so zero inflated)</p>

<p>2- The rest of variables are outputs from machine and some of them are highly correlated.</p>

<p>The aim is to find a model of the first variable on the rest 7 variables so that an inexpert person can find the grade of voice by puttingÂ only the outputs from machine in the model.</p>

<p>So far I have found that I can use Tweedie glm. Is this suitable for the aim of this study? or there are better options?</p>

<p>Some variables are highly correlated but there is no preference of one over the others; We don't know which one of them are more appropriate to keep. Is there any function like stepwise in linear regression that can help?</p>

<p>I have softwares R and Stata available.</p>

<p>I hope it is a question within the site criteria!!</p>
"
"0.0929144419623766","0.0944559849109725","195120","<p>I recently ran a beta regression model in R using the <code>betareg</code> package. I am modeling a continuous dependent variable (a fraction out of 1) that is bound between 0 and 1, as a function of a continuous variable that only takes on positive values. Model code, and code to generate residual vs. fitted is here:</p>

<pre><code>fit &lt;- betareg(y ~ x, data=d)
plot(residuals(fit) ~ fitted(fit))
</code></pre>

<p>The residual vs. fitted plot looks like this:
<a href=""http://i.stack.imgur.com/5hVHy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5hVHy.png"" alt=""enter image description here""></a></p>

<p>So like... what is going on here. Is this normal for beta regression, or have I mis-specified my model somehow?</p>

<p>Histogram of dependent variable, <code>y</code>:
<a href=""http://i.stack.imgur.com/bB2wK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bB2wK.png"" alt=""enter image description here""></a></p>

<p>Histogram of independent variable, <code>x</code>:
<a href=""http://i.stack.imgur.com/8T7gq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8T7gq.png"" alt=""enter image description here""></a></p>

<p>output from <code>summary(fit)</code></p>

<pre><code>Call:
betareg(formula = relEM ~ mat, data = d1)

Standardized weighted residuals 2:
    Min      1Q  Median      3Q     Max 
-2.0716 -0.3940 -0.1730  0.4468  2.0633 

Coefficients (mean model with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.863468   0.062820  13.745   &lt;2e-16 ***
mat         -0.053734   0.005667  -9.482   &lt;2e-16 ***

Phi coefficients (precision model with identity link):
      Estimate Std. Error z value Pr(&gt;|z|)    
(phi) 0.261182   0.005735   45.54   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Type of estimator: ML (maximum likelihood)
Log-likelihood: 1.156e+04 on 3 Df
Pseudo R-squared: 0.03853
Number of iterations: 14 (BFGS) + 1 (Fisher scoring) 
</code></pre>
"
"0.119952028780813","0.121942152169938","195359","<p>I have a set of complex survey data with sampling weights. I am using the <code>svyglm()</code> function from the <code>survey</code> package in R to describe the relationship between 2 variables in a GLM. I am using the quasipoisson family because both variables are over-dispersed. </p>

<p>The GLM output is as follows:</p>

<pre><code>hlsereg &lt;- svyglm(formula = HLSEPALLACRESFIX ~ HLSE_ACRE, sbdiv, family = quasipoisson)

Survey design:
svydesign(id = ~1, weights = ~spwgtdividedby3, data = sportsbind)

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.489465   0.414979  13.228   &lt;2e-16 ***
HLSE_ACRE   -0.002744   0.001118  -2.454   0.0144 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 2.601914e+15)

Number of Fisher Scoring iterations: 12
</code></pre>

<p>I have used the <code>predict()</code> and <code>lines()</code> function to plot this model output:</p>

<pre><code>acreaxis &lt;- seq(0,2000,.1)
hlse = predict(hlsereg, list(HLSE_ACRE = acreaxis))
    plot(jitter(sportsbind$HLSE_ACRE,  amount = 2.5), jitter(sportsbind$HLSEPALLACRESFIX),pch = 16,  xlab = ""Acres"", ylab = ""Price per person per acre"",  xlim = c(0, 350), ylim = c(0,35), col=alpha(""red"",.35), font = 2, font.lab = 2)
    lines(acreaxis, hlse, lwd=4, col = ""red"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/3EUZ6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3EUZ6.png"" alt=""enter image description here""></a></p>

<p>This plots a line given by the regression output of an intercept at 5.5 and a very slow negative slope of -.003, but I'm uncertain if this is a correct representation of the line.</p>

<p>I have found others using the <code>predict(..., type = ""response"")</code> option, which is shown in various plots of quasipoisson models, including the one found by @Glen_b at <a href=""http://stats.stackexchange.com/a/177926/45582"">this question</a> and for <a href=""http://stats.stackexchange.com/questions/38201/problems-plotting-glm-data-of-binomial-proportional-data?rq=1"">binomial GLMs here</a>. The <code>predict.glm()</code> help page notes for the <code>type</code> argument that: ""The default is on the scale of the linear predictors; the alternative ""response"" is on the scale of the response variable."" I just don't understand what that means.  The ""response"" type yields a very different prediction line, which is curved and at a much higher value (note the scale of the y-axis, with an intercept at ~250):</p>

<pre><code>hlse = predict(hlsereg, list(HLSE_ACRE = acreaxis), type = ""response"")
plot(jitter(sportsbind$HLSE_ACRE,  amount = 2.5), jitter(sportsbind$HLSEPALLACRESFIX),pch = 16,  xlab = ""Acres"", ylab = ""Price per person per acre"",  xlim = c(0, 350), ylim = c(0,400), col=alpha(""red""),     font = 2, font.lab = 2)
lines(acreaxis, hlse, lwd=4, col = ""black"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/jnY9T.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jnY9T.png"" alt=""enter image description here""></a></p>

<p>I have also tried to run a GLM using the negative binomial distribution, but despite inputting the quasipoisson coefficient values for starting values, the model can't find valid coefficients (I have purged all zeros from the data):</p>

<pre><code> hlsereg.nb &lt;- glm.nb(HLSEPALLACRESFIX~HLSE_ACRE,data = model.frame(sbdiv.scaledweights), start = c(5.45, -.003))
Error: no valid set of coefficients has been found: please supply starting values
In addition: Warning message:
glm.fit: fitted rates numerically 0 occurred 
</code></pre>

<p>My questions:</p>

<p>1) What is the most appropriate illustration of the GLM output from a quasipoisson family?<br>
2) If the negative binomial is more appropriate to describe this relationship, why can't it find a coefficient? If I figure out how to get it to find a coefficient, how would I visualize that output?</p>
"
"0.0402331335589365","0.0545341883149212","195478","<p>Resampling is usually used to find the best tuning parameters for a model. However, for some models, such as linear regression model, there is no tuning parameters. In this case, what can we get from resampling on them?</p>

<p>In particular, in R caret package, you can train a linear regression model by using cross validation control function. In this case, how is the coefficient estimated? On the whole training sample? If so, what extra information can we get from applying CV on linear regression models?</p>

<p>Thank you.</p>
"
"0.0608267804924532","0.072141950116023","196829","<p>I have a dataset with 15 binary covariates and a continuous response variable bounded between 0 and 1. The binary variables represent correct or incorrect answers on a short test and the response variable is a measure of the same test takers performance on a related but more advanced and reliable test. I would like to select the best variables and weights to predict the score on the more advanced test. What would be the best way of doing this?</p>

<p>PS. I'm not a statistician but a computer scientist with only basic statistics and machine learning in my portfolio.</p>

<p>(Side note: One idea I had was to use some kind of logistic L1 or L2 regularized regression, however, glmnet does not seem to accept non-binary response variables when fitting a logistic model, which I guess is reasonable for normal use. The built-in glm function does accept a (0,1)-bounded response but does not perform regularization. If this approach seems reasonable, any tips on suitable packages or would I have to implement it myself? Other ideas I had was using ""normal"" regularized regression, or perhaps Principal Component Regression, however, I have tried both these and they give very different results and neither perform very well.)</p>
"
"0.093190562755245","0.102024124270123","196901","<p>I'm trying to figure out how to find the marginal effect of an interaction term from a restricted cubic spline in a non-linear model.  The post <a href=""http://stats.stackexchange.com/questions/134526/nonlinear-effect-in-an-interaction-term"">Nonlinear effect in an interaction term</a> is a good start on modeling the nonlinear effects and how to get plots, but does not address finding the marginal effect.  </p>

<p>The package <a href=""http://maartenbuis.nl/software/postrcspline.html"" rel=""nofollow"">postrcspline</a> in <code>STATA</code> has a function <a href=""http://repec.org/bocode/m/mfxrcspline.html"" rel=""nofollow"">mfxrcspline</a> which ""displays the marginal effect of a restricted cubic spline,""
 which is exactly what I am after. (See Figure 1 below)  </p>

<p>R does not seem to offer this feature as conveniently ,so I'm trying to figure out how to get these same results.</p>

<p>As I understand it, suppose I have a multi-variable regression with restricted cubic splines and an interaction:</p>

<p>$$y = \beta_{0} + \beta_{1}x1 + \beta_{2} \mathcal{f}(x2) + \beta_{3} \mathcal{f}(x2) \cdot x1 + \epsilon$$</p>

<p>where $\mathcal{f}(x2)$ is a spline of the time-series (year)</p>

<p>The marginal effect of $\frac{\partial y}{\partial x1}$ is:</p>

<p>$$\frac{\partial y}{\partial x1} = \beta_{1} + \beta_{3} \mathcal{f}(x2)$$</p>

<p>where $\beta_{3}$ is the coefficient on the spline and $ \mathcal{f}(x2)$ is a design matrix for each year in the regression that causes the slope to change for each $y$.  </p>

<p>To say in words, I would like to find the marginal effect of $y$ for each year $x2$ in the spline given $\beta_{3}$.  </p>

<p>In other words, it shows for each value of the spline variable how much the expected value of your explained variable changes for a unit change in the spline variable. It is the first derivative of the curve.</p>

<p>This appears to be simple matrix multiplication to plot the marginal effect, but I'm not sure how to statistically do this.  </p>

<p>Here is a plot to illustrate what I'm after:</p>

<p><strong>Figure 1:</strong> The left plot shows the results of the regression using a restricted cubic spline and the right provides the marginal effect--note the changes on the y-axis.
<a href=""http://i.stack.imgur.com/uqcX4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uqcX4.png"" alt=""Figure 1""></a></p>

<hr>

<p>Here is an R example to demonstrate the nonlinear effect from the regression (left plot in Figure 1):</p>

<pre><code>library(rms)
set.seed(5)
# Fit a complex model and approximate it with a simple one
x1 &lt;- runif(200)
x2 &lt;- runif(200)
y &lt;- x1 + x2 + rnorm(200)
f &lt;- ols(y ~ x1 + rcs(x2,4)  + rcs(x2,4)*x1)
ddist &lt;- datadist(x1,x2)
options(datadist='ddist')
plot(Predict(f))
</code></pre>

<p><a href=""http://i.stack.imgur.com/DAuXS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DAuXS.png"" alt=""enter image description here""></a></p>
"
"0.0763370036711974","0.0776035104406608","197001","<p>I am trying to follow the procedure offered by <a href=""http://www.jstor.org/stable/2082979?seq=1#page_scan_tab_contents"" rel=""nofollow"">Beck and Katz 1995</a> in a way that I also have a TSCS data with $T=100$ (time dimension) and $N=12$ (unit dimension). My data is not balanced, which means that for some time periods, not all units have observations. </p>

<p>I am using R, and I found a <code>pcse</code> package that does what I need. It calculates panel corrected standard errors which accounts for contemporaneous correlation of errors across units and unit level heteroskedasity of errors. However, the steps I have to take to calculate panel robust standard errors for this type of regression start with the need to correct for serial correlation of errors, if I understand it well. Particularly, that is what is recommended in <code>pcse</code> package documentation:</p>

<p><a href=""http://i.stack.imgur.com/GNowz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/GNowz.png"" alt=""enter image description here""></a></p>

<p>So, I am lost trying to understand what I need to do. My options how I see them:</p>

<ol>
<li>Run simple OLS regression on my pooled panel data. </li>
<li><p>Test for serial correlation of error term using Durbinâ€“Watson test and examining ACF/PACF. In most cases, I will have AR(1) in errors. </p>

<ul>
<li>Either compute clustered standard errors - it should account for the fact that errors should be clustered on the unit variable. After this step, I would get robust standard errors, but I cannot use it in pcse estimation - I don't need the VCV of errors as an input for the <code>pcse</code> function, but the OLS <code>lm</code> object itself.</li>
<li>Or use Cochraneâ€“Orcutt transformation first, and then use transformed  model as an input for pcse estimation. I started doing it, but realized that after CO transformation, error term became serially independent, but had the kurtosis of 20 (normality assumption fails).</li>
</ul></li>
</ol>

<p>So, my options are not so suitable. How do you think I should approach this situation?</p>
"
"0.018966081045272","0.0385614943639849","197477","<p>I have a very simple basic question but I am unable to get my head wrap around.</p>

<p>I have a model Y = slope1*variable1 + slope2*variable2 + Intercept.</p>

<p>I used ""lm"" in R to get slope1, slope2 and Intercept.</p>

<p>In this case, variable1 is my main effect and I want to remove the effect of variable2. The goal is to see if there is any association between Y controlled for variable 2 (regressed Y) and variable 1.</p>

<p>In order to plot these : (1) Should I subtract observed_Y-slope2*variable2-Intercept-Residuals and whatever is remaining (I call this regressed Y) is actually the value that is due to variable 1 or
(2) Can I use model_fitted_dot_values from R and plot that as a function of variable1 and claim that model_fitted_dot_values are the regressed values of my dependent variable?</p>

<p>Any help is greatly appreciated.</p>

<p>Thanks</p>

<p>Regards</p>
"
"NaN","NaN","197745","<p>I'm searching for a best subset selection algorithm for ridge regression in R. There is a wide range of algoritms for an ordinary least squares fit. There also exists a function like <code>stepAIC</code> for the ordinary least squares fit. </p>

<p>Does anybody have experience with ridge regression in R?</p>
"
"0.080466267117873","0.0818012824723818","198268","<p>I'm am trying to predict disease states in a medical setting where I have three subject groups (1,2,3). I have cross-validated a multinomial logistic regression model using the following</p>

<pre><code>cvfit=cv.glmnet(Xtrain, ytrain, family=""multinomial"", type.multinomial = ""grouped"", parallel = TRUE, standardize=TRUE)
</code></pre>

<p>where Xtrain is a 42x20 matrix with 42 observations and 20 predictors.</p>

<p>If I run the following to get the coefficients of the model</p>

<pre><code>coef(cvfit)
</code></pre>

<p>I get the following output</p>

<pre><code>$`1`
21 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)  2.519025
V1           2.955347
V2           .       
V3           .       
V4          -3.508274
V5           .       
V6           .       
V7           .       
V8           .       
V9           .       
V10          .       
V11          .       
V12          .       
V13          .       
V14          .       
V15          .       
V16          .       
V17          .       
V18          .       
V19          .       
V20         -2.108070

$`2`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)  1.5460376
V1          -5.2882709
V2           .        
V3           .        
V4           0.4144632
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          1.4674672

$`3`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept) -4.0650622
V1           2.3329236
V2           .        
V3           .        
V4           3.0938106
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          0.6406032
</code></pre>

<p>I would like to be able to say something concerning the risk of being in one group compared to another based on increments in the predictors with non-zero coefficients, however, I cannot seem to find any information as to which class the cvglmnet() function uses as base in order to calculate the risks. </p>

<p>Does anyone know this, or have an idea on how to interpret the results for use in a model?</p>

<p><strong>EDIT:</strong></p>

<p>I realize now that I may have overlooked a crucial detail. In ""The Elements of Statistical Learning: Data Mining, Inference, and Prediction"" by Hastie, T et al (2009), it is stated on page 657 that a multiclass logistic model can be described as</p>

<p>$P(Y=k|X=x) = \frac{\exp{(\beta_{k0}+x^{T}\beta_{k})}}{\sum_{l=1}^{K}\exp{(\beta_{l0}+x^{T}\beta_{l})}}$</p>

<p>where I can see that the denominator is just a normalization factor. I guess this means that I can interpret the obtained coefficients above directly for each subject group. Or is this wrongly interpreted?</p>
"
"0.0758643241810882","0.0674826151369737","199688","<p>I am looking for R code or package to fit some monotonic regression function to 2D (resp. 3D) data, with additional constraints that are ""fixed values"". I found the SCAM package for fitting monotonic splines. However, I do not know how to add my ""fixed values"" additional constraints.</p>

<p>The covariates are $\{(x_1^{(i)},x_2^{(i)})\}_{i=1}^n \in \mathbb{R}^2$ (resp. $(x_1^{(i)},x_2^{(i)},x_3^{(i)})\}_{i=1}^n \in \mathbb{R}^3$). </p>

<p>The responses are $\{y^{(i)}\}_{i=1}^n \in \mathbb{R}$. </p>

<p>I also have:</p>

<ul>
<li>one data point $X^{min}=(x_1^{min},x_2^{min})$ (resp. $(x_1^{min},x_2^{min},x_3^{min})$) with response $y^{min}$, </li>
<li>one data point $X^{max}=(x_1^{max},x_2^{max})$ (resp. $(x_1^{max},x_2^{max},x_3^{max})$)
with response $y^{max}$.</li>
</ul>

<p>As the notations suggest, I know that these two data points represent the (unique) min and the (unique) max of the data set, in the sense of the response $y$.</p>

<p>I want to fit a smooth function $f$ to these data, so that:</p>

<ul>
<li>$f$ is monotonic,</li>
<li>$f$ has its unique min at $X^{min}$ with value $y^{min}$ and its unique max at $X^{max}$ with value $y^{max}$,</li>
<li>$grad f (X^{min}) = grad f (X^{max}) = 0$</li>
</ul>

<p>In other words, I want to fix the values of the smoothing function and of its derivatives at two points.</p>

<p>Thank you for your help!</p>
"
"0.080466267117873","0.0727122510865616","199978","<p>I've been building a logistic regression model (using the ""glm"" method in caret). The training dataset is extremely imbalanced (99% of the observations in the majority class), so I've been trying to optimize the probability threshold during the resampling process using the train function from the caret package as described in this example of a svm model: <a href=""http://topepo.github.io/caret/custom_models.html#Illustration5"" rel=""nofollow"">Illustrative Example 5: Optimizing probability thresholds for class imbalances.</a></p>

<p>The idea is to get the classification parameters for different values of the probability thershold, like this:</p>

<pre><code>threshold   ROC    Sens   Spec   Dist   ROC SD  Sens SD  Spec SD  Dist SD
 0.0100     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.0616     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.1132     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.1647     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
  ...        ...                  ...                      ...      ...
</code></pre>

<p>I noticed that the 'glm' method in caret uses 0.5 as the probability cutoff value as can be seen in the predict function of the model:</p>

<pre><code>code_glm &lt;- getModelInfo(""glm"", regex = FALSE)[[1]]
code_glm$predict
    function(modelFit, newdata, submodels = NULL) {
                    if(!is.data.frame(newdata)) newdata &lt;- as.data.frame(newdata)
                    if(modelFit$problemType == ""Classification"") {
                      probs &lt;-  predict(modelFit, newdata, type = ""response"")
                      out &lt;- ifelse(probs &lt; .5,
                                    modelFit$obsLevel[1],
                                    modelFit$obsLevel[2])
                } else {
                  out &lt;- predict(modelFit, newdata, type = ""response"")
                }
                out
              }
</code></pre>

<p>Any ideas about how to pass a grid of probability cutoff values to the predict function shown above to get the optime cutoff value?</p>

<p>I've been trying to adapt the code from the <a href=""http://topepo.github.io/caret/custom_models.html#Illustration5"" rel=""nofollow"">example shown in the caret website</a>, but I haven't been able to make it work. I think I'm finding difficult to understand how caret uses the model's interfaces... </p>

<p>Any help to make this work would be much appreciated... Thanks in advance.</p>
"
"0.134110445196455","0.130882051955811","200155","<p>Just writing with a question about fixed effects when you have panel data with</p>

<ul>
<li>More than two time periods</li>
<li>Clusters, as in individual students within schools</li>
<li>An intervention given to some of the clusters mid-way through panel</li>
</ul>

<p>In particular, I'm trying to understand the diff-in-diff model used in a particular paper, ""<em>The Effects of Targeted Recruitment and Comprehensive Supports for Low-Income High Achievers at Elite Universities: Evidence from Texas Flagships</em>,"" by Rodney Andrews, Scott Imberman, and Michael Lovenheim.
You can find a copy of their paper here: 
<a href=""https://www.msu.edu/~imberman/LOS-CS%20-%209-4-15.pdf"" rel=""nofollow"">https://www.msu.edu/~imberman/LOS-CS%20-%209-4-15.pdf</a></p>

<p>Their regression equation is at the very bottom of page 17 (which is what I'm curious about).</p>

<p>Here's a short, super simplified summary, for the sake of my question, which is really about diff-in-diff and fixed effects, rather than the particulars of the paper itself:</p>

<p>The authors have a bunch of high schools in Texas. Each of these schools has many students. The schools and students are observed for a few years. Then some of the schools receive an intervention, meant to increase students' college-going and eventually their earnings as adults. Pretend for this example it's just a college scholarship program.* We observe the students in all these schools as they complete high school (or don't), enter college (or don't), and hopefully earn adult incomes. </p>

<p>The diff-in-diff part is comparing the change in adult earnings across cohorts of students that went to schools that received the intervention, compared to the trend among student cohorts that didn't attend the intervention high schools. </p>

<p>I've created some pretend data that tries to mimic the authors' real data in greatly simplified form. You can find that here, along with my (probably wrong) R script:
<a href=""https://drive.google.com/folderview?id=0B6Sk_VEqK32Gb1M4bVQxYVIzOTQ&amp;usp=sharing"" rel=""nofollow"">https://drive.google.com/folderview?id=0B6Sk_VEqK32Gb1M4bVQxYVIzOTQ&amp;usp=sharing</a></p>

<p>(You'll need to paste the cells into excel, which I used to create the values, since google docs doesn't have the same formulas. Apologies for the inconvenience.)</p>

<p>It's got 8 variables</p>

<ul>
<li>""id"": a row index variable</li>
<li>""student"": indexes students within schools, across cohorts. So the index goes from 1 to 20 within each school. But there are only 5 students per cohort, or high school graduating class, because there are 4 time periods. (not sure if that was smart).</li>
<li>""random_uniform"": just a uniform random variable between 0.01 and 0.99. I just used this to create the next variable.</li>
<li>""test_score"": a covariate, student test score. All are normally distributed with a standard deviation of 4, and a mean that's specific to the school and graduating cohort. For interest, I made it so that some schools started with lower overall means, but each school's mean score improved a little over time (about 10 points). All the scores are around 40-60. </li>
<li>""school"": a factor variable that indicates the school. There are 4 schools.</li>
<li>""treat_indicator"": a factor variable that is 0 before the intervention, and 1 after the intervention at the schools that receive the intervention (schools 1 and 2).</li>
<li>""time_period"": a factor variable that denotes the graduating class cohort. There are 4. </li>
<li>""adult_earnings"": a numeric variable that's a function of the students' high school test score (""test_score""), plus a bunch of noise. For the kids that received the ""college scholarship"" intervention--kids in the latter 2 cohorts at schools 1 &amp; 2--I've also added an additional earnings bump between 1,000 and 2,000 dollars, to simulate a treatment effect. </li>
</ul>

<p><strong>So my question is, how do I find the true effect of the intervention, if I want to use both high school graduating cohort (time) and school (cluster) fixed effects?</strong> </p>

<p>My R script is in that shared folder, but I'm not sure it's correct. The regression equation I gave R was</p>

<pre><code>fixedreg &lt;- lm(adult_earnings ~ treat_indicator + test_score + school + time_period, 
                     data=mydata)
</code></pre>

<p>Does that model the time and cluster fixed effects correctly, and create an unbiased coefficient on the ""treat_indicator"" variable?</p>

<p>Any insight would be much appreciated. Thanks!</p>

<hr>

<p>*Or read the actual paper and laugh at my ridiculous attempt to simplify all this.</p>
"
"0.0892693083195917","0.0983129061176287","200703","<p>I'm using matched pairs logistic regression (1-1 matched case-control; Hosmer and Lemeshow 2000) to model differences between vegetation selected at nest sites vs. paired random sites. To do this, I created a data frame that contained the difference in vegetation measurements between nest and random sites (so nest minus random) and used R to fit a logistic regression model, using a vector of all 1's as the 'Response' and a no-intercept model.</p>

<p>Here's the data frame (I only include 1 of the covariates, grass density, for the example):</p>

<pre><code>nest&lt;-structure(list(VerGR = c(1.380952381, 1.952380953, 2.666666667, 
-3.809523809, 2.428571428, 2.142857143, 0.142857143, 2.095238095, 
1.952380952, 3.333333334, 3.190476191, -2.857142857, 2.857142858, 
-1.666666667, 0.523809524, 4.761904762, 0.571428571, 2.238095238, 
-2.809523809, 0.857142857, 1.523809524, -2.476190476, -0.428571428, 
-5.190476191, 4.142857143, 2.857142858, -2.476190476, 4.095238096, 
1.428571428, 1.714285714, -2.80952381, 3.142857143, 2.809523809, 
7.238095238, 2.523809523, 2.333333333, -0.095238096, -0.095238096, 
-0.142857143, 4.047619048, 4.761904759, -1.285714285, -1.190476191, 
2.523809524, -2.095238095, -2, 4.761904761, 8.952380952, 1.095238096, 
5.666666666, -0.714285714, 0, 2.809523809, -0.238095239, 3.666666667, 
0.904761905, -4.952380952, -3.666666667, 2, -0.619047619, 4.523809524, 
1.523809524, 4.619047619, 6.142857143, 3.19047619, -2.190476191, 
-1.666666667, 2.714285714, -1.285714286, 2.857142857, 2.761904762, 
2.809523809, -7.142857139, -5.952380949, -1.19047619, 1.523809524, 
-0.38095238, 5.571428571, 5.238095239, 2.047619048, 7.857142857, 
0.61904761, 2.523809524, -1.190476191), Response = c(1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L)), .Names = c(""VerGR"", ""Response""), class = ""data.frame"", row.names = c(NA, 
-84L))
</code></pre>

<p>And the no-intercept logistic regression models I am running:</p>

<pre><code>grass.mod &lt;- glm(Response ~ VerGR - 1, data=nest, family=""binomial"")
grass2.mod &lt;- glm(Response ~ VerGR + I(VerGR^2) - 1, data=nest, family=""binomial"")
</code></pre>

<p>For the most part the models run fine, and give the same parameter estimates as models implemented using the 'clogit' function from the survival R package. The data set for the clogit models is slightly different, with Responses = 1 (nest) or = 0 (random point), and includes a column called 'PairID' to indicate nest-random pairs. Here's what the clogit models look like:</p>

<pre><code>library(survival)
grass.mod.clog &lt;- clogit(Response ~ VerGR + strata(PairID), data=full)
grass2.mod.clog &lt;- clogit(Response ~ VerGR + I(VerGR^2) + strata(PairID), data=full)
</code></pre>

<p>But when I run the glm's, I get these 2 warnings if using a quadratic term:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>I'm able to satisfy the first warning if I use more iterations in the glm formula, but I'm not sure what is happening with the second warning. I would be glad to use the 'clogit' function (which works with quadratic terms), but I'm unsure how to create prediction plots to visually display the data when going that route. Any suggestions?</p>

<p>Thanks,
Jay</p>
"
"0.026822089039291","0.0272670941574606","200794","<p>I have a problem with outputting the terms for a logistic regression model in R. For a given list of independent values, say list l of terms {w,y,z} to determine dependent variable {x}, I want to find out what the biggest regressor is when we pair two terms together. I want to be able to group multiple independent variables together and say ""when a record has this combination of values, then they have a very strong chance of predicting X"". I tried to just add the interactions when calling the glm function like glm(x~y + w + z + w:z + y:z + y:w, data = l). But the results come out very hard to explain, because of how they are measured between themselves and not just measured against the mean. Does anyone know a way to do this?</p>
"
"0.0464572209811883","0.0472279924554862","201893","<p>I'm using the function <code>randomForest</code> in R's <code>randomForest</code> package to do a regression. However, when I'm trying to include an interaction term in the following codes:</p>

<pre><code>library(MASS)
library(randomForest)
Boston_f &lt;- within(Boston, factor(rad))
mdl &lt;- randomForest(lstat ~ rad * . , data = Boston_f)
</code></pre>

<p>The result <code>mdl$term</code> does include interaction, but if I peek into the trees that <code>mdl</code> is using, </p>

<pre><code>getTree(mdl, 1, T)
</code></pre>

<p>I cannot find any split variable using interaction term.</p>

<p>Does anyone know how to include interaction term using <code>randomForest</code> or other function?</p>
"
"0.0709645772411954","0.072141950116023","202264","<p>I am doing a comparison between mlogit in R and statsmodels in python and have had trouble getting them to produce the same result. I'm wondering if the difference is a result of libraries or I am specifying something incorrectly. Any help would be appreciated.</p>

<p>I am using the ""TravelMode"" dataset to test the two.
In R:
</p>

<pre><code>&gt; library(""mlogit"")
&gt; library(""AER"")
&gt; data(""TravelMode"", package=""AER"")
&gt; write.csv(TravelMode, ""travelmode.csv"")
&gt; TM &lt;- mlogit.data(TravelMode, choice = ""choice"", shape = ""long"", 
                    chid.var = ""individual"", alt.var = ""mode"", drop.index = TRUE)
&gt; TMlogit = mlogit(mFormula(choice ~ vcost), TM)
&gt; summary(TMlogit)
Call:
mlogit(formula = mFormula(choice ~ vcost), data = TM, method = ""nr"", 
    print.level = 0)

Frequencies of alternatives:
    air   train     bus     car 
0.27619 0.30000 0.14286 0.28095 

nr method
4 iterations, 0h:0m:0s 
g'(-H)^-1g = 0.000482 #'
successive function values within tolerance limits 

Coefficients :
                    Estimate Std. Error t-value  Pr(&gt;|t|)    
train:(intercept) -0.3885180  0.2622157 -1.4817 0.1384272    
bus:(intercept)   -1.3712065  0.3599380 -3.8096 0.0001392 ***
car:(intercept)   -0.8711172  0.3979705 -2.1889 0.0286042 *  
vcost             -0.0138883  0.0055318 -2.5106 0.0120514 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -280.54
McFadden R^2:  0.011351 
Likelihood ratio test : chisq = 6.4418 (p.value = 0.011147)
</code></pre>

<p>In statsmodels:
</p>

<pre><code>&gt; import pandas as pd
&gt; import statsmodels.formula.api as smf
&gt; TM = pd.read_csv('travelmode.csv')
&gt; TM = pd.concat([TM, pd.get_dummies(TM['mode'])], axis=1)
&gt; TMlogit = smf.mnlogit('choice ~ train + bus + car + vcost -1', TM)
&gt; TMlogit_fit = TMlogit.fit()
Optimization terminated successfully.
         Current function value: 0.550273
         Iterations 6
&gt; TMlogit_fit.summary()
&lt;class 'statsmodels.iolib.summary.Summary'&gt;
""""""
                          MNLogit Regression Results                          
==============================================================================
Dep. Variable:                      y   No. Observations:                  840
Model:                        MNLogit   Df Residuals:                      836
Method:                           MLE   Df Model:                            3
Date:                Thu, 17 Mar 2016   Pseudo R-squ.:                 0.02145
Time:                        15:04:48   Log-Likelihood:                -462.23
converged:                       True   LL-Null:                       -472.36
                                        LLR p-value:                 0.0001497
=================================================================================
y=choice[yes]       coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
---------------------------------------------------------------------------------
train            -0.3249      0.172     -1.891      0.059        -0.662     0.012
bus              -1.4468      0.205     -7.070      0.000        -1.848    -1.046
car              -0.7247      0.157     -4.603      0.000        -1.033    -0.416
vcost            -0.0105      0.002     -6.282      0.000        -0.014    -0.007
=================================================================================
""""""
</code></pre>

<p>I would think the values of the coefficients would be closer to each other when comparing between the two models. Any help would be appreciated.</p>
"
"0.0379321620905441","0.0385614943639849","202326","<p>I have a huge dataset with over 4000 companies and I have estimated a liquidity measure for these each 4000 companies. But liquidity is highly persistent and exihibits auto-correlation. In order to mitigate this autocorrelation problem each of the liquidity measure estimated for the company has to be trasformed by AR(2) process i.e. residuals of autoregressive model are used instead of actual values.
But when I estiamate the AR(2) with following code in r</p>

<pre><code>AR&lt;- data.frame(dfAR1, apply(dfAR1, 2, function(x) arima(x, order = c(2,0,0),optim.method=""Nelder-Mead"")$res))
</code></pre>

<p>I receive warning </p>

<pre><code> arima(x, order = c(2, 0, 0), optim.method = ""Nelder-Mead"") :
  possible convergence problem: optim gave code = 10
</code></pre>

<p>When I looked up in the manual it says: 
""10
indicates degeneracy of the Nelderâ€“Mead simplex""
I don't understand how bad this warning is for my estimations and how can I fix it.
I would really appreciate your help in this regard.</p>
"
"0.0547503599848004","0.0445269783028019","203359","<p>Consider the following heteroscedastic model:
$$y_i = f(x_i, \beta) + g(x_i, \theta)\varepsilon_i, i = 1, \ldots, n, \tag{1}$$
where $f(\cdot, \beta)$ is the regression function and $g(\cdot, \theta)$
is the variance function. For simplicity, assume the errors $\{\varepsilon_i\}$ are i.i.d. with mean $0$ and variance $\sigma^2$.</p>

<p>Regarding model $(1)$, I understand (but I am not quite sure) that the <code>gls</code> function in <code>nlme</code> package can be used (at least when $f$ is linear) to implement the iteratively reweighted least squares algorithm (Carroll, Ruppert, <em>Transformation and Weighting in Regression</em>, pp. 69). When I read the manual of <code>nlme</code>, it looks to me that <code>gls</code> function restricts the forms of $g$ to a very small class of functional forms. For example, given observations $\{(y_i, x_{i1}, x_{i2}): i = 1, \ldots, n\}$, is it
possible to use <code>gls</code> to fit the following special case of $(1)$:
$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \sqrt{\theta_0 + \theta_1 x_{i1}^2 + \theta_2 x_{i2}^2}\varepsilon_i, i = 1, \ldots, n$$
, where $\theta_0 &gt; 0, \theta_1 \geq 0, \theta_2 \geq 0$? If yes, how should I specify my own square-root variance functional form in <code>gls</code>? If
no, are there any other available R packages to implement IRLS algorithm?</p>
"
"0.0402331335589365","0.0545341883149212","203417","<p>Since my original question was to R-code-specific I'm trying to rewrite it:</p>

<p>I want to make a regression where my dependent variable <code>y</code> should follow a log-normal-distribution influenced by the explanatory variable <code>x</code> where the mean and variance changes across the observation.</p>

<p>Since log-normal doesn't belong to the exponential-family I can't try a glm.
Then I found gamlss which should exactly do the trick.</p>

<p>I was looking for some paper explaining the theory a little deeper - especially in my case of log-normal and where all the parameters of the distribution are  functions of the explanatory variables.</p>

<p>First of all I would like to know if there is a formula like the one below for an ordinary linear regression to calculate <code>y</code> after fitting:</p>

<p><a href=""http://i.stack.imgur.com/5Xqte.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5Xqte.png"" alt=""enter image description here""></a></p>

<p>But my biggest problem is, that I have no idea on how to handle that the moments of <code>y</code> change across the observation of <code>x</code>.
So let's say I have the following:</p>

<p><a href=""http://i.stack.imgur.com/dv61k.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dv61k.png"" alt=""enter image description here""></a></p>

<p>I'm trying to achieve the regression via R and its <code>gamlss</code>-package.</p>

<p>There I start with <code>gamlss(y~x,familiy=LOGNO())</code> and then one has the possibility the make <code>sigma</code> depending on <code>x</code> via <code>sigma.formula=~</code> but no option for <code>mu</code>. So is it even possible in general?</p>

<p>Here is my code:</p>

<pre><code>library(gamlss)

y&lt;-c(1495418, 1684470, 1997120, 1901727, 2070008, 2213829, 2364602, 2333710, 2491570, 2540110, 2620947, 2761075, 2943475, 2854544)
x&lt;-c(3932300, 4119100, 4354400, 4483752, 4585303, 4803234, 4989701, 5177605, 5380031, 5494672, 5606376, 5783627, 6015992, 6171564)

fm&lt;-gamlss(y~x,familiy=LOGNO())
summary(fm)
fitted(fm)
residuals(fm)
y-fitted(fm) #How come this aren't the residuals?

fitted(fm,""mu"")
fitted(fm,""sigma"")
</code></pre>
"
"0.110761366336029","0.112599007499728","203454","<p>Although I have visited this site several times, this is the first time I make a question, so be kind if it is not in a appropriate form.</p>

<p>My problem is part statistical and part R. I am trying to build a Cox PH model in order to make prediction of unemployment. I have a big dataset, N=32538 with covariates p=37 . I split this sample in 3 parts according to Hastie &amp; Tibshirani, train =50%, test=25% and validation = 25%. So, I now have a training set of N=16270 cases. I would like to reduce the number of predictors, but from what I know and have read, it is not wise to do any kind of stepwise elimination. Therefore, I am trying to perform a penalized cox regression, especially with LASSO, using the R package 'penalized'. </p>

<pre><code>library( penalized )
</code></pre>

<p>However, it seems that it cannot run...I am not sure why, but I suppose that either my laptop is not very powerful, or that the respective functions are not very efficient for such a bog dataset. </p>

<pre><code>optL1( Surv ( time, status ) ~ . , minlambda=5, fold=3, data=mydata )
optL1( Surv ( time, status ) ~ . , minlambda=5,maxlamda=15, fold=3, data=mydata )
</code></pre>

<p>As you can see, I specify minlambda in the first case and both min and max lambda in the second. If I leave it unspecified, it just crushes my whole OS. Now, my pc runs veeeeeeery slow, and after 3 hours ( the most I left it running ), although it seems still running, nothing at all was produced. Those familiar with this function, know that while it is running, it produces in the console ""what is going on"". That is , for every lambda that it checks, it shows it in the consore along with the according cvl( log-lik ). </p>

<p>In some cases, but not always,  it produces the well-known irritating message of memory error....</p>

<pre><code> Error: cannot allocate vector of size xxx Mb
</code></pre>

<p>My details :</p>

<pre><code>session(info)
R version 3.2.4 Revised  ( 2016-03-16 r70336)
Platform: x86_64-w64-mingw32/x64 ( 64 bit)
Running under: Windows &gt;= 8 x64 ( build 9200)
</code></pre>

<p>For now, I tried to run the function in subsets of the full dataset, and I ""managed"""" to make it run until  N=8000( the half sample).</p>

<p><strong>Question</strong> 1:</p>

<p>Do you now if I am doing something wrong in running the specific function, or it is an unsolved problem and I have to find another way to proceed ?  </p>

<p><strong>Question 2</strong></p>

<p>Do you know if there are any other packages in R, that can accommodate more efficient the penalized cox regression, and also be capable of making predictions ?</p>

<p>Many thanks!!
Giannis</p>

<p><strong>EDIT</strong></p>

<p>actually, as you can see, I used the classic formula for regression. Meaning, I used the 'dot' in order to include all the predictors in the model. Moreover, as you maybe have guessed, I have many categorical predictors. Do you think that it is better to add the variables all by name in the model, and specify the factors with :</p>

<pre><code>factor(var1) 
</code></pre>

<p>????</p>

<p>Because by reading the vignette( penalized) , I realized that they use only continuous predictors, leaving out of the model the categorical ones! </p>
"
"0.13686025610802","0.139130903795187","203785","<p>I'm using the <code>tgp</code> package in R for fully Bayesian Gaussian Process Regression, and it's great! I'm currently performing regression for experimental data coming from turbomachinery testing, and I'm using the <code>bgp</code> function. This function uses a GP prior with either a <code>linear</code> mean or a <code>constant</code> mean (respectively, option <code>meanfn=""linear""</code> or <code>meanfn=""constant""</code>, which is the default). Note that <code>tgp</code> allows the use of treed Gaussian priors, but for now I'm staying simple, so I'm using the <code>bgp</code> function which doesn't use regression trees, just ordinary Gaussian Processes.</p>

<p>I would like my posterior predictive mean to go to zero away from the training set data, for physical reasons. How can I impose that? I was thinking to set the prior over $\beta_0$ to a Normal distribution centered at 0 and with an extremely small variance, but I'm not sure how to do that. From <code>help(btgp)</code></p>

<pre><code>bprior Linear (beta) prior, default is ""bflat""; alternates include ""b0"" hierarchical Normal
prior, ""bmle"" empirical Bayes Normal prior, ""b0not"" Bayesian treed LMstyle
prior from Chipman et al. (same as ""b0"" but without tau2), ""bmzt"" a independent
Normal prior (mean zero) with inverse-gamma variance (tau2), and
""bmznot"" is the same as ""bmznot"" without tau2. The default ""bflat"" gives
an â€œimproperâ€ prior which can perform badly when the signal-to-noise ratio is
low. In these cases the â€œproperâ€ hierarchical specification ""b0"" or independent
""bmzt"" or ""bmznot"" priors may perform better
</code></pre>

<p>Default is the improper prior <code>""bflat""</code>, which is not what I want. If I use the <code>""b0""</code> hierarchical Normal prior, I guess I cannot set the mean and the variance because they should become additional hyperparameters to be determined in the Bayesian paradigm. Thus, I may go for <code>""bmzt""</code>, the independent Normal prior with zero mean. However, with this prior I cannot set the variance, which is again an hyperparameter. Basically, I want my prior mean function to be zero, so that away from the data, also the posterior predictive mean will be zero. Is there a way to achieve that?</p>

<p>EDIT: nobody wants to have a try? :) As my actual case is quite complicated, I wrote a small test case which illustrates the main problem, with the help of the <code>tgp</code> package author. NOTE: unless you have an optimized version of R, you may want to set <code>BTE = c(1000,10000,2)</code> in the call to <code>bgp</code>, or you may have to wait for a very long time to get an answer.</p>

<pre><code># clear the workspace
rm(list=ls())
gc()
graphics.off()

# set seed for reproducibility
set.seed(825)

# load required packages
library(tgp)
library(ggplot2)

# simulated data
x &lt;- seq(-1,1,len=100)
eps &lt;- rnorm(n=100,mean=0,sd=0.5)
y &lt;- -5*x^2+eps
ymean &lt;- mean(y)

# prediction points
xpred &lt;- seq(-20,20,len=100)

# fit GP
GPModel &lt;- bgp(X=x,Z=y,XX=xpred,meanfn = ""constant"", bprior=""bmzt"", 
BTE = c(2000,52000,2), tau2.p=c(1,10000), tau2.lam=""fixed"")    
ypred &lt;- GPModel$ZZ.mean 

# plots
ymean_vector &lt;- rep(ymean,100)
df &lt;- data.frame(x,y,xpred,ypred,ymean_vector)
p &lt;- ggplot(data=df)
p &lt;- p + geom_point(aes(x=x,y=y)) + 
    geom_line(aes(x=xpred,y=ypred),col=""blue"") +
    geom_line(aes(x=xpred,y=ymean_vector),col=""red"") +
    geom_line(aes(x=xpred,y=GPModel$ZZ.q1), col=""green"") + 
        geom_line(aes(x=xpred,y=GPModel$ZZ.q2), col=""green"")
p
</code></pre>

<p>The resulting plot is</p>

<p><a href=""http://i.stack.imgur.com/VjePX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VjePX.png"" alt=""enter image description here""></a></p>

<p>The mean response is the red line: the blue line is the GP posterior predictive mean, and the green lines give the 90% credible interval.Thus, outside the training data range, the data mean is indeed included in the 90% credible interval, but I would like the predictive mean to converge to it...I think that if I could find a way to set the standard deviation of the prior for $\beta_0$ to some  extremely small value, I would achieve what I want, but I don't know how to do it.</p>

<p>EDIT2: I can use either a multiplicative (separable) squared exponential kernel
or an additive squared exponential kernel.</p>

<pre><code>sep_Gaussian_Kernel &lt;- function(x,y,sigma,l) {
    prod(sigma*exp(-0.5*(abs(x-y)/l)^2))
}    

add_Gaussian_Kernel &lt;- function(x,y,sigma,l) {
    sum(sigma*exp(-0.5*(abs(x-y)/l)^2))/length(x)
} 
</code></pre>
"
"0.0900306369383777","0.0985646681332268","203816","<p>I am trying to duplicate the results from <code>sklearn</code> logistic regression library using <code>glmnet</code> package in R.</p>

<p>From the <code>sklearn</code> logistic regression <a href=""http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"" rel=""nofollow"">documentation</a>, it is trying to minimize the cost function under l2 penalty
$$\min_{w,c} \frac12 w^Tw + C\sum_{i=1}^N \log(\exp(-y_i(X_i^Tw+c)) + 1)$$</p>

<p>From the <a href=""https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.html#log"" rel=""nofollow"">vignettes</a> of <code>glmnet</code>, its implementation minimizes a slightly different cost function
$$\min_{\beta, \beta_0} -\left[\frac1N \sum_{i=1}^N y_i(\beta_0+x_i^T\beta)-\log(1+e^{(\beta_0+x_i^T\beta)})\right] + \lambda[(\alpha-1)||\beta||_2^2/2+\alpha||\beta||_1]$$</p>

<p>With some tweak in the second equation, and by setting $\alpha=0$, $$\lambda\min_{\beta, \beta_0} \frac1{N\lambda} \sum_{i=1}^N \left[-y_i(\beta_0+x_i^T\beta)+\log(1+e^{(\beta_0+x_i^T\beta)})\right] + ||\beta||_2^2/2$$</p>

<p>which differs from <code>sklearn</code> cost function only by a factor of $\lambda$ if set $\frac1{N\lambda}=C$, so I was expecting the same coefficient estimation from the two packages. But they are different. I am using the dataset from UCLA idre <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">tutorial</a>, predicting <code>admit</code> based on <code>gre</code>, <code>gpa</code> and <code>rank</code>. There are 400 observations, so with $C=1$, $\lambda = 0.0025$.</p>

<pre><code>#python sklearn
df = pd.read_csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
y, X = dmatrices('admit ~ gre + gpa + C(rank)', df, return_type = 'dataframe')
X.head()
&gt;  Intercept  C(rank)[T.2]  C(rank)[T.3]  C(rank)[T.4]  gre   gpa
0          1             0             1             0  380  3.61
1          1             0             1             0  660  3.67
2          1             0             0             0  800  4.00
3          1             0             0             1  640  3.19
4          1             0             0             1  520  2.93

model = LogisticRegression(fit_intercept = False, C = 1)
mdl = model.fit(X, y)
model.coef_
&gt; array([[-1.35417783, -0.71628751, -1.26038726, -1.49762706,  0.00169198,
     0.13992661]]) 
# corresponding to predictors [Intercept, rank_2, rank_3, rank_4, gre, gpa]


&gt; # R glmnet
&gt; df = fread(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; X = as.matrix(model.matrix(admit~gre+gpa+as.factor(rank), data=df))[,2:6]
&gt; y = df[, admit]
&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)      -3.984226893
gre               0.002216795
gpa               0.772048342
as.factor(rank)2 -0.530731081
as.factor(rank)3 -1.164306231
as.factor(rank)4 -1.354160642
</code></pre>

<p>The <code>R</code> output is somehow close to logistic regression without regularization, as can be seen <a href=""http://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels"">here</a>. Am I missing something or doing something obviously wrong?</p>

<p>Update: I also tried to use <code>LiblineaR</code> package in <code>R</code> to conduct the same process, and yet got another different set of estimates (<code>liblinear</code> is also the solver in <code>sklearn</code>):</p>

<pre><code>&gt; fit = LiblineaR(X, y, type = 0, cost = 1)
&gt; print(fit)
$TypeDetail
    [1] ""L2-regularized logistic regression primal (L2R_LR)""
    $Type
[1] 0
$W
            gre          gpa as.factor(rank)2 as.factor(rank)3 as.factor(rank)4         Bias
[1,] 0.00113215 7.321421e-06     5.354841e-07     1.353818e-06      9.59564e-07 2.395513e-06
</code></pre>

<p>Update 2: turning off standardization in <code>glmnet</code> gives:</p>

<pre><code>&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0, standardize = F)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)      -2.8180677693
gre               0.0034434192
gpa               0.0001882333
as.factor(rank)2  0.0001268816
as.factor(rank)3 -0.0002259491
as.factor(rank)4 -0.0002028832
</code></pre>
"
"0.026822089039291","0.0272670941574606","203877","<p>I have a model of returns Yt = Bt.Xt with 4 independent variables and I am trying to estimate the time-varying factor loadings. If Y &amp; Xs are defined as GARCH models, is it possible to estimated the time-varying factor loadings as a function of the covariance matrix obtained with a MGARCH model? A model with one regressor is explained in <a href=""http://onlinelibrary.wiley.com/doi/10.1111/1468-5957.00209/abstract"" rel=""nofollow"">Link</a>, however I have difficulties to understand its implementation for a model with 2+ regressors. </p>

<p>Your comments/suggestions would be greatly appreciated.</p>
"
"0.053644178078582","0.0545341883149212","203904","<p>I am new to glms and have picked up the following <a href=""http://www.planta.cn/forum/files_planta/glm_2002_crc_213.pdf"" rel=""nofollow"">text</a>, I am trying to do the exercises and I'm a little stuck on exercises 4.5 question 4.1. The question states that a possible model for the data is a poisson distribution with parameter $\lambda_i = i^\theta$ where $i= (1,2,\dots,20)$ is the time index. I want to fit a poisson regression in R using the log link function, such that:
$$
g(\lambda_i)=\log(\lambda_i) = \beta_1 + \beta_2 \log i
$$
In R, I've done the following:</p>

<pre><code>y&lt;-c(1,6,16,23,27,39,31,30,43,51,63,70,88,97,91,104,110,113,149,159)
x&lt;-1:20
</code></pre>

<p>I'm confused about the glm function, I'm pretty sure I should be fitting:</p>

<pre><code>n1&lt;-glm( y~log(x), family = poisson (link = log) )
plot(log(x),y)
</code></pre>

<p>What I'm finding hard to understand is when plotting the regression line, we should be plotting:</p>

<p>$$
\lambda_i =\exp ( \beta_1 + \beta_2 x_i)
$$
So we should have:</p>

<pre><code>plot(log(x),y)
lines(log(x), exp(n1$fit))
</code></pre>

<p>which doesn't give a decent looking result, although</p>

<pre><code>lines(log(x),n1$fit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/BrbNL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/BrbNL.png"" alt=""enter image description here""></a></p>

<p>seems to be the right way to go, but doesn't make intuitive sense to me, aren't the fit values giving the linear part of the model??</p>
"
"0.053644178078582","0.0545341883149212","203929","<p>I have a dataset with the same dependent and independent variables as those for a logistic regression model whose equation has been published in the literature. How do I go about testing whether that equation fits well with my data, since their model was obviously fitted with a different dataset?</p>

<p>In other words I want to know if their model can be generalisable to a different sample/population.</p>

<p>I want to do this in R and all the searches I have done seem to only discuss how to fit a model with my data using the glm() function. I can fit a new model with my data and will therefore get different coefficients to those published, how do I then compare and contrast the two?</p>
"
"0.053644178078582","0.0545341883149212","204057","<p>I have a very large number of R-squared statistics to collect, basically I'm collecting a monthly R-squared series for a period of around 25 years for a large number of variables where I'm interested in how well each of the variables is explained by the mean of the others in the bunch. There are thousands of variables under examination.</p>

<p>I am currently simply doing it like this:</p>

<pre><code>fit &lt;- lm(df[,i] ~ me, na.action=na.omit);
rsqPeriod &lt;- summary(fit)$r.squared;
</code></pre>

<p>where <code>df[,i]</code> is the individual one-month series and <code>me</code> is the respective series of means of the others. But as there are thousands of variables and a few hundred months, this takes a very long time to calculate. In total, we're talking of several days on a high-end Xeon. </p>

<p>Now the question is, is there a considerably more efficient way of calculating the R-squared values? I do not need any other results from the regressions, and I am wondering if there is e.g. very much unnecessary calculation in the summary function, since I don't need any of the other values.</p>
"
"0.0464572209811883","0.0472279924554862","204119","<p>There is a package in R called <code>pwr</code>. This is useful to make power analysis when designing the sampling of a project. here are few examples: </p>

<pre><code>library(pwr)
pwr.anova.test(k = 4, f = 0.5, sig.level = 0.05, power = .9)
pwr.2p.test(h = 0.5, sig.level =0.05, power = .9)
pwr.f2.test(u = 4, f2 = .5, sig.level = 0.05, power = .8)
</code></pre>

<p>However, is it possible to run a power analysis for a spline regression (or a generalized additive model (GAM))? I want to know how may organisms I would have to sample to detect an effect of selection, that is a shift in morphology of the beak of birds of only 0.5Â mm, given that my sig.level = 0.05 and that I have 4 species. </p>

<p>Also, Iâ€™m recapturing birds in a population each year since 2003. Is there a power calculation to estimate how many birds should I sample to get a probability of recapture of 25%? Iâ€™m running a recapture model in Bayesian statistics, so there is not a function in the package <code>pwr</code> that can do this. </p>
"
"0.0379321620905441","0.0385614943639849","204440","<p>I'm using the <code>auto.arima</code> function in R's <code>forecast</code> package to build an ARIMA model with external regressors. I have a non-seasonal monthly stationary time-series dataset as shown below:</p>

<pre><code>&gt; dim(tsdata)
[1] 95  4
&gt; head(tsdata)
                    y         x1         x2          x3
2007-02-01  0.0532113 -0.7547812 -1.1156320  1.15193457
2007-03-01 -0.4461565  0.5104070  1.2489777 -1.19172591
2007-04-01 -1.4087036  2.0866994  0.2835917  0.15941672
2007-05-01 -0.4960451 -1.9455242 -2.6847517 -0.06603252
2007-06-01  0.8025322 -2.9295067 -0.6049654  0.34332637
2007-07-01 -0.8053754 -0.2385492 -1.7850528 -1.29843072
</code></pre>

<p>I can use <code>auto.arima(tsdata[,1], xreg=tsdata[,2:4])</code> to fit a model with <code>x1</code>, <code>x2</code>, and <code>x3</code> as regressors. My question is, is there a way to model the interaction between external regressions?</p>
"
"0.0599760143904067","0.0609710760849692","204751","<p>Let the dichotomous observation for the $i$ th cluster be $y_{i}=(y_{i1},...,y_{im})'$ and $p_{ij}$ be the probability of win/success, which defined as
$$p_{ij}= Pr(y_{ij}=1|v_{i})=\frac{\exp(z_{ij}'\beta)}{1+\exp(z_{ij}'\beta+v_{i})}$$
where $i=1,...,n; j=1,...,m$ and $z_{ij}=(z_{ij1},...,z_{ijp})$ is a set of independent variables corresponding to $y_{ij}$ and $\beta= (\beta_{1},...,\beta{p})$ is the regression parameters. Here, $v_{i}$ is the random effect for the $i$th cluster having $N(0, \sigma_{v}^2)$.</p>

<p>Now I have tried to generate simulated data from this model using following code in R.</p>

<pre><code>logit &lt;- function(x) log(x/(1-x))
invlogit &lt;- function(x) exp(x)/(1+exp(x))

## simulate cases
simcase&lt;-function(N,p) rbinom(N,1,p)
## simulation scenario
pi_ij &lt;- 0.05
n &lt;- 2; # number of clusters
Nmin &lt;- 2; # min number of subjects per cluster
Nmax &lt;- 10; # max number of subjects per cluster
set.seed(123)
v_i &lt;- rnorm(n,0,1) ; # random effects
logit1 &lt;- logit(pi_ij)+v_i
pind &lt;- invlogit(logit1)
Nsim &lt;- sample(Nmin:Nmax,n,replace=TRUE); # number of cases per cluster

data &lt;- data.frame(ev=do.call(c,mapply(simcase,Nsim,pind,SIMPLIFY=TRUE)),
    id=do.call(c,mapply(function(i,N) rep(i,N),1:n,Nsim)))
    complications$id&lt;-factor(complications$id)
data
</code></pre>

<p>Are these code correct? Can anybody help? 
Thanks in advance.</p>
"
"0.080466267117873","0.0818012824723818","205419","<p>I am using implementations of the Levenberg-Marquardt algorithm for non-linear least squares regression based on MINPACK-1 utilizing either the R function nlsLM() from minpack.lm or an implementation in C using the <a href=""http://www.physics.wisc.edu/~craigm/idl/cmpfit.html"" rel=""nofollow"">mpfit</a> library. </p>

<p>I have observed that while I come up with comparable parameter estimates in both cases, the calculated uncertainty is drastically different in the two approaches. Below you find the code of the two approaches:</p>

<p><strong>In R:</strong></p>

<pre><code>library(minpack.lm)

x &lt;- seq(1,9)
y &lt;- c(2.93,1.79,1.03,0.749,0.562,0.218,0.068,0.155,0.03)
table &lt;- data.frame(x,y)

fit &lt;- nlsLM(y ~ N * exp( -rate * x ), data = table, start = list(rate = 0.1, N = 1)

summary(fit)
</code></pre>

<p>which returns the estimates and errors:</p>

<pre><code>rate    0.47825 +/- 0.02117
   N    4.69723 +/- 0.18950
</code></pre>

<p><strong>In C:</strong></p>

<pre><code>#include &lt;math.h&gt;
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;

#include ""cmpfit-1.2/mpfit.h""

// Data structure to keep my vectors
typedef struct my_data {

  double *x;
  double *y;
  int n;

} my_data;

// defining the function that is to be fitted
int my_exp(int m, int n, double *p, double *dev, double **deriv, my_data *data)
{

  int i;
  double *x, *y, f;

  x = data-&gt;x;
  y = data-&gt;y;

  for (i = 0; i &lt; m; i++) {

    f = p[0] * exp(-p[1]*x[i]);
    dev[i] = y[i] - f;

  }

  return 0;
}


int main(void)
{

  // define test data
  my_data data;
  data.x = (double[]){1,2,3,4,5,6,7,8,9};
  data.y = (double[]){2.93,1.79,1.03,0.749,0.562,0.218,0.068,0.155,0.032};
  data.n = 9;

  // start values
  double p[2] = {1.0, 0.1};

  mp_result result;
  memset(&amp;result, 0, sizeof(result));

  double perr[2];
  result.xerror = perr;

  int ret;

  ret = mpfit((mp_func)my_exp, data.n, 2, p, 0,0, &amp;data, &amp;result);


  printf(""rate:   %f +/- %f\n"", p[1], perr[1]); 
  printf(""   N:   %f +/- %f\n"", p[0], perr[0]);

  return 0;
}
</code></pre>

<p>which returns:</p>

<pre><code>rate:   0.479721 +/- 0.270632
   N:   4.707439 +/- 2.422189
</code></pre>

<p>As you can see, the errors returned by mpfit are about 12x higher then from R. I also implemented the same thing using <a href=""http://www.gnu.org/software/gsl/"" rel=""nofollow"">GSL</a> which produces results identical to mpfit. Unfortunately I am not very comfortable with the R code base, so it would be great if anyone has any inside into what nlsLM in R does differently then mpfit and GSL when calculating the errors.</p>

<p>Thanks!</p>
"
"0.0379321620905441","0.0192807471819925","205918","<p>My data follows a sigmoidal function of the form<br>
$$y=asym/(1+e^{(xmid-x)/scale)})$$<br>
I have taken the function from the SSLogis function in R.<br>
My supervisor and I think that there is a second variable that influences the asymptote and scale of the function.  </p>

<p>So I'm stuck with the questions:<br>
1. Is it at all possible to do a multiple regression based on this function?<br>
If yes, 2. Is it possible to transform the sigmoidal to a linear function and use it in a multiple linear regression?<br>
Or 3. Is there a way to do a multiple non-linear regression?</p>

<p>Any help is really welcome.</p>
"
"0.0599760143904067","0.0487768608679754","206454","<p>I wish to compute <code>MSE</code> of my models.  Say my data was generated from the following model:</p>

<p>$y_i=f(x_i)+e_i$ </p>

<p>where $e_i$ is some noise around the true relationship $f(x)$.  I estimated the function $f(x)$ as $f\hat(x)$, and now I'd like to compute the MSE.  </p>

<p>My professor often writes MSE as the following:</p>

<p>$1/n \sum_{i=1}^n (f(x_i)-f\hat(x_i))^2$</p>

<p>Let's say I know $f(x)$, the true function and I'm using it for simulation.</p>

<p>My question is, when I compute MSE, do I use my observations $y_i$?  or do I use the true function without the noise $f(x_i)$?  Because, the professor writes the true function in the formula above, but this means that computing MSE involves taking the difference between the functions at the $x_i$ value of each observation, without actually using the value $y_i$ of that observation?</p>

<p>This formulation seems much more intuitive to me:  </p>

<p>$1/n \sum_{i=1}^n (y_i-f\hat(x_i))^2$  , because this will actually capture the observations.</p>

<p>Which formulation is correct?  And when might one use one over the other?  Feel free to use linear regression as an example, since that will allow easy illustration.  </p>
"
"0.026822089039291","0.0272670941574606","206735","<p>I've created an example table (just in order to create a function) with:</p>

<pre><code>ex&lt;-data.frame(b=c(rep('A',50),rep('B',30), rep('C',20)), 
fl=round(runif(100,0,1),0),r=runif(100,0,0.5))
ex2&lt;-cbind(ex,model.matrix(~b-1,ex))
lineal&lt;-ex2$bB+ex2$bA*ex$fl+ex$fl
ex$clase&lt;-round(1/(1+exp(-lineal)),0)
</code></pre>

<p>Then I run a logistic regression model (MASS library)</p>

<pre><code>fm&lt;-as.formula(clase~b+fl+r)
modT&lt;-glm(clase~1, family=binomial, data = ex)
modT&lt;-stepAIC(modT, scope = fm, family=binomial, data =ex, k = 4)
summary(modT)
</code></pre>

<p>As you can see coefficients are not significant, but I've created the class using them. So I don't understand why this is happening.</p>

<p><a href=""http://i.stack.imgur.com/yR4jV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yR4jV.png"" alt=""enter image description here""></a></p>
"
"0.053644178078582","0.0545341883149212","207177","<p>I am fitting a logistic regression model for the likelihood of patients suffering morbidity after surgery. The most commonly used prediction tool at the moment is POSSUM (Physiological and Operative Severity Score for the enUmeration of Mortality and Morbidity), which I would like to compare my model against.</p>

<p>In terms of discrimination, I have the Area Under the ROC curves calculated for both and would like to compare the two. </p>

<p>It seems in Stata that the command to use is <code>roccomp</code>. This produces a chi2 statistic and a p-value.</p>

<p>The R equivalent seems to require the <code>pROC</code> package and the function to use is <code>roc.test()</code>. However this function returns a z-statistic and p-value.</p>

<p>Looking at the documentation, both seem to be implementations of DeLong et al's methods of comparing AUROCs[1], but I cannot for the life of me understand why one gives a chi2 and the other a z-statistic. Are the tests equivalent?</p>

<p><em>Reference</em>:
1. Elisabeth R. DeLong, David M. DeLong and Daniel L. Clarke-Pearson (1988) â€œComparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approachâ€. Biometrics 44, 837--845.</p>

<p><strong>EDIT</strong>: Does this have anything to do with the explanation: <a href=""http://stats.stackexchange.com/questions/173415/at-what-level-is-a-chi2-test-mathematically-identical-to-a-z-test-of-propo/173483#173483"">At What Level is a $\chi^2$ test Mathematically Identical to a $z$-test of Proportions?</a> ?</p>
"
"0.0709645772411954","0.072141950116023","208078","<p>If you run the following code, you will have a data frame <code>real.dat</code> which has 1063 samples for 20531 genes. There are 2 extra columns named <code>time</code> and <code>event</code> where <code>time</code> is the survival time and <code>event</code> is <code>death</code> in case of <code>1</code> and <code>0</code> in case of <code>censored</code>.</p>

<pre><code>lung.dat &lt;- read.table(""genomicMatrix_lung"")
lung.clin.dat &lt;- read.delim(""clinical_data_lung"")

# For clinical data, get only rows which do not have NA in column ""X_EVENT""
lung.no.na.dat &lt;- lung.clin.dat[!is.na(lung.clin.dat$X_EVENT), ]

# Getting the transpose of main lung cancer data
ge &lt;- t(lung.dat)
# Getting a vector of all the id's in the clinical data frame without any 'NA' values
keep &lt;- lung.no.na.dat$sampleID

# getting only the samples(persons) for which we have a value rather than 'NA' values
real.dat &lt;- ge[ge[, 1] %in% keep, ]

# adding the 2 columns from clinical data to gene expression data
keep_again &lt;- real.dat[, 1]
temp_df &lt;- lung.no.na.dat[lung.no.na.dat$sampleID %in% keep_again, ]

# naming the columns into our gene expression data
col_names &lt;- ge[1, ]
colnames(real.dat) &lt;- col_names

dd &lt;- temp_df[, c('X_TIME_TO_EVENT', 'X_EVENT')]
real.dat &lt;- cbind(real.dat, dd)

# renaming the 2 new added columns
colnames(real.dat)[colnames(real.dat) == 'X_TIME_TO_EVENT'] &lt;- 'time'
colnames(real.dat)[colnames(real.dat) == 'X_EVENT'] &lt;- 'event'
</code></pre>

<p>I want to get the univariate Cox regression p-value for each gene in the above data frame. Now, when I try to run the <code>coxph</code> function from the <code>survival</code> package even for one gene, it shows the following error -</p>

<p><code>&gt; coxph(Surv(time, event) ~ HIF3A, real.dat)
Error in fitter(X, Y, strats, offset, init, control, weights = weights,  : 
  NA/NaN/Inf in foreign function call (arg 6)
In addition: Warning message:
In fitter(X, Y, strats, offset, init, control, weights = weights,  :
  Ran out of iterations and did not converge</code></p>

<p>What am I doing wrong here?</p>

<p>You can download the data from <a href=""https://drive.google.com/open?id=0B2p9dpw7AL_zMTNXR1JwRW1KRmM"" rel=""nofollow"">here</a>.</p>
"
"0.026822089039291","0.0272670941574606","208277","<p>I find it difficult to connect the coefficients of a regression model that includes splines to the actual prediction equation. For example, how could that be done with the following model?</p>

<pre><code>&gt; library(rms)
&gt; x &lt;- 1:11
&gt; y &lt;- c(0.2,0.40, 0.6, 0.75, 0.88, 0.99, 1.1, 1.15, 1.16, 1.16, 1.16 )
&gt; dd &lt;- datadist(x); options(datadist='dd')
&gt;  f &lt;- ols(y ~ rcs(x, c(3, 5, 7, 9)))
&gt; f  


  Linear Regression Model

ols(formula = y ~ rcs(x, c(3, 5, 7, 9)))

            Model Likelihood     Discrimination    
               Ratio Test           Indexes        
Obs       11    LR chi2     66.08    R2       0.998    
sigma 0.0201    d.f.            3    R2 adj   0.996    
d.f.       7    Pr(&gt; chi2) 0.0000    g        0.383    

Residuals

  Min        1Q    Median        3Q       Max 
-0.027360 -0.011739  0.001227  0.009892  0.031166 

           Coef    S.E.   t     Pr(&gt;|t|)
Intercept  0.0465 0.0224  2.08 0.0762  
x          0.1741 0.0072 24.18 &lt;0.0001 
x'        -0.1004 0.0311 -3.23 0.0144  
x''        0.0542 0.0913  0.59 0.5715  


&gt; Function(f)
function(x = 6){
  0.046475489 + 0.17411942*x - 0.002790266*pmax(x-3,0)^3 + 
  0.0015048699*pmax(x-5,0)^3 + 0.0053610582*pmax(x-7,0)^3 - 
  0.0040756621*pmax(x-9,0)^3 
}
</code></pre>
"
"0.0547503599848004","0.0667904674542028","208571","<p>I am trying to model my dependent variable (ordinal - three levels) using a set of independent variables (5 ordinal and 10 numeric). I am using <code>lrm</code> function in ""rms"" package of R. I am conducting principle component regression. <code>S1</code>, <code>C5</code>, <code>C2</code>, <code>C3</code>, <code>S7</code> and <code>S4</code> are the selected independent variables using PCA. </p>

<pre><code>          Coef         S.E.   Wald   Z    Pr(&gt;|Z|)
          y&gt;=2      -1.0469 0.6092 -1.72  0.0857  
          y&gt;=3      -8.5826 1.0354 -8.29  &lt;0.0001 
          S1=Simple -2.9091 0.6112 -4.76  &lt;0.0001 
          C5         0.8389 0.1475  5.69  &lt;0.0001 
          C2         1.4904 0.1889  7.89  &lt;0.0001 
          C3         1.2139 0.1908  6.36  &lt;0.0001 
          S7         0.8803 0.2701  3.26  0.0011  
          S4=TN     -1.2460 0.4659 -2.67  0.0075  
</code></pre>

<p>I understand, the output of the ordinal regression model is given by,</p>

<pre><code>ln(Fij/ 1-Fij) = Boj + B1X1 + B2X2 + .....BkXk

where Fi1 is probability that Y=1, 
Fi2 is probability that Y=2, 
Fi3 is probability that Y=3
B0, B1.....Bk - coefficients
X0, X1.....Xk - Independent variables
</code></pre>

<p>My question is, how do we interpret negative coefficients here? Also, does ranking the values of Wald statistics from largest to smallest indicate descending strength of evidence of an association with the dependent variable?</p>
"
"0.0599760143904067","0.0609710760849692","208765","<p>I know its possible to extract r-squared values to quantify the 'goodness-of-fit' of regressions in R, with something to the effect of:</p>

<pre><code>fit &lt;- lm(y ~ x1 + x2 + x3, data=mydata)  # Not actual data
r-sq &lt;- summary(fit)$r.squared # or $adj.r.squared
</code></pre>

<p>I've recently been using the <code>cumSeg</code> package for step-function regressions, but it doesn't appear to offer this functionality, though it does provide residuals as a vector.</p>

<p>Is there some way to extract an r-squared (or adj. r squared) that I don't know about? Or can it be calculated 'de novo' with something that <code>cumSeg</code> does actually provide?</p>

<p><strong>EDIT</strong>
This is the output of <code>summary()</code> for my stepfunction created via <code>cumSeg</code>. Perhaps someone more mathematically versed with stepfunctions knows if the  nomenclature for an r-squared (or whatever the equivalent is) is just different and the data I'm looking for is actually there (or if it is even a legitimate question to ask for an R-squared for stepfunctions?! I'm assuming it should be calculable from any fitted model really.</p>

<pre><code>&gt; summary(stepfunc)
              Length Class  Mode   
coefficients   3     -none- numeric
residuals     16     -none- numeric
effects       16     -none- numeric
rank           1     -none- numeric
fitted.values 16     -none- numeric
assign         0     -none- NULL   
qr             5     qr     list   
df.residual    1     -none- numeric
epsilon        1     -none- numeric
it             1     -none- numeric
psi            1     -none- numeric
beta.c         1     -none- numeric
gamma.c        1     -none- numeric 
V             16     -none- numeric
y             16     -none- numeric
id.group      16     -none- numeric
est.means      2     -none- numeric
n.psi          1     -none- numeric
</code></pre>
"
"0.0379321620905441","0.0385614943639849","209030","<p>I fitted a Cox PH model in R with the survival package and the <code>coxph</code> function.
I get the beta estimates from this model.
How can I use these coefficients to manually predict on new data, like the predict function does.</p>

<p>In a linear regression this is just the matrix multiplication <code>X %*% beta</code> if $X$ is the data and $beta$ is the vector of coefficients.</p>

<p>How is this in the Cox model? I also see that predict has several options for types of predictions.</p>

<p>here is a minimal example:</p>

<pre><code>library(survival)
data(""ovarian"")
m &lt;- coxph(formula = Surv(futime, fustat) ~., data=ovarian)
</code></pre>

<p>these two give different results:</p>

<pre><code>head(as.matrix(ovarian[, -c(1:2)]) %*% m$coefficients)

      [,1]
1 10.102002
2 10.371810
3  9.706097
4  6.820160
5  7.357138
6  7.627324

head(predict(m, ovarian))
          1           2           3           4           5           6 
 2.66935119  2.93915962  2.27344680 -0.61249088 -0.07551308  0.19467374 
</code></pre>
"
"0.0929144419623766","0.0944559849109725","209412","<p>I'm trying to build a bivariate copula-based model of income and wealth in Italy and I'm having trouble handling weighted data. I have access to micro data, a survey of about 10,000 households that includes the corresponding sample weights.</p>

<p>When calculating basic statistics (like mean and median) and even when performing linear regressions it is pretty easy to account for weights, besides there are useful packages for that (e. g. survey). But what do I do when I want to fit a parametric model of the distribution to weighted data? Or to estimate its kernel density?</p>

<p>I have a few ideas, but they seem to be pretty crude. For one, I could inflate my sample to the size of the universe. That is, I could multiply all weights by 100 (which would turn them into integers) and then create a vector that repeats each value of income and wealth a given number of times. But that would lead to a very large sample (which I believe still wouldn't be a perfect representation of the population) and will certainly put some extra strain on my computer.</p>

<p>I could also just round the weights off instead of multiplying them by 100, but this would still make the sample noticeably bigger and will inevitably skew the real proportions.</p>

<p>Another approach I came up with would be to normalize the weights (so that they sum up to one) and then randomly sample with repetitions from my initial sample with the corresponding vector of probability weights. R doesn't allow to draw the samples that are larger in size than the one that they are being drawn from. But I think that drawing the sample of the same size as the initial one will lead to some loss of information about the observed proportions. So I could draw the samples of the initial size as described above several times (how would I know how many is though?) and then combine them into one sample. And again, I will have a larger sample with some of the information lost along the way.</p>

<p>So I was wondering if there is a better way to handle weighted data. In some cases I think I could technically introduce the weights into the formula for computing the maximum likelihood for fitting a particular model, although I certainly wouldn't like to code that from the ground up. I will have to fit a lot of models as part of my project, both univariate (e. g. Singh-Mandala) for income and wealth and bivariate for copulas. I don't think the built in functions in any of the copula-related packages that I'm aware of allow one to account for weights. So any advice would help!</p>
"
"0.0625848744250123","0.0818012824723818","209510","<p>I want to regress a data set that contains a lot of zero's (~55%) and is determined by a typical 2-stage decision process generating the zeros:</p>

<ol>
<li>Consumer decides to apply for a bank loan or not (0-1)</li>
<li>Bank grants application or not (0-Inf; on a continuous scale)</li>
</ol>

<p>Based on the research I conducted so far I am pretty certain that the double hurdle model proposed by Cragg (or a version of it) is a promising approach for my regression. 
(Research includes e.g., Frees, E. W. Regression Modeling with Actuarial and Financial Applications 2011; Cragg, J. G. Some Statistical Models for Limited Dependent Variables with Application to the Demand for Durable Goods Econometrica, 1971; Smith, On Dependency in Double-Hurdle Models, 1998 (available at: <a href=""https://core.ac.uk/download/files/454/12162575.pdf"" rel=""nofollow"">https://core.ac.uk/download/files/454/12162575.pdf</a>); Vignette of the mhurdle package in R: <a href=""https://cran.r-project.org/web/packages/mhurdle/vignettes/mhurdle.pdf"" rel=""nofollow"">https://cran.r-project.org/web/packages/mhurdle/vignettes/mhurdle.pdf</a>)</p>

<p>With the mhurdle package in R this would suggest a regression as follows:</p>

<pre><code>hurd &lt;- function (x,y) {
         mhurdle(DV ~ IV1 + IV2 | IV2 + IV3 | 0,
                 data = xx, dist = x, corr = y)
}
# with x being ""n"" for normal, ""ln"" for log-normal, ""tn"" for truncated-normal"" and y being ""i"" for independent or ""d"" for dependent
</code></pre>

<p>I am though still struggeling to determine the following two things with required certainty - maybe someone is able to help me with this:</p>

<ul>
<li>Double Hurdle models (also the ones proposed by Cragg) are reguarly referenced in the context of count DV - are those models appropriate to handle a continuous DV as well? Are there any specific modifications required? (the mhurdle package e.g., suggests that it is possible)</li>
<li>Zero-inflation models (that are though only applicable to count data) can generally handle two sources of zeros. Are double hurdle models also capable of handling zeros arising from both hurdles? (the vignette of the mhurdle package implies that the Cragg DH model with a ""normal"" distribution accounts for that)</li>
</ul>

<p>Thanks a lot,
Jan</p>
"
"NaN","NaN","209617","<p>I have noticed in the CRAN documentation for the <code>survival</code> package that survival time tie-handling is discussed extensively for Cox-PH regression (allowing for Efron, Breslow, or Exact methods), but not for Aalen's Additive Model.  How would the below function call handle a tie in <code>DURATION</code>?  </p>

<p><code>aareg(formula = Surv(DURATION, OBSERVED) ~ regressor1 + regressor2 -1, data=df, nmin = 1)</code></p>
"
"0.0599760143904067","0.0609710760849692","209912","<p>I have a quick question I was hoping to get your input on. I am new to R and the smooth statistical regression world, and am trying to wrap my mind around the issues concerning using splines for mixed effect modeling.</p>

<p>My question is the following: in the â€˜gammâ€™ function, generalized additive mixed models can be estimated by including random components. These can be explicitly defined in the syntax, where you can also define whether the random component is an intercept, slope, or both. My understanding is that in the gam/bam function the same is achieved by including the bs=""reâ€ option for random intercepts and linear random slopes. Am I correct? If so, is there a way to specify whether it is the intercept or slope we are interested in, and does that have any effect on the output of the model?</p>

<p>I hope this question make sense, and I look forward to learning more about this.  Thanks for taking the time to read through this.</p>
"
"0.026822089039291","0.0272670941574606","210012","<p>I have percentage areas values from 4 treatments (25 replicates per treatment). I would like to compare these percentages.</p>

<p>I am supposed to use a beta regression, because my response variable is a proportion (not resulting from a count). For other variables I use GLMM and then use the glht() function to get the significance of pairwise comparisons. This does not seem to work with my betareg model. <strong>Is there any way to do that ?</strong></p>

<p>Also I would like to include random effects because of nested sampling protocol. From this <a href=""http://stats.stackexchange.com/questions/167340/beta-regression-with-random-effect-of-source-plot-in-two-seasons"">answer</a> this does not seem very appropriate/easy to do this with beta regression. </p>

<p>Do you have any suggestion ?</p>

<hr>

<p>EDIT</p>

<hr>

<p>Thanks to @rvl's comment I could calculate pairwise comparisons and extract group letters.</p>

<pre><code>   library(lsmeans)
   library(betareg)       
   betalive = betareg(Live ~ Crop)
   live.rg &lt;- ref.grid(betalive)        
   live.lsm = lsmeans(live.rg, ""Crop"")
   cld(live.lsm, alpha =  0.05)$.group
   &gt; [1] "" 1 "" "" 1 "" "" 1 "" ""  2""
</code></pre>

<p>However I could not solve my problem of random effects yet. </p>
"
"0.026822089039291","0.0272670941574606","210285","<p>I'm new to R (used to work with SPSS), and looking for a function that will output the Cox &amp; Snell and Nagelkerke R-Square measures of logistic regression. In SPSS they are displayed as part of the regular output, but in R I'm not sure what manipulation should I employ on the <code>glm</code> summary to output those measures.</p>
"
"0.0758643241810882","0.0771229887279699","210515","<p>Here are some sample data in R:</p>

<pre><code>set.seed(42)
df &lt;- data.frame(g = factor(rep(1:2, each= 50)), y = rnorm(100)+rep(0:1, each=50))
</code></pre>

<p>One can easily get group means using e.g. <code>with(df, tapply(y,g,mean))</code> but there is no such easy way to get the confidence intervals for group means. This is why I tried:</p>

<pre><code>lm(y ~ g-1, df)
# correct group means:
# -0.03567   1.10070 
confint(lm(y~g-1, df))
# too narrow CI's
#         2.5 %    97.5 %
# g1 -0.3287751 0.2574315
# g2  0.8075981 1.3938047
</code></pre>

<p>That is, one can estimate the group means using a linear model with dummy group indicators, omitting the overall intercept. But the confidence intervals of these regression parameters are narrower than the confidence intervals of group means. The latter could be found group by group with the same function:</p>

<pre><code>confint(lm(y~1, data=df, subset=g==1))
#                  2.5 %    97.5 %
# (Intercept) -0.3629177 0.2915742
</code></pre>

<p>Or a manual check using textbook formulae:</p>

<pre><code>ci.mean &lt;- function(x, alfa=0.05){
   n &lt;- length(x)
   a&lt;-qt(1-alfa/2, n-1)
   m&lt;-mean(x);          s&lt;-sd(x)
   se&lt;-s/sqrt(n)
   res &lt;- c(m, m-a*se,m+a*se)
   setNames(res, c(""mean"", paste(100*alfa/2, ""%""), paste(100*(1-alfa/2), ""%"")))
}
ci.mean(with(df, y[g==1])
#        mean       2.5 %      97.5 % 
# -0.03567178 -0.36291775  0.29157418 
</code></pre>

<p>There is probably an easy answer to the question of why the CIs of seemingly the same parameters are different. (The answer, obviously, has to start with difference in standard errors.) But I would be interested in the interpretation: why is that I can trust the group means found with <code>lm(y~g-1)</code> but I can't trust the confidence intervals around those ""means"" found with <code>confint(lm(y~g-1))</code>? And another naive question, why is the standard error for a group mean smaller if another group is present? That is:</p>

<pre><code>coef(summary(lm(y~g-1, df)))[1,2]
# [1] 0.1476987
coef(summary(lm(y~1, df, subset=g==1)))[1,2]
# [1] 0.1628433
</code></pre>

<p>Again, I am more interested in the substantial interpretation than  the formula showing why this is so. (I suppose after some sleep I could figure out the formula but would still be in trouble with interpretation).</p>

<p>Thanks in advance!</p>
"
"0.0657004319817604","0.0667904674542028","210646","<p>I am replicating an analysis that models tree mortality data. Data are structured such that forest sites are revisted at some random interval, which is recorded. It is then determined if a tree lived or died over that random interval, generating 0 1 mortality data (if a tree dies, it gets a 1 in the dependent variable). The interval between initial and final observation varies continuously, from 5-15 years. This is relevant, as the more time that passes, the more likely a tree will die. </p>

<p>Here are some pseudo data for R:</p>

<pre><code>mort &lt;- c(0,1,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,1,0)
interval &lt;- runif(length(mort), 5, 15)
pollution &lt;- rnorm(length(mort), 25,5)
data&lt;- data.frame(mort, interval, pollution)
</code></pre>

<p>I am trying to replicate an analysis which uses a logistic regression model for binary mortality data using the the logit transformation. Authors then model how pollution affects tree mortality rates. In the manuscript the authors write, ""because recensus is not annual, we relate annual mortality probability, <code>pi</code>, of tree <code>i</code> to the observed binomial data on whether that tree lived or died <code>Mi</code> via a Bernoulli likelihood,</p>

<p><a href=""http://i.stack.imgur.com/7i7jA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7i7jA.png"" alt=""enter image description here""></a></p>

<p>where <code>ti</code> is the time interval between successive censuses.""</p>

<p>My question: How would I implement this using the <code>glm</code> function, or something analagous, in R? Note: I understand modeling this as a hazard function would also be appropriate, but it is not what I am interested in.</p>
"
"0.053644178078582","0.0545341883149212","210914","<p>I have a logistic regression model obtained in R comparing association between two index diagnoses (0 or 1) with <code>Age</code> (continuous) + <code>Sex</code> (Factor) + <code>Renal.Fn</code> (continuous). My variable of interest is <code>Renal.Fn</code>. </p>

<pre><code> model &lt;- glm(diagnosis ~ Age + Sex + Renal.Fn, data = data, family = ""binomial"")
</code></pre>

<p>Currently to obtain Odds Ratio:</p>

<pre><code>exp(coef(model))[4]       # Renal.Fn 0.9884664
exp(confint(model))[[4]]  # Renal.Fn 0.9848022 0.9920815
</code></pre>

<p>My interpretation: Per unit increase in renal function the odds of diagnosis of interest reduces.</p>

<p>I wish to demonstrate the opposite.  For example: Per unit decrease in renal function, the odds of diagnosis increases.</p>

<p>Question:</p>

<ol>
<li>Is it correct to take <code>1 / exp(coef)</code> to derive the odds per unit decrease?</li>
<li>Is it subsequently correct to take <code>1/exp(coef) ^10</code> to derive odds per ten unit decrease?</li>
</ol>
"
"0.0464572209811883","0.0472279924554862","211094","<p>I am following the advice to ""keep it maximal"", and am analyzing the results from several psycholinguistic experiments. My main interest is in the fixed effects, with the random effect terms in there to provide a better test of these fixed effects.</p>

<p>That being said, I would like to keep things simple and, as much as possible, have the same analyses in each experiment.</p>

<p>In several of the experiments my random subject intercepts and slopes are perfectly, negatively correlated.</p>

<p>Given everything I said, would I be committing a cardinal sin by keeping both of them in the model?</p>

<p>Some details: I am the glmer function in R. I have only one fixed effect. I am running a logistic regression.</p>
"
"NaN","NaN","211670","<p>I want to make predictions use ARIMA in forecast package. I find that basically the prediction is just a lag of the actuals. Is there any way that I can better fit the model or any other approach available? (I find the ARIMA parameters through function <code>auto.arima</code> in ""forecast"" package in R.)</p>

<pre><code>Model and Plot:
        fit &lt;- arima(dataf$actuals, xreg=dataf$regressor,order=c(0,1,0))
        labDates &lt;- seq(as.Date(""2016-01-01"", format = ""%Y-%m-%d""),as.Date(""2016-01-16"", format = ""%Y-%m-%d""),by = ""day"")
        plot(labDates, dataf$actuals,col=""red"",type='l')
        lines(labDates,fitted(fit),col=""blue"")
        legend('topleft',c(""Actual Number"",""Predicted Number""),pch=c(20,20,20),col=c(""red"",""blue""))

Data:
    actuals&lt;-c(26952, 38178, 36377, 45718, 40393, 43111, 39947, 40853, 38792, 41816, 46091, 41866, 35701, 52738,73834, 82813) 
    actuals_next&lt;-c(38178, 36377, 45718, 40393, 43111, 39947, 40853, 38792, 41816, 46091, 41866, 35701, 52738, 73834,82813,NA)  
    regressor&lt;-c(519020, 671049 ,501083 ,288259 ,290899 ,260817, 276166, 274859 ,279405, 286689, 234050,95562,15138,  16401,  27145,  53717)    
    dataf&lt;-as.data.frame(cbind(actuals, actuals_next, regressor))
</code></pre>
"
"0.0657004319817604","0.0667904674542028","211936","<p>I am trying to use H2O in R to run a random forest.  <a href=""http://docs.h2o.ai/h2oclassic/Ruser/rtutorial.html"" rel=""nofollow"">http://docs.h2o.ai/h2oclassic/Ruser/rtutorial.html</a></p>

<p>In the documentation, I saw that there is an option for an offset parameter but I cannot find much information about how it is leveraged.  </p>

<p>In logistic regression, I have used an offset in two ways:
1.  To adjust for oversampling a binary event (<a href=""http://support.sas.com/kb/22/601.html"" rel=""nofollow"">http://support.sas.com/kb/22/601.html</a>)
2.  To do a two stage model where the first stage logit is calculated and then I used the logit score as an offset in the second stage so in effect the residual is modeled in stage two.</p>

<p>I would like to replicate both of these through random forest, if possible, but did not think it was possible until I saw the offset parameter in the H2O implementation of Random Forest.  Does anyone know if the H2O offset parameter functions the same as the offset option in SAS proc logistic? </p>

<p>Thank you!
nsl</p>
"
"0.0464572209811883","0.0314853283036575","212027","<p>I am using <code>gbm</code> to predict an imbalanced binary outcome, with the intent of obtaining a ranking by class probability estimation that produces a strong class separation on out-of-sample data.  (I am combining this class probability with other predictions, including from logistic regression, in an ensemble model.)</p>

<p>According to <a href=""http://bioconductor.wustl.edu/extra/vignettes/gbm/inst/doc/gbm.pdf"" rel=""nofollow"">this gbm vignette</a> (Ridgeway, 2007), under ""common user options"" for loss functions:</p>

<blockquote>
  <p>This should be easily dictated by the application.  For most
  classification problems either <code>bernoulli</code> or <code>adaboost</code> will be
  appropriate, the former being recommended. (p. 5)</p>
</blockquote>

<p>There's no explanation provided for favoring bernoulli over <a href=""http://stats.stackexchange.com/questions/37497/how-to-use-r-gbm-with-distribution-adaboost"">adaboost</a> nor any mention of the option for <a href=""https://en.wikipedia.org/wiki/Huber_loss"" rel=""nofollow""><code>huberized</code> loss function</a>, although this function may have been added at a later date.</p>

<p>Related question, but broader than mine:  <a href=""http://stats.stackexchange.com/questions/112359/choosing-between-loss-functions-for-binary-classification"">Choosing between loss functions for binary classification</a>.  This answer references <a href=""http://www.eecs.berkeley.edu/~wainwrig/stat241b/bartlettetal.pdf"" rel=""nofollow"">Bartlett (2006)</a> which is a challenging read for me.</p>

<p>Although performance is satisfactory under the bernoulli loss function, I am having a hard time understanding the justification for selecting one over another.  I'm trying all of them, but are there any theoretical justifications that are at least somewhat intuitive?</p>
"
"0.0657004319817604","0.0667904674542028","212301","<p>I have a huge doubt, which I believe is Basic. I have no difficulty in interpreting the results of our logistic regression model using the ODD ratio, but I do not know what to do when I work with Mixed effects model for longitudinal data.</p>

<p>Below they use the <code>glmer</code> function to estimate a mixed effects logistic regression model with Il6, CRP, and LengthofStay as patient level continuous predictors, CancerStage as a patient level categorical predictor (I, II, III, or IV), Experience as a doctor level continuous predictor, and a random intercept by DID, doctor ID.</p>

<p>The <code>glmer</code> function created 407 groups that refer to the number of doctors.</p>

<p>What would it mean for example the -0.0568 of IL6 and the -2.3370 of CancerStageIV's in the study presented?</p>

#################

<p>m &lt;â€ glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer =  ""bobyqa""),      nAGQ = 10) 
print(m, corr = FALSE) </p>

<h1>Generalized linear mixed model fit by maximum likelihood</h1>

<h2>Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]</h2>

<h2>Family:</h2>

<p>binomial ( logit )  </p>

<h2>Formula:</h2>

<p>remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +<br>
   (1 | DID)  </p>

<p>Data: hdp  </p>

<pre><code>  AIC        BIC    logLik     deviance  df.resid   
 7397        7461    -3690        7379     8516 
</code></pre>

<h2>Random effects:</h2>

<p>Groups Name         Std.Dev.<br>
     DID    (Intercept) 2.01 </p>

<p>Number of obs: 8525, groups: DID, 407  </p>

<h1>Fixed Effects:</h1>

<pre><code>  Intercept    IL6        CRP       CancerStageII  
 â€2.0527     â€0.0568    â€0.0215       â€0.4139 

CancerStageIII   CancerStageIV       LengthofStay      Experience  
 â€1.0035           â€2.3370              â€0.1212          0.1201 
</code></pre>
"
"0.0379321620905441","0.0385614943639849","212375","<p>I work most of my time with categorical data (predictors and outcome), I usually do a trees in SPSS to make groups and rank which groups are more predominant to buy / not buy.</p>

<p>But now I'm into R, and I'm finding limitations trying to use other modelling techniques as they require numerical data (SVM, NN, Logistic Regression)... I'm using the package ""caret"" to do the models but I usually get no results.</p>

<p>How do you treat categorical variables to fit in those models?</p>

<p>Most of the data is multinomial... should I create n-1 predictors for a n-level predictor? Does R have a function to do this automatically?</p>
"
"0.0599760143904067","0.0487768608679754","212430","<p>When building models with the <code>glm</code> function in R, one needs to specify the family. A family specifies an error distribution (or variance) function and a link function. For example, when I perform a logistic regression, I use the <code>binomial(link = ""logit"")</code> family.</p>

<p>What are (or represent) the error distribution (or variance) and link function in R ?</p>

<p>I assume that the link function is the type of model built (hence why using the <code>logit</code> function for the logistic regression. But I am not too sure about the error distribution function.</p>

<p>I had a look at R's documentation but could not find detailed information other than how to use them and what parameters can be specified.</p>
"
"0.0663812836584521","0.0674826151369737","212518","<p>Is there a function to weight values by sample size in a LOESS regression in R? I have been told that this function is included in SAS, and I was hoping a similar function existed in R.</p>

<p>I am working on a demographic study and I have run into a problem regarding how to weight my values in R. Data on total population is unknown and the sample sizes vary considerably. Due to the effects that random variations have on small sample sizes, I would like to weight my values derived from these different samples, i.e. values derived from larger sample sizes are weighted more than those from smaller sample sizes proportionally.</p>

<p>Looking at the sample below, since the sample size in ID 1 is much larger than the sample size of ID 17, ID 1 should be a more reliable value and should be weighted more to reflect the greater reliability of the data point. In my full data set, there are far more entries with lower values than higher ones. Due to a paucity of data, rather than simply eliminating these lower values, I can still involve them in the study with proper weighting.These values will then be graphed using a LOESS regression afterwards charting the change in values over time.</p>

<p>I saw a previous post regarding weighting in R, but in that post, the weighting values were already known. I am specifically looking for a function to weight values by sample size that ties to the LOESS regression function.</p>

<p><strong>EDIT</strong>Updated data set (let me know if the format </p>

<p>Site    Region  Period  Value   Sample_size Sample_sub<br>
Cat 6   -2715   0.132625995 188.5   25<br>
Dog 4   -3500   0.163120567 105.75  17.25<br>
Bear    4   -3500   0.228840125 79.75   18.25<br>
Chicken 5   -4933.333333    0.154931973 70  10.8452381<br>
Cow 7   -2715   0.110047847 52.25   5.75<br>
Bird    6   -2715   0.054347826 46  2.5<br>
Tiger   2   -2715   0.188679245 39.75   7.5<br>
Lion    4   -3500   0.245014245 39  9.555555556<br>
Leopard 6   -2715   0.128378378 37  4.75<br>
Wolf    7   -4350   0.021428571 35  0.75<br>
Puppy   7   -3500   0.226277372 34.25   7.75<br>
Kitten  4   -3500   0.176923077 32.5    5.75<br>
Emu 6   -2950   0.104938272 32.4    3.4<br>
Ostrich 4   -4350   0.17659805  30.76666667 5.433333333<br>
Elephant    4   -3650   0.143798024 30.36666667 4.366666667<br>
Sheep   4   -3500   0.086956522 28.75   2.5<br>
Goat    4   -3500   0.0625  28  1.75<br>
Fish    3   -2715   0.160714286 28  4.5<br>
Iguana  4   -3650   0   28  0<br>
Monkey  4   -4350   0.235588972 26.6    6.266666667<br>
Gorilla 4   -3500   0   25  0<br>
Baboon  4   -3500   0   25  0<br>
Lemming 4   -3400   0.208333333 24  5<br>
Mouse   4   -4350   0.202247191 22.25   4.5<br>
Rat 4   -3500   0.364705882 21.25   7.75<br>
Hamster 4   -3500   0.174757282 20.6    3.6<br>
Eagle   3   -4350   0   20  0<br>
Parrot  4   -3900   0.4 20  8<br>
Crow    4   -3808.333333    0.1 20  2<br>
Dove    4   -4233.333333    0.184027778 19.2    3.533333333<br>
Falcon  4   -3500   0.232876712 18.25   4.25<br>
Hawk    4   -3900   0.160493827 18  2.888888889<br>
Sparrow 4   -3958.333333    0.74702381  18  13.44642857<br>
Kite    4   -3900   0.126984127 16.8    2.133333333<br>
Chimpanzee  8   -3500   0.080645161 15.5    1.25<br>
Giraffe 4   -3500   0.06557377  15.25   1<br>
Bear    6   -3500   0   15  0<br>
Donkey  6   -2715   0.057692308 13  0.75<br>
Mule    4   -3650   0.285714286 12.6    3.6<br>
Horse   2   -2715   0.489361702 11.75   5.75<br>
Zebra   7   -2715   0.108695652 11.5    1.25<br>
Ox  4   -2715   0.377777778 11.25   4.25<br>
Snake   2   -2715   0   11  0<br>
Cobra   2   -3500   0.522727273 11  5.75<br>
Iguana  7   -2715   0.024390244 10.25   0.25<br>
Lizard  3   -2715   0.097560976 10.25   1<br>
Fly 4   -3500   0.275   10  2.75<br>
Mosquito    7   -4350   0   10  0<br>
Llama   4   -3650   0.0625  9.6 0.6<br>
Butterfly   4   -3650   0.255319149 9.4 2.4<br>
Moth    4   -4350   0.135135135 9.25    1.25<br>
Worm    7   -5400   0.216216216 9.25    2<br>
Centipede   7   -3500   0.222222222 9   2<br>
Unicorn 4   -3958.333333    0.296296296 9   2.666666667<br>
Pegasus 4   -3400   0.222222222 9   2<br>
Griffin 4   -3400   0.146341463 8.2 1.2<br>
Ogre    4   -4350   0.09375 8   0.75<br>
Monster 4   -3600   0.125   8   1<br>
Demon   4   -4350   0.041666667 8   0.333333333<br>
Witch   4   -3650   0.078947368 7.6 0.6<br>
Vampire 4   -3500   0.2 7.5 1.5<br>
Mummy   4   -2715   0.137931034 7.25    1<br>
Ghoul   2   -5400   0.571428571 7   4<br>
Zombie  8   -3500   0.142857143 7   1  </p>
"
"0.0464572209811883","0.0314853283036575","212803","<p>I am trying to test out the effects of second order methods on logistic regression. I have a function that looks like</p>

<pre><code>log_reg &lt;- function(x, y, test, maxit=100) {
  trainErr &lt;- numeric(maxit)
  testErr &lt;- numeric(maxit)

  w &lt;- runif(ncol(x), -0.01, 0.01)
  for(epoch in 1:maxit) {
    o &lt;- optim(par=w, fn=function(w) cross_entropy(x, y, w), 
               gr=function(w) logistic_gradient(x, y, w), method=""BFGS"")
    w &lt;- o$par

    trainErr[epoch] &lt;- o$value
    testErr[epoch] &lt;- cross_entropy(test[,-1], test[,1], w)
  }

  return(list(w, trainErr, testErr))
}
</code></pre>

<p>with helper functions</p>

<pre><code>logistic_gradient &lt;- function(x, y, w) {
  delta &lt;- sigmoid(x %*% w) - y
  dw &lt;- as.vector(t(delta) %*% x)
  return(dw / nrow(x))
}

cross_entropy &lt;- function(x, y, w) {
  sigma &lt;- sigmoid(x %*% w)
  error &lt;- -colSums(y*log(sigma) + (1-y)*log(1 - sigma))
  return(error / nrow(x))
}

sigmoid &lt;- function(x) return(1/(1+exp(-x)))
</code></pre>

<p>But when I plot train and test errors, it turns out that there is massive overfitting (the test error is even non-decreasing!). Because I cannot find a reason why second order methods would overfit that badly, I have this feeling I understood something wrong. Either in my implementation or conceptually...</p>

<p>Is there anyone who could enlighten me?</p>
"
"0.0967084173462244","0.0983129061176287","212840","<p>I read Chen et al. <a href=""http://onlinelibrary.wiley.com/doi/10.1002/for.1134/abstract"" rel=""nofollow"">""Forecasting volatility with support vector machine-based GARCH model""</a> (2010) where they implented a recurrent SVM procedure to estimate volatility by a GARCH based model. 
The model is of the form </p>

<p>$y_t = f(y_{t-1}) + u_t \qquad \qquad \ \ \ (1)$ </p>

<p>$u^2_t = g(u^2_{t-1}, w_{t-1}) + w_t \qquad  (2)$ </p>

<p>At first they got estimates for $u_t$ by estimating $(1)$ by a SVM. Then, the following recurrent SVM algorithm was proposed to estimate $(2)$.</p>

<hr>

<p><strong><em>Recurrent SVM Algorithm:</em></strong></p>

<p><strong>Step 1:</strong> Set $i = 1$ and start with all residuals at zero: $w_t^{(1)} = 0 $.</p>

<p><strong>Step 2:</strong> Run an SVM procedure to get the decision function $f^{(i)}$ to the points $\{x_t, y_t\} = \{u_{t - 1}^2, u_t^2 \}$ with all inputs $x_t = \{u_{t - 1}^2, w_{t-1} \}$</p>

<p><strong>Step 3:</strong> Compute the new residuals $w_t^{i+1} = u_t^2 - f^{(i)}$.</p>

<p><strong>Step 4:</strong> Terminate the computaion process if the stopping criterion is satisfied; otherwise, set $i = i + 1$ and go back to Step 2.</p>

<hr>

<p>The proposed stopping critrerion is based a Ljung-Box-Test for the residuals $w_t$. Only if the $p$-values of the test in five consecutive periods are higher than 0.1 the process is stopped. </p>

<p>As real world example the log-returns of the New York Stock Exchange (NYSE) composite stock index for the period from January 8, 2004 to December 31, 2007 was used. The last 60 observations where used as test sample. Hence, the estimation was done with the first 940 observations. In their study, the process converged after 121 interations. <strong>(Question:) However, my implementation in R does not converge. I think I have a misunderstanding of the concept.</strong> Because I think I implemented it exactly as stated. My R code is the following</p>

<pre><code>rm(list = ls())

library(quantmod)
library(e1071)

#Get NYSE data and convert to log returns
id     &lt;- ""^NYA""
data   &lt;- getSymbols(id, source = ""yahoo"", auto.assign = FALSE, 
                     from = ""2004-01-08"", to = ""2007-12-31"")
series &lt;- data[,6]  #Get adjusted closing prices
series &lt;- na.omit(diff(log(series)))*100  #Compute log returns

#Lagged data for analysis
x      &lt;- na.omit(cbind(series, lag(series)))

#Set parameters as in paper
svm_eps   &lt;- 0.05
svm_cost  &lt;- 0.005
sigma     &lt;- 0.02
svm_gamma &lt;- 1/(2*sigma^2)


#SVM to get u_t
svm     &lt;- svm(x = x[,-1], y = x[,1], scale = FALSE,
               type = ""eps-regression"", kernel = ""radial"",
               gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

u    &lt;- svm$residuals  #Extract u_t
n    &lt;- 60  #Size of test set
u_tr &lt;- u[1:(nrow(u) - n)]  #Subset to training set
u_tr &lt;- na.omit(cbind(u_tr, lag(u_tr)))^2  #Final training set


#Recurrent SVM for vola estimation
i       &lt;- 1
p_count &lt;- 0

while(p_count &lt; 5){

  print(i)  #Print number of loops

  #Estimate SVM for u^2
  svmr     &lt;- svm(x = u_tr[,-1], y = u_tr[,1], scale = FALSE,
                  type = ""eps-regression"", kernel = ""radial"",
                  gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

  #Test autocorrelation of residuals to lag 1
  test    &lt;- Box.test(svmr$residuals, lag = 1, type = ""Ljung-Box"")
  p_val   &lt;- test$p.value
  p_count &lt;- ifelse(p_val &gt; 0.1, p_count + 1, 0)

  #Extract residuals for next estimation step
  w        &lt;- svmr$residuals
  w        &lt;- c(0, w[-length(w)])  #lag 1

  u_tr &lt;- cbind(u_tr[,1:2], w)

  i &lt;- i + 1
}
</code></pre>
"
"0.0889588054368324","0.0904347204435887","212903","<p>I have the data <a href=""https://docs.google.com/spreadsheets/d/1lEzUt0QdFCp1ho-iWd4HzEIZoo8IyAM8YP2gu-K7BQo/edit?usp=sharing"" rel=""nofollow"">here</a>.But When I tried to build the logistic regression model using glm function its shows NA in TotalVisits. I have found similar question on stack overflow but that is answered for linear model.  </p>

<pre><code> str(quality)
'data.frame':   131 obs. of  14 variables:
 $ MemberID            : int  1 2 3 4 5 6 7 8 9 10 ...
 $ InpatientDays       : int  0 1 0 0 8 2 16 2 2 4 ...
 $ ERVisits            : int  0 1 0 1 2 0 1 0 1 2 ...
 $ OfficeVisits        : int  18 6 5 19 19 9 8 8 4 0 ...
 $ Narcotics           : int  1 1 3 0 3 2 1 0 3 2 ...
 $ DaysSinceLastERVisit: num  731 411 731 158 449 ...
 $ Pain                : int  10 0 10 34 10 6 4 5 5 2 ...
 $ TotalVisits         : int  18 8 5 20 29 11 25 10 7 6 ...
 $ ProviderCount       : int  21 27 16 14 24 40 19 11 28 21 ...
 $ MedicalClaims       : int  93 19 27 59 51 53 40 28 20 17 ...
 $ ClaimLines          : int  222 115 148 242 204 156 261 87 98 66 ...
 $ StartedOnCombination: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ AcuteDrugGapSmall   : int  0 1 5 0 0 4 0 0 0 0 ...
 $ PoorCare            : int  0 0 0 0 0 1 0 0 1 0 ...



table(is.na(quality))
FALSE 
1834
</code></pre>

<p>My data does not contain any NA values.</p>

<pre><code>set.seed(100)
split &lt;- sample.split(quality$PoorCare, SplitRatio = .5)
train &lt;-subset(quality, split ==TRUE)
test &lt;- subset(quality, split ==FALSE)
</code></pre>

<p>Building the model using all variable </p>

<pre><code>log.Quality &lt;- glm(PoorCare ~ ., data = train, family = 'binomial')

summary(log.Quality)      
Call:
glm(formula = PoorCare ~ ., family = ""binomial"", data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5679  -0.6384  -0.3604  -0.1154   2.1298  

Coefficients: (1 not defined because of singularities)
                          Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)              -3.583178   1.807020  -1.983   0.0474 *
MemberID                 -0.008742   0.010988  -0.796   0.4263  
InpatientDays            -0.106578   0.095632  -1.114   0.2651  
ERVisits                  0.275225   0.310364   0.887   0.3752  
OfficeVisits              0.126433   0.066140   1.912   0.0559 .
Narcotics                 0.190862   0.106890   1.786   0.0742 .
DaysSinceLastERVisit     -0.001221   0.002026  -0.603   0.5467  
Pain                     -0.020104   0.023057  -0.872   0.3832  
TotalVisits                     NA         NA      NA       NA  
ProviderCount             0.046297   0.040637   1.139   0.2546  
MedicalClaims             0.025123   0.030564   0.822   0.4111  
ClaimLines               -0.010384   0.012746  -0.815   0.4152  
StartedOnCombinationTRUE  2.205058   1.724923   1.278   0.2011  
AcuteDrugGapSmall         0.217813   0.139890   1.557   0.1195  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 72.549  on 64  degrees of freedom
Residual deviance: 49.213  on 52  degrees of freedom
AIC: 75.213

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Can anyone provide me a good explanation why this is happening ? </p>
"
"0.0464572209811883","0.0472279924554862","213011","<p>In the <code>car</code> package, we have the function <code>powerTransform</code> which transforms variables in a regression equation to make the residuals in the transformed equation as normal as possible. I am confused about what this transformation is and further in the following example:</p>

<pre><code># Box Cox Method, univariate
summary(p1 &lt;- powerTransform(cycles ~ len + amp + load, Wool))

# fit linear model with transformed response:
coef(p1, round=TRUE)
summary(m1 &lt;- lm(bcPower(cycles, p1$roundlam) ~ len + amp + load, Wool))
</code></pre>

<p>What I am confused about is what exactly the model <code>p1</code> is. Is it simply the linear model without a transformation, then it finds the optimal parameter, we then use that to specify <code>m1</code>? So what is the regression equation for <code>p1</code>, <code>m1</code>??</p>
"
"0.104607957095138","0.0938325062497729","213159","<p>I have two time series $d_t(t)$, $d_c(t)$, where I'm modelling charge as a function of time. Lengths of time series, $N$ are equal to $101$ data points. For the $d_t(t)$ (test sample, short-term) the time interval between observations is $t_1 = 0.1$ sec, and for the $d_c(t)$ (control sample, long-term) the time interval is $t_2 = 1$ sec. Thus, the right boundary of time for $d_t(t)$ is equal to $0.1\times 100 = 10$ sec, and the right boundary of time for $d_c(t)$ is equal to $1 \times 100 = 100$ sec. Both $d_t(t)$, $d_c(t)$ are correspond to the one experiment. The accuracy of measurements is $tol=0.001$. </p>

<p>Here's some sample data and visualization,</p>

<pre><code>dtest &lt;- data.frame(
   TIME = seq(0,100,by=1),
   CHARGE=c(-1.824, -1.150, -1.112, -1.097, -1.090, -1.085, -1.080, -1.075, -1.072, -1.069, 
            -1.067, -1.064, -1.061, -1.060, -1.058, -1.056, -1.055, -1.052, -1.051, -1.050, 
            -1.049, -1.048, -1.048, -1.045, -1.044, -1.043, -1.042, -1.041, -1.040, -1.039, 
            -1.038, -1.037, -1.037, -1.036, -1.036, -1.034, -1.034, -1.033, -1.032, -1.032, 
            -1.031, -1.031, -1.030, -1.030, -1.029, -1.029, -1.028, -1.027, -1.027, -1.028, 
            -1.028, -1.026, -1.025, -1.025, -1.026, -1.024, -1.025, -1.023, -1.023, -1.023, 
            -1.023, -1.023, -1.022, -1.021, -1.020, -1.020, -1.020, -1.019, -1.019, -1.018, 
            -1.018, -1.018, -1.018, -1.017, -1.016, -1.017, -1.017, -1.016, -1.015, -1.015, 
            -1.015, -1.014, -1.014, -1.013, -1.013, -1.012, -1.012, -1.011, -1.011, -1.011, 
            -1.011, -1.010, -1.011, -1.010, -1.010, -1.009, -1.008, -1.008, -1.008, -1.008, -1.008))


dcont &lt;- data.frame(
   TIME = seq(0,100,by=1),
   CHARGE=c(-1.827, -1.071, -1.056, -1.047, -1.039, -1.034, -1.030, -1.027, -1.025, -1.020, 
            -1.017, -1.016, -1.013, -1.010, -1.009, -1.006, -1.007, -1.004, -1.004, -1.002, 
            -1.000, -0.999, -0.997, -0.997, -0.995, -0.995, -0.993, -0.991, -0.991, -0.991, 
            -0.989, -0.988, -0.988, -0.986, -0.985, -0.984, -0.984, -0.984, -0.982, -0.982, 
            -0.981, -0.981, -0.979, -0.978, -0.977, -0.976, -0.975, -0.975, -0.975, -0.974, 
            -0.973, -0.973, -0.972, -0.972, -0.971, -0.970, -0.970, -0.970, -0.969, -0.967, 
            -0.966, -0.966, -0.966, -0.966, -0.966, -0.965, -0.965, -0.964, -0.964, -0.963, 
            -0.962, -0.961, -0.962, -0.962, -0.962, -0.960, -0.960, -0.959, -0.959, -0.958, 
            -0.958, -0.958, -0.958, -0.957, -0.956, -0.956, -0.955, -0.955, -0.955, -0.955, 
            -0.955, -0.954, -0.953, -0.953, -0.954, -0.952, -0.952, -0.951, -0.952, -0.951, -0.952))
</code></pre>

<p><a href=""http://i.stack.imgur.com/Tg2cT.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Tg2cT.jpg"" alt=""enter image description here""></a></p>

<p>As we can see from figure above the data from start region is increasing exponentially $f(x)=(1+exp(-x/t))$, later they are increasing (stationary) with linear rule, like $f(x)=kx+b$.</p>

<p>I'd like to forecast a value of $d_t(t)$ at time point $t=100$ sec, i.e. I have 10 seconds of history and want to forecast out 90 seconds. Then verify the forecasting $d_t^p(t=100)$ with corresponding value from the $d_c(t=100)$. The forecasting is satisfactory, if $$0.95 \times d_c(t) \leq d_t^p(t)\leq 1.05 \times d_c(t).$$
On physical grounds, the experimental data can be described with an exponential function $f(x) = a \times (1+exp(-(x/\tau)))$, where $a$, $\tau$ are parameters. I have been doing curve-fitting $d(t)$ in <strong>R</strong> using <code>nlminb</code>. The initial values of the parameters for optimization are chosen based on the physical characteristics of the process: <code>parConv &lt;- c(a=-0.762,tau=5.88)</code>. The result is below: $a= -1.03084$, $\tau= 0.50464$. Unfortunatly, the forecasting $f(t=100)=-1.030845$ does not satisfy to the range: </p>

<p>$$f(t=100) = -1.030845 \bar{\in} [0.95 \cdot (-0.952), 1.05 \cdot (-0.952)]=[-0.9044, -0.9996].$$</p>

<p>I have tried to split the original time series $d_t(t)$ into two parts: $d_{t_1}(t=1..10)$ and $d_{t_2}(t=11..101)$, then I have approximated the $d_{t_2}(t=11..101)$ using a linear function $f_1(x)=kx+b$ and a polynomial $f_2(x)=ax^2+bx+c$. </p>

<p>Result are better but not satisfy to the range: </p>

<p>$f_1(x)=-1.05851+x \cdot 5.61364\cdot 10^{-4}$, and $f_2(x)=-1.0718+x\cdot 0.0012+x^2 \cdot (-5.86943\cdot 10^{-6})$, <code>Adj. R-Square = 0,9815</code>. Then I have obtained the forecasts: $f_1(t=100)=-1.002374$ and $f_2(t=100)= -1.010494$. </p>

<p><strong>My question is</strong>: How to improve a bad long-term forecasting of time series in common case?</p>

<p><strong>Another possible solutions are:</strong></p>

<ol>
<li><p>Apply function <code>ln()</code> to the original data $d_t(t)$ and repeat fitting.</p></li>
<li><p>To use an exponential function and to assign greater weight to the $k$ last points (like <a href=""http://stackoverflow.com/questions/33539287/how-to-force-specific-points-in-curve-fitting"">here</a>).</p></li>
<li><p>To use some alternative models, for example, <a href=""https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model"" rel=""nofollow"">Autoregressive moving-average model</a>, _https://en.wikipedia.org/wiki/Backcasting.</p></li>
</ol>

<p>Example code:</p>

<pre><code>library(minpack.lm)
library(ggplot2)
library(optimx)

d &lt;- data.frame(
   TIME = seq(0,100,by=1),
   CHARGE=c(-1.824, -1.150, -1.112, -1.097, -1.090, -1.085, -1.080, -1.075, -1.072, -1.069, 
            -1.067, -1.064, -1.061, -1.060, -1.058, -1.056, -1.055, -1.052, -1.051, -1.050, 
            -1.049, -1.048, -1.048, -1.045, -1.044, -1.043, -1.042, -1.041, -1.040, -1.039, 
            -1.038, -1.037, -1.037, -1.036, -1.036, -1.034, -1.034, -1.033, -1.032, -1.032, 
            -1.031, -1.031, -1.030, -1.030, -1.029, -1.029, -1.028, -1.027, -1.027, -1.028, 
            -1.028, -1.026, -1.025, -1.025, -1.026, -1.024, -1.025, -1.023, -1.023, -1.023, 
            -1.023, -1.023, -1.022, -1.021, -1.020, -1.020, -1.020, -1.019, -1.019, -1.018, 
            -1.018, -1.018, -1.018, -1.017, -1.016, -1.017, -1.017, -1.016, -1.015, -1.015, 
            -1.015, -1.014, -1.014, -1.013, -1.013, -1.012, -1.012, -1.011, -1.011, -1.011, 
            -1.011, -1.010, -1.011, -1.010, -1.010, -1.009, -1.008, -1.008, -1.008, -1.008, -1.008))


d1 &lt;- data.frame(
   TIME = seq(0,100,by=1),
   CHARGE=c(-1.827, -1.071, -1.056, -1.047, -1.039, -1.034, -1.030, -1.027, -1.025, -1.020, 
            -1.017, -1.016, -1.013, -1.010, -1.009, -1.006, -1.007, -1.004, -1.004, -1.002, 
            -1.000, -0.999, -0.997, -0.997, -0.995, -0.995, -0.993, -0.991, -0.991, -0.991, 
            -0.989, -0.988, -0.988, -0.986, -0.985, -0.984, -0.984, -0.984, -0.982, -0.982, 
            -0.981, -0.981, -0.979, -0.978, -0.977, -0.976, -0.975, -0.975, -0.975, -0.974, 
            -0.973, -0.973, -0.972, -0.972, -0.971, -0.970, -0.970, -0.970, -0.969, -0.967, 
            -0.966, -0.966, -0.966, -0.966, -0.966, -0.965, -0.965, -0.964, -0.964, -0.963, 
            -0.962, -0.961, -0.962, -0.962, -0.962, -0.960, -0.960, -0.959, -0.959, -0.958, 
            -0.958, -0.958, -0.958, -0.957, -0.956, -0.956, -0.955, -0.955, -0.955, -0.955, 
            -0.955, -0.954, -0.953, -0.953, -0.954, -0.952, -0.952, -0.951, -0.952, -0.951, -0.952))


(g1 &lt;- ggplot(d,aes(TIME,CHARGE))+geom_point())
g1+geom_smooth()  ## with loess fit

# Parameter choices:
parConv &lt;- c(a=-0.762,tau=5.88) #

#Perturbed parameters:
parStart      &lt;- parConv
parStart[""a""] &lt;- parStart[""a""]+3e-4

Ebos &lt;- -1.161 # start value at x=0

#The formulae:
RCCircuits&lt;-function(parS,x)
    with(as.list(parS), 
                       ifelse(x==0, Ebos, a*(1+exp(-(x/tau))) ) 
         )

# A sum-of-squares function
ssqfun &lt;- function(parS, Observed, x) {
   sum(ResidFun(parS, Observed, x)^2)
}

# Local minimizer for smooth nonlinear functions subject to bound-constrained parameters
opt1 &lt;- nlminb(start=parStart, objective = ssqfun,
    Observed = d$CHARGE, x = d$TIME,
    control= list(eval.max=5000,iter.max=5000))

parNLM &lt;- opt1$par

#SSE Review:
sapply(list(parConv,parNLM),
  ssqfun,Observed=d$CHARGE,x=d$TIME)  

pred0 &lt;- RCCircuits(as.list(parConv), d$TIME)
pred1 &lt;- RCCircuits(as.list(parNLM),  d$TIME)

# forecasting at t=100 sec
pred100 &lt;- RCCircuits(as.list(parNLM), 1000) 
</code></pre>
"
"0.104084994078794","0.105811867590468","213192","<p>I have half-hourly electricity data of several homes for a duration of one month. Also, I have ambient temperature at same sampling rate. Now, I need to make half-hourly forecasts using historical electricity data and forecasted temperature for coming day.</p>

<p>Currently, I am facing a problem with seasonality, i.e., in some homes I observe (via visual inspection) day-wise seasonality and in some other homes no pattern is found. <a href=""http://stats.stackexchange.com/a/212797/60072"">Stephan</a> suggested to check for weekly seasonality as well, but visually I do not find any. So, I thought to try models with different forced seasonalities to find the prediction accuracy. I can think of two options:</p>

<ol>
<li>Model_1 with daily seasonality (frequency = 48 observations)</li>
<li>Model_2 with weekly and daily seasonality (48, 7*48)</li>
</ol>

<p>Keeping the above approach in mind, I am facing the following issue:</p>

<ol>
<li>For Model_1, I can use <code>auto.arima()</code> with <code>xreg</code> option to specify an extra <code>temperature</code> predictor. However, this only works for a single seasonality.</li>
<li>For Model_2, I tried to use the <code>tbats()</code> forecasting function, which models multiple seasonalities, but does not allow extra predictor variables. </li>
</ol>

<p>Is there a forecasting function which allows <em>both</em> multiple predictors and multiple seasonalities?</p>

<p>Here is the electricity data of one month. </p>

<pre><code>    data &lt;- structure(c(1642.8, 1467.1, 165.57, 1630.99, 1618.65, 1629.29, 
        1598.93, 1839.9, 1604.52, 1606.73, 1473.82, 1669.17, 1698.9, 
        2111.21, 2056.41, 3671.29, 2808.01, 1336.15, 794.11, 1212.15, 
        377.36, 888.54, 174.58, 218.54, 420.76, 389.58, 397.77, 395.31, 
        359.11, 364.8, 376.13, 389.37, 929.5, 1702.38, 519.65, 2452.28, 
        1354.45, 1842.96, 725.41, 661.11, 528.44, 733.4, 429.51, 310.47, 
        279.72, 407.83, 1791.1, 1754.53, 1536.73, 1608.37, 1432.23, 1401.72, 
        1582.14, 1558.75, 1536.24, 1745.59, 1375.61, 1556.71, 1671.12, 
        1206.77, 1391.84, 876.23, 1617.3, 1638.99, 1833.61, 1591.42, 
        1455, 183.87, 177.55, 184.36, 332.99, 352.95, 425.1, 945.67, 
        342.3, 348.45, 227.18, 382.15, 268.91, 335.88, 326.94, 233.23, 
        169.71, 179.51, 195.3, 207.23, 1681.9, 1493.32, 941.52, 980.36, 
        924.31, 379.02, 1229.89, 1590.21, 1250.92, 1149.24, 1124.04, 
        993.78, 883.98, 860.69, 934.17, 969.31, 1049.55, 1104.94, 904.3, 
        1220.23, 1183.9, 891.26, 825.33, 787.77, 1060.93, 1029.1, 982.25, 
        193.13, 182.65, 181.75, 167.68, 165.89, 291.02, 300.2, 418.4, 
        297.66, 231.89, 305.66, 701.18, 338.5, 337.24, 332.11, 332.66, 
        187.8, 179.03, 130.22, 177.73, 172.24, 173.45, 334.23, 810.53, 
        359.41, 330.23, 333.29, 568.85, 2462.46, 1660.32, 1156.13, 1136.2, 
        1189.07, 832.73, 181.91, 185.93, 1076.77, 672.78, 1376.71, 1020.17, 
        382.18, 1160.74, 791.36, 1569.4, 817.78, 850.71, 747.21, 826.12, 
        1306.46, 506.23, 140.05, 132.6, 304.19, 308.14, 406.13, 290.73, 
        188.9, 165.59, 174.56, 145.97, 151.83, 142.29, 443.58, 799.9, 
        279.36, 223.88, 221.03, 291.26, 374.97, 431.36, 598.98, 625.82, 
        1052.02, 2036.83, 1230.03, 1429.81, 1099.34, 1646.03, 1668.56, 
        1631.79, 1604.04, 2849.49, 2998.63, 2476.96, 1601.04, 1216.42, 
        2004.2, 1868.51, 1961.91, 1813.35, 1500.22, 1276.94, 1369.29, 
        632.43, 238.15, 488.76, 467.6, 330.27, 144.67, 153.36, 924.12, 
        1348.18, 799.01, 524.3, 420.5, 264.34, 283.86, 198.95, 206.52, 
        217.33, 356.16, 207.83, 197.6, 194.03, 193.01, 249.85, 271.22, 
        244.25, 442.5, 660.49, 245.66, 356.13, 443.32, 336.15, 849.74, 
        1709.21, 1542.09, 1315.69, 2628.6, 2261.8, 1576.42, 1776.48, 
        1239.64, 1401.12, 1106.17, 1378.55, 1315.57, 1141, 1642.63, 2484.13, 
        1968.94, 3059.42, 1317.32, 905.05, 484.42, 486.86, 96.79, 183.45, 
        173.94, 342, 255.96, 320.54, 106.19, 147.88, 150.77, 176.17, 
        344.75, 371.73, 309.46, 237.86, 187.32, 202.61, 292.5, 248.28, 
        259.3, 283.67, 365.09, 230.47, 326.15, 350.92, 335.42, 419.39, 
        345.31, 1093.22, 1392.87, 1298.11, 919.16, 1654.53, 1045.99, 
        558.42, 437.29, 857.9, 758.34, 1220.04, 1390.62, 956.74, 909.93, 
        584.67, 409.87, 387.3, 387.93, 1276.28, 871.06, 413.8, 313.2, 
        199.5, 330.21, 210.9, 358.28, 352.13, 233.62, 259.18, 123.57, 
        255.58, 411.78, 427.65, 318.95, 298.5, 283.23, 279.85, 200.45, 
        205.97, 254.24, 307.98, 1090.53, 289.71, 215.14, 286.63, 328.55, 
        288.96, 1281.19, 1354.8, 1302.05, 1254.46, 261.95, 270.09, 243.28, 
        696.61, 314.27, 241.73, 245.68, 157.74, 222.55, 294.36, 185.46, 
        203.49, 182.14, 246.69, 178.26, 397.5, 330.2, 212.02, 248.72, 
        265.48, 249.37, 130.59, 248.97, 279.94, 319.07, 358.5, 278.98, 
        251.92, 304.66, 455.05, 365.95, 340.93, 287.51, 264.82, 260.18, 
        34.35, 35.11, 184.09, 247.18, 160.9, 139.27, 284.96, 296.31, 
        252.3, 342.65, 353.03, 380.52, 346.19, 350.06, 218.52, 133.94, 
        173.7, 128.26, 167.8, 112.77, 147.8, 129, 170.54, 89.88, 243.08, 
        97.61, 190.31, 193.94, 268.17, 233.5, 205.27, 92.29, 167.43, 
        168.34, 151.99, 193.84, 379.1, 318.69, 327.28, 487.39, 414.01, 
        336.06, 278.02, 168.05, 155.6, 236.4, 264.94, 296.05, 326.46, 
        357.43, 356.31, 340.29, 319.81, 312.79, 341.53, 317.36, 309.62, 
        440.6, 285.5, 282.06, 288.99, 334.48, 196.54, 144.24, 218.55, 
        173.64, 242.29, 251.78, 186.81, 184.36, 141.62, 208.91, 157.53, 
        154.03, 139.44, 137.66, 256.75, 1202.05, 177.36, 177.93, 72.83, 
        252.9, 231.35, 1090.39, 442.91, 363.12, 248.96, 478.75, 249.64, 
        297.29, 227.28, 365.82, 879.7, 488.93, 184.79, 138.13, 151.77, 
        123.18, 175.76, 251.84, 208.06, 126.68, 246.3, 307.34, 319.79, 
        324.3, 379.6, 309.53, 253.17, 221.91, 228.42, 150.24, 148.59, 
        118.79, 86.89, 140.51, 200.43, 212.15, 276.14, 441.81, 125.77, 
        152.42, 329.28, 269.21, 177.35, 1106.29, 128.92, 96.35, 63.53, 
        520.62, 940.25, 1014.34, 314.99, 390.2, 330.1, 377.04, 341.35, 
        342.79, 241.79, 249.9, 391.92, 292.68, 105.02, 179.99, 118.53, 
        154.17, 90.53, 206.7, 345.33, 244.75, 291.68, 820.57, 1777.84, 
        1805.83, 1753.73, 1416.7, 279.2, 262.82, 1345.88, 467.98, 1136.66, 
        170.02, 159.96, 1478.8, 1414.12, 1347.93, 1505.59, 1341.69, 445.53, 
        277.59, 1609.61, 1476.45, 244.08, 192.57, 213.55, 439.02, 112.86, 
        128.54, 376.09, 251.15, 116.27, 254.82, 302.56, 304.6, 198.5, 
        240.05, 219.35, 70.3, 190.96, 211.95, 328.53, 714.62, 3176.56, 
        2604.09, 191.65, 145.25, 93.93, 83.6, 81.13, 146.12, 331.75, 
        250.24, 1144.53, 1616.47, 1008.7, 316.65, 311.46, 1152.99, 1504.86, 
        1543.21, 1081.54, 1428.07, 1358.23, 1349.75, 190.23, 2398.92, 
        2196.11, 1466.94, 2249.77, 2150.2, 2542.25, 618.03, 453.22, 880.99, 
        1497.86, 440.96, 161.85, 324.88, 434.11, 316.33, 444.66, 359.14, 
        277.41, 1237.28, 761.41, 183.53, 309.44, 213.48, 121.64, 346.7, 
        149.86, 2060.39, 1102.13, 347.97, 600.24, 912.6, 590.77, 1805.76, 
        1673.93, 1573.91, 505.74, 446.76, 1033.41, 1668.68, 1293.9, 383.81, 
        1419.99, 1349.4, 711.55, 218.63, 182, 401.93, 1876, 1486.34, 
        1543.11, 2313.8, 478.57, 615.19, 542.68, 971.98, 531.11, 766.21, 
        489.76, 344.47, 319.86, 321.26, 311.41, 288.67, 310.67, 305.15, 
        419.33, 422.84, 950.08, 2188.88, 3454.92, 1989.54, 590.33, 327.05, 
        354.78, 578.41, 1583.29, 2016.66, 1481.03, 293.21, 1864.84, 399.65, 
        366.76, 357.7, 2074.97, 1626.86, 1133, 1624.61, 1506.93, 628.4, 
        1405.68, 217.8, 1223.11, 1356.97, 1171.72, 1182.86, 1642.11, 
        2289.02, 814.39, 595.76, 542.78, 1596.41, 884.97, 235.25, 1540.68, 
        781.95, 115.71, 1204.98, 718.66, 452.09, 305.58, 444.67, 356.76, 
        182.54, 674.47, 153.8, 862.25, 1322.88, 323.33, 1659.64, 496.72, 
        304.74, 246.6, 327.12, 239.31, 246.72, 225.72, 234.7, 324.07, 
        304.27, 171.86, 97.64, 242.69, 295, 324.53, 513.81, 1100.65, 
        1151.77, 231.56, 189.88, 786.3, 1164.87, 676.09, 882.82, 1496.3, 
        1027.91, 872.92, 809.1, 840.31, 1302.18, 2055.87, 677.74, 934.66, 
        263.91, 186.68, 248.5, 214.62, 371.54, 298, 294.52, 304.86, 1295.77, 
        942.5, 305, 265.78, 255.89, 255.63, 151.54, 108.16, 116.81, 100.19, 
        224.75, 84.11, 1143.89, 262.15, 784.21, 1728.29, 1506.79, 434.94, 
        374.29, 265.43, 560.74, 1651.49, 1063.07, 1054.69, 1298.4, 1261.59, 
        1132.75, 692.82, 660.57, 198.25, 97.81, 1258.67, 833.64, 796.35, 
        868.76, 999.86, 2240.02, 885.72, 1317.52, 1267.18, 167.93, 133.22, 
        364.44, 267.17, 406.13, 412.52, 1036.04, 779.34, 655.43, 1901.2, 
        270.18, 266.31, 284.21, 288.66, 135.38, 176.11, 154.86, 160.21, 
        146.28, 163.72, 139.75, 278.12, 253.51, 319.62, 396.39, 1662.69, 
        1577.09, 1059.71, 241.64, 407.54, 290.49, 846.17, 1325.31, 1418.23, 
        1432.5, 1412.6, 1015.87, 1619.88, 1426.58, 1333.32, 1963.35, 
        1638.11, 1081.89, 285.56, 1084.58, 2038.77, 1022.39, 1145.92, 
        513.87, 107.7, 176.19, 143.77, 374.3, 373.99, 221.76, 148.3, 
        331.01, 2323.12, 1502.09, 347.17, 296.7, 306.06, 313.03, 221.69, 
        295.35, 301.95, 250.92, 231.54, 140.6, 717.63, 863.5, 402.15, 
        1337.78, 1575.44, 1738.49, 1675.57, 1617.66, 1365.58, 242.8, 
        286.29, 712.34, 1559.59, 1600.34, 3447.88, 3432.89, 3337.23, 
        1472.21, 1323.76, 1265.31, 1221.65, 1312.63, 2016.09, 2972.13, 
        1451.67, 735.67, 130.13, 379.87, 162.21, 226.48, 417.18, 357.51, 
        346.37, 200.89, 190.15, 276.05, 942.74, 1471.99, 1047.53, 1240.06, 
        742.62, 169.34, 144.28, 220.58, 165.23, 344.8, 227.47, 254.64, 
        742.37, 1809.01, 436.11, 1692.87, 1697.74, 1474.91, 1635.68, 
        1664.2, 1489.85, 1316.4, 1364.07, 1510.02, 1497.89, 1709.17, 
        2846.71, 2736.08, 1015.43, 1017.08, 1195.21, 751.18, 523.89, 
        199.69, 2148.71, 1151.39, 1182.84, 788.91, 259.31, 146.29, 141.09, 
        299.38, 349.53, 404.08, 449.9, 391.77, 251.89, 222.25, 281.04, 
        565.4, 371.24, 219.75, 324.45, 227, 157.67, 212.48, 201.69, 140.84, 
        220.67, 187.64, 399.79, 157.92, 275.49, 326.99, 1340.51, 1578.68, 
        1599.41, 1667.74, 1129.07, 1312.55, 1393.09, 1368.69, 1130.85, 
        968.71, 1130.2, 1223.1, 1124.5, 1077.09, 1052.42, 1255.31, 918.21, 
        1263.93, 706.21, 3080.29, 1620.18, 1122.48, 750.06, 262.89, 110.02, 
        236.83, 413, 227.53, 355, 277.51, 258.03, 368.44, 656.68, 1808.17, 
        1493.41, 1272.21, 330.67, 1674.18, 719.45, 1089.68, 784.01, 275.73, 
        312.69, 345.11, 761.8, 1020.11, 259.16, 345.98, 270.23, 580.15, 
        1303.68, 1659.7, 1732.94, 204.9, 373.58, 373.36, 1381.52, 1437.74, 
        1262.78, 1264.6, 1184.54, 1175.12, 857.09, 1428.34, 841.92, 232.47, 
        223.22, 473.24, 382.7, 189.84, 1737.48, 1689.34, 378.48, 872.56, 
        180.12, 363.03, 301.38, 412.57, 401.17, 387.35, 417.63, 300.61, 
        376.82, 284.31, 232.31, 269.96, 188.41, 203.79, 134.88, 193.66, 
        57.86, 89.71, 167.66, 60.84, 197.03, 703.66, 1638.7, 1467.03, 
        347.22, 1397.23, 1511.92, 1362.25, 1397.18, 1106.19, 826.5, 1033.04, 
        1039.46, 584.98, 706.35, 548.07, 373.39, 681.6, 1231.28, 288.94, 
        649.36, 79.28, 209.23, 290.75, 304.12, 132.57, 91.2, 355.37, 
        197.45, 343.17, 339.2, 284.65, 229, 234.73, 322.36, 323.43, 295.1, 
        197.03, 308.17, 223.88, 235.08, 225.16, 172.83, 236.26, 135.17, 
        394.89, 479.2, 315.34, 280.9, 282.36, 204.78, 367.24, 1712.29, 
        1521, 1686.09, 960.19, 1019.12, 1062.77, 851.88, 1369.98, 689.2, 
        580.5, 751.74, 547.67, 556.64, 493.85, 404.15, 428.07, 716.2, 
        1442.05, 1045.81, 1497.07, 567.59, 155.07, 537.72, 446.03, 282.57, 
        642.88, 409.37, 338.91, 173.89, 358.63, 195.42, 209.95, 186.44, 
        152.34, 105.51, 132.26, 82.14, 122.88, 149.38, 211.42, 350.17, 
        429.72, 336.4, 982.21, 1436.25, 1726.87, 1830.42, 1282.05, 1293.4, 
        1121.01, 946.2, 707.1, 154.53, 767.56, 607.78, 448, 288.23, 270.32, 
        223.93, 166.22, 262.45, 223.74, 159.31, 210.44, 257.94, 183.99, 
        151.38, 206.11, 193.43, 388.95, 577.98, 304.64, 285.13, 256.59, 
        420.26, 289.34, 356.02, 358.08, 325.22, 275.95, 164.46, 213.23, 
        142.99, 221.66, 270.61, 206.56, 213.68, 254.33, 250.15, 267.99, 
        403.95, 671.2, 1574.11, 396.34, 477.88, 631.08, 618.25, 1366.88, 
        298.19, 287.05, 290.38, 332.44, 235.9, 229.79, 831.6, 1320.86, 
        477.31, 944.84, 547.33, 411.21, 705.43, 873.49, 572.81, 585.36, 
        1229.69, 701.01, 653.49, 74.81, 162.47, 179.54, 330.27, 544.51, 
        332.41, 296.66, 130.66, 1055.61, 556.79, 265.43, 383.44, 398.22, 
        362.66, 223.99, 130.35, 193.67, 217.68, 273.3, 247.84, 161.66, 
        320.08, 322.52, 274.61, 811.44, 353.85, 323.41, 383.61, 389.5
        ), .Dim = c(1248L, 1L), .Dimnames = list(NULL, ""power""))
</code></pre>

<p>Temperature predictor values are:</p>

<pre><code>temp&lt;-structure(c(31, 31, 31, 31, 30, 29, 28, 27.5, 27, 26, 26, 26, 
26, 26, 26, 28, 29, 29, 30, 31, 32, 33, 33, 34, 34, 35, 36, 36, 
36.5, 37, 38, 38, 39, 39, 39, 38, 37, 36, 36, 35, 34, 33, 33, 
33, 32.5, 32, 32, 31, 30, 26, 24, 24, 24, 24, 24, 24, 25, 25, 
25, 25, 26, 26, 26, 27, 28.3, 29.7, 31, 32, 33, 33, 34, 34, 34, 
35, 35, 35, 36, 37, 38, 39, 39, 39, 39, 38.5, 38, 37, 36, 35, 
34, 34, 33, 32.5, 32, 30, 30, 30, 28, 28, 27, 25, 25, 24, 24, 
24, 24, 24, 25, 25, 25, 25, 25, 27, 28, 30, 31, 33, 34, 37, 37, 
38, 38, 39, 39.3, 39.7, 40, 40, 40, 40, 41, 40, 40, 38, 38, 36, 
35, 34, 33, 31, 31, 30, 31, 30, 30, 29, 28, 28, 28, 27, 27, 28, 
27, 24, 24, 24, 23, 23, 23, 24, 24, 26, 27, 28, 29.7, 31.3, 33, 
34, 35.3, 36.7, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 
39, 38, 37, 36, 34, 33, 32, 32, 30, 28.5, 27, 27, 27, 27, 27, 
26, 26, 26, 25.3, 24.7, 24, 24, 24, 24, 23, 23, 24.5, 26, 28, 
29.7, 31.3, 33, 34.5, 36, 37, 38, 39, 40, 40, 40.3, 40.7, 41, 
41, 41, 41, 41, 41, 41, 39.5, 38, 37, 37, 35, 34, 34, 33, 33, 
33, 32, 31, 31, 30, 30, 30, 29, 29, 28.5, 28, 28, 28, 29, 29, 
28.5, 28, 29, 29, 30, 31, 31, 32.5, 34, 35, 37, 39.5, 42, 42, 
42, 42, 42, 43, 44, 45, 45, 44, 43, 42, 42, 41, 39, 36.5, 34, 
34, 33, 33, 33, 33, 33, 33, 32, 32, 32, 32, 31.5, 31, 30, 30, 
30, 29, 29, 28, 27, 28, 29, 30, 31, 32.5, 34, 34, 35, 37, 40, 
41, 41, 42, 42, 42.5, 43, 43, 43, 42, 42, 43, 42, 42, 41, 40, 
39, 37.5, 36, 35, 35, 34, 34, 33, 32, 32, 32, 32, 31, 31, 31, 
31, 31, 28, 28, 28, 28, 28, 28.5, 29, 30, 30.5, 31, 32, 33.5, 
35, 36, 37, 38, 39, 38, 39, 40, 41, 42, 42, 43, 42, 42, 42, 41, 
41, 40, 39, 38, 37, 36, 35, 34, 34, 33, 32, 32, 31, 30.5, 30, 
30, 30, 30, 29, 29, 29, 28, 27, 27, 27, 27, 28.5, 30, 31.5, 33, 
34, 35, 36, 37, 38, 38, 39, 40, 41, 42, 42, 42, 42, 42, 42, 42, 
42, 42, 42, 42, 40, 40, 39, 37, 37, 36.5, 36, 35, 34, 34, 33, 
33, 32, 32, 32, 32, 32, 32, 31, 31, 30, 30, 30, 30, 31, 31, 31, 
32, 33.5, 35, 35.5, 36, 37, 37, 38, 39, 40, 41, 41, 42, 42, 42, 
43, 43, 42, 41, 41, 40, 38, 36, 35, 34, 34, 35, 36, 36, 35, 35, 
33, 33, 32, 31, 31, 30, 30, 29, 29, 29, 29, 29, 31, 31.5, 32, 
31, 30, 31, 32, 33, 34, 34, 35, 36, 36, 37, 37, 38, 38, 38, 39, 
39, 39, 39, 39, 39, 39, 38, 37, 37, 37, 37, 35, 33, 31, 31, 31, 
30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 28, 28, 27, 27, 26, 29, 
29, 30, 31, 32, 32, 33, 33, 33, 34, 34, 35, 35, 37, 37, 37, 37, 
37, 38, 38, 38, 38, 37, 37, 36, 35, 34.5, 34, 34, 33, 33, 32, 
32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 30, 30, 29.5, 29, 
29, 30, 30, 31, 32, 32, 33, 33.5, 34, 34, 34, 30.7, 27.3, 24, 
24, 24, 24.7, 25.3, 26, 27, 28, 29, 29, 29, 28, 28, 27, 27, 26, 
25, 25, 25, 24.5, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 
23, 23, 23, 24, 24, 24, 25, 25, 26, 28, 29, 30, 31, 31, 32, 33, 
34, 35, 35, 36, 37, 37, 37, 37, 37, 37, 37, 35, 34, 33, 33, 33, 
32, 31.7, 31.3, 31, 31, 31, 31, 30, 30, 30, 30, 28, 28, 27, 27, 
26, 26, 25, 25, 25, 25, 25, 26, 27, 28, 29.7, 31.3, 33, 34, 34, 
35, 36, 37, 36, 36, 35, 35, 34, 32, 32, 33, 31.3, 29.7, 28, 28, 
29, 29, 29, 29, 28, 27.5, 27, 26.5, 26, 26, 26, 26, 26, 26, 25, 
25, 25, 25, 25, 24, 24, 23, 23, 23, 24.5, 26, 27, 28, 29, 30, 
31, 32, 32, 32, 33, 34, 35, 36, 36, 36, 36, 36, 36, 36, 37, 37, 
37, 36, 36, 34, 34, 33, 32, 32, 31, 31, 30, 28, 28, 28, 28, 27, 
27, 27, 27, 27, 27, 27, 27, 26.5, 26, 26, 26, 26, 27, 28.8, 30.5, 
32.2, 34, 34, 34, 35, 36, 37, 38, 38, 38, 39, 40, 39, 39, 39, 
40, 40, 40, 39, 39, 38, 37, 35, 35, 34, 34, 34, 33, 32, 32, 32, 
32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 33, 34, 
35, 37, 37, 37.3, 37.7, 38, 39, 40, 43, 43, 43, 44, 44, 43, 43, 
43, 43, 43, 43, 42, 41, 40, 39, 38, 37, 36, 35, 35, 34, 34, 34, 
34, 34, 33, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 34, 
35, 36, 37, 38, 38, 40, 40.5, 41, 42, 42, 42, 42, 41, 40, 40, 
39, 39, 37, 32, 27, 26, 26, 27, 28, 27, 27, 27, 26, 25, 25, 25, 
25, 25, 25, 25, 25, 25, 25, 25, 24, 23, 23, 22.5, 22, 22, 23, 
24, 25, 26.3, 27.7, 29, 30, 32, 33, 34, 36, 37, 37, 39, 40, 40.5, 
41, 42, 42, 42, 43, 43, 42, 41.5, 41, 39, 38, 35, 35, 35, 34, 
33, 33, 33, 33, 32, 32, 31, 31, 30, 30, 30, 29, 29, 28, 28, 28, 
28.5, 29, 29, 29, 30, 32, 32, 34, 35, 37, 38, 39, 40, 41, 42, 
42, 43, 44, 44, 44, 45, 45, 45, 44, 43, 43, 42, 40, 38.5, 37, 
35, 35, 34, 34, 33, 34, 33, 33, 33, 33, 32, 32, 31, 32, 31, 30, 
29, 29, 29, 29, 30, 30, 31, 32.7, 34.3, 36, 37, 38, 39, 40, 41, 
41, 42, 42, 43, 44, 44, 44, 44, 44, 43.5, 43, 43, 42, 42, 40, 
38, 38, 37, 36, 36, 36, 36, 35, 35, 34, 34, 34, 34, 32, 32, 31.5, 
31, 31, 30, 30, 30, 30, 31, 33, 34, 35, 37, 38, 39, 40, 40, 42, 
42, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 42, 40, 40, 
40, 39, 38, 38, 38, 37, 37, 36, 36, 35.5, 35, 34.5, 34, 33, 33, 
32, 32, 31, 30, 30, 30, 31, 32, 33.5, 35, 36.5, 38, 38, 39, 40, 
41, 41, 41, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 
42, 40.5, 39, 38, 38, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 
35, 35, 34, 34, 34, 32, 32, 31, 31, 33, 34, 35, 35, 36, 38, 39, 
40, 41, 42, 42.5, 43, 43, 44, 44, 45, 45, 45, 45, 45, 45, 45, 
44, 43.5, 43, 43, 42, 41, 40, 40, 39, 39, 38, 38, 37, 36, 36, 
35, 34, 34, 34, 32, 32, 32, 32, 32, 32, 32, 34, 35, 35, 37, 38, 
38, 39, 39, 40, 41, 42, 43, 44, 44, 44, 45, 45, 46, 46, 46, 45, 
45, 45, 44, 42, 40, 38, 37, 37, 37, 37, 37, 36, 35), .Dim = c(1248L, 
1L), .Dimnames = list(NULL, ""temperature""))
</code></pre>

<p>Forecasted temperature values are:</p>

<pre><code>forecast_temp &lt;- structure(c(35, 34, 34, 33, 33, 30, 29.7, 29.3, 29, 28, 28, 27, 
27.3, 27.7, 28, 29, 30.5, 32, 33.5, 35, 36, 36, 37, 39, 39.5, 
40, 40, 41, 42, 42, 43, 43, 43, 44, 44, 44, 43, 42, 40, 39, 39, 
38, 37, 37, 36, 35, 35, 34), .Dim = c(48L, 1L), .Dimnames = list(
    NULL, ""temperature""))
</code></pre>

<h3><em>UPDATE-1</em></h3>

<p>From @Stephan's suggestion, I followed the approach mentioned by Prof. Rob at <a href=""http://robjhyndman.com/hyndsight/dailydata/"" rel=""nofollow"">link1</a>, <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">link2</a> as</p>

<pre><code>library(forecast)
tsob &lt;- ts(train_power,frequency = 48) #training electriciy data at daily frequency
tsob_weekly &lt;- fourier(ts(train_power,frequency = 7*48),K=3) #training data at weekly frequency
tempob &lt;- ts(train_temperature,frequency = 48) #Temperature, another predictor variable
fit &lt;- auto.arima(tsob,xreg=cbind(tsob_weekly,tempob),seasonal=FALSE)
tempob_forecast &lt;- ts(test_temperature,frequency = 48) #forecasted temperauture values
forecast_val &lt;- forecast(fit,xreg=tempob_forecast,h=48*5) #forecast for coming 5 days
</code></pre>

<p>As evident from the code, I have used Fourier transformation suggested at above links to show weekly seasonal affect. Up to <code>auto.arima()</code>, it works properly but at forecast function an error is thrown as:</p>

<pre><code>Error in forecast.Arima(fit, xreg = tempob_forecast, h = 48 * 5) : 
  Number of regressors does not match fitted model
</code></pre>

<p>The error is clear, i.e., I do not provide the forecasted regressor values of <code>tsob_weekly</code>, which I don't have. How should I handle this issue? Prof. Rob has used the same value of regressor for both the <code>auto.arima</code> and <code>forecast</code> functions.</p>
"
"0.026822089039291","0.0272670941574606","213233","<p>The following plot is generated using the <code>scatterplot</code> function from <code>R</code>'s <code>car</code> package. </p>

<p>The red ""nonparametric-regression smooth"" generated by <code>loessLine</code> drops way below any values in the data set around 2270 and on the end. What am I to make of this?</p>

<p><a href=""http://i.stack.imgur.com/YkB2D.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YkB2D.png"" alt=""Scatterplot with unexpected low loess regression values""></a></p>

<p>R code generating the plot:</p>

<pre><code>df &lt;- read.table(header=T, text=""X     Y
2040    5.5
2186    4.5
2232    4.6
2238    4.6
2238    4.6
2262    4.3
2272    4
2272    4.4
2272    4.4
2281    4.8
2281    5
2289    4.4
2295    3.6
2318    4.3
2353    4.5"")

require('car')

scatterplot(Y~X,data=df,ylim=c(0,8))
</code></pre>
"
"0.0657004319817604","0.0667904674542028","213253","<p>In general, my question is how to estimate some prediction intervals in the case of penalized linear models (in particular, I think about the glmnet R package). I understood that the introduction of a penalization in the objective function generates a shrinkage effect, which is a bias on the estimated coefficients. 
I understand that in this case the calculation of the uncertainties is troublesome</p>

<p><a href=""https://air.unimi.it/retrieve/handle/2434/153099/133417/phd_unimi_R07738.pdf"" rel=""nofollow"">https://air.unimi.it/retrieve/handle/2434/153099/133417/phd_unimi_R07738.pdf</a></p>

<p>(see sections 3.2 and 3.3 the quoted papers)</p>

<p>Two bootstrap methods (random x vs fixed x) are discussed in the context of standard linear models here</p>

<p><a href=""http://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients?rq=1"">Two ways of using bootstrap to estimate the confidence interval of coefficients in regression</a></p>

<p>but again the focus is on the beta coefficients.
However, I am not interested in the estimate of the confidence intervals on the beta coefficients, but only on the predicted values.
For instance, consider the following R code</p>

<pre><code>library(glmnet)


# Generate data
set.seed(19875)  # Set seed for reproducibility
n &lt;- 1000  # Number of observations
p &lt;- 5000  # Number of predictors included in model
real_p &lt;- 15  # Number of true predictors
x &lt;- matrix(rnorm(n*p), nrow=n, ncol=p)
y &lt;- apply(x[,1:real_p], 1, sum) + rnorm(n)

# Split data into train (2/3) and test (1/3) sets
train_rows &lt;- sample(1:n, .66*n)
x.train &lt;- x[train_rows, ]
x.test &lt;- x[-train_rows, ]

y.train &lt;- y[train_rows]
y.test &lt;- y[-train_rows]



fit.elnet &lt;- glmnet(x.train, y.train, family=""gaussian"", alpha=.5)

yhat &lt;- predict(fit.elnet, s=fit.elnet$lambda, newx=x.test)
</code></pre>

<p>Does anybody know how to calculate a meaningful confidence interval for yhat?</p>

<p>Thanks!</p>
"
"0.0657004319817604","0.0667904674542028","213571","<p>I am trying to do L2-regularized MLR on a data set using caret. Following is what I have done so far to achieve this:</p>

<pre><code>r_squared &lt;-  function ( pred, actual){
    mean_actual = mean (actual)
    ss_e = sum ((pred - actual )^2)
    ss_total = sum ((actual-mean_actual)^2 )
    r_squared = 1 - (ss_e/ss_total)
}

df = as.data.frame(matrix(rnorm(10000, 10, 3), 1000))
colnames(df)[1] = ""response""
set.seed(753)
inTraining &lt;- createDataPartition(df[[""response""]], p = .75, list = FALSE)
training &lt;- df[inTraining,]
testing  &lt;- df[-inTraining,]
testing_response &lt;- base::subset(testing,
                                 select = c(paste (""response"")))
gridsearch_for_lambda =  data.frame (alpha = 0,
                                      lambda = c (2^c(-15:15), 3^c(-15:15)))
regression_formula = as.formula (paste (""response"", ""~ "", "" ."", sep = "" ""))
train_control = trainControl (method=""cv"", number =10,
                              savePredictions =TRUE , allowParallel = FALSE )
model = train (regression_formula,
                           data = training,
                           trControl = train_control,       
                           method = ""glmnet"",
                           tuneGrid =gridsearch_for_lambda,
                           preProcess = NULL
            )
prediction = predict (model, newdata = testing)
testing_response[[""predicted""]] = prediction
r_sq = round (r_squared(testing_response[[""predicted""]],
              testing_response[[""response""]] ),3)
</code></pre>

<p>Here I am concerned about assurance that the model I am using for prediction is the best one (the optimal tuned lambda value).</p>

<p>P.S.: The data is sampled from random normal distribution, which is not giving a good R^2 value, but I want to get the idea correctly</p>
"
"0.026822089039291","0.0272670941574606","213789","<p>I have a data set and its response variable consists of only two results that are success and failure.That's why I used logistic regression method to construct a model. However, I don't know how I determine the outliers or leverage points etc. I used plot() function, but the given outputs are hard to interpret. In addition, I used outliersTest() but it also did not work. How can I detect outliers ? </p>
"
"0.050688983743711","0.072141950116023","213810","<p>I'm trying to build ZINB and hurdle NB models for my data. Everything seems reasonable, but I have some problems by understanding some problems.</p>

<p>1.One of regressors,which should be reasonable gives NA values in models summary in both parts in the model,thought all other regressors seems significant and clear. I tried to put offset(sent) or offset(log(sent)) or offset = sent, but it not helps. In first two ways it gives an error : no valid set of coefficients has been found: please supply starting values . That regressor is integer. Does anyone know the way that I could fix this problem? (All criteria shows better model with that regressor in the model).</p>

<pre><code>                                     Estimate Std. Error z value Pr(&gt;|z|)    

Sent                                  2.048e-05         NA      NA       NA 
 Warning message:
 In sqrt(diag(object$vcov)) : NaNs produced
</code></pre>

<p>2.Is it possible to check the deviance in ZINB and hurdle models? Because in simple NB function deviance() works well and in ZINB it gives NULL. How should I interpret this thing?</p>

<p>3.To find the best model I'm using Vuong criteria, LR test,AIC. Is there some better criteria for models comparison? How to check and interprete residuals for zero-inflated models ?</p>

<p>4.Is it right that for regressor estimates I should use exp(Estimates), not just simple coefficients? Is it the same for both ZINB and hurdle models?</p>

<p>I hope my problems are clear, it's little hard to understand those two models for my, so I really hope for help. Thank you in advance !</p>
"
"0.116914775576919","0.118854507916379","213982","<p>I am trying to use ""propodds""  in the VGAM function in R, but am not sure if I am doing it right and don't really understand how to analyze the output I got so far to check to see if I am using it right. Any help on how to correctly use ""propodds"" or analyze the output would be appreciated. This is what I have so far:</p>

<pre><code>    &gt; fittest &lt;-vglm(rp ~ is.native + is.male + age2 + is.debt + oh + ms + cjs, propodds, data = dummydata2)
&gt; fittest
Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

    Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5     is.native 
  2.827674173  -0.463602645  -0.474290665  -0.614877500  -2.514394420  -0.063546621 
      is.male          age2       is.debt            oh            ms           cjs 
  0.114052675   0.067835161  -0.058563607  -0.089420626   0.109135966   0.003937505 

Degrees of Freedom: 52000 Total; 51988 Residual
Residual deviance: 24702.04 
Log-likelihood: -12351.02 
&gt; summary(fittest)

Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

Pearson residuals:
                    Min      1Q  Median      3Q     Max
logit(P[Y&gt;=2])  -5.1080  0.1300  0.2803  0.3016  0.3335
logit(P[Y&gt;=3])  -0.6976 -0.5876 -0.5490  0.5937 14.7717
logit(P[Y&gt;=4]) -13.1157 -0.5173 -0.4831  0.6080  3.0626
logit(P[Y&gt;=5])  -4.0174 -0.4072 -0.3746  1.0167  1.2176
logit(P[Y&gt;=6])  -0.6164 -0.5749 -0.1610 -0.1541  3.6060

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept):1  2.827674   0.079827  35.423  &lt; 2e-16 ***
(Intercept):2 -0.463603   0.068894  -6.729 1.71e-11 ***
(Intercept):3 -0.474291   0.068901  -6.884 5.83e-12 ***
(Intercept):4 -0.614878   0.069009  -8.910  &lt; 2e-16 ***
(Intercept):5 -2.514394   0.074892 -33.573  &lt; 2e-16 ***
is.native     -0.063547   0.062409  -1.018  0.30857    
is.male        0.114053   0.039694   2.873  0.00406 ** 
age2           0.067835   0.024789   2.737  0.00621 ** 
is.debt       -0.058564   0.052983  -1.105  0.26902    
oh            -0.089421   0.057526  -1.554  0.12008    
ms             0.109136   0.041587   2.624  0.00868 ** 
cjs            0.003938   0.043653   0.090  0.92813    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of linear predictors:  5 

Names of linear predictors: 
logit(P[Y&gt;=2]), logit(P[Y&gt;=3]), logit(P[Y&gt;=4]), logit(P[Y&gt;=5]), logit(P[Y&gt;=6])

Dispersion Parameter for cumulative family:   1

Residual deviance: 24702.04 on 51988 degrees of freedom

Log-likelihood: -12351.02 on 51988 degrees of freedom

Number of iterations: 4 

Exponentiated coefficients:
is.native   is.male      age2   is.debt        oh        ms       cjs 
0.9384304 1.1208112 1.0701889 0.9431182 0.9144608 1.1153140 1.0039453 
</code></pre>

<p>A little background on my data that may help: I am trying to determine if risk preferences (variable ""rp"" in the code) is determined by immigration status (variable ""is.native"" in the code, which is a dummy variable where 0 = native and 1 = immigrant). I have a few factors that I want to control for since they may affect risk preferences [age2, is.debt, oh(owns home), ms (marital status), and cjs (current job status)]. Based on similar research the best way to analyze this is the cumulative logistic regression and they seemed to look at the proportional odds. The data came from the 2014 Health and Retirement Study which is representative of the US population over age 50. There are about 20,000 participants. </p>

<p>I'm not sure if my model is formatted correctly. ""rp"" has 6 categories - a control group, low risk tolerance (rt), some rt, high rt, substantial rt and ""ignore"" which is answers of ""don't know"" or ""NA"".  All other variables are dummy variables with only options for ""0"" or ""1"" besides ""age2"" which has 6 categories (under 50, 50-60, 60-70, 70-80, 80-90, 90+). Are these dummy variables appropriate to use or should I just use the actual answers provided by the participants?</p>

<p>I know the significant codes in the ""summary"" section tell me gender, age, and marital status are significant at the 1% significance level, but I don't understand any of the other results. Such as, what does it mean that all the intercepts are significant? Is the model as a whole significant? What is the dispersion parameter? What are the exponentiated coefficients? </p>
"
"0.0379321620905441","0.0192807471819925","214022","<p>I was trying out the Subselect R package to see how it worked and if it would be useful for a logistic regression problem I'm working on.  <a href=""https://cran.r-project.org/web/packages/subselect/vignettes/subselect.pdf"" rel=""nofollow"">Link</a> to the package.</p>

<p>I decided I would follow Example 4 on page 28 to see if I could perform the Anneal function on the glm I previously fit to my data.  I used the helper function, glmHmat, to extract the required matrices in the same way sa done in example 4.</p>

<pre><code>Fullmodel&lt;-glm(G,family=binomial,data)
Hmat &lt;- glmHmat(Fullmodel)
</code></pre>

<p>I then tried the anneal function and got the following error.</p>

<pre><code>test&lt;-anneal(Hmat$mat,1,10,H=Hmat$H,r=1,nsol=10,criterion = ""Wald"")
</code></pre>

<p>Yet, I got this error.</p>

<pre><code>Error in validmat(mat, p, tolval, tolsym, allowsingular = FALSE, algorithm) : 

 The covariance/total matrix supplied is not symmetric.
  Symmetric entries differ by up to 1.02445483207703e-07.
</code></pre>

<p>So, I thought I would test if this were true:</p>

<pre><code>isSymmetric(Hmat$mat,tol=1e-09)
[1] TRUE
isSymmetric(Hmat$H,tol=1e-09)
[1] TRUE
</code></pre>

<p>So I can't make heads or tails of this error message.  Any ideas?</p>
"
"0.0309714806541255","0.0472279924554862","214665","<p>I have 15-minute streamflow observations for a small stream, but the dataset has some gaps in it. I want to fill the gaps with a regression using observations from a nearby stream (and quantify the uncertainty caused by these gaps). </p>

<p>If I use a linear relationship between the two streams, I get negative predictions for my target stream when the other has low flow, because my target stream dries out before the predictor stream. If I force the regression through zero (it has just a slope), then the model doesnâ€™t fit very well, which influences my results for the uncertainty analysis.</p>

<p>Is a truncated or a linear regression a good solution?  If so, how do I fit the two parameters?</p>

<p>Is there a better model?  It should be zero when my predictor stream has low flow, and then increase roughly in proportion to flow at the predictor stream. I'm working in R if anyone has suggestions for functions.</p>
"
"0.0758643241810882","0.0578422415459774","214682","<p>I am trying to understand the basic difference between stepwise and backward regression in R using the step function. 
For stepwise regression I used the following command </p>

<pre><code>  step(lm(mpg~wt+drat+disp+qsec,data=mtcars),direction=""both"")
</code></pre>

<p>I got the below output for the above code.</p>

<p><a href=""http://i.stack.imgur.com/dpR8s.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dpR8s.png"" alt=""forward""></a></p>

<p>For backward variable selection I used the following command </p>

<pre><code> step(lm(mpg~wt+drat+disp+qsec,data=mtcars),direction=""backward"")
</code></pre>

<p>And I got the below output for backward</p>

<p><a href=""http://i.stack.imgur.com/7hZj9.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7hZj9.jpg"" alt=""backward""></a></p>

<p>As much as I have understood, when no parameter is specified, stepwise selection acts as backward unless the parameter ""upper"" and ""lower"" are specified in R. Yet in the output of stepwise selection, there is a +disp that is added in the 2nd step. What is the function trying to achieve by adding the +disp again in the stepwise selection? Why is R adding the +disp in the 2nd step whereas the results are the same (AIC values and model selection values) as the backward selection. How is R exactly working in the stepwise selection? </p>

<p>I really want to understand how this function is working in R. 
Thanks in advance for the help! </p>
"
"0.0657004319817604","0.0667904674542028","214882","<p>I am currently performing a retrospective study that is comparing a surgical procedure vs a modified version of the same procedure. There is obvious selection bias because of the selection criteria necessary to perform the modified procedure. I was wondering how I would control for these 3 variables (all are simple T/F requirements)? Should I just perform a logistic regression for each dependent variable we are investigating and hope none of them reach significance? Or is there a statistical test that automatically adjusts for these 3 selected covariates?</p>

<p>Initially I did not perform any statistical tests between these groups for this reason, but if we were to prove that these variables are not confounding then I could simply perform the appropriate two sample test, correct?</p>

<p>My Data:</p>

<ul>
<li>Independent Variable (2 groups) = Procedure 1, Procedure 2</li>
<li>We also have multiple dependent variables we want to compare between the two groups: Numerical and Logical (e.g. Length of Stay,
or Re-operation within 30 days, etc.)</li>
<li>But I have 3 Logical variables that I am afraid are confounding.</li>
</ul>

<p>PS. I'm using R to carry out this analysis and any reference to R functions would be a plus. </p>
"
"NaN","NaN","214886","<p>Suppose I'm doing regression with training, validation, and test sets. I can find RMSE and R squared (R^2, the coefficient of determination) from the output of my software (such as R's lm() function).</p>

<p>My understanding is that the test RMSE (or MSE) is the measure of goodness of predicting the validation/test values, while R^2 is a measure of goodness of fit in capturing the variance in the training set.</p>

<p>In the real world, what I really care about is generalized prediction accuracy on data I haven't seen. So then what is the utility of the R^2 value compared to RMSE?</p>
"
"0.0464572209811883","0.0472279924554862","214892","<p>I'm trying to construct a univariate prediction model using logistic regression in order to predict credit default likelihood from overdue level in telecommunication companies:</p>

<p><a href=""https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg"" rel=""nofollow"">https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg</a></p>

<p>For this, I used the function glm and found two problematic ranks:
        RANK_OVERDUE between S/. 3,000 and S/. 5,000 &amp; RANK_OVERDUE More than S/. 5,000.</p>

<p>which have p-values of 0.946 and 0.473:</p>

<pre><code>Call:
glm(formula = impago ~ MONTO_VENCIDO_DOC_IMPAGOS, family = binomial, 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.1355  -0.0569  -0.0569  -0.0569   3.5855  

Coefficients:
                                                  Estimate
(Intercept)                                       -6.42627
RANK_OVERDUE&lt;S/. 0 - S/. 500]         0.69763
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000]   1.73952
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000] -10.13980
RANK_OVERDUE&lt;S/. 500 - 1,500]         1.13854
RANK_OVERDUEMÃ¡s de S/. 5,000          0.71916
</code></pre>

<p></p>

<pre><code>                                                 Pr(&gt;|z|)    
(Intercept)                                       &lt; 2e-16 ***
RANK_OVERDUE&lt;S/. 0 - S/. 500]       1.78e-15 ***
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000] 2.51e-05 ***
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000]    0.946    
RANK_OVERDUE&lt;S/. 500 - 1,500]       1.23e-06 ***
RANK_OVERDUEMÃ¡s de S/. 5,000           0.473    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 9683.9  on 345828  degrees of freedom
Residual deviance: 9603.5  on 345823  degrees of freedom
AIC: 9615.5

Number of Fisher Scoring iterations: 15
</code></pre>

<p>I would need to know what options I have on order to deal with this situation. Should these ranks be included in the model? I tried to join them into one (overdue over S/. 3,000) but when applying again the model, it continued to be not significant (I obtained a p-value of 0.919).</p>
"
"0.0379321620905441","0.0385614943639849","215105","<p>I want to simulate a data set for logistic regression in which my $Y_i \sim Bin(n_i, p_i)$ and $n_i &gt;1 ~ \forall i$. I want something like:</p>

<p><a href=""http://i.stack.imgur.com/Nx0yU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Nx0yU.png"" alt=""enter image description here""></a></p>

<p>In another <a href=""http://stats.stackexchange.com/questions/46523/how-to-simulate-artificial-data-for-logistic-regression"">question</a>, data has been generated for a logistic in which $n_i = 1$. I am confused as to whether it would be correct to follow this method and then bin the $x$ variables and call that a population. I'm not quite sure how to do this without creating some sort of bias in the data that I won't account for in the logistic regression. I'm looking for an explicit description of how to account for $n_i&gt;1$, if possible using R.</p>

<p><strong>EDIT</strong>: Using the code in the question which I've tweaked, here is what I have:</p>

<pre><code>set.seed(1)
x1 &lt;- rnorm(6)           # some continuous variables 
n &lt;- round(runif(6, min = 1, max = 20))
z = 1 + 2*x1                
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y &lt;- matrix(0,6,1)
for( i in 1:6 ) { y[i] &lt;- sum(rbinom(n[i], 1, pr[i]) == 1)}

Y &lt;- y/n
</code></pre>

<p>Are there any reasons this is not a reasonable way of doing things?</p>
"
"0.0910743698189447","0.105811867590468","215224","<p>I am going to explain my question using a reproducibile toy example. I would like to regress a numerical variable using a multiple regression model with either numerical and categorical variables. I would like to do that without using the functions provided by R, but I am worried that I am not coding the categorical variables properly. These are the toy data:</p>

<pre><code>  mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")

  mydata$admit &lt;- factor(mydata$admit)
  mydata$gre &lt;- scale(mydata$gre)
  mydata$gpa &lt;- scale(mydata$gpa)
  mydata$rank &lt;- factor(mydata$rank)

  head(mydata)

          admit        gre        gpa rank
        1     0 -1.7980110  0.5783479    3
        2     1  0.6258844  0.7360075    3
        3     1  1.8378321  1.6031352    1
        4     1  0.4527490 -0.5252692    4
        5     0 -0.5860633 -1.2084607    4
        6     1  1.4915613 -1.0245245    2

  model &lt;- lm(gpa ~. , data=mydata)
  #linear multiple regression
  summary(model)
</code></pre>

<p>This is the result using the lm function:</p>

<pre><code>  Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
  (Intercept) -0.04585    0.12924  -0.355   0.7230    
  admit1       0.24980    0.10273   2.432   0.0155 *  
  gre          0.36816    0.04672   7.879 3.24e-14 ***
  rank2       -0.14424    0.13993  -1.031   0.3033    
  rank3        0.14189    0.14723   0.964   0.3358    
  rank4       -0.13094    0.16620  -0.788   0.4313    
</code></pre>

<p>Manually, I am coding the model matrix X in this way:</p>

<pre><code>  mydata$rank2 &lt;- sapply(mydata$rank, function (x){ if(x==2) return(1) else return(0)})
  mydata$rank3 &lt;- sapply(mydata$rank, function (x){ if(x==3) return(1) else return(0)})
  mydata$rank4 &lt;- sapply(mydata$rank, function (x){ if(x==4) return(1) else return(0)})

  X &lt;- data.matrix(mydata[,-c(3,4)])
  Y &lt;- data.matrix(mydata[,3])
  X[,1] &lt;- X[,1] - 1
</code></pre>

<p>So I am creating for each level a binary variable and I am not considering the first level as I saw in the literature. This is the final matrix</p>

<pre><code>  head(X)
       admit        gre rank2 rank3 rank4
  [1,]     0 -1.7980110     0     1     0
  [2,]     1  0.6258844     0     1     0
  [3,]     1  1.8378321     0     0     0
  [4,]     1  0.4527490     0     0     1
  [5,]     0 -0.5860633     0     0     1
  [6,]     1  1.4915613     1     0     0
</code></pre>

<p>but when I compute the regression coefficients in this way:</p>

<pre><code>  Xbeta &lt;- solve(t(X) %*% X) %*% t(X) %*% Y
</code></pre>

<p>I am obtaining different values compared with the ones obtained with lm.
In particular, these are:</p>

<pre><code>               [,1]
  admit  0.23456544
  gre    0.36804870
  rank2 -0.18463006
  rank3  0.09954882
  rank4 -0.17408055
</code></pre>

<p>What I am doing wrong, please? I would like also to compute the residuals, the sd of the coefficients and the t-statistic, but again I am not obtaining the same results of the lm function for them, and I believe it is due to the fact that I am coding in a wrong way the categorical variables.</p>
"
"0.0758643241810882","0.0771229887279699","215256","<p>I wanted to do something equivalent to a PCA on a mixed data set containing categorical variables and continuous numerical predictor variables which are normally distributed but measured in very different units. The aims are (a) to explore/describe the variable relationships, and (b) hopefully to reduce the dimensions of the data set for predictive modelling. </p>

<p>Based on this <a href=""http://stats.stackexchange.com/questions/5774/can-principal-component-analysis-be-applied-to-datasets-containing-a-mix-of-cont/5777#5777"">cross validated post</a> I have been using (and liking!) the Factor Analysis of Mixed Data function FAMD() of the FactoMineR package in R.  </p>

<p>But I can't work out if this analysis can be treated the same way as a PCA. Two specific questions:</p>

<ol>
<li>I understand the <a href=""http://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia"">need to scale</a> (ie subtract mean and divide by sd) such numerical variables in a PCA to stop some variables unhelpfully dominating the components. But is this necessary in factor analysis of mixed data as done by the FactoMineR package? Running with both scaled and unscaled as supplementary variables seems to show no difference. 

<ol start=""2"">
<li>Can the principle component dimensions of a mixed data analysis be extracted (from a training set) and applied to a test set, as we might with a true PCA? E.g. if I extract the coordinates of each individual in the training set of Dimension1, then run a regression using the original variables predicting Dimension1, and use the coefficients as the weights of each variable to make a new composite 'Dimension 1 variable' which can be applied to the test set variables - would that be valid?</li>
</ol></li>
</ol>
"
"0.0889588054368324","0.0822133822214443","215901","<p><strong>USE CASE</strong></p>

<p>Use R to fit/train a binary classification model, then interpret the model for the purpose of manual calculating classifications in Excel, not R.</p>

<p><strong>MODEL COEFFICIENTS</strong></p>

<pre><code>&gt;coef(model1)
#(Intercept) PetalLength  PetalWidth 
#-31.938998   -7.501714   63.670583 

&gt;exp(coef(model1))
#(Intercept)  PetalLength   PetalWidth 
#1.346075e-14 5.521371e-04 4.485211e+27 
</code></pre>

<p><strong>QUESTIONS</strong></p>

<p>(1) what is the classification formula from the fit model in example code below named '<em>model1</em>'?. (is it formula A, B or Neither)? </p>

<p>(2) how does '<em>model1</em>' determine if class == 1 vs. 2?</p>

<ul>
<li>Formula A: 

<blockquote>
  <p>class(Species{1:2}) = (-31.938998) + (-7.501714 * [PetalLength]) + (63.670583 * [PetalWidth])</p>
</blockquote></li>
<li>Formula B: 

<blockquote>
  <p>class(Species{1:2}) = 1.346075e-14 + (5.521371e-04 * [PetalLength]) + (4.485211e+27 * [PetalWidth])</p>
</blockquote></li>
</ul>

<p><strong>R CODE EXAMPLE</strong></p>

<pre><code># Load data (using iris dataset from Google Drive because uci.edu link wasn't working for me today)
#iris &lt;- read.csv(url(""http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data""), header = FALSE)
iris &lt;- read.csv(url(""https://docs.google.com/spreadsheets/d/1ovz31Y6PrV5OwpqFI_wvNHlMTf9IiPfVy1c3fiQJMcg/pub?gid=811038462&amp;single=true&amp;output=csv""), header = FALSE)
dataSet &lt;- iris

#assign column names
names(dataSet) &lt;- c(""SepalLength"", ""SepalWidth"", ""PetalLength"", ""PetalWidth"", ""Species"")

#col names
dsColNames &lt;- as.character(names(dataSet))

#num of columns and rows
dsColCount &lt;- as.integer(ncol(dataSet))
dsRowCount &lt;- as.integer(nrow(dataSet))

#class ordinality and name
classColumn &lt;- 5 
classColumnName &lt;- dsColNames[classColumn]
y_col_pos &lt;- classColumn

#features ordinality
x_col_start_pos &lt;- 1
x_col_end_pos &lt;- 4

# % of [dataset] reserved for training/test and validation  
set.seed(10)
sampleAmt &lt;- 0.25
mainSplit &lt;- sample(2, dsRowCount, replace=TRUE, prob=c(sampleAmt, 1-sampleAmt))

#split [dataSet] into two sets
dsTrainingTest &lt;- dataSet[mainSplit==1, 1:5] 
dsValidation &lt;- dataSet[mainSplit==2, 1:5]
nrow(dsTrainingTest);nrow(dsValidation);

# % of [dsTrainingTest] reserved for training
sampleAmt &lt;- 0.5
secondarySplit &lt;- sample(2, nrow(dsTrainingTest), replace=TRUE, prob=c(sampleAmt, 1-sampleAmt))

#split [dsTrainingTest] into two sets 
dsTraining &lt;- dsTrainingTest[secondarySplit==1, 1:5]
dsTest &lt;- dsTrainingTest[secondarySplit==2, 1:5]
nrow(dsTraining);nrow(dsTest);

nrow(dataSet) == nrow(dsTrainingTest)+nrow(dsValidation)
nrow(dsTrainingTest) == nrow(dsTraining)+nrow(dsTest)

library(randomGLM)

dataSetEnum &lt;- dsTraining[,1:5]
dataSetEnum[,5] &lt;- as.character(dataSetEnum[,5])
dataSetEnum[,5][dataSetEnum[,5]==""Iris-setosa""] &lt;- 1 
dataSetEnum[,5][dataSetEnum[,5]==""Iris-versicolor""] &lt;- 2 
dataSetEnum[,5][dataSetEnum[,5]==""Iris-virginica""] &lt;- 2 
dataSetEnum[,5] &lt;- as.integer(dataSetEnum[,5])

x &lt;- as.matrix(dataSetEnum[,1:4])
y &lt;- as.factor(dataSetEnum[,5:5])

# number of features
N &lt;- ncol(x)

# define function misclassification.rate
if (exists(""misclassification.rate"") ) rm(misclassification.rate);
misclassification.rate&lt;-function(tab){
  num1&lt;-sum(diag(tab))
  denom1&lt;-sum(tab)
  signif(1-num1/denom1,3)
}

#Fit randomGLM model - Ensemble predictor comprised of individual generalized linear model predictors
RGLM &lt;- randomGLM(x, y, classify=TRUE, keepModels=TRUE,randomSeed=1002)

RGLM$thresholdClassProb

tab1 &lt;- table(y, RGLM$predictedOOB)
tab1
# y  1  2
# 1  2  0
# 2  0 12

# accuracy
1-misclassification.rate(tab1)

# variable importance measure
varImp = RGLM$timesSelectedByForwardRegression
sum(varImp&gt;=0)

table(varImp)

# select most important features
impF = colnames(x)[varImp&gt;=5]
impF

# build single GLM model with most important features
model1 = glm(y~., data=as.data.frame(x[, impF]), family = binomial(link='logit'))

coef(model1)
#(Intercept) PetalLength  PetalWidth 
#-31.938998   -7.501714   63.670583 

exp(coef(model1))
#(Intercept)  PetalLength   PetalWidth 
#1.346075e-14 5.521371e-04 4.485211e+27 

confint.default(model1)
#                2.5 %   97.5 %
#(Intercept) -363922.5 363858.6
#PetalLength -360479.0 360464.0
#PetalWidth  -916432.0 916559.4
</code></pre>
"
"0.0405511869949688","0.0515299643685879","216005","<p>I am going to try to give as much information as possible. 
I have a data base <a href=""http://i.stack.imgur.com/rgrDc.png"" rel=""nofollow""><code>describe(Df)</code></a></p>

<p>So all binary variables except one with 1,2,3. 
Therefore I wanted to do a probit model out of it with y113 being endogenous. 
However, I want to use the lavaan package for SEM because I want to add variables and links to this model later. </p>

<p>So I created a model: </p>

<pre><code>&gt;Model2 &lt;- 'y113~x14+x15+x16'
From which I get from &gt;sem(Model2,data=shortD,ordered=c(""y113"",""x14"",""x15"",""x16""),estimator = ""WLSMV"")
: 

lavaan (0.5-20) converged normally after  19 iterations

                                  Used       Total
  Number of observations   .............   1671 ......... 1672

  Estimator  .................................. .   DWLS ..... Robust

  Minimum Function Test Statistic.. 0.000   .......    0.000

  Degrees of freedom   ..  ..................0      ..............     0

  Minimum Function Value  ............. 0.0000000000000

 Scaling correction factor           ................................   NA

and
Regressions:

                   Estimate   Std.Err   Z-value   P(&gt;|z|)

  y113 ~   

    x14               0.101    0.050    2.009    0.045
    x15               0.047    0.095    0.497    0.619
    x16              -0.381    0.076   -5.038    0.000

Intercepts:

                   Estimate  Std.Err  Z-value  P(&gt;|z|)
    y113              0.000                           

Thresholds:

                   Estimate  Std.Err  Z-value  P(&gt;|z|)
    y113|t1           0.923    0.089   10.372    0.000

Variances:

                   Estimate  Std.Err  Z-value  P(&gt;|z|)
    y113              1.000                           

Scales y*:

                   Estimate  Std.Err  Z-value  P(&gt;|z|)
    y113              1.000                           
</code></pre>

<p>So this table gives values that don't look too real, however I this is my first attempt at SEM so I am not comfortable with it.So I wanted to know how well fitted my model is : fitMeasures(sem)</p>

<pre><code>                     npar                          fmin 
                    4.000                         0.000 
                    chisq                            df 
                    0.000                         0.000 
                   pvalue                  chisq.scaled 
                       NA                         0.000 
                df.scaled                 pvalue.scaled 
                    0.000                            NA 
     chisq.scaling.factor                baseline.chisq 
                       NA                        29.660 
              baseline.df               baseline.pvalue 
                    3.000                         0.000 
    baseline.chisq.scaled            baseline.df.scaled 
                   29.487                         3.000 
   baseline.pvalue.scaled baseline.chisq.scaling.factor 
                    0.000                         1.007 
                      cfi                           tli 
                    1.000                         1.000 
                     nnfi                           rfi 
                    1.000                         1.000 
                      nfi                          pnfi 
                    1.000                         0.000 
                      ifi                           rni 
                    1.000                         1.000 
               cfi.scaled                    tli.scaled 
                    1.000                         1.000 
              nnfi.scaled                    rfi.scaled 
                    1.000                         1.000 
               nfi.scaled                    ifi.scaled 
                    1.000                         1.000 
               rni.scaled                         rmsea 
                    1.000                         0.000 
           rmsea.ci.lower                rmsea.ci.upper 
                    0.000                         0.000 
             rmsea.pvalue                  rmsea.scaled 
                    1.000                         0.000 
    rmsea.ci.lower.scaled         rmsea.ci.upper.scaled 
                    0.000                         0.000 
      rmsea.pvalue.scaled                          wrmr 
                    1.000                         0.000 
                    cn_05                         cn_01 
                    1.000                         1.000 
                      gfi                          agfi 
                    1.000                         1.000 
                     pgfi                           mfi 
                    0.000                         1.000
</code></pre>

<p>Still did not expect to see fmin = 0 or chisq = 0 or pvalue = NA, or anything like those. </p>

<p>I expect to have made mistakes in my programming. 
Do these results make sense ?</p>

<p>(I wish to help as many people that I can with my question (and your answers) so please help me improve it if needed :) )</p>

<p>If you need anymore information just let me know !</p>
"
"0.026822089039291","0.0272670941574606","216119","<p>I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket.</p>

<pre><code>  logitshots &lt;- glm(df$shot_made_flag ~ df$shot_distance, family = binomial(link=""logit""))
Call:  glm(formula = df$shot_made_flag ~ df$shot_distance, family = binomial(link = ""logit""))

Coefficients:
 (Intercept)  df$shot_distance  
      0.3681           -0.0441  

Degrees of Freedom: 25696 Total (i.e. Null);  25695 Residual
Null Deviance:      35330 
Residual Deviance: 34290    AIC: 34300
</code></pre>

<p>As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. </p>

<p>To do so I have done it this way, but I get a positive effect, so I am not sure about it. </p>

<pre><code>(exp(coef(logitshots))/(1+exp(coef(logitshots))))
(Intercept) df$shot_distance 
   0.5909933        0.4889768 
</code></pre>

<p>So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?)  </p>

<p>I'd really appreciate any interesting insight and help! :)</p>
"
"0.0402331335589365","0.0545341883149212","216310","<p>I do not know much about regression, and, in order to practise, I need to code this different regressions in R or Matlab, for the next function:</p>

<p>$$y=\alpha+\beta_1 x_1+\beta_2 x_2+\epsilon$$ where $x_1, x_2$ are uniform variables and $\epsilon$ is normally distributed with mean zero. </p>

<ul>
<li>OLS: $\displaystyle \min \sum_i\left(y_i - x_i^T \beta\right)^2 = ||y-X\beta||_2^2$</li>
<li>Least-absolute regression: $\displaystyle \min ||y-X\beta||_1 = \sum_i |y_i-x_i^T\beta|$</li>
<li>Chebyshev regression: $ \min ||y-X\beta||_{\inf} \equiv \min \max |y_i-x_i^T\beta|$</li>
<li>Ridge regression: $\min ||y-X\beta||_2^2 + \rho ||\beta||_2^2$. You can estimate $\rho$ by cross-validation.</li>
<li>Lasso regression: $||y-X\beta||_2^2 + \lambda ||\beta||_1$, You can estimate $\lambda$ by cross-validation.</li>
<li>Forward regression: you can estimate the final model by the BIC.</li>
<li>Backward regression: you can estimate the final model by the BIC. </li>
</ul>

<p>Thansk a lot. Any help will be very useful for me. </p>
"
"0.0379321620905441","0.0385614943639849","217972","<p>Hoping for some help related to a survival analysis using R and the <code>survival</code> package. I've been relying heavily on a series of blog posts done by Dayne Batten, particularly this portion:
[<a href=""http://daynebatten.com/2015/12/survival-analysis-customer-churn-time-varying-covariates/]"" rel=""nofollow"">http://daynebatten.com/2015/12/survival-analysis-customer-churn-time-varying-covariates/]</a></p>

<p>I've collected and merged the data as instructed using the <code>tmerge</code> function. My model relies on a cumulative time-dependent covariate, which strays away from the example provided. So my first question is does a cumulative covariate affect the validity of the Cox Regression? Here is my code at the moment:</p>

<pre><code>fit &lt;- coxph( Surv(tstart, tstop, had_event) ~ review_event, data = newdatatestcum)
</code></pre>

<p>My second question pertains to the lack of an ID being assigned within this model. For each customer ID within this data I have up to a few hundred lines of events with my covariate. I don't see how this regression could possibly be accounting for that.</p>
"
"0.0464572209811883","0.0472279924554862","218676","<p>I'm running a maximum likelihood of a logit regression, but the estimated parameters value and the loglikelihood value are depending on the value of the algorithm's start. For example, if my start is a vector of 0's I get one value, ifI change that I get another. Any way to avoid that?</p>

<p>I'm running the optimization with the maxLik package. (Sorry for the function is huge)</p>

<pre><code>    g = function(b){
    bb = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
             + b[7]*DENSIDADE - exp(b[8])*p.br - exp(b[9])*p.bs - exp(b[10])*p.bc - exp(b[11])*p.bi - exp(b[12])*p.bh)

      caixa = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
                + b[7]*DENSIDADE - exp(b[13])*p.br- exp(b[14])*p.bs - exp(b[15])*p.bb - exp(b[16])*p.bi - exp(b[17])*p.bh)

      bradesco = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
                   + b[7]*DENSIDADE - exp(b[18])*p.bb- exp(b[19])*p.bs - exp(b[20])*p.bc - exp(b[21])*p.bi - exp(b[22])*p.bh)

      hsbc = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
               + b[7]*DENSIDADE- exp(b[23])*p.br- exp(b[24])*p.bs - exp(b[25])*p.bc - exp(b[26])*p.bi - exp(b[27])*p.bb)

               itau = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
               + b[7]*DENSIDADE - exp(b[28])*p.br - exp(b[29])*p.bs - exp(b[30])*p.bc - exp(b[31])*p.bb - exp(b[32])*p.bh)

      santander = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
                    + b[7]*DENSIDADE - exp(b[33])*p.br - exp(b[34])*p.bb - exp(b[35])*p.bc - exp(b[36])*p.bi - exp(b[37])*p.bh)

      l = sum(BB*(log(bb))
      +(1-BB)*(log(1 - bb))
      + CAIXA*(log(caixa))
      +(1-CAIXA)*(log(1 - caixa ))

      + BRADESCO*(log(bradesco))
      +(1-BRADESCO)*(log(1 - bradesco))

      + HSBC*(log(hsbc))
      +(1-HSBC)*(log(1 - hsbc))

      + ITAU*(log(itau))
      +(1-ITAU)*(log(1 - itau))

      + SANTANDER*(log(santander))
      +(1-SANTANDER)*(log(1 - santander))

      )
      return(l)

    }


    ll = maxLik(logLik = g, start = rep(0,46), method = ""BFGS"", control = list(tol = 1e-20, iterlim = 1000))
</code></pre>

<p>Output:
Maximum Likelihood estimation
BFGS maximization, 179 iterations
Return code 0: successful convergence 
Log-Likelihood: -11065.77 (46 free parameter(s))
Estimate(s): -4.278447 0.002979005 4.625648 8.724918e-05 -0.04196911 -0.01600133 -6.890406e-06 -1.14766 -0.6503709 -1.265141 -1.051562 -1.08185 -0.9048582 -0.5394084 -0.5875544 -0.30975 -0.1041046 -1.87203 -1.706692 -0.4717443 -0.8142002 -0.3034415 -2.413816 0.1269036 -1.353817 -1.455872 1.122101 -0.7207807 0.2147704 -0.1741227 -0.9657993 0.5873628 -1.392237 0.4828495 -0.3115696 -2.190175 -0.5272591 0.8638808 1.085701 0.4827111 0.6055928 1.302957 0.377728 -1.119531 -0.4588106 -0.6437521 </p>
"
"0.108400182289439","0.120693758497354","219288","<p>UPDATE: This problem was (embarrassingly) solved by specifying the intercept in the regression equation as shown below: </p>

<pre><code>lcs ~ 1 + 0*Y1_bl_ctr #gamma is set to 0 for equivilance with the t-test
</code></pre>

<hr>

<p>This question is distinct enough from (<a href=""http://stats.stackexchange.com/questions/219040/change-score-model-in-lavaan"">Change Score Model in lavaan</a>) that I am migrating it here, but I am including the link for reference purposes. </p>

<p>At the aforementioned post, I was attempting to calculate a latent change score for two waves of observation. As indicated there, I believe the model I specified was over-identified. </p>

<p>Having thought about this for a day and reading a little more, I think I may have been over-complicating the problem.</p>

<p>In a recent paper by Colman and colleagues (<a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3794455/"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3794455/</a>), it was shown how under certain conditions a latent change score model is equivalent to a simple t-test. </p>

<p>The authors specify a path model in the paper as follows: </p>

<p><a href=""http://i.stack.imgur.com/X6Z3U.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/X6Z3U.jpg"" alt=""enter image description here""></a>
As noted in the path diagram, the authors state that the model is equivalent to a paired sample t-test when $\gamma=0$. </p>

<p>The authors additionally make the following constraints on the model: </p>

<ol>
<li>No $Y_2$ residual error; </li>
<li>Intercept of $Y_2$ is set to zero; </li>
<li>The auto-regressive path is set to 1; </li>
<li>The $LCS$ to $Y_2$ path is set to 1.</li>
</ol>

<p>NOTE: Within the body of the paper, the authors also specify that they are baseline-centering the $Y_1$ and $Y_2$ values. </p>

<p>The authors supply the data for their paper at (<a href=""http://trippcenter.uchc.edu/modeling"" rel=""nofollow"">http://trippcenter.uchc.edu/modeling</a>). For convenience, I provide the <code>dput</code> output of the data in the following code chunk and assign it to an object named <code>dat</code>.</p>

<pre><code>dat &lt;- structure(list(Y1 = c(7.6, 8.4, 8.4, 8.7, 9.7, 10.9, 9.7, 7.4, 
7.9, 9.3, 10.3, 11.3, 14, 8.6, 13.3, 10.9, 7.9, 9.5, 9.9, 11.5, 
11.8, 9.9, 8.8, 10.6, 11.3, 7.6, 8, 8.1, 8.6, 8.2, 7.4, 8.5, 
9.3, 9.4, 10.6, 10.3, 10.8, 8.2, 10.1, 7.8, 7.2, 7.8, 8.1, 8.1, 
8.2, 9.1, 8.7, 8.6, 7.6, 8.8, 10.3, 11.3, 7.7, 7.8, 8.5, 8.4, 
8.3, 9.2, 9.9, 9.8, 10.9, 7.8, 10.1, 9.5, 8, 8.3, 8.2, 7.8, 8.8, 
9.4, 8.7, 8.8, 10.8, 11.5, 7.6, 7.6, 7.9, 8.2, 9.7, 10, 8.8, 
10, 12.7, 7.5, 13.4, 8.3, 13.8, 14, 8.4, 14, 10, 9.5, 6.2, 11.7, 
9.7, 14, 7.3), Y2 = c(7.9, 7.8, 5.9, 7.5, 7.5, 9.1, 8, 7.9, 6.7, 
8.1, 8.5, 12, 14, 6.9, 10, 10.9, 7.2, 8.9, 10.8, 7.1, 7.4, 9.7, 
10.3, 9.3, 11.6, 8, 7.1, 6.7, 8.2, 9.3, 7.6, 9.9, 8.9, 8.8, 7.2, 
10.1, 6.7, 6.2, 8.9, 7.3, 7.6, 7.5, 7.3, 9.6, 8.1, 7.8, 8.7, 
8.4, 11.4, 9, 10.2, 12.5, 7, 8.7, 8, 7.2, 8.9, 10.4, 9.4, 10.8, 
9.9, 6.3, 5.7, 10.1, 7.8, 8.2, 7.4, 7.7, 11.8, 7.1, 6.8, 8.1, 
9.2, 10.2, 8.4, 7.1, 9, 6.9, 8.7, 8.8, 9.3, 8.6, 8.5, 7.7, 13.8, 
8.7, 10.5, 14, 10.1, 14, 14, 9, 14, 14, 10.3, 14, 8.4)), .Names = c(""Y1"", 
""Y2""), class = ""data.frame"", row.names = c(NA, -97L))
</code></pre>

<p>In <code>laavan</code> I am trying to implement the model as follows: </p>

<pre><code># baseline mean center Y1 and Y2
dat$Y1_bl_ctr = dat$Y1 - mean(dat$Y1)
dat$Y2_bl_ctr = dat$Y2 - mean(dat$Y1)

test &lt;- '
  # measurement model
    lcs =~ 1*Y2_bl_ctr #4 - the LCS to Y2 path is set to 1
  # regressions
    lcs ~ 0*Y1_bl_ctr #gamma is set to 0 for equivilance with the t-test
    Y2_bl_ctr ~ 1*Y1_bl_ctr #3 - The auto-regressive path is set to 1
  # residual error
    lcs ~~ 1*lcs 
    Y2_bl_ctr ~~ 0*Y2_bl_ctr #1 - No Y2 residual error
'


summary(test &lt;- lavaan(test
                       ,data=dat
                       ,int.lv.free = TRUE #intercepts of LCS is to be estimated
                       ,int.ov.free = FALSE #2- Intercept of Y2 is set to zero;
                       )
        )
</code></pre>

<p>As is probably obvious, this overly-restricted specification results in an unestimated model.</p>

<pre><code>#Error in lav_syntax_parse_rhs(rhs = rhs.formula[[2L]], op = op) : 
#  lavaan ERROR: I'm confused parsing this line: offsetY2_bl_ctr 
</code></pre>

<p>If I respecify the measurement model as <code>lcs =~ Y2_bl_ctr</code>, I get the following output from <code>laavan</code>.</p>

<pre><code>#lavaan (0.5-20) converged normally after   7 iterations
#
#  Number of observations                            97
#
#  Estimator                                         ML
#  Minimum Function Test Statistic               11.543
#  Degrees of freedom                                 1
#  P-value (Chi-square)                           0.001
#
#Parameter Estimates:
#
#  Information                                 Expected
#  Standard Errors                             Standard
#
#Latent Variables:
#                   Estimate  Std.Err  Z-value  P(&gt;|z|)
#  lcs =~                                              
#    Y2_bl_ctr         1.780    0.128   13.928    0.000
#
#Regressions:
#                   Estimate  Std.Err  Z-value  P(&gt;|z|)
#  lcs ~                                               
#    Y1_bl_ctr         0.000                           
#  Y2_bl_ctr ~                                         
#    Y1_bl_ctr         1.000                           
#
#Variances:
#                   Estimate  Std.Err  Z-value  P(&gt;|z|)
#    lcs               1.000                           
#    Y2_bl_ctr         0.000  
</code></pre>

<p>However, I still don't appear to get an intercept estimated - just the $Y_2$ to $LCS$ path coefficient. </p>

<pre><code>t.test(dat$Y1,dat$Y2,paired=TRUE)

#   Paired t-test
#
#data:  dat$Y1 and dat$Y2
#t = 2.1734, df = 96, p-value = 0.03221
#alternative hypothesis: true difference in means is not equal to 0
#95 percent confidence interval:
# 0.03423662 0.75545410
#sample estimates:
#mean of the differences 
#              0.3948454
</code></pre>

<p>The 0.395 value is the correct value based on the paper. </p>

<p>Any thoughts on how this model can be specified in <code>laavan</code> in order to produce equivalent results to the t-test?</p>
"
"0.0848188929679971","0.0862261227118454","219304","<p>I am examining social interaction data in individuals within two groups. Each social encounter has been coded to one of 4 categories, and these encounters are nested within individual, whom are nested within groups. The number of social encounters per individual is variable and my groups are unequal sample sizes. </p>

<p>I want to examine whether the proportion of social encounters in different categories significantly differ as a function of group. I previously examined a different DV in this data that was continuous, not categorical, and used a multilevel model in R (nlme package) to do so (data nested within individuals within groups). I have done some looking online and as far as I can tell, R should be able to run a multilevel model with categorical dependent variable as well. (i.e., <a href=""http://www.upa.pdx.edu/IOA/newsom/mlrclass/ho_binary.pdf"" rel=""nofollow"">http://www.upa.pdx.edu/IOA/newsom/mlrclass/ho_binary.pdf</a>). However, I am not sure how to implement this and I think that the naming online is inconsistent (some sources referring to this analysis as MLM with categorical variable, others calling it a multinomial logistic regression). </p>

<p>Is it possible to modify my current R script for continuous DV so that it analyzes for a categorical DV instead? Or do I need a different script? Thank you in advance for any help.</p>
"
"0.053644178078582","0.0545341883149212","219390","<p><a href=""http://i.stack.imgur.com/fUmBg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fUmBg.png"" alt=""enter image description here""></a></p>

<p>This is a graph of revenues for different products with the Y-axis showing normalized revenues (mean of 3 and SD of 1) and X-axis is weeks. I need do a regression analysis of sorts on this data and am unsure how to find a curve/function in R that fits this data. </p>

<p>The data points can be interpreted as being: Week 0 of product release yield normalized revenues between 2.25 to 3.25, etc.</p>

<p>Any help regarding what kind of statistical analysis I can use to create a regression model (linear and logistic wouldn't work clearly) with the end goal being to do predictive analysis (ie. if a new product is released, what normalized revenues would it yield in the first 6 weeks)</p>

<p>Thanks</p>
"
"0.0929144419623766","0.0944559849109725","219679","<p>I would like to know how to find out the analytical solution of a simple linear regression with fixed intercept = 0:</p>

<p>$$ s = e^{-ht}$$
$$ y = -ln(s)  = h\cdot t$$</p>

<p>Here ist the background: I have three survival probabilities $s$ at 30, 90 and 180 days. Obviously, I have at day = 0 100% survival, so I  include this <em>observation</em>. I know that this is contested (<a href=""http://stats.stackexchange.com/questions/102709/when-forcing-intercept-of-0-in-linear-regression-is-acceptable-advisable"" title=""here"">here</a>) but I think in this special case it makes sense. The data I use for fitting the linear regression:</p>

<pre><code>&gt;     obs
    t    s          y
1   0 1.00 0.00000000
2  30 0.98 0.02020271
3  90 0.90 0.10536052
4 180 0.80 0.22314355
</code></pre>

<p>If I fit with simple regression I get this:</p>

<pre><code>&gt;     (fit1 &lt;- lm(y~t, data=obs))

Call:
lm(formula = y ~ t, data = obs)

Coefficients:
(Intercept)            t  
  -0.008464     0.001275
</code></pre>

<p>This can be obtained analytically if the following function is derived:</p>

<p>$$f(h) = \sum (y_i - ht_i)^2$$</p>

<p>which gives:</p>

<p>$$ \frac{\sum (y_i-\bar{y})\cdot (t_i-\bar{t})}{\sum (t_i-\bar{t})^2}$$</p>

<hr>

<p>UPDATE 1: This is the result of the minimization of 
$$f(h) = \sum (y_i - c - ht_i)^2$$. The correct result (see answers):
$$ \frac{\sum (y_i\cdot t_i)}{\sum t_i^2}$$</p>

<hr>

<p>The analytical results is:</p>

<pre><code>yc &lt;- with(obs,y-mean(y))
tc &lt;- with(obs, t -  mean(t))
sum(yc*tc)/sum(tc^2)
[1] 0.001275204
</code></pre>

<p>The same as coefficient in the fit1. Now, if I fix intercept to intercept=0 I get this:</p>

<pre><code>&gt;     (fit2 &lt;- lm(y~0+t, data=obs))

Call:
lm(formula = y ~ 0 + t, data = obs)

Coefficients:
   t  
0.001214  
</code></pre>

<p>I'm wondering how I can get an analytical solution for this. How I have to consider the fix intercept in the function $f(h)$ above?</p>

<p>Any idea is appreciated.</p>

<p><a href=""http://i.stack.imgur.com/ilwvG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ilwvG.png"" alt=""enter image description here""></a></p>

<hr>

<p>Here is the way I constructed the data:</p>

<pre><code>set.seed(123)
# Hazard ratio
h &lt;- 0.7
# Number of observation
n &lt;- 50
# Model: exponential
t &lt;- rexp(n,h)
# scale to days
t &lt;- t*365.25
hist(t)
t &lt;- sort(t)
# Put data into a dataframe
df0 &lt;- data.frame(t=t)
head(df0)
# Compute probablities
df0$s &lt;- 1 - c(1:n)/n
head(df0)
# Extract survival probabilities at 30,90 and 180 days
df0$t2 &lt;- ceiling(df0$t/30)*30
# Select survival probablity 30, 90, 180 days
library(sqldf)
obs &lt;- sqldf(""SELECT t2 t, MAX(s) s FROM df0 WHERE t2 IN (30,90,180) GROUP BY t2"")
# Add survival probability=1 at day 0
obs &lt;- rbind(data.frame(t = 0, s = 1), obs)
# s = e^(-ht)  =&gt; y = -ln(s) = h*t
obs$y &lt;- -log(obs$s)
plot(y~t, data=obs)
fit1 &lt;- lm(y~t, data=obs)
abline(fit1,lty=2)
fit2 &lt;- lm(y~0+t, data=obs)
abline(fit2,lty=2, col=""red"")
legend(""topleft"", legend=c(""fit1"",""fit2""), col=c(1,2), lty=c(2,2))
</code></pre>
"
"0.0663812836584521","0.0674826151369737","219857","<p>I have a series of plots, showing time delay against saturation ratio. For each plot I perform a third order polynomial regression to relate time delay to saturation ratio. This generates a function illustrated as the red line. As shown in Figure 1. </p>

<p>The regression function has a corresponding R$^2$ adjusted value, also shown in Figure 1. For comparison, a BPR function is taken from the literature and also plotted. </p>

<p><a href=""http://i.stack.imgur.com/8wRBD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8wRBD.png"" alt=""Figure 1""></a></p>

<p>I have around 40 of these plots. Each consisting of an individual road. For each road I perform a third order polynomial regression to generate a function. </p>

<p>I wish to assess the collective value of this method and display the collective R$^2$ values in some way.</p>

<p>I have looked at using a density plot in order to do this. </p>

<p>Using ggplot I plotted the range of R$^2$ adjusted values, as shown in Figure 2.</p>

<p><a href=""http://i.stack.imgur.com/hPkf3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hPkf3.png"" alt=""Figure 2""></a></p>

<p>This appears to show a significant number of functions fall between 0.4 and 0.7 R$^2$ adjusted values.</p>
"
"0.113796486271632","0.115684483091955","220001","<p>I'm trying to model a logistic regression in R between two simple variables:</p>

<ul>
<li>Rating: An independent ordered categorical one, ranging from 1 to 99 (1, 2, 3, 4, 5, 99 in particular, 1 is the best)</li>
<li>Result: A dependent binary variable (0-1, not accepted/accepted)</li>
</ul>

<p>The formula I use is </p>

<pre><code>glm(formula = result_dummy ~ best_rating, family = binomial(link = ""logit""), 
    data = cd[1:10000, ])
</code></pre>

<p>result_dummy is a 0/1 numerical variable (original result column was a factor) and scaled_rating is the rating column after use the R <code>scale</code> function.</p>

<p>My thought here was to find a negative correlation (low rating -> more probability to accept) but the more samples I use the more odd results I find:</p>

<pre><code>10 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.6484     0.7413   0.875    0.382
scaled_rating  -5.9403     5.8179  -1.021    0.307
</code></pre>

<hr>

<pre><code>100 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)   -0.09593    0.27492  -0.349  0.72714   
scaled_rating -5.06251    1.76645  -2.866  0.00416 **
</code></pre>

<hr>

<pre><code>1000 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.03539    0.09335  -0.379    0.705    
scaled_rating -6.81964    0.62003 -10.999   &lt;2e-16 ***
</code></pre>

<hr>

<pre><code>10000 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     0.2489     0.0291   8.553   &lt;2e-16 ***
scaled_rating  -7.2319     0.2004 -36.094   &lt;2e-16 ***
</code></pre>

<hr>

<p>Notes:
I know that after the fit I should check residual plot, normality assumptions, etc. etc. but nonetheless I find really strange this behaviour.</p>

<p>I also have similar results using simply the rating column instead of the scaled one.</p>

<p>Edit:
The <code>rating</code> variable is not really an ordinal one, so as pointed out by @Scortchi maybe it would be better to treat it as a categorical one.
I have surely better results and model stability, obviously the model is a simple one and the residual error would be always high (because some variables as not been included in the model).
Indeed, including the frequency table as requested shows that the rating variable IS NOT sufficient for having a clear separation between the result outcome.</p>

<pre><code>          0      1
  1    2881  42564
  2   13878 129292
  3   36839 179500
  4   43511  97148
  5   37330  47002
  6   31801  21228
  7   19096   6034
  99  10008      3
</code></pre>
"
"0.0892693083195917","0.0907503748778111","220317","<p>I'm running a fixed-effects Poisson regression and get different results in Stata and R. Unfortunately I cannot upload and share the data due to legal restrictions.</p>

<p>My code in R is (formula shortened for illustration):</p>

<pre><code>library(pglm)    
pdf &lt;- pdata.frame(data, index=c(""id"",""timevar""))    
model &lt;- pglm(y ~ x1 + x3 + x3_lag + x3_lag2 + season + x4 + x1*x3_lag + x1*x3_lag2+x1*x4,
              data = pdf,
              effect = ""individual"",
              model = ""within"",
              family = ""poisson"")
</code></pre>

<p>where <code>season</code> controls for seasonal effects, <code>Y</code> is a count variable and <code>x3</code> is a treatment. </p>

<p><code>summary(model)</code> yields this:</p>

<pre><code>Maximum Likelihood estimation
Newton-Raphson maximisation, 4 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -47369.66 
19  free parameters
Estimates:
                   Estimate Std. error t value Pr(&gt; t)
x1            -1.031e-04        Inf       0       1
x3            -1.196e-02        Inf       0       1
x3_lag        -4.783e-02        Inf       0       1
x3_lag2       -5.159e-02        Inf       0       1
season02      -7.038e-02        Inf       0       1
season03       9.323e-02        Inf       0       1
season04       1.257e-01        Inf       0       1
season05       1.427e-01        Inf       0       1
season06      -1.217e-01        Inf       0       1
season07      -1.566e+01        Inf       0       1
season08      -2.095e-01        Inf       0       1
season09      -1.886e-01        Inf       0       1
season10       4.488e-02        Inf       0       1
season11      -9.954e-02        Inf       0       1
season12       8.201e-02        Inf       0       1
x4             6.055e-01        Inf       0       1
x1:x3_lag      1.888e-05        Inf       0       1
x1:x3_lag2     3.529e-05        Inf       0       1
x1:x4          4.948e-04        Inf       0       1
</code></pre>

<p>In Stata14 I used:</p>

<pre><code>xtset id timevar
xtpoisson x1 x3 x3_lag x3_lag2 season x4 x1#x3_lag x1#x3_lag2 x1#x4, fe
</code></pre>

<p>The results in Stata (see below) yield standard errors and where the standard errors are significant the coefficients are very much the same as in R, so I assume in both cases the same model was calculated. The issue is: why do I get standard errors in Stata but ""Inf"" in R?</p>

<pre><code>Conditional fixed-effects Poisson regression    Number of obs     =    110,233
Group variable: id                              Number of groups  =     15,945

                                                Obs per group:
                                                              min =          2
                                                              avg =        6.9
                                                              max =         13

                                                Wald chi2(19)     =     816.49
Log likelihood  = -47369.663                    Prob &gt; chi2       =     0.0000

--------------------------------------------------------------------------------------
           y         |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
---------------------+----------------------------------------------------------------
                  x1 |  -.0001031   .0002119    -0.49   0.626    -.0005184    .0003121
                  x3 |  -.0119605   .0058094    -2.06   0.040    -.0233467   -.0005744
              x3_lag |  -.0478287   .0122489    -3.90   0.000    -.0718361   -.0238212
                     |
           x3_lag#x1 |   .0000189   .0000225     0.84   0.401    -.0000252     .000063
                     |
             x3_lag2 |  -.0515859   .0138096    -3.74   0.000    -.0786523   -.0245195
                     |
          x3_lag2#x1 |   .0000353   .0000245     1.44   0.149    -.0000127    .0000833
                     |
              season |
                 02  |  -.0703797   .0211975    -3.32   0.001    -.1119261   -.0288333
                 03  |    .093219   .0233858     3.99   0.000     .0473836    .1390544
                 04  |   .1256642   .0270927     4.64   0.000     .0725635    .1787649
                 05  |   .1426421   .0335767     4.25   0.000     .0768331    .2084512
                 06  |  -.1217462    .046838    -2.60   0.009     -.213547   -.0299454
                 07  |  -18.24382   741.5026    -0.02   0.980    -1471.562    1435.075
                 08  |  -.2095094   .0373193    -5.61   0.000    -.2826539   -.1363649
                 09  |  -.1886333   .0341701    -5.52   0.000    -.2556054   -.1216611
                 10  |    .044879   .0278778     1.61   0.107    -.0097604    .0995184
                 11  |  -.0995352   .0249665    -3.99   0.000    -.1484686   -.0506018
                 12  |   .0819983   .0214643     3.82   0.000      .039929    .1240675
                     |
                  x4 |   .6055111   .0754535     8.02   0.000      .457625    .7533972
                     |
               x4#x1 |   .0004948   .0001416     3.49   0.000     .0002173    .0007723
--------------------------------------------------------------------------------------
</code></pre>

<p>If you have any suggestions on how I could provide a replicable example (maybe with some sample panel data, any suggestions?), I will gladly do so.</p>

<p><strong>EDIT</strong></p>

<p>A smaller model produces identical results in R and Stata. It seems that with poor models R is more restrictive and does not show standard errors.</p>
"
"0.0379321620905441","0.0385614943639849","220830","<p>I need to forecast using <code>HoltWinters</code> with regression parameters using R. But I found there is not any option of <code>xreg</code> in <code>HoltWinters</code> function in R. I thought to use <code>auto.arima</code> with <code>xreg</code> option but my <code>HoltWinters</code> is performing better than <code>auto.arima</code> without any regression parameters.</p>

<p>Can you please suggest me how to incorporate <code>xreg</code> in <code>HoltWinters</code> function in R?</p>
"
"0.0709645772411954","0.072141950116023","221011","<p>I have two monthly time series: </p>

<ul>
<li>one for house prices expressed in annual change growth rates: $\left( \text{ln}(X_t) - \text{ln}(X_{t-12})\right) - \left( \text{ln}(X_{t-1}) - \text{ln}(X_{t-1-12})\right)$;</li>
<li>the other simply in growth rates: $\text{ln}(X_t) - \text{ln}(X_{t-1})$. </li>
</ul>

<p>Here is the data: </p>

<pre><code>House Prices = [1]  0.009189829  0.022612618  0.003952796 -0.015179184  0.001903336 -0.028779902  0.025668239 -0.011237850
  [9]  0.014782630 -0.018844480 -0.023547458  0.020613233  0.029281069 -0.010539781  0.006707366  0.023693144
 [17] -0.002632498  0.148738752 -0.154539337  0.013908319 -0.002294980  0.013274177  0.010043605 -0.007862785
 [25] -0.018297295 -0.003167249  0.022984841  0.001666694 -0.001310199 -0.131548705  0.114723242 -0.003431495
 [33]  0.000953231 -0.010096108 -0.009434595 -0.037774255  0.030877947 -0.011245971 -0.018800312 -0.012805013
 [41]  0.001326392 -0.012034079 -0.045279346 -0.017308170  0.002490863 -0.007340975  0.005052948 -0.024053201
 [49] -0.004190424 -0.028607790  0.004678486  0.026626293 -0.015166864  0.006988983  0.038257855  0.020798177
 [57]  0.008175391  0.021294030 -0.013331432  0.030969145  0.017065249 -0.002672683  0.019435476 -0.037047871
 [65]  0.001844432  0.007663458  0.034406137 -0.049379845 -0.012527106 -0.012859680  0.012954488 -0.015463951
 [73] -0.025509006  0.006318645  0.012977464  0.019940525 -0.025592828  0.020774198 -0.033613414  0.018338077
 [81]  0.001765807  0.009236604 -0.041413104  0.030227358  0.017180849  0.012593360 -0.039001526 -0.004994992
 [89]  0.037766071 -0.043167230 -0.016613786  0.023199890 -0.016214873 -0.012282560  0.065978520 -0.031465767
 [97]  0.006355108 -0.000449523 -0.005810647  0.016823517 -0.021988463  0.026178014  0.007654339 -0.008356379
[105]  0.013273736  0.031645473 -0.046408064  0.022334664  0.008517194 -0.014892335  0.019147342  0.007955040
[113]  0.014122506 -0.035722162  0.018174284  0.021410306 -0.038943797 -0.014517888  0.032750195  0.022506553
[121] -0.003870785  0.130924075 -0.057934974 -0.174228244  0.016937619  0.010647759  0.015691962 -0.033174094
[129]  0.038263205  0.003456250 -0.013422897

B = [1] -0.0223848461  0.0102749646  0.0913403867 -0.0758207770 -0.0053898407 -0.0204047336  0.0050358986
  [8]  0.0195335195 -0.0200303353 -0.0045390828  0.0056380761 -0.0004492945  0.0040043649  0.0012918928
 [15] -0.0104850394  0.0047110190  0.0049805985 -0.0046957178  0.0095002549  0.0202597343 -0.0183526932
 [22]  0.0237185217 -0.0137022065  0.0133787918 -0.0212629487  0.0070512978  0.0959447868 -0.0801519036
 [29] -0.0362526334 -0.0000278572  0.0269014993  0.0009862920 -0.0329868357  0.0283667004 -0.0135186142
 [36] -0.0004975495  0.0053822189  0.0108219907 -0.0078419784  0.0418340658 -0.0316367599 -0.0092324801
 [43] -0.0192830637  0.0336003682  0.0021479539 -0.0146426306  0.0003717930  0.0216259502 -0.0323127786
 [50]  0.0033077606 -0.0123735085 -0.0014757035  0.0266339779 -0.0228959378  0.0002848944  0.0133572802
 [57] -0.0093035312 -0.0034350607  0.0052349772  0.0115210916 -0.0122443122  0.0435497970 -0.0100099291
 [64]  0.0267252321 -0.0654005679  0.0088385287 -0.0089122237  0.0155299273 -0.0027394997 -0.0126183268
 [71]  0.0090999709  0.0017039487 -0.0144843611  0.0269128625  0.0042663583  0.0220574344 -0.0523831016
 [78] -0.0059331639  0.0171559908  0.0125030653  0.0151902738  0.0471484001 -0.0477394702  0.0888317354
 [85] -0.1044700154  0.0234134906 -0.0215966718  0.0157974035  0.0970094980 -0.1049559862 -0.0290578406
 [92]  0.0617653831 -0.0132202439  0.0022117274  0.0091225692  0.0424813190 -0.0614889434  0.0163745828
 [99] -0.0112793057  0.0666179349 -0.0352838073 -0.0259179501  0.0269557599  0.0127882202 -0.0430512536
[106]  0.0862308560 -0.0633012329  0.0596481270  0.0900367605 -0.0303162498 -0.0153738373 -0.0442218848
[113] -0.0116158350 -0.0531058308  0.2036373944  0.1598602057 -0.3837940703 -0.0069112146 -0.0192015196
[120]  0.0110269191 -0.0351135484  0.0439917033  0.0522746614  0.0036354828 -0.0414276671 -0.0361649669
[127]  0.0080753079  0.0352684982 -0.0282391428 -0.0141622744  0.0045799464
</code></pre>

<p>I am studying if <code>B</code> has an effect on <code>House prices</code>. For this reason first a take a simple liner regression between the two and I get a negative and significant estimate at the 95% confidence interval: (-0.0004189 *). </p>

<p>Wanting to reach a step forward I undertake a Granger causality test as following:</p>

<p>I) Determine the optimal number of lags using the AIC/BIC test using:</p>

<pre><code>select.lags&lt;-function(x,y,max.lag=20) {
  y&lt;-as.numeric(y)
  y.lag&lt;-embed(y,max.lag+1)[,-1,drop=FALSE]
  x.lag&lt;-embed(x,max.lag+1)[,-1,drop=FALSE]

  t&lt;-tail(seq_along(y),nrow(y.lag))

  ms=lapply(1:max.lag,function(i) lm(y[t]~y.lag[,1:i]+x.lag[,1:i]))

  pvals&lt;-mapply(function(i) anova(ms[[i]],ms[[i-1]])[2,""Pr(&gt;F)""],max.lag:2)
  ind&lt;-which(pvals&lt;0.05)[1]
  ftest&lt;-ifelse(is.na(ind),1,max.lag-ind+1)

  aic&lt;-as.numeric(lapply(ms,AIC))
  bic&lt;-as.numeric(lapply(ms,BIC))
  structure(list(ic=cbind(aic=aic,bic=bic),pvals=pvals,
                 selection=list(aic=which.min(aic),bic=which.min(bic),ftest=ftest)))
}

s&lt;-select.lags(Topic.15,House.Prices,20)
t(s$selection)
plot.ts(s$ic)
</code></pre>

<p>As a result I get:     </p>

<pre><code>aic bic ftest
14  12  13   
</code></pre>

<p>Here is when it comes the first doubt: why are they giving me different results? Nevertheless, when I do the Granger causality test for both directions using these numbers as possible lags I get in all high significant results (***) only in the direction that <code>B</code> is causing <code>House prices</code> movements:</p>

<pre><code>lmtest::grangertest(Topic.15,House.Prices,12)
lmtest::grangertest(House.Prices,Topic.15,12)
</code></pre>

<p>I do not seem to see the direction of the cause, is it possitive or negative (an increase in <code>B</code> produces an increase or a drop in <code>House prices</code> at time $t+1$?).<br>
Another question, is the conclusion valid that changes in <code>B</code> produce changes in <code>House prices</code>? What are the weakness in this line of argument?</p>
"
"0.026822089039291","0.0272670941574606","221012","<p>I have a question about kernel regression: 
assume that we have a time series (Time, Speed)  of a vehicle during a trip. I know that how I can smooth my data, using ksmooth(Time, speed, h). This function apply a symmetric window width (2h) to my data. In other words, it considers the same distance(h) after x0 and before x0. Now I want to do some smoothing that apply for example 3/4 h to data before x0,and 1/4h to data after that. Please let me know how I can do that in R. </p>
"
"0.103881504159667","0.105605001571314","221161","<p>I have been having trouble with the predict function underestimating (or overestimating) the predictions from an lmer model with some polynomials. Hopefully my edits make it clearer. I have scaled data that looks like this:</p>

<pre><code>Terr      Date     Year            Age  
T.092     123      0.548425     -0.86392            
T.104     102      1.2072       -0.48185            
T.104     105      1.075445     -0.86392            
T.104     112      0.94369      -1.24599            
T.040     116     -0.2421        2.192652           
T.040     114     -0.37386       1.810581           
T.040     119     -0.50561       1.428509           
T.040     128      0.15316      -0.09978            
T.040     113      0.021405     -0.48185
</code></pre>

<p>Iâ€™m trying to determine how Year affects lay date after controlling for Age, with Terr (territory) as a random variable. I usually include polynomials and do model averaging, but whether I use a single model or do model averaging, the predict function gives predictions that are a bit lower or higher than they should be. I realize that the model below would not be a good model for this data, Iâ€™m just trying to provide a simplified example.  </p>

<p>Below is my code  </p>

<pre><code>library(lme4
m1 &lt;- lmer(Date ~ (1|Terr) + Year + Age + I(Age^2), data=data)
new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictions=predict(m1, newdata = new.dat, re.form=NA)
pred.l&lt;-cbind(new.dat, Predictions)
pred.l  

      Year          Age Predictions
    1   -2 2.265676e-16    124.4439
    2   -1 2.265676e-16    123.2124
    3    0 2.265676e-16    121.9810
    4    1 2.265676e-16    120.7496
</code></pre>

<p>When plotted with the means, the graph looks like this:</p>

<p><a href=""http://i.stack.imgur.com/mwIpJ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mwIpJ.jpg"" alt=""graph1""></a></p>

<p>When I use effects, I get a much better fit  </p>

<pre><code>library(effects)
ef.1c=effect(c(""Year""), m1, xlevels=list(Year=-2:1))
pred.lc=data.frame(ef.1c)
pred.lc

      Year      fit        se    lower    upper
    1   -2 126.0226 0.6186425 124.8089 127.2363
    2   -1 124.7911 0.4291211 123.9493 125.6330
    3    0 123.5597 0.3298340 122.9126 124.2068
    4    1 122.3283 0.3957970 121.5518 123.1048
</code></pre>

<p><a href=""http://i.stack.imgur.com/SvI3f.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SvI3f.jpg"" alt=""graph2""></a></p>

<p>After much trial and error, I have discovered that the problem is with the Age polynomial, because when the Age polynomial is not included, the predicted and fitted are equal and both fit well. Below is the same  model but with Age as a linear term.  </p>

<pre><code>m2 &lt;- lmer(Date ~ (1|Terr) + Year + Age, data=data)
new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictionsd=predict(m2, newdata = new.dat, re.form=NA)  
pred.ld&lt;-cbind(new.dat, Predictionsd)
pred.ld

      Year          Age Predictionsd
    1   -2 2.265676e-16     125.9551
    2   -1 2.265676e-16     124.7653
    3    0 2.265676e-16     123.5755
    4    1 2.265676e-16     122.3857

library(effects)
ef.1e=effect(c(""Year""), m2, xlevels=list(Year=-2:1))
pred.le=data.frame(ef.1e)
pred.le

      Year      fit        se    lower    upper
    1   -2 125.9551 0.6401008 124.6993 127.2109
    2   -1 124.7653 0.4436129 123.8950 125.6356
    3    0 123.5755 0.3406741 122.9072 124.2439
    4    1 122.3857 0.4093021 121.5827 123.1887
</code></pre>

<p>I do many similar analyses, and this issue with the predictions being slightly lower (or higher) than they should be often happens when Age is included as a polynomial. When I include a polynomial for Year, there is no problem and the predicted and fitted are equal, so I know the problem is not with all polynomials.</p>

<pre><code>m3 &lt;- lmer(Date ~ (1|Terr) + Year + I(Year^2) + Age, data=data)

new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictionsf=predict(m3, newdata = new.dat, re.form=NA)  
pred.lf&lt;-cbind(new.dat, Predictionsf)
pred.lf

      Year          Age Predictionsf
    1   -2 2.265676e-16     125.6103
    2   -1 2.265676e-16     124.8494
    3    0 2.265676e-16     123.7483
    4    1 2.265676e-16     122.3070

library(effects)
ef.1g=effect(c(""Year""), m3, xlevels=list(Year=-2:1))
pred.lg=data.frame(ef.1g)
pred.lg

      Year      fit        se    lower    upper
    1   -2 125.6103 0.8206625 124.0003 127.2203
    2   -1 124.8494 0.4615719 123.9438 125.7549
    3    0 123.7483 0.4275858 122.9094 124.5871
    4    1 122.3070 0.4262110 121.4708 123.1431
</code></pre>

<p>I've looked for answers (e.g., <a href=""http://stats.stackexchange.com/questions/180010/overestimated-and-underestimated-predictions-in-regression"">here</a>) but haven't found anything that is directly helpful. I can provide the whole data set if needed. Does anyone have any insight?</p>
"
"0.0709645772411954","0.072141950116023","221525","<p>I am using the <code>svyglm</code> function in the <code>survey</code> package in <code>R</code> to fit logistic regression models to a stratified, cluster survey. I want to calculate confidence intervals for my regression coefficients. The default method for <code>confint.svyglm</code> says that it creates Wald confidence intervals by adding and subtracting a multiple of the standard error. But the confidence interval this produces is not consistent with the p-value from the model - confidence intervals that do not overlap 0 still have p-values greater than .05.</p>

<p>I tried to replicate the p-value and confidence interval calculations by hand. It appears the p-value is calculated using a t-test, with the df of the t distribution taken from the residual degrees of freedom from the model. So far so good. But the confidence interval provided by <code>confint.svyglm</code> is just coefficient +/- 1.96*standard.error. This seems wrong - for a 95% confidence interval, I think the multiplier for the standard error should be the .975 quantile of a t-distribution with the appropriate degrees of freedom (in my case 10), which can be somewhat different from 1.96 (the .975 quantile of a z-distribution). True? Has anyone else had this problem? I am relatively new to working with survey data. Is there a reason to always use the z-quantile instead of the t-quantile for complex surveys specifically, or is this just a bug in the package?</p>
"
"0.0309714806541255","0.0472279924554862","221681","<p>I have a <code>quantile regression model</code> that I fit with the <code>rq()</code> function in the <code>quantreg</code> package in R. </p>

<p>However, since my sample size if fairly small (n = 36) compared to  the number of X variables (= 8), I need to estimate power for the various regression coefficients. How can I do that?</p>

<p>It would be great to be pointed to a function in R that can do that for a <code>quantreg model</code>, but a general explanation of how to calculate power in such a model would be fine as well (I can then code that in R myself). </p>

<p>How can I determine the power for a given <code>quantile regression model</code> with a given dataset?</p>

<p>thanks, Steve</p>
"
"0.120088260543947","0.127894008160743","221880","<p>To explore how the <code>LASSO</code> regression works, I wrote a small piece of code that should optimize <code>LASSO</code> regression by picking the best alpha parameter.</p>

<p>I cannot figure out why the <code>LASSO</code> regression is giving me such unstable results for the alpha parameter after cross validation.</p>

<p>Here is my Python code:</p>

<pre><code>from sklearn.linear_model import Lasso
from sklearn.cross_validation import KFold
from matplotlib import pyplot as plt

# generate some sparse data to play with
import numpy as np
import pandas as pd 
from scipy.stats import norm
from scipy.stats import uniform

### generate your own data here

n = 1000

x1x2corr = 1.1
x1x3corr = 1.0
x1 = range(n) + norm.rvs(0, 1, n) + 50
x2 =  map(lambda aval: aval*x1x2corr, x1) + norm.rvs(0, 2, n) + 500
y = x1 + x2 #+ norm.rvs(0,10, n)

Xdf = pd.DataFrame()
Xdf['x1'] = x1
Xdf['x2'] = x2

X = Xdf.as_matrix()

# Split data in train set and test set
n_samples = X.shape[0]
X_train, y_train = X[:n_samples / 2], y[:n_samples / 2]
X_test, y_test = X[n_samples / 2:], y[n_samples / 2:]

kf = KFold(X_train.shape[0], n_folds = 10, )
alphas = np.logspace(-16, 8, num = 1000, base = 2)

e_alphas = list()
e_alphas_r = list()  # holds average r2 error
for alpha in alphas:
    lasso = Lasso(alpha=alpha, tol=0.004)
    err = list()
    err_2 = list()
    for tr_idx, tt_idx in kf:
        X_tr, X_tt = X_train[tr_idx], X_test[tt_idx]
        y_tr, y_tt = y_train[tr_idx], y_test[tt_idx]
        lasso.fit(X_tr, y_tr)
        y_hat = lasso.predict(X_tt)

        # returns the coefficient of determination (R^2 value)
        err_2.append(lasso.score(X_tt, y_tt))

        # returns MSE
        err.append(np.average((y_hat - y_tt)**2))
    e_alphas.append(np.average(err))
    e_alphas_r.append(np.average(err_2))

## print out the alpha that gives the minimum error
print 'the minimum value of error is ', e_alphas[e_alphas.index(min(e_alphas))]
print ' the minimizer is ',  alphas[e_alphas.index(min(e_alphas))]

##  &lt;&lt;&lt; plotting alphas against error &gt;&gt;&gt;

plt.figsize = (15, 15)
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(alphas, e_alphas, 'b-')
ax.plot(alphas, e_alphas_r, 'g--')
ax.set_ylim(min(e_alphas),max(e_alphas))
ax.set_xlim(min(alphas),max(alphas))
ax.set_xlabel(""alpha"")
plt.show()
</code></pre>

<p>If you run this code repeatedly, it gives wildly different results for alpha:</p>

<pre><code>&gt;&gt;&gt; 
the minimum value of error is  3.99254192539
 the minimizer is  1.52587890625e-05
&gt;&gt;&gt; ================================ RESTART ================================
&gt;&gt;&gt; 
the minimum value of error is  4.07412455842
 the minimizer is  6.45622425334
&gt;&gt;&gt; ================================ RESTART ================================
&gt;&gt;&gt; 
the minimum value of error is  4.25898253597
 the minimizer is  1.52587890625e-05
&gt;&gt;&gt; ================================ RESTART ================================
&gt;&gt;&gt; 
the minimum value of error is  3.79392968781
 the minimizer is  28.8971008254
&gt;&gt;&gt; 
</code></pre>

<p>Why is the alpha value not converging properly?  I know that my data is synthetic, but the distribution is the same.  Also, the variation is very small in <code>x1</code> and <code>x2</code>.</p>

<p>what could be causing this to be so unstable?  </p>

<h2>The same thing written in R gives different results - it always returns the highest possible value for alpha as the ""optimal_alpha"".</h2>

<p>I also wrote this in R, which gives me a slightly different answer, which I don't know why?</p>

<pre><code>library(glmnet)
library(lars)
library(pracma)

set.seed(1)
k = 2 # number of features selected 

n = 1000

x1x2corr = 1.1
x1 = seq(n) + rnorm(n, 0, 1) + 50
x2 =  x1*x1x2corr + rnorm(n, 0, 2) + 500
y = x1 + x2 

filter_out_label &lt;- function(col) {col!=""y""}

alphas = logspace(-5, 6, 100)

for (alpha in alphas){
  k = 10
  optimal_alpha = NULL
  folds &lt;- cut(seq(1, nrow(df)), breaks=k, labels=FALSE)
  total_mse = 0
  min_mse = 10000000
  for(i in 1:k){
    # Segement your data by fold using the which() function
    testIndexes &lt;- which(folds==i, arr.ind=TRUE)
    testData &lt;- df[testIndexes, ]
    trainData &lt;- df[-testIndexes, ]

    fit &lt;- lars(as.matrix(trainData[Filter(filter_out_label, names(df))]),
                trainData$y,
                type=""lasso"")
    # predict
    y_preds &lt;- predict(fit, as.matrix(testData[Filter(filter_out_label, names(df))]),
                       s=alpha, type=""fit"", mode=""lambda"")$fit # default mode=""step""

    y_true = testData$y
    residuals = (y_true - y_preds)
    mse=sum(residuals^2)
    total_mse = total_mse + mse
  }
  if (total_mse &lt; min_mse){
    min_mse = total_mse
    optimal_alpha = alpha
  }
}

print(paste(""the optimal alpha is "", optimal_alpha))
</code></pre>

<p>The output from the R code above is: </p>

<pre><code>&gt; source('~.....')
[1] ""the optimal alpha is  1e+06""
</code></pre>

<p>In fact, no matter what I set for the line ""<code>alphas = logspace(-5, 6, 100)</code>"", I always get back the highest value for alpha.</p>

<p>I guess there are actually 2 different questions here :</p>

<ol>
<li><p>Why is the alpha value so unstable for the version written in Python? </p></li>
<li><p>Why does the version written in R give me a different result?  (I realize that the <code>logspace</code> function is different from <code>R</code> to <code>python</code>, but the version written in <code>R</code> always gives me the largest value of <code>alpha</code> for the optimal alpha value, whereas the python version does not).</p></li>
</ol>

<p>It would be great to know these things...</p>
"
"0.0929144419623766","0.0944559849109725","222233","<p>I try to reproduce with <code>optim</code> the results from a simple linear regression fitted with <code>glm</code> or even <code>nls</code> R functions.<br>
The parameters estimates are the same but the residual variance estimate and the standard errors of the other parameters are not the same particularly when the sample size is low. I suppose that this is due differences in the way the residual standard error is calculated between Maximum Likelihood and Least square approaches (dividing by n or by n-k+1 see bellow in the example).<br>
I understand from my readings on the web that optimization is not a simple task but I was wondering if it would be possible to reproduce in a simple way the residual standard error estimate from <code>glm</code> while using <code>optim</code>. </p>

<p>Simulate a small dataset</p>

<pre><code>set.seed(1)
n = 4 # very small sample size !
b0 &lt;- 5
b1 &lt;- 2
sigma &lt;- 5
x &lt;- runif(n, 1, 100)
y =  b0 + b1*x + rnorm(n, 0, sigma) 
</code></pre>

<p>Estimate with optim</p>

<pre><code>negLL &lt;- function(beta, y, x) {
    b0 &lt;- beta[1]
    b1 &lt;- beta[2]
    sigma &lt;- beta[3]
    yhat &lt;- b0 + b1*x
    likelihood &lt;- dnorm(y, yhat, sigma)
    return(-sum(log(likelihood)))
}

res &lt;- optim(starting.values, negLL, y = y, x = x, hessian=TRUE)
estimates &lt;- res$par     # Parameters estimates
se &lt;- sqrt(diag(solve(res$hessian))) # Standard errors of the estimates
cbind(estimates,se)


    &gt; cbind(estimates,se)
      estimates         se
b0     9.016513 5.70999880
b1     1.931119 0.09731153
sigma  4.717216 1.66753138
</code></pre>

<p>Comparison with glm and nls</p>

<pre><code>&gt; m &lt;- glm(y ~ x)
&gt; summary(m)$coefficients
            Estimate Std. Error   t value    Pr(&gt;|t|)
(Intercept) 9.016113  8.0759837  1.116411 0.380380963
x           1.931130  0.1376334 14.030973 0.005041162
&gt; sqrt(summary(m)$dispersion) # residuals standard error
[1] 6.671833
&gt; 
&gt; summary(nls( y ~ b0 + b1*x, start=list(b0 = 5, b1= 2)))

Formula: y ~ b0 + b1 * x

Parameters:
   Estimate Std. Error t value Pr(&gt;|t|)   
b0   9.0161     8.0760   1.116  0.38038   
b1   1.9311     0.1376  14.031  0.00504 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6.672 on 2 degrees of freedom
</code></pre>

<p>I can reproduce the different residual standard error estimates like this : </p>

<pre><code>&gt; # optim / Maximum Likelihood estimate
&gt; sqrt(sum(resid(m)^2)/n)
[1] 4.717698
&gt; 
&gt; # Least squares estimate (glm and nls estimates)
&gt; k &lt;- 3 # number of parameters
&gt; sqrt(sum(resid(m)^2)/(n-k+1))
[1] 6.671833
</code></pre>
"
"0.0851715717988452","0.0944559849109725","222479","<p>I'm new in this area, hope my question is understandable.
I need to fit conditional logistic regression model in R and use it for predictions on unseen data (output should be probability).
My datasets are  quite large (over 150k rows) and contains many (~500) noisy features.
I found package called <strong>clogitboost</strong> and tried to use it with relatively small number of boosting iterations (max 30, because with larger values it takes too long to compute and raises an error in the end - perhaps, it's resources limitations) - results are mediocre. I tried to use unconditional approach with regularization - <strong>glmnet</strong> and got better results, however, due nature of data I guess it will be better to use conditional regression with regularization similar to what is used in <strong>glmnet</strong> (tried to remove some features and apply <strong>clogitboost</strong> again and got slightly better results). There is package called <strong>clogitL1</strong> , which seems to do that, I tried to use it and it fits model quickly, but it doesn't provide <strong>predict()</strong> function, Usage described in  paper with attached R code here:
<a href=""https://www.jstatsoft.org/article/view/v058i12"" rel=""nofollow"">https://www.jstatsoft.org/article/view/v058i12</a>, they made some predictions in some way, but I can't understand it. Can I somehow manually predict using  unseen data, just like it's possible with <strong>clogitboost</strong> <strong>predict()</strong> (parameters are Model, X and Strata column) using model that was fitted with <strong>clogitL1</strong>? Note: in description of package <strong>clogitL1</strong> - ""Tools for the fitting and cross validation of <strong><em>exact</em></strong> conditional logistic regression models"" - so I'm not sure about what ""exact"" means here and  if it makes sense to use that package for my purposes. If it's not possible to predict, then, should I manually select features by checking their ""importance"" that can be found in <strong>clogitL1</strong> model? </p>
"
"0.0379321620905441","0.0192807471819925","223098","<p>I know that neural nets use activation functions, but where do distribution functions play into deep neural networks? For example, the <code>h2o.deeplearning()</code> function in R has the variable <code>distribution = c(""AUTO"", ""gaussian"", ""bernoulli"", ""multinomial"", ""poisson"", ""gamma"", ""tweedie"", ""laplace"", ""huber"", ""quantile"")</code>.  Where does this apply in deep learning? Also, I came across this quote <a href=""https://github.com/h2oai/h2o-tutorials/blob/master/tutorials/deeplearning/README.md"" rel=""nofollow"">from an h2o tutorial</a> that says ""H2O Deep Learning supports regression for distributions other than Gaussian such as Poisson, Gamma, Tweedie, Laplace"", but again I am confused as to where a distribution function plays into the concept of multilayer perceptrons.</p>
"
"0.144441369948218","0.141774423578904","223230","<p>I've inherited observations of a response variable ($y$) measured over time ($t$) during which the response increases and subsequently decreases.  The measurement of this response occurs repeatedly over ~ 50 replications ($rep$). The code below pulls in some data from six example $rep$s spanning the range of the replicates.  In this case, time $t$ has been centered across all $rep$s but it need not be.</p>

<p>Some complications of the data include, as the figure illustrates, that the response is usually not measured until after is underway and before the decline has concluded.  This is especially true in later replicates, which observe a smaller window and make fewer measurements of the response curve (e.g., see $rep$ 6).  </p>

<pre><code>tmp &lt;- tempfile()
download.file(""https://gist.githubusercontent.com/adamdsmith/cbb8fa530fa6f1f9c32e41be487ded63/raw/18d3ccf205df181a6343712798b384ecf71dbc27/data.R"", tmp)
source(tmp)
library(""ggplot2"")
ggplot(dat, aes(t,y,group = rep, colour = rep)) + geom_line(size=1) + 
  geom_point() + theme_classic()
</code></pre>

<p><a href=""http://i.stack.imgur.com/g12E8.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/g12E8.jpg"" alt=""Six replicates""></a></p>

<p>The variability in the nature of the response curve most of interest is how the response function changes over replicates, most notably whether:</p>

<ol>
<li><p>the temporal location of the maximum response (i.e., the $t$ of peak $y$) changes consistently over $rep$s, and</p></li>
<li><p>the shape of the response curve, particularly its width at some standardized level of response $y$, also changes as a function of $rep$ (i.e., is the response curve becoming more protracted or abbreviated over $rep$s?).  This may be too simplistic, and other parameters describing the response curve may be worth considering (e.g., skew, kurtosis, etc.).</p></li>
</ol>

<p>What makes the most intuitive sense is to have a single estimated underlying response function that describes the mean change in $y$ over $t$ and then evaluate whether the parameters that describe this function vary over $rep$.  This ""hierarchical"" structure can then be evaluated readily in a Bayesian framework.  The actual height of the curve in each $rep$ is, for various reasons, not of interest and thus variability in any related parameter(s) is preferentially be captured with a random $rep$ effect.</p>

<p>There's no shortage of possible ways to model this response function and its change with $rep$, but each approach that I've considered has apparent shortcomings and I'm mentally stuck on a reasonable way forward:</p>

<ol>
<li><p><strong>Quadratic function</strong></p>

<p>This seems an easy way, I think, to get at the questions, particularly using the vertex form: $y = a(t - h)^2 + c$, where $a$ controls the width of the parabola (and the direction it opens), $h$ controls the horizontal position of the vertex, and both can be modeled as a function of $rep$. $c$ controls the vertical position of the vertex and is easily captured with a random effect.  Unfortunately, this approach assumes symmetry about the peak response, which doesn't hold as the decline after peak response seems to occur more quickly than the increase (see, e.g., $rep$s 2-4). </p></li>
<li><p><strong>Gaussian function</strong></p>

<p>Another relatively easy way, where $y = ae^{-\frac{(t - m)^2}{2s^2}}$ and $a$ controls the height of curve's peak, $m$ controls the horizontal ($t$) position of peak (mean), and $s$ controls width of curve (standard deviation).  But this form has the same symmetry problem as the quadratic function.  Certainly Guassian functions can incorporate various degrees of skew and kurtosis but I don't think there's an easy way to model these aspects without cumulative distribution functions/integration.</p></li>
<li><p><strong>Trigonometric regression (sine/cosine Fourier terms)</strong></p>

<p>Trigonometric functions based on $sin$/$cos$ pairs are another relatively easy option (see, e.g., <a href=""http://stats.stackexchange.com/a/60504/21074"">here</a>.  The first $sin$/$cos$ pair terms produces a symmetric sine wave, but additional $sin$/$cos$ pairs with different functional periods get past this limitation.  Unfortunately, what experimentation I've done suggests it takes several additional $sin$/$cos$ pairs of unknown period (i.e., need to be estimated) to adequately capture the asymmetry, which quickly gets unwieldy and yields inadequate fits in some $rep$s.</p></li>
<li><p><strong>Additive models/smoothing splines</strong></p>

<p>An additive model seems to get around many of these limitations, but do I not lose the seemingly preferable state of having a fixed underlying response function defined by parameters than I can allow to vary with increasing $rep$?  Would a factor-smooth interaction that forced the smooth for each $rep$ to have the same smoothing parameter be a reasonable compromise?  For example, something like:</p>

<pre><code>library(mgcv)
?factor.smooth.interaction
m &lt;- gam(y ~ s(t, rep, bs = ""fs""), data = dat)
plot(m)
</code></pre>

<p><a href=""http://i.stack.imgur.com/9lINC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9lINC.png"" alt=""GAM plot""></a></p>

<p>If this is a reasonable solution, how then can I relate the peak or the width of each smooth to $rep$? Finding the peak of fitted smooths is a relatively simple exercise (see <a href=""http://stats.stackexchange.com/a/191489/21074"">here</a> for example), but is it reasonable to then model these derived values (and uncertainty) as a function of $rep$ in a separate model? </p></li>
</ol>

<p>Thanks very much for considering. As is likely obvious, I'm most comfortable with R, but I'm open to any solutions.</p>
"
"0.0758643241810882","0.0674826151369737","223379","<p>I'm fitting an <code>arima</code>(1,0,0) model using the <code>forecast</code> package in R on the <code>usconsumption</code> dataset. However, when I mimic the same fit using <code>lm</code>, I get different coefficients. My understanding is that they should be the same (in fact, they give the same coefficients if I model an <code>arima</code>(0,0,0) and <code>lm</code> with only the external regressor, which is related to this post: <a href=""http://stats.stackexchange.com/questions/28472/regression-with-arima0-0-0-errors-different-from-linear-regression"">Regression with ARIMA(0,0,0) errors different from linear regression</a>). </p>

<p>Is this because <code>arima</code> and <code>lm</code> use different techniques to calculate coefficients? If so, can someone explain the difference?  </p>

<p>Below is my code.</p>

<pre><code>&gt; library(forecast)
&gt; library(fpp)
&gt; 
&gt; #load data
&gt; data(""usconsumption"")
&gt; 
&gt; #create equivalent data frame from time-series
&gt; lagpad &lt;- function(x, k=1) {
+   c(rep(NA, k), x)[1 : length(x)] 
+ }
&gt; usconsumpdf &lt;- as.data.frame(usconsumption)
&gt; usconsumpdf$consumptionLag1 &lt;- lagpad(usconsumpdf$consumption)
&gt; 
&gt; #create arima model
&gt; arima(usconsumption[,1], xreg=usconsumption[,2], order=c(1,0,0))

Call:
arima(x = usconsumption[, 1], order = c(1, 0, 0), xreg = usconsumption[, 2])

Coefficients:
         ar1  intercept  usconsumption[, 2]
      0.2139     0.5867              0.2292
s.e.  0.0928     0.0755              0.0605

sigma^2 estimated as 0.3776:  log likelihood = -152.87,  aic = 313.74
&gt; 
&gt; #create lm model
&gt; lm(consumption~consumptionLag1+income, data=usconsumpdf)

Call:
lm(formula = consumption ~ consumptionLag1 + income, data = usconsumpdf)

Coefficients:
    (Intercept)  consumptionLag1           income  
         0.3779           0.2456           0.2614  
</code></pre>
"
"0.0599760143904067","0.0487768608679754","223447","<p>Let's suppose I have <em>p</em> predictor variables. For those predictors, there exists a weight vector <em>w</em> of length <em>p</em> that, if multiplied by the predictors, will minimize an error function. This is not any different than what linear regression performs when the error metric is RMSE. The problem is that I am not using RMSE to determine performance. Instead, I must multiply my weights by my predictors, then plug them into a complex function that takes .5 seconds to compute, and only then do I know if my error improved or worsened. </p>

<p>Pseudo R Code:</p>

<pre><code>vec=rnorm(150,0,1)
p=matrix(unlist(split(vec, ceiling(seq_along(vec)/15))),ncol=10)
response=rnorm(15,0,1)
w=rnorm(15,0,1)

for(i in 1:500){
  #multiply predictors by weights to get predictions
  preds=colSums(t(p)*w)

  #complex error function that takes .5 seconds, e.g.:
  #this isn't the true error function, just an example:
  preds=ifelse(preds&gt;1,preds,ifelse(preds&lt;=1&amp;preds&gt;0,0,-1)) 
  error=mean(abs(response-preds))

  #update weight vector w to move in the most optimal pattern to minimize error
  w= ???
}
</code></pre>

<p>How to update <em>w</em> in the most efficient manner?</p>
"
"0.0464572209811883","0.0472279924554862","223463","<p>I am trying to fit a piecewise linear model in R. The line is constrained to be constant, then linearly increase, then remain constant, then linearly decrease, then finally remain constant again (so it's hill-shaped). The problem can be reduced to solving for 3 parameters; the first, second, and third constant values. I have read methods for piecewise models, but cannot find anything for including constraints. </p>

<p>I have seen <a href=""https://stats.stackexchange.com/questions/149627/piecewise-regression-with-constraints"">this question</a> which is very similar, but treating it as an optimization function is not ideal for me because I am fitting this model within an optimization function. </p>

<p>The data is actual transitions in a nonhomogeneous Markov chain, so here is a small sample: </p>

<pre><code>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]    0    1    1    1    0    1    0    0    0
[2,]    0    0    1    1    1    0    1    0    0
[3,]    0    0    0    1    0    0    0    0    0
[4,]    0    1    1    1    1    0    0    0    0
[5,]    0    1    1    1    1    0    0    0    0
[6,]    0    1    1    1    1    1    0    0    0
</code></pre>
"
"0.0851715717988452","0.0944559849109725","223582","<p>I am trying to tie the odds ratio from a 2x2 cross classification table to the intercepts of a logistic regression on those 2 variables. I have a cross classification table that produces 2 odds ratios and the results of a logistic regression of PLACE3 ~ VIOL should produce intecepts should match the odds ratio of the contingency table. i.e. Odds ratio = exp(intercepts)  BUT the POLR package is not producing the correct intercepts.</p>

<p>Here is the data.  In the logistic regression PLACE3 is the outcome and VIOl is the independent variable.   You can see the PLACE3 vs. VIOL contingency table below and the logistic regression of PLACE3 ~ VIOL.  The odds ratios in the contingency table 1.79 and 3.1 are correct but the polr function seems off. Any thoughts on why  exp(summary(m)$zeta) does not produce 1.79 and 3.1?</p>

<p>For reference this is from Lemeshow's Applied Logisitic Regression book page 274.</p>

<pre><code>library(data.table)
aps &lt;- fread('http://www.umass.edu/statdata/statdata/data/aps.dat')
colnames(aps) = c(""ID"",""PLACE"",""PLACE3"",""AGE"",""RACE"",""GENDER"",""NEURO"",""EMOT"",""DANGER"",""ELOPE"",""LOS"",""BEHAV"",""CUSTD"",
                    ""VIOL"")
head(aps)
</code></pre>

<p>Here is  a cross classification table of PLACE3 vs. VIOl variables</p>

<pre><code>table(aps$PLACE3,aps$VIOL) 
      0   1
  0  80 179
  1  26 104
  2  15 104
</code></pre>

<p>using PLACE3 = 0 as the reference the 2 odds ratios from the contingency table are </p>

<pre><code>(104*80)/(179*26)  #1.79
(104*80)/(179*15)  #3.10
</code></pre>

<p>These odds ratios should be the same as exponentiating the slope coefficients  from 
a logistic model  PLACE3 ~ VIOL which is below</p>

<pre><code>aps$constant = rep(1,dim(aps)[1])
m &lt;- polr(as.factor(PLACE3) ~ constant + as.factor(VIOL), data = aps, Hess=TRUE,model=TRUE,method = c(""logistic""))
summary(m)

&gt; summary(m)
Call:
polr(formula = as.factor(PLACE3) ~ constant + as.factor(VIOL), 
    data = aps, Hess = TRUE, model = TRUE, method = c(""logistic""))

Coefficients:
                  Value Std. Error t value
as.factor(VIOL)1 0.8454     0.2112   4.003

Intercepts:
    Value  Std. Error t value
0|1 0.6869 0.1884     3.6464 
1|2 1.8608 0.2032     9.1557 

Residual Deviance: 1031.75 
AIC: 1037.75 
</code></pre>

<p>But you can see the exponentiation of the zeta vector is not 1.79 and 3.10</p>

<pre><code>exp(summary(m)$zeta)

&gt; exp(summary(m)$zeta)
     0|1      1|2 
1.987495 6.429049 
</code></pre>
"
"0.053644178078582","0.0545341883149212","224078","<p>I have three related questions on the package CausalImpact in R. The package can be found <a href=""https://github.com/google/CausalImpact"" rel=""nofollow"">here</a> and a reproducible example is below.</p>

<ol>
<li>Do I basically understand correctly, that the model makes ""1-step ahead""
predictions? I assume it works like a simple lm model that makes
lots of regressions for t+1 with predictors values from t-1 and then looks for the most contributory predictors?</li>
<li>When talking about ""coefficients"", does that mean they are (Pearson)
correlation coefficients?</li>
<li>The function <code>plot(impact$model$bsts.model,""coefficients"")</code> produces
a plot with inclusion probabilities ranging from 0 to 1. Is there
any way to access the actual values in a table? I found
that <code>colMeans(impact$model$bsts.model$coefficients)</code> provides some
values but I'd like to have a confirmation for this.</li>
<li>In my code, I changed <code>bsts.model &lt;- bsts(y ~ x1, ss, niter = 1000)</code> to <code>bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)</code> in order to get values for different variables that I defined before. I did not change the cbind functions for that, as I weren't sure if that was necessary. Now I wonder what to do when I do have a table with -let's say- 200 predictors? Do I have to enter x1 to x200 after <code>bsts(y ~</code> to find the best predictors?</li>
</ol>

<p>R code below:</p>

<pre><code>install.packages(""devtools"")
library(devtools)
devtools::install_github(""google/CausalImpact"")
#Download the tar from the git and then install the package in RStudio.

library(CausalImpact)

set.seed(1)
x1 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = 100)
x2 &lt;- 50 + arima.sim(model = list(ar = 0.899), n = 100)
x3 &lt;- 80 + arima.sim(model = list(ar = 0.799), n = 100)
x4 &lt;- 1.25 * x1 + rnorm(100)
x5 &lt;- 101 + arima.sim(model = list(ar = 0.999), n = 100)
y &lt;- 1.2 * x1 + rnorm(100)
y[71:100] &lt;- y[71:100] + 10
data &lt;- cbind(y, x1)

dim(data)
head(data)
data
matplot(data, type = ""l"")

time.points &lt;- seq.Date(as.Date(""2014-01-01""), by = 1, length.out = 100)
data &lt;- zoo(cbind(y, x1), time.points)
head(data)

pre.period &lt;- as.Date(c(""2014-01-01"", ""2014-03-11""))
post.period &lt;- as.Date(c(""2014-03-12"", ""2014-04-10""))

impact &lt;- CausalImpact(data, pre.period, post.period)
plot(impact)

summary(impact)
summary(impact, ""report"")
impact$summary

post.period &lt;- c(71, 100)
post.period.response &lt;- y[post.period[1] : post.period[2]]
y[post.period[1] : post.period[2]] &lt;- NA

ss &lt;- AddLocalLevel(list(), y)

bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)

impact &lt;- CausalImpact(bsts.model = bsts.model,post.period.response =     post.period.response)

plot(impact)
summary(impact)
summary(impact, ""report"")

plot(impact$model$bsts.model,""coefficients"")
plot(impact$model$bsts.model, ""coef"", inc = .1)
plot(impact$model$bsts.model, ""coef"", inc = .05)

colMeans(impact$model$bsts.model$coefficients)
</code></pre>
"
"0.0379321620905441","0.0385614943639849","224153","<p>I am performing a mediation analysis to investigate the relationship between National Error Management Culture (error_is) on Entrepreneurial Activity (TEA) with the mediator Innovation Index (innov_perc). So first I ran these regressions:</p>

<pre><code>lm(TEA~error_is) --&gt; 5,085 / p=0,0053 
lm(innov_perc~error_is) --&gt; -0,313 / p=0,0001 
lm(TEA~error_is+innov_perc) --&gt; for error_is X: 0,25, p not significant / for innov_perc: -15,55, p=0,007
</code></pre>

<p>Am I correct to conclude, that there is a full mediation? If so, how can I explain it, because Y~X is positive, and M~X and Y~M is negative. What does this mean?</p>

<p>I then ran the mediate function:</p>

<pre><code>model.1 &lt;- lm(innov_perc~error_is, data)
model.2 &lt;- lm(TEA~ error_is+innov_perc, data)
out.1 &lt;- mediate(model.1, model.2, treat = ""error_is"", sims=1000, dropobs = TRUE, mediator = ""innov_perc"")
summary(out.1)
</code></pre>

<p>With these results:</p>

<pre><code>               Estimate 95% CI Lower 95% CI Upper p-value
ACME              4.723        1.518        8.661    0.00
ADE               0.270       -3.902        4.756    0.92
Total Effect      4.993        1.474        8.425    0.01
Prop. Mediated    0.954        0.278        2.784    0.01

Sample Size Used: 35 


Simulations: 1000 
</code></pre>

<p>Does this mean that Error_is has a negative influence on innoc_perc and therefore TEA increases because Entrepreneurial Activity is often not innovative?</p>

<p>Please help me to understand how I can interpret this!! 
Thanks a lot!!</p>
"
"NaN","NaN","224265","<p>I'm fitting  a Bayesian logistic regression to model the effect of two covariates on a Randomized Response, with the 'rr' package.
I would like to compare two nested models by using the Bayes factor. Unfortunately, the 'rr'package does not estimate it by default. Is there any way to obtain it from the output of the 'rrreg.bayes' function?</p>

<p>For anybody who would like to help me, here is an example of application. If you were able to explain me how to compute the Bayes factor from the output of the example provided, I will be able to do the same on my data.</p>

<p><a href=""http://www.inside-r.org/node/333540"" rel=""nofollow"">http://www.inside-r.org/node/333540</a></p>

<p>Kind regards,</p>

<p>Jacopo Cerri </p>
"
"0.0599760143904067","0.0487768608679754","224310","<p>This is a two part question concerning linear regression in R. Here is my code and what my residual plot looks like before transformation:</p>

<pre><code>linear_model &lt;- lm(dependent ~ ., data=independent)
residuals &lt;- resid(linear_model)
plot(residuals)
</code></pre>

<p>[I'll add image, as soon as I've got enough reputation to post three links.]</p>

<p>Obviously there is a pattern, although I find it hard to describe. In any case I would think there is evidence for a non-linear relationship. The only function I can think of is $sine$. So I applied the $sine$ function on the dependent variable, played with it a bit and ended up with a shift of $1/2*\pi$ (so the same as cosinus). This what my code and residual plot look like after transformation:</p>

<pre><code>linear_model &lt;- lm(sin((1/2)*pi+dependent) ~ ., data=independent)
residuals &lt;- resid(linear_model)
plot(residuals)
</code></pre>

<p><a href=""http://i.stack.imgur.com/pJ91J.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pJ91J.png"" alt=""enter image description here""></a></p>

<p><strong>Question 1</strong>
Is the application of $sine$ on the dependent variable legitimate?</p>

<p><strong>Question 2</strong>
Although there still is a pattern - maybe even more distinct than before - the residuals are much lower. Does that indicate that the transformation was the right thing to do? And should I try to get rid of the still existing pattern?</p>

<p>Finally, the model's adjusted RÂ² value increased from .13 to .3.</p>

<p><strong>Edit:</strong> As a reaction to the comments, some more information on the dataset. The data has been recorded over a time span of <s>two</s> four weeks, 18 hours each day with intervals between roughly 2-10 minutes. Here is the autocorrelation function estimate of the dependent variable:</p>

<pre><code>acf(dependent)
</code></pre>

<p><a href=""http://i.stack.imgur.com/A0S1D.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/A0S1D.png"" alt=""enter image description here""></a></p>

<p>Apparently, the answer to question 1 is <em>No</em>.</p>
"
"0.0379321620905441","0.0385614943639849","224929","<p>main equation</p>

<p>$y_1 = y_2 \beta + x_1  \gamma + u_i $</p>

<p>Instrumental equation</p>

<p>$y_2=x_1 \pi_1 + x_2 \pi_2$</p>

<p>I have a binary endogenous variable $y_2$ in my main estimation equation. 
My instrument $x_2$ is binary as well and recently my supervisor suggested a <a href=""http://blog.stata.com/2013/11/07/fitting-ordered-probit-models-with-endogenous-covariates-with-statas-gsem-command/"" rel=""nofollow"">structural equation approach</a> to me. Generally I ran </p>

<pre><code>gsem (y1 &lt;- y2 x1 L@a, oprobit) (y2 &lt;- x1 x2 L@a)
</code></pre>

<p>Where the latent variable $L$ is part of the parametrisation (cf. link before). The main issue is that this procedure is that I do not account for the binary nature of the instrument and the instrumented variable and consequently the log-likelihood function breaks as it is not continous. </p>

<p>Is there a way around this discontinous function problem? Or do I need to estimate the first part with probit/logit? If so is that then still IV-regression or something else? </p>
"
"0.0969560705490221","0.105605001571314","225283","<p>Iâ€™m analyzing crowdsourced Twitter data, where workers labeled tweets. Within my dataset (N=2,400), I have one IV (call it â€˜dsâ€™) with 2 levels that differentiates which dataset the workers labeled. I have four factors of interest (what workers labeled) -- these are my DVs (let's call them f1, f2, f3, f4). Three of those factors are binomial &lt;0,1>, and one multinomial &lt;0, 1, 2>. Even though the latter can be treated as ordinal, I'm working under the assumption it is nominal. Finally, my datasets are of unequal lengths.</p>

<p>My goal is to analyze the relationship between each of the labeled factors for each level of the IV. More specifically, <strong>I want to tease out the different contributions of each of those factors on each dataset quantitatively, i.e., show amount of variance explained</strong> (e.g., ds1 influenced f1 more than f2, while the inverse for ds2). The end game is to model each factor into a scoring function, which allows me to compute a unified score. Hence, I need to back up the parameter weights for this function.</p>

<p>A snippet of my data frame looks like this:</p>

<pre><code>   f1 f2 f3 f4 ds
1   1  0  1  0  1
2   0  0  0  2  1
3   0  0  1  1  2
4   1  1  0  2  2
</code></pre>

<p>What I initially did was to compute correlations between each factor, and used the strengths of those correlations to back up my scoring function. However, given the many posts and tutorials I've been reading, it seems I need to make use of a mix of logistic and multinomial regression. What I have done so far is run binomial logit (using ?glm with class â€˜binomial') on the first 3 factors, and multinomial regression (using ?nnet) on f4. However, it seems I can only assess one outcome variable at a time.</p>

<p>For f1-f3, I have run the following R code:</p>

<pre><code>fit &lt;- glm(f1 ~ ds, data = xx, family = ""binomial"")
summary(fit)
confint.default(fit)
wald.test(b = coef(fit), Sigma = vcov(fit), Terms = 2)
</code></pre>

<p>For f4:</p>

<pre><code>fit &lt;- multinom(f4 ~ ds, data = xx)
summary(fit)
z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1))*2
</code></pre>

<p>My questions:</p>

<p><strong>1.</strong> Is running such logistic regression analyses appropriate for what I want to do, namely to tease out contributions of each factor? If so, is it meaningful to compare the coefficients of each factor with the other, when computed separately? Or is simply showing a correlation matrix sufficient in my case?</p>

<p><strong>2.</strong> Are there alternative techniques to assess all outcome variables/DVs at once, with respect to each level of my IV? If so, could you please provide me with some pointers (ideally for R)? I'm now looking into hierarchical multinomial marginal (HMM) models... </p>

<p>If something is unclear above, Iâ€™d be happy to clarify.</p>
"
"0.104084994078794","0.0991986258660637","226066","<p>This problem has held me up for three days now, so I really hope somebody here has a solution for the problem.</p>

<p>I have a model with an excessive number of zeros, so I use a zero-inflated poisson regression model with the following code and summary.</p>

<pre><code>cr_f1 = formula(cr ~ depth + habtype2 + month + year + lightregime + depth*month + depth*lightregime + depth*habtype2 + habtype2*year | depth + habtype2 + month + year + lightregime + depth*month + depth*lightregime + depth*habtype2)
summary(zeroinfl(cr_f1, dist = ""poisson"", link = ""logit"", data = allUVCdata))

Call:
zeroinfl(formula = cr_f1, data = allUVCdata, dist = ""poisson"", link = ""logit"")

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-1.6430 -0.5680 -0.2893  0.1426 16.8090 

Count model coefficients (poisson with log link):
                            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)                -4.515522   2.182503  -2.069  0.03855 * 
depth                       0.108941   0.072278   1.507  0.13175   
habtype2Pinnacles           0.879765   0.791166   1.112  0.26614   
habtype2Unexposed          -0.604246   0.786129  -0.769  0.44211   
month2                      0.628468   0.380450   1.652  0.09855 . 
month3                      0.309282   0.367690   0.841  0.40026   
month4                      0.649411   0.371667   1.747  0.08059 . 
month5                      0.758717   0.364079   2.084  0.03717 * 
month6                      0.467611   0.341024   1.371  0.17031   
month7                      0.523043   0.343363   1.523  0.12768   
month8                      0.563272   0.356843   1.578  0.11445   
month9                      0.204509   0.400398   0.511  0.60952   
month10                     0.662415   0.341616   1.939  0.05249 . 
month11                     0.934844   0.335077   2.790  0.00527 **
month12                     0.252216   0.360512   0.700  0.48417   
year2013                   -1.271010   1.282158  -0.991  0.32154   
year2014                    1.221887   0.753644   1.621  0.10495   
year2015                   -0.463176   0.771131  -0.601  0.54808   
lightregimeLight            2.754925   1.948779   1.414  0.15746   
depth:month2               -0.019864   0.008906  -2.230  0.02572 * 
depth:month3               -0.014157   0.008106  -1.747  0.08071 . 
depth:month4               -0.020553   0.008332  -2.467  0.01364 * 
depth:month5               -0.021213   0.008373  -2.533  0.01129 * 
depth:month6               -0.013561   0.007393  -1.834  0.06663 . 
depth:month7               -0.015043   0.007544  -1.994  0.04615 * 
depth:month8               -0.017383   0.008011  -2.170  0.03003 * 
depth:month9               -0.012340   0.008990  -1.373  0.16988   
depth:month10              -0.019631   0.007629  -2.573  0.01008 * 
depth:month11              -0.024101   0.007611  -3.167  0.00154 **
depth:month12              -0.014319   0.007952  -1.801  0.07174 . 
depth:lightregimeLight     -0.079860   0.071024  -1.124  0.26084   
depth:habtype2Pinnacles    -0.006819   0.011178  -0.610  0.54182   
depth:habtype2Unexposed     0.014857   0.011103   1.338  0.18086   
habtype2Pinnacles:year2013  1.351509   1.277930   1.058  0.29025   
habtype2Unexposed:year2013  1.538282   1.256047   1.225  0.22069   
habtype2Pinnacles:year2014 -1.213233   0.754305  -1.608  0.10775   
habtype2Unexposed:year2014 -0.495275   0.726863  -0.681  0.49563   
habtype2Pinnacles:year2015  0.389117   0.775476   0.502  0.61582   
habtype2Unexposed:year2015  0.659117   0.750396   0.878  0.37975   

Zero-inflation model coefficients (binomial with logit link):
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)             -4.61555    7.04621  -0.655 0.512442    
depth                    0.28728    0.28211   1.018 0.308524    
habtype2Pinnacles        9.41037    3.82210   2.462 0.013813 *  
habtype2Unexposed        2.11213    1.46465   1.442 0.149282    
month2                   8.67847    3.91193   2.218 0.026523 *  
month3                   7.12210    3.86428   1.843 0.065320 .  
month4                   4.10296    2.41285   1.700 0.089044 .  
month5                  12.76919    4.28035   2.983 0.002852 ** 
month6                   3.57695    2.49820   1.432 0.152198    
month7                   5.85534    3.27394   1.788 0.073700 .  
month8                   5.59503    3.33054   1.680 0.092974 .  
month9                   4.22953    3.76919   1.122 0.261807    
month10                  6.35022    3.59424   1.767 0.077265 .  
month11                  5.92079    3.36405   1.760 0.078404 .  
month12                  4.36214    3.17233   1.375 0.169113    
year2013                -0.18722    0.42651  -0.439 0.660688    
year2014                -1.50194    0.45263  -3.318 0.000906 ***
year2015                -9.79773    4.87536  -2.010 0.044469 *  
lightregimeLight         0.79826    5.62419   0.142 0.887133    
depth:month2            -0.39212    0.16795  -2.335 0.019557 *  
depth:month3            -0.36363    0.16695  -2.178 0.029397 *  
depth:month4            -0.21521    0.10211  -2.108 0.035059 *  
depth:month5            -0.57543    0.16933  -3.398 0.000678 ***
depth:month6            -0.24336    0.10398  -2.341 0.019256 *  
depth:month7            -0.33704    0.13975  -2.412 0.015874 *  
depth:month8            -0.35343    0.14683  -2.407 0.016082 *  
depth:month9            -0.31787    0.16903  -1.881 0.060026 .  
depth:month10           -0.37550    0.16021  -2.344 0.019087 *  
depth:month11           -0.34650    0.14821  -2.338 0.019397 *  
depth:month12           -0.29639    0.14221  -2.084 0.037142 *  
depth:lightregimeLight   0.08117    0.21795   0.372 0.709571    
depth:habtype2Pinnacles -0.57765    0.17049  -3.388 0.000704 ***
depth:habtype2Unexposed -0.17897    0.06252  -2.863 0.004200 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 146 
Log-likelihood: -3977 on 72 Df
</code></pre>

<p>So I included the interaction 'habtype2*year' in the count part of the formula, but now want to include it in the second model aswel (the binomial), but if I do I get the following error:</p>

<pre><code>cr_f1 = formula(cr ~ depth + habtype2 + month + year + lightregime + depth*month + depth*lightregime + depth*habtype2 + habtype2*year | depth + habtype2 + month + year + lightregime + depth*month + depth*lightregime + depth*habtype2 + habtype2*year)

summary(zeroinfl(cr_f1, dist = ""poisson"", link = ""logit"", data = allUVCdata))

Error in solve.default(as.matrix(fit$hessian)) : 
  system is computationally singular: reciprocal condition number = 2.08629e-37
</code></pre>

<p>This also happens if I want to try to include any of the other interaction terms that I still want to put into the model (""month<em>year"", ""month</em>lightregime"" and ""month*habtype2"").</p>

<p>I searched here on the forum and on google, seems like more people have encountered this error (also in other functions that doing a zeroinfl), but I have not found any suitable solution.</p>

<p>Data: sightings of as species on 29 different locations, >5200 observations (including zeros). </p>

<p>What could possibly solve this, so that I can run the model with the interaction terms that I want?</p>

<p><strong>EDIT</strong>: added some new output to give insight to the problem.</p>

<pre><code>allUVCdata$year = as.numeric(as.character(allUVCdata$year))
cr_f1 = formula(cr ~ depth + lightregime + month + year + habtype2 + month*habtype2 + month*year + habtype2*year + depth*month)
summary(hurdle(cr_f1, dist = ""poisson"", link = ""logit"", data = allUVCdata))

Error in solve.default(as.matrix(fit_count$hessian)) : 
  system is computationally singular: reciprocal condition number = 6.23277e-26
&gt; allUVCdata$year = as.factor(as.character(allUVCdata$year))
&gt; table(allUVCdata$year, allUVCdata$cr)

          0    1    2    3    4    5    6    7
  2012  750  149   25   12    3    0    0    0
  2013 1133  209   69   16    4    1    1    0
  2014  844  387  142   42   11    7    0    1
  2015  833  401  125   31    5    3    1    2
&gt; table(allUVCdata$month, allUVCdata$cr)

       0   1   2   3   4   5   6   7
  1  299  53  18   7   1   1   1   2
  10 346 104  40   9   4   4   0   0
  11 328 114  43  17   5   0   0   0
  12 350 112  29  10   2   0   0   0
  2  248  80  16   1   1   0   0   1
  3  303  82  24   6   0   0   1   0
  4  329  93  32   4   0   0   0   0
  5  277 105  28   9   1   1   0   0
  6  312 111  36  12   3   2   0   0
  7  362 113  46  14   5   2   0   0
  8  213 100  25   8   1   1   0   0
  9  193  79  24   4   0   0   0   0
&gt; table(allUVCdata$month, allUVCdata$year)

     2012 2013 2014 2015
  1    41  129   72  140
  10   91  149  152  115
  11  112  121  150  124
  12  112  124  154  113
  2    35  108   79  125
  3    33  149  101  133
  4    88  105  150  115
  5   101  108   95  117
  6    94  115  142  125
  7   118  133  153  138
  8    61  114  100   73
  9    53   78   86   83

table(allUVCdata$habtype2, allUVCdata$year)

            2012 2013 2014 2015
  Exposed     93  138  120  144
  Pinnacles  274  386  339  338
  Unexposed  572  909  975  919
</code></pre>
"
"0.026822089039291","0.0272670941574606","226849","<p>How can I find the function 'petest' in R? </p>

<p>A little background, I want to compare two different #regressions, 
y=x1+x2
logy=logx1+logx2</p>

<p>It is clear to me that I cannot use the BIC or AIC values, because the outcome variable is different. I searched in verbeek 2008 (A guide to modern Econometrics) and he suggests the PE test to compare linear and loglinear models.</p>

<p>I searched on this website <a href=""http://artax.karlin.mff.cuni.cz/r-help/library/lmtest/html/petest.html"" rel=""nofollow"">http://artax.karlin.mff.cuni.cz/r-help/library/lmtest/html/petest.html</a> and there I could find that the PE test exist in R, however, I cannot find it. I have the package lmtest, and 'petest' is supposed to be there, but when I try to use it, it says: Error: could not find function ""petest"".</p>

<p>Also, I have looked for more explanation, like examples, or videos that show how to apply this PE model, but I did not find any. Any help on this topic is highly appreciated!</p>
"
"0.0889588054368324","0.0904347204435887","228238","<p>I'm having a strange problem running a meta-regression using the function <code>rma.mv()</code> in the 'metafor' package in R.</p>

<p>Since some of my data are from multiple-endpoint studies, I have calculated the variance-covariance matrix so that correlations between outcomes are taken into account. I'm also using random effects at study and treatment level. As far as I'm aware, I have now covered all issues with regard to dependent effect sizes.</p>

<p>The model looks like this:</p>

<pre><code>cov_mod &lt;- rma.mv(Hedges_g, cov, mods = ~ days, random = ~ treatment | study, data = rev)
</code></pre>

<p>When running the code, it gives this error message:</p>

<pre><code>Error in rma.mv(Hedges_g, cov9, mods = ~days, random = ~1 | treatment/study,  : 
  Error during optimization.
In addition: Warning message:
In rma.mv(Hedges_g, cov9, mods = ~days, random = ~1 | treatment/study,  :
  V appears to be not positive definite.
</code></pre>

<p>I have discovered that the problem lies with one particular study (9 effect sizes in total, coming from 3 treatment groups that were each tested at 3 moments in time). When I remove this study from the data set, the code runs without problem.</p>

<p>Thus, apparently this particular study causes the matrix to be 'not positive definite'. I have read that this likely means that ""at least one of [the] variables can be expressed as a linear combination of the others"" (<a href=""http://stats.stackexchange.com/questions/30465/what-does-a-non-positive-definite-covariance-matrix-tell-me-about-my-data"">source</a>).</p>

<p>However, here comes the strange thing: I have replaced all values in the variance-covariance matrix relating to this particular study with random numbers between 0-1 (maintaining the symmetry), and the error message remains unchanged. I am puzzled, because the matrix can no longer be linearly predictable if it contains random numbers.</p>

<p>What could be the issue?</p>
"
"0.0479808115123254","0.0487768608679754","228257","<p>I have performed mixed effect Cox hazard regressions, and <a href=""http://stats.stackexchange.com/questions/228229/reconstructing-slopes-in-mixed-effect-models"">reconstructed the slopes</a> to get group specific slopes (e.g. sex-specific responses to the explanatory variable). I aim to test whether the slopes differ from one another (e.g. do males and females respond differently to the explanatory variable?). To do this I will use Z-tests (<a href=""http://stats.stackexchange.com/questions/55501/test-a-significant-difference-between-two-slope-values"">here</a> and <a href=""http://stats.stackexchange.com/questions/13112/what-is-the-correct-way-to-test-for-significant-differences-between-coefficients"">here</a>) where</p>

<p>$$Z=  \frac{\beta_1-\beta_2}{\sqrt{{SE_{\beta_1}}^{2}+{SE_{\beta_2}}^2}}$$</p>

<p>However, I have performed my models in R using the coxme package which gives the following output, from which I reconstruct the sex- and group-specific slopes with the included function.</p>

<pre><code>...
Fixed coefficients
                        coef exp(coef)   se(coef)      z    p
SexM             0.091305017 1.0956031 0.09085235   1.00 0.31
GroupG2         -0.036313825 0.9643376 0.08889039  -0.41 0.68
NE              -0.192009224 0.8252993 0.01317388 -14.57 0.00
SexM:GroupG2     0.009757875 1.0098056 0.12750426   0.08 0.94
SexM:NE         -0.212264676 0.8087506 0.02008058 -10.57 0.00
GroupG2:NE      -0.006933708 0.9930903 0.01814987  -0.38 0.70
SexM:GroupG2:NE  0.044999019 1.0460268 0.02756553   1.63 0.10
...


coxSlopeFunc = function(model, nfixed = 1){
    if(nfixed ==1){
        # Slope for Females + G1
        FG1 = model$coefficients[3]
	# Slope for Males + G1
	MG1 = model$coefficients[3] + model$coefficients[5]

        # Slope for Females + G2
        FG2 = model$coefficients[3] + model$coefficients[6]
        # Slope for Males + G2
        MG2 = model$coefficients[3] + model$coefficients[5] + model$coefficients[6] + model$coefficients[7]

        # Sex differences in slope
        SG1 = FG1 - MG1
        SG2 = FG2 - MG2

    matrix(c(FG1,MG1,FG2,MG2,SG1,SG2), ncol = 1, byrow = T)}
}
round(coxSlopeFunc(coxdum),3)

&gt; round(coxSlopeFunc(coxdum),3)
       [,1]
[1,] -0.192
[2,] -0.404
[3,] -0.199
[4,] -0.366
[5,]  0.212
[6,]  0.167
</code></pre>

<p>However, I am unsure how to calculate the SE of the slope for each - should I just sum the standard errors of the components? </p>

<p>$$\frac{(-0.192009224 - (-0.192009224 + -0.212264676))}{\sqrt{0.01317388^2 + (0.01317388 +  0.02008058)^2}}$$</p>
"
"0.0709645772411954","0.072141950116023","228493","<p>The literature on Survival Analysis is mainly from the Medical science where tipically the researcher want to evaluate the effect of a treatment to that of another one. So far, all the example I read and studied thus contain one or more categorical variable (with at least 2 levels) and possibly some continuous variable as a covariate. Anyway the main interest is on a categorical variable (e.g. treatment).</p>

<p>Is it possible and correct to run a non parametric Cox model (or alternatively a parametric one) using only one or more continuous variables? In particular without categorizing the continuous var into 2 or more groups? </p>

<p>Something like a logistic regression. </p>

<p>To give you a more practical example, I'm trying to model the survival of say bush in a field depending on the number of cows in the same field. </p>

<p>I'm pretty sure it can be done but the lack of examples leave me in the doubt.</p>

<p>If possible how can one use the predict function for example to predict the survival when the predictor has a specified value? like survival of my plant when 10 cows are in the field...</p>

<p>any help is welcome!</p>
"
"0.0808716413062113","0.0904347204435887","228641","<p>Consider the following dataset I want to use as the independent variables to conduct linear regression on:</p>

<pre><code>set.seed(42)
sa = runif(10)
sb = runif(10)
sc = sb+sa
sd = sb-sa
df = data.frame(sa,sb,sc,sd)
</code></pre>

<p>Now I want to perform tests for multicollinearity. I'm aware of the <code>ppcor</code> package, which calculates the partial correlation between the variables. In this case:</p>

<pre><code>&gt; pcor(df)
$estimate
            [,1]        [,2]       [,3]       [,4]
[1,]  1.00000000  0.06649968 -0.7325597  0.7706902
[2,]  0.06649968  1.00000000 -0.6304810 -0.6870502
[3,] -0.73255975 -0.63048097  1.0000000  0.1308260
[4,]  0.77069021 -0.68705016  0.1308260  1.0000000
</code></pre>

<p>As far as I know, there is no way of telling that <code>sc</code> and <code>sd</code> are linear combinations of <code>sa</code> and <code>sb</code>, just by looking at the estimates (or the other outputs of <code>pcor</code>, for that matter).</p>

<p>The only method that comes to my mind, is applying linear regression on each of the independent variables like so:</p>

<pre><code>summary(lm(sc~sa+sb,df))

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 1.404e-16  3.915e-17 3.587e+00   0.0089 ** 
sa          1.000e+00  4.716e-17 2.121e+16   &lt;2e-16 ***
sb          1.000e+00  4.135e-17 2.418e+16   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.672e-17 on 7 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 

summary(lm(sd~sa+sb,df))

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 1.404e-16  3.915e-17 3.587e+00   0.0089 ** 
sa          1.000e+00  4.716e-17 2.121e+16   &lt;2e-16 ***
sb          1.000e+00  4.135e-17 2.418e+16   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.672e-17 on 7 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 
</code></pre>

<p>I'm wondering two things: </p>

<ol>
<li><p>Is my approach with linear regression reasonable? I think the downside is, that it can only detect linear correlation. But non-linear correlation shouldn't be a problem with linear regression, right?</p></li>
<li><p>Is there an R function/package that automatically checks for multiple correlation?</p></li>
</ol>
"
"0.0402331335589365","0.0409006412361909","228679","<p>I understand that ""glmnet"" package has alpha and lambda regularization parameters which can be optimized by ""caret"" package's train function. Optimal lambda value and lambda values of trained model are in the image. </p>

<p><strong>Can some one please help me understand what these lambda values mean, do they mean while minimizing the criterion function of multinomial regression, the aforementioned lambda values represent values at each iteration?</strong></p>

<pre><code>library(caret)
library(nnet)
ctrl &lt;- trainControl(method = ""repeatedcv"", number = 10, savePredictions = TRUE)
model_train_glmnet &lt;- train(Class2 ~ ZCR + Energy + EntropyE + SpectralC + SpectralS + SpectralE + SpectralF + SpectralR + MFCC1 + MFCC2 + MFCC3 + MFCC4 + MFCC5 + MFCC6 + MFCC7 + MFCC8 + MFCC9 + MFCC10 + MFCC11 + MFCC12 + MFCC13, data = training, method=""glmnet"", trControl = ctrl, tuneLength = 5)

print(model_train_glmnet$finalModel$lambdaOpt)
[1] 0.007676627
&gt; 
&gt; print(model_train_glmnet$finalModel$lambda)
[1] 3.838314e-01 3.497328e-01 3.186635e-01 2.903543e-01 2.645601e-01
[6] 2.410573e-01 2.196424e-01 2.001300e-01 1.823510e-01 1.661514e-01
[11] 1.513910e-01 1.379418e-01 1.256875e-01 1.145217e-01 1.043479e-01
[16] 9.507796e-02 8.663150e-02 7.893539e-02 7.192299e-02 6.553355e-02
[21] 5.971173e-02 5.440710e-02 4.957373e-02 4.516973e-02 4.115698e-02
[26] 3.750071e-02 3.416925e-02 3.113375e-02 2.836791e-02 2.584778e-02
[31] 2.355154e-02 2.145928e-02 1.955290e-02 1.781587e-02 1.623316e-02
[36] 1.479105e-02 1.347706e-02 1.227979e-02 1.118889e-02 1.019490e-02
[41] 9.289211e-03 8.463983e-03 7.712066e-03 7.026948e-03 6.402693e-03
[46] 5.833895e-03 5.315628e-03 4.843402e-03 4.413128e-03 4.021078e-03
[51] 3.663856e-03 3.338369e-03 3.041798e-03 2.771573e-03 2.525354e-03
[56] 2.301009e-03 2.096593e-03 1.910338e-03 1.740629e-03 1.585996e-03
[61] 1.445100e-03 1.316722e-03 1.199748e-03 1.093165e-03 9.960517e-04
[66] 9.075652e-04 8.269396e-04 7.534766e-04 6.865398e-04 6.255495e-04
[71] 5.699774e-04 5.193422e-04 4.732052e-04 4.311670e-04 3.928633e-04
[76] 3.579624e-04 3.261620e-04 2.971867e-04 2.707854e-04 2.467296e-04
[81] 2.248108e-04 2.048393e-04 1.866419e-04 1.700611e-04 1.549534e-04
[86] 1.411878e-04 1.286450e-04 1.172166e-04 1.068034e-04 9.731524e-05
[91] 8.867002e-05 8.079282e-05 7.361541e-05 6.707562e-05 6.111681e-05
[96] 5.568736e-05 5.074025e-05 4.623262e-05 4.212544e-05 3.838314e-05
</code></pre>
"
"0.0709645772411954","0.072141950116023","228781","<p>I recently estimated some OLS regressions with daily returns scaled by 100 as dependent variable (thus in percentage points). As I learned (and empirically confirmed), this scaling only scales coefficients by 100, but has no impact on statistical significance.</p>

<p>I used the same, scaled returns to estimate a simple EGARCH(1,1) model and found that the scaling directly impacts the coefficient for the unconditional variance <code>omega</code> and the significance of all other coefficients. The results are thus vastly different when using scaled data, especially when it comes to the interpretation of significance.</p>

<p>I used the SP500 data from the ""rugarch"" package in R and the <code>ugarchfit</code> function to produce following example:</p>

<p>Here with normal returns:</p>

<pre><code>        Estimate  Std. Error    t value Pr(&gt;|t|)  
mu      0.000670    0.000211     3.17900 0.001478               
ar1    -0.679036    0.017029   -39.87517 0.000000               
ma1     0.701977    0.016065    43.69611 0.000000               
omega  -0.269569    0.005428   -49.65921 0.000000               
alpha1 -0.197466    0.025594    -7.71537 0.000000               
alpha2  0.129236    0.005627    22.96744 0.000000               
beta1   0.970782    0.000080 12106.51240 0.000000               
gamma1 -0.009223    0.068496    -0.13465 0.892888               
gamma2  0.124195    0.055994     2.21802 0.026553               
shape   4.670759    0.848486     5.50481 0.000000                           
LogLikelihood : 3204.702 
</code></pre>

<p>And here with returns scaled by 100:</p>

<pre><code>        Estimate  Std. Error    t value Pr(&gt;|t|)  
mu      0.067050    0.021114   3.175639 0.001495  
ar1    -0.679038    0.016626 -40.840967 0.000000  
ma1     0.701978    0.016644  42.176242 0.000000  
omega  -0.000460    0.006190  -0.074301 0.940771  
alpha1 -0.197462    0.060240  -3.277926 0.001046  
alpha2  0.129237    0.061149   2.113468 0.034561  
beta1   0.970786    0.003998 242.815325 0.000000  
gamma1 -0.009222    0.073792  -0.124977 0.900542  
gamma2  0.124189    0.075641   1.641818 0.100628  
shape   4.670628    0.881931   5.295909 0.000000  

LogLikelihood : -1400.468 
</code></pre>

<p>Look especially at <code>omega</code> and the significance of <code>gamma2</code>!  </p>

<p>Does anybody know why this is the case?</p>
"
"0.0758643241810882","0.0771229887279699","228956","<p>I've been trying to generate the deviance residuals for a model I've made using R. It's a Gompertz regression with a number of covariates in the regression and the data is left-truncated, right-censored with an event being a death (i.e. there is an entry age, exit age and an indicator of censoring).</p>

<p>I've used R's phreg() function to handle the truncation/censoring and was hoping to assess the validity of the model using deviance residuals but I'm not entirely sure how to calcuate them straight from the phreg() output.</p>

<p>A manual attempt I tried is as follows:</p>

<p>A paper I've been following ""APPLYING SURVIVAL MODELS TO PENSIONER MORTALITY
DATA"" Richards (2008) states that the the number of deaths in any age range can be assumed Poisson distributed with parameter equal to the sum of the integrated hazard function over that age range over all individuals at risk.</p>

<p>The actual number of deaths is just the calculated from the data.</p>

<p>So, from what I gather, the deviance residuals are then:</p>

<p>$r_{i}=Sign(D_{i}-\hat{D_{i}})\,sqrt(2\times(D_{i}\times log(D_{i}/\hat{D_{i}})-(D_{i}-\hat{D_{i}}))$</p>

<p>where,
$D$ is the actual number of deaths in age $i$ across all individuals and $\hat{D}$ is the estimated number of deaths according to the model.</p>

<p>Have I interpreted this correctly? Apologies if I've left any obvious required information out.</p>
"
"0.0379321620905441","0.0385614943639849","229286","<p>So, I did do a search already and came across <a href=""http://stats.stackexchange.com/questions/60777/what-are-the-assumptions-of-negative-binomial-regression"">this</a> response but, his explanation went over my head a bit. The research i've done online hasn't been any more helpful. I've used this code,</p>

<pre><code>m3 &lt;- glm(daysabs ~ math + prog, family = ""poisson"", data = dat)
X2 &lt;- 2 * (logLik(m1) - logLik(m3))
</code></pre>

<p>to find out whether or not the Poisson model is more applicable but i'm not sure what to do if the value of X2 is close to 0. Also, in the link, he mentions, </p>

<pre><code>Linearity: The model is still linear in the parameters 
(i.e. the linear predictor is XÎ²), but the expected response
is not linearly related to them (unless you use the identity link function!).
</code></pre>

<p>but I'm not sure what that means. Should I be looking at the linearity between Y vs. X? or the Residuals of the regression model vs. X?</p>

<p>Please advise.</p>
"
"0.0889588054368324","0.0904347204435887","229884","<p>I have a cancer classification problem (type A vs type B) on radiological images from which i have generated 756 texture-based predictive features (wavelet transform followed by texture analysis, i.e., features described by Haralick, Amasadun etc) and 8 semantic features based on subjective assessment by expert radiologist. This is entirely for research and publication to show that these predictive features may be useful in this particular problem. I do not intend to deploy the model for practitioners. </p>

<p>I have 107 cases. 60% cases are type A and 40% type B (in keeping with their natural proportions in population). I have done several iterations of model development with varying results. One particular method is giving me an 80% 80% classification accuracy but I am suspicious that my method is not going to stand critical analysis. I am going to outline my method and a few alternatives. I will be grateful if someone can pick if it is flawed. I have used R for this:</p>

<p>Step 1: Split into 71 training and 36 test cases.<br>
Step 2: remove correlated features from training dataset (766 -> 240) using findcorrelation function in R (caret package)<br>
Step 3: rank training data features using Gini index (Corelearn package)<br>
Step 4: Train multivariate logistic regression models on top 10 ranked features using subsets of sizes 3 , 4, 5 ,and 6 in all possible combination (<sup>10</sup>C<sub>3</sub>=252, <sup>10</sup>C<sub>4</sub>=504, <sup>10</sup>C<sub>5</sub>=630). So <strong>total 1386 multivariate logistic regression models were trained</strong> using 10-fold cross-validation and tested on test dataset.<br>
Step 5: Of these I selected a model which gave the best combination of training and test dataset accuracy, i.e., 3 feature model with 80% 80% accuracy.<p></p>

<p>Somehow running 1300 permutations seems quite dodgy to me and seems to have introduced some false discovery. Just want to confirm if this is a valid ML technique or whether I should skip step 4 and only train on top 5 ranked features without running and permutations.</p>

<p>Thanks. <p> PS I experiemented a bit with naive bayes and random forests but get rubbish test set accuracy so dropped them</p>

<p>====================</p>

<h1>UPDATE</h1>

<p>Following discussion with SO members, i have changed the model drastically and thus moved more recent questions regarding model optimisation into a new post <a href=""http://stats.stackexchange.com/questions/232829/lasso-regularised-classification-highly-variable-choice-of-lambda-min-on-repeate"">LASSO regularised classification highly variable choice of lambda.min on repeated cv</a></p>
"
"0.0758643241810882","0.0578422415459774","229905","<p>I have read <a href=""http://stats.stackexchange.com/questions/145870/how-to-fit-an-exponential-equation-of-the-form-y-a-becx-to-data"">this article</a> and those linked to it, but I am still having difficulties fitting a function of this form to data I have using the nls function in R. Invariably, I fail to get convergence regardless of what starting values I choose for <code>A</code> through <code>C</code>.</p>

<p>Here is a plot of all six of the relationships I would ultimately like to fit:
<a href=""http://i.stack.imgur.com/JKmq3.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JKmq3.jpg"" alt=""enter image description here""></a></p>

<p>While <code>VEG.STM3</code> and <code>PROP.RIPE</code> probably will conform to a curve of this shape fairly well, the others probably won't, and that's fine. I still have a justifiable reason for trying it.</p>

<p>My questions are as follows:</p>

<ol>
<li>What are the parameters <code>A</code>, <code>B</code>, and <code>C</code> doing, mathematically, in a function of this form? <code>A</code> appears to be the Y asymptote, <code>B</code> seems to be a scalar of some kind, and <code>C</code> seems to control the rate of decay, but, beyond that, I can't seem to figure out what a reasonable range of values for each parameter should be, especially from one set of data to the next.</li>
<li>Is there a way to ""linearize"" this problem so that <code>lm</code> can be used in place of <code>nls</code>? My understanding is that this function is equivalent to <code>ln(Y) ~ ln(A) + ln(B) + CX</code>, but I don't understand how to fit that equation any more than I understand how to fit this current curve. </li>
<li>Is there any way to get <code>nls</code> to be less fussy and more robust to lousy starting guesses?</li>
<li>Once I get <code>nls</code> to run successfully, how can I compare the fit of the model to one fit by simple linear regression? Would use of AICc be appropriate in that case? </li>
</ol>

<p>Here is some data to work with:</p>

<pre><code>PROP.RIPE = c(0.37, 0.223, 0.223, 0.224, 0.388, 0.413, 0.406, 0.422, 0.554, 
0.453, 0.569, 0.511, 0.13, 0.166, 0.16, 0.216, 0.297, 0.344, 
0.339, 0.292, 0.601, 0.535, 0.65, 0.535, 0.269, 0.238, 0.334, 
0.272, 0.523, 0.358, 0.449, 0.393, 0.458, 0.426, 0.576, 0.468, 
0.581, 0.579, 0.527, 0.568, 0.348, 0.313, 0.317, 0.267, 0.623, 
0.527, 0.646, 0.589, 0.488, 0.444, 0.498, 0.449, 0.109, 0.103, 
0.171, 0.153, 0.505, 0.343, 0.345, 0.213, 0.029, 0.011, 0.071, 
0.013, 0.697, 0.604, 0.624, 0.639, 0.386, 0.508, 0.38, 0.471, 
0.618, 0.488, 0.513, 0.485, 0.602, 0.597, 0.625, 0.495, 0.318, 
0.457, 0.423, 0.547, 0.88, 0.949, 0.912, 0.771, 0.628, 0.635, 
0.486, 0.567, 0.621, 0.549, 0.698, 0.709, 0.541, 0.563, 0.789, 
0.692, 0.525, 0.395, 0.449, 0.597, 0.57, 0.487, 0.556, 0.546, 
0.495, 0.617, 0.754, 0.71, 0.585, 0.719, 0.508, 0.536, 0.592, 
0.472, 0.481, 0.658, 0.937, 0.853, 0.981, 0.887)

NODES0 = c(124L, 362L, 198L, 343L, 152L, 193L, 98L, 167L, 148L, 284L, 
113L, 137L, 227L, 323L, 156L, 362L, 166L, 327L, 137L, 312L, 166L, 
350L, 97L, 222L, 182L, 456L, 143L, 277L, 172L, 272L, 110L, 184L, 
138L, 288L, 102L, 124L, 236L, 280L, 159L, 127L, 104L, 176L, 93L, 
167L, 178L, 400L, 126L, 248L, 189L, 336L, 181L, 304L, 245L, 283L, 
151L, 327L, 116L, 179L, 144L, 177L, 397L, 642L, 322L, 443L, 125L, 
249L, 100L, 144L, 56L, 22L, 23L, 17L, 252L, 387L, 184L, 308L, 
115L, 267L, 82L, 157L, 223L, 226L, 79L, 73L, 101L, 139L, 104L, 
60L, 200L, 164L, 66L, 49L, 173L, 204L, 64L, 107L, 435L, 215L, 
51L, 129L, 392L, 550L, 174L, 178L, 276L, 204L, 98L, 74L, 421L, 
303L, 126L, 150L, 168L, 195L, 77L, 75L, 72L, 142L, 59L, 47L, 
391L, 479L, 109L, 111L)

NODE.SUCCESS = c(1.05, 0.79, 0.86, 0.69, 0.85, 0.8, 0.77, 0.84, 0.53, 0.88, 
0.88, 0.79, 0.85, 0.88, 1, 1.03, 0.95, 0.77, 0.92, 0.73, 0.98, 
0.92, 0.97, 0.99, 0.88, 0.82, 0.84, 0.78, 0.63, 0.54, 0.54, 0.47, 
1.09, 0.88, 0.95, 0.99, 0.96, 1.13, 0.91, 1.01, 0.81, 0.89, 0.99, 
0.85, 0.95, 0.65, 0.87, 0.73, 0.64, 0.82, 0.82, 0.75, 0.93, 1.06, 
0.94, 0.89, 0.65, 0.53, 0.6, 0.62, 0.91, 0.89, 0.93, 1.13, 0.81, 
0.63, 0.75, 0.67, 0.93, 0.82, 0.7, 0.88, 0.8, 0.96, 0.9, 0.94, 
0.58, 0.66, 0.65, 0.63, 0.66, 0.62, 0.66, 0.77, 0.51, 0.6, 0.47, 
0.9, 0.69, 0.73, 0.59, 0.63, 0.97, 0.93, 0.95, 0.91, 0.81, 0.92, 
0.88, 1.1, 0.56, 0.57, 0.44, 0.51, 0.85, 0.83, 0.96, 0.85, 1, 
0.97, 0.94, 0.95, 0.61, 0.71, 0.73, 0.8, 1.06, 0.96, 0.9, 0.91, 
0.45, 0.41, 1.3, 0.55)
</code></pre>
"
"0.0709645772411954","0.0618359572423054","230106","<p>I am running a simple beta regression model in JAGS, following the example given <a href=""http://stats.stackexchange.com/questions/41536/how-can-i-model-a-proportion-with-bugs-jags-stan"">here</a>. Here is my JAGS model code as an R object:</p>

<pre><code>jags.model = ""
model {
#model
for (i in 1:N){
y[i] ~ dbeta(alpha[i], beta[i])
alpha[i] &lt;- mu[i] * phi
beta[i]  &lt;- (1-mu[i]) * phi
logit(mu[i]) &lt;- a + b*x1[i] + c*x2[i] + d*x3[i] + e*x4[i] + f*x5[i]
}

#priors
a  ~ dnorm(0, .0001)
b  ~ dnorm(0, .0001)
c  ~ dnorm(0, .0001)
d  ~ dnorm(0, .0001)
e  ~ dnorm(0, .0001)
f  ~ dnorm(0, .0001)
t0 ~ dnorm(0, 0.010)
phi &lt;- exp(t0)
}"" 
</code></pre>

<p>In the linked example there is a gamma prior on the <code>phi</code> parameter. However, I kept getting errors, so I shifted to an alternative expoential prior on <code>phi</code> to keep it positive. Every time I run this code using <code>runjags</code> I get the error: <code>value out of range in 'gammafn'</code>, which prints out many many times until I tell the code to stop running. I am unsure what the problem is here, as I am no longer using a gamma function in any of this code. Any insight, or alternative methods to run a beta-regression in JAGS, would be appreciated. </p>
"
"0.080466267117873","0.0727122510865616","230123","<p>I'm trying to find the maximum output for a quite complex function in R. The input of the function requires 4 parameters: <code>x1, x2, x3, x4</code>, the output is a single value.</p>

<p>The constraints are : </p>

<pre><code>5 &lt; x1 &lt; 15
0 &lt; x2 &lt; 89
0 &lt; x3 &lt; 89
0 &lt; x4 &lt; 89
x3 &lt; x4
</code></pre>

<p>Now what I did is quite dumb and slow: run all the possible combinations through the function and find the maximum value, it works but takes 3-4 hours to ""search"" the maximum output of the function.</p>

<p>I believe there is a better way to optimize this. I tried run linear regression using the output against all input variables but can't see any good explanation(R-squared &lt; 0.1). I manually change each parameter to see the output but the changes of the output is quite unpredictable to me. I tried using symbolic regression to find a formula for the function so that I can find the cost function later but not working. I also checked some optimization functions in R such as <code>optim(c(5,1,1,2),findVar,lower = c(5,1,1,2), upper = c(15,89,89,89))</code> but looks like they were just not working as well (Error Message:<code>Your panel, as described by unit.variable and time.variable, is unbalanced. Balance it and run again.</code>). </p>

<p>I'm a newbie to optimization problems so any good advices will be really appreciated !</p>
"
"0.0727844771755901","0.0904347204435887","230201","<p>I'm using the glmnet package in R to do ridge regression. When I have a full set of dummy variables (if you took a horizontal sum of all these dummy variables you would get the constant), ridge regression with lambda = 0 is NOT dropping any of the dummy variables. In contrast, OLS gives the expected result by dropping at least 1 of the dummies to prevent perfect multi-collinearity. I'd like to know why the discrepancy exists. </p>

<pre><code> library(glmnet)
 set.seed(1)
make_dummies_out_of_factors&lt;- function(your_df, names_of_factor_variables) {
  indices&lt;- which(names(your_df) %in% names_of_factor_variables) #Finds columns corresponding to factor variables
  model_matrices_list&lt;- lapply(indices, function(x) {
    model.matrix(~your_df[,x] - 1, your_df)
  })
  #create a model matrix for each factor variable, and stores each one as a list
  model_matrices_together&lt;- do.call(cbind, model_matrices_list)
  #Column bind all model matrices which are stored as lists
  final&lt;- cbind(your_df, model_matrices_together)
  #Column bind all the model matrices to the original data
  final&lt;- final[,-indices]
  #Get rid of the original factor variables

  names(final)&lt;- gsub(""your_df.*\\]"", ""dummy_"", names(final))
  #Give appropriate names to the dummies

  return(final)
}
test_df&lt;- data.frame(numeric1 = rnorm(1000), numeric2 = rnorm(1000), 
                     state = rep(letters[1:4], 250), year = rep(c(""yr1"", ""yr2""), 500)) #This data frame has 2 factor variables
test_df&lt;- make_dummies_out_of_factors(test_df, names_of_factor_variables = c(""state"", ""year""))

linear_alldum&lt;- lm(test_df$numeric2 ~ test_df$numeric1 + test_df$dummy_yr1 + test_df$dummy_yr2 + test_df$dummy_a + 
                     test_df$dummy_b + test_df$dummy_c + test_df$dummy_d)


X_test&lt;- as.matrix(test_df[,-1]) #Remove dependent variable out of X matrix
y_test&lt;- test_df[,1] #This is the dependent variable

ridge_alldum&lt;- glmnet(x = X_test, y = y_test, lambda = seq(200, 0, by = -1), alpha = 0)


comparison = data.frame(as.matrix(coef(ridge_alldum))[,201], coefficients(linear_alldum))
names(comparison)[1]&lt;- ""coefficients_ridge_l0""
names(comparison)[2]&lt;- ""coefficients_linear_reg""
#Note that coefficients aren't identical, and that ridge regression doesn't drop coefficients. 

prediction_linear&lt;- predict(linear_alldum)
prediction_ridge&lt;- predict(ridge_alldum, newx = X_test, s = 0)
predictions&lt;- data.frame(prediction_linear, prediction_ridge = prediction_ridge)
names(predictions)[2]&lt;- ""prediction_ridge""

#Note that the predictions using linear regression and ridge regression aren't the same. 

sapply(predictions, mean) #Means of predictions using linear and ridge.
sapply(predictions, sd) #SDs of predictions using linear and ridge. 
</code></pre>
"
"0.0379321620905441","0.0385614943639849","230456","<p>I have a large n (>1,000,000) dataset with a small number of features to estimate (regression) random forest and have been looking to implement Rborist (in R). I'd like to parallelize my work, but am not finding much guidance on how that would be done. I have 16 processors to use on the machine where it's running. When I use doParallel with the randomForest package, for example, the command:</p>

<blockquote>
  <p>rf &lt;- foreach(ntree=rep(32, 16), .combine=combine, .packages='randomForest') %dopar% randomForest(x, y, nodesize = 25, ntree=ntree)</p>
</blockquote>

<p>It launches 16 R processes, and works slowly as randomForest does, but works.</p>

<p>The analogous command for Rborist:</p>

<blockquote>
  <p>rb &lt;- foreach(ntree=rep(32, 16), .combine=combine, .packages='Rborist') %dopar% Rborist(x, y, minNode = 25, ntree=ntree)</p>
</blockquote>

<p>Throws the error:</p>

<blockquote>
  <p>error calling combine function:
  
  Warning message:
  In mclapply(argsList, FUN, mc.preschedule = preschedule, mc.set.seed = set.seed,  :
    all scheduled cores encountered errors in user code</p>
</blockquote>

<p>Does anyone know how to parallelize with Rborist? It does not appear to be happening under the hood as it's only using 1 cpu when I run:</p>

<blockquote>
  <p>rb &lt;- Rborist(x, y, minNode = 25, ntree = 512)</p>
</blockquote>
"
"0.080466267117873","0.0727122510865616","230532","<p>Trying to get the Bayes Factor for a correlation between two variables in my data, I tried three different functions. All implement the Jeffreysâ€“Zellnerâ€“Siow (JZS) prior, but I get quite different results with the three approaches. Two questions:</p>

<ol>
<li><p>Is this suspicious, or is it reasonable that they produce different values, as the implementations are slightly different?</p></li>
<li><p>Is there a consensus on the best measure to use?  </p></li>
</ol>

<p>My data:</p>

<pre><code>a=rnorm(100,1,2)
b=rnorm(100,.8,1.5)
myData &lt;- data.frame(a=a, b=b)
</code></pre>

<p>I try the <code>jzs_corbf</code> function, described and implemented <a href=""http://www.ncbi.nlm.nih.gov/pubmed/22798023"" rel=""nofollow"">here</a> (<a href=""http://dsquintana.com/post/98962697485/how-to-calculate-a-bayes-factor-for-correlations"" rel=""nofollow"">shorter version</a>)</p>

<pre><code>cor.resu.a_b &lt;- cor.test(myData$a, myData$b, method=c(""pearson""))
cor.resu.a_b$estimate
n = 100
r = cor.resu.a_b$estimate
jzs_corbf(r,n)
[1] 0.08206358
</code></pre>

<p>I also tried the convenience function from the <code>BayesFactor</code> package:</p>

<pre><code>require(BayesFactor)
regressionBF(b ~ a, data = myData, progress=FALSE)

Bayes factor analysis
--------------
[1] a : 0.2181081 Â±0%

Against denominator:
  Intercept only 
---
Bayes factor type: BFlinearModel, JZS
</code></pre>

<p>And I also tried the a function described <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4891395/"" rel=""nofollow"">recently</a> (<a href=""https://osf.io/9d4ip/"" rel=""nofollow"">code</a>)</p>

<pre><code>bf10JeffreysIntegrate(n=100, r=r)

      cor 
0.1297927
</code></pre>

<p>While in this case the differences are only numerical, in my real data I get quite big differences that make it more difficult to decide on an interpretation. </p>

<p><a href=""http://stats.stackexchange.com/questions/184950/calculating-bayes-factor-from-a-correlation-coefficient"">Related</a></p>
"
"0.0709645772411954","0.072141950116023","230581","<p>My problem is the following, my data has a lot of branch off points and the tree grows very rapidly. The end result is not readable, the end nodes are overlapped and even conversion to rules is more or less useless. 
I am using the rpart package. </p>

<pre><code>#Scoring model
d = sort(sample(nrow(Memmbers),nrow(Memmbers)* .6))
#select training sample
train&lt;-Memmbers[d, ]
test&lt;-Memmbers[-d, ]


s&lt;-glm(verifikation ~ . - userId,data = Memmbers,family = binomial())
summary(s)

library(ROCR)

#score test data set 
test$score &lt;- predict(s,type='response',test)
pred&lt;-prediction(test$score,test$verifikation)
perf&lt;- performance(pred,""tpr"",""fpr"")
plot(perf)

max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])

#get results of terms in regression 
g&lt;-predict(s,type='terms',test)
#function to pick top 3 reasons
#works by sorting coefficient terms in equation
# and selecting top 3 in sort for each loan scored 
ftopk&lt;- function(x,top=3){
  res=names(x)[order(x, decreasing = TRUE)][1:top]
  paste(res,collapse="";"",sep="""")
}
# Application of the function using the top 3 rows
topk=apply(g,1,ftopk,top=3)
#add reason list to scored tets sample
test&lt;-cbind(test, topk)

library(rpart)
library(rattle)

fit1 &lt;- rpart(verifikation ~ . - userId, data = train)
fancyRpartPlot(fit1);
test$t&lt;-predict(fit1,type='class',test)

################## PLot tree with priors 
#score test data 
test$score1 &lt;- predict(fit2,type = 'prob',test)
pred5&lt;-prediction(test$score1[,2],test$verifikation)
perf5&lt;- performance(pred5,""tpr"",""fpr"")

#90-10 priors with smaller complexity parameter to allow more complex trees
fit2 &lt;- rpart(verifikation ~ . - userId , data = train,method = ""class"",parms = list(prior=c(.9,.1)),cp=.0002)
plot(fit2);text(fit2,pos=2,cex=0.1,col=""blue"");

#compare complexity
printcp(fit1)
printcp(fit2)
plotcp(fit2)

#convert trees to rules 
amess&lt;-asRules(fit2)
t.b&lt;-rpart.rules.table(fit2)
library(rattle)
library(rpart.plot)
library(RColorBrewer)

fancyRpartPlot(fit2)
</code></pre>

<p>And here is the output of fancyRpartPlot(fit2) </p>

<p><a href=""http://i.stack.imgur.com/otsP3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/otsP3.png"" alt=""enter image description here""></a></p>

<p>My goal is to extract some useful rules from the entire process to implement in a score card. </p>
"
"0.104084994078794","0.112425109314872","230709","<p>I'm working on a dataset with around 7000 points, a binary response variable and (at minimum) five predictors, two of which are binary, one is a number, and two are factors. One of those factors (""speaker"") is the participant id, with 26 levels once participants with categorical behaviour are excluded. The other (""vowel"") is a variable with six levels with represents the main condition I'm interested in. I (mostly) have a minimum of 20 tests per participant-condition combination. What I'm specifically trying to investigate is whether the effect of the different conditions differs for different participants. For that reason, I've tried to fit a binomial regression with an interaction between speaker and vowel. Here's the call:</p>

<pre><code>glm_vowel_interactions&lt;-glm(formula = perceptually.rhotic ~ vowel * speaker + function_word + modified_clip_start + prepausal, family = ""binomial"", data =data_excluding_rare_vowels_and_categorical_speakers)
</code></pre>

<p>This appears to work. However, it produces some alarmingly large (> 15 and &lt; -15) coefficient estimates. I ran it with the safeBinaryRegession package, and sure enough, it reports that separation is occurring with 33 points - 30 specific vowel/speaker (participant) combinations (of the 162 total combinations) and two specific values of ""speaker"" (which I think represent separation in the combination of those participants and the reference level for ""vowel""):</p>

<pre><code>Error in glm(formula = perceptually.rhotic ~ vowel * speaker + function_word +  : 
  The following terms are causing separation among the sample points: speaker4, speaker7, vowelNURSE:speaker19, vowellettER:speaker21, vowelNURSE:speaker21, vowelNORTH~FORCE:speaker22, vowelNEAR:speaker24, vowelNURSE:speaker27, vowellettER:speaker28, vowelSTART:speaker28, vowellettER:speaker4, vowelNEAR:speaker4, vowelNORTH~FORCE:speaker4, vowelNURSE:speaker4, vowelSTART:speaker4, vowelNEAR:speaker6, vowelNORTH~FORCE:speaker6, vowelSTART:speaker6, vowellettER:speaker7, vowelNEAR:speaker7, vowelNORTH~FORCE:speaker7, vowelNURSE:speaker7, vowelSTART:speaker7, vowelSTART:speakerb3, vowelNEAR:speakerb5, vowelNURSE:speakerb5, vowelSTART:speakerb5, vowelNEAR:speakerb6, vowelNEAR:speakerb7, vowelSTART:speakerb7, vowelNEAR:speakerb8, vowelNURSE:speakerb8, vowelSTART:speakerb8
</code></pre>

<p>Looking at my data I find that many of these are indeed categorical cells, with participants producing 100% or 0% TRUE of the response variable in that condition. As there are participants who overall produce mostly TRUE (up to 96%) or mostly FALSE (as low as 5%), it's unsurprising that there should be some categorical cells and doesn't necessarily imply that there's anything drastically unusual going on in these cells.</p>

<p>So, how can I investigate the differing effect of this condition (""vowel"") on the response variable for the different participants, while taking into account my other independent variables? I assume I can't interpret the results of the regression that did converge without safeBinaryRegression, as the coefficients and p-values are likely to be inflated? I came across <a href=""http://www.carlislerainey.com/papers/separation.pdf"" rel=""nofollow"">this paper</a> which suggests adding a prior to limit the largest possible coefficient estimates, so that instances with separation aren't infinite - would this be a sensible approach here? If so, is it implemented in R? What packages are recommended? Alternatively, should I be using some other (non regression?) method to investigate this question?</p>

<p>Thanks very much for your help!</p>

<p>Full disclosure: I've cross-posted this on the Maths stackexchange, but my experience is that here is more active!</p>
"
"0.026822089039291","0","230776","<p>I'am trying to do a multinomial logistic regression to explain the political orientations in Tunisia but I'am really confused about using the vglm() function from package VGAM or the multinom() function from package nnet or the mlogit function... can someone tell me please the difference and wich one is the best ?
Thank you very much</p>
"
"0.018966081045272","0.0385614943639849","230797","<p>I am new to R coding and was hoping someone could help. Am trying to create a regression line where the dependent variable is a proportion (I only have the proportion, not the denominator and numerator). With it being a proportion a linear regression line isn't appropriate as it needs to sit between 0-1, I think a sigmoid shape would be best. So far I have had limited success with the Loess function, however ideally I want to be able to gain the coefficients and AIC from the regression. So far the best shape I have been able to obtain is using the binomial family in ggplot2, but I don't think this is an appropriate distribution.</p>

<p>I have attached my code, am wondering if anyone could suggest an improvement.</p>

<pre><code> c &lt;- ggplot(dat, aes(y=ITN_Coverage, x=Study_Date))
c + stat_smooth(method =""glm"",  method.args = list(family=""binomial""), size=0.5, col = ""black"") + geom_point(aes(color = Country)) + 
labs(title = ""Scatter plot: Insecticide treated net coverage against year"", x= ""Study date"", y= ""ITN coverage"")
</code></pre>
"
"0.0969560705490221","0.105605001571314","231059","<p>So first of all I did some research on this forum, and I know <a href=""http://stats.stackexchange.com/questions/140991/comparing-difference-between-two-polynomial-regression-models-in-r"">extremely similar</a>  questions have been asked but they usually haven't been answered properly or sometimes the answer are simply not detailed enough for me to understand. So this time my question is : I have two sets of data, on each, I do a polynomial regression like so :</p>

<pre><code>Ratio&lt;-(mydata2[,c(2)])
Time_in_days&lt;-(mydata2[,c(1)])
fit3IRC &lt;- lm( Ratio~(poly(Time_in_days,2)) )
</code></pre>

<p>The polynomial regressions plots are:</p>

<p><a href=""http://i.stack.imgur.com/T7r3i.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/T7r3i.png"" alt=""enter image description here""></a></p>

<p>The coefficients are :</p>

<pre><code>&gt; as.vector(coef(fit3CN))
[1] -0.9751726 -4.0876782  0.6860041
&gt; as.vector(coef(fit3IRC))
[1] -1.1446297 -5.4449486  0.5883757 
</code></pre>

<p>And now I want to know, if there is a way to use an R function to do a test that would tell me whether or not there is a statistical significance in the difference between the two polynomials regression knowing that the relevant interval of days is [1,100].</p>

<p>From what I understood I can not apply directly the anova test because the values come from two different sets of data nor the AIC, which is used to compare model/true data.</p>

<p>I tried to follow the instructions given by @Roland in the related question but I probably misunderstood something when looking at my results :</p>

<p>Here is what I did : </p>

<p>I combined both my datasets into one.</p>

<p><code>f</code> is the variable factor that @Roland talked about. I put 1s for the first set and 0s for the other one.</p>

<pre><code>y&lt;-(mydata2[,c(2)])
x&lt;-(mydata2[,c(1)])
f&lt;-(mydata2[,c(3)])

plot(x,y, xlim=c(1,nrow(mydata2)),type='p')

fit3ANOVA &lt;- lm( y~(poly(x,2)) )

fit3ANOVACN &lt;- lm( y~f*(poly(x,2)) )
</code></pre>

<p>My data looks like this now :</p>

<p><a href=""http://i.stack.imgur.com/dNpMQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dNpMQ.png"" alt=""enter image description here""></a></p>

<p>The red one is <code>fit3ANOVA</code> which is still working but I have a problem with the blue one <code>fit3ANOVACN</code> the model has weird results. I don't know if the fit model is correct, I do not understand what @Roland meant exactly.</p>

<p>Considering @DeltaIV solution I suppose that in that case :
<a href=""http://i.stack.imgur.com/HLLp9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HLLp9.png"" alt=""enter image description here""></a>
The models are significantly different even though they overlap. Am I right to assume so ?</p>
"
"0.053644178078582","0.0545341883149212","231260","<p>This is my first post here, so please bear with me! I'm comparing several biomarkers with Kaplan-Meier curves and calculating hazard ratios for different risk groups (defined by a certain, well established cut-off value of the biomarker) by using Cox regression in R. We have 3 tiers with a low, intermediate and high risk group, however the low risk group for one biomarker contains no events.</p>

<p>This, so my understanding, leads to quasi-separation in the Cox regression and hence infinite values and large coefficients and SEs. I understand the Likelihood ratio is still valid, but what I'm obviously interested in is a calculation of the HR from exp(coef). A sample of the data is displayed below:</p>

<pre><code>   &gt; head(riskgroups)
   ID FU_3y_death FU_3y_death_days biomarker bm.riskcat
   1           0             1095           58.2    group.3
   2           1               79           11.5    group.2
   22           0             1095           11.7    group.2
   27           0              929            9.0    group.2
   44           0              949            7.0    group.2
   46           0             1095            7.5    group.2
</code></pre>

<p>Now I have found that using Firth's method might allow a workaround, hence I've tried to run the analysis using coxphf with the following code:</p>

<pre><code>cox.groups &lt;- coxphf(riskgroups, formula=Surv(FU_3y_death_days,FU_3y_death) ~ bm.riskcat, pl=T, firth = T)
</code></pre>

<p>Rather bizarrely, this results in the following error:</p>

<blockquote>
  <p>Error in coxphf(riskgroups, formula = Surv(FU_3y_death_days, FU_3y_death) ~  : 
    NA/NaN/Inf in foreign function call (arg 3)</p>
</blockquote>

<p>I would have assumed that this is exactly what coxphf is trying to avoid? When setting pl to FALSE (to base the tests on the Wald method instead of profile penalised LL) I get results with all NaN. Of course the fact that there is no event in the lowest risk group is in itself an important message, but I do require hazard ratios for the second and third tier of risk categories to compare the different biomarkers. Any bright thoughts on this, my research into this has hit a wall after 3 days of reading...</p>
"
"0.100582833897341","0.102251603090477","231435","<p>So basically I was trying to compare two models in R using the anova function, here is what my data looks like :</p>

<p><a href=""http://i.stack.imgur.com/gVvJD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gVvJD.png"" alt=""enter image description here""></a></p>

<p>I wanted to compare tose two plots (know if there was a statistical difference), in order to do so I was advised to compare two regression, one model where regression was applied to all the points I had and one model where in the regression model I took into account the fact variable.</p>

<p><code>log(y) ~ poly(x, 3) * fact</code>  vs <code>log(y) ~ poly(x, 3)</code> </p>

<p>(First I tried to use <code>y ~ poly(x,3)</code>, but the <code>log(y)</code> seemed graphically like a way better fit, this might be one of the mistakes I made)</p>

<p>Using the Anova test would then have told me whether the models where statistically different.</p>

<p>But when doing so, I had weird results. When I compared the two models, I had very low p-value (<code>&lt;2.2e-16</code>) even though, they intuitively didn't seem that different.</p>

<p>Just to make sure everything was working correctly, I wanted to know if the degree for the polynomial regression that used (3) was a good fit. So I did an ANOVA test on <code>log(y) ~ poly(x, n) * fact</code>  vs <code>log(y) ~ poly(x, n+1) * fact</code>. Until the values were not significant anymore, but it went on (I had p-values<code>&lt;2.2e-16</code>) till n=10. When I made the same algorithm on the regessions without the <code>* fact</code>, it only got through n=3, which graphically seemed way more logical. I do know that I shouldn't only trust the graphical aspect of things, but I'm afraid I may be overfitting my models, chosing the wrong model, using the <code>anova()</code> function the wrong way.</p>

<p>Here is the graphical aspect (regressions with fact taken into account) of things that made me wonder whether my results were accurate or not : (polynomial degrees are 6,9,10 in that order)</p>

<p><a href=""http://i.stack.imgur.com/4pTOy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4pTOy.png"" alt=""Degree 6""></a></p>

<p><a href=""http://i.stack.imgur.com/FcOqK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FcOqK.png"" alt=""Degree 9""></a></p>

<p><a href=""http://i.stack.imgur.com/eyf4H.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eyf4H.png"" alt=""Degree 10""></a></p>

<p>Maybe the <code>anova()</code> function in R isn't as accurate for models with grouping variables which is why when I compare the models with <code>* fact</code> to the ones without it I always get a p-value which is <code>&lt;2.2e-16</code>.</p>

<p>I also believe I may have chosen a wrong model by doing a polynomial regression on my functions which is why all my results would be useless.</p>

<p>I don't know if adding the actual data would be useful to anyone but if so I'll edit it in.</p>
"
"0.0379321620905441","0.0385614943639849","231591","<p>I am running a regression using the gam function.</p>

<pre><code>fit &lt;- gam(y~ s(x))
</code></pre>

<p>The smoothing parameter used is the default one and number of knots are kept to be determined automatically.</p>

<p>The plot below shows the scatter plot for variable x and the smoothed curve. I fail to understand as why is there an upward trend in the smoothed curve when there is no such pattern seen in the scatter plot. Please advice where am I going wrong. Thanks.</p>

<p><a href=""http://i.stack.imgur.com/AgS4k.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AgS4k.png"" alt=""enter image description here""></a></p>
"
"0.053644178078582","0.0409006412361909","231974","<p>Many Gaussian process packages are available in R. For example there is $\textbf{BACCO}$ that offers some calibration techniques, $\textbf{mlegp}$ and $\textbf{tgp}$ focusing on treed models and parameter estimation and $\textbf{GPML}$ for Gaussian process classification and regression. </p>

<p>The problem with these packages is that the choice of correlation function is restricted. Only some choices are provided for building the correlation function (Gaussian, Matern, etc...).</p>

<p>Does anyone have specific experience on how to can insert my own correlation function and just use the optimization routines available in these packages which are specifically tailored for GP's. Or is there a package which allows me to do that ?</p>
"
"0.0709645772411954","0.072141950116023","232450","<p>We are developping a software that run hierarchical linear model in R with the lme4 package. The model we are trying to fit is of the following shape:</p>

<pre><code>&gt; data
ID | Dummy_1 | Dummy_2 | Dummy_3 | Rating 
1  | 0       | 1       | 1       | 14
1  | 1       | 0       | 1       | 15
1  | 0       | 1       | 0       | 11
2  | 1       | 0       | 1       | 15
2  | 1       | 0       | 0       | 12

x = lmer(formula = Rating ~ Dummy_1 + Dummy_2 + Dummy_3 + (1 + Dummy_1 + Dummy_2 + Dummy_3 | ID), data = data)
</code></pre>

<p>It is important to note that this is the shape that the data will take, however, Rating can have very different range depending on the data te user provide.</p>

<p>The example above illustrate a limit case we are trying to deal with which occurs when the dummy variables perfectly predict the independant variable.</p>

<p>Here we can see that for example with <code>intercept = 10</code>, <code>B1 = 2</code>, <code>B2 = 1</code> and <code>B3 = 3</code> we perfectly predict the Rating variable. It implies first that the <code>ID</code> is useless and that we are in a case of <code>complete separation</code>.</p>

<p><strong>Question:</strong> How do you deal with (quasi-)complete separation when the independant variable is not binomial but continuous as it is the case here ? I could only find explanations for logistic regression. Please, ignore the fact that I use a linear regression for discrete-ordinal data and that I treat them as continuous :)</p>

<p>So that you know the warnings I get from R are the following:</p>

<pre><code>1: In optwrap(optimizer, devfun, getStart(start, rho$lower,  ... :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  unable to evaluate scaled gradient
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  Model failed to converge: degenerate  Hessian with 4 negative eigenvalues
</code></pre>
"
"0.0379321620905441","0.0385614943639849","232548","<p>I'm doing research into understand the influential factors within a logistic regression model I've built in R using the glm() function.</p>

<p>From my research, it seems that using the summary() function to summarize the model is a popular method to identify which variables are significant. What I can't seem to find however is a description of what the summary function is doing to determine the significance codes (eg. the *) for each variable. <a href=""http://stats.stackexchange.com/questions/72258/how-to-interpret-the-significance-code"">This answer</a> states that the significance codes are simply categorizations of the p-value, but I don't really understand that. </p>

<p>Is there anyone out there that could maybe help me understand how R computes this?</p>
"
"0.0464572209811883","0.0472279924554862","232682","<p>I have a <em>balanced panel</em> dataset in which I have observations for N countries across T quarters. I use a <em>fixed effect model</em> to explore the relationship between my variables. Since all countries have the same number of observations, they are all given the same ""weight"" in the regression. I would like to apply a weigthing by GDP such that observations of big countries (in terms of wealth) have more impact on my estimation. My first idea was to compute weights for all countries and rescale the dependent variable. However, I think this method will only affect the (country-specific) intercepts and won't have much impact on the coefficients. I also don't think that controlling for GDP by including it in the regression is the solution. </p>

<p>I am working with the R plm package to estimate my models. I know that the lm function has a weight parameter but couldn't find an equivalent in the plm package. </p>

<p>Any idea how I should proceed?</p>
"
"0.0379321620905441","0.0385614943639849","233063","<p>I have created a logistic regression in R and would like to use the trained model to create an predict function (lets say in Excel).  How can I convert the coefficients into a predict equation?</p>

<pre><code>glm(formula = is_bad ~ is_rent + dti + bc_util + open_acc +    pub_rec_bankruptcies + 
chargeoff_within_12_mths, family = binomial, data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8659  -0.5413  -0.4874  -0.4322   2.4289  

Coefficients:
                            Estimate Std. Error  z value Pr(&gt;|z|)    
(Intercept)              -2.9020574  0.0270641 -107.229  &lt; 2e-16 ***
is_rentTRUE               0.3105513  0.0128643   24.141  &lt; 2e-16 ***
dti                       0.0241821  0.0008331   29.025  &lt; 2e-16 ***
bc_util                   0.0044706  0.0002561   17.458  &lt; 2e-16 ***
open_acc                  0.0030552  0.0012694    2.407   0.0161 *  
pub_rec_bankruptcies      0.1117733  0.0163319    6.844 7.71e-12 ***
chargeoff_within_12_mths -0.0268015  0.0564621   -0.475   0.6350    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 173006  on 233017  degrees of freedom
Residual deviance: 170914  on 233011  degrees of freedom
(2613 observations deleted due to missingness)
AIC: 170928

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.0663812836584521","0.0771229887279699","233406","<p>I am using the <code>Cochrane.orcutt</code> procedure to do a time series analysis.</p>

<p>I did the initial regression with the <code>lm</code> function, and then past the result to <code>Cochrane.orcutt</code>. </p>

<pre><code>reg &lt;- lm(data$Y ~ data$Y_Lag1 + data$X1 + data$X2)

regfinal &lt;- cochrane.orcutt(reg)
regfinal
Cochrane.Orcutt

Call:
lm(formula = YB ~ XB - 1)

Residuals:
 Min       1Q   Median       3Q      Max 
-0.72984 -0.32750  0.03553  0.17989  0.69595 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
XB(Intercept)                0.39347    0.09500   4.142  0.00252 ** 
XBdata$Y_Lag1                1.05427    0.09556  11.033 1.57e-06 ***
XBdata$X1                   -3.16739    0.80754  -3.922  0.00350 ** 
XBdata$X2                   -3.30504    0.86569  -3.818  0.00410 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.447 on 9 degrees of freedom
Multiple R-squared:  0.9451,    Adjusted R-squared:  0.9208 
F-statistic: 38.76 on 4 and 9 DF,  p-value: 1.115e-05


$rho
[1] -0.6855955

$number.interaction
[1] 8
</code></pre>

<hr>

<p>I am doing a time series with regressors.</p>

<pre><code>Y_t = a + b*Y_t-1 +c*X1_t +d*X2_t
</code></pre>

<p>I just have a few questions.</p>

<ol>
<li><p>This Cochrane procedure looks like it gives back a list, not a lm. I want to plot the fitted vs actuals, and also use the predict function. The code below gives an error (whereas it works on stuff from ""lm"")</p>

<pre><code>plot(data$Y,col=""red"")
    lines(data$Y)
lines(fitted(regfinal),col=""blue"")
</code></pre></li>
<li><p>Is what I am doing with the regression correct for a theory point of view? That is, including in the lagged value of the response? I am new to Regression with ARMA errors. I have seen somewhere that you are supposed to run the standard regression with only the predictors (no Y_Lag1), and then you get the residuals and you build a model with it. The problem is, when I exclude Y_Lag1 from my lm, the significance of everything else drops away:</p></li>
</ol>

  

<pre><code>Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)
XB(Intercept)               -0.02025    0.73246  -0.028    0.978
XBdata$X1                   -1.31001    0.98394  -1.331    0.213
XBdata$X2                   -1.32077    1.42550  -0.927    0.376

Residual standard error: 0.8196 on 10 degrees of freedom
Multiple R-squared:  0.2635,    Adjusted R-squared:  0.04256 
F-statistic: 1.193 on 3 and 10 DF,  p-value: 0.3616


$rho
[1] 0.6886191

$number.interaction
[1] 6
</code></pre>
"
"0.0709645772411954","0.072141950116023","233619","<p>I have data from an EEG experiment. Originally, the design was balanced, but because in EEG you loose a lot of data to noise and malfunctioning equipment, the end-result is unbalanced data. </p>

<p>I am working with R. In R, my data looks like this:</p>

<pre><code>          Name                 Condition Channel EpochCountStd EpochCountDeviant ErpMinTime ErpMinVoltage ErpMinAv Frequency
380   AY11042016B1-Deci.EEG      HFMM     AF3           423               103        203          -1.2     -1.1      High
388 AvdP23052016B1-Deci.EEG      HFMM     AF3           410               101        144          -3.7     -3.2      High
397   EW20042016B1-Deci.EEG      HFMM     AF3           457               118        123          -2.6     -2.3      High
413  IG160312016B1-Deci.EEG      HFMM     AF3           435               105        214          -2.2     -1.6      High
422  IJB18042016B1-Deci.EEG      HFMM     AF3           408               110        121          -1.3     -1.1      High
439   MC31032016B1-Deci.EEG      HFMM     AF3           438               101        116          -4.0     -3.3      High
        Hemisphere  Region      Lexicality Subject
380       Left AnterioFrontal       Word Subject08
388       Left AnterioFrontal       Word Subject14
397       Left AnterioFrontal       Word Subject12
413       Left AnterioFrontal       Word Subject02
422       Left AnterioFrontal       Word Subject10
439       Left AnterioFrontal       Word Subject05
</code></pre>

<p>For present purposes, my dependent variable is <code>ErpMinAv</code>, a continuus numerical variable and my independent variables are <code>Lexicality</code> (2 levels), <code>Frequency</code>(2 levels), <code>Hemisphere</code>(3 levels) and <code>Region</code> (5 levels).</p>

<p>Moreover, I have tested for sphericity and the result is highly significant, meaning my data violate the sphericity assumptions. </p>

<p>I have already run anovas and regressions on my data, but I am always onsure of some things:</p>

<ol>
<li>Should I be using anova if I am violating the sphericity assumption? I understand most statistical programs include corrections in the tests themselves, but from what I have read on the internet, this is not necessarily the case in R. I have seen people recommending the use of <code>lm()</code> or <code>lme4()</code>. </li>
<li><p>How should I order my independent variables in the R syntax? So far, I tried:</p>

<pre><code>MM_Model &lt;- aov(ErpMinAv ~ Lexicality*Frequency*Hemisphere*Region 
                + Error(Subject),data = MM_Table)
</code></pre></li>
</ol>

<p>But I have seen different ways of ordering the factors (e.g. with sums instead of multiplication signs) and many different ways of writing the <code>Error()</code> term (e.g. like this: <code>Error(Subject/(v1*v2*v3*v4</code>). I have not, however, been able to find explanations for a case where there is no sphericity and the data is unbalanced. </p>
"
"0.0379321620905441","0.0385614943639849","233875","<p>I am running randomForest regression. My observation number is 70 and independent variables is 122. For variable selection for the final model, I combined rfcv() function and Importance(randomForest) function. For example, if the model with six variables has lowest OOB error based on rfcv. Then the final model will have the six most importance variable based on Importance function.</p>

<p>I wonder that whether my approach is appropriate or not?</p>
"
"0.0379321620905441","0.0385614943639849","233913","<p>I am aware of the theory of stochastic gradient descent, which is a faster way of developing linear regression. Through this we can have an '<em>optimized implementation</em>' of linear regression. There are similar techniques for non-parametric methods as well, which allows you to converge faster keeping in mind cost function.</p>

<p>I need suggestion for a book which has worked out implementations or examples of these type of optimized models with R/Python code or pseudo code. So that i can run sophisticated machine learning algorithms faster, without increasing my hardware further. I am open about increasing hardware though. What interests me is a faster implementation of techniques, so that i can use scalable implementations of machine learning algorithms for bigger data.</p>

<p>Thanks!!</p>
"
"0.018966081045272","0.0385614943639849","234220","<p>I am new to time series analysis. I am trying to use the R package dlm for time-varying regression with multiple regressors. I got the basic dlmModReg to work, so I can get the coefficients for all the regressors as a function of time. My question is, what's the correct way to estimate the relative importance of each regressor at each time point (or within some window)? Thank you!</p>
"
"0.0727844771755901","0.0739920439992998","234470","<p>I have data from these set of experiments:</p>

<p>In each experiment I infect a neuron with a rabies virus. The virus climbs backwards across the dendrites of the infected neuron and jumps across the synapse to the input axons of that neuron. In the input neurons the rabies will then express a marker gene thereby labeling them. This allows me to see which neurons are inputs to the target neuron I infected and thus create a connectivity map of a certain region in the brain.</p>

<p>In each experiment I obtain counts of all the infected input neurons of the target neuron I infected.</p>

<p>Here's a simulation of the data: (3 targets and 5 inputs)</p>

<pre><code>set.seed(1)
probs &lt;- list(c(0.4,0.1,0.1,0.2,0.2),c(0.1,0.3,0.4,0.1,0.1),c(0.1,0.1,0.4,0.2,0.2))
mat &lt;- matrix(unlist(lapply(probs,function(p) rmultinom(1, as.integer(runif(1,50,150)), p))),ncol=3)
inputs &lt;- LETTERS[1:5]
targets &lt;- letters[1:3]
df &lt;- data.frame(input = c(unlist(apply(mat,2,function(x) rep(inputs ,x)))),target = rep(targets ,apply(mat,2,sum)))
</code></pre>

<p>What I'd like to estimate is the effect of each target neuron on these counts, relative to the grand mean. I was thinking that a multinomial regression model is appropriate in this case, where the contrasts are set to the <code>contr.sum</code> option:</p>

<pre><code>library(foreign)
library(nnet)
library(reshape2)

df$input &lt;- factor(df$input,levels=inputs)
df$target &lt;- factor(df$target,levels=targets)
fit &lt;- multinom(input ~ target, data = df,contrasts = list(target = ""contr.sum""))
# weights:  20 (12 variable)
initial  value 505.363505 
iter  10 value 445.057386
final  value 441.645283 
converged
</code></pre>

<p>Which gives me:</p>

<pre><code>&gt; summary(fit)$coefficients
  (Intercept)   target1   target2
B  0.08556288 -1.743854 1.6062660
C  0.55375003 -2.094266 1.2616939
D -0.17624590 -1.364270 0.6284231
E -0.04091248 -1.617374 0.6601274
</code></pre>

<p>So the effects for <code>input A</code> are not reported and I would like to obtain both the effects of all <code>targets</code> on all <code>inputs</code>.</p>

<p>I'm wondering if adding a mean across <code>targets</code> and a mean across <code>inputs</code>, and setting them as baseline <code>dummy</code> variables is a good solution:</p>

<pre><code>#add target mean
mat &lt;- cbind(mat,round(apply(mat,1,mean)))
colnames(mat)[ncol(mat)] &lt;- ""x""
targets &lt;- c(targets,""x"")

#add input mean
mat &lt;- rbind(mat,round(apply(mat,2,mean)))
rownames(mat)[nrow(mat)] &lt;- ""X""
inputs &lt;- c(inputs,""X"")
</code></pre>

<p>So <code>x</code> and <code>X</code> represent the means of <code>targets</code> and <code>inputs</code>, respectively, and are rounded so that they are counts.</p>

<pre><code>df &lt;- data.frame(input = c(unlist(apply(mat,2,function(x) rep(inputs ,x)))),target = rep(targets ,apply(mat,2,sum)))

df$input &lt;- factor(df$input,levels=rev(inputs))
df$target &lt;- factor(df$target,levels=rev(targets))
</code></pre>

<p>And then fit the <code>multinom</code> regression using <code>dummy coding</code>:</p>

<pre><code>fit &lt;- multinom(input ~ target, data = df)
</code></pre>

<p>Thanks</p>
"
"0.0758643241810882","0.0771229887279699","234537","<p>I've split my data set into a training and test set. I've performed a principal component analysis on the training set and have used the first 3 principal components to generate a logistic regression model for my response.</p>

<p>I now want to use this model to make predictions for my test data set and check if this is true. </p>

<p>I've been trying to use the predict function but obviously the model uses the principal components of the training set as the predictors whereas my test set just has all the original predictors so obviously they're not compatible.</p>

<p><strong>How do I go about 'projecting' my test data onto the principal components I've already generated so I can use my model to make predictions?</strong></p>

<p>Ideally I'd like to do this without using any external packages (it's for university). I am working in R.</p>
"
"0.0599760143904067","0.0609710760849692","234538","<p>I am trying to combine two linear models (one linear-quadratic and one linear) into one unified model by means of piecewise regression. The tail of the lhs (linear-quadratic part) should continue to be the asymptote for the rhs (linear part). Here's <a href=""http://www.intechopen.com/source/html/45395/media/image4.png"" rel=""nofollow"">a link</a>! The piecewise function is,</p>

<p>$$y = ax + bx^2,\ x \lt x_t$$ and</p>

<p>$$y = cx + d,\ x \ge x_t$$</p>

<p>where $a, b, c, d$ and $x_t$ (a breakpoint) are parameters to be determined. This unified model should be compared with the linear-quadratic model for the whole range of $x$ by R.squared.adjusted as a measure of goodness of fit.</p>

<pre><code>&gt; y
[1] 1.00000 0.59000 0.15000 0.07800 0.02000 0.00470 0.00190 1.00000 0.56000 0.13000 0.02500 0.00510 0.00160 0.00091 1.00000 0.61000 0.12000
[18] 0.02600 0.00670 0.00085 0.00040
&gt; x
[1] 0.00  5.53 12.92 16.61 20.30 23.07 24.92  0.00  5.53 12.92 16.61 20.30 23.07 24.92  0.00  5.53 12.92 16.61 20.30 23.07 24.92
</code></pre>

<p>I'm after continuity of the first derivative and to find the parameters, including determining the breakpoint. Since i want continuity at $x=x_t$, I have rewritten the piecewise function to</p>

<p>$$y = ax + bx^2,\ x \lt x_t$$ and</p>

<p>$$y = ax_t + bx_t^2 + k(x - x_t),\ x \ge x_t$$</p>

<p>where $k$ is a constant. So my attempt goes as follows (assuming I have derived $x_t$ theoretically):</p>

<pre><code>I = ifelse(x &lt; xt, 0, 1)*(x - xt)
x1 = ifelse(x &lt; xt, x, xt)
mod = lm(y ~  x1 + I(x1^2) + I)
</code></pre>

<p>But the tail (asymptote) doesn't seem to be parallel to the linear part in the upper range...</p>
"
"0.0464572209811883","0.0472279924554862","234741","<p>I am looking for the equivalent of constrained nonlinear regression (CNLR command) in SPSS in R.</p>

<p>The constraints on the parameter needs to be a <em>function</em> rather than simply being that the parameters need to lie in certain region (thus having simple upper or lower bounds)</p>

<p>I have looked at the <em>nloptr</em> and <em>optim</em> packages but it appears not to be what I am looking for.</p>

<p>My SPSS syntax:</p>

<pre><code>MODEL PROGRAM a=0.21771052038064 b=0.69276310944875 c=1.3015054535771 
d=-0.95674643053895  .
COMPUTE PRED_ = a + b * X + c * X ** 2 + d * X ** 3 .
CONSTRAINED FUNCTION.
COMPUTE CONSTR1_= sin((a) + (b*(-1)) + 
            (c*(-1)**2) + (d*(-1)**3)  ) *
            cos((a) + (b*(-1)) + (c*(-1)**2) + (d*(-1)**3)  ) *
            ((b) + (2*c*(-1)) + (3*d*(-1)**2)  ) .
COMPUTE CONSTR2_= sin((a) + (b*(1)) + 
            (c*(1)**2) + (d*(1)**3)  ) *
            cos((a) + (b*(1)) + (c*(1)**2) + (d*(1)**3)  ) *
            ((b) + (2*c*(1)) + (3*d*(1)**2)  ) .

_set Printback off.
CNLR G
  /OUTFILE='TempFile'
  /PRED PRED_
  /BOUNDS CONSTR1_ &gt; 0;CONSTR2_ &gt; 0
  /SAVE PRED
  /CRITERIA STEPLIMIT 2 ISTEP 1E+20 .
</code></pre>

<p>a, b, c and d being my starting values, X my data points.</p>
"
"0.0599760143904067","0.0609710760849692","234763","<p>I have a fairly large dataset ($\approx 3 \bar{M}$ observations for a dozen candidate predictors) and I would like to perform a logistic regression on that dataset.
I have a problem of separation in that dataset so usual model can't converge. That's why I am using Firth penalization (logistf package for R) to have my model to adjust.</p>

<p>I would like to select the best subset of variables for my final model but I can't find the proper way to do that. I know that stepwise selection is out of question and I usually would use L1 or L2 penalized regression so that some coefficients are reduced to 0.</p>

<p>My problem is : the function I am using to adjust my model doesn't handle extra penalization so no Elasticnet-Firth regression.</p>

<p>Is there, apart from stepwise selection, another way to select my variables ?</p>

<p>Thanks in advance !</p>
"
