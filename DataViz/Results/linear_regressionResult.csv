"V1","V2","V3","V4"
"0.09392108820677","0.0920574617898323","  1571","<p>I am trying to recreate (in R) a frequentist hypothesis testing in Bayesian from, by calculating Bayes factors of the null (H0) and alternative (H1) models.</p>

<p>The model is simply a simple linear regression that tries to detect a trend in global temp. data from 1995 to 2009 (<a href=""http://www.cru.uea.ac.uk/cru/data/temperature/hadcrut3gl.txt"" rel=""nofollow"">here</a>). Therefore, H0 is no trend (i.e. slope = 0), or similary, the H0 model is a linear model with only the intercept. </p>

<p>So I calculated the <code>lm()</code> of both models to arrive at negative log likelihood values that are significantly different. The p-value for the H1 lm() model is 0.0877.</p>

<p>I also calculated this in a Bayesian way by using <a href=""http://cran.r-project.org/web/packages/MCMCpack/index.html"" rel=""nofollow"">MCMCpack</a>, and I get negative log likelihood values that are <strong>super duper uber</strong> different. Log likelihood values of 13.7 and 4.3 are about a 10000 fold difference in their likelihood ratios (where <a href=""http://en.wikipedia.org/wiki/Bayes_factor"" rel=""nofollow"">>100 is considered to be ""decisive""</a>).</p>

<p>The means and sds of the estimates are very similar, so why am I getting such different likelihood values? (particularly for the Bayesian H0 model) I feel like there is a gap in my understanding on marginal likelihoods, but I can't pinpoint the problem.</p>

<p>Thanks</p>

<pre><code>library(MCMCpack)

## data: http://www.cru.uea.ac.uk/cru/data/temperature/hadcrut3gl.txt

head(hadcru, 2)
##  Year      1      2      3      4      5      6      7      8      9     10
## 1 1850 -0.691 -0.357 -0.816 -0.586 -0.385 -0.311 -0.237 -0.340 -0.510 -0.504
## 2 1851 -0.345 -0.394 -0.503 -0.480 -0.391 -0.264 -0.279 -0.175 -0.211 -0.123
##       11     12    Avg
## 1 -0.259 -0.318 -0.443
## 2 -0.141 -0.151 -0.288

hadcru.lm &lt;- lm(Avg ~ 1 + Year, data = subset(hadcru, (Year &lt;= 2009 &amp; Year &gt;= 1995)))
hadcru.lm.zero &lt;- lm(Avg ~ 1, data = subset(hadcru, (Year &lt;= 2009 &amp; Year &gt;= 1995)))

hadcru.mcmc &lt;- MCMCregress(Avg ~ 1 + Year, data = subset(hadcru, (Year &lt;= 2009 &amp; Year &gt;= 1995)), thin = 100, mcmc = 100000, b0 = c(-20, 0), B0 = c(.00001, .00001), marginal = ""Laplace"")
hadcru.mcmc.zero &lt;- MCMCregress(Avg ~ 1, data = subset(hadcru, (Year &lt;= 2009 &amp; Year &gt;= 1995)), thin = 100, mcmc = 100000, b0 = c(0), B0 = c(.00001), marginal = ""Laplace"")

-logLik(hadcru.lm)
## 'log Lik.' -14.55338 (df=3)
-logLik(hadcru.lm.zero)
## 'log Lik.' -12.80723 (df=2)

attr(hadcru.mcmc, ""logmarglike"")
##           [,1]
## [1,] -13.65188
attr(hadcru.mcmc.zero, ""logmarglike"")
##           [,1]
## [1,] -4.310564
</code></pre>

<p><img src=""http://www.skepticalscience.com/images/HadCRUT_1995_2009.gif"" alt=""alt text""></p>
"
"0.02831827358943","0.0277563690826684","  2854","<p>Dear all, 
I was encouraged to ask this question here as well as on stackoverflow and would be very appreciative of any answers...</p>

<p>Due to hetereoscedasticity I'm doing bootstrapped linear regression (appeals more to me than robust regression). I'd like to create a plot along the lines of what I've done in the script here. However the <code>fill=int</code> is not right since <code>int</code> should (I believe) be calculated using a bivariate normal distribution. </p>

<ul>
<li>Any idea how I could do that in this setting? </li>
<li>Also is there a way for <code>bootcov</code> to return bias-corrected percentiles?</li>
</ul>

<p>sample script:</p>

<pre><code>library(ggplot2) 
library(Hmisc) 
library(Design) # for ols()

o&lt;-data.frame(value=rnorm(10,20,5),
              bc=rnorm(1000,60,50),
              age=rnorm(1000,50,20),
              ai=as.factor(round(runif(1000,0,4),0)),
              Gs=as.factor(round(runif(1000,0,6),0))) 

reg.s&lt;-function(x){      
    ols(value~as.numeric(bc)+as.numeric(age),data=x,x=T,y=T)-&gt;temp 
    bootcov(temp,B=1000,coef.reps=T)-&gt;t2 

    return(t2) 
    } 

dlply(o,.(ai,Gs),function(x) reg.s(x))-&gt;b.list 
llply(b.list,function(x) x[[""boot.Coef""]])-&gt;b2 

ks&lt;-llply(names(b2),function(x){ 
    s&lt;-data.frame(b2[[x]]) 
    s$ai&lt;-x 
    return(s) 
    }) 


ks3&lt;-do.call(rbind,ks) 
ks3$ai2&lt;-with(ks3,substring(ai,1,1)) 

ks3$gc2&lt;-sapply(strsplit(as.character(ks3$ai), ""\\.""), ""[["", 2) 


k&lt;-ks3 
j&lt;-dlply(k,.(ai2,gc2),function(x){ 
    i1&lt;-quantile(x$Intercept,probs=c(0.025,0.975))[1] 
    i2&lt;-quantile(x$Intercept,probs=c(0.025,0.975))[2] 

    j1&lt;-quantile(x$bc,probs=c(0.025,0.975))[1] 
    j2&lt;-quantile(x$bc,probs=c(0.025,0.975))[2] 

    o&lt;-x$Intercept&gt;i1 &amp; x$Intercept&lt;i2 

    p&lt;-x$bc&gt;j1 &amp; x$bc&lt;j2 

    h&lt;-o &amp; p 
    return(h) 
    }) 

m&lt;-melt(j) 
ks3$int&lt;-m[,1]   

ggplot(ks3,aes(x=bc,y=Intercept,fill=int)) +
  geom_point(,alpha=0.3,size=1,shape=21) +
  facet_grid(gc2~ai2,scales = ""free_y"")+theme_bw()-&gt;plott 
plott&lt;-plott+opts(panel.grid.minor=theme_blank(),panel.grid.major=theme_blank()) 
plott&lt;-plott+geom_vline(x=0,color=""red"") 
plott+xlab(""BC coefficient"")+ylab(""Intercept"") 
</code></pre>
"
"NaN","NaN","  3531","<p>I would like to perform reversible jump on a network model, but before arriving there, I'm wondering if there are any R packages which support reversible jump for a user specified generalized linear model or spatial-GLM?</p>

<p>Something as simple as an RJMCMC procedure (in R) for the selection of predictors in a logistic regression would be a nice place for me to start?  Does such a function exist?</p>

<p>Through googling, I've only found <a href=""http://cran.r-project.org/web/packages/RJaCGH/index.html"" rel=""nofollow"">RJaCGH</a> which appears to be a bit more complicated (and application specific) than I was hoping for.</p>
"
"0.1271697545995","0.124646392502318","  6141","<p>I am now writing my bachelors thesis and I have come across some difficulties. I am about to do some panel regressions with time and entity fixed effects and I would therefore like to use the plm package. But when I do add fixed effects and want to have heteroscedasticity robust standard errors they seem to be incorrect.</p>

<p>Does anyone know why the HC standard errors differ?</p>

<p>Here is my code</p>

<pre><code># Load data
load(file=""panel"")
attach(panel)

# Load packages
library(lmtest)
library(plm)


# Create two models. The lm.model is a linear model and as the
# LAND variable is a factor variable representing countries
# (Land = Country in Swedish) this model will have entity fixed
# effects. In the plm.model the plm package is used and
# individual effects and within model is turned on (which is
# the same as entity fixed effects)
lm.model&lt;-lm(NETTOSPARANDE ~ EURO + LAND, data=panel)
plm.model&lt;-plm(NETTOSPARANDE ~ EURO, index=c(""LAND"",""Ã…R""), effect=""individual"", model=""within"", data=panel)

# When looking at the coefficents without heteroscadisity robust
# standard errors they are identical. They do also have the same
# value in stata.
coeftest(lm.model)[1:2,]
coeftest(plm.model)

# But when looking at the coefficents using heteroscadisity
# robust standard errors the lm.model and the plm.model produces
# different standard errors.
coeftest(lm.model, vcov.=vcovHC(lm.model, method=""white2"", type=""HC1""))[1:2,]
coeftest(plm.model, vcov.=vcovHC(plm.model, method=""white2"", type=""HC1""))
</code></pre>

<p>If you want to test the data it can be found here (the panel file)
  [1]: <a href=""https://sourceforge.net/projects/emumoralhazard/files/"" rel=""nofollow"">https://sourceforge.net/projects/emumoralhazard/files/</a> <em>R-data</em></p>

<p>And here is my output</p>

<pre><code>1&gt; # Load data
1&gt; load(file=""panel"")

1&gt; attach(panel)

1&gt; # Load packages
1&gt; library(lmtest)
Loading required package: zoo

1&gt; library(plm)
Loading required package: kinship
Loading required package: survival
Loading required package: splines
Loading required package: nlme
Loading required package: lattice
[1] ""kinship is loaded""
Loading required package: Formula
Loading required package: MASS
Loading required package: sandwich

1&gt; # Create two models. The lm.model is a linear model and as the
1&gt; # LAND variabel is a factor variable representing countries
1&gt; # (Land = Country in swedish) this model will have entity fixed
1&gt; # effects. In the plm.model the plm package is used and
1&gt; # individual effects and within model is turned on (which is
1&gt; # the same as entity fixed effects)
1&gt; lm.model&lt;-lm(NETTOSPARANDE ~ EURO + LAND, data=panel)

1&gt; plm.model&lt;-plm(NETTOSPARANDE ~ EURO, index=c(""LAND"",""Ã…R""), effect=""individual"", model=""within"", data=panel)

1&gt; # When looking at the coefficients without heteroscedasticity robust
1&gt; # standard errors they are identical. They do also have the same
1&gt; # value in Stata.
1&gt; coeftest(lm.model)[1:2,]
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) -3.731024  0.7731778 -4.825570 1.726921e-06
EURO1        2.187170  0.4076720  5.365024 1.112984e-07

1&gt; coeftest(plm.model)

t test of coefficients:

      Estimate Std. Error t value  Pr(&gt;|t|)    
EURO1  2.18717    0.40767   5.365 1.113e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 


1&gt; # But when looking at the coefficients using heteroscedasticity 
1&gt; # robust standard errors the lm.model and the plm.model produces
1&gt; # different standard errors.
1&gt; coeftest(lm.model, vcov.=vcovHC(lm.model, method=""white2"", type=""HC1""))[1:2,]
             Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) -3.731024  0.3551280 -10.506138 5.102122e-24
EURO1        2.187170  0.3386029   6.459395 2.009894e-10

1&gt; coeftest(plm.model, vcov.=vcovHC(plm.model, method=""white2"", type=""HC1""))

t test of coefficients:

      Estimate Std. Error t value  Pr(&gt;|t|)    
EURO1  2.18717    0.33849  6.4615 1.983e-10 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>
"
"NaN","NaN","  6214","<p>How should I define a model formula in R, when one (or more) exact linear restrictions binding the coefficients is available. As an example, say that you know that b1 = 2*b0 in a simple linear regression model. </p>

<p>Thank you!</p>
"
"0.0379929508508414","0.0496521024619361","  6268","<p>I'm searching how to (visually) explain simple linear correlation to first year students.</p>

<p>The classical way to visualize would be to give an Y~X scatter plot with a straight regression line.</p>

<p>Recently, I came by the idea of extending this type of graphics by adding to the plot 3 more images, leaving me with: the scatter plot of y~1, then of y~x, resid(y~x)~x and lastly of residuals(y~x)~1 (centered to the mean)</p>

<p>Here is an example of such a visualization:
<img src=""http://i.stack.imgur.com/Pe2ul.png"" alt=""alt text""></p>

<p>And the R code to produce it:</p>

<pre><code>set.seed(345)
x &lt;- runif(50) * 10
y &lt;- x +rnorm(50)


layout(matrix(c(1,2,2,2,2,3 ,3,3,3,4), 1,10))
plot(y~rep(1, length(y)), axes = F, xlab = """", ylim = range(y))
points(1,mean(y), col = 2, pch = 19, cex = 2)
plot(y~x, ylab = """", )
abline(lm(y~x), col = 2, lwd = 2)

plot(c(residuals(lm(y~x)) + mean(y))~x, ylab = """", ylim = range(y))
abline(h =mean(y), col = 2, lwd = 2)

plot(c(residuals(lm(y~x)) + mean(y))~rep(1, length(y)), axes = F, xlab = """", ylab = """", ylim = range(y))
points(1,mean(y), col = 2, pch = 19, cex = 2)
</code></pre>

<p>Which leads me to my question: I would appreciate any <strong>suggestions on how this graph can be enhanced</strong> (either with text, marks, or any other type of relevant visualizations). Adding relevant R code will also be nice.</p>

<p>One direction is to add some information of the R^2 (either by text, or by somehow adding lines presenting the magnitude of the variance before and after the introduction of x)
Another option is to highlight one point and showing how it is ""better explained"" thanks to the regression line.  Any input will be appreciated.</p>
"
"0.0200240432865818","0.039253433598943","  6734","<p>I have been reading the description of ridge regression in <em><a href=""http://rads.stackoverflow.com/amzn/click/007310874X"" rel=""nofollow"">Applied Linear Statistical Models</em>, 5th Ed</a> chapter 11. The ridge regression is done on body fat data available <a href=""http://www.cst.cmich.edu/users/lee1c/spss/V16_materials/DataSets_v16/BodyFat-TxtFormat.txt"" rel=""nofollow"">here</a>. </p>

<p>The textbook matches the output in SAS, where the back transformed coefficients are given in the fitted model as:<br>
$$
Y=-7.3978+0.5553X_1+0.3681X_2-0.1917X_3
$$</p>

<p>This is shown from SAS as:</p>

<pre><code>proc reg data = ch7tab1a outest = temp outstb noprint;
  model y = x1-x3 / ridge = 0.02;
run;
quit;
proc print data = temp;
  where _ridge_ = 0.02 and y = -1;
  var y intercept x1 x2 x3;
run;
Obs     Y    Intercept       X1         X2         X3

 2     -1     -7.40343    0.55535    0.36814    -0.19163
 3     -1      0.00000    0.54633    0.37740    -0.13687
</code></pre>

<p>But R gives very different coefficients:</p>

<pre><code>data &lt;- read.table(""http://www.cst.cmich.edu/users/lee1c/spss/V16_materials/DataSets_v16/BodyFat-TxtFormat.txt"", 
                   sep="" "", header=FALSE)
data &lt;- data[,c(1,3,5,7)]
colnames(data)&lt;-c(""x1"",""x2"",""x3"",""y"")
ridge&lt;-lm.ridge(y ~ ., data, lambda=0.02)   
ridge$coef
coef(ridge)

&gt;   ridge$coef
       x1        x2        x3 
10.126984 -4.682273 -3.527010 
&gt;   coef(ridge)
                   x1         x2         x3 
42.2181995  2.0683914 -0.9177207 -0.9921824 
&gt; 
</code></pre>

<p>Can anyone help me understand why?</p>
"
"0.0633215847514023","0.0620651280774201","  6927","<p>I'm doing a simulation study which requires bootstrapping estimates obtained from a generalized linear mixed model (actually, the product of two estimates for fixed effects, one from a GLMM and one from an LMM). To do the study well would require about 1000 simulations with 1000 or 1500 bootstrap replications each time. This takes a significant amount of time on my computer (many days). </p>

<p><code>How can I speed up the computation of these fixed effects?</code></p>

<p>To be more specific, I have subjects who are measured repeatedly in three ways, giving rise to variables X, M, and Y, where X and M are continuous and Y is binary. We have two regression equations 
$$M=\alpha_0+\alpha_1X+\epsilon_1$$
$$Y^*=\beta_0+\beta_1X+\beta_2M+\epsilon_2$$
where Y$^*$ is the underlying latent continuous variable for $Y$ and the errors are not iid.<br>
The statistic we want to bootstrap is $\alpha_1\beta_2$. Thus, each bootstrap replication requires fitting an LMM and a GLMM. My R code is (using lme4)</p>

<pre><code>    stat=function(dat){
        a=fixef(lmer(M~X+(X|person),data=dat))[""X""]
        b=fixef(glmer(Y~X+M+(X+M|person),data=dat,family=""binomial""))[""M""]
        return(a*b)
    }</code></pre>

<p>I realize that I get the same estimate for $\alpha_1$ if I just fit it as a linear model, so that saves some time, but the same trick doesn't work for $\beta_2$.</p>

<p>Do I just need to buy a faster computer? :)</p>
"
"0.0400480865731637","0.039253433598943","  7102","<p>I'm fitting a 4 parameter nonlinear regression model to multiple datasets, some of which fail to converge, however, the parameters output after a failure provide a fit that looks good, if not exceptional to my (and other's) eyes.</p>

<p>I've explored convergence criteria and they do converge eventually but the visual fit is terrible.</p>

<p>Is there any precedent for taking the visual fit ad ignoring convergence or are there some other things I can try?</p>

<p>I'm fitting a model of the form $\sqrt{c_1 x+c_2\exp(x)^y}/\sqrt{\exp(x)^y}+c_3$ where $y$ is a known constant, using nlminb in R.</p>
"
"0.02831827358943","0.0277563690826684","  7344","<p>I'm trying to write a function to graphically display predicted vs. actual relationships in a linear regression.  What I have so far works well for linear models, but I'd like to extend it in a few ways.</p>

<ol>
<li>Handle glm models</li>
<li>Deal with NAs in the predicted values</li>
</ol>

<p>Does what I have so far seem like a good solution, or is there an existing package somewhere that's already implemented this?</p>

<pre><code>DF &lt;- as.data.frame(na.exclude(airquality))
DF$Month &lt;- as.factor(DF$Month)
DF$Day &lt;- as.factor(DF$Day)

my_model &lt;- lm(Ozone~Solar.R+Wind+Temp+Month+Day,DF)

PvA&lt;- function(model,varlist=NULL,smooth=.5) { #Plot predicted vs actual for a model

    indvars &lt;- attr(terms(model),""term.labels"")

    if (is.null(varlist)) {
        varlist &lt;- indvars
    }

    Y &lt;- as.character(as.list(attr(terms(model),""variables""))[2])
    P.Y &lt;- paste('P',Y,sep='.')

    DF &lt;- as.data.frame(get(as.character(model$call$data)))
    DF[,P.Y] &lt;- predict.lm(model)

    par(ask=TRUE)
    for (X in varlist) {
        print(X)
        A &lt;- na.omit(DF[,c(X,Y)])
        P &lt;- na.omit(DF[,c(X,P.Y)])
        plot(A)
        points(P,col=2)
        lines(lowess(A,f=smooth),col=1)
        lines(lowess(P,f=smooth),col=2)
    }

}
PvA(my_model)
</code></pre>
"
"0.0400480865731637","0.039253433598943","  7639","<p>I have 4301 lines of data from my science project.  I have to do unorthodox statistical analysis for my results because they are 4-dimensional; I have three independent variables.</p>

<p>With Excel I can do a lot of analysis, even multivariable linear regression, but what I need is a step up:  Multivariable nonlinear regression.</p>

<p>I have R.  Is there a way I can do it with R?</p>

<p>Also, is there a way to generate multiple 3-dimensional graphs with slices of the data automatically?</p>

<p>EDIT:  I've searched with no luck.  Is there <em>any</em> way of doing multivariable nonlinear regression automatically?</p>
"
"0.0633215847514023","0.0620651280774201","  7775","<p>Does anyone have suggestions or packages that will calculate the coefficient of partial determination?</p>

<p>The coefficient of partial determination can be defined as the percent of variation that cannot be explained in a reduced model, but can be explained by the predictors specified in a full(er) model. This coefficient is used to provide insight into whether or not one or more additional predictors may be useful in a more fully specified regression model.</p>

<p>The calculation for the partial r^2 is relatively straight forward after estimating your two models and generating the ANOVA tables for them. The calculation for the partial r^2 is:</p>

<p>(SSEreduced - SSEfull) / SSEreduced</p>

<p>I've written this relatively simple function that will calculate this for a multiple linear regression model. I'm unfamiliar with other model structures in R where this function may not perform as well:</p>

<pre><code>partialR2 &lt;- function(model.full, model.reduced){
    anova.full &lt;- anova(model.full)
    anova.reduced &lt;- anova(model.reduced)

    sse.full &lt;- tail(anova.full$""Sum Sq"", 1)
    sse.reduced &lt;- tail(anova.reduced$""Sum Sq"", 1)

    pR2 &lt;- (sse.reduced - sse.full) / sse.reduced
    return(pR2)

    }
</code></pre>

<p>Any suggestions or tips on more robust functions to accomplish this task and/or more efficient implementations of the above code would be much appreciated.</p>
"
"NaN","NaN","  7919","<p>I am working on a linear regression with R and there are many 0 values in my predictor variables. How are these handled in R's <code>lm()</code> function? Should I remove this data for more accurate analysis? </p>

<p>Any advice is appreciated. Thanks. </p>
"
"0.09392108820677","0.0920574617898323","  7996","<p>I am evaluating a scenario's output parameter's dependence on three parameters: A, B and C. For this, I am conducting the following experiments:</p>

<ul>
<li>Fix A+B, Vary C - Total four sets of (A+B) each having 4 variations of C</li>
<li>Fix B+C, Vary A - Total four sets of (B+C) each having 3 variations of C</li>
<li>Fix C+A, Vary B - Total four sets of (C+A) each having 6 variations of C</li>
</ul>

<p>The output of any simulation is the value of a variable over time. For instance, A could be the area, B could be the velocity and C could be the number of vehicles. The output variable I am observing is the number of car crashes over time. </p>

<p>I am trying to determine which parameter(s) dominate the outcome of the experiment. By dominate, I mean that sometimes, the outcomes just does not change when one of the parameters change but when some other parameter is changed even by a small amount, a large change in the output is observed. I need to capture this effect and output some analysis from which I can understand the dependence of the output on the input parameters. A friend suggested Sensitivity Analysis but am not sure if there are simpler ways of doing it. Can someone please help me with a good (possibly easy because I don't have a Stats background) technique? It would be great if all this can be done in R.</p>

<p><strong>Update:</strong> 
I used linear regression to obtain the following:</p>

<pre><code>lm(formula = T ~ A + S + V)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35928 -0.06842 -0.00698  0.05591  0.42844 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.01606    0.16437  -0.098 0.923391    
A            0.80199    0.15792   5.078 0.000112 ***
S           -0.27440    0.13160  -2.085 0.053441 .  
V           -0.31898    0.14889  -2.142 0.047892 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1665 on 16 degrees of freedom
Multiple R-squared: 0.6563, Adjusted R-squared: 0.5919 
F-statistic: 10.18 on 3 and 16 DF,  p-value: 0.0005416 
</code></pre>

<p>Does this mean that the output depends mostly on A and less on V?</p>
"
"NaN","NaN","  8254","<p>Lets say I am regressing Y on X1 and X2, where X1 is a numeric variable and X2 is a factor with four levels (A:D). Is there any way to write the linear regression function <code>lm(Y ~ X1 + as.factor(X2))</code> so that I can choose a particular level of X2 -- say, B -- as the baseline? </p>
"
"0.126787000519633","0.130188910980824","  8340","<p><strong>Update: I wanted to clarify that this is a simulation. Sorry if I confused everyone. I have also used meaningful names for my variables.</strong></p>

<p>I am not a statistician so please correct me if I make a blunder in explaining what I want. In regard to my <a href=""http://stats.stackexchange.com/questions/7996/what-is-a-good-way-of-estimating-the-dependence-of-an-output-variable-on-the-inpu"">previous question</a>, I have reproduced parts of my question here for reference.</p>

<blockquote>
  <p>I am evaluating a scenario's output
  dependence on three
  variables: Area, Speed and NumOfVehicles. For this, I am
  conducting the following experiments:</p>
  
  <ul>
  <li>Fix Area+Speed, Vary NumOfVehicles - Total four sets of (Area+Speed) each having 4 variations of NumOfVehicles</li>
  <li>Fix Speed+NumOfVehicles, Vary Area - Total four sets of (Speed+NumOfVehicles) each having 3 variations of Area</li>
  <li>Fix NumOfVehicles+Area, Vary Speed - Total four sets of (NumOfVehicles+Area) each having 6 variations of Speed</li>
  </ul>
  
  <p>The output of any simulation is the
  value of a variable over time. The output
  variable I am observing is the time at which 80% of the cars crashe.</p>
  
  <p>I am trying to determine which
  parameter(s) dominate the outcome of
  the experiment. By dominate, I mean
  that sometimes, the outcomes just does
  not change when one of the parameters
  change but when some other parameter
  is changed even by a small amount, a
  large change in the output is
  observed. I need to capture this
  effect and output some analysis from
  which I can understand the dependence
  of the output on the input parameters.
  A friend suggested Sensitivity
  Analysis but am not sure if there are
  simpler ways of doing it. Can someone
  please help me with a good (possibly
  easy because I don't have a Stats
  background) technique? It would be
  great if all this can be done in R.</p>
</blockquote>

<p>My previous result was not very satisfactory looking at the regression results. So what I did was that I went ahead and repeated all my experiments 20 times each with different variations of each variable (so for instance, instead of 4 variations of Area, I now have 8 and so on). Following is the summary I obtained out of R after using linear regression:</p>

<pre><code>Call:
lm(formula = T ~ Area + Speed + NumOfVehicles)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.13315 -0.06332 -0.01346  0.04484  0.29676 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      0.04285    0.02953   1.451    0.148    
Area             0.70285    0.02390  29.406  &lt; 2e-16 ***
Speed           -0.15560    0.02080  -7.479 2.12e-12 ***
NumOfVehicles   -0.27447    0.02927  -9.376  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.08659 on 206 degrees of freedom
Multiple R-squared: 0.8304, Adjusted R-squared: 0.8279 
F-statistic: 336.2 on 3 and 206 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>as opposed to my previous result:</p>

<pre><code>lm(formula = T ~ Area + Speed + NumOfVehicles)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35928 -0.06842 -0.00698  0.05591  0.42844 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   -0.01606    0.16437  -0.098 0.923391    
Area           0.80199    0.15792   5.078 0.000112 ***
Speed         -0.27440    0.13160  -2.085 0.053441 .  
NumOfVehicles -0.31898    0.14889  -2.142 0.047892 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1665 on 16 degrees of freedom
Multiple R-squared: 0.6563, Adjusted R-squared: 0.5919 
F-statistic: 10.18 on 3 and 16 DF,  p-value: 0.0005416 
</code></pre>

<p>From my understanding, my current results have a lower standard error so that is good. In addition the Pr value also seems quite low which tells me that this result is better than my previous result. So can I go ahead and say that A has the maximum effect on the output and then come S and V in that order? Can I make any other deductions from this result?</p>

<p>Also, I was suggested that I look into adding additional variates like $A^2$ etc. but if $A$ is the area, what does saying ""time"" depends on $A^2$ actually mean? </p>
"
"0.0693653206906364","0.0679889413649005","  8351","<p>I have a quarterly time series and test for stationarity with an augmented Dickey-Fuller test using R.</p>

<pre><code>adf.test(myseries)
# returns
# Dickey-Fuller = -3.9828, Lag order = 4, p-value = 0.01272
# alternative hypothesis: stationary 
</code></pre>

<p>so the H0 is rejected. I tried to validate this intuitively and regressed the same series on a linear trend.</p>

<pre><code>x&lt;- 104:1
fit.1&lt;-lm(myseries~x)
summary(fit.1)
#returns
# x      0.024  1.31e-05 ***
</code></pre>

<p>Even though a simple linear model is not so appropriate here and the intercept is large (around 80), there seems to be a slight downwards trend over time, which is in line with my thoughts after looking at the initial data. So do I get the adf.test wrong or is the trend just to small to be discovered? </p>

<p>Besides I used</p>

<pre><code>plot(stl(myseries,""per""))
</code></pre>

<p>and ended up with a graph which sidebars suggested that trend and remainder were the main components driving the data, while seasonal influence was negligible. I saw that <code>stl()</code> uses Local Polynomial Regression Filtering and got a rough idea how that works (still I wonder why smoothed trends of Hadley's ggplot2 package looked that different even though it uses the same method by default).</p>

<p>So summing up I got:
- adf finding no evidence for a trend
- a slight downwards trends ""detected"" by eyeballing and the naive approach
- loess decomposition stating that the trend has strong influence (by the relation of its bars in the plot)</p>

<p>So what can I learned from this? Probably I do have a terminology problem here, because the former two seem to address time trends while the latter address some other trend I cannot fully grasp yet. Maybe my question is just: Can you help me to understand the trend extracted by loess? And how is it related to smoothed / filtered stuff like HP-Filter or Kalman Smoothing (if there is a relationship and similarity does not only occur in my case)?</p>
"
"NaN","NaN","  8689","<p>In R, if I write</p>

<pre><code>lm(a ~ b + c + b*c) 
</code></pre>

<p>would this still be a linear regression? </p>

<p>How to do other kinds of regression in R? I would appreciate any recommendation for textbooks or tutorials?</p>
"
"0.0693653206906364","0.0679889413649005","  8702","<p>I am examining the difference between a physical feature of different species of animals. Due to the nature of my experiments I'm using a nonlinear mixed model with the following setup:</p>

<pre><code>lme(log10(feature) ~ log10(Body.mass) + factor(Trial.Number), random = ~1 | IndividualID, data=animals, subset=Frfactor==""low"", na.action=na.omit )
</code></pre>

<p>where <code>subset=Frfactor==""low""</code> refers to a specific speed range that I'm interested in.</p>

<p>I get great results which I'm happy about. But now I want to see how species affects my feature. Since the same conditions apply (tons of repeated effects) I've kept the lme and changed the structure to: </p>

<pre><code>lme(log10(feature) ~ specfactor + factor(Trial.Number), random = ~1 | IndividualID, data=animals, subset=Frfactor==""low"", na.action=na.omit )
</code></pre>

<p>where specfactor lists the names of the species. Looking at the p values it looks like these species are not significantly different from the intercept (which is specfactorserval). However when I create a boxplot, it certainly looks like there are some big interspecies differences!</p>

<p>I guess because the lme is a test for regressions, it doesn't really make sense to use when comparing the feature against categorical variables. But I still need to account for repeated effects. My question is if there's a better way to test for significance between species using the boxplot? I need the usual- p-values, confidence intervals. The ""list"" command seems to fall short of such comparisons. I don't know if a t-test would cut it.</p>

<p>Thanks!</p>

<p>PS I originally posted an image of my test results and of the boxplot, but I'm too new of a user to be allowed....</p>
"
"0.09392108820677","0.0920574617898323","  9506","<p>I am new to R and to time series analysis. I am trying to find the trend of a long (40 years) daily temperature time series and tried to different approximations. First one is just a simple linear regression and second one is Seasonal Decomposition of Time Series by Loess.</p>

<p>In the latter it appears that the seasonal component is greater than the trend. But, how do I quantify the trend? I would like just a number telling how strong is that trend.</p>

<pre><code>     Call:  stl(x = tsdata, s.window = ""periodic"")
     Time.series components:
        seasonal                trend            remainder               
Min.   :-8.482470191   Min.   :20.76670   Min.   :-11.863290365      
1st Qu.:-5.799037090   1st Qu.:22.17939   1st Qu.: -1.661246674 
Median :-0.756729578   Median :22.56694   Median :  0.026579468      
Mean   :-0.005442784   Mean   :22.53063   Mean   : -0.003716813 
3rd Qu.:5.695720249    3rd Qu.:22.91756   3rd Qu.:  1.700826647    
Max.   :9.919315613    Max.   :24.98834   Max.   : 12.305103891   

 IQR:
         STL.seasonal STL.trend STL.remainder data   
         11.4948       0.7382    3.3621       10.8051
       % 106.4          6.8      31.1         100.0  
     Weights: all == 1
     Other components: List of 5   
$ win  : Named num [1:3] 153411 549 365  
    $ deg  : Named int [1:3] 0 1 1   
$ jump : Named num [1:3] 15342 55 37  
    $ inner: int 2  
$ outer: int 0
</code></pre>

<p><img src=""http://i.stack.imgur.com/jwCSr.png"" alt=""enter image description here""></p>
"
"NaN","NaN","  9759","<p>I'm new here, so I hope this hasn't been covered already, but my first few searches didn't find anything.</p>

<p>I am about to dive into learning R and my learning project will entail applying mixed- or random-effects regression to a dataset in order to develop a predictive equation.  I share the concern of the writer in this post
<a href=""http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models"">How to choose nlme or lme4 R library for mixed effects models?</a> in wondering whether NLME or LME4 is the better package to familiarize myself with.  A more basic (hopefully not dumb) question is:  what's the difference between linear and nonlinear mixed-effects modeling?</p>

<p>For background, I applied M-E modeling in my MS research (in MATLAB, not R), so I'm familiar with how fixed vs. random variables are treated.  But I'm uncertain whether the work I did was considered linear or nonlinear M-E.  Is it simply the functional form of the equation used, or something else?</p>
"
"0.0490486886395286","0.0480754414848157"," 10697","<p>I'm studying R package dlm. So far it seems very powerful and flexible package, with nice programming interfaces and good documentation.</p>

<p>I've been able to successfully use dlmMLE and dlmModARMA to estimate the parameters of AR(1) process:</p>

<pre><code>u &lt;- arima.sim(list(ar = 0.3), 100)
fit &lt;- dlmMLE(u, parm = c(0.5, sd(u)),
              build = function(x)
                dlmModARMA(ar = x[1], sigma2 = x[2]^2))
fit$par
</code></pre>

<p>Now I'm trying to use similar code to estimate the parameters of simple linear regression model:</p>

<pre><code>r &lt;- rnorm(100)
u &lt;- -1*r + 0.5*rnorm(100)
fit &lt;- dlmMLE(u, parm = c(0, 1),
              build = function(x)
                dlmModReg(x[1]*r, FALSE, dV = x[2]^2))
fit$par
</code></pre>

<p>I expect fit$par to be close to c(-1, 0.5), but I keep getting something like</p>

<pre><code>[1] -0.0002118851  0.4884367070
</code></pre>

<p>The coefficient -1 is not estimated correctly. However, the strange thing is that the variance of the noise is returned correctly.</p>

<p>I understand that max-likelihood estimation might fail given bad initial values, but I observed that the likelihood function returned by dlmLL is very flat in the first coordinate.</p>

<p>So I wonder: can such model be estimated at all using dlm? I believe the model is ""non-singular"", however I'm not sure how the likelihood function is calculated inside the dlm.</p>

<p>Any hint greatly appreciated.</p>
"
"0.0506572678011219","0.0620651280774201"," 11236","<p>I've been using R's <code>lm</code> to do some linear regression, but decided to give <code>MCMCregress</code> a try to get a feel for how it works. As expected, I got basically the same coefficients, but the extra <code>sigma2</code> value puzzles me.</p>

<p>When I do a <code>qqmath</code> plot of the coefficients, I get the following graph, and I'm puzzled by the sigma2 plot. It's obviously not linear, but I'm not sure if that's meaningful in this context. I assume it's sigma <em>squared</em>, and when I took the square root and plotted it, the line was straighter, but still curved.</p>

<p>I guess my question boils down to: what is sigma2 telling me about the MCMC regression fit, and is a graph of it useful or should I ignore the graph and focus on something else? (All of the diagnostics and graphs I've done on my original <code>lm</code> fit seem to indicate that the fit is good, so I'm also wondering if the MCMC regression gives me more information or not.)</p>

<p><img src=""http://i.stack.imgur.com/wayi4.png"" alt=""qqmath plot of MCMCregress results""></p>

<p>(If I need to provide the actual data, I can. I'm hoping that an answer depends more on what sigma2 is rather than on specific values.)</p>
"
"0.0578044339088637","0.0679889413649005"," 11457","<p>is it possible to do stepwise (direction = both) model selection in nested binary logistic regression in R? I would also appreciate if you can teach me  how to get:</p>

<ul>
<li>Hosmer-Lemeshow statitistic,</li>
<li>Odds ratio of the predictors, </li>
<li>Prediction success of the model.</li>
</ul>

<p>I used lme4 package of R. This is the script I used to get the general model with all the independent variables:</p>

<pre><code>nest.reg &lt;- glmer(decision ~ age + education + children + (1|town), family = binomial, data = fish)
</code></pre>

<p>where:</p>

<ul>
<li>fish -- dataframe</li>
<li>decision -- 1 or 0, whether the respondent exit or stay, respectively.</li>
<li>age, education and children -- independent variables.</li>
<li>town -- random effect (where our respondents are nested)</li>
</ul>

<p>Now my problem is how to get the best model. I know how to do stepwise model selection but only for linear regression. (<code>step( lm(decision ~ age + education + children, data = fish), direction +""both"")</code>). But this could not be used for binary logistic regression right? also when i add <code>(1|town)</code> to the formula to account for the effects of town, I get an error result. </p>

<p>By the way... I'm very much thankful to Manoel Galdino <a href=""http://stackoverflow.com/questions/5906272/step-by-step-procedure-on-how-to-run-nested-logistic-regression-in-r"">who provided me with the script on how to run nested logistic regression</a>. </p>

<p>Thank you very much for your help.</p>
"
"0.113469578623964","0.111218061863672"," 11498","<p>I'm fitting a linear model where the response is a function both of time and of static covariates (i.e. ones that are independent of time). The ultimate goal is to identify significant effects of the static covariates.</p>

<p>Is this the best general strategy for selecting variables (in R, using the <code>nlme</code> package)? Anything I can do better?</p>

<ol>
<li>Break the data up by groups and plot it against time. For continuous covariates, bin it and plot the data in each bin against time. Use the group-specific trends to make an initial guess at what time terms to include-- time, time^n, sin(2*pi*time)+cos(2*pi*time), log(time), exp(time), etc.</li>
<li>Add one term at a time, comparing each model to its predecessor, never adding a higher order in the absence of lower order terms. Sin and cos are never added separately. <strong><em>Is it acceptable to pass over a term that significantly improves the fit of the model if there is no physical interpretation of that term?</em></strong>.</li>
<li>With the full dataset, use forward selection to add static variables to the model and then relevant interaction terms with each other and with the time terms. <strong><em>I've seen some strong criticism of stepwise regression, but doesn't forward selection ignore significant higher order terms if the lower order terms they depend on are not significant? And I've noticed that it's hard to pick a starting model for backward elimination that isn't saturated, or singular, or fails to converge. How do you decide between variable selection algorithms?</em></strong></li>
<li>Add random effects to the model. <strong><em>Is this as simple as doing the variable selection using <code>lm()</code> and then putting the final formula into <code>lme()</code> and specifying the random effects? Or should I include random effects from the very start?</em></strong>. Compare the fits of models using a random intercept only, a random interaction with the linear time term, and random interaction with each successive time term. </li>
<li>Plot a semivariogram to see if an autoregressive error term is needed. <strong><em>What should a semivariogram look like if the answer is 'no'? A horizontal line? How straight, how horizontal? Does including autoregression in the model again require checking potential variables and interactions to make sure they're still relevant?</em></strong></li>
<li>Plot the residuals to see if the variance changes as a function of fitted value, time, or any of the other terms. If it does, weigh the variances appropriately (for <code>lme()</code>, use the <code>weights</code> argument to specify a <code>varFunc()</code>) and compare to the unweighted model to see if this improves the fit. <strong><em>Is this the right sequence in which to do this step, or should it be done before autocorrelation?</em></strong>.</li>
<li>Do <code>summary()</code> of the fitted model to identify significant coefficients for numeric covariates. Do <code>Anova()</code> of the fitted model to identify significant effects for qualitative covariates.</li>
</ol>
"
"0.0642198081225601","0.0734364498908627"," 11679","<p>I have a nested-case control study that I have been using for analysis. At the end of my work I have deduced a set of variables that I use later to to classify new cases. One example of a simple classifier I am using is a naive Bayes, which will output simply a probability. </p>

<p>So here is my question:</p>

<p>Could I make my probabilities reflect the real world? In my specific example, the condition that I am testing for has a prevalence of 33% in my study, but a it has a population prevalence of only 10%.  Bayes factors have been suggested to me as a way to achieve this, however I am little unsure how to set up the problem. </p>

<p>As an example I have seen a Bayes factor as a logit between the true vs. study prevalence of the outcome. The classifier however was a logistic regression, and in that case the Bayes factor was just added to the linear predictors. I think the example there was very specific, and perhaps an inappropriate method for probabilities of a naive Bayes. Instead what I did was add the logit Bayes factor to the logged probabilities, but I am also not convinced this is right either. I also think a simpler solution would be to use Bayes theorem directly, but there I am not sure how to represented my study vs.population prevalences. The method below isn't quite right, but gets at what I want:</p>

<pre><code>        p_final = classier_posterior*(population_prev)/(study_prev)
</code></pre>

<p>I should contextualize that I use the probabilities to establish a threshold for classification down stream.</p>
"
"NaN","NaN"," 11703","<p>Assume the following easy example of a glm regression with an offset:</p>

<pre><code>numberofdrugs&lt;-rpois(84, 10)
healthvalue&lt;-rpois(84,75)
age&lt;-rnorm(84,50,5)
test&lt;-glm(healthvalue~age, family=poisson, offset=log(numberofdrugs))
summary(test)
fitted(test) #how to get one of these values manually?
</code></pre>

<ul>
<li>How can I compute the fitted values manually? </li>
<li>Also, why is there no estimation of log(numberofdrugs)? 
<ul>
<li>In the book <a href=""http://rads.stackoverflow.com/amzn/click/0412317605"" rel=""nofollow""><em>Generalized Linear Models</em></a> on page 205-207 there is an example where the offset is estimated. It was done to see if the coefficient is close to one. It's 0.903 (see page 207 if you've this classic book) and from this follows, that there is nearly a constant rate in the number of damage incident!</li>
</ul></li>
</ul>

<p>Previous related questions asked: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/11182/when-to-use-an-offset-in-a-poisson-regression"">When to use an offset?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/11595/whether-to-use-an-offset-in-a-poisson-regression-when-predicting-total-career-goa"">Whether to use an offset when predicting hockey scores?</a></li>
</ul>
"
"0.0693653206906364","0.0679889413649005"," 11959","<p>In R the <code>princomp()</code>and the <code>factanal()</code> are somewhat similar. At least their output looks pretty similar. I learned that this is not surprising since the print function of <code>princomp</code> comes from <code>factanal</code>. I understand that SS loadings do not make much sense for <code>princomp</code> as it is bounded to <code>1</code> anyway. Moreover, as Joris stated on nabble, the proportion of variance is only printed because of the common print function, but does not contain valuable information when princomp is used. </p>

<p>What I do not understand is rather not an R question but more a multivariate stats question what is the conceptual difference between these PCA and Factor Analysis functions as they are used in R? This question relates particularly to the scores (let's assume ""regression"" scores for FA) respectively the difference between scores in both concepts? 
What should I rather use when I want to use to resulting scores in a regression model (for example in order to circumvent multicollinearity)? I also understand that PCA has a fixed number of components while FA has fewer factors than variables. </p>

<p>richiemorrisroe's answer in the thread suggested by Rob Hyndman might go into that direction.</p>
"
"0","0.0277563690826684"," 12469","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/7527/change-point-analysis-using-rs-nls"">Change point analysis using R&#39;s nls()</a>  </p>
</blockquote>



<p>I want to do a nonlinear regression with nls() but also include a specific type of segmented or piecewise regression. The Formula I want to implement is:</p>

<pre><code>S ~ b0 + (A &gt; T) * b1 * (A - T)
</code></pre>

<p><code>T</code> should be the threshold value or breakpoint as identified by the nonlinear-segmented regression. I know that I can use <code>""algorithm = plinear""</code> but that does not work at all.</p>

<hr>

<p>The data I have is:</p>

<pre><code>A   S
0.000809371 1
0.003642171 3
0.009712455 4
0.010521827 2
0.004046856 4
0.015378054 5
0.000404686 0
0.000404686 0
0.000404686 0
0.000809371 0
0.000809371 3
0.037635765 3
0.008903084 2
0.016187426 5
0.043301364 1
0.000404686 1
0.002428114 1
0.003642171 1
0.013759312 4
0.051395077 9
0.394568501 9
0.005665599 1
0.013354626 1
0.028732681 3
0.026304567 2
0.004451542 1
0.050585705 2
0.00647497  1
0.010926512 0
0.013354626 1
1.695632841 4
0.013354626 2
</code></pre>
"
"NaN","NaN"," 12554","<p>What parameterization to <code>glmnet</code> will give the same results as <code>glm</code>?  (I'm mainly interested in logistic and linear regressions, if that matters.)</p>
"
"0.0749231094763201","0.0734364498908627"," 13053","<p>I'm trying to fit a line+exponential curve to some data. As a start, I tried to do this on some artificial data. The function is:
$$y=a+b\cdot r^{(x-m)}+c\cdot x$$
It is effectively an exponential curve with a linear section, as well as an additional horizontal shift parameter (<em>m</em>). However, when I use R's <code>nls()</code> function I get the dreaded ""<em>singular gradient matrix at initial parameter estimates</em>"" error, even if I use the same parameters that I used to generate the data in the first place.<br>
I've tried the different algorithms, different starting values and tried to use <code>optim</code> to minimise the residual sum of squares, all to no avail. I've read that a possible reason for this could be an over-parametrisation of the formula, but I don't think it is (is it?)<br>
Does anyone have a suggestion for this problem? Or is this just an awkward model?</p>

<p>A short example:</p>

<pre><code>#parameters used to generate the data
reala=-3
realb=5
realc=0.5
realr=0.7
realm=1
x=1:11 #x values - I have 11 timepoint data
#linear+exponential function
y=reala + realb*realr^(x-realm) + realc*x
#add a bit of noise to avoid zero-residual data
jitter_y = jitter(y,amount=0.2)
testdat=data.frame(x,jitter_y)

#try the regression with similar starting values to the the real parameters
linexp=nls(jitter_y~a+b*r^(x-m)+c*x, data=testdat, start=list(a=-3, b=5, c=0.5, r=0.7, m=1), trace=T)
</code></pre>

<p>Thanks!</p>
"
"0.0400480865731637","0.039253433598943"," 13069","<p>I am very interested about the potential of statistical analysis for simulation/forecasting/function estimation, etc. </p>

<p>However, I don't know much about it and my mathematical knowledge is still quite limited -- I am a junior undergraduate student in software engineering. </p>

<p>I am looking for a book that would get me started on certain things which I keep reading about: linear regression and other kinds of regression, bayesian methods, monte carlo methods, machine learning, etc.
I also want to get started with R so if there was a book that combined both, that would be awesome. </p>

<p>Preferably, I would like the book to explain things conceptually and not in too much technical details -- I would like statistics to be very intuitive to me, because I understand there are very many risky pitfalls in statistics. </p>

<p>I am off course willing to read more books to improve my understanding of topics which I deem valuable.</p>
"
"0.0633215847514023","0.0620651280774201"," 13091","<p>I have this model:</p>

<pre><code>model &lt;- zelig(dv~(product*intervention), model = ""negbin"", data = data)
</code></pre>

<p>intervention has <strong>two levels</strong>: neutral(=0), treatment(=1)<br />
product has <strong>two levels</strong>: product1(=0), product2(=1)</p>

<p>I build f_all to just have one factor with 4 groups for comparison analysis.</p>

<p>Thus I have <strong>4 groups</strong> in f_all<br />
1. product1-neutral<br />
2. product1-treatment<br />
3. product2-neutral<br />
4. product2-treament<br /></p>

<p><strong>My interaction hypothesis is that treatment only works for product2.</strong></p>

<p>Zelig gives me my predicted significant interaction. <br /></p>

<p>Yet, I need planned contrasts to test my specific hypothesis: c(-1,1,0,0) and c(0,0,1,-1)</p>

<p>I researched and found a description of doing this with multcomp on this page: <a href=""http://stats.stackexchange.com/questions/12993/how-to-setup-and-interpret-anova-contrasts-with-the-car-package-in-r"">post comparisons</a></p>

<p>The regression output shows my predicted interaction</p>

<pre><code>(Intercept)  1.34223    0.08024  16.728   &lt;2e-16 ***
product      0.08747    0.08025   1.090   0.2757
intervention 0.07437    0.07731   0.962   0.3361
interaction  0.45645    0.22263   2.050   0.0403 * 
</code></pre>

<p>However, it said multcomp and the glht function is for linear models, but I am using a negbin model.</p>

<p><strong>3 Questions regarding this problem:</strong><br />
1. Can I do planned comparisons on my negbin model using multcomp?<br />
2. If not what appropriate method is there to do this for my negbin model?<br />
3. Based on R using treatment contrasts per default could I just interpret the interaction coefficient as the contrast comparing product2-neutral versus product2-treatment? Can I then interpret the intervention coefficient as contrast comparing product1-neutral versus product1-treament?</p>
"
"0.0490486886395286","0.0480754414848157"," 13152","<p>I always use <code>lm()</code> in R to perform linear regression of $y$ on $x$. That function returns a coefficient $\beta$ such that $$y = \beta x.$$</p>

<p>Today I learned about <strong>total least squares</strong> and that <code>princomp()</code> function (principal component analysis, PCA) can be used to perform it. It should be good for me (more accurate). I have done some tests using <code>princomp()</code>, like:</p>

<pre><code>r &lt;- princomp(Â ~Â xÂ +Â y)
</code></pre>

<p>My problem is: how to interpret its results? How can I get the regression coefficient? By ""coefficient"" I mean the number $\beta$ that I have to use to multiply the $x$ value to give a number close to $y$.</p>
"
"0.02831827358943","0.0277563690826684"," 13262","<p>The GRS statistic is the Gibbons et al. (1989) statistic that tests whether the estimated intercepts from a multiple regression model are jointly zero. </p>

<p>The typical scenario involves a multivariate linear panel regression where you are explaining the returns to securities in terms of its exposures to factor return series. Theoretically, a good factor model will have an intercept statistically indistinguishable from zero.</p>

<p>How do I calculate the GRS statistic in R?</p>

<p>Thank you</p>
"
"0.09392108820677","0.0920574617898323"," 13353","<p>I will give my examples with R calls. First a simple example of a linear regression with a dependent variable 'lifespan', and two continuous explanatory variables.</p>

<pre><code>data.frame(height=runif(4000,160,200))-&gt;human.life
human.life$weight=runif(4000,50,120)
human.life$lifespan=sample(45:90,4000,replace=TRUE)
summary(lm(lifespan~1+height+weight,data=human.life))

Call:
lm(formula = lifespan ~ 1 + height + weight, data = human.life)

Residuals:
Min       1Q   Median       3Q      Max 
-23.0257 -11.9124  -0.0565  11.3755  23.8591 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 63.635709   3.486426  18.252   &lt;2e-16 ***
height       0.007485   0.018665   0.401   0.6884    
weight       0.024544   0.010428   2.354   0.0186 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 13.41 on 3997 degrees of freedom
Multiple R-squared: 0.001425,   Adjusted R-squared: 0.0009257 
F-statistic: 2.853 on 2 and 3997 DF,  p-value: 0.05781
</code></pre>

<p>In order to find the estimate of 'lifespan' when the value of 'weight' is 1, I add (Intercept)+height=63.64319</p>

<p>Now what if I have a similar data frame, but one where one of the explanatory variables is categorical?</p>

<pre><code>data.frame(animal=rep(c(""dog"",""fox"",""pig"",""wolf""),1000))-&gt;animal.life
animal.life$weight=runif(4000,8,50)
animal.life$lifespan=sample(1:10,replace=TRUE)
summary(lm(lifespan~1+animal+weight,data=animal.life))

Call:
lm(formula = lifespan ~ 1 + animal + weight, data = animal.life)

Residuals:
Min      1Q  Median      3Q     Max 
-4.7677 -2.7796 -0.1025  3.1972  4.3691 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 5.565556   0.145851  38.159  &lt; 2e-16 ***
animalfox   0.806634   0.131198   6.148  8.6e-10 ***
animalpig   0.010635   0.131259   0.081   0.9354    
animalwolf  0.806650   0.131198   6.148  8.6e-10 ***
weight      0.007946   0.003815   2.083   0.0373 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 2.933 on 3995 degrees of freedom
Multiple R-squared: 0.01933,    Adjusted R-squared: 0.01835 
F-statistic: 19.69 on 4 and 3995 DF,  p-value: 4.625e-16
</code></pre>

<p>In this case, to find the estimate of 'lifespan' when the value of 'weight' is 1, should I add each of the coefficients for 'animal' to the intercept: (Intercept)+animalfox+animalpig+animalwolf? Or what is the proper way to do this?</p>

<p>Thanks
Sverre</p>
"
"0.0642198081225601","0.0734364498908627"," 13478","<p>Using R, I have developed three models:  </p>

<ul>
<li>linear regression using <code>lm()</code>;</li>
<li>decision tree using <code>rpart()</code>;</li>
<li>k-nearest neighbor using <code>kknn()</code>. </li>
</ul>

<p>I would like to conduct leave-one-out cross-validation tests and compare these models. However, which error metric should I use for better representation? Does mean absolute percentage error (MAPE) or sMAPE (symmetric MAPE) look fine? Please suggest me a metric. </p>

<p>For example, when I conducted leave-one-out CV tests on linear regression (LR) and decision tree (DT) models, the sMAPE error values are 0.16 and 0.20. However, the R-squared values of LR and DT are 0.85 and 0.92 respectively. Where sMAPE computed as <code>[sum (abs(predicted - actual)/((predicted + actual)/2))] / (number of data points)</code>. Here DT is pruned regression tree. These R^2 values are computed on full data set. There are a total of 60 data points in the set.</p>

<pre><code>Model  R^2   sMAPE
 LR    0.85   0.16
 DT    0.92   0.20
</code></pre>
"
"0.0400480865731637","0.039253433598943"," 13485","<p>Using <code>R plot()</code> and <code>plotcp()</code> methods, we can visualize linear regression model (<code>lm</code>) as an equation and decision tree model (<code>rpart</code>) as a tree. We can develop k-nearest neighbour model using R <code>kknn()</code> method, but I don't know how to present this model. Please suggest me some R methods that produce nice graphs for knn model visualization.</p>
"
"0.0400480865731637","0.039253433598943"," 13550","<p>I have a simple matrix:</p>

<pre><code>     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    7    8    9
[4,]   10   11   12
</code></pre>

<p>I have to calculate linear regression and orthogonal regression using lm() and prcomp() respectively. (for orthogonal see: <a href=""http://my.safaribooksonline.com/book/programming/r/9780596809287/beyond-basic-numerics-and-statistics/recipe-id264"" rel=""nofollow"">here</a>)</p>

<p>Assume that the first column is the the X and M the matrix I wrote before.</p>

<p><strong>LINEAR REG.</strong></p>

<pre><code>mod1 &lt;- lm(M[,1] ~ M[,2] + M[,3] + 0)
</code></pre>

<p>Its output is (coefficient):</p>

<pre><code>Coefficients: M[, 2]  M[, 3]  
     2      -1
</code></pre>

<p>Ok, I have these coefficients.</p>

<p>Now for </p>

<p><strong>ORTHOGONAL REG.</strong></p>

<pre><code>mod2 &lt;- prcomp(~ M[,1] + M[,2] + M[,3])
</code></pre>

<p>Its output is:</p>

<pre><code>             PC1        PC2        PC3
M[, 1] 0.5773503  0.0000000  0.8164966
M[, 2] 0.5773503 -0.7071068 -0.4082483
M[, 3] 0.5773503  0.7071068 -0.4082483
</code></pre>

<p>The question is: out to interpret prcomp() result instead of lm() result ?
Using lm() the coefficients are using to predict the X values.</p>

<p>What about prcomp() ?</p>

<p>Thank you!</p>
"
"0","0.0277563690826684"," 13615","<p>At the moment I'm using linear regression of 4 series with:</p>

<pre><code>mod &lt;- lm(x ~ y + z + v + 0) # I need zero intercept
</code></pre>

<p>I'm using the linear regression to calculate the coefficients of <code>y</code>, <code>z</code> and <code>v</code> to predict the <code>x</code> value.</p>

<p>Is there something more accurate then lm? 
For example, I heard about orthogonal regression; could it be good?</p>
"
"0.0506572678011219","0.0620651280774201"," 13617","<p>I've been implementing the GLMNET version of elastic net for linear regression with another software than R. I compared my results with the R function glmnet in lasso mode on <a href=""http://www.stanford.edu/~hastie/Papers/LARS/diabetes.data"">diabetes data</a>.</p>

<p>The variable selection is ok when varying the value of the parameter (lambda) but I obtain slightly different values of coefficients. For this and other reasons I think it comes from the intercept in the update loop, when I compute the current fit, because I don't vary the intercept (which I take as the mean of the target variable) in the whole algorithm : as explained in Trevor Hastie's article ( <a href=""http://www.jstatsoft.org/v33/i01/paper"">Regularization Paths for Generalized Linear Models via Coordinate Descent</a>, Page 7, section 2.6):</p>

<blockquote>
  <p>the intercept is not regularized, [...] for all values of [...] lambda [the L1-constraint parameter]</p>
</blockquote>

<p>But despite the article, the R function glmnet does provide different values for the intercept along the regularization path (the lambda different values). Does anyone has a clue about how the values of the Intercept are computed?</p>
"
"0.0578044339088637","0.0566574511374171"," 13696","<p>I am trying to model a linear regression to predict the number of interested students in a class based on a couple of properties of that class- </p>

<ol>
<li>whether that class has an undergraduate curriculum (assigned 0 or 1)</li>
<li>whether that class has mandatory homework (assigned 0 or 1)</li>
<li>whether that class is held for multiple weeks (assigned 0 or 1)</li>
<li>the difficulty of that class (assigned a number from 1.0 to 4.0)</li>
</ol>

<p>I have a little over 80 data points, but the R^2 values I get vary upon whether I take the products of certain variables.</p>

<pre><code>summary(lm(AA.Int~undergrad + multi + HW + Spiciness, data=realdata))
</code></pre>

<p>returns an R^2 value of 0.06419</p>

<p>whereas </p>

<pre><code>summary(lm(AA.Int~undergrad * HW * multi * Spiciness, data=realdata))
</code></pre>

<p>returns R^2 = 0.4231</p>

<p>And similarly, all summaries involving these ""products"" seem to give me higher R values. Is this a good thing? How should I interpret my data?</p>

<p>Finally, should I model my data using something other than a simple linear model? In case it will help, here is my data:</p>

<pre><code>   undergrad multi HW AA.Int Spiciness 
1          0     0  1   6.00       4.0 
2          0     1  1  27.50       2.5
3          1     0  1  54.50       2.0 
4          1     1  0   7.50       2.0 
5          1     1  1  16.50       2.0 
6          1     0  0  15.00       3.0 
7          0     0  0  18.00       3.0 
8          0     0  0  29.00       1.5 
9          0     0  0   9.50       3.0 
10         0     1  1  33.83       3.0 
11         0     0  0  22.93       2.0 
12         0     0  1  12.93       1.0 
13         0     0  1  13.30       3.0 
14         1     1  1  13.20       2.0 
15         0     1  1  16.00       3.5 
16         0     0  0  21.50       1.0 
17         0     0  0  21.10       3.0 
18         0     1  0  19.50       1.5 
19         0     0  0  12.20       3.0 
20        NA    NA NA     NA        NA 
21         0     1  1  25.30       2.5 
22         0     0  0  26.20       3.0 
23         0     0  0   3.50       4.0 
24         0     0  0  21.40       3.5 
25         0     1  0  10.00       2.0 
26         1     1  1  12.00       2.0 
27         0     1  0  20.80       3.0 
28         1     1  0  20.00       3.0 
29         1     0  0  20.50       3.0 
30         0     0  0  16.50       2.0 
31         0     0  0  15.90       2.0 
32         0     1  1  37.80       3.0 
33         0     0  0   4.00       3.0 
34         0     0  0  17.60       1.5 
35         0     0  1  11.80       3.5 
36         0     0  0   4.60       1.0 
37         0     0  0  18.40       3.0 
38         0     1  1  13.05       3.0 
39         1     1  1  12.50       2.0 
40         0     1  1   6.50       3.5 
41         0     0  1  15.95       3.5 
42         0     1  0  16.00       1.5 
43         0     1  1   9.50       3.0 
44         0     0  0  13.30       3.0 
45        NA    NA NA     NA        NA 
46         0     0  0  18.90       3.5 
47         0     0  0  21.80       2.0 
48         0     1  0  24.70       2.0 
49         0     1  0  19.60       3.0 
50         0     0  0  20.30       1.0 
51         0     1  0   5.80       4.0 
52         0     0  0  20.50       2.0 
53         0     0  0  13.90       4.0 
54         0     0  0  16.00       2.0 
55         0     0  0   9.80       2.5 
56         0     1  1  29.00       3.0 
57         0     0  0  14.30       3.0 
58         0     0  0  17.00       2.0 
59         0     0  0  12.70       3.5 
60        NA    NA NA     NA        NA 
61         0     1  0   6.70       3.0 
62         0     1  0  15.00       2.5 
63        NA    NA NA     NA        NA 
64         0     0  0   7.83       1.0 
65        NA    NA NA     NA        NA 
66         0     0  0  30.83       2.0 
67         0     0  0  14.80       4.0 
68         0     0  0  18.70       2.5 
69         0     1  0  16.30       3.5 
70         0     0  0  11.60       2.0 
71         0     0  0  37.80       2.0 
72         0     1  0   9.40       4.0 
73         0     0  0  12.80       1.0 
74         0     1  0   4.00       4.0 
75         0     0  0  11.80       3.0 
76         0     0  0  20.50       2.5 
77         0     0  0  23.90       2.0 
78         0     0  0  11.00       3.5 
79         0     1  0  12.50       4.0 
80         0     0  1  25.60       3.0 
81         0     0  0  16.20       1.5 
82         0     0  0  17.80       2.5 
83         0     0  0   4.50       2.5 
84         0     0  1   7.90       1.0 
85         0     0  1   5.50       4.0 
86         0     0  0  15.85       3.0 
87         0     0  0  11.50       1.5 
</code></pre>
"
"NaN","NaN"," 13702","<p>I just found ""Robust Fitting of Linear Models"" rlm() function in the MASS library.</p>

<p>I would like to know what is the difference between this function and the standard lm() (linear regression).</p>

<p>Could someone give me a short explanation?</p>

<p>Thank you</p>
"
"0.0506572678011219","0.0620651280774201"," 13836","<p>Some research has shown that in linear regression applications the Mahalanobis distance approach can be used to perform regressions that lower the influence of outliers. The idea is that in the regression every observation is given a weight as an inverse of the Mahalanobis distance. </p>

<p>I see that there is a package <a href=""http://cran.r-project.org/web/packages/RLMM/vignettes/RLMM.pdf"" rel=""nofollow"">RLMM</a> for applying Mahalanobis distance in a classification setting. However, I do not see a regression technique that allows one to apply this as a robust regression technique. </p>

<p>My assumption is that I can use the <code>lm()</code> function and specify weights as the inverse of the output of Mahalanobis distance function. Since it seems the Mahalanobis distance function is <a href=""http://en.wikipedia.org/wiki/Generalized_least_squares"" rel=""nofollow"">equivalent</a> to using GLS then can I simply use the <code>gls()</code> function?</p>
"
"0.02831827358943","0.0277563690826684"," 14111","<p>I'm doing an unit root test using <strong>Phillips-Perron</strong> in the <em>tseries</em> library (pp.test).</p>

<p>I tried this code:</p>

<pre><code>&gt; pp.test(c(1:1000))
</code></pre>

<p>and the result is:</p>

<pre><code>Error in pp.test(c(1:1000)) : Singularities in regression
</code></pre>

<p>doing a research I found the lines on the pp.test function with this error:</p>

<pre><code>if (res$rank &lt; 3)
   stop (""singularities in regression"")
</code></pre>

<p><em>where <strong>res</strong> is the linear model (lm).</em></p>

<p>Why i get this kind of error?</p>

<p>If I do the same thing with another unit root test (in the same library) like ADF I don't get errors.</p>

<p>Thank you</p>
"
"0.0895502439463906","0.0877733458775107"," 14206","<p>I am using SVM to predict diabetes. I am using the <a href=""http://www.cdc.gov/BRFSS/"">BRFSS</a> data set for this purpose. The data set has the dimensions of $432607 \times 136$ and is skewed. The percentage of <code>Y</code>s in the target variable is $11\%$ while the <code>N</code>s constitute the remaining $89\%$.</p>

<p>I am using only <code>15</code> out of <code>136</code> independent variables from the data set. One of the reasons for reducing the data set was to have more training samples when rows containing <code>NA</code>s are omitted.</p>

<p>These <code>15</code> variables were selected after running statistical methods such as random trees, logistic regression and finding out which variables are significant from the resulting models. For example, after running logistic regression we used <code>p-value</code> to order the most significant variables.</p>

<p>Is my method of doing variable selection correct? Any suggestions to is greatly welcome. </p>

<p>The following is my <code>R</code> implementation. </p>

<pre><code>library(e1071) # Support Vector Machines

#--------------------------------------------------------------------
# read brfss file (huge 135 MB file)
#--------------------------------------------------------------------
y &lt;- read.csv(""http://www.hofroe.net/stat579/brfss%2009/brfss-2009-clean.csv"")
indicator &lt;- c(""DIABETE2"", ""GENHLTH"", ""PERSDOC2"", ""SEX"", ""FLUSHOT3"", ""PNEUVAC3"", 
    ""X_RFHYPE5"", ""X_RFCHOL"", ""RACE2"", ""X_SMOKER3"", ""X_AGE_G"", ""X_BMI4CAT"", 
    ""X_INCOMG"", ""X_RFDRHV3"", ""X_RFDRHV3"", ""X_STATE"");
target &lt;- ""DIABETE2"";
diabetes &lt;- y[, indicator];

#--------------------------------------------------------------------
# recode DIABETE2
#--------------------------------------------------------------------
x &lt;- diabetes$DIABETE2;
x[x &gt; 1]  &lt;- 'N';
x[x != 'N']  &lt;- 'Y';
diabetes$DIABETE2 &lt;- x; 
rm(x);

#--------------------------------------------------------------------
# remove NA
#--------------------------------------------------------------------
x &lt;- na.omit(diabetes);
diabetes &lt;- x;
rm(x);

#--------------------------------------------------------------------
# reproducible research 
#--------------------------------------------------------------------
set.seed(1612);
nsamples &lt;- 1000; 
sample.diabetes &lt;- diabetes[sample(nrow(diabetes), nsamples), ]; 

#--------------------------------------------------------------------
# split the dataset into training and test
#--------------------------------------------------------------------
ratio &lt;- 0.7;
train.samples &lt;- ratio*nsamples;
train.rows &lt;- c(sample(nrow(sample.diabetes), trunc(train.samples)));

train.set  &lt;- sample.diabetes[train.rows, ];
test.set   &lt;- sample.diabetes[-train.rows, ];

train.result &lt;- train.set[ , which(names(train.set) == target)];
test.result  &lt;- test.set[ , which(names(test.set) == target)];

#--------------------------------------------------------------------
# SVM 
#--------------------------------------------------------------------
formula &lt;- as.formula(factor(DIABETE2) ~ . );
svm.tune &lt;- tune.svm(formula, data = train.set, 
    gamma = 10^(-3:0), cost = 10^(-1:1));
svm.model &lt;- svm(formula, data = train.set, 
    kernel = ""linear"", 
    gamma = svm.tune$best.parameters$gamma, 
    cost  = svm.tune$best.parameters$cost);

#--------------------------------------------------------------------
# Confusion matrix
#--------------------------------------------------------------------
train.pred &lt;- predict(svm.model, train.set);
test.pred  &lt;- predict(svm.model, test.set);
svm.table &lt;- table(pred = test.pred, true = test.result);
print(svm.table);
</code></pre>

<p>I ran with $1000$ (training = $700$ and test = $300$) samples since it is faster in my laptop. The confusion matrix for the test data ($300$ samples)  I get is quite bad.</p>

<pre><code>    true
pred   N   Y
   N 262  38
   Y   0   0
</code></pre>

<p>I need to improve my prediction for the <code>Y</code> class. In fact, I need to be as accurate as possible with <code>Y</code> even if I perform poorly with <code>N</code>. Any suggestions to improve the accuracy of classification would be greatly appreciated.</p>
"
"0.113469578623964","0.111218061863672"," 14694","<p>I have a dataset of genomic information which I'm going to be comparing with various biochemical markers. Unfortunately a lot of the biochemical markers have limited ranges in their assays, so I have a lot of data that looks like ""40"", "">45"", ""35"", "">45"" for tests that have a threshold at 45 (for example).
My intended analysis for most of this data is linear regression in R. So what is the statistically correct way to deal with this data?</p>

<ol>
<li><p>Ignore it, let R cast the values with "">"" to <code>NA</code> and potentially lose information about important associations</p></li>
<li><p>Make the over threshold values equal to the threshold. This has similar problems to 1)</p></li>
<li><p>It depends. Sigh. Could you please give me some pointers as to what other considerations I should be thinking about or information you might need to answer my question? </p></li>
</ol>

<p>Edit: Based on the comments I've given more information about my datasets. The values which are out of range (GFR and Fol) are independent variables which I'll use in linear regression like so:</p>

<pre><code>lm(H~allele+Age+Sex+as.double(GFR)+as.double(Fol))
</code></pre>

<p>GFR looks like: </p>

<pre><code>summary(as.double(GFR)) 
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
31.00   70.00   77.00   75.66   83.00  100.00  105.00
</code></pre>

<p>and appears to be normally distributed:</p>

<pre><code>V = qqnorm(na.omit(as.double(GFR))
cor(V$x, V$y)
[1] 0.9911351
</code></pre>

<p>There are 105 values coded as "">90"" (not sure why the summary said Max is 100) out of 434.</p>

<p>Fol is distributed like so:</p>

<pre><code>summary(as.double(Fol))
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
6.10   23.20   29.80   29.14   35.70   45.30    8.00
</code></pre>

<p>and also appears to be normally distributed:</p>

<pre><code>V  = qqnorm(na.omit(as.double(Fol)))   
cor(V$x, V$y)
[1] 0.9911351
</code></pre>

<p>There are 8 out of 434 variables in Fol coded are "">45.3"". I took my cue for calling these normally distributed from <a href=""http://www.math.utah.edu/~davar/ps-pdf-files/Assessing_Normality.pdf"" rel=""nofollow"">this assessment of normality guide</a> ).</p>

<p>I also have another variable CRP which is a dependent variable, which I'd like to do linear regression on similarly to the above. CRP has 11 out of 434 coded as ""&lt;0.2"". Its distribution is:</p>

<pre><code>summary(as.double(CRP))
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
0.200   0.600   1.300   2.674   2.650 112.400  11.000
</code></pre>

<p>The data graphed is clearly not normaly and it has a correlation with qqnorm of 0.5153663. The value of 112 is a clear outlier. </p>

<p>I hope that makes it more clear. Please let me know if you need more information. Thanks for your help.</p>
"
"NaN","NaN"," 14842","<p>Please, take a look at the chart below. </p>

<p>As you can see the first period the volatility is high and the second is low. </p>

<p>How can I check if the volatility is stationary (homogeneous) during the entire period? </p>

<p>The plot represents the residuals of a simple linear regression.
<img src=""http://i.stack.imgur.com/HiFyd.png"" alt=""Residual Chart""></p>
"
"0.0633215847514023","0.0620651280774201"," 14872","<p>I'm testing the residuals of a linear regression using <strong>Breusch-Pagan</strong> Test to detect Heteroscedasticity.</p>

<p>This is the plot of the residuals:
<img src=""http://i.stack.imgur.com/28sNL.jpg"" alt=""Residuals""></p>

<p>and this is the R code:</p>

<pre><code>&gt; library(lmtest)
&gt; 
&gt; mod &lt;- lm(rnorm(1000)~1)
&gt; 
&gt; bptest(mod)

    studentized Breusch-Pagan test

data:  mod 
BP = 0, df = 0, p-value &lt; 2.2e-16
</code></pre>

<p>Could someone tell me why it rejects the null hypothesis of homoscedastic errors?</p>

<p>The plot doesn't look heteroscedastic.</p>

<p><strong>EDIT:</strong></p>

<p>However the plot is an example, I have two list of prices (<strong>priceA</strong> and <strong>priceB</strong>), I need to check if the residuals generated by a linear regression of these two list: lm(priceA ~ priceB + 0) I need zero intercept are homescedastic or not. Could someone give me a small example? The length of each price list is 750.</p>

<p><strong>EDIT:</strong></p>

<p>I also get: </p>

<p>BP = 67.4362, df = 1, p-value &lt; 2.2e-16</p>

<p>with this chart
<img src=""http://i.stack.imgur.com/vKe1G.png"" alt=""new charts""></p>

<p>Does it be homoscedastic? I have plotted the residuals.</p>

<p><strong>@Wolfgang</strong>, I get this result following the example you posted.</p>

<pre><code>&gt; summary(mod)$r.squared * 750
[1] 681.0114
</code></pre>
"
"NaN","NaN"," 14914","<p>I have a matrix with two columns that have many prices (750).
In the image below I plotted the residuals of the follow linear regression:</p>

<pre><code>lm(prices[,1] ~ prices[,2])
</code></pre>

<p>Looking at image, seems to be a very strong autocorrelation of the residuals.</p>

<p>However how can I test if the autocorrelation of those residuals is strong? What method should I use?</p>

<p><img src=""http://i.stack.imgur.com/kwASv.jpg"" alt=""Residuals of the linear regression""></p>

<p>Thank you!</p>
"
"0.0490486886395286","0.0480754414848157"," 15469","<p>Recently we discussed on SO how to update a standard linear regression summary with NeweyWest standard errors. I used <code>coeftest</code>from the <code>sandwich</code> package. It was told to use unclass to update my already existing summary like this: </p>

<pre><code>library(sandwich)
library(lmtest)
temp.lm &lt;- lm(runif(100) ~ rnorm(100))
temp.summ &lt;- summary(temp.lm)
temp.summ$coefficients &lt;- unclass(coeftest(temp.lm, vcov. = NeweyWest)
</code></pre>

<p>Now I wonder whether the joint parameters shown in the summary aren't affected at all when using a NeweyWest VC matrix? I mean with this code they are not affected obviously â€“Â but is this correct? Note this is not a syntax but a stats question :) Stuff like</p>

<pre><code>Residual standard error: 1.177 on 83 degrees of freedom  
Multiple R-squared: 0.7265, Adjusted R-squared:  0.71 
F-statistic:  44.1 on 5 and 83 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>remains the same. Are there any cases that need adjustment as well?</p>
"
"0.0566365471788599","0.0555127381653369"," 15597","<p>As my previous questions I'm trying to solve a problem with my stocks tests.
I tried Breusch-Pagan test for heteroscedasticity but some residuals still pass these tests.</p>

<p>My procedure is:</p>

<ul>
<li><p>Get two stocks prices (I have a matrix with two columns that represent the price lists)</p></li>
<li><p>I do a linear regression, like: <code>lm(prices[,1] ~ prices[,2])</code></p></li>
<li><p>Then I test the residuals of the linear regression with Unit Root tests (PP and KPSS)</p></li>
<li><p>After these tests I do Breusch-Pagan test because I only need to work with stocks that have constant variance during all the period. I do: <code>bptest(prices[,1] ~ prices[,2])</code></p></li>
</ul>

<p>Ok, now I still get strange results, with strange i mean that when i plot the residuals i see that the variance is not constant (take a look at the chart below). So now, I need to understand how better test is the variance is constant. </p>

<p>I read someone use GARCH (1,1) but I never used it, could someone exmplain it or maybe give me other tests to try?</p>

<p><img src=""http://i.stack.imgur.com/OYqKq.png"" alt=""enter image description here""></p>
"
"NaN","NaN"," 15974","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/15427/kruskal-wallis-or-fligner-test-to-check-homogeneity-of-variances"">Kruskal-Wallis or Fligner test to check homogeneity of variances?</a>  </p>
</blockquote>



<p>Is there a test to check if the residuls of a linear regression are constant?</p>

<p>Thanks</p>
"
"0.0566365471788599","0.0555127381653369"," 16346","<p>(Please note the cross-post at <a href=""http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit"">http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit</a>)</p>

<p>I am not sure I see the difference between different examples for local logistic regression in the documentation of the gold standard locfit package for R: <a href=""http://cran.r-project.org/web/packages/locfit/locfit.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/locfit/locfit.pdf</a></p>

<p>I get starkingly different results with</p>

<pre><code>fit2&lt;-scb(closed_rule ~ lp(bl),deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>from</p>

<pre><code>fit2&lt;-scb(closed_rule ~ bl,deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>.</p>

<p>What is the nature of the difference? Maybe that can help me phrase which I wanted. I had in mind an index linear in bl within a logistic link function predicting the probability of closed_rule. The documentation of lp says that it fits a local polynomial -- which is great, but I thought that would happen even if I leave it out. And in any case, the documentation has examples for ""local logistic regression"" either way...</p>
"
"0.0716401951571125","0.0789960112897596"," 16605","<p>I'll preface my question with the fact that I'm just learning about linear regression so I may be thinking about this wrong. </p>

<p>I have a set of data. In this set I have one dependent variable and about 10 independent variables and the data set is growing regularly. It's rows of data in database with 10 columns of independent variables and one column of the dependent variable. You can see my previous question for an example of what I'm trying to do: <a href=""http://stats.stackexchange.com/questions/13673/variables-importance-who-can-do-the-most-pushups"">Variables importance: who can do the most pushups?</a></p>

<p>The output of a linear regression is a formula right? </p>

<p>Now I want to write a python script (I could use R also but I'd greatly prefer python) to take this data as input and output the linear regression formula. Is there a python method to do this? Do I need to run a regression comparing each independent variable with the dependent variable one at a time? Or is there a python method to feed in the data with all 10 independent variables and come out with a formula? </p>
"
"0.0566365471788599","0.0555127381653369"," 16915","<p>Note that I do most of my analysis using R and Excel.</p>

<p>Let's take this data set for example. I modified it as the data itself is proprietary: the years are also different:</p>

<pre><code>1967    2,033,407
1968    2,162,275
1969    2,159,640
1970    2,312,352
1971    2,554,449
1972    2,548,425
1973    2,101,225
1974    1,951,944
1975    2,106,250
1976    1,687,625
1977    1,636,496
1978    1,494,525
1979    1,606,825
1980    1,460,937
1981    1,310,494
1982    1,319,750
1983    1,263,643
1984    1,171,656
1985    1,194,950
</code></pre>

<p>What I usually do:</p>

<ol>
<li>A linear regression</li>
<li>Some form of polynomial trending</li>
<li>Moving average and double moving average</li>
<li>Basic ARIMA using p = 1, q = 0.</li>
<li>I calculate the errors for all these as well</li>
<li>I average all the forecasts out and the error to have my final result.</li>
</ol>

<p>Note that I'm an engineer that wants to get into statistics and the ability to properly validate and calibrate my models.</p>

<h2>Question</h2>

<p>What is the correct way to forecast this to 5, 10, or even 15 future years?</p>

<p>In a way I'm looking to move beyond the plugging data into a model and believe the data. Yes, I'm aware I can look at the errors. I mainly use RMSE or MAE. But I still am not confident when it comes to just predicting data the right way.</p>

<h3>Note</h3>

<p>this is also related to <a href=""http://stats.stackexchange.com/questions/16545/how-can-i-be-confident-about-my-forecasts-and-improve-my-methodologies"">this question</a> I posted here before.</p>
"
"0.102102987459307","0.100077011948264"," 17552","<p>I have a gene expression data-set with log2-transformed expression values (no NAs) for 495 genes for 59 samples for which values of a continuous response variable (r) are also known (no NAs). I want to use leave-one-out cross validation to test if r of the test sample can be predicted from the sample's gene expression.</p>

<p>For this, I intend to use the <a href=""http://cran.r-project.org/web/packages/samr/index.html"" rel=""nofollow"">samr</a> R package for Significance Analysis of Microarrays to identify significant genes associated with r in the training set of samples. Then, I want to generate a linear model using the significant genes as variables, which will then be used to predict r of the test sample. I have tried the following code to begin with, but when I generate the model and examine it, I see many NAs in the model summary, which makes me suspect that I am doing something wrong.</p>

<p>Can someone tell me what I might be doing wrong?</p>

<p>Secondly, I will appreciate any comment on the use of nperms (in SAM) with a value of 100. Is it too low for an expression data-set for 495 genes. </p>

<pre><code># rVals with the r values is read as a vector from a row of a table for phenotypic data read from a tab-delimited file with sample-names as column names and phenotype features as row-names
# geneVals is the log2-transformed gene expression data-set read as a matrix from a tab-delimited file with sample-names as column names and gene-names as row-names

# Perform SAM with FDR of 5% and obtain list of significant genes

sam &lt;- SAM(x=geneVals, y=rVals, resp.type=c(""Quantitative""),
testStatistic=c(""standard""), regression.method=c(""standard""), logged2=TRUE, 
fdr.output=0.05, eigengene.number=1, knn.neighbors=10, nperms=100, 
genenames=as.vector(rownames(geneVals)))

sigGenes &lt;- rbind(sam$siggenes.table$genes.up, sam$siggenes.table$genes.lo)

# Generate linear model
toModel &lt;- data.frame(t(rbind(rVals, geneVals)), check.names=FALSE)
myModel &lt;- lm(toModel[c('r', sigGenes[,c(""Gene ID"")])])

# Examine model
summary(myModel)

...output...

Call:
lm(formula = toModel[c(""rVals"", sigGenes[, c(""Gene ID"")])])

Residuals:
ALL 59 residuals are 0: no residual degrees of freedom!

Coefficients: (58 not defined because of singularities)
           Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   -18.29363         NA      NA       NA
`let-7e`       -1.70545         NA      NA       NA
`miR-125a-5p`   2.43177         NA      NA       NA
`miR-151-5p`    2.67439         NA      NA       NA
...
</code></pre>
"
"0.02831827358943","0.0277563690826684"," 17815","<p>I was wondering if it is possible to use the caret package with non numerical data.
I know, for example, if I want to use a simple linear regression <code>lm</code> I could have a factor variable for classification.
However, caret blows up if I attempt this. I'm also following the step outlined here <a href=""http://www.r-project.org/conferences/useR-2010/slides/Kuhn.pdf"" rel=""nofollow"">The caret Package: A Unifed Interface for Predictive
Models</a></p>

<p>for illustration I'm attempting to run <code>stepDurationlm &lt;- train (x= trainDescr, y=trainClass, method=""lm"")</code></p>

<p>on</p>

<pre><code>str(trainDescr)
'data.frame':   589235 obs. of  2 variables:
 $ Anon.Student.Id    : Factor w/ 574 levels &quot;02i5jCrfQK&quot;,&quot;02ZjVTxC34&quot;,..: 7 7 7 7 7 7 7 7 7 7 ...
 $ Step.Duration..sec.: num  5 5 5 4 5 5 5 5 4 4 ...
</code></pre>

<p>I get</p>

<pre><code>Error in train.default(x = trainDescr, y = trainClass, method = ""lm"") : 
All predictors must be numeric for this model. Use the formula interface: train(formula, data)
</code></pre>

<p>alternatively, could anyone explain how to have a test set for model performance in R? That's what's motivating me to get caret working.</p>
"
"0.0749231094763201","0.0734364498908627"," 18385","<p>So I am using the R code behind Fig. 3.14 in <a href=""http://rads.stackoverflow.com/amzn/click/0387772375"" rel=""nofollow"">Dynamic Linear Models With R</a> (p. 124-5)  to make a dynamic version of a simple pair trading model:</p>

<p>$$
Y = \alpha + \beta X.
$$</p>

<p>If I use log returns (<code>diff(log(P))</code> in R, P being a timeseries of equity prices) I get results that makes sense, they are pretty similar to the static regression of the whole dataset (comparable to the R code on p. 123). So far so good.</p>

<p>But if I instead use log prices (<code>log(P)</code> in R, P being a timeseries of equity prices), the results don't make sense at all, $\alpha$ has the wrong sign and $\beta$ is almost constant throughout the timeseries at half of what I would expect compared to the regular static regression.</p>

<p>So what am I doing wrong here? How can I change the code to produce better results for log prices?</p>
"
"NaN","NaN"," 18387","<p>Could someone please explain what the difference is between the two, and perhaps avoid the worst statistical jargon?</p>

<p>I am currently using the <code>dlm</code> package to model dynamic regressions as can be seen on p.122-5 in <a href=""http://rads.stackoverflow.com/amzn/click/0387772375"" rel=""nofollow"">Dynamic Linear Models with R</a>, and I don't really grasp the theoretical difference between using <code>dlmFilter</code> and <code>dlmSmooth</code>.</p>

<p>More specifically, why is the <code>dlmFilter</code> example on p. 123 not considered dynamic, while the <code>dlmSmooth</code> example on p.124-5 is? And also, why are the two results fairly different?</p>
"
"0.02831827358943","0.0277563690826684"," 18391","<p>Is there a method to understand if two lines are (more or less) parallel? I have two lines generated from linear regressions and I would like to understand if they are parallel. In other words, I would like to get the different of the slopes of those two lines.</p>

<p>Is there an R function to calculate this?</p>

<p><em>EDIT:</em>
... and how can I get the slope (in degrees) of a linear regression line?</p>
"
"0.192424860981002","0.192455792208201"," 18709","<p>I want to fit mixed model using lme4, nlme, baysian regression package or any available. </p>

<p><em><strong>Mixed model in Asreml- R  coding conventions</em></strong></p>

<p>before going into specifics, we might want to have details on asreml-R conventions, for those who are unfamiliar with ASREML codes.</p>

<pre><code>y = XÏ„ + Zu + e ........................(1) ; 
</code></pre>

<p>the usual mixed model with, y denotes the n Ã— 1 vector of observations,where Ï„ is the pÃ—1 vector of ï¬xed eï¬€ects, X is an nÃ—p design matrix of full column rank which associates observations with the appropriate combination of ï¬xed eï¬€ects, u is the q Ã— 1 vector of random eï¬€ects, Z is the n Ã— q design matrix which associates observations with the appropriate combination of random eï¬€ects, and e is the n Ã— 1 vector of residual errors.The model (1) is called a linear mixed model or linear mixed eï¬€ects model. It is assumed </p>

<p><img src=""http://i.stack.imgur.com/gxdur.jpg"" alt=""enter image description here""></p>

<p>where the matrices G and R are functions of parameters Î³ and Ï†, respectively.</p>

<p>The parameter Î¸ is a variance parameter which we will refer to as the scale parameter.</p>

<p>In mixed eï¬€ects models with more than one residual variance, arising for example in the
analysis of data with more than one section or variate, the parameter Î¸ is
ï¬xed to one. In mixed eï¬€ects models with a single residual variance then Î¸ is equal to
the residual variance (Ïƒ2). In this case R must be correlation matrix. Further details on the models are provided in the <a href=""http://www.vsni.co.uk/downloads/asreml/release2/doc/asreml-R.pdf"">Asreml manual (link)</a>. </p>

<p>Variance structures for the errors: R structure and Variance structures for the random eï¬€ects: G structures can be specified.</p>

<p><img src=""http://i.stack.imgur.com/or4Gj.jpg"" alt=""enter image description here""><img src=""http://i.stack.imgur.com/oXTgc.jpg"" alt=""enter image description here""></p>

<p>variance modelling in asreml() it is important to understand the formation of variance structures via direct products. The usual least squares assumption (and the default in asreml()) is that these are independently and identically distributed (IID). However, if the data was from a field experiment laid out in a rectangular array of r rows by c columns, say, we could arrange the residuals e as a matrix and potentially consider that they were autocorrelated within rows and columns.Writing the residuals as a vector in field order, that is, by sorting the residuals rows
within columns (plots within blocks) the variance of the residuals might then be</p>

<p><img src=""http://i.stack.imgur.com/SPE5b.jpg"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/IcikW.jpg"" alt=""enter image description here""> are correlation matrices for the row model (order r, autocorrelation parameter Â½r) and column model (order c, autocorrelation parameter Â½c)
respectively. More specifically, a two-dimensional separable autoregressive spatial structure
(AR1 x Â­ AR1) is sometimes assumed for the common errors in a field trial analysis.</p>

<p><em><strong>The example data:</em></strong></p>

<p>nin89 is from asreml-R library, where different varities were grown in replications / blocks in rectangular field. To control additional variability in row or column direction each plot is referenced as Row and Column variables (row column design). Thus this row column design with blocking. Yield is measured variable. </p>

<p><strong>Example models</strong> </p>

<p>I need something equivalent to the asreml-R codes:</p>

<p>The simple model syntax will look like the follows:</p>

<pre><code> rcb.asr &lt;- asreml(yield âˆ¼ Variety, random = âˆ¼ Replicate, data = nin89)  
 .....model 0
</code></pre>

<p>The linear model is specified in the fixed (required), random (optional) and rcov (error
component) arguments as formula objects.The default is a simple error term and does not need to be formally specified for error term as in the model 0. </p>

<p>here the variety is fixed effect and random is replicates (blocks). Beside random and fixed terms we can specify error term. Which is default in this model 0. The residual or error component of the model is specified in a formula object through the rcov argument, see the following models 1:4. </p>

<p>The following model1 is more complex in which both G (random) and R (error) structure are specified.</p>

<p><strong>Model 1:</strong> </p>

<pre><code>data(nin89)


 # Model 1: RCB analysis with G and R structure
     rcb.asr &lt;- asreml(yield ~ Variety, random = ~ idv(Replicate), 
      rcov = ~ idv(units), data = nin89)
</code></pre>

<p>This model is equivalent to above model 0, and introduces the use of G and R variance model. Here the option random and rcov specifies random and rcov formulae to explicitly specify the G and R structures. where idv() is the special model function in asreml() that identifies the variance model. The expression idv(units) explicitly sets the variance matrix for e to a scaled identity.</p>

<p><em><strong># Model 2: two-dimensional spatial model with correlation in one direction</em></strong></p>

<pre><code>  sp.asr &lt;- asreml(yield ~ Variety, rcov = ~ Column:ar1(Row), data = nin89)
</code></pre>

<p>experimental units of nin89 are indexed by Column and Row. So we expect random variation in two direction - row and column direction in this case. where ar1() is a special function specifying a first order autoregressive variance model for Row. This call specifies a two-dimensional spatial structure for error but with spatial correlation in the row direction only.The variance model for Column is identity (id()) but does not need to be formally
specified as this is the default.</p>

<p><em><strong># model 3: two-dimensional spatial model, error structure in both direction</em></strong></p>

<pre><code> sp.asr &lt;- asreml(yield ~ Variety, rcov = ~ ar1(Column):ar1(Row),  
 data = nin89)
sp.asr &lt;- asreml(yield ~ Variety, random = ~ units, 
 rcov = ~ ar1(Column):ar1(Row), data = nin89)
</code></pre>

<p>similar to above model  2, however the correlation is two direction - autoregressive one. </p>

<p>I am not sure how much of these models are possible with open source R packages. Even if solution of any one of these models will be of great help. <strong><em>Even if the bouty of +50 can stimulate to develop such package will be of great help !</em></strong></p>

<p><em><strong>See MAYSaseen has provided output from each model and data  (as answer)  for comparision.</em></strong> </p>

<p><em><strong>Edits: 
The following is suggestion I received in mixed model discussion forum:</em></strong>
"" You might look at the regress and spatialCovariance packages of David Clifford.  The former allows fitting of (Gaussian) mixed models where you can specify the structure of the covariance matrix very flexibly (for example, I have used it for pedigree data).  The spatialCovariance package uses regress to provide more elaborate models than AR1xAR1, but may be applicable.  You may have to correspond with the author about applying it to your exact problem."" </p>
"
"0.0755153962384799","0.0832691072480053"," 18880","<p>Recently I have opened a question here to understand the output of a GARCH model.
My goal is to understand if the series I'm checking is heteroscedastic or not.</p>

<p>I'm using the <code>garch()</code> function from the <a href=""http://cran.r-project.org/web/packages/tseries/index.html"" rel=""nofollow"">tseries</a> package.</p>

<p>First I built a linear regression like this:</p>

<pre><code>mod &lt;- lm(a ~ b)
</code></pre>

<p>Then I need to check if the residuals of this linear regression present heteroscedasticity.</p>

<p>I did:</p>

<pre><code>g &lt;- garch(resid(mod), order(c(1,1)))
</code></pre>

<p>and then </p>

<pre><code>summary(g)
</code></pre>

<p>I get the follow output:</p>

<pre><code>&gt; summary(g) 

Call: 
garch(x = lm(A ~ B)$resi, order = order(c(1, 1))) 

Model: 
GARCH(1,2) 

Residuals: 
    Min      1Q  Median      3Q    Max 
-4.2058 -1.0262  0.1404  1.1069  3.6553 

Coefficient(s): 
    Estimate  Std. Error  t value Pr(&gt;|t|)    
a0 3.361e-04  9.352e-05    3.594 0.000326 *** 
a1 3.045e-01  4.486e-02    6.787 1.14e-11 *** 
a2 1.209e-06  8.855e-02    0.000 0.999989    
b1 4.938e-01  1.060e-01    4.660 3.17e-06 *** 
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Diagnostic Tests: 
    Jarque Bera Test 

data:  Residuals 
X-squared = 18.84, df = 2, p-value = 8.108e-05 


    Box-Ljung test 

data:  Squared.Residuals 
X-squared = 49.7251, df = 1, p-value = 1.769e-12 
</code></pre>

<p>Thanks to the user who answered my question, I now understand that the <code>ao</code> is the intercept and the other <code>a1</code> and <code>b1</code> are the coefficients I need to check to understand if this time series is heteroscedastic or not.</p>

<p>The problem (doubt) is that now I also see <code>a2</code> in the regression table: What does it stand for?</p>

<p>Is it correct to say that if all coefficients have a $p$-value above 0.05 (...)</p>
"
"0.0693653206906364","0.0679889413649005"," 19469","<p>Here is an example: I have a set of observations of different individuals from lots of different families of grasses:</p>

<pre><code>individual#, Fam, Genus, Factor1(3 levels), Factor2(7 levels), Factor3(5 levels), Response1(3 levels), Response2(3 levels)
</code></pre>

<p>What I am hoping to discover is whether the frequency of occurrences of Response1 and 2 are linked to family groups, and whether Factors 1 - 3 (things like soil type, sun exposure etc) have an impact. </p>

<p>Example: </p>

<pre><code>family,  resp1a,  resp1b,   resp1c 
1,       14%(20), 16%(24),  67%(98),  Total N = 147  
2,       38%(98), 86%(220), 48%(123), Total N = 256
...
</code></pre>

<p>First, I need to see whether these differences in responses between families is significant (chi-squared?). Secondly, I need to see if one of the 3 factors has an effect on the response.</p>

<p>Now it seems in my basic understanding, that if the response(s) were continuous measurement, ANOVA/MANOVA would work. Easy-peasy. However, since everything is discreet categories (including the independent and dependent variables) I can't do this. Additionally, since the responses are not mutually exclusive, this seems to violate an assumption of the log-linear model.</p>

<p>I've scoured, and keep bouncing around between Multinomial Logistic Regression, or just independent Chi-Square tests, or... hell I don't know anymore.</p>

<p>And yes, I am trying to swim before learning to float.</p>

<p>Oh, and this is all happening in R.</p>
"
"0.0853828074607","0.0836886016271203"," 19772","<p>Can someone please tell me how to have R estimate the break point in a piecewise linear model (as a fixed or random parameter), when I also need to estimate other random effects? </p>

<p>I've included a toy example below that fits a hockey stick / broken stick regression with random slope variances and a random y-intercept variance for a break point of 4. I want to estimate the break point instead of specifying it. It could be a random effect (preferable) or a fixed effect.</p>

<pre><code>library(lme4)
str(sleepstudy)

#Basis functions
bp = 4
b1 &lt;- function(x, bp) ifelse(x &lt; bp, bp - x, 0)
b2 &lt;- function(x, bp) ifelse(x &lt; bp, 0, x - bp)

#Mixed effects model with break point = 4
(mod &lt;- lmer(Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject), data = sleepstudy))

#Plot with break point = 4
xyplot(
        Reaction ~ Days | Subject, sleepstudy, aspect = ""xy"",
        layout = c(6,3), type = c(""g"", ""p"", ""r""),
        xlab = ""Days of sleep deprivation"",
        ylab = ""Average reaction time (ms)"",
        panel = function(x,y) {
        panel.points(x,y)
        panel.lmline(x,y)
        pred &lt;- predict(lm(y ~ b1(x, bp) + b2(x, bp)), newdata = data.frame(x = 0:9))
            panel.lines(0:9, pred, lwd=1, lty=2, col=""red"")
        }
    )
</code></pre>

<p>Output:</p>

<pre><code>Linear mixed model fit by REML 
Formula: Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject) 
   Data: sleepstudy 
  AIC  BIC logLik deviance REMLdev
 1751 1783 -865.6     1744    1731
Random effects:
 Groups   Name         Variance Std.Dev. Corr          
 Subject  (Intercept)  1709.489 41.3460                
          b1(Days, bp)   90.238  9.4994  -0.797        
          b2(Days, bp)   59.348  7.7038   0.118 -0.008 
 Residual               563.030 23.7283                
Number of obs: 180, groups: Subject, 18

Fixed effects:
             Estimate Std. Error t value
(Intercept)   289.725     10.350  27.994
b1(Days, bp)   -8.781      2.721  -3.227
b2(Days, bp)   11.710      2.184   5.362

Correlation of Fixed Effects:
            (Intr) b1(D,b
b1(Days,bp) -0.761       
b2(Days,bp) -0.054  0.181
</code></pre>

<p><img src=""http://i.stack.imgur.com/HnAfg.jpg"" alt=""Broken stick regression fit to each individual""></p>
"
"0.0600721298597455","0.078506867197886"," 19869","<p>I'm running a predictive model to predict the probability of winning a certain item based on the price that I bid (other factors also). After running the model (ols) in R, I wanted to account for all the variables in my model and develop a graph highlighting the 'predicted probabilities' regarding the primary variables I'm concerned about. So want to have a line graph showing the probability of winning on the y axis, and the bid on the x axis. The following data would result in a graph which shows that the probability of winning decreases as the bid increases.</p>

<pre><code>Bid                  8  6      4
Probability Winning 30% 22%    18%
</code></pre>

<ol>
<li>Are predicted probabilities only relevant for logistic regression models or can be equally relevant for linear regression models?</li>
<li>What is the reasoning and logic behind going from a model to a probability curve which would show the 'trend' in one variable as predicted by another, while accounting for all other factors.</li>
</ol>

<p>Sorry for the elementary question, I'm just a little clueless.
Thanks for the help!</p>
"
"NaN","NaN"," 19889","<p>Currently working in Octave, but due to the poor documentation progress is very slow.</p>

<p>What language is easy to learn and use, and well documented to solve machine learning problems? I am looking to prototype on a small dataset (thousands of examples), so speed is not important.</p>

<p>EDIT: I am developing a recommendation engine. So, I am interested in using Regularized Linear Regression, Neural Nets, SVN or Collaborative Filtering.</p>
"
"NaN","NaN"," 19945","<p>Let's say in linear regression, I got a fit and I can plot residuals to see whether there is any systematic trend in such a plot. How to quantitatively determine whether the residues are really random? Is Durbin-Watson test used for this purpose? How to interpret such test if so?</p>

<p>Please provide an example, preferably in R.</p>
"
"0.132824476734854","0.130188910980824"," 20452","<p>My primary question is how to interpret the output (coefficients, F, P) when conducting a Type I (sequential) ANOVA? </p>

<p>My specific research problem is a bit more complex, so I will break my example into parts. First, if I am interested in the effect of spider density (X1) on say plant growth (Y1) and I planted seedlings in enclosures and manipulated spider density, then I can analyze the data with a simple ANOVA or linear regression. Then it wouldn't matter if I used Type I, II, or III Sum of Squares (SS) for my ANOVA. In my case, I have 4 replicates of 5 density levels, so I can use density as a factor or as a continuous variable. In this case, I prefer to interpret it as a continuous independent (predictor) variable. In R I might run the following:</p>

<pre><code>lm1 &lt;- lm(y1 ~ density, data = Ena)
summary(lm1)
anova(lm1)
</code></pre>

<p>Running the anova function will make sense for comparison later hopefully, so please ignore the oddness of it here. The output is:</p>

<pre><code>Response: y1
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density    1 0.48357 0.48357  3.4279 0.08058 .
Residuals 18 2.53920 0.14107 
</code></pre>

<p>Now, let's say I suspect that the starting level of inorganic nitrogen in the soil, which I couldn't control, may have also significantly affected the plant growth. I'm not particularly interested in this effect but would like to potentially account for the variation it causes. Really, my primary interest is in the effects of spider density (hypothesis: increased spider density causes increased plant growth - presumably through reduction of herbivorous insects but I'm only testing the effect not the mechanism). I could add the effect of inorganic N to my analysis. </p>

<p>For the sake of my question, let's pretend that I test the interaction density*inorganicN and it's non-significant so I remove it from the analysis and run the following main effects:</p>

<pre><code>&gt; lm2 &lt;- lm(y1 ~ density + inorganicN, data = Ena)
&gt; anova(lm2)
Analysis of Variance Table

Response: y1
           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density     1 0.48357 0.48357  3.4113 0.08223 .
inorganicN  1 0.12936 0.12936  0.9126 0.35282  
Residuals  17 2.40983 0.14175 
</code></pre>

<p>Now, it makes a difference whether I use Type I or Type II SS (I know some people object to the terms Type I &amp; II etc. but given the popularity of SAS it's easy short-hand). R anova{stats} uses Type I by default. I can calculate the type II SS, F, and P for density by reversing the order of my main effects or I can use Dr. John Fox's ""car"" package (companion to applied regression). I prefer the latter method since it is easier for more complex problems.</p>

<pre><code>library(car)
Anova(lm2)
            Sum Sq Df F value  Pr(&gt;F)  
density    0.58425  1  4.1216 0.05829 .
inorganicN 0.12936  1  0.9126 0.35282  
Residuals  2.40983 17  
</code></pre>

<p>My understanding is that type II hypotheses would be, ""There is no linear effect of x1 on y1 given the effect of (holding constant?) x2"" and the same for x2 given x1. I guess this is where I get confused. <strong>What is the hypothesis being tested by the ANOVA using the type I (sequential) method above compared to the hypothesis using the type II method?</strong></p>

<p>In reality, my data is a bit more complex because I measured numerous metrics of plant growth as well as nutrient dynamics and litter decomposition. My actual analysis is something like:</p>

<pre><code>Y &lt;- cbind(y1 + y2 + y3 + y4 + y5)
# Type II
mlm1 &lt;- lm(Y ~ density + nitrate + Npred, data = Ena)
Manova(mlm1)

Type II MANOVA Tests: Pillai test statistic
        Df test stat approx F num Df den Df  Pr(&gt;F)    
density  1   0.34397        1      5     12 0.34269    
nitrate  1   0.99994    40337      5     12 &lt; 2e-16 ***
Npred    1   0.65582        5      5     12 0.01445 * 


# Type I
maov1 &lt;- manova(Y ~ density + nitrate + Npred, data = Ena)
summary(maov1)

          Df  Pillai approx F num Df den Df  Pr(&gt;F)    
density    1 0.99950     4762      5     12 &lt; 2e-16 ***
nitrate    1 0.99995    46248      5     12 &lt; 2e-16 ***
Npred      1 0.65582        5      5     12 0.01445 *  
Residuals 16                                           
</code></pre>
"
"0.0693653206906364","0.0679889413649005"," 20672","<p>I have two continuous variables, X and Y, that are correlated - they are not independent. To correct for non-independence, I have a known correlation structure, a matrix S.</p>

<p>If one calls <code>gls(Y ~ X, correlation = S)</code>, what I think happens is that, internally, gls() transforms X and Y in some way so that the regression ends up being <code>S^(-1)*Y = S^(-1) * X</code>.</p>

<p>How is this transformation actually performed? From the literature I've consulted, I've seen everything from:</p>

<pre><code>X.transformed &lt;- solve(chol(S)) %*% X 
#The inverse of the Choleski decomposition of S times the vertical vector X, 
#which in my case does nothing to the data
</code></pre>

<p>to</p>

<pre><code>X.transformed &lt;- chol(solve(S)) %*% X 
# which has negative values and gives meaningless values of X
</code></pre>

<p>Another method I've seen is transforming the dependent variable by </p>

<pre><code>chol(solve(S)) %*% Y 
</code></pre>

<p>and the independent variable by </p>

<pre><code>chol(solve(S)) %*% cbind(1,X) 
</code></pre>

<p>and doing the linear model using the transformed intercept terms in the first column of the X matrix: </p>

<pre><code>lm(Y ~ X - 1)
</code></pre>

<p>On a related note, is there any point to manually transforming the data in order to plot it? Do the transformed values have any meaning, or are they simply there to estimate regression coefficients? (In other words, if X is a variable of body mass figures, X values are not necessarily errant if they're negative since they're still linear?) I suppose it would follow from this that an $R^2$ statistic on transformed variables is also meaningless?</p>
"
"0.0400480865731637","0.039253433598943"," 20890","<p>Sorry to ask a stupid question, but I have problems using the package segmented.</p>

<p>My linear regression is very simple, between offer and demand:</p>

<blockquote>
  <p>linearModel &lt;- lm(demand~offer)</p>
</blockquote>

<p>And so should be my model using ""segmented"":</p>

<blockquote>
  <p>piecewiseModel &lt;- segmented(lm(demand~offer), seg.Z = ~ offer, psi = NA)</p>
</blockquote>

<p>But I have an error message and I really cannot find a reason for this:</p>

<pre><code>Error in seg.lm.fit(y, XREG, Z, PSI, weights, offs, opz) : 
  (Some) estimated psi out of its range
</code></pre>

<p>Here are my data:</p>

<pre><code>demand  offer
1155    39.3
362 23.5
357 22.4
111 6.1
703 35.9
494 35.5
410 23.2
63  9.1
616 27.5
468 28.6
973 41.3
235 16.9
180 18.2
69  9
305 28.6
106 12.7
155 11.8
422 27.9
44  21.6
1008    45.9
225 11.4
321 16.6
1001    40.7
531 22.4
143 17.4
251 14.3
216 14.6
57  6.6
146 10.6
226 14.3
169 3.4
32  5.1
75  4.1
102 4.1
4   1.7
68  7.5
102 7.8
462 22.6
295 8.6
196 7.7
50  7.8
739 34.7
287 15.6
226 18.5
706 35
127 16.5
85  11.3
234 7.7
153 14.8
4   2
373 12.4
54  9.2
81  11.8
18  3.9
</code></pre>
"
"0.0490486886395286","0.0480754414848157"," 20939","<p>I have a regression with a harmonic effect of day of the year, which interacts with other variables. I am not sure how to interpret the coefficients. My model is:</p>

<pre><code>m1 &lt;- lme(lcount ~ AirT + sin(2*pi/360*DOY) + cos(2*pi/360*DOY) + 
          AirT*sin(2*pi/360*DOY) + AirT*cos(2*pi/360*DOY) + RainAmt + RainAmt*AirT,
          random = ~1|plot))
</code></pre>

<p>I get significant interaction effects of air temperature with the linearized harmonic day of the year (DOY) function. My response variable is the log of animal counts on each day. I want to describe how the effect of air temperature on animal observations changes depending on the day of the year.</p>

<p>Does anyone have suggestions on how I can interpret my beta values and/or how I can visualize the effect? I am using R but am not that skilled. The package I used for analyzing my data is nlme.</p>

<p>EDIT: <strong>My primary goals are (1) to describe the response of animals to environmental variables and (2) to predict future activity periods (i.e. when and under what conditions should a research bother trying to catch these animals).</strong> So if there is a better way to model this data, I would be interested in hearing it (such as cubic splines - see comments below).</p>
"
"0.0424774103841449","0.0555127381653369"," 21043","<p>R linear regression seems to fail if my predictor variance is very small, but nonzero:</p>

<pre><code>&gt; reg = lm(V1~V2,data)
&gt; summary(reg)

Call:
lm(formula = V1 ~ V2, data = data)

Residuals:
Min      1Q  Median      3Q     Max
-15.968  -4.898   1.627   5.218   8.468

Coefficients: (1 not defined because of singularities)
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  11.7963     0.6036   19.54   &lt;2e-16 ***
V2                NA         NA      NA       NA
</code></pre>

<p>Here is an excerpt from my data... (it comes out this way after boxcox...)</p>

<pre><code>&gt; print(data$V2,digits=15)
 [1] -0.640196668095416 -0.640196668115515 -0.640196668075674 -0.640196668083867
 [5] -0.640196668103316 -0.640196668073982 -0.640196668094188 -0.640196668081038
etc
</code></pre>

<p>And here is the regression working if I manually remove the mean value:</p>

<pre><code>&gt; shifted = data$V2+0.640196668
&gt; shifted
 [1] -9.541601e-11 -1.155150e-10 -7.567402e-11 -8.386702e-11 -1.033160e-10
 [6] -7.398204e-11 -9.418799e-11 -8.103807e-11 -8.288503e-11 -9.411305e-11
etc

&gt; reg = lm(data$V1~shifted)
&gt; reg

Coefficients:
(Intercept)      shifted
  6.308e+00   -5.771e+10
</code></pre>

<p>Can anyone tell me if I'm using <code>lm</code> wrong?  Thank you...</p>
"
"0.02831827358943","0.0277563690826684"," 21137","<p>Considering a simple linear regression model Y= beta0+beta1 x X, with beta0 and beta1 computed, I have to estimate the expected X given a new Y and 95% confidence intervals. 
I used the formula X=(Y-beta0)/slope. How can I compute in R the 95% interval for the calculated value of ind, given a new value of the height?</p>

<blockquote>
  <p>head(df)</p>
</blockquote>

<pre><code>   ind height
1 4.27    174
2 3.60    159
3 3.61    175

summary(lm(df$ind~df$height))

Call:
lm(formula = df$ind ~ df$height)

Residuals:
 Min       1Q   Median       3Q      Max 

-0.56263 -0.27596  0.03866  0.26632  0.55440 

Coefficients:
         Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept) -0.968895   1.512371  -0.641  0.52903   

df$height    0.027871   0.008985   3.102  0.00562 **

Residual standard error: 0.3287 on 20 degrees of freedom
Multiple R-squared: 0.3248,     Adjusted R-squared: 0.2911 
F-statistic: 9.622 on 1 and 20 DF,  p-value: 0.005621 
</code></pre>

#

<p>I tried:</p>

<pre><code>pred.frame &lt;- data.frame(ind=seq(3,5,0.25)) 
predict(bclm,int=""c"",level=0.95,data=pred.frame)
    fit      lwr      upr
1  174.5780 169.3146 179.8414
2  166.7696 163.6419 169.8973
3  166.8862 163.7806 169.9917
...............
</code></pre>
"
"0.0400480865731637","0.039253433598943"," 21257","<p>I am performing simple (first-order terms) linear regression on data having several categorical variables (i.e. factors), and it is often desired that for each factor, one of the levels should add nothing to the regressand, and the other levels should add positive values to the regressand.  When I perform a regression analyses, however, I often get a lot of negative coefficients.</p>

<p>Is there a <strong>non-manual</strong> way of choosing which levels of a factor should be used as regressor variables in order to maximize the number of positive coefficients in the equation?  In other words, how can I get R to do this (somewhat tedious) task for me?</p>
"
"0.0200240432865818","0.0196267167994715"," 21447","<p>Simple question: How to specify a lognormal distribution in the GLM family argument in R?
I could not find how this can be achieved. Why is lognormal (or exponential) not an option in the family argument?</p>

<p>Somewhere in the R-Archives I read that one simply has to use the log-link for the family set to gaussian in the GLM, in order to specify a lognormal. However, this is nonsense because this will fit a non-linear regression and R starts asking for starting values.</p>

<p>Is anybody aware how to set a lognormal (or exponential) distribution for a GLM?</p>
"
"NaN","NaN"," 21506","<p>There are at least three R packages providing some functions to perform a Bayesian selection variable in linear Gaussian regression model: LearnBayes, mombf and BMA.</p>

<p>I would be glad to know some opinions about which one is the best.</p>
"
"0.0200240432865818","0.039253433598943"," 21565","<p>I see a similar constrained regression here:</p>

<p><a href=""http://stats.stackexchange.com/questions/12484/constrained-linear-regression-through-a-specified-point"">Constrained linear regression through a specified point</a></p>

<p>but my requirement is slightly different. I need the coefficients to add up to 1. Specifically I am regressing the returns of 1 foreign exchange series against 3 other foreign exchange series, so that investors may replace their exposure to that series with a combination of exposure to the other 3, but their cash outlay must not change, and preferably (but this is not mandatory), the coefficients should be positive. </p>

<p>I have tried to search for constrained regression in R and Google but with little luck. </p>
"
"0.02831827358943","0.0277563690826684"," 21596","<p>Sorry if this is a newb question; I'm trying to teach myself statistics for the first time.  I think I have the basic procedure down, but I'm struggling to execute it with R.</p>

<p>So, I'm trying to evaluate the significance of regression coefficients in a multiple linear regression of form</p>

<p>$$ \hat y = X \hat \beta$$</p>

<p>I think the t-statistic for testing $H_0: \hat \beta_j = 0, H_a: \hat \beta_j \neq 0$ is given by</p>

<p>$$t_0 = \frac{\hat \beta_j - 0}{\text{se}(\hat \beta_j)} = \frac{\hat \beta_j}{\sqrt{\hat \sigma^2 C_{jj}}} = \frac{\hat \beta_j}{\sqrt{C_{jj} SS_{Res}/(n-p)}}$$
where $C_{jj}$ is the $j^{th}$ diagonal entry in $(X&#39;X)^{-1}$.</p>

<p>So far, so good. I know how to calculate all of these values using matrix operations in R.  But in order to reject the null, the book says I need
$$|t_0| &gt; t_{\alpha/2,n-p}$$</p>

<p><strong>How can I compute this critical value $t_{\alpha/2,n-p}$ using R?</strong>  Right now the only way I know how to find these values is by looking in the table in the back of the book.  There must be a better way.</p>
"
"0.0633215847514023","0.0620651280774201"," 22118","<p>Quick question.</p>

<p>I want to perform a linear regression that looks like this:</p>

<pre><code>lm(y ~ x1 + x2 + x3 + x4 +x5, mydata)
</code></pre>

<p>This works fine if I manually write out this code.</p>

<p>However, the independent variables that I want to use are stored as a character, like this:</p>

<pre><code>&gt; vars
[1] ""x1 + x2 + x3 + x4 +x5""
</code></pre>

<p>I tried typing this:</p>

<pre><code>lm(y ~ vars, mydata)
Error in model.frame.default...
</code></pre>

<p>But it gives an error!</p>

<p>So then I tried this:</p>

<pre><code>lm(y ~ noquote(vars), mydata)
Error in model.frame.default...
</code></pre>

<p>And then this</p>

<pre><code>lm(y ~ print(vars, quote = FALSE), mydata)
Error in model.frame.default...
</code></pre>

<p>Anyone have a clue how I can get around this problem? The character string in ""vars"" is being provided to me by a program upstream, so I can't work around it at that level.</p>

<p>Thanks!</p>
"
"0.0578044339088637","0.0679889413649005"," 22392","<p>I am learning logistic regression modeling using the book ""Applied Logistic Regression"" by Hosmer.</p>

<p>In chpaters, he suggested using Fractional Polynomials for fitting continuous variable which does not seems to be related to logit in linear fashion. I tried the <code>mfp</code> package and can give exactly the same verbose as the book. </p>

<p>But I don't know how to write the transformed variable based on the output of fractional polynomials. The book only shows example of the transformed variable when $J=2$ with $p_1=0$ and $p_2=-0.5$ (page 101) and when $J=2$ with $p_1=2$ and $p_2=2$ (page 101), But what about the others? Currently my case is $J=2$ with $p_1=-1$ and $p_2=-1$.</p>

<p>I know little about fractional polynomials and the book seems not giving sufficient hits on this part. Can anyone refer me to some place which I can know how to write the polynomial? Thanks.</p>
"
"0.0400480865731637","0.039253433598943"," 22468","<p>Suppose we ran a simple linear regression $y=\beta_0+\beta_1x+u$, saved the residuals $\hat{u_i}$ and draw a histogram of distribution of residuals. If we get something which looks like a familiar distribution, can we assume that our error term has this distribution? Say, if we found out that residuals resemble normal distribution, does it make sense to assume normality of error term in population? I think it is sensible, but how can it be justified?</p>
"
"NaN","NaN"," 22800","<p>As an example, consider the <code>ChickWeight</code> data set in R. The variance obviously grows over time, so if I use a simple linear regression like:</p>

<pre><code>m &lt;- lm(weight ~ Time*Diet, data=ChickWeight)
</code></pre>

<p>My questions:  </p>

<ol>
<li>Which aspects of the model will be questionable? </li>
<li>Are the problems limited to extrapolating outside the <code>Time</code> range? </li>
<li>How tolerant is linear regression to violation of this assumption (i.e., how heteroscedastic does it have to be to cause problems)?</li>
</ol>
"
"0.0400480865731637","0.039253433598943"," 22902","<p>When comparing results obtained with different models in R, what should I look for to select the best one?</p>

<p>If I use for example the following 4 models applied to the same presence/absence sample taken from a species dataset, with the same variables:</p>

<ul>
<li><p>Generalized linear model</p></li>
<li><p>Generalized additive models Classification</p></li>
<li><p>Regression Tree</p></li>
<li><p>Artificial Neural Networks</p></li>
</ul>

<p>Should I compare all methods by AIC, Kappa, or cross-validation?</p>

<p>Will I ever be certain of selecting the best model?</p>

<p>What happens if I compare those 4 models prediction with a Bayes factor? Can I compare them?</p>
"
"0.105957277556576","0.10385482340819"," 23042","<p>Can someone explain my Cox model to me in plain English? </p>

<p>I fitted the following Cox regression model to <strong>all</strong> of my data using the <code>cph</code> function. My data are saved in an object called <code>Data</code>. The variables <code>w</code>, <code>x</code>, and <code>y</code> are continuous; <code>z</code> is a factor of two levels. Time is measured in months. Some of my patients are missing data for variable <code>z</code> (<em>NB</em>: I have duly noted Dr. Harrell's suggestion, below, that I impute these values so as to avoid biasing my model, and will do so in the future).</p>

<pre><code>&gt; fit &lt;- cph(formula = Surv(time, event) ~ w + x + y + z, data = Data, x = T, y = T, surv = T, time.inc = 12)

Cox Proportional Hazards Model
Frequencies of Missing Values Due to Each Variable
Surv(time, event)    w    x    y    z 
                0    0    0    0   14 

                Model Tests          Discrimination 
                                            Indexes        
Obs       152   LR chi2      8.33    R2       0.054    
Events     64   d.f.            4    g        0.437    
Center 0.7261   Pr(&gt; chi2) 0.0803    gr       1.548    
                Score chi2   8.07                      
                Pr(&gt; chi2) 0.0891                      

                   Coef    S.E.   Wald Z   Pr(&gt;|Z|)
         w      -0.0133  0.0503    -0.26     0.7914  
         x      -0.0388  0.0351    -1.11     0.2679  
         y      -0.0363  0.0491    -0.74     0.4600  
         z=1     0.3208  0.2540     1.26     0.2067
</code></pre>

<p>I also tried to test the assumption of proportional hazards by using the <code>cox.zph</code> command, below, but do not know how to interpret its results. Putting <code>plot()</code> around the command gives an error message.</p>

<pre><code> cox.zph(fit, transform=""km"", global=TRUE)
            rho chisq      p
 w      -0.1125 1.312 0.2520
 x       0.0402 0.179 0.6725
 y       0.2349 4.527 0.0334
 z=1     0.0906 0.512 0.4742
 GLOBAL      NA 5.558 0.2347
</code></pre>

<hr>

<h3>First Problem</h3>

<ul>
<li>Can someone explain the results of the above output to me in plain English? I have a medical background and no formal training in statistics.</li>
</ul>

<h3>Second Problem</h3>

<ul>
<li><p>As suggested by Dr. Harrell, I would like to internally validate my model by performing 100 iterations of 10-fold cross-validation using the <code>rms</code> package (from what I understand, this would entail building <code>100 * 10 = 1000</code> different models and then asking them to predict the survival times of patients that they had never seen).</p>

<p>I tried using the <code>validate</code> function, as shown.</p>

<pre><code>&gt; v1 &lt;- validate(fit, method=""crossvalidation"", B = 10, dxy=T)
&gt; v1
      index.orig training    test optimism index.corrected  n
Dxy      -0.2542  -0.2578 -0.1356  -0.1223         -0.1320 10
R2        0.0543   0.0565  0.1372  -0.0806          0.1350 10
Slope     1.0000   1.0000  0.9107   0.0893          0.9107 10
D         0.0122   0.0128  0.0404  -0.0276          0.0397 10
U        -0.0033  -0.0038  0.0873  -0.0911          0.0878 10
Q         0.0155   0.0166 -0.0470   0.0636         -0.0481 10
g         0.4369   0.4424  0.6754  -0.2331          0.6700 10
</code></pre>

<p>How do you perform the 100x resampling? I think my above code only performs the cross-validation once.</p></li>
<li><p>I then wanted to know how good my model was at prediction. I tried the following:</p>

<pre><code>&gt; c_index &lt;- abs(v1[1,5])/2 + 0.5
&gt; c_index
[1] 0.565984
</code></pre>

<p>Does this mean that my model is only very slightly better than flipping a coin?</p></li>
</ul>

<h3>Third Problem</h3>

<p>Dr. Harrell points out that I have assumed linearity for the covariate effects, and that the number of events in my sample is just barely large enough to fit a reliable model if all covariate effects happen to be linear.</p>

<ul>
<li>Does this mean that I should include some sort of interaction term in my model? If so, any advice as to what to put?</li>
</ul>
"
"0.0642198081225601","0.0734364498908627"," 23110","<p>I have a data set that's 200k rows X 50 columns.  I'm trying to use a <code>knn</code> model on it but there is huge variance in performance depending on which variables are used (i.e., <code>rsqd</code> ranges from .01 (using all variables) to .98 (using only 5 variables)).</p>

<p>This kind of compounds my problem as now I need to determine <code>k</code> <em>and</em> which variables to use.</p>

<p>Is there a package in R that helps with selecting variables for a <code>knn</code> model, while tuning <code>k</code>?  I've looked at <code>rfe()</code> in <code>caret</code> but it seems to only be built for linear regression, <code>randomforest</code>, naive bayes, etc but no <code>knn</code>.  </p>

<p>As an aside, I've tried manually building a loop to use the caret train function like this:</p>

<pre><code>for(i in 2:50){
knnFit &lt;- train(x[,i],y,...) ## trains model using single variable
}
</code></pre>

<p>My problem is that <code>knnFit$results</code> prints all of the results and <code>knnFit$bestTune</code> only prints the final parameter of <code>k</code>.  </p>

<pre><code>&gt; data1 &lt;- data.frame(col1=runif(20), col2=runif(20), col3=runif(20), col4=runif(20), col5=runif(20))
&gt; bootControl &lt;- trainControl(number = 1)
&gt; knnGrid &lt;- expand.grid(.k=c(2:5))
&gt; set.seed(2)
&gt; knnFit1 &lt;- train(data1[,-c(1)], data1[,1]
+ , method = ""knn"", trControl = bootControl, verbose = FALSE,
+ tuneGrid = knnGrid )
&gt; knnFit1 
20 samples
 4 predictors

No pre-processing
Resampling: Bootstrap (1 reps) 

Summary of sample sizes: 20 

Resampling results across tuning parameters:

  k  RMSE   Rsquared
  2  0.485  0.124   
  3  0.54   0.369   
  4  0.52   0.241   
  5  0.528  0.232   

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was k = 2. 

&gt; knnFit1$results
      k      RMSE  Rsquared RMSESD RsquaredSD
    1 2 0.4845428 0.1241031     NA         NA
    2 3 0.5401009 0.3690569     NA         NA
    3 4 0.5197262 0.2410814     NA         NA
    4 5 0.5277939 0.2317607     NA         NA

&gt; knnFit1$bestTune
      .k
    1  2
</code></pre>

<p>I need some way to print the RMSE/rsqd/other metric for the best single performing model (i.e., just ""R-Squared: .91"").</p>

<p>Any suggestions?</p>
"
"0.109676202005208","0.107499955208361"," 23795","<p>I am using a relevance vector machine as implemented in the kernlab-package in R, trained on a dataset with 360 continuous variables (features) and 60 examples (also continuous, so it's a relevance vector regression).</p>

<p>I have several datasets with equivalent dimensions from different subjects. Now it works fine for most of the subjects, but with one particular dataset, I get this strange results:</p>

<p>When using leave-one-out cross validation (so I train the RVM and try to subsequently predict one observation that was left out of the training), most of the predicted values are just around the mean of the example-values.
So I really don't get good predictions, but just a slightly different value than the mean.</p>

<p>It seems like the SVM is not working at all;
When I plot the fitted values against the actual values, I see the same pattern; predictions around the mean. So the RVM is not even able to predict the values it was trained on (for the other datasets I get correlations of around .9 between fitted and actual values).</p>

<p>It seems like, that I can at least improve the fitting (so that the RVM is at least able to predict the values it was trained on) by transforming the dependent variable (the example-values), for example by taking the square root of the dependent variable.</p>

<p>so this is the output for the untransformed dependent variable:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 5 
Variance :  1407.006
Training error : 1383.534902093 
</code></pre>

<p>this, if I first transform the dependent variable by taking the square root:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 55 
Variance :  1.711355
Training error : 0.89601609 
</code></pre>

<p>How is it, that the RVM-results change so dramatically, just by transforming the dependent variable? And what is going wrong, when an SVM just predicts values around the mean of the dependent variable (even for the values and observations it was trained on)?</p>
"
"0.0633215847514023","0.0620651280774201"," 24187","<p>I have a dataset containing multiple proportions that add up to 1.
I am interested in the change of these proportions along a gradient (see below for example data).</p>

<pre><code>gradient &lt;- 1:99
A1 &lt;- gradient * 0.005
A2 &lt;- gradient * 0.004
A3 &lt;- 1 - (A1 + A2)

df &lt;- data.frame(gradient = gradient,
                 A1 = A1,
                 A2 = A2,
                 A3 = A3)

require(ggplot2)
require(reshape2)
dfm &lt;- melt(df, id = ""gradient"")
ggplot(dfm, aes(x = gradient, y = value, fill = variable)) +
  geom_area()
</code></pre>

<p><img src=""http://i.stack.imgur.com/aBbHc.png"" alt=""enter image description here""></p>

<p><strong>Additional information:</strong>
It need not be necessarily linear, I did this just for easiness of the example.
The original counts from which these proportions are calculated are also available. 
The real dataset contains more variable adding up to 1 (eg. B1, B2 &amp; B3, C1 to C4, etc) - so a hint for a multivariate solution is would be also helpful... But for now I'll stick on the univariate side of statistics.</p>

<p><strong>Question:</strong> 
How can one analyze such kind of data? 
IÂ´ve read a little bit around, and perhaps a multinomial model or a glm is suited? - If I run 3 (or 2) glms, how can I incorporate the constraint that the predicted values sum up to 1?
I dont want to only plot such kind of data, I also want to do a deeper regression like analysis.
I preferably want to use R - how can i do this in R?</p>
"
"0.0749231094763201","0.0734364498908627"," 24251","<p>Complex survey data is that typically found produced by the National Center for Health Statistics (NCHS) or the NSLY; it typically contains information on PSU, strata, and weights. To make nationally representative samples, one would traditionally perform a weighted regression that accounts for the sampling design by Taylor linearization (i.e. the survey analog to Huber-White errors). </p>

<p>I'm interested in matched analyses (e.g., King's <a href=""http://gking.harvard.edu/matchit"" rel=""nofollow"">MatchIt program</a>) as a manner in which to improve causal inference. What remains unclear from a first look is: (1) what criteria should be used to determine when matched analyses are appropriate with complex survey data; and (2) how such matched analyses ought to account for weights and/or survey sampling. </p>

<p>My understanding of (1) is that there is nothing different about these analyses than any other, but that it might/must improve inference and efficiency when the number of matched cases is small. As regards (2), my understanding is that common recommendations suggest including weights, and not the sampling design, in the matching (e.g., a weighted logistic regression to develop propensity scores) and not the later causal inference. </p>

<p>Should the sampling structure (e.g., PSU, strata) not be taken into account? Any references, suggestions, confirmations, or contradictions of what is above would be welcomed. </p>
"
"0.09392108820677","0.0920574617898323"," 24365","<p>I am using the mlogit package in R to run a multinomial logistic regression on pooled discrete choice data collected using two different questionnaire formats. I want to test whether the format had a significant effect on choices. When I run the basic model I get a result. But when I run the same model with a dummy variable indicating which format the respondents saw, I get an error: ""Error in solve.default(H, g[!fixed]) : Lapack routine dgesv: system is exactly singular""</p>

<p>I was able to replicate the error using Train's Electricity dataset in the mlogit package, setting a dummy based on whether the respondent ID was odd or even:</p>

<pre><code>library(mlogit)
data(""Electricity"", package = ""mlogit"")
Electr &lt;- mlogit.data(Electricity, id = ""id"", choice = ""choice"", 
                      varying = 3:26, shape = ""wide"", sep = """")
Electr$odd.dummy &lt;- ifelse(Electr$id %% 2 == 0, 0, 1) # As example, set dummy if ID is odd
summary(mlogit(choice ~ pf + cl + loc + wk + tod + seas | 0, data=Electr)) # Basic model
summary(mlogit(choice ~ pf + cl + loc + wk + tod + seas + odd.dummy | 0, data=Electr)) # Basic + dummy
summary(mlogit(choice ~ odd.dummy | 0, data=Electr)) # Only dummy
</code></pre>

<p>As with my data, the first model runs, but the second two are singular.</p>

<p>I understand that a result will be singular if there is perfect colinearity between variables, but I don't see how this is the case here.  Respondents were randomly assigned to one format or the other, and the underlying experimental design was the same in both formats, so there shouldn't be any colinearity between the dummy and the other variables.</p>

<p>I would be grateful if someone could explain why adding the dummy leads to a singular result, and even more grateful if they could suggest a solution to avoid it.</p>
"
"0.02831827358943","0.0277563690826684"," 24445","<p>I'm trying to estimate a multiple linear regression in R with an equation like this:</p>

<pre><code>regr &lt;- lm(rate ~ constant + askings + questions + 0)
</code></pre>

<p>askings and questions are quarterly data time-series, constructed with <code>askings &lt;- ts(...)</code>.</p>

<p>The problem now is that I got autocorrelated residuals. I know that it is possible to fit the regression using the gls function, but I don't know how to identify the correct AR or ARMA error structure which I have to implement in the gls function. </p>

<p>I would try to estimate again now with,</p>

<pre><code>gls(rate ~ constant + askings + questions + 0, correlation=corARMA(p=?,q=?))
</code></pre>

<p>but I'm unfortunately neither an R expert nor an statistical expert in general to identify p and q.</p>

<p>I would be pleased If someone could give me a useful hint.
Thank you very much in advance!</p>

<p>Jo</p>
"
"0.116759233144359","0.107710533187266"," 24452","<p>I hope you all don't mind this question, but I need help interpreting output for a linear mixed effects model output I've been trying to learn to do in R. I am new to longitudinal data analysis and linear mixed effects regression. I have a model I fitted with weeks as the time predictor, and score on an employment course as my outcome. I modeled score with weeks (time) and several fixed effects, sex and race. My model includes random effects. I need help understanding what the variance and correlation means. The output is the following:</p>

<pre><code>Random effects  
Group   Name    Variance  
EmpId intercept 680.236  
weeks           13.562  
Residual 774.256  
</code></pre>

<p>The correlaton is .231.</p>

<p>I can interpret the correlation as there is a a positive relationship between weeks and score but I want to be able to say it in terms of ""23% of ..."".</p>

<p>I really appreciate the help. </p>

<hr>

<p>Thanks ""guest"" and Macro for replying. Sorry, for not replying, I was out at a conference and Iâ€™m now catching up. 
Here is the output and the context. </p>

<p>Here is the summary for the LMER model I ran. </p>

<pre><code>&gt;summary(LMER.EduA)  
Linear mixed model fit by maximum likelihood  
Formula: Score ~ Weeks + (1 + Weeks | EmpID)   
   Data: emp.LMER4 

  AIC     BIC   logLik   deviance   REMLdev   
 1815     1834  -732.6     1693    1685

Random effects:    
 Groups   Name       Variance Std.Dev. Corr  
 EmpID   (Intercept)  680.236  26.08133        
          Weeks         13.562 3.682662  0.231   
 Residual             774.256  27.82546        
Number of obs: 174, groups: EmpID, 18


Fixed effects:    
            Estimate Std. Error  t value  
(Intercept)  261.171      6.23     37.25    
Weeks          11.151      1.780    6.93

Correlation of Fixed Effects:  
     (Intr)  
Days -0.101
</code></pre>

<p>I donâ€™t understand how to interpret the variance and residual for the random effects and explain it to someone else. I also donâ€™t know how to interpret the correlation, other than it is positive which indicates that those with higher intercepts have higher slopes and those with those with lower intercepts have lower slopes but I donâ€™t know how to explain the correlation in terms of 23% of . . . . (I donâ€™t know how to finish the sentence or even if it makes sense to do so). This is a different type analysis for us as we (me) are trying to move into longitudinal analyses. </p>

<p>I hope this helps.</p>

<p>Thanks for your help so far. </p>

<p>Zeda</p>
"
"0.0200240432865818","0.0196267167994715"," 24572","<p>In my previous <a href=""http://stats.stackexchange.com/questions/24380/how-to-get-ellipse-region-from-bivariate-normal-distributed-data"">question</a> I needed to help with ellipse region extraction and determine if point lies in that region or not.
I ended up with this code:</p>

<pre><code>library(ellipse)
library(mvtnorm)
require(spatstat)

netflow &lt;- read.csv(file=""data.csv"",head=FALSE,sep="" "")
#add headers
names(netflow)&lt;-c('timestamps','flows','flows_tcp','flows_udp','flows_icmp','flows_other','packe ts','packets_tcp','packets_udp','packets_icmp','packets_other','octets','octets_tcp','octets_udp','octets_icmp','octets_other')
attach(netflow)

#load library
library(sfsmisc)
#plot
plot(packets,flows,type='p',xlim=c(0,500000),ylim=c(0,50000),main=""Dependence number of flows on number of packets"",xlab=""packets"",ylab=""flows"",pch = 16, cex = .3,col=""#0000ff22"",xaxt=""n"")
#Complete the x axis
eaxis(1, padj=-0.5, cex.axis=0.8)

pktsFlows=subset(na.omit(netflow),select=c(packets,flows))
head(pktsFlows)
#plot(pktsFlows,pch = 16, cex = .3,col=""#0000ff22"")

cPktsFlows &lt;- apply(pktsFlows, 2, mean)
elpPktsFlows=ellipse::ellipse(var(pktsFlows),centre=cPktsFlows,level=0.8)

png(file=""graph.png"")
plot(elpPktsFlows,type='l',xlim=c(0,500000), ylim=c(0,50000))
points(pktsFlows,pch = 19, cex = 0.5,col=""#0000FF82"")
grid(ny=10,nx=10)
dev.off()

W &lt;- owin(poly=elpPktsFlows)
inside.owin(100000,18000,W)
</code></pre>

<p>This produces this <a href=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/ellipse.png"" rel=""nofollow"">graph</a>.</p>

<p><img src=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/ellipse.png"" alt=""graph ellipse""></p>

<p>Here is the same data with the <a href=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/linRegAll.png"" rel=""nofollow"">regression line plotted</a></p>

<p><img src=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/linRegAll.png"" alt=""Plot all with linear regression line"">.</p>

<p>Can you explain me, why the ellipse has this shape? I expected that main axe of ellipse will have the same direction with linear regression line, but it hasn't.</p>

<p>Btw. <a href=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/kernel/kernelPoints.png"" rel=""nofollow"">kernel density estimation</a> also points to 100000 althought there are no points...</p>

<p><img src=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/kernel/kernelPoints.png"" alt=""kernel density estimation""></p>
"
"0.0490486886395286","0.0480754414848157"," 24840","<p>I need to determine if there is any relationship between two count variables. 
I have 60+ observations for 4 variables and I want to see if any of the pairs of these variables are significantly correlated with one another. </p>

<p>Mostly I use R, so forgive me if you're not familiar.</p>

<p>I have been using the <code>cor(...,method=""pearson"")</code> and <code>cor.test()</code> functions to test each pair, but now I'm not so sure that this is the right approach/test. 
Would a non-linear regression like <code>glm(...,family=""poisson"")</code> be more appropriate?</p>

<p>I started thinking like this because when I looked at a histogram of the counts across my observations, I noticed that there seemed to be a slight tendency for the pink and green variables to go up and down to together.</p>

<p>I produced a scatter plot of each of the variables plotted against each of the other variables. I used the tests mention above to try and quantify this relationship and to test weather it was real or just noise. </p>

<p><img src=""http://i.stack.imgur.com/rNFTE.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/iZ3hW.png"" alt=""enter image description here""></p>
"
"0.0800961731463273","0.078506867197886"," 24948","<p>I have a reasonable understanding of why multicollinearity is a problem is regression models, along the <a href=""http://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r"">lines</a> of this excellent post.</p>

<p>To summarise my understanding, for a regression model of $y = \alpha + \beta_1x + \beta_2z$ (where $x$ and $z$ are correlated), beta coefficient estimates (as well as being unstable) are difficult to interpret, as a situation where you might increase $z$ without increasing $x$ is unlikely to occur, and not supported by the data.</p>

<p>I understand multicollinearity is less harmful to purely <em>predictive</em> as opposed to explanatory or descriptive models.</p>

<p>I'm interested in another interpretation:</p>

<p><em>If I decided to increase $z$, and let $x$ vary as it pleases in reaction, what would I see happen to $y$, accounting for the fact that $x$ is likely to move with $z$, and also have it's own effect?</em></p>

<p>In other words, accepting the causal interpretation that $x$ and $z$ both cause $y$, and are themselves correlated to some extent (.7 say), how would all three variables move if $z$ is (linearly) increased by some amount?</p>

<p>I've tried to model this sort of thing before, fitting $y = \alpha + \beta_1x + \beta_2z$ (model 1), and $x = \alpha + \beta_1z$ (model 2). Hypothetical increased $z$ values are produced, and resulting $x$ values are predicted with model 2. The hypothetical $x$ and $z$ values are used to predict $y$ using model 1. However this feels very unsatisfactory, complicated simulations are required to capture uncertainty (I used <code>sim</code> in <code>arm</code>). Additionally, my gut tells me that apart from being painfully inelegant, it's a bad idea for other reasons I can't put my finger on.</p>

<ul>
<li>Is such an 'observational'/conditional-when-I-feel-like-it interpretation possible?</li>
<li>Does anyone know of a better method for this interpretation?</li>
<li>Can anyone recommend a paper or <code>R</code> package along these lines?</li>
<li>Is the above multi-model mess at-all valid?</li>
</ul>

<p>I'm aware that a model along the lines of $y = \alpha + \beta_1z$ would yield a similar answer to the two-stage mess above, but would lose information in $x$.</p>

<p>I understand that these ideas are similar to structural equation modelling, but apart from having scant knowledge of SEM, I'm yet to find an <code>R</code> package which allows flexibly extending these models with different link functions for proportional odds models, etc.</p>
"
"NaN","NaN"," 25068","<p>I am trying to fit data with a GLM (poisson regression) in R.  When I plotted the residuals vs the fitted values, the plot created multiple (almost linear with a slight concave curve) ""lines"".  What does this mean? </p>

<pre><code>library(faraway)
modl &lt;- glm(doctorco ~ sex + age + agesq + income + levyplus + freepoor + 
            freerepa + illness + actdays + hscore + chcond1 + chcond2,
            family=poisson, data=dvisits)
plot(modl)
</code></pre>

<p><img src=""http://i.stack.imgur.com/7gwmX.png"" alt=""enter image description here""></p>
"
"NaN","NaN"," 25316","<p>How would we measure the predictive power of predictors in time series models. For e.g. in linear regression we have the magnitude and direction of the regression co-efficients and their p-values.</p>

<p>Is there any measure like that to evaluate the performance of predictors in kalman filter?</p>
"
"0.0755153962384799","0.0740169842204492"," 25538","<p>I am looking into time series data compression at the moment.</p>

<p>The idea is to fit a curve on a time series of n points so that the maximum deviation of any of the points is not greater than a given threshold. In other words, none of the values that the curve takes at the points where the time series is defined should be ""further away"" than a certain threshold from the actual values.</p>

<p>Till now I have found out how to do nonlinear regression using the least squares estimation method in R (<code>nls</code> function) and other languages, but I haven't found any packages that implement nonlinear regression with the L-infinity norm.</p>

<p>I have found papers on <a href=""http://www.jstor.org/discover/10.2307/2006101?uid=3737864&amp;uid=2&amp;uid=4&amp;sid=21100693651721"">""Non-linear curve fitting in the $L_1$ and $L_{\infty}$ norms""</a>, by Shrager and Hill and <a href=""http://www.dtic.mil/dtic/tr/fulltext/u2/a080454.pdf"">""A linear  programming algorithm  for curve fitting in the $L_{\infty}$ norm""</a>, by Armstrong and Sklar.</p>

<p>I could try to implement this in R for instance, but I first looking to see if this hasn't already been done and that I could maybe reuse it.</p>

<p>I have found a solution that I don't believe to be ""very scientific"": I use nonlinear least squares regression to find the starting values of the parameters which I subsequently use as starting points in the R <code>optim</code> function that minimizes the maximum deviation of the curve from the actual points.</p>

<p>The idea is to be able to find out if this type of curve-fitting is possible on a given time series sequence and to determine the parameters that allow it.</p>
"
"0.0633215847514023","0.0620651280774201"," 25611","<p>I have a dataset with 9 continuous independent variables. I'm trying to select amongst these variables to fit a model to a single percentage (dependent) variable, <code>Score</code>. Unfortunately, I know there will be serious collinearity between several of the variables.</p>

<p>I've tried using the <code>stepAIC()</code> function in R for variable selection, but that method, oddly, seems sensitive to the order in which the variables are listed in the equation...</p>

<p>Here's my R code (because it's percentage data, I use a logit transformation for Score):</p>

<pre><code>library(MASS)
library(car)

data.tst = read.table(""data.txt"",header=T)
data.lm = lm(logit(Score) ~ Var1 + Var2 + Var3 + Var4 + Var5 + Var6 + Var7 +
             Var8 + Var9, data = data.tst)

step = stepAIC(data.lm, direction=""both"")
summary(step)
</code></pre>

<p>For some reason, I found that the variables listed at the beginning of the equation end up being selected by the <code>stepAIC()</code> function, and the outcome can be manipulated by listing, e.g., <code>Var9</code> first (following the tilde).</p>

<p>What is a more effective (and less controversial) way of fitting a model here? I'm not actually dead-set on using linear regression: the only thing I want is to be able to understand which of the 9 variables is truly driving the variation in the <code>Score</code> variable. Preferably, this would be some method that takes the strong potential for collinearity in these 9 variables into account.</p>
"
"0.0749231094763201","0.0734364498908627"," 25702","<p>I have spent much time looking for a special package that could run the Pesaran(2007) unit root test (which assumes cross-sectional dependence unlike most others) and I have found none. So, I decided to do it manually; however, I don't know where I'm going wrong, because my results are very different from Microsoft Excel's results (in which it is done very easily).</p>

<p>My data frame is made up of 22 countries with 506 observations of daily price indices. Following is the model to run using the Pesaran(2007) unit root test:</p>

<p>(i) With an intercept only</p>

<p>$$\Delta Y_{i,t} = a_i + b_iY_{i,t-1} + c_i\overline{Y}_{t-1} + d_i\Delta\overline{Y}_{t-1}+ e_i\Delta\overline{Y}_{t-2}+ f_i\Delta\overline{Y}_{i,t-1}+ g_i\Delta\overline{Y}_{i,t-2} + \varepsilon_{i,t}$$</p>

<p>where $\overline{Y}$ is the cross-section average of the observations across countries at each time $t$ and $b$ is the coefficient of interest to us because it will allow us to compute the ADF test statistic and then determine whether the process is stationary or not.</p>

<p>I constructed each of these variables in the following way:</p>

<p>$\Delta Y_t$</p>

<pre><code>dif.yt = diff(yt) 
## yt is the object containing all the observations for a specific country 
## (e.g. Australia)
</code></pre>

<p>$Y_{t-1}$</p>

<pre><code>yt.lag.1 = lag(yt, -1)
</code></pre>

<p>$\overline{Y}_{t-1}$</p>

<pre><code>ybar.lag.1 = lag(c(rowMeans(x)), -1) 
## x is the object containing my entire data frame
</code></pre>

<p>$\Delta \overline{Y}_{t-1}$</p>

<pre><code>dif.ybar.lag.1 = diff(ybar.lag.1)
</code></pre>

<p>$\Delta \overline{Y}_{t-2}$</p>

<pre><code>dif.ybar.lag.2 = diff(lag(c(rowMeans(x)), -2))
</code></pre>

<p>$\Delta Y_{t-1}$</p>

<pre><code>dif.yt.lag.1 = diff(yt.lag.1)
</code></pre>

<p>$\Delta Y_{t-2}$</p>

<pre><code>dif.yt.lag.2 = diff(lag(yt, -2)
</code></pre>

<p>After constructing each variable individually, I then run the linear regression</p>

<pre><code>reg = lm(dif.yt ~ yt.lag.1[-1] + ybar.lag.1[-1] + dif.ybar.lag.1 + 
                  dif.ybar.lag.2 + dif.yt.lag.1 + dif.yt.lag.2)
summary(reg)
</code></pre>

<p>It is obvious that the explanatory variables in my regression equation differ in length, so I'd like to know whether there is a way in R to make all the variables of equal length (perhaps with a function).</p>

<p>Also, I'd like to know whether the procedure I used was correct and if there are more optimal ways.</p>
"
"0.0506572678011219","0.0496521024619361"," 25839","<p>First off, I'll say I am a biologist and new to the statistics side of things so excuse my ignorance</p>

<p>I have a data set that consists of a binary outcome and then a bunch of trinary explanatory variables that looks something like this:</p>

<pre><code>head()
 Category block21_hap1 block21_hap2 block21_hap3 block21_check
1        1            1            1            0             2
2        1            2            0            0             2
3        1            1            0            1             2
4        1            1            0            1             2
5        1            1            1            0             2
6        1            1            1            0             2
</code></pre>

<p>A quick summary of the data</p>

<pre><code>summary()
Category block21_hap1 block21_hap2 block21_hap3 block21_check
 1:718    0:293        0:777        0:1026       2:1467       
 0:749    1:709        1:577        1: 390                    
          2:465        2:113        2:  51  
</code></pre>

<p>and another summary grouped by outcome levels</p>

<pre><code>by(hap.ped.final, hap.ped.final$Category, summary)
hap.ped.final$Category: 1
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:146        0:374        0:518        2:718        
 1:336        1:286        1:174                     
 2:236        2: 58        2: 26                     
---------------------------------------------------------------------------- 
hap.ped.final$Category: 0
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:147        0:403        0:508        2:749        
 1:373        1:291        1:216                     
 2:229        2: 55        2: 25          
</code></pre>

<p>So I am trying to run logistic regression on this data. When I do this:</p>

<pre><code>fit = glm(Category~ block21_hap1 + block21_hap2 + block21_hap3, data = hap.ped.final ,family = ""binomial"")
summary(fit)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.301  -1.177   1.059   1.177   1.200  

Coefficients: (1 not defined because of singularities)
                             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)                 -0.039221   0.280110  -0.140    0.889
hap.ped.final$block21_hap11  0.123555   0.183087   0.675    0.500
hap.ped.final$block21_hap12  0.009111   0.295069   0.031    0.975
hap.ped.final$block21_hap21 -0.084334   0.183087  -0.461    0.645
hap.ped.final$block21_hap22 -0.013889   0.337468  -0.041    0.967
hap.ped.final$block21_hap31  0.201113   0.183087   1.098    0.272
hap.ped.final$block21_hap32        NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2033  on 1466  degrees of freedom
Residual deviance: 2028  on 1461  degrees of freedom
AIC: 2040

Number of Fisher Scoring iterations: 3
</code></pre>

<p>So I don't really know what a singularity is or what's going wrong here that is throwing up NA's as a result of my analysis. Is it my data, or what I'm doing to it.
I tried googling the warning (or whatever you might call it) and I got some pages talking about collinearity and multilinearity, which I do not understand at all. 
Again, sorry for lack of knowledge here. I wish I had done more maths in undergrad. </p>
"
"0.0400480865731637","0.039253433598943"," 25949","<p>I'm going through the videos in Andrew Ng's free <a href=""http://www.ml-class.org/"">online machine learning course</a> at Stanford.  He discusses Gradient Descent as an algorithm to solve linear regression and writing functions in Octave to perform it.  Presumably I could rewrite those functions in R, but my question is doesn't the lm() function already give me the output of linear regression?  Why would I want to write my own gradient descent function?  Is there some advantage or is it purely as a learning exercise?  Does lm() do gradient descent?</p>
"
"0.106193525960362","0.104086384060007"," 25988","<p>An assumption of the ordinal logistic regression is the proportional odds assumption. Using R and the 2 packages mentioned I have 2 ways to check that but I have questions in each one.</p>

<p>1) Using the rms package</p>

<p>Given the next commands</p>

<pre><code>library(rms)
ddist &lt;- datadist(Ki67,Cyclin_E)
options(datadist='ddist')
f &lt;- lrm(grade ~Ki67+Cyclin_E);f
sf &lt;- function(y)
c('Y&gt;=1'=qlogis(mean(y &gt;= 1)),'Y&gt;=2'=qlogis(mean(y &gt;= 2)),'Y&gt;=3'=qlogis(mean(y &gt;= 3)))
s &lt;- summary(grade ~Ki67+Cyclin_E, fun=sf)
plot(s,which=1:3,pch=1:3,xlab='logit',main='',xlim=c(-2.5,2.5))
</code></pre>

<p>I have</p>

<pre><code>lrm(formula = grade ~ Ki67 + Cyclin_E)

Frequencies of Missing Values Due to Each Variable
   grade     Ki67 Cyclin_E 
       0        0        3 


                     Model Likelihood     Discrimination    Rank Discrim.    
                        Ratio Test            Indexes          Indexes       

Obs            42    LR chi2     11.38    R2       0.268    C       0.728    
 1             11    d.f.            2    g        1.279    Dxy     0.456    
 2             15    Pr(&gt; chi2) 0.0034    gr       3.592    gamma   0.458    
 3             16                         gp       0.192    tau-a   0.308    
max |deriv| 1e-07                         Brier    0.166                     


         Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=2     -0.1895 0.8427 -0.22  0.8221  
y&gt;=3     -2.0690 0.9109 -2.27  0.0231  
Ki67      0.0971 0.0330  2.94  0.0033  
Cyclin_E -0.0076 0.0227 -0.33  0.7387 
</code></pre>

<p>The <code>s</code> table gives: (unfortunately I don't know how to upload a graph made in R)</p>

<pre><code>grade    N=45

+--------+-------+--+----+---------+----------+
|        |       |N |Y&gt;=1|Y&gt;=2     |Y&gt;=3      |
+--------+-------+--+----+---------+----------+
|Ki67    |[ 2, 9)|12|Inf |0.6931472|-1.0986123|
|        |[ 9,16)|12|Inf |0.3364722|-2.3978953|
|        |[16,24)|10|Inf |2.1972246| 0.0000000|
|        |[24,44]|11|Inf |2.3025851| 1.5040774|
+--------+-------+--+----+---------+----------+
|Cyclin_E|[ 3,16)|15|Inf |1.0116009|-0.1335314|
|        |[16,22)| 7|Inf |1.7917595|-0.9162907|
|        |[22,33)|10|Inf |1.3862944|-0.8472979|
|        |[33,80]|10|Inf |0.4054651|-0.4054651|
|        |Missing| 3|Inf |      Inf| 0.6931472|
+--------+-------+--+----+---------+----------+
|Overall |       |45|Inf |1.1284653|-0.4054651|
+--------+-------+--+----+---------+----------+
</code></pre>

<p>Where for the Ki67 I see that 3 out of the 4 differences  <code>logit(P[Y&gt; = 2])-logit(P[Y&gt; = 3])</code> are close to 2. Only the last one is quite lower (around 0.8). But here Ki67 is continuous and not categorical so I don't know if the results of the table are correct and there isn't any p-value to decide. By the way I run the above in SPSS and I didn't reject the assumption.</p>

<p>2) Using the VGAM package</p>

<p>Here using the next commands I have the model under the assumption of proportional odds</p>

<pre><code>library(VGAM)
fit1 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=T))
summary(fit1)
</code></pre>

<p>And the results</p>

<pre><code>Coefficients:
                Estimate Std. Error  z value
(Intercept):1  0.1894723   0.820442  0.23094
(Intercept):2  2.0690395   0.886732  2.33333
Ki67          -0.0970972   0.032423 -2.99467
Cyclin_E       0.0075887   0.021521  0.35261

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 79.86801 on 80 degrees of freedom

Log-likelihood: -39.93401 on 80 degrees of freedom

Number of iterations: 5 
</code></pre>

<p>While using the next commands I have the model without the assumption of proportional odds</p>

<pre><code>fit2 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=F))
</code></pre>

<p>where unfortunately i receice the next message </p>

<blockquote>
  <p>Warning message: In vglm.fitter(x = x, y = y, w = w, offset = offset,
  Xm2 = Xm2,  :   convergence not obtained in 30 iterations</p>
</blockquote>

<p>However if I type <code>summary(fit2)</code> I get results but again I don't know if they are correct. My intention was to use the next commands and get the answer but know I doubt if this is correct (by the way if I do it I get <code>p-value=0.6</code>. </p>

<pre><code>pchisq(deviance(fit1)-deviance(fit2),
df=df.residual(fit1)-df.residual(fit2),lower.tail=FALSE)
</code></pre>

<p>So, regarding the methods mentioned above does anyone knows whether the results I get are valid or in the case of the VGAM package is there any way to increase the number of itterations?Is there any other way to check it? </p>
"
"0.0908205236199223","0.0816002183921495"," 26180","<p>I run an ordinal regression model and I wanted to check the proportional odds assumption. In order to do that I used the <code>VGAM</code> package and I run olr twice, the first under the assumption and the second without the assumption. Below is the code and the results</p>

<pre><code>&gt; fit1 &lt;- vglm(stage ~Ki67+Cyclin_E,family=cumulative(parallel=T))
&gt; summary(fit1)

Call:
vglm(formula = stage ~ Ki67 + Cyclin_E, family = cumulative(parallel = T))

Pearson Residuals:
                     Min       1Q  Median      3Q    Max
logit(P[Y&lt; = 1]) -3.1177 -0.43593 0.37246 0.53111 1.4927
logit(P[Y&lt; = 2]) -3.8479  0.14119 0.18785 0.28679 1.9903

Coefficients:
                Estimate Std. Error  z value
(Intercept):1  2.2414705   1.091225  2.05409
(Intercept):2  3.2164214   1.178916  2.72829
Ki67          -0.1157273   0.039889 -2.90124
Cyclin_E       0.0085266   0.028626  0.29786

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 50.82946 on 62 degrees of freedom

Log-likelihood: -25.41473 on 62 degrees of freedom

Number of iterations: 5 


&gt; fit2 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=F),maxit=50)
&gt; summary(fit2)

Call:
vglm(formula = grade ~ Ki67 + Cyclin_E, family = cumulative(parallel = F), 
    maxit = 50)

Pearson Residuals:
                     Min       1Q   Median      3Q    Max
logit(P[Y&lt; = 1]) -1.1870 -0.65271 -0.23199 0.44910 3.4798
logit(P[Y&lt; = 2]) -2.6235 -0.70599  0.27305 0.72691 2.8544

Coefficients:
               Estimate Std. Error   z value
(Intercept):1 -0.059702   0.928078 -0.064328
(Intercept):2  2.687277   1.050909  2.557097
Ki67:1        -0.100832   0.047754 -2.111483
Ki67:2        -0.101817   0.036567 -2.784377
Cyclin_E:1     0.018768   0.022708  0.826474
Cyclin_E:2    -0.015416   0.022927 -0.672390

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 78.93318 on 78 degrees of freedom

Log-likelihood: -39.46659 on 78 degrees of freedom

Number of iterations: 34 
</code></pre>

<p>In order to check if the difference of the two models is significant I run the next command</p>

<pre><code>pchisq(deviance(fit2)-deviance(fit1),df=df.residual(fit2)-df.residual(fit1),lower.tail=FALSE)
[1] 0.03072927
</code></pre>

<p>As you see the result is that the 2 models differ and so the proportional odds assumption isn't true. But if you see the coefficients about the Ki67 (Cyclin is not significantly important so i guess i can skip it) they are almost the same. In that case what should I do? I believe that I could stick with the model under the po assumption but I'd like to know what others think</p>
"
"0.0400480865731637","0.039253433598943"," 26234","<p>(this question addresses an expanded case of <a href=""http://stats.stackexchange.com/questions/21257/how-can-factor-levels-be-automatically-chosen-in-r-to-maximize-the-number-of-pos"">How can factor-levels be automatically chosen in R to maximize the number of positive coefficients in a regression model?</a>)</p>

<p>I am performing linear regression (using R) on data having both categorical (factor) and numeric variables, and fitting to a model having the form <code>y ~ (.)^2</code> (i.e. including all first order and second-order interaction terms).</p>

<p>The question is:  is there is a <strong>programmatic</strong> way of determining a coefficient vector among the set of optimal vectors $\Theta_{opt}$, where the length of the vector is minimized under the constraint that all elements of the vector are positive.</p>

<p>There may be cases where it is impossible to find an all-positive coefficient vector in $\Theta_{opt}$, but let's assume that the particular data which is being analyzed allows for such vectors to exist.</p>

<p>Perhaps one could start out with an initial (least-squares) optimal coefficient vector, and then manipulate this vector based on certain rules that depend on the nature of the terms which the coefficients are associated with.  I can deduce some general rules for doing these manipulations, but don't know how to algorithmically perform the manipulations in a manner such that I arrive at a minimal-length parameter vector (parameters==0 don't count towards the vector's length).  This may be heading the wrong direction though...?</p>
"
"0.0566365471788599","0.0555127381653369"," 26539","<p>This is just an example that I have come across several times, so I don't have any sample data. Running a linear regression model in R:</p>

<pre><code>a.lm = lm(Y ~ x1 + x2)
</code></pre>

<p><code>x1</code> is a continuous variable. <code>x2</code> is categorical and has three values e.g. ""Low"", ""Medium"" and ""High"". However the output given by R would be something like:</p>

<pre><code>summary(a.lm)
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   0.521     0.20       1.446   0.19        
x1            -0.61     0.11       1.451   0.17
x2Low         -0.78     0.22       -2.34   0.005
x2Medium      -0.56     0.45       -2.34   0.005
</code></pre>

<p>I understand that R introduces some sort of dummy coding on such factors (<code>x2</code> being a factor). I'm just wondering, how do I interpret the <code>x2</code> value ""High""? For example, what effect does ""High"" <code>x2</code>s have on the response variable in the example given here?</p>

<p>I've seen examples of this elsewhere (e.g. <a href=""http://www.ats.ucla.edu/stat/r/modules/dummy_vars.htm"">here</a>) but haven't found an explanation I could understand. </p>
"
"0.0700841515030364","0.078506867197886"," 26614","<p>Continuing on from this <a href=""http://stats.stackexchange.com/questions/26329/what-is-a-unit-information-prior"">question</a> and this <a href=""http://stats.stackexchange.com/questions/26339/the-unit-information-prior-and-its-bic-approximation"">question</a> re BIC and its approximation to the Bayes factor with a unit information prior <a href=""http://www.jstor.org/discover/10.2307/2291327?uid=3737536&amp;uid=2&amp;uid=4&amp;sid=47698890289477"" rel=""nofollow"">(Kass &amp; Wasserman, 1995)</a>, I'm trying to quantify this relationship as a stepping stone into Bayesian stats. So far, my calculation of the BIC approximation of the Bayes factor (based upon my impression of <a href=""http://www.jstor.org/discover/10.2307/2291327?uid=3737536&amp;uid=2&amp;uid=4&amp;sid=47698890289477"" rel=""nofollow"">Wagenmakers 2007</a>) is linearly related to my Bayes factor that is calculated from my interpretation of the unit information prior using the <a href=""http://www.r-inla.org/home"" rel=""nofollow"">INLA</a> package in R. Good start! However, my BIC Bayes factor is ~ 3 times smaller than the Bayes factor calculated with INLA and I'm not sure why. </p>

<p>The prior I've used in the ""<strong>inla</strong>"" function is N(0, 1/(variance * n)) and this seems to me the likely place where I'm out. I'm not sure how I got the multiply by n in the formula, but it appears to work... roughly. Kass and Wasserman have N(0, variance / n) which when converted to precision would be N(0, n / variance), but this gives me a less good relationship. </p>

<p>Help based on other Bayesian packages is also welcome.</p>

<p><em>EDIT</em></p>

<pre><code>*Deleted code, see below answer instead*
</code></pre>

<p><em>EDIT</em></p>

<p>So I'm pretty sure I've figured out the one sample case. I would still appreciate help for the two sample case and the regression case (which I'll start working on now).</p>
"
"NaN","NaN"," 26790","<p>I'm working on a securities pricing project and have a bunch of models I'd like to stack/ensemble together.  I've been using simple linear regression in R (the <code>lm()</code> function) so far but the results are over fitting pretty badly.  </p>

<p>Does anyone have any suggestions for whether some other stacking method might be better or any papers/articles that describe how to stack regression models (as opposed to classification models).</p>
"
"0.0471971226490499","0.064764861192893"," 27351","<p>I want to compare three models, one linear-regression-model, one regression-tree-model (from <code>rpart</code>) and one MARS-model (from <code>mda</code> package).</p>

<p>I want to compare the models using a <em>leave one out cross validation</em> using the mean square error and MAPE. I have the following implementation in R:</p>

<pre><code>library(data.table)
library(rpart)
library(mda)

#Load Sample-Data
data(trees)

#The following models should be compared:
# lm(Volume~Girth+Height, data=trees)
# rpart(Volume~Girth+Height, data=trees)
# mars(trees[,-3], trees[3])

LOOCV&lt;-function(modelCall) {
  unlist(sapply(seq(1,nrow(trees)), function(i) {         
    training=trees[-i,]
    test=trees[i,]

    fit=eval(modelCall)
    testValue = predict(fit, test[1:2])

    test[3]-testValue
  }))
}

LOOCV_MSE&lt;-function(modelCall) {
   sum(LOOCV(modelCall)^2)/nrow(trees)
}

LOOCV_RMSE&lt;-function(modelCall) {
   sqrt(LOOCV_MSE(modelCall))
}

LOOCV_MAPE&lt;-function(modelCall) {
  sum(abs(LOOCV(modelCall)/sapply(seq(1, nrow(trees)), function(i) {trees[i,3]})))/nrow(trees)*100                                    
}


cat(""Cross-Validation Metrics:\n"")
cat(""-------------------------\n"")
cat(""LOOCV MSE for LM:"", LOOCV_MSE(quote(lm(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MSE for CART:"", LOOCV_MSE(quote(rpart(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MSE for MARS:"", LOOCV_MSE(quote(mars(training[,-3], training[3]))),""\n"")
cat(""\n"")

cat(""LOOCV RMSE for LM:"", LOOCV_RMSE(quote(lm(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV RMSE for CART:"", LOOCV_RMSE(quote(rpart(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV RMSE for MARS:"", LOOCV_RMSE(quote(mars(training[,-3], training[3]))),""\n"")
cat(""\n"")

cat(""LOOCV MAPE for LM:"", LOOCV_MAPE(quote(lm(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MAPE for CART:"", LOOCV_MAPE(quote(rpart(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MAPE for MARS:"", LOOCV_MAPE(quote(mars(training[,-3], training[3]))),""\n"")
</code></pre>

<p>Outputs:</p>

<pre><code>Cross-Validation Metrics:
-------------------------
LOOCV MSE for LM: 18.15783 
LOOCV MSE for CART: 69.83769 
LOOCV MSE for MARS: 13.72282 

LOOCV RMSE for LM: 4.2612 
LOOCV RMSE for CART: 8.356895 
LOOCV RMSE for MARS: 3.704432 

LOOCV MAPE for LM: 14.6114 
LOOCV MAPE for CART: 23.51401 
LOOCV MAPE for MARS: 10.00316 
</code></pre>

<p>Does this implementation make sense? When whould using MSE on the errors make sense? When would I use MAPE/SMAPE instead? I already read ""<a href=""http://stats.stackexchange.com/questions/13478/metric-to-compare-models"">Metric to compare models?</a>"" and the conclusion there was <em>it depends</em>, can someone explain this further. On what does it depend?</p>

<p>My data is not a time series, it is more like the <code>tree</code> example data. </p>
"
"0.165189929271675","0.166538214496011"," 27553","<h2>Background</h2>

<p>There is a lot of discussion around this, so I thought that I could find my answer from earlier treads on StackExchange and by googling furiously. After using half a day trying to find only one reference book for (bio)statistics with R, I got utterly confused and had to give up. Maybe the free material combined is actually better than any of the books you can buy at the moment. Letâ€™s it find out. </p>

<p>The internet is full of good <a href=""http://cran.r-project.org/other-docs.html"">free literature for R language</a>, so there is really no point paying for a mediocre book, which ends up being used as an office decoration most of the time. The R home site lists <a href=""http://www.r-project.org/doc/bib/R-books.html"">books related to R</a> and there are a lot of them. To be more exact: 115. Only one of them is advertised with words â€œ<a href=""http://www.biomedical-engineering-online.com/content/4/1/18"">standalone statistics reference book</a>â€. It is 8 years old now and may be outdated. The fourth edition of <a href=""http://www.stats.ox.ac.uk/pub/MASS4/"">Modern Applied Statistics with S</a> is even older. <a href=""http://onlinelibrary.wiley.com/book/10.1002/9780470515075"">The R Book</a> is often chewed out as <a href=""http://rads.stackoverflow.com/amzn/click/0470510242"">too basic</a> and <a href=""http://www.springerlink.com/content/l36754377r182731/fulltext.pdf"">not recommended</a> because of lack of references, poorly formatted code and sloppy finish. </p>

<p>However, I am looking for <strong>one book</strong>, which I could use as a <strong>standalone reference to practical statistics</strong> (first and foremost) <strong>with R</strong> (secondary). The book should live on my office desk collecting annotations, coffee stains and greasy finger prints instead of dust on the book shelf. It should replace the collection of free pdfâ€™s I have been using so far, not forgetting that R comes with an excellent reference library. â€œ<em>What is the right approach?</em>â€, â€œ<em>why?</em>"" and â€œ<em>technically, how does it work?</em>â€ are often more burning questions than â€œ<em>how to do it with R?</em>â€ </p>

<p>Since I am an ecologist, I am mostly interested about applications to biostatistics. However, since these things are often connected, an interdisciplinary general reference would be the most valuable for me.</p>

<h2>The task</h2>

<p>If such a book exists (I doubt it), please provide the name of the book (only one per answer) and a short review of the book explaining why it should be named as the reference book for the topic. Since this question is not very different than the existing ones, please use <a href=""http://stats.stackexchange.com/questions/421/what-book-would-you-recommend-for-non-statistician-scientists""><strong>this tread</strong></a> for your answer. You can also list flaws of the book so that we can list those as the features for the ideal reference book.</p>

<p>My question is <strong>what should the reference book for statistics (of most used kinds)  with R contain?</strong></p>

<p>Some initial thoughts are following general features (please, update):</p>

<ul>
<li>Thick as a brick</li>
<li>Concise, but understandable</li>
<li>Filled with figures (with the R code provided)</li>
<li>Easy to understand tables and diagrams describing the most important details from the text</li>
<li>Easy to understand, descriptive text about the statistics / methods containing the most important equations.</li>
<li>Good examples for each approach (with R code)</li>
<li>Broad and up-to-date list of references</li>
<li>Minimal number of typos</li>
</ul>

<p><strong>Table of contents</strong></p>

<p>Since I am not a statistician and would need this (not existing?) book to answer the question it's hard for me to write about the contents. Because <a href=""http://onlinelibrary.wiley.com/book/10.1002/9780470515075"">The R Book</a> clearly intends to be the reference book for statistics with R, but is often criticized, I copied the table of contents from the book as a starting point for the table of contents for the standalone R statistics reference book. Additional task: please, provide additions, suggestions, deletions, etc for the table of contents. </p>

<ol>
<li>Getting Started </li>
<li>Essentials of the R Language </li>
<li>Data Input </li>
<li>Dataframes </li>
<li>Graphics </li>
<li>Tables </li>
<li>Mathematics </li>
<li>Classical Tests </li>
<li>Statistical Modelling </li>
<li>Regression</li>
<li>Analysis of Variance</li>
<li>Analysis of Covariance</li>
<li>Generalized Linear Models</li>
<li>Count Data</li>
<li>Count Data in Tables</li>
<li>Proportion Data</li>
<li>Binary Response Variables</li>
<li>Generalized Additive Models</li>
<li>Mixed-Effects Models</li>
<li>Non-linear Regression</li>
<li>Tree Models</li>
<li>Time Series Analysis</li>
<li>Multivariate Statistics</li>
<li>Spatial Statistics</li>
<li>Survival Analysis </li>
<li>Simulation Models</li>
<li>Changing the Look of Graphics</li>
<li>References and Further Reading</li>
<li>Index </li>
</ol>

<h2>What has been said earlier?</h2>

<p>StackExhange contains several treads asking statistics and R book suggestions. <a href=""http://stackoverflow.com/questions/192369/books-for-learning-the-r-language"">Books for learning the R language</a> asks about a reference book learning R language without statistics aspect. <a href=""http://www.use-r.org/downloads/_The_Art_of_R_Programming__A_Tour_of_Statistical_Software_Design.pdf"">The Art of R Programming</a> is ranked out as the best single suggestion. <a href=""http://stats.stackexchange.com/questions/25632/what-book-is-recommendable-to-start-learning-statistics-using-r-at-the-same-time"">Book to Learn Statistics using R</a> asks for an ideal introductory book to statistics, which is really not the same thing than a reference book. <a href=""http://stats.stackexchange.com/questions/614/open-source-statistical-textbooks"">Open Source statistical textbooks</a> ranks <a href=""http://www.opentextbook.org/2009/04/03/multivariate-statistics-with-r/"">Multivariate statistics with R</a> as the best alternative. <a href=""http://stats.stackexchange.com/questions/421/what-book-would-you-recommend-for-non-statistician-scientists"">What book would you recommend for non-statistician scientists?</a> asks about the best statistics reference book without specifying the program of choice. <a href=""http://stats.stackexchange.com/questions/10532/reference-or-book-on-simulation-of-experimental-design-data-in-r"">Reference or book on simulation of experimental design data in R</a> scores perhaps closest to my question. <a href=""http://rads.stackoverflow.com/amzn/click/1420068725"">Introduction to Scientific Programming and Simulation Using R</a> is the most recommended book here and might be close to what I am looking for. However, this book either won't suffice as a single reference book to statistics with R.</p>

<h2>Some suggestions for the reference book and their flaws</h2>

<p><a href=""http://www.manning.com/kabacoff/"">R in Action</a> has received better reviews than The R Book, yet it is apparently <a href=""http://www.jstatsoft.org/v46/b02/paper"">rather introductory</a>.</p>

<p><a href=""http://users.monash.edu.au/~murray/BDAR/index.html"">Biostatistical design and analysis using R: a practical guide</a> is perhaps close to what I am looking for. It has received a <a href=""http://www.tandfonline.com/doi/abs/10.1080/03949370.2011.618191"">good review</a>, but apparently also this one contains many typos. In addition, this book does not concentrate on explaining statistics, but rather gives statistical analyses as readymade recipes for researchers to use. </p>

<p><a href=""http://press.princeton.edu/titles/8709.html"">Ecological Models and Data in R</a> skips the introductory level. This is a very useful feature seeing that word ""introduction"", scores 43 occurrences in <a href=""http://www.r-project.org/doc/bib/R-books.html"">the R book list</a>,  but perhaps not entirely satisfying, if weâ€™re after the reference book for statisticsâ€¦?</p>

<p><a href=""http://www.crcpress.com/product/isbn/9781420068726"">Introduction to Scientific Programming and Simulation Using R</a> received very <a href=""http://www.jstatsoft.org/v36/b04/paper"">positive review</a>, but is limited to data simulation.</p>

<p>Richiemorrisroe suggests that <a href=""http://www.stats.ox.ac.uk/pub/MASS4/"">Modern Applied Statistics with S</a> is sufficient for a standalone statistics reference book with R. This book has received excellent reviews (<a href=""http://rads.stackoverflow.com/amzn/click/0387954570"">1</a>,<a href=""http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-0258%2819970715%2916:13%3C1548%3a%3aAID-SIM561%3E3.0.CO;2-U/abstract"">2</a>) and is probably the best candidate for the title at the moment? The most recent version came out 10 years ago, which is quite a long time considering program development.</p>

<p>Dimitriy V. Masterov suggests <a href=""http://www.stat.columbia.edu/~gelman/arm/"">Data Analysis Using Regression and Multilevel/Hierarchical Models</a>. Haven't checked this book out yet.</p>

<hr>

<p>After reading lots of book reviews, it seems apparent that the perfect book asked here does not exist yet. However, it is perhaps possible to choose one that is pretty close. This tread is intended as a community wiki for statistics users to find the best existing reference book and as a motivation for the new and old book writers to improve their work. </p>
"
"0.13914291759888","0.136381978368745"," 27830","<p>In a previous post Iâ€™ve wondered how to <a href=""http://stats.stackexchange.com/questions/22494/is-using-a-questionnaire-score-euroqols-eq-5d-with-a-bimodal-distribution-as"">deal with EQ-5D scores</a>. Recently I stumbled upon logistic quantile regression suggested by <a href=""http://www.ncbi.nlm.nih.gov.proxy.kib.ki.se/pubmed/19941281"">Bottai and McKeown</a> that introduces an elegant way to deal with bounded outcomes.
The formula is simple:</p>

<p>$logit(y)=log(\frac{y-y_{min}}{y_{max}-y})$</p>

<p>To avoid log(0) and division by 0 you extend the range by a small value, $\epsilon$. This gives an environment that respects the boundaries of the score. </p>

<p>The problem is that any $\beta$ will be in the logit scale and that makes doesnâ€™t make any sense unless transformed back into the regular scale but that means that the $\beta$ will be non-linear. For graphing purposes this doesnâ€™t matter but not with more $\beta$:s this will be very inconvenient. </p>

<p>My question:</p>

<p><strong>How do you suggest to report a logit $\beta$ without reporting the full span?</strong></p>

<hr>

<h2>Implementation example</h2>

<p>For testing the implementation Iâ€™ve written a simulation based on this basic function:</p>

<p>$outcome=\beta_0+\beta_1* xtest^3+\beta_2*sex$</p>

<p>Where $\beta_0 = 0$, $\beta_1 = 0.5$ and $\beta_2 = 1$. Since there is a ceiling in scores Iâ€™ve set any outcome value above 4 and any below -1 to the max value.</p>

<h3>Simulate the data</h3>

<pre><code>set.seed(10)
intercept &lt;- 0
beta1 &lt;- 0.5
beta2 &lt;- 1
n = 1000
xtest &lt;- rnorm(n,1,1)
gender &lt;- factor(rbinom(n, 1, .4), labels=c(""Male"", ""Female""))
random_noise  &lt;- runif(n, -1,1)

# Add a ceiling and a floor to simulate a bound score
fake_ceiling &lt;- 4
fake_floor &lt;- -1

# Just to give the graphs the same look
my_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, 
             fake_ceiling + abs(fake_ceiling)*.25)
my_xlim &lt;- c(-1.5, 3.5)

# Simulate the predictor
linpred &lt;- intercept + beta1*xtest^3 + beta2*(gender == ""Female"") + random_noise
# Remove some extremes
linpred[linpred &gt; fake_ceiling + abs(diff(range(linpred)))/2 |
    linpred &lt; fake_floor - abs(diff(range(linpred)))/2 ] &lt;- NA
#limit the interval and give a ceiling and a floor effect similar to scores
linpred[linpred &gt; fake_ceiling] &lt;- fake_ceiling
linpred[linpred &lt; fake_floor] &lt;- fake_floor
</code></pre>

<p>To plot the above:</p>

<pre><code>library(ggplot2)
# Just to give all the graphs the same look
my_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, 
             fake_ceiling + abs(fake_ceiling)*.25)
my_xlim &lt;- c(-1.5, 3.5)
qplot(y=linpred, x=xtest, col=gender, ylab=""Outcome"")
</code></pre>

<p>Gives this image:</p>

<p><img src=""http://i.stack.imgur.com/luZGu.png"" alt=""Scatterplot from simulation""></p>

<h3>The regressions</h3>

<p>In this section I create the regular linear regression, quantile regression (using the median) and logistic quantile regression. All estimates are based on bootstrapped values using the bootcov() function.</p>

<pre><code>library(rms)

# Regular linear regression
fit_lm &lt;- Glm(linpred~rcs(xtest, 5)+gender, x=T, y=T)
boot_fit_lm &lt;- bootcov(fit_lm, B=500)
p &lt;- Predict(boot_fit_lm, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))
lm_plot &lt;- plot.Predict(p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)

# Quantile regression regular
fit_rq &lt;- Rq(formula(fit_lm), x=T, y=T)
boot_rq &lt;- bootcov(fit_rq, B=500)
# A little disturbing warning:
# In rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique

p &lt;- Predict(boot_rq, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))
rq_plot &lt;- plot.Predict(p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)

# The logit transformations
logit_fn &lt;- function(y, y_min, y_max, epsilon)
    log((y-(y_min-epsilon))/(y_max+epsilon-y))


antilogit_fn &lt;- function(antiy, y_min, y_max, epsilon)
    (exp(antiy)*(y_max+epsilon)+y_min-epsilon)/
        (1+exp(antiy))


epsilon &lt;- .0001
y_min &lt;- min(linpred, na.rm=T)
y_max &lt;- max(linpred, na.rm=T)
logit_linpred &lt;- logit_fn(linpred, 
                          y_min=y_min,
                          y_max=y_max,
                          epsilon=epsilon)

fit_rq_logit &lt;- update(fit_rq, logit_linpred ~ .)
boot_rq_logit &lt;- bootcov(fit_rq_logit, B=500)


p &lt;- Predict(boot_rq_logit, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))

# Change back to org. scale
transformed_p &lt;- p
transformed_p$yhat &lt;- antilogit_fn(p$yhat,
                                    y_min=y_min,
                                    y_max=y_max,
                                    epsilon=epsilon)
transformed_p$lower &lt;- antilogit_fn(p$lower, 
                                     y_min=y_min,
                                     y_max=y_max,
                                     epsilon=epsilon)
transformed_p$upper &lt;- antilogit_fn(p$upper, 
                                     y_min=y_min,
                                     y_max=y_max,
                                     epsilon=epsilon)

logit_rq_plot &lt;- plot.Predict(transformed_p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)
</code></pre>

<h3>The plots</h3>

<p>To compare with the base function Iâ€™ve added this code:</p>

<pre><code>library(lattice)
# Calculate the true lines
x &lt;- seq(min(xtest), max(xtest), by=.1)
y &lt;- beta1*x^3+intercept
y_female &lt;- y + beta2
y[y &gt; fake_ceiling] &lt;- fake_ceiling
y[y &lt; fake_floor] &lt;- fake_floor
y_female[y_female &gt; fake_ceiling] &lt;- fake_ceiling
y_female[y_female &lt; fake_floor] &lt;- fake_floor

tr_df &lt;- data.frame(x=x, y=y, y_female=y_female)
true_line_plot &lt;- xyplot(y  + y_female ~ x, 
                         data=tr_df,
                         type=""l"", 
                         xlim=my_xlim, 
                         ylim=my_ylim, 
                         ylab=""Outcome"", 
                         auto.key = list(
                           text = c(""Male"","" Female""),
                           columns=2))


# Just for making pretty graphs with the comparison plot
compareplot &lt;- function(regr_plot, regr_title, true_plot){
  print(regr_plot, position=c(0,0.5,1,1), more=T)
  trellis.focus(""toplevel"")
  panel.text(0.3, .8, regr_title, cex = 1.2, font = 2)
  trellis.unfocus()
  print(true_plot, position=c(0,0,1,.5), more=F)
  trellis.focus(""toplevel"")
  panel.text(0.3, .65, ""True line"", cex = 1.2, font = 2)
  trellis.unfocus()
}

compareplot(lm_plot, ""Linear regression"", true_line_plot)
compareplot(rq_plot, ""Quantile regression"", true_line_plot)
compareplot(logit_rq_plot, ""Logit - Quantile regression"", true_line_plot)
</code></pre>

<p><img src=""http://i.stack.imgur.com/74Uid.png"" alt=""Linear regression for bounded outcome""></p>

<p><img src=""http://i.stack.imgur.com/xHRtF.png"" alt=""Quantile regression for bounded outcome""></p>

<p><img src=""http://i.stack.imgur.com/XfLy8.png"" alt=""Logistic quantile regression for bounded outcome""></p>

<h3>The contrast output</h3>

<p>Now I've tried to get the contrast and it's almost ""right"" but it varies along the span as expected:</p>

<pre><code>&gt; contrast(boot_rq_logit, list(gender=levels(gender), 
+                              xtest=c(-1:1)), 
+          FUN=function(x)antilogit_fn(x, epsilon))
   gender xtest Contrast   S.E.       Lower      Upper       Z      Pr(&gt;|z|)
   Male   -1    -2.5001505 0.33677523 -3.1602179 -1.84008320  -7.42 0.0000  
   Female -1    -1.3020162 0.29623080 -1.8826179 -0.72141450  -4.40 0.0000  
   Male    0    -1.3384751 0.09748767 -1.5295474 -1.14740279 -13.73 0.0000  
*  Female  0    -0.1403408 0.09887240 -0.3341271  0.05344555  -1.42 0.1558  
   Male    1    -1.3308691 0.10810012 -1.5427414 -1.11899674 -12.31 0.0000  
*  Female  1    -0.1327348 0.07605115 -0.2817923  0.01632277  -1.75 0.0809  

Redundant contrasts are denoted by *

Confidence intervals are 0.95 individual intervals
</code></pre>
"
"0.0400480865731637","0.039253433598943"," 28070","<p>I was wondering if anyone had experience using the mice function, as described in <a href=""http://www.jstatsoft.org/v45/i03/paper"" rel=""nofollow"">mice: Multivariate Imputation by Chained Equations in R</a> (JSS 2011 45(3))? I have a dataset with a number of variables, each with varying degrees of missing data. </p>

<p>My primary question is: say I use Bayesian linear regression to impute missing data, does <code>mice</code> automatically use predictor variables from most significant to least significant to impute? Also, is it common to perhaps average all the imputed datasets?</p>
"
"0.120144259719491","0.117760300796829"," 28688","<p>I ran <code>lm()</code> on my data with models selected by individual <code>lm</code>'s of each characteristic and then combined the top $R^2$ based on $p$-value. For instance, the first few characteristics are taken, then the rest are evaluated if they have $p&lt;.005$. My characteristics contain some duplication: for instance, I have a characteristic and its normalized variant in test P. My $p$-values are all very small but my diagrams do not look correct for R and T. (Referring to this blog post: <a href=""http://www.findnwrite.com/musings/evaluating-linear-regression-model-in-r/"" rel=""nofollow"">Evaluating Linear Regression Model in R</a>.)</p>

<p>In test P (and T) there is one outlier according to Cooks Distance. How do I find and eliminate that instance?</p>

<p>According to this tutorial on <a href=""http://www.montefiore.ulg.ac.be/~kvansteen/GBIO0009-1/ac20092010/Class8/Using%20R%20for%20linear%20regression.pdf"" rel=""nofollow"">Using R for Linear Regression</a>,</p>

<blockquote>
  <p>The plot in the upper left shows the residual errors plotted versus
  their fitted values.  The residuals should be randomly distributed
  around the horizontal line representing a residual error of zero; that
  is, there should not be a distinct trend in the distribution of
  points.</p>
</blockquote>

<p>Test P looks ok in the residual error but test R and T have a grouping what does that mean and how do I account for it?</p>

<blockquote>
  <p>The plot in the lower left is a standard Q-Q plot, which should
  suggest that the  residual errors are normally distributed.  The
  scale-location plot in the upper right shows the square root of the
  standardized residuals (sort of a square root of relative error) as a
  function of the fitted values.  Again, there should be no obvious
  trend in this plot.</p>
</blockquote>

<p>Again Test P looks ok in the standard Q-Q plot but test R and T have a grouping what does that mean and how do I account for it?</p>

<p>Also what is the coefficients on the output. I notice it lists the characteristics and a p value but i don't understand what it means.</p>

<p>And finally how do I make predictions using the model I created? </p>

<p><strong>Test P</strong>
F-statistic: 2.684 on 280 and 2221 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/10A8r.png"" alt=""enter image description here""></p>

<p><strong>Test R</strong>
F-statistic: 3.691 on 258 and 2243 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/jy6IR.png"" alt=""enter image description here""></p>

<p><strong>Test T</strong>
F-statistic: 4.029 on 268 and 2233 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/bs69P.png"" alt=""enter image description here""></p>

<p>edit after running gls my p looks like this</p>

<p><img src=""http://i.stack.imgur.com/cUBGB.png"" alt=""""></p>
"
"0.0805952195517515","0.0877733458775107"," 28732","<p>I am using the randomForest package in R (R version 2.13.1, randomForest version 4.6-2) for regression and noticed a significant bias in my results: the prediction error is dependent on the value of the response variable. High values are under-predicted and low values are over-predicted. At first I suspected this was a consequence of my data but the following simple example suggests that this is inherent to the random forest algorithm:</p>

<pre><code>n = 1000; 
x1 = rnorm(n, mean = 0, sd = 1)
response = x1
predictors = data.frame(x1=x1) 
rf = randomForest(x=predictors, y=response)
error = response-predict(rf, predictors)
plot(x1, error)
</code></pre>

<p>I suspect the bias is dependent on the distribution of the response, for example, if <code>x1</code> is uniformly-distributed, there is no bias; if <code>x1</code> is exponentially distributed, the bias is one-sided. Essentially, the values of the response at the tails of a normal distribution are outliers. It is no surprise that a model would have difficulty predicting outliers. In the case of randomForest, a response value of extreme magnitude from the tail of a distribution is less likely to end up in a terminal leaf and its effect will be washed out in the ensemble average.</p>

<p>Note that I tried to capture this effect in a previous example, ""RandomForest in R linear regression tails mtry"". This was a bad example. If the bias in the above example is truly inherent to the algorithm, it follows that a bias correction could be formulated given the response distribution one is trying to predict, resulting in more accurate predictions.  </p>

<p>Are tree-based methods, such as random forest, subject to response distribution bias? If so, is this previously known to the statistics community and how is it usually corrected (e.g. a second model that uses the residuals of the biased model as input)?</p>

<p>Correction of a response-dependent bias is difficult because, by nature, the response is not known. Unfortunately, the estimate/predicted response does not often share the same relationship to the bias.</p>
"
"0.11327309435772","0.111025476330674"," 28756","<p>I have two datasets a training and a test dataset. The dependent variable is a proportion and there are 54 predictors which are positive and negative real numbers and another 7 predictors that are text. </p>

<p>There are three response variables. Total the normalized total number of hits. Treatment the normalized total number during treatment and a percent which is a ratio of the other two responses.</p>

<p>At the moment using lm on the percent prediction data I have a corolation of .4. 85% of the varibles are within 20% of their target. For the treatment response variable using glm in poisson mode i have a correlation of .6 percent but the variables do not match the target data at all.</p>

<p>I have two main issues I need advice on: </p>

<p>(1) it rejected the text predictors because it said factor has new level(s)
I would like it to ignore the information for those that have new level but not disregard it for those that have the correct information how do i do that? </p>

<p>(2) To make my dependent variable a real number, rather than a proportion bounded between 0 and 1, I was advised to transform the response using, for example, the logit transform or the Normal quantile function (<code>qnorm</code> in R). The problem is that these transformations (and others like it) will map 0 and 1 to non-finite values. How can I model these data in a regression setting when the response is a proportion that can be 0 or 1? </p>

<p>Using linear regression with outlier removal I am able to get 2239 of 2583 testing data within 20% of their actual value I would like to have that many within 10%. </p>

<p>Using the posson distribution glm the amount of treatment correlates with 69%.</p>

<p>Ignoring this second issue for the moment, I transform the y~x1+x2 such that y=log(y/(1-y)) the correlation of my predictions to actual data drops from 6% to 2%
This is what the data looks like after the logit transform</p>

<p><img src=""http://i.stack.imgur.com/rqkaD.png"" alt=""log distribution""></p>

<p>This is what the data looks like before the log distribution
<img src=""http://i.stack.imgur.com/Mx8sh.png"" alt=""normal percentages""></p>
"
"0.0490486886395286","0.0480754414848157"," 28819","<p>I have created a multiple linear regression model with R using <code>lm</code> and <code>glm</code>. I am using <code>lm</code> on a training set and <code>predict</code> on a testing set to validate the model. In one test my results are within 80% of what they should be for 80% of the cases. It correlates with 40% for one response variable and with 63% for another response variable (but the response variable with 63% correlation isn't near the actual values of the prediction). I have 53 predicates. What is the probability of that occurring randomly?
I've tried to build an multi-class svm off of the features using the predicates but so far the svm has been unable to properly predict the results.</p>
"
"NaN","NaN"," 28882","<p>Is there any function for $M$-estimation in multivariate linear regression model in <code>R</code>. I can estimate the $\beta$'s in my model by using the <code>rlm()</code> by rewriting the $y$-variables into one column but, I would like to use one function to get the $\beta$'s. </p>
"
"0.0633215847514023","0.0620651280774201"," 28957","<p>I am interested in understanding the graph plots we get after running <code>lm()</code> command (for linear regression) in R like, for example</p>

<pre><code>lm.mod1 = lm(y ~ x1 + x2)
</code></pre>

<p>I then get the do the summary by:</p>

<pre><code>summary(lm.mod1)
</code></pre>

<p>I get the result as: </p>

<pre><code> Residuals:
  Min      1Q  Median      3Q     Max
 -750.32 -160.54  -49.83  115.83 2923.74

 Coefficients:
                           Estimate Std. Error     t value Pr(&gt;|t|)    
(Intercept)               -345.1552      37.0393   -9.319   &lt;2e-16 ***
         x1                52.9091       2.4929    21.224   &lt;2e-16 ***
         x2                8.9669        0.5395    16.620   &lt;2e-16 ***

Residual standard error: 274.4 on 1985 degrees of freedom
Multiple R-squared: 0.2059, Adjusted R-squared: 0.2051 
F-statistic: 257.3 on 2 and 1985 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>I then do the plotting by </p>

<pre><code>par(mfrow = c(2,2))
plot(lm.mod1)
</code></pre>

<p>I get 4 graphs (I can't post the graphs since I am a new user and my experience level is below 10. :/)</p>

<p>My questions are : </p>

<ol>
<li><p>How do they calculate F-statistics and t-value?</p></li>
<li><p>Could someone explain me the what do we interpret with the last two graphs i.e. $\text{Scale-Location vs. (Standardized residuals)}^{1/2}$ and $\text{Residuals vs. Leverage}$. What do you mean by Leverage?</p></li>
<li><p>What do you mean by Cook's Distance? I saw it on wikipedia but I didnt get it. </p></li>
<li><p>How could we suggest if our model is a good model or not?</p></li>
</ol>
"
"0.149935386600491","0.152027894610165"," 29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"0.0700841515030364","0.078506867197886"," 29390","<p>I'm running a generalized linear model (quasi-poisson regression) as a cron job in R that trains on data from an SQL query. The SQL query pulls data from the last 30 days. Depending on the sample of data from the last 30 days, the regression coefficients of course change. As a result, I'm then writing the regression coefficients generated in R back to an SQL table, with year/month/day as the the primary key. Thus, I can see the daily change in regression coefficients based on the most recent 30 days of data. </p>

<p>My question is this: how do I best interpret the change in regression coefficients over time? </p>

<p>If I can see a broad overview of techniques, I can hopefully find the one most appropriate given my data. </p>

<p>My objective is to reduce the RMSE of a predictive model. The response variable is the number of events in the 30 day interval from the current time. I have multiple predictors, including interaction terms. </p>

<p>Edit: it is not a requirement to only query the last 30 days of data. I simply wish to weight data closer in time more heavily. A discrete-time (days, in this case) weighted regression on all data would probably be ideal.</p>
"
"0.0400480865731637","0.039253433598943"," 29406","<p>I have the following linear model:</p>

<p>$$w^*=\text{arg min}_w\sum_{i=1}^N \bigg(Y_i-\sum_{j=1}^M X_{i,j}\times w_j\bigg)^2$$</p>

<p>Let $T \in N^*$ and $e_i=|Y_i-\sum_{j=1}^M X_{i,j}\times w_j|$. </p>

<p>It's possible using logistic regression to predict which errors will be less than $T$ (i.e., $e_i&lt;T$) and greater or equal with $T$ (i.e., $e_i \ge T$)?</p>

<p>Here is more information to make the question clearer:</p>

<p>$N$ represent the number of observations. My data has the following property: the histogram of errors using multiple linear regression has a Laplace distribution. My data come from digital images represented on 8 bits. The $Y_i$ are current pixels and $X_{ij}$ are neighborhoods pixels. I want to predict which pixels produce errors less than $T$. I want to know what R functions can I use to make a test? $T$ is not very large, it has the values between 1 and 15 in general.</p>
"
"0.0400480865731637","0.039253433598943"," 29449","<p>Could you please shed some lights about how to interpret linear regresssion results (2-stage vs. 1 stage)?</p>

<p>For example, I have the following:</p>

<pre><code>lmStage1 &lt;- lm(y~x1)
lmStage2 &lt;- lm(residuals(lmStage1)~x2)
summary(lmStage2)
</code></pre>

<p>vs.</p>

<pre><code>lmAll &lt;- lm(y~x1+x2)
summary(lmAll)
</code></pre>

<hr>

<p>How do I interpret and compare the coefficients/t-stats, etc. of the above two models?</p>

<p>And how do I compare the two approaches and what observations/diagnosis/studies shall I draw from the above two models?</p>

<p>In general, I feel that I am quite weak in drawing observations and obtaining intuitions from regression studies... are there books focusing on these interpretations and intuitions?</p>

<p>Thanks a lot!</p>
"
"NaN","NaN"," 29477","<p>Is there an easy way in R to create a linear regression over a model with 100 parameters in R?
Let's say we have a vector Y with 10 values and a dataframe X with 10 columns and 100 rows
In mathematical notation I would write <code>Y = X[[1]] + X[[2]] + ... + X[[100]]</code>.
How do I write something similar in R syntax?</p>
"
"0.02831827358943","0.0277563690826684"," 29525","<p>I have data with a starting (y) value that sequentially increments/decrements as (x) time  measured in days passes. 
I found this link for creating a linear regression of the data 
<a href=""http://www.easycalculation.com/statistics/regression.php"">http://www.easycalculation.com/statistics/regression.php</a></p>

<p>I would like to automate the slope calculation in excel. Does anyone have an idea of how to do it? I see the math formula at the bottom of the page</p>

<p>$$\frac{N\sum XY- \sum X\sum Y}{N\sum X^2-(\sum X)^2}$$</p>

<p>but i don't know how to translate it to an excel formula. The problem is mainly the $\sum XY$ and $\sum X^2$. The others are easy with the <code>count</code>, <code>sum</code> and <code>pow</code> function. 
My x coordinates and y coordinates are in rows such that <code>C1</code> is <code>x1</code> and <code>D1</code> is <code>x2</code>.</p>
"
"0.0849548207682898","0.0832691072480053"," 29981","<p>Let's have some linear model, for example just simple ANOVA:</p>

<pre><code># data generation
set.seed(1.234)                      
Ng &lt;- c(41, 37, 42)                    
data &lt;- rnorm(sum(Ng), mean = rep(c(-1, 0, 1), Ng), sd = 1)      
fact &lt;- as.factor(rep(LETTERS[1:3], Ng)) 

m1 = lm(data ~ 0 + fact)
summary(m1)
</code></pre>

<p>Result is as follows:</p>

<pre><code>Call:
lm(formula = data ~ 0 + fact)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.30047 -0.60414 -0.04078  0.54316  2.25323 

Coefficients:
      Estimate Std. Error t value Pr(&gt;|t|)    
factA  -0.9142     0.1388  -6.588 1.34e-09 ***
factB   0.1484     0.1461   1.016    0.312    
factC   1.0990     0.1371   8.015 9.25e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.8886 on 117 degrees of freedom
Multiple R-squared: 0.4816,     Adjusted R-squared: 0.4683 
F-statistic: 36.23 on 3 and 117 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>Now I try two different methods to estimate confidence interval of these parameters</p>

<pre><code>c = coef(summary(m1))

# 1st method: CI limits from SE, assuming normal distribution
cbind(low = c[,1] - qnorm(p = 0.975) * c[,2], 
    high = c[,1] + qnorm(p = 0.975) * c[,2])

# 2nd method
confint(m1)
</code></pre>

<h2>Questions:</h2>

<ol>
<li>What is the distribution of estimated linear regression coefficients? Normal or $t$?</li>
<li>Why do both methods yield different results? Assuming normal distribution and correct SE, I'd expect both methods to have the same result.</li>
</ol>

<p>Thank you very much!</p>

<p>data ~ 0 + fact</p>

<p><strong>EDIT after an answer</strong>:</p>

<p>The answer is exact, this will give exactly the same result as <code>confint(m1)</code>!</p>

<pre><code># 3rd method
cbind(low = c[,1] - qt(p = 0.975, df = sum(Ng) - 3) * c[,2], 
    high = c[,1] + qt(p = 0.975, df = sum(Ng) - 3) * c[,2])
</code></pre>
"
"0.0506572678011219","0.0620651280774201"," 29990","<p>I am doing research on the field of functional response of mites.
I would like to do a regression to estimate the parameters (attack rate and handling time) of the Rogers type II function.
I have a dataset of measurements.
<strong>How can I can best determine outliers?</strong></p>

<p>For my regression I use the following script in R (a non linear regression):
(the dateset is a simple 2 column text file called <code>data.txt</code> file with <code>N0</code> values (number of initial prey) and <code>FR</code> values (number of eaten prey during 24 hours):</p>

<pre><code>library(""nlstools"")
dat &lt;- read.delim(""C:/data.txt"")    
#Rogers type II model
a &lt;- c(0,50)
b &lt;- c(0,40)
plot(FR~N0,main=""Rogers II normaal"",xlim=a,ylim=b,xlab=""N0"",ylab=""FR"")
rogers.predII &lt;- function(N0,a,h,T) {N0 - lambertW(a*h*N0*exp(-a*(T-h*N0)))/(a*h)}
params1 &lt;- list(attackR3_N=0.04,Th3_N=1.46)
RogersII_N &lt;-  nls(FR~rogers.predII(N0,attackR3_N,Th3_N,T=24),start=params1,data=dat,control=list(maxiter=    10000))
hatRIIN &lt;- predict(RogersII_N)
lines(spline(N0,hatRIIN))
summary(RogersII_N)$parameters
</code></pre>

<p>For plotting the calssic residuals graphs I use following script:</p>

<pre><code>res &lt;- nlsResiduals (RogersII_N)
plot (res, type = 0)
hist (res$resi1,main=""histogram residuals"")
    qqnorm (res$resi1,main=""QQ residuals"")
hist (res$resi2,main=""histogram normalised residuals"")
    qqnorm (res$resi2,main=""QQ normalised residuals"")
par(mfrow=c(1,1))
boxplot (res$resi1,main=""boxplot residuals"")
    boxplot (res$resi2,main=""boxplot normalised residuals"")
</code></pre>

<h3>Questions</h3>

<ul>
<li><strong>How can I best determine which data points are outliers?</strong></li>
<li><strong>Are there tests I can use in R which are objective and show me which data points are outliers?</strong></li>
</ul>
"
"0.09392108820677","0.0920574617898323"," 30035","<p>For a linear regression with multiple groups (natural groups defined a priori) is it acceptable to run two different models on the same data set to answer the following two questions?</p>

<ol>
<li><p>Does each group have a non-zero slope and non-zero intercept and what are the parameters for each within group regression?</p></li>
<li><p>Is there, regardless of group membership, a non-zero trend and non-zero intercept and what are the parameters for this across groups regression?</p></li>
</ol>

<p>In R, the first model would be <code>lm(y ~ group + x:group - 1)</code>, so that the estimated coefficients could be directly interpreted as the intercept and slope for each group.The second model would be <code>lm(y ~ x + 1)</code>. </p>

<p>The alternative would be <code>lm(y ~ x + group + x:group + 1)</code>, which results in a complicated summary table of coefficients, with within group slopes and intercepts having to be calculated from the differences in slopes and intercepts from some reference. Also you have to reorder the groups and run the model a second time anyway in order to get a p-value for the last group difference (sometimes). </p>

<p>Does this using two separate models negatively affect inference in any way or this standard practice?</p>

<p>To put this into context, consider x to be a drug dosage and the groups to be different races. It may be interesting to know the dose-response relationship for a particular race for a doctor, or which races the drug works for at all, but it may also be interesting sometimes to know the dose-response relationship for the entire (human) population regardless of race for a public health official. This is just an example of how one might be interested in both within group and across group regressions separately. Whether a dose-response relationship should be linear isn't important.</p>
"
"0.0895502439463906","0.0877733458775107"," 30451","<p>I'm trying to build a model that would describe some process of payment and distribution of payments in time. I believe that time of payment has <a href=""http://en.wikipedia.org/wiki/L%C3%A9vy_distribution"" rel=""nofollow"">Levy distribution</a> with probability density function:</p>

<p>$ f(x,c)=\sqrt{\frac{c}{2\pi}}~~\frac{e^{ -\frac{c}{2x}}} {x^{3/2}} $</p>

<p>This distribution depends on parameter <em>c</em> which actually defines the shape of distribution. My task is to build model that explains dependency of this parameter on some explaining variables. I'm trying linear dependency $c = \sum_i \beta_i x_i$ </p>

<p>This is example of generalized linear model and is implemented in the <a href=""http://cran.r-project.org/web/packages/VGAM/index.html"" rel=""nofollow"">VGAM package</a> in R. Problem is that in a sample for building this model I have data only from some period at the beginning and this period is different for different groups of cases. And because of that I can not just to run the model in VGAM package on this data as the result will be incorrect significantly exaggerating probability of early payments.</p>

<p>One possible solution I can think about is to change the likelihood function from which parameter is estimated. If we have information only from time up to <em>t</em> and as cumulative distribution function of Levy distribution is:</p>

<p>$ F(x,c)=\textrm{erfc}\left(\sqrt{c/2x}\right) $</p>

<p>the density of distribution up to time <em>t</em>  is  $ f_1(x,c,t)= \frac{f(x,c)}{F(t,c)} $ (where $f(x,c), F(t,c)$ defined as above). This new density functions can be used in estimating regression parameters with maximum likelyhood method. But can it be done in R using methods from VGAM package or usual <strong>glm</strong> function or some other packages? Or there are some better approaches to my problem? I'm interested in implementation in R.</p>

<p>Thank you in advance for any help!</p>
"
"NaN","NaN"," 30731","<p>What is the $R^2$  value given in the summary of a coxph model in R?
For example,</p>

<pre><code>Rsquare= 0.186   (max possible= 0.991 )
</code></pre>

<p>I foolishly included it a manuscript as an $R^2$ value and the reviewer jumped on it saying he wasn't aware of an analogue of the $R^2$  statistic from the classic linear regression being developed for the Cox model and if there was one please provide a reference. Any help would be great!</p>
"
"0.0749231094763201","0.0734364498908627"," 31494","<p>I'm very new to all this, and I am testing different ways to perform a two-way type III ANOVA on my data.</p>

<ul>
<li>I have tried <code>anova()</code> from the <code>stats</code> package, after fitting a linear regression with <code>lm()</code>;</li>
<li>I have tried <code>Anova()</code> from the <code>car</code> package, using the same linear regression (and this gives me the same result as <code>anova()</code> when I use <code>type=""II""</code> - I thought <code>anova()</code> used type I SS by default?).</li>
<li>And I am now trying to use <code>ezANOVA()</code> from the <code>ez</code> package.</li>
</ul>

<p>With this last one, I can't understand what the <code>wid=.()</code> argument is (even reading the help), and as it is not optional, I can't leave it blank. What I am trying to use is as follows, with its result:</p>

<pre><code>&gt; attach(data)
&gt; library(""ez"")
&gt; ezANOVA(data=data, dv=.(AG.DW), wid=.(), within=.(Genotype, Treatment), type=3)
Warning: Converting """" to factor for ANOVA.
Error in sort.list(y) : 'x' must be atomic for 'sort.list'
Have you called 'sort' on a list?
</code></pre>

<p>Is this the right script? What is <code>wid</code> and what should I fill it with?</p>

<p>Concerning my data, the columns <code>Genotype</code> and <code>Treatment</code> are my two factors, and I want to see if there is an interaction when looking at the aboveground dry weight of my plants (column <code>AG.DW</code>). My data is balanced.</p>

<p>I am sorry if information is missing or inaccurate here: this is my first contribution here, and I am only discovering statistics at the moment (and I can't see how to join my data file).</p>
"
"0.0490486886395286","0.0320502943232105"," 31703","<p>I'm trying to <strong>automate</strong> linear regression with R, although I don't really have a concrete background in statistics. I was wondering:</p>

<p>Are there numerical techniques in determining whether the predictor values is even worth trying to fit against a response value before attempting to do this in R:</p>

<pre><code>lmfit &lt;- lm(x ~ y)
</code></pre>

<p>My initial guess was covariance and correlation, but again what are the accepted values?   Also, what if the relationship between response and predictor isn't linear.</p>

<p>In most scenarios (as mentioned by commenters already) one would base predictors based on an a priori hypothesis. However; in knowledge discovery, there are times where you <strong><em>don't know</em> what the hypothesis is</strong>, hence my motivation towards formulating a set of rules whereby a quick numerical parameter would allow me to decide if a certain predictor variable should be added or removed in the linear regression model.</p>

<p>Note that the discussion above is not data dependent, it should work with any general data that may or may not have linear relationships between variables.</p>
"
"0.0490486886395286","0.0480754414848157"," 31735","<p>Related my earlier question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>. Having ""mastered"" linear regression, I'm trying to learn everything I can about logistic regression and am having issues turning largely ""useless"" coefficients into meaningful information.</p>

<p>I asked in my previous question about graphing the probability curve for every permutation of a logit model. However, I was working on just plotting the main curve and was having some issues. </p>

<p>If I was running a logit with just one predictor, I'd run the following:</p>

<pre><code>mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:250,predict(mod1,newdata=data.frame(bid&lt;-c(000:250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>However, what about with multiple predictors? I tried to use the mtcars data set to mess around and couldn't get it.</p>

<p>Any suggestion on how to plot the main probability curve for the logit model.</p>

<pre><code>head(mtcars)

m1 = glm(vs ~ disp + wt, data=mtcars, family=binomial(link=""logit""))
summary(m1)

all.x &lt;- expand.grid(vs=unique(mtcars$vs), disp=unique(mtcars$disp), wt=unique(mtcars$wt))

y.hat.new &lt;- predict(m1, newdata=all.x, type=""response"")
plot(disp&lt;-000:250,predict(m1,newdata=data.frame(disp&lt;-c(000:250), wt&lt;-c(0,250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>EDIT = OR is my previous question what I need to do. Since Y = B0 + B1X1 + B2X2, a one unit change in X1 is associated with a exp(B1) change in Y, regardless of the value of B2. Then would it be possible that my original question is really all that should be done. </p>

<p>My appologies on my stupidity if that is indeed the case.</p>
"
"0.135927713229264","0.127679297780275"," 32040","<p>I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>

<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)
&gt; summary(fix1)

Call:
lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5835 -0.3543 -0.0024  0.3944  4.7294 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
Group1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***
Group2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***
Group3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***
Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** 
Group2:Year  0.0032369  0.0031098   1.041  0.29802    
Group3:Year  0.0006081  0.0032666   0.186  0.85235    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.7926 on 2981 degrees of freedom
Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 
F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>

<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>

<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)
&gt; summary(mix1a)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 4727 4775  -2356     4671    4711
Random effects:
 Groups     Name        Variance Std.Dev.
 Individual (Intercept) 0.39357  0.62735 
 Residual               0.24532  0.49530 
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.1010868   45.90
Group2       4.8094268  0.1158095   41.53
Group3       4.5607287  0.1216522   37.49
Group1:Year -0.0084165  0.0016963   -4.96
Group2:Year  0.0032369  0.0019433    1.67
Group3:Year  0.0006081  0.0020414    0.30

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.252  0.000  0.000              
Group2:Year  0.000 -0.252  0.000  0.000       
Group3:Year  0.000  0.000 -0.252  0.000  0.000
</code></pre>

<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>

<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 2941 3001  -1461     2885    2921
Random effects:
 Groups     Name        Variance  Std.Dev. Corr   
 Individual (Intercept) 0.1054790 0.324775        
            Year        0.0017447 0.041769 -0.246 
 Residual               0.1223920 0.349846        
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.0541746   85.64
Group2       4.8094268  0.0620648   77.49
Group3       4.5607287  0.0651960   69.95
Group1:Year -0.0084165  0.0065557   -1.28
Group2:Year  0.0032369  0.0075105    0.43
Group3:Year  0.0006081  0.0078894    0.08

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.285  0.000  0.000              
Group2:Year  0.000 -0.285  0.000  0.000       
Group3:Year  0.000  0.000 -0.285  0.000  0.000
</code></pre>

<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>

<p>How is this possible? I would expect that the random effect will ""eat"" the unexplained variability and increase ""sureness"" of the estimate!</p>

<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>

<h2>Edit</h2>

<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>

<pre><code>indivSlope = c()
for (indiv in 1:103) {
    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])
    indivSlope[indiv] = coef(mod1)['Year']
}

indivGroup = unique(mydata[,c(""Individual"", ""Group"")])[,""Group""]


anova1 = lm(indivSlope ~ 0 + indivGroup)
summary(anova1)

Call:
lm(formula = indivSlope ~ 0 + indivGroup)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.176288 -0.016502  0.004692  0.020316  0.153086 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
indivGroup1 -0.0084165  0.0065555  -1.284    0.202
indivGroup2  0.0032369  0.0075103   0.431    0.667
indivGroup3  0.0006081  0.0078892   0.077    0.939

Residual standard error: 0.04248 on 100 degrees of freedom
Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 
F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 
</code></pre>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>
"
"0.0578044339088637","0.0566574511374171"," 32313","<p>I have a linear regression model that is used to forecast the 'afluent natural energy' (ANE) of some region.</p>

<p>The predictors for this model are:</p>

<ul>
<li>the previous month ANE (<code>ANE0</code>)</li>
<li>the previous month rain volume (<code>PREC0</code>)</li>
<li>the current month forecast for rain volume (<code>PREC1</code>)</li>
</ul>

<p>We have 7 years of historical data for all of these variables, for each month. The current model just runs a OLS linear regression. I feel there's a lot of improvements to be done, but i'm not a time series specialist.</p>

<p>The first thing I notice is that the predictors are highly correlated (multicollinearity).
I'm not certain of the impacts of multicollinearity on prediction confidence.</p>

<p>I decided to try a time series approach, so I ran a ACF and PACF on the historic data:
The ACF shows a sine wave pattern, and the PACF has a spike at 1 and 2. So I tried both <code>ARIMA (2, 0, 0)</code> and <code>ARIMA(2,0,1)</code> to predict 20 periods ahead.</p>

<p>The ARIMA(2,0,1) shows good results, but I'm not certain as to how to compare it to the linear regression model.</p>

<p>What's the best way to test the performance of these model?  I'm using R as analysis tool (together with the <code>forecast</code> package). </p>
"
"0.0716401951571125","0.0877733458775107"," 32735","<p>As a financial institution, we often run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  Recently we are building another model in which I believe we have regression with autocorrelated errors.The residuals from linear model have <code>lm(object)</code> has clearly a AR(1) structure, as evident from ACF and PACF.  I took two different approaches, the first one was obviously to fit the model using Generalized least squares <code>gls()</code> in R. My expectation was that the residuals from gls(object) would be a white noise (independent errors).  But the residuals from <code>gls(object)</code> still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing that I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (the residuals are white noise). I really want to use <code>gls()</code> in <code>nlme</code> package so that coding will be lot simpler and easier. What would be the approach I should take here? Am I supposed to use REML? or is my expectation of non-correlated residuals (white noise) from gls() object wrong?</p>

<pre><code>gls.bk_ai &lt;- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, 
                 correlation=corARMA(p=1), method='ML',  data  = fit.cap01A)

gls2.bk_ai  &lt;- update(gls.bk_ai, correlation = corARMA(p=2))

gls3.bk_ai &lt;- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai &lt;- update(gls.bk_ai, correlation = NULL)

anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  
     ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise
</code></pre>

<p>Is there something wrong with what I am doing???????</p>
"
"0.0800961731463273","0.0686935087981502"," 33174","<p>I'm failing to understand the value of the intercept value in a multiple linear regression with categorical values. Taking the ""warpbreaks"" data set as an example, when I do:</p>

<pre><code>&gt; lm(breaks ~ wool, data=warpbreaks)

Call:
lm(formula = breaks ~ wool, data = warpbreaks)

Coefficients:
(Intercept)        woolB
     31.037       -5.778
</code></pre>

<p>I'm able to understand that the value of intercept is the mean value of breaks when wool equals ""A"", and that adding up the ""woolB"" coefficient to the intercept value I get the mean value of breaks when wool equals ""B"". However, if I also consider the tension variable in the model, I'm unable to figure out the meaning of the intercept value:</p>

<pre><code>&gt; lm(breaks ~ wool + tension, data=warpbreaks)

Call:
lm(formula = breaks ~ wool + tension, data = warpbreaks)

Coefficients:
(Intercept)        woolB     tensionM     tensionH
     39.278       -5.778      -10.000      -14.722
</code></pre>

<p>I thought it would be the mean value of breaks when either wool equals ""A"" or tension equals ""L"", but that isn't true for this dataset.</p>

<p>Any clues on interpreting the value of intercept?</p>
"
"0.0642198081225601","0.0629455284778823"," 33265","<p>I have made 9 models using simple linear regression. I'm now checking that each of models meets the assumption of homogeneity of variance. Each of the models used either categorical or numeric (year as an integer) IVs. I carried out a Levenes test either at each level of a categorical IV or against the integer IV (year).</p>

<p>I have concluded that the IVs for which the p>0.05 show homogenous variance, while IVs for which the p&lt;0.05 show heterogenous variance. Three (out of 9) models contained IVs for which the p&lt;0.05. These were Model (1 out of 2 IVs showed p&lt;0.05), Model 2 (2 out of 2 IVs showed p&lt;0.05) and Model 3 (3 out of 3 IVs showed p&lt;0.05).</p>

<p>The following plots show these 3 models (IVs which have p&lt;0.05 only):</p>

<p><img src=""http://i.stack.imgur.com/sGWW8.jpg"" alt=""Model 1""></p>

<p><img src=""http://i.stack.imgur.com/VTSFX.jpg"" alt=""Model 2""></p>

<p><img src=""http://i.stack.imgur.com/rzVom.jpg"" alt=""Model 3""></p>

<p><strong>My questions are:</strong></p>

<p>â€¢ am I correct in concluding that the Levenes tests which gave a p&lt;0.05 indicates a violation of homogeneity of variance?</p>

<p>â€¢ in Model 1, in which only 1 out of 2 IVs gave a p&lt;0.05, need only the IV for which p&lt;0.05 be corrected (as opposed to also correcting the IV for which p>0.05)?</p>

<p>â€¢ looking at the plots, could anyone suggest possible solutions for the IVs which gave p>0.05: transformation of response variable, interaction IVs, quadratic IVs, Poisson generalised linear model, generalised additive modelling?????</p>
"
"0.087740961604166","0.100333291527804"," 33463","<p>I've got a dataset for Temperature &amp; KwH and I'm currently performing the regression below. (further regression based on coeffs is performed within PHP)</p>

<pre><code># Some kind of List structure..
UsageDataFrame  &lt;- data.frame(Energy, Temperatures);

# lm is used to fit linear models. It can be used to carry out regression,
# single stratum analysis of variance and analysis of covariance (although
# aov may provide a more convenient interface for these).
LinearModel     &lt;- lm(Energy ~ 1+Temperatures+I(Temperatures^2), data = UsageDataFrame)

# coefficients
Coefficients    &lt;- coefficients(LinearModel)

system('clear');

cat(""--- Coefficients ---\n"");
print(Coefficients);
cat('\n\n');
</code></pre>

<p>The issue comes with our data, we can't ensure there isn't random communication failures or just random errors. This can leave us with values like</p>

<pre><code>Temperatures &lt;- c(16,15,13,18,20,17,20);
Energy &lt;- c(4,3,3,4,0,60,4)

Temperatures &lt;- c(17,17,14,17,21,16,19);
Energy &lt;- c(4,3,3,4,0,0,4)
</code></pre>

<p>Now as humans we can clearly see that the 60 for Kwh is a mistake based on the temperature, however we have over 2,000 systems each with multiple meters and each in different locations all over the country.. and with different levels of normal Energy usage.</p>

<p>A normal dataset would be 48 values for both Temperatures &amp; Energy per day, per meter. In a full year its likely we could have around 0-500 bad points per meter out of 17520 points.</p>

<p>I've read other posts about the <code>tawny</code> package however I've not really seen any examples which would me to pass a <code>data.frame</code> and it process them through cross analysis.</p>

<p>I understand not much can be done, however big massive values surely could be stripped based on the temperature? And the number of times it occurs..  </p>

<p>Since R is maths based I see no reason to move this into any other language.</p>

<p>Please note: I'm a Software Developer and have never used R before.</p>

<p>-- Edit --</p>

<p>Okay here's a real world example, seems this meter is a good example. You can see the Zeros are building up then a massive value is inserted. ""23, 65, 22, 24"" being examples of this. This happens when its in comms failure and it holds the data value and continues to add it up on the device. </p>

<p>(Just to say the comms failures are out of my hands nor can I change the software)</p>

<p>However because Zero is a valid value im wanting to remove any massive numbers against the temperatures or Zeros where its clear they are an Error.</p>

<p>The thought of detecting this and averaging the data back isn't a fix for this either, however it was discussed but since this meter data is every 30mins and comms failures can happen for days.</p>

<p>Most systems are using more Energy then this so its perhaps a bad example from a removing Zero's point of view.</p>

<p>Energy: <a href=""http://pastebin.com/gBa8y5sM"" rel=""nofollow"">http://pastebin.com/gBa8y5sM</a>
Temperatures: <a href=""http://pastie.org/4371735"" rel=""nofollow"">http://pastie.org/4371735</a></p>

<p>(Pastebin seems to have gone down for me after posting such a big file)</p>
"
"0.0749231094763201","0.0734364498908627"," 33516","<p>When computing regression models with R, I regularly use the relevel function to get my model to give me results for the other level, too. I noticed that sometimes, but not often, this changed the model in the sense that levels of other factors that were significant before the relevelling are not any more. Is this inherent to relevelling or exceptional and maybe due to some problem with my data? Does it show that my data likely does not meet one of the prerequisites of linear models?</p>

<p>Related to that, is it alright if I use relevel, recompute my model, and then report significance values from both models in my article? If significance differs between the two models for a certain factor, I suppose I should then go with one that is less optimistic?</p>

<p>I suppose my question betrays that I don't know enough about lm to grasp the need for a base level. I thought I understood it pretty well ;) Somehow none of the introductions I read explained that point, or I was too daft to grasp it. So if someone could direct me to a site where the point of having base levels in lm is explained or explain it themselves, that would be great, too!</p>

<p>Edit: Here's a minimal example:</p>

<pre><code>library(datasets)
sprays&lt;-OrchardSprays
model&lt;-lm(decrease~treatment+rowpos+colpos,data=sprays)
summary(model)
</code></pre>

<p>Part of the summary says</p>

<pre><code>treatmentC    20.625      9.731   2.120  0.03866 *
</code></pre>

<p>So if treatment == C this has significant positive influence on 'decrease'.
Now I relevel 'treatment' to B to find out what influence treatment == A has:</p>

<pre><code>sprays$treatment&lt;-relevel(sprays$treatment,""B"")
summary(model)
</code></pre>

<p>And now treatment == C is not significant in this new model:</p>

<pre><code>treatmentC    17.625      9.731   1.811  0.07567 .
</code></pre>

<p>Sorry for posting in the wrong place! Can I move my question to stats statexchange or should I open a new one there?</p>
"
"0.123591078356221","0.121138726036629"," 33712","<p>I have a question about which prediction variance to use to calculate prediction intervals from a fitted <code>lm</code> object in R. </p>

<p>For a certain multiple linear regression model I have obtained an error variance with leave-one-out-cross-validation (LOOCV) by taking the mean of the squared difference between observed and predicted values (i.e., mean squared prediction error). I am aware of some of the drawbacks of LOOCV (e.g., <a href=""http://stats.stackexchange.com/questions/2352/when-are-shaos-results-on-leave-one-out-cross-validation-applicable"">When are Shao&#39;s results on leave-one-out cross-validation applicable?</a>), but for my specific application this was the easiest (and probably the only realistically) implementable CV method. The final fitted linear model (<code>fitted_lm</code>) is fitted with all observations and with this model I would like to make predictions for new observations (<code>new_observations</code>). For this I am using the <code>predict.lm</code>  function in R.</p>

<pre><code>predict(fitted_lm, new_observations, interval = ""prediction"", pred.var = ???)
</code></pre>

<p>My questions are:  </p>

<ul>
<li>What value do I use for <code>pred.var</code> (i.e., â€œthe variance(s) for future observations to be assumed for prediction intervalsâ€) in order to obtain realistic prediction intervals for my new_observations?  </li>
<li>Do I use the error variance obtained from the LOOCV, or do I use the functionâ€™s default (i.e., â€œthe default is to assume that future observations have the same error variance as those used for fittingâ€)?  </li>
<li>Is the mean squared prediction error not appropriate in this case?</li>
</ul>

<p>Following up on Michael Chernick's answer hereunder, I had a look in the Draper &amp; Smith (1998) book  (â€œApplied regression analysis. 3rd Editionâ€). In this book <em>s<sup>2</sup></em> is defined as â€œvariance about the regressionâ€ (p 32). This is, I presume, what we describe below as the model estimate of residual variance. Furthermore, this book mentions: </p>

<blockquote>
  <p>â€œSince the actual observed value of <em>Y</em> varies about the true mean value <em>Ïƒ<sup>2</sup></em> [independent of the <em>V(Å¶)</em>], a predicted value of an individual observation will still be given with <em>Å¶</em> but will have variance</p>
  
  <p><img src=""http://i.stack.imgur.com/uUPXs.jpg"" alt=""formula""></p>
  
  <p>With corresponding estimated value obtained by inserting <em>s<sup>2</sup></em> for <em>Ïƒ<sup>2</sup></em>â€ (pp 82-81).</p>
</blockquote>

<p>Thus, as far as I understand, in the D &amp; S book they only use the model estimate of residual variance to calculate confidence intervals. This would be the default setting in the <code>predict</code> function (function help: â€œthe default is to assume that future observations have the same error variance as those used for fittingâ€). However, as fosgen states below, â€œalthough LOOCV mean squared prediction error is not equal to the real mean squared prediction error, it is much more close to real than error variance of fitted modelâ€.</p>

<p>To make this more concrete; in my dataset I get a model estimate of residual variance of <code>0.005998</code> and a LOOCV mean squared prediction error of <code>0.007293</code>. What should I then fill in as <code>pred.var</code> in the <code>predict.lm</code> function:</p>

<ul>
<li>Nothing (i.e. use the default, which would equal to the model estimate of residual variance)</li>
<li><code>0.007293</code> (i.e. the LOOCV mean squared prediction error) </li>
<li><code>0.005998 + 0.007293</code> (Michael Chernick: â€œThe model estimate of residual variance gets added to the error variance due to estimating the parameters to get the prediction error variance for a new observationâ€).</li>
</ul>
"
"0.02831827358943","0.0277563690826684"," 33964","<p>I have some repeated measures with 196 individuals and some of my regressions have a plateau. So I want to characterize this with an eventual breakpoint for each ID.</p>

<p>Example: subset of my personal data</p>

<pre><code> ID     time   y
7G009   0       9
7G009   108,33  13
7G009   185,69  16
7G009   309,22  20
7G009   515,08  21
7G051   0       10
7G051   108,33  14
7G051   185,69  19
7G051   309,22  23
7G051   515,08  25
8S027   0       8
8S027   108,33  13
8S027   185,69  17
8S027   309,22  22
8S027   515,08  23
</code></pre>

<p>I have tested with the <code>strucchange</code> package (<code>breakpoint()</code>), or with the <code>segmented</code> package (<code>segemented()</code>). I have also tried with the <code>siZer</code> package (<code>piecewise.linear()</code>); itâ€™s OK with one ID, but I am stuck when I want to deal with all my IDs.</p>

<p>My first attempt was with using the code in the review named <a href=""http://ftp.csie.ntu.edu.tw/pub/R/CRAN/doc/Rnews/Rnews_2008-1.pdf#page=20"" rel=""nofollow"">segmented: An R Package to Fit Regression Models with Broken-Line Relationships</a> by Vito M. R. Muggeo. Cf page 23: </p>

<pre><code>&gt; data(""plant"")
&gt; attach(plant)
&gt; X&lt;-model.matrix(~0+group)*time
&gt; time.KV&lt;-X[,1]
&gt; time.KW&lt;-X[,2]
&gt; time.WC&lt;-X[,3]
</code></pre>

<p>But I am stuck when I try to use my 196 different explanatory variables.  Can anybody can tell me how deal with the segmented package or offer any other solution?  </p>
"
"0.0700841515030364","0.078506867197886"," 34076","<p>Let's say I have the following data and am running a regression model:</p>

<pre><code>df=data.frame(income=c(5,3,47,8,6,5),
              won=c(0,0,1,1,1,0),
              age=c(18,18,23,50,19,39),
              home=c(0,0,1,0,0,1))
</code></pre>

<p>On one hand, I run a linear model to predict on income:</p>

<pre><code>md1 = lm(income ~ age + home + home, data=df)
</code></pre>

<p>Second, I run a logit model to predict on the won variable:</p>

<pre><code>md2 = glm(factor(won) ~ age + home, data=df, family=binomial(link=""logit""))
</code></pre>

<p>For both models, I wonder how I can generate a table or data frame with the predictor response category, fitted value, and the model predicted value.</p>

<p>So for the linear model, something like:</p>

<pre><code>age  fitted_income  predicted_income
18    3              5 
23    3              3
50    4              2
19    5              5
39    6              4

home   fitted_income    predicted_income
0       5               6       
1       3               9
</code></pre>

<p>Or perhaps it should be for each data point. So for x_i data point, the fitted and predicted values are:</p>

<pre><code>id   age  fitted_income  predicted_income
1     18    3              5 
2     23    3              3
3     50    4              2
4     19    5              5
5     39    6              4
</code></pre>

<ol>
<li><p>From a statistical standpoint, is such an undertaking useful? Why or why not?</p></li>
<li><p>How can this be done in R? (looked at names(md1) and found what I can pull from the model, but haven't proceeded past that)</p></li>
</ol>

<p>Thanks! </p>
"
"0.0800961731463273","0.078506867197886"," 34445","<p>I'd be really grateful for recommendations of a robust package for fitting discrete choice models to a large amount ($n$ in the millions and $p$ in 2000 range) of data. I want a smoothed model that can deal with multi-colinear dependent variables and matrix inversion issues sensibly - like <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"" rel=""nofollow"">glmnet</a>. I'm happy to bootstrap samples, which may be the only way to deal with big data in R.</p>

<p>I've tried using the <a href=""http://cran.r-project.org/web/packages/mlogit/index.html"" rel=""nofollow"">mlogit</a> package and it falls apart with more than a few hundred predictors, producing errors to do with matrix inversion.</p>

<p>My alternative is to use the <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"" rel=""nofollow"">glmnet</a> package for binary regression and then use transforms to approximate the discrete choice model using something called Begg and Gray's approximation.  </p>

<p>This data is not multinomial, in the traditional sense. It is discrete-choice, that is the classes themselves change from observation to observation -- possibly also the number of classes. Each of the classes has a set of predictors which are measured on the same scale and are class specific -- cf. <a href=""http://en.wikipedia.org/wiki/Discrete_choice"" rel=""nofollow"">Discrete Choice Models</a>. I wrote to the maintainer of <code>glmnet</code>, Trevor Hastie,  who says there is no mapping to discrete choice models in their package.</p>

<p>Another name for discrete-choice is conditional logit, with the correct parameterization. I found the package <a href=""http://cran.r-project.org/web/packages/pglm/index.html"" rel=""nofollow"">pglm</a>, but it is also lacking in robustness. There's reference, <a href=""http://stats.stackexchange.com/questions/10141/"">Discrete choice panel models in R</a> to <a href=""http://cran.r-project.org/web/packages/lme4/index.html"" rel=""nofollow"">lme4</a> also, but I have found no examples of the conditional logit with it.</p>
"
"0.0633215847514023","0.0620651280774201"," 34859","<p>I would like to find predictors for a continuous dependent variable out of a set of 30 independent variables. I am using Lasso regression as implemented in the <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"">glmnet</a> package in R. Here is some dummy code:</p>

<pre><code># generate a dummy dataset with 30 predictors (10 useful &amp; 20 useless) 
y=rnorm(100)
x1=matrix(rnorm(100*20),100,20)
x2=matrix(y+rnorm(100*10),100,10)
x=cbind(x1,x2)

# use crossvalidation to find the best lambda
library(glmnet)
cv &lt;- cv.glmnet(x,y,alpha=1,nfolds=10)
l &lt;- cv$lambda.min
alpha=1

# fit the model
fits &lt;- glmnet( x, y, family=""gaussian"", alpha=alpha, nlambda=100)
res &lt;- predict(fits, s=l, type=""coefficients"")
res 
</code></pre>

<p>My questions is how to interpret the output:</p>

<ul>
<li><p>Is it correct to say that in the final output all predictors that show a coefficient different from zero are related to the dependent variable? </p></li>
<li><p>Would that be a sufficient report in the context of a journal publication? Or is it expected to provide test-statistics for the significance of the coefficients? (The context is human genetics)</p></li>
<li><p>Is it reasonable to calculate p-values or other test-statistic to claim significance? How would that be possible? Is a procedure implemented in R? </p></li>
<li><p>Would a simple regression plot (data points plotted with a linear fit) for every predictor be a suitable way to visualize this data?</p></li>
<li><p>Maybe someone can provide some easy examples of published articles showing the use of Lasso in the context of some real data &amp; how to report this in a journal?</p></li>
</ul>
"
"0.120311011027664","0.117923743347098"," 35489","<p>I have real daily market data which I'm looking at to create a model for forecasting. The model that I created (below) used autoregressive terms within a linear regression.</p>

<p>I was sharing this with a colleague and he said ""autoregressive variables are correlated with the other variables in multiple linear setting which creates multicollinarity problem, creating unreliable result.""</p>

<p>So I'm turning to the group for help. Here is the data and the analysis that I performed in R.</p>

<pre><code>#Read in Data
MarketData = read.table('http://sharp-waterfall-3397.herokuapp.com/MarketCategories6.txt', header=TRUE,na.strings = ""NA"", sep="","")
MarketData$Month &lt;- as.factor(MarketData$Month)
MarketData$Weekday &lt;- as.factor(MarketData$Weekday)

str(MarketData)
</code></pre>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/PERregress/index.html"" rel=""nofollow"">PERregress</a> library to help with the autoregression using the <code>back()</code> function and to help with the residual diagnostics:</p>

<pre><code>library(PERregress)
descStat(MarketData)
</code></pre>

<p>Subsetting the data for model building and prediction purposes:</p>

<pre><code>Total = MarketData
MarketData = MarketData[1:268,]
attach(MarketData)
</code></pre>

<p>Here is a regression with everything that I can think of. Note you can have higher autoregressive terms but this will start to mask events since R will ignore the first several rows. Also just an FYI for some reason the residual analysis is breaking which I liked to look for points with undue leverage.</p>

<pre><code>#Market1Category1 Regression for the markets with everything that I can think of it
Market1Category1Output=lm(Market1Category1 ~ Trend+Month2+Month3+Month4+
                          Month5+Month6+Month7+Month8+Month9+Monday+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday2+Holiday3+Holiday4+
                          Event1+Event2+Event3+Event4+Event5+Event6+Event7+
                          Event8+Event9+Event10+Event11+Event12+Event13+
                          Event14+Event15+Event16+Event17+Event18+Event19+
                          Event20+Event21+Event22+Event23+Event24+Event25+
                          Event26+Event27+Event28+
                          back(Market1Category1)+back(Market1Category1, 2))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is the final equation. I'd like to say that I reduced the variables using partial f-test but I couldn't find an easy way to do this so if you know a function please let me know. Basically I looked at the change in adjusted $R^2$.</p>

<pre><code>#Final regression equation 
Market1Category1Output=lm(Market1Category1 ~ Month5+Month6+Month7+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday3+Event2+Event7+Event10+
                          Event13+Event16+Event25+Event28+
                          back(Market1Category1)+back(Market1Category1, 6))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is a plot of the actuals in green vs the predictions in blue but there's a problem:</p>

<pre><code>plot(Time, Market1Category1, col='green')
points(Time, predict(Market1Category1Output, MarketData), col='blue', pch=20)
</code></pre>

<p>The issue is that predict will use the data values instead of it's predicted values for the autoregressive terms. In order to make it use predicted terms I created this loop. If you know a better way let me know.</p>

<pre><code>dataSet2 &lt;- Total
dataSet2[8:length(dataSet2$Time),""Market1Category1""] &lt;- NA
    for (i in (1:(length(dataSet2$Time)-7))) {
  dataSet2[6+i+1,""Market1Category1""] &lt;- 1
  dataSet2[6+i+1,""Market1Category1""] &lt;- predict(Market1Category1Output, 
                                                dataSet2[0:6+i+1,])[6+1] 
}
</code></pre>

<p>Here is the plot again with the results in blue using the predicted results for the autoregressive terms (with the exception of the first 7 since the model needs those to <code>predict</code>):</p>

<pre><code>plot(Total$Time, Total$Market1Category1, col='green')
points(dataSet2$Time, dataSet2$Market1Category1, col='blue', pch=20)
</code></pre>

<p>So here are my questions in order of importance:</p>

<ol>
<li>Does using autoregressive and linear terms violate any fundamental assumptions?</li>
<li>What issues can this cause and what analysis/steps should I do take to avoid these problems?</li>
<li>Is there a better approach to modeling this timeseries?</li>
<li>Is there a more efficient approach?</li>
<li>Given the residuals what steps would you take?</li>
</ol>

<p>Finally two questions which is just causing me more work than possibly necessary:</p>

<ol>
<li>As you can see instead of using the factors for weekday and month I'm using separate conditional variables. I'm doing this because if I use the factor and a level turns out to be insignificant (e.g., Monday for days of the week). I can't remove it. Perhaps there's a way?</li>
<li>Is there a quick way to run a partial F-statistic to understand whether removing a variable makes sense?</li>
</ol>
"
"0.0908205236199223","0.0816002183921495"," 35590","<p>I was unable to figure out how to perform linear regression in R in for a repeated measure design. In a <a href=""http://stackoverflow.com/questions/12182373/plot-of-a-linear-regression-with-interactions"">previous question</a> (still unanswered) it was suggested to me to not use <code>lm</code> but rather to use mixed models. I used <code>lm</code> in the following way:  </p>

<pre><code>lm.velocity_vs_Velocity_response &lt;- lm(Velocity_response~Velocity*Subject, data=mydata)
</code></pre>

<p>(more details on the dataset can be found at the link above)</p>

<p>However I was not able to find on the internet any example with R code showing how to perform a linear regression analysis.</p>

<p>What I want is on one hand a plot of the data with the line fitting the data, and on the other hand the $R^2$ value along with the p-value for the test of significance for the model.</p>

<p>Is there anyone who can provide some suggestions? Any R code example could be of great help.</p>

<hr>

<p><strong>Edit</strong><br>
According to the suggestion I received so far, the solution to my analyze my data in order to understand if there is a linear relation between the two variables Velocity_response (deriving from the questionnaire) and Velocity (deriving from the performance) should be this:</p>

<pre><code>library(nlme)
summary(lme(Velocity_response ~ Velocity*Subject, data=scrd, random= ~1|Subject))
</code></pre>

<p>The result of summary gives this:</p>

<pre><code>    &gt; summary(lme(Velocity_response ~ Velocity*Subject, data=scrd, random= ~1|Subject))
    Linear mixed-effects model fit by REML
     Data: scrd 
           AIC      BIC   logLik
      104.2542 126.1603 -30.1271

    Random effects:
     Formula: ~1 | Subject
            (Intercept) Residual
    StdDev:    2.833804 2.125353

Fixed effects: Velocity_response ~ Velocity * Subject 
                              Value Std.Error DF    t-value p-value
(Intercept)               -26.99558  25.82249 20 -1.0454288  0.3083
Velocity                   24.52675  19.28159 20  1.2720292  0.2180
SubjectSubject10           21.69377  27.18904  0  0.7978865     NaN
SubjectSubject11           11.31468  33.51749  0  0.3375754     NaN
SubjectSubject13           52.45966  53.96342  0  0.9721337     NaN
SubjectSubject2           -14.90571  34.16940  0 -0.4362299     NaN
SubjectSubject3            26.65853  29.41574  0  0.9062674     NaN
SubjectSubject6            37.28252  50.06033  0  0.7447517     NaN
SubjectSubject7            12.66581  26.58159  0  0.4764880     NaN
SubjectSubject8            14.28029  31.88142  0  0.4479188     NaN
SubjectSubject9             5.65504  34.54357  0  0.1637076     NaN
Velocity:SubjectSubject10 -11.89464  21.07070 20 -0.5645111  0.5787
Velocity:SubjectSubject11  -5.22544  27.68192 20 -0.1887672  0.8522
Velocity:SubjectSubject13 -41.06777  44.43318 20 -0.9242591  0.3664
Velocity:SubjectSubject2   11.53397  25.41780 20  0.4537754  0.6549
Velocity:SubjectSubject3  -19.47392  23.26966 20 -0.8368804  0.4125
Velocity:SubjectSubject6  -29.60138  41.47500 20 -0.7137162  0.4836
Velocity:SubjectSubject7   -6.85539  19.92271 20 -0.3440992  0.7344
Velocity:SubjectSubject8  -12.51390  22.54724 20 -0.5550080  0.5850
Velocity:SubjectSubject9   -2.22888  27.49938 20 -0.0810519  0.9362
 Correlation: 
                          (Intr) Velcty SbjS10 SbjS11 SbjS13 SbjcS2 SbjcS3 SbjcS6 SbjcS7 SbjcS8 SbjcS9 V:SS10 V:SS11 V:SS13 Vl:SS2 Vl:SS3
Velocity                  -0.993                                                                                                         
SubjectSubject10          -0.950  0.943                                                                                                  
SubjectSubject11          -0.770  0.765  0.732                                                                                           
SubjectSubject13          -0.479  0.475  0.454  0.369                                                                                    
SubjectSubject2           -0.756  0.751  0.718  0.582  0.362                                                                             
SubjectSubject3           -0.878  0.872  0.834  0.676  0.420  0.663                                                                      
SubjectSubject6           -0.516  0.512  0.490  0.397  0.247  0.390  0.453                                                               
SubjectSubject7           -0.971  0.965  0.923  0.748  0.465  0.734  0.853  0.501                                                        
SubjectSubject8           -0.810  0.804  0.769  0.624  0.388  0.612  0.711  0.418  0.787                                                 
SubjectSubject9           -0.748  0.742  0.710  0.576  0.358  0.565  0.656  0.386  0.726  0.605                                          
Velocity:SubjectSubject10  0.909 -0.915 -0.981 -0.700 -0.435 -0.687 -0.798 -0.469 -0.883 -0.736 -0.679                                   
Velocity:SubjectSubject11  0.692 -0.697 -0.657 -0.986 -0.331 -0.523 -0.607 -0.357 -0.672 -0.560 -0.517  0.637                            
Velocity:SubjectSubject13  0.431 -0.434 -0.409 -0.332 -0.996 -0.326 -0.378 -0.222 -0.419 -0.349 -0.322  0.397  0.302                     
Velocity:SubjectSubject2   0.753 -0.759 -0.715 -0.580 -0.360 -0.992 -0.661 -0.389 -0.732 -0.610 -0.563  0.694  0.528  0.329              
Velocity:SubjectSubject3   0.823 -0.829 -0.782 -0.634 -0.394 -0.622 -0.984 -0.424 -0.799 -0.667 -0.615  0.758  0.577  0.360  0.629       
Velocity:SubjectSubject6   0.462 -0.465 -0.438 -0.356 -0.221 -0.349 -0.405 -0.995 -0.449 -0.374 -0.345  0.425  0.324  0.202  0.353  0.385
Velocity:SubjectSubject7   0.961 -0.968 -0.913 -0.740 -0.460 -0.726 -0.844 -0.496 -0.986 -0.778 -0.718  0.886  0.674  0.420  0.734  0.802
Velocity:SubjectSubject8   0.849 -0.855 -0.807 -0.654 -0.406 -0.642 -0.746 -0.438 -0.825 -0.988 -0.635  0.783  0.596  0.371  0.649  0.709
Velocity:SubjectSubject9   0.696 -0.701 -0.661 -0.536 -0.333 -0.526 -0.611 -0.359 -0.676 -0.564 -0.990  0.642  0.488  0.304  0.532  0.581
                          Vl:SS6 Vl:SS7 Vl:SS8
Velocity                                      
SubjectSubject10                              
SubjectSubject11                              
SubjectSubject13                              
SubjectSubject2                               
SubjectSubject3                               
SubjectSubject6                               
SubjectSubject7                               
SubjectSubject8                               
SubjectSubject9                               
Velocity:SubjectSubject10                     
Velocity:SubjectSubject11                     
Velocity:SubjectSubject13                     
Velocity:SubjectSubject2                      
Velocity:SubjectSubject3                      
Velocity:SubjectSubject6                      
Velocity:SubjectSubject7   0.450              
Velocity:SubjectSubject8   0.398  0.828       
Velocity:SubjectSubject9   0.326  0.679  0.600

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-1.47194581 -0.46509026 -0.05537193  0.39069634  1.89436646 

Number of Observations: 40
Number of Groups: 10 
Warning message:
In pt(q, df, lower.tail, log.p) : NaNs produced
&gt; 
</code></pre>

<p>Now, I do not understand where I can get the R^2 and the corresponding p-values indicating me wether there is a linear relationship between the two variables or not,
nor I have understood how my data can be plotted with the line fitting the regression.</p>

<p>Can anyone be so kind to enlighten me? I really need your help guys...</p>
"
"0.0849548207682898","0.0832691072480053"," 35719","<p>I am just learning R. I have developed a regression model with six predictor variables. While developing it, I found the relationships are not very linear. So, maybe because of this the predictions of my model are not exact.</p>

<p>Here is Headers of my data set:</p>

<pre><code>1.bouncerate(To be predicted)
2.avgServerResponseTime
3.avgServerConnectionTime
4.avgRedirectionTime
5.avgPageDownloadTime
6.avgDomainLookupTime
7.avgPageLoadTime
</code></pre>

<p>Sample datasets:</p>

<pre><code>28.57142857,4.132,0.234,0,0.505,0,14.168
42.85714286,3.356777778,0.090777778,0.077333333,0.459,0.105444444,14.78644444
0,3.372,0.1105,0.0015,0.425,0.1305,34.3425
33.33333333,3.583,0.218,0,0.385,0.649,11.816
66.66666667,2.438,0.234,0,0.3405,0,8.645
100,2.805,0.179666667,3.203666667,0.000333333,0.11,13.47066667
66.66666667,0.977,0,0.003,0,0,12.847
0,2.776,0,7.888,0,0,14.393
100,2.59,0.261,0,0.517,0,6.216
</code></pre>

<p>Here is the summary of my model:</p>

<pre><code>Call:
lm(formula = y ~ x_1 + x_2 + x_3 + x_4 + x_5 + x_6)

Residuals:
     Min       1Q   Median       3Q      Max 
-125.302  -26.210    0.702   26.261  111.511 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 48.62944    0.27999 173.684  &lt; 2e-16 ***
x_1         -0.67831    0.08053  -8.423  &lt; 2e-16 ***
x_2          0.07476    0.49578   0.151 0.880143    
x_3         -0.22981    0.06489  -3.541 0.000399 ***
x_4          0.01845    0.09070   0.203 0.838814    
x_5          3.76952    0.67006   5.626 1.87e-08 ***
x_6          0.07698    0.01565   4.919 8.75e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 33.76 on 19710 degrees of freedom
Multiple R-squared: 0.006298,   Adjusted R-squared: 0.005995 
F-statistic: 20.82 on 6 and 19710 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>plot with all single variable are below:
<img src=""http://i.stack.imgur.com/jsW0j.png"" alt=""bouncerate vs avgServerConnectionTime"">
<img src=""http://i.stack.imgur.com/uhrya.png"" alt=""bouncerate vs ServerResponseTime"">
<img src=""http://i.stack.imgur.com/iuROe.png"" alt=""bouncerate vs avgRedirectionTime"">
<img src=""http://i.stack.imgur.com/wbfsP.png"" alt=""bouncerate vs avgDomainLookupTime"">
<img src=""http://i.stack.imgur.com/arTC6.png"" alt=""bouncerate vs avgPageLoadTime""></p>

<p>I have certain questions about this model:  </p>

<ol>
<li>Is there any way to improve the accuracy of this model?  </li>
<li>Which of the values is most useful: residual standard error, degrees of freedom, multiple R-squared, adjusted R-squared, F-statistics, or p-values for choosing best model?  </li>
<li>Is it appropriate to use polynomial transformations with these data?  </li>
<li>In case I do use polynomial terms in my model, which degree is most appropriate?  </li>
</ol>
"
"0.02831827358943","0.0277563690826684"," 35936","<p>I am new to R and trying to practice with some exercises. Given a data set with 40  observations and 5 variables. Spending is the the response and there are 4 predictors. I started with a linear model Residuals:</p>

<pre><code>    Min      1Q  Median      3Q     Max 
-51.082 -11.320  -1.451   9.452  94.252 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  22.55565   17.19680   1.312   0.1968    
sex         -22.11833    8.21111  -2.694   0.0101 *  
status        0.05223    0.28111   0.186   0.8535    
income        4.96198    1.02539   4.839 1.79e-05 ***
verbal       -2.95949    2.17215  -1.362   0.1803    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 22.69 on 42 degrees of freedom
Multiple R-squared: 0.5267, Adjusted R-squared: 0.4816 
F-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06 
</code></pre>

<p>First, is this what they mean by fit regression model and Secondly, how do I compute the correlation of the residuals with the fitted values? </p>
"
"0.02831827358943","0.0277563690826684"," 36145","<p>I want to predict Tree Heights in a certain area using some variables obtained through remote sensing. Like approximate Biomass, etc. I want to first use a linear regression (I know it's not the best idea but it is a must step for my project). I wanted to know how badly can spatial autocorrelation affect it and what is the easiest way to correct this if it is even possible. I'm doing everything in R by the way. Thank you in advanced.</p>

<p>J.</p>
"
"0.0849548207682898","0.0832691072480053"," 36221","<p>After development of recommendation engine with the R, before removal of outliers from data-set value of residual standard error was 1351 and after removal of outlier its 656. Still there is no accurate prediction which gives 10% correct(near) prediction. For more fitting i also have tried polynomial model with two ,three and four degree but still no improvement. Is there any most important thing to consider without R-squared or adjusted R-squared.   </p>

<p>Where i am using dataset with linear regression model for prediction of product purchase revenue on the base of total numbers of time product added to cart, removed from cart, total numbers of page views of product page. For checking model prediction accuracy i am considering only minimum residual standard error.</p>

<p>Here is Model summary</p>

<pre><code>&gt; summary(model_out)

Call:
lm(formula = yitemrevenue_out ~ xcartaddtotalrs_out + xcartremove_out + 
    xproductviews_out + xuniqprodview_out + xprodviewinrs_out, 
    data = as)

Residuals:
    Min      1Q  Median      3Q     Max 
-2671.1  -173.6   -83.4   -42.9 14288.6 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          3.992e+01  1.254e+01   3.183  0.00147 ** 
xcartaddtotalrs_out -7.888e-03  2.570e-03  -3.070  0.00216 ** 
xcartremove_out     -3.410e+01  2.431e+01  -1.403  0.16076    
xproductviews_out    1.248e+01  1.222e+00  10.215  &lt; 2e-16 ***
xuniqprodview_out   -1.350e+01  1.487e+00  -9.076  &lt; 2e-16 ***
xprodviewinrs_out    3.705e-04  5.151e-05   7.193 7.62e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 656.4 on 3721 degrees of freedom
Multiple R-squared: 0.1398, Adjusted R-squared: 0.1386 
F-statistic: 120.9 on 5 and 3721 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>Thanks</p>
"
"0.0755153962384799","0.0740169842204492"," 36303","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/31690/how-to-test-the-statistical-significance-for-categorical-variable-in-linear-regr"">How to test the statistical significance for categorical variable in linear regression?</a>  </p>
</blockquote>



<p>As we know we can use linear models for numeric dataset(independent variables are numerical only), but what type model is applicable here when I have numeric + categorical dataset(independent variables are combination of numeric and categorical).</p>

<p>for example I have two datasets</p>

<p>1.numeric dataset
2.numeric dataset + categorical dataset</p>

<pre>
1.numeric dataset (Prediction of price of home)

Independent variables
x1 =  numbers of bedrooms
x2 =  size of home in sq. feet

dependent variable
x3 =  price of home

here
dependent variable is numerical
independent variable is with numerical values


2.numeric dataset + categorical dataset(prediction of web visits)

Independent variables
x1 =  search time
x2 =  search query
x3 =  browser
x4 = country

dependent variable
x3 =  visits

here 
dependent variable is numerical
independent variable is with combination of numerical and categorical values
</pre>

<p>I assume here that for dataset 1 linear model with lm() is applicable, but its not possible for second dataset. can any one suggest best technique for dataset 2 to be implemented with model for prediction.</p>
"
"0.0326991257596857","0.0320502943232105"," 37383","<p>I am using the <a href=""http://cran.r-project.org/web/packages/np/index.html"" rel=""nofollow"">np</a> package in R with the <code>npregiv</code> command.  The program is in beta, and I cannot call ordered(var) on one of my instruments (a bug in the program I am pretty sure, the help file says this is allowed).  The variable in question is the number of a particular type of institutions in a district, and all districts are on the [0,13] interval.  If I do not call <code>ordered()</code>, what are the consequences?  In a parametric setting it is a question of whether the increments of the ordinal variable are linear or not - I am not sure how to interpret this in the non parametric regression setting.</p>

<p>Otherwise I could call <code>factor()</code> and convert the variable to an indicator of whether or not there exists an institution in a particular district, but then I am throwing away information.</p>
"
"0.0424774103841449","0.0555127381653369"," 37395","<p>What is the canonical example which show situation when robust linear regression has advantage over least square linear regression ? I was trying to simulate situation when some errors (20% of them) are generated from t-student distribution and 80% are from normal - both distribution with the same variance ! on datasets with 50 observation, and I cant see clearly that robust regression is better, here is my R code for this experiment :</p>

<pre><code>library(MASS)
n=50 # size of datasets
N=1000 # number of regressions
wynik=matrix(0,N,2) # matrix with estimated coefficients
v=5  # parametr of t-student distribution
Sd=(v/(v-2))^.5 # standard deviation of gaussian distribution
a=1 # coefficient 

for(i in 1:N){

x=rnorm(n,mean=1,sd=Sd)

e_norm&lt;-rnorm(n,sd=Sd)
e_t&lt;-rt(10, df=v )

y_norm=a*x+e_norm
y_t=a*x+c(e_t,rnorm(40,sd=Sd)) # wariant 2 czÄ™Å›Ä‡ to outliery

Zm1=lm(y_t~x)$coef[2]
    Zm2=rlm(y_t~x)$coef[2]

wynik[i,1]=Zm1
wynik[i,2]=Zm2
plot(1,1,main=paste(i))
}

plot(density(wynik[,1]),main=""density of LS estimator(black) and robust estimator (green)"")
lines(density(wynik[,2]),col=""green"")
# average values of LS and robust estimator
colMeans(wynik)
</code></pre>
"
"0.0749231094763201","0.0734364498908627"," 37785","<p>I come from an SPSS background and am attempting to move to <code>R</code> for it's superior flexibility and data manipulation abilities. I have some concerns however as to whether the <code>lm()</code> is really using partial correlations.</p>

<p>I'm basically trying to run a linear regression, using something similar to the ""enter"" setting in SPSS, which essentially builds the model one variable at a time, reporting the change in $R^2$ with each additional variable. This allows you to determine how much predictive power each variable adds to the model.</p>

<p>When I run the same analysis in <code>R</code> however, I don't get any information on the $R^2$ contributed by individual variables, and I'm not even sure that it's using partial corrrelations to calculate the p-values it's reporting!</p>

<p>My code follows:</p>

<pre><code>summary(m1 &lt;- lm(totalprop ~ cos(Angle) + Alignment + colour + 
  Angle*Alignment, dataset))
</code></pre>

<p>My questions:</p>

<ol>
<li>Does R use partial correlations to determine reported p-values from <code>lm()</code>?</li>
<li>How can I make <code>R</code> report change in $R^2$ with each additional variable.</li>
<li>How can I make <code>R</code> act like SPSS in calulating the model piece by piece? Is this possible without running multiple iterations of the lm() function? If not, how does one control for the effects of covariates in R?</li>
</ol>
"
"0.0899225958391357","0.0961508829696314"," 37840","<p>Okay, so I am trying to understand linear regression. I've got a data set and it looks all quite alright, but I am confused. This is my linear model-summary:</p>

<pre><code>Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.2068621  0.0247002   8.375 4.13e-09 ***
temp        0.0031074  0.0004779   6.502 4.79e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.04226 on 28 degrees of freedom
Multiple R-squared: 0.6016, Adjusted R-squared: 0.5874 
F-statistic: 42.28 on 1 and 28 DF,  p-value: 4.789e-07 
</code></pre>

<p>so, the p-value is really low, which means it is very unlikely to get the correlation between x,y just by chance.
If I plot it and then draw the regression line it looks like this:
<a href=""http://s14.directupload.net/images/120923/l83eellv.png"" rel=""nofollow"">http://s14.directupload.net/images/120923/l83eellv.png</a>
(Had it in as a picture but I am - as a new user - currently not allowed to post it)
Blue lines = confidence interval
Green lines = prediction interval</p>

<p>Now, a lot of the points do not fall into the confidence interval, why would that happen? I think none of the datapoints falls on the regression line b/c they are just quite far away from each other, but what I am not sure of: Is this a real problem? They still are around the regression line and you can totally see a pattern. But is that enough?
I'm trying to figure it out, but I just keep asking myself the same questions over and over again.</p>

<p>What I thought of so far:
The confidence interval says that if you calculate CI's over and over again, in 95% of the times the true mean falls into the CI.
So: It it is not a problem that the dp do not fall into it, as these are not the means really.
The prediction interval on the other hand says, that if you calculate PI's over and over again, in 95% of the times the true VALUE falls into the interval. So, it is quite important to have the points in it (which I do have).
Then I've read the PI always has to have a wider range than the CI. Why is that?
This is what I have done:</p>

<pre><code>conf&lt;-predict(fm, interval=c(""confidence""))
prd&lt;-predict(fm, interval=c(""prediction""))
</code></pre>

<p>and then I plotted it by:</p>

<pre><code>matlines(temp,conf[,c(""lwr"",""upr"")], col=""red"")
matlines(temp,prd[,c(""lwr"",""upr"")], col=""red"")
</code></pre>

<p>Now, if I calculate CI and PI for additional data, it does not matter how wide I choose the range, I get the exact same lines as above. I cannot understand. What does that mean?
This would then be:</p>

<pre><code>conf&lt;-predict(fm,newdata=data.frame(x=newx), interval=c(""confidence""))
prd&lt;-predict(fm,newdata=data.frame(x=newx), interval=c(""prediction""))
</code></pre>

<p>for new x I chose different sequences.
If the sequence has a different # of observations than the variables in my regression, I am getting a warning. Why would that be?</p>
"
"0.0400480865731637","0.039253433598943"," 37973","<p>I am fitting a simple linear regression model with 4 predictors:</p>

<p><code>lm(Outcome ~ Predictor1 + Predictor2 + Predictor3 + Predictor4, data=dat.s)</code></p>

<p>I'm finding that the model predictions are consistently off as shown in this graph:
<img src=""http://i.stack.imgur.com/CNLJz.png"" alt=""scatterplot of predictions and residuals""></p>

<p>The model clearly overestimates the low values and underestimates the high values, but the miss-estimation is very linear -- it seems like the model should be able to just adjust the slope and fit the data better. Why is that not happening? In case it helps, here are scatterplots of the the Outcome against each of the four Predictors:
<img src=""http://i.stack.imgur.com/uc55e.png"" alt=""enter image description here""></p>

<p>Using the <code>car</code> package <code>outlierTest</code> function did not identify any outliers.</p>
"
"0.109891042959396","0.0807828998904497"," 38379","<p>I am trying to match the outputs of PROC REG with PROC GENMOD. I ran a sample test on the 'iris' dataset of R.</p>

<p>The data set is as follows (150 rows in total):</p>

<pre><code>  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
</code></pre>

<p>My PROC REG code is:</p>

<pre><code>proc reg data=iris;
    model Sepal_Length = Sepal_Width Petal_Length Petal_Width;
run;
</code></pre>

<p>My PROC GENMOD code is:</p>

<pre><code>proc genmod data=iris;
    model Sepal_Length = Sepal_Width Petal_Length Petal_Width / dist=normal;
run;
</code></pre>

<p>The output for PROC REG is:</p>

<pre><code>                        Parameter       Standard
Variable        DF       Estimate          Error    t Value    Pr &gt; |t|

Intercept        1        1.85600        0.25078       7.40      &lt;.0001
Sepal_Width      1        0.65084        0.06665       9.77      &lt;.0001
Petal_Length     1        0.70913        0.05672      12.50      &lt;.0001
Petal_Width      1       -0.55648        0.12755      -4.36      &lt;.0001
</code></pre>

<p>The output for PROC GENMOD is:</p>

<pre><code>                               Standard     Wald 95% Confidence      Chi-
Parameter      DF   Estimate      Error           Limits           Square   Pr &gt; ChiSq

Intercept       1     1.8560     0.2474     1.3711       2.3409     56.28       &lt;.0001
Sepal_Width     1     0.6508     0.0658     0.5220       0.7797     97.98       &lt;.0001
Petal_Length    1     0.7091     0.0560     0.5995       0.8188    160.59       &lt;.0001
Petal_Width     1    -0.5565     0.1258    -0.8031      -0.3098     19.56       &lt;.0001
Scale           1     0.3103     0.0179     0.2771       0.3475
</code></pre>

<p>According to my understanding, the standard errors of both codes should match as the generalized linear model is run on a normal distribution.</p>

<p>Also, I ran both regression on R using <code>lm()</code> and <code>glm(..., family=gaussian)</code> and the standard error came out equal. Moreover, they are the same as the standard error of PROC REG.</p>

<p>Can anyone elaborate on why they are not matching?</p>
"
"0.09392108820677","0.0920574617898323"," 38491","<p>If we have a spatial autoregressive process, we can estimate a model to control for the autoregression with a spatial lag,
$$y=\rho W y + X\beta + \epsilon$$
Where $\rho$ is the strength of the spatial correlation, and $W$ is a matrix of spatial weights. The <code>spdep</code> package for R contains the <code>lagsarlm</code> command which is designed to estimate precisely this model. The package contains methods for creating the weights. But there seems to be some discrepancy between the model fit between <code>lagsarlm()</code> and <code>lm()</code> fitted to what should be a similar model.</p>

<p>As an example, consider the example given with <code>?lagsarlm</code> in R. </p>

<pre><code>library(spdep)
data(oldcol)
COL.lag &lt;- lagsarlm(CRIME ~ INC + HOVAL, data=COL.OLD,
                nb2listw(COL.nb, style=""W""), method=""eigen"", quiet=TRUE)
summary(COL.lag)
Residuals:
      Min        1Q    Median        3Q       Max 
-37.68585  -5.35636   0.05421   6.02013  23.20555 

Type: lag 
Coefficients: (asymptotic standard errors) 
             Estimate Std. Error z value  Pr(&gt;|z|)
(Intercept) 45.079251   7.177347  6.2808 3.369e-10
INC         -1.031616   0.305143 -3.3808 0.0007229
HOVAL       -0.265926   0.088499 -3.0049 0.0026570

Rho: 0.43102, LR test value: 9.9736, p-value: 0.001588
Asymptotic standard error: 0.11768
    z-value: 3.6626, p-value: 0.00024962
Wald statistic: 13.415, p-value: 0.00024962
</code></pre>

<p>We can estimate what (I think) should be the same model by computing the actual spatial lag variable,</p>

<pre><code>crime.lag &lt;- lag.listw(nb2listw(COL.nb, style=""W""), COL.OLD$CRIME)
linearlag &lt;- lm(CRIME ~ crime.lag + INC + HOVAL, data=COL.OLD)
Residuals:
    Min      1Q  Median      3Q     Max 
-38.644  -6.103   0.266   6.563  21.610 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 38.18099    9.21531   4.143 0.000149 ***
crime.lag    0.55733    0.15029   3.709 0.000570 ***
INC         -0.86584    0.35541  -2.436 0.018864 *  
HOVAL       -0.26358    0.09136  -2.885 0.005986 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 10.12 on 45 degrees of freedom
Multiple R-squared: 0.6572, Adjusted R-squared: 0.6343 
F-statistic: 28.75 on 3 and 45 DF,  p-value: 1.543e-10 
</code></pre>

<p>The two models, which I think should be identical, are in fact significantly different from each other in every parameter and in model fit (with the <code>linearlag</code> model providing significantly lower AIC). Are there reasons why this should be? Why should I just not use the second model and abandon the special methods?</p>
"
"NaN","NaN"," 40588","<p>I started learning ridge regression in R. I applied the linear ridge regression to my full data set and got the following results. </p>

<pre><code>gridge&lt;-lm.ridge(divorce ~., data=divusa, lambda=seq(0,35,0.02)) 
select(gridge) 
modified HKB estimator is 0.07693804 
modified L-W estimator is 0.3088377 
smallest value of GCV at 0.02 which.min(gridge$GCV) 
0.02 
2 

round(coef(gridge)[2,-1],3) 
year   unemployed femlab marriage birth military
-0.195  -0.053    0.790    0.148 -0.118   -0.042      

round(coef(g)[-1],3)
 year unemployed femlab marriage birth  military
-0.203  -0.049   0.808    0.150 -0.117 -0.043 
</code></pre>

<p><strong>Questions:</strong>  </p>

<ol>
<li>How do I interpret the results?  </li>
<li>Do I have to do anything else for interpretation?</li>
</ol>
"
"NaN","NaN"," 40603","<p>I am running a logistic regression in R and am attempting to determine if multicollinearity is a problem with my model.<br>
 When I run <code>vif()</code> on my final model, I get <code>GVIF</code> and <code>GVIF^1/(2*Df)</code> columns. From what I have read <code>GVIF^1/(2*Df)</code> is what I should use to assess muticollinearity, but I have been unable to determine what values I should use as a cut-off point. </p>

<p>Any help would be greatly appreciated.</p>
"
"0.133288088883473","0.119756380259162"," 40670","<p>I am familiar with linear regression models but the random section of linear mixed models just melts my mind. I did find an excellent guide that could have helped me but the languageR package is not compatible with newer versions of lme4 so I've been unable to implement it in my work.</p>

<p>For me the fixed effects are very understandable (below lactation and a higher yr2 value both contribute to a higher weight but the lactation effect is more consistent which results in a higher t-value).</p>

<p>The first problem is to understand what I am actually putting in. To a certain extent I understand that <code>(1|P$grupp)</code> means that the mixed model add to the base line (intercept) while <code>(P$grupp|P$lweek)</code> mean that belong to a group is expected to affect the average weight increase (or decrease) while <code>P$lweek</code> adds to the baseline value. But why does all tutorials seem to favor write ups like <code>(1+P$fgrupp|P$lweek)</code> rather than <code>(P$grupp|P$lweek)</code>?</p>

<p>Now on to the actual output (see below for full output). I've used the following models (sorry for the Swenglish but the sample is the weight of cows <code>P$vikt</code> is the weight at certain time points and <code>P$lweek</code> is the time since a calf was born, <code>P$fgrupp</code> is a factor telling if the cow belongs to feed group 1,2 or 3):</p>

<pre><code>Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 | P$fgrupp)            #$
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 + P$fgrupp | P$lweek)
</code></pre>

<p>Where I understand it as the first one being rather useless (essentially it tells us that the average weight of the cows isn't affected of which feed group it belongs too). This is reflected by fgrupp having variance 0 in the first formula below. The second is more interesting as the <code>P$fgrupp|P$lweek</code> as I understand it should show if different feed groups affect the weight increase of cows as function of the time. But I am really not competent enough to understand the input. I understand that variance somehow mean that belonging to group2 or 3 explain some of the variation in the growth curves but I really don't understand how to interpret this.</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr        
 P$lweek  (Intercept)   13.068  3.6149                                     #$
          P$fgrupp2     77.230  8.7881  1.000                              #$
          P$fgrupp3     81.188  9.0104  1.000 1.000                        #$
 Residual             4031.831 63.4967              
Number of obs: 1048, groups: P$lweek, 84
</code></pre>

<p><strong>Full output</strong></p>

<pre><code>#First model#
Linear mixed model fit by REML 
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 | P$fgrupp)            #$
   AIC   BIC logLik deviance REMLdev
 11703 11732  -5845    11698   11691
Random effects:
 Groups   Name        Variance Std.Dev.
 P$fgrupp (Intercept)    0.0    0.000                                      #$
 Residual             4139.9   64.342  
Number of obs: 1048, groups: P$fgrupp, 3                                   #$

Fixed effects:
            Estimate Std. Error t value
(Intercept)  509.593      4.683  108.82
P$lweek        1.028      0.105    9.79                                    #$
P$laktation   22.789      1.454   15.67                                    #$
P$yr2         35.294      4.093    8.62                                    #$

Correlation of Fixed Effects:
            (Intr) P$lwek P$lktt                                           #$
P$lweek     -0.560                                                         #$
P$laktation -0.636  0.030                                                  #$
P$yr2       -0.240 -0.034 -0.141                                           #$


#Second model#

Linear mixed model fit by REML 
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 + P$fgrupp | P$lweek)  #$
       AIC   BIC logLik deviance REMLdev
     11707 11761  -5842    11693   11685
    Random effects:
     Groups   Name        Variance Std.Dev. Corr        
     P$lweek  (Intercept)   13.068  3.6149                                     #$
              P$fgrupp2     77.230  8.7881  1.000                              #$
              P$fgrupp3     81.188  9.0104  1.000 1.000                        #$
     Residual             4031.831 63.4967              
    Number of obs: 1048, groups: P$lweek, 84                                   #$

Fixed effects:
            Estimate Std. Error t value
(Intercept) 508.2291     5.1770   98.17
P$lweek       1.0662     0.1192    8.94                                    #$
P$laktation  22.6525     1.4459   15.67                                    #$
P$yr2        35.6343     4.0848    8.72                                    #$

Correlation of Fixed Effects:
            (Intr) P$lwek P$lktt                                           #$$
P$lweek     -0.627                                                         #$
P$laktation -0.570  0.025                                                  #$
P$yr2       -0.224 -0.018 -0.136                                           #$
</code></pre>
"
"NaN","NaN"," 40884","<p>How do I test for Lack Of Fit (F-test) using R? I've seen a similar question, but that was for SPSS and it was just said that is can be easily done in R, but not how. </p>

<p>I know in simple linear regression I would use <code>anova(fm1,fm2)</code>, <code>fm1</code> being my model, <code>fm2</code> being the same model with <code>x</code> as a factor (if there are several <code>y</code> for <code>x</code>).
How do I do it in multiple linear regression? </p>
"
"0.174622975695462","0.171158024461146"," 41362","<p>I have no formal training in statistics so please correct me if I use the wrong terms as I try to explain my problem.</p>

<p>I have a set of data (less than 80 points) with essentially 1 single outcome (a float we will call <code>dcl</code>) that can potentially depends on 10 of other variables, most of them floats maybe one or two boolean.</p>

<p>While I might ask some multi-variate regression question later, let's start with something simple. </p>

<p>Historically, people in my field have focused on the strongest correlation between <code>dcl</code> and variable <code>J</code> and some of my data suggests some other dependence on a float <code>Re</code> which is I'm sure at least weakly correlated with 'J' as they share some variables in their respective expressions. So my first question is how do I test the correlation and/or the independence of 'Re' and 'J' on the outcome 'dlc'? Intuitively and physically, I expect 'dlc' to depend strongly on 'J' and weakly on 'Re'. How do I prove this with a statistical analysis?</p>

<p>Here are a few graphs to illustrate the data:</p>

<p><img src=""http://i.stack.imgur.com/dVRMa.png"" alt=""Re vs J"">   </p>

<p><img src=""http://i.stack.imgur.com/hNPvb.png"" alt=""dcl vs J""></p>

<p><img src=""http://i.stack.imgur.com/D3YJe.png"" alt=""dcl vs Re""></p>

<p>Final point, in terms of software, I have python and R installed, I'm fairly proficient in python but I just installed R and know pretty much nothing about it.</p>

<p>EDIT 1: </p>

<p>Following gung's suggestion, I ran my dataset through R:</p>

<pre><code>Call:
lm(formula = dcl ~ J + I(J^2) + Re + I(Re^2), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.0078 -3.7930 -0.4458  2.0869 21.2538 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.648e+01  1.232e+00  13.380  &lt; 2e-16 ***
J           -2.662e+00  3.773e-01  -7.054 6.58e-10 ***
I(J^2)       1.096e-01  2.071e-02   5.293 1.10e-06 ***
Re           1.966e-06  1.621e-05   0.121    0.904    
I(Re^2)      2.191e-11  3.441e-11   0.637    0.526    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 5.369 on 77 degrees of freedom
Multiple R-squared: 0.4831, Adjusted R-squared: 0.4562 
F-statistic: 17.99 on 4 and 77 DF,  p-value: 1.818e-10
</code></pre>

<p>So now I need some help to decipher this (but I will look into R documentation too). I don't know if it's relevant but on physical grounds only, it's probable the dependency in J is $dcl \sim \frac{1}{\sqrt{J}}$. Can I put this directly into the model? Does that already tell me something about the dependency on $J$ vs $Re$?</p>

<pre><code>Call:
lm(formula = dcl ~ J + I(J^(-0.5)) + Re + I(Re^(-0.5)), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8119 -3.0097 -0.8504  1.8506 12.1439 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   8.175e-01  1.634e+00   0.500   0.6184    
J            -2.946e-01  1.258e-01  -2.343   0.0217 *  
I(J^(-0.5))   4.516e+00  7.053e-01   6.403 1.09e-08 ***
Re            3.332e-05  6.684e-06   4.985 3.72e-06 ***
I(Re^(-0.5))  6.009e+02  1.354e+02   4.438 2.98e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.426 on 77 degrees of freedom
Multiple R-squared: 0.6487, Adjusted R-squared: 0.6305 
F-statistic: 35.55 on 4 and 77 DF,  p-value: &lt; 2.2e-16

&gt; model = lm(dcl ~ J+I(J^(-0.5)) + Re+I(Re^(-0.5)), data=df)
&gt; summary(model)
</code></pre>

<p><strong>EDIT 2</strong>: OK I'm starting to understand things better. Also, again based on physical grounds, I would think that the dependency is more something like $dcl ~ \frac{1}{\sqrt{J}} Re^{n}$ with possibly other variables in that product that I ignore. So when I enter such model in R (can I still use <code>lm</code> for something non-linear?), here is what I get:</p>

<pre><code>Call:
lm(formula = dcl ~ I(J^(-0.5)) * I(Re^(-0.1)), data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.5363  -3.0192  -0.2556   1.4373  17.1494 

Coefficients:
                         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               -43.220      9.164  -4.716 1.03e-05 ***
I(J^(-0.5))                63.813     11.088   5.755 1.62e-07 ***
I(Re^(-0.1))              124.245     24.038   5.169 1.77e-06 ***
I(J^(-0.5)):I(Re^(-0.1)) -142.744     27.269  -5.235 1.36e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.668 on 78 degrees of freedom
Multiple R-squared: 0.6042, Adjusted R-squared: 0.5889 
F-statistic: 39.68 on 3 and 78 DF,  p-value: 1.122e-15
</code></pre>

<p>Does that 4th line tell me something about the dependence between $J$ and $Re$? What kind of tools could I use to get an estimate on the exponent on Re? Because right now I'm just trying a few different numbers empirically to see how the errors evolve. Next for me is to plot the dcl vs the new model and see how well the data collapses visually...</p>

<p>EDIT 3:</p>

<p>In the end, I used <code>nls</code> to explore the possible exponents of my fit. I also removed some outliers in my data that used a different experimental method. I settled on a fit that gave me decent Pr(>|t|) and residuals and which visually produce a decent fit. The last outlier is actually another experiment with a different setup, but one that I trust. So in a sense it's good that it shows up as an outlier as it hints at other parameters that need to be explored. Thank you gung, I accept your answer as I believe it guided me in the right direction.</p>

<pre><code>&gt; model = nls(L.D ~ C*I(J^(c1))*I(Re_s^(c2)), start=list(C=10,c1=-0.25,c2=-0.25),data=df)
&gt; summary(model)
Formula: L.D ~ C * I(J^(c1)) * I(Re_s^(c2))
Parameters:
   Estimate Std. Error t value Pr(&gt;|t|)    
C  57.20389   26.40011   2.167   0.0337 *  
c1 -0.27721    0.05901  -4.698 1.27e-05 ***
c2 -0.16424    0.04936  -3.327   0.0014 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
Residual standard error: 4 on 70 degrees of freedom
Number of iterations to convergence: 8 
Achieved convergence tolerance: 2.91e-06 
</code></pre>

<p><img src=""http://i.stack.imgur.com/dfgWX.png"" alt=""enter image description here""></p>
"
"0.0400480865731637","0.039253433598943"," 41547","<p>I'm trying to implement the ""Free Step-Down Resampling Method"" described by Westfall and Young in ""<a href=""http://books.google.es/books?id=nuQXORVGI1QC&amp;printsec=frontcover&amp;hl=es&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false"" rel=""nofollow"">Resampling-Based Multiple Testing</a>"" (algorithm ~2.8 in the text). My goal is to perform a multivariate linear regression. </p>

<p>So, I have an error estimate (from the original sample) like this (using OLS): </p>

<p>$\epsilon = Y - (\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots \beta_pX_p)$</p>

<p>[BTW, $X_1 \dots X_p$ are dummy variables.] </p>

<p>In order to resample ($i$ times), I have to do: </p>

<p>$Y_i^* = \epsilon_i^*$ </p>

<p>where $\epsilon_i^*$ is a <em>with replacement</em> sample from the original $\epsilon$. </p>

<p>Here is the problem: </p>

<p>In my dataset, responses (rows) are clustered (data come from related individuals); so, I would usually have applied Huber-White estimators to account for correlations in OLS-based linear regressions. </p>

<p>I don't know how to proceed here... Should Huber-White estimators be used? If so, how? </p>

<p>Apologies if my question is too simple, but I'm new in resampling methods... I guess the answer is simple, too. Suggestions are welcome. </p>
"
"0.0749231094763201","0.0629455284778823"," 41917","<p>Analyzing educational datasets we have samples of children from samples of class in samples of schools - we have sampling weights, so I use the survey package e.g. to do a linear model. But this kind of design also requires looking at the mixed effects. But this isnâ€™t possible using the survey package. I can do this in nlme â€“ but then I don't know how to account for the weighting. I guess I could use the sample weights as predictors in nlme regressions but I donâ€™t think that is correct.</p>

<p>It seems that this kind of design (in fact any stratified survey sample which includes nested levels) needs analysing from both perspectives â€“ (survey weights and mixed effects) at once â€“ but the packages of choice for each of these perspectives, survey and nlme, each donâ€™t seem to have slots for the other perspective.</p>

<p>Can someone put me on the right track, or suggest another package which does both at the same time?</p>
"
"0.152583362427192","0.149555731567779"," 43040","<p>I need some guidance related to regression model verification using validation data. 
I am new to R-tool &amp; statistics and trying my best to learn. I did search on internet too but I couldn't get a final answer to my questions. 
Actually I have a lot of questions, I may try my best to explain the problems:
I am experimenting with network packets and R-tool.
I have captured some packets from a network using a custom made packet sniffer in java. The sniffer will capture some packets and save the information of packet header like: tcp window size, tcp sequence numbers, date-time, ip header length, ip time to live etc... in a csv file.</p>

<p>Also the sniffer will add category number to each csv file so that we can know which packet belongs to which category. I created 9 different categories saved in 9 different csv files.
Now I extracted 1000 observation from each of the csv files and created a data set named ""alldata"".</p>

<p>Then I created training data set and validation data set from ""alldata"" data set.</p>

<p>Now I want to perform linear regression, logistic regression, decision tree analysis, cluster analysis etc on this ""alldata"" data set.</p>

<p>So my plan was to use training data set to create models and then later use validation data set to verify my models. </p>

<p>Category will be my target variable in any case. I want to predict the category from other independent variables.</p>

<ol>
<li><p>My first confusion is that after I created scatter plot of category with other independent variables and I don't see any linear relationship between them. Moreover I even don't know what relation exists between category and independent variables. From scatter plots it seems to me that there is no specific relation between category and other independent variables(except date_time it is bit linear to category). Am I doing the correct interpretation ?
Here are some of the plots:
<a href=""http://imageshack.us/photo/my-images/211/tcpdport.png/"" rel=""nofollow"">plot 1</a>
<a href=""http://imageshack.us/photo/my-images/547/tcpchksum.png/"" rel=""nofollow"">plot 2</a></p></li>
<li><p>I think doing linear regression won't make any sense now after having a look at scatter plots. Is this correct assumption?</p></li>
<li><p>Although I tried to do make some regression models with training data set, but the R-square values for all the models is quite low (for example like 0.00019, 0.0035, 0.018 etc. ) 
So can I assume that these models are not good due to very low r-square vales?</p></li>
<li><p>As logistic regression is used when we have target variables having only two values 1 or 0, or some probabilities between 0.0 - 1.0.
This means performing logistic regression is not possible for this type of data set.
Is my assumption true?</p></li>
<li><p>My main question was how to verify a model created with training data set by using validation data set?
Please let me know the commands and the procedure.
Please let me know if I am doing this in wrong way or if you can suggest me a better way to do this whole work. I think if someone could please clear my doubts then I may ask further more questions.</p></li>
</ol>

<p>If you don't understand my problem we can discuss in more detail
I look forward for your replies.
Thank you!</p>

<hr>

<p>@Wayne</p>

<p>Hello thanks for the reply, but the thing is for each category I have almost same range of values of independent variables like(tcpheader, ipttl, iplen). For example iptype is only having two values 6 and 17. So most of the categories are having iptype value of 6 &amp; 17.
So it is also same is for tcpheader, tcp sequence number, tcp acknowledgement number etc. I don't think there is any way to distinguish a particular packet based on these independent variables. Only the independent variable that can be helpful is time.
But when I created a model with time it had good r-squared value but the regression line equation doesn't predict category with any value of date_time.
I don't understand this behaviour.</p>

<p>Thanks.</p>
"
"0.02831827358943","0.0277563690826684"," 43176","<p>I am analyzing the number of donors cancelling commitments as a monthly time series which varies significantly with economic indicators, some according to internal data, and maybe according to season. I want to see the variance after adjusting for the economy and season (assuming the remainder of variance is due to the non-profit's behavior).  Is a good way to do this to fit a linear regression model in R pseudo-code:</p>

<pre><code>fit &lt;- lm(cancels ~ economy + season)
</code></pre>

<p>And then use the residuals?</p>
"
"0.0749231094763201","0.0734364498908627"," 43699","<p>I am R-tool beginner. I have a question regarding how to know the performance of a linear regression model by using validation data.
My approach was</p>

<ol>
<li><p>Create training and validation data sets from original data set.
""train"" is name of my training data set and ""valid"" is name of my validation data set. ""category"" will be my target variable and ""date_time"" is my independent variable.</p></li>
<li><p>Use training data set to create a regression model</p>

<blockquote>
  <p>attach(train)</p>
  
  <p>lreg=lm(category~date_time)</p>
</blockquote></li>
<li><p>Now do predictions for validation data set using model created with training data set</p>

<blockquote>
  <p>p=predict(lreg,valid)</p>
</blockquote></li>
<li><p>Now check the accuracy by finding the values of ACC, AUC.</p>

<blockquote>
  <p>mmetric(valid$category,p,""AUC"")</p>
  
  <p>mmetric(valid$category,p,""ACC"")</p>
</blockquote></li>
</ol>

<p>Now if AUC and ACC have small values then it means that model created by training data set is not good in making predictions.</p>

<p>Is my approach correct ?</p>

<p>Thanks and regards!</p>
"
"0.0566365471788599","0.0555127381653369"," 43747","<p>Using the method in <a href=""http://stackoverflow.com/questions/1395147/best-way-to-plot-interaction-effects-from-a-linear-model"">this post</a>, I have made a plot to visualize the interaction between two predictor variables using the effects package in <a href=""/questions/tagged/r"" class=""post-tag"" title=""show questions tagged 'r'"" rel=""tag"">r</a>, but I'm not really sure what I am looking at. </p>

<p>Tide heights and rain averages are continuous. 8 bins were the maximum the function would allow me to use. The following is the call to <code>effect</code> producing this plot:</p>

<pre><code> R &gt; plot(effect(term=""rain.avg2:tide.avg"",mod=bkrain9.lm,default.levels=8),
          main="""", xlab=""Precipitation - 24hr average (cm)"",
          ylab=expression(""TCB Concentration - CFU*100m""*L^-1),multiline=TRUE)
</code></pre>

<p><strong>Plot updated.</strong> Could somebody explain the purpose of this plotting feature in context to predictor interactions?</p>

<p><img src=""http://i.stack.imgur.com/neZql.jpg"" alt=""P""></p>

<p>Background:
For a class project, I have created a linear regression model to evaluate the effects of the interaction between two predictor variables (tide height and precipitation) on bacteria concentrations.</p>

<p>Thermo-tolerant coliform bacteria concentrations were sampled at 5 sites in a day, where a sample time was recorded at sampling completion. I took an average of these for roughly 20 days, and calculated an associated 24hr average of precipitation before sampling completion, and a 50 minute (the sampling duration) average of tide height before sampling completion.</p>
"
"NaN","NaN"," 43785","<p>If I have a set of continuous predictors $X$ and a binary outcome $Y$ and I wanted to build a predictive model of $P(Y|X)$, I would start with a logistic regression model.</p>

<p>However, in my particular case, my $Y$ isn't binary, it's continuous between 0 and 1.  Is there a similar Generalized Linear Model that can be applied in this case?  My optimistic/naive attempt in R reveals that </p>

<pre><code>set.seed(123)
df &lt;- data.frame(y=runif(8), x1=rnorm(8), x2=rnorm(8))
mod &lt;- glm(y ~ ., data=df, family=binomial('logit'))

# Warning message:
# In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!

rbind(yhat=predict(mod, newdata=df), y=df$y)
#              1         2          3         4         5          6         7           8
# yhat 0.7461449 0.4869853 -0.1092115 1.9854276 0.8328304 -1.3708688 1.0150934 -0.03496334
# y    0.2875775 0.7883051  0.4089769 0.8830174 0.9404673  0.0455565 0.5281055  0.89241904
</code></pre>

<p>Note that some of the predictions are outside of $(0,1)$.  Any suggestions?</p>
"
"0.0566365471788599","0.0555127381653369"," 44227","<p>I am a newbie in data mining world. I have a general question.
I have a data set which has 10 independent variables and one target variable named as category which has 9 values like: 1, 2, 3, 4, 5, 6, 7, 8, 9.
the 10 independent variables have different kind of range of values. some of them have values between 0 - 5000, some have big range like 5,000,000 - 100,000,000 etc.</p>

<p>Moreover there is no specific relation  (linear etc.) existing between target and independent variables.</p>

<p>I am basically trying to predict the target variable category by using all of these independent variables.</p>

<p>Can someone suggest what should be my approach? I am very confused. Should I use regression models, decision trees or cluster analysis?</p>
"
"0.0400480865731637","0.039253433598943"," 44358","<p>I am trying to fit a problem with <code>regsubsets</code> with leaps in R. My problem is particularly strongly collinear, which is why I chose to use it in the first place. </p>

<p>The number of variables is about 200 and I have about 2 million independent observations. All the variables have a strong correlation structure with each other. </p>

<p>On running regsubsets with <code>really.big = TRUE</code>, and <code>nvmax = 5</code> and <code>nbest = 1</code>, I get the following:</p>

<blockquote>
  <p>Error in leaps.setup - 31 linear dependencies found </p>
</blockquote>

<p>and it crashes. All I am looking to do is a simple forward stepwise, say order the variables in the order of correlation and run nested regressions. </p>

<p>Is that too much for the software to handle? I think the problem is well posed in that sense. </p>
"
"NaN","NaN"," 44384","<p>I an trying to perform a multivariate linear regression in R. </p>

<p>I have two dependent variables (outcomes) and one independent variable (predictor). 
The model would be something like: </p>

<pre><code>y1,y2~x
</code></pre>

<p>I did not find any way to implement this in R. 
Any help?</p>
"
"0.0700841515030364","0.078506867197886"," 44895","<p>I am doing a regression analysis which troubled me. </p>

<p>My independent variable are 4 interplanetary condition components, and the dependent variable is the latitude of auroral oval boundary. 
So far, the specific relationship is still unknown in physical principle, what we want to do is to get a model (function expression) from the massive data which shows how these independent variables affect the dependent variable.</p>

<p>I used the Matlab statistical toolbox to do the regression analysis, but the results were very bad. The p values of the F statistic and t statistic are very small, but the RÂ² is also very low, about 20%. </p>

<p>So how should I improve the RÂ²? Are there good methods? I see that SVM (or LS-SVM) can do regression anaysis, is it a good way to manage the massive data, multiple independent variables regression anaysis?</p>

<p>The following are the results:</p>

<pre><code>mdl = 
Linear regression model:
    y ~ 1 + x1*x2 + x1*x3 + x1*x4 + x2*x3 + x2*x4 + x1^2 + x2^2 + x3^2 + x4^2

Number of observations: 18471, Error degrees of freedom: 18457    
Root Mean Squared Error: 2.44  
R-squared: 0.225,  Adjusted R-Squared 0.225  
F-statistic vs. constant model: 413, p-value = 0  
</code></pre>

<p>when we add another predictor, i.e., the independent variables become 5, the resuts of the regression analysis are:</p>

<p>Linear regression model:</p>

<pre><code>y ~ 1 + x1*x2 + x1*x3 + x1*x4 + x1*x5 + x2*x3 + x2*x4 + x2*x5 + x3*x5 + x2^2 + x3^2 + x4^2 + x5^2
</code></pre>

<p>Number of observations: 18457, Error degrees of freedom: 18439
Root Mean Squared Error: 2.21
R-squared: 0.366,  Adjusted R-Squared 0.366
F-statistic vs. constant model: 627, p-value = 0</p>
"
"0.0506572678011219","0.0620651280774201"," 44998","<p>I am trying to perform multinomial logistic regression on my data which is as below(just the header).
<img src=""http://i.stack.imgur.com/LLZjK.jpg"" alt=""enter image description here""></p>

<p>""category"" is my target variable and all other variables are independent variables. category has values like 1,2,3,4,5,6,7,8,9. My main motive is to predict category from independent variables.
Here is summary of my data set.
<img src=""http://i.stack.imgur.com/hXuNg.jpg"" alt=""summary""></p>

<p>I used following command to perform multinomial logistic regression:</p>

<pre><code>&gt; mod=multinom(category~hlen+iplen+ipttl+iptype+tcpsport+tcpdport+tcpsec+tcpack+tcpwindow+tcpchksum+date_time, data=train)
</code></pre>

<p><img src=""http://i.stack.imgur.com/bZMWw.jpg"" alt=""command""></p>

<p>I got the following output but i don't know how to interpret it? 
<img src=""http://i.stack.imgur.com/c2QQR.jpg"" alt=""enter image description here"">
What should be the starting point or is there other way to do so? (For example I know how to interpret linear and logistic regression output to create regression equations)</p>

<p>Please right click any image and select view to see it clearly.
Thank you,</p>
"
"0.0600721298597455","0.078506867197886"," 45449","<p>I have a large set of predictors (more than 43,000) for predicting a dependent variable which can take 2 values (0 or 1). The number of observations is more than 45,000. Most of the predictors are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. My problem is how can I report p-value significance of the predictors. I do get the beta coefficient, but is there a way to claim that the beta coefficients are statistically significant?</p>

<p>Here is my code:</p>

<pre><code>library('glmnet')
data &lt;- read.csv('datafile.csv', header=T)
mat = as.matrix(data)
X = mat[,1:ncol(mat)-1] 
y = mat[,ncol(mat)]
fit &lt;- cv.glmnet(X,y, family=""binomial"")
</code></pre>

<p>Another question is:
I am using the default alpha=1, lasso penalty which causes the additional problem that if two predictors are collinear the lasso will pick one of them at random and assign zero beta weight to the other. I also tried with ridge penalty (alpha=0) which assigns similar coefficients to highly correlated variables rather than selecting one of them. However, the model with lasso penalty gives me a much lower deviance than the one with ridge penalty. Is there any other way that I can report both predictors which are highly collinear?</p>
"
"0.02831827358943","0.0277563690826684"," 45546","<p>Suppose I have two real-valued PMFs $f(i)$ and $g(i)$ with $1\le i\le N$ and $N$ is about 1 billion.  The functions $f$ and $g$ can be assumed to be continuous on the positive integers, one-sidedly so at the endpoints of course. (Meaning if i and j are close, then $f(i)$ and $f(j)$ are close.)</p>

<p>Lets say I have several samples of more than 10 billion observations each.  I tally each sample to get a series of histograms. The histograms basically give us functions $s_1(i)$, $s_2(i)$, $s_3(i)$ and so on.</p>

<p>For each sample, I know the percentage of the observations that come from $f$ and the percentage that come from the distribution described by $g$.</p>

<p>Can I use the samples to reconstruct the distributions $f$ and $g$.</p>

<p>My first guess was to do linear regression.  Although, I'm unsure if linear regression makes use of all the information.</p>
"
"0.0424774103841449","0.0416345536240027"," 45696","<p>I'm not sure if I used the concept ""extreme values"" right. Anyhow, I'm trying to produce a model that estimates maximum tree heights / $\text{km}^2$. I have a database of around 24000 points ($\text{km}^2$), each has the max tree height value and 33 predictors. After playing around with random forest I manage to achieve a correlation of 0.67 between the real height and the estimated height on the test sample (20%). A MSE of around 1.6 meters. But Maximum errors of up to 33 meters. What I can see is that patches with very tall trees or very short trees (50 meters - 1 meters) are out of the scope of the model. Thinking in linear regression it is analogous to losing prediction power as you move away from the center of gravity of the observations. Right? How can I cope with this if at all?</p>

<p>p.s. this was implemented in R</p>
"
"0.02831827358943","0.0277563690826684"," 45824","<p>My data is structured like so:</p>

<p>'data.frame':   50 obs. of  3 variables:</p>

<ul>
<li>Project : Factor w/ 2 levels ""A"",""B"": 1 1 1 1 1 </li>
<li>x: int  2 2 2 2 6 4 4 4 6 4 ...    </li>
<li>y: num  0.622 0.425 0.363 0.344 0.346 ...</li>
</ul>

<p>I have 'attached' the data and plotted both levels in my scatter plot using:</p>

<pre><code>plot(x,y,pch=as.numeric(Project))
TSF&lt;-x[order(x)]
SD&lt;-y[order(x)]
</code></pre>

<p>And fitted a non-linear regression to the data</p>

<pre><code>nls_fit &lt;- nls(SD ~ a - (b*TSF)+ ((c*TSF)^(2)), start = list(a = 0.34, b = 0.017, 
+     c = 0.0003))
lines(TSF, predict(nls_fit), col = ""red"")
</code></pre>

<p>This works well...</p>

<p>But how can I fit this equation to only the factor ""A"" of the data? I'm very new at this so if you have the time an inclination it would be great if you could describe what any code you write does.</p>

<p>Thanks
Kris</p>
"
"0.0633215847514023","0.0620651280774201"," 45939","<p>When I try to use the data and example for HLR from <a href=""http://dl.dropbox.com/u/10246536/Web/RTutorialSeries/dataset_hlr.csv"" rel=""nofollow"">this example dataset</a> taken from this post in the R Tutorial Series on <a href=""http://rtutorialseries.blogspot.com/2010/01/r-tutorial-series-basic-hierarchical.html"" rel=""nofollow"">hierarchical linear regression</a> the results don't match when I try to use the same method in SPSS.  Is it because SPSS is using a different type of sum of squares (III)?</p>

<p>The F values match for the final model, but not the 2nd one and some of the sum of squares seem off.</p>

<pre><code>#R method
url &lt;- ""http://dl.dropbox.com/u/10246536/Web/RTutorialSeries/dataset_hlr.csv""
datavar &lt;- read.csv(url, header=T)

#create three linear models using lm(FORMULA, DATAVAR)
#one predictor model
onePredictorModel &lt;- lm(ROLL ~ UNEM, datavar)
#two predictor model
twoPredictorModel &lt;- lm(ROLL ~ UNEM + HGRAD, datavar)
#three predictor model
threePredictorModel &lt;- lm(ROLL ~ UNEM + HGRAD + INC, datavar)

#get summary data for each model using summary(OBJECT)
summary(onePredictorModel)
summary(twoPredictorModel)
summary(threePredictorModel)

#compare successive models using anova(MODEL1, MODEL2, MODELi)
test&lt;- anova(onePredictorModel, twoPredictorModel, threePredictorModel)
</code></pre>

<p>Below here is the code for SPSS.</p>

<pre><code>*SPSS method
data list free /YEAR ROLL UNEM HGRAD INC.
begin data
1   5501    8.1 9552    1923
2   5945    7   9680    1961
3   6629    7.3 9731    1979
4   7556    7.5 11666   2030
5   8716    7   14675   2112
6   9369    6.4 15265   2192
7   9920    6.5 15484   2235
8   10167   6.4 15723   2351
9   11084   6.3 16501   2411
10  12504   7.7 16890   2475
11  13746   8.2 17203   2524
12  13656   7.5 17707   2674
13  13850   7.4 18108   2833
14  14145   8.2 18266   2863
15  14888   10.1    19308   2839
16  14991   9.2 18224   2898
17  14836   7.7 18997   3123
18  14478   5.7 19505   3195
19  14539   6.5 19800   3239
20  14395   7.5 19546   3129
21  14599   7.3 19117   3100
22  14969   9.2 18774   3008
23  15107   10.1    17813   2983
24  14831   7.5 17304   3069
25  15081   8.8 16756   3151
26  15127   9.1 16749   3127
27  15856   8.8 16925   3179
28  15938   7.8 17231   3207
29  16081   7   16816   3345
end data.


REGRESSION /MISSING LISTWISE 
/STATISTICS COEFF OUTS R ANOVA CHANGE 
/CRITERIA=PIN (.05) POUT(.10) 
/NOORIGIN /DEPENDENT ROLL 
/METHOD=ENTER UNEM 
/METHOD=ENTER HGRAD 
/METHOD=ENTER INC.
</code></pre>

<p>Or did I mess something up in the procedure for SPSS?</p>
"
"0.0400480865731637","0.039253433598943"," 46053","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/138/resources-for-learning-r"">Resources for learning R</a>  </p>
</blockquote>



<p>I would like to learn more about statistics.   I don't feel like doing multivariable regression by hand.  So I downloaded R because its free; however, I haven't found a good introduction upon how to use it. I do want to RTFM, but I want a good manual to read.   </p>

<p>I would like example that I could follow: </p>

<p>T-test: follow this script. 
Linear Regression: Follow this script. </p>

<p>Something along those lines. </p>

<p>Sorry, if this is the wrong place to ask. </p>
"
"0.116939835314396","0.120987207870969"," 46096","<p>I'm working in R, using glm.nb (of the MASS package) to model count data with a negative binomial regression model.  I'd like to compare the relative importance of each of my predictor variables regarding their impact on the response variable (note: the predictors each have quite different scales - sometimes by orders of magnitude).  Unfortunately, the output from R gives me results as unstandardized (<em>b</em>) coefficients (""estimates"").  I'm hoping someone can give me a hint as to how to go about getting standardized (<em>beta</em>) coefficients from the NB regression model... or another 'better' way to determine the relative importance of each of my predictors on my response variable.</p>

<p>I've investigated several potential ways like: </p>

<ol>
<li>using the R package 'relimpo' (as suggested in a comment to <a href=""http://stats.stackexchange.com/a/7118"">http://stats.stackexchange.com/a/7118</a>), but it does not work on a NB regression model, thus completely changing the assumptions I should be accounting for and making the outcomes very different; </li>
<li>mean-centering and scaling my data, which changes the interpretation and makes it so that I can't use NB model due to response variables now having negative values; </li>
<li>scaling-only, so that I can still run a NB model... which I <em>thought</em> would only affect the scale of the coefficients without changing their direction (viz., <a href=""http://stats.stackexchange.com/a/29784"">http://stats.stackexchange.com/a/29784</a> ) - but I do get some positive coefficients that flip to neg. and vice-verse... which seems strange to me and makes me wonder whether I'm making a mistake.</li>
</ol>

<p>I've benefited from looking at <a href=""http://stats.stackexchange.com/q/29781"">When should you center your data &amp; when should you standardize?</a> (and the suggested links from comments on the question such as <a href=""http://andrewgelman.com/2009/07/when_to_standar/"" rel=""nofollow"">http://andrewgelman.com/2009/07/when_to_standar/</a> and <a href=""http://stats.stackexchange.com/q/7112"">When and how to use standardized explanatory variables in linear regression</a> and <a href=""http://stats.stackexchange.com/q/19216"">Variables are often adjusted (e.g. standardised) before making a model - when is this a good idea, and when is it a bad one?</a>).  </p>

<p>Bottom line: I have not yet found a way to use a NB model in R (which I have statistically confirmed is more appropriate than lm, glm, or poisson for modeling my data) and still get at the relative importance - or at least to the standardized beta coefficients - for my predictors...</p>

<p>The R scripts is something like this:</p>

<pre><code>library(""MASS"")
nb = glm.nb(responseCountVar ~ predictor1 + predictor2 + 
  predictor3, data=myData, control=glm.control(maxit=125))
summary(nb)

scaled_nb = glm.nb(scale(responseCountVar, center = FALSE) ~ scale(predictor1, center = FALSE) + scale(predictor2, center = FALSE) + 
  scale(predictor3, center = FALSE), data=myData, control=glm.control(maxit=125))
summary(scaled_nb)
</code></pre>
"
"0.0490486886395286","0.0480754414848157"," 46434","<p>The <code>summary.rq</code> function from the <a href=""http://cran.r-project.org/web/packages/quantreg/quantreg.pdf"">quantreg vignette</a> provides a multitude of choices for standard error estimates of quantile regression coefficients. What are the special scenarios where each of these becomes optimal/desirable?</p>

<ul>
<li><p>""rank"" which produces confidence intervals for the estimated parameters by inverting a rank test as described in Koenker (1994). The default option assumes that the errors are iid, while the option iid = FALSE implements the proposal of Koenker Machado (1999). See the documentation for rq.fit.br for additional arguments.</p></li>
<li><p>""iid"" which presumes that the errors are iid and computes an estimate of the asymptotic covariance matrix as in KB(1978).</p></li>
<li><p>""nid"" which presumes local (in tau) linearity (in x) of the the conditional quantile functions and computes a Huber sandwich estimate using a local estimate of the sparsity.</p></li>
<li><p>""ker"" which uses a kernel estimate of the sandwich as proposed by Powell(1990).</p></li>
<li><p>""boot"" which implements one of several possible bootstrapping alternatives for estimating standard errors.</p></li>
</ul>

<p>I have read at least 20 empirical papers where this is applied either in the time-series or the cross-sectional dimension and haven't seen a mention of standard error choice. </p>
"
"0.0400480865731637","0.039253433598943"," 46523","<p>I know I'm missing something in my understanding of logistic regression, and would really appreciate any help.</p>

<p>As far as I understand it, the logistic regression assumes that the probability of a '1' outcome given the inputs, is a linear combination of the inputs, passed through an inverse-logistic function. This is exemplified in the following R code:</p>

<pre><code>#create data:
x1 = rnorm(1000)           # some continuous variables 
x2 = rnorm(1000)
z = 1 + 2*x1 + 3*x2        # linear combination with a bias
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = pr &gt; 0.5               # take as '1' if probability &gt; 0.5

#now feed it to glm:
df = data.frame(y=y,x1=x1,x2=x2)
glm =glm( y~x1+x2,data=df,family=""binomial"")
</code></pre>

<p>and I get the following error message: </p>

<blockquote>
  <p>Warning messages:
  1: glm.fit: algorithm did not converge 
  2: glm.fit: fitted probabilities numerically 0 or 1 occurred </p>
</blockquote>

<p>I've worked with R for some time now; enough to know that probably I'm the one to blame..
what is happening here?</p>
"
"0.09392108820677","0.0920574617898323"," 46821","<p>I am producing a script for creating bootstrap samples from the <code>cats</code> dataset (from the <code>-MASS-</code> package). </p>

<p>Following the Davidson and Hinkley textbook [1] I ran a simple linear regression and adopted a fundamental non-parametric procedure for bootstrapping from iid observations, namely <strong>pairs resampling</strong>.</p>

<p>The original sample is in the form:</p>

<pre><code>Bwt   Hwt

2.0   7.0
2.1   7.2

...

1.9    6.8
</code></pre>

<p>Through an univariate linear model we want to explain cats hearth weight through their brain weight. </p>

<p>The code is:</p>

<pre><code>library(MASS)
library(boot)


##################
#   CATS MODEL   #
##################

cats.lm &lt;- glm(Hwt ~ Bwt, data=cats)
cats.diag &lt;- glm.diag.plots(cats.lm, ret=T)


#######################
#   CASE resampling   #
#######################

cats.fit &lt;- function(data) coef(glm(data$Hwt ~ data$Bwt)) 
statistic.coef &lt;- function(data, i) cats.fit(data[i,]) 

bootl &lt;- boot(data=cats, statistic=statistic.coef, R=999)
</code></pre>

<p>Suppose now that there exists a clustering variable <code>cluster = 1, 2,..., 24</code> (for instance, each cat belongs to a given litter). For simplicity, suppose that data are balanced: we have 6 observations for each cluster. Hence, each of the 24 litters is made up of 6 cats (i.e. <code>n_cluster = 6</code> and <code>n = 144</code>).</p>

<p>It is possible to create a fake <code>cluster</code> variable through:</p>

<pre><code>q &lt;- rep(1:24, times=6)
cluster &lt;- sample(q)
c.data &lt;- cbind(cats, cluster)
</code></pre>

<p>I have two related questions:</p>

<p>How to simulate samples in accordance with the (clustered) dataset strucure? That is, <strong>how to resample at the cluster level?</strong> I would like to sample the clusters with replacement and to set the observations within each selected cluster as in the original dataset (i.e. sampling with replacenment the clusters and without replacement the observations within each cluster). </p>

<p>This is the strategy proposed by Davidson (p. 100). 
Suppose we draw <code>B = 100</code> samples. Each of them should be composed by 24 possibly recurrent clusters (e.g. <code>cluster = 3, 3, 1, 4, 12, 11, 12, 5, 6, 8, 17, 19, 10, 9, 7, 7, 16, 18, 24, 23, 11, 15, 20, 1</code>), and each cluster should contain the same 6 observations of the original dataset. How to do that in <code>R</code>? (either with or without the <code>-boot-</code> package.) Do you have alternative suggestions for proceeding?</p>

<p>The second question concerns the initial regression model. Suppose I adopt a <strong>fixed-effects model</strong>, with cluster-level intercepts. <strong>Does it change the resampling procedure</strong> adopted? </p>

<p>[1] Davidson, A. C., Hinkley, D. V. (1997). <em>Bootstrap methods and their applications</em>. Cambridge University press.</p>
"
"0.0400480865731637","0.039253433598943"," 47008","<p>I have a fairly simple dataset consisting of one independent variable, one dependent variable, and a categorical variable. 
I have plenty of experience running frequentist tests like <code>aov()</code> and <code>lm()</code>, but I cannot figure out how to perform their bayesian equivalents in R. </p>

<p>I would like to run a bayesian linear regression on the first two variables and a bayesian analysis of variance using the categorical variable as the groupings, but I cannot find any simple examples on how to do this with R. Can someone provide a basic example for both? Additionally, what exactly are the output statistics created by bayesian analysis and what do they express?</p>

<p>I am not very well-versed in stats, but the consensus seems to be that using basic tests with p-values is now thought to be somewhat misguided, and I am trying to keep up.
Regards.</p>
"
"0.123436492831862","0.120987207870969"," 47258","<p>I have following data stored in a file. I am applying 'glm' in R to find linear regression equation to best predict the 'output'. </p>

<pre><code>&gt; tmpData
   logOfOutput randomSample multiplied part1 part2 randNormalMean100Std20 output
1    0.0000000           33         11     1    19               89.65387      1
2    0.6931472           76         24     2    18              128.23471      2
3    1.0986123           12         39     3    17              103.70930      3
4    1.3862944           68         56     4    16               99.12617      4
5    1.6094379           50         75     5    15               95.68173      5
6    1.7917595            7         96     6    14              129.27551      6
7    1.9459101           70        119     7    13              104.59333      7
8    2.0794415           55        144     8    12              102.15247      8
9    2.1972246           20        171     9    11               72.43795      9
10   2.3025851           24        200    10    10               80.63634     10
11   2.3978953           32        231     9    11              105.03423     11
12   2.4849067           97        264     8    12               78.10613     12
13   2.5649494           28        299     7    13              107.95286     13
14   2.6390573           99        336     6    14               80.07396     14
15   2.7080502           66        375     5    15              102.01156     15
16   2.7725887           95        416     4    16              119.07361     16
17   2.8332133           42        459     3    17               64.19354     17
18   2.8903718           53        504     2    18              106.23402     18
19   2.9444390           85        551     1    19              151.07976     19
20   2.9957323           48        600     0    20               82.78324     20
</code></pre>

<p>I am using the following code to perform the same </p>

<pre><code>fn = ""delnowSample.txt""
tmpData = read.table(fn, header = TRUE,  sep= ""\t"" , blank.lines.skip = TRUE)
cnames = colnames(tmpData)
(fmla &lt;- as.formula(paste(cnames[length(cnames)], "" ~ "", paste(cnames[1:(length(cnames)-1)],collapse= ""+"")))  )
model &lt;- try(glm(formula = fmla, family=binomial(), na.action=na.omit, data=tmpData));
summary(model) 
</code></pre>

<p>The output that I get is as follow: </p>

<pre><code>&gt; summary(model)

Call:
glm(formula = as.formula(paste(dep, "" ~ "", paste(xn, collapse = ""+""))), 
    family = gaussian(), na.action = na.omit)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.37926  -0.11242  -0.03441   0.16087   0.28200  

Coefficients: (1 not defined because of singularities)
                                            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                                0.2638036  0.3078536   0.857  0.40592    
unlist(tmpData[""logOfOutput""])             0.9202273  0.2727884   3.373  0.00455 ** 
unlist(tmpData[""randomSample""])            0.0026201  0.0018177   1.441  0.17145    
unlist(tmpData[""multiplied""])              0.0288073  0.0012359  23.308 1.34e-12 ***
unlist(tmpData[""part1""])                   0.2106002  0.0403442   5.220  0.00013 ***
unlist(tmpData[""part2""])                          NA         NA      NA       NA    
unlist(tmpData[""randNormalMean100Std20""]) -0.0006214  0.0024922  -0.249  0.80673    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for gaussian family taken to be 0.04403284)

    Null deviance: 665.00000  on 19  degrees of freedom
Residual deviance:   0.61646  on 14  degrees of freedom
AIC: 1.1676

Number of Fisher Scoring iterations: 2
</code></pre>

<p>To a large extent it is predicting the Pr(z) correctly as we can see the probabilities of random variable are not significant. The R-square is also high (1-residual.deviance/null.deviance), close to 1.  </p>

<p>Question 1:
In the above data 'part1+part2' is equal to output variable. Is 'glm' not able to identify such type of relations? </p>

<p>Question 2: 
Why the degree of freedom of  null and residual deviance are different? </p>

<p>Question 3:
I need to convert the output variable into categorical variable (i.e. Everything &lt;=10 is 'no' and more than this is 'yes'). What is the best way to call 'glm', when the response variable is 'categorical'. I tried converting 'no' to '0' and 'yes' to 1, and called glm as follows:</p>

<pre><code> model &lt;- try(glm(formula = as.formula(paste(dep, "" ~ "", paste(xn, collapse= ""+""))), family=binomial(), na.action=na.omit));
</code></pre>

<p>I am getting warning message with this code. Also, I am not sure if this is the correct way to call categorical variable. </p>

<p>Edit:</p>

<p>I have the following categorical data:</p>

<pre><code>&gt; tmpData
           x1  x2 x3  y1
1  0.16294456   1  1  no
2  0.80494934   2  2  no
3  0.28962222   1  3  no
4  0.07177347   2  4  no
5  0.54830544   1  5  no
6  0.67655327   2  6  no
7  0.45189608   1  7  no
8  0.82412502   2  8  no
9  0.09076793   1  9  no
10 0.12221227   2 10  no
11 0.56751754 111 11 yes
12 0.04970992 222 12 yes
13 0.56162037 111 13 yes
14 0.96617891 222 14 yes
15 0.50994534 112 15 yes
16 0.70093692 212 16 yes
17 0.02034940 212 17 yes
18 0.78356903 121 18 yes
19 0.58439662 213 19 yes
20 0.31729282 212 20 yes
</code></pre>

<p>And the following code:</p>

<pre><code>  fn = ""delnowSample.txt""
  tmpData = read.table(fn, header = TRUE,  sep= ""\t"" , blank.lines.skip = TRUE)
  tmpData
  model &lt;- glm(formula = 'y1~x1+x2+x3', family=binomial(), na.action=na.omit, data=tmpData)
  summary(model) 
</code></pre>

<p>This one doesn't seem to be working?? </p>
"
"0.0693653206906364","0.0679889413649005"," 47302","<p>I am currently working on time series modeling, especially on stationarity tests. For this purpose, I am extensively using Pfaff's book ""<a href=""http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-75966-1"" rel=""nofollow"">Analysis of integrated and cointegrated time series with R</a>"" and I have some questions :</p>

<ol>
<li><p>On page 63, there is a nice ordinogram (Figure 3.3) explaining how all the ADF tests are related, and what should be the underlying decision tree. First of all, one needs to estimate the ADF equation with a linear trend and test for $\pi=0$ (this statistic is called <code>tau3</code> in the associated package <a href=""http://cran.r-project.org/web/packages/urca/index.html"" rel=""nofollow"">urca</a>). If we reject the null hypothesis, then there is no unit root. If we cannot reject, we test for $\beta_2=0$ given $\pi=0$ (this statistic is called <code>phi3</code> in <code>urca</code>). If we reject, then Pfaff writes ""test again for a unit root using a standardized normal"" with no further explanation.</p>

<p>Does anyone understand what he is talking about? Does this ""normal test"" appear somewhere in the urca implementation?</p></li>
<li><p>Suppose the test <code>tau3</code> for $\pi=0$ is rejected. Then the conclusion should be that there is no unit root but a trend in the series (the series is trend stationary). I have at disposal  the underlying linear regression result given by <code>ur.df()</code> from the package <code>urca</code>. Is it correct to conclude that there is actually no trend when the p-value of the t-statistic for the trend coefficient is significant? </p></li>
</ol>

<p>Thanks in advance for your help.</p>
"
"0.0693653206906364","0.0679889413649005"," 47774","<p>I am carrying a linear regression on some data. One of my variables is a factor (categorical). Using regression with an intercept leads to difficult interpretation, since one of the factor levels is taken as the intercept, and the remaining levels are given relative to that. Removing the intercept give me an effect of each level of the factor, which I what I want.</p>

<p>As far as I know, both models are precisely equivalent. They produce identical predictions (on the training set) - to within ~3e-15. However, their RÂ² scores vary wildly.</p>

<pre><code># MWE
library(car)
int &lt;- lm(fscore ~ 1 + partner.status + conformity + fcategory,
          data = Moore)  #with intercept
nint &lt;- lm(fscore ~ 0 + partner.status + conformity + fcategory,
           data = Moore) #w/o intercept
summary(int)$r.squared
summary(nint)$r.squared  #RÂ² values are not remotely the same
max(predict(int)-predict(nint)) #Predictions are essentially identical
</code></pre>

<p>Why are the models not identical? Is it because RÂ² is a comparision of the model to ""no model"", and that ""no model"" corresponds to ""y=0"" and ""y=mean(fscore)"", for nint and int, respectively?</p>
"
"0.0506572678011219","0.0496521024619361"," 47862","<p>Ok I am guessing this is a trivial question however having pondered it for a few days the only thing I have become clear on is my lack of statistical prowess. Yesterday I asked a question on fitting linear regression to a dataset piecewise and <a href=""http://stackoverflow.com/questions/14337439/piece-wise-linear-and-non-linear-regression-in-r"">this was successfully answered</a>. Now I would like to compare these fits to a lognormal fit of the whole dataset with the reason that this is a standard method for comparing such data (even if it really doesn't fit well); I basically want to highlight that it does not fit as well. So without further ado here is the data:</p>

<pre><code>x&lt;-c(1e-08, 1.1e-08, 1.2e-08, 1.3e-08, 1.4e-08, 1.6e-08, 1.7e-08, 
1.9e-08, 2.1e-08, 2.3e-08, 2.6e-08, 2.8e-08, 3.1e-08, 3.5e-08, 
4.2e-08, 4.7e-08, 5.2e-08, 5.8e-08, 6.4e-08, 7.1e-08, 7.9e-08, 
8.8e-08, 9.8e-08, 1.1e-07, 1.23e-07, 1.38e-07, 1.55e-07, 1.76e-07, 
1.98e-07, 2.26e-07, 2.58e-07, 2.95e-07, 3.25e-07, 3.75e-07, 4.25e-07, 
4.75e-07, 5.4e-07, 6.15e-07, 6.75e-07, 7.5e-07, 9e-07, 1.15e-06, 
1.45e-06, 1.8e-06, 2.25e-06, 2.75e-06, 3.25e-06, 3.75e-06, 4.5e-06, 
5.75e-06, 7e-06, 8e-06, 9.25e-06, 1.125e-05, 1.375e-05, 1.625e-05, 
1.875e-05, 2.25e-05, 2.75e-05, 3.1e-05)

y2&lt;-c(-0.169718017273307, 7.28508517630734, 71.6802510299446, 164.637259265704, 
322.02901173786, 522.719633360006, 631.977073772459, 792.321270345847, 
971.810607095548, 1132.27551798986, 1321.01923840546, 1445.33152600664, 
1568.14204073109, 1724.30089942149, 1866.79717333592, 1960.12465709003, 
2028.46548012508, 2103.16027631327, 2184.10965255236, 2297.53360080873, 
2406.98288043262, 2502.95194879366, 2565.31085776325, 2542.7485752473, 
2499.42610084412, 2257.31567571328, 2150.92120390084, 1998.13356362596, 
1990.25434682546, 2101.21333152526, 2211.08405955931, 1335.27559108724, 
381.326449703455, 430.9020598199, 291.370887491989, 219.580548355043, 
238.708972427248, 175.583544448326, 106.057481792519, 59.8876372379487, 
26.965143266819, 10.2965349811467, 5.07812046132922, 3.19125838983254, 
0.788251933518549, 1.67980552001939, 1.97695007279929, 0.770663673279958, 
0.209216903989619, 0.0117903221723813, 0.000974437796492681, 
0.000668823762763647, 0.000545308757270207, 0.000490042305650751, 
0.000468780182460397, 0.000322977916070751, 0.000195423690538495, 
0.000175847622407421, 0.000135771259866332, 9.15607623591363e-05)
</code></pre>

<p>You can see a plot of the data at the link above for quick interpretation.</p>

<p>So to my question- how to regress a lognormal curve to it? My school stats says this is not univariate data so I cannot use something like fitdistr in R. Am I right? If this is the case how might a find an approximate lognormal curve for this data if at all? Any help or pointers in a relevant direction would be great.</p>

<p>[Edit]
With reference to the negative value I am wondering whether there is an approach that can handle it? This is because I have a second set of why values as shown below that are all negative and I would like to fit a similar model to these as well.</p>

<pre><code>y3&lt;-c(0.0530500094068018, 0.160928860126123, -0.955328071740233, 
-2.53940686389203, -7.3241148240459, -8.32055726147533, -10.1979192158835, 
-12.0304091519687, -16.2527095992605, -19.9106624262052, -24.5089014234888, 
-28.7705733263437, -31.4294017506492, -35.8411743936776, -37.9005809712801, 
-40.2384353560669, -42.5902603334382, -44.6732472915729, -47.6606530197197, 
-54.2197956720375, -58.6590075712884, -61.5736755669354, -65.7179971091261, 
-63.1056155765268, -65.5404821687269, -59.2950249004724, -57.7385677458644, 
-63.3729518981994, -56.9303570422243, -69.2878310104119, -60.3990492926747, 
-14.1184714024129, -8.29495412660418, -3.44061704811896, 0.473805205298244, 
-5.47720584050456, -4.02534147802113, -2.13820456379, -0.641737730083625, 
0.25648844079225, 0.697033621376916, 0.622208830019496, 0.276775608633299, 
0.219104625574544, -0.0411577088679307, -0.0191732850594168, 
-0.0210255752601919, -0.0156084146143567, -0.00423245275017332, 
-0.000297816450447215, -1.24063266749809e-05, -1.53585832955629e-05, 
-2.32966128710771e-05, -2.39386641905819e-05, -2.15491949944693e-05, 
-1.50167366691665e-05, -8.3066700184436e-06, -7.89314461608438e-06, 
-6.20937293357605e-06, -3.64313623751525e-06)
</code></pre>
"
"NaN","NaN"," 47870","<p>I have a non-linear reglationship and I want to find the best way to determine the value for the exponent $\gamma$ in the following regression:</p>

<p>$y = \beta x ^ \gamma$</p>

<p>I would preferably like to do this in R.</p>
"
"0.0578044339088637","0.0566574511374171"," 47947","<p>I have a dataset containing information about a bunch of wireless devices.  Specifically, the number of other wireless devices each device encounters.  Based on other researcher's prior work in the same area, I have reason to believe my distribution may potentially be well fitted by the not so well known <a href=""ftp://netlib.bell-labs.com/who/nuzman/papers/nssw_TCP.pdf"" rel=""nofollow"">biPareto distribution</a>, whose CCDF is defined as:</p>

<p>$1 - F(x) = (x/k)^{-\alpha}(\frac{x+kb}{k + kb})^{\alpha-\beta}$, for $ x &gt; k$</p>

<p>and simply </p>

<p>$1 - F(x) = 1$, for $x \le k$.</p>

<p>Assume I already know $k = 1$.  This leaves a function to fit with the three parameters $\alpha$,$b$,$\beta$, with the restriction that each parameter is $&gt; 0$.</p>

<p>What I am unsure about is how one would go about determining the most suitable fitting parameter values, ideally in <code>R</code>.  </p>

<p>I am quite the novice when it comes to fitting data, so aside from the syntactical considerations in <code>R</code>, I am also interested in any advice regarding what specific methods of fitting would be most appropriate for this particular function.  For example:</p>

<ol>
<li>Does this function qualify for linear regression?</li>
<li>Is least squares appropriate/applicable here?  What about minimum mean squared error (MMSE)?</li>
<li>How sensitive is any given fitting method to any ""guiding"" initial parameter values I give it (iterative methods?), or is the result determined analytically (always produces the same answer in the end?) </li>
</ol>

<p>If it's at all relevant, I'm ultimately looking to use the Kolmogorov-Smirnov test on the data to see how well it matches the parameterized biPareto distribution.</p>
"
"0.217771872018751","0.21689349395672"," 48040","<p>I'm trying to test the significance of the ""component"" effect in a multivariate regression model. I'm not sure what is the right way. Using R, I have tried a way with <code>lm()</code> and another way with <code>gls()</code>, and they don't yield compatible results. </p>

<p><strong>Please note that this is not a question about which methodology is the right one to use to analyze my data. By the way these are simulated data. My question is about the understanding in mathematical terms of the R procedures I use.</strong></p>

<p>The dataset:</p>

<pre><code>&gt; str(dat)
'data.frame':   31 obs. of  5 variables:
 $ group: Factor w/ 5 levels ""1"",""5"",""2"",""3"",..: 1 1 1 1 1 1 1 1 3 3 ...
     $ id   : Factor w/ 8 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 1 3 ...
 $ x    : num  2.5 3 3 4 1.2 3.8 3.9 4 2.5 2.9 ...
     $ y    : num  2.6 3.8 3.9 3.8 1.6 5.2 1.3 3.6 4 3.2 ...
 $ z    : num  3.1 3.6 4.9 3.8 2.1 6 2.1 2.9 4.2 2.9 ...
&gt; head(dat,10)
   group id   x   y   z
1      1  1 2.5 2.6 3.1
2      1  2 3.0 3.8 3.6
3      1  3 3.0 3.9 4.9
4      1  4 4.0 3.8 3.8
5      1  5 1.2 1.6 2.1
6      1  6 3.8 5.2 6.0
7      1  7 3.9 1.3 2.1
8      1  8 4.0 3.6 2.9
9      2  1 2.5 4.0 4.2
10     2  3 2.9 3.2 2.9
</code></pre>

<p>I convert this dataset into ""long format"" for graphics (and later for <code>gls()</code>):</p>

<pre><code>dat$subject &lt;- dat$group : dat$id
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

xyplot(value ~ component | group, data=dat.long, 
    pch=16, 
    strip = strip.custom(strip.names=TRUE,var.name=""group"" ), layout=c(5,1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/14KtA.png"" alt=""enter image description here""></p>

<p>Each individual of each group has $3$ repeated measures $x$,$y$,$z$ (I should join the points in the graphic to see the repeated measures).</p>

<p>I want to fit a MANOVA model using group as factor and $(x,y,z)$ is the multivariate response:
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right), \quad i=1,\ldots,5
$$
(of course we could use the default R parameterization $\mu_{ik}=\mu_{1k} + \alpha_{ik}$ by considering <code>group1</code>as the ""intercept"" for each response but I prefer ""my"" parameterization).</p>

<p>This model is fitted as follows using <code>lm()</code>:</p>

<pre><code>###  multivariate least-squares fitting  ###
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )
</code></pre>

<p>I think the model can also  be fitted with <code>gls()</code> as follows (but with a different fitting procedure) :</p>

<pre><code>### generalized least-squares fitting  ###
library(nlme)
gfit &lt;- gls(value ~ group*component, data=dat.long, correlation=corSymm(form= ~ 1 | subject))
</code></pre>

<p>Recall that <code>subject = group:id</code> is the identifier of the individuals. The <code>correlation=corSymm(form= ~ 1 | subject)</code> argument means that the responses $x$, $y$, $z$ for each individual are correlated. Here <code>corSymm</code> means a general, ""unrestricted"",  covariance structure (termed as ""unstructured"" in SAS language).</p>

<p>To check that <code>mfit</code> and <code>gfit</code> are equivalent, we can check for instance that we can deduce the estimated parameters of <code>mfit</code> from the estimated parameters of <code>gfit</code>and vice-versa (so the ""mean"" parameters have exactly the same fitted values):</p>

<pre><code>&gt; coef(mfit)
                  x          y          z
(Intercept)  3.1750  3.2250000  3.5625000
group5      -0.9500 -0.4750000  0.1125000
group2      -1.0750 -0.5678571 -0.2339286
group3      -0.7875 -0.1000000  0.1875000
group4      -0.3750  0.4000000 -0.0125000
&gt; coef(gfit)
      (Intercept)            group5            group2            group3 
        3.1750000        -0.9500000        -1.0750000        -0.7875000 
           group4        componenty        componentz group5:componenty 
       -0.3750000         0.0500000         0.3875000         0.4750000 
group2:componenty group3:componenty group4:componenty group5:componentz 
        0.5071429         0.6875000         0.7750000         1.0625000 
group2:componentz group3:componentz group4:componentz 
        0.8410714         0.9750000         0.3625000 
</code></pre>

<p>Now I want to test the ""component effect"". Rigorously speaking, writing the model as 
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right),
$$
I want to test the hypothesis $\boxed{H_0\colon \{\mu_{1i}=\mu_{2i}=\mu_{3i} \quad \forall i=1,2,3,4,5 \}}$.</p>

<p>Below are my attempts, one attempt with <code>gfit</code> and two attempts with <code>mfit()</code>:</p>

<pre><code>###########################################
## testing significance of the component ##
###########################################

&gt; ### with gfit  ###
&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
&gt; 
&gt; ### with mfit ###
&gt; library(car)
&gt; 
&gt; # first attempt : 
&gt; idata &lt;- data.frame(component=c(""x"",""y"",""z""))
&gt; ( av.ok &lt;- Anova(mfit, idata=idata, idesign=~component, type=""III"") )

Type III Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.84396  140.625      1     26 5.449e-12 ***
group            4   0.10369    0.752      4     26    0.5658    
component        1   0.04913    0.646      2     25    0.5328    
group:component  4   0.22360    0.818      8     52    0.5901    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; 
&gt; # second attempt :
&gt; linearHypothesis(mfit, ""(Intercept) = 0"", idata=idata, idesign=~component, iterms=""component"")

 Response transformation matrix:
  component1 component2
x          1          0
y          0          1
z         -1         -1

Sum of squares and products for the hypothesis:
           component1 component2
component1    1.20125    1.04625
component2    1.04625    0.91125

Sum of squares and products for error:
           component1 component2
component1   31.46179   14.67696
component2   14.67696   21.42304

Multivariate Tests: 
                 Df test stat  approx F num Df den Df  Pr(&gt;F)
Pillai            1 0.0491253 0.6457903      2     25 0.53277
Wilks             1 0.9508747 0.6457903      2     25 0.53277
Hotelling-Lawley  1 0.0516632 0.6457903      2     25 0.53277
Roy               1 0.0516632 0.6457903      2     25 0.53277
</code></pre>

<p>With <code>anova(gfit)</code> the component is significant, but not with my two attempts using <code>mfit</code> and the <code>car</code> package. </p>

<p>I know that <code>gls()</code> use a different fitting method than <code>lm()</code> but this is surely not the cause of the difference. </p>

<p>So my questions are :</p>

<ul>
<li>did I do something wrong ?</li>
<li>which method tests my $H_0$ hypothesis ?</li>
<li>what is the $H_0$ hypothesis of the other methods ?</li>
</ul>

<p>And I have an auxiliary question: how to get $\hat\Sigma$ with <code>mfit</code> and <code>gfit</code> ?</p>

<h2>Update 1</h2>

<p>Below is a reproducible example which simulates the dataset. 
Now I think I understand : both ANOVA methods are correct (the first one with <code>anova(gfit)</code> and the second one with <code>Anova(mfit, ...)</code>, <strong>and they yield very close results when using the type II sum of squares in <code>Anova(mfit, ...)</code></strong>.  For the above example: </p>

<pre><code>&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>is very close to </p>

<pre><code>&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>

<p>Below is the reproducible code with the data sampler (I simulate uncorrelated repeated measures but it suffices to include a covariance matrix in the <code>rmvnorm()</code> function to simulate correlated repeated measures) :</p>

<pre><code>library(mvtnorm)
library(nlme)
library(car)

# set data parameters 
I &lt;- 5 # number of groups
J &lt;- 16 # number of individuals per group
dat &lt;- data.frame(
    group = gl(I,J),
    id = gl(J,1,I*J),
    x=NA, 
    y=NA, 
    z=NA
)
Mu &lt;- c(1:I) # group means of components (assuming E(x)=E(y)=E(z) in each group)

# simulates data: 
for(i in 1:I){
    which.group.i &lt;- which(dat$group==i)
    dat[which.group.i,c(""x"",""y"",""z"")] &lt;- round(rmvnorm(n=J, mean=rep(Mu[i],3)), 1)
}

dat$subject &lt;- droplevels( dat$group : dat$id )
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

# multivariate least-squares fitting 
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )

# gls fitting
dat.long$order.xyz &lt;- as.numeric(dat.long$component)
gfit &lt;- gls(value ~ group*component , data=dat.long, correlation=corSymm(form=  ~ order.xyz | subject)) 

# compares ANOVA : 
anova(gfit)
idata &lt;- data.frame(component=c(""x"",""y"",""z""))
Anova(mfit, idata=idata, idesign=~component, type=""II"")
Anova(mfit, idata=idata, idesign=~component, type=""III"")
</code></pre>

<p>So now I wonder which type of sum of squares is the more appropriate one for my real study... but this is another question</p>

<h2>Update 2</h2>

<p>About my question <em>""how to get $\hat\Sigma$""</em>, here is the answer for <code>gls()</code>:</p>

<pre><code>&gt; getVarCov(gfit)
Marginal variance covariance matrix
        [,1]    [,2]    [,3]
[1,] 0.92909 0.47739 0.24628
[2,] 0.47739 0.92909 0.53369
[3,] 0.24628 0.53369 0.92909
  Standard Deviations: 0.96389 0.96389 0.96389 
</code></pre>

<p>That shows that <strong><code>mfit</code>and <code>gfit</code> were not equivalent models</strong>: <code>gfit</code>assumes the same variance for the three components.</p>

<p>In order to fit a fully unrestricted covariance matrix for the repeated measures, we have to type:</p>

<pre><code>gfit2 &lt;- gls(value ~ group*component , data=dat.long, 
    correlation=corSymm(form=  ~ 1 | subject), 
    weights=varIdent(form = ~1 | component))

&gt; summary(gfit2)
Generalized least squares fit by REML
  Model: value ~ group * component 
  Data: dat.long 
       AIC      BIC    logLik
  264.0077 313.4986 -111.0038

Correlation Structure: General
 Formula: ~1 | subject 
 Parameter estimate(s):
 Correlation: 
  1     2    
2 0.529      
3 0.300 0.616
Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | component 
 Parameter estimates:
       x        y        z 
1.000000 1.253534 1.169335 

....

Residual standard error: 0.8523997 
</code></pre>

<p>But yet I don't understand the extracted covariance matrix given by <code>getVarCov()</code> (but this is not important since we get this matrix with <code>summary(gfit2)</code>): </p>

<pre><code>   &gt; getVarCov(gfit2)
    Error in t(S * sqrt(vars)) : 
      dims [product 9] do not match the length of object [0]
    &gt; getVarCov(gfit2, individual=""1:1"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 0.72659 0.48164 0.25500
    [2,] 0.48164 1.14170 0.65562
    [3,] 0.25500 0.65562 0.99349
      Standard Deviations: 0.8524 1.0685 0.99674 
    &gt; getVarCov(gfit2, individual=""1:2"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 1.14170 0.56319 0.27337
    [2,] 0.56319 0.99349 0.52302
    [3,] 0.27337 0.52302 0.72659
      Standard Deviations: 1.0685 0.99674 0.8524 
</code></pre>

<p>Unfortunately, the <code>anova(gfit2)</code> table is not as close to <code>Anova(mfit, ..., type=""II"")</code> as <code>anova(gfit)</code>:</p>

<pre><code>&gt; anova(gfit2)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 498.1744  &lt;.0001
group               4   1.0514  0.3864
component           2  13.1801  &lt;.0001
group:component     8   0.8310  0.5780

&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>
"
"0.103022852774434","0.11444244151147"," 48381","<p>I'm trying to estimate power in a logistic regression with a continuous exposure in a cohort study (ie, the ratio of the sampling probabilities is 1). I have population cumulative incidence (probability) and population exposure variability and exposure mean and an expected odds ratio. I also have a total sample size.</p>

<p>I'm using R and it seems like <code>Hmisc::bpower</code> is only for logistic regression with binary exposure and I can't seem to find any packages that estimate binomial power with continuous exposure.</p>

<p>I've attempted the following simulation but it's quite slow given my total sample size and I'm not sure if it's right:</p>

<pre><code>p &lt;- vector()
betahat &lt;- vector()
for(i in 1:1000){
n &lt;- 40000  #total sample size
intercept = log(0.008662265)  #where exp(intercept) = P(D=1)
beta &lt;- log(1.4) #where exp(beta)=OR corresponding to a one unit change in xtest
xtest &lt;- rnorm(n,1.2,.31)  #xtest is vector length 40,000 with mean 1.2 and sd .31
linpred &lt;- intercept + xtest*beta #linear predictor
prob &lt;- exp(linpred)/(1 + exp(linpred)) #link function
runis &lt;- runif(n,0,1) #generate a vector length n from a uniform distribution 0,1
ytest &lt;- ifelse(runis &lt; prob,1,0)  #if a random value from a uniform distribution 0,1 is less than prob, then the outcome is 1.  otherwise the outcome is 0
coefs &lt;- coef(summary(glm(ytest~xtest, family=""binomial"")))  #run a logistic regression
p[i] &lt;- coefs[2,4] #store the p value
betahat[i] &lt;- coefs[2,1] #store the unexponentiated betahat
}

mean(p &lt; .05)
#power

exp(mean(betahat))
#sanity check, should equal 1.4--it does
</code></pre>

<p>Is there anything wrong with this approach?</p>

<p>One concern of mine is that the cumulative incidence (ie, probability of event over the given time period) comes from a population that did not have 0 exposure.  In fact, it's reasonable to assume that the value i'm using for an intercept is actually from a population that has an exposure variability similar to mine. In that case, how would I estimate the unexposed probability given an odds ratio (and other information that I would find in say, a published paper) to use in my power calculation?</p>
"
"0.02831827358943","0.0277563690826684"," 48777","<p>I need to automate the transformation on some linear regression models. There is only one predictor in this case. Sometimes i get a good model with the original variables, sometimes i need to log the predictor, and in some cases log both sides.</p>

<p>I'm using R, so what kind of tests/packages can i use to automate this? I'm using Pearson correlation now, but i'm not sure if it makes sense.</p>

<p>thanks!</p>

<p>PS: This may look a duplicate question, but i couldn't find yet the methodology to apply.</p>
"
"0.0400480865731637","0.039253433598943"," 48854","<p>I have linear regression code written in R and I have to do the same thing in Java. I used <a href=""http://commons.apache.org/math/apidocs/overview-summary.html"" rel=""nofollow"">Apache Commons math</a> library for this. I used the same data in R code and in Java code, but I got different intercept value. I could not figure out what stupid thing I have done in the code.</p>

<p><strong>R Code:</strong></p>

<pre><code>test_trait &lt;- c( -0.48812477 , 0.33458213, -0.52754476, -0.79863471, -0.68544309, -0.12970239,  0.02355622, -0.31890850,0.34725819 , 0.08108851)
geno_A &lt;- c(1, 0, 1, 2, 0, 0, 1, 0, 1, 0)
geno_B &lt;- c(0, 0, 0, 1, 1, 0, 0, 0, 0, 0) 
fit &lt;- lm(test_trait ~ geno_A*geno_B)
fit
</code></pre>

<p><strong>R Output</strong>:</p>

<pre><code>Call:
lm(formula = test_trait ~ geno_A * geno_B)

Coefficients:
  (Intercept)         geno_A         geno_B  geno_A:geno_B  
    -0.008235      -0.152979      -0.677208       0.096383 
</code></pre>

<p><strong>Java Code (includes EDIT1):</strong></p>

<pre><code>package linearregression;
import org.apache.commons.math3.stat.regression.SimpleRegression;
public class LinearRegression {
    public static void main(String[] args) {

        double[][] x = {{1,0},
                        {0,0},
                        {1,0},
                        {2,1},
                        {0,1},
                        {0,0},
                        {1,0},
                        {0,0},
                        {1,0},
                        {0,0}
        };

        double[]y = { -0.48812477,
                       0.33458213,
                      -0.52754476,
                      -0.79863471,
                      -0.68544309,
                      -0.12970239,
                       0.02355622,
                      -0.31890850,
                       0.34725819,
                       0.08108851
        };
        SimpleRegression regression = new SimpleRegression(true);
        regression.addObservations(x,y);

        System.out.println(""Intercept: \t\t""+regression.getIntercept());
// EDIT 1 -----------------------------------------------------------
System.out.println(""InterceptStdErr: \t""+regression.getInterceptStdErr());
System.out.println(""MeanSquareError: \t""+regression.getMeanSquareError());
System.out.println(""N: \t\t\t""+regression.getN());
System.out.println(""R: \t\t\t""+regression.getR());
System.out.println(""RSquare: \t\t""+regression.getRSquare());
System.out.println(""RegressionSumSquares: \t""+regression.getRegressionSumSquares());
System.out.println(""Significance: \t\t""+regression.getSignificance());
System.out.println(""Slope: \t\t\t""+regression.getSlope());
System.out.println(""SlopeConfidenceInterval: ""+regression.getSlopeConfidenceInterval());
System.out.println(""SlopeStdErr: \t\t""+regression.getSlopeStdErr());
System.out.println(""SumOfCrossProducts: \t""+regression.getSumOfCrossProducts());
System.out.println(""SumSquaredErrors: \t""+regression.getSumSquaredErrors());
System.out.println(""XSumSquares: \t\t""+regression.getXSumSquares());
// EDIT1 ends here --------------------------------------------------

    }
}
</code></pre>

<p><strong>Java Output:</strong></p>

<pre><code>Intercept:      -0.08732359363636362
</code></pre>

<p><strong>Java Output of EDIT1:</strong></p>

<pre><code>Intercept:      -0.08732359363636362
InterceptStdErr:    0.17268454347538026
MeanSquareError:    0.16400973355415271
N:          10
R:          -0.3660108396736771
RSquare:        0.13396393475863017
RegressionSumSquares:   0.20296050132281976
Significance:       0.2982630977579106
Slope:          -0.21477287227272726
SlopeConfidenceInterval: 0.4452137360615129
SlopeStdErr:        0.193067188937234
SumOfCrossProducts:     -0.945000638
SumSquaredErrors:   1.3120778684332217
XSumSquares:        4.4
</code></pre>

<p>I will greatly appreciate your help. Thanks !</p>
"
"0.0506572678011219","0.0620651280774201"," 49141","<p>My predictions coming from a logistic regression model (glm in R) are not bounded between 0 and 1 like I would expected. My understanding of logistic regression is that your input and model parameters are combined linearly and the response is transformed into a probability using the logit link function. Since the logit function is bounded between 0 and 1, I expected my predictions to be bounded between 0 and 1.</p>

<p>However that's not what I see when I implement logistic regression in R:</p>

<pre><code>data(iris)
iris.sub &lt;- subset(iris, Species%in%c(""versicolor"",""virginica""))
model    &lt;- glm(Species ~ Sepal.Length + Sepal.Width, data = iris.sub, 
                family = binomial(link = ""logit""))
hist(predict(model))
</code></pre>

<p><img src=""http://i.stack.imgur.com/0BHU5.png"" alt=""enter image description here""></p>

<p>If anything the output of predict(model) looks normal to me. Can anyone explain to me why the values I get are not probabilities?</p>
"
"0.0424774103841449","0.0416345536240027"," 49506","<p>I'm running a series of permutation tests inside 2 for loops (2 for loops will calculate some new data and then  this data will be shuffled and then i apply a linear regression based on the shuffled data). Here is my code:</p>

<pre><code>for(i in seq(1,12000,200))
{
    for j in seq(1, 12000, 200))
       {
            for(ind in 1:nrow(df))
            {
            ###calculate something (called X) depending on i,j, ind and df
            }
      for (i in 1:100) # 100 sample
        {
                sample(df$X,nrow(df), replace=FALSE) 
                 fit=lm(X ~ Y, df)   #linear regression calculation
                 regerror=sum(residuals(fit)^2)
             res &lt;- rbind(res, data.frame(shufflederr = regerror))
                 }
        }
}
</code></pre>

<p>The p-value is calculated based on the sum squared error of the linear regression in R which is:</p>

<pre><code>&gt; sum(res$shufflederr &lt; observed$regerror)    #observed df contain the same error value but without shuffling
</code></pre>

<p>I have 3 questions:</p>

<ol>
<li>Can a p-value of a pair <code>(i,j</code>) be equal to 0 ??is this statistically possible ?</li>
<li>Is there any optimal way to create a good cutoff of the <code>p-value</code> ?</li>
<li>Is there any better idea to do the permutation(or any other significance) test in my case ?</li>
</ol>
"
"0.0800961731463273","0.0654223893315716"," 49543","<p>I have sampled 8 individuals (birds) from two regions. For each of these 16 individuals I have sampled 9 feathers that have each grown in sequential order (from 1 to 9). Next I have measure both carbon and nitrogen isotopes in each of the feather samples. I have plotted (Fig. 1) my data and in some individuals the relationship between the isotope value and feather position is linear, in some cases monotonic, and in a few cases neither.</p>

<p><img src=""http://i.stack.imgur.com/Nz6oi.png"" alt=""Delta15N by feather position for all individuals in the &quot;South&quot; region""></p>

<p>I am looking for a non-parametric (?) method to test these three alternative hypotheses for each individual in my dataset.</p>

<p>H1: If the 9 sequentially grown feathers on each individual are grown in the same place under the same diet, the isotope values will be highly correlated with feather position and the regression line essentially flat.</p>

<p>H2: If an individual moves or changes their diet in a systematic way, the isotope values and feather position will correlated but the regression line will be either positive or negative.</p>

<p>H3: If the individual abruptly moves or changes their diet, the isotope values will be uncorrelated and the relationship will not be linear.</p>

<p>Finally, I would like to test the hypothesis that their are differences between the relationship of feather isotopes and feather position is different in individuals between the two regions.</p>

<p>This would more intuitive if all the data (for each individual) was linear or monotonic, but they are not. From my nascent understanding for using either a Pearson's correlation or GLM, these tests assume the data is linear, while the Spearman's assumes the data is monotonic.</p>

<p>Sample Data:</p>

<pre><code>WW_Wing_SI &lt;- structure(list(Individual_ID = c(""WW_08A_02"", ""WW_08A_02"", ""WW_08A_02"", 
""WW_08A_02"", ""WW_08A_02"", ""WW_08A_02"", ""WW_08A_02"", ""WW_08A_02"", 
""WW_08A_02"", ""WW_08A_03"", ""WW_08A_03"", ""WW_08A_03"", ""WW_08A_03"", 
""WW_08A_03"", ""WW_08A_03"", ""WW_08A_03"", ""WW_08A_03"", ""WW_08A_03"", 
""WW_08A_04"", ""WW_08A_04"", ""WW_08A_04"", ""WW_08A_04"", ""WW_08A_04"", 
""WW_08A_04"", ""WW_08A_04"", ""WW_08A_04"", ""WW_08A_04"", ""WW_08A_05"", 
""WW_08A_05"", ""WW_08A_05"", ""WW_08A_05"", ""WW_08A_05"", ""WW_08A_05"", 
""WW_08A_05"", ""WW_08A_05"", ""WW_08A_05"", ""WW_08A_06"", ""WW_08A_06"", 
""WW_08A_06"", ""WW_08A_06"", ""WW_08A_06"", ""WW_08A_06"", ""WW_08A_06"", 
""WW_08A_06"", ""WW_08A_06"", ""WW_08A_08"", ""WW_08A_08"", ""WW_08A_08"", 
""WW_08A_08"", ""WW_08A_08"", ""WW_08A_08"", ""WW_08A_08"", ""WW_08A_08"", 
""WW_08A_08"", ""WW_08A_09"", ""WW_08A_09"", ""WW_08A_09"", ""WW_08A_09"", 
""WW_08A_09"", ""WW_08A_09"", ""WW_08A_09"", ""WW_08A_09"", ""WW_08A_09"", 
""WW_08A_13"", ""WW_08A_13"", ""WW_08A_13"", ""WW_08A_13"", ""WW_08A_13"", 
""WW_08A_13"", ""WW_08A_13"", ""WW_08A_13"", ""WW_08A_13"", ""WW_08B_02"", 
""WW_08B_02"", ""WW_08B_02"", ""WW_08B_02"", ""WW_08B_02"", ""WW_08B_02"", 
""WW_08B_02"", ""WW_08B_02"", ""WW_08B_02"", ""WW_08G_01"", ""WW_08G_01"", 
""WW_08G_01"", ""WW_08G_01"", ""WW_08G_01"", ""WW_08G_01"", ""WW_08G_01"", 
""WW_08G_01"", ""WW_08G_01"", ""WW_08G_02"", ""WW_08G_02"", ""WW_08G_02"", 
""WW_08G_02"", ""WW_08G_02"", ""WW_08G_02"", ""WW_08G_02"", ""WW_08G_02"", 
""WW_08G_02"", ""WW_08G_05"", ""WW_08G_05"", ""WW_08G_05"", ""WW_08G_05"", 
""WW_08G_05"", ""WW_08G_05"", ""WW_08G_05"", ""WW_08G_05"", ""WW_08G_05"", 
""WW_08G_07"", ""WW_08G_07"", ""WW_08G_07"", ""WW_08G_07"", ""WW_08G_07"", 
""WW_08G_07"", ""WW_08G_07"", ""WW_08G_07"", ""WW_08G_07"", ""WW_08I_01"", 
""WW_08I_01"", ""WW_08I_01"", ""WW_08I_01"", ""WW_08I_01"", ""WW_08I_01"", 
""WW_08I_01"", ""WW_08I_01"", ""WW_08I_01"", ""WW_08I_03"", ""WW_08I_03"", 
""WW_08I_03"", ""WW_08I_03"", ""WW_08I_03"", ""WW_08I_03"", ""WW_08I_03"", 
""WW_08I_03"", ""WW_08I_03"", ""WW_08I_07"", ""WW_08I_07"", ""WW_08I_07"", 
""WW_08I_07"", ""WW_08I_07"", ""WW_08I_07"", ""WW_08I_07"", ""WW_08I_07"", 
""WW_08I_07"", ""WW_08I_12"", ""WW_08I_12"", ""WW_08I_12"", ""WW_08I_12"", 
""WW_08I_12"", ""WW_08I_12"", ""WW_08I_12"", ""WW_08I_12"", ""WW_08I_12""
), Feather = c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", 
""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", 
""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", 
""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", 
""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", 
""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", 
""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", 
""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", 
""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", 
""6"", ""7"", ""8"", ""9"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9""
), Delta13C = c(-18.67, -19.16, -20.38, -20.96, -21.61, -21.65, 
-21.31, -20.8, -21.28, -20.06, -20.3, -21.21, -22.9, -22.87, 
-21.13, -20.68, -20.58, -20.69, -16.54, -15.6, -16.61, -19.65, 
-20.98, -21.18, -21.7, -21.18, -21.33, -20.33, -20.28, -20.58, 
-20.8, -21.24, -20.94, -20.54, -21.04, -20.42, -21.28, -21.24, 
-21.22, -21.2, -21.47, -21.23, -21.89, -21.89, -21.6, -23.86, 
-23.95, -24, -24.16, -24.93, -24.93, -24.48, -24.17, -23.1, -21.3, 
-21.44, -21.49, -21.49, -21.1, -20.84, -20.78, -21.58, -20.76, 
-21.34, -24.13, -23.03, -21.77, -21.4, -21.57, -21.45, -21.32, 
-21.59, -20.87, -20.95, -20.76, -20.9, -21.02, -20.84, -21.11, 
-20.64, -20.11, -20.32, -20.02, -19.92, -20.05, -20.23, -20.73, 
-20.91, -19.87, -19.58, -19.35, -19.38, -19.7, -19.94, -20.43, 
-20.08, -20.81, -20.9, -19.24, -21.2, -21.29, -21.85, -22.22, 
-22.34, -22.42, -22.69, -22.75, -22.73, -21.61, -21.42, -21.84, 
-21.68, -21.79, -21.49, -21.88, -21.62, -21.54, -18.3, -18.53, 
-19.55, -20.18, -20.96, -21.08, -21.5, -17.42, -13.18, -22.3, 
-22.2, -22.18, -22.14, -21.55, -20.85, -23.1, -20.75, -20.9, 
-21.6, -21.77, -22.17, -22.21, -22.24, -22.47, -22.19, -21.89, 
-21.89, -24.12, -24.08, -24, -24.2, -24.16, -22.87, -22.51, -22.12, 
-22.3), Delta15N = c(7.35, 7.27, 7.23, 7.07, 7.13, 7.38, 6.98, 
6.88, 6.72, 5.72, 5.76, 5.51, 6.12, 5.8, 5.34, 5.47, 5.78, 6.2, 
7.33, 7.45, 7.3, 7.19, 7.56, 7.54, 8.12, 7.71, 7.44, 9.45, 9.81, 
9.7, 9.08, 8.6, 9.34, 10.38, 9.67, 10.48, 7.71, 7.76, 7.95, 7.73, 
7.69, 7.24, 6.64, 6.42, 7.31, 8.26, 8.1, 8.07, 8.7, 8.98, 9.44, 
7.84, 7.26, 6.05, 8.04, 7.73, 7.55, 6.77, 6.99, 6.84, 7.09, 6.78, 
7.07, 6.96, 6, 5.91, 6.48, 7.06, 7.27, 8.32, 7.85, 7.45, 6.9, 
6.73, 6.97, 6.67, 6.76, 6.59, 6.58, 6.42, 6.3, 11.64, 11.83, 
11.66, 11.3, 11.32, 11.29, 10.91, 10.77, 11.4, 9.5, 9.55, 9.22, 
8.84, 8.89, 9.14, 9.8, 9.13, 8.51, 7.7, 7.8, 8.29, 9.65, 10.25, 
13.67, 14.66, 13.48, 13.76, 8.7, 8.7, 8.36, 8.11, 8.47, 8.13, 
6.88, 7.21, 7.16, 14.07, 13.91, 14.07, 14.26, 13.99, 13.51, 13.77, 
14.83, 15.13, 10.93, 10.85, 11.31, 11.28, 11.96, 13.41, 8.12, 
12.96, 12.03, 8.16, 8.29, 8.43, 8.53, 8.1, 7.65, 7.6, 7.51, 7.38, 
6.44, 6.18, 6.33, 6.49, 6.34, 8.65, 7.73, 7.13, 7.07), Region = c(""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""South"", ""South"", ""South"", ""South"", 
""South"", ""South"", ""South"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"", ""North"", ""North"", 
""North"", ""North"", ""North"", ""North"", ""North"")), .Names = c(""Individual_ID"", 
""Feather"", ""Delta13C"", ""Delta15N"", ""Region""), row.names = c(NA, 
153L), class = ""data.frame"")
</code></pre>
"
"0.0849548207682898","0.0832691072480053"," 49638","<p>I have written an R script for obtaining bootstrapped standard errors in the linear regression setting.</p>

<p>In practice, first in a model building step I select the final model to be applied at each bootstrapped sample (for simplicity suppose that it is a simple univariate linear model).</p>

<p>Then I simulate <code>B = 1000</code> bootstrap samples (with replacement from the initial true dataset). For each simulated sample I fit the initially defined univariate regression model to the simulated data and store the estimated coefficient of the covariate of interest in <code>b_x</code>.</p>

<p>Suppose now for simplicity that <code>B = 10</code>. At the end of the <code>B</code> simulations, I obtain the output matrix <code>boot_out</code> in this form:</p>

<pre><code>boot_out &lt;- read.table(text = ""

iter      b_x
    1       1.19
    2       0.81
    3       1.21
    4       1.05
    5       0.99
    6       1.11
    7       1.09
    8       0.88
    9       0.91
    10      1.12"",

header=TRUE)
</code></pre>

<p>Now I need to compute the bootstrapped standard error of the effect of the regressor of interest, call it <code>se_boot</code>.</p>

<p>To this aim, I used:</p>

<pre><code>se_boot &lt;- sd(boot_out[,""b_x""])
se_boot
</code></pre>

<p>but I get a unexpected result: the bootstrapped standard error of <code>b_x</code> is <strong>higher</strong> than the standard error estimated through the initial model fitted to the true data..</p>

<p>By looking at my <code>b_x</code> values I found that the estimates of the partial effect of interest vary within a wide range, but I guessed that 1000 bootstrap replications were sufficient for refining the estimated standard error of <code>b_x</code>..</p>

<p>Before trying to do even more replications, could you please tell me whether the above passages are right?</p>

<p>Thanks a lot for any help.</p>
"
"0.02831827358943","0.0277563690826684"," 49666","<p>I have a linear regression with two explanatory variables. One of them is a two level categorical variable. When I perform the fit, R tells me that the intercept is highly non significant. When I force the intercept to be zero in the regression, it tells me that the coefficient for the 0-level of the categorical variable is highly non significant. What should I do?</p>
"
"NaN","NaN"," 49939","<p>What is the meaning of <code>t value</code> and <code>Pr(&gt;|t|)</code> when using <code>summary()</code> function on linear regression model in R?</p>

<pre><code>Coefficients:
                              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                    10.1595     1.3603   7.469 1.11e-13 ***
log(var)                        0.3422     0.1597   2.143   0.0322 *
</code></pre>
"
"0.136722856160266","0.134009936927703"," 50086","<p>Assume for example a trivariate Gaussian model:
$$
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \quad (*)
$$
with ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$. </p>

<p>The Bayesian conjugate theory of this model is well known. 
This model is the most simple case of a  multivariate linear regression model. 
And more generally, there is a well known Bayesian conjugate theory of multivariate linear regression, which is  the extension to the case when  the multivariate mean  ${\boldsymbol \mu}= {\boldsymbol \mu}(x_i)$ is allowed to depend on the covariates $x_i$ of individual $i \in \{1, \ldots, n \}$, with linear constraints about the multivariate means ${\boldsymbol \mu}(x_i)$. See for instance <a href=""http://books.google.be/books?id=GL8VS9i_B2AC&amp;dq=bayesian%20econometrics%20bayesm&amp;hl=fr&amp;source=gbs_navlinks_s"" rel=""nofollow"">Rossi &amp; al's book</a> accompanied by the crantastic <a href=""http://cran.r-project.org/web/packages/bayesm/index.html"" rel=""nofollow""><code>bayesm</code></a> package for <code>R</code>. 
We know in addition that the Jeffreys prior is a limit form of the conjugate prior  distributions.</p>

<p>Let us come back to the simple multivariate Gaussian model $(*)$. Instead of generalizing this model to the case of linearly dependent multivariate means ${\boldsymbol \mu}(x_i)$ depending on individuals $i=1,\ldots,n$, we can consider a more restrictive model by assuming linear constraints about the components $\mu_1$, $\mu_2$, $\mu_3$ of the multivariate mean ${\boldsymbol \mu}$. </p>

<h3>Example</h3>

<p>Consider some concentrations $x_{i,t}$ of $4$ blood samples $i=1,2,3,4$ measured at $3$ timepoints $t=t_1,t_2,t_3$. 
Assume that the samples are independent and that the series of the three measurements 
$(x_{i,t_1},x_{i,t_2},x_{i,t_3}) \sim {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right)$ for each sample $i$ with a mean 
${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$ whose three components are 
linearly related to the timepoints: $\mu_j = \alpha + \beta t_j$.</p>

<p>This example falls into the context of <em>Multivariate linear regression with a within-design structure</em>. 
See for instance <a href=""http://wweb.uta.edu/management/Dr.Casper/Fall10/BSAD6314/Coursematerial/O%27Brien%20&amp;%20Kaiser%201985%20-%20MANOVA%20-%20RM%20-%20Psy%20Bull%2085.pdf"" rel=""nofollow"">O'Brien &amp; Kaiser 1985</a> and <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a></p>

<p>So my example is a simple example of this situation because there are only some predictors (the timepoints) for the components of the mean, but there are no predictors for individuals. This example could be written as follows:
$$
(**) \left\{\begin{matrix} 
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \\ 
{\boldsymbol \mu} = X {\boldsymbol \beta}
\end{matrix}\right.
$$
with ${\boldsymbol \beta}=(\alpha, \beta)'$ and $X=\begin{pmatrix} 1 &amp; t_1 \\ 1 &amp; t_2 \\ 1 &amp; t_3 \end{pmatrix}$ is the matrix of covariates for the components of the multivariate mean ${\boldsymbol \mu}$. The second line of $(**)$ could be termed as the <em>within design</em>, or the <em>repeated measures design</em>, or the <em>structural design</em> (I would appreciate if a specialist had some comments about this vocabulary).</p>

<p>I think such a model can be fitted as a generalized least-squares model, as follows in  <code>R</code> :</p>

<pre><code>gls(response ~ ""between covariates"" , data=dat, 
  correlation=corSymm(form=  ~ ""within covariates"" | individual ))
</code></pre>

<p>(after stacking the data in long format).</p>

<p><strong>My first question</strong> is Bayesian: what about the Bayesian analysis of model $(**)$ and more generally the Bayesian analysis of the multivariate linear regression model with a structural design ? 
Is there a conjugate family ? What about the Jeffreys prior ? Is there an appropriate R package to perform this Bayesian analysis ?</p>

<p><strong>My second question</strong> is not Bayesian: I have recently discovered some possibilities of John Fox's great <code>car</code> package to analyse 
such models with ordinary least squares theory (the <code>Anova()</code> function with the <code>idesign</code> argument --- see <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a>). Perhaps I'm wrong, but I am under the impression that this package only allows to get the MANOVA table (sum of squares analysis) with an orthogonal matrix $X$, and I'd like to get (exact) confidence intervals about the within-design parameters for an arbitrary matrix $X$, as well as prediction intervals. Is there a way to do so with <code>R</code> using ordinary least squares ?  </p>
"
"0.0326991257596857","0.0320502943232105"," 50180","<p>In my research I have performed a series of measurements on 5 different brands of blocks. Each block has been inspected for deformation under incremental forces (20, 30, 40, 50, 60, 70, 80, 90, 100, 110 and 120 N). The deformation for each force was measured 3 times and the mean values were assigned to each brand for a specific amount of force. I was successful in creating linear regression graphs for these 5 different brands.</p>

<p>Now my wish is to see whether a brand makes a significant difference in deformation values and to perform a post-hoc analysis to compare brands among themselves. In other words to compare the linear regression lines. Sorry if what I am saying makes no sense.</p>

<p>So far, I have tried the following commands:</p>

<pre><code>anova(lm(Deformation~Force*Brand, data=Data))
lm(Deformation~Force, data=Data))

# and
aov.data = aov(Deformation~Force*Brand, Data)
</code></pre>

<p>I have gotten suspiciously low p-values (<em>*</em>) which clearly indicates that I might be doing something wrong. I would be grateful if you could help me with this issue.</p>

<pre><code>Force   Brand   Deformation  
20  Brand1  0.65  
30  Brand1  1.23  
40  Brand1  1.25  
50  Brand1  2.39  
60  Brand1  2.45  
70  Brand1  2.93  
80  Brand1  3.13  
90  Brand1  3.57  
100 Brand1  4.68  
110 Brand1  4.84  
120 Brand1  5.33  
20  Brand2  1.24  
30  Brand2  1.11  
40  Brand2  1.6  
50  Brand2  2.13  
60  Brand2  2.69  
70  Brand2  3.60  
80  Brand2  3.90  
90  Brand2  3.99  
100 Brand2  4.51  
110 Brand2  4.74  
120 Brand2  5.98  
20  Brand3  1.21  
30  Brand3  1.37  
40  Brand3  2.56  
50  Brand3  2.49  
60  Brand3  3.17  
70  Brand3  3.33  
80  Brand3  3.38  
90  Brand3  4.2  
100 Brand3  4.22  
110 Brand3  5.22  
120 Brand3  6.28  
20  Brand4  0.92  
30  Brand4  0.89  
40  Brand4  1.2  
50  Brand4  1.67  
60  Brand4  1.98  
70  Brand4  2.25  
80  Brand4  3.8  
90  Brand4  4.17  
100 Brand4  4.94  
110 Brand4  5.4  
120 Brand4  5.76  
20  Brand5  0.69  
30  Brand5  1.26  
40  Brand5  1.61  
50  Brand5  2.17  
60  Brand5  2.07  
70  Brand5  3.35  
80  Brand5  3.27  
90  Brand5  4.13  
100 Brand5  4.25  
110 Brand5  4.59  
120 Brand5  5  
</code></pre>
"
"0.0490486886395286","0.0320502943232105"," 50454","<p>From what I understood, these models differ from CARTs for regression, mostly because they fit a linear model at the leaves of the tree instead of simply taking an average. They also ""smooth"" the tree by generating linear models in the intermediate steps of the tree growth process.</p>

<p>I have been using the R implementation of them a bit for regression getting very good results. But I wonder about the assumptions of a usual linear model? Multicolinearity, Autocorrelation, Non-Normality being violated doesnt worry people in this case? </p>
"
"0.02831827358943","0.0277563690826684"," 51105","<p>As a general statistics question: When using BRTs should you avoid using strongly correlated variables the way you would in say, GLMs or multiple linear regression? Any useful references would be greatly appreciated.</p>
"
"0.0424774103841449","0.0555127381653369"," 51347","<p>I'd like to perform an exponential regression with multiple independent variables (similar to the LOGEST function in Excel)</p>

<p>I'm trying to model the function $Y = b {m_1}^{x_1}{m_2}^{x_2}$  where $b$ is a constant, $x_1$ and $x_2$ are my independent variables, and $m_1$ and $m_2$ are the coefficients of the independent variables.</p>

<p>I think I can linearize the function by doing something like <code>glm(log(Y) ~ x1 + x2)</code> but I don't totally understand why that would work. Also, I'd like to run a true non-linear regression if there is such a thing.</p>

<p>My goal is to run both a linear and an exponential regression, and find the best fit line based on the higher $R^2$ value. </p>

<p>I would also really appreciate your help in understanding how to plot the predicted curve in a scatter plot of my data as well.</p>
"
"0.0326991257596857","0.0480754414848157"," 51786","<p>Does anyone know what exact data cleaning steps one need to undertake in order to clean data for a logit regression (not a logistic regression)?</p>

<p>I have only time variables, meaning year and month, as my independent variables, and I am using R.</p>

<p>A logit regression is simply a normal linear regression where the DV have been transformed with the following formula:</p>

<blockquote>
  <p><code>logit(y) = ln(y/(1-y)</code> for </p>
</blockquote>

<p>An example:</p>

<blockquote>
  <p>3 of 12 people gets cured from taking a pill in period 3 ->
  <code>ln(0.25/(1-0.25)</code></p>
  
  <p>5 of 25 people gets cured taking a pill in period 5 ->
  <code>ln(0.20/(1-0.20)</code></p>
</blockquote>

<p>One can use the logit transformation if you have ratios and in many papers and books it is closely related to the logistic regression.</p>
"
"0.0400480865731637","0.039253433598943"," 52035","<p>I have a weekly time series representing costs for a cohort. I want to tell whether an intervention on the cohort (we can assume it happened in a single week) has decreased costs for the cohort. I happen to know that the trend over this period for the population from which this cohort was taken was -120 per week per week.</p>

<p>My initial thought was simply to do a linear regression <code>lm(Costs~Weeks,offset=-120*Weeks)</code> but (obviously) the significance is not only a function of the effect of the intervention but also how far back I look (if I look back to $-\infty$ it will of course appear non-significant).</p>

<p>I looked at this website: <a href=""http://www.r-bloggers.com/time-series-intervention-analysis-wih-r-and-sas/"" rel=""nofollow"">http://www.r-bloggers.com/time-series-intervention-analysis-wih-r-and-sas/</a> and tried to replicate the R code with my data, but when I enter the arimax() command, I got the error message </p>

<pre><code>Error in stats:::arima(x=x,order=order,seasonal=seasonal,fixed=par[1:narma], : wrong length for 'fixed'
</code></pre>

<p>Now, I'm not sure what to do. Can anyone give me some guidance?</p>
"
"0.0400480865731637","0.039253433598943"," 52155","<p>Here's my context for this question: From what I can tell, we cannot run an ordinary least squares regression in R when using weighted data and the <code>survey</code> package. Here, we have to use <code>svyglm()</code>, which instead runs a generalized linear model (which may be the same thing? I am fuzzy here in terms of what is different).</p>

<p>In OLS and through the <code>lm()</code> function, it calculates an R-squared value, the interpretation of which I do understand. However, <code>svyglm()</code> does not seem to calculate this and instead gives me a Deviance, which my brief trip around the internet tells me is a goodness-of-fit measure that is interpreted differently than an R-squared.</p>

<p>So I guess I essentially have two questions on which I was hoping to get some direction:</p>

<ol>
<li>Why can we not run OLS in the <code>survey</code> package, while it seems that this is possible to do with weighted data in Stata?</li>
<li>What is the difference in interpretation between the deviance of a generalized linear model and an r-squared value?</li>
</ol>

<p>Thanks.</p>
"
"0.102102987459307","0.100077011948264"," 52206","<p>I have a data set which consists of binomial proportions, let's say the success rate of converting a customer depending on the advertisement, the customer age, and various other factors.</p>

<p>For some common combinations of covariates, I have a lot of data, and therefore the binomial proportion of successes has low variance. For rare combinations of covariates, however, I have very little data, and therefore the variance of the proportion is high.</p>

<p>The magnitude of differences is very large, for example I might have 1 million trials for some combinations of covariates, and only 50 for others. However, I want to include ALL data in my model and weight it appropriately to get the best model fit.</p>

<p>I've tried to use R to do binomial (logistic) regression using a generalized linear model:</p>

<pre><code>lrfit &lt;- glm ( cbind(converted,not_converted) ~ advertisement + age, family = binomial)
</code></pre>

<p>This is a good start because it automatically weights the observations by the number of trials.</p>

<p>However, it's not good enough. Here's why: Let's say you have some observations with 100,000 trials and others with 1,000,000 trials. If you weight by number of trials the latter group is going to receive 10 times the weight. This seems nonsensical, however, because both observations are easily precise enough to receive equal treatment in the model. Clearly you want to penalize groups with only 10 or 100 trials, but as the number of trials gets larger, the weight should stop increasing.</p>

<p>Since in weighted least squares the reciprocal of the error variance is used as the weight, my idea would be to use calculate the posterior variance of the proportion (using Jeffrey's prior), then add some constant term to it (this will make sure the variance stops increasing at a certain number of trials) and then use the reciprocal of the sum as the weight.</p>

<p>Is this approach reasonable? Am I missing something? Can someone give me more information about this method?</p>
"
"0.0400480865731637","0.039253433598943"," 52256","<p>I'm working with some data and I used R to the a linear regression model <code>Y = aX + b</code>.
The code I used was
<code>summary(lm(Y~X))</code>
What I got was</p>

<pre><code>              Estimate  Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.3884045  0.0260232  -14.93   &lt;2e-16 ***
X            0.0062095  0.0004635   13.40   &lt;2e-16 ***
</code></pre>

<p>What I want to test now is the null hypothesis <code>H0: a=0</code>, that is, the case where the slope is zero.</p>

<p>I'm confused about how to do that. I tried using the offset parameter ( the idea was to subtract the 'a' coefficient found previously in the former fit ), but I'm not sure it is the correct way to test this hypothesis. 
What I did was:</p>

<p><code>summary(lm(Y~X,offset=0.0062095*X)</code></p>

<p>and I got:</p>

<pre><code>             Estimate   Std. Error t value Pr(&gt;|t|)    
(Intercept) -3.884e-01  2.602e-02  -14.93   &lt;2e-16 ***
X           -2.464e-08  4.635e-04    0.00        1  
</code></pre>

<p>Is it right? Am I now supposed to reject H0 since the p-value found was 1?</p>
"
"0.0490486886395286","0.0480754414848157"," 52379","<p>My case is that I have one continuous DV variable and two categorical IVs containing 11 and 12 different levels (YEAR &amp; MONTH) form 1998 to 2008.</p>

<p>Until now I have experimented a lot with contrasts() and the deviation coding seems to be the one I want to use, because I would rather compare the individuals levels in each IV to the  mean of YEAR rather than some default baseline.</p>

<p>An example of my output is provided here:</p>

<blockquote>
  <p>model &lt;- lm(LN.IDEA ~ 0 + MONTH + MONTH*MONTH + YEAR + YEAR*YEAR, data
  = ds)</p>
</blockquote>

<pre><code>             Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept) -3.467431   0.031038 -111.717  &lt; 2e-16 ***
MONTH1       0.207696   0.098558    2.107   0.0375 *  
MONTH2       0.080218   0.098558    0.814   0.4176    
MONTH3      -0.197687   0.098558   -2.006   0.0475 *  
MONTH4      -0.110153   0.098558   -1.118   0.2663    
MONTH5       0.039526   0.098558    0.401   0.6892    
MONTH6      -0.194322   0.098558   -1.972   0.0514 .  
MONTH7       0.174461   0.098558    1.770   0.0797 .  
MONTH8       0.014709   0.098558    0.149   0.8817    
MONTH9      -0.025038   0.094821   -0.264   0.7923    
MONTH10     -0.086207   0.094821   -0.909   0.3654    
MONTH11      0.060783   0.094821    0.641   0.5229    
YEAR1        0.081754   0.155188    0.527   0.5995    
YEAR2        0.545592   0.090489    6.029 2.65e-08 ***
YEAR3        0.044065   0.090489    0.487   0.6273    
YEAR4       -0.103906   0.090489   -1.148   0.2535    
YEAR5        0.005907   0.090489    0.065   0.9481    
YEAR6       -0.110614   0.090489   -1.222   0.2244    
YEAR7        0.076436   0.090489    0.845   0.4003    
YEAR8        0.069966   0.090489    0.773   0.4412    
YEAR9       -0.218867   0.090489   -2.419   0.0173 *  
YEAR10      -0.204054   0.090489   -2.255   0.0263 * 
</code></pre>

<p>It has been claimed that this question is a duplicate of </p>

<p><a href=""http://stats.stackexchange.com/questions/31690/how-to-test-the-statistical-significance-for-categorical-variable-in-linear-regr/31694#31694"">Claimed answer</a></p>

<p>I realize that this answer touches upon what levels in regression are, but the question is about testing and interpreting p-values in regression. Therefore I argue that this is not a duplicate.</p>
"
"0.0693653206906364","0.0679889413649005"," 52516","<p>I have a data set with the following:</p>

<p>N = 60;
x = developmental stage (range 25 to 44);
y = proportion of 10 minute trial performing a behavior (range 0 to 0.81; 30 zeros)</p>

<p>A scatterplot produces a quadratic looking curve where those in mid-development clearly performed the behavior for more time. Most of the zeros are in the youngest and oldest individuals. If I break up the data into 5 groups according to developmental stage, an ANOVA/Tukey strongly supports this pattern. However, I would like to analyze this data continuously without breaking it into groups.</p>

<p>I have considered arcsine square root transformed proportion data in a linear regression, but I am unsure if that can incorporate a quadratic term, and this analysis results in a very small R squared value (less than 0.1). I have also considered arcsine square root transformed proportion data in a GLM containing a quadratic term or a beta regression (zeros??), but am not sure where to go from here.</p>

<p>I am planning to say in the paper that the individuals in mid-development perform the behavior more than those in early or late development, but am struggling to interpret the data in a way that supports that statement.</p>

<p>I appreciate any suggestions, thank you!</p>
"
"NaN","NaN"," 52856","<p>I am running a multiple regression of the form y~a+b+c+ab+ac+bc</p>

<p>I have checked the VIF values for the direct effects - should I check them for the interactions?  </p>

<p>I am assuming not as that would equate to looking at the multicolinearity between a variable and itself (albeit in an interaction) which will surely be very high?</p>
"
"NaN","NaN"," 53254","<p>I have been given data </p>

<pre><code>x = c(21,34,6,47,10,49,23,32,12,16,29,49,28,8,57,9,31,10,21,26,31,52,21,8,18,5,18,26,27,26,32,2,59,58,19,14,16,9,23,28,34,70,69,54,39,9,21,54,26)
y = c(47,76,33,78,62,78,33,64,83,67,61,85,46,53,55,71,59,41,82,56,39,89,31,43,29,55, 
     81,82,82,85,59,74,80,88,29,58,71,60,86,91,72,89,80,84,54,71,75,84,79)
</code></pre>

<p>How can I obtain the residuals and plot them versus $x$? And how can I test if the residuals appear to be approximately normal? </p>

<p>I'm not sure if I do the original linear fit correctly as I got the equation $y=6.9x-5.5$ but the lecture notes says that the linear regression line should be of the form $y_i=\beta_0+\beta_1x+\epsilon$.</p>
"
"NaN","NaN"," 54519","<p>Could you please advise whether studentized residuals are meaningful when computed on a robust linear regression model using an M-estimator? </p>

<p>I'd like to use it to detect outliers by doing something like this:</p>

<pre><code>rfit = rlm(y~x, data=d)
pt(rstudent(rfit), df=nrow(d)-3) 
</code></pre>

<p>Is this reasonable? I'd be quite happy with a rather crude measure and I'd rather err on the conservative side.</p>

<p>I'm also wondering whether I should run some kind of diagnostic of the general goodness of fit on this robust model before doing this.</p>
"
"0.0400480865731637","0.0196267167994715"," 54668","<p>I want to know if it is possible for a library in R to evaluate the association of independent variables and create a formula? I am trying to come up with a model to predict power consumption of a machine, using some hardware counters and performance attributes. When I use linear regression, I have no problem since I could represent my formula like <code>power~lm(a1+a2+a3+a4)</code>, but for the non-linear case, I am not sure what would be the formula or which model should I choose. 
I would want to have a way to do this:</p>

<pre><code>power ~ &lt;some-non-linear-reg-pkg&gt;(a1+a2+a3+non-linear(a4))
</code></pre>

<p>I reviewed some packages for non-linear regression such as <code>nls</code> and <code>gnm</code>, and they expect a formula to be provided by the user. I am however able to identify which variables have linear associations and which are non-linear (by performing correlation tests), the problem is building a formula out of them. </p>
"
"0.0400480865731637","0.039253433598943"," 54683","<p>I am struggling with a linear regression model of the shape $y = a + b_1\text{month} + b_2\text{year}$. I have 12 months for each year and 10 years. My dependent variable is a log transformed ratio. I have understood that much that when setting such a model up in R, R automatically picks a level for each variable to go into the intercept.  March and 2005 goes into the intercept, in order to provide a baseline for comparison for the other factors.</p>

<p>As stated my problem is that i cannot really figure out what the intercept represents. Is it simply an average of the ratio of March and 2005 or what is it?</p>

<pre><code>Residuals:                  
Min 1Q  Median  3Q  Max 
-0.90339    -0.16789    -0.00373    0.15472 0.88338 

Coefficients:                   
    Estimate    Std. Error  t   value   Pr(&gt;|t|)
(Intercept) -3.586154   0.131642    -27.242 &lt;2.00E-16   ***
MONTHJan    0.381735    0.140731    2.713   0.007875    **
MONTHFeb    0.256457    0.140731    1.822   0.071426    .
MONTHApr    0.072824    0.140731    0.517   0.605981    
MONTHMay    0.207984    0.140731    1.478   0.142613    
MONTHJun    -0.008194   0.140731    -0.058  0.953686    
MONTHJul    0.363693    0.140731    2.584   0.011217    *
MONTHAug    0.195791    0.140731    1.391   0.16727 
MONTHSep    0.212562    0.140731    1.51    0.134124    
MONTHOct    0.124234    0.140731    0.883   0.379495    
MONTHNov    0.204009    0.140731    1.45    0.15032 
MONTHDec    0.175348    0.140731    1.246   0.215711    
YEAR1999    0.477663    0.128469    3.718   0.000333    ***
YEAR2000    -0.027343   0.128469    -0.213  0.83189 
YEAR2001    -0.166637   0.128469    -1.297  0.197612    
YEAR2002    -0.060508   0.128469    -0.471  0.638684    
YEAR2003    -0.173492   0.128469    -1.35   0.179948    
YEAR2004    0.003592    0.128469    0.028   0.977753    
YEAR2006    -0.283261   0.128469    -2.205  0.029776    *
YEAR2007    -0.267752   0.128469    -2.084  0.03972 *
YEAR2008    -0.240654   0.128469    -1.873  0.063985    .
---                 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1                  

Residual standard error: 0.3147 on 99 degrees of freedom                    
Multiple R-squared: 0.4167,                 
Adjusted R-squared: 0.2988                  
F-statistic: 3.536 on 20 and 99 DF,  p-value: 1.491e-05 
</code></pre>
"
"0.02831827358943","0.0277563690826684"," 54856","<p>As you see, I have more than 500 groups of data. how can I get the std.coef in mult-linear regression for each group? Until now, I only know how to get one group by using <code>lm.beta</code>. For example:</p>

<pre><code>library(QuantPsyc)
p &lt;- read.csv('Data1.csv')
model1 &lt;- lm(YCOORD110 ~ TEMMIN + TEMVAR)
lm.beta(model1) 
</code></pre>

<p>For groups, several codes like <code>by</code>, <code>apply</code> can be used, but seem much more complicated beyond my control.</p>

<p>very appreciate for your answers!</p>

<p><img src=""http://i.stack.imgur.com/VdwVd.png"" alt=""enter image description here""></p>
"
"0.0693653206906364","0.0679889413649005"," 54962","<p>I want to learn the statistics package <a href=""http://gretl.sourceforge.net/gretl_and_R.html"" rel=""nofollow"">gretl</a>.</p>

<p>My first attempt to do so is to calculate a linear regression model of a set of data:</p>

<p>$$y_i = \alpha + \beta x_i + u_i$$</p>

<p>First I want to create a crossplot of the data and then calculate the variance of the residuals $s^2_u$ and also those of $\alpha$ and $\beta$.</p>

<p>My questions are:</p>

<p>How to get the crossplot, I do not see any option in the menu which says crossplot? Is this possible in gretl?</p>

<p>When I click Summary statistics I think I get the $s^2$ of the data, however is it also possible to get the the variance of the residuals $s^2_u$ and also those of $\alpha$ and $\beta$?</p>

<p>Is it also possible to calculate in <code>gretl</code> a model without $\alpha$, such as:
$$y_i = \beta x_i + u_i$$</p>

<p>I appreciate your answers!</p>

<p><strong>UPDATE</strong></p>

<p>My gretl window:</p>

<p><img src=""http://i.stack.imgur.com/OSRkv.png"" alt=""enter image description here""></p>
"
"0.02831827358943","0.0277563690826684"," 55240","<p>I'm working on a data set modeling road kills (0 = random point, 1 = road kill) as a function of a number of habitat variables.  Following Hosmer and Lemeshow, I've examined each continuous predictor variable for linearity, and a couple appear nonlinear.  I'd like to try a fractional polynomial transformation for each, also following Hosmer and Lemeshow, and have looked at the R package mfp, but I'm having trouble coming up with (and understanding) the R code that will correctly transform the variable.  Can anyone suggest R code that would help me accomplish the concepts on p. 101 - 102 of Hosmer and Lemeshow's Applied Logistic Regression (2000).  Thanks!</p>
"
"0.02831827358943","0.0277563690826684"," 55273","<p>I have a multiple linear regression with a couple of independent variables in it. Most of them are significant at p&lt;0.001. The model has an RÂ² of 0.83. When I add more variables, the old and the new variables are all highly significant, but RÂ² does not improve at all.  </p>

<p>What does that tell me?</p>
"
"0.0942489115008991","0.0846805485716084"," 55393","<p>I have a PDF (Probability Density Function) generated from a vector of 1,000,000 empirical values. This empirical PDF is heavily skewed to the right.</p>

<p>In this form, I can't make accurate predictions using a linear regression.</p>

<p>To fix this, is there some method to find the function F(x) to transform (i.e. ""squash"") the values in the vector into a standard normal distribution, so I can feed said transformed vector into a linear regression?</p>

<p>Of course, this would also involve finding the inverse of F(x) that transforms (i.e. ""de-squashes"") any predictions back into the original empirical PDF.</p>

<p><strong>What I have tried</strong></p>

<p>So far, I have managed to generate the density function from the empirical data:</p>

<p><img src=""http://i.stack.imgur.com/HIBUP.png"" alt=""enter image description here""></p>

<p>Here is the R code:</p>

<pre><code>par(mfrow=c(2,1))

install.packages(""bootstrap"")
library(bootstrap)
data(stamp)
nobs &lt;- dim(stamp)[1]
hist(stamp$Thickness,col=""grey"",breaks=100,freq=F)
	dens &lt;- density(stamp$Thickness)
lines(dens,col=""blue"",lwd=3)

plot(density(stamp$Thickness),col=""black"",lwd=3, main=""Simulation to choose density plot"")
	for(i in 1:10)
	{
		newThick &lt;- rnorm(nobs,mean=stamp$Thickness,sd=dens$bw*1.5)
		lines(density(newThick,bw=dens$bw),col=""grey"",lwd=3)
}

# If I wanted to do a linear regression to predict stamp thickness,
# what is the function F(x) to ""squash"" (i.e. transform) the ""stamp""
# vector into a normal distribution, and the corresponding inverse 
# function Finv(x) to ""desquash"" (i.e. untransform) any predictions back 
# into the original prediction?
</code></pre>

<p><strong>Update 1</strong></p>

<p>@Andre Silva sugggested that:</p>

<blockquote>
  <p>What need to have normal distribution are the residuals (predicted
  versus observed) derived from your (multiple) linear regression model.</p>
</blockquote>

<p>According to <a href=""http://www.stat.yale.edu/Courses/1997-98/101/linmult.htm"" rel=""nofollow"">post on Multiple Linear Regression</a>:</p>

<blockquote>
  <p>After fitting the regression line, it is important to investigate the
  residuals to determine whether or not they appear to fit the
  assumption of a normal distribution. A normal quantile plot of the
  standardized residuals y -  is shown to the left. Despite two large
  values which may be outliers in the data, the residuals do not seem to
  deviate from a random sample from a normal distribution in any
  systematic manner.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/3ybm0.gif"" alt=""enter image description here""></p>

<p><strong>Update 2</strong></p>

<p>See <a href=""http://stats.stackexchange.com/questions/11351/left-skewed-vs-symmetric-distribution-observed/11352#11352"">Left skewed vs. symmetric distribution observed</a> for R code that illustrates that the only relevant concern is if the residuals are normally distributed.</p>
"
"0.0490486886395286","0.0480754414848157"," 55462","<p>I am using <code>KFAS</code> package for <code>R</code>.</p>

<p>You can run</p>

<pre><code>install.packages(""KFAS"")
library(KFAS)
?regSSM
</code></pre>

<p>to see how this package allows to build a state space representation of linear regression models and many others.</p>

<p>Now let we have the following state space system:</p>

<p>$S_{t}=\alpha+(1+k_{t})L_{t}+v_{t}$</p>

<p>$k_{t}=\phi k_{t-1}+(1-\phi)\bar{k}+w_{t}$</p>

<p>being $\bar{k}$ a constant, a.k.a. the unconditional mean of the unobservable AR(1) process.</p>

<p>Anyone can tell me how may I set this state space representation in <code>KFAS</code> through <code>regSSM</code> or any other <code>KFAS</code> package's function (like <code>arimaSSM</code>)?</p>
"
"0.0700841515030364","0.078506867197886"," 56066","<p>I understand that the Wald test for regression coefficients is based on the following property  that holds asymptotically (e.g. Wasserman (2006): <a href=""http://rads.stackoverflow.com/amzn/click/1441923225"">All of Statistics</a>, pages 153, 214-215):
$$
\frac{(\hat{\beta}-\beta_{0})}{\widehat{\operatorname{se}}(\hat{\beta})}\sim \mathcal{N}(0,1)
$$
Where $\hat{\beta}$ denotes the estimated regression coefficient, $\widehat{\operatorname{se}}(\hat{\beta})$ denotes the standard error of the regression coefficient and $\beta_{0}$ is the value of interest ($\beta_{0}$ is usually 0 to test whether the coefficient is significantly different from 0). So the size $\alpha$ Wald test is: reject $H_{0}$ when $|W|&gt; z_{\alpha/2}$ where
$$
W=\frac{\hat{\beta}}{\widehat{\operatorname{se}}(\hat{\beta})}.
$$</p>

<p>But when you perform a linear regression with <code>lm</code> in R, a $t$-value instead of a $z$-value is used to test if the regression coefficients differ significantly from 0 (with <code>summary.lm</code>). Moreover, the output of <code>glm</code> in R sometimes gives $z$- and sometimes $t$-values as test statistics. Apparently, $z$-values are used when the dispersion parameter is assumed to be known and $t$-values are used when the dispersion parameter is esimated (see <a href=""https://stat.ethz.ch/pipermail/r-help/2006-March/100878.html"">this link</a>).</p>

<p>Could someone explain, why a $t$-distribution is sometimes used for a Wald test even though the ratio of the coefficient and its standard error is assumed to be distributed as standard normal?</p>

<p><strong>Edit after the question was answered</strong></p>

<p><a href=""http://stats.stackexchange.com/a/61292/21054"">This post</a> also provides useful information to the question.</p>
"
"0.0566365471788599","0.0555127381653369"," 56427","<p>So a colleague and myself are using principal component analysis (PCA) or non metric multidimensional scaling (NMDS) to examine how environmental variables influence patterns in benthic community composition. A common method is to fit environmental vectors on to an ordination. The length and direction of the vectors seems somewhat straighforward but I don't understand how an R squared value or a p-value is calculated for these vectors. I have looked at a dozen papers and the most I can gather is that these numbers are calculated using permutations of the data. This does not seem very intuitive. What data is being permuted? How does this permutation create an R squared value and what variance is being explained? My limited understanding of an R squared value comes from linear regressions. I need to explain this to people who have little to no background in statistics so any help understanding these concepts or a link to an available text would be greatly appreciated. Thanks so much! </p>
"
"0.0805952195517515","0.0877733458775107"," 56521","<p>I need to calculate the regression variance ($\sigma^2$) in order to estimate both the confidence intervals and the prediction intervals in a gls regression analysis.  For the analysis, the covariance matrix ($V$) of the response variable ($y$) is known in advance, and so I use it directly as the weighting matrix (=$V^{-1}$) in the gls regression analysis.</p>

<p>The regression variance is a weighted sum of the residual error:
$\sigma^2 = \frac{ (Y â€“ X\beta)^T C^{-1} (Y â€“ X\beta)}{n â€“ p}$</p>

<p>My question/problem is how to determine the weighting matrix $C^{-1}$?  $C$ cannot be set equal to $V$ since (according to the above equation) $C$ must be dimensionless while $V$ has the same units as $\sigma^2$.</p>

<p>Based on my reading of the literature and available texts, it seems that $C$ is the correlation matrix and is a scaled or normalized form of the covariance matrix $V$.  i.e., $V = Var(\epsilon^2) = \sigma^2 C$.  But my problem is that $\sigma^2$ is not yet known, and so I need another way find $C$ from $V$.</p>

<p>R functions such as gls() will compute the regression variance (if I knew how gls() does this, it would answer my question).  However I cannot use gls() in this case since I am specifying a user-defined covariance (weighting) matrix, and gls() only accepts a limited set of specific correlation structures.</p>

<p>In fact a possible solution can be found in this <a href=""http://stats.stackexchange.com/questions/14426/prediction-with-gls"">earlier post</a> where an equation for the SEE (or sigma2) for a GLS regression was cited :</p>

<p>GLS calc of SEE: sqrt( sum( ( residuals from linear model) ^ 2 * glsWeight ) ) / sum( glsWeight ) * length( glsWeight ) / residualDegreeFreedom )</p>

<p>However I am unable to ascertain the validity of this equation and cannot find its source reference.</p>
"
"0.0693653206906364","0.0679889413649005"," 56534","<p>I am trying to find a more aesthetic way to present an interaction with a quadratic term in a logistic regression (categorisation of continuous variable is not appropriate).</p>

<p>For a simpler example I use a linear term.</p>

<pre><code>set.seed(1)

df&lt;-data.frame(y=factor(rbinom(50,1,0.5)),var1=rnorm(50),var2=factor(rbinom(50,1,0.5)))
mod&lt;-glm(y ~ var2*var1  , family=""binomial"" , df)

 #plot of predicted probabilities of two levels

new.df&lt;-with(df,data.frame(expand.grid(var1=seq(-2,3,by=0.01),var2=levels(var2))))
pred&lt;-predict(mod,new.df,se.fit=T,type=""r"")

with(new.df,plot(var1,pred$fit))

 #plot the difference in predicted probabilities

trans.logit&lt;-function(x) exp(x)/(1+exp(x))

pp&lt;-trans.logit(coef(mod)[1] + seq(-2,3,by=0.01) * coef(mod)[3]) -trans.logit((coef(mod)[1]+coef(mod)[2]) + seq(-2,3,by=0.01) * (coef(mod)[3]+coef(mod)[4]))

plot(seq(-2,3,by=0.01),pp)
</code></pre>

<h3>Questions</h3>

<ul>
<li>How can I plot the predicted probability difference between the two levels of var2 (rather than the 2 levels separately)  at different values of var1?</li>
<li>Is there a way to define contrasts so I can use these in the glm so I can then pass this to predict? - I need a CI for the difference in probabilities</li>
</ul>
"
"0.0346826603453182","0.0453259609099337"," 56608","<p>I'm doing some clinical database research and in an effort to lessen the burden on our statistical staff, I started to look for different software solutions to get the analyses I need, and that's where BIDS (business intelligence development studio).  BIDS allows queries to be run against a SQL Server and store the results of those queries in a table or view.  That table or view is then consumed by BIDS and logistic and linear regressions as while as CART analyses can be done on them.<br>
BIDS Version</p>

<p>The x axis that is cut off on the lift chart is 'overall population %'
<img src=""http://i.stack.imgur.com/LbpXf.jpg"" alt=""enter image description here""> </p>

<p>A mining accuracy chart of the CART
<img src=""http://i.stack.imgur.com/Vm5Te.jpg"" alt=""enter image description here""></p>

<p>I'm not quite sure how this works in other stats packages, but in BIDS you define a certain percentage of your data set to train the model and the rest of the set is compared against that model and the lift chart shows the improvement in identifying the outcome you desire vs. a guess.  I'm only vaguely familiar with CART analyses in the first place, and don't know the first thing about R, but this same analysis was done in R with very similar results.  The red portion of what looks like a health meter in a video game corresponds nearly identical to the analysis done in R.  However, there are no p values in the BIDS version.  Consider the node Max Total Poly Pharm >=7 and &lt; 13.
BIDS shows me (not present in the picture)
value         cases    probability
not present   2133     91.89</p>

<p>Is there any way to ascertain a p value from that.  And does R use any of it's cases as training data for the model?</p>
"
"0.0490486886395286","0.0480754414848157"," 56935","<p><img src=""http://i.stack.imgur.com/dDglK.jpg"" alt=""Histogram of my data""></p>

<p>I have a set of data which in my opinion can be treated as coming from a Poisson distribution (they are all positive and represent days). I need to perform several regressions, with R, using these data as dependent variables. I tried linear models (manipulating the data using the Box-Cox transformation) and <code>glm</code> with all possible families but when I check if the residuals are normally distributed (using Shapiro's test) the answer is always negative. Any idea about any other possible distribution family? Thanks!</p>
"
"0.0693653206906364","0.0679889413649005"," 56962","<p>Greedings to everybody.</p>

<p>I have the dataset which you can find <a href=""https://dl.dropboxusercontent.com/u/8546316/Dataset.csv"" rel=""nofollow"" title=""here"">here</a>, containing many different characteristics of different houses, including their types of heating, or the number of adults and children living in the house. In total there are about 500 records. I want to use an algorithm, that can be trained using the dataset above, in order to be able to predict the electricity consumption of a house that is not in the set.</p>

<p>I have tried every possible machine learning algorithm (using weka) (linear regression, SVM etc) . However I had about 350 mean absolute error, which is not good. I tried to make my data to take values from 0 to 1, or to delete some characteristics. I did not managed to find some good results.</p>

<p>I also tried to use R tool, and I did not have good results either...</p>

<p>I would be very grateful, if someone could give me some advice, or if you could examine a little the dataset and run some algorithms on it. What type of preprocessing should I use, and what type of algorithm?</p>
"
"0.02831827358943","0"," 57012","<p>If I try and fit the linear regression </p>

<pre><code>lm(y~V1+V3,data=x)
</code></pre>

<p>with data:</p>

<pre><code>structure(list(V1 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,    
1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L,    
3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L    
), .Label = c(""A"", ""B"", ""C"", ""D""), class = ""factor""), V2 = c(14.95,
    14.95, 14.95, 14.95, 14.95, 14.95, 14.95, 14.95, 14.95, 14.95,
    12.59, 12.59, 12.59, 12.59, 12.59, 12.59, 12.59, 12.59, 12.59,
    12.59, 10.55, 10.55, 10.55, 10.55, 10.55, 10.55, 10.55, 10.55,
    10.55, 10.55, 15.5, 15.5, 15.5, 15.5, 15.5, 15.5, 15.5, 15.5,
    15.5, 15.5), V3 = c(3.33, 3.33, 3.33, 3.33, 3.33, 3.33, 3.33,
    3.33, 3.33, 3.33, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99,
    3.99, 3.99, 4.02, 4.02, 4.02, 4.02, 4.02, 4.02, 4.02, 4.02, 4.02,
    4.02, 3.96, 3.96, 3.96, 3.96, 3.96, 3.96, 3.96, 3.96, 3.96, 3.96
    ), y = c(3.87904870689558, 4.53964502103344, 8.11741662829825,
    5.14101678284915, 5.25857547032189, 8.43012997376656, 5.9218324119784,
    2.46987753078693, 3.62629429621295, 4.10867605980008, 4.44816359487892,
    2.71962765411473, 2.8015429011881, 2.22136543189024, 0.88831773049185,
    5.57382627360616, 2.99570095645848, -1.93323431325928, 3.40271180312737,
    1.05441718454413, -3.54258964789476, 3.25620068273364, -3.20803558645792,
    -0.831129834329122, -0.000314142794054927, -8.49354648593931,
    11.7022963559562, 6.22698494269212, -4.10509549609558, 15.0305193685594,
    -1.98486866562679, 3.77692131760739, 3.26675717101425, 13.2397209466905,
    6.33304746822537, 9.02654811195804, 6.77162595721038, 6.65703634166947,
    17.9488182721157, 5.19383211472586)), .Names = c(""V1"", ""V2"",
    ""V3"", ""y""), row.names = c(NA, -40L), class = ""data.frame""
</code></pre>

<p>the coefficient on V3 is NA. I assume this is because somehow V3 is a linear combination of V1.</p>

<p>V1 is a factor that seems to default to dummy coding with the level 'A' set as the reference level. So, how is there a perfect linear combination in V3 and dummy codes for 'B', 'C', and 'D' (as I assume 'A' is the reference level)""?</p>
"
"NaN","NaN"," 57549","<p>I'm doing a multivariate linear regression with R, and i find myself with the following residuals vs fitted plot:</p>

<p><img src=""http://i.stack.imgur.com/1azOo.png"" alt=""plot""></p>

<p>As you can see there is a very regular line of points that seems to follow a precise pattern.</p>

<p><strong>My questions are:</strong></p>

<ol>
<li>How do I interpret such a behavior, and what can I do to fix it?</li>
<li>Is there a way to isolate/extract those points?  I'd like to take a look at them individually in my data set to see if by examining them I notice some patterns in the data.</li>
</ol>

<p><strong>Additional info:</strong>
My model is:</p>

<pre><code>v.lm = lm(sqrt(v.stima$Y)~., data=v.stima)
</code></pre>

<p>Y is a count-variable (non-negative integer). I'm using sqrt because without it the plot has the typical ""funnel"" shape that indicates a non-homoscedastic error.</p>
"
"0.0749231094763201","0.0734364498908627"," 57811","<p>I'm trying to do LASSO in R with the package glmpath. However, I'm not sure if I am using the accompanying prediction function <em>predict.glmpath()</em> correctly. Suppose I fit some regularized binomial regression model like so:</p>

<pre><code>fit &lt;- glmpath(x = data$x, y=data$y, family=binomial)
</code></pre>

<p>Then I can use predict.glmpath() to estimate the value of the response variable $y$ at $x$ for varying values of $\lambda$ through</p>

<pre><code>pred &lt;- predict.glmpath(fit, newx = x, mode=""lambda"", s=seq(0,10,1),type=""response"")
</code></pre>

<p>However, in the help file it can be seen that there is also an option <em>newy</em>. How should one interpret the result when calling <em>predict.glmpath()</em> with <em>newy = some.y</em>? </p>

<p><strong>[Edit]</strong> An additional question came to mind:</p>

<p>The option <em>type</em> can have the following values, according to the help file:</p>

<pre><code>                      description in help file

""response""            the estimated responses are returned
""loglik""              the log-likelihoods are returned
""coefficients""        the coefficients are returned. The coefficients for the initial input variables are returned (rather than the standardized coefficients)
""link""(default)       the linear predictors are returned
</code></pre>

<p>However, to which linear predictors and coefficients are they referring to? Surely not those of the original model?</p>
"
"0.0895502439463906","0.0877733458775107"," 58101","<p>I am doing predictions on monthly temperature data for 100 years, from 1901 to 2000 (i.e 1200 data points). I want to know if the method I follow is correct because in my output, I do not see the requisite ""randomness"" of temperature being reproduced in the prediction.  </p>

<p>Here is a link to the plot of the prediction (in red)<br>
<a href=""https://docs.google.com/file/d/0B1Lm03a_91xiYks5TVJDYU05VUE/edit?usp=sharing"" rel=""nofollow"">https://docs.google.com/file/d/0B1Lm03a_91xiYks5TVJDYU05VUE/edit?usp=sharing</a>  </p>

<p>EDIT: added the ACF and PACF of the detrended and de-seasonalised time series:
<a href=""https://docs.google.com/file/d/0B1Lm03a_91xia2RTOHZrajJtZXM/edit?usp=sharing"" rel=""nofollow"">https://docs.google.com/file/d/0B1Lm03a_91xia2RTOHZrajJtZXM/edit?usp=sharing</a></p>

<p>Below is the dput() of my data:</p>

<pre><code>&gt; dput(fr.monthly.temp.ts)
structure(c(2.7, 0.4, 4.7, 10, 13, 16.9, 19.2, 18.3, 15.7, 10.6,   
4.9, 3.5, 4.1, 3.2, 7.5, 10.3, 10, 15.1, 18.2, 17.4, 15, 10.2, 
6.3, 3.5, 3.8, 5.9, 7.6, 7.1, 12.9, 14.9, 17.6, 17.3, 15.5, 12.1, 
6.9, 2.7, 3, 4.6, 5.5, 10.3, 13.6, 16.3, 20.2, 18.5, 13.9, 11.2, 
5.4, 4.8, 1.7, 4, 7.4, 9.3, 11.9, 16.5, 20, 17.6, 14.7, 8.4, 
5.5, 3.8, 4.3, 3.1, 5.6, 8.5, 12.6, 16.1, 18.2, 18.9, 16, 12.7, 
7.4, 2.3, 2.5, 2.1, 6.3, 8.4, 12.7, 15.1, 16.5, 17.9, 16.2, 11.6, 
7.6, 5.6, 1.7, 4.8, 5, 7.7, 14.2, 16.8, 17.9, 17.1, 14.8, 12.1, 
6.5, 3.6, 2.2, 2, 4.7, 10.4, 12.8, 14.2, 16.3, 18, 14.2, 12.2, 
5, 4.9, 4, 5.4, 6.6, 8.5, 11.9, 16.1, 16.4, 17.3, 14.2, 11.9, 
5.9, 6, 1.6, 4.5, 6.4, 8.3, 13.6, 16.1, 20.8, 20.7, 17.5, 11.3, 
7.3, 6.6, 4.6, 6.8, 8.4, 9.2, 13.8, 15.5, 17.9, 15.5, 12.5, 10, 
5.5, 5.8, 5.4, 4.7, 7.9, 9.1, 13, 15.8, 16.5, 17.6, 15.4, 12.3, 
9.2, 4, 0.7, 6.5, 7.4, 11.2, 12.2, 15.3, 17.3, 18.2, 15.3, 10.6, 
6.3, 5.7, 3.5, 4.3, 5.7, 8.5, 14.2, 17, 17.2, 17.5, 14.7, 9.6, 
4.6, 7, 6.4, 4.8, 5.9, 9.5, 13.8, 14, 17.4, 18.4, 14.5, 11.5, 
7, 4.3, 1.1, 1.4, 4.4, 6.7, 15.1, 17.6, 18.3, 17.2, 16.4, 9.4, 
7.3, 1.4, 3.7, 5.4, 6.5, 8.4, 14.2, 15, 18, 18.1, 15.4, 9.7, 
6.4, 6.9, 3.3, 3.7, 6.2, 7.8, 13.8, 16.3, 15.9, 18.9, 16.2, 8.8, 
4.6, 5.5, 5, 6.4, 8.2, 9.9, 14.4, 16, 17.4, 16.5, 15.2, 11.5, 
6, 4, 6.4, 4.2, 7.2, 8.9, 13.7, 16.9, 20.6, 18, 17, 14.1, 4.7, 
4.5, 3.4, 4.7, 6.6, 8, 14.8, 16.3, 16.7, 16.9, 13.7, 9.2, 5.4, 
4.5, 3.7, 6.3, 7.6, 9.4, 12.2, 14.1, 19.9, 18.8, 15.1, 12.3, 
5.3, 3.8, 3.8, 2.4, 6.4, 9.2, 14.1, 16.2, 18, 15.9, 15.2, 11.7, 
7.1, 4.5, 4.8, 5.6, 4.3, 9.1, 12.9, 17, 18, 17.6, 13.3, 11.8, 
4.9, 3.9, 4.1, 8.3, 7.2, 10.3, 11.6, 14.5, 18.2, 18.7, 17.3, 
11.5, 8.3, 2.5, 4.3, 4.5, 7.7, 9.8, 13.7, 15.7, 18, 17.8, 15.2, 
11.3, 6.7, 2.9, 5, 6.4, 7.1, 9.3, 11.8, 16.1, 20.5, 19.3, 15.8, 
11.5, 8.2, 3.7, 0.3, -0.2, 6.7, 7.8, 13.2, 16.3, 19.1, 18.1, 
18.4, 11.4, 7.3, 6.4, 5.8, 3.3, 7, 9.7, 12.1, 17.7, 17.3, 18.2, 
15.9, 11.9, 8.6, 4.5, 3.7, 3.3, 5.8, 8.8, 13.8, 17.5, 17.7, 17, 
12.8, 10.6, 8.2, 3.2, 4.8, 1.4, 5.5, 8, 12.1, 15.8, 17.4, 20.4, 
17.2, 11, 7.4, 5, 1.8, 4.3, 7.8, 10.1, 13.1, 15.4, 19.5, 20.1, 
16.7, 12, 5.5, 0.3, 3.3, 3.1, 6.3, 10.4, 13.8, 17.2, 20, 17.5, 
17.1, 11.9, 5.8, 7.6, 2.6, 5.1, 6.2, 9.1, 11.6, 17.2, 19.5, 18.1, 
16.1, 10.7, 7, 3.9, 6.5, 4.6, 7.9, 8.3, 13.4, 16.1, 17.2, 18, 
16, 9.1, 6.6, 4.2, 5.3, 6.9, 5.6, 9.9, 14.2, 16.6, 18.6, 19.1, 
15.5, 11.7, 6.3, 3.2, 4.4, 3.9, 8.8, 7.7, 11.7, 16.8, 17.5, 18.2, 
15.6, 11.3, 9.3, 2.5, 5.3, 4.7, 5.4, 10.2, 11.5, 16.4, 17.3, 
18.1, 15.2, 10.3, 8.7, 2.6, -0.9, 4.5, 7.1, 9.6, 13.5, 17.1, 
17.1, 17.5, 15.6, 10.6, 7.6, 1.1, 0.7, 4.5, 7.3, 8.2, 10.3, 16.8, 
19.3, 16.9, 15.5, 10.8, 6.6, 3.7, -0.2, -0.1, 7.7, 10.6, 13.1, 
16.7, 18.1, 18.7, 16.7, 13.2, 5.5, 4.8, 4.8, 5.3, 8, 11.5, 14.2, 
16.4, 19.2, 19.2, 16, 12.4, 5.9, 3.4, 5.1, 2.2, 5.1, 11.1, 13.4, 
16, 18.6, 20.6, 15.2, 10.1, 7.1, 3.4, -1, 7.1, 8.4, 11.9, 14.8, 
17.8, 20, 18.1, 16.7, 12.3, 6.5, 4.8, 1.7, 6.4, 6.7, 11.2, 13.1, 
15.7, 18.9, 17.9, 16.2, 11.3, 7.1, 2.1, 1, 1.3, 7.3, 11.3, 14.8, 
17.9, 20.4, 20.9, 17.6, 12.1, 8.3, 3.8, 5.7, 4.5, 9.5, 10.4, 
14, 15.8, 17, 17.8, 15.5, 11.4, 7.2, 4.6, 4.5, 5.4, 5.7, 11.7, 
12.2, 16.8, 20.6, 19.8, 18.6, 13.4, 6.4, 5.1, 3, 6.4, 8, 8.7, 
14.2, 18.3, 20.2, 18.6, 15.2, 11.4, 7.4, 1.1, 4.6, 4.7, 5.8, 
9.1, 11.8, 16.1, 18.7, 17.5, 16.5, 10.5, 8.7, 4.9, 2.7, 2.8, 
8.1, 11.2, 14.5, 17.9, 20.2, 18.9, 13.1, 10.9, 5.5, 3.5, 1.1, 
3, 7.5, 10.1, 14.8, 15.4, 18, 18.8, 16.2, 12.1, 7, 6.8, 1.7, 
2.3, 7.5, 8.6, 12.6, 16, 16.4, 16.9, 15.5, 12.4, 8, 6.2, 4.4, 
3.6, 4.6, 10.3, 12.5, 16.4, 19.1, 19.2, 15.7, 10.4, 6.7, 6.4, 
4.4, -1.8, 6.7, 8.1, 13.8, 14.4, 17.8, 16.4, 16.4, 10.6, 5.3, 
5.2, 3.1, 6.9, 9.8, 9.6, 11.5, 17, 18.5, 17.6, 15.1, 11.8, 6.8, 
3.6, 3.7, 6.2, 4.9, 7.9, 13.9, 15.6, 17.9, 18.4, 17.3, 11.4, 
6.7, 5.1, 3.4, 4.5, 8.6, 10.2, 13.8, 17, 20.3, 18.9, 17.2, 12.2, 
6.8, 5.7, 3.5, 5, 8, 9.6, 14.5, 17.6, 16.8, 17.3, 14.5, 11.1, 
8.4, 3.5, 3.6, 7.6, 8.3, 11.7, 12.5, 16.6, 17.7, 18, 18.5, 12.3, 
6.4, 4.5, 4.8, 3.7, 3.9, 9.1, 11.5, 15.8, 17.6, 18.6, 15.5, 11.9, 
5.4, 1.3, -1.6, -0.3, 6.5, 9.6, 12.2, 15.8, 18.5, 16.5, 15.2, 
11.5, 9.3, 1.3, 1.5, 5.2, 5.6, 9.6, 14.5, 16.8, 19.6, 18.2, 16.7, 
9.6, 7.2, 3.2, 3.6, 1.7, 6.6, 8.7, 12.7, 16.1, 16.7, 17.1, 13.7, 
12.2, 6.3, 5.7, 2.6, 7.9, 6.2, 10.5, 13.2, 17, 16.8, 17.2, 16.6, 
12.7, 5, 5.3, 3.5, 5.5, 7.7, 8.8, 12.5, 15.6, 19.8, 18.1, 15.3, 
13.2, 7.1, 3, 3.3, 4.3, 6.8, 9.9, 11.8, 15.9, 17.8, 17.2, 15.1, 
13.5, 6.8, 3, 4.8, 2.1, 6.2, 9.2, 13.2, 15, 19.1, 18.1, 15.9, 
13.1, 7.1, 1.4, 4.1, 4.3, 4.4, 7.6, 12.8, 17.6, 17.8, 18.3, 16.6, 
11.3, 8.7, 2.6, 3.1, 4.2, 3.8, 10.5, 13.7, 14.8, 19.7, 18.7, 
15.7, 12.3, 5.8, 4.9, 3.2, 5.5, 7.9, 8.9, 11.7, 14.3, 18, 17.1, 
13.3, 10.9, 7.3, 4.5, 3, 3.4, 6.1, 7.6, 13.5, 17, 18.1, 19.9, 
16.7, 10.6, 6.8, 3.7, 6.2, 5.5, 7.3, 9.4, 12.5, 15.9, 17.7, 18.6, 
14.5, 8.2, 7.4, 6.8, 6.4, 5.5, 5.3, 9, 12.1, 15.9, 19.1, 19.8, 
16.1, 10.4, 6.7, 3.1, 4.1, 4.8, 6, 8.9, 14, 18.8, 20.1, 19, 14.8, 
11.8, 6.6, 3.1, 3.7, 6.6, 8.3, 8.3, 12.1, 14.8, 17.8, 16.9, 14.7, 
12.9, 7, 5.3, 3.3, 4, 7.2, 7.8, 12.3, 15.2, 17.3, 17.2, 15.6, 
11.8, 6.7, 5.1, 1.3, 4, 6.6, 8.2, 12.3, 16.5, 18.5, 17.1, 15.7, 
12.4, 6.7, 5.7, 2.2, 6.3, 6.2, 8.4, 11.9, 15, 16.4, 18.6, 16.5, 
10.8, 5.8, 3.1, 3.3, 2.9, 9.2, 10, 12.6, 16, 17.5, 18.8, 16.2, 
11.2, 7.2, 3.8, 4.6, 5, 6.3, 9.3, 13.4, 17.4, 20.1, 18, 17.4, 
11.4, 8.3, 4.9, 5.5, 2.5, 7, 8.9, 11.5, 17.1, 22.2, 19.3, 16.3, 
11.9, 7.6, 4.5, 4.2, 3.5, 5.2, 9.6, 10.4, 15.8, 18.8, 18.4, 14.7, 
11.9, 9, 4.5, -1, 3.8, 5.2, 9.7, 12.5, 15.3, 19.4, 17.6, 17.3, 
12.3, 4.4, 5.6, 3.9, -0.6, 5.9, 6.9, 13.7, 16.9, 18.7, 17.6, 
14.9, 13.1, 7.9, 5, -0.8, 3.7, 4.8, 10.9, 11.4, 15, 18.6, 18.6, 
17.8, 12.4, 7.1, 5.2, 6.4, 4.9, 6.5, 10.1, 13.8, 16.2, 17.8, 
18.7, 15.7, 12.9, 6.3, 6, 4.2, 5.6, 9.3, 8.2, 15.3, 16.9, 20.2, 
19.5, 16.5, 13.2, 7, 5.6, 4.8, 8.8, 8.7, 8.9, 15.3, 16, 19.7, 
20.4, 15.9, 13.3, 7.2, 3.1, 3.9, 1.9, 9, 8.7, 11.7, 14.9, 19.6, 
20.7, 17.9, 10.9, 6.9, 3.6, 2.8, 4.9, 7.6, 9.5, 15.3, 16.1, 19.1, 
19.9, 15.5, 9.6, 9, 4.8, 5.9, 3.5, 7, 10.4, 14.1, 17.3, 17.8, 
18.7, 14.7, 10.4, 4.8, 6.2, 5.2, 5.1, 9.4, 8.7, 13.6, 17.1, 21.4, 
19.9, 15, 12, 10.2, 6.5, 4.5, 7.5, 6.5, 9.9, 13.6, 16.1, 21.1, 
20.2, 14.5, 14.6, 7.5, 3.8, 5, 2.9, 6, 10, 12.2, 17.5, 18.7, 
18.2, 14.2, 11.9, 6.9, 3.4, 2.3, 6.9, 9.3, 10, 14.2, 16.3, 18.6, 
21, 17, 12.4, 8.4, 5.5, 5, 5.9, 8.1, 9, 14.9, 17, 18.5, 19.4, 
16.1, 11.6, 5.2, 4.5, 5.3, 4.3, 8, 10, 15.2, 16.3, 20.2, 19.4, 
17.9, 12.2, 6.4, 5, 3.7, 6.6, 7.5, 9.9, 15, 17.8, 17.5, 19.6, 
16.9, 12.2, 8.2, 7.1), .Tsp = c(1901, 2000.91666666667, 12), class = ""ts"")  
</code></pre>

<p>I run <code>stl()</code> on it to remove the seasonality:  </p>

<pre><code># calculate and remove the seasonality  
fr.monthly.temp.ts.stl &lt;- stl(fr.monthly.temp.ts, s.window=""periodic"")    # get the    components  
fr.monthly.temp.seas &lt;- fr.monthly.temp.ts.stl$time.series[,""seasonal""]  
#plot(fr.monthly.temp.seas)  

fr.monthly.temp.ts.noseas &lt;- fr.monthly.temp.ts - fr.monthly.temp.seas  
#plot(fr.monthly.temp.ts.noseas)  
</code></pre>

<p>Then remove the trend with a regression:</p>

<pre><code>fr.mtrend.noseas &lt;- lm(fr.monthly.temp.ts.noseas~t)  
summary(fr.mtrend.noseas)  
</code></pre>

<p>and then use the residuals of this model to fit an ARIMA model (after checking the ACF and PACF for which one is appropriate):</p>

<pre><code># create time series of residuals..this is our ""detrended"" series..for now use only linear trend result  
fr.monthly.temp.ts.new &lt;- ts(fr.mtrend.noseas$resid, start=c(1901,1), frequency=12)
#plot.ts(fr.monthly.temp.ts.new, main=""Detrended and de-seasonalized time series"")

# ARIMA 1,1,1  
fit6 &lt;- arima(fr.monthly.temp.ts.new,order=c(1,1,1))  
fit6  
tsdiag(fit6)  
</code></pre>

<p>I then make a prediction on the stationary time series:</p>

<pre><code>#forecast for the stationary TS, for next 50 yrs months  
forecast &lt;- predict(fit6,n.ahead=600)  
</code></pre>

<p>And then add back the trend and seasonality:</p>

<pre><code>t.new &lt;- (n+1):(n+600)  

#initial time series = stationaryTS + seasonality + trend  
fr.monthly.temp.ts.init &lt;- fr.monthly.temp.ts.new + fr.monthly.temp.seas +
                            fr.mtrend.noseas$coefficients[1] + t * fr.mtrend.noseas$coefficients[2]  

#same for the prediction: we need to add seasonality and trend  
pred.Xt &lt;- forecast$pred + fr.monthly.temp.seas[1:(1+50*12 - 1)] + 
                                fr.mtrend.noseas$coefficients[1] + t.new * fr.mtrend.noseas$coefficients[2]  

plot(fr.monthly.temp.ts.init,type=""l"",xlim=c(1940,2060))  
lines(pred.Xt,col=""red"",lwd=2)  
</code></pre>

<p>So going back to my question: Do I need to add some white noise to the prediction to be able to realistically predict temperature? And more generally, is my method correct?</p>
"
"0.0800961731463273","0.078506867197886"," 58315","<p>I want to find the most important predictors for a binomial dependent variable out of a set of more than 43,000 independent variables (These form the columns of my input dataset). The number of observations is more than 45,000 (these form the rows of my input dataset). Most of the independent variables are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. Here is some code:</p>

<pre><code>library('glmnet')
data &lt;- read.csv('datafile.csv', header=T)
mat = as.matrix(data)
X = mat[,1:ncol(mat)-1] 
y = mat[,ncol(mat)]
fit &lt;- cv.glmnet(X,y, family=""binomial"", type.measure = ""class"")
betacoeff = as.matrix(fit$glmnet.fit$beta[,ncol(fit$glmnet.fit$beta)])
</code></pre>

<p>betacoeff returns the betas for all the independent variables. I am thinking of showing the predictors corresponding to the top 50 betas as the most important predictors. 
My questions are:</p>

<ol>
<li><p>glmnet picks one good predictor out of a bunch of highly correlated good predictors. So I am not sure how much I can rely on the betas returned by the above model run.</p></li>
<li><p>Should I manually sample the data (say 10 times) and each time run the above model, get the list of predictors with the top betas and then find those which are present in all 10 repetitions? Is there any standard way of doing this? What is the standard way of sampling in this case?</p></li>
<li><p>My other question is about cvm (cross validation error) returned by the above model. Since I use type.measure = ""class"", cvm gives the misclassification error for different values of lambda. How do I report the misclassification error for the entire model? Is it the cvm corresponding to lambda.min?</p></li>
</ol>
"
"0.135809669165982","0.133114869819364"," 58321","<p>I need some help with the statistical analysis of a study of a particular surgery to remove a particular cancer. I am using the statistical program R to conduct my analysis. My data are saved in the object <code>study_data</code>.</p>

<h3>Data</h3>

<pre><code># Create reproducible example data
set.seed(50)

study_data &lt;- data.frame(
              Patient_ID = 1:500,
              Institution = sample(c(""New York"",""San Francisco"",""Houston"",""Chicago""),500,T),
              Gender = sample(c(""Male"",""Female""),500,T),
              Race = sample(c(""White"",""Black"",""Hispanic"",""Asian""),500,T),
              Tumor_grade = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Pathologic_stage = sample(c(""P0"",""Pa"",""Pis"",""P1"",""P2a"",""P2b"",""P3a"",""P3b"",""P4a"",""P4b""),500,T),
              Treatment_arm = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Surgery_age = round(runif(500,20,100)),
              Nodes_removed = round(runif(500,1,130)))
</code></pre>

<p>Here is what the data look like:</p>

<pre><code># Peak at the first six lines of the data
head(study_data)

  Patient_ID   Institution Gender     Race Tumor_grade Pathologic_stage Treatment_arm Surgery_age Nodes_removed
1          1       Houston   Male Hispanic         One              P2b           Two          77           130
2          2 San Francisco Female Hispanic       Three               Pa           Two          38           112
3          3      New York Female    Black        Four               P0          Four          90            90
4          4       Chicago   Male Hispanic         Two              Pis          Four          46             4
5          5       Houston Female    Black        Four              P2a          Four          96           114
6          6      New York   Male    Black       Three              P3b          Four          92             7
</code></pre>

<h3>My interest</h3>

<p>I am interested in learning more about what variables are associated with the number of lymph nodes removed during the surgery. My first thought was to simply stratify the data by a particular variable and then calculate the median number of nodes removed.</p>

<p>For example, to see if the institution at which the surgery was performed mattered, I could write:</p>

<pre><code>cbind(do.call(rbind, by(study_data$Nodes_removed, study_data$Institution, summary)))

              Min. 1st Qu. Median  Mean 3rd Qu. Max.
Chicago          1   25.50   65.5 64.48   98.75  129
Houston          1   40.00   71.0 69.26  100.00  130
New York         4   36.00   67.0 67.96  100.00  129
San Francisco    3   36.75   61.0 65.76   99.00  127
</code></pre>

<p>This lets me compare the median nodes removed in each institutional city.</p>

<h3>My question</h3>

<p>I would like to fully examine the association between all of my variables and the outcome <code>Nodes_removed</code>.</p>

<ol>
<li>Should I just do these simple summary statistics for all of my variables?</li>
<li>Do I need to perform some sort of hypothesis test for all of the associations to say whether or not the summary statistics differ? For example, should I calculate a median and a confidence interval for each comparison?</li>
<li>Or should I be using t-tests to compare one group to another?</li>
<li>In the case of a multi-level variable, should I use ANOVA?</li>
<li>Is there any role for linear regression analysis here? </li>
<li>If I wanted to build a single model that includes every possible predictor variable, what method should I use?</li>
</ol>

<p>For example, say that I am most interested in the association between the age at which the surgery was performed, <code>Surgery_age</code>, and <code>Nodes_removed</code>. However, I would like to adjust this association for potential confounders like gender, race, tumor grade, treatment arm, etc. What is the best way for me to do this?</p>

<p>Thanks for any advice you can give!</p>
"
"0.0853828074607","0.0836886016271203"," 58446","<p>I'm trying to compare a few regression models for my data. For linear regression everything is quite understandable, but robust and quantile regressions are not so obvious. I could not find almost anything about calculating confidence interval for these regression models unless I looked for something wrong.</p>

<p>This is my code in R</p>

<pre><code># Robust linear modeling
library(MASS)
library(robustbase)
library(robust)
set.seed(343); 
x &lt;- rnorm(1000)
y &lt;- x + 2*rnorm(1000)

lm1&lt;-lm(y~x); rlm1&lt;-rlm(y~x); rlm2 &lt;- lmRob(y~x); rlm3 &lt;- lmrob(y~x)
cbind(summary(lm1)$coeff,  confint(lm1)) 
cbind(summary(rlm1)$coeff, confint(rlm1))
cbind(summary(rlm2)$coeff, confint(rlm2))
cbind(summary(rlm3)$coeff, confint(rlm3))
</code></pre>

<p>This code produces the following result:</p>

<pre><code>&gt; cbind(summary(lm1)$coeff,  confint(lm1))
                   Estimate Std. Error   t value     Pr(&gt;|t|)      2.5 %     97.5 %
(Intercept) -0.06973191 0.06408983 -1.088034 2.768429e-01 -0.1954982 0.05603438
x            0.97647196 0.06619635 14.751145 1.071805e-44  0.8465720 1.10637196
&gt; cbind(summary(rlm1)$coeff, confint(rlm1))
                  Value Std. Error    t value 2.5 % 97.5 %
(Intercept) -0.06131788 0.06714405 -0.9132288    NA     NA
x            0.96016596 0.06935096 13.8450275    NA     NA
&gt; cbind(summary(rlm2)$coeff, confint(rlm2))
    Error in UseMethod(""vcov"") : 
      no applicable method for 'vcov' applied to an object of class ""lmRob""
&gt; cbind(summary(rlm3)$coeff, confint(rlm3))
              Estimate Std. Error    t value     Pr(&gt;|t|)      2.5 %     97.5 %
(Intercept) -0.0568964 0.06608987 -0.8608945 3.895029e-01 -0.1865874 0.07279464
x            0.9612520 0.06821558 14.0913850 2.921913e-41  0.8273896 1.09511448
</code></pre>

<p>It's easy to spot that linear model works OK and only one robust regression gives a sensible result. Another observation is that lmrob(), which produces some actual confidence interval, calculates it in the same manner as lm(), with using 1.96 as the student coefficient.</p>

<p>Is it a correct way to produce a confidence interval for the robust regression model? May the same method be used for the quantile regression model?</p>
"
"0.0895502439463906","0.0877733458775107"," 58448","<p>I'm stuck with a regression modeling problem. I have panel data where the dependent variable is a probability. Below is an excerpt from my data. The complete panel covers more countries and years, however it is unbalanced. What I can observe is the number of events and the number of trials. The event probability was derived from those values (estimation of this probability should be quite good, given the large number of trials). All independent variables are county-year specific.</p>

<pre><code>     country  year  event_prob  events trials    x    x_lag2 ... more variables
  1   Cyprus  2008  0.03902140  11342  290661   4.60   4.13  ...
  2   Cyprus  2009  0.04586650  13482  293940   4.60   4.48  ...
  3   Cyprus  2010  0.05188398  15206  293077   4.60   4.60  ...
  4   Cyprus  2011  0.06433411  18505  287639   5.79   4.60  ...
  5  Estonia  2008  0.07872978  21686  275449   6.02   4.11  ...
  6  Estonia  2009  0.09516270  33599  353069  13.18   4.91  ...
  7  Estonia  2010  0.08645905  36180  418464   7.95   6.03  ...
  8  Estonia  2011  0.07731997  31590  408562   5.53  13.18  ...
  ...
165  USA  2011  0.06100000  9192822  150702000   2.73  3.27  ...
</code></pre>

<p>My goal is to use regression analysis to find out which variables are significant for the event probability. In R-terminology, I'm looking for a model of the form <code>event_prob ~ x + x_lag2 + ...</code> .</p>

<p>The problem is as follows: <code>event_prob</code> has to be between 0 and 1, hence using <code>event_prob ~ x + x_lag2 + ...</code> might not be the best idea. So I was thinking of using the logit transform of <code>event_prob</code> such that <code>logit(event_prob)</code> ranges from $-\infty$ to $\infty$. The first idea was to use the R's <code>plm</code> package, i.e. <code>plm(logit(event_prob)~x+x_lag2,data,index=c(""country"",""year""),model=""random"")</code> or <code>model=""within""</code> (see below). Is that a reasonable approach or am I violating some essential assumptions?</p>

<p>I was also thinking of using panel generalized linear models from the package <code>pglm</code> (with the logit link function), however since I don't know the outcome of the binary events (only the total number of events and trials) is known, I got stuck there. Maybe someone can help me how to proceed here.</p>

<p>Since I have panel data, I'd like to compute both fixed-effects models and random-effects model and then apply the Hausman (1978) test to decide which model is more appropriate.</p>

<p>Do my first attempts at modeling make sense? I'm really not sure how to correctly address this problem. I hope the description of my problem is detailed enough. If not, I'm happy to provide more details</p>

<p>In terms of software, I'd prefer R. SAS and SPSS are also ok since my university has licences for them. I just don't have much experience with them.</p>
"
"0.0535165067688","0.0629455284778823"," 58538","<p>I'm looking for breakpoints in species abundance as a function of spatial distance, using the <code>segmented</code> package for R. 'segmented' appears to return a breakpoint no matter what; I don't understand whether it returns an estimate of the signifcance of the breakpoint (or whether the segmented linear model is better than an unsegmented model). </p>

<p>For instance (R code): </p>

<pre><code>require(segmented)
set.seed(1)
x &lt;- 1:100
y &lt;- rnorm(100) + x # No real breakpoint
y2 &lt;- c(50-x[1:50], x[51:100-50]) + rnorm(100) # Clear breakpoint at 50
plot(x,y)
points(x, y2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/DRysM.png"" alt=""enter image description here""></p>

<pre><code># Segmented model for the unsegmented data
testM &lt;- lm(y ~ x)
testMs &lt;- segmented(testM, seg.Z = ~x, psi=90)
summary(testMs)
</code></pre>

<p>Despite the fact that there is clearly no breakpoint in the data, <code>summary</code> reports that <code>t value for the gap-variable(s) V:  0</code>, and the standard error is fairly small (3.28).</p>

<pre><code>testM2 &lt;- lm(y2 ~ x)
testM2s &lt;- segmented(testM2, seg.Z=~x, psi=50)
summary(testM2s)
</code></pre>

<p>Here the guess is correct (49.78), the standard error is even smaller (0.2), and the t value is the same. How do I interpret this result? </p>

<p><strong>Note:</strong> I have no particular attachment to the <code>segmented</code> package - I just want to test the hypothesis that a segmented regression models my data better than a single-domain regression. But the other breakpoint-analysis packages I've looked at seem to require that points on the domain be evenly-spaced (e.g. timeseries) and there is a single dependent value per independent value. These assumptions are not met for my spatial data.</p>
"
"0.0535165067688","0.0524546070649019"," 58811","<p>1. Which one is NOT a linear regression models? Please give a 1-2 sentences brief
explanation to your choice.<br>
(a)  $y_i = Î²_0 +\exp(Î²_1x_i)+E_i, i = 1, 2, \ldots, n$<br>
(b)  $y_i = Î²_0 + Î²_1x_i + Î²_2 x_{ii} + E_i , i = 1, 2, \ldots, n$<br>
(c)  $y_i =Î²_0\exp(x_i)+Î²_2x_i^7 +E_i, i=1, 2,\ldots, n$  </p>

<p>2. Suppose $X$ and $Y$ has linear correlation coefficient $r = 0.5$, and there are 77 observations, what is the test statistic for the hypothesis test  </p>

<p>$$H0:Î²_1=0 \quad\text{vs.}\quad Ha:Î²_1\neq0 $$</p>

<p>where $Î²_1$ comes from the simple linear regression model below? Please give a 1-2
sentences brief explanation to your choice.   $\quad Y = Î²_0 + Î²_1X + E$  </p>

<p>(a). Not enough information<br>
(b). 5<br>
(c). 0.25  </p>

<p>3. Which model is more possible to have smaller $R^2$? Please give a 1-2 sentences brief explanation to your choice.<br>
A: $Y=Î²_0+Î²_1X_1+E$<br>
B: $Y=Î²_0^*+Î²_1^*X_1+Î²_2^*X_2+E^*$<br>
where $Y$ and $X_1$ in model A and B are the same.</p>

<p>(a). Not enough information<br>
(b). Model A<br>
(c). Model B  </p>
"
"0.0805952195517515","0.0877733458775107"," 58874","<p>As the title says, what I'd like to do is stepwise introduction of predictor variables to a mixed-effects model. I'm going to first say what I'd be doing if it were stepwise linear regression, just to make sure I've got that part right, and then describe the full model to which I want to apply an analogous approach.</p>

<p>I have a student population who took a pretest, then a tutorial, then a posttest. The tutorial involved doing problems from several categories with feedback, and the users could control which category the next problem would come from and when to stop the tutorial.</p>

<p>I want to create a model that will account for posttest performance using pretest score and some measures of behavior during the tutorial, including total number of problems done, accuracy, and probability of switching category. The last of these is of greatest theoretical interest. There are other variables I'm not mentioning for simplicity.</p>

<p>For the linear regression approach, I first did a simple regression using posttest score as the DV and including the main effects (only) of pretest score, tutorial accuracy, and number of problems as predictors. Then, I added probability of switching as an additional predictor, and compared the resulting model to the previous one to see if it had significantly better explanatory power (it did). The R code I used is below.</p>

<pre><code>lm1 &lt;- lm( posttestScore ~ pretestScore + practiceAccuracy + practiceNumTrials, data=subj.data )
lm2 &lt;- lm( posttestScore ~ pretestScore + practiceAccuracy + practiceNumTrials + probCategorySame, data=subj.data )
anova( lm1, lm2 )
</code></pre>

<p>So far so good? OK, next, I switched to a mixed model in order to include a binary within-subjects factor, 'test question type'. Both pretest and posttest have values for each level of this factor for every subject. (It's unrelated to the 'problem category' I mentioned for the tutorial.) The other predictors, however, only have one value for each participant. My models then became:</p>

<pre><code>library( nlme )
lm1 &lt;- lme( posttestScore ~ pretestScore + questionType + practiceAccuracy + practiceNumTrials, random=~1|sid, method=""REML"", data=D )
lm2 &lt;- lme( posttestScore ~ pretestScore + questionType + practiceAccuracy + practiceNumTrials + probCategorySame, random=~1|sid, method=""REML"", data=D )
</code></pre>

<p>However, I don't know how to test whether the second model resulted in a significant improvement over the first model. Is that the right question I should be asking and, if so, how should I do it?</p>
"
"NaN","NaN"," 59062","<p>I'm new to the R language. I would like to know how to simulate from a multiple linear regression model that fulfills all four assumptions of the regression.</p>
"
"0.0800961731463273","0.078506867197886"," 59069","<p>I've been spending quite some time to figure out how I can get the best R squared value from randomization of some values in a linear regression equation. I have allele frequency data and 14 environmental gradient data. Allele frequency value is fixed, but 2~14 combinations of the 14 environmental variables are used.</p>

<p>My aim here is to find a combination of the environmental variables that yield high R squared value. Here is a simple linear regression equation code that returns R squared value.</p>

<pre><code>&gt; summary(lm(allele ~ compositevalues))$r.squared
</code></pre>

<p>""compositevalues"" is a sum of standardized 14 different environmental values. I want to make 2~14 combinations of variables (with no replacement:i.e. var1+var2, var1+var3, var1+var4, var1+var2+var3, var2+var3+var4, var2+var3, var2+var4, var3+var4....etc. but not var1+var1+var2) as I mentioned above.</p>

<p>I would appreciate it if you could instruct me on how to write a code that generate random combination of (sum of ) the variables and returns combinations of variables that are used with R squared value of >0.4.</p>

<p>I was looking for permutation and resampling function in R, couldn't find ones that serve my purpose.....</p>

<p>Below is a part of my data set.</p>

<pre><code> 1.  Location   allele           var1             var2          var3
 2.  site1,     0.230271924,    -0.872093023,   -0.696403914,   -0.398671096
 3.  site2,     -1.061563963,   0.944767442,    1.104640692,    -0.398671096
 4.  site3,     -0.524508594,   0.339147287,    -1.296752116,   0.431893688
 5.  site4,     0.027061785,    2.156007752,    -0.096055712,   0.431893688
 6.  site5,     0.186726894,    0.944767442,    1.104640692,    -0.398671096
 7.  site6,     -0.118088315,   -0.266472868,   -0.696403914,   -0.398671096
 8.  site7,     -1.003503923,   0.339147287,    -1.296752116,   0.431893688
 9.  site8,     -1.569589312,   0.339147287,    -1.296752116,   0.431893688
 10. site9,     -1.119624003,   0.944767442,     0.50429249,    -1.22923588
 11. site10,    1.362442702,    -1.477713178,   -0.096055712,   1.262458472
 12. site11,    0.215756914,    0.339147287,    -1.897100318,   1.262458472
 13. site12,    0.665722223,    -1.477713178,   -0.096055712,   1.262458472
 14. site13,    1.086657513,    -1.477713178,   -0.096055712,   1.262458472
 15. site14,    -0.001968235,   0.339147287,    1.704988894,    -2.059800664
 16. site15,    -1.656679372,   0.339147287,    1.104640692,    -2.059800664
 17. site16,    0.433482064,    0.339147287,    1.704988894,    -2.059800664
 18. site17,    -0.814808794,   1.550387597,    -1.296752116,   -0.398671096
 19. site18,    -0.713203724,   1.550387597,    -0.696403914,   -0.398671096
</code></pre>

<p>Many thanks!</p>
"
"0.0817478143992143","0.0801257358080262"," 59166","<p>Iâ€™ve simulated some data consisting of one response variable (â€˜yâ€™) and two collinear predictor variables (â€˜Amountâ€™ and â€˜MPSâ€™), where collinearity arises from one of two causes: (1) Amount causes MPS, or (2) Amount and MPS are jointly affected by an unmeasured variable. </p>

<p>What I'm trying to do is figure out whether path analysis can discriminate between these two causes of collinearity. But I'm having trouble specifying a path model for collinearity scenario (2). </p>

<p>My question:
<strong>Is it possible to specify a path model that implies that two exogenous variables are jointly influenced by an unmeasured variable?</strong></p>

<p>I'm working in lavaan, but answers for how to do this conceptually would also be appreciated (if you aren't familiar with lavaan).</p>

<p>Here are my data, simulated in R:</p>

<pre><code># collinearity cause (1)
Amount &lt;- rnorm(n=350, mean=0, sd=1)   
MPS &lt;- rnorm(n=350, mean=0.76*Amount, sd=0.653) 
y &lt;- rnorm(n=350, mean=0.367*Amount + 0.367*MPS, sd=0.72) 

# collinearity cause (2)
Lurking &lt;- rnorm(n=350, mean=0, sd=1) 
Amount &lt;- rnorm(n=350, mean=0.872*Lurking, sd=0.486)  
MPS &lt;- rnorm(n=350, mean=0.872*Lurking, sd=0.486)  
y &lt;- rnorm(n=350, mean=0.367*Amount + 0.367*MPS, sd=0.72)         
</code></pre>

<p>And this is my path model for (1), specified in lavaan:</p>

<pre><code>model1 &lt;- '
  #regressions
  y ~ Amount
  y ~ MPS
  MPS ~ Amount
  '
</code></pre>

<p>And this is a path model I tried for (2):</p>

<pre><code>model2&lt;- '
  #regressions
  y ~ Amount
  y ~ MPS
  #residual correlations
  MPS ~~ Amount
  '
</code></pre>

<p>so for path model (2) my approach was to specify a residual correlation between MPS and Amount. I'm uncertain if this is the correct approach. but even if it is, it doesn't work â€“ to make it work I have to specify that exogenous variables are not fixed, and this uses up my degrees of freedom so I can't test the model.</p>

<p>If anyone has any suggestions for how I can do this â€“ or if it is possible at all - I'd really appreciate it.</p>
"
"NaN","NaN"," 59691","<p>I have a question about the <code>rugarch</code> package. </p>

<p>My sample size is 43 and I have a problem to model a garch whose mean equation includes an exogenous model; otherwise my mean equation is linear regression, that is, $y_i=a+bx_i+e_i$ that $i=1, \ldots, 43$ and $e_i$ follow a $\text{garch}(1,1)$. But when I run it simultaneously R says <code>you need at least 100 points</code>. I don't know what to do.</p>
"
"0.0506572678011219","0.0496521024619361"," 59769","<p>I'm about to open the door to a very thorny issue in the social sciences.  How does one correctly model and test hypotheses about mediating variables using observational data?</p>

<p>I'm familiar with the Baron-Kenny approach to mediation <a href=""http://stats.stackexchange.com/questions/11971/mediation-model-with-linear-regression"">(see previous answer here)</a>, and also with structural equation modeling.  However, I've heard both approaches disparaged by more quantitative-minded social scientists than myself -- especially when one is using observational rather than experimental data.</p>

<p>So, let's say that I'm trying to resolve the following: </p>

<p><strong>Y</strong> is a behavioral outcome. Both <strong>X</strong> and <strong>Z</strong> are observed characteristics of subjects that cannot be manipulated by an experiment. <strong>X</strong> is an attitude (something that can be changed over the long term) and <strong>Z</strong> is an unchangeable characteristic such as age, race, etc. </p>

<p>I hypothesize that <strong>X</strong> is a mediating variable, thus <strong>Z</strong> affects <strong>Y</strong> through the pathway of <strong>X</strong>.  While it's reasonable to suggest that <strong>Z</strong> is in some way correlated with <strong>X</strong>, my theory argues that it has no impact on <strong>Y</strong> (<strong>other than through **X</strong>).</p>

<p>How would one best test these hypotheses using best practices in current research?</p>
"
"0.106193525960362","0.104086384060007"," 59784","<p>I have a dataset which is statistics from a web discussion forum. I'm looking at the distribution of the number of replies a topic is expected to have. In particular, I've created a dataset which has a list of topic reply counts, and then the count of topics which have that number of replies.</p>

<pre><code>""num_replies"",""count""
0,627568
1,156371
2,151670
3,79094
4,59473
5,39895
6,30947
7,23329
8,18726
</code></pre>

<p>If I plot the dataset on a log-log plot, I get what is basically a straight line:</p>

<p><img src=""http://i.stack.imgur.com/W16yN.png"" alt=""Data plotted on log-log scale""></p>

<p>(This is a <a href=""http://en.wikipedia.org/wiki/Zipf%27s_law"">Zipfian distribution</a>). Wikipedia tells me that straight lines on log-log plots imply a function that can be modelled by a monomial of the form $y = ax^k$. And in fact I've eyeballed such a function:</p>

<pre><code>lines(data$num_replies, 480000 * data$num_replies ^ -1.62, col=""green"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/u8eeD.png"" alt=""Eyeballed model""></p>

<p>My eyeballs obviously aren't as accurate as R. So how can I get R to fit the parameters of this model for me more accurately? I tried polynomial regression, but I don't think that R tries to fit the exponent as a parameter - what is the proper name for the model I want?</p>

<p>Edit: Thanks for the answers everyone. As suggested, I've now fit a linear model against the logs of the input data, using this recipe:</p>

<pre><code>data &lt;- read.csv(file=""result.txt"")

# Avoid taking the log of zero:
data$num_replies = data$num_replies + 1

plot(data$num_replies, data$count, log=""xy"", cex=0.8)

# Fit just the first 100 points in the series:
model &lt;- lm(log(data$count[1:100]) ~ log(data$num_replies[1:100]))

points(data$num_replies, round(exp(coef(model)[1] + coef(model)[2] * log(data$num_replies))), 
       col=""red"")
</code></pre>

<p>The result is this, showing the model in red:</p>

<p><img src=""http://i.stack.imgur.com/JudrC.png"" alt=""Fitted model""></p>

<p>That looks like a good approximation for my purposes.</p>

<p>If I then use this Zipfian model (alpha = 1.703164) along with a random number generator to generate the same total number of topics (1400930) as the original measured dataset contained (using <a href=""http://coderepos.org/share/browser/lang/cplusplus/boost-supplement/trunk/boost_supplement/random/zipf_distribution.hpp"">this C code I found on the web</a>), the result looks like:</p>

<p><img src=""http://i.stack.imgur.com/soakm.png"" alt=""Random number generated results""></p>

<p>Measured points are in black, randomly generated ones according to the model are in red.</p>

<p>I think this shows that the simple variance created by randomly generating these 1400930 points is a good explanation for the shape of the original graph.</p>

<p>If you're interested in playing with the raw data yourself, I have <a href=""http://s3.chickensmoothie.com/misc/research/topic_reply_counts_for_1400930_forum_topics.txt.zip"">posted it here</a>.</p>
"
"0.02831827358943","0.0277563690826684"," 59866","<p>I have two variables which show significant correlation (Spearman). I would like to graphically show the strength of the relationship, much like showing a linear regression fit with confidence bands. What would be the best correct way of doing that?</p>
"
"0.0950527084045132","0.100333291527804"," 59952","<p>In the ""Dynamic Linear Models with R"" book, the Regression models section reads: ""The static regression linear model corresponds to the case where $W_t = 0$ for any $t$, so that $\theta_t = \theta$ is constant over time.""  </p>

<p>I am not understanding what this means, because when I fit a <code>dlmModReg</code> with <code>dW = 0</code> (default values) and then plot the predicted values of the state vectors, my regression coefficients vary over time, and eventually stabilize to a value similar to that of standard least-squares regression.</p>

<p>Plot of regression slope coefficient with <code>dW = 0</code>: </p>

<p><img src=""http://i.stack.imgur.com/I5MLB.jpg"" alt=""dW = 0""></p>

<p>However, if I use <code>dlmMLE</code> to find the MLE of <code>dW</code> before fitting the model, plotting the predicted values of the state vectors results in non-sensible values with multiple large discontinuities.</p>

<p>Plot of regression slope coefficient with <code>dW</code> MLE and no intercept: </p>

<p><img src=""http://i.stack.imgur.com/VzdIB.jpg"" alt=""dW != 0 no intercept""></p>

<p>Plot of regression slope with <code>dW</code> MLE with intercept:
<img src=""http://i.stack.imgur.com/2Yu7P.jpg"" alt=""dw != 0 with intercept""></p>

<p>I've yet to find any literature on how the intercept coefficient gets set, but I'm observing a near perfect linear relationship between the intercept and slope coefficient.  The inclusion of an intercept term in the parameter MLE also causes my <code>dV</code> to drop from 16 to 0.003.  Can anyone point me to any references which discuss how the intercept term is set on each update?</p>

<p>Plot of intercept and slope with <code>dW</code> MLE:
<img src=""http://i.stack.imgur.com/TjvTW.jpg"" alt=""intercept and slope""></p>

<p><strong>My questions are:</strong></p>

<ol>
<li>Why are the regression coefficients dynamic/time-varying even with <code>dW = 0</code> in the first example?  If the regression coefficient changes at every time step regardless of whether <code>dW</code> is 0 or not due to the measurement update stage $\beta_t = \hat{\beta_t} + K_t(y_t - \alpha_t - \hat{\beta_t}x_t)$ ($K$ is the Kalman gain), what's the difference between simple linear regression and dynamic linear regression except for some random variation added in the time update equation $\hat{P_t} = P_{t-1} + dW$ ($P$ is estimate error covariance)?</li>
<li>Why does the plot of regression coefficients have so many discontinuities when my <code>dW != 0</code>?</li>
<li>What is the relationship between the intercept and the slope coefficients?  Plotting them reveals a near-perfect linear relationship, but I can't find any literature explaining this.  I haven't found the intercept term included in any formulation of the Kalman update equations.</li>
</ol>

<p>Would anyone be willing to take a look at my data/code?</p>
"
"0.0400480865731637","0.039253433598943"," 60434","<p>I am using <a href=""http://lavaan.ugent.be/"" rel=""nofollow"">R lavaan package</a> to estimate a structural equation model. Let's say the model consists of 1 endogenous manifest variable with 1 latent and 2 manifest explanatory variables: </p>

<pre><code>group = {0,1}
attitude1 = latent,scale
age = respondent's age
</code></pre>

<p>The desired lavaan model is then (doesn't work): </p>

<pre><code>model &lt;- '
attitude1 =~ att1 + att2 + att3
outcome ~ age*group + attitude1*group'
</code></pre>

<p>My goal is, in the lines of what can be done in linear regression, to <strong>establish main and interaction effects between each variable and group. Can this be done?</strong> </p>
"
"0.102102987459307","0.0846805485716084"," 60476","<p>I've run a regression on U.S. counties, and am checking for collinearity in my 'independent' variables.  Belsley, Kuh, and Welsch's <em>Regression Diagnostics</em> suggests looking at the Condition Index and Variance Decomposition Proportions:</p>

<pre><code>library(perturb)
## colldiag(, scale=TRUE) for model with interaction
Condition
Index   Variance Decomposition Proportions
           (Intercept) inc09_10k unins09 sqmi_log pop10_perSqmi_log phys_per100k nppa_per100k black10_pct hisp10_pct elderly09_pct inc09_10k:unins09
1    1.000 0.000       0.000     0.000   0.000    0.001             0.002        0.003        0.002       0.002      0.001         0.000            
2    3.130 0.000       0.000     0.000   0.000    0.002             0.053        0.011        0.148       0.231      0.000         0.000            
3    3.305 0.000       0.000     0.000   0.000    0.000             0.095        0.072        0.351       0.003      0.000         0.000            
4    3.839 0.000       0.000     0.000   0.001    0.000             0.143        0.002        0.105       0.280      0.009         0.000            
5    5.547 0.000       0.002     0.000   0.000    0.050             0.093        0.592        0.084       0.005      0.002         0.000            
6    7.981 0.000       0.005     0.006   0.001    0.150             0.560        0.256        0.002       0.040      0.026         0.001            
7   11.170 0.000       0.009     0.003   0.000    0.046             0.000        0.018        0.003       0.250      0.272         0.035            
8   12.766 0.000       0.050     0.029   0.015    0.309             0.023        0.043        0.220       0.094      0.005         0.002            
9   18.800 0.009       0.017     0.003   0.209    0.001             0.002        0.001        0.047       0.006      0.430         0.041            
10  40.827 0.134       0.159     0.163   0.555    0.283             0.015        0.001        0.035       0.008      0.186         0.238            
11  76.709 0.855       0.759     0.796   0.219    0.157             0.013        0.002        0.004       0.080      0.069         0.683            

## colldiag(, scale=TRUE) for model without interaction
Condition
Index   Variance Decomposition Proportions
           (Intercept) inc09_10k unins09 sqmi_log pop10_perSqmi_log phys_per100k nppa_per100k black10_pct hisp10_pct elderly09_pct
1    1.000 0.000       0.001     0.001   0.000    0.001             0.003        0.004        0.003       0.003      0.001        
2    2.988 0.000       0.000     0.001   0.000    0.002             0.030        0.003        0.216       0.253      0.000        
3    3.128 0.000       0.000     0.002   0.000    0.000             0.112        0.076        0.294       0.027      0.000        
4    3.630 0.000       0.002     0.001   0.001    0.000             0.160        0.003        0.105       0.248      0.009        
5    5.234 0.000       0.008     0.002   0.000    0.053             0.087        0.594        0.086       0.004      0.001        
6    7.556 0.000       0.024     0.039   0.001    0.143             0.557        0.275        0.002       0.025      0.035        
7   11.898 0.000       0.278     0.080   0.017    0.371             0.026        0.023        0.147       0.005      0.038        
8   13.242 0.000       0.001     0.343   0.006    0.000             0.000        0.017        0.129       0.328      0.553        
9   21.558 0.010       0.540     0.332   0.355    0.037             0.000        0.003        0.003       0.020      0.083        
10  50.506 0.989       0.148     0.199   0.620    0.393             0.026        0.004        0.016       0.087      0.279        
</code></pre>

<p><code>?HH::vif</code> suggests that VIFs >5 are problematic:</p>

<pre><code>library(HH)
## vif() for model with interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         8.378646         16.329881          1.653584          2.744314          1.885095          1.471123          1.436229          1.789454 
    elderly09_pct inc09_10k:unins09 
         1.547234         11.590162 

## vif() for model without interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         1.859426          2.378138          1.628817          2.716702          1.882828          1.471102          1.404482          1.772352 
    elderly09_pct 
         1.545867 
</code></pre>

<p>Whereas John Fox's <em>Regression Diagnostics</em> suggests looking at the square root of the VIF:</p>

<pre><code>library(car)
## sqrt(vif) for model with interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         2.894589          4.041025          1.285917          1.656597          1.372987          1.212898          1.198428          1.337705 
    elderly09_pct inc09_10k:unins09 
         1.243879          3.404433 
## sqrt(vif) for model without interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         1.363608          1.542121          1.276251          1.648242          1.372162          1.212890          1.185108          1.331297 
    elderly09_pct 
         1.243329 
</code></pre>

<p>In the first two cases (where a clear cutoff is suggested), the model is problematic only when the interaction term is included.</p>

<p>The model with the interaction term has until this point been my preferred specification.</p>

<p>I have two questions given this quirk of the data:</p>

<ol>
<li>Does an interaction term always worsen the collinearity of the data?</li>
<li>Since the two variables without the interaction term are not above the threshold, am I ok using the model with the interaction term.  Specifically, the reason I think this might be ok is that I'm using the King, Tomz, and Wittenberg (2000) method to interpret the coefficients (negative binomial model), where I generally hold the other coefficients at the mean, and then interpret what happens to predictions of my dependent variable when I move <code>inc09_10k</code> and <code>unins09</code> around independently and jointly.</li>
</ol>
"
"0.0950527084045132","0.107499955208361"," 60648","<p>I am trying to forecast electricity consumption in GWh for 2 years ahead (from June 2013 ahead), using R (the forecast package). For that purpose, I tried regression with ARIMA errors. I fitted the model using the <code>auto.arima</code> function, and I used the following variables in the <code>xreg</code> argument in the <code>forecast.Arima</code> function: </p>

<p>- Heating and Cooling Degree Days,<br>
- Dummies for all 12 months and<br>
- Moving holidays dummies (Easter and Ramadan)  </p>

<p>I have several questions regarding the model:</p>

<p>1) Is it correct to use all 12 dummies for monthly seasonality, since when I tried to include 11, the function returned error. The <code>Auto.arima</code> function returned the model ARIMA(0,1,2)</p>

<p>2)The model returned the following coefficients (I won't specify all of them as there are too many coefficients):</p>

<pre><code>ma1      ma2     HDD     CDD   January  February  March     April
-0.52 -0.16      0.27    0.12  525.84   475.13    472.57    399.01
</code></pre>

<p>I am trying to determine the influence of the temperature component over electricity load. In percentages, (interpreting the coefficients just as with the usual regression) the temperature components (<code>HDD</code>+<code>CDD</code>) account for 11,3% of the electricity consumption. Isn't this too little, considering the fact that the electricity consumption is mostly influenced by the weather component? On the other hand, taking look at the dummies' coefficients, it turns out that the seasonality accounts for the greater part of the load. Why is this? Is the model completely incorrect?</p>

<p>I tried linear regression, and the temperature component accounts for 20%, but it is still a low percentage. Why is this?</p>

<p>3) I am obviously making some mistakes in the use of <code>forecast.Arima</code> or the plot function parameters since when I plot the forecasts, I get a picture of the original time series which is continued (merged) with the forecasts for the whole time series period (from 2004 until 2015). I don't know how to explain this better, I tried to paste the picture, but it seems I cannot paste pictures here.</p>
"
"0.0566365471788599","0.0555127381653369"," 60809","<p>Why are my confidence intervals performing so poorly?</p>

<p>I have a data frame <code>may</code> of two variables $C$ and $F$. I perform a linear regression</p>

<pre><code>lm.1 &lt;- lm(may$C ~ may$F)
lm.1$coef[2]
    #  may$F
#  0.0161821  
confint(lm.1)[c(2,4)]
# [1] 0.0151  0.0173
</code></pre>

<p>So far so good. I then want to simulate running a sample and getting the confidence interval from each sample, similar to <a href=""http://en.wikipedia.org/wiki/File%3aNYW-confidence-interval.svg"" rel=""nofollow"">this illustration from wikipedia</a>. However when I do -- expecting to get 95% of my intervals to intersect the true parameter (<code>0.0161</code>) -- I find that only 55% (!!) of my intervals intersect the correct parameter.<img src=""http://i.stack.imgur.com/aLiFh.png"" alt=""100 confidence intervals with true parameter""></p>

<pre><code>sample.size &lt;- 15

plot(c(1,n), c(0, 0.03), type=""n"")
  lines(x=c(-1,n+1), y=c(0.016,0.016), lty=2, col=2)
  x &lt;- cbind(rep(0,n), rep(0,n))
for (i in 1:n){
  may.temp &lt;- may
  lm.temp &lt;- NA
  set.seed(i*3)
  may.temp &lt;- may.temp[sample(1:nrow(may.temp), sample.size), ]
  lm.temp &lt;- lm(may.temp$C ~ may.temp$F)

  x[i, 1] &lt;- confint(lm.temp)[2,1]
  x[i, 2] &lt;- confint(lm.temp)[2,2]

  lines(x=c(i,i), y=c(x[i,1], x[i,2]), col=""blue"", lwd=2)
}
y &lt;- rep(0,n)
for (i in 1:n){
  if( x[i,1] &lt; 0.017 &amp; x[i,2] &gt; 0.015){
  y[i] &lt;- 1
}
}
sum(y)
# 55
</code></pre>

<p>I had two thoughts:</p>

<ol>
<li>My sample size (15) is too low. But I thought the confidence intervals would simple become larger to take this into account.</li>
<li>My data has outliers. My data does have outliers, but (from my non-expert eye) these don't seem to bias the model.</li>
</ol>

<p>Where have I gone wrong?</p>
"
"0.02831827358943","0.0277563690826684"," 61144","<p>I am a biology student. We do many Enzyme Linked Immunosorbent Assay (ELISA) experiments and Bradford detection. A 4-parametric logistic regression (<a href=""http://www.miraibio.com/blog/2010/08/the-4-parameter-logistic-4pl-nonlinear-regression-model/"" rel=""nofollow"">reference</a>) is often used for regression these data following this function:
$$
F(x) = \left(\frac{A-D}{1+(x/C)^B}\right) + D 
$$
How can I do this in <code>R</code>? I want to get the $A$, $B$, $C$ and $D$ values and plot the curve.</p>

<p>PS. If I have some data, how can I use the calculated function $F(x)$ to get the value? I mean how do I go from ""data -> F(x) -> value""?</p>
"
"0.0578044339088637","0.0679889413649005"," 61480","<p>I am trying to estimate a GAM regression model using the implementation of <code>gam</code> from the <code>mgcv</code> package. I have a working Gaussian model for the dispersion and a log link for the linear predictors but I receive the error </p>

<pre><code>&gt;""Error in eval(expr, envir, enclos) : cannot find valid starting values: please specify some"". 
</code></pre>

<p><strong>Edit 1</strong> - The exact syntax is </p>

<pre><code>splineWAR &lt;- gam(WAR ~ s(zAge, bs=""cr"") + s(zAdjProd, bs=""cr"") + s(zSOPct, bs=""cr"") + s(zBBPct, bs=""cr""), family=gaussian(link=""log""), data = mydata,  start=c(0, 0, 0, 0, 0))
</code></pre>

<p>I have read the relevant threads <a href=""http://stackoverflow.com/questions/13567169/glm-function-in-r-with-log-link-not-working"">here</a> and <a href=""http://stackoverflow.com/questions/8212063/r-glm-starting-values-not-accepted-log-link"">here</a> but have unable to apply the steps suggested to a multiple regression. For instance, when I try and set start values for the 5 variables in my regression (1 dependent and 4 independent) by adding the <code>start=c(n1, n2, n3, n4, n5)</code> argument (where the <code>n</code>'s are the mean of the relevant variable), I receive the same error even though I am seemingly copying the syntax exactly from the first link. Can anyone make a suggestion as to what I should try next? Thanks. </p>

<p><strong>Edit 2</strong> The code in the <code>gam.fit</code> function that runs right before the error is - </p>

<pre><code>if (!(validmu(mu) &amp;&amp; valideta(eta))) 

stop(""Can't find valid starting values: please specify some"")
</code></pre>
"
"0.106193525960362","0.111025476330674"," 61532","<p>I am trying to find the best fit between an species dataset and prevailing climatic conditions, in order to be able to predict the environmental conditions from the species dataset (paleoclimate research). </p>

<p>I have 15 species(sp1-sp15), expressed as relative amounts (some are 0).
I have done some data exploration in excel, and have seen that I get good fits using the ratio  sp2/(sp1+sp2). </p>

<p>I have done multiple linear regression on this dataset, but none give a correlation as good as the ratio I stumbled on. The ultimate goal is of course to test whether this ratio I stumbled upon is yields the best model to explain changes in the climate variable.</p>

<p>I would thus like to create the ratios: <code>sum(spj)/sum(spj)</code>, where spj refers to one of the species. Here, both the numerator and denominator can be any possible combination of all species. </p>

<p>Can I generate a formula model to include a ratio? Can I then select the best model using regbsubsets?</p>

<p><strong>EDIT</strong>
Based on answers below, this will not be possible for all 15 variables. How would it be possibel for 5 variables (I have seen this published before, using R).</p>

<p><strong>EDIT</strong>
On stackoverflow (where I posted the same question), I got the comment to use the function (I), to write expressions inside formulae. However, if I use</p>

<pre><code>leapsMAT&lt;-regsubsets(x1 ~ I(1+ (y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 + y12+ y13 + y14 +y15 )/(y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 + y12+ y13 + y14 + y15)), force.in= FTFALSE,nvmax=15,data=my.data, nbest=1)
</code></pre>

<p>the regsubsets doesn't recognize the variables as variables, of course. How do I implement the I(function), or is there another way to deal with this?</p>

<p>At the moment I have obtained the same linear regression using two methods:</p>

<pre><code>library(leaps)
attach(my.data)
FTFALSE&lt;-c(FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE)
leapsMAT&lt;-regsubsets(x1 ~ 1+ y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 + y12+ y13 + y14 + y15 , force.in= FTFALSE,nvmax=15,data=my.data, nbest=1)
</code></pre>

<p>And, using a longer method specified here (implemented from somewhere else on the internet):</p>

<pre><code>allModelsList &lt;- apply(regMat, 1, function(x) as.formula (paste(c(""x1 ~ 1"", namevar2[x]),collapse="" + "" )))
allModelsList
warnings()

#Calculating the model
my.data
allModelsResults &lt;- lapply(allModelsList, function(x) lm(x, data=my.data))
allModelsResults

dfCoefNum   &lt;- ldply(allModelsResults, function(x) as.data.frame(t(coef(x))))

dfStdErrors &lt;- ldply(allModelsResults, function(x) as.data.frame(t(coef(summary(x))[, ""Std. Error""])))


dftValues   &lt;- ldply(allModelsResults, function(x) as.data.frame(t(coef(summary(x))[, ""t value""])))

dfpValues   &lt;- ldply(allModelsResults, function(x) as.data.frame(t(coef(summary(x))[, ""Pr(&gt;|t|)""]))) 
dfpValues
(warnings)
# rename DFs so we know what the column contains
names(dfStdErrors) &lt;- paste(""se"", names(dfStdErrors), sep=""."")
names(dftValues) &lt;- paste(""t"", names(dftValues), sep=""."")
names(dfpValues) &lt;- paste(""p"", names(dfpValues), sep=""."")

# p-value for overall model fit
calcPval &lt;- function(x){
    fstat &lt;- summary(x)$fstatistic
    pVal &lt;- pf(fstat[1], fstat[2], fstat[3], lower.tail = FALSE)
    return(pVal)
}

# Before creating ONE data frame with all important entries,
# we need to compute some more indices 
NoOfCoef &lt;- unlist(apply(regMat, 1, sum))
R2       &lt;- unlist(lapply(allModelsResults, function(x)
                          summary(x)$r.squared))
    adjR2    &lt;- unlist(lapply(allModelsResults, function(x)
                              summary(x)$adj.r.squared))
RMSE     &lt;- unlist(lapply(allModelsResults, function(x)
                          summary(x)$sigma))
fstats   &lt;- unlist(lapply(allModelsResults, calcPval))



# now we can combine all the data into one data frame
results &lt;- data.frame( model = as.character(allModelsList),
                       NoOfCoef = NoOfCoef,
                       dfCoefNum,
                       dfStdErrors,
                       dftValues,
                       dfpValues,
                       R2 = R2,
                       adjR2 = adjR2,
                       RMSE = RMSE,
                       pF = fstats  )
results[1:20,]
# round the results
results[,-c(1,2)] &lt;- round(results[,-c(1,2)], 3)
results

model.maxRadj&lt;-which(results$adjR2 == max(results$adjR2), arr.ind = TRUE)
maxRadj&lt;-results[model.maxRadj,]
</code></pre>

<p>Many thanks in advance! Please let me know if more information is required.</p>
"
"0.0867708542418546","0.111218061863672"," 61547","<p><strong>EDIT: Since making this post, I have followed up with an additional post <a href=""http://stats.stackexchange.com/questions/61711/fitting-a-zero-inflated-negative-binomial-regression-with-r"">here</a>.</strong></p>

<p><strong>Summary of the text below: I am working on a model and have tried linear regression, Box Cox transformations and GAM but have not made much progress</strong></p>

<p>Using <code>R</code>, I am currently working on a model to predict the success of minor league baseball players at the major league (MLB) level. The dependent variable, offensive career wins above replacement (oWAR), is a proxy for success at the MLB level and is measured as the sum of offensive contributions for every play the player is involved in over the course of his career (details here - <a href=""http://www.fangraphs.com/library/misc/war/"" rel=""nofollow"">http://www.fangraphs.com/library/misc/war/</a>). The independent variables are z-scored minor league offensive variables for statistics that are thought to be important predictors of success at the major league level including age (players with more success at a younger age tend to be better prospects), strike out rate [SOPct], walk rate [BBrate] and adjusted production (a global measure of offensive production). Additionally, since there are multiple levels of the minor leagues, I have included dummy variables for the minor league level of play (Double A, High A, Low A, Rookie and Short Season with Triple A [the highest level before the major leagues] as the reference variable]). Note: I have re-scaled WAR to to be a variable that goes from 0 to 1. </p>

<p>The variable scatterplot is as follows:</p>

<p><img src=""http://i.imgur.com/IJNowem.jpg"" alt=""scatterplot""></p>

<p>For reference, the dependent variable, oWAR, has the following plot:</p>

<p><img src=""http://i.stack.imgur.com/AMXDm.jpg"" alt=""dependentvariableplot""></p>

<p>I started with a linear regression <code>oWAR = B1zAge + B2zSOPct + B3zBBPct + B4zAdjProd + B5DoubleA + B6HighA + B7LowA + B8Rookie + B9ShortSeason</code> and obtained the following diagnostics plots: </p>

<p><img src=""http://i.imgur.com/U6ABt5a.jpg"" alt=""linearRegressionDiagnostics""></p>

<p>There are clear problems with a lack of unbiasedness of the residuals and a lack of random variation. Additionally, the residuals are not normal. The results of the regression are shown below:</p>

<p><img src=""http://i.imgur.com/3kW3ZAO.jpg"" alt=""linearRegressionResults""></p>

<p>Following the advice in a previous <a href=""http://stats.stackexchange.com/questions/61217/transforming-variables-for-multiple-regression-in-r"">thread</a>, I tried a Box-Cox transformation with no success. Next, I tried a GAM with a log link and received these plots:</p>

<p><img src=""http://i.imgur.com/hWQAS2d.jpg"" alt=""splines""></p>

<p><strong>Original</strong>
<img src=""http://i.imgur.com/y6KVJtB.jpg"" alt=""diagnosticChecksGAM""></p>

<p><strong>New Diagnostic Plot</strong>
<img src=""http://i.imgur.com/UKD2ycB.jpg"" alt=""GAMDiag""></p>

<p>It looks like the splines helped fit the data but the diagnostic plots still show a poor fit. <strong>EDIT: I thought I was looking at the residuals vs fitted values originally but I was incorrect. The plot that was originally shown is marked as Original (above) and the plot I uploaded afterwards is marked as New Diagnostic Plot (also above)</strong></p>

<p><img src=""http://i.imgur.com/78BObfU.jpg"" alt=""GAMResults""></p>

<p>The $R^2$ of the model has increased</p>

<p>but the results produced by the command <code>gam.check(myregression, k.rep = 1000)</code> are not that promising. </p>

<p><img src=""http://i.imgur.com/XoubLbh.jpg"" alt=""GAMResults2""></p>

<p>Can anyone suggest a next step for this model? I am happy to provide any other information that you think might be useful to understand the progress I've made thus far. Thanks for any help you can provide. </p>
"
"0.0785407595840826","0.100077011948264"," 61711","<p>In this <a href=""http://stats.stackexchange.com/questions/61547/help-me-fit-this-non-linear-multiple-regression-that-has-defied-all-previous-eff"">thread</a>, I laid out a problem involving fitting a model that attempts to use minor league baseball statistics to predict success at the major league level (explained in full in the thread). After doing further research outside of the thread, I have come to the conclusion that a zero-inflated negative binomial model is likely the best fit given that I believe there are two processes generating the data. The first process determines whether a player will make the majors and once the player reaches the majors, a second process governs their success (as measured by WAR - also explained in the linked thread). </p>

<p>I ran the model and the ZINB model appears to be a reasonable fit given the following diagnostic plot of fitted values vs residuals (broken into two plots to make it easier to inspect visually).</p>

<p><img src=""http://i.imgur.com/GO28KA3.jpg"" alt=""image3"">
<img src=""http://i.imgur.com/6J1QGOj.jpg"" alt=""image1"">
<img src=""http://i.imgur.com/Qe3vmGx.jpg"" alt=""image2""></p>

<p><strong>EDIT 2: Here is a plot of the Pearson residuals.</strong></p>

<p><img src=""http://i.imgur.com/eyRBF7C.jpg/"" alt=""image3""></p>

<p><strong>EDIT 3: Here is a plot of the Pearson residuals vs the fitted values.</strong> </p>

<p><img src=""http://i.imgur.com/nNsgFPl.jpg"" alt=""image4""></p>

<p>Although this is a big improvement over my previous models, there is clearly a bias for the model to underestimate a player's career WAR i.e., a majority of the residuals are greater than zero. <strong>EDIT: It turns out that the plot is misleading due to the high number of overlapping residuals. In reality, only 10% of the residuals are greater than zero.</strong> I am guessing that this may be improved by either a) a better specification of the functional form of the covariates or b) using a different yet similar model e.g., ZIP. Given that this type of regression goes well beyond what I have worked with before, I would appreciate any suggestions on further diagnostics I can use to both test this model and compare it to others and how to improve the functional form of the covariates given that the R function I have used, zeroinfl (from the pscl package), does not appear to be that flexible. Thank you!</p>
"
"NaN","NaN"," 61733","<p>I want to perform a very simple linear regression in <code>R</code>. The formula is as simple as $y = ax + b$. However I would like the slope ($a$) to be inside an interval, let's say, between 1.4 and 1.6.</p>

<p>How can this be done?</p>
"
"0.02831827358943","0.0277563690826684"," 61747","<p>I have a set of values $x$ and $y$ which are theoretically related exponentially:</p>

<p>$y = ax^b$</p>

<p>One way to obtain the coefficients is by applying natural logarithms in both sides and fitting a linear model:</p>

<pre><code>&gt; fit &lt;- lm(log(y)~log(x))
&gt; a &lt;- exp(fit$coefficients[1])
&gt; b &lt;- fit$coefficients[2]
</code></pre>

<p>Another way to obtain this is using a nonlinear regression, given a theoretical set of start values:</p>

<pre><code>&gt; fit &lt;- nls(y~a*x^b, start=c(a=50, b=1.3))
</code></pre>

<p>My tests show better and more theory-related results if I apply the second algorithm. However, I would like to know the statistical meaning and implications of each method.</p>

<p>Which of them is better?</p>
"
"0.0535165067688","0.0629455284778823"," 61805","<p><strong>The situation</strong></p>

<p>I have a dataset with one dependent $y$ and one independent variable $x$. I want to fit a continuous piecewise linear regression with $k$ known/fixed breakpoints occurring at $(a_{1}, a_{2}, \ldots, a_{k})$. The breakpoins are known without uncertainty, so I don't want to estimate them. Then I fit a regression (OLS) of the form
$$
y_{i} = \beta_{0} + \beta_{1}x_{i} + \beta_{2}\operatorname{max}(x_{i}-a_{1},0) +  \beta_{3}\operatorname{max}(x_{i}-a_{2},0) +\ldots+ \beta_{k+1}\operatorname{max}(x_{i}-a_{k},0) +\epsilon_{i}
$$
Here is an example in <code>R</code></p>

<pre><code>set.seed(123)
x &lt;- c(1:10, 13:22)
y &lt;- numeric(20)
y[1:10] &lt;- 20:11 + rnorm(10, 0, 1.5)
y[11:20] &lt;- seq(11, 15, len=10) + rnorm(10, 0, 2)
</code></pre>

<p>Let's assume that the breakpoint $k_1$ occurs at $9.6$:</p>

<pre><code>mod &lt;- lm(y~x+I(pmax(x-9.6, 0)))
summary(mod)

Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          21.7057     1.1726  18.511 1.06e-12 ***
x                    -1.1003     0.1788  -6.155 1.06e-05 ***
I(pmax(x - 9.6, 0))   1.3760     0.2688   5.120 8.54e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The intercept and slope of the two segments are: $21.7$ and $-1.1$ for the first and $8.5$ and $0.27$ for the second, respectively.</p>

<p><img src=""http://i.stack.imgur.com/2GsoE.png"" alt=""Breakpoint""></p>

<hr>

<p><strong>Questions</strong></p>

<ol>
<li>How to easily calculate the intercept and slope of each segment? Can the model be reparemetrized to do this in one calculation?</li>
<li>How to calculate the standard error of each slope of each segment?</li>
<li>How to test whether two adjacent slopes have the same slopes (i.e. whether the breakpoint can be omitted)?</li>
</ol>
"
"0.109891042959396","0.11444244151147"," 62000","<p>I've tried to create three models (using R): an intercept only linear regression, a simple mixed effects regression and a by-subject effects mixed effects regression.</p>

<p>An intercept only regression models the grand mean of a response variable plus error. In <code>mtcars</code>, the variable <code>drat</code> may be considered a response variable. In the model below, have I correctly modelled the grand mean of <code>drat</code>, plus error?</p>

<pre><code>interceptOnly &lt;- lm(drat ~ 1, data=mtcars)
</code></pre>

<p>A simple mixed effects regression models the grand mean of a response variable, plus subject deviation, plus error. In <code>mtcars</code>, <code>drat</code> may be considered a response variable and <code>cyl</code> a subject deviation. In the model below, have I correctly modelled the grand mean of <code>drat</code>, plus the deviation of <code>cyl</code> from <code>drat</code>, plus error?</p>

<pre><code>library(lme4)
simpleMixedEffects &lt;- lmer(drat ~ (1|cyl), data=mtcars)
</code></pre>

<p>A by-subject effects mixed effects regression models the grand mean of a response variable, plus subject deviation, plus condition effect, plus error. In <code>mtcars</code>, <code>drat</code> may be considered a response variable, <code>cyl</code> a subject deviation and <code>wt</code> a condition effect. In the model below, have I correctly modelled the grand mean of <code>drat</code>, plus the deviation of <code>cyl</code> from <code>drat</code>, plus the effect of <code>wt</code>, plus error?</p>

<pre><code>bySubjectMixedEffects &lt;- lmer(drat ~ (1|cyl) + wt, data=mtcars)
</code></pre>

<p>I have one further question: </p>

<p>How can I model a by-subject varying condition effect model. This is a mixed effects model which models the grand mean of a response variable, plus group deviation from grand mean (random effect), plus condition effect (fixed effect), plus group deviation from condition effect (random effect), plus error. Could someone provide R code that outputs a ""by-subject varying condition effect model""?</p>
"
"0.113469578623964","0.111218061863672"," 62070","<p>Let's say you have a response variable and an independent variable. Your data is measured across several levels of a categorical independent variable. One approach in analysing these data would be to use linear regression to estimate a slope at each level of the categorical independent variable. This is the approach I've used here, using <code>sleepstudy</code> dataset from the <code>R</code> <code>lme4</code> package (I've stored the betas from each model in <code>lmBetas</code>):</p>

<pre><code>library(lme4); library(plyr); library(ggplot2)
lmBetas &lt;- daply(sleepstudy, .(Subject), function(x) coef(lm(Reaction ~ Days, data=x))[""Days""])
</code></pre>

<p>Another approach in analysing these data would be to use a mixed effects model to estimate slopes for each level of the categorical independent variable, which in this case is <code>Subject</code>. This is the approach I've taken here (I've stored the betas from the model in <code>lmerBetas</code>):</p>

<pre><code>lmerBetas &lt;- coef(lmer(Reaction ~ Days + (Days | Subject), data=sleepstudy))$Subject[,""Days""]
</code></pre>

<p>I have learned that a single mixed effects model, as implemented through the <code>lmer</code> function in R, is more accurate at estimating slopes than a multiple linear regression model applied to multilevel data. This can be demonstrated with this plot of betas from the above models. </p>

<pre><code>betas &lt;- data.frame(method.betas = c(lmerBetas, lmBetas))
betas$method &lt;- c(rep(""lmer"", 18), rep(""lm"", 18))

ggplot(betas, aes(method.betas)) + 
  geom_histogram() +
  facet_grid(method ~ .)
</code></pre>

<p>The top histogram shows betas estimated using linear regression, and the bottom histogram shows betas estimated using mixed effects. You can see betas estimated using linear regression are more widely spread than those estimated through the mixed effects model.</p>

<p><img src=""http://i.stack.imgur.com/YtB22.jpg"" alt=""enter image description here""></p>

<p>So finally, my questions:</p>

<ol>
<li><p>Is a mixed effects model's higher accuracy in betas estimation connected with the fact that it models intercepts and slopes for each level of the categorical independent variable under a joint probability distribution?</p></li>
<li><p>Generally speaking, why is a mixed effects model more accurate in its betas estimation?</p></li>
</ol>
"
"0.201073037009547","0.211416578576444"," 62106","<p>I have been working on a baseball model to predict success at the major league level using minor league statistics. After posting multiple threads on this site (<a href=""http://stats.stackexchange.com/questions/61217/transforming-variables-for-multiple-regression-in-r"">1</a>, <a href=""http://stats.stackexchange.com/questions/61547/help-me-fit-this-non-linear-multiple-regression-that-has-defied-all-previous-eff"">2</a>, <a href=""http://stats.stackexchange.com/questions/61711/fitting-a-zero-inflated-negative-binomial-regression-with-r"">3</a>) and receiving valuable feedback, I have settled on a zero-inflated negative binomial model as being the best fit for my data.</p>

<p>For those who do not want to go back through old threads,  I will recap some of the story here. Also, for those who have read the old threads, some of the details regarding the variables I am using have changed. </p>

<p>In my model, the dependent variable, offensive career wins above replacement (oWAR), is a proxy for success at the MLB level and is measured as the sum of offensive contributions for every play the player is involved in over the course of his career (details here - <a href=""http://www.fangraphs.com/library/misc/war/"" rel=""nofollow"">http://www.fangraphs.com/library/misc/war/</a>). The independent variables are z-scored minor league offensive variables for statistics that are thought to be important predictors of success at the major league level including age (players with more success at a younger age tend to be better prospects), strike out rate [SOPct], walk rate [BBPct] and adjusted production (a global measure of offensive production). Additionally, since position is an important determinant of whether a players makes the major leagues (those who play at easier positions will be required to perform at a higher offensive level in order to have the same value as a player at a more difficult position), I have included dummy variables to account for position. Note that I have not included the position dummy in the count portion of the model as the oWAR dependent variable has already been adjusted for the difficulty level of the position played by the player. </p>

<p><strong>EDIT: I have added the paragraphs below in response to the following comments in the answer below:</strong></p>

<p><em>""I see you do not include the same covariates in the Logit and the negative binomial process - why not? Ususally, each relevant predictor would be expected to influence both processes.""</em></p>

<p>I think it would help if I explained the data generating process. A player plays in the minor leagues. At some point, when they have demonstrated enough skill at the minor league level (this is a combination of statistical success and observed traits that scouts believe will allow them to be successful at the major league level), a player is promoted to the major leagues. At this point, they have an opportunity to accrue oWAR. At this point, the first data-generating process  (captured by the logit model) ends. Now, a different data generating process takes over, whereby players accumulate oWAR depending on how they play at the major league level. Some players will not perform well and accumulate zero oWAR, the same as a player who did not make the majors. That is one of the reasons I think this model is appropriate. It is not necessarily easy to separate a player who accumulates zero because they aren't good enough to make the major leagues from a player who makes the major leagues but does not succeed at that level (and still ends his career with zero oWAR). I have not included the positional dummy in the count part of the model because the oWAR measure is already adjusted for the position played by the player whereas the minor league statistics are not. When I tried testing them in the model, they were, not surprisingly, not significant. I omitted the BB Pct statistic from the logit part of the model as it was not significant (p = 0.22)</p>

<p><em>""Since your data appears to be a panel (you observe players/teams repeatedly), you can think about more sophisticated stuff like fixed or random effects. ""</em></p>

<p>In terms of how the data is sampled, a player can be in the dataset once (if they spend only a year at the level of the minor leagues) or multiple times if they take multiple years to advance. After reading up on fixed vs random effects model, I don't see how I can used a fixed model to predict out of sample players. However, I am sure that there are fixed effects (effects determined by the player that are not captured in the dependent variables) so I don't fully understand how to handle that issue.</p>

<p><strong>END EDIT</strong></p>

<p>After trying a linear, Box-Cox transformed and basic GLM model with generally poor results, I was directed to the zero-inflated negative binomial distribution set of models. After trying out different combination of variables and following the steps in this excellent <a href=""http://www.jstatsoft.org/v27/i08/paper"" rel=""nofollow"">step-by-step</a> guide for regression models for count data in R, I settled on the following model (shown below).</p>

<p><strong>Model</strong>
<img src=""http://i.imgur.com/neOP2RE.jpg"" alt=""model1""></p>

<p>Furthermore, when I re-estimate the standard errors using sandwich standard errors, the model still appears to have appropriate independent variables.</p>

<p><strong>Sandwich Standard Errors</strong>
<img src=""http://i.imgur.com/47JjOae.jpg"" alt=""sandwich""></p>

<p>At this point, I do not think I will find a better model type given the dataset I have. However, I am still left with some issues. The first issue may be a function of the dataset I am using. In most examples I have seen that discuss zero-inflated data, there are clearly more zeros than other values (hence, the name). However, the number of zeros still appears to be less than 50% of the total dependent variables and usually not even that high. In my dataset, approximately 87% of the dependent variables are zero i.e., it is hard to have success in major league baseball. I am guessing the model should technically be able to account for this scenario (albeit with less predictive value than a model with more non-zeros) but I am not sure how to check if that is the case. When I create a plot of fitted values and Pearson residuals, and a plot of fitted values and raw residuals, they appear as below:</p>

<p><strong>Fitted vs Pearson residuals</strong>
<img src=""http://i.imgur.com/tlx6ibn.jpg"" alt=""FvP""></p>

<p><strong>Fitted vs raw residuals</strong>
<img src=""http://i.imgur.com/vL8OIcV.jpg"" alt=""FvR""></p>

<p>Not knowing exactly what these plots look like in a good-fitting regression, I decided to take the sample data described <a href=""http://www.ats.ucla.edu/stat/r/dae/zipoisson.htm"" rel=""nofollow"">here</a> and examine the plots in an example where I know the fit has been deemed to be good. </p>

<p><strong>Fitted vs Pearson residuals - Sample problem</strong>
<img src=""http://i.imgur.com/TJh2zHF.jpg"" alt=""FvPEx""></p>

<p><strong>Fitted vs raw residuals - Sample problem</strong>
<img src=""http://i.imgur.com/YJMxJl9.jpg"" alt=""FvREx""></p>

<p>Clearly, these plots do not look that similar to mine. I am not sure how much has to do with a) model misspecification b) the fact that my dependent variable has 87% zeros and c) the fact that this is a simple sample problem designed to perfectly fit this model whereas my data is messy, real world data. Any thoughts on this issue would be appreciated. </p>

<p>My second issue, which I am not sure if I should be tackling after or simultaneously with the first issue, has to do with the functional form specification. I don't know if my independent variables are in the right form. It has been suggested to me by a friend that I could try a) multiple fractional polynomials with loops or b) informally play around with adding polynomials of covariates, interactions, etc. My issue at this point is that I do not know how to implement point a in R and and I am not sure which forms to try for point b besides randomly choosing some. Once again, help on this separate (but related?) issue would be greatly appreciated. </p>

<p>If anyone has any questions, I will do my best to answer them. In my first post (<a href=""http://stats.stackexchange.com/questions/61217/transforming-variables-for-multiple-regression-in-r"">1</a>), I mentioned I could not provide the dataset but I have been given permission to do so if anyone wants to take a look. Thanks again. </p>
"
"0.106193525960362","0.104086384060007"," 62125","<p>I'm interested in the effect on beta estimation of including/excluding independent variables in linear regression. </p>

<p>I've made this data below:</p>

<pre><code>set.seed(50)
predictor1 &lt;- rnorm(10, 3, 1)
predictor2 &lt;- rnorm(10, 6, 1)
</code></pre>

<p>I've also simulated a model using these data, with an intercept of 2, a beta of 50 for <code>predictor1</code> and a beta
of 2 for <code>predictor2</code>:</p>

<pre><code>response &lt;- 2 + (50 * predictor1) + (2*predictor2)
</code></pre>

<p>A linear regression in <code>R</code> correctly calculates the intercept and both the betas:</p>

<pre><code>summary(lm(response ~ predictor1 + predictor2))

Call:
lm(formula = response ~ predictor1 + predictor2)

Residuals:
       Min         1Q     Median         3Q        Max 
-4.150e-14 -6.833e-15  1.100e-15  9.879e-15  2.757e-14 

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 2.000e+00  8.194e-14 2.441e+13   &lt;2e-16 ***
predictor1  5.000e+01  7.730e-15 6.468e+15   &lt;2e-16 ***
predictor2  2.000e+00  1.353e-14 1.479e+14   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.081e-14 on 7 degrees of freedom
Multiple R-squared:      1,  Adjusted R-squared:      1 
F-statistic: 2.095e+31 on 2 and 7 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>However, a linear regression with only <code>predictor2</code> in the model calculates a beta of -3.467 for <code>predictor2</code>:   </p>

<pre><code>summary(lm(response ~ predictor2))

Call:
lm(formula = response ~ predictor2)

Residuals:
    Min      1Q  Median      3Q     Max 
-75.201 -24.049   5.193  38.423  56.074 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  171.133    177.588   0.964    0.363
predictor2    -3.467     30.872  -0.112    0.913

Residual standard error: 47.58 on 8 degrees of freedom
Multiple R-squared:  0.001574,  Adjusted R-squared:  -0.1232 
F-statistic: 0.01261 on 1 and 8 DF,  p-value: 0.9134
</code></pre>

<p>Here is plot of <code>response</code> against <code>predictor1</code> and <code>predictor2</code>, with line of best fit calculated through regression:</p>

<pre><code> library(ggplot2)
ggplot(dat, aes(predictor2 , response )) + geom_point() + 
  geom_point(aes(predictor1 , response )) + 
  geom_abline(intercept = 2, slope = 2) + 
  geom_abline(intercept = 2, slope = 50) + 
  ylim(0, 300) + 
  xlim(0,10)
</code></pre>

<p><img src=""http://i.stack.imgur.com/fXEE8.jpg"" alt=""enter image description here""></p>

<p>And here is plot of just <code>response</code> against just <code>predictor2</code>, against with line of best fit calculated through regression:</p>

<pre><code> ggplot(dat, aes(predictor2 , response )) + 
      geom_point() + 
      geom_abline(intercept = 171.133, slope = -3.467) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/svEXN.jpg"" alt=""enter image description here""></p>

<p>I have two questions:</p>

<ol>
<li><p>How did the linear regression arrive at a beta of -3.467 in the second model, when the real beta is 2? Or in other words, how has excluding <code>predictor1</code> from the second model caused <code>predictor2</code>'s beta to drop by over 5?</p></li>
<li><p>Is someone able to provide a visual display of how linear regression calculates betas?</p></li>
</ol>
"
"0.0490486886395286","0.0480754414848157"," 62180","<p>I want to do some regression analysis that constrains the coefficients to vary smoothly as a function of their sequence.
It is similar to the ""Phoneme Recognition"" example in the part 5 ""Basis Expansions and Regularization"" of <em><a href=""http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf"" rel=""nofollow"">The Elements of Statistical Learning</a></em> (Hastie, Tibshirani &amp; Friedman, 2008).</p>

<p>in the book it says:</p>

<blockquote>
  <p>The smooth red curve was obtained through a very simple use of natural cubic splines. We can represent the coefficient function as an expansion of splines $\beta(f)=\sum_{m=1}^M h_m(f)\theta_m$. In practice this means that $\beta=\mathbf{H}\theta$ where, $\mathbf{H}$ is a $p Ã— M$ basis matrix of natural cubic splines, defined on the set of frequencies. Here we used $M = 12$ basis functions, with knots uniformly placed over the integers 1, 2, . . . , 256 representing the frequencies. Since $x^T\beta=x^T\mathbf{H}\theta$, we can simply replace the input features $x$ by their filtered versions $x^* = \mathbf{H}^T x$, and fit $\theta$ by linear logistic regression on the $x^*$. The red curve is thus $\hat\beta(f) = h(f)^T\hat \theta$.</p>
</blockquote>

<p>But I am not sure about how to create the basis matrix $\mathbf{H}$. When using ns() function in R, how to set the knots?</p>

<p>Any hint will be appreciated.</p>
"
"0.10236445520486","0.107499955208361"," 62208","<p>I've tried to simulate data for a power analysis of a logistic regression. The results of the power analysis look reasonable: power=90% for a sample of 6000 persons. But I feel that the analysis lacks something. So, my question is: when generating the data should I include something about how the variables are correlated, or their covariance, other than just defining their linear relationship as I have done in the example below, and if so where do I write that into the code?</p>

<p>I know other questions look like this but I'm not confident that they answer this question.</p>

<pre><code>library(plyr) # functions
## Define Function
simfunktion &lt;- function() {
   # Number in each sample
  antal &lt;- 6000
  beta0 &lt;- log(0.16) # logit in reference group
  beta1 &lt;- log(1.1)  # logit given smoking
  beta2 &lt;- log(1.1)  # logit given SNP(genevariation)
  beta3 &lt;- log(1.2)  # logit for interactioncoefficient for SNP*rygning
   ## Smoking variable, with probabilities defined according to empirical studies.
  smoking  &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,25,40))
   ## SNP variables with probabilities defined according to empirical studies
  SNP      &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,40,20))
   ## calculated probabilites given the model:
  pi.x     &lt;- exp( beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) / 
              ( 1 + exp(beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) )
   ## binoial events given the probabilities:
  sim.y    &lt;- rbinom( n = antal, size = 1, prob = pi.x)  
  sim.data &lt;- data.frame(sim.y, smoking, SNP)
   #################### p-value of the interaction is extracted:
   ## the model is run:
  glm1     &lt;- glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial )
   ## p-value of the interactionterm is extracted:
  summary(    glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial ))$coef[4,4]
}
pvalue     &lt;- as.vector(replicate( 100 , simfunktion()))
mean(pvalue &lt; 0.05)
</code></pre>
"
"0.02831827358943","0.0277563690826684"," 62225","<p>I have paired data (GWAS case/control study) and I have heard using conditional logistic regression or generalized linear mixed models (GLMM) is appropriate. Which should I use in this case? Why would you use one over the other. More importantly can you guys point me towards resources for doing these methods in <code>R</code>? I'm finding a lot of material for <code>SAS</code>, which I do not prefer. I can provide more details if necessary.  </p>
"
"0.0853828074607","0.0920574617898323"," 62646","<p>I've got data on mail volume sent by household for seven age groups, with 12 years of data for each age group. I originally ran a simple regression on each age group individually and realized I needed to dig deeper. My aim now is to pool the data (giving me 84 observations) and try to identify some period effects (or year effects, whichever you prefer). My pooled data are currently organized like this (PPHPY stands for Pieces per Household Per Year):</p>

<pre><code>Age Group    Year   PPHPY
1            2001   127.62
1            2002   144.47
1            2003   111.70
1            2004   95.96
1            2005   96.46
1            2006   139.91
1            2007   85.52
1            2008   75.43
1            2009   109.34
1            2010   53.16
1            2011   64.09
1            2012   50.94        
2            2001   176.48
2            2002   172.86
2            2003   137.79
.              .      .
.              .      .
.              .      .
7            2012   163.39
</code></pre>

<p>I first regressed PPHPY on year and year dummies (leaving the intercept as 0 to avoid perfect multicollinearity). This gave me period effects for the aggregated data (ie something like a period effect across all age groups, I think). This looked like the following:</p>

<pre><code>&gt; ## Generate YearDummy using factor()
&gt;
&gt; YearDummy &lt;- factor(YearVar)
&gt;
&gt; ## Check to see that YearDummy is indeed a factor variable
&gt;
&gt; is.factor(YearDummy)
[1] TRUE
&gt;
&gt; ## (...+0) ensures intercept is left out and thus YearDummy1 remains in.
    ## One or the other must be subtracted out to avoid perfect mutlicollinearity
&gt;
&gt; LSDVYear &lt;- lm(PPHPY ~ YearVar + YearDummy + 0, data=maildatapooled)
&gt; summary(LSDVYear)
Call:
lm(formula = PPHPY ~ YearVar + YearDummy + 0, data = maildatapooled)
Residuals:
Min 1Q Median 3Q Max
-99.658 -39.038 8.814 43.670 82.300
Coefficients: (1 not defined because of singularities)
Estimate Std. Error t value Pr(&gt;|t|)
YearVar 5.743e-02 9.851e-03 5.830 1.45e-07 ***
YearDummy2001 1.099e+02 2.795e+01 3.930 0.000193 ***
YearDummy2002 1.209e+02 2.796e+01 4.324 4.85e-05 ***
YearDummy2003 7.791e+01 2.797e+01 2.786 0.006819 **
YearDummy2004 8.053e+01 2.797e+01 2.879 0.005251 **
YearDummy2005 6.887e+01 2.798e+01 2.461 0.016236 *
YearDummy2006 6.572e+01 2.799e+01 2.348 0.021618 *
YearDummy2007 5.975e+01 2.799e+01 2.134 0.036210 *
YearDummy2008 5.836e+01 2.800e+01 2.084 0.040696 *
YearDummy2009 4.119e+01 2.801e+01 1.471 0.145745
YearDummy2010 3.056e+01 2.801e+01 1.091 0.278990
YearDummy2011 1.472e+01 2.802e+01 0.525 0.600951
YearDummy2012 NA NA NA NA
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Residual standard error: 52.44 on 72 degrees of freedom
Multiple R-squared: 0.9316, Adjusted R-squared: 0.9202
F-statistic: 81.71 on 12 and 72 DF, p-value: &lt; 2.2e-16
</code></pre>

<p>What I want, however, is to tease out period effects for each age group individually. This is what I'm not sure how to set up. I was hoping someone might help me devise some code in R that would kick out those period effects for <em>each</em> of the seven age groups using the pooled data, as well as help me understand the problem conceptually. </p>

<p>EDIT: I forgot to mention that I see I must include an interaction term involving the time dummies to allow the coefficients to vary across age groups. I'm just having difficulty constructing the proper interaction term and resulting regression equation.</p>

<p>EDIT 2: I came up with two models and ran them. I felt like the question had evolved at this point and might merit a new post, which is can be found <a href=""http://stats.stackexchange.com/questions/62755/period-effects-in-pooled-time-series-data-in-r"">here</a>.</p>
"
"0.0642198081225601","0.0734364498908627"," 62678","<p>I have a regression problem that I implement in R using for loop. Basically, I have an equation (as a result of a long procedure) as a function of temperature, with five unknown parameters. I have 12 different temperatures that I can use to drive the equations, so for each combination of the five parameters, I can have 12 points.</p>

<p>The 12 points are actually derived from two line segments that cross at a certain point. Depending on the values of the five unknown parameters, the two segments can run anywhere. With the right parameters I hope that the two segments form a straight line.</p>

<p>For that purpose, I need to run a linear regression on these  points, and then over the whole combination of the five parameters, I need to find one set of parameters that results in the minimum residual sum of squares.</p>

<p>The problem is that it takes hours to solve the parameters with a fine resolution (by=0.05 instead of by=0.5 in the example below).</p>

<p>Is there a way to solve this problem faster? I think gradient descend can do this faster, but I do not know how to formulate it.</p>

<p>Any suggestions would be greatly appreciated.</p>

<p>Here is the code:</p>

<pre><code>A1 = 1.25
A2 = 0.01136
A3 = -0.0433

a = seq(0,1,by=0.5)
b = seq(0,1,by=0.5)
c = seq(-3,3,by=0.5)
d = seq(0,1,by=0.5)
e = seq(-3,3,by=0.5)

Temp_F = c(28.99, 36.87, 42.92, 52.84, 58.31, 67.60, 76.17, 70.20, 63.26, 53.05, 39.28, 35.35)

df &lt;- expand.grid(a=a, b=b, c=c, d=d, e=e)
nrow(df)

for (i in 1:nrow(df) ) {
    X.points = df$a[i]*A1 - (A2*df$b[i]*Temp_F) - df$c[i] + (A3*df$d[i]*Temp_F) + df$e[i]
    	glm.X = glm(X.points ~ Temp_F)
    	df$res[i]=sum(residuals(glm.X)^2)
}

write.csv(df, file=""df.csv"")
index=which(df$res == min(df$res[df$res &gt; 0]))
    df$a[index]
df$b[index]
    df$c[index]
df$d[index]
    df$e[index]
</code></pre>
"
"0.0755153962384799","0.0832691072480053"," 62829","<p>I'm trying to calculate partitioned sum of squares in a linear regression. In the first model, there are two predictors. In the second model, one of these predictors in removed. In the model with two predictors versus the model with one predictor, I have calculated the difference in regression sum of squares to be 2.72 - is this correct? If this is correct, why is the difference in sum of squares 2.72 (quite small) when the difference in r-squared is ~30% (quite large).</p>

<pre><code># model two predictors
mod &lt;- lm(drat ~ hp + wt, mtcars)

# regression sum of squares two predictors
regressionSumSquares &lt;- sum((predict(mod)-mean(mtcars$drat))*(predict(mod)-mean(mtcars$drat)))

# residuals two predictors
residualSumSquares &lt;- sum((predict(mod)-mtcars$drat)*(predict(mod)-mtcars$drat))

# r-squared two predictors
totalSumSquares &lt;- regressionSumSquares + residualSumSquares
rSquared &lt;- regressionSumSquares/totalSumSquares



# model one predictor 
modJustHp &lt;- lm(drat ~ hp, mtcars)

# regression sum of squares one predictor
regressionSumSquaresJustHp &lt;- sum((predict(modJustHp)-mean(mtcars$drat))*(predict(modJustHp)-mean(mtcars$drat)))

# residual sum of squares one predictor
residualSumSquaresJustHp &lt;- sum((predict(modJustHp)-mtcars$drat)*(predict(modJustHp)-mtcars$drat))

# r-squared one predictor
totalSumSquaresJustHp &lt;- regressionSumSquaresJustHp + residualSumSquaresJustHp
rSquaredJustHp &lt;- regressionSumSquaresJustHp/totalSumSquaresJustHp

# difference in sum of squares one vs two predictors
regressionSumSquares - regressionSumSquaresJustHp
</code></pre>
"
"0.0400480865731637","0.0196267167994715"," 62853","<p>I am running a regression analyis in r:</p>

<pre><code>fit &lt;- lm(Cost ~ Slope + YardDist, data = test)
</code></pre>

<p>I want to test the two independent variables for multicollinearity. I tested it with <code>vif()</code> (from the car package) and <code>kappa()</code>.</p>

<pre><code>&gt; vif(fit)
   Slope YardDist 
1.000121 1.000121 
&gt; kappa(fit)
[1] 11631.87
</code></pre>

<p>VIF tells me there is no multicollinearity and kappa tells me there is very high multicollinearity. 
What is the difference between both and which one is 'right'?</p>
"
"0.109891042959396","0.107710533187266"," 62899","<p>I hope you can help me get some confidence in my confidence interval... I am trying to get the confidence interval for a particular (threshold) point on a predicted curve.</p>

<p>I find the confidence interval (C.I.) looks funny, so I would like to know whether what I am doing is correct... Thank you for any advice you can give me!</p>

<pre><code>#Here is the data to which I fit a nonlinear least squares regression (power function)
y &lt;-c(0.5745373, 0.8255836, 0.7139635, 0.6318004, 0.8688738, 0.8341626, 0.9278573, 0.6548638,
0.6995722, 0.8410211, 1.0000000, 0.8512973, 0.7917790, 0.5722589, 0.6918069, 0.7117084,
0.5279689, 0.6121315, 0.5869292, 0.7332605, 0.5991816, 0.5470566, 0.5987166, 0.4440854,
0.3719892, 0.4394820, 0.4410862, 0.4966288, 0.4291826, 0.4221613, 0.3951223, 0.3595973,
0.4617549, 0.5106482, 0.5939970, 0.5340835, 0.6036920, 0.5181577, 0.3892170, 0.3667581)

x &lt;- c(0.95149254, 0.57954545, 0.56763699, 0.57089552, 1.00000000, 0.66870629, 0.70833333,
0.53125000, 0.58776596, 0.78061224, 0.61551724, 0.63750000, 0.85000000, 0.52397260,
0.66870629, 0.50328947, 0.52192982, 0.40691489, 0.36016949, 0.37500000, 0.35915493, 
0.53968254, 0.36334197, 0.23486842, 0.19615385, 0.35961538, 0.30039267, 0.26424870,
0.54838710, 0.23363874, 0.26936620, 0.09514925, 0.15324519, 0.32465278, 0.33909574, 
0.31587838, 0.24401914, 0.28813559, 0.23181818, 0.29272959)

plot(x,y, xlim=c(0,1), ylim=c(0,1))
m1 &lt;- nls(y~a*x^b, start=list(a=2,b=0.2))
summary(m1)
curve((0.86888*x^0.43042), add=TRUE)
points(0.2276312,0.4595148, col='red', pch=17) # this is the threshold value I  
                                               # previously calculated
</code></pre>

<p>now I would like to know the Confidence Interval of the threshold</p>

<pre><code>#lower CI
0.86888-(2*0.04578) # intercept minus 2* S.E.
0.43042-(2*0.06333) # exponent minus 2*S.E.
curve((0.77732*x^0.30376), add=TRUE) # plot the lower C.I.
0.77732*0.2276312^0.30376 # Now I enter the x coordinate of threshold point in the 
# Confidence interval function, to get the y coordinate
points(0.2276312,0.4958526, col='red', pch=""-"", cex=1.5) # plotting the CI of threshold value

#Upper CI
0.86888+(2*0.04578) # intercept plus 2* S.E.
0.43042+(2*0.06333) # exponent plus 2*S.E.
curve((0.96044*x^0.55708), add=TRUE) # plot upper C.I.
0.96044*0.2276312^0.55708 # Now I enter the x coordinate of threshold point in the 
# Confidence interval function, to get the y coordinate
points(0.2276312,0.4211113, col='red', pch=""-"", cex=1.5) # plot the CI of threshold value
lines(c(0.2276312,0.2276312),c(0.4211113,0.4958526), col=""red"")

# then check whether correct
confint(m1) # more or less, but slightly different, don't understand why?
</code></pre>

<p><img src=""http://i.stack.imgur.com/PVhBA.png"" alt=""nls confints""></p>

<p>To me, a C.I. that looks like this would make more sense...</p>

<pre><code>curve((0.96044*x^0.30376), col=""green"",add=TRUE)
curve((0.77732*x^0.55708), col=""green"",add=TRUE)
</code></pre>

<p><img src=""http://i.stack.imgur.com/vTl6d.png"" alt=""NLS_confint2""></p>

<p>but that would be mixing the intercept of upper and exponent of lower C.I. 
and vice versa</p>

<p>My question is then, are the black Confidence Intervals correct (and thus the thresholds red confidence interval?</p>
"
"0.0805952195517515","0.0702186767020086"," 63029","<p>I have a non-linear model of the following form:</p>

<p>$y = a*x^b$</p>

<p>I can fit it using logarithms and a linear model or directly with a non-linear model.</p>

<p>First approach, logarithms and linear model:</p>

<pre><code>lmfit &lt;- lm(log(y)~log(x))
</code></pre>

<p>Second approach, non-linear model:</p>

<pre><code>nlsfit &lt;- nls(y~a*x^b, start=list(a=200, b=1.6))
</code></pre>

<p>In the first case I can simply get the $R^2$ value from the linear model or calculate it myself by:</p>

<pre><code>rsquared &lt;- var(fitted(lmfit)) / var(log(y))
</code></pre>

<p>In the second case there is no $R^2$ value generated, but I can obtain one $pseudoR^2$ value myself by:</p>

<pre><code>pseudorsquared &lt;- var(fitted(nlsfit)) / var(y)
</code></pre>

<p>In a linear model I can calculate the fraction of variance unexplained by simply doing $1-R^2$. I have read that this is not applicable to non-linear regressions. I would like to know if there is an equivalent version of this measure, so that I can compare both regressions and use the best one.</p>

<p>As an extra information, I would like to add that this is a regression of physical variables, and that the non-linear approach is providing more close-to-literature results for the coeficients, whereas the linear approach gives better statistical performance ($R^2$, bias, etc.).</p>
"
"0.0863948355424908","0.0923787802599364"," 63039","<p>I've set up a Bayesian regression model in WinBUGS to determine values for the unknown parameters (b1, b2, b3, b4) and intercept value (b0) in a linear regression model. The code is as follows:</p>

<pre><code>model {
for (i in 1:(J-1)) {
  FC[i]       ~ dnorm(mu[i], tau)
  mu[i] &lt;- b0 + b1*(Factor_b1[i]-mean_Factor_b1) + b2*(Factor_b2[i]-mean_Factor_b2) + b3*(Factor_b3[i]-mean_Factor_b3) + b4*(Factor_b4[i]-mean_Factor_b4) 
}

b0        ~ dflat()
b1         ~ dflat()
b2         ~ dflat()
b3         ~ dflat()
b4         ~ dflat()
tau         &lt;- 1/sigma2
log(sigma2) &lt;- 2*log.sigma
log.sigma    ~ dflat()
}

Inits:
list(b0 =0,b1 = 0, b2 =0, b3 = 0, b4 =0, log.sigma=0)

Data1

list(J = 20, FC = c(1.87315166256848, 1.87315166256848, 
1.87315166256848, 1.8708501655802, 1.8708501655802, 1.8708501655802, 
1.93248104062608, 1.93248104062608, 1.93248104062608, 1.93248104062608, 
1.80846914258265, 1.80846914258265, 1.80846914258265, 2.10555453929548, 
2.10555453929548, 2.10555453929548, 2.10555453929548, 2.10555453929548, 
2.12908503670568, 2.12908503670568), Factor_b1 = c(7.0057890192535, 
7.0057890192535, 7.0057890192535, 7.05012252026906, 7.05012252026906, 
7.05012252026906, 7.13329595489607, 7.13329595489607, 7.13329595489607, 
7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 
7.11720550316434, 7.11720550316434, 7.11720550316434, 7.11720550316434, 
7.11720550316434, 7.14124512235049, 7.14124512235049), mean_Factor_b1 = 7.09846620316814, 
Factor_b2 = c(7.2211050981825, 7.2211050981825, 7.2211050981825, 
7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 
7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 
7.2211050981825, 7.2211050981825, 7.37650812632622, 7.37650812632622, 
7.37650812632622, 7.37650812632622, 7.37650812632622, 7.46565531013406, 
7.46565531013406), mean_Factor_b2 = 7.28441087641358, Factor_b3 = c(2.37954613413017, 
2.37954613413017, 2.37954613413017, 2.28238238567653, 2.28238238567653, 
2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 
2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 
2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559, 
2.33214389523559, 2.33214389523559, 2.33214389523559), mean_Factor_b3 = 2.31437347629025, 
Factor_b4 = c(2.06686275947298, 2.06686275947298, 2.06686275947298, 
2.09186406167839, 2.09186406167839, 2.09186406167839, 2.10413415427021, 
2.10413415427021, 2.10413415427021, 2.10413415427021, 2.06686275947298, 
2.06686275947298, 2.06686275947298, 2.32238772029023, 2.32238772029023, 
2.32238772029023, 2.32238772029023, 2.32238772029023, 2.2082744135228, 
2.2082744135228), mean_Factor_b4 = 2.15608963937253)
</code></pre>

<p>This WinBUGS code returns the following outputs for the unknown intercept (b0) and parameter (b1, b2, b3, b4) values:</p>

<pre><code>     node    mean    sd  MC error   2.5%    median  97.5%   start   sample
b0  1.957   0.009337    3.764E-5    1.939   1.957   1.976   1001    56000
b1  0.1068  0.3296  0.001438    -0.5529 0.1072  0.7615  1001    56000
b2  0.5977  0.2758  0.001068    0.05286 0.5967  1.147   1001    56000
b3  0.1892  0.4394  0.001825    -0.6871 0.1899  1.061   1001    56000
b4  0.5757  0.1886  7.423E-4    0.1986  0.5765  0.9472  1001    56000
</code></pre>

<p>MY PROBLEM: When I compare these Bayesian estimates with results from a linear MLE regression in R, I seem to be getting a different result for the intercept value (b0). The code for the R linear regression is as follows:</p>

<pre><code>FC = c(1.87315166256848, 1.87315166256848, 1.87315166256848, 1.8708501655802, 1.8708501655802, 1.8708501655802, 1.93248104062608, 1.93248104062608, 1.93248104062608, 1.93248104062608, 1.80846914258265, 1.80846914258265, 1.80846914258265, 2.10555453929548, 2.10555453929548, 2.10555453929548, 2.10555453929548, 2.10555453929548, 2.12908503670568, 2.12908503670568)

b1 = c(7.0057890192535, 7.0057890192535, 7.0057890192535, 7.05012252026906, 7.05012252026906, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.11720550316434, 7.11720550316434, 7.11720550316434, 7.11720550316434,  7.11720550316434, 7.14124512235049, 7.14124512235049) 

b2 = c(7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.37650812632622, 7.37650812632622, 7.37650812632622, 7.37650812632622, 7.37650812632622, 7.46565531013406, 7.46565531013406)

b3 = c(2.37954613413017, 2.37954613413017, 2.37954613413017, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559) 

b4 = c(2.06686275947298, 2.06686275947298, 2.06686275947298, 2.09186406167839,       2.09186406167839, 2.09186406167839, 2.10413415427021, 2.10413415427021, 2.10413415427021, 2.10413415427021, 2.06686275947298, 2.06686275947298, 2.06686275947298, 2.32238772029023, 2.32238772029023, 2.32238772029023, 2.32238772029023, 2.32238772029023, 2.2082744135228, 2.2082744135228)

# ======================= Linear Model =======================


lmfit_Linear_Model_Test =lm(FC ~ (b1 + b2 + b3 + b4))

print (summary(lmfit_Linear_Model_Test))
</code></pre>

<p>And the results from this MLE regressions are as follows:</p>

<pre><code>Call:
lm(formula = FC ~ (b1 + b2 + b3 + b4))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.05837 -0.00823 -0.00044  0.01307  0.04593 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   -5.247      2.305   -2.28   0.0379 * 
b1             0.105      0.298    0.35   0.7280   
b2             0.674      0.195    3.45   0.0035 **
b3             0.177      0.394    0.45   0.6599   
b4             0.529      0.141    3.75   0.0019 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.0359 on 15 degrees of freedom
Multiple R-squared: 0.931,  Adjusted R-squared: 0.913 
F-statistic: 50.8 on 4 and 15 DF,  p-value: 0.0000000153 
</code></pre>

<p>SUMMARY: Why is the intercept value (b0) coming to -5.247 with the MLE model and 1.957 with the Bayesian model? Should they not be the same?</p>
"
"0.109891042959396","0.100978624863062"," 63226","<p>I am trying to model some data regarding a predator prey interaction experiment (n=26). Predation rate is my response variable and I have 4 explanatory variables: predator density (1,2,3,4 5), predator size, prey density (5,10,15,20,25,30) and prey type (3 categories). I started with several linear models (GLM) and found (as expected) that prey and predator density were non-linearly related to predation rates. If I use a log transformation on these variables I get really nice curves and an adjusted $R^{2}$ of 0.82, but it is not really the right approach for modelling non-linear relationships.</p>

<pre><code>model &lt;-glm(rates ~ log(pred) + log (prey) + type)
</code></pre>

<p>Therefore I switched to non-linear least square regression (<code>nls</code>). I have several predator-prey models based on existing ecological literature e.g.:</p>

<pre><code> ### Holling's type II functional response

model1 &lt;- nls(rates ~ (a * prey)/(1 + b * prey),
start = list(a = 0.27,b = 0.13), trace = TRUE)

### Beddington-DeAngelis functional response

model2 &lt;- nls(rates ~ (a*prey)/(1+ (b * prey) + c * (pred -1 )),
start = list(a=0.22451, b=-0.18938, c=1.06941), trace=TRUE, subset=I1) 
</code></pre>

<p>These models work perfectly, but now I want to add prey type as well. In the linear models prey type was the most important variable so I don't want to leave it out. I understand that you can't add categorical variables in nls, so I thought I try a generalized additive model (GAM).</p>

<p>The problem with the gam models is that the smoothers (both spline and loess) don't work on both variables because there are only a very restricted number of values for prey density and predator density. I can manage to get a model with a single variable smoothed using loess. But for two variables it is simply not working. The spline function does not work at all because I have so few values (5) for my variables (see model 4).</p>

<pre><code>model3 &lt;- gam(rates~ lo(pred, span=0.9)+prey)
## this one is actually working but does not include a smoother for prey.

model4 &lt;- gam(rates~ s(pred)+prey)
## this one gives problems: 
A term has fewer unique covariate combinations than specified maximum degrees of freedom
</code></pre>

<p>My question is: are there any other possibilities to model data with 2 non-linear related variables in which I can also include a categorical variable. I would prefer to use <code>nls</code> (<code>model2</code>) with for example different intercepts for each category but I'm not sure how to get this sorted, if it is possible at all. The dataset is too small to split it up into the three categories, moreover, one of the categories only contains 5 data points.</p>

<p>Any help would be really appreciated.</p>
"
"0.0749231094763201","0.0734364498908627"," 63566","<p>I have conducted an experiment with multiple (categorical) conditions per subject, and multiple subject measurements.</p>

<p>My data-frame in short: A subject has one property, <code>is_frisian</code> which is either 0 or 1 depending on the subject. And it is tested for two conditions, <code>person</code> and <code>condition</code>. The measurement variable is <code>error</code>, which is either 0 or 1.</p>

<p>My mixed linear model in R is:</p>

<pre><code>&gt; model &lt;- lmer(error~is_frisian*condition*person+(1|subject_id), data=output)
</code></pre>

<p>However, the residuals plot of this model gives an unexpected (?) result.</p>

<p><img src=""http://i.stack.imgur.com/nz2KY.png"" alt=""Residuals lmer model""></p>

<p>I was taught that this plot should show randomly scattered points, and they should be normal distributed. When plotting the density of the fitted and the residuals, it shows a reasonable normal distribution. The lines you can see in the graph, however, how is this to be explained? And is this okay?</p>

<p>The only thing I could come up with is that the graph has two lines due to the categorical variables. The output variable <code>error</code> is either 0 or 1. But I do not have that much knowledge of the underlying system to confirm this. And then again, the lines also seem to have a low negative slope, is this then perhaps a problem?</p>

<p><strong>UPDATE:</strong></p>

<pre><code>&gt; model &lt;- glmer(error~is_frisian*condition*person + (1|subject_id), data=output, family='binomial')
&gt; binnedplot(fitted(model),resid(model))
</code></pre>

<p>Gives the following result:</p>

<p><img src=""http://i.stack.imgur.com/XMXFx.png"" alt=""binned residual plot""></p>

<p><strong>FINAL EDIT:</strong></p>

<p>The density-plots have been omitted, they have nothing to do with satisfaction of assumptions in this case. For a list of assumptions on logistic regression (when using family=binomial), <a href=""https://www.statisticssolutions.com/academic-solutions/resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/"" rel=""nofollow"">see here at statisticssolutions.com</a></p>
"
"0.0980973772790571","0.0961508829696314"," 63597","<p>I have data in a dataframe with the following columns: date, time, symbol, price
I am attempting to run the following model in R</p>

<pre><code> price ~ factor(time) + factor(symbol)*factor(time) + 0
</code></pre>

<p>where the first coefficient comes from a dummy variable of the time column in the dataframe and the second coefficient comes from the product of the dummy variable of the time column and the symbol column. I used lm() to attempt to do the model.</p>

<pre><code> fit&lt;-lm(price~factor(time) + factor(symbol)*factor(time) + 0, data=mydata) 
</code></pre>

<p>However, that didn't work as it gets me some really weird coefficients that didn't say anything. I guessing as that was matching effects in some way and not just the straight forward product.
I tried making a new variable by multiplying the product of the factors and then putting it in the regression but that didn't work as r told me that * is not in the possible operations for factor().
So, how can you multiply two factors in a linear regression?
Your help is much appreciated, thank you in advance.</p>

<p>edit: when I say weird I guess I should more say not what I want. I get coefficients for the firm, then coefficients for time then coefficients for firm and time. However, I only want the coefficients for firm and time to impact the model and ultimately I am really interested in what this does to the intraday effects which are approximated using the time dummy variables. If I am getting the interaction effects of the firm, time and then firm and time then this will impact the first coefficient as opposed to only firm and time affecting the first coefficient?</p>
"
"NaN","NaN"," 63698","<p>I have been checking out which diagnostics to use for a GEE analysis. It seem that influence measures are appropriate (Preisser, 1996). Does anyone know of a package that can be used in R to examine influence measures in GEE? The ""stats"" package does not work: it returns an error when I run</p>

<pre><code>influence.measures(model) 
</code></pre>

<p>on a gee model, but works fine with an ordinary linear regression. </p>
"
"0.0942489115008991","0.0923787802599364"," 63796","<p>This is related to a <a href=""http://stats.stackexchange.com/questions/62646/pooled-time-series-regression-in-r"">question</a> I asked a couple weeks ago, but I've got a new question related to the same data. You can find the data and its accompanying explanation in the link provided.</p>

<p>I felt that a regression including year as a covariate along with year dummies would lead to a linear dependence problem, but I was told to try it anyway as </p>

<blockquote>
  <p>""the year dummies as independent variables [may] pick up year-specific
  random effects not accounted for by a time trend, e.g. for example the
  trend over all years could be down by say 2 percent per year which
  could apply to most years, but a negative macro shock in one
  particular year could make that year lie way off the regression
  line--a simple example of why the year dummies are not co-linear with
  a time trend.""</p>
</blockquote>

<p>This makes sense, I suppose, so I ran a regression that simply included year and year dummies for each year as the independent variables (including AR(1) corrections). This looked like the following:</p>

<pre><code>&gt; ## Generate YearFactor and AgeGroupFactor using factor()
&gt; 
&gt; YearFactor &lt;- factor(YearVar)
&gt; AgeGroupFactor &lt;- factor(AgeGroup)
&gt; 
&gt; ## Check to see that YearFactor and AgeGroupFactor are indeed factor variables
&gt; 
&gt; is.factor(YearFactor)
[1] TRUE
&gt; is.factor(AgeGroupFactor)
[1] TRUE
&gt;
&gt; ## Run regressions with both time trend and year dummies to determine if a linear dependence problem exists.
&gt; 
&gt; TrendDummies &lt;- gls(PPHPY ~ YearVar + YearFactor, correlation=corARMA(p=1))
Error in glsEstimate(object, control = control) : 
 computed ""gls"" fit is singular, rank 13
&gt; summary(TrendDummies)
Error in summary(TrendDummies) : object 'TrendDummies' not found
&gt;
</code></pre>

<p>I interpret the error message ""Error in glsEstimate(object, control = control) : 
     computed ""gls"" fit is singular, rank 13"" to mean that there indeed is a linear dependence problem in this case. Am I properly interpreting this? </p>

<p>Also, given the advice in quotes above, would my regression as constructed (if there were no linear dependence problems) capture the effects mentioned therein?</p>

<p>And finally, if I run the same regression as OLS with no AR(1) correlation structure, I do indeed get some results (instead of an error message). Any thoughts on that?</p>
"
"0.0490486886395286","0.0480754414848157"," 63883","<p>Is there a method to find the right distance function in non-parametric regression?
I use some time series to learn forecasting. Series are nonlinear and non-gaussian.
I can get the right dimension and delay. I can find the right bandwidth with the hdrcde library.
I have no problem with kernel functions.<br>
My problem is with distances. I use Euclidian, Cosine and Correlation weighting functions.
These are the kernel which give good results, but one time, Euclidian is good, then after adding some data, one to 5-7 generally, Euclidian give nothing, even with very little change in statistics.
So, my question is if there is a method to choose the right distance. I would like to have advices on that point and on articles that will help solve this problem. What package in R could eventually help?</p>

<p>Thank you. </p>
"
"0.0895502439463906","0.0877733458775107"," 63913","<p>I conducted an experiment in a factorial design: I measured light (PAR) in three herbivore treatments as well as six nutrient treatments. The experiment was blocked.</p>

<p>I've run the linear model as follows (you can download the data from my website to replicate)</p>

<pre><code>dat &lt;- read.csv('http://www.natelemoine.com/testDat.csv')
mod1 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
</code></pre>

<p>The residual plots look pretty good</p>

<pre><code>par(mfrow=c(2,2))
plot(mod1)
</code></pre>

<p>When I look at the ANOVA table, I see main effects of Nutrient and Herbivore. </p>

<pre><code>anova(mod1)

Analysis of Variance Table 

Response: light 
                    Df  Sum Sq Mean Sq F value    Pr(&gt;F)     
Nutrient             5  4.5603 0.91206  7.1198 5.152e-06 *** 
Herbivore            2  2.1358 1.06791  8.3364 0.0003661 *** 
BlockID              9  5.6186 0.62429  4.8734 9.663e-06 *** 
Nutrient:Herbivore  10  1.7372 0.17372  1.3561 0.2058882     
Residuals          153 19.5996 0.12810                       
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
</code></pre>

<p>However, the regression table shows non-significant main effects and significant interactions.</p>

<pre><code>summary(mod1)

Call: 
lm(formula = light ~ Nutrient * Herbivore + BlockID, data = dat) 

Residuals: 
     Min       1Q   Median       3Q      Max  
-0.96084 -0.19573  0.01328  0.24176  0.74200  

Coefficients: 
                           Estimate Std. Error t value Pr(&gt;|t|)     
(Intercept)                1.351669   0.138619   9.751  &lt; 2e-16 *** 
Nutrientb                  0.170548   0.160064   1.066  0.28833     
Nutrientc                 -0.002172   0.160064  -0.014  0.98919     
Nutrientd                 -0.163537   0.160064  -1.022  0.30854     
Nutriente                 -0.392894   0.160064  -2.455  0.01522 *   
Nutrientf                  0.137610   0.160064   0.860  0.39129     
HerbivorePaired           -0.074901   0.160064  -0.468  0.64049     
HerbivoreZebra            -0.036931   0.160064  -0.231  0.81784     
... 
Nutrientb:HerbivorePaired  0.040539   0.226364   0.179  0.85811     
Nutrientc:HerbivorePaired  0.323127   0.226364   1.427  0.15548     
Nutrientd:HerbivorePaired  0.642734   0.226364   2.839  0.00513 **  
Nutriente:HerbivorePaired  0.454013   0.226364   2.006  0.04665 *   
Nutrientf:HerbivorePaired  0.384195   0.226364   1.697  0.09168 .   
Nutrientb:HerbivoreZebra   0.064540   0.226364   0.285  0.77594     
Nutrientc:HerbivoreZebra   0.279311   0.226364   1.234  0.21913     
Nutrientd:HerbivoreZebra   0.536160   0.226364   2.369  0.01911 *   
Nutriente:HerbivoreZebra   0.394504   0.226364   1.743  0.08338 .   
Nutrientf:HerbivoreZebra   0.324598   0.226364   1.434  0.15362     
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3579 on 153 degrees of freedom 
Multiple R-squared:  0.4176,    Adjusted R-squared:  0.3186  
F-statistic: 4.219 on 26 and 153 DF,  p-value: 8.643e-09 
</code></pre>

<p>I know that this question has been previously <a href=""http://stats.stackexchange.com/questions/20002/regression-vs-anova-discrepancy"">asked and answered</a> in <a href=""http://stats.stackexchange.com/questions/28938/why-do-linear-regression-and-anova-give-different-p-value-in-case-of-consideri"">multiple posts</a>. In the earlier posts, the issue revolved around the different types of SS used in anova() and lm(). However, I don't think that is the issue here. First of all, the design is balanced:</p>

<pre><code>with(dat, tapply(light, list(Nutrient, Herbivore), length))
</code></pre>

<p>Second, using the Anova() option doesn't change the anova table. This isn't a surprise because the design is balanced.</p>

<pre><code>Anova(mod1, type=2)
Anova(mod1, type=3)
</code></pre>

<p>Changing the contrast doesn't change the results (qualitatively). I still get pretty much backwards intepretations from anova() vs. summary().</p>

<pre><code>options(contrasts=c(""contr.sum"",""contr.poly""))
mod2 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
anova(mod2)
summary(mod2)
</code></pre>

<p>I'm confused because everything I've read on regression not agreeing with ANOVA implicates differences in the way R uses SS for summary() and anova() functions. However, in the balanced design, the SS types are equivalent, and the results here don't change. How can I have completely opposite interpretations depending on which output I use?</p>
"
"0.0800961731463273","0.078506867197886"," 63927","<p>I am struggling to fit a simple logistic regression for one dependent value (group) by one independent qualitative variable (dilat) measured twice independently (rater).</p>

<p>I try many solutions and think according <a href=""http://www.ats.ucla.edu/stat/mult_pkg/whatstat/"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/whatstat/</a> that the solution is a Mixed Effects Logistic Regression.</p>



<pre class=""lang-r prettyprint-override""><code>glmer_dilat&lt;-glmer(group ~ dilat + (1 | rater), data = ex, family = binomial)
summary(glmer_dilat)
</code></pre>



<pre class=""lang-r prettyprint-override""><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: group ~ dilat + (1 | rater) 
   Data: ex 
   AIC   BIC logLik deviance
 105.5 112.5 -49.74    99.48
Random effects:
 Groups Name        Variance Std.Dev.
 rater  (Intercept)  0        0      
Number of obs: 76, groups: rater, 2

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4880   1.736   0.0825 .
dilat        -1.2827     0.5594  -2.293   0.0219 *
</code></pre>

<p>But the result is the same without !</p>

<pre class=""lang-r prettyprint-override""><code>summary(glm(group ~ dilat, data =ex, family = binomial))

glm(formula = group ~ dilat, family = binomial, data = ex)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.552  -0.999  -0.999   1.367   1.367  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4879   1.736   0.0825 .
dilat        -1.2826     0.5594  -2.293   0.0219 *
</code></pre>

<p>What is the solution?</p>

<p>please find my data set here after applying a dput command to it.</p>

<pre class=""lang-r prettyprint-override""><code>structure(list(id = structure(c(38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 23L, 15L, 24L, 25L, 37L, 26L, 38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 22L, 23L, 15L, 24L, 37L, 26L), .Label = c(""1038835"", ""2025267"", ""2053954"", ""3031612"", ""40004760"", ""40014515"", ""40040532"", ""40092413"", ""40101857"", ""40105328"", ""4016213"", ""40187296"", ""40203950"", ""40260642"", ""40269263"", ""40300349"", ""40308059"", ""40327146"", ""40333651"", ""40364468"", ""40435267"", ""40440293"", ""40443920"", ""40485124"", ""40609779"", ""40628741"", ""40662695"", ""5025220"", ""E9701737"", ""M/377313"", ""qsc22913"", ""QSC29371"", ""QSC43884"", ""QSC62220"", ""QSC75555"", ""QSC92652"", ""QSD01289"", ""QSD02237"", ""U/FY0296"" ), class = ""factor""), group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), rater = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), dilat = c(1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), midbrain_atroph = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), quadrigemi_atroph = c(1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), hum_sig = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), flower_sig = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), fp_atroph = c(0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), scp_atroph = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L)), .Names = c(""id"", ""group"", ""rater"", ""dilat"", ""midbrain_atroph"", ""quadrigemi_atroph"", ""hum_sig"", ""flower_sig"", ""fp_atroph"", ""scp_atroph""), class = ""data.frame"", row.names = c(NA, -76L))
</code></pre>
"
"0.0400480865731637","0.039253433598943"," 64268","<p>Hi currently I am conducting simple linear regression on two variables for data of different regions. I know that I can use the lmList function to get the coefficients at once for all the regions. But can I generate the Q-Q residual plot for all the regions in one graph with different panels at once? And for the output for lmList function, only the coefficients are displays, without R-square for each regression. How can I see that?</p>

<p>Thanks a lot for help in advance!</p>
"
"NaN","NaN"," 64469","<p>If you would fit a small sample like with the standard framework for linear models. Would you suggest from the residuals any special correlation structure like compound symmetry or first order autoregression?</p>

<pre><code>Y&lt;-data.frame(response=c(10,19,27,28,9,13,25,29,4,10,20,18,5,6,12,17),
               treatment=factor(rep(1:4,4)),
               subject=factor(rep(1:4,each=4))
               )
fit&lt;-lm(response~-1+treatment,Y)
</code></pre>
"
"0.0490486886395286","0.0480754414848157"," 64861","<p>I'm fitting a linear regression model in R:</p>

<pre><code>head(data)  
   `x1`         `x2`     `y`  
    4.9711     32.70   0.2810632  
    6.756072   31.60   0.3115225  
    5.895213   56.50   0.4853171  
    1.951329   29.80   0.3010127  

EDIT: fit &lt;- glm(y ~ x1 + x2,family = binomial(link = logit), data = data, control= list(epsilon = 0.0001, maxit = 50, trace = F)))
</code></pre>

<p>where <code>y</code> is a proportion.</p>

<p>Here's my code:</p>

<pre><code>fit.l.hat=predict.glm(fit,se.fit=TRUE,type=""response"")    
ci=c(fit.l.hat$fit - 1.96*fit.l.hat$se.fit, fit.l.hat$fit + 1.96*fit.l.hat%se.fit)
</code></pre>

<p>For the first fitted value, <code>x1 = 4.97</code> and <code>x2 = 32.7</code>.
So from my above code, I get this output:</p>

<pre><code>Fitted value: 0.35, Lower 95%: 0.01, Upper 95%: 0.68, Standard Error: 0.17
</code></pre>

<p>and so on.</p>

<p>So my question is how to actually interpret this for ""dummies"". 
I would say ""For an input of 4.97 and 32.7, 95 out of 100 samples will conclude a proportion between 0.01 and 0.68."" Is that correct?</p>

<p>What other information can I get out of this, or what other ""dummy"" ways are there to present this information? And more importantly, <strong>how do I quantify the error?</strong></p>
"
"0.0424774103841449","0.0416345536240027"," 65174","<p>I am doing a logistic regression analysis using the glm command in R. It is to identify causes of valve narrowing beyond a certain threshold; 0=no narrowing, 1=narrowed. One of my variables is the size of a medical device that is implanted (range 25-36mm). Sometimes the device isn't implanted and I've left this as a blank field, but of course this is interpreted as a missing field. Not implanting the device seems to have a significant effect using Chi-sq analysis, and the size of the device has a significant effect using a t-test. How do I get around this in a linear regression model?</p>

<p>To make it more complicated I actually have two different makes of the device: ""C"" and ""D"" with sizes 25-36mm, another device without a size ""S"" and then no device ""N"". Can it all be entered together or is it best to analyze separately outside of regression?</p>

<p>What effect does the ""missingness"" have on various other variables that are in the analysis?</p>

<p>Please &amp; thankyou</p>
"
"0.0633215847514023","0.0620651280774201"," 65548","<p>Here is the kind of data I have:</p>

<p>I have two predictor variables: </p>

<p>1) discrete non-ordinal --> c('a','b','c') </p>

<p>2) discrete ordinal --> c(10,100,200,500)</p>

<p>Response variable: Proportion of TRUE over a list of TRUE/FALSE. If the list is of length 3, my variable can take only 4 values. But not all my values come from the same list's length. And moreover the lists are globally long ! So it is discrete proportions but can take more than 100 values.</p>

<p>Here is an example (resp is a subset of my data):</p>

<pre><code>pred_1 = rep(c(10,20,50,100),30)
pred_2 = rep(c('a','b','c'),40)

resp = c(0.08666667, 0.04000000, 0.13333333, 0.04666667, 0.50000000, 0.04000000, 0.02666667, 0.24666667, 0.15333333, 0.04000000, 0.06666667, 0.06666667, 0.03333333,
    0.04000000, 0.26000000, 0.04000000, 0.04000000, 1.00000000, 0.28666667, 0.03333333, 0.06666667, 0.15333333, 0.06666667, 0.28000000, 0.35333333, 0.06000000,
    0.06000000, 0.05333333, 0.96666667, 0.06666667, 0.03333333, 0.22000000, 0.04666667, 0.04666667, 0.05333333, 0.05333333, 0.05333333, 0.08000000, 0.48666667,
    0.08666667, 0.02666667, 0.21333333, 0.45333333, 0.04666667, 0.36000000, 0.06666667, 0.04000000, 0.06000000, 0.07333333, 0.06000000, 0.04000000, 0.04666667,
    0.30000000, 0.08666667, 0.07333333, 0.06666667, 0.29333333, 0.36000000, 0.17333333, 0.04000000, 0.09333333, 0.11333333, 0.03333333, 0.08000000, 0.27333333,
    0.08666667, 0.03333333, 0.04000000, 0.02666667, 0.07333333, 0.07333333, 0.02000000, 0.02666667, 0.08000000, 0.07333333, 0.02666667, 0.06666667, 0.07333333,
    0.95333333, 0.05333333, 0.04000000, 0.11333333, 0.04000000, 0.07333333, 0.06666667, 0.05333333, 0.04000000, 0.04000000, 0.06000000, 0.12666667, 0.04666667,
    0.04000000, 0.21333333, 0.05333333, 0.97333333, 0.11333333, 0.02666667, 0.04000000, 0.03333333, 0.37333333, 0.25333333, 0.06000000, 0.06000000, 0.06000000,
    0.04666667, 0.26666667, 0.98000000, 0.02000000, 0.26000000, 0.06000000, 0.05333333, 0.28000000, 0.99333333, 0.04666667, 0.02666667, 0.04000000, 0.12666667,
    0.04666667, 0.18000000, 0.03333333) 
</code></pre>

<p>my response variable is not at all normally distributed (kolmogorov-smirnow and shapiro test + visual checking with qqplot()) nor is the residuals of a linear model (lm()). Moreover the common assumption of homoscedasticity is not respected neither.</p>

<p>I've always asked a similar question but not as much accurate <a href=""http://stats.stackexchange.com/questions/65388/which-model-should-i-use-logistic-regression"">here</a>.
Peter Flom has suggested that I use a ordinal logistic regression (polr()) but I might not have given him enough information (he did not know the number of levels for example). What do you think ? Which model would you suggest me ? Can I make a polr() with that much levels ? When I do it I actually get this:</p>

<p>Error message:
""Initial value ""vmin"" in not finite""</p>

<p>Notification message:
""glm.fit: fitted probabilities numerically 0 or 1 occurred ""</p>

<p>I'm struggling on this problem for quite a long time. All your contributions are more than welcome !</p>

<p>Thanks a lot !</p>
"
"0.02831827358943","0.0277563690826684"," 65900","<p>I'm not used to using variables in the date format in R. I'm just wondering if it is possible to add a date variable as an explanatory variable in a linear regression model. If it's possible, how can we interpret the coefficient? Is it the effect of one day on the outcome variable? </p>

<p>See my <a href=""https://gist.github.com/pachevalier/5966314"" rel=""nofollow"">gist</a> with an example what I'm trying to do. </p>
"
"NaN","NaN"," 66335","<p>I calculate multiple linear regression in R with </p>

<pre><code>lm(var ~ VAR1+VAR2+VAR3+VAR4)
</code></pre>

<p>Do you know how to calculate R-squared change for each variable <code>VAR1</code>, <code>VAR2</code>, <code>VAR3</code> ?
Thank you</p>
"
"0.0490486886395286","0.0480754414848157"," 66390","<p>Perhaps this is more of a programming question than a stats question, but I'm sure someone here has the answer.  I am new to R and am trying to run a linear regression on multiple subsets (""Cases"") of data in a single file.  I have 50 different cases, so I don't want to have to run 50 different regressions...be nice to automate this.  I have found and experimented with the <code>ddply</code> method, but this, for some reason, returns the same coefficients to me for each case.  Code I'm using is as follows:</p>

<p><code>ddply(MyData, ""Case"", function(x) coefficients(lm(Y~X1+X2+X3, MyData)))</code></p>

<p>Results I get, again, are the same coefficients for each ""Case"".  Any ideas on how I can improve my code so that the regression runs once for each case and gives me unique coefficients for each case?</p>
"
"0.0490486886395286","0.0480754414848157"," 66757","<p>I ran an OLS regression model on data set with 5 independent variables. The independent variables and dependent variable are both continuous and are linearly related. The R Square is about 99.3%. But when I run the same using random forest in R my result is '% Var explained: 88.42'. Why would random forest result be so inferior to regression? My assumption was that random forest would be at least as good as OLS regression.</p>
"
"0.0895502439463906","0.0877733458775107"," 67049","<p>Surprisingly, I was unable to find an answer to the following question using Google:</p>

<p>I have some biological data from several individuals that show a roughly sigmoid growth behaviour in time. Thus, I wish to model it using a standard logistic growth</p>

<pre><code>P(t) = k*p0*exp(r*t) / (k+p0*(exp(r*t)-1))
</code></pre>

<p>with p0 being the starting value at t=0, k being the asymptotic limit at t->infinity and r being the growth speed. As far as I can see, I can easily model this using nls (lack of understanding on my part: why can I not model something similar using standard logit regression by scaling time and data? EDIT: Thanks Nick, apparently people do it e.g. for proportions, but rarely <a href=""http://www.stata-journal.com/article.html?article=st0147"">http://www.stata-journal.com/article.html?article=st0147</a> . Next question on this tangent would be if the model can possibly handle outliers >1).</p>

<p>Now I wish to allow some fixed (mainly categorical) and some random (an individual ID and possibly also a study ID) effects on the three parameters k, p0 and r. Is nlme the best way of doing this? The SSlogis model seems sensible for what I am trying to do, is that correct? Is either of the following a sensible model to begin with? I cannot seem to get the starting values right and update() only seems to work for random effects, not fixed ones - any hints?</p>

<pre><code>nlme(y ~ k*p0*exp(r*t) / (k+p0*(exp(r*t)-1)), ## not working at all (bad numerical properties?)
            data = data,
            fixed = k + p0 + r ~ var1 + var2,
            random = k + p0 + r ~ 1|UID,
            start = c(p0=1, k=100, r=1))

nlme(y ~ SSlogis(t, Asym, xmid, scal), ## not working, as start= is inappropriate
            data = data,
            fixed = Asym + xmid + scal ~ var1 + var2, ## works fine with ~ 1
            random = Asym + xmid + scal ~ 1|UID,
            start = getInitial(y ~ SSlogis(Dauer, Asym, xmid, scal), data = data))
</code></pre>

<p>As I am new to non-linear mixed models in particular and non-linear models in general, I would appreciate some reading recommendations or links to tutorials / FAQs with newbie questions.</p>
"
"0.0633215847514023","0.0620651280774201"," 67209","<p>I'm a novice attempting to predict automobile sales using a combination of previous sales (seasonal AR model), macroeconomic indicators such as CPI, consumer sentiment index etc. and more significantly, I'm using weekly data from google search trends for the automobile and other variables from the category (automobile) in google trends. (code developed using Choi and Varian's 2009 paper). The model essentially looks at search trend indices for three weeks before the month in question</p>

<p>In addition to running spike and slab regression to determine attribute importance, I also ran a gradient boosted machine (R package GBM). as of now, the model seems to be performing fairly satisfactorily, with the Mean Average Error = 5.4 (monthly sales are in the range of 10000), however there are clear seasonal trends in the difference between predicted variables and actual sales that seem to coincide with sales promotions and other promotional events. My question, therefore is:</p>

<p>How do I account for these seasonal trends while using GBM? or for a linear model?</p>
"
"0.0400480865731637","0.039253433598943"," 67257","<p>I have difficulties fitting a joint model in <code>R</code>. My data consists of two responses <code>X</code> &amp; <code>Y</code> and one predictor variable <code>Z</code>. Now I want to model both <code>X</code> and <code>Y</code> in function of <code>Z</code> (just linear regression: $E(X|Z)=Z\alpha$ and $E(Y|Z)=Z\beta$, both outcomes are normally distributed) but while doing so I also want to estimate the variance covariance matrix since it is the correlation between <code>X</code> and <code>Y</code> that I am interested in.</p>

<p>I already looked into a couple of functions (<code>lm</code>, <code>mcer</code>, <code>lme</code>) but it doesn't seem to do the trick. Is there something that I am overlooking in a certain package or a new suggestions to try?</p>
"
"0.0400480865731637","0.039253433598943"," 67420","<p>I'm trying to do a multiple linear regression model to correlate a dependent $Y$ variable (normally distributed) against a set of 642 variables. These 642 variables codify for the presence or absence of a particular chemical group in a molecule, therefore they can assume only <code>1</code> or <code>0</code> as value. The size of the dataset is 1300 experiments (one $Y$) and 642 $X$ variables. The problem I have is that the 642 variables are set to <code>1</code> only 0.3% of the times, i.e., my 1300 x 642 X-matrix is made of 2600 <code>1</code> and 832000 <code>0</code>. Nothing else (except the $Y$ of course). Does high frequency of <code>0</code> create problem for the MLR implementation in R? Is there a better statistical method more suitable to treat this case?</p>
"
"0.0400480865731637","0.039253433598943"," 67473","<p>I'm using <a href=""http://cran.r-project.org/web/packages/kernlab/index.html"" rel=""nofollow"">kernlab package</a></p>

<p>Here are two examples:<br/>
First:</p>

<pre><code>library(kernlab)
x &lt;- runif(1020, 1, 5000)
y &lt;- sqrt(x)
model.vanilla &lt;- rvm(x, y, kernel='vanilladot')
</code></pre>

<p>Got error:</p>

<pre><code>Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) :
the leading minor of order 2 is not positive definite
</code></pre>

<p>Second:</p>

<pre><code>library(kernlab)
x &lt;- runif(1020, 1, 5000)
y &lt;- sqrt(x)
model.rbf &lt;- rvm(x[1:1000], y[1:1000], kernel='rbfdot')
print(model.rbf)
py.rbf &lt;- predict(model.rbf, x[1001:1020])
print(paste(""MSE: "", sum((py.rbf - y[1001:1020]) ^ 2) / length(py.rbf)))
</code></pre>

<p>OK:</p>

<pre><code>Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Relevance Vector Machine object of class ""rvm"" 
Problem type: regression 

Gaussian Radial Basis kernel function. 
 Hyperparameter : sigma =  5.44268665122008e-06 

Number of Relevance Vectors : 247 
Variance :  4.368e-06
Training error : 3.418e-06 
[1] ""MSE:  4.921706631013e-05""
</code></pre>

<p>Why doesn't using linear kernel work here? <code>polydot</code> (polynomial kernel function) doesn't work either.</p>

<p>Can this be fixed?</p>
"
"0.02831827358943","0.0277563690826684"," 67484","<p>I've got a set of input-output vector pairs, and I want to find a function that approximates the output vectors from the input vectors. Specifically, I want a <em>matrix</em> by which to multiply an input vector such that I get a good approximation for the output.</p>

<p>I guess <em>linear</em> regression is not what I'm looking for, or else I wouldn't know what to do with the linear function. So what sort of regression do I need to apply here? Literature tips and / or Java / Python / R packages are very welcome!</p>
"
"0.0633215847514023","0.0620651280774201"," 67843","<p>I am working on a project in which I am using several independent variables to ""predict"" the values of an outcome using linear regression.</p>

<p>In R this is done quite simply as</p>

<pre><code>model  &lt;- lm(outcome ~ predictor1 + predictor2 + predictor3)
fitted &lt;- model$fitted.values
</code></pre>

<p>I am interested in the <em>difference</em> between the predicted values and the actual values - i.e. how accurate the predictors are. </p>

<pre><code>residuals &lt;- model$residuals
</code></pre>

<p>My question relates to the relationship between <code>residuals</code> and <code>outcome</code>. </p>

<p>Samples with lower values of <code>outcome</code> tend to have negative values for <code>residuals</code>, and vice versa for samples with high <code>outcome</code> values. </p>

<p>Plotting the values against one another is the simplest way to see this:</p>

<p><img src=""http://i.stack.imgur.com/U5dPa.png"" alt=""Various plots""></p>

<p>The $R^2$ for the original LM (outcome ~ predictors) is 0.42, the $R^2$ between <code>residuals</code> and <code>outcome</code> is 0.58, and the $R^2$ between <code>fitted</code> and <code>outcome</code> is 0.39.</p>

<p>What could explain the phenomenon? Why would samples with high <code>outcome</code> tend to be predicted lower than they actually are, and vice versa for lower values of <code>outcome</code>? Or indeed, am I missing something conceptually here?</p>

<p>Many thanks for your input</p>

<hr>

<p>Edited (13.08.20) to include an updated plots and terminology (now use ""residuals"" rather than ""difference"") - but in essence the questions remains the same. Thanks all for the input so far.</p>
"
"0.18574331527078","0.177920033821938"," 67873","<p><strong>TLDR</strong>: How can I perform inference for the between group differences in a possibly logistic growth with time in the presence of outliers, unequal measurement times and frequency, bounded measurements and possible random effects on individual and per study level?</p>

<p>I am attempting to analyse a dataset where measurements for individuals were made at different time points. Measurements start low at time 0 and follow (very roughly) a logistic growth pattern with time. I am trying to establish if there are differences between two groups of individuals. The analysis is complicated by the following factors:</p>

<ul>
<li>The effect of time is non-linear, so either a non-linear logistic regression (biologically plausible, but not particularly well fitting) or a non-parametric regression seem appropriate</li>
<li>There are massive outliers, so regression using the sum of squared residuals seems off the table. Quantile regression seems appropriate.</li>
<li>Random effects may be appropriate on a per individual and per study level. Mixed effects models seems appropriate.</li>
<li>Measurement times, number of available measurements and end of monitoring differ between individuals. Survival analysis techniques seem appropriate. Possibly also applying weights equal to 1 / number of observations for individual.</li>
<li>Measurements are bounded below at 0 and while there is no obvious boundary above, arbitrarily high measurements seem biologically implausible. However, quite a few individuals have some measurements of zero (partly due to the measurement accuracy of the device).</li>
<li>A few models I tried so far failed to fit, usually with an unhelpful error related to the numerical procedure. This leads me to believe that I will need a reasonably robust method able to deal with this somewhat ugly dataset.</li>
<li>Finally, I want to produce inference of the form ""group 1 has faster growth than group 2"" or ""group 1 has a higher asymptotic level than group 2"".</li>
</ul>

<p>What I have tried so far (all in R) - I was aware that most of the below are not particularly appropriate for the dataset, but I wanted to see which models could actually be fitted without numerical errors:</p>

<ul>
<li>Non-parametric regression using crs in the crs package. Nicely produces a curve reasonably close to logistic growth for most of the time period with some strange behavious toward the end of the monitoring period (where there are fewer observations). Using individuals as fixed effects reveals some outliers. Using the variable of interest as fixed effects shows some difference. However, I am not sure if there is any way to assess fits and do inference on a model this complex.</li>
<li>Non-linear mixed effects regression using nlme in package nlme and SSlogis. Gradually building up the model with update() works reasonably well. Getting too complex with the fixed effects or the random effects leads to convergence failure. Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further. Edit: I have recently become aware that it is possible to specify autocorrelated residuals in nlme. However, at the moment it seems I cannot even get fixed weights to work. Advice on the correct syntax is welcome.</li>
<li>Non-linear mixed effects regression using nlmer in package LME4 and a custom likelihood for the logistic growth model. Works fairly well, but standard errors on the fixed effects get massive, probably due to the outliers. I also have the slight suspicion that some of the models fail to fit without error, as I sometimes get tiny random effects (about 10^10 smaller than with slightly simpler models). Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further.</li>
<li>Non-linear quantile regression using nlrq in package quantreg and SSlogis. Fits reliably and quickly, but percentile lines intersect. This means that an area containing 90% of the data is not fully contained in an area containing 95% of the data.</li>
<li>Non-parametric quantile regression using the LMS method with package VGAM. Even trivial models failed with obscure errors using this dataset. I believe the number of zeros in the dataset and / or the large range of the data while also getting close to zero may be an issue.</li>
<li>To complete this list, I should probably also mention the lqmm package for Linear Quantile Mixed Models, which I have not used yet. While the package cannot use non-linear models as far as I know, transforming the time variable may produce something reasonably close.</li>
</ul>

<p>I would appreciate feedback if these or any other method might be used to produce reasonably robust inference in this scenario. Maybe regression is not needed at all and another, possibly simpler method is sufficient. I'd be happy to provide an example dataset, if required, but think this question might also be of interest beyond the current dataset.</p>
"
"0.0633215847514023","0.0620651280774201"," 68129","<p>In addition to <code>proc varclus</code>, <code>randomForest</code>, and assessing multicollinearity among potential predictor variables, I am seeking other methods of variable selection in lieu of using stepwise methods for building more parsimonious binary logistic regression models from a wide array of potential predictor variables. I have done some research into other methods such as <a href=""http://en.wikipedia.org/wiki/Mutual_information"" rel=""nofollow"">Mutual Information</a> (MI), and I have two questions in regards to its use:</p>

<p>1) Has anyone used MI for binary logistic regression variable selection? If so, what are your thoughts on its application?</p>

<p>2) Does anyone know how to calculate MI using either Base SAS or R for potential predictor variables in reference to the outcome of interest? Any help or references in this area would be greatly appreciated!</p>

<p>Thanks!</p>
"
"0.0578044339088637","0.0566574511374171"," 68137","<p>I have a data set consisting of one continuous response variable and about 70 predictors. Using this data, I want to construct a linear regression model. However, I don't know what predictors are worth including in the model, so I'll need to utilize a variable selection method that will allow me to isolate specific response variables. Unfortunately, I've noticed that my data violates a number of assumptions associated with linear regression. Therefore, I'll need to utilize a different estimation than OLS, such as robust or least squares estimation.</p>

<p>When running a linear regression with a different estimation method than least squares, how does one utilize a variable selection method such as stepwise? How can this be implemented in R?</p>
"
"0.139595015110802","0.152027894610165"," 68812","<p>I'm really new to ARIMA methods and am trying to forecast electricity load. I've integrated: electricity load, temperature, weekday (dummy), public holidays, and school holidays. My model tries to perform a non seasonal ARIMA with linear regression for each hour of the day.</p>

<p>Here is my code for an example of one of the 24 hours (6 AM):</p>

<pre><code># ElecLoad contains hourly loads and other data for 2005 and 2006 (=2*365*24 entries):
# 1. Electricity load in MW
# 2. day of weak: sunday=0, monday=1, etc 
# 3. Hour of the day 0 -&gt; 23
# 4. Public Holiday: 1 if Public Holiday, 0 otherwise
# 5. Scool vacation: 1 if no scool
# 6. Temperature in Â°F

# Create the weak matrix = dumy variables for the weakdays
weakmatrix&lt;-model.matrix(~as.factor(ElecLoad[,2]))
#Remove intercept
weakmatrix&lt;-weakmatrix[,-1]

#Generate FullTable
FullTable&lt;-cbind(load=ElecLoad[,1], weakmatrix, ElecLoad[,4],
                 ElecLoad[,3],ElecLoad[,5],ElecLoad[,5]^2, ElecLoad[,6])
colnames(FullTable)&lt;-c(""Load"",""mon"",""tue"",""wed"",""thu"",""fri"",""sat"",
                       ""ScoolHol"",""PubHol"",""Temp"",""Temp2"",""Hour"")

#Create the xreg = substed for a specific hour of the day (column 12 = Hour)
xreg&lt;-subset(FullTable[,2:11], FullTable[,12] == 7)

#Create the Load time serie, also a subset of the full table
LoadTs&lt;-ts(subset(FullTable[,1], FullTable[,12] == 7),start=1,frequency=1)

#Launch of auto.arima
ArimaLoad&lt;-auto.arima(LoadTs, xreg=xreg, lambda=0)
</code></pre>

<p>When I try to forecast with the same 2 years data as <code>xreg</code>, here is my output</p>

<pre><code>plot(forecast(ArimaLoad,xreg=xreg), include=0)
</code></pre>

<p><img src=""http://i.stack.imgur.com/MpNeH.png"" alt=""enter image description here""></p>

<p>While when I try to plot the fitted it looks identical to my original Load</p>

<pre><code>plot(fitted(ArimaLoad))
</code></pre>

<p><img src=""http://i.stack.imgur.com/zsw81.png"" alt=""enter image description here""></p>

<p>I don't understand why the <code>prediction()</code> is so much different than the <code>fitted()</code> with the same <code>xreg</code> matrix. Is this a normal behaviour, how can I improve my model to better fit with the real situation?</p>

<hr>

<p>Thank you so much for your support.</p>

<p>I'm not sure I understood everything from what you propose.</p>

<p>You mean that I should build a first model to forecast the daily average load (I prefer the average than the sum because due to DST, some days don't have 24 hours...). This model would be deterministic, but I don't see what kind of model you're thinking off? Is a multilinear regression ok? I prefer to consider the log(load) to make the different parameters multiplicative which I think is better fit to the reality.
Then I should have 24 hourly models, taking the daily average then split with a sort seasonal effect?
Should I use somewhere an ARIMA model?
I'm not convince of considering the month as having an effect, in my opinion there is no reason that consumption is more important in January than August except if we consider the Temperature and Holidays effects. The hour of the day is related to the activity that's the reason why I'm considering the specific model for each hour. The same way each day of the weak is different.</p>

<p>I've tried a multilinear regression for the same hour (7:00 AM) and the result looks not so bad.</p>

<pre><code>#Create the frame.data
Load&lt;-subset(FullTable[,1], FullTable[,12] == 7)
FullData&lt;-cbind(LogLoad=log(Load), xreg)
FrameData&lt;-data.frame(FullData)

# multilinear regression
mlin&lt;-lm(LogLoad ~ mon+tue+wed+thu+fri+sat+ScoolHol+PubHol+Temp+`Temp2`, FrameData)
plot(exp(mlin$model$LogLoad), type=""l"",col=""blue"")
lines(exp(fitted(mlin)), col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/MpNeH.png"" alt=""enter image description here""></p>

<p>fitted() in red which is now exactly the same as predict() if I re-use the same data entry (2005-2006) and looks not so far from the original load in blue (no so bad for a simple model). I still don't fully understand why it did not work with ARIMA as it also takes into consideration multilinear regression.</p>

<p>Now my ""simple"" model already takes into account several parameters, like the temperature, the holiday, the school vacations the day of the weak and the hour of the day (local time, not UCT).
How can I improve my model further more? How can I make sure that the parameters are invariant? Is there a specific method?</p>
"
"0.09392108820677","0.0920574617898323"," 69371","<p>I have a set of predictors in a linear regression, as well as three control variables. The issue here is that one of my variables of interest is only statistically significant if the control variables are included in the final model. However, the control variables themselves are not statistically significant.</p>

<p>Here is how the multicollinearity of all my variables look like (including control variables):</p>

<pre><code> &gt; vif(lm(return ~ EQ + EFF + SIZE + MOM + MSCR + UMP, data = as.data.frame(port.df)))
       EQ      EFF     SIZE      MOM     MSCR      UMP 
 3.687171 3.481672 2.781901 1.064312 1.438596 1.003408

 &gt; vif(lm(return ~ EQ + MOM + MSCR, data = as.data.frame(port.df)))
       EQ      MOM     MSCR 
 1.359992 1.048142 1.412658 
</code></pre>

<p>My variables of interest are <strong>EQ, MOM and MSCR</strong>, and the control variables are <em>EFF, SIZE and UMP</em>. EQ is only significant if the three control var are included, and becomes insignificant when they are not:</p>

<ul>
<li><p>Here are the coefficients (1rst row) and t-stats (2nd row) when control variables are included (notice that EQ is statistically significant)</p>

<pre><code>       intercept           EQ          EFF        SIZE         MOM       MSCR          UMP
[1,] 0.005206246 -0.006310531 0.0001229055 0.004125551 0.007738259 0.00473377 5.838596e-06
[2,] 1.866628909 -1.746583234 0.0388823612 1.178460997 2.145062820 2.08131100 1.994863e-01
</code></pre></li>
<li><p>Now, here is the result of the regression when the control variables are excluded (notice that EQ is NOT statistically significant anymore)</p>

<pre><code>       intercept           EQ         MOM       MSCR
[1,] 0.007313402 -0.002111833 0.007128606 0.00668364
[2,] 2.652662996 -0.595391117 2.036985378 2.80177366
</code></pre></li>
</ul>

<p>The problem is that when I include my control variables, all my variables of interest are significant, but my control variables are not.</p>

<p>Which variables should I include in my final model? How should I structure my final model then, given the fact that the model will be used for forecasting?</p>

<p>Thank you,</p>
"
"0.0755153962384799","0.0832691072480053"," 69524","<p>I am trying to fit a nonlinear regression model in R using <code>nls()</code>. I have a form of the equation I want to fit to:</p>

<p>$$y = (a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e)$$</p>

<p>where the coefficients to be found in regression are a,b,c,d, and e. My data is output from a simulation model where $x_{1}$, $x_{2}$, and $x_{3}$ are all integers from $0$ to $10$, with the condition that $x_{1} + x_{2} + x_{3} \le 10$. $y$ is also integer valued and ranges from $0$ to roughly $1000$. The objective is to fit these data to a rate function that will be used in a Markov Chain.</p>

<p>When I try to fit this regression model directly using <code>nls()</code>, my <code>nlsResiduals</code> plot looks like this:</p>

<p><img src=""http://i.stack.imgur.com/6scJ3.png"" alt=""nls residuals""></p>

<p>I know that autocorrelated residuals are problematic, and that non-normal residuals can also be problematic. How can I fix this problem? I was thinking of using transforms on the data like</p>

<p>$$\log(y) = \log((a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e))$$</p>

<p>or</p>

<p>$$y^{1/n} = ((a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e))^{1/n}$$  where $n &gt; 1$. I've noticed if $n$ increases, my autocorrelation graph and QQ-plot look ""better"" (i.e., more scattered and more normal, respectively). </p>

<p>Both of these seem to correct a lot (but not all) of the autocorrelated residuals, and help to make the residuals more normally distributed. Am I on the right track here, or am I committing some cardinal sin in statistics? Once I settle on a transformation, how can I tell which is best?</p>

<p>Any help, suggestions, or comments are very appreciated.</p>
"
"0.0326991257596857","0.0480754414848157"," 69528","<p>This is my R code and running result:(See below)</p>

<p>How to judge is the linear regression model appropriate for this data set? Except R^2 value, Can the</p>

<p>p-value in last row of the running result mean something? What does this      p-value mean and can it mean the linear regression model appropriate for this data set? Why?</p>

<p>Thanks in advance.</p>

<pre><code>x=c(7,12,10,10,14,25,30,25,18,10,4,6)    
y=c(128,213,191,178,205,446,540,547,324,117,75,107)    
list(x,y)    
reg1 &lt;- lm(y~x)    
summary(reg1)    
plot(x, y)
abline(reg1)    
reg2 &lt;- lm(y~x-1)    
reg2    
summary(reg2)
#------------------    
Call:
lm(formula = y ~ x)

Residuals:
    Min      1Q  Median      3Q     Max 
-55.805 -21.085   3.139  14.946  80.859 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -22.753     21.846  -1.041    0.322    
x             19.556      1.335  14.652 4.38e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 37.23 on 10 degrees of freedom
Multiple R-squared:  0.9555,    Adjusted R-squared:  0.951 
F-statistic: 214.7 on 1 and 10 DF,  p-value: 4.38e-08
</code></pre>
"
"0.0326991257596857","0.0480754414848157"," 69860","<p>For the purpose of model selection, I am using the Bayes' factor to compare different combinations of predictors in a linear regression model.</p>

<p>I have used the function <code>regressionBF()</code> from the <code>library(BayesFactor)</code>, and I got the following results:</p>

<pre><code># &gt; regressionBF(return ~ FSCR + VAL, data = dataf)

# Bayes factor analysis
# --------------
#[1] FSCR       : 65.17482  Â±0%
#[2] VAL        : 0.1979875 Â±0.02%
#[3] FSCR + VAL : 23.58704  Â±0%

#Against denominator:
#  Intercept only 
</code></pre>

<p>I am not sure how to interpret these results. What do the percentage numbers next to the Bayes' factors mean?
Also, 65 and 23 seem pretty high for a Bayes' factor. How can I interpret that?</p>

<p>Any help would be appreciated. Thanks!</p>
"
"0.0800961731463273","0.0686935087981502"," 69906","<p>Is there a way/method/approach to decompose a time series data using regression splines:</p>

<ol>
<li>Seasonal time series into trend+seasonal+random component ?</li>
<li>A non seasonal time series into trend+random component ?</li>
</ol>

<p>I'm familiar with STL, Census and classical decomposition in R. All these techniques require time series data with seasonal component. We cannot extract trend if the time series is non seasonal (i.e., Frequency = 1). </p>

<p>I recently came across this interesting article which is data driven in the recent 2013 ISF. Any insights on methods like these that are data driven decomposition using regression splines and that can be readily programmed in software packages such R would be greatly helpful.</p>

<p>Thanks so much</p>

<p><strong>Detrending time series with cycle and seasonal components</strong>
<em>Tatyana Krivobokova and Francisco Rosales</em>
In this work we discuss a nonparametric and completely data-driven approach to the decomposition of time series into a trend (cycle), seasonal and random components. Two former are modeled with penalized splines, while the latter is assumed to follow an ARMA structure. Empirical Bayesian approach allows to estimate both smoothing parameters and the orders of the ARMA process simultaneously resulting in an efficient, fast and data-driven decomposition procedure. The practical relevance of the approach is illustrated by real-data examples. The work is the extension of Kauermann, G., Krivobokova, T., Semmler, W. (2011) Filtering time series with penalized splines. <em>Studies in Nonlinear Dynamics &amp; Econometrics.</em></p>
"
"0.02831827358943","0.0277563690826684"," 70209","<p>I have the following data:</p>

<pre><code>t       mean
147     1.4
143     3
137.5       1.8
133     1.9
129.5       1.8
124.5       2.5
115.5       1.9
107     2.5
102.5       6.3
98.5        6.5
94.5        5
89      5.5
81      4.8
73      9.3
</code></pre>

<p>To me, the slope looks more exponential than linear when plotted as a scatterplot. I've been using the following code in R:</p>

<pre><code>data&lt;-read.csv(""regression.csv"")
attach(data)
plot(t,mean)
data.lm&lt;-lm(mean~t,data=data)
summary(data.lm)
data.exp&lt;-lm(log(mean) ~ log(t) ,data=data)
summary (data.exp)
AIC(data.lm, k=2)
AIC(data.exp, k=2)
</code></pre>

<p>data.exp, the exponential regression, has a much lower p-value and a much lower AIC score than data.lm, the linear model: 6.869e-05 vs. 0.000194, and 11.4641 vs. 52.22926.</p>

<p>But (how) can I demonstrate that the data fits an exponential line better than a straight line? Is the use of AIC legitimate here? Sorry to ask such as simple question but I've looked online and haven't found an answer.</p>

<p>Thank you!</p>
"
"0.0800961731463273","0.078506867197886"," 70249","<p>I would like to use GLM and Elastic Net to select those relevant features + build a linear regression model (i.e., both prediction and understanding, so it would be better to be left with relatively few parameters). The output is continuous. It's $20000$ genes per $50$ cases. I've been reading about the <code>glmnet</code> package, but I'm not 100% sure about the steps to follow:</p>

<ol>
<li><p>Perform CV to choose lambda:<br>
<code>cv &lt;- cv.glmnet(x,y,alpha=0.5)</code><br>
<strong>(Q1)</strong> given the input data, would you choose a different alpha value?<br>
<strong>(Q2)</strong> do I need to do something else before build the model?</p></li>
<li><p>Fit the model:<br>
<code>model=glmnet(x,y,type.gaussian=""covariance"",lambda=cv$lambda.min)</code><br>
<strong>(Q3)</strong> anything better than ""covariance""?<br>
<strong>(Q4)</strong> If lambda was chosen by CV, why does this step need <code>nlambda=</code>?<br>
<strong>(Q5)</strong> is it better to use <code>lambda.min</code> or <code>lambda.1se</code>?</p></li>
<li><p>Obtain the coefficients, to see which parameters have fallen out ("".""):<br>
<code>predict(model, type=""coefficients"")</code></p>

<p>In the help page there are many <code>predict</code> methods (e.g., <code>predict.fishnet</code>, <code>predict.glmnet</code>, <code>predict.lognet</code>, etc). But any ""plain"" predict as I saw on an example.<br>
<strong>(Q6)</strong> Should I use <code>predict</code> or <code>predict.glmnet</code> or other?</p></li>
</ol>

<p>Despite what I've read about regularization methods, I'm quite new in R and in these statistical packages, so it's difficult to be sure if I'm adapting my problem to the code. Any suggestions will be welcomed.</p>

<p><strong>UPDATE</strong><br>
<a href=""http://www.jstatsoft.org/v28/i05/paper"">Based on</a> ""As previously noted, an object of class train contains an element called <code>finalModel</code>, which is the fitted model with the tuning parameter values selected by resampling. This object can be used in the traditional way to generate predictions for new samples, using that model's
predict function.""  </p>

<p>Using <code>caret</code> to tune both alpha and lambda:    </p>

<pre><code>  trc = trainControl(method=cv, number=10)  
  fitM = train(x, y, trControl = trC, method=""glmnet"")  
</code></pre>

<p>Does <code>fitM</code> replace previous step 2? If so, how to specify the glmnet options (<code>type.gaussian=""naive"",lambda=cv$lambda.min/1se</code>) now?<br>
And the following <code>predict</code> step, can I replace <code>model</code> to <code>fitM</code>?</p>

<p>If I do  </p>

<pre><code>  trc = trainControl(method=cv, number=10)  
  fitM = train(x, y, trControl = trC, method=""glmnet"")  
  predict(fitM$finalModel, type=""coefficients"")
</code></pre>

<p>does it make sense at all or am I incorrectly mixing both package vocabulary?</p>
"
"0.0490486886395286","0.0480754414848157"," 70261","<p>I am working with a set of historical data that I did not collect myself. I cannot add to this data set in any way. For a 350-year period I have many thousands of data points, each associated with one of ~50 time points. Some of the time points have hundreds of data points associated with them, so I'm sure that those time points can be used in the linear regression. However, others have very few, even just one.</p>

<p>So my question is, how can I determine a cut-off for the time points that have enough data (and should be included in my regression) and the time points that do not have enough data (and should be excluded)? I'd like to do this in R.</p>

<p>Also, perhaps this is relevant - I'm interested in the minimum for each time point, not the mean. Thank you!</p>
"
"0.02831827358943","0.0277563690826684"," 70322","<p>Is it possible to illustrate partial correlation scatter plots for 2 subgroups on the same graph? </p>

<p>e.g. I want to make scatter plots of data controlled for age, differentiated by males or females.</p>

<p>I've tried doing partial regression plots generated by linear regression analysis, but I can't split it by groups. </p>

<p>Options to do it in excel or R would be fine too. Thanks!</p>
"
"NaN","NaN"," 70345","<p>I am going to teach undergraduate laboratory exercises for linear regression models in either Matlab or R.</p>

<p>Is there any book that is not emphasizing a lot in theory but focuses mostly in applications and in examples with any of the above languages?</p>

<p>I want to thank you all for your insights and for time replying to me. 
I cannot accept an answer, this wouldn't be fair for the others since everyone contributed.</p>
"
"0.0490486886395286","0.0480754414848157"," 70598","<p>I am estimating an instrumental variables linear regression that has a large number of indicator (factor) variables.  I don't particularly care about the coefficient estimates on those indicator variables.  In Stata's ivreg2 package there is a ""partial"" option that applies the Frisch-Waugh-Lovell theorem to orthogonalize the dependent and exogenous variables to the indicator variables.  After this transformation the indicator variables are not estimated because they do not affect the coefficients on the variables I am interested in.</p>

<p>My question is, is there something like this in R?  It doesn't have to be part of an IV regression package but I am looking to orthogonalize one set of variables to another set of variables.  This seems like something that would have already been implemented.  Thanks.</p>
"
"0.0755153962384799","0.0740169842204492"," 70699","<p>I have an independent variable called ""quality""; this variable has 3 modalities of response (bad quality; medium quality; high quality). I want to introduce this independent variable into my multiple linear regression. When I have a binary independent variable (dummy variable, I can code <code>0</code> / <code>1</code>) it is easy to introduce it into a multiple linear regression model.</p>

<p>But with 3 modalities of response, I have tried to code this variable like this :</p>

<pre><code>Bad quality      Medium quality      High quality

     0                1                  0
     1                0                  0
     0                0                  1
     0                1                  0
</code></pre>

<p>But there is a problem when I try to do my multiple linear regression: the modality <code>Medium quality</code> gives me <code>NA</code>:  </p>

<pre><code>Coefficients: (1 not defined because of singularities) 
</code></pre>

<p>How can I code this variable ""quality"" with 3 modalities? Do I have to create a variable as a factor (<code>factor</code> in <code>R</code>) but then can I introduce this factor in a multiple linear regression?</p>
"
"NaN","NaN"," 70716","<p>I know that it is possible when you realize a multiple linear regression to calculate the confidence interval on the R-squared and on the adjusted R-squared. Does somebody know how to do it with R?</p>
"
"0.0700841515030364","0.078506867197886"," 70866","<p>I have a modelling dilemma. I am creating a model that attempts to predict demand (leads not sales) based upon the correlation to advertising spend. We know that without advertising spend, demand is driven by seasonality. So our models include seasonal factors like month of the year and even day of the week. 
If I were building a regular linear regression model, I would fit a linear regression model to a training dataset, to get estimates of the coefficients of the seasonal factors and advertising spend to demand. In order to get an estimate of future baseline demand, I would forecast demand using all the coefficients from the model and then I would estimate a baseline by setting adspend equal to zero. 
For ARIMA models, there are additional factors such as AR and MA terms. Would I estimate my baseline the same way by just setting the coefficient on advertising spend equal to zero?
Thanks for any thoughts.</p>
"
"0.02831827358943","0"," 71001","<p>I would like to test a linear hypothesis in a median regression model similar to example below.</p>

<pre><code>require(AER);require(car);require(quantreg)
data(""CPS1985"")

#Regular linear model
model.lm &lt;- lm(wage ~ ethnicity + age*gender,CPS1985)
summary(model.lm)
linearHypothesis(model.lm,""age + age:genderfemale = 0"")

#Quantile regression
model.quant &lt;- rq(wage ~ ethnicity + age*gender,tau=0.5,data=CPS1985)
</code></pre>

<p>I'm not sure, however, how to execute a linear Hypothesis for <code>model.quant</code> to test that the sum of the coefficients on age and age:gender are equal to zero.</p>
"
"0.0633215847514023","0.0620651280774201"," 71070","<p>I'm modeling the amount of organic content in bird bones (a percentage) in two different conditions and also over two time periods. The design is repeated measures - observations in both conditions and time periods come from the same bone (divided into pieces). I want to test the hypotheses: 1) there is no difference in E(Y) across conditions, 2) there is no difference in E(Y) across time, 3) there is no difference in difference of E(Y) (i.e., time*condition interaction). I've tried the following (here with dummy data):</p>

<pre><code>set.seed(6753)
dat &lt;- data.frame(
    id = rep(1:15, each = 4),
    pc.organic = rnorm(60, 0.11, 0.055),
    condition = factor(rep(c(""raw"", ""advanced""), times = 30)),
    year = factor(rep(c(1, 1, 2, 2), times = 15))
    )

library(lme4)
fm1 &lt;- lmer(pc.organic ~ condition * year + (1 | id), data = dat)
summary(fm1)
</code></pre>

<p>This, I think, is an appropriate model to account for the non-independence of observations in the repeated measures design. I'm unsure, however, whether this is ok given the nature of the response variable. The response varies between about .01 (1%) and .2 (20%). It is bounded at zero (obviously), but also at 40% (this is the maximum amount of organic content in any bone - 60% is inorganic). Another option would be to use 40% as the denominator when I define the percentage, thus, the previous values would be .025 and .5 respectively. However, this would still leave the response bounded between 0 and 1.</p>

<p>I've read about beta regression and also about using a logit transformation to linearize the data. If possible, I'd  like to avoid going down these paths, as other researchers in my field are not familiar with these methods. Any suggestions are most welcome. </p>
"
"0.136246357332024","0.138884608733912"," 71414","<p>I'm trying to use a zero-inflated gamma model (or a gamma 'hurdle' model). The model is a mixture of logistic regression and generalized linear modeling. I can do this analysis in two steps: 1) do a logistic regression against presence/absence data and then 2) Use a generalized linear model with a gamma distribution on the positive values. I'm trying to set the model up where, if y = 0, then E(y) = p, but if y > 0, then E(y) is gamma distributed.</p>

<p>I'm trying to set this up in BUGS/JAGS, because I've seen these models <a href=""http://stats.stackexchange.com/questions/15967/how-can-i-set-up-a-zero-inflated-poisson-in-jags"">worked before for poisson-distributions</a>. I tried to adapt the code from that link into a gamma distribution, but I can't quite seem to get it to work. I know the code won't work because my data have zeroes, and the likelihood function can't be evaluated with zeroes. However, even when restricted to positive values, I get errors for invalid parent values. Even so, I'm not even sure the model is specified correctly.</p>

<p>Here is the model:</p>

<pre><code># For the ones trick
C &lt;- 10000

# for every observation
for(i in 1:N){
    # log-likelihood of the observation from the gamma likelihood
    LogPos[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - N*shape[i]*log(scale[i]) - N*loggam(scale[i])
    #likelihood
    Lpos[i] &lt;- exp(LogPos[i])

    # redefine the shape and rate parameters as a function of the mean and sd
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # mu is a function of MTD: use the inverse link
    #mu[i] &lt;- 1/eta[i]
    mu[i] &lt;- beta0 + beta1*MTD[i]


    # zero-inflated part, where w[i] is the probability of being zero
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # ones trick
    p[i] &lt;- Lpos[i] / C
    ones[i] ~ dbern(p[i])

    # Full likelihood
    Lik[i] &lt;- Lpos[i]*(1 - w[i]) + equals(y[i], 0)*w[i]
  } 

# PRIORS
beta0 ~ dnorm(0, 0.001)
beta1 ~ dnorm(0, 0.001)

gamma0 ~ dnorm(0, 0.001)
gamma1 ~ dnorm(0, 0.001)

sd ~ dunif(0, 100)
</code></pre>

<p>Has anyone set a model up like this or have any advice on how to set it up correctly?</p>

<p><strong>UPDATE</strong></p>

<p>I've tried a new set of code that's similar, but slight different. I still have not gotten it to work</p>

<pre><code>model{

  # For the ones trick
  C &lt;- 10000

  # for every observation
  for(i in 1:N){

    # make a dummy variable that is 0 if y is &lt; 0.0001 and 1 if y &gt; 0.0001. This is essentially a presence
    # absence dummy variable
    z[i] &lt;- step(y[i] - 0.0001)

    # define the logistic regression model, where w is the probability of occurance.
    # use the logistic transformation exp(z)/(1 + exp(z)), where z is a linear function
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # define the gamma regression model for the mean. use the log link the ensure positive, non-zero mu
    mu[i] &lt;- exp(eta[i])
    eta[i] &lt;- beta0 + beta1*MTD[i]

    # redefine the mu and sd of the continuous part into the shape and scale parameters
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # for readability, define the log-likelihood of the gamma here
    logGamma[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i])

    # define the total likelihood, where the likelihood is (1 - w) if y &lt; 0.0001 (z = 0) or
    # the likelihood is w * gammalik if y &gt;= 0.0001 (z = 1). So if z = 1, then the first bit must be
    # 0 and the second bit 1. Use 1 - z, which is 0 if y &gt; 0.0001 and 1 if y &lt; 0.0001
    logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + logGamma[i] )

    # Use the ones trick
    p[i] &lt;- logLik[i] / C
    ones[i] ~ dbern(p[i])
  } 

  # PRIORS
  beta0 ~ dnorm(0, 0.001)
  beta1 ~ dnorm(0, 0.001)

  gamma0 ~ dnorm(0, 0.001)
  gamma1 ~ dnorm(0, 0.001)

  sd ~ dgamma(1, 2)

}
</code></pre>

<p><strong>UPDATE 2</strong></p>

<p>I've gotten it to run by deleting the definition of logGamma[i] and putting it directly into the likelihood function, which now reads:</p>

<pre><code>logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i]) )
</code></pre>

<p>The problem was that JAGS was trying to evaluate the log likelihood at all observations first, resulting in NA's for the 0's. In the new way, it only evaluates it if z = 1 (I think). I'm still having trouble getting the parameter estimates to line up. For example, the gamma's are almost identical to a logistic regression of the same form (hooray!). But the betas are pretty far off from a gamma GLM of the positive values. I don't know if this is normal or not, but I suspect there are still problems with my model specification.</p>
"
"0.02831827358943","0.0277563690826684"," 71599","<p>Suppose that one has data on a proportion $P$ measured on a continuous scale.  Further suppose that this data has a grouped structure -- some proportions are clustered according to a certain variable $g$ (location, say).  Further say that you've got a covariate $X$ that your think is correlated with $P$.  You want to know the effect of $X$ on $P$ in order to get conditional distributions of $P$ given $X$.  </p>

<p>One could simply run a linear model with random effects, but predictions may go beyond the 0,1 bounds, and intervals will be wrong.  </p>

<p>One could fit a binomial(logit) GLM, but this may be a poor model for the data, as it would lead to overly upward (downward) skewed intervals near the boundaries, and a perhaps-artificial sigmoidal shape to the predictions.</p>

<p>Could a Beta regression work for this sort of problem?  If so, how would one incorporate the grouped structure of the data?  Any ideas for implementation in R?</p>
"
"NaN","NaN"," 71719","<p>I am looking for papers in the management area that discuss more on the use of linear mixed models in R. Yet all I am getting the results where most, if not all, use GLS regression technique. Would you enlighten me on these two things? I know that that they are similar, such as the case of panel data and unobserved heterogeneity due to random effects.</p>
"
"0.0749231094763201","0.0734364498908627"," 71727","<p>In addition to <a href=""http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_varclus_sect004.htm"">PROC VARCLUS</a>, <a href=""http://cran.r-project.org/web/packages/randomForest/index.html"">randomForest</a>, <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"">glmnet</a>, and assessing multicollinearity among potential predictor variables (without regards to the outcome of interest), I am seeking other methods of variable selection in lieu of using stepwise methods for building more parsimonious binary logistic regression models (containing 8 to 12 variables to predict outcomes such as loan payment/default or current/late payment history) from a wide array of potential predictor variables (500+ variables, 200k+ records). </p>

<p>Below I have included an R script using <a href=""http://cran.r-project.org/web/packages/FSelector/index.html"">FSelector</a> to select the 8 highest ""ranked"" variables:</p>

<pre><code>library(FSelector)
fit &lt;- information.gain(outcome ~ ., dataset)
fit2 &lt;- cutoff.k(fit,8)
reducedmodel &lt;- as.simple.formula(fit2,""outcome"")
print(reducedmodel)
</code></pre>

<p>I have two questions regarding this script and the <code>FSelector</code> algorithm in general:   </p>

<ol>
<li><p>Is the <code>information.gain</code> criteria in the above script synonymous with <a href=""http://en.wikipedia.org/wiki/Kullback-Leibler_divergence"">Kullback-Leibler divergence</a>?
If so, can someone explain this in more layman terms than Wikipedia as I am relatively new to this area of statistics and would like to start off with the right idea of this concept as I may likely use this approach a great deal in the future?</p></li>
<li><p>Is this a valid approach, if there is such a thing as a valid approach, to select a desired number of variables for a binary logistic regression model (e.g., selecting the 8 highest ""ranked"" variables for use in a parsimonious model)? If not, can you provide an alternative approach to do so?</p></li>
</ol>

<p>Any insight or references regarding this topic and/or these questions will be greatly appreciated!</p>
"
"0.02831827358943","0.0277563690826684"," 71816","<p>I've been trying to learn about effect size in relation to linear regression and am wondering how to implement it in R. Sure, I have p-values and they indicate how ""predictive"" the explanatory variable is. However, for each variable in a linear model, I was wondering how to compute a standardized score for how much it impacts the response variable.</p>

<p>Some sample data if you can present a R solution.</p>

<pre><code>x1 = rnorm(10)
x2 = rnorm(10)
y1 = rnorm(10)

mod = lm(y1 ~ x1 + x2)
summary(mod)
</code></pre>
"
"0.0805952195517515","0.0789960112897596"," 71948","<p>I'm fitting a natural spline fit to some data points. I'd like to estimate the prediction error for the predicted value. In linear regression (I agree that natural spline is also a linear regression with a specific type of design matrix), we know:</p>

<p>$\hat{\beta} = (X^T X)^{-1}X^TY \rightarrow \text{ assuming var(Y) = } \sigma^2 I \text{ then : }var(\hat{\beta}) = (X^TX)^{-1} \sigma^2 $ </p>

<p>Now consider $\hat{Y} = {X_i}^T \hat{\beta} + \epsilon_i$. We can then write:</p>

<p>$Var(\hat{Y}) = {X_i}^T ((X^TX)^{-1} \sigma^2) (X_i) + \sigma^2$</p>

<p>This is easy to calculate for linear regression. How should I do it with natural spline? I can get the design matrix for natural spline. I can get  $(X^TX)^{-1} \sigma^2$ but how can I get the rest of it:</p>

<p>Here is an example in R:</p>

<pre><code>set.seed(12345)
x &lt;- c(1:100)
y &lt;- sin(pi*x/50)
epsilon &lt;- rnorm(100, 0, 3)
knots &lt;- c(10, 20, 30, 40, 50, 60, 70, 80, 90)
myFit &lt;- lm(y ~ ns(x, knots = knots))
</code></pre>

<p>Now consider x = 32.5 . How can I get the variance for the $\hat{Y}$ corresponding to x = 32.5 ? I know we can use the predict function. however, what I do really want is to get calculate it similar to linear regression by getting the design matrix and multiplying them together.</p>

<p>I really appreciate your help.</p>
"
"0.0863948355424908","0.0923787802599364"," 72468","<p>I am interested in finding a procedure to simulate data that are consistent with a specified mediation model.  According to the general linear structural equation model framework for testing mediation models first outlined by <a href=""https://umdrive.memphis.edu/grelyea/public/PUBH%207152-Stat%20Methods%20II/Chapter%2010/Mediation/Baron_&amp;_Kenny_1986.pdf"" rel=""nofollow"">Barron and Kenny (1986)</a> and described elsewhere such as <a href=""http://www.psor.ucl.ac.be/personal/yzerbyt/Judd%20et%20al.%20HRMSP%202013.pdf"" rel=""nofollow"">Judd, Yzerbyt, &amp; Muller (2013)</a>, mediation models for outcome $Y$, mediator $\newcommand{\med}{\rm med} \med$, and predictor $X$ and are governed by the following three regression equations:
\begin{align}
Y    &amp;= b_{11} + b_{12}X + e_1                \tag{1}  \\
\med &amp;= b_{21} + b_{22}X + e_2                \tag{2}  \\
Y    &amp;= b_{31} + b_{32}X + b_{32} \med + e_3  \tag{3}
\end{align}
The indirect effect or mediation effect of $X$ on $Y$ through $\med$ can either be defined as $b_{22}b_{32}$ or, equivalently, as $b_{12}-b_{32}$.  Under the old framework of testing for mediation, mediation was established by testing $b_{12}$ in equation 1, $b_{22}$ in equation 2, and $b_{32}$ in equation 3.</p>

<p>So far, I have attempted to simulate values of $\med$ and $Y$ that are consistent with values of the various regression coefficients using <code>rnorm</code> in <code>R</code>, such as the code below:</p>

<pre><code>x   &lt;- rep(c(-.5, .5), 50)
med &lt;- 4 + .7 * x + rnorm(100, sd = 1) 

# Check the relationship between x and med
mod &lt;- lm(med ~ x)
summary(mod)

y &lt;- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)

# Check the relationships between x, med, and y
mod &lt;- lm(y ~ x + med)
summary(mod)

# Check the relationship between x and y -- not present
mod &lt;- lm(y ~ x)
summary(mod)
</code></pre>

<p>However, it seems that sequentially generating $\med$ and $Y$ using equations 2 and 3 is not enough, since I am left with no relationship between $X$ and $Y$ in regression equation 1 (which models a simple bivariate relationship between $X$ and $Y$) using this approach.  This is important because one definition of the indirect (i.e., mediation) effect is $b_{12}-b_{32}$, as I describe above.</p>

<p>Can anyone help me find a procedure in R to generate variables $X$, $\med$, and $Y$ that satisfy constraints that I set using equations 1, 2, and 3?</p>
"
"0.0755153962384799","0.0740169842204492"," 72516","<p>I am examining how English ivy affects the occurrence of a salamander species under cover objects (e.g., logs). Soil moisture is assumed to be the major factor that affect their occurrence. </p>

<p>My hypothesized pathway: The presence/absence of salamanders under cover objects is either a direct consequence of changes in ivy-induced abioitc environment (i.e., drier soil) or an indirect result of changes in prey community that resulted from altered abiotic factors. But, there are multiple factors, other than English ivy, that affect soil moisture.</p>

<p><img src=""http://i.stack.imgur.com/k65Ag.jpg"" alt=""enter image description here""></p>

<p>My questions are:</p>

<ol>
<li><p>I think that a path analysis is most suitable for testing my causal mechanisms. But, given a small sample size (n = 71), is a path analysis appropriate?</p></li>
<li><p>Another potential problem for a path analysis is that the effects of English ivy on soil moisture seem to depend on the other factors (e.g., the number of overstory trees), as shown below. Are there any way to account for such patterns in a path analysis?</p>

<p><img src=""http://i.stack.imgur.com/ArgZm.jpg"" alt=""The relationship between soil moisture and English ivy cover on cover objects (&quot;the number of overstory trees&quot; for the left graph) for different levels of the surrounding overstory trees (&quot;English ivy cover on cover objects&quot; for the left graph""></p></li>
<li><p>Are there any other analyses suitable for testing my hypothesized relationships? I am considering multiple (linear and logistic) regressions, but again my sample size is small <strong>AND</strong> regressions do not reflect my hypothesized causal relationships accurately.</p></li>
</ol>

<p>I am using R, so any recommended code would be greatly helpful (I am a relatively new R user, though). </p>
"
"0.0800961731463273","0.078506867197886"," 72569","<p>What does it mean when two random effects are highly or perfectly correlated?<br>
That is, in R when you call summary on a mixed model object, under ""Random effects"" ""corr"" is 1 or -1.</p>

<pre><code>summary(model.lmer) 
Random effects:
Groups   Name                    Variance   Std.Dev.  Corr                 
popu     (Intercept)             2.5714e-01 0.5070912                      
          amdclipped              4.2505e-04 0.0206167  1.000               
          nutrientHigh            7.5078e-02 0.2740042  1.000  1.000        
          amdclipped:nutrientHigh 6.5322e-06 0.0025558 -1.000 -1.000 -1.000
</code></pre>

<p>I know this is bad and indicates that the random effects part of the model is too complex, but I'm trying to understand</p>

<ul>
<li>1)what is doing on statistically  </li>
<li>2)what is going on practically with
the structure of the response variables.</li>
</ul>

<p><strong>Example</strong></p>

<p>Here is an example based on ""<a href=""http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CDYQFjAC&amp;url=http://glmm.wdfiles.com/local--files/examples/Banta_ex.pdf&amp;ei=hTNYUpuzBu7J4APN5YHYBg&amp;usg=AFQjCNG65VjvqOLeYLFxJZnzmlMevgEbuA&amp;bvm=bv.53899372,d.dmg"">GLMMs in action: gene-by-environment interaction in total fruit production of wild populations of Arabidopsis thaliana</a>""
by Bolker et al</p>

<p>Download data</p>

<pre><code>download.file(url = ""http://glmm.wdfiles.com/local--files/trondheim/Banta_TotalFruits.csv"", destfile = ""Banta_TotalFruits.csv"")
dat.tf &lt;- read.csv(""Banta_TotalFruits.csv"", header = TRUE)
</code></pre>

<p>Set up factors</p>

<pre><code>dat.tf &lt;- transform(dat.tf,X=factor(X),gen=factor(gen),rack=factor(rack),amd=factor(amd,levels=c(""unclipped"",""clipped"")),nutrient=factor(nutrient,label=c(""Low"",""High"")))
</code></pre>

<p>Modeling log(total.fruits+1) with ""population"" (popu) as random effect</p>

<pre><code>model.lmer &lt;- lmer(log(total.fruits+1) ~ nutrient*amd + (amd*nutrient|popu), data= dat.tf)
</code></pre>

<p>Accessing the Correlation matrix of the random effects show that everything is perfectly correlated</p>

<pre><code>attr(VarCorr(model.lmer)$popu,""correlation"")

                         (Intercept) amdclipped nutrientHigh amdclipped:nutrientHigh
(Intercept)                       1          1            1                      -1
amdclipped                        1          1            1                      -1
nutrientHigh                      1          1            1                      -1
amdclipped:nutrientHigh          -1         -1           -1                       1
</code></pre>

<p>I understand that these are the correlation coefficients of two vectors of random effects coefficients, such as</p>

<pre><code>cor(ranef(model.lmer)$popu$amdclipped, ranef(model.lmer)$popu$nutrientHigh)
</code></pre>

<p>Does a high correlation mean that the two random effects contain redundant information?  Is this analogous to multicollinearity in multiple regression when a model with highly correlated predictors should be simplified?</p>
"
"0.0566365471788599","0.0555127381653369"," 73004","<p>I am fitting a gam model in R (using the <code>gam</code> function in <code>mgcv</code>) to account for some non-linear effects in my data. A stripped down example of what I am doing in R is:</p>

<pre><code>mod=gam(y~s(x)+s(z),data=df)
</code></pre>

<p>However, I want to add a slightly more complicated variance model to my regression of the form </p>

<p>$$\epsilon \sim N(0,\sigma^2),\ \sigma = f(\hat{\mu})$$</p>

<p>where $\hat{\mu}$ is the fitted value of the model. (Actually, it would be nice if $\epsilon \sim t_\nu$ for some $\nu$ but sticking to this for now. I have managed to do this in the <code>gls</code> function from <code>nlme</code> using the <code>varFunc(form=fitted(.))</code> type approach, but can't figure out if there is an option to do the same kind of thing using <code>gam</code>.</p>

<p>I recognise this is not really the intention of a GLM/GAM model, but I don't want to reinvent the wheel if I am just missing something obvious</p>

<p>Edit: In response to the question in the comment below, I am hoping to fit a linear or quadratic function for $f$. I do not know the exact form of $f$ but plan to iteratively estimate it from the residuals if this can't be done automatically.</p>

<p>Edit2: Typo in R code - first spline is not meant to be a function of y!</p>
"
"0.0849548207682898","0.0832691072480053"," 73191","<p>For ordinary linear regression with Gaussian noise, it is easy to interpret the significance of a variable.  This is consistent with a partial F test.  The square of the t-test for the second variable equals to the partial F-test statistic, and their p-values are the same.</p>

<p>I wrote simple R codes to confirm this.</p>

<p>Is there something like this for logistic regression?  I thought/hoped that the likelihood ratio test would correspond to this, but no.  What should I do if the variable and the likelihood ratio test (of adding that particular variable) do not have the same (in)significant effect?</p>

<p>I appreciate your time and help,</p>

<pre><code>rm(list=ls(all=TRUE)) 
n = 100   ;       x1 = runif(n,-4,4)   ;       x2 = runif(n,6,10)
y = 3*x1 + 8*x2 + rnorm(n,2,4)
l1 = lm(y~x1)  ;  l2 = lm(y~x1+x2)  ;  a = anova(l1,l2)

summary(l1)$coeff
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) 66.093853  1.0123131 65.289929 1.385202e-82
x1           3.199212  0.4292828  7.452458 3.664499e-11

summary(l2)$coeff
            Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) 2.767750  2.7871368  0.9930441 3.231592e-01
x1          2.870897  0.1707022 16.8181610 1.648852e-30
x2          7.871545  0.3428392 22.9598753 5.370614e-41

(summary(l2)$coeff[3,3])^2
527.1559
&gt;     a 
    Analysis of Variance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     98 9899.1                                  
2     97 1538.4  1    8360.6 527.16 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt;     a$F ; a$Pr
   [1]       NA 527.1559
[1]           NA 5.370614e-41
&gt; 
&gt; 
&gt; 
&gt; rm(list=ls(all=TRUE)) 
&gt; n = 100
&gt; x1 = runif(n,-4,4)
&gt; x2 = runif(n,6,10)
&gt; 
&gt; y = rbinom(n,1,1/(1+exp(-3*x1 - 2*x2 + 20)))
&gt; 
&gt; l1 = glm(y~x1,family=binomial)
&gt; l2 = glm(y~x1+x2,family=binomial)
&gt; 
&gt; a = anova(l1,l2)
&gt; 
&gt; summary(l1)$coeff
                 Estimate Std. Error   z value     Pr(&gt;|z|)
    (Intercept) -2.988069   0.812041 -3.679702 2.335068e-04
    x1           2.115333   0.498431  4.243984 2.195858e-05
    &gt; summary(l2)$coeff
              Estimate Std. Error   z value     Pr(&gt;|z|)
(Intercept) -17.215960  5.5710699 -3.090243 0.0019999276
x1            3.048657  0.8618367  3.537395 0.0004040949
x2            1.675323  0.5976386  2.803238 0.0050592272
&gt; 
&gt; (summary(l2)$coeff[3,3])^2
    [1] 7.858145
    &gt; 
    &gt; l1$deviance -  l2$deviance
    [1] 13.65371
    &gt; pchisq(l1$deviance -  l2$deviance,df=1)
[1] 0.9997802
&gt; 
&gt; a
Analysis of Deviance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Resid. Df Resid. Dev Df Deviance
1        98     45.534            
2        97     31.880  1   13.654
&gt; a$F
    NULL
    &gt; a$Pr
    NULL
</code></pre>
"
"0","0.0277563690826684"," 74304","<p>What goodness of fit tests are usually used for quantile regression? Ideally I need something similar to F-test in linear regression, but something like AIC in logistic regression will suite as well. I use quantreg R package, but found only some Khmaladze test in there. To be fair I hardly understand what is does.</p>
"
"0.0633215847514023","0.0620651280774201"," 74545","<p>I have a dataset with columns that represent lagged values of predictors. To illustrate with a simple example, suppose we had car sales data for 3 years and the only predictors available were income and population for a number of car dealers, the dataset could be represented as follows,</p>

<pre><code>ID  IncLag1  PopLag1  SalesLag1  IncLag2  PopLag2 SalesLag2  IncCurrent  PopCurr  SalesCurr
a       100      1000     200        150      2000    300        500       2500         450
b       10        300      50         60       900     80         90       1000         100
</code></pre>

<p>...</p>

<pre><code>k       30        60      10        200      2000     60         80          800         ??
</code></pre>

<p>My dependent variable is SalesCurr - i.e., given a history of past sales and corresponding Income and Population values (which we can use as the train-test data), predict what the Sales will be in the current year (SalesCurr). </p>

<p>My question is as follows -- Using R or GRETL, how is it possible to create an ARIMA/TimeSeries model with the above data to predict the SalesCurrent variable. Using simple Linear Regression, one could simply have a formula such as say, <code>lm (SalesCurrent ~ ., data=mytable)</code>, but it would not be a time-series model since it does not take into account the relationship between the different variables.</p>

<p>Alternatively, I am quite familiar with Machine Learning models and wanted to get your thoughts on how such a dataset could be modeled using say, randomForest, GBM, etc. </p>

<p>Thanks in advance.</p>
"
"0.0506572678011219","0.0620651280774201"," 74678","<p>I am comparing multiple published equation forms, refit with independent data.  I'm trying to be true to the original authors' methods as much as possible. Therefore, I have 3 linear equations (fit in R using lm()), two of which use transformed Y-variables, and one equation fit using nonlinear regression (fit in R using the gnls() function).</p>

<p>In all instances cases I'm weighting the residual variance structure using the inverse of one of the predictors to account for observed heteroskedasticity.</p>

<p>I have been evaluating the models using R2, and RMSE- using back-transformed data for the two models with transformations.</p>

<p>I've calculated RMSE ""by hand"" using the following equation:</p>

<pre><code> RMSE&lt;-sqrt(sum(residuals(Equation)^2)/length(residuals(Equation))-2))
</code></pre>

<p>Should I use similar code to calculate RMSE for the linear and nonlinear regression models?  Is the metric still a valid statistic for comparison, or am I missing some important assumption?  </p>

<p>Edited: I initially stated that I was also comparing models using AIC; I later recalled that AIC would not be appropriate if the Y-variables were transformed because the models would be estimating different things.</p>
"
"NaN","NaN"," 74847","<p>I'm trying to do some exploratory analysis on some weather. I'd like to do a multiple linear regression on my data and then plot the predicted value against the actual value. Here's where I've got so far: </p>

<pre><code>data&lt;-read.csv(""Amsterdam.csv"", header=TRUE)
data2&lt;-data[SolarAltitude&gt;0,]
data2.lm&lt;-lm(DirectRadiation ~ DryBulbTemperature + RelHum
   +AtmosphericPressure, data=data2)
data.data.frame(data2,fitted.value=fitted(data2.lm),residual=resid(data2.lm)) 
</code></pre>

<p>If you could help, I would be very grateful,</p>
"
"0.0424774103841449","0.0555127381653369"," 76212","<p>I'm working in R. I'd like to run a regression analysis for predicting price against terms in a text field. </p>

<p>I have a dataset of jewellery auction listings, with price paid, date, and an unstructured description of the item type: </p>

<pre><code>text,date,price_usd
""Ruby necklace, Spanish"",1925,45000
""Diamond ring, 0.7 carat, bezier cut"",1972,24000
""Diamond necklace"",1980,87000
...
</code></pre>

<p>I know how to run a linear regression for price against date: </p>

<pre><code>data &lt;- read.csv('jewels.csv')
lm1 &lt;- lm(data$price~data$date)
summary(lm1)
</code></pre>

<p>Now what I'd like to do is build a similar model, using the words in the description field that are most associated with higher prices. </p>

<p>Intuitively I'd guess these include ""diamond"" and ""necklace"", while (say) ""amethyst"" and ""ring"" were associated with lower prices, but is there a way I can build a model to look at this?</p>

<p>My sense is that I need to do the following things: </p>

<ul>
<li>turn the text field into a bag of words (vector)</li>
<li>remove stop words</li>
<li>normalize each word for overall count(?)</li>
<li>run some kind of regression against price. </li>
</ul>

<p>I'd really welcome some guidance on how to approach each step. </p>
"
"0.0983889005882491","0.0964366217361766"," 76250","<p>I am new to statistics and I am trying to understand the difference between ANOVA and linear regression. I am using R to explore this. I read various articles about why ANOVA and regression are different but still the same and how the can be visualised etc. I think I am pretty there but one bit is still missing.</p>

<p>I understand that ANOVA compares the variance within groups with the variance between groups to determine whether there is or is not a difference between any of the groups tested. (<a href=""https://controls.engin.umich.edu/wiki/index.php/Factor_analysis_and_ANOVA"" rel=""nofollow"">https://controls.engin.umich.edu/wiki/index.php/Factor_analysis_and_ANOVA</a>)</p>

<p>For linear regression, I found a post in this forum which says that the same can be tested when we test whether b (slope) = 0.
(<a href=""http://stats.stackexchange.com/questions/555/why-is-anova-taught-used-as-if-it-is-a-different-research-methodology-compared"">Why is ANOVA taught / used as if it is a different research methodology compared to linear regression?</a>)</p>

<p>For more than two groups I found a website stating:</p>

<p>The null hypothesis is: $\text{H}_0: Âµ_1 = Âµ_2 = Âµ_3$</p>

<p>The linear regression model is: $y = b_0 + b_1X_1 + b_2X_2 + e$</p>

<p>The output of the linear regression is, however, then the intercept for one group and the difference to this intercept for the other two groups. 
(<a href=""http://www.real-statistics.com/multiple-regression/anova-using-regression/"" rel=""nofollow"">http://www.real-statistics.com/multiple-regression/anova-using-regression/</a>)</p>

<p>for me, this looks like that actually the intercepts are compared and not the slopes?</p>

<p>Another example where they compare intercepts rather than the slopes can be found here:
(<a href=""http://www.theanalysisfactor.com/why-anova-and-linear-regression-are-the-same-analysis/"" rel=""nofollow"">http://www.theanalysisfactor.com/why-anova-and-linear-regression-are-the-same-analysis/</a>)</p>

<p>I am now struggling to understand what is actually compared in the linear regression? the slopes, the intercepts or both? </p>
"
"0.0991139575630048","0.0971472917893396"," 76490","<h2>Background</h2>

<p>A laboratory wants to evaluate whether a certain form of <a href=""http://en.wikipedia.org/wiki/Polyacrylamide_gel_electrophoresis"" rel=""nofollow"">gel electrophoresis</a> is suited as a classification method for the quality of a certain substance. Several gels were loaded, each with a clean sample of the substance and with a sample that contains impurities. In addition, a molecular marker was also loaded which serves as a reference. The following picture illustrates the setup (the picture doesn't show the actual experiment, I have taken it from Wikipedia for illustration):</p>

<p><img src=""http://i.stack.imgur.com/vC53q.png"" alt=""Example of a gel electrophoresis""></p>

<p>Two parameters were measured for each gel and each lane:</p>

<ol>
<li>The <strong>molecular weight</strong> (that is how ""high up"" a compound wandered during the electrophoresis)</li>
<li>The <strong>relative quantity.</strong> The total quantity of each lane is normalized to 1 and the density of each band is measured which results in the relative quantity of each band.</li>
</ol>

<p>A scatterplot of the relative quantity vs. molecular weight is then produced which could look something like this (it's artificial data):</p>

<p><img src=""http://i.stack.imgur.com/ndzh9.png"" alt=""Example scatterplot""></p>

<p>This graphic can be read as follows: Both the ""good"" (blue points) and ""impure"" (red points) substance exhibit two bands, one at around a molecular weight of 120 and one at around 165. The bands of the ""impure"" substance at a molecular weight around 120 are considerably less dense than the ""good"" substance and can be well distinguished.</p>

<hr>

<h2>Goal</h2>

<p>The goal is to determine two boxed (see graphic below) which determine a ""good"" substance. These boxes will then be used for classification of the substance in the future into ""good"" and ""impure"". If a substance exhibits lanes that fall within the boxes it is classified as ""good"" and else as ""impure"".</p>

<p>These decision-rules should be <em>simple</em> to apply for someone in the laboratory. That's why it should be boxes instead of curved decision boundaries.</p>

<p>False-negatives (i.e. classify a sample as ""impure"" when it's really ""good"") are considered worse than false-positives. That is, an emphasis should be placed on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity"" rel=""nofollow"">sensitivity</a>, rather than on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity"" rel=""nofollow"">specificity</a>.</p>

<p><img src=""http://i.stack.imgur.com/vhzEW.png"" alt=""Example decision boxed""></p>

<hr>

<h2>Question</h2>

<p>I'm am no expert in machine learning. I know, however, that there are quite a few machine learning algorithms/techniques that could be helpful: $k$-nearest neighbors (e.g. <code>knn</code> in <code>R</code>), classification trees (e.g. <code>rpart</code> or <code>ctree</code>), support vector machines (<code>ksvm</code>), logistic regression, boosting and bagging methods and many more.</p>

<p>One problem of many of those algorithms is that they don't provide a simple ruleset or linear boundaries. In addition, the <strong>sample size</strong> is around <strong>70.</strong></p>

<p>My questions are:</p>

<ul>
<li>Has anyone an idea of how to proceed here?</li>
<li>Does it make sense to split the dataset into training- and test-set?</li>
<li>What proportion of the data should the training set be (I thought around a 60/40-split).</li>
<li>What, in general, is the workflow for such an analysis? Something like: Splitting dataset -> fit algorithm on the training set -> predict outcome for the test set?</li>
<li>How to avoid overfitting (i.e. boxes that are too small)?</li>
<li>What is a good statistic to assess the predictive performance in this case? AUC? Accurary? Positive predictive value? <a href=""http://en.wikipedia.org/wiki/Matthews_correlation_coefficient"" rel=""nofollow"">Matthews correlation coefficient</a>?</li>
</ul>

<p>Assume that I'm familiar with <code>R</code> and the <code>caret</code> package. Thank you very much for you time and help.</p>

<hr>

<h2>Example data</h2>

<p>Here is an example dataset.</p>

<pre><code>structure(list(mol.wt = c(125.145401455869, 118.210252208676, 
165.048583787746, 126.003687476776, 170.149347112565, 127.761533014759, 
155.523172614798, 120.094514977175, 161.234986765321, 168.471542655269, 
156.522990530521, 154.377948321209, 165.365756398877, 167.965538771316, 
116.132241687833, 115.143539160903, 156.696830822196, 162.578494491556, 
136.830624758899, 123.886594633942, 124.247484227948, 126.257226352824, 
160.684010454816, 166.618872115047, 126.599387146887, 165.690375912529, 
159.786861142652, 114.520735974329, 125.753594471656, 157.551537154148, 
157.320636890647, 171.5759136115, 158.580005438661, 125.647463565197, 
130.404710783509, 127.128218318572, 162.144126888907, 161.804616951055, 
167.917268243627, 168.582197247178), rel.qtd = c(57.68339235957, 
54.0514508510085, 25.0703901938793, 37.6933881305906, 36.6853653723001, 
53.6650555524679, 52.268438087776, 52.8621831466857, 43.1242291166037, 
46.6771236380788, 38.0328239221277, 40.0454611708371, 44.6406366176158, 
40.8238699987682, 51.9464749018547, 54.0302533272953, 37.9792331383524, 
48.3853988095525, 38.2093977349102, 42.2636098418388, 42.9876895407144, 
40.8018728193786, 40.1097096927465, 38.7432550253867, 39.2633283608111, 
43.4673723102812, 53.3740718733815, 49.1067921475768, 52.3002598744634, 
44.9847844953241, 44.3014423068017, 44.0191971364465, 47.0805245356855, 
55.0124134796556, 57.9938440244052, 62.8314454977068, 45.8093815891894, 
43.2300677500964, 39.4801550161538, 51.6253515591173), quality = structure(c(2L, 
2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 1L), .Label = c(""bad"", ""good""), class = ""factor"")), .Names = c(""mol.wt"", 
""rel.qtd"", ""quality""), row.names = c(10L, 14L, 47L, 16L, 57L, 
54L, 45L, 12L, 43L, 67L, 25L, 21L, 1L, 55L, 20L, 22L, 37L, 15L, 
8L, 38L, 46L, 64L, 51L, 65L, 52L, 61L, 63L, 32L, 50L, 27L, 19L, 
69L, 23L, 42L, 6L, 48L, 11L, 13L, 5L, 71L), class = ""data.frame"")
</code></pre>
"
"0.02831827358943","0.0277563690826684"," 76625","<p>I'm looking for a way to run a repeated-measures multiple regression in R, which would take care of sphericity - either by applying some corrections (such as Huynh-Feldt), or by avoiding the problem in some other way.</p>

<p>I have 2 factorial repeated measure variables: 3- and 2-level (<code>roi_ant</code>, <code>roi_lat</code>), and a quantitative between-subject variable (<code>pred</code>), and one dependent quantitative variable (<code>mv</code>). I want to test for a full model including all possible interactions between the two within-subject and the between-subject variables (i.e., <code>mv ~ pred * roi_ant * roi_lat</code>). I am most interested in the slope of <code>pred</code> - whether it is different from 0, and whether it changes depending on <code>roi_ant</code> and <code>roi_lat</code>.</p>

<p>I have tried to do repeated-measures MANCOVA (that would remove the sphericity assumption) using car::Anova package, but if I got <a href=""http://r.789695.n4.nabble.com/car-Anova-Can-it-be-used-for-ANCOVA-with-repeated-measures-factors-td4637324.html"" rel=""nofollow"">this</a> discussion right, <code>car::Anova()</code> is not able to handle such a design.</p>

<p><code>ezANOVA()</code> performs ANCOVA with applying corrections for sphericity, but reports only the repeated-measure variables (after removing the influence of the covariate), and does not report anything on the covariate <code>pred</code>.</p>

<p>Mixed linear models (<code>lme4</code>) should handle this design well, but since <code>mcmsamp</code> is still not implemented, it is difficult to get p-values out of them.</p>

<p>Do you have any other suggestions? </p>
"
"0.0400480865731637","0.039253433598943"," 76627","<p>I am using the data <code>veteran</code> from  <code>R</code> package <code>survival</code>. </p>

<p>How can I  diagnose the normality assumption about <code>time</code>? Should I need to perform a linear regression to measure the dependency of <code>time</code> on <code>age</code> and <code>karno</code>? </p>

<p>Are the following commands all to answer the question? or Do I need to add more information?</p>

<pre><code>   #diagnoses normality assumption about 'time'.
   shapiro.test(veteran$time)
   qqnorm(veteran$time)
   qqline(veteran$time)

   #perform a linear regression
   with(veteran,lm(time~age+karno))
   with(veteran,cor.test(time,age))
   with(veteran,cor.test(time,karno))
</code></pre>

<h3>EDIT :</h3>

<p>I want to diagnose the normality assumption. How can I do this for the above data?
And for diagnosing the normality assumption, I chose <code>R</code> software.
So my question involves both diagnosie the normality assumption and R syntax.</p>
"
"0.0424774103841449","0.0555127381653369"," 76925","<p>Is it possible to penalize coefficients toward a number other than zero in a ridge regression in R?</p>

<p>For example, let's say I have dependent variable Y and independent variables X1,X2,X3, and X4. Because of the multicollinear nature of the ivs, ridge regression is appropriate. But say I'm fairly certain that the coefficient of X1 is near 5, X2 is near 1, X3 is near -1, and X4 is near -5. </p>

<p>Is there a ridge package and method in R where I can implement penalties on the coefficients of the ivs that penalize them toward those numbers instead of 0? I'd love to see an example in R with my example data, if possible. Thank you.</p>
"
"NaN","NaN"," 77087","<p>Consider the following linear regressions on two sets of data x and y (of same length)</p>

<ul>
<li>y=ax</li>
<li>x=by</li>
</ul>

<p>As you know, the usual optimisation by OLS is usually not giving (at all!) b=1/a</p>

<p>This is because the optimisation algorithm is not symmetric.</p>

<p>My question: is there coded somewhere in R the algorithm that fits symmetrically? (using as error the shortest distance to the line - for those who follow me here..)</p>

<p>Edit: <a href=""http://en.wikipedia.org/wiki/Total_least_squares"" rel=""nofollow"">http://en.wikipedia.org/wiki/Total_least_squares</a></p>
"
"0.0755153962384799","0.0740169842204492"," 77154","<p>To build two linear regression model, (dependant var : B, independant vars : A1, A2, A3) I have to set the cut point of A1. (high A1 and low A2) I want to pick up the model*s*(a model for high A1, and the other for low A2) with with the lowest residual sums of square*s*</p>

<p>In summary, How I can get the optimal cut-off point to get best linear regression models for each groups?</p>

<p>The article I am trying to replicate is below, that description is all. A is dep Var, and B is ind var. ""Considered two linear regression models, one for subjects below a certain level of A and the other for subjects above that level. To determine the specific cutoffs, we fitted the two linear regression models described above and calculated the sums of squares of residuals (=observed B - estimated B) from the two models for each level of A. The models with the lowest residual sums of squares were our best models, and the corresponding level of A were defined as the optimal cutoff values.""</p>

<p>Url for the article is : <a href=""http://link.springer.com/article/10.1007/s00223-012-9669-3"" rel=""nofollow"">http://link.springer.com/article/10.1007/s00223-012-9669-3</a></p>
"
"0.0980973772790571","0.0961508829696314"," 77546","<p>I am trying to fit a multivariate linear regression model with approximately 60 predictor variables and 30 observations, so I am using the <strong>glmnet</strong> package for regularized regression because p>n.</p>

<p>I have been going through documentation and other questions but I still can't interpret the results, here's a sample code (with 20 predictors and 10 observations to simplify):</p>

<p>I create a matrix x with num rows = num observations and num cols = num predictors and a vector y which represents the response variable</p>

<pre><code>&gt; x=matrix(rnorm(10*20),10,20)
&gt; y=rnorm(10)
</code></pre>

<p>I fit a glmnet model leaving alpha as default (= 1 for lasso penalty)</p>

<pre><code>&gt; fit1=glmnet(x,y)
&gt; print(fit1)
</code></pre>

<p>I understand I get different predictions with decreasing values of lambda (i.e. penalty)</p>

<pre><code>Call:  glmnet(x = x, y = y) 

        Df    %Dev   Lambda
  [1,]  0 0.00000 0.890700
  [2,]  1 0.06159 0.850200
  [3,]  1 0.11770 0.811500
  [4,]  1 0.16880 0.774600
   .
   .
   .
  [96,] 10 0.99740 0.010730
  [97,] 10 0.99760 0.010240
  [98,] 10 0.99780 0.009775
  [99,] 10 0.99800 0.009331
 [100,] 10 0.99820 0.008907
</code></pre>

<p>Now I predict my Beta values choosing, for example, the smallest lambda value given from <code>glmnet</code></p>

<pre><code>&gt; predict(fit1,type=""coef"", s = 0.008907)

21 x 1 sparse Matrix of class ""dgCMatrix""
                  1
(Intercept) -0.08872364
V1           0.23734885
V2          -0.35472137
V3          -0.08088463
V4           .         
V5           .         
V6           .         
V7           0.31127123
V8           .         
V9           .         
V10          .         
V11          0.10636867
V12          .         
V13         -0.20328200
V14         -0.77717745
V15          .         
V16         -0.25924281
V17          .         
V18          .         
V19         -0.57989929
V20         -0.22522859
</code></pre>

<p>If instead I choose lambda with </p>

<pre><code>cv &lt;- cv.glmnet(x,y)
model=glmnet(x,y,lambda=cv$lambda.min)
</code></pre>

<p>All of the variables would be (.).</p>

<p>Doubts and questions:</p>

<ol>
<li>I am not sure about how to choose lambda.</li>
<li>Should I use the non (.) variables to fit another model? In my case I would like to keep as much variables as possible.</li>
<li>How do I know the p-value, i.e. which variables significantly predict the response?</li>
</ol>

<p>I apologize for my poor statistical knowledge! And thank you for any help.</p>
"
"0.0749231094763201","0.0629455284778823"," 77580","<p>I have a number of objective functions like:</p>

<pre><code>y1 = a11* x11 + a12*x11*x11+ a13*x12+.......
y2 = a21* x21 + a22*x21*x21+ a23*x22+.......
:::::
:::::
:::::
</code></pre>

<p>These are multiple objective functions. However, the constraints of the objective functions have dependency on each other.
Something like, </p>

<pre><code>x11+ x21 + x22 &lt; const1
x12 + x21 &gt; const2

:::::
:::::
</code></pre>

<p>What is the way to optimize such a system of equations? I would ideally like to use R to do the same?</p>

<p>y1 = 0.32 x1 + 0.21 x1*x1 + 0.49 x2... y2... y3 . . . The equations that i have is a non-linear function. These are non-linear regression equations or non-linear Market Mix Models. The x's are TV spend, Digital Spend etc. I want to 'include' all these models, use some constraints on them and optimize the spends in all the models with respect to constraints like...x1 (TV spend) &lt; 100, TV + Digital spend &lt; 500. I want to be able to say that of an amount of 100, i should spend, 30 on model 1, 20 on model 2( equation 2) etc</p>
"
"0.0326991257596857","0.0320502943232105"," 77838","<p>I have two linear regressions. The linear coefficients of both of them are very close to $1$. The first plot seems reasonable linear regression, while the second one is tricky since if the bottom left part or the top right part of the data (they are quite random) are fitted, we can not get a close-to-1 coefficient.</p>

<p>The question is how to distinguish the two regressions after I get their very similar regression coefficient values? Instead of by looking at the plots, a statistic is preferred to describe the difference of the two regressions. Any ideas? </p>

<p><img src=""http://i.stack.imgur.com/nqFjm.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/TfdGK.png"" alt=""enter image description here""> </p>
"
"0.0942489115008991","0.0923787802599364"," 77851","<p>I am trying to get an optimal cut-off value dividing group with minimum sums of squares of residuals (=observed y - estimated y) the model is like below.</p>

<blockquote>
  <p>In group 1 : model y= a1x + b1z + C1v ...</p>
  
  <p>In group 2 : model y= a2x + b1z + C1v ...</p>
</blockquote>

<p>I have the data of y, x, z, v... The problem is the group 1 and 2 are not divided yet and the purpose of the analysis is finding optimal cut-off point of x using regression models.</p>

<p>I searched again and again, but couldn't find the way to make models varying 'a' and share b1 and c1... and fitting it to data.</p>

<p>I asked similar quesion in stackexchange, and somebody advised me the problems of this kind of approach, however, I need this approach, because it's some clinical research want to 'find' optimal (not perfect) cut-off point of x.</p>

<p>The article I read described below
The authors of the article mentioned that they used R, but I cannot find any reference or examples about this kind of analysis.</p>

<blockquote>
  <p>To determine the relationship between serum 25(OH)D and iPTH concen-
  trations while adjusting for confounders that could affect serum
  25(OH)D concentrations (i.e., age, gender, body weight, calcium
  intake, physical activity, and season of year), we considered two
  linear regression models, one for subjects below a certain
  concentration of serum 25(OH)D and the other for subjects above that
  concentration. To determine the specific cutoffs, we fitted the two
  linear regression models described above and calculated the sums of
  squares of residuals (=observed PTH - estimated PTH) from the two
  models for each concentration of serum 25(OH)D. The models with the
  lowest residual sums of squares were our best models, and the
  corresponding concentrations of serum 25(OH)D were defined as the
  optimal cutoff values.</p>
</blockquote>

<p>Somebody said that this question is already answered in ""regression model fitting for define cut-off"" but, I don't think so... It's not regression discontinued design, because there is no a-priori cut-off. Finding cutoff is the purpose of analysis.
Thanks.</p>
"
"0.0200240432865818","0.039253433598943"," 77915","<p>I have a time series $X_t$, which is shown in the first plot. In the second plot, I am doing a linear regression on $X_t\sim X_{t-1}$. The regression line is very close to $y=x$. But this is tricky since if if we look at the bottom left or the top right part of the data, they are almost random. From the diagnosis of residuals, it is not a good regression either. But the model passes all the $t$ tests and $F$ tests. How can I say it's not a good model then? Is there a statistic  to describe (not visually) the failure of this modelling?</p>

<p>Here are the <code>R</code> codes I used to generate the plots:</p>

<pre><code># Generating X_t
x=c(arima.sim(list(order = c(1,0,0),ar=0.1),n=64,sd=1),3,5,7,11,14,17,rep(20,64)+arima.sim(list(order = c(1,0,0),ar=0.1),n=64,sd=1))
# Regression X_t~X_{t-1}
reg=lm(x[2:length(x)]~x[1:(length(x)-1)])
# Plotting
par(mfrow=c(3,2))
plot(x,xlab='',ylab=expression(X[t]),ty='l')
plot(x[1:(length(x)-1)],x[2:length(x)],xlab=expression(x[t-1]),ylab=expression(x[t]) ,main=paste('coeff= ',round(reg$coefficients[2],2)))
# Plotting the regression line
abline(reg,col=2)
# Plotting the residual diagnose
plot(reg)
</code></pre>

<p><img src=""http://i.stack.imgur.com/2xJ65.png"" alt=""enter image description here""></p>
"
"0.0490486886395286","0.0480754414848157"," 78001","<p>I would like to perform something like a linear regression on my distribution of data, but I'm interested in a trendline that estimates the <strong>minimum</strong>, <em>not mean</em>, value for each time bin. I'd like to do this in R.</p>

<p>The image below shows a scatterplot of the minimum value for each time bin. The black line is a typical linear regression, which estimates the mean. What I'd like is something like what I painted in red - an estimation of the minimum.</p>

<p><img src=""http://i.stack.imgur.com/D0O7Q.gif"" alt=""enter image description here""></p>

<p>My data look like this:</p>

<p><img src=""http://i.stack.imgur.com/qGAiS.png"" alt=""enter image description here""></p>

<p>Those are just the first few lines but you get the idea.</p>

<p>Thank you!</p>
"
"NaN","NaN"," 78018","<p>Say I have two factor variables, X and Y, each with 3 levels. However, X==3 if and only if Y==3, while such a connection doesn't hold for X,Y==1,2. In this case, while X and Y are not redundant, my design matrix, when expanded, will contain the same column twice (0's everywhere, except 1's where X==Y==3). This is not good.</p>

<p>Theoretically, there is no problem: Just remove one of the redundant columns. However, I don't want to go ahead and re-implement the entire linear regression just for this case. Is there some technical or theoretical trick that will still allow me to use R's <code>lm</code>?</p>
"
"0.0578044339088637","0.0679889413649005"," 78022","<p>I am performing quantile regressions in R using the package quantreg. My dataset includes 12,328 observations ranging from 0.12 to 330. The timepoints for my data are not exactly continuous; all data fall into one of a few dozen bins ranging from 73 to 397.</p>

<p>When I performed a linear regression on this data using the lm() function, I was able to do this with polynomials up to 4:</p>

<pre><code>lm(Y~poly(X,3,raw=TRUE),data=mydata)
</code></pre>

<p>However, with the package quantreg and the rq() command, I cannot use any polynomials. A simple regression works just fine:</p>

<pre><code>rq(Y~X,data=mydata,tau=.15)
</code></pre>

<p>But as soon as I get into polynomials, no dice. When I enter this:</p>

<pre><code>rq(Y~poly(X,2,raw=TRUE),data=mydata,tau=.15)
</code></pre>

<p>I get the following error message:</p>

<pre><code>Error in rq.fit.br(x, y, tau = tau, ...) : Singular design matrix
</code></pre>

<p>I am very new to all of this. I've read up on singular matrices as much as I can and I think there might be two reasons for this: (1) I only have one variable on each axis, or (2) my data are binned/the Y variable isn't truly continuous.</p>

<p>Can anyone tell me why I'm getting this error?</p>

<p>Thank you!</p>

<p>PS - This is how the graph looks:</p>

<p><img src=""http://i.stack.imgur.com/htUdA.png"" alt=""enter image description here""></p>
"
"0.0400480865731637","0.039253433598943"," 78149","<p>I'm running a linear regression. The response variable is a proportion, and has quite a lot of zeros. The predictor variable is normal distributed. </p>

<p>My two variables look something like these:</p>

<pre><code>set.seed(50)
y &lt;- sample(c(rep(0, 20), seq(0, 1, by=0.01)), 100)
set.seed(50)
x &lt;- rnorm(100)
</code></pre>

<p>And my model looks something like this:</p>

<pre><code>summary(lm(y ~ x))

Call:
lm(formula = y ~ x)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.42550 -0.31907 -0.02064  0.29710  0.59214 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.410990   0.033378  12.313   &lt;2e-16 ***
x           -0.006252   0.033574  -0.186    0.853    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.3316 on 98 degrees of freedom
Multiple R-squared:  0.0003537, Adjusted R-squared:  -0.009847 
F-statistic: 0.03468 on 1 and 98 DF,  p-value: 0.8527
</code></pre>

<p>Is there a statistic that will indicate if the model is zero inflated? </p>
"
"0.0490486886395286","0.0480754414848157"," 78284","<p>I loaded the data to R and fitted it using GLM function.</p>

<blockquote>
  <p>fit.glm = glm(y~ aX+bZ+cW)</p>
</blockquote>

<p>Then, I found the cuttinf-point using ""segmented"" tool of R.</p>

<blockquote>
  <p>o&lt;-segmented(fit.glm,seg.Z=~X,psi=10)</p>
</blockquote>

<p>Now I have the cut-point and two different slope of X. </p>

<blockquote>
  <p>Call: segmented.glm(obj = fit.glm1, seg.Z = ~PTH, psi = 10)</p>
  
  <p>Meaningful coefficients of the linear terms: (Intercept)          X   Z       W</p>
  
  <p>19.43840           2.29574           0.08701           8.75784      </p>
  
  <p>Estimated Break-Point(s) psi1.PTH : 8.5 </p>
  
  <p>Degrees of Freedom: 324 Total (i.e. Null);  314 Residual Null
  Deviance:     17320  Residual Deviance: 7645      AIC: 1971</p>
</blockquote>

<p>However, I'm trying to get estimated y value of two linear regression models using the result of 'segmented'.</p>

<p>Is it possible using R, or I have to substitute Z and W with mean values of Z &amp; W, and calculate the y value myself?</p>
"
"0.02831827358943","0.0277563690826684"," 78360","<p>I need your help with a Statistical Learning homework in R.
I have to perform classification over this dataset: <a href=""http://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/"" rel=""nofollow"">mammographic masses</a> predicting Severity (0=""not severe"",1 = ""severe) using these predictors:</p>

<ul>
<li>Age (quantitative)</li>
<li>Margin (qualitative)</li>
<li>Shape (qualitative)</li>
</ul>

<p>Everything is fine and understandable when I use logistic regression, but I don't know if it's possible to run QDA (or linear discriminant analysis either), since two of the variables are qualitative.</p>
"
"0.10236445520486","0.0931666278472465"," 78455","<p>Disclaimer: Statistics is not my strong side, so if my question is nonsense I apologize. I'm a beginner, but really wanting to understand this.</p>

<p>My question is: why do I get so widely different parameter estimates when using different transformations on my data in a non-linear regression ?</p>

<p>I'm trying to do a nonlinear regression and to estimate the uncertainty of the fit (confidence interval) using linear approximation. From my understanding the more linear-like the shape of the nonlinear function, the more accurate will the confidence interval calculation by linear approximation be.  I therefore want to transform the data to make it as linear as possible. The errors in $y$ can be assumed to be log-normal. My data is monotonic and assumed to follow a power function in most cases.</p>

<p>$$ y = a*(x-x_0)^b $$</p>

<p>where $y$ is river discharge, $x$ is an arbitrary water level in the river and $x_0$ is the water level where where discharge $y$ is 0. This can be rewritten as log transformed, and nice and linear
$$ log(y) = a + b \times log(x-x_0) $$.</p>

<p>I need to estimate the parameters $a$, $b$ and $x_0$, so to do so simultaneously I use nonlinear regression. I also have some data that follows quadratic functions, so I would like to set up (and understand) a non-linear method.</p>

<p>I use r and <code>nlsLM()</code> from <code>minpack.lm</code> to carry out the non-linear regression.
Here is some example code:</p>

<pre><code>library(minpack.lm)

xdata &lt;- c(19,  21,  24,    25, 29, 34, 35, 40, 40, 46, 48, 48, 52, 56, 57, 65, 65, 68)
ydata &lt;- c(10,  11, 14, 20, 24, 50, 42, 96, 89, 134,    135,    161,    171,    218,    261,    371,    347,    393)
df&lt;-data.frame(x=xdata, y=ydata)

#weights applied in the case of no transformation (relative error assumed to be the same for all y data)
W&lt;-1/ydata

# NLS regression with weights, no transformation
nlsmodel1&lt;-nlsLM(y ~ a*(x-x0)^b,data=df,start=list(a=0.1, b=2.5,x0=0))

# log transformed
nlsmodel2&lt;-nlsLM(log(y) ~ a+b*(log(x-x0)),data=df,start=list(a=0.1, b=2.5,x0=0))
&gt; coef(nlsmodel1)
          a           b          x0 
0.005158377 2.719693093 4.896772931 
&gt; coef(nlsmodel2)
        a         b        x0 
-8.683758  3.445699 -4.139127 

&gt; exp(-8.683758)
[1] 0.0001693136
</code></pre>

<p>I understand that the weights are very important, and can have a say in the differences here, but not by this much? My judgement of the two parameter sets is that <code>nlsmodel1</code> performs ""better"", and that the <code>b</code> coefficient is too high in the fit from <code>nlsmodel2</code>. <code>nlsmodel2</code> does a poor job in the upper end of the data, with large residuals there. But why are they so different? I feel like I'm doing something very silly here, and is unable to see the error. I have tried some other transformations, for example only transforming LHS as <code>log(y)</code>, but the problem remains.</p>

<p>I appreciate any tips that can help me improve, and not the least understand, the transformed fit.</p>

<p>Cheers</p>

<p>Related <a href=""http://stats.stackexchange.com/questions/58928/nonlinear-regression-confidence-intervals-on-transformed-or-untransformed-param"">post #1</a> and <a href=""http://stats.stackexchange.com/questions/69524/on-nonlinear-regression-fits-and-transformations"">post #2</a></p>
"
"0.0578044339088637","0.0566574511374171"," 78529","<p>I have data from survey, and 
Trying to build a linear regression model using R like
A~ B
however, want to control C, D, E, F, G. like Age, Sex, and other confounding variables.
I tried to make some models using lm, glm, etc.
and fitted it to my data.</p>

<p>However, in Multivariate regression models, I cannot get the graph like below.
To use 'segmented' function of R to cut-off point, I guess, I should able to get estimated equation between A and B, while all other variables are controlled.</p>

<p><img src=""http://i.stack.imgur.com/RuWC4.png"" alt=""enter image description here""></p>

<p>The image above describes what I want to do.
linear model between A and B, but actual model includes confounders C, D, F, and somehow'controlled' them.
The author described that </p>

<blockquote>
  <p>fitted the two linear regression models for high B and low B, and
  calculated the sums of squares of residuals (=observed A -estimated
  A), from the two models for each B. The models with the lowest
  residual sums of squares were the best models.</p>
</blockquote>

<p>It seems to be drawn from R(and author said so), but I cannot find any good approach.</p>

<p>I'm afraid this question seems to 'tool specific' question/
If so, I'll amend this question, and update it with any help I got.</p>

<p>Thanks.</p>
"
"0.116939835314396","0.114619460088286"," 78563","<p><strong>Short version:</strong> I'm looking for an R package that can build decision trees whereas each leaf in the decision tree is a full Linear Regression model. AFAIK, the library <code>rpart</code> creates decision trees where the dependent variable is constant in each leaf. Is there another library (or a <code>rpart</code> setting I'm not aware of) that can build such trees?</p>

<p><strong>Long version:</strong> I'm looking for an algorithm that builds a decision tree based on a training data set. Each decision in the tree splits the training data set into two parts, according to a condition on one of the independent variables. The root of the tree contains the full data set, and each item in the data set is contained in exactly one leaf node.</p>

<p>The algorithm goes like this:</p>

<ol>
<li>Begin with the full dataset, which is the root node of the tree. Pick this node and call it $N$.</li>
<li>Create a Linear Regression model on the data in $N$.</li>
<li>If $R^2$ of $N$'s linear model is higher than some threshold $\theta_{R^2}$, then we're done with $N$, so mark $N$ as a leaf and jump to step 5.</li>
<li>Try $n$ random decisions, and pick the one that yields the best $R^2$ in the subnodes:
<ul>
<li>Pick a random independent variable $v_i$, as well as a random threshold $\theta_i$.</li>
<li>The decision $v_i \leq \theta_i$ splits the data set of $N$ into two new nodes, $\hat{N}$ and $\tilde{N}$.</li>
<li>Create Linear Regression models on both $\hat{N}$ and $\tilde{N}$, and calculate their $R^2$ (call them $\hat{r}$ and $\tilde{r}$).</li>
<li>From all those $n$ tuples $(v_i, \theta_i, \hat{r}, \tilde{r})$, select the one with the maximal $min(\hat{r}, \tilde{r})$. This yields a new decision in the tree, and $N$ has two new subnodes $\hat{N}$ and $\tilde{N}$.</li>
</ul></li>
<li>We have finished processing $N$. Pick a new node $N$ which has not yet been processed and go back to step 2. If all nodes have been processed, the algorithm ends.</li>
</ol>

<p>This will recursively build a decision tree which splits the data into smaller parts, and calculates a Linear Model on each of those parts.</p>

<p>Step 3 is the exit condition, which prevents the algorithm from overfitting. Of course, there are other possible exit conditions:</p>

<ul>
<li>Exit if $N$'s depth in the tree is above $\theta_{depth}$</li>
<li>Exit if the data set in $N$ is smaller than $\theta_{data set}$</li>
</ul>

<p><strong>Is there such an algorithm in an R package?</strong></p>
"
"0.0566365471788599","0.0555127381653369"," 78633","<p>I'm currently playing around with linear regression in R, and I've come up with a regression that fits data quite well. I'm just having some problems with interpreting the coefficients of my model. I know how to interpret log-log models in a simpler form, but when I have interactions I'm not quite sure how to interpret them.</p>

<p>Here's my output from R:</p>

<pre><code>Call:
lm(formula = log(y) ~ log(x1) + x2 * log(x1) + x3 * log(x1) + 
I(x3^2), data = Data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.56943 -0.12082  0.00012  0.11123  0.54579 

Coefficients:
                Estimate   Std. Error t value          Pr(&gt;|t|)    
(Intercept) -2.393889950  0.545879641  -4.385 0.000025149470154 ***
log(x1)      0.497477722  0.056113496   8.866 0.000000000000009 ***
x2          -0.000264760  0.000055476  -4.773 0.000005220020368 ***
x3           0.041126987  0.017930934   2.294           0.02357 *  
I(x3^2)     -0.000688879  0.000231778  -2.972           0.00358 ** 
log(x1):x2   0.000031580  0.000006691   4.720 0.000006494076511 ***
log(x1):x3   0.003145219  0.001277909   2.461           0.01528 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1932 on 119 degrees of freedom
Multiple R-squared: 0.9865,     Adjusted R-squared: 0.9859 
F-statistic:  1454 on 6 and 119 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>I've been Googling for the past hour, but I can only find answers to some simpler models like the answer given here: <a href=""http://stats.stackexchange.com/questions/18480/interpretation-of-log-transformed-predictor"">Interpretation of log transformed predictor</a> or <a href=""http://www.ats.ucla.edu/stat/sas/faq/sas_interpret_log.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/sas/faq/sas_interpret_log.htm</a></p>

<p>I hope someone out there can help me with interpreting the interaction terms and the polynomial term in my model. </p>
"
"0.0400480865731637","0.039253433598943"," 79746","<pre><code>&gt; ncvTest(alm)
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 121.2316    Df = 1     p = 3.400245e-28 

&gt; spreadLevelPlot(alm)
Suggested power transformation:  4.428269
</code></pre>

<p>I am having issues adjusting my regression formula based on what the results of the non-constant variable test shows. How do I implement the suggested power transformation here? Obviously with such a low p value this is heteroscedastic. I have run a robust standard error linear regression already and it does not change the BP test p value. I have also performed a variance inflation factor test to see if multicolinearity is an issue here, which it does not seem to be. </p>

<p>Any and all help is appreciated!. </p>
"
"0.02831827358943","0.0277563690826684"," 80172","<p>I performed a multivariate linear regression such that:</p>

<pre><code>fit&lt;-lm(as.matrix(y)~mwtkg+mbmi+mage,data=x)
</code></pre>

<p>where $y$ is a $500 \times 26$ multivariate outcomes. Then, I am wondering how to explain the <code>anova(fit)</code>:</p>

<pre><code>&gt; anova(fit)
Analysis of Variance Table

             Df  Pillai approx F num Df den Df    Pr(&gt;F)    
(Intercept)   1 0.99959    63064     25    651 &lt; 2.2e-16 ***
mwtkg         1 0.03506        1     25    651    0.5403    
mbmi          1 0.20862        7     25    651 &lt; 2.2e-16 ***
mage          1 0.09016        3     25    651 4.567e-05 ***
Residuals   675                                             
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the three Dfs, Pillai, and P values mean for the model?</p>
"
"0.0400480865731637","0.039253433598943"," 80175","<p>I am wondering whether there is a routine way to diagnose a multivariate linear model. For example:</p>

<pre><code>y   &lt;- rmvt(n=100, sigma=diag(3), df=3)  # 3 response vars as spherical multivariate t-dist
x1  &lt;- rchisq(100, 2)                    # x1 distributed as chi-squared
x2  &lt;- rt(100)                           # x2 distributed as t
fit &lt;- lm(as.matrix(y)~x1+x2)            # fits the multivariate regression
</code></pre>

<p>How can you diagnose whether the model is fitted well?</p>
"
"0.0400480865731637","0.039253433598943"," 80312","<p>I have carried out this linear regression that includes month coded as a dummy variable:</p>

<pre><code>library(plyr)
set.seed(1)
y &lt;- rnorm(120)
x1 &lt;- c(rep(""adult"", 60), rep(""juvenile"", 60))
x2 &lt;- c(rep(""male"", 60), rep(""female"", 60))
x3 &lt;- unlist(llply(month.abb, function(x) rep(x, 10)))

summary(lm(y ~ x1 + x2 + x3))

Call:
lm(formula = y ~ x1 + x2 + x3)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.46354 -0.51524 -0.03981  0.57625  1.95041 

Coefficients: (2 not defined because of singularities)
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  0.12073    0.28564   0.423    0.673
x1juvenile   0.00663    0.40396   0.016    0.987
x2male            NA         NA      NA       NA
x3Aug       -0.37510    0.40396  -0.929    0.355
x3Dec       -0.24718    0.40396  -0.612    0.542
x3Feb        0.12812    0.40396   0.317    0.752
x3Jan        0.01147    0.40396   0.028    0.977
x3Jul        0.32385    0.40396   0.802    0.424
x3Jun        0.02273    0.40396   0.056    0.955
x3Mar       -0.25440    0.40396  -0.630    0.530
x3May        0.01341    0.40396   0.033    0.974
x3Nov        0.22012    0.40396   0.545    0.587
x3Oct       -0.01502    0.40396  -0.037    0.970
x3Sep             NA         NA      NA       NA

Residual standard error: 0.9033 on 108 degrees of freedom
Multiple R-squared:  0.04703,   Adjusted R-squared:  -0.05003 
F-statistic: 0.4845 on 11 and 108 DF,  p-value: 0.9093
</code></pre>

<p>I now want to present the results of this linear regression within a table. Instead of presenting the beta for every month, is there a way to summarise the overall effect of month on <code>y</code> within the same table? For example, would if be acceptable to summarise the beta, se, t value and p value of <code>x3</code> by using their mean values across months?</p>
"
"0.0490486886395286","0.0480754414848157"," 80888","<p>I am trying to understand how to get the coefficient of a multiple linear regression. </p>

<p>The formula is:</p>

<p>$b = (X'X)^{-1}(X')Y$</p>

<p>I try to calculate $b$ without package and with the <code>lm</code> package inside R. </p>

<p>Doing so, I got different results. </p>

<p>I want to know why. Did I made a mistake? Or does the <code>lm</code> package calculate differently because of the intercept?</p>

<pre><code>&gt; y &lt;-  c(1,2,3,4,5)
&gt; x1 &lt;- c(1,2,3,4,5)
&gt; x2 &lt;- c(1,4,5,7,9)
&gt; Y &lt;- as.matrix(y)
&gt; X &lt;- as.matrix(cbind(x1,x2))
&gt; beta = solve(t(X) %*% X) %*% (t(X) %*% Y) ; beta
            [,1]
x1  1.000000e+00
x2 -1.421085e-14
&gt; model &lt;- lm(y~x1+x2) ; model$coefficients
 (Intercept)           x1           x2 
1.191616e-15 1.000000e+00 1.192934e-15 
</code></pre>

<p><img src=""http://i.stack.imgur.com/KHD2q.png"" alt=""3d""></p>

<h1>Update</h1>

<p>As Alex and the other told me, it was a question of roundoff error. Therefore, I decided to take another data from the book ""Essential Statistics for business and economics"" by Anderson and all. In this case, the coefficients are the same in both <code>lm</code> function and in my own matrix.</p>

<pre><code>&gt; y &lt;- c(9.3, 4.8, 8.9, 6.5, 4.2, 6.2, 7.4, 6, 7.6, 6.1)
&gt; x0 &lt;- c(1,1,1,1,1,1,1,1,1,1) 
&gt; x1 &lt;-  c(100,50,100,100,50,80,75,65,90,90)
&gt; x2 &lt;- c(4,3,4,2,2,2,3,4,3,2)
&gt; Y &lt;- as.matrix(y)
&gt; X &lt;- as.matrix(cbind(x0,x1,x2))

&gt; beta = solve(t(X) %*% X) %*% (t(X) %*% Y);beta
         [,1]
x0 -0.8687015
x1  0.0611346
x2  0.9234254
&gt; model &lt;- lm(y~+x1+x2) ; model$coefficients
(Intercept)          x1          x2 
 -0.8687015   0.0611346   0.9234254 
</code></pre>
"
"0.0424774103841449","0.0555127381653369"," 80955","<p>Is there anyway that I can perform LASSO with Negative Binomial Regression on R?
I am performing a negative binomial regression on my dataset because the data are too dispersed to impose poisson regression. Meanwhile, I am also facing some multicollinearity problem. I already tried using <code>glmnet</code> with <code>family = poisson</code>, but the data is not fitting very well (for both alpha = 0 and alpha = 1)...I honestly don't know what to do to analyze this big mess of data :/</p>

<p>thank you</p>

<p>EDIT: here is variance-covariance table of the negative binomial fit</p>

<pre><code>       8.392729e+18  1.239178e+06  -3.624090e+05  1.896258e+17  -3.702521e+17
       1.239178e+06  1.119052e-04   5.201989e-06 -1.877590e+05  -2.558095e+05
      -3.624090e+05  5.201989e-06   5.179343e-06 -8.021543e+04  -1.436381e+05
       1.896258e+17 -1.877590e+05  -8.021543e+04  2.193290e+17   6.413947e+16
      -3.702521e+17 -2.558095e+05  -1.436381e+05  6.413947e+16   2.142183e+17
</code></pre>
"
"0.0800961731463273","0.078506867197886"," 81120","<p>I frequently use this model to test catch efficiency and size selection properties of a given trawl fishing gear:</p>

<p>\begin{equation}
\theta(l)=\frac{s\times r(l)}{(1-s)+s\times r(l)}
\end{equation}</p>

<p>where $\theta(l)$ denotes the expected catch rate in the test gear ($T$), which has been fishing in parallel with a non-selective gear (the control gear with blind meshes,$C$). The parameters affecting $\theta(l)$ are:</p>

<ul>
<li><p><strong>Split parameter</strong> $(s)$: It defines the probability of a fish to enter in $T$ ($s$) or in $C$ ($1-s$), $s\in\{0,1\}$</p></li>
<li><p><strong>Fish size selection</strong> ($r(l)$): It defines the likelihood of fish retention in $T$. This likelihood is conditioned to fish body length, therefore it describes the size selection in $T$. Fish size selection is used to be defined using the logit function:</p></li>
</ul>

<p>\begin{equation}
r(l)=\frac{exp(\beta_1+\beta_2\times l )}{1+exp(\beta_1+\beta_2\times l )} 
\end{equation}</p>

<p>Overall, there is a total of 3 parameters to be estimated ($s$, $\beta_1$, $\beta_2$). We use nonlinear regression techniques to estimate such parameters by maximizing the binomial log-likelihood function:</p>

<p>\begin{equation}
\sum_l(N_{l}^T\times \log\theta(l)+N_{l}^C\times \log(1-\theta(l))) 
\end{equation}</p>

<p>where $N_{l}^T$ is the number of fishes per length-class caught in $T$, and $N_{l}^C$ is the numbers caught in $C$.</p>

<p>During a normal experiment, we deploy the pair of gears ($T$ and $C$) several times to perform parallel fishing. To account for the between-haul variation, we use the bootstrap (using a resampling scheme based on resampling between hauls and fishes within hauls) to estimate the errors of $s$, $\beta_1$ and $\beta_2$.</p>

<p>IÂ´m wondering if itÂ´s possible to shift towards a nonlinear mixed modeling approach, where the hauls are considered as a random component. At the moment I only could find such approach by using least squares as minimization criteria. But I could not find a way to keep using the log-likelihood binomial mass function as target criteria.</p>

<p>Thank you beforehand for any comment or guidance.</p>
"
"0.0942489115008991","0.0846805485716084"," 81612","<p>Recently I was trying to do logistic regression using the <code>rms::lrm()</code> function. But I had some trouble understanding the model objects from the function. Here is the example from the package:</p>

<pre><code>#dataset
n            &lt;- 1000    # define sample size
set.seed(17)            # so can reproduce the results
treat        &lt;- factor(sample(c('a','b','c'), n,TRUE))
num.diseases &lt;- sample(0:4, n,TRUE)
age          &lt;- rnorm(n, 50, 10)
cholesterol  &lt;- rnorm(n, 200, 25)
weight       &lt;- rnorm(n, 150, 20)
sex          &lt;- factor(sample(c('female','male'), n,TRUE))
L            &lt;- .1*(num.diseases-2) + .045*(age-50) +
                (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
                3.5*(treat=='b')+2*(treat=='c'))
y            &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)
#fit model
g            &lt;- lrm(y ~ treat*rcs(age))

&gt; g

Logistic Regression Model

lrm(formula = y ~ treat * rcs(age))

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          1000    LR chi2      76.77    R2       0.099    C       0.656    
 0            478    d.f.            14    g        0.665    Dxy     0.312    
 1            522    Pr(&gt; chi2) &lt;0.0001    gr       1.945    gamma   0.314    
max |deriv| 3e-06                          gp       0.156    tau-a   0.156    
                                           Brier    0.231    
&gt; anova(g)
                Wald Statistics          Response: y 

 Factor                                     Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)        5.62      10   0.8462
  All Interactions                           1.30       8   0.9956
 age  (Factor+Higher Order Factors)         65.99      12   &lt;.0001
  All Interactions                           1.30       8   0.9956
  Nonlinear (Factor+Higher Order Factors)    2.23       9   0.9872
 treat * age  (Factor+Higher Order Factors)  1.30       8   0.9956
  Nonlinear                                  0.99       6   0.9858
  Nonlinear Interaction : f(A,B) vs. AB      0.99       6   0.9858
 TOTAL NONLINEAR                             2.23       9   0.9872
 TOTAL NONLINEAR + INTERACTION               2.57      11   0.9953
 TOTAL                                      69.06      14   &lt;.0001
</code></pre>

<p><strong>Here are my questions:</strong><br>
For the object <code>g</code>,  </p>

<ul>
<li>What does the <code>max |deriv| 3e-06</code> mean?  </li>
<li>What do the Discrimination and Rand Discrim. Indexes suggest?  </li>
</ul>

<p>For the <code>anova(g)</code> object,  </p>

<ul>
<li>What's the <code>Factor +Higher Order Factors</code> for the treat?  </li>
<li>Why there are two <code>all interactions</code>? How to explain the nonlinear parts?</li>
</ul>
"
"0.0400480865731637","0.039253433598943"," 81774","<p>I want to perform linear regression using the following command:</p>

<p><code>lm(clinicVisits ~ clinicRank)</code> where <code>clinicVisits</code> is the number of visits to a clinic, and <code>clinicRank</code> is the rank of the clinic. </p>

<p><code>clinicVisits</code> is an integer in the range of 1â€“200K, and clinicRank is a continuous variable and has a range of  1.0â€“150.0 where 1.0 is the best ranking. </p>

<p>I am trying to predict what an increase in rank will mean in terms of number of visits, as I believe there is a relationship between the two, so I am seeking this linear regression equation:</p>

<pre><code>clinicVisits = y intercept + clinicRank*(b1)
</code></pre>

<p>I also assume that <code>b1</code> will be an increasingly negative number as the <code>clinicRank</code> gets smaller (i.e. closer to 1, the best rank). </p>

<p>Do I need to apply a scale to the variables since the <code>clinicRank</code> is relatively small compared to visits, or a log to the <code>clinicVisits</code> since it has a relative large range?</p>
"
"0.0633215847514023","0.0620651280774201"," 82153","<p>I have a multivariate time series dataset including interacting biological and environmental variables (plus possibly some exogenous variables). Beside seasonality, there is no clear long-term trend in the data. My purpose is to see which variables are related to each other. Forecasting is not really looked for. </p>

<p>Being new to time-series analysis, I read several references. As far as I understand, Vector Autoregressive (VAR) model would be appropriate, but I donâ€™t feel comfortable with seasonality and most examples I found concerned economics field (as often with time series analysisâ€¦) without seasonality.</p>

<p>What should I do with my seasonal data?
I considered deseasonalizing them â€“ for example in R, I would use <code>decompose</code> and then use the <code>$trend + $rand</code> values to obtain a signal which appears pretty stationary (as judged per <code>acf</code>).
Results of the VAR model are confusing me (a 1-lag model is selected while I would have intuitively expected more, and only coefficients for autoregression â€“ and not for regression with other lagged variables - are significant). 
Am I doing anything wrong, or should I conclude that my variables are not (linearly) related / my model is not the good one (subsidiary question: is there a non-linear equivalent to VAR?).</p>

<p>[Alternatively, I read I could probably use dummy seasonal variables, though I canâ€™t figure out exactly how to implement it].</p>

<p>Step-by-step suggestions would be very appreciated, since details for experienced users might actually be informative to me (and R code snippets or links towards concrete examples are very welcome, of course). Thank you.</p>
"
"0.0506572678011219","0.0620651280774201"," 82236","<p>I am using <code>earth package</code><a href=""http://cran.r-project.org/web/packages/earth/index.html"" rel=""nofollow"">earth: Multivariate Adaptive Regression Spline Models</a> regression to get a constant piecewise approximation of my data. I want to plot a band of confidence around it.  Does this make sense to estimate a confidence interval of the smoothed function? If yes how can I do this?</p>

<p>I know that confidence intervals cannot be calculated directly, since it is a non parametric regression but I want to have a coherent result like (plot a band of confidence around my smoothed function) what I can get using <code>lm</code> or <code>loess</code> smoothing.</p>

<p><strong>EDIT</strong> </p>

<p>I add some code to clarify my idea, Here what I would do if I am using linear regression or <code>loess</code>.</p>

<pre><code>set.seed(1)
x &lt;- rnorm(15)
dat &lt;- data.frame(x = x, y = c(x + rnorm(15)))
</code></pre>

<h3>Using lm</h3>

<pre><code>mod &lt;- lm(y ~ x,data=dat)
predfit &lt;- predict(mod,se=TRUE,interval=""confidence"")$fit
</code></pre>

<h3>Using loess</h3>

<pre><code>level=0.95
mod &lt;- loess(y~x,data=dat)
pred &lt;- predict(mod, se = TRUE)
y = pred$fit
    ci &lt;- pred$se.fit * qt(level / 2 + .5, pred$df)
data.frame(ymin = y - ci,
           ymax = y + ci)
</code></pre>

<p>My question how to get <code>ymin</code> and <code>ymax</code> if I use <code>earth</code> :</p>

<pre><code>library(earth)
mod = earth(y~x,data=dat)
</code></pre>
"
"0.0400480865731637","0.039253433598943"," 82270","<p>I run the following linear model in R :</p>

<pre><code>lm(formula = NA. ~ PC + I(1/SPCI), data = DSET)
</code></pre>

<p>The p-value for each predictor is significant, and it works fairly well with respect to most of the assumptions in linear regression, such as:</p>

<ul>
<li>Normal distribution of errors.</li>
<li>High correlation between predicted values and estimated values.</li>
<li>Homoscedasticity.</li>
<li>Non-collinearity between the predictors: PC and (1/SPCI) are not correlated at all!</li>
</ul>

<p>But, digging more into the topic, there's an assumption that fails in my model, and it says: </p>

<blockquote>
  <p>Linearity of the relationship between dependent and independent variables. </p>
</blockquote>

<p>This kind of contradicts the non-collinearity assumption because, if NA. and PC are highly correlated and NA. and (1/SPCI) are too, then PC and (1/SPCI) are correlated, and this is violating the assumption of non-collinearity. </p>

<p>I think that I misunderstood the assumption, or there's an explanation to this.</p>
"
"0.0895502439463906","0.0877733458775107"," 82277","<h3>Background</h3>

<p>I have a data set of patients who were operated on at two different hospitals, A and B. Lymph nodes were removed from each patient during the operation and counted, this is saved as <code>LN_reviewed</code> for each patient. I want to know how much variability there is in the lymph node number that is <strong>not</strong> accounted for by gender, the year of the operation, or the age of the patient when operated on. My assumption (hypothesis) is that any additional variability is likely due to the pathologist at the institution (this was not actually measured in my study).</p>

<h3>My question</h3>

<p>What is the best way to go about estimating the variability in lymph node number that is not accounted for by gender (a factor), year (a continuous variable), or age (a continuous variable)?</p>

<h3>My initial attempt at answering this question</h3>

<p>I built a linear regression model using the number of lymph nodes as the response variable and the gender, operation year, and operation age as predictors. I am not sure how to interpret the results to answer my specific question. Should I be looking at the R squared? If so, is there a way to get a confidence interval for it? Thanks to anyone who can help. If you think I am going about this the wrong way, please let me know.</p>

<pre><code>Call:
lm(formula = LN_reviewed ~ Gender + Operation__year + Operation__age, data = sample_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.436 -15.280  -0.495  13.450  61.564 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     -1.190e+04  1.459e+03  -8.159 9.95e-14 ***
GenderMALE      -5.542e+00  4.685e+00  -1.183    0.239    
Operation__year  5.980e+00  7.296e-01   8.196 8.01e-14 ***
Operation__age  -2.524e-01  1.675e-01  -1.507    0.134    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 22.46 on 158 degrees of freedom
Multiple R-squared:  0.2999,    Adjusted R-squared:  0.2866 
F-statistic: 22.56 on 3 and 158 DF,  p-value: 3.268e-12
</code></pre>
"
"0.0400480865731637","0.039253433598943"," 82346","<p>I just did a binary linear regression in R with a dataset that has 100000 lines. The output of the regression is, that almost every parameter is highly significant. I wouldn't expect that when I look at the <a href=""http://picpaste.com/yGsn701X.png"" rel=""nofollow"">boxplots</a>. Did I do something wrong in my code or can that be right?</p>

<pre><code>Call:
glm(formula = damage ~ dist_gerst + dist_gew + dist_hunt + dist_kart + 
    dist_mais + dist_raps + dist_road1 + dist_road2 + dist_road3 + 
    dist_road4 + dist_roada + dist_rog + dist_rmr + dist_ruben + 
    dist_sg + dist_wald + dist_hecke + dist_weize + dist_wg + 
    dist_bra, family = binomial(logit), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.3963  -0.0024   0.2446   0.4947   5.1474  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -9.7073574  0.5951280 -16.311  &lt; 2e-16 ***
dist_gerst   0.0355374  0.0053699   6.618 3.65e-11 ***
dist_gew     0.0033584  0.0012147   2.765 0.005698 ** 
dist_hunt    0.0531545  0.0017567  30.259  &lt; 2e-16 ***
dist_kart    0.0472300  0.0022333  21.148  &lt; 2e-16 ***
dist_mais    0.0578135  0.0031780  18.192  &lt; 2e-16 ***
dist_raps    0.0470257  0.0021689  21.682  &lt; 2e-16 ***
dist_road1  -0.0135003  0.0023328  -5.787 7.15e-09 ***
dist_road2   0.0304884  0.0016027  19.023  &lt; 2e-16 ***
dist_road3  -0.0003806  0.0011631  -0.327 0.743505    
dist_road4  -0.0515227  0.0048316 -10.664  &lt; 2e-16 ***
dist_roada  -0.0186244  0.0050640  -3.678 0.000235 ***
dist_rog    -0.0403263  0.0031825 -12.671  &lt; 2e-16 ***
dist_rmr    -0.1133255  0.0045872 -24.705  &lt; 2e-16 ***
dist_ruben   0.1168154  0.0032703  35.721  &lt; 2e-16 ***
dist_sg     -0.0450639  0.0020300 -22.199  &lt; 2e-16 ***
dist_wald    0.1127090  0.0035169  32.047  &lt; 2e-16 ***
dist_hecke   0.1065537  0.0028434  37.474  &lt; 2e-16 ***
dist_weize  -0.1496686  0.0038303 -39.075  &lt; 2e-16 ***
dist_wg     -0.0937316  0.0051061 -18.357  &lt; 2e-16 ***
dist_bra    -0.0599667  0.0023916 -25.074  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 64559  on 53510  degrees of freedom
Residual deviance: 32357  on 53490  degrees of freedom
  (46231 observations deleted due to missingness)
AIC: 32399

Number of Fisher Scoring iterations: 9
</code></pre>
"
"0.0800961731463273","0.078506867197886"," 82356","<p>I have the following table in <code>R</code></p>

<pre><code>df &lt;- structure(list(x = structure(c(12458, 12633, 12692, 12830, 13369, 
13455, 13458, 13515), class = ""Date""), y = c(6080, 6949, 7076, 
7818, 0, 0, 10765, 11153)), .Names = c(""x"", ""y""), row.names = c(""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""8"", ""9""), class = ""data.frame"")

&gt; df
           x     y
1 2004-02-10  6080
2 2004-08-03  6949
3 2004-10-01  7076
4 2005-02-16  7818
5 2006-08-09     0
6 2006-11-03     0
8 2006-11-06 10765
9 2007-01-02 11153
</code></pre>

<p>I can plot the points and a Tukey's linear fitting (<code>line</code> function in <code>R</code>) via </p>

<pre><code>plot(data=df,  y ~ x)
lines(df$x, line(df$x, df$y)$fitted.values)
</code></pre>

<p>which produces:</p>

<p><img src=""http://i.stack.imgur.com/n5TGd.png"" alt=""enter image description here""></p>

<p>All fine. The above plot shows energy consumption values, expected only to increase, so I'm happy with the fit not passing through those two points (which will be subsequently flagged as outliers).</p>

<p>However, ""just"" removing the last point and replot again</p>

<pre><code>df &lt;- df[-nrow(df),]
plot(data=df,  y ~ x)
lines(df$x, line(df$x, df$
)$fitted.values)
</code></pre>

<p>The result is completely different.</p>

<p><img src=""http://i.stack.imgur.com/4Ddau.png"" alt=""enter image description here""></p>

<p>My need is to have ideally the same result in both scenarios above. R doesn't seem to have ready to use function for monotonic regression, besides <code>isoreg</code> which however is piecewise constant.</p>

<p>EDIT:</p>

<p>As @Glen_b pointed out the outliers-to-sample size ratio is too big (~28%) for the regression technique used above. However, I believe there might be something else to consider. If I add the points at the beginning of the table:</p>

<pre><code>df &lt;- rbind(data.frame(x=c(as.Date(""2003-10-01""), as.Date(""2003-12-01"")), y=c(5253,5853)), df)
</code></pre>

<p>and recalculate again like above <code>plot(data=df,  y ~ x); lines(df$x, line(df$x,df$y)$fitted.values)</code> I get the same result, with a ration of ~22%</p>

<p><img src=""http://i.stack.imgur.com/hwIZp.png"" alt=""enter image description here""></p>
"
"0.0642198081225601","0.0629455284778823"," 82509","<p>I have time series data on fish catches from 1950-2011. </p>

<p>I wish to implement a regression model with varying coefficients. I'm aware that cox models etc. exist and implementation via the <code>survival</code> package in R. My data is not survival data, it's just several variables with fish catches and year.</p>

<p>Is there a way in R to implement such models? I've yet to come across this but I don't think it's unreasonable to want to model such data without it being survival data.</p>

<p>I want to model <code>inlandfao</code> from <code>marinefao</code>. </p>

<p>Here is my data and some plots:</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)

require(reshape2)
require(ggplot2)
theme_set(theme_bw())
require(scales)

df2 &lt;- data.frame(cbind(year,totalmarinefao, totalinlandfao))
df2
dd &lt;- melt(df2, id.vars = ""year"")
dd
pp &lt;- ggplot(dd, aes(year, value, colour=variable)) + geom_point() + geom_line(size=1)
pp_final &lt;- pp +  xlab(""Year"") + ylab(""Catches (Tons)"") + ggtitle(""Time Series of Variables (1950-2011)"") 
pp_final
pp_final2 &lt;- pp_final +  scale_colour_discrete(name = ""Variable - Catches (FAO)"", breaks = c(""totalmarinefao"", ""totalinlandfao""),
                                               labels=c(""Marine"", ""Inland"")) + 
  scale_shape_discrete(name = ""Variable (FAO)"", breaks = c(""totalmarinefao"", ""totalinlandfao""), labels=c(""Marine"", ""Inland"")) + 
  scale_x_continuous(breaks=seq(1950,2011,10)) + scale_y_continuous(labels=comma)

pp_final2
pp_3 &lt;- pp_final2 + theme(axis.text.x  = element_text(vjust=1, size=16)) + theme(axis.title.x = element_text(size=20))
pp_4 &lt;- pp_3 + theme(axis.text.y = element_text(vjust=0, size=16)) + theme(axis.title.y = element_text(size=20, vjust=0.2))
pp_5 &lt;- pp_4 + theme(plot.title = element_text(lineheight=.8, face=""bold"", size=20))
pp_5

qplot(marinefao, inlandfao, data=fishdata, main=""Scatterplot of the Marine &amp; \n Inland 
fish Catches (Tons)"", xlab=""Marine Catches"", ylab=""Inland Catches"") + 
scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma)
</code></pre>

<p>From these plots, a linear model isn't appropriate. I have fitted GAMs etc. to these data. </p>

<p>Let me more if you require details.</p>
"
"0.0200240432865818","0.039253433598943"," 82603","<p>I'm trying to understand the result I see in my graph below. Usually, I tend to use Excel and get a linear-regression line but in the case below I'm using R and I get a polynomial regression with the command:</p>

<pre><code>ggplot(visual1, aes(ISSUE_DATE,COUNTED)) + geom_point() + geom_smooth()
</code></pre>

<p>So my questions boil down to this: </p>

<ol>
<li><p>What is the gray area (arrow #1) around the blue regression line? Is this the standard deviation of the polynomial regression? </p></li>
<li><p>Can I say that the whatever is outside the gray area (arrow #2) is an 'outlier' and whatever falls inside the gray area (arrow #3) is within the standard deviation? </p></li>
</ol>

<p><img src=""http://i.stack.imgur.com/XRurN.png"" alt=""enter image description here""></p>
"
"0.0400480865731637","0.0196267167994715"," 82963","<p>A colleague of mine sent me this problem apparently making the rounds on the internet:</p>

<pre><code>If $3 = 18, 4 = 32, 5 = 50, 6 = 72, 7 = 98$, Then, $10 =$ ?
</code></pre>

<p>The answer seems to be 200.</p>

<pre><code>3*6  
4*8  
5*10  
6*12  
7*14  
8*16  
9*18  
10*20=200  
</code></pre>

<p>When I do a linear regression in R:</p>

<pre><code>data     &lt;- data.frame(a=c(3,4,5,6,7), b=c(18,32,50,72,98))  
lm1      &lt;- lm(b~a, data=data)  
new.data &lt;- data.frame(a=c(10,20,30))  
predict  &lt;- predict(lm1, newdata=new.data, interval='prediction')  
</code></pre>

<p>I get:   </p>

<pre><code>  fit      lwr      upr  
1 154 127.5518 180.4482  
2 354 287.0626 420.9374  
3 554 444.2602 663.7398  
</code></pre>

<p>So my linear model is predicting $10 = 154$.</p>

<p>When I plot the data it looks linear... but obviously I assumed something that is not correct.</p>

<p>I'm trying to learn how to best use linear models in R. What is the proper way to analyze this series? Where did I go wrong?</p>
"
"NaN","NaN"," 83401","<p>Is there a way to get the variance of prediction for a linear regression model in R? The variance that I need is $s_f^2=s^2\left(1+\frac{1}{n}+\frac{(x_{n+1}-\bar{x})^2}{\sum_{i=1}^n (x_i-\bar{x})^2}\right)$.</p>
"
"0.127734418513328","0.125199852089124"," 83493","<p>I have a data set on which I'm trying to do regression, and failing.</p>

<p>The situation:</p>

<ul>
<li>Thousands of battle robot operators are fighting battles among each other using battle robots.</li>
<li>Some battle robots are strong and powerful, and others are weak; the strong ones win more often and deal more damage.</li>
<li>Robot operators vary in skill, with the more skilled ones winning more often, and delivering more damage</li>
<li>We have some summary information about the outcomes of their battles, but not all of the details.</li>
<li>We know what battle robots they used in their battles, and how many times (including how many of those battles they won), and we know the total damage they dealt (of two kinds, damageA and damageB) in total</li>
<li>Some robots are better at inflicting damageA, while others damageB</li>
<li>For unknown battle robot operators based only on what robots they have used in battles (and how many times), we would like to estimate how much damage of each kind they would achieve, and what % of battles they have most likely won</li>
</ul>

<p>For example:</p>

<ul>
<li>John has used Robot A for 4 battles, and Robot B for 2 battles, and has dealt 240 units worth of DamageA</li>
<li>James has used Robot A for 1 battle, and Robot B for 10 battles, and has dealt 1010 units worth of DamageA</li>
</ul>

<p>I can therefore estimate that Robot A probably deals 10 units of Damage A per battle, while Robot B deals 100 units of Damage A per battle, and thus if asked to estimate Damage A dealt by Matthew who has only played each of the two robots for 2 battles each, will estimate at 220 == (10*2 + 100*2).</p>

<p>Unfortunately, the real data are not as clean and straightforward, probably because:</p>

<ul>
<li>There is a significant variance due to robot operator skill, e.g., a good operator could deal 20 units of damage with Robot A while a bad one only 5 units. </li>
<li>There is some random variance due to opponents drawn in case of a small sample (e.g. somebody draws a strong opponent and loses despite having a better robot than the opponent), although eventually it would even out</li>
<li>There may be some minor selection bias in that the best robot operators manage to pick the best robots to take into battle more often</li>
</ul>

<p>The real data set is available here (630k entries of known battle operator results):</p>

<p><a href=""http://goo.gl/YAJp4O"" rel=""nofollow"">http://goo.gl/YAJp4O</a></p>

<p>The data set is organized as follows, with one robot operator entry per row:</p>

<ul>
<li>Column 1 with no label - operator ID</li>
<li>battles - total battles this operator has participated in</li>
<li>victories - total battles this operator has won</li>
<li>defeats - total battles this operator has lost</li>
<li>damageA - total Damage A points inflicted</li>
<li>damageB - total Damage B points inflicted</li>
<li>130 pairs of columns as follows:
<ul>
<li>battles_[robotID] - battles using robot [robotID]</li>
<li>victories_[robotID] - victories attained using robot [robotID]</li>
</ul></li>
</ul>

<p>What I've done so far:</p>

<ul>
<li>Tried a couple of linear models using R <code>biglm</code> package which build a formula such as <code>damageA ~ 0 + battles_1501 + battles_4201 + ...</code> to try to get fitted ""expected"" values for each of the robots.</li>
<li>Same, but removing the forced origin intercept by not including <code>0 +</code> in the formula</li>
<li>Same, but also included the <code>victories_[robotID]</code> in the independent variables</li>
<li>Same as before, but only selecting those robot operators whose victory numbers are close to their defeat numbers</li>
<li>A linear regression model for <code>damageA ~ 0 + battles_1501 + battles_non_1501</code> where <code>battles_non_1501</code> are all the battles in robots other than robot model 1501. Then repeated for all the other robot types.</li>
</ul>

<p>I did sanity checks by looking at the predicted damageA and damageB values, as well as comparing the victories/battles ratio with the actual victories/battles ratio that we can actually precisely calculate for each of the robots.</p>

<p>In all cases while the results weren't completely off, they were sufficiently off to see that the model isn't quite working. For example, some robots achieved negative damage numbers which shouldn't really happen as you cannot do negative damage in a battle. </p>

<p>In case where I also used the known <code>victories_[robotID]</code> values in the formula, many of the <code>battle_[robotID]</code> coefficients ended up being somewhat large negative numbers, so I tried estimating for the ""average"" operator by <code>battle_[robotID] + victories_[robotID] / 2</code> but that also didn't give reasonable results.</p>

<p>I'm somewhat out of ideas now.</p>
"
"0.02831827358943","0.0277563690826684"," 83554","<p>Sorry for this beginner's question... I have googled this for a while with no success.</p>

<p>I do a linear regression using R lm function:</p>

<pre><code>x = log(errors)
plot(x,y)
lm.result = lm(formula = y ~ x)
abline(lm.result, col=""blue"") # showing the ""fit"" in blue
</code></pre>

<p><img src=""http://i.stack.imgur.com/2p7hZ.png"" alt=""enter image description here""></p>

<p>but it does not fit well. Unfortunately I can't make sense of the manual.</p>

<p>Can someone point me in the right direction to fit this better?</p>

<p>By fitting I mean I want to minimize the Root Mean Squared Error (RMSE).</p>

<hr>

<p><strong>Edit</strong>:
I have posted a related question (it's the same problem) here:
<a href=""http://stats.stackexchange.com/questions/83576/can-i-decrease-further-the-rmse-based-on-this-feature"">Can I decrease further the RMSE based on this feature?</a></p>

<p>and the raw data here:</p>

<p><a href=""http://tny.cz/c320180d"" rel=""nofollow"">http://tny.cz/c320180d</a></p>

<p>except that on that <a href=""http://tny.cz/c320180d"" rel=""nofollow"">link</a> x is what is called errors on the present page here, and there are less samples (1000 vs 3000 in the present page plot). I wanted to make things simpler in the other question.</p>
"
"0.0424774103841449","0.0555127381653369"," 83576","<p>I have a feature x, that I use to predict a probability y.</p>

<hr>

<p><strong>Some background on (x,y)</strong></p>

<p>I can't go into too much details, but hopefully the following should be enough to explain what x and y are, at least conceptually <em>[square and circles are NOT the actual label I am working with]</em>:</p>

<p><strong>y</strong></p>

<p>y is the probability of an image being of Class 0 or 1, with: </p>

<ul>
<li>Class 0 means that the image contains a <em>square</em>.</li>
<li>Class 1 means that the image contains a <em>circle</em>.</li>
</ul>

<p>100 people watched the training images, and classified them.
y is the result probability, so y=0 means there is definitely a square, y=1 means there is definitely a round.</p>

<p><strong>x</strong></p>

<p>x is a feature derived from the images, by <em>trying to fit them to a model of a circle</em>, and calculating the error.
So for example when x is very low, the probability of the image having a circle is high (relatively).</p>

<hr>

<p>plot(x,y)</p>

<p><img src=""http://i.stack.imgur.com/05230.png"" alt=""enter image description here""></p>

<p>x,y (1000 values for each) pasted here:
<a href=""http://tny.cz/c320180d"" rel=""nofollow"">http://tny.cz/c320180d</a></p>

<p>Using mean(y) as a predictor, I get <strong>RMSE = 0.285204</strong>:</p>

<pre><code>N = length(x)
average = mean(y)
RMSE = sqrt( 1/N * sum( (average-y)^2 ) )
RMSE
[1] 0.285204
</code></pre>

<p>Then using a linear regression on log(x), I could improve a little bit the <strong>RMSE = 0.2694513</strong>:</p>

<pre><code>log_x = log(x)
plot(log_x,y)
lm.result = lm(formula = y ~ log_x)
abline(lm.result, col=""blue"") # not working very well
linear_prediction = predict( lm.result, new, se.fit = TRUE)
prediction_linear_regression = matrix(0,N,1)
prediction_linear_regression = linear_prediction$fit
RMSE_linear_regression = sqrt( 1/N * sum( (prediction_linear_regression-y)^2 ) )
RMSE_linear_regression
[1] 0.2694513
</code></pre>

<p><img src=""http://i.stack.imgur.com/59Etc.png"" alt=""enter image description here""></p>

<p>Can the RMSE be further improved? What should I try?</p>
"
"NaN","NaN"," 83777","<p>I computed a feature <code>x</code> that I use to predict <code>y</code> which is a <strong>probability</strong> of being a certain class.</p>

<p><strong>Raw data in R format for (x,y) is pasted here:</strong> 
<a href=""http://tny.cz/a97b3fd0"" rel=""nofollow"">http://tny.cz/a97b3fd0</a>
(500 samples)</p>

<p>plot(x,y) looks like this:</p>

<p><img src=""http://i.stack.imgur.com/sKoJo.png"" alt=""enter image description here""></p>

<p>It seems similar to a <a href=""http://stats.stackexchange.com/questions/83554/linear-regression-not-fitting-well"">previous question</a> I asked, so I am tempted to try <a href=""http://stats.stackexchange.com/a/83613/21720"">this advice</a>, which I understand is the ""Least Squared Fit""on log(y) (because y is a probability).</p>

<p>Am I correct?</p>

<p>(My objective is to minimize the MSE.)</p>
"
"0.0980973772790571","0.0881383093888288"," 83826","<p>I estimated a robust linear model in <code>R</code> with MM weights using the <code>rlm()</code> in the MASS package. `R`` does not provide an $R^2$ value for the model, but I would like to have one if it is a meaningful quantity. I am also interested to know if there is any meaning in having an $R^2$ value that weighs the total and residual variance in the same way that observations were weighted in the robust regression. My general thinking is that, if, for the purposes of the regression, we are essentially with the weights giving some of the estimates less influence because they are outliers in some way, then maybe for the purpose of calculating $r^2$ we should also give those same estimates less influence? </p>

<p>I wrote two simple functions for the $R^2$ and the weighted $R^2$, they are below. I also included the results of running these functions for my model which is called HI9.  EDIT: I found web page of Adelle Coster of UNSW that gives a formula for <code>R2</code> that includes the weights vector in calculating the calculation of both <code>SSe</code> and <code>SSt</code> just as I did, and asked her for a more formal reference: <a href=""http://web.maths.unsw.edu.au/~adelle/Garvan/Assays/GoodnessOfFit.html"">http://web.maths.unsw.edu.au/~adelle/Garvan/Assays/GoodnessOfFit.html</a> (still looking for help from Cross Validated on how to interpret this weighted $r^2$.)</p>

<pre><code>#I used this function to calculate a basic r-squared from the robust linear model
r2 &lt;- function(x){  
+ SSe &lt;- sum((x$resid)^2);  
+ observed &lt;- x$resid+x$fitted;  
+ SSt &lt;- sum((observed-mean(observed))^2);  
+ value &lt;- 1-SSe/SSt;  
+ return(value);  
+ }  
r2(HI9)  
[1] 0.2061147

#I used this function to calculate a weighted r-squared from the robust linear model
&gt; r2ww &lt;- function(x){
+ SSe &lt;- sum((x$w*x$resid)^2); #the residual sum of squares is weighted
+ observed &lt;- x$resid+x$fitted;
+ SSt &lt;- sum((x$w*(observed-mean(observed)))^2); #the total sum of squares is weighted      
+ value &lt;- 1-SSe/SSt;
+ return(value);
+ }
 &gt; r2ww(HI9)
[1] 0.7716264
</code></pre>

<p>Thanks to anyone who spends time answering this. Please accept my apologies if there is already some very good reference on this which I missed, or if my code above is hard to read (I am not a code guy).</p>
"
"0.0200240432865818","0.039253433598943"," 83861","<p>Let's say I have the following data on leads, monthly media spend, and clicks</p>

<pre><code>Month  Leads   Media     Clicks
Jan     150    1000       500
Feb     200    1000       550 
March   300    1200       800
...
</code></pre>

<p>Let's say I run a linear regression where y is leads and the predictors are media and clicks. That's good, I know the relationships between these variable and can generate some lags to produce predictions. But what if I had spent 500 (or 2000 or 0) on media, what would have occurred. How do I perform this type of 'counter-factual' analysis where I attempt to find the results of a model if the actual value from one or two of the predictors was lower or higher? What is the standard approach (aka statistically proper approach)? Is it just a matter to ""adjusting"" the data to the 'new' number and rerunning the regression? or maybe simulating a regression 100+ times with 100+ different values for media?</p>
"
"0.0400480865731637","0.039253433598943"," 83945","<p>How do you solve the following problem?</p>

<blockquote>
  <p>A Simulation Study (Probit Regression).</p>
  
  <p>Assume $y|x\sim {\rm Binary}(p)$, where $p= E(y|x)$, and $Î¦^{-1}(\pi)=-1+5.1x_{1i}-0.3x_{2i}$
  Generate data with $x_{1i}\sim{\rm Unif}(0,1)$, $x_{2i}=1$ for $i$ odd and $x_{2i}=0$ for $i$ even, and sample size $n=500$. Try generalized linear model (GLM) with logistic and probit links.</p>
</blockquote>

<p>Here is what I did, I know there is a problem, but I don't know what:</p>

<pre><code>n         &lt;- 500
beta0     &lt;- -1
beta1     &lt;- 5.1
beta2     &lt;- -0.3
x1        &lt;- runif(n=n, min=0, max=1)
x2        &lt;- (1:n)%%2
y         &lt;- pnorm(beta0 + beta1*x1 + beta2*x2)
prob.glm  &lt;- glm(y~x1+x2, family=binomial(link=probit))
logit.glm &lt;- glm(y~x1+x2, family=binomial)
</code></pre>

<p>I know <code>y</code> is a probability here, but how do you simulate a binary variable from the probability? </p>
"
"0.0660759717086699","0.0832691072480053"," 84054","<p>I encountered a real-world problem where I want to model the effectiveness of various advertising media of a brand (measured in terms of sales). Basically, the Y in this case is weekly sales, and the X's are media investments in newspaper, magazine, display boards, tv, radio and online, as well as incentive, which is a percentage (like 10% off the original).</p>

<p>There are a few problems with the modelling work:</p>

<ul>
<li><p>all variables should have positive coefficients. Typically, more advertising or incentive is at least as good as less advertising/incentive (maybe this is not true if you buy all of the advertising spots in the world, as then your consumer will start to hate your brand, but this is not going to happen here). However, when I fit a typical regression (e.g. lm, glm, gls etc), some coefficients turn out to be negative (as data may be a bit noiser than expected, hence causing this problem?). I wonder if this can be controlled (I know in nonlinear regressions you can set constraints for parameters)</p></li>
<li><p>there should be some sort of diminishing marginal return of advertising spendings, but I am not exactly sure how to model that. Some ideas include using a log or square root transformation, another idea may be to use a nonlinear regression and estimator something like a*newspaper^b, where a is some coefficient, and b is an exponent between 0 and 1.</p></li>
<li><p>this is serial correlation, but this may not be exactly important here as the goal is only to estimate the parameters (if I use a regression I think I still get the unbiased estimators right? Autocorrelation only screws up the p-values, which is ignored here). Also, how to deal with seasonalities? I don't have much data (2 years) so maybe there is nothing we can do about it, but I have seen adding cos(0.0172*time) + sin(0.0172*time) to the regression equation to adjust for seasonal changes.</p></li>
</ul>

<p>Thanks.</p>
"
"0.0400480865731637","0.039253433598943"," 84243","<p>I have made this linear regression model:</p>

<pre><code>mtcars_lm &lt;- lm(mpg ~ drat + hp, mtcars)
</code></pre>

<p>Using the effects package, I can predict values of <code>mpg</code> for every value of <code>hp</code> between 70 and 150, plus get a confidence interval for each value of <code>mpg</code>:</p>

<pre><code>library(effects)
as.data.frame(effect(""hp"", mtcars_lm, xlevels = list(hp=seq(70, 150, 10))))

   hp      fit        se    lower    upper
1  70 24.06201 0.9066002 22.20781 25.91622
2  80 23.54415 0.8355302 21.83530 25.25300
3  90 23.02628 0.7691351 21.45322 24.59934
4 100 22.50841 0.7087300 21.05890 23.95793
5 110 21.99055 0.6559716 20.64894 23.33216
6 120 21.47268 0.6128381 20.21929 22.72608
7 130 20.95481 0.5814753 19.76556 22.14407
8 140 20.43695 0.5638509 19.28374 21.59015
9 150 19.91908 0.5612604 18.77118 21.06699
</code></pre>

<p>And I can plot the result:</p>

<pre><code>plot(effect(""hp"", mtcars_lm, xlevels = list(hp=seq(70, 150, 10))))
</code></pre>

<p><img src=""http://i.stack.imgur.com/KEmDi.jpg"" alt=""enter image description here""></p>

<p>My question is: what is the maths behind the calculation of the standard error and confidence interval for every value of <code>mpg</code>?</p>
"
"0.0693653206906364","0.0679889413649005"," 84319","<p>I am working on an age estimation method using 4 types of biological measurements as age predictors. I am using RStudio. 
So far, I have good results when I use linear regression (<code>lm(age~predictor)</code>), but I am encountering heteroskedasticity, and therefore cannot build prediction intervals for my models.<br> 
I have tried transformations to normalize the predictors using ln, inverse, and square root, but to no avail.<br>
I have found a paper explaining the <code>wls</code> function, and I have used it in my models with the weight: $$\frac 1 {1+\frac{\text{predictor}^2} 2}$$ 
This has given me better age predictions, but does not solve the heteroskedasticity problem. </p>

<p>I have done some research, and apparently, one of my options is to create homoscedastic groups in my data by finding the data points where the residual variances change. 
For that, I have used the breakpoints function of strucchange, which gave me 5 breakpoints by default. 
I now want to give 6 different weights (weights are $\frac 1 {\text{var(age)}}$ of each interval) to my 6 intervals of data, but I cannot find a function to do that. I would greatly appreciate any help on the subject. 
Thanks.</p>
"
"0.0566365471788599","0.0555127381653369"," 85719","<p>I was used R and mongodb for finding predictions of next date outcome for that I write R code as below</p>

<pre><code>library('RMongo')
mg1 &lt;- mongoDbConnect('demo','localhost',27017)
query &lt;- dbGetQuery(mg1,'hosts',""{'hostId' : '101.10.202.10'}"") 
date &lt;- query$runtimeMillis
    memory &lt;- query$memoryUtilization
plot(date,memory)
lm1 &lt;- lm(memory~date)
</code></pre>

<p>when I plot this graph it looks like below 
<img src=""http://i.stack.imgur.com/57OkC.jpg"" alt=""enter image description here""></p>

<p>Now I want to predict given date memory utilization so I add following code for finding prediction</p>

<pre><code>  new &lt;- data.frame(date=1377843220)
  prediction &lt;- predict(lm1, newdata=new, interval=""predict"")
</code></pre>

<p>but given prediction not match actual values I also tried interval=""confidence"" it also shows me wrong results. How should I find out prediction of given data is linear regression algorithm fitted here or any other algorithm helps?</p>

<p>Data excel file here <a href=""https://fs01n4.sendspace.com/dl/598e001c9c3a45696e48e8030d0f3a6d/52f49628461e5785/3mi7kr/memory_file.xls"" rel=""nofollow"">Download</a></p>
"
"0.0749231094763201","0.0734364498908627"," 85723","<p>As the title suggests, I'd like to ask what the difference between using Raw Prices vs. Daily Return vs. Price Ratio in the Augmented Dickey-Fuller unit-root test. The context is that I am trying to find cointegrated pairs of stocks from a given time series in R.</p>

<p>Take a look at these three different resources, all using different metrics (Raw, Daily and Ratio) for the ADF test:<br>
<a href=""http://quanttrader.info/public/testForCoint.html"" rel=""nofollow"">1. Raw Prices</a><br>
<a href=""http://gekkoquant.com/2012/12/17/statistical-arbitrage-testing-for-cointegration-augmented-dicky-fuller/"" rel=""nofollow"">2. Daily Returns</a><br>
3. Price Ratio (A UBS Research monograph, sorry couldn't find link but here is an excerpt):</p>

<blockquote>
  <p>Adjusted Dickey-Fuller (ADF) test â€“ a test of stationarity of the price ratio
  with null hypothesis of a unit root.</p>
</blockquote>

<p>Also worth noting that #3 doesn't perform linear regression (OLS), performing the ADF test on the price ratio itself, while #1 and #2 do it on the spread (residuals) of OLS. Any reason for this?</p>

<p>So if anyone could shed some light on the issue, that would be fantastic. Maybe it's just a matter of preference, but hopefully there are reasons for choosing them, with solid evidence backing it up to boot!</p>
"
"0.0700841515030364","0.078506867197886"," 85913","<p>I want to fit a DLM with time-varying coefficients, i.e. an extension to the usual linear regression,</p>

<p>$y_t = \theta_1 + \theta_2x_2$.</p>

<p>I have a predictor ($x_2$) and a response variable ($y_t$), marine &amp; inland annual fish catches respectively from 1950 - 2011. I want the DLM regression model to follow,</p>

<p>$y_t = \theta_{t,1} + \theta_{t,2}x_t$</p>

<p>where the system evolution equation is</p>

<p>$\theta_t = G_t \theta_{t-1}$</p>

<p>from page 43 of Dynamic Linear Models With R by Petris et al.</p>

<p>Some coding here,</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)
x &lt;- fishdata$marinefao
    y &lt;- fishdata$inlandfao

lmodel &lt;- lm(y ~ x)
summary(lmodel)
plot(x, y)
abline(lmodel)
</code></pre>

<p>Clearly time-varying coefficients of the regression model are more appropriate here. I follow his example from pages 121 - 125 and want to apply this to my own data. This is the coding from the example</p>

<pre><code>############ PAGE 123
require(dlm)

capm &lt;- read.table(""http://shazam.econ.ubc.ca/intro/P.txt"", header=T)
capm.ts &lt;- ts(capm, start = c(1978, 1), frequency = 12)
colnames(capm)
plot(capm.ts)
IBM &lt;- capm.ts[, ""IBM""]  - capm.ts[, ""RKFREE""]
x &lt;- capm.ts[, ""MARKET""] - capm.ts[, ""RKFREE""]
x
plot(x)
outLM &lt;- lm(IBM ~ x)
outLM$coef
    acf(outLM$res)
qqnorm(outLM$res)
    sig &lt;- var(outLM$res)
sig

mod &lt;- dlmModReg(x,dV = sig, m0 = c(0, 1.5), C0 = diag(c(1e+07, 1)))
outF &lt;- dlmFilter(IBM, mod)
outF$m
    plot(outF$m)
outF$m[ 1 + length(IBM), ]

########## PAGES 124-125
buildCapm &lt;- function(u){
  dlmModReg(x, dV = exp(u[1]), dW = exp(u[2:3]))
}

outMLE &lt;- dlmMLE(IBM, parm = rep(0,3), buildCapm)
exp(outMLE$par)
    outMLE
    outMLE$value
mod &lt;- buildCapm(outMLE$par)
    outS &lt;- dlmSmooth(IBM, mod)
    plot(dropFirst(outS$s))
outS$s
</code></pre>

<p>I want to be able to plot the smoothing estimates <code>plot(dropFirst(outS$s))</code> for my own data, which I'm having trouble executing. </p>

<p><strong>UPDATE</strong></p>

<p>I can now produce these plots but I don't think they are correct.</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)
x &lt;- as.numeric(fishdata$marinefao)
    y &lt;- as.numeric(fishdata$inlandfao)
xts &lt;- ts(x, start=c(1950,1), frequency=1)
xts
yts &lt;- ts(y, start=c(1950,1), frequency=1)
yts

lmodel &lt;- lm(yts ~ xts)
#################################################
require(dlm)
    buildCapm &lt;- function(u){
  dlmModReg(xts, dV = exp(u[1]), dW = exp(u[2:3]))
}

outMLE &lt;- dlmMLE(yts, parm = rep(0,3), buildCapm)
exp(outMLE$par)
        outMLE$value
mod &lt;- buildCapm(outMLE$par)
        outS &lt;- dlmSmooth(yts, mod)
        plot(dropFirst(outS$s))

&gt; summary(outS$s); lmodel$coef
       V1              V2       
 Min.   :87.67   Min.   :1.445  
 1st Qu.:87.67   1st Qu.:1.924  
 Median :87.67   Median :3.803  
 Mean   :87.67   Mean   :4.084  
 3rd Qu.:87.67   3rd Qu.:6.244  
 Max.   :87.67   Max.   :7.853  
 (Intercept)          xts 
273858.30308      1.22505 
</code></pre>

<p>The intercept smoothing estimate (V1) is far from the lm regression coefficient. I assume they should be nearer to each other. </p>
"
"0.11327309435772","0.111025476330674"," 86273","<p>I'm trying to calculate the log-likelihood for a generalized nonlinear least squares regression for the function $f(x)=\frac{\beta_1}{(1+\frac x\beta_2)^{\beta_3}}$ optimized by the <code>gnls</code> function in the R package <code>nlme</code>, using the variance covariance matrix generated by distances on a a phylogenetic tree assuming Brownian motion (<code>corBrownian(phy=tree)</code> from the <code>ape</code> package). The following reproducible R code fits the gnls model using x,y data and a random tree with 9 taxa:</p>

<pre><code>require(ape)
require(nlme)
require(expm)
tree &lt;- rtree(9)
x &lt;- c(0,14.51,32.9,44.41,86.18,136.28,178.21,262.3,521.94)
y &lt;- c(100,93.69,82.09,62.24,32.71,48.4,35.98,15.73,9.71)
data &lt;- data.frame(x,y,row.names=tree$tip.label)
model &lt;- y~beta1/((1+(x/beta2))^beta3)
f=function(beta,x) beta[1]/((1+(x/beta[2]))^beta[3])
start &lt;- c(beta1=103.651004,beta2=119.55067,beta3=1.370105)
correlation &lt;- corBrownian(phy=tree)
fit &lt;- gnls(model=model,data=data,start=start,correlation=correlation)
logLik(fit) 
</code></pre>

<p>I would like to calculate the log-likelihood ""by hand"" (in R, but without use of the <code>logLik</code> function) based on the estimated parameters obtained from <code>gnls</code> so it matches the output from <code>logLik(fit)</code>. NOTE: I am not trying to estimate parameters; I just want to calculate log-likelihood of the parameters estimated by the <code>gnls</code> function (although if someone has a reproducible example of how to estimate parameters without <code>gnls</code>, I would be very interested in seeing it!). </p>

<p>I'm not really sure how to go about doing this in R. The linear algebra notation described in Mixed-Effects Models in S and S-Plus (Pinheiro and Bates) is very much over my head and none of my attempts have matched <code>logLik(fit)</code>. Here are the details described by Pinheiro and Bates:</p>

<p>The log-likelihood for the generalized nonlinear least squares model  $y_i=f_i(\phi_i,v_i)+\epsilon_i$ where $\phi_i=A_i\beta$ is calculated as follows:</p>

<p>$l(\beta,\sigma^2,\delta|y)=-\frac 12 \Bigl\{ N\log(2\pi\sigma^2)+\sum\limits_{i=1}^M{\Bigl[\frac{||y_i^*-f_i^*(\beta)||^2}{\sigma^2}+\log|\Lambda_i|\Bigl]\Bigl\}}$</p>

<p>where $N$ is the number of observations, and $f_i^*(\beta)=f_i^*(\phi_i,v_i)$.</p>

<p>$\Lambda_i$ is positive-definite, $y_i^*=\Lambda_i^{-T/2}y_i$ and $f_i^*(\phi_i,v_i)=\Lambda_i^{-T/2}f_i(\phi_i,v_i)$</p>

<p>For fixed $\beta$ and $\lambda$, the ML estimator of $\sigma^2$ is </p>

<p>$\hat\sigma(\beta,\lambda)=\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2 / N$</p>

<p>and the profiled log-likelihood is</p>

<p>$l(\beta,\lambda|y)=-\frac12\Bigl\{N[\log(2\pi/N)+1]+\log\Bigl(\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2\Bigl)+\sum\limits_{i=1}^M\log|\Lambda_i|\Bigl\}$</p>

<p>which is used with a Gauss-Seidel algorithm to find the ML estimates of $\beta$ and $\lambda$. A less biased estimate of $\sigma^2$ is used:</p>

<p>$\sigma^2=\sum\limits_{i=1}^M\Bigl|\Bigl|\hat\Lambda_i^{-T/2}[y_i-f_i(\hat\beta)]\Bigl|\Bigl|^2/(N-p)$</p>

<p>where $p$ represents the length of $\beta$.</p>

<p>I have compiled a list of specific questions that I am facing:</p>

<ol>
<li>What is $\Lambda_i$? Is it the distance matrix produced by <code>big_lambda &lt;- vcv.phylo(tree)</code> in <code>ape</code>, or does it need to be somehow transformed or parameterized by $\lambda$, or something else entirely?</li>
<li>Would $\sigma^2$ be <code>fit$sigma^2</code>, or the equation for the less biased estimate (the last equation in this post)?</li>
<li>Is it necessary to use $\lambda$ to calculate log-likelihood, or is that just an intermediate step for parameter estimation? Also, how is $\lambda$ used? Is it a single value or a vector, and is it multiplied by all of $\Lambda_i$ or just off-diagonal elements, etc.?</li>
<li>What is $||y-f(\beta)||$? Would that be <code>norm(y-f(fit$coefficients,x),""F"")</code> in the package <code>Matrix</code>? If so, I'm confused about how to calculate the sum $\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2$, because <code>norm()</code> returns a single value, not a vector.</li>
<li>How does one calculate $\log|\Lambda_i|$? Is it <code>log(diag(abs(big_lambda)))</code> where <code>big_lambda</code> is $\Lambda_i$, or is it <code>logm(abs(big_lambda))</code> from the package <code>expm</code>? If it is <code>logm()</code>, how does one take the sum of a matrix (or is it implied that it is just the diagonal elements)?</li>
<li>Just to confirm, is $\Lambda_i^{-T/2}$ calculated like this: <code>t(solve(sqrtm(big_lambda)))</code>?</li>
<li>How are $y_i^*$ and $f_i^*(\beta)$ calculated? Is it either of the following:</li>
</ol>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) %*% y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) %*% f(fit$coefficients,x)</code></p>

<p>or would it be</p>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) * y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) * f(fit$coefficients,x)</code> ?</p>

<p>If all of these questions are answered, in theory, I think the log-likelihood should be calculable to match the output from <code>logLik(fit)</code>. Any help on any of these questions would be greatly appreciated. If anything needs clarification, please let me know. Thanks!</p>

<p><strong>UPDATE</strong>: I have been experimenting with various possibilities for the calculation of the log-likelihood, and here is the best I have come up with so far. <code>logLik_calc</code> is consistently about 1 to 3 off from the value returned by <code>logLik(fit)</code>. Either I'm close to the actual solution, or this is purely by coincidence. Any thoughts?</p>

<pre><code>  C &lt;- vcv.phylo(tree) # variance-covariance matrix
  tC &lt;- t(solve(sqrtm(C))) # C^(-T/2)
  log_C &lt;- log(diag(abs(C))) # log|C|
  N &lt;- length(y)
  y_star &lt;- tC%*%y 
  f_star &lt;- tC%*%f(fit$coefficients,x)
  dif &lt;- y_star-f_star  
  sigma_squared &lt;-  sum(abs(y_star-f_star)^2)/N
  # using fit$sigma^2 also produces a slightly different answer than logLik(fit)
  logLik_calc &lt;- -((N*log(2*pi*(sigma_squared)))+
       sum(((abs(dif)^2)/(sigma_squared))+log_C))/2
</code></pre>
"
"0.0566365471788599","0.0555127381653369"," 86888","<p>I implemented AVAS on my data in R. </p>

<p>$y = \text{weight}$, 
$x =$ matrix with several predictors, e.g. age, height, gender. </p>

<p>From what I understand, AVAS estimates transformations of $x$ and $y$ such that the regression of $y$ on $x$ is approximately linear with constant variance.</p>

<p>I followed <a href=""http://rgm3.lab.nig.ac.jp/RGM/R_rdfile?f=acepack/man/avas.Rd&amp;d=R_CC"" rel=""nofollow"">the help file</a> and did a plot for the following: </p>

<p><code>plot(a$y,a$ty)</code> â€“ this looks like a cubic curve which is not on any of the graphs on the help file. 
<code>plot(a$x,a$tx)</code> â€“ this looks like a big blob of black.</p>

<p>My question is â€“ how do I interpret the results, and how do I know what transformation was used to transform the data? E.g if I have a linear model: $\text{weight = age + height + gender}$ then how do I transform this using AVAS in R? </p>

<p>Also â€“ if AVAS transforms the data â€“ can I then perform variable selection on this data to 'eliminate' variables? Or does it also eliminate variables in the process?</p>
"
"0.0853828074607","0.0920574617898323"," 87105","<p>I have answered all of the following questions I need someone to verify me or if there is a better approach I would like to know about.</p>

<blockquote>
  <p>Q1) Look at your model summary to find the x variable whose model coefficient is most significantly different from 0. (You don't have to write R code to find this other variableâ€“just read your model summary.)<br>
      Q2) Make a simple linear regression model for PBE vs. this x.<br>
      Q3) Make a scatterplot of PBE vs. this x.<br>
      Q4) Add the simple regression line to your scatterplot.<br>
      Q5) Include a reasonable title and axis labels.  </p>
</blockquote>

<hr>

<pre><code>A0) beeflm = lm(PBE ~., data=beef)

A1) (coef(beeflm))
  (Intercept)          YEAR           CBE 
2693.01348650   -1.28693774   -1.84919910 
          PPO           CPO           PFO 
  -0.99901169   -1.73045916    1.27410503 
         DINC           CFO         RDINC 
  -2.49792219    1.04485422    1.32154103 
          RFP 
  -0.01729997 

A2) PBEvsDINC = lm(PBE~DINC, data=beef)

A3,A5) plot(beef$PBE, beef$DINC, main=""Beef PBE vs DINC"", xlab=""DINC"", ylab=""PBE"")   
(Am I right?) 
Error in plot.new() : figure margins too large

A4) abline(PBEvsDINC)
</code></pre>

<p>And finally here's the question I don't know the answer:</p>

<blockquote>
  <p>Q: Are the coefficients (y-intercept and slope in the x direction) the same for this second simple linear regression model as they are in the first multiple regression model?</p>
</blockquote>

<p>Here's the data for beef data.frame:</p>

<pre><code>YEAR    PBE     CBE     PPO     CPO     PFO     DINC    CFO     RDINC   RFP
1925    59.7    58.6    60.5    65.8    65.8    51.4    90.9    68.5     877
1926    59.7    59.4    63.3    63.3      68    52.6    92.1    69.6     899
1927      63    53.7    59.9    66.8    65.5    52.1    90.9    70.2     883
1928      71    48.1    56.3    69.9    64.8    52.7    90.9    71.9     884
1929      71      49      55    68.7    65.6    55.1    91.1    75.2     895
1930    74.2    48.2    59.6    66.1    62.4    48.8    90.7    68.3     874
1931    72.1    47.9      57    67.4    51.4    41.5      90      64     791
1932      79      46    49.5    69.7    42.8    31.4    87.8    53.9     733
1933    73.1    50.8    47.3    68.7    41.6    29.4      88    53.2     752
1934    70.2    55.2    56.6    62.2    46.4    33.2    89.1      58     811
1935    82.2    52.2    73.9    47.7    49.7      37    87.3    63.2     847
1936    68.4    57.3    64.4    54.4    50.1    41.8    90.5    70.5     845
1937      73    54.4    62.2      55    52.1    44.5    90.4    72.5     849
1938    70.2    53.6    59.9    57.4    48.4    40.8    90.6    67.8     803
1939    67.8    53.9      51    63.9    47.1    43.5    93.8    73.2     793
1940    63.4    54.2    41.5    72.4    47.8    46.5    95.5    77.6     798
</code></pre>

<h2>    1941      56      60    43.9    67.4    52.2    56.3    97.5    89.5     830</h2>

<p>Here is the t-value results, so is CBE the most significant variable according to this?:</p>

<pre><code>summary(lm(formula = PBE ~ ., data = beef))

Call:
lm(formula = PBE ~ ., data = beef)
Residuals:
Min 1Q Median 3Q Max 
-1.1986 -0.5658 -0.1001 0.7067 1.3251
Coefficients:
Estimate Std. Error t value
(Intercept) 2693.0135 1337.9523 2.013
YEAR -1.2869 0.6970 -1.846
CBE -1.8492 0.2834 -6.525
PPO -0.9990 0.4051 -2.466
CPO -1.7305 0.5685 -3.044
PFO 1.2741 1.9645 0.649
DINC -2.4979 2.6018 -0.960
CFO 1.0449 1.0678 0.979
RDINC 1.3215 1.7117 0.772
RFP -0.0173 0.1131 -0.153
Pr(&gt;|t|) 
(Intercept) 0.084020 . 
YEAR 0.107337 
CBE 0.000326 ***
PPO 0.043064 * 
CPO 0.018743 * 
PFO 0.537314 
DINC 0.368995 
CFO 0.360411 
RDINC 0.465337 
RFP 0.882717 
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Residual standard error: 1.238 on 7 degrees of freedom
Multiple R-squared: 0.986, Adjusted R-squared: 0.968 
F-statistic: 54.77 on 9 and 7 DF, p-value: 1.165e-05
</code></pre>
"
"NaN","NaN"," 87117","<p>I have 20 independent variables that sum to one. I have 181 vectors so my X matrix is 181 by 20 so I do have an overdetermined system. However, when I run lm() in R with this data it gives me nonsensical data with equal coefficients for every single independent variable. Also, the intercept is -1*coefficient. Could someone please explain why linear regression doesn't work in this situation?</p>
"
"0.132824476734854","0.124271233208968"," 87278","<p>I don't know if a similar problem has been asked before so if it has been, please provide me a link to the related/duplicate questions. I am sorry if I seem to be asking too much. But I really like to learn this stuff and this seems to be a good place to start asking.</p>

<p>I have been teaching myself statistics through self-study and I found Logan's <a href=""http://as.wiley.com/WileyCDA/WileyTitle/productCd-1405190086.html"" rel=""nofollow""><strong>Biostatistical Design and Analysis Using R</strong></a> very helpful in that it shows how the actual computations are done (in R) and how the results are interpreted. I particularly like the part about multiple regression (Chapter 9). I use R since it is the most accessible (and free) software that I can get my hands into. </p>

<p>Right now, I am trying to learn multivariate multiple regression. But unfortunately, I can't find a good resource. My specific problem is finding the best linear model for each response variable for the following morphometric data of a plant species (that some of my high school biology students are investigating), where <code>Leaves</code>, <code>CorL</code>, <code>CorD</code>, <code>FilL</code>, <code>AntL</code>, <code>AntW</code>, <code>StaL</code>, <code>StiW</code>, and <code>HeiP</code> are the response variables and <code>pH</code>, <code>OM</code>, <code>P</code>, <code>K</code> (nutrient variables), <code>Elev</code>, <code>SoilTemp</code>, and <code>AirTemp</code> (environment variables) are the independent variables.</p>

<p>I don't know if it is okay to proceed as in the case of only one dependent variable, but I went through the steps of Example 9B of Logan anyhow.</p>

<h3>lily.csv</h3>

<pre><code>Leaves,CorL,CorD,FilL,AntL,AntW,StaL,StiW,HeiP,Elev,pH,OM,P,K,SoilTemp,AirTemp
55,213.4,114.6,170.3,10.6,2.35,210,6.7,0.93,1431,6.37,1,3,170,29,26
44,192.15,95.25,160.6,7.1,2.25,176.4,6.55,0.79,1471,6.02,1,0,180,25,23
38,156.75,95.5,155.2,5.65,1.8,170.9,4.4,0.78,1471,6.02,1,0,180,25,23
29,191.8,88.35,155.2,10,2.5,178.25,5.9,0.75,1464,5.99,1,3,150,25,22
36,200.85,99.4,161.9,6.5,1.55,187.4,6.15,0.8,1464,5.99,1,3,150,25,22
43,210.2,74,147,7,1,170,5,0.8,1464,5.99,1,3,150,25,22
34,183.2,97.3,149.5,6.9,1.85,168.8,5.45,0.71,1464,5.99,1,3,150,25,22
52,233.3,107.7,179.6,9.2,3.05,210,6.45,0.82,1464,5.99,1,3,150,25,22
43,205.7,108.8,164.6,9.4,2,190.9,5.15,0.66,1464,5.99,1,3,150,25,22
28,203.15,119.35,160.6,8.9,2.3,180,6.85,0.77,1503,5.98,3,2,240,29.5,25.5
45,188.85,100.5,150.6,6.4,2.3,174.85,7.7,0.84,1503,5.98,3,2,240,29.5,25.5
49,205.2,126.85,150.8,10.1,2.8,177.5,9,0.84,1487,6.09,4,4,180,26,25
35,187.7,102.35,142.1,5.55,1.85,175.35,5.75,0.56,1485,6.17,3.5,1,220,24,23
23,181.05,94.6,136.6,6.9,1.8,169.3,5.8,0.59,1485,6.17,3.5,1,220,24,23
31,172.5,63.7,113.6,5.2,1.5,151.2,4.7,0.57,1482,6.29,5,2,280,24,23
34,190.5,93.1,151.9,5.65,1.85,172.5,5.25,0.68,1482,6.29,5,2,280,24,23
41,185.85,85.2,148.6,5.9,1.05,169.6,5.9,0.62,1472,6.48,0.5,3,170,25.22,22.89
29,195,159.2,159.3,15,4,185,6.3,0.59,1472,6.48,0.5,3,170,25.22,22.89
31,115.6,108.6,165.8,8.5,3,200.5,7.5,0.83,1454,5.53,5,14,350,25.22,22.89
27,176.65,93.1,128.65,6.65,2.85,180.5,6.65,0.53,1454,5.53,5,14,350,25.22,22.89
33,210,119,148,7,3,193,6,0.62,1454,5.53,5,14,350,25.22,22.89
42,200,93,166,18.3,4.55,177,8,1.12,1454,5.53,5,14,350,25.22,22.89
42,205,101.4,166.8,9,2.5,190,8.2,1.12,1454,5.53,5,14,350,25.22,22.89
25,192.9,94.15,147.8,6.45,2.3,167.65,7.15,0.61,1445,5.59,4,7,260,25.22,22.89
36,187.95,65.05,150.2,6.55,2.7,177.5,6.55,0.52,1445,5.59,4,7,260,25.22,22.89
32,110.4,11.6,168.15,7.6,2,197.95,7.85,0.73,1481,6.29,1.5,1,80,25.22,22.89
29,185,80,143,9,2,179,7.5,0.69,1481,6.29,1.5,1,80,25.22,22.89
29,179.8,70.6,134.8,11.15,3.2,165.65,5.3,0.6,1481,6.29,1.5,1,80,25.22,22.89
</code></pre>

<p>Firstly, I tried to investigate for possible collinearity among the variables.</p>

<pre><code>library(car)
lily = read.csv(""lily.csv"",header=T)
scatterplotMatrix(lily,diag=""boxplot"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/Ytsnd.png"" alt=""enter image description here""></p>

<p><code>FilL</code>, <code>AntL</code>, <code>AntW</code>, and <code>HeiP</code> seem to be non-normal so I made <code>log10</code> transformations. This <em>seems</em> to work fine. (And it is fine for you to educate me at this point if I am doing it wrong. I'd appreciate it very much.)</p>

<pre><code>scatterplotMatrix(~Elev + pH + OM + P + K + SoilTemp + AirTemp +
AirTemp + Leaves + CorL + CorD + log10(FilL) + log10(log10(log10(AntL)+0.1)+0.1) + 
log10(AntW) + StaL + StiW + log10(HeiP),data=lily,diag=""boxplot"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/t9nmI.png"" alt=""enter image description here""></p>

<p>I check for multicolinearity among the independent variables.</p>

<pre><code>&gt; cor(lily[,10:16])
                Elev          pH          OM           P           K
Elev      1.00000000  0.48252995 -0.06601928 -0.56726786 -0.28159580
pH        0.48252995  1.00000000 -0.58587694 -0.81673123 -0.70434283
OM       -0.06601928 -0.58587694  1.00000000  0.65931857  0.86478172
P        -0.56726786 -0.81673123  0.65931857  1.00000000  0.79782480
K        -0.28159580 -0.70434283  0.86478172  0.79782480  1.00000000
SoilTemp  0.14558365  0.01543524 -0.10436250 -0.05023853 -0.01041523
AirTemp   0.26450883  0.15711849  0.16862694 -0.09735977  0.11655030
            SoilTemp     AirTemp
Elev      0.14558365  0.26450883
pH        0.01543524  0.15711849
OM       -0.10436250  0.16862694
P        -0.05023853 -0.09735977
K        -0.01041523  0.11655030
SoilTemp  1.00000000  0.83202496
AirTemp   0.83202496  1.00000000
</code></pre>

<p>Among the independent variables, pairs <code>P</code> and <code>pH</code>, <code>K</code> and <code>pH</code>, <code>P</code> and <code>K</code>, <code>OM</code> and <code>K</code>, and <code>SoilTemp</code> and <code>AirTemp</code> have strong collinearity. </p>

<p>I also checked for collinearity among the dependent variables although I don't have an idea if this is a alright.</p>

<pre><code>&gt; cor(lily[,1:9])
            Leaves        CorL       CorD      FilL      AntL        AntW
Leaves 1.000000000  0.44495257 0.17903019 0.5222644 0.1495016 0.004680606
CorL   0.444952572  1.00000000 0.51084625 0.1319070 0.2101801 0.097530007
CorD   0.179030187  0.51084625 1.00000000 0.2368117 0.3297344 0.376806953
FilL   0.522264352  0.13190704 0.23681171 1.0000000 0.3932006 0.284738542
AntL   0.149501570  0.21018008 0.32973443 0.3932006 1.0000000 0.796401542
AntW   0.004680606  0.09753001 0.37680695 0.2847385 0.7964015 1.000000000
StaL   0.416083096  0.06574503 0.23272070 0.7762797 0.2701401 0.318744025
StiW   0.194927129 -0.05594094 0.08322138 0.3752195 0.3755628 0.445964273
HeiP   0.577737137  0.17603412 0.13911530 0.6348948 0.4583508 0.254173681
             StaL        StiW      HeiP
Leaves 0.41608310  0.19492713 0.5777371
CorL   0.06574503 -0.05594094 0.1760341
CorD   0.23272070  0.08322138 0.1391153
FilL   0.77627970  0.37521953 0.6348948
AntL   0.27014013  0.37556279 0.4583508
AntW   0.31874403  0.44596427 0.2541737
StaL   1.00000000  0.38306631 0.3794643
StiW   0.38306631  1.00000000 0.5039679
HeiP   0.37946433  0.50396793 1.0000000
</code></pre>

<p>From here, I can check for variance inflation and their inverses and possibly investigate interactions but I am really not sure now how to proceed or if it is alright at all to do these things in the multivariate case. And it seems to be a long way still to assessing the best multivariate model. In the case of the one dependent variable case, I can use the <code>MuMIn</code> package to automate the determination of the best fit but it doesn't work in the multiple response  case.</p>

<p>How do I proceed from this point? I will also appreciate it very much if you can point me to a good book or online material (preferably with applications in R).</p>
"
"NaN","NaN"," 87345","<p>I have tried calculating the AIC of a linear regression in R but without using the <code>AIC</code> function, like this:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ drat, mtcars)

nrow(mtcars)*(log((sum(lm_mtcars$residuals^2)/nrow(mtcars))))+(length(lm_mtcars$coefficients)*2)
[1] 97.98786
</code></pre>

<p>However, <code>AIC</code> gives a different value:</p>

<pre><code>AIC(lm_mtcars)
[1] 190.7999
</code></pre>

<p>Could somebody tell me what I'm doing wrong?</p>
"
"0.116759233144359","0.107710533187266"," 87487","<p><strong>Short version</strong></p>

<p>Is there a difference <strong>per treatment</strong> given time and this dataset?</p>

<p><strong>Or</strong> if the difference we're trying to demonstrate is important, what's the best method we have for teasing this out?</p>

<p><strong>Long version</strong></p>

<p>Ok, sorry if a bit <em>biology 101</em> but this appears to be an edge case where the data and the model need to line up in the right way in order to draw some conclusions. </p>

<p>Seems like a common issue... Would be nice to demonstrate an intuition rather than repeating this experiment with larger sample sizes. </p>

<p>Let's say I have this graph, showing mean +- std. error:</p>

<p><img src=""http://i.stack.imgur.com/eIKeF.png"" alt=""p1""></p>

<p>Now, it looks like there's a difference here. Can this be justified (avoiding Bayesian approaches)?</p>

<p>The simpleminded man's  approach would be to take Day 4 and apply a <em>t-test</em> (as usual: 2-sided, unpaired, unequal variance), but this doesn't work in this case. It appears the variance is too high as we only had 3x measurements per time-point (err.. mostly my design, p = 0.22).</p>

<p><strong>Edit</strong> On reflection the next obvious approach would be ANOVA on a linear regression. Overlooked this on first draft. This also doesn't seem like the right approach as the usual linear model is impaired from heteroskedasticity (<em>exaggerated variance over time</em>). <strong>End Edit</strong></p>

<p>I'm guessing there's a way to include <strong>all</strong> the data which would fit a simple (1-2 parameter) model of growth over time per predictor variable then compare these models using some formal test. </p>

<p>This method should be justifiable yet accessible to a relatively unsophisticated audience.</p>

<p>I have looked at <code>compareGrowthCurves</code> in <a href=""http://cran.r-project.org/web/packages/statmod/statmod.pdf"" rel=""nofollow"">statmod</a>, read about <a href=""http://www.jstatsoft.org/v33/i07/paper"" rel=""nofollow"">grofit</a> and tried a linear mixed-effects model adapted from <a href=""http://stats.stackexchange.com/questions/61153/nlme-regression-curve-comparison-in-r-anova-p-value"">this question on SE</a>. This latter is closest to the bill, although in my case the measurements are not from the <strong>same subject</strong> over time so I'm not sure mixed-effects/multilevel models are appropriate. </p>

<p>One sensible approach would be to model the rate of growth per time as linear and fixed and have the random effect be <strong>Tx</strong> then <a href=""http://www.statistik.uni-dortmund.de/useR-2008/slides/Scheipl+Greven+Kuechenhoff.pdf"" rel=""nofollow"">test it's significance</a>, although I gather there's <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">some debate</a> about the merits of such an approach.</p>

<p>(Also this method specifies a linear model which would not appear to be the best way to model a comparison of growth which in the case of one predictor has not yet hit an upper boundary and in the other appears basically static. I'm guessing there's a generalized mixed-effects model approach to this difficulty which would be more appropriate.)</p>

<p>Now the code:</p>

<pre><code>df1 &lt;- data.frame(Day = rep(rep(0:4, each=3), 2),
              Tx = rep(c(""Control"", ""BHB""), each=15),
              y = c(rep(16e3, 3),
              32e3, 56e3, 6e3,
              36e3, 14e3, 24e3,
              90e3, 22e3, 18e3,
              246e3, 38e3, 82e3,
              rep(16e3, 3),
              16e3, 34e3, 16e3,
              20e3, 20e3, 24e3,
              4e3, 12e3, 16e3,
              20e3, 5e3, 12e3))
### standard error
stdErr &lt;- function(x) sqrt(var(x)) / sqrt(length(x))
library(plyr)
### summarise as mean and standard error to allow for plotting
df2 &lt;- ddply(df1, c(""Day"", ""Tx""), summarise,
             m1 = mean(y),
             se = stdErr(y) )
library(ggplot2)
### plot with position dodge
pd &lt;- position_dodge(.1)
ggplot(df2, aes(x=Day, y=m1, color=Tx)) +
 geom_errorbar(aes(ymin=m1-se, ymax=m1+se), width=.1, position=pd) +
 geom_line(position=pd) +
 geom_point(position=pd, size=3) +
 ylab(""No. cells / ml"")
</code></pre>

<p>Some formal tests:</p>

<pre><code>### t-test day 4
with(df1[df1$Day==4, ], t.test(y ~ Tx))
### anova
anova(lm(y ~ Tx + Day, df1))
### mixed effects model
library(nlme)
f1 &lt;- lme(y ~ Day, random = ~1|Tx, data=df1[df1$Day!=0, ])
library(RLRsim)
exactRLRT(f1)
</code></pre>

<p>this last giving</p>

<pre><code>    simulated finite sample distribution of RLRT.  (p-value based on 10000
    simulated values)

data:  
RLRT = 1.6722, p-value = 0.0465
</code></pre>

<p>By which I conclude that the probability of this data (or something more extreme), <em>given the null hypothesis that there is no influence of <strong>treatment</strong> on <strong>change over time</em></strong> is close to the elusive 0.05. </p>

<p>Again, sorry if this appears a bit basic but I feel a case like this could be used to illustrate the importance of modelling in avoiding further needless experimental repetition. </p>
"
"0.0942489115008991","0.0846805485716084"," 87608","<p>I have run a few tests/methods on my data and am getting contradictory results.</p>

<p>I have a linear model saying:
reg1 = lm(weight = height + age + gender (categorical) + several other variables). </p>

<p>If I model each term linearly i.e. no squared or interaction term, and run vif(reg1), 4 variables are >15. If I delete the variable with the highest vif number and re-run it the gifs change and now only 2 variables are >15. I repeat this until I'm left with 20 variables (out of 30) below 10. If I use stepwise directly on reg1 then it does not delete the 'highest vic' factor. <strong>I don't understand how it tells me 'what' is linearly dependant on 'what variable' and how (and I cannot seem to find this information despite googling for ages).</strong> </p>

<p>Furthermore, when I look at the residual plots, most appear horizontal except a few which are upside down u curved (none of these have high vifs). Does this means a transformation is needed? (I removed outliers, leverage points etc - but now there seem to be more!)</p>

<p>reg2 = lm(weight = (height + age + gender (categorical) + several other variables)^2). </p>

<p>If I run vif on this all of the terms are >500! </p>

<p>What else I have tried (without cutting any variables): 
(1) The errors seem correlated when i run diagnostics and check with Durbin Waston statistics indicating the model is not linear... however...
(2) Box Cox gives lambda = 1 so no transformation is needed.
(3) LASSO gives the lowest mallows cp on the full 30 variable model (i.e. least squares)
(4) Ridge regression gives lambda = 0 which did surprise me. </p>

<p>I'm getting really confused about this data. <strong>To determine a suitable model for weight should I be looking just at linear terms or linear and interaction terms (remember there are 25 variables so there are 30^2 interaction terms)?</strong> </p>

<p>When I check which ones are significant in reg2 only 12 predictors and 6 interaction terms seem significant (AIC is lowest with this combination after I run step). <strong>Should I just use this 'new model with deleted variables/interaction terms' and do all my tests e.g. stepwise method, LASSO etc or do I do it on the entire model?</strong> </p>

<p>I'm getting quite lost in terms of making sense of steps to find a suitable model for weight using the variables. </p>

<p><strong>My final question is once I have the model - how do i test/prove its the best/a decent model?</strong> </p>

<p>Any help would really be appreciated. </p>
"
"0.0980973772790571","0.0881383093888288"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.0490486886395286","0.0480754414848157"," 87694","<p>I'm running a linear regression analysis in R. One variable is a continuous outcome variable (<code>score2</code>) and the other is a categorical variable for treatment group (<code>group</code>). I also have a covariate which is the time 1 measure of the outcome variable (<code>score1</code>). So the model in R looks like this:</p>

<pre><code>lm(score2~group + score1, data=dataSet)
</code></pre>

<p>Here's the question: When I plot the residuals, the plots shows evidence of non-linearity (a U-shaped plot). If both of my variables were continuous rather than categorical, I'd be sure that I need to correct for non-linearity (using Box-Cox for example). Does having a categorical predictor change this or do I still need to make the correction?</p>
"
"0.0633215847514023","0.0620651280774201"," 87720","<p>R is creating dummy variables for the color variable (color is either white or red) in my linear regression model. For example, color:pH returns the interaction term colorred:pH. Some questions: </p>

<p>Why is R only returning <code>colorwhite</code> for some interaction terms and <code>colorred</code> with other terms? Why doesn't it do <code>colorwhite</code> and <code>colorred</code> for each term interacting with color?</p>

<p>Next, if I want to rebuild a model using only some of the dummy variables R generated, how can I reference them in a linear model? If I retype the model manually and include <code>colorwhite:pH</code> as a term, R returns an error that the object <code>colorwhite</code> isn't found. </p>

<p><img src=""http://i.stack.imgur.com/dVR9H.png"" alt=""enter image description here""></p>
"
"0.0566365471788599","0.0555127381653369"," 87814","<p>Suppose I have a set of (discrete) variables, say $X_1,\dots,X_n$. Each $i$ belongs to either class A or class B. When it belongs to A the contribution is $Y_{A,i}f(X_i)$, and when it belongs to B the contribution is $Y_{B,i}g(X_i)$, so that the final outcome is $Z=\sum_{i=1}^N Y_{A,i}f(X_i) I_{\{i \in A\}} + Y_{B,i}g(X_i) I_{\{i \in B\}}$, where $I_{\{.\}}$ denotes the indicator-function.  </p>

<p>The values for $X_1,\dots,X_n$ are given, as are the functions $f$ and $g$. Now suppose I have $M$ obervations for $Z$, what method do you advice to</p>

<p>(i)  Identify whether $i$ is in $A$ or $B$</p>

<p>(ii) Estimate the values for $Y_{A,i}$ (if $i\in A)$ and $Y_{B,i}$ (if $i \in B$).</p>

<p>I tried to estimate the coefficients $Y_{A,i}$ and $Y_{B,i}$ using standard linear regression, assuming that $i\in A$ for all $i$ or $i \in B$ for all $i$, but that gives bad results.</p>

<p>(iii) What if I have $M$ different inputs $X_{j,1},\dots,X_{j,N}$, for $j=1,\dots,M$ and one observation for $Z$ for each input?</p>

<p>Edit:</p>

<p>As an example, take the following R-code:</p>

<pre><code>N=50
M=10
X=rbinom(N*M,1,0.5)-rbinom(N*M,1,0.5) #Generate -1, 0, 1 variables
dim(X) &lt;- c(N,M)
pA=1/2 #pB=1-pA
XA = X #f(x)=x
XB = -X^2 + 1 #g(x)=-x^2+1
isA = as.logical(rbinom(M,1,pA))
YA = rnorm(M,1,5) #generate coefficients
YB = rnorm(M,1,5) #generate coefficients
Z = XA[,isA] %*% YA[isA] + XB[,!isA] %*% YB[!isA] #Calculate Z without noise
Z = Z+rnorm(N,sd=sqrt(var(Z))) #add noise 
</code></pre>

<p>Thus given the observations <code>Z</code> and the input data <code>XA, XB</code> (these are known) I want to estimate <code>isA, YA[isA]</code> and <code>YB[!isA]</code>.</p>
"
"0.0633215847514023","0.0620651280774201"," 87903","<p>I am fitting a GLM to binomial data with two independent variables, <code>a</code> and <code>b</code>. I have tried to fit a binomial GLM with the form <code>y~a+b+a*b</code>, so that I have the contributions of the two variables as well as an interaction term. This works OK, but in the data the effect of <code>b</code> is very gradual in comparison to <code>a</code>. The best fit I've tried by far is simply <code>y~a+log(b+1)+a*log(b+1)</code> (the +1 is because b=0 for some datapoints) with a binomial link function. Is it possible to use this as a GLM? It's just that I've not seen this done when I've searched online so I don't know if there's some good reason that people only seem to use polynomials. There is a question that seems related in a previous post <a href=""https://stats.stackexchange.com/questions/78633/interpret-interactions-and-logarithms-in-linear-regression"">Interpret interactions and logarithms in linear regression</a>, but this was using a log-log function, and used <code>lm()</code> instead of <code>glm()</code>; so I'm not at all confident of the relation. </p>
"
"0.0633215847514023","0.0620651280774201"," 87946","<p>I have lots of time series with periods: day, week or month. With <code>stl()</code> function or with <code>loess(x ~ y)</code> I can see how trends of particular time series look. I need to detect if trend of time series is increasing or decreasing. How can I manage that?</p>

<p>I tried to compute linear regression coefficients with <code>lm(x ~ y)</code> and play with slope coefficient. (<code>If |slope|&gt;2 and slope&gt;0 then</code> increasing trend, <code>else if |slope|&gt;2 and slope&lt;0</code> â€“ decreasing).
Maybe there is another and more effective method for trend detection? Thank you!</p>

<p>For example: I have <code>timeserie1</code>, <code>timeserie2</code>. I need a simple algorithm that would tell me that <code>timeserie2</code> is an increasing algorithm, and in <code>timeserie1</code>, the trend isn't increasing or decreasing. What criteria should I use?</p>

<p><code>timeserie1</code>:</p>

<pre><code>1774 1706 1288 1276 2350 1821 1712 1654 1680 1451 1275 2140 1747 1749 1770 1797 1485 1299 2330 1822
1627 1847 1797 1452 1328 2363 1998 1864 2088 2084  594  884 1968 1858 1640 1823 1938 1490 1312 2312
1937 1617 1643 1468 1381 1276 2228 1756 1465 1716 1601 1340 1192 2231 1768 1623 1444 1575 1375 1267
2475 1630 1505 1810 1601 1123 1324 2245 1844 1613 1710 1546 1290 1366 2427 1783 1588 1505 1398 1226
1321 2299 1047 1735 1633 1508 1323 1317 2323 1826 1615 1750 1572 1273 1365 2373 2074 1809 1889 1521
1314 1512 2462 1836 1750 1808 1585 1387 1428 2176 1732 1752 1665 1425 1028 1194 2159 1840 1684 1711
1653 1360 1422 2328 1798 1723 1827 1499 1289 1476 2219 1824 1606 1627 1459 1324 1354 2150 1728 1743
1697 1511 1285 1426 2076 1792 1519 1478 1191 1122 1241 2105 1818 1599 1663 1319 1219 1452 2091 1771
1710 2000 1518 1479 1586 1848 2113 1648 1542 1220 1299 1452 2290 1944 1701 1709 1462 1312 1365 2326
1971 1709 1700 1687 1493 1523 2382 1938 1658 1713 1525 1413 1363 2349 1923 1726 1862 1686 1534 1280
2233 1733 1520 1537 1569 1367 1129 2024 1645 1510 1469 1533 1281 1212 2099 1769 1684 1842 1654 1369
1353 2415 1948 1841 1928 1790 1547 1465 2260 1895 1700 1838 1614 1528 1268 2192 1705 1494 1697 1588
1324 1193 2049 1672 1801 1487 1319 1289 1302 2316 1945 1771 2027 2053 1639 1372 2198 1692 1546 1809
1787 1360 1182 2157 1690 1494 1731 1633 1299 1291 2164 1667 1535 1822 1813 1510 1396 2308 2110 2128
2316 2249 1789 1886 2463 2257 2212 2608 2284 2034 1996 2686 2459 2340 2383 2507 2304 2740 1869  654
1068 1720 1904 1666 1877 2100  504 1482 1686 1707 1306 1417 2135 1787 1675 1934 1931 1456 1363 2027
1740 1544 1727 1620 1232 1199
</code></pre>

<p><code>timeserie2</code>:</p>

<pre><code> 122  155  124   97  155  134  115  122  162  115  102  163  135  120  139  160  126  122  169  154
 121  134  143  100  121  182  139  145  135  147   60   58  153  145  130  126  143  129   98  171
 145  107  133  115  113   96  175  128  106  117  124  107  114  172  143  111  104  132  110   80
 159  131  113  123  123  104  101  179  127  105  133  127  101   97  164  134  124   90  110  102
  90  186   79  145  130  115   79  104  191  137  114  131  109   95  119  173  158  137  128  119
 109  120  182  140  133  113  121  110  122  159  129  124  119  109  108   95  167  138  125  105
 139  118  115  166  140  112  116  139  121  109  164  135  118  121  112  111  102  169  136  151
 132  135  130  112  156  134  121  116  114   91   86  141  160  116  118  112   84  114  165  141
 109  123  122  110  100  162  145  121  118  115  107  103  162  142  130  139  134  121  118  164
 147  125  120  134  107  130  158  141  144  148  124  135  118  212  178  154  167  155  176  143
 201  170  144  138  152  136  123  223  189  160  153  190  136  144  276  213  199  211  196  170
 179  460  480  499  550  518  493  557  768  685  637  593  507  611  569  741  635  563  577  498
 456  446  677  552  515  441  438  462  530  699  629  555  641  625  544  585  705  584  553  622
 506  500  533  777  598  541  532  513  434  510  714  631 1087 1249 1102  913  888 1147 1056 1073
1075 1136  927  922 1066 1074  996 1189 1062  999  974 1174 1097 1055 1053 1097 1065 1171  843  441
 552  779  883  773  759  890  404  729  703  810  743  743  946  883  813  876  841  742  715  960
 862  743  806  732  669  621
</code></pre>
"
"0.0693653206906364","0.0679889413649005"," 88212","<p>I have 30 variables and am trying to select the best model. I have run the following methods on a 'large' data set (having removed a smaller test set): </p>

<ul>
<li>OLS, </li>
<li>best subset selection, </li>
<li>stepwise selection, </li>
<li>ridge regression, </li>
<li>LASSO, </li>
<li>PCR and </li>
<li>PLS. </li>
</ul>

<p>All outliers were removed from both data sets. None of the variables/response have been transformed in any way prior to running the above methods and there is little/no collinearity between variables. </p>

<p>I ran each model (for OLS I ran the entire 30 variable model) and computed the MSE and variance for each. They differ only by 0.001 in MSE (best = PLS, worst = stepwise) and the variance (between the best â€“ stepwise and the worst â€“ ridge). </p>

<p><strong>How do I now choose the best model?</strong> I'm pretty stuck! </p>

<p>One idea I had is to cross validate the MSE and var on the test set, but I'm unsure about how to write this code in R. </p>

<p>I'm using code similar to this <a href=""http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/linearregression/linearregression.R"" rel=""nofollow"">website</a>'s. I'm not sure that will solve the problem though. I'm using <code>summary(model - y.test)^2</code> at the moment. </p>
"
"0.0633215847514023","0.0620651280774201"," 88263","<p>I would like to perform a linear regression. However, the predictor variables are families of variables indexed by time. Let's say the regression problem is:</p>

<p>target ~ x(1)+x(2)+...+x(40)+y(1)+...+y(40)</p>

<p>and imagine the x(t) and y(t) are measurements in 40 successive years. The variables inside the families x(t),y(t) are thus highly collinear.</p>

<p>The usual way to deal with this problem would be variable selection. However, in the problem at hand it is much desired that the coefficients the x(t) resp. y(t) variables are chosen smoothly in time, so setting some of them to 0 is not a good choice.  </p>

<p>My idea was to choose the coefficients for the variable families as the values of penalised splines with knots every 5 years.</p>

<p>Is there a way to do this in R?<br>
Or is there another way of smoothing the coefficients without the use of splines?</p>
"
"0.02831827358943","0.0277563690826684"," 88388","<p>I would like to use cross-validation to test how predictive my mixed-effect logistic regression model is (model run with glmer). Is there an easy way to do this using a package in R? I've only seen cross validation functions in R for use with linear models.</p>
"
"0.0326991257596857","0.0480754414848157"," 88508","<p>I've just run a linear regression on an entire data set, but now I need to run the regression with data just from females within the data. </p>

<p>Females are denoted under the <code>female</code> column of the data set by a <code>1</code>. Males are denoted by a <code>0</code> under the same column. I don't know how to remove the male data so I can run the regression on female data only.</p>
"
"NaN","NaN"," 88524","<p>I am very inexperienced with R and have only a limited background with Excel but have some data that I need to run a multiple non-linear regression with. What is the best way to do this? With R or Excel? What commands would I use?</p>
"
"0.02831827358943","0.0277563690826684"," 89033","<p>I have fitted a zero-inflated model with a random effect using a negative binomial distribution in R, using the function glmmadmb. This is due to a large number of zeros and over dispersion. </p>

<p>For a standard poisson regression I know that one must test collinearity, leverage and stability of coefficients (DF Beta). I can do all this in R using various functions for a poisson regression, but none exist for glmmadmb that I can find. I wondered if anyone knew a way to test these? </p>

<p>Thanks</p>
"
"NaN","NaN"," 89172","<p>I understand the concept of scaling the data matrix to use in a linear regression model. For example, in R you could use: </p>

<pre><code>scaled.data &lt;- scale(data, scale=TRUE)
</code></pre>

<p>My only question is, for new observations for which I want to predict the output values, how are they correctly scaled? Would it be, <code>scaled.new &lt;- (new - mean(data)) / std(data)</code>?</p>
"
"0.106193525960362","0.0971472917893396"," 89760","<p>I am trying to use R to find the optimal solution for my problem with positive coefficients. Here are my data:  </p>

<pre><code>      th inp      tcyc        tinst     tmem      tcom
  1   2   2  26219765385  1975872868  52449810   782964
  2   2   4  38080459431  3155342008  76744867  1878903
  3   2   8  64572439641  6230494010 137754355  4351706
  4   2  16 140168021516 13757989992 285524252 10605705
  5   2  32 308925389816 31497131498 628391048 26040711
  6   4   2  13206650786   988226883  25631315   844126
  7   4   4  19078145632  1577873809  37085281  2125333
  8   4   8  33742095874  3114415906  65962626  5222236
  9   4  16  70956149286  6881357755 134957687 12180392
  10  4  32 153411672670 15754506070 296548768 31057252
  11  8   2   6572843040   494094967  12380740   808816
  12  8   4   9452222628   788984621  17538152  2034061
  13  8   8  16765943294  1557329849  30549900  5016827
  14  8  16  34677550217  3440679505  61614420 12493699
  15  8  32  74852648112  7876116794 133525620 29824686
  16 16   2   3252373719   247026385   5958559   672396
  17 16   4   4669800482   394452497   8097991  1676579
  18 16   8   8269859136   778889584  13651458  4196829
  19 16  16  16353025378  1720301596  26775255 10393194
  20 16  32  37113657641  3938965759  55505822 25011009
  21 32   2   1630888153   123512114   2683400   461526
  22 32   4   2293598746   197173135   3682504  1213596
  23 32   8   4045995970   389408822   5858031  3055324
  24 32  16   8217603991   860041282  10973460  7502244
  25 32  32  17978101850  1969647650  22909347 17953100
  26 48   2   1064344042    82295143   1822133   381178
  27 48   4   1523091067   131488491   2331228   949354
  28 48   8   2677097592   259536252   3552229  2381626
  29 48  16   5400541381   573140686   6489032  5875310
  30 48  32  11837404077  1313066425  13318331 13968230
</code></pre>

<p>I use linear regression in R, <code>s &lt;- lm(tcyc ~ 0+tinst+tmem+tcom, data=fit)</code>, to get the optimal value with intercept 0. But I get negative coefficients which does not make any sense.</p>

<pre><code>coef(s)

 tinst      tmem      tcom 
20.8745 -281.2288 -320.7204 
</code></pre>

<p>I am not sure whether is it the best way to model and find the optimal parameter for <code>tinst</code>, <code>tmem</code> and <code>tcom</code>. How do you find positive coefficients for the model?</p>

<p>Further explaining this problem in Detail:::</p>

<p>Background:
Trying to predict the execution time of an application in the future many-core systems empirically by learning the application behavior. As it is a multithreaded program, it will have communication contnention bottleneck if the application demands high inter-core communication. The general system equation looks like</p>

<p>Total executiong time cycles (T_cyc) = Total cycles spent in Instruction (T_inst) + Total cycle spent in Memory instructions (T_mem) + Total cycle spent in Communication (T_com)</p>

<p>i,e T_cyc=T_inst+T_mem+T_com.</p>

<p>If I use a simulator I can get the T_inst,T_mem and T_com directly and find out the independent contribution of each component to the T_cyc. But using a hardware, I can only get the counts or number of events. Ie, N_inst, N_mem and N_com. 
So what I have is </p>

<p>T_cyc= a* N_inst + b* N_mem + c* N_com</p>

<p>Where a,b,c has to be determined.</p>

<p>I tried solving the problem using lsqnonneg (non-negative least square method)  in MATLAB to find the a,b,c. At times from the data I get b and c value ZERO which is totally meaningless.</p>

<p>Things to notice:
N_inst is a very high value. N_mem and N_com are bit lower in magnitude and hence I face this problem of b and c results as ZERO. </p>

<p>Questions:
1.  Is this a proper tool to solve such a linear equation system? If not, what else should I try?
2.  Is it a problem due to the sample size fed to the solver?
3.  I see that for most applications trend of N_cyc, N_inst,N_mem are monotonic but N_com is non-monotonic and can it affect the solved values? If so, how to isolate this component and find its contribution individually?</p>
"
"0.0400480865731637","0.039253433598943"," 89874","<p>I have a hypothetical data given below that consists of 11 pairs of points (xi, yi ), to which the simple linear regression mean function $\mathbb E(y|x) = Î²_0 + Î²_1x$ is fit.:</p>

<pre><code> X     Y
 10    8.04
  8    6.95
 13    7.58
  9    8.81
 11    8.33
 14    9.96
  6    7.24
  4    4.26
 12    10.84
  7    4.82
  5    5.68
</code></pre>

<p>I have got intercept parameter,$\beta_0=3.001$</p>

<p>But the plot of the data is not showing the y-intercept is $3.001$. Rather the y-intercept is more than $3.001$. <strong>WHY?</strong></p>

<p><img src=""http://i.stack.imgur.com/HtTTU.png"" alt=""enter image description here""></p>

<p>I have used <code>R software</code> to calculate the parameters, $\beta_0$,$\beta_1$ and also to produce the plot.</p>

<pre><code> x1 &lt;- c(10,8,13,9,11,14,6,4,12,7,5)
 y1 &lt;- c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68)

 lm(y1~x1)

 plot(y1~x1)
 abline(lm(y1~x1))
</code></pre>

<p><strong>EDIT</strong></p>

<pre><code>  ht &lt;- c(169.6,166.8,157.1,181.1,158.4,165.6,166.7,156.5,168.1,165.3)
  wt &lt;- c(71.2,58.2,56.0,64.5,53.0,52.4,56.8,49.2,55.6,77.8)

  lm(wt~ht)

  windows(9,6)
  par(mfrow=c(1,2))

  plot(wt~ht)
  abline(lm(wt~ht))

  plot(wt~ht,xlim=c(0,180),ylim=c(0,75))
  abline(lm(wt~ht))
</code></pre>

<p><img src=""http://i.stack.imgur.com/i1bqw.png"" alt=""enter image description here""></p>

<p>How can i get the y-intercept? By expanding the straight line(population regression line) to negative axis of Y ?</p>
"
"0.0490486886395286","0.0480754414848157"," 89886","<p>I'm trying to produce a linear regression model, but I only have 25 observations and 34 predictors.</p>

<p>I'm trying feature selection,</p>

<pre><code>library(MASS)
full.m &lt;- lm(fmla, data=mydata)
fsel.m &lt;- step(full.m, direction = ""both"")
</code></pre>

<p>but I get this error,</p>

<pre><code>Error in step(full.m, direction = ""both"") : 
  AIC is -infinity for this model, so 'step' cannot proceed
</code></pre>

<p>Well, then I tried PCA and factor analysis but I get this weird image from PCA.</p>

<pre><code>r &lt;- prcomp(formula=pca.fmla, mydata, scale=FALSE)
</code></pre>

<p><img src=""http://i.stack.imgur.com/gg45b.png"" alt=""enter image description here""></p>

<p>In factor analysis,</p>

<pre><code>f &lt;- factanal(x=pca.fmla, mydata, factors=2)
</code></pre>

<p>I get,</p>

<pre><code>Error in solve.default(cv) : 
  system is computationally singular: reciprocal condition number = 1.09137e-18
</code></pre>

<p>Any help please?</p>

<p>EDIT: I can answer this.</p>
"
"0.10236445520486","0.100333291527804"," 89897","<p>I'm trying to improve my statistics knowledge using football(soccer) results. So, this is a self study problem. You don't have to provide a complete solution. Pointing me in the right direction is sufficient as well.</p>

<p>I have a book where the author tries to show how a current season home edge can be estimated using information from earlier seasons. He writes ""In order to incorporate both aspects we have treated the home edge of the last 10 years as a regression problem."" And continues by presenting this formula:</p>

<p>$$
HE_{est} = c_1HE_{last3} + c_2HE_{current}
$$</p>

<p>$HE_{current}$ is the current season home edge which is updated using information from the last three seasons ($HE_{last3}$) to arrive at a home edge estimate $HE_{est}$.</p>

<p>He then presents the following table where $N$ is the number of match days:</p>

<pre><code>N | 0 | 6 | 11| 17| 22| 27| 33|
c1|.89|.80|.67|.62|.61|.53|.52|
c2| 0 |.11|.24|.29|.31|.40|.42|
</code></pre>

<p>Here's what I understand he is doing:
First of all, this is a multivariate linear regression problem with $c_1$ and $c_2$ being the coefficients. Since the formula is probably an attempt at predicting the season home edge from running season information, I assume that $HE_{est}$, the dependent variable, is actually the future home advantage. So, I figure, he calculated the home edge of each season of the last ten years and the home edge of the preceding seasons. Finally, he also calculated the home edge at each $N$ for each season (e.g. $HE$ at $N=6$, at $N=11$ and so on)  and estimated the coefficients $c_1$ and $c_2$ using the linear model above.</p>

<p>I tried to do this with R myself and I couldn't reproduce the numbers in the table above. My data looks like this:</p>

<pre><code>data &lt;- matrix(c(.15,.46,.46,.5,.02,.38,.21,.4,.43,.26,.43,.18,.57,.54,.16,.42,.20,.49,.47,.18,.43,.28,.47,.45,.14,.41,.19,.51,.46,.2,.4,.19,.53,.48,.2,.51,.49,.34,.37,.4),5,8)

colnames(data) &lt;-c(""HE.6"",""HE.11"",""HE.17"",""HE.22"",""HE.27"",""HE.33"",""HE.curr"",""HE.last3"")
</code></pre>

<p>This is only the information of 5 years. However, it's the same years he used for his data set, just the first five years are missing. So, the resulting coefficients should be good approximations of what he calculated. However, they are not. If I do e.g.:</p>

<pre><code>lm(HE.curr ~ HE.last3 + HE.17,as.data.frame(data))
</code></pre>

<p>I get:</p>

<pre><code>Coefficients:
(Intercept)         HE.17     HE.last3  
    0.09440      0.79265     -0.07591 
</code></pre>

<p>This result is incorrect. <code>HE.17</code> is $HE_{current}$ at $N=17$. The coefficient $c_2$ should be 0.29 but according to this linear model it's 0.79. Since my results are wrong and he doesn't report any intercept (except maybe $c_1=0.89$ at $N=0$ which I'm not sure about), I get the impression, I'm using the wrong model here. What am I missing?</p>
"
"0.0899225958391357","0.0801257358080262"," 89930","<p>I am attempting to construct a contrast matrix that I can run in R, using the limma bioconductor package, but I am not sure that I have coded the contrast matrix correctly. A previous <a href=""https://stats.stackexchange.com/questions/64249/creating-contrast-matrix-for-linear-regression-in-r?newreg=add2674ca9d04b7eb85fad255b45b7f5"">post</a> and the limma guide were helpful, but my two factorial design is more complicated than what is illustrated there.</p>

<p>The first factor is the treatment, with two levels (control=c and stress=s), and the second factor is the genotype, with five levels (g1, g2, g3, g4, g5). Each genotype/treatment consists of 3-biological replicates (30xsamples total). My dataset has already been normalized and log2 transformed. It consists of 1208 proteins (based upon spectral counting for those that care) that measures protein abundance differences in the five genotypes and two treatments. The dataset is complete, meaning each sample/condition has a datapoint.</p>

<h2>Subset of the data:</h2>

<pre><code>proteinID   g1.s1   g1.s2   g1.s3   g1.c1   g1.c2   g1.c3   g2.s1   g2.s2   g2.s3   g2.c1   g2.c2   g2.c3   g3.s1   g3.s2   g3.s3   g3.c1   g3.c2   g3.c3   g4.s1   g4.s2   g4.s3   g4.c1   g4.c2   g4.c3   g5.s1   g5.s2   g5.s3   g5.c1   g5.c2   g5.c3
prot1   -9.70583694 -9.940059478    -9.764489183    -9.691937821    -9.547306096    -9.668928704    -9.821333234    -10.00376839    -9.843380585    -10.0789111 -9.958506961    -9.791583706    -10.04996359    -10.10279896    -10.0689715 -9.989303332    -10.05414639    -10.00619809    -9.907032795    -10.09700113    -10.00902876    -10.05603575    -10.26218387    -10.15527373    -9.88009858 -9.748974338    -9.730010667    -9.899956956    -9.773955101    -9.957684691
prot2   -9.810354967    -9.844319231    -9.896748977    -9.777040294    -9.821308434    -9.906798728    -9.832236541    -9.876359355    -9.935535795    -10.05991278    -9.831098077    -9.789738587    -10.08470861    -10.18515166    -10.10371621    -10.01971224    -9.977142493    -10.09055782    -9.739831978    -9.586647999    -9.949407778    -9.800183583    -9.83900565 -9.943521592    -9.99229056 -9.744850134    -9.794814509    -9.98542989 -9.766324886    -9.95430439
prot3   -11.70842601    -11.72521838    -11.90389475    -11.98273998    -11.915401  -11.88620205    -11.91603643    -11.96029519    -12.14926486    -12.23846499    -12.26650985    -11.84300821    -12.64562082    -12.41471031    -12.66462278    -12.577619  -12.90001898    -12.31577711    -11.66323243    -11.50283992    -11.4844068 -11.60402491    -11.95270942    -11.68245512    -12.32380181    -12.24294758    -12.23990879    -12.21563403    -12.33730369    -12.437377
prot4   -10.88942769    -11.16906693    -11.13942576    -11.31332257    -11.04718433    -11.11811122    -11.17687812    -11.12503828    -10.9724186 -11.16837945    -11.19642214    -10.96468249    -11.3975887 -11.28808753    -11.32778647    -11.34124725    -11.30972182    -11.29564372    -10.74370929    -10.92223539    -10.97733154    -11.40528844    -11.1238659 -11.15938598    -11.24937805    -10.8691392 -11.12478375    -10.75566728    -10.99485703    -11.09493115
prot5   -10.0102959 -9.936796529    -9.964629149    -9.842835973    -9.791578592    -9.773380518    -9.72290866 -9.715837804    -9.79028651 -9.951486129    -9.636225505    -9.820715987    -10.41899204    -10.25269382    -10.26949484    -10.02644184    -10.13120897    -10.20756299    -9.752087376    -9.687001368    -10.07111473    -9.815279198    -9.995624174    -9.993526894    -9.722360141    -9.551502595    -9.551929198    -9.724500546    -9.502769792    -9.65324573
prot6   -10.34051005    -10.27571947    -10.14968761    -10.17419023    -10.47812301    -10.11019796    -10.40447672    -10.15885481    -10.22900798    -10.26612428    -10.21920493    -10.17186677    -10.66125689    -10.95438025    -10.63751536    -10.65825783    -10.60857688    -10.78516027    -10.33890785    -10.49726978    -10.47100414    -10.64742463    -10.78932619    -10.5318634 -10.26494688    -9.975182247    -10.24870036    -10.2356165 -10.26689552    -10.13061368
prot7   -10.24930429    -10.37307132    -10.03573128    -10.29985129    -9.991216794    -10.05854902    -10.1958704 -10.30549818    -10.2078462 -10.28795766    -10.23314344    -10.23897922    -9.997472306    -10.27461285    -10.20805608    -10.06261332    -10.24876706    -10.12643737    -9.906088449    -10.07316322    -10.23545822    -10.30970717    -10.40745591    -10.36432166    -10.22423532    -10.25703553    -10.44925268    -9.902554721    -9.891163766    -10.0695915
prot8   -10.98782595    -10.84184533    -10.76496107    -10.68290092    -10.55763113    -10.91736394    -10.87505278    -10.76474268    -10.58319007    -10.87547281    -10.71948079    -10.95011831    -10.99753277    -11.061728  -10.8852958 -10.86371208    -10.96638746    -11.24112703    -10.46809937    -10.78446288    -10.71240489    -10.80931259    -10.6598091 -10.54801115    -10.70612733    -10.7339808 -10.8184854 -10.53370359    -10.47323989    -10.62675183
prot9   -8.83857166 -8.736344638    -8.743339515    -8.8152675  -8.743086044    -8.719612156    -8.898093257    -8.902781886    -9.071574958    -8.945970659    -8.862394746    -8.825061244    -8.82313363 -9.161452294    -8.905846232    -8.940119002    -9.024995852    -8.943721201    -8.768488159    -8.802155458    -8.721187011    -8.84850416 -8.931513624    -8.86743278 -8.856904592    -8.675257846    -8.900833162    -8.676117406    -8.758661701    -8.925717389
prot10  -10.65297508    -10.74532307    -10.65940071    -10.36671791    -10.50431649    -10.54915637    -11.07154003    -10.79884265    -10.97164196    -11.1201714 -11.14821342    -10.9254445 -10.92875918    -10.90806369    -10.77581175    -11.2324716 -11.31360896    -11.01070959    -11.04450945    -10.89694291    -10.76865867    -10.92983387    -11.07365287    -11.43888216    -11.14948441    -10.69611194    -10.85827316    -10.64470128    -10.79046792    -10.86048168
</code></pre>

<h2>Code that I am attempting to utilize:</h2>

<pre><code>proteins.mat &lt;- as.matrix(proteins.df)
treat = c(""g1.s"",""g1.c"",""g2.s"",""g2.c"",""g3.s"",""g3.c"",""g4.s"",""g4.c"",""g5.s"",""g5.c"")
factors = gl(10,3,labels=treat)
design &lt;- model.matrix(~0+factors)
colnames(design) &lt;- treat
</code></pre>

<h2>Here is the design for my model:</h2>

<pre><code>&gt; design
   g1.s g1.c g2.s g2.c g3.s g3.c g4.s g4.c g5.s g5.c
1     1    0    0    0    0    0    0    0    0    0
2     1    0    0    0    0    0    0    0    0    0
3     1    0    0    0    0    0    0    0    0    0
4     0    1    0    0    0    0    0    0    0    0
5     0    1    0    0    0    0    0    0    0    0
6     0    1    0    0    0    0    0    0    0    0
7     0    0    1    0    0    0    0    0    0    0
8     0    0    1    0    0    0    0    0    0    0
9     0    0    1    0    0    0    0    0    0    0
10    0    0    0    1    0    0    0    0    0    0
11    0    0    0    1    0    0    0    0    0    0
12    0    0    0    1    0    0    0    0    0    0
13    0    0    0    0    1    0    0    0    0    0
14    0    0    0    0    1    0    0    0    0    0
15    0    0    0    0    1    0    0    0    0    0
16    0    0    0    0    0    1    0    0    0    0
17    0    0    0    0    0    1    0    0    0    0
18    0    0    0    0    0    1    0    0    0    0
19    0    0    0    0    0    0    1    0    0    0
20    0    0    0    0    0    0    1    0    0    0
21    0    0    0    0    0    0    1    0    0    0
22    0    0    0    0    0    0    0    1    0    0
23    0    0    0    0    0    0    0    1    0    0
24    0    0    0    0    0    0    0    1    0    0
25    0    0    0    0    0    0    0    0    1    0
26    0    0    0    0    0    0    0    0    1    0
27    0    0    0    0    0    0    0    0    1    0
28    0    0    0    0    0    0    0    0    0    1
29    0    0    0    0    0    0    0    0    0    1
30    0    0    0    0    0    0    0    0    0    1
attr(,""assign"")
[1] 1 1 1 1 1 1 1 1 1 1
attr(,""contrasts"")
attr(,""contrasts"")$factors
[1] ""contr.treatment""
</code></pre>

<h2>My contrast model. I want to test for interaction, differences between genotypes, and to see if specific genotypes respond differently to the treatment from one another:</h2>

<pre><code>cmtx &lt;- makeContrasts(
  GenotypevsTreatment=(g1.s-g1.c)-(g2.s-g2.c)-(g3.s-g3.c)-(g4.s-g4.c)-(g5.s-g5.c),
  genotype=(g1.s+g1.c)-(g2.s+g2.c)-(g3.s+g3.c)-(g4.s+g4.c)-(g5.s+g5.c),
  Treatment=(g1.s+g2.s+g3.s+g4.s+g5.s)-(g1.c+g2.c+g3.c+g4.c+g5.c),
  levels=design)
</code></pre>

<h2>What my contrast model looks like, but I don't think this is correct:</h2>

<pre><code>&gt; cmtx
      Contrasts
Levels GenotypevsTreatment Genotype Treatment
  g1.s                   1        1         1
  g1.c                  -1        1        -1
  g2.s                  -1       -1         1
  g2.c                   1       -1        -1
  g3.s                  -1       -1         1
  g3.c                   1       -1        -1
  g4.s                  -1       -1         1
  g4.c                   1       -1        -1
  g5.s                  -1       -1         1
  g5.c                   1       -1        -1
</code></pre>

<h2>Fitting the linear model by empirical bayes statistics for differential expression:</h2>

<pre><code>fit &lt;- eBayes(contrasts.fit(lmFit(proteins.mat, design), cmtx))
topTable(fit, adjust.method=""BH"")
</code></pre>

<h2>The below topTable proteins are the same as the subset of data from above:</h2>

<pre><code>&gt; topTable(fit, adjust.method=""BH"")
       GenotypevsTreatment Genotype    Treatment    AveExpr        F      P.Value    adj.P.Val
prot1        -0.40786338 60.30918  0.073054723  -9.918822 17308.55 1.124646e-39 1.232079e-36
prot2        -0.09255219 59.60864  0.061701713  -9.897968 15801.43 3.304533e-39 1.232079e-36
prot3        -0.23880357 73.48557  0.536672827 -12.090016 15650.65 3.701463e-39 1.232079e-36
prot4        -0.11834000 66.76931  0.305471823 -11.122034 15522.46 4.079731e-39 1.232079e-36
prot5        -0.15210172 59.21509 -0.183849274  -9.876144 14734.51 7.556112e-39 1.423908e-36
prot6        -0.15761118 62.87467  0.155340561 -10.389362 14565.87 8.658504e-39 1.423908e-36
prot7        -0.03886438 61.15652 -0.166795475 -10.182834 14551.88 8.757515e-39 1.423908e-36
prot8        -0.10425341 64.63523 -0.186904167 -10.780359 14461.18 9.429854e-39 1.423908e-36
prot9        -0.03426380 53.48057  0.007403722  -8.854471 13713.49 1.767090e-38 2.021378e-36
prot10       -0.75250251 66.62646  0.327497120 -10.894506 13480.51 2.164184e-38 2.021378e-36
</code></pre>

<p>Aside from thinking that I didnâ€™t do this correctly, the result for Genotype looks incorrect to me. Any input would be much appreciated.</p>
"
"NaN","NaN"," 90104","<p>Given a linear regression model with all the assumptions checked and validated, I would like to obtain the probability that $Y&gt;y|X=x$. For example for the iris dataset, I would do the following to obtain the probability of $Y&gt;5|X=1,2,3...7$:</p>

<pre><code>plot(Sepal.Length~Petal.Length, data=iris)
lm1&lt;-lm(Sepal.Length~Petal.Length, data=iris)
summary(lm1)
abline(lm1)
predict(lm1, newdata=data.frame(Petal.Length=1:7))
(summary(lm1))$sigma
    pnorm(5, mean = predict(lm1, newdata=data.frame(Petal.Length=1:7)),
        sd = (summary(lm1))$sigma, lower.tail = F)
</code></pre>

<p>Is such an approach correct assuming constant variance?</p>
"
"0.0942489115008991","0.0923787802599364"," 90799","<p>I am having trouble training a model for nested data about house prices. Lets say my data looks like following:</p>

<pre><code>  logPrice bedCount bathCount                city
 0.6517920        4       2-3        Redwood City
 0.4402192        1       1-2 South San Francisco
 0.5922396        2       1-2           San Mateo
 0.4606918        3       1-2 South San Francisco
 0.7592523    5plus       3-4           San Mateo
 0.4710397        1       1-2        Redwood City
</code></pre>

<p><code>bedCount</code>, <code>bathCount</code> and <code>city</code> are factors.</p>

<p>As a baseline, I trained a simple linear model ignoreing nested structure of the data (houses are nested within cities).</p>

<pre><code>lm.model = lm(formula = logPrice ~ 1 + bedCount + bathCount + city)
</code></pre>

<p>which corresponds to following assumption:</p>

<p><code>logPrice</code>$_i = \beta_0 + \beta_1\cdot$ <code>bedCount</code>$_i + \beta_2\cdot$ <code>bathCount</code>$_i + \beta_{3,j[i]}\cdot I$(<code>city</code>$_{j[i]}) + \epsilon_i$</p>

<p>where </p>

<p>$\epsilon_i \sim N(0, \sigma^2_{logPrice})$ and $I$(<code>city</code>$_{j[i]}$) is the indicator function for city of the $i^{th}$ house (which is 1).</p>

<p>Now, I trained a 2-level hierarchical model:</p>

<pre><code>lmer.model = lmer(formula = logPrice ~ 1 + bedCount + bathCount + (1 | city))
</code></pre>

<p>which corresponds to the following assumption:</p>

<p><code>logPrice</code>$_i = \beta_0 + \beta_1\cdot$ <code>bedCount</code>$_i + \beta_2\cdot$ <code>bathCount</code>$_i + \beta_{3,j[i]}\cdot I$(<code>city</code>$_{j[i]}) + \epsilon_i$</p>

<p>where
$\epsilon_i \sim N(0, \sigma^2_{logPrice})$ and $\beta_{3,j} \sim N(0, \sigma^2_{\beta_3})$</p>

<p>Now, on the training data, <code>lm.model</code> gives me lesser average RMSE than <code>lmer.model</code> which shouldn't happen because linear regression is a special case of multilevel linear regression (I didn't care to check average RMSE on test data because that on training data itself should be lower for 2nd model than that for 1st model). In fact, my data has multiple levels (houses nested within subdivisions, which are nested within zipcodes, which in turn are nested within cities) and the performance gets worse as I add more and more levels to the model (i.e. model with random effect <code>(1 | subdivision)</code> does worse than that with random effect <code>(1 | zipcode) + (1 | zipcode:subdivision)</code>, which in turn does worse than a model with random effect <code>(1 | city) + (1 | city:zipcode) + (1 | city:zipcode:subdivision)</code>).</p>

<p>What am I missing?</p>
"
"0.100120216432909","0.0981335839973574"," 91243","<p>I have a large data frame in the following form (I apologize for this formatting):</p>

<pre><code>Site    Season  T          SC    pH    Chl   DO.S   DO      BGA  Tur    fDOM    Flow    Rainfall    Solar      Rain
300N    Winter  14.05   1692.77 7.93    NA  82.26   8.42    NA  9.25    NA      NA      0.00          219.18     no
</code></pre>

<p>If you can't understand the formatting, there are 12 numerical factors, and 3 categorical factors (<code>Site</code>, <code>Season</code>, <code>Rain</code> [yes/no]). Each row represents the average daily values that I have calculated from 15-minute time series. I have spent a good amount of time doing data exploration (linear regression analysis, looking at time series plots for patterns), but haven't found a method that works for me yet. I have also worked with <code>corrplot</code>, correlation matrices, and covariance functions in an arduous way, where I subset each categorical combination and found <code>corrplot</code>s for each (I have also tried it with <code>ddply</code>, but the resulting format is not in the correlation matrix format that is easy to plot). I have also attempted PCA on the data to little avail.</p>

<p>My question is first and foremost, does anyone have an idea for data visualization of this kind of dataset? The main question I am after is, ""What are the factors that influence <code>DO</code> (dissolved oxygen)?"". How does this change by location (<code>Site</code>), <code>Season</code>, and with the influence of <code>Rain</code>. I would really like a quick method for shooting out correlation matrices (or heat maps; I have tried both) for each categorical subset. I tried this with <code>ggplot</code> and <code>facet_wrap</code>, but it wasn't happening for me. I also tried <code>ggpairs</code> from the GGally package, but honestly didn't spend too much time with that method.</p>

<p>I was starting to get into the idea of star graphs (on polar coordinates), which can be used to visualize repeating periodicity in time series, but am running out of time and decided to seek the advisement of Stack Overflow. I really appreciate any advice or thoughts on visualizing this data that come to your mind. I feel like some combination of <code>ddply</code> and graphing is what I need, but I haven't gotten there yet.
Thank you for your time.</p>

<p>EDIT:
<code>dput</code> of the data frame in question:</p>

<pre><code>structure(list(Site = structure(c(2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""2100S"", 
""300N"", ""3300S"", ""800S"", ""Burnham"", ""Center""), class = ""factor""), 
    Season = structure(c(4L, 4L, 4L, 4L, 2L, 2L), .Label = c(""Fall"", 
    ""Spring"", ""Summer"", ""Winter""), class = ""factor""), T = c(14.05, 
    14.18, 14.5, 14.58, 14.07, 11.91), SC = c(1692.77, 1671.31, 
    1680.71, 1661.79, 1549.56, 1039.63), pH = c(7.93, 7.92, 7.96, 
    7.95, 7.93, 7.79), Chl = c(NA_real_, NA_real_, NA_real_, 
    NA_real_, NA_real_, NA_real_), DO.S = c(82.26, 78.79, 82.05, 
    80.92, 74.33, 73.96), DO = c(8.42, 8.04, 8.31, 8.18, 7.61, 
    7.97), BGA = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, 
    NA_real_), Tur = c(9.25, 9.77, 9.41, 10.6, 40.38, 50.25), 
    fDOM = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, 
    NA_real_), Flow = c(NA, 178.08, 178.53, 188.13, 306.15, 382.22
    ), Rainfall = c(0, 0, 0, 0, 0.01, 0.81), Solar = c(219.18, 
    228.33, 244.3, 247.69, 105.15, 220.73), Rain = structure(c(1L, 
    1L, 1L, 1L, 2L, 2L), .Label = c(""no"", ""yes""), class = ""factor"")), .Names = c(""Site"", 
""Season"", ""T"", ""SC"", ""pH"", ""Chl"", ""DO.S"", ""DO"", ""BGA"", ""Tur"", 
""fDOM"", ""Flow"", ""Rainfall"", ""Solar"", ""Rain""), row.names = c(NA, 
6L), class = ""data.frame"")
</code></pre>
"
"0.02831827358943","0.0277563690826684"," 91386","<p>Can anyone expalin to me in simple terms what happens when we use weights in <code>regsubsets</code> or <code>lm</code> in R? What effect do weights have on a linear regression? 
for example : </p>

<pre><code>Model1&lt;-lm(Ozone~Solar.R,data=airquality)
summary(Model1)
#Coefficients:
#            Estimate Std. Error t value Pr(&gt;|t|)    
#(Intercept) 18.59873    6.74790   2.756 0.006856 ** 
#Solar.R      0.12717    0.03278   3.880 0.000179 ***
Model1&lt;-lm(Ozone~Solar.R,data=airquality,weights=(2*seq(nrow(airquality),1,-1)))
summary(Model1)
#Coefficients:
#            Estimate Std. Error t value Pr(&gt;|t|)    
#(Intercept) 18.57106    6.26067   2.966 0.003704 ** 
#Solar.R      0.10824    0.02927   3.699 0.000341 ***
</code></pre>

<p>please explain the changes in intercepts and slope.</p>
"
"0.02831827358943","0.0277563690826684"," 91479","<p>I'm using R to compute robust multiple linear regression.
I use the command <code>rlm</code> from the package MASS.</p>

<p>As psi function I use <code>psi.huber</code> or <code>psi.bisquare</code>.</p>

<p>Is there a way to get an estimator of the goodness of fit of the model? Maybe something comparable to the Adjusted R-squared, for the parametric multiple linear regression?</p>

<p>Moreover, am I right saying that this kind of robust regression doesn't solve the problem of non-normality of the dependent or independent variables?</p>
"
"0.0853828074607","0.0836886016271203"," 91592","<p>Can anyone point me towards a good explanation of when a residualized variable in a regression will give you the same answer as using a non-residualized variable with controls? </p>

<p>For instance, say I want to know the effect of a variable x on y and need to control for a and b. In a classic linear model framework I can either add a and b as covariates (i.e., control variables) to the model of y on x, or I can first regress x on a and b, and then use the residuals from this regression (the residualized x) to predict y. Both will give the same coefficient for x.</p>

<p>This works in the linear model case, but does a residualized x give the same coefficient as x with controls for other types of models, e.g., logit models or poisson models? My own simple simulations suggest they do not (see R code below), but I am trying to understand why, and if residualization can ever be used in place of adding controls outside of the linear model framework. Can anyone point me towards a good explanation?</p>

<pre><code>#generate the data
n=10000
set.seed(3345)
a=rnorm(n); b=rnorm(n)
x = .4*a + .4*b*b + rnorm(n)
y = .5*x + .3*a + .3*b*b + rnorm(n)

## LINEAR MODEL ####
#a model with controls gets the right coefficient
summary(lm(y ~ x + a + I(b^2)))
residmod=lm(x ~ a + I(b^2))
x.resid=resid(residmod)
#using a residualized variable gets the same coefficient
summary(lm(y ~ x.resid))

## LOGIT MODEL ####
y=.5*x + .3*a + .3*b*b + rlogis(n)
ydichot=ifelse(y &gt;0, 1, 0)
#a model with controls gets the right coefficient
summary(glm(ydichot ~ x + a + I(b^2), family=binomial))
#using a residualized variable does NOT get the same coefficient
summary(glm(ydichot ~ x.resid, family=binomial))

## POISSON MODEL ####
mu=exp(.5*x + .3*a + .3*b*b)
ycount=rpois(n, mu)
summary(glm(ycount ~ x + a + I(b^2), family=poisson))
#using a residualized variable does NOT get the same coefficient
summary(glm(ycount ~ x.resid, family=poisson))
</code></pre>
"
"0.0633215847514023","0.0496521024619361"," 91700","<p>I am trying to understand the steps behind the linear regression process. I already have a linear model like:</p>

<p><code>lmodel1 &lt;- lm(y~x1+x2+x3, data=dataset)</code></p>

<p>for which R calculates several different things (<code>Coefficients, Intercept, Residuals, F-statistic</code> and <code>p-value</code>) among  others.</p>

<p>At this point, i am mostly interested in <code>F-statistic</code> and <code>p-value</code>.
So far, i have concluded to the following:</p>

<p>The process is iterative and begins taking under consideration every variable. In order to achieve an optimal <code>y</code> some <code>x</code> variables have to be ""taken out"". This comes as a result of calculating F-statistic, which quantifies the importance of each <code>xi</code> and the dependent variable <code>y</code>.
When <code>F value</code> is smaller than <code>p-value</code>(?) that variable is removed.
Next step of the process is to compare that <code>F-statistic</code> of a <code>xi</code> independent variable, with an <code>F-to-enter</code> and <code>F-to-remove</code> in order see if the removed variable will be re-inserted to the equation.(?)</p>

<p>Now, please do correct me if i am wrong regarding the steps desribed above.
Is that what happens under <code>lm()</code>'s hood. Are those the right variables.?</p>

<p>R-wise speaking how does these values can be shown, inserted or calculated in a multiple linear regression model.? How is <code>ANOVA</code> related to the above?</p>

<p>I am afraid R's <code>summary</code> and <code>help</code>  take too much for granted.</p>

<p>Thanks in advance for any suggestions.</p>
"
"0.0991139575630048","0.111025476330674"," 92150","<p>Actually, I thought I had understood what one can show a with partial dependence plot, but using a very simple hypothetical example, I got rather puzzled. In the following chunk of code I generate three independent variables (<em>a</em>, <em>b</em>, <em>c</em>) and one dependent variable (<em>y</em>) with <em>c</em> showing a close linear relationship with <em>y</em>, while <em>a</em> and <em>b</em> are uncorrelated with <em>y</em>. I make a regression analysis with a boosted regression tree using the R package <code>gbm</code>:</p>

<pre><code>a &lt;- runif(100, 1, 100)
b &lt;- runif(100, 1, 100)
c &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
y &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
par(mfrow = c(2,2))
plot(y ~ a); plot(y ~ b); plot(y ~ c)
Data &lt;- data.frame(matrix(c(y, a, b, c), ncol = 4))
names(Data) &lt;- c(""y"", ""a"", ""b"", ""c"")
library(gbm)
gbm.gaus &lt;- gbm(y ~ a + b + c, data = Data, distribution = ""gaussian"")
par(mfrow = c(2,2))
plot(gbm.gaus, i.var = 1)
plot(gbm.gaus, i.var = 2)
plot(gbm.gaus, i.var = 3)
</code></pre>

<p>Not surprisingly, for variables <em>a</em> and <em>b</em> the partial dependence plots yield horizontal lines around the mean of <em>a</em>. What me puzzles is the plot for variable <em>c</em>. I get horizontal lines for the ranges <em>c</em> &lt; 40 and <em>c</em> > 60 and the y-axis is restricted to values close to the mean of <em>y</em>. Since <em>a</em> and <em>b</em> are completely unrelated to <em>y</em> (and thus there variable importance in the model is 0), I expected that <em>c</em> would show partial dependence along its entire range instead of that sigmoid shape for a very restricted range of its values. I tried to find information in Friedman (2001) ""Greedy function approximation: a gradient boosting machine"" and in Hastie et al. (2011) ""Elements of Statistical Learning"", but my mathematical skills are too low to understand all the equations and formulae therein. Thus my question: What determines the shape of the partial dependence plot for variable <em>c</em>? (Please explain in words comprehensible to a non-mathematician!)     </p>

<p>ADDED on 17th April 2014:</p>

<p>While waiting for a response, I used the same example data for an analysis with R-package <code>randomForest</code>. The partial dependence plots of randomForest resemble much more to what I expected from the gbm plots: the partial dependence of explanatory variables <em>a</em> and <em>b</em> vary randomly and closely around 50, while explanatory variable <em>c</em> shows partial dependence over its entire range (and over almost the entire range of <em>y</em>). What could be the reasons for these different shapes of the partial dependence plots in <code>gbm</code> and <code>randomForest</code>?</p>

<p><img src=""http://i.stack.imgur.com/PrlC1.jpg"" alt=""partial plots of gbm and randomForest""></p>

<p>Here the modified code that compares the plots:</p>

<pre><code>a &lt;- runif(100, 1, 100)
b &lt;- runif(100, 1, 100)
c &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
y &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
par(mfrow = c(2,2))
plot(y ~ a); plot(y ~ b); plot(y ~ c)
Data &lt;- data.frame(matrix(c(y, a, b, c), ncol = 4))
names(Data) &lt;- c(""y"", ""a"", ""b"", ""c"")

library(gbm)
gbm.gaus &lt;- gbm(y ~ a + b + c, data = Data, distribution = ""gaussian"")

library(randomForest)
rf.model &lt;- randomForest(y ~ a + b + c, data = Data)

x11(height = 8, width = 5)
par(mfrow = c(3,2))
par(oma = c(1,1,4,1))
plot(gbm.gaus, i.var = 1)
partialPlot(rf.model, Data[,2:4], x.var = ""a"")
plot(gbm.gaus, i.var = 2)
partialPlot(rf.model, Data[,2:4], x.var = ""b"")
plot(gbm.gaus, i.var = 3)
partialPlot(rf.model, Data[,2:4], x.var = ""c"")
title(main = ""Boosted regression tree"", outer = TRUE, adj = 0.15)
title(main = ""Random forest"", outer = TRUE, adj = 0.85)
</code></pre>
"
"0.02831827358943","0.0277563690826684"," 92205","<p>I am using <code>randomForest</code> to generate a model, and at the end I don't know how I can get the final coefficients that the model is fitting. I know that for linear regression, you just type <code>summary(lm)</code> where <code>lm</code> is your model, and you get the coefficients.</p>
"
"0.0400480865731637","0.039253433598943"," 92839","<p>I'm currently testing a (binary) logistic regression model, which seems to have at least some issues with multicollinearity. Now I don't really trust the data anymore and would like to also test it on heteroscedasticity. I found some information on Breusch-Pagan Test on the internet, but I could not find an answer to the question if this test also applys on Maximum-Likelihood-Methods, as it is usually mentioned in the context of OLS. So, can I apply the Breusch-Pagan Test on my model?</p>

<p>Related to this question: How could I plot for heteroscedasticity-detection? I found <a href=""http://stats.stackexchange.com/questions/33028/measures-of-residuals-heteroscedasticity"">this thread</a>, but due to the binary nature of my dependent variable, the plot does not really work and unfortunately I'm pretty novice on plotting with R.</p>

<p>Thanks in advance!</p>
"
"0.0980973772790571","0.0961508829696314"," 93392","<p>First of all, sorry i am new about this and any helps are really welcome.</p>

<p>I am reading a reaserch paper where the authors report: <em>Stepwise forward regression (Zar 1996) was used to select the most informative variables, which were included in a multiple (linear) regression model. A 5% significance level was chosen as a threshold for the inclusion of the model variables.</em></p>

<p>with a private email the first author told me that the variable selection was performed using stepAIC of MASS library using direction ""forward"" and they considered only for the final model the variables with a significance level of &lt; 5%.</p>

<p>using junk data i tried to rewrite the analysis in order to understand the procedure</p>

<pre><code>state.x77
st = as.data.frame(state.x77) str(st) colnames(st)[4] = ""Life.Exp""
colnames(st)[6] = ""HS.Grad"" st[,9] = st$Population * 1000 / st$Area colnames(st)[9] = ""Density""
str(st) model1 = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + + HS.Grad + Frost + Area + Density, data=st)
model1.stepAIC &lt;- stepAIC(model1, direction=c(""both""))
summary(model1.stepAIC)


Call:
lm(formula = Life.Exp ~ Population + Murder + HS.Grad + Frost, 
    data = st)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.47095 -0.53464 -0.03701  0.57621  1.50683 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  7.103e+01  9.529e-01  74.542  &lt; 2e-16 ***
Population   5.014e-05  2.512e-05   1.996  0.05201 .  
Murder      -3.001e-01  3.661e-02  -8.199 1.77e-10 ***
HS.Grad      4.658e-02  1.483e-02   3.142  0.00297 ** 
Frost       -5.943e-03  2.421e-03  -2.455  0.01802 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7197 on 45 degrees of freedom
Multiple R-squared:  0.736,     Adjusted R-squared:  0.7126 
F-statistic: 31.37 on 4 and 45 DF,  p-value: 1.696e-12
</code></pre>

<p>followint the protocol of the paper the final model is </p>

<pre><code>Life.Exp ~ Murder + HS.Grad + Frost (final model) 
</code></pre>

<p>because Population is > 0.05.</p>

<p>I wish to know if this final model approach is correct, and then:</p>

<pre><code>fmodel = lm(Life.Exp ~ Murder + HS.Grad + Frost, data=st)
summary(fmodel)

Call:
lm(formula = Life.Exp ~ Murder + HS.Grad + Frost, data = st)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.5015 -0.5391  0.1014  0.5921  1.2268 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 71.036379   0.983262  72.246  &lt; 2e-16 ***
Murder      -0.283065   0.036731  -7.706 8.04e-10 ***
HS.Grad      0.049949   0.015201   3.286  0.00195 ** 
Frost       -0.006912   0.002447  -2.824  0.00699 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7427 on 46 degrees of freedom
Multiple R-squared:  0.7127,    Adjusted R-squared:  0.6939 
F-statistic: 38.03 on 3 and 46 DF,  p-value: 1.634e-12
</code></pre>
"
"0.0578044339088637","0.0566574511374171"," 93423","<p>please help a sociologist struggling to get to grips with R and statistics in general..!</p>

<p>I've got a data set with 11 variables. I need to investigate the possible relationships between one of the variables (percentage of smokers in a population) and the rest of them- things like unemployment rate, education level, and so on. I'm working purely with linear regression.</p>

<p>I'm getting there with correlation, but having looked around at numerous examples on the web, I'm still unsure with regards to method. Apologies if it's completely obvious, but I can't seem to find an answer anywhere.</p>

<p>As I see it, it seems sensible to look for correlation between smoking and all the other variables firstly with scatterplots, and then by calculating the correlation coefficient if there is an identifiable linear relationship (Pearson's r or Spearman, depending on normality). If we then continue and do separate simple linear regressions between smoking rate and our identified variable, that all seems well and good.</p>

<p>But where does multiple regression fit in here? Does it make any sense, statistically speaking, to do simple linear regression first and then multiple linear regression also? Or would it be best to just jump straight from testing correlation to multiple regression?</p>

<p>Any help much appreciated!</p>
"
"0.02831827358943","0.0277563690826684"," 93444","<p>Is it possible to estimate the beta changes corresponding to employee's departmental change or title change or Both.</p>

<p>Can i follow linear regression directly to calculate slope ?</p>

<p>I am using R statistical tool to compute beta's.</p>

<p><img src=""http://i.stack.imgur.com/xMIR3.png"" alt=""enter image description here""></p>
"
"0.0490486886395286","0.0480754414848157"," 94257","<p>I'm working with a biased sample of web users.  I'm only able to track responses of users who have navigated my site in a certain way, and I'd like to run an analysis to determine how certain factors (which products they are into, etc) influence how much they buy from my online store.</p>

<p>I've searched around online and found some information on the Heckman Correction, which somehow both won a Nobel Prize and also has extremely little information online, including no R support or youtube videos that I can find.</p>

<p>What are some ways you work with building models that try to correct for biased data?  Are there easy to use software libraries in R?  I have a ton of unlabeled data (products that every user has seen), so it's possible that I could create a naive model for determining probability that a user ends up in my sample if that helps, and I'd be willing to work with more advanced learners like SVM (which I've read can work better for these types of problems) in case linear regressions are a bad choice.</p>
"
"0.0400480865731637","0.039253433598943"," 95049","<p>Here is some sample data:</p>

<pre><code>structure(list(
y.t2 = c(NaN, 0.05, 0.02, 0.02, 0.02, 0.04, 0.06, 0.05, 0.07, 0.1), 
n.t2 = c(0, 231, 228, 219, 210, 175, 250, 255, 270, 257), 
y.t3 = c(0.07, 0.11, 0.2, 0.19, 0.17, 0.12, 0.18, 0.23, 0.18, NaN), 
n.t3 = c(226, 224, 223, 208, 206, 224, 228, 246, 233, 0), 
x = c(-6, 0, 2.7, -0.1, -2.4, -2.5, -1.5, -0.4, 0.3, -0.3), 
diff = c(NaN, 0.06, 0.18, 0.17, 0.15, 0.08, 0.12, 0.18, 0.11, NaN)), 
.Names = c(""y.t2"", ""n.t2"", ""y.t3"", ""n.t3"", ""x"", ""diff""), 
row.names = c(NA, -10L), class = ""data.frame"")
</code></pre>

<p><code>y</code> was measured at times <code>t2</code> and <code>t3</code>. I used <code>glm</code> like:</p>

<pre><code>summary(glm(y.t2~x,binomial,my.df,n.t2))
summary(glm(y.t3~x,binomial,my.df,n.t3))
</code></pre>

<p>to test the effect of <code>x</code> on <code>y</code> from two time points, <code>t2</code> and <code>t3</code>. But if I want to test the effect of <code>x</code> on the difference, as in, <code>y.t3-y.t2</code>, how do I go about it? Though in my case, I always expect a positive difference (to use <code>glm</code> with logit link), in theory, it can be negative as well, ranging between -1 and 1.</p>

<p>Do I do a linear regression then, or what method would be appropriate in this case? I want to do <code>diff~x</code>.
I am also wondering about the weights argument. Some subjects dropped out and new ones joined between time <code>t2</code> and time <code>t3</code>; the number of cases varies between the two instances. </p>
"
"0.0942489115008991","0.100077011948264"," 95386","<p>What I have is a medical data set with several variables, all 0-1 variables. I want to make inference about them with logistic regression. I have a few problems:</p>

<ol>
<li><p>I have location variables for the disease. I was advised by my statistic advisor to put them in bins as follows: If it was solely in the right part of the organ then I would mark 1 in the column for right and similarily for left. However if it were in both places I marked in neither of the left and right column but marked one in column both. Using this approach I get error in R, numeric 0 1 error when I use glm in R and I think it is due to how these variables are constructed. Shouldn't I rather have just left and right variables and when we have the disease in both sides I should mark in left and right column and skip the both column and maybe introduce interaction term between left and right (that I would at least do in a linear model).</p></li>
<li><p>Using glm (family binomial for logistic regression) in R I was thinking how to find the best model describing some variable. I started with one usual approach with finding univarietly which variables had p-value less than $0.1$ in Fischer exact test. Then I included those variables in my model and started to delete them after which had the highest p-value. In most medical reasearches I have read when applying multivariate regression I see the usage of p-value $0.05$ but I have a feeling that it might be because of lack of understanding of the subject. When I ranked the model according to AIC and explored the best model I usually got variable with p-value around $0.1$. Which approach is preferably, is it justifyable to just cut of at p-value $0.05$ or should use AIC as an estimator of the best multivariate model? AIC does punish for extra variables and so it shouldnt give one too many variables.</p></li>
</ol>
"
"0.0895502439463906","0.0789960112897596"," 95974","<p>This is a follow-up question from this post, here:
<a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">Confidence intervals for predictions from logistic regression</a></p>

<p>The answer from @Gavin is excellent, but I have some additional questions which I think would be useful for others. I am working with a Poisson model, so basically it is the same approach described in the other post, only <code>family=poisson</code> instead of <code>family=binomial</code>.</p>

<p>To my first question:
@Gavin writes:</p>

<pre><code>mod &lt;- glm(y ~ x, data = foo, family = binomial)
preddat &lt;- with(foo, data.frame(x = seq(min(x), max(x), length = 100))
preds &lt;- predict(mod, newdata = preddata, type = ""link"", se.fit = TRUE)
</code></pre>

<p>What is the point of the second line there? Is it necessary to create a data.frame with minimum and maximum of the explanatory variable? Could I not, for some explanatory variable(s) <code>x</code> (stored in some data frame <code>data</code>), just go from the first line and directly to the third?</p>

<p>To my second question:
In the beginning of his answer @Gavin writes:</p>

<blockquote>
  <p>The usual way is to compute a confidence interval on the scale of the
  linear predictor, where things will be more normal (Gaussian) and then
  apply the inverse of the link function to map the confidence interval
  from the linear predictor scale to the response scale.</p>
</blockquote>

<p>Why are ""things"" more normal on the scale of the linear predictor(s)? Is this also the case when I do my Poisson regression?
I assume the reason for using critical value 1.96 when constructing the CI's, is because of the assumptions that ""things"" are normal. Can somebody explain this further?</p>

<p>My third question:</p>

<p>Is there a relationship between the standard deviation which we get by using <code>se.fit=TRUE</code>  in predict() and the standard deviations of the coefficients of the explanatory variables, which we simply get from <code>summary(mod)</code>? (<code>mod</code> is some glm object)</p>
"
"NaN","NaN"," 96727","<p>My Error's plot from a linear regression</p>

<p><img src=""http://i.stack.imgur.com/bprYB.png"" alt=""enter image description here"">.</p>

<p>Is it Normal distributted? If not, why? What that negative big bar means?</p>

<p>The code used is:</p>

<pre><code>  library(MASS)
sresid &lt;- studres(reg3) 
hist(sresid, freq=FALSE, 
     main=""Distribution of Studentized Residuals"")
xfit&lt;-seq(min(sresid),max(sresid),length=40) 
yfit&lt;-dnorm(xfit) 
lines(xfit, yfit)
</code></pre>

<p>Edited:</p>

<p><img src=""http://i.stack.imgur.com/mWffB.png"" alt=""enter image description here""></p>
"
"0.0693653206906364","0.0679889413649005"," 97811","<p>I am using leave-one-out cross-validation to evaluate a linear regression model. In subsequent analysis, I need three specific values for each observation: observed value, predicted value, prediction standard error. Prediction standard error values can be retrieved from function <code>predict.lm</code> setting argument <code>se.fit = TRUE</code>. The following code (adapted from <a href=""http://www.analyticbridge.com/profiles/blogs/cross-validation-in-r-a-do-it-yourself-and-a-black-box-approach"" rel=""nofollow"">here</a>) can be used to do what I currently need:</p>

<pre><code>library(faraway)
gala[1:3, ]
c1 &lt;- c(1:30)
gala2 &lt;- cbind(gala, c1)
gala2[1:3, ]
obs  &lt;- numeric(30)
pred &lt;- numeric(30)
se   &lt;- numeric(30)
for (i in 1:30) {
     model1  &lt;- lm(Species ~ Endemics + Area + Elevation,
                   subset = (c1 != i), data = gala2)
     specpr  &lt;- predict(model1, gala2[i, ], se.fit = TRUE)
     obs[i]  &lt;- gala2[i, 1]
     pred[i] &lt;- specpr$fit
     se[i]   &lt;- specpr$se.fit
}
res &lt;- data.frame(obs, pred, se)
head(res)
  obs       pred       se
1  58  70.185063 5.524249
2  31  72.942732 6.509655
3   3  -8.303608 7.055163
4  25  20.948932 6.998093
5   2 -15.953141 7.403062
6  18  27.274440 6.220029
</code></pre>

<p>I searched through the documentation of some of the packages that offer functions for cross-validation, but did not find any that saves prediction standard errors. Is there any package that already offers such functionality?</p>
"
"0.0566365471788599","0.0555127381653369"," 99626","<p>I'm trying to show a correlation between growth in a petri dish of some fungi and its effect on a plant. I have ten strains of fungi which I tested in the plant and in petri dishes. I can put data from both experiments into linear models (lm) to get estimated means and variances. If I run a regression on the estimated means for each strain I find a significant correlation, but this doesn't take uncertainty in the strain means into account.
So far I've tried a parametric bootstrap, but I'm having a hard time figuring out the ultimate test statistic. I've included the parameter estimates and my R code. At the moment it generates simulated strain means from the estimated parameters. I need a p value for the covariance between growth and virulence. Any help would be greatly appreciated! My specific questions are:
1) Is a parametric boostrap the right way to go about analyzing this?
2) How would one go about doing this in R?</p>

<pre><code>Strain  MeanVirulence MeanGrowth    VirulenceVarGrowthVar  
1  -5.26064 0.066716    0.67834 0.053247  
2   -4.05482    -0.055524   0.68385 0.047111  
3   -5.47282    0.029047    0.68385 0.046739  
4   -3.50632    -0.161811   0.68385 0.047083  
5   -4.94051    -0.224949   0.68385 0.04727  
6   -4.04982    -0.062938   0.68385 0.047647  
7   -4.53178    -0.142985   0.68385 0.04788  
8   -3.01697    -0.199349   0.68385 0.047255  
9   -3.81093    -0.254793   0.68385 0.047255  
10  -1.61882    -0.325289   0.68385 0.0469
</code></pre>

<p>And here is my R code:</p>

<pre><code>gendata&lt;-function(par,npar=TRUE,print=TRUE){
    n     = 10
    k     = 2
    x=matrix(data=NA, nrow=n, ncol=k)
    for(i in 1:n){
      x[i,1] = rnorm(1,mean=par[i,2],sd=par[i,4])
      x[i,2] = rnorm(1,mean=par[i,3],sd=par[i,5])
    }
    return(x)
}

lmp &lt;- function (modelobject) {
    f &lt;- summary(modelobject)$fstatistic
    p &lt;- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) &lt;- NULL
    return(p)
}


samp=20000
rescor=matrix(data=NA, samp)
resvar=matrix(data=NA, samp)
pvals=matrix(data=NA, samp)
numsig = 0
numnotsig = 0
for (i in 1:samp){
    x&lt;-gendata(parameters)
    rescor[i]&lt;-cor(x[,1],x[,2], method = ""pearson"")
    resvar[i]&lt;-var(x[,1])
    a&lt;-lm(x[,1]~x[,2])
    pvals[i]&lt;-lmp(a)
    if (pvals[i] &lt; 0.05){
        numsig = numsig + 1
    }
    if (pvals[i] &gt; 0.05){
        numnotsig = numnotsig + 1
    }    
}

Strain Plate Growth  
1      1     200   
1      2     210  
1      3     190  
2      1     150   
2      2     130  
2      3     140  
...  

Strain Plant Growth  
1      1     70  
1      2     40  
1      3     50  
1      4     45  
2      1     80  
2      2     90  
2      3     85  
2      4     75  
...
</code></pre>
"
"0.0800961731463273","0.078506867197886"," 99862","<p>Here's my situation.  </p>

<p>I have a multiple linear regression which I've used to come up with a prediction interval to predict a value y for a given (x1,x2,x3,x4,x5,x6).   It reads something like lower: 30, upper:48.  </p>

<p>I also have the same exact thing to predict a value y* at another given (x1*,x2*,x3*,x4*,x5*,x6*).  It reads something like lower:35, upper:51. </p>

<p>I want to answer this question:<br>
What is the probability that the value y* is greater than the value y?</p>

<p>I think it's a basic question, but I'm not sure. 
I could likely come up with this probability if I knew the formula for how the prediction interval is calculated in a multi-variable situation.<br>
Here's what I think should be done, but I wanted to run it by you guys first. </p>

<p>Prediction Intervals are based on a t-distribution with (n-6) degrees of freedom (I have a forced 0 y-int).  So I believe the margin of error calculated is then some constant multiplied by the corresponding value from the t-distribution (t_.05/2 with n-6 degrees of freedom).  The ""some constant"" would be the standard error of this particular estimate. </p>

<p>I then just do a basic 2 sample t-test using the point estimate prediction as the means and these constants as the standard errors with my n-6 degrees of freedom.   Is this accurate? </p>

<p>Is there a better way?</p>

<p>Thanks</p>
"
"0.0490486886395286","0.0480754414848157"," 99924","<p>For regression analysis, it is often useful to know the data generating process to check how the used method works. While it is fairly simple to do this for a simple linear regression, this is not the case when the dependent variable has to follow a specific distribution. </p>

<p>Consider a simple linear regression:</p>

<pre><code>N    &lt;- 100
x    &lt;- rnorm(N)
beta &lt;- 3 + 0.4*rnorm(N)
y    &lt;- 1 + x * beta + .75*rnorm(N)
</code></pre>

<p>Is there any way to use the same approach but to get <code>y</code> be other than normal, say left skewed?</p>
"
"0.02831827358943","0.0277563690826684"," 99981","<p>I am doing bivariate kernel regression using the sm.regression function:<br>
<a href=""http://cran.r-project.org/web/packages/sm/sm.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/sm/sm.pdf</a></p>

<p>There is an option to compare the nonparametric estimate with linear model. When requested, it returns p-value that the nonparametric estimate is not different from the linear (null hypothesis).</p>

<p>Does anyone know what test is used to calculate the p-value? I can't find it anywhere.</p>

<p>The answer might be in:
Bowman, A.W. (2006). Comparing nonparametric surfaces. Statistical Modelling, 6, 279-299.</p>

<p>Thank you!</p>
"
"0.0693653206906364","0.0679889413649005","100453","<p>I am running a glm in R on data with quite many predictors (~50), both initially continuous and factors. The response is binary and the volume of the data is OK (~100K rows), in order to model non-linear relationships, I convert the continuous variables to factors as well. In the end this results in some levels of some variables being insignificant. For example, variable ""age"" is binned into 20-30, 40-50, 50-60, 60+ and only the first two are significant.</p>

<p>All this results in a reasonable model with AUC 0.86 and residuals looking random with a small bias. </p>

<p>I understand that R converts each factor level to a binary variable and then runs the regression, and I'd still like to improve the model. Would you please help me understand if: </p>

<ul>
<li>converting all the variables' factor levels to binary myself,</li>
<li>selecting only the variables that were significant in the initial model</li>
<li>then re-fitting the model with the new (reduced) variable set</li>
</ul>

<p>sound like a good idea?</p>

<p>Or should I continue to use predict() on the model with many insignificant coefficients for factor levels happily as before?</p>
"
"0.0700841515030364","0.0686935087981502","100682","<p>In ordinary least squares regression (OLS), if the plot of the residuals against the fitted values form a horizontal line around 0, then we can say that the dependent variable is linearly related to the independent variable.</p>

<p>I had thought that this is true because $E(y_i - \hat{y}_I)=0$ when the dependent variable is linearly related to the independent variable, see <a href=""http://stats.stackexchange.com/questions/100653/how-to-do-ordinary-least-squares-ols-when-the-observations-are-not-linear"">here</a>.</p>

<p>However, suppose:</p>

<p>$y_i = \alpha + \sin(x_i) + \epsilon_i$.</p>

<p>Then $E(y_i - \hat{y}_i)$ is still 0, see <a href=""http://stats.stackexchange.com/questions/100653/how-to-do-ordinary-least-squares-ols-when-the-observations-are-not-linear"">here</a> but then the plot of its residuals against its fitted value is no longer a horizontal line around 0, as this R code shows:</p>

<pre><code>n &lt;- 10^3
df &lt;- data.frame(x=runif(n, 1, 10))
df$mean.y.given.x &lt;- sin(df$x)
df$y &lt;- df$mean.y.given.x + rnorm(n)
model &lt;- lm(y ~ x, data=df)
plot(predict(model, newdata=df), residuals(model))
abline(a=0,b=0,col='blue')
</code></pre>

<p><img src=""http://i.stack.imgur.com/kyBwt.png"" alt=""enter image description here""></p>

<p>So my question is, which assumption(s) of OLS that causes the plot of the residuals and the fitted value to be a horizontal line around 0 and why/how is it true? </p>
"
"0.02831827358943","0.0277563690826684","100751","<p>In one of the slides of a statistics course that I followed the following about using drop1 or ANCOVA is stated.</p>

<blockquote>
  <p>Using drop1 the p-values shown are p-values for deleting one variable at a time from the full model, whereas the p-values in the output of anova are sequential, as in a step-up strategy. This problem does not arise in ANOVA or linear regression, only in ANCOVA and mixed models.)</p>
</blockquote>

<p>I looked at it for 20 minutes, and cannot understand that if this is true why someone would still use the command <code>anova()</code> in R anyway.</p>

<p>Does anyone have an idea?</p>

<p>The reason why I ask this question here is because the subject was given a few months ago.</p>
"
"NaN","NaN","101126","<p>I was wondering how can I best fit nonlinear regression model to this data, using R package. How can I check if model is good fitted since Rsquared value is not returned in most functions for nonlinear models?</p>

<p>Thanks in advance for answers
<img src=""http://ramza.tarchomin.pl/rozklad.jpeg"" alt=""data""></p>
"
"0.0424774103841449","0.0555127381653369","102689","<p>I have a problem with some analysis I need to do.</p>

<p>I have a series of regressions. Some of the predictors of these regression are categorical with multiple levels. I performed regressions, both linear and logistic, choosing a baseline for these category according to various factors.</p>

<p>The problem is that my colleagues asked not only for a confrontation of the factors to a baseline but also a pairwise confrontation. Like you it's used to do with a post-hoc test for ANOVA (they are pretty new to regressions and their benefits).</p>

<p>How should I approach this?
I thought of some solutions:</p>

<ul>
<li>Subsetting: That is subset the data to include two factors at time, and therefore repeating the regression once per every subset.</li>
<li>Splitting: Splitting the category column in a column for every factor and put 0 and 1 as levels. This approach can furthermore be conducted in two ways:
<ul>
<li>Putting all the new columns in the regression (minding that they are mutually exclusive).</li>
<li>Putting one column at time, multiplying the regressions.</li>
</ul></li>
</ul>

<p>Which approach would you suggest, minding statistical correctness and workload?</p>

<p>Especially, what's the conceptual difference between the three methods?</p>

<p>Thanks a lot!</p>
"
"0.0633215847514023","0.0620651280774201","102884","<p>I've just began looking into survival analyses, and I'm having some difficulty interpreting the results. </p>

<p>I have a model linear regression model where time ~ x1 * x2 * x3... x9, this model seems to fit well (adjusted R${^2}$ of ~0.9) however it was suggested that survival analyses may be a better direction.</p>

<p>I have tried using the R tool for this, but I can't interpret the data. I do have some trails where they timed out (after 1 hour) so I set up my censor column for that. I then tried plotting the full data set (~200 trials), and got a nice downward stepped curve. <img src=""http://i.stack.imgur.com/JVEiV.png"" alt=""Full data set"">, however this doesnt seem to tell me anything, the model used was just survfit(my.surv ~ 1). If I replace the ~ 1 with one of the variables, for example survfit(mysurv ~ x1), I get a total mess. Lines all over the place (mostly vertical).</p>

<p>So, my questions are these:</p>

<p>1) Is it possible to gain useful information, other than saying that 80% of the trials finished before 2000 seconds, which is what I think this plot shows</p>

<p>2) Is it possible to include factors in the model, for example time ~ x1 + x2, etc. If not how does one ascertain which variables are important by this method. </p>

<p>3) How can I use this to make predictions about new data, not included in this model?</p>
"
"0.0805952195517515","0.0877733458775107","102892","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Statistical test chosen: logistic regression</p>

<p>I need to find the variables that best explain variations in the outcome variable (I am not interested in making predictions).</p>

<p>The problem: This question is a follow-up on the 2 questions listed below. From them, I got that performing automated stepwise regression has its downsides. Anyway, it seems that my sample size would be too small for that. It seems that my sample is also too small to enter all variables at once (using the SPSS 'Enter' method). This leaves me with my issue unresolved: how can I select a subset of variables from my original long list in order to perform multivariate logistic regression analysis?</p>

<p>UPDATE1: I am not an statistician, so I would appreciate if jargons can be reduced to the minimum. I am working with SPSS and am not familiar with other packages, so options that could be run with that software would be highly preferable.</p>

<p>UPDATE2: It seems that SPSS does not support LASSO for logistic regression. So following one of your suggestions, I am now struggling with R. I have passed through the basics, and managed to run a univariate logistic regression routine successfully using the glm code. But as I tried glmnet with the same dataset, I am receiving an error message. How could I fix it? Below is the code I used, followed by the error message:</p>

<pre><code>data1 &lt;- read.table(""C:\\\data1.csv"",header=TRUE,sep="";"",na.string=99:9999)

y &lt;- data1[,1]

x &lt;- data1[,2:45]

glmnet(x,y,family=""binomial"",alpha=1)  

**in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  : 
(list) object cannot be coerced to type 'double'**
</code></pre>

<p>UPDATE3: I got another error message, now related to missing values. My question concerning that matter is <a href=""http://stats.stackexchange.com/questions/104194/how-to-handle-with-missing-values-in-order-to-prepare-data-for-feature-selection"">here</a>. </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/88482/can-univariate-linear-regression-be-used-to-identify-useful-variables-for-a-subs"">Can univariate linear regression be used to identify useful variables for a subsequent multiple logistic regression?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856"">Algorithms for automatic model selection</a></li>
</ul>
"
"0.0853828074607","0.0920574617898323","103077","<p>I am having a lot of fun with regression analysis at the moment, and by fun I mean bashing myself repeatedly over the head. I have a set of 200 data points, by filtering on a property of interest, I end up with 153 points of use. </p>

<p>I initially used these 153 points to generate a linear regression, with an excellent R${^2}$ and a plot of fitted vs actual variables of almost a perfect diagonal. Great! However, it was suggested that this might only be an internally predictive model (which as I understand it means the model fits the data, rather than the opposite). So, I then tried this: I randomly selected a sample of 100 of the 153 results, and built the same model, it still gave a relatively good fit. I then used the predict function in R to try to predict the outcome of the other 53 records. It did not go well. What I got was one of 2 things.</p>

<ol>
<li>the predictions made no sense at all, not even on the same scale as the actual values.</li>
<li>most of the predictions made sense (although weren't very accurate) and one or two, were on an entirely different scale (orders of magnitude larger, or smaller).</li>
</ol>

<p>Since the model I am fitting has time as the response variable, it was suggested I use a Gamma fit regression instead of a plain old linear regression. I tried this and ended up essentially with the result.</p>

<p>So, am I using R correctly, was Gamma a good choice for this? I'm pretty sure my data is good (non biased) so if I am unable to predict, despite the good model - does this mean my model is useless? I've been working on this for some weeks now, and it would be great if I could salvage something.</p>

<p>The R commands I have used:</p>

<pre><code>modelSet&lt;-sample(1:nrow(myData),100)
modelData&lt;-myData[modelSet,]
predictData&lt;-myData[-modelSet,]

fit&lt;-lm(""time~(x1+x2+x3+x4+x5+x6)^3"", data=modelData)
pred&lt;-predict(fit, predictData)
plot(predictData$time, pred) &lt;- gives a really not useful plot


fit2&lt;-glm(""time~(x1+x2+x3+x4+x5+x6)^3"", data=modelData, family=Gamma) # tried with link=log too
pred2&lt;-predict(fit2, predictData)
plot(predictData$time, pred2) &lt;- gives an even less useful plot
</code></pre>
"
"0.0755153962384799","0.0832691072480053","103129","<p>I am trying to understand what the reported intercept is showing when I use <code>arima()</code> with <code>xreg=</code>. The documentation says</p>

<p>""If am xreg term is included, a linear regression (with a constant term if include.mean is true and there is no differencing) is fitted with an ARMA model for the error term.""</p>

<p>Thus I expect the intercept shown to come from the regression using <code>xreg=</code> as the X variables, before any arima model is done on those residuals. </p>

<p>However I tried to double check this by actually doing the regression with <code>lm()</code> and the intercept from that does not match what is reported from <code>arima()</code> (although the slope coefficient is pretty close). </p>

<p>Here is my example:</p>

<pre><code>set.seed(456)
v = rnorm(100,1,1)
x = cumsum(v)  ; x = as.xts(ts(x)) 

# Fit AR(1) after taking out a time trend (aka, drift)
model5 = arima(x, order=c(1,0,0), xreg=1:length(x), include.mean=TRUE)
# Coefficients:
#         ar1     intercept  1:length(x)
#       0.8995     0.8815       1.1113
# s.e.  0.0422     1.6193       0.0265


# Double check
MyTime = 1:length(x)
model5_Part1 = lm(x ~ MyTime )
# Coefficients:
#      (Intercept)       MyTime  
#         1.856           1.096
</code></pre>

<p>The intercepts do not match, thus I do not know what the intercept is showing from the arima with xreg.</p>

<p>Note the example shown is based on ""Issue 2"" shown here <a href=""http://www.stat.pitt.edu/stoffer/tsa3/Rissues.htm"" rel=""nofollow"">http://www.stat.pitt.edu/stoffer/tsa3/Rissues.htm</a></p>

<p>Also note that this isn't a problem particular to modeling drift. Here is another example, where in addition to the intercept not matching, even the slope coefficient on the <code>xreg=</code> variable doesn't match what is shown from using <code>lm()</code>. This example has nothing to do with drift and uses the cars dataset as if it were time series data.</p>

<pre><code>data(cars)
cars = as.xts(ts(cars, start=c(1980,1), freq=12))
model6 = arima(cars$speed, xreg=cars$dist, order=c(1,0,0), include.mean=TRUE)
# Coefficients:
#         ar1    intercept   dist
#       0.9979    15.2890  -0.0172
# s.e.  0.0030    10.5452   0.0055

model6_Part1 = lm(cars$speed ~ cars$dist)
# Coefficients:
#      (Intercept)    cars$dist  
#        8.2839        0.1656 
</code></pre>

<p>Intercepts do not match, slope coefficient does not match.</p>
"
"0.0980973772790571","0.0961508829696314","103459","<p>I am trying to figure out which cross validation method is best for my situation. </p>

<p>The following data are just an example for working through the issue (in R), but my real <code>X</code> data (<code>xmat</code>) are correlated with each other and correlated to different degrees with the <code>y</code> variable (<code>ymat</code>). I provided R code, but my question is not about R but rather about the methods. <code>Xmat</code> includes X variables V1 to V100 while <code>ymat</code> includes a single y variable. </p>

<pre><code>set.seed(1233)
xmat           &lt;- matrix(sample(-1:1, 20000, replace = TRUE), ncol = 100)
colnames(xmat) &lt;- paste(""V"", 1:100, sep ="""")
rownames(xmat) &lt;- paste(""S"", 1:200, sep ="""")
  # the real y data are correlated with xmat
ymat           &lt;- matrix(rnorm(200, 70,20), ncol = 1)
rownames(ymat) &lt;- paste(""S"", 1:200, sep="""")
</code></pre>

<p>I would like to build a model for predicting <code>y</code> based on all the variables in <code>xmat</code>. So it will be a linear regression model <code>y ~ V1 + V2 + V3+ ... + V100</code>. From a review, I can see the following three cross validation methods:</p>

<ol>
<li><p><strong>Split data in about half</strong> and use one for training and another half for testing (cross validation):</p>

<pre><code>prop       &lt;- 0.5 # proportion of subset data
set.seed(1234)
  # training data set 
training.s &lt;- sample (1:nrow(xmat), round(prop*nrow(xmat),0))
xmat.train &lt;- xmat[training.s,]
ymat.train &lt;- ymat[training.s,]

  # testing data set 
testing.s &lt;- setdiff(1:nrow(xmat), training)
xmat.test &lt;- xmat[testing.s,]
ymat.test &lt;- ymat[testing.s,]
</code></pre></li>
<li><p><strong>K-fold cross validation</strong> - using 10 fold cross validation: </p>

<pre><code>mydata &lt;- data.frame(ymat, xmat)
fit    &lt;- lm(ymat ~ ., data=mydata)
library(DAAG)
cv.lm(df=mydata, fit, m=10) # ten-fold cross validation 
</code></pre></li>
<li><p><strong>Masking one value or few values at a time</strong> : In this method we randomly mask a value in dataset (y) by replacing it with NA and predict it. The process is repeated n times.</p>

<pre><code>n = 500 
predicted.v &lt;- rep(NA, n)
real.v      &lt;- rep(NA, n)

for (i in 1:n){
  masked.id &lt;- sample (1:nrow(xmat), 1)
  ymat1     &lt;- ymat 
  real.v[i] &lt;- ymat[masked.id,]
  ymat1[masked.id,] &lt;- NA
  mydata            &lt;- data.frame(ymat1, xmat)
  fit               &lt;- lm(ymat1 ~ ., data=mydata)
  predicted.v[i]    &lt;- fit$fitted.values[masked.id]
}
</code></pre></li>
</ol>

<p>How do I know which is best for any situation? Are there other methods? <code>Bootstrap validation</code> vs <code>CV</code> ? Worked examples would be appreciated. </p>
"
"0.0400480865731637","0.039253433598943","103638","<p>I'm using R and the package boot.</p>

<p>my boot function returns the coefficients and the r.square and sqrt of the r.square </p>

<pre><code> rsq2 &lt;- function(formula, data, indices) {
     d &lt;- data[indices,] # allows boot to select sample 
     fit &lt;- lm(formula, data=d)
     return(c( coef(fit),summary(fit)$r.square,sqrt(summary(fit)$r.square)))
 }

boot(data=mtcars,rsq2,1000,formula=""mpg~wt"")

#ORDINARY NONPARAMETRIC BOOTSTRAP

Call:
boot(data = mtcars, statistic = rsq2, R = 1000, formula = ""mpg~wt"")


Bootstrap Statistics :
      original        bias    std. error
t1* 37.2851262  0.1170681005  2.32470420
t2* -5.3444716 -0.0557873180  0.70523904
t3*  0.7528328  0.0019940314  0.05758118
t4*  0.8676594  0.0004980607  0.03362469
</code></pre>

<p>I determine a pvalue using the original value and the std. error determined by the boot program</p>

<pre><code>pvalue of the regression coefficient
2*pnorm(-abs(-5.3444716/0.70523904))
[1] 3.502706e-14
pvalue of the CC
 2*pnorm(-abs(0.8676594/0.03362469))
[1] 7.947938e-147
</code></pre>

<p>Why are these not the same value?</p>

<p>In a simple univariable the linear regression of the p-value of the coefficient and Pearson's CC are the same value?</p>
"
"0.0749231094763201","0.0734364498908627","103786","<p>I have a model where time is the response variable. I'd like to generate confidence intervals for the estimates. I have established that the error in the estimation is roughly normally distributed (it may be more cauchy). The Mean and Median are very different, with the median more accurately representing the middle of the data. Am I allowed to use the median for my confidence interface and if so is there a different method for doing so?</p>

<p>I have reviewed this question: <a href=""http://stats.stackexchange.com/questions/21103/confidence-interval-for-median"">Confidence interval for median</a> but it is not clear if they are trying to accomplish the same thing I am.</p>

<p>EDIT
The model is for an estimation of the amount of time a process takes to complete. I performed a linear regression and established that the model has a relatively good fit. I then took repeatedly (2000 times) took a random sample of 75% of the original sample and rebuilt the model. I then predicted the time for the remaining 25%, and stored the error in each case. This led to ~90000 results, which roughly follow a normal distribution (or possibly cauchy) I would like to find an estimate for the confidence interval of an individual result, e.g. for one specific process the actual time taken was 46 seconds, and the predicted time taken was 1 minute. I'd like to be able to say with 95% certainty that my estimate is accurate within +- 15 seconds (for example).</p>
"
"0.105957277556576","0.10385482340819","104180","<p>Suppose X is the design matrix of my experiment of which I want to model a linear regression model. Such a design matrix can be created by (in this example it is a full factorial design)</p>

<pre><code>library(BHH2)
Des2 &lt;- ffDesMatrix(5)
</code></pre>

<p>Now, a contrast matrix can be obtained by the following code, corresponding to ( t(X) %x% X)^-1 %x% t(X) (in which %x% represents matrix multiplication):</p>

<pre><code>solve(t(Des2) %*% Des2) %*% t(Des2)
</code></pre>

<p>This results in a matrix giving the Y observations that will be contrasted against each other in order to obtain a parameter estimate (e.g. first row will correspond to beta1).</p>

<p>Now, one can also make a design matrix using the model.matrix function</p>

<pre><code>Des &lt;- as.data.frame(Des2)
names(Des) &lt;- c(""X1"",""X2"",""X3"",""X4"",""X5"")
model.matrix(~X1*X2*X3*X4*X5,data=Des)
</code></pre>

<p>This is a model matrix that can be used to estimate all possible interactions between five factorial variables with two levels.</p>

<p>When comparing the columns of the model matrix obtained by the last piece of code with the rows of our contrast matrix, one can note that they are rather similar. The values might be different, but the signs representing the contrasts are identical for a row/column representing the same parameter to be estimated.</p>

<p>My question now refers to these columns and rows: is there any difference to be noted between these two? Or are they identical as in that they both represent contrasts of the Y observations that will be used for estimation of the parameter, and have I thus summed up two equivalent way in obtaining these contrasts?</p>
"
"0.0400480865731637","0.0196267167994715","104216","<p>I have created a scatterplot. I want to see the trend of how the number of fishing cat scats vary with an increase in perimeter of water body. Hence my response variable is <code>number of fishing cat scats</code> and predictor variable is <code>perimeter of water body</code>. After creating a scatterplot, I gave an <code>abline</code> (<code>lm</code>) command, which has given me a linear regression line, which doesn't exactly portray the relationship between <code>number of fishing cat scats</code> and <code>perimeter of water body</code>. What command do I need to use to generate a curve which fits the data?
I am using R as my statistical software.</p>
"
"0.0823180601768231","0.0868911793083882","104276","<p>Using R, I want to run a linear regression to estimate the abnormal return on days with positive, negative and neutral news (CLASS). I'm a beginner in R, as well as in using regression models! 
First of all the data structure is as follows. CONTROLVAR just represents all the columns I use as control variables.</p>

<pre><code>DATE &lt;- c(""1"",""2"",""3"",""4"",""5"",""6"",""7"",""1"",""2"",""3"",""4"",""5"",""6"",""7"")
COMP &lt;- c(""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"")
RET &lt;- c(-2.0,1.1,3,1.4,-0.2, 0.6, 0.1, -0.21, -1.2, 0.9, 0.3, -0.1,0.3,-0.12)
CLASS &lt;- c(""positive"", ""negative"", ""aneutral"", ""positive"", ""positive"", ""negative"", ""aneutral"", ""positive"", ""negative"", ""negative"", ""positive"", ""aneutral"", ""aneutral"", ""aneutral"")
SUBJECT.1 &lt;- c(""LITIGATION"",""LAYOFF"",""POLLUTION"",""CHEMICAL DISASTER"",""PRESS RELEASE"",""PEOPLE"",""EMISSIONS"",""ENERGY"",""WASTE MANAGEMENT"",""EMPLOYEES"",""SUBJECT11"",""SUBJECT12"",""SUBJECT13"",""SUBJECT14"")
SUBJECT.2 &lt;- c(""POLLUTION"",""EMPLOYEES"",""NUCLEAR"",""FUELS"",""STOCK OPTION PLAN"",""EXECUTIVES"",""CO2"",""SOLAR"",""POLLUTION"",""EXECUTIVES"",""SUBJECT21"",""SUBJECT22"",""SUBJECT23"",""SUBJECT24"")
SUBJECT.3 &lt;- c(""ENVIRONMENT"",""JOB REDUCTIONS"",""POWER PLANTS"",""POLLUTION"",""EMPLOYEES"",""FRAUD"",""CLIMATE CHANGE"",""SUSTAINABILITY"",""HAZARDOUS WASTE"",""BONUS PAY"",""SUBJECT31"",""SUBJECT32"",""SUBJECT33"",""SUBJECT34"")
CONTROLVAR &lt;- c(""11"",""13"",""13"",""14"",""13"",""14"",""12"",""11"",""13"",""13"",""14"",""13"",""14"",""12"")

df &lt;- data.frame(DATE, COMP, RET, CLASS, SUBJECT.1, SUBJECT.2, SUBJECT.3, CONTROLVAR, stringsAsFactors=F)

df

#    DATE COMP   RET    CLASS         SUBJECT.1         SUBJECT.2       SUBJECT.3 CONTROLVAR
# 1     1    A -2.00 positive        LITIGATION         POLLUTION     ENVIRONMENT         11
# 2     2    A  1.10 negative            LAYOFF         EMPLOYEES  JOB REDUCTIONS         13
# 3     3    A  3.00 aneutral         POLLUTION           NUCLEAR    POWER PLANTS         13
# 4     4    A  1.40 positive CHEMICAL DISASTER             FUELS       POLLUTION         14
# 5     5    A -0.20 positive     PRESS RELEASE STOCK OPTION PLAN       EMPLOYEES         13
# 6     6    A  0.60 negative            PEOPLE        EXECUTIVES           FRAUD         14
# 7     7    A  0.10 aneutral         EMISSIONS               CO2  CLIMATE CHANGE         12
# 8     1    B -0.21 positive            ENERGY             SOLAR  SUSTAINABILITY         11
# 9     2    B -1.20 negative  WASTE MANAGEMENT         POLLUTION HAZARDOUS WASTE         13
# 10    3    B  0.90 negative         EMPLOYEES        EXECUTIVES       BONUS PAY         13
# 11    4    B  0.30 positive         SUBJECT11         SUBJECT21       SUBJECT31         14
# 12    5    B -0.10 aneutral         SUBJECT12         SUBJECT22       SUBJECT32         13
# 13    6    B  0.30 aneutral         SUBJECT13         SUBJECT23       SUBJECT33         14
# 14    7    B -0.12 aneutral         SUBJECT14         SUBJECT24       SUBJECT34         12
</code></pre>

<p>This regression model would look like this:</p>

<pre><code>mymodel1 &lt;- lm(RET ~ CLASS + CONTROLVAR, data=df)
</code></pre>

<p>aneutral (neutral) will be the reference category. I would also like to see the effect of certain subjects of the article on the abnormal return. How can I do that?
Let's say I want to include the subjects LITIGATION, POLLUTION and LAYOFF.
I'd like to see how the effect of positive, negative and neutral news change, if the article is about POLLUTION for example.
If I make ""dummy columns"" for the three subjects of interest, my data.frame looks like this:</p>

<pre><code>DATE &lt;- c(""1"",""2"",""3"",""4"",""5"",""6"",""7"",""1"",""2"",""3"",""4"",""5"",""6"",""7"")
COMP &lt;- c(""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"")
RET &lt;- c(-2.0,1.1,3,1.4,-0.2, 0.6, 0.1, -0.21, -1.2, 0.9, 0.3, -0.1,0.3,-0.12)
CLASS &lt;- c(""positive"", ""negative"", ""aneutral"", ""positive"", ""positive"", ""negative"", ""aneutral"", ""positive"", ""negative"", ""negative"", ""positive"", ""aneutral"", ""aneutral"", ""aneutral"")
SUBJECT.1 &lt;- c(""LITIGATION"",""LAYOFF"",""POLLUTION"",""CHEMICAL DISASTER"",""PRESS RELEASE"",""PEOPLE"",""EMISSIONS"",""ENERGY"",""WASTE MANAGEMENT"",""EMPLOYEES"",""SUBJECT11"",""SUBJECT12"",""SUBJECT13"",""SUBJECT14"")
SUBJECT.2 &lt;- c(""POLLUTION"",""EMPLOYEES"",""NUCLEAR"",""FUELS"",""STOCK OPTION PLAN"",""EXECUTIVES"",""CO2"",""SOLAR"",""POLLUTION"",""EXECUTIVES"",""SUBJECT21"",""SUBJECT22"",""SUBJECT23"",""SUBJECT24"")
SUBJECT.3 &lt;- c(""ENVIRONMENT"",""JOB REDUCTIONS"",""POWER PLANTS"",""POLLUTION"",""EMPLOYEES"",""FRAUD"",""CLIMATE CHANGE"",""SUSTAINABILITY"",""HAZARDOUS WASTE"",""BONUS PAY"",""SUBJECT31"",""SUBJECT32"",""SUBJECT33"",""SUBJECT34"")
LITIGATION &lt;- c(1,0,0,0,0,0,0,0,0,0,0,0,0,0)
POLLUTION  &lt;- c(1,0,1,1,0,0,0,0,1,0,0,0,0,0)
LAYOFF     &lt;- c(0,1,0,0,0,0,0,0,0,0,0,0,0,0)
CONTROLVAR &lt;- c(""11"",""13"",""13"",""14"",""13"",""14"",""12"",""11"",""13"",""13"",""14"",""13"",""14"",""12"")

df2 &lt;- data.frame(DATE, COMP, RET, CLASS, SUBJECT.1, SUBJECT.2, SUBJECT.3, LITIGATION, POLLUTION, LAYOFF, CONTROLVAR, stringsAsFactors=F)

df2 

#    DATE COMP   RET    CLASS         SUBJECT.1         SUBJECT.2       SUBJECT.3 LITIGATION POLLUTION LAYOFF CONTROLVAR
# 1     1    A -2.00 positive        LITIGATION         POLLUTION     ENVIRONMENT          1         1      0         11
# 2     2    A  1.10 negative            LAYOFF         EMPLOYEES  JOB REDUCTIONS          0         0      1         13
# 3     3    A  3.00 aneutral         POLLUTION           NUCLEAR    POWER PLANTS          0         1      0         13
# 4     4    A  1.40 positive CHEMICAL DISASTER             FUELS       POLLUTION          0         1      0         14
# 5     5    A -0.20 positive     PRESS RELEASE STOCK OPTION PLAN       EMPLOYEES          0         0      0         13
# 6     6    A  0.60 negative            PEOPLE        EXECUTIVES           FRAUD          0         0      0         14
# 7     7    A  0.10 aneutral         EMISSIONS               CO2  CLIMATE CHANGE          0         0      0         12
# 8     1    B -0.21 positive            ENERGY             SOLAR  SUSTAINABILITY          0         0      0         11
# 9     2    B -1.20 negative  WASTE MANAGEMENT         POLLUTION HAZARDOUS WASTE          0         1      0         13
# 10    3    B  0.90 negative         EMPLOYEES        EXECUTIVES       BONUS PAY          0         0      0         13
# 11    4    B  0.30 positive         SUBJECT11         SUBJECT21       SUBJECT31          0         0      0         14
# 12    5    B -0.10 aneutral         SUBJECT12         SUBJECT22       SUBJECT32          0         0      0         13
# 13    6    B  0.30 aneutral         SUBJECT13         SUBJECT23       SUBJECT33          0         0      0         14
# 14    7    B -0.12 aneutral         SUBJECT14         SUBJECT24       SUBJECT34          0         0      0         12
</code></pre>

<p>The first problem is, that the dummy variables partially overlap. 
My model would look something like this.</p>

<pre><code>mymodel2 &lt;- lm(RET ~ CLASS + LITIGATION + POLLUTION + LAYOFF  
               + LITIGATION * CLASS + POLLUTION * CLASS + LAYOFF * CLASS   # Interaction Variables
               + CONTROLVAR,     # Control Variables
               data=df2)
</code></pre>

<p>I'm quite sure this model is wrong, but I don't know exactly what is wrong and how to implement a model that works for this task. Can anyone help me with this problem? 
Thank You!</p>
"
"0.0506572678011219","0.0496521024619361","104548","<p>I followed <a href=""http://rtutorialseries.blogspot.hk/2010/01/r-tutorial-series-basic-hierarchical.html"" rel=""nofollow"">this tutorial</a> to learn Hierarchical Linear Regression (HLR) in R, but couldn't understand how to interpret its sample output of <code>&gt;anova(model1,model2,model3)</code></p>

<p><img src=""http://i.stack.imgur.com/MxXIM.png"" alt=""enter image description here""></p>

<p>The tutorial simply says </p>

<blockquote>
  <p>each predictor added along the way is making an important contribution to the overall model.</p>
</blockquote>

<p>But I would like some more details to <strong>quantify</strong> the contribution of each explanatory variable, like:</p>

<ol>
<li><p>""UNEM"" explains <code>X</code> (or <code>X%</code>) variance</p></li>
<li><p>Adding the ""HGRAD"" variable explains <code>Y</code> (or <code>Y%</code>) more variance</p></li>
<li><p>Adding the ""INC"" variable further explains <code>Z</code> (or <code>Z%</code>) more variance</p></li>
</ol>

<p>So, can I get the value of <code>X</code>, <code>Y</code>, and <code>Z</code> using the above ANOVA table? How? Specifically, what do <code>Res.Df</code>, <code>RSS</code>, <code>Sum of Sq</code> mean in this ANOVA table?</p>
"
"NaN","NaN","104571","<p>I have measurements for daily space heating vs daily mean outdoor temperature for two different control strategies. The data is shown here:</p>

<p><img src=""http://i.stack.imgur.com/c4L6Y.png"" alt=""enter image description here""></p>

<p>I have also performed linear regression, of the form:</p>

<pre><code>lm(Energy ~ Control * MeanOutdoorTemp)
</code></pre>

<p>This yields four coefficients:</p>

<pre><code>Coefficients:
                           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                170.8293     4.2083  40.594  &lt; 2e-16 ***
ControlMPC                 -30.7044     4.9025  -6.263 1.38e-07 ***
MeanOutdoorTemp             -6.2924     1.6466  -3.821 0.000413 ***
ControlMPC:MeanOutdoorTemp   0.8211     1.7162   0.478 0.634709    
</code></pre>

<p>I'd like to test whether these two regression lines cross at zero energy. What would be the statistically correct way to do that?</p>
"
"0.03415312298428","0.0418443008135602","104585","<p>Using R, I want to run a linear regression to estimate the abnormal return on days with positive, negative and neutral news (CLASS). I'm a beginner in R, as well as in using regression models! 
First of all the data structure is as follows. CONTROLVAR just represents all the columns I use as control variables.</p>

<pre><code>DATE &lt;- c(""1"",""2"",""3"",""4"",""5"",""6"",""7"",""1"",""2"",""3"",""4"",""5"",""6"",""7"")
COMP &lt;- c(""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"")
RET &lt;- c(-2.0,1.1,3,1.4,-0.2, 0.6, 0.1, -0.21, -1.2, 0.9, 0.3, -0.1,0.3,-0.12)
CLASS &lt;- c(""positive"", ""negative"", ""aneutral"", ""positive"", ""positive"", ""negative"", ""aneutral"", ""positive"", ""negative"", ""negative"", ""positive"", ""aneutral"", ""aneutral"", ""aneutral"")
LITIGATION &lt;- c(1,0,0,0,0,0,0,0,0,0,0,0,0,0)
POLLUTION  &lt;- c(1,0,1,1,0,0,0,0,1,0,0,0,0,0)
LAYOFF     &lt;- c(0,1,0,0,0,0,0,0,0,0,0,0,0,0)
CONTROLVAR &lt;- c(""11"",""13"",""13"",""14"",""13"",""14"",""12"",""11"",""13"",""13"",""14"",""13"",""14"",""12"")

mydf &lt;- data.frame(DATE, COMP, RET, CLASS, LITIGATION, POLLUTION, LAYOFF, CONTROLVAR, stringsAsFactors=F)

mydf 

#    DATE COMP   RET    CLASS LITIGATION POLLUTION LAYOFF CONTROLVAR
# 1     1    A -2.00 positive          1         1      0         11
# 2     2    A  1.10 negative          0         0      1         13
# 3     3    A  3.00 aneutral          0         1      0         13
# 4     4    A  1.40 positive          0         1      0         14
# 5     5    A -0.20 positive          0         0      0         13
# 6     6    A  0.60 negative          0         0      0         14
# 7     7    A  0.10 aneutral          0         0      0         12
# 8     1    B -0.21 positive          0         0      0         11
# 9     2    B -1.20 negative          0         1      0         13
# 10    3    B  0.90 negative          0         0      0         13
# 11    4    B  0.30 positive          0         0      0         14
# 12    5    B -0.10 aneutral          0         0      0         13
# 13    6    B  0.30 aneutral          0         0      0         14
# 14    7    B -0.12 aneutral          0         0      0         12
</code></pre>

<p>aneutral (neutral) will be the reference category. I would also like to see the effect of certain subjects of the article on the abnormal return.</p>

<p>I'd like to include interaction, so my model looks like this:</p>

<pre><code>mymodel &lt;- lm(RET ~ CLASS * (LITIGATION + POLLUTION + LAYOFF)   # Interaction Variables
               + CONTROLVAR,     # Control Variables
               data=mydf)
</code></pre>

<p>Now I don't really understand how to interpret the coefficients I get and I would like to plot a regression line or anything that visualizes these effects to better understand the results. What's a good way of doing this for a three-class-problem like this? <code>abline()</code> doesn't seem to work, because there are too many variables.</p>

<p>Thank you!</p>
"
"NaN","NaN","104641","<p>I have some functions of $x$, in the form of $d\sqrt{x}$ or $d\log(x)$ where $d$ is known. I would like to rewrite (approximate is fine) them in the form $a/(1 + bx^c)$, where $a$, $b$ and $c$ are arbitrary.</p>

<p>I don't think there are $a$, $b$ and $c$ such that the two curves will match exactly, so I think maybe try to fit a nonlinear regression and find the closest $a$, $b$ and $c$. I tried the following R code, and I get the singular gradient error. </p>

<pre><code>x     = seq(1, 50000, by=1000)
y     = 50*sqrt(x)
model = nls(y ~ a/(1 + b*x^c), start=list(a=1, b=-0.01, c=0.01))
</code></pre>
"
"0.0566365471788599","0.0555127381653369","104733","<p>I have a vector of data with their standard errors:</p>

<pre><code>Estimate &lt;- c(0.254719513441046, 0.130492717014416, 0.0386710035855823, 0.14118562325405, 0.160649388742147, 0.60363287936294, 0.173485345603584, 0.425817607348994, 0.128802795868366, 0.104136474748465)
SE &lt;- c(0.126815201703205, 0.240179692822184, 0.248612907189712, 0.379800224602374, 0.0799874163236805, 0.170568135051654, 0.108163615496468, 0.0585237357996271, 0.16702614577514, 0.124308993809982)
</code></pre>

<p>I need to form a prediction interval for new elements that will be drawn from the same parent population. At first I thought I would do a linear regression to estimate the mean and the error on the mean:</p>

<pre><code>summary(lm(Estimate ~ 1, weights = 1/SE^2))
Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.28014    0.04996   5.607 0.000331 ***
</code></pre>

<p>But when I try naively to call <code>predict()</code> here is what I get:</p>

<pre><code>predict(lm(Estimate ~ 1, weights = 1/SE^2), newdata = data.frame(x = 0), interval = ""prediction"")
        fit       lwr      upr
1 0.2801405 -2.860802 3.421083
Warning message:
In predict.lm(lm(Estimate ~ 1, weights = 1/SE^2), newdata = data.frame(x = 0),  :
  Assuming constant prediction variance even though model fit is weighted
</code></pre>

<p>Did I do this right? Is the 95% confidence interval really [-2.86 3.42]? Do I need to worry about the warning?</p>
"
"0.0490486886395286","0.0480754414848157","105115","<p>I cannot understand the usage of polynomial contrasts in regression fitting. In particular, I am referring to an encoding used by <code>R</code> in order to express an interval variable (ordinal variable with equally spaced levels), described at <a href=""http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm#ORTHOGONAL"">this page</a>.</p>

<p>In the example of <a href=""http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm#ORTHOGONAL"">that page</a>, if I understood correctly, R fits a model for an interval variable, returning some coefficients which weights its linear, quadratic, or cubic trend. Hence, the fitted model should be:</p>

<p>$${\rm write} = 52.7870 + 14.2587X - 0.9680X^2 - 0.1554X^3,$$</p>

<p>where $X$ should take values $1$, $2$, $3$, or $4$ according to the different level of the interval variable.</p>

<p>Is this correct? And, if so, what was the purpose of polynomial contrasts?</p>
"
"0.0400480865731637","0.039253433598943","105424","<p>I'm trying to do a linear regression in R, however I have the added constraint the coefficients from the linear regression need to sum to a user given value between 0 and 1. (I understand forcing the coefficients like this will make the fit rather poor, but it's a needed constraint)</p>

<p>I've been though the documentation for lm() and the glmc package, as well as some similar questions, but none seem to tackle how to get the coefficients to sum to a specific value.</p>

<p>For data T, N, and target sum for coefficients p:</p>

<pre><code>Regression &lt;- function(T, N, p) {    
     fit &lt;- lm(T[,1] ~ N)
     coef &lt;- coef(fit)
...
}
</code></pre>

<p>Ideally I want sum(coef[-1]) = p (ignoring the intercept)</p>

<p>Sorry I can't provide more code, but I don't think I can do what I would like with lm().</p>

<p>Does anyone know of a way to do this, or of a package that will let me do this? </p>
"
"0.0400480865731637","0.039253433598943","105457","<p>So, I had a weighted dynamic graph having info about 10 consecutive timesteps ( basically 10 files ). Now, I had to mine out patterns in the weight and structure of the complete graph. I did that. The output file (having around 10,000 rows) was something like,</p>

<pre><code>Node 1 Node 2 Pattern
 191    570    ""00""  
 21     570    ""00"" 
 378    570    ""00"" 
 459    570    ""00"" 
 552    570    ""00"" 
 223    570    ""00""
 197    570    ""00"" 
 570    689    ""00"" 
 ...................
</code></pre>

<p>Basically gives 2 nodes, and the pattern associated. What I wish to ask is, what kind of model like (linear regression, or bar plots, .... ) can I use here, in order to gather some meaningful info, i.e. let's say, I am able to come up with a prediction model or I can say, that these are 2 graphs which are similar in nature.</p>
"
"0.0633215847514023","0.0620651280774201","106064","<p>I am trying to fit an exponential model to some data. The data is:</p>

<pre><code>Wavelength  aCDOM
350.01  0.80605
350.22  0.78302
350.43  0.78302
350.64  0.78302
350.85  0.78302
351.06  0.78302
351.27  0.78302
351.48  0.78302
351.68  0.75999
351.89  0.75999
352.1   0.75999
352.31  0.75999
352.52  0.75999
352.73  0.73696
352.94  0.73696
353.15  0.73696
353.36  0.73696
353.57  0.73696
353.78  0.73696
353.99  0.73696
354.2   0.73696
354.41  0.73696
354.62  0.73696
354.83  0.73696
355.04  0.73696
355.25  0.71393
355.46  0.71393
355.67  0.71393
</code></pre>

<p>I know that the best model fit this type of data is an exponential function in the for <code>y ~ a * exp(-b * c)</code> where <code>a</code> is the absorbance estimate at the reference wavelength, <code>b</code> is the spectral slope which is the value I am looking for, and <code>c</code> is the wavelength minus the reference wavelength (440nm). I have tried using the <code>nls</code> function by estimating the starting parameters following the method outlined in Fox (2002) but I keep getting the following error in <code>nlsModel(formula, mf, start, wts)</code>:</p>

<pre><code>singular gradient matrix at initial parameter estimates
</code></pre>

<p>The formula I am using is <code>model1 &lt;- nls(St0104 ~ a * exp(-b * (Wavelength-440)), start = list(a=0.1, b=0.0012), trace=T)</code>, I'm not even if I have calculated the starting parameters correctly. </p>

<p>I have tried instead to plot the log of <code>Y</code> and do linear as well as polynomial regression, but I know this is not correct for this data and the residuals are horrible. I'm pretty new to R but I have tried everything I can but this is driving me nuts can anybody help please?</p>
"
"0.02831827358943","0.0277563690826684","106209","<p>Sorry for my title, but I really don't know how to describe this question. I am fitting a linear regression in R now, and I find that there is one parameter showing linear relationship before certain point and cubic relationship after that point. What I did is to separate the whole dataset with reference to that point and fit the model with parameter value and cubic value respectively. However, that point is determined manually. Does anyone here know how to determine that point with formal algorithm? Thanks in advance </p>
"
"0.0800961731463273","0.078506867197886","107471","<p>Lets say I have continuous y and x variable and I run a linear regression:</p>

<pre><code>mdl1&lt;-lm(y ~ x)
</code></pre>

<p>A generalised linear model should also give me the same parameters value if I do not specify the error structure (i.e. by default it assumes that the error structure is Gaussian)</p>

<pre><code>mdl2&lt;-glm(y  ~ x)
</code></pre>

<p>Both the above model should give me the same results (since in the <code>mdl2</code>, by default the error structure is gaussian)</p>

<p>My question is if the residuals are not normally distributed in the <code>mdl1</code> (i.e. I do a shapiro.wilk test on <code>mdl1</code> residuals, which gives me a p-value of 0.02),</p>

<pre><code>shapiro.test(rstandard(mdl1)
</code></pre>

<p>then in the glm what error family do I specify considering both y and x are continuous. What my understanding was I can specify <code>family=poisson</code> or <code>family=binomial</code> if my response variable was either count or proportion. </p>

<pre><code>mdl3&lt;-glm(y  ~ x,family=""poisson"") # when y is count data
mdl4&lt;-glm(y  ~ x,family=""binomial"") # when y is proportion data
</code></pre>

<p>But in case of response variable being continuous and errors not normally distributed what error structure do I need to give?</p>

<pre><code>mdl5&lt;-glm(y ~ x, family=?????) # y is continuous data
</code></pre>
"
"0.0749231094763201","0.0734364498908627","107507","<p>Lets say that I have ""y"" that I want to model with linear regression. ""x"" and ""z"" are the things I'm interrested in showing folks, but I also have things that I want to adjust for, but not really show in my plot.</p>

<p>Now, I would like to show my coefficients in a plot, but I would like to keep the things I adjusted for out of it. So perhaps this could be coronary artery calcification modeled as ""CAC ~ SomeBloodStuff + Bloodpressure + BMI + Smoking_status"". The BMI and Smoking_status would be something that I would want to take into account, but just note that I have adjusted for them.</p>

<p>I gather this is how to do the model:</p>

<pre><code>MyModel &lt;- lm( CAC ~ SomeBloodstuff + BP + BMI + Smoking_status, data=MyData)
summary(MyModel) 
coefficients(MyModel)
confint(MyModel, level=0.95) # Seems like a legit model.
</code></pre>

<p>The coefplot function in arm library gives just the sort of presentation that I want, but shows all the dimensions.</p>

<pre><code>library(arm)
coefplot(MyModel)
</code></pre>

<p>How could I leave those few variables out of the plot while keeping them in the regression and get a plot that looks like what the coefplot produces?</p>
"
"0.0566365471788599","0.0555127381653369","107643","<p>Let a linear regression model obtained by the R function lm would like to know if it is possible to obtain by the Mean Squared Error command.</p>

<p>I had the FOLLOWING output of an example</p>

<pre><code>&gt; lm &lt;- lm(MuscleMAss~Age,data)
&gt; sm&lt;-summary(lm)
&gt; sm

Call:
lm(formula = MuscleMAss ~ Age, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-16.1368  -6.1968  -0.5969   6.7607  23.4731 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 156.3466     5.5123   28.36   &lt;2e-16 ***
Age          -1.1900     0.0902  -13.19   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 8.173 on 58 degrees of freedom
Multiple R-squared:  0.7501,    Adjusted R-squared:  0.7458 
F-statistic: 174.1 on 1 and 58 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Multiple R-squared is the sum square error? if the answer is no could explain the meaning of Multiple R-squared and Multiple R-squared</p>
"
"0.0749231094763201","0.0734364498908627","107865","<p>I'm investigating environmental effects (wind) on acoustic receiver detection probability for two types of transmitters using a binomial glmer. While my model analysis indicates that there's a significant effect between wind speed and transmitter type, graphical visualisation does not confirm this. If I'm correct, an interaction should demonstrate different regression slopes.</p>

<pre><code>m1 &lt;- glmer(cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth + 
               Receiver.depth + Water.temperature + Wind.speed + Transmitter + 
               Distance + Habitat + Replicate + (1 | Day) + (Distance | SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat + 
               Receiver.depth:Habitat + Wind.speed:Transmitter, data=df, family=binomial(link=logit))
</code></pre>

<p>The model summary is as follows:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth +  
    Receiver.depth + Water.temperature + Wind.speed + Transmitter +  
    Distance + Habitat + Replicate + (1 | Day) + (Distance |  
    SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat +      Receiver.depth:Habitat + Wind.speed:Transmitter
   Data: df

     AIC      BIC   logLik deviance df.resid 
  3941.9   4043.8  -1953.9   3907.9     2943 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-9.4911  0.0000  0.0000  0.5666  1.9143 

Random effects:
 Groups Name        Variance Std.Dev. Corr
 SUR.ID (Intercept)  0.33414 0.5781       
        Distance     0.09469 0.3077   1.00
 Day    (Intercept) 15.96629 3.9958       
Number of obs: 2960, groups:  SUR.ID, 20 Day, 6

Fixed effects:
                                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      3.20222    2.84984   1.124  0.26116    
Transmitter.depth               -0.35015    0.11794  -2.969  0.00299 ** 
Receiver.depth                  -0.57331    0.51919  -1.104  0.26949    
Water.temperature               -0.26595    0.11861  -2.242  0.02495 *  
Wind.speed                       1.31735    1.50457   0.876  0.38127    
TransmitterPT-04                -0.68854    0.08016  -8.590  &lt; 2e-16 ***
Distance                        -0.39547    0.09228  -4.286 1.82e-05 ***
HabitatFinger                   -0.23746    3.57783  -0.066  0.94708    
Replicate2                      -0.21559    0.08009  -2.692  0.00710 ** 
TransmitterPT-04:Distance       -0.27874    0.08426  -3.308  0.00094 ***
Transmitter.depth:HabitatFinger  0.73965    0.28612   2.585  0.00973 ** 
Receiver.depth:HabitatFinger     3.02083    0.74546   4.052 5.07e-05 ***
Wind.speed:TransmitterPT-04     -0.15540    0.06572  -2.364  0.01806 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Trnsm. Rcvr.d Wtr.tm Wnd.sp TrPT-04 Distnc HbttFn Rplct2 TPT-04: Tr.:HF Rc.:HF
Trnsmttr.dp -0.024                                                                               
Recevr.dpth -0.120 -0.267                                                                        
Watr.tmprtr  0.019 -0.159  0.007                                                                 
Wind.speed   0.130  0.073 -0.974  0.040                                                          
TrnsmtPT-04 -0.015  0.027  0.020 -0.018 -0.024                                                   
Distance     0.022 -0.080  0.151 -0.052 -0.141 -0.164                                            
HabitatFngr -0.813  0.010  0.241 -0.025 -0.253  0.009   0.029                                    
Replicate2  -0.067  0.033  0.377 -0.293 -0.394  0.010   0.085  0.103                             
TrnsPT-04:D -0.006  0.043 -0.007 -0.050 -0.003  0.516  -0.373  0.004  0.006                      
Trnsmtt.:HF  0.017 -0.352  0.021  0.055  0.049  0.026  -0.142  0.031 -0.088  0.025               
Rcvr.dpt:HF  0.103  0.189 -0.830  0.051  0.817 -0.036  -0.143 -0.224 -0.385 -0.003  -0.229       
Wnd.:TPT-04 -0.002  0.026 -0.015  0.003 -0.009  0.176  -0.114 -0.002  0.016  0.306  -0.008  0.014
</code></pre>

<p><img src=""http://i.stack.imgur.com/DFxyQ.png"" alt=""enter image description here""></p>

<p>A side question: I noticed a strong negative correlation between the intercept and a dichotome categorical predictor. I wonder if this causes any problems for my data analysis. All the covariates are centered and scaled for numerical stability during modelling.  </p>
"
"0.155186335471009","0.152107055195904","107951","<p>The question I'm asking is related to this question <a href=""http://stats.stackexchange.com/questions/105611/dealing-with-non-normal-distribution-in-big-datasets-when-do-we-throw-out-the/106301#106301"">here</a> and <a href=""http://stats.stackexchange.com/questions/105611/dealing-with-non-normal-distribution-in-big-datasets-when-do-we-throw-out-the/106301#106301"">here</a>. And I apologize for asking so many questions here, as I am thoroughly a stats novice and probably my MD thinking is clouding my ability to interpret my findings. </p>

<p>So, because of the input I've received and following some contemplation, I've decided to run a Cox proportional hazards model on this dataset regarding the treatment of patients and how this affects their time of wound healing. As the duration ""wound healing"" is affected by censoring, following @Glen_b and @Frank Harrell's input, I believe running a normal linear and/or Poisson regression model (as the days are counted and the Poisson's residual qqplots actually have a more linear/normal distribution) isn't quite feasible, because neither of these models actually account for the fact, that when patients are healed, they drop out of the ""study"" and thus are ""censored"". Therefore I've decided to explore Cox proportional hazards modelling a bit further. </p>

<p>Although not originally planned from a study endpoint perspective (long story), I've decided to chose the need for operative intervention (because the wound fails to heal) as the time to event here (also because I need an event variable in the <code>coxph</code> and <code>Surv</code> function in R). 
The study sample size is 4918 patients, of which 575 required operative treatment (11.7%). Now I'm not even sure if this is a high enough event rate as the independent variables are n = 14 and if I go by the ""rule of thumb"" of 10 events per predictor variable, I'd actually require 1400 events (although I read <a href=""http://aje.oxfordjournals.org/content/165/6/710.full"" rel=""nofollow"">here</a>, that this rule can sometimes be relaxed), but that's also one of the reasons why I'm posting my question here. </p>

<p>I've run a multivariable Cox proportional hazards model, adjusting for those variables that may influence the decision to ""treat"" a patient and or/wound healing dynamics as well as the treatment the patients received. </p>

<p>The output looks as follows: </p>

<pre><code>library(survival)
multicoxph&lt;-coxph(Surv(Timetoheal,Operation==""Yes"")~FA
+Age+Woundsurface+Mechanism+Sup+Mid+Deep)
summary(multicoxph)
Call:
coxph(formula = Surv(Timetoheal, Operation == ""Yes"") ~ FA + Age + 
    Woundsurface + Mechanism + Sup + Mid + Deep)

  n= 4877, number of events= 575 
   (41 observations deleted due to missingness)

                         coef exp(coef)  se(coef)      z Pr(&gt;|z|)    
FAInadequate        -0.225084  0.798449  0.089056 -2.527   0.0115 *  
Age                 -0.012107  0.987966  0.002401 -5.042 4.60e-07 ***
Woundsurface         0.042953  1.043889  0.017338  2.477   0.0132 *  
Mechanism1          -0.341706  0.710557  0.168422 -2.029   0.0425 *  
Mechanism2          -0.169782  0.843848  0.312529 -0.543   0.5870    
Mechanism3          -0.421703  0.655929  0.523798 -0.805   0.4208    
Mechanism4          -0.131178  0.877062  0.175603 -0.747   0.4551    
Mechanism5           0.071292  1.073895  0.313708  0.227   0.8202    
Mechanism6          -1.132178  0.322331  0.473741 -2.390   0.0169 *  
Mechanism7          -1.216408  0.296292  0.308236 -3.946 7.94e-05 ***
Mechanism8          -0.011187  0.988876  0.162881 -0.069   0.9452    
SupYes              -1.476173  0.228511  1.121847 -1.316   0.1882    
MidYes              -0.056176  0.945373  1.010415 -0.056   0.9557    
DeepYes              1.568644  4.800135  1.002629  1.565   0.1177    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

                    exp(coef) exp(-coef) lower .95 upper .95
FAInadequate           0.7984     1.2524   0.67057    0.9507
Age                    0.9880     1.0122   0.98333    0.9926
Woundsurface           1.0439     0.9580   1.00901    1.0800
Mechanism1             0.7106     1.4073   0.51079    0.9885
Mechanism2             0.8438     1.1850   0.45734    1.5570
Mechanism3             0.6559     1.5246   0.23496    1.8311
Mechanism4             0.8771     1.1402   0.62167    1.2374
Mechanism5             1.0739     0.9312   0.58067    1.9861
Mechanism6             0.3223     3.1024   0.12737    0.8157
Mechanism7             0.2963     3.3750   0.16194    0.5421
Mechanism8             0.9889     1.0112   0.71862    1.3608
SupYes                 0.2285     4.3762   0.02535    2.0598
MidYes                 0.9454     1.0578   0.13048    6.8497
DeepYes                4.8001     0.2083   0.67269   34.2526

Concordance= 0.746  (se = 0.017 )
Rsquare= 0.066   (max possible= 0.769 )
Likelihood ratio test= 331.9  on 14 df,   p=0
Wald test            = 240.3  on 14 df,   p=0
Score (logrank) test = 288.5  on 14 df,   p=0
</code></pre>

<p>Now if I understand my results correctly, this is telling me that the variable <code>FAInadequate</code> reduces the hazards for the event which in this case is ""requirement for an operation"", right? Now this is a bit bizarre, because if I perform simple comparative statistics, I find that FAInadequate patients have longer healing times and more operations as shown by these results: </p>

<pre><code>t.test(log(Timetoheal+0.5)~FA, var.equal=T)

    Two Sample t-test

data:  log(Timetoheal + 0.5) by FA
t = -7.9285, df = 4916, p-value = 2.724e-15
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.2773227 -0.1673666
sample estimates:
  mean in group Adequate mean in group Inadequate 
                2.338534                 2.560879 
</code></pre>

<p>and this</p>

<pre><code>require(gmodels)
CrossTable(Operation, FA, format=""SPSS"", chisq=T)

   Cell Contents
|-------------------------|
|                   Count |
| Chi-square contribution |
|             Row Percent |
|          Column Percent |
|           Total Percent |
|-------------------------|

Total Observations in Table:  4877 

             | FA 
    Operation|   Adequate | Inadequate |  Row Total | 
-------------|------------|------------|------------|
          No |      2583  |      1719  |      4302  | 
             |     2.764  |     3.835  |            | 
             |    60.042% |    39.958% |    88.210% | 
             |    91.143% |    84.141% |            | 
             |    52.963% |    35.247% |            | 
-------------|------------|------------|------------|
         Yes |       251  |       324  |       575  | 
             |    20.682  |    28.690  |            | 
             |    43.652% |    56.348% |    11.790% | 
             |     8.857% |    15.859% |            | 
             |     5.147% |     6.643% |            | 
-------------|------------|------------|------------|
Column Total |      2834  |      2043  |      4877  | 
             |    58.109% |    41.891% |            | 
-------------|------------|------------|------------|


Statistics for All Table Factors


Pearson's Chi-squared test 
------------------------------------------------------------
Chi^2 =  55.971     d.f. =  1     p =  7.354796e-14 

Pearson's Chi-squared test with Yates' continuity correction 
------------------------------------------------------------
Chi^2 =  55.29973     d.f. =  1     p =  1.03483e-13 


       Minimum expected frequency: 240.8704 
</code></pre>

<p>The fact of the matter is, that in plain ""medical"" terms, the longer it takes a wound to heal, the worse the outcome for the patient, so I would've originally expected the inadequate treatment to increase the hazards, especially in light of the simple t-test cross-tabulation results. Or could it be, that in light of this, running a proportional hazards function is incorrect and I should be looking at cumulative hazards? If that is the case how can I run a cumulative hazards function in R?  Further, I'm a bit concerned, that the likelihood ratio, logrank and Wald test all resulted in p-values = 0. Finally, how would you ideally chose to cross-validate this model and adjust for the slightly ""too low"" event per variable rate? </p>

<p>I'm sorry for all the questions, but this has been thoroughly been driving me nuts over the last couple of days, as I can't really wrap my head around the output for some reason. </p>

<p>I appreciate any help greatly. </p>

<p>Thanks. </p>
"
"0.0578044339088637","0.0679889413649005","108181","<p>So, this is a question that I wish to ask. I was reading about linear regression and stuff. I understood that linear regression basically associates two variables with each other. I have a doubt on its applicability if it can be applied somehow in my case as well.</p>

<p>So, assume I have 10 files. Each file basically contains 1000's of rows and 3 columns. The first two columns basically represent the nodes (as in a graph) and the third column gives a rule that the nodes follow. Let us assume the rule as any sequence of 1's and 0's mixed together.</p>

<p>Sample:</p>

<pre><code> Node 1     Node 2     Rule
   2          3        110
   3          4        110
..........................
</code></pre>

<p>Another file contains nodes for another rule. Is there any way I can apply some regression model on any of the parametres in R?</p>
"
"0.0424774103841449","0.0555127381653369","108256","<p>I wish to test my time series data for volatility clustering, i.e. conditional heteroskedasticity.</p>

<p>So far, I have used the ACF test on the squared and absolute returns of my data, as well as the Ljung-Box test on the squared data (i.e. McLeod.Li.test).</p>

<p>In a recent paper (<a href=""http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1771862"" rel=""nofollow"">http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1771862</a>, the test is reported on page 8) co-authored by a well-known researcher, they have employed the White test (<a href=""http://ideas.repec.org/a/ecm/emetrp/v48y1980i4p817-38.html"" rel=""nofollow"">http://ideas.repec.org/a/ecm/emetrp/v48y1980i4p817-38.html</a>) to directly test for heteroskedasticity.</p>

<p>I have tried the same approach, however was unable to do so.
From my understanding, the White test needs residual variance (usually from a linear regression model) as an input.</p>

<p>Now my question is: How did the researchers perform the White test? I do not understand which inputs they used for their White test.</p>

<p>While searching for solutions, I have found the sandwich package which uses the vcovHC and vcovHAC functions to estimate a heteroskedasticity-consistent covariance matrix, however the input is also a fitted linear regression model..</p>
"
"0.0490486886395286","0.0480754414848157","108284","<p>Here is the R code that produced the output below:</p>

<pre><code>library(caret)
set.seed(934)
fitControl &lt;- trainControl(method= ""repeatedcv"", number=10, repeats=10)
logitfit &lt;- train(z~ a+b+c+d+e, data=train, method=""glm"", trControl = fitControl)
logitfit
</code></pre>

<p>Consider the following output from R:</p>

<pre><code>Generalized Linear Model 

900 samples
 20 predictors
  2 classes: '0', '1' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 

Summary of sample sizes: 846, 847, 846, 845, 847, 846, ... 

Resampling results

  Accuracy  Kappa  Accuracy SD  Kappa SD
  0.80     0.58  0.076       0.18    
</code></pre>

<p>This is a logistic regression model using a 10-fold cross validation repeated 10 times. Does the output above indicate that the average accuracy is 0.80? </p>
"
"0.0490486886395286","0.0480754414848157","108302","<p>I am running multiple linear regression with R.</p>

<pre><code>mod=lm(varP ~ var1 +var2+var3+var4)
</code></pre>

<p>The table is:</p>

<pre><code>all:
lm(formula = varP ~ var1 + var2 + var3 + var4)

Residuals:
    Min      1Q  Median      3Q     Max     
-4.9262 -0.6985  0.0472  0.7319  4.3305 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.700823   0.084737   8.271 1.45e-15 ***
var1      1.080172   0.175348   6.160 1.59e-09 ***
var2     -0.057803   0.007777  -7.432 5.25e-13 ***
var3     -9.924772   4.268235  -2.325   0.0205 *  
var4     -0.015104   0.001290 -11.710  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.139 on 460 degrees of freedom
Multiple R-squared:  0.657, Adjusted R-squared:  0.654 
F-statistic: 220.3 on 4 and 460 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>it means that my model explains 65.4% of the variance.
But now, I would like to determine the importance of each predictor.</p>

<p>I was using: </p>

<pre><code>lm.sumSquares(mod) 
</code></pre>

<p>Is dR-sqr relevant to interpret this importance ?</p>

<pre><code>              SS       dR-sqr pEta-sqr  df        F p-value
(Intercept)   88.73054 0.0510   0.1294   1  68.4015  0.0000
var4         177.88026 0.1022   0.2296   1 137.1262  0.0000
var2          71.65234 0.0412   0.1072   1  55.2361  0.0000
var1          49.22579 0.0283   0.0762   1  37.9477  0.0000
var3           7.01377 0.0040   0.0116   1   5.4069  0.0205

Error (SSE)  596.71237     NA       NA 460       NA      NA    
Total (SST) 1739.76088     NA       NA  NA       NA      NA
</code></pre>
"
"0.0400480865731637","0.039253433598943","108313","<p>I have the following data structures and would like to know your opinion which regression models are most suitable (and available in R)</p>

<p>model 1:
- 20,000 cases
- nested in 500 spatial units (only intercept variations are needed)
- the response variable is censored (only values >1) are possible
- 10 independent variables (only linear main effects are needed)</p>

<p>model 2:
- 20,000 cases
- nested in 500 spatial units (only intercept variations are needed)
- the response variable is categorized in 3 classes where class 1 &lt; class 2 &lt; class 3
- 10 independent variables (only linear main effects are needed)</p>
"
"0.105957277556576","0.0964366217361766","108374","<p>I have a monthly time series with an intervention and I would like to quantify the effect of this intervention on the outcome. I realize the series is rather short and the effect is not yet concluded.</p>

<p><strong>The Data</strong></p>

<pre><code>  cds&lt;- structure(c(2580L, 2263L, 3679L, 3461L, 3645L, 3716L, 3955L, 
    3362L, 2637L, 2524L, 2084L, 2031L, 2256L, 2401L, 3253L, 2881L, 
    2555L, 2585L, 3015L, 2608L, 3676L, 5763L, 4626L, 3848L, 4523L, 
    4186L, 4070L, 4000L, 3498L), .Dim = c(29L, 1L), .Dimnames = list(
        NULL, ""CD""), .Tsp = c(2012, 2014.33333333333, 12), class = ""ts"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/lNOEk.jpg"" alt=""enter image description here""></p>

<p><strong>The methodology</strong></p>

<p>1) The pre-intervention series (up until October 2013) was used with the <code>auto.arima</code> function. The model suggested was ARIMA(1,0,0) with non-zero mean. The ACF plot looked good.</p>

<pre><code>pre&lt;-window(cds,start = c(2012,01), end=c(2013,09))

mod.pre&lt;-auto.arima(log(pre))

Coefficients:
         ar1  intercept
      0.5821     7.9652
s.e.  0.1763     0.0810

sigma^2 estimated as 0.02709:  log likelihood=7.89
AIC=-9.77   AICc=-8.36   BIC=-6.64
</code></pre>

<p>2) Given the plot of the full series, the pulse response was chosen below, with T = Oct 2013,</p>

<p><img src=""http://i.stack.imgur.com/YU3nB.jpg"" alt=""enter image description here""></p>

<p>which according to cryer and chan can be fit as follows with the arimax function:</p>

<pre><code>   mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
            xtransf=data.frame(Oct13=1*(seq(cds)==22)),
            transfer=list(c(1,1))
          )

    mod.arimax


Series: log(cds) 
ARIMA(1,0,0) with non-zero mean 

Coefficients:
         ar1  intercept  Oct13-AR1  Oct13-MA0  Oct13-MA1
      0.7619     8.0345    -0.4429     0.4261     0.3567
s.e.  0.1206     0.1090     0.3993     0.1340     0.1557

sigma^2 estimated as 0.02289:  log likelihood=12.71
AIC=-15.42   AICc=-11.61   BIC=-7.22
</code></pre>

<p>The residuals from this appeared OK:</p>

<p><img src=""http://i.stack.imgur.com/wvdXD.jpg"" alt=""enter image description here""></p>

<p>The plot of fitted and actuals:</p>

<pre><code>plot(fitted(mod.arimax),col=""red"", type=""b"")
lines(window(log(cds),start=c(2012,02)),type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/kJ1pj.jpg"" alt=""enter image description here""></p>

<p><strong>The Questions</strong></p>

<p>1) Is this methodology correct for intervention analysis?</p>

<p>2) Can I look at estimate/SE for the components of the transfer function and say that the effect of the intervention was significant?</p>

<p>3) How can one visualize the transfer function effect (plot it?)</p>

<p>4) Is there a way to estimate how much the intervention increased the output after 'x' months? I guess for this (and maybe #3) I am asking how to work with an equation of the model - if this were simple linear regression with dummy variables (for example) I could run scenarios with and without the intervention and measure the impact - but I am just unsure how to work this this type of model.</p>

<p><strong>ADD</strong></p>

<p>Per request, here are the residuals from the two parametrizations.</p>

<p>First from the fit:</p>

<pre><code>fit &lt;- arimax(log(cds), order = c(1,0,0), 
              xtransf = data.frame(Oct13a = 1*(seq_along(cds)==22), Oct13b = 1*(seq_along(cds)==22)),
              transfer = list(c(0,0), c(1,0)))

plot(resid(fit), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/sqMZN.jpg"" alt=""enter image description here""></p>

<p>Then, from this fit</p>

<pre><code>mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
                   xtransf=data.frame(Oct13=1*(seq(cds)==22)),
                   transfer=list(c(1,1))
)

mod.arimax
plot(resid(mod.arimax), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/DjAyu.jpg"" alt=""enter image description here""></p>
"
"0.162746503992576","0.164208893982226","109464","<p>I am new to regression and having problem in solving Heteroscedasticity in OLS. Have done lots of homework and test before seeking your advice. Sharing the background and what I have done to solve the problem. Hope you can share your thoughts if my approach was correct.</p>

<p><strong>Objectives:</strong></p>

<ol>
<li>To find the relationship (model) between an explanatory variable (x) and an explained variable (y) using OLS regression.</li>
<li>if a model (relationship) is found, its usefulness and accuracy of prediction will be studied.</li>
</ol>

<p><strong>Dataset (Cross-sectional):</strong></p>

<ol>
<li>Have 4 datasets, with each 350 sample size.</li>
<li>Each dataset obtained using different intensity of experiment and this is already captured by the explanatory variable in x.</li>
<li>Due to the heterogenity of data, not possible to lump all into a single dataset.</li>
</ol>

<p><strong>Requirement:</strong></p>

<p>One common and statistically acceptable model for all the 4 datasets using OLS</p>

<p><strong>Steps Followed:</strong></p>

<ol>
<li><p>Explanatory Analysis: Found Non-linear relationship </p></li>
<li><p>As intending to use OLS, did 3 transformations of variables in attempt to have linearity:
a) ln(x) ~ ln(y);
b) ln(x) ~ y;
c) x ~ ln(y).
<strong>Note:</strong> Kept d) x ~ y as benchmark</p></li>
<li><p>Did heteroscedasticity test using Breusch-Pagan (BP) test in R for 2(a)-(d) for all the datasets in attempt to find valid model(s).
On the best case i.e 2b), only 2 out of 4 datasets passed the BP test (p-value>0.05)</p></li>
<li><p>As the aim is to have one common model for all the 4 datasets, another variable transformation is done using Tukey's Ladder of Transformation in attempt to have homoscedasticity:
a) ? ? {-2,-1,-0.5, 0.5, 1, 2} is used for x/y/x and y for each of the models in 2(a)-(d). Have total of 64 models (16 x 4) to consider. X and Y refer to the transformed x and y;
b) Now have 2 models passed BP test for 3 out of 4 datasets in the best case;
c) The one that failed has p-value &lt;2.20E-16.</p></li>
<li><p>[deadlock unable to find one valid model that passes all the 4 datasets]</p></li>
<li><p>Proceeded to take the two valid models in Step 4 and done inference Test:
a) the p-values for t-test and F-test are below 0.05 for all the 4 datasets;
b) R-square are above 0.9402 for all the 4 datasets.</p></li>
<li><p>Did cross validation and selected the best model using the smallest mean square error against the two ""valid"" models. Did back transformation on the original scale first before the selection is done so that its apple to apple data comparison. The mean average percentage error for the best model is below 10%</p></li>
<li><p>Now tried to use the best model for prediction:
a) Selected 20 random x values which were not part of the dataset;
b) Predicted y and compared it against Measured y;
c) the  mean average percentage error is below 8% and within the model's mean average percentage error i.e below 10%.</p></li>
</ol>

<p><strong>The problem:</strong></p>

<p>With the steps above I am unable to get a model that passes the heteroscedasticity test all the 4 datasets. Have I done anything incorrectly or is there anything more can be done in Step 4? </p>

<p>Believe mis-specification issue has duly been attended. Not intending to use GLS as I need to use .OLS</p>

<p>I have used heteroscedasticity robust standard errors as a remedy of heteroscedasticity on the one dataset that failed BP test per the Youtube below.
Refer - <a href=""https://www.youtube.com/watch?v=hFoDDwTF4KY"" rel=""nofollow"">https://www.youtube.com/watch?v=hFoDDwTF4KY</a></p>

<p>The standard error increased and t-value decreased for Y for the HC3 corrected dataset. 
But the Y= a  + b X model remain the same.</p>

<p>Is it sufficient to show the p-value for t-test and F-test for the corrected dataset are still below 0.05 hence its ok to use the same Y= a+bX though it failed the BP test earlier?</p>

<p>Hope you can share your thoughts as I am new to regression. </p>

<p>Using many reference books to learn such as </p>

<ol>
<li>Introduction to Econometrics by Wooldridge</li>
<li>Basic Econometrics by Gujerati</li>
<li>Regression Analysis by Example by Chatterjee</li>
</ol>

<p><strong>Original:</strong></p>

<pre><code>Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -0.612116   0.009006  -68.76   &lt;2e-16 ***
Y                     5.955984   0.039653  145.65   &lt;2e-16 ***
---

Residual standard error: 0.04138 on 348 degrees of freedom
Multiple R-squared:  0.9832,    Adjusted R-squared:  0.9831 
F-statistic: 2.092e+04 on 1 and 348 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Heteroskedasticity Robust Standard Errors corrected using HC3:</strong></p>

<pre><code>Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -0.61212    0.01767  -33.77   &lt;2e-16 ***
Y                     5.95598    0.08432   69.12   &lt;2e-16 ***
---

Residual standard error: 0.04138 on 348 degrees of freedom
Multiple R-squared:  0.9832,    Adjusted R-squared:  0.9831 
F-statistic:  4640 on 1 and 348 DF,  p-value: &lt; 2.2e-16

Note: Heteroscedasticity-consistent standard errors using adjustment hc3 
</code></pre>

<p>Thanks</p>
"
"0.0490486886395286","0.0480754414848157","109673","<p>I have built a cox model in R using the coxph function in the survival package, and now I need to replicate the model in SQL for scoring.  From my understanding, the model has the form described on the bottom of page 2 of this document, <a href=""http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf"" rel=""nofollow"">http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf</a>, which gives it semi-parametric flexibility. Since there is an unspecified alpha term, I cannot just take the coefficients and use the model like a typical linear model or generalized linear model (and exponentiate). There are ways to estimate this alpha term, and I believe this added term to the hazard is needed to specify the complete model.  If this is the case, how do I get my hands on this alpha term?</p>
"
"0.0400480865731637","0.0196267167994715","109925","<p>I'm trying to develop a non-linear model, but I'd like to have the values for the parameters vary by group. To give you an example, a section of my data (just random numbers here) looks like:</p>

<pre><code>  MONTH FUTURE.PC FUTURE.SALES
1   APR        35         1498
2   APR        22         1124
3   MAY        24          744
4   MAY        45          453
5   MAY        13         1024  
6   JUN        26          689
</code></pre>

<p>Say I wanted to make a non-linear model that looked like</p>

<pre><code>nls(FUTURE.SALES ~ a + b * FUTURE.PC ^ c)
</code></pre>

<p>Is there a way to change the values of $a$, $b$, and $c$ based on which month the record belongs to? Alternatively, is there a way to incorporate factor variables in non-linear regression in $R$? </p>
"
"0.0424774103841449","0.0555127381653369","110005","<p>I've seen a few similar questions about constraining coefficients so they sum to 1, but I'm not sure if there's a simple change in these approaches to allow the sum to be anything in [0,1]. I need to implement this in R.</p>

<p><a href=""http://stats.stackexchange.com/questions/21565/how-do-i-fit-a-constrained-regression-in-r-so-that-coefficients-total-1"">How do I fit a constrained regression in R so that coefficients total = 1?</a></p>

<p><a href=""http://stats.stackexchange.com/questions/12484/constrained-linear-regression-through-a-specified-point"">Constrained linear regression through a specified point</a></p>

<p>The data sets I'm doing the regression on vary in size, so my model looks something like this:
$Y = {\pi}_{1}{X}_{1}+{\pi}_{2}{X}_{2}+...+{\pi}_{n}{X}_{n}+\epsilon \quad s.t. \quad \sum_{i=1}^{n}{\pi}_{i} \le 1$ and ${\pi}_{i} \ge 0 \quad \forall i$</p>

<p>Before I realized I needed these constraints, I was using quadratic programming, finding $min||Y - (\sum_{i=1}^{n}{\pi}_{i}{X}_{i})||^{2}$ </p>

<p>Thanks!</p>

<p>Edit: I'm not sure that what I'm doing actually has any validity, I'm just experimenting at the moment. Essentially it's sort of a mixture model where $Y$ has contributions from ${X}_{1}...{X}_{n}$ and then from some other source, and I want to see if I can estimate what proportion of Y comes from any X. The reason they don't necessarily sum to 1 is because of that unknown other source. Again, I'm just experimenting, so not sure if this will even work, just curious.  </p>
"
"NaN","NaN","110091","<p>How can I calculate the robust standard error of the predicted <em>y</em> from a linear regression model in R? Any suggestion is appreciated.</p>
"
"0.0895502439463906","0.0877733458775107","110136","<p>I'm working on a prediction model for a continuous variable (amount of medicine injected) .I use R for modeling.My project flow is to multiply the prediction of a glm (logistic regression) model that is used to predict 0/1 if a medicine was injected at all with an lm (linear regression) model that is used predict amount of medicine injected - this model works rather good In R .My problem is that when I move this model to MSSQL I get different values for the prediction (i.e. for a random row the value in the R is 400 and in SQL the value for the same row is 640.The model in SQL is made by attaching the models coefficiants from the glm model to produce the glm prediction values and then multiplying it with the lm model prediction values. I don't understand why there is a difference if I use the same coefficients?</p>

<p>Here is the code for the lm and glm models in r:</p>

<pre><code>d7_lm&lt;-lm(Ttl_Inject~UserSource+IsNewIndividual+IsCross,data=train)
d7_glm&lt;-glm(Is_Injected~UserSource+IsNewIndividual+IsCross,data=train)
</code></pre>

<p>Here is a part of the r code for the prediction:</p>

<pre><code>demo$d7_lm_pred&lt;-predict(d7_lm,newdata=demo,type='response')
    demo$d7_glm_pred_response&lt;-ifelse(predict(d7_glm,newdata=demo,type='response')&gt;0.5,1,0)
demo$glm01_lm_response&lt;-demo$d7_lm_pred*demo$d7_glm_pred_response # this is used for a container of the prediction model's values.
</code></pre>

<p>Here is a part of the SQL code : </p>

<pre><code>select TOP 1000*, InjectionAmount_pred= (-2.213e -1.180e+00*(case when User='IAF' then 1 else 0 end)-1.665e+00*(case when UserSource='Viral' then 1 else 0 end)
+IsNewIndividual  *  1.167e+00+IsCross )

* IIF((1 / (1 + EXP(-(-1.346e-03+1.140e-02*(case when UserSource='IAF' then 1 else 0 end) -2.975e-03*(case when UserSource='Viral' then 1 else 0 end)
-IsNewIndividual  * 1.503e-04 +IsCross ))))&gt;0.5,1,0) 
</code></pre>
"
"0","0.0277563690826684","110236","<p>I have a survey data which has one dependent variable (""Overall experience"") and several independent variables(Quality of food, creativity of menu etc.). The response scorecard for both dependent and independent variables are as follows:</p>

<p>Excellent   5
Very Good   4
Good        3
Fair        2
Poor        1 </p>

<p>As per my understanding and from what I have read, I cannot run a simple linear regression and thus I have opted for logistic regression.
Can anyone guide me in the right direction regarding logistic regression using R. </p>
"
"0.0895502439463906","0.0877733458775107","110301","<p>I made this linear regression that shows how well estimated animal locations (longitude) predict actual animal locations. </p>

<pre><code>estimate &lt;- c(-1.514276, -1.513683, -1.514253, -1.514207, -1.513557, -1.513634, -1.513870, -1.511210, -1.511552, -1.511772, -1.511580, -1.511802, -1.509500, -1.510037, -1.510214)

actual &lt;- c(-1.514255, -1.514053, -1.514527, -1.514223, -1.513672, -1.513729, -1.513934, -1.511118, -1.511567, -1.511658, -1.511585, -1.511830, -1.509438, -1.509843, -1.510080)

lm_longitude &lt;- lm(actual ~ estimate)
summary(lm_longitude)

Call:
lm(formula = actual ~ estimate)

Residuals:
       Min         1Q     Median         3Q        Max 
-2.630e-04 -3.825e-05  8.945e-06  6.530e-05  1.645e-04 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.09325    0.02706   3.445  0.00435 ** 
estimate     1.06167    0.01790  59.325  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.000112 on 13 degrees of freedom
Multiple R-squared:  0.9963,    Adjusted R-squared:  0.996 
F-statistic:  3519 on 1 and 13 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>As you can see, estimated locations are very good predictors for actual locations. I was initially alarmed at the residuals vs fitted values plot. It appears to shows residuals that are correlated with the fitted values:</p>

<pre><code>library(ggplot2)
df_lm_longitude &lt;- ggplot2::fortify(lm_longitude) 
ggplot(df_lm_longitude, aes(.fitted, .resid)) + geom_point() + stat_smooth()
</code></pre>

<p><img src=""http://i.stack.imgur.com/0c7Hk.jpg"" alt=""enter image description here""></p>

<p>But change the scale of the y axis, and residuals vs fitted values plot looks perfect:</p>

<pre><code>ggplot(df_lm_longitude, aes(.fitted, .resid)) + geom_point() + stat_smooth() + ylim(-0.01, 0.01)
</code></pre>

<p><img src=""http://i.stack.imgur.com/NHres.jpg"" alt=""enter image description here""></p>

<p>So one of the assumptions of linear regression is that residuals should not be correlated with fitted values. In the model above, the residuals are correlated with the fitted values at a large scale. But zoom out to a small scale, and residuals are not correlated at all?</p>

<p>What resolution should I be using for y axis in residuals vs fitted values plot?</p>
"
"0.0506572678011219","0.0620651280774201","110469","<p>So the background is that the I collected yield data for past 5-6 decades and location from where I collected yield data had high yielding varieties introduced over time. I am looking at the relationship between yield and rainfall but this introduction of HYV might affect the true impact of monsoon on yield and therefore I am detrending the data to remove the effect of HYV.</p>

<p>I did a linear regression of yield against time in R:</p>

<pre><code>mdl1 &lt;- lm(yield ~ time, data=data)
</code></pre>

<p>and then removed the linear trend by taking the residuals of the above regression: </p>

<pre><code>yield.res &lt;- resid(mdl1)
</code></pre>

<p>Now I am using these residuals for my subsequent analysis. For example, the relationship between yield and rainfall is: </p>

<pre><code> mdl2 &lt;- lm(yield.res ~ rain, data=data)
</code></pre>

<p>In this case, do my <code>yield.res</code> have to be normally distributed before I do this regression? If yes, what sort of transformation do I need to use? Since <code>yield.res</code> consists of both negative and positive numbers, I am slightly confused how to go about it.</p>
"
"NaN","NaN","110570","<p>For my survey data analysis, I ran an Ordinal Logistic regression using the 'polr' function.
The summary of the regression is as follows:</p>

<p><img src=""http://i.stack.imgur.com/csKGq.png"" alt=""enter image description here""></p>

<p>My question is:</p>

<ol>
<li>Do I need to standardize my  beta values?</li>
<li>If so, is lm.beta the right approach (as per my understanding, it only works for linear models)? And if not, could you please provide a method to do so.</li>
</ol>

<p>Thanks everyone!</p>
"
"NaN","NaN","110597","<p>My question is simple: is there a function in <code>R</code> which estimates the linear regresion model in a similar fashion as <code>lm</code>, but only using the means, variances, and covariance (correlations), i.e. the sufficient statistics? I am looking for a function to which I can input these statistics (plus sample size) and it returns regression coefficients and tests.</p>
"
"0.0693653206906364","0.0679889413649005","111457","<p>I ran two logistic regression models, one with a dataset including outliers and one without outliers, with multiple predictors.</p>

<p>I checked each model's fit with the le Cessie â€“ van Houwelingen â€“ Copas â€“ Hosmer unweighted sum of squares test for global goodness of fit from the rms package in R (following advice <a href=""http://www.r-bloggers.com/veterinary-epidemiologic-research-glm-evaluating-logistic-regression-models-part-3/"" rel=""nofollow"">here</a>).</p>

<pre><code>model1 &lt;- lrm(y ~ a + b + c + d, data1, method = ""lrm.fit"", model = TRUE, x = TRUE, y = TRUE, linear.predictors = TRUE, se.fit = FALSE)
residuals(model1, type = ""gof"")
</code></pre>

<p>For the model with outliers the p value was close to 0, indicating a lack of fit. For the model without outliers p value was 0.52, indicating that my model was not incorrect.</p>

<p>I then ran 10-fold cross validation for both models with DAAG package and was surprised to get identical (poor) accuracy results for both = 0.56</p>

<pre><code>cv10&lt;-CVbinary(model1,nfolds=10)
</code></pre>

<p>I thought that the model created using the dataset without outliers, having a much better fit, will give me higher accuracy. Am I missing something here? I will be grateful for your help.</p>
"
"0.02831827358943","0.0277563690826684","111559","<p>In R, when I have a (generalized) linear model (<code>lm</code>, <code>glm</code>, <code>gls</code>, <code>glmm</code>, ...), how can I test the coefficient (regression slope) against any other value than 0? In the summary of the model, t-test results of the coefficient are automatically reported, but only for comparison with 0. I want to compare it with another value.</p>

<p>I know I can use a trick with reparametrizing <code>y ~ x</code> as <code>y - T*x ~ x</code>, where <code>T</code> is the tested value, and run this reparametrized model, but I seek simpler solution, that would possibly work on the original model.</p>
"
"0.141696211625305","0.144226324454447","112241","<p><strong>Summary:</strong> Is there any statistical theory to support the use of the $t$-distribution (with degrees of freedom based on the residual deviance) for tests of logistic regression coefficients, rather than the standard normal distribution?</p>

<hr>

<p>Some time ago I discovered that when fitting a logistic regression model in SAS PROC GLIMMIX, under the default settings, the logistic regression coefficients are tested using a $t$ distribution rather than the standard normal distribution.$^1$ That is, GLIMMIX reports a column with the ratio $\hat{\beta}_1/\sqrt{\text{var}(\hat{\beta}_1)}$ (which I will call $z$ in the rest of this question), but also reports a ""degrees of freedom"" column, as well as a $p$-value based on assuming a $t$ distribution for $z$ with degrees of freedom based on the residual deviance -- that is, degrees of freedom = total number of observations minus number of parameters. At the bottom of this question I provide some code and output in R and SAS for demonstration and comparison.$^2$</p>

<p>This confused me, since I thought that for generalized linear models such as logistic regression, there was no statistical theory to support the use of the $t$-distribution in this case. Instead I thought what we knew about this case was that</p>

<ul>
<li>$z$ is ""approximately"" normally distributed;</li>
<li>this approximation might be poor for small sample sizes;</li>
<li>nevertheless it <em>cannot</em> be assumed that $z$ has a $t$ distribution like we can assume in the case of normal regression.</li>
</ul>

<p>Now, on an intuitive level, it does seem reasonable to me that if $z$ is approximately normally distributed, it might in fact have some distribution that is basically ""$t$-like"", even if it is not exactly $t$. So the use of the $t$ distribution here does not seem crazy. But what I want to know is the following:</p>

<ol>
<li>Is there in fact statistical theory showing that $z$ really does follow a $t$ distribution in the case of logistic regression and/or other generalized linear models?</li>
<li>If there is no such theory, are there at least papers out there showing that assuming a $t$ distribution in this way works as well as, or maybe even better than, assuming a normal distribution?</li>
</ol>

<p>More generally, is there any actual support for what GLIMMIX is doing here other than the intuition that it is probably basically sensible?</p>

<p>R code:</p>

<pre><code>summary(glm(y ~ x, data=dat, family=binomial))
</code></pre>

<p>R output:</p>

<pre><code>Call:
glm(formula = y ~ x, family = binomial, data = dat)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.352  -1.243   1.025   1.068   1.156  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.22800    0.06725   3.390 0.000698 ***
x           -0.17966    0.10841  -1.657 0.097462 .  
---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1235.6  on 899  degrees of freedom
Residual deviance: 1232.9  on 898  degrees of freedom
AIC: 1236.9

Number of Fisher Scoring iterations: 4
</code></pre>

<p>SAS code:</p>

<pre><code>proc glimmix data=logitDat;
    model y(event='1') = x / dist=binomial solution;
run;
</code></pre>

<p>SAS output (edited/abbreviated):</p>

<pre><code>The GLIMMIX Procedure

               Fit Statistics

-2 Log Likelihood            1232.87
AIC  (smaller is better)     1236.87
AICC (smaller is better)     1236.88
BIC  (smaller is better)     1246.47
CAIC (smaller is better)     1248.47
HQIC (smaller is better)     1240.54
Pearson Chi-Square            900.08
Pearson Chi-Square / DF         1.00


                       Parameter Estimates

                         Standard
Effect       Estimate       Error       DF    t Value    Pr &gt; |t|

Intercept      0.2280     0.06725      898       3.39      0.0007
x             -0.1797      0.1084      898      -1.66      0.0978
</code></pre>

<p>$^1$Actually I first noticed this about <em>mixed-effects</em> logistic regression models in PROC GLIMMIX, and later discovered that GLIMMIX also does this with ""vanilla"" logistic regression.</p>

<p>$^2$I do understand that in the example shown below, with 900 observations, the distinction here probably makes no practical difference. That is not really my point. This is just data that I quickly made up and chose 900 because it is a handsome number. However I do wonder a little about the practical differences with small sample sizes, e.g. $n$ &lt; 30.</p>
"
"0.02831827358943","0.0277563690826684","112263","<p>I have four variables, two are categorical and two are numeric:</p>

<pre><code>a&lt;-c(""yes"",""yes"",""no"",""no"",""no"",NA,""yes"",""no"")
b&lt;-c(""high"",""low"",""medium"",""medium"",""medium"",""low"",NA,NA)
c&lt;-c(12,23,23,12,23,34,12,NA)
d&lt;-c(1.2,1.3,4.5,3.4,NA,5.4,9.4,7.4)

df&lt;-data.frame(a,b,c,d)
</code></pre>

<p>I want to replace the <code>NA</code> values in each variable by using a model fit.
For example:  </p>

<pre><code>a~b+c+d; b~a+c+d 
</code></pre>

<p>and so on.</p>

<p>I know that if all the variables are numeric, a simple multiple linear regression model can be built to estimate the value of the <code>NA</code> data.  However, I am not sure how I can achieve this if the response variable is categorical. </p>
"
"0.11327309435772","0.104086384060007","112380","<p>I have a problem to build and to explain the linear multiple regression.</p>

<p>I have a data set called <code>Cars93</code> with 26 variables (numeric and not numeric) and 93 observations. This data set you can find in the <code>MASS</code> R package. I want to build a linear regression model for predicting the price of a car. Then I have to do a variable selection (forward and backward stepwise) using AIC and BIC in R.  My knowledge in R is too little thatÂ´s why I have some problems. I really hope you can help me! </p>

<p>1) The data set has some missing values </p>

<p>I just solved this problem like this:</p>

<pre><code> Cars93 [! Complete.cases (Cars93)] 
 Cars93new &lt;- na.omit (Cars93) 
 Cars93 = Cars93new 
</code></pre>

<p>I think some informations are going lost. Is there another solution to eliminate the missing values? </p>

<p>2) Some variables from the dataset are not numeric
I tried to convert these values into numerical values like this:</p>

<pre><code>Cars93 $ airbags = factor (Cars93 $ airbags, labels = c (2,1,0)) 
Cars93 $ airbags 
  [1] 0 2 1 2 1 1 1 1 1 1 2 0 1 2 0 1 2 2 1 0 1 1 1 1 0 2 0 0 0 1 1 1 1 0 1 1 2 2 
[39] 0 0 0 0 1 1 2 2 2 0 0 1 1 2 1 0 0 1 1 1 1 0 1 1 0 0 0 2 0 2 1 1 0 0 1 0 1 1 
[77] 1 0 0 0 1 2 
Levels: 2 1 0 
</code></pre>

<p>I did the same with other not numeric variables.</p>

<p>Afterwards I tried to build a linear model regression with all variables:</p>

<pre><code>Modell=lm(Price~Horsepower+EngineSize+MPG.city+MPG.highway+Rev.per.mile+Man.trans.avail+Fuel.tank.capacity+Passengers+Length+Wheelbase+Width+Turn.circle+Weight+Rear.seat.room+Luggage.room+Origin+AirBags+Type+Cylinders+Weight+PRM)
summary(Modell)
</code></pre>

<p>But the output does make any sense:</p>

<pre><code>Call:
lm(formula = Price ~ Horsepower + EngineSize + MPG.city + MPG.highway + 
    Rev.per.mile + Man.trans.avail + Fuel.tank.capacity + Passengers + 
    Length + Wheelbase + Width + Turn.circle + Weight + Rear.seat.room + 
    Luggage.room + Origin + AirBags + Type + Cylinders + Weight + 
    RPM)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.4893 -2.3664 -0.0062  2.1180 18.1112 

Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        81.335018  37.993697   2.141 0.036826 *  
Horsepower          0.123535   0.049355   2.503 0.015372 *  
EngineSize         -0.615828   3.047223  -0.202 0.840602    
MPG.city           -0.392888   0.470385  -0.835 0.407259    
MPG.highway         0.013646   0.428978   0.032 0.974740    
Rev.per.mile        0.001498   0.002511   0.597 0.553206    
Man.trans.availYes -1.600967   2.480497  -0.645 0.521387    
Fuel.tank.capacity  0.462731   0.572169   0.809 0.422219    
Passengers          0.615593   1.823089   0.338 0.736925    
Length              0.074875   0.130511   0.574 0.568547    
Wheelbase           0.740146   0.343760   2.153 0.035796 *  
Width              -1.745792   0.571082  -3.057 0.003473 ** 
Turn.circle        -0.695287   0.415708  -1.673 0.100203    
Weight             -0.004068   0.006255  -0.650 0.518171    
Rear.seat.room      0.101150   0.420050   0.241 0.810619    
Luggage.room        0.176183   0.367199   0.480 0.633306    
Originnon-USA       1.881047   1.762845   1.067 0.290696    
AirBagsDriver only -3.294049   1.888346  -1.744 0.086777 .  
AirBagsNone        -8.535307   2.289737  -3.728 0.000464 ***
TypeLarge          -1.692122   3.999146  -0.423 0.673887    
TypeMidsize         2.684947   2.639047   1.017 0.313504    
TypeSmall           1.913341   2.896592   0.661 0.511710    
TypeSporty          4.686129   3.268426   1.434 0.157407    
Cylinders4         -3.126727   4.554852  -0.686 0.495360    
Cylinders5         -4.732933   7.498898  -0.631 0.530605    
Cylinders6          0.224795   5.695793   0.039 0.968664    
Cylinders8          4.020677   7.255406   0.554 0.581755    
RPM                -0.002778   0.002450  -1.134 0.261805    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 5.009 on 54 degrees of freedom
  (11 observations deleted due to missingness)
Multiple R-squared:  0.8313,    Adjusted R-squared:  0.747 
F-statistic: 9.859 on 27 and 54 DF,  p-value: 1.014e-12
</code></pre>

<p>I have 4 times ""Cylinders"" and ""Type"" and twice ""AirBag"" in the summary. I dont know why... And only 4 variables are significant in the model. Can somebody tell me, where is the mistake in my model?</p>

<p>I also would like to know how to test other assumptions in R for multiple linear model.</p>
"
"0.087740961604166","0.100333291527804","112481","<p>I am attempting to plot hurdle regression output with an interaction term in R, but I have some trouble doing so with the count portion of the model. </p>

<pre><code>Model &lt;- hurdle(Suicide. ~ Age + gender + bullying*support, dist = ""negbin"", link = ""logit"")
</code></pre>

<p>Since I am unaware of any packages that would allow me to plot the hurdle model in its entirety, I estimated each component separately (binomial logit and negative binomial count), using the <code>MASS</code> package, and I am attempting to plot the results. </p>

<p>So far, I have had the best luck using the <code>visreg</code> package for plotting, but I would like to hear other suggestions. I have been able to reproduce and successfully plot the original logistic output from the hurdle model in <code>MASS</code>, but not the negative binomial count data (i.e., the parameter estimates from <code>MASS</code> are not the same as they are in the hurdle regression output).</p>

<p>I would greatly appreciate any insight regarding how others have plotted hurdle regression results in the past, or how I might be able to reproduce negative binomial coefficients originally obtained from the hurdle model using <code>glm.nb()</code> in <code>MASS</code>. </p>

<p>Here is what I am using to plot my data:</p>

<pre><code>##Logistic 

logistic&lt;-glm(SuicideBinary ~ Age + gender + bullying*support, data = sx, family=""binomial"")

data(""sx"", package = ""MASS"")
##Linear scale
visreg(logistic, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Log odds (Suicide yes/no)"")

##Logistic/probability scale
visreg(logistic, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Initial Attempt)"")

##Count model

NegBin&lt;-glm.nb(Suicide. ~ Age + gender + bullying*support, data = sx)

data(""sx"", package = ""MASS"")
##Linear scale
visreg(NegBin, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Count model (number of suicide attempts)"")

##Logistic/probability scale
visreg(NegBin, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Subsequent Attempts)"")
</code></pre>
"
"0.0200240432865818","0.039253433598943","112660","<p>Say I have search data like this</p>

<p><code>AvgCost   QualityScore   SearchShare</code></p>

<p><code>3.12      6               0.6364</code></p>

<p>Where AvgCost is a continuous numerical variable, qualityScore is a categorical variable with values 1-10, and SearchShare is a percentage...I am wondering how to tease out the effect Avgcost and QualityScore have as well as whether they are too related to be both included. I'm also wondering what kinds of regressions besides simple linear regressions can be run on data sets where I have these sorts of variables i.e. categorical and numerical explanatory variables and a percentage response variable. </p>

<p>I have done some obvious things, like check <code>cor()</code> between qualityScore and AvgCost as well as do a simple linear regression. I looked at the <code>relaimpo</code> package, not sure if that's what I need. </p>

<p>Anyway, thanks very much for any help!</p>
"
"0.105957277556576","0.10385482340819","112760","<p>I have used <code>mlogit</code> package and I am trying to summarize the results I have from my model.  I have a question regarding the reference value and will get to that in a moment.</p>

<pre><code>redata.full &lt;- mlogit(no.C~ 1| WR+age+age2+BP+noC.1yr, data=redata, reflevel=""0"", na.action=na.fail)

no.C = number of offspring    
WR = risk
age+age2 = the non-linear relationship that as an individual ages their production decreases
BP = browsing pressure
noC.1yr = number of offspring produced the year before
</code></pre>

<p>I recognize that my data is ordinal in nature, but Im following other people's methods who have done this and used the reference based approach rather than ordinal logistic regression.  However, I am still shakey on justification other than citing the other person and saying ""he did it too!""  If anyone has a suggestion I would appreciate it.</p>

<p>My results for this model are: </p>

<pre><code>Call:
mlogit(formula = no.C ~ 1 | WR + age + age2 + BP + noC.1yr, data = redata, 
    na.action = na.fail, reflevel = ""0"", method = ""nr"", print.level = 0)

Frequencies of alternatives:
       0        1        2 
0.233766 0.675325 0.090909 

nr method
5 iterations, 0h:0m:0s 
g'(-H)^-1g = 2.16E-07 
gradient close to zero 

Coefficients :
               Estimate Std. Error t-value Pr(&gt;|t|)  
1:(intercept) -0.281226   1.225763 -0.2294  0.81854  
2:(intercept) -0.605312   1.997179 -0.3031  0.76183  
1:WR           0.847273   0.518854  1.6330  0.10248  
2:WR           1.347976   0.689916  1.9538  0.05072 .
1:age          0.314075   0.275486  1.1401  0.25425  
2:age         -0.422368   0.395240 -1.0686  0.28523  
1:age2        -0.018998   0.014446 -1.3151  0.18847  
2:age2         0.022572   0.018949  1.1912  0.23359  
1:BP          -0.143720   0.173585 -0.8280  0.40770  
2:BP          -0.074553   0.331108 -0.2252  0.82185  
1:noC.1yr      0.574304   0.377821  1.5200  0.12850  
2:noC.1yr      1.251673   0.626033  1.9994  0.04557 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -116.6
McFadden R^2:  0.079844 
Likelihood ratio test : chisq = 20.236 (p.value = 0.0271)

exp(cbind(OddsRatio = coef(redata.full), ci))
              OddsRatio      2.5 %    97.5 %
1:(intercept) 0.7548580 0.06831155  8.341351
2:(intercept) 0.5459038 0.01089217 27.360107
1:WR          2.3332750 0.84394900  6.450831
2:WR          3.8496270 0.99577472 14.882511
1:age         1.3689929 0.79782462  2.349065
2:age         0.6554925 0.30209181  1.422317
1:age2        0.9811815 0.95379086  1.009359
2:age2        1.0228284 0.98553735  1.061530
1:BP          0.8661299 0.61634947  1.217136
2:BP          0.9281585 0.48504538  1.776078
1:noC.1yr     1.7758933 0.84686698  3.724076
2:noC.1yr     3.4961862 1.02497823 11.925441
</code></pre>

<p>I would like confirmation of my interpretations:
The model is better than a null - obtained from the likelihood ratio test.</p>

<p>Question: How do I test how well the model is actually working (i.e., goodness of fit)?  Hosmer-Lemshow test? Ive read warnings about using the McFaddin's Pseudo R where they really aren't applicable to multinomial regressions.  Ive found a HL test with <code>ResourceSelection</code> library and it says my model is NOT doing well at all.  Now what?</p>

<p>Interpretation:
WR and noC.1yr are the only variables that are coming out as slightly significant.  But this is only between the reference value of 0 and production of 2 calves.  It is not significantly different between 0 or 1 for these variables.  </p>

<p>Question: Ive been trying to find somewhere in the vignette what the t-value is - it is just a t-test?  How would I refer to the estimate as being significant?  ""The estimated odds for 2-offspring being produced versus 0 were 3.85 (95% CI = 1.0-14.88) which was significant (t= 1.99, P=0.05)""</p>

<p>Referring to my statement regarding setting the reference value.  When I run this exact same model using my other options of 0 or 1 offspring - I get completely different results of which variables are significant.  If I use 2 as the reference value then Age+WR+noC.yr are significant.  If I use 1, then Age only is sig.  So, which one to use?  I have read you want to pick one that is most relevant to your hypothesis, but in this case I could motivate any of the 3 levels.  </p>
"
"0.0633215847514023","0.0620651280774201","112860","<p>I am doing a time series regression between 2 variables. I used the dynlm library in R. I'm trying to understand how to interpret the results. </p>

<p>Could you please point out where I am getting it wrong: </p>

<p>1) The R squared seems very low -- this indicates a weak linear relationship. Or too many outliers. 
2) Normal QQ plot shows that there are a significant number of outliers -- at a later date in the time series? 
3) Does the 'Residuals vs Fitted' plot show that the model is a pretty good fit for most of the data, except for those outliers? </p>

<p>Any suggested readings (esp open-sourced materials available online) also appreciated. </p>

<hr>

<pre><code>Call:
dynlm(formula = P ~ L(B))
Residuals:
    Min      1Q  Median      3Q     Max 
-63.711 -27.687 -14.907   2.364 146.157 
Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 17.485051  13.500833   1.295    0.197     
L(B)         0.019422   0.002384   8.146 5.84e-14
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
Residual standard error: 48.17 on 182 degrees of freedom
Multiple R-squared:  0.2672,    Adjusted R-squared:  0.2632 
F-statistic: 66.36 on 1 and 182 DF,  p-value: 5.838e-14
</code></pre>

<p><img src=""http://i.stack.imgur.com/xJbHF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/6cpT2.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/JZbmX.png"" alt=""enter image description here""></p>
"
"0.0566365471788599","0.0555127381653369","112889","<p>I'm trying to use rjags to predict what the curve-maximums will be for different groups of incomplete data (one metric x and one metric y). Here is an example of the indata:</p>

<p><img src=""http://i.stack.imgur.com/315aR.png"" alt=""enter image description here""></p>

<pre><code>in.data &lt;- structure(list(x = c(10.4055555555556, 16.1694444444444, 25.25, 
                                33.3305555555556, 37.4736111111111, 31.2805555555556, 24.3347222222222, 
                                24.1638888888889, 27.8222222222222, 34.0055555555556, 31.6611111111111, 
                                33.8666666666667, 26.6486111111111, 25.3986111111111, 24.8611111111111, 
                                15.2125, 6.33194444444444, 11.7541666666667, 15.6694444444444, 
                                27.9430555555556, 34.1180555555556, 36.5092592592593, 34.4763888888889, 
                                32.8055555555556, 26.9041666666667, 27.9319444444444, 28.7319444444444, 
                                30.3638888888889, 31.3861111111111, 26.4722222222222, 27.0152777777778, 
                                26.6333333333333, 17.3902777777778, 8.09861111111111, 12.9305555555556, 
                                21.8277777777778, 30.8652777777778, 36.9486111111111, 33.5722222222222, 
                                29.1652777777778, 25.6444444444444, 28.7222222222222, 32.9319444444444, 
                                32.8375, 31.9402777777778, 29.4569444444444, 26.9013888888889, 
                                25.7152777777778, 24.9638888888889, 14.3722222222222, 7.36527777777778, 
                                13.4722222222222, 20.7263888888889, 31.6388888888889, 41.6555555555556, 
                                49.0513888888889, 39.1333333333333, 33.5847222222222, 28.2722222222222, 
                                28.2583333333333, 31.2361111111111, 33.1569444444444, 29.9166666666667, 
                                27.925, 26.9680555555556, 24.7611111111111, 18.8319444444444, 
                                6.33055555555556, 9.87083333333333, 14.5541666666667, 18.5041666666667, 
                                23.0652777777778, 21.8875, 20.7291666666667, 14.7375, 11.4083333333333, 
                                12.7930555555556, 12.8013888888889, 16.8277777777778, 14.8263888888889, 
                                13.3222222222222, 9.74305555555556, 6.42638888888889, 6.275, 
                                9.76666666666667, 9.98472222222222, 8.5, 11.4958333333333, 10.9777777777778, 
                                13.3083333333333, 8.69722222222222, 6.24305555555556, 5.11111111111111, 
                                6.41527777777778, 8.04583333333333, 9.46111111111111, 11.0333333333333, 
                                7.3, 9.42916666666667, 8.86805555555556, 9.84305555555556, 10.8555555555556, 
                                7.32916666666667, 5.77361111111111, 7.52083333333333, 9.99305555555556, 
                                10.9875, 10.0888888888889, 11.1, 13.8277777777778, 15.3458333333333, 
                                9.74166666666667, 6.59305555555556, 7.28611111111111, 5.95833333333333, 
                                9.20972222222222, 8.85138888888889, 11.6277777777778, 12.9680555555556, 
                                12.8541666666667, 14.1041666666667, 11.1236111111111, 6.77777777777778, 
                                6.48055555555556, 9.77361111111111, 8.14444444444444, 7.92222222222222, 
                                7.85555555555556, 6.63611111111111, 5.40416666666667, 0, 1, 2, 
                                3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 
                                20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 
                                36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 
                                52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 
                                68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 
                                84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 
                                100), y = c(1.36111111111111, 2.08805555555556, 2.83666666666667, 
                                            3.17583333333333, 3.53194444444444, 1.4125, 2.44166666666667, 
                                            2.45777777777778, 2.78527777777778, 3.17611111111111, 3.02138888888889, 
                                            2.60861111111111, 2.88805555555556, 2.79527777777778, 2.575, 
                                            1.77333333333333, 0.937777777777778, 1.57027777777778, 1.69888888888889, 
                                            2.74, 3.32888888888889, 2.6975, 3.42444444444444, 3.40555555555556, 
                                            2.77277777777778, 2.6425, 2.96166666666667, 3.25472222222222, 
                                            3.23777777777778, 2.70472222222222, 2.76805555555556, 2.39694444444444, 
                                            1.97083333333333, 1.00722222222222, 1.58055555555556, 2.52138888888889, 
                                            3.16083333333333, 3.30555555555556, 3.61805555555556, 3.11472222222222, 
                                            2.7825, 3.11416666666667, 3.25944444444444, 3.12666666666667, 
                                            3.50555555555556, 3.07972222222222, 2.92666666666667, 2.45361111111111, 
                                            2.33055555555556, 1.61555555555556, 0.814444444444444, 1.41722222222222, 
                                            2.30222222222222, 3.27166666666667, 3.70638888888889, 5.32527777777778, 
                                            3.8375, 3.42, 2.985, 2.99194444444444, 3.355, 3.36472222222222, 
                                            3.07388888888889, 3.095, 3.00222222222222, 2.61583333333333, 
                                            2.38277777777778, 0.776111111111111, 1.15611111111111, 1.53361111111111, 
                                            2.16611111111111, 2.62861111111111, 2.60777777777778, 2.46111111111111, 
                                            1.91111111111111, 1.35083333333333, 1.50833333333333, 1.50722222222222, 
                                            1.63555555555556, 1.61527777777778, 1.53333333333333, 1.09055555555556, 
                                            0.396666666666667, 0.401666666666667, 0.547777777777778, 0.594722222222222, 
                                            0.356388888888889, 0.790277777777778, 0.510555555555556, 0.823611111111111, 
                                            0.563611111111111, 0.260277777777778, 0.146944444444444, 0.363333333333333, 
                                            0.369444444444444, 0.585, 0.740833333333333, 0.320833333333333, 
                                            0.765277777777778, 0.698611111111111, 0.6625, 0.764722222222222, 
                                            0.611388888888889, 0.276388888888889, 0.518611111111111, 0.735, 
                                            0.779722222222222, 0.732222222222222, 0.882222222222222, 1.02444444444444, 
                                            1.06638888888889, 0.730555555555556, 0.469444444444444, 0.565277777777778, 
                                            0.573333333333333, 0.720277777777778, 0.780277777777778, 0.898888888888889, 
                                            1.02305555555556, 0.910277777777778, 1.01194444444444, 0.884722222222222, 
                                            0.541388888888889, 0.469166666666667, 0.709166666666667, 0.599166666666667, 
                                            0.502777777777778, 0.523333333333333, 0.547222222222222, 0.243333333333333, 
                                            0.0156494522691706, 0.264475743348983, 0.510172143974961, 0.752738654147105, 
                                            0.992175273865415, 1.22848200312989, 1.46165884194053, 1.69170579029734, 
                                            1.91862284820031, 2.14241001564945, 2.36306729264476, 2.58059467918623, 
                                            2.79499217527387, 3.00625978090767, 3.21439749608764, 3.41940532081377, 
                                            3.62128325508607, 3.82003129890454, 4.01564945226917, 4.20813771517997, 
                                            4.39749608763693, 4.58372456964006, 4.76682316118936, 4.94679186228482, 
                                            5.12363067292645, 5.29733959311424, 5.4679186228482, 5.63536776212833, 
                                            5.79968701095462, 5.96087636932707, 6.1189358372457, 6.27386541471049, 
                                            6.42566510172144, 6.57433489827856, 6.71987480438185, 6.8622848200313, 
                                            7.00156494522692, 7.1377151799687, 7.27073552425665, 7.40062597809077, 
                                            7.52738654147105, 7.6510172143975, 7.77151799687011, 7.88888888888889, 
                                            8.00312989045383, 8.11424100156495, 8.22222222222222, 8.32707355242566, 
                                            8.42879499217527, 8.52738654147105, 8.62284820031299, 8.7151799687011, 
                                            8.80438184663537, 8.89045383411581, 8.97339593114241, 9.05320813771518, 
                                            9.12989045383412, 9.20344287949922, 9.27386541471049, 9.34115805946792, 
                                            9.40532081377152, 9.46635367762128, 9.52425665101721, 9.57902973395931, 
                                            9.63067292644758, 9.679186228482, 9.7245696400626, 9.76682316118936, 
                                            9.80594679186228, 9.84194053208138, 9.87480438184664, 9.90453834115806, 
                                            9.93114241001565, 9.95461658841941, 9.97496087636933, 9.99217527386542, 
                                            10.0062597809077, 10.0172143974961, 10.0250391236307, 10.0297339593114, 
                                            10.0312989045383, 10.0297339593114, 10.0250391236307, 10.0172143974961, 
                                            10.0062597809077, 9.99217527386542, 9.97496087636933, 9.95461658841941, 
                                            9.93114241001565, 9.90453834115806, 9.87480438184664, 9.84194053208138, 
                                            9.80594679186228, 9.76682316118936, 9.7245696400626, 9.679186228482, 
                                            9.63067292644758, 9.57902973395931, 9.52425665101721, 9.46635367762128, 
                                            9.40532081377152), Group_ID = c(""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", ""Group_1"", 
                                                                            ""Group_1"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", ""Group_2"", 
                                                                            ""Group_2"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", ""Prior"", 
                                                                            ""Prior"", ""Prior"", ""Prior"", ""Prior"")), .Names = c(""x"", ""y"", ""Group_ID""
                                                                            ), row.names = c(NA, -231L), class = ""data.frame"")

library(ggplot2)
p &lt;- ggplot(in.data, aes(x=x, y=y, group=Group_ID, color=Group_ID))
p &lt;- p + geom_point()
p
</code></pre>

<p>Group_1 and Group_2 will follow the same parabolic shape as the prior but, as can be seen in the graph, will have lower maximums when x increase.</p>

<p>Can Bayesian Regression be used for this? If so, how could the model for rjags look? The closest I think I've gotten is this, but can't get it to work:</p>

<pre><code>N[i] ~ dpois(z[i])
log(z[i]) &lt; beta0 + beta1 * x[i] + beta2 * x[i]^2
</code></pre>

<p>I've studied the truly great book by Kruschke (Doing Bayesian Data Analysis) where chapter 16 explains many steps, but when I want to expand from linear y = b1 + b2 * x to a more advanced curve-shape I get stuck.</p>
"
"0.0566365471788599","0.0416345536240027","113251","<p>In my design, I have two groups of subjects and every subject is tested in four different conditions. So, I have a within-subject factor ('span_num', which ranges from 0 to 3) and a between-subject factor (group, which can be 'Linear' or 'U-shape').</p>

<p>My goal is to show that the slope between the spans (from 0 to 3) is higher in the Linear group than in the U-shape group.</p>

<p>The slopes look very different and are significantly different when I compare the regression models I get when I ignore that my within-factor is a within-factor and treat it as a between-factor.</p>

<p>I compared the slopes like this (but I don't trust the comparison because I don't trust the SE values of the slopes because I am treating my within-factor like a between-factor):</p>

<pre><code>linear_lm &lt;- lm(RT ~ span_num, dat = data[data$group == ""Linear"",])
ushape_lm &lt;- lm(RT ~ span_num, dat = data[data$group == ""U-shape"",])
linear_intercept &lt;- summary(linear_lm)$coefficients[[1]]; linear_ise &lt;- summary(linear_lm)$coefficients[[3]]; linear_slope &lt;- summary(linear_lm)$coefficients[[2]]; linear_sse &lt;- summary(linear_lm)$coefficients[[4]]
ushape_intercept &lt;- summary(ushape_lm)$coefficients[[1]]; ushape_ise &lt;- summary(ushape_lm)$coefficients[[3]]; ushape_slope &lt;- summary(ushape_lm)$coefficients[[2]]; ushape_sse &lt;- summary(ushape_lm)$coefficients[[4]]

z_intercept &lt;- (ushape_intercept - linear_intercept) / sqrt(linear_ise^2 + ushape_ise^2)   #z = -0.45; p = .67, n.s.
z_slope &lt;- (ushape_slope - linear_slope) / sqrt(linear_sse^2 + ushape_sse^2)   #z = -1.50; p = .93, sig.
</code></pre>
"
"0.0490486886395286","0.0480754414848157","113641","<p>I have a plot like this. </p>

<p><img src=""http://i.stack.imgur.com/bru73.png"" alt=""enter image description here""></p>

<p>I wish to apply a model to this, however, I guess a linear regression model won't work on this. What I did was plot it on logarithm x and logarithm y axis as well but it came out to be of no use. </p>

<p>With logarithm:<p>
    <img src=""http://i.stack.imgur.com/blNav.png"" alt=""https://www.dropbox.com/s/vyzk4u26vnludoq/1.png?dl=0""></p>

<p>I tried fitting in a model, but as expected, on plotting, the residual and fitted, it didn't turn out to be of much use:<p>
    <img src=""http://i.stack.imgur.com/Dy7Zs.png"" alt=""https://www.dropbox.com/s/0uaf53bxdc3wd1l/3.png?dl=0""></p>

<p>What else can I do? Anything that someone can suggest? Also, I wish to know how can I apply a non linear regression model?</p>
"
"0.0633215847514023","0.0620651280774201","113652","<p>I need to calculate the temporal trends for some climate variables with missing values. For example, last frost days defines as the last day of year with minimum temperature less than 0C. However, there are no any frost days in some years. </p>

<p>My data look like: </p>

<pre><code>lfd &lt;- c(NA, NA, NA, NA, NA, 190, 192, 189, 200, 185, 205, 203, 200, 207, NA, NA, 205)
years &lt;- seq(1957, length.out = length(lfd))
</code></pre>

<p>Now I use the linear regression in R (function lm) to calculate temporal trend. It seems the results are unreasonable for datasets with many missing values.</p>

<p>How could I calculate the temporal trends with missing values? Thanks for any suggestions. </p>
"
"0.0490486886395286","0.0480754414848157","113737","<p>I am performing linear regression in R and I have a variable called diversityscore which is a value ranging from 1 to 10 indicating #activities a user performs with 1 meaning one activity to 10 meaning all ten activities. I am not sure if this is to be counted as a factor variable or a non-factor variable. How do I make this decision?</p>

<p>(Expanding the question for the comprehensive answer below:...)</p>

<p>If it is (not) a factor, is this true for all ratio variables (with meaningful 0 and equal intervals)? For ex, temperature in Kelvin (assuming discrete values in output), age, etc? How about dates? I'm guessing they must be counted as factors. Can we take nominal and ordinal variables in general to be factors? or any more exceptions that I should be watchful of?</p>
"
"0.02831827358943","0.0277563690826684","113839","<p>I am using the nls function in R to perform a nonlinear regression and need to calculate the studentized residuals.  Is this something I need to manually do?  It seems like I need to manually do a QR decomposition and calculate the hat matrix to do this.  I don't have a problem doing this but I wanted to ask if anyone knew an easier way to do this.</p>
"
"0.0800961731463273","0.078506867197886","114184","<p>Specifically, are there any binomial regression models that use a kernel with heavier tails and higher kurtosis than the standard kernels (logistic/probit/cloglog)?</p>

<p>As a function of the linear predictor $\textbf{x}'\mathbf{\hat{\beta}}$, the logistic distribution</p>

<ul>
<li>Underestimates the probability of my data being in the tails of the distribution</li>
<li>Underestimates the kurtosis, or clustering of data, in the middle of the distribution:</li>
</ul>

<p>This can be seen from a diagnostic plot of my fit:</p>

<p><img src=""http://i.stack.imgur.com/ar6OW.png"" alt=""enter image description here""></p>

<ul>
<li>The red line is the logistic CDF, representing a perfect fit</li>
<li>The black line represents the fitted probabilities from my dataset (calculated by binning observations into 0.1 intervals of $\textbf{x}'\mathbf{\hat{\beta}}$, where $\mathbf{\hat{\beta}}$ is obtained from my fit)</li>
<li>The grey bars in the background represent number of observations on which the true probabilities are based upon</li>
<li>The grey areas are where the tail 10% of the data lie (5% each side).</li>
</ul>

<p>Ideally, any solution would use R.</p>

<h2>Edit</h2>

<p>Why am I talking about CDFs? Our GLM equation is:</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{E}[Y] = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Where $g$ is the link function.</p>

<p>Further, if $g^{-1}$ is a valid probability distribution (i.e. monotonically increasing from 0 to 1, indeed the case with probit, logit, cloglog), then consider a latent (not directly observed) continuous random variable $Y^{*}$ whose distribution (CDF) is given by $g^{-1}$. Then by definition</p>

<p>$$\mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta}) = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Equating the two equations above, we see the probability of $Y=1$ is exactly equal to the CDF of $Y^{*}$</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta})$$</p>

<p>Hence I talk interchangeably about the expected response $\mathbb{E}[Y]$ and CDF of $Y^{*}$ over linear-predictor ($\textbf{x}'\mathbf{\hat{\beta}}$) space.</p>
"
"0.0805952195517515","0.0789960112897596","114211","<p>Out of curiosity, I want to understand how to model this problem. I've been hearing people suggest the use of linear regression but <strong>I am not sure how to encode this problem</strong> (included my attempt below) in R as I am a complete beginner in this area. </p>

<p>I have a task that can be done any number of times (each individual instance is a task instance). Everytime the task completes 1%, I recorded the time elapsed since the task's start time. Therefore, for each task, I will have 100 points (100 1% increments) at which I recorded the time elapsed. </p>

<p>Given that I have this data for many instances, is it possible to predict the finish time for this task when a new task instance is given? </p>

<pre><code>      TaskID Percent TimeElapsed
   1:      1       0   0.2035333
   2:      1       1   0.2062833
   3:      1       2   0.2137167
   4:      1       3   0.2180833
   5:      1       4   0.2490833
  ---                           
3127:     31      96   4.9391667
3128:     31      97   4.9970500
3129:     31      98   5.5644500
3130:     31      99   5.6532667
3131:     31     100   5.8359833
</code></pre>

<p>A quick look at the task behavior (below) tells me there is a bit of a variance in how the task behaves so its hinting that the output should not just be a time prediction but rather a time prediction with some confidence? </p>

<p>In addition, I'm thinking just using the information about the current progress of the task might not be sufficient - the task may have slowed down in some its previous progress points so the finsh time would be affected. Therefore, this information should somehow be encoded into the model?</p>

<p><img src=""http://i.stack.imgur.com/eiEKh.png"" alt=""enter image description here""></p>

<p>I am particularly interested in how to do this using R. I included my initial attempt at using linear regression here but the result does not look good to me. Any suggestions on how to improve this or use some other methods? </p>

<p>I have given the output of dput (on a data table: <code>install.packages(""data.table"")</code>) on <a href=""http://pastebin.com/zX5GdKP2"" rel=""nofollow"">pastebin</a>. If you want a data.frame instead, please see this <a href=""http://pastebin.com/JwVhTCTU"" rel=""nofollow"">paste</a> instead.</p>

<p><strong>EDIT:</strong> Attempt at using linear regression</p>

<p>The thick black line is the median at every point. The thick red line is the regression line fit to the median line. </p>

<p><img src=""http://i.stack.imgur.com/mZFZd.png"" alt=""enter image description here""></p>
"
"0.0490486886395286","0.0480754414848157","114221","<p>I plotted normal probability plot in R using <code>qqnorm</code> and <code>qqline</code>. I want some help on:</p>

<ol>
<li>How to estimate ""probability"" that a data has normal distribution? (I read in a paper that a probability of 0.10 is required to assume that a data is normally distributed). </li>
<li>Also, how to calculate correlation coefficient for a normal probability plot in R?</li>
<li>Is the normality test valid for nonlinear regression too? (This might be a silly question, excuse me for that!)</li>
</ol>

<p>Thanks!</p>
"
"0.0566365471788599","0.0555127381653369","114399","<p>I have a theoretical growth function that can be perturbed by events, and I'd like to estimate the growth parameters as well as the perturbation, and the rate of falloff after that perturbation.</p>

<p>I'm thinking of using a logistic function to model the effect of the event and the falloff of that effect (if any).</p>

<p>To ground this, $x$ is time, and $t$ is the time the event occurs. Before time $t$, or if the event never occurs, we have a simple linear regression. After the event occurs, I model the contribution of the event with magnitude controlled by $\beta_2$ and rate of falloff by $\beta_3$.</p>

<p>$y_i=\left\{x_{i}&lt;t:\beta_{0}+\beta_1x_i+\epsilon_i,x_i&gt;t:\beta_0+\beta_1x_i+2\beta_2\frac{1}{\left(1+e^{\beta_3\left(x_i-t\right)}\right)}+\epsilon_i\right\}$</p>

<p>(<em>edited to add the error term</em>)</p>

<p>Here's a <a href=""https://www.desmos.com/calculator/nzmusqqosq"" rel=""nofollow"">Desmos graph</a> if it helps.</p>

<p>I'm really not sure how to estimate parameters for this model in any of the stats packages I'm familiar with in R. Do I need to turn to Bayesian methods?</p>
"
"0.0633215847514023","0.0620651280774201","114441","<p>I'm playing around with the Abalone dataset in R and following along with <a href=""http://scg.sdsu.edu/linear-regression-in-r-abalone-dataset/"" rel=""nofollow"">this article</a>. </p>

<p>The dataset has 8 variables that are taken into account to predict the number of <code>rings</code>. To find the pairwise correlation, the blog post does this:</p>

<pre><code>as.matrix(cor(na.omit(abalone[,-1])))
</code></pre>

<p>and comes to conclusion that the data is heavily correlated. My question is how do they come to this conclusion? What information should I be looking for to come to this conclusion? </p>

<p>Here is the code </p>

<pre><code>&gt; aburl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'
&gt; abnames = c('sex','length','diameter','height','weight.w','weight.s','weight.v','weight.sh','rings')
&gt; abalone = read.table(aburl, header = F , sep = ',', col.names = abnames)
&gt; as.matrix(cor(na.omit(abalone[,-1])))

             length  diameter    height  weight.w  weight.s  weight.v weight.sh     rings
length    1.0000000 0.9868116 0.8275536 0.9252612 0.8979137 0.9030177 0.8977056 0.5567196
diameter  0.9868116 1.0000000 0.8336837 0.9254521 0.8931625 0.8997244 0.9053298 0.5746599
height    0.8275536 0.8336837 1.0000000 0.8192208 0.7749723 0.7983193 0.8173380 0.5574673
weight.w  0.9252612 0.9254521 0.8192208 1.0000000 0.9694055 0.9663751 0.9553554 0.5403897
weight.s  0.8979137 0.8931625 0.7749723 0.9694055 1.0000000 0.9319613 0.8826171 0.4208837
weight.v  0.9030177 0.8997244 0.7983193 0.9663751 0.9319613 1.0000000 0.9076563 0.5038192
weight.sh 0.8977056 0.9053298 0.8173380 0.9553554 0.8826171 0.9076563 1.0000000 0.6275740
rings     0.5567196 0.5746599 0.5574673 0.5403897 0.4208837 0.5038192 0.6275740 1.0000000
&gt; pairs(abalone[,-1]
</code></pre>

<p><img src=""http://i.stack.imgur.com/xD98j.png"" alt=""enter image description here""></p>

<p><strong>Questions</strong></p>

<ul>
<li>What is the problem when the data is heavily correlated?</li>
<li>How can one come to conclusion that the data is heavily correlated by looking at the matrix or by looking at the scatterplot? Is it because the scatterplot shows linear lines in almost every row-column?</li>
</ul>
"
"0.02831827358943","0.0277563690826684","114692","<p>I would like to solve a linear regression (in R) with weights $w$ and a constraint.</p>

<p>In other words, I would like to find $x$ that minimizes the sum of squares
$$\sum_i w_i(b_i-Ax_i)^2$$</p>

<p>On top of that I have an external vector $d$, which I would like to use in a constraint, such that $d \cdot x \le 5$.</p>

<p>Is this something that would be possible to do in R with <code>solve.QP</code> or perhaps some other R script?</p>

<p><strong>Edit</strong>: I am adding a bounty for a solution that doesn't require any other custom software except the cran packages. While rstan works perfectly unfortunately I am unable to install it on my production servers due to old versions of some libraries.</p>
"
"0.162942342511842","0.159709163896255","115065","<p>I am working with data from a computer task which has 288 total trials, each of which can be categorically classified according to <strong>Trial Type</strong>, <strong>Number of Stimuli</strong>, and <strong>Probe Location</strong>.  Because I want to also examine a continuous variable, the total Cartesian <strong>Distance</strong> between stimuli per trial (divided by number of stimuli to control for varying numbers), I have opted to use a mixed linear model with repeated measures.  In addition to each of these task variables, I am also interested in whether folks in various diagnostic groups perform differently on the task, as well as whether or not there is a <strong>Dx</strong> interaction with any of the above variables.  Thus (if I'm not mistaken), I have the following effects in my model:</p>

<p><strong>Trial Type</strong>, a fixed effect
<strong>Number of Stimuli</strong>, a fixed effect
<strong>Probe Location</strong>, a fixed effect
<strong>Dist</strong>(ance), a fixed effect
<strong>Dx</strong>, a fixed effect
<strong>Dx*Trial Type</strong>, a fixed effect
<strong>Dx*Number of Stimuli</strong>, a fixed effect
<strong>Dx*Probe Location</strong>, a fixed effect
<strong>Dx*Dist</strong>, a fixed effect
<strong>Trial</strong>, a random effect, nested within
<strong>SubID</strong>, a random effect</p>

<p>Based on my examination of documentation, it seems that the nesting of random effects does not seem to be important to lme4, and so I specify my model as follows:</p>

<p><code>tab.lmer &lt;- lmer(Correct ~  Dx+No_of_Stim+Trial_Type+Probe_Loc+Dist+Dx*No_of_Stim+Dx*Trial_Type+Dx*Probe_Loc+Dx*Dist+(1|Trial)+(1|SubID),data=bigdf)</code></p>

<p>This would be my first question: <strong>1) Is the above model specification correct?</strong></p>

<p>Assuming so, I am a bit troubled by my results, but as I read and recall my instruction on such models, I am wondering if interpretation of particular coefficients is bad practice in this case:</p>

<pre><code>Linear mixed model fit by REML ['merModLmerTest']
Formula: Correct ~ Dx + No_of_Stim + Trial_Type + Probe_Loc + Dist + Dx *  
    No_of_Stim + Dx * Trial_Type + Dx * Probe_Loc + Dx * Dist +  
    (1 | Trial) + (1 | SubID)
   Data: bigdf

REML criterion at convergence: 13600.4

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.89810 -0.03306  0.27004  0.55363  2.81656 

Random effects:
 Groups   Name        Variance Std.Dev.
 Trial    (Intercept) 0.013256 0.11513 
 SubID    (Intercept) 0.006299 0.07937 
 Residual             0.131522 0.36266 
Number of obs: 15840, groups:  Trial, 288; SubID, 55

Fixed effects:
                         Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)             4.196e-01  4.229e-02  4.570e+02   9.922  &lt; 2e-16 ***
DxPROBAND               8.662e-02  4.330e-02  2.920e+02   2.000  0.04640 *  
DxRELATIVE              9.917e-02  4.009e-02  2.920e+02   2.474  0.01394 *  
No_of_Stim3            -9.281e-02  1.999e-02  4.520e+02  -4.642 4.53e-06 ***
Trial_Type1             3.656e-02  2.020e-02  4.520e+02   1.810  0.07097 .  
Probe_Loc1              3.502e-01  2.266e-02  4.520e+02  15.456  &lt; 2e-16 ***
Probe_Loc2              3.535e-01  3.110e-02  4.520e+02  11.369  &lt; 2e-16 ***
Dist                    1.817e-01  2.794e-02  4.520e+02   6.505 2.06e-10 ***
DxPROBAND:No_of_Stim3  -1.744e-02  1.759e-02  1.548e+04  -0.992  0.32144    
DxRELATIVE:No_of_Stim3 -2.886e-02  1.628e-02  1.548e+04  -1.773  0.07628 .  
DxPROBAND:Trial_Type1  -9.250e-03  1.777e-02  1.548e+04  -0.521  0.60267    
DxRELATIVE:Trial_Type1  1.336e-02  1.645e-02  1.548e+04   0.812  0.41682    
DxPROBAND:Probe_Loc1   -8.696e-02  1.993e-02  1.548e+04  -4.363 1.29e-05 ***
DxRELATIVE:Probe_Loc1  -4.287e-02  1.845e-02  1.548e+04  -2.323  0.02018 *  
DxPROBAND:Probe_Loc2   -1.389e-01  2.735e-02  1.548e+04  -5.079 3.83e-07 ***
DxRELATIVE:Probe_Loc2  -8.036e-02  2.532e-02  1.548e+04  -3.173  0.00151 ** 
DxPROBAND:Dist         -3.920e-02  2.457e-02  1.548e+04  -1.595  0.11066    
DxRELATIVE:Dist        -1.485e-02  2.275e-02  1.548e+04  -0.653  0.51390    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In general, these results make sense to me.  The troubling portion, however, comes in the positive, significant (yes, I am using LmerTest) p-value for DxProband, particularly in light of the fact that in terms of performance means, Probands are performing worse than Controls.  So, this mismatch concerns me.  Examining the corresponding ANOVA:</p>

<pre><code>&gt; anova(tab.lmer)
Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq Mean Sq NumDF   DenDF F.value    Pr(&gt;F)    
Dx             0.8615  0.4308     2   159.0   1.412   0.24662    
No_of_Stim     0.6984  0.6984     1   283.5  37.043 3.741e-09 ***
Trial_Type     8.3413  8.3413     1   283.5   4.456   0.03565 *  
Probe_Loc     25.7223 12.8612     2   283.5 116.405 &lt; 2.2e-16 ***
Dist           5.8596  5.8596     1   283.5  43.399 2.166e-10 ***
Dx:No_of_Stim  1.4103  0.7051     2 15483.7   1.590   0.20395    
Dx:Trial_Type  2.0323  1.0162     2 15483.7   0.841   0.43128    
Dx:Probe_Loc   3.5740  0.8935     4 15483.7   7.299 7.224e-06 ***
Dx:Dist        0.3360  0.1680     2 15483.7   1.277   0.27885    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...the results seem to more or less line up with the regression, with the exception of the <strong>Dx</strong> variable.  So, my second question is <strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong></p>

<p>Finally, as a basic (and somewhat embarrassing) afterthought, <strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p>In summation,
<strong>1) Is the above model specification correct?</strong>
<strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong>
<strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p><strong>ADDENDUM</strong></p>

<p>By request, I'll describe the task and data a little further.  The data come from a computer task in which participants are presented a number of stimuli, either two or three, in various locations about the screen.  These stimuli can either be ""targets"" or ""distractors"".  After these stimuli, a probe stimulus is presented; if it appears in a position where a previous target has appeared, participants should respond ""yes""; if it appears in the position of a previous distractor or elsewhere, the correct answer is ""no.""  There are 288 trials of this nature; some have two stimuli and some have three, and some lack distractors entirely.  The variables in my model, then, can be elaborated as follows:</p>

<p><strong>Number of Stimuli:</strong> 2 or 3 (2 levels)</p>

<p><strong>Trial Type:</strong> No Distractor (0) or Distractor (1) (2 levels)</p>

<p><strong>Probe Location:</strong> Probe at Target (1), Probe at Distractor (2), or Probe Elsewhere (0) (3 levels)</p>

<p><strong>Distance:</strong> Total Cartesian distance between stimuli, divided by number of stimuli per trial (Continuous)</p>

<p><strong>Dx:</strong> Participant's clinical categorization</p>

<p><strong>Sub ID:</strong> Unique subject identifier (random effect)</p>

<p><strong>Trial:</strong> Trial number (1:288) (random effect)</p>

<p><strong>Correct:</strong> Response classification, either incorrect (0) or correct (1) per trial</p>

<p>Note that the task design makes it inherently imbalanced, as trials without distractors cannot have Probe Location ""Probe at Distractor""; this makes R mad when I try to run RM ANOVAs, and it is another reason I opted for a regression.</p>

<p>Below is a sample of my data (with SubID altered, just in case anyone might get mad):</p>

<pre><code>     SubID      Dx Correct No_of_Stim Trial_Type Probe_Loc      Dist Trial
1 99999999 PROBAND       1          3          0         1 0.9217487     1
2 99999999 PROBAND       0          3          0         0 1.2808184     2
3 99999999 PROBAND       1          3          0         0 1.0645292     3
4 99999999 PROBAND       1          3          1         2 0.7838786     4
5 99999999 PROBAND       0          3          0         0 1.0968788     5
6 99999999 PROBAND       1          3          1         1 1.3076598     6
</code></pre>

<p>Hopefully, with the above variable descriptions these data should be self-explanatory.</p>

<p>Any assistance that people can provide in this matter is very much appreciated.</p>

<p>Sincerely,
peteralynn</p>
"
"0.0849548207682898","0.0832691072480053","115126","<p>I need to do a Multiple Imputation on a dataset with several missing values, and I need to do it with mice, because later I'll have to compare the results with those of imputations ran with other programs.</p>

<p>My colleague obtained a completa dataset by running a MI on the incomplete dataset, with 5 iterations, and then taking the 5 imputed datasets and manually calculating mean values. Se essentially he pooled manually. I'm far from an expert so I don't know if this operation is valid. </p>

<p>Anyways, in MICE so far I could run the imputation (again, with maxit=5), using the function ""imp&lt;-mice(eco)"", where ""eco"" is the incomplete dataset. So I obtained the 5 imputed datasets, stored in the object ""imp"", of class ""MIDS"". Now I just need to pool the 5 completed datasets to obtain a unique complete one, i don't wanna run analyses on the 5 datasets and then pool the results. Can that be done? If I got it right from the manual, it seems that MICE allows you to pool only after you ran some analysis on the imputed datasets. The analysis is repeated on each dataset and the results are stored in an object of class ""MIRA"". I tried to run the function ""pool()"" on ""imp"" but it can't be done because imp is of class ""mids"" and you can only pool mira objects.</p>

<p>The manual also says that to pool you need a variance/covariance relation, and in the example given there they run a linear regression and then they pool the results of the regression. But I doubt if it's what I need. I'm confused</p>
"
"0.0899225958391357","0.0881383093888288","115154","<p>Relatively new to stats. I use linear regression  and get R^2, which is quite low.</p>

<p><strong>MODEL 1</strong></p>

<pre><code>    lmoutar=lm(formula = ts_y ~ ts_y_lag + ts_x)
</code></pre>

<p>So switched to arima with external regressor. Using ""auto.arima"", I formulate arimax model</p>

<p><strong>MODEL 2</strong></p>

<pre><code>    fitarima &lt;- auto.arima(ts_y, xreg=ts_x)
    arimaout&lt;-arima(ts_y,order=c(2,0,5),xreg=ts_x)
</code></pre>

<p>How can I compare the explanability of AR model with arima model. From the thread <a href=""http://stats.stackexchange.com/questions/8750/how-can-i-calculate-the-r-squared-of-a-regression-with-arima-errors-using-r"">How can I calculate the R-squared of a regression with arima errors using R?</a>, I understand R^2 is not an option for ARIMA.</p>

<p>From the thread <a href=""http://stats.stackexchange.com/questions/11850/model-comparison-between-an-arima-model-and-a-regression-model"">Model comparison between an ARIMA model and a regression model</a>, AIC/BIC is not the right criteria and MSE from forcast/predict can be possible criteria for comparison across AR and ARIMA model. Is MSE the best option for model comparison, if so how would I generate MSE for AR and ARIMA?</p>

<p>I tried to compare the above ar and arima model using anova, but I get following error message</p>

<pre><code>anova.lm(lmoutar,arimaout)
   Warning message:
    In anova.lmlist(object, ...) :
            models with response â€˜""NULL""â€™ removed because response differs from model 1
</code></pre>

<p>What does this error message mean? </p>

<p><strong><em>EDIT</em></strong></p>

<p>Thanks for the response so far and insight that AR is nested within ARIMA. How would one answer this question, if I rephrase  as ""How to compare AR, ARIMA and General Linear Models?"". The first model I listed has AR(1) and independent variable; it is a general linear model. So how would I compare a GLM versus ARIMAX model? Any thing else besides MSE that I could use to judge between GLM and ARIMAX</p>
"
"0.0853828074607","0.0920574617898323","115234","<p>I have a time series (std) of 324 observations with no missing values, starting from January 1987 and ending in December 2013.</p>

<p>I want to regress via OLS the one in the question.</p>

<p>In R, the code:</p>

<pre><code> lm(dstd ~ lstd)
</code></pre>

<p>Where 'dstd' is the differenced variable, via:</p>

<pre><code> dstd&lt;-diff(std,differences=1)
</code></pre>

<p>And lstd is the lagged variable via:</p>

<pre><code> lstd&lt;-lag(std, k=1)
</code></pre>

<p>Does not work, as it gives the following error:</p>

<pre><code> Error in model.frame.default(formula = dstd ~ lstd, drop.unused.levels = TRUE) : 
 variable lengths differ (found for 'lstd')
</code></pre>

<p>Which is, of course, natural as the length of 'dstd' is 323 while the length of 'lstd' is 324.</p>

<p>Searching for the error, na.omit has been suggested as the answer for other models. However, no combinations of 'na.omit' or 'na.exclude' would work. This <a href=""http://www.youtube.com/watch?v=10cuDKGytMw&amp;t=4m5s"" rel=""nofollow"">video</a> (at the relevant time) says ' you need to change these', referring to these variables.</p>

<p>I have tried excluding while forming another object, as well as subsetting in the regression model, yet none of these seem to work.</p>

<p>What code would I need to fix this?</p>

<p>Here is a Minimal Reproducible Example:</p>

<p>An object 'stdshort' is created with the same characteristics of the parent object, via this code:</p>

<pre><code> &gt; stdshort&lt;-dput(window(std, 1987, 1989, 12))
 structure(c(4.5, 4.7, 4.2, 4.4, 3.9, 3.9, 3.7, 3.7, 3.4, 3.6, 
 3.5, 3.1, 3.5, 3.3, 3.7, 3.7, 3.7, 3.8, 3.6, 3.5, 3.5, 3.3, 3.5, 
 3.5, 3.4), .Tsp = c(1987, 1989, 12), class = ""ts"")
</code></pre>

<p>The following code replicates the steps previously mentioned: differencing, lagging and linear regression.</p>

<p>First, a differenced object is created. The object dtstdshort's structure looks like this:</p>

<pre><code> &gt; dstdshort&lt;-diff(stdshort,differences=1)
 &gt; dput(dstdshort)
  structure(c(0.2, -0.5, 0.2, -0.5, 0, -0.2, 0, -0.3, 0.2, -0.1, 
 -0.4, 0.4, -0.2, 0.4, 0, 0, 0.0999999999999996, -0.2, -0.1, 0, 
 -0.2, 0.2, 0, -0.1), .Tsp = c(1987.08333333333, 1989, 12), class = ""ts"")
</code></pre>

<p>A following object, lstdshort, for 1 period lagging is constructed. This is the structure:</p>

<pre><code> &gt; lstdshort&lt;-lag(stdshort,k=1)
 &gt; dput(lstdshort)
 structure(c(4.5, 4.7, 4.2, 4.4, 3.9, 3.9, 3.7, 3.7, 3.4, 3.6, 
 3.5, 3.1, 3.5, 3.3, 3.7, 3.7, 3.7, 3.8, 3.6, 3.5, 3.5, 3.3, 3.5, 
 3.5, 3.4), .Tsp = c(1986.91666666667, 1988.91666666667, 12), class = ""ts"")
</code></pre>

<p>The regression of those two objects as explained before, gives the following error (with code):</p>

<pre><code> &gt; lm(dstdshort ~ lstdshort)
 Error in model.frame.default(formula = dstdshort ~ lstdshort, drop.unused.levels = TRUE) : 
   variable lengths differ (found for 'lstdshort')
</code></pre>
"
"0.109676202005208","0.107499955208361","115304","<p>I am learning about building linear regression models by looking over someone elses R code.  Here is the example data I am using:</p>

<pre><code>v1  v2  v3  response
0.417655013 -0.012026453    -0.528416414    48.55555556
-0.018445979    -0.460809371    0.054017873 47.76666667
-0.246110341    0.092230159 0.057435968 49.14444444
-0.521980295    -0.428499038    0.119640369 51.08888889
0.633310578 -0.224215856    -0.153917427    48.97777778
0.41522316  0.050609412 -0.642394965    48.5
-0.07349941 0.547128578 -0.539018121    53.95555556
-0.313950353    0.207853678 0.713903994 48.16666667
0.404643796 -0.326782199    -0.785848428    47.7
0.028246796 -0.424323318    0.289313911 49.34444444
0.720822953 -0.166712488    0.323246062 50.78888889
-0.430825851    -0.308119827    0.543823856 52.65555556
-0.964175294    0.661700584 -0.11905972 51.03333333
-0.178955757    -0.11148414 -0.151179885    48.28888889
0.488388035 0.515903257 -0.087738159    48.68888889
-0.097527627    0.188292773 0.207321867 49.86666667
0.481853599 0.21142728  -0.226700254    48.38888889
1.139561277 -0.293574756    0.574855693 54.55555556
0.104077762 0.16075114  -0.131124443    48.61111111
</code></pre>

<p>I read in the data and use a call to <code>lm()</code> to build a model:</p>

<pre><code>&gt; my_data&lt;- read.table(""data.csv"", header = T, sep = "","")
&gt; my_lm &lt;- lm(response~v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, data=my_data)
&gt; summary(my_lm)

Call:
lm(formula = response ~ v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, 
data = my_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.0603 -0.6615 -0.1891  1.0395  1.8280 

Coefficients:
         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  49.33944    0.42089 117.226  &lt; 2e-16 ***
v1            0.06611    0.82320   0.080  0.93732    
v2           -0.36725    1.06359  -0.345  0.73585    
v3            0.72741    1.00973   0.720  0.48508    
v1:v2        -2.54544    2.21663  -1.148  0.27321    
v1:v3         0.80641    2.77603   0.290  0.77640    
v2:v3       -12.16017    3.62473  -3.355  0.00573 ** 
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.375 on 12 degrees of freedom
Multiple R-squared:  0.697, Adjusted R-squared:  0.5455 
F-statistic:   4.6 on 6 and 12 DF,  p-value: 0.01191
</code></pre>

<p>Following along with their code I then use a call to <code>anova()</code>:</p>

<pre><code>&gt; my_lm_anova &lt;- anova(my_lm)
&gt; my_lm_anova
Analysis of Variance Table

Response: response
          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
v1         1  0.0010  0.0010  0.0005 0.982400   
v2         1  0.2842  0.2842  0.1503 0.705036   
v3         1  9.8059  9.8059  5.1856 0.041891 * 
v1:v2      1  4.3653  4.3653  2.3084 0.154573   
v1:v3      1 16.4582 16.4582  8.7034 0.012141 * 
v2:v3      1 21.2824 21.2824 11.2545 0.005729 **
Residuals 12 22.6921  1.8910                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, I am not sure:</p>

<ol>
<li>Why I would use the call to ANOVA in this situation, and</li>
<li>What the ANOVA table is telling me about the predictor variables.</li>
</ol>

<p>From the code they appear to use the ANOVA table as follows.  For predictor variable v1, the result of </p>

<ul>
<li>Adding the 'Sum Sq' entry for v1 together with half of the 'Sum Sq' entry for v1:v2 and half of the 'Sum Sq' entry for v1:v3, </li>
<li>Dividing by the sum of the entire 'Sum Sq' column, and</li>
<li>Multiplying by 100</li>
</ul>

<p>gives the percent of variance of the response variable that is explained by predictor variable v1 in the <code>lm()</code> model.  I don't see why this is nor why half of the 'Sum Sq' entry for v1:v2 is attributed to v1 and half to v2.  Is this just convenience?</p>
"
"0.02831827358943","0.0277563690826684","115343","<p>I have done a linear regression in R, using glm function. The calculated intercept says 0.98, but when I plot it, it does not seem to hit the estimated intercept on Y axis. Its far below. Here are my data and function: </p>

<pre><code>event = c(2.2, 6.4, 3.4, 10.2, 4.45, 2.65, 8.25, 4.65, 3, 6.5, 5.25, 
8.65, 7.25, 6.4, 7.75, 7.45)

c(230208, 813178, 316617, 1531919, 576869, 270148, 1090947, 562643, 
439885, 745741, 666454, 1078175, 924429, 784333, 1091289, 948062)

fit=glm(event~size)

Call:  glm(formula = chr.co.count.wt ~ size)

Coefficients:
(Intercept)         size  
  9.783e-01    6.528e-06  

Degrees of Freedom: 15 Total (i.e. Null);  14 Residual
Null Deviance:      83.08 
Residual Deviance: 2.849    AIC: 23.8




plot(size,events,col=""blue"",pch=16,xlab=""size"",ylab=""events"",ylim=c(0,12),frame.plot=FALSE,xlim=c(0,2000000),axes = F)

axis(side = 1,at = c(0,0.5e6,1e6,1.5e6),labels =  c(0,0.5e6,1e6,1.5e6))
axis(side = 2,at = seq(from = 0,to = 12,by = 0.5),labels = seq(from = 0,to = 12,by = 0.5))
abline(fit.wt)
</code></pre>

<p><img src=""http://i.stack.imgur.com/iqDK2.png"" alt=""enter image description here""></p>

<p>Why is this discrepancy ? Am i missing something here ? I have also checked the std. err which is 0.27, still higher than what is being observed on plot. </p>

<p>Thank you.</p>
"
"0.0942489115008991","0.100077011948264","115356","<p>I'm a beginner in statistics and I have to run multilevel logistic regressions. I am confused with the results as they differ from logistic regression with just one level. </p>

<p>I don't know how to interpret the variance and correlation of the random variables. And I wonder how to compute the ICC.</p>

<p>For example : I have a dependent variable about the protection friendship ties give to individuals (1 is for individuals who can rely a lot on their friends, 0 is for the others). There are 50 geographic clusters of respondant and one random variable which is a factor about the social situation of the neighborhood. Upper/middle class is the reference, the other modalities are working class and underprivileged neighborhoods. </p>

<p>I get these results :</p>

<pre><code>&gt; summary(RLM3)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Arp ~ Densite2 + Sexe + Age + Etudes + pcs1 + Enfants + Origine3 +      Sante + Religion + LPO + Sexe * Enfants + Rev + (1 + Strate |  
    Quartier)
   Data: LPE
Weights: PONDERATION
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  3389.9   3538.3  -1669.9   3339.9     2778 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2216 -0.7573 -0.3601  0.8794  2.7833 

Random effects:
 Groups   Name           Variance Std.Dev. Corr       
 Neighb. (Intercept)     0.2021   0.4495              
          Working Cl.    0.2021   0.4495   -1.00      
          Underpriv.     0.2021   0.4495   -1.00  1.00
Number of obs: 2803, groups:  Neigh., 50

Fixed effects:
</code></pre>

<p>The differences with the ""call"" part is due to the fact I translated some words.</p>

<p>I think I understand the relation between the random intercept and the random slope for linear regressions but it is more difficult for logistics ones. I guess that when the correlation is positive, I can conclude that the type of neighborhood (social context) has a positive impact on the protectiveness of friendship ties, and conversely. But how do I quantify that ?</p>

<p>Moreover, I find it odd to get correlation of 1 or -1 and nothing more intermediate.</p>

<p>As for the ICC I am puzzled because I have seen a post about lmer regression that indicates that intraclass correlation can be computed by dividing the variance of the random intercept by the variance of the random intercept, plus the variance the random variables, plus the residuals. </p>

<p>But there are no residuals in the results of a glmer. I have read in a book that ICC must be computed by dividing the random intercept variance by the random intercept variance plus 2.36 (piÂ²/3). But in another book, 2.36 was replaced by the inter-group variance (the first level variance I guess). 
What is the good solution ?</p>

<p>I hope these questions are not too confused.
Thank you for your attention !</p>
"
"0.0863948355424908","0.0846805485716084","116272","<p>I have read similar posts to this but my problem is not resolved by the answers given. I want to do a v simple linear regression to see if bite incidence is related to district, zone (vacc or control) and year. As you can see in the output one of the districts RORYA is given NA coefficients, and I get the message ""Coefficients: (1 not defined because of singularities)"". I have read up on this and it seems its to do with co-linearity of factors. One solution given is to add -1 to the call, which removes the intercept but does not solve my problem as RORYA district still has NAs in the summary output.</p>

<p>Another solution I have tried is changing the order of the explanatory variables in the call. This does change things...Rorya district suddenly has coefficients but the Zone variable becomes NA'd. Neither of which is good as I would like a coefficent for all the explanatory variables.</p>

<p>I was wondering whether anyone might know why this is happening and whether there is a solution to this problem so that all the variables can have coefficients?</p>

<p>Thanks in advance.</p>

<p>A Reproducible example:</p>

<pre><code>df &lt;- structure(list(DISTRICT = structure(c(1L, 6L, 5L, 3L, 2L, 4L, 
1L, 6L, 5L, 3L, 2L, 4L, 1L, 6L, 5L, 3L, 2L, 4L, 1L, 6L, 5L, 3L, 
2L, 4L), .Label = c(""BUNDA"", ""MASWA"", ""MUSOMA"", ""RORYA"", ""SERENGETI"", 
""TARIME""), class = ""factor""), zone = structure(c(2L, 2L, 2L, 
1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 
2L, 2L, 1L, 1L, 1L), .Label = c(""c"", ""v""), class = ""factor""), 
year = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L), .Label = c(""2010"", 
""2011"", ""2012"", ""2013""), class = ""factor""), bites = c(7.461327937, 
NA, NA, NA, 35.16164185, 26.39109338, 57.89990479, 1.47191729, 
3.608371422, 51.36718605, NA, 16.21167165, 46.85713945, 15.89670673, 
5.212092054, 259.8137381, 30.80276062, 20.73585909, 10.44585911, 
9.420270656, 7.617673001, 307.4586643, 27.31565565, 30.16124958
), deaths = c(0, NA, NA, NA, 0, 1.508062479, 0.298453117, 
0, 0, 0, NA, 2.262093719, 0.298453117, 0.294383458, 0, 2.233355915, 
0.581184163, 1.131046859, 0.298453117, 0.588766916, 1.202790474, 
2.977807887, 0, 1.885078099)), .Names = c(""DISTRICT"", ""zone"", 
""year"", ""bites"", ""deaths""), row.names = c(NA, -24L), class = ""data.frame"")
</code></pre>

<p>Code:</p>

<pre><code>summary(df )
names(df)
attach(df)
is.numeric(year)
df$year  &lt;- as.factor(as.character(df$year))
is.factor(df$year)

model1 &lt;- lm(bites ~   zone + DISTRICT-1 +year, data = df)
summary(model1)

&gt; sessionInfo()
R version 3.1.0 (2014-04-10)
Platform: x86_64-apple-darwin13.1.0 (64-bit)

locale:
[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8

attached base packages:
[1] grid      stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] ggplot2_1.0.0

loaded via a namespace (and not attached):
[1] colorspace_1.2-4 digest_0.6.4     gtable_0.1.2     MASS_7.3-34      munsell_0.4.2   plyr_1.8.1       proto_0.3-10     Rcpp_0.11.2     
[9] reshape2_1.4     scales_0.2.4     stringr_0.6.2    tools_3.1.0  
</code></pre>
"
"0.02831827358943","0","116428","<p>I have two data sets with y and X1,X2,X3,....,X6 all Xi's are independent variables and y is dependent variable. </p>

<p>Instructor said that one data set will follow linear regression and other will follow non- linear regression</p>

<p>I don't have any clue about this, how to check it (linearity/ Non-linearity) with my dataset </p>

<p>Thanks in Adv </p>
"
"0.0326991257596857","0.0480754414848157","116487","<p>I need to predict payment day of the month (1-31) for each client (I have at most 9 month of payments and on average is 5). I have both categorical variables and numerical. I tried to use rpart to do a regression tree (method='anova') but I'm not sure if it's using the nominal variables. </p>

<p>I also tried a regression (linear to start) and doesn't work good either, but it's better then the regression tree.</p>

<p>If I use a Weibull for this, will it mean that each client is going to have a parameter of shape and scale? what about the other variables? How can I insert them into the distribution?</p>

<p>So, what model would you recommend?</p>

<p>Thanks</p>
"
"0.02831827358943","0.0277563690826684","116560","<p>It seems odd to scale a categorical variable, but I need to get the correct coefficients for each of my variables in linear regression. Is it correct to scale the same way you would with continuous variables, or what is the right thing to do here? </p>

<p>For example if x is categorical and y is continuous:</p>

<pre><code>model=lm(DV ~ scale(x) + scale(y), data=myData)
</code></pre>

<p>Is the above the right thing to do?</p>
"
"0.0490486886395286","0.0480754414848157","116681","<p>I am working on a large linear regression with a volume metric as my dependent. Right now I am multiplying the model.matrix by the respective coefficients to get to the relative volume contribution by variable.</p>

<pre><code>decomp =  t(apply(model.matrix(fit$terms, data = Data[Data$RetailRead == ""Retail"",]), 1, function(x) {x*fit$coef})) 
</code></pre>

<p>However, this leads to some of the volume being meaningless due to a factor variables within the model. The factor variables need to be added to the base category in order to get to the total overall volume for that variable. </p>

<pre><code>asgn = attr(model.matrix(fit$terms, data = Data), ""assign"") #find indexes which need to be summed

merged =data.frame(t(apply(decomp, 1, function(x) {tapply(x, asgn, sum)})))#sum each appropriate collumn
colnames(merged) = c(""Intercept"", attr(terms(fit), ""term.labels"")) #label collumns
</code></pre>

<p>The code I have written to summarize the volume driven per variable is clunky and inefficient - there are several further steps then above to allow each column of the variable decomposition to be able to stand alone. I am surprised that there is little literature on doing this within R and I wonder if any one know of a package to better perform this task.</p>
"
"0.04281320541504","0.0629455284778823","116825","<p>I'm exploring linear regressions in R and Python, and usually get the same results but this is an instance I do not. </p>

<p>I added the sum of <code>Agriculture</code> and <code>Education</code> to the <code>swiss</code> dataset as an additional explanatory variable, with <code>Fertility</code> as the regressor.</p>

<p>R gives me an <code>NA</code> for the $\beta$ value of <code>z</code>, but Python gives me a numeric value for <code>z</code> and a warning about a very small eigenvalue. Is there a way to make Python and statmodels explicitly tell me that <code>z</code> adds no information to the regressor?</p>

<p>Additionally, I originally did this analysis in an iPython notebook, where there is no need to do an explicit <code>print</code> of the regression summary results <code>reg_results</code>, and when the <code>print</code> command is omitted there is no warning about the low eigenvalues which makes it more difficult to know that <code>z</code> is worthless.</p>

<p>R code:</p>

<pre><code>data(swiss)
swiss$z &lt;- swiss$Agriculture + swiss$Education
formula &lt;- 'Fertility ~ .'
print(lm(formula, data=swiss))
</code></pre>

<p>R output:</p>

<pre><code>Call:
lm(formula = formula, data = swiss)

Coefficients:
     (Intercept)       Agriculture       Examination         Education
         66.9152           -0.1721           -0.2580           -0.8709
        Catholic  Infant.Mortality                 z
          0.1041            1.0770                NA
</code></pre>

<p>Python Code:</p>

<pre><code>import statsmodels.formula.api as sm
import pandas.rpy.common as com

swiss = com.load_data('swiss')

# get rid of periods in column names
swiss.columns = [_.replace('.', '_') for _ in swiss.columns]

# add clearly duplicative data
swiss['z'] = swiss['Agriculture'] + swiss['Education']

y = 'Fertility'
x = ""+"".join(swiss.columns - [y])
formula = '%s ~ %s' % (y, x)
reg_results = sm.ols(formula, data=swiss).fit().summary()
print(reg_results)
</code></pre>

<p>Python output:</p>

<pre><code>                            OLS Regression Results
==============================================================================
Dep. Variable:              Fertility   R-squared:                       0.707
Model:                            OLS   Adj. R-squared:                  0.671
Method:                 Least Squares   F-statistic:                     19.76
Date:                Thu, 25 Sep 2014   Prob (F-statistic):           5.59e-10
Time:                        22:55:42   Log-Likelihood:                -156.04
No. Observations:                  47   AIC:                             324.1
Df Residuals:                      41   BIC:                             335.2
Df Model:                           5
====================================================================================
                       coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------------
Intercept           66.9152     10.706      6.250      0.000        45.294    88.536
Agriculture          0.1756      0.062      2.852      0.007         0.051     0.300
Catholic             0.1041      0.035      2.953      0.005         0.033     0.175
Education           -0.5233      0.115     -4.536      0.000        -0.756    -0.290
Examination         -0.2580      0.254     -1.016      0.315        -0.771     0.255
Infant_Mortality     1.0770      0.382      2.822      0.007         0.306     1.848
z                   -0.3477      0.073     -4.760      0.000        -0.495    -0.200
==============================================================================
Omnibus:                        0.058   Durbin-Watson:                   1.454
Prob(Omnibus):                  0.971   Jarque-Bera (JB):                0.155
Skew:                          -0.077   Prob(JB):                        0.925
Kurtosis:                       2.764   Cond. No.                     1.11e+08
==============================================================================

Warnings:
[1] The smallest eigenvalue is 3.87e-11. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
</code></pre>

<p>```</p>
"
"0.0693653206906364","0.0679889413649005","117052","<p>I have been trying to replicate the results of the Stata option <code>robust</code> in R. I have used the <code>rlm</code> command form the MASS package and also the command <code>lmrob</code> from the package ""robustbase"". In both cases the results are quite different from the ""robust"" option in Stata. Can anybody please suggest something in this context?</p>

<p>Here are the results I obtained when I ran the robust option in Stata:</p>

<pre><code>. reg yb7 buildsqb7 no_bed no_bath rain_harv swim_pl pr_terrace, robust

Linear regression                                      Number of obs =    4451
                                                       F(  6,  4444) =  101.12
                                                       Prob &gt; F      =  0.0000
                                                       R-squared     =  0.3682
                                                       Root MSE      =   .5721

------------------------------------------------------------------------------
             |               Robust
         yb7 |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
   buildsqb7 |   .0046285   .0026486     1.75   0.081    -.0005639     .009821
      no_bed |   .3633841   .0684804     5.31   0.000     .2291284    .4976398
     no_bath |   .0832654   .0706737     1.18   0.239    -.0552904    .2218211
   rain_harv |   .3337906   .0395113     8.45   0.000     .2563289    .4112524
     swim_pl |   .1627587   .0601765     2.70   0.007     .0447829    .2807346
  pr_terrace |   .0032754   .0178881     0.18   0.855    -.0317941    .0383449
       _cons |   13.68136   .0827174   165.40   0.000     13.51919    13.84353
</code></pre>

<p>And this is what I obtained in R with the lmrob option:</p>

<pre><code>&gt; modelb7&lt;-lmrob(yb7~Buildsqb7+No_Bed+Rain_Harv+Swim_Pl+Gym+Pr_Terrace, data&lt;-bang7)
&gt; summary(modelb7)

Call:
lmrob(formula = yb7 ~ Buildsqb7 + No_Bed + Rain_Harv + Swim_Pl + Gym + Pr_Terrace, 
    data = data &lt;- bang7)
 \--&gt; method = ""MM""
Residuals:
      Min        1Q    Median        3Q       Max 
-51.03802  -0.12240   0.02088   0.18199   8.96699 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 12.648261   0.055078 229.641   &lt;2e-16 ***
Buildsqb7    0.060857   0.002050  29.693   &lt;2e-16 ***
No_Bed       0.005629   0.019797   0.284   0.7762    
Rain_Harv    0.230816   0.018290  12.620   &lt;2e-16 ***
Swim_Pl      0.065199   0.028121   2.319   0.0205 *  
Gym          0.023024   0.014655   1.571   0.1162    
Pr_Terrace   0.015045   0.013951   1.078   0.2809    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Robust residual standard error: 0.1678 
Multiple R-squared:  0.8062,    Adjusted R-squared:  0.8059 
</code></pre>
"
"0.0200240432865818","0.039253433598943","117437","<p>I want to replicate a fuzzy regression using a linear programming problem approach. </p>

<p>I have the following information: "" A fuzzy regression analysis with only one independent variable X results in the following bivariate regression model:
$$ \hat{Y}=\tilde{A_o}+\tilde{A_1}X,$$</p>

<p>where $\tilde{A_o}$ is a is a fuzzy intercept, $\tilde{A_1}$is a fuzzy slope coefficient, the parameters are expressed as $\tilde{A_i}=(m_i,c_i)$ where $m_i$ is a centre  and $c_i$ is the fuzzy half-width.""</p>

<p>To determine the fuzzy coefficients the following linear programming problem is used:</p>

<p>minimize $$ S= nc_0 + c_1\sum_{i=1}^{n}|X_i|$$</p>

<p>subject to $$c_0\geqslant0,\geqslant0,$$</p>

<p>$$\sum_{j=0}^{l}m_iX_{ij}+(1-h)\sum_{j=0}^{l}c_i|X_{ij}| \geqslant Y_i+(1-h), \mbox{for i=1 to n}$$</p>

<p>$$\sum_{j=0}^{l}m_iX_{ij}-(1-h)\sum_{j=0}^{l}c_i|X_{ij}| \geqslant Y_i-(1-h), \mbox{for i=1 to n}$$</p>

<p>where $h=0$</p>

<p>I have the following data :
$[X_i : Y_i]=[(2:14),(4:16),(6:14),(8:18),(10:18),(12:22),(14:18),
(16:22)]$</p>

<p>How to  solve the LP using R?</p>
"
"0.02831827358943","0.0277563690826684","117489","<p>From my limited statistical knowledge, I could use MANOVA if I had multiple independent variables (x1, x2...xn). What can I do (specifically in R) with one ""x"" variable and multiple ""y"" groups? I'm trying to see if there is any relationship between the y's with respect to their regression with x. I've already set up a loop that computes bivariate, piecewise linear regressions between each pair (x-y1, x-y2, ... x-yn), but that does not include any analysis of variation between the y variables. Does anybody know how I might do this (in a statistically sound manner, of course) in R? My data looks like this:</p>

<pre><code>x         y1       y2      y3      y4      y5
4.19    5.51    19.76   50.00   19.36   54.07
8.60    10.16   33.01   82.99   38.48   44.95
8.03    7.82    31.29   79.05   40.12   59.18
6.64    8.99    27.13   69.13   30.44   59.02
7.03    8.22    25.29   74.45   36.02   50.88
1.50    5.90    10.69   22.88   10.34   34.50
4.36    7.61    19.27   44.47   20.06   24.62
7.17    8.30    26.72   68.68   31.61   20.16
2.68    5.61    14.25   37.07   15.20   67.75
7.91    7.75    30.93   82.01   38.62   65.36
3.74    5.24    16.42   40.17   17.54   15.19
</code></pre>
"
"0.132238914103567","0.134600158411001","117664","<p>In order to run Lasso and elastic net multiple regressions on my company's SAS server (which doesn't support R), I've been working on a coordinate descent macro for performing least squares regressions (as described in the 2010 paper <a href=""http://www.jstatsoft.org/v33/i01/paper"" rel=""nofollow"">""Regularization Paths for Generalized Linear Models via Coordinate Descent""</a> by Jerome Friedman, Trevor Hastie, and Rob Tibshirani). </p>

<p>Ideally, I would like the coefficient estimates from my SAS algorithm to match the outcomes from the  <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"" rel=""nofollow"">glmnet package</a> in R written by Friedman, et al. which also implements coordinate descent for least squares regression. </p>

<p>I've decided to test the algorithm on the Fitness data from SAS documentation, with Oxygen as the response variable: </p>

<pre><code>data fitness;
  input Age Weight Oxygen RunTime RestPulse RunPulse MaxPulse @@;
  datalines;
   44 89.47 44.609 11.37 62 178 182   40 75.07 45.313 10.07 62 185 185
   44 85.84 54.297  8.65 45 156 168   42 68.15 59.571  8.17 40 166 172
   38 89.02 49.874  9.22 55 178 180   47 77.45 44.811 11.63 58 176 176
   40 75.98 45.681 11.95 70 176 180   43 81.19 49.091 10.85 64 162 170
   44 81.42 39.442 13.08 63 174 176   38 81.87 60.055  8.63 48 170 186
   44 73.03 50.541 10.13 45 168 168   45 87.66 37.388 14.03 56 186 192
   45 66.45 44.754 11.12 51 176 176   47 79.15 47.273 10.60 47 162 164
   54 83.12 51.855 10.33 50 166 170   49 81.42 49.156  8.95 44 180 185
   51 69.63 40.836 10.95 57 168 172   51 77.91 46.672 10.00 48 162 168
   48 91.63 46.774 10.25 48 162 164   49 73.37 50.388 10.08 67 168 168
   57 73.37 39.407 12.63 58 174 176   54 79.38 46.080 11.17 62 156 165
   52 76.32 45.441  9.63 48 164 166   50 70.87 54.625  8.92 48 146 155
   51 67.25 45.118 11.08 48 172 172   54 91.63 39.203 12.88 44 168 172
   51 73.71 45.790 10.47 59 186 188   57 59.08 50.545  9.93 49 148 155
   49 76.32 48.673  9.40 56 186 188   48 61.24 47.920 11.50 52 170 176
   52 82.78 47.467 10.50 53 170 172
   ;
run;
</code></pre>

<p>Here's my first attempt at writing the code for a simple OLS model. (I realize running a data set inside a macro loop is bad form &amp; slows down execution times - this is just a first pass at the problem.)</p>

<p>For the example here I'm fitting a model for a single value of lambda and alpha in an elastic net model. I'm achieving the closest match to glmnet output when I standardize the six predictor variables using proc standard. Initial values for the coefficients are fit via proc reg. Output coefficient values are then converted back to the original unstandardized scale (scroll to bottom of the code below). </p>

<pre><code>            /* Calculate mean and stnd dev values for standardizing fitness variables. */
            proc means data=fitness mean std;
               var Oxygen Age Weight RunTime RestPulse RunPulse MaxPulse;
               output out=fitness_mean_std;
            run;

            data fitness_mean_std (drop=_TYPE_ _FREQ_);
            set fitness_mean_std;
               if _STAT_ in ('MEAN','STD');
            run;

            %let t=7;
            data _null_;
            set fitness_mean_std;
               if _STAT_='MEAN' then do;
                 array mean[1:&amp;t] Oxygen Age Weight RunTime RunPulse RestPulse MaxPulse;
                 do m = 1 to &amp;t;
                    call symputx(cats('mean',m),mean[m],'g');
                 end;
               end;
               else if _STAT_='STD' then do;
                 array std[1:&amp;t] Oxygen Age Weight RunTime RunPulse RestPulse MaxPulse;
                 do s = 1 to &amp;t;
                     call symputx(cats('std',s),mean[s],'g');
                 end;
               end;
            run;

            /* Create input dataset for coordinate descent macro. */
            proc standard data=fitness mean=0 std=1 out=fitness_stnd;
               var Age Weight RunTime RunPulse RestPulse MaxPulse;
            run;

            proc reg data=fitness_stnd outest=params_stnd;
               model Oxygen = Age Weight RunTime RunPulse RestPulse MaxPulse;
            run;
            quit;

            %let t=6;
            data _null_;
            set params_stnd;
               array x[0:&amp;t] Intercept Age Weight RunTime RunPulse RestPulse MaxPulse;
               do _n_ = 0 to &amp;t;
                  call symputx(cats('p',_n_),x[_n_],'g');
               end;
            run;
            %put &amp;p0 &amp;p1 &amp;p2 &amp;p3 &amp;p4 &amp;p5 &amp;p6;

            %macro assignvar(k);
            data fitness_array (drop=Oxygen Age Weight RunTime RunPulse RestPulse MaxPulse);
            set fitness_stnd;
               y=Oxygen;
               array a[6] Age Weight RunTime RunPulse RestPulse MaxPulse;
               array x[6];
               %do i=1 %to 6;
                 x[&amp;i]=a[&amp;i];
               %end;
            run;
            %mend;
            %assignvar(6)    

            /* Coordinate descent macro. */
            %macro test(dataset=, numvars=, numiter=, lambda=, alpha=);
               %do i=1 %to &amp;numiter;
                 %do j=1 %to &amp;numvars;
                    data &amp;dataset (keep=y x1-x&amp;numvars);
                    set &amp;dataset end=end_data;
                       array x[&amp;numvars] x1-x&amp;numvars;
                       %let gamma = %sysevalf(&amp;lambda*&amp;alpha);

                       /* Calculate partial residuals for fitting coefficients.*/
                       yhat_&amp;j = &amp;p0 - &amp;&amp;p&amp;j*x[&amp;j];
                       %do k=1 %to &amp;numvars;
                            yhat_&amp;j = yhat_&amp;j + &amp;&amp;p&amp;k*x[&amp;k];
                       %end;
                       if _n_=1 then z_&amp;j = x&amp;j*(y - yhat_&amp;j);                                           else z_&amp;j = x&amp;j*(y - yhat_&amp;j) + z_&amp;j; end;
                       if end_data then do;
                          z_avg_&amp;j = z_&amp;j/_n_;
                          if (z_avg_&amp;j &gt; 0 and &amp;gamma &lt; abs(z_avg_&amp;j)) then do;
                             p&amp;j = (z_avg_&amp;j - &amp;gamma)/(1 + &amp;lambda - &amp;gamma);
                             call symputx(""p&amp;j"", p&amp;j, 'g');
                          end;  
                          else if (z_avg_&amp;j &lt; 0 and &amp;gamma &lt; abs(z_avg_&amp;j)) then do;
                             p&amp;j = (z_avg_&amp;j + &amp;gamma)/(1 + &amp;lambda - &amp;gamma);
                             call symputx(""p&amp;j"", p&amp;j, 'g');
                          end;
                          else if &amp;gamma &gt;= abs(z_avg_&amp;j) then do;
                             p&amp;j = 0;
                             call symputx(""p&amp;j"", p&amp;j, 'g');
                          end;
                       end;
                       retain z_&amp;j;
                    run;
                 %end;
               %end;
               %put _user_;
            %mend;
            %test(dataset=fitness_array, numvars=6, numiter=50, lambda=.1, alpha=.5)            

            /* Return regression coefficients in original scale. */
                %let p0_unstand = %sysevalf(&amp;p0-(&amp;p1*&amp;mean2/&amp;std2)-(&amp;p2*&amp;mean3/&amp;std3)-(&amp;p3*&amp;mean4/&amp;std4)-(&amp;p4*&amp;mean5/&amp;std5)-(&amp;p5*&amp;mean6/&amp;std6)-(&amp;p6*&amp;mean7/&amp;std7));
                %let p1_unstand = %sysevalf(&amp;p1/&amp;std2);
                %let p2_unstand = %sysevalf(&amp;p2/&amp;std3);
                %let p3_unstand = %sysevalf(&amp;p3/&amp;std4);
                %let p4_unstand = %sysevalf(&amp;p4/&amp;std5);
                %let p5_unstand = %sysevalf(&amp;p5/&amp;std6);
                %let p6_unstand = %sysevalf(&amp;p6/&amp;std7);

                %put 
                p0_unstand = &amp;p0_unstand 
                p1_unstand = &amp;p1_unstand 
                p2_unstand = &amp;p2_unstand 
                p3_unstand = &amp;p3_unstand 
                p4_unstand = &amp;p4_unstand 
                p5_unstand = &amp;p5_unstand 
                p6_unstand = &amp;p6_unstand;
                /*
                p0_unstand = 107.068671308076 
                p1_unstand = -0.24178321947146 
                p2_unstand = -0.05008520720235
                p3_unstand = -2.47736090772018 
                p4_unstand = -0.16124847253703 
                p5_unstand = -0.03822018686055
                p6_unstand = 0.06524084784678
                */
</code></pre>

<p>The corresponding commands in R:</p>

<pre><code>&gt; elasticnet_fit = glmnet(x, y, family=""gaussian"", lambda=.1, alpha=.5); 
&gt; coef(elasticnet_fit);   
7 x 1 sparse Matrix of class ""dgCMatrix""
                      s0
(Intercept) 105.10824556
x1           -0.22996264
x2           -0.05775625
x3           -2.64766834
x4           -0.01998125
x5           -0.26222028
x6            0.18003526
</code></pre>

<p>My question: The coefficients output from the SAS macro doesn't match the output from glmnet, although the values are close. Is there a flaw in my code or should I not be too concerned? Thanks!</p>
"
"0.0566365471788599","0.0555127381653369","117867","<p>I have a basic linear regression model I fitted to a time series. Unfortunately I have to account for autocorrelation and heteroskedasicity in the model and I have done so with the NeweyWest function from the sandwich package in R while analyzing the coefficients. </p>

<p>Now I would like to create prediction intervals using the predict() function (or any other function) while utilizing the NeweyWest matrix/SEs.</p>

<p>As this is the first quesiton I post on here and my experinece in R is very limited here is some information:</p>

<pre><code>LMModel = lm(Return~Sentiment, data=Time Series)
</code></pre>

<p><strong>This is the function I used for my coefficient testing:</strong></p>

<pre><code>coeftest(LMModel , vcov=NeweyWest(LMModel , lag=27, ar.method=""ols""))
</code></pre>

<p><strong>I would like thsi function to use NeweyWest in some way:</strong></p>

<pre><code>predict(LMModel, newdata, interval = ""prediction"", level = 0.95) 
</code></pre>

<p>Thanks a lot in advance!</p>
"
"0.0326991257596857","0.0480754414848157","118051","<p>I am performing the multiple linear regression below in R to predict returns on fund managed. </p>

<pre><code>reg &lt;- lm(formula=RET~GRI+SAT+MBA+AGE+TEN, data=rawdata)
</code></pre>

<p>Here only GRI &amp; MBA are binary/dichotomous predictors; the remaining predictors are continuous.</p>

<p>I am using this code to generate residual plots for the binary variables.</p>

<pre><code>plot(rawdata$GRI, reg$residuals)
abline(lm(reg$residuals~rawdata$GRI, data=rawdata), col=""red"") # regression line (y~x) 

plot(rawdata$MBA, reg$residuals)
abline(lm(reg$residuals~rawdata$MBA, data=rawdata), col=""red"") # regression line (y~x) 
</code></pre>

<p><strong>My Question:</strong>
I know how to inspect residual plots for continuous predictors but how do you test  assumptions of linear regression such as homoscedasticity when an independent variable is binary? </p>

<p><strong>Residual Plots:</strong></p>

<p><img src=""http://i.stack.imgur.com/7a7C7.jpg"" alt=""Residual Plot for GR1"">
<img src=""http://i.stack.imgur.com/bqbVJ.jpg"" alt=""Residual Plot for MBA""></p>
"
"0.0942489115008991","0.0923787802599364","118394","<p>I have a dataset in which individuals were assessed at two time points during the study on a cognitive test, as such I was wondering which statistical model would be more appropriate for my data, either linear regression or mixed effects models? </p>

<p>The average length of follow up for my data is 59 months with a standard deviation of 43.03 (range is 0.63-167 months) with 88 (33%) of people having data for only one time point. </p>

<p>For linear regression, the approach I was thinking utilising was taking the delta of the test score between the two time points and regressing that against time (months between test scores). </p>

<p>If I used mixed effects models, the main issue I have is how to handle individuals who have only wave of data? While I know mixed effects models are especially robust in regards to the analysis of unbalanced data, would 33% missingnes cause issues?</p>

<p>Just sample R code highlighting the output using either linear regression or mixed models.</p>

<pre><code>fm1 &lt;- lm(mmse_difference ~ mmse_months_between*ORgrs_apoe, data = dat.wide)
summary(fm1)

Call:
lm(formula = mmse_difference ~ mmse_months_between * ORgrs_apoe, 
    data = newdat)

Residuals:
    Min      1Q  Median      3Q     Max 
-20.960  -3.957   1.854   5.200  12.550 

Coefficients:
                               Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)                    -2.74185    2.20667  -1.243    0.216
mmse_months_between            -0.01768    0.03051  -0.579    0.563
ORgrs_apoe                      0.35163    1.17782   0.299    0.766
mmse_months_between:ORgrs_apoe -0.01973    0.01748  -1.129    0.261

Residual standard error: 7.3 on 170 degrees of freedom
  (88 observations deleted due to missingness)
Multiple R-squared:  0.08481,   Adjusted R-squared:  0.06866 
F-statistic: 5.251 on 3 and 170 DF,  p-value: 0.001725
Num. obs. 174

fm2 &lt;- lme(mmse ~ mmse_months*ORgrs_apoe, random = ~mmse_months|patientid, data = dat.long, method = ""ML"", na.action = na.exclude)
summary(fm2)
Linear mixed-effects model fit by maximum likelihood
 Data: dat.long 
       AIC      BIC    logLik
  2797.467 2829.537 -1390.733

Random effects:
 Formula: ~mmse_months | patientid
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr  
(Intercept) 7.2972822 (Intr)
mmse_months 0.1132399 0.85  
Residual    2.9431616       

Fixed effects: mmse ~ mmse_months * ORgrs_apoe 
                           Value Std.Error  DF   t-value p-value
(Intercept)            24.635821 1.0959420 231 22.479130  0.0000
mmse_months            -0.069918 0.0223198 172 -3.132544  0.0020
ORgrs_apoe             -1.283348 0.6062892 231 -2.116726  0.0354
mmse_months:ORgrs_apoe -0.024952 0.0130561 172 -1.911103  0.0577
 Correlation: 
                       (Intr) mms_mn ORgrs_
mmse_months             0.438              
ORgrs_apoe             -0.882 -0.377       
mmse_months:ORgrs_apoe -0.357 -0.891  0.397

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-3.48643949 -0.31734164  0.07636708  0.26575764  2.49901891 

Number of Observations: 407
Number of Groups: 233 
</code></pre>

<p>Thanks.     </p>
"
"0.0961546625894718","0.0875148082146538","118621","<p>I am trying to perform a relatively simple multiple regression with a single breakpoint using the <em>segmented</em> package in R. My question is how to handle the interaction between the predictor variables as follows.</p>

<p>My model is: $$Runoff = \beta_0+\beta_1(A+\beta_2P)$$ where there is a breakpoint at some value of $(A+\beta_2P)$. I expect $\beta_0$ and $\beta_1$ to be different on each side of the breakpoint, and $\beta_2$ to be the same (in fact I'll just say <em>a priori</em> that $\beta_2$ should not vary across the breakpoint, but this would be interesting to test)</p>

<p>My question is how to implement this using <em>segmented</em> (or a similar breakpoint linear regression model). Options are:<br>
1. Run some loop through values of $\beta_2$ and create an intermediate variable $\tau = (A+\beta_2P)$, then it's a simple application of <em>segmented</em>. My concern is that I'd like to get a fitted value of $\beta_2$ from the data and this seems like a crude way of doing it.<br>
2. Some clever way of using <em>segmented</em> on the fully explicit model that will estimate $\beta_0, \beta_1,$ and $\beta_2$<br>
any help appreciated</p>

<p><strong>ADDENDUM:</strong></p>

<p>To describe what I'm trying to do. This is for a watershed runoff generation project. My hypothesis is that runoff amount is a function of both precipitation and soil water table position, and that there is a very strong threshold response in the latter. Whenever it rains, the water table goes up, but if it does not reach the threshold no runoff is generated. If enough rain falls to raise the water table above the threshold, then runoff is generated. Thus, runoff is a function of $(Ant + Pcp)$ with a notable breakpoint. </p>

<p>So, in the model as formulated in my original question $\beta_0$ and $\beta_1$ are the slope and intercept of the runoff response, and $\beta_2$ is a coefficient that converts rainfall depth to water table rise, which I assume is something like the porosity of the soil.  </p>

<p>I'm interested in (in order of importance): (1) is this model better than just a linear relationship between runoff and precipitation?; (2) can the model estimate where the water table threshold is?; (3) Can the model estimate the ""porosity"" coefficient (this would be a nice check against physical reality and I think also improve the physical fidelity of the estimated threshold position).  </p>

<p>Below is a graph using a hand-picked value of $\beta_2$, which let me do a piece wise linear regression. Solid line is the piece wise, dashed is a linear regression between runoff and precip alone. I'd like to be able to estimate the parameters of the model ($\beta_0, \beta_1, \beta_2$, and the breakpoint position) from the data. Ultimately, I have three replicates of three watershed types, so I'm interested in comparisons there as well.</p>

<p>Thanks for the help,</p>

<p><img src=""http://i.stack.imgur.com/L5JuQ.jpg"" alt=""enter image description here""></p>
"
"0.0853828074607","0.0836886016271203","119738","<p>I've used an ordinary least square linear regression model in R that looks something like this:</p>

<pre><code>ols &lt;- lm(DV ~ IV1 + IV2)
</code></pre>

<p>When I type this:</p>

<pre><code>summary(ols) 
</code></pre>

<p>I get a table showing Estimate, Std Error, t value and P(>|t|) for each coefficient. I also get the residual standard error, multiple r-square, adjusted r-square, f-statistic, and p-value for the model.</p>

<p>And I've used a robust linear regression model that looks something like this:</p>

<pre><code>roblm &lt;- rlm(DV ~ IV1 + IV2) 
</code></pre>

<p>When I type this:</p>

<pre><code>summary(roblm)
</code></pre>

<p>I get a table showing Value, Std Error, and t value for each coefficient. But I don't get a p-value for each coefficient. Similarly, I get the residual standard error for the model, but I don't get multiple r-square, adjusted r-square, f-statistic, and p-value for the model.</p>

<p>Why aren't these additional statistics provided for the robust linear regression model? Do they not make sense in the context of this model? If these statistics do make sense, how would I go about getting them? Any help would be greatly appreciated.</p>
"
"0.0642198081225601","0.0524546070649019","120008","<p>I am interested in fitting an ARIMAX model using R.
As known, ARIMAX can be understood as a composition of ARIMA models and regression models with exogenous (independent) variables. I have a time series $Y_i$, and want to estimate the ARIMA and nonlinear coefficients. The nonlinear model is the following:</p>

<p>$y_i=Î²_0+Î²_1t_i+Î²_2d+Î²_3 sin(2Ï€t_i/Î²_4 )+Î²_5 (-1^{t_i})+Îµ_i$,  nonlinear 
regression with an exogenous variable.
Where 
$t_i$ =1, 2â€¦, 60
and</p>

<p>d = dummy variable with 20 0's and 40 number 1's</p>

<pre><code>d=c(rep(0,20),rep(1,40))
</code></pre>

<p>And an ARIMA model (1,1,1) for $Y_i$. Therefore, I want to estimate simultaneously the $Î²_i$ and the ARIMA coefficients in order to avoid the confusion between the exogenous coefficients and ARIMA coefficients.  I know that $arima()$ can deal with this formulation but, how do the nonlinear model can be set within function function?. It seems that the <em>xreg</em> term only deals with linear parameters.</p>
"
"0.0578044339088637","0.0566574511374171","120201","<p>Being aware of <a href=""http://www.ssicentral.com/lisrel/techdocs/HowLargeCanaStandardizedCoefficientbe.pdf"" rel=""nofollow"">that article</a>, I am curious about the question how big standardized coefficients can get. I had a discussion with my professor about that issue and she was arguing standardized coefficients (beta) in multiple linear regressions can not become greater than |1|. I have also heard that predictors with standardized coefficients greater than 1 should not be be included/appear in multiple linear regression. When I recently estimated a multiple linear regression in R using lm(), I estimated the standardized coefficients with lm.beta() function from the package 'lm.beta'. In the results I could observe a standardized coefficient greater than one. Right now I am just not sure about what is the truth.</p>

<p>Can standardized coefficients become greater than |1|?
If yes, what does that mean and should they be excluded from the model?
If yes, why?</p>

<p>I would be very thankful, if somebody could make this issue clear for me. </p>

<p>Thanks in advance!!</p>
"
"0.0506572678011219","0.0620651280774201","120443","<p>I have run a linear regression with the following equation (in r):</p>

<pre><code>lm(formula = logTotal ~ Continent + logArea + Method + Servs)
</code></pre>

<p>where <code>Total</code> is $/ha/year (numeric), <code>Area</code> is hectare (numeric), <code>Continent</code> and <code>Method</code> are factors and <code>Servs</code> is numeric.  It returns the output:</p>

<pre><code>Call:
lm(formula = logTotal ~ Continent + logArea + Method + Servs)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.99416 -0.26931 -0.00622  0.28885  1.19875 

Coefficients:
                       Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)            -1.82886    0.71446  -2.560 0.016903 *  
ContinentAsia           3.82452    0.60471   6.325 1.28e-06 ***
ContinentAustralasia    4.52516    0.96517   4.688 8.35e-05 ***
ContinentEurope         2.18022    0.48260   4.518 0.000130 ***
ContinentGlobal         2.44750    0.74092   3.303 0.002881 ** 
ContinentNorth America  2.35244    0.55281   4.255 0.000256 ***
ContinentSouth America  3.67853    0.61454   5.986 2.99e-06 ***
logArea                 0.03643    0.03583   1.017 0.318911    
MethodCVM              -0.18171    0.43296  -0.420 0.678300    
MethodOther hedonic    -1.53284    0.79781  -1.921 0.066165 .  
MethodValue Transfer    0.98101    0.29773   3.295 0.002941 ** 
Servs                   0.10723    0.04273   2.509 0.018948 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.552 on 25 degrees of freedom
  (65 observations deleted due to missingness)
Multiple R-squared:  0.7582,    Adjusted R-squared:  0.6518 
F-statistic: 7.127 on 11 and 25 DF,  p-value: 2.501e-05  
</code></pre>

<p>I wish to predict <code>Total</code> based on various inputs, however I'm a bit lost on fully understanding the output. If I wished to predict ""Total"" on the basis of:</p>

<pre><code>Continent:Global, Area:1 hectare, Method:CVM, Servs:11
</code></pre>

<p>is the following equation correct?</p>

<pre><code>exp(Total) = 2.44750 + exp(1*0.03643) - 0.18171 + (11*0.10723)
</code></pre>

<p>I have read UCLA's statistics help site's <a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/log_transformed_regression.htm"" rel=""nofollow"">FAQ on log transformed regression</a>. I feel like I've oversimplified it but I just keep reading that link over and over and still not fully understanding.  Also read <a href=""http://stats.stackexchange.com/questions/20397/how-to-interpret-logarithmically-transformed-coefficients-in-linear-regression"">How to interpret logarithmically transformed coefficients in linear regression?</a>.</p>
"
"0.0578044339088637","0.0679889413649005","120526","<p>This is for my honors thesis. I have a large data set, of which I'm sharing only what I call the ""Low phosphorus"" series:</p>

<pre><code>&gt; P0
    R   N P D.weight
1  r1   0 0     63.8
2  r2   0 0     34.2
3  r3   0 0     24.9
4  r4   0 0     30.4
5  r5   0 0     33.3
6  r1  45 0     24.5
7  r2  45 0     20.1
8  r3  45 0     23.7
9  r4  45 0     20.0
10 r5  45 0     66.8
11 r1  90 0     27.8
12 r2  90 0     17.2
13 r3  90 0     36.4
14 r4  90 0     33.5
15 r5  90 0     14.0
16 r1 180 0     20.6
17 r2 180 0      9.7
18 r3 180 0      8.8
19 r4 180 0     14.4
20 r5 180 0     21.6
21 r1 360 0     18.4
22 r2 360 0      8.9
23 r3 360 0     31.4
24 r4 360 0     13.3
25 r5 360 0     21.9
</code></pre>

<ul>
<li>R is rep</li>
<li>N is nitrogen applied to the soil</li>
<li>P is phosphorus applied to the soil</li>
<li>D.weight is average dry weight of plants in grams</li>
</ul>

<p>The way to visualize these data is to put N on the x-axis and dry weight on the y axis:</p>

<p><img src=""http://i.stack.imgur.com/o4gKS.png"" alt=""Plot""></p>

<p>I have to make a nonlinear regression of these data, but I don't want to fit it to a quadratic model; instead, I wanna fit it to the equation below (an alternative to the Mitscherlich equation):</p>

<p>$Y = a - b \times\exp(-cx)$</p>

<ul>
<li>Y is dry weight</li>
<li>a is a fitted parameter representing the maximum biomass</li>
<li>b is a fitted parameter representing the initial level of the added nutrient in the soil</li>
<li>c is a fitted parameter representing the rate of increase in biomass with increasing nutrient amendment</li>
<li>x is, in this case, nitrogen level </li>
</ul>

<p>The problem is, I just don't know how to code for this. I have been going crazy trying to find out how to ""tell"" R that I wanna use that equation for my regression, and not $Y = ax + b$ (like in a linear regression), or $Y = ax^2 + bx + c$ like in a quadratic regression, etc.</p>
"
"0.0424774103841449","0.0555127381653369","120749","<p>I'm working on a biological question, with species data derived from an external database, which has multiple response and predictor variables. As a result, I want to do multivariate regression across a phylogeny to empirically test if my response variables are significantly different in respect to my predictors.</p>

<p>Please refer to source [2] and it's citations for your own investigation of this process.</p>

<p>I know how to do multiple regression via pGLS, but the R package [1] only mentions predictors and response. Furthermore, another source [2] discusses how multivariate regression though pGLS in R can be done, but requires one to transform the data under a Brownian motion model. (Edit: It seems that [2] is a solution...so I'm looking the process).</p>

<p>Sources:</p>

<ol>
<li><p>The vignette for the pGLS package (<a href=""http://cran.r-project.org/web/packages/pGLS/pGLS.pdf"" rel=""nofollow"">pdf</a>)</p></li>
<li><p>D.C. Adams. 2014. A Method for Assessing Phylogenetic Least Squares Models for Shape and Other High-Dimensional Multivariate Data. Evolution. 68:9 2675-2688. doi: 10.1111/evo.12463</p></li>
<li><p>Revell, L. J. (2010), Phylogenetic signal and linear regression on species data. Methods in Ecology and Evolution, 1: 319â€“329. doi: 10.1111/j.2041-210X.2010.00044.x</p></li>
</ol>
"
"0.123436492831862","0.114619460088286","120892","<p>I have on question regarding standardized coefficients (beta) in linear models. I have already asked one question <a href=""http://stats.stackexchange.com/questions/120201/magnitude-of-standardized-coefficients-beta-in-multiple-linear-regression"">here</a>. From the answers I assume that I should use R's <code>scale()</code> function on the dependent variable as well as on all independent variables (IV), to estimate the standardized coefficients for the model. But when I used the <code>scale()</code> function on an IV, which belongs to the factor class I get following error message:</p>

<p><code>Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric</code></p>

<p>To illustrate my problem here is a MWE:</p>

<p>First the linear model with unstandardized coefficients:</p>

<pre><code>&gt; data(ChickWeight)
&gt; aa &lt;- lm(weight ~ Time + Diet, data=ChickWeight)
&gt; summary(aa)

Call: 
lm(formula = weight ~ Time + Diet, data = ChickWeight)

Residuals:
     Min       1Q   Median       3Q      Max 
-136.851  -17.151   -2.595   15.033  141.816 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  10.9244     3.3607   3.251  0.00122 ** 
Time          8.7505     0.2218  39.451  &lt; 2e-16 ***
Diet2        16.1661     4.0858   3.957 8.56e-05 ***
Diet3        36.4994     4.0858   8.933  &lt; 2e-16 ***
Diet4        30.2335     4.1075   7.361 6.39e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 35.99 on 573 degrees of freedom
Multiple R-squared:  0.7453,    Adjusted R-squared:  0.7435 
F-statistic: 419.2 on 4 and 573 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Now I want to estimate the standardized coefficients using the <code>scale</code> function, which results in following error message:</p>

<pre><code>&gt; bb &lt;- lm(scale(weight) ~ scale(Time) + scale(Diet), data=ChickWeight)
Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric
</code></pre>

<p>As I figured out by myself the error message appears, because <code>Diet</code> belongs to the factor class and is not a numeric variable as required from the <code>scale()</code> function. I tried the following alternatively by including the <code>Diet</code> variable without <code>scale()</code>:</p>

<pre><code>&gt; cc &lt;- lm(scale(weight) ~ scale(Time) + Diet, data=ChickWeight)
&gt; summary(cc)

Call:
lm(formula = scale(weight) ~ scale(Time) + Diet, data = ChickWeight)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.92552 -0.24132 -0.03652  0.21151  1.99538 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.24069    0.03415  -7.048 5.25e-12 ***
scale(Time)  0.83210    0.02109  39.451  &lt; 2e-16 ***
Diet2        0.22746    0.05749   3.957 8.56e-05 ***
Diet3        0.51356    0.05749   8.933  &lt; 2e-16 ***
Diet4        0.42539    0.05779   7.361 6.39e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.5064 on 573 degrees of freedom
Multiple R-squared:  0.7453,    Adjusted R-squared:  0.7435 
F-statistic: 419.2 on 4 and 573 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>My question now is, if this is the right way to estimate the standardized coefficients for a model with both numeric and factor variables?</p>

<p>Thank you very much in advance for an answer.</p>

<p>Regards,</p>

<p>Magnus</p>
"
"0.0693653206906364","0.0679889413649005","121037","<p>I have a dataset with approximately <strong>4000 rows and 150 columns</strong>. I want to predict the values of a single column (= target).</p>

<p>The data is on cities (demography, social, economic, ... indicators). A lot of these are highly correlated, so I want to do a PCA - Principal Component Analysis. </p>

<p>The problem is, that <strong>~40% of the values are missing</strong>.</p>

<p>My current approach is:
Remove target indicator and do <strong>PCA with mean/median imputation of missing values</strong>.
Select x principal components (PC).
Append target indicator to these PC.
Use PC as predictors for the target variable and try common regression techniques, e.g. knn, linear regression, random forest etc.</p>

<p>With this approach, I'm getting quite good results. My metric is RMSE% - root mean squared relative prediction error. I tried this for all columns in the dataset, the RMSE% is between 0.5% and 8% (depending on the column). These errors are for values I actually know, NOT imputed values.</p>

<p>So, here's my problem: <strong>I'm not sure how much my data is distorted by replacing the missing values with the column mean/median</strong>. Is there any other way of imputing the missing values with minimal effect on the PCA results?</p>
"
"0.0400480865731637","0.039253433598943","121131","<p>I'm new to R and I've been searching for a while for a function which can reduce the number of explanatory variables in my lda function (linear discriminant analysis).</p>

<p>Basically, I've loaded the dataset and ran the lda function on my binomial dependent variable explained by 30 independent variables. (received a warning that the independent variables are collinear).</p>

<p>My professor has shown us stepwise feature selection (leaps package, regsubsets function) in a regression framework, but these codes aren't compatible for LDA/QDA.</p>

<p>Thanks in advance,
Marvin</p>
"
"0.102102987459307","0.0923787802599364","121192","<p>I have some data I need to fit a model to that can be used for prediction (interpolation). The data is summarized by the plot below. The black line is x=y.</p>

<p><img src=""http://i.stack.imgur.com/x0FqM.png"" alt=""enter image description here""></p>

<p>I want to be able to fit a model so as I can use it to predict any value of the y axis as a function of x axis, as well as get the uncertainty in that estimate.</p>

<p>However in my data, the variance of the y axis variable increases as the x axis variable increases.
In addition, there is another continuous explanatory variable called SequenceSize (plotted as factor to clearly see the colours) which I think I have to take into account, as it is also correlated (negatively) with the variance of the y axis variable, whilst not really affecting the mean so much. As can be seen in the two plots below.</p>

<p><img src=""http://i.stack.imgur.com/7KdHZ.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/d43W1.png"" alt=""enter image description here""></p>

<p>So from a model fit to the data I would like to be able to use it to do.</p>

<ol>
<li>Plugin the value of SequenceSize and the x axis variable.</li>
<li>Get out an estimate of the y axis variable along with some measure of uncertainty in the y estimate, given how the variance and uncertainty is affected by the x axis variable and by SequenceSize.</li>
</ol>

<p>However I'm reading the massive R book by Crawley, and I'm having trouble deciding which model would be best to do this. I'm thinking maybe a multiple regression if I linearized the data by taking log of x and y, but I'm unsure if that's right because of how the variance of the data acts.  </p>

<p>Thanks,
Ben W.</p>
"
"0.138730641381273","0.130312137616059","121255","<p>I am working on a dataset with a continuous response (which could be dichotomized), one continuous covariate, and multiple categorical variables. The continuous covariate (weight) is directly correlated to the response, and must be accounted for so that we can determine which of the categorical variables are most influential to the response. Here is <a href=""http://pastebin.com/891mheRf"" rel=""nofollow"">example data</a>.</p>

<p>Each row is an individual subject, with the continuous response, the covariate of underlying primary importance (weight), then 10 categorical variables that are to be tested (individuals can score yes = 1 to multiple categories). </p>

<p>My first thought in working with this data was a linear model, with stepwise elimination of categorical variables.</p>

<pre><code> lm(Response~Weight+var1+var2...+var11)
</code></pre>

<p>However, I believe there is extensive collinearity, since some variables may be eliminated early, but then are significant if you add them back into the model at the end. I'm curious if there is a better way to approach this data in R, that may help sort through which of the variables are of most importance to influencing the response. My two thoughts are</p>

<p>1) Building a single model with the continuous covariate and 5 categorical variables that were selected to be of most interest before the study, and refrain from any stepwise reduction of this model</p>

<p>2) Some sort of princicpal component regression, which I know little about at this point and thus wanted to ask advice before proceeding down that path</p>

<p>To help visualize the data, and the effect of Weight on the Response, I've constructed the follow plots. In the second plot, I attempt to control for the natural Response~Weight relationship.</p>

<pre><code> #GRAPH
 library(ggplot2)
 library(reshape2)

 Data &lt;- read.table(""Fake Data.txt"",header=TRUE)
 #Creating long format for ggplot2
 Data2&lt;-melt(Data, id.vars = c(""Subject"",""Response"",""Weight""), measure.vars = c(""var1"",""var2"",""var3"",""var4"",""var5"",""var6"",""var7"",""var8"",""var10"",""var11""))

 #Adding in weight to the varibles to be plotted
 Data2&lt;-rbind(Data2,Data2[1:31,])
 levels(Data2$variable)&lt;-c(levels(Data2$variable),""Weight"")
 Data2[311:341,4]&lt;-""Weight""
 Data2[311:341,5]&lt;-1

 #Removing rows where the categorical variable is 0=No
 for(i in 1:length(Data2[,1])){
 if(Data2[i,5]==0)Data2[i,]&lt;-NA
 }
 Data3&lt;-na.omit(Data2)

 #Plotting Response vs Weight for each 'Yes' group for the categorical variables
  scatter &lt;- ggplot(Data3, aes(Weight, Response, colour = variable))
 scatter + geom_point(aes(color = variable), size = 3) + geom_smooth(method = ""lm"",aes(fill = variable), alpha = 0.1) + facet_wrap(~variable)+ guides(fill=FALSE,color=FALSE) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/AgQog.jpg"" alt=""enter image description here""></p>

<pre><code> #Zeroing the Response~Weight relationship to remove its influence. Correction coefficients from linear model fit to Response~Weight
 Data4&lt;-Data3
 Data4$Response&lt;-Data4$Response-(0.01494*(Data4$Weight)+ 84.67715)

 #Plotting Response vs Weight for each 'Yes' group for the categorical variables for zeroed Response~Weight relationship (as seen in bottom right facet)
 scatter2 &lt;- ggplot(Data4, aes(Weight, Response, colour = variable))
 scatter2 + geom_point(aes(color = variable), size = 3) + geom_smooth(method = ""lm"",aes(fill = variable), alpha = 0.1) + facet_wrap(~variable)+ guides(fill=FALSE,color=FALSE) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/JfCrn.jpg"" alt=""enter image description here""></p>

<p>This second plot helps to show how, when the Response~Weight relationship is controlled for, variables like 'var10' have no influence on the response, while variables like 'var11' have all individuals below that zero-centered mean. Thus, from a visual test, I could identify var11 as a categorical variable of interest that negatively influences our response.</p>

<p>Additionally, this plot shows some of the confounding in this dataset, as you can see certain categorical variables 'clump'/are only documented in certain weight ranges. This is due to the underlying biology.</p>

<p>As a final note, I wonder if it is appropriate to use the corrected response in the second plot as the 'Response' for a linear model, thus eliminating the need for a 'Weight' covariate, or if it is incorrect to use such a transformation</p>

<p>Any thoughts are much appreciated</p>
"
"0.0424774103841449","0.0416345536240027","121315","<p>I am very new to statistical analysis and R. Recently I worked on a simple linear regression model to predict values. For example: consider the below data set</p>

<pre><code>Col A    Col B
1         10
2         16
3         67
4         ?
5         ?
</code></pre>

<p>For such data i was able to predict the values using <code>lm</code> and <code>predict</code> functions in R. </p>

<p>Now, suppose I am given the below data set:</p>

<pre><code>inventory  jan-sales   feb-sales   mar-sales   apr-sales  may-sales
12         4           0           2           ?          ?
190        54          67          89          ?          ? 
123        67          22          11          ?          ? 
654       167          100        300          ?          ?
789       567          10         80           ?          ? 
543       223          221        0            ?          ?
</code></pre>

<p>In this data set, each line has total available units of a particular item and how many of those items were sold on each month. Now based on this data if I am asked to predict how many will be sold in the months of April and May, how do I use R and Linear regression or multiple linear regression to predict sales for April and May?</p>
"
"0.0892864724045096","0.094246716538858","122212","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 0-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>Using these models, given the dichotomous dependent variable, I have built a logistic regression using lrm.</p>

<p>The method of model variable selection was based on existing clinical literature modelling the same diagnosis. All have been modelled with a linear fit with the exception of ISS which has been modelled traditionally through fractional polynomials. No publication has identified known significant interactions between the above variables.</p>

<p>Following advice from Frank Harrell, I have proceeded with the use of regression splines to model ISS (there are advantages to this approach highlighted in the comments below). The model was thus pre-specified as follows:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ Age + GCS + rcs(ISS) +
    Year + inctoCran + oth, data = ASDH_Paper1.1, x=TRUE, y=TRUE)
</code></pre>

<p>Results of the model were:</p>

<pre><code>&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Age + GCS + rcs(ISS) + Year + inctoCran + 
    oth, data = ASDH_Paper1.1, x = TRUE, y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          2135    LR chi2     342.48    R2       0.211    C       0.743    
 0            629    d.f.             8    g        1.195    Dxy     0.486    
 1           1506    Pr(&gt; chi2) &lt;0.0001    gr       3.303    gamma   0.487    
max |deriv| 5e-05                          gp       0.202    tau-a   0.202    
                                           Brier    0.176                     

          Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept -62.1040 18.8611 -3.29  0.0010  
Age        -0.0266  0.0030 -8.83  &lt;0.0001 
GCS         0.1423  0.0135 10.56  &lt;0.0001 
ISS        -0.2125  0.0393 -5.40  &lt;0.0001 
ISS'        0.3706  0.1948  1.90  0.0572  
ISS''      -0.9544  0.7409 -1.29  0.1976  
Year        0.0339  0.0094  3.60  0.0003  
inctoCran   0.0003  0.0001  2.78  0.0054  
oth=1       0.3577  0.2009  1.78  0.0750  
</code></pre>

<p>I then used the calibrate function in the rms package in order to assess accuracy of the predictions from the model. The following results were obtained:</p>

<pre><code>plot(calibrate(rcs.ASDH, B=1000), main=""rcs.ASDH"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/HYTsp.png"" alt=""Bootstrap calibration curves penalized for overfitting""></p>

<p>Following completion of the model design, I created the following graph to demonstrate the effect of the Year of incident on survival, basing values of the median in continuous variables and the mode in categorical variables:</p>

<pre><code>ASDH &lt;- Predict(rcs.ASDH, Year=seq(1994,2013,by=1),Age=48.7,ISS=25,inctoCran=356,Other=0,GCS=8,Sex=""Male"",neuroYN=1,neuroFirst=1)
Probabilities &lt;- data.frame(cbind(ASDH$yhat,exp(ASDH$yhat)/(1+exp(ASDH$yhat)),exp(ASDH$lower)/(1+exp(ASDH$lower)),exp(ASDH$upper)/(1+exp(ASDH$upper))))
names(Probabilities) &lt;- c(""yhat"",""p.yhat"",""p.lower"",""p.upper"")
ASDH&lt;-merge(ASDH,Probabilities,by=""yhat"")
plot(ASDH$Year,ASDH$p.yhat,xlab=""Year"",ylab=""Probability of Survival"",main=""30 Day Outcome Following Craniotomy for Acute SDH by Year"", ylim=range(c(ASDH$p.lower,ASDH$p.upper)),pch=19)
arrows(ASDH$Year,ASDH$p.lower,ASDH$Year,ASDH$p.upper,length=0.05,angle=90,code=3)
</code></pre>

<p>The code above resulted in the following output:</p>

<p><img src=""http://i.stack.imgur.com/KGYcz.png"" alt=""Year trend with lower and upper""></p>

<p><strong><em>My remaining questions are the following:</em></strong></p>

<p><strong>1. Spline Interpretation</strong> - How can I calculate the p-value for the splines combined for the overall variable?</p>
"
"0.113469578623964","0.111218061863672","122336","<p>I have a problem with coding of a 2-level categorical predictor variable in R, and subsequently using it as a random slope in lmer().</p>

<p>I can keep the factor as numeric, coded using the treatment coding:</p>

<pre><code>&gt; unique (b$multi)
[1] 0 1
</code></pre>

<p>Running lmer() using a dataset coded in this way yields:</p>

<pre><code>&gt; l1 = glmer(OK ~ multi + (0 + multi|item) + (1|subject)+ (1|item), family=""binomial"", data=b)
&gt; summary(l1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: OK ~ multi + (0 + multi | item) + (1 | subject) + (1 | item)
   Data: b

     AIC      BIC   logLik deviance df.resid 
  4806.5   4838.9  -2398.3   4796.5     4792 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-7.8294 -0.5560 -0.1548  0.5623 14.3342 

Random effects:
 Groups  Name        Variance Std.Dev.
 subject (Intercept) 1.84379  1.3579  
 item    (Intercept) 2.40306  1.5502  
 item.1  multi       0.04145  0.2036  
Number of obs: 4797, groups:  subject, 123; item, 39
[...]
</code></pre>

<p>Above there is only one random slope related to <code>multi</code>. However, something very different happens when I convert the variable into a factor:</p>

<pre><code>&gt; b$multi = as.factor(b$multi)
&gt; levels (b$multi)
[1] ""0"" ""1""
</code></pre>

<p>When I fit a model using <code>multi</code> as a random slope variable:</p>

<blockquote>
  <p>l2 = glmer(OK ~ multi + (0+multi|item) + (1|subject)+ (1|item), family=""binomial"", data=b)
      Warning message:
      In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
        Model failed to converge: degenerate  Hessian with 1 negative eigenvalues</p>
</blockquote>

<p>... the model fails to converge and I get a very different random effects structure:</p>

<pre><code>&gt; summary(l2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: OK ~ multi + (0 + multi | item) + (1 | subject) + (1 | item)
   Data: b

     AIC      BIC   logLik deviance df.resid 
  4807.8   4853.1  -2396.9   4793.8     4790 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-8.3636 -0.5608 -0.1540  0.5627 15.2515 

Random effects:
 Groups  Name        Variance Std.Dev. Corr
 subject (Intercept) 1.8375   1.3555       
 item    (Intercept) 0.9659   0.9828       
 item.1  multi0      1.5973   1.2638       
         multi1      1.0224   1.0111   1.00
Number of obs: 4797, groups:  subject, 123; item, 39
[...]
</code></pre>

<p>The number of parameters in the model clearly change (reflected by the change in AIC, etc.), and I get two random slopes. </p>

<p>My question is which way of coding the categorical variable is better? Intuition tells me that it is the first one, but I have seen recommendations for both ways of coding in various tutorials and classes about running GLMMs in R and this is why it baffles me. Both types of the predictor variable work identically in ordinary regression using lm().</p>
"
"0.0200240432865818","0.039253433598943","122387","<p>I am using the segmented package to run a piecewise linear regression.  My regression model is called fmod, and I use: </p>

<pre><code>fmod$psi[1]
</code></pre>

<p>to extract the first breakpoint from the summary. To view the slopes of each piece of the regression, I use:</p>

<pre><code>slope(fmod)
</code></pre>

<p>and to view the intercepts of each piece, I use:</p>

<pre><code>intercept(fmod)
</code></pre>

<p>However, I am unable to extract a single slope estimate or intercept estimate from the output. Does anyone know how to do this?</p>
"
"NaN","NaN","122640","<p>I have the following table:</p>

<pre><code>d &lt;- read.table(textConnection(""y x1 x2 x3 x4
                  40 5 10 8 2 
                  60 9 19 9 9 
                  75 18 27 19 5 
                  80 15 36 25 20 
                  115 25 45 39 30 
                  120 35 48 40 19""), header=TRUE)
</code></pre>

<p>I did linear regression analysis:</p>

<pre><code>summary(lm(y~., data=d))

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 30.50427    7.24960   4.208    0.149
x1           2.46856    1.13807   2.169    0.275
x2          -0.04313    0.91344  -0.047    0.970
x3          -0.45983    1.01308  -0.454    0.729
x4           1.35522    0.82608   1.641    0.348

Residual standard error: 4.855 on 1 degrees of freedom
Multiple R-squared:  0.9951,    Adjusted R-squared:  0.9756 
F-statistic: 51.02 on 4 and 1 DF,  p-value: 0.1046
</code></pre>

<p>How can I figure out if I need to do nonlinear-regression analysis or not?</p>
"
"0.0693653206906364","0.0679889413649005","122704","<p>I have used auto.arima to fit a time series model (a linear regression with ARIMA errors, as described <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">on Rob Hyndman's site</a> )  When finished - the output reports that the best model has a (5,1,0) with drift structure - and reports back values of information criteria as </p>

<p>AIC:  2989.2
AICC:  2989.3
BIC: 3261.2</p>

<p>When I use Arima to fit a model with a (1,1,1) with drift structure - the output reports back noticeably lower IC's of</p>

<p>AIC:  2510.3
AICC:  2510.4
BIC:  2759</p>

<p>I can force auto.arima to consider the (1,1,1) with drift model (using the start.p and start.q parameters), and when I do that, and set ""trace=TRUE"" - I do see that the (1,1,1) with drift model is considered, but rejected, by auto.arima.  It still reports back the (5,1,0) with drift model as the best result.</p>

<p>Are there circumstances when auto.arima uses other criteria to choose between models?</p>

<p>Edited to add (in response to request)</p>

<p>Data for this example can be found at <a href=""https://drive.google.com/file/d/0B6afOuS0y79aenBMeFYyWVNwUUU/view?usp=sharing"" rel=""nofollow"">this Google spreadsheet</a></p>

<p>and R code to reproduce the example is</p>

<pre><code>repro = read.csv(""mindata.csv"")
reprots = ts(repro, start=1, frequency=24)
fitauto = auto.arima(reprots[,""lnwocone""],
xreg=cbind(fourier(reprots[,""lnwocone""], K=11),
reprots[,c(""temp"",""sqt"",""humidity"",""windspeed"",""mist"",""rain"")]),
start.p=1, start.q=1, trace=TRUE, seasonal=FALSE)
fitdirect &lt;- Arima(reprots[,""lnwocone""], order=c(1,1,1), seasonal=c(0,0,0),
xreg=cbind(fourier(reprots[,""lnwocone""], K=11),
reprots[,c(""temp"",""sqt"",""humidity"",""windspeed"",""mist"",""rain"")]), include.drift=TRUE)
summary(fitauto)
summary(fitdirect)
</code></pre>

<p>Apologies if the Google docs data - inline code is not the best way to provide the example.  I think I have seen in the past guidelines on the best way to do this - but could not locate those guidelines in searching this morning.</p>
"
"0.0490486886395286","0.0480754414848157","122875","<p>I am doing multiple linear regression analysis in R and I got the following summary:</p>

<pre><code>Call:
lm(formula = Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + 
    X10 + X11 + X12 + X13)

Residuals:
ALL 20 residuals are 0: no residual degrees of freedom!

Coefficients: (151 not defined because of singularities)
                   Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)       -15462.94         NA      NA       NA
X1                    63.31         NA      NA       NA
X2                  1363.12         NA      NA       NA
X31,266,019,376     5518.54         NA      NA       NA
X31,483,786,035    29894.78         NA      NA       NA
X31,619,000,000    39338.01         NA      NA       NA
X31,687,000,000    65308.07         NA      NA       NA
X31,720,264,324    35548.79         NA      NA       NA
X31,749,000,000    31693.75         NA      NA       NA

.......................................................

X13692,062,808           NA         NA      NA       NA
X13693,179,733           NA         NA      NA       NA
X13724,817,439           NA         NA      NA       NA

Residual standard error: NaN on 0 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:    NaN 
F-statistic:   NaN on 19 and 0 DF,  p-value: NA
</code></pre>

<p>Could anybody explain what does that result mean? And what should I do?</p>

<p>Thank you!</p>

<p>Another question.  What should I do if the following error appears: </p>

<pre><code>Error in step(model) : 
number of rows in use has changed: remove missing values?
</code></pre>
"
"0.0490486886395286","0.0480754414848157","122929","<p>When a regression is calculated with a simple linear model that returns intercept and slope for an equation like this y=<em>a</em> + <em>b</em>x one can predict y (the response variable) based on that equation. Equally one could rearrange for x; x={y-<em>a</em>}/<em>b</em> and calculate the value of x. This isn't available in R's <code>predict()</code> function but can easily be done. Can one do this calculation and still be statistically sound?</p>
"
"0.0863948355424908","0.0923787802599364","123012","<p>I'm using <code>glmboost</code> in the mboost package to fit a boosted regression using linear models as the base learner. There are 13200 observations and about 75 variables, and I want to get a measure of the importance of each variable.</p>

<p>At the moment, I'm exploring the following two options:</p>

<ol>
<li><p>The boosted model object returned by <code>glmboost</code> includes information on the selection probabilities of the variables, ie how frequently they are selected by the boosting algorithm.</p></li>
<li><p>I can use <code>stabsel</code>, from the package of the same name, to identify the important variables. This uses a resampling approach to perturb the data, and the output is the frequency with which each variable appears in the resampled models (if I understand correctly).</p></li>
</ol>

<p>The problem is that these two methods are giving radically different results. This is the output from 1:</p>

<pre><code>&gt; summary(php.glmb)

         Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = p_hp ~ ., data = hdata.php.trn)

....

Selection frequencies:
degc238   etc48    etc7   per45   bar25   bar43    etc8   etc60   bar33   bar59
   0.28    0.17    0.07    0.06    0.05    0.05    0.05    0.05    0.04    0.03
  per15   per60   degc1   etc65   bar67 degc209    per5    per23   etc70 
   0.03    0.03    0.02    0.02    0.01    0.01    0.01     0.01    0.01 
</code></pre>

<p>And this is the output from 2:</p>

<pre><code>&gt; stabsel(php.glmb, cutoff=0.75, q=10)
        Stability Selection with unimodality assumption

Selected base-learners:
degc1  per5 per15 per45 per60  etc8 etc60 etc65 etc70 
   20    43    44    52    54    60    72    74    76 

Selection probabilities:
(Intercept)       bar25       bar26       bar28       bar29       bar32       bar42 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      bar43       bar45       bar46       bar49       bar50       bar59       bar60 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      bar62       bar63       bar66       degc2       degc3      degc16     degc109 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
    degc111     degc147     degc154     degc155     degc158     degc181     degc183 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
    degc204     degc205     degc206     degc209     degc229     degc231     degc238 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
    degc255     degc256     degc257     degc260       per21       per24       per34 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      per40       per42       per43       per58       per61       per63        etc5 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
       etc6        etc7       etc11       etc15       etc26       etc30       etc32 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      etc33       etc41       etc45       etc47       etc48       etc59       etc64 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      etc69       bar67       bar33       per23       per60       degc1        per5 
       0.00        0.15        0.18        0.71        0.96        1.00        1.00 
      per15       per45        etc8       etc60       etc65       etc70 
       1.00        1.00        1.00        1.00        1.00        1.00 
</code></pre>

<p>So the important variables are almost completely disjoint: method 1 says <code>degc238</code>, <code>etc48</code>, <code>etc7</code> and <code>per45</code> are the most important (have the highest selection probabilities), while method 2 says <code>etc70</code>, <code>etc65</code>, <code>etc60</code>, <code>etc8</code>, <code>per60</code> and so on. </p>

<p>What can be the reason for this? I should also mention that there's a significant amount of collinearity in this dataset; several predictors have univariate correlations of 90%+ with the response and with each other. Could this be impacting the result?</p>
"
"0.0400480865731637","0.039253433598943","123059","<p>As the title says, does a linear regression model make assumptions about distributions of depended and independent variables and what should these distributions be for the model to work as expected ?  I have a log normal distribution (or rather a very positively skewed one) for both  dependent and independent variables and I always log transformed them before doing any fitting but came across an article saying that is not necessary. Also just to add to this, I would like to use the model to predict values of individual data points. </p>
"
"0.0490486886395286","0.0480754414848157","123172","<p>I am making a table from results of an analysis using generalised linear model which involves detecting association of a categorical predictor variable over multiple outcome variables. Of those multiple outcome variables, few are binary where I display the odds ratio for each category of the predictor (as we do in logistic regression); while few are continuous outcome variables, in which case I can display the beta estimate for each category of the predictor. My question is will it be ok if exponentiate the beta value  and express it as odds ratios. Can I do that?</p>
"
"NaN","NaN","123201","<p>I am working on building a regression model. There are 51 points. The number of predictor variables is 37.  The following is the result of running lm result. When trying to detecting the multicollinearity issue, the <code>vif</code> also drops the error message. What are the problems of this model.</p>

<pre><code>model1&lt;-lm(test.1[,3] ~ as.matrix(test[,-c(1,2,3)]),data=test)
summary(model1)
vif(model1)
Error in vif.default(model1) : model contains fewer than 2 terms
</code></pre>

<p><img src=""http://i.stack.imgur.com/5m9o3.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/9UtYU.png"" alt=""enter image description here""></p>
"
"0.0200240432865818","0.039253433598943","123272","<p>I am aware of how the M5 regression model trees work. I know that they fit linear regression models at every leaf of the regression tree and that every parent in the node is also associated with a regression model. Furthermore, I even understand that in the M5 regression model, it is possible to perform smoothing by considering the models in the nodes above the leaf.</p>

<p>Now, what is the difference between such a regression model such as M5 as described above and the cubist model? Does cubist do anything different from the above explanation?</p>

<p>I have used the methods in R</p>
"
"0.0400480865731637","0.039253433598943","123568","<p>I need to create a Multiple Linear regression model on those data explaining max03 T9 T12 T15 Ne9 Ne12 Ne15 Vx9 Vx12 Vx15 maxO3v</p>

<p>!My data <a href=""http://i.stack.imgur.com/mGU6I.png"" rel=""nofollow"">1</a></p>

<p>My first intuition was to make a backward selection : </p>

<pre><code>attach(ozone)
res &lt;- lm(maxO3~T9+T12+T15+Ne9+Ne12+Ne15+Vx9+Vx12+Vx15+maxO3v)
shapiro.test(res$residuals)
</code></pre>

<p>data:  res$residuals</p>

<p>W = 0.9682, p-value = 0.008945</p>

<p>But the first full model return non-normal residuals.</p>

<p>Is it okay to continue doing a backward selection (AIC criterion)?</p>

<p>I don't think it [non-normal residuals] has an impact on that sort of selection, but I can't find a definite answer to that question.</p>

<p>If I keep doing the backward selection process</p>

<pre><code>[...]
res &lt;- lm(maxO3~T12+Ne9+Vx15+maxO3v)
drop1(res)

summary(res)
shapiro.test(res$residuals)
</code></pre>

<p>Shapiro-Wilk normality test</p>

<p>data:  res$residuals
W = 0.9622, p-value = 0.002946</p>

<p>My residuals aren't normal at the end ...</p>
"
"0.0700841515030364","0.078506867197886","124214","<p>I build a prediction modeling using both regression and random forest.</p>

<pre><code>testmodel2&lt;-lm(y~as.matrix(xtest))
summary(testmodel2)

rf2&lt;-randomForest(y~.,data=df,importance=TRUE)
varImpPlot(rf2)
</code></pre>

<p>The regression model result shows that <code>t1, t10 and t11</code> are not significant. However, the <code>varImpPlot</code> show that they are pretty important. On the other side, <code>t3,t5 and t6</code> are significant in terms of P-value in the regression result, but they are not important in the Random forest result. </p>

<p>Is there any reason that linear regression result is different with random forest? Which one should be more reliable? The correlation matrix is also attached for the reference. The result of <code>backward step-wise variable selection</code> is also attached.</p>

<p><img src=""http://i.stack.imgur.com/G0h3k.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/lRnDt.png"" alt=""enter image description here""> </p>

<p><img src=""http://i.stack.imgur.com/8BzXW.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/I3zGF.png"" alt=""enter image description here""></p>
"
"0.0899225958391357","0.0881383093888288","124233","<p>I am running following data and code for analyzing non-linear regression and to get simplest equation of curve that fits the data:</p>

<pre><code>&gt; dput(ddf)
structure(list(xx = 1:23, yy = c(10L, 9L, 11L, 9L, 7L, 6L, 9L, 
8L, 5L, 4L, 6L, 6L, 5L, 4L, 6L, 8L, 4L, 6L, 8L, 11L, 8L, 10L, 
9L)), .Names = c(""xx"", ""yy""), row.names = c(NA, -23L), class = ""data.frame"")
&gt; 
&gt; head(ddf)
  xx yy
1  1 10
2  2  9
3  3 11
4  4  9
5  5  7
6  6  6
</code></pre>

<p><img src=""http://i.stack.imgur.com/vkx9G.png"" alt=""enter image description here""></p>

<pre><code>&gt; fit = lm(yy ~ poly(xx, 9), data=ddf)
&gt; summary(fit)

Call:
lm(formula = yy ~ poly(xx, 9), data = ddf)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.9890 -1.2031  0.1086  0.7493  2.4248 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   7.347826   0.356758  20.596 2.62e-11
poly(xx, 9)1 -0.880172   1.710953  -0.514 0.615582
poly(xx, 9)2  7.821383   1.710953   4.571 0.000524  # NOTE THIS
poly(xx, 9)3  0.424579   1.710953   0.248 0.807892
poly(xx, 9)4 -2.151779   1.710953  -1.258 0.230641
poly(xx, 9)5 -0.876964   1.710953  -0.513 0.616857
poly(xx, 9)6 -0.961726   1.710953  -0.562 0.583610
poly(xx, 9)7 -0.002171   1.710953  -0.001 0.999007
poly(xx, 9)8 -0.051884   1.710953  -0.030 0.976269
poly(xx, 9)9  0.840177   1.710953   0.491 0.631571

Residual standard error: 1.711 on 13 degrees of freedom
Multiple R-squared:  0.6451,    Adjusted R-squared:  0.3993 
F-statistic: 2.625 on 9 and 13 DF,  p-value: 0.05575
</code></pre>

<p>If I use 'raw=TRUE' : </p>

<pre><code>&gt; fit = lm(yy ~ poly(xx, 9, raw=TRUE), data=ddf)
&gt; summary(fit)

Call:
lm(formula = yy ~ poly(xx, 9, raw = TRUE), data = ddf)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.9890 -1.2031  0.1086  0.7493  2.4248 

Coefficients:
                           Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)               2.844e+00  1.529e+01   0.186    0.855
poly(xx, 9, raw = TRUE)1  1.310e+01  2.711e+01   0.483    0.637
poly(xx, 9, raw = TRUE)2 -8.439e+00  1.723e+01  -0.490    0.632  # NOTE THIS
poly(xx, 9, raw = TRUE)3  2.637e+00  5.432e+00   0.485    0.635
poly(xx, 9, raw = TRUE)4 -4.719e-01  9.715e-01  -0.486    0.635
poly(xx, 9, raw = TRUE)5  5.112e-02  1.048e-01   0.488    0.634
poly(xx, 9, raw = TRUE)6 -3.400e-03  6.937e-03  -0.490    0.632
poly(xx, 9, raw = TRUE)7  1.355e-04  2.757e-04   0.492    0.631
poly(xx, 9, raw = TRUE)8 -2.967e-06  6.032e-06  -0.492    0.631
poly(xx, 9, raw = TRUE)9  2.739e-08  5.578e-08   0.491    0.632

Residual standard error: 1.711 on 13 degrees of freedom
Multiple R-squared:  0.6451,    Adjusted R-squared:  0.3993 
F-statistic: 2.625 on 9 and 13 DF,  p-value: 0.05575
</code></pre>

<p>I find that if I do not use 'raw=TRUE', one P value (2nd) is significant, but it is not significant if I use 'raw=TRUE'. Why does this occur and what does it mean?</p>

<p>I asked above question at stackoverflow but was advised to post here. Thanks for your help.</p>
"
"NaN","NaN","124373","<p>I'm using the earth package (using caret train function) MARS spline implementation in order to perform non - linear regression modeling. I would like to obtain a measure of prediction uncertainty (not only the expected value). Is there any way to obtain it? Thanks in advance for any help.</p>
"
"0.0642198081225601","0.0629455284778823","124500","<p>I would like to run by you an algorithm for predicting one of two values from a testing data set, based on a linear model applied to a training set. Please let me know whether this algorithm makes sense, whether it can be improved and whether there is already a well established algorithm to accomplish the same result, possibly already encapsulated in some R routine.</p>

<p>Given: Two dataframes, <code>training</code> and <code>testing</code>, comprising two columns: <code>Y</code> and <code>X</code>, in this order. The <code>Y</code> columns of both dataframes take values in the set {2,3}.</p>

<p>Assignment: Predict <code>testing$Y</code> from <code>testing$X</code> based on a linear model with coefficients obtained from the linear regression <code>training$Y ~ training$X</code>.</p>

<p>Suggested solution (R based pseudo-code):</p>

<ol>
<li><code>m &lt;- lm(Y ~ X, data = training)</code></li>
<li><code>p &lt;- predict(training, new_data = testing, interval = ""prediction"")</code></li>
<li>for every row of <code>p</code> do as follows:</li>
<li><code>if p$upr &lt;= 2 or (p$lwr &lt;= 2 &lt; p$upr) or p@fit &lt;= 2, then set p$fit &lt;- 2</code></li>
<li><code>else if p$lwr &gt;= 3 or (p$lwr &lt; 3 &lt;= p$upr) or p$fit &gt;= 3, then set pfit &lt;- 3</code></li>
<li><code>else set p$fit &lt;- round(p$fit)</code></li>
</ol>
"
"0.09392108820677","0.0836886016271203","124532","<p>I am estimating a (semi)parametric and a parametric model for a panel data set, and I want to test the functional form by applying the method proposed by <a href=""http://www.sciencedirect.com/science/article/pii/S030440760800016X"" rel=""nofollow"">Henderson et al. (2008, p.267)</a>. In particular, given the two models</p>

<pre><code>y = beta1 X + Z'gamma + u  (parametric)
y = beta2 X + g(z) + e    (semiparametric)
</code></pre>

<p>they use <code>H0</code> to denote the null hypothesis of the linear regression model, against <code>H1</code>: the corresponding alternative is the semiparametric model. The test statistic for testing <code>H0</code> is</p>

<pre><code>I= [beta1* X + Z'gamma* - beta2* X + g(z)]^2
</code></pre>

<p>Under <code>H0</code>. <code>I</code> converges to 0 in probability, whil it converges to a positive constant under <code>H1</code> . Therefore, the statistics <code>I</code> can be used to detect whether <code>H0</code> is true or not. However, given some problems with the asymptotical distribution of <code>I</code>, the authors propose to compute its empirical distribution by resampling <code>n</code> times the residuals <code>u</code>, using them to generate <code>n</code> new <code>y</code> and then re-estimating <code>n</code> times both models. By this way it is possible to obtain <code>n</code> times <code>I*</code> and its empiric distribution which should be approximating the null distribution of <code>I</code>. Therefore it can be used to detect whether <code>H0</code> holds.</p>

<p>My problem is in the interpretation of the results of the test. I reasoned as follows: I plotted the empirical distribution of <code>I*</code> which, in my case, has <code>mean &gt; 0</code>. Therefore what I did was to verify $where$  the <code>I</code> statistics lies. This is what I obtain:</p>

<p><img src=""http://i.stack.imgur.com/TihCM.png"" alt=""Empirical distribution and test""></p>

<p>where I.BB.IVO is my <code>I</code>. Can I say that <code>I</code> belongs to the empirical distribution (since it is not in the rejection zone) with mean <code>&gt; 0</code>, therefore the null hypothesis is rejected?</p>

<p>Is there any other alternative of doing the job?</p>
"
"0.0749231094763201","0.0734364498908627","124616","<p>I am testing the logistic regression classifier in R. I created some test data like this:</p>

<pre><code>x=runif(10000)
y=runif(10000)
df=data.frame(x,y,as.factor(x-y&gt;0))
</code></pre>

<p>basically I am sampling the 2D unit square [0,1] and classifying a point belonging to class A or B depending on which side of y=x it lies.</p>

<p>I generated a scatter plot of the data like below:</p>

<pre><code>names(df) = c(""feature1"", ""feature2"", ""class"")
levels=levels(df[[3]])
obs1=as.matrix(subset(df,class==levels[[1]])[,1:2])
obs2=as.matrix(subset(df,class==levels[[2]])[,1:2])
# make scatter plot
dev.new()
plot(obs1[,1],obs1[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=0,col=colors[[1]])
points(obs2[,1],obs2[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=1,col=colors[[2]])
</code></pre>

<p>it gives me below graph:</p>

<p><img src=""http://i.stack.imgur.com/5zN4y.png"" alt=""scatter plot""></p>

<p>Now I tried running LR (logistic regression) on this data using code below:</p>

<pre><code>model=glm(class~.,family=""binomial"",data=df)
summary(model) # prints summary
</code></pre>

<p>here are the results:</p>

<pre><code>Call:
glm(formula = class ~ ., family = ""binomial"", data = df)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.11832   0.00000   0.00000   0.00000   0.08847  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  5.765e-01  1.923e+01   0.030    0.976
feature1     9.761e+04  8.981e+04   1.087    0.277
feature2    -9.761e+04  8.981e+04  -1.087    0.277

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.3863e+04  on 9999  degrees of freedom
Residual deviance: 2.9418e-02  on 9997  degrees of freedom
AIC: 6.0294

Number of Fisher Scoring iterations: 25
</code></pre>

<p>I also get these warning messages:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>If I try plotting the ROC curve using a varying threshold, I get following graph (AUC=1 which is good):
<img src=""http://i.stack.imgur.com/xbyPX.png"" alt=""enter image description here""></p>

<p><strong>Could someone please explain why the algorithm does not converge and coefficient estimates are not statistically significant (high std. error in coeff estimates)?</strong></p>

<p>I also compared to LDA:</p>

<pre><code>lda_classifier=lda(class~., data=df)
</code></pre>

<p>gives:</p>

<pre><code>Call:
lda(class ~ ., data = df)

Prior probabilities of groups:
 FALSE   TRUE 
0.5007 0.4993 

Group means:
       feature1  feature2
FALSE 0.3346288 0.6676169
TRUE  0.6710111 0.3380432

Coefficients of linear discriminants:
               LD1
**feature1  4.280490
feature2 -4.196388**
</code></pre>
"
"0.0800961731463273","0.078506867197886","124700","<p>I am trying to create a linear regression model containing two predictors and 1 response variable. My response variable has a short term pattern, i.e. surge during weekdays and slump during weekends and I suspect this pattern is a result of two things: 
1) A natural trend - people are more active on weekdays and 
2) Partially related to my independent variables which follows a similar pattern.</p>

<p>There is also lagged cross-correlation between predictor and response.</p>

<p>Should I take some steps to normalize the data before running a linear regression? I've been reading about detrending time series, ARIMA, moving averages etc. but am a little lost on the right approach. Attached below are are time series plots of the predictor and response and the lagged cross correlation.</p>

<p><img src=""http://i.stack.imgur.com/NNkB1.jpg"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/cxYFk.jpg"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/zBzmC.jpg"" alt=""enter image description here""></p>
"
"0.120749524304413","0.118353555437113","125414","<p>I have two variables:</p>

<ul>
<li>urban areas</li>
<li>protected areas.</li>
</ul>

<p>My observations are urban areas and protected areas in each year. But these observations are the cumulative ones, so observations in each variable have auto-correlation.</p>

<p>Can I use the general correlation such as yielded by <code>cor()</code> in R to measure the correlation between these two variables? If not, which indicator or method can I use?</p>

<p>I have the scatter plot: the horizontal variable is urban area in a specific year, and the vertical variable is another one in that specific year. And these two variables are increasing as years pass. I can see these two variables present a linear relationship. And my purpose is to find a indicator which can measure this linear relationship. I actually have tested the linear regression: the urban area as independent variable, the protected area as dependent variable, and I put 14 pairs of each year into the regression model, and the coefficients can pass the t-test, and model can pass the t-test, the $R^2$ can reach more than 0.9. </p>

<p>I want to research the relationship between urban development and protected area development. And the scatter plot below is urban and protected area pairs on global scale for 1950-2014 with 5 year intervals (except for 2010 and 2014).</p>

<p>I want to test two questions: First, are these two areas (urban and protected areas) both increasing over the research period? Second, does urbanization (here I mean the development of urban area) cause the development of protected areas?</p>

<p>I want to use some correlation analysis to solve the first question, such as correlation, linear regression or MIC value. However, because my data are time series, I'm not sure it can be used in the calculation of correlation? So I raise this question. In addition, I don't know other methods that could be used to measure strength of linear relationship between two time series. </p>

<p>And for the second question, I want to use Granger causality test to test the causality relationship between these two areas statistically. I know the result of Granger causality can't be sure to determine the causality relationship. And in my opinion, the reasons to improve the development of urban areas or protected areas are both complex, and some of them may be shared. At this level, I simply want to test the causality relationship between these two variables.</p>

<p><img src=""http://i.stack.imgur.com/tHlOm.jpg"" alt=""scatter plot between urban and farm land, the point is a variable pair in a specific year""></p>
"
"0.14449456827576","0.146872899781725","125453","<p>I have used the â€˜polrâ€™ function in the MASS package to run an ordinal logistic regression for an ordinal categorical response variable with 15 continuous explanatory variables.</p>

<p>I have used the code (shown below) to check that my model meets the proportional odds assumption following advice provided in <a href=""http://www.ats.ucla.edu/stat/r/dae/ologit.htm"">UCLA's guide</a>. However, Iâ€™m a little worried about the output implying that not only are the coefficients across various cutpoints similar, but they are exactly the same (see graphic below). </p>

<pre><code>FGV1b &lt;- data.frame(FG1_val_cat=factor(FGV1b[,""FG1_val_cat""]), 
                    scale(FGV1[,c(""X"",""Y"",""Slope"",""Ele"",""Aspect"",""Prox_to_for_FG"", 
                          ""Prox_to_for_mL"", ""Prox_to_nat_border"", ""Prox_to_village"", 
                          ""Prox_to_roads"", ""Prox_to_rivers"", ""Prox_to_waterFG"", 
                          ""Prox_to_watermL"", ""Prox_to_core"", ""Prox_to_NR"", ""PCA1"", 
                          ""PCA2"", ""PCA3"")]))
b     &lt;- polr(FG1_val_cat ~ X + Y + Slope + Ele + Aspect + Prox_to_for_FG + 
                            Prox_to_for_mL + Prox_to_nat_border + Prox_to_village + 
                            Prox_to_roads + Prox_to_rivers + Prox_to_waterFG + 
                            Prox_to_watermL + Prox_to_core + Prox_to_NR, 
              data=FGV1b, Hess=TRUE)
</code></pre>

<p>View a summary of the model:</p>

<pre><code>summary(b)
(ctableb &lt;- coef(summary(b)))
q        &lt;- pnorm(abs(ctableb[, ""t value""]), lower.tail=FALSE) * 2
(ctableb &lt;- cbind(ctableb, ""p value""=q))
</code></pre>

<p>And now we can look at the confidence intervals for the parameter estimates:</p>

<pre><code>(cib &lt;- confint(b)) 
confint.default(b)
</code></pre>

<p>But these results are still quite hard to interpret, so let's convert the coefficients into odds ratios</p>

<pre><code>exp(cbind(OR=coef(b), cib))
</code></pre>

<p>Checking the assumption. So the following code will estimate the values to be graphed. First it shows us the logit transformations of the probabilities of being greater than or equal to each value of the target variable</p>

<pre><code>FG1_val_cat &lt;- as.numeric(FG1_val_cat)
sf &lt;- function(y) {
  c('VC&gt;=1' = qlogis(mean(FG1_val_cat &gt;= 1)),
    'VC&gt;=2' = qlogis(mean(FG1_val_cat &gt;= 2)),
    'VC&gt;=3' = qlogis(mean(FG1_val_cat &gt;= 3)),
    'VC&gt;=4' = qlogis(mean(FG1_val_cat &gt;= 4)),
    'VC&gt;=5' = qlogis(mean(FG1_val_cat &gt;= 5)),
    'VC&gt;=6' = qlogis(mean(FG1_val_cat &gt;= 6)),
    'VC&gt;=7' = qlogis(mean(FG1_val_cat &gt;= 7)),
    'VC&gt;=8' = qlogis(mean(FG1_val_cat &gt;= 8)))
}
(t &lt;- with(FGV1b, summary(as.numeric(FG1_val_cat) ~ X + Y + Slope + Ele + Aspect + 
                             Prox_to_for_FG + Prox_to_for_mL + Prox_to_nat_border + 
                             Prox_to_village + Prox_to_roads + Prox_to_rivers + 
                             Prox_to_waterFG + Prox_to_watermL + Prox_to_core + 
                             Prox_to_NR, fun=sf)))
</code></pre>

<p>The table above displays the (linear) predicted values we would get if we regressed our dependent variable on our predictor variables one at a time, without the parallel slopes assumption. So now, we can run a series of binary logistic regressions with varying cutpoints on the dependent variable to check the equality of coefficients across cutpoints</p>

<pre><code>par(mfrow=c(1,1))
plot(t, which=1:8, pch=1:8, xlab='logit', main=' ', xlim=range(s[,7:8]))
</code></pre>

<p><img src=""http://i.stack.imgur.com/4Uicq.jpg"" alt=""polr assumption check""></p>

<p>Apologies that I am no statistics expert and perhaps I am missing something obvious here. However, I have spent a long time trying to figure out if there is a problem in how I tested the model assumption and also trying to figure out other ways to run the same kind of model. </p>

<p>For example, I read in many help mailing lists that others use the vglm function (in the VGAM package) and the lrm function (in the rms package) (for example see here:  <a href=""http://stats.stackexchange.com/questions/25988/proportional-odds-assumption-in-ordinal-logistic-regression-in-r-with-the-packag"">Proportional odds assumption in ordinal logistic regression in R with the packages VGAM and rms</a>). I have tried to run the same models but am continuously coming up against warnings and errors.</p>

<p>For example, when I try to fit the vglm model with the â€˜parallel=FALSEâ€™ argument (as the previous link mentions is important for testing the proportional odds assumption), I encounter the following error:</p>

<blockquote>
  <p>Error in lm.fit(X.vlm, y = z.vlm, ...) : NA/NaN/Inf in 'y'<br>
  In addition: Warning message:<br>
  In Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals = residuals,  :
    fitted values close to 0 or 1</p>
</blockquote>

<p>I would like to ask please if there is anyone who might understand and be able to explain to me why the graph I produced above looks as it does. If indeed it means that something isnâ€™t right, could you please help me find a way to test the proportional odds assumption when just using the polr function. Or if that is just not possible, then I will resort to trying to use the vglm function, but would then need some help to explain why I keep getting the error given above.</p>

<p>NOTE: As a background, there are 1000 datapoints here, which are actually location points across a study area. I am looking to see if there are any relationships between the categorical response variable and these 15 explanatory variables. All of those 15 explanatory variables are spatial characteristics (for example, elevation, x-y coordinates, proximity to forest etc.). The 1000 datapoints were randomly allocated using a GIS, but I took a stratified sampling approach. I made sure that 125 points were randomly chosen within each of the 8 different categorical response levels. I hope this information is also helpful.</p>
"
"0.0633215847514023","0.0620651280774201","125465","<p>I'm searching for a built-in function in R for calculating the required sample size (given certain power, alpha,..) of a mixed ANOVA (2 between, 1 within variable, 2x2x2 design). Does this function exist? I didn't find it in the following packages:</p>

<p>â€¢pwr is the oldest power-analysis library; some introductory info can be found on Quick-R</p>

<p>â€¢PoweR: Computation of power and level tables for hypothesis tests</p>

<p>â€¢Power2Stage: Power and Sample size distribution of 2-stage BE studies via simulations</p>

<p>â€¢powerAnalysis: Power analysis in experimental design</p>

<p>â€¢powerGWASinteraction: Power Calculations for Interactions for GWAS</p>

<p>â€¢powerMediation: Power/Sample size calculation for mediation analysis, simple linear 
regression, logistic regression, or longitudinal study</p>

<p>â€¢powerpkg: Power analyses for the affected sib pair and the TDT design</p>

<p>â€¢powerSurvEpi: Power and sample size calculation for survival analysis of epidemiological studies</p>

<p>â€¢PowerTOST: Power and Sample size based on two one-sided t-tests (TOST) for (bio)equivalence studies</p>

<p>â€¢longpower: Power and sample size for linear model of longitudinal data</p>

<p>Thanks!</p>
"
"0.0400480865731637","0.039253433598943","125523","<p>What I'm trying to do is to construct a linear model in a form like</p>

<p>$$
Y = \beta_0X_0-\beta_1X_1+\beta_2X_2 + \beta_3
$$</p>

<p>where $\beta_0$, $\beta_1$ and $\beta_2$ are coefficient of predictors $X_0$, $X_1$ and \beta_2 respectively. And, They all are <strong>positive</strong>, which means my assumption of the model is that $X_1$ has negative effect towards the response $Y$. Perhaps, I misunderstand the concept of regression, but if anyone has an idea how to achieve this in R, please enlighten me. Or any other approach apart from regression model. Thanks in advance.</p>
"
"0.0908205236199223","0.0964366217361766","125701","<p>As part of ongoing research I'm to test a certain model on some data. One of the questions asked (c.f. one of the hypotheses) involves estimating the quadratic term of an independent variable (in R). The problem is that there is concern that the variable is endogenous and maybe should be instrumented. Estimating the linear endogenous variable and including its fitted values into my main model (while correcting the standard errors) is not the problem. My question is how I can include the quadratic (instrumented) term of that endogenous variable into my main regression equation?</p>

<p>Assume my main linear model (in R):</p>

<p><strong>Y ~ X1 + X2,</strong></p>

<p>where X2 is the endogenous variable and X1 is some exogenous control. Then the model that corrects for the endogeneity of X2 through 2sls method:</p>

<p><strong>Y ~ X1 + X2hat,</strong>
where <strong>X2hat ~ Z1 + X1,</strong>
and Z1 is some exogenous instrument that influences Y through X2.</p>

<p>I read somewhere that in order to include the quadratic term of the 'instrumented' endogenous variable (X2hat), I would need to estimate the regular quadratic term based on the same regressors (i.e. Z1 + X1) as the linear 'instrumented' endogenous variable.</p>

<p>Main quadratic model (in R):</p>

<p><strong>Y ~ X1 + poly(X2,2)</strong>,</p>

<p>where poly(X2,2) creates orthogonal linear and quadratic terms of X2 (through QR decomposition). Then the model that adjusts for endogeneity through 2sls would be (assuming what I read is correct):</p>

<p><strong>Y ~ X1 + X2hat + (X2^2)hat</strong>, 
where <strong>(X2^2)hat ~ Z1 + X1</strong>
Is the method I'm using the correct one? Or should I approach this problem in a different way? Is this even possible or would you expect problems with multicollinearity?</p>
"
"0.0400480865731637","0.039253433598943","125728","<p>In linear regression model, the <code>predict</code> in <code>R</code> is able to calculate the <code>confidence band</code> and <code>prediction band</code>. But for the general predictive models, what are the normal approaches to calculate <code>confidence band</code> and <code>prediction band</code>? I am asking this is because I am experimenting different models for a given data set. The users would like to get <code>confidence band</code> and <code>prediction band</code> as what <code>predict.lm</code> can give. However, not all those model packages provide these two bands-related information. I may have to write my algorithm to calculate them.</p>
"
"0.0566365471788599","0.0555127381653369","125764","<p>I'm trying to understand what factors contribute to the a certain outcome which is a ordered factor variable. In order to just understand which factor is statistically more significant than the others, I would like to build a model and given that my output variable is an ordered factorial variable - I thought I should go for Ordinal Logistic Regression. Given the tradeoff between interpretability and flexibility of models, in my case since I'm only making inferences and not predictions, should I rather go for a easier model to handle like Generalized Linear Models? It kind of boils down to me choosing the <code>polr</code> package in R versus the <code>glm</code> one.</p>
"
"0.10236445520486","0.085999964166689","125787","<p>I would like to learn what is the correct way to approach analysis of this data. I have done some reading on the subject, but I still feel uncertain. Perhaps many approaches are valid, but simply  that some are more conservative than others?</p>

<p>My study: </p>

<p>I have 5 grasslands, and in each grassland I have 30 spiders. For each spider I have an estimate of what proportion of herbivores it consumes ""Diet"" (so 5 x 30, n = 150). For each grassland I also have an estimate of the overall biomass of herbivores that exist there ""Biomass"". Thus I have 5 values of ""Biomass"" (one for each grassland) and 150 of ""Diet"" (30 spiders per grassland). Both Diet and Biomass are continous variables. </p>

<p>I would like to run an anlysis that tests how Diet changes across Biomass and derive a slope value, thus keeping Biomass as a continous variable:</p>

<p>Diet ~ Biomass</p>

<p>As I understand it, if I use raw data for Diet (n=150) then using anova is more approrpiate, and grassland becomes a factor with 5 levels.</p>

<p>Or I could run it as a linear regression and thus keep Biomass as a continuous variable and derive a slope value. However, as a linear regression, should I use the raw data (n=150) or mean values (so 5 means - one for each grassland based on 30 samples). Which of the 2 linear regression approaches is correct? (means or raw data). </p>

<p>While I am familiar with the notion that both anova and regression have the same underlying mathematics and are now regarded as general linear modelling, I still don't know how this affects the data that I should be using when running a linear model of the form:  Diet ~ Biomass</p>

<p>Using raw data seems better because it captures the variability in the dataset, but if i use it with Biomass as a continous variable to get a slope value (i.e regression analysis) I am concerned that it inflates the degrees of freedom (df=1,149) and is psuedo-replicated, so inaccurately increases my chances of a significant result? Therefore, is it incorrect to model the raw data (n=150) against only 5 values of ""Biomass"" in a linear form (and not as factors as required in an anova)?</p>
"
"0.02831827358943","0.0277563690826684","126126","<p>I am quite new in R and am on a stage of running a regression model there.
The approach we have chosen is linear regression with dummy variables.
As far as my knowledge and experience go when using dummies one should choose a null interval - class(group) within a variable that will receive 0 points. This should help tackle the multicollinearity.</p>

<p>My question is - how do we set (if we can) the null interval?
I reviewed the ""dummies"" package but did not see an option there.</p>

<p>Thanks,
N</p>
"
"0.0942489115008991","0.0923787802599364","126252","<p>As part of ongoing research I'm to test a certain model on some data. One of the questions asked (c.f. one of the hypotheses) involves estimating the quadratic term of an independent variable (in R). The problem is that there is concern that the variable is endogenous and maybe should be instrumented. Estimating the linear endogenous variable and including its fitted values into my main model (while correcting the standard errors) is not the problem. My question is how I can include the quadratic (instrumented) term of that endogenous variable into my main regression equation?</p>

<p>Assume my main linear model (in R):</p>

<p><strong>Y ~ X1 + X2</strong>,<p>
 where X2 is the endogenous variable and X1 is some exogenous control. Then the model that corrects for the endogeneity of X2 through 2sls method:</p><br />
<strong>Y ~ X1 + X2hat</strong>,<br />
where <strong>X2hat ~ Z1 + X1</strong>,<br />
and Z1 is some exogenous instrument that influences Y through X2. </p>

<p>I read somewhere that in order to include the quadratic term of the 'instrumented' endogenous variable (X2hat), I would need to estimate the regular quadratic term based on the same regressors (i.e. Z1 + X1) as the linear 'instrumented' endogenous variable.</p>

<p>Main quadratic model (in R):</p>

<p><strong>Y ~ X1 + poly(X2,2)</strong>,<br /><p>
 where poly(X2,2) creates orthogonal linear and quadratic terms of X2 (through QR decomposition). Then the model that adjusts for endogeneity through 2sls would be (assuming what I read is correct):</p>
<strong>Y ~ X1 + X2hat + (X2^2)hat</strong>, <br />
 where <strong>(X2^2)hat ~ Z1 + X1</strong><br /></p>

<p>Is the method I'm using the correct one? Or should I approach this problem in a different way? Is this even possible or would you expect problems with multicollinearity? </p>

<p>Thanks in advance!</p>

<p>Kind Regards.</p>

<p>Jack</p>
"
"0.0700841515030364","0.0686935087981502","126356","<p>For linear and parametric regression there are multiple tests where variables and residuals are used by means of performing a linear regression function to test serial correlation of regression errors and homocedasticity of regression errors. </p>

<p>My question is about non linear and non parametric regression for prediction or classification such us SVM, NeuralNets, knn, Recursive Partitioning, Adaptive Regression Spline, etc. </p>

<p>In this regard my questions are:</p>

<ol>
<li><p>As is not linear regression what is the equivalent of OLS assumptions for non linear non parametric regression. Are the consequences of OLS violation in the context of nonlinear and non-parametric regression still valid? </p></li>
<li><p>How could I test or what tests exist for serial correlation of errors for non linear and non parametric regression which are not derived from visual inspection of a graph and by using error residuals only. (something comes in mind like testing for significant acf or pacf on the residual errors - Unsure if this is OK).</p></li>
<li><p>How could I test or what tests exist for homocedasticity for non linear and non parametric regression which are not derived from visual inspection of a graph and by using error residuals only. (something comes too mind like homogenity of distances between the residual errors across time).</p></li>
<li><p>Would it be better to transfor the data into linear by seeking some adequate transformation as to avoid all the non linearity issues mentioned above? </p></li>
</ol>

<p>Thank you</p>
"
"0.0400480865731637","0.039253433598943","126510","<p>How do we do two-way ANOVA (one observation per mean), as testing H_A in Section 8.5 in Seber and Lee's Linear Regression Analysis, in R?
Note that the linear model for this case doesn't have interaction between the row and column factors.</p>

<p>For example, I want to test in the following 3 x 2 table, if the mean of each row is the same. </p>

<p>5 | 4<br>
7 | 6<br>
4 | 7  </p>

<p>Note that I used <code>lm</code> for one-way ANOVA, but couldn't find out which function and arguments to do two-way ANOVA (one observation per mean). I am not trying to implement it in R.</p>

<p>Thanks.</p>
"
"0.0400480865731637","0.039253433598943","126976","<p>I am playing a data without any background information. First, I try multiple linear regression. The model fits well, since the $r^2$ is larger than 90%. I deleted several variables by AIC. The fits improves a little bit. I am interesting what are possible directions that I should look into, if the model fits well at the very beginning.</p>

<p>The target of the analysis is find the model that make the most accurate prediction.</p>

<p>What I can come up with:</p>

<ol>
<li><p>Try other models like regression tree, random forest, SVM or whatever data mining models. The prediction may or may not become better.</p></li>
<li><p>Perform cross validation.</p></li>
<li><p>Check overfitting.</p></li>
<li><p>Diagnostic residual.</p></li>
</ol>
"
"0.0966361295372218","0.0947186299929268","127134","<p><strong>Updated</strong></p>

<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Yeardecimal - Date of procedure (expressed as decimal of year) = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 16-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
Mechanism - Mechanism of injury = Fall &lt;2m, Fall &gt;2m, Shooting/stabbing, RTC (Road Traffic Collision), Other
neuroFirst - Location of first admission (Neurosurgical Unit) = NSU vs. Non-NSU
rcteye - Pupil reactivity = NA / Both unreactive = O, 1 reactive = 1, both reactive = 2
rcteyeYN - dummy = 0 or 1 for presence or absence of data
GCS - Glasgow Coma Scale = 3-15
GCSYN - dummy = 0 or 1 for presence or absence of data
</code></pre>

<p>Dummy variables were included to enable a larger sample size where the majority of cases were excluding  <code>GCS</code> and <code>rcteye</code> variables (missing not at random).</p>

<p>In order to test for interactions, initially I ran the following:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + rcteye + rcteyeYN + GCS + GCSYN + rcs(Yeardecimal))^2, data = ASDH_Paper1.1)
</code></pre>

<p>but when I did I got the following error:</p>

<pre><code>singular information matrix in lrm.fit (rank= 151 ).  Offending variable(s):
GCSYN * Yeardecimal''' GCSYN * Yeardecimal' GCSYN * Yeardecimal GCS * Yeardecimal''' GCS * Yeardecimal GCS * GCSYN rcteyeYN * Yeardecimal''' rcteyeYN * Yeardecimal'' rcteyeYN * Yeardecimal rcteyeYN * GCSYN rcteye * Yeardecimal''' rcteye * Yeardecimal rcteye * rcteyeYN Mechanism=RTC * Yeardecimal''' Mechanism=Other * Yeardecimal''' Mechanism=Fall &gt; 2m * Yeardecimal''' Mechanism=Shooting / Stabbing * Yeardecimal Mechanism=RTC * Yeardecimal Mechanism=Other * Yeardecimal Mechanism=Fall &gt; 2m * Yeardecimal neuroFirst * Yeardecimal ISS'' * Yeardecimal''' ISS * Yeardecimal''' ISS'' * Yeardecimal'' ISS'' * Yeardecimal ISS' * Yeardecimal ISS * Yeardecimal ISS'' * GCSYN ISS'' * rcteyeYN ISS'' * Mechanism=RTC Age'' * Yeardecimal''' Age'' * Yeardecimal'' Age''' * Yeardecimal' Age''' * Yeardecimal Age'' * Yeardecimal Age' * Yeardecimal Age * Yeardecimal Age'' * GCSYN Age''' * rcteyeYN 
Error in lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + neuroFirst + Mechanism +  : 
  Unable to fit model using â€œlrm.fitâ€
</code></pre>

<p>The only way I could run the model is with an adjustment. <code>Yeardecimal</code> is excluded from any interaction as is the interaction of <code>GCS:GCSYN</code> and <code>rcteye:rcteyeYN</code> which produced the same error as written above. It made sense to exclude the interactions between a variable and its missing dummy but I am not sure what to do about <code>Yeardecimal</code>:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + rcteye + rcteyeYN) * (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + GCS + GCSYN) + rcs(Yeardecimal), data = ASDH_Paper1.1)
</code></pre>

<p>From this model the following interactions were identified with an <code>anova</code> output:</p>

<pre><code>&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor                                                Chi-Square d.f. P     
 Age  (Factor+Higher Order Factors)                    130.42      52  &lt;.0001
  All Interactions                                      78.68      48  0.0034
  Nonlinear (Factor+Higher Order Factors)               46.53      39  0.1901
 ISS  (Factor+Higher Order Factors)                    181.65      42  &lt;.0001
  All Interactions                                      52.43      39  0.0738
  Nonlinear (Factor+Higher Order Factors)               55.01      28  0.0017
 neuroFirst  (Factor+Higher Order Factors)              37.68      16  0.0017
  All Interactions                                      11.54      15  0.7136
 Mechanism  (Factor+Higher Order Factors)               63.72      52  0.1277
  All Interactions                                      58.35      48  0.1455
 rcteye  (Factor+Higher Order Factors)                 242.07      15  &lt;.0001
  All Interactions                                      19.39      14  0.1507
 rcteyeYN  (Factor+Higher Order Factors)               204.58      15  &lt;.0001
  All Interactions                                      29.88      14  0.0079
 GCS  (Factor+Higher Order Factors)                    162.81      15  &lt;.0001
  All Interactions                                      11.62      14  0.6365
 GCSYN  (Factor+Higher Order Factors)                   94.50      15  &lt;.0001
  All Interactions                                      41.74      14  0.0001
 Yeardecimal                                            51.96       4  &lt;.0001
  Nonlinear                                             10.27       3  0.0164
 Age * ISS  (Factor+Higher Order Factors)               11.90      12  0.4534
  Nonlinear                                              9.40      11  0.5851
  Nonlinear Interaction : f(A,B) vs. AB                  9.40      11  0.5851
  f(A,B) vs. Af(B) + Bg(A)                               7.96       6  0.2411
  Nonlinear Interaction in Age vs. Af(B)                 8.75       9  0.4605
  Nonlinear Interaction in ISS vs. Bg(A)                 8.58       8  0.3790
 Age * neuroFirst  (Factor+Higher Order Factors)         2.66       4  0.6166
  Nonlinear                                              2.05       3  0.5624
  Nonlinear Interaction : f(A,B) vs. AB                  2.05       3  0.5624
 Age * Mechanism  (Factor+Higher Order Factors)         17.58      16  0.3493
  Nonlinear                                             13.82      12  0.3127
  Nonlinear Interaction : f(A,B) vs. AB                 13.82      12  0.3127
 Age * GCS  (Factor+Higher Order Factors)                6.24       4  0.1819
  Nonlinear                                              3.89       3  0.2741
  Nonlinear Interaction : f(A,B) vs. AB                  3.89       3  0.2741
 Age * GCSYN  (Factor+Higher Order Factors)             20.11       4  0.0005
  Nonlinear                                              8.86       3  0.0312
  Nonlinear Interaction : f(A,B) vs. AB                  8.86       3  0.0312
 ISS * neuroFirst  (Factor+Higher Order Factors)         3.23       3  0.3571
  Nonlinear                                              0.87       2  0.6480
  Nonlinear Interaction : f(A,B) vs. AB                  0.87       2  0.6480
 ISS * Mechanism  (Factor+Higher Order Factors)         23.95      12  0.0206
  Nonlinear                                             20.66       8  0.0081
  Nonlinear Interaction : f(A,B) vs. AB                 20.66       8  0.0081
 ISS * GCS  (Factor+Higher Order Factors)                0.77       3  0.8570
  Nonlinear                                              0.42       2  0.8102
  Nonlinear Interaction : f(A,B) vs. AB                  0.42       2  0.8102
 ISS * GCSYN  (Factor+Higher Order Factors)              6.53       3  0.0886
  Nonlinear                                              2.35       2  0.3085
  Nonlinear Interaction : f(A,B) vs. AB                  2.35       2  0.3085
 neuroFirst * Mechanism  (Factor+Higher Order Factors)   2.45       4  0.6533
 neuroFirst * GCS  (Factor+Higher Order Factors)         0.00       1  0.9726
 neuroFirst * GCSYN  (Factor+Higher Order Factors)       1.39       1  0.2382
 Mechanism * GCS  (Factor+Higher Order Factors)          0.10       4  0.9987
 Mechanism * GCSYN  (Factor+Higher Order Factors)        1.74       4  0.7828
 Age * rcteye  (Factor+Higher Order Factors)             8.66       4  0.0702
  Nonlinear                                              7.29       3  0.0633
  Nonlinear Interaction : f(A,B) vs. AB                  7.29       3  0.0633
 ISS * rcteye  (Factor+Higher Order Factors)             4.18       3  0.2424
  Nonlinear                                              1.49       2  0.4744
  Nonlinear Interaction : f(A,B) vs. AB                  1.49       2  0.4744
 neuroFirst * rcteye  (Factor+Higher Order Factors)      0.10       1  0.7460
 Mechanism * rcteye  (Factor+Higher Order Factors)       3.44       4  0.4867
 rcteye * GCS  (Factor+Higher Order Factors)             2.30       1  0.1297
 rcteye * GCSYN  (Factor+Higher Order Factors)           2.57       1  0.1090
 Age * rcteyeYN  (Factor+Higher Order Factors)           7.23       4  0.1242
  Nonlinear                                              7.23       3  0.0649
  Nonlinear Interaction : f(A,B) vs. AB                  7.23       3  0.0649
 ISS * rcteyeYN  (Factor+Higher Order Factors)           2.47       3  0.4814
  Nonlinear                                              0.11       2  0.9462
  Nonlinear Interaction : f(A,B) vs. AB                  0.11       2  0.9462
 neuroFirst * rcteyeYN  (Factor+Higher Order Factors)    0.12       1  0.7280
 Mechanism * rcteyeYN  (Factor+Higher Order Factors)     1.81       4  0.7701
 rcteyeYN * GCS  (Factor+Higher Order Factors)           3.70       1  0.0543
 rcteyeYN * GCSYN  (Factor+Higher Order Factors)         8.74       1  0.0031
 TOTAL NONLINEAR                                       102.74      64  0.0015
 TOTAL INTERACTION                                     178.52     103  &lt;.0001
 TOTAL NONLINEAR + INTERACTION                         241.87     111  &lt;.0001
 TOTAL                                                 889.91     123  &lt;.0001
</code></pre>

<p>The <code>summary</code> function revealed the following results:</p>

<pre><code>             Effects              Response : Survive 

 Factor                                    Low    High   Diff. Effect       S.E.   Lower 0.95 Upper 0.95    
 Age                                         37.6   72.0 34.40         0.15   0.38   -0.58      8.900000e-01
  Odds Ratio                                 37.6   72.0 34.40         1.16     NA    0.56      2.430000e+00
 ISS                                         20.0   26.0  6.00        -1.34   0.31   -1.95     -7.400000e-01
  Odds Ratio                                 20.0   26.0  6.00         0.26     NA    0.14      4.800000e-01
 neuroFirst                                   0.0    1.0  1.00        -0.23   0.37   -0.95      5.000000e-01
  Odds Ratio                                  0.0    1.0  1.00         0.80     NA    0.39      1.650000e+00
 rcteye                                       0.0    2.0  2.00         3.20   0.50    2.22      4.170000e+00
  Odds Ratio                                  0.0    2.0  2.00        24.41     NA    9.24      6.452000e+01
 rcteyeYN                                     0.0    1.0  1.00        -3.34   0.44   -4.21     -2.480000e+00
  Odds Ratio                                  0.0    1.0  1.00         0.04     NA    0.01      8.000000e-02
 GCS                                          0.0   12.0 12.00         1.94   0.49    0.98      2.890000e+00
  Odds Ratio                                  0.0   12.0 12.00         6.94     NA    2.67      1.799000e+01
 GCSYN                                        0.0    1.0  1.00        -1.32   0.45   -2.20     -4.400000e-01
  Odds Ratio                                  0.0    1.0  1.00         0.27     NA    0.11      6.400000e-01
 Yeardecimal                               2005.5 2012.4  6.85         0.20   0.12   -0.03      4.400000e-01
  Odds Ratio                               2005.5 2012.4  6.85         1.22     NA    0.97      1.550000e+00
 Mechanism - Fall &gt; 2m:Fall &lt; 2m              1.0    2.0    NA        -0.89   0.35   -1.58     -2.000000e-01
  Odds Ratio                                  1.0    2.0    NA         0.41     NA    0.21      8.200000e-01
 Mechanism - Other:Fall &lt; 2m                  1.0    3.0    NA         0.25   0.42   -0.58      1.080000e+00
  Odds Ratio                                  1.0    3.0    NA         1.28     NA    0.56      2.930000e+00
 Mechanism - RTC:Fall &lt; 2m                    1.0    4.0    NA        -0.68   0.43   -1.52      1.700000e-01
  Odds Ratio                                  1.0    4.0    NA         0.51     NA    0.22      1.190000e+00
 Mechanism - Shooting / Stabbing:Fall &lt; 2m    1.0    5.0    NA        18.97 116.63 -209.63      2.475600e+02
  Odds Ratio                                  1.0    5.0    NA 172906690.96     NA    0.00     3.272814e+107

Adjusted to: Age=54.2 ISS=25 neuroFirst=0 Mechanism=Fall &lt; 2m rcteye=1 rcteyeYN=0 GCS=3 GCSYN=0 
</code></pre>

<p>Remaining questions are:</p>

<p><strong>1</strong> - Is my dummy variable treatment for variables missing not at random appropriate, including the exclusion of interactions with the main term?</p>

<p><strong>2</strong> - Can I resolve the issues with assessing interaction of the Yeardecimal term?</p>

<p><strong>3</strong> - Should I exclude non-significant interaction terms? I read that exclusion only of a ""chunk"" is advised - <a href=""http://stats.stackexchange.com/questions/11009/including-the-interaction-but-not-the-main-effects-in-a-model"">Including the interaction but not the main effects in a model</a></p>

<p><strong>4</strong> - Is the odds ratio for each variable the ""Effect"" column? If so, is this the OR between the lowest and highest value of each variable?</p>
"
"0.0633215847514023","0.0620651280774201","128815","<p>I'm trying to estimate the value of a property depending on the property characteristics. I did some research and I found out, that it would be better to use the <strong>Hedonic Model/Regression</strong> instead of <strong>Linear Square Regression</strong>.</p>

<p>After reading a couple of papers about it, I still have some questions.</p>

<p>I work with <strong>R</strong>, so I have the data (information about other properties) saved as a data.frame, with the following columns (c stands for characteristic).</p>

<pre><code>----------------------------------
| price | c1 | c2 | c3 | c4 | c5 |
----------------------------------
</code></pre>

<p>My questions:</p>

<ol>
<li>I know how to estimate the coefficients with the <strong>Least Square Regression</strong>, but how do I do it with the <strong>Hedonic Regression</strong>? I know, that in <strong>R</strong> is no function for it.</li>
<li>The environment characteristics (air pollution, criminality rate, etc.) are almost the same, because the properties are in the same district. The <strong>Last Square Regression</strong> gives them a very small coefficient, but they have a big importance in real life. How can I tell the regression, that they have a big importance?</li>
<li>As I understood so far, if an attribute of an observation is missing, I should not use the observation, is that right?</li>
<li>In the calculation of the coefficients, should I use only the date from nearby (example: same district) real estates or it would be better to use all real estates from the town?</li>
</ol>

<p>Could somebody please give me a hint?</p>

<p>Thank you very much!       </p>
"
"0.02831827358943","0.0277563690826684","128951","<p>I have been asked to compared between Robustness of absolute lost regression and its variants compared to least squares. I have done the least squares should I use Lasso now?</p>

<pre><code>con = url (""http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data"")
prost=read.csv(con,row.names=1,sep=""\t"")

summary(prost)

plot (prost$age, prost$lcavol) # standard plot

plot (prost) # all vs all in the R window
# into file:
#       postscript(""prost.ps"")
#       plot (prost) #into postscript file in current directory (also see commands pdf, jpeg, etc..)
#       dev.off()


prost.tr = prost[prost$train,] # train observations
    prost.te = prost[!prost$train,] # test observations

attach(prost.tr) # now we can treat columns as variables
summary (age)
detach()


####linear regression#####

prost.linreg =  lm (lpsa~.-train, data=prost.tr)
summary(prost.linreg)


lin.pred.te = predict (prost.linreg, newdata=prost.te)
lin.pred.te[14] = 500000000


summary((prost.te$lpsa-lin.pred.te)^2)
    rmse.lin = sqrt(mean((prost.te$lpsa-lin.pred.te)^2))


y.lin=0
x.lin= c(1:30)
for (i in 1:30){
  y.lin[i] = lin.pred.te[i]

}


lin.reg=summary(lm(y.lin~x.lin))
</code></pre>
"
"0.135086047469658","0.136543281770324","129761","<p>These multiple imputation results relate to data I have previously described and shown here - <a href=""http://stats.stackexchange.com/questions/129739/skewed-distributions-for-logistic-regression"">Skewed Distributions for Logistic Regression</a></p>

<p>Three variables I am using have missing data. Their names, descriptions and % missing are shown below.</p>

<pre><code>inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis) - 58% missing
GCS - Glasgow Coma Scale = 3-15 - 37% missing
rcteyemi - Pupil reactivity (1 = neither, 2 = one, 3 = both) - 56% missing
</code></pre>

<p>I have been using mutliple imputation to model the missing data above following advice in a previous post here - <a href=""http://stats.stackexchange.com/questions/127134/describing-results-from-logistic-regression-with-restricted-cubic-splines-using"">Describing Results from Logistic Regression with Restricted Cubic Splines Using rms in R</a></p>

<p>Given this is a longitudinal analysis, a key variable of importance is the year of the treatment so we can investigate how our patient management has improved. The variable in question, <code>Yeardecimal</code> is highly significant in univariate analysis:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)
&gt; 
&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2      91.47    R2       0.023    C       0.572    
 0           1281    d.f.             1    g        0.309    Dxy     0.143    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       1.362    gamma   0.146    
max |deriv| 3e-12                          gp       0.054    tau-a   0.048    
                                           Brier    0.165                     

             Coef   S.E.   Wald Z Pr(&gt;|Z|)
Intercept    0.8696 0.0530 16.42  &lt;0.0001 
Yeardecimalc 0.0551 0.0057  9.70  &lt;0.0001 
</code></pre>

<p>To deal with missingness, I used <code>aregImpute</code> and <code>fit.mult.impute</code> to conduct multiple imputation prior to multivariate logisic regression. When including Yeardecimal, the results were as follows:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS + Yeardecimalc, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS + Yeardecimalc, data = ASDH_Paper1.1, n.impute = 10, 
    nk = 4)

n: 5998     p: 12   Imputations: 10     nk: 4 

Number of NAs:
   Outcome30          Age          GCS        Other          ISS    inctoCran     rcteyemi   neuroFirst      neuroYN 
           0            0         2242            0            0         3500         3376            0            0 
   Mechanism          LOS Yeardecimalc 
           0            0            0 

             type d.f.
Outcome30       c    1
Age             s    3
GCS             s    3
Other           c    1
ISS             s    3
inctoCran       s    3
rcteyemi        l    1
neuroFirst      l    1
neuroYN         l    1
Mechanism       c    4
LOS             s    3
Yeardecimalc    s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.421     0.181     0.358 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)

&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1609.98    R2       0.365    C       0.836    
 0           1281    d.f.            25    g        1.584    Dxy     0.672    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.875    gamma   0.674    
max |deriv| 0.001                          gp       0.222    tau-a   0.226    
                                           Brier    0.121                     

                              Coef    S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     21.3339 67.4400  0.32  0.7517  
Age                           -0.0088  0.0132 -0.67  0.5052  
Age'                          -0.0294  0.0643 -0.46  0.6471  
Age''                         -0.0134  0.2479 -0.05  0.9570  
Age'''                         0.2588  0.3534  0.73  0.4639  
GCS                            0.1100  0.0145  7.61  &lt;0.0001 
Mechanism=Fall &gt; 2m           -0.0651  0.1162 -0.56  0.5754  
Mechanism=Other                0.2285  0.1338  1.71  0.0876  
Mechanism=RTC                  0.0449  0.1332  0.34  0.7360  
Mechanism=Shooting / Stabbing  2.1150  1.1142  1.90  0.0577  
ISS                           -0.1069  0.0318 -3.36  0.0008  
ISS'                          -0.0359  0.1306 -0.27  0.7835  
ISS''                          1.8296  1.9259  0.95  0.3421  
neuroFirst                    -0.3483  0.0973 -3.58  0.0003  
inctoCrand                     0.0001  0.0053  0.02  0.9872  
inctoCrand'                   -0.0745  0.3060 -0.24  0.8077  
inctoCrand''                   0.1696  0.5901  0.29  0.7738  
inctoCrand'''                 -0.1167  0.3150 -0.37  0.7110  
inctoCranYN                   -0.2814  0.6165 -0.46  0.6480  
Yeardecimalc                  -0.0101  0.0337 -0.30  0.7641  
Yeardecimalc'                  0.0386  0.0651  0.59  0.5536  
Yeardecimalc''                -0.7417  0.8210 -0.90  0.3663  
Yeardecimalc'''                7.0367  4.9344  1.43  0.1539  
Sex=Male                       0.0668  0.0891  0.75  0.4534  
Other=1                        0.3238  0.1611  2.01  0.0445  
rcteyemi                       1.1589  0.1050 11.04  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              83.07      4   &lt;.0001
  Nonlinear        5.97      3   0.1131
 GCS              57.89      1   &lt;.0001
 Mechanism         8.14      4   0.0867
 ISS              77.31      3   &lt;.0001
  Nonlinear       35.04      2   &lt;.0001
 neuroFirst       12.81      1   0.0003
 inctoCrand        2.32      4   0.6777
  Nonlinear        2.29      3   0.5149
 inctoCranYN       0.21      1   0.6480
 Yeardecimalc      4.19      4   0.3807
  Nonlinear        3.77      3   0.2874
 Sex               0.56      1   0.4534
 Other             4.04      1   0.0445
 rcteyemi        121.80      1   &lt;.0001
 TOTAL NONLINEAR  47.27     11   &lt;.0001
 TOTAL           679.09     25   &lt;.0001
&gt; 
</code></pre>

<p>Yeardecimal is no longer significant. However, if I exclude Yeardecimal from aregImpute only, I have the alternative result below:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS, data = ASDH_Paper1.1, n.impute = 10, nk = 4)

n: 5998     p: 11   Imputations: 10     nk: 4 

Number of NAs:
 Outcome30        Age        GCS      Other        ISS  inctoCran   rcteyemi neuroFirst    neuroYN  Mechanism        LOS 
         0          0       2242          0          0       3500       3376          0          0          0          0 

           type d.f.
Outcome30     c    1
Age           s    3
GCS           s    3
Other         c    1
ISS           s    3
inctoCran     s    3
rcteyemi      l    1
neuroFirst    l    1
neuroYN       l    1
Mechanism     c    4
LOS           s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.407     0.194     0.320 
&gt; 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)
&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1607.92    R2       0.364    C       0.834    
 0           1281    d.f.            25    g        1.578    Dxy     0.667    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.846    gamma   0.669    
max |deriv| 0.003                          gp       0.221    tau-a   0.224    
                                           Brier    0.120                     

                              Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     -55.6574 58.3464 -0.95  0.3401  
Age                            -0.0084  0.0128 -0.66  0.5105  
Age'                           -0.0335  0.0612 -0.55  0.5838  
Age''                           0.0050  0.2365  0.02  0.9830  
Age'''                          0.2321  0.3387  0.69  0.4930  
GCS                             0.1099  0.0124  8.88  &lt;0.0001 
Mechanism=Fall &gt; 2m            -0.0631  0.1138 -0.55  0.5793  
Mechanism=Other                 0.2354  0.1381  1.70  0.0883  
Mechanism=RTC                   0.0315  0.1319  0.24  0.8114  
Mechanism=Shooting / Stabbing   1.9297  1.0930  1.77  0.0775  
ISS                            -0.1012  0.0335 -3.02  0.0025  
ISS'                           -0.0599  0.1366 -0.44  0.6613  
ISS''                           2.1581  2.0120  1.07  0.2834  
neuroFirst                     -0.3753  0.0888 -4.23  &lt;0.0001 
inctoCrand                     -0.0007  0.0054 -0.13  0.9002  
inctoCrand'                    -0.0496  0.3116 -0.16  0.8734  
inctoCrand''                    0.1316  0.6021  0.22  0.8270  
inctoCrand'''                  -0.1078  0.3224 -0.33  0.7381  
inctoCranYN                    -0.1697  0.6172 -0.27  0.7834  
Yeardecimalc                    0.0281  0.0291  0.96  0.3349  
Yeardecimalc'                   0.0682  0.0600  1.14  0.2553  
Yeardecimalc''                 -1.4037  0.7685 -1.83  0.0678  
Yeardecimalc'''                10.2513  4.8156  2.13  0.0333  
Sex=Male                        0.0595  0.0890  0.67  0.5037  
Other=1                         0.3579  0.1641  2.18  0.0292  
rcteyemi                        1.1862  0.0799 14.85  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              78.39      4   &lt;.0001
  Nonlinear        6.23      3   0.1011
 GCS              78.86      1   &lt;.0001
 Mechanism         7.53      4   0.1104
 ISS              76.46      3   &lt;.0001
  Nonlinear       31.16      2   &lt;.0001
 neuroFirst       17.87      1   &lt;.0001
 inctoCrand        3.22      4   0.5214
  Nonlinear        3.19      3   0.3630
 inctoCranYN       0.08      1   0.7834
 Yeardecimalc     44.83      4   &lt;.0001
  Nonlinear        4.67      3   0.1979
 Sex               0.45      1   0.5037
 Other             4.76      1   0.0292
 rcteyemi        220.51      1   &lt;.0001
 TOTAL NONLINEAR  45.39     11   &lt;.0001
 TOTAL           715.22     25   &lt;.0001
&gt; 
</code></pre>

<p>Can anyone help me understand why the statistical results for Yeardecimal are so starkly different?</p>
"
"0.02831827358943","0.0277563690826684","129788","<p>I am running a linear regression model in R:</p>

<pre><code>data(iris)
fit1.iris = lm(Sepal.Length ~ Petal.Length+Petal.Width , data=iris) 
summary(fit1.iris)
</code></pre>

<p>These are my coefficients:</p>

<pre><code>Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   4.19058    0.09705  43.181  &lt; 2e-16 ***
Petal.Length  0.54178    0.06928   7.820 9.41e-13 ***
Petal.Width  -0.31955    0.16045  -1.992   0.0483 * 
</code></pre>

<p>I am trying to plot the density curve for parameter estimates, and below is how I did it for intercept. Am I doing it right ?</p>

<pre><code>  fit_iris = lm(Sepal.Length~ Petal.Length+Petal.Width , data=iris, x=TRUE, y=TRUE)
  summary(fit_iris)
  x_iris = seq(0, 10, length.out=1000)
  plot(density(dnorm(x,4.190582,0.09705)), type='l')
</code></pre>
"
"0.02831827358943","0.0277563690826684","129864","<p>I have to model the 4 seasons on basis of temperature and precipitation.
I have no idea which model to use. I'm a new R user and I've used only linear model and logistic regression till now. Is there another model that could fit well with this kind of dataset ?</p>
"
"0.02831827358943","0.0277563690826684","130476","<p>This model is a simple linear regression:</p>

<pre><code>mtcars_lm &lt;- lm(mpg ~ wt, mtcars)
</code></pre>

<p>And this model adds <code>cyl</code> as a random effect:</p>

<pre><code>library(lme4)
mtcars_mixed_effects &lt;- lmer(mpg ~ wt + (1 | cyl), mtcars)
</code></pre>

<p>Is there a way to test whether adding <code>cyl</code> as random effect is worthwhile? I tried this but it threw an error:</p>

<pre><code>anova(mtcars_mixed_effects, mtcars_lm)
</code></pre>

<p>(please disregard the fact that <code>cyl</code> only has three groups, I'm just using one of R's built in datasets to make question reprodicible).</p>
"
"0.02831827358943","0.0277563690826684","130484","<p>I've ran this linear regression:</p>

<pre><code>mtcars_lm &lt;- lm(mpg ~ wt, mtcars)
</code></pre>

<p>Lets say I observe a value of <code>mpg</code> that is 2 above the predicted value given x <code>wt</code>. Am I right in saying this would be 0.67 standard deviations above the predicted value. Here's my workings out:</p>

<pre><code>library(broom)
mtcars_lm_df &lt;- augment(mtcars_lm)
2/sd(mtcars_lm_df$.resid) 
</code></pre>
"
"0.0633215847514023","0.0620651280774201","131093","<p>I hope this is not a duplicate but I cannot find the answer to this question. In a linear model
$$Y_i = \beta_1 X_{i,1} + \dots + \beta_{p-1} X_{i,p-1} + \varepsilon_i, \qquad i = 1, \ldots, n$$
with the usual assumptions, is the regression sum of squares, $SSR$, still</p>

<p>$$ SSR = \sum_{i=1}^n (\hat{y_i} - \overline{y})^2 \text{ ?}$$
where $\hat{y_i} = X \hat{\beta}$ is the $i$-th fitted value, $\hat{\beta} = (X^TX)^{-1}X^T y$ and $X$ is the design matrix without the column of ones that it would have if we couldn't assume $\beta_0 = 0$.</p>

<p>Now, I'm asking this question because using the ""anova"" function in R, you can obtain the $SSR$ by simply adding the corresponding $SSR$'s of each variable (I believe this is called a type I decomposition), but this doesn't match the $SSR$ as calculated above for a model with $\beta_0 = 0$.</p>

<p>Am I missing something or did I just screw up calculating it? </p>

<p>I had a sample of 2 variables, $X_1$ and $X_2$ with $n=11$ observations, as follows:
$x_1 = (1,4,9,11,3,8,5,10,2,7,6)^T$, $x_2 = (8,2,-8,-10,6,-6,0,-12,4,-2,-4)^T$ and $y=(6,8,1,0,5,3,2,-4,10,-3,5)^T$.</p>

<p>I introduced them in R as y, x1 and x2. Then using anova(lm(y~0+x1+x2)) I got Sum Sq of 14.279 for x1 and 161.846 for x2. Their sum is 176.154.</p>

<p>However, using the design matrix with $x_1$ and $x_2$ as its columns, I got $\beta = (\beta_1, \beta_2)^T = (0.7211, 0.8089)^T$ (which matches the ones obtained in R) so $SSR = 96.37352$, which is obviously different from the one obtained in R.</p>
"
"0.0942489115008991","0.0923787802599364","131152","<p>Let say I've ran this linear regression:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt + vs, mtcars)
</code></pre>

<p>I can use <code>anova()</code> to see the amount of variance in the dependent variable accounted for by the two predictors:</p>

<pre><code>anova(lm_mtcars)

Analysis of Variance Table

Response: mpg
          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
wt         1 847.73  847.73 109.7042 2.284e-11 ***
vs         1  54.23   54.23   7.0177   0.01293 *  
Residuals 29 224.09    7.73                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Lets say I now add a random intercept for <code>cyl</code>:</p>

<pre><code>library(lme4)
lmer_mtcars &lt;- lmer(mpg ~ wt + vs + (1 | cyl), mtcars)
summary(lmer_mtcars)

Linear mixed model fit by REML ['lmerMod']
Formula: mpg ~ wt + vs + (1 | cyl)
   Data: mtcars

REML criterion at convergence: 148.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.67088 -0.68589 -0.08363  0.48294  2.16959 

Random effects:
 Groups   Name        Variance Std.Dev.
 cyl      (Intercept) 3.624    1.904   
 Residual             6.784    2.605   
Number of obs: 32, groups:  cyl, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  31.4788     2.6007  12.104
wt           -3.8054     0.6989  -5.445
vs            1.9500     1.4315   1.362

Correlation of Fixed Effects:
   (Intr) wt    
wt -0.846       
vs -0.272  0.006
</code></pre>

<p>The variance accounted for by each fixed effect now drops because the random intercept for <code>cyl</code> is now accounting for some of the variance in <code>mpg</code>:</p>

<pre><code>anova(lmer_mtcars)

Analysis of Variance Table
   Df  Sum Sq Mean Sq F value
wt  1 201.707 201.707 29.7345
vs  1  12.587  12.587  1.8555
</code></pre>

<p>But in <code>lmer_mtcars</code>, how can I tell what proportion of the variance is being accounted for by <code>wt</code>, <code>vs</code> and the random intecept for <code>cyl</code>?</p>
"
"0.0490486886395286","0.0480754414848157","131218","<p>I have 50 time series and I'd like to form a VAR equation for each of the time series. I'm looking for a method to find the best subset required for each time series VAR equation. For instance only lag of a few other time series might be enough for predicting y1. There are similar methods for multi-variant linear regression based on information criteria and t-test such as step-wise regression but I couldn't find any for VAR in R.</p>
"
"0.16759783801218","0.159709163896255","131312","<p>I have some R code (which I did not write) and which performs some state space analysis on some time-series. The data itself is shown as dots (scatter plot) and the Kalman filtered and smoothed state is the solid line.</p>

<p><img src=""http://i.stack.imgur.com/41jaI.png"" alt=""Plot""></p>

<p>My question is regarding the confidence intervals shown in this plot. I calculate <em>my own</em> confidence intervals using the standard method (my C# code is below)</p>

<pre><code>public static double ConfidenceInterval(
    IEnumerable&lt;double&gt; samples, double interval)
{
    Contract.Requires(interval &gt; 0 &amp;&amp; interval &lt; 1.0);

    double theta = (interval + 1.0) / 2;
    int sampleSize = samples.Count();
    double alpha = 1.0 - interval;
    double mean = samples.Mean();
    double sd = samples.StandardDeviation();

    var student = new StudentT(0, 1, samples.Count() - 1);
    double T = student.InverseCumulativeDistribution(theta);
    return T * (sd / Math.Sqrt(samples.Count()));
}
</code></pre>

<p>Now this will return a single interval (and it does it correctly) which I will add/subtract from each point on the series I have applied the calculation to to give me my confidence interval. But this is a constant and the R implementation seems to change over the time-series.</p>

<p>My question is why is <strong>the confidence interval changing for the R implementation? Should I be implementing my confidence levels/intervals differently?</strong></p>

<p>Thanks for your time.</p>

<hr>

<p>For reference the R code that produces this plot is below:</p>

<pre><code>install.packages('KFAS')
require(KFAS)

# Example of local level model for Nile series
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='BFGS',control=list(REPORT=1,trace=1))$model

# Can use different optimisation: 
# should be one of â€œNelder-Meadâ€, â€œBFGSâ€, â€œCGâ€, â€œL-BFGS-Bâ€, â€œSANNâ€, â€œBrentâ€
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='L-BFGS-B',control=list(REPORT=1,trace=1))$model

# Filtering and state smoothing
out&lt;-KFS(modelNile,filtering='state',smoothing='state')
out$model$H
out$model$Q
out

# Confidence and prediction intervals for the expected value and the observations.
# Note that predict uses original model object, not the output from KFS.
conf&lt;-predict(modelNile,interval='confidence')
pred&lt;-predict(modelNile,interval='prediction')
ts.plot(cbind(Nile,pred,conf[,-1]),col=c(1:2,3,3,4,4),
ylab='Predicted Annual flow', main='River Nile')
KFAS 13

# Missing observations, using same parameter estimates
y&lt;-Nile
y[c(21:40,61:80)]&lt;-NA
modelNile&lt;-SSModel(y~SSMtrend(1,Q=list(modelNile$Q)),H=modelNile$H)
out&lt;-KFS(modelNile,filtering='mean',smoothing='mean')

# Filtered and smoothed states
plot.ts(cbind(y,fitted(out,filtered=TRUE),fitted(out)), plot.type='single',
col=1:3, ylab='Predicted Annual flow', main='River Nile')

# Example of multivariate local level model with only one state
# Two series of average global temperature deviations for years 1880-1987
# See Shumway and Stoffer (2006), p. 327 for details
data(GlobalTemp)
model&lt;-SSModel(GlobalTemp~SSMtrend(1,Q=NA,type='common'),H=matrix(NA,2,2))

# Estimating the variance parameters
inits&lt;-chol(cov(GlobalTemp))[c(1,4,3)]
inits[1:2]&lt;-log(inits[1:2])
fit&lt;-fitSSM(inits=c(0.5*log(.1),inits),model=model,method='BFGS')
out&lt;-KFS(fit$model)
    ts.plot(cbind(model$y,coef(out)),col=1:3)
legend('bottomright',legend=c(colnames(GlobalTemp), 'Smoothed signal'), col=1:3, lty=1)

# Seatbelts data
## Not run:
model&lt;-SSModel(log(drivers)~SSMtrend(1,Q=list(NA))+
SSMseasonal(period=12,sea.type='trigonometric',Q=NA)+
log(PetrolPrice)+law,data=Seatbelts,H=NA)

# As trigonometric seasonal contains several disturbances which are all
# identically distributed, default behaviour of fitSSM is not enough,
# as we have constrained Q. We can either provide our own
# model updating function with fitSSM, or just use optim directly:
# option 1:
ownupdatefn&lt;-function(pars,model,...){
model$H[]&lt;-exp(pars[1])
    diag(model$Q[,,1])&lt;-exp(c(pars[2],rep(pars[3],11)))
model #for option 2, replace this with -logLik(model) and call optim directly
}
14 KFAS
fit&lt;-fitSSM(inits=log(c(var(log(Seatbelts[,'drivers'])),0.001,0.0001)),
model=model,updatefn=ownupdatefn,method='BFGS')
out&lt;-KFS(fit$model,smoothing=c('state','mean'))
    out
    ts.plot(cbind(out$model$y,fitted(out)),lty=1:2,col=1:2,
    main='Observations and smoothed signal with and without seasonal component')
    lines(signal(out,states=c(""regression"",""trend""))$signal,col=4,lty=1)
legend('bottomleft',
legend=c('Observations', 'Smoothed signal','Smoothed level'),
col=c(1,2,4), lty=c(1,2,1))

# Multivariate model with constant seasonal pattern,
# using the the seat belt law dummy only for the front seat passangers,
# and restricting the rank of the level component by using custom component
# note the small inconvinience in regression component,
# you must remove the intercept from the additional regression parts manually
model&lt;-SSModel(log(cbind(front,rear))~ -1 + log(PetrolPrice) + log(kms)
+ SSMregression(~-1+law,data=Seatbelts,index=1)
+ SSMcustom(Z=diag(2),T=diag(2),R=matrix(1,2,1),
Q=matrix(1),P1inf=diag(2))
+ SSMseasonal(period=12,sea.type='trigonometric'),
data=Seatbelts,H=matrix(NA,2,2))
likfn&lt;-function(pars,model,estimate=TRUE){
model$H[,,1]&lt;-exp(0.5*pars[1:2])
    model$H[1,2,1]&lt;-model$H[2,1,1]&lt;-tanh(pars[3])*prod(sqrt(exp(0.5*pars[1:2])))
    model$R[28:29]&lt;-exp(pars[4:5])
if(estimate) return(-logLik(model))
model
}
fit&lt;-optim(f=likfn,p=c(-7,-7,1,-1,-3),method='BFGS',model=model)
model&lt;-likfn(fit$p,model,estimate=FALSE)
    model$R[28:29,,1]%*%t(model$R[28:29,,1])
    model$H
out&lt;-KFS(model)
out
ts.plot(cbind(signal(out,states=c('custom','regression'))$signal,model$y),col=1:4)

# For confidence or prediction intervals, use predict on the original model
pred &lt;- predict(model,states=c('custom','regression'),interval='prediction')
ts.plot(pred$front,pred$rear,model$y,col=c(1,2,2,3,4,4,5,6),lty=c(1,2,2,1,2,2,1,1))

## End(Not run)
## Not run:
# Poisson model
model&lt;-SSModel(VanKilled~law+SSMtrend(1,Q=list(matrix(NA)))+
SSMseasonal(period=12,sea.type='dummy',Q=NA),
KFAS 15
data=Seatbelts, distribution='poisson')

# Estimate variance parameters
fit&lt;-fitSSM(inits=c(-4,-7,2), model=model,method='BFGS')
model&lt;-fit$model

# use approximating model, gives posterior mode of the signal and the linear predictor
out_nosim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=0)

# State smoothing via importance sampling
out_sim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=1000)
out_nosim
out_sim

## End(Not run)
# Example of generalized linear modelling with KFS
# Same example as in ?glm
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
print(d.AD &lt;- data.frame(treatment, outcome, counts))
glm.D93 &lt;- glm(counts ~ outcome + treatment, family = poisson())
model&lt;-SSModel(counts ~ outcome + treatment, data=d.AD,
distribution = 'poisson')
out&lt;-KFS(model)
coef(out,start=1,end=1)
coef(glm.D93)
summary(glm.D93)$cov.s
    out$V[,,1]
outnosim&lt;-KFS(model,smoothing=c('state','signal','mean'))
set.seed(1)
outsim&lt;-KFS(model,smoothing=c('state','signal','mean'),nsim=1000)

## linear
# GLM
glm.D93$linear.predictor

# approximate model, this is the posterior mode of p(theta|y)
c(outnosim$thetahat)

# importance sampling on theta, gives E(theta|y)
c(outsim$thetahat)

## predictions on response scale
16 KFAS

# GLM
fitted(glm.D93)

# approximate model with backtransform, equals GLM
c(fitted(outnosim))

# importance sampling on exp(theta)
fitted(outsim)

# prediction variances on link scale
# GLM
as.numeric(predict(glm.D93,type='link',se.fit=TRUE)$se.fit^2)

# approx, equals to GLM results
c(outnosim$V_theta)
    # importance sampling on theta
    c(outsim$V_theta)
# prediction variances on response scale
# GLM
as.numeric(predict(glm.D93,type='response',se.fit=TRUE)$se.fit^2)
    # approx, equals to GLM results
    c(outnosim$V_mu)
# importance sampling on theta
c(outsim$V_mu)
    ## Not run:
    data(sexratio)
    model&lt;-SSModel(Male~SSMtrend(1,Q=list(NA)),u=sexratio[,'Total'],data=sexratio,
    distribution='binomial')
    fit&lt;-fitSSM(model,inits=-15,method='BFGS',control=list(trace=1,REPORT=1))
    fit$model$Q #1.107652e-06

# Computing confidence intervals in response scale
# Uses importance sampling on response scale (4000 samples with antithetics)
pred&lt;-predict(fit$model,type='response',interval='conf',nsim=1000)
    ts.plot(cbind(model$y/model$u,pred),col=c(1,2,3,3),lty=c(1,1,2,2))

# Now with sex ratio instead of the probabilities:
imp&lt;-importanceSSM(fit$model,nsim=1000,antithetics=TRUE)
    sexratio.smooth&lt;-numeric(length(model$y))
sexratio.ci&lt;-matrix(0,length(model$y),2)
    w&lt;-imp$w/sum(imp$w)
    for(i in 1:length(model$y)){
sexr&lt;-exp(imp$sample[i,1,])
sexratio.smooth[i]&lt;-sum(sexr*w)
oo&lt;-order(sexr)
sexratio.ci[i,]&lt;-c(sexr[oo][which.min(abs(cumsum(w[oo]) - 0.05))],
+ sexr[oo][which.min(abs(cumsum(w[oo]) - 0.95))])
}

# Same by direct transformation:
out&lt;-KFS(fit$model,smoothing='signal',nsim=1000)
    KFS 17
    sexratio.smooth2 &lt;- exp(out$thetahat)
sexratio.ci2&lt;-exp(c(out$thetahat)
    + qnorm(0.025) * sqrt(drop(out$V_theta))%o%c(1, -1))
ts.plot(cbind(sexratio.smooth,sexratio.ci,sexratio.smooth2,sexratio.ci2),
col=c(1,1,1,2,2,2),lty=c(1,2,2,1,2,2))

## End(Not run)
# Example of Cubic spline smoothing
## Not run:
require(MASS)
data(mcycle)
model&lt;-SSModel(accel~-1+SSMcustom(Z=matrix(c(1,0),1,2),
T=array(diag(2),c(2,2,nrow(mcycle))),
Q=array(0,c(2,2,nrow(mcycle))),
P1inf=diag(2),P1=diag(0,2)),data=mcycle)
model$T[1,2,]&lt;-c(diff(mcycle$times),1)
model$Q[1,1,]&lt;-c(diff(mcycle$times),1)^3/3
model$Q[1,2,]&lt;-model$Q[2,1,]&lt;-c(diff(mcycle$times),1)^2/2
    model$Q[2,2,]&lt;-c(diff(mcycle$times),1)
    updatefn&lt;-function(pars,model,...){
    model$H[]&lt;-exp(pars[1])
    model$Q[]&lt;-model$Q[]*exp(pars[2])
    model
    }
    fit&lt;-fitSSM(model,inits=c(4,4),updatefn=updatefn,method=""BFGS"")
    pred&lt;-predict(fit$model,interval=""conf"",level=0.95)
plot(x=mcycle$times,y=mcycle$accel,pch=19)
lines(x=mcycle$times,y=pred[,1])
    lines(x=mcycle$times,y=pred[,2],lty=2)
lines(x=mcycle$times,y=pred[,3],lty=2)
## End(Not run)
</code></pre>

<p>The time-series data is:</p>

<pre><code>Time, 2.4, 2.6, 3.2, 3.6, 4, 6.2, 6.6, 6.8, 7.8, 8.2, 8.8, 8.8, 9.6, 10, 10.2, 10.6, 11, 11.4, 13.2, 13.6, 13.8, 14.6, 14.6, 14.6, 14.6, 14.6, 14.6, 14.8, 15.4, 15.4, 15.4, 15.4, 15.6, 15.6, 15.8, 15.8, 16, 16, 16.2, 16.2, 16.2, 16.4, 16.4, 16.6, 16.8, 16.8, 16.8, 17.6, 17.6, 17.6, 17.6, 17.8, 17.8, 18.6, 18.6, 19.2, 19.4, 19.4, 19.6, 20.2, 20.4, 21.2, 21.4, 21.8, 22, 23.2, 23.4, 24, 24.2, 24.2, 24.6, 25, 25, 25.4, 25.4, 25.6, 26, 26.2, 26.2, 26.4, 27, 27.2, 27.2, 27.2, 27.6, 28.2, 28.4, 28.4, 28.6, 29.4, 30.2, 31, 31.2, 32, 32, 32.8, 33.4, 33.8, 34.4, 34.8, 35.2, 35.2, 35.4, 35.6, 35.6, 36.2, 36.2, 38, 38, 39.2, 39.4, 40, 40.4, 41.6, 41.6, 42.4, 42.8, 42.8, 43, 44, 44.4, 45, 46.6, 47.8, 47.8, 48.8, 50.6, 52, 53.2, 55, 55, 55.4, 57.6                                                                                                
mcycle, 0, -1.3, -2.7, 0, -2.7, -2.7, -2.7, -1.3, -2.7, -2.7, -1.3, -2.7, -2.7, -2.7, -5.4, -2.7, -5.4, 0, -2.7, -2.7, 0, -13.3, -5.4, -5.4, -9.3, -16, -22.8, -2.7, -22.8, -32.1, -53.5, -54.9, -40.2, -21.5, -21.5, -50.8, -42.9, -26.8, -21.5, -50.8, -61.7, -5.4, -80.4, -59, -71, -91.1, -77.7, -37.5, -85.6, -123.1, -101.9, -99.1, -104.4, -112.5, -50.8, -123.1, -85.6, -72.3, -127.2, -123.1, -117.9, -134, -101.9, -108.4, -123.1, -123.1, -128.5, -112.5, -95.1, -81.8, -53.5, -64.4, -57.6, -72.3, -44.3, -26.8, -5.4, -107.1, -21.5, -65.6, -16, -45.6, -24.2, 9.5, 4, 12, -21.5, 37.5, 46.9, -17.4, 36.2, 75, 8.1, 54.9, 48.2, 46.9, 16, 45.6, 1.3, 75, -16, -54.9, 69.6, 34.8, 32.1, -37.5, 22.8, 46.9, 10.7, 5.4, -1.3, -21.5, -13.3, 30.8, -10.7, 29.4, 0, -10.7, 14.7, -1.3, 0, 10.7, 10.7, -26.8, -14.7, -13.3, 0, 10.7, -14.7, -2.7, 10.7, -2.7, 10.7
</code></pre>
"
"0.02831827358943","0.0277563690826684","132774","<p>I trying to conduct linear discriminant analysis using the lda package and I keep getting a warning message saying that the variables are collinear.</p>

<p>I want to pinpoint and remove the redundant variables. What is the best method for doing this in R?</p>

<p>I've read about solutions such as stepwise selection which can be used to do this but this doesn't work with discriminant analysis.</p>

<p>I tried lasso regression but this shrank my 66 variables down to just 12 - the optimal set and it's hard to identify the order in which it's done this as I would prefer to keep a larger number. </p>
"
"0.0633215847514023","0.0620651280774201","133387","<p>I'm trying to create a prediction model for estimation of continuous variable based on about 35 Independent variables.My data set has circa 27k observartions.
Here is the summary of the the targeted continuous variable:</p>

<pre><code>              Frequency Percent
(0,5]              2706  10.053
(5,10]             5226  19.415
(10,25]            4397  16.335
(25,100]           7142  26.533
(100,1e+03]        6465  24.018
(1e+03,1e+05]       981   3.645
Total             26917 100.000
</code></pre>

<p>I tried (by using R) Random Forest (RandomForest package),Linear regression, Conditional Inference Trees (ctree function in party package) but all of them have results that have a significant overestimation.
Here are the results of the prediction where I counted number of observations by thier distance from the actual values:
Any idea how can i balance the results?</p>

<p><img src=""http://i.stack.imgur.com/y70OM.png"" alt=""enter image description here""></p>

<p>Here are some views on the data:
The target variable is LTV for a user, I would like to predict LTV value after 180 days  based on users behavior of the first 7 days.
Here Is a summary fot the target variavle:</p>

<pre><code>  vars     n   mean     sd median trimmed   mad  min      max    range skew kurtosis   se
1    1 26917 178.35 622.29  33.49   66.63 39.28 0.03 22103.73 22103.71 14.1   325.08 3.79
</code></pre>

<p>UPDATE:
Here are the distributions of the targeted variable (first)and the prediction (secound)results:
<img src=""http://i.stack.imgur.com/b3MBs.png"" alt=""targeted variable"">
<img src=""http://i.stack.imgur.com/3N7d1.png"" alt=""prediction results based on the linear regression model that was the best""></p>
"
"0.116939835314396","0.120987207870969","133571","<p>I know there are already lots of questions around this topic (especially <a href=""http://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models/16415#16415"">this one</a> and <a href=""http://stats.stackexchange.com/questions/24689/interpreting-coefficients-of-ordinal-logistic-regression-when-there-is-clusterin/29701#29701"">this one</a>) but I haven't really seen anything that directly helps me (It will be obvious I'm not a great statistician, but I'll do my best to explain). </p>

<p>I am running an ordinal regression in R (<code>clm</code> and <code>clmm</code>). My response variable is a rating between 0 and 4. I have two types of explanatory variables: individual and scenario variables [let's say <code>IVs</code> and <code>SVs</code>]. </p>

<p>Six different scenario variables (all dummies with at most 4 different values) represent potential collaboration scenarios that get rated by the respondent (between 0 and 4) creating the response variable. (Research design is a conjoint analysis; there are a total of 192 different scenarios possible)</p>

<p>On top of that I have a variety of individual characteristics about the respondent (age, gender, work experience, networking skills, ...) all derived from a survey.</p>

<p>Every respondent rates between 3 and 16 different scenarios (average 8.1); every scenario is rated by at least 8 respondents. Every respondent and every scenario have a unique identifier (called <code>IVid</code> and <code>SVid</code>). So they are non nested within each other.</p>

<p>Thus the basic regression looks like this:</p>

<pre><code>clm.base &lt;- clm(rating ~ SVs + IVs, data = dt) 
</code></pre>

<p>The hypothesis I am trying to test is that there are specific individual characteristics, that will influence the rating of the scenarios, independent of the actual content of the scenarios. Basically, some people are more or less favourable to all types of collaboration scenarios. </p>

<p>Now a reviewer of my paper asks me to include individual fixed effects (which in management [my field] basically means dummies for each individual). My assumption originally was that this would result in all individual variables being dropped. This is exactly what happens when I use another model (package <code>lfe</code>)</p>

<pre><code>felm.complete &lt;- felm(rating ~ SVs + IVs | SVid + IVid | 0 | IVid, data = dt) 
</code></pre>

<p>In this regression basically all my variables are perfectly collinear as expected.
However, when I approximate this in the ordinal package, there is no perfect collinearity. I presume this is related that <code>clmm</code> adds so-called 'random effects'. The regression takes a couple of minutes to run but eventually returns results</p>

<pre><code>clmm.complete &lt;- clmm(rating ~ SVs + IVs + (1|SVid) + (1|IVid), data = dt)
</code></pre>

<p>Now, the results here are pretty useless:</p>

<ul>
<li>All but one of my IVs are insignificant</li>
</ul>

<p>I am trying to understand what exactly happens when adding the <code>(1|IVid)</code> term in the <code>clmm</code> model. If it basically adds something like an individual dummy than the fact almost everything is now insignificant is no surprise. The coefficients of the <code>IVid</code> dummies would capture the effect I am looking for (some people rate all scenarios higher or lower, regardless of scenario content) most accurately.</p>

<p>Now I wonder whether this interpretation is correct or whether the results I got from running the simple <code>clm</code> regression are just not reliable? </p>

<p>Concretely, I'd like to find out:</p>

<ul>
<li>What happens when adding a random effect to <code>clmm</code></li>
<li>A laymen explanation of how the Laplace approximation works</li>
<li>How to group errors around individuals when running <code>clm</code></li>
<li>Is it possible to extract the coefficients of these random effects <code>(1|id)</code> for as far as there is such a thing?</li>
</ul>
"
"0.02831827358943","0.0277563690826684","133979","<p>Consider that we have a problem with 4 variables (y, x1, x2 and x3) and we want to do a multiple linear regression model. As we need to know which variables are the most important in the problem, we look for it with a step selection as (it's just an example, we could also used back, both...) :</p>

<pre><code>g0 = lm(Y~1,data=dat)
gxf = formula(gx)
forward=step(g0,scope=gxf,direction=""forward"",test=""F"")
</code></pre>

<p>Suppose that this function says to us that our model should be y ~ ax1 + bx3. If we now do a summary to the object ""forward"" and we get this:</p>

<pre><code>Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.071923   0.150266   0.479    0.636    
X1           0.009716   0.001890   5.140 2.09e-05 ***
X3          -0.013497   0.009230  -1.462    0.155    
</code></pre>

<p>Do we should change our model to y ~ x1? Why isn't significative x3? And in case we change to only y ~ x1, if we do a lm(y~x3) and in a summary of this model now x3 is also significative, what model is better? The one that have a better r^2?</p>
"
"0.105957277556576","0.10385482340819","134499","<h2>Can anyone explain How I can interpret my result from the below model :</h2>

<p>I am trying to build the linear regression model for finding the transaction behavior of the customer in bank accounts. I have created the below model and done the nvcTest for the same. I need some one help to summary the result of this model.</p>

<pre><code>&gt; str(trans_data)
'data.frame':   8597 obs. of  20 variables:
 $ cust_id          : int  1 1 1 1 1 1 1 1 1 1 ...
 $ acc_type_id      : int  3 3 3 3 3 3 3 3 3 3 ...
 $ loc_id           : int  260 144 362 321 114 331 343 17 345 284 ...
 $ tx_type          : Factor w/ 2 levels ""CR"",""DB"": 1 1 1 2 1 1 1 2 2 2 ...
 $ total_bal        : num  8.36e+08 8.70e+08 9.69e+08 8.93e+08 9.60e+08 ...
 $ age              : int  30 30 30 30 30 30 30 30 30 30 ...
 $ gender           : Factor w/ 2 levels ""F"",""M"": 2 2 2 2 2 2 2 2 2 2 ...
 $ city             : chr  ""New Orleans"" ""New Orleans"" ""New Orleans"" ""New Orleans"" ...
 $ county           : chr  ""Orleans"" ""Orleans"" ""Orleans"" ""Orleans"" ...
 $ state            : chr  ""LA"" ""LA"" ""LA"" ""LA"" ...
 $ category         : Factor w/ 6 levels ""HNI"",""LNI"",""MNI"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ employment_status: Factor w/ 3 levels ""SAL"",""SEL"",""UNE"": 1 1 1 1 1 1 1 1 1 1 ...
 $ marital_status   : Factor w/ 2 levels ""MARRIED"",""UNMARRIED"": 1 1 1 1 1 1 1 1 1 1 ...
 $ branch_num       : int  1 1 1 1 1 1 1 1 1 1 ...
 $ acc_type         : Factor w/ 3 levels ""current"",""savings"",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ acc_sub_type     : Factor w/ 6 levels ""eqty"",""fd"",""gold"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ yyyy             : int  2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...
 $ mm               : int  1 1 1 1 1 1 1 1 1 1 ...
 $ dd               : int  3 4 8 9 11 12 13 17 21 22 ...
 $ tx_amt           : num  78626273 33171055 98937915 75726140 67162109 ...


model1&lt;-lm(formula = tx_amt ~ tx_type + total_bal +   age + gender + category + employment_status + marital_status + acc_type + 
acc_sub_type + yyyy + mm + dd , data=trans_data)

&gt; summary(model1)

Call:
lm(formula = tx_amt ~ tx_type + total_bal + age + gender + category + 
    employment_status + marital_status + acc_type + acc_sub_type + 
    yyyy + mm + dd, data = trans_data)

Residuals:
      Min        1Q    Median        3Q       Max 
-54030777 -24870155    230953  24933123  52907029 

Coefficients: (2 not defined because of singularities)
                          Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)              1.901e+09  1.161e+09   1.638   0.1015  
tx_typeDB               -6.713e+05  6.245e+05  -1.075   0.2824  
total_bal                1.118e-05  3.877e-04   0.029   0.9770  
age                     -7.315e+03  8.957e+04  -0.082   0.9349  
genderM                  2.784e+05  3.258e+06   0.085   0.9319  
categoryLNI             -1.648e+04  3.142e+06  -0.005   0.9958  
categoryMNI             -3.674e+05  2.095e+06  -0.175   0.8608  
categoryNNI              8.522e+05  5.419e+06   0.157   0.8751  
categoryXXX              2.184e+06  2.425e+06   0.901   0.3677  
categoryYYY             -1.949e+06  1.469e+06  -1.327   0.1847  
employment_statusSEL    -1.219e+06  2.755e+06  -0.443   0.6581  
employment_statusUNE     1.606e+06  1.551e+06   1.035   0.3006  
marital_statusUNMARRIED         NA         NA      NA       NA  
acc_typesavings         -1.882e+06  1.192e+06  -1.580   0.1143  
acc_typesecurity         1.909e+05  5.429e+06   0.035   0.9719  
acc_sub_typefd          -1.019e+07  1.288e+07  -0.791   0.4292  
acc_sub_typegold         9.310e+05  1.522e+06   0.612   0.5406  
acc_sub_typemutual_fnd  -1.245e+06  1.041e+06  -1.196   0.2317  
acc_sub_typenormal       1.446e+05  5.311e+06   0.027   0.9783  
acc_sub_typerd                  NA         NA      NA       NA  
yyyy                    -9.185e+05  5.763e+05  -1.594   0.1110  
mm                          -1.904e+05  8.972e+04  -2.123   0.0338 *
dd                       2.334e+04  3.547e+04   0.658   0.5105  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 28850000 on 8576 degrees of freedom
Multiple R-squared:  0.003322,  Adjusted R-squared:  0.0009977 
F-statistic: 1.429 on 20 and 8576 DF,  p-value: 0.09664

library(car)

durbinWatsonTest(model1)
 lag Autocorrelation D-W Statistic p-value
   1   -0.0008975802      2.001674   0.916
 Alternative hypothesis: rho != 0

ncvTest(model1)
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.132947    Df = 1     p = 0.7153958

model4&lt;-lm(formula = tx_amt ~ cust_id + acc_type_id + loc_id + tx_type + total_bal +   age + gender + category + employment_status + marital_status + acc_type + 
acc_sub_type + yyyy + mm + dd , data=trans_data)

 summary(model4)

Call:
lm(formula = tx_amt ~ cust_id + acc_type_id + loc_id + tx_type + 
    total_bal + age + gender + category + employment_status + 
    marital_status + acc_type + acc_sub_type + yyyy + mm + dd, 
    data = trans_data)

Residuals:
      Min        1Q    Median        3Q       Max 
-54109589 -24870576    214972  24958230  52899612 

Coefficients: (4 not defined because of singularities)
                          Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)              1.891e+09  1.161e+09   1.629   0.1033  
cust_id                  1.915e+05  1.822e+05   1.051   0.2933  
acc_type_id             -4.634e+04  2.656e+06  -0.017   0.9861  
loc_id                  -1.941e+03  2.173e+03  -0.893   0.3718  
tx_typeDB               -6.715e+05  6.245e+05  -1.075   0.2824  
total_bal                1.602e-05  3.877e-04   0.041   0.9670  
age                      9.159e+04  9.163e+04   1.000   0.3175  
genderM                 -4.463e+06  4.155e+06  -1.074   0.2828  
categoryLNI             -2.748e+06  3.170e+06  -0.867   0.3860  
categoryMNI             -2.091e+06  2.305e+06  -0.907   0.3644  
categoryNNI             -6.203e+06  6.973e+06  -0.890   0.3737  
categoryXXX             -1.115e+06  2.649e+06  -0.421   0.6738  
categoryYYY             -1.290e+06  1.242e+06  -1.039   0.2989  
employment_statusSEL    -4.898e+06  2.978e+06  -1.645   0.1001  
employment_statusUNE            NA         NA      NA       NA  
marital_statusUNMARRIED         NA         NA      NA       NA  
acc_typesavings         -1.708e+06  1.084e+07  -0.158   0.8747  
acc_typesecurity         1.317e+05  5.429e+06   0.024   0.9807  
acc_sub_typefd          -1.049e+07  1.209e+07  -0.868   0.3856  
acc_sub_typegold         9.841e+05  3.047e+06   0.323   0.7467  
acc_sub_typemutual_fnd  -1.282e+06  2.874e+06  -0.446   0.6557  
acc_sub_typenormal              NA         NA      NA       NA  
acc_sub_typerd                  NA         NA      NA       NA  
yyyy                    -9.124e+05  5.764e+05  -1.583   0.1134  
mm                      -1.905e+05  8.972e+04  -2.124   0.0337 *
dd                       2.302e+04  3.548e+04   0.649   0.5164  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 28850000 on 8575 degrees of freedom
Multiple R-squared:  0.003415,  Adjusted R-squared:  0.0009741 
F-statistic: 1.399 on 21 and 8575 DF,  p-value: 0.1055

ncvTest(model4)
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.08904766    Df = 1     p = 0.7653914
</code></pre>
"
"NaN","NaN","134885","<p>Since a feedforward NN with a logistic function as activation function is not linear, does it make sense to reduce variables first with principal components or discriminant analysis?</p>

<p>Because shouldn't be done this before training the NN as with logistic regression?</p>
"
"0.0490486886395286","0.0480754414848157","135011","<p>I'm having trouble finding a time series technique to deal with a data set I am working on. It contains multiple subjects and multiple variables, not all of which will likely be part of the time series. It looks something like this:</p>

<pre><code>Subject  Date      T1  T2  V1  V2  V3
A        1/1/2012  1   5   9   13  17
A        2/1/2012  2   6   10  14  18
...
B        1/1/2012  3   7   11  15  19
B        2/1/2012  4   8   12  16  20
...
</code></pre>

<p>Where T1, T2 are likely time series, and V1, V2, and V3 are likely not. I'm sure that this distinction is probably unnecessary, since techniques like Box-Jenkins should detect autoregression in any variable.</p>

<p>Ultimately, I want to be able to do forecasting on other subjects that were probably not used to build this model.</p>

<p>If you know of any R package(s) that can take this on, please let me know. Some example code would also be greatly appreciated. Thank you for any insight you can provide.</p>

<p>Edit: I am looking into dynamic linear regression using the <code>dynlm</code> package, but am having trouble coding it to include the dates and subjects.</p>
"
"0.0755153962384799","0.0832691072480053","135258","<p>I'm doing a linear regression using the h2o deep learning interface with R.  I'm comparing the predictions to the ones I'm getting from the randomForest R module.  The predictions from randomForest seem to roughly match up with what I'd expect given the distribution of the variable that I'm trying to predict.  However, the predictions from the h2o deep learning module don't seem to match up with what I'd expect.  Here's the summary from the variable I'm trying to predict:</p>

<pre><code>Profit            
Min.   :-1438.56  
1st Qu.: -133.80  
Median :   -0.59  
Mean   :   19.54  
3rd Qu.:  127.39  
Max.   :  508.41 
</code></pre>

<p>and here are the predictions from h2o deep learning:</p>

<pre><code>preddp             
Min.   :-0.079954  
1st Qu.:-0.017919  
Median :-0.010088  
Mean   :-0.011903  
3rd Qu.:-0.003921  
Max.   : 0.060259
</code></pre>

<p>and here are the predictions from randomForest:</p>

<pre><code>Min.       1st Qu.  Median   Mean     3rd Qu. Max. 
-212.500   -6.346   20.530   24.690   50.600  244.700
</code></pre>

<p>which is roughly what I'd expect.  Why are the h2o deep learning predictions so strange?  Also the correction between the predictions and the target variable are strongly positive with randomForest and even more strongly negative with h2o deep learning so I figure I have to be doing something wrong.</p>

<p>Here's the command I'm using to train the deep learning model:</p>

<pre><code>sdmodel.deep &lt;- h2o.deeplearning(columnIndices, 7, df, classification=FALSE)
</code></pre>
"
"0.0700841515030364","0.078506867197886","135613","<p>I'm using R to create a linear regression model from survey data about public sentiment for a new technology. I am encountering a problem where the addition of a new explanatory variable raises the model's $R^2$ value from 0.52 to precisely 1. This is absurd, but I'm new to this stuff and can't figure out what's going on.</p>

<p>The survey asks several questions about demographic and values and technical knowledge. These items become the explanatory variables in the model. Most are either dummy variables or likert scales that extend from 1 to 7 (meaning that for every such question, each respondent chooses a number between 1 and 7). The survey also asks respondents to what extent they'd support government investment in the new technology. That question becomes the dependent variable in the model. It is also a likert scale that extends from 1 to 7.</p>

<p>I'm using R's <code>lm()</code> function to regress the knowledge, demographics, and values variables against the support for new technology variable. The functional form is:</p>

<pre><code>lm(support~demographics+values+knowledge,data=survey). 
</code></pre>

<p>Out of about 2000 survey responses, 900 remain after NA's are discarded. I created a model comprising approximately 20 explanatory variables, with an $R^2$ value of 0.52. Then, I added in a 21st explanatory variable, and the $R^2$ jumped to 1. When I do a simple regression of only this new variable and the dependent variable, the $R^2$ is 0.67. What could be going on?</p>
"
"0.0693653206906364","0.0679889413649005","135621","<p>I want to do linear regression between vector inputs and vector output. That is each y is a vector with M components, and each x is a vector with N components and the answer should look like y ~ Ax + b where A is an M x N matrix and b is a vector with M components.</p>

<p>I have a very clear understanding of the concept and what I want R to do, but it is the proper syntax I am lacking.</p>

<p>Trying to google around to find this has been quite difficult because terms like multivariable seem to always point me to answers of the form </p>

<p>y ~ x1 + x2 + x3 + .. + xn</p>

<p>where there are multiple input sources (or rather, a multidimensional input), but never with multidimensional outputs.</p>

<p>If I just feed in matrices for y and x that MIGHT give what I want, but it might also just treat each y component as directly related to each x component and give an answer based on that (M = N for the important instance I have). So I have to be sure that I am doing it correctly.</p>

<p>What is the correct means for using R to do linear regression of the sort</p>

<p>y ~ A x + b </p>

<p>where the solution A is an M x N matrix, and b is a vector of length M, and each datum x is a vector of length N and each corresponding datum y is a vector of length M?</p>
"
"0.02831827358943","0.0277563690826684","135798","<p>I have some variables and I am interested to find non-linear relationships between them. So I decided to fit some spline or loess, and print nice plots (see the code below). But, I also want to have some statistics that gives me an idea how much likely is that the  relationship is a matter of randomness... i.e., I need some overall p-value, like I have for linear regression for example. In other words, I need to know whether the fitted curve makes any sense, since my code will fit a curve to any data.</p>

<pre><code>x &lt;- rnorm(1000)
y &lt;- sin(x) + rnorm(1000, 0, 0.5)

cor.test(x,y)
plot(x, y, xlab = xlab, ylab = ylab)
spl1 &lt;- smooth.spline(x, y, tol = 1e-6, df = 8)
lines(spl1, col = ""green"", lwd = 2)

spl2 &lt;- loess(y ~ x)
x.pr &lt;- seq(min(x), max(x), length.out = 100)
lines(x.pr, predict(spl2, x.pr), col = ""blue"", lwd = 2)
</code></pre>
"
"0.0749231094763201","0.0629455284778823","135847","<p>After you decompose a univariate time series with stl() function in R you are left with the trend, seasonal and random components of the time series. Is it valid to use those components to then model the original timer series with additional other variables?</p>

<p>For example:</p>

<pre><code>&gt; tsData
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2012  22  26  34  33  40  39  39  45  50  58  64  78
2013  51  60  80  80  93 100  96 108 111 119 140 164
2014 103 112 154 135 156 170 146 156 166 176 193 204

&gt; stl(tsData, s.window = ""periodic"")
 Call:
 stl(x = tsData, s.window = ""periodic"")

Components
            seasonal     trend   remainder
Jan 2012 -24.0219753  36.19189   9.8300831
Feb 2012 -20.2516062  37.82808   8.4235219
Mar 2012  -0.4812396  39.46428  -4.9830367
Apr 2012 -10.1034302  41.32047   1.7829612
...
Sep 2014   2.2193527 165.55136  -1.7707170
Oct 2014   7.3239448 169.33893  -0.6628760
Nov 2014  18.4285405 173.12650   1.4449614
Dec 2014  30.5244146 176.84390  -3.3683103
</code></pre>

<p>Now if I wanted to model the time series with a linear model with some other variables is it valid to do so?</p>

<pre><code>lm(index ~ trend + seasonal + s1 + s2, data)
</code></pre>

<p>When I run that model I get an R-squared = .98 which make sense considering that the original time series index is just the sum of trend + season + error. I guess what I'm concerned about is using a linear model with time series data I want to make sure I'm not violating some major rules of linear regression. I figure since I have the seasonal variable I'm essentially controlling for that element and hopefully reducing the auto correlation or am I since the R-squared is so high? Any help is appreciated!</p>
"
"0.0660759717086699","0.064764861192893","135852","<p>I'm trying to replicate an analysis done in Stata with R that involves calculating the autocorrelation for a particular outcome measured in many different areas. I've already run a linear regression on the data, which produced residuals for the outcome of interest. While I can't post my actual data, here is what it sort of looks like:</p>

<pre><code>year&lt;-c('2003', '2004', '2005','2003', '2004', '2005','2003', '2004', '2005')
location&lt;-c(rep('North', 3), rep('South',3), rep('West',3))
resid &lt;-c(-2.42, -3.563, -2.112, -0.543, 2.391, -1.556, -0.177, 0.983, 1.225)
mydata&lt;-data.frame(location,year, resid)
mydata

  location year resid
1    North 2003  -2.420
2    North 2004  -3.563
3    North 2005  -2.112
4    South 2003  -0.543
5    South 2004   2.391
6    South 2005  -1.556
7     West 2003  -0.177
8     West 2004   0.983
9     West 2005   1.225
</code></pre>

<p>I'm interested in the autocorrelation in the residuals, so I'm running the following:</p>

<pre><code> myacf &lt;-acf(mydata$resid, plot=F)
</code></pre>

<p>I'm not getting the same autocorrelation values as I got with Stata. I'm wondering if I should be specifying somehow that my data aren't a continuous time series from t=1 to t=9, but rather 3 sets of 3 time points that were measured in different locations (so really there can only be lag of 1 or 2, at least in the toy data above). </p>

<p>Also, above, I've made it so every region has 3 years of outcomes, but in my real (more complicated) dataset, some regions might have more rows than others (e.g., North might have 7 rows for 2000-2006, but South might only have 5 because we missed the outcome in 2003 and 2006 so there are no residuals for those 2 years, etc.). Any help would be appreciated in understanding whether acf() is the right way to approach this problem or not, and if so, what I might consider changing.</p>
"
"0.0490486886395286","0.0480754414848157","136055","<p>The deviance of a model is defined as:
$D =  -2(loglikelihood(model) - loglikelihood(saturated.model)) $</p>

<p>So I tried to compare the result of this formula with the output of the deviance() function with different models:</p>

<p>Linear regression model:</p>

<pre><code>&gt; ?cats
&gt; m1=lm(Hwt~Bwt+Sex,data=cats)
&gt; m1.saturated=lm(Hwt~factor(1:nrow(cats)),data=cats)
&gt; deviance(m1)
[1] 299
&gt; sum(residuals(m1)^2)
[1] 299
&gt; deviance(m1.saturated)
[1] 0
&gt; as.numeric(-2*(logLik(m1)-logLik(m1.saturated)))
[1] Inf
</code></pre>

<p>299 $\neq $ Inf<br>
Something is wrong</p>

<p>GLM: Poisson regression</p>

<pre><code>&gt; x = rnorm(10)
&gt; y = rpois(10,lam=exp(1 + 2*x))
&gt; m2 = glm(formula = y ~ x, family = poisson)
&gt; m2.saturated &lt;- glm(y ~ factor(1:10),family=poisson)
&gt; deviance(m2) 
[1] 14
&gt; deviance(m2.saturated) 
[1] 4.5e-10
&gt; as.numeric(-2*(logLik(m2)-logLik(m2.saturated))) 
[1] 14
</code></pre>

<p>14 = 14<br>
Ok, it works!</p>

<p>GLM: Gamma regression</p>

<pre><code> &gt; cement &lt;- read.table(""cement.dat"", col.names = c(""time"",""resistance""), dec = "","") 
 &gt; attach(cement)
 &gt; m3 &lt;- glm(resistance ~ I(1/time), family = Gamma)
 &gt; m3.saturated &lt;- glm(resistance ~ factor(1:nrow(cement)), family = Gamma)
 &gt; deviance(m3)
 [1] 0.16
 &gt; deviance(m3.saturated)
 [1] 9.3e-17
 &gt; as.numeric(-2*(logLik(m3)-logLik(m3.saturated)))
 [1] 758
</code></pre>

<p>0.16 $\neq $ 758<br>
There must be a mistake somewhere</p>

<p>Shouldn't the results be the same?
Thank you.</p>
"
"0.02831827358943","0.0277563690826684","136071","<p>I am hoping someone can check this code to ensure that I have interpreted the various pieces of PCA correctly. I am trying to figure out a way to identify the leading contributors to the performance of multiple securities. For example, one idea I had was to run a multivariate regression using the securities returns as dependent variables and include things like oil, the dollar, the euro, treasury yields, etc.
E.g.,
SBUX + AAPL + MCD + BAC + TWTR = intercept + oil + dollar + euro + steel + gold + e</p>

<p>I then thought that PCA would probably be better suited for this type of exercise. Here is my code from R. The csv file consists of a matrix of 900 securities and 30 rows (30 daily returns for 900 securities)</p>

<pre><code>FD &lt;- read.csv(""U:/Personal Projects/R/Data Files/FD Securities Jan 2015.csv"")

#Removes columns with any na values
FD1 &lt;- FD[, sapply(FD, function(x) !any(is.na(x)))]

#removes ""zero/constant variance"" columns, which I think are NaNs I couldnt erase using is.nan
FD2 &lt;- FD1[,apply(FD1, 2, var, na.rm=TRUE) != 0]

#Calc PCs using singe value decomposition (prcomp). Should data be a correlation matrix of the variables? I get reasonable looking results both ways, i.e., PC1 explains 30-50% of variance, PC2 ~10%-15%, etc.
FD2.pca &lt;- prcomp(cor(FD2), retx = TRUE, scale = TRUE)
summary(FD2.pca)
plot(FD2.pca)

#These are the 'loadings', i.e., coefficients used for each linear combination?
as.matrix(FD2.pca$rotation[,1])

#I think these ""scores"" are the coefficients of interest, as they incorporate the factor     weightings because the output is pca$rotation * scale (stddev of each factor)
as.matrix(FD2.pca$x[,1]) as.matrix(FD2.pca$x[,2]) as.matrix(FD2.pca$x[,3])     as.matrix(FD2.pca$x[,4])

#Scatterplot of the first two principal components. Not sure if this is right of if $x should be used.
plot(x = FD2.pca$rotation[,1], FD2.pca$rotation[,2], xlab = ""PC1"", ylab = ""PC2"", main=""Principal Component Analysis:"")
</code></pre>
"
"0.02831827358943","0.0277563690826684","136138","<p>I need some help getting pointed in the right direction for creating a regression model in R with data that looks like this.</p>

<p>This is my first foray into this. So using Excel's trend line equation as my reference, I was able to create a logarithmic trend line for another set of data which matched between the two applications. </p>

<p>However, with this specific example, I'm not sure how to formulate the model or even if I should be using non-linear vs linear regression with transformation. Below is an example of the data in the plot.</p>

<pre><code>x = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)
y = c(0.008,0.004,0.0025,0.0024,0.0023,0.0022,0.0021,0.002,0.0018,0.0005,0.012,0.006,
     0.00375,0.0036,0.00345,0.0033,0.00315,0.003,0.0027,0.00075)
z = c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2)

df = data.frame(x, y, z)

plot(df$y ~ df$x, type=""p"", pch=20, col=df$z)
</code></pre>

<p><img src=""http://i.stack.imgur.com/467Co.png"" alt=""Sample Data""></p>
"
"0.0633215847514023","0.0620651280774201","136193","<p>I ran a linear regression of acceptance into college against SAT scores and family / ethnic background. The data are fictional. This is a follow-up on a prior question, already answered. The question focuses in the gathering and interpretation of odds ratios when leaving the SAT scores aside for simplicity. </p>

<p>The variables are <code>Accepted</code> (0 or 1) and <code>Background</code> (""red"" or ""blue""). I set up the data so that people of ""red"" background were more likely to get in:</p>

<pre><code>fit &lt;- glm(Accepted~Background, data=dat, family=""binomial"")
exp(cbind(Odds_Ratio_RedvBlue=coef(fit), confint(fit)))

                        Odds_Ratio_RedvBlue             2.5 %       97.5 %
(Intercept)             0.7088608                     0.5553459   0.9017961
Backgroundred           2.4480042                     1.7397640   3.4595454
</code></pre>

<p>Questions:</p>

<ol>
<li><p>Is 0.7 the odd ratio of a person of ""blue"" background being accepted? I'm asking this because I also get 0.7 for ""<code>Backgroundblue</code>"" if instead I run the following code:</p>

<pre><code>fit &lt;- glm(Accepted~Background-1, data=dat, family=""binomial"")
exp(cbind(OR=coef(fit), confint(fit)))
</code></pre></li>
<li><p>Shouldn't the odds ratio of ""red"" being accepted ($\rm Accepted/Red:Accepted/Blue$) just the reciprocal: ($\rm OddsBlue = 1 / OddsRed$)?</p></li>
</ol>
"
"0.0400480865731637","0.039253433598943","136563","<p>I am using the <code>nnls()</code> function from the <code>nnls package</code> in R to do a linear regression for regressors $x_i$ and observations $y$.
The function delivers beta coefficients $\beta_i\geq{0}, \forall i$. However, is it possible to apply the constraints only to <strong>some</strong> regressors so that</p>

<p>$$\beta_k \geq 0 \quad k \in \{1...,10\}, k\neq i \\ 
\beta_i \in \mathbb{R} \quad i \in \{1...,10\}$$</p>

<p>given that I have 10 regressor variables?</p>

<p><code>nnls()</code> offers the possibility to enforce some coefficients to be negative and others to be positive. I only want the positive constraint for some of them, the other ones can be either positive or negative.</p>
"
"0.0490486886395286","0.0480754414848157","136619","<p>I want to create a linear regression model to predict an output that uses two different coefficients based on some threshold within the data. For example: <code>df</code>:</p>

<pre><code>Value   Temperature
8.2     70
3.2     51
5.8     54
7.2     61
</code></pre>

<p>and so on. For this data, I want to figure out how to make the following model:</p>

<pre><code>Value = B0 + B1(HighTemp) + B2(LowTemp)
</code></pre>

<p>Where B1 is 0 if the temperature is below 55, and B2 is 0 is the temperature is above 55. I tried the following:  </p>

<pre><code>fit  = lm(Value ~ I(Temperature&gt;55), data=df) 
fit2 = lm(Value ~ Temperature * I(Temperature&gt;55), data=df) 
</code></pre>

<p><code>fit</code> only gives me a coefficient for when the temperature is above 55, and <code>fit2</code> gives output that I don't fully understand. I was also thinking of creating a third column, <code>HighorLow</code>, with an indicator variable (1 or 0) for whether or not the temperature is high or low. The I would have:</p>

<pre><code>fit = lm(Value~Temperature:HighorLow, data = df)
</code></pre>

<p>Does anyone have any input? </p>
"
"0.116759233144359","0.11444244151147","136843","<p>I have a Cox proportional hazards model in R (see made-up example below) that models the effect of some variable, say weight. From this model, I'd like to extrapolate what a change in weight from say 90 to 60 would mean to survival, taking into account the fact that for such a change occurring at say age 40, certain amount of risk has already accumulated (and assuming weight change is instantaneous).</p>

<p>I've attached some code which involves</p>

<ol>
<li>fitting the Cox model (using age as the time scale);</li>
<li>extracting the predicted cumulative survival $S(t)$ using survfit for weight=90 and 60;</li>
<li>getting the cumulative hazard $H(t) = -\log(S(t))$;</li>
<li>getting the ""instantaneous"" hazard $h(t)$ via differencing $H(t)$ (plus small fudge factor to avoid zero hazard), which seems to do the job but probably a bit hacky;</li>
<li>adding a constant to the $\log(h(t))$ for all timepoints after the change, equivalent to the $\beta$ coefficient from the Cox regression times the <em>difference</em> in weights (90-60=30);</li>
<li>get the new survival functions $S^\prime(t)$ as $\exp(-{\rm cumsum}(\exp(\log(h^\prime(t)))))$.</li>
</ol>

<p>This procedure produces reasonable results (plotted as $1 - S(t)$), but is it correct or am I just lucky?</p>

<p><img src=""http://i.stack.imgur.com/ax5Bu.png"" alt=""enter image description here""></p>

<pre><code>library(survival)
set.seed(1)
rm(list=ls())

# Simulate some semi-realistic data
n      &lt;- 1e3
age    &lt;- round(runif(n, 1, 60))
weight &lt;- round(rnorm(n, 70, 10))
height &lt;- round(runif(n, 1.3, 1.9), 2)
sex    &lt;- sample(c(""M"", ""F""), length(age), replace=TRUE, prob=c(0.7, 0.3))
d.time &lt;- ceiling(rexp(n, weight / 1e4))
cens   &lt;- round(runif(n, 1, 60))
death  &lt;- d.time &lt;= cens
d.time &lt;- pmin(d.time, cens)
d      &lt;- data.frame(age=age, weight=weight, height=height, difftime=d.time, 
                     time=d.time + age, sex=sex, death=death)

s     &lt;- coxph(Surv(age, time, death) ~ height + weight, data=d)
d.new &lt;- data.frame(weight=c(60, 90), height=1.7)
sf    &lt;- survfit(s, d.new)

# The cumulative hazard function H is -log(S(t)) where S(t) is the survivor function
# (aka cumulative survival)
S &lt;- sf$surv[,2]

# Assume we start off with high weight
H &lt;- -log(S)

# The hazard is the derivative (here, finite difference) of the cumulative hazard H
# But the hazard can't be zero exactly as when we take log hazard, won't make sense
h &lt;- diff(c(0, H)) + 1e-6

# We introduce a changepoint in the hazard, but must make sure that the
# hazard does not become negative - this is naturally achieved because the
# Cox model is linear in the log-hazard. This means that the final survivor
# function will always be monotonically decreasing for any value of delta in 
# (-Inf, +Inf); delta &gt; 0 increases hazard, delta &lt; 0 decreases hazard
delta &lt;- coef(s)[""weight""] * (d.new$weight[1] - d.new$weight[2])
logh  &lt;- log(h)
age   &lt;- 40
logh[sf$time &gt; age] &lt;- logh[sf$time &gt; age] + delta
h     &lt;- exp(logh)

# Get the new cumulative hazard and new survivor functions
H &lt;- cumsum(h)
S &lt;- exp(-H)

# Compare original survivor function with modified one
plot(sf, lwd=5, col=1:2, conf.int=FALSE, mark=NA, fun=""event"",
     xlab=""Age"", ylab=""Cumulative risk"")
lines(c(0, sf$time), 1 - c(1, S), type=""s"", col=3, lwd=5)
abline(v=age, lty=2)
legend(x=""topleft"", legend=c(""Weight=60"", ""Weight=90"", ""Weight decreased 90 to 60""),
       col=1:3, lwd=5)
</code></pre>
"
"0.0755153962384799","0.0832691072480053","136925","<p>I have some short grouped time series data. I would like to fit a dynamic multilevel regression model in R, with random coefficients for the mean and first order auto-correlation in each group, and with no cross correlation between the two variance parameters; i.e. this model:</p>

<p>$y_{i,t} - \mu_{i} = \rho_{i} (y_{i,t-1} - \mu_{i}) + \epsilon_{i,t}$</p>

<p>$\mu_{i} = \mu + v_{\mu}$ </p>

<p>$\rho_{i} = \rho + v_{\rho}$ </p>

<p>$\epsilon_{i,t} \sim N(0,\sigma_\epsilon^2), v_{\mu} \sim N(0,\sigma_\mu^2), v_{\rho} \sim N(0,\sigma_\rho^2)$ </p>

<p>The best I can do so far in R is:</p>

<pre><code>library(nlme)
library(dplyr)

#create toy data set
df0 &lt;- Orthodont %&gt;% 
  group_by(Subject) %&gt;% 
  mutate(lag1=lag(distance)) %&gt;% 
  filter(!is.na(lag1))

#multilevel model, mean (not Subject specific mean) centered
m1 &lt;- lme(fixed = distance ~ I(lag1 - mean(distance)), data=df0, 
          random= list(Subject = pdDiag(~ + I(lag1 - mean(distance)))) )
m1
# Linear mixed-effects model fit by REML
#   Data: df0 
#   Log-restricted-likelihood: -164.8976
#   Fixed: distance ~ I(lag1 - mean(distance)) 
#              (Intercept) I(lag1 - mean(distance)) 
#               25.7907622                0.8289975 
# 
# Random effects:
#  Formula: ~+I(lag1 - mean(distance)) | Subject
#  Structure: Diagonal
#          (Intercept) I(lag1 - mean(distance)) Residual
# StdDev: 7.892818e-05                0.3031237 1.675277
# 
# Number of Observations: 81
# Number of Groups: 27 
</code></pre>

<p>This 1) does not estimate ($\mu$) and 2) centres the distance lag ($y_{i,t-1}$) on the population mean ($\mu$) rather than the subject ($i$) specific mean ($\mu_i$) that I desire. </p>
"
"0.102102987459307","0.100077011948264","137120","<p>I am using a software in my analysis, from which I obtained the $R^2$ and estimated effects ($\beta$) of a linear regression model. In addition, it outputs the design matrix ($X$) of the model. To understand how $R^2$ was calculated in this software, I tried to calculate it in R with the design matrix and estimated effects. </p>

<p>Full model: y = cross + x</p>

<p>Reduced model: y = cross</p>

<p>The formula for calculating $R^2$ is 1-RSS_full/RSS_red, where RSS_full refers to the residual sum of squares of the full model and RSS_red is the residual sum of squares of the reduced model. I calculated the RSS_full with the residuals obtained from lm function in R. For RSS_red, I tried two methods to get the residuals and then RSS_red. </p>

<p>Method 1: apply lm function in R, which is sum(lm(y~cross-1,tol=1e-4)$residuals^2)</p>

<p>Method 2: firstly calculate the fitted value of the reduced model (y_red) with its design matrix and the estimated effects from the full model obtained from the software (y_red = cross%*%cross_effects), and then get the RSS_red by sum((y-y_red)^2).</p>

<p>The results of the two methods are different. However, the result of method 1 is the same as the one obtained from the Software. I do not know why they differ. Could someone give me some suggestions? Thank you! The design matrix, R codes and the output from the sotware are as follows.</p>

<p>Design matrix (ModelSJ file):</p>

<p>It can be downloaded from the dropbox share link: <a href=""https://www.dropbox.com/s/hd3po8g8ixs2wod/ModelSJ?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/hd3po8g8ixs2wod/ModelSJ?dl=0</a></p>

<p>R codes for calculating $R^2$:</p>

<pre><code>data=as.matrix(read.table(file=""https://dl.dropboxusercontent.com/u/24381951/ModelSJ"")) # reads the file from dropbox
y=data[,1] # the first column is the response/observed values.
x=data[,-1] # design matrix of the full model
cross=x[,10:12] # design matrix of the reduced model
RSS_full &lt;- sum(lm(y~x-1,tol=1e-4)$residuals^2) # RSS of the full model

# Method 1
RSS_red &lt;- sum(lm(y~cross-1,tol=1e-4)$residuals^2)
R2_1 &lt;- 1-RSS_full/RSS_red 

# Method 2
cross_effects &lt;- c(3.9171, 4.4411, 3.6381) # the estimated effects ÃŸ obtained 
#from the software based on the full model

y_red &lt;- cross%*%cross_effects # fitted value
RSS_red &lt;- sum((y-y_red)^2)
R2_2 &lt;- 1-RSS_full/RSS_red

# Why are R2_1 and R2_2 not equal?
</code></pre>

<p>Output from the software:</p>

<p>it can be downloaded from <a href=""https://www.dropbox.com/s/8hp8p3kcf0d2nc5/testSJ_DON_iQTLm.xml?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/8hp8p3kcf0d2nc5/testSJ_DON_iQTLm.xml?dl=0</a></p>
"
"0.0800961731463273","0.078506867197886","137209","<p>When I run regression analysis I find it important to run some model diagnostics, such as detection of outliers, influential observations, multi-collinearity (much like these examples <a href=""http://www.statmethods.net/stats/rdiagnostics.html"" rel=""nofollow"">http://www.statmethods.net/stats/rdiagnostics.html</a>).</p>

<p>Example of Diagnostics I use:</p>

<pre><code>#Assessing the Assumption of Independence, using Durbin Watson Test
dwt(lmModel)

#Controlling for Multicollinearity
vif(lmModel)
1/vif(lmModel)
mean(vif(lmModel))
</code></pre>

<p>I have a sample with a lot of missing data across most variables. Thus, I need to use multiple imputations. </p>

<p>However, model diagnostics seems to be impossible to explore when using multiple imputations. So far, I have used the mice package and since I am still a novice at R my multiple imputation script basically looks like this:</p>

<pre><code>#Imputes 5 datasets    
imp &lt;- mice(myData, m=5)    

#Runs regression analysis on each imputed dataset    
fit &lt;- with(imp, lm(A ~ B + C))   

#Pools the results
pooled &lt;- pool(fit)
summary(pooled)
</code></pre>

<p>Is there some way to use the diagnostic test on the pooled data? or do I have to use diagnostic tests on each imputed dataset (before being pooled)? or is there some other smart way of solving this issue?</p>

<p>Thanks for your time</p>
"
"0.0693653206906364","0.0679889413649005","137424","<p>I am fitting a binomial logistic regression in R using glm. By chance, I have found out that if I change the order of my predictor variables, glm fails to estimate the model. The message I get is  <em>unexpected result from lpSolveAPI for primal test</em>. </p>

<p>I am using the safeBinaryRegression package, so I am confident there are no separation issues between my outcome and predictor variables. However, I am not so confident that there are no quasi-separation issues among my predictor variables. Am I correct that if this is the case, then I might be running into multicolinearity, and this is the source of glm not being able to fit the model? </p>

<p>If so, my question is for advice on how to approach the issue. Should I look for the predictor variables highly correlated and omit one of them? Is there any convenient way of doing so for 11 categorical predictors? </p>

<p>What I see right now: </p>

<pre><code>lModel &lt;- glm(mob_change ~ education + gender + start_age + income + dist_change + lu_change + dou_change + marriage + student2work + wh_change,
              data = regression_data, 
              family = binomial())
# Fine, and I can inspect the model. No predictor has std. error &gt; 1.05

# Now if I move the last variable (or any of the last three, for what I've tested) to
# be the first in predictor... 
lModel.3 &lt;- glm(mob_change ~ wh_change + gender + education + start_age + income + dist_change + lu_change + dou_change + marriage + student2work,
            data = regression_data, 
            family = binomial())

Error in separator(X, Y, purpose = ""find"") : 
  unexpected result from lpSolveAPI for primal test
</code></pre>
"
"0.0578044339088637","0.0566574511374171","137498","<p>I implemented both those tests with R, using the lmtest package.  Both tests directionally say the same thing (I think) with a very similar p-value of very close to 0.  But, are those tests saying that the underlying regression model's residuals are adequately linear.  Or are they saying just the opposite.  I know that the tests have slightly different nuances.  The Harvey-Collier test indicates whether the residuals are linear.  Meanwhile, the Rainbow test indicates whether the linear fit of the model is adequate even if some underlying relationships are not linear.  Any insight, on the interpretation of those results is greatly appreciated.   </p>

<p>I am posting the results of the tests below:</p>

<p>In R with lmtest package.</p>

<blockquote>
  <p>harvtest(Regression, order.by = NULL)</p>
</blockquote>

<pre><code>    Harvey-Collier test
</code></pre>

<p>data:  Regression
HC = 4.3826, df = 119, p-value = 2.543e-05</p>

<blockquote>
  <p>raintest(Regression, fraction = 0.5, order.by = NULL, center = NULL)</p>
</blockquote>

<pre><code>    Rainbow test
</code></pre>

<p>data:  Regression
Rain = 1.7475, df1 = 62, df2 = 58, p-value = 0.01664</p>
"
"0.0566365471788599","0.0416345536240027","137650","<p>Linear Regression has the below set of Assumptions,</p>

<ol>
<li>The Y-Values (or the errors, ""e"") are independent!</li>
<li>The Y-Values can be expressed as a linear function of the X variable.</li>
<li>Variation of observations around the regression line (the residual SE) is constant (homoscedasticity).</li>
<li>For given value of X, Y values (or the error) are Normally distributed.</li>
</ol>

<p>Is there any empirical test instead of visual test in R that can be used to validate the Assumptions in 1, 2 and 4?</p>

<p>I can only find for Assumption 3,
Empirical Test: ncvTest() from CAR package
Value to be observed: p-value
Pass criteria: > 0.05</p>

<p>This is to make an automated script that can assist to choose the best model for a set non linear data using linear regression.</p>

<p>Do assist to point to any books or website if this has been discussed previously as my search has been futile. I find many approaches are visual based then empirical.</p>
"
"0.0700841515030364","0.078506867197886","137673","<p>This is the data set I have:</p>

<pre><code>vector &lt;- c( -7.459981, 13.26651, 12.10128, 2.380662, 26.42393)
</code></pre>

<p>Doing an estimation of the coefficient with a linear regression in Java and R I got different results. This is my code in Java.</p>

<pre><code>List&lt;Double&gt; y= new ArrayList&lt;&gt;(Arrays.asList(
        -7.4599812, 13.2665113, 12.1012781, 2.3806622, 26.4239262
        ));
SimpleRegression ls = new SimpleRegression();
List&lt;Double&gt; yNew = new ArrayList&lt;Double&gt;();
for (int i = 0; i &lt; y.size(); i++) {
    ls.addData(i, y.get(i));
}

double alpha0 = ls.getIntercept();
double beta0 = ls.getSlope();
double coefficients[] = { alpha0, beta0 };
</code></pre>

<p>This is the output result:</p>

<pre><code>alpha0 = -2.0339138199999995
beta0  =  5.68819657
</code></pre>

<p>In R it seems to be that we should start for x with 1 instead of 0:</p>

<blockquote>
  <p>lsfit(x, y, wt = NULL, intercept = TRUE, tolerance = 1e-07, yname =
  NULL)</p>
</blockquote>

<p>Starting with 1 instead of 0 in R, I get:</p>

<pre><code>vector &lt;- c( -7.459981, 13.26651, 12.10128, 2.380662, 26.42393)
vecx &lt;- c(1,2,3,4,5)
lsfit(vecx, vector)
alpha = -7.72211 
beta = 5.688197
</code></pre>

<p>When I start with 0 in R, I get: </p>

<pre><code>vector &lt;- c( -7.459981, 13.26651, 12.10128, 2.380662, 26.42393)
vecx &lt;- c(0,1,2,3,4)
lsfit(vecx, vector)
alpha = -2.033915  
beta = 5.688197
</code></pre>

<p>which is the same value as my code in Java. On the internet I have found that the values of my Java code and with vecx &lt;- c(0,1,2,3,4) in R are the values which are correct but I am not sure. </p>

<p><strong>My question is: What is the truth? To start with 0 or with 1 to estimate the coefficients?</strong> </p>

<p>Thanks for your answer. </p>
"
"0.0716401951571125","0.0702186767020086","138276","<p>I conducted some experiments where I modified two variables (var 1 and var 2), and measured an outcome variable (yield). For each combination of var 1 and var 2, I assigned a TestNumber. Some combinations of var 1 and var 2 were tested with a single individual (such as TestNumber 9 in the figure), some were tested with two individuals (such as TestNumber 6 in the figure), and others were tested with three individuals (such as TestNumber 1 in the figure).</p>

<p>I'd like to determine if there is any systematic difference between the three individuals (orange, purple, and black), so that I can know if I can pool their results together for downstream analyses. But I want to take into account the fact that some TestNumbers generally performed better than others.</p>

<p>I've done linear regression, trying to predict the yield using the individual as the predictor variable:</p>

<pre><code>Call:
lm(formula = exampleData$Yield ~ exampleData$individual)
 Residuals:
     Min       1Q   Median       3Q      Max 
 -0.56544 -0.19684  0.03037  0.23367  0.39646 

Coefficients:
                              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                   0.58854    0.05335  11.031   &lt;2e-16 ***
exampleData$individualorange  0.11139    0.07627   1.460    0.149    
exampleData$individualpurple  0.08079    0.07399   1.092    0.279    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2614 on 70 degrees of freedom
Multiple R-squared:  0.03184,   Adjusted R-squared:  0.004178 
F-statistic: 1.151 on 2 and 70 DF,  p-value: 0.3222
</code></pre>

<p>However, I don't think that's good enough, because it ignores the information from var1 and var2--one of the individuals might be present at a higher frequency in the higher TestNumbers, which have a higher yield.</p>

<p>If I try <code>summary(lm(exampleData$Yield ~ exampleData$var1 + exampleData$var2 + exampleData$individual))</code> then I also get a large p-value for the slope on exampleData$individual, but is that sufficient to determine that the three individuals are not distinguishable?</p>

<p>Any pointing in the right direction would be much appreciated. Thanks very much!</p>

<p><img src=""http://i.stack.imgur.com/6USl8.png"" alt=""enter image description here""></p>

<p><strong>Data</strong></p>

<pre><code>individual condition var1 var2 Yield
black      1         10   15   0.2131
purple     1         10   15   0.3189
orange     1         10   15   0.9993
purple     2         12   15   0.1667
orange     2         12   15   0.2423
black      2         12   15   0.1462
purple     3         13   15   0.1876
black      3         13   15   0.2297
black      4         18   15   0.8693
purple     4         18   15   0.6991
orange     5         25   15   0.804
black      5         25   15   0.2084
purple     5         25   15   0.9396
black      6         30   15   0.0231
purple     6         30   15   0.5337
orange     7         10   45   0.4939
orange     8         12   45   0.446
orange     9         13   45   0.7962
black     10         15   45   0.6812
purple    10         15   45   0.5147
orange    10         15   45   0.439
black     11         17   45   0.2627
purple    11         17   45   0.4074
orange    11         17   45   0.9948
black     12         20   45   0.9716
purple    12         20   45   0.8603
orange    12         20   45   0.9434
black     13         26   45   0.6757
purple    13         26   45   0.8618
black     14         28   45   0.3611
purple    14         28   45   0.7874
orange    14         28   45   0.2899
black     15         30   45   0.5435
purple    15         30   45   0.4882
black     16         34   45   0.5456
purple    16         34   45   0.491
orange    16         34   45   0.619
purple    17         45   45   0.698
black     17         45   45   0.3917
orange    17         45   45   0.9568
black     18         50   45   0.9163
purple    18         50   45   0.3317
purple    19         55   45   0.8703
orange    20         56   45   0.4262
black     20         56   45   0.4751
black     21         59   45   0.6788
purple    21         59   45   0.9172
orange    21         59   45   0.5628
orange    22         65   45   0.9585
purple    22         65   45   0.9056
black     22         65   45   0.9022
orange    23         10   90   0.8286
purple    23         10   90   0.703
black     23         10   90   0.8413
orange    24         15   90   0.9259
purple    24         15   90   0.903
black     24         15   90   0.5991
orange    25         20   90   0.7303
purple    25         20   90   0.5813
black     26         21   90   0.9131
orange    26         21   90   0.7542
purple    26         21   90   0.9896
orange    27         28   90   0.5668
purple    27         28   90   0.8928
purple    28         30   90   0.8766
orange    29         35   90   0.6794
black     30         41   90   0.8845
black     31         45   90   0.8067
orange    31         45   90   0.6489
purple    32         46   90   0.6454
black     32         48   90   0.985
orange    32         48   90   0.9922
purple    33         50   90   0.8317
</code></pre>
"
"0.109676202005208","0.107499955208361","138424","<p>My data is binary with two linear independent variables.  For both predictors, as they get bigger, there are more positive responses.  I have plotted the data in a heatplot showing density of positive responses along the two variables.  There are the most positive responses in the top right corner and negative responses in the bottom left, with a gradient change visible along both axes.</p>

<p>I would like to plot a line on the heatplot showing where a logistic regression model predicts that positive and negative responses are equally likely.  (My model is of the form <code>response~predictor1*predictor2+(1|participant)</code>.)</p>

<p>My question: How can I figure out the line based on this model at which the positive response rate is 0.5?</p>

<p>I tried using predict(), but that works the opposite way; I have to give it values for the factor rather than giving the response rate I want.  I also tried using a function that I used before when I had only one predictor (<code>function(x) ((log(x/(1-x)))-fixef(fit)[1])/fixef(fit)[2]</code>), but I can only get single values out of that, not a line, and I can only get values for one predictor at a time.</p>

<p>I am using R.</p>

<p>Edit: I have added a contour plot over the heat plot (using geom_contour in ggplot2), which produces this:</p>

<p><img src=""http://i.stack.imgur.com/qObZc.png"" alt=""Each cell represents the frequency of positive responses for a single stimulus.  I added the numbers for clarity.""></p>

<p>I'd like to have a line that actually predicts the cutoff point in a fine-grained way; right now for the independent variables I have stimuli at points 40, 45, 50, etc. but I would like to see a line that predicts, e.g., that when x=32 and y=36 that's the threshold for 50% positive responses.  It could be a curve or it could even be a straight line (whose slope might help visualise the relative contributions of the two factors), but I'm not looking for a pure description of the cells which are >50 vs &lt;50, which is what I think this is doing, I'm looking for a way to plot the regression's predictions.</p>
"
"0.0983889005882491","0.10385482340819","138506","<p>I have created regression models using robust regression - in particular, LTS and MM-estimators (using the R package robustbase).  I am now looking to creation prediction intervals.</p>

<p>The standard formula for prediction intervals for linear regression is:
$$
\hat{y_0} \pm t_{\alpha/2, n-p} \sqrt{\hat{\sigma^2}(1+x_0'(X'X)^{-1}x_0)}
$$
(see Montgomery and Peck, Introduction to Linear Regression Analysis, 1992)</p>

<p>For robust regression, obviously, the term $x_0'(X'X)^{-1}x_0$ cannot be used.  The Hat Matrix is different due to the weights.  It would seem to me that we can instead use the Hat Matrix modified with the weights:</p>

<p>$$
x_0'(X'WX)^{-1}x_0
$$</p>

<p>(see page 44 of the PhD thesis by Christopher Assaid at Virigina Tech)(<a href=""http://scholar.lib.vt.edu/theses/available/etd-3649212139711101/unrestricted/Ch6.PDF"" rel=""nofollow"">http://scholar.lib.vt.edu/theses/available/etd-3649212139711101/unrestricted/Ch6.PDF</a>)</p>

<p>We can approximate $\hat{\sigma^2}$ from the data as
$$
\hat{\sigma^2} = \frac{1}{df}\sum e_i^2
$$
where $df$ are the number of degrees of freedom.</p>

<p>I have three questions on this formulation for prediction intervals:</p>

<p>Is my formula correct?  Is the simple adjustment by factoring in the Weight Matrix enough to adapt the OLS formula for prediction intervals to robust regression.</p>

<p>If it is correct, does it apply to <em>all</em> types of robust regression, or just a subset?</p>

<p>Is it correct to estimate the variance as above?  If so, it would seem to me that  robust regression will always have a larger variance, and thus larger prediction intervals, than OLS.  The reason is is that OLS, by definition, is set up to minimize the residual sum of squares.  Robust regression, on the other hand, by definition of down-weighting potential outliers, even though it may give an overall better fit, will see a larger net residual sum of squares because of the contribution of the squared residuals from the outlier points.  Consequently, since the length of the interval is $\sqrt{\hat{\sigma}^2(1+\delta)}$ (where granted $\delta$ is not necessarily small, but the interval is always $\sigma$ plus something), if $\hat{\sigma}_{RR} &gt; \hat{\sigma}_{OLS}$, in general, the length of the interval for RR will be larger.  It seems counter-intuitive to me that if data is fit with both OLS and robust regression and prediction intervals are made, those from OLS will be by definition narrower and may even be contained within the robust ones.  It seems to thus minimize the power of robust regression.</p>

<p>Any answers to these questions or other suggestion/advice on creating prediction intervals for LTS and MM regression would be appreciated.</p>
"
"0.0400480865731637","0.039253433598943","138510","<p>I am trying to predict categorical response by using several categorical variables and quantitative variables? I tried linear regression model in R, but I don't think it works well as the response is categorical.</p>

<p>Is there any way to predict categorical response? Any recommended book or online pdf?</p>

<p>Thanks.</p>
"
"0.0700841515030364","0.078506867197886","138570","<p>We are looking to use a ""hierarchical"" method of regression in order to input predictor variables in three steps. From what we can tell, the default method of regression is ""stepwise,"" but we can't seem to find out how to fit a model hierarchically or with forced entry. Note that we are not trying to fit a Hierarchical Linear Model (HLM) / Multi-level Model (MLM), but are trying to change the method of regression to specify the order variables are entered into the model.</p>

<p>Specifically, we are trying to fit the following three models, with the two-way interactions in m2ai, and the three-way interaction in m3ai, being input as 2nd and 3rd steps, respectively:</p>

<pre><code>m1ai &lt;- lm(PostValUVAve ~ cPreValUVAve + Int + Gender + SciTeacher + cPreEff + cPreInt)
m2ai &lt;- lm(PostValUVAve ~ cPreValUVAve + Int + Gender + SciTeacher + cPreEff + cPreInt +
                          cPreEff*Int + cPreInt*Int)
m3ai &lt;- lm(PostValUVAve ~ cPreValUVAve + Int + Gender + SciTeacher + cPreEff + cPreInt +
                          cPreEff*Int + cPreInt*Int + cPreEff*Int*cPreInt)
</code></pre>
"
"0.126787000519633","0.124271233208968","138691","<p>I have been unsuccessfully trying to model a relationship between two measured variables described by two different power functions, on either side of a threshold. My question is how to best estimate this relationship with a model. The aims of this model is to find the threshold $x_0$ and interpolate as exactly as possible the values of $y$ close to $x_0$. Estimating the relationship further from $x_0$ is less important. Based on theory I derived the following function: \begin{equation} f(x) = \begin{cases}
    a(x_0-x)^b+c;&amp;  x\leq x_0\\
    \frac{c-y_0}{d(x-x_0)+1}+y_0;              &amp; x &gt; x_0.
\end{cases} \end{equation} where $a&gt;0, b,c&gt;0,d&gt;0, \text{and } x_0&gt;0$ are unknown parameters. This function behaves like a power function below $x_0$, and decreases roughly as $1/x$ to the asymptotic value of $y_0$ in the region above $x_0$. The function looks roughly like this:</p>

<p><img src=""http://i.stack.imgur.com/jUXoP.png"" alt=""enter image description here""></p>

<p>So far I have tried using non-linear least squares to estimate this model, but I am getting the ""singular gradient matrix at initial parameter estimates"" error. My data and code in <strong>R</strong> is as follows:</p>

<pre><code>x &lt;- c( 0.33, 0.35, 0.39, 0.44, 0.48, 0.53, 0.57, 0.63, 0.74, 0.99, 1.12, 1.23, 1.37)
y &lt;- c(72354.00, 23578.20, 1863.40, 743.80, 113.00, 9.80, 7.38, 5.30, 5.22, 5.03, 4.74, 4.53, 4.32)
</code></pre>

<p>and the code for the model I tried fitting is:</p>

<pre><code>starting.values &lt;- c(a = 8, b = 17, c = 8, d = 1,y_0 = 3, x_0 = .55)

model2 &lt;- nls(y~ifelse(px &lt; x_0,a*(x_0-px)^b+c,(c-y_0)/(d*(px-x_0)+1)+y_0),data = data.frame(x,y), 
              start = starting.values)
</code></pre>

<p>I have been trying a variety of likely starting parameters without success, is the problem too few data points, or is the model impossible to evaluate this way?</p>

<p>I have been successful, however, in modeling the relationship with a much simpler function: \begin{equation} g(x) = \begin{cases}
    ax^b;&amp;  x\leq x_0\\
    dx^c;              &amp; x &gt; x_0.
\end{cases} \end{equation}</p>

<p>where $d = ax_0^{b-c}$ to ensure continuity at the threshold. I did this by first fitting the data after applying the log-log transform, and than using the resulting values as inputs to the final model evaluated with nls. The code is as follows:</p>

<pre><code># transforming the data
x.log &lt;- log(x)
y.log &lt;- log(y)

# fitting a broken regression line to the log-log data
starting.values.log &lt;- c(a = -5, b = 10, c = -.1, x_0 = .55)
model1 &lt;- nls(y.log ~ifelse(x.log &lt; log(x_0),b*x.log + a, c*x.log+b+log(x_0)*(a-c)),
              data = data.frame(x.log,y.log), 
              start = starting.values.log)
</code></pre>

<p>below is the plot of the resulting model on the log-log plot:</p>

<p><img src=""http://i.stack.imgur.com/zDRvT.png"" alt=""The model vs. the measurements on a log-log scale""></p>

<p>and now I use the obtained parameters to fit the function:</p>

<pre><code># fitting the actual model using the parameters found previously
starting.values &lt;- c(a = exp(-8.6238),b = -17.7984, c = -.4418, x_0 = 0.555)
model2 &lt;- nls(y~ifelse(x &lt; x_0,a*x^b,a*x_0^((b-c))*x^c),data = data.frame(x,y), 
              start = starting.values)
</code></pre>

<p>The parameters are:</p>

<pre><code>Parameters:
      Estimate Std. Error t value Pr(&gt;|t|)    
a    3.653e-05  1.130e-05   3.233   0.0103 *  
b   -1.931e+01  2.804e-01 -68.857 1.45e-13 ***
c   -4.816e-01  8.832e+01  -0.005   0.9958    
x_0  5.341e-01  1.335e+00   0.400   0.6984    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 388.5 on 9 degrees of freedom

Number of iterations to convergence: 18 
Achieved convergence tolerance: 3.328e-07
</code></pre>

<p>The main problems with this method is that a) I am unable to determine the exponent of the power law when $x \rightarrow x_0$ from below and b) it does not allow for an asymptote other than 0 in $x \rightarrow \infty$. The first problem is much more important to me.</p>

<p>My question is how to best model the relationship between x and y, bering in mind that the theory supports the first function? Is the problem with trying to evaluate the first function too few data points or is it impossible to evaluate and I should try to use the second method instead? If so, than is there any way to obtain the exponent of the power law describing y when $x \rightarrow x_0$ from below?</p>
"
"0.09392108820677","0.0920574617898323","138908","<p>I am looking at the effects role has an opportunities to collaborate between groups in a social network. At a basic level the data are modeled as:</p>

<pre><code>relRatio~role
</code></pre>

<p>With relative ratio being the percentage of teammates who are part of the subject's normal group. The data I have come from multiple time slices over the years, with some of the subjects being polled two or more times. Not every subject has multiple entries, nor does every subject with multiple entries have the same number of entries. From some advice I received it was suggested that I test the differences between groups using a random effect ANOVA model, which would be modeled (in R) as</p>

<pre><code>relRatio~role+Error(subjectId)
</code></pre>

<p>After trying to read up more on random effects ANOVA, I started to get the impression that linear mixed effects models (with the lmer) package are preferred over random effects ANOVA, although I have yet to see a clear distinction between the two. This leads to my first question: Which approach is best for modeling my data?</p>

<p>If it involves using the random effects ANOVA, I would greatly appreciate it if someone could recommend a resource for the process of interpreting the results.</p>

<p>My second question is, if the better approach is to use the mixed effects models, which I have tried, why do I get striping in my residuals?</p>

<p><img src=""http://i.stack.imgur.com/vqWjn.png"" alt=""enter image description here""></p>

<p>One guide suggested that I am dealing with categorical data, which requires the use of logistic regression. However, the dependent variable for my data is continuous, and the IV is categorical, which I thought LMM are supposed to handle. This leads to my second question - does my residual plot indicate something is wrong with the way I have modeled my data?</p>
"
"0.0749231094763201","0.0734364498908627","138938","<p>What is the correct way to calculate the standard errors of the coefficients in a weighted linear regression?</p>

<p>The regression equation I am using is $y_i = a + bx_i$, and I have weights, $w_i = 1/\sigma_i$.  The numerical recipes formula for a straight line fit, and the formula given in ""An introduction to error analysis"" by J. R Taylor, (and Wikipedia too) state that the standard error in the $b$ coefficient is calculated as $$\sigma_b = \sqrt{\frac{\sum w_i}{\sum w_i\sum w_i x_i^2-(\sum w_i x_i)^2}}$$ (or alternatively in matrix form the standard errors are, $\sigma^2 = (X'WX)^{-1}$).  This formula can be derived from propagation of errors. </p>

<p>Using R's $lm()$ function (and python's StatsModels), I get a standard error in the $b$ coefficient which appears* to be calculated as $$\sigma_b = \sigma_e\sqrt{\frac{\sum w_i}{\sum w_i\sum w_i x_i^2-(\sum w_i x_i)^2}}$$
where $\sigma_e^2 = \sum w_i(y_i - a - bx_i)^2/(N-2)$ (alternatively, $\sigma^2 = \sigma_e^2(X'WX)^{-1}$ ).  So they are the same, except for the $\sigma_e$ multiplier in R and StatsModel.</p>

<p>Is it possible that these actually different measures that are just being called the same thing? Is one preferred over the other for an estimate of the standard error?</p>

<p>*I say ""appears"" because I couldn't find the actual formula anywhere.</p>

<p>edited because I had omitted the weight terms in the denominators.   </p>
"
"0.10236445520486","0.107499955208361","139528","<p>When modelling continuous proportions (e.g. proportional vegetation cover at survey quadrats, or proportion of time engaged in an activity), logistic regression is considered inappropriate (e.g. <a href=""http://www.esajournals.org/doi/full/10.1890/10-0340.1"" rel=""nofollow"">Warton &amp; Hui (2011) The arcsine is asinine: the analysis of proportions in ecology</a>). Rather, OLS regression after logit-transforming the proportions, or perhaps beta regression, are more appropriate.</p>

<p>Under what conditions do the coefficient estimates of logit-linear regression and logistic regression differ when using R's <code>lm</code> and <code>glm</code>?</p>

<p>Take the following simulated dataset, where we can assume that <code>p</code> are our raw data (i.e. continuous proportions, rather than representing ${n_{successes}\over n_{trials}}$):</p>

<pre><code>set.seed(1)
x &lt;- rnorm(1000)
a &lt;- runif(1)
b &lt;- runif(1)
logit.p &lt;- a + b*x + rnorm(1000, 0, 0.2)
p &lt;- plogis(logit.p)

plot(p ~ x, ylim=c(0, 1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/AzWOX.png"" alt=""enter image description here""></p>

<p>Fitting a logit-linear model, we obtain:</p>

<pre><code>summary(lm(logit.p ~ x))
## 
## Call:
## lm(formula = logit.p ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.64702 -0.13747 -0.00345  0.15077  0.73148 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.868148   0.006579   131.9   &lt;2e-16 ***
## x           0.967129   0.006360   152.1   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## Residual standard error: 0.208 on 998 degrees of freedom
## Multiple R-squared:  0.9586, Adjusted R-squared:  0.9586 
## F-statistic: 2.312e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Logistic regression yields:</p>

<pre><code>summary(glm(p ~ x, family=binomial))
## 
## Call:
## glm(formula = p ~ x, family = binomial)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.32099  -0.05475   0.00066   0.05948   0.36307  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.86242    0.07684   11.22   &lt;2e-16 ***
## x            0.96128    0.08395   11.45   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 176.1082  on 999  degrees of freedom
## Residual deviance:   7.9899  on 998  degrees of freedom
## AIC: 701.71
## 
## Number of Fisher Scoring iterations: 5
## 
## Warning message:
## In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>Will the logistic regression coefficient estimates always be unbiased with respect to the logit-linear model's estimates?</p>
"
"0.02831827358943","0.0277563690826684","140381","<p>I have extremely large number of observations (8524152) of soil moisture, precipitation, evapotranspiration, delta precipitation, and delta evapotranspiration. I ran a multiple linear regression model and my result looks like </p>

<pre><code>Call:
lm(formula = SMDI ~ ET + delta_ET + PRCP + delta_PRCP, data = regData)

Residuals:
 Min  vvvv     1Q   Median       3Q      Max 
-10414.0     67.1    133.9    192.2   8737.3 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -87.508196   0.797889 -109.67   &lt;2e-16 ***
ET            0.083853   0.001225   68.46   &lt;2e-16 ***
delta_ET      0.267973   0.001270  211.04   &lt;2e-16 ***
PRCP          0.237649   0.003255   73.02   &lt;2e-16 ***
delta_PRCP    0.257458   0.003250   79.23   &lt;2e-16 ***



Residual standard error: 1705 on 8524147 degrees of freedom
Multiple R-squared:  0.4424,    Adjusted R-squared:  0.4424 
F-statistic: 1.691e+06 on 4 and 8524147 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>The t-stat for evapotranspiration (ET), Precipitation (PRCP), delta_PRCP, and delta_ET are same, and the combined p-value is also extremely small. allmost &lt; 2.2e-16. is this possible?</p>

<p>Juvin</p>
"
"0.116939835314396","0.114619460088286","140972","<p>Iâ€™m using a maximal logistic regression model to analyze some data. I would like to keep using this technique if possible, just include more data in the model. The main data Iâ€™m looking at is counts of a particular behavior over items in a sequence, and I would like my analysis to also include data from a post-experiment questionnaire (8 items, 1-9 Likert scored). Hereâ€™s some info about my data:</p>

<pre><code>'data.frame':
Pair          : Factor w/ 36 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1 ...
SpeakerID     : Factor w/ 72 levels ""10A"",""10B"",""11A"",..: 21 22 21 22 22 21 22 21 21 22 ...
Speaker       : Factor w/ 2 levels ""A"",""B"": 1 2 1 2 2 1 2 1 1 2 ...
Condition1     : Factor w/ 4 levels ""ANTI"",""CONTROL"",..: 1 1 1 1 1 1 1 1 1 1 ...
..- attr(*, ""contrasts"")= num [1:4, 1:3] -0.333 1 -0.333 -0.333 0.25 ...
.. ..- attr(*, ""dimnames"")=List of 2
.. .. .. : chr  ""ANTI"" ""CONTROL"" ""IN"" ""OUT""
.. .. .. : NULL
Condition2         : Factor w/ 3 levels ""0"",""90"",""180"": 2 3 1 1 2 1 1 2 2 3 ...
..- attr(*, ""contrasts"")= num [1:3, 1:2] 0 -0.5 0.5 -0.5 0.25 0.25
.. ..- attr(*, ""dimnames"")=List of 2
.. .. ..$ : chr  ""0"" ""90"" ""180""
    .. .. ..$ : NULL
Item         : Factor w/ 16 levels ""MAP1"",""MAP10"",..: 1 9 10 11 12 13 14 15 16 2 ...
Foo       : num  0.847 1.099 1.946 -1.099 -0.452 ...
wtsFoo          : num  0.952 0.889 2.286 0.889 0.468 ...
Close      : num  -1.798 0.202 -1.798 0.202 0.202 ...
Similar    : num  0.505 0.505 0.505 0.505 0.505 ...
Like       : num  -0.833 0.167 -0.833 0.167 0.167 ...
Task1Hard   : num  -0.89 4.11 -0.89 4.11 4.11 ...
Task2Hard: num  -1.02 2.98 -1.02 2.98 2.98 ...
</code></pre>

<p>My analysis is based on this guide to empirical logit analyses:
<a href=""http://talklab.psy.gla.ac.uk/tvw/elogit-wt.html"" rel=""nofollow"">http://talklab.psy.gla.ac.uk/tvw/elogit-wt.html</a>
So far, so good. In my regression model, Iâ€™m testing the fixed effects of Condition1 (4 levels) and Condition2 (3 levels) on Foo (the behavior, expressed as a proportion converted into empirical logit form, see link for how and why). Pair, Pair:Subject (Subject nested within Pair) and Item are included as random effects. Condition1 is between-subjects/pairs and Condition2 is within-subjects. Hereâ€™s the model Iâ€™m using in R:</p>

<pre><code>model &lt;- lmer(Foo ~ Condition1*Condition2 + (1+Condition1 | Pair) 
+ (1+Condition1 | Pair:Subject) + (1+Condition2 | Item), weights=1/wtsFoo, data)
</code></pre>

<p>This all works fine, but hereâ€™s where it gets fun. Where should the questionnaire data go? </p>

<p>Bad idea #1: Each participant has one score for each questionnaire item, so each questionnaire item type should be included as a fixed effect, so that Foo can be predicted by any of the variables discovered in the post-experiment questionnaire (things like social closeness and task difficulty). This is a terrible idea because the questionnaire items are NOT independent variables from Condition1 and Condition2, and if I include them as fixed effects it will introduce a mess of multicollinearity and will just be flat-out wrong.</p>

<p>Bad idea #2: Analyzing the questionnaire data separately. Not such a bad idea, just one that my committee doesnâ€™t like. </p>

<p>Less bad ideas: please suggest a model that allows me to observe the effects of Condition1 and Condition2 on questionnaire items (Close, Similar, Like, Task1Hard, Task2Hard) AND allows me to observe the effects of questionnaire items on Foo. Failing that, explain to me why the only good thing to do is analyze the questionnaire separately from the observation data.</p>

<p>I've read around on Stackexchange and I haven't seen this particular problem covered, although some answers come close to looking useful, I don't yet have the R or stats chops to make them work for me. If I've missed something obvious, please clue me.</p>
"
"NaN","NaN","141072","<p>I am investigating the association of air pollution with birth weight. Given that there is collinearity between air pollutants, I chose to use PLS regression. </p>

<p>To my understanding, we could use partial least square (PLS) regression to account for the collinearity between different predictors. Therefore, it is not necessary to add any interaction term in the PLS model, do I understand correctly?</p>

<p>And would it be at risk of overfitting if I add interaction term? </p>
"
"0.0400480865731637","0.039253433598943","141243","<p>I am trying to replicate what the function <code>dfbetas()</code> does in <strong><em>R</em></strong>.</p>

<p><code>dfbeta()</code> is not an issue... Here is a set of vectors:</p>

<pre><code>x &lt;- c(0.512, 0.166, -0.142, -0.614, 12.72)
y &lt;- c(0.545, -0.02, -0.137, -0.751, 1.344)
</code></pre>

<p>If I fit two regression models as follows:</p>

<pre><code>fit1 &lt;- lm(y ~ x)
fit2 &lt;- lm(y[-5] ~ x[-5])
</code></pre>

<p>I see that eliminating the last point results in a very different slope (blue line - steeper):</p>

<p><img src=""http://i.stack.imgur.com/4ypID.jpg"" alt=""enter image description here""></p>

<p>This is reflected in the change in slopes:</p>

<pre><code>fit1$coeff[2] - fit2$coeff[2]
-0.9754245
</code></pre>

<p>which coincides with the <code>dfbeta(fit1)</code> for the fifth value:</p>

<pre><code>   (Intercept)            x
1  0.182291949 -0.011780253
2  0.020129324 -0.001482465
3 -0.006317008  0.000513419
4 -0.207849024  0.019182219
5 -0.032139356 -0.975424544
</code></pre>

<p>Now if I want to standardize this change in slope (obtain <strong><em>dfbetas</em></strong>) and I resort to: </p>

<blockquote>
  <p>Williams, D. A. (1987) Generalized linear model diagnostics using the
  deviance and single case deletions. Applied Statistics 36, 181â€“191</p>
</blockquote>

<p>which I think may be one of the references in the R documentation under the package <strong>{stats}</strong>. There the formula for <strong><em>dfbetas</em></strong> is:</p>

<p>$\large \mathrm{dfbetas} (i, \mathrm{fit}) = \Large {(\hat{b} - \hat{b}_{-i})\over \mathrm{SE}\, \hat{b}_{-i}}$</p>

<p>This could be easily calculated in R:</p>

<pre><code>(fit1$coef[2] - fit2$coef[2])/summary(fit2)$coef[4]
</code></pre>

<p>yielding: <code>-6.79799</code> </p>

<p>The question is why I am not getting the fifth value for the slope in:</p>

<pre><code>dfbetas(fit1)

  (Intercept)            x
1  1.06199661  -0.39123009
2  0.06925319  -0.02907481
3 -0.02165967   0.01003539
4 -1.24491242   0.65495527
5 -0.54223793 -93.81415653!
</code></pre>

<p>What is the right equation to go from <strong><em>dfbeta</em></strong> to <strong><em>dfbetas</em></strong>?</p>
"
"0.0490486886395286","0.0480754414848157","141339","<p>I'm having trouble in taking a direction of my research project. I have independent variables that are commonly used as economic indicators and I want to include variables/indicators that are not commonly used to improve my eventual forecasts. I have 31 independent variables with 607 monthly observations after making it stationary and applying the scale function.(scale was applied cause my variable series are of different units/measures)
   I used the PCA function and got down to 13 components that capture 80% cumulative of the variance.
   Question is now that I have 13 new independent variables and the one dependent variable that is ternary in the sense that in the 607 observations it indicates 1 for peak, 0 for nothing, and -1 as trough, what model is best for forecasting/predicting the next 1 &amp; -1 of my dependent variable series based on my 13 independent principal components?</p>

<p>FYI: I have looked at VAR, Cointegration, Granger Causality, Multiple Linear Regression, but can't really make sense if what I'm using is correct and appropriate for my topic.</p>
"
"0.02831827358943","0.0277563690826684","141404","<p>When I forecast from a linear Regression model in R using the following code, I get an
arguments of length zero error , which I understand as a null pointer:</p>

<pre><code>library(forecast)
Mwh = c(16.3,16.8,15.5,18.2,15.2,17.5,19.8,19.0,17.5,16.0,19.6,18.0)
temp = c(29.3,21.7,23.7,10.4,29.7,11.9,9.0,23.4,17.8,30.0,8.6,11.8)
t=data.frame(Mwh,temp)
fit = lm(Mwh~temp,data=t)
fcast=forecast(fit,newdata=35)
</code></pre>

<p>However, the following code, which looks very, very similar to me, does not produce the error, but the forecast as desired - why? Where is my blind spot? This works:</p>

<pre><code>library(forecast)
electricity =c(16.3,16.8,15.5,18.2,15.2,17.5,19.8,19.0,17.5,16.0,19.6,18.0) 
maximumDailyTemp= c(29.3,21.7,23.7,10.4,29.7,11.9,9.0,23.4,17.8,30.0,8.6,11.8) 
d = data.frame(maximumDailyTemp, electricity)      
fit&lt;-lm(electricity ~ maximumDailyTemp , data=d)
electricityForecast&lt;-forecast(fit,newdata=35)
</code></pre>

<p>Why is there an error above and no error below? The order of variables in the data Frames cannot be the reason, right?</p>
"
"0.0950527084045132","0.100333291527804","141423","<p>I use the <code>decompose</code> function in <code>R</code> and come up with the 3 components of my monthly time series (trend, seasonal and random). If I plot the chart or look at the table, I can clearly see that the time series is affected by seasonality.</p>

<p>However, when I regress the time series onto the 11 seasonal dummy variables, all the coefficients are not statistically significant, suggesting there is no seasonality.</p>

<p>I don't understand why I come up with two very different results. Did this happen to anybody? Am I doing something wrong?</p>

<hr>

<p>I add here some useful details.</p>

<p>This is my time series and the corresponding monthly change. In both charts, you can see there is seasonality (or this is what I would like to assess). Especially, in the second chart (which is the monthly change of the series) I can see a recurrent pattern (high points and low points in the same months of the year).</p>

<p><img src=""http://i.stack.imgur.com/rILAU.jpg"" alt=""TimeSeries""></p>

<p><img src=""http://i.stack.imgur.com/LdVnv.jpg"" alt=""MonthlyChange""></p>

<p>Below is the output of the <code>decompose</code> function. I appreciate that, as @RichardHardy said, the function does not test whether there is actual seasonality. But the decomposition seems to confirm what I think.</p>

<p><img src=""http://i.stack.imgur.com/ZaVRV.jpg"" alt=""Decompose""></p>

<p>However, when I regress the time series on 11 seasonal dummy variables (January to November, excluding December) I find the following:</p>

<pre><code>    Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept) 5144454056  372840549  13.798   &lt;2e-16 ***
    Jan     -616669492  527276161  -1.170    0.248    
    Feb     -586884419  527276161  -1.113    0.271    
    Mar     -461990149  527276161  -0.876    0.385    
    Apr     -407860396  527276161  -0.774    0.443    
    May     -395942771  527276161  -0.751    0.456    
    Jun     -382312331  527276161  -0.725    0.472    
    Jul     -342137426  527276161  -0.649    0.520    
    Aug     -308931830  527276161  -0.586    0.561    
    Sep     -275129629  527276161  -0.522    0.604    
    Oct     -218035419  527276161  -0.414    0.681    
    Nov     -159814080  527276161  -0.303    0.763
</code></pre>

<p>Basically, all the seasonality coefficients are not statistically significant.</p>

<p>To run linear regression I use the following function:</p>

<p><code>lm.r = lm(Yvar~Var$Jan+Var$Feb+Var$Mar+Var$Apr+Var$May+Var$Jun+Var$Jul+Var$Aug+Var$Sep+Var$Oct+Var$Nov)</code></p>

<p>where I set up Yvar as a time series variable with monthly frequency (frequency = 12).</p>

<p>I also try to take into account the trending component of the time series including a trend variable to the regression. However, the result does not change.</p>

<pre><code>                  Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept) 3600646404   96286811  37.395   &lt;2e-16 ***
    Jan     -144950487  117138294  -1.237    0.222    
    Feb     -158048960  116963281  -1.351    0.183    
    Mar      -76038236  116804709  -0.651    0.518    
    Apr      -64792029  116662646  -0.555    0.581    
    May      -95757949  116537153  -0.822    0.415    
    Jun     -125011055  116428283  -1.074    0.288    
    Jul     -127719697  116336082  -1.098    0.278    
    Aug     -137397646  116260591  -1.182    0.243    
    Sep     -146478991  116201842  -1.261    0.214    
    Oct     -132268327  116159860  -1.139    0.261    
    Nov     -116930534  116134664  -1.007    0.319    
    trend     42883546    1396782  30.702   &lt;2e-16 ***
</code></pre>

<p>Hence my question is: am I doing something wrong in the regression analysis?</p>
"
"0.0749231094763201","0.0734364498908627","141552","<p>I have some data that I fitted using a LOESS model in R, giving me this:</p>

<p><img src=""http://i.stack.imgur.com/JfYFZ.png"" alt=""enter image description here""></p>

<p>The data has one predictor and one response, and it is heteroscedastic.</p>

<p>I also added confidence intervals. The problem is that the intervals are confidence intervals for the line, whereas I am interested in the prediction intervals. For example, the bottom panel is more variable then the top panel, but this is not captured in the intervals.</p>

<p>This question is slightly related:
<a href=""http://stats.stackexchange.com/questions/82603/understanding-the-confidence-band-from-a-polynomial-regression"">Understanding the confidence band from a polynomial regression</a>, especially the answer by @AndyW, however in his example he uses the relatively straightforward <code>interval=""predict""</code> argument that exists in <code>predict.lm</code>, but it is absent from <code>predict.loess</code>.</p>

<p>So I have two very related questions:</p>

<ol>
<li>How do I get the pointwise prediction intervals for LOESS?</li>
<li>How can I predict values that will capture that interval, i.e. generate a bunch of random numbers that will eventually look somewhat like the original data?</li>
</ol>

<p>It is possible that I don't need LOESS and should use something else, but I am unfamiliar with my options. Basically it should fit the line using local regression or multiple linear regression, giving me error estimates for the lines, and in addition also different variances for different explanatory variables, so I can predict the distribution of the response variable (y) at certain x values.</p>
"
"0.0566365471788599","0.0555127381653369","141583","<p>I am working on a few (both simple and multivariable) regression analyses, and I have cases where the residuals are non-normal, to varying degrees. As I've understood, the Gauss-Markov theorem states that normality of residuals is not necessary for the coefficient point estimates to be correct, i.e., I can trust that the regression summary tells me the BLUE coefficient estimates. However, the standard errors may be biased, and thus, the corresponding t- and p-values may not be correct.</p>

<p>A previous question (<a href=""http://stats.stackexchange.com/questions/83012/how-to-obtain-p-values-of-coefficients-from-bootstrap-regression"">How to obtain p-values of coefficients from bootstrap regression?</a>) asked if it was possible to calculate p-values from bootstrapped coefficients and their CIs. However, if I bootstrap the coefficients and use the bootstrapped mean and the bootstrapped SE to re-calculate t- and p-values, would that be a sound approach?</p>

<p>Here is the code I use, in R:</p>

<pre><code># create linear model
mod &lt;- lm(Y~X,data=dataset); summary(mod)

# create function to return coefficient
mod.bootstrap &lt;- function(data, indices) {    
d &lt;- data[indices, ]
mod &lt;- lm(Y~X, data=d)  
return(coef(mod))
}

# set seed
set.seed(1234)

# begin bootstrap using the boot package
mod.boot &lt;- boot(data=dataset, statistic=mod.bootstrap, R=2000)

# now here is how I re-calculate t- and p-values
bootmean &lt;- mean(mod.boot$t[,2])
booter &lt;- sd(boot.t[,2])
tval &lt;- bootmean/booter
p &lt;- 2*pt(-abs(tval),df=mod$df.residual)
</code></pre>

<p>The new t- and p-values come out fairly close to the LM summary, albeit a bit more conservative, as expected. Is this something I could report? I am very new at this, so I can't really tell if my logic is valid or not.</p>

<p>Edit: Clarified a bit.</p>
"
"0.0749231094763201","0.0734364498908627","141684","<p>I'm fitting a fixed effect model with <code>plm</code> and know that I'm dealing with multi-collinearity between two of the independent variables.  I working on identifying multicolliearity in models as a practice and have identified the variable with <code>alias()</code>, then verified with <code>vif()</code>.  I was also able to use <code>kappa()</code> to show a very large conditional number verifying the multicollinearity.</p>

<p>My question is why does <code>plm()</code> ommit this multicolliearity variable from the coefficients?  There is no output clarifying why and I couldn't find anything in the documentation.  Stata automatically omits this variable and I'm curious if <code>plm()</code> does a check and then omits.</p>

<p>Multicollinearity variable <code>dfmfd98</code></p>

<p>Reproducible example : </p>

<p>dput : </p>

<pre><code>data &lt;- 
structure(list(lexptot = c(8.28377505197124, 9.1595012302023, 
8.14707583238833, 9.86330744180814, 8.21391453619232, 8.92372556833205, 
7.77219149815994, 8.58202430280175, 8.34096828565733, 10.1133857229336, 
8.56482997492403, 8.09468633074053, 8.27040804817704, 8.69834992618814, 
8.03086333985764, 8.89644392254136, 8.20990433577082, 8.82621293136669, 
7.79379981225575, 8.16139809188569, 8.25549748271241, 8.57464947213076, 
8.2714431846277, 8.72374048671495, 7.98522888221012, 8.56460042433047, 
8.22778847721461, 9.15431416391622, 8.25261818916933, 8.88033778695326
), year = c(0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 
1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 
1L), dfmfdyr = c(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 
0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0), dfmfd98 = c(1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 0, 0, 0, 0), nh = c(11054L, 11054L, 11061L, 11061L, 
11081L, 11081L, 11101L, 11101L, 12021L, 12021L, 12035L, 12035L, 
12051L, 12051L, 12054L, 12054L, 12081L, 12081L, 12121L, 12121L, 
13014L, 13014L, 13015L, 13015L, 13021L, 13021L, 13025L, 13025L, 
13035L, 13035L)), .Names = c(""lexptot"", ""year"", ""dfmfdyr"", ""dfmfd98"", 
""nh""), class = c(""tbl_df"", ""data.frame""), row.names = c(NA, -30L
))
</code></pre>

<p>Regression code : </p>

<pre><code>library(plm)
lm &lt;- plm(lexptot ~ year + dfmfdyr + dfmfd98 + nh, data = data, model = ""within"", index = ""nh"")
summary(lm)
</code></pre>

<p>Output : </p>

<pre><code>Oneway (individual) effect Within Model

Call:
plm(formula = lexptot ~ year + dfmfdyr + dfmfd98 + nh, data = data, 
    model = ""within"", index = ""nh"")

Balanced Panel: n=15, T=2, N=30

Residuals :
     Min.   1st Qu.    Median   3rd Qu.      Max. 
-4.75e-01 -1.69e-01  4.44e-16  1.69e-01  4.75e-01 

Coefficients :
        Estimate Std. Error t-value Pr(&gt;|t|)  
year     0.47552    0.23830  1.9955  0.06738 .
dfmfdyr  0.34635    0.29185  1.1867  0.25657  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Total Sum of Squares:    5.7882
Residual Sum of Squares: 1.8455
R-Squared      :  0.68116 
      Adj. R-Squared :  0.29517 
F-statistic: 13.8864 on 2 and 13 DF, p-value: 0.00059322
</code></pre>
"
"0.0800961731463273","0.078506867197886","141771","<p>I've been using <code>nls()</code> to fit a custom model to my data, but I don't like how the model is fitting and I would like to use an approach that minimizes residuals in both x and y axes.  </p>

<p>I've done a lot of searching, and have found solutions for fitting <strong>linear</strong> models: </p>

<ul>
<li>via the <a href=""http://cran.r-project.org/web/packages/deming/index.html"" rel=""nofollow""><code>deming</code> package</a>, </li>
<li>various stackoverflow posts:
<ul>
<li><a href=""http://stackoverflow.com/questions/6889809/"">total-least-square-method-using-r</a>,</li>
<li><a href=""http://stackoverflow.com/questions/6872928/"">how-to-calculate-total-least-squares-in-r-orthogonal-regression</a>, </li>
</ul></li>
<li>and this very nice CrossValidated post: <a href=""http://stats.stackexchange.com/questions/13152/how-to-perform-orthogonal-regression-total-least-squares-via-pca"">How to perform orthogonal regression (total least squares) via PCA?</a>.  </li>
<li>I've also found a MATLAB solution: <a href=""http://stats.stackexchange.com/questions/110772/total-least-squares-curve-fit-problem"">Total least squares curve fit problem</a>, </li>
</ul>

<p>but these fit a second order polynomial and not a custom, user-defined model.</p>

<p>What I would like is something similar to <code>nls()</code> that does the x and y residual minimization.  This would allow me to enter my custom model.  Is anyone aware of any solution in R?</p>

<p>Here's an example, but please note that I'm seeking suggestions on a general solution for nonlinear total least squares regression, and not something specific to this dataset (this is just an example data from <a href=""http://stackoverflow.com/questions/21815745/modifying-a-curve-to-prevent-singular-gradient-matrix-at-initial-parameter-estim/21819042#21819042"">here</a>):</p>

<pre><code>df &lt;- structure(list(x = c(3, 4, 5, 6, 7, 8, 9, 10, 11), y = c(1.0385, 
1.0195, 1.0176, 1.01, 1.009, 1.0079, 1.0068, 1.0099, 1.0038)), .Names = c(""x"", 
""y""), row.names = c(NA, -9L), class = ""data.frame"")

(nlsfit &lt;- nls(y ~ a^b^x, data = df, start = c(a=0.9, b=0.6)))

library(ggplot2)
ggplot(df, aes(x=x, y=y)) + 
    geom_point() + 
    geom_smooth(method=""nls"", formula = y ~ a^b^x, se=F, start = list(a=0.9, b=0.6))
</code></pre>

<p>Does anyone have any suggestion for how I might proceed? </p>
"
"0.0400480865731637","0.0196267167994715","141884","<p>I want to know about the relationship between a multiple linear regression model and the Kalman Filter. Mainly, I want to know how I can write a multiple regression model using Kalman Filter. For that reason, I simulate some data and I run the code $lm(Y \sim X_1 + X_2)$ in R project. From that I take the estimated coefficients. I want now a code in R, in order to estimate the coefficients using Kalman Filter so as to compare my results. </p>

<p>Lets suppose that my dataset is produced by the code:</p>

<pre><code>a=c(10,3,-5)
n=100
x=matrix(rnorm(n*2), byrow=F, ncol=2)
y=cbind(rep(1,n),x)%*%a + 2*rnorm(n)
lm(y~x[,1]+x[,2])
</code></pre>

<p>Thank you...</p>
"
"NaN","NaN","142248","<p>When Performing a linear regression in <code>r</code> I came across the following terms. </p>

<pre><code> NBA_test =read.csv(""NBA_test.csv"")
 PointsPredictions  = predict(PointsReg4, newdata =  NBA_test)
 SSE = sum((PointsPredictions - NBA_test$PTS)^2)
     SST = sum((mean(NBA$PTS) - NBA_test$PTS) ^ 2)
 R2 = 1- SSE/SST
</code></pre>

<p>In this case I am predicting the number of points. I understood what is meant by SSE(sum of squared errors), but what actually is SST and R square? Also what is the difference between R2 and RMSE?</p>
"
"0.02831827358943","0.0277563690826684","142312","<p>I'm having an interesting dilemma with the <code>neuralnet</code> and <code>nnet</code> packages in <code>R</code>.  I recently tried a series of feed-forward neural networks giving each the same data sets and every single time, no matter how I tweak the algorithms, hidden layers, neuron sizes, maximum iterations or error thresholds, both functions keep converging their predictions to approximately the mean of whatever they are training on.</p>

<p>A linear regression does way better for each series in terms of fit, and both of these packages seem to do a better job fitting random data from the <code>rnorm</code> function than real data.  In regards to the mathematics of the problem, what could be causing this and how should I resolve?  I have sample code below and can paste a sample dataset below if requested.  Thanks!</p>

<pre><code>model6 &lt;- neuralnet(
    target ~ 1 + majorholiday + mon + sat + sun + thu + tue + wed + tickets + l1_target + l7_target, data = data_nn
    ,algorithm = ""rprop+"", hidden = c(8), stepmax = 500000
    ,err.fct = ""sse"", threshold = 0.01, lifesign = ""full"", lifesign.step = 100
    , linear.output= T)
</code></pre>

<p><strong>EDIT</strong></p>

<p>A user requested I paste some data.  Here is one set below and I just tried the same code again prior to uploading and the same thing happens, converges to the mean of <code>target</code> at about 17.45</p>

<pre><code>    row.names   target  majorholiday    mon sat sun thu tue wed backtickets l1_target   l7_target
1   8   18.976573088    0   0   0   0   0   0   0   13806   18.114001584    36.521334684
2   9   20.701716096    0   1   0   0   0   0   0   15308   18.976573088    35.477867979
3   10  25.014573616    0   0   1   0   0   0   0   13439   20.701716096    28.173601042
4   11  15.706877377    1   0   0   0   0   0   0   11283   25.014573616    27.602288128
5   12  19.633596721    0   0   0   0   1   0   0   12272   15.706877377    13.801144064
6   13  20.049395337    0   0   0   0   0   1   0   9528    19.633596721    32.777717152
7   14  21.720178282    0   0   0   1   0   0   0   13747   20.049395337    18.114001584
8   15  23.390961226    0   0   0   0   0   0   0   15277   21.720178282    18.976573088
9   16  16.707829447    0   1   0   0   0   0   0   16058   23.390961226    20.701716096
10  17  15.872437975    0   0   1   0   0   0   0   14218   16.707829447    25.014573616
11  18  23.295531996    1   0   0   0   0   0   0   11249   15.872437975    15.706877377
12  19  22.363710716    0   0   0   0   1   0   0   13993   23.295531996    19.633596721
13  20  24.227353276    0   0   0   0   0   1   0   13402   22.363710716    20.049395337
14  21  20.500068156    0   0   0   1   0   0   0   14244   24.227353276    21.720178282
15  22  26.090995836    0   0   0   0   0   0   0   14502   20.500068156    23.390961226
16  23  18.636425597    0   1   0   0   0   0   0   16296   26.090995836    16.707829447
17  24  15.840961757    0   0   1   0   0   0   0   13694   18.636425597    15.872437975
18  25  20.650050308    1   0   0   0   0   0   0   10774   15.840961757    23.295531996
19  26  13.467424114    0   0   0   0   1   0   0   12348   20.650050308    22.363710716
20  27  19.752222033    0   0   0   0   0   1   0   12936   13.467424114    24.227353276
21  28  27.832676502    0   0   0   1   0   0   0   14342   19.752222033    20.500068156
22  29  18.854393759    0   0   0   0   0   0   0   14390   27.832676502    26.090995836
23  30  10.773939291    0   1   0   0   0   0   0   16724   18.854393759    18.636425597
24  31  12.569595839    0   0   1   0   0   0   0   14091   10.773939291    15.840961757
25  32  28.153882107    1   0   0   0   0   0   0   11250   12.569595839    20.650050308
26  33  24.400031160    0   0   0   0   1   0   0   12803   28.153882107    13.467424114
27  34  21.584642949    0   0   0   0   0   1   0   13318   24.400031160    19.752222033
28  35  27.215419370    0   0   0   1   0   0   0   14193   21.584642949    27.832676502
29  36  21.584642949    0   0   0   0   0   0   0   14312   27.215419370    18.854393759
30  37  15.015403791    0   1   0   0   0   0   0   16445   21.584642949    10.773939291
31  38  26.276956633    0   0   1   0   0   0   0   13753   15.015403791    12.569595839
32  39  15.139500902    1   0   0   0   0   0   0   11619   26.276956633    28.153882107
33  40  12.467824272    0   0   0   0   1   0   0   14006   15.139500902    24.400031160
34  41  21.373413039    0   0   0   0   0   1   0   14098   12.467824272    21.584642949
35  42  8.015029889 0   0   0   1   0   0   0   14462   21.373413039    27.215419370
36  43  16.030059779    0   0   0   0   0   0   0   15367   8.015029889 21.584642949
37  44  19.592295285    0   1   0   0   0   0   0   17868   16.030059779    15.015403791
38  45  18.701736409    0   0   1   0   0   0   0   15052   19.592295285    26.276956633
39  46  16.002499062    1   0   0   0   0   0   0   10035   18.701736409    15.139500902
40  47  16.943822536    0   0   0   0   1   0   0   13708   16.002499062    12.467824272
41  48  11.295881691    0   0   0   0   0   1   0   13463   16.943822536    21.373413039
42  49  19.767792959    0   0   0   1   0   0   0   13998   11.295881691    8.015029889
43  50  19.767792959    0   0   0   0   0   0   0   14745   19.767792959    16.030059779
44  51  16.943822536    0   1   0   0   0   0   0   16156   19.767792959    19.592295285
45  52  14.119852113    0   0   1   0   0   0   0   13552   16.943822536    18.701736409
46  53  22.869570079    1   0   0   0   0   0   0   11554   14.119852113    16.002499062
47  54  10.481886286    0   0   0   0   1   0   0   13437   22.869570079    16.943822536
48  55  19.057975066    0   0   0   0   0   1   0   14076   10.481886286    11.295881691
49  56  20.010873819    0   0   0   1   0   0   0   14567   19.057975066    19.767792959
50  57  9.528987533 0   0   0   0   0   0   0   14277   20.010873819    19.767792959
51  58  21.916671326    0   1   0   0   0   0   0   16545   9.528987533 16.943822536
52  59  11.000000000    1   0   0   0   0   0   1   15599   21.916671326    14.119852113
53  60  17.000000000    0   0   0   0   1   0   1   17463   11.000000000    22.869570079
54  61  10.000000000    0   0   0   0   0   1   1   17935   17.000000000    10.481886286
55  62  20.000000000    0   0   0   1   0   0   1   18357   10.000000000    19.057975066
56  63  19.000000000    0   0   0   0   0   0   1   19246   20.000000000    20.010873819
57  64  17.000000000    0   1   0   0   0   0   1   21234   19.000000000    9.528987533
58  65  11.000000000    0   0   1   0   0   0   1   18493   17.000000000    21.916671326
59  66  9.000000000 1   0   0   0   0   0   1   15315   11.000000000    11.000000000
60  67  22.000000000    0   0   0   0   1   0   1   17841   9.000000000 17.000000000
61  68  9.000000000 0   0   0   0   0   1   1   18312   22.000000000    10.000000000
62  69  11.000000000    0   0   0   1   0   0   1   17880   9.000000000 20.000000000
63  70  5.000000000 0   0   0   0   0   0   1   19371   11.000000000    19.000000000
64  71  15.000000000    0   1   0   0   0   0   1   21696   5.000000000 17.000000000
65  72  12.000000000    0   0   1   0   0   0   1   18829   15.000000000    11.000000000
66  73  10.000000000    1   0   0   0   0   0   1   14749   12.000000000    9.000000000
67  74  15.000000000    0   0   0   0   1   0   1   17928   10.000000000    22.000000000
68  75  7.000000000 0   0   0   0   0   1   1   18254   15.000000000    9.000000000
</code></pre>
"
"0.0400480865731637","0.039253433598943","142316","<p>As a programmer I have used the spdep package successfully for spatial filtering. But would appreciate it if someone could offer a description (preferably with supporting references) of how this concept works. Let's suppose I have a standard linear regression model with an integer response variable and 3 or more predictors comprising real values:</p>

<ol>
<li><p>According to <a href=""http://books.google.de/books/about/Spatial_Statistics_and_Geostatistics.html?id=xSPMCHuchEgC&amp;redir_esc=y"" rel=""nofollow"">Chun and Griffith</a>, a spatial filter would be constructed from missing predictors which are spatially correlated, and which will help to model the autocorrelation of the all observations. How is the autocorrelation among multivariate predictors taken into account?</p></li>
<li><p>How does this approach compare to the spatial Durbin model?</p></li>
<li><p>Does this concept have any relationship with the <a href=""http://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem"" rel=""nofollow"">Perronâ€“Frobenius theorem</a> from linear algebra?</p></li>
</ol>
"
"0.11327309435772","0.104086384060007","142317","<p>I want to do <strong>multivariate</strong> (with more than 1 response variables) <strong>multiple</strong> (with more than 1 predictor variables) <strong>nonlinear regression</strong> in <strong>R</strong>.</p>

<p>The data I am concerned with are 3D-coordinates, thus they interact with each other, i.e. the x,y,z-coordinates are not independent. So I cannot just call the <em>nls</em> separately for each response variable (which I tried at first). </p>

<p>A subset of the data-frame with 3D-coordinates where x,y,z are the predictive variables and a,b,c the response variables:</p>

<pre><code>              x           y         z           a            b         c
1  -2.26470e-03 -0.05081670 0.0811701 -0.00671079 -0.045721600 0.0705679
2  -9.13106e-05 -0.00670734 0.0724838 -0.00676299 -0.001638430 0.0588486
3   3.81399e-04  0.03556000 0.0782059 -0.00783726  0.038503800 0.0641364
4   1.42293e-03  0.06133920 0.0708688 -0.00820760  0.062697100 0.0572740
5  -5.06043e-02  0.04759040 0.0418189 -0.05949350  0.040427800 0.0266159
6   5.92963e-02  0.04183450 0.0431029  0.05124780  0.038396500 0.0327903
7  -4.44213e-02 -0.00909717 0.0459059 -0.05021130 -0.005634520 0.0329833
8  -3.75400e-02 -0.00625770 0.0567296 -0.04255200 -0.000666089 0.0436465
9  -2.37768e-02 -0.00707318 0.0581552 -0.03048950 -0.001260670 0.0457355
10 -1.56645e-02 -0.01326670 0.0540247 -0.02101350 -0.009021990 0.0413755
</code></pre>

<p><strong>My question:</strong> Is it possible to call the <em>nls</em> function with more than 1 (in my case 3) response variables? In other words is it possible to substitute <em>y</em> in <code>nls(y ~ f(x,y,z, parameters), data)</code> with something like <em>c(a,b,c)</em> or <em>cbind(a,b,c)</em>, such that <code>nls(cbind(a,b,c) ~ f(x,y,z, parameters), data)</code> ?</p>

<p>In the post <a href=""http://stackoverflow.com/questions/12161659/how-to-write-r-formula-for-multivariate-response"">How to write R formula for multivariate response?</a> it is shown that one can combine several response variables with <em>cbind</em> in the case of linear modeling with the <em>lm</em> function.
This doesn't seem to work for nonlinear modeling with <em>nls</em> .., because the <em>nls</em> call in the code sample at the bottom of my question throws the following error:</p>

<p><code>Error in parse(text = x) : &lt;text&gt;:2:0: unexpected end of input
1: ~ 
   ^</code></p>

<p>which I could not find a solution for online concerning my case of a multivariate regression..</p>

<hr>

<p>My web-searches to my main question only gave me results concerning <em>multivariate <strong>linear</strong> regression</em>, which for example included <a href=""http://stats.stackexchange.com/questions/11127/multivariate-multiple-regression-in-r/11132#11132"">solutions with the manova function</a>..</p>

<p>Therefore, <strong>my question asked in a more general way:</strong> How do you in general solve such a non-linear multivariate multiple regression problem in R which takes into account interactions/dependencies between variables?</p>

<p>Here is my code where </p>

<ul>
<li>function <em>f</em> computes the rotations of coordinates about three axes
in the order x-axis, y-axis, and then z-axis (unfortunately I cannot
include the pic of the equation I wrote in LaTeX here since I haven't
got 10 reputation points yet);</li>
<li><em>rot_data_all</em> is structured as the data-subset above, just with more rows;</li>
<li>alpha1, alpha2 and so on are the parameters which nonlinear
regression should approximate:</li>
</ul>

<p>The code:</p>

<pre><code>f &lt;-function(x, y, z, alpha1, alpha2, alpha3, gamma, theta, phi, s) { 
      a &lt;- alpha1 + s*(cos(theta)*cos(phi)*x - cos(theta)*sin(phi)*y + sin(theta)*z)
      b &lt;- alpha2 + s*((sin(gamma)*sin(theta)*cos(phi) + cos(gamma)*sin(phi))*x 
                          + (-sin(gamma)*sin(theta)*sin(phi) + cos(gamma)*cos(phi))*y
                          - sin(gamma)*cos(phi)*z)
      c &lt;- alpha3 + s*((cos(gamma)*sin(theta)*cos(phi) + sin(gamma)*sin(phi))*x 
                          + (cos(gamma)*sin(theta)*sin(phi) + sin(gamma)*cos(phi))*y
                          + cos(gamma)*cos(phi)*z)
      return(c(a,b,c))
    }

    rot.nls &lt;- nls(cbind(a, b, c) ~ f(x, y, z, alpha1, alpha2, alpha3, gamma, theta, phi, s), 
                   data = rot_data_all, 
                   start = c(alpha1 = 0, alpha2 = 0, alpha3 = 0, gamma = 0.1, theta = 0.1, phi = 0.1, s = 0.1), trace = TRUE)
</code></pre>

<hr>

<p>I hope to find a solution which is general enough to also solve other transformations which cannot be easily linearized like the set of equations for <strong>projective transformation</strong>, i.e. something like the following function:</p>

<pre><code>f.proj &lt;-function(x, y, z, betas) {
  a &lt;- (betas[1,1]*x + betas[1,2]*y + betas[1,3]*z + betas[1,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  b &lt;- (betas[2,1]*x + betas[2,2]*y + betas[2,3]*z + betas[2,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  c &lt;- (betas[3,1]*x + betas[3,2]*y + betas[3,3]*z + betas[3,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  return(c(a,b,c))
}
</code></pre>

<hr>

<p>I am happy to provide more information if needed! Thank you so much!</p>
"
"0.160265947190769","0.15708587487024","142489","<p>I'm analysing PAM fluorescence data from an experimental set-up that I duplicated from an earlier experiment with a missing control. That's why I haven't given the statistics of the experiment much (if any) thought in advance.</p>

<p>The set-up consisted of 8 containers with peat moss (<em>Sphagnum magellanicum</em>), divided over 4 treatments, so that each treatment was performed in duplicate. At regular (weekly) intervals, over the course of 3 months, I performed life PAM fluorescence measurements on a number of capitula (growth tops) in each container to determine a kinetic response curve for each of these capitula.</p>

<p>To minimize intraleaf (in my case, intra<em>capitula</em>) variance, ideally, PAM fluorescence measurements would have been repeated for the same leaf every week in the 3-month time series, but for practical reasons, my AOIs (areas of interest) for the fluorescence meter where located on different capitula every week. This is also my first subquestion: can I consider measurements at different time points in the same container as <em>repeated measures</em>, or would this only be valid if I had been measuring the same AOIs every week? And does this depend on whether I aggregate the measured values of the different AOIs per container before further analysis?</p>

<p>After nightfall, once every week, for 5â€“7 AOIs in each container, I determined a kinetic curve, for which the PAM software performs 20 measurements. The first measurement represents the dark-adapted fluorescence values, after which an actinic light source (at a wavelength that can facilitate photosynthesis) is started for the 19 remaining measurements. From the start of the kinetic curve (the dark adapted $\phi_{PSII}$ values), I determine $F_v/F_m$ and from the end of the curve (the flat part), I determine $\text{mean}(\phi_{PSII})$. $\phi_{PSII}$ and $F_v/F_m$ measure the quantum yield of photosystem II and the max. efficiency of photosystem II, respectively; $F_v/F_m = \phi_{PSII}$ in a dark-adapted state.</p>

<p>I'm interested in building two models, one in which the response (dependent) variable is $\phi_{PSII}$ and one in which it is $F_v/F_m$. The (independent) predictor variables are:</p>

<ul>
<li><code>AOI</code> (factor): a number between 1â€“6; </li>
<li><code>Container</code> (factor): a number between 1â€“8; </li>
<li><code>Treatment</code>: (factor): a number between 1â€“4; and</li>
<li><code>DaysTreated</code> (integer): the number of days since the treatments began.</li>
</ul>

<p>My guess is that I should treat <code>AOI</code> and <code>Container</code> as random effects variables, with <code>AOI</code> nested in <code>Container</code> and <code>Container</code> nested in the fixed effect variable <code>Treatment</code>. <code>DaysTreated</code>, then, would be my continuous predictor (covariate). For $\phi_{PSII}$, I would model this in R like this:</p>

<pre><code>library(nlme)
YII_m1 &lt;- lme(mean_YII ~ DaysTreated * Treatment,
              random = ~1 | Container / AOI,
              method = ""ML"",
              data = fluor_aoi)
# fluor_aoi is a data-frame in which each AOI kinetic curve is
# aggregated into one row, where mean_YII = mean( YII[15:19] )
# and FvFm = YII[1]
</code></pre>

<p>I'm not sure if this is the most parsimious model. To find out, I want to try different models with different fixed effects but all with the same random effects. <code>anova.lme()</code> warned me that comparing between these models is a <a href=""http://stats.stackexchange.com/questions/116770/"">no-go</a> when using the default method (<code>method = ""REML""</code>), which is why I use <code>method = ""ML""</code>.</p>

<pre><code>anova(YII_m1, # ~ DaysTreated * Treatment
      YII_m2, # ~ DaysTreated:Treatment + Treatment
      YII_m3, # ~ DaysTreated:Treatment + DaysTreated
      YII_m4, # ~ DaysTreated:Treatment
      YII_m5, # ~ DaysTreated + Treatment
      YII_m6, # ~ DaysTreated
      YII_m7  # ~ Treatment
     )

       Model df       AIC       BIC   logLik   Test  L.Ratio p-value
YII_m1     1 11 -2390.337 -2340.578 1206.168                        
YII_m2     2 11 -2390.337 -2340.578 1206.168                        
YII_m3     3  8 -2390.347 -2354.158 1203.173 2 vs 3  5.99019  0.1121
YII_m4     4  8 -2390.347 -2354.158 1203.173                        
YII_m5     5  8 -2366.481 -2330.293 1191.241                        
YII_m6     6  5 -2363.842 -2341.224 1186.921 5 vs 6  8.63915  0.0345
YII_m7     7  7 -2264.868 -2233.203 1139.434 6 vs 7 94.97389  &lt;.0001
</code></pre>

<p>I would have liked it if the <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">best fit</a> was model 2 with the fixed effects formula <code>~ DaysTreated:Treatment + Treatment</code>, because my expectation at the onset of my experiment was to see a decline in <em>Sphagnum</em> vitality, but only for some of the treatments and hopefully not in the controls. (The acclimatization period was very long, hoping that any effects on the mosses of the new (greenhouse) environment would have flattened out by the onset of the treatments.)</p>

<p><strong>Edit 2015-May-1:</strong> First I compared only 6 models; model 4 was missing from my initial question. Also, I forgot to factorize treatment, so that instead of model 2, now, different models give the â€˜best fitâ€™.</p>

<p>Anyway, so far (unless you tell me otherwise), I feel I can continue to use model 2, which also best fits the visual observation that 4 of the 8 containers where doing very badly at the end of the experiment while the other 4 seemed to do ok.</p>

<pre><code>anova(YII_m2)
                  numDF denDF  F-value p-value
(Intercept)           1   620 526.9698  &lt;.0001
Treatment             3     4   5.0769  0.0753
DaysTreated:Treatment 4   620  36.4539  &lt;.0001
</code></pre>

<p>An ANCOVA test on model 2 reveals that only the interaction between <code>DaysTreated</code> and <code>Treatment</code> is significant, which makes sense to me, given that the containers started out in roughly the same condition after acclimatization. There was visible difference between containers in the same treatments, but that should have been taken care of by correcting for the random error effect.</p>

<p>Mean $\phi_{PSII}$ plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments:</p>

<p><img src=""http://i.stack.imgur.com/GgGhG.png"" alt=""Mean Y_II plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments.""></p>

<p>Now that I've made an <em>attempt</em> at constructing and testing a somewhat decent model (which I'd love to receive criticism on), I'd like to perform a multiple pairwise comparison to find out which treatments diverge significantly from each other over time, but I have no idea what is the proper way to approach this.</p>

<p>Also, I want to try a linear correlation, but again, I'm clueless as to how. Is there an appropriate way to integrate this in my model or should I try to model a regression per treatment? </p>

<p>Please forgive the ignorance in my approach and my questions. I'm a BSc student whose statistical background mainly consists of a brief entry-level course, followed by a recipe-level R course. RTFM comments are definitely welcome, as long as they include a link to TFM.</p>
"
"0.0693653206906364","0.0679889413649005","142914","<p>I want to control for a nuisance covariate in a linear model. Since the covariate interacts significantly with one of the fixed factors, the homogeneity of regression slopes assumption is violated for an ANCOVA approach. My understanding is that this can be handled in a multi-level, or mixed effects, model.</p>

<p>My data:</p>

<pre><code>&gt; str(seeds)
'data.frame':   186 obs. of  4 variables:
 $ fixed.A : Factor w/ 31 levels ""A1"",""A2"",""A3"",..: 7 7 7 7 7 7 10 10 10 10 ...
     $ fixed.B : Factor w/ 2 levels ""Y"",""N"": 2 2 2 1 1 1 2 2 2 1 ...
 $ cov     : num  10.3 10.5 11 12.8 12.9 ...
     $ response: num  10.8 11 11.1 14.7 15.3 ...
</code></pre>

<p>The covariate <code>cov</code> has a significant interaction with <code>fixed.A</code>. To fit a random intercept and slope for <code>cov</code> conditioned on <code>fixed.A</code>, it seems an approach using <code>lmer</code> (in lme4) might be:</p>

<pre><code>&gt; lmer(response ~ fixed.A*fixed.B + (1 + cov | fixed.A), data = seeds)
</code></pre>

<p>I'm aware that this is not the usual approach in <code>lmer</code> since grouping factors tend to be random and therefore don't also appear as fixed factors in the model formula. However, since I'm interested in the interaction between my two fixed factors, I don't see how else to proceed. Any help is greatly appreciated.</p>
"
"0.0400480865731637","0.039253433598943","143640","<p>I'm using <code>R</code>'s <code>nls</code> to fit different datasets to the same model. I've read that using R-squared is usually not correct for non-linear regressions, however, I'd like to be able to tell which fit is apparently better.</p>

<p>I don't plan to use the deviance (residual sum-of-squares) to choose or say which data is better, but a numeric parameter will come handy in order to help me optmize the algorithm which manipulates the data.</p>

<p>The datasets have the same order of magnitude, and visual inspection of the original vs fitted curves doesn't show much difference. However, I have values of e.g. ${dev_1 = 0.0240}$ vs ${dev_2 = 0.0072}$, which I think are considerably different.</p>

<p>So, can I suppose ${fit_2}$ was better than ${fit_1}$? If not, is there any other (relatively simple) way to compare this values?</p>
"
"0.0490486886395286","0.0480754414848157","144076","<p>I have a bunch of data that I fit a linear regression to, and now I need to find the variance of my slope. Is there an analytical way to get this?</p>

<p>If an example is necessary, consider this my data in R:</p>

<pre><code>x &lt;- c(1:6)
y &lt;- c(18, 14, 15, 12, 7, 6)
lm(y ~ x)$coefficients
</code></pre>

<p>So I have a slope estimate of <code>-2.4</code>, but I want to know the variance of that estimate.</p>

<p>After looking at previous questions, I've seen a few equations for estimating the slope parameter, but I'm a little confused about what the differences between equations are and what approach is valid for my problem.</p>

<p>For example, the answers in <a href=""http://stats.stackexchange.com/questions/122406/the-variance-of-linear-regression-estimator-beta-1"">this question</a> say that $\newcommand{\Var}{\rm Var}\newcommand{\slope}{\rm slope}\Var[\slope] = \frac{V[Y]}{\sum\left(\frac{x_i-\bar{x}}{\sum(x_i-\bar{x})^2}\right)}$.</p>

<p><a href=""http://stats.stackexchange.com/questions/12186/expected-value-and-variance-of-estimation-of-slope-parameter-beta-1-in-simple"">This question</a> says that $\Var[\slope] = \frac{V[Y]}{\sum(x_i-\bar{x})^2}$.</p>

<p>And if I look at the output in R (as a ""check"" mechanism), I'm given two other ways I could potentially calculate the slope variance (one using the standard error, another given the covariance matrix). I feel like I'm missing something key because all these estimates give me similar (but not the same) answer.</p>
"
"0.0983889005882491","0.0964366217361766","144247","<p>This is my first time attempting to build a linear regression model and I am not sure what to do next given the results I have.</p>

<p>I have a data set with 24 predictors and 1 response and there are 999 rows in the data set.  The data can be found <a href=""http://pastebin.com/Whv6tgiv"" rel=""nofollow"">here</a>.  I am trying to build a linear regression model with the end goal of being able to predict the response variable.</p>

<p>I decided to log transform the response variable as this resulted in the histogram looking more Normally distributed:
<img src=""http://i.imgur.com/vK1hF7Q.png"" alt=""Untransformed response"">
<img src=""http://i.imgur.com/eVERAw8.png"" alt=""Log transformed response""></p>

<p>My first linear regression model was the following:</p>

<pre><code>Call:
lm(formula = log(y) ~ ., data = the_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.70154 -0.13329  0.01642  0.14626  1.10267 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.112026   0.061692  82.863  &lt; 2e-16 ***
v1           0.136381   0.024563   5.552 3.64e-08 ***
v2           0.069991   0.024519   2.855  0.00440 ** 
v3           0.034584   0.024504   1.411  0.15845    
v4           0.031069   0.024562   1.265  0.20620    
v5          -0.078188   0.024556  -3.184  0.00150 ** 
v6          -0.007898   0.024579  -0.321  0.74803    
v7          -0.062695   0.024613  -2.547  0.01101 *  
v8          -0.007664   0.024553  -0.312  0.75501    
v9          -0.019956   0.024558  -0.813  0.41664    
v10          0.025143   0.024561   1.024  0.30623    
v11          0.003946   0.024583   0.161  0.87250    
v12          0.024605   0.024551   1.002  0.31650    
v13         -0.005139   0.024558  -0.209  0.83429    
v14         -0.071931   0.024534  -2.932  0.00345 ** 
v15          0.002112   0.024552   0.086  0.93145    
v16         -0.050584   0.024510  -2.064  0.03930 *  
v17          0.012561   0.024491   0.513  0.60815    
v18         -0.029778   0.024515  -1.215  0.22478    
v19          0.030362   0.024559   1.236  0.21666    
v20         -0.022925   0.024519  -0.935  0.35002    
v21         -0.003210   0.024532  -0.131  0.89591    
v22          0.027668   0.024617   1.124  0.26132    
v23         -0.009467   0.024557  -0.386  0.69993    
v24          0.003332   0.024567   0.136  0.89214    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.223 on 974 degrees of freedom
Multiple R-squared:  0.07552,   Adjusted R-squared:  0.05274 
F-statistic: 3.315 on 24 and 974 DF,  p-value: 1.603e-07
</code></pre>

<p>The R-squared value (0.076) tells me that this model is not very good, correct?</p>

<p>Looking at the Residuals vs Fitted and Scale-Location plots I think that there is some pattern (upside down parabola).<br>
<img src=""http://i.imgur.com/C7i2kgA.png"" alt=""First model residuals""></p>

<p>So I decided to fit the following model which has first order interactions</p>

<pre><code>Call:
lm(formula = log(y) ~ .^2, data = the_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.60878 -0.11444  0.00161  0.11522  0.71236 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  4.510425   0.372884  12.096  &lt; 2e-16 ***
v2           0.417113   0.211401   1.973 0.048879 *  
v3           0.554384   0.218996   2.531 0.011577 *  
...   
v12          0.644474   0.219123   2.941 0.003378 ** 
...
v1:v2       -0.501372   0.088464  -5.668 2.12e-08 ***
... 
v1:v5        0.226257   0.091674   2.468 0.013823 *  
...
v1:v12      -0.265495   0.089155  -2.978 0.003003 ** 
...
v1:v16      -0.226053   0.090448  -2.499 0.012674 *  
...
v2:v9        0.175206   0.088957   1.970 0.049285 *  
...
v2:v12      -0.558579   0.087183  -6.407 2.73e-10 ***
...
v2:v16       0.230151   0.088761   2.593 0.009716 ** 
v2:v17      -0.235761   0.087570  -2.692 0.007267 ** 
...
v2:v22       0.219280   0.085991   2.550 0.010984 *  
...
v3:v5       -0.305964   0.087337  -3.503 0.000489 ***
...
v3:v9       -0.303287   0.086619  -3.501 0.000492 ***
...
v4:v5       -0.382494   0.091162  -4.196 3.07e-05 ***
...
v5:v13       0.222259   0.089072   2.495 0.012816 *  
...
v6:v9        0.185673   0.090535   2.051 0.040656 *  
v6:v10       0.216236   0.090258   2.396 0.016849 *  
...
v6:v17       0.255650   0.087794   2.912 0.003707 ** 
...
v6:v22      -0.198706   0.089566  -2.219 0.026838 *  
...
v7:v15      -0.192992   0.089102  -2.166 0.030652 *  
...
v8:v16       0.230660   0.090440   2.550 0.010971 *  
...
v9:v13      -0.190571   0.087136  -2.187 0.029070 *  
...
v9:v23       0.322755   0.088111   3.663 0.000268 ***
...
v12:v17      0.182697   0.090321   2.023 0.043480 *  
...
v12:v22     -0.229433   0.089649  -2.559 0.010700 *  
...
v13:v14      0.232871   0.086159   2.703 0.007043 ** 
...
v14:v15      0.206125   0.089665   2.299 0.021810 *  
...
v15:v16     -0.203924   0.091111  -2.238 0.025523 *  
...   
v22:v23     -0.184572   0.090622  -2.037 0.042055 *  
...
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2055 on 698 degrees of freedom
Multiple R-squared:  0.4369,    Adjusted R-squared:  0.1949 
F-statistic: 1.805 on 300 and 698 DF,  p-value: 1.863e-10
</code></pre>

<p>I have omitted multiple lines that are not significant at the 0.05 level or below in the output (denoted by ...) for brevity.</p>

<p>The residual plots for this model are shown below:
<img src=""http://i.imgur.com/VFk6ClY.png"" alt=""Second interaction model residuals""></p>

<p>The R-squared value is greatly improved (0.437 versus 0.076) but still nowhere close to what I think I need for a good model (R-squared > 0.8).</p>

<p>I am unsure of how to proceed - what to try next in order to get a better model.</p>
"
"0.0400480865731637","0.039253433598943","144650","<p>Numerous books and lecture slides start to discuss regression analysis as follows:<br>
$$Y=X\beta+\epsilon;\text{ where, } Y\sim N(X\beta, \sigma^2 ) \text{ and } \epsilon\sim N(0, \sigma^2)$$
George Seber wrote in his book (Linear Regression Analysis, Ed 2) at page 42:
$$Var[Y] = Var[Y-X\beta] = Var[\epsilon]$$
I was intended to verify it as follows:</p>

<pre><code>set.seed(123)
n=10000
int=rep(1,n)
x1=rnorm(n,5,3)
x2=rnorm(n, 20, 10)
x3=rnorm(n, -10, 2)
x=cbind(int,x1,x2, x3)
beta=c(10, 2, 0.5, 1.5)
err=rnorm(n, 0, 7)
y=x%*%beta+err
mean(err) # very close to zero
var(err) # very close to 49
mean(y) # very close to 15
var(y) # very close to 118
</code></pre>

<p>Somehow I triggered to check whether <code>R square</code> have a role in equating $Var[Y]$ and $Var[\epsilon]$. Then I checked:</p>

<pre><code>var(y)*(1-(summary(lm(y~x1+x2+x3))$r.squared)) # very close to 49
</code></pre>

<p>Yes it is. So $Var[\epsilon] = Var[Y]\times (1-R^2)$. I do not know what I am missing here! Why George Seber and many others never mention it? Certainly, I am wrong, not George Seber. But what is my mistake?<br>
Any lights on my confusion will be appreciated.<br>
Thanks.</p>
"
"0.0566365471788599","0.0555127381653369","144690","<p>I have high dimensional data (1000 genes for 100 patients). I have few clinical parameters for this data, some demographic data and some different metal levels in blood. In trying to evaluate how each gene responds, I'm trying to fit multiple linear regression using all these clinical, demographic and  metal parameters, using gene as my outcome variable. Since I have 1000's of these genes, I am not sure how to go about model selection. Some genes differentiate on race, some on other factors. Some have very low $R^2$ values, I tried to see a few dozens. I am trying to perform univariate analysis to eliminate few genes which are not useful and then take the rest for the multivariate approach.</p>
"
"0.0566365471788599","0.0555127381653369","145053","<p>I have a linear regression model:</p>

<pre><code>model &lt;- lm(data=df, var1~var2+var3+var4+var5+var6+var7)
</code></pre>

<p>Hypothesis about absence of heteroscedasticity is rejected as Breusch-Pagan test gives small p-value:</p>

<pre><code>bptest(model)$p.value
#BP 
#1.014577e-06
</code></pre>

<p>But when I use robust estimations for parameters of the model:</p>

<pre><code>library(""sandwich"")
coeftest(model, vcov. = vcovHC(model))
</code></pre>

<p>... for all parameters the value <code>Pr(&gt;|t|)</code> is reducing. So the coefficients seem to become more significant although robust estimators usualy do vice versa.</p>

<p>Would you explain the matter of why this happens? Or robust estimators don't reduce statistical significance of parameters for some special cases? Would you mention these cases?</p>

<p>Thank you.</p>
"
"NaN","NaN","145226","<p>Upon performing <em>binary logistic regression</em>, I have found <code>VIF</code>, using <code>R</code> programming, as follows:</p>

<pre><code>             GVIF Df  GVIF^(1/(2*Df))
agem     2.213242  3        1.141576
eduM     2.842857  3        1.190216
eduF     2.576725  3        1.170877
ageC     1.315301  1        1.146866
diarrhea 1.031031  1        1.015397
uweight  1.129919  1        1.062977
fever    1.033433  1        1.016579
res      1.341470  1        1.158218
dis      1.440215  6        1.030866
WI       2.610752  4        1.127446
nlc      2.407934  3        1.157730
</code></pre>

<p>Based on those results, should I remove <code>agem</code>, <code>eduM</code>, <code>eduF</code>, <code>WI</code> and <code>nlc</code> for multi-collinearity? Or do I need to apply another approach? Could anybody help me?</p>
"
"0.0490486886395286","0.0480754414848157","145315","<p>I have age as a covariate in my material. A continuous variable. The age varies between 18-70 years.</p>

<p>I'm into a logistic regression and do not really know how to treat the variable. As a linear effect or as a polynomial?</p>

<pre><code>   gender       passinggrade age    prog
1    man          FALSE      69     FRIST
2    man             NA      70     FRIST
3 woman             NA       65     FRIST
4 woman           TRUE       68      FRIST
5 woman             NA       65     NMFIK
6    man          FALSE      70     FRIST
</code></pre>

<p>my model;</p>

<pre><code>mod.fit&lt;-glm(passinggrade ~prog+gender+age,family=binomial,data=both)
</code></pre>

<p>summary(mod.fit)</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  2.42653    0.28096   8.636  &lt; 2e-16 ***
progLARAA    0.44931    0.25643   1.752 0.079746 .  
progNASTK   -0.15524    0.26472  -0.586 0.557597    
progNBFFK    0.12091    0.65460   0.185 0.853462    
progNBIBK   -0.18850    0.37656  -0.501 0.616659    
progNDATK   -2.84617    0.73077  -3.895 9.83e-05 ***
progNFYSK    0.64391    0.19634   3.280 0.001040 ** 
progNMATK    0.18424    0.16451   1.120 0.262733    
progNMETK    0.22433    0.29086   0.771 0.440554    
progNMFIK    0.38877    0.42152   0.922 0.356373    
progNSFYY    0.97205    0.29320   3.315 0.000915 ***
progSMEKK   -0.58043    0.18185  -3.192 0.001414 ** 
genderman   -0.05623    0.10477  -0.537 0.591496        
age         -0.11780    0.01028 -11.462  &lt; 2e-16 ***
</code></pre>

<p>how would you treat the variable age?
and how should I interpret the results for age?</p>
"
"0.105957277556576","0.0964366217361766","145684","<p>My problem (question at the end) is to calculate confidence interval (CI) (NOT prediction interval) of the response of a nonlinear model.</p>

<p>I am working with R but this question is not R-specific.</p>

<p>I want to model some data after the following equation (model):</p>

<p>Y ~ a * X^b/b</p>

<p>First, I estimate the parameters a and b through nonlinear regression (using R's ""nls()""), which yields estimates and error on the corresponding estimate.</p>

<pre><code> Nonlinear regression model
 model: Y ~ (A * X^B/B)
  data: data.frame(X = X, Y = Y)
    A      B 
  7.4154 0.6041 
   residual sum-of-squares: 88983
</code></pre>

<p>Then I calculate 95% CI for a and b (using confint(nlm &lt;- nls(Y ~  A * X^B/B, start=list(A=1,B=1)))</p>

<pre><code> &gt; confint(nlm)
 Waiting for profiling to be done...
         2.5%     97.5%
 A 1.21719414 11.549562
 B 0.08583486  1.482389
</code></pre>

<p>In order to calculate 95%CI for Y, given some fixed, certain value of X, my first idea was to propagate uncertainties on a and b to Y through the model equation. This yields some value for 95% CI of Y, given X.</p>

<p>I then came accross the ""propagate"" package that proposes to calculate 95%CI of Y, given X, ""based on asymptotic normality"" (citation from ""<a href=""http://127.0.0.1:22638/library/propagate/html/predictNLS.html"" rel=""nofollow"">http://127.0.0.1:22638/library/propagate/html/predictNLS.html</a>""). However this method yields a VERY different 95%CI. </p>

<p><strong>My question is: Why aren't these two CI equal ?</strong></p>

<p>A worked example (with some random equation that just crossed my mind):</p>

<p>Values needed for error propagation : A, CI(A), B, CI(B), X, CI(X) :</p>

<p>Parameters (A &amp; B)' estimates and 95%CI were calculated from 
     confint(nlm &lt;- nls(Y ~  A * X^B/B, start=list(A=1,B=1))</p>

<p>X was then fixed at 30 for the sake of the argument, and considered error-free.</p>

<pre><code>                A         B  X
 value   7.415380 0.6041404 30
 95% CI  5.166184 0.6982769  0
</code></pre>

<p>The general formula for uncertainties propagation (works for sd, se, ci95%) is :</p>

<p>Y=f(Ai | i = 1 to n)
=> delta(Y) = sqrt( sum( ( dY/dAi * delta(Ai) )^2 ) )</p>

<p>The equation being        Y =  A * X^B/B </p>

<p>Partial derivatives are then: </p>

<pre><code> dF/dA  =  X^B/B
 dF/dB  =  A * (X^B * log(X))/B - A * X^B/B^2
 dF/dX  =  A * (X^(B - 1) * B)/B
</code></pre>

<p>Then</p>

<pre><code> dF/dA = 12.9196498927581
 dF/dB = 167.269472901412
 dF/dX = 1.92930443474376
</code></pre>

<p>This yields</p>

<pre><code> Y = 95.8041099173585 +- 134.526084150286
</code></pre>

<p>However, when using the predictNLS() function from ""propagate"" R package:</p>

<pre><code> predictNLS(nlm, newdata=data.frame(X=30), interval = ""confidence"")$summary

 Propagating predictor value #1 ...
   Prop.Mean.1 Prop.Mean.2 Prop.sd.1 Prop.sd.2 Prop.2.5%
      95.80411    102.8339  20.89399  24.86949  51.89104
   Prop.97.5% Sim.Mean   Sim.sd Sim.Median  Sim.MAD  Sim.2.5%
     153.7767  93.5643 1712.894   97.85209 21.98703 -117.3541
   Sim.97.5%
    210.3916
</code></pre>

<p>Which yields</p>

<pre><code>    Y = 95.80411 +- (153.7767-51.89104)/2
 =&gt; Y = 95.80411 +- 50.94283
</code></pre>

<p>Obviously I must have missed / misunderstood some essential information about CI of response variable, because I believe the person who coded the predictNLS() function must be way more knowledgeable than me about it.</p>

<p>Thanks in advance for your explanations.</p>
"
"0.0490486886395286","0.0480754414848157","145799","<p>I was wondering why do I get linear model when I'm using exponential model,
<code>y = a * exp(-b*-x)</code>, to fit my data.</p>

<p>Here is my code:</p>

<pre><code>ff &lt;- function(x,a,b){a * exp(-b*-x)}
fit2 &lt;- nls(y ~ ff(x,a,b) , data = newdat, start =c(a=107.4623,b=-0.0037)
</code></pre>

<p>The graph below is mydata with the exponential fit (prediction of <code>fit2</code>) in purple curve. The green curve is what I though it would be, it is <code>Smooth.splines</code> fit.
<img src=""http://i.stack.imgur.com/qrUkj.png"" alt=""enter image description here""></p>

<p>Result from <code>fit2</code>:</p>

<pre><code>Nonlinear regression model
  model: dif2 ~ ff(age, a, b)
   data: newdat
         a          b 
109.743680  -0.003793 
 residual sum-of-squares: 2585

Number of iterations to convergence: 2 
Achieved convergence tolerance: 1.446e-06
</code></pre>

<p><img src=""http://i.stack.imgur.com/pdCrx.png"" alt=""enter image description here"">
Here is my data:</p>

<pre><code>   ID  x   y
    1 18 106.47
    1 19 100.35
    1 20 97.4
    1 21 101.03
    1 22 100.3
    1 23 99.06
    1 24 100.81
    2 18 101.95
    2 19 100.69
    2 20 100.89
    3 14 105.87
    3 15 107.44
    3 16 103.05
    3 17 104.86
    3 18 101.86
    3 19 101.48
    3 20 102.77
    3 21 99.63
    3 22 100.21
    3 23 101.28
    3 24 98.77
    3 25 99.91
    4 17 102.42
    4 18 101.85
    4 19 101.31
    5 18 101.24
    5 19 102.27
    5 20 100.03
    5 21 101.53
    6 20 98.08
    6 21 101.2
    6 22 103.16
    6 23 98.3
    6 24 102.21
    6 25 100.18
    6 27 95.28
    6 28 102.05
    6 29 100.72
    6 30 101.4
    7 13 111.3
    7 14 106.55
    7 15 103.23
    7 16 102.31
    7 17 101.11
    7 18 101.52
    7 19 100.14
    8 18 101.05
    8 19 98.15
    8 20 100.55
    8 21 101.62
    8 22 101.04
    8 23 98.22
    9 18 102.87
    9 19 101.46
    9 20 101.07
    9 21 101.32
    10 20 101.93
    10 21 101.73
    10 22 100.24
    11 19 99.75
    11 20 101.35
    11 21 99.34
    11 22 100.12
    12 18 103.34
    12 19 109.52
    12 20 106.98
    12 21 105.21
    12 22 98.87
    12 23 103.81
    12 24 100.38
    12 25 100.12
    12 26 99.7
    12 27 101.16
    12 28 99.02
    12 29 100.15
    12 30 97.32
    13 13 116.43
    13 14 111.75
    13 15 107.42
    13 16 103.5
    13 17 103.37
    13 18 100.66
    13 19 100.73
    13 20 100.84
    13 21 100.05
    14 18 101.66
    14 19 99.9
    14 20 101.4
    14 21 99.86
    14 22 100.82
    15 15 101.27
    15 16 100.01
    15 17 104.27
    16 19 100.26
    16 20 104.13
    17 18 106.12
    18 21 101.18
    18 22 99.51
    18 23 100.59
    19 18 100
    19 19 100.81
    19 20 99.37
    19 21 102.6
    20 22 102.18
    20 23 104.5
    20 24 100.74
    21 22 103.74
    21 23 98.66
    21 24 100.65
    21 25 99.63
    22 24 102.59
    22 25 94.62
    22 26 103.85
    23 20 100.7
    23 21 101.38
    23 22 102.36
    23 23 99.56
    23 24 100
    24 18 101.16
    24 19 99.64
    25 21 96.9
    25 22 109.3
    25 23 101.4
    25 24 98.04
    25 25 99.28
    25 26 99.63
    25 27 101.29
    25 28 100.08
    26 14 109
    26 15 112.37
    26 16 102.4
    26 17 102.15
    26 18 100.82
    27 18 101.14
    27 19 101.38
    28 17 105.09
    28 18 101.74
    28 19 100.2
    29 19 102.11
    29 20 100.57
    29 21 100.91
    29 22 99.61
    29 23 99.99
    30 18 99.81
    30 19 102.07
    31 19 100.75
    31 21 95.43
    32 23 99.73
    32 24 100.8
    32 25 100.1
    32 26 100.88
    32 27 97.73
    32 28 100.36
    33 22 99.4
    33 24 101.46
    33 18 97.65
    33 25 102.75
    33 26 97.7
    33 27 100.67
    34 21 98.27
    34 22 100.42
    34 23 101.16
    34 24 100.13
    34 25 98.55
    35 17 107.46
    35 18 100.22
    35 19 102.03
    35 20 101.52
    35 21 102.05
    35 22 102.46
    35 23 101.56
    35 24 96.88
    35 25 98.97
    35 26 101.68
    35 28 94.12
    36 20 98.63
    36 21 101.59
    36 22 98.76
    37 19 101.9
    37 20 98.66
    37 21 100.19
    37 22 100.03
    37 23 99.97
    38 15 104.32
    38 16 102.98
    38 17 103.4
    38 18 102.78
    38 19 101.73
    38 20 95.57
    39 22 101.5
    39 23 98.37
    39 24 100.4
    39 25 100.79
    40 19 102.93
    40 20 100.88
    40 21 99
    40 22 99.66
    41 21 107.08
    41 22 93.08
    41 24 100.91
    41 25 107.24
    41 26 99.8
    42 14 109.82
    42 15 106.09
    42 16 106.32
    42 17 102.8
    42 18 100.21
    42 19 102.08
    42 21 99.22
    42 22 100.13
    42 23 101.63
    43 16 100.95
    43 17 100.6
    43 18 101.81
    43 19 102.78
    43 20 98.43
    43 23 101.4
    43 24 103.12
    43 25 99.31
    43 26 100.47
    43 27 99.67
    43 28 98.75
    43 29 95.68
    44 23 103.78
    44 24 100.38
    44 25 99.39
    44 26 100.87
    44 27 99.64
    44 28 98.39
    44 29 97.62
    45 18 100.47
    45 19 101.41
    45 20 99.33
    45 21 101.08
    45 22 100.08
    45 23 100.22
    45 24 99.67
    45 25 100.45
    45 26 102.4
    45 27 95.7
    46 20 101.35
    46 21 98.73
    46 22 109.29
    46 23 100.04
    46 24 95.74
    46 25 100.44
    46 26 98.72
    47 19 100.51
    47 20 99.88
    47 21 101.7
    47 22 101.94
    47 23 100.72
    47 24 98.73
    47 25 102.16
    47 26 100.25
    47 27 95.1
    47 28 103.08
    48 25 105.21
    48 26 100.48
    48 27 98.07
    48 28 99.88
    48 29 95.61
    49 16 111.35
    49 17 92.43
    49 18 112.04
    49 19 100.8
    49 20 95.36
    49 21 103.13
    49 22 102.16
    49 23 98.81
    49 25 98.86
    49 26 99.93
    49 27 95.26
    50 23 98.15
    50 24 105.93
    50 25 99.01
    50 26 99.34
    50 27 93.68
    50 28 105.35
    51 24 100.96
    51 25 100.53
    51 26 99.2
    51 27 100.52
    51 28 100.86
    52 25 101.38
    52 26 98.45
    52 27 100.32
    52 28 99.24
    52 29 102.74
    53 24 101.37
    53 25 99.75
    53 27 96.31
    53 28 100.67
    54 22 98.09
    54 23 100.55
    54 24 100.25
    54 25 101.54
    54 26 98.48
    54 27 102.76
    54 28 98.5
    54 30 99.85
    55 22 103.87
    55 23 94.37
    55 24 105.12
    56 18 101.23
    56 19 99.26
    56 20 102.63
    56 21 100.75
    56 23 101.5
    56 24 99.14
    56 27 95.11
    57 16 107.57
    57 17 101.75
    57 18 107.18
    57 19 100.23
    57 20 105.48
    57 21 103.1
    57 22 100.45
    57 23 99.28
    57 24 100.52
    57 25 98.69
    58 27 103.13
    58 28 97.86
    58 29 101.33
    58 30 98.33
    58 32 102.14
    58 34 94.47
    58 35 98.29
    59 19 97.6
    59 20 98.93
    59 22 101.35
    59 23 93.88
    60 20 99.62
    60 22 97.36
    60 23 102.94
    60 24 98.98
    60 25 99.47
    61 18 100.15
    61 19 101.92
    61 20 101.34
    61 21 98.87
    61 22 97.68
    61 23 99.92
    61 24 100.78
    61 25 98.21
    62 20 102.7
    62 21 99.7
    62 22 100.17
    62 23 99.62
    62 24 100.59
</code></pre>
"
"0.136722856160266","0.149472621957822","145849","<p>Iâ€™ve got a question concerning the R package <em>strucchange</em> that I use for testing and dating structural breaks in my PhD thesis.  To be specific, I use the generalized fluctuation test framework with CUSUM/MOSUM and in particular Moving Estimates (<strong>ME</strong>) tests for my analysis. Thus, the following description focuses on the ME test, but in principle is more general to all fluctuation tests.</p>

<p><strong>The problem:</strong> I am testing time series data for structural breaks with the ME test that draws on the function <strong>efp</strong> provided by strucchange. Given the nature of time series data, I want to tackle potential heteroskedasticity and autocorrelation in the data. Strucchange provides some functionality with respect to calculating <em>heteroskedasticity</em> (<strong>HC</strong>) and <em>autocorrelation</em> (<strong>HAC</strong>) consistent covariance matrices,  e.g., the approaches suggested by Newey-West (1987) or Andrews (1991). </p>

<p>However, this functionality in strucchange is limited to the function <strong>gefp</strong> that calculates Generalized Empirical M-Fluctuation Processes that as far as I know does not allow to perform estimates-based tests such as the ME test. Thus, I cannot use <strong>efp</strong> to estimate ME tests (or other tests that are available in this function) using HAC covariance matrices. </p>

<p><strong>The question:</strong> Does anybody know how I could make use of the <strong>efp</strong> function in <em>strucchange</em> for testing and dating structural changes but use HAC covariance matrices to take heteroskedasticity and autocorrelation into account? Maybe there is some way to use the sandwich package for this?</p>

<p><strong>Many thanks for any help!</strong></p>

<p>Here is a minimal working example to show the problem</p>

<pre><code>library(foreign)
library(strucchange)

data(""Nile"")

#using the function efp to perform a moving estimates test
#assuming sperical disturbances
ocus.nile &lt;- efp(Nile ~ 1, type = ""ME"")
sctest(ocus.nile)

#applying the vcov function with the kernHAC option to take heteroskedasticity and autocorrelation does not work, i.e., the option is not used and the result is the same
sctest(ocus.nile, vcov=kernHAC)

#using the function gefp to perform a generalized M-fluctuation process however works with vcov
#assuming spherical disturbances
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm)
sctest(ocus.nile2)

#controlling for heteroskedasticity and autocorrelation using an appropriate covariance matrix changes the result, i.e. works
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm, vcov= kernHAC)
sctest(ocus.nile2)
</code></pre>

<p><strong>Some background</strong></p>

<p>Though probably not necessary, here is some more in-depth background about the problem for the interested reader (and the archive). The formulas are taken from Zeileis et al., 2005, â€Monitoring structural change in dynamic econometric modelsâ€. </p>

<p>The ME test is used to detect structural breaks in the standard linear regression model over time. What it does it in essence partitioning the data and rather than estimating the regression based on the whole sample, it sequentially moves â€œthroughâ€ time in a fixed-width windows containing only a sub-sample of the observations and in each window it estimates the model. These estimates are used to the computation of empirical fluctuation processes that capture fluctuations in regression coefficients and residuals over time. Significant fluctuations of the coefficients are signs of a structural break in the regression. The test statistic of the Moving estimates test is</p>

<p><img src=""http://i.stack.imgur.com/qQ0FY.png"" alt=""Moving estimates test statistic""></p>

<p>where <em>n</em> is the number of observations, <em>h</em> is the bandwith (how many percent of the total number of observations are used for the window), <em>nh</em> is thus the size of the window, Q_(n)=X_(n)^T X_(N)/n, i=[k+t(n-k)], and sigma^2 is an estimate of the variance. The way I understand the above statistic is that it compares the difference between the sub-sample estimate of beta with the whole sample (the window) estimate and how this difference develops over time. A zero difference would indicate a sub-sample estimate that perfectly equals the whole-sample estimate, which would indicate perfect stability of the coefficient. In my understanding, the efp function in strucchange calculates sigma^2 based on the standard OLS residuals u^ i.e., sigma^2=(1/n-k)âˆ‘_(i=1)^n u_i^2 . Thus, in the presence of heteroskedasticity or autocorrelation, the OLS assumption of spherical disturbances will be violated. Thus, ideally, sigma^2 should be estimated based on a HAC covariance matrix to avoid wrong inference.</p>

<p>The question that comes to my mind is whether there is a way to use the ME test based on a HAC estimate. If not, it seems to me that it is limited to spherical disturbances of residuals, which seems to be violated in most applications.</p>
"
"NaN","NaN","145870","<p>I need some assistance with a nonlinear adjust. I am trying to make a mathematical model that describes the rate of silicic acid escaping from an underwater sediment. For theoretical reasons, the equation resulting from the nonlinear regression must be exponential and have the form $Y = A + Be^{CX}$, in which $A$, $B$ and $C$ are constants, $X$ is an independent predictor (time) and $Y$ is a dependent variable (concentration). I lack but the basic understanding of statistical software, so I have just tried to make it on Excel. The problem is that there I can only adjust my data to equations like $Y = Ae^{Bx}$, that are without a constant. I know for sure that R with a specific package will work just fine for this problem, but I cannot find any tutorials to orientate me. I guess that there is a really easy answer for this almost trivial question, but for now it is eluding me. Thank you very much for your time and help!</p>
"
"0.0633215847514023","0.0496521024619361","145949","<p>I'm doing a linear regression, in R. The values are like this -</p>

<pre><code>u  &lt;-  c(1,2,3,4,5,6,7,8,9,10)
v &lt;- c(21,22,23,24,25,26,27,28,29,30)
w &lt;- c(41,42,43,44,45,46,47,48,49,50)
y &lt;- c(128.2305,132.4040,140.1732,147.3236, 154.5410, 158.7206, 165.1761, 169.7121,178.9751,181.0309)
</code></pre>

<p>If I call linear regression function, it's returning a model, which is disregarding v and w.</p>

<pre><code>model &lt;- lm(y~u+v+w)

Coefficients:
(Intercept)            u            v            w   
    122.074        6.101           NA           NA  

summary(model)
</code></pre>

<p>Output: </p>

<pre><code>Call:
lm(formula = y ~ u + v + w)

Residuals:
       Min       1Q   Median       3Q      Max 
-2.05143 -0.92734  0.04845  0.73362  1.99357 

Coefficients: (2 not defined because of singularities)
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 122.0743     1.0197  119.72 2.65e-14 ***
u             6.1008     0.1643   37.12 3.04e-10 ***
v                 NA         NA      NA       NA    
w                 NA         NA      NA       NA    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.493 on 8 degrees of freedom
Multiple R-squared:  0.9942,    Adjusted R-squared:  0.9935 
F-statistic:  1378 on 1 and 8 DF,  p-value: 3.04e-10
</code></pre>

<p>I tried to fit a linear model before with different values of y,u,v (with two predictor variables, w was absent), and there also, v was being assigned NA, and only u was getting co-efficients. What's happening?</p>
"
"0.0400480865731637","0.039253433598943","145977","<p>I am working on Boston data set from MASS library. I separated the training and test data (70 / 30)</p>

<p>In order to train my data, should I run linear regression multiple times on training data? Is this what training a dataset means? I'm using <code>lm</code> function in R to do this. Here is my code:</p>

<pre><code>smp_size &lt;- floor(.7 * nrow(Boston))

set.seed(133)
train_boston &lt;- sample(seq_len(nrow(Boston)), size=smp_size)
train_ind    &lt;- sample(seq_len(nrow(Boston)), size=smp_size)
train_boston &lt;- Boston[train_ind, ]
test_boston  &lt;- Boston[-train_ind,]
nrow(train_boston)   # [1] 354
nrow(test_boston)    # [1] 152
train_boston.lm &lt;- lm(lstat~medv, train_boston)
summary(train_boston.lm)
</code></pre>
"
"0.0895502439463906","0.0789960112897596","146046","<p>I'm investigating whether there is a relationship between the day of the week and an outcome value using linear regression in <code>R</code>, and would like to understand how to interpret the residual plots.</p>

<p><strong>Data</strong></p>

<p>Example dummy data (the mean and SD are based on actual data I have): </p>

<pre><code>set.seed(14)
mon &lt;- data.frame(id=seq(6, 60*7, by=7), value = rnorm(60, 4372, 145))
tue &lt;- data.frame(id=seq(7, 60*7, by=7), value = rnorm(60, 4433, 206))
wed &lt;- data.frame(id=seq(1, 60*7, by=7), value = rnorm(60, 4671, 143))
thu &lt;- data.frame(id=seq(2, 60*7, by=7), value = rnorm(60, 4555, 154))
fri &lt;- data.frame(id=seq(3, 60*7, by=7), value = rnorm(60, 4268, 149))
sat &lt;- data.frame(id=seq(4, 60*7, by=7), value = rnorm(60, 1579, 110))
sun &lt;- data.frame(id=seq(5, 60*7, by=7), value = rnorm(60, 1136, 68))
startdate &lt;- seq.Date(as.Date(""2014-01-01""), by=""day"", length.out=(60*7) )
id &lt;- seq(1, 60*7)
wd &lt;- weekdays(startdate)
df &lt;- data.frame(id, startdate, wd)
days &lt;- rbind(mon, tue, wed, thu, fri, sat, sun)
df &lt;- merge(df, days)

head(df)
  id  startdate        wd    value
1  1 2014-01-01 Wednesday 4593.117
2  2 2014-01-02  Thursday 4686.159
3  3 2014-01-03    Friday 4352.982
4  4 2014-01-04  Saturday 1825.172
5  5 2014-01-05    Sunday 1206.759
6  6 2014-01-06    Monday 4276.032
</code></pre>

<p>which looks like</p>

<pre><code>library(ggplot2)
ggplot(data=df, aes(x=startdate, y=value, colour=wd)) +
  geom_point() +
  geom_smooth( alpha=.3, size=1, aes(fill=wd)) +
  facet_wrap(~wd) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/lX6CG.png"" alt=""data plot""></p>

<p><strong>Model</strong></p>

<p>Modelling the data using <code>fit &lt;- lm(data=df, value ~ wd)</code> produces the coefficients:</p>

<pre><code>summary(fit)
....
Coefficients:
             Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept)  4242.60      18.83  225.319  &lt; 2e-16 ***
wdMonday      148.93      26.63    5.593 4.07e-08 ***
wdSaturday  -2661.93      26.63  -99.965  &lt; 2e-16 ***
wdSunday    -3113.78      26.63 -116.933  &lt; 2e-16 ***
wdThursday    299.65      26.63   11.253  &lt; 2e-16 ***
wdTuesday     189.04      26.63    7.099 5.51e-12 ***
wdWednesday   412.52      26.63   15.492  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 145.9 on 413 degrees of freedom
Multiple R-squared:  0.9896,    Adjusted R-squared:  0.9894 
F-statistic:  6539 on 6 and 413 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>The plot of the data, and the coefficients seem to suggests there is a relationship between day of the week and the outcome value. </p>

<p>However, I know I also need to consider the residual plots when interpreting the validity of a model. For this example the residual plots are:</p>

<pre><code>par(mfrow=c(2,2))
plot(fit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/gzpGZ.png"" alt=""residual plots""></p>

<p><strong>Question</strong></p>

<p>Through various stats courses/uni/research (e.g. <a href=""http://stats.stackexchange.com/questions/76226/interpreting-the-residuals-vs-fitted-values-plot-for-verifying-the-assumptions"">this question</a>) I know that for a good linear model you are looking for unbiased homoscedastic residuals. But my knowledge on this subject is a bit rusty. Therefore, do my residuals suggest a linear model is not an appropriate fit for the data? And/or is there another aspect to this that I should be considering, or have I completely missed the point all together?</p>
"
"0.129770632274032","0.102967917131134","146421","<p>I am using rlm robust linear regression of MASS package on modified iris data set as follows:</p>

<pre><code>&gt; myiris = iris
&gt; myiris$Species = as.numeric(myiris$Species)
&gt; head(myiris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2       1
2          4.9         3.0          1.4         0.2       1
3          4.7         3.2          1.3         0.2       1
4          4.6         3.1          1.5         0.2       1
5          5.0         3.6          1.4         0.2       1
6          5.4         3.9          1.7         0.4       1

&gt; library(MASS)
&gt; rmod = rlm(Species~., data=myiris)
&gt; rmod
Call:
rlm(formula = Species ~ ., data = myiris)
Converged in 6 iterations

Coefficients:
 (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
  1.14943807  -0.11067690  -0.02603537   0.21581357   0.63793686 

Degrees of freedom: 150 total; 145 residual
Scale estimate: 0.191 
&gt; 
&gt; sumrmod = summary(rmod)
&gt; sumrmod

Call: rlm(formula = Species ~ ., data = myiris)
Residuals:
     Min       1Q   Median       3Q      Max 
-0.59732 -0.15769  0.01089  0.10955  0.56317 

Coefficients:
             Value   Std. Error t value
(Intercept)   1.1494  0.2056     5.5906
Sepal.Length -0.1107  0.0579    -1.9128
Sepal.Width  -0.0260  0.0599    -0.4346
Petal.Length  0.2158  0.0571     3.7821
Petal.Width   0.6379  0.0948     6.7287

Residual standard error: 0.1913 on 145 degrees of freedom
</code></pre>

<p>This does not give p.values so I calculated them as follows (using pt function of base R):</p>

<pre><code>&gt; dd = data.frame(sumrmod$coefficients)                             #$
&gt; dd$p.value =  pt(dd$t.value, sumrmod$df[2])                       #$
&gt; dd
                   Value Std..Error    t.value    p.value
(Intercept)   1.14943807 0.20560264  5.5905804 0.99999995
Sepal.Length -0.11067690 0.05786107 -1.9128044 0.02887227
Sepal.Width  -0.02603537 0.05991073 -0.4345693 0.33226054
Petal.Length  0.21581357 0.05706173  3.7821068 0.99988663
Petal.Width   0.63793686 0.09480869  6.7286751 1.00000000
</code></pre>

<p>However, these are not correct since ordinary lm function and other regression functions show that Petal.Length and Petal.Width are highly significant in this regression:</p>

<pre><code>&gt; summary(lm(Species~., data=myiris))

Call:
lm(formula = Species ~ ., data = myiris)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.59215 -0.15368  0.01268  0.11089  0.55077 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.18650    0.20484   5.792 4.15e-08 ***
Sepal.Length -0.11191    0.05765  -1.941   0.0542 .  
Sepal.Width  -0.04008    0.05969  -0.671   0.5030    
Petal.Length  0.22865    0.05685   4.022 9.26e-05 ***
Petal.Width   0.60925    0.09446   6.450 1.56e-09 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2191 on 145 degrees of freedom
Multiple R-squared:  0.9304,    Adjusted R-squared:  0.9285 
F-statistic: 484.5 on 4 and 145 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Where is the error? Am I not using correct method to calculate p.value here?</p>

<p>Edit: As suggested (further) by @Glen_b in the comments: </p>

<pre><code>&gt; dd$p.value =  2*pt(abs(dd$t.value), sumrmod$df[2], lower.tail=FALSE)      #$
&gt; dd
               Value Std..Error    t.value      p.value
(Intercept)   1.14943807 0.20560264  5.5905804 1.089792e-07
Sepal.Length -0.11067690 0.05786107 -1.9128044 5.774455e-02
Sepal.Width  -0.02603537 0.05991073 -0.4345693 6.645211e-01
Petal.Length  0.21581357 0.05706173  3.7821068 2.267410e-04
Petal.Width   0.63793686 0.09480869  6.7286751 3.691993e-10
</code></pre>

<p>These seem to be correct (finally).</p>
"
"0.0749231094763201","0.0734364498908627","146431","<p>In the output of felm function which is a function for the Linear Models with Multiple Fixed Effects, two R-squared information are provided: Multiple R-squared(full model) and Multiple R-squared(proj model).</p>

<p>How to interpret the Multiple R-squared(proj model)? I guess the R-squared(full model) refer to the effect of all the variables (x1, f1, f2 and f3) in the model, in which f1, f2 and f3 serves like categorical variables in regression. And Multiple R-squared(proj model) refers to the effect of purely f1, f2 and f3 when x1 is not included in the model. Am I right? I need to understand this in order to calculate the effect size of the independent variable (x1 in this case). Thanks!</p>

<pre><code>   summary(est &lt;- felm(y ~ x1 | f1 + f2 + f3))

Call:
   felm(formula = y ~ x1 | f1 + f2 + f3) 

Residuals:
     Min       1Q   Median       3Q      Max 
-2.35266 -0.57436 -0.00792  0.61786  2.13316 

Coefficients:
   Estimate Std. Error t value Pr(&gt;|t|)    
x1   2.4043     0.1217   19.75   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    Residual standard error: 0.9832 on 76 degrees of freedom
    Multiple R-squared(full model): 0.9058   Adjusted R-squared: 0.8773 
    Multiple R-squared(proj model): 0.8369   Adjusted R-squared: 0.7876 
    F-statistic(full model):31.79 on 23 and 76 DF, p-value: &lt; 2.2e-16 
    F-statistic(proj model): 390.1 on 1 and 76 DF, p-value: &lt; 2.2e-16 
    *** Standard errors may be too high due to more than 2 groups and exactDOF=FALSE
</code></pre>
"
"0.02831827358943","0.0277563690826684","146521","<p>Just for ""train"" with linear regression in <code>R</code> I'm doing a <code>Durbin-Watson test</code> over the residuals of a regression (over stock prices) comparing these with their value at t-1 (lag=1). From my data it's clear that residuals shows a strong autocorrelation. But I understood that from the autoregressive process on the residuals (regressor=1 and R square very close to 1) but I don't understand the <code>Durbin-Watson output</code>, that is:</p>

<pre><code>        Durbin-Watson test

data:  residui[, 1] ~ residui[, 2]
DW = 1.91, p-value = 0.01888
alternative hypothesis: true autocorrelation is greater than 0
</code></pre>
"
"0.0633215847514023","0.0620651280774201","146699","<p>I have residuals from a linear regression model on my data set. I want to find an appropriate distribution of my residuals.  </p>

<ol>
<li><p>Say, I assume my residuals are Skew-T Distributed, how can I find the (best)<br>
distribution parameters by Fitting?  </p></li>
<li><p>If the sample distribution of my residuals indicate bimodality, how can I determine an appropriate distribution?  </p></li>
<li><p>How can I model kurtotic behaviour and long (but not heavy) tails of my residuals ?</p></li>
<li>How can I do this in R?</li>
<li>How can I model Heteroscedasticity of data?</li>
</ol>
"
"0.0716401951571125","0.0877733458775107","146853","<p><strong>What I am doing so far:</strong></p>

<p>I am doing a constraint linear regression with R's <code>quadprog</code> package, function <code>solve.QP()</code>. The regression does not have an intercept $\alpha$, therefore the objective function can be stated by $$min_{b} (Y-Xb)^\top(Y-Xb)$$ which is the squared residuals. </p>

<p>Quadprog optimizes the function $$min_{b}\Big(\frac{1}{2}b^\top Db-d^\top b\Big)$$ Therefore I have to transform the first function into the second one. The end result is $$\frac{1}{2}b^\top X^\top Xb-Y^\top Xb$$ where $X^\top X =:D$ and $Y^\top X=:d$.</p>

<p>There are two risk factors in this example (hence Y is a vector of the dependent variable and X is a 2-dim matrix of the independent variables), whereas the first one is restricted to be greater or equal to -10 and the second one greater or equal to zero. The code for this is the following:</p>

<pre><code>require(""quadprog"")

Dmat = t(X) %*% X
Amat = t(diag(2))
bvec = c(-10,0)
dvec = t(Y) %*% X

solve.QP(Dmat = Dmat, dvec = dvec, Amat = Amat, bvec = bvec, meq = 0, factorized = F)
</code></pre>

<hr>

<p><strong>What I want to add:</strong></p>

<p>I want to add penalties to the regression in order to replicate a Lasso regression. Therefore, the initial objective function has to be expanded by the penalty term $\lambda |b|$ $$min_{b} (Y-Xb)^\top(Y-Xb)+\lambda |b|$$ 
In order to bring it into the form usable by the algorithm, I form it into $$\frac{1}{2}b^\top X^\top Xb-(Y^\top X-\frac{1}{2}\lambda)|b|$$
The penalty term $\lambda$ is a vector with two entries $\lambda=(80.56,5.65)$. However, when I now run the algorithm, the objective function gets negative and $b_1$ will be $b_1 = -10$, which is the most negative piossible value allowed by the constraints. $b_2$ will be $b_2=0$. </p>

<p>These results are not equal to results I get with the <code>glmnet</code> package which allows me to perform a Lasso regression with the same penalties. Those results have been checked and are correct. Hence, I do not know why the quadprog algorithm delivers different results. Any hints? Is the objective function wrong? Did I specify any input parameter for <code>quadprog</code> incorrectly? </p>
"
"NaN","NaN","146959","<p>So the <a href=""http://stats.stackexchange.com/questions/143394/help-constructing-a-simple-regression-model-with-a-breakpoint"">saga</a> continues...</p>

<p>So I am trying to fit the model
$$Runoff = \begin{cases}
\beta_0 + \beta_1Pcp &amp; \text{if $(Ant + Pcp) &lt; Thold$;}\\
\beta_2 + \beta_3Pcp &amp; \text{if $(Ant + Pcp) \geq Thold$;}
\end{cases}$$
Where $Pcp$ and $Ant$ are observed variables, and $Thold$ and the other parameters are estimated. I do not think I can do this with the <em>segmented</em> package, as the breakpoint is not in the predictor variables, but I think I should be able to do it fairly simply as a non-linear least-squares regression? I am not so good with the nls syntax, so any help would be appreciated</p>
"
"0.0490486886395286","0.0480754414848157","146970","<p>I'm interested in visualising variance partitioning in the context of linear models. Say you run an linear regression that predicts peoples weight based on their height and age. How can you visualise the amount of the variation in people's weight that is explained by their height, the amount of variation that is explained by their weight and the amount of variation that is unexplained?</p>
"
"0.0849548207682898","0.0740169842204492","147119","<p>I have a plot of residual values of a linear model in function of the fitted values where the heteroscedasticity is very clear. However I'm not sure how I should proceed now because as far as I understand this heteroscedasticity makes my linear model invalid. (Is that right?)</p>

<ol>
<li><p>Use robust linear fitting using the <code>rlm()</code> function of the <code>MASS</code> package because it's apparently robust to heteroscedasticity. </p></li>
<li><p>As the standard errors of my coefficients are wrong because of the heteroscedasticity, I can just adjust the standard errors to be robust to the heteroscedasticity? Using the method posted on Stack Overflow here: <a href=""http://stackoverflow.com/questions/4385436/regression-with-heteroskedasticity-corrected-standard-errors"">Regression with Heteroskedasticity Corrected Standard Errors</a></p></li>
</ol>

<p>Which would be the best method to use to deal with my problem? If I use solution 2 is my predicting capability of my model completely useless?</p>

<p>The Breusch-Pagan test confirmed that the variance is not constant.</p>

<p>My residuals in function of the fitted values looks like this:  </p>

<p><a href=""http://i.stack.imgur.com/OtlGC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/OtlGC.png"" alt=""http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png""></a> </p>

<p>(larger version)</p>

<p><img src=""http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png"" width=""600""></p>
"
"0.0749231094763201","0.0629455284778823","147170","<p>I have a plot of residual values of a linear model in function of the fitted values where the heteroscedasticity is very clear. However I'm not sure how I should proceed now because as far as I understand this heteroscedasticity makes my linear model invalid right?</p>

<p>So I have been doing some reading about this subjet and I found two suggestions in other stackoverflow threads.</p>

<p>1) Use robut linear fitting using the rlm() function of the MASS package because it's apparently robust to heteroscedasticity. </p>

<p>2) As the standard errors of my coefficients are wrong because of the heteroscedasticity, I can just adjust the standard errors to be robust to the heteroskedasticity? Using the method posted <a href=""http://stackoverflow.com/questions/4385436/regression-with-heteroskedasticity-corrected-standard-errors"">here</a></p>

<p>Which would be the best method to use to deal with my problem?
If I use solution 2 is my predicting capability of my model completely useless?</p>

<p>My residuals in function of the fitted values looks like this <a href=""http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png"" rel=""nofollow"">http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png</a> and the Breusch-Pagan test confirmed that the variance is not constant.</p>
"
"0.0749231094763201","0.0629455284778823","147530","<p>I'm trying to perform a lagged linear regression on time series data sourced from ~10,000 hospital patients, for the purpose of estimating causal relationships between administration of a drug and a certain physiological response. For example: Do non-steroidal anti inflammatory drugs cause hypertension?</p>

<p>Basically, the linear model I'm trying to fit is like this:</p>

<p><img src=""http://i.stack.imgur.com/qukXs.png"" alt=""AR/cross-correlation model""></p>

<p>This assumes a maximum of 30 lags. $y$ represents hypertension, $x$ is taking the drug, and $h$ is whether the patient is admitted or not (a covariate). </p>

<p><strong>My question is this</strong>: Given a unique time series for <em>each patient</em> (all truncated to the same length of 30 time points), how can I pool all of the time series data together to estimate things like the cross-correlation (e.g., using <code>ccf</code>) and auto-correlation (<code>acf</code>) over the entire data set? If I were just trying to fit a linear model, this can be done relatively easily using something like the <code>plm</code> library, but I haven't been able to find anything similar for single functions.</p>

<p>For reference, here is a very small example of what my data set looks like (note that I only retained 6 of the 30 total time points for each patient, for brevity):</p>

<pre><code>   patient_id            time     nsaid hypertension admission
1           1               1 0.4427955    0.0000000 0.0000000
2           1               2 1.0000000    0.2097246 0.0000000
3           1               3 0.0000000    0.4916697 0.0000000
4           1               4 0.0000000    1.0000000 0.0000000
5           1               5 0.0000000    0.7902754 0.0000000
6           1               6 0.0000000    0.0000000 0.0000000
7           2               1 0.0000000    0.0000000 0.0000000
8           2               2 0.4104132    0.0000000 0.0000000
9           2               3 0.8236088    0.0000000 1.0000000
10          2               4 1.0000000    0.0000000 0.6994038
11          2               5 0.5895868    0.0000000 0.0000000
12          2               6 0.1763912    0.0000000 0.0000000
</code></pre>
"
"0.0633215847514023","0.0620651280774201","147554","<p>I am currently stuck with a problem regarding predictions from linear regressions. I estimated a simple (multivariate) regression model y = b0 + b1 * x + b2 * X, where x is my variable of interest and X is a matrix of controls, using the <code>lm()</code> fct in R.</p>

<p>Now I want to predict y for two different values of x. Finally, I want to know whether those two predictions of y are statistically different from each other. </p>

<p>So far, I used <code>predict(model, se.fit = TRUE, interval = ""prediction"")</code> and got a point prediction as well as the corresponding prediction interval. Using the prediction intervals of the two points, I decided whether there are statistically different based on the overlapping of the prediction intervals.</p>

<p>I got almost no significant differences using this technique even when the estimated coefficients are significant. Is this the right track, or are there different techniques one can use?</p>

<p>Thanks for your help!</p>
"
"NaN","NaN","147593","<p>Consider the following scenario where you use the same data <code>X</code> (the same number of predictors <code>p</code>, same number of observations <code>n</code>) to predict a continuous outcome <code>y</code>, in 2 different regression models (e.g. Linear Regression and Random Forests).
For both models you calculate <code>RMSE</code> and <code>R-squared</code> (assume simple correlation between y and y-hat, squared).</p>

<p>You get:</p>

<pre><code>RMSE1 &lt; RMSE2

R-squared1 &lt; R-squared2
</code></pre>

<p>Could someone explain if this scenario is possible and how it occurs?
A simple simulation in <code>R</code> could also do.</p>
"
"0.0490486886395286","0.0480754414848157","147843","<p>Take this example output.</p>

<pre><code>Residuals:
     Min       1Q   Median       3Q      Max 
-2.35775 -0.49911  0.07299  0.58762  2.54753 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 26.31154    0.44184  59.550  &lt; 2e-16 ***
x        2.52228    0.28907   8.726 4.95e-13 ***
I(y^2)  -0.52882    0.04042 -13.082  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.8906 on 75 degrees of freedom
Multiple R-squared:  0.8838,    Adjusted R-squared:  0.8807 
F-statistic: 285.1 on 2 and 75 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>I know how to change the standard error for the coefficients for a linear model to be robust  to heteroscedasticity using coeftest(regression_result, df = Inf, var_cov)</p>

<p>But what about the residual standard error of the model that you see when you call summary(lm()) at the end of the output, don't I need to compensate that as well to be robust to heteroscedasticity? How do I do that in R?</p>
"
"NaN","NaN","147904","<p>This is covariate age in my logistic regression. How should I treat it? Gets a little insecure. Have tried to read, but still insecure. 
Right now I treat it as if it were linear. A polynomial is not appropriate.
Some tips? I have a binary response variable</p>

<p><img src=""http://i.stack.imgur.com/dmg3i.png"" alt=""enter image description here""></p>

<p>x axis is age, the y axis is the percentage of approved students first semester</p>
"
"0.0633215847514023","0.0496521024619361","147958","<p>As a result of linear regression, we can have its residual and see its plot to check whether it shows normal distributed or not as follows :</p>

<pre><code>library(car)
m1 &lt;- lm(mpg~disp+hp+wt+drat, data=mtcars)  #Create a linear model
resid(m1) #List of residuals
plot(density(resid(m1))) #A density plot
qqnorm(resid(m1)) # A quantile normal plot - good for checking normality
qqline(resid(m1))
</code></pre>

<p>We can have std for each parameters, however, it seems I could not find the source to create linear regression residuals for each parameters.</p>

<p>residualPlots function (in car package) is fine, but it would be nice if I could have its distribution or boxplot as well.</p>

<p>Let me know how I can plot each parameters in R.</p>
"
"0.02831827358943","0.0277563690826684","148859","<p>I have a variable I do not know how I should handle my logistic regression.
The variable is the number of registered students each semester.
If I plot it against my binary outcome, I get the following plot:
<img src=""http://i.stack.imgur.com/cdeEd.png"" alt=""enter image description here""></p>

<p><strong>what kind of explanatory variables should I use? Linear, polynomial, categorical? I feel myself confused when it looks like this and would therefore need some tips.</strong></p>
"
"0.0942489115008991","0.0846805485716084","149064","<p>I have a nominal categorical predictor and a continuous dependent variable..I want to perform linear regression using lm in R. If the contrasts are such that the resulting dummy variables are uncorrelated then the regression is merely the direct linear combination of dummy variables weighted by their respective coefficients obtained from regression  of continuous variable with individual dummy variable..To have this advantage what way should the categorical predictor be contrast coded?I found this method <a href=""http://www.psychstat.missouristate.edu/multibook/mlt08m.html"" rel=""nofollow"">here</a> ..
It is helpful but the only problem is the order seems to be important here..The relation between only adjacent categories can  be interpreted from the result of linear regression..</p>

<p>So my question is - for nominal categorical predictor is there anyway to get good insights about dependent variable at category level of the predictor  from regression analysis.</p>

<p><strong>edit</strong> :</p>

<p>I'd like to  provide some clarifications here</p>

<p>Why do i need uncorrelated dummies? </p>

<p>bcoz in case of uncorrelated dummies i need not worry about which dummy enters the regression model first. The p value for the dummy1 is different when it enters the model second when compared to that when it enters first..By 'enters the model' i mean stepwise linear regression..So to avoid that problems i want them to be uncorrelated.</p>

<p>But if you see the pain vs treatment regression from the link provided by me the order certainly matters while doing contrast coding..I have no prior knowledge of the categories of my nominal category variable..so i cant order them like in pain vs treatment case. For more details - my dependent variable is Sales and category variable is product category which has 15 categories.</p>
"
"0.0490486886395286","0.0320502943232105","149352","<p>I would be very thankful if somebody could help me and explain the answer in simple terms.</p>

<p>I have y, x, z variables. They are countinuous, no missing values. y is dependent variable, x and z independent. There is significant correlation between them (varies from 0.25 to 0.35).</p>

<p>My hypothesis is that when z is very high, then x and y dependency is linear (x goes bigger, y smaller).
When z is very low, then in the beginning the situation is the same (dependency is linear), but in the end (about last quarter) it changes to quadratic.</p>

<p>So basically there are two different relationships between x and y, but the nature of the relationship depends on z which is again a continuous variable. </p>

<p>How can I test this model? Is it possible doing linear regression or do I need to use something else?</p>
"
"0.02831827358943","0.0277563690826684","149413","<p>I created a spreadlevel plot on my simple linear regression model in R. Here is my code,</p>

<pre><code>spreadLevelPlot(ols_reg)
</code></pre>

<p>where <code>ols_reg</code> is my regression model, <code>ols_reg &lt;- lm(y~0+.,dat)</code>. I first encountered spread-level plots from this <a href=""http://www.statmethods.net/stats/rdiagnostics.html"" rel=""nofollow"">link</a>, but I don't fully understand how to read the plot and what to do with the transformation this function provides. Suggested power transformation: 0.718 </p>

<p>This is how my plot looks,</p>

<p><img src=""http://i.stack.imgur.com/DEp1O.jpg"" alt=""enter image description here""></p>

<p>How do I read it?</p>

<p>These are the libraries I included in my script.</p>

<pre><code>library(car)
library(MASS)
library(lmtest)
</code></pre>
"
"NaN","NaN","149440","<p>This relates to using bootstrapping of residuals in regression.</p>

<p>I do not understand the point of this procedure, i.e. bootstrapping the errors and then adding the residual back to the predicted value. (Please correct me if I am mistaken here).</p>

<p>Is there some way that doing the aforementioned method would include non linearity in the otherwise linear regression model? </p>
"
"0.0755153962384799","0.0832691072480053","149627","<p>I want to make a piecewise linear regression in R. I have a large dataset with 3 segments where I want the first and third segment to be without slope, i.e. parallel to x-axis and I also want the regression to be continuous. I have a small example dataset and example code below. I'm rather new to R and have tried to write the model formula without success. Can anyone help with the formula and how to extract intercept and slope for the different segments? Three specific questions are inserted as comments in the code. I have looked at package segmented but are unable to understand how to constrain segment 1 and 3 to be parallel to x-axis.</p>

<pre><code>#Example data
y &lt;- c(4.5,4.3,2.57,4.40,4.52,1.39,4.15,3.55,2.49,4.27,4.42,4.10,2.21,2.90,1.42,1.50,1.45,1.7,4.6,3.8,1.9)  
x &lt;- c(320,419,650,340,400,800,300,570,720,480,425,460,675,600,850,920,975,1022,450,520,780)  
plot(x, y, col=""black"",pch=16)

#model 1 not continuous, Q1: how to get that?
fit1&lt;-lm(y ~ I(x&lt;500)+I((x&gt;=500&amp;x&lt;800)*x) + I(x&gt;=800)) 
summary(fit1) #intercepts and slopes extracted from summary(fit1)
lines(c(min(x),500),c(round(summary(fit1)[[4]][1]+summary(fit1)[[4]][2],2),round(summary(fit1)[[4]][1]+summary(fit1)[[4]][2],2)),col=""red"")
lines(c(800,max(x)),c(round(summary(fit1)[[4]][1]+summary(fit1)[[4]][4],2),round(summary(fit1)[[4]][1]+summary(fit1)[[4]][4],2)),col=""red"")
lines(c(500,800),c((summary(fit1)[[4]][1])+(500*(summary(fit1)[[4]][3])),
               (summary(fit1)[[4]][1])+(800*(summary(fit1)[[4]][3]))),col=""red"")

#model 2 continuous but first and third segment not parallell to x-axis, Q2: how to get that?
fit2&lt;-lm(y ~ x + I(pmax(x-500,0)) + I(pmax(x-800,0)))                                                  
summary(fit2) # Q3: how to get intercept and slope from summary(fit2) for model2?
mg &lt;- seq(min(x),max(x),length=400)
mpv &lt;- predict(fit2, newdata=data.frame(x = mg,t = (mg - 800)*as.numeric(mg &gt; 800)))
lines(mg, mpv,col=""blue"")
</code></pre>
"
"0.0490486886395286","0.0320502943232105","149908","<p>The function <code>powerTranform</code> from the ""car"" package in R mentions the following code for Box-Cox transformation for multiple regression: </p>

<pre><code>summary(p1 &lt;- powerTransform(cycles ~ len + amp + load, Wool))
# fit linear model with transformed response:
coef(p1, round=TRUE)
summary(m1 &lt;- lm(bcPower(cycles, p1$roundlam) ~ len + amp + load, Wool))
</code></pre>

<p>Is it sensible to apply Box-Cox method to just the dependent variable (and not the whole formula) and proceed with the regression: </p>

<pre><code>library(fifer)
cycles = boxcoxR(cycles)
summary(m1 &lt;- lm(cycles ~ len + amp + load, Wool))
</code></pre>

<p>I suspect this method is not right but I am not sure.</p>
"
"0.0490486886395286","0.0480754414848157","151547","<p>I am looking for examples where linear regression analysis is used in answering real problems. That is, from formulating real questions as a statistical question, validating assumptions so on to making conclusions.</p>

<p>I think I have a reasonable understanding of various statistical methods. I want to be able to put them in a ordered fashion.     </p>

<p>EDIT: I guess I am looking for examples like <a href=""http://www.stat.ncsu.edu/people/nelson/courses/st512/Multiple%20Regression%20Example.pdf"" rel=""nofollow"">this one (pdf)</a>. In this they look for the relative importance of variables and see whether they can drop any variables or need additional variables. Some examples that include concepts of mediation, controlling for variables, so on would be great. Even if they are in books or research articles. </p>
"
"0.0490486886395286","0.0480754414848157","151595","<p>I am using R to run a linear regression. I have a group of 3 dummy variables that represent 4 plots of land (labeled as group 1, 2, 3, and 4). I would like to set Group 4 as the control group when I generate the dummy variables, but R sets group 1 as the control by default. I have tried changing the contrasts.arg, but that does not appear to work.</p>

<pre><code>ferti.f = factor(ferti)
model.matrix(~ferti.f, contrasts.arg = c(1, 2, 3))
</code></pre>

<p>My output looks like the following:</p>

<pre><code>   (Intercept) ferti.f2 ferti.f3 ferti.f4
1            1        0        0        0
2            1        1        0        0
3            1        0        1        0
3            1        0        0        1
</code></pre>

<p>Whereas I would like something like below using the same data:</p>

<pre><code>   (Intercept) ferti.f2 ferti.f3 ferti.f4
1            1        1        0        0
2            1        0        1        0
3            1        0        0        1
3            1        0        0        0
</code></pre>

<p>I feel like I am doing the contrasts.arg incorrectly, but don't know how to correct it. Thank you for your help!</p>
"
"0.0566365471788599","0.0555127381653369","152012","<p>This might fit better here than on stackoverflow, I guess.</p>

<p>I was <a href=""http://stackoverflow.com/questions/30139874/r-dynamic-linear-regression-with-dynlm-package-how-to-predict"">trying to build a dynamic regression model with the dynlm</a> package, but it did not work out. After reading <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">this</a> by Hyndman, I now switched to an ARMAX model:</p>

<pre><code>y_t = a_1*x1_t + a_2*x2_t + ... + a_k*xk_t + n_t
</code></pre>

<p>where the error term follows an ARMA model</p>

<pre><code>n_t ~ ARMA(p,q)
</code></pre>

<p>So far I am using the function <code>auto.arima(y, xreg=cbind(x1, ..., xk))</code> from the <code>forecast</code>package, which is doing the job!</p>

<p>As a benchmark I am running a pure multiple regression with <code>lm()</code>, where I make use of the <code>step()</code> function to kick out non relevant variables (about 100 variables, from which 96 are dummies) to optimize the model according to <code>AIC</code>.</p>

<p>The in-sample forecasting for both models is more or less equal. As the ARMAX model always includes <strong>all</strong> independent variables <code>(x1, ..., xk)</code>, I am pretty sure that, if I could apply the <code>step()</code> function on it, I would achieve a further improvement here.</p>

<p>The problem is that the <code>step()</code> function does not work on <code>auto.arima()</code>?!</p>

<p>Do you have any suggestions how I could still do this? Or would I need a totally new approach?</p>

<p>(I have not provide a reproducible example, as this is a rather general question of which methods/functions/packages to use. If the question is not clear enough, please tell me and I will try to provide one)</p>
"
"0.0633215847514023","0.0620651280774201","152158","<p>I have a linear model containing a few continuous variables and four categorical variables, each represented by 12, 3, 4, and 5 dummy variables respectively. When using model selection criteria such as PRESS, Mallows's Cp, and BIC using the leaps package, the best model returned for each criterion contains only some of the dummy variables for each categorical variable. It is my understanding that this is not good practice, and either all or none of the dummies must be included in the model. Is there a way to have leaps treat the dummy variables for each categorical variable as one variable?</p>

<p>Also, could this method be extended to use with the glmnet package? I'm having the same issue with lasso and ridge regression.</p>

<p>EDIT: Is there a way to specify an lm object with a subset of independent variables to be treated as one?</p>
"
"0.0490486886395286","0.0480754414848157","152203","<p>I've found <a href=""http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/linearregression/linearregression.R"" rel=""nofollow"">this line</a> of code to calculate predicted values from a ridge.lm model:</p>

<pre><code># Predict is not implemented so we need to do it ourselves
y.pred.ridge = scale(data.test[,1:8],center = F, scale = m.ridge$scales)%*% m.ridge$coef[,which.min(m.ridge$GCV)] + m.ridge$ym
</code></pre>

<p>Why center is set to FALSE? And why do I need to add the mean of $Y$ to the predicted values?</p>

<p>I thought $X$ values should be scaled before running a ridge regression, which implies the out of samples predictors should be centered and scaled?
And scaling the predictors doesn't imply centering the outcomes of the regression, so why to add the mean of $Y$ to the predictions?</p>
"
"0.0749231094763201","0.0734364498908627","153423","<p>I have an assignment at university and  I have been given a simple situation which I would like to explain here. </p>

<p><strong>Situation:</strong>
I would like to perform  simulation on <strong>purchase price</strong> of a product and see the impact of this change on <strong>sales price</strong> and <strong>quantities sold</strong> of a product.</p>

<p>As long as <strong>purchase price</strong> varies, the sales price also varies. If sales price of product increases, people buy fewer goods hence quantities sold varies and vice versa. So,</p>

<p>1) Sales price of a product depends on the purchase price.</p>

<p>2) Quantities sold depends on sales price.</p>

<p>For this, I thought of using simple linear regression to build two models keeping <strong>purchase price</strong> as independent variable and changing the two dependent variables each time. Later combine the two models to predict the future values for every new purchase value.</p>

<p>Kindly share your thoughts if you believe it could be done with a better solution.</p>

<p>Thanks.</p>

<p><strong>NOTE</strong> : To anyone referring to this question: I am just looking for ideas on my approach. Based on that I will come up with a solution on my own. Any thoughts fruitful/not fruitful doesn't matter. It will help me broadening my knowledge on this topic.</p>
"
"0.0805952195517515","0.0789960112897596","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.10236445520486","0.100333291527804","153802","<p>I have a large data set with repeated measurements of same blood value (co) (2 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement.</p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to right and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>At first I assumed random intercepts among patients. I constructed a null model and model with time as independent.</p>

<pre><code>fit0&lt;-(lmer(lgco~(1|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(1|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (1 | id)
fit1: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit0  3 200.44 213.16 -97.219   194.44                             
fit1  4 189.62 206.59 -90.811   181.62 12.815      1  0.0003438 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Ok, so I have an empty model and model with independent variable.
<img src=""http://i.stack.imgur.com/SFIFL.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/8RbgF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/phYJ1.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/G4HNH.png"" alt=""enter image description here""></p>

<p>Adding covariate time in my model improves it significantly and also the graphical explanation is clear.</p>

<p>Fixed slopes, however, are not reasonable in my data, so I should use random slopes.</p>

<pre><code>fit0&lt;-(lmer(lgco~(time|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(time|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (time| id)
fit1: lgco ~ time + (time | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
fit0  5 190.15 211.36 -90.076   180.15                            
fit1  6 182.06 207.51 -85.029   170.06 10.094      1   0.001487 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>At this point I dont understand my model equations. Graphical outputs for fit0 and fit1 are as follows:
<img src=""http://i.stack.imgur.com/E4w5D.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/XyeTJ.png"" alt=""enter image description here""></p>

<p>For the fit1 the model equation is:
<img src=""http://i.stack.imgur.com/Z6WLa.png"" alt=""enter image description here""></p>

<p>Why the lines in fit0 have non-zero slopes? What are they and what is the equation in that case? Also I dont understand how should I clarify the change in model fit? In the case of only random intercepts I can state that ""adding fixed factor <em>beta1</em> to model improves it significantly"". What would be the equal statement in the case of random slopes?</p>
"
"0.105957277556576","0.10385482340819","153846","<p>I am familiar with linear regression models, but I am in the process of learning about linear mixed effects models.</p>

<p>My data consists of measurements for each month for a set of subjects over a long period of time (~15 years). The subjects and time frames are partially crossed - subjects do not appear for each time point. I also have a number of covariates measured at the per date per subject level, and a single boolean variable indicating a whether a <code>count</code> is before or after a particular time point. The point of this particular model is to measure whether or not a particular event (occurring at the ""mid date"") had an effect on the <code>count</code> variable. Due to the partially crossed, longitudinal nature of my data and the general discontinuity of my data over time, I don't believe that simple paired t-tests can properly answer this question. My data frame is as follows:</p>

<p><code>head</code></p>

<pre><code>   subject_id date_monthly count subject_join_date covar1_per_subject_per_date subject_group after_mid_date_bool covar2_per_subject_per_date covar3_per_subject_per_date
1:          0   2013-05-01     3        2011-07-01                           1     afteronly                TRUE                    22.33333                    195.7986
2:          0   2013-04-01     1        2011-07-01                           1     afteronly                TRUE                    21.33333                    194.7986
3:          0   2013-02-01    19        2011-07-01                           1     afteronly                TRUE                    19.36806                    192.8333
4:          0   2013-12-01     3        2011-07-01                           1     afteronly                TRUE                    29.46806                    202.9333
5:          0   2013-10-01     4        2011-07-01                           1     afteronly                TRUE                    27.43333                    200.8986
</code></pre>

<hr>

<p><code>tail</code></p>

<pre><code>       subject_id date_monthly count subject_join_date covar1_per_subject_per_date subject_group after_mid_date_bool covar2_per_subject_per_date covar3_per_subject_per_date
22407:       6911   2013-08-01     3        2011-08-01                           1     afteronly                TRUE                    24.36667                    198.8653
22408:       6911   2013-07-01     1        2011-08-01                           1     afteronly                TRUE                    23.33333                    197.8319
22409:       6911   2013-06-01     1        2011-08-01                           1     afteronly                TRUE                    22.33333                    196.8319
22410:       6931   2009-05-01     7        2009-05-01                           1    beforeonly               FALSE                     0.00000                    147.0986
22411:        238   2013-09-01     1        2012-10-01                           1     afteronly                TRUE                    11.16667                    199.8986
</code></pre>

<p><code>count</code> is the response I am looking to model.</p>

<p>I've read through all of Bates' lme4 paper, but I am still confused as to how to specify the random effects part of my model.</p>

<p>My attempt at a model specification is:</p>

<pre><code>lmer(log(count) ~ covar1_per_subject_per_date + covar2_per_subject_per_date + 
covar3_per_subject_per_date + after_mid_date_bool + 
subject_group + subject_join_date + (1|subject_id) + (1|date_monthly),
data=df, REML=F)
</code></pre>

<p>Which ""works"" (no errors from <code>lmer</code>). However, my primary question is:</p>

<p>Is this the correct specification for a mixed effects model with random, uncorrelated intercepts for <code>subject_id</code> and <code>date_monthly</code>? Correct here means that we model independent fixed effects for each of the fixed effects specified in the model, accounting for multiple trials of the same subject over time with subjects not appearing at every time point.</p>

<p>A secondary but related question is:</p>

<p>Have I organized my data frame in the proper way? My worry is that the <code>after_mid_date</code> column may be specified improperly.</p>

<p>I apologize if this is long-winded or too-specific of a question. My intention of providing my exact data is to be as clear as possible with my question.</p>
"
"NaN","NaN","154027","<p>When I run linear regression on my test data I get the following report:</p>

<p><img src=""http://i.stack.imgur.com/kMm7M.png"" alt=""enter image description here""></p>

<p>You can find the test data in <a href=""http://paste.ofcode.org/33VQKk2X82NnAHQ2uYF2s7K"" rel=""nofollow"">here</a>.</p>

<p>The graph of actual vs predicted looks like:
<img src=""http://i.stack.imgur.com/TU8T7.png"" alt=""enter image description here""></p>

<p>I would like to know if this is fairly a good fit because summary gives R2 value is 0.43 which says that the variable has no significant linearity. But the avg error percentage is close to 97%.</p>

<p>Should I still keep this results as my analysis? </p>
"
"0.0566365471788599","0.0555127381653369","154043","<p>In polynomial regression, it is recommended to center predictor input variables to break multi colinear relationships of x to x^2.</p>

<p>From Wikipedia: The underlying monomials can be highly correlated ""For example, x and x2 have correlation around 0.97 when x is uniformly distributed on the interval (0, 1). ""</p>

<p>When a variable x is between -1 and 1, x^2 makes the magnitude smaller while when x is outside of that range, x^2 makes x's magnitude larger.</p>

<p>Making the variable into an integer variable could change the behavior.</p>

<p>E.g.</p>

<pre><code>df$x=round((df$x - mean(df$))*100)
</code></pre>

<p>Any opinions on the scale especially in regards to interval [-1,1] vs [-100,100]</p>

<p>It is common to normalize predictors subtracting the mean and dividing by the standard deviation when doing inference analysis but this question pertains to regression prediction.</p>

<p>Asking a similar question in regards to natural log, a variable that has a range (0,1] has a dramatically different transformed value than [1,100].</p>

<pre><code>log(seq(0.1,1,.1)) #mostly negative
log(seq(0.1,1,.1)*100) #rather positive
</code></pre>

<p>If the predictor variable in the case of log happened to be sometimes less than 1 and others greater than 1, that could make the transformation act a little ""wild"". Would it be best to transform the variable to be within (0,1] or [1,] but not both?</p>
"
"NaN","NaN","154485","<p>As a prequel to a question about linear-mixed models in R, and to share as a reference for beginner/intermediate statistics aficionados, I decided to post as an independent ""Q&amp;A-style"" the steps involved in the ""manual"" computation of the coefficients and predicted values of a simple linear regression.</p>

<p>The example is with the R in-built dataset, <code>mtcars</code>,  and would be set up as miles per gallon consumed by a vehicle acting as the independent variable, regressed over the weight of the car (continuous variable), and the number of cylinders as a factor with three levels (4, 6 or 8) without interactions.</p>

<p>EDIT: If you are interested in this question, you will definitely find a detailed and satisfactory answer in this <a href=""http://madrury.github.io/jekyll/update/2016/07/20/lm-in-R.html"">post by Matthew Drury outside CV</a>.</p>
"
"0.141981427551046","0.144318393614449","154669","<p>I'm working on a small project where I need to create a multivariate linear regression model to predict the frequency of some airline companies. I'm a bit confused as I don't know if I have to remove the intercept because its <code>Pr(&gt;|t|)</code> had, after removing the first variable <code>dist</code>, the biggest value among the other values. Here is what I get after removing <code>dist</code>: </p>

<pre><code>flights_lm = lm(freq~dist+capa+nbrt+depf+lcco+prbi)
summary(flights_lm)
##################################################################
# &gt; summary(flights_lm)
# 
# Call:
#   lm(formula = freq ~ dist + capa + nbrt + depf + lcco + prbi)
# 
# Residuals:
#   Min      1Q  Median      3Q     Max 
# -204884  -12347    1145   12382  297908 
# 
# Coefficients:
#               Estimate  Std. Erro  t value Pr(&gt;|t|)    
# (Intercept)  1.857e+04  1.487e+04   1.248  0.21437    
# dist        -5.145e+00  6.729e+00  -0.765  0.44610    
# capa        -7.928e+01  6.540e+01  -1.212  0.22784    
# nbrt         7.665e+01  7.188e+00  10.663  &lt; 2e-16 ***
# depf         3.408e-05  1.204e-05   2.832  0.00546 ** 
# lcco         3.531e+04  2.151e+04   1.642  0.10339    
# prbi         4.084e+00  2.017e+01   0.203  0.83988    
# ---
#   Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1                                      
#                                                                                                     
# Residual standard error: 60280 on 116 degrees of freedom                                            
# Multiple R-squared:  0.8719,  Adjusted R-squared:  0.8653                                           
# F-statistic: 131.6 on 6 and 116 DF,  p-value: &lt; 2.2e-16                                             
#####################################################################

flights_lm2 = update(flights_lm, .~. -prbi)
summary(flights_lm2)
####################################################################
# Call:
#   lm(formula = freq ~ dist + capa + nbrt + depf + lcco)
# 
# Residuals:
#   Min      1Q  Median      3Q     Max 
# -204913  -12471    1098   12201  297917 
# 
# Coefficients:
#               Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)  1.918e+04  1.450e+04   1.323  0.18854    
# dist        -5.132e+00  6.701e+00  -0.766  0.44528    
# capa        -7.813e+01  6.488e+01  -1.204  0.23093    
# nbrt         7.665e+01  7.158e+00  10.708  &lt; 2e-16 ***
# depf         3.406e-05  1.199e-05   2.842  0.00529 ** 
# lcco         3.506e+04  2.138e+04   1.639  0.10382    
# ---
#   Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
# 
# Residual standard error: 60030 on 117 degrees of freedom
# Multiple R-squared:  0.8719,  Adjusted R-squared:  0.8664 
# F-statistic: 159.3 on 5 and 117 DF,  p-value: &lt; 2.2e-16
#####################################################################

flights_lm3 = update(flights_lm2, .~. -dist)
summary(flights_lm3)
#####################################################################
# Call:
#   lm(formula = freq ~ capa + nbrt + depf + lcco)
# 
# Residuals:
#   Min      1Q  Median      3Q     Max 
# -206975  -12147    4077   11489  297630 
# 
# Coefficients:
#               Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)  1.526e+04  1.355e+04   1.127  0.26212    
# capa        -9.031e+01  6.279e+01  -1.438  0.15303    
# nbrt         7.705e+01  7.127e+00  10.811  &lt; 2e-16 ***
# depf         3.302e-05  1.189e-05   2.778  0.00637 ** 
# lcco         3.329e+04  2.122e+04   1.569  0.11939    
# ---
#   Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
# 
# Residual standard error: 59920 on 118 degrees of freedom
# Multiple R-squared:  0.8713,  Adjusted R-squared:  0.8669 
# F-statistic: 199.6 on 4 and 118 DF,  p-value: &lt; 2.2e-16
################################################################
</code></pre>
"
"0.0980973772790571","0.0961508829696314","155040","<p>I am struggling with interpreting coefficients from a multiple regression analysis with multiple categorical (dummy) variables. I am running a linear mixed model with biodiversity (<code>LnS_Add1</code>) as independent variable, and several continuous and categorical dependent variables.</p>

<p>With a single categorical/dummy variable (e.g. <code>LnS_Add1 ~ AREA_AM_2.5 + System_Type3</code>; where <code>AREA_AM_2.5</code> is continuous and <code>System_Type3</code> is categorical with 3 levels, i.e. <em>Arable</em>, <em>Grassland</em> and <em>Orchard</em>) this is pretty straightforward. In this case the intercept represents the mean of the reference dummy variable (e.g. <em>Arable</em>) and the mean of the 2nd and 3rd levels <em>Grassland</em> and <em>Orchard</em> can be calculated manually by adding intercept to the slope coefficient.</p>

<pre><code>globmod1 &lt;- lmer(LnS_Add1 ~ AREA_AM_2.5 + System_Type3 + 
     (1|Study_Code/Pair_Code), data1_plant)
summary(globmod1)
</code></pre>

<p>Which returns</p>

<pre><code>Fixed effects:
                        Estimate Std. Error t value
(Intercept)            0.3585534  0.1238470   2.895
AREA_AM_2.5            0.0004256  0.0001371   3.104
System_Type3Grassland -0.5227684  0.0915722  -5.709
System_Type3Orchard   -0.4057969  0.5477567  -0.741
</code></pre>

<p>To get a summary output that shows the means of both <em>Arable</em>, <em>Grassland</em> and <em>Orchard</em> in R I suppress the intercept by adding a -1 (or +0) to the model.</p>

<pre><code>globmod1.coef &lt;- lmer(LnS_Add1 ~ AREA_AM_2.5 + System_Type3 -1 +
                   (1|Study_Code/Pair_Code), data1_plant)
summary(globmod1.coef)
</code></pre>

<p>Which returns:</p>

<pre><code>Fixed effects:
                        Estimate Std. Error t value
AREA_AM_2.5            0.0004256  0.0001371   3.104
System_Type3Arable     0.3585534  0.1238470   2.895
System_Type3Grassland -0.1642149  0.1341851  -1.224
System_Type3Orchard   -0.0472434  0.5457304  -0.087
</code></pre>

<p>But what do I do if I have multiple categorical variables (e.g. <code>LnS_Add1 ~ AREA_AM_2.5 + System_Type3 + Habitat2</code>; where Habitat2 is a categorical variable with 3 levels, i.e. <em>Farm aggregated</em>, <em>Outside field</em>, and <em>Within field</em>)?. Now the intercept represents the mean of the reference level of a combination of <code>System_Type3</code> and <code>Habitat2</code> (e.g. all data in arable systems and measured at farm aggregate level). But what I am interested in are the means for the different levels of each of my 2 categorical variables, holding everything else constant.</p>

<p>How do I create a summary table that contains means of all levels of all categorical variables in my model? The -1 command doesnt help me anymore, as it removes the intercept but the intercept now represents a mean of 2 reference dummy variables. I am only interested here in the fixed effect estimates, not in any hypothesis testing.</p>
"
"0.09392108820677","0.0920574617898323","155121","<p>First cross-validated question so please be gentle :o)</p>

<p>I have two datasets all gathered and managed in '<strong>R</strong>'... </p>

<p><strong>Dataset 1</strong> - News Corpus. Contains 3,270 entries from the period 1/Apr/13 to 31/Mar/14.  There are often multiple stories on any one day, and indeed days with no stories at all (which I believe makes for an incomplete time series and problems).  The dataset structure is;</p>

<pre><code>Date - (a date)
Domain - (a string) with 8 levels i.e. there are 8 web domains
DomainType - (a string) with 4 levels e.g. ""other news"" or ""technology news""
Sentiment_Title - (a numeric) a score that currently sits in range -4:4
Sentiment_Description - (a numeric) a score that currently sits in range -6:7
Sentiment_Body - (a numeric) a score that currently sits in range -53:146
CCAT - (logical)
ECAT - (logical)
GCAT - (logical)
MCAT - (logical)
</code></pre>

<p><a href=""https://mega.co.nz/#!HVQTkCJJ!UUFJMzN6i0xI_GKDEtzVV1WfUzkphYCEiB36oMsOINo"" rel=""nofollow"">DOWNLOAD corpusData.csv from Mega</a></p>

<p><strong>Dataset 2</strong> - Bitcoin Market Data. 365 day time series of weighted price, volume and intra-day spread for four different exchanges.  </p>

<p><a href=""https://mega.co.nz/#!3RokVLKB!8rsEBIL8N-F-SXP2lucsjnUfo40MBfN13YRFPGAMSlQ"" rel=""nofollow"">DOWNLOAD finData.csv from Mega</a> </p>

<p><strong>The Problem</strong>
What I really want to know is which features (if any) of dataset 1 (the corpus) are significantly related to the time series and how.  I guess the time series also needs leads and lags applied to know which direction any relationship goes and how far away from the story publication date that relationship lays.</p>

<p>I have spent a couple of weeks applying the very basic stats knowledge I have to the task and have spent a couple of hours with a post-grad stats support group who also proved unable to find a method that could be readily applied.</p>

<p>I (we) looked at basic Pearson's and Spearman's, moved on to look at linear regression and generalised linear models and so far there appears to be issues with the residuals that makes the output bunkum apparently.  I believe vector-autoregression could also be applied but we are way off into realms I just don't understand yet.</p>

<p><strong>The Question</strong> Given the datasets (and, ideally R) can anyone suggest or indeed offer up an approach to solving my problem?  Even better some simple explanation of how to interpret the results of any such approach.</p>
"
"NaN","NaN","155459","<p>I'm trying to predict the outcome ""Decision"" in the function of Age, Gender, Occupation, .... </p>

<p>The independent variable ""Occupation"" is known to be significant. But when I do the logistic model, each sub-group (modality) of it is not.</p>

<p>Should I regroup the levels having the same value of estimated coefficient? (which I guess doesn't make many sense because the levels are not statistically significant)</p>

<p>The variable Occupation has 74 different sub-groups.</p>

<p>And another problem is that when checking the multicollinearity, the function VIF in R doest work, it produces the NaN value, may be its due to the large number of sub-groups of Occupation.</p>

<p><img src=""http://i.stack.imgur.com/ruScu.png"" alt=""Summary(Logistic Regression)""></p>
"
"0.02831827358943","0.0277563690826684","155495","<p>Could someone explain to me what is criterion for interpretation of Breusch-Pagan test?</p>

<p>I have applied ncvTest test from the package car in R on a simple linear regression with one predictor variable e.g. lm(weight~size). I have the following result:</p>

<p>Chisquare = 7.182687    Df = 1     p = 0.007361039 </p>

<p>I see in other questions that p = 0.073459 implies heteroscedasticity
while p = 0.6283239 and p-value = 0.858 imply homoscedascity. By looking at these samples I would assume that my result set is heteroscedasticit, but I would like to know is p value only criterion and is there some boundary value for yes/no decision (i.e. some p value between 0.007 and 0.6).</p>

<p>Does Chisquare value matters?</p>
"
"0.0566365471788599","0.0555127381653369","155509","<p>Working on a linear regression problem in R, I created a first model </p>

<pre><code>flights_lm = lm(freq~dist+capa+nbrt+depf+lcco+prbi)
</code></pre>

<p>where freq is frequency, dist is distance, capa for capacity, nrt stands for number of roads, fuel is depf. I then started to remove variables which doesn't contribute to the model explination, and ended up with
 Coefficients:</p>

<pre><code>              Estimate Std. Error t value Pr(&gt;|t|)    
 (Intercept) 2.822e+02  6.239e+03   0.045  0.96400    
 nbrt        8.072e+01  6.515e+00  12.390  &lt; 2e-16 ***
 depf        3.052e-05  1.128e-05   2.704  0.00784 ** 
</code></pre>

<p>meaning that the frequency was well explained by nbrt and depf. Then I proceeded to remove points that affect the model (I guess they're called outliers). for that I used the R function <code>Cooks.distance</code> and couldn't get rid of all the points. each time I apply the function and plot it according to the model some other points pop out of the 0.65 limit that set for the model.
I have one doubt though, once I remove the points, Do I need to restart the process from the beginning (meaning creating the model with all the variables then delete each that p-value is the biggest until I get all the p values &lt;0.05)
or just delete the points and plot the model ?
Another question is :  could the function log help me with this case ?  </p>
"
"0.0424774103841449","0.0555127381653369","156034","<p>I am dealing with a heteroscedastic censored dataset. I tried to use the survival analysis package in R to estimate a linear model for it. So before doing that, I conducted a simulation study, where I generate a sample for x:
$x  \sim Unif[-2,2]$
and y:
$y \sim 1+0.3x+0.6(1-0.3x)\epsilon $ where $\epsilon\sim N(0,1)$.
y is censored as $y=y$ if $y&lt;x$; $y=x$ otherwise.</p>

<p>Then I use the survreg function in R to estimate a linear model for it.</p>

<p><code>x&lt;-runif(10000,-2,2)</code></p>

<p><code>y&lt;-1+0.3*x+rnorm(length(x))*0.6*(1-0.3*x)</code></p>

<p><code>y[y&gt;=x]=x[y&gt;=x]</code></p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian')</code></p>

<p>The result is really poor, the estimate is around(intercept = 0.5, slope = 0.6) and it is very stable no matter what initial points I gave.</p>

<p>The result does not get significant improved even if I feed the true weight to it:</p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian', 'weights=1/(1-0.3x)^2')</code></p>

<p>But the estimation is great when no heteroscedasticity is presented:</p>

<p><code>x&lt;-runif(10000,-2,2)</code></p>

<p><code>y&lt;-1+0.3*x+rnorm(length(x))*0.6</code></p>

<p><code>y[y&gt;=x]=x[y&gt;=x]</code></p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian')</code></p>

<p>I also tried quantile regression for censored data with tau=0.5. (basically LAD regression which is proposed by J.Powell 1984). The result is still poor under this parameter setting.</p>

<p>Is there any good way to get a consistent estimation for heteroscedastic censored data?</p>

<p>Any suggestion on package, software or papers are welcome.</p>

<p>Thanks a lot.</p>
"
"0.02831827358943","0.0277563690826684","156058","<p>I want to perform a simple linear regression in R. However, the plot of fitted and residual values has outliers. Transformations (ie log, square root) did not solve this problem. Removing these outliers created new outliers. So, what can I do? Is it wrong to adjust a Poisson distribution for this case of a simple linear regression and perform a generalized model? Are there other options?</p>
"
"0.0326991257596857","0.0480754414848157","156098","<p>The R function cv.glm (library: boot) calculates the estimated K-fold cross-validation prediction error for generalized linear models and returns delta. Does it make sense to use this function for a lasso regression (library: glmnet) and if so, how can it be carried out? The glmnet library uses a cross-validation to get the best turning parameter, but I did not find any example that cross-validates the final glmnet equation.</p>
"
"0.0633215847514023","0.0620651280774201","156275","<p>I have two paired samples following normal distributions N(0, $\sigma_1^2$) and N(0, $\sigma_2^2$). Samples represent estimation errors (residuals) of two linear regression models used to predict the same response variable using two different methods/independent variables. I have 30 pairs of residuals, so I would like to apply Wilcoxon signed-rank test to check whether means of absolute values or relative errors are different. Since absolute values do not follow normal distribution, I cannot use t-test or something similar.</p>

<p>I would like to find type II error and statistic power of Wilcoxon signed-rank test.
Is there some R function (or any other tool) that can be used? I have found a number of functions for testing the power of tests here <a href=""http://www.statmethods.net/stats/power.html"" rel=""nofollow"">http://www.statmethods.net/stats/power.html</a>  but Iâ€™m not sure could they be applied on Wilcoxon signed-rank test. If there is no built-in function is there some other tool or algorithm to manually calculate error?   </p>
"
"0.0693653206906364","0.0679889413649005","156654","<p>I get a fan-shaped scatter plot of the relation between two different quantitative variables:</p>

<p><img src=""http://i.stack.imgur.com/L7rS3.png"" alt=""enter image description here""></p>

<p>I am trying to fit a linear model for this relation. I think I should apply some kind of transformation to the variables in order to unify the ascent variance in the relation before fitting a linear regression model, but I can't find the way to do it. Or maybe, there is a better model to use in these cases, I can't either find it.</p>

<p>I have tried <code>rlm</code>, but the residuals still have heteroscedasticity. I have also tried to apply a SD ratio calculated from all the y of each x and other similar erratic approaches.</p>

<p>My questions:</p>

<ul>
<li>Is there any typical way of fitting a model for a fan-shaped relation or a typical model to use in these cases?</li>
<li>Is there any typical transformation that could be applied to the variables in order to reduce its variance?</li>
</ul>

<p>Thanks!</p>
"
"0.0849548207682898","0.0832691072480053","156661","<p><img src=""http://i.stack.imgur.com/6gO09.gif"" alt=""enter image description here""></p>

<p>I get a similar scatter plot (as above) showing the relation between two different quantitative variables. It is also fan-shaped.</p>

<p>I am trying to fit a linear model for this relation. I think I should apply some kind of transformation to the variables in order to unify the ascent variance in the relation before fitting a linear regression model, but I can't find the way to do it. Or maybe, there is a better model to use in these cases, I can't either find it.</p>

<p>I have tried <code>rlm</code>, but when plotting the residuals vs the predictor they are very skewed. I have tried to use the weights retrieved by the model to transform the output:</p>

<pre><code>fit &lt;- rlm(y ~ x, data=df)
plot(fit$resid ~ df$x) # Heteroskedastic &amp; Skewed

df$y &lt;- df$y * fit$w 
    plot(df$y ~ df$x) # Wrong
</code></pre>

<p>But it is obviously wrong.</p>

<p>I have also tried to apply a SD ratio calculated from all the y of each x and other similar erratic approaches.</p>

<p>Is there any typical way of fitting a model for this kind of relation (fan-shaped) or a typical model (and R package) to use in this cases?</p>

<p>Is there any typical transformation that could be applied to the variables in order to reduce the variance?</p>

<p><strong>Sorry, I duplicated the post: Follow <a href=""http://stats.stackexchange.com/questions/156654/fit-regression-model-from-a-fan-shaped-relation-in-r"">Fit regression model from a fan-shaped relation, in R</a>, instead.</strong></p>
"
"0.0642198081225601","0.0734364498908627","157142","<p>A couple years ago I performed a linear regression on data that looked like this:</p>

<pre><code>   company year     y       x1      x2      x3      x4
1        A 2012  1.83  34811.8 14755.5   278.2     0.0
2        B 2012  3.87  10435.5  9692.6   522.2   317.9
3        C 2012 19.76 199670.6 23428.7 10675.5  2815.8
4        D 2012  1.22   3204.4  2087.5  2282.8  2804.1
5        E 2012  0.00      5.2    53.5     0.2   193.8
6        F 2012  0.81 161936.0 25777.9  2364.8   540.6
7        G 2012  1.22   1479.3    28.6     0.4     3.9
8        H 2012  2.24   9716.3   888.2  2073.9  1059.1
9        I 2012 25.25 331396.9 15162.0 87062.1 32724.7
10       J 2012  0.20   9812.0 10363.4    49.9 36664.9
11       K 2012  1.02  62715.3  5746.5  1007.7   866.3
12       L 2012  3.87 121397.5  5842.2  1481.6   621.0
13       M 2012 12.22 243189.5 50370.8 16747.1 23025.8
14       N 2012 18.33 147305.6 87916.3 15098.3 16449.7
15       O 2012  0.61  20699.1  8345.6     0.0    26.4
16       P 2012  2.44  30735.1  1840.6  4900.1     0.0
</code></pre>

<p>Each row is a different company and the main objective was to interpret the coefficients. Its been 3 years since that regression and I want to look at it again with data from each year so the dataset would look like this:</p>

<pre><code>   company year     y       x1      x2       x3      x4
1        A 2012  1.83  34811.8 14755.5    278.2     0.0
2        B 2012  3.87  10435.5  9692.6    522.2   317.9
3        C 2012 19.76 199670.6 23428.7  10675.5  2815.8
4        D 2012  1.22   3204.4  2087.5   2282.8  2804.1
5        E 2012  0.00      5.2    53.5      0.2   193.8
6        F 2012  0.81 161936.0 25777.9   2364.8   540.6
7        G 2012  1.22   1479.3    28.6      0.4     3.9
8        H 2012  2.24   9716.3   888.2   2073.9  1059.1
9        I 2012 25.25 331396.9 15162.0  87062.1 32724.7
10       J 2012  0.20   9812.0 10363.4     49.9 36664.9
11       K 2012  1.02  62715.3  5746.5   1007.7   866.3
12       L 2012  3.87 121397.5  5842.2   1481.6   621.0
13       M 2012 12.22 243189.5 50370.8  16747.1 23025.8
14       N 2012 18.33 147305.6 87916.3  15098.3 16449.7
15       O 2012  0.61  20699.1  8345.6      0.0    26.4
16       P 2012  2.44  30735.1  1840.6   4900.1     0.0
17       A 2013  0.20   4832.1 10691.6      0.6     0.0
18       B 2013  3.02  12575.8  1270.3    106.6   368.0
19       C 2013 16.00 184628.5 38269.7   5343.1  4645.6
20       D 2013  1.76   4684.6  1445.2   2150.1  1727.0
21       E 2013  1.27      4.3    22.9     38.3   314.6
22       F 2013  0.39 141808.6 26368.8    673.6  2259.2
23       G 2013  0.59    986.3    38.6      7.0     5.8
24       H 2013  2.83  20111.4  3518.3    549.5    59.6
25       I 2013 21.17 303925.9 20248.0 107366.7 19979.1
26       J 2013  1.37   7792.8 16000.7     33.5 39541.7
27       K 2013  1.66 141071.9 11136.1    162.2     0.0
28       L 2013  3.80 130359.7  8882.5     40.5   520.8
29       M 2013 10.63 280250.3 39029.7  16208.6 29284.3
30       N 2013 19.41 145278.1 55141.6  14115.5  1783.4
31       O 2013  0.98   1517.6  3610.4      0.0   547.3
32       P 2013  3.32 101484.2  1140.5   5489.9     0.0
33       A 2014  0.10      0.0  9520.7      0.9     0.0
34       B 2014  4.02  14886.8  2331.5      0.0   631.8
35       C 2014 14.22 143760.9 50222.1   6118.1  4342.1
36       D 2014  0.88    936.1  1802.7   1273.6  4394.3
37       E 2014  0.78    231.5    15.8     64.1   291.9
38       F 2014  0.78 244303.2 29148.3   3161.4  4908.1
39       G 2014  0.78   1032.6    30.3      1.3     7.8
40       H 2014  2.55  26322.6 11726.1   2859.2     0.0
41       I 2014 21.96 614241.5  9138.2  94273.7 17702.0
42       J 2014  1.27   8946.5 13853.7    693.9 19672.0
43       K 2014  1.18 164269.7  7088.1     29.7   825.0
44       L 2014  2.35 107152.3  3275.2     94.7   490.9
45       M 2014  8.73 284267.4 51896.4  12838.1 28019.5
46       N 2014 20.69  84554.6 32341.0  11408.2   624.9
47       O 2014  1.08      0.0  7663.2      0.0     0.0
48       P 2014  3.63 109392.9  5229.2   4691.0    11.1
</code></pre>

<p>When I think about this dataset I don't immediately think its a time series but I also don't think I should be ignoring year all together and regressing it like so in R:</p>

<pre><code>lm(y ~ x1 + x2 + x3 + x4)
</code></pre>

<p>So I'm wondering how I should model this dataset. Should I just include dummy variables for year or are there better approaches here?</p>
"
"0.0800961731463273","0.078506867197886","157597","<p>I am trying to understand how to interpret log-linear models for contingency tables, fitted by way of Poisson GLMs. </p>

<p>Consider this example from CAR (Fox and Weisberg, 2011, p. 252). </p>

<pre><code>require(car)
data(AMSsurvey)
(tab.sex.citizen &lt;- xtabs(count ~ sex + citizen, data=AMSsurvey))
</code></pre>

<p>Yielding:</p>

<pre><code>        citizen
sex      Non-US  US
  Female    260 202
  Male      501 467
</code></pre>

<p>Then we fit the model of (mutual) independence: </p>

<pre><code>AMS2 &lt;- as.data.frame(tab.sex.citizen)
(phd.mod.indep &lt;- glm(Freq ~ sex + citizen, family=poisson, data=AMS2))
pchisq(2.57, df=1, lower.tail=FALSE)
</code></pre>

<p>Outputting: </p>

<pre><code>&gt; (phd.mod.indep &lt;- glm(Freq ~ sex + citizen, family=poisson, data=AMS2))

Call:  glm(formula = Freq ~ sex + citizen, family = poisson, data = AMS2)

Coefficients:
(Intercept)      sexMale    citizenUS  
     5.5048       0.7397      -0.1288  

Degrees of Freedom: 3 Total (i.e. Null);  1 Residual
Null Deviance:      191.5 
Residual Deviance: 2.572    AIC: 39.16
&gt; pchisq(2.57, df=1, lower.tail=FALSE)
[1] 0.1089077
</code></pre>

<p>The p value is close to 0.1 indicating weak evidence to reject independence. However, let us <strong>assume</strong> that we have sufficient evidence to reject the NULL (i.e. for our purposes, the 0.10 p value is indicative of an association between the two variables). </p>

<p><strong>Question</strong>: How, then, do we interpret this loglinear model? </p>

<p>(Do we fit the saturated model (i.e. <code>update(phd.mod.indep, . ~ . + sex:citizen)</code>)? Do we interpret the estimated regression coefficients? In CAR they stop at this point, because of weak evidence for rejecting the NULL, but I'm interested in understanding the mechanics of the interpretation of this simple log-linear model <em>as if</em> the ""interaction"" were significant...)</p>
"
"0.0200240432865818","0.039253433598943","157735","<p>Polynomial regression is a common way of doing curvilinear regression.  It is common to also use the inverse transform x^-1 (<a href=""http://pareonline.net/getvn.asp?v=8&amp;n=6"" rel=""nofollow"">http://pareonline.net/getvn.asp?v=8&amp;n=6</a>).</p>

<p>One can extend the concept of the inverse transform by thinking about Laurent polynomials.</p>

<p>My question is do some practitioners use Laurent polynomial regression? What are the general ""rules"" around them?  With regular polynomials one tends to get a hill or a valley for each additional power.</p>

<p>I read this post which got me thinking about it:
<a href=""http://math.stackexchange.com/questions/231357/the-degree-of-a-polynomial-which-also-has-negative-exponents"">http://math.stackexchange.com/questions/231357/the-degree-of-a-polynomial-which-also-has-negative-exponents</a></p>

<p><a href=""https://en.wikipedia.org/wiki/Laurent_polynomial"" rel=""nofollow"">https://en.wikipedia.org/wiki/Laurent_polynomial</a></p>

<p>Example R code:</p>

<pre><code>x=runif(100,2,10)/2
y=jitter(1+abs(cos(x)))
df=data.frame(x=x,y=y)
df=df[order(df$x),]
    plot(df$x,df$y)
    lines(df$x,predict(lm(y~x+I(x^2)+I(x^3),df)))
title(""3rd degree poly"");

plot(df$x,df$y)
lines(df$x,predict(lm(y~x+I(x^-1)+I(x^-2),df)))
title(""Laurent poly"");
</code></pre>

<p>I wonder if some regular polynomial regression could benefit by having negative powers in addition to the positive?</p>

<p>Thoughts?</p>
"
"0.09392108820677","0.0920574617898323","157763","<p>I recently ran a test on our application that models a fairly contrived example:</p>

<p>Users are expected to add things to a bucket. We count the things in this bucket. A binary condition variable exists which could have an effect on the number of things users put things in a bucket. </p>

<p>The question I'm attempting to answer is if the variable has an effect on the number of things dropped in the bucket.</p>

<p>In statistical language:</p>

<p><em>H0:</em> The binary variable has no effect on the # of things</p>

<p><em>H-alt:</em> The binary variable increases the # of things in the bucket</p>

<p><strong>Question 1:</strong>
What is the most appropriate statistical test to apply to infer if a difference exists in the counts?</p>

<p>I started with a student t test, but the assumption of normality and sample of continuous outcome variable is violated. The data are counts and I believe that the poisson distribution is the most appropriate. Chi square is tempting the data are not appropriate for a contingency table since each user drops 0 or more things in a bucket.</p>

<p><strong>Question 2:</strong></p>

<p>I think a general linear regression attempts to answer my question. Is my interpretation of the results correct?</p>

<pre><code>group1 &lt;- append(rep(0, 100), rpois(50, 110))
group2 &lt;- append(rep(0, 100), rpois(50, 100))
df1 &lt;- data.frame(cnt = group1,
                  gr  = TRUE)
df2 &lt;- data.frame(cnt = group2,
                  gr  = FALSE)
dat &lt;- rbind(df1, df2)
fit &lt;- glm(data=dat, cnt ~ gr, family='poisson')
summary(fit)
# Coefficients:
#              Estimate Std. Error z value Pr(&gt;|z|)    
# (Intercept)  3.48493    0.01430 243.771  &lt; 2e-16 ***
#grTRUE        0.11840    0.01964   6.027 1.67e-09 ***

fit$coefficients[[1]]

prTrue &lt;- exp(fit$coefficients[[1]] + fit$coefficients[[2]] * 1)
(prTrue) / (1 + prTrue)  # =&gt; 0.9734889

prFalse &lt;- exp(fit$coefficients[[1]] + fit$coefficients[[2]] * 0)
(prFalse) / (1 + prFalse) # =&gt; 0.9702558
</code></pre>

<p>Interpretation:</p>

<p>The sign of the grTrue coeffient is positive so the presence of the variable increases the number of things dropped in the bucket.</p>

<p>The presence of the variable increases the likelihood of increasing the # of things in the bucket by about 0.3% (0.973 - 0.970)</p>

<p>Thanks in advance. As you can tell, I'm kind of a noob at this and keep thinking myself in circles.</p>
"
"0.109676202005208","0.107499955208361","157851","<p>In the <code>lmer</code> function within <code>lme4</code> in <code>R</code> there is a call for constructing a model matrix of random effects, $Z$, as explained <a href=""http://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf"" rel=""nofollow"">here</a>, pages 7 - 9.</p>

<p>Calculating $Z$ entails KhatriRao and/or Kronecker products of two matrices, $J_i$ and $X_i$.  </p>

<p>The matrix $J_i$ is a mouthful: ""Indicator matrix of grouping factor indices"", but it seems to be a sparse matrix with dummy coding to select which unit (for example, subjects in repetitive measurements) corresponding to higher hierarchical levels are ""on"" for any observation. The $X_i$ matrix seems to act as a selector of measurements in the lower hierarchical level, so that the combination of both ""selectors"" would yield a matrix, $Z_i$ of the form illustrated in the paper via the following example:</p>

<pre><code>(f&lt;-gl(3,2))

[1] 1 1 2 2 3 3
Levels: 1 2 3

(Ji&lt;-t(as(f,Class=""sparseMatrix"")))

6 x 3 sparse Matrix of class ""dgCMatrix""
     1 2 3
[1,] 1 . .
[2,] 1 . .
[3,] . 1 .
[4,] . 1 .
[5,] . . 1
[6,] . . 1

(Xi&lt;-cbind(1,rep.int(c(-1,1),3L)))
     [,1] [,2]
[1,]    1   -1
[2,]    1    1
[3,]    1   -1
[4,]    1    1
[5,]    1   -1
[6,]    1    1
</code></pre>

<p>Transposing each of these matrices, and performing a Khatri-Rao multiplication:</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp;. &amp;. &amp;. &amp;.\\.&amp;.&amp;1&amp;1&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;1&amp;1 \end{smallmatrix}\right]\ast \left[\begin{smallmatrix}\,\,\,\,1 &amp; 1 &amp;\,\,\,\,1 &amp;1 &amp;\,\,\,\,1 &amp;1\\-1&amp;1&amp;-1&amp;1&amp;-1&amp;1 \end{smallmatrix}\right]=
\left[\begin{smallmatrix}\,\,1 &amp; 1 &amp;.&amp;.&amp;.&amp;.\\\,\,\,\,-1 &amp;1&amp;.&amp;.&amp;.&amp;.\\ .&amp;.&amp;\,\,\,\,\,1 &amp;1&amp;.&amp;.\\.&amp;.&amp;\,\,-1&amp;1&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;\,\,\,1&amp;1\\.&amp;.&amp;.&amp;.&amp;-1&amp;1 \end{smallmatrix}\right]$</p>

<p>But $Z_i$ is the transpose of it:</p>

<pre><code>(Zi&lt;-t(KhatriRao(t(Ji),t(Xi))))

6 x 6 sparse Matrix of class ""dgCMatrix""

[1,] 1 -1 .  . .  .
[2,] 1  1 .  . .  .
[3,] .  . 1 -1 .  .
[4,] .  . 1  1 .  .
[5,] .  . .  . 1 -1
[6,] .  . .  . 1  1
</code></pre>

<p>It turns out that the authors make use of the database <code>sleepstudy</code> in <code>lme4</code>, but don't really elaborate on the design matrices as they apply to this particular study. So I'm trying to understand how the made up code in the paper reproduced above would translate into the more meaningful <code>sleepstudy</code> example.</p>

<p>For visual simplicity I have reduced the data set to just three subjects - ""309"", ""330"" and ""371"":</p>

<pre><code>require(lme4)
sleepstudy &lt;- sleepstudy[sleepstudy$Subject %in% c(309, 330, 371), ]
rownames(sleepstudy) &lt;- NULL
</code></pre>

<p>Each individual would exhibit a very different intercept and slope should a simple OLS regression be considered individually, suggesting the need for a mixed-effect model with the higher hierarchy or unit level corresponding to the subjects:</p>

<pre><code>    par(bg = 'peachpuff')
    plot(1,type=""n"", xlim=c(0, 12), ylim=c(200, 360),
             xlab='Days', ylab='Reaction')
    for (i in sleepstudy$Subject){
                fit&lt;-lm(Reaction ~ Days, sleepstudy[sleepstudy$Subject==i,])
            lines(predict(fit), col=i, lwd=3)
            text(x=11, y=predict(fit, data.frame(Days=9)), cex=0.6,labels=i)
        }
</code></pre>

<p><img src=""http://i.stack.imgur.com/opwVvm.png"" alt=""enter image description here""></p>

<p>The mixed-effect regression call is:</p>

<pre><code>fm1&lt;-lmer(Reaction~Days+(Days|Subject), sleepstudy)
</code></pre>

<p>And the matrix extracted from the function yields the following:</p>

<pre><code>parsedFormula&lt;-lFormula(formula= Reaction~Days+(Days|Subject),data= sleepstudy)
parsedFormula$reTrms

$Ztlist
    $Ztlist$`Days | Subject`
6 x 12 sparse Matrix of class ""dgCMatrix""

309 1 1 1 1 1 1 1 1 1 1 . . . . . . . . . . . . . . . . . . . .
309 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . . . . . . . . . . . .
330 . . . . . . . . . . 1 1 1 1 1 1 1 1 1 1 . . . . . . . . . .
330 . . . . . . . . . . 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . .
371 . . . . . . . . . . . . . . . . . . . . 1 1 1 1 1 1 1 1 1 1
371 . . . . . . . . . . . . . . . . . . . . 0 1 2 3 4 5 6 7 8 9
</code></pre>

<p>This seems right, but if it is, what is linear algebra behind it? I understand the rows of <code>1</code>'s being the selection of individuals like. For instance, subject <code>309</code> is on for the baseline + nine observations, so it gets four <code>1</code>'s and so forth. The second part is clearly the actual measurement: <code>0</code> for baseline, <code>1</code> for the first day of sleep deprivation, etc.</p>

<p><strong>But what are the actual</strong> $J_i$ <strong>and</strong> $X_i$ <strong>matrices and the corresponding</strong> $Z_i= (J_i^{T}âˆ—X_i^{T})^âŠ¤$ <strong>or</strong> $Z_i= (J_i^{T}\otimes X_i^{T})^âŠ¤$, <strong>whichever is pertinent?</strong></p>

<p>Here is a possibility,</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;.  &amp;. &amp;. &amp;. &amp;.&amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.\\
.&amp;.&amp;.&amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;.&amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;.&amp;.\\&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;.&amp;.&amp;.&amp;.&amp;.&amp;1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1\end{smallmatrix}\right]\ast \left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp; 1&amp;1&amp;1&amp;1 &amp; 1 &amp; 1 &amp; 1\\0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9 \end{smallmatrix}\right]=$</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\0 &amp; 1 &amp; 2 &amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;&amp;.&amp;.&amp;.&amp;.&amp;.&amp;1 &amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\ &amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;0 &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1\\&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9 \end{smallmatrix}\right] $</p>

<p>The problem is that it is not the transposed as the <code>lmer</code> function seems to call for, and still is unclear what the rules are to create $X_i$.</p>
"
"0.0400480865731637","0.039253433598943","157907","<p>I'm trying to understand Principal Component Regression(PCR) using dummy data (no multicollinearity). I have managed to regress dependent variable on scores but how do I convert the coefficients back to coefficients of original independent data? Below is my example using R</p>

<pre><code># create dummy data for testing
a1&lt;-1:10
a2&lt;-rnorm(10,0,5)  # uncorrelated to a1, for testing purposes
a&lt;-cbind(a1,a2)
y&lt;-a1-2*a2   # dependent variable

pr&lt;-princomp(a,cor=TRUE)
s&lt;-pr$scores
m2&lt;-lm(y~s)

# results
m2$coefficients 
(Intercept)     sComp.1     sComp.2 
 2.461620   -7.370569  -11.432588
</code></pre>

<p><strong>How do I convert <code>m2$coefficients</code> back to coefficients in terms of original data?</strong></p>

<p>From what I have learned so far, I only need to multiply EigenVectors with the above coefficients. But the coefficients have dimension of 3x1 and eigenvectors are 2x2!</p>
"
"NaN","NaN","158056","<p>I developed a negative binomial generalized linear regression model in R but now need to put it into Java. What equation do I use given these coefficients? (Sample below is just an example.)</p>

<p>As mentioned in <a href=""http://www.ats.ucla.edu/stat/r/dae/nbreg.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/nbreg.htm</a></p>

<pre><code>## glm.nb(formula = daysabs ~ math + prog, data = dat, init.theta = 1.032713156, 
##     link = log)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -2.155  -1.019  -0.369   0.229   2.527  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     2.61527    0.19746   13.24  &lt; 2e-16 ***
## math           -0.00599    0.00251   -2.39    0.017 *  
## progAcademic   -0.44076    0.18261   -2.41    0.016 *  
## progVocational -1.27865    0.20072   -6.37  1.9e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>Thanks</p>
"
"0.02831827358943","0.0277563690826684","158076","<p>I am trying to use principal component analysis (PCA) to reduce dimensionality before applying linear regression. The problem is that my first 10 components are so weak (explaining only tiny variances - the 10th component's cumulative is 0.2577). What else could I try to apply the PCA? </p>

<p>Also I understand the whole process is referred to as principal component regression (PCR). Is there a tutorial or example I could learn in Stata/R?</p>
"
"0.0490486886395286","0.0480754414848157","158783","<p>I am running a Multiple Linear Regression model using R. I am looking at travel behavior and most of the variables are factors with YES or NO as responses. However, I am concerned about using 3 categorical factors and only 1 continuous variable. The independent variable is a continuous variable. The model runs, I am just not sure the summary will make sense. </p>

<p>Would a Multiple Linear Regression in R using three factors and one categorical variable produce logical results? </p>

<p>Thanks! </p>
"
"0.0633215847514023","0.0620651280774201","158817","<p>I am working on a multiple linear regression problem where I would like to constrain only some of the parameters to non-negative values.  There have been discussions of how to solve for the parameters posted on SE <a href=""http://stats.stackexchange.com/questions/136563/linear-regression-with-individual-constraints-in-r"">here</a> and on <a href=""http://www.magesblog.com/2013/03/how-to-use-optim-in-r.html"" rel=""nofollow"">another site</a>.  Both solutions used optim() or constrOptim() to minimize the residual sum of squares, which worked very well for me and gives the same values as lm() for the unconstrained version of the problem.</p>

<p>But I would like now to take this a step further and estimate standard errors of those parameters, similar to what I'd get if I used lm().  However, there is nothing in the optim() object that would suggest a route to estimating errors, and I haven't found anything in the control settings that would suggest a route either.  So I'm a little bit at a loss as to how to proceed.  My hunch is that this approach - an optimization problem that assumes the data values are fixed and the parameter solutions represent a global minimum - does not allow for error.  Is there any validity to that, or am I just missing something basic?</p>

<p>This is a small reproducible example, adapted from the example provided in the first link to work with the optim() function, rather than constrOptim().</p>

<pre><code>    min.RSS &lt;- function(data, par){
      with(data, sum((par[1]*x1 + par[2]*x2 - y)^2))
    }
    dat = data.frame(x1=c(1,2), x2=c(2,3), y=c(5,6))
    result = optim(par=c(0,1), min.RSS, data=dat, method=""L-BFGS-B"", lower=c(-Inf,0), upper=c(0,Inf))
    result$convergence
    result$par
</code></pre>

<p>Thanks for any guidance you can provide.</p>
"
"0.0326991257596857","0.0480754414848157","158821","<p>In R, lm(y~x) will get the linear regression in terms of y=a+bx, giving the estimates of both slope and intercept. Now I want to set the slope to 1, and do the regression, which means the form of the regression should be y=a+x (since b is forced to be one), and the estimate of intercept will be generated. How can I achieve that in R? And how can I compare the regression of the slope 1 with regression of the estimated slope b?</p>
"
"0.02831827358943","0.0277563690826684","159053","<p>I'm using the CausalImpact to evaluate the effect of a programme. My covariates are seasonal and I wonder whether I need to deseasonalise/ detrend the regressors before using the R package?</p>

<p>Hal Varian did this in the below analysis: </p>

<p><a href=""http://people.ischool.berkeley.edu/~hal/Papers/2013/pred-present-talk.pdf"" rel=""nofollow"">http://people.ischool.berkeley.edu/~hal/Papers/2013/pred-present-talk.pdf</a></p>

<p>slide 24:</p>

<ul>
<li>Deseasonalize predictors using R command stl</li>
<li>Detrend predictors using simple linear regression</li>
<li>Let bsts choose predictors</li>
</ul>
"
"0.0424774103841449","0.0555127381653369","159316","<p>I have been running logistic regression in R, and have been having an issue where as I include more predictors the z-scores and respective p-values approach 0 and 1 respectively.  For example if have few predictors:</p>

<pre><code>&gt; model1
b17 ~ i74 + i73 + i72 + i71
&gt; step1&lt;-glm(model1,data=newdat1,family=""binomial"")
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -6.9461     1.8953  -3.665 0.000247 ***
i74           0.6842     0.9543   0.717 0.473384    
i73           1.7691     4.8008   0.368 0.712502    
i72           0.5134     2.0142   0.255 0.798812    
i71          -0.6753     4.9173  -0.137 0.890771    
</code></pre>

<p>The results appear to be fairly reasonable; however, if I have more predictors:</p>

<pre><code> &gt; model1
b17 ~ i90 + i89 + i88 + i87 + i86 + i85 + i84 + i83 + i82 + i81 + 
i80 + i79 + i78 + i77 + i76 + i74 + i73 + i72 + i71
&gt; step1&lt;-glm(model1,data=newdat1,family=""binomial"")
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.887e+02  3.503e+05  -0.001    0.999
i90          1.431e-01  1.009e+04   0.000    1.000
i89          8.062e+01  1.027e+05   0.001    0.999
i88          9.738e+01  7.398e+04   0.001    0.999
i87         -1.980e+01  9.469e+03  -0.002    0.998
i86          9.829e+00  1.098e+05   0.000    1.000
i85          5.917e+01  3.074e+04   0.002    0.998
i84         -2.373e+01  1.378e+05   0.000    1.000
i83          7.257e+00  2.173e+05   0.000    1.000
i82         -1.397e+01  1.894e+05   0.000    1.000
i81          6.503e+01  1.373e+05   0.000    1.000
i80          3.728e+01  4.904e+04   0.001    0.999
i79          1.010e+02  5.556e+04   0.002    0.999
i78         -2.628e+01  1.546e+05   0.000    1.000
i77          4.725e+01  3.027e+05   0.000    1.000
i76         -6.517e+01  1.509e+05   0.000    1.000
i74          1.267e+01  1.175e+05   0.000    1.000
i73          2.796e+02  5.280e+05   0.001    1.000
i72         -2.533e+02  4.412e+05  -0.001    1.000
i71         -1.240e+02  4.387e+05   0.000    1.000
</code></pre>

<p>I know it is hard to say exactly what is going on without seeing the data, but the predictors are all 5-point Likert Scale items.  However, are there any thoughts to what is occurring here?  I don't have much experience with logistic regression, so I apologize if the question seems naive, but is there a certain threshold of predictors where logistic regression falls apart due to having such a large amount of predictors what is ultimately a very small amount of variance?  Is the potentially a multi-co-linearity issue?  Finally, when I run OLS regression on the data I get results that make more sense (or at least appear to), is it okay/what are the consequences of running OLS regression on a binary outcome?  Thank you!</p>
"
"NaN","NaN","159337","<p>I am unfamiliar with the implementation used in the <code>R</code> package GVLMA. What are some basic tests of heteroscedasticity in linear regression models and how or where are they implemented?</p>
"
"0.132677139499561","0.151718580160313","159355","<p>I performed regression with robust variances (after Stata 12.1 lnskew transformation). A question of overfitting has been raised.</p>

<p>To summarise what I did: </p>

<ol>
<li>Comparison of disease B (disgrp=2) versus disease C (disgrp=3)
patients with 45-54 dependent observations (FibrosisP, continuous variable) in each disease group. Each patient had observations taken from Regions P, Q and R and Walls X, Y and Z (i.e. 9 observations per patient). </li>
<li>lnskew0 transformation (natural log transformation with zero skew of
resulting distribution) of FibrosisP to give lfibr. </li>
<li>Simple and then multiple regression with clustered robust variances/standard errors (clustered by patient and using independent categorical variables Disease, Wall and Region).</li>
</ol>

<p>Note that Disease A is excluded from this analysis (and is not provided in data set below).</p>

<p>$$ multipleregression: lfibr \sim Disease + Wall + Region $$</p>

<p>I have copied the original Stata v12.1 log file below, which will hopefully tell the whole story. </p>

<pre><code>. gen disgrp=.
(153 missing values generated)

. replace disgrp=1 if disease==""A""
(54 real changes made)

. replace disgrp=2 if disease==""B""
(54 real changes made)

. replace disgrp=3 if disease==""C""
(45 real changes made)

. lnskew0 lfibr= fibrosisp

       Transform |         k     [95% Conf. Interval]       Skewness
-----------------+--------------------------------------------------
  ln(fibrosis-k) |   .0116473      (not calculated)        -2.77e-08

    . gen region1=.
(153 missing values generated)

. replace region1=1 if region==""P""
(51 real changes made)

. replace region1=2 if region==""Q""
(51 real changes made)

. replace region1=3 if region==""R""
(51 real changes made)

. gen wall1=.
(153 missing values generated)

. replace wall1=1 if wall==""X""
(51 real changes made)

. replace wall1=2 if wall==""Y""
(51 real changes made)

. replace wall1=3 if wall==""Z""
(51 real changes made)

. **Comparing C with B**

. xi:regress lfibr disgrp if disgrp &gt; 1  , cluster(pat)

Linear regression                                      Number of obs =      99
                                                       F(  1,    10) =   20.51
                                                       Prob &gt; F      =  0.0011
                                                       R-squared     =  0.3884
                                                       Root MSE      =   .5833

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .9241372    .204061     4.53   0.001     .4694609    1.378813
       _cons |  -4.667246   .4602243   -10.14   0.000    -5.692689   -3.641802
------------------------------------------------------------------------------

. xi:regress lfibr region1 if disgrp &gt; 1  , cluster(pat)

Linear regression                                      Number of obs =      99
                                                       F(  1,    10) =    3.17
                                                       Prob &gt; F      =  0.1055
                                                       R-squared     =  0.0068
                                                       Root MSE      =  .74333

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
     region1 |   .0748154   .0420368     1.78   0.105    -.0188483    .1684792
       _cons |   -2.54854    .177876   -14.33   0.000    -2.944872   -2.152207
------------------------------------------------------------------------------

. xi:regress lfibr i.region1 if disgrp &gt; 1  , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  2,    10) =    5.59
                                                       Prob &gt; F      =  0.0235
                                                       R-squared     =  0.0612
                                                       Root MSE      =  .72645

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
 _Iregion1_2 |   .4400513   .1398977     3.15   0.010     .1283397    .7517628
 _Iregion1_3 |   .1496308   .0845103     1.77   0.107    -.0386698    .3379314
       _cons |   -2.59547   .1905071   -13.62   0.000    -3.019946   -2.170993
------------------------------------------------------------------------------

. xi:regress lfibr i.wall1 if disgrp &gt; 1  , cluster(pat)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  2,    10) =    6.17
                                                       Prob &gt; F      =  0.0180
                                                       R-squared     =  0.0630
                                                       Root MSE      =  .72575

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
   _Iwall1_2 |   .3285724   .1654396     1.99   0.075    -.0400499    .6971948
   _Iwall1_3 |   .4356131   .1305289     3.34   0.008     .1447766    .7264496
       _cons |  -2.653637   .1780801   -14.90   0.000    -3.050425    -2.25685
------------------------------------------------------------------------------

. xi3:regress lfibr disgrp*i.region1*i.wall1 if disgrp &gt; 1 , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  8,    10) =       .
                                                       Prob &gt; F      =       .
                                                       R-squared     =  0.5401
                                                       Root MSE      =  .55355

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .5419458   .4154947     1.30   0.221    -.3838342    1.467726
 _Iregion1_2 |  -.2963738   1.409317    -0.21   0.838    -3.436529    2.843781
 _Iregion1_3 |  -.2791626   1.335259    -0.21   0.839    -3.254304    2.695979
   _Iwall1_2 |  -1.039268     .97762    -1.06   0.313    -3.217541    1.139005
   _Iwall1_3 |  -.9227228   1.131622    -0.82   0.434    -3.444133    1.598687
    _IdiXre2 |    .305921   .5859783     0.52   0.613    -.9997201    1.611562
    _IdiXre3 |   .2146185   .5228838     0.41   0.690    -.9504393    1.379676
    _IdiXwa2 |   .5887627   .4158743     1.42   0.187    -.3378629    1.515388
    _IdiXwa3 |   .5677226   .5322211     1.07   0.311      -.61814    1.753585
   _Ire2Xwa2 |   .9560212   1.372943     0.70   0.502    -2.103087    4.015129
   _Ire2Xwa3 |   1.876106   1.632401     1.15   0.277    -1.761111    5.513323
   _Ire3Xwa2 |   .1403149   1.711091     0.08   0.936    -3.672233    3.952863
   _Ire3Xwa3 |   .5961959   1.627029     0.37   0.722     -3.02905    4.221442
_IdiXre2Xwa2 |  -.4387073   .5346165    -0.82   0.431    -1.629907    .7524925
_IdiXre2Xwa3 |  -.7328102   .7126107    -1.03   0.328    -2.320606    .8549855
_IdiXre3Xwa2 |  -.1024311   .6268405    -0.16   0.873    -1.499119    1.294257
_IdiXre3Xwa3 |  -.3174033   .6961228    -0.46   0.658    -1.868461    1.233655
       _cons |  -4.217918   .9792797    -4.31   0.002     -6.39989   -2.035947
------------------------------------------------------------------------------

. xi3:regress lfibr disgrp i.region1 i.wall1 if disgrp &gt; 1 , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  5,    10) =   10.60
                                                       Prob &gt; F      =  0.0010
                                                       R-squared     =  0.5127
                                                       Root MSE      =  .53177

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .9241372   .2084032     4.43   0.001     .4597858    1.388488
 _Iregion1_2 |   .4400513   .1421362     3.10   0.011      .123352    .7567505
 _Iregion1_3 |   .1496308   .0858625     1.74   0.112    -.0416828    .3409444
   _Iwall1_2 |   .3285724   .1680868     1.95   0.079    -.0459482    .7030931
   _Iwall1_3 |   .4356131   .1326175     3.28   0.008     .1401229    .7311033
       _cons |  -5.118535   .4532473   -11.29   0.000    -6.128433   -4.108637
------------------------------------------------------------------------------
</code></pre>

<p>csv data:</p>

<pre><code>""row"",""PatientID"",""Disease"",""Wall"",""Region"",""FibrosisP""
""1"",1,""C"",""X"",""P"",0.11574464021797
""2"",1,""C"",""X"",""Q"",0.06409239204845
""3"",1,""C"",""X"",""R"",0.05589004594181
""4"",2,""C"",""X"",""P"",0.08452786770152
""5"",2,""C"",""X"",""Q"",0.19765474370344
""6"",2,""C"",""X"",""R"",0.29491566808792
""7"",3,""C"",""X"",""P"",0.13849556170319
""8"",3,""C"",""X"",""Q"",0.21529108879539
""9"",3,""C"",""X"",""R"",0.23260346696877
""10"",4,""C"",""X"",""P"",0.03242538798989
""11"",4,""C"",""X"",""Q"",0.18213249953927
""12"",4,""C"",""X"",""R"",0.0464009382069
""13"",17,""C"",""X"",""P"",0.12925196186539
""14"",17,""C"",""X"",""Q"",0.16685146683109
""15"",17,""C"",""X"",""R"",0.16298253982187
""16"",5,""B"",""X"",""P"",0.06082167946576
""17"",5,""B"",""X"",""Q"",0.06179248715729
""18"",5,""B"",""X"",""R"",0.04635879285168
""19"",6,""B"",""X"",""P"",0.0512284261286
""20"",6,""B"",""X"",""Q"",0.05560175796177
""21"",6,""B"",""X"",""R"",0.05038057719884
""22"",7,""B"",""X"",""P"",0.03485909775192
""23"",7,""B"",""X"",""Q"",0.07526805988175
""24"",7,""B"",""X"",""R"",0.03989544438546
""25"",8,""B"",""X"",""P"",0.05069990522336
""26"",8,""B"",""X"",""Q"",0.11638788902232
""27"",8,""B"",""X"",""R"",0.23086071670409
""28"",9,""B"",""X"",""P"",0.12712370092246
""29"",9,""B"",""X"",""Q"",0.05070659692429
""30"",9,""B"",""X"",""R"",0.06183074530974
""31"",10,""B"",""X"",""P"",0.04509566111129
""32"",10,""B"",""X"",""Q"",0.09050081347533
""33"",10,""B"",""X"",""R"",0.05178363738579
""52"",1,""C"",""Y"",""P"",0.14421181658066
""53"",1,""C"",""Y"",""Q"",0.1299066509205
""54"",1,""C"",""Y"",""R"",0.14904819595697
""55"",2,""C"",""Y"",""P"",0.08801608368174
""56"",2,""C"",""Y"",""Q"",0.24864891863453
""57"",2,""C"",""Y"",""R"",0.15962998919524
""58"",3,""C"",""Y"",""P"",0.4272296674396
""59"",3,""C"",""Y"",""Q"",0.2593375589095
""60"",3,""C"",""Y"",""R"",0.26700346966879
""61"",4,""C"",""Y"",""P"",0.14002780500134
""62"",4,""C"",""Y"",""Q"",0.28346720806288
""63"",4,""C"",""Y"",""R"",0.19312813953225
""64"",17,""C"",""Y"",""P"",0.17668051188556
""65"",17,""C"",""Y"",""Q"",0.18609876357474
""66"",17,""C"",""Y"",""R"",0.26587590290484
""67"",5,""B"",""Y"",""P"",0.05356234036154
""68"",5,""B"",""Y"",""Q"",0.04731210983269
""69"",5,""B"",""Y"",""R"",0.04877515848359
""70"",6,""B"",""Y"",""P"",0.06240572241178
""71"",6,""B"",""Y"",""Q"",0.13301297541279
""72"",6,""B"",""Y"",""R"",0.17973855854636
""73"",7,""B"",""Y"",""P"",0.06463245380331
""74"",7,""B"",""Y"",""Q"",0.10244742460486
""75"",7,""B"",""Y"",""R"",0.0599854720435
""76"",8,""B"",""Y"",""P"",0.05824947941558
""77"",8,""B"",""Y"",""Q"",0.11926213239492
""78"",8,""B"",""Y"",""R"",0.04685947691071
""79"",9,""B"",""Y"",""P"",0.06752011460398
""80"",9,""B"",""Y"",""Q"",0.09542812038592
""81"",9,""B"",""Y"",""R"",0.08668150350578
""82"",10,""B"",""Y"",""P"",0.06486814661182
""83"",10,""B"",""Y"",""Q"",0.05854476138367
""84"",10,""B"",""Y"",""R"",0.04438863783229
""103"",1,""C"",""Z"",""P"",0.05133333746688
""104"",1,""C"",""Z"",""Q"",0.14821006659988
""105"",1,""C"",""Z"",""R"",0.08174176027544
""106"",2,""C"",""Z"",""P"",0.23884995419341
""107"",2,""C"",""Z"",""Q"",0.2099355433643
""108"",2,""C"",""Z"",""R"",0.13176723596276
""109"",3,""C"",""Z"",""P"",0.46479557484677
""110"",3,""C"",""Z"",""Q"",0.33304596595977
""111"",3,""C"",""Z"",""R"",0.29770388592371
""112"",4,""C"",""Z"",""P"",0.15308213537672
""113"",4,""C"",""Z"",""Q"",0.28081619128875
""114"",4,""C"",""Z"",""R"",0.24592983188039
""115"",17,""C"",""Z"",""P"",0.21312809357862
""116"",17,""C"",""Z"",""Q"",0.23336174725733
""117"",17,""C"",""Z"",""R"",0.22714195157817
""118"",5,""B"",""Z"",""P"",0.06818263568709
""119"",5,""B"",""Z"",""Q"",0.07257093444773
""120"",5,""B"",""Z"",""R"",0.08201262934886
""121"",6,""B"",""Z"",""P"",0.0644884733419
""122"",6,""B"",""Z"",""Q"",0.11937946452025
""123"",6,""B"",""Z"",""R"",0.07081608918845
""124"",7,""B"",""Z"",""P"",0.06720225949377
""125"",7,""B"",""Z"",""Q"",0.12509595330262
""126"",7,""B"",""Z"",""R"",0.06657357031905
""127"",8,""B"",""Z"",""P"",0.05878644062606
""128"",8,""B"",""Z"",""Q"",0.26638352132337
""129"",8,""B"",""Z"",""R"",0.06789933388591
""130"",9,""B"",""Z"",""P"",0.0908078338911
""131"",9,""B"",""Z"",""Q"",0.17670466924957
""132"",9,""B"",""Z"",""R"",0.10642489420997
""133"",10,""B"",""Z"",""P"",0.05107976253608
""134"",10,""B"",""Z"",""Q"",0.07242867177979
""135"",10,""B"",""Z"",""R"",0.05074329491013
</code></pre>

<p>My interpretation is that we have 99 observations for 3 categorical variables (2-3 categories each) in the regression analysis; Root MSE = 0.53177.</p>

<p>Is overfitting a valid concern here? If so, is there any way to address it?</p>

<p>A general answer would be helpful. Moreover, I'm trying to move to R (rather than Stata), so advice on replicating the analysis and/or addressing overfitting with R would be gratefully received.</p>

<p>ADDENDUM1:
I've partially worked out how to replicate analysis in R. Haven't yet replicated lnskew0, though <a href=""https://rpubs.com/chrisbrunsdon/skewness"" rel=""nofollow"">https://rpubs.com/chrisbrunsdon/skewness</a> looks similar.</p>

<pre><code>p2.df &lt;- read.table(""data_above.csv"", header=TRUE, sep="","")

library(foreign)
library(sandwich)
library(lmtest)
library(DAAG)

options(digits = 8)  # for more exact comparison with Stata's output

#create ln FibrosisP (need to replicate lnskew0 from Stata - manual k entered here from Stata calculation)

p2.df$FibrosisPln &lt;- log(p2.df$FibrosisP-0.0116473)

p1.df &lt;- p2.df[c(""PatientID"", ""DiseaseG"", ""WallG"", ""RegionG"", ""FibrosisPln"")]

p1.df$DWR &lt;- paste(p1.df$DiseaseG, p1.df$WallG, p1.df$RegionG)

p.df &lt;- pdata.frame(p1.df, index = c(""PatientID"", ""DWR""), drop.index = F, row.names = T)

# tools_reg.R from http://www.existencia.org/pro/?p=134
source(""tools_reg.R"")
mod &lt;- lm(FibrosisPln~factor(DiseaseG)+factor(WallG)+factor(RegionG),data=p.df)
get.coef.clust(mod, p.df$PatientID) # identical to Stata output
</code></pre>

<p>ADDENDUM2:
I have performed 10-fold cross-validation with CVlm from R package DAAG. However, I am uncertain how to interpret the results. Does the presence of overlapping lines in the plot for all 10 folds suggest no overfitting is present?</p>

<pre><code>library(foreign)
library(sandwich)
library(lmtest)
library(DAAG)
CVlm(df=p.df, form.lm=mod, m=10, plotit = c(""Observed"",""Residual""), main=""Small symbols show cross-validation predicted values"", legend.pos=""topleft"", printit=TRUE)
</code></pre>

<p><img src=""http://i.stack.imgur.com/HqTzi.png"" alt=""CVlm plot""></p>

<pre><code>t test of coefficients:

                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -3.2703     0.1441  -22.70  &lt; 2e-16 ***
factor(DiseaseG)C   0.9241     0.2084    4.43  2.5e-05 ***
factor(WallG)Y      0.3286     0.1681    1.95   0.0536 .  
factor(WallG)Z      0.4356     0.1326    3.28   0.0014 ** 
factor(RegionG)Q    0.4401     0.1421    3.10   0.0026 ** 
factor(RegionG)R    0.1496     0.0859    1.74   0.0847 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>ADDENDUM3:
After a lot of searching, the only explanation I could find online (which I presume is accurate) is given at <a href=""http://rstatistics.net/regression-modelling/"" rel=""nofollow"">http://rstatistics.net/regression-modelling/</a></p>

<blockquote>
  <p>""The fitted lines of different colors are parallel and on-top of each other. Indicating a stable model direction and less influence of outliers.""</p>
</blockquote>
"
"0.0693653206906364","0.0679889413649005","159470","<p>I have a collection of data, obtained from different studies. To plot the ratio of means against different CO2 concentrations, I used a random effects model with a continues predictor (the CO2 concentrations given in ppm). </p>

<p>I also did a meta-regression, by assigning co2 values in to three groups (High co2, medium co2 and low co2). The ratio of means was the highest for the ""low co2"" group and the lowest for the ""medium co2"" group. The response is clearly not linear. </p>

<p>How could one fit various non linear models do the date, and test how well they fit the data? A <a href=""http://stats.stackexchange.com/questions/122196/nonlinear-meta-regression"">very nice tutorial</a> on fitting a quadratic polynomial model to the data exits. Is the best way to just try different polynoms and see which model result gets the highest p value?</p>

<pre><code>dat&lt;- read.csv(file=""C:/data.csv"",head=TRUE,sep="","")
library(metafor)

dat&lt;- escalc(measure=""ROM"", m1i = mean_t, m2i = mean_c, sd1i = sd_t, sd2i = sd_c, n1i = n, n2i = n, data = dat)
metaa&lt;- rma(yi, vi, method=""DL"", data=dat, mods=cbind(ppm))

wi &lt;- 1/sqrt(dat$vi)
size  &lt;- 0.5 + 3.0 * (wi - min(wi))/(max(wi) - min(wi))
plot(dat$ppm, exp(dat$yi), pch=19, cex=size,  xlab=""PPM"", ylab=""Reaction rate"",las=1, bty=""l"", log=""y"")

preds &lt;- predict(metaa, newmods=c(540:740), transf=exp)
lines(540:740, preds$pred)
lines(540:740, preds$ci.lb, lty=""dashed"")
lines(540:740, preds$ci.ub, lty=""dashed"")
</code></pre>

<p>The data: </p>

<pre><code>   number mean_c mean_t   sd_c   sd_t  n ppm
1       1  36.85  48.61  28.40  24.54 20 700
2       2  31.36  29.01  16.83  21.04 20 700
3       8  29.00  35.00   3.03   3.03  4 700
4       9  26.12  41.05   7.50   4.14 12 700
5      13  38.20  34.90   9.68  11.23 15 550
6      14  38.20  36.30   9.68  12.01 15 550
7      15  21.00  55.20  12.07   8.05 20 550
8      16  62.00  62.00   9.80   9.80  6 700
9      17  53.00  53.00   7.35   9.80  6 700
10     18  76.00  63.00   7.35  17.15  6 700
11     23 258.00 249.00 101.19 199.22 10 700
12     24  12.00  23.75   6.48   6.48  8 560
13     25  11.25  20.63   6.48   6.48  8 560
14     26  17.63  25.75   6.48   6.48  8 560
15     27  16.38  19.00   6.48   6.48  8 560
16     46 360.00 360.00 200.92 259.81 12 600
17     47 170.00 234.00  90.07  62.35 12 600
18     48 228.00 284.00  38.11 131.64 12 600
19     49 260.00 340.00 263.27 443.41 12 600
20     50  75.00 147.00  65.82 110.85 12 600
21     51 138.00 240.00 110.85 242.49 12 600
22     52  94.00 157.00 110.85 138.56 12 600
23     82 154.00 154.00  90.07  31.18 12 540
24     83 156.00 329.00 110.85  76.21 12 540
25     84 163.00 293.00 100.46  45.03 12 540
26     94 376.00 418.00 148.63 132.82 10 740
27     95  29.00  36.00  41.11  82.22 10 740
28     96 188.00 403.00 117.00  94.87 10 740
29     97 121.30 207.80  34.47  43.64 10 700
30     98 278.30 146.20  82.54  25.93 10 700
31    120 212.00 226.00 153.36 169.79 30 700
32    121 568.00 663.00  83.14 121.24 12 550
33    122 677.00 648.00 131.64 173.21 12 550
34    123 279.00 449.00 117.00 154.95 10 730
35    124 266.00 352.00 139.14 211.87 10 730
36    125  51.66  53.94  52.81  40.49  8 700
37    126  44.81  44.19  66.90  61.03  8 700
38    127  14.56  21.10  26.76  17.60  8 700
</code></pre>

<p><img src=""http://i.stack.imgur.com/Vqknd.jpg"" alt=""enter image description here""></p>
"
"0.0424774103841449","0.0555127381653369","160108","<p>I would like help understanding why a survival regression with no censored
data-points does not give the same variance estimates as a linear model
(see code below).</p>

<p>I think it must be something to do with the fact that the variance is an
actual parameter in the survival version via the log(scale), and possibly
that different assumptions are made about the distribution of the variance.
But I really don't know, I'm just guessing.</p>

<p>The reason I ask is because I am moving a process, that has always been
modelled using a linear model, to a survival model (because there are
sometimes a few censored data points). In the past, the censored data
points have been treated as missing which imparts bias. The variance of the
estimates in this process is key, so I need to know why they are changing
in this systematic way?!</p>

<pre><code>library(survival)

ctl &lt;- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
ctl.surv &lt;- Surv(ctl)

trt &lt;- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)

lmod &lt;- lm     (ctl      ~ trt                )
smod &lt;- survreg(ctl.surv ~ trt,dist=""gaussian"")

coef(lmod)
coef(smod) # same

vcov(lmod)
vcov(smod) # smod is smaller

diag(vcov(lmod))     /
diag(vcov(smod))[1:2]  # 1.25 == 0.5*(n/(n-1))

( summary(lmod)$coef [   ,""Std. Error""] /
      summary(smod)$table[1:2,""Std. Error""]   )^2    # 1.25 = 0.5*(n/(n-1))
</code></pre>
"
"0.0490486886395286","0.0480754414848157","160281","<p>Here is what i am doing. I am building a logarithmic model in linear form based on the correlation between two variables shown in the graph!</p>

<p>lm(y~logx,data=logdata) -- i have only one predictor and one response variable<img src=""http://i.stack.imgur.com/6bqBB.png"" alt=""enter image description here""></p>

<p>cor(x,y) = -0.57(bit low for the data i have but they are negatively correlated in general sense and cor should be about atleast -0.80 in normal cases)</p>

<p>Multiple R-square = 35.78%</p>

<p>Data set doesn't contain any null/missing values. But from the graph you can see the concentration of data points is more at the center!</p>

<p>What are the different factors that i should think of in order to improve my model? here is one what i think off -</p>

<ol>
<li>Remove the outliers -- how should i identify outliers?</li>
<li>Should I remove all the data points that are sparsely distributed? -- I am concerned like that is going to effect my model?</li>
</ol>

<p>Do i have to go with regression modeling in this case which is not giving me the best prediction? If not can anyone please suggest me the best algorithm that should be used in this case?</p>
"
"0.0633215847514023","0.0620651280774201","160435","<p>I'm diving into arima models and was trying to repreduce the results of auto regression.</p>

<p>here is a reproducable example:</p>

<pre><code>set.seed(1)
z=arima.sim(n = 101, list(ar = c(0.8)))
</code></pre>

<p>when running ar(1) without an intercept </p>

<pre><code>&gt; ceof(arima(z, order = c(1,0,0),include.mean =FALSE))
ar1 
0.7622461
</code></pre>

<p>when comparing to a linear regression </p>

<pre><code>&gt; coef(lm(z[2:101] ~ z[1:100] + 0))
z[1:100] 
0.7586725 
</code></pre>

<p>which are very similar and can be explained by the different methods used.
However when I do this comparison with models that include an intercept, I get again similar results in the ar1 coefficient but very different measures for the intercept. while the intercept that I get in the arima model is the one that makes less sense to me.</p>

<pre><code>&gt; coef(arima(z, order = c(1,0,0)))
      ar1 intercept 
0.7274511 0.4241322 
&gt; coef(lm(z[2:101] ~ z[1:100]))
(Intercept)    z[1:100] 
  0.1578015   0.7130261 
</code></pre>

<p>Any ideas on these differencing and in what way the arima procedure is different?</p>
"
"0.0506572678011219","0.0496521024619361","160545","<p>I recently ran two tests in R - one using glm() and one using lm() with the goal being to test the relationship between a binary response and binary predictor.  I ran glm() first and got an estimate of -0.68 for the predictor coefficient which I thought was pretty good.  P&lt;.05 and AIC of 653.  </p>

<p>When I ran lm() however I got an estimate of -.14, a multiple r-squared of .008, P&lt;.05.  </p>

<p>My understanding is that linear regression is usually a poor choice for a categorical response compared with logistic regression, but when is this not the case? I noticed in this post <a href=""http://statisticalhorizons.com/linear-vs-logistic"" rel=""nofollow"">http://statisticalhorizons.com/linear-vs-logistic</a> that the author states there's middle ground where it does make sense to use linear regression.  Are there any common rules (or rules of thumb you personally use) that determine when to try out linear regression on a categorical response?  Do any of these differ from the author's cases?</p>
"
"0.0849548207682898","0.0832691072480053","161113","<p>I am working on example 7.3.1 from the Second Edition of the book <a href=""https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=an+introduction+to+generalized+linear+models+second+edition+pdf"" rel=""nofollow"">An Introduction to Generalized Linear Models</a> in section <em>7.3 Dose response models</em>. This example fits a simple logistic regression model on the following data: </p>

<p><img src=""http://i.stack.imgur.com/YkHCG.png"" alt=""enter image description here""></p>

<p>This seems easy enough. However, I am having an issue with the Deviance Statistic calculated for this example. The following is my R code that will reproduce a Deviance Statistic $D=11.23$ just like this example in the book has. </p>

<pre><code>#original data
#copied in by row
( df &lt;-  data.frame( 
  Trial = 1:8,
  Dose = c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839),
  Yes = c(6, 13, 18, 28, 52, 53, 61, 60),
  No = c(59, 60, 62, 56, 63, 59, 62, 60)- c(6, 13, 18, 28, 52, 53, 61, 60),
  Total = c(59, 60, 62, 56, 63, 59, 62, 60)
) )

#Logistic Regression Model
mle_beet &lt;- glm(cbind(Yes, No)~Dose, family=binomial(logit), data=df)
mle_beet$deviance
##
</code></pre>

<p>Section 5.6.1 of this same book derives the <em>Deviance Statistic</em> for the Binomial Model to be: </p>

<p>$D = 2\sum^{N}_{i=1}y_{i}[ log_{e}(\frac{y_i}{\hat{y_i}})+(n_i - y_i)log_{e}(\frac{n_i - y_i}{n_i - \hat{y_i}}) ]$</p>

<p>However, looking closely at the given data, it can be seen that for the last row, the number of beetles killed is the same as the total number of beetles ( $n_{8}=y_{8}$ ). This means that the very last part in the sum for <code>D</code> is: </p>

<p>$ y_{8}log_{e}(\frac{y_8}{\hat{y_8}})+(n_8 - y_8)log_{e}(\frac{n_8 - y_8}{n_8 - \hat{y_8}}) = 60log_{e}(\frac{60}{\hat{y_8}})+(0)log_{e}(\frac{0}{n_8 - \hat{y_8}})$</p>

<p>In particular, this value contains: </p>

<p>$0log_{e}(0)=0(-\infty)=$ <strong><em>undefined</em></strong></p>

<p>Here is the R code that agrees with this: </p>

<pre><code>sum( 2*(df$Yes*(log(df$Yes/(mle_beet$fitted.values*df$Total))) + (df$Total-df$Yes)*
log((df$Total-df$Yes)/(df$Total-mle_beet$fitted.values*df$Total) ) ) )
</code></pre>

<p>My question is: What is the mathematical reasoning for computing the Deviance Statistic when $n_i=y_i$? What do the book and R do in the background to obtain $D=11.23$?</p>

<p>(Note that the book likely didn't use R to get this value, but the two agree)</p>

<p>Thank you!</p>

<p>EDIT: See the accepted answer and its comments for a great explanation.</p>

<p>If you happen to be computing the Deviance through the formula in R (you likely shouldn't since <code>mle_beet$deviance</code> shows this for you), you can replace <code>-Inf</code> or <code>Nan</code> in each vector that results from an individual operation. The following works for this example: </p>

<pre><code>x &lt;- df$Yes*(log(df$Yes/(mle_beet$fitted.values*df$Total))) 
x[is.na(x) | x==-Inf ] &lt;- 0 #only in a case $n_i = y_i$ 
y &lt;- (df$Total-df$Yes)*
    log((df$Total-df$Yes)/(df$Total-mle_beet$fitted.values*df$Total) ) ) 
    y[is.na(y) | y==-Inf ] &lt;- 0 #only in a case $n_i = y_i$ 

sum(x+y)*2 #the deviance
</code></pre>
"
"0.0633215847514023","0.0620651280774201","161234","<p>Les say I have a data set with several measures and one factor (classification) like the one bellow (for the sake of simplicity, I'm simulating 10 rows and 5 variables only)</p>

<p>I'd like to know how much each variable contribute to the overall classification. I thought about running a linear regression, but I'm wondering if it makes sense to use it to ""explain"" a factor </p>

<p>When I run <code>lm(classification ~ ., data =data)</code> I get a warning   saying </p>

<pre><code>Warning messages:
1: In model.response(mf, ""numeric"") :
  using type = ""numeric"" with a factor response will be ignored
2: In Ops.factor(y, z$residuals) : â€˜-â€™ not meaningful for factors
</code></pre>

<p>but I do get a result (intercept and coefficients for each variable).</p>

<p>My questions are: do they make any sense? And: is there a better way to get to the answer I'm looking for?</p>

<pre><code>   classification  variable_1  variable_2 variable_3 variable_4 variable_5
1               5 -0.90174176 -0.64796703  1.2106427 -0.9229394 -0.6578518
2               5  1.75760214  0.18486432  0.2018499  0.1301168 -0.6510428
3               8 -0.29445029 -0.23108298 -2.6244614  0.3745607  0.3124868
4               4  0.78639724  1.04943276 -0.6047869 -0.4275781  0.6395614
5               3 -2.06554518  0.07336021  2.8142735  1.0558045 -0.1818247
6               4  0.04374419 -0.13775079  0.6132946 -0.5890983  1.9965892
7              10 -1.46731867  1.00367532 -0.8626940 -1.8378582  0.2702731
8               8  0.27206146 -0.13775707  2.6827356  1.5554446  0.1549394
9               5  0.58075881  2.03567118  0.2056770 -0.2935464 -1.3586576
10              9  0.57725709 -0.25396790  0.6640166 -1.9626897  0.3650243
</code></pre>

<p>Code to reproduce it:</p>

<pre><code>data &lt;- data.frame(classification=sample(3:10,replace=TRUE,size=10))

for(i in 1:5){
  data[,paste0(""variable_"",i)]&lt;-rnorm(10)
}
</code></pre>

<p>thanks</p>
"
"NaN","NaN","161439","<p>I'm in the process of evaluating some behavioral data I've collected and cannot find a package to obtain relative factor importance for the following linear model:
<code>
cbind(accuracy, rt) ~ A + B + C + D
</code></p>

<p>In an earlier, parallel â€“ but univariate â€“ experiment I was able to use the <code>relaimpo</code> R package, which worked fine. But it is unable, I believe, to support multivariate analysis. I'm aware of Tonidandel &amp; LeBreton's <a href=""http://relativeimportance.davidson.edu/multivariateregression.html"" rel=""nofollow"">online tool/downloadable R code</a>, but was hoping their might be an alternative package-based solution. Am I in luck, or is their code the best bet out there right now?</p>
"
"0.0200240432865818","0.039253433598943","161614","<p>I want to solve the first exercice of the Multiple Regression Chapter of R. Hyndman's online book on Time Series Forecasting (see <a href=""https://www.otexts.org/fpp/5/8"" rel=""nofollow"">https://www.otexts.org/fpp/5/8</a>). I use <code>R</code> with <code>fpp</code> package as wanted in the exercise.</p>

<p>I am blocked in the following question:
c. Use R to fit a regression model to the logarithms of these sales data with a linear trend, seasonal dummies and a â€œsurfing festivalâ€ dummy variable.</p>

<p>Indeed, I don't know how to make the function <code>tslm</code> work with my dummy vector for the surfing festival. Here is my code.</p>

<pre><code>library(fpp)
log_fancy = log(fancy)
dummy_fest_mat = matrix(0, nrow=84, ncol=1)
for(h in 1:84)
    if(h%%12 == 3)   #this loop builds a vector of length 84 with
        dummy_fest_mat[h,1] = 1   #1 corresponding to each month March
dummy_fest_mat[3,1] = 0 #festival started one year later

dummy_fest = ts(dummy_fest_mat, freq = 12, start=c(1987,1))
fit = tslm(log_fancy ~ trend + season + dummy_fest)
</code></pre>

<p>When I do <code>summary(fit)</code>, I see that the regression coefficients have been well calculated, but when I continue with <code>forecast(fit)</code>
I get the following error : </p>

<pre><code>Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
  variables have not equal length (found for 'factor(dummy_fest)')
In addition: Warning message:
'newdata' had 50 rows but variables found have 84 rows 
</code></pre>

<p>But what is even stranger is that when I do <code>forecast(fit, h=84)</code>, it works!!
I don't know what is happening here, can someone explain me?</p>
"
"0.0633215847514023","0.0620651280774201","162113","<p>Let's say I have factor variable Var_1 with 3 levels, ""1"", ""2"", and ""3"". ""1"" is the base level of the factor variable. So when I run a linear regression, I get a p value for each level of that variable. For example, the P value for ""Var_12"" is .05 and ""Var_13"" is .15. In this situation, the following code will work:</p>

<pre><code>p_values &lt;- summary(model_result)$coefficients[substr(names(coef(model_result)),1,nchar(test_var))==test_var,4]
</code></pre>

<p>Now, if I have another variable factor variable named ""Var11"" with levels ""A"" ""B"" and ""C"", and I run the model, and extract the p values with the code above, I am also going to get the p values from Var_11 because the substring from character 1 to 4 is Var_1, the same for ""Var_1"" and ""Var_11"".</p>

<p>I could rename the variables.
I could change the levels from ""1"" ""2"" and ""3"" to letters.</p>

<p>I don't really fancy either solution, is there a better solution for this? Is there a better way in general to extract the p values for one variable?</p>

<p>Thank you!!</p>
"
"0.02831827358943","0.0277563690826684","162218","<p>I have a very basic problem. My time series is modeled as:</p>

<p>$$
\begin{align}
Y_t &amp;= a_t + \beta x_t + v_t \\
a_t &amp;= a_{t-1} + w_t   
\end{align}
$$</p>

<p>where $ w_t$ ~ $N(0,W) $  with known $W$, and $v$ is white noise.</p>

<p>I don't know if I can use the usual linear regression here, since the values for $a$ are not deterministic and are not stationary. What method should I use to estimate $\beta$, and specifically, get $t$-statistics or $p$-values for it? Implementation wise, I use <code>lm()</code> for estimating my regression in R. I would appreciate it if you could mention how to do it in R.</p>
"
"0.0490486886395286","0.0480754414848157","162251","<p>I am trying to reproduce the following example of logistic regression with a transformed linear regression:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
predict(am.glm, newdata, type=""response"") 
##         1 
## 0.6418125
</code></pre>

<p>The equation for the probability of $Y=1$ is the following:
$$
P(Y=1) = {1 \over 1+e^{-(b_0+\sum{(b_iX_i)})}}
$$</p>

<p>So I tried something like this:</p>

<pre><code>am.lm &lt;- lm(am ~ 1/(1+exp(-(hp + wt))),data=mtcars)
predict(am.lm, newdata)
##       1 
## 0.40625
</code></pre>

<p>So this is obviously wrong! (I also tried transforming the given value but nothing worked so far).</p>

<p><strong>My question</strong><br>
How would I have to set up logistic regression with explicitly specifying the formula for the non-linear transformation of the linear model?</p>
"
"0.0642198081225601","0.0734364498908627","162691","<p>I am struggling to calculate / define confidence bands to use with a custom function that I must fit to my data. I have seen plenty of examples for standard models (linear regression, log functions, etc.). However, calculating the intervals to add bands to my custom function is proving tricky. Below I have the data and my function (working in R):</p>

<pre><code>data &lt;- matrix(c(0.08, 0.1, 0.12, 0.13, 0.49, 0.11, 0.12, 0.15, 
                 0.22, 0.47, 7, 8, 9, 21, 30, 3, 8, 13, 15, 17 ), ncol=2)
mycurve &lt;- function(x){ a + (b*log(x)) }
a    &lt;- 31
b    &lt;-  9
data &lt;- as.data.frame(data) # So that ggplot can use it
ggplot(data, aes(V1,V2)) + geom_point() + stat_function(fun=mycurve, color=""red"")
</code></pre>

<p>And the result is:  </p>

<p><a href=""http://i.stack.imgur.com/QZt92.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QZt92.png"" alt=""enter image description here""></a></p>

<p>The red curve is the custom function that I <em>must</em> use for this dataset. This is one example, but I have a few more datasets and must fit a different function to to each dataset. So I'm looking for an approach for calculating confidence bands that I can use for any function.</p>

<p>I have spent quite a while on this, but haven't yet found a ""generic"" solution. </p>
"
"0.0535165067688","0.0524546070649019","162831","<p>I've been playing around with the package <code>strucchange</code> (and to some extent <code>segmented</code>) in R. I'm trying to determine whether there are changes in slope in a linear regression and more importantly, how many breakpoints. A toy dataset:</p>

<pre><code>x &lt;- c(0, 5, 10, 15, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80)
y &lt;- c(-84.16, -86.67, -87.74, -86.07, -89.15, -91.90, -93.64, -95.92,
  -95.96, -99.19, -100.73, -107.29, -106.10, -107.29)
</code></pre>

<p>First problem: if I use the breakpoints function:</p>

<pre><code>breakpoints(y ~ x, data = data.frame(x, y))
</code></pre>

<p>I get the following error:</p>

<pre><code>Error in breakpoints.formula(y ~ x, data = data.frame(x, y)) : 
minimum segment size must be greater than the number of regressors
</code></pre>

<p>I think this arises because the default h parameter in the breakpoints command is 0.15 and 14 (the number of observations I have) * 0.15 = 2.1 which, rounded down, is not greater than 2 (the ""number of regressors"": incidentally, I would have thought the number of regressors would be 1 given my formula but I've learned from other working examples of <code>y ~ x</code> that nreg = 2 in these cases. I guess the intercept counts as a regressor?). </p>

<p>If I set h to 3 or some fraction such that 14 * h >= 3, the command works.</p>

<pre><code>breakpoints(y ~ x, data = data.frame(x, y), h = 3)
</code></pre>

<p>Two breakpoints are returned. But the result is sensitive to h. Such that if I use:</p>

<pre><code>breakpoints(y ~ x, data = data.frame(x, y), h = 4)
</code></pre>

<p>I get a different solution. In the latter case, a single optimal break is found because the minimum number of observations before a break can be called is higher. Is there a way to somehow determine whether one solution has more support than the other? In other words, how best to optimize not the position of breakpoints, but the number of breakpoints (perhaps across values of h)? I think the Fstats command might be the key but I'm having a lot of trouble understanding the help for this command...</p>
"
"0.0424774103841449","0.0555127381653369","163092","<p>Iâ€™m looking to build an ARIMA model in R to help me predict the number of shots a football player is going to take in a game. </p>

<p>I have last season's data to analyse to determine the optimal lags for my AR and MA parameters. I have a data frame in R, with the columns for the player name, date of match and the number of shots. </p>

<p>Unfortunately, I only have a maximum 38 data points for each player which isnâ€™t enough to build a statistically confident model. I suspect I need a way to analyse the data holistically/all-at-once to help me determine the optimal lags.</p>

<p>I donâ€™t, however, know how to do that or even if this is a statistically sound technique. </p>

<p>At the moment I am just analysing my residuals (which have come from a linear regression with independent variables such as Home/Away and Team Possession) with code such as the following:</p>

<pre><code>arima(residuals, order=c(3,0,0))
</code></pre>

<p>Is there a way to instruct R to perform this ARIMA analysis whilst looking at lots of mini-groups (where the groups are categorised by player name)?</p>

<p>Any help would be much appreciated. </p>

<p>Will </p>
"
"0.0400480865731637","0.0196267167994715","163385","<p>I've got some baseball stats, RBIs by season, let's say:</p>

<pre><code>player      s1  s2  s3
Brian_Giles 66  68  70
Joe_Thomas  71  72  71
Robin_Yount 71  69  68
Jim_Jones   66  66  65
</code></pre>

<p>And I want to do a simple linear regression using lm() on this data to predict their RBI #s in a 4th season. Would I need another variable here to create my formula? How would I most simply complete this linear regression?</p>

<p>I'm trying to work off of this tutorial (<a href=""http://www.r-bloggers.com/wp-content/uploads/2009/11/simpleLinRegExample1.txt"" rel=""nofollow"">http://www.r-bloggers.com/wp-content/uploads/2009/11/simpleLinRegExample1.txt</a>), which seems like I might need a second variable, (I'm new to linear regressions, obviously) but I can't figure out what it should be. The slope of a best-fit line for those three seasons of data?</p>
"
"0.0693653206906364","0.0566574511374171","163410","<p>How can I determine whether one coding of a linear predictor leads to a better fit of the corresponding regression model than the other?
In the following example, the restricted cubic spline coding of albumin leads to a higher chi-square value of the resulting model compared to the linear coding. However, it has also more degrees of freedom. As I understand it, I cannot use the log likelihood test in this case, since both models are not nested.</p>

<p>What should I do?</p>

<pre><code>&gt; library(rms)
&gt; 
&gt; data(pbc)
&gt; d &lt;- pbc
&gt; rm(pbc, pbcseq)
&gt; d$status &lt;- ifelse(d$status != 0, 1, 0)
&gt; 
&gt; dd = datadist(d)
&gt; options(datadist='dd')
&gt; 
&gt; # linear model
&gt; m1 &lt;- cph(Surv(time, status) ~  albumin, data=d)
&gt; anova(m1)
                Wald Statistics          Response: Surv(time, status) 

 Factor     Chi-Square d.f. P     
 albumin    73.51      1    &lt;.0001
 TOTAL      73.51      1    &lt;.0001
&gt; 
&gt; # rcs model
&gt; m2 &lt;- cph(Surv(time, status) ~  rcs(albumin, 4), data=d)
&gt; anova(m2)
                Wald Statistics          Response: Surv(time, status) 

 Factor     Chi-Square d.f. P     
 albumin    82.80      3    &lt;.0001
  Nonlinear  4.73      2    0.094 
 TOTAL      82.80      3    &lt;.0001
</code></pre>

<h2>UPDATE #1</h2>

<p>I thought plotting both models would be a good way to decide whether a linear coding or a restricted cubic spline coding would be best. In this case (see below), I would think that the more complex coding is not better. However, the core of my question aimed to reinforce the eyeballing by a statistical test. But as I understand you correct, this is prone to over-fitting?</p>

<p><a href=""http://i.stack.imgur.com/UD5wK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UD5wK.png"" alt=""enter image description here""></a></p>
"
"0.0849548207682898","0.0832691072480053","163604","<p>I'm using a plate reader to measure optical density of different bacterial
strains so I can compare their responses (growth rates and changes in them over
time) to stress conditions. The growth curves often don't follow any standard
shape so I'm fitting them empirically with the <code>loess</code> or <code>locfit</code> functions in
R, breaking the fits into intervals, and taking the derivatives to get growth
rates. My plots look like this:</p>

<p><a href=""http://i.stack.imgur.com/fiiLH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fiiLH.png"" alt=""locfit fitted data points""></a>
<a href=""http://i.stack.imgur.com/4dui4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4dui4.png"" alt=""simplified fitted curves""></a>
<a href=""http://i.stack.imgur.com/4t7K4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4t7K4.png"" alt=""derivatives of simplified curves""></a></p>

<p>As you can see the fitted curves have confidence intervals, but I'm not sure
how to transform them into a meaningful form (95% confidence or standard
deviation for example). And assuming that's doable, how do I go on to calculate
uncertainty in the rates?</p>

<p>I suppose I could just use the worst-case difference in slopes like this:</p>

<p><a href=""http://i.stack.imgur.com/fcEaY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fcEaY.png"" alt=""bad idea""></a></p>

<p>But that seems like a bad idea.</p>

<p>I could fit each well separately or split them into groups--there are a few
replicates for each strain and I could add more if needed--and just use the
standard deviation of the final calculated rates. Is that the best way? If so,
how do I decide the optimal group size to balance accurate fits with a good
number of replicates? I would also be open to using a different type of fit of course.</p>

<p>I've found a couple related questions, but neither one quite answers it:</p>

<ul>
<li><p><a href=""http://stats.stackexchange.com/questions/70629/calculate-uncertainty-of-linear-regression-slope-based-on-data-uncertainty"">This one</a> seems to rely on the true relationship being linear, which my curves violate</p></li>
<li><p><a href=""http://stats.stackexchange.com/questions/18391/how-to-calculate-the-difference-of-two-slopes"">This one</a> may well be correct but my stats knowledge is too basic to understand the answer</p></li>
</ul>

<p>EDIT: I'm using <code>deg=1</code> for both types of fits because I expect growth during log-phase to be linear on a log-transformed scale, but maybe higher-degree polynomials would be more accurate?</p>

<p>EDIT: <a href=""http://stats.stackexchange.com/questions/147106/determining-if-two-growth-curves-are-significantly-different"">This answer</a> looks very promising and I'm off to read the suggested paper.
EDIT: Nope, also depends on having a known underlying physical model.</p>
"
"0.0490486886395286","0.0480754414848157","163696","<p>I have analysed a dataset with a linear regression model, including an interaction term between a binary variable and a continuous variable. The interaction was significant. Afterwards, I have fitted 2 separate models of the continuous variable for each of the 2 groups of the binary variable. The 2 slopes have different signs and one of the two slopes is significant. </p>

<p>I need to calculate the power of the significance of this interaction. I prefer to do this with an R function.  </p>
"
"0.0693653206906364","0.0566574511374171","163922","<p>I am trying to find any evidence of warming in monthly times series data of water temperature over a 21-year period that is serially correlated. Essentially I am looking to determine a global trend, like what can be done with OLS regression with data that is from independent observations. I am at a crossroads in trying to determine whether a seasonal ARIMA model or a linear mixed model with a trend component as detailed by Crawley on page 799 of ""The R Book"" (2nd ed.) is the most appropriate method to use. I therefore explored both techniques, but got very contradicting answers!</p>

<p>ARIMA modelling gave me a seasonal ARIMA of form (2,0,2)(0,0,1)[12], indicating that no differencing is required and therefore that the series is stationary with NO trend.</p>

<p>However, the linear mixed affects modelling, comparing two models with and without a trend component using ANOVA and maximum likelihood indicated a highly significant trend (R notation):</p>

<pre><code>model2: ave ~ sin(time * 2 * pi) + cos(time * 2 * pi) + (1 | factor(yr))

model1: ave ~ index + sin(time * 2 * pi) + cos(time * 2 * pi) + (1 | factor(yr))

ANOVA(model2,model1)

      Df  AIC     BIC     logLik deviance Chisq Chi Df Pr(&gt;Chisq)   
model2 5 346.82   364.49    -168.41   336.82                           
model1 6 338.54   359.74    -163.27   326.54 10.28      1   0.001345 **
</code></pre>

<p>How can this be? What am I missing? Is it about assuming whether the trend is a parametric form (appropriate for linear mixed model) or whatever weird shape (appropriate for ARIMA)? If so how do I go about choosing which approach to adopt?</p>

<p>Thank you kindly for any advice.</p>
"
"NaN","NaN","164099","<p>I have this R code for linear regression:</p>

<pre><code>fit &lt;- lm(target ~ age+sales+income, data = new)
</code></pre>

<p>How to identify influential observations based upon cook's distance and removing the same from data in R ?</p>
"
"0.0424774103841449","0.0555127381653369","164110","<p>I am running a logistic regression on a data set containing Continuous, Ordinal, Categorical and Dichotomic variables.</p>

<p>I would like to know how to calculate the correlation for all possible combinations (see matrix below - cases marked with an X do not occur in my data set) in order to check for colinearity. I can do this either with SAS or R.</p>

<hr>

<pre><code>             Continuous    Ordinal    Categorical     Dichotomic
Continuous        X           1             2              3
Ordinal                       5             6              7
Categorical                                 8              9
Dichotomic                                                 X
</code></pre>

<p>Case 8 I use <code>proc freq data=data chisq ;</code> to return Cramer's V.</p>

<p>As for the rest I am unsure - is it possible for cases 3, 7 and 9 to consider a dichotomic variable as categorical in two classes in order to compute Cramer's V?</p>
"
"0.0400480865731637","0.039253433598943","164314","<p>I have a data set with performance and training data that looks something like (this is not the exact data, but gives a general idea):</p>

<pre><code>&gt;dat
Performance Training
1           1
0           1
1           2
0           2
1           3
1           3
</code></pre>

<p>I want to find if there is are any significant differences between performance means for the respective levels of performance in R.  I have tried linear regression and anova, such as: <code>summary(lm(performance~training))</code> or <code>summary(aov(performance~training))</code> both of which yield non-significant results.  However, when I do a T-test to compare some of the means manually it is telling be significant differences exist.  Any thoughts on how to code what I am looking for or what might be going on here?</p>
"
"0.0490486886395286","0.0480754414848157","164434","<p>Sometimes during modeling we are faced with non-normal data.  One of the presumptions of linear regression is the response data being normal.</p>

<p><em>I wish I found a better example data set to illustrate this.  The cars data doesn't need this proposed method...</em></p>

<p>I never seen this done in a book nor in a class so most likely this is not a good idea. I am curious what others think of this method.  Is there a documented approach that works similar to this?</p>

<p>One method to make continuous response data normal is to take the rank converted to a percentile and bound it between 0.001 and 0.999. (Winsoring it.) Lastly converted it to a zscore.</p>

<pre><code>cars2=cars;
cars2$dist=qnorm(pmin(0.999,pmax(0.001,rank(cars2$dist)/length(rank(cars2$dist)))))
</code></pre>

<p>Then with the original data, making a mapping variable for the values 1:999</p>

<pre><code>map=(quantile(cars$dist,probs = seq(0.001,0.999,0.001)))
</code></pre>

<ul>
<li>skewness(cars dist) is 0.7824835 </li>
<li>skewness(cars2 dist) is 0.378929</li>
<li>rmse(cars dist,fit) is 15.06886    </li>
<li>rmse(cars dist,fit2) is 14.80842</li>
</ul>

<p>So the skew is reduced and performs slightly better.
On large data sets this method almost assuredly gets rid of the skew in the response.</p>

<p>Here is plainly the code:</p>

<pre><code>require(datasets)

hist(cars$speed)
    hist(cars$dist)
m=lm(dist~speed,cars)
fit=predict(m)
skewness(cars$dist)
summary(m)

cars2=cars;
cars2$dist=qnorm(pmin(0.999,pmax(0.001,rank(cars2$dist)/length(rank(cars2$dist)))))
    map=(quantile(cars$dist,probs = seq(0.001,0.999,0.001)))
hist(cars2$dist)
    skewness(cars2$dist)
length(map)
hist(map)
m3=lm(dist~speed,cars2); 
m3=stepAIC(m3,trace=F)
summary(m3)
data=round(pnorm(predict(m3))*1000)
range(data)
fit2=map[data]
plot(cars$dist,fit2,col=""blue"")
    points(cars$dist,fit,col=""red"")

rmse=function(x,y,k=0){
  return( sqrt(sum((x-y)^2)/(length(x)-k)));
}
rmse(cars$dist,fit)
    rmse(cars$dist,fit2)
</code></pre>

<p>So how crazy of an idea is this?
Has this approach been documented/studied before? Where?</p>

<p>Thank you for you commentary. :)</p>
"
"0.0400480865731637","0.039253433598943","164438","<p>I just ran a linear regression in R, where the following is my result:</p>

<pre><code>Call:
lm(formula = Posttest ~ TotalHints + Pretest, 
    data = all)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.51904 -0.09000  0.01243  0.11979  0.41820 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept)      0.3205183  0.0450642   7.112 1.88e-10 ***
    TotalHints      -0.0007066  0.0003323  -2.127    0.036 *  
    Pretest         0.4323069  0.0770656   5.610 1.88e-07 ***
</code></pre>

<p>All terms in the regression significantly contribute to the model, but the coefficient term of TotalHints is significant with an estimate of... 0?  I'm not sure how to conceptually understand this. Does that mean the model is saying that the TotalHints term significantly had no effect?</p>
"
"0.0805952195517515","0.0877733458775107","164705","<p>I'm working on analyzing a time series of physical variables in many lakes in Florida for an associate, and I've run into an issue. I'm attempting to run a regression for each time series of physical variables in each lake. I can get regression results in R easily, but they don't match up with my coworker's JMP results. Anyway, here's a sample from the data:</p>

<pre><code>Year = seq(1987,2015)
TP = c(14, 12, 14, 14, 17, 16, 15, 12, 18, 14, 15, 18, 18, 21, 21, 17, 17, 20, 19, 17, 18, 18, 26, 20, 18, 21, 21, 20, 18)
summary(lm(TP~Year))
</code></pre>

<p>gives </p>

<pre><code>Call:
lm(formula = TP ~ Year)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.7310 -1.3724 -0.4305  0.9685  6.3675 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -502.90542   98.13981  -5.124 2.18e-05 ***
Year           0.26010    0.04904   5.303 1.35e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.21 on 27 degrees of freedom
Multiple R-squared:  0.5102,    Adjusted R-squared:  0.4921 
F-statistic: 28.12 on 1 and 27 DF,  p-value: 1.35e-05
</code></pre>

<p>His JMP analysis spits out the following:</p>

<pre><code>Parameter Estimates
Term        Estimate    Std Error   t Ratio Prob&gt;|t|
Intercept   -500.4634   96.74332    -5.17   &lt;.0001*
Year        0.2588707   0.048347    5.35    &lt;.0001*
</code></pre>

<p>For all lakes and all parameters of interest, the SS, slope estimates, etc. are all slightly off. I have looked into different types of Sum of Squares for ANOVA, but changing to different types (e.g. Type III using Anova()) still doesn't get the results to match up. What am I missing? Any assistance would be appreciated.</p>

<p>Edit: Thanks for y'all's help. Sorry for the belated response, I had to meet up with my colleague. To address the questions:</p>

<ul>
<li>I have hardcoded the data in my question, but it's merely a subset of a much larger dataset from Excel. We are using the same data and the remainder of my code is working properly. <a href=""https://www.dropbox.com/s/k21v38sfdbm0ola/LWFormatted.csv?dl=0"" rel=""nofollow"">Here's what the actual data look like.</a></li>
<li>I know OLS isn't great, but it's being used for some really basic trend descriptions for informing stakeholders. I may pursue a better option in the future.</li>
<li>The JMP model is setup using Y by X with the Bivariate option, then applying a linear regression. Below is a screenshot.</li>
</ul>

<p>Thanks again for your help!</p>

<p><a href=""http://i.stack.imgur.com/ndtBr.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ndtBr.jpg"" alt=""JMP Input""></a></p>
"
"0.0400480865731637","0.039253433598943","165010","<p>I have data $\vec x$ and targets $\vec y$. The targets are each uncertain in either direction up to a number $\delta y$. I also have a vector of weights $\vec w$. </p>

<p>As a simple example in R, let's just say</p>

<pre><code>delta_y &lt;- 1
x &lt;- 1:10
y &lt;- 1:10 + runif(10, -delta_y, delta_y)
w &lt;- 1:10
</code></pre>

<p>Like so:</p>

<p><a href=""http://i.stack.imgur.com/gFSQk.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gFSQk.png"" alt=""simple example""></a></p>

<p>Now, I can do weighted linear regression with</p>

<pre><code>fit &lt;- lm(y ~ x, weights=w)
</code></pre>

<p>From this I can obtain the slope $\beta$ of this relation using</p>

<pre><code>beta &lt;- coef(fit)[2]
</code></pre>

<p>How can I propagate the uncertainty $\delta y$ into $\beta$?</p>

<p>I know I can do</p>

<pre><code>summary(fit)
</code></pre>

<p>and obtain Std. Error for my slope, but is that really taking my uncertainties into account?</p>
"
"0.0506572678011219","0.0620651280774201","165018","<p>I think I understand why orthogonality matters when doing regression with polynomial fits (so that the linear and quadratic, cubic, etc... can be evaluated independently). However, I don't understand what orthogonality even means when it comes to doing simply a linear regression. Specifically, in a regression with both continuous (age) and categorical (sex) variables, how I set up my continuous variable in the model will affect all coefficients.</p>

<pre><code>options(contrasts = c(""contr.treatment"", ""contr.poly""))
agevar=1:60
sexvar &lt;- rep(c(""male"",""female""),each=30)
set.seed(8093)
xvals &lt;- sample(-100:100,60)
m1 &lt;- summary(lm(xvals~sexvar*poly(agevar,1,raw=T))) # uses raw contrasts (i.e. 1:60)
# m1b &lt;- summary(lm(xvals~sexvar*agevar)) # alternative to m1, same result

coef(m1)

                                    Estimate Std. Error    t value  Pr(&gt;|t|)
(Intercept)                      25.13963663 28.0506961  0.8962215 0.3739710
sexvar1                          35.44078606 28.0506961  1.2634548 0.2116605
poly(agevar, 1, raw = T)          0.09955506  0.7997637  0.1244806 0.9013805
sexvar1:poly(agevar, 1, raw = T) -1.26395996  0.7997637 -1.5804168 0.1196438

m2 &lt;- summary(lm(xvals~sexvar*poly(agevar,1,raw=F))) # uses set of contrasts that sum to zero

coef(m2)

                                    Estimate Std. Error    t value   Pr(&gt;|t|)
(Intercept)                        28.176066   13.85039  2.0343159 0.04666608
sexvar1                            -3.109993   13.85039 -0.2245419 0.82315296
poly(agevar, 1, raw = F)           13.354858  107.28465  0.1244806 0.90138052
sexvar1:poly(agevar, 1, raw = F) -169.554469  107.28465 -1.5804168 0.11964377
</code></pre>

<p>Any idea on why the result is different and which way is correct? My actual data set uses the same variables, but is much larger with an unequal number of males/females and different sample size at each age. </p>
"
"0.0749231094763201","0.0734364498908627","165110","<p>I'm struggling with the interpretation of a regression model where a categorial variable (5 levels) is dummy coded. Here is the result of my calculation in R:</p>

<pre><code>Call:
lm(formula = DV ~ Age + Gender + factor(Categorial) + 
Continuous 1 + Continuous 2 + Continuous 3, 
data = dat)

Residuals:
 Min       1Q   Median       3Q      Max 
-1.30058 -0.25326  0.00349  0.28123  1.49877 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)           -0.42367    0.30694  -1.380  0.16842   
Age                   -0.05949    0.02026  -2.936  0.00356 **
Gender                -0.01800    0.04828  -0.373  0.70952   
factor(Categorial)2   -0.30625    0.12645  -2.422  0.01596 * 
factor(Categorial)3   -0.03441    0.07752  -0.444  0.65736   
factor(Categorial)4   -0.12603    0.09914  -1.271  0.20453   
factor(Categorial)5   -0.08417    0.13269  -0.634  0.52630    
Continuous 1           0.12080    0.04346   2.779  0.00575 **
Continuous 2          -0.06592    0.04383  -1.504  0.13354   
Continuous 3          -0.06230    0.03475  -1.793  0.07392 . 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.4259 on 336 degrees of freedom
  (6 observations deleted due to missingness)
Multiple R-squared:  0.1315,    Adjusted R-squared:  0.1057 
F-statistic: 5.089 on 10 and 336 DF,  p-value: 6.353e-07
</code></pre>

<p>Ok. Age, Factor 2 of the categorial variable and the first continuous variable are significant predictors of the dependent variable. so far so good. </p>

<p>What I'm not understanding is:</p>

<ol>
<li><p>The reference category of the dummy coded categorial variable is the intercept and the first category of the categorial variable. right? How do I interpret this? </p></li>
<li><p>When doing an anova with the categorial variable as a independent variable, this factor is a significant predictor. With the results of the linear model, one could conclude that this is only due to category 2, right?</p></li>
<li><p>Can I test contrasts with this linear regression model (e.g. Category1 vs. Category2)?</p></li>
<li><p>Should I include interactions?</p></li>
</ol>

<p>I'd be glad for any help :-)</p>
"
"0.0849548207682898","0.104086384060007","166584","<p>I am conducting a regression in order to predict a tennis player's service point win % i.e. the percentage of points he wins when he is the server.
Model 1 If my DV data lies in the range 0.3-0.9, does it make sense to use a logistic regression? If using logistic I would endeavor to build a model with serve win % as my DV and my IV's as:</p>

<p>+average serve win % of last n matches (maybe n=5 or 10) to account for form </p>

<p>+surface </p>

<p>+player ranking </p>

<p>+opposition ranking</p>

<p>..... Would this be a good model to use? Preliminary logistic regressions just involving serve win % regressed on surface + player ranking + opponent ranking ... are showing some strange results so im losing faith in logistic for this data.</p>

<p>An alternative I'm considering is to use raw variables in a linear regression type model with interactions.... Along the lines of Aiken &amp; West 1991
My dependent variable will be number of service points won in match, and my independent variables will be:</p>

<p>+no. service points played in match +the surface the match played on </p>

<p>+the player's ranking points +the opponents ranking points</p>

<p>+an interaction between player and opponent ranking points </p>

<p>+an interaction between surface and no. points played </p>

<p>+average service points won in last n matches</p>

<p>+average % of service points won in last m matches</p>

<p>Do either of these models stand out as smart or appropriate ways to model this data? For context, for each player I have between 100-350 matches worth of data. I would love to hear what you guys think, or if you have any other suggestions on how to predict serve win % using the stated variables I would really appreciate it. I'm conducting this analysis in R so any code/package suggestions would also be great </p>
"
"0.0755153962384799","0.064764861192893","166796","<p>I performed an experiment where I took the heights of plants and measured a number of environmental conditions (air temp, soil temp, lux, air humidity, soil pH, wind) for each of those plants. I want to then determine the effect that these conditions had on plant height. I have used multiple linear regression many times before in R using the form of:</p>

<pre><code>LinearModelTemp&lt;-lm(Height~ AirTemp+SoilTemp+lux+..., data=data)
</code></pre>

<p>but I would not expect the response of plants to all these conditions to be linear. How do you perform multiple non-linear regression? </p>

<p>So far the options I have found are non-linear least squares and segmented linear regression. For non-linear least squares I would have to set the parameters of the curve and I have no prior ideas for what these are. Furthermore, I am not aware of being able to perform multiple regression using this format. The other option is segmented linear regression, but again I would have to choose where my breakpoints are. I have some idea for where I would expect these to be but surely choosing these breakpoints will have a significant impact on form of the final regression model. </p>

<p>Is there a standard protocol I can follow when performing this or are there other options that I have not considered? </p>
"
"0.0379929508508414","0.0620651280774201","167324","<p>I'm trying to obtain the variance-covariance matrix of a logistic regression:</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
mylogit &lt;- glm(admit ~ gre + gpa, data = mydata, family = ""binomial"")
</code></pre>

<p>through matrix computation. I have been following the example published <a href=""http://www.ats.ucla.edu/stat/r/library/matrix_alg.htm"" rel=""nofollow"">here</a> for the basic linear regression</p>

<pre><code>X &lt;- as.matrix(cbind(1, mydata[,c('gre','gpa')]))
beta.hat &lt;- as.matrix(coef(mylogit))
Y &lt;- as.matrix(mydata$admit)
y.hat &lt;- X %*% beta.hat

n &lt;- nrow(X)
p &lt;- ncol(X)

sigma2 &lt;- sum((Y - y.hat)^2)/(n - p)        
v &lt;- solve(t(X) %*% X) * sigma2
</code></pre>

<p>But then my var/cov matrix doesn't not equals the matrix computed with <code>vcov()</code></p>

<pre><code>v == vcov(mylogit)

1   gre   gpa
1   FALSE FALSE FALSE
gre FALSE FALSE FALSE
gpa FALSE FALSE FALSE
</code></pre>

<p>Did I miss some log transformation?</p>
"
"0.02831827358943","0.0277563690826684","167361","<p>I have run a probit regression and am now trying to run post-hoc tests.  I am trying to compare differences between a 3 level factor variable.</p>

<p>I am confused about the difference between running a 'simultaneous tests for general linear hypotheses' and running the same thing but with a 'Tukey' adjustment- they get very similar answers but is either 'better' or 'worse'? Or does it not matter?  For example:  </p>

<pre><code>library(lsmeans)
lsmeans(m1, pairwise~Name.Origin, adjust=""tukey"") 

library(multcomp)
summary(glht(m1, lsm(pairwise~Name.Origin)))
</code></pre>

<p>In addition, is there a more formal name for the first method which just fits the model?</p>
"
"0.0942489115008991","0.100077011948264","167363","<p>I have no training in Bayesian data analysis, so I can't wrap my head around how to start solving the following problem and am hoping you can help:</p>

<p>I am using linear regression to forecast the net scores (home - visitor) of (American) pro-football games from differences in team-strength scores (home - visitor). Those strength scores fall on a 0-100 scale, and they represent the percent chance that the team in question would beat another team selected at random from the 31 others in the league. The differences between those strength scores and the net game scores are both normally distributed.</p>

<p>Right now, I am using team-strength scores that are fixed for the entire season in a mixed-effects model that also includes random intercepts for each team as the home team. The strength scores are fixed because they come from a preseason survey. I would like to see if I can make the predictions more accurate by using Bayesian updating to allow that team-strength score to vary over the course of the season, as we learn more about how teams are performing relative to preseason expectations.</p>

<p>The single piece of information that strikes me as most useful in that regard is the cumulative sum of each team's prediction errors --- in other words, the cumulative sum of the differences between the team's predicted game performance (based on the preseason strength scores and where each game is played) and its actual game performance. </p>

<p>How might I go about doing that? In R, I have gotten as far as computing those cumulative errors, which turn out to be normally distributed for the season with a mean of ~0 and sd of ~50. I have tinkered with algebraic ways to adjust the strength scores as a function of that cumulative error. The forecasts based on those algebraic adjustments are slightly more accurate, but the approach seems clunky, and I'd like to use this problem as an opportunity to learn about Bayesian updating if I can. Any suggestions on how to do that in the context of this problem --- and, ideally, in R --- would be much appreciated.</p>
"
"0.0490486886395286","0.0480754414848157","167600","<p>I am trying to build a linear regression model for my data which has following variables. </p>

<pre><code>[1] ""Productcode""   ""Category""    ""Month""   ""Mode.of.operations"" ""sales""   ""profit.margin""     
 [7] ""Name""         ""Packaging.content""  ""Specifications""     ""Unit       ""Origin""   
</code></pre>

<p>Now as some of my variables are non numeric e.g <code>Origin</code> has values which are names of cities and countries, <code>Mode of operation</code> has values (joint venture, reseller, distributor).  In non numeric variable i dont know how to represent it in my linear regression model. One way I can think is to assign numeric values to these variables e.g <code>(Joint venture =1 , Reseller = 2 and Distribution =3)</code> but then it won't be right because it implies Distribution is better or 3 times higher than Joint venture. </p>

<p>Can anyone guide me how to solve this problem in R. </p>
"
"0.02831827358943","0.0277563690826684","167780","<p>I am getting the exact same results for a probit regression and post-hoc tests (simultaneous tests for linear hypotheses) - is this because I have used a dummy variable in the probit model and so it is effectively comparing each factor level to the reference group thus when I run the post-hoc, which is comparing differences between the two groups, that I get the same answers?</p>

<p>This is the model I fitted:</p>

<pre><code> m1&lt;-glmer(Success~Name.Origin+(1|Job.ID),family=binomial(link=""probit""))
</code></pre>

<p>and this is the post hoc test that I did:</p>

<pre><code> summary(glht(m1, lsm(pairwise ~ Name.Origin)))
</code></pre>
"
"0.0983889005882491","0.0964366217361766","167825","<p>I am trying to use ""Cursor"" , ""PostCursor"" and ""CTLE"" to predict ""left"", and I added interactions and quandratic in the model.</p>

<pre><code>  &gt;left_int3&lt;-lm(Left ~ Cursor + PostCursor + CTLE + I(Cursor^2) + I
               (PostCursor^2), data = QPI)
  &gt;summary(left_int3)

  Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
  (Intercept)     -412.58163   71.34574  -5.783 8.16e-09 ***
  Cursor            21.46885    2.85689   7.515 7.63e-14 ***
  PostCursor         2.96808    0.38768   7.656 2.62e-14 ***
  CTLE              -0.20459    0.01884 -10.858  &lt; 2e-16 ***
  I(Cursor^2)       -0.22646    0.02837  -7.982 2.09e-15 ***
  I(PostCursor^2)    0.24471    0.04070   6.013 2.06e-09 ***
  ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
  Residual standard error: 2.171 on 2794 degrees of freedom
  Multiple R-squared:  0.4174,  Adjusted R-squared:  0.4164 
  F-statistic: 400.4 on 5 and 2794 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Then I inspected the 4 assumptions of regression and found that normality, linearity and constant variance are violated so need to transform:</p>

<pre><code> **HOMOSCEDASTICITY**

 &gt; ncvTest(left_int3)
 Non-constant Variance Score Test 
 Variance formula: ~ fitted.values 
 Chisquare = 3.505792    Df = 1     p = 0.06115458 
 &gt; spreadLevelPlot(left_int3)
 Suggested power transformation:  1.12032

 **Linearity**

 &gt; boxTidwell(Left ~ Cursor + PostCursor + CTLE + I(Cursor^2) + I
             (PostCursor^2), data = QPI)  #

                 Score Statistic   p-value MLE of lambda
 Cursor                 7.162587 0.0000000      7.123073
 PostCursor            -3.534346 0.0004088     16.129858
 CTLE                  -1.921833 0.0546268      3.891245
 I(Cursor^2)           -7.641956 0.0000000      4.145477
 I(PostCursor^2)        4.937534 0.0000008      8.687134

 **Normality**

&gt; summary(powerTransform(QPI$Left))
bcPower Transformation to Normality 

           Est.Power Std.Err. Wald Lower Bound Wald Upper Bound
QPI$Left    3.7107   0.4409           2.8466           4.5749

Likelihood ratio tests about transformation parameters
                         LRT df         pval
LR test, lambda = (0) 72.13642  1 0.000000e+00
LR test, lambda = (1) 38.30386  1 6.054269e-10

**Independence** 

boxTidwell(Left ~ Cursor + PostCursor + CTLE + I(Cursor^2) + 
         I(PostCursor^2), data = QPI)  #
                Score Statistic   p-value MLE of lambda
Cursor                 7.162587 0.0000000      7.123073
PostCursor            -3.534346 0.0004088     16.129858
CTLE                  -1.921833 0.0546268      3.891245
I(Cursor^2)           -7.641956 0.0000000      4.145477
I(PostCursor^2)        4.937534 0.0000008      8.687134
</code></pre>

<p>Then I performed the transformations and fit again, but the R^2 is still low, so I am wondering my transformations are correct or not.</p>

<pre><code> &gt;QPI$Left&lt;-QPI$Left^3.7107  
 &gt;QPI$Cursor&lt;-QPI$Cursor^7.123
 &gt;QPI$PostCursor&lt;-QPI$PostCursor^16.129
 &gt;QPI$CTLE&lt;-QPI$CTLE^3.891245
 &gt;left_int3&lt;-lm(Left ~ Cursor + PostCursor + CTLE + I(Cursor^2) + 
             I(PostCursor^2), data = QPI)
 &gt;summary(left_int3)
 &gt;Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
 (Intercept)      1.455e+07  6.651e+05  21.880  &lt; 2e-16 ***
 Cursor           2.299e-06  1.302e-06   1.766  0.07754 .  
 PostCursor       1.150e-06  2.147e-06   0.536  0.59231    
 CTLE            -4.772e+00  4.548e-01 -10.493  &lt; 2e-16 ***
 I(Cursor^2)     -2.162e-18  6.854e-19  -3.154  0.00163 ** 
 I(PostCursor^2) -2.775e-19  5.977e-19  -0.464  0.64253    
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Residual standard error: 1175000 on 2794 degrees of freedom
 Multiple R-squared:  0.3942,    Adjusted R-squared:  0.3932 
 F-statistic: 363.7 on 5 and 2794 DF,  p-value: &lt; 2.2e-16
</code></pre>
"
"0.0853828074607","0.0920574617898323","167897","<p>I have the data:</p>

<pre><code>Distances   Diversity
-300        3.532833
-300        3.319447
-300        3.331814
-300        3.284599
-150        3.167693
-150        3.343932
-150        3.400182
-150        3.347922
-50         3.185409
-50         3.590527
-50         3.163942
-50         3.102254
 50         3.382986
 50         2.78799
 50         3.204374
 50         2.756762
 150        2.784996
 150        3.206704
 150        2.431388
 150        2.911236
 300        2.10763
 300        2.393464
 300        3.527539
 300        2.552804
</code></pre>

<p>After investigating the data it seems that there is a slightly curved relationship:
<img src=""http://imgur.com/QTQDVnS.jpg"" alt=""graph of Diversity against distance""></p>

<p>I have performed a simple polynomial regression and linear regression with the code:</p>

<pre><code>m1 &lt;- lm(Diversity ~ Distances + I(Distances^2), data = Data)
m2 &lt;- lm(Diversity ~ Distances, data = Data) 
</code></pre>

<p>However the output shows that the linear model is a better predictor with a p-value of &lt;0.001 while the polynomial is not significant (p>0.1).</p>

<p>This confuses me as the predicted values from m1 seem to better match the trend shown in the data:</p>

<pre><code> plot(Diversity ~ Distances)
 lines(lowess(Diversity ~ Distances))
 lines(Distances, predict(m1), col = ""red"")
 lines(Distances, predict(m2), col = ""blue"")
</code></pre>

<p><img src=""http://i.imgur.com/3Ymwnry.jpg"" alt=""Trend lines""></p>

<p>can someone explain why the p values suggest that the polynomial regression is such a poor predictor? </p>

<p>Thank you in advance</p>

<p>Summary (m1):</p>

<pre><code>Call:
lm(formula = Diversity ~ Distances + I(Distances^2), data = Data.Col)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.50157 -0.12025 -0.04278  0.11443  0.91834 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     3.131e+00  9.027e-02  34.689  &lt; 2e-16 ***
Distances      -1.305e-03  3.221e-04  -4.051 0.000576 ***
I(Distances^2) -1.454e-06  1.685e-06  -0.863 0.397985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.309 on 21 degrees of freedom
Multiple R-squared:  0.4496,    Adjusted R-squared:  0.3971 
F-statistic: 8.576 on 2 and 21 DF,  p-value: 0.001894
</code></pre>

<p>Summary(m2):
    Call:
    lm(formula = Diversity ~ Distances, data = Data.Col)</p>

<pre><code>Residuals:
     Min       1Q   Median       3Q      Max 
-0.57667 -0.15650 -0.00791  0.08949  0.84323 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.0757678  0.0627046  49.052  &lt; 2e-16 ***
Distances   -0.0013049  0.0003203  -4.074 0.000503 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.3072 on 22 degrees of freedom
Multiple R-squared:  0.4301,    Adjusted R-squared:  0.4042 
F-statistic:  16.6 on 1 and 22 DF,  p-value: 0.0005031
</code></pre>
"
"0.0424774103841449","0.0555127381653369","167947","<p>I understand total variance and R squared in linear regression outputs, but I have difficulty to understand the percent of variation explained by each covariates in a multiple regression analysis. I have two question.
1. How could I explaine the %var explained by one covariates in multiple regression?. 
2. Above all how could implement this in Stata or R?. A paper on Table 4, page 7 of the link below has provided such output.  Could anyone explain the 1.3%  output of Gas stove?  The adjusted R squared for the whole regression is 0.79 and the % variance explained by the covariates is 56.8. What does explain the rest?</p>

<p><a href=""http://www3.imperial.ac.uk/portal/pls/portallive/docs/1/7292108.PDF"" rel=""nofollow"">http://www3.imperial.ac.uk/portal/pls/portallive/docs/1/7292108.PDF</a></p>

<p>Thanks</p>
"
"0.126643169502805","0.12413025615484","168068","<p>I am using R for this analysis, and so examples and graphics will be produced in this language. I am willing to provide equivalent examples in similar languages if it will help someone, and am willing to accept answers in terms of other languages.</p>

<p>In this question, I intend to display graphs produced in order to verify assumptions, and ask for help in getting a better model. I understand that this may be considered too specific. However, it is my opinion that it would be helpful to have more examples of bad models and how to correct them on this site. If a moderator finds this not to be the case, I will happily delete this post.</p>

<p>I have conducted an initial linear model (lm) in R. It is multiple categorical regression with approx 100,000 cases, two categorical regressors and a continuous regressand. The goal of this regression is prediction: specifically, I would like to estimate prediction intervals. Find below some diagnostics of the initial model:</p>

<p>Residuals histogram (full) below. It may be difficult (impossible) to see, but there exist (sparse) values between 300 and 2000, as well as -50 and -500. Between -50 and 300, values are very dense. This indicates, to my understanding, heavy tails.</p>

<p><a href=""http://i.stack.imgur.com/FoGN7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FoGN7.png"" alt=""Residuals Historgram""></a></p>

<p>Residuals histogram (partial) below. Same image as above, but zoomed to the dense area.</p>

<p><a href=""http://i.stack.imgur.com/Q9bBl.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Q9bBl.png"" alt=""enter image description here""></a></p>

<p>A normal Quantile Quantile (normal QQ plot) is found below. Again, according to the <a href=""http://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot"">holy grail of qqplots</a>, (super) heavy tails are indicated.</p>

<p><a href=""http://i.stack.imgur.com/TrATp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TrATp.png"" alt=""Initial QQPlot""></a></p>

<p>Below is predicted vs residuals. Clearly, funky stuff is going on, suggesting heteroscedasticity:</p>

<p><a href=""http://i.stack.imgur.com/oOMRU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oOMRU.png"" alt=""Resid Vs Predicted""></a></p>

<p>I first tried some transformations. BoxCox yields a value very close to zero. So I will try to take the log of the regressand (in accordance with <a href=""https://en.wikipedia.org/wiki/Power_transform#Box.E2.80.93Cox_transformation"" rel=""nofollow"">the Wikipedia page</a>). </p>

<p><a href=""http://i.stack.imgur.com/3IbMv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3IbMv.png"" alt=""boxcox""></a></p>

<p><strong>Log Transform:</strong></p>

<p>Log transformed histogram, looks a lot better, but we still have some skew:</p>

<p><a href=""http://i.stack.imgur.com/exSgd.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/exSgd.png"" alt=""Histogram of Log transform""></a></p>

<p>And the NormalQQ Plot. Still seems that the residuals are not normally distributed.</p>

<p><a href=""http://i.stack.imgur.com/JosvR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JosvR.png"" alt=""log QQPlot""></a></p>

<p>Logarithm transformed Residual vs Predicted. Seems we have some decreasing variance now, but I would be willing to accept this assumption.</p>

<p><a href=""http://i.stack.imgur.com/Sg4B9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Sg4B9.png"" alt=""Log Resid Vs Predicted""></a></p>

<p>Other transformations I tried: raising regressand to powers 1/2, 1/3 and -1. None of these had satisfactory results; I choose not to include information about these transformations in order to save space, but will happily provide such information should it be requested.</p>

<p><strong>Here lie my questions:</strong></p>

<p>1) Is the solution to this problem simply to keep trying increasingly wacky transformations (ex: $1/log(x^{\pi/3})$)?</p>

<p>2) I have been looking (intermittently over a period of weeks) at Generalized Linear Models, which seem to allow a non-normal distribution of residuals. Unfortunately, I have not been able to understand them, and non of my (undergraduate statistics) peers have knowledge of them. If GLM's present a solution to this issue, I would be grateful if someone could explain them in this context. (Even if they are not a solution, I would be grateful for a simple explanation, or a reference to one).</p>

<p>2i) If GLM's are a good fit, I believe I would still need a distribution to model error by. What ways are there of detecting which (family) of distribution is the best fit for the residuals, after which I assume I can perform MLE to get the parameters? I've been having issues trying to evaluate heavy tailed distributions with respect to skew, because they tend not to have any moments, and so have $\infty$ or indeterminate skew.</p>

<p>3) Is there another class of models not aforementioned I should look into?</p>

<p>4) Is my current model sufficient for prediction intervals, despite the non-normality of residuals?</p>

<p>Some more information about the model: I am predicting a cost, thus the log transform is appealing in that my predicted values are positive reals.</p>

<p>I will be hanging around my computer all day, and have R gui open on my other monitor, so should be able to fulfill most requests for additional information.</p>
"
"0.0490486886395286","0.0480754414848157","168182","<p>I am trying to create a model of refrigeration having the energy consumption and the temperature over time. So far, I've tried regression but fitting this data into linear model seems impossible. Another thing that I've tried is cross correlation but it's insignificant (around 0.11 at lag 0). I also clustered the data and for another fridge I was able to state that if the fridge is in 'idle mode' (e.g. not consuming electricity) the temperature goes above certain value. However, for this fridge, this doesn't work as the data seems pretty random. Here is a scatter plot of the data, the bigger the circle, the higher the frequency.
<a href=""http://i.stack.imgur.com/1w5lU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1w5lU.png"" alt=""enter image description here""></a></p>

<p>Any ideas what type of analysis can I use to derive insights from this? I would like to know if there is any correlation between the kW data and the temperature data. A new plot for the full duration that I have:</p>

<p><a href=""http://i.stack.imgur.com/yfZAr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yfZAr.png"" alt=""enter image description here""></a></p>
"
"0.0535165067688","0.0629455284778823","168552","<p>I'm wondering if there is a good procedure people are using for gradient descent that is pretty well validated--something like a package for R or Python, or generic code many people adapt. After taking Andrew Ng's machine learning course in Coursera I was able to implement gradient descent in Octave, but I'm hoping to work with it in R or Python, with which I am more familiar. I'm also hoping for something more standardized than code I would write itself, to give it more credibility with my place of employment or other researchers.</p>

<p>Most of what I've found on this site is questions about writing code for gradient descent, like this post (<a href=""http://stats.stackexchange.com/questions/115425/multiplicative-gradient-descent"">Multiplicative gradient descent?</a>
) and this one (<a href=""http://stats.stackexchange.com/questions/142257/procedure-for-gradient-descent"">procedure for gradient descent</a>). And I've found examples of code from other sites, like this <a href=""http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/"" rel=""nofollow"">http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/</a>.</p>

<p>So I think there is no gradient descent package for R. But I was wondering if people had any thoughts for a good way to start implementing it in R or Python that doesn't rely on trust in the individual coder (e.g. me). Is there a blog post or tutorial with good generic code that can easily be used to standardize different gradient descent implementations?</p>
"
"0.0642198081225601","0.0629455284778823","168857","<p>I have a fairly large dataset of the following form, and I want to run a linear regression returning coefficients for each factor:</p>

<pre><code>Case    Variable1   Variable2   Result
1       Factor1     FactorA     50
2       Factor2     FactorA     60
3       Factor1     FactorB     55
4       Factor2     FactorB     65
...     ...         ...         ...
</code></pre>

<p>Running a linear regression using <code>lm()</code> on this would be very straightforward, but the size of the dataset seems to be too large.  I have about 1,000,000 cases, with about 10,000 factors in each variable.</p>

<p><code>lm()</code> (or other standard linear regression methods) translates this to an extremely wide matrix where each factor is a Boolean variable, correct?  So ~ 20,000 wide x 1,000,000 tall?  Running <code>lm()</code> on just a 25,000 case sample still takes several minutes and over a gb of memory.</p>

<p>My initial thought was to attack this regression problem using package <code>biglm</code>, but for it to behave properly, I believe <code>biglm</code> requires every factor to be present in every ""chunk"" of data it digests.  This would not occur in my data; some of the factors are only present a few times.  This is called <em>rank deficiency</em>, I believe. (However, an answer at <a href=""http://stackoverflow.com/questions/10502882/r-biglm-with-categorical-variables"">this StackOverflow question</a> indicates there might be a workaround?)</p>

<p>So my question: is there a better way to structure my data to run this regression?  Is there a better package or approach I should be using to run this analysis?</p>
"
"0.0326991257596857","0.0480754414848157","169334","<p>I'm trying to use the <code>circular</code> package in R to perform regression of a circular response variable and linear predictor, and I do not understand the coefficient value I'm getting. I've spent considerable time searching in vain for an explanation that I can understand, so I'm hoping somebody here may be able to help.</p>

<p>Here's an example:</p>

<pre><code>library(circular)

# simulate data
x &lt;- 1:100
set.seed(123)
y &lt;- circular(seq(0, pi, pi/99) + rnorm(100, 0, .1))

# fit model
m &lt;- lm.circular(y, x, type=""c-l"", init=0)

&gt; coef(m)
[1] 0.02234385
</code></pre>

<p>I don't understand this coefficient of 0.02 -- I would expect the slope of the regression line to be very close to pi/100, as it is in garden variety linear regression:</p>

<pre><code>&gt; coef(lm(y~x))[2]
         x
0.03198437
</code></pre>

<p>Does the circular regression coefficient not represent the change in response angle per unit change in the predictor variable? Perhaps the coefficient needs to be transformed via some link function to be interpretable in radians? Or am I thinking about this all wrong? Thanks for any help you can offer.</p>
"
"0.0800961731463273","0.0686935087981502","169438","<p>As we all know, there are 2 methods to evaluate the logistic regression model and 
they are testing very different things</p>

<ol>
<li><p>Predictive power:</p>

<p>Get a statistic that measures how well you can predict the dependent variable 
based on the independent variables. The well-know Pseudo R^2 are McFadden 
(1974) and Cox and Snell (1989).</p></li>
<li><p>Goodness-of-fit statistics</p>

<p>The test is telling whether you could do even better by making the model more 
complicated, which is actually testing whether there are any non-linearities or 
interactions.</p>

<p>I implemented both tests on my model, which added quadratic and interaction<br>
already: </p>

<pre><code>&gt;summary(spec_q2)

Call:
glm(formula = result ~ Top + Right + Left + Bottom + I(Top^2) + 
 I(Left^2) + I(Bottom^2) + Top:Right + Top:Bottom + Right:Left, 
 family = binomial())

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.955431   8.838584   0.108   0.9139    
Top          0.311891   0.189793   1.643   0.1003    
Right       -1.015460   0.502736  -2.020   0.0434 *  
Left        -0.962143   0.431534  -2.230   0.0258 *  
Bottom       0.198631   0.157242   1.263   0.2065    
I(Top^2)    -0.003213   0.002114  -1.520   0.1285    
I(Left^2)   -0.054258   0.008768  -6.188 6.09e-10 ***
I(Bottom^2)  0.003725   0.001782   2.091   0.0366 *  
Top:Right    0.012290   0.007540   1.630   0.1031    
Top:Bottom   0.004536   0.002880   1.575   0.1153    
Right:Left  -0.044283   0.015983  -2.771   0.0056 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 3350.3  on 2799  degrees of freedom
Residual deviance: 1984.6  on 2789  degrees of freedom
AIC: 2006.6
</code></pre></li>
</ol>

<p>and the predicted power is as below, the MaFadden is 0.4004, and the value between 0.2~0.4 should be taken to present very good fit of the model(Louviere et al (2000), Domenich and McFadden (1975))                                                :</p>

<pre><code> &gt; PseudoR2(spec_q2)
    McFadden     Adj.McFadden        Cox.Snell       Nagelkerke McKelvey.Zavoina           Effron            Count        Adj.Count 
   0.4076315        0.4004680        0.3859918        0.5531859        0.6144487        0.4616466        0.8489286        0.4712500 
         AIC    Corrected.AIC 
2006.6179010     2006.7125925 
</code></pre>

<p>and the goodness-of-fit statistics:</p>

<pre><code> &gt; hoslem.test(result,phat,g=8)

     Hosmer and Lemeshow goodness of fit (GOF) test

  data:  result, phat
  X-squared = 2800, df = 6, p-value &lt; 2.2e-16
</code></pre>

<p>As my understanding, GOF is actually testing the following null and alternative hypothesis:</p>

<pre><code>  H0: The models does not need interaction and non-linearity
  H1: The models needs interaction and non-linearity
</code></pre>

<p>Since my models added interaction, non-linearity already and the p-value shows H0 should be rejected, so I came to the conclusion that my model needs interaction, non-linearity indeed. Hope my interpretation is correct and thanks for any advise in advance, thanks. </p>
"
"0.0633215847514023","0.0620651280774201","171151","<p>I am trying to build a multiple regression model using R. I have a number of predictor variables. I have some basic domain knowledge for which I am trying to build the model. To start with, I included a few predictor variables based on domain knowledge and high correlation coefficients with the response variable, while excluding some other predictors due to multicollinearity. I would like to figure out if I should include some interaction terms. But, due to large number of predictors, I am having a hard time trying to figure out which all interaction terms I should include in the model. Based on what I have read on this site about automated model selection (thanks, @gung et. al), I am trying to avoid using it. </p>
"
"0.0490486886395286","0.0480754414848157","171448","<p>Doing ridge regression in R I have discovered</p>

<ul>
<li><p><code>linearRidge</code> in the <code>ridge</code> package - which fits a model, reports coefficients and p values but nothing to measure the overall goodness of fit</p></li>
<li><p><code>lm.ridge</code> in the <code>MASS</code> package - which reports coefficients and GCV but no p values for parameters</p></li>
</ul>

<p>How can I get all of these things (goodness of fit, coefficients and p values) from the same ridge regression?  I'm new to R so not familiar with facilities that may be available e.g. for computing $r^2$ from the data and fitted coefficients.</p>
"
"0.0490486886395286","0.0480754414848157","171473","<p>I'm currently working on analysing data from a simulation. The simulation result are statistical twins, which only vary in one point, the usage of an application to find a fueling station, that is cheap. In advance I have to say that my statistic skills are a little rusty.</p>

<p>I want to measure the effect of such an application on the fuel price. To do that me and my advisor at the university decided to use a linear regression. I came up with the following formula:
$$
Y_{â‚¬â„L}=Î²_0+Î²_1 X_{App}+Î²_2 X_{Oilprice} +Î²_3 X_{hour}+Î²_4 X_{day}+Î²_5 X_{fueltype}+ Î²_6 X_{highwaystation}+Î²_7 X_{brand}+ Z_{Home}+ Z_{Work}+ Z_{Station}+Ïµ
$$
The $Z_{work}$, as well as the others, represent a geometry within germany, that is identified by something call  RegionalschlÃ¼ssel, which is a unique number like 03255. The reason why I want to include this kind of information is, that this way I can observe the effects on the fuelprice that might be caused by local price differences. (I really hope I understood that right)</p>

<p>My current linear regression in R looks like
<code>lm1 &lt;- lm(price ~ app + oilprice + hour + day + fuel_type + brand + factor(start_rs) + factor(end_rs) + factor(station), data=queried_data)</code></p>

<p>The problem with this is, that my machine runs out of memory due to the very high amount of observations (9.759.911).</p>

<pre><code>Error: cannot allocate vector of size 85.5 Gb
In addition: Warning messages:
1: In model.matrix.default(mt, mf, contrasts) :
  Reached total allocation of 16005Mb: see help(memory.size)
2: In model.matrix.default(mt, mf, contrasts) :
  Reached total allocation of 16005Mb: see help(memory.size)
3: In model.matrix.default(mt, mf, contrasts) :
  Reached total allocation of 16005Mb: see help(memory.size)
4: In model.matrix.default(mt, mf, contrasts) :
  Reached total allocation of 16005Mb: see help(memory.size)
</code></pre>

<p>I have already done some reseach on other packages like</p>

<ul>
<li>lfe</li>
<li>lme4</li>
</ul>

<p>but as I said in the beginning my skillset is not high enough to comprehend them, nor is my english.</p>

<p>It would really help me if you could point me in the right directions.</p>
"
"0.0899225958391357","0.0961508829696314","171483","<p>I am trying to model stock returns with the help of google trends data. As explained in my <a href=""http://stats.stackexchange.com/questions/171212/check-for-normality-is-it-possible-to-combine-variables-to-get-an-overall-view"">first question</a> this data is normalised by google so that it is not normally distributed as User EdM kindly pointed out. Below I attached a <code>qqnorm()</code>plot for the accumulated values of some of my variables for further insight into their distribution.</p>

<p><a href=""http://i.stack.imgur.com/V0AKH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/V0AKH.png"" alt=""enter image description here""></a></p>

<p>Since my data is not normally distributed am I'm still allowed to do an OLS regression and look at performance measures like adjusted R squared and p-values? I saw some posts where it said that those tests are only used for normally distributed data, but that one can still use them due to the central limit theorem. Each variable has approximately 250 data entries in the in-sample data. Is this true? (Sorry, cannot find the posts atm and might be mixing something up)</p>

<p>Additionally, I would be really thankful if you could give me some further insights into model selection. I read a lot about it and as far as I understood it there are no set rules on how to find the best model and that it can get really complex the more variables you have. My problem is that I have more than 100 possible variables and - as far as my knowledge goes - those are simply too many. Hence, I thought I might be able to cut down on the variables by calculating the correlation (spearman) between every independent variable and the dependent variable. I would rank the correlations and start to cut out collinear variables (e.g. variabels that have higher absolute correlation than 0.5 with the variable I am inspecting). I know that I might lose some information by just watching correlation in the first place, but I dont know where else to start. I would do this until I get, lets say, 10 variables with relativ high correlation to the dependent variable and ""low"" collinearity. Then I would like to use stepwise or best subset regression to find the ""best"" model and finally use a rolling window approach or cross validation to approve the model.</p>

<p>As you might have guessed I am fairly new to regression statistics or statistics in general. I would be really thankful for any opinion or hint towards the right direction. I did a lot of research over the past few days, but couldnt simply find the right solution for the problems mentioned above.</p>
"
"0.0346826603453182","0.0453259609099337","171785","<p>I am running multiple linear regression with categorical variables and I need confidence interval 95% for standardized regression coefficient. I searched around and found 2 methods:</p>

<ol>
<li><p>Using the <code>QuantPsyc</code> package, with the function <code>lm.beta</code>. However, using <code>lm.beta</code> I can only get the standardized coefficients whereas I need with their 95% CI too. Is there a way?</p></li>
<li><p>To extract standardized regression coefficient, first standardize all the variables involved, and then run it in linear regression then you'll get estimates for standardized coefficients.</p></li>
</ol>

<p>So here is my model:</p>

<pre><code>model1 &lt;- lm(Life_Satisfaction ~ Subjective + Age + Sex + CountryCat11 + 
                                 CountryCat12 + CountryCat13 + CountryCat14 + 
                                 CountryCat15 + CountryCat16 + CountryCat17 + 
                                 CountryCat18 + CountryCat19 + CountryCat20 + 
                                 CountryCat23 + CountryCat25 + CountryCat28 + 
                                 CountryCat29 + CountryCat30 + Education_ISCED1 + 
                                 Education_ISCED2 + Education_ISCED3 + 
                                 Education_ISCED4 + Education_ISCED5 + 
                                 Education_ISCED6 + Education_stillinschool + 
                                 Education_None + Education_other, data=lifesat)

lm.beta (model1)
</code></pre>

<p>I ran that, but I cannot get the 95% CI.</p>

<p>So I tried the scale method:</p>

<pre><code>model2 &lt;- lm(scale(Life_Satisfaction) ~ scale(Subjective) + scale(Age) + 
                                        scale(Sex) + scale(CountryCat11) + 
                                        scale(CountryCat12) + scale(CountryCat13) + 
                                        scale(CountryCat14) + scale(CountryCat15) + 
                                        scale(CountryCat16) + scale(CountryCat17) + 
                                        scale(CountryCat18) + scale(CountryCat19) + 
                                        scale(CountryCat20) + scale(CountryCat23) + 
                                        scale(CountryCat25) + scale(CountryCat28) + 
                                        scale(CountryCat29) + scale(CountryCat30) + 
                                    scale(Education_ISCED1) + scale(Education_ISCED2) + 
                                    scale(Education_ISCED3) + scale(Education_ISCED4) + 
                                    scale(Education_ISCED5) + scale(Education_ISCED6) + 
                               scale(Education_stillinschool) + scale(Education_None) + 
                                        scale(Education_other), data=lifesat)

summary(model2)
</code></pre>

<p>I ran that, and I got the standardized regression and 95% CI but it was different from the standardized regression results I got from SPSS? Did I do it wrong?</p>
"
"0.0693653206906364","0.0679889413649005","172087","<p>I have a dataframe <code>df</code> (see below):</p>

<pre><code>dput(df)
structure(list(x = c(49, 50, 51, 52, 53, 54, 55, 56, 1, 2, 3, 
4, 5, 14, 15, 16, 17, 2, 3, 4, 5, 6, 10, 11, 3, 30, 64, 66, 67, 
68, 69, 34, 35, 37, 39, 2, 17, 18, 99, 100, 102, 103, 67, 70, 
72), y = c(2268.14043972082, 2147.62290922552, 2269.1387550775, 
2247.31983098201, 1903.39138268307, 2174.78291538358, 2359.51909126411, 
2488.39004804939, 212.851575751527, 461.398994384333, 567.150629704352, 
781.775113821961, 918.303706148872, 1107.37695799186, 1160.80594193377, 
1412.61328924168, 1689.48879626486, 260.737164468854, 306.72700499362, 
283.410379620422, 366.813913489692, 387.570173754128, 388.602676983443, 
477.858510450125, 128.198042456082, 535.519377609133, 1028.8780498564, 
1098.54431357711, 1265.26965941035, 1129.58344809909, 820.922447928053, 
749.343583476846, 779.678206156474, 646.575242339517, 733.953282899613, 
461.156280127354, 906.813018662913, 798.186995701282, 831.365377249207, 
764.519073183124, 672.076289062505, 669.879217186302, 1341.47673353751, 
1401.44881976186, 1640.27575962036)), .Names = c(""x"", ""y""), row.names = c(NA, 
-45L), class = ""data.frame"")
</code></pre>

<p>I have created on a non-linear regression (nls) based on my dataset.</p>

<pre><code>nls1 &lt;- nls(y~A*(x^B)*(exp(k*x)), 
            data = df, 
            start = list(A = 1000, B = 0.170, k = -0.00295), algorithm = ""port"")
</code></pre>

<p>I then computed a bootstrap for this function to get multiple sets of parameters (A,B and k) and created a dataframe which contains the different set of parameters. </p>

<pre><code>Boo &lt;- nlsBoot(nls1, niter = 200)
Param_Boo &lt;- Boo$coefboot
</code></pre>

<p>I now want to plot in the same plot all the 200 possible gamma functions created where their parameters have been computed from the bootstrap. I have only managed to plot one function out of the 200 possibilities so far (see below).</p>

<pre><code>ggplot(df, aes(x, y)) +
    geom_point()+
    geom_line(aes(y = predict(nls1)))
</code></pre>

<p>Can someone help me out with that? Thanks in advance. </p>
"
"0.0693653206906364","0.0679889413649005","172189","<p>I have a dataframe <code>df</code> (see below):</p>

<pre><code>dput(df)
structure(list(x = c(49, 50, 51, 52, 53, 54, 55, 56, 1, 2, 3, 
4, 5, 14, 15, 16, 17, 2, 3, 4, 5, 6, 10, 11, 3, 30, 64, 66, 67, 
68, 69, 34, 35, 37, 39, 2, 17, 18, 99, 100, 102, 103, 67, 70, 
72), y = c(2268.14043972082, 2147.62290922552, 2269.1387550775, 
2247.31983098201, 1903.39138268307, 2174.78291538358, 2359.51909126411, 
2488.39004804939, 212.851575751527, 461.398994384333, 567.150629704352, 
781.775113821961, 918.303706148872, 1107.37695799186, 1160.80594193377, 
1412.61328924168, 1689.48879626486, 260.737164468854, 306.72700499362, 
283.410379620422, 366.813913489692, 387.570173754128, 388.602676983443, 
477.858510450125, 128.198042456082, 535.519377609133, 1028.8780498564, 
1098.54431357711, 1265.26965941035, 1129.58344809909, 820.922447928053, 
749.343583476846, 779.678206156474, 646.575242339517, 733.953282899613, 
461.156280127354, 906.813018662913, 798.186995701282, 831.365377249207, 
764.519073183124, 672.076289062505, 669.879217186302, 1341.47673353751, 
1401.44881976186, 1640.27575962036)), .Names = c(""x"", ""y""), row.names = c(NA, 
-45L), class = ""data.frame"")
</code></pre>

<p>I have created on a non-linear regression (nls) based on my dataset.</p>

<pre><code>nls1 &lt;- nls(y~A*(x^B)*(exp(k*x)), 
            data = df, 
            start = list(A = 1000, B = 0.170, k = -0.00295), algorithm = ""port"")
</code></pre>

<p>I then computed a bootstrap for this function to get multiple sets of parameters (A,B and k) and created a dataframe which contains the different set of parameters. </p>

<pre><code>Boo &lt;- nlsBoot(nls1, niter = 200)
Param_Boo &lt;- Boo$coefboot
</code></pre>

<p>I have then plotted all the 200 output functions from the bootstrapping (see below).</p>

<pre><code># Plot curves with bootstrapped params
x &lt;- seq(min(df$x),max(df$x),length=50)
curveDF &lt;- data.frame(matrix(0,ncol = 3,nrow = 200*length(x)))

for(i in 1:200)
{
  for(j in 1:length(x))
  {
    # Function value
    curveDF[j+(i-1)*200,1] &lt;- Param_Boo[i,1]*(x[j]^Param_Boo[i,2])*(exp(Param_Boo[i,3]*x[j]))
    # Bootstrap sample number
    curveDF[j+(i-1)*200,2] &lt;- i
    # x value
    curveDF[j+(i-1)*200,3] &lt;- x[j]
  }
}
colnames(curveDF) &lt;- c('ys','bsP','xs')

p1 &lt;- ggplot(curveDF, aes(x=xs, y=ys, group=bsP)) +
  geom_line() +
  ggtitle(""Curves for bootstrapped params"")
</code></pre>

<p>However, the visibility of this plot is not nice if someone wants to add the points of my dataframe on the plot. Therefore,I was wondering if it was possible to plot one curve (the mean of the 200 curves for instance) with the upper and lower confidence interval (or something else). Visually it will look a bit like the picture (top right) below. 
<a href=""http://i.stack.imgur.com/8Ues6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8Ues6.png"" alt=""enter image description here""></a></p>

<p>Can someone help me out with that? Thanks in advance. </p>
"
"0.0642198081225601","0.0629455284778823","172573","<p>It probably been asked quite a few times, but I do not know what to look / search for. And english </p>

<p>My question is: If I have a X variable in my linear regression, that is a percentage of something, does this ""destroy"" my linear regression? Further more if I have multiple variables that are a percentage of something, will this destryo my linear regression even more.</p>

<p>Since even I find it hard to understand my question I will make an example
$$
Y_{totalcost} = 0 + \beta_0X_{app} + \beta_1X_{morning} + \beta_2X_{midday} + \beta_3X_{afternoon} + \beta_4X_{night} + \beta_5X_{babstation} + \beta_7X_{notbabstaion} + \beta_7X_{drivendistance} + \beta_8X_{fillingstations}
$$
I want to find out how the usage of a refueling app, where consumers can look up the current fuel prices in their area, affect the total cost. Therefor I have created statistical twins, where one observation has used an app and the other has not.</p>

<p>The X[morning,midday, afternoon, night] varibales describe: sum of refill events in the morning/.. divided by the total number refill events and they sum up to 1.</p>

<p>The X[babstation, notbabstation] describe: sum of refill event that have taken place at a highway refilling station divided by the total number of refilling evnets.</p>

<p>As you can see I have two groups of multiple variables that sum up to one and I'm not sure if I can do this in a linear regression. I'm trying to implement such a model I R, so if you could also give me hints how to do this in R it would be great.</p>
"
"0.0400480865731637","0.039253433598943","172923","<p>I need some urgent help. I've performed a linear multivariate regression using R, and my residuals graphs are as follows.
I'm still new at this. The patterns shown in my graphs are quite weird compared to the usual examples used to illustrate regressions. Are these graphs telling me something special about my data? How can I improve the model based on the graphs?</p>

<p>The dependant variable (Y) is discrete (it's a measurement of ""quality"" that goes from 0 to 10). All independent variables are numerical and continuous. </p>

<p>Thanks a lot.</p>

<p><img src=""http://i.imgur.com/KO0fe6G.png"" alt=""Residuals vs Fitted"">
<img src=""http://i.imgur.com/jxMuKrH.png"" alt=""Normal QQ"">
<img src=""http://i.imgur.com/Sj6I1i7.png"" alt=""Residuals vs Leverage"">
<img src=""http://i.imgur.com/Whmyr13.png"" alt=""Scale Location""></p>
"
"0.0805952195517515","0.0789960112897596","173026","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>

<p><strong>EDIT</strong> The result of the features reversed as commented by @Michael M:</p>

<pre><code>&gt; model_All2 &lt;- lm(y ~ x2 + x1, data=df)
&gt; anova(model_All2)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x2         1 17.468  17.468  22.907 0.0001718 ***
x1         1 53.612  53.612  70.304 1.914e-07 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0700841515030364","0.0686935087981502","173047","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>
"
"0.0800961731463273","0.078506867197886","173076","<p>I was following the procedure in a statistics textbook to run a multinomial logistic regresion using <code>mlogit</code>. However, the Odds Ratios calculated seemed too high for some of the variables (>1000). Can someone take a look at this and check wether I am doing everything correctly? The data can be downloaded from <a href=""https://dl.dropboxusercontent.com/u/14303378/LogisticRegressionSample.csv"" rel=""nofollow"">here</a>. I prepared the data with the following commands:</p>

<pre><code>#read in the data
test&lt;-read.csv(file=""LogisticRegressionSample.csv"",sep="","")
#trasnform data into the correct form for mlogit
mlogitData&lt;-mlogit.data(test,choice=""Outcome"",shape=""wide"")
#build model
MLogitFit&lt;-mlogit(Outcome~1|V1+V2+V3+V4+V5+V6+V7+V8,reflevel=3,data=mlogitData)
#summary of the model
summary(MLogitFit)
#OddsRatios
data.frame(exp(MLogitFit$coefficients))
# confidence Interval of the odds Ratios
exp(confint(MLogitFit))
</code></pre>

<p>The summary of mlogit gives me:</p>

<pre><code>    Call:
mlogit(formula = Outcome ~ 1 | V1 + V2 + V3 + V4 + V5 + V6 + 
    V7 + V8, data = mlogitData, reflevel = 3, method = ""nr"", 
    print.level = 0)

Frequencies of alternatives:
      Z       A       B 
0.43333 0.25556 0.31111 

nr method
7 iterations, 0h:0m:0s 
g'(-H)^-1g = 1.56E-06 
successive function values within tolerance limits 

Coefficients :
               Estimate Std. Error t-value  Pr(&gt;|t|)    
A:(intercept)  -6.74640    5.97451 -1.1292 0.2588147    
B:(intercept)  -7.12401    4.50350 -1.5819 0.1136759    
A:V1            3.65979    3.90808  0.9365 0.3490331    
B:V1            4.24363    3.25687  1.3030 0.1925822    
A:V2          -15.11554    6.92901 -2.1815 0.0291475 *  
B:V2           -4.88778    3.65249 -1.3382 0.1808302    
A:V3            1.71465    6.57907  0.2606 0.7943839    
B:V3            2.94335    3.96557  0.7422 0.4579497    
A:V4           -1.70660    1.58849 -1.0744 0.2826633    
B:V4           -1.67210    1.17575 -1.4222 0.1549820    
A:V5            1.18494    1.60760  0.7371 0.4610682    
B:V5            1.03084    1.25573  0.8209 0.4116971    
A:V6            8.28902    2.51631  3.2941 0.0009873 ***
B:V6            3.44578    1.91844  1.7961 0.0724727 .  
A:V7           -1.34395    2.67943 -0.5016 0.6159612    
B:V7            1.04803    1.95147  0.5370 0.5912343    
A:V8           -7.46263    4.12978 -1.8070 0.0707577 .  
B:V8            0.21861    2.13596  0.1023 0.9184810    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -64.636
McFadden R^2:  0.33149 
Likelihood ratio test : chisq = 64.1 (p.value = 1.0515e-07)
</code></pre>

<p>Running <code>data.frame(exp(MLogitFit$coefficients))</code> to calculate the odds ratios gives:</p>

<pre><code>              exp.MLogitFit.coefficients.
A:(intercept)                1.175103e-03
B:(intercept)                8.055280e-04
A:V1                         3.885310e+01
B:V1                         6.966040e+01
A:V2                         2.725226e-07
B:V2                         7.538147e-03
A:V3                         5.554743e+00
B:V3                         1.897938e+01
A:V4                         1.814819e-01
B:V4                         1.878524e-01
A:V5                         3.270504e+00
B:V5                         2.803423e+00
A:V6                         3.979917e+03
B:V6                         3.136764e+01
A:V7                         2.608125e-01
B:V7                         2.852036e+00
A:V8                         5.741439e-04
B:V8                         1.244345e+00
</code></pre>

<p>I obtained the confidence interavls with: <code>exp(confint(MLogitFit))</code>:</p>

<pre><code>                     2.5 %       97.5 %
A:(intercept) 9.650816e-09 1.430830e+02
B:(intercept) 1.182216e-07 5.488637e+00
A:V1          1.831725e-02 8.241213e+04
B:V1          1.176881e-01 4.123248e+04
A:V2          3.446800e-13 2.154711e-01
B:V2          5.864847e-06 9.688857e+00
A:V3          1.394913e-05 2.211978e+06
B:V3          7.994348e-03 4.505896e+04
A:V4          8.066986e-03 4.082774e+00
B:V4          1.875058e-02 1.881996e+00
A:V5          1.400307e-01 7.638467e+01
B:V5          2.392271e-01 3.285238e+01
A:V6          2.870699e+01 5.517731e+05
B:V6          7.303065e-01 1.347282e+03
A:V7          1.366460e-03 4.978060e+01
B:V7          6.223884e-02 1.306918e+02
A:V8          1.752860e-07 1.880591e+00
B:V8          1.891518e-02 8.185990e+01
</code></pre>

<p>The predicted Probabilities are as following:</p>

<pre><code>fitted(MLogitFit, outcome=FALSE)
                 Z            A          B
 [1,] 0.2790108926 3.880184e-01 0.33297074
 [2,] 0.5191458618 2.900625e-01 0.19079169
 [3,] 0.7263001933 1.633014e-02 0.25736966
 [4,] 0.8386056883 3.700203e-03 0.15769411
 [5,] 0.8050365007 7.487290e-03 0.18747621
 [6,] 0.7855655154 3.860347e-02 0.17583101
 [7,] 0.7878404896 7.992930e-03 0.20416658
 [8,] 0.8386056883 3.700203e-03 0.15769411
 [9,] 0.7878404896 7.992930e-03 0.20416658
[10,] 0.4363708036 2.827104e-01 0.28091885
[11,] 0.6126060746 3.320075e-02 0.35419317
[12,] 0.0274357267 8.418204e-01 0.13074390
[13,] 0.1438998597 5.869087e-01 0.26919146
[14,] 0.1850027820 2.105586e-01 0.60443858
[15,] 0.8427092407 5.933393e-03 0.15135737
[16,] 0.1537160539 4.929905e-01 0.35329341
[17,] 0.0434283140 6.358897e-01 0.32068201
[18,] 0.1868202029 1.141679e-01 0.69901186
[19,] 0.3064594418 1.156597e-01 0.57788084
[20,] 0.5737141160 6.734724e-02 0.35893865
[21,] 0.5841338911 1.374758e-01 0.27839031
[22,] 0.0866451414 4.019366e-01 0.51141821
[23,] 0.2794060013 9.964607e-02 0.62094793
[24,] 0.0252343516 7.343045e-01 0.24046118
[25,] 0.1314775919 4.602643e-01 0.40825811
[26,] 0.0274357267 8.418204e-01 0.13074390
[27,] 0.1303195991 6.649645e-01 0.20471586
[28,] 0.2818251202 4.896734e-01 0.22850146
[29,] 0.0063990341 8.874618e-01 0.10613917
[30,] 0.0002408527 9.742025e-01 0.02555668
[31,] 0.0523052465 7.073015e-01 0.24039322
[32,] 0.3287956423 2.756959e-01 0.39550841
[33,] 0.0419093705 7.521689e-01 0.20592173
[34,] 0.0523052465 7.073015e-01 0.24039322
[35,] 0.3287956423 2.756959e-01 0.39550841
[36,] 0.0100998700 7.475180e-01 0.24238212
[37,] 0.1609808596 2.268570e-01 0.61216212
[38,] 0.0119603037 8.065964e-01 0.18144331
[39,] 0.0697132279 4.549378e-01 0.47534896
[40,] 0.5756435353 6.315652e-02 0.36119994
[41,] 0.4689676672 6.796615e-02 0.46306619
[42,] 0.2652679745 6.358962e-02 0.67114240
[43,] 0.7870195702 2.038999e-03 0.21094143
[44,] 0.6438437943 9.222002e-03 0.34693420
[45,] 0.7462282258 5.881047e-04 0.25318367
[46,] 0.3532662528 2.193975e-01 0.42733620
[47,] 0.9563852795 4.133754e-05 0.04357338
[48,] 0.9079031419 2.786314e-03 0.08931054
[49,] 0.0220230156 8.017508e-01 0.17622619
[50,] 0.2268852285 1.745210e-01 0.59859376
[51,] 0.2268852285 1.745210e-01 0.59859376
[52,] 0.0751929214 6.261548e-01 0.29865225
[53,] 0.9426667411 4.520877e-06 0.05732874
[54,] 0.0212631471 6.729961e-01 0.30574075
[55,] 0.0212631471 6.729961e-01 0.30574075
[56,] 0.9218535421 1.166953e-02 0.06647693
[57,] 0.6374868816 3.856300e-02 0.32395012
[58,] 0.2920703240 2.410709e-01 0.46685876
[59,] 0.7047942848 1.728601e-02 0.27791970
[60,] 0.1850395244 5.297673e-01 0.28519316
[61,] 0.4402296785 8.870861e-03 0.55089946
[62,] 0.6781988218 3.852569e-04 0.32141592
[63,] 0.9889453179 4.036588e-05 0.01101432
[64,] 0.1618635354 8.011851e-02 0.75801796
[65,] 0.3008372801 9.835522e-02 0.60080750
[66,] 0.0740319347 4.284039e-01 0.49756417
[67,] 0.5529727485 1.768537e-01 0.27017351
[68,] 0.7824740564 5.001713e-03 0.21252423
[69,] 0.5343045050 5.865850e-02 0.40703700
[70,] 0.4564647083 1.733995e-01 0.37013579
[71,] 0.4711837972 8.449081e-03 0.52036712
[72,] 0.9154349308 2.364316e-02 0.06092191
[73,] 0.1858643216 2.217595e-01 0.59237621
[74,] 0.3770813535 9.943397e-02 0.52348468
[75,] 0.8124141650 3.243679e-04 0.18726147
[76,] 0.3195206223 2.932236e-01 0.38725578
[77,] 0.8615871019 5.063299e-04 0.13790657
[78,] 0.8615871019 5.063299e-04 0.13790657
[79,] 0.8254986241 2.059378e-03 0.17244200
[80,] 0.1208591778 4.615235e-01 0.41761730
[81,] 0.0035765650 9.093754e-01 0.08704806
[82,] 0.7583239965 3.544345e-02 0.20623255
[83,] 0.8141948591 5.016280e-03 0.18078886
[84,] 0.1204323818 2.545405e-01 0.62502710
[85,] 0.9594950290 3.694056e-05 0.04046803
[86,] 0.6858228916 1.691396e-01 0.14503752
[87,] 0.8254986241 2.059378e-03 0.17244200
[88,] 0.8254986241 2.059378e-03 0.17244200
[89,] 0.2463233530 2.793410e-01 0.47433568
[90,] 0.5674338104 1.448538e-02 0.41808081
</code></pre>

<p>To assess multicolinearity I calculated the VIF statistic but using the a glm model of the same dataset.</p>

<pre><code>fullmod&lt;-glm(as.factor(Outcome)~.,data=test,family=binomial())
vif(fullmod)
      V1       V2       V3       V4       V5       V6       V7       V8 
1.789116 1.822252 2.216444 1.320244 1.821820 1.439183 1.512865 1.121805 
</code></pre>
"
"0.0400480865731637","0.039253433598943","173343","<p>Suppose we have a Gaussian regression model, y_i~N(Î±+Î²_1 x_i+Î²_2 z_i,Ïƒ). This model is fitted to the trees data in R where y is Height, x is Girth and z is Volume. 
Following is the result from the simple linear regression:</p>

<p>Call:
lm(formula = Height ~ Girth + Volume)</p>

<p>Coefficients:
(Intercept)        Girth       Volume<br>
    83.2958      -1.8615       0.5756</p>

<p>Generate random values for Height based on the model with Ïƒ = 5 that corresponds with each pair of values for Girth and Volume.</p>
"
"0.0578044339088637","0.0679889413649005","173394","<p>I'm working on an analysis of a team-based dataset, where teams of 3 compete against each other under specific, identical tasks. The hierarchy therefore is person :: team :: task. Or, put another way, 6 people :: 2 teams :: 1 task. I am using <code>lme4</code> in R for linear regression (to predict a continuous variable).</p>

<p>For the hierarchy, I'm writing in the regression equation <code>(1|team)</code> and <code>(1|task)</code> for random effects (with a bunch of other fixed effect variables). However, in our dataset, each set of teams within any task is coded as ""left"" or ""right"" team (ie., 0 or 1, across each task). </p>

<p>When setting up the regression equation in <code>lme4</code>, do I need to recode team_id as a unique value taking into account task_id? E.g., I could concatenate to get unique team values, so task_1_team_0, task_1_team_1, task_2_team_0, task_2_team_1, etc.? Or will the <code>lme4</code> package take care of this for me, and I can keep the 2nd level values as 0 and 1?</p>

<p>I want to make sure that it is not putting half the dataset into one team and half into another team, because they are all coded the same, and then applying the task level on top of ""two"" teams.</p>
"
"0.0400480865731637","0.039253433598943","173468","<p>I am reading through the book <code>An introduction to Statistical learning with Applications in R</code>, and get stuck:</p>

<p>When doing linear regression, we can write the relation as</p>

<p>$$Y = \beta_0 + \beta_1X + \epsilon$$</p>

<p>where $$E(\epsilon)=0$$</p>

<p>And, we can define the residual sum of squares (RSS) as
  $$RSS = e_1^2 + e_2^2 + ... +e_n^2$$
where $e_i = y_i-\hat{y_i}$</p>

<p>To minimize the RSS, we can get </p>

<p>$$\hat{\beta_1} = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$ $$\hat{\beta_0} = \bar{y}-\hat{\beta_1}\bar{x}$$</p>

<p>But, I cannot get the standard error with $\hat{\beta_0}$, which is (as the book says but I do not know how to prove it)</p>

<p>$$SE(\hat{\beta_0})^2 = var(\epsilon)[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}]$$
$$SE(\hat{\beta_1})^2 = \frac{var(\epsilon)}{\sum_{i=1}^{n}(x_i - \bar{x})^2}$$</p>

<p>How to prove $SE(\hat{\beta_0})^2$  and $SE(\hat{\beta_1})^2$?</p>
"
"0.0633215847514023","0.0496521024619361","173588","<p>I am not able to interpret properly the Tukey Test results from a linear regression model i have built. The Tukey Test for the model i believe is to test is the model is linear. Please correct me if i am wrong. We get this Tukey result from residualPlots command in R. </p>

<p>From the Tukey Test, all the variable have a p-value that are non-significant (p>0.05) but the Tukey Test for the entire model is significant (p=000). How can i make the Tukey Test non- significant for the entire model.</p>

<p>Please let me know the approach i need to use to make the tukey test non-significant for the entire model. And also the R codes that will help me here. Thanks.</p>

<p>The data is ordinal survey data. Most of the questions are scaled from 1 to 5. However, i am treating them to be continuous to build an overall model. There are a few variables that are binary 1/0. In total there are 18 independent variables, and three additional transformed variables (i.e. squared).  The sample size is ~2000 observations.</p>
"
"0.11327309435772","0.104086384060007","173704","<p>I have a function I wrote to calculate idiosyncratic volatility, but its really inefficient. Does anyone have any suggestions to make the code below more efficient?</p>

<p>Specifically, I am trying to take a time series and run a linear regression using a N_Max lookback. I then want to extract the residuals for the dates used in the linear regression and calculate the standard deviation. If there is not N_Max dates, I want to use as many dates as I have available provided it is larger than N_Min.</p>

<pre><code>Idiosyncratic_Volatility &lt;- function(dependent, independent, dates, Min_n, Max_n) {
  # Define size as the number of observations in the dependent
  size &lt;- length(dependent)

  n &lt;- Max_n

  # If the lookback is less than 0, stop the function and error out
  if (Min_n &lt; 1 | Max_n &lt; 1) stop(""n must be positive"")
  if (Max_n &lt; Min_n) stop(""Max_n must be greater than Min_n"")

  # Define variables for the recursive function  
  i &lt;- size                           # set the counter for itterations
  result &lt;- NA; result[1:size] &lt;- NA  # create a vector of NAs to store the results
  residuals_vector &lt;- NA              # create a variable to store residuals

  while (i &gt;= Max_n)
  {
    # Window is an R function that allows you to do run calculations over
    # desired rolling date ranges.
    RollingIndependent &lt;- window(independent, start=i-n+1, end=i)
    RollingDependent  &lt;- window(dependent, start=i-n+1, end=i)

    # If the data can cause an error, return NA
    if(
      n &gt; length(RollingDependent) | 
        length(RollingDependent) &gt; length(RollingIndependent) | 
        length(RollingDependent) &gt; length(dates)) {
      result[i] &lt;- NA

      # else calculate idiosyncratic volatility
    } else {  
      # fit a linear model to the data.
      model &lt;- lm(RollingDependent ~ RollingIndependent)

      # extract the residuals from the model
      residuals_vector &lt;- summary(model)$residuals

      # calculate the standard deviation of the residuals
      result[i] &lt;- sd(residuals_vector,na.rm=FALSE)

      # iterate the recursive factor
      i &lt;- i - 1
    }
  } 

  n &lt;- i

  # This next part does the ""burn in"" period and runs off available information
  # until it hits the Min_n.
  while (i &gt;= Min_n)
  {
    # Window is an R function that allows you to do run calculations over
    # desired rolling date ranges.
    RollingIndependent &lt;- window(independent, start=i-n+1, end=i)
    RollingDependent  &lt;- window(dependent, start=i-n+1, end=i)

    # If the data can cause an error, return NA
    if(
      n &gt; length(RollingDependent) | 
        length(RollingDependent) &gt; length(RollingIndependent) | 
        length(RollingDependent) &gt; length(dates)) {
      result[i] &lt;- NA

      # else calculate idiosyncratic volatility
    } else {  
      # fit a linear model to the data.
      model &lt;- lm(RollingDependent ~ RollingIndependent)

      # extract the residuals from the model
      residuals_vector &lt;- summary(model)$residuals

      # calculate the standard deviation of the residuals
      result[i] &lt;- sd(residuals_vector,na.rm=FALSE)

      # iterate the recursive factor
      i &lt;- i - 1
      n &lt;- i
    }
  } 

  # print the results
  result
}
</code></pre>

<p>Thank you for your time!</p>
"
"0.0853828074607","0.0836886016271203","173996","<p>I'm using R (package lmer) to run linear mixed model My study looks at allergy levels of skin patches from patients and readings (repeated 5 times) are measured over 4 time points.</p>

<p>I need to determine if the allergy level for skin patch changes over time
(e.g., if allergy level from skin patch 1 for patient 1 at time 0 is different from allergy level for skin patch 1 for patient 1 at time 1 etc.) I do not want to see the difference between skin patch 1 and skin patch 2. Using package lmer:  </p>

<pre><code>model &lt;- lmer(allergy_level ~ time +(time|patient/patch))
</code></pre>

<p><strong>Results from this model indicate that time is not significant - the average patient allergy level for individual skin patches does not change over time</strong> (see below for output). However, <strong>I need to be able to tell if there is a significant difference for individual patches for individual patients over time</strong>.</p>

<p>If I run individual regression models for each skin patch for each patient, this will result in a large number of models as I have There are 16 skin patches per patient. (10 patients in total) 5 readings are taken at each of the 4 time points. I thought linear mixed models would be an appropriate method to answer my question (I need to be able to tell if there is a significant difference for individual patches for individual patients over time). </p>

<p>Output:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr             
 ID:patch (Intercept) 17.4109  4.1726                    
          time1        2.7109  1.6465   -0.30            
          time2        3.0082  1.7344   -0.26  0.60      
          time3        5.7643  2.4009   -0.35  0.15  0.54
 patch    (Intercept) 19.1576  4.3769                    
          time1        0.2103  0.4586   -0.56            
          time2        0.4372  0.6612   -0.94  0.48      
          time3        0.5895  0.7678   -0.48  0.96  0.49
 Residual              4.9467  2.2241                    
Number of obs: 2956, groups:  ID:patch, 149; patch, 16

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.44763    1.15028   5.605
time1       -0.01907    0.21237  -0.090
time2       -0.03172    0.24759  -0.128
time3       -0.01124    0.29940  -0.038

model1: AllergyLevel ~ 1 + (1 + time | patch/ID)
model2: AllergyLevel ~ time + (1 + time | patch/ID)
         Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
model11 22 14281 14413 -7118.5    14237                         
model12 25 14287 14437 -7118.4    14237 0.0208      3     0.9992
</code></pre>

<p>I have extracted the random coefficients from model 1:</p>

<pre><code>ranef(model1)

`ID:patch`
      (Intercept)       time1        time2        time3
1:11    5.9845070  0.34088535  0.431998708  1.590906238
1:12    5.1236456 -0.03178611 -0.149784278 -0.116150278
1:13    6.3746877 -0.76853294 -0.550037715  0.842518786
   :
   :
</code></pre>
"
"0.0849548207682898","0.0832691072480053","174057","<p>This is probably an embarrassingly easy question, but where else can I turn to... </p>

<p>I'm trying to put together examples of regression with mixed effects using <code>lmer</code> {lme4}, so that I can present [R] code that automatically downloads toy datasets in Google Drive and run every instance in <a href=""http://stats.stackexchange.com/a/13173/67822"">this blockbuster post</a>. </p>

<p>And starting with the first case (i.e. <code>V1 ~ (1|V2) + V3</code>, where <code>V3</code> is a continuous variable acting as a fixed effect, and <code>V2</code> is <code>Subjects</code>, both trying to account for <code>V1</code>, a continuous DV), I was expecting to retrieve different intercepts for each one of the <code>Subjects</code> and a single slope for all of them. Yet, this was not the case consistently.</p>

<p>I don't want to bore you with the origin or meaning of the datasets below, because I'm sure most of you get the idea without much explaining. So let me show you what I get... If you're so inclined you can just copy and paste in [R]... it should work if you have {lme4} in your Environment:</p>

<h1>Expected Output:</h1>

<pre><code>politeness &lt;- read.csv(""http://www.bodowinter.com/tutorial/politeness_data.csv"")
head(politeness)

  subject   gender scenario  attitude frequency
1      F1      F        1      pol     213.3
2      F1      F        1      inf     204.5
3      F1      F        2      pol     285.1
4      F1      F        2      inf     259.7    


library(lme4)

fit &lt;- lmer(frequency ~ (1|subject) + attitude, data = politeness)

coefficients(fit)
            $subject
               (Intercept) attitudepol
            F1    241.1352   -19.37584
            F2    266.8920   -19.37584
            F3    259.5540   -19.37584
            M3    179.0262   -19.37584
            M4    155.6906   -19.37584
            M7    113.2306   -19.37584
</code></pre>

<h1>Surprising Output:</h1>

<pre><code>library(gsheet)
recall &lt;- read.csv(text = 
    gsheet2text('https://drive.google.com/open?id=1iVDJ_g3MjhxLhyyLHGd4PhYhsYW7Ob0JmaJP8MarWXU',
              format ='csv'))
head(recall)

 Subject Time Emtl_Value Recall_Rate Caffeine_Intake
1     Jim    0   Negative          54              95
2     Jim    0    Neutral          56              86
3     Jim    0   Positive          90             180
4     Jim    1   Negative          26             200

fit &lt;- lmer(Recall_Rate ~ (1|Subject) + Caffeine_Intake, data = recall)

coefficients(fit)
        $Subject
               (Intercept) Caffeine_Intake
        Jason     51.51206        0.013369
        Jim       51.51206        0.013369
        Ron       51.51206        0.013369
        Tina      51.51206        0.013369
        Victor    51.51206        0.013369
</code></pre>

<p>Here is the output of (<code>summary(fit)</code>):</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: Recall_Rate ~ (1 | Subject) + Caffeine_Intake
   Data: recall

REML criterion at convergence: 413.9

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.54125 -0.98422  0.04967  0.81465  1.83317 

Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept)   0.0     0.00   
 Residual             601.2    24.52   
Number of obs: 45, groups:  Subject, 5

Fixed effects:
                Estimate Std. Error t value
(Intercept)     51.51206    5.92408   8.695
Caffeine_Intake  0.01337    0.03792   0.353

Correlation of Fixed Effects:
            (Intr)
Caffen_Intk -0.787
</code></pre>

<h1>Question:</h1>

<p><strong>Why are all the Intercepts for the different subjects the same in the second example? The structure of the datasets and the <code>lmer</code> syntax appear very similar... and the boxplots don't seem to support the result:</strong></p>

<p><a href=""http://i.stack.imgur.com/xXYdS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xXYdS.png"" alt=""enter image description here""></a></p>

<p>Thank you in advance!</p>
"
"0.116759233144359","0.11444244151147","174136","<p>I have a dataset of a metric predictor variable $X$, and an ordered categorical predicted value $Y$ for several individuals. The dataset are from two groups $G_1$ and $G_2$. I want to estimate $Y$ from $X$, and I want to be able to compare the forecast accuracy of models, in group and individual level. For example, I want to know if these models helps to estimate $Y$ from a $X$ for a new user of a category, or a new experiment from the same user of a known category?</p>

<p>In <a href=""https://en.wikipedia.org/wiki/Ordered_probit"" rel=""nofollow"">ordered probit</a>, we suppose that $Y^*$ is the exact but unobserved dependent variable, and $X$ is the vector of independent variables, and $\beta$ is the a regression coefficient which we wish to estimate.</p>

<p>$Y^* = \mathbf{x}' \beta + \epsilon$</p>

<p>We can not observer $y*$ directly, but we instead can only observe the categories of response:</p>

<p>$
Y= \begin{cases}
0~~ \text{if}~~y^* \le 0, \\
1~~ \text{if}~~0&lt;y^* \le \mu_1, \\
2~~ \text{if}~~\mu_1 &lt;y^* \le \mu_2 \\
\vdots \\
N~~ \text{if}~~ \mu_{N-1} &lt; y^*.
\end{cases}
$</p>

<p>I came across this article from Gelman et al. that describes Bayesian Hierarchical Model: <a href=""https://en.wikipedia.org/wiki/Ordered_probit"" rel=""nofollow"">Multilevel (Hierarchical) Modeling: What It Can and Cannot Do</a>, which has been implemented in Python <a href=""http://nbviewer.ipython.org/github/fonnesbeck/multilevel_modeling/blob/master/multilevel_modeling.ipynb"" rel=""nofollow"">here</a>.</p>

<p>I am processing data in R, and I have selected a <strong>thresholded Bayesian hierarchical model</strong> to use with the <strong>generalized linear model</strong>. I have calculated the parameters of it using MCMC. My question is that how should I compare accuracy of ordered probit, and the equivalent Bayesian hierarchical model in R?</p>

<p>Gelman has used <a href=""https://en.wikipedia.org/wiki/Root-mean-square_deviation"" rel=""nofollow"">RMSE</a> for comparison using cross-validation. First he <em>removed single data points and checked the prediction from the model fit to the rest of the data, then removed single counties and performed the same procedure. For each cross-validation step, we compare complete-pooling, no-pooling, and multilevel estimates.</em></p>

<p>I have done MCMC simulation using RJags, which gave me the posterior distribution of the parameters, but how can I compare posterior distribution with a single point estimate of <strong>ordered probit</strong> to compare accuracy? Should I do as Gleman did and use RMSE? How? Or should I compare posterior distribution with results of several experiments with ordered probit? Is <a href=""http://www.stat.columbia.edu/~gelman/presentations/ggr.pdf"" rel=""nofollow"">posterior predictive check</a> usable here? I usually prefer cross-validation, but I don't know how to do this here.</p>

<p>PS: The notion of <strong>Goodness of fit</strong> in Bayesian analysis is ambigious to me. <a href=""http://people.stat.sfu.ca/~tim/papers/survey.pdf"" rel=""nofollow"">This paper</a> states:</p>

<blockquote>
  <p>GOODNESS-OF-FIT:</p>
  
  <p>In Bayesian statistics, there is no consensus on the
  correct"" approach to the assessment of goodness-of fit. When Bayesian
  model assessment is considered, it appears that the prominent modern
  approaches are based on the posterior predictive distribution (Gelman,
  Meng and Stern 1996).</p>
</blockquote>
"
"0.0633215847514023","0.0620651280774201","174462","<p>I am using the glmnet package in R to predict credit default. I have a 50 x variables which I have used in the first model. </p>

<pre><code>fit1=cv.glmnet(x[1:test,1:50], y[1:test], type.measure=""class"")
</code></pre>

<p>I have also generated interaction action terms covering all possible two-way interactions between x variables (i.e. x1*x2, x1*x3... xn*xn). This adds approx 2000 variables to the data set. Some of these interactions I know to be significant above and beyond the impact of of their linear effects. I then used both the 50 original variable, plus the 2000 interactions to fit my second model.  </p>

<pre><code> fit2=cv.glmnet(x[1:test,], y[1:test], type.measure=""class"")
</code></pre>

<p>I figured fit2 would be atleast as good as fit1 in out of sample testing. But strangely, the the best out-of-sample classification rate from fit1 (tested across all values of lambda) beats the equivalent from fit2. </p>

<p>I would've though fit2 to perform at least as well and probably better than fit1 (given that the lasso algorithm should push out all non-important interactions, which is likely to be most of them). I could manually add in only the interaction terms I suspect to be important, but then I face the dilemma about where to draw the line and lose the feature selection capability, which is one of the most useful aspects of lasso regression. My questions:</p>

<ol>
<li>Is there a logical explanation as to why fit2 performs better out of
sample than fit2?</li>
<li>How can I improve fit1 by including interaction variables that I am
confident are significant, without reducing predictive performance?</li>
</ol>
"
"0.02831827358943","0.0277563690826684","174476","<p>how to best predict data like this which contains multiple levels of nearly constant data?</p>

<p>Simple linear models even with weights (exponential) did not cut it.</p>

<p>I experimented with some clustering and then robust linear regression but my problem is that the relationship between these levels of constant data is lost.</p>

<p>Here is the data from the picture:</p>

<pre><code>structure(list(date = structure(c(32L, 10L, 11L, 14L, 5L, 6L, 
1L, 2L, 12L, 9L, 19L, 13L, 4L, 17L, 15L, 3L, 18L, 7L, 8L, 21L, 
16L, 22L, 28L, 29L, 30L, 26L, 27L, 31L, 20L, 23L, 24L, 25L), .Label = c(""18.02.13"", 
""18.03.13"", ""18.11.13"", ""19.08.13"", ""19.11.12"", ""20.01.13"", ""20.01.14"", 
""20.02.14"", ""20.05.13"", ""20.08.12"", ""20.09.12"", ""21.04.13"", ""21.07.13"", 
""21.10.12"", ""21.10.13"", ""22.04.14"", ""22.09.13"", ""22.12.13"", ""23.06.13"", 
""25.01.15"", ""25.03.14"", ""25.05.14"", ""26.02.15"", ""26.03.15"", ""26.04.15"", 
""26.10.14"", ""26.11.14"", ""27.07.14"", ""27.08.14"", ""28.09.14"", ""28.12.14"", 
""29.03.10""), class = ""factor""), amount = c(-4, -12.4, -9.9, -9.9, 
-9.94, -14.29, -9.97, -9.9, -9.9, -9.9, -9.9, -9.9, -9.9, -9.9, 
-9.9, -9.9, -9.9, -4, -4, -11.9, -11.9, -11.9, -11.9, -11.98, 
-11.98, -11.9, -13.8, -11.64, -11.96, -11.9, -11.9, -11.9)), .Names = c(""date"", 
""amount""), class = ""data.frame"", row.names = c(NA, -32L))
</code></pre>

<p><a href=""http://i.stack.imgur.com/DWypm.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DWypm.jpg"" alt=""regression for multiple levels""></a></p>

<h1>revisiting rollmedian</h1>

<p>@Gaurav - you asked: Have you tried building a model with moving averages? as ARIMA didn't work - I did not try it. But I have now.</p>

<pre><code>zoo::rollmedian(rollTS, 5)
</code></pre>

<p>Seems to get the pattern of the data. However I wonder now how to reasonably forecast it. Is this possible?</p>

<p><a href=""http://i.stack.imgur.com/dPhK8.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dPhK8.png"" alt=""rollmedian""></a></p>
"
"0.02831827358943","0.0277563690826684","174554","<p>My challenge: maximize $R^2$ on an out of sample data set. 
Constraints: </p>

<ol>
<li>Continuous dependent variable with negative values</li>
<li>Over 150 variables with no information about them</li>
<li>Some of these independent variables are categorical</li>
<li>Over 50,000 observations</li>
</ol>

<p>My strategy: run a random forest, LASSO and linear regression model. Then blend (haven't done this before) them together. Thoughts on this?</p>
"
"0.0849548207682898","0.0832691072480053","174861","<p>Here is <a href=""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"" rel=""nofollow"">sample data</a>:</p>

<pre><code>    brainIQ &lt;- 
  read.table (file= ""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"",
 head = TRUE)
</code></pre>

<p>I am trying to fit multiple linear regression.</p>

<pre><code>mylm &lt;- lm(PIQ ~  Brain + Height + Weight, data = brainIQ)
anova(mylm)
</code></pre>

<p>Default function anova in R provides sequential sum of squares (type I) sum of square. </p>

<pre><code>Analysis of Variance Table

Response: PIQ
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
Brain      1  2697.1 2697.09  6.8835 0.01293 *
Height     1  2875.6 2875.65  7.3392 0.01049 *
Weight     1     0.0    0.00  0.0000 0.99775  
Residuals 34 13321.8  391.82                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I belief, thus the SS are Brain, Height | Brain, Weight | (Brain, Weight) and residuals respectively.</p>

<p>Using package car we can also get type II sum of square. </p>

<pre><code>library(car)
Anova(mylm, type=""II"")
Anova Table (Type II tests)

Response: PIQ
           Sum Sq Df F value    Pr(&gt;F)    
Brain      5239.2  1 13.3716 0.0008556 ***
Height     1934.7  1  4.9378 0.0330338 *  
Weight        0.0  1  0.0000 0.9977495    
Residuals 13321.8 34                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Here sum of squares are like: Brian | (Height, Weight), Height | (Brain, Weight), Weight | (Brain, Height).</p>

<p>Which look pretty like Mintab output:</p>

<p><a href=""http://i.stack.imgur.com/0iXgH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0iXgH.png"" alt=""enter image description here""></a></p>

<p>My question is how can I calculate the regression row in the above table in R ? </p>
"
"0.0424774103841449","0.0555127381653369","174920","<p>Are there any easy to use alternatives to stepwise variable selection for GLMMs? I have seen implementations of e.g. LASSO for linear regression, but so far not seen anything for mixed models. Mixed models seem non-trivial in general, so I am wondering if any of the fancy new methods have been adapted from them (and possibly implemented in R). Using whatever selection procedure you like and then validating the results seems a sensible way to go in the meantime.</p>

<p>To give some context: in my current project, I am looking at approximately 700 variables and 5000 binary observations. Stepwise selection takes about 1 day; many variables have about 10% missingness.</p>

<p>Edit: Thank you for the very interesting answers so far! Two concerns that I have are: do these new methods have longer runtimes than stepwise selection and can they deal with missing data (if each variable has different missingness, than for hundreds of variables it is very easy to loose all observations in a complete case analysis - something that stepwise selection can deal with by only using small subsets of the available variables at the same time).</p>
"
"NaN","NaN","175026","<p>I googled and searched on stats.stackexchange but I cannot find the formula to calculate a 95% confidence interval for an $R^2$ value for a linear regression. Can anyone provide it?</p>

<p>Even better, let's say I had ran the linear regression below in R. How would I calculate a 95% confidence interval for the $R^2$ value using R code.</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt, mtcars)
</code></pre>
"
"0.0566365471788599","0.0555127381653369","175079","<p>Is it so that:</p>

<ul>
<li>$y_i$ is not a discrete value, but a range with probability density function</li>
<li>Which means for the same predictor(s) value $y_i$ could have different results</li>
<li>In linear regression this distribution can only be normal</li>
<li>In GLM, this distribution can be any distribution from the exponential family</li>
<li>distribution of a single $y_i$ has nothing to do with distribution of all $y(s)$</li>
<li>$\mu_i$ is expected value of $y_i$</li>
<li>In practical use, $\mu_i$ is the predicted value $y_i$, specially if dataset has only one y for given predictor(s)</li>
</ul>

<p>Are above correct? Where am I wrong?</p>

<p>Based on the above I've tried simulating <code>glm</code> with <code>lm</code> in R, and it kinda works:</p>

<pre><code>library(boot)
download.file(""https://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""./ravensData.rda"",method=""curl"")
load(""./ravensData.rda"")
# download manually and loadhere if above fails
# load(""/yourpath/ravensData.rda"")

# calling logit(ravensData$ravenWinNum) results in 
# [1]  Inf  Inf  Inf  Inf  Inf -Inf  Inf  Inf  Inf  Inf -Inf  Inf  Inf  Inf  Inf -Inf
# [17] -Inf -Inf  Inf -Inf
# that's way too much, as inv.logit goes to 1 at 20
# so we'll write our own dummy ""logit"" routine
# this will give us 5 when winNum=1 and -5 when it's zero
win &lt;- ravensData$ravenWinNum*10-5

# now we can do a simple lm
fit &lt;- lm(win~ravensData$ravenScore)

# and get probability of win using inv.logit
fitwin &lt;- inv.logit(fit$fitted.values)
plot(ravensData$ravenScore, fitwin)

# now glm
fitglm &lt;- glm(ravensData$ravenWinNum ~ ravensData$ravenScore, family=""binomial"")
plot(ravensData$ravenScore,fitglm$fitted)
</code></pre>
"
"NaN","NaN","175127","<p>I am using R to perform linear regression.  I have seen ways to calculate prediction intervals, but these depend on homoscedastic data.  Is there a way to calculate prediction intervals with heteroscedastic data?</p>
"
"NaN","NaN","175193","<p>I need to handle measurement error in independent variables. I have the CEV case i.e. that $$CoV(x_1^*, e_1) = 0$$ where $x_1^* = ability$ (a persons innate ability) and $x_1 = sat$ (a test score). </p>

<p>In stata I would use <em>eivreg</em> but what do I use in R?</p>

<p>See example here for how he did it in stata <a href=""http://www.philender.com/courses/linearmodels/notes4/merror.html"" rel=""nofollow"">http://www.philender.com/courses/linearmodels/notes4/merror.html</a></p>

<hr>

<p>To be precise, I have a situation where $colgpa = sat + u$ was my first regression but then we introduce $colgpa = ability + e_1 + u$ and we do the assumption that $Cov(ability, e_1) = 0$. We know from theory that </p>
"
"0.0490486886395286","0.0480754414848157","175613","<p>I have a data frame like this</p>

<pre><code>  head(mydata)
    y         class
  -0.06047565     1
   0.76982251     1
   3.05870831     1
   2.07050839     1
   2.62928774     1
   4.71506499     1
   3.96091621     1
   2.73493877     1
   3.81314715     1
   4.55433803     1
   2.22408180     2
   2.35981383     2
   3.40077145     2
   4.11068272     2
   4.44415887     2
   7.78691314     2
   7.49785048     2
   6.03338284     2
   9.70135590     2
   9.52720859     2
</code></pre>

<p>where <code>y</code> are numeric values and <code>class</code> are factors</p>

<pre><code>class(mydata$class)
[1] ""factor""
</code></pre>

<p>I would like to perform a linear regression with zero slope and intercept that depends on the class.</p>

<p>However when I do</p>

<pre><code>lm(y ~ class,data = mydata)
</code></pre>

<p>I get this result which I don't understand</p>

<pre><code>Call:
lm(formula = y ~ class, data = mydata)

Coefficients:
(Intercept)       class2       class3       class4       class5  
      2.825        2.884        5.001        8.497       10.917 
</code></pre>

<p>Why there is only one intercept? And what are the values for each class, intercepts or slopes?</p>

<p>Many thanks</p>
"
"0.0800961731463273","0.078506867197886","175652","<p>I am familiar with fixed-effects linear regression models, and have done reading on mixed-effects models.</p>

<p>I am attempting to fit a model based on observational data, where treatments come at varying times and do not exist at all for a majority of subjects.</p>

<p>I am interested in whether or not the treatment has an effect on the trajectory of a subject's response over time. Graphically:</p>

<p><img src=""http://i.imgur.com/szfwOL6.png"" alt=""Varying treatment time mixed model""></p>

<p>The most relevant analogous model I have found would be the one specified <a href=""http://www.ats.ucla.edu/stat/seminars/mlm_longitudinal/"" rel=""nofollow"">here</a>, specifically Part 3. However, this example does not use R. I have read through all of Bates' lme4 paper, but I am still uncertain how to specify this effect.</p>

<p>An excerpt of my data:</p>

<pre><code>     ID RESPONSE ID.CONST.1 ID.VAR.1 ID.VAR.2 TREATMENT_ACTIVE RESPONSE.TIME
1077415        7         41        0        5            FALSE           314
1077415        8         41        1        6            TRUE            316
1077415        9         41        10       7            TRUE            319
1077688        1         59        0        1            FALSE           313
1079475        1         85        0        1            FALSE           313
1080811        1         24        0        1            FALSE           314
1081156        1        502        0        1            FALSE           314
1082437        1         50        0        0            FALSE           315
1083154        1        257        0        0            FALSE           315
1083154        2        257        0        0            TRUE            316
1083527        1         69        0        0            FALSE           315
1086283        1         31        0        0            FALSE           316
1088810        1        120        2        1            FALSE           317
1090019        1         93        2        1            TRUE            317
1091048        1         27        0        0            FALSE           317
1091114        1         62        0        1            FALSE           317
</code></pre>

<p>Each subject (<code>ID</code>) has time-varying measurements (<code>ID.VAR.X</code>), constant measurements (<code>ID.CONST.X</code>), as well as the time of observation (<code>RESPONSE.TIME</code>). <code>TREATMENT_ACTIVE</code> indicates whether or not the treatment is active for a given subject at the corresponding <code>RESPONSE.TIME</code>. Some subjects have a single observation, others have multiple observations, and treatment times are rarely the same between subjects.</p>

<p>I've attempted to fit models as:</p>

<pre><code>lmer(RESPONSE ~ ID.CONST.1 + ID.VAR.1 + ID.VAR.2 + TREATMENT_ACTIVE + RESPONSE.TIME + (1|ID) + (1|RESPONSE.TIME)
lmer(RESPONSE ~ ID.CONST.1 + ID.VAR.1 + ID.VAR.2 + RESPONSE.TIME + (1|ID) + (1+TREATMENT_ACTIVE|RESPONSE.TIME)
</code></pre>

<p>However, I'm fairly certain this is misspecified. I am not sure how to specify the random effects to ensure that the <code>TREATMENT_ACTIVE</code> variable is interpreted as I intend. I am interested in testing both an intercept-only model as well as a intercept+slope model for the treatment effect.</p>
"
"0.09392108820677","0.0836886016271203","175770","<p>This question is more of theoretical. I am not sure if this is the right place, but still giving it a try. </p>

<p>I have two variables &mdash; direct cost and indirect cost. When sales persons go for a sales pitch to a customer they know about direct cost that they are going to incur for this service, but they don't know much about indirect cost (they will come to know about it in latter stages). An estimate of indirect cost at this stage will be valuable for sales persons. </p>

<p>I am trying to predict indirect cost as a function of direct cost. I am doing this via a simple linear regression. I plotted scatter plot between direct cost and indirect cost and see a <strong>good linear relationship</strong> between them. I also see that direct cost and indirect cost are <strong>highly corelated</strong> to each other with correlation coefficient as 0.98, so I expected a very good prediction accuracy. But surprisingly, my prediction accuracy is not so good. I have around 200,000 points in my training data and average prediction error on training data is 17 %. Though adjusted R-Square value is 0.97. I am using <code>lm()</code> function from R.       </p>

<p>My question is that in case of simple linear regression, in general, should we expect better prediction accuracy if dependent and independent variables are highly correlated or is it my misconception? If we expect good accuracy, am I missing something here. Please note that I have also tried centering these variables around mean. </p>
"
"0.0566365471788599","0.0416345536240027","175983","<p>I've been asked a question regarding a linear model made with R's <code>lm</code>:</p>

<p>""Did the regression use linear or non-linear iterative least squares?""</p>

<p>I searched a bit and [think that I] understand the difference between the two, but couldn't find any evidence of R's use of linear least squares in <code>lm</code> (which is the one I think it uses). </p>

<p>I combed throuhg <code>lm</code> and its underlying function <code>lm.fit</code> documentation, but couldn't find anything related.</p>

<p>I think the question I was asked is a dumb question, and it's probably wrongly formulated, but I'd appreciate any help as to how I could reply to it.</p>
"
"0.0566365471788599","0.0555127381653369","176147","<p>Im trying to estimate the linear curve (y~x) where I know intercept must be normally distributed around -100, and slope always positive and normally distributed around 2 (blue continous line in plots below).</p>

<p>The example.data.1 below is ""clean"" and the linear regression (red dashed line) is ok. The resulting red dashed line is what I want.</p>

<p>But example.data.2 has many measurement errors so the red dashed line becomes unrealistic. The resulting line should be parallel with the blue line, but lower.</p>

<p>How can I assign a strong prior similar to the blue line in the plots, so that I get a posterior reasonably similar to the blue line?</p>

<pre><code>example.data.1 &lt;- structure(list(x = c(1.36, 2.22, 2.53, 3.09, 3.44, 3.25, 3.15, 
                                       3.21, 3.57, 3.63, 3.51, 2.85, 2.56, 2.25, 1.61, 1.35, 1, 1.6, 
                                       1.92, 1.9, 2.3, 2.61, 3.9, 3.74, 3.77, 3.77, 3.49, 3.37, 3.35, 
                                       2.79, 2.31, 1.88, 1.5, 1.18, 1.83, 2.32, 3.06, 3.37, 3.77, 3.82, 
                                       3.75, 3.72, 3.53, 3.35, 3.67, 3.18, 3.11, 2.43, 1.9, 1.39, 1.17, 
                                       1.48, 2.05, 2.62, 3.08, 3.65, 3.92, 4.08, 4.1, 3.47, 3.84, 3.45, 
                                       2.87, 2.83, 2.49, 1.87, 2.06, 2.49, 1.78, 2.33, 2.95, 3.73, 3.64, 
                                       3.62, 4.1, 3.85, 4.06, 3.67, 3.3, 2.86, 2.46, 2.32, 2.08, 1.64, 
                                       1.96), y = c(-101.04, -99.42, -98.33, -96.88, -95.22, -91.89, 
                                                    -91.63, -90.19, -92.98, -95.58, -95.69, -96.32, -96.94, -98.25, 
                                                    -100.11, -100.81, -101.87, -99.72, -99.94, -100.87, -100.38, 
                                                    -98.64, -93.38, -92.98, -93.39, -93.76, -93.25, -93.12, -94.46, 
                                                    -96.45, -97.46, -99.75, -100.09, -101.62, -101.1, -97.8, -96.33, 
                                                    -96.21, -94.37, -93.18, -93.32, -93.73, -94.13, -94.4, -94.63, 
                                                    -94.83, -96.29, -98.11, -100.2, -100.82, -101.56, -101.35, -100.61, 
                                                    -98.65, -97.37, -95.36, -95.45, -95.33, -95.63, -95.26, -97.08, 
                                                    -97.1, -97.14, -96.9, -98.17, -99.47, -100.17, -100.58, -100.55, 
                                                    -99.94, -99.02, -97.3, -96.25, -95.44, -95.69, -95.21, -95.87, 
                                                    -95.87, -97.71, -96.91, -97.62, -97.94, -98.9, -99.79, -99.88
                                       )), .Names = c(""x"", ""y""), row.names = c(NA, -85L), class = ""data.frame"")

example.data.2 &lt;- structure(list(x = c(3.11, 3.46, 3.42, 3.34, 3.3, 2.45, 4, 4.2, 
                                       4.08, 3.57, 1.97, 1.83, 1.07, 0.68, 0.54, 0.47, 0.63, 3.19, 3.52, 
                                       3.49, 3.47, 3.36, 2.76, 3.42, 3.17, 3.54, 2.56, 1.06, 1.09, 0.84, 
                                       0.64, 0.61, 0.74, 0.49, 3.49, 3.56, 3.46, 3.25, 3.72, 3.57, 3.58, 
                                       2.62, 1.99, 1.85, 1.04, 1.06, 0.62, 0.49, 0.48, 0.68, 0.5, 3.63, 
                                       3.71, 3.75, 3.67, 3.78, 3.52, 3.04, 2.26, 1, 1.17, 1.01, 0.92, 
                                       0.65, 0.54, 0.36, 0.38, 0.3, 3.08, 3.79, 3.9, 3.5, 3.4, 2.57, 
                                       3.03, 1.93, 2.02, 1.5, 0.67, 0.63, 0.72, 0.6, 0.67, 0.63, 0.53
), y = c(-105.28, -104.1, -104.81, -104.34, -104.37, -105.31, 
         -103.59, -103.32, -102.66, -103.57, -103.73, -104.47, -97.69, 
         -92.56, -95.9, -95.72, -107.6, -104.39, -105.12, -104.18, -104.46, 
         -102.19, -103.59, -103.38, -103.48, -102.84, -96.52, -88.54, 
         -90.36, -93.7, -85.21, -89.68, -99.47, -91.92, -104.58, -103.91, 
         -104.47, -104.49, -104.41, -104.41, -102.6, -98.65, -87.98, -89.23, 
         -86.34, -94.21, -91.57, -84.62, -84.14, -95.33, -102.14, -104.18, 
         -103.8, -102.47, -101.75, -101.73, -102.84, -97.49, -92.67, -91.72, 
         -80.45, -80.97, -84.94, -80.2, -81.05, -77.84, -82.72, -91.75, 
         -105.19, -104.66, -104.36, -104.31, -103.57, -102.68, -98.4, 
         -89.48, -85.92, -84.59, -84.49, -81.13, -83.28, -83.12, -85.62, 
         -85.89, -90.07)), .Names = c(""x"", ""y""), row.names = c(NA, -85L
         ), class = ""data.frame"")


lm.1 &lt;- coefficients(lm(example.data.1$y~example.data.1$x))
lm.2 &lt;- coefficients(lm(example.data.2$y~example.data.2$x))

library(ggplot2)
p &lt;- ggplot(example.data.1, aes(x=x, y=y))
p &lt;- p + geom_point()
p &lt;- p + geom_abline(intercept=(-100), slope=2, color=""blue"")
p &lt;- p + geom_abline(intercept=lm.1[1], slope=lm.1[2], color=""red"", linetype=""dashed"")
p &lt;- p + xlim(0, 10)
p &lt;- p + ylim(-110, -50)
p 

p &lt;- ggplot(example.data.2, aes(x=x, y=y))
p &lt;- p + geom_point()
p &lt;- p + geom_abline(intercept=(-100), slope=2, color=""blue"")
p &lt;- p + geom_abline(intercept=lm.2[1], slope=lm.2[2], color=""red"", linetype=""dashed"")
p &lt;- p + xlim(0, 10)
p &lt;- p + ylim(-110, -50)
p
</code></pre>

<p>I need to do this for tens of thousands of data-groups like each example above, so a fast algorithm is important. I have tried to use Stan, Jags, and arm-package, but don't understand how to tell those functions what I want.</p>

<p>My limited knowledge about statistics lead me to think that a bayesian approach is best, but I could be wrong.</p>
"
"0.02831827358943","0.0277563690826684","176174","<p>I am building a <strong>reaction network</strong> to model an industrial reactor with over 30 reactions part of the network. I have all the data i need regarding the regarding the reactor and I was thinking of using non-linear regression (in Matlab) to build my statistical model. The equations are of the form :</p>

<p>Change in concentration= (ki)<em>(concentration)</em>(space velocity)*(constants)</p>

<p>I need the values of ""ki"" as well.Could anyone suggest alternate models which might give me better results(I have data for a time period of 2 years).I am fairly new to this and any help would really be appreciated. :) </p>
"
"0.0749231094763201","0.0734364498908627","176351","<p>I know there is an analytic solution to the following problem (OLS). Since I try to learn and understand the principles and basics of MLE, I implemented the fisher scoring algorithm for a simple linear regression model. </p>

<p>$$
y = X\beta + \epsilon\\
\epsilon\sim N(0,\sigma^2)
$$</p>

<p>The loglikelihood for $\sigma^2$ and $\beta$ is given by:
$$
-\frac{N}{2}ln(2\pi)-\frac{N}{2}ln(\sigma^2)-\frac{1}{2\sigma^{2}}(y-X\beta)^{'}(y-X\beta)
$$ </p>

<p>To compute the score function $S(\theta)$, where $\theta$ is the vector of parameters $(\beta,\sigma^{2})^{'}$, I take the first partial derivatives with respect to $\beta$ and $\sigma^{2}$:
$$
\frac{\partial L}{\partial \beta} = \frac{1}{\sigma^{2}}(y-X\beta)^{'}X
\\
\frac{\partial L}{\partial \sigma^2} = -\frac{N}{\sigma^{2}}+\frac{1}{2\sigma^{4}}(y-X\beta)^{'}(y-X\beta)
$$</p>

<p>Then the Fisher scoring algorithm is implemented as:
$$
\theta_{j+1} = \theta_{j} - (S(\theta_{j})S(\theta_{j})^{'})S(\theta_{j})
$$</p>

<p>Please note, the following code is a very naive implementation (no stopping rule, etc.)</p>

<pre class=""lang-R prettyprint-override""><code>library(MASS)
x &lt;- matrix(rnorm(1000), ncol = 2)
y &lt;- 2 + x %*% c(1,3) + rnorm(500)

fisher.scoring &lt;- function(y, x, start = runif(ncol(x)+1)){
    n &lt;- nrow(x)
    p &lt;- ncol(x)
    theta &lt;- start
    score &lt;- rep(0, p+1)
    for (i in 1:1e5){
        # betas
        score[1:p] &lt;- (1/theta[p+1]) * t((y - x%*%theta[1:p])) %*% x
        # sigma
        score[p+1] &lt;- -(n/theta[p+1]) + (1/2*theta[p+1]^2) * crossprod(y - x %*% theta[1:p])
        # new
        theta &lt;- theta - MASS::ginv(tcrossprod(score)) %*% score
    }
    return(theta)
}

# Gives the correct result 
lm.fit(cbind(1,x), y)$coefficients
# Does not give the correct result
fisher.scoring(y, cbind(1,x))
# Even if you start with the correct values
fisher.scoring(y, cbind(1,x), start=c(2,1,3,1))
</code></pre>

<p><strong>My Question</strong></p>

<p><em>What did I miss? Where is my mistake?</em></p>
"
"0.0490486886395286","0.0480754414848157","176470","<p>I've used R's non-linear regression function <code>nls</code> to find estimates for two different functions that both give a reasonably good plot for my data. However, I'm supposed to find out which one of the two functions is a better fit for my data. How can I do that? I've already made QQplots for the residuals of both functions, and both show that the residuals are normally distributed. Are there any other ways to check which one of my two functions fits my data best?</p>
"
"0.0424774103841449","0.0555127381653369","176595","<p>I am trying to fit a regression model, as the plot says the relation is log.</p>

<p><a href=""http://i.stack.imgur.com/bWnMa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bWnMa.png"" alt=""enter image description here""></a></p>

<p>I tried to use </p>

<pre><code>lm(logData$x ~ logData$b3, data = logData) 
</code></pre>

<p>but it did not work because it fits the linear model.</p>

<p>Also I tried to use </p>

<pre><code>model = nls(logData$x ~ logData$b3) 
</code></pre>

<p>but it gives me errors.</p>

<p>So, what I need to do is fit the simple log regression also plot the regression curve on the scatter plot. </p>
"
"0.09392108820677","0.0920574617898323","176622","<p>In many papers in the social sciences, missing data are handled by <em>direct</em> or <em>full information</em> maximum likelihood estimation (FIML). Unfortunately this is almost always done with closed source software.  </p>

<p>To get an idea how FIML works, I tried to implement it myself in R. </p>

<p>In case of a multivariate normal distribution the loglikelihood for a single observation is given by (according to Enders, 2001, p. 134):</p>

<p>$$
log L_{i} = K_{i} - \frac{1}{2}log|\Sigma_i|-\frac{1}{2}(x_{i}-\mu_{i})^{'}\Sigma^{-1}(x_{i}-\mu_{i})
$$</p>

<p>Where $K_{i}$ is the numer of observed variables for obesrvation $i$. </p>

<p>In case of a simple linear regression model with $N$ observations and assumed homoskedascity the formula should reduce to:</p>

<p>$$
log L_{i} = K_{i} - \frac{1}{2}log(\sigma^{2})-\frac{1}{2\sigma^2}(y_{i}-X_{i}\beta_{i})^{2}
$$</p>



<pre><code>set.seed(42)
x &lt;- matrix(rnorm(1000), nrow=500) ; x &lt;- cbind(1,x)
y &lt;- x %*% c(2,1,3) + rnorm(500, mean = 0, sd = 1)
z.full &lt;- cbind(y,x)

# MCAR predictors
x[sample.int(n = 500, size = 50),2] &lt;- NA
x[sample.int(n = 500, size = 50),3] &lt;- NA
z.miss &lt;- cbind(y,x)

llog.single &lt;- function(z, beta, sigma){
    y &lt;- z[1]
    x &lt;- z[-1]
    idx &lt;- !is.na(x)  # = K_{i}
    return(sum(idx) + dnorm(y, mean = x[idx]%*%beta[idx], sd = sqrt(sigma), log = TRUE))
}

loglikelihood &lt;- function(y, x, theta) {
    p &lt;- ncol(x)
    beta  &lt;- theta[1:p]
    sigma &lt;- theta[p+1]
    return(-sum(apply(cbind(y,x), 1, llog.single, beta = beta, sigma = sigma)))
}

# Full information maximum likelihood 
optim(par = c(0,0,0,1), fn = loglikelihood, method = ""BFGS"", x=z.mis[,-1], y=z.miss[,1])$par
# OLS on full dataset
lm(V1 ~ V3 + V4, data = as.data.frame(z.full))
</code></pre>

<p><strong>My Question</strong></p>

<p><em>What should I do, if there is missing data in the outcome $Y_{i}$?</em></p>

<p>From my current point of view I would just ignore cases with missing data in $Y$. Is this correct or is there maybe a better way?</p>

<hr>

<p>Enders, C. K. (2001). A primer on maximum likelihood algorithms available for use with missing data. Structural Equation Modeling, 8(1), 128-141.</p>
"
"0.0566365471788599","0.0555127381653369","176671","<p>During the first half of 2015 I did the <a href=""https://www.coursera.org/learn/machine-learning"">coursera course of Machine Learning</a> (by Andrew Ng, GREAT course). And learned the basics of machine learning (linear regression, logistic regression, SVM, Neuronal Networks...)</p>

<p>Also I have been a developer for 10 years, so learning a new programming language would not be a problem.</p>

<p>Lately, I have started learning R in order to implement machine learning algorithms.</p>

<p>However I have realized that if I want to keep learning I will need a more formal knowledge of statistics, currently I have a non-formal knowledge of it, but so limited that, for example, I could not properly determine which of several linear models would be better (normally I tend to use R-square for it, but apparently that is not a very good idea). </p>

<p>So to me it seems pretty obvious that I need to learn the basics of statistics (I studied that in uni but forgot most of it), where should I learn, please note that I don't really need a fully comprehensive course, just something that within a month allows me to know enough so I can get eager and learn more :).</p>

<p>So far I have read about ""<a href=""https://books.google.co.in/books/about/Statistics_Without_Tears.html?id=WyB1QgAACAAJ&amp;redir_esc=y"">Statistics without tears</a>"", any other suggestion?</p>
"
"0.0400480865731637","0.039253433598943","176768","<p>I am fitting a simple regression model on my data and I am using <code>nls()</code>
the only problem is it does not provide the coefficient of determination $R^2$ and adjusted $R^2$.</p>

<p>My question is how can I calculate both of $R^2$ and adjusted $R^2$.</p>

<p>The data and codes are </p>

<pre><code># generate data

beta &lt;- 0.012

n &lt;- 300

Data&lt;- data.frame(y = exp(beta * seq(n)) + rnorm(n), x = seq(n))

# plot data

plot(Data$x, Data$y)

# fit non-linear model

mod &lt;- nls(y ~ exp(a + b * x), data = Data, start = list(a = 0, b = 0))

# add fitted curve

lines(Data$x, predict(mod, list(x = Data$x)))
</code></pre>
"
"0.0400480865731637","0.039253433598943","177189","<p>I am currently trying to solve a Kaggle Competition (Rossmann Store Sales). 
My linear regression model consists of all fields in the dataset. The objective is to predict sales.</p>

<p>List of all variables: <a href=""https://www.kaggle.com/c/rossmann-store-sales/data"" rel=""nofollow"">rossmann-store-sales</a></p>

<pre><code>ctrl &lt;- trainControl(method = ""cv"",number = 10)
mod1 &lt;- train(Sales ~ . - Date ,data = train,method = ""lm"",
              trControl = ctrl, metric=""Rsquared"")
summary(mod1)
mod1
</code></pre>

<p>In the training set, the data contains a Field called <code>Customers</code> and it is missing in the testing set. </p>

<p>My predict function fails since it encounters a missing field (<code>Customers</code>) in my testing set.</p>

<pre><code>pred &lt;- predict(mod1,newdata = test)
</code></pre>

<p>How to handle such a case or is it, that this field is suppose to help me in feature engineering?</p>

<p>Please let me know if any further information is required.</p>
"
"0.0578044339088637","0.0679889413649005","177388","<p>I am a beginner and trying to learn new concepts in statistical analysis.</p>

<p>I have some very basic question. With a given data set of individuals I am trying to ascertain as to whether or not they are eligible to be granted for a loan - credit scoring.</p>

<p>My data set has 300 obs of 10 variables which I figured out using:
str(data)
summary(data):</p>

<p><a href=""http://i.stack.imgur.com/2q8Dl.png"" rel=""nofollow"">Detailed Data Desc</a></p>

<p>After applying linear regression on all variables:  </p>

<pre><code>linreg=lm(Rating~.,data=data)  
cor(linreg$fitted.values,data$Rating)
</code></pre>

<p><a href=""http://i.stack.imgur.com/DtbQk.png"" rel=""nofollow"">Linear Regression detailed</a></p>

<p><strong>I understand:</strong>  </p>

<ol>
<li>Having 3 stars - p value means very significant and high and positive Estimates indicats that it has a positive significance and vice versa  </li>
<li>The correlation between fitted values and Rating comes to  0.9867324</li>
</ol>

<p><strong>Questions:</strong>  </p>

<ol>
<li>Does this mean regression predicts correctly 98.67% of the observations  </li>
<li>If everything else are equal then variables like Education and Gender have a positive impact on the rating? Because they have lowest p-values.  </li>
<li>What about Student and Income variables?  </li>
<li>Also does it mean individuals with high Income will have a greater rating, everything else being equal?  </li>
</ol>
"
"0.0899225958391357","0.0881383093888288","177805","<p>R and statistics beginner here, trying to do a quantile regression on a non-linear dataset. </p>

<p>I want to identify datapoints that have a higher y axis value that expected given their value on the x axis. 
I should highlight that the y-data are means of discrete values (0.1-1, in steps of 0.1) taken in dependence on the x-data. x values are number of SNPs in a gene. Each SNP has a discrete value and the y value is a mean of these SNP values for each gene.</p>

<p>After initially investigating  funnel plots it seems that a quantile regression might be most appropriate for this dataset, though thoughts on this are welcome.  I'd appreciate any guidance in fitting a quantile regression to identify that don't fall within 95 percent of the data.</p>

<p>Sample of data (I actually have ~20,000 datapoints):</p>

<pre><code>GENE    mean  total
X1  0.1 3
X2  0.1466666667    30
X3  0.1375  8
X4  0.24    5
X5  0.2625  8
X6  0.2 1
X7  0.1466666667    15
X8  0.2 1
X9  0.1666666667    9
X10 0.1 1
X11 0.1928571429    14
X12 0.1 2
X13 0.1545454545    11
X14 0.1333333333    3
X15 0.1666666667    3
X16 0.2117647059    34
X17 0.1452380952    42
X18 0.16    5
X19 0.2 1
X20 0.25    2
X21 0.125   4
X22 0.2 13
X23 0.1714285714    7
X24 0.15    6
X25 0.2 3
X26 0.2894736842    19
X27 0.2352941176    17
X28 0.1333333333    6
X29 0.12    5
X30 0.2 3
X31 0.1 1
X32 0.1571428571    7
X33 0.2125  8
X34 0.18125 16
X35 0.26    10
X36 0.1368421053    19
X37 0.1333333333    6
X38 0.15    2
X39 0.14    5
X40 0.18    15
X41 0.14    5
X42 0.3 1
X43 0.1 2
X44 0.1 6
X45 0.1 4
X46 0.1 1
X47 0.1333333333    3
X48 0.1166666667    6
X49 0.225   4
X50 0.2 15
X51 0.125   12
X52 0.1 3
X53 0.1714285714    14
X54 0.175   4
X55 0.3404761905    42
X56 0.1 1
X57 0.25    2
X58 0.15    4
X59 0.1 1
X60 0.1666666667    3
X61 0.3 2
X62 0.225   4
X63 0.3076923077    13
X64 0.1 1
X65 0.1666666667    3
X66 0.1666666667    6
X67 0.1 3
X68 0.1 3
X69 0.1166666667    6
X70 0.125   8
X71 0.2 1
X72 0.2 2
X73 0.1333333333    42
X74 0.1 1
X75 0.2 8
X76 0.1444444444    9
X77 0.1666666667    15
X78 0.1 2
X79 0.176744186 43
X80 0.1275  40
X81 0.1666666667    3
X82 0.125   4
X83 0.2545454545    11
X84 0.1304347826    46
X85 0.21    10
X86 0.1571428571    7
X87 0.3 9
X88 0.275   16
X89 0.11    10
X90 0.1333333333    6
X91 0.2333333333    3
X92 0.2 2
X93 0.2866666667    15
X94 0.25    2
X95 0.1125  8
X96 0.4 11
X97 0.1 1
X98 0.2 2
X99 0.15    2
X100    0.1625  8
X101    0.24    5
X102    0.175   4
X103    0.15    4
X104    0.1333333333    3
X105    0.4 2
X106    0.2 3
X107    0.25    2
X108    0.32    5
X109    0.2333333333    3
X110    0.1714285714    7
X111    0.2 1
X112    0.225   4
X113    0.2 1
X114    0.1714285714    7
X115    0.15    2
X116    0.1166666667    6
X117    0.16875 16
X118    0.1555555556    9
X119    0.15    6
X120    0.12    5
X121    0.1 1
X122    0.1333333333    6
X123    0.2333333333    3
X124    0.1 1
X125    0.2333333333    3
X126    0.1333333333    3
X127    0.1 1
X128    0.1827586207    29
X129    0.25    8
X130    0.2 7
X131    0.25    6
X132    0.1 1
X133    0.125   4
X134    0.2 1
X135    0.1666666667    3
X136    0.1 3
X137    0.12    5
X138    0.1 1
X139    0.175   4
X140    0.1 1
X141    0.1666666667    3
X142    0.1666666667    3
X143    0.1 1
X144    0.1375  8
X145    0.1 9
X146    0.1 2
X147    0.125   4
X148    0.1333333333    3
X149    0.1769230769    13
X150    0.15    2
X151    0.1214285714    14
X152    0.1 1
X153    0.2555555556    18
X154    0.2 1
X155    0.1 1
X156    0.1 1
X157    0.1 1
X158    0.4 1
X159    0.14    5
X160    0.1 2
X161    0.1333333333    3
X162    0.375   8
X163    0.2263157895    19
X164    0.1636363636    11
X165    0.3 1
X166    0.1 3
X167    0.2 1
X168    0.3 1
X169    0.1428571429    7
X170    0.1 2
X171    0.1222222222    9
X172    0.1 8
X173    0.1 5
X174    0.1 8
X175    0.1666666667    3
X176    0.2 5
X177    0.1 4
X178    0.1166666667    6
X179    0.15    2
X180    0.3666666667    3
X181    0.25    4
X182    0.1 1
X183    0.1 2
X184    0.1 1
X185    0.1 1
X186    0.1 1
X187    0.184   25
X188    0.2333333333    3
X189    0.2333333333    3
X190    0.1 2
X191    0.32    5
X192    0.1 2
X193    0.12    5
X194    0.1 5
X195    0.2 1
X196    0.1 6
X197    0.1 2
X198    0.4 1
X199    0.2 2
X200    0.1 2
X201    0.2 1
X202    0.2333333333    6
X203    0.35    2
X204    0.1 1
X205    0.12    5
X206    0.14    5
X207    0.125   4
X208    0.3333333333    3
X209    0.1 2
X210    0.1 3
X211    0.1 1
X212    0.2 4
X213    0.15    8
X214    0.125   4
X215    0.1548387097    31
X216    0.2 7
X217    0.225   4
X218    0.125   4
X219    0.15    2
X220    0.4 1
X221    0.275   4
X222    0.325   4
X223    0.2 3
X224    0.175   4
X225    0.3 1
X226    0.1 1
X227    0.19    10
X228    0.25    4
X229    0.2666666667    9
X230    0.1 1
X231    0.2 1
X232    0.3 1
X233    0.2166666667    6
X234    0.26    5
X235    0.225   4
X236    0.1 1
X237    0.1857142857    7
X238    0.58    5
X239    0.25    10
X240    0.6066666667    15
X241    0.3 1
X242    0.5 2
X243    0.2333333333    3
X244    0.25    2
X245    0.1 4
X246    0.1 1
X247    0.1714285714    7
X248    0.16875 16
X249    0.2 1
X250    0.4 3
X251    0.1 1
X252    0.1666666667    6
X253    0.2 6
X254    0.3166666667    12
X255    0.1 1
X256    0.1 2
X257    0.4 1
X258    0.1333333333    3
X259    0.225   4
X260    0.2571428571    7
X261    0.4 5
X262    0.15    10
X263    0.1571428571    7
X264    0.2 11
X265    0.2285714286    7
X266    0.15    4
X267    0.3 1
X268    0.1384615385    13
X269    0.1 4
X270    0.1 1
X271    0.16    5
X272    0.1285714286    7
X273    0.1 1
X274    0.2222222222    9
X275    0.2083333333    12
X276    0.2153846154    13
X277    0.1888888889    9
X278    0.1 1
X279    0.1 2
X280    0.3 2
X281    0.17    10
X282    0.1 5
X283    0.2833333333    6
X284    0.1333333333    6
X285    0.1833333333    6
X286    0.1833333333    12
X287    0.1953488372    43
X288    0.2526315789    19
X289    0.1 1
X290    0.125   4
X291    0.26    5
X292    0.1 2
X293    0.2578947368    19
X294    0.2545454545    11
X295    0.1 1
X296    0.3666666667    3
X297    0.1714285714    7
X298    0.1833333333    6
X299    0.16    5
X300    0.2733333333    15
X301    0.275   4
X302    0.1 1
X303    0.2 7
X304    0.1583333333    12
X305    0.1666666667    3
X306    0.1 1
X307    0.1 6
X308    0.1642857143    14
X309    0.1 1
X310    0.1606060606    33
X311    0.1428571429    7
X312    0.1888888889    9
X313    0.2 2
X314    0.1388888889    18
X315    0.35    2
X316    0.3 2
X317    0.1 4
X318    0.15    16
X319    0.1166666667    12
X320    0.1888888889    9
X321    0.16    5
X322    0.2333333333    3
X323    0.1857142857    14
X324    0.31    20
X325    0.2 1
X326    0.1 1
X327    0.1952380952    21
X328    0.215625    32
X329    0.1 1
X330    0.1 1
X331    0.1307692308    13
X332    0.1 4
X333    0.1666666667    3
X334    0.2 14
X335    0.1583333333    12
X336    0.1961538462    26
X337    0.2222222222    9
X338    0.1 3
X339    0.1 2
X340    0.1285714286    14
X341    0.175   4
X342    0.125   4
X343    0.1 4
X344    0.1428571429    7
X345    0.1 4
X346    0.1 2
X347    0.15    2
X348    0.25    4
X349    0.22    5
X350    0.1 2
X351    0.1 3
X352    0.14    10
X353    0.1666666667    18
X354    0.1333333333    3
X355    0.2 3
X356    0.16    5
X357    0.3 1
X358    0.175   4
X359    0.5 1
X360    0.1111111111    9
X361    0.2333333333    6
X362    0.175   4
X363    0.227027027 37
X364    0.3857142857    7
X365    0.1 2
X366    0.2 3
X367    0.1916666667    12
X368    0.1428571429    14
X369    0.2666666667    3
X370    0.2 9
X371    0.25    2
X372    0.2 1
X373    0.1 2
X374    0.225   4
X375    0.1 1
X376    0.1 3
X377    0.3 2
X378    0.1 1
X379    0.1545454545    11
X380    0.1730769231    52
X381    0.1 3
X382    0.1333333333    3
X383    0.1814814815    27
X384    0.108   25
X385    0.2666666667    6
X386    0.1666666667    3
X387    0.25    8
X388    0.225   4
X389    0.24    25
X390    0.2666666667    6
X391    0.1 2
X392    0.15    4
X393    0.1666666667    6
X394    0.1 1
X395    0.2375  8
X396    0.125   4
X397    0.1 7
X398    0.1 7
X399    0.1 4
X400    0.1 2
X401    0.1625  8
X402    0.3 1
X403    0.3 2
X404    0.25    4
X405    0.2 1
X406    0.1285714286    7
X407    0.15    8
X408    0.5 1
X409    0.1 1
X410    0.1285714286    7
X411    0.1 1
X412    0.2166666667    30
X413    0.22    5
X414    0.2714285714    14
X415    0.1214285714    14
X416    0.2 8
X417    0.28    5
X418    0.24    35
X419    0.15    4
X420    0.1333333333    12
X421    0.125   4
X422    0.1 1
X423    0.1666666667    3
X424    0.2111111111    9
X425    0.3 4
X426    0.2 2
X427    0.2 3
X428    0.1 1
X429    0.1 1
X430    0.1617021277    47
X431    0.15    8
X432    0.1142857143    14
X433    0.15    4
X434    0.1384615385    13
X435    0.1 2
X436    0.1166666667    12
X437    0.1714285714    14
X438    0.2416666667    12
X439    0.1 1
X440    0.1428571429    7
X441    0.1 1
X442    0.1416666667    12
X443    0.3333333333    6
X444    0.2 1
X445    0.14    5
X446    0.2 3
X447    0.225   28
X448    0.1571428571    14
X449    0.1 1
X450    0.1583333333    12
X451    0.1518518519    27
X452    0.1363636364    11
X453    0.2 1
X454    0.1666666667    6
X455    0.1 1
X456    0.1333333333    3
X457    0.2368421053    19
X458    0.1222222222    9
X459    0.15    2
X460    0.2 1
X461    0.1625  24
X462    0.2 6
X463    0.1666666667    3
X464    0.1 3
X465    0.3 8
X466    0.1523809524    21
X467    0.1 3
X468    0.1 3
X469    0.15    4
X470    0.1 1
X471    0.1642857143    28
X472    0.1 5
X473    0.1 2
X474    0.12    15
X475    0.1 3
X476    0.1090909091    11
X477    0.1346153846    26
X478    0.125   4
X479    0.1444444444    9
X480    0.2 1
X481    0.1 1
X482    0.1 3
X483    0.2 3
X484    0.1375  8
X485    0.1 4
X486    0.12    5
X487    0.1739130435    23
X488    0.25    2
X489    0.1333333333    6
X490    0.3 1
X491    0.225   20
X492    0.175   4
X493    0.1 3
X494    0.1222222222    9
X495    0.1 1
X496    0.175   4
X497    0.2333333333    6
X498    0.1615384615    13
X499    0.15    8
X500    0.1666666667    6
X501    0.2 2
X502    0.1777777778    9
X503    0.15    4
X504    0.2666666667    3
X505    0.1 4
X506    0.1222222222    9
X507    0.15    2
X508    0.2 3
X509    0.1333333333    15
X510    0.14    5
X511    0.1 1
X512    0.4 1
X513    0.2125  8
X514    0.36    5
X515    0.34    5
X516    0.4 1
X517    0.1428571429    7
X518    0.3333333333    3
X519    0.1 3
X520    0.2277777778    18
X521    0.1916666667    12
X522    0.2 4
X523    0.1857142857    7
X524    0.1 2
X525    0.1 5
X526    0.2222222222    9
X527    0.1818181818    11
X528    0.2151515152    33
X529    0.1 3
X530    0.1214285714    14
X531    0.2 1
X532    0.1 2
X533    0.1 3
X534    0.1166666667    12
X535    0.1 2
X536    0.1 2
X537    0.1 1
X538    0.2379310345    29
X539    0.175   4
X540    0.1363636364    11
X541    0.1 1
X542    0.1479166667    48
X543    0.1928571429    28
X544    0.4 1
X545    0.1951219512    41
X546    0.1333333333    3
X547    0.15    4
X548    0.2833333333    6
X549    0.1547619048    42
X550    0.1555555556    9
X551    0.2363636364    11
X552    0.2142857143    7
X553    0.5 1
X554    0.15    4
X555    0.1709677419    31
X556    0.17    10
X557    0.1 2
X558    0.2866666667    15
X559    0.4 2
X560    0.15    2
X561    0.1424242424    66
X562    0.25    2
X563    0.1 3
X564    0.1285714286    7
X565    0.12    5
X566    0.25    4
X567    0.2263157895    19
X568    0.1 12
X569    0.1666666667    6
X570    0.5 1
X571    0.147826087 23
X572    0.1 1
X573    0.1818181818    11
X574    0.2 2
X575    0.15    2
X576    0.2 3
X577    0.16    15
X578    0.1621621622    37
X579    0.1333333333    3
X580    0.1333333333    12
X581    0.18    5
X582    0.1534482759    58
X583    0.1538461538    26
X584    0.1 9
X585    0.2142857143    7
X586    0.1 1
X587    0.1222222222    9
X588    0.1 1
X589    0.1 3
X590    0.1 6
X591    0.15    2
X592    0.1 2
X593    0.3 1
X594    0.1285714286    21
X595    0.2 2
X596    0.12    5
X597    0.1 1
X598    0.1 1
X599    0.1 2
X600    0.1153846154    13
X601    0.1 15
X602    0.1 1
X603    0.1 1
X604    0.1 4
X605    0.15    10
X606    0.15    4
X607    0.15    4
X608    0.2 1
X609    0.14    5
X610    0.2 1
X611    0.1 2
X612    0.1 3
X613    0.125   4
X614    0.172   25
X615    0.2 4
X616    0.1727272727    11
X617    0.2090909091    22
X618    0.1333333333    3
X619    0.1 7
X620    0.15    4
X621    0.1181818182    11
X622    0.1375  8
X623    0.1666666667    3
X624    0.1 3
X625    0.1090909091    11
X626    0.125   8
X627    0.1 2
X628    0.12    5
X629    0.1 8
X630    0.13    40
X631    0.1666666667    3
X632    0.34    5
X633    0.1714285714    7
X634    0.1636363636    11
X635    0.1 1
X636    0.1 1
X637    0.18125 16
X638    0.2 4
X639    0.2 8
X640    0.1 2
X641    0.1 1
X642    0.1166666667    6
X643    0.2 1
X644    0.6 1
X645    0.2666666667    9
X646    0.2666666667    3
X647    0.2 2
X648    0.1 2
X649    0.1 1
X650    0.1 2
X651    0.1 1
X652    0.125   4
X653    0.15    2
X654    0.1 1
X655    0.1 1
X656    0.35    4
X657    0.2666666667    3
X658    0.1 2
X659    0.1 1
X660    0.2 1
X661    0.1 2
X662    0.1 2
X663    0.1333333333    3
X664    0.1 2
X665    0.1 1
X666    0.225   4
X667    0.1666666667    6
X668    0.1 2
X669    0.1 3
X670    0.175   4
X671    0.1 3
X672    0.15    4
X673    0.1666666667    3
X674    0.1 3
X675    0.175   4
X676    0.25    8
X677    0.25    4
X678    0.2571428571    7
X679    0.1 1
X680    0.2571428571    7
X681    0.208   25
X682    0.325   12
X683    0.1 1
X684    0.25    2
X685    0.1 2
X686    0.3047619048    21
X687    0.24    5
X688    0.15    6
X689    0.1333333333    6
X690    0.3 1
X691    0.1 1
X692    0.15    2
X693    0.23    20
X694    0.2 2
X695    0.1666666667    6
X696    0.1342857143    35
X697    0.25    6
X698    0.2 8
X699    0.2 5
X700    0.5 1
X701    0.1333333333    6
X702    0.3 1
X703    0.15    2
X704    0.15    2
X705    0.1833333333    6
X706    0.15    6
X707    0.1493506494    77
X708    0.36    5
X709    0.3 2
X710    0.15    2
X711    0.38    5
X712    0.2666666667    3
X713    0.25    4
X714    0.225   4
X715    0.5 1
X716    0.1 2
X717    0.16    5
X718    0.3 2
X719    0.3538461538    13
X720    0.1 2
X721    0.175   4
X722    0.22    5
X723    0.175   4
X724    0.2333333333    6
X725    0.34    5
X726    0.2 7
X727    0.1 1
X728    0.3 3
X729    0.1 1
X730    0.1 3
X731    0.3 5
X732    0.35    6
X733    0.2875  8
X734    0.1 1
X735    0.1 2
X736    0.2 5
X737    0.1714285714    7
X738    0.375   4
X739    0.1 4
X740    0.3 1
X741    0.1 1
X742    0.1142857143    7
X743    0.1 1
X744    0.2285714286    7
X745    0.14    5
X746    0.15    6
X747    0.1 1
X748    0.125   4
X749    0.1666666667    6
X750    0.125   8
X751    0.1 1
X752    0.15    2
X753    0.2 1
X754    0.225   4
X755    0.3 1
X756    0.3 5
X757    0.175   4
X758    0.1 3
X759    0.1333333333    18
X760    0.1230769231    13
X761    0.2 1
X762    0.11    10
X763    0.1666666667    6
X764    0.1 1
X765    0.2090909091    11
X766    0.145   20
X767    0.14    5
X768    0.2375  8
X769    0.1571428571    7
X770    0.1 1
X771    0.1 2
X772    0.2 2
X773    0.16    5
X774    0.2 1
X775    0.1777777778    9
X776    0.1210526316    19
X777    0.2 1
X778    0.225   12
X779    0.1666666667    3
X780    0.1 6
X781    0.2333333333    6
X782    0.1692307692    13
X783    0.19    10
X784    0.2 3
X785    0.1489361702    47
X786    0.2 5
X787    0.45    2
X788    0.1666666667    6
X789    0.18    5
X790    0.3 1
X791    0.2 2
X792    0.11    10
X793    0.3333333333    3
X794    0.25    2
X795    0.2 1
X796    0.25    2
X797    0.2 2
X798    0.2 1
X799    0.1 3
X800    0.1333333333    18
X801    0.1473684211    19
X802    0.2 5
X803    0.14    5
X804    0.125   4
X805    0.1583333333    12
X806    0.1857142857    7
X807    0.1 1
X808    0.2 1
X809    0.1769230769    26
X810    0.1 1
X811    0.1 2
X812    0.1833333333    6
X813    0.1409090909    22
X814    0.1416666667    24
X815    0.1307692308    13
X816    0.1235294118    17
X817    0.1 1
X818    0.1 1
X819    0.18    30
X820    0.2514285714    35
X821    0.18    5
X822    0.2 4
X823    0.1 1
X824    0.2333333333    9
X825    0.1222222222    9
X826    0.15    2
X827    0.14    5
X828    0.1588235294    51
X829    0.15    2
X830    0.2 4
X831    0.1 2
X832    0.1391304348    23
X833    0.18    20
X834    0.15    2
X835    0.3 1
X836    0.1 8
X837    0.1666666667    9
X838    0.1954545455    22
X839    0.225   16
X840    0.1222222222    9
X841    0.1210526316    19
X842    0.1 2
X843    0.1 2
X844    0.125   4
X845    0.1 4
X846    0.1 1
X847    0.2 2
X848    0.275   4
X849    0.1 3
X850    0.2833333333    6
X851    0.175   4
X852    0.32    5
X853    0.1 1
X854    0.1428571429    7
X855    0.2277777778    18
X856    0.15    8
X857    0.12    5
X858    0.1 2
X859    0.175   4
X860    0.18    5
X861    0.16    5
X862    0.2333333333    6
X863    0.1 1
X864    0.3333333333    3
X865    0.1 2
X866    0.15    12
X867    0.1636363636    11
X868    0.4 1
X869    0.4 1
X870    0.1 3
X871    0.1555555556    9
X872    0.2 1
X873    0.3 1
X874    0.2 2
X875    0.15    12
X876    0.1 1
X877    0.1181818182    11
X878    0.1428571429    7
X879    0.1461538462    13
X880    0.3076923077    13
X881    0.2 2
X882    0.3 1
X883    0.205   20
X884    0.2 5
X885    0.1333333333    3
X886    0.15    2
X887    0.25    2
X888    0.15    4
X889    0.3 1
X890    0.125   4
X891    0.1875  8
X892    0.1428571429    7
X893    0.2333333333    3
X894    0.1 2
X895    0.1 1
X896    0.35    6
X897    0.1444444444    9
X898    0.2 2
X899    0.3 1
X900    0.1 2
X901    0.1 1
X902    0.25    2
X903    0.1 1
X904    0.1 1
X905    0.7 1
X906    0.2 1
X907    0.45    4
X908    0.25    2
X909    0.15    4
X910    0.1 2
X911    0.4 13
X912    0.1 2
X913    0.1842105263    19
X914    0.1 1
X915    0.1333333333    3
X916    0.2 2
X917    0.1 7
X918    0.1 1
X919    0.225   4
X920    0.2 1
X921    0.2 3
X922    0.18    5
X923    0.1 1
X924    0.1875  8
X925    0.2833333333    6
X926    0.5 3
X927    0.2 1
X928    0.1 1
X929    0.1 2
X930    0.2 3
X931    0.4 1
X932    0.2875  16
X933    0.1857142857    7
X934    0.1 1
X935    0.2 2
X936    0.1 1
X937    0.2 13
X938    0.2444444444    9
X939    0.1 1
X940    0.1714285714    7
X941    0.3 1
X942    0.1 1
X943    0.2857142857    7
X944    0.15    2
X945    0.1 1
X946    0.15625 16
X947    0.1666666667    3
X948    0.3 1
X949    0.2 2
X950    0.1 8
X951    0.1 1
X952    0.1 3
X953    0.3 1
X954    0.3 1
X955    0.1 3
X956    0.1125  8
X957    0.18    5
X958    0.2666666667    3
X959    0.2 1
X960    0.125   4
X961    0.1333333333    3
X962    0.2444444444    9
X963    0.25    10
X964    0.25    4
X965    0.2 1
X966    0.225   4
X967    0.1625  8
X968    0.1333333333    3
X969    0.1333333333    3
X970    0.1 1
X971    0.2 7
X972    0.3 10
X973    0.1 1
X974    0.3 2
X975    0.225   4
X976    0.1 1
X977    0.1 2
X978    0.4 1
X979    0.1333333333    3
X980    0.1333333333    9
X981    0.13125 16
X982    0.1 1
X983    0.2 1
X984    0.1782608696    23
X985    0.2225806452    31
X986    0.15    4
X987    0.1 3
X988    0.1 3
X989    0.15    4
X990    0.2285714286    14
X991    0.2384615385    26
X992    0.4 1
X993    0.4 2
X994    0.1 1
X995    0.1 1
X996    0.1666666667    3
X997    0.1 6
X998    0.13    20
X999    0.2666666667    3
</code></pre>

<p>Code I am using:</p>

<pre><code>Asianpig &lt;- NULL; Asianpig$x &lt;- (Asianpig_data$total)
Asianpig$y &lt;- (Asianpig_data$mean)
plot(Asianpig)

#increase maxiterations for nls
nlc &lt;- nls.control(maxiter = 21811)

# fit first a nonlinear least-square regression
Dat.nls &lt;- nls(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, control = nlc); Dat.nls
lines(1:8000, predict(Dat.nls, newdata=list(x=1:8000)), col=1)

# and finally ""external envelopes"" holding 95 percent of the data
Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, tau=0.025, trace=TRUE)
lines(1:8000, predict(Dat.nlrq, newdata=list(x=1:8000)), col=4)

Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, tau=0.975, trace=TRUE)
lines(1:8000, predict(Dat.nlrq, newdata=list(x=1:8000)), col=4)
</code></pre>

<p>How this looks: </p>

<p><a href=""http://i.stack.imgur.com/tF8Vu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tF8Vu.png"" alt=""enter image description here""></a></p>

<p>I was expecting the quantile regression line to more dynamically follow the slope of the datapoints. 
I adapted the code from an example that was using <code>SSlogis()</code> for the input data:</p>

<pre><code># build artificial data with multiplicative error
Dat &lt;- NULL; Dat$x &lt;- rep(1:25, 20)
    set.seed(1)
    Dat$y &lt;- SSlogis(Dat$x, 10, 12, 2)*rnorm(500, 1, 0.1)
plot(Dat)
</code></pre>

<p>I have a feeling I should not be using <code>SSlogis()</code> in my code, but instead should be modelling an exponential distribution. SSlogis is a selfStart model evaluates the logistic function and its gradient. It has an initial attribute that creates initial estimates of the parameters Asym, xmid, and scale.</p>

<p>But I am still trying to understand how to fit a quantile regression for this non-linear data.</p>

<p>Here is a hexbin plot that gives a feeling for how the data is clustered:<a href=""http://i.stack.imgur.com/NCrLX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NCrLX.png"" alt=""enter image description here""></a></p>
"
"0.135927713229264","0.138781845413342","177823","<p><strong>I need to perform manually two-stage Least Squares(to illustrate its advantages)</strong>, where the first stage is <em>repeated median estimate</em> and the second stage should be weighted least squares, where weights are obtained(as far, as I understand) from polynomial regression of first-stage residuals on regressors.</p>

<p>Suppose I have generated the following heteroscedastic model:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=Y_i%20%3D%20b_0%2Bb_1%20X_i%20%2B%20%5Cepsilon_i"" alt=""Y_i = b_0+b_1 X_i + \epsilon_i""></p>

<p>where error depends on regressor:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Cepsilon_i%20%5Csim%20N[0%2C[X_i-1]%5E2]"" alt=""\epsilon_i \sim N(0,(X_i-1)^2)""></p>

<pre><code>set.seed(100)
b&lt;-c(12,7.25) ## my coefficients
num&lt;-50 ## number of observations

raw_x&lt;-runif(num,min=0,max=2) ## regressors

my_y&lt;-as.vector(b%*%t(data.frame(rep(1,num),raw_x))+
     rnorm(num,mean=0,sd=(raw_x-1)^2)) ## observations

l&lt;-lm(my_y~raw_x) ## let's create linear model

plot(fitted(l),residuals(l)) ## we see heteroskedasticity
## we got to higher values, our residuals explode

abline(0,0)
title(""Residual vs Fit. value"");
</code></pre>

<p><a href=""http://i.stack.imgur.com/4S6pY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4S6pY.png"" alt=""Residual vs fit value""></a></p>

<p>So I perform repeated median regression (formula in the <a href=""http://www.cs.huji.ac.il/~werman/Papers/rep.pdf"" rel=""nofollow"">Introduction</a>):</p>

<pre><code>## Generating first model using repeated median

## slope 

fij = function(i,j)
{
  (my_y[i]-my_y[j])/(raw_x[i]-raw_x[j])
}

bij&lt;-outer(1:num,1:num,fij) ##NaN's were produced on the diagonal    

rowmeds &lt;- apply(bij, 1, median,na.rm=TRUE)
b_med&lt;-median(rowmeds)

# colmeds &lt;- apply(bij, 2, median,na.rm=TRUE) ## column medians are the same
# b_med3&lt;-median(colmeds)

## Intercept    

## med(y_i - b*x_i)

a_med&lt;-median(my_y-b_med*raw_x)
</code></pre>

<p><strong>The fit is extremely accurate!</strong> In this example <code>a_med</code> is 11.97634 and <code>b_med</code> equals 7.27022.</p>

<p>Now I perform 2nd order polynomial regression of residuals:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7B%5Cepsilon_i%7D%3Da_0%20%2B%20a_1X_i%2Ba_2X_i%5E2%2B%5Cdelta_i"" alt=""\hat{\epsilon_i}=a_0 + a_1X_i+a_2X_i^2+\delta_i""></p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7B%5Cepsilon%7D%20%3D%20%5Cbegin%7Bpmatrix%7D%201%20%26%20X_1%20%26%20X_1%5E2%20%5C%5C%20%5Cvdots%20%5C%5C%201%20%26%20X_m%20%26%20X_m%5E2%20%5Cend%7Bpmatrix%7D%5Cbegin%7Bpmatrix%7Da_0%20%5C%5C%20a_1%20%5C%5C%20a_2%20%5Cend%7Bpmatrix%7D%2B%5Cdelta"" alt=""\hat{\epsilon} = \begin{pmatrix} 1 &amp; X_1 &amp; X_1^2 \\ \vdots \\ 1 &amp; X_m &amp; X_m^2 \end{pmatrix}\begin{pmatrix}a_0 \\ a_1 \\ a_2 \end{pmatrix}+\delta""></p>

<p>so that (<strong>X</strong> here is m x 3 matrix):</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7Ba%7D%20%3D%20%5BX%5ETX%5D%5E%7B-1%7DX%5ET%5Chat%7B%5Cepsilon%7D"" alt=""\hat{a} = [X^TX]^{-1}X^T\hat{\epsilon}""></p>

<p>I was told that as long as residual variances can be roughly estimated from only one observation, actual fit from this model can be used; residual variances = coefficients for the weighted least squares:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7B%5Chat%7B%5Csigma%7D%7D%3DX%5Chat%7Ba%7D"" alt=""\hat{\hat{\sigma}}=X\hat{a}""></p>

<pre><code>## Obtaining 2nd order polynomial estimator for residuals

Xmatr = t(rbind(rep(1,num),raw_x,(raw_x)^2))

## coef_var = (X^T*X)^(-1)*X^T^e
coef_var&lt;-solve(t(Xmatr)%*%Xmatr)%*%t(Xmatr)%*%t(my_y-c(a_med,b_med)%*%rbind(rep(1,num),raw_x))

## Obtaining sigma (residual deviation) esitmate

my_sigma&lt;-t(Xmatr%*%coef_var)

## performing regression with sigma-weights

b_wls&lt;-lm(as.numeric(my_y)~raw_x,weights=as.numeric(my_sigma^2))$coef

## final plot

library(scales)
plot(raw_x,my_y, pch=20,col=alpha(""salmon"",0.6))

abline(b[1],b[2], col=""black"") ## real line
abline(a_med,b_med,col=""blue"") ## repeated median fit
abline(b_wls, col=""magenta"")
legend('bottomright', c(""Real"",""Repeat median"",""Two-Level LS"") , 
       lty=1, col=c('black', 'blue','magenta'), bty='n', cex=.75)
</code></pre>

<p>The resulting fit is always worse (and sometimes turns into complete garbage). <strong>Please, can you explain me what I'm doing wrong? I need to obtain the result where two-level LS is better than repeated median fit(provided the error depends on regressors as shown before)</strong>.</p>

<p><a href=""http://i.stack.imgur.com/Wo0ZM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Wo0ZM.png"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/huTRw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/huTRw.png"" alt=""enter image description here""></a></p>

<p>EDIT: Using R function lm seems to produce the same picture:</p>

<pre><code>Xmatr = t(rbind(rep(1,num),raw_x,(raw_x)^2))
residual&lt;-as.vector(my_y-c(a_med,b_med)%*%rbind(rep(1,num),raw_x))
polycoef&lt;-lm(residual ~ poly(raw_x, 2, raw=TRUE))$coefficients
my_sigma&lt;-t(Xmatr%*%polycoef)
</code></pre>

<p>EDIT2: Checking the quality of the residual fit (as far as I understand what's going on):</p>

<pre><code>plot(raw_x,abs(residual))
lines(sort(raw_x),(sort(raw_x)-1)^2) ## real residuals
lines(sort(raw_x), my_sigma[order(raw_x)],col = ""magenta"") ## fitted residuals
legend('topleft', c(""Real res"",""Fitted res"") , 
       lty=1, col=c('black','magenta'), bty='n', cex=.75)
</code></pre>

<p><a href=""http://i.stack.imgur.com/3J5D3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3J5D3.png"" alt=""enter image description here""></a></p>

<p>EDIT3: As long as my standard deviation is $(X_i-1)^2$, so the variance has power 4 and maybe I should do for the residual fit</p>

<pre><code>residual&lt;-abs(as.vector(my_y-c(a_med,b_med)%*%rbind(rep(1,num),raw_x)))
</code></pre>

<p>this makes residual fit by the quadratic function better, but the regression line moves even more far than before.
<a href=""http://i.stack.imgur.com/nt1Ij.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nt1Ij.png"" alt=""enter image description here""></a></p>
"
"0.0693653206906364","0.0679889413649005","177987","<p>Good afternoon,
I need help in intpretting the significance results when performing a linear regression with a categorical interaction.  I'm running this analysis in R.</p>

<p>The question is whether I can use qsec as an explantory variable in the below model given that one of the categorical variables is not significantly different to the other.</p>

<p>Model &amp; Outputs</p>

<pre><code>lm(mpg~qsec+am+qsec*am, data=mtcars)

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  -9.0099     8.2179  -1.096  0.28226   
qsec          1.4385     0.4500   3.197  0.00343 **
am1         -14.5107    12.4812  -1.163  0.25481   
qsec:am1      1.3214     0.7017   1.883  0.07012 . 
</code></pre>

<p>The second coefficient is not statistical significant - though only just with a p value of 0.07.
Does this mean that I should look for another explanatory variable?</p>

<p>When I run the regression on one of the values - am1 - alone, then it is still significantly different to zero.  As is am0 from the above results.</p>

<p>When I look at the data, the slope appears significantly different so I think I can use it however the understanding of the theory says no - which is frustrating.  I would add my plot to highlight my point but I'm struggling to load it.</p>

<p>I hope this isn't a stupid question to ask...I've found some good explanations of categorical variables in general, however nothing that tells me how I should interpret this secondary variable.</p>

<p>James</p>

<p>** edit changed data to be qsec per question below</p>
"
"NaN","NaN","178071","<p>so for my question it asks me to test if the intercept for a linear regression model is greater than 9 in R. I'm not exactly sure how to do that. any ideas? I tried the t-test manual but it's not working for me. </p>
"
"0.0749231094763201","0.0734364498908627","178160","<p>I have a run a linear mixed effects model in R to model clinical data. However, this model is heteroscedastic (as there excess zeros in the response variable).</p>

<p>I have tried transforming the data (log transform) and (sqrt). Still, neither transformation resolves the issue (see residual versus fitted value plot). I have not used Cox proportional hazards model as the data is not time-to-event data, the data measures force and there are a large number of observations have a reading of zero. I cannot exclude these readings as they are valid.</p>

<p>I have found an R package that runs Tobit regression (AER). Nevertheless, this will not accommodate the random effects in the model.
I cannot find any R packages that run Weibull mixed effects models (or gamma mixed effects models)... </p>

<p>Does anyone know if there is a package to run these type of models? (or can they suggest any alternative approach). </p>

<p>Many thanks</p>

<p>Etn</p>
"
"0.0400480865731637","0.039253433598943","178657","<p>I have a data set; sample size is 16, the number of  independent variable is 18 
and one dependent variable . there are correlations between independent variables. I want to conduct Monte Carlo according the linear regression 
relationship of these variables but some of independent variables do not follow  known distributions. in this regard, can anyone suggest a way ?</p>

<p>Thank you in advance</p>

<p>(I read this article <a href=""http://www.iaees.org/publications/journals/ces/articles/2011-1%284%29/a-fitter-use-of-Monte-Carlo-simulations.pdf"" rel=""nofollow"">article</a>, but I have no experience with simulation)</p>
"
"0.0800961731463273","0.078506867197886","178974","<p>I have a dataset with 100 predictor variables (95 continuous, and 5 categorical) and 1 target variable (continuous). After plotting the density plots of the continuous predictor variables, they are all normally distributed. </p>

<p>My goal is to build a linear regression model, as a start, and use the 100 predictor variables to predict the target variable. </p>

<ol>
<li>How do I know if I need to ""normalize"" my data (the predictor variables that are continuous)?</li>
<li>If I determine that I need to normalize my predictor variables, do I also need to normalize my target variable? </li>
<li>How do I determine which method of normalization is appropriate? Is this a local (per variable) decision or global (one normalization approach for all variables)? </li>
</ol>

<p>I am using R, if there are any packages that can help, please let me know. </p>

<p>I am not sure if I should make the decision to normalize values before or after the regression model is built. For example, I could forgo data normalization, build the model, and cross-validate it, and if I don't like the results, repeat the process by tinkering with data normalization. To me, this approach would seem like fudging the process until I get a reasonable result. </p>
"
"0.0863948355424908","0.100077011948264","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"0.0853828074607","0.0836886016271203","179498","<p>I'm working on a project with R and I don't think I'm using the appropriate linear regression or plot, I've made both but they don't seem to match.  The study is an ANOVA comparing $CO_2$ emissions per capita with 5 groups of income levels and a relevant linear regression.  For the linear regression I want use $CO_2$ as the dependent variable and $GDP$ as the independent variable and the 5 $income$ levels as dummy variables.</p>

<p>Begin by ordering the variables and remove the intercept:</p>

<pre><code>income_factor = factor(Data01$income, levels=c(""Low income"", 
""Lower middle income"", ""Upper middle income"", ""High income: OECD"", ""High
income: nonOECD"")) 

lm.r = lm(CO2 ~ income_factor -1, data=Data01)
</code></pre>

<p>Gives</p>

<pre><code>summary(lm.r)
Coefficients:
                              Estimate Std. Error t value Pr(&gt;|t|)    
income_factorLow income             0.2318     0.6943   0.334  0.73902    
income_factorLower middle income    1.7727     0.6355   2.789  0.00603 ** 
income_factorUpper middle income    4.7685     0.6271   7.604 4.12e-12 ***
income_factorHigh income: OECD      8.7926     0.7305  12.036  &lt; 2e-16 ***
income_factorHigh income: nonOECD  19.4642     1.3667  14.242  &lt; 2e-16 ***
</code></pre>

<p>So that we may write the linear regression in the form:</p>

<p>$$ CO_2 = \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + \beta_5 X_5 $$</p>

<p>Where $X_i$ is a dummy variable 1 at the level of income and 0 otherwise</p>

<p>For the corresponding plot I used:</p>

<pre><code> plot &lt;- ggplot(data=Data01, aes(x=GDP, y=CO2, colour=factor(income)))
 plot + stat_smooth(method=lm, fullrange=FALSE) + geom_point()
</code></pre>

<p>Which gives the graph</p>

<p><a href=""http://i.stack.imgur.com/5Iw53.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5Iw53.png"" alt=""CO2 ~ GDP""></a></p>

<p>But here is my confusion, it looks like there is the <em>lm</em> term in the plot, but I don't think it is using the same values taken from the previous linear regression.  As Looking at summary from the linear regression, High income: OECD the estimate is 8.79, but the line for it is pretty much flat.</p>

<p>While I was typing this I realized that the graph has $GDP$ as the X-axis, but is not included in the linear regression.  Would multiplying by $income$_$factor*GDP$ help?</p>
"
"0.0400480865731637","0.0196267167994715","179803","<p>Can Theil-Sen be defined for multiple linear regression? If so, is there an implementation in R for it?</p>

<p>I simply want a formula <code>a~b+c</code>, but the package <code>mblm</code> fails with the (misleading) error message: <code>stop(""Only linear models are accepted"")</code>. Of course, this is a linear model: but they mean that Theil-Sen only works for two-dimensional linear models. </p>
"
"0.0800961731463273","0.078506867197886","179885","<p>I am analyzing dataset where I would like to try to predict which of the following measurements scales will best predict scores on likability scales that measure how much participants ""like"" specific pictures.</p>

<blockquote>
  <p><strong>Dependent Variable</strong> (measurements scales):   Scale: Likability of
  specific picture Factor: Pictures Types: Levels: A, B, C Factor:
  Pictures Gender: Levels: Female, Male Pictures: Male_A + Female_A +
  Male_B + Female_B + Male_C + Female_C</p>
  
  <p><strong>Independent Variable</strong>: ScaleX + ScaleY + ScaleZ + ScaleW</p>
</blockquote>

<p>What I am interested in is to answer how well will:
ScaleX + ScaleY + ScaleZ + ScaleW
Predict score in:
Male_A + Female_A + Male_B + Female_B + Male_C + Female_C</p>

<blockquote>
  <p><strong>Formula:</strong> Pictures(Male_A + Female_A + Male_B + Female_B + Male_C + Female_C)  ~ Scales (X,Y,Z,W)</p>
</blockquote>

<p>I could theoretically try to run several linear regression but this is really just an desperate attempt and it will not explain which of XYZW scales would be best to predict the Likability score. </p>

<p>My thoughts are that it could be Structural equation modeling with some more simple model without any latent variables, more about measurement. However I have not much experience and I do not wish to invest huge amount of time to learn it and then realize it is useless in this example.</p>

<p><strong>Question is:</strong>
What approach for analyzing this data should I use? I will welcome even more technical answers with <strong>R</strong> examples as this will be software of my choice.</p>

<p>Thank you kindly for answers,
Cheers! :)</p>
"
"0.0800961731463273","0.078506867197886","180178","<p>Using the package <code>mfp</code> in R and would like to plot the fit with 95% confidence intervals.</p>

<p>This seems straightforward with an <code>lm</code> object (<a href=""http://stackoverflow.com/questions/15180008/how-to-calculate-the-95-confidence-interval-for-the-slope-in-a-linear-regressio"">as in this question</a>, <a href=""http://stats.stackexchange.com/questions/135707/plotting-a-polynomial-regression-with-its-confidence-interval-of-95-in-r"">and this question</a>) and a <code>glm</code> object as shown in  <a href=""http://stackoverflow.com/questions/20620277/get-95-confidence-interval-with-glm-in-r"">this question</a>.  </p>

<p>However, the <code>mfp</code> is not recognized/structured as either <code>lm</code> or <code>glm</code>as the function <code>mfp</code> generates an <code>mfp</code> object.  <code>mfp</code> objects do use <code>predict.glm</code>, from which I can get the <code>se.fit</code>. </p>

<p><em>All that to say, that I want to confirm I'm using the appropriate formula to calculate CI around the fit as shown below:</em></p>

<p><a href=""http://i.stack.imgur.com/rQ6I3.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rQ6I3.gif"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/RYzou.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RYzou.gif"" alt=""enter image description here""></a></p>

<pre><code>somedat &lt;- data.frame(x=1:20, y=pexp(1:20, rate=1/3))
library(mfp)
mymfp &lt;- mfp(formula = y~fp(x), data = somedat)
mypred &lt;- predict(object = mymfp, type = 'response', se.fit = T)

plot(somedat$x, somedat$y, ylim=c(0,1.2))
lines(predict(mymfp,  type = 'response')) #fit
lines(mypred$fit-mypred$se.fit*1.96, col='orange') #lower
lines(mypred$fit+mypred$se.fit*1.96, col='orange') #upper
</code></pre>

<p><a href=""http://i.stack.imgur.com/xhpHG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xhpHG.png"" alt=""enter image description here""></a></p>

<p>Is the above an appropriate equation/solution for a 95% confidence interval around the fractional polynomial fit?</p>
"
"0.0506572678011219","0.0620651280774201","180483","<p>I am considering a Bayesian Gaussian spatial regression model</p>

<pre><code>y(s) = x(s) b + w(s) + e 
</code></pre>

<p>where </p>

<pre><code>w|theta ~ N(0,k2 H(phi))
e ~ N(0,tau2)
</code></pre>

<p>Assuming the range <code>phi</code> and the <code>tau2/sigma2</code> ratio fixed, a normal prior on the regression coefficients <code>b</code> and an inverse gamma on <code>sigma2</code> reduces the problem to a conjugate Bayesian linear regression problem. Hence no need for MCMC and I can use the <code>bayesGeostatExact</code> function.</p>

<p>My question is: if we assume instead that <code>phi</code> and the <code>tau2/sigma2</code> ratio are NOT fixed we need to specify priors for the spatial parameters as well. 
Assuming a uniform prior for <code>phi</code> and inverse gamma priors for <code>tau2</code> and <code>sigma2</code> ratio makes the conjugacy still hold?</p>

<p>I don't understand why in this case we need to use a MCMC simulation (through the <code>spLM</code> function) and there is no equivalent <code>bayesGeostatExact</code> function if the spatial parameters are not fixed.</p>

<p>Thanks</p>
"
"NaN","NaN","180580","<p>To understand my logistic regression fit and identify non linear effects, I plan to estimate the conditional density and then calculate the log odds comparing to log odds from logistic regression. To me  this is the equivalent of scatter plot of single  independent variable vs dependent and prediction. </p>

<p>A) Does this seem like the right approach? </p>

<p>B) I am using R, and I am surprised that there is no package already doing this? </p>
"
"0.0800961731463273","0.078506867197886","180660","<p>let's say mydata contains N rows  and 101 columns with label (y, x1, x2, ...x100).
One can do multiple linear regression like this:</p>

<pre><code>fit &lt;- lm(y ~ x1 + x2 + x3 ... x100, data = mydata)
</code></pre>

<p>There a shortcut for it,</p>

<pre><code>fit &lt;- lm(y ~ ., data = mydata)
</code></pre>

<p>where ""."" represents everything other than y. Now to add second order terms, you can use this shortcut:</p>

<pre><code>fit2 &lt;- lm(y ~ . + .^2, data = mydata)
</code></pre>

<p>This generates cross terms xi * xj for i != j, but it doesn't generate xi * xi. If anybody knows how to generate xi * xi terms please let me know. Now my question is, how do you predict using fit2, assuming you have test sample (x1, x2, ... x100)? Any shortcuts?</p>

<p>EDIT: I figured this out myself. One could use the following command to make higher order terms.</p>

<pre><code>mydata &lt;- data.frame(model.matrix(~(.-y)^2 , mydata))
</code></pre>
"
"0.0490486886395286","0.0480754414848157","180813","<p>I know linear regression is the workhorse of machine learning. I understand the internals of it and I am playing with some real data samples.</p>

<p>Obviously using a simple line (polynomial degree = 1) is not very useful for most of the datasets, my understanding is that as I increase the polynomial degree I will</p>

<ul>
<li>Get a more accurate prediction</li>
<li>Eventually will face the danger of overfiting</li>
</ul>

<p>Now, I have been playing with R and some datasets and this is what I got...</p>

<pre><code>stock &lt;- EuStockMarkets[, 'DAX']
plot(stock)
model &lt;- lm(stock ~ lm(poly(time(stock), 1, raw=TRUE)))
points(time(stock), predict(model), type=""l"", col=""blue"", lwd=2)
model10 &lt;- lm(stock ~ poly(time(stock), 10, raw=TRUE))
points(time(stock), predict(model10), type=""l"", col=""red"", lwd=2)
text(1994, 5000, paste(""Degree, blue=1, red=10""), pos=2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/0BLlF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0BLlF.png"" alt=""linear regression output""></a></p>

<p>Now, the red line is definately a muuuuch better fit thant the blue one, that said, two questions come to my mind</p>

<ol>
<li>The red line is almost always (if not always) ascending, meaning that technically any time would be good to buy shares, that does not represent the truth (from 1994 to 1995 there are ups and downs, not to mention 1997 to 1998)</li>
<li>Does R automatically apply regularization to prevent overfiting.</li>
</ol>

<p>I am fully aware that this is a very simplistic example (normally I would use more features, not just the date, in order to predict the price.</p>

<p>Would a more sophisticated tech such as neuronal network provide a better output here?</p>
"
"0.102102987459307","0.100077011948264","180854","<p>I am trying to test for cointegration between two series that based on qualitative reasoning, should be cointegrated. They are the prices of XLE ETF (<code>XLE US equity</code>) and 1st futures of Brent (<code>CO1 Comdty</code>). However, the results that I arrive at using two different methods both show that there exists no cointegration between the two series - not sure if my execution or the interpretation of the data is wrong? </p>

<p>(Both XLE and Brent 1st Futures have been tested for non-stationarity using ADF test from ""urca"" package)</p>

<p><strong>1st test - Engle Granger 2-step test:</strong><br>
In doing this, I referenced <em>Using R to Test Pairs of Securities for Cointegration</em> by Paul Teetor</p>

<p><strong>(1)</strong> Conducting Spread</p>

<pre><code>&gt; M&lt;-lm(XLE~Brent+0,data=XLE.Brent)
&gt; beta&lt;-coef(M)[1]
&gt; spread&lt;-XLE.Brent$XLE-beta*XLE.Brent$Brent
&gt; 
&gt; summary(M)

Call:
lm(formula = XLE ~ Brent + 0, data = XLE.Brent)

Residuals:
   Min      1Q  Median      3Q     Max 
-20.363  -9.543  -2.909  13.294  36.269 

Coefficients:
     Estimate Std. Error t value Pr(&gt;|t|)    
&gt;Brent  0.74962    0.02004    37.4   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 16.37 on 68 degrees of freedom
Multiple R-squared:  0.9536,    Adjusted R-squared:  0.953 
F-statistic:  1399 on 1 and 68 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>(2)</strong> Testing the stationarity of the spread using ADF test (from package ""urca""):</p>

<pre><code>&gt; spread.ADF&lt;-ur.df(spread,type=""none"",selectlags=""AIC"")
&gt; summary(spread.ADF)

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test #  
</code></pre>

#########################################

<pre><code>Test regression none 


Call:
lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)

Residuals:
   Min      1Q  Median      3Q     Max 
 -6.1449 -2.2523  0.5559  2.9194  8.4567 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
z.lag.1    -0.0003928  0.0266919  -0.015    0.988
z.diff.lag  0.1207084  0.1278700   0.944    0.349

Residual standard error: 3.443 on 65 degrees of freedom
Multiple R-squared:  0.01395,   Adjusted R-squared:  -0.01639 
F-statistic: 0.4596 on 2 and 65 DF,  p-value: 0.6335


Value of test-statistic is: -0.0147 

Critical values for test statistics: 
    1pct  5pct 10pct
tau1 -2.6 -1.95 -1.61
</code></pre>

<p>My interpretation: since $t$-value = <code>-0.0147</code> is bigger than <code>-1.61</code>, do not reject null. Spread is not stationary. Hence no cointegration between XLE and Brent.</p>

<p><strong>Second Test: Johansen Test</strong></p>

<pre><code>&gt; XLE.brent.coint&lt;-ca.jo(data.frame(XLE,Brent),type=""trace"",ecdet=""trend"",K=2,spec=""longrun"")
&gt; summary(XLE.brent.coint)
&gt;
&gt;###################### 
&gt;# Johansen-Procedure # 
&gt;###################### 
&gt;
&gt;Test type: trace statistic , with linear trend in cointegration 
&gt;
&gt;Eigenvalues (lambda):
&gt;[1] 8.179514e-02 6.025284e-02 2.775558e-17
&gt;
&gt;Values of teststatistic and critical values of test:
&gt;
&gt;        test 10pct  5pct  1pct
&gt;r &lt;= 1 | 4.16 10.49 12.25 16.26
&gt;r = 0  | 9.88 22.76 25.32 30.45
&gt;
&gt;Eigenvectors, normalised to first column:
&gt;(These are the cointegration relations)
&gt;
&gt;          XLE.l2   Brent.l2   trend.l2
&gt;XLE.l2   1.000000  1.0000000  1.0000000
&gt;Brent.l2 1.467806 -0.4346323  0.1610563
&gt;trend.l2 1.896366 -0.4903454 -0.8891875
&gt;
&gt;Weights W:
&gt;(This is the loading matrix)
&gt;
&gt;            XLE.l2    Brent.l2      trend.l2
&gt;XLE.d   -0.01629102 -0.13534537 -4.695795e-17
&gt;Brent.d -0.03819241 -0.03886418  5.127543e-17
</code></pre>

<p>My interpretation: Since $t$-value for <code>r=0: 9.88&lt;22.76</code>, do not reject null. Hence <code>r=0</code>, there exists no cointegration between <strong>XLE and Brent</strong>.</p>

<p>Additionally, I have carried out cointegration tests (both methods) on <strong>US 10 year and 2 year yields</strong>, and the results on both tell me that the series are not co-integrated, which does not make sense intuitively. Something must be wrong with the way I'm doing the tests!</p>

<p><a href=""http://i.stack.imgur.com/eApO4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eApO4.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/sf2HV.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sf2HV.jpg"" alt=""enter image description here""></a></p>
"
"0.0908205236199223","0.0964366217361766","181333","<p>I am analyzing count data with a lot of zeros and found that although the data do not fit a poisson glm, they fit the zero-inflated poisson (ZIP) regression significantly better than the standard poisson glm. </p>

<p>This analysis is for a BACI study, in which I have data before, during, and after the treatment, in three zones: control, on-trail (treatment1) and off-trail (treatment2). </p>

<p>I am interested in the difference in change of detection rate of a species between the on-trail (treatment) site vs. control site, before and after the treatment. I have performed contrast on this data to determine this difference with a simple linear regression model (using lm), but I'm unsure how to find this difference using the zero-inflated poisson model. </p>

<p>The results of my ZIP model are here (ZP = Zone+Phase combined into one variable; the ""After"" phase is called ""Open"" in the dataset)</p>

<pre><code>Call:
zeroinfl(formula = Deer ~ ZP | ZP, data = zinb)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.6756 -0.5180 -0.4137 -0.1243 14.8998 

Count model coefficients (poisson with log link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.076730   0.031034  34.695  &lt; 2e-16 ***
ZPCDuring    0.080611   0.055391   1.455    0.146    
ZPCOpen     -0.696793   0.092432  -7.538 4.76e-14 ***
ZPFBefore    0.062467   0.042638   1.465    0.143    
ZPFDuring    0.112727   0.067928   1.659    0.097 .  
ZPFOpen     -0.765391   0.080475  -9.511  &lt; 2e-16 ***
ZPTBefore   -0.008428   0.045729  -0.184    0.854    
ZPTDuring   -0.063361   0.063193  -1.003    0.316    
ZPTOpen     -0.717266   0.078422  -9.146  &lt; 2e-16 ***

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.40428    0.06617   6.110 9.97e-10 ***
ZPCDuring    0.85610    0.11076   7.729 1.08e-14 ***
ZPCOpen      0.79893    0.13448   5.941 2.84e-09 ***
ZPFBefore   -0.04894    0.09287  -0.527    0.598    
ZPFDuring    1.51339    0.13035  11.610  &lt; 2e-16 ***
ZPFOpen      0.20254    0.12932   1.566    0.117    
ZPTBefore   -0.08598    0.09830  -0.875    0.382    
ZPTDuring    0.98729    0.11720   8.424  &lt; 2e-16 ***
ZPTOpen      0.17416    0.12823   1.358    0.174    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 25 
Log-likelihood: -8572 on 18 Df
</code></pre>

<p>and the code i have been using to get the contrasts using the lm model is here (based on <a href=""http://people.stat.sfu.ca/~cschwarz/Stat-650/Notes/PDFbigbook-R/R-part013.pdf"" rel=""nofollow"">this tutorial</a>): </p>

<pre><code>result.lm &lt;- lm(Deer ~ Zone + PhaseBDO + Zone:PhaseBDO, data=counts)
options(contrasts=c(unordered=""contr.sum"", ordered=""contr.poly"")) 
result.lsmo.SP &lt;- lsmeans::lsmeans(result.lm, ~Zone:PhaseBDO)

contrast(result.lsmo.SP, list(bact=c(1, 0, -1, 0, 0, 0, -1, 0, 1)))
confint(contrast(result.lsmo.SP, list(bact=c(1, 0, -1, 0, 0, 0, -1, 0, 1))))
</code></pre>

<p>Can anyone suggest how I can calculate the contrast from my ZIP regression model to test for significant differences between the difference in detection rates between the two time periods (before/after) in the two zones (control/treatment)? </p>

<p>For example, I want to be able to say if there was a significantly larger increase (or decrease) in the detection rate of deer after the treatment was implemented, in the treatment zone compared with the control zone. </p>

<p>Thanks in advance for your suggestions.</p>
"
"0.0490486886395286","0.0480754414848157","181400","<p>I am working with mtcars dataset and using linear regression</p>

<pre><code>data(mtcars)
fit &lt;- lm(mpg ~.,mtcars)    
summary(fit)
</code></pre>

<p>When I fit the model with lm it shows the result like this</p>

<pre><code>Call:
lm(formula = mpg ~ ., data = mtcars)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.5087 -1.3584 -0.0948  0.7745  4.6251 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) 23.87913   20.06582   1.190   0.2525  
cyl6        -2.64870    3.04089  -0.871   0.3975  
cyl8        -0.33616    7.15954  -0.047   0.9632  
disp         0.03555    0.03190   1.114   0.2827  
hp          -0.07051    0.03943  -1.788   0.0939 .
drat         1.18283    2.48348   0.476   0.6407  
wt          -4.52978    2.53875  -1.784   0.0946 .
qsec         0.36784    0.93540   0.393   0.6997  
vs1          1.93085    2.87126   0.672   0.5115  
amManual     1.21212    3.21355   0.377   0.7113  
gear4        1.11435    3.79952   0.293   0.7733  
gear5        2.52840    3.73636   0.677   0.5089  
carb2       -0.97935    2.31797  -0.423   0.6787  
carb3        2.99964    4.29355   0.699   0.4955  
carb4        1.09142    4.44962   0.245   0.8096  
carb6        4.47757    6.38406   0.701   0.4938  
carb8        7.25041    8.36057   0.867   0.3995  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.833 on 15 degrees of freedom
Multiple R-squared:  0.8931,    Adjusted R-squared:  0.779 
F-statistic:  7.83 on 16 and 15 DF,  p-value: 0.000124
</code></pre>

<p>I found that none of variables are marked as significant at 0.05 significant level.</p>

<p>How to make variables significant at 0.05 significant level?</p>
"
"0.0749231094763201","0.0734364498908627","181603","<p>I am enrolled in a <a href=""http://www.youtube.com/playlist?list=PLA89DCFA6ADACE599"" rel=""nofollow"">machine learning course</a> for machine learning where we have a lab to implement linear regression
I am attempting to do it in R to get a better understanding of the material and of R for myself (i don't intend to submit this as a lab as the course doesn't use R) but am coming up against a wall</p>

<p>My understanding of the process is as follows</p>

<ul>
<li><p>User Generates a model based on the hypothesis
$h_\theta(x) = \theta^TX= \theta_0x_0 +\theta_1x_1+\dots$</p></li>
<li><p>Take error rate of your model by using squared error cost function, then iterate, create a new hypothesis and get the error rate of this. Continue through $n$ iterations based on the formula
$J(\theta_0,\theta_1)=\frac{1}{2m}\displaystyle\sum_1^m(h_\theta(x^{(i)})âˆ’y^{(i)})^2$. </p></li>
<li><p>Take all the error rates you have recorded based on the cost history and use <code>gradient descent</code> to find automatically the optimal values of your hypothesis.</p></li>
</ul>

<p>Using the code on <a href=""http://www.r-bloggers.com/linear-regression-by-gradient-descent/"" rel=""nofollow"">R-Bloggers</a> where the gradient descent is implement below based on vectors <code>x</code> and <code>y</code></p>

<pre><code># add a column of 1's for the intercept coefficient
X &lt;- cbind(1, matrix(x))

# gradient descent
for (i in 1:num_iters) {
 error &lt;- (X %*% theta - y)
 delta &lt;- (t(X) %*% error) / length(y)
 theta &lt;- theta - alpha * delta
 cost_history[i] &lt;- cost(X, y, theta)
 theta_history[[i]] &lt;- theta
}
</code></pre>

<p>I was wondering if people could help me tease out the logic</p>

<ol>
<li><p>Why is the number 1 applied to the matrix <code>X</code>. Is this so that X has 2 columns so that it can be multiplied by theta - y?</p></li>
<li><p>What is the formula delta actually calculating and why is the Transpose of X being used</p></li>
</ol>

<p>Conceptually I think i understand the overall process but i just need to relate this back to the R code as i want to grasp the concept before proceeding to Multiple linear regression</p>
"
"0.0942489115008991","0.0923787802599364","181665","<p>My question relates to calculating the uncertainty associated with the estimation of slopes in a varying intercept, varying slope hierarchical model.</p>

<p>I would like to calculate the effect of a treatment in different districts. If I ran a simple linear regression in which there was no pooling at district level, my model would look something like:</p>

<pre><code>    fm1 &lt;- lm(response ~ treatment*district)
</code></pre>

<p>I could then find the effect of treatment in district j by summing the coefficient for treatment with that of the interaction term treatment:districtj, and the standard error for that estimate could be found with:</p>

<pre><code>    sqrt(vcov(fm1)[2,2] + vcov(fm1)[4,4] + 2*vcov(fm1)[2,4])
</code></pre>

<p>I get stuck when moving to a multilevel model with random slopes along the lines of:</p>

<pre><code>    fm2 &lt;- lmer(response ~ treatment + (1 + treatment | district))
</code></pre>

<p>I can again find the effect of treatment in district j easily by summing the coefficient for treatment with the random slope estimate for district j. I would like to account for the uncertainty of both coefficients and I'm not sure how to do that. I can use the arm package to extract the standard errors associated with the slope estimates for each district and I know the standard error of the treatment estimate but I don't know how to estimate their correlation.  </p>

<p>It seems like this should be an easy question to answer but I haven't been able to find it. Hoping someone can point me in the right direction.</p>
"
"0","0.0277563690826684","181695","<p>In linear regression, if I have a model,</p>

<pre><code>b0 + b1x1 + b2x2 + b3x3 + b4x4 = y
</code></pre>

<p>and I want to fix some of the coefficients ,say b1 = 1 and b3 = 2, I could just do the following</p>

<pre><code>b0 + b2x2 + b4x4 = y - x1 - 2x3
</code></pre>

<p>and just fit a linear regression on the other three parameters on the new y. Is there a way to do this for logistic regression? The sigmoid function seems to complicate things. Im looking to do this in r, so if theres an easy way to do it in r, that would be very appreciated.</p>
"
"0.02831827358943","0.0277563690826684","181790","<p>I would like to know how I can go about examining the results of a partial least squares regression. Specifically, I am interested to know what the coefficient is for each component, and what the linear combination of variables within each component looks like.</p>

<p>Lets consider this as an example:</p>

<pre><code>library(AppliedPredictiveModeling)
library(plsr)
data(solubility)

trainingData &lt;- solTrainXtrans
trainingData$Solubility &lt;- solTrainY

plsFit &lt;- plsr(Solubility ~ ., data = trainingData, ncomp=8)
</code></pre>

<p>Furthermore, if somebody could also help me understand what the different attributes within the mvr object are I would be greatly appreciative.</p>

<pre><code>&gt; attributes(plsFit)
$names
 [1] ""coefficients""    ""scores""          ""loadings""        ""loading.weights""
 [5] ""Yscores""         ""Yloadings""       ""projection""      ""Xmeans""         
 [9] ""Ymeans""          ""fitted.values""   ""residuals""       ""Xvar""           
[13] ""Xtotvar""         ""fit.time""        ""ncomp""           ""method""         
[17] ""call""            ""terms""           ""model""          
</code></pre>

<p>How do coefficients, scores, loadings, and weights all relate to each other?</p>

<p>Thanks!</p>
"
"0.0424774103841449","0.0555127381653369","181862","<p>I'm trying to build robust linear regression model (lmrob from robustbase) using several (&lt; 15) features. I know that traditional stepwise algorithms aren't the best alternative since they are biased. Could I use this algorithm:</p>

<p>1) Add a variable to model. (If it is not significant on whole dataset should I remove it from the model?)</p>

<p>2) Run bootstrap to calculate list of RMSEs of different train and test sets for current model.</p>

<p>3) Compare medians of current and previous model using non-parametric test. (What is the best? Should I use corrections for multiple comparisons such as Holm method?)</p>

<p>I think the best way to select features is to run a LASSO regression but I don't know if robust LASSO regression exists... Is there a package in R for it?</p>

<p>Sorry, if questions are stupid - I'm newbie in statistics and data science</p>
"
"0.0400480865731637","0.039253433598943","182100","<p>I created an algorithm and I tested it against a current algorithm.</p>

<p>The results are in this form:</p>

<pre><code>Power   Processes   Method  Time(s)
1          3          1     19,94
1          4          1     20,04
1          5          1     20,06
1          6          1     19,95
1          7          1     20,1
1          8          1     20,03
1          3          0     30,3 
...
</code></pre>

<p>for each method where my method is ""1"" and the other message is represented by ""0"". 
Process indicates the available processing power (I only have 4 servers therefore 3,4 processes may run each on a single server, 5-8 servers have to share resources - not indicated in the example table)</p>

<p>I've made 10 replications each test.</p>

<p>I wanted to create a linear regression comparing both models in order to show that even with more processes my algorithm runs faster. But the graphs and statics I could generate with ANOVA didn't really help me?</p>

<p>Which methods do I have to use?
And how may I generate graphs explaining the differences between the regressions?</p>
"
"0.0566365471788599","0.0555127381653369","182595","<p>I'm trying to use sparse linear model for my data,input x(29*50),output y(29*1). In R, the package of <strong><em>glmnet</em></strong> can be used. </p>

<p>Firstly, cv.glmnet() choose lambda and coefficients(at min error), here with leave-one-out cv method,and then plot it. </p>

<pre><code>cv.fit = cv.glmnet(x,y,family=""gaussian"",nfolds=29)

plot(cv.fit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/PEEeb.png"" rel=""nofollow"">the plot of mse aganist log(lambda) in cv model</a></p>

<p>Next, print the coefficients</p>

<pre><code>coef(cv.fit,s=""lambda.min"")
</code></pre>

<blockquote>
  <p>51 x 1 sparse Matrix of class ""dgCMatrix""               </p>

<pre><code>              1
</code></pre>
  
  <p>(Intercept)   267.7241</p>
  
  <p>cluster_0  .<br>
  cluster_1     .<br>
  cluster_2     .<br>
  cluster_3     .<br>
  cluster_4     .<br>
  ...</p>
  
  <p>cluster_47    .<br>
  cluster_48    .<br>
  cluster_49    .  </p>
</blockquote>

<p>Finally, to measure the model's ability for prediction, accuracy is calculated(defined as 1 minus average absolute error divided by numeric range of y)</p>

<pre><code>py &lt;- predict(cv.fit,newx=x,s=""lambda.min"")
py
</code></pre>

<blockquote>
  <p>V1     267.7241</p>
  
  <p>V2     267.7241</p>
  
  <p>...</p>
  
  <p>v29    267.7241  </p>
</blockquote>

<pre><code>ave_abs_error &lt;- mean(abs(py-y))
n_range &lt;- max(y)-min(y)
acc &lt;- 1-ave_abs_error/n_range
acc
</code></pre>

<blockquote>
  <blockquote>
    <p>0.918365</p>
  </blockquote>
</blockquote>

<p>Although the acc(0.918365) is very high, there is a serious problem. As seen from the plot above, the lambda.min is very large(73.03439),and all coefficients  are zero(only with intercept value 267.7241), all predicted py are the same as intercept.
That's really weird! </p>

<p>I searched lots of threads in forum, here<a href=""http://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome"">http://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome</a> explains that there is no local min for too few observations and all coefficients were shrunk to zero with the shrinkage penalties.</p>

<p>Does anybody has other interpretations?</p>

<p>Thanks in advance!</p>
"
"0.0566365471788599","0.0555127381653369","182713","<p>I am trying to build a regression model using R where,</p>

<p><strong>reponse variable:</strong> </p>

<ul>
<li>price (continuous)</li>
</ul>

<p><strong>explanatory variables:</strong> </p>

<ul>
<li><p>Age,accumulated kilometers,weight,horsepower,cylinder 
volume, tax (continuous)</p></li>
<li><p>Metallic color,ABS,Airbag,CD_Player,Power steering, Metallic_Rim  (categorical - binary)</p></li>
<li><p>Color (Categorical - 5 levels)</p></li>
<li><p>Fuel Type (Categorical - 3 levels)</p></li>
</ul>

<p>How can I detect the variables that should be used in my linear model in one step.
I know that binary variables influence on response variable can be checked using T-test.
How can I check which all of these variables(continous, categorigal-binary, categorical-multiple levels) influnece my response variable price in one step ??</p>
"
"0.113469578623964","0.111218061863672","182905","<p>I am trying to create a regression comparing resampled data from 2 sampling periods using $n=6$ samples. 
I have tried a few different models (e.g., linear, log-transform, and exponential), and have received mixed results. </p>

<p>For example, the linear models look best graphically, but because I have so few data points, each point has a strong effect on the trend and therefore points with greater distance from the mean are resulting in the model having a negative adjusted $R^2$ value. </p>

<p>An exponential model has a much improved, and positive, adjusted $R^2$ value, but graphically doesn't make as much sense as the linear model. 
Essentially, the exponential model is reacting to the dispersion of points around the linear regression line and trying to make up for the residual error of more outlier-ish points. 
However, these points would almost certainly be within reasonable distance from the mean if more points were added.</p>

<p>So I have 2 questions:</p>

<ol>
<li>Do I go with what looks more sensible graphically, or just blindly believe the adjusted $R^2$?</li>
<li>Obviously, more data points would be beneficial, but what are my options if I can't supplement with more data?</li>
</ol>

<p><strong>Examples:</strong></p>

<p>3 variables demonstrating this issue. Solid color lines are model fits and dashed lines are 95% CI. Black = linear, red = log-trasnformed, green = exponential. x-axis = 1977, y-axis = 2015. Created in R using <code>lm()</code>.</p>

<p><a href=""http://i.stack.imgur.com/Evpzp.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Evpzp.jpg"" alt=""Example1""></a> 
<a href=""http://i.stack.imgur.com/7yJrx.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7yJrx.jpg"" alt=""Example2""></a>
<a href=""http://i.stack.imgur.com/fssgU.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fssgU.jpg"" alt=""Example3""></a></p>

<p>Update: </p>

<p>Background info:
I have 21 soil variables I resampled in 1977 &amp; 2015. Each of these variables had 6 sample plots shared b/w each sampling period. However, I have a number of 1977 samples &amp; 2015 samples that were only sampled once (in 1 year &amp; not the other). The soil processing was different b/w years, so the 2 years aren't directly comparable. I'm trying to 'correct' the non-resampled 1977 samples to match 2015 data (not sampled in 1977) by regressing my 6 resampled samples for each variable. I'm assuming soil has not changed in 40 years, so I'm not trying to compare soil change, but rather trying to lump all extant samples (resampled and not) for common analyses. </p>

<p><strong>My example data:</strong></p>

<pre><code>dput(soil.dat)
structure(list(plot.2015 = c(5L, 10L, 24L, 25L, 44L, 51L), Silt.2015 = c(28.88, 21.84, 22.89, 25.34, 16.96, 22.36), Sand.2015 = c(63.92, 73.67, 60.38, 69.03, 81.6, 70.18), BS.2015 = c(70, 60.4, 43.2, 41, 84.4, 54), Silt.1977 = c(44L, 38L, 40L, 42L, 50L, 36L), Sand.1977 = c(37L, 49L, 37L, 36L, 39L, 42L), BS.1977 = c(36.6, 26.48, 44.08, 36.6, 53.32, 44.08)), .Names = c(""plot.2015"", ""Silt.2015"", ""Sand.2015"", ""BS.2015"", ""Silt.1977"", ""Sand.1977"", ""BS.1977""), class = ""data.frame"", row.names = c(1L, 2L, 15L, 16L, 18L, 19L))
</code></pre>
"
"0.0400480865731637","0.039253433598943","183451","<p>I have yearly rainfall projections from five climate models for the period of 2010 through 2099.</p>

<p>This is a sample of my data:</p>

<pre><code>df=structure(list(Year = c(2010, 2011, 2012, 2013, 2014, 2015, 2016, 
2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 
2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 
2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 
2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 
2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 
2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 
2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 
2094, 2095, 2096, 2097, 2098, 2099), CanESM2 = c(1163.46560706632, 
1045.27764563553, 1192.99035592859, 1039.18159594737, 1069.85056057463, 
1109.61718257189, 1080.22225446686, 996.465673784495, 1330.31482267773, 
1135.09951956191, 1036.5620174695, 1171.19645849667, 1230.07980354375, 
1224.51936031341, 1059.0667847652, 1119.61709915399, 1093.01684435802, 
1123.72623649933, 1088.75970830321, 1096.55713940808, 1136.73460669118, 
1225.56589926383, 1230.47636335948, 971.878201373981, 1077.94938659653, 
1067.66509425384, 1278.93601510725, 1132.26048108291, 1172.28768446317, 
1152.31800688181, 1226.66419877102, 1284.17386265492, 1065.46324990181, 
1126.16088253523, 1199.68965697911, 1048.76918572934, 1067.22448843151, 
1233.49853962937, 1125.31148343304, 1110.58047100806, 1096.19567721952, 
1277.141766876, 1024.98721582214, 1065.74405206603, 1067.80045192583, 
1115.30598493354, 1189.9948818424, 1437.13931627192, 1157.00834795935, 
976.858312813719, 1116.52499339511, 1111.09759483922, 1199.08438212517, 
1071.78135303469, 1007.25547265899, 1071.39018135202, 1089.81301384961, 
1312.96135982164, 1119.14850815955, 1128.62351901672, 1085.83469993455, 
1490.14291868226, 1082.82332451136, 1094.93028550704, 1138.81957874767, 
1097.4652709511, 1106.83810592802, 1229.91613882742, 1133.7894640904, 
1138.07334287779, 1281.84520786833, 1044.82250530464, 1187.35200775616, 
1156.74028624375, 1128.97358661251, 1168.80857077186, 1029.65489771463, 
1024.77267482949, 1085.6743046951, 1400.35428529776, 1165.82818082067, 
1117.43356446918, 1158.87599998443, 1188.6860869167, 985.854151901199, 
1147.12318390124, 1077.81579069868, 1154.53800683884, 1129.23840124991, 
1230.68311935564), `GFDL-ESM2M` = c(1110.40656291483, 1165.52810385097, 
986.538514416152, 1095.38727970136, 908.291964523058, 1015.48130275974, 
910.223751648018, 1037.17545203031, 1064.46407778575, 821.925055711163, 
1096.30232777925, 1145.56041166767, 994.615426183943, 1017.5660122588, 
1019.44059497268, 1074.6107636677, 1199.64822769138, 965.649952638057, 
947.58734190787, 946.097731390942, 1001.45702762066, 1115.81807500611, 
1037.58905387885, 917.925884051693, 974.408569840561, 1065.98187555499, 
988.032759990321, 1156.4922081412, 1053.23214676772, 868.078070630601, 
1117.82655721168, 1129.78430893212, 1078.10041660786, 792.460976651464, 
913.253156177221, 919.321633035608, 1040.56072059552, 1090.87082960431, 
1071.27625924979, 1033.46091240528, 883.895086023128, 939.489773000334, 
1102.2485093512, 779.851230298964, 1100.51135836672, 966.685826958901, 
961.744760284696, 989.753864405085, 826.629851232945, 909.284738712619, 
873.476834043498, 940.715939174158, 1005.83652099677, 1146.69688740834, 
978.082249009606, 1075.71089568088, 1101.2751541009, 962.851445193924, 
1024.69987402044, 1114.93474064477, 1024.47604232346, 1056.49825285734, 
975.492763800606, 1101.02607222684, 920.749421732859, 903.596908739447, 
912.489759611543, 987.64092666866, 808.10762021923, 973.408506903507, 
969.489829593018, 968.373783576046, 993.01049783438, 1080.98673871791, 
869.923091297771, 934.285784415424, 956.3081757549, 1013.38311159891, 
856.448744053944, 874.516625599295, 1061.51117387629, 1128.54448558007, 
1009.21257737855, 1017.16456465614, 833.802010570892, 927.740880449226, 
1001.38592283682, 952.145679780795, 996.893937239244, 931.957115223409
), inmcm4 = c(997.516654764994, 1145.13308691203, 959.542879520275, 
903.786765066992, 937.432661124838, 1007.46247945661, 1013.04583737278, 
1061.26827856464, 1056.61600975265, 892.433893874083, 1061.97862849247, 
978.554696188197, 939.55857188082, 996.416292720483, 1079.06762584418, 
1068.98156370385, 822.342905469512, 1019.26614712021, 1116.36245976214, 
1008.22817419423, 902.855504214986, 1087.38893040866, 885.956566984022, 
1016.16044809847, 1230.18564397572, 870.800309733513, 1037.21587559441, 
861.236791929213, 948.771432189766, 871.382698104435, 1089.3915218932, 
1054.49129365067, 941.903074357385, 973.558427724714, 938.638805666033, 
1006.66304703962, 863.28436030157, 806.021669160746, 869.369012721652, 
934.149986789314, 974.449974533922, 953.915114968656, 998.865234642777, 
949.027492106268, 967.701742440291, 952.306409621697, 959.027780253679, 
1033.69516116182, 992.987130941704, 1132.85542518598, 857.240436050392, 
1125.7240784834, 905.912164925948, 791.911242057151, 950.50688942409, 
684.62210670821, 928.505401464914, 898.455471168554, 881.937660493059, 
1043.17046368419, 990.173635925243, 872.642891720416, 968.112830276678, 
1156.70704372798, 954.681404468287, 1208.90589934053, 717.191066272881, 
985.50086154963, 1019.84388106248, 836.40573448035, 923.512492484388, 
942.453644704655, 1297.40333367214, 993.491117038875, 1035.26544859565, 
1056.57888641097, 968.592730428887, 887.591583109264, 1109.2935814163, 
923.189139410888, 972.33707020049, 1071.81484320842, 921.974872423456, 
956.677679924482, 886.258785523372, 903.500244794601, 1006.38055244068, 
914.283438551077, 779.704380831126, 1121.49522817412), 
`MRI-CGCM3` = c(848.301135629757, 
1065.98505740902, 888.813467571221, 1070.05226905271, 929.86581326247, 
949.338629810498, 1055.34828003874, 950.605580125944, 1025.98651471033, 
1062.58098758771, 1129.04220061327, 1113.69640112993, 983.264122816401, 
909.350171139101, 1049.76982306699, 991.477943714084, 1050.37713605355, 
957.954418661432, 1198.88425343781, 1019.03177011263, 892.820372968912, 
1117.83057226823, 1172.12350726162, 1045.54420748303, 1021.44374290702, 
1226.69766824788, 1076.26278716493, 892.835182446986, 1081.36552627508, 
1119.61825607435, 1101.7977981361, 1166.29589473449, 1114.24627378901, 
1111.95127710174, 1188.29593429323, 1005.43226899398, 1062.11294060983, 
1177.29197380333, 1233.89949211259, 949.362448506262, 1080.82477119637, 
848.404135782853, 1173.85210197539, 983.618625752799, 1023.98178052282, 
1174.67507789645, 1061.16686611069, 1145.73728230606, 1109.37233672472, 
1131.39932373277, 1192.52567101682, 1035.72075610234, 1170.69030772479, 
1402.30470791808, 828.360972784472, 1119.41148932651, 1418.65415788168, 
1111.1623167992, 1031.72445008109, 1133.74439397242, 1256.44423788796, 
1205.53255936418, 1246.028794886, 1353.06641467104, 1203.75084317503, 
1260.7607967303, 1133.17128359068, 1128.75225848513, 1248.50107501562, 
1125.34125579299, 1107.47674713103, 1286.39203071485, 1134.21641721783, 
1368.91059626765, 1249.62807137382, 900.188446846731, 1215.72863596362, 
1217.36216136844, 1239.46334238843, 1225.6431042983, 1293.28424474932, 
1316.91205393836, 1344.5824880094, 1431.35684547467, 1228.35587386956, 
1063.69382941901, 1366.01835107431, 1178.54993691379, 1425.59197731446, 
1439.18433794687), `NorESM1-M` = c(1104.36330130108, 1143.59864836685, 
1156.96992286211, 1225.47647032022, 1051.47222270552, 1225.96709059087, 
1169.02846946487, 1008.33133254077, 1041.4377020821, 1094.28488840048, 
1055.7942901173, 1182.85110699334, 1074.64700888046, 1220.40875347693, 
974.413995298429, 1082.02850933558, 1149.38849910412, 1177.55048525853, 
1159.36173358997, 1159.47946095381, 1013.84251995501, 1039.38081678502, 
1175.99482174589, 1075.9123379481, 1124.78923639009, 1072.4595220739, 
1059.99103769952, 1147.12456096131, 1054.66087104772, 1176.13768054148, 
1051.63089069775, 1124.1299767682, 958.783755377516, 1117.04595412512, 
1187.68311854194, 922.314885386565, 1118.97706673769, 1089.94177070272, 
1104.64546357211, 1129.91397601017, 1076.23273471979, 1117.36435679852, 
950.901478997885, 1171.61731400339, 1161.05025755828, 1122.1168542936, 
1132.68254406784, 1116.39483899695, 1052.38202713855, 1136.25445489531, 
1212.37181415053, 1113.58170133876, 988.657267351285, 1077.72113312282, 
1132.82563231238, 1060.24278563685, 1054.61879253374, 1138.19823195196, 
1086.79680899531, 1089.53650740066, 1101.56581663204, 1163.2291892284, 
1139.8293826996, 1125.56954148811, 1024.34059527431, 1152.0981675668, 
1064.32755358318, 1009.13933210229, 1210.55906508268, 1136.30532890842, 
1167.56870811327, 1163.3444730351, 1030.3320225021, 1334.11561872902, 
1186.00627022209, 1208.89776922561, 1172.03331588195, 1118.47337542874, 
1087.92709804022, 942.085245232004, 1159.78077235011, 1112.90807832063, 
1105.76965865985, 1104.31469614124, 1231.97413518222, 1228.57728152883, 
1208.44334175124, 1120.57185259478, 1014.7309899663, 1144.9384886849
)), .Names = c(""Year"", ""CanESM2"", ""GFDL-ESM2M"", ""inmcm4"", ""MRI-CGCM3"", 
""NorESM1-M""), row.names = c(NA, -90L), class = ""data.frame"")
</code></pre>

<p>Based on visual analysis, I suspect that there is a increasing <code>precip</code> trend in all models. But I need some kind of metric that can confirm an quantify this trend.</p>

<ul>
<li><strong>For each model in this dataset, how can I calculate the rainfall trends for every year and for decades (2010-2019, 2020-2029 and so on)?</strong>  </li>
<li><strong>Is it a simple linear regression of year against <code>precip</code> sufficient to show the trend?</strong></li>
</ul>
"
"0.0633215847514023","0.0620651280774201","183461","<p>I'm working on a regression that predicts the number of births a female will have based on roughly ~15 variables. Therefore I have a discrete dependent variable. I was doing some research and it seems unfavorable to use a linear model for this estimation, so I then turned to a logit model. If I use the logit model I cannot predicts number of births, instead I can only get a probability of whether they will have a child or not. I was reading about multinomial regressions, but with the number of variables I have this could get extremely cumbersome very quickly, I tested it with only 2 variables, age and church attendance and my output was extremely long. I didn't see any significance levels either. I also ran into problems when I added all my variables,</p>

<pre><code>Error in nnet.default(X, Y, w, mask = mask, size = 0, skip = TRUE, softmax = TRUE,  : too many (1044) weights
</code></pre>

<p>I don't know how to adjust the weights in R, I tried but I kept getting errors.</p>

<p>So what do you think would be the best course of action? The simpler logit model or the multinomial model? My lack of knowledge with the multinom is making we lean toward the former option. </p>
"
"NaN","NaN","183534","<p>I am using function <code>neuralnet</code> in the package <code>neuralnet</code> to build the neural network, and I see the error:</p>

<p><code>algorithm did not converge in 1 of 1 repetition(s) within the stepmax</code></p>

<p>The neural network has 20 inputs and 1 output. The problem is, with the same data and same set of inputs, I ran linear regression or random forest without any problem. So what should I look to for debugging my problem?</p>
"
"0.0490486886395286","0.0480754414848157","183846","<p>I created a SEM model in R (lavaan package), but one of myÂ dependent variables is continuous, while the other is binary.</p>

<p>The model isÂ as follows:</p>

<pre><code>modelx &lt;- '
a =~ a1Â + a2Â + a3

bÂ =~ b1Â + b2 +Â b3

c =~Â c1 + c2 + c3

x ~ a + b + c + zÂ +Â w

y ~ a + b + cÂ + zÂ +Â w

'

sem(modelx, data=mydata, estimator=""DWLS"")
</code></pre>

<p>zÂ and wÂ are covariates. x is a scale (0-12), however y is a binary variable (0;1).Â </p>

<p>Thus, the question is, how can I implement a linear regression for predicting x and a logistic regression for y in a single SEM model? I assume that lavaan doesn't automatically account for the different dependent variables. If it does, I would like to see a reference for that. All ideas are welcome.</p>

<p>Edit: Using the latent variable factor scores from the measurement model for a, b, c in a glm (binomial reg for y and linear for x) and lavaan, the results are more closely aligned for x than for y. Does it mean that lavaan ignores/doesn't do good with the dichotomous variable in this particular case, or my question from the start is moot or unnecessary?</p>
"
"NaN","NaN","183854","<p>I want to estimate a vector-valued model</p>

<p>$$\mathbf{y}_t = a\mathbf{y}_{t-1}+b\mathbf{y}_{t-2}+\cdots$$</p>

<p>Here, each $\mathbf{y}_t\in\mathbb{R}^n$ and the coefficients $a,b,\dotsc$ are real numbers (unlike in VAR, where they are matrices). I wonder what the best way to do this in R is. Applying the function <code>ar</code> doesn't seem to work for me ([i] <strong>is this only for series that lie in $\mathbb{R}^1$</strong>?), and when I apply standard linear regression, I'm not sure how I should test for the order of the model, etc. [ii] <strong>Should I go for the $R^2$ value and significance of the coefficients?</strong></p>

<p>Also, I have my data in a matrix $Y$ where each column corresponds to one time point. Currently, I regress as follows </p>

<pre><code>Y = c(Y[,10], Y[,9],..., Y[,2])
Y.l1 = c(Y[,9],..., Y[,1])
fit = lm(Y ~ 0 + Y.l1)
</code></pre>

<p>[iii] <strong>Is there a more elegant way?</strong></p>
"
"0.0400480865731637","0.039253433598943","183926","<p>I'm doing a cox regression analysis. My model is given by</p>

<pre><code>final_model  = coxph(S ~ gearbox_model + cumGwh + manufac + turbine_model + 
                  gearbox_model:cumGwh + cumGwh:turbine_model + 
                  manufac:turbine_model, timelist),
</code></pre>

<p>where </p>

<pre><code>S = Surv(tdm$start, tdm$end, tdm$delta)
</code></pre>

<p>and timelist consists of my data. Now, from R I get the following (which is just the weird part of the analysis. All the other coefficients are estimated by R).</p>

<pre><code>Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

                                 exp(coef) exp(-coef) lower .95 upper .95
manufacManufacturer2:turbine_model12        NA         NA        NA        NA
manufacManufacturer3:turbine_model12        NA         NA        NA        NA
</code></pre>

<p>I've read somewhere that I should consider collinearity, but I dont know how. Also, I've read that it could be due to not having enough data. For example, there is no events/deaths/ (delta = 1) for the combination of turbine_model =3 and Manufacturer 3.</p>

<pre><code>, , turbine_model = 3
              delta
  Manufacturer1   2
  Manufacturer2   0
  Manufacturer3   0
  Manufacturer4  15
</code></pre>

<p>But I'm kinda clueless how to proceed.</p>
"
"0.0853828074607","0.0920574617898323","183973","<p>I'm reaching out to you because I am unsure whether my implementation of a group of random forests in R (using library randomForest) is valid or whether I have an error in reasoning.</p>

<p>I have a sales dataset with a binary outcome (1: Sale, 0: No Sale) and a set of possibly significant predictors x1-x14. My data is highly imbalanced, with ~124k '0' observations (No Sale) and ~18k '1' observations (Sale). I balance it by randomly cutting down the 124k observations to 18k, as suggested in <a href=""http://bit.ly/1I7F0AC"" rel=""nofollow"">http://bit.ly/1I7F0AC</a>.</p>

<p>Cross-validation is not necessary due to the nature of random forests, however: In order to find a random forest with a good F-score, I loop through a set of possible predictors and a set of tree-numbers for the forest:</p>

<pre><code>possiblyUsefulPredictors=
  c(""x1"",...""x14"") # Shortened to pseudo-code

treerange=c(1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50,60,70,80,90,100,
            200,300,400,500,750,1000)
# Create a multitude of models by looping 
# through different settings for parameters
for (i in 2:length(possiblyUsefulPredictors)){
for (j in treerange){

### Choose model here by setting data, outcome and predictors:
x=possiblyUsefulPredictors[1:i] # Set predictors
ntree=j # Set number of trees
# Tune mtry
bestMtry=tuneRF(x=x, y=y, ntreeTry=1, 
                stepFactor=1, improve=0.01, trace=FALSE, 
                plot=FALSE, doBest=FALSE)    
# Run random forest
rf=randomForest(y=y,x=x,data=df,mtry=bestMtry,ntree=ntree,
type=""classification"",importance=T)
}
}
</code></pre>

<p>I then store model diagnostics precision, recall, and F-score in a table and choose the model that created the highest F-score (13 predictors, 90 trees, mtry=1, which leads to an F-score of 78%).</p>

<p>Specific questions:</p>

<ol>
<li><p>Obviously, the way I subset and loop through the predictors is highly arbitrary. Could a more sophisticated approach (e.g. looping through all possible subsets) get me anywhere, or does a random forest inherently choose significant predictors, so that I wouldn't have to try to find a meaningful subset myself (like I do when using step-wise in linear regression)?</p></li>
<li><p>By building a set of 416 random forests, do I simply overfit the dataset? I am skeptical that the predictors are as good as my best model suggests.</p></li>
</ol>

<p>Thank you and kind regards,
Jan</p>
"
"0.0400480865731637","0.039253433598943","183976","<p>I created a SEM model in R (<code>lavaan</code> package), but one of my dependent variables is continuous, while the other is binary.</p>

<p>The model is as follows:</p>

<pre><code>modelx &lt;- '
a =~ a1 + a2 + a3
b =~ b1 + b2 + b3
c =~ c1 + c2 + c3
x ~ a + b + c + z + w

y ~ a + b + c + z + w
'
sem(modelx, data=mydata, estimator=""DWLS"")
</code></pre>

<p><code>z</code> and <code>w</code> are covariates. <code>x</code> is a scale (0-12), however <code>y</code> is a binary variable (0;1). </p>

<p>Thus, the question is, how can I implement a linear regression for predicting x and a logistic regression for y in a single SEM model? I assume that lavaan doesn't automatically account for the different dependent variables. If it does, I would like to see a reference for that.</p>
"
"0.0749231094763201","0.0734364498908627","184226","<p><strong>Background:</strong></p>

<p>I have an overall time series of close to 3 years of data. I need to forecast for different slices of data. When I slice the data, some slices results in a shorter time series. We go with the assumption that the slices of data will have the same seasonality as in the aggregated time series across all series.</p>

<p>For the shorter time series, I plan to use some simple methodologies like exponential smoothing and then use the overall seasonality factor and adjust the forecast.</p>

<p><strong>Problem:</strong></p>

<p>Is there any simple methodology to extract the monthly seasonality factors in a time series (Assuming that the data is of monthly granularity). Say, will a simple linear regression on the response variable with dummy month variables etc. Is there any better/robust methodology which gives the seasonality factors for each month of the year from the data given ?</p>
"
"0.0749231094763201","0.0629455284778823","184699","<p>I have a question regarding p-values in the linear regression.The purpose of using the linear regression model is mainly to predict future values with accuracy. As I read, when the purpose is prediction, I can somehow not may too much attention to multicollinearity and the assumption of the linear model.
Some of my predictors are categorical variables: can I include the levels having non-significant p-values when I am only interested in prediction?</p>

<p>The output is as follows:</p>

<pre><code>lm(formula = log(cost1) ~ log(cost2) + program + location+ month + type, data=data)
Residuals:
Min       1Q   Median       3Q      Max
-0.88768 -0.10647  0.00169  0.09248  0.91612
Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              0.1526869  0.1186113   1.287  0.19858
log(cost2)               0.9812236  0.0072015 136.253  &lt; 2e-16 ***
program1                -0.0055709  0.0475793  -0.117  0.90684 
program2                -0.0007374  0.0593048  -0.012  0.99008  
program3                 0.0531385  0.0734250   0.724  0.46958 
program4                 0.0712944  0.0472402   1.509  0.13188
locationA                0.0210172  0.0319844   0.657  0.51141
locationB                0.0415091  0.0298623   1.390  0.16514
locationC                0.0898111  0.0316606   2.837  0.00474 ** 
month02                 -0.0631733  0.0454815  -1.389  0.16545  
month03                  0.0195483  0.0449924   0.434  0.66412 
month04                  0.0037596  0.0446384   0.084  0.93291
month05                  0.0387446  0.0422586   0.917  0.35966    
month06                  0.0899078  0.0494497   1.818  0.06963 .
month07                  0.0974763  0.0459993   2.119  0.03457 * 
month08                  0.0351214  0.0472294   0.744  0.45744 
month09                  0.0652653  0.0629235   1.037  0.30013
month10                 -0.5510485  0.1986461  -2.774  0.00574 ** 
TypeI                   -0.0815081  0.0821450  -0.992  0.32155
TypeII                   0.0340512  0.0436612   0.780  0.43582 
TypeIII                  0.0703337  0.0268013   2.624  0.00895 **
TypeIV                  -0.0658808  0.0411735  -1.600  0.11021   
TypeV                    0.1327603  0.0331560   4.004 7.16e-05 ***
TypeVI                   0.0994576  0.0264572   3.759  0.00019 ***
</code></pre>

<p>This model gave me the best prediction among other combination of predictors. Can I still use this model even though not all p-values are significant or no?</p>

<p>I would greatly appreciate your help!
Thank you</p>
"
"0.0633215847514023","0.0620651280774201","184795","<p>I have a number (48) bivariate relationships (N = 10 for each) where I want to fit a linear model and estimate the confidence interval (CI) using bootstrapping. </p>

<p>What I want to present, is the slope and CI for this regression. However, instead of picking one CI, I'd rather present the distribution of the slope estimates so the reader can judge for himself. What I thought about doing, is to present the histogram of the bootstrapped slope estimates along with the information about how many % of the estimates where > 0.</p>

<p>Is that a valid and/or good way to present the data? And is it valid to say that if 97.7 % of the slope estimates are > 0, the slope is significant with alpha = 0.023?</p>

<p>here is an example of what I mean</p>

<p><a href=""http://i.stack.imgur.com/33AxH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/33AxH.png"" alt=""scatterplots""></a></p>

<p><a href=""http://i.stack.imgur.com/FMaFT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FMaFT.png"" alt=""bootstrapped slope estimates, 10000 draws""></a></p>

<p>another way to formulate the question would be: prior to going this way, I calculated bootstrapped CI with the <code>boot.ci</code> function in in the <code>boot</code> package in <code>R</code>. However, the CI seem to be wider that what is suggested by the histograms. How exactly are bootstrapped CI calculated and is it wrong to assume that it should span 95% of the bootstrapped slope estimates? </p>
"
"0.0490486886395286","0.0480754414848157","185033","<p>I have a regression (time series) problem </p>

<p>$$y_t = w_1x_t+w_2z_t$$</p>

<p>Now, when I include a time-1 lag in this problem, so that the equation becomes </p>

<p>$$y_t = w_1x_t+w_2z_t+w_3x_{t-1}+w_4z_{t-1}$$</p>

<p>my $R^2$ values go up a little bit, but further including a time-2 lag:</p>

<p>$$y_t = w_1x_t+w_2z_t+w_3x_{t-1}+w_4z_{t-1}+w_5x_{t-2}+w_6z_{t-2}$$</p>

<p>completely ruins the model: Multiple $R^2$ is equal to $1$, but Adjusted $R^2$ is NaN (in R), all Std. Errors, etc. are NA. Also, when I use the learned models for prediction, the first model is slightly better than the lag-1 model, which is immensely better than the lag-2 model (the lag-2 model predicts nonsense).</p>

<p>What could be the reason for this (other than some implementation error on my side)? Too few data points to estimate the model (I have only 11 observations, but quite a few variables: The lag-0 model has about 20 variables, then there are 40 for the lag-1 model, and 60 for the lag-2 model)? Or is this an instance of multicollinearity? Does this indicate that the lag-0 model is most suitable? </p>
"
"0.0506572678011219","0.0620651280774201","185800","<p>I try to find a model using logistic regression. More precisely, what I did so far, is using stepwise regression and subset selection (although I know, it is often a bad idea) to find the ""best"" model. Clearly, depending on the information criteria I used, I got different results. </p>

<p>Now, I found an interesting example on page 250 in the book <a href=""http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"" rel=""nofollow"">""An Introduction to Statistical Learning""</a>. They chose among the models of different sizes using cross-validation, that is they make predictions for each model and compute the test errors. Eventually, the compute the cross validation error and choose the model corresponding to the minimal average cross-validation error. </p>

<p>However, the function <code>regsubsets</code> of the R package ""leaps"" is only working for linear models. How can I implement this for logistic regression or glm models in general? </p>

<p>My idea was, to just estimate the models within a cross-validation using the <code>step</code> function of the ""stats"" package and then kind of take the average number of features (which is determined by minimum AIC, for example). Is this a legitimate approach?</p>
"
"0.0535165067688","0.0734364498908627","185989","<p>I have to do some panel regressions and because I received the data as an .dta stata file, I first ran all regressions in Stata and all went fine. Later I wanted to reproduce these regressions in R which I much prefer for several reasons. It turned out that R refused to run a fixed effects regression with both individual and time effects.</p>

<p>Here's some sample data:</p>

<pre><code>   id year type1 type2 var1 var2
1   1 1991     1     1    2   11
2   1 1992     1     1    2   14
3   1 1993     1     1    3   13
4   1 1994     1     1    5   16
5   1 1995     1     1    6   17
6   2 1991     0     1    1   16
7   2 1992     0     1    3   16
8   2 1993     0     1    3   17
9   2 1994     0     1    5   20
10  2 1995     0     1    5   21
11  3 1991     1     0    1   11
12  3 1992     1     0    4   14
13  3 1993     1     0    4   15
14  3 1994     1     0    5   15
15  3 1995     1     0    8   19
</code></pre>

<p>To consider fixed and time effects in Stata, I run:</p>

<pre><code>. xtreg var2 var1 type1 type2 i.year, fe
note: type1 omitted because of collinearity
note: type2 omitted because of collinearity

Fixed-effects (within) regression               Number of obs      =        15
Group variable: id                              Number of groups   =         3

R-sq:  within  = 0.9133                         Obs per group: min =         5
       between = 0.2879                                        avg =       5.0
       overall = 0.5511                                        max =         5

                                                F(5,7)             =     14.76
corr(u_i, Xb)  = -0.0492                        Prob &gt; F           =    0.0013

------------------------------------------------------------------------------
        var2 |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        var1 |   .4102564   .4298219     0.95   0.372    -.6061109    1.426624
       type1 |          0  (omitted)
       type2 |          0  (omitted)
             |
        year |
       1992  |   1.316239   1.074077     1.23   0.260    -1.223549    3.856028
       1993  |   1.512821   1.174497     1.29   0.239    -1.264424    4.290065
       1994  |    2.82906   1.767562     1.60   0.154     -1.35056    7.008679
       1995  |   4.282051   2.293279     1.87   0.104    -1.140691    9.704794
             |
       _cons |   12.11966   .8053985    15.05   0.000     10.21519    14.02412
-------------+----------------------------------------------------------------
     sigma_u |  2.1671081
     sigma_e |  .98014477
         rho |  .83017912   (fraction of variance due to u_i)
------------------------------------------------------------------------------
F test that all u_i=0:     F(2, 7) =    21.30                Prob &gt; F = 0.0011
</code></pre>

<p>To run the same procedure in R with plm(), I tried:</p>

<pre><code>a &lt;- plm(var2 ~ var1 + type1 + type2, model=""within"", effect=""twoways"", data=data)
</code></pre>

<p>and got </p>

<pre><code>summary(a)
Error in crossprod(t(X), beta) : non-conformable arguments
</code></pre>

<p>So, my question is: Why does R have a problem and Stata not? Is there really a problem, and if so, how does Stata deal with that?</p>
"
"0.0506572678011219","0.0620651280774201","186240","<p>Hi I am trying a mediation analysis (using library(""mediation"") in R)</p>

<p>My model has 3 predictors and one mediator (n=455), but I am only interested in predictor 1. There is some collinerarity between predictor 1 and 2 - 0.383444 (Pearson). No collinerarity between predictor 3 and the others. The Mediator is correlated with IV1 and slightly with IV2. Predictors, Mediator and dependent variable are all continuous.</p>

<pre><code>lm(DV ~ IV1 + IV2 + IV3 , data = data)
</code></pre>

<p>Only IV2 is significant, R2 = 0.050</p>

<pre><code>lm(DV ~ Mediator + IV1 + IV2 + IV3 , data = data)
</code></pre>

<p>Mediator and IV2 is significant, R2 = 0.056</p>

<p>I have a much bigger dataset with n = 1200, but unfortunately I don't have Mediator information available for them. If I do a linear regression to predict DV with this dataset, IV1 and IV2 are both highly significant, the standardized beta meaningful.</p>

<ol>
<li><p>With this information can I investigate the mediating effect of the mediator on IV1 with my small dataset with 455 subjects (using the mediate()-Function of the ""mediation""-package in R) , even though the dataset itself is too small to show a significant effect of IV1 on the DV?</p></li>
<li><p>Also, I was wondering whether my mediator might mediate IV2-effect. The correlation between IV1 and the mediator is higher than between IV2 and the mediator though. </p></li>
</ol>

<p>I am thankful for any ideas.</p>
"
"NaN","NaN","186309","<p>This is my exam preparation, which will be held completely in R. Teacher said that similar tasks will be there with very limited amount of time given for solving it. Here I need to recover linear regression values (i.e., where are a-i missing values) having only this R output. I don't have the input data so I can't just copy the command and reproduce the result.</p>

<p><a href=""http://i.stack.imgur.com/4SbUj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4SbUj.png"" alt=""Given coefficient is R output. I don&#39;t have the input data.""></a></p>

<p>My question is how can I make it using R? Are there any specific commands? Or could it be done only manually? If it is possible only by manual calculation than please explain how to do it more effectively.</p>

<p>Please, describe it in detail.</p>

<p>Thank you so much for your help and deep explanation.</p>
"
"0.0490486886395286","0.0480754414848157","186464","<p>I am working on a data set (n= 230) with a categorical dependent variable (outcome: 0/1) and six categorical independent variables (mostly, with only two levels). </p>

<p>There is a certain degree of multicollinearity between two variables (X1 and X6. Anova model comparison shows that a model with X1 performs slightly better than one containing X6) and <strong>a quasi-complete separation issue regarding X4</strong> (due to an empty cell).</p>

<p>I first ran a Random Forest model (all variables were included. Ntree = 5000, mtry = 3). The result was that X1, X2 and X3 are by far the most significant predictors. X4, X5 and X6 seem to have almost no discriminative power (especially X4 whose value  in vimp() is 0.00).The model seems to be reliable (C = 0.73).  </p>

<p><strong>Question 1</strong>: does it make sense at this point to fit Binary Logistic Regression only on the most important predictors obtained through the Random Forest model (X1, X2, X3) without even considering the other three?</p>

<p><strong>Question 2:</strong> In order to avoid the separation problem with Binary Logistic Regression would it make sense to get rid of X4? 
I am quite sure that the empty cell is a bias of my data set. Moreover, this category as a whole represents only 3% of the data (The contingency table is a: 140 <strong>b:0</strong> c:86 <strong>d:6</strong>).</p>
"
"0.0633215847514023","0.0620651280774201","186578","<p>I have a set of points and I would like to fit a linear regression model to them, where each point has its own error value, and I want to find the gradient of the regression line. How do I calculate the standard error on the gradient? In R particularly would be helpful.</p>

<p>Edit:
I feel like I am misusing these terms horribly, so here is what my data looks like:</p>

<p><a href=""http://i.stack.imgur.com/Ui7c1.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ui7c1.png"" alt=""enter image description here""></a></p>

<p>with a  lm fitted line in R. The gradient of the line corresponds to a physical quantity, I need to find the value and standard error in this quantity.</p>

<p>Edit 2: I feel that I should point out that in this case, all the data values have the same precision. I am interested in both this and the general case though.</p>
"
"0.0566365471788599","0.0555127381653369","186667","<p>Let's say I have a small dataset:</p>

<pre><code>data &lt;- replicate(4,rnorm(13))
</code></pre>

<p>I want to test the out-of-sample predictions of a regression model as a function of increasing training set size (increasing by 10% in each increment).</p>

<p>I use the following procedure:</p>

<pre><code>test.set &lt;- 3

#for each iteration increase the test set by 10%
train.set &lt;- (nrow(data)-test.set) * seq(0.1,0.9,0.1)
train.set &lt;- train.set[train.set&gt;1]
results &lt;- vector()

  y=0
  for (t in train.set){
    y=y+1
    trainSize &lt;- t
    train &lt;- sample(1:nrow(data),trainSize)
    test &lt;- sample(1:nrow(data),test.set) 

    test.data &lt;- data[test,]
    train.data &lt;- data[train,]



    #fit a linear regression:
    train.data &lt;- as.data.frame(train.data)
    model &lt;- lm(train.data[,1] ~., data=train.data[,2:4])

    #get predictions:
    test.data &lt;- as.data.frame(test.data)
    predictions &lt;- predict(model,test.data)

    #calculate out of sample R squared (1-SSE/TSS): 

    error &lt;- 1 - sum( (test.data[,1] - predictions)^2 ) / ((nrow(test.data)-1) * var(test.data[,1]))

    results[y] &lt;- error
  }
</code></pre>

<p>I repeat this procedure several times and take the average of the repetitions.</p>

<p>My problem is that I get unreliable results. I am assuming this is because the dataset is really small. What could I do to get more reasonable estimates?</p>
"
"0.0980973772790571","0.0961508829696314","186725","<p>Short version: How would one be able to quantify an intervention effect in time-series analysis when the intervention decreases seasonal amplitude variation but doesn't directly effect the median?</p>

<p><a href=""https://www.dropbox.com/s/hb3g7j17igeqnoc/dat.csv?dl=0"" rel=""nofollow"">Here</a> is a link to my raw data.</p>

<p>I have a complex time-series of daily incidence numbers for a population over 7 years, totaling 2557 observations. There is a strong weekly and yearly seasonality (high incidence in winter months and low incidence in summer months). There is a baseline negative trend which is orders of magnitude smaller than the seasonality. An intervention was introduced at time = 1700. This intervention should theoretically not cause a level shift. My aim is to detect whether the intervention increases the baseline negative trend.</p>

<p>I have attempted to fit a dynamic linear regression with ARIMA errors in R using <code>auto.arima()</code> in the <code>forecast</code> package. I modeled the weekly season using a dummy variable for each weekday and the weekend. I modeled the monthly seasonality with harmonics using <code>fourier()</code> function in the <code>forecast</code> package. An the intervention effect was coded in by specifying the time index and post-intervention times as independent variables using the methods described in <a href=""http://isites.harvard.edu/fs/docs/icb.topic79832.files/L06_Program_Evaluation_2/Segmented_Regression.Wagner.2002.pdf"" rel=""nofollow"">Segmented regression analysis of interrupted time series studies in medication use research</a>. With these variables specified <code>auto.arima()</code> suggests an ARMA(7,7) process. The coefficients for baseline trend and post-intervention trend are however non-significant.  </p>

<p>I am concerned that by using fourier terms to model away the seasonality I am artificially removing any intervention effect, as visual analysis of the time series indicates that the intervention is specifically decreasing incidence during the winter months and therefore reducing the yearly seasonal variability. </p>
"
"0.0566365471788599","0.0555127381653369","186842","<p>I'm trying to fit a generalized linear model in R, but am quite new to regression, and struggling to work out how to have predictors nested within categorical variables.  </p>

<p>An example of my data:</p>

<pre><code>Response   Predictor   Category  
1          1.22           A  
5          5.67           A  
3          4.52           B  
3          7.23           B  
9          2.75           C  
4          1.11           C
</code></pre>

<p>etc....</p>

<p>I want to test for an effect of the Predictor on the Response, within each Category.  I have been able to test within each Category by sub-setting the data into each Category, but then I get lots of different test results.  I thought there is probably a way to do an overall test, but I can't figure out what formula I'd use.
Thanks</p>
"
"0.0980973772790571","0.0961508829696314","186845","<p>I created some data using the following code:</p>

<pre><code>set.seed(1221)
x &lt;- runif(500)
y &lt;- runif(500,0,2)
z &lt;- rep(0,500)
z[-0.8*x + y - 0.75 &gt; 0] &lt;- 1
plot(x,y,col=as.factor(z))
</code></pre>

<p>This produces the following plot</p>

<p><a href=""http://i.stack.imgur.com/ycWdr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ycWdr.png"" alt=""enter image description here""></a></p>

<p>The data is linearly separable. Then, I applied the glm function to create a logistic regression model.</p>

<pre><code>df &lt;- data.frame(class = z, x = x, y = y)
model &lt;- glm(z ~ x + y, family = binomial, data = df)
</code></pre>

<p>This produces the following output:</p>

<pre><code>summary(model)
Call:
glm(formula = z ~ x + y, family = binomial, data = df)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.127e-04  -2.000e-08  -2.000e-08   2.000e-08   7.699e-04  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    -1062      52666   -0.02    0.984
x              -1163      57197   -0.02    0.984
y               1433      70408    0.02    0.984

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.8274e+02  on 499  degrees of freedom
Residual deviance: 1.3345e-06  on 497  degrees of freedom
AIC: 6

Number of Fisher Scoring iterations: 25
</code></pre>

<p>The result surprised me, first because the parameter estimates are huge, and second because I was expecting such estimates to be close to the original decision boundary function, i.e. <code>-0.8x + y - 0.75 = 0</code>.</p>

<p>I then used the <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">glmnet</a> package to see if I could solve this issue. This package creates a penalised logistic regression model in order to deal with the large values in the parameter estimates. The code I used is the following:</p>

<pre><code>library(glmnet)
cvfit &lt;- cv.glmnet(as.matrix(df[,-1]), as.factor(df$class), family =   ""binomial"", type.measure = ""class"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/vH4AV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vH4AV.png"" alt=""enter image description here""></a></p>

<p>And the coefficients for the optimal penalty strength are:</p>

<pre><code>coef(cvfit, s = ""lambda.min"")
3 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept) -84.01446
x           -91.40983
y           113.18736
</code></pre>

<p>Such coefficients are smaller than the ones obtained with the <code>glm</code> function. Still they are not the same as the decision boundary function. </p>

<p>Does anybody know why this is happening? Any help is greatly appreciated.</p>
"
"0.135809669165982","0.133114869819364","187100","<p>I have a certain knowledge in stochastic processes (specially analysis of nonstationary signals), but in addition to be a beginner in R, I have never worked with regression models before.
Well, I have some doubts on understanding the outcome of the function summary() in R, when using with the results of a glm model fitted to my data. Well, suppose I used the following command to fit a generalized linear model to my data:**</p>

<pre><code>glm_model &lt;- glm(Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
</code></pre>

<p>Then I use summary(glm_model) to obtain the following:</p>

<pre><code>Call: 
glm(formula = Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-7.4583  -0.8985   0.1628   1.0670   6.0673  
Coefficients:

Estimate Std. Error t value Pr(&gt;|t|)    

(Intercept)        8.522e+00  6.553e-02 130.041  &lt; 2e-16 ***

Input1            -3.819e-04  3.021e-05 -12.642  &lt; 2e-16 ***

Input2            -2.557e-04  2.518e-05 -10.156  &lt; 2e-16 ***

Input3            -3.202e-02  1.102e-02  -2.906  0.00367 ** 

Input4            -1.268e-01  7.608e-02  -1.666  0.09570 .  

Input1:Input2      1.525e-08  2.521e-09   6.051 1.53e-09 ***


Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 2.487504)
    Null deviance: 18544  on 5959  degrees of freedom
Residual deviance: 14811  on 5954  degrees of freedom
  (1708 observations deleted due to missingness)
AIC: 22353
Number of Fisher Scoring iterations: 2
</code></pre>

<p>From a estimation theory perspective, I understand that ""estimate"" and ""Std. Error"" are the estimates and the standard deviation of the unknown parameters (beta1, beta2,...) of my model. However, there are some things I do not understand:</p>

<p>1) How can I assess how good my fit is from the output of <code>summary()</code>? We could not use only the information of the standard deviation of the parameter estimators to assess the goodness-of-fit. I would expect to have access to the sampling distribution of a given parameter estimator to know the % of estimates within +- 1std, +-0.5std or any +-x*std, for example. Other option would be knowing the theoretical distribution of the parameter estimator, so as to try to calculate its Cramer Rao Lower Bound and compare with the calculated std.</p>

<p>2) What does the t value (or Pr(>|t|) ) have to do with the goodness-of-fit? Since I am not familiar with regression models, I do not know the connection between the student t distribution and the estimation of the model parameters. What does it mean? Is the parameter estimator of the glm model distributed according to the student t pdf (like the sample estimator for small samples of an unknown population)? What conclusions should I take from Pr(>|t|)?</p>

<p>3) Do we have a more general form of assessing the goodness-of-fit, like a measure of the variability of the data my model can capture, maybe a table of critical values for such a measure given a certain significance level?** </p>

<p>4) When fitting a glm model, do we need to specify a significance level? If yes, why such an information is not provided by the summary function?</p>

<p>5) The summary function outputs some measures based on information theory, like AIC: 22353. Can we define an optimal reference value for AIC? What is a good AIC value? My intuition is that we could not do so, like other information theory measures (mutual information, entropym,...)</p>

<p>Thank you for your help!</p>
"
"0.0749231094763201","0.0734364498908627","187181","<p>For a linear regression model I tried on a dataset, when I fitted OLS, the output is as follows:</p>

<pre><code>fit = lm(price ~., data = art)

# Coefficients:
#                                  Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)                     2.326e+03  8.863e+02   2.625 0.008777 ** 
# temp_stagelate                  3.029e-01  8.735e-02   3.467 0.000544 ***
# temp_stagemature                4.009e-01  1.154e-01   3.473 0.000533 ***
# temp_stagemiddle                5.346e-01  8.646e-02   6.184 8.55e-10 ***
# temp_stagena                    1.766e-01  8.645e-02   2.042 0.041322 *  
# tonedark                       -4.306e-01  5.511e-02  -7.814 1.20e-14 ***
# tonelight                      -4.267e-01  6.214e-02  -6.866 1.05e-11 ***
# subjectflower-animal           -3.997e-01  6.972e-02  -5.734 1.24e-08 ***
# subjectlandscape                1.609e-02  6.429e-02   0.250 0.802480    
# size.square                     2.454e-04  1.696e-05  14.469  &lt; 2e-16 ***
# size.sqq                       -6.205e-09  9.412e-10  -6.593 6.44e-11 ***
# coloringink and color           3.665e-01  5.522e-02   6.637 4.81e-11 ***
# further.inscribed.or.signedyes  2.997e-01  7.868e-02   3.810 0.000146 ***
# signedyes                       9.487e-01  2.058e-01   4.610 4.45e-06 ***
# inscribedyes                    1.865e-01  5.966e-02   3.126 0.001812 **
#   
# Residual standard error: 0.6806 on 1511 degrees of freedom
# Multiple R-squared:  0.726,    Adjusted R-squared:  0.7186 
</code></pre>

<p>and when I tried to fit a weighted least squares (WLS) model, </p>

<pre><code>gls(price ~. , data=art, weights = varFixed(~size.square)) 
</code></pre>

<p>the output is of the following:</p>

<pre><code># Standardized residuals:
#     Min          Q1         Med          Q3         Max 
# -4.27128213 -0.58938641 -0.06659419  0.53758125  6.03938177 
# 
# Coefficients:
#                                    Value Std.Error    t-value p-value
# (Intercept)                    1851.4155  924.1690   2.003330  0.0454
# temp_stagelate                    0.3717    0.1020   3.645983  0.0003
# temp_stagemature                  0.6992    0.1257   5.563261  0.0000
# temp_stagemiddle                  0.4881    0.1032   4.729729  0.0000
# temp_stagena                      0.2973    0.0978   3.038328  0.0024
# tonedark                         -0.4489    0.0571  -7.867553  0.0000
# tonelight                        -0.4451    0.0627  -7.093028  0.0000
# subjectflower-animal             -0.3605    0.0716  -5.036949  0.0000
# subjectlandscape                  0.0625    0.0669   0.934295  0.3503
# size.square                       0.0003    0.0000  13.128687  0.0000
# size.sqq                          0.0000    0.0000  -5.179399  0.0000
# coloringink and color             0.4053    0.0560   7.235716  0.0000
# further.inscribed.or.signedyes    0.3425    0.0866   3.955705  0.0001
# signedyes                         0.8654    0.2979   2.905369  0.0037
# inscribedyes                      0.3211    0.0574   5.593102  0.0000
# 
# Residual standard error: 0.01437576 
# Degrees of freedom:  1511 residual
</code></pre>

<p>I get a much smaller residual standard error. I am wondering if the two residual errors I have are comparable, and if the smaller residual standard error in the WLS model indicates that the WLS yields a better fit? </p>

<p>Does a smaller residual standard error in general indicate a better fit?</p>
"
"0.0490486886395286","0.0480754414848157","187199","<p>I am trying to develop a Multivariate linear regression, in R 3.2.2,  using </p>

<blockquote>
  <p>Ad GRP</p>
</blockquote>

<p>and </p>

<blockquote>
  <p>Ad Spending</p>
</blockquote>

<p>to predict Sales.</p>

<p>This is my GRP Data</p>

<pre><code>adGRPAugToDec &lt;- c(2020, 1278, 1195, 1310, 495)
</code></pre>

<p>This is my adstock function , based on this blogpost <a href=""https://analyticsartist.wordpress.com/2014/08/17/marketing-mix-modeling-explained-with-r/"" rel=""nofollow"">MMM</a></p>

<pre><code>    # Define Adstock Function
adstock &lt;- function(x, rate=0){
 return(as.numeric(filter(x=x, filter=rate, method=""recursive"")))
}
</code></pre>

<p>I call the above function like this </p>

<pre><code>ad.adstock &lt;- adstock(adGRPAugToDec,0.5)
</code></pre>

<p>I get this error</p>

<blockquote>
  <p>Error in filter_(.data, .dots = lazyeval::lazy_dots(...)) :<br>
  argument "".data"" is missing, with no default</p>
</blockquote>
"
"0.0490486886395286","0.0480754414848157","187474","<p>These are plots created in R. All of them are residuals (errors)
vs. fitted values. They come from multiple linear regression models fitted by least squares. The five plots represent 5 different output variables (5 different models), but they come from one dataset. All of the models use the same predictors. I would  like to ask if any conclusions can be drawn from these plots. What can be done to improve the models?</p>

<p>I am not very well educated in statistics and hence the only thing I can see are outliers. I am especially interested in patterns from last two plots (from first link). I added the next two plots to show that the number of data used is the same for each plot. </p>

<p><a href=""http://i.stack.imgur.com/ErPAV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ErPAV.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/YoBwC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YoBwC.png"" alt=""enter image description here""></a></p>
"
"0.0980973772790571","0.0961508829696314","187487","<p>Let's say we have data that looks like this:</p>

<pre><code>set.seed(1)
b0 &lt;- 0 # intercept
b1 &lt;- 1 # slope
x &lt;- c(1:100) # predictor variable
y &lt;- b0 + b1*x + rnorm(n = 100, mean = 0, sd = 200) # predicted variable
</code></pre>

<p>We fit a simple linear model:</p>

<pre><code>mod.1 &lt;- lm(y~x) 
summary(mod.1) 
#             Estimate   Std. Error  t value  Pr(&gt;|t|)
# (Intercept) 26.3331    36.3795     0.724    0.471
# x           0.9098     0.6254      1.455    0.149 
b0.est &lt;- summary(mod.1)$coefficients[1,1]
b1.est &lt;- summary(mod.1)$coefficients[2,1]
</code></pre>

<p>And a model where we (1) subtract off the intercept term fit in the first model from the dataset and (2) prevent the intercept term from being fit (or in other words, force the model through zero):</p>

<pre><code>mod.2  &lt;- lm(y - b0.est  ~ 0 + x) 
summary(mod.2) 
#             Estimate   Std. Error t value   Pr(&gt;|t|)   
# x           0.9098     0.3088     2.946     0.00401 **
b1.est.2 &lt;- summary(mod.2)$coefficients[1,1]
</code></pre>

<p>As to be expected the slope parameter stays the same (0.9098).</p>

<p>However, while the slope parameter was not significant in the first model, it is in the second model (the standard error on the estimate in the second model is much lower than in the first model, 0.3088 vs. 0.6254).</p>

<p>The data is the same shape in both models with the same slope parameter being estimated by the two models. <strong>How is it the second model is so much more ""certain"" of the slope parameter estimate?</strong></p>

<p><strong>Or to put it another way, how are these standard errors calculated?</strong> </p>

<p>Using the equation for standard error I found <a href=""http://stattrek.com/regression/slope-test.aspx?Tutorial=AP"" rel=""nofollow"">here</a>, I calculated the standard errors for model 1 and 2 this way:</p>

<pre><code># Model 1
DF &lt;- length(x)-2 
y.est &lt;- b0.est + b1.est*x 
numerator &lt;- sqrt(sum((y - y.est)^2)/DF) 
denominator &lt;- sqrt(sum((x - mean(x))^2))
numerator/denominator 
# SE = 0.6254
</code></pre>

<p>This matches the R output.</p>

<pre><code># Model 2
DF &lt;- length(x)-1 
y.est &lt;- b1.est.2*x 
numerator &lt;- sqrt(sum((y - (y.est+b0.est))^2)/DF) 
denominator &lt;- sqrt(sum((x - mean(x))^2))
numerator/denominator 
# SE = 0.6223
</code></pre>

<p>This doesn't match the R output which has the SE = 0.3088. </p>

<p>What am I missing?</p>
"
"0.0400480865731637","0.039253433598943","187509","<p>With a small book-exercise with four metric variables on 10 cases (one dependent/outcome, three independent/predictor) I ran <em>linear regression</em> in <code>SPSS</code> and <code>R</code>, and <em>ANOVA</em> (in <code>SPSS</code> declaring the predictors as ""covariates"").<br>
I found the output of the <em>SSQ</em> (Sum-of-Squares) different - and obviously from this also the F-test statistic and the p-values. Except from the last predictor the displayed values are different (the predictors in <code>R</code>may be reordered and the analysis be rerun to find all <code>SPSS</code>- coefficients).                   </p>

<p>By reengineering the computations in matrix-formulae I could reproduce the SPSS-values as well as the R-values and found, that <code>SPSS</code> uses the (partial) SSQ based on the logic of the ""usefulness""-coefficients for each predictor (which is sort of semipartial coefficient), while <code>R</code> simply uses the (hierarchically) partial SSQ. <em>(Unfortunately I'm not sure how to express that two approaches correctly so this toy-characterizing might be improved)</em> .             </p>

<p><strong><em>Q:</em></strong> Has that property of different focuses/philosophies been discussed anywhere? Is there some advantage of one over the other?   </p>

<p><hr>
Data: (taken from M. Backhaus et al., multivariate Verfahren)             </p>

<pre><code>predictors                   outcome-item
---------------------------+-------------
Preis   VerkFoer  Vertreter  Absatzmenge
12.50      2000      109      2298
10.00       550      107      1814
 9.95      1000       99      1647
11.50       800       70      1496
12.00         0       81       969
10.00      1500      102      1918
 8.00       800      110      1810
 9.00      1200       92      1896
 9.50      1100       87      1715
12.50      1300       79      1699
</code></pre>

<p>The comparision of the output:
<a href=""http://i.stack.imgur.com/zNJDB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zNJDB.png"" alt=""bild""></a></p>
"
"0.0895502439463906","0.0877733458775107","188011","<p>I'm doing machine learning in R. I would like to know how we can create a model object that we can pass to ""predict"" function along with new data so that we obtain predicted values. 
To elaborate, I'm trying to write a new machine learning algorithm in R. Till now I have only used predict function but don't know how to create ""model"" objects to pass to predict function. If we're doing a linear regression, calling lm would create ""lm"" object. If we're doing naiveBayes classifier, and call it from e1071 package, it would create naiveBayes classifier object, which we will pass to predict function. Now, if I'm writing a new algorithm, how do I create an object of that algorithm? And how exactly predict function will process that? What class variables/methods that ""model"" object should have so that it can be processed by ""predict"" function available in R? I know this is a bit open ended question, but I couldn't find any proper documentation. A basic/prototype example in terms of code would be highly appreciated. Though I've been using R, I'm not familiar to classes/objects concept in R. Thank you very much.</p>
"
"0.0980973772790571","0.0881383093888288","188098","<p>I am trying to estimate a model for an event modelled by probability of happening which is a linear function of x (distributed normally) plus an error term, u.</p>

<p>Then I simulate whether the event really happened for each X comparing the probability of it happening against a uniformly distributed random variable.</p>

<p>So, I wrote a little function that simulates this model for a given b0, b1, X (mean and sd.) and error term (mean = 0 and sd.):</p>

<pre><code>SAMPLE_SIZE = 10000

underlying &lt;- function(b0, b1, mean_x, sd_x, sd_u) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean_x, sd_x)
  us &lt;- rnorm(SAMPLE_SIZE, 0.0, sd_u)
  ys &lt;- b0 + (b1 * xs) + us
  ws &lt;- runif(SAMPLE_SIZE) &lt; ys  
  list(ws = ws, ys = ys, xs = xs, us = us)
}
</code></pre>

<p>It neatly returns both the probability of the event taking place in the ys component plus a simulation on the ws component.</p>

<p>I then tested whether I can correctly estimate b0 and b1 using linear regressions. And I got a very weird result.</p>

<p>This is how I simulated the samples and did the regressions:</p>

<pre><code>b1s &lt;- seq(from = 0.0, to = 1.0, length.out = 100)

datasets &lt;- lapply(b1s, FUN = function(x) underlying(0.5, x, 1.0, 0.2, 0.05)) 
regs     &lt;- lapply(datasets,  FUN = function(x) lm(data = x, ws ~ xs))
b0s_hat = sapply(regs, function(x) x$coefficients[[1]])
    b1s_hat = sapply(regs, function(x) x$coefficients[[2]])
</code></pre>

<p>So, for different b1s (and b0 = 0.5) I can plot the estimated b0 and b1 against the real b1:</p>

<pre><code>plot(b1s, b0s_hat)
plot(b1s, b1s_hat)
</code></pre>

<p>And what we get for b1s_hat looks sigmoid-ish like a cumulative distribution function, and b0s_hat looks like a bell curve (like the density function).</p>

<p>I thought I could recover the coefficients using the linear regression. What exactly is smelling weird here?</p>
"
"0.0755153962384799","0.0832691072480053","188112","<p>I am studying logistic regressions and I wonder why are estimators biased when the independent variables have low variance (maybe low variance compared to its mean, but anyway).</p>

<p>I simulate the underlying model as a linear function of a single variable <code>x</code> and I do not include an error term. <code>x</code> is generated from a normal distribution, with mean <code>mx</code> and sd <code>sx</code>.</p>

<p><code>f</code> is a helper to map the probabilities using a logistic function</p>

<p>I use <code>mx = 1.0</code>, and sample <code>sx</code> from a uniform distribution from 0 to 1, so I can estimate the model for different values of <code>sx</code>.</p>

<pre><code>SAMPLE_SIZE = 1000
set.seed(100)

f &lt;- function(v) exp(v) / (1 + exp(v));

sim = function(b0, b1, mx, sx) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean = mx, sd = sx)
  ps &lt;- f(b0 + b1 * xs)
  ys &lt;- rbinom(SAMPLE_SIZE, 1, ps)
  glm(ys ~ xs, family = binomial)
}  


sx &lt;- runif(n = 1000, min = 0.05, max = 1.0)
b0 = 1.5
b0s &lt;- sapply(sx, function(v) {
  sim(b0 = b0, b1 = 1.0, mx = 1.0, sx = v)$coefficients[[1]]
})
</code></pre>

<p>And then I plot the error between the estimated <code>b0</code> coefficient and the real one, for different values of <code>sx</code>:</p>

<pre><code>plot(sx, b0s - b0)
</code></pre>

<p>What I get is that the error gets smaller the greater <code>sx</code> is.</p>

<p>From common linear regressions, we know that the estimators get more precise the larger the variance in the independent variables. But that does not say anything about the biases. </p>

<p>How to interpret this result? Are the estimators really biased in logistic regressions? What's missing here? Is there any problem related to numerical estimates here?</p>

<p><a href=""http://i.stack.imgur.com/aj8md.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aj8md.png"" alt=""Estimation error vs. standard deviation in X""></a></p>
"
"0.0749231094763201","0.0524546070649019","188115","<p>I currently have 3 models to predict Y from a linear combination of independent variables:</p>

<p>Model 1: Y ~ A + B</p>

<p>Model 2: Y ~ A + C</p>

<p>Model 3: Y ~ A + D</p>

<p>Now, I want to compare their in-sample fitting and out-of-sample prediction performances. I could split the data into training and testing sets, run the linear regression (lm) on the training set, and predict Y of the testing set with the result. However, the results vary depends on seeds used to split the data set.</p>

<p>I came across ""Cross Validation"" concept, but got confused on how to use it both in in-sample fitting and out-of-sample prediction. Across folds, the coefficients obtained from linear regression on training set will be different. This will affect the prediction part on the testing set as well. (Also that for every fold, the training and testing sets keep changing.)</p>

<p>Could someone help me with how to actually use cross-validation in this setting? Thank you!</p>
"
"0.0424774103841449","0.0555127381653369","188146","<p>I am building a linear regression for Market Mix Modeling, with time series variables. The problem is that, few of the variables are not available before a certain time period, almost half of the series. </p>

<p>Now, these variables have no values in those time period, because the company did not invest in that particular market channel. So this is a structural change problem than a missing value problem. </p>

<p>What is the right way to treat this variable while including it part of the regression? Or Is linear regression not the right way to go about it?</p>
"
"0.0600721298597455","0.0686935087981502","188510","<p>I have developed a model (TSM) which is very good at forecasting daily revenue, however it is very black box. The TSM is univariate, whereas the regression models are multivariate. My goal here is to identify the causal factors of revenue for policy recommendations using regression analysis in R. The models that I am using are decision tree, random forest, linear model (some variables), linear model 2 (all variables), xgboost, and lasso regression. All variables are numeric. </p>

<p>Looking at random forest vs linear regression, (both rmse and mae are similar for these two), we see something odd: Looking at the variable importance plot for random forest, variable x is the least important. However, for the regression model, it is highly significant. Furthermore, lasso regression also indicates that the variable is very important. How can this be so? </p>

<p>This variable has a important policy implication to the business, so which model is right - is variable x the most important or is it the least important?  I vaguely recall Breiman writing a paper some time ago discussing something along the lines of this. </p>

<p>The below chart is the out of sample accuracy of the models over a 4 month time period. </p>

<pre><code>&gt; a
    tsm      tree        rf       lm       lm2      xgb      lasso
 0.9715964 0.9854246 0.9904363 0.981333 0.9817757 0.974603  0.997324
</code></pre>

<p>PS, I am sorry I cannot disclose the variables themselves since they are confidential to the company.</p>

<p>Thank you for your assistance!</p>
"
"0.0400480865731637","0.039253433598943","188632","<p>I have a multiple linear regression model in <code>R</code> where I shall analyse the rating of a product by demographic variables(<code>age</code>, <code>education</code>, <code>gender</code>, <code>income</code>) and by product variables (<code>price</code> and the different labels (dummy variables)).</p>

<p>My model is:</p>

<pre><code>model &lt;- lm(marketing$rating ~ marketing$age + marketing$education + 
            marketing$gender + marketing$income + marketing$price +
            marketing$wa + marketing$kr + marketing$vo + marketing$ju)
</code></pre>

<p>where <code>wa</code>, <code>kr</code>, <code>vo</code> &amp; <code>ju</code> are the labels and <code>rq</code> is the basis.</p>

<p>Now I am given the following question:</p>

<p>""Test the Hypothesis: the influence of the label <code>ju</code> and <code>vo</code> is identical and the <code>income</code> has no influence""</p>

<p>I would have formulated the null- and alternativ hypothesis: $$H_{0}: \beta_{10} - \beta_{9} = 0 \wedge \beta_{5}=0$$ $$H_{A}:   \beta_{10} - \beta_{9} \neq 0 \vee \beta_{5} \neq 0$$</p>

<p>Then I would have used the <code>linearHypothesis()</code> function of <code>R</code>: </p>

<pre><code>linearHypothesis(model, c(""marketing$ju = marketing$vo"",""marketing$income""))
</code></pre>

<p>I am not sure if this is correct - could you provide me with help please.</p>
"
"0","0.0277563690826684","188753","<p>I'm attempting to predict vegetation productivity based on climatic and land use variables (the latter are categorical). I found that there is a multicollinearity problem between the predictors (especially land use) as seen from the Variance Inflation Factor (VIF of the Ordinary Least Squares Regression). </p>

<p>Although my knowledge of lasso regression is basic, I assume lasso regression might solve the multicollinearity problem and also select variables that are driving the system. I appreciate an R code for estimating the standardized beta coefficients for the predictors or approaches on how to proceed.</p>

<pre><code>Variable           Coeff.  Std Coeff.  VIF    Std Error    t      P  Value 
Constant          -0.228   0            0      0.086       -2.644  0.008  
Precipitation      &lt;.001   0.151       2.688   &lt;.001        8.541  0.0  
Solar Rad          0.002   0.343       2.836   &lt;.001        18.939 &lt;.001  
Temp              -0.116  -1.604       28.12   0.004       -28.11  0.0  
Water Stress       0.881   0.391       2.352   0.037        23.7   &lt;.001  
Vapor Pressure     0.135   1.382       30.49   0.006        23.259 0.0    
  1               -0.103   -0.109      52.086  0.074       -1.398  0.162    
  2               -0.14    -0.048      6.49    0.079       -1.761  0.078   
  3               -0.11    -0.048      10.007  0.077       -1.42   0.156    
  4               -0.104   -0.234      236.288 0.073       -1.416  0.157    
  5               -0.097   -0.242      285.244 0.073       -1.331  0.183    
  6               -0.104   -0.09       35.067  0.074       -1.406  0.16    
  8               -0.119   -0.261      221.361 0.073       -1.629  0.103 
ELEVATION          &lt;.001   -0.115      3.917   &lt;.001       -5.381  &lt;.001
Condition Number: 59.833 
Mean of Correlation Matrix: 0.221 1st    
Eigenvalue divided by m: 0.328
</code></pre>
"
"0.0800961731463273","0.078506867197886","189188","<p>If I create a linear model in R, I get a p-value for the whole model. When I create a logistic regression model, I don't. Why is this?</p>

<p><strong>Linear Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-x+rnorm(100)
summary(lm(y~x))

 Call: lm(formula = y ~ x)

 Residuals:
      Min       1Q   Median       3Q      Max 
 -2.46237 -0.52810 -0.04574  0.48878  2.81002 

 Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)     (Intercept) -0.02318    0.09394  -0.247    0.806     x            1.10130    0.09421  11.690   &lt;2e-16***
 --- Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Residual standard error: 0.9374 on 98 degrees of freedom Multiple
 R-squared:  0.5824,    Adjusted R-squared:  0.5781  F-statistic: 136.7 on
 1 and 98 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Logistic Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-factor(c(rep(""ONE"",50),rep(""TWO"",50)))
summary(glm(y~x,family = ""binomial""))

 Call: glm(formula = y ~ x, family = ""binomial"")

 Deviance Residuals: 
      Min        1Q    Median        3Q       Max  
 -1.20658  -1.18093  -0.00499   1.17444   1.21414  

 Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|) (Intercept)  3.857e-05  .000e-01   0.000    1.000 x           -3.924e-02  2.055e-01  -0.191    0.849

 (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom Residual deviance: 138.59  on 98  degrees of freedom AIC: 142.59

 Number of Fisher Scoring iterations: 3
</code></pre>
"
"0.0642198081225601","0.0734364498908627","189202","<p>I would be very grateful if you could please advise on sample size....</p>

<p>I have estimated a sample size using R - library(pwr) using command
""pwr.f2.test(u = 6, v = NULL, f2 =0.02 , sig.level =0.05 , power =0.8 )""</p>

<p>This estimates a sample size of 97.</p>

<p>The above sample size is estimated for a linear regression model.
My data will be cost data, thus would be all positive data (with no zeros). To account for an all positive data set I can log transform my data to satisfy model assumptions, or alternatively I can run a gamma regression model</p>

<p>My query: if I decide to run a gamma regression model, is the above sample size sufficient? (or is there any way of calculating a sample size for a gamma regression in R?). </p>

<p>Any advice is greatly appreciated</p>

<p>Kind regards</p>

<p>Etn</p>
"
"0.0490486886395286","0.0480754414848157","189327","<p>I am checking assumptions(multivariate normality) of multiple linear regression in R. I have 100 independent variables and want to check their normality.</p>

<p>Should i need to check one by one for all 100 variables?
Or should we just assume there is a normality and proceed with the regression?
Or is there any way from which I can check the normality of all 100 variables in a single piece of code in R?</p>

<p>Also is there any way the linearity of all these 100 IVs can be checked with the DV?</p>
"
"0.0490486886395286","0.0480754414848157","189515","<p>I would like to fit the following model by ridge regression (the xs correlate strongly with one another)</p>

<p>$y = \beta_1 {x_1}^{\lambda_1} + \beta_2 {x_2}^{\lambda_2} + \beta_3 {x_3}^{\lambda_3} + \cdots$</p>

<p>My current approach is to try Box-Tidwell transformation on the $x$s followed by linear ridge regression, but <code>cv.glmnet</code> in R throws an error (<code>lm.fit: NA/NaN/Inf in x</code>.  There are definitely none of these in my data, nor any zeros; all $x\gt0$ and $y\gt0$).  If you think these errors can be solved both for my current and future data then perhaps the real question is how to do that; example data pasted below if needed.</p>

<p>Assuming Box-Tidwell isn't robust enough for the job (in the everyday sense of the word, not statistical) then what are my alternatives?  Preferably with <code>R</code> libraries?</p>

<p><em>Data that causes the error, if relevant</em></p>

<p>Y</p>

<pre><code>2.121973109561620 
1.356081828171345 
3.386240338106292 
4.191699045929254 
3.335211073713898 
3.361101880939723 
1.356081828171345 
4.861907349730742 
3.894757399692251 
1.886732223373362 
1.079580255790821 
1.079580255790821 
2.653488025839402 
2.011958073636261 
3.677721001141955 
0.685538468679168 
3.361101880939723 
2.901321785495637 
0.685538468679168 
3.894757399692251 
2.011958073636261 
0.685538468679168 
4.410650146393066 
1.079580255790821 
2.761096650525593 
0.685538468679168 
0.685538468679168 
3.997709960290063 
0.685538468679168 
1.079580255790821 
1.886732223373362 
3.361101880939723 
3.128459524780916 
1.079580255790821 
3.695194367478646 
4.410650146393066 
4.534314613554609 
2.983752390192320 
1.356081828171345 
1.886732223373362 
3.623052422197949 
4.333074550938076 
</code></pre>

<p>XS (columns are variables)</p>

<pre><code>478.666646867709005 3334.999755859999823 8875.997070310000709 18324.000000000000000 73794.968750000000000 84945.000000000000000 86130.000000000000000 97553.000000000000000 95064.997070309997071 216030.000000000000000 259379.000000000000000 
10099.663411979709053 11680.000000000000000 12200.000000000000000 7958.000000000000000 8131.000000000000000 18328.009765599999810 20007.000000000000000 15786.959960899999714 21986.997070310000709 33306.000000000000000 33565.000000000000000 
33075.661458879709244 85765.968750000000000 164516.000000000000000 236134.000000000000000 257607.000000000000000 251330.703125000000000 253966.000000000000000 267988.000000000000000 313167.997070309997071 674297.000000000000000 386713.000000000000000 
18147.663411979709053 43723.000000000000000 112686.968750000000000 232787.000000000000000 359299.000000000000000 416794.687500000000000 448409.000000000000000 456474.000000000000000 580426.997070310055278 1177783.000000000000000 806379.000000000000000 
1751.666341689709043 742.000000000000000 1373.000000000000000 3774.000000000000000 3417.997070309999799 7048.000000000000000 8601.000000000000000 12807.000000000000000 12036.997070310000709 58174.968750000000000 88478.000000000000000 
8614.666341689709952 15974.997070299999905 35111.000000000000000 31349.009765599999810 45828.960937500000000 60135.000000000000000 94882.000000000000000 126677.000000000000000 162313.997070309997071 536617.687500000000000 588591.000000000000000 
1497.666341689709043 1377.000000000000000 1679.000000000000000 1488.000000000000000 2739.000000000000000 3895.997070309999799 4282.000000000000000 6349.000000000000000 7341.997070309999799 17356.000000000000000 15809.000000000000000 
9709.666341689709952 24608.996093800000381 49844.011718800000381 83619.960937500000000 108208.000000000000000 122971.000000000000000 120834.000000000000000 129527.000000000000000 176157.997070309997071 770005.687500000000000 1558291.000000000000000 
3656.666341689708588 10260.997070299999905 9192.000000000000000 19816.000000000000000 40678.011718800000381 66365.960937500000000 83218.000000000000000 96234.000000000000000 90788.997070309997071 122521.000000000000000 68211.000000000000000 
1435.666341689709043 1039.000000000000000 1120.000000000000000 1548.000000000000000 2795.000000000000000 2819.997070309999799 1381.000000000000000 2814.000000000000000 2597.997070309999799 7300.000000000000000 4795.000000000000000 
326.666646867709005 222.000000000000000 233.000000000000000 590.999694824000017 646.000000000000000 1294.000000000000000 902.000000000000000 1876.000000000000000 1755.997070310000026 4264.000000000000000 2435.000000000000000 
15090.663411979709053 30045.000000000000000 51073.011718800000381 56378.960937500000000 74637.000000000000000 113832.000000000000000 99594.000000000000000 107876.000000000000000 92927.997070309997071 124484.000000000000000 90528.000000000000000 
6778.666341689709043 15995.997070299999905 28689.000000000000000 35889.011718800000381 39506.960937500000000 38009.000000000000000 42170.000000000000000 57875.000000000000000 51799.997070309997071 106503.000000000000000 115013.000000000000000 
728.666646867709005 4308.999511719999646 5236.997070309999799 7361.000000000000000 7402.000000000000000 7643.000000000000000 10397.000000000000000 8805.000000000000000 8702.997070310000709 16156.009765599999810 25867.000000000000000 
11485.663411979709053 29994.000000000000000 55584.011718800000381 110733.960938000003807 196418.000000000000000 224935.000000000000000 301638.000000000000000 415136.687500000000000 486633.997070309997071 1537518.000000000000000 2612288.000000000000000 
14.999997049609000 113.000003814999999 569.999938964999956 2424.999755859999823 1697.999694820000059 4809.000000000000000 4335.000000000000000 3392.996948240000165 2.000000000000000 9806.997070310000709 9400.997070310000709 
12241.996419779708958 16586.000000000000000 43257.997070299999905 51420.968750000000000 64047.000000000000000 46403.000000000000000 45279.000000000000000 56147.000000000000000 44679.006835909996880 149701.960938000003807 272353.000000000000000 
600.999944597709032 5093.999511719999646 25560.996093800000381 73356.007812500000000 132986.958007999986876 184185.000000000000000 253398.000000000000000 249111.000000000000000 259542.700195309997071 468466.000000000000000 598152.000000000000000 
2.000000894069000 2.000000000000000 3.999999046330000 261.999938965000013 664.000000000000000 1598.999389650000012 1488.000000000000000 4466.000000000000000 6187.997070309999799 8815.997070310000709 4447.000000000000000 
27280.663411979709053 60442.011718800000381 154128.953125000000000 303640.000000000000000 601715.687500000000000 953291.000000000000000 1463397.000000000000000 1965766.000000000000000 2591598.997070310171694 6003257.000000000000000 6577179.000000000000000 
12.666664034109001 73.000007629400002 223.999954223999993 757.999694824000017 1546.000000000000000 2144.000000000000000 2766.000000000000000 6687.997070309999799 11251.997070310000709 56485.011718800000381 124376.960938000003807 
24.999994188509000 291.999938965000013 1250.000000000000000 1965.999389650000012 6617.000000000000000 868.000000000000000 4410.000000000000000 132.000000000000000 3736.997070309999799 8040.994140630000402 8349.000000000000000 
18833.996419779708958 50967.997070299999905 85909.968750000000000 101534.000000000000000 93684.009765599999810 98219.960937500000000 105931.000000000000000 143515.000000000000000 144080.997070309997071 231816.000000000000000 253980.703125000000000 
181.000020891708999 698.999938964999956 648.000000000000000 557.999389648000033 803.000000000000000 1047.000000000000000 423.000000000000000 1839.000000000000000 2772.997070309999799 3955.000000000000000 2865.000000000000000 
220.666631608708997 479.000000000000000 999.999694824000017 1255.000000000000000 4805.000000000000000 1357.000000000000000 7213.997070309999799 2235.000000000000000 2047.997070310000026 2.000000000000000 2991.000000000000000 
2.000000000000000 18.999996900599999 218.999977112000010 933.999969481999983 1800.999450680000109 7763.000000000000000 9354.997070310000709 2147.000000000000000 4617.997070309999799 9389.997070310000709 2.000000000000000 
2.000000059604000 4.999999761580000 55.999992370599998 161.999980926999996 431.999954224000021 1728.999694820000059 2003.999694820000059 3507.000000000000000 4759.997070309999799 16910.994140599999810 22492.000000000000000 
475.999944597709032 2699.999450679999882 9437.000000000000000 35624.996093800000381 110005.965819999997620 230986.000000000000000 389725.000000000000000 489768.687500000000000 540855.997070310055278 689805.000000000000000 557554.000000000000000 
208.999975114708974 487.999969481999983 865.000000000000000 1298.999389650000012 1367.000000000000000 2181.000000000000000 2760.000000000000000 4338.000000000000000 4262.997070309999799 7572.994140630000402 5933.000000000000000 
13.999999910609001 258.999938965000013 700.000000000000000 4038.999450679999882 1804.000000000000000 2.000000000000000 3711.000000000000000 2.000000000000000 2893.997070309999799 9079.997070310000709 15039.000000000000000 
145.999961763708995 443.999969481999983 1364.999694820000059 3855.999755859999823 8110.000000000000000 18502.994140599999810 27661.000000000000000 57368.011718800000381 89620.958007809997071 275004.000000000000000 570516.009765999973752 
9875.999349509709646 30551.996093800000381 62530.008789100000286 125178.960938000003807 210236.000000000000000 341185.011719000001904 598850.648438000003807 965927.000000000000000 1534750.997070309938863 4654071.000000000000000 5840271.000000000000000 
7055.666341689709043 16131.997070299999905 15552.000000000000000 21378.000000000000000 51034.968750000000000 85692.000000000000000 134986.000000000000000 233489.000000000000000 280857.997070309997071 517137.687500000000000 554130.000000000000000 
184.000020891708999 1451.999877929999911 12273.999755900000309 28868.993164100000286 1043.000000000000000 10414.000000000000000 10303.000000000000000 3733.000000000000000 18091.997070310000709 30986.009765599999810 59903.011718800000381 
3310.999349509708736 5260.000000000000000 8476.997070310000709 12254.997070299999905 25941.000000000000000 63078.000000000000000 113956.937500000000000 171371.000000000000000 230371.997070309997071 522103.000000000000000 647156.687500000000000 
9667.666341689709952 20902.996093800000381 42888.011718800000381 76499.960937500000000 110618.000000000000000 175940.000000000000000 213728.000000000000000 193852.000000000000000 154408.997070309997071 322866.687500000000000 361995.000000000000000 
3845.999349509708736 22311.996093800000381 60476.008789100000286 114873.960938000003807 231065.000000000000000 433093.009765999973752 637188.687500000000000 736014.959961000015028 754746.997070310055278 1528714.000000000000000 1540701.000000000000000 
4525.999349509709646 16227.997070299999905 43726.997070299999905 75031.968750000000000 105352.000000000000000 129998.000000000000000 166115.000000000000000 219251.009766000002855 267542.997070309997071 643765.648438000003807 947828.000000000000000 
3696.999654679708783 10310.996826200000214 17780.000000000000000 20852.000000000000000 28198.009765599999810 50840.958007799999905 50278.000000000000000 87680.000000000000000 129214.997070309997071 398212.000000000000000 670668.687500000000000 
54.999992281209003 259.999938965000013 808.000000000000000 2680.999450679999882 8467.000000000000000 21471.997070299999905 32668.997070299999905 59477.968750000000000 90554.997070309997071 267148.000000000000000 571955.009765999973752 
1435.999959859708952 5265.999206540000159 15566.997070299999905 26677.000000000000000 38977.008789100000286 45958.960937500000000 53466.000000000000000 61751.000000000000000 40377.997070309997071 52698.000000000000000 63474.000000000000000 
1067.666341689709043 6814.000000000000000 30005.996093800000381 52336.011718800000381 113843.960938000003807 168087.000000000000000 227640.000000000000000 306229.000000000000000 340356.684570309997071 650659.000000000000000 878088.000000000000000 
</code></pre>
"
"0.06830624596856","0.0753197414644083","190080","<p>I've created a linear regression model in R that contains the following interaction terms.</p>

<pre><code>lm.data &lt;- lm(sharer_prob ~ sympathy + trust + fear + greed, na.action=NULL, data=data)
</code></pre>

<p>Greed, Sympathy, Trust and fear are independent variables with allowable values of 0, 1, 2, or 3. The response variable is sharer_prob, which has values from 0 to 1. The model contains the following interaction terms. </p>

<pre><code>IX_greed    &lt;- data$greed * data$sharer_prob
IX_sympathy &lt;- data$sympathy * data$sharer_prob
IX_fear     &lt;- data$fear * data$sharer_prob
IX_trust    &lt;- data$trust * data$sharer_prob
</code></pre>

<p>That makes it possible for me to regress pairs of the independent variables like so:</p>

<pre><code>lmFGData = lm( data$sharer_prob ~ IX_fear * IX_greed )
lmFSData = lm( data$sharer_prob ~ IX_fear * IX_sympathy )
lmFTData = lm( data$sharer_prob ~ IX_fear * IX_trust )
lmGSData = lm( data$sharer_prob ~ IX_greed * IX_sympathy )
lmGTData = lm( data$sharer_prob ~ IX_greed * IX_trust )
lmTSData = lm( data$sharer_prob ~ IX_trust * IX_sympathy ) 
</code></pre>

<p>Unfortunately, the resulting models fail three of the four assumptions for linear regression. So I created a new model that regresses the logit of sharer_prob against the independent variables like so:</p>

<pre><code>lm.Logitdata = lm(logit(sharer_prob, , ) ~ sympathy + trust + fear + greed, 
                  na.action=NULL, data=data)
</code></pre>

<p>How do I create expressions that regress the interacting pairs of variables? </p>

<ul>
<li><p>Option A: Use the same expressions, but change the name of the<br>
objects that represent each new model?</p></li>
<li><p>Option B: Create a new dataframe containing the independent
variables and the transformed response variable, and use that in each
expression?</p></li>
<li><p>Option C: Do something else?</p></li>
</ul>
"
"0.0633215847514023","0.0620651280774201","190389","<p>I built a conditional logistic regression with the function clogit (package survival) in R and in which I included one categorical independent variable (habitat type) with 15 levels. I noted that the sign of parameter estimates changed between models that were built for each level of the categorical independent variable and a model that included the categorical variable (thus, all levels). Contrary to the model including the categorical variable, the results of models for each level of the categorical variable made sense from a biological standpoint. Does sign changes signify a multicollinearity issue? However, in my case, the values of VIFs for each level of the categorical variable were &lt; 3. Should I group some levels of my categorical variable because I noted the levels that were significant, were often those with few observations ?</p>
"
"0.0693653206906364","0.0566574511374171","190748","<p>I am using <code>faraway::choccake</code> data, and I want to fit a linear model. 
I have used the following code:</p>

<pre><code>library(faraway)
attach(chocake)
choccake                 #to have a sense of the data 
choccake.lm&lt;-lm(breakang~recipe+batch+temp,data=choccake)
summary(choccake.lm) 
</code></pre>

<p>I have fitted linear model using 'lm' in R before too. But, here the output of <code>summary(choccake.lm)</code> looks a little different. </p>

<p>Here is the output (I'm attaching the choccake data too).</p>

<p><pre> > choccake
    recipe batch temp breakang
    1        1     1  175       42
    2        1     1  185       46
    3        1     1  195       47
    4        1     1  205       39
    5        1     1  215       53
    6        1     1  225       42
    7        1     2  175       47
    8        1     2  185       29
    9        1     2  195       35
    10       1     2  205       47
    11       1     2  215       57
    12       1     2  225       45
    13       1     3  175       32
    14       1     3  185       32
    15       1     3  195       37
    16       1     3  205       43
    17       1     3  215       45
    18       1     3  225       45
    19       1     4  175       26
    20       1     4  185       32
    21       1     4  195       35
    22       1     4  205       24
    23       1     4  215       39
    24       1     4  225       26
    25       1     5  175       28
    26       1     5  185       30
    27       1     5  195       31
    28       1     5  205       37
    29       1     5  215       41
    30       1     5  225       47
    31       1     6  175       24
    32       1     6  185       22
 33       1     6  195       22
34       1     6  205       29
35       1     6  215       35
36       1     6  225       26
37       1     7  175       26
38       1     7  185       23
39       1     7  195       25
40       1     7  205       27
41       1     7  215       33
42       1     7  225       35
43       1     8  175       24
44       1     8  185       33
45       1     8  195       23
46       1     8  205       32
47       1     8  215       31
48       1     8  225       34
49       1     9  175       24
50       1     9  185       27
51       1     9  195       28
52       1     9  205       33
53       1     9  215       34
54       1     9  225       23
55       1    10  175       24
56       1    10  185       33
57       1    10  195       27
58       1    10  205       31
59       1    10  215       30
60       1    10  225       33
61       1    11  175       33
62       1    11  185       39
63       1    11  195       33
64       1    11  205       28
65       1    11  215       33
66       1    11  225       30
67       1    12  175       28
68       1    12  185       31
69       1    12  195       27
70       1    12  205       39
71       1    12  215       35
72       1    12  225       43
73       1    13  175       29
74       1    13  185       28
75       1    13  195       31
76       1    13  205       29
77       1    13  215       37
78       1    13  225       33
79       1    14  175       24
80       1    14  185       40
81       1    14  195       29
82       1    14  205       40
83       1    14  215       40
84       1    14  225       31
85       1    15  175       26
86       1    15  185       28
87       1    15  195       32
88       1    15  205       25
89       1    15  215       37
90       1    15  225       33
91       2     1  175       39
92       2     1  185       46
93       2     1  195       51
94       2     1  205       49
95       2     1  215       55
96       2     1  225       42
97       2     2  175       35
98       2     2  185       46
99       2     2  195       47
100      2     2  205       39
101      2     2  215       52
102      2     2  225       61
103      2     3  175       34
104      2     3  185       30
105      2     3  195       42
106      2     3  205       35
107      2     3  215       42
108      2     3  225       35
109      2     4  175       25
110      2     4  185       26
111      2     4  195       28
112      2     4  205       46
113      2     4  215       37
114      2     4  225       37
115      2     5  175       31
116      2     5  185       30
117      2     5  195       29
118      2     5  205       35
119      2     5  215       40
120      2     5  225       36
121      2     6  175       24
122      2     6  185       29
123      2     6  195       29
124      2     6  205       29
125      2     6  215       24
126      2     6  225       35
127      2     7  175       22
128      2     7  185       25
129      2     7  195       26
130      2     7  205       26
131      2     7  215       29
132      2     7  225       36
133      2     8  175       26
134      2     8  185       23
135      2     8  195       24
136      2     8  205       31
137      2     8  215       27
138      2     8  225       37
139      2     9  175       27
140      2     9  185       26
141      2     9  195       32
142      2     9  205       28
143      2     9  215       32
144      2     9  225       33
145      2    10  175       21
146      2    10  185       24
147      2    10  195       24
148      2    10  205       27
149      2    10  215       37
150      2    10  225       30
151      2    11  175       20
152      2    11  185       27
153      2    11  195       33
154      2    11  205       31
155      2    11  215       28
156      2    11  225       33
157      2    12  175       23
158      2    12  185       28
159      2    12  195       31
160      2    12  205       34
161      2    12  215       31
162      2    12  225       29
163      2    13  175       32
164      2    13  185       35
165      2    13  195       30
166      2    13  205       27
167      2    13  215       35
168      2    13  225       30
169      2    14  175       23
170      2    14  185       25
171      2    14  195       22
172      2    14  205       19
173      2    14  215       21
174      2    14  225       35
175      2    15  175       21
176      2    15  185       21
177      2    15  195       28
178      2    15  205       26
179      2    15  215       27
180      2    15  225       20
181      3     1  175       46
182      3     1  185       44
183      3     1  195       45
184      3     1  205       46
185      3     1  215       48
186      3     1  225       63
187      3     2  175       43
188      3     2  185       43
189      3     2  195       43
190      3     2  205       46
191      3     2  215       47
192      3     2  225       58
193      3     3  175       33
194      3     3  185       24
195      3     3  195       40
196      3     3  205       37
197      3     3  215       41
198      3     3  225       38
199      3     4  175       38
200      3     4  185       41
201      3     4  195       38
202      3     4  205       30
203      3     4  215       36
204      3     4  225       35
205      3     5  175       21
206      3     5  185       25
207      3     5  195       31
208      3     5  205       35
209      3     5  215       33
210      3     5  225       23
211      3     6  175       24
212      3     6  185       33
213      3     6  195       30
214      3     6  205       30
215      3     6  215       37
216      3     6  225       35
217      3     7  175       20
218      3     7  185       21
219      3     7  195       31
220      3     7  205       24
221      3     7  215       30
222      3     7  225       33
223      3     8  175       24
224      3     8  185       23
225      3     8  195       21
226      3     8  205       24
227      3     8  215       21
228      3     8  225       35
229      3     9  175       24
230      3     9  185       18
231      3     9  195       21
232      3     9  205       26
233      3     9  215       28
234      3     9  225       28
235      3    10  175       26
236      3    10  185       28
237      3    10  195       27
238      3    10  205       27
239      3    10  215       35
240      3    10  225       35
241      3    11  175       28
242      3    11  185       25
243      3    11  195       26
244      3    11  205       25
245      3    11  215       38
246      3    11  225       28
247      3    12  175       24
248      3    12  185       30
249      3    12  195       28
250      3    12  205       35
251      3    12  215       33
252      3    12  225       28
253      3    13  175       28
254      3    13  185       29
255      3    13  195       43
256      3    13  205       28
257      3    13  215       33
258      3    13  225       37
259      3    14  175       19
260      3    14  185       22
261      3    14  195       27
262      3    14  205       25
263      3    14  215       25
264      3    14  225       35
265      3    15  175       21
266      3    15  185       28
267      3    15  195       25
268      3    15  205       25
269      3    15  215       31
270      3    15  225       25</pre></p>

<pre><code>choccake.lm&lt;-lm(breakang~recipe+batch+temp,data=choccake)

&gt; summary(choccake.lm)

Call:
lm(formula = breakang ~ recipe + batch + temp, data = choccake)

Residuals:
 Min       1Q   Median       3Q      Max 
-15.1851  -2.5682  -0.0419   2.7553  13.4816 

Coefficients:
         Estimate Std. Error t value Pr(&gt;|t|)    
      (Intercept)  16.22698    3.63640   4.462 1.22e-05 ***
      recipe2      -1.47778    0.71744  -2.060   0.0404 *  
      recipe3      -1.52222    0.71744  -2.122   0.0348 *  
      batch2       -1.27778    1.60424  -0.796   0.4265    
      batch3       -9.88889    1.60424  -6.164 2.79e-09 ***
       batch4      -13.55556    1.60424  -8.450 2.36e-15 ***
       batch5      -14.44444    1.60424  -9.004  &lt; 2e-16 ***
      batch6      -18.11111    1.60424 -11.289  &lt; 2e-16 ***
      batch7      -19.50000    1.60424 -12.155  &lt; 2e-16 ***
      batch8      -19.44444    1.60424 -12.121  &lt; 2e-16 ***
      batch9      -19.50000    1.60424 -12.155  &lt; 2e-16 ***
      batch10     -18.00000    1.60424 -11.220  &lt; 2e-16 ***
      batch11     -16.94444    1.60424 -10.562  &lt; 2e-16 ***
      batch12     -15.88889    1.60424  -9.904  &lt; 2e-16 ***
      batch13     -14.94444    1.60424  -9.316  &lt; 2e-16 ***
      batch14     -18.94444    1.60424 -11.809  &lt; 2e-16 ***
      batch15     -20.22222    1.60424 -12.605  &lt; 2e-16 ***
      temp          0.15803    0.01715   9.215  &lt; 2e-16 ***
      ---
     Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

     Residual standard error: 4.813 on 252 degrees of freedom
     Multiple R-squared:  0.6783,    Adjusted R-squared:  0.6566 
     F-statistic: 31.25 on 17 and 252 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>My questions: </p>

<ol>
<li>the variables are 'recipe', 'batch' and 'temp'..then why for different values of recipe and batch it's showing different coefficient? it seems from the result that there are 17 dependent variables.</li>
<li>why there's no mention of recipe1 and batch1? </li>
<li>is it by any chance computing for several different regression lines?</li>
</ol>
"
"0.02831827358943","0.0277563690826684","190899","<p>I installed the R package AER, from which you get the data <code>PSID1982</code>. Then I define this model:</p>

<pre><code>data(""PSID1982"")
set.seed(15606)
selectedobs = sample.int(nrow(PSID1982),size = 400, replace = FALSE)
attach(PSID1982);
experiencesq = experience^2
dgender = 1*(gender==""male"")
dmarried = 1*(married==""yes"")
dunion = 1*(union==""yes"")
dindustry = 1*(industry==""yes"")

outreg = lm(log(wage) ~ experience + experiencesq + education + dgender + 
                        dmarried + dunion + dindustry)
summary(outreg);
</code></pre>

<p>And now I need to find out if the impact of the amount of work <code>experience</code> on the <code>log(wage)</code> is different for women compared to men, and what do I expect it to be intuitively?</p>

<p>I want to divide the set of data into 2 subsets being men and women and perform a linear regression each dataset, so I do this. but then I get this error message and I don't know how to fix this.... </p>

<pre><code>outreg = lm(wage~ experience+experiencesq+education+dmarried+dunion+dindustry, 
            subset(PSID1982,gender==""male""))
Error in model.frame.default(formula = wage ~ experience + experiencesq +  : 
  variable lengths differ (found for 'experiencesq')
</code></pre>

<p>Thanks in advance!</p>
"
"0.0693653206906364","0.0679889413649005","191222","<p>I'm using R to fit a linear regression model and then I use this model to predict values but it does not predict very well boundary values. Do you know how to fix it?</p>

<p>ZLFPS is:</p>

<pre><code>ZLFPS&lt;-c(27.06,25.31,24.1,23.34,22.35,21.66,21.23,21.02,20.77,20.11,20.07,19.7,19.64,19.08,18.77,18.44,18.24,18.02,17.61,17.58,16.98,19.43,18.29,17.35,16.57,15.98,15.5,15.33,14.87,14.84,14.46,14.25,14.17,14.09,13.82,13.77,13.76,13.71,13.35,13.34,13.14,13.05,25.11,23.49,22.51,21.53,20.53,19.61,19.17,18.72,18.08,17.95,17.77,17.74,17.7,17.62,17.45,17.17,17.06,16.9,16.68,16.65,16.25,19.49,18.17,17.17,16.35,15.68,15.07,14.53,14.01,13.6,13.18,13.11,12.97,12.96,12.95,12.94,12.9,12.84,12.83,12.79,12.7,12.68,27.41,25.39,23.98,22.71,21.39,20.76,19.74,19.49,19.12,18.67,18.35,18.15,17.84,17.67,17.65,17.48,17.44,17.05,16.72,16.46,16.13,23.07,21.33,20.09,18.96,17.74,17.16,16.43,15.78,15.27,15.06,14.75,14.69,14.69,14.6,14.55,14.53,14.5,14.25,14.23,14.07,14.05,29.89,27.18,25.75,24.23,23.23,21.94,21.32,20.69,20.35,19.62,19.49,19.45,19,18.86,18.82,18.19,18.06,17.93,17.56,17.48,17.11,23.66,21.65,19.99,18.52,17.22,16.29,15.53,14.95,14.32,14.04,13.85,13.82,13.72,13.64,13.5,13.5,13.43,13.39,13.28,13.25,13.21,26.32,24.97,23.27,22.86,21.12,20.74,20.4,19.93,19.71,19.35,19.25,18.99,18.99,18.88,18.84,18.53,18.29,18.27,17.93,17.79,17.34,20.83,19.76,18.62,17.38,16.66,15.79,15.51,15.11,14.84,14.69,14.64,14.55,14.44,14.29,14.23,14.19,14.17,14.03,13.91,13.8,13.58,32.91,30.21,28.17,25.99,24.38,23.23,22.55,20.74,20.35,19.75,19.28,19.15,18.25,18.2,18.12,17.89,17.68,17.33,17.23,17.07,16.78,25.9,23.56,21.39,20.11,18.66,17.3,16.76,16.07,15.52,15.07,14.6,14.29,14.12,13.95,13.89,13.66,13.63,13.42,13.28,13.27,13.13,24.21,22.89,21.17,20.06,19.1,18.44,17.68,17.18,16.74,16.07,15.93,15.5,15.41,15.11,14.84,14.74,14.68,14.37,14.29,14.29,14.27,18.97,17.59,16.05,15.49,14.51,13.91,13.45,12.81,12.6,12,11.98,11.6,11.42,11.33,11.27,11.13,11.12,11.11,10.92,10.87,10.87,28.61,26.4,24.22,23.04,21.8,20.71,20.47,19.76,19.38,19.18,18.55,17.99,17.95,17.74,17.62,17.47,17.25,16.63,16.54,16.39,16.12,21.98,20.32,19.49,18.2,17.1,16.47,15.87,15.37,14.89,14.52,14.37,13.96,13.95,13.72,13.54,13.41,13.39,13.24,13.07,12.96,12.95,27.6,25.68,24.56,23.52,22.41,21.69,20.88,20.35,20.26,19.66,19.19,19.13,19.11,18.89,18.53,18.13,17.67,17.3,17.26,17.26,16.71,19.13,17.76,17.01,16.18,15.43,14.8,14.42,14,13.8,13.67,13.33,13.23,12.86,12.85,12.82,12.75,12.61,12.59,12.59,12.45,12.32)

QPZL&lt;-c(36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16)

ZLDBFSAO&lt;-c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2)    
</code></pre>

<p>My model is:</p>

<pre><code>fit32=lm(log(ZLFPS) ~ poly(QPZL,2,raw=T) + ZLDBFSAO)

results3 &lt;- coef(summary(fit32))

first3&lt;-as.numeric(results3[1])
second3&lt;-as.numeric(results3[2])
third3&lt;-as.numeric(results3[3])
fourth3&lt;-as.numeric(results3[4])
fifth3&lt;-as.numeric(results3[5])

#inverse model used for prediction of FPS
f1 &lt;- function(x) {first3 +second3*x +third3*x^2 + fourth3*1}
</code></pre>

<p>You can see my dataset <a href=""https://docs.google.com/spreadsheets/d/1vlc5c6qO973vgIKZaadL5J5nz5hpFFFSua4P2Qb1Kig/edit?usp=sharing"" rel=""nofollow"">here</a>. This dataset contains the values that I have to predict.  The FPS variation per QP is heterogenous. See dataset. I added a new column.
The fitted dataset is a different one.</p>

<p>To test the model just write <code>exp(f1(selected_QP))</code> where selected QP varies from 16 to 36. See the given dataset for QP values and the FPS value that the model should predict.</p>

<p>You can run the model online <a href=""http://www.r-fiddle.org/#/fiddle?id=E5J5usMi&amp;version=1"" rel=""nofollow"">here</a>.</p>

<p>When I'm using QP values in the middle, let's say between 23 and 32 the model predicts the FPS value pretty well. Otherwise, the prediction has big error value.</p>
"
"0.0490486886395286","0.0480754414848157","191381","<p>I have a quasi-poisson regression model for analysing the correlation between academic prestige and bulic visibility.
I have a set of independent variables- continouse and dummies in this model.
I would like to create scatter plots which describes this model and maby indicate of an interaction effects.
I understand that poisson models are log-linear models, so I tried to plot scatter plot in which the dependent variable is in log transformation.
However,  I don't know which line to choose- should it be a linear regression line or  loess line (I'm using ggplot2 plots).
for example, in the model, the economics department seem to be more visible then the sociology department. However, in the the plots it seem to be the oposite.
<a href=""http://i.stack.imgur.com/T6XMt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/T6XMt.png"" alt=""this is the plot with lm line""></a>
<a href=""http://i.stack.imgur.com/ZowTL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZowTL.png"" alt=""this is the plot with non-method line""></a></p>

<p>How can I make a reliable plot? </p>
"
"0.206456611170035","0.184292144646375","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.0490486886395286","0.0480754414848157","192256","<p>Here is a table with values that depend on two variables (VAR1=column names, VAR2=rownames).</p>

<pre><code>&gt; df &lt;- data.frame(""A""=c(1,1,2,2,2,3,3,3,3), ""B""=c(2,2,3,3,3,4,4,4,4), ""C""=c(3,3,4,4,4,5,5,5,5), ""D""=c(5,5,6,6,6,10,10,10,10), ""E""=c(10,10,15,15,15,20,20,20,20))

&gt; df
  A B C  D  E
1 1 2 3  5 10
2 1 2 3  5 10
3 2 3 4  6 15
4 2 3 4  6 15
5 2 3 4  6 15
6 3 4 5 10 20
7 3 4 5 10 20
8 3 4 5 10 20
9 3 4 5 10 20
</code></pre>

<p>I want to find out how much the values depend on VAR1 and VAR2.</p>

<p>As you can see, the data have levels: the rows (VAR2) 1-2, 3-5 and 6-9 all have the same values.</p>

<p>Looking at the data, what is the best approach to take for finding out the dependence of values on VAR1 and VAR2? Can this be done with linear regressions (values ~ VAR1, values ~ VAR2)?</p>
"
"0.0980973772790571","0.0961508829696314","192436","<p>I'm not a stats major and I'd appreciate any help I can get.
I've got data with each point defined by a score (between 0-1) and a frequency (0-100%). At 100%, the score is most reliable, at 1% the score is VERY UNRELIABLE. A score of 1 represents a very interesting case, 0.5 is not interesting. I realistically only care about scores between 0.5-1 (0 would technically be ""interesting"", but I'm keeping things one-tailed). This score is normally distributed, with mean centred around 0.5 (at least it should be). Ideally, a score of 1 and frequency of 100% would be the most interesting case. I'm trying to rank data from most interesting to least interesting. Standard score makes sense to me in order to rank this, but I'm having some trouble computing the variance.</p>

<p>Currently, I'm assuming the mean is consistent across all frequencies (0.5), and I know the observed score, so I only need to determine variance (which I'm assuming is different at any given frequency). The issue that is complicating this is that the frequencies are NOT normally distributed. The majority of the data (>99% of the data) falls below 5% frequency (and score at lower frequencies becomes increasingly quantized. At 0.006% frequency (the lowest frequency), scores are either 0 or 1. At 0.012% frequency, scores are either 0, 0.5 or 1. etc.. At most points above 15% frequency, only one data point exists, so I'm not sure at all how to calculate the variance given a single point and only an estimate of the mean based on normal distribution.</p>

<p>Is there a way to create a ranked list from most interesting to least? Is standard score even appropriate?</p>

<p>What I've tried so far on R (data represents the entire data set, col1 is a row.name, V2 (column2) is frequency, V3 (column3) is the score): </p>

<pre><code>    &gt; head(data)
                          V2  V3
    CREB3L1      0.013793103 1.0
    MMP2         0.006896552 0.0
    PCDHB15      0.020689655 1.0
    FEZF1        0.006896552 1.0
    TRAF3IP2-AS1 0.013793103 0.5
    PP12613      0.013793103 0.5
data$half &lt;- abs(data$v3-.5);
datalp &lt;- locpoly(data$V2, data$half, bandwidth=dpill(data$V2,data$half));
Error in if (!missing(bandwidth) &amp;&amp; bandwidth &lt;= 0) stop(""'bandwidth' must be strictly positive"") : 
  missing value where TRUE/FALSE needed
dpill(data$V2, data$half)
[1] NaN 
&gt; summary(data)
       V2                 V3        
 Min.   :0.006897   Min.   :0.0000  
 1st Qu.:0.006897   1st Qu.:0.0000  
 Median :0.006897   Median :0.5000  
 Mean   :0.010443   Mean   :0.5105  
 3rd Qu.:0.013793   3rd Qu.:1.0000  
 Max.   :0.868965   Max.   :1.0000  
plot(data)
</code></pre>

<p><a href=""http://i.stack.imgur.com/uiGr4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uiGr4.jpg"" alt=""enter image description here""></a></p>

<p>Above clearly does not work as I can't generate a regression estimate using dpill. I'm assuming it is because the data is so quantized and discrete at lower frequencies? Again, I am not limited to ranking based on standard score, if anyone has any better idea or a method to transform the data to a linear scale, the goal is just to rank using a single score the most to least interesting cases (where frequency determines reliability of score).</p>
"
"0.0908205236199223","0.0890184200641631","192714","<p>I want to estimate the current maximum capacity (in kWh) having the current power consumption (in kWh) and the state of charge of the battery (in %) available in a time series. </p>

<p>I do not have a full battery charge circle recorded but only a snippet with the state of charge going from ~95% to ~35% in a 1 second data recording interval.</p>

<p>At first, i cumulated the current power consumption and noticed that the progression between cumulated power consumption and state of charge was almost the same (see figure 1 and 2). </p>

<p><strong>progression of state of charge vs. cumulated power consumption</strong>
<a href=""http://i.stack.imgur.com/Y5tTp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Y5tTp.png"" alt=""enter image description here""></a></p>

<p>So i tried to use a linear regression model to predict two values of cumulated power consumption at 0% and 100% state of charge. In my assumption the delta of these two leaves me with the current maximum capacity of the battery.</p>

<p>Ideally i want to monitor the capacity during the data recording. Which means i only have data of a few percent of state of charge.
In figure 3 and 4 you can see my attempt of trying to create a linear regression model for every 2% state of charge. As you can see in the right plot, the estimated capacity has a very high variance.</p>

<p><strong>LEFT: regression lines (green) of every 2% state of charge; RIGHT: estimated capacity of every 2% state of charge</strong>
<a href=""http://i.stack.imgur.com/qOmlY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qOmlY.png"" alt=""enter image description here""></a></p>

<p>Since i am not an expert in neither the matter of regression nor batteries/physics, my questions are:</p>

<ul>
<li><p>Is there a much simple or more exact way (or both) to estimate the current maximum capacity of the battery?</p></li>
<li><p>If yes, is there a good way to estimate an sufficiently exact capacity with an even smaller snippet in an ongoing process, lets say every 2% state of charge? (maybe using a characteristic curve of the discharge process)</p></li>
</ul>
"
"0.0908205236199223","0.0964366217361766","192785","<p><strong>Objective</strong></p>

<p>I have a crossed and implicitly nested design and am trying to validate the correct â€˜maximalâ€™ model (including all linear and pairwise interactions of the variables) for use in <code>lmer()</code>.  I intend to use this as the starting point for some kind of backward stepwise regression, possibly making use of the function <code>mixed()</code> in the  <code>{afex}</code> package.</p>

<p><strong>Experimental design</strong></p>

<p>This a linguistics study.  We have 20 <code>Subjects</code>, each speaking 180 utterances, amounting to 3600 observations in total. Each utterance is initiated via prompting, and an associated Response Time is measured. Log Response Time is the dependent variable. </p>

<p><em>Conditions &amp; Blocks</em></p>

<p>The Response Time for the utterances is affected by 3 <code>Conditions</code> (coded 1 to 3). Each <code>Condition</code> is implemented by prompting the <code>Subject</code> to recite 1 of 4 <code>Blocks</code> of utterances (coded 1 to 12).</p>

<p><em>Words &amp; Tones</em></p>

<p>Each <code>Block</code> brings about its associated <code>Condition</code> via 15-utterance repetition of 3 carefully chosen <code>Words</code>.  There are a total of 12 <code>Words</code> used in the experiment (coded 1 to 12). The <code>Words</code> within each <code>Block</code> can also be categorized by <code>Tone</code> (coded 1 to 2).  There are 6 <code>Words</code> per <code>Tone</code>.  </p>

<p><em>Summary</em></p>

<p>Each of the 20 <code>Subjects</code> utter all 12 <code>Blocks</code> of 15 utterances each.  In doing so, they repeatedly utter all 12 <code>Words</code> (15 utterances per <code>Word</code>), and thereby use both <code>Tones</code> (90 utterances per <code>Tone</code>).</p>

<p>I would like to consider <code>Block</code>, <code>Word</code>, and <code>Subject</code> as random effects, and <code>Condition</code> and <code>Tone</code> as fixed.</p>

<p><strong>Proposed Model</strong></p>

<p>I think the model can be written in the following wayâ€¦</p>

<p><code>RT_log ~ Condition*Tone + (Condition*Tone|Subject) + (Condition|Word) + (Tone|Block)</code></p>

<p><strong>Questions</strong></p>

<p><strong>1.</strong> Is this the 'maximal' model (with linear plus pairwise interactions) appropriate for my experimental design?_ </p>

<p><strong>2.</strong> There is correlation between <code>Block</code> and <code>Condition</code> (there are only 4 possible blocks - out of the total 12 - for each <code>Condition</code>).  There is, similarly, correlation between <code>Word</code> and <code>Tone</code>.  Is it 'okay' to leave this correlation in the model? I don't see a good way of removing it.</p>

<p><strong>3.</strong> How will lme4 handle implicit nesting: I.e., the blocks, which are implicitly nested in the 3 conditions (i.e., only 4 blocks are applicable to each of the 3 conditions, even though the blocks are coded from 1 to 12), and the words, which are implicitly nested within the 2 tones (only 6 words are applicable to each tone, even though words are coded from 1 to 12)?</p>

<p><strong>4.</strong> Some <code>Blocks</code> utilize <code>Words</code> of only a single <code>Tone</code>, whereas other <code>Blocks</code> utilize words of both <code>Tones</code>.  Will that cause problems for the <code>(Tone|Block)</code> term in the model? It will only make sense for certain values of Block.</p>

<p><strong>5.</strong> It has been suggested by some that we might need a ""Subject:Word"" grouping (random effect).  Why might we need this grouping?</p>
"
"0.02831827358943","0.0277563690826684","192996","<p>In <code>getMethod(arm::sim, ""lm"")</code>, the source code shows that $\sigma$ is simulated from inverse chi square:</p>

<pre><code>for (s in 1:n.sims) {
            sigma[s] &lt;- sigma.hat * sqrt((n - k)/rchisq(1, n - 
                k))
            beta[s, ] &lt;- MASS::mvrnorm(1, beta.hat, V.beta * 
                sigma[s]^2)
        }
</code></pre>

<p>This makes sense if one is doing Bayesian linear regression with the default uniform prior for $(\beta, \log \sigma)$. But if I'm running frequentist MLE regression, isn't it wrong to use <code>arm::sim</code> to do simulation? Indeed, for MLE, the asymptotic variance of $\sigma^2$ is normal instead.</p>
"
"0.106794897528436","0.111218061863672","193187","<p><strong>Here is a description of my experimental design:</strong>  </p>

<ul>
<li>Randomized complete block design with 4 blocks/replications and a split-plot factorial design.  </li>
<li>I have the following treatments:     

<ol>
<li>2 agronomic practices (1 and 2) in main plots  </li>
<li>50 plant varieties in subplots  </li>
</ol></li>
</ul>

<p><strong>The measurements I took from each variety are:</strong>  </p>

<ul>
<li>plant weight = <code>pwtg</code>.  </li>
<li>content of substance X = <code>X</code>.</li>
</ul>

<p><strong>I fit the following model:</strong><br>
<strong>Model 1:</strong> </p>

<pre><code>lme(fixed = pwtg ~ Variety*X, random = ~1|Block)
</code></pre>

<p>where:<br>
<code>pwtg</code> = plant weight.<br>
<code>Variety</code> = factor for variety.<br>
<code>X</code> =  Continuous variable for content of substance X in the plant.   </p>

<p><strong>NOTICE:</strong> I am using <code>X</code> to estimate the effect of the agronomic practices (I hope this is not confusing) because when the varieties were grown with agronomic practice 1 they had low content of X and when the plants were grown with agronomic practice 2 they had high content of X.</p>

<p><strong>Objective:</strong> I need to estimate the intercept, slope, and their corresponding confidence intervals of the linear regression <code>pwtg ~ Variety*X</code>.  </p>

<p><strong>The Problem:</strong> Varieties have different variances of the normalized residuals when I use Model 1 and when I try to model the heterogeneity of variances as seen below:   </p>

<p><strong>Model 2:</strong>  </p>

<pre><code>lme(fixed = pwtg ~ Variety*X, random = ~1|Block, weights = varIdent(1|Variety))
</code></pre>

<p>The variances of residuals for each variety look more homogeneous but I get the following error:  </p>

<pre><code>[1] ""Non-positive definite approximate variance-covariance""
</code></pre>

<p><strong>My attempt at solving the problem:</strong><br>
I noticed the varieties that have small variances of residuals tend to have small plant weights and varieties with big variances of residuals tend to have big plant weights. I decided to create a new variable that will group the varieties by having small or big plant weights (I created 10 groups in total, e.g. group 1 = very very small weight, group 2 = very small weight, ..., group 9 =  very big weight, group 10 = very very big weight). Then fitted the following model:  </p>

<p><strong>Model 3:</strong>  </p>

<pre><code>lme(fixed = pwtg ~ Variety*X, random = ~1|Block, weights = varIdent(1|weightsize))
</code></pre>

<p>where weight size is a grouping variable with ten levels as described in the previous paragraph.  </p>

<p>Model 3 solves the heterogeneity of variances problem and does not give me a <code>Non-positive definite approximate variance-covariance</code> message. I don't know if this is a valid solution to the problem. Does anyone know if this procedure is OK? If not, are there any suggestions for a solution?   </p>
"
"0.0424774103841449","0.0555127381653369","193289","<p>Hi I am having trouble acquiring the final results for presentation. 
The results from a multiple regression are different to my results in a simple linear regression. </p>

<p>For example, the multiple regression model
<code>mul &lt;- lm(Response ~ Temp + Wave + Overcast, data = env)</code> gives me a p-value for 'Temp' of $0.01$, while the others are $&gt;0.05$.</p>

<p>However, a simple linear regression <code>sim &lt;- lm(Response ~ Temp, data =env)</code>
returns a p-value for 'Temp' of $0.03$.</p>

<p>Which result should I be presenting, and also how to I interpret the relationship between the predictors in the multiple regression?</p>
"
"0.0642198081225601","0.0734364498908627","193417","<p>I have an experiment where we measure the energy used by a building and want to regress this energy linearly against so-called degree-days, calculated with two different methods. The data looks like this:</p>

<p><a href=""http://i.stack.imgur.com/eR1yF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eR1yF.png"" alt=""enter image description here""></a></p>

<p>A regression line has been added to each group, that has been forced to go through the origin.</p>

<p>I want to compute the slope of these lines (with std. error), but I'm not sure what is the right way. My data looks like this:</p>

<pre><code>&gt; alvDegreeDays[sample(nrow(alvDegreeDays), 4),]
          Energy  BaseTemp DegreeDays
Feb 2014   984.7 Estimated   365.9771
Mar 2014   864.7 Estimated   307.2246
Apr 20151  512.8       SIA    50.0000
Sep 2015   239.2 Estimated    95.4787
</code></pre>

<p>I've tried this first:</p>

<blockquote>
  <p>lm(Energy ~ DegreeDays * BaseTemp + 0, alvDegreeDays)</p>
</blockquote>

<pre><code>Call:
lm(formula = Energy ~ DegreeDays * BaseTemp + 0, data = alvDegreeDays)

Coefficients:
            DegreeDays       BaseTempEstimated             BaseTempSIA  
                 2.436                  23.094                 174.390  
DegreeDays:BaseTempSIA  
                 1.181  
</code></pre>

<p>But this yields <code>BaseTempEstimated</code> and <code>BaseTempSIA</code> terms which are, in effect, intercept terms.</p>

<p>Next I tried the following:</p>

<blockquote>
  <p>(foo &lt;- lm(Energy ~ DegreeDays + DegreeDays:BaseTemp + 0, alvDegreeDays))</p>
</blockquote>

<pre><code>Call:
lm(formula = Energy ~ DegreeDays + DegreeDays:BaseTemp + 0, data = alvDegreeDays)

Coefficients:
                  DegreeDays  DegreeDays:BaseTempEstimated        DegreeDays:BaseTempSIA  
                       4.401                        -1.897                            NA  
</code></pre>

<p>This looks better, but when I try to call <code>predict</code> on this model I get weird error messages:</p>

<pre><code>&gt; predict(foo, list(DegreeDays = 1, BaseTemp = ""Estimated""))
       1 
2.504507 
Warning message:
In predict.lm(foo, list(DegreeDays = 1, BaseTemp = ""Estimated"")) :
  prediction from a rank-deficient fit may be misleading
</code></pre>

<p>Any idea what I may be doing wrong (or right) here?</p>
"
"0.0942489115008991","0.0846805485716084","193611","<p>The maximum capacity of a battery during usage can vary from manufacturer's specifications due to factors like temperatures and battery health. Because of that i want to estimate the real maximum capacity (in kWh) of a battery using the percentage of the battery's remaining capacity (in %) and the current power consumption (in kW).</p>

<p>I will get the above mentioned measurements in real time, in a time interval of every second, i.e., in a time series. During data recording, i will never see a full battery charge cycle but instead the battery will be between ~95% and ~35% percent.</p>

<p>In my data exploration, i cumulated the current power consumption, visualized it together with the battery percentage and noticed that they share a linear dependency, as you can see in figure 1.</p>

<p><strong>Figure 1: progression of battery capacity percentage and cumulated power consumption</strong>
<a href=""http://i.stack.imgur.com/Y5tTp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Y5tTp.png"" alt=""enter image description here""></a></p>

<p>My approach of holding measurements in a bin for every 2% lost battery capacity and performing on that bin a linear regression to predict the cumulated power consumption at zero percent battery capacity is giving me rather unstable results. (see figure 2)</p>

<p><strong>Figure 2: estimated capacity of every two percent battery capacity</strong>
<a href=""http://i.stack.imgur.com/CB2jq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CB2jq.png"" alt=""enter image description here""></a></p>

<ul>
<li><p>Is there a way to get the results in a more stable (less varying) state?</p></li>
<li><p>Or is there even a more exact way to estimate the current maximum capacity of the battery using a different method?</p></li>
</ul>
"
"0.0749231094763201","0.0734364498908627","194293","<p>I want to be able to calculate the confidence interval from the estimated coefficient and respective standard errors.</p>

<p>I have a linear regression model which can be summarized (in R):</p>

<pre><code>summary(fit1)

Call:
lm(formula = bwt ~ height + weight + parity, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-66.913 -10.624   0.991  10.979  55.621 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 31.19217   13.56879   2.299   0.0217 *  
height       1.24964    0.23083   5.414 7.48e-08 ***
weight       0.06781    0.02823   2.402   0.0164 *  
parity1     -1.83309    1.19838  -1.530   0.1264    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 17.9 on 1170 degrees of freedom
Multiple R-squared:  0.04898,   Adjusted R-squared:  0.04654 
F-statistic: 20.08 on 3 and 1170 DF,  p-value: 1.071e-12
</code></pre>

<p>With this model I can calculate the respective confidence intervals:</p>

<pre><code>&gt; confint(fit1)
                  2.5 %     97.5 %
(Intercept)  4.57029503 57.8140351
height       0.79676227  1.7025207
weight       0.01243198  0.1231932
parity1     -4.18429933  0.5181151
</code></pre>

<p>I would expect the intervals of the predictor <em>height</em> to be given by</p>

<p>$$
1.24964 \pm (1.96*0.23083) = [0.7972132,1.702067]
$$</p>

<p>where 1.24964 is the estimated value for the coefficient and 0.23083 is the standard error for this coefficient. The numbers are close but not quite the same.</p>

<p>What am I doing wrong?</p>
"
"0.109676202005208","0.107499955208361","194597","<p>I am doing a meta-regression with metafor package in R. The mixed-effect model for proportion is used to assess the linearity between study performed year and medication prevalence. Here below is my script in R:</p>

<pre><code>model_A &lt;- rma.glmm(xi=A, ni=Sample, measure=""PLO"", mods=~year)
print(model_A)
</code></pre>

<p>And results I got from R are:</p>

<pre><code>Mixed-Effects Model (k = 32; tau^2 estimator: ML)

tau^2 (estimated amount of residual heterogeneity):     1.6349
tau (square root of estimated tau^2 value):             1.2786
I^2 (residual heterogeneity / unaccounted variability): 99.40%
H^2 (unaccounted variability / sampling variability):   168.00

Tests for Residual Heterogeneity: 
Wld(df = 30) = 2221.4535, p-val &lt; .0001
LRT(df = 30) = 3187.7073, p-val &lt; .0001

Test of Moderators (coefficient(s) 2): 
QM(df = 1) = 22.7322, p-val &lt; .0001

Model Results:

          estimate        se     zval    pval      ci.lb      ci.ub
intrcpt  -554.8145  116.4605  -4.7640  &lt;.0001  -783.0728  -326.5561  ***
year        0.2767    0.0580   4.7678  &lt;.0001     0.1630     0.3905  ***

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Followed by this model, I would also like to perform a scatterplot in R. So my script is:</p>

<pre><code>wi &lt;- 0.5/sqrt(dat$vi)
preds &lt;- predict(model_A, transf = transf.ilogit, addx=TRUE)
plot(year, transf.ilogit(dat$yi), cex=wi)
lines(year, preds$pred)
</code></pre>

<p>The plot I got is: 
<a href=""http://i.stack.imgur.com/7Ej3P.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7Ej3P.png"" alt=""enter image description here""></a></p>

<p>Apparently, it doesn't seem right!. So my questions are:</p>

<ol>
<li><p>Did I use the right model with <code>rma.glmm</code>?</p></li>
<li><p>How could I weight individual study (<code>cex=wi</code>?)? How to calculate standard error for individual study?</p></li>
<li><p>How could I fit a right estimated line in scatterplot?</p></li>
</ol>

<p>Many thanks.</p>

<p>Updates:</p>

<p>Followed by Wolfgang's suggestions, I managed to rescale the bubble and get predicted line fitted (the model remains the same):</p>

<p><a href=""http://i.stack.imgur.com/u8N0t.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/u8N0t.png"" alt=""enter image description here""></a></p>

<p>Obviously, the line wasn't straight! Should I change model into polynomial regression? Or is that normal with this graph?</p>

<p>I tried polynomial model like:</p>

<blockquote>
  <p>model1&lt;-rma.glmm(xi=A, ni=Sample, measure=""PLO"", mods=~year+I(year^2))</p>
</blockquote>

<p>The error came with ""Error in print(model1) : 
  error in evaluating the argument 'x' in selecting a method for function 'print': Error: object 'model1' not found""</p>

<p>And I tried another model:</p>

<blockquote>
  <p>model2: model2&lt;-rma.glmm(xi=A, ni=Sample, measure=""PLO"", mods=~year+year^2)</p>
</blockquote>

<p>I got exactly the same result as original model, which has only the year as covariate fitted. I am not sure where the problem is....</p>

<p>Many thanks!</p>

<p>Min</p>
"
"NaN","NaN","194885","<p>Suppose I have following non-periodic time series. Obviously the trend is decreasing and I would like to prove it by some test (with <em>p-value</em>). <strong>I am unable to use classic linear regression due to strong temporal (serial) auto-correlation among values.</strong></p>

<pre><code>library(forecast)
my.ts &lt;- ts(c(10,11,11.5,10,10.1,9,11,10,8,9,9,
               6,5,5,4,3,3,2,1,2,4,4,2,1,1,0.5,1),
            start = 1, end = 27,frequency = 1)
plot(my.ts, col = ""black"", type = ""p"",
     pch = 20, cex = 1.2, ylim = c(0,13))
# line of moving averages 
lines(ma(my.ts,3),col=""red"", lty = 2, lwd = 2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/PtQje.png""><img src=""http://i.stack.imgur.com/PtQje.png"" alt=""enter image description here""></a></p>

<p>What are my options?</p>
"
"0.0400480865731637","0.039253433598943","194897","<p>I have a data which includes 8 variables:</p>

<p>1- The first variable contains grades given by a voice expert to participants' voices. It takes any value from 0 to 3 with mode 0 (so zero inflated)</p>

<p>2- The rest of variables are outputs from machine and some of them are highly correlated.</p>

<p>The aim is to find a model of the first variable on the rest 7 variables so that an inexpert person can find the grade of voice by puttingÂ only the outputs from machine in the model.</p>

<p>So far I have found that I can use Tweedie glm. Is this suitable for the aim of this study? or there are better options?</p>

<p>Some variables are highly correlated but there is no preference of one over the others; We don't know which one of them are more appropriate to keep. Is there any function like stepwise in linear regression that can help?</p>

<p>I have softwares R and Stata available.</p>

<p>I hope it is a question within the site criteria!!</p>
"
"0.109676202005208","0.0931666278472465","195293","<p>I thought I understood this issue, but now I'm not as sure and I'd like to check with others before I proceed.</p>

<p>I have two variables, <code>X</code> and <code>Y</code>. <code>Y</code> is a ratio, and it is not bounded by 0 and 1 and is generally normally distributed. <code>X</code> is a proportion, and it is bounded by 0 and 1 (it runs from 0.0 to 0.6). When I run a linear regression of <code>Y ~ X</code> and I find out that <code>X</code> and <code>Y</code> are significantly linearly related. So far, so good.</p>

<p>But then I investigate further and I start to think that maybe <code>X</code> and <code>Y</code>'s relationship might be more curvilinear than linear. To me, it looks like the relationship of <code>X</code> and <code>Y</code> might be closer to <code>Y ~ log(X)</code>, <code>Y ~ sqrt(X)</code>, or <code>Y ~ X + X^2</code>, or something like that. I have empirical reasons to assume the relationship might be curvilinear, but not reasons to assume that any one non-linear relationship might be better than any other. </p>

<p>I have a couple of related questions from here. First, my <code>X</code> variable takes four values: 0, 0.2, 0.4, and 0.6. When I log- or square-root-transform these data, the spacing between these values distorts so that the 0 values are much further away from all the others. For lack of a better way of asking, is this what I want? I assume it isn't, because I get very different results depending on the level of distortion I accept. If this isn't what I want, how should I avoid it?</p>

<p>Second, to log-transform these data, I have to add some amount to each <code>X</code> value because you can't take the log of 0. When I add a very small amount, say 0.001, I get very substantial distortion. When I add a larger amount, say 1, I get very little distortion. Is there a ""correct"" amount to add to an <code>X</code> variable? Or is it inappropriate to add <em>anything</em> to an <code>X</code> variable in lieu of choosing an alternative transformation (e.g. cube-root) or model (e.g. logistic regression)? </p>

<p>What little I've been able to find out there on this issue leaves me feeling like I should tread carefully. For fellow R users, this code would create some data with a sort of similar structure as mine.</p>

<pre><code>X = rep(c(0, 0.2,0.4,0.6), each = 20)
Y1 = runif(20, 6, 10)
Y2 = runif(20, 6, 9.5)
Y3 = runif(20, 6, 9)
Y4 = runif(20, 6, 8.5)
Y = c(Y4, Y3, Y2, Y1)
plot(Y~X)
</code></pre>
"
"0.0942489115008991","0.100077011948264","195306","<p>I am trying to compare and plot regression lines of an ANCOVA when their slopes are the same and no interaction effect </p>

<p>There are two sets of ANCOVAs plots that I am trying to create:</p>

<p>a) 1 categorical variable (2 levels) and 1 covariate and;
b) 1 categorical variable (2 levels) and 2 covariates</p>

<p>For a) I didn't have much of an issue plotting the data, the code I used was:</p>

<blockquote>
  <p>modSMR &lt;- lm(SMR ~ CROSS + BM, data = CTMB)</p>
  
  <p>predSMR &lt;- predict(modSMR) </p>
  
  <p>ggplot(data = cbind(CTMB, predSMR), aes(SMR, BM, shape=CROSS)) +    geom_line(aes(y=predSMR)</p>
</blockquote>

<p>This code more or less created what I was looking for, a plot with two, straight, parallel regression lines running through the data points.</p>

<p>However once I add in the second covariate everything seems to be turned upside down.</p>

<p>In attempt to plot b) I tried using this code:</p>

<blockquote>
  <p>modSMR.ET &lt;- lm(ET ~ CROSS + SMR +  BM, data = CTMB)</p>
  
  <p>predSMR.ET &lt;- predict(modSMR.ET) </p>
  
  <p>ggplot(data = cbind(CTMB, predSMR.ET), aes(SMR, ET, shape=CROSS)) +    geom_line(aes(y=predSMR.ET)</p>
</blockquote>

<p>With this code, the regression lines were all wonky. They seemed to be oriented in a parallel fashion but they were no where near being straight or displayed the same patterns.  </p>

<p>I have tried taking the residuals of SMR and BM as well as adjusting SMR to a mean BM and plugging in to the model so that there is only 1 covariate but neither method produces the straight parallel lines I am looking for. I am not sure why I was able to produce them in a) using 1 covariate but not in b) when using 1 covariate. </p>

<p>Ideally I would like to create the plot with the 2 covariates described above. </p>

<p>*Note: SMR and BM are correlated with one another so there is some collinearity there however I am not sure how it will effect the model. I did try taking the residuals of the two as well as adjusting SMR to a mean BM in attempt to solve this issue but I ended up with similar results.</p>

<p>I hope this all makes sense and I look forward to hearing some suggestions</p>

<p>Thanks in advance </p>
"
"0.126643169502805","0.12413025615484","195359","<p>I have a set of complex survey data with sampling weights. I am using the <code>svyglm()</code> function from the <code>survey</code> package in R to describe the relationship between 2 variables in a GLM. I am using the quasipoisson family because both variables are over-dispersed. </p>

<p>The GLM output is as follows:</p>

<pre><code>hlsereg &lt;- svyglm(formula = HLSEPALLACRESFIX ~ HLSE_ACRE, sbdiv, family = quasipoisson)

Survey design:
svydesign(id = ~1, weights = ~spwgtdividedby3, data = sportsbind)

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.489465   0.414979  13.228   &lt;2e-16 ***
HLSE_ACRE   -0.002744   0.001118  -2.454   0.0144 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 2.601914e+15)

Number of Fisher Scoring iterations: 12
</code></pre>

<p>I have used the <code>predict()</code> and <code>lines()</code> function to plot this model output:</p>

<pre><code>acreaxis &lt;- seq(0,2000,.1)
hlse = predict(hlsereg, list(HLSE_ACRE = acreaxis))
    plot(jitter(sportsbind$HLSE_ACRE,  amount = 2.5), jitter(sportsbind$HLSEPALLACRESFIX),pch = 16,  xlab = ""Acres"", ylab = ""Price per person per acre"",  xlim = c(0, 350), ylim = c(0,35), col=alpha(""red"",.35), font = 2, font.lab = 2)
    lines(acreaxis, hlse, lwd=4, col = ""red"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/3EUZ6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3EUZ6.png"" alt=""enter image description here""></a></p>

<p>This plots a line given by the regression output of an intercept at 5.5 and a very slow negative slope of -.003, but I'm uncertain if this is a correct representation of the line.</p>

<p>I have found others using the <code>predict(..., type = ""response"")</code> option, which is shown in various plots of quasipoisson models, including the one found by @Glen_b at <a href=""http://stats.stackexchange.com/a/177926/45582"">this question</a> and for <a href=""http://stats.stackexchange.com/questions/38201/problems-plotting-glm-data-of-binomial-proportional-data?rq=1"">binomial GLMs here</a>. The <code>predict.glm()</code> help page notes for the <code>type</code> argument that: ""The default is on the scale of the linear predictors; the alternative ""response"" is on the scale of the response variable."" I just don't understand what that means.  The ""response"" type yields a very different prediction line, which is curved and at a much higher value (note the scale of the y-axis, with an intercept at ~250):</p>

<pre><code>hlse = predict(hlsereg, list(HLSE_ACRE = acreaxis), type = ""response"")
plot(jitter(sportsbind$HLSE_ACRE,  amount = 2.5), jitter(sportsbind$HLSEPALLACRESFIX),pch = 16,  xlab = ""Acres"", ylab = ""Price per person per acre"",  xlim = c(0, 350), ylim = c(0,400), col=alpha(""red""),     font = 2, font.lab = 2)
lines(acreaxis, hlse, lwd=4, col = ""black"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/jnY9T.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jnY9T.png"" alt=""enter image description here""></a></p>

<p>I have also tried to run a GLM using the negative binomial distribution, but despite inputting the quasipoisson coefficient values for starting values, the model can't find valid coefficients (I have purged all zeros from the data):</p>

<pre><code> hlsereg.nb &lt;- glm.nb(HLSEPALLACRESFIX~HLSE_ACRE,data = model.frame(sbdiv.scaledweights), start = c(5.45, -.003))
Error: no valid set of coefficients has been found: please supply starting values
In addition: Warning message:
glm.fit: fitted rates numerically 0 occurred 
</code></pre>

<p>My questions:</p>

<p>1) What is the most appropriate illustration of the GLM output from a quasipoisson family?<br>
2) If the negative binomial is more appropriate to describe this relationship, why can't it find a coefficient? If I figure out how to get it to find a coefficient, how would I visualize that output?</p>
"
"0.0424774103841449","0.0416345536240027","195478","<p>Resampling is usually used to find the best tuning parameters for a model. However, for some models, such as linear regression model, there is no tuning parameters. In this case, what can we get from resampling on them?</p>

<p>In particular, in R caret package, you can train a linear regression model by using cross validation control function. In this case, how is the coefficient estimated? On the whole training sample? If so, what extra information can we get from applying CV on linear regression models?</p>

<p>Thank you.</p>
"
"0.0506572678011219","0.0620651280774201","196839","<p>From a data stream i'm receiving a pair of measurements consisting of a current consumption and a current percentage every second. By accumulating the consumption over time it will represent eventually the maximum capacity when the percentage reaches from 100% to 0%.</p>

<p>I want to predict the maximum capacity in (almost) real time using linear regression with a small sample size window of two percent. However, when i compare the models of these local regressions of every two percent with the model of the whole data regression, i get very different results due to perhaps local fluctuation. (see figure)</p>

<p>Is there a way to bring the local regression models closer to the whole data model? (in a way that i can see the differences due to fluctuation but overall closer predictions to the whole data model)</p>

<p><a href=""http://i.stack.imgur.com/e1c8w.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/e1c8w.png"" alt=""enter image description here""></a></p>
"
"0.0983889005882491","0.0964366217361766","196901","<p>I'm trying to figure out how to find the marginal effect of an interaction term from a restricted cubic spline in a non-linear model.  The post <a href=""http://stats.stackexchange.com/questions/134526/nonlinear-effect-in-an-interaction-term"">Nonlinear effect in an interaction term</a> is a good start on modeling the nonlinear effects and how to get plots, but does not address finding the marginal effect.  </p>

<p>The package <a href=""http://maartenbuis.nl/software/postrcspline.html"" rel=""nofollow"">postrcspline</a> in <code>STATA</code> has a function <a href=""http://repec.org/bocode/m/mfxrcspline.html"" rel=""nofollow"">mfxrcspline</a> which ""displays the marginal effect of a restricted cubic spline,""
 which is exactly what I am after. (See Figure 1 below)  </p>

<p>R does not seem to offer this feature as conveniently ,so I'm trying to figure out how to get these same results.</p>

<p>As I understand it, suppose I have a multi-variable regression with restricted cubic splines and an interaction:</p>

<p>$$y = \beta_{0} + \beta_{1}x1 + \beta_{2} \mathcal{f}(x2) + \beta_{3} \mathcal{f}(x2) \cdot x1 + \epsilon$$</p>

<p>where $\mathcal{f}(x2)$ is a spline of the time-series (year)</p>

<p>The marginal effect of $\frac{\partial y}{\partial x1}$ is:</p>

<p>$$\frac{\partial y}{\partial x1} = \beta_{1} + \beta_{3} \mathcal{f}(x2)$$</p>

<p>where $\beta_{3}$ is the coefficient on the spline and $ \mathcal{f}(x2)$ is a design matrix for each year in the regression that causes the slope to change for each $y$.  </p>

<p>To say in words, I would like to find the marginal effect of $y$ for each year $x2$ in the spline given $\beta_{3}$.  </p>

<p>In other words, it shows for each value of the spline variable how much the expected value of your explained variable changes for a unit change in the spline variable. It is the first derivative of the curve.</p>

<p>This appears to be simple matrix multiplication to plot the marginal effect, but I'm not sure how to statistically do this.  </p>

<p>Here is a plot to illustrate what I'm after:</p>

<p><strong>Figure 1:</strong> The left plot shows the results of the regression using a restricted cubic spline and the right provides the marginal effect--note the changes on the y-axis.
<a href=""http://i.stack.imgur.com/uqcX4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uqcX4.png"" alt=""Figure 1""></a></p>

<hr>

<p>Here is an R example to demonstrate the nonlinear effect from the regression (left plot in Figure 1):</p>

<pre><code>library(rms)
set.seed(5)
# Fit a complex model and approximate it with a simple one
x1 &lt;- runif(200)
x2 &lt;- runif(200)
y &lt;- x1 + x2 + rnorm(200)
f &lt;- ols(y ~ x1 + rcs(x2,4)  + rcs(x2,4)*x1)
ddist &lt;- datadist(x1,x2)
options(datadist='ddist')
plot(Predict(f))
</code></pre>

<p><a href=""http://i.stack.imgur.com/DAuXS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DAuXS.png"" alt=""enter image description here""></a></p>
"
"0.0800961731463273","0.078506867197886","197337","<p>I have monthly temperature averages for a weather station across 100 years.
I am wondering how I should analyze this data in a regression. </p>

<p>The data are set up in the following fashion:</p>

<pre><code>year  month  temp.avg
1900    11      9 
1900    12      6       
1901    01      5 
1901    02      4 
....
2015    12      7 
</code></pre>

<p>My question is: <strong>how do I go about accounting for the time and incorporating it into my model?</strong></p>

<p>Here are my <strong>4 proposed methods:</strong></p>

<ol>
<li><p>Should I add a ""time"" variable which essentially counts my months from 1 through <em>n</em> rows of data? </p>

<ul>
<li>Ex: Using the example above, I would have a ""time"" column of 1,2,3,4,etc.</li>
<li>I essentially lose the actual year &amp; month data, but does this matter? -- can I just add back, for example, ""Dec 1900"" for time 1?</li>
</ul></li>
<li><p>Given a month <em>i</em> in year <em>j</em>, should I create a continuous time by adding 0.0833 (1/12) to each year for each i-1 month? </p>

<ul>
<li><p>Ex. Again using the above example, I would have a ""time"" column consisting of values 1900, 1900.8333, 1900.9167, 1901.0000, 1901.0833, etc...</p></li>
<li><p>The linear regression model for the prior two methods would essentially be: <code>lm(temp.avg ~ time)</code></p></li>
</ul></li>
<li><p>Do I just incorporate year and month (or perhaps more usefully <em>season</em>) in the model together?</p>

<ul>
<li>This would result in: <code>lm(temp.avg ~ year + month)</code></li>
</ul></li>
<li><p>Or is 3 wrong and instead I'd have to create a dummy variable for each month (or season)?</p>

<ul>
<li><code>lm(temp.avg ~ year + jan + feb + mar + apr ...)</code></li>
</ul></li>
</ol>

<p>So <strong>which is correct?</strong> 
I assume perhaps the questions I'm asking would dictate this to some degree. But perhaps someone could describe simply the validity of each method and when to apply each?</p>

<p>Note: I understand that I will have to account for temporal autocorrelation, but I'm wondering how I incorporate time data <em>prior</em> to worrying about that.</p>

<p>I will note that I perform my analyses in <code>R</code>.</p>
"
"0.0849548207682898","0.0832691072480053","197488","<p>How would you go about making an unbiased comparison of two interventions (old vs. New <em>prtcl.binary</em> (0 and 1; individual worksheets vs. group work).) when there's a negative longitudinal slope present (longitudinal meaning the data is measured over time, but not repeatedly since each student only had one measurement)?
<a href=""http://i.stack.imgur.com/jRP29.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jRP29.png"" alt=""enter image description here""></a></p>

<p>The switch happened in 2014 (NOTE: there's little data for 2015, ~5 cases, so you can pretty much group it with 2014) But the mean comparison seems a little questionable when the previous intervention was having an effect already, see the negative slope before 2014.</p>

<p>So if my outcome is a count (i.e., number of days student did Y behavior), that is overdispersed (so I'm using a negative binomial dist.)</p>

<p>I was concerned with just comparing the one protocol to another using <code>prtcl.binary</code> as a dummy variable, so I added <code>year</code> to adjust the means for the the linear effect (black line) and an interaction between <code>year</code> and <code>prtcl.binary</code>, the differential, to compare the slopes. But I'm not sure this fixes my problem with interpreting the main effect of <code>prtcl.binary</code>.</p>

<pre><code>MASS::glm.nb(formula = y.count ~ 1 + prtcl.binary + year + prtcl.binary : year, data = d0)
</code></pre>

<p>Someone told me this is a good case for detrending data (but I'm not fond of the idea of interpreting the regression coefficients of residuals; i.e., I'm dumb). Someone else suggested I used a mixed model and use year as a random effect.
I thought it might make sense to use the 2008 to 2013 data to predict a value for 2014 with a standard error and then compare it with the mean and standard error measured in just 2014.</p>

<p>Suggestions?</p>
"
"0.0566365471788599","0.0555127381653369","197566","<p>I have 2 dependent variables which depend upon on 5 independent variables. So, I performed multivariate multiple linear regression in R and got the coefficients for my variable of interest for both the dependent variable. For one dependent variable, I got a coefficient of -4 and for the other dependent variable for the same variable of interest, I got a coefficient of 8. These are hypothetical numbers. I want to statistically compare if there is any relationship between 2 dependent variables. What is the way to do that in R?</p>

<p>Any help is appreciated. Thank you.</p>
"
"0.120311011027664","0.12413025615484","197710","<p><strong>Experiment:</strong></p>

<p>I have 2 groups and both groups undergo 2 set of evaluations, one with MRI scanner and the other in the lab to test for their behavior. Both these evaluations are known to have statistically significant relationship with age and gender. </p>

<p>Statistical questions:</p>

<p>Whether there is: </p>

<p>1) statistically significant difference between the 2 groups on each evaluation? </p>

<p>2) any relationship between and within the 2 groups between each evaluation?  </p>

<p><strong>Model:</strong></p>

<p>I model the problem as </p>

<p>$\text{MRI_measure} = \beta_{0} + \beta_{1} \text{Age} + \beta_{2} \text{Gender} + \beta_{3} \text{Group} $</p>

<p>$\text{Lab_measure} = \beta_{0}+ \beta_{1}  \text{Age} + \beta_{2}  \text{Gender} +\beta_{3}  \text{Group}$</p>

<p>[Age is continuous and Gender, Group are factors/categorical] </p>

<p>In R: </p>

<pre><code>MRI_model&lt;-lm(cbind(MRI_measure, Lab_measure) ~ age+gender+group, data=data) 
</code></pre>

<p><strong>Result of R:</strong> </p>

<p><code>manova(MRI_model)</code> suggests that yes indeed all the slopes are significantly different than 0 suggesting a relationship between my measures. </p>

<p><strong>Questions</strong> </p>

<p>1) In order to test whether the difference in the MRI_measure is statistically significant between the 2 groups, I use MRI_model$fitted.values for each dependent measure and do a statistical test (either t-test or Wilcox) and claim that the difference is significant. </p>

<p>In the paper I write, multivariate multiple linear regression was performed for the groups while controlling for age and gender. The regressed out MRI_measure was statistically compared to see if the difference is different. </p>

<p>I am assuming that the predicted/fitted.values in model are the regressed out variables. Can I show this and use this result? Is this right? </p>

<p>If no, what is the correct way to statistically compare whether my 2 groups differ in their MRI measure and lab measure when controlled for age and gender. Any R library, literature, possibly a script will be greatly appreciated. </p>

<p>2) I also want to see if there is any relationship between MRI_measure and Lab_measure within the group after they are controlled for age and gender. What is the correct way to do this in R? </p>

<p>Further, I also want to see if there is any significantly different association between the 2 groups for my set of dependent variables. I am thinking of first finding the correlation between 2 dependent variable in each group and test if this correlation is statistically different between the 2 groups? Is this logic right? And if it is, how do I compare the correlation? If not, what is the right way to do this? Any R library, literature, possibly a script will be greatly appreciated. </p>
"
"0.02831827358943","0.0277563690826684","198007","<p>Could someone clarify the purpose of the <em>drop in dispersion test</em> (<code>drop.test</code>) in the <a href=""https://cran.r-project.org/web/packages/Rfit/index.html"" rel=""nofollow"">Rfit package</a> in R? I think that it shows the difference in explanatory power between the full and reduced models in linear regression. I don't understand where 'dispersion' comes into it.</p>

<p>The output for the drop in dispersion test of my models is:  </p>

<pre><code>F-statistic = 1.90451, p-value = 0.14911... 
</code></pre>

<p>How would I interpret this other than not being significant?</p>

<p>NB. to clarify, I do not need help with the code, just the interpretation of the test output results! </p>
"
"0.07684452671463","0.0836886016271203","198221","<p>I have a data set from which I am trying to establish a relationship between drug level (the level of a drug administered) and the amount of allergens present. In the data set is the level of drug administered, the number of allergens, the race and the gender of the participant. I cannot figure out how to do what I am trying to achieve. Basically I need to see if there is a relationship between how much drug is administered and the levels of allergens and based on the persons gender and race. In my data set there is male and female for genders and Latino,Black,White,Indian, and Asian for the races. So 12 possible models to account for. I tried narrowing the data for each based on their gender and race and then performing a regression but I can't use that model as the residual plot is linear. Which is a problem as I have to be able to justify my model with residuals. Any ideas? Thank you for your help. The first 50 data points are below</p>

<pre><code>Patient Allergens Druglevel           Gender Race
1 1105.89222071003 0                  Male NativeAmerican
2 946.532242718927 0.0001000100010001 Female White
3 961.784556253583 0.0002000200020002 Female White
4 761.633983582958 0.0003000300030003 Male White
5 789.287202723709 0.0004000400040004 Male White
6 1082.21802116621 0.0005000500050005 Male Latino
7 1082.41599284693 0.0006000600060006 Male Latino
8 901.514449034478 0.0007000700070007 Male White
9 848.869529267382 0.0008000800080008 Male Indian
10 988.89267695125 0.0009000900090009 Female White
11 1096.06385274373 0.001000100010001 Female White
12 1174.40425047674 0.0011001100110011 Male NativeAmerican
13 1118.38621556877 0.0012001200120012 Female White
14 1018.88755483131 0.0013001300130013 Female Latino
15 857.587462502481 0.0014001400140014 Male White
16 1142.37946612908 0.0015001500150015 Female Latino
17 1184.791488685 0.0016001600160016 Female White
18 991.97451043021 0.0017001700170017 Female Indian
19 1167.81684189997 0.0018001800180018 Female White
20 1056.42999016393 0.0019001900190019 Female White
21 1134.03618393437 0.002000200020002 Male Black
22 893.427900010009 0.0021002100210021 Female Black
23 1081.28124238587 0.0022002200220022 Female White
24 1109.13680837734 0.0023002300230023 Male Latino
25 1196.00714418014 0.0024002400240024 Male Latino
26 1062.57267030469 0.0025002500250025 Female White
27 1053.94879193869 0.0026002600260026 Female White
28 1106.44720855374 0.0027002700270027 Male White
29 1066.12969376892 0.0028002800280028 Female White
30 952.763590664435 0.0029002900290029 Female Indian
31 1078.89856748464 0.003000300030003 Male Latino
32 923.910903440025 0.0031003100310031 Female White
33 962.752261731627 0.0032003200320032 Male White
34 1120.96469946613 0.0033003300330033 Male Latino
35 1095.54707243727 0.0034003400340034 Male Latino
36 982.400000888278 0.0035003500350035 Male White
37 976.784364731648 0.0036003600360036 Female Latino
38 1014.97914545206 0.0037003700370037 Female White
39 735.162663957816 0.0038003800380038 Male White
40 1153.52770789304 0.0039003900390039 Male White
41 1083.25413280186 0.004000400040004 Female Latino
42 1046.29606246639 0.0041004100410041 Male Indian
43 1159.84975350075 0.0042004200420042 Female White
44 1183.66799015661 0.0043004300430043 Male Indian
45 1023.49130030426 0.0044004400440044 Male Latino
46 1146.45093359615 0.0045004500450045 Male White
47 1229.18413031701 0.0046004600460046 Female NativeAmerican
48 1017.74259235552 0.0047004700470047 Female White
49 1062.12179605505 0.0048004800480048 Male Indian
50 992.334224239803 0.0049004900490049 Female White
</code></pre>
"
"0.10236445520486","0.107499955208361","198315","<p>I am currently getting slightly confused with how a rolling forecast should be setup in R, as in how the data should be organised in order to <em>train</em> and <em>test</em> my model. I feel there is a large gap in my understanding somewhere.</p>

<p>I have seen many examples of forecasting methods, but not many on time-series (using lagged variables) that go into detail, i.e. perform everything manually using <code>predict()</code>. Instead it normally just points to a built in R package. <strong>My question has to do with the training of the model - the alignment of the data for the training.</strong></p>

<p>I know that the regression equation (assuming I am performing a linear regression) looks like this:</p>

<p>$$ y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + \beta_3 x_{t-1} + \beta_4 x_{t-2} + \varepsilon_t $$ </p>

<p>So I have my outcome variable, $y$, being explained by two of its own lagged values, plus two lagged values of a second variable, $x$. Each variable has its own coefficient, all of which my model is estimating.</p>

<p>Let's say I have a <em>data.table</em> (or data.frame), where each row consists of the data aligned according to the equation above. Each row is one day, and represents the given equation. I have five columns, and for this example say 200 rows/days.</p>

<blockquote>
  <p>Day  |  $y_t$  |  $y_{t-1}$  |  $y_{t-2}$  |  $x_{t-1}$  |  $x_{t-2}$</p>
  
  <p>001  | <em>Values</em> --></p>
  
  <p>002  | <em>Values</em> --></p>
  
  <p>003  | <em>Values</em> --></p>
</blockquote>

<p>I use the above equation as the formula to fit my model to obtain estimates for the four coefficients (neglecting $\varepsilon$) using a fixed 40-day time frame.</p>

<pre><code>model &lt;- lm(y ~ y_1 + y_2 + x_1, x_2,            ## my regression formula
            data = input_data[1:40])             ## my data.table 
</code></pre>

<p>Now I want to make a prediction using the fitted <code>model</code>.
I do this by using <code>predict()</code> in <strong>R</strong> as follows:
I take the next (41st) row of data, minus the outcome variable</p>

<pre><code>my_pred &lt;- predict(model, newdata = input_data[41][, outcome := NULL])
</code></pre>

<p>And then calculate my error:</p>

<pre><code>my_error &lt;- input_data[41, outcome] - my_pred
</code></pre>

<p>I then shift everything forward one row, so still a 40-day frame <code>input_data[2:41]</code> updating the coefficients for all variables and predicting the following outcome variable, <code>input_data[42]</code>. This is yielding terrible results for my model, with overall accuracies not much better than a naÃ¯ve forecast, i.e. random guessing.</p>

<p>Should I realign the data for the training segment, so that each row rather represents the data I had on that day? This would mean adding one more column, $x_t$.</p>

<p>Any other suggestions or comments?</p>

<p>Thanks.</p>
"
"0.0490486886395286","0.0480754414848157","198374","<p>This is a question for those out there working in data scientist roles within your organizations. How many variables in acceptable to use within models that are going to be deployed in production for marketing or other purposes?</p>

<p>The reason I ask is this, we have four analysts, two of whom have been with the company for 10 years, and two of us who are recently graduated with our Masters in Predictive Analytics. Our senior analysts primarily build with linear / logistic regression models, and think that using the least amount of variables (regardless of technique) is always best, usually trying to use around 10-15 variables.</p>

<p>Us newer analysts work primarily with random forest and xgboost, and are comfortable using 100-800 variables in our models. I havnt encountered anything to say that using this many variables in random forest or xgboost should cause any concern, but we cannot come to an agreement. Even if holdout results are better using 100+ variables, we are still encouraged to use less.</p>

<p>Can anyone provide any information regarding this topic that might help shape our decision making progress?</p>

<p>Thank you,</p>

<p>Nate</p>
"
"0.113978852552524","0.111717230539356","198484","<p>Consider this example:</p>

<pre><code>foo &lt;-data.frame(x=c(0.010355057,0.013228936,0.016313905,0.019261687,0.021710159,0.023973474,0.025968176,0.027767232,0.029459730,0.030213807,0.023582566,0.008689883,0.006558429,0.005144958),
                 y=c(971.3800,1025.2271,1104.1505,1034.2607,902.6324,713.9053,621.4824,521.7672,428.9838,381.4685,741.7900, 979.7046,1065.5245,1118.0616))
Model3 &lt;- lm(y~poly(x,3),data=foo)
Model4 &lt;- lm(y~poly(x,4),data=foo)
</code></pre>

<p>For <code>Model3</code>, the <code>poly(x,3)</code> term is not significant:</p>

<pre><code>&gt; summary(Model3)

Call:
lm(formula = y ~ poly(x, 3), data = foo)

Residuals:
   Min     1Q Median     3Q    Max 
-76.47 -51.61  -0.55  38.22 100.57 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   829.31      17.85  46.463 5.14e-13 ***
poly(x, 3)1  -819.37      66.78 -12.269 2.37e-07 ***
poly(x, 3)2  -373.05      66.78  -5.586 0.000232 ***
poly(x, 3)3   -87.85      66.78  -1.315 0.217740    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 66.78 on 10 degrees of freedom
Multiple R-squared:  0.9483,    Adjusted R-squared:  0.9328 
F-statistic: 61.15 on 3 and 10 DF,  p-value: 9.771e-07
</code></pre>

<p>However, for <code>Model4</code> it is:</p>

<pre><code>&gt; summary(Model4)

Call:
lm(formula = y ~ poly(x, 4), data = foo)

Residuals:
    Min      1Q  Median      3Q     Max 
-34.344 -19.982   1.229  18.499  33.116 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  829.310      7.924 104.655 3.37e-15 ***
poly(x, 4)1 -819.372     29.650 -27.635 5.16e-10 ***
poly(x, 4)2 -373.052     29.650 -12.582 5.14e-07 ***
poly(x, 4)3  -87.846     29.650  -2.963 0.015887 *  
poly(x, 4)4  191.543     29.650   6.460 0.000117 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 29.65 on 9 degrees of freedom
Multiple R-squared:  0.9908,    Adjusted R-squared:  0.9868 
F-statistic: 243.1 on 4 and 9 DF,  p-value: 3.695e-09
</code></pre>

<p>Why does this happen? Note that the estimate of all coefficients is the same in both cases, since the polynomials are orthogonal. However, the significance is not. This seems to me difficult to understand: if I performed a degree 3 regression, it looks like I could drop the <code>poly(x, 4)3</code> term, thus reverting to a degree 2 orthogonal regression. However, if I performed a degree 4 regression, I shouldn't, even though the coefficients of the common terms have exactly the same estimate. What do I conclude? Probably that one should never trust subset selection :) An <code>anova</code> analysis says that the difference among the degree 2, degree 3 and degree 4 models is significant:</p>

<pre><code>&gt; Model2 &lt;- lm(y~poly(x,2),data=foo)     
&gt; anova(Model2,Model3,Model4)
Analysis of Variance Table

Model 1: y ~ poly(x, 2)
Model 2: y ~ poly(x, 3)
Model 3: y ~ poly(x, 4)
  Res.Df   RSS Df Sum of Sq       F    Pr(&gt;F)    
1     11 52318                                   
2     10 44601  1      7717  8.7782 0.0158868 *  
3      9  7912  1     36689 41.7341 0.0001167 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>EDIT: following a suggestion in comments, I add the residual vs fitted plots for <code>Model2</code>, <code>Model3</code> and <code>Model4</code></p>

<p><a href=""http://i.stack.imgur.com/9ZU8h.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9ZU8h.png"" alt=""enter image description here""></a>` </p>

<p>It's true that the maximum residual error is more or less the same for <code>Model2</code> and <code>Model3</code>, and it becomes nearly one third going from <code>Model3</code> to <code>Model4</code>. There seems to be still some kind of trend in the residuals, though it is less evident than for <code>Model2</code> and <code>Model3</code>. However, why does this invalidate the <em>p</em>-values? Which hypothesis of the linear model paradigm is violated here? I seem to remember that the residuals only had to be uncorrelated with the predictor. However, if they also have to uncorrelated among themselves, then clearly this assumption is violated and the <em>p</em>-values based on the t-test are invalid.</p>
"
"0.0566365471788599","0.0555127381653369","198530","<p>I am trying to draw a smoothing line on a frequency graph with ggplot2. My data are represented like this:</p>

<pre><code>   |    date      |freq
1  |  2012-02-02  |  1
2  |  2012-02-04  |  1
3  |  2012-02-07  |  3
4  |  2012-02-08  |  1
5  |  2012-02-09  |  2
6  |  2012-02-10  |  5
</code></pre>

<p>As I understand it, LOESS is a local linear regression model so while moving from left to right along the x-axis, the smoothing should not be influenced by the previous data.</p>

<p>However, when I run it with ggplot2, I can easily see that on the left part of the chart, the smoothed line does not fit the data as I understand it should. Any help ?</p>

<p>My code :</p>

<pre><code>ggplot(data=data_freq, aes(date, freq, group = 1)) +
  scale_x_date(breaks = waiver()) +
  stat_smooth(se = FALSE, formula = y ~ poly(x,2), na.rm = TRUE) +
  geom_line() +
  theme(legend.position=""none"")
</code></pre>

<p>Below the graphic representation :</p>

<p><a href=""http://i.stack.imgur.com/CT5qC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CT5qC.png"" alt=""enter image description here""></a></p>

<p>As we can see, the smooth line is less sensible to the data at the end, according to the beggining. I would like it to be as sensible as at the beggining and I do not understand why it's not...</p>
"
"0.0980973772790571","0.0961508829696314","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.02831827358943","0.0277563690826684","199141","<p>I would like to perform an anomaly temperature trends analysis in R. I have temperature data from 1901 to 2012. My idea to compute anomaly temperature trends was as follows: 
1. Compute temperature means per month over this time period
2. Subtract temperature means from actual data, to get an anomaly in temperature per month. 
3. Perform a linear regression on this anomaly values to get the anomaly temperature trends.</p>

<p>However, I am not quite sure about the approach. Anyone has done this before and could give me some hints? </p>
"
"0.0578044339088637","0.0679889413649005","199736","<p>I am working on a price elasticity problem and i am using <strong>Log-Log linear model</strong>. Below are the set of variables</p>

<p><em>Dependent variable</em>: logarithm of my quantity sold (LogQ)</p>

<p><em>Independent variable</em>: logarithm of my price(LogMyP), logarithm of price of my competitor price(for cross-price elasticity)(LogCoP), logarithm of my social media rating(LogMyR), logarithm of my competitors social media rating(LogCoR), logarithm of my unsold stock(LogMyS), logarithm of remaining shelf life of my product(LogMyShl), logarithm of market supply in percentage(LogMktspl), day of the week (6 dummy variables)</p>

<p>I need help in identifying set of endogenous and instrumental variables so that i can specify that in <strong>Instrumental Variable regression</strong>. I believe only logarithm of my price and logarithm of my competitor price are for the rest i do believe that it impacts my demand. The problem i am facing is how to decide which of the independent variable is exogenous or instrumental variable. Is there any test available or any procedure to identify if a variable is instrumental variable or is this purely a subjective thing? in any case can anyone help me identify the instrumental variables?</p>
"
"NaN","NaN","200103","<p>This is with reference to <a href=""http://analyticspro.org/2016/03/05/r-tutorial-residual-analysis-for-regression/"">http://analyticspro.org/2016/03/05/r-tutorial-residual-analysis-for-regression/</a>.</p>

<p><a href=""http://i.stack.imgur.com/UJpHy.png""><img src=""http://i.stack.imgur.com/UJpHy.png"" alt=""Figure""></a></p>

<p>For Residual plot (b), where the residuals are increasing linearly (with respect to the predicted values), can we interpret that we are missing a variable? What else is the interpretation?</p>
"
"0.0424774103841449","0.0555127381653369","200182","<p>I am wondering how I can present the results of nonparametric regression. I performed the nonparametric tests using R, and R package 'np'.</p>

<p>The commands used for this are</p>

<blockquote>
  <p>freq &lt;- npreg(Respno ~ Colony + Localden + Agg.prop, regtype = ""ll"",bwmethod = ""cv.aic"",gradients = TRUE, data = resp)</p>
  
  <p>summary(freq2)</p>
  
  <p>npsigtest(freq2)</p>
</blockquote>

<p>Using the last command, 'npsigtest', I get results like this</p>

<blockquote>
  <p>npsigtest(freq)</p>
  
  <p>Kernel Regression Significance Test
  Type I Test with IID Bootstrap (399 replications, Pivot = TRUE, joint = FALSE)</p>
  
  <p>Explanatory variables tested for significance:</p>
  
  <p>Colony (1), Localden (2), Agg.prop (3)</p>
  
  <p>Colony Localden Agg.prop</p>
  
  <p>Bandwidth(s): 21.88052 5956578 0.3183519</p>
  
  <p>Individual Significance Tests</p>
  
  <p>P Value: </p>
  
  <p>Colony 0.0025063 ** </p>
  
  <p>Localden &lt; 2.22e-16 *** </p>
  
  <p>Agg.prop 0.0802005 .</p>
</blockquote>

<p>How do I present this data in a scientific paper? For the simple linear regression results, I included the n, df, t and P. </p>

<p>Thank you, any advice would be greatly appreciated!!</p>
"
"0.0633215847514023","0.0620651280774201","200587","<p>I have a set of data I'm analyzing in R with 2 explanatory variables (X1 and X2), and one response variable (Y). </p>

<pre><code>X1&lt;-c(1,2,2,4,5,8,5,4,3,2,1,0,1,2,3,4,6,6,5,4,3,2,1,0,1,1,3,4,3,6,5,5,3,2,1)
X2&lt;-c(20,40,50,40,50,50,50,30,10,5,10,20,10,10,10,10,50,80,20,10,20,40,40,40,5,20,30,40,50,60,20,20,10,20,10)
Y&lt;-c(70,140,200,240,250,250,250,230,160,105,60,20,60,110,160,210,250,250,250,210,170,140,90,40,55,120,180,240,250,250,250,220,160,120,60)

MyData&lt;-data.frame(X1,X2,Y)
</code></pre>

<p>My goal is to calculate an equation that will allow me to predict Y based on future X1 and Y1 values. In the past I have used a linear regression like so:</p>

<pre><code>MyFit&lt;-lm(Y~X1+X2,data=MyData)
</code></pre>

<p>And then use this formula to predict Y</p>

<pre><code>Y= coefficients(MyFit)[1]+coefficients(MyFit)[2]*X1+coefficients(MyFit)[3]*X2
</code></pre>

<p>In the above dummy set, Y is strongly driven by X1. But the issue with my real data is that at some point Y gets saturated, so that further increases in X1 do not bring about increases in Y (for this data the saturation value is 250, but the actual values begin to slow down and form a saturation curve as it approaches peak value, as opposed to an absolute saturation point). The result is that the linear model will always over-predict the peak Y values. This can be seen in a plot here:</p>

<pre><code>plot(Y,type=""l"",ylim=c(0,350))
lines(X1,col='red')
lines(X2,col='blue')
lines(coefficients(MyFit)[1]+coefficients(MyFit)[2]*X1+coefficients(MyFit)[3]*X2,col='green')
</code></pre>

<p>How can I correct for this. Is it possible to do a multiple non-linear regression for this data? Or is there some other technique I can use here?</p>
"
"0.102102987459307","0.100077011948264","200708","<p>I have a data on some overall conversion rates (i.e. out of x users visiting, y buy something hence y/x is my conversion rate, essentially proportions) over a time period, now this overall proportion can be broken by if they came from channel 1, channel 2 or channel 3 and for each channel there would be again similar proportions. My objective is to see how these proportions from different channels impact the overall proportion</p>

<p>I have run a simple linear regression in R and below is the result. </p>

<pre><code>Call:
lm(formula = target_variable ~ . - date, data = data_lcr)

Residuals:
  Min        1Q    Median        3Q       Max 
-0.034173 -0.003217 -0.000704  0.002331  0.073845 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.0049876  0.0006139  -8.124  7.4e-15 ***
exp1         0.0785438  0.0086230   9.109  &lt; 2e-16 ***
exp2         0.0290531  0.0175517   1.655   0.0987 .  
exp3        -0.1026385  0.0080550 -12.742  &lt; 2e-16 ***
exp4         1.0760312  0.0669632  16.069  &lt; 2e-16 ***
exp5         0.2466149  0.0195844  12.592  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.007503 on 358 degrees of freedom
Multiple R-squared:  0.9843,    Adjusted R-squared:  0.9841 
F-statistic:  4503 on 5 and 358 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>The Model has great R-squared which is significant, all variables turn out to be significant. Next I am checking if my residuals are normally distributed</p>

<pre><code>&gt;  skewness(fitlm$residuals)
    [1] 2.863341
    &gt; kurtosis(fitlm$residuals)
[1] 33.83711

Shapiro-Wilk normality test

data:  fitlm$residuals
W = 0.72781, p-value &lt; 2.2e-16

Anderson-Darling normality test

data:  fitlm$residuals
A = 17.485, p-value &lt; 2.2e-16
</code></pre>

<p>These tests suggest that my residuals are not normally distributed. Should I still consider the model based on R-squared and F-Value or make some corrections? Please suggest</p>

<p>Here is the residual plot:
<a href=""http://i.stack.imgur.com/pUIfB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pUIfB.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/ZWTux.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZWTux.png"" alt=""enter image description here""></a></p>

<p><strong>EDIT</strong>
After removing outliers:
    <a href=""http://i.stack.imgur.com/bmCBB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bmCBB.png"" alt=""enter code here""></a></p>
"
"0.0400480865731637","0.039253433598943","200866","<p>I have a dataset here that describes the relationship between problem_complexity and solution_complexity in a work process.</p>

<pre><code>library(RCurl)
data &lt;- getURL(""https://gist.githubusercontent.com/aronlindberg/a8a6644a639c35ceb9d9/raw/5b7c13bbcd439dacbca70e07c96b6c33804273d2/data.csv"")
object &lt;- read.csv(text = data)
qplot(object$solution_complexity, object$problem_complexity)
</code></pre>

<p>...and when we plot it it looks like below:
<a href=""http://i.stack.imgur.com/O5qyA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/O5qyA.png"" alt=""Plot""></a></p>

<p>I am trying to capture the relationship in the data. To me it seems that it could either be an increasing relationship (blue line) or a quadratic relationship (green line). However, even if the relationship is visually discernible, as the values on the Y-axis increase, they become more sparse, and therefore the broad range of variation at low Y-values overwhelm these other observations.</p>

<p>How can I characterize these relationships, despite the weak effect size?</p>

<p>I've considered using bootstrap and segmented regression, as a positive linear relationship becomes more clear if one only considers the upper portions of the data (on the y-axis).</p>
"
"0.109891042959396","0.107710533187266","201105","<p>I'm about to have data from intercept surveys conducted in parks. The goal of the survey is to determine which characteristics of parks users find most important to park quality (do they care a lot about safety, a little about the facilities, and not at all about who else is there?).</p>

<p>We've designed a survey with open-ended questions to answer this question. The current plan is to take down the responses, and then, once we have them, group them into categories (safety, facilities, social environment, accessibility, etc). </p>

<p>For example, one question on the survey asks the user why they came to park. </p>

<p>Each user's response (we're allowing them to list as many reasons as they like, but are asking for primary reasons first, then secondary reasons and so on) will then be associated with some field coding. For one user it might be, say, facilities and park aesthetics, for another it might be easy access. We'll also have some demographic data (age, sex, ethnicity, activity at the park) for each user.</p>

<p><strong>Question 1:</strong> We want to determine which of the categories is most important to users, and if possible, by how much. I've never done any categorical data analysis, and I have no idea what to do here. For some questions we're just going to have counts: 16 people came for facilities, 10 for open spaces, etc.</p>

<p><strong>Question 2:</strong> A separate series of questions asks users to categorize park quality on a Likert-like scale (low to high quality), and also to rate sub-components of park quality in the same way (quality of facilities, from low to high, and so on). We want to determine which predictors have the largest effect on perceived park quality here as well.</p>

<p><strong>I want to know what type of models to fit to our data, and why.</strong></p>

<p>I'm presuming we want some categorical analogue of regression. I want to pick up theoretical underpinnings, learn how to fit models in R, and also how to perform diagnostics on them. </p>

<p>Once I've decided on the appropriate analysis and have picked up the necessary background, I'd like to pre-register my data analysis plan. I've never done this before and am curious what the convention is for this.</p>

<p>Some details about the sample of parks: the city Parks and Recreation department has selected 10 parks for us to visit. Their park selection criteria is not entirely known, but I think they want to visit some well developed and some under developed parks. There are five pairs of parks that the Parks department thinks are comparable. In each pair of parks, one has recently undergone renovation, and the other hasn't.</p>

<p>My background:</p>

<p>I have taken a first course in math-stat, a course on linear regression, and am halfway through a course on experimental design/ANOVA/EM/Bootstrap. I have some pure math, multi, lin-alg and optimization background as well. I have some limited experience in R as well.</p>
"
"0.02831827358943","0.0277563690826684","201859","<p>I made $n$ measurements of two variables $x$ and $y$. They both have known uncertainties $\sigma_x$ and $\sigma_y$ associated with them. I want to find the relation between $x$ and $y$. How can I do it? </p>

<p><strong>EDIT</strong>: each $x_i$ has a different $\sigma_{x,i}$ associated with it, and the same with the $y_i$.</p>

<hr>

<p>Reproducible R example:</p>

<pre><code>## pick some real x and y values 
true_x &lt;- 1:100
true_y &lt;- 2*true_x+1

## pick the uncertainty on them
sigma_x &lt;- runif(length(true_x), 1, 10) # 10
sigma_y &lt;- runif(length(true_y), 1, 15) # 15

## perturb both x and y with noise 
noisy_x &lt;- rnorm(length(true_x), true_x, sigma_x)
noisy_y &lt;- rnorm(length(true_y), true_y, sigma_y)

## make a plot 
plot(NA, xlab=""x"", ylab=""y"",
    xlim=range(noisy_x-sigma_x, noisy_x+sigma_x), 
    ylim=range(noisy_y-sigma_y, noisy_y+sigma_y))
arrows(noisy_x, noisy_y-sigma_y, 
       noisy_x, noisy_y+sigma_y, 
       length=0, angle=90, code=3, col=""darkgray"")
arrows(noisy_x-sigma_x, noisy_y,
       noisy_x+sigma_x, noisy_y,
       length=0, angle=90, code=3, col=""darkgray"")
points(noisy_y ~ noisy_x)

## fit a line 
mdl &lt;- lm(noisy_y ~ noisy_x)
abline(mdl)

## show confidence interval around line 
newXs &lt;- seq(-100, 200, 1)
prd &lt;- predict(mdl, newdata=data.frame(noisy_x=newXs), 
    interval=c('confidence'), level=0.99, type='response')
lines(newXs, prd[,2], col='black', lty=3)
lines(newXs, prd[,3], col='black', lty=3)
</code></pre>

<p><a href=""http://i.stack.imgur.com/S5OK9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/S5OK9.png"" alt=""linear regression without considering errors in variables""></a></p>

<p>The problem with this example is that I think it assumes that there are no uncertainties in $x$. How can I fix this? </p>
"
"0.113978852552524","0.111717230539356","201971","<p>I am challenged by a simple, but hopefully interesting, data set.</p>

<h2>Data</h2>

<p>The data are driving times of ambulances to the scene (<code>data$actual</code>) as well as the driving times I created by using a GIS to calculate the time (<code>data$simulation</code>). The times differ because the GIS does not take into account that the ambulance drives faster than a standard car. Both times are in seconds. The actual data was provided in minutes, thus the steps in the data. You will find the data a the end of this post.</p>

<h2>Goals</h2>

<p>In order to use the GIS to predict which area the ambulance is able to cover I like to create a model that predicts a simulation driving time based on the actual time which I will then feed into the GIS simulation. This is necessary since the GIS itself does not account for the fact that ambulances drive faster than standard cars. The goal then is to use a longer driving time for the simulation in order to take this fact into account.</p>

<h2>Approach</h2>

<p>My first approach was to build a simple linear regression model for the data:</p>

<pre><code>model1 &lt;- lm(simulation ~ actual, data)
</code></pre>

<p>This gives site a bad R2 and residual standard error. In addition, I took into account the fact that if there is 0 seconds of actual driving time, there should also be 0 seconds of simulation driving time, resulting in:</p>

<pre><code>model2 &lt;- lm(simulation ~ 0 + actual, data)
</code></pre>

<p>Now the R2 drastically increases but the residual standard error also increases. Another thought involves the fact that the ambulance should always be faster than the normal car. So I filtered the data for <code>simulation &gt; actual</code> and created a third model:</p>

<pre><code>newData &lt;- data[data$simulation &gt; data$actual,]
model3 &lt;- lm(simulation ~ 0 + actual, newData)
</code></pre>

<p>This again increases the R2 and now also reduces the error even below the value of <code>model1</code>.</p>

<h2>My question</h2>

<p>Is this a legitimate way to handle the data given what I try to create? I think reducing the amount of data will often yield better results since less data points need to be taken care of. In addition, if you look at the variation of simulation time for every value of the actual driving time one could also try to create a model involving just the means and medians of the simulation time per actual time value (which yields even better results!).</p>

<h2>The data</h2>

<pre><code>structure(list(actual = c(120, 60, 120, 120, 240, 60, 120, 180, 
120, 60, 180, 420, 420, 180, 300, 240, 60, 180, 180, 60, 300, 
180, 240, 180, 60, 180, 420, 240, 60, 360, 180, 60, 240, 180, 
60, 60, 780, 60, 180, 240, 480, 240, 180, 120, 660, 180, 60, 
300, 420, 180, 240, 360, 840, 180, 240, 600, 300, 120, 60, 180, 
120, 60, 60, 120, 60, 180, 180, 180, 120, 360, 300, 180, 60, 
180, 360, 180, 180, 180, 180, 180, 240, 300, 600, 60, 60, 180, 
180, 600, 300, 60, 120, 300, 180, 60, 120, 60, 120, 120, 180, 
120, 120, 120, 240, 120, 120, 600, 120, 120, 180, 360, 300, 240, 
60, 180, 120, 420, 120, 180, 60, 120, 180, 240, 360, 300, 240, 
120, 180, 180, 300, 240, 180, 120, 180, 120, 120, 120, 240, 120, 
180, 180, 180, 60, 120, 180, 120, 420, 60, 180, 180, 240, 180, 
300, 180, 180, 360, 240, 540, 240, 120, 60, 120, 120, 60, 60, 
180, 180, 60, 180, 360, 300, 180, 240, 180, 180, 120, 120, 180, 
60, 180, 180, 240, 240, 180, 180, 180, 180, 180, 240, 120, 180, 
120, 180), simulation = c(194.28940773, 212.275300026, 220.287079812, 
24.607690572, 407.197437288, 81.217067244, 24.607690572, 150.680236818, 
478.658294676, 136.179299352, 377.049865722, 194.28940773, 261.164245608, 
319.750185012, 220.287079812, 351.498241422, 8.703469632, 478.658294676, 
24.607690572, 173.848915098, 220.287079812, 81.217067244, 212.275300026, 
24.607690572, 136.179299352, 150.680236818, 220.287079812, 407.197437288, 
377.049865722, 204.83267784, 220.287079812, 173.848915098, 220.287079812, 
212.275300026, 136.179299352, 194.28940773, 351.498241422, 377.049865722, 
478.658294676, 407.197437288, 664.460391996, 659.49136734, 171.987490656, 
162.42626667, 485.496425628, 360.000858306, 121.588454244, 24.607690572, 
478.658294676, 171.987490656, 152.808523176, 664.460391996, 659.49136734, 
360.000858306, 485.496425628, 162.42626667, 24.607690572, 274.938783648, 
121.588454244, 115.878911016, 385.97213745, 94.89244938, 140.229663846, 
262.36567497, 94.89244938, 115.878911016, 115.878911016, 115.878911016, 
239.758086204, 303.008880618, 519.334259034, 68.913009168, 239.758086204, 
353.441877366, 303.008880618, 68.913009168, 68.913009168, 303.008880618, 
280.39235115, 428.468284608, 259.42299843, 182.360544204, 671.648883822, 
96.808075902, 96.598634718, 186.045684816, 369.657411576, 293.113288878, 
392.484369276, 56.862205266, 343.983478548, 369.657411576, 428.468284608, 
80.855455398, 144.722843172, 60.819990636, 157.677226068, 139.932003024, 
78.863933088, 212.355537414, 158.009676936, 243.857574462, 292.072420122, 
167.319359778, 158.009676936, 270.116386416, 158.009676936, 100.485241416, 
349.8108387, 194.206109046, 538.366470336, 174.882373812, 97.03774452, 
428.468284608, 20.02849281, 615.891094206, 169.016976354, 100.77576399, 
158.009676936, 78.04938555, 99.34376478, 226.997423172, 490.142440794, 
88.538596632, 243.464784624, 266.780548098, 212.355537414, 206.20563984, 
343.983478548, 428.468284608, 428.468284608, 158.009676936, 186.045684816, 
144.722843172, 157.677226068, 212.355537414, 428.468284608, 428.468284608, 
210.082454682, 243.857574462, 280.39235115, 96.808075902, 20.02849281, 
369.657411576, 169.016976354, 490.142440794, 80.855455398, 266.780548098, 
428.468284608, 226.997423172, 158.009676936, 343.983478548, 343.983478548, 
243.857574462, 490.142440794, 428.468284608, 671.648883822, 428.468284608, 
428.468284608, 169.016976354, 139.932003024, 78.863933088, 60.819990636, 
96.598634718, 99.34376478, 369.657411576, 80.855455398, 167.319359778, 
194.206109046, 369.657411576, 158.009676936, 212.355537414, 169.016976354, 
186.045684816, 210.082454682, 428.468284608, 144.722843172, 157.677226068, 
212.355537414, 158.009676936, 194.206109046, 158.009676936, 243.857574462, 
428.468284608, 428.468284608, 99.34376478, 428.468284608, 538.366470336, 
280.39235115, 164.87254143, 177.99147606, 99.029567244)), .Names = c(""actual"", 
""simulation""), row.names = c(NA, -192L), class = ""data.frame"")
</code></pre>
"
"0.0578044339088637","0.0679889413649005","202181","<p>I'm getting some odd coefficients when I apply <code>lm</code> to dates that have been processed and rounded using the <code>lubridate</code> package.  MWE:  </p>

<pre><code>library(ggplot2)
library(lubridate)
library(dplyr)

lakers$month &lt;- ymd(lakers$date) %&gt;% round_date(unit = 'month')
items_by_month &lt;- lakers %&gt;% group_by(month) %&gt;% summarize(count = n()) %&gt;%
    mutate(count = count / 1000)

ggplot(data = items_by_month, aes(x = month, y = count)) + 
    geom_line() +
    stat_smooth(method = 'lm', data = items_by_month)

model &lt;- lm(data = items_by_month, count ~ month)
summary(model)
time &lt;- max(items_by_month$month) - min(items_by_month$month)
coef(model)['month'] * as.numeric(time)
</code></pre>

<p>The plot indicates that <code>ggplot</code>, at least, understands what's going on with the regression model.<br>
<a href=""http://i.stack.imgur.com/HYTks.png""><img src=""http://i.stack.imgur.com/HYTks.png"" alt=""Plot with monthly totals and regression line""></a></p>

<p>But in <code>summary(model)</code> the coefficient on <code>month</code> is on the order of 10^-7, which is about 5 orders of magnitude too small:  the plot shows an increase of about 2.5 between the first and last dates, but the last line shows an increase of about 2.5 * 10^-5.  </p>

<p>Note that I've divided the <code>count</code> column by 10^3, in order to get values that are easier to read (and closer to my actual use case).  But that shouldn't effect either the plot or <code>lm</code>.  Also, I know there are more sophisticated techniques than linear regression for analyzing time series data; but I'm just looking at gross trends over time, not factor out seasonal patterns, etc.  </p>
"
"0.0693653206906364","0.0679889413649005","202215","<p>I am working on a daily response variable.  As part of weekly prediction methods, multiple linear regression is also used.  We also have monthly prediction on the same response variable.  In the monthly prediction, I used 4 months, 5 months, and 12 months lag of the response variable as the predictors.  For the weekly prediction, should I use 4 weeks, 5 weeks, and 12 weeks as lag or is it something else?</p>
"
"NaN","NaN","202963","<p>I am learning R and how to do regressions in a biological context, so please forgive me.</p>

<p>I am stumped on how to test if a slope parameter is less than a certain number at the alpha = 0.05 level. These are my two correlation(?) coefficients:</p>

<blockquote>
  <p>reg.fit &lt;- lm(gro3 ~ gro2) # fit linear model
  reg.fit</p>
  
  <p>Call:
  lm(formula = gro3 ~ gro2)</p>
  
  <p>Coefficients:</p>
  
  <p>(Intercept),         gro2<br>
     -0.8426,       0.3582    </p>
</blockquote>

<p>I <em>think</em> I should use a t-test to test these somehow. But I'm not sure where to start.</p>
"
"0.0400480865731637","0.039253433598943","203132","<p>Let's say I have the following regression:</p>

<pre><code>mort_probit &lt;- glm(mort ~ age + I(age*age) + hs_grad + some_college + college + post_grad + black + hisp + other + rich + middle_class, family = binomial(""probit""), data=data)
</code></pre>

<p>This is set up such that the base case is a poor white.  I want to test if a wealthy black individual has a lower risk for mortality than a poor white.</p>

<p>I set up a linear combination of coefficients in the following way:</p>

<pre><code>summary(glht(mort_probit, linfct = c(""(black + rich) = 0 "")))
</code></pre>

<p>But I'm not sure if this answers my question.  What exactly is this command doing?  I'm guessing it sets ""black"" and ""rich"" to 1 and test if that minus the base case = 0.  </p>

<p>What is the correct way of doing this? </p>
"
"0.02831827358943","0.0277563690826684","203284","<p>I have a linear multiple regression with two dummy variables.</p>

<p>IQ = b_1 + b_2*X_german + b_3*X_american + b_4*age</p>

<p>Let's say my regression gives a predicted value of IQ = 105 for a german individual of age 30. And an american individual of the same age has a predicted value of IQ = 106.</p>

<p>What I want to do is to test if the predicted IQs are significantly different in R. I.e. H_0: 105 = 106</p>
"
"0.0535165067688","0.0524546070649019","203359","<p>Consider the following heteroscedastic model:
$$y_i = f(x_i, \beta) + g(x_i, \theta)\varepsilon_i, i = 1, \ldots, n, \tag{1}$$
where $f(\cdot, \beta)$ is the regression function and $g(\cdot, \theta)$
is the variance function. For simplicity, assume the errors $\{\varepsilon_i\}$ are i.i.d. with mean $0$ and variance $\sigma^2$.</p>

<p>Regarding model $(1)$, I understand (but I am not quite sure) that the <code>gls</code> function in <code>nlme</code> package can be used (at least when $f$ is linear) to implement the iteratively reweighted least squares algorithm (Carroll, Ruppert, <em>Transformation and Weighting in Regression</em>, pp. 69). When I read the manual of <code>nlme</code>, it looks to me that <code>gls</code> function restricts the forms of $g$ to a very small class of functional forms. For example, given observations $\{(y_i, x_{i1}, x_{i2}): i = 1, \ldots, n\}$, is it
possible to use <code>gls</code> to fit the following special case of $(1)$:
$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \sqrt{\theta_0 + \theta_1 x_{i1}^2 + \theta_2 x_{i2}^2}\varepsilon_i, i = 1, \ldots, n$$
, where $\theta_0 &gt; 0, \theta_1 \geq 0, \theta_2 \geq 0$? If yes, how should I specify my own square-root variance functional form in <code>gls</code>? If
no, are there any other available R packages to implement IRLS algorithm?</p>
"
"0.0424774103841449","0.0555127381653369","203417","<p>Since my original question was to R-code-specific I'm trying to rewrite it:</p>

<p>I want to make a regression where my dependent variable <code>y</code> should follow a log-normal-distribution influenced by the explanatory variable <code>x</code> where the mean and variance changes across the observation.</p>

<p>Since log-normal doesn't belong to the exponential-family I can't try a glm.
Then I found gamlss which should exactly do the trick.</p>

<p>I was looking for some paper explaining the theory a little deeper - especially in my case of log-normal and where all the parameters of the distribution are  functions of the explanatory variables.</p>

<p>First of all I would like to know if there is a formula like the one below for an ordinary linear regression to calculate <code>y</code> after fitting:</p>

<p><a href=""http://i.stack.imgur.com/5Xqte.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5Xqte.png"" alt=""enter image description here""></a></p>

<p>But my biggest problem is, that I have no idea on how to handle that the moments of <code>y</code> change across the observation of <code>x</code>.
So let's say I have the following:</p>

<p><a href=""http://i.stack.imgur.com/dv61k.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dv61k.png"" alt=""enter image description here""></a></p>

<p>I'm trying to achieve the regression via R and its <code>gamlss</code>-package.</p>

<p>There I start with <code>gamlss(y~x,familiy=LOGNO())</code> and then one has the possibility the make <code>sigma</code> depending on <code>x</code> via <code>sigma.formula=~</code> but no option for <code>mu</code>. So is it even possible in general?</p>

<p>Here is my code:</p>

<pre><code>library(gamlss)

y&lt;-c(1495418, 1684470, 1997120, 1901727, 2070008, 2213829, 2364602, 2333710, 2491570, 2540110, 2620947, 2761075, 2943475, 2854544)
x&lt;-c(3932300, 4119100, 4354400, 4483752, 4585303, 4803234, 4989701, 5177605, 5380031, 5494672, 5606376, 5783627, 6015992, 6171564)

fm&lt;-gamlss(y~x,familiy=LOGNO())
summary(fm)
fitted(fm)
residuals(fm)
y-fitted(fm) #How come this aren't the residuals?

fitted(fm,""mu"")
fitted(fm,""sigma"")
</code></pre>
"
"0.0566365471788599","0.0555127381653369","203782","<p>I've performed linear regression on two different combination of variables.
First combination gives Multiple R-Squared Value greater than the one in second combination.
Second combination gives Adjusted R-Squared Value greater than the one in first combination.So I'm confused to find the better model of these two.
Can anybody help me in this?</p>

<p><strong>Below is the image of First model</strong>
<a href=""http://i.stack.imgur.com/EWVCC.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/EWVCC.jpg"" alt=""enter image description here""></a></p>

<p><strong>Below is the image of Second model</strong>
<a href=""http://i.stack.imgur.com/RbsTn.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RbsTn.jpg"" alt=""enter image description here""></a></p>
"
"0.14449456827576","0.146872899781725","203785","<p>I'm using the <code>tgp</code> package in R for fully Bayesian Gaussian Process Regression, and it's great! I'm currently performing regression for experimental data coming from turbomachinery testing, and I'm using the <code>bgp</code> function. This function uses a GP prior with either a <code>linear</code> mean or a <code>constant</code> mean (respectively, option <code>meanfn=""linear""</code> or <code>meanfn=""constant""</code>, which is the default). Note that <code>tgp</code> allows the use of treed Gaussian priors, but for now I'm staying simple, so I'm using the <code>bgp</code> function which doesn't use regression trees, just ordinary Gaussian Processes.</p>

<p>I would like my posterior predictive mean to go to zero away from the training set data, for physical reasons. How can I impose that? I was thinking to set the prior over $\beta_0$ to a Normal distribution centered at 0 and with an extremely small variance, but I'm not sure how to do that. From <code>help(btgp)</code></p>

<pre><code>bprior Linear (beta) prior, default is ""bflat""; alternates include ""b0"" hierarchical Normal
prior, ""bmle"" empirical Bayes Normal prior, ""b0not"" Bayesian treed LMstyle
prior from Chipman et al. (same as ""b0"" but without tau2), ""bmzt"" a independent
Normal prior (mean zero) with inverse-gamma variance (tau2), and
""bmznot"" is the same as ""bmznot"" without tau2. The default ""bflat"" gives
an â€œimproperâ€ prior which can perform badly when the signal-to-noise ratio is
low. In these cases the â€œproperâ€ hierarchical specification ""b0"" or independent
""bmzt"" or ""bmznot"" priors may perform better
</code></pre>

<p>Default is the improper prior <code>""bflat""</code>, which is not what I want. If I use the <code>""b0""</code> hierarchical Normal prior, I guess I cannot set the mean and the variance because they should become additional hyperparameters to be determined in the Bayesian paradigm. Thus, I may go for <code>""bmzt""</code>, the independent Normal prior with zero mean. However, with this prior I cannot set the variance, which is again an hyperparameter. Basically, I want my prior mean function to be zero, so that away from the data, also the posterior predictive mean will be zero. Is there a way to achieve that?</p>

<p>EDIT: nobody wants to have a try? :) As my actual case is quite complicated, I wrote a small test case which illustrates the main problem, with the help of the <code>tgp</code> package author. NOTE: unless you have an optimized version of R, you may want to set <code>BTE = c(1000,10000,2)</code> in the call to <code>bgp</code>, or you may have to wait for a very long time to get an answer.</p>

<pre><code># clear the workspace
rm(list=ls())
gc()
graphics.off()

# set seed for reproducibility
set.seed(825)

# load required packages
library(tgp)
library(ggplot2)

# simulated data
x &lt;- seq(-1,1,len=100)
eps &lt;- rnorm(n=100,mean=0,sd=0.5)
y &lt;- -5*x^2+eps
ymean &lt;- mean(y)

# prediction points
xpred &lt;- seq(-20,20,len=100)

# fit GP
GPModel &lt;- bgp(X=x,Z=y,XX=xpred,meanfn = ""constant"", bprior=""bmzt"", 
BTE = c(2000,52000,2), tau2.p=c(1,10000), tau2.lam=""fixed"")    
ypred &lt;- GPModel$ZZ.mean 

# plots
ymean_vector &lt;- rep(ymean,100)
df &lt;- data.frame(x,y,xpred,ypred,ymean_vector)
p &lt;- ggplot(data=df)
p &lt;- p + geom_point(aes(x=x,y=y)) + 
    geom_line(aes(x=xpred,y=ypred),col=""blue"") +
    geom_line(aes(x=xpred,y=ymean_vector),col=""red"") +
    geom_line(aes(x=xpred,y=GPModel$ZZ.q1), col=""green"") + 
        geom_line(aes(x=xpred,y=GPModel$ZZ.q2), col=""green"")
p
</code></pre>

<p>The resulting plot is</p>

<p><a href=""http://i.stack.imgur.com/VjePX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VjePX.png"" alt=""enter image description here""></a></p>

<p>The mean response is the red line: the blue line is the GP posterior predictive mean, and the green lines give the 90% credible interval.Thus, outside the training data range, the data mean is indeed included in the 90% credible interval, but I would like the predictive mean to converge to it...I think that if I could find a way to set the standard deviation of the prior for $\beta_0$ to some  extremely small value, I would achieve what I want, but I don't know how to do it.</p>

<p>EDIT2: I can use either a multiplicative (separable) squared exponential kernel
or an additive squared exponential kernel.</p>

<pre><code>sep_Gaussian_Kernel &lt;- function(x,y,sigma,l) {
    prod(sigma*exp(-0.5*(abs(x-y)/l)^2))
}    

add_Gaussian_Kernel &lt;- function(x,y,sigma,l) {
    sum(sigma*exp(-0.5*(abs(x-y)/l)^2))/length(x)
} 
</code></pre>
"
"0.087740961604166","0.107499955208361","203816","<p>I am trying to duplicate the results from <code>sklearn</code> logistic regression library using <code>glmnet</code> package in R.</p>

<p>From the <code>sklearn</code> logistic regression <a href=""http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"" rel=""nofollow"">documentation</a>, it is trying to minimize the cost function under l2 penalty
$$\min_{w,c} \frac12 w^Tw + C\sum_{i=1}^N \log(\exp(-y_i(X_i^Tw+c)) + 1)$$</p>

<p>From the <a href=""https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.html#log"" rel=""nofollow"">vignettes</a> of <code>glmnet</code>, its implementation minimizes a slightly different cost function
$$\min_{\beta, \beta_0} -\left[\frac1N \sum_{i=1}^N y_i(\beta_0+x_i^T\beta)-\log(1+e^{(\beta_0+x_i^T\beta)})\right] + \lambda[(\alpha-1)||\beta||_2^2/2+\alpha||\beta||_1]$$</p>

<p>With some tweak in the second equation, and by setting $\alpha=0$, $$\lambda\min_{\beta, \beta_0} \frac1{N\lambda} \sum_{i=1}^N \left[-y_i(\beta_0+x_i^T\beta)+\log(1+e^{(\beta_0+x_i^T\beta)})\right] + ||\beta||_2^2/2$$</p>

<p>which differs from <code>sklearn</code> cost function only by a factor of $\lambda$ if set $\frac1{N\lambda}=C$, so I was expecting the same coefficient estimation from the two packages. But they are different. I am using the dataset from UCLA idre <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">tutorial</a>, predicting <code>admit</code> based on <code>gre</code>, <code>gpa</code> and <code>rank</code>. There are 400 observations, so with $C=1$, $\lambda = 0.0025$.</p>

<pre><code>#python sklearn
df = pd.read_csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
y, X = dmatrices('admit ~ gre + gpa + C(rank)', df, return_type = 'dataframe')
X.head()
&gt;  Intercept  C(rank)[T.2]  C(rank)[T.3]  C(rank)[T.4]  gre   gpa
0          1             0             1             0  380  3.61
1          1             0             1             0  660  3.67
2          1             0             0             0  800  4.00
3          1             0             0             1  640  3.19
4          1             0             0             1  520  2.93

model = LogisticRegression(fit_intercept = False, C = 1)
mdl = model.fit(X, y)
model.coef_
&gt; array([[-1.35417783, -0.71628751, -1.26038726, -1.49762706,  0.00169198,
     0.13992661]]) 
# corresponding to predictors [Intercept, rank_2, rank_3, rank_4, gre, gpa]


&gt; # R glmnet
&gt; df = fread(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; X = as.matrix(model.matrix(admit~gre+gpa+as.factor(rank), data=df))[,2:6]
&gt; y = df[, admit]
&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)      -3.984226893
gre               0.002216795
gpa               0.772048342
as.factor(rank)2 -0.530731081
as.factor(rank)3 -1.164306231
as.factor(rank)4 -1.354160642
</code></pre>

<p>The <code>R</code> output is somehow close to logistic regression without regularization, as can be seen <a href=""http://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels"">here</a>. Am I missing something or doing something obviously wrong?</p>

<p>Update: I also tried to use <code>LiblineaR</code> package in <code>R</code> to conduct the same process, and yet got another different set of estimates (<code>liblinear</code> is also the solver in <code>sklearn</code>):</p>

<pre><code>&gt; fit = LiblineaR(X, y, type = 0, cost = 1)
&gt; print(fit)
$TypeDetail
    [1] ""L2-regularized logistic regression primal (L2R_LR)""
    $Type
[1] 0
$W
            gre          gpa as.factor(rank)2 as.factor(rank)3 as.factor(rank)4         Bias
[1,] 0.00113215 7.321421e-06     5.354841e-07     1.353818e-06      9.59564e-07 2.395513e-06
</code></pre>

<p>Update 2: turning off standardization in <code>glmnet</code> gives:</p>

<pre><code>&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0, standardize = F)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)      -2.8180677693
gre               0.0034434192
gpa               0.0001882333
as.factor(rank)2  0.0001268816
as.factor(rank)3 -0.0002259491
as.factor(rank)4 -0.0002028832
</code></pre>
"
"0.0566365471788599","0.0555127381653369","203904","<p>I am new to glms and have picked up the following <a href=""http://www.planta.cn/forum/files_planta/glm_2002_crc_213.pdf"" rel=""nofollow"">text</a>, I am trying to do the exercises and I'm a little stuck on exercises 4.5 question 4.1. The question states that a possible model for the data is a poisson distribution with parameter $\lambda_i = i^\theta$ where $i= (1,2,\dots,20)$ is the time index. I want to fit a poisson regression in R using the log link function, such that:
$$
g(\lambda_i)=\log(\lambda_i) = \beta_1 + \beta_2 \log i
$$
In R, I've done the following:</p>

<pre><code>y&lt;-c(1,6,16,23,27,39,31,30,43,51,63,70,88,97,91,104,110,113,149,159)
x&lt;-1:20
</code></pre>

<p>I'm confused about the glm function, I'm pretty sure I should be fitting:</p>

<pre><code>n1&lt;-glm( y~log(x), family = poisson (link = log) )
plot(log(x),y)
</code></pre>

<p>What I'm finding hard to understand is when plotting the regression line, we should be plotting:</p>

<p>$$
\lambda_i =\exp ( \beta_1 + \beta_2 x_i)
$$
So we should have:</p>

<pre><code>plot(log(x),y)
lines(log(x), exp(n1$fit))
</code></pre>

<p>which doesn't give a decent looking result, although</p>

<pre><code>lines(log(x),n1$fit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/BrbNL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/BrbNL.png"" alt=""enter image description here""></a></p>

<p>seems to be the right way to go, but doesn't make intuitive sense to me, aren't the fit values giving the linear part of the model??</p>
"
"0","0.0277563690826684","203957","<p>I am learning the ropes of total least squares regression and I found this thread <a href=""https://stats.stackexchange.com/questions/13152"">How to perform orthogonal regression (total least squares) via PCA?</a> where the answer by @amoeba, together with some R code, is just spectacular.</p>

<p>However, unlike a vanilla linear regression, I am unsure about how to calculate the confidence interval of my prediction.</p>

<p>Can anyone help and, if possible, provide some R code?</p>
"
"0.0800961731463273","0.078506867197886","204708","<p>Is Studentized residuals v/s Standardized residuals same in regression model ? I build a linear regression model in R and try to plot the graph of Studentized residuals v/s fitted values  but didn't find automated way of doing this in R . 
Suppose I have a model </p>

<pre><code>library(MASS)

lm.fit=lm(Boston$medv~(Boston$lstat))
</code></pre>

<p>then using plot(lm.fit) does not provide any plot b/w Studentized residuals v/s fitted values but yet it provides plot b/w Standardized residuals v/s fitted values </p>

<p>I used plot(lm.fit$fitted.values,studres(lm.fit) and it will plot the desired graph.So just want to confirm that am i going the right way and Studentized and Standardized residuals isn't the same thing. If they are different then please provide some guide to calculate them and their definitions. I searched through the net and find it bit confusing </p>
"
"0.09392108820677","0.0920574617898323","204763","<p>Using linear regression as an equation for prediction is straightforward with,</p>

<p>$$ Y_i = \beta_0 + \beta_1 X_i. $$</p>

<p>Once the betas are estimated I can insert different values of $X$ to use as a what-if analysis for different scenarios. </p>

<p>But trying to do the same with ARIMA models is proving difficult to translate. For example with an ARIMA(2,1,1) model, how do I create an equation where I can try out different scenarios to see how the projection changes? </p>

<p>Below I have the output for a projection of sales based on past sales and extra regressors. I see that a unit change in <code>poc0_3_PER</code> results in a <code>135.2229</code> change in sales. But how do I account for the moving average and auto-regression components?</p>

<pre><code>arima(ts.count, order=c(2,1,1), xreg=df.back[3:4])

Call:
arima(x = ts.count, order = c(2, 1, 1), xreg = df.back[3:4])

Coefficients:
          ar1     ar2     ma1  poc0_3_PER
      -0.4569  0.2458  0.9455    135.2229
</code></pre>

<p>I have <code>ar1</code> and <code>ar2</code> estimates along with <code>ma1</code> and the extra regressors. How do I convert this into a working equation wherein I can try out different scenarios for the extra regressors to see how the prediction is affected?</p>

<p>I'm hoping that the solution is not an equation like <a href=""http://stats.stackexchange.com/questions/69407/how-do-i-write-a-mathematical-equation-for-arima-2-1-0-x-0-2-2-period-12?rq=1"">this post here</a>. I do have SARIMA models at times with orders like <code>SARIMA(2,0,1)(1,0,1)[12]</code>.</p>
"
"0.0693653206906364","0.0679889413649005","205020","<p>I want to fit a random slope and intercept model to my data (see below), with <code>rating</code> being regressed on <code>position</code>. The plot below shows the results for all subjects. The blue lines are separate linear regressions fitted to each individual. The red lines show the results from the mixed effects model. I am a bit surprised, that the difference between the lines is quite big in some cases. For example, the lines for subject 4 or 7 are very different. In the full dataset, the difference is sometimes even more pronounced.</p>

<p>Fitting one mixed effects model or many single models is a different thing, of course. Still I do not quite understand why such big differences arise. I am not very familiar with mixed models, so my naive expectation was that the two lines would not deviate too much, given that slopes and intercepts can vary freely. </p>

<p><strong>Am I wrong to expect smaller deviations and why do they arise?</strong> (or did I go wrong at some point)?</p>

<p><a href=""http://i.stack.imgur.com/bRHAv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bRHAv.png"" alt=""enter image description here""></a></p>

<p>The R code for the model and plot follows. The data is found below.</p>

<pre><code>library(nlme)
library(ggplot2)

m &lt;- lme(rating ~ 1+ position, random= ~ 1 + position | subject, data=x)

ggplot(x, aes(position, rating)) +
  geom_point(color=""grey"") + 
  geom_smooth(method=""lm"", se = FALSE) +
  geom_line(aes(y=predict(m)), color=""red"") +
  facet_wrap( ~ subject) 
</code></pre>

<p>Data:</p>

<pre><code>x &lt;- structure(list(position = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 
1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 
2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 
3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 
4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 
5, 6, 7, 8, 9), subject = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 
10L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 
9L, 9L), .Label = c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", 
""10""), class = ""factor""), rating = c(2, 3, 4, 3, 2, 4, 4, 3, 
2, 1, 4, 2, 4, 3, 2, 3, 4, 3, 5, 4, 3, 2, 2, 2, 4, 2, 4, 3, 2, 
2, 3, 5, 3, 4, 4, 4, 3, 2, 3, 5, 4, 5, 2, 3, 4, 2, 4, 4, 1, 2, 
4, 5, 4, 2, 3, 4, 3, 2, 2, 2, 4, 5, 4, 4, 5, 2, 3, 4, 3, 2, 4, 
3, 4, 4, 4, 3, 4, 5, 4, 5, 4, 3, 4, 5, 4, 5, 4, 5, 5, 5, 1, 3, 
3, 4, 3, 3, 5, 3, 5, 3)), .Names = c(""position"", ""subject"", ""rating""
), row.names = c(NA, -100L), class = ""data.frame"")
</code></pre>
"
"NaN","NaN","205227","<p>I'm a little new to R and I haven't done stats in a while. I know a one way ANOVA is the same as a linear regression, but is there a difference between a two way ANOVA and a linear regression with two covariates? And if they are different I'm not sure which one I performed. Below is my sample code:</p>

<pre><code>data.frame[[""Acute""]] = factor(data.frame[[""Acute""]])
data.frame[[""Frequency""]] = factor(data.frame[[""Frequency""]])
DishMortalityVsTime.Total.Acute.Freq = aov(Dish.Mortality ~ Time * Acute * Frequency, data=data.frame)
summary(DishMortalityVsTime.Total.Acute.Freq)
</code></pre>

<p>and the output</p>

<pre><code>                      Df Sum Sq Mean Sq F value               Pr(&gt;F)    
Days                   1  1.352  1.3524  65.189  0.00000000000000429 ***
Acute                  2  5.885  2.9423 141.822 &lt; 0.0000000000000002 ***
Frequency              3  0.539  0.1795   8.653  0.00001279126504853 ***
Days:Acute             2  1.672  0.8361  40.302 &lt; 0.0000000000000002 ***
Days:Frequency         3  0.050  0.0165   0.796                0.496    
Acute:Frequency        6  0.787  0.1311   6.320  0.00000192315201011 ***
Days:Acute:Frequency   6  0.038  0.0064   0.309                0.932    
Residuals            552 11.452  0.0207 
</code></pre>

<p>Any help would be appreciated, Thanks!</p>
"
"0.0490486886395286","0.0480754414848157","205401","<p>I'm performing a standard multiple linear regression on telecommunications data and one of the variables can have infinity/unlimited as its value. e.g. number of minutes has a series of discrete values but can also be infinity.</p>

<p>I can create a separate 0/1 variable to capture the ""unlimited minutes"" records but then how to I add the extra information that I have for plans that have a discrete number of minutes? A colleague suggested contriving a number of minutes to be 100,000 say but I'm not convinced this is the right way to go.</p>

<p>I'm using R to do this modelling</p>
"
"0.0755153962384799","0.0832691072480053","205419","<p>I am using implementations of the Levenberg-Marquardt algorithm for non-linear least squares regression based on MINPACK-1 utilizing either the R function nlsLM() from minpack.lm or an implementation in C using the <a href=""http://www.physics.wisc.edu/~craigm/idl/cmpfit.html"" rel=""nofollow"">mpfit</a> library. </p>

<p>I have observed that while I come up with comparable parameter estimates in both cases, the calculated uncertainty is drastically different in the two approaches. Below you find the code of the two approaches:</p>

<p><strong>In R:</strong></p>

<pre><code>library(minpack.lm)

x &lt;- seq(1,9)
y &lt;- c(2.93,1.79,1.03,0.749,0.562,0.218,0.068,0.155,0.03)
table &lt;- data.frame(x,y)

fit &lt;- nlsLM(y ~ N * exp( -rate * x ), data = table, start = list(rate = 0.1, N = 1)

summary(fit)
</code></pre>

<p>which returns the estimates and errors:</p>

<pre><code>rate    0.47825 +/- 0.02117
   N    4.69723 +/- 0.18950
</code></pre>

<p><strong>In C:</strong></p>

<pre><code>#include &lt;math.h&gt;
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;

#include ""cmpfit-1.2/mpfit.h""

// Data structure to keep my vectors
typedef struct my_data {

  double *x;
  double *y;
  int n;

} my_data;

// defining the function that is to be fitted
int my_exp(int m, int n, double *p, double *dev, double **deriv, my_data *data)
{

  int i;
  double *x, *y, f;

  x = data-&gt;x;
  y = data-&gt;y;

  for (i = 0; i &lt; m; i++) {

    f = p[0] * exp(-p[1]*x[i]);
    dev[i] = y[i] - f;

  }

  return 0;
}


int main(void)
{

  // define test data
  my_data data;
  data.x = (double[]){1,2,3,4,5,6,7,8,9};
  data.y = (double[]){2.93,1.79,1.03,0.749,0.562,0.218,0.068,0.155,0.032};
  data.n = 9;

  // start values
  double p[2] = {1.0, 0.1};

  mp_result result;
  memset(&amp;result, 0, sizeof(result));

  double perr[2];
  result.xerror = perr;

  int ret;

  ret = mpfit((mp_func)my_exp, data.n, 2, p, 0,0, &amp;data, &amp;result);


  printf(""rate:   %f +/- %f\n"", p[1], perr[1]); 
  printf(""   N:   %f +/- %f\n"", p[0], perr[0]);

  return 0;
}
</code></pre>

<p>which returns:</p>

<pre><code>rate:   0.479721 +/- 0.270632
   N:   4.707439 +/- 2.422189
</code></pre>

<p>As you can see, the errors returned by mpfit are about 12x higher then from R. I also implemented the same thing using <a href=""http://www.gnu.org/software/gsl/"" rel=""nofollow"">GSL</a> which produces results identical to mpfit. Unfortunately I am not very comfortable with the R code base, so it would be great if anyone has any inside into what nlsLM in R does differently then mpfit and GSL when calculating the errors.</p>

<p>Thanks!</p>
"
"0.0749231094763201","0.0734364498908627","205453","<p>I don't have a working example, because this question is more conceptual.  Let's say I'm running a linear regression using the <code>plm</code> package in R on the relationship between graduating from college and getting lunch-subsidies as a child.  </p>

<p>I have panel data that includes observations from individuals that may be in the same family, so I add family-level fixed effects to allow for arbitrary correlations within the family.  </p>

<p>If I were to add race covariates, however, R would omit them because of multicollinearity in the family level.  </p>

<p>How would I test the hypothesis, then, that blacks have differential effects than whites, but still resolving the issues solved by fixed effects?  </p>

<p><strong>To answer the questions below:</strong></p>

<p>Let's say I have the following regression, attempting to test for the hypothesis that different races and sexes are helped differentially from a free lunch program insofar as it relates to graduating college:</p>

<pre><code>plm(graduate_college ~ free_lunch + black + male + hispanic + data=data, index=c(""mother_id""), model=""within"")
</code></pre>

<p>Mother_ID just tracks siblings from the same mother.  Now, if I want to test the hypothesis that blacks have different effects than whites as it relates to the effect of free lunch on graduating college, how would I test this?  My guess is to remove fixed effects, add clustered standard errors at the mother_id level and add an interaction term for black*free_lunch?</p>
"
"0.0693653206906364","0.0679889413649005","205586","<p>Consider a data set where you have a <code>tenure</code> variable that takes non-negative values (e.g. from <code>0</code> to <code>44</code>) and a <code>tenurel</code> dummy that is <code>FALSE</code> when <code>tenure</code> is <code>0</code> and <code>TRUE</code> otherwise. How do you interepret their coefficients when both are included in a linear regression?</p>

<p>Take this example (originally from Wooldridge 2008): </p>

<pre><code>library(foreign);library(lmtest)
wage1 &lt;- read.dta(""http://fmwww.bc.edu/ec-p/data/wooldridge/wage1.dta"")
wage1$tenurel &lt;- as.logical(wage1$tenure)

coeftest(fitnestint &lt;- lm(log(wage) ~ educ+exper+tenurel+tenure, data=wage1))
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 0.2757858  0.1042105  2.6464  0.008381 ** 
## educ        0.0899010  0.0074526 12.0630 &lt; 2.2e-16 ***
## exper       0.0038215  0.0017323  2.2060  0.027819 *  
## tenurelTRUE 0.0732658  0.0480708  1.5241  0.128084    
## tenure      0.0200777  0.0033542  5.9859 4.008e-09 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p><strong>Question:</strong> So how do you interpret the coefficients for <code>tenurel</code> (<code>0.073</code>) and <code>tenure</code> (<code>0.020</code>)? </p>

<hr>

<p>In my understanding <code>tenurel</code> would represent the effect on Y of ""having some tenure when <code>tenure=0</code>"" (even if its literal interpretation would seem nonsensical, as it can happen for main-effect regressors in usual interactions) and <code>tenure</code> would represent the <em>additional</em> effect on Y ""of one more year of tenure"". </p>

<p>It seems to me that in this specification <code>tenure</code> is a ""nested interaction"", insofar as if we add an interaction term between the two regressors, it would be undefined: </p>

<pre><code>coef(fitint &lt;- lm(log(wage)~educ+exper+tenurel*tenure, data=wage1))
##        (Intercept)               educ              exper 
##        0.275785819        0.089900995        0.003821454 
##        tenurelTRUE             tenure tenurelTRUE:tenure 
##        0.073265829        0.020077744                 NA 
</code></pre>

<p>Indeed, the interaction term <code>tenurel:tenure</code> would be identical to <code>tenure</code>: </p>

<pre><code>with(wage1, all.equal(tenure, tenurel*tenure))  ##interaction term identical to tenure
## [1] TRUE
</code></pre>
"
"0.0490486886395286","0.0480754414848157","205614","<p>I'm running some regression analyses and got pretty confused about R's output when it comes to robust regression models.
When I run a <code>OLS</code> -- using the command <code>lm()</code> -- I get this as output:</p>

<blockquote>
  <p>Coefficients:</p>

<pre><code>                Estimate Std. Error t value Pr(&gt;|t|)
</code></pre>
</blockquote>

<p>But when I run a robust linear model using the <code>rlm()</code> command, the output looks like so:</p>

<blockquote>
  <p>Coefficients:  </p>

<pre><code>                Value   Std. Error t value
</code></pre>
</blockquote>

<p>How do I get  the <code>p-values</code> and the significance-levels in an <code>rlm</code> then? Without that, the whole analysis is somewhat pointless.
Unfortunately, I couldn't find an answer to that anywhere, so I hope someone around here can help me out. Thanks a lot!!</p>
"
"0.0566365471788599","0.0555127381653369","205664","<p>Nice and simple. I've spent two hours googling, reading cross validated, and several r blogs to attempt to find a simple method of outputting the representative tree in R.</p>

<p>I was attempting to demonstrate to a coworker that random forest was producing better results (better in accuracy and more reproducible) than his linear regression on the same data set. However he ultimately said it didn't matter because he'd given up trying to explain it to his superior. His boss wants to use the linear regression model because he can in-turn explain it to his superiors. While essentially they have to trust the output of the random forest.</p>

<p>I recall that it's possible to display a tree producted by a CART model, and in my googling I found you can also simply call plot() on the output of ctree from the cforest package. However I can't seem to find a way to plot the output of randomforest (or cforest) in the same fashion.</p>

<p>Is there a way to do this? Or alternatively is there a known way to extract the tree from the forest to plot using the existing tools?</p>
"
"0.0400480865731637","0.039253433598943","205918","<p>My data follows a sigmoidal function of the form<br>
$$y=asym/(1+e^{(xmid-x)/scale)})$$<br>
I have taken the function from the SSLogis function in R.<br>
My supervisor and I think that there is a second variable that influences the asymptote and scale of the function.  </p>

<p>So I'm stuck with the questions:<br>
1. Is it at all possible to do a multiple regression based on this function?<br>
If yes, 2. Is it possible to transform the sigmoidal to a linear function and use it in a multiple linear regression?<br>
Or 3. Is there a way to do a multiple non-linear regression?</p>

<p>Any help is really welcome.</p>
"
"0.0700841515030364","0.0686935087981502","206058","<p>Using R, I can only find tools for performing L1 and/or L2 regularized linear regression (lars, glmnet) and tools for constrained linear regression (quadprog , or lsei {limSolve} , where the inequality and equality constraints can be only given in the form Ax = b , Gx &lt;= h).</p>

<p>It seems inutitive for me that the possibility of combining both should be required very oft when solving specific regression problems, but so far I havent been able to do it. </p>

<p>Instead of providing information on my specific set of constraints and algebraic system, IÂ´d be interested to know if this is a problem I can actually solve using the above mentioned packages? Are there any packages at all in R built for both parameter regularization and specific parameter penalties? </p>

<p>Update: For better understanding: I am not trying to combine different regularization methods (like in elastic net), nor trying to combine different parameter constraints. My goal is to combine regularization with specific coefficient constraints, so for example:</p>

<p>Find the most sparse solution (penalizing absolute values through LASSO) of a linear regression y = bx which satisfy the coefficient constraints bA &lt; h for some given matrix A and threshold h. </p>

<p>min($\parallel$ $\beta$x-y$\parallel_2^2$ + $\lambda$ $\mid$ $\beta$ $\mid_1$ ), s.t   $\beta$ A $\leqslant$ h</p>
"
"0.0424774103841449","0.0555127381653369","206117","<p>I'm studying linear regression model with censored data. ~ Buckley James Estimator.</p>

<p>For example, I have a data set which contains groups (two categories; 1,2). </p>

<p>I estimated bj regression category by category. To do that, I write code group==1 or group=2  in R software. But I get error. </p>

<p>What can I do?</p>

<p>For example, how to estimate survival time of treated people or non-treated people in buckley&amp;james regression in R?</p>

<p>Code;</p>

<p>df&lt;-bj(Surv(log10(time),status)~age+sex+semptom+hb1+dissek+lokal+evregrup+kt, data=data, link='ident',kt==1 , x=TRUE , y=TRUE)</p>

<p>The Error is that</p>

<blockquote>
  <blockquote>
    <p>Error in solvet(t(xm) %<em>% xm, t(xm) %</em>% yy, tol = tol) : 
      apparently singular matrix</p>
  </blockquote>
</blockquote>
"
"0.0899225958391357","0.0881383093888288","206119","<p>I have two sets of data from two groups of participants that are identical in all but one way: one set of data were collected using software A and the other set was collected with software B. It's possible that the outcome variables are roughly equivalent across groups, and it's possible that they're not. (The groups are quite similar otherwise; I've run chi-square tests on gender and race and there are no differences by the latter and only minor differences by the former. Regardless, such differences should not be systematic.)</p>

<p>What I want to do is run some preliminary tests to see whether there are significant effects of software engine on my three primary outcome variables. If there are, I'd then treat these as two separate experiments. If there are not, I'd instead group them together. </p>

<p>What I had in mind was to use a straightforward linear regression, as so (I'm using R, but obviously this is not specific to the stats program):</p>

<pre><code>combined &lt;- rbind(data1, data2)
reg1 &lt;- lm(outcome1 ~ engine, data=combined)
summary(reg1)
# and repeat for outcome2 and outcome3
</code></pre>

<p>If this regression shows a significant effect of engine for any of these outcome variables, then that would mean that I should keep the groups separate. It also occurred to me to do this with other covariates in the model: </p>

<pre><code>combined &lt;- rbind(data1, data2)
reg1 &lt;- lm(outcome1 ~ engine + age + gender + race, data=combined)
summary(reg1)
# and repeat for outcome2 and outcome3
</code></pre>

<p>In this case, I'd be seeing if engine had a significant effect <em>in the presence of other relevant variables</em>. </p>

<p>Any thoughts on which method makes more sense, or if there's a better way?</p>
"
"0.02831827358943","0.0277563690826684","206405","<p>I am working on one of the discrete probability distribution having pmf as</p>

<p>P(x)={p^log(1+x^c)}-{p^log(1+(x+1)^c)}</p>

<pre><code>  0&lt;p&lt;1; c&gt;0; x=0,1,2,.
</code></pre>

<p>It fits well to count data of DMFT in dentistry with p value 0.93.</p>

<p>Decays of teeth: 0, 1, 2, 3 ,4</p>

<p>observed count: 64,17,10,3,4</p>

<p>Expected count: 63,16,9,4,4</p>

<p>Now i need suggestions how add other explanatory variables like sex, age, etc. to use this as nonlinear regression model in R.</p>
"
"0.0633215847514023","0.0620651280774201","206454","<p>I wish to compute <code>MSE</code> of my models.  Say my data was generated from the following model:</p>

<p>$y_i=f(x_i)+e_i$ </p>

<p>where $e_i$ is some noise around the true relationship $f(x)$.  I estimated the function $f(x)$ as $f\hat(x)$, and now I'd like to compute the MSE.  </p>

<p>My professor often writes MSE as the following:</p>

<p>$1/n \sum_{i=1}^n (f(x_i)-f\hat(x_i))^2$</p>

<p>Let's say I know $f(x)$, the true function and I'm using it for simulation.</p>

<p>My question is, when I compute MSE, do I use my observations $y_i$?  or do I use the true function without the noise $f(x_i)$?  Because, the professor writes the true function in the formula above, but this means that computing MSE involves taking the difference between the functions at the $x_i$ value of each observation, without actually using the value $y_i$ of that observation?</p>

<p>This formulation seems much more intuitive to me:  </p>

<p>$1/n \sum_{i=1}^n (y_i-f\hat(x_i))^2$  , because this will actually capture the observations.</p>

<p>Which formulation is correct?  And when might one use one over the other?  Feel free to use linear regression as an example, since that will allow easy illustration.  </p>
"
"NaN","NaN","206576","<p><strong>Background</strong>:</p>

<p>The most recent statistics class I've taken in university goes over topics such as experimental design, tests for Normality, comparing two samples, categorical data analysis, linear regression, MLR, and power analysis. As a result, I am working towards a conceptual base in these topics.</p>

<p><strong>Question</strong>:</p>

<p>Are there any books that use R to reinforce these concepts and build on them towards advanced statistics such as advanced pattern recognition, if so which ones?</p>
"
"0.0693653206906364","0.0679889413649005","207999","<p>I am working on a project to predict a range for patient length of stay.  My data consists of 215,000 rows of the following variables (30 total):</p>

<ul>
<li><code>LOS</code> (length of stay in days)</li>
<li><code>AGE</code> (in years)</li>
<li><code>GENDER</code> </li>
<li><code>MARITAL</code></li>
<li><code>DIAGNOSIS 1</code></li>
<li><code>DIAGNOSIS 2</code></li>
<li><code>DIAGNOSIS 3</code></li>
<li>... and so on</li>
</ul>

<p>With the exception of <code>AGE</code> and <code>LOS</code>, all the variables are binary.  The distribution for <code>LOS</code> is heavily skewed - almost all values are between 1-30, with extreme outliers from 50-370 that account for only 0.02% of the data.</p>

<p>My approach to modeling the relationship between <code>LOS</code> and the rest of the variables is as follows.  First, remove the 0.02% outliers for the dependent variable.  Second, do a simple log transform of the dependent variable.  After taking these two steps, the <code>LOS</code> data is normally distributed.  </p>

<p>My question is - is there any reason why I should not simply use plain old multivariate linear regression on this normalized <code>LOS</code> data?  </p>

<p>When I do this, I get highly significant p-values and an R-squared of 0.207.  Which, as I understand it, isn't horrible for complex health care data (please correct me if I am wrong).  This approach also results in nicely distributed residuals.</p>

<p>However, I was looking up different data distributions to see if I should be modeling in a different way.  Other length of stay models on the internet treat the data as a Poisson distribution, which led me here to inquire and hopefully acquire a greater understanding of how to treat this data!  </p>

<p>So, is my methodology sound in this case?  Any and all feedback is greatly appreciated!</p>
"
"0.0633215847514023","0.0620651280774201","208149","<p>I have been struggling with this problem for several months. I would really appreciate if someone could help me solve this.
I am working on a pairwise relationship as shown in the data below (<code>new_cars</code>) which shows the theoretical efficiency of cars if combined together(i.e. hybrids). I have principal component analysis done based on their multiple specifications(including oil efficiency, engine power etc.) obtained from their practical use. I want to correct for any biases in the theoretical values for the hybrid cars using pca data. I want to fit the linear regression model and replace the values with <code>fitted.values</code>. The formula for fitted value is <code>fit1&lt;-lm(a_car~PCA1+PCA2+PCA3+PCA4)</code>, but the problem is I need to do this for pairwise cars. Since PCA is for point, I want to expand this to pairwise relationship and correct for the respective pairs using these prinicipal components. Any suggestion would be really appreciated. </p>

<p>Fitted.values can be obtained from fit1$fitted.values</p>

<pre><code>  new_cars&lt;-structure(list(mercedes = c(0.8, 0.7, 0.5, 0.3, 0.9), vw = c(0.7, 
0.6, 0.4, 0.2, 0.8), camry = c(0.5, 0.4, 0.3, 0.1, 0.6), civic = c(0.3, 
0.2, 0.1, 0.05, 0.4), ferari = c(0.9, 0.8, 0.6, 0.4, 0.93), PCA1 = c(0.021122, 
0.019087, 0.022184, 0.021464, 0.021122), PCA2 = c(0.023872, 0.024295, 
0.022471, 0.027509, 0.023872), PCA3 = c(0.000784, 0.001996, 0.003911, 
0.006119, 0.000784), PCA4 = c(-0.004811, -0.003296, 0.001868, 
-0.001636, -0.004811)), .Names = c(""mercedes"", ""vw"", ""camry"", 
""civic"", ""ferari"", ""PCA1"", ""PCA2"", ""PCA3"", ""PCA4""), row.names = c(""mercedes"", 
""vw"", ""camry"", ""civic"", ""ferari""), class = ""data.frame"")
</code></pre>
"
"0.02831827358943","0.0277563690826684","208277","<p>I find it difficult to connect the coefficients of a regression model that includes splines to the actual prediction equation. For example, how could that be done with the following model?</p>

<pre><code>&gt; library(rms)
&gt; x &lt;- 1:11
&gt; y &lt;- c(0.2,0.40, 0.6, 0.75, 0.88, 0.99, 1.1, 1.15, 1.16, 1.16, 1.16 )
&gt; dd &lt;- datadist(x); options(datadist='dd')
&gt;  f &lt;- ols(y ~ rcs(x, c(3, 5, 7, 9)))
&gt; f  


  Linear Regression Model

ols(formula = y ~ rcs(x, c(3, 5, 7, 9)))

            Model Likelihood     Discrimination    
               Ratio Test           Indexes        
Obs       11    LR chi2     66.08    R2       0.998    
sigma 0.0201    d.f.            3    R2 adj   0.996    
d.f.       7    Pr(&gt; chi2) 0.0000    g        0.383    

Residuals

  Min        1Q    Median        3Q       Max 
-0.027360 -0.011739  0.001227  0.009892  0.031166 

           Coef    S.E.   t     Pr(&gt;|t|)
Intercept  0.0465 0.0224  2.08 0.0762  
x          0.1741 0.0072 24.18 &lt;0.0001 
x'        -0.1004 0.0311 -3.23 0.0144  
x''        0.0542 0.0913  0.59 0.5715  


&gt; Function(f)
function(x = 6){
  0.046475489 + 0.17411942*x - 0.002790266*pmax(x-3,0)^3 + 
  0.0015048699*pmax(x-5,0)^3 + 0.0053610582*pmax(x-7,0)^3 - 
  0.0040756621*pmax(x-9,0)^3 
}
</code></pre>
"
"NaN","NaN","208367","<p>I'm intended to run a linear regression model (Rain~dBZ) for my data set.</p>

<p>I would like to know how to transform non-normal set of ""Rain"" column in to a normal distribution.</p>

<p>I would really appreciate it to have your kindly assistant.</p>

<p>I attach the data-set in the following link:</p>

<p><a href=""https://drive.google.com/file/d/0B7aMnS118Vltd19UdThrNVVVcDg/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B7aMnS118Vltd19UdThrNVVVcDg/view?usp=sharing</a></p>
"
"0.0490486886395286","0.0480754414848157","208574","<p>Dataset and goal:
One continuous measurement( to be modeled as a dependent variable) and four other measurements (one binary  and the rest are category variables with multiple levels) to be modeled as independent variables.</p>

<p>dataset samples: roughly 3 million rows</p>

<p>model used: multilinear regression.</p>

<p>The binary independent variable is very heavily skewed.i.e. 3% samples are true and the rest are false. </p>

<p>Does it make sense to use such an independent variable in a multi linear regression model?. what should I be careful about while modeling with such a situation?</p>
"
"0.02831827358943","0.0277563690826684","208691","<p>I have a dataset of 30 social variables such as Facebook Likes, Posts, Comments, etc. I would like to see if these variables predict Website Views. </p>

<p>MY problem is I only have 3 months of data- or 3 data points. Thus I have a 3 by 30 dataset. My question is, how do I model with such few data points but lots of variables?</p>

<p>In addition, I would love a resource that walks through regression in R. My data does not even remotely look linear, so I don't know what to do next. Suggestions?</p>
"
"0.0400480865731637","0.039253433598943","209030","<p>I fitted a Cox PH model in R with the survival package and the <code>coxph</code> function.
I get the beta estimates from this model.
How can I use these coefficients to manually predict on new data, like the predict function does.</p>

<p>In a linear regression this is just the matrix multiplication <code>X %*% beta</code> if $X$ is the data and $beta$ is the vector of coefficients.</p>

<p>How is this in the Cox model? I also see that predict has several options for types of predictions.</p>

<p>here is a minimal example:</p>

<pre><code>library(survival)
data(""ovarian"")
m &lt;- coxph(formula = Surv(futime, fustat) ~., data=ovarian)
</code></pre>

<p>these two give different results:</p>

<pre><code>head(as.matrix(ovarian[, -c(1:2)]) %*% m$coefficients)

      [,1]
1 10.102002
2 10.371810
3  9.706097
4  6.820160
5  7.357138
6  7.627324

head(predict(m, ovarian))
          1           2           3           4           5           6 
 2.66935119  2.93915962  2.27344680 -0.61249088 -0.07551308  0.19467374 
</code></pre>
"
"0.0626851707624734","0.0614413421142575","209243","<p>I am doing some regression analysis and in certain cases I'm getting errors I cannot understand.</p>

<p>R environment image is shared here: <a href=""https://www.dropbox.com/s/n6opew73l7xgcbz/data.RData?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/n6opew73l7xgcbz/data.RData?dl=0</a></p>

<p>And what I am doing with errors I get:</p>

<p>Something about infinite vector length:</p>

<pre><code>regsubsets(data$UzaverRekonstrukce ~  data$PacientRocnik + data$PacientIchs + data$PacientDm + data$PacientHyperlipidemie + data$PacientAh + data$PacientKoureni + data$Koncetina + data$UmisteniOperace + data$Material + data$BercoveTepny + data$PedalniOblouk + data$Antikoagulace + data$Antiagregace + data$Vykon + data$ChirRevProxAnas + data$ChirPlastProxAnas + data$TrombectomieBypassu + data$Eni + data$PtaDistAnas + data$Infekce + data$TerapieInfekce , nvmax = 5, really.big = T , nbest = 3, data)

Error in numeric(nbest * nvmax) : vector size cannot be infinite
In addition: Warning messages:
1: In cbind(1, xx) :
  number of rows of result is not a multiple of vector length (arg 1)
2: In leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in =     force.in,  :
  31  linear dependencies found
3: In max((1:np)[!rightorder]) :
  no non-missing arguments to max; returning -Inf
4: In leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in =     force.in,  :
  nvmax reduced to  -Inf
5: In leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in =     force.in,  :
  NAs introduced by coercion to integer range
</code></pre>

<p>This one is OK:</p>

<pre><code>regsubsets(data$UzaverRekonstrukce ~  data$PacientRocnik + data$PacientIchs + data$PacientDm + data$PacientHyperlipidemie + data$PacientAh + data$PacientKoureni + data$Koncetina + data$UmisteniOperace + data$Material + data$BercoveTepny + data$PedalniOblouk + data$Antikoagulace + data$Antiagregace + data$Vykon + data$ChirRevProxAnas + data$ChirPlastProxAnas + data$TrombectomieBypassu + data$Eni + data$PtaDistAnas + data$Infekce, nvmax = 5, really.big = T , nbest = 3, data)
</code></pre>

<p>And something about different input data length:</p>

<pre><code>regsubsets(data$UzaverRekonstrukce ~  data$PacientRocnik + data$PacientIchs + data$PacientDm + data$PacientHyperlipidemie + data$PacientAh + data$PacientKoureni + data$Koncetina + data$UmisteniOperace + data$Material + data$BercoveTepny + data$PedalniOblouk + data$Antikoagulace + data$Antiagregace + data$Vykon + data$ChirRevProxAnas + data$ChirPlastProxAnas + data$TrombectomieBypassu + data$Eni + data$PtaDistAnas + data$Infekce + data$PuvodceInfekce , nvmax = 5, really.big = T , nbest = 3, data)

Error in leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in = force.in,  : 
  y and x different lengths
</code></pre>

<p>I am very confused of this messages. I understand there are many variables and only few observations (in this dataset). But I don't understand that in one case it is OK, add one predictor - infinite length error, remove one variable - different X and Y errors.</p>

<p>I would be glad if someone could explain me the problem. Optimally how to detect problems leading to this errors before running the regsubsets.</p>
"
"0.0991139575630048","0.0971472917893396","209247","<p>I am trying to predict surface temperature using solar energy. I have 3650 daily averages for both variables. The plots of both are below: </p>

<p><a href=""http://i.stack.imgur.com/ywSHs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ywSHs.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/JjFDG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JjFDG.png"" alt=""enter image description here""></a></p>

<p>I attempt to seasonally adjust with a periodic regression in R for both: </p>

<pre><code>stmp.model.sa &lt;- lm(stmp ~ sin((2*pi/365)*t) + cos((2*pi/365)*t))
slrd.model.sa &lt;- lm(slrd ~ sin((2*pi/365)*t) + cos((2*pi/365)*t))
</code></pre>

<p>Here are the plots of the residuals from these models: 
<a href=""http://i.stack.imgur.com/jhdLw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jhdLw.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/fyzuq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fyzuq.png"" alt=""enter image description here""></a></p>

<p>As you can see, the temperature data responded well to the treatment. The solar energy data did not, as the yearly humps are still somewhat present.</p>

<p>A few questions: </p>

<ul>
<li>Is there a more effective way to remove seasonality for the solar data? </li>
<li>Is further treatment of the temperature trend recommended? Would a polynomial regression be sufficient to remove this trend?</li>
<li>What model might be recommended to predict the temperature? Linear model, ARIMA/ARIMAX models, linear regression with ARIMA errors, etc? </li>
</ul>

<p>Thanks in advance for any responses! </p>

<p>EDIT:</p>

<p>I attempted to apply a Hodrick Prescott filter (lambda = 100*365^2) with poor results. I then attempted to fit a cycle to the solar data using a 20 period moving maximum. This was done using the following code:</p>

<pre><code>seq &lt;- 11:(n-11)
ns &lt;- length(seq)
slrd.seasonal &lt;- slrd[seq]
for(i in seq){
  slrd.seasonal[i-10] &lt;- max(slrd[(i-10):(i+10)])
}
</code></pre>

<p>The moving maximum, the solar cycle, and the cycle subtracted from the original series is presented below: </p>

<p><a href=""http://i.stack.imgur.com/k1RuT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/k1RuT.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/2QwSR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2QwSR.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/IMMjE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IMMjE.png"" alt=""enter image description here""></a></p>

<p>This did not remove the yearly cycle entirely either. Any advice?
EDIT 2: </p>

<p>I have successfully removed seasonality by fitting a 20 order polynomial to the first three years (using more years proves computationally difficult). If anyone can think of a better or more elegant way to achieve this, let me know. </p>
"
"0.0980973772790571","0.0961508829696314","209412","<p>I'm trying to build a bivariate copula-based model of income and wealth in Italy and I'm having trouble handling weighted data. I have access to micro data, a survey of about 10,000 households that includes the corresponding sample weights.</p>

<p>When calculating basic statistics (like mean and median) and even when performing linear regressions it is pretty easy to account for weights, besides there are useful packages for that (e. g. survey). But what do I do when I want to fit a parametric model of the distribution to weighted data? Or to estimate its kernel density?</p>

<p>I have a few ideas, but they seem to be pretty crude. For one, I could inflate my sample to the size of the universe. That is, I could multiply all weights by 100 (which would turn them into integers) and then create a vector that repeats each value of income and wealth a given number of times. But that would lead to a very large sample (which I believe still wouldn't be a perfect representation of the population) and will certainly put some extra strain on my computer.</p>

<p>I could also just round the weights off instead of multiplying them by 100, but this would still make the sample noticeably bigger and will inevitably skew the real proportions.</p>

<p>Another approach I came up with would be to normalize the weights (so that they sum up to one) and then randomly sample with repetitions from my initial sample with the corresponding vector of probability weights. R doesn't allow to draw the samples that are larger in size than the one that they are being drawn from. But I think that drawing the sample of the same size as the initial one will lead to some loss of information about the observed proportions. So I could draw the samples of the initial size as described above several times (how would I know how many is though?) and then combine them into one sample. And again, I will have a larger sample with some of the information lost along the way.</p>

<p>So I was wondering if there is a better way to handle weighted data. In some cases I think I could technically introduce the weights into the formula for computing the maximum likelihood for fitting a particular model, although I certainly wouldn't like to code that from the ground up. I will have to fit a lot of models as part of my project, both univariate (e. g. Singh-Mandala) for income and wealth and bivariate for copulas. I don't think the built in functions in any of the copula-related packages that I'm aware of allow one to account for weights. So any advice would help!</p>
"
"0.0578044339088637","0.0566574511374171","209421","<p>If I have theoretical reasons to suppose the data might be fit with an unusual equation such as the following:</p>

<p>$$Y_i = (\beta_0 + \beta_1x_{1i} +  \beta_2x_{2i} + \epsilon_i)^{\beta_3}$$</p>

<p>Can I use Ordinary Least Squares Multiple Linear Regression after a transformation to estimate parameters $\beta{_0,_1,_2,_3}$? If yes, what transformation?</p>

<p>If not, is there some specialized package in R (and brief reading) that might help me compare the fit and residuals from this model against a more typical MLR model?</p>

<p>Thanks.</p>

<p>Example Code:</p>

<pre><code>## while I can run ""nls,"" I cannot get $\epsilon$ inside parentheses nor
## can I have four BETAs

var1 &lt;- rnorm(50, 100, 1)
var2 &lt;- rnorm(50, 120, 2)
var3 &lt;- rnorm(50, 500, 5)

## make a model without $\beta_1$ and $\beta_2$ and with $\epsilon_i$ on outside
nls(var3 ~ (a + var1 + var2)^b, start = list(a = 0.12345, b = 0.54321))

Nonlinear regression model
  model: var3 ~ (a + var1 + var2)^b
  data: parent.frame()
   a        b 
 475.5234   0.9497 
 residual sum-of-squares: 1365

Number of iterations to convergence: 6 
Achieved convergence tolerance: 8.332e-08

## FAILS with exponent on left-hand side and $\epsilon$ inside parentheses
nls(var3^(1/b) ~ (a + var1 + var2), start = list(a = 0.12345, b = 0.54321))
Error in eval(expr, envir, enclos) : object 'b' not found

## FAILS with all BETAs
nls(var3 ~ (a + b*var1 + c*var2)^d, start = list(a = 4, b = 1, c = 1, d = 1))
Error in numericDeriv(form[[3L]], names(ind), env) : 
Missing value or an infinity produced when evaluating the model
</code></pre>
"
"0.123436492831862","0.114619460088286","209864","<p>This is a very simple exercise that I'm hoping may help people with limited knowledge in statistical analysis (like myself). 
I am having trouble deciding what statistical analysis I can perform (in R) to determine whether or not my data are closer to one linear model or another. </p>

<p>For example: I have measurements of sodium and chloride in various dilute solutions: </p>

<pre><code>#
Na &lt;- c(1.56, 1.00, 1.60, 3.23, 2.02, 2.81, 2.09, 26.24, 1.59, 0.42)
Cl &lt;- c(1.40, 0.91, 1.22, 2.67, 1.67, 3.01, 2.17, 27.42, 1.45, 0.51)
</code></pre>

<p>For simplicity, this solution is a dilution of either table salt dissolved in water or natural seawater. For each case, Cl/Na will be a specific ratio that reflects the composition of the original solution. We can visualize this by:</p>

<pre><code>plot(Na,Cl)
abline(0,1)    # expected slope for table salt dissolved in water
abline(0,1.16) # expected slope for natural seawater.
</code></pre>

<p>I want to know which model, table salt in water or seawater, is a more statistically accurate fit to the provided data. Linear regression analysis in R gives a line of best fit with a slope of 1.05 (<code>lm(Cl~Na)</code>), right in between the two models.</p>

<p>So, which solution do I more likely have and why? The line of best fit slope is closer to that of table salt dissolved in water, but that does not seem very statistically sound. Thoughts? </p>

<p>Edit: @whuber mentioned that there is one anomaly in the dataset - in reality, the provided data is just a subset of the original data. There are actually hundreds of data points in between the apparent outlier and the rest of the provided data.</p>

<p>Also, here is a <code>log(Na)-log(Cl)</code> summary of the complete dataset:</p>

<pre><code>    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's 
-0.46870 -0.06186  0.02654  0.02218  0.12780  0.47510      183 
</code></pre>

<p>Edit2: As for the ""true nature of my investigation"": The 'solution' in question is likely a mixture of both table salt water and natural seawater. What I'd like to do is find a definitive way (through statistical analysis) to show that I have more of one or the other. I had hoped that my simplified question/dataset would yield an answer from the community, but it seems I was off base. If it helps, a complete dataset is now hosted below:</p>

<p><a href=""http://www.filedropper.com/clna"" rel=""nofollow"">http://www.filedropper.com/clna</a></p>

<p>Looking at the distribution of the complete data shows I have more Cl/Na about 1.00, but this does not seem 'sound enough' to back up an argument. The probability that I have one solution or the other is unknown. I have the raw data and relevant models for Cl to Na to run with.</p>

<p>For clarification, the original question is still the one I'd like to solve.  An alternative question could be: Which solution do I have <em>more</em> of and what analysis did I use to come to that conclusion?</p>
"
"0.0633215847514023","0.0620651280774201","209912","<p>I have a quick question I was hoping to get your input on. I am new to R and the smooth statistical regression world, and am trying to wrap my mind around the issues concerning using splines for mixed effect modeling.</p>

<p>My question is the following: in the â€˜gammâ€™ function, generalized additive mixed models can be estimated by including random components. These can be explicitly defined in the syntax, where you can also define whether the random component is an intercept, slope, or both. My understanding is that in the gam/bam function the same is achieved by including the bs=""reâ€ option for random intercepts and linear random slopes. Am I correct? If so, is there a way to specify whether it is the intercept or slope we are interested in, and does that have any effect on the output of the model?</p>

<p>I hope this question make sense, and I look forward to learning more about this.  Thanks for taking the time to read through this.</p>
"
"0.0853828074607","0.0920574617898323","209999","<p>Let's say I fit a regression model to standardized data and then use it to predict out-of-sample data. I evaluate model performance using mean squared error.</p>

<p>I compare this model to another model that simply selects a single predictor and 
calculates mean squared error as: <code>mean((DV - single predictor)^2)</code></p>

<p>The weird thing is that this single predictor model performs better than regression.</p>

<p>What am I missing?</p>

<pre><code>    #load data
    data(mtcars)

    #standardize data
    data &lt;- scale(mtcars)

    #matrix for storing results of a 100 replications
    MSE &lt;- matrix(0,ncol=2,nrow=100)

    #replicate procedure 100 times
    for(reps in 1:100){

    #divide data into training and test set
    train &lt;- sample(1:nrow(data),nrow(data)*0.5)
    test &lt;- setdiff(1:nrow(data),train)

    train.data &lt;- as.data.frame(data[train,])
    test.data &lt;- as.data.frame(data[test,])

    #fit linear regression and get predictions for test data
    fit &lt;- lm(train.data[,1]~.,data=train.data[,2:ncol(train.data)])
    preds &lt;- suppressWarnings(predict(fit,test.data))

    #calculate mean squared error for regression  
    MSE[reps,1] &lt;- mean((test.data[,1] - preds)^2)

    #calculate mean squared error for the single variable prediction
    MSE[reps,2] &lt;- mean((test.data[,1] - test.data[,5])^2)

    }

    #compare MSE for regression and single variable prediction.    
    colMeans(MSE)
    [1] 1.3268521 0.6020453
</code></pre>
"
"0.09392108820677","0.0920574617898323","210015","<p>I have a statistical question.</p>

<p>I have data from an experiment with two conditions (dichotomous IV: 'condition'). I also want to make use of another IV which is metric ('hh'). My DV is also metric ('attention.hh'). I've already run a multiple regression model with an interaction of my IVs. Therefore, I centered the metric IV by doing this:</p>

<pre><code>hh.cen &lt;- as.numeric(scale(data$hh, scale = FALSE))
</code></pre>

<p>with these variables I ran the following analysis:</p>

<pre><code>model.hh &lt;- lm(attention.hh ~ hh.cen * condition, data = data)
summary(model.hh)
</code></pre>

<p>The results are as follows:</p>

<pre><code>Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)        0.04309    3.83335   0.011    0.991
hh.cen             4.97842    7.80610   0.638    0.525
condition          4.70662    5.63801   0.835    0.406
hh.cen:condition -13.83022   11.06636  -1.250    0.215
</code></pre>

<p>However, the theory behind my analysis tells me, that I should expect a quadratic relation of my metric IV (hh) and the DV (but only in one condition).</p>

<p>Looking at the plot, one could at least imply this relation:
<a href=""http://i.stack.imgur.com/k47jD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/k47jD.png"" alt=""enter image description here""></a></p>

<p>Of course I want to test this statistically. However, I'm struggling now if and when to center the metric IV, because the interaction is still important (as there is only a quadratic relation in one condition).</p>

<p>Do I first center the variable and then compute the quadratic term or the other way round?
Do I compute the interaction with both the linear and the quadratic term or only one of them?</p>

<p>My gut feeling would suggest something like this:</p>

<pre><code>hh.sqr &lt;- hh * hh

sqr.model.hh &lt;- lm(attention.hh ~ hh.sqr + hh.cen * condition, data = data)
    summary(sqr.model.hh)
</code></pre>

<p>In a nutshell, I want to test whether there is a quadratic relation in one of my conditions.
However, I am not sure which terms I have to include into the model (or whether I calculate hh.sqr * condition vs. hh.cen * condition -- or both)?!</p>
"
"0.0800961731463273","0.078506867197886","210515","<p>Here are some sample data in R:</p>

<pre><code>set.seed(42)
df &lt;- data.frame(g = factor(rep(1:2, each= 50)), y = rnorm(100)+rep(0:1, each=50))
</code></pre>

<p>One can easily get group means using e.g. <code>with(df, tapply(y,g,mean))</code> but there is no such easy way to get the confidence intervals for group means. This is why I tried:</p>

<pre><code>lm(y ~ g-1, df)
# correct group means:
# -0.03567   1.10070 
confint(lm(y~g-1, df))
# too narrow CI's
#         2.5 %    97.5 %
# g1 -0.3287751 0.2574315
# g2  0.8075981 1.3938047
</code></pre>

<p>That is, one can estimate the group means using a linear model with dummy group indicators, omitting the overall intercept. But the confidence intervals of these regression parameters are narrower than the confidence intervals of group means. The latter could be found group by group with the same function:</p>

<pre><code>confint(lm(y~1, data=df, subset=g==1))
#                  2.5 %    97.5 %
# (Intercept) -0.3629177 0.2915742
</code></pre>

<p>Or a manual check using textbook formulae:</p>

<pre><code>ci.mean &lt;- function(x, alfa=0.05){
   n &lt;- length(x)
   a&lt;-qt(1-alfa/2, n-1)
   m&lt;-mean(x);          s&lt;-sd(x)
   se&lt;-s/sqrt(n)
   res &lt;- c(m, m-a*se,m+a*se)
   setNames(res, c(""mean"", paste(100*alfa/2, ""%""), paste(100*(1-alfa/2), ""%"")))
}
ci.mean(with(df, y[g==1])
#        mean       2.5 %      97.5 % 
# -0.03567178 -0.36291775  0.29157418 
</code></pre>

<p>There is probably an easy answer to the question of why the CIs of seemingly the same parameters are different. (The answer, obviously, has to start with difference in standard errors.) But I would be interested in the interpretation: why is that I can trust the group means found with <code>lm(y~g-1)</code> but I can't trust the confidence intervals around those ""means"" found with <code>confint(lm(y~g-1))</code>? And another naive question, why is the standard error for a group mean smaller if another group is present? That is:</p>

<pre><code>coef(summary(lm(y~g-1, df)))[1,2]
# [1] 0.1476987
coef(summary(lm(y~1, df, subset=g==1)))[1,2]
# [1] 0.1628433
</code></pre>

<p>Again, I am more interested in the substantial interpretation than  the formula showing why this is so. (I suppose after some sleep I could figure out the formula but would still be in trouble with interpretation).</p>

<p>Thanks in advance!</p>
"
"0.0700841515030364","0.078506867197886","210697","<p><strong>Problem</strong></p>

<p>Suppose I have two variables: (1) heat index for each county in a state, $h_{it}$, and (2) acres in each county, $acres_{it}$.  The data has 10 years and also includes a variable for the amount of ice cream melted, $y_{it}$ for each county and year in the sample.  </p>

<p>I'm told that a strong predictor of ice cream melt can be found by weighting the heat index by the size of the county and then aggregate to state-level data, such that:   </p>

<p>$$\frac{\sum_{i} h_{it} \cdot acres_{it}}{\sum_{i} acres_{it}} = h_{st}$$</p>

<p>A simple linear regression can then predict the state-level ice cream melt by:</p>

<p>$$y_{st} = \beta_{1}h_{st} + \epsilon_{it}$$</p>

<p><strong>Question</strong></p>

<p>My question is related to the weighting.  Is there a specific name for this type of weight and how is the weighting performed?</p>

<p>To me, I'm thinking about it as subsetting each year, applying the weight for all counties in that year, and then aggregate down to state level data.  This would provide a heat index for each state in each year.  </p>

<p>Is this the appropriate way to do this type of weighting?</p>

<p><strong><code>R</code> Code:</strong></p>

<p>In <code>R</code> I would do something like this:</p>

<pre><code>library(dplyr)

# Sample Data
dat &lt;- structure(list(year = c(2000L, 2001L, 2002L, 2000L, 2001L, 2002L, 
2000L, 2001L, 2002L, 2000L, 2001L, 2002L), county = c(1L, 1L, 
1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L), state = c(""CA"", ""CA"", 
""CA"", ""CA"", ""CA"", ""CA"", ""CO"", ""CO"", ""CO"", ""CO"", ""CO"", ""CO""), 
    y = c(5L, 10L, 7L, 4L, 2L, 8L, 9L, 11L, 2L, 5L, 6L, 8L), 
    h = c(5L, 7L, 1L, 9L, 6L, 4L, 8L, 2L, 5L, 8L, 7L, 1L), acres = c(10L, 
    25L, 40L, 8L, 13L, 42L, 50L, 24L, 57L, 24L, 35L, 15L)), .Names = c(""year"", 
""county"", ""state"", ""y"", ""h"", ""acres""), class = ""data.frame"", row.names = c(NA, 
-12L))


# Build Weighted Variable
dat &lt;- dat %&gt;% 
  group_by(year) %&gt;% 
  mutate(w = acres/sum(acres, na.rm = TRUE))

# Apply Weight
dat$h &lt;- dat$h * dat$w

# Aggregate to State-level
dat &lt;- dat %&gt;% 
 group_by(year, state) %&gt;% 
  summarise(h = sum(h, na.rm = TRUE))
</code></pre>
"
"0.02831827358943","0.0277563690826684","210855","<p>I have 3 variables of my cellular recordings and made 9 linear regressions for their connection to fourth variable (a~x1...9, a~y1...9 and a~z1...9). All variables are random, not controlled.</p>

<p>Each variable is represented in a (very) long numeric sequence; all four variable measurements were made every 2ms simultaneously, but their summary lengths differ.
I have to find out how much influence is there of my ""x"" , ""y"" and ""z"" on ""a"".</p>

<p>What is the best way to generalize my linear models to draw a conclusion about the relations between these variables and to represent them graphically?</p>
"
"0.0633215847514023","0.0620651280774201","211562","<p>I'm trying to learn how to use R by replicating the regression of the <a href=""http://dx.doi.org/10.2307/2118477"" rel=""nofollow"">MRW 1992</a> paper (see Table 1). I've done this in Mathematica, and I got the right coefficients, even for the intercept.</p>

<p>The following R script is what I'm using for this example:</p>

<pre><code>library(foreign)
mrw &lt;- read.dta(""https://www.nuffield.ox.ac.uk/teaching/economics/bond/mrw.dta"")

mrw$popgrowth &lt;- mrw$popgrowth/100 #the data is in percentage points
mrw$gdpgrowth &lt;- mrw$gdpgrowth/100 #the data is in percentage points

noil &lt;- mrw[mrw$n==1,] #non-oil countries

form1noil &lt;- log(rgdpw85) ~ 1+log(i_y)+log(popgrowth+0.05) #we have to add 0.05 (see paper)
noil.lm &lt;- lm(form1noil, data = noil)
summary(noil.lm) #the intercept has the wrong value !
#Call:
#lm(formula = form1noil, data = noil)
#
#Residuals:
#     Min       1Q   Median       3Q      Max 
#-1.79144 -0.39367  0.04124  0.43368  1.58046 
#
#Coefficients:
#                      Estimate Std. Error t value Pr(&gt;|t|)    
#(Intercept)            -1.1279     1.4274  -0.790 0.431371    
#log(i_y)                1.4240     0.1431   9.951  &lt; 2e-16 ***
#log(popgrowth + 0.05)  -1.9898     0.5634  -3.532 0.000639 ***
#---
#Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
#
#Residual standard error: 0.6891 on 95 degrees of freedom
#Multiple R-squared:  0.6009,   Adjusted R-squared:  0.5925 
#F-statistic: 71.51 on 2 and 95 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>The strange thing is that I get the wrong intercept, but all the remaining betas are correct, and so are the R^2 and the std errors. Why is that?</p>

<p>Any help would be appreciated.</p>

<p>EDIT:  Mathematica Code and output (data is imported from an excel file, with same values. sav is the i_y column)</p>

<pre><code>YL85 = data[[1, 2 ;; n, 3]];
sav = data[[1, 2 ;; n, 6]]/100;
popgr = data[[1, 2 ;; n, 5]]/100;

logYL85 = Log[YL85];
logsav = Log[sav];
logrates = Log[popgr + 0.05];

lm = LinearModelFit[
   Transpose[{logsav, logrates, logYL85}], {x1, x2}, {x1, x2}];

Normal[lm]

5.42988 + 1.42401 x1 - 1.98977 x2
</code></pre>
"
"0.0749231094763201","0.0734364498908627","212007","<p>Hellow,</p>

<ol>
<li><p>In the first time,I want to do a multiple linear regression based on 3 parameters (d=fn(a,b,c).</p>

<pre><code>##
data&lt;-read.table(file=file.choose(), sep=""\t"", header=TRUE)
head(data)

# model

model=lm(d~a+b+c,data=data)
vals=round(predict(model),2) 
vals
summary(model)

plot(data$d,vals,xlim=c(0,1.2),ylim=c(-0.2,1.2),,col='orange')
model=lm(data$d~vals)
coef(model)
abline(model, col=""orange"",lwd=2)
##
</code></pre>

<p>But I have a result with a negative intercept, so I will probably have negative values.</p>

<pre><code>d = -0.3349742 + 0.9409406*a + 0.0027562*b + 0.0222850*c
</code></pre></li>
<li><p>In order to avoid these negative values, I tried to force the model through the origin 0</p>

<pre><code># model through origin 

model0=lm(d~a+b+c-1,data=data)
vals0=round(predict(model0),2) 
vals0

par(new=T)

plot(data$d,vals0,col='blue')
model0=lm(data$d~vals0)
coef(model0)
abline(model0, col=""blue"",lwd=2)
</code></pre>

<p>This is true in the new model does not intercept,</p>

<pre><code>d = 1.0716260 * a + 0.0012543 * b +  0.0033465 * c
</code></pre>

<p>But when I display a graph between observed and modeled parameter (d), the curve does not through the origin. it displays a curve with an intercept = -0.2205448</p></li>
</ol>

<p>How can I force my regression through 0, and have a model fn = (a, b, c) without the intercept? and without much change in the shape of the first curve (ie it must remain close to the curve 1: 1)</p>

<p>Thank you</p>

<pre><code>data 

a   b   c   d
0.351   300 8.30    1.20
0.396   293 7.52    1.09
0.300   278 7.37    1.09
0.513   263 7.81    1.08
0.469   286 2.49    1.05
0.411   290 6.26    1.04
0.39    191 10.89   0.98
0.49    221 6.57    0.91
0.40    226 7.29    0.88
0.35    286 6.91    0.86
0.40    217 4.98    0.85
0.32    249 5.80    0.83
0.341   262 4.45    0.82
0.24    238 6.91    0.79
0.34    185 7.22    0.78
0.35    249 9.12    0.76
0.42    232 6.73    0.75
0.45    185 10.06   0.73
0.24    217 8.46    0.73
0.29    189 5.93    0.73
0.39    202 3.83    0.71
0.16    242 5.79    0.71
0.20    235 7.94    0.70
0.17    237 7.91    0.70
0.39    144 6.67    0.69
0.21    226 5.93    0.68
0.38    151 6.82    0.68
0.30    206 5.59    0.67
0.45    190 5.02    0.67
0.26    191 6.29    0.66
0.35    253 4.26    0.66
0.43    232 5.23    0.66
0.38    195 3.56    0.65
0.31    156 9.86    0.64
0.22    181 5.07    0.63
0.30    142 3.79    0.62
0.18    181 7.02    0.61
0.40    157 11.01   0.58
0.308   164 4.44    0.57
0.20    181 3.84    0.57
0.14    197 3.98    0.56
0.39    129 10.87   0.55
0.38    167 5.96    0.55
0.12    190 4.81    0.53
0.28    188 6.47    0.52
0.31    128 3.15    0.51
0.23    146 5.28    0.49
0.31    195 6.34    0.49
0.315   156 3.30    0.49
0.21    152 4.67    0.48
0.42    157 3.24    0.47
0.25    148 4.69    0.47
0.14    191 5.61    0.47
0.18    113 5.96    0.46
0.280   149 7.78    0.46
0.278   157 2.50    0.46
0.13    130 5.50    0.46
0.17    152 4.96    0.46
0.23    138 5.37    0.44
0.14    169 5.21    0.44
0.180   167 4.63    0.44
0.18    123 4.91    0.40
0.280   132 3.07    0.40
0.14    168 6.78    0.39
0.12    154 4.09    0.38
0.25    152 3.48    0.37
0.19    128 2.89    0.36
0.167   151 8.01    0.36
0.12    120 4.55    0.36
0.24    137 3.34    0.35
0.25    135 7.68    0.33
0.258   136 3.22    0.31
0.25    133 2.56    0.31
0.228   126 5.18    0.30
0.18    202 5.38    0.30
0.24    148 2.90    0.29
0.16    210 4.47    0.29
0.20    123 4.17    0.29
0.28    137 4.80    0.27
0.28    139 6.43    0.27
0.30    139 4.95    0.27
0.20    190 5.16    0.25
0.13    103 2.31    0.24
0.08    122 3.78    0.23
0.14    197 3.61    0.22
0.11    101 1.95    0.22
0.15    104 5.53    0.22
0.18    112 3.88    0.22
0.15    134 5.69    0.22
0.11    97  4.61    0.22
0.074   105 5.79    0.21
0.13    181 4.43    0.20
0.10    103 3.17    0.20
0.023   65  0.82    0.20
0.12    115 4.68    0.20
0.13    190 4.00    0.18
0.24    113 4.18    0.18
0.13    84  3.94    0.18
0.09    166 3.00    0.17
0.15    116 2.70    0.16
0.21    109 3.55    0.15
0.107   105 1.93    0.15
0.04    99  3.36    0.13
0.30    98  3.18    0.12
0.13    160 4.19    0.11
0.11    79  3.23    0.10
0.08    78  2.04    0.10
0.10    138 2.95    0.09
0.17    141 3.99    0.09
0.16    150 2.76    0.09
0.13    162 3.98    0.09
0.03    92  2.54    0.09
0.14    110 2.55    0.06
0.12    118 4.48    0.06
0.13    116 2.27    0.06
0.07    109 2.23    0.05
0.05    94  0.95    0.05
0.06    89  2.97    0.03
0.06    81  2.20    0.03
0.07    179 1.93    0.03
0.07    124 2.16    0.03
0.08    84  8.78    0.03
0.06    103 1.71    0.02
0.07    89  6.09    0.01
0.02    99  1.23    0.01
0.05    75  3.61    0.01
0.05    74  3.12    0.00
</code></pre>
"
"0.0693653206906364","0.0679889413649005","212301","<p>I have a huge doubt, which I believe is Basic. I have no difficulty in interpreting the results of our logistic regression model using the ODD ratio, but I do not know what to do when I work with Mixed effects model for longitudinal data.</p>

<p>Below they use the <code>glmer</code> function to estimate a mixed effects logistic regression model with Il6, CRP, and LengthofStay as patient level continuous predictors, CancerStage as a patient level categorical predictor (I, II, III, or IV), Experience as a doctor level continuous predictor, and a random intercept by DID, doctor ID.</p>

<p>The <code>glmer</code> function created 407 groups that refer to the number of doctors.</p>

<p>What would it mean for example the -0.0568 of IL6 and the -2.3370 of CancerStageIV's in the study presented?</p>

#################

<p>m &lt;â€ glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer =  ""bobyqa""),      nAGQ = 10) 
print(m, corr = FALSE) </p>

<h1>Generalized linear mixed model fit by maximum likelihood</h1>

<h2>Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]</h2>

<h2>Family:</h2>

<p>binomial ( logit )  </p>

<h2>Formula:</h2>

<p>remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +<br>
   (1 | DID)  </p>

<p>Data: hdp  </p>

<pre><code>  AIC        BIC    logLik     deviance  df.resid   
 7397        7461    -3690        7379     8516 
</code></pre>

<h2>Random effects:</h2>

<p>Groups Name         Std.Dev.<br>
     DID    (Intercept) 2.01 </p>

<p>Number of obs: 8525, groups: DID, 407  </p>

<h1>Fixed Effects:</h1>

<pre><code>  Intercept    IL6        CRP       CancerStageII  
 â€2.0527     â€0.0568    â€0.0215       â€0.4139 

CancerStageIII   CancerStageIV       LengthofStay      Experience  
 â€1.0035           â€2.3370              â€0.1212          0.1201 
</code></pre>
"
"0.09392108820677","0.0920574617898323","212903","<p>I have the data <a href=""https://docs.google.com/spreadsheets/d/1lEzUt0QdFCp1ho-iWd4HzEIZoo8IyAM8YP2gu-K7BQo/edit?usp=sharing"" rel=""nofollow"">here</a>.But When I tried to build the logistic regression model using glm function its shows NA in TotalVisits. I have found similar question on stack overflow but that is answered for linear model.  </p>

<pre><code> str(quality)
'data.frame':   131 obs. of  14 variables:
 $ MemberID            : int  1 2 3 4 5 6 7 8 9 10 ...
 $ InpatientDays       : int  0 1 0 0 8 2 16 2 2 4 ...
 $ ERVisits            : int  0 1 0 1 2 0 1 0 1 2 ...
 $ OfficeVisits        : int  18 6 5 19 19 9 8 8 4 0 ...
 $ Narcotics           : int  1 1 3 0 3 2 1 0 3 2 ...
 $ DaysSinceLastERVisit: num  731 411 731 158 449 ...
 $ Pain                : int  10 0 10 34 10 6 4 5 5 2 ...
 $ TotalVisits         : int  18 8 5 20 29 11 25 10 7 6 ...
 $ ProviderCount       : int  21 27 16 14 24 40 19 11 28 21 ...
 $ MedicalClaims       : int  93 19 27 59 51 53 40 28 20 17 ...
 $ ClaimLines          : int  222 115 148 242 204 156 261 87 98 66 ...
 $ StartedOnCombination: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ AcuteDrugGapSmall   : int  0 1 5 0 0 4 0 0 0 0 ...
 $ PoorCare            : int  0 0 0 0 0 1 0 0 1 0 ...



table(is.na(quality))
FALSE 
1834
</code></pre>

<p>My data does not contain any NA values.</p>

<pre><code>set.seed(100)
split &lt;- sample.split(quality$PoorCare, SplitRatio = .5)
train &lt;-subset(quality, split ==TRUE)
test &lt;- subset(quality, split ==FALSE)
</code></pre>

<p>Building the model using all variable </p>

<pre><code>log.Quality &lt;- glm(PoorCare ~ ., data = train, family = 'binomial')

summary(log.Quality)      
Call:
glm(formula = PoorCare ~ ., family = ""binomial"", data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5679  -0.6384  -0.3604  -0.1154   2.1298  

Coefficients: (1 not defined because of singularities)
                          Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)              -3.583178   1.807020  -1.983   0.0474 *
MemberID                 -0.008742   0.010988  -0.796   0.4263  
InpatientDays            -0.106578   0.095632  -1.114   0.2651  
ERVisits                  0.275225   0.310364   0.887   0.3752  
OfficeVisits              0.126433   0.066140   1.912   0.0559 .
Narcotics                 0.190862   0.106890   1.786   0.0742 .
DaysSinceLastERVisit     -0.001221   0.002026  -0.603   0.5467  
Pain                     -0.020104   0.023057  -0.872   0.3832  
TotalVisits                     NA         NA      NA       NA  
ProviderCount             0.046297   0.040637   1.139   0.2546  
MedicalClaims             0.025123   0.030564   0.822   0.4111  
ClaimLines               -0.010384   0.012746  -0.815   0.4152  
StartedOnCombinationTRUE  2.205058   1.724923   1.278   0.2011  
AcuteDrugGapSmall         0.217813   0.139890   1.557   0.1195  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 72.549  on 64  degrees of freedom
Residual deviance: 49.213  on 52  degrees of freedom
AIC: 75.213

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Can anyone provide me a good explanation why this is happening ? </p>
"
"0.0490486886395286","0.0480754414848157","213011","<p>In the <code>car</code> package, we have the function <code>powerTransform</code> which transforms variables in a regression equation to make the residuals in the transformed equation as normal as possible. I am confused about what this transformation is and further in the following example:</p>

<pre><code># Box Cox Method, univariate
summary(p1 &lt;- powerTransform(cycles ~ len + amp + load, Wool))

# fit linear model with transformed response:
coef(p1, round=TRUE)
summary(m1 &lt;- lm(bcPower(cycles, p1$roundlam) ~ len + amp + load, Wool))
</code></pre>

<p>What I am confused about is what exactly the model <code>p1</code> is. Is it simply the linear model without a transformation, then it finds the optimal parameter, we then use that to specify <code>m1</code>? So what is the regression equation for <code>p1</code>, <code>m1</code>??</p>
"
"NaN","NaN","213067","<p>I'm working in R. I have a data set of 21 fish, roughly half in each of 2 treatments. I measured their behaviour over 10 minutes and want to analyse this to look for changes over time (gradient) and difference between the two groups. I know i cant just do a normal regression because I have repeated measures and I think I need to perform a linear mixed model, but when I've tried this in R it comes up with errors. <a href=""http://i.stack.imgur.com/9XO84.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9XO84.png"" alt=""enter image description here""></a></p>

<p>in these graphs, I'm looking at B and C and want to know if/how they are different. What test can I use and what is the correct R script for this?</p>

<p>This is the script I would use to compare the gradients, intercepts and interactions in a normal linear regression but how do i control for the fact that these are repeated measures?</p>

<pre><code>model4 &lt;- lm(tankdiving$distance~ tankdiving$Genotype*tankdiving$time)
summary(model4)
</code></pre>
"
"0.0800961731463273","0.0686935087981502","213187","<p>I'm very new to R and stats in general but I think I need to do a repeated measures analysis perhaps using a linear mixed model on my data.</p>

<p>Data are behavioural measurements e.g. distance swum for 21 fish of 2 different genotypes. Each fish was tested for 10 minutes, and data taken for each minute. I've made graphs to show this (B and C).</p>

<p><img src=""http://i.stack.imgur.com/8oKg9.png"" alt=""""></p>

<p>My issue is that I can't do just a normal regression as each of the points is linked over time by the same fish. I've been taught to use linear mixed models such as:</p>

<pre><code>model &lt;- lmer(swimdurbot~1+start+(1+start|file), data=tdautodisc)
model2 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
model3 &lt;- lmer(swimdurbot~Genotype+(1+start|file), data=tdautodisc)
model4 &lt;- lmer(swimdurbot~Genotype+start+ Genotype*start +(1+start|file), data=tdautodisc)
model5 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
</code></pre>

<p>and to then perform anovas on them but I honestly can't work out what each model is showing, how they are different from each other and which part of the analysis that R comes up with tells me what I want to know.</p>

<p>I want to find out from my data: Does fish behaviour change over 10 mintutes (gradient)? Is the behaviour different between genotypes? and Does genotype affect the change in behaviour over time?</p>

<p>I would really appreciate some help on this as I can't get my head around it, and the graphs don't show any obvious trend so aren't useful for me to guess which part of the analysis relates to which aspect of the graphs.</p>
"
"0.0693653206906364","0.0679889413649005","213253","<p>In general, my question is how to estimate some prediction intervals in the case of penalized linear models (in particular, I think about the glmnet R package). I understood that the introduction of a penalization in the objective function generates a shrinkage effect, which is a bias on the estimated coefficients. 
I understand that in this case the calculation of the uncertainties is troublesome</p>

<p><a href=""https://air.unimi.it/retrieve/handle/2434/153099/133417/phd_unimi_R07738.pdf"" rel=""nofollow"">https://air.unimi.it/retrieve/handle/2434/153099/133417/phd_unimi_R07738.pdf</a></p>

<p>(see sections 3.2 and 3.3 the quoted papers)</p>

<p>Two bootstrap methods (random x vs fixed x) are discussed in the context of standard linear models here</p>

<p><a href=""http://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients?rq=1"">Two ways of using bootstrap to estimate the confidence interval of coefficients in regression</a></p>

<p>but again the focus is on the beta coefficients.
However, I am not interested in the estimate of the confidence intervals on the beta coefficients, but only on the predicted values.
For instance, consider the following R code</p>

<pre><code>library(glmnet)


# Generate data
set.seed(19875)  # Set seed for reproducibility
n &lt;- 1000  # Number of observations
p &lt;- 5000  # Number of predictors included in model
real_p &lt;- 15  # Number of true predictors
x &lt;- matrix(rnorm(n*p), nrow=n, ncol=p)
y &lt;- apply(x[,1:real_p], 1, sum) + rnorm(n)

# Split data into train (2/3) and test (1/3) sets
train_rows &lt;- sample(1:n, .66*n)
x.train &lt;- x[train_rows, ]
x.test &lt;- x[-train_rows, ]

y.train &lt;- y[train_rows]
y.test &lt;- y[-train_rows]



fit.elnet &lt;- glmnet(x.train, y.train, family=""gaussian"", alpha=.5)

yhat &lt;- predict(fit.elnet, s=fit.elnet$lambda, newx=x.test)
</code></pre>

<p>Does anybody know how to calculate a meaningful confidence interval for yhat?</p>

<p>Thanks!</p>
"
"0.0642198081225601","0.0734364498908627","213446","<p>In a sample regression like this, $r=b_1f_1+b_2f_2$, where $f_1$ and $f_2$ are financial risk factors, I want to see if one of the factors say $f_1$ drives out the other $f_2$, described in John Cochrane's Asset Pricing as a 'Horce Race Regression'. Specifically, in the presence of $f_1$, is $b_2=0.$ John Cochrane describes a Wald Test to do this. 
For references, the description from his book Asset Pricing,</p>

<blockquote>
  <p>We want to know, given factors $f_1$, do we need $f_2$ to price assets-ie, is $b_2=0$. 
  First and foremost obviously, we have an asymptotic covariance matrix for [$b_1,b_2$],so we can form a t-test(if $b_2$ is scalar) or $\chi^2$ for $b_2=0$ by forming the statistic, 
  $\hat{b_2^{'}}Var(\hat{b_2^{'}}) \backsim \chi_{\#b_{2}}^{2}$ where $\#b_2$ is the number of elements in the $b_2$ vector. This is a Wald Test. [Asset Pricing 13.3, page 259]   </p>
</blockquote>

<ol>
<li>Is there an R package or any other statistical package which performs such a test automatically? </li>
<li>At a cursory reading, it seems to me that a simple linear regression where I add factors one by one should suffice to conclude if the factor in question is relevant or now. What's special about 'Horse Races' as described by Cochrane?   </li>
</ol>

<p>Edit: Manela and Moreira JFE 2015 (<em>forthcoming</em>) has a Table that presents the results of a 'Horse Race'. 
<a href=""http://i.stack.imgur.com/3mB7U.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3mB7U.jpg"" alt=""enter image description here""></a></p>
"
"0.0693653206906364","0.0566574511374171","213804","<p>I am running some linear regressions in R. I am dealing with a linear dependent and linear as well as categorical independent variables using <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html"" rel=""nofollow"">lm</a>. So far, I have looked at the output that <code>summary(model)</code> gives me. </p>

<p>Other studies instead run <a href=""http://www.inside-r.org/packages/cran/car/docs/Anova"" rel=""nofollow"">Anova()</a> from the <a href=""https://cran.r-project.org/web/packages/car/index.html"" rel=""nofollow"">car</a> package on their linear model, which returns a similar table. The docs for <code>Anova()</code> state that it</p>

<blockquote>
  <p>Calculates type-II or type-III analysis-of-variance tables for model objects. </p>
</blockquote>

<p>I am under the impression that this <code>Anova()</code> returns an F instead of the t-statistic but is ~ equivalent in what its tell me. (sample output below). So I was wondering</p>

<ul>
<li><p>Are standard R <code>summary(lm)</code> and car <code>Anova(lm)</code> indeed doing pretty much the same calculations here? If not, what is the difference?</p></li>
<li><p>They both report the same p-value, however the F-statistic at the bottom of the standard output is different from the <code>Anova()</code> one. Why is that?</p></li>
<li><p>What are applications where one would choose one over the other?</p></li>
</ul>

<p>Any help is much appreciated!</p>

<p>Sample output:</p>

<p>Standard R</p>

<pre><code>summary(linreg)
...
         Estimate    t value    Pr(&gt;|t|)
Age      -18.016     -3.917     0.000107
Gender   -45.4912    -4.916     1.35e-06
---
Residual standard error: 85.81 on 359 degrees of freedom
F-statistic: 16.71 on 2 and 359 DF, p-value: 1.147e-07
</code></pre>

<p>Anova() output</p>

<pre><code>Anova(linreg)

Anova Table (Type II tests)

           Sum Sq    F value   Pr (&gt;F)
Age        112997    15.345    0.0001072
Gender     1777936   24.164    1.348e-06
</code></pre>
"
"NaN","NaN","213857","<p>I am in the process of learning about Bayesian statistics with the help of R, and I would like to know the kind of analysis questions I should pose. 
Say for instance, with this <a href=""https://docs.google.com/spreadsheets/d/1Bjfr3pgiqEHWgIGiu6OKVKP8X4d0yDgKkpUKF2mx8l0/edit?usp=sharing"" rel=""nofollow"">dataset</a> of food production in various countries. So far, I've just created simple linear regression models. </p>
"
"0.112190596267551","0.109964457676866","213982","<p>I am trying to use ""propodds""  in the VGAM function in R, but am not sure if I am doing it right and don't really understand how to analyze the output I got so far to check to see if I am using it right. Any help on how to correctly use ""propodds"" or analyze the output would be appreciated. This is what I have so far:</p>

<pre><code>    &gt; fittest &lt;-vglm(rp ~ is.native + is.male + age2 + is.debt + oh + ms + cjs, propodds, data = dummydata2)
&gt; fittest
Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

    Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5     is.native 
  2.827674173  -0.463602645  -0.474290665  -0.614877500  -2.514394420  -0.063546621 
      is.male          age2       is.debt            oh            ms           cjs 
  0.114052675   0.067835161  -0.058563607  -0.089420626   0.109135966   0.003937505 

Degrees of Freedom: 52000 Total; 51988 Residual
Residual deviance: 24702.04 
Log-likelihood: -12351.02 
&gt; summary(fittest)

Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

Pearson residuals:
                    Min      1Q  Median      3Q     Max
logit(P[Y&gt;=2])  -5.1080  0.1300  0.2803  0.3016  0.3335
logit(P[Y&gt;=3])  -0.6976 -0.5876 -0.5490  0.5937 14.7717
logit(P[Y&gt;=4]) -13.1157 -0.5173 -0.4831  0.6080  3.0626
logit(P[Y&gt;=5])  -4.0174 -0.4072 -0.3746  1.0167  1.2176
logit(P[Y&gt;=6])  -0.6164 -0.5749 -0.1610 -0.1541  3.6060

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept):1  2.827674   0.079827  35.423  &lt; 2e-16 ***
(Intercept):2 -0.463603   0.068894  -6.729 1.71e-11 ***
(Intercept):3 -0.474291   0.068901  -6.884 5.83e-12 ***
(Intercept):4 -0.614878   0.069009  -8.910  &lt; 2e-16 ***
(Intercept):5 -2.514394   0.074892 -33.573  &lt; 2e-16 ***
is.native     -0.063547   0.062409  -1.018  0.30857    
is.male        0.114053   0.039694   2.873  0.00406 ** 
age2           0.067835   0.024789   2.737  0.00621 ** 
is.debt       -0.058564   0.052983  -1.105  0.26902    
oh            -0.089421   0.057526  -1.554  0.12008    
ms             0.109136   0.041587   2.624  0.00868 ** 
cjs            0.003938   0.043653   0.090  0.92813    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of linear predictors:  5 

Names of linear predictors: 
logit(P[Y&gt;=2]), logit(P[Y&gt;=3]), logit(P[Y&gt;=4]), logit(P[Y&gt;=5]), logit(P[Y&gt;=6])

Dispersion Parameter for cumulative family:   1

Residual deviance: 24702.04 on 51988 degrees of freedom

Log-likelihood: -12351.02 on 51988 degrees of freedom

Number of iterations: 4 

Exponentiated coefficients:
is.native   is.male      age2   is.debt        oh        ms       cjs 
0.9384304 1.1208112 1.0701889 0.9431182 0.9144608 1.1153140 1.0039453 
</code></pre>

<p>A little background on my data that may help: I am trying to determine if risk preferences (variable ""rp"" in the code) is determined by immigration status (variable ""is.native"" in the code, which is a dummy variable where 0 = native and 1 = immigrant). I have a few factors that I want to control for since they may affect risk preferences [age2, is.debt, oh(owns home), ms (marital status), and cjs (current job status)]. Based on similar research the best way to analyze this is the cumulative logistic regression and they seemed to look at the proportional odds. The data came from the 2014 Health and Retirement Study which is representative of the US population over age 50. There are about 20,000 participants. </p>

<p>I'm not sure if my model is formatted correctly. ""rp"" has 6 categories - a control group, low risk tolerance (rt), some rt, high rt, substantial rt and ""ignore"" which is answers of ""don't know"" or ""NA"".  All other variables are dummy variables with only options for ""0"" or ""1"" besides ""age2"" which has 6 categories (under 50, 50-60, 60-70, 70-80, 80-90, 90+). Are these dummy variables appropriate to use or should I just use the actual answers provided by the participants?</p>

<p>I know the significant codes in the ""summary"" section tell me gender, age, and marital status are significant at the 1% significance level, but I don't understand any of the other results. Such as, what does it mean that all the intercepts are significant? Is the model as a whole significant? What is the dispersion parameter? What are the exponentiated coefficients? </p>
"
"0.0961546625894718","0.094246716538858","214200","<p>I was wondering if I could have some help analyzing the output from a regression. Some background on what I'm trying to find/my data:</p>

<p>I am trying to determine if immigration status is a determinant of risk preferences. My data comes from the 2014 Health and Retirement Survey with about ~20,000 participants. The risk measure (""rp"" in the equation) is my ""y"" variable. ""rp"" is a categorical variable, with levels of no risk, low risk, some risk, etc. My ""x"" variable is immigration status (""is.native"" in the equation) and is a dummy variable that takes the value of 0 if the person is native to the U.S. and 1 if the person is an immigrant to the U.S.  The other variables in the equation are factors that may effect risk preferences so I want to control for them. My regression is:</p>

<pre><code> fit1_usesrp &lt;-vglm(rp ~ is.native + is.male + oh + cjs +  + age2 + tw,propodds, data = dummydata2)
</code></pre>

<p>and the output is: </p>

<pre><code>fit1_usesrp
Call:
vglm(formula = rp ~ is.native + is.male + oh + cjs + +age2 + 
    tw, family = propodds, data = dummydata2)

Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5     is.native       is.male 
 2.796024e+00 -4.948484e-01 -5.055242e-01 -6.460196e-01 -2.545089e+00 -7.381110e-02  1.509080e-01 
           oh           cjs          age2            tw 
-2.070633e-02  3.643551e-03  7.149891e-02 -3.092727e-06 

Degrees of Freedom: 52000 Total; 51989 Residual
Residual deviance: 24705.39 
Log-likelihood: -12352.69 

summary(fit1_usesrp)

Call:
vglm(formula = rp ~ is.native + is.male + oh + cjs + +age2 + 
    tw, family = propodds, data = dummydata2)

Pearson residuals:
                    Min      1Q  Median      3Q     Max
logit(P[Y&gt;=2])  -5.0292  0.1305  0.2830  0.2989  0.5201
logit(P[Y&gt;=3])  -0.6676 -0.5986 -0.5517  0.5911 14.8594
logit(P[Y&gt;=4]) -13.1696 -0.5270 -0.4856  0.6051  3.0437
logit(P[Y&gt;=5])  -4.0121 -0.4082 -0.3802  1.0345  1.2412
logit(P[Y&gt;=6])  -0.6133 -0.5764 -0.1601 -0.1548  3.5492

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept):1  2.796e+00  6.689e-02  41.803  &lt; 2e-16 ***
(Intercept):2 -4.948e-01  5.352e-02  -9.246  &lt; 2e-16 ***
(Intercept):3 -5.055e-01  5.353e-02  -9.444  &lt; 2e-16 ***
(Intercept):4 -6.460e-01  5.368e-02 -12.035  &lt; 2e-16 ***
(Intercept):5 -2.545e+00  6.113e-02 -41.635  &lt; 2e-16 ***
is.native     -7.381e-02  6.224e-02  -1.186   0.2357    
is.male        1.509e-01  3.764e-02   4.010 6.08e-05 ***
oh            -2.071e-02  4.747e-02  -0.436   0.6627    
cjs            3.644e-03  4.365e-02   0.083   0.9335    
age2           7.150e-02  2.449e-02   2.920   0.0035 ** 
tw            -3.093e-06  1.479e-06  -2.092   0.0365 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of linear predictors:  5 

Names of linear predictors: 
logit(P[Y&gt;=2]), logit(P[Y&gt;=3]), logit(P[Y&gt;=4]), logit(P[Y&gt;=5]), logit(P[Y&gt;=6])

Dispersion Parameter for cumulative family:   1

Residual deviance: 24705.39 on 51989 degrees of freedom

Log-likelihood: -12352.69 on 51989 degrees of freedom

Number of iterations: 4 

Exponentiated coefficients:
is.native   is.male        oh       cjs      age2        tw 
0.9288471 1.1628897 0.9795066 1.0036502 1.0741170 0.9999969 
</code></pre>

<p>All I understand from the output is that the p-values tell me gender, age, and tw (total wealth) are significant. I don't know how to read anything else in the output. Any help would be greatly appreciated. Thank you. </p>
"
"NaN","NaN","214308","<p>I am studying time-series econometrics and in particular Dynamic Linear Models for multivariate time-series. </p>

<p>Someone can help me in understanding which is the difference between SUTSE (Seemingly Unrelated Time Series Equations) and SUR (Seemingly Unrelated Regression) Dynamic Linear Models?</p>

<p>How can I specify a SUTSE and a SUR model on <code>r</code>? I think <code>dlmModPoly</code> may be of help but I am a bit stuck</p>
"
"0.0867708542418546","0.0915913450642003","214404","<p>Hi I'd like some help plotting the following regression in ""r"": </p>

<pre><code>fit1_usesrp &lt;-vglm(rp ~ is.native + is.male + oh + cjs + age2 + tw ,propodds, data = dummydata
</code></pre>

<p>Through this regression I am interested in finding if immigration status is a determinant of risk preferences. ""rp"" in the regression is a categorical variable that measures respondent's willingness to take risk (no risk tolerance, low risk tolerance, etc.). ""is.native"" is a dummy variable for immigration status (0 = native, 1 = immigrant). oh, cjs, and age2 are all dummy variables for factors that may affect risk preferences. ""tw"" is for total wealth and I used the actual raw values for wealth instead of trying to turn this into a dummy variable. </p>

<p>the output for this regression is:</p>

<pre><code>Call:
vglm(formula = rp ~ is.native + is.male + oh + cjs + age2 + tw, 
    family = propodds, data = dummydata2)

Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5     is.native       is.male 
 2.796024e+00 -4.948484e-01 -5.055242e-01 -6.460196e-01 -2.545089e+00 -7.381110e-02  1.509080e-01 
           oh           cjs          age2            tw 
-2.070633e-02  3.643551e-03  7.149891e-02 -3.092727e-06 

Degrees of Freedom: 52000 Total; 51989 Residual
Residual deviance: 24705.39 
Log-likelihood: -12352.69 
&gt; summary(fit1_usesrp)

Call:
vglm(formula = rp ~ is.native + is.male + oh + cjs + age2 + tw, 
    family = propodds, data = dummydata2)

Pearson residuals:
                    Min      1Q  Median      3Q     Max
logit(P[Y&gt;=2])  -5.0292  0.1305  0.2830  0.2989  0.5201
logit(P[Y&gt;=3])  -0.6676 -0.5986 -0.5517  0.5911 14.8594
logit(P[Y&gt;=4]) -13.1696 -0.5270 -0.4856  0.6051  3.0437
logit(P[Y&gt;=5])  -4.0121 -0.4082 -0.3802  1.0345  1.2412
logit(P[Y&gt;=6])  -0.6133 -0.5764 -0.1601 -0.1548  3.5492

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept):1  2.796e+00  6.689e-02  41.803  &lt; 2e-16 ***
(Intercept):2 -4.948e-01  5.352e-02  -9.246  &lt; 2e-16 ***
(Intercept):3 -5.055e-01  5.353e-02  -9.444  &lt; 2e-16 ***
(Intercept):4 -6.460e-01  5.368e-02 -12.035  &lt; 2e-16 ***
(Intercept):5 -2.545e+00  6.113e-02 -41.635  &lt; 2e-16 ***
is.native     -7.381e-02  6.224e-02  -1.186   0.2357    
is.male        1.509e-01  3.764e-02   4.010 6.08e-05 ***
oh            -2.071e-02  4.747e-02  -0.436   0.6627    
cjs            3.644e-03  4.365e-02   0.083   0.9335    
age2           7.150e-02  2.449e-02   2.920   0.0035 ** 
tw            -3.093e-06  1.479e-06  -2.092   0.0365 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of linear predictors:  5 

Names of linear predictors: 
logit(P[Y&gt;=2]), logit(P[Y&gt;=3]), logit(P[Y&gt;=4]), logit(P[Y&gt;=5]), logit(P[Y&gt;=6])

Dispersion Parameter for cumulative family:   1

Residual deviance: 24705.39 on 51989 degrees of freedom

Log-likelihood: -12352.69 on 51989 degrees of freedom

Number of iterations: 4 

Exponentiated coefficients:
is.native   is.male        oh       cjs      age2        tw 
0.9288471 1.1628897 0.9795066 1.0036502 1.0741170 0.9999969 
</code></pre>
"
"0.02831827358943","0.0277563690826684","214608","<p>I have an interpretation problem. As you can see below there's a linear regression output for the CAPM. I don't know how to interpret the significance level. ExIndex has a very low p-value, but the Significance level is 0. So can I reject H0 or not? The same question for the intercept.</p>

<blockquote>
<pre><code>Coefficients:
              Estimate     Std. Error  t value  Pr(&gt;|t|)    
(Intercept) - 0.003258     0.001560    -2.089       0.0377 *  
 ExIndex      0.898980     0.106511     8.440     2.3e-15 ***
 Signif. codes:   0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.02508 on 258 degrees of freedom Multiple
R-squared:  0.2164,    Adjusted R-squared:  0.2133 
F-statistic: 71.24 on 1 and 258 DF,  p-value: 2.304e-15
</code></pre>
</blockquote>
"
"0.0490486886395286","0.0480754414848157","214613","<p>I am trying to make a simple linear regression to see if my variable ""totalssq"" has an influence on my variable ""hadsa"". (my data is ""dstatss"") Both are quantitative.
I made a model with lm() and tested it with an ANOVA.
Here are the outputs :</p>

<pre><code>    Analysis of Variance Table

    Response: dstatss$hadsa
             Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
      dstatss$totalssq  1  88.272  88.272  5.6848 0.03623 *
     Residuals        11 170.805  15.528                  
     ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The p-value is significant, but i don't know what it should mean to me ?
Does it means that there is a significant relationship between my variables ? I don't really know how to interpret this.</p>
"
"0.0326991257596857","0.0480754414848157","214665","<p>I have 15-minute streamflow observations for a small stream, but the dataset has some gaps in it. I want to fill the gaps with a regression using observations from a nearby stream (and quantify the uncertainty caused by these gaps). </p>

<p>If I use a linear relationship between the two streams, I get negative predictions for my target stream when the other has low flow, because my target stream dries out before the predictor stream. If I force the regression through zero (it has just a slope), then the model doesnâ€™t fit very well, which influences my results for the uncertainty analysis.</p>

<p>Is a truncated or a linear regression a good solution?  If so, how do I fit the two parameters?</p>

<p>Is there a better model?  It should be zero when my predictor stream has low flow, and then increase roughly in proportion to flow at the predictor stream. I'm working in R if anyone has suggestions for functions.</p>
"
"0.02831827358943","0.0277563690826684","215207","<p>I have a question regarding Dynamic regression linear models.
I wonder if it is possible to implement a MLR model (in R) using 'lm' and creating lagged values of predictors and dependent variables.
For example, considering the linear regression described at <a href=""https://www.otexts.org/fpp/5/1"" rel=""nofollow"">https://www.otexts.org/fpp/5/1</a> 
 (""Forecasting: principles and practice""), can I retrive lagged values for savings, income, etc. and the same output ""score"" variable and build a model like below?</p>

<p>require(dplyr)
log.savings_lag1&lt;-lag(log.savings, 1)
........
score_lag1&lt;-lag(score,1)</p>

<p>fit &lt;- lm(score ~ log.savings + log.income +
log.address + log.employed+ log_savings_lag1,score_lag1, data=creditlog)</p>

<p>If this feasible? There is any particular concern I have to be aware?
Thanks in advance!</p>
"
"0.0991139575630048","0.111025476330674","215224","<p>I am going to explain my question using a reproducibile toy example. I would like to regress a numerical variable using a multiple regression model with either numerical and categorical variables. I would like to do that without using the functions provided by R, but I am worried that I am not coding the categorical variables properly. These are the toy data:</p>

<pre><code>  mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")

  mydata$admit &lt;- factor(mydata$admit)
  mydata$gre &lt;- scale(mydata$gre)
  mydata$gpa &lt;- scale(mydata$gpa)
  mydata$rank &lt;- factor(mydata$rank)

  head(mydata)

          admit        gre        gpa rank
        1     0 -1.7980110  0.5783479    3
        2     1  0.6258844  0.7360075    3
        3     1  1.8378321  1.6031352    1
        4     1  0.4527490 -0.5252692    4
        5     0 -0.5860633 -1.2084607    4
        6     1  1.4915613 -1.0245245    2

  model &lt;- lm(gpa ~. , data=mydata)
  #linear multiple regression
  summary(model)
</code></pre>

<p>This is the result using the lm function:</p>

<pre><code>  Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
  (Intercept) -0.04585    0.12924  -0.355   0.7230    
  admit1       0.24980    0.10273   2.432   0.0155 *  
  gre          0.36816    0.04672   7.879 3.24e-14 ***
  rank2       -0.14424    0.13993  -1.031   0.3033    
  rank3        0.14189    0.14723   0.964   0.3358    
  rank4       -0.13094    0.16620  -0.788   0.4313    
</code></pre>

<p>Manually, I am coding the model matrix X in this way:</p>

<pre><code>  mydata$rank2 &lt;- sapply(mydata$rank, function (x){ if(x==2) return(1) else return(0)})
  mydata$rank3 &lt;- sapply(mydata$rank, function (x){ if(x==3) return(1) else return(0)})
  mydata$rank4 &lt;- sapply(mydata$rank, function (x){ if(x==4) return(1) else return(0)})

  X &lt;- data.matrix(mydata[,-c(3,4)])
  Y &lt;- data.matrix(mydata[,3])
  X[,1] &lt;- X[,1] - 1
</code></pre>

<p>So I am creating for each level a binary variable and I am not considering the first level as I saw in the literature. This is the final matrix</p>

<pre><code>  head(X)
       admit        gre rank2 rank3 rank4
  [1,]     0 -1.7980110     0     1     0
  [2,]     1  0.6258844     0     1     0
  [3,]     1  1.8378321     0     0     0
  [4,]     1  0.4527490     0     0     1
  [5,]     0 -0.5860633     0     0     1
  [6,]     1  1.4915613     1     0     0
</code></pre>

<p>but when I compute the regression coefficients in this way:</p>

<pre><code>  Xbeta &lt;- solve(t(X) %*% X) %*% t(X) %*% Y
</code></pre>

<p>I am obtaining different values compared with the ones obtained with lm.
In particular, these are:</p>

<pre><code>               [,1]
  admit  0.23456544
  gre    0.36804870
  rank2 -0.18463006
  rank3  0.09954882
  rank4 -0.17408055
</code></pre>

<p>What I am doing wrong, please? I would like also to compute the residuals, the sd of the coefficients and the t-statistic, but again I am not obtaining the same results of the lm function for them, and I believe it is due to the fact that I am coding in a wrong way the categorical variables.</p>
"
"0.0400480865731637","0.039253433598943","215560","<p>I have three data sets that, when joined, have O(320) independent variables for a classification problem.  </p>

<p>Principal component analysis (PCA) seems out of the question because the data is mostly factors, not continuous.</p>

<p>I'm at a loss as to how to proceed.  </p>

<p>How do experienced analysts go about winnowing a large data set with hundreds of columns to something manageable?  How do you decide between variables?  What calculations can you go on to supplement your gut and experience?  How do you avoid throwing away significant variables?</p>

<p>A large number of columns might not be a problem for R, given enough CPU and RAM, but coming up with a cogent story should include identifying what is truly significant.  How to accomplish that?</p>

<p>Should I just toss all of it into a logistic regression and see what happens, without any forethought?</p>

<p>More detail in response to comments:</p>

<ol>
<li>Classification. </li>
<li>Many more observations than columns. </li>
<li>Yes, big oh notation meaning approximately. </li>
<li>Linear model at first. Also interested in boosted models in addition to logistic regression. </li>
</ol>
"
"0.0490486886395286","0.0480754414848157","215657","<p>Please can someone provide an accessible interpretation of the parameter estimates from a discrete weibull regression model, e.g in R: </p>

<pre><code>library(DWreg)
library(COMPoissonReg)

data(freight)

dw.reg(broken ~ transfers, data = freight,
       para.beta=FALSE,para.q1=FALSE,para.q2=TRUE)
</code></pre>

<p>produces:</p>

<pre><code>Maximum Likelihood estimation
Newton-Raphson maximisation, 11 iterations
Return code 1: gradient close to zero
Log-Likelihood: -18.82916 
3  free parameters
Estimates:
            Estimate Std. error t value  Pr(&gt; t)    
(Intercept) -26.7814     7.3943  -3.622 0.000292 ***
transfers    -2.5857     0.7579  -3.412 0.000646 ***
beta         10.8723     2.9356   3.704 0.000213 ***
---
Signif. codes:  
0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 
</code></pre>

<p>I am trying to understand how one would communicate the results to a layman in an analogous way that one might interpret the parameter estimates from linear (unit increase in x is assoc with beta increase in y) or poisson (mulitplicative) etc.</p>
"
"0.09392108820677","0.0753197414644083","215901","<p><strong>USE CASE</strong></p>

<p>Use R to fit/train a binary classification model, then interpret the model for the purpose of manual calculating classifications in Excel, not R.</p>

<p><strong>MODEL COEFFICIENTS</strong></p>

<pre><code>&gt;coef(model1)
#(Intercept) PetalLength  PetalWidth 
#-31.938998   -7.501714   63.670583 

&gt;exp(coef(model1))
#(Intercept)  PetalLength   PetalWidth 
#1.346075e-14 5.521371e-04 4.485211e+27 
</code></pre>

<p><strong>QUESTIONS</strong></p>

<p>(1) what is the classification formula from the fit model in example code below named '<em>model1</em>'?. (is it formula A, B or Neither)? </p>

<p>(2) how does '<em>model1</em>' determine if class == 1 vs. 2?</p>

<ul>
<li>Formula A: 

<blockquote>
  <p>class(Species{1:2}) = (-31.938998) + (-7.501714 * [PetalLength]) + (63.670583 * [PetalWidth])</p>
</blockquote></li>
<li>Formula B: 

<blockquote>
  <p>class(Species{1:2}) = 1.346075e-14 + (5.521371e-04 * [PetalLength]) + (4.485211e+27 * [PetalWidth])</p>
</blockquote></li>
</ul>

<p><strong>R CODE EXAMPLE</strong></p>

<pre><code># Load data (using iris dataset from Google Drive because uci.edu link wasn't working for me today)
#iris &lt;- read.csv(url(""http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data""), header = FALSE)
iris &lt;- read.csv(url(""https://docs.google.com/spreadsheets/d/1ovz31Y6PrV5OwpqFI_wvNHlMTf9IiPfVy1c3fiQJMcg/pub?gid=811038462&amp;single=true&amp;output=csv""), header = FALSE)
dataSet &lt;- iris

#assign column names
names(dataSet) &lt;- c(""SepalLength"", ""SepalWidth"", ""PetalLength"", ""PetalWidth"", ""Species"")

#col names
dsColNames &lt;- as.character(names(dataSet))

#num of columns and rows
dsColCount &lt;- as.integer(ncol(dataSet))
dsRowCount &lt;- as.integer(nrow(dataSet))

#class ordinality and name
classColumn &lt;- 5 
classColumnName &lt;- dsColNames[classColumn]
y_col_pos &lt;- classColumn

#features ordinality
x_col_start_pos &lt;- 1
x_col_end_pos &lt;- 4

# % of [dataset] reserved for training/test and validation  
set.seed(10)
sampleAmt &lt;- 0.25
mainSplit &lt;- sample(2, dsRowCount, replace=TRUE, prob=c(sampleAmt, 1-sampleAmt))

#split [dataSet] into two sets
dsTrainingTest &lt;- dataSet[mainSplit==1, 1:5] 
dsValidation &lt;- dataSet[mainSplit==2, 1:5]
nrow(dsTrainingTest);nrow(dsValidation);

# % of [dsTrainingTest] reserved for training
sampleAmt &lt;- 0.5
secondarySplit &lt;- sample(2, nrow(dsTrainingTest), replace=TRUE, prob=c(sampleAmt, 1-sampleAmt))

#split [dsTrainingTest] into two sets 
dsTraining &lt;- dsTrainingTest[secondarySplit==1, 1:5]
dsTest &lt;- dsTrainingTest[secondarySplit==2, 1:5]
nrow(dsTraining);nrow(dsTest);

nrow(dataSet) == nrow(dsTrainingTest)+nrow(dsValidation)
nrow(dsTrainingTest) == nrow(dsTraining)+nrow(dsTest)

library(randomGLM)

dataSetEnum &lt;- dsTraining[,1:5]
dataSetEnum[,5] &lt;- as.character(dataSetEnum[,5])
dataSetEnum[,5][dataSetEnum[,5]==""Iris-setosa""] &lt;- 1 
dataSetEnum[,5][dataSetEnum[,5]==""Iris-versicolor""] &lt;- 2 
dataSetEnum[,5][dataSetEnum[,5]==""Iris-virginica""] &lt;- 2 
dataSetEnum[,5] &lt;- as.integer(dataSetEnum[,5])

x &lt;- as.matrix(dataSetEnum[,1:4])
y &lt;- as.factor(dataSetEnum[,5:5])

# number of features
N &lt;- ncol(x)

# define function misclassification.rate
if (exists(""misclassification.rate"") ) rm(misclassification.rate);
misclassification.rate&lt;-function(tab){
  num1&lt;-sum(diag(tab))
  denom1&lt;-sum(tab)
  signif(1-num1/denom1,3)
}

#Fit randomGLM model - Ensemble predictor comprised of individual generalized linear model predictors
RGLM &lt;- randomGLM(x, y, classify=TRUE, keepModels=TRUE,randomSeed=1002)

RGLM$thresholdClassProb

tab1 &lt;- table(y, RGLM$predictedOOB)
tab1
# y  1  2
# 1  2  0
# 2  0 12

# accuracy
1-misclassification.rate(tab1)

# variable importance measure
varImp = RGLM$timesSelectedByForwardRegression
sum(varImp&gt;=0)

table(varImp)

# select most important features
impF = colnames(x)[varImp&gt;=5]
impF

# build single GLM model with most important features
model1 = glm(y~., data=as.data.frame(x[, impF]), family = binomial(link='logit'))

coef(model1)
#(Intercept) PetalLength  PetalWidth 
#-31.938998   -7.501714   63.670583 

exp(coef(model1))
#(Intercept)  PetalLength   PetalWidth 
#1.346075e-14 5.521371e-04 4.485211e+27 

confint.default(model1)
#                2.5 %   97.5 %
#(Intercept) -363922.5 363858.6
#PetalLength -360479.0 360464.0
#PetalWidth  -916432.0 916559.4
</code></pre>
"
"NaN","NaN","215940","<p>I am a statistician. I'm pretty good with the concepts of topics like Linear &amp; Logistic Regression &amp; Time Series.
But in order to run data I need to learn the R language. Since, having no programming background makes it difficult for me to understand it.</p>

<p>How can I easily learn and construct commands in R software? What could help me with achieving that?</p>
"
"NaN","NaN","216099","<p>I'm running a linear regression in R.</p>

<p>If i have an independent variable <code>Gender</code> with only values 0 (for Male) and 1 (for Male), do i need to convert them to factor or character?</p>

<p>What is going to be the impact on my analysis?</p>
"
"0.02831827358943","0.0277563690826684","216247","<p>I ran a linear regression example in R and as a result got the following summary:</p>

<pre><code>Call:
lm(formula = Income ~ Age + Education + Gender, data = income_input)

Residuals:
    Min      1Q  Median      3Q     Max 
-37.340  -8.101   0.139   7.885  37.271 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  7.26299    1.95575   3.714 0.000212 ***
Age          0.99520    0.02057  48.373  &lt; 2e-16 ***
Education    1.75788    0.11581  15.179  &lt; 2e-16 ***
Gender      -0.93433    0.62388  -1.498 0.134443    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 12.07 on 1496 degrees of freedom
Multiple R-squared:  0.6364,    Adjusted R-squared:  0.6357 
F-statistic:   873 on 3 and 1496 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>As you can see, we get the t-values. In this case, how can i change my type of test from t to z?</p>

<p>Furthermore, both t and z tests assume normality. So what if i want to run the Wilcox Test?</p>
"
"0.0566365471788599","0.0555127381653369","217643","<p>I'm trying to find out if my numeric predictors have a linear relation to the logit of my logistic regression. I tried to use the lrm fit in the rms package where I have used 3 knot cubic spline on all numeric predictors like this:</p>

<pre><code>&gt; fit &lt;- lrm(y ~ rcs(x1,3)+rcs(x2,3)+.....)
</code></pre>

<p>There after I used anova on lrm fit. The main question is how do I use the result in anova(fit)? </p>

<p>My understanding is that the wald statistics are just the associated coefficients squared and dived by it's se. But what about the statistics for nonlinear terms here? are they the wald statistics for the coefficients for the squared predictors? </p>

<p>If none of the statistics are significant, can I conclude that there are no quadratic effect from my predictors?</p>
"
"0.0424774103841449","0.0416345536240027","217926","<p><em>Will the signs of coef (Estimate) of lm and glm always be the same?</em> <strong>^</strong></p>

<p>According to below toy example, it seems yes. Can you provide a case where they might be different? (If it matters in my real data the outcome is binary, hence used <code>mpg &gt; 20</code>)</p>

<pre><code># dummy data
d &lt;- mtcars

# fit lm, glm, glm_bi
fit_lm &lt;- lm(mpg &gt; 20 ~ cyl + disp, data = d)
fit_glm &lt;- glm(mpg &gt; 20 ~ cyl + disp, data = d)
fit_glm_bi &lt;- glm(mpg &gt; 20 ~ cyl + disp, family = binomial, data = d)

# Signs are always same?
# lm compared to glm
all.equal(sign(coef(fit_lm)),
          sign(coef(fit_glm)))
# output
# [1] TRUE

# lm compared to glm(family = binomial)
all.equal(sign(coef(fit_lm)),
          sign(coef(fit_glm_bi)))

# output
# [1] TRUE
</code></pre>

<p><strong>^</strong> Very much sounds like a dupe, found this similar post: <a href=""http://stats.stackexchange.com/questions/91666/sign-of-coefficients-in-linear-regression-vs-the-sign-of-correlation"">Sign of coefficients in linear regression vs. the sign of correlation</a>. Let me know if this is a dupe.</p>
"
"0.138841759253618","0.136086795749048","218085","<p>I have two questions concerning planned contrasts: </p>

<ol>
<li>I would like to know how factor-based contrasts (obtained through an interaction term) compare to model-paramter-based contrasts (obtained by specifying model parameters). </li>
<li>I would like to know this for a simple case of comparing one condition with another, but ultimately I am interested in comparing one condition vs all other conditions. </li>
</ol>

<p>Below are my attempts at understanding factor-based contrasts and model-parameter based contrasts:</p>

<pre><code>#create some dummy data
data &lt;- mtcars
#create interaction terms
data$interaction &lt;- interaction(mtcars$am, mtcars$vs, sep=""X"")
	data$interaction &lt;- gsub(""^0"", ""am0"", data$interaction)
	data$interaction &lt;- gsub(""^1"", ""am1"", data$interaction)
	data$interaction &lt;- gsub(""0$"", ""vs0"", data$interaction)
data$interaction &lt;- gsub(""1$"", ""vs1"", data$interaction)
	data$interaction &lt;- factor(data$interaction)
	levels(data$interaction)
#[1] ""am0Xvs0"" ""am0Xvs1"" ""am1Xvs0"" ""am1Xvs1""
</code></pre>

<p>From Eric Fuchs' <a href=""http://r-eco-evo.blogspot.nl/2007/10/one-of-most-neglected-topics-in_06.html"" rel=""nofollow"">blogpost</a> I think I figured out how to obtain factor-based contrasts. Let us assume for now that I am interested in the comparison of <code>am0Xvs0</code> vs. <code>am0Xvs1</code>. To this purpose, I create a contrast matrix where I assign equal weights with opposing signs to my two levels of interest, and 0 to the other two levels:</p>

<pre><code>#specify contrasts:
c.f &lt;- c(-1, 1, 0, 0) 
mat.f &lt;- cbind(c.f)
contrasts(data$interaction) &lt;- mat.f
#fit model
fit.f &lt;- aov(mpg~interaction, data)
#get coefficients for contrasts
summary(fit.f, split=list(interaction=list(""am0Xvs0 vs. am0Xvs1""=1)))
</code></pre>

<p>Output:</p>

<pre><code>                                       Df Sum Sq Mean Sq F value   Pr(&gt;F)    
interaction                         3  788.6  262.86   21.81 1.73e-07 ***
  interaction: am0Xvs0 vs. am0Xvs1  1  232.3  232.28   19.27 0.000147 ***
Residuals                          28  337.5   12.05                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<hr>

<p>For comparison, my attempt at model-parameter-based contrasts:</p>

<pre><code>fit.m &lt;- aov(mpg~am*vs, data)
</code></pre>

<p>with $E[mpg]=b_0 + b_1 \text{am} + b_2 \text{vs} + b3 (\text{am} \times \text{vs}).$ Based on Matt Blackwell's <a href=""http://stats.stackexchange.com/a/13168/79643"">answer</a> I think that a comparison of <code>am0Xvs0</code> vs. <code>am0Xvs1</code> means that $H_0: b_1 = 0$ (i.e., the regression weight associated with <code>am</code>).  Therefore:     </p>

<pre><code>## construct contrast matrices
mat.m &lt;- rbind(""am0:vs0 - vs1"" = c(0, 0, 1, 0))
library(car)
lht(fit.m, mat.m)
</code></pre>

<p>Output: </p>

<pre><code>Linear hypothesis test

Hypothesis:
am = 0

Model 1: restricted model
Model 2: mpg ~ am * vs

  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  
1     29 425.84                              
2     28 337.48  1     88.36 7.3311 0.01142 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The values differ, so one of the two solutions is not correct (and my suspicion is that I did not correctly translate Matt's answer to this case). My question now is: which one of the two approaches is correct and how can the other be rewritten correctly? </p>

<hr>

<p>Ultimately, I am interested in the comparison of 1 of the 4 levels of the interaction term vs. the other 3. Let's say I want to compare <code>am0Xvs0</code> vs. the other 3 conditions. The factor-based version is simply an extension of what I have written above (if what I wrote above was correct): </p>

<pre><code>#create contrast matrix
c.f2 &lt;- c(1, -1/3, -1/3, -1/3) 
mat.f2 &lt;- cbind(c.f2)
contrasts(data$interaction) &lt;- mat.f2
#fit model
fit.f2 &lt;- aov(mpg~interaction, data)
#get coefficients for contrasts
summary(fit.f2, split=list(interaction=list(""am0Xvs0 vs. rest""=1)))
</code></pre>

<p>Output: </p>

<pre><code>                                Df Sum Sq Mean Sq F value   Pr(&gt;F)    
interaction                      3  788.6   262.9   21.81 1.73e-07 ***
  interaction: am0Xvs0 vs. rest  1  487.8   487.8   40.48 6.95e-07 ***
Residuals                       28  337.5    12.1                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Unfortunately I would not know how to start with the parameters for the model-parameter-based version, which is what I am ultimately interested in. </p>
"
"0.0633215847514023","0.0620651280774201","218486","<p>How can we do weighted ridge regression in R?</p>

<p>In MASS package in R, I can do weighted linear regression by passing a weight parameter to <code>lm</code>. It can be seen that the model with weights is different from the one without weights.</p>

<pre><code># with weights
&gt; model = lm( y ~ X - 1, weights = w)
&gt; model$coeff
X(Intercept) XSepal.Length  XSepal.Width XPetal.Length  XPetal.Width
-2.1135159    -0.1890203     1.8198141    -1.1771699     2.2840825 

# without weights
&gt; model = lm( y ~ X - 1)
&gt; model$coeff
X(Intercept) XSepal.Length  XSepal.Width XPetal.Length  XPetal.Width 
-5.23869771    0.09802533    2.16742901   -1.07331102    2.40425352 
</code></pre>

<p>However, when I try to replicate the same with <code>lm.ridge</code>, model generated with and without weights are same.</p>

<pre><code>  # with weights
  &gt; model = lm.ridge(y ~ X - 1, lambda=lmd, weights = w)
  &gt; model
  X(Intercept) XSepal.Length  XSepal.Width XPetal.Length  XPetal.Width 
  -5.17253104    0.08770593    2.15946954   -1.06284572    2.38714738

  # without weights
  &gt; model = lm.ridge(y ~ X - 1, lambda=lmd, weights = w)
  &gt; model
  X(Intercept) XSepal.Length  XSepal.Width XPetal.Length  XPetal.Width 
  -5.17253104    0.08770593    2.15946954   -1.06284572    2.38714738
</code></pre>

<p>Edit 1: 
In linear model, I can calculate stderr of coefficients as follows:</p>

<pre><code>rss = sum( residuals( model, type=""pearson"")^2 )
dispersion = rss / model$df.residual
stderr = sqrt( diag(vcov(model)) ) / sqrt(dispersion)
</code></pre>
"
"0.0400480865731637","0.039253433598943","218598","<p>Suppose that my dependent variable has hard cutoffs in the sense that it is physically impossible to obtain values outside of a certain range.  (If it matters, the dependent variable is a percentage and the independent variables are a mixture of binary and regular numeric data.)  What regression tool is appropriate for this case?</p>

<p>Originally I wanted to use a standard linear regression, but that seems inappropriate here as it would sometimes predict results outside of the ""natural"" range of values for the dependent variable.</p>
"
"0.09392108820677","0.0920574617898323","218738","<p>I want to build a linear regression model where I predict a mean of a group of participants (how they rate something on average). Predictors should be </p>

<ol>
<li>age (continuous)</li>
<li>origin (deviation coded, each level compared to grand mean, levels=1,2,3,4)</li>
<li>education (Helmert coded, each level compared to subsequent ones, haven't decided on number of levels yet)</li>
<li>gender/sex (dummy coded, 0/1)</li>
</ol>

<p>Following questions:</p>

<p><strong>1.</strong> In R, I use the following code for the coding, for example for 4) sex:</p>

<pre><code>    data$sex &lt;- factor(data$sex, labels=c(""1"",""2"")) 
    contr.treatment(2) 
    contrasts(data$sex) = contr.treatment(2)  
</code></pre>

<p>That gives me the right (dummy) coding, for the other 2 and 3 a little differently. Can I use run this kind of code for each predictor (except age) and then throw all predictors into a model like this:</p>

<pre><code>    model &lt;- lm(Mean ~ age +  sex + educ..., data)
</code></pre>

<p>It seems wrong because: what is the common intercept going to be with these different coding systems? It's different for each coding system.
But then, how am I going to enter these different predictors into a model?</p>

<p><strong>2.</strong> Can I leave age in there as it is, unchanged, continuous?</p>

<p><strong>3. Quite a different question:</strong> This is my <em>participant analysis</em>. For the <em>item analysis</em>, I used a logistic regression based on medians instead of means. That's because I did four rating surveys with Likert scales. 4 surveys - 4 participant groups - each group rated the items on <strong>one</strong> property only, such that each item was rated on 4 properties by different people.</p>

<p>Given this, is it okay if I use linear models and means in this analysis now? And can I even build my model as I suggested above?</p>

<p><strong>Many thanks</strong> for any input! I've been trying some things, but confusion isn't fading yet...</p>
"
"0.0566365471788599","0.0555127381653369","219390","<p><a href=""http://i.stack.imgur.com/fUmBg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fUmBg.png"" alt=""enter image description here""></a></p>

<p>This is a graph of revenues for different products with the Y-axis showing normalized revenues (mean of 3 and SD of 1) and X-axis is weeks. I need do a regression analysis of sorts on this data and am unsure how to find a curve/function in R that fits this data. </p>

<p>The data points can be interpreted as being: Week 0 of product release yield normalized revenues between 2.25 to 3.25, etc.</p>

<p>Any help regarding what kind of statistical analysis I can use to create a regression model (linear and logistic wouldn't work clearly) with the end goal being to do predictive analysis (ie. if a new product is released, what normalized revenues would it yield in the first 6 weeks)</p>

<p>Thanks</p>
"
"0.0980973772790571","0.0961508829696314","219679","<p>I would like to know how to find out the analytical solution of a simple linear regression with fixed intercept = 0:</p>

<p>$$ s = e^{-ht}$$
$$ y = -ln(s)  = h\cdot t$$</p>

<p>Here ist the background: I have three survival probabilities $s$ at 30, 90 and 180 days. Obviously, I have at day = 0 100% survival, so I  include this <em>observation</em>. I know that this is contested (<a href=""http://stats.stackexchange.com/questions/102709/when-forcing-intercept-of-0-in-linear-regression-is-acceptable-advisable"" title=""here"">here</a>) but I think in this special case it makes sense. The data I use for fitting the linear regression:</p>

<pre><code>&gt;     obs
    t    s          y
1   0 1.00 0.00000000
2  30 0.98 0.02020271
3  90 0.90 0.10536052
4 180 0.80 0.22314355
</code></pre>

<p>If I fit with simple regression I get this:</p>

<pre><code>&gt;     (fit1 &lt;- lm(y~t, data=obs))

Call:
lm(formula = y ~ t, data = obs)

Coefficients:
(Intercept)            t  
  -0.008464     0.001275
</code></pre>

<p>This can be obtained analytically if the following function is derived:</p>

<p>$$f(h) = \sum (y_i - ht_i)^2$$</p>

<p>which gives:</p>

<p>$$ \frac{\sum (y_i-\bar{y})\cdot (t_i-\bar{t})}{\sum (t_i-\bar{t})^2}$$</p>

<hr>

<p>UPDATE 1: This is the result of the minimization of 
$$f(h) = \sum (y_i - c - ht_i)^2$$. The correct result (see answers):
$$ \frac{\sum (y_i\cdot t_i)}{\sum t_i^2}$$</p>

<hr>

<p>The analytical results is:</p>

<pre><code>yc &lt;- with(obs,y-mean(y))
tc &lt;- with(obs, t -  mean(t))
sum(yc*tc)/sum(tc^2)
[1] 0.001275204
</code></pre>

<p>The same as coefficient in the fit1. Now, if I fix intercept to intercept=0 I get this:</p>

<pre><code>&gt;     (fit2 &lt;- lm(y~0+t, data=obs))

Call:
lm(formula = y ~ 0 + t, data = obs)

Coefficients:
   t  
0.001214  
</code></pre>

<p>I'm wondering how I can get an analytical solution for this. How I have to consider the fix intercept in the function $f(h)$ above?</p>

<p>Any idea is appreciated.</p>

<p><a href=""http://i.stack.imgur.com/ilwvG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ilwvG.png"" alt=""enter image description here""></a></p>

<hr>

<p>Here is the way I constructed the data:</p>

<pre><code>set.seed(123)
# Hazard ratio
h &lt;- 0.7
# Number of observation
n &lt;- 50
# Model: exponential
t &lt;- rexp(n,h)
# scale to days
t &lt;- t*365.25
hist(t)
t &lt;- sort(t)
# Put data into a dataframe
df0 &lt;- data.frame(t=t)
head(df0)
# Compute probablities
df0$s &lt;- 1 - c(1:n)/n
head(df0)
# Extract survival probabilities at 30,90 and 180 days
df0$t2 &lt;- ceiling(df0$t/30)*30
# Select survival probablity 30, 90, 180 days
library(sqldf)
obs &lt;- sqldf(""SELECT t2 t, MAX(s) s FROM df0 WHERE t2 IN (30,90,180) GROUP BY t2"")
# Add survival probability=1 at day 0
obs &lt;- rbind(data.frame(t = 0, s = 1), obs)
# s = e^(-ht)  =&gt; y = -ln(s) = h*t
obs$y &lt;- -log(obs$s)
plot(y~t, data=obs)
fit1 &lt;- lm(y~t, data=obs)
abline(fit1,lty=2)
fit2 &lt;- lm(y~0+t, data=obs)
abline(fit2,lty=2, col=""red"")
legend(""topleft"", legend=c(""fit1"",""fit2""), col=c(1,2), lty=c(2,2))
</code></pre>
"
"0.0633215847514023","0.0620651280774201","220614","<p>My problem is how to find the best decreasing 3rd degree polynomial regression in <code>R</code>.
I have data, lets say</p>

<pre><code>x &lt;- x &lt;- c(1:5, 17, 23, 30, 35, 36)
x &lt;- x/max(x)
y &lt;- c(600, 555, 400, 333, 332, 331, 330, 214, 210, 190)
</code></pre>

<p>Here I want to find best polynomial fitting these points, with a constraint, that polynomial must be in whole interval decreasing (here the is interval from 0 to 1). When I use simple <code>lm</code></p>

<pre><code>m &lt;- lm(y ~ I(x^3) +I(x^2) + x)
</code></pre>

<p>the polynomial is not strictly decreasing. </p>

<p>Simple math: I want to find a polynomial $$ y = ax^3 + bx^2 +cx + d $$ </p>

<p>This polynomial is decreasing when </p>

<p>$$ dy / dx &lt; 0 $$,</p>

<p>which means</p>

<p>$$ 3ax^2 + 2bx + c &lt; 0 $$.</p>

<p>From here I can find the constraints, that are</p>

<ol>
<li>$ a &lt; (-2bx - c)/3x^2 $,</li>
<li>$ b &lt; (-3x^2 - c)/2x $,</li>
<li>$ c &lt; -3ax^2 - 2bx $.</li>
</ol>

<p>Is there any way how to input these constraints into <code>lm</code> ? </p>

<p>PS: I found <a href=""http://stats.stackexchange.com/questions/61733/linear-regression-with-slope-constraint"">this link</a> with similar question, but my problem is little more complex. Hope somebody can help!</p>

<p>EDIT: I tested the current answer on this new data set and it doesn't work, why?</p>

<pre><code>x1 &lt;- c(0.01041667, 0.30208333, 0.61458333, 0.65625000, 0.83333333)
y1 &lt;- c(772, 607, 576, 567, 550)
</code></pre>
"
"0.0424774103841449","0.0555127381653369","220833","<p>I want to do a log-log regression in R.
I managed to do a simple linear and log-linear regression by using this code:</p>

<pre><code>lm &lt;- lm(Price ~ ., data=data_price2)
lm2 &lt;- lm(log(Price) ~ ., data=data_price2)
</code></pre>

<p>Now, I want to do a log-log regression, but I can't find out how to add the independent variables in the logarithmic form. Some of these independent variables are dummy variables.
If I add them individually after the '~' in the equation, R gives me this error:</p>

<pre><code>lm4 &lt;- lm(log(Price) ~ log(nbrCores)+ log(nbrSims) + log(CameraBack), data = data_price2)
</code></pre>

<p><strong>Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : 
  NA/NaN/Inf in 'x'</strong></p>

<p>I have about 140 independent variables, so I prefer not to add them manually.</p>

<p>Can anyone help me with this?</p>
"
"0.106193525960362","0.104086384060007","221161","<p>I have been having trouble with the predict function underestimating (or overestimating) the predictions from an lmer model with some polynomials. Hopefully my edits make it clearer. I have scaled data that looks like this:</p>

<pre><code>Terr      Date     Year            Age  
T.092     123      0.548425     -0.86392            
T.104     102      1.2072       -0.48185            
T.104     105      1.075445     -0.86392            
T.104     112      0.94369      -1.24599            
T.040     116     -0.2421        2.192652           
T.040     114     -0.37386       1.810581           
T.040     119     -0.50561       1.428509           
T.040     128      0.15316      -0.09978            
T.040     113      0.021405     -0.48185
</code></pre>

<p>Iâ€™m trying to determine how Year affects lay date after controlling for Age, with Terr (territory) as a random variable. I usually include polynomials and do model averaging, but whether I use a single model or do model averaging, the predict function gives predictions that are a bit lower or higher than they should be. I realize that the model below would not be a good model for this data, Iâ€™m just trying to provide a simplified example.  </p>

<p>Below is my code  </p>

<pre><code>library(lme4
m1 &lt;- lmer(Date ~ (1|Terr) + Year + Age + I(Age^2), data=data)
new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictions=predict(m1, newdata = new.dat, re.form=NA)
pred.l&lt;-cbind(new.dat, Predictions)
pred.l  

      Year          Age Predictions
    1   -2 2.265676e-16    124.4439
    2   -1 2.265676e-16    123.2124
    3    0 2.265676e-16    121.9810
    4    1 2.265676e-16    120.7496
</code></pre>

<p>When plotted with the means, the graph looks like this:</p>

<p><a href=""http://i.stack.imgur.com/mwIpJ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mwIpJ.jpg"" alt=""graph1""></a></p>

<p>When I use effects, I get a much better fit  </p>

<pre><code>library(effects)
ef.1c=effect(c(""Year""), m1, xlevels=list(Year=-2:1))
pred.lc=data.frame(ef.1c)
pred.lc

      Year      fit        se    lower    upper
    1   -2 126.0226 0.6186425 124.8089 127.2363
    2   -1 124.7911 0.4291211 123.9493 125.6330
    3    0 123.5597 0.3298340 122.9126 124.2068
    4    1 122.3283 0.3957970 121.5518 123.1048
</code></pre>

<p><a href=""http://i.stack.imgur.com/SvI3f.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SvI3f.jpg"" alt=""graph2""></a></p>

<p>After much trial and error, I have discovered that the problem is with the Age polynomial, because when the Age polynomial is not included, the predicted and fitted are equal and both fit well. Below is the same  model but with Age as a linear term.  </p>

<pre><code>m2 &lt;- lmer(Date ~ (1|Terr) + Year + Age, data=data)
new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictionsd=predict(m2, newdata = new.dat, re.form=NA)  
pred.ld&lt;-cbind(new.dat, Predictionsd)
pred.ld

      Year          Age Predictionsd
    1   -2 2.265676e-16     125.9551
    2   -1 2.265676e-16     124.7653
    3    0 2.265676e-16     123.5755
    4    1 2.265676e-16     122.3857

library(effects)
ef.1e=effect(c(""Year""), m2, xlevels=list(Year=-2:1))
pred.le=data.frame(ef.1e)
pred.le

      Year      fit        se    lower    upper
    1   -2 125.9551 0.6401008 124.6993 127.2109
    2   -1 124.7653 0.4436129 123.8950 125.6356
    3    0 123.5755 0.3406741 122.9072 124.2439
    4    1 122.3857 0.4093021 121.5827 123.1887
</code></pre>

<p>I do many similar analyses, and this issue with the predictions being slightly lower (or higher) than they should be often happens when Age is included as a polynomial. When I include a polynomial for Year, there is no problem and the predicted and fitted are equal, so I know the problem is not with all polynomials.</p>

<pre><code>m3 &lt;- lmer(Date ~ (1|Terr) + Year + I(Year^2) + Age, data=data)

new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictionsf=predict(m3, newdata = new.dat, re.form=NA)  
pred.lf&lt;-cbind(new.dat, Predictionsf)
pred.lf

      Year          Age Predictionsf
    1   -2 2.265676e-16     125.6103
    2   -1 2.265676e-16     124.8494
    3    0 2.265676e-16     123.7483
    4    1 2.265676e-16     122.3070

library(effects)
ef.1g=effect(c(""Year""), m3, xlevels=list(Year=-2:1))
pred.lg=data.frame(ef.1g)
pred.lg

      Year      fit        se    lower    upper
    1   -2 125.6103 0.8206625 124.0003 127.2203
    2   -1 124.8494 0.4615719 123.9438 125.7549
    3    0 123.7483 0.4275858 122.9094 124.5871
    4    1 122.3070 0.4262110 121.4708 123.1431
</code></pre>

<p>I've looked for answers (e.g., <a href=""http://stats.stackexchange.com/questions/180010/overestimated-and-underestimated-predictions-in-regression"">here</a>) but haven't found anything that is directly helpful. I can provide the whole data set if needed. Does anyone have any insight?</p>
"
"NaN","NaN","221231","<p>I want to check multicollinearity to avoid any redundancy in my database before doing the multinomial logistic regression with categorical dependent variable using R, knowing that the majority of my variables expressed as dichotomous and ordinal. Not the VIF method! Is there any other method that I can use before the regression?</p>
"
"0.0326991257596857","0.0320502943232105","221630","<p>Since standard error of a linear regression is usually given for the response variable, I'm wondering how to obtain confidence intervals in the other direction - e.g. for an x-intercept. I'm able to visualize what it might be, but I'm sure there must be a straightforward way to do this. Below is an example in R of how to visualize this:</p>

<pre><code>set.seed(1)
x &lt;- 1:10
a &lt;- 20
b &lt;- -2
y &lt;- a + b*x + rnorm(length(x), mean=0, sd=1)

fit &lt;- lm(y ~ x)
XINT &lt;- -coef(fit)[1]/coef(fit)[2]

plot(y ~ x, xlim=c(0, XINT*1.1), ylim=c(-2,max(y)))
abline(h=0, lty=2, col=8); abline(fit, col=2)
points(XINT, 0, col=4, pch=4)
newdat &lt;- data.frame(x=seq(-2,12,len=1000))

# CI
pred &lt;- predict(fit, newdata=newdat, se.fit = TRUE) 
newdat$yplus &lt;-pred$fit + 1.96*pred$se.fit 
newdat$yminus &lt;-pred$fit - 1.96*pred$se.fit 
lines(yplus ~ x, newdat, col=2, lty=2)
lines(yminus ~ x, newdat, col=2, lty=2)

# approximate CI of XINT
lwr &lt;- newdat$x[which.min((newdat$yminus-0)^2)]
upr &lt;- newdat$x[which.min((newdat$yplus-0)^2)]
abline(v=c(lwr, upr), lty=3, col=4)
</code></pre>

<p><a href=""http://i.stack.imgur.com/bgrM3.png""><img src=""http://i.stack.imgur.com/bgrM3.png"" alt=""enter image description here""></a></p>
"
"0.02831827358943","0.0277563690826684","221819","<p>I have a doubt about use of linear regression.</p>

<ol>
<li>If the correlation between two variables is 0, is there any use of applying linear regression on those variables?</li>
<li>If possible can you explain when we should use regression methods (only when correlation is +-1?).</li>
</ol>
"
"0.118095364492158","0.133114869819364","221880","<p>To explore how the <code>LASSO</code> regression works, I wrote a small piece of code that should optimize <code>LASSO</code> regression by picking the best alpha parameter.</p>

<p>I cannot figure out why the <code>LASSO</code> regression is giving me such unstable results for the alpha parameter after cross validation.</p>

<p>Here is my Python code:</p>

<pre><code>from sklearn.linear_model import Lasso
from sklearn.cross_validation import KFold
from matplotlib import pyplot as plt

# generate some sparse data to play with
import numpy as np
import pandas as pd 
from scipy.stats import norm
from scipy.stats import uniform

### generate your own data here

n = 1000

x1x2corr = 1.1
x1x3corr = 1.0
x1 = range(n) + norm.rvs(0, 1, n) + 50
x2 =  map(lambda aval: aval*x1x2corr, x1) + norm.rvs(0, 2, n) + 500
y = x1 + x2 #+ norm.rvs(0,10, n)

Xdf = pd.DataFrame()
Xdf['x1'] = x1
Xdf['x2'] = x2

X = Xdf.as_matrix()

# Split data in train set and test set
n_samples = X.shape[0]
X_train, y_train = X[:n_samples / 2], y[:n_samples / 2]
X_test, y_test = X[n_samples / 2:], y[n_samples / 2:]

kf = KFold(X_train.shape[0], n_folds = 10, )
alphas = np.logspace(-16, 8, num = 1000, base = 2)

e_alphas = list()
e_alphas_r = list()  # holds average r2 error
for alpha in alphas:
    lasso = Lasso(alpha=alpha, tol=0.004)
    err = list()
    err_2 = list()
    for tr_idx, tt_idx in kf:
        X_tr, X_tt = X_train[tr_idx], X_test[tt_idx]
        y_tr, y_tt = y_train[tr_idx], y_test[tt_idx]
        lasso.fit(X_tr, y_tr)
        y_hat = lasso.predict(X_tt)

        # returns the coefficient of determination (R^2 value)
        err_2.append(lasso.score(X_tt, y_tt))

        # returns MSE
        err.append(np.average((y_hat - y_tt)**2))
    e_alphas.append(np.average(err))
    e_alphas_r.append(np.average(err_2))

## print out the alpha that gives the minimum error
print 'the minimum value of error is ', e_alphas[e_alphas.index(min(e_alphas))]
print ' the minimizer is ',  alphas[e_alphas.index(min(e_alphas))]

##  &lt;&lt;&lt; plotting alphas against error &gt;&gt;&gt;

plt.figsize = (15, 15)
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(alphas, e_alphas, 'b-')
ax.plot(alphas, e_alphas_r, 'g--')
ax.set_ylim(min(e_alphas),max(e_alphas))
ax.set_xlim(min(alphas),max(alphas))
ax.set_xlabel(""alpha"")
plt.show()
</code></pre>

<p>If you run this code repeatedly, it gives wildly different results for alpha:</p>

<pre><code>&gt;&gt;&gt; 
the minimum value of error is  3.99254192539
 the minimizer is  1.52587890625e-05
&gt;&gt;&gt; ================================ RESTART ================================
&gt;&gt;&gt; 
the minimum value of error is  4.07412455842
 the minimizer is  6.45622425334
&gt;&gt;&gt; ================================ RESTART ================================
&gt;&gt;&gt; 
the minimum value of error is  4.25898253597
 the minimizer is  1.52587890625e-05
&gt;&gt;&gt; ================================ RESTART ================================
&gt;&gt;&gt; 
the minimum value of error is  3.79392968781
 the minimizer is  28.8971008254
&gt;&gt;&gt; 
</code></pre>

<p>Why is the alpha value not converging properly?  I know that my data is synthetic, but the distribution is the same.  Also, the variation is very small in <code>x1</code> and <code>x2</code>.</p>

<p>what could be causing this to be so unstable?  </p>

<h2>The same thing written in R gives different results - it always returns the highest possible value for alpha as the ""optimal_alpha"".</h2>

<p>I also wrote this in R, which gives me a slightly different answer, which I don't know why?</p>

<pre><code>library(glmnet)
library(lars)
library(pracma)

set.seed(1)
k = 2 # number of features selected 

n = 1000

x1x2corr = 1.1
x1 = seq(n) + rnorm(n, 0, 1) + 50
x2 =  x1*x1x2corr + rnorm(n, 0, 2) + 500
y = x1 + x2 

filter_out_label &lt;- function(col) {col!=""y""}

alphas = logspace(-5, 6, 100)

for (alpha in alphas){
  k = 10
  optimal_alpha = NULL
  folds &lt;- cut(seq(1, nrow(df)), breaks=k, labels=FALSE)
  total_mse = 0
  min_mse = 10000000
  for(i in 1:k){
    # Segement your data by fold using the which() function
    testIndexes &lt;- which(folds==i, arr.ind=TRUE)
    testData &lt;- df[testIndexes, ]
    trainData &lt;- df[-testIndexes, ]

    fit &lt;- lars(as.matrix(trainData[Filter(filter_out_label, names(df))]),
                trainData$y,
                type=""lasso"")
    # predict
    y_preds &lt;- predict(fit, as.matrix(testData[Filter(filter_out_label, names(df))]),
                       s=alpha, type=""fit"", mode=""lambda"")$fit # default mode=""step""

    y_true = testData$y
    residuals = (y_true - y_preds)
    mse=sum(residuals^2)
    total_mse = total_mse + mse
  }
  if (total_mse &lt; min_mse){
    min_mse = total_mse
    optimal_alpha = alpha
  }
}

print(paste(""the optimal alpha is "", optimal_alpha))
</code></pre>

<p>The output from the R code above is: </p>

<pre><code>&gt; source('~.....')
[1] ""the optimal alpha is  1e+06""
</code></pre>

<p>In fact, no matter what I set for the line ""<code>alphas = logspace(-5, 6, 100)</code>"", I always get back the highest value for alpha.</p>

<p>I guess there are actually 2 different questions here :</p>

<ol>
<li><p>Why is the alpha value so unstable for the version written in Python? </p></li>
<li><p>Why does the version written in R give me a different result?  (I realize that the <code>logspace</code> function is different from <code>R</code> to <code>python</code>, but the version written in <code>R</code> always gives me the largest value of <code>alpha</code> for the optimal alpha value, whereas the python version does not).</p></li>
</ol>

<p>It would be great to know these things...</p>
"
"NaN","NaN","221903","<p>I have tried both Multiple linear regression and classification for a specific data. I understand that both are different methods. Is there any specific parameter that I can compare on outputs of these two parameters ? </p>
"
"0.0980973772790571","0.0961508829696314","222233","<p>I try to reproduce with <code>optim</code> the results from a simple linear regression fitted with <code>glm</code> or even <code>nls</code> R functions.<br>
The parameters estimates are the same but the residual variance estimate and the standard errors of the other parameters are not the same particularly when the sample size is low. I suppose that this is due differences in the way the residual standard error is calculated between Maximum Likelihood and Least square approaches (dividing by n or by n-k+1 see bellow in the example).<br>
I understand from my readings on the web that optimization is not a simple task but I was wondering if it would be possible to reproduce in a simple way the residual standard error estimate from <code>glm</code> while using <code>optim</code>. </p>

<p>Simulate a small dataset</p>

<pre><code>set.seed(1)
n = 4 # very small sample size !
b0 &lt;- 5
b1 &lt;- 2
sigma &lt;- 5
x &lt;- runif(n, 1, 100)
y =  b0 + b1*x + rnorm(n, 0, sigma) 
</code></pre>

<p>Estimate with optim</p>

<pre><code>negLL &lt;- function(beta, y, x) {
    b0 &lt;- beta[1]
    b1 &lt;- beta[2]
    sigma &lt;- beta[3]
    yhat &lt;- b0 + b1*x
    likelihood &lt;- dnorm(y, yhat, sigma)
    return(-sum(log(likelihood)))
}

res &lt;- optim(starting.values, negLL, y = y, x = x, hessian=TRUE)
estimates &lt;- res$par     # Parameters estimates
se &lt;- sqrt(diag(solve(res$hessian))) # Standard errors of the estimates
cbind(estimates,se)


    &gt; cbind(estimates,se)
      estimates         se
b0     9.016513 5.70999880
b1     1.931119 0.09731153
sigma  4.717216 1.66753138
</code></pre>

<p>Comparison with glm and nls</p>

<pre><code>&gt; m &lt;- glm(y ~ x)
&gt; summary(m)$coefficients
            Estimate Std. Error   t value    Pr(&gt;|t|)
(Intercept) 9.016113  8.0759837  1.116411 0.380380963
x           1.931130  0.1376334 14.030973 0.005041162
&gt; sqrt(summary(m)$dispersion) # residuals standard error
[1] 6.671833
&gt; 
&gt; summary(nls( y ~ b0 + b1*x, start=list(b0 = 5, b1= 2)))

Formula: y ~ b0 + b1 * x

Parameters:
   Estimate Std. Error t value Pr(&gt;|t|)   
b0   9.0161     8.0760   1.116  0.38038   
b1   1.9311     0.1376  14.031  0.00504 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6.672 on 2 degrees of freedom
</code></pre>

<p>I can reproduce the different residual standard error estimates like this : </p>

<pre><code>&gt; # optim / Maximum Likelihood estimate
&gt; sqrt(sum(resid(m)^2)/n)
[1] 4.717698
&gt; 
&gt; # Least squares estimate (glm and nls estimates)
&gt; k &lt;- 3 # number of parameters
&gt; sqrt(sum(resid(m)^2)/(n-k+1))
[1] 6.671833
</code></pre>
"
"NaN","NaN","222726","<p>I'm quite new at linear regression.
I get the following Residuals vs Fitted plot :</p>

<p><a href=""http://i.stack.imgur.com/UL48X.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UL48X.png"" alt=""enter image description here""></a></p>

<p>I do not understand why there are two parallel lines. Is there a problem with my data?</p>

<p>Thx !</p>
"
"0.0800961731463273","0.078506867197886","222803","<p>I am using elastic net regression on a dataset with quite a small number of observations (clinical risk scores) and large number (1000+) of potential predictor variables (gene expression values). The ultimate aim is to identify variables (genes) that could be explored further experimentally. </p>

<p>However, I noticed that the variables being selected (e.g. coefficients not equal to zero) vary when I leave out a single observation from the dataset (some are maintained, some drop, some are added in), and I would like get some more confidence regarding which variables are relatively robust to such changes.</p>

<ol>
<li><p>Would it be methodologically acceptable to generate either jackknife or bootstrap datasets from my original dataset, and repeat the whole model selection procedure (e.g. repeated selection of tuning parameters based on cross validation and the associated model) for each of these datasets, and then for each of those models determine how often a variable was selected?</p></li>
<li><p>Would it be acceptable to re-run a linear regression analysis with sets of selected genes of decreasing frequency of selection (e.g. only use the top 3 genes selected most often, or the top 4 etc), and select the cut-off of frequency of selection of genes based on the cross validation metrics I get for each of these models? If I do this I see a sort of leveling of cross validation prediction error metrics after a certain point (e.g. adding further variables does not help to reduce prediction error metrics). This second approach feels like a ""wrong"" thing to do. However, it is just to determine a threshold of which subset of selected genes I want to focus on further.</p></li>
</ol>
"
"0.0749231094763201","0.0734364498908627","223379","<p>I'm fitting an <code>arima</code>(1,0,0) model using the <code>forecast</code> package in R on the <code>usconsumption</code> dataset. However, when I mimic the same fit using <code>lm</code>, I get different coefficients. My understanding is that they should be the same (in fact, they give the same coefficients if I model an <code>arima</code>(0,0,0) and <code>lm</code> with only the external regressor, which is related to this post: <a href=""http://stats.stackexchange.com/questions/28472/regression-with-arima0-0-0-errors-different-from-linear-regression"">Regression with ARIMA(0,0,0) errors different from linear regression</a>). </p>

<p>Is this because <code>arima</code> and <code>lm</code> use different techniques to calculate coefficients? If so, can someone explain the difference?  </p>

<p>Below is my code.</p>

<pre><code>&gt; library(forecast)
&gt; library(fpp)
&gt; 
&gt; #load data
&gt; data(""usconsumption"")
&gt; 
&gt; #create equivalent data frame from time-series
&gt; lagpad &lt;- function(x, k=1) {
+   c(rep(NA, k), x)[1 : length(x)] 
+ }
&gt; usconsumpdf &lt;- as.data.frame(usconsumption)
&gt; usconsumpdf$consumptionLag1 &lt;- lagpad(usconsumpdf$consumption)
&gt; 
&gt; #create arima model
&gt; arima(usconsumption[,1], xreg=usconsumption[,2], order=c(1,0,0))

Call:
arima(x = usconsumption[, 1], order = c(1, 0, 0), xreg = usconsumption[, 2])

Coefficients:
         ar1  intercept  usconsumption[, 2]
      0.2139     0.5867              0.2292
s.e.  0.0928     0.0755              0.0605

sigma^2 estimated as 0.3776:  log likelihood = -152.87,  aic = 313.74
&gt; 
&gt; #create lm model
&gt; lm(consumption~consumptionLag1+income, data=usconsumpdf)

Call:
lm(formula = consumption ~ consumptionLag1 + income, data = usconsumpdf)

Coefficients:
    (Intercept)  consumptionLag1           income  
         0.3779           0.2456           0.2614  
</code></pre>
"
"0.0633215847514023","0.0620651280774201","223447","<p>Let's suppose I have <em>p</em> predictor variables. For those predictors, there exists a weight vector <em>w</em> of length <em>p</em> that, if multiplied by the predictors, will minimize an error function. This is not any different than what linear regression performs when the error metric is RMSE. The problem is that I am not using RMSE to determine performance. Instead, I must multiply my weights by my predictors, then plug them into a complex function that takes .5 seconds to compute, and only then do I know if my error improved or worsened. </p>

<p>Pseudo R Code:</p>

<pre><code>vec=rnorm(150,0,1)
p=matrix(unlist(split(vec, ceiling(seq_along(vec)/15))),ncol=10)
response=rnorm(15,0,1)
w=rnorm(15,0,1)

for(i in 1:500){
  #multiply predictors by weights to get predictions
  preds=colSums(t(p)*w)

  #complex error function that takes .5 seconds, e.g.:
  #this isn't the true error function, just an example:
  preds=ifelse(preds&gt;1,preds,ifelse(preds&lt;=1&amp;preds&gt;0,0,-1)) 
  error=mean(abs(response-preds))

  #update weight vector w to move in the most optimal pattern to minimize error
  w= ???
}
</code></pre>

<p>How to update <em>w</em> in the most efficient manner?</p>
"
"0.0490486886395286","0.0320502943232105","223463","<p>I am trying to fit a piecewise linear model in R. The line is constrained to be constant, then linearly increase, then remain constant, then linearly decrease, then finally remain constant again (so it's hill-shaped). The problem can be reduced to solving for 3 parameters; the first, second, and third constant values. I have read methods for piecewise models, but cannot find anything for including constraints. </p>

<p>I have seen <a href=""https://stats.stackexchange.com/questions/149627/piecewise-regression-with-constraints"">this question</a> which is very similar, but treating it as an optimization function is not ideal for me because I am fitting this model within an optimization function. </p>

<p>The data is actual transitions in a nonhomogeneous Markov chain, so here is a small sample: </p>

<pre><code>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]    0    1    1    1    0    1    0    0    0
[2,]    0    0    1    1    1    0    1    0    0
[3,]    0    0    0    1    0    0    0    0    0
[4,]    0    1    1    1    1    0    0    0    0
[5,]    0    1    1    1    1    0    0    0    0
[6,]    0    1    1    1    1    1    0    0    0
</code></pre>
"
"0.109891042959396","0.11444244151147","223901","<p><strong>Does anyone know how R computes the model matrix from a formula in aov() or lm()?</strong> </p>

<p>I wonder about some things. Just assume the models below makes sense (although it might not). You have factor S with 3 levels and 2 regression variables, x1 &amp; x2. Printing model.matrix gives me 1st row output as below. <strong>I would like to know what R does when reading in the formula (taking into account the formula terms which are either factors or regression variables)</strong>.</p>

<pre><code>~S+x1+x2
(Intercept) S2 S3 x1 x2
</code></pre>

<p>The first case, R assumes S1 to be the intercept and fits the main effects each level of S and each of the regression variables. I think this should be correct.</p>

<pre><code>~S:(x1+x2)
(Intercept) S1:x1 S2:x1 S3:x1 S1:x2 S2:x2 S3:x2
</code></pre>

<p>The second case, R expands the formula such that ~S:x1+S:x2. It considers S1:x1 which is the first term of the formula as a regression coefficient so it fits an ordinary intercept. Also, it fits all the linear combinations of levels of S and the regression variables. </p>

<pre><code>~S*(x1+x2)
(Intercept) S2 S3 x1 x2 S2:x1 S3:x1 S2:x2 S3:x2
</code></pre>

<p>The third case, R expands the formula such that ~S+x1+x2+S:x1+S:x2. It considers S1 as the first term of the formula so it makes it its intercept. The main effects are fit first. Then something weird happens. <strong>Why does S2:x1 appear but not S1:x1 first?</strong> <strong>Furthermore, why does S2:x2 appear but not S1:x2 first?</strong>. </p>

<pre><code>~S*(x1+x2)+F
(Intercept) S2 S3 x1 x2 F2 F3 S2:x1 S3:x1 S2:x2 S3:x2
</code></pre>

<p>The fourth case, I added a factor F with 3 levels into the equation. The formula expands so ~S+x1+x2+F. It appears that all main effect terms are brought forward but the order in which they appear in the equation is preserved. Then followed by interaction terms and after that higher order interaction terms(if there are). Since S1 appears first I would assume that S1 is the intercept. <strong>But why doesn't F1 appear before F2 , why doesn't S1:x1 appear before S2:x1 and why doesn't S1:x2 appear before S2:x2?</strong></p>

<p>I may have a conceptual misunderstanding with intercepts and how it relates to how the coefficients are fitted. Thanks for the help in advance.</p>
"
"0.0400480865731637","0.039253433598943","224165","<p>I built a mixed linear regression model which includes a dependent variable 'dv', independent variable 'v1' &amp; 'v2', and subject ID 'subject'. </p>

<p>The R syntax is shown below:</p>

<p>output &lt;-lmer(dv ~ v1 + v2 + v1*v2 + (1|subject)+(0+ v1|subject)+(0+ v2 |subject), data=matrix, control=lmerControl(optimizer=""bobyqa"",optCtrl=list(maxfun=2e5)))</p>

<p>The dataset is here:
<a href=""https://1drv.ms/t/s!AitdBHtSjoIpiCXWmzLEnvqX1iuE"" rel=""nofollow"">dataset</a></p>

<p><em>My question:</em></p>

<p>There is a point very strange, the significance of interaction is not consistent with the data pattern (as shown on attached plot), the fixed effect of interaction shows a very small p-value, meaning the slope of two lines should be significantly different, however, the plot does not show an expected pattern. I am worrying there is something wrong, which one (mixed model output or plot result) is correct?</p>

<p><a href=""http://i.stack.imgur.com/IMWEm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IMWEm.png"" alt=""enter image description here""></a></p>
"
"0.0633215847514023","0.0620651280774201","224310","<p>This is a two part question concerning linear regression in R. Here is my code and what my residual plot looks like before transformation:</p>

<pre><code>linear_model &lt;- lm(dependent ~ ., data=independent)
residuals &lt;- resid(linear_model)
plot(residuals)
</code></pre>

<p>[I'll add image, as soon as I've got enough reputation to post three links.]</p>

<p>Obviously there is a pattern, although I find it hard to describe. In any case I would think there is evidence for a non-linear relationship. The only function I can think of is $sine$. So I applied the $sine$ function on the dependent variable, played with it a bit and ended up with a shift of $1/2*\pi$ (so the same as cosinus). This what my code and residual plot look like after transformation:</p>

<pre><code>linear_model &lt;- lm(sin((1/2)*pi+dependent) ~ ., data=independent)
residuals &lt;- resid(linear_model)
plot(residuals)
</code></pre>

<p><a href=""http://i.stack.imgur.com/pJ91J.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pJ91J.png"" alt=""enter image description here""></a></p>

<p><strong>Question 1</strong>
Is the application of $sine$ on the dependent variable legitimate?</p>

<p><strong>Question 2</strong>
Although there still is a pattern - maybe even more distinct than before - the residuals are much lower. Does that indicate that the transformation was the right thing to do? And should I try to get rid of the still existing pattern?</p>

<p>Finally, the model's adjusted RÂ² value increased from .13 to .3.</p>

<p><strong>Edit:</strong> As a reaction to the comments, some more information on the dataset. The data has been recorded over a time span of <s>two</s> four weeks, 18 hours each day with intervals between roughly 2-10 minutes. Here is the autocorrelation function estimate of the dependent variable:</p>

<pre><code>acf(dependent)
</code></pre>

<p><a href=""http://i.stack.imgur.com/A0S1D.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/A0S1D.png"" alt=""enter image description here""></a></p>

<p>Apparently, the answer to question 1 is <em>No</em>.</p>
"
"0.0895502439463906","0.0877733458775107","224377","<p>I am runnning a Random Coefficient Mixed Model in <code>R</code> using <code>lme</code> in <code>{lme4}</code>. I had to transform my dependent variable by square-root because of problems of uniqual variance of the errors. However, with this formulation of the DV, the interpretation of my coefficients' predictors becames quite tricky.</p>

<p>My sample counts 20,000 observations.</p>

<p>Originally I have thought of switching to Non-Linear Mixed models, but in <em>stackoverflow.com</em> someone suggested that ""fitting a variance structure with the weights parameter"" could be a valid alternative to the use of Non-Linear Mixed Model.</p>

<p>I have thus tried fitting a regression of the kind below with <code>lme</code> <code>{lme4}</code> in <code>R</code>,</p>

<p><strong>note that</strong> part of the heteroskedasticity takes place between groups, the random coefficient model improves the structure of the errors when taking into account for the province and district levels, however the non-normality of the DV causes the errors' distribution to be non-normal too.
The square root transformation makes the DV approximate a normal almost perfectly- see at the end of the post.</p>

<p><code>Model2 &lt;-
  lme(
       fcs ~ hh_size + head_sex + head_age + head_edu + residence_code + head_marital_status + ...,
       random = list(
       dist_code_unique = ~ 1 + some vars,
       prov_code = ~ 1
       ),
       weights =~ fcs_sqrt,
       data = data
  )</code>
where fcs is my original dependent variable, and fcs_sqrt is the square root transformation of it.</p>

<p>The result using and not using weights in terms of standardize residuals is shown in the two graphs below.</p>

<p><strong>The question is</strong>: Am I allowed to give the weights in this manner? Are there any implications for the interpretation of the results?</p>

<p><a href=""http://i.stack.imgur.com/svIQo.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/svIQo.png"" alt=""No weights used""></a>
<a href=""http://i.stack.imgur.com/h4iZh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/h4iZh.png"" alt=""Weights = fcs_sqrt""></a></p>

<p><a href=""http://i.stack.imgur.com/xv6Ve.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xv6Ve.png"" alt=""enter image description here""></a></p>
"
"0.0899225958391357","0.0961508829696314","224706","<p>Imagine a simple experiment:</p>

<p>I have 10 ""almost identical"" pieces.<br>
I take the first one and I measure the temperature at 5 different distances from its center.<br>
I take the second one and I do the same 1 hour later.<br>
I take the third one and I do the same 2 hour laters.<br>
and so on till the last piece (I measure its temperatures at 5 different positions 9 hours later).   </p>

<p>My teacher has donde a simple regression model like (R syntax)</p>

<pre><code>Temp  ~ Time + Position
</code></pre>

<p>But, is it right?<br>
From my point of view the measures are not independent.<br>
Shouldn't we use a mixed effects model like this?</p>

<pre><code>Temp  ~ Time + Position + (1|Piece)
</code></pre>

<p>The problem I see here is that in such a model the temperature variation could be wrongly attributed to being a different piece instead of being due to the time.</p>

<p>PD:In fact there are not just one piece per time, it could be up to 3, but I've simplified the problem. </p>

<p>REPLIES:</p>

<p>The same piece is never measured at two time points. True.</p>

<p>The research question is:  How the temperature varies along the position and time (in a linear model).</p>

<p>The pieces start at the same temperature at time zero, and their temperature varies along time, I don't know how, I need to extract that information from the measures.
They cool without independently, without contact, but are supposed to cool on the same way.</p>

<p>In fact my model is not exactly this one but I've written the question like this because it's equivalent and I thought it could be easier.</p>

<p>In the real model I want to measure how different graftings degrade through time.  And I quantify it measuring their absortivity with a CT scan, taking readings at different positions on each grafting.</p>

<p>A simplified graphic:
<a href=""http://i.stack.imgur.com/MWe8H.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MWe8H.jpg"" alt=""enter image description here""></a></p>

<p>Each ball is measured once at different times.
Each ball is measured at several positions.</p>
"
"0.0326991257596857","0.0480754414848157","224896","<p>I am relative new in regression analysis. I would like to know, if there is a way in regression analyis to estimate the risk or calculate the risk for future values?</p>

<p>An example:</p>

<p>We want to predict the export of a country for next year. We can use linear regression to estimate the export value for the next year. This value may be influenced by another parameter for example Weather catastrophe. Is there any way to estimate such a risk in linear regrssion?</p>
"
"0.0849548207682898","0.0832691072480053","224947","<p>What are some of the best practices and steps to building models for prediction and or inferences? </p>

<p>What have been taught to me during my classes was the steps outlined in Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates"". The method to screen a large dataset with many potential predictors is to use a algorithmic approach such as Stepwise, best subset regression, etc. Then verify the model after the fact for potential collinearity, confounders, etc.  However, I have read much criticism on this site in regards to those said steps and methods.</p>

<p>For example - if I was provided a dataset with ~100 potential predictors, what would be the best practice to selecting those said predictors for inclusion or exclusion of the model for prediction/inference ? </p>

<p>According to Hosmer et al., the steps would be to perform univariate analysis to screen for all of those potential predictors (p &lt; .25), then move to inclusion of those said predictors to a multivariate model. Take a stepwise approach to removing insignificant predictors, then add back and verify the significance of each non significant predictor. </p>

<p>However - the more I've read on this site the more confused I've gotten about what is considered best practices, and I've come to question more and more of what was taught during my classes.</p>

<p>Once again just to reiterate - </p>

<ol>
<li><p>What would be the best practices for building a model for obtaining unbiased measure of association for each individual predictors?</p></li>
<li><p>What would be the best practices for building a model strictly for prediction?</p></li>
</ol>

<p>I'm still learning much about the world of data science and appreciate any help that is provided!</p>

<p>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</p>
"
"NaN","NaN","224988","<p>I have a simple linear DiD model for patent count data
$$ 
patent_{ut}=\beta_0 +\beta_1 grant_u +\beta_2ptreatment_t +\beta_3 interaction_{ut}+error_{ut}
$$
where u denotes a university and t denotes year.
However, because I'm using patent count data, literature says I should consider a Quasi-Maximum Likelihood Poisson Regression or a Negative Binomial Regression and I was wondering how I can do that combined with a difference-in-difference in R.</p>

<p>Thank you so much</p>
"
"0.02831827358943","0.0277563690826684","225713","<p>Let's say that I have a full and a restricted model that looks like this:</p>

<pre><code>Full&lt;- polr(Y ~ X1+X2+X3+X4, data=data, Hess = TRUE,  method=""logistic"")

Restricted&lt;- polr(Y ~ X1+X2+X3, data=data, Hess = TRUE,  method=""logistic"")
</code></pre>

<p>I want to conduct F-tests to determine whether the information from the <code>X4</code>variable statistically improves our understanding of Y. </p>

<p>What command is convenient for carrying out this test for logistic regression? Is it <code>aov()</code>? 
For example: </p>

<pre><code>summary(aov(Y ~ X1+X2+X3+X4)) #Full model
summary(aov(Y ~ X1+X2+X3)) #Restricted model
</code></pre>

<p>In linear regression case this would be the way to do it, I am not sure for ordered logistic regression...</p>
"
"0.02831827358943","0.0277563690826684","226849","<p>How can I find the function 'petest' in R? </p>

<p>A little background, I want to compare two different #regressions, 
y=x1+x2
logy=logx1+logx2</p>

<p>It is clear to me that I cannot use the BIC or AIC values, because the outcome variable is different. I searched in verbeek 2008 (A guide to modern Econometrics) and he suggests the PE test to compare linear and loglinear models.</p>

<p>I searched on this website <a href=""http://artax.karlin.mff.cuni.cz/r-help/library/lmtest/html/petest.html"" rel=""nofollow"">http://artax.karlin.mff.cuni.cz/r-help/library/lmtest/html/petest.html</a> and there I could find that the PE test exist in R, however, I cannot find it. I have the package lmtest, and 'petest' is supposed to be there, but when I try to use it, it says: Error: could not find function ""petest"".</p>

<p>Also, I have looked for more explanation, like examples, or videos that show how to apply this PE model, but I did not find any. Any help on this topic is highly appreciated!</p>
"
"0.0400480865731637","0.039253433598943","227073","<p>I have a small perplexity some of you might be able to help me with. 
I have fitted a linear model in R of the form</p>

<pre><code>fullmodel = lm(Y ~ X1 + X2)
</code></pre>

<p>and I want to obtain Likelihood Ratio Tests on the regression coefficients for <code>X1</code> and <code>X2</code>. 
One way to get them is using:</p>

<pre><code>anova(fullmodel, test=""LRT"")
</code></pre>

<p>But, in my understanding, if I use <code>anova</code> on the full model it removes covariates and performs LRT sequentially, indeed results differed depending on ordering of predictors.
<code>drop1</code>, on the other hand, drops one covariate at a time and leaves the rest untouched; thus I could use:</p>

<pre><code>drop1(fullmodel, test=""Chisq"")
</code></pre>

<p>This should work. Yet, out of curiosity, I also tried the following:</p>

<pre><code>fullmodel = lm(Y ~ X1 + X2)
reducedmodel1 = lm(Y ~ X1)
reducedmodel2 = lm(Y ~ X2)

anova(fullmodel, reducedmodel1, test=""LRT"")
anova(fullmodel, reducedmodel2, test=""LRT"")
</code></pre>

<p>In my understanding, the two procedures (<code>drop1</code> and the two separate <code>anova</code>) have identical meaning and should give exactly the same p-values. That's not the case, though; they differ already at the 3rd decimal number. 
Can anyone explain to me why this happens? Am I doing something wrong?</p>
"
"0.09392108820677","0.0920574617898323","228238","<p>I'm having a strange problem running a meta-regression using the function <code>rma.mv()</code> in the 'metafor' package in R.</p>

<p>Since some of my data are from multiple-endpoint studies, I have calculated the variance-covariance matrix so that correlations between outcomes are taken into account. I'm also using random effects at study and treatment level. As far as I'm aware, I have now covered all issues with regard to dependent effect sizes.</p>

<p>The model looks like this:</p>

<pre><code>cov_mod &lt;- rma.mv(Hedges_g, cov, mods = ~ days, random = ~ treatment | study, data = rev)
</code></pre>

<p>When running the code, it gives this error message:</p>

<pre><code>Error in rma.mv(Hedges_g, cov9, mods = ~days, random = ~1 | treatment/study,  : 
  Error during optimization.
In addition: Warning message:
In rma.mv(Hedges_g, cov9, mods = ~days, random = ~1 | treatment/study,  :
  V appears to be not positive definite.
</code></pre>

<p>I have discovered that the problem lies with one particular study (9 effect sizes in total, coming from 3 treatment groups that were each tested at 3 moments in time). When I remove this study from the data set, the code runs without problem.</p>

<p>Thus, apparently this particular study causes the matrix to be 'not positive definite'. I have read that this likely means that ""at least one of [the] variables can be expressed as a linear combination of the others"" (<a href=""http://stats.stackexchange.com/questions/30465/what-does-a-non-positive-definite-covariance-matrix-tell-me-about-my-data"">source</a>).</p>

<p>However, here comes the strange thing: I have replaced all values in the variance-covariance matrix relating to this particular study with random numbers between 0-1 (maintaining the symmetry), and the error message remains unchanged. I am puzzled, because the matrix can no longer be linearly predictable if it contains random numbers.</p>

<p>What could be the issue?</p>
"
"0.02831827358943","0.0277563690826684","228351","<p>As the titles states, I would like to compare two coefficients in my multiple regression model but I'm not quite sure how.</p>

<pre><code>Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       68.9483    29.7439   2.318 0.024493 *  
Shots.PG          -0.5074     1.4696  -0.345 0.731334    
Shots.OT.PG        7.4992     3.1410   2.388 0.020707 *  
Dribbles.PG        0.6081     0.8121   0.749 0.457401    
Fouled.PG         -0.9856     0.8783  -1.122 0.267031    
Offsides.PG        1.0520     3.0728   0.342 0.733477    
Tackles.PG         0.2705     0.6721   0.402 0.689016    
Fouls.PG          -0.4230     0.7893  -0.536 0.594329    
Ints.PG            0.3414     0.5962   0.573 0.569451    
Shots.Allowed.PG  -3.3604     0.8063  -4.167 0.000119 ***
</code></pre>

<p>Above are the results I've obtained. At first glance I thought it was interesting Shots OT has double the impact of Shots Allowed but I see that their standard errors are significantly different so that worries me.</p>

<p>How would I go about comparing these two values?</p>

<p>Using linear.hypothesis() I get:</p>

<p>Linear hypothesis test</p>

<pre><code>Hypothesis:
Shots.OT.PG  + 2 Shots.Allowed.PG = 0

Model 1: restricted model
Model 2: Points ~ Shots.PG + Shots.OT.PG + Dribbles.PG + Fouled.PG + Offsides.PG + 
    Tackles.PG + Fouls.PG + Ints.PG + Shots.Allowed.PG

  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
1     52 4488.5                           
2     51 4484.2  1    4.2107 0.0479 0.8277
</code></pre>

<p>How do I interpret this? Does this mean they are not different due to its large P Value. I am trying to find out whether or not the Shots OT has a larger effect on the Points total than the Shots Allowed PG</p>
"
"0.0853828074607","0.0836886016271203","228641","<p>Consider the following dataset I want to use as the independent variables to conduct linear regression on:</p>

<pre><code>set.seed(42)
sa = runif(10)
sb = runif(10)
sc = sb+sa
sd = sb-sa
df = data.frame(sa,sb,sc,sd)
</code></pre>

<p>Now I want to perform tests for multicollinearity. I'm aware of the <code>ppcor</code> package, which calculates the partial correlation between the variables. In this case:</p>

<pre><code>&gt; pcor(df)
$estimate
            [,1]        [,2]       [,3]       [,4]
[1,]  1.00000000  0.06649968 -0.7325597  0.7706902
[2,]  0.06649968  1.00000000 -0.6304810 -0.6870502
[3,] -0.73255975 -0.63048097  1.0000000  0.1308260
[4,]  0.77069021 -0.68705016  0.1308260  1.0000000
</code></pre>

<p>As far as I know, there is no way of telling that <code>sc</code> and <code>sd</code> are linear combinations of <code>sa</code> and <code>sb</code>, just by looking at the estimates (or the other outputs of <code>pcor</code>, for that matter).</p>

<p>The only method that comes to my mind, is applying linear regression on each of the independent variables like so:</p>

<pre><code>summary(lm(sc~sa+sb,df))

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 1.404e-16  3.915e-17 3.587e+00   0.0089 ** 
sa          1.000e+00  4.716e-17 2.121e+16   &lt;2e-16 ***
sb          1.000e+00  4.135e-17 2.418e+16   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.672e-17 on 7 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 

summary(lm(sd~sa+sb,df))

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 1.404e-16  3.915e-17 3.587e+00   0.0089 ** 
sa          1.000e+00  4.716e-17 2.121e+16   &lt;2e-16 ***
sb          1.000e+00  4.135e-17 2.418e+16   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.672e-17 on 7 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 
</code></pre>

<p>I'm wondering two things: </p>

<ol>
<li><p>Is my approach with linear regression reasonable? I think the downside is, that it can only detect linear correlation. But non-linear correlation shouldn't be a problem with linear regression, right?</p></li>
<li><p>Is there an R function/package that automatically checks for multiple correlation?</p></li>
</ol>
"
"0.0400480865731637","0.0196267167994715","229286","<p>So, I did do a search already and came across <a href=""http://stats.stackexchange.com/questions/60777/what-are-the-assumptions-of-negative-binomial-regression"">this</a> response but, his explanation went over my head a bit. The research i've done online hasn't been any more helpful. I've used this code,</p>

<pre><code>m3 &lt;- glm(daysabs ~ math + prog, family = ""poisson"", data = dat)
X2 &lt;- 2 * (logLik(m1) - logLik(m3))
</code></pre>

<p>to find out whether or not the Poisson model is more applicable but i'm not sure what to do if the value of X2 is close to 0. Also, in the link, he mentions, </p>

<pre><code>Linearity: The model is still linear in the parameters 
(i.e. the linear predictor is XÎ²), but the expected response
is not linearly related to them (unless you use the identity link function!).
</code></pre>

<p>but I'm not sure what that means. Should I be looking at the linearity between Y vs. X? or the Residuals of the regression model vs. X?</p>

<p>Please advise.</p>
"
"0.0490486886395286","0.0480754414848157","229709","<p>I have several slightly related variables measured in two instruments on the same sample at different time points. I'm trying to know how well the differences between the two instruments can be explained by other variables and the time point.</p>

<p>1.- I'd like to know if my method is adequate: I have chosen to perform a multivariate regression (with lm or glm) that includes a ""day"" as a polinomial variable and the average of both instruments (ex. AVGvar2) and differences between them (ex. DIFvar2) as linear variables. And I perform a StepAIC on the fit. All this for each variable (so that I eventually can say <strong>what variables significantly influde in observed differences</strong> for each variable).</p>

<pre><code>fit &lt;- glm(var1dif~var1+AVGvar2+DIFvar2+AVGvar3+DIFvar3+AVGvar4+DIFvar4+AVGvar5+DIFvar5+day+I(day^2)+I(day^3)+I(day^4)+I(day^5)+I(day^5); step1 &lt;-stepAIC(fit,direction=""both"")
fit2 &lt;- lm(var1dif~var1+AVGvar2+DIFvar2+AVGvar3+DIFvar3+AVGvar4+DIFvar4+AVGvar5+DIFvar5+poly(day,6); step2 &lt;-stepAIC(fit2,direction=""both"")
</code></pre>

<p>(the same would go for DIFvar2, DIFvar3, DIFvar4 and DIFvar5)
Any correction, advice or further step?</p>

<p>2.- When I compare summary(step) and summary(step2) the output (estimates, std error, coefficients) is the same for the variables. The results for variable day differ when the linear model has orthogonal or raw polinomials. Which one is better for my regression?</p>

<p>NOTE: I understand stepwise is frowned upon, but I think for this retrospective analysis it is decent and cost-effective enough.</p>

<p>Thank you.</p>
"
"0.0800961731463273","0.0686935087981502","229905","<p>I have read <a href=""http://stats.stackexchange.com/questions/145870/how-to-fit-an-exponential-equation-of-the-form-y-a-becx-to-data"">this article</a> and those linked to it, but I am still having difficulties fitting a function of this form to data I have using the nls function in R. Invariably, I fail to get convergence regardless of what starting values I choose for <code>A</code> through <code>C</code>.</p>

<p>Here is a plot of all six of the relationships I would ultimately like to fit:
<a href=""http://i.stack.imgur.com/JKmq3.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JKmq3.jpg"" alt=""enter image description here""></a></p>

<p>While <code>VEG.STM3</code> and <code>PROP.RIPE</code> probably will conform to a curve of this shape fairly well, the others probably won't, and that's fine. I still have a justifiable reason for trying it.</p>

<p>My questions are as follows:</p>

<ol>
<li>What are the parameters <code>A</code>, <code>B</code>, and <code>C</code> doing, mathematically, in a function of this form? <code>A</code> appears to be the Y asymptote, <code>B</code> seems to be a scalar of some kind, and <code>C</code> seems to control the rate of decay, but, beyond that, I can't seem to figure out what a reasonable range of values for each parameter should be, especially from one set of data to the next.</li>
<li>Is there a way to ""linearize"" this problem so that <code>lm</code> can be used in place of <code>nls</code>? My understanding is that this function is equivalent to <code>ln(Y) ~ ln(A) + ln(B) + CX</code>, but I don't understand how to fit that equation any more than I understand how to fit this current curve. </li>
<li>Is there any way to get <code>nls</code> to be less fussy and more robust to lousy starting guesses?</li>
<li>Once I get <code>nls</code> to run successfully, how can I compare the fit of the model to one fit by simple linear regression? Would use of AICc be appropriate in that case? </li>
</ol>

<p>Here is some data to work with:</p>

<pre><code>PROP.RIPE = c(0.37, 0.223, 0.223, 0.224, 0.388, 0.413, 0.406, 0.422, 0.554, 
0.453, 0.569, 0.511, 0.13, 0.166, 0.16, 0.216, 0.297, 0.344, 
0.339, 0.292, 0.601, 0.535, 0.65, 0.535, 0.269, 0.238, 0.334, 
0.272, 0.523, 0.358, 0.449, 0.393, 0.458, 0.426, 0.576, 0.468, 
0.581, 0.579, 0.527, 0.568, 0.348, 0.313, 0.317, 0.267, 0.623, 
0.527, 0.646, 0.589, 0.488, 0.444, 0.498, 0.449, 0.109, 0.103, 
0.171, 0.153, 0.505, 0.343, 0.345, 0.213, 0.029, 0.011, 0.071, 
0.013, 0.697, 0.604, 0.624, 0.639, 0.386, 0.508, 0.38, 0.471, 
0.618, 0.488, 0.513, 0.485, 0.602, 0.597, 0.625, 0.495, 0.318, 
0.457, 0.423, 0.547, 0.88, 0.949, 0.912, 0.771, 0.628, 0.635, 
0.486, 0.567, 0.621, 0.549, 0.698, 0.709, 0.541, 0.563, 0.789, 
0.692, 0.525, 0.395, 0.449, 0.597, 0.57, 0.487, 0.556, 0.546, 
0.495, 0.617, 0.754, 0.71, 0.585, 0.719, 0.508, 0.536, 0.592, 
0.472, 0.481, 0.658, 0.937, 0.853, 0.981, 0.887)

NODES0 = c(124L, 362L, 198L, 343L, 152L, 193L, 98L, 167L, 148L, 284L, 
113L, 137L, 227L, 323L, 156L, 362L, 166L, 327L, 137L, 312L, 166L, 
350L, 97L, 222L, 182L, 456L, 143L, 277L, 172L, 272L, 110L, 184L, 
138L, 288L, 102L, 124L, 236L, 280L, 159L, 127L, 104L, 176L, 93L, 
167L, 178L, 400L, 126L, 248L, 189L, 336L, 181L, 304L, 245L, 283L, 
151L, 327L, 116L, 179L, 144L, 177L, 397L, 642L, 322L, 443L, 125L, 
249L, 100L, 144L, 56L, 22L, 23L, 17L, 252L, 387L, 184L, 308L, 
115L, 267L, 82L, 157L, 223L, 226L, 79L, 73L, 101L, 139L, 104L, 
60L, 200L, 164L, 66L, 49L, 173L, 204L, 64L, 107L, 435L, 215L, 
51L, 129L, 392L, 550L, 174L, 178L, 276L, 204L, 98L, 74L, 421L, 
303L, 126L, 150L, 168L, 195L, 77L, 75L, 72L, 142L, 59L, 47L, 
391L, 479L, 109L, 111L)

NODE.SUCCESS = c(1.05, 0.79, 0.86, 0.69, 0.85, 0.8, 0.77, 0.84, 0.53, 0.88, 
0.88, 0.79, 0.85, 0.88, 1, 1.03, 0.95, 0.77, 0.92, 0.73, 0.98, 
0.92, 0.97, 0.99, 0.88, 0.82, 0.84, 0.78, 0.63, 0.54, 0.54, 0.47, 
1.09, 0.88, 0.95, 0.99, 0.96, 1.13, 0.91, 1.01, 0.81, 0.89, 0.99, 
0.85, 0.95, 0.65, 0.87, 0.73, 0.64, 0.82, 0.82, 0.75, 0.93, 1.06, 
0.94, 0.89, 0.65, 0.53, 0.6, 0.62, 0.91, 0.89, 0.93, 1.13, 0.81, 
0.63, 0.75, 0.67, 0.93, 0.82, 0.7, 0.88, 0.8, 0.96, 0.9, 0.94, 
0.58, 0.66, 0.65, 0.63, 0.66, 0.62, 0.66, 0.77, 0.51, 0.6, 0.47, 
0.9, 0.69, 0.73, 0.59, 0.63, 0.97, 0.93, 0.95, 0.91, 0.81, 0.92, 
0.88, 1.1, 0.56, 0.57, 0.44, 0.51, 0.85, 0.83, 0.96, 0.85, 1, 
0.97, 0.94, 0.95, 0.61, 0.71, 0.73, 0.8, 1.06, 0.96, 0.9, 0.91, 
0.45, 0.41, 1.3, 0.55)
</code></pre>
"
"0.0849548207682898","0.0832691072480053","230123","<p>I'm trying to find the maximum output for a quite complex function in R. The input of the function requires 4 parameters: <code>x1, x2, x3, x4</code>, the output is a single value.</p>

<p>The constraints are : </p>

<pre><code>5 &lt; x1 &lt; 15
0 &lt; x2 &lt; 89
0 &lt; x3 &lt; 89
0 &lt; x4 &lt; 89
x3 &lt; x4
</code></pre>

<p>Now what I did is quite dumb and slow: run all the possible combinations through the function and find the maximum value, it works but takes 3-4 hours to ""search"" the maximum output of the function.</p>

<p>I believe there is a better way to optimize this. I tried run linear regression using the output against all input variables but can't see any good explanation(R-squared &lt; 0.1). I manually change each parameter to see the output but the changes of the output is quite unpredictable to me. I tried using symbolic regression to find a formula for the function so that I can find the cost function later but not working. I also checked some optimization functions in R such as <code>optim(c(5,1,1,2),findVar,lower = c(5,1,1,2), upper = c(15,89,89,89))</code> but looks like they were just not working as well (Error Message:<code>Your panel, as described by unit.variable and time.variable, is unbalanced. Balance it and run again.</code>). </p>

<p>I'm a newbie to optimization problems so any good advices will be really appreciated !</p>
"
"0.07684452671463","0.0836886016271203","230201","<p>I'm using the glmnet package in R to do ridge regression. When I have a full set of dummy variables (if you took a horizontal sum of all these dummy variables you would get the constant), ridge regression with lambda = 0 is NOT dropping any of the dummy variables. In contrast, OLS gives the expected result by dropping at least 1 of the dummies to prevent perfect multi-collinearity. I'd like to know why the discrepancy exists. </p>

<pre><code> library(glmnet)
 set.seed(1)
make_dummies_out_of_factors&lt;- function(your_df, names_of_factor_variables) {
  indices&lt;- which(names(your_df) %in% names_of_factor_variables) #Finds columns corresponding to factor variables
  model_matrices_list&lt;- lapply(indices, function(x) {
    model.matrix(~your_df[,x] - 1, your_df)
  })
  #create a model matrix for each factor variable, and stores each one as a list
  model_matrices_together&lt;- do.call(cbind, model_matrices_list)
  #Column bind all model matrices which are stored as lists
  final&lt;- cbind(your_df, model_matrices_together)
  #Column bind all the model matrices to the original data
  final&lt;- final[,-indices]
  #Get rid of the original factor variables

  names(final)&lt;- gsub(""your_df.*\\]"", ""dummy_"", names(final))
  #Give appropriate names to the dummies

  return(final)
}
test_df&lt;- data.frame(numeric1 = rnorm(1000), numeric2 = rnorm(1000), 
                     state = rep(letters[1:4], 250), year = rep(c(""yr1"", ""yr2""), 500)) #This data frame has 2 factor variables
test_df&lt;- make_dummies_out_of_factors(test_df, names_of_factor_variables = c(""state"", ""year""))

linear_alldum&lt;- lm(test_df$numeric2 ~ test_df$numeric1 + test_df$dummy_yr1 + test_df$dummy_yr2 + test_df$dummy_a + 
                     test_df$dummy_b + test_df$dummy_c + test_df$dummy_d)


X_test&lt;- as.matrix(test_df[,-1]) #Remove dependent variable out of X matrix
y_test&lt;- test_df[,1] #This is the dependent variable

ridge_alldum&lt;- glmnet(x = X_test, y = y_test, lambda = seq(200, 0, by = -1), alpha = 0)


comparison = data.frame(as.matrix(coef(ridge_alldum))[,201], coefficients(linear_alldum))
names(comparison)[1]&lt;- ""coefficients_ridge_l0""
names(comparison)[2]&lt;- ""coefficients_linear_reg""
#Note that coefficients aren't identical, and that ridge regression doesn't drop coefficients. 

prediction_linear&lt;- predict(linear_alldum)
prediction_ridge&lt;- predict(ridge_alldum, newx = X_test, s = 0)
predictions&lt;- data.frame(prediction_linear, prediction_ridge = prediction_ridge)
names(predictions)[2]&lt;- ""prediction_ridge""

#Note that the predictions using linear regression and ridge regression aren't the same. 

sapply(predictions, mean) #Means of predictions using linear and ridge.
sapply(predictions, sd) #SDs of predictions using linear and ridge. 
</code></pre>
"
"0.0566365471788599","0.0555127381653369","230257","<p>I am teaching myself regression using Regression Modeling Strategies by Harell and the author goes at quite the length to showcase the importance of modeling interactions and transformations of the initial variables. I can't help but wonder how to approach this in a more structured/automated way when dealing with a lot of potential variables. </p>

<p>Can we use recursive partitioning, for example, to somehow to do the work for us and then use the output as variables, shrink the estimates with LASSO to deal with colinearity and do a final step where we use some sort of filtering for feature importance. </p>

<p>In my mind this will leave us with a well specified model which can be manually inspected and improved if need be, but is this reasonable? Are there other ways to approach this? Are there some resources that deal with problems like this? </p>
"
"0.0490486886395286","0.0480754414848157","230303","<p>I am teaching myself regression using Regression Modeling Strategies by Harell and the author goes at quite the length to showcase the importance of modeling interactions and transformations of the initial variables. I can't help but wonder how to approach this in a more structured/automated way when dealing with a lot of potential variables.</p>

<p>Can we use recursive partitioning, for example, to somehow to do the work for us and then use the output as variables, shrink the estimates with LASSO to deal with colinearity and do a final step where we use some sort of filtering for feature importance.</p>

<p>In my mind this will leave us with a well specified model which can be manually inspected and improved if need be, but is this reasonable? Are there other ways to approach this? Are there some resources that deal with problems like this?</p>
"
"NaN","NaN","230521","<p>I have the typical linear regression model:</p>

<p>$$y_i = x_i^T\beta + e_i,$$
where $e_i\sim N(0,\sigma^2)$, iid. However, in my case, <em>some</em> (not all of them, only around 1/3 of them) response variables $y_i$ are right-censored. As far as I understand, I cannot apply LASSO (R package <code>lars</code>) directly. What alternative methods (I would be interested in a LASSO-type method) are there for selecting variables with right-censored outcomes? If they are implemented in R, much better.</p>
"
"0.0755153962384799","0.0832691072480053","230532","<p>Trying to get the Bayes Factor for a correlation between two variables in my data, I tried three different functions. All implement the Jeffreysâ€“Zellnerâ€“Siow (JZS) prior, but I get quite different results with the three approaches. Two questions:</p>

<ol>
<li><p>Is this suspicious, or is it reasonable that they produce different values, as the implementations are slightly different?</p></li>
<li><p>Is there a consensus on the best measure to use?  </p></li>
</ol>

<p>My data:</p>

<pre><code>a=rnorm(100,1,2)
b=rnorm(100,.8,1.5)
myData &lt;- data.frame(a=a, b=b)
</code></pre>

<p>I try the <code>jzs_corbf</code> function, described and implemented <a href=""http://www.ncbi.nlm.nih.gov/pubmed/22798023"" rel=""nofollow"">here</a> (<a href=""http://dsquintana.com/post/98962697485/how-to-calculate-a-bayes-factor-for-correlations"" rel=""nofollow"">shorter version</a>)</p>

<pre><code>cor.resu.a_b &lt;- cor.test(myData$a, myData$b, method=c(""pearson""))
cor.resu.a_b$estimate
n = 100
r = cor.resu.a_b$estimate
jzs_corbf(r,n)
[1] 0.08206358
</code></pre>

<p>I also tried the convenience function from the <code>BayesFactor</code> package:</p>

<pre><code>require(BayesFactor)
regressionBF(b ~ a, data = myData, progress=FALSE)

Bayes factor analysis
--------------
[1] a : 0.2181081 Â±0%

Against denominator:
  Intercept only 
---
Bayes factor type: BFlinearModel, JZS
</code></pre>

<p>And I also tried the a function described <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4891395/"" rel=""nofollow"">recently</a> (<a href=""https://osf.io/9d4ip/"" rel=""nofollow"">code</a>)</p>

<pre><code>bf10JeffreysIntegrate(n=100, r=r)

      cor 
0.1297927
</code></pre>

<p>While in this case the differences are only numerical, in my real data I get quite big differences that make it more difficult to decide on an interpretation. </p>

<p><a href=""http://stats.stackexchange.com/questions/184950/calculating-bayes-factor-from-a-correlation-coefficient"">Related</a></p>
"
"NaN","NaN","230556","<p>I have two vectors, <code>actual</code> and <code>predict</code> with the same length.</p>

<ol>
<li><p>Is  it possible to calculate <code>R-square</code> value for these two vectors in R, or it is only possible if <code>predict</code> comes from a linear regression?</p></li>
<li><p>If the answer is yes, what is difference between <code>R-square</code> and <code>RMSE</code>? What metric is better to measure performance of a predictor?</p></li>
</ol>

<p>Thank you very much,</p>
"
"0.0200240432865818","0.039253433598943","230760","<p>I need to do a multinomial logistic regression with a nominal variable, and I've heard about the Gmulti package in r ,and how it provides automatic selection methods models, However all the examples that i found are only applied on binaire logistic regression, so I wonder if it's even working on a multinomial logistic regression and in case is true, is the Gmulti take in consideration the problem of multicolinearity between the independents variables. Please help me thank you </p>
"
"0.0200240432865818","0.039253433598943","230797","<p>I am new to R coding and was hoping someone could help. Am trying to create a regression line where the dependent variable is a proportion (I only have the proportion, not the denominator and numerator). With it being a proportion a linear regression line isn't appropriate as it needs to sit between 0-1, I think a sigmoid shape would be best. So far I have had limited success with the Loess function, however ideally I want to be able to gain the coefficients and AIC from the regression. So far the best shape I have been able to obtain is using the binomial family in ggplot2, but I don't think this is an appropriate distribution.</p>

<p>I have attached my code, am wondering if anyone could suggest an improvement.</p>

<pre><code> c &lt;- ggplot(dat, aes(y=ITN_Coverage, x=Study_Date))
c + stat_smooth(method =""glm"",  method.args = list(family=""binomial""), size=0.5, col = ""black"") + geom_point(aes(color = Country)) + 
labs(title = ""Scatter plot: Insecticide treated net coverage against year"", x= ""Study date"", y= ""ITN coverage"")
</code></pre>
"
"0.0400480865731637","0.039253433598943","230851","<p>When we specify the â€œfamily=â€ argument inside glm() in R, how is the distribution being used to regress between the dependent and independent.?</p>

<p>In simple linear regression( lm() in R ) ,we simply calculate the mean of Y for each X and that becomes the predicted value. How different is this when we mention a family? </p>

<p>I do know that each distribution has few paramters that describes it ( mean, shape, scale etc). So how are they used to get predictions?</p>

<p>Bascially I would like to learn what does it mean to fit a distribution to a data.</p>

<p>Edit : to clarify the question</p>

<p>Lets say I trained my model on a dataset of 1lac obs with 2 independent variables and the coefficients are 1,2. i.e. beta1 = 1 and beta2 = 2</p>

<pre><code>          Y    x1   x2
1st obs.  2    .5    1
2nd obs.  1    .25  .25
</code></pre>

<p>So if I choose Poisson distribution, then</p>

<pre><code>1st obs. mean(mu) = exp(.5*1+1*2) = 12.18
</code></pre>

<p>similarily we get a mean for the second obs and so on. 
Now how is this mean related to what you gonna predict? I am not able to connect this. </p>

<p>Another major concern is also how the shape/scale i.e.(sigma, nu, tau for gamlss) being modeled. However they are secondary and for now wanna focus on glm My questions may be stupid but even any resources/links will be helpful and I shall sincerely read them </p>
"
"0.102102987459307","0.100077011948264","230911","<p>I'm not entirely sure of fitting the model for experiment we've made. The variables and relevant description are as follows:</p>

<ul>
<li>ID - participant ID </li>
<li>Trial - 60 for each participant</li>
<li>Memory - between subject binary factor</li>
<li>State - within subject binary factor  </li>
</ul>

<hr>

<ul>
<li>Correct - whether classification a participant made was correct or not</li>
<li>Rating - the judgement made after each trial on four point Likert scale</li>
</ul>

<p>Procedure brief: each participant (N=60) was randomly assigned to experimental or control group (Memory) and had 120 Trials (60 for State = 0 and 60 for State = 1). Each trial composed of perceptual classification (Correct) and judgment of how easy it was (Rating). The classification problem was randomly selected from two groups each trial (State).</p>

<p>I would like to calculate what impacts the performance (Correct) most - is it memory, state, a specific rating on a scale or any combination of above? I'm not interested in between subject variance, on the oposite, it is a random factor here. Also, it appears that there is bias in responses on Likert scales, so that part of variance should be excluded too. </p>

<p>The way I was thinking to approach this is generalized mixed linear model, but I'm not sure I'm doing it right; there is what I've got so far:</p>

<pre><code>model = glmer(Correct ~ (1|ID/Rating) + Memory * State * Rating, data, family=binomial, 
              control = glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun=100000)))
</code></pre>

<p>Is this approach correct? I'll appreciate your input.</p>

<p>Relevant resources I used: </p>

<ul>
<li><a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">Formulae in R: ANOVA and other models, mixed and fixed</a></li>
<li><a href=""http://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/"" rel=""nofollow"">The Difference Between Crossed and Nested Factors</a> </li>
<li><a href=""http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model"">When is it ok to remove the intercept in a linear regression model?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/225198/nested-random-factor-with-confounding-random-variable"">Nested random factor with confounding (random?) variable</a></li>
</ul>
"
"0.11741152443841","0.121138726036629","231066","<p>*EDIT: I ran test again with data set provided and realized that the cause of problem is definitely rank deficiency, because estimated values of parameters in nonlinear regression showed non existing p values and there was no way to create confidence intervals with this data. </p>

<h2>Thank you all for reading and help! This question is closed.</h2>

<p>I researched seed germination. I took 75 seed replicates and put them in  different ecological parameters (like temperature) and took data about sprouts in different time intervals. </p>

<p>Reading statistical science papers about this topic, I found that I should analyze my data in a time-to-event model (dose response curve), where I can use log-logistic regression or nonlinear regression (Ritz et al., 2013 -<a href=""http://dx.doi.org/10.1016/j.eja.2012.10.003"" rel=""nofollow"">http://dx.doi.org/10.1016/j.eja.2012.10.003</a>). </p>

<p>Two models (nonlinear and log-logistic) lead to quantitatively very similar fitted germination curves, i.e., similar parameter estimates, but qualitatively different statements about the precision of estimates. Nonlinear regression model yields an overly precise estimate of the proportion of seeds that germinated during the experiment, so the precision reported by the nonlinear regression is too high.</p>

<p>Similarly, the 95% confidence intervals of the fitted curves also demonstrate the dramatic difference in precision of the two models: Accurate prediction of germination percentages is not warranted by the data unless very low percentages are of interest.</p>

<p>Because of that I choose log-logistic regression as a model. First few data sets; treatments analyzed in R using analysis of Dose-Response Curves (drc package) went smooth, and I was able to plot and get final graph. Such data, which was successfully analyzed, contained treatments where max seeds germination was for example 50% of total seed number.</p>

<p>Example:</p>

<p><a href=""http://i.stack.imgur.com/yUqOu.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yUqOu.jpg"" alt=""Example of successful data analysis""></a></p>

<p>The problems arose when I entered the log-logistic model with treatment where all the seeds germinated in a short amount of time (meaning the treatment for this set of seeds is most adequate for their successful sprouting). For example, 100% of seeds germinated in only 5 days, so there are only two or three time intervals and a large number of sprouted seeds. The R program here reported  convergence error:</p>

<pre><code>Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
non-finite value supplied by optim
Error in drmOpt(opfct, opdfct1, startVecSc, optMethod, constrained, warnVal,  : 
Convergence failed 
</code></pre>

<p>Since I'm still a student in biology I have a very basic knowledge in statistics, so I tried to solve the problem with literature. </p>

<p>At first I thought that convergence failed because of perfect or complete separation, but through longer research it seems that the problem lies in rank deficiency. </p>

<p>When I analyzed the same data with nonlinear regression I've managed to fit curve and plot a graph without a problem.  </p>

<p>So, is there a way to make log-logistic model work even though I have obviously small data in cases of 100% germination? Should I switch to nonlinear regression  even though the reported precision would be too high. </p>
"
"0.0800961731463273","0.0686935087981502","231252","<p>I'm doing a regression using R.Initially I used the <code>fit=lm(data)</code>.Got all of my variables are significant including intercept.I checked the VIF using <code>vif(fit)</code> &amp; got maximum VIF as 2.5. But my customer wants model without intercept and I don't have any option other than removing intercept. So I used following line of code
<code>fit=lm(A ~ B+C+D+E+F-1,data=data)</code> , I'm just coding the variables as it is client data &amp; I can't share that.The data set I used in first model is same as the data set used in second model with same set of variables.Only in second model I removed intercept forcefully.
But after running the model I'm seeing my maximum vif is coming 2079.30. I'm not able to understand the reason for such high vif as I used to think that VIF determines how much the variance of a coefficient is â€œinflatedâ€ because of linear dependence with other predictors &amp; it does not depend on intercept.
Can you expert please help me understand why VIF is drastically changed after removing intercept in R</p>
"
"0.0817478143992143","0.0801257358080262","231632","<p>After answering to question <a href=""http://stats.stackexchange.com/q/231059/58675"">Compare the statistical significance of the difference between two polynomial regressions in R</a>, I realized that I have always assumed that <code>ggplot2</code> plots <a href=""https://en.wikipedia.org/wiki/Confidence_and_prediction_bands"" rel=""nofollow"">simultaneous confidence bands</a>, not pointwise confidence bands, without actually knowing that for sure. I asked on SO: <a href=""http://stackoverflow.com/q/39110516/1711271"">http://stackoverflow.com/q/39110516/1711271</a>. I got an interesting answer, which I tried to apply. Results however can be weird:</p>

<pre><code>library(dplyr)
# sample datasets
setosa &lt;- iris %&gt;% filter(Species == ""setosa"") %&gt;% select(Sepal.Length, Sepal.Width, Species)
virginica &lt;- iris %&gt;% filter(Species == ""virginica"") %&gt;% select(Sepal.Length, Sepal.Width, Species)

# compute simultaneous confidence bands
setosa &lt;- setosa %&gt;% arrange(Sepal.Length)
virginica &lt;- virginica %&gt;% arrange(Sepal.Length)

# 1. compute linear models
Model &lt;- as.formula(Sepal.Width ~ poly(Sepal.Length,2))
fit1  &lt;- lm(Model, data = setosa)
fit2  &lt;- lm(Model, data = virginica)
# 2. compute design matrices
X1  &lt;- model.matrix(fit1)
X2  &lt;- model.matrix(fit2)
# 3. general linear hypotheses
cht1 &lt;- multcomp::glht(fit1, linfct = X1) 
cht2 &lt;- multcomp::glht(fit2, linfct = X2) 
# 4. simultaneous confidence bands (finally!)
cc1 &lt;- confint(cht1); cc1 &lt;- as.data.frame(cc1$confint)
cc2 &lt;- confint(cht2); cc2 &lt;- as.data.frame(cc2$confint)
setosa$LowerBound &lt;- cc1$lwr
setosa$UpperBound &lt;- cc1$upr
virginica$LowerBound &lt;- cc2$lwr
virginica$UpperBound &lt;- cc1$upr

# combine datasets
mydata &lt;- rbind(setosa, virginica)

# plot both simultaneous confidence bands and pointwise confidence
# bands, to show the difference
library(ggplot2)
# prepare a plot using dataframe mydata, mapping sepal Length to x,
# sepal width to y, and grouping the data by species
ggplot(data = mydata, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + 
# add data points
geom_point() +
# add quadratic regression with orthogonal polynomials and 95% pointwise
# confidence intervals
geom_smooth(method =""lm"", formula = y ~ poly(x,2)) +
# # add 95% simultaneous confidence bands
geom_ribbon(aes(ymin = LowerBound, ymax = UpperBound),alpha = 0.5, fill = ""grey70"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/giSOG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/giSOG.png"" alt=""enter image description here""></a></p>

<p>Questions:</p>

<ol>
<li>How would you plot the simultaneous confidence bands? Using ribbons with some transparency sort of does the job, but I'd rather not have the colored contours around the ribbons.</li>
<li>Why both the upper and lower boundary of the <code>setosa</code> ribbon are so smooth, while the upper bound of the <code>virginca</code> ribbon is so jagged? I would expect simultaneous confidence bands to be ""hyperbolic"" bands around the regression curve, thus very smooth. Am I computing the right thing here?</li>
</ol>

<p>PS just for the sake for clarity, I'm not interested in <strong>prediction</strong> bands. Here the focus is on <strong>simultaneous confidence bands</strong> and <strong>pointwise confidence bands</strong>.</p>
"
"0.0942489115008991","0.100077011948264","231913","<p>I am using quantile regressions in R, using <code>quantreg</code>. I want to use locally linear fitting as per the example on page 13 of <a href=""http://www.econ.uiuc.edu/~roger/research/rq/vig.pdf"" rel=""nofollow"">the vignette</a>. I have a response variable and am looking at the relationships of various predictors. However, running the regressions with some of the predictors result in a singular design matrix.</p>

<p>The response variable is:</p>

<p><code>y  &lt;-  c(21, 1, 0, 0, 3, 0, 13, 6, 7, 3, 0, 5, 5, 3, 2, 10, 10, 4, 3, 9, 0, 0, 4, 9, 6, 1, 1, 8, 8, 2, 2, 8, 9, 2, 0, 0, 0, 6, 1, 9, 17, 14, 2, 13, 30, 4, 4, 7, 2, 5, 0, 15, 9, 4, 3, 5, 2, 1, 8, 1)</code></p>

<p>Two examples of predictor variables are:</p>

<p><code>x1  &lt;-  c(7, 7, 9, 8, 8, 8, 8, 7, 8, 6, 6, 7, 10, 9, 8, 7, 8, 7, 6, 6, 6, 7, 9, 9, 8, 7, 7, 7, 7, 5, 6, 7, 9, 9, 8, 8, 9, 8, 7, 7, 7, 8, 8, 7, 9, 7, 8, 7, 7, 7, 6, 8, 8, 8, 8, 9, 9, 8, 9, 8)</code></p>

<p><code>x2  &lt;-  c(241, 304, 263, 301, 257, 445, 332, 329, 330, 269, 324, 338, 315, 309, 320, 311, 227, 297, 246, 339, 424, 394, 289, 381, 362, 334, 409, 304, 301, 350, 288, 288, 298, 403, 415, 503, 452, 302, 347, 369, 492, 441, 443, 369, 449, 311, 289, 274, 361, 449, 502, 371, 373, 312, 380, 303, 294, 330, 303, 405)</code></p>

<p>A locally linear quantile regression using <code>fit1  &lt;-  lprq(x1, y, h=1,tau=.9)</code> works fine for <code>x1</code>. However, running <code>fit2 &lt;- lprq(x2, y, h=1,tau=.9)</code> results in the following message:</p>

<p><code>Error in rq.fit.br(wx, wy, tau = tau, ...) : Singular design matrix</code>.</p>

<p>I assumed that this was a result of repeated values, but I can't understand why <code>x1</code> works (with lots of repeated values) while <code>x2</code> doesn't work. I tried to get around this by using <a href=""http://stats.stackexchange.com/questions/78022/cause-of-singularity-in-matrix-for-quantile-regression"">this advice</a>, but without success.</p>

<p>Any suggestions as to:
1) what the problem is?; and
2) how to resolve this?</p>

<p><strong>Edit</strong>: I managed to run the quantile regression successfully after increasing the bandwidth until it no longer gave an error message. However, it would still be useful if someone could help with understanding why the singular design matrix error occurred.</p>
"
"0.02831827358943","0.0277563690826684","232327","<p>I have Two factor A and B where B is nested under A and count dependent variable (Y). I have used Poisson Regression Model in SPSS using following syntax command.</p>

<pre><code> *Generalized Linear Models. 
  GENLIN Y BY A B (ORDER=ASCENDING)
 /MODEL  A B(A) INTERCEPT=YES 
 DISTRIBUTION=POISSON LINK=LOG 
 /CRITERIA METHOD=FISHER(1) SCALE=1 COVB=MODEL MAXITERATIONS=100 MAXSTEPHALVING=5 PCONVERGE=1E-006(ABSOLUTE) SINGULAR=1E-012 ANALYSISTYPE=3(WALD)  CILEVEL=95 CITYPE=WALD LIKELIHOOD=FULL 
 /PRINT CPS DESCRIPTIVES MODELINFO FIT SUMMARY SOLUTION.   
</code></pre>

<p>I found Factor A insignificant But B(A) significant. 
Please Guide me </p>

<ol>
<li>Can I interpret this B(A)factor? and</li>
<li>If Yes, How can I develop pairwise comparison?</li>
<li>Is there some batter option in R language? </li>
</ol>
"
"0.0749231094763201","0.0734364498908627","232450","<p>We are developping a software that run hierarchical linear model in R with the lme4 package. The model we are trying to fit is of the following shape:</p>

<pre><code>&gt; data
ID | Dummy_1 | Dummy_2 | Dummy_3 | Rating 
1  | 0       | 1       | 1       | 14
1  | 1       | 0       | 1       | 15
1  | 0       | 1       | 0       | 11
2  | 1       | 0       | 1       | 15
2  | 1       | 0       | 0       | 12

x = lmer(formula = Rating ~ Dummy_1 + Dummy_2 + Dummy_3 + (1 + Dummy_1 + Dummy_2 + Dummy_3 | ID), data = data)
</code></pre>

<p>It is important to note that this is the shape that the data will take, however, Rating can have very different range depending on the data te user provide.</p>

<p>The example above illustrate a limit case we are trying to deal with which occurs when the dummy variables perfectly predict the independant variable.</p>

<p>Here we can see that for example with <code>intercept = 10</code>, <code>B1 = 2</code>, <code>B2 = 1</code> and <code>B3 = 3</code> we perfectly predict the Rating variable. It implies first that the <code>ID</code> is useless and that we are in a case of <code>complete separation</code>.</p>

<p><strong>Question:</strong> How do you deal with (quasi-)complete separation when the independant variable is not binomial but continuous as it is the case here ? I could only find explanations for logistic regression. Please, ignore the fact that I use a linear regression for discrete-ordinal data and that I treat them as continuous :)</p>

<p>So that you know the warnings I get from R are the following:</p>

<pre><code>1: In optwrap(optimizer, devfun, getStart(start, rho$lower,  ... :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  unable to evaluate scaled gradient
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  Model failed to converge: degenerate  Hessian with 4 negative eigenvalues
</code></pre>
"
"NaN","NaN","233256","<p>I did a toy experiment with linear regression, but getting different results for $R^2$, could any one help me?</p>

<pre><code>library(ggplot2)
fit=lm(price~carat+depth+table+x+y+z-1,data=diamonds)
summary(fit)
sse=crossprod(diamonds$price-fit$fitted.values)
sst=crossprod(diamonds$price-mean(diamonds$price))
1-sse/sst
</code></pre>

<p><a href=""http://i.stack.imgur.com/nBOd0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nBOd0.png"" alt=""enter image description here""></a></p>
"
"0.0693653206906364","0.0679889413649005","233858","<p>I am unable to interpret this graph. My dependent variable is total number of movie tickets that will be sold for a show. The independent variables are the number of days left before the show, seasonality dummy variables (day of week, month of year, holiday), price, tickets sold till date, movie rating, movie type (thriller, comedy, etc., as dummies). Also, please note that movie hall's capacity is fixed. That is, it can host maximum of x number of people only. I am creating a linear regression solution and it's not fitting my test data. So I thought of starting with regression diagnostics. The data are from a single movie hall for which I want to predict demand.</p>

<p>The is a multivariate dataset. For every date, there are 90 duplicate rows, representing days before the show. So, for 1 Jan 2016 there are 90 records. There is a 'lead_time' variable which gives me number of days before the show. So for 1 Jan 2016, if lead_time has value 5, it means it will have tickets sold until 5 days before the show date. In the dependent variable, total tickets sold, I will have the same value 90 times.</p>

<p>Also, as a side remark, is there any book that explains how to interpret residual plot and improve model afterwards?</p>

<p><a href=""http://i.stack.imgur.com/Q0ZuC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Q0ZuC.png"" alt=""enter image description here""></a></p>
"
"0.0326991257596857","0.0320502943232105","233913","<p>I am aware of the theory of stochastic gradient descent, which is a faster way of developing linear regression. Through this we can have an '<em>optimized implementation</em>' of linear regression. There are similar techniques for non-parametric methods as well, which allows you to converge faster keeping in mind cost function.</p>

<p>I need suggestion for a book which has worked out implementations or examples of these type of optimized models with R/Python code or pseudo code. So that i can run sophisticated machine learning algorithms faster, without increasing my hardware further. I am open about increasing hardware though. What interests me is a faster implementation of techniques, so that i can use scalable implementations of machine learning algorithms for bigger data.</p>

<p>Thanks!!</p>
"
"NaN","NaN","234039","<p>I have estimated a linear regression model in R and have extracted the estimated coefficients into a row vector B = (B1, B2, B3, B4). I'm supposed to test the hypothesis H0:r'B=q where r=(0,1,10,1)' (' indicates transpose) and B=(B1, B2, B3, B4)' and q=100 (Î²1 is the intercept) and report the t-statistic for this test. I suspect that the purpose is to impose restrictions on the first model but I have no idea how to go about this and would much appreciate any direction or advice.</p>
"
"0.0578044339088637","0.0453259609099337","234538","<p>I am trying to combine two linear models (one linear-quadratic and one linear) into one unified model by means of piecewise regression. The tail of the lhs (linear-quadratic part) should continue to be the asymptote for the rhs (linear part). Here's <a href=""http://www.intechopen.com/source/html/45395/media/image4.png"" rel=""nofollow"">a link</a>! The piecewise function is,</p>

<p>$$y = ax + bx^2,\ x \lt x_t$$ and</p>

<p>$$y = cx + d,\ x \ge x_t$$</p>

<p>where $a, b, c, d$ and $x_t$ (a breakpoint) are parameters to be determined. This unified model should be compared with the linear-quadratic model for the whole range of $x$ by R.squared.adjusted as a measure of goodness of fit.</p>

<pre><code>&gt; y
[1] 1.00000 0.59000 0.15000 0.07800 0.02000 0.00470 0.00190 1.00000 0.56000 0.13000 0.02500 0.00510 0.00160 0.00091 1.00000 0.61000 0.12000
[18] 0.02600 0.00670 0.00085 0.00040
&gt; x
[1] 0.00  5.53 12.92 16.61 20.30 23.07 24.92  0.00  5.53 12.92 16.61 20.30 23.07 24.92  0.00  5.53 12.92 16.61 20.30 23.07 24.92
</code></pre>

<p>I'm after continuity of the first derivative and to find the parameters, including determining the breakpoint. Since i want continuity at $x=x_t$, I have rewritten the piecewise function to</p>

<p>$$y = ax + bx^2,\ x \lt x_t$$ and</p>

<p>$$y = ax_t + bx_t^2 + k(x - x_t),\ x \ge x_t$$</p>

<p>where $k$ is a constant. So my attempt goes as follows (assuming I have derived $x_t$ theoretically):</p>

<pre><code>I = ifelse(x &lt; xt, 0, 1)*(x - xt)
x1 = ifelse(x &lt; xt, x, xt)
mod = lm(y ~  x1 + I(x1^2) + I)
</code></pre>

<p>But the tail (asymptote) doesn't seem to be parallel to the linear part in the upper range...</p>
"
"0.0490486886395286","0.0480754414848157","234741","<p>I am looking for the equivalent of constrained nonlinear regression (CNLR command) in SPSS in R.</p>

<p>The constraints on the parameter needs to be a <em>function</em> rather than simply being that the parameters need to lie in certain region (thus having simple upper or lower bounds)</p>

<p>I have looked at the <em>nloptr</em> and <em>optim</em> packages but it appears not to be what I am looking for.</p>

<p>My SPSS syntax:</p>

<pre><code>MODEL PROGRAM a=0.21771052038064 b=0.69276310944875 c=1.3015054535771 
d=-0.95674643053895  .
COMPUTE PRED_ = a + b * X + c * X ** 2 + d * X ** 3 .
CONSTRAINED FUNCTION.
COMPUTE CONSTR1_= sin((a) + (b*(-1)) + 
            (c*(-1)**2) + (d*(-1)**3)  ) *
            cos((a) + (b*(-1)) + (c*(-1)**2) + (d*(-1)**3)  ) *
            ((b) + (2*c*(-1)) + (3*d*(-1)**2)  ) .
COMPUTE CONSTR2_= sin((a) + (b*(1)) + 
            (c*(1)**2) + (d*(1)**3)  ) *
            cos((a) + (b*(1)) + (c*(1)**2) + (d*(1)**3)  ) *
            ((b) + (2*c*(1)) + (3*d*(1)**2)  ) .

_set Printback off.
CNLR G
  /OUTFILE='TempFile'
  /PRED PRED_
  /BOUNDS CONSTR1_ &gt; 0;CONSTR2_ &gt; 0
  /SAVE PRED
  /CRITERIA STEPLIMIT 2 ISTEP 1E+20 .
</code></pre>

<p>a, b, c and d being my starting values, X my data points.</p>
"
"0.0578044339088637","0.0679889413649005","234947","<p>I'm looking to run a linear mixed effect model using lme4, where my dependent variable <code>one_syllable_words / total_words_generated</code> is a proportion and my random effect <code>(1 | participant_ID)</code> reflects the longitudinal nature of the design. Independent, fixed effect variables of interest include <code>age</code>, <code>group</code>, <code>timepoint</code>, and interactions between them. </p>

<p>I've come across two main ways to deal with the proportional nature of the DV:  </p>

<ol>
<li><p><strong>Standard logistic regression / binomial GLM</strong>  </p>

<p>In my scenario, I envision the lme4 equation looking like this:  </p>

<pre><code>glmer(one_syllable_words / total_words_generated ~ age + group +
timepoint + age:timepoint + age:group + timepoint:group + (1 |
participant_ID), family = ""binomial"", weights =
total_words_generated, data = mydat)  
</code></pre></li>
<li><p><strong>Beta regression</strong>  </p>

<p>I would apply a transformation to my DV <code>(DV * (n - 1) + .5)/ n)</code> so that it cannot equal 0 or 1. (There are a few instances where it equals zero, no instances where it equals one.)  </p></li>
</ol>

<p>I'm unclear whether logistic regression or beta regression is preferred in this example. My DV isn't a clear-cut case of successes and failures (unless we stretch the definition of ""success""), so I'm worried logistic regression might not be appropriate. However, I'm having trouble getting a firm grasp on beta regression &amp; all it entails. If beta regression is preferred:  </p>

<ol>
<li>Why is it preferred?  </li>
<li>What is it doing ""behind the scenes"" to the data?  </li>
<li>How can it be applied in R?  </li>
</ol>
"
"0.0800961731463273","0.0686935087981502","235272","<p>Given a multinomial logistic regression model with 4 independent variables, 4 relevant interactions and a dependent variable with 3 categorical outcomes, I wanted to test for linearity of the logit.</p>

<p>R told me, it is always a good idea to scale the independent variables to the range [0,1], so I did.</p>

<p>So when I wanted to test for linearity of the logit by including the interactions between each predictor and its natural log in the model, I found that two of them were significant, so I had to reject the hypothesis of linearity of the logit.</p>

<p>However, when I ran the same model without scaling my predictors to [0,1] (the original range is [0,1500]) p-values for the log interactions were > 0.8 suggesting that I don't have to reject the linearity of the logit assumption.</p>

<p>When I looked at the transforms in the different ranges, it made sense why the outcome would be different:</p>

<p><a href=""http://i.stack.imgur.com/WFEbW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WFEbW.png"" alt=""log(x)*x[0,1]""></a></p>

<p><a href=""http://i.stack.imgur.com/PiPOE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PiPOE.png"" alt=""log(x)*x[0,1000]""></a></p>

<p>So my question is, does the Box Tidwell test for linearity of the logit require predictors to be in the range [0,1]? If so, why is it so hard to find any mention of this on the internet? If not, what is a valid range for the test? Because the test-results obviously depend on the range.</p>

<p>Thank you very much for your help.</p>
"
"0.0400480865731637","0.039253433598943","235422","<p>I am migrating from Weka+Pentaho forecasting and I am trying to get a regression model working in R.</p>

<p>As for my data, it is a time series of network utilization (second column).  How to make sure that I make use of timestamp?</p>

<p>I know that discretization becomes inconsistent at one point but this should not be related to the problem. In Weka I realigned discretization to appear linear. Need to fix this in R if this will cause issues.</p>

<p>Here is two samples from a set of 22k+ records sitting in a list of two columns:</p>

<p><code>2015-12-21 04:11:56          87
 2015-12-21 04:16:56          82
 2015-12-21 04:21:56          76
 2015-12-21 04:26:56          88
 2015-12-21 04:31:56          83</code></p>

<p><code>21999 2016-03-03 23:03:16          59
22000 2016-03-03 23:08:16          51
22001 2016-03-03 23:13:16          58
22002 2016-03-03 23:18:16          42
22003 2016-03-03 23:23:16          56
22004 2016-03-03 23:28:16          53
22005 2016-03-03 23:33:16          61</code>
I suspect I may confuse dimensions here... </p>

<p>I succeeded with SVM and KNN models in Weka, So far, neither is working in R. Below is my attempt to predict with KKNN library which fails.</p>

<p><code>library(kknn)
 fit = train.kknn(utilization~., d)
 predict(fit)
</code></p>

<p>The snippet above fails with the following message:
<code>Error: C stack usage  7969804 is too close to the limit</code></p>

<p>I need some guidelines on R pipeline to get at least a rough model and produce a prediction of N points, and calculate prediction performance metrics.</p>
"
