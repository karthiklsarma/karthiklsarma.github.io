"V1","V2","V3","V4"
"0.06213697660012","0.0601929265428846","9512","<p>I have a time series data of 30 years and found that ARIMA(0,1,1) has best model among others. I have used the simulate.Arima (forecast package) function to simulate the series into the future.</p>

<pre><code>library(forecast)

series &lt;- ts(seq(25,55), start=c(1976,1))

arima_s &lt;- Arima(series, c(0,1,1))

simulate(arima_s, nsim=50, future=TRUE)
</code></pre>

<p>Later on, i have found the updated value of first forecasted year (i.e. series[31] &lt;- 65). Now i want to simulate the series with this updated value. I am wondering how to do this in R.</p>
"
"0.0878749550327494","0.0851256530758749","55168","<p>Hi all I'm trying to do one step ahead forecast. Lets say I have 1000 data and fit an ARIMA model with it and then I do a forecast for one period ahead. When I get more data I would like to forecast another step using the new data without having to reestimate all coefficients and so on...</p>

<p>This is my code but for some reason it's very slow for a bigger dataset and am not too sure that is doing what I want:</p>

<pre><code>set.seed(1234)
y=ts(log(35+10*rnorm(1000)))
set.seed(4567)
new.data=ts(log(35+10*rnorm(10)))

library(forecast)
model = auto.arima(y)

onestep.for=forecast(model,h=1)
for (i in 1:10) {
  data=c()
  data=c(y,new.data[1:i])
  newfit=Arima(data, model=model)
  forec=forecast(newfit,h=1)
  onestep.for=c(onestep.for,forec)
}
</code></pre>
"
"0.06213697660012","0.0601929265428846","226286","<p>I need to compute a seasonal ARIMA model and make forecasts using Kalman filter.<br>
I do not understand how to feed the output of SARIMA((p,d,q)(P,D,Q)s) to Kalman filter.</p>

<p>Also, there are many R packages such as ""dlm"", ""Stats"", ""FKF"", etc. Which one should I use?</p>

<pre><code> I know how to use SARIMA like this:
 k &lt;- arima(data, order = c(2, 1, 2),
      seasonal = list(order = c(1, 1, 1), period = 12),
      xreg = NULL, include.mean = T ) 
</code></pre>

<p>But, I do not understand how to use Kalman filter accurately. Is the following is correct? </p>

<pre><code>result &lt;- KalmanForecast(&lt;ahead&gt;, k$model)
</code></pre>

<p>Here, the issue is how the Kalman filter can handle the seasonality?</p>
"
"NaN","NaN","55937","<p>I have fitted an ARIMA(1,1,2) to time series <code>TS1</code> as below:</p>

<pre><code>arima112 &lt;- arima(TS1, c(1,1,2))
</code></pre>

<p>Now I want to use the coefficients of AR and MA that I got from <code>arima112</code> to forecast another time series (<code>TS2</code>). How can I apply the <code>arima112</code> model on <code>TS2</code>?</p>
"
"0.12427395320024","0.120385853085769","121298","<p>It seems that the <code>auto.arima</code> function in the ""forecast"" package in R only considers full ARIMA models. By ""full"" I mean that if an AR lag $k$ is included, AR lag $j$ will also be included for $k&gt;1$, $0&lt;j&lt;k$ (and the same with MA in place of AR). This was claimed <a href=""http://stats.stackexchange.com/questions/26668/subset-models-in-auto-arima-function-in-forecast-package"">here</a> by the author of the <code>auto.arima</code> function himself. </p>

<p>I am interested in non-full (restricted) ARIMA models, e.g. an AR(2) model where the first AR lag is restricted to zero: $x_t=\varphi_2 x_{t-2}+\varepsilon_t$.</p>

<p><strong>Question 1:</strong> Is there a good <strong>theoretical</strong> reason for not considering the non-full ARIMA models?</p>

<p><strong>Question 2:</strong> Is there a good <strong>practical</strong> reason for not considering the non-full ARIMA models? (Besides the argument of high computational burden if all sub-models within given maximum AR and MA orders are to be estimated.)</p>
"
"0.0439374775163747","0.0425628265379374","203105","<p>My apologies in advance for asking what I suspect is a dumb question. I have looked around and I can't figure this out.</p>

<p>I've done an auto.arima model in r.</p>

<pre><code>revenue = ts(arima$both.markets, frequency = 7)
    media=ts (arima$all_media, frequency = 7)
xreg &lt;- cbind(media=model.matrix(~as.factor(media)))
xreg &lt;- xreg[,-1]
modArima &lt;- auto.arima(revenue, xreg=xreg)
</code></pre>

<p>The output includes this:</p>

<pre><code>                ma1     ma2     sar1    (media)1    (media)2    (media)3    media)4 (media)5    (media)6    (media)7    (media)8    (media)9    (media)11   (media)13   (media)17   (media)18   (media)20   (media)23   (media)39
Coefficients:   -0.4081 -0.5391 0.6145  -8345.84    20129.82    1809.952    -14906.92   -42454.82   1885.815    -101350.54  12055.98    56197.28    -49130.22   128427.87   45600.38    -28911.02   46118.11    -95280.16   62833.34
s.e.            0.0791  0.0778  0.0718  11672.8     13384.69    21822.541   34298.06    23533.55    30755.171   35534.13    57394.97    44116.15    55263.58    55887.56    56920.24    60269.17    40252.73    49884.66    63023.11
</code></pre>

<p>Are the various <code>media</code> outputs lags of the media variable? If not, how can I include lags in the model?</p>
"
"NaN","NaN","203114","<p>I am trying to fit mortality data to best ARIMA model. What is the difference between function forecast.lca and auto.arima when choosing a model to fit. forecast.lca also includes random walk, but is there any other difference?</p>
"
"0.0878749550327494","0.0851256530758749","121882","<p>I am facing a strange issue with auto.arima. On a dataset named data, I run the following code</p>

<pre><code>auto.arima(data,d=0,D=1,xreg=1:length(data),max.p=3,max.q=3,max.order=10,
       seasonal=TRUE,stepwise=FALSE,approximation=FALSE,ic=(""aic""),parallel=TRUE)
</code></pre>

<p>The outcome is </p>

<pre><code>   Series: data 
   ARIMA(1,0,3)(2,1,2)[12]                    

 Coefficients:
        ar1      ma1      ma2     ma3    sar1     sar2     sma1    sma2  1:length(data)
        0.6939  -0.6417  -0.2391  0.4350  0.6197  -0.5055  -1.3211  0.6027   5e-04
  s.e.  0.1349   0.1654   0.1169  0.1502  0.3040   0.1762   0.3907  0.4269   3e-04

 sigma^2 estimated as 0.001792:  log likelihood=143.69
 AIC=-267.38   AICc=-264.79   BIC=-241.73
</code></pre>

<p>Then I try to fit this model using the Arima function:</p>

<pre><code> Arima(data,order=c(1,0,3),seasonal=list(order=c(2,1,2),period=12),
       xreg=1:length(data),method=""ML"")
</code></pre>

<p>but I get the following error message:</p>

<pre><code> Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
   non-finite finite-difference value [1]
</code></pre>

<p>Does someone understand why this appears ? Thanks.</p>
"
"0.0878749550327494","0.0851256530758749","179378","<p>I was thinking, is it possible to implement a quarterly forecast for one year ahead such that its sum over year equals some constant number?</p>

<p>This problem may arise if we have, for example, some external forecast over next year, and we need to produce a quarterly forecast that is consistent with the yearly one.</p>

<p>Theoretically, I can write down an ML-maximization problem, and write then some stack of code. But is there maybe some existing solutions?</p>

<p>Thanks!</p>
"
"NaN","NaN","97019","<p>We have three years of data for online visits at a daily level. We want to forecast the daily visits for the next 90 days. What would be the best method to capture weekday seasonality , holiday seasons, and also the drift.</p>

<p>Can this be successfully done in R? We are currently using R. We have considered ARIMA but it does not capture seasonality.</p>

<p>While converting the data to a time series in R,  what should be the ""frequency""?</p>

<p>Should we use ARIMA with regressors?</p>
"
"0.172336965564645","0.200334168988253","123723","<p>Can anyone tell me the formula behind the <code>forecast</code> function in R? Preferably in the form easily understood by mathematicians (e.g  x_t, Î¸ etc)</p>

<p>Here is my code in case it helps</p>

<pre><code>suppressMessages(library(lmtest))
suppressMessages(library(car))
suppressMessages(library(tseries))
suppressMessages(library(forecast))
suppressMessages(library(TTR))
suppressMessages(library(geoR))
suppressMessages(library(MASS))
suppressMessages(library(gtools))
#-------------------------------------------------------------------------------
Model &lt;- ""choosing ARIMA""
Series.title &lt;- ""EMEA GAM&lt;250K""
#-------------------------------------------------------------------------------
Input.data &lt;- matrix(c(""08Q1"",""08Q2"",""08Q3"",""08Q4"",""09Q1"",""09Q2"",""09Q3"",""09Q4"",""10Q1"",""10Q2"",""10Q3"",""10Q4"",""11Q1"",""11Q2"",""11Q3"",""11Q4"",""12Q1"",""12Q2"",""12Q3"",""12Q4"",""13Q1"",""13Q2"",""13Q3"",""13Q4"",""14Q1"",""14Q2"",""14Q3"",5403.675741,6773.504993,7231.117289,7835.55156,5236.709983,5526.619467,6555.781711,11464.72728,7210.068674,7501.610403,8670.903486,10872.93518,8209.022658,8153.393088,10196.44775,13244.50201,8356.732878,10188.44157,10601.32205,12617.82102,11786.52641,10044.98676,11006.0051,15101.9456,10992.27282,11421.18922,10731.31198),ncol=2,byrow=FALSE)

#-------------------------------------------------------------------------------
# The frequency of the data. 1/4 for QUARTERLY, 1/12 for MONTHLY

Frequency &lt;- 1/4

#-------------------------------------------------------------------------------
# How many quarters/months to forecast

Forecast.horizon &lt;- 4

#-------------------------------------------------------------------------------
# The first date in the series. Use c(8, 1) to denote 2008 q1

Start.date &lt;- c(8, 1)

#-------------------------------------------------------------------------------
# The dates of the forecasts

Forecast.dates &lt;- c(""14Q4"", ""15Q1"", ""15Q2"", ""15Q3"")

#-------------------------------------------------------------------------------
# Selects the data column from Input.data

Data.col &lt;- as.numeric(Input.data[, 2])

#-------------------------------------------------------------------------------
# Turns the Data.col into a time-series

Data.col.ts &lt;- ts(Data.col, deltat=Frequency, start = Start.date)

#-------------------------------------------------------------------------------
# A character vector of the dates from Input.data

Dates.col &lt;- as.character(Input.data[,1])

#------- Transform ------------------------------------------------------------------------
# Starts the testing to see if the data should be logged

transform.method &lt;- round(BoxCox.lambda(Data.col.ts, method = ""loglik""), 5)

log.values &lt;- seq(0, 0.24999, by = 0.00001)
sqrt.values &lt;- seq(0.25, 0.74999, by = 0.00001)

which.transform.log &lt;- transform.method %in% log.values
which.transform.sqrt &lt;- transform.method %in% sqrt.values

if (which.transform.log == ""TRUE""){
  as.log &lt;- ""log""
  Data.new &lt;- log(Data.col.ts)
} else {
  if (which.transform.sqrt == ""TRUE""){
    as.log &lt;- ""sqrt""
    Data.new &lt;- sqrt(Data.col.ts)
  } else {
    as.log &lt;- ""no""
    Data.new &lt;- Data.col.ts
  }
}

#----- Find best ARIMA model ---------------------------------------------------

a &lt;- permutations(n = 3, r = 6, v = c(0:2), repeats.allowed = TRUE)
a &lt;- a[ifelse((a[, 1] + a[, 4] &gt; 2 | a[, 2] + a[, 5] &gt; 2 | a[, 3] + a[, 6] &gt; 2),
              FALSE, TRUE), ]

Arimafit &lt;- matrix(0,
                   ncol  = length(Data.new),
                   nrow  = length(a[, 1]),
                   byrow = TRUE)

totb &lt;- matrix(0, ncol = 1, nrow = length(a[, 1]))
arimaerror &lt;- matrix(0, ncol = length(Data.new), nrow = 1)

for (i in 1:length(a[, 1])){
  ArimaData.new &lt;- try(Arima(Data.new,
                             order    = a[i, c(1:3)],
                             seasonal = list(order = a[i, c(4:6)]),
                             method   = ""ML""),
                       silent = TRUE)

  if (is(ArimaData.new, ""try-error"")){
    ArimaData.new &lt;- arimaerror
  } else {
    ArimaData.new &lt;- ArimaData.new
  }

  arimafitted &lt;- try(fitted(ArimaData.new), silent = TRUE)

  if (is(arimafitted, ""try-error"")){
    fitarima &lt;- arimaerror
  } else {
    fitarima &lt;- arimafitted
  }

  if (as.log == ""log""){
    Arimafit[i, ] &lt;- c(exp(fitarima))
    Datanew &lt;- c(exp(Data.new))
  } else {
    if (as.log == ""sqrt""){
      Arimafit[i, ] &lt;- c((fitarima)^2)
      Datanew &lt;- c((Data.new)^2)
    } else {
      Arimafit[i, ] &lt;- c(fitarima)
      Datanew &lt;- c(Data.new)
    }
  }

  data &lt;- c(Datanew)

  arima.fits &lt;- c(Arimafit[i, ])

  fullres &lt;- data - arima.fits

  v &lt;- acf(fullres, plot = FALSE)

  w &lt;- pacf(fullres, plot = FALSE)

  if (v$acf[2]&gt;0.4|v$acf[2]&lt;(-0.4)|v$acf[3]&gt;0.4|v$acf[3]&lt;(-0.4)|v$acf[4]&gt;0.4|v$acf[4]&lt;(-0.4)|v$acf[5]&gt;0.4|v$acf[5]&lt;(-0.4)|v$acf[6]&gt;0.4|v$acf[6]&lt;(-0.4)|v$acf[7]&gt;0.4|v$acf[7]&lt;(-0.4)|w$acf[1]&gt;0.4|w$acf[1]&lt;(-0.4)|w$acf[2]&gt;0.4|w$acf[2]&lt;(-0.4)|w$acf[3]&gt;0.4|w$acf[3]&lt;(-0.4)|w$acf[4]&gt;0.4|w$acf[4]&lt;(-0.4)|w$acf[5]&gt;0.4|w$acf[5]&lt;(-0.4)|w$acf[6]&gt;0.4|w$acf[6]&lt;(-0.4)){
    totb[i] &lt;- ""n""
  } else {
    totb[i] &lt;- sum(abs(w$acf[1:4]))
  }

  j &lt;- match(min(totb), totb)

  order.arima &lt;- a[j, c(1:3)]

  order.seasonal.arima &lt;- a[j, c(4:6)]
}

#----- ARIMA -------------------------------------------------------------------
# Fits an ARIMA model with the orders set
Arima.Data.new &lt;- Arima(Data.new,
                        order    = order.arima,
                        seasonal = list(order=order.seasonal.arima),
                        method   = ""ML"")

#-------------------------------------------------------------------------------
# Forecasts from the ARIMA model

suppressWarnings(forecast.Data.new &lt;- forecast(Arima.Data.new,
                                               h        = ifelse(frequency(Arima.Data.new) &gt; 1, 2 * frequency(Arima.Data.new), 10),
                                               simulate = TRUE,
                                               fan      = TRUE))
</code></pre>
"
"NaN","NaN","205055","<p>I am trying to follow the Arima Modelling Procedure outlined in Rob Hyndman's online text book, (link below) </p>

<p><a href=""https://www.otexts.org/fpp/8/7"" rel=""nofollow"">https://www.otexts.org/fpp/8/7</a></p>

<p>I'm using the data below.  I'm confused about step 4 since both my Acf and Pacf plots are sinusoidal.  Does that mean that either an AR(p) or MA(q) model is appropriate?</p>

<p>Code:</p>

<pre><code>Acf(tsTrain)
Pacf(tsTrain)
</code></pre>

<p>Data:</p>

<pre><code> dput(tsTrain)
</code></pre>

<p>c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 8.5, 4, 5, 9, 10, 11, 
7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 2, 15, 2.5, 17, 5, 5.5, 
7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 9.5, 3.5, 5, 4, 4, 9, 4.5, 
6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 12, 17.5)</p>
"
"0.30000221895953","0.30179373795668","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.117427848075255","0.136504726557987","186505","<p>I am working on project to forecast sales of stores to learn forecasting. Until now I have successfully used simple <code>auto.arima</code> function for forecasting. But to make these forecast more accurate I can make use of covariates. I have defined covariates like holidays, promotion which affect on sales of store using <code>xreg</code> argument with the help of this post:
<a href=""http://stats.stackexchange.com/questions/41070/how-to-setup-xreg-argument-in-auto-arima-in-r"">How to setup xreg argument in auto.arima() in R?</a></p>

<p>But my code fails at line:</p>

<pre><code>ARIMAfit &lt;- auto.arima(saledata, xreg=covariates)
</code></pre>

<p>and gives error saying:</p>

<pre><code>Error in model.frame.default(formula = x ~ xreg, drop.unused.levels = TRUE) : 
  variable lengths differ (found for 'xreg')
In addition: Warning message:
In !is.na(x) &amp; !is.na(rowSums(xreg)) :
  longer object length is not a multiple of shorter object length
</code></pre>

<p>Below is link to my Dataset:
<a href=""https://drive.google.com/file/d/0B-KJYBgmb044blZGSWhHNEoxaHM/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B-KJYBgmb044blZGSWhHNEoxaHM/view?usp=sharing</a></p>

<p>This is my code:</p>

<pre><code>data = read.csv(""xdata.csv"")[1:96,]
View(data)

saledata &lt;- ts(data[1:96,4],start=1, end=96,frequency =7 )
View(saledata)

saledata[saledata == 0] &lt;- 1
View(saledata)

covariates = cbind(DayOfWeek=model.matrix(~as.factor(data$DayOfWeek)),
                 Customers=data$Customers,
             Open=data$Open,
                 Promo=data$Promo,
             SchoolHoliday=data$SchoolHoliday)
View(head(covariates))


# Remove intercept
covariates &lt;- covariates[,-1]
View(covariates)

require(forecast)
ARIMAfit &lt;- auto.arima(saledata, xreg=covariates)//HERE IS ERROR LINE
summary(ARIMAfit)
</code></pre>

<p>Also tell me how I can forecast for the next 48 days. I know how to forecast using simple <code>auto.arima</code> using the argument <code>n.ahead</code> but I don't know how to do it when the argument <code>xreg</code> is used.</p>
"
"0.0878749550327494","0.0851256530758749","81632","<p>i want to use an ARIMA model in R for predicting an electrical load on a minutely basis. By examining the ACF I figured out which model could suit. The ACF has shown that the value one day ahead has a periodic autocorrelation. Therefore I'd like to implement a seasonal difference with a lag of 1440 (min/day).</p>

<p>Thus, I found this page (<a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) describing how to deal with long seasonal periods in R.</p>

<p>However, by applying that method, I experienced the following problem in R:</p>

<pre><code>&gt;Arima(x,order=c(2,0,2),xreg=fourier1(1:length(x),4,1440))

Error in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg,  : 
lengths of 'x' and 'xreg' do not match
</code></pre>

<p>x is the dataset as a zoo-Object with the following structure (it's just an example, I do not have access to the real structure at the moment; main difference: much more data!):</p>

<pre><code>&gt; str(x)
â€˜zooâ€™ series from 2010-01-01 00:00:00 to 2010-01-01 00:06:00
Data: num [1:7] 1 2 3 4 5 6 7
Index:  chr [1:7] ""2010-01-01 00:00:00"" ""2010-01-01 00:01:00"" ...
</code></pre>

<p>Since the number of rows in xreg should be exactly the same as in x, they are apparently not.</p>

<p>Does anyone has any suggestions about or experiecend this?</p>

<p>I'll appreciate any hints!</p>

<p>Marc</p>
"
"0.0878749550327494","0.0851256530758749","205967","<p>Suppose I have a website which has some baseline hourly traffic. I also run TV advertising intermittently which drives up my web traffic. I want to determine how much effect my TV advertising is having in terms of driving up web traffic.</p>

<p>If I fit an ARMAX model with hourly TV advertising spend or impressions as exogenous variables, is it valid to claim that the AR terms represent the ""baseline traffic"" while the regression terms represent the traffic that should be attributed to TV advertising?</p>

<p>Here is some example code of what I'm trying to do:</p>

<pre><code>library(forecast)

xmat &lt;- as.matrix(cbind(data[,c(""AdSpend"",""Impressions"")]))
xvar &lt;- data$WebSessions

fit &lt;- Arima(x=xvar, xreg=xmat, order=c(12,0,0), include.constant=FALSE)

reg_terms &lt;- fit$coef[""AdSpend""] * data$AdSpend + fit$coef[""Impressions""] * data$Impressions
AR_terms &lt;- fitted(fit) - reg_terms
</code></pre>

<p>I can then create a stacked area chart using AR_terms (the baseline hourly web traffic) and reg_terms (the TV attributed hourly traffic).</p>

<p><a href=""http://i.stack.imgur.com/PjaLr.png""><img src=""http://i.stack.imgur.com/PjaLr.png"" alt=""enter image description here""></a></p>

<p>Is this a valid approach?</p>

<p>Thanks for the help.</p>
"
"0.197311400916898","0.208514414057075","59814","<p>I am using forecast() package in R to predict future values. I have a time series data
for approx 6-7 years.</p>

<p>First, I split the data into training set and test
set. Test set contained values of the last 12 months of original data. Then, applied ets() and auto.arima() on the training set. This can be done by iterating 12 times using 1-step ahead OR by running once to get 12 points at the same time, which is my question no. 1</p>

<p>Then, the output received is compared with the test set to check which of the two (ets or auto.arima) gives minimal error in comparison with test set. The selected model class is then applied on the original data set to predict future 12 values. So, here arises my question no. 2</p>

<p>In Short, I have two things to do</p>

<ol>
<li>Compare accuracy measures for test set of ets and auto.arima to select one</li>
<li>Predict future values using the derived model from the first step</li>
</ol>

<p><b>Questions</b></p>

<ol>
<li><p>Should I use one-step ahead or 12-step ahead forecasts to generate data for test set?</p></li>
<li><p>Should I use one-step ahead or 12-step ahead forecasts to predict future values?</p></li>
<li><p>Should the method be same in both the steps mentioned above?</p></li>
</ol>

<p>Kindly help.</p>
"
"0.208568078815152","0.202042708452068","206188","<p>I'm trying to forecast data that has an hourly and weekly pattern.  The model I made using predictors created using seasonaldummy does a nice job of picking up the hourly weekly pattern, but it takes a long time to train the model.  I tried to create a similar forecast using fourier function, but it doesn't seem to be picking up the hourly pattern as well.  Am I setting up fourier correctly to try to achieve the effect I've gotten with seasonaldummy?  Should the frequency in ts be something other than 168?  My data is hourly.  I've provided some sample data below.  </p>

<p>My end goal is to combine the predictors for the hourly weekly pattern with other predictors, that's why I'm not just using tbats. I've provided examples below of how I'm trying to combine dummy variables for the hourly weekly pattern with other predictors.  </p>

<p>Code:</p>

<pre><code>##BoxCox

TTTlambda &lt;- BoxCox.lambda(tsData)

##Partitioning Time Series
EndTrain&lt;-1344
ValStart&lt;-EndTrain+1
ValEnd&lt;-ValStart+336

tsTrain &lt;-tsData[1:EndTrain]
tsValidation&lt;-tsData[ValStart:ValEnd]
tsTest &lt;- tsData[TestStart:TestEnd]

##Predictors
xregTrain&lt;-dfPredictors[1:EndTrain,]
xregVal&lt;-dfPredictors[ValStart:ValEnd,]
xregTest&lt;-dfPredictors[TestStart:TestEnd,]

##Seasonal Dummies
x=ts(tsData,freq=168) 
dummies=seasonaldummy(x)
xreg2Train&lt;-dummies[1:EndTrain,]
xreg2Val&lt;-dummies[ValStart:ValEnd,]
xreg2Test&lt;-dummies[TestStart:TestEnd,]


##Fourier Terms

tsTTT&lt;-ts(tsData, freq=168)

bestfit &lt;- list(aicc=Inf)
for(i in 1:25)
{
  fit &lt;- auto.arima(tsTTT, xreg=fourier(tsTTT, K=i), seasonal=FALSE)
  if(fit$aicc &lt; bestfit$aicc)
    bestfit &lt;- fit
  else break;
}

bestfit$coef ## K=2

xreg3&lt;-fourier(tsTTT,2)

xreg3Train&lt;-xreg3[1:EndTrain,]
xreg3Val&lt;-xreg3[ValStart:ValEnd,]
xreg3Test&lt;-xreg3[TestStart:TestEnd,]


##hourly weekly
Arima.fit_D &lt;- auto.arima(tsTrain, lambda = TTTlambda, xreg=xreg2Train, stepwise=FALSE, approximation = FALSE, seasonal = FALSE )

Arima.fit_D_P &lt;- auto.arima(tsTrain, lambda = TTTlambda, xreg=cbind(xreg2Train,xregTrain$Predictor), stepwise=FALSE, approximation = FALSE, seasonal = FALSE )

##Fourier hourly weekly
Arima.fit_F &lt;- auto.arima(tsTrain, lambda = TTTlambda, xreg=xreg3Train, stepwise=FALSE, approximation = FALSE, seasonal = FALSE )

Arima.fit_F_P &lt;- auto.arima(tsTrain, lambda = TTTlambda, xreg=cbind(xreg3Train,xregTrain$Predictor), stepwise=FALSE, approximation = FALSE, seasonal = FALSE )

##Forecast Model

Acast_D&lt;-forecast(Arima.fit_D,xreg=xreg2Val, h=336)

Acast_D_P&lt;-forecast(Arima.fit_D_P,xreg=cbind(xreg2Val,xregVal$Predictor), h=336)

Acast_F&lt;-forecast(Arima.fit_F,xreg=xreg3Val, h=336)

Acast_F_P&lt;-forecast(Arima.fit_F_P,xreg=cbind(xreg3Val,xregVal$Predictor), h=336)
</code></pre>

<p>Data:</p>

<pre><code>dput(tsData[1:1681])
</code></pre>

<p>c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 8.5, 4, 5, 9, 10, 11, 
7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 2, 15, 2.5, 17, 5, 5.5, 
7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 9.5, 3.5, 5, 4, 4, 9, 4.5, 
6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 12, 17.5, 19, 7, 14, 17, 3.5, 
6, 15, 11, 10.5, 11, 13, 9.5, 9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 
19, 6, 7, 7.5, 7.5, 7, 6.5, 9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 
5, 12, 6, NA, 4, 2, 5, 7.5, 11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 
7, 4.5, 9, 3, 4, 6, 17.5, 11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 
7, 7, 4, 7.5, 11, 6, 11, 7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 
6, 8.5, 7.5, 6, 5, 8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 
11.5, 3, 4, 16, 3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 
6.5, 9, 12, 17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 
6.5, 15, 8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 
16.5, 2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 
13, 10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 11.5, 
12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 10, 10, 
13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 5.5, 6, 14, 
16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 13, 6, 7, 3, 5.5, 
7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 13, NA, 12, 1.5, 7, 
7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 8, 6, 3, 7.5, 4, 7, 7.5, 
NA, NA, NA, NA, 6.5, 2, 16.5, 7.5, 8, 8, 5, 2, 7, 4, 6.5, 4.5, 
10, 6, 4.5, 6.5, 9, 2, 6, 3.5, NA, 5, 7, 3.5, 4, 4.5, 13, 19, 
8.5, 10, 8, 13, 10, 10, 6, 13.5, 12, 11, 5.5, 6, 3.5, 9, 8, NA, 
6, 5, 8.5, 3, 12, 10, 9.5, 7, 24, 7, 9, 11.5, 5, 7, 11, 6, 5.5, 
3, 4.5, 4, 5, 5, 3, 4.5, 6, 10, 5, 4, 4, 9.5, 5, 7, 6, 3, 13, 
5.5, 5, 7.5, 3, 5, 6.5, 5, 5.5, 6, 4, 3, 5, NA, 5, 5, 6, 7, 8, 
5, 5.5, 9, 6, 8.5, 9.5, 8, 9, 6, 12, 5, 7, 5, 3.5, 4, 7.5, 7, 
5, 4, 4, NA, 7, 5.5, 6, 8.5, 6.5, 9, 3, 2, 8, 15, 6, 4, 10, 7, 
13, 14, 9.5, 9, 18, 6, 5, 4, 6, 4, 11.5, 17.5, 7, 8, 10, 4, 7, 
5, 9, 6, 5, 4, 8, 4, 2, 1.5, 3.5, 6, 5.5, 5, 4, 8, 10.5, 4, 11, 
9.5, 5, 6, 11, 21, 9.5, 11, 13.5, 7.5, 13, 10, 7, 9.5, 6, 10, 
5.5, 6.5, 12, 10, 10, 6.5, 2, 8, NA, 10, 5, 4, 4.5, 5, 7.5, 12, 
22, 5, 8.5, 2.5, 3, 10.5, 4, 7, 13, 4, 3, 5, 6.5, 3, 9, 9.5, 
16, NA, 4, 12, 4.5, 7, 5.5, 8, 14, 3, 8, 12, 14, 7, 8, 6, 8.5, 
6, 6.5, 15.5, 13, 3.5, 12, 7, 6, NA, 3, 5.5, 8.5, 9, 12, 13, 
8, 6.5, 8, 3, 5, 16.5, 2, 7, 6, 2, 5, 6.5, 3, 3, 7, 2, NA, 13, 
7, 16, 13, 12.5, 12, 7, 13, 11, 21.5, 16, 20, 3, 4, 5, 7, 11, 
7, 9, 11, 7, 13, 4, 14, 5, 12, 6, 7, 9, 12, 7, 12.5, 6.5, 16, 
5, 12, 9, 9.5, 9, 7, 9.5, 3, 13, 8, 7, 7, 7, 9, 6, 6, 11, 15, 
9, 6, 19, 10.5, 4, 6, 14.5, 9, 17, 14, 4, 16, 5, 6.5, 10, 9, 
17, 11.5, 3, 5, 9, 8, 16, 10, NA, 7, 5, 12.5, 12, 11, 3, 3, 3.5, 
14, 12, 7, 4, NA, 6, NA, 6, 10, 8, 10, 2, NA, 4, 5.5, 14, 4, 
4.5, 8.5, 13, 21, 10, 11.5, 18, 5, 3, 2, 6, 11, 3, 7.5, 6, 3, 
5, 9, 7.5, 7.5, 5, 9, 17, 3, 9.5, 5.5, 9.5, 15, 14.5, 10, 9, 
13.5, 12, 12, 3, 11, 6, 4, 8, 17.5, 7.5, 7.5, NA, 7, 4, 6, 6, 
6, 6, 6, 5, 8.5, 6, 6, 5, 6, 7, 5, 5, 5, 5, 7, 6, 8, 14, 6.5, 
9.5, 5, 18.5, 5, 8, 10, 16, 12, 13, 7, 6, 13, 9, 18, 17, 8, 7, 
3, 8, 2, 9, 11, 5, 2, 5.5, 6.5, 7, 10, 2, 3, 2, 3, 5, 4, 5, 6, 
3, 5, 3.5, 5, 4, 9, NA, 10.5, 16, NA, 11, 8.5, 13, 4, 12.5, 12, 
13, 18.5, 21, 5, 9, 4.5, 3, 3, 4, 3, 4, 4, 2, 8, 4.5, 4, 5, 9, 
5, 4.5, 4, 7.5, 6, 7, 22, 5, 8, 5, 7, 4, 8, 5.5, 3, 8, 7, 6, 
7.5, 6, 15, 13.5, 10, 7, 2.5, 7.5, 9, 9.5, 8, 19, 8, 8, 10, 6, 
9, 5, 4.5, 9, 3.5, 4, 3.5, 8, 5, 3.5, 8.5, 9, 12.5, 7, 8, 10.5, 
10, 1.5, 5, 10, 9, 2, 5, 8, 11, 3, 4.5, 2, 8.5, 4, 8, 2, 3, 4, 
5.5, 2, 4, 6, 4.5, 6, 6.5, 0, 2, 3.5, 10, 7, 14, 14, 12.5, 3, 
7, 8, 3, 7, 12, 12.5, 2, 2.5, 3, 9, 10.5, 8, 6, 6.5, 8.5, 5, 
10.5, 9, 3.5, 7, 5, 8, 5, 5, 5.5, 4, 9, 8, 5.5, 5, 6, 10.5, 4, 
9, 6, 5, 11, 10.5, 10.5, 4, 11.5, 11, 6, 2, 9, 5, 9, 5, 5.5, 
7, 4, 10, 5, 3, 9, 9, 19.5, 13, 6, 15, 7, 10, 8, 10.5, 8, 16, 
7, 10.5, 8.5, 10.5, 8, 8, 7, 5, 5, 6, 6, 5, 4, 9, 6.5, 4, 7, 
7, 5, 4, 7, 6, 3, 6, 8.5, 8.5, 4, 5.5, 7, 8, 5, 6, 3, 9, 12, 
6, 7.5, 4, 3, 5.5, 2, 5.5, 7, NA, 8.5, 2, 5, 8, 8, 4, 3, 6, 4, 
4.5, 5, 3, 7.5, 9, 13, 8, 10, 12, 6.5, 3, 3.5, 8.5, 9, NA, 12, 
8, 9, 4, 6, 8, 8, 9.5, 8, 6, 5, 4, 10.5, 6.5, 4, 3.5, 5, 7, 7, 
5, 9, 6, NA, 6, 6, 5, 10, 7, 9, 9, 5, 4, 5, 4, 6, 8, 5, 3, 2.5, 
2.5, 13, 4.5, 2.5, 2, 3, 9.5, 3, 5.5, 6, 10, 9, 10, 13, 14.5, 
9, 7, 6, 5, 4, 4, 4, 5, 6.5, 11, 13.5, 11, 12, 3, 3, 14, 11, 
6, 8, 5.5, 9, 8, 8, 7, 7, 5.5, 3.5, 10.5, 6, 5.5, 8, 8, 15, 6.5, 
8, 9.5, 6.5, 5, 7, 6, 4, 14.5, 4, 2.5, 5, 8, 18, 13, 10, 6, 7, 
18, 4.5, 7, 6.5, 5, 17, 7, 3, 5.5, 4, 6.5, 5.5, 6, 8, NA, 9.5, 
14, 9, 11, 8, 7, 17, 7, 8, 8, 9, 2, 2, 4, 3, 8, 4, 9, 6, 9, 11, 
13, 7.5, 8.5, 6, 6, 10, 17.5, 18.5, 14, 8.5, 4, 5, 6, 3, 2, 4, 
4, 12, 11, 5, 2.5, 8, 6, 10, 5, 8, 8, 10.5, 14, 7, 16, 15, 6, 
4.5, 10, 19, 3, 3, 4.5, 6.5, 4, 7.5, 8, 6, 20, 6, 7, 13, 13, 
4, 10, 6, 5, 4.5, 6, 10, 6, 4, 8.5, 7.5, 3, 3.5, 3, 2, 2, 20.5, 
6, 18, 5.5, 7.5, 5, 3.5, 8, 6, 6.5, 3, 4, 8, 5, 15.5, 4, 5, 8, 
5, 3, 4, 5, 3, 3, 3, 6, 4, 12, 8, 10, 12, 5.5, 9.5, NA, 5, 4.5, 
7, 16, 7, 4.5, 5, 5, 10, 6, 19, 8, 15, 7, 19.5, 10, 7.5, 9, 9, 
7, 8, 3, 6, 5.5, 6, 7, 8, 14, 8, 13, 5.5, 3.5, 5, 9, 4.5, 4, 
4, 3, 7.5, 4, 5, 6.5, 9, 4, NA, 12, 5.5, 6, 12.5, 6.5, 6.5, 5, 
11, 4.5, 8, 2, 4, 5, 5, 3, 2.5, 6, 7, 4, 17, 4, 3, 5, 6, 2, 8, 
8.5, 6.5, 4, 10, 12.5, 11, 6.5, 9, 12.5, 5.5, 5, 7.5, 16, 11.5, 
4, 5.5, 3.5, 4, 3, 6, 4, NA, 5, 6, 7, 3, 4.5, 7, 5.5, 4, 7, 11, 
7, 3, 3, 4, 3.5, 9, 4.5, 8, 5, 6, 8, 5, 5.5, 8, 5, 9, 8, 8, 6.5, 
6, 10, 7, 7, 9, 12, 8, 13, 6.5, 6, 4, 5.5, 6, 3, 7, 8, 15, 10, 
8, 3, NA, 5, 7, 7, 6, 9, 19, 13, 7, 7.5, 11, 8.5, 4, 7.5, 6, 
13.5, 17, 9, 5, 6.5, 6, 4, 5, NA, 3, 6, 10.5, 6, 14, 6, 9.5, 
6, 10, 11, 10, 3, 7, 9, 16.5, 5.5, 12.5, 8, 5, 10, 6, 1, 5, 6.5, 
10, 8.5, 5, NA, 9.5, 13, 10, 10, 20, 7, 8, 5, 3, 3, 4.5, 3.5, 
2, 5, 11, 3, 7.5, NA, 5.5, NA, 6, 6, 11, 12, 7, 5, 15, 11, 6, 
17.5, 13.5, 16, 16.5, 5, 4, 3, 5.5, 3, 8, 11, 8, 12, 14, NA, 
10, 6, 4, 5, 8, 10, 12.5, 6, 3, 6, 5, 8, 6, 11, 12.5, 7, 6, 9.5, 
2, 8.5, 9.5, 8, 8, 2, 7.5, NA, 6, 2.5, 4, 5, 5, 6, 9, 4, 7, 6, 
2, 4.5, 3, 4, 4, 5, 4, 3, 7.5, 8.5, NA, 12, 9, 11, 9, 3, 2.5, 
7, 4, 4, 7, 8.5, 12.5, 3.5, 6.5, 10, 6, 8, 7, 13, 13.5, 12, 13, 
8, NA, 8, 9, 15, NA, 4, 3.5, 2, 7, 8, 7.5, 9.5, 1.5, 5, 4, 8, 
11, 5, 12, 4, 3, 11, 8, 7.5, 5.5, 13, 11, NA, 12, 7, 8, 6, 13, 
8, 5, 4, 7, 8, 2, 3, 4, 4, 5, 5.5, 5.5, 6, 5, 6, 14, 12, 6, 11.5, 
13, 5, 5, 8, 9, 2, 5, 6, 10, 4.5, 4, 7, 7.5, 7, 4, 10, 6.5, 6, 
10)</p>

<pre><code>dput(dfPredictors$Predictor[1:1681])
</code></pre>

<p>c(2, 6, 3, 5, 3, 2, 2, NA, 2, 6, 12, 11, 9, 10, 13, 9, 11, 7, 
12, 8, 6, 4, 10, 6, 2, 7, 2, 1, 3, 2, 1, 3, 8, 7, 7, 8, 13, 13, 
13, 11, 12, 4, 12, 18, 12, 7, 5, 4, 6, 4, 3, 3, NA, 4, 2, 8, 
8, 8, 7, 3, 5, 3, 7, 8, 7, 7, 11, 8, 10, 3, 10, 6, 5, 5, 3, 1, 
2, 1, 1, 3, 4, 8, 8, 5, 9, 12, 12, 11, 8, 5, 9, 10, 7, 8, 4, 
6, 4, 1, 3, 1, 3, NA, 2, 1, 4, 10, 7, 13, 6, 9, 6, 16, 12, 11, 
10, 12, 9, 7, 7, 7, 6, 2, 3, 1, 1, 2, 2, 3, 11, 10, 9, 8, 9, 
13, 6, 6, 10, 9, 11, 10, 8, 7, 6, 4, 2, 3, 5, 3, 2, 4, 4, 4, 
8, 5, 12, 8, 7, 12, 9, 12, 12, 12, 13, 12, 9, 8, 9, 10, 4, 7, 
4, 2, 2, 4, 1, 7, 6, 6, 8, 11, 11, 5, 7, 6, 9, 12, 15, 9, 11, 
5, 10, 5, 4, 4, 2, 3, 3, 2, 5, 4, 7, 8, 6, 6, 5, 12, 10, 8, 10, 
10, 4, 13, 12, 6, 8, 6, 3, 1, 4, 2, NA, 4, 3, 2, 6, 5, 8, 10, 
4, 13, 2, 13, 8, 11, 13, 8, 9, 10, 9, 5, 1, NA, 1, 1, 2, NA, 
1, 7, 6, 10, 7, 8, 12, 12, 9, 5, 6, 8, 13, 13, 13, 8, 8, 1, 5, 
7, 6, 2, NA, 2, 1, 2, 7, 9, 12, 12, 10, 10, 10, 6, 8, 2, 8, 3, 
4, 5, 6, 2, 2, 1, 4, 1, NA, 3, 1, 3, 8, 8, 11, 11, 12, 5, 7, 
14, 9, 10, 14, 11, 8, 6, 8, 7, 5, 4, 3, 4, 9, NA, 2, 4, 5, 8, 
2, 12, 8, 15, 12, 8, 9, 12, 9, 9, 12, 7, 7, 8, 7, 5, 4, NA, 1, 
NA, NA, 4, 9, 8, 8, 8, 12, 13, 7, 11, 8, 14, 12, 13, 15, 8, 6, 
4, 4, 5, 2, NA, 2, 5, 4, 5, 6, 15, 11, 10, 16, 10, 5, 5, 10, 
13, 10, 9, 8, 7, 5, 4, 5, 6, NA, 2, 5, 4, 1, 6, 5, 8, 4, 3, 10, 
11, 8, 12, 10, 10, 10, 12, 10, 10, 7, 5, 7, 3, 4, 3, 3, 3, 3, 
8, 4, 8, 10, 5, 10, 10, 10, 11, 10, 11, 7, 10, 7, 6, 7, 7, 3, 
3, NA, 3, 6, 5, 3, 3, 5, 6, 6, 13, 14, 14, 7, 13, 9, 10, 4, 9, 
10, 8, 3, 6, 10, 5, 2, 1, NA, 3, 4, 4, 12, 12, 11, 12, 11, 13, 
10, 9, 11, 11, 14, 10, 13, 10, 7, 11, 1, 3, 1, 4, 1, 2, 2, 3, 
9, 6, 9, 9, 8, 9, 7, 12, 17, 13, 9, 10, 8, 8, 10, 2, 3, 3, 6, 
2, 2, 1, 6, 8, 7, 9, 5, 11, 8, 8, 12, 13, 14, 10, 7, 5, 11, 11, 
8, 5, 7, 3, 2, 3, 5, NA, 1, 3, 3, 4, 9, 12, 12, 3, 5, 12, 10, 
9, 14, 15, 12, 7, 8, 7, 3, 4, 1, 5, 6, 4, NA, 5, 9, 6, 7, 8, 
15, 13, 9, 12, 9, 7, 7, 6, 7, 8, 8, 6, 4, 5, 4, 1, 5, 1, NA, 
5, 4, 12, 7, 20, 12, 14, 10, 11, 11, 12, 6, 6, 11, 5, 6, 7, 4, 
7, 5, 1, 2, NA, 2, 7, 16, 9, 4, 12, 14, 12, 9, 8, 12, 7, 6, 11, 
9, 15, 9, 4, 4, 3, 3, 2, 5, 2, 1, 6, 8, 3, 12, 11, 14, 9, 6, 
3, 12, 11, 10, 14, 10, 10, 12, 2, 3, 3, 5, 3, 2, 3, 3, 5, 9, 
5, 10, 14, 9, 14, 11, 9, 12, 9, 15, 13, 12, 15, 11, 4, 7, 3, 
3, 3, 2, 5, 5, 11, 4, 2, NA, 3, 6, 10, 8, 5, 9, 9, 10, 11, 8, 
9, 8, NA, 3, NA, 1, 1, 4, 3, 3, NA, 4, 8, 3, 9, 6, 12, 9, 7, 
11, 6, 6, 12, 5, 4, 11, 7, 1, 2, 3, 2, 4, 8, 2, 6, 5, 9, 3, 7, 
8, 8, 8, 14, 10, 12, 5, 12, 9, 13, 7, 3, 5, 3, 4, 2, 4, 2, NA, 
5, 5, 9, 8, 7, 11, 9, 5, 6, 10, 13, 10, 9, 16, 11, 7, 5, 6, 2, 
5, 3, 5, 2, 2, 6, 5, 11, 7, 13, 6, 10, 7, 9, 7, 8, 9, 12, 7, 
7, 5, 3, 5, 3, 3, 5, 3, 1, 2, 10, 11, 8, 1, 9, 10, 14, 12, 7, 
11, 11, 10, 7, 5, 9, 8, 6, NA, 4, 2, NA, 3, 4, 4, 6, 10, 9, 6, 
8, 9, 10, 9, 14, 12, 8, 11, 16, 16, 13, 8, 7, 4, 4, 1, 3, 4, 
2, 2, 5, 9, 9, 3, 8, 12, 6, 11, 10, 6, 8, 15, 12, 12, 7, 6, 7, 
3, 2, 1, 3, 2, 2, 5, 7, 11, 8, 3, 4, 5, 5, 9, 6, 10, 9, 7, 17, 
12, 3, 8, 6, 4, 4, 4, 3, 4, 2, 1, 4, 7, 12, 5, 4, 8, 7, 15, 8, 
6, 6, 10, 5, 10, 7, 3, 4, 4, 1, 5, 3, 4, 5, 2, 1, 8, 6, 8, 9, 
7, 13, 11, 11, 6, 9, 9, 9, 9, 8, 1, 8, 1, 3, 2, 2, 1, 3, 2, 3, 
8, 9, 10, 12, 6, 9, 12, 7, 8, 8, 14, 10, 10, 8, 10, 4, 4, 4, 
4, 3, 2, 4, 2, 3, 16, 3, 9, 8, 15, 9, 11, 10, 12, 12, 7, 15, 
13, 8, 9, 3, 8, 1, 2, 2, 3, 3, 3, 10, 7, 5, 10, 4, 8, 8, 10, 
8, 9, 9, 6, 7, 7, 4, 6, 8, 4, 5, 5, 1, 1, 1, 5, 7, 3, 3, 12, 
8, 7, 10, 7, 12, 11, 7, 6, 14, 13, 9, 14, 3, 4, 6, 3, 2, 3, NA, 
2, 3, 7, 6, 9, 7, 5, 9, 8, 10, 7, 7, 6, 11, 11, 9, 7, 5, 2, 3, 
2, 2, 5, NA, 2, 5, 6, 7, 8, 14, 11, 6, 9, 10, 10, 9, 8, 16, 9, 
6, 6, 3, 2, 5, 1, 1, NA, 3, 3, 2, 5, 10, 9, 10, 13, 11, 8, 17, 
13, 5, 11, 10, 11, 6, 9, 6, 4, 5, 6, 5, 2, 4, 1, 2, 5, 9, 12, 
10, 10, 18, 9, 13, 16, 8, 13, 5, 16, 11, 8, 10, 3, 6, 3, 1, 2, 
2, 6, 2, 12, 9, 5, 8, 10, 11, 4, 10, 9, 9, 9, 19, 11, 8, 9, 4, 
4, 4, 7, 5, 2, 2, 1, 2, 6, 10, 11, 12, 9, 9, 12, 9, 8, 14, 8, 
5, 3, 5, 7, 6, 3, 4, 6, 3, 3, NA, 2, 2, 7, 12, 11, 14, 7, 10, 
10, 13, 12, 5, 8, 8, 9, 10, 4, 9, 4, 5, 3, 1, 4, 2, 1, 7, 9, 
10, 10, 11, 9, 8, 6, 3, 8, 10, 9, 9, 10, 8, 11, 4, 1, 1, 3, 1, 
1, 3, 6, 2, 3, 10, 4, 9, 6, 10, 11, 6, 7, 8, 6, 11, 8, 4, 8, 
5, 5, 1, 7, 1, 3, 3, 3, 6, 6, 8, 8, 9, 7, 6, 10, 9, 10, 6, 13, 
10, 20, 7, 9, 2, 6, 3, 2, 1, 1, 2, 1, 3, 13, 7, 6, 7, 11, 7, 
9, 7, 9, 10, 9, 10, 6, 11, 7, 5, 5, 5, 4, 6, 4, NA, 4, 4, 11, 
11, 9, 8, 9, 9, 12, 8, 11, 12, 3, 9, 12, 10, 8, 7, 3, 5, 2, 2, 
3, 2, 3, 4, 8, 13, 6, 8, 7, 4, 7, 13, 10, 9, 11, 10, 10, 5, 12, 
4, 3, 7, NA, 2, 2, 1, 6, 2, 8, 8, 9, 6, 11, 7, 7, 9, 11, 9, 6, 
8, 11, 10, 6, 5, 3, 5, 5, 1, 1, 2, 2, 5, 7, 13, 11, 7, 17, 10, 
4, 10, 9, 10, 6, 5, 8, 4, 6, 6, 3, 5, NA, 5, 1, 3, 1, 5, 3, 8, 
11, 6, 8, 9, 7, 11, 5, 12, 10, 10, 8, 9, 8, 7, 3, 2, 4, 5, 1, 
3, 1, 2, 9, 10, 4, 7, 10, 11, 6, 11, 8, 8, 7, 10, 9, 9, 11, 9, 
2, 3, 4, 3, NA, 2, 5, 5, 8, 13, 7, 14, 11, 4, 7, 10, 15, 10, 
15, 8, 6, 9, 5, 4, 2, 1, 3, NA, 1, 1, 2, 3, 13, 6, 8, 6, 12, 
10, 13, 5, 14, 11, 14, 14, 10, 10, 5, 3, 4, 2, 4, 2, 3, 2, 2, 
NA, 10, 9, 6, 14, 10, 7, 7, 12, 3, 13, 12, 12, 14, 8, 11, 3, 
6, NA, 2, NA, 3, 2, 5, 5, 11, 3, 5, 7, 7, 12, 8, 5, 11, 5, 2, 
8, 9, 6, 7, 3, 3, 1, 1, NA, 1, 3, 1, 3, 5, 10, 10, 7, 3, 5, 8, 
11, 9, 11, 7, 9, 9, 10, 5, 10, 4, 3, 3, 1, 2, NA, 3, 4, 6, 6, 
7, 11, 9, 11, 9, 6, 6, 8, 7, 9, 12, 12, 9, 5, 4, 3, NA, 1, 2, 
1, 2, 2, 2, 6, 9, 8, 8, 8, 12, 8, 13, 7, 12, 7, 8, 12, 8, 5, 
5, 1, NA, 4, 3, 1, NA, 5, 6, 10, 7, 15, 12, 12, 6, 11, 12, 12, 
10, 16, 10, 8, 11, 8, 5, 4, 2, 1, 2, NA, 5, 2, 8, 9, 7, 9, 14, 
7, 13, 3, 6, 9, 13, 10, 8, 6, 4, 6, 4, 5, 1, 3, 3, 2, 1, 4, 4, 
9, 11, 4, 8, 9, 11, 8, 8, 9, 19, 9, 7, 7, 11, 6, 8)</p>
"
"0.06213697660012","0.0601929265428846","181533","<p>I have a time series (quarterly data) that I will use to predict the upcoming 4 quarters.The total number of observations is 20 quarters, thus, I need to predict quarter 21 -> 24.
First I took the diff(data) to have a stationary data and I want to fit AR(1). I am using the following in R:</p>

<p><code>arima(diff(data), order=c(1,0,0))</code> and I obtained: ar1 (- 0.2441) and Intercept (1.2004) </p>

<p>Is the following correct?</p>

<pre><code>âˆ†y(t+1) = 1.2004 - 0.2441*âˆ†y(t)

âˆ†y(t+2) = 1.2004 - 0.2441*âˆ†y(t+1)
</code></pre>

<p>If I want to predict y(t+1), do I find âˆ†y(t+1) and then</p>

<pre><code>y(t+1) = y(t) + âˆ†y(t+1)

y(t+2) = y(t) + âˆ†y(t+1)+ âˆ†y(t+2)
</code></pre>

<p>and so on... until y(t+4)</p>

<p>Is this analogy correct? Do you know how can I get the predicted value in R without doing it manually? How can I tell R that first I need to predict the delta and then the original value.</p>
"
"0.09320546490018","0.0902893898143269","206327","<p>I have time series data on drug trade revenue in a city over a four-month period (DP). I also have data on police crackdowns and raids in the same period, in the same city (IV). I want to see if the crackdowns/raids have any effect on the trade volume.</p>

<p>This is fairly straightforward and easy to follow, I think. However, when I do an ARIMA model with the DP and a bunch of lagged versions of the IV, the output is... Hard to understand, and even harder to explain. </p>

<p>Take the plot below, for example. In this model the IV is included (as xreg), but the graph is not very intuitive. Is there a way to illustrate both variables and how they impact each other (if at all) in a more intuitive way?</p>

<p><a href=""http://i.stack.imgur.com/gSAWY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gSAWY.png"" alt=""enter image description here""></a></p>

<p>I also have a hard time understanding the actual output. The model below is the correct ARIMA model, but as the coefficients are useless in ARIMA models, what can I really say based on these numbers? Can I say anything about how the IV affects the DP?</p>

<p>Most textbook resources I've found online, such as R.H. Shumway &amp; D.S. Stoffer's and Hyndman's, are mostly about how to find the best ARIMA model etc. and relatively little about how we can interpret and explain findings from an ARIMA+xreg model.</p>

<p>I hope someone can help!</p>

<pre><code>Series: revenue 
ARIMA(0,1,2)(2,0,0)[7]                    

Coefficients:
          ma1      ma2    sar1    sar2  (media)1  (media)2  (media)3
      -0.4801  -0.4825  0.4974  0.1835           14530.95           14153.26          -40388.58
s.e.   0.0834   0.0818  0.0945  0.1016           12270.92           16683.66           31806.25
      (media)4  (media)5  (media)6  (media)7  (media)8
              -20874.23          -32847.12          -22016.12           29745.91           13306.47
s.e.           29056.25           25904.39           34676.39           39702.54           55336.30
      (media)9  (media)11  (media)13  (media)17  (media)23
               12164.75            29435.90            87281.65            19487.28          -187469.01
s.e.           48021.79            54428.17            55982.59            57683.76            55452.48
      (media)25
                42058.20
s.e.            53967.83

sigma^2 estimated as 3.325e+09:  log likelihood=-1549.47
AIC=3136.93   AICc=3144.1   BIC=3190.82

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE      ACF1
Training set 4718.048 57203.09 43598.36 -2.884122 16.07953 0.7763964 0.0171537
</code></pre>
"
"0.206804358677574","0.217028683070608","60648","<p>I am trying to forecast electricity consumption in GWh for 2 years ahead (from June 2013 ahead), using R (the forecast package). For that purpose, I tried regression with ARIMA errors. I fitted the model using the <code>auto.arima</code> function, and I used the following variables in the <code>xreg</code> argument in the <code>forecast.Arima</code> function: </p>

<p>- Heating and Cooling Degree Days,<br>
- Dummies for all 12 months and<br>
- Moving holidays dummies (Easter and Ramadan)  </p>

<p>I have several questions regarding the model:</p>

<p>1) Is it correct to use all 12 dummies for monthly seasonality, since when I tried to include 11, the function returned error. The <code>Auto.arima</code> function returned the model ARIMA(0,1,2)</p>

<p>2)The model returned the following coefficients (I won't specify all of them as there are too many coefficients):</p>

<pre><code>ma1      ma2     HDD     CDD   January  February  March     April
-0.52 -0.16      0.27    0.12  525.84   475.13    472.57    399.01
</code></pre>

<p>I am trying to determine the influence of the temperature component over electricity load. In percentages, (interpreting the coefficients just as with the usual regression) the temperature components (<code>HDD</code>+<code>CDD</code>) account for 11,3% of the electricity consumption. Isn't this too little, considering the fact that the electricity consumption is mostly influenced by the weather component? On the other hand, taking look at the dummies' coefficients, it turns out that the seasonality accounts for the greater part of the load. Why is this? Is the model completely incorrect?</p>

<p>I tried linear regression, and the temperature component accounts for 20%, but it is still a low percentage. Why is this?</p>

<p>3) I am obviously making some mistakes in the use of <code>forecast.Arima</code> or the plot function parameters since when I plot the forecasts, I get a picture of the original time series which is continued (merged) with the forecasts for the whole time series period (from 2004 until 2015). I don't know how to explain this better, I tried to paste the picture, but it seems I cannot paste pictures here.</p>
"
"0.12427395320024","0.120385853085769","109826","<p>I am trying to quantify the effect of a future random shocks on my seasonal ARIMA model. If I have understood the theory correctly, the easiest way is to express my seasonal ARIMA model in its ""random shock"" form, and calculate the corresponding psi weights.</p>

<p>Is there a way to do this in R? There is ARMAtoMA, but I think this only works for ARMA models, and not seasonal ARIMA models.</p>

<p>Thank you for your help.</p>

<hr>

<p>UPDATE: Apologies, I'll post the question about R onto stack overflow. It would be good to get confirmation that this is the correct method to quantify the effect of future random shocks to a seasonal ARIMA model.</p>
"
"0.12683657235796","0.122868296795748","152831","<p>I create a demand forecast for a company that sells, say, toasters. We have one old standby model that's just finally stocked out, and a series of much newer models with shorter time series of sales to work with.</p>

<p>Our historic forecasting method (in R) was a much more complicated version of this:</p>

<pre><code>fit &lt;- Arima(old_standby)
refit &lt;- Arima(shiny_new_model, model = fit)
new_model_future &lt;- forecast(refit, h = 60)
</code></pre>

<p>This was all well and good until two things happened. First, we stopped selling the old standby, so we don't have any recent data on that. Not necessarily a problem - as long as sales for the old standby in the past look like sales for new models now, it shouldn't screw things up.</p>

<p>The second issue is the problem though. We also gave out 1-week coupons for our new models that caused a massive temporary spike in sales. Usually, I'd incorporate these as additional regressors in the <code>Arima</code>, but I can't do it because we had already stocked out of the old standby at that point.</p>

<p>Is there any way to fit an Arima to the data for the old standby, refit it to the data for the new model (incorporating a dummy regressor for the coupon), and then predict? Something like this:</p>

<pre><code>fit &lt;- Arima(old_standby)
refit &lt;- Arima(shiny_new_model, model = fit, xreg = coupon_dummy)
new_model_future &lt;- forecast(refit, h = 60, xreg = new_coupons)
</code></pre>

<p>I'm guessing the answer is ""no,"" because you can't fit the auto-regressive portion of the model against two different sets of data at the same time. But I still thought it was worth asking if anybody had any ideas.</p>
"
"0.263624865098248","0.255376959227625","168655","<p>I have got monthly data from 1993 to 2015 and would like to do forecasting on these data. I used tsoutliers package to detect the outliers, but I do not know how do I continue to forecast with my set of data .</p>

<p>This is my code:</p>

<pre><code>product.outlier&lt;-tso(product,types=c(""AO"",""LS"",""TC""))
plot(product.outlier)
</code></pre>

<p>This is my output from tsoutliers package</p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p><a href=""http://i.stack.imgur.com/qKI4N.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qKI4N.jpg"" alt=""This is my plot""></a></p>

<p>I have these warning messages as well.</p>

<pre><code>Warning messages:
1: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
2: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
3: In locate.outliers.oloop(y = y, fit = fit, types = types, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
4: In arima(x, order = c(1, d, 0), xreg = xreg) :
  possible convergence problem: optim gave code = 1
5: In auto.arima(x = c(5.77, 5.79, 5.79, 5.79, 5.79, 5.79, 5.78, 5.78,  :
  Unable to fit final model using maximum likelihood. AIC value approximated
</code></pre>

<p><strong>Doubts:</strong></p>

<ol>
<li>If I am not wrong, tsoutliers package will remove the outliers it detect and through the use of the dataset with outliers removed, it
will give us the best arima model suited for the data set, is it
correct?</li>
<li>The adjust series data set is being shifted down by a lot due to remove of the level shift,etc. Doesn't this mean that if the forecasting is done on the adjusted series, the output of the forecast will be very inaccurate, since the more recent data are already more than 12, while adjusted data shift it to around 7-8.</li>
<li>What does warning message 4 and 5 means? Does it mean it cannot do auto.arima using the adjusted series?</li>
<li>What does the [12] in ARIMA(0,1,0)(0,0,1)[12] mean? Is it just my frequency/periodicity of my dataset, which I set it to monthly? And does this also means that my data series is seasonal as well? </li>
<li>How do I detect seasonality in my data set? As from the visualisation of the time series plot, I cant see any obvious trend, and if I use the decompose function, it will assume that there is a seasonal trend? So do I just believe what the tsoutliers tell me, where there is seasonal trend, since there is MA of order 1?</li>
<li>How do I continue to do my forecasting with this data after identifying these outliers?</li>
<li><strong>How to incorporate these outliers to other forecasting models - Exponential Smoothing, ARIMA, Strutural Model, Random Walk, theta? I am sure I cannot remove the outliers since there are level shift, and if I only take adjusted series data, the values will be too small, so what do I do?</strong></li>
</ol>

<p><strong>Do I need to add these outliers as regressor in the auto.arima for forecasting? How does this work then?</strong></p>
"
"0.138942503594182","0.134595475514541","152976","<p>My goal is to forecast sensor measurements (e.g. temperature, humidity) in a lightweight (and real-time) fashion.</p>

<p>To this end I use ARIMA forecasting as implemented in R, where I retrain the model for every new forecast. Ideally I'd like to retrain the model as little as possible to save computing power, but this is where I get confused. Let's say we use the hypothetical ARIMA(2,0,2) model as follows:</p>

<p>$$
y_t = 0.3 y\prime_{t-1} + 0.4 y\prime_{t-2} + 0.4 e_{t-1} + 0.5 e_{t-2} + c + e_t
$$</p>

<p>Now say I train some model based on measurements from $$t_0 \text{ to } t_{250}$$ If at this point I want to forecast for t = 251, we use the measurements for t-1=250 and t-2=249 in the above equation. Now I want to forecast for t=252. If I use the same approach (in this case use t-1=251 and t-2=250), is this essentially the same as L-step forecasting? And if setting the n.ahead parameter of the R <a href=""https://stat.ethz.ch/R-manual/R-patched/library/stats/html/predict.arima.html"" rel=""nofollow"">predict</a> function, does this actually work in this fashion (using previously predicted values)?</p>

<p>If I were to fit a model, and then use it for say, 2000 forecasts. Would this essentially be L-step forecasting with L=2000? </p>
"
"0.196494372972965","0.19034674690672","153204","<p>I have a time series for sales data on a weekly and monthly basis.  I tried using <code>holt.winter</code> and <code>auto.arima</code>. <code>holt.winter</code> can work only on monthly data (freq = 12 &lt; 24), and gives good results, but <code>auto.arima</code> gives very bad results on both monthly and weekly data, just a straight line in the following figures:</p>

<p><img src=""http://i.stack.imgur.com/zNZ9i.png"" alt=""arima monthly"">  </p>

<p><img src=""http://i.stack.imgur.com/b6W12.png"" alt=""enter image description here""></p>

<p>I have the following questions:  </p>

<ol>
<li>Can anyone provide some theoretical basis on why ARIMA performs poorly and why HW performs better?  </li>
<li>Also what model should I use for relatively high frequency data (weekly or daily)?  </li>
</ol>

<p>Also if someone can guide me to advanced books in that area (I have done some reading in Brockwell, 2002, <em>Introduction to Time Series</em>).</p>

<p>[Update]</p>

<p>I tried holt-winter . auto.arima . arima and got the following results</p>

<pre><code>ARIMA (1,0,1)(1,1,1) : sigma^2 estimated as 94587:  log likelihood = -266.51,  aic = 543.02

AUTO.ARIMA =&gt;ARIMA(0,0,0) with non-zero mean : sigma^2 estimated as 141005:  log likelihood=-352.67
AIC=709.33   AICc=709.6   BIC=713.07

Holt-Winter : ETS(A,A,A) 
  Smoothing parameters:
    alpha = 0.0298 
    beta  = 1e-04 
    gamma = 0.0133 
sigma:  306.1749
sigma^2 : 93743.06939001
     AIC     AICc      BIC 
767.3367 784.8851 797.2759 
</code></pre>

<p>it seems arima(1,0,1)(1,1,1) gives better AIC and log-likelihood that result of auto arima , also HW detects seasonality , is that auto.arima stucking at some local optima</p>
"
"0.18641092980036","0.180578779628654","229894","<p>I have monthly data. I am trying make this data stationary by taking normal and seasonal differences. When I am taking seasonal difference, I choose period that gives the lowest Chi-Square result in Ljung-Box test. I have recently notice that other periods with higher Chi-Square results can give better results according to <code>mape, rmse</code> and <code>mae</code>. To be clear, I want to share two steps of my work. First, I take first difference and apply <code>Ljung-Box</code> test with <code>lag=20</code>. If it is higher than the chi-square value in the table, I try seasonal difference. I use a for loop to try different periods and collect test results. And select the one with the smallest Chi-Square result. And in further candidate models, I take period as the period I found. Is it a good logic or not? I also try to choose the one that is closest to Chi-Square table value. But it doesn't give the best model again. It changes according to data. What should I try?  I'am using <code>Forecast::Arima</code> function in R.</p>
"
"0.206085036989691","0.181488502160157","56374","<p>as I am stepping into forecasting with ARIMA models, I am trying to understand how I can improve a forecast based on ARIMA fit with seasonality and drift. </p>

<p>My data is the following time series ( over 3 years, with clear trend upwards and visible seasonality, which seems to be not supported by autocorrelation at lags 12, 24, 36??). </p>

<pre><code>    &gt; bal2sum3years.ts
             Jan     Feb     Mar     Apr     May     Jun     Jul     Aug          
    2010 2540346 2139440 2218652 2176167 2287778 1861061 2000102 2560729 
    2011 3119573 2704986 2594432 2362869 2509506 2434504 2680088 2689888 
    2012 3619060 3204588 2800260 2973428 2737696 2744716 3043868 2867416 
             Sep     Oct     Nov     Dec
    2010 2232261 2394644 2468479 2816287
    2011 2480940 2699780 2760268 3206372
    2012 2951516 3119176 3032960 3738256
</code></pre>

<p>The model that was suggested by <code>auto.arima(bal2sum3years.ts)</code> gave me the following model:</p>

<pre><code>    Series: bal2sum3years.ts 
    ARIMA(0,0,0)(0,1,0)[12] with drift         

    Coefficients:
              drift
          31725.567
    s.e.   2651.693

    sigma^2 estimated as 2.43e+10:  log likelihood=-321.02
    AIC=646.04   AICc=646.61   BIC=648.39
</code></pre>

<p>However, the <code>acf(bal2sum3years.ts,max.lag=35)</code> does not show acf coefficients higher than 0.3. The seasonality of the data is, however, pretty obvious - spike at the beginning of every year. This is what the series looks like on the graph:
<img src=""http://i.stack.imgur.com/kQi5N.png"" alt=""Original Time Series""></p>

<p>The forecast using <code>fit=Arima(bal2sum3years.ts,seasonal=list(order=c(0,1,0),period=12),include.drift=TRUE)</code> , called by function <code>forecast(fit)</code>, results in the next 12months's means being equal to the last 12 months of the data plus constant. This can be seen by calling <code>plot(forecast(fit))</code>, </p>

<p><img src=""http://i.stack.imgur.com/GJqcG.png"" alt=""Actual and Forecasted Data""></p>

<p>I have also checked the residuals, which are not autocorrelated but have positive mean ( non zero). </p>

<p>The fit does not model the original time series precisely, in my opinion ( blue the original time series, red is the <code>fitted(fit)</code>:</p>

<p><img src=""http://i.stack.imgur.com/ux3i7.png"" alt=""Original vs fit""></p>

<p>The guestion is, is the model incorrect? Am I missing something? How can I improve the model? It seems that the model literally takes the last 12 months and adds a constant to achieve the next 12 months. </p>

<p>I am a relative beginner in time series forecasting models and statistics. </p>

<p>Thank you very much for your answers!</p>
"
"0.264952958466348","0.28232985128664","169468","<p>I have monthly time series data, and would like to do forecasting with detection of outliers .</p>

<p><strong>This is the sample of my data set:</strong></p>

<pre><code>       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
2006  7.55  7.63  7.62  7.50  7.47  7.53  7.55  7.47  7.65  7.72  7.78  7.81
2007  7.71  7.67  7.85  7.82  7.91  7.91  8.00  7.82  7.90  7.93  7.99  7.93
2008  8.46  8.48  9.03  9.43 11.58 12.19 12.23 11.98 12.26 12.31 12.13 11.99
2009 11.51 11.75 11.87 11.91 11.87 11.69 11.66 11.23 11.37 11.71 11.88 11.93
2010 11.99 11.84 12.33 12.55 12.58 12.67 12.57 12.35 12.30 12.67 12.71 12.63
2011 12.60 12.41 12.68 12.48 12.50 12.30 12.39 12.16 12.38 12.36 12.52 12.63
</code></pre>

<p>I have referred to <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r?lq=1"">Timeseries analysis procedure and methods using R</a>, to do a series of different model of forecasting, however it does not seems to be accurate. In additional, I am not sure how to incorporate the tsoutliers into it as well.</p>

<p>I have got another post regarding my enquiry of tsoutliers and arima modelling and procedure over <a href=""http://stats.stackexchange.com/questions/168655/how-to-interpret-and-do-forecasting-using-tsoutliers-package-and-auto-arima/168869#168869"">here</a> as well.</p>

<p>So these are my code currently, which is similar to link no.1.</p>

<p><strong>Code:</strong></p>

<pre><code>product&lt;-ts(product, start=c(1993,1),frequency=12)

#Modelling product Retail Price

#Training set
product.mod&lt;-window(product,end=c(2012,12))
#Test set
product.test&lt;-window(product,start=c(2013,1))
#Range of time of test set
period&lt;-(end(product.test)[1]-start(product.test)[1])*12 + #No of month * no. of yr
(end(product.test)[2]-start(product.test)[2]+1) #No of months
#Model using different method
#arima, expo smooth, theta, random walk, structural time series
models&lt;-list(
#arima
product.arima&lt;-forecast(auto.arima(product.mod),h=period),
#exp smoothing
product.ets&lt;-forecast(ets(product.mod),h=period),
#theta
product.tht&lt;-thetaf(product.mod,h=period),
#random walk
product.rwf&lt;-rwf(product.mod,h=period),
#Structts
product.struc&lt;-forecast(StructTS(product.mod),h=period)
)

##Compare the training set forecast with test set
par(mfrow=c(2, 3))
for (f in models){
    plot(f)
    lines(product.test,col='red')
}

##To see its accuracy on its Test set, 
#as training set would be ""accurate"" in the first place
acc.test&lt;-lapply(models, function(f){
    accuracy(f, product.test)[2,]
})
acc.test &lt;- Reduce(rbind, acc.test)
row.names(acc.test)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.test &lt;- acc.test[order(acc.test[,'MASE']),]

##Look at training set to see if there are overfitting of the forecasting
##on training set
acc.train&lt;-lapply(models, function(f){
    accuracy(f, product.test)[1,]
})
acc.train &lt;- Reduce(rbind, acc.train)
row.names(acc.train)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.train &lt;- acc.train[order(acc.train[,'MASE']),]

 ##Note that we look at MAE, MAPE or MASE value. The lower the better the fit.
</code></pre>

<p>This is the plot of my different forecasting, which doesn't seem very reliable/accurate, through the comparison of the red""test set"", and blue""forecasted"" set.
<strong>Plot of different forecast</strong>
<a href=""http://i.stack.imgur.com/WZSNq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WZSNq.jpg"" alt=""Different forecast""></a></p>

<p><strong>Different accuracy of the respective models of test and training set</strong></p>

<pre><code>Test set
                    ME      RMSE       MAE        MPE     MAPE      MASE      ACF1 Theil's U
theta      -0.07408833 0.2277015 0.1881167 -0.6037191 1.460549 0.2944165 0.1956893 0.8322151
expsmooth  -0.12237967 0.2681452 0.2268248 -0.9823104 1.765287 0.3549976 0.3432275 0.9847223
randomwalk  0.11965517 0.2916008 0.2362069  0.8823040 1.807434 0.3696813 0.4529428 1.0626775
arima      -0.32556886 0.3943527 0.3255689 -2.5326397 2.532640 0.5095394 0.2076844 1.4452932
struc      -0.39735804 0.4573140 0.3973580 -3.0794740 3.079474 0.6218948 0.3841505 1.6767075

Training set
                     ME      RMSE       MAE         MPE     MAPE      MASE    ACF1 Theil's U
theta      2.934494e-02 0.2101747 0.1046614  0.30793753 1.143115 0.1638029  0.2191889194        NA
randomwalk 2.953975e-02 0.2106058 0.1050209  0.31049479 1.146559 0.1643655  0.2190857676        NA
expsmooth  1.277048e-02 0.2037005 0.1078265  0.14375355 1.176651 0.1687565 -0.0007393747        NA
arima      4.001011e-05 0.2006623 0.1079862 -0.03405395 1.192417 0.1690063 -0.0091275716        NA
struc      5.011615e-03 1.0068396 0.5520857  0.18206018 5.989414 0.8640550  0.1499843508        NA
</code></pre>

<p>From the models accuracy, we can see that the most accurate model would be theta model.
I am not sure why is the forecast very inaccurate, and I think that one of the reasons would be that, I did not treat the ""outliers"" in my data set, resulting in a bad forecast for all model.</p>

<p><strong>This is my outliers plot</strong></p>

<p><strong>Outliers Plot</strong>
<a href=""http://i.stack.imgur.com/bZDQv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bZDQv.jpg"" alt=""Outliers""></a></p>

<p><strong>tsoutliers output</strong></p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p>I would like to know how can I further ""analyse""/forecast my data, with these relevant data set and detection of outliers, etc.
Please do help me in treatment of my outliers as well to do my forecasting as well . </p>

<p>Lastly, I would like to know how to combined the different model forecasting together, as from what @forecaster had mentioned in link no.1, combining the different model will most likely result in a better forecasting/prediction.</p>

<p><strong>EDITED</strong></p>

<p>I would like to incorporate the outliers in other models are well.</p>

<p>I have tried some codes, eg. </p>

<pre><code>forecast.ets( res$fit ,h=period,xreg=newxreg)
    Error in if (object$components[1] == ""A"" &amp; is.element(object$components[2], : argument is of length zero

forecast.StructTS(res$fit,h=period,xreg=newxreg)
Error in predict.Arima(object, n.ahead = h) : 'xreg' and 'newxreg' have different numbers of columns
</code></pre>

<p>There are some errors produced, and I am unsure about the correct code to incorporate the outliers as regressors.
Furthermore, how do I work with thetaf or rwf, as there are no forecast.theta or forecast.rwf?</p>
"
"0.206085036989691","0.181488502160157","229721","<p>I have 4 years electrical load data. I split the data into 3 years (75%) training data, 1 year for testing (25%). Also I have the temperature data for each day during the previous period. (The link to the dataset: <a href=""https://drive.google.com/open?id=0B08HdcWBksWcTUxqc1ByOW1UVEU"" rel=""nofollow"">here</a>.) </p>

<p>I want to make use of the temperature data to enhance the forecasting using argument <code>xreg</code> in <code>arima</code> function. </p>

<p>Here is my code:</p>

<pre><code>mydata1&lt;-read.csv(""1st pape/kaggle_data.csv"");
mydata&lt;-ts(mydata1[,2],start = c(2004),frequency = 365)

#split the data into trainData and test data
trainData = window(mydata, end=c(2007))
testData = window(mydata, start=c(2007))
temp&lt;-ts(mydata1[,3],start = c(2004),frequency = 365)

#split the temperature into trainData and test data
trainReg = window(temp, end=c(2007))
testReg = window(temp, start=c(2007))
</code></pre>

<p>Apply ARIMA model without using <code>xreg</code>:</p>

<pre><code>mod_arima &lt;- auto.arima(trainData, ic='aicc', stepwise=FALSE)
summary(mod_arima)
Series: trainData 
ARIMA(1,0,3) with non-zero mean 

Coefficients:
         ar1      ma1      ma2      ma3  intercept
      0.9642  -0.2098  -0.2157  -0.1693  24008.122
s.e.  0.0110   0.0322   0.0330   0.0325   1018.007

sigma^2 estimated as 9318421:  log likelihood=-10347.38
AIC=20706.75   AICc=20706.83   BIC=20736.75

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 6.102332 3045.638 2293.946 -1.519484 9.625694 0.5151126
                    ACF1
Training set 0.004483007

plot(forecast(mod_arima)); lines(testData , col=""red"", start= c(2007,1,1)); 
legend(""topleft"", lty=1,col=c(4,2),legend=c(""forecasted data"",""real data""))

y &lt;- msts(trainData, c(7,365)) # multiseasonal ts
x &lt;- msts(trainReg, c(7,365)) # multiseasonal ts

fit &lt;- auto.arima(y, xreg=(fourier(y, K=c(3,30))))
fit_f &lt;- forecast(fit, xreg= fourier(y, K=c(3,30), 365), 365)
plot(fit_f)
</code></pre>

<p>the red line is the actual data, while the blue is the foretasted data. The left plot is appeared before using fourier function, while the right after using it. </p>

<p><a href=""http://i.stack.imgur.com/QxvKC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QxvKC.png"" alt=""enter image description here""></a></p>

<p>Apply ARIMA model using <code>xreg</code>:</p>

<pre><code>mod_arima2 &lt;- auto.arima(trainData ,xreg = trainReg, ic='aicc', stepwise=FALSE)
summary(mod_arima2)
Series: trainData 
ARIMA(1,0,3) with non-zero mean 

Coefficients:
         ar1      ma1      ma2      ma3  intercept  trainReg
      0.9709  -0.2403  -0.2108  -0.1609  29984.188  -88.3976
s.e.  0.0094   0.0320   0.0330   0.0321   1468.108   13.1966

sigma^2 estimated as 8955023:  log likelihood=-10325.13
AIC=20664.26   AICc=20664.36   BIC=20699.26

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 6.030471 2984.292 2267.803 -1.464553 9.529988 0.5092422
                    ACF1
Training set 0.005526977

plot(forecast(mod_arima2,xreg = testReg)); lines(testData , col=""red"", start= c(2007,1,1)); 
legend(""topleft"", lty=1,col=c(4,2), legend=c(""forecasted data"",""real data""))

l = (fourier(y, K=c(3,30)))
z = cbind(l,x)
fit2 &lt;- auto.arima(y, xreg=z)
fit_f2 &lt;- forecast(fit, xreg= z, 365)
plot(fit_f2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/TgJE5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TgJE5.png"" alt=""enter image description here""></a>
<strong>Questions</strong>:</p>

<ol>
<li>Did I use <code>xreg</code> correctly?</li>
<li>If yes, why is the summary the same without using <code>xreg</code>?</li>
<li>Why are the forecasts far away from the real data?</li>
</ol>
"
"0.107624400500126","0.104257207028537","169564","<p>The <code>arimax</code> function in the <code>TSA</code> package is to my knowledge the only <code>R</code> package that will fit a transfer function for intervention models. It lacks a <a href=""http://stats.stackexchange.com/questions/34106/forecasting-with-arimax-model-including-xtransf"">predict function</a> though which is sometimes needed.</p>

<p>Is the following a work-around for this issue, leveraging the excellent <code>forecast</code> package? Will the predictive intervals be correct? In my example, the std errors are ""close"" for the components.</p>

<ol>
<li>Use the forecast package arima function to determine the pre-intervention noise series and add any outlier adjustment.</li>
<li>Fit the same model in <code>arimax</code> but add the transfer function</li>
<li>Take the fitted values for the transfer function (coefficients from <code>arimax</code>) and add them as xreg in <code>arima</code>. </li>
<li>Forecast with <code>arima</code></li>
</ol>

<blockquote>
<pre><code>library(TSA)
library(forecast)
data(airmiles)
air.m1&lt;-arimax(log(airmiles),order=c(0,0,1),
              xtransf=data.frame(I911=1*(seq(airmiles)==69)),
              transfer=list(c(1,0))
              )
</code></pre>
  
  <p>air.m1</p>
</blockquote>

<p>Output:</p>

<pre><code>Coefficients:
  ma1  intercept  I911-AR1  I911-MA0
0.5197    17.5172    0.5521   -0.4937
s.e.  0.0798     0.0165    0.2273    0.1103

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.09   BIC=-155.02
</code></pre>

<p>This is the filter, extended out 5 more periods that the data</p>

<pre><code>tf&lt;-filter(1*(seq(1:(length(airmiles)+5))==69),filter=0.5521330,method='recursive',side=1)*(-0.4936508)
forecast.arima&lt;-Arima(log(airmiles),order=c(0,0,1),xreg=tf[1:(length(tf)-5)])
forecast.arima
</code></pre>

<p>Output:</p>

<pre><code>Coefficients:
         ma1  intercept  tf[1:(length(tf) - 5)]
      0.5197    17.5173                  1.0000
s.e.  0.0792     0.0159                  0.2183

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.28   BIC=-157.74
</code></pre>

<p>Then to Predict</p>

<pre><code>predict(forecast.arima,n.ahead = 5, newxreg=tf[114:length(tf)])
</code></pre>
"
"0.0878749550327494","0.0851256530758749","230269","<p>I am sitting with a couple of time-series that I am analysing using ARIMA models. I have a question regarding prediction intervals. When predicting using a model that takes a first difference (a SARIMA(1,1,0)x(1,0,0) model), I get an increasing size of the prediction interval. Without I get a very constant and narrow band (see below):</p>

<p><a href=""http://i.stack.imgur.com/UaHX6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UaHX6.png"" alt=""Graphs""></a></p>

<p>The corresponding results are as follows:</p>

<p><a href=""http://i.stack.imgur.com/Fu2nU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Fu2nU.png"" alt=""Results""></a></p>

<p>Can anyone explain why the band is so constant? First I thought it was because of a large significant MA coefficient. This, however, I removed and the ""problem"" persisted. Then I though it was because the ARIMA without differencing automatically included an intercept. However, again, when I specified <code>include.mean = FALSE</code>, nothing changed.</p>

<p>Any help would be appreciated.</p>
"
"0.187350033626992","0.199637352376173","156449","<p>I am working with workersâ€™ remittance quarterly data for Bangladesh. Here I am doing time series forecasting using R. I am applying auto.arima model and exponential smoothing model. I want to compare between them to check which best fits the data and gives better forecast.</p>

<p>Here is the output:</p>

<p>fit1 &lt;- auto.arima(lremit, d=1, D=NA, stationary=FALSE,
+                    seasonal=TRUE,ic=""aic"",trace=TRUE,
+                    allowdrift=FALSE,allowmean=TRUE)</p>

<p>Best model: ARIMA(2,1,3)(0,1,1)[4]                    </p>

<blockquote>
  <p>summary(fit1)
  Series: lremit 
  ARIMA(2,1,3)(0,1,1)[4]<br>
  Coefficients:
            ar1      ar2     ma1     ma2      ma3     sma1
        -0.5024  -0.1691  0.3940  0.1516  -0.1899  -0.9605
  s.e.   0.6321   0.4860  0.6298  0.4465   0.1060   0.1098
  sigma^2 estimated as 0.007314:  log likelihood=135.59
  AIC=-257.18   AICc=-256.3   BIC=-236.84</p>
</blockquote>

<p>Training set error measures:
                       ME       RMSE        MAE         MPE     MAPE      MASE         ACF1
Training set -0.003608938 0.08398593 0.06532171 -0.09985958 1.110818 0.4381367 -0.004851439</p>

<blockquote>
  <p>fit1 &lt;- Arima(lremit,order=c (2,1,3),seasonal=c (0,1,1))
  h11=plot(forecast(fit1,h=20))
  h11
  $mean
           Qtr1     Qtr2     Qtr3     Qtr4
  2015 8.256047 8.283843 8.300686 8.341204
  2016 8.372717 8.406483 8.413318 8.457855
  2017 8.489041 8.522291 8.529440 8.573906
  2018 8.605075 8.638346 8.645488 8.689954
  2019 8.721124 8.754394 8.761536 8.806002</p>
  
  <h2>ETS</h2>
  
  <p>fit2&lt;-ets(lremit)
  summary(fit2)</p>
</blockquote>

<p>ETS(A,A,N) </p>

<p>Call:
 ets(y = lremit) </p>

<p>Smoothing parameters:
    alpha = 0.8594 
    beta  = 1e-04 </p>

<p>Initial states:
    l = 4.2135 </p>

<p>sigma:  0.0858</p>

<pre><code> AIC     AICc      BIC 
</code></pre>

<p>12.20515 12.50145 23.97172 </p>

<p>Training set error measures:
                        ME       RMSE        MAE         MPE     MAPE      MASE         ACF1
Training set -7.229862e-05 0.08579429 0.06800397 -0.01942594 1.169213 0.4561276 -0.002900248</p>

<p>It is my first work using R and I am facing problems regarding this. They are:</p>

<ol>
<li>auto.arima output shows seasonality in every 4th quarter, but exponential smoothing shows non seasonality, what is the interpretation of this contradictory result?</li>
<li>How can I compare between them, what is the proper measure?</li>
<li>What is the command for in sample forecast in auto.arima? If I write h=0, then it shows error</li>
<li>Where can I find elaborate interpretation of auto.arima and exponential smoothing output and about the comparison?</li>
<li>Which error measure should I prefer like ME, MAPE, RMSE etc. as they are almost same for the two models?</li>
<li>In case of auto.arima it shows same output for allowing drift or no drift</li>
</ol>
"
"0.12683657235796","0.122868296795748","194756","<p>I've got a question regarding ARIMA modeling. 
I am having a hard time to make to model out the seasonalities of my time series. The pictures below shows my tries in modeling. 
The topic is to forecast sales of a shop. 
It shows both the real Sales Values and the fitted/forecasted values.</p>

<p><a href=""http://i.stack.imgur.com/cOnr9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cOnr9.png"" alt=""enter image description here""></a></p>

<p>Below you can see my avalable data. <code>Promo</code> means whether there is a promotion on a certain day in the store.
<code>Holiday1-3</code> are different kind of holidays, like Easter holidays (<code>Holiday2</code>) and Christmas holidays (<code>Holiday1</code>).
The Variables <code>PxH1</code>, <code>PxH2</code>, <code>Pxh3</code> and <code>PxS</code> are made by me and multiplicate the value of <code>Promo</code> with each holiday. A Promo and school holiday at the same time give a higher bump in Sales then just a promo or just a school holiday.
Promo usually is every 2 weeks for a whole week. That's where the 2 weekly bump comes from. The 2 weekly bump and the holiday bumps are already modeled quite nice in my opinion...</p>

<p><a href=""http://i.stack.imgur.com/Fvnri.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Fvnri.png"" alt=""enter image description here""></a></p>

<p>My question is on how to model out the yearly seasonality properly. The sales clearly stay the same from New Year to Easter, grow slowly from Easter until beginning of August, with August being the peak of sales, and then continue to fall from August to New Year.</p>

<p>If you can help me with this topic with something different than SPSS Modeler, like R, then feel free to answer in R.
If you need additional information please feel free to tell me!</p>

<p>Thank you a lot!</p>
"
"0.18641092980036","0.180578779628654","209874","<p>I have a model fitted with <code>auto.arima</code>, the model is ARIMA(0,1,0)x(0,1,0)[6] with seasonal period 6. The data is bi-monthly so there is an annual seasonality. There is only one regressor indicating an intervention (dummy). </p>

<p>Then I used this model to old data to see what would have happened if the intervention would have done since and earlier period, using the model and forecast from an earlier data. <strong>The thing I do not understand yet</strong> is that if I suppose the intervention only occur in one period, the series only differ in this period. Therefore, there is no persistence on the intervention.</p>

<p>As I understand, the model has ARIMA errors. The error in the intervention period should change and so there should be an effect in the next periods when using forecast to predict futures values. If the intervention occurs in only one period, <strong>why</strong> in the forecast the intervention does not affect futures predictions?</p>

<hr>

<p>EDIT:</p>

<p>The code I am using is</p>

<pre><code>model1&lt;-auto.arima(ts,xreg = X.ts)
</code></pre>

<p>Where <code>X.ts</code> is a <code>ts</code> object with <code>0</code> and a period with intervention. </p>

<p>Then I used </p>

<pre><code>model2&lt;-Arima(Xold, xreg= X.ts.old, model=model1)
</code></pre>

<p>So I used the first model on earlier data to make the following</p>

<pre><code>forecast(model2, xreg=cbin(c(0,1,1,1,1...))
</code></pre>

<p>So I am trying to show what would have been expected from an earlier period (the forecast) if the intervention would have started earlier.</p>

<p>The thing I do not understand yet is that for instance</p>

<pre><code>forecast(model2, xreg=cbin(c(0,1,1,1,0...))
forecast(model2, xreg=cbin(c(0,1,1,1,1...))
</code></pre>

<p>only differ in the periods the <code>xreg</code> differ, with no persistence of these differences. I did not expect this, <strong>why is that?</strong></p>
"
"0.197311400916898","0.191138212885652","234192","<p>I referred to <a href=""https://www.otexts.org/fpp/9/4"" rel=""nofollow"">this link</a> and I have the following questions regarding my data. Let me start by explaining the time series that I am dealing with.</p>

<p>I have <strong>daily</strong> hospital data with various <strong>departments</strong> and numerous <strong>doctors</strong> working in each department. I have several years of data and my forecast horizon is for the next 365 days. My data has weekly and annual seasonality. Moreover I intend to capture the effects of holidays and Sundays in my forecasts. As a result I have not created a hierarchical time series as suggested towards the end of the link(primarily because I am not sure whether we can pass a regressor to it and more so because I do not know how many doctors I end up predicting for in each department). </p>

<p>The reason for this is that some doctors do not have good data(short time series or sparse data). In this case I collect these doctors and aggregate them to form something I call ""OtherDocs"". Typically in <code>DeptXYZ -&gt; Doc1 , Doc2 , Doc3 , Doc4 , Doc5 and Doc6</code> I could end up creating forecasts for <code>DeptXYZ -&gt; Doc1 , Doc3 , Doc4 , Doc6 and OtherDocs</code>. If <code>OtherDocs</code> is still not predictable I generate a naive forecast. In this fashion I created <strong>base forecasts for every level in the hierarchy individually using <code>arima</code> and passing my <code>xreg</code> to it and selecting the best model on the basis of AIC</strong>.</p>

<p>Now, consider this example - </p>

<p><code>Total -&gt; DeptX and DeptY</code></p>

<p><code>DeptX -&gt; DocA and DocB</code></p>

<p><code>DeptY -&gt; Doc1 , Doc2 and Doc3</code></p>

<p>There are cases where <code>DocA</code> has a time series that starts from ""2011-03-11"" and ends on ""2016-09-07"" while <code>DocB</code> has a time series that starts from ""2011-05-17"" and ends on ""2016-09-07"". Generating the base forecasts for <code>DocA</code> and <code>DocB</code> results in the predicted values(<code>fit$mean</code>) being of a time series from ""2016-09-08"" to ""2017-09-07"". As long as the time series refers to the same dates within the Department I believe we are good to go.</p>

<p>In my attempt to reconcile the forecasts from each level I employed the forecasted proportions like so -</p>

<p>$\Largeá»¹_{DocA,365} = \frac{Å·_{DocA,365}*Å·_{DeptX,365}}{(Å·_{DocA,365}+Å·_{DocB,365})*(Å·_{DeptX,365}+Å·_{DeptY,365})}Å·_{Total,365}$</p>

<p><strong>1. Am I doing anything wrong in the above step?</strong></p>

<p><strong>2. Suppose for one moment that the topmost level forecasted values do not capture the low points of data in the case of Holidays and Sundays. Does that intuitively mean that revised forecasts for DocA might not correctly capture the same(being a proportion of $Å·_{Total,365}$)?</strong></p>

<p>Another query I have is to do with the Optimal Combination Approach -</p>

<p>$\Largeá»¹_h = S(Sâ€²S)^{-1}Sâ€²Å·_h$</p>

<p><strong>3. I am unfamiliar with this matrix notation $S'$. Is it the inverse of $S$? Could you shed some light on this? And how do you suggest I calculate the summing up matrix in my case?(Is it absolutely necessary to proceed with the exact knowledge of the number of doctors in each department?)</strong></p>
"
"0.0878749550327494","0.0851256530758749","171618","<p>I'm having trouble changing what prediction/confidence intervals/bands are plotted when plotting a forecasted arima object generated.</p>

<blockquote>
  <p>model &lt;- auto.arima(x)</p>
  
  <p>forecast &lt;- forecast(model, h=29)</p>
  
  <p>plot(forecast)</p>
</blockquote>

<p>I see the call automagically chooses some standard intervals (maybe 90% and 95%?), but the documentation for forecast.Arima() and plot.forecast() don't seem to mention which they are and how to alter which are plotted. The closest I can see to it is in the plot.forecast() help page, you can change the colors the prediction intervals are shaded with.</p>

<p>Ideally, I'd like to see 70% and 90% bands on my 29 step-ahead forecast plot.</p>

<p>thanks!</p>
"
"0.107624400500126","0.104257207028537","6330","<p>I have previously used <a href=""http://www.forecastpro.com/"">forecast pro</a> to forecast univariate time series, but am switching my workflow over to R.  The forecast package for R contains a lot of useful functions, but one thing it doesn't do is any kind of data transformation before running auto.arima().  In some cases forecast pro decides to log transform data before doing forecasts, but I haven't yet figured out why.</p>

<p>So my question is: when should I log-transform my time series before trying ARIMA methods on it?</p>

<p>/edit: after reading your answers, I'm going to use something like this, where x is my time series:</p>

<pre><code>library(lmtest)
if ((gqtest(x~1)$p.value &lt; 0.10) {
    x&lt;-log(x)
}
</code></pre>

<p>Does this make sense?</p>
"
"0.140913417690306","0.159255514317652","6329","<p>I've been using the ets() and auto.arima() functions from the <a href=""http://robjhyndman.com/software/forecast/"">forecast package</a> to forecast a large number of univariate time series.  I've been using the following function to choose between the 2 methods, but I was wondering if CrossValidated had any better (or less naive) ideas for automatic forecasting.</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"") {
    XP=ets(x, ic=ic) 
    AR=auto.arima(x, ic=ic)

    if (get(ic,AR)&lt;get(ic,XP)) {
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
        model
}
</code></pre>

<p>/edit: What about this function?</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"",holdout=0) {
    S&lt;-start(x)[1]+(start(x)[2]-1)/frequency(x) #Convert YM vector to decimal year
    E&lt;-end(x)[1]+(end(x)[2]-1)/frequency(x)
    holdout&lt;-holdout/frequency(x) #Convert holdout in months to decimal year
    fitperiod&lt;-window(x,S,E-holdout) #Determine fit window

    if (holdout==0) {
        testperiod&lt;-fitperiod
    }
    else {
        testperiod&lt;-window(x,E-holdout+1/frequency(x),E) #Determine test window
    }

    XP=ets(fitperiod, ic=ic)
    AR=auto.arima(fitperiod, ic=ic)

    if (holdout==0) {
        AR_acc&lt;-accuracy(AR)
        XP_acc&lt;-accuracy(XP)
    }
    else {
        AR_acc&lt;-accuracy(forecast(AR,holdout*frequency(x)),testperiod)
        XP_acc&lt;-accuracy(forecast(XP,holdout*frequency(x)),testperiod)
    }

    if (AR_acc[3]&lt;XP_acc[3]) { #Use MAE
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
    model
}
</code></pre>

<p>The ""holdout"" is the number of periods you wish to use as an out of sample test.  The function then calculates a fit window and a test window based on this parameter.  Then it runs the auto.arima and ets functions on the fit window, and chooses the one with the lowest MAE in the test window.  If the holdout is equal to 0, it tests the in-sample fit.</p>

<p>Is there a way to automatically update the chosen model with the complete dataset, once it has been selected?</p>
"
"0.06213697660012","0.0601929265428846","210117","<p>I have the code below which trains ARIMA models for a range of order combinations. I'm getting the error below in the step training the ARIMA models.  The code worked just fine with the <code>hsales</code> time-series provided for Hyndman's text book in the ""fpp"" package in R. If anyone can point out the issue or suggest how to solve it, I would be grateful.</p>

<p>Code:</p>

<pre><code>library(""forecast"")
library(""tseries"")
library(""sqldf"")
library(""manipulate"")
library(""dplyr"")
library(""xts"")

tsTrain &lt;- tsTrain
tsTest &lt;- tsValidation

pvar&lt;-1:17
dvar&lt;-1:2
qvar&lt;-1:17

##Creating All Combingations

OrderGrid&lt;-expand.grid(pvar,dvar,qvar)

##Vectorize Suggestion

n &lt;- function(a,b,c) {Arima(tsTrain, order=c(a,b,c),method=""ML"")}
mod_fit &lt;- do.call(Vectorize(n, SIMPLIFY=FALSE), unname(OrderGrid))
</code></pre>

<p>Error:</p>

<pre><code>Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
  non-finite finite-difference value [3] 
</code></pre>

<p>Data:</p>

<pre><code>c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 8.5, 4, 5, 9, 10, 11, 
7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 2, 15, 2.5, 17, 5, 5.5, 
7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 9.5, 3.5, 5, 4, 4, 9, 4.5, 
6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 12, 17.5, 19, 7, 14, 17, 3.5, 
6, 15, 11, 10.5, 11, 13, 9.5, 9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 
19, 6, 7, 7.5, 7.5, 7, 6.5, 9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 
5, 12, 6, NA, 4, 2, 5, 7.5, 11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 
7, 4.5, 9, 3, 4, 6, 17.5, 11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 
7, 7, 4, 7.5, 11, 6, 11, 7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 
6, 8.5, 7.5, 6, 5, 8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 
11.5, 3, 4, 16, 3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 
6.5, 9, 12, 17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 
6.5, 15, 8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 
16.5, 2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 
13, 10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 11.5, 
12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 10, 10, 
13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 5.5, 6, 14, 
16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 13, 6, 7, 3, 5.5, 
7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 13, NA, 12, 1.5, 7, 
7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 8, 6, 3, 7.5, 4, 7, 7.5, 
NA, NA, NA, NA, 6.5, 2, 16.5, 7.5, 8, 8, 5, 2, 7, 4, 6.5, 4.5, 
10, 6, 4.5, 6.5, 9, 2, 6, 3.5, NA, 5, 7, 3.5, 4, 4.5, 13, 19, 
8.5, 10, 8, 13, 10, 10, 6, 13.5, 12, 11, 5.5, 6, 3.5, 9, 8, NA, 
6, 5, 8.5, 3, 12, 10, 9.5, 7, 24, 7, 9, 11.5, 5, 7, 11, 6, 5.5, 
3, 4.5, 4, 5, 5, 3, 4.5, 6, 10, 5, 4, 4, 9.5, 5, 7, 6, 3, 13, 
5.5, 5, 7.5, 3, 5, 6.5, 5, 5.5, 6, 4, 3, 5, NA, 5, 5, 6, 7, 8, 
5, 5.5, 9, 6, 8.5, 9.5, 8, 9, 6, 12, 5, 7, 5, 3.5, 4, 7.5, 7, 
5, 4, 4, NA, 7, 5.5, 6, 8.5, 6.5, 9, 3, 2, 8, 15, 6, 4, 10, 7, 
13, 14, 9.5, 9, 18, 6, 5, 4, 6, 4, 11.5, 17.5, 7, 8, 10, 4, 7, 
5, 9, 6, 5, 4, 8, 4, 2, 1.5, 3.5, 6, 5.5, 5, 4, 8, 10.5, 4, 11, 
9.5, 5, 6, 11, 21, 9.5, 11, 13.5, 7.5, 13, 10, 7, 9.5, 6, 10, 
5.5, 6.5, 12, 10, 10, 6.5, 2, 8, NA, 10, 5, 4, 4.5, 5, 7.5, 12, 
22, 5, 8.5)
</code></pre>
"
"0.06213697660012","0.0601929265428846","26668","<p>I wanted to ask whether it was possible to use the auto.arima function to identify subset ARIMA models rather than those of pure lags? I have identified a model in Stata in subset lags that performs well and wanted to cross check this with the <code>auto.arima()</code> function but I can't seem to figure out if subset lags are supported.</p>"
"NaN","NaN","","<r><arima><forecasting>"
"0.140913417690306","0.136504726557987","65064","<p>I'm using R forecast package with a daily time series data, that has complex i.e. Multiple seasonality (weekly, Yearly, monthly). The fit/forecast process also needs to take into account certain day specific effects.</p>

<p>I plan to:</p>

<ul>
<li>Use auto.arima function </li>
<li>Set TS frequency to 7, to take care of the weekly seasonality</li>
<li>Use Fourier terms for 'xreg' parameter to take care of monthly and yearly seasonality</li>
<li><p>Use a regressor matrix for other effects e.g. Holidays.</p>

<ol>
<li><p>Can someone help in providing a concrete example on how to use the fourier function, which can take both monthly and yearly seasonality into account?</p></li>
<li><p>Would like to see an example of how to combine a regressor matrix with the fourier result, so that it can be assigned to 'xreg' parameter or together?</p></li>
</ol></li>
</ul>

<p>On both these questions, I have only found possibilities of the above mentioned by Dr.Hyndman, but concrete examples can really be useful to the community as well.</p>

<p>Thanks.</p>
"
"0.06213697660012","0.0601929265428846","158493","<p>I've been using the R forecast package's <em>auto.arima()</em> function to fit an ARIMA model to my time series data. I want to see how good of a fit the ARIMA model is to my original data. I hope to plot my original time series and the ARIMA simulation on the same plot and see how well they match up. How can I do this?</p>"
"NaN","NaN","<p>Thanks!  </p>",""
"NaN","NaN","","<r><regression><forecasting><arima>"
"0.215248801000253","0.191138212885652","192739","<p>I have been working with the forecast package in R a lot, recently. And my question might seem trivial (or not, maybe I'm missing something), but for the life of me I can't seem to find a way to fit an Arima model with exogenous variables (<code>xreg</code> argument) that has been computed by the <code>auto.arima</code> function to previously unseen test data.</p>

<p>So, I'm basically trying to do the following:</p>

<pre><code>library(forecast)
fit &lt;- auto.arima(trainingdata, xreg = trainingvariables)
</code></pre>

<p>...and then I would like to ""apply"" the model to new test data, for which I also have new exogenous variables available. I can see the following methods:</p>

<pre><code>fitted(fit)
</code></pre>

<p>That returns one-step in-sample forecasts, so, in effect, that's exactly what I want. Except that it's in-sample. However, I would like to calculate <strong>one-step out-of-sample forecasts</strong> (with <strong>new exogenous variables</strong> that I have available). Another method:</p>

<pre><code>forecast(fit, xreg = newvariables, h = ...)
</code></pre>

<p>That works for exactly one step, but then seems to merely forecast the trainingdata stored in the model fit. But I don't think I can use new testdata here? (So, I can't use this method for testing one-step prediction accuracy.) One more idea:</p>

<pre><code>fit2 &lt;- Arima(testdata, model = fit)
</code></pre>

<p>According to the manual, if the <code>model</code> parameter is used, ""this same model is
fitted to [testdata] without re-estimating any parameters"". Great, but I don't think I can supply any new exogenous variables, can I?</p>

<p>I really think, I must be missing something simple. Any help would be much appreciated.</p>
"
"0.06213697660012","0.0601929265428846","193178","<p>Please consider the following code (in R)</p>

<pre><code>library(forecast)
tt&lt;-structure(c(1494.5, 1367.57, 1357.57, 1222.23, 1124.02, 1011.64, 
4575.64, 3201.87, 3050.04, 2173.38, 1967.88, 1838.55, 1666.05, 
1656.05, 1524.96, 835.96, 775.36, 592.36, 494.15, 4058.15, 2624.36, 
2448.47, 1598.47, 1398.47, 1264.14, 1165.88, 1053.67, 941.36, 
821.36, 471.36, 373.15, 259.91, 3808.91, 2262.26, 1940.39, 1011.39, 
800.81, 790.81), index = structure(c(16563L, 16565L, 16570L, 
16572L, 16577L, 16579L, 16584L, 16585L, 16586L, 16587L, 16588L, 
16589L, 16590L, 16592L, 16593L, 16599L, 16606L, 16607L, 16608L, 
16612L, 16613L, 16614L, 16617L, 16618L, 16619L, 16620L, 16621L, 
16628L, 16633L, 16635L, 16638L, 16642L, 16647L, 16648L, 16649L, 
16650L, 16651L, 16654L), class = ""Date""), class = ""zoo"")

tt2&lt;-as.ts(tt)
tt2&lt;-na.locf(tt2) #I replace the NA with the previous non-NA value
mm&lt;-auto.arima(tt2)

plot(forecast(mm, h=60))
</code></pre>

<p>The results of the auto.arima function is puzzling...
There is a clear seasonality in the data (this is the balance of an account: every month a salary is cashed in and there is a spike in the value of the series, followed by a decrease until the next salary is received). I would like to forecast a couple of cycles, but the auto.arima forecast is nothing like I expect.
Does anybody have any suggestions (also outside the auto.arima)?
Any suggestion is welcome.</p>
"
"0.06213697660012","0.0601929265428846","193550","<p>How can we decide the size or portion of the data given to get the ARIMA that has the best forecasting properties?</p>

<p>I mean, for example, we have a hourly series with over 28.000 elements.</p>

<p>Which is the criteria that tells us: do ARIMA over last 100 elements, or 250 last elements, so the ARIMA we get is better for forecasting?
I am interested in short time prediction, like for 24 hours.</p>

<p>I read everywhere but found no criterion yet.</p>
"
"0.06213697660012","0.0601929265428846","160244","<p>I have a time series which shows an yearly spike around summer but otherwise is predictable by an AR(1) model. The tests on the data also show that the time series shows stationarity and is non-seasonal. How do I model the spike?"
"NaN","NaN","(More details about the time series here: <a href=http://stats.stackexchange.com/questions/159769/what-does-the-following-acf-curve-mean-picture-attached?noredirect=1#comment304724_159769>What does the following ACF curve mean ? (Picture attached)</a>)</p>",""
"NaN","NaN","<p><img src=http://i.stack.imgur.com/8ayfF.png alt=enter image description here></p>",""
"NaN","NaN","","<r><time-series><forecasting><arima>"
"0.196494372972965","0.19034674690672","88400","<p>Let's say I fit an ARIMA model on a time series up to date t.</p>

<p>I want to forecast the 10 next values without refitting the model but also using the latest data available for each date.</p>

<p>So
forecast for (t+1) uses the model fitted from 1 to t and the time serie from 1 to t</p>

<p>forecast for (t+2) uses the model fitted from 1 to t and the time serie from 1 to t+1</p>

<p>forecast for (t+3) uses the model fitted from 1 to t and the time serie from 1 to t+2</p>

<p>...</p>

<p>This is not the same as simply using ""forecast"" with an horizon of 5, as I want to take into account all data points prior to the forecasted point. (e.g. a shock on t+2 woudl have an effect on the forecast for t+3, even though we do not refit the entire model)</p>

<p>One way would be to fit the model using data from 1 to t, and then apply this model to all the data and take the fitted values as forecasts.</p>

<p>However this does not work.
Even for an horizon of 1, the two methods produces different results.</p>

<pre><code>library(forecast)
data&lt;-c(1,4,3,5,7,8,1,2,6,7,2,3,4,4);
mymodel&lt;-Arima(data[1:10], order=c(1,1,1)) # Fitting the model using data from 1 to 10

forecast(mymodel, h=1)$mean[1] # Forecasting point 11
[1] 5.263669

fitted(Arima(data[1:11],  model = mymodel))[11] # Applying the model estimated from 1 to 10 to data from 1 to 11 and taking fitted values
[1] 5.125379
</code></pre>
"
"0.138942503594182","0.134595475514541","65585","<p>I have a daily weather data set, which has, unsurprisingly, very strong seasonal effect.</p>

<p><img src=""http://i.stack.imgur.com/B5Zpo.jpg"" alt=""enter image description here""></p>

<p>I adapted an ARIMA model to this data set using the function auto.arima from forecast package.
To my surprise the function does not apply any seasonal operations- seasonal differencing, seasonal ar or ma components. Here is the model it estimated:</p>

<pre><code>library(forecast)
data&lt;-ts(data,frequency=365)
auto.arima(Berlin)

Series: data
ARIMA(3,0,1) with non-zero mean 

Coefficients:
         ar1      ar2     ar3      ma1  intercept
      1.7722  -0.9166  0.1412  -0.8487   283.0378
s.e.  0.0260   0.0326  0.0177   0.0214     1.7990

sigma^2 estimated as 5.56:  log likelihood=-8313.74
AIC=16639.49   AICc=16639.51   BIC=16676.7
</code></pre>

<p>And also the forecasts using this model are not really satisfying. Here is the plot of the forecast:
<img src=""http://i.stack.imgur.com/IkpIq.jpg"" alt=""enter image description here""></p>

<p>Can anyone give me a hint what is wrong here?</p>
"
"0.196494372972965","0.19034674690672","159428","<p>I have a set of data, let's say average weight of employees, captured every month over a period of 5 years (2010 - 2014). I cannot find a seasonality trend in the data over these years. Also, I have found that it is not dependent on any other factors.</p>

<p>I am trying to forecast values for 2015 to get a general sense of this data as it is an important metric in the operations of my business. </p>

<p>I have tried ARIMA, R-regression, Exponential smoothing, Excel forecast to find any seasonality whatsoever. However, my efforts are yet to materialize. </p>

<p>My question is: How do I forecast a variable that has no seasonality?</p>

<p>I have attached my data herewith. </p>

<p><strong>Graphs</strong></p>

<p>Yearly Values for years 2010 - 2014</p>

<p><img src=""http://i.stack.imgur.com/rmoeD.jpg"" alt=""enter image description here""></p>

<p>Value Cumulative over 2010 - 2014</p>

<p><img src=""http://i.stack.imgur.com/iwyh8.jpg"" alt=""enter image description here""></p>

<p>All Values from 2010 - 2014</p>

<p><img src=""http://i.stack.imgur.com/dfcGd.jpg"" alt=""enter image description here""></p>

<p><strong>Auto ARIMA in R</strong></p>

<pre><code># Map 1-based optional input ports to variables
dataset1 &lt;- maml.mapInputPort(1) # class: data.frame
library(forecast)


dates &lt;-  dataset1$Date
values &lt;- dataset1$Weight

dates &lt;-  as.Date(dates, format = '%m/%d/%Y')
values &lt;- as.numeric(values)

train_ts &lt;- ts(values, frequency=12)
fit1 &lt;- auto.arima(train_ts)
train_model &lt;- forecast(fit1, h = 12)
plot(train_model)

# produce forecasting
train_pred &lt;- round(train_model$mean,2)
data.forecast &lt;- as.data.frame(t(train_pred))
#colnames(dataset1.forecast) &lt;- paste(""Forecast"", 1:data$horizon, sep="""")

# Select data.frame to be sent to the output Dataset port
maml.mapOutputPort(""data.forecast"");
</code></pre>

<p><strong>Forecasted Value with Auto ARIMA</strong></p>

<pre><code>Date        Weight
01-01-15    11.77
01-02-15    11.76
01-03-15    11.77
01-04-15    11.76
01-05-15    11.77
01-06-15    11.77
01-07-15    11.76
01-08-15    11.77
01-09-15    11.76
01-10-15    11.77
01-11-15    11.77
01-12-15    11.76
</code></pre>

<p><strong>Data</strong></p>

<pre><code>Date        Weight      Cumulative Weight
01-01-10    11.8800     11.8800
01-02-10    10.4000     22.2800
01-03-10    6.9500      29.2300
01-04-10    15.5000     44.7300
01-05-10    17.0400     61.7700
01-06-10    10.4700     72.2400
01-07-10    12.1400     84.3800
01-08-10    2.5800      86.9600
01-09-10    12.6300     99.5900
01-10-10    11.6800     111.2700
01-11-10    9.0700      120.3400
01-12-10    10.8900     131.2300
01-01-11    1.7500      132.9800
01-02-11    -1.7700     131.2100
01-03-11    5.9300      137.1400
01-04-11    -4.9200     132.2200
01-05-11    4.3900      136.6100
01-06-11    1.5100      138.1200
01-07-11    1.2200      139.3400
01-08-11    10.2900     149.6300
01-09-11    13.0600     162.6900
01-10-11    10.1400     172.8300
01-11-11    8.5250      181.3550
01-12-11    6.4350      187.7900
01-01-12    -5.5100     182.2800
01-02-12    -4.3000     177.9800
01-03-12    2.3200      180.3000
01-04-12    4.0700      184.3700
01-05-12    12.2700     196.6400
01-06-12    14.7400     211.3800
01-07-12    8.4600      219.8400
01-08-12    11.6300     231.4700
01-09-12    -0.1500     231.3200
01-10-12    2.5200      233.8400
01-11-12    6.7400      240.5800
01-12-12    35.6300     276.2100
01-01-13    26.4000     302.6100
01-02-13    26.1300     328.7400
01-03-13    16.2100     344.9500
01-04-13    56.0800     401.0300
01-05-13    32.2300     433.2600
01-06-13    17.5100     450.7700
01-07-13    3.6700      454.4400
01-08-13    7.7700      462.2100
01-09-13    -14.2800    447.9300
01-10-13    1.0800      449.0100
01-11-13    9.4000      458.4100
01-12-13    7.3400      465.7500
01-01-14    6.1400      471.8900
01-02-14    3.8200      475.7100
01-03-14    16.7600     492.4700
01-04-14    0.4900      492.9600
01-05-14    17.9800     510.9400
01-06-14    14.8000     525.7400
01-07-14    12.6400     538.3800
01-08-14    5.7300      544.1100
01-09-14    -2.0900     542.0200
01-10-14    9.1300      551.1500
01-11-14    12.5100     563.6600
01-12-14    -1.3900     562.2700
</code></pre>

<p><strong>Actual Values for 2015</strong></p>

<pre><code>Date        Weight
01-01-15    -18.43
01-02-15    13.94
01-03-15    26.14
01-04-15    24.36
01-05-15    18.37
</code></pre>
"
"0.107624400500126","0.104257207028537","206447","<p>I have a good understanding of ARIMA models but I've always found significant spikes in ACFs and PACFs that gave me the appropriate AR and MA parameters. </p>

<p>Now I'm dealing with a series that is more like an ARIMA(0,1,0) which I think is the same as a random walk? This random walk concept is a little new to me but I sort of understand it. My question now is how do I even create a model for this series?</p>

<p>I've seen some methods in the <a href=""https://cran.r-project.org/web/packages/forecast/forecast.pdf"" rel=""nofollow"">{forecast}</a> package that might be the ones I'm looking for but I want to understand how they are different.</p>

<p>The functions that I am confused about are <code>naive()</code> and <code>rwf()</code>. It seems like both try to address the same random walk problem. But also, what happens if I just build an <code>Arima()</code> model with parameters 0,1,0? how is that different?</p>
"
"0.175749910065499","0.17025130615175","212773","<p>I have half-hourly electricity data of several homes for a duration of one month. This data is represented in <code>xts</code> time-series format. Now, I need to make half-hourly forecasts using the same data for coming day. The forecasting interval is one day (i.e., very short term forecast). I assume that electricity usage follows daywise seasonality as the number of users/occupants remain fixed. This assumption is obeyed in some homes while some other follow random electricity usage. </p>

<p>Currently, I use historical one month data to make half-hourly forecasts of coming day using <code>auto.arima</code> found in <code>forecast</code> pacakage. Using this approach I do get forecasts for next day (48 forecasted values). 48 forecasted values correspond to 48 half-hours of the day. But, I do not know </p>

<ol>
<li>How should I specify the seasonality fact, i.e., how should I mention that data is assumed seasonal day-wise. In other words, how should I mention within historical data of one month, there are 30 periods and each period consist of 48 observations. </li>
<li>Is <code>xts</code> representation suitable for this task or I need to represent this in <code>ts</code> format?</li>
</ol>

<p>Here I have attached half-hourly data of 26 days. I have removed timestamps in order to fit the data according to stackExchange limits. This data does not contain any missing readings. It contains 1248 (26 x 48) observations</p>

<pre><code>data &lt;- structure(c(1642.8, 1467.1, 165.57, 1630.99, 1618.65, 1629.29, 
1598.93, 1839.9, 1604.52, 1606.73, 1473.82, 1669.17, 1698.9, 
2111.21, 2056.41, 3671.29, 2808.01, 1336.15, 794.11, 1212.15, 
377.36, 888.54, 174.58, 218.54, 420.76, 389.58, 397.77, 395.31, 
359.11, 364.8, 376.13, 389.37, 929.5, 1702.38, 519.65, 2452.28, 
1354.45, 1842.96, 725.41, 661.11, 528.44, 733.4, 429.51, 310.47, 
279.72, 407.83, 1791.1, 1754.53, 1536.73, 1608.37, 1432.23, 1401.72, 
1582.14, 1558.75, 1536.24, 1745.59, 1375.61, 1556.71, 1671.12, 
1206.77, 1391.84, 876.23, 1617.3, 1638.99, 1833.61, 1591.42, 
1455, 183.87, 177.55, 184.36, 332.99, 352.95, 425.1, 945.67, 
342.3, 348.45, 227.18, 382.15, 268.91, 335.88, 326.94, 233.23, 
169.71, 179.51, 195.3, 207.23, 1681.9, 1493.32, 941.52, 980.36, 
924.31, 379.02, 1229.89, 1590.21, 1250.92, 1149.24, 1124.04, 
993.78, 883.98, 860.69, 934.17, 969.31, 1049.55, 1104.94, 904.3, 
1220.23, 1183.9, 891.26, 825.33, 787.77, 1060.93, 1029.1, 982.25, 
193.13, 182.65, 181.75, 167.68, 165.89, 291.02, 300.2, 418.4, 
297.66, 231.89, 305.66, 701.18, 338.5, 337.24, 332.11, 332.66, 
187.8, 179.03, 130.22, 177.73, 172.24, 173.45, 334.23, 810.53, 
359.41, 330.23, 333.29, 568.85, 2462.46, 1660.32, 1156.13, 1136.2, 
1189.07, 832.73, 181.91, 185.93, 1076.77, 672.78, 1376.71, 1020.17, 
382.18, 1160.74, 791.36, 1569.4, 817.78, 850.71, 747.21, 826.12, 
1306.46, 506.23, 140.05, 132.6, 304.19, 308.14, 406.13, 290.73, 
188.9, 165.59, 174.56, 145.97, 151.83, 142.29, 443.58, 799.9, 
279.36, 223.88, 221.03, 291.26, 374.97, 431.36, 598.98, 625.82, 
1052.02, 2036.83, 1230.03, 1429.81, 1099.34, 1646.03, 1668.56, 
1631.79, 1604.04, 2849.49, 2998.63, 2476.96, 1601.04, 1216.42, 
2004.2, 1868.51, 1961.91, 1813.35, 1500.22, 1276.94, 1369.29, 
632.43, 238.15, 488.76, 467.6, 330.27, 144.67, 153.36, 924.12, 
1348.18, 799.01, 524.3, 420.5, 264.34, 283.86, 198.95, 206.52, 
217.33, 356.16, 207.83, 197.6, 194.03, 193.01, 249.85, 271.22, 
244.25, 442.5, 660.49, 245.66, 356.13, 443.32, 336.15, 849.74, 
1709.21, 1542.09, 1315.69, 2628.6, 2261.8, 1576.42, 1776.48, 
1239.64, 1401.12, 1106.17, 1378.55, 1315.57, 1141, 1642.63, 2484.13, 
1968.94, 3059.42, 1317.32, 905.05, 484.42, 486.86, 96.79, 183.45, 
173.94, 342, 255.96, 320.54, 106.19, 147.88, 150.77, 176.17, 
344.75, 371.73, 309.46, 237.86, 187.32, 202.61, 292.5, 248.28, 
259.3, 283.67, 365.09, 230.47, 326.15, 350.92, 335.42, 419.39, 
345.31, 1093.22, 1392.87, 1298.11, 919.16, 1654.53, 1045.99, 
558.42, 437.29, 857.9, 758.34, 1220.04, 1390.62, 956.74, 909.93, 
584.67, 409.87, 387.3, 387.93, 1276.28, 871.06, 413.8, 313.2, 
199.5, 330.21, 210.9, 358.28, 352.13, 233.62, 259.18, 123.57, 
255.58, 411.78, 427.65, 318.95, 298.5, 283.23, 279.85, 200.45, 
205.97, 254.24, 307.98, 1090.53, 289.71, 215.14, 286.63, 328.55, 
288.96, 1281.19, 1354.8, 1302.05, 1254.46, 261.95, 270.09, 243.28, 
696.61, 314.27, 241.73, 245.68, 157.74, 222.55, 294.36, 185.46, 
203.49, 182.14, 246.69, 178.26, 397.5, 330.2, 212.02, 248.72, 
265.48, 249.37, 130.59, 248.97, 279.94, 319.07, 358.5, 278.98, 
251.92, 304.66, 455.05, 365.95, 340.93, 287.51, 264.82, 260.18, 
34.35, 35.11, 184.09, 247.18, 160.9, 139.27, 284.96, 296.31, 
252.3, 342.65, 353.03, 380.52, 346.19, 350.06, 218.52, 133.94, 
173.7, 128.26, 167.8, 112.77, 147.8, 129, 170.54, 89.88, 243.08, 
97.61, 190.31, 193.94, 268.17, 233.5, 205.27, 92.29, 167.43, 
168.34, 151.99, 193.84, 379.1, 318.69, 327.28, 487.39, 414.01, 
336.06, 278.02, 168.05, 155.6, 236.4, 264.94, 296.05, 326.46, 
357.43, 356.31, 340.29, 319.81, 312.79, 341.53, 317.36, 309.62, 
440.6, 285.5, 282.06, 288.99, 334.48, 196.54, 144.24, 218.55, 
173.64, 242.29, 251.78, 186.81, 184.36, 141.62, 208.91, 157.53, 
154.03, 139.44, 137.66, 256.75, 1202.05, 177.36, 177.93, 72.83, 
252.9, 231.35, 1090.39, 442.91, 363.12, 248.96, 478.75, 249.64, 
297.29, 227.28, 365.82, 879.7, 488.93, 184.79, 138.13, 151.77, 
123.18, 175.76, 251.84, 208.06, 126.68, 246.3, 307.34, 319.79, 
324.3, 379.6, 309.53, 253.17, 221.91, 228.42, 150.24, 148.59, 
118.79, 86.89, 140.51, 200.43, 212.15, 276.14, 441.81, 125.77, 
152.42, 329.28, 269.21, 177.35, 1106.29, 128.92, 96.35, 63.53, 
520.62, 940.25, 1014.34, 314.99, 390.2, 330.1, 377.04, 341.35, 
342.79, 241.79, 249.9, 391.92, 292.68, 105.02, 179.99, 118.53, 
154.17, 90.53, 206.7, 345.33, 244.75, 291.68, 820.57, 1777.84, 
1805.83, 1753.73, 1416.7, 279.2, 262.82, 1345.88, 467.98, 1136.66, 
170.02, 159.96, 1478.8, 1414.12, 1347.93, 1505.59, 1341.69, 445.53, 
277.59, 1609.61, 1476.45, 244.08, 192.57, 213.55, 439.02, 112.86, 
128.54, 376.09, 251.15, 116.27, 254.82, 302.56, 304.6, 198.5, 
240.05, 219.35, 70.3, 190.96, 211.95, 328.53, 714.62, 3176.56, 
2604.09, 191.65, 145.25, 93.93, 83.6, 81.13, 146.12, 331.75, 
250.24, 1144.53, 1616.47, 1008.7, 316.65, 311.46, 1152.99, 1504.86, 
1543.21, 1081.54, 1428.07, 1358.23, 1349.75, 190.23, 2398.92, 
2196.11, 1466.94, 2249.77, 2150.2, 2542.25, 618.03, 453.22, 880.99, 
1497.86, 440.96, 161.85, 324.88, 434.11, 316.33, 444.66, 359.14, 
277.41, 1237.28, 761.41, 183.53, 309.44, 213.48, 121.64, 346.7, 
149.86, 2060.39, 1102.13, 347.97, 600.24, 912.6, 590.77, 1805.76, 
1673.93, 1573.91, 505.74, 446.76, 1033.41, 1668.68, 1293.9, 383.81, 
1419.99, 1349.4, 711.55, 218.63, 182, 401.93, 1876, 1486.34, 
1543.11, 2313.8, 478.57, 615.19, 542.68, 971.98, 531.11, 766.21, 
489.76, 344.47, 319.86, 321.26, 311.41, 288.67, 310.67, 305.15, 
419.33, 422.84, 950.08, 2188.88, 3454.92, 1989.54, 590.33, 327.05, 
354.78, 578.41, 1583.29, 2016.66, 1481.03, 293.21, 1864.84, 399.65, 
366.76, 357.7, 2074.97, 1626.86, 1133, 1624.61, 1506.93, 628.4, 
1405.68, 217.8, 1223.11, 1356.97, 1171.72, 1182.86, 1642.11, 
2289.02, 814.39, 595.76, 542.78, 1596.41, 884.97, 235.25, 1540.68, 
781.95, 115.71, 1204.98, 718.66, 452.09, 305.58, 444.67, 356.76, 
182.54, 674.47, 153.8, 862.25, 1322.88, 323.33, 1659.64, 496.72, 
304.74, 246.6, 327.12, 239.31, 246.72, 225.72, 234.7, 324.07, 
304.27, 171.86, 97.64, 242.69, 295, 324.53, 513.81, 1100.65, 
1151.77, 231.56, 189.88, 786.3, 1164.87, 676.09, 882.82, 1496.3, 
1027.91, 872.92, 809.1, 840.31, 1302.18, 2055.87, 677.74, 934.66, 
263.91, 186.68, 248.5, 214.62, 371.54, 298, 294.52, 304.86, 1295.77, 
942.5, 305, 265.78, 255.89, 255.63, 151.54, 108.16, 116.81, 100.19, 
224.75, 84.11, 1143.89, 262.15, 784.21, 1728.29, 1506.79, 434.94, 
374.29, 265.43, 560.74, 1651.49, 1063.07, 1054.69, 1298.4, 1261.59, 
1132.75, 692.82, 660.57, 198.25, 97.81, 1258.67, 833.64, 796.35, 
868.76, 999.86, 2240.02, 885.72, 1317.52, 1267.18, 167.93, 133.22, 
364.44, 267.17, 406.13, 412.52, 1036.04, 779.34, 655.43, 1901.2, 
270.18, 266.31, 284.21, 288.66, 135.38, 176.11, 154.86, 160.21, 
146.28, 163.72, 139.75, 278.12, 253.51, 319.62, 396.39, 1662.69, 
1577.09, 1059.71, 241.64, 407.54, 290.49, 846.17, 1325.31, 1418.23, 
1432.5, 1412.6, 1015.87, 1619.88, 1426.58, 1333.32, 1963.35, 
1638.11, 1081.89, 285.56, 1084.58, 2038.77, 1022.39, 1145.92, 
513.87, 107.7, 176.19, 143.77, 374.3, 373.99, 221.76, 148.3, 
331.01, 2323.12, 1502.09, 347.17, 296.7, 306.06, 313.03, 221.69, 
295.35, 301.95, 250.92, 231.54, 140.6, 717.63, 863.5, 402.15, 
1337.78, 1575.44, 1738.49, 1675.57, 1617.66, 1365.58, 242.8, 
286.29, 712.34, 1559.59, 1600.34, 3447.88, 3432.89, 3337.23, 
1472.21, 1323.76, 1265.31, 1221.65, 1312.63, 2016.09, 2972.13, 
1451.67, 735.67, 130.13, 379.87, 162.21, 226.48, 417.18, 357.51, 
346.37, 200.89, 190.15, 276.05, 942.74, 1471.99, 1047.53, 1240.06, 
742.62, 169.34, 144.28, 220.58, 165.23, 344.8, 227.47, 254.64, 
742.37, 1809.01, 436.11, 1692.87, 1697.74, 1474.91, 1635.68, 
1664.2, 1489.85, 1316.4, 1364.07, 1510.02, 1497.89, 1709.17, 
2846.71, 2736.08, 1015.43, 1017.08, 1195.21, 751.18, 523.89, 
199.69, 2148.71, 1151.39, 1182.84, 788.91, 259.31, 146.29, 141.09, 
299.38, 349.53, 404.08, 449.9, 391.77, 251.89, 222.25, 281.04, 
565.4, 371.24, 219.75, 324.45, 227, 157.67, 212.48, 201.69, 140.84, 
220.67, 187.64, 399.79, 157.92, 275.49, 326.99, 1340.51, 1578.68, 
1599.41, 1667.74, 1129.07, 1312.55, 1393.09, 1368.69, 1130.85, 
968.71, 1130.2, 1223.1, 1124.5, 1077.09, 1052.42, 1255.31, 918.21, 
1263.93, 706.21, 3080.29, 1620.18, 1122.48, 750.06, 262.89, 110.02, 
236.83, 413, 227.53, 355, 277.51, 258.03, 368.44, 656.68, 1808.17, 
1493.41, 1272.21, 330.67, 1674.18, 719.45, 1089.68, 784.01, 275.73, 
312.69, 345.11, 761.8, 1020.11, 259.16, 345.98, 270.23, 580.15, 
1303.68, 1659.7, 1732.94, 204.9, 373.58, 373.36, 1381.52, 1437.74, 
1262.78, 1264.6, 1184.54, 1175.12, 857.09, 1428.34, 841.92, 232.47, 
223.22, 473.24, 382.7, 189.84, 1737.48, 1689.34, 378.48, 872.56, 
180.12, 363.03, 301.38, 412.57, 401.17, 387.35, 417.63, 300.61, 
376.82, 284.31, 232.31, 269.96, 188.41, 203.79, 134.88, 193.66, 
57.86, 89.71, 167.66, 60.84, 197.03, 703.66, 1638.7, 1467.03, 
347.22, 1397.23, 1511.92, 1362.25, 1397.18, 1106.19, 826.5, 1033.04, 
1039.46, 584.98, 706.35, 548.07, 373.39, 681.6, 1231.28, 288.94, 
649.36, 79.28, 209.23, 290.75, 304.12, 132.57, 91.2, 355.37, 
197.45, 343.17, 339.2, 284.65, 229, 234.73, 322.36, 323.43, 295.1, 
197.03, 308.17, 223.88, 235.08, 225.16, 172.83, 236.26, 135.17, 
394.89, 479.2, 315.34, 280.9, 282.36, 204.78, 367.24, 1712.29, 
1521, 1686.09, 960.19, 1019.12, 1062.77, 851.88, 1369.98, 689.2, 
580.5, 751.74, 547.67, 556.64, 493.85, 404.15, 428.07, 716.2, 
1442.05, 1045.81, 1497.07, 567.59, 155.07, 537.72, 446.03, 282.57, 
642.88, 409.37, 338.91, 173.89, 358.63, 195.42, 209.95, 186.44, 
152.34, 105.51, 132.26, 82.14, 122.88, 149.38, 211.42, 350.17, 
429.72, 336.4, 982.21, 1436.25, 1726.87, 1830.42, 1282.05, 1293.4, 
1121.01, 946.2, 707.1, 154.53, 767.56, 607.78, 448, 288.23, 270.32, 
223.93, 166.22, 262.45, 223.74, 159.31, 210.44, 257.94, 183.99, 
151.38, 206.11, 193.43, 388.95, 577.98, 304.64, 285.13, 256.59, 
420.26, 289.34, 356.02, 358.08, 325.22, 275.95, 164.46, 213.23, 
142.99, 221.66, 270.61, 206.56, 213.68, 254.33, 250.15, 267.99, 
403.95, 671.2, 1574.11, 396.34, 477.88, 631.08, 618.25, 1366.88, 
298.19, 287.05, 290.38, 332.44, 235.9, 229.79, 831.6, 1320.86, 
477.31, 944.84, 547.33, 411.21, 705.43, 873.49, 572.81, 585.36, 
1229.69, 701.01, 653.49, 74.81, 162.47, 179.54, 330.27, 544.51, 
332.41, 296.66, 130.66, 1055.61, 556.79, 265.43, 383.44, 398.22, 
362.66, 223.99, 130.35, 193.67, 217.68, 273.3, 247.84, 161.66, 
320.08, 322.52, 274.61, 811.44, 353.85, 323.41, 383.61, 389.5
), .Dim = c(1248L, 1L), .Dimnames = list(NULL, ""power""))
</code></pre>
"
"0.0878749550327494","0.0851256530758749","65865","<p>This is the dataset on which I am working currently, which is production data.</p>

<p>Data:</p>

<pre><code>&gt; test.ts
        Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct    Nov    Dec
1990                                                            0.0   10.8  180.0  418.2
1991  561.9  517.9  531.3  448.1  254.9   49.0    3.2    0.0    0.0   10.4  207.7  526.2
1992  597.2  581.5  596.4  518.4  378.3  209.9   32.1    0.0    0.0    7.9  166.7  571.7
1993  650.4  578.5  565.7  280.5   35.3    0.7    0.0    0.0    0.0   39.5  289.2  638.9
1994  643.8  533.8  410.9  159.3    0.0    0.0    0.0    0.0    0.0   38.3  322.8  684.9
1995  695.6  665.8  640.2  415.4  113.0   20.7   12.1    0.0    0.0   13.6  316.3  677.5
1996  754.5  683.4  719.6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  678.0
1997  774.5  808.1  847.9  677.8  208.7    9.9    0.0    0.0    0.0    5.2  296.4  794.9
1998  952.0  873.1  732.0  264.6    3.9    0.0    0.0    0.0    0.0    0.0  245.8  833.0
1999  843.5  812.3  708.3  275.2   10.8    0.0    0.0    0.0    0.0    5.4  300.4  884.9
2000  949.0  898.6  892.7  474.7  130.0   19.8    0.0    0.0    0.0    8.5  367.2 1000.8
2001 1092.1  987.7  864.3  392.8   41.8    0.0    0.0    0.0    0.0    7.0  425.0  968.0
2002  983.6  925.0 1018.0  696.0  209.0   26.0    0.0    0.0    0.0    0.0   63.0  823.0
2003 1066.0  930.0  929.0 1071.0  614.0  125.0   29.0    0.0    0.0    0.5  300.0 1005.8
2004 1043.0 1051.9  863.2  279.1    8.0    0.0    0.0    0.0    0.0   67.8  597.1 1120.3
2005 1087.9 1015.9  855.7  292.9    0.8    0.0    0.0    0.0    0.0   78.3  683.3 1139.2
2006 1185.5 1162.1 1131.3  386.9   16.4    1.2    0.0    0.0    0.0    7.1  728.5 1493.0
2007 1572.9 1341.0 1652.9 1279.3  386.4   14.3    0.0    0.0    0.0    0.0  102.5 1570.1
2008 1864.7 1786.7 1523.9  422.7   48.1    0.8    0.0    0.0    0.0    0.0  192.4 1556.9
2009 1260.8  763.8  284.1    6.1    0.0    0.0    0.0    0.0    0.0    0.0   73.8 1495.6
2010 1280.8 1248.8  887.2  185.6    7.3    0.0    0.0    0.0    0.0    0.8  182.0 1524.9
2011 1461.5 1497.7 1111.5  108.6    0.0    0.0    0.0    0.0    0.0    2.9  519.3 1652.5
2012 1552.5 1563.2 1380.4  295.2    7.7    0.0    0.0    0.0    0.0    0.1  225.0 1677.6
2013 1686.2 1420.0 1691.0  795.0    0.0  
</code></pre>

<p>I used auto.arima() from forecast package.</p>

<p>Code:</p>

<pre><code>ARIMAfit &lt;- auto.arima(test.ts)
test.ar &lt;- forecast(ARIMAfit, level=70, h=12)
</code></pre>

<p>Following is the output I got</p>

<p>Output:</p>

<pre><code>&gt; test.ar
         Point Forecast       Lo 70     Hi 70
Jun 2013      -4.429870  -186.37952  177.5198
Jul 2013      -4.429870  -261.74553  252.8858
Aug 2013      -4.429870  -319.57590  310.7162
Sep 2013      -4.429870  -368.32916  359.4694
Oct 2013      -3.416802  -410.26858  403.4350
Nov 2013     296.121405  -149.56239  741.8052
Dec 2013    1505.197792  1023.80428 1986.5913
Jan 2014    1477.195886   962.56457 1991.8272
Feb 2014    1327.574562   781.72562 1873.4235
Mar 2014    1423.251183   847.87588 1998.6265
Apr 2014     550.206881   -53.25183 1153.6656
May 2014      -1.892754  -632.18482  628.3993
</code></pre>

<p>Questions:</p>

<ol>
<li><p>Why does the output shows negative value, when there has been no negative value in the historic data? The data is of production, which cannot be negative.</p></li>
<li><p>Is there any other model class which handles ""zero"" values appropriately?</p></li>
</ol>

<p>Kindly help.</p>
"
"0.12427395320024","0.120385853085769","194400","<p>I have data on newspaper articles about police cracking down on crime, and data on crimes reported to the police. The data are daily, covering about six months and the same city.</p>

<p>I want to try an ARIMA model on these data to forecast the deterrent effect of newspaper coverage of law enforcement operations on crime.</p>

<p>I'm using the most recent forecast package in R, but as I have little experience with this sort of stuff I am unsure of the results. I am hoping that someone more knowledgeable can tell me if I need to massage the data for the analysis. I have read up on the forecast package and I'm unsure of how much of the hard work the code below is actually doing for me.</p>

<pre><code>rate &lt;- ts(crime$rate,frequency=7)
news &lt;- ts(crime$news,frequency=7)
fit &lt;- auto.arima(rate, xreg=news)
summary(fit)
</code></pre>

<p>My understanding, based on the answer to <a href=""http://stats.stackexchange.com/questions/14742/auto-arima-with-daily-data-how-to-capture-seasonality-periodicity"">this</a> question, is that the <code>frequency=7</code> here adjusts the data for daily seasonality (which is present in both datasets. There are fewer articles published and fewer crimes reported on Sundays). Are further adjustments required for these seasonal effects?</p>

<p>Output:</p>

<pre><code>ARIMA(2,1,0)(1,0,0)[7]                    

Coefficients:
          ar1      ar2    sar1         m2
      -0.4469  -0.2135  0.6080  1598.3622
s.e.   0.0907   0.0865  0.0727   612.6812

sigma^2 estimated as 1.93e+09:  log likelihood=-1635.99
AIC=3281.99   AICc=3282.45   BIC=3296.55

Training set error measures:
                    ME     RMSE      MAE       MPE     MAPE      MASE         ACF1
Training set -601.4693 43614.13 34018.82 -4.343747 20.00794 0.8864897 -0.004283847
</code></pre>

<p>Assuming that there is no missing data, and if the seasonality is already adjusted for, are there other things I need to check?</p>
"
"0.234333213420665","0.255376959227625","89531","<p>I'm expanding a question I posed earlier because I think it was lacking detail. </p>

<p>I'm attempting to forecast daily demand for a restaurant that sells take away food, primarily to office workers on their lunch breaks. They are located in the downtown core of a major city.</p>

<p>They are only open on workdays - no holidays, no weekends. I'm familiar with models that take into account seasonality and trend - Holt-Winters triple exponential smoothing, for example. I'm also familiar with models that take into account complex seasonality and trend - the TBATS package for R, for example.</p>

<p>My problem is that I've identified 8 components that determine sales on a given day:</p>

<ol>
<li>The yearly seasonal component. Sales are lower in the summer, for example, when many office workers are on vacation.</li>
<li>The weekly component. Sales very obviously peak on Thursdays (in the absence of other effects - see below)</li>
<li>The <em>Friday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the coming Friday is a holiday. Wednesday will typically have higher sales, for example.</li>
<li>The <em>Post-Friday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the week before was shortened due to the Friday being a holiday.</li>
<li>The <em>Monday-Long-Weekend-Effect</em>. The weekly sales pattern changes in week $t$ if the Monday in week $t+1$ is a holiday. For example, sales are much lower on Fridays preceding Monday-Long-Weekends. Presumably people are leaving the office early and skipping lunch.</li>
<li>The <em>Post-Monday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the week is shortened due to the Monday being a holiday.</li>
<li>The trend component. </li>
<li>The noise component.</li>
</ol>

<p>If holidays fell on the same date every year, then the ""long-weekend-effects"" would be captured in the yearly seasonal component. However, they don't. </p>

<p>My first thought was to include dummy variables. For example, let $X_{M+1}$ be the ""Monday-long-weekend-effect"" component, and $\beta_{M+1}$ be the associated coefficient, for a given day. Then for the Friday preceding a Monday-Long-Weekend, $X_{M+1}=1$, and for a Friday not preceding a Monday-Long-Weekend, $X=0$.</p>

<p>I'm only using three years of data, so it would be easy for me to change the $X_{M+1}$ values to 0 or 1 by hand for each year. However, I don't know how to include such dummy variables in models like those that I've mentioned.</p>

<p>Any input as to a model that can take into account the components I've mentioned would be greatly appreciated. It seems like I need to capture moving-holiday-effects, day-of-the-week effects, seasonal patterns, and trend, all in one.</p>

<p><strong>Question: Is there a model I can use that can be implemented in R and take into account the components I've listed?</strong></p>

<p><em>My background: I'm a forth year mathematics and economics student. I've also taken statistics classes, and I'm using R to perform my analysis. This is for a final report for a forth year data analysis class.</em></p>
"
"NaN","NaN","119931","<p>How can I in R fit a time series, $x_t$, with external regressors, $v_t$, and an autoregressive error? This time series model is given as follows,
$x_t = \beta v_t + \epsilon_t$ where $\epsilon_t = w_t + \sum_{i = 1}^p \gamma_i\epsilon_{t - i}$ and $w_t \sim N(0, \sigma^2)$.</p>
"
"0.263624865098248","0.255376959227625","213192","<p>I have half-hourly electricity data of several homes for a duration of one month. Also, I have ambient temperature at same sampling rate. Now, I need to make half-hourly forecasts using historical electricity data and forecasted temperature for coming day.</p>

<p>Currently, I am facing a problem with seasonality, i.e., in some homes I observe (via visual inspection) day-wise seasonality and in some other homes no pattern is found. <a href=""http://stats.stackexchange.com/a/212797/60072"">Stephan</a> suggested to check for weekly seasonality as well, but visually I do not find any. So, I thought to try models with different forced seasonalities to find the prediction accuracy. I can think of two options:</p>

<ol>
<li>Model_1 with daily seasonality (frequency = 48 observations)</li>
<li>Model_2 with weekly and daily seasonality (48, 7*48)</li>
</ol>

<p>Keeping the above approach in mind, I am facing the following issue:</p>

<ol>
<li>For Model_1, I can use <code>auto.arima()</code> with <code>xreg</code> option to specify an extra <code>temperature</code> predictor. However, this only works for a single seasonality.</li>
<li>For Model_2, I tried to use the <code>tbats()</code> forecasting function, which models multiple seasonalities, but does not allow extra predictor variables. </li>
</ol>

<p>Is there a forecasting function which allows <em>both</em> multiple predictors and multiple seasonalities?</p>

<p>Here is the electricity data of one month. </p>

<pre><code>    data &lt;- structure(c(1642.8, 1467.1, 165.57, 1630.99, 1618.65, 1629.29, 
        1598.93, 1839.9, 1604.52, 1606.73, 1473.82, 1669.17, 1698.9, 
        2111.21, 2056.41, 3671.29, 2808.01, 1336.15, 794.11, 1212.15, 
        377.36, 888.54, 174.58, 218.54, 420.76, 389.58, 397.77, 395.31, 
        359.11, 364.8, 376.13, 389.37, 929.5, 1702.38, 519.65, 2452.28, 
        1354.45, 1842.96, 725.41, 661.11, 528.44, 733.4, 429.51, 310.47, 
        279.72, 407.83, 1791.1, 1754.53, 1536.73, 1608.37, 1432.23, 1401.72, 
        1582.14, 1558.75, 1536.24, 1745.59, 1375.61, 1556.71, 1671.12, 
        1206.77, 1391.84, 876.23, 1617.3, 1638.99, 1833.61, 1591.42, 
        1455, 183.87, 177.55, 184.36, 332.99, 352.95, 425.1, 945.67, 
        342.3, 348.45, 227.18, 382.15, 268.91, 335.88, 326.94, 233.23, 
        169.71, 179.51, 195.3, 207.23, 1681.9, 1493.32, 941.52, 980.36, 
        924.31, 379.02, 1229.89, 1590.21, 1250.92, 1149.24, 1124.04, 
        993.78, 883.98, 860.69, 934.17, 969.31, 1049.55, 1104.94, 904.3, 
        1220.23, 1183.9, 891.26, 825.33, 787.77, 1060.93, 1029.1, 982.25, 
        193.13, 182.65, 181.75, 167.68, 165.89, 291.02, 300.2, 418.4, 
        297.66, 231.89, 305.66, 701.18, 338.5, 337.24, 332.11, 332.66, 
        187.8, 179.03, 130.22, 177.73, 172.24, 173.45, 334.23, 810.53, 
        359.41, 330.23, 333.29, 568.85, 2462.46, 1660.32, 1156.13, 1136.2, 
        1189.07, 832.73, 181.91, 185.93, 1076.77, 672.78, 1376.71, 1020.17, 
        382.18, 1160.74, 791.36, 1569.4, 817.78, 850.71, 747.21, 826.12, 
        1306.46, 506.23, 140.05, 132.6, 304.19, 308.14, 406.13, 290.73, 
        188.9, 165.59, 174.56, 145.97, 151.83, 142.29, 443.58, 799.9, 
        279.36, 223.88, 221.03, 291.26, 374.97, 431.36, 598.98, 625.82, 
        1052.02, 2036.83, 1230.03, 1429.81, 1099.34, 1646.03, 1668.56, 
        1631.79, 1604.04, 2849.49, 2998.63, 2476.96, 1601.04, 1216.42, 
        2004.2, 1868.51, 1961.91, 1813.35, 1500.22, 1276.94, 1369.29, 
        632.43, 238.15, 488.76, 467.6, 330.27, 144.67, 153.36, 924.12, 
        1348.18, 799.01, 524.3, 420.5, 264.34, 283.86, 198.95, 206.52, 
        217.33, 356.16, 207.83, 197.6, 194.03, 193.01, 249.85, 271.22, 
        244.25, 442.5, 660.49, 245.66, 356.13, 443.32, 336.15, 849.74, 
        1709.21, 1542.09, 1315.69, 2628.6, 2261.8, 1576.42, 1776.48, 
        1239.64, 1401.12, 1106.17, 1378.55, 1315.57, 1141, 1642.63, 2484.13, 
        1968.94, 3059.42, 1317.32, 905.05, 484.42, 486.86, 96.79, 183.45, 
        173.94, 342, 255.96, 320.54, 106.19, 147.88, 150.77, 176.17, 
        344.75, 371.73, 309.46, 237.86, 187.32, 202.61, 292.5, 248.28, 
        259.3, 283.67, 365.09, 230.47, 326.15, 350.92, 335.42, 419.39, 
        345.31, 1093.22, 1392.87, 1298.11, 919.16, 1654.53, 1045.99, 
        558.42, 437.29, 857.9, 758.34, 1220.04, 1390.62, 956.74, 909.93, 
        584.67, 409.87, 387.3, 387.93, 1276.28, 871.06, 413.8, 313.2, 
        199.5, 330.21, 210.9, 358.28, 352.13, 233.62, 259.18, 123.57, 
        255.58, 411.78, 427.65, 318.95, 298.5, 283.23, 279.85, 200.45, 
        205.97, 254.24, 307.98, 1090.53, 289.71, 215.14, 286.63, 328.55, 
        288.96, 1281.19, 1354.8, 1302.05, 1254.46, 261.95, 270.09, 243.28, 
        696.61, 314.27, 241.73, 245.68, 157.74, 222.55, 294.36, 185.46, 
        203.49, 182.14, 246.69, 178.26, 397.5, 330.2, 212.02, 248.72, 
        265.48, 249.37, 130.59, 248.97, 279.94, 319.07, 358.5, 278.98, 
        251.92, 304.66, 455.05, 365.95, 340.93, 287.51, 264.82, 260.18, 
        34.35, 35.11, 184.09, 247.18, 160.9, 139.27, 284.96, 296.31, 
        252.3, 342.65, 353.03, 380.52, 346.19, 350.06, 218.52, 133.94, 
        173.7, 128.26, 167.8, 112.77, 147.8, 129, 170.54, 89.88, 243.08, 
        97.61, 190.31, 193.94, 268.17, 233.5, 205.27, 92.29, 167.43, 
        168.34, 151.99, 193.84, 379.1, 318.69, 327.28, 487.39, 414.01, 
        336.06, 278.02, 168.05, 155.6, 236.4, 264.94, 296.05, 326.46, 
        357.43, 356.31, 340.29, 319.81, 312.79, 341.53, 317.36, 309.62, 
        440.6, 285.5, 282.06, 288.99, 334.48, 196.54, 144.24, 218.55, 
        173.64, 242.29, 251.78, 186.81, 184.36, 141.62, 208.91, 157.53, 
        154.03, 139.44, 137.66, 256.75, 1202.05, 177.36, 177.93, 72.83, 
        252.9, 231.35, 1090.39, 442.91, 363.12, 248.96, 478.75, 249.64, 
        297.29, 227.28, 365.82, 879.7, 488.93, 184.79, 138.13, 151.77, 
        123.18, 175.76, 251.84, 208.06, 126.68, 246.3, 307.34, 319.79, 
        324.3, 379.6, 309.53, 253.17, 221.91, 228.42, 150.24, 148.59, 
        118.79, 86.89, 140.51, 200.43, 212.15, 276.14, 441.81, 125.77, 
        152.42, 329.28, 269.21, 177.35, 1106.29, 128.92, 96.35, 63.53, 
        520.62, 940.25, 1014.34, 314.99, 390.2, 330.1, 377.04, 341.35, 
        342.79, 241.79, 249.9, 391.92, 292.68, 105.02, 179.99, 118.53, 
        154.17, 90.53, 206.7, 345.33, 244.75, 291.68, 820.57, 1777.84, 
        1805.83, 1753.73, 1416.7, 279.2, 262.82, 1345.88, 467.98, 1136.66, 
        170.02, 159.96, 1478.8, 1414.12, 1347.93, 1505.59, 1341.69, 445.53, 
        277.59, 1609.61, 1476.45, 244.08, 192.57, 213.55, 439.02, 112.86, 
        128.54, 376.09, 251.15, 116.27, 254.82, 302.56, 304.6, 198.5, 
        240.05, 219.35, 70.3, 190.96, 211.95, 328.53, 714.62, 3176.56, 
        2604.09, 191.65, 145.25, 93.93, 83.6, 81.13, 146.12, 331.75, 
        250.24, 1144.53, 1616.47, 1008.7, 316.65, 311.46, 1152.99, 1504.86, 
        1543.21, 1081.54, 1428.07, 1358.23, 1349.75, 190.23, 2398.92, 
        2196.11, 1466.94, 2249.77, 2150.2, 2542.25, 618.03, 453.22, 880.99, 
        1497.86, 440.96, 161.85, 324.88, 434.11, 316.33, 444.66, 359.14, 
        277.41, 1237.28, 761.41, 183.53, 309.44, 213.48, 121.64, 346.7, 
        149.86, 2060.39, 1102.13, 347.97, 600.24, 912.6, 590.77, 1805.76, 
        1673.93, 1573.91, 505.74, 446.76, 1033.41, 1668.68, 1293.9, 383.81, 
        1419.99, 1349.4, 711.55, 218.63, 182, 401.93, 1876, 1486.34, 
        1543.11, 2313.8, 478.57, 615.19, 542.68, 971.98, 531.11, 766.21, 
        489.76, 344.47, 319.86, 321.26, 311.41, 288.67, 310.67, 305.15, 
        419.33, 422.84, 950.08, 2188.88, 3454.92, 1989.54, 590.33, 327.05, 
        354.78, 578.41, 1583.29, 2016.66, 1481.03, 293.21, 1864.84, 399.65, 
        366.76, 357.7, 2074.97, 1626.86, 1133, 1624.61, 1506.93, 628.4, 
        1405.68, 217.8, 1223.11, 1356.97, 1171.72, 1182.86, 1642.11, 
        2289.02, 814.39, 595.76, 542.78, 1596.41, 884.97, 235.25, 1540.68, 
        781.95, 115.71, 1204.98, 718.66, 452.09, 305.58, 444.67, 356.76, 
        182.54, 674.47, 153.8, 862.25, 1322.88, 323.33, 1659.64, 496.72, 
        304.74, 246.6, 327.12, 239.31, 246.72, 225.72, 234.7, 324.07, 
        304.27, 171.86, 97.64, 242.69, 295, 324.53, 513.81, 1100.65, 
        1151.77, 231.56, 189.88, 786.3, 1164.87, 676.09, 882.82, 1496.3, 
        1027.91, 872.92, 809.1, 840.31, 1302.18, 2055.87, 677.74, 934.66, 
        263.91, 186.68, 248.5, 214.62, 371.54, 298, 294.52, 304.86, 1295.77, 
        942.5, 305, 265.78, 255.89, 255.63, 151.54, 108.16, 116.81, 100.19, 
        224.75, 84.11, 1143.89, 262.15, 784.21, 1728.29, 1506.79, 434.94, 
        374.29, 265.43, 560.74, 1651.49, 1063.07, 1054.69, 1298.4, 1261.59, 
        1132.75, 692.82, 660.57, 198.25, 97.81, 1258.67, 833.64, 796.35, 
        868.76, 999.86, 2240.02, 885.72, 1317.52, 1267.18, 167.93, 133.22, 
        364.44, 267.17, 406.13, 412.52, 1036.04, 779.34, 655.43, 1901.2, 
        270.18, 266.31, 284.21, 288.66, 135.38, 176.11, 154.86, 160.21, 
        146.28, 163.72, 139.75, 278.12, 253.51, 319.62, 396.39, 1662.69, 
        1577.09, 1059.71, 241.64, 407.54, 290.49, 846.17, 1325.31, 1418.23, 
        1432.5, 1412.6, 1015.87, 1619.88, 1426.58, 1333.32, 1963.35, 
        1638.11, 1081.89, 285.56, 1084.58, 2038.77, 1022.39, 1145.92, 
        513.87, 107.7, 176.19, 143.77, 374.3, 373.99, 221.76, 148.3, 
        331.01, 2323.12, 1502.09, 347.17, 296.7, 306.06, 313.03, 221.69, 
        295.35, 301.95, 250.92, 231.54, 140.6, 717.63, 863.5, 402.15, 
        1337.78, 1575.44, 1738.49, 1675.57, 1617.66, 1365.58, 242.8, 
        286.29, 712.34, 1559.59, 1600.34, 3447.88, 3432.89, 3337.23, 
        1472.21, 1323.76, 1265.31, 1221.65, 1312.63, 2016.09, 2972.13, 
        1451.67, 735.67, 130.13, 379.87, 162.21, 226.48, 417.18, 357.51, 
        346.37, 200.89, 190.15, 276.05, 942.74, 1471.99, 1047.53, 1240.06, 
        742.62, 169.34, 144.28, 220.58, 165.23, 344.8, 227.47, 254.64, 
        742.37, 1809.01, 436.11, 1692.87, 1697.74, 1474.91, 1635.68, 
        1664.2, 1489.85, 1316.4, 1364.07, 1510.02, 1497.89, 1709.17, 
        2846.71, 2736.08, 1015.43, 1017.08, 1195.21, 751.18, 523.89, 
        199.69, 2148.71, 1151.39, 1182.84, 788.91, 259.31, 146.29, 141.09, 
        299.38, 349.53, 404.08, 449.9, 391.77, 251.89, 222.25, 281.04, 
        565.4, 371.24, 219.75, 324.45, 227, 157.67, 212.48, 201.69, 140.84, 
        220.67, 187.64, 399.79, 157.92, 275.49, 326.99, 1340.51, 1578.68, 
        1599.41, 1667.74, 1129.07, 1312.55, 1393.09, 1368.69, 1130.85, 
        968.71, 1130.2, 1223.1, 1124.5, 1077.09, 1052.42, 1255.31, 918.21, 
        1263.93, 706.21, 3080.29, 1620.18, 1122.48, 750.06, 262.89, 110.02, 
        236.83, 413, 227.53, 355, 277.51, 258.03, 368.44, 656.68, 1808.17, 
        1493.41, 1272.21, 330.67, 1674.18, 719.45, 1089.68, 784.01, 275.73, 
        312.69, 345.11, 761.8, 1020.11, 259.16, 345.98, 270.23, 580.15, 
        1303.68, 1659.7, 1732.94, 204.9, 373.58, 373.36, 1381.52, 1437.74, 
        1262.78, 1264.6, 1184.54, 1175.12, 857.09, 1428.34, 841.92, 232.47, 
        223.22, 473.24, 382.7, 189.84, 1737.48, 1689.34, 378.48, 872.56, 
        180.12, 363.03, 301.38, 412.57, 401.17, 387.35, 417.63, 300.61, 
        376.82, 284.31, 232.31, 269.96, 188.41, 203.79, 134.88, 193.66, 
        57.86, 89.71, 167.66, 60.84, 197.03, 703.66, 1638.7, 1467.03, 
        347.22, 1397.23, 1511.92, 1362.25, 1397.18, 1106.19, 826.5, 1033.04, 
        1039.46, 584.98, 706.35, 548.07, 373.39, 681.6, 1231.28, 288.94, 
        649.36, 79.28, 209.23, 290.75, 304.12, 132.57, 91.2, 355.37, 
        197.45, 343.17, 339.2, 284.65, 229, 234.73, 322.36, 323.43, 295.1, 
        197.03, 308.17, 223.88, 235.08, 225.16, 172.83, 236.26, 135.17, 
        394.89, 479.2, 315.34, 280.9, 282.36, 204.78, 367.24, 1712.29, 
        1521, 1686.09, 960.19, 1019.12, 1062.77, 851.88, 1369.98, 689.2, 
        580.5, 751.74, 547.67, 556.64, 493.85, 404.15, 428.07, 716.2, 
        1442.05, 1045.81, 1497.07, 567.59, 155.07, 537.72, 446.03, 282.57, 
        642.88, 409.37, 338.91, 173.89, 358.63, 195.42, 209.95, 186.44, 
        152.34, 105.51, 132.26, 82.14, 122.88, 149.38, 211.42, 350.17, 
        429.72, 336.4, 982.21, 1436.25, 1726.87, 1830.42, 1282.05, 1293.4, 
        1121.01, 946.2, 707.1, 154.53, 767.56, 607.78, 448, 288.23, 270.32, 
        223.93, 166.22, 262.45, 223.74, 159.31, 210.44, 257.94, 183.99, 
        151.38, 206.11, 193.43, 388.95, 577.98, 304.64, 285.13, 256.59, 
        420.26, 289.34, 356.02, 358.08, 325.22, 275.95, 164.46, 213.23, 
        142.99, 221.66, 270.61, 206.56, 213.68, 254.33, 250.15, 267.99, 
        403.95, 671.2, 1574.11, 396.34, 477.88, 631.08, 618.25, 1366.88, 
        298.19, 287.05, 290.38, 332.44, 235.9, 229.79, 831.6, 1320.86, 
        477.31, 944.84, 547.33, 411.21, 705.43, 873.49, 572.81, 585.36, 
        1229.69, 701.01, 653.49, 74.81, 162.47, 179.54, 330.27, 544.51, 
        332.41, 296.66, 130.66, 1055.61, 556.79, 265.43, 383.44, 398.22, 
        362.66, 223.99, 130.35, 193.67, 217.68, 273.3, 247.84, 161.66, 
        320.08, 322.52, 274.61, 811.44, 353.85, 323.41, 383.61, 389.5
        ), .Dim = c(1248L, 1L), .Dimnames = list(NULL, ""power""))
</code></pre>

<p>Temperature predictor values are:</p>

<pre><code>temp&lt;-structure(c(31, 31, 31, 31, 30, 29, 28, 27.5, 27, 26, 26, 26, 
26, 26, 26, 28, 29, 29, 30, 31, 32, 33, 33, 34, 34, 35, 36, 36, 
36.5, 37, 38, 38, 39, 39, 39, 38, 37, 36, 36, 35, 34, 33, 33, 
33, 32.5, 32, 32, 31, 30, 26, 24, 24, 24, 24, 24, 24, 25, 25, 
25, 25, 26, 26, 26, 27, 28.3, 29.7, 31, 32, 33, 33, 34, 34, 34, 
35, 35, 35, 36, 37, 38, 39, 39, 39, 39, 38.5, 38, 37, 36, 35, 
34, 34, 33, 32.5, 32, 30, 30, 30, 28, 28, 27, 25, 25, 24, 24, 
24, 24, 24, 25, 25, 25, 25, 25, 27, 28, 30, 31, 33, 34, 37, 37, 
38, 38, 39, 39.3, 39.7, 40, 40, 40, 40, 41, 40, 40, 38, 38, 36, 
35, 34, 33, 31, 31, 30, 31, 30, 30, 29, 28, 28, 28, 27, 27, 28, 
27, 24, 24, 24, 23, 23, 23, 24, 24, 26, 27, 28, 29.7, 31.3, 33, 
34, 35.3, 36.7, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 
39, 38, 37, 36, 34, 33, 32, 32, 30, 28.5, 27, 27, 27, 27, 27, 
26, 26, 26, 25.3, 24.7, 24, 24, 24, 24, 23, 23, 24.5, 26, 28, 
29.7, 31.3, 33, 34.5, 36, 37, 38, 39, 40, 40, 40.3, 40.7, 41, 
41, 41, 41, 41, 41, 41, 39.5, 38, 37, 37, 35, 34, 34, 33, 33, 
33, 32, 31, 31, 30, 30, 30, 29, 29, 28.5, 28, 28, 28, 29, 29, 
28.5, 28, 29, 29, 30, 31, 31, 32.5, 34, 35, 37, 39.5, 42, 42, 
42, 42, 42, 43, 44, 45, 45, 44, 43, 42, 42, 41, 39, 36.5, 34, 
34, 33, 33, 33, 33, 33, 33, 32, 32, 32, 32, 31.5, 31, 30, 30, 
30, 29, 29, 28, 27, 28, 29, 30, 31, 32.5, 34, 34, 35, 37, 40, 
41, 41, 42, 42, 42.5, 43, 43, 43, 42, 42, 43, 42, 42, 41, 40, 
39, 37.5, 36, 35, 35, 34, 34, 33, 32, 32, 32, 32, 31, 31, 31, 
31, 31, 28, 28, 28, 28, 28, 28.5, 29, 30, 30.5, 31, 32, 33.5, 
35, 36, 37, 38, 39, 38, 39, 40, 41, 42, 42, 43, 42, 42, 42, 41, 
41, 40, 39, 38, 37, 36, 35, 34, 34, 33, 32, 32, 31, 30.5, 30, 
30, 30, 30, 29, 29, 29, 28, 27, 27, 27, 27, 28.5, 30, 31.5, 33, 
34, 35, 36, 37, 38, 38, 39, 40, 41, 42, 42, 42, 42, 42, 42, 42, 
42, 42, 42, 42, 40, 40, 39, 37, 37, 36.5, 36, 35, 34, 34, 33, 
33, 32, 32, 32, 32, 32, 32, 31, 31, 30, 30, 30, 30, 31, 31, 31, 
32, 33.5, 35, 35.5, 36, 37, 37, 38, 39, 40, 41, 41, 42, 42, 42, 
43, 43, 42, 41, 41, 40, 38, 36, 35, 34, 34, 35, 36, 36, 35, 35, 
33, 33, 32, 31, 31, 30, 30, 29, 29, 29, 29, 29, 31, 31.5, 32, 
31, 30, 31, 32, 33, 34, 34, 35, 36, 36, 37, 37, 38, 38, 38, 39, 
39, 39, 39, 39, 39, 39, 38, 37, 37, 37, 37, 35, 33, 31, 31, 31, 
30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 28, 28, 27, 27, 26, 29, 
29, 30, 31, 32, 32, 33, 33, 33, 34, 34, 35, 35, 37, 37, 37, 37, 
37, 38, 38, 38, 38, 37, 37, 36, 35, 34.5, 34, 34, 33, 33, 32, 
32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 30, 30, 29.5, 29, 
29, 30, 30, 31, 32, 32, 33, 33.5, 34, 34, 34, 30.7, 27.3, 24, 
24, 24, 24.7, 25.3, 26, 27, 28, 29, 29, 29, 28, 28, 27, 27, 26, 
25, 25, 25, 24.5, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 
23, 23, 23, 24, 24, 24, 25, 25, 26, 28, 29, 30, 31, 31, 32, 33, 
34, 35, 35, 36, 37, 37, 37, 37, 37, 37, 37, 35, 34, 33, 33, 33, 
32, 31.7, 31.3, 31, 31, 31, 31, 30, 30, 30, 30, 28, 28, 27, 27, 
26, 26, 25, 25, 25, 25, 25, 26, 27, 28, 29.7, 31.3, 33, 34, 34, 
35, 36, 37, 36, 36, 35, 35, 34, 32, 32, 33, 31.3, 29.7, 28, 28, 
29, 29, 29, 29, 28, 27.5, 27, 26.5, 26, 26, 26, 26, 26, 26, 25, 
25, 25, 25, 25, 24, 24, 23, 23, 23, 24.5, 26, 27, 28, 29, 30, 
31, 32, 32, 32, 33, 34, 35, 36, 36, 36, 36, 36, 36, 36, 37, 37, 
37, 36, 36, 34, 34, 33, 32, 32, 31, 31, 30, 28, 28, 28, 28, 27, 
27, 27, 27, 27, 27, 27, 27, 26.5, 26, 26, 26, 26, 27, 28.8, 30.5, 
32.2, 34, 34, 34, 35, 36, 37, 38, 38, 38, 39, 40, 39, 39, 39, 
40, 40, 40, 39, 39, 38, 37, 35, 35, 34, 34, 34, 33, 32, 32, 32, 
32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 33, 34, 
35, 37, 37, 37.3, 37.7, 38, 39, 40, 43, 43, 43, 44, 44, 43, 43, 
43, 43, 43, 43, 42, 41, 40, 39, 38, 37, 36, 35, 35, 34, 34, 34, 
34, 34, 33, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 34, 
35, 36, 37, 38, 38, 40, 40.5, 41, 42, 42, 42, 42, 41, 40, 40, 
39, 39, 37, 32, 27, 26, 26, 27, 28, 27, 27, 27, 26, 25, 25, 25, 
25, 25, 25, 25, 25, 25, 25, 25, 24, 23, 23, 22.5, 22, 22, 23, 
24, 25, 26.3, 27.7, 29, 30, 32, 33, 34, 36, 37, 37, 39, 40, 40.5, 
41, 42, 42, 42, 43, 43, 42, 41.5, 41, 39, 38, 35, 35, 35, 34, 
33, 33, 33, 33, 32, 32, 31, 31, 30, 30, 30, 29, 29, 28, 28, 28, 
28.5, 29, 29, 29, 30, 32, 32, 34, 35, 37, 38, 39, 40, 41, 42, 
42, 43, 44, 44, 44, 45, 45, 45, 44, 43, 43, 42, 40, 38.5, 37, 
35, 35, 34, 34, 33, 34, 33, 33, 33, 33, 32, 32, 31, 32, 31, 30, 
29, 29, 29, 29, 30, 30, 31, 32.7, 34.3, 36, 37, 38, 39, 40, 41, 
41, 42, 42, 43, 44, 44, 44, 44, 44, 43.5, 43, 43, 42, 42, 40, 
38, 38, 37, 36, 36, 36, 36, 35, 35, 34, 34, 34, 34, 32, 32, 31.5, 
31, 31, 30, 30, 30, 30, 31, 33, 34, 35, 37, 38, 39, 40, 40, 42, 
42, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 42, 40, 40, 
40, 39, 38, 38, 38, 37, 37, 36, 36, 35.5, 35, 34.5, 34, 33, 33, 
32, 32, 31, 30, 30, 30, 31, 32, 33.5, 35, 36.5, 38, 38, 39, 40, 
41, 41, 41, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 
42, 40.5, 39, 38, 38, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 
35, 35, 34, 34, 34, 32, 32, 31, 31, 33, 34, 35, 35, 36, 38, 39, 
40, 41, 42, 42.5, 43, 43, 44, 44, 45, 45, 45, 45, 45, 45, 45, 
44, 43.5, 43, 43, 42, 41, 40, 40, 39, 39, 38, 38, 37, 36, 36, 
35, 34, 34, 34, 32, 32, 32, 32, 32, 32, 32, 34, 35, 35, 37, 38, 
38, 39, 39, 40, 41, 42, 43, 44, 44, 44, 45, 45, 46, 46, 46, 45, 
45, 45, 44, 42, 40, 38, 37, 37, 37, 37, 37, 36, 35), .Dim = c(1248L, 
1L), .Dimnames = list(NULL, ""temperature""))
</code></pre>

<p>Forecasted temperature values are:</p>

<pre><code>forecast_temp &lt;- structure(c(35, 34, 34, 33, 33, 30, 29.7, 29.3, 29, 28, 28, 27, 
27.3, 27.7, 28, 29, 30.5, 32, 33.5, 35, 36, 36, 37, 39, 39.5, 
40, 40, 41, 42, 42, 43, 43, 43, 44, 44, 44, 43, 42, 40, 39, 39, 
38, 37, 37, 36, 35, 35, 34), .Dim = c(48L, 1L), .Dimnames = list(
    NULL, ""temperature""))
</code></pre>

<h3><em>UPDATE-1</em></h3>

<p>From @Stephan's suggestion, I followed the approach mentioned by Prof. Rob at <a href=""http://robjhyndman.com/hyndsight/dailydata/"" rel=""nofollow"">link1</a>, <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">link2</a> as</p>

<pre><code>library(forecast)
tsob &lt;- ts(train_power,frequency = 48) #training electriciy data at daily frequency
tsob_weekly &lt;- fourier(ts(train_power,frequency = 7*48),K=3) #training data at weekly frequency
tempob &lt;- ts(train_temperature,frequency = 48) #Temperature, another predictor variable
fit &lt;- auto.arima(tsob,xreg=cbind(tsob_weekly,tempob),seasonal=FALSE)
tempob_forecast &lt;- ts(test_temperature,frequency = 48) #forecasted temperauture values
forecast_val &lt;- forecast(fit,xreg=tempob_forecast,h=48*5) #forecast for coming 5 days
</code></pre>

<p>As evident from the code, I have used Fourier transformation suggested at above links to show weekly seasonal affect. Up to <code>auto.arima()</code>, it works properly but at forecast function an error is thrown as:</p>

<pre><code>Error in forecast.Arima(fit, xreg = tempob_forecast, h = 48 * 5) : 
  Number of regressors does not match fitted model
</code></pre>

<p>The error is clear, i.e., I do not provide the forecasted regressor values of <code>tsob_weekly</code>, which I don't have. How should I handle this issue? Prof. Rob has used the same value of regressor for both the <code>auto.arima</code> and <code>forecast</code> functions.</p>
"
"0.107624400500126","0.104257207028537","213201","<p>I am having basically the same issue than in <a href=""http://stats.stackexchange.com/questions/65585/auto-arima-does-not-recognize-seasonal-pattern"">this thread</a>, except one thing:</p>

<p>The difference, in my case, is that my data is measured <strong>weekly</strong> and not daily, so the argument of a too high seasonality (> 350) does not hold for my data, since the seasonality in my case is <strong>52</strong> (52 weeks in a year). </p>

<p>And yet, when I use <code>auto.arima()</code>, R returns the ARIMA model (p,d,q) = (2,1,1) and (P,D,Q) = (0,0,0), while the seasonal pattern in my data is blatant... How could you explain that R completely dismisses the seasonality in my data?</p>

<p>Since I'm still in a learning phase, I am using the data set <code>cmort</code> available in the <code>astsa</code> library, so everyone here can use the same data as me. </p>

<p>And I have done <code>cmort &lt;- ts(cmort,frequency=52)</code> to be sure that the seasonality in my data is taken account of, but it didn't change anything.</p>
"
"0.0878749550327494","0.0851256530758749","83433","<p>I would like to ask how the long-term (multiple step ahead) prediction intervals are calculated by function <code>predict.Arima</code> in R. I am particularly interested in ARIMA models, SARIMA models and in ARIMA models with external regressors (include argument xreg => regression with ARIMA errors) </p>
"
"0.06213697660012","0.0601929265428846","198887","<p>I am trying to model a time series that contains a sequence of zeros. I tried fitting an ARIMA model using <code>auto.arima</code> function from the forecast package in R but the MAPE is reported as infinity (probably due to division by zero). Moreover, the <code>auto.arima</code> fits an ARIMA(0,1,0) model over the data. </p>

<p>Can you suggest any types of models that may be appropriate for such data?</p>
"
"0.06213697660012","0.0601929265428846","116842","<p>I have a SarimaX model with three regressor variables:</p>

<pre><code>ARIMA(1,0,0)(0,1,1)[7]                    

Coefficients:
          ar1       sma1   C1 (for xreg1)   C2 (for xreg2)   C3 (for xreg3)
      -0.0260    -0.9216          -0.0354           0.0316           0.9404
s.e.   0.0291     0.0350           0.0016           0.0017           0.0128
</code></pre>

<p>I would like to know how to use these coefficients to obtain the actual equation, like:</p>

<pre><code>y[t] = f(ar1, sma1, C1|xreg1[t], C2|xreg2[t], C3|xreg3[t])
</code></pre>

<p>I have read the following:</p>

<p><a href=""https://www.otexts.org/fpp/8/9"" rel=""nofollow"">https://www.otexts.org/fpp/8/9</a> - I'm using the forecast package in R, so I'm quite grateful for Mr. Hyndman's work,</p>

<p><a href=""http://people.duke.edu/~rnau/arimreg.htm"" rel=""nofollow"">http://people.duke.edu/~rnau/arimreg.htm</a></p>

<p>and others, and I devised some formulas, but they generated values less acurate than those from the R forecast. Somehow, my error-related terms are probably wrong.</p>

<hr>

<p><strong>EDIT</strong>: This is what I have so far:</p>

<p>$$ \ (1-ar1*B)*(1-B^7)*y_t=$$
$$ = (1-ar1*B)*(1-B^7)*(C1*xreg1_t + C2*xreg2_t+C3*xreg3_t)+ $$
$$ + e_t + sma1*e_{t-7}$$</p>

<p>I would like to know if this formula is correct, could anyone please help? Thank you.</p>
"
"NaN","NaN","205057","<p>I'm trying to forecast 10 hours ahead using an Arima model.  I was wondering what a good rule of thumb is for the length of training data required.  If I use too long a training set I wind up with a flat forecast.</p>

<p>An example of my code and data are below.  </p>

<p>Code:</p>

<pre><code>Arima.fit &lt;- Arima(tsTrain, order=c(17,1,0))

Acast&lt;-forecast(Arima.fit2, h=10)
</code></pre>

<p>Data:</p>

<p>dput(tsTrain[1:40])
c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 8.5, 4, 5, 9, 10, 11, 
7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 2, 15, 2.5, 17, 5, 5.5, 
7, 6, 3.5, 6, 9.5, 5, 7) </p>
"
"0.152203886829552","0.147441956154897","110798","<p>I have a problem with the <code>forecast</code> function for ARIMA models in R. It calls <code>predict</code> that calls <code>KalmanForecast</code>. Ok...here's the deal.</p>

<p>the mean one-step forecast of the Arima object produced by this call</p>

<pre><code>forecast(Arima, h=1)$mean[[1]]
</code></pre>

<p>is often significantly different from the result of a manual forecast by conditional expected value (best linear predictor). </p>

<p>For example a non seasonal Arima(1,1,1) without drift has of course the structure</p>

<pre><code>y[t] = y[t-1] + AR1*(y[t-1] - y[t-2]) + MA1*epsilon[t-1] + epsilon[t]
</code></pre>

<p>so the one-step prediction is very straightforward </p>

<pre><code>y[t] = y[t-1] + AR1*(y[t-1] - y[t-2]) + MA1*epsilon[t-1]
</code></pre>

<p>but this result is always different from the result of the forecast function call.
Is it due to approximation errors in the Kalman recursion?</p>

<p>Try yourself with this code, it only needs the <code>forecast</code> package</p>

<pre><code>  x = arima.sim(n = 1000, list(ar = c(0.8897, -0.4858), ma = c(-0.2279, 0.2488)),sd=sqrt(0.1796))
  t = length(x) + 1
 Arimafit = Arima(x = x, order = c(1,1,1), seasonal = list(order = c(0,0,0), period =     1), include.mean = FALSE,include.drift = FALSE)
 manualforecast = x[t-1] + coef(Arimafit)[[""ar1""]]*(x[t-1] - x[t-2]) + coef(Arimafit)    [[""ma1""]]*Arimafit$residuals[t-1]
     autoforecast = forecast(Arimafit, h = 1)$mean[[1]]
</code></pre>

<p><code>autoforecast</code> is always different from <code>manualforecast</code>, sometimes significantly.</p>
"
"0.0878749550327494","0.0851256530758749","163580","<p>For testing I generated a very simple time series with a clear recurring pattern. I expected that auto.arima will generate a model, that can forecast that pattern, but Ã³bviously it doesn't. Can anyone give me some hints how I can improve the model in order to predict that pattern correctly?</p>

<pre><code>library(forecast)

ts&lt;-c(1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1)
fit &lt;- auto.arima(ts)
plot(forecast(fit,h=20))
</code></pre>

<p><a href=""http://i.stack.imgur.com/AgVha.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AgVha.png"" alt=""enter image description here""></a>
Thanks!</p>
"
"NaN","NaN","211022","<p>I have 288 data points of the Wolf's sunspot data for the years 1700 to 1987. "
"NaN","NaN","I need to predict one step ahead forecasts for a forecast horizon of 25.",""
"NaN","NaN","I kept the last 25 data points of the time series to test against the predictions. </p>",""
"NaN","NaN","<p>Will fitting an Arima to (288-25 =) 263 data points like suggested here <a href=http://stats.stackexchange.com/questions/55168/one-step-ahead-forecast-with-new-data-collected-sequentially?rq=1>One step ahead forecast with new data collected sequentially</a> work?",""
"NaN","NaN","Or do I need to iteratively increase the size of training data by 1 and then predict the next value?</p>",""
"NaN","NaN","","<r><time-series><forecasting><arima>"
"0.12427395320024","0.120385853085769","163878","<p>I want to estimate an ARIMA model on my timeseries, then represent it in state space format, mainly because it will be more responsive to change in pattern.
I used <code>auto.arima</code> from <code>forecast</code> package to find the best ARIMA, and then, if it didn't need differencing, tried to estimate the same ARMA using <code>dlm</code>package. </p>

<pre><code>fit.arima &lt;- auto.arima(training_data_list, seasonal = FALSE)
Series: training_data_list 
ARIMA(1,0,1) with zero mean     

Coefficients:
         ar1      ma1
      0.8247  -0.4913
s.e.  0.0395   0.0618

sigma^2 estimated as 489.7:  log likelihood=-3161.25
AIC=6328.5   AICc=6328.53   BIC=6342.15
</code></pre>

<p>Now the same thing using <code>dlm</code> package:</p>

<pre><code>test.model.arma &lt;- function(u){
  arma  &lt;- dlmModARMA(ar = ARtransPars(u[1]), ma = u[2], sigma2 = exp(u[3]))   # ma = c(u[4]),
  return(arma)
}
init &lt;-c(rep(0,3))
outMLE2 &lt;- dlmMLE(training_data_list, init, test.model.arma,method = ""Nelder-Mead"")
# outMLE2 &lt;- dlmMLE(training_data_list, init, test.model.arma)
if(outMLE2$convergence != 0) {print(""MLE did NOT converge!"")}

dlmModel4 &lt;- test.model.arma(outMLE2$par)
</code></pre>

<p>Now the loglikelihood from the later one is $2523.158$, while the <code>auto.arima</code> gives $3161.25$. the coefficients for $ar$ and $ma$ are very close, so I think it might be unlikely to be the reason.</p>

<pre><code>dput(training_data_list)
structure(c(0.92, 3.76, 2.64, 2.72, -4.48, 4.68, 6.2, 4.16, 22.32, 
28.96, 5.72, 3.44, 29.56, 37.28, 38.16, 31.28, 32.04, 21.32, 
2.88, 7.08000000000001, 52.32, 9.80000000000001, 8.56, 2.24000000000001, 
29.8, 49.2, 23.88, 42.32, -0.08, 3.76, -0.359999999999999, 2.72, 
8.52, 26.68, 7.2, -18.84, -13.68, -3.03999999999999, 10.72, -14.56, 
-16.44, 44.28, -17.84, -8.72000000000003, 3.04000000000002, 30.32, 
-21.12, -13.92, -3.68000000000001, -17.2, -16.44, -7.75999999999999, 
39.8, -1.80000000000001, 25.88, 9.31999999999999, -0.08, 1.76, 
2.64, -5.28, -4.48, 3.68, 1.2, -10.84, -21.68, -1.03999999999999, 
-6.28, 22.44, 62.56, 27.28, 1.16000000000003, -8.72000000000003, 
23.04, -11.68, -40.12, -33.92, -43.68, -26.2, -58.44, 5.24000000000001, 
-33.2, -18.8, -48.12, -57.68, -0.08, 3.76, -2.36, -7.28, -1.48, 
-1.32, 2.2, -6.84, 14.32, 23.96, -6.28, -11.56, -13.44, 14.28, 
-38.84, -31.72, 4.04000000000002, 4.31999999999999, 24.88, -3.91999999999999, 
-19.68, 30.8, 13.56, 4.24000000000001, 22.8, 36.2, 44.88, -17.68, 
-0.08, -1.24, -4.36, -1.28, -5.48, 7.68, 1.2, 14.16, -1.68000000000001, 
9.96000000000001, -3.28, 8.44, 59.56, 33.28, 23.16, 72.28, 48.04, 
36.32, 41.88, 7.08000000000001, 13.32, 20.8, 19.56, -0.759999999999991, 
-4.19999999999999, 31.2, 40.88, -10.68, -0.08, 6.76, 2.64, 2.72, 
7.52, 1.68, 1.2, -8.84, -7.68000000000001, 11.96, 31.72, -5.56, 
38.56, 27.28, -15.84, 60.28, 21.04, -22.68, 33.88, 55.08, 41.32, 
24.8, 6.56, 20.24, 60.8, 36.2, -9.12, 42.32, -0.08, 2.76, 2.64, 
1.72, -1.48, 2.68, 8.2, -18.84, -24.68, -4.03999999999999, 2.72, 
-31.56, -9.44, -11.72, -48.84, -5.72000000000003, 2.04000000000002, 
-19.68, -2.12, 27.08, 7.31999999999999, -22.2, 42.56, 11.24, 
9.80000000000001, 17.2, 25.88, 55.32, -0.08, -3.24, -4.36, 1.72, 
-6.48, 0.68, -0.799999999999997, 6.16, -13.68, -14.04, -2.28, 
12.44, -24.44, -15.72, 13.16, 1.27999999999997, -16.96, -40.68, 
-76.12, -50.92, -49.68, -58.2, -42.44, -36.76, -8.19999999999999, 
-43.8, -71.12, -62.68, -0.08, -3.24, -2.36, -1.28, 8.52, 4.68, 
-0.799999999999997, 18.16, 16.32, -18.04, 14.72, -18.56, 6.56, 
14.28, 53.16, 41.28, 35.04, -40.68, -12.12, -5.91999999999999, 
22.32, 11.8, -16.44, -6.75999999999999, -36.2, 3.19999999999999, 
-8.12, -12.68, -0.08, 3.76, 2.64, -3.28, -0.48, 13.68, 11.2, 
10.16, 9.31999999999999, -15.04, -6.28, -22.56, 14.56, -5.72000000000003, 
53.16, 36.28, 45.04, -12.68, -0.120000000000005, 21.08, -24.68, 
-10.2, -17.44, 3.24000000000001, 8.80000000000001, -11.8, -26.12, 
-27.68, -0.08, 2.76, 0.640000000000001, 2.72, -4.48, -2.32, -4.8, 
18.16, 7.31999999999999, 12.96, 45.72, 37.44, -22.44, 9.27999999999997, 
36.16, 51.28, 7.04000000000002, 54.32, 5.88, -1.91999999999999, 
25.32, 27.8, 15.56, 22.24, 18.8, -38.8, 31.88, 31.32, -0.08, 
6.76, -0.359999999999999, -0.279999999999999, 1.52, 1.68, -3.8, 
18.16, -8.68000000000001, 7.96000000000001, -7.28, 30.44, 12.56, 
51.28, 9.16000000000003, 16.28, -14.96, 50.32, -18.12, -32.92, 
10.32, -8.19999999999999, -5.44, 22.24, 23.8, 33.2, -12.12, 51.32, 
-0.08, -4.24, -2.36, -0.279999999999999, -2.48, -4.32, -4.8, 
0.159999999999997, -17.68, 7.96000000000001, 9.72, 9.44, 1.56, 
-15.72, 4.16000000000003, 6.27999999999997, -28.96, -38.68, -18.12, 
-36.92, -34.68, -25.2, -23.44, -43.76, -59.2, -37.8, -54.12, 
-61.68, -0.08, 0.76, 2.64, 9.72, -1.48, 2.68, 21.2, 13.16, -1.68000000000001, 
10.96, 20.72, 22.44, 23.56, 18.28, -3.83999999999997, -16.72, 
-19.96, 46.32, 47.88, 25.08, 0.319999999999993, 21.8, 14.56, 
32.24, 1.80000000000001, -5.80000000000001, -12.12, 1.31999999999999, 
-0.08, 5.76, -4.36, 4.72, -3.48, -5.32, 4.2, 4.16, -12.68, -14.04, 
-11.28, 0.439999999999998, -13.44, 25.28, 58.16, 29.28, 32.04, 
38.32, 14.88, 32.08, 39.32, 47.8, 16.56, 17.24, -25.2, 13.2, 
27.88, -6.68000000000001, -0.08, -6.24, -2.36, -6.28, -3.48, 
3.68, -1.8, -9.84, 11.32, 4.96000000000001, -26.28, -32.56, -38.44, 
-49.72, -62.84, -40.72, -20.96, -13.68, -52.12, -24.92, -49.68, 
-25.2, -19.44, -82.76, -36.2, -51.8, -60.12, -65.68, 0.92, -3.24, 
-1.36, -0.279999999999999, -19.48, -34.32, -22.8, 1.16, 19.32, 
-2.03999999999999, 27.72, 4.44, 12.56, -53.72, -35.84, -60.72, 
24.04, -15.68, 27.88, 11.08, 12.32, 33.8, 5.56, 3.24000000000001, 
24.8, -21.8, 18.88, -15.68, -0.08, -3.24, -1.36, -0.279999999999999, 
8.52, -5.32, 7.2, -12.84, 9.31999999999999, -0.039999999999992, 
-19.28, 13.44, -4.44, -5.72000000000003, -75.84, -82.72, -39.96, 
-60.68, 17.88, 23.08, 11.32, 38.8, 2.56, -6.75999999999999, 26.8, 
36.2, 27.88, 48.32, -0.08, -2.24, 1.64, 2.72, 2.52, -5.32, 0.200000000000003, 
-10.84, -4.68000000000001, -16.04, -15.28, -9.56, -27.44, -32.72, 
-63.84, -32.72, -49.96, -37.68, 38.88, 27.08, 7.31999999999999, 
5.80000000000001, 5.56, 10.24, -13.2, -20.8, 39.88, 36.32, -0.08, 
-3.24, 1.64, -9.28, -3.48, 1.68, -13.8, -2.84, -12.68, -15.04, 
8.72, 16.44, 7.56, 1.27999999999997, -23.84, -4.72000000000003, 
21.04, -22.68, -47.12, -16.92, -29.68, -32.2, -25.44, -40.76, 
-72.2, -50.8, -54.12, -99.68, -0.08, -4.24, 0.640000000000001, 
11.72, 4.52, -10.32, 1.2, 8.16, 15.32, -5.03999999999999, -14.28, 
-12.56, -34.44, -1.72000000000003, -11.84, -56.72, -25.96, -5.68000000000001, 
10.88, -4.91999999999999, 4.31999999999999, -40.2, 46.56, 20.24, 
5.80000000000001, 21.2, -1.12, 38.32, -0.08, -2.24, 2.64, -1.28, 
4.52, -1.32, -3.8, 1.16, 13.32, -0.039999999999992, 3.72, 2.44, 
19.56, 4.27999999999997, -41.84, -21.72, -29.96, 15.32, 39.88, 
-22.92, 28.32, -11.2, 67.56, 23.24, 25.8, -9.80000000000001, 
54.88, 18.32, -0.08, -1.24, 2.64, -1.28, 10.52, -4.32, -7.8, 
2.16, 16.32, -9.03999999999999, -0.280000000000001, 8.44, -47.44, 
-35.72, 51.16, 12.28, 15.04, 27.32, 19.88, 38.08, 50.32, 35.8, 
-16.44, 25.24, -1.19999999999999, 15.2, 25.88, 21.32, -0.08, 
-2.24, 1.64, 0.720000000000001, -0.48, 2.68, 6.2, -5.84, 13.32, 
11.96, -33.28, -32.56, -39.44, -50.72, 42.16, -38.72, -12.96, 
49.32, -4.12, -21.92, -13.68, -11.2, 2.56, 25.24, 16.8, 34.2, 
25.88, 56.32, -0.08, -2.24, -1.36, -6.28, 6.52, -4.32, -13.8, 
-12.84, -26.68, -15.04, -30.28, -0.560000000000002, 2.56, -28.72, 
58.16, 52.28, -51.96, -30.68, -36.12, -1.91999999999999, -56.68, 
-22.2, -26.44, -21.76, -27.2, -12.8, -58.12, -13.68), .Tsp = c(1, 
25.9642857142857, 28), class = ""ts"")
</code></pre>
"
"0.0878749550327494","0.0851256530758749","28472","<p>A Regression with ARIMA errors is given by the following formula (saw on Hyndman et al, 1998):</p>

<p>$Y_t = b_0 + b_1 X_{1,t} + \dots + b_k X_{k,t} + N_t$</p>

<p>where $N_t$ is modeled as an ARIMA process.</p>

<p>If we have that the model for $N_t$ is ARIMA$(0,0,0)$, then $N_t = e_t$, and $Y_t$ is modeled by an ordinary regression.</p>

<p>Suppose the following data:</p>

<pre><code>a &lt;- structure(c(29305, 9900, 9802, 17743, 49300, 17700, 24100, 11000, 
10625, 23644, 38011, 16404, 14900, 16300, 18700, 11814, 13934, 
12124, 18097, 30026, 3600, 15700, 12300, 14600), .Tsp = c(2010.25, 
2012.16666666667, 12), class = ""ts"")
b &lt;- structure(c(1.108528016, 1.136920872, 1.100239002, 1.057191265, 
1.044200511, 1.102063834, 1.083847756, 1.068585841, 1.084879628, 
1.232979511, 1.168894672, 1.257302058, 1.264967051, 1.234793782, 
1.306452369, 1.252644047, 1.178593218, 1.124432965, 1.132878661, 
1.189926986, 1.17249669, 1.176285957, 1.176552, 1.179178082), .Tsp = 
c(2010.25, 2012.16666666667, 12), class = ""ts"")
</code></pre>

<p>If I model it using <code>auto.arima</code> function, I have:</p>

<pre><code>auto.arima(a, xreg=b)
Series: a 
ARIMA(0,0,0) with zero mean     

Coefficients:
              b
      15639.266
s.e.   1773.186

sigma^2 estimated as 101878176:  log likelihood=-255.33
AIC=514.65   AICc=515.22   BIC=517.01

lm(a~b)

Call:
lm(formula = a ~ b)

Coefficients:
(Intercept)            b  
      48638       -26143  
</code></pre>

<p>Coefficients from the models differ. Shouldn't they be the same? What am I missing?</p>
"
"0.152203886829552","0.122868296795748","120008","<p>I am interested in fitting an ARIMAX model using R.
As known, ARIMAX can be understood as a composition of ARIMA models and regression models with exogenous (independent) variables. I have a time series $Y_i$, and want to estimate the ARIMA and nonlinear coefficients. The nonlinear model is the following:</p>

<p>$y_i=Î²_0+Î²_1t_i+Î²_2d+Î²_3 sin(2Ï€t_i/Î²_4 )+Î²_5 (-1^{t_i})+Îµ_i$,  nonlinear 
regression with an exogenous variable.
Where 
$t_i$ =1, 2â€¦, 60
and</p>

<p>d = dummy variable with 20 0's and 40 number 1's</p>

<pre><code>d=c(rep(0,20),rep(1,40))
</code></pre>

<p>And an ARIMA model (1,1,1) for $Y_i$. Therefore, I want to estimate simultaneously the $Î²_i$ and the ARIMA coefficients in order to avoid the confusion between the exogenous coefficients and ARIMA coefficients.  I know that $arima()$ can deal with this formulation but, how do the nonlinear model can be set within function function?. It seems that the <em>xreg</em> term only deals with linear parameters.</p>
"
"0.107624400500126","0.104257207028537","68261","<p>I started evaluating and comparing some methods in forecasting. I used Price of dozen eggs in US, 1900â€“1993, in constant dollars in the R software FMA package. I held out the last 10 years for assessment of forecast. Below are the results:</p>

<p>I used auto arima method in the R software. Obviously the results are way off. Am I doing something incorrect ? Below is the forecast. It does not recognize the declining trend. </p>

<p><img src=""http://i.stack.imgur.com/KIM9O.jpg"" alt=""auto arima""></p>

<p>I also used an unobserved components model (UCM) and obtained a good forecast,  as below.</p>

<ol>
<li>Without outliers/level shifts there are very large standard errors and therefore wide confidence bands. <img src=""http://i.stack.imgur.com/dlIXM.png"" alt=""UCM without outliers level shifts""></li>
<li>After some iterative work, below is the output with outliers/level shifts (I know I'm overfitting here) but it did a pretty good job in forecasting; there are also narrow confidence bands. <img src=""http://i.stack.imgur.com/5SrYJ.png"" alt=""UCM with outliers level shifts""></li>
</ol>

<p>In looking at just this example the UCM seems to predict the hold-out sample more accurately than auto.arima.</p>

<p><strong>Why is auto.arima not providing a reasonable forecast?</strong></p>

<p><strong>Are state space models/UCMs better for forecasting long range?</strong></p>

<p><strong>Are there any benefits of using one method over other?</strong></p>
"
"0.09320546490018","0.0902893898143269","164667","<p>I have a bunch of sales data. It is from distributors of 2000 different items, who service big companies and large distributors to a number of small independent stores. They sell some items which do good volume, and others where not even 100 units are sold in a year. What's more is that the method to determine true demand is not perfect - this is because if an item is out of stock, a customer who orders weekly will keep reordering until they get the stock and this will lead to an inflation in true demand as their order will be counted more than once. Conversely, going by sold data will not include demand from customers who did not reorder out of stock items.</p>

<p>Because of all this (as well as some other factors), the data, whilst showing some trend over large periods looks to me to be incredibly ""noisy"". Any trend over a short period of forecasting time would be wiped out.</p>

<p>Here's an example of a couple different plots. 
<a href=""http://i.stack.imgur.com/iaPi6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iaPi6.png"" alt=""Visual representation of noisey data""></a></p>

<p>ETS from the forecast package in R could give some insight, or ARIMA (not that I know how it's fully used yet). Or do you think in such cases where there is so little information in the data that a Simple Exponential Smoothing technique will yield results as good as it gets?</p>
"
"NaN","NaN","212528","<p>I am trying to create adequate time series model in R. I have doubt about adequacy. My data is year and total number of events:</p>

<p><code>year&lt;-c(2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015)
total&lt;-c(14,14,28,15,26,17,19,11,14,20,25)</code></p>

<p>Under is my R code for ARIMA</p>

<pre><code>mt&lt;-ts(total,start=2005)
ma&lt;-Arima(mt,order=c(3,1,1),seasonal = c(3,1,0))

summary(ma)
forecast&lt;-(ma)
plot(forecast(ma,5))
tsdisplay(residuals(ma))
</code></pre>

<p>If there is somebody who can help to find best model and explain how to do it.</p>
"
"0.328797974610715","0.318511028635303","68812","<p>I'm really new to ARIMA methods and am trying to forecast electricity load. I've integrated: electricity load, temperature, weekday (dummy), public holidays, and school holidays. My model tries to perform a non seasonal ARIMA with linear regression for each hour of the day.</p>

<p>Here is my code for an example of one of the 24 hours (6 AM):</p>

<pre><code># ElecLoad contains hourly loads and other data for 2005 and 2006 (=2*365*24 entries):
# 1. Electricity load in MW
# 2. day of weak: sunday=0, monday=1, etc 
# 3. Hour of the day 0 -&gt; 23
# 4. Public Holiday: 1 if Public Holiday, 0 otherwise
# 5. Scool vacation: 1 if no scool
# 6. Temperature in Â°F

# Create the weak matrix = dumy variables for the weakdays
weakmatrix&lt;-model.matrix(~as.factor(ElecLoad[,2]))
#Remove intercept
weakmatrix&lt;-weakmatrix[,-1]

#Generate FullTable
FullTable&lt;-cbind(load=ElecLoad[,1], weakmatrix, ElecLoad[,4],
                 ElecLoad[,3],ElecLoad[,5],ElecLoad[,5]^2, ElecLoad[,6])
colnames(FullTable)&lt;-c(""Load"",""mon"",""tue"",""wed"",""thu"",""fri"",""sat"",
                       ""ScoolHol"",""PubHol"",""Temp"",""Temp2"",""Hour"")

#Create the xreg = substed for a specific hour of the day (column 12 = Hour)
xreg&lt;-subset(FullTable[,2:11], FullTable[,12] == 7)

#Create the Load time serie, also a subset of the full table
LoadTs&lt;-ts(subset(FullTable[,1], FullTable[,12] == 7),start=1,frequency=1)

#Launch of auto.arima
ArimaLoad&lt;-auto.arima(LoadTs, xreg=xreg, lambda=0)
</code></pre>

<p>When I try to forecast with the same 2 years data as <code>xreg</code>, here is my output</p>

<pre><code>plot(forecast(ArimaLoad,xreg=xreg), include=0)
</code></pre>

<p><img src=""http://i.stack.imgur.com/MpNeH.png"" alt=""enter image description here""></p>

<p>While when I try to plot the fitted it looks identical to my original Load</p>

<pre><code>plot(fitted(ArimaLoad))
</code></pre>

<p><img src=""http://i.stack.imgur.com/zsw81.png"" alt=""enter image description here""></p>

<p>I don't understand why the <code>prediction()</code> is so much different than the <code>fitted()</code> with the same <code>xreg</code> matrix. Is this a normal behaviour, how can I improve my model to better fit with the real situation?</p>

<hr>

<p>Thank you so much for your support.</p>

<p>I'm not sure I understood everything from what you propose.</p>

<p>You mean that I should build a first model to forecast the daily average load (I prefer the average than the sum because due to DST, some days don't have 24 hours...). This model would be deterministic, but I don't see what kind of model you're thinking off? Is a multilinear regression ok? I prefer to consider the log(load) to make the different parameters multiplicative which I think is better fit to the reality.
Then I should have 24 hourly models, taking the daily average then split with a sort seasonal effect?
Should I use somewhere an ARIMA model?
I'm not convince of considering the month as having an effect, in my opinion there is no reason that consumption is more important in January than August except if we consider the Temperature and Holidays effects. The hour of the day is related to the activity that's the reason why I'm considering the specific model for each hour. The same way each day of the weak is different.</p>

<p>I've tried a multilinear regression for the same hour (7:00 AM) and the result looks not so bad.</p>

<pre><code>#Create the frame.data
Load&lt;-subset(FullTable[,1], FullTable[,12] == 7)
FullData&lt;-cbind(LogLoad=log(Load), xreg)
FrameData&lt;-data.frame(FullData)

# multilinear regression
mlin&lt;-lm(LogLoad ~ mon+tue+wed+thu+fri+sat+ScoolHol+PubHol+Temp+`Temp2`, FrameData)
plot(exp(mlin$model$LogLoad), type=""l"",col=""blue"")
lines(exp(fitted(mlin)), col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/MpNeH.png"" alt=""enter image description here""></p>

<p>fitted() in red which is now exactly the same as predict() if I re-use the same data entry (2005-2006) and looks not so far from the original load in blue (no so bad for a simple model). I still don't fully understand why it did not work with ARIMA as it also takes into consideration multilinear regression.</p>

<p>Now my ""simple"" model already takes into account several parameters, like the temperature, the holiday, the school vacations the day of the weak and the hour of the day (local time, not UCT).
How can I improve my model further more? How can I make sure that the parameters are invariant? Is there a specific method?</p>
"
"0.107624400500126","0.104257207028537","69405","<p>I am fitting a model using the <code>auto.arima</code> function in package <code>forecast</code>. I get a model that is AR(1), for example. I then extract residuals from this model. How does this generate the same number of residuals as the original vector? If this is an AR(1) model then the number of residuals should be 1 less than the dimensionality of the original time series. What am I missing?</p>

<p>Example:</p>

<pre><code>require(forecast)
arprocess = as.numeric(arima.sim(model = list(ar=.5), n=100))
#auto.arima(arprocess, d=0, D=0, ic=""bic"", stationary=T)
#  Series: arprocess 
#  ARIMA(1,0,0) with zero mean     

#  Coefficients:
#          ar1
#       0.5198
# s.e.  0.0867

# sigma^2 estimated as 1.403:  log likelihood=-158.99
# AIC=321.97   AICc=322.1   BIC=327.18
r = resid(auto.arima(arprocess, d=0, D=0, ic=""bic"", stationary=T))
&gt; length(r)
  [1] 100
</code></pre>

<p>Update: Digging into the code of auto.arima, I see that it uses Arima which in turn uses <code>stats:::arima</code>. Therefore the question is really how does <code>stats:::arima</code> compute residuals for the very first observation?</p>
"
"NaN","NaN","199579","<p>I would like to forecast some data. But I'm not sure whether I have implemented everything correctly. The accuracy is bad and I'm not sure whether it relates to methodology or some mistakes:</p>

<pre><code>library(caret)
library(forecast)
data(economics)
# Here I would like to use that approach and compare other models to arima
# via training/testing
timeSlices &lt;- createTimeSlices(1:nrow(economics), 
                           initialWindow = 500, horizon = 74, fixedWindow = TRUE)

trainSlices &lt;- timeSlices[[1]]
testSlices &lt;- timeSlices[[2]]


economics[trainSlices[[1]],]
economics[testSlices[[1]],]

# Here I fit the model
fit &lt;- Arima(economics[trainSlices[[1]],]$unemploy, order=c(4,1,3), seasonal = list(order = c(1, 0, 1), period = 7), lambda=2,method=""ML"")
    # Here I predict on new data
    pred &lt;- forecast(fit,h=length(economics[testSlices[[1]],]$unemploy))
# Here I extract the estimate
yHat &lt;- pred$mean
    # Here I check the accuracy
    accuracy(yHat,economics[testSlices[[1]],]$unemploy)
</code></pre>
"
"0.152203886829552","0.147441956154897","223457","<p>Suppose I have a training dataset, I use <code>auto.arima</code> (from ""forecast"" package in R) to fit the training data. As a result I get the lag and integration orders $(p, d, q)$ and the corresponding coefficients $\psi_i$ and $\theta_i$.</p>

<pre><code>ytrain = c(0.435477843, 0.435394762, 0.195528995, 1.451623315, 1.740084831 2.379904714, 1.092366508, 0.001031411, 0.592164090, 0.670323418)

fit &lt;- auto.arima(ytrain)
</code></pre>

<p>Now I have new data </p>

<pre><code>ytest = c(-0.1349199  0.9001208 -0.5171740 -0.9958452  0.4125953 -0.3320575  0.1633313  0.2890109 -0.4284824  0.7902680)
</code></pre>

<p>I want to fit this new data by using the model from training data (using the same $(p, d, q)$ and also the same corresponding coefficients). I.e. I want to use the model I have from <code>ytrain</code> to make prediction based on <code>ytest</code>. As a result I can know if there are any points in the new data looking like anomaly points (compared to the training data)</p>

<p>I have searched long time and haven't find a R function to implement it. I know I can compute this by hand, e.g. for ARMA(1,2):</p>

<p>$\hat{Y}_n =  \hat{\mu} + \hat{\psi}_1 Y_{n-1} - \hat{\theta}_{1} \epsilon_{n-1} - \hat{\theta}_2 \epsilon_{n-2} $</p>

<p>But if I do this, I am not sure how to start to get $\epsilon_1 = Y_1 - \hat{Y}_1$ and $\epsilon_2 = Y_2 - \hat{Y}_2$ to start since I don't have $\hat{Y}_1$ and $\hat{Y}_2$. </p>

<ul>
<li>Could anyone suggest an R function for doing this? Or if not,  </li>
<li>Could anyone help me with this question if there is no R function doing this?</li>
</ul>
"
"0.138942503594182","0.134595475514541","219440","<p>I am using the excellent <code>tsoutliers</code> R package to detect outliers (additive outliers, temporary changes etc.), but the <code>cval</code> parameter in the <code>tso</code> function is providing me with inconsistent, or at least counter-intuitive results. I was under the impression that a lower value for <code>cval</code> would include more outliers (but possibly also irrelevant ones), but this doesn't always seem the case. For example, using the following data and logic, I get the following output, which is nearly exactly what I was after:</p>

<pre><code>data &lt;- c(121.54, 119.79, 119.18, 118.56, 104, 65.52, 66, 119.18,
123.42, 119.18, 118.56, 99, 61.74, 67.98, 119.18, 123.42, 120.36,
115.14, 98, 62.37, 67.98, 122.72, 121, 116.82, 117.42, 98, 83.538,
103.096, 165.332, 185.6955, 145.848, 129.162, 101, 62.37, 64.68,
115.64, 124.63, 115.64, 118.56, 102, 62.37, 67.32, 115.64, 122.21,
121.54, 114, 103, 62.37, 65.34, 118, 122.21, 119.18, 114, 99, 65.52,
65.34, 118, 122.21, 115.64, 117.42, 73.5, 40.131, 41.184, 79.4376,
95.832, 105.138, 117.42, 100, 63, 66.66, 122.72, 123.42, 116.82, 114,
98, 61.74, 64.68, 116.82, 121, 188.152, 114, 99, 61.74, 66, 122.72,
118.58, 115.64, 112.86, 101, 63.63, 66.66)

# simple tso function
volume &lt;- ts(data, start = c(2016,1,1), frequency = 7)
data.ts.outliers &lt;- tso(volume, types = c(""AO"", ""LS"", ""TC""), cval = 3.0)
data.ts.outliers
plot(data.ts.outliers)
</code></pre>

<p><a href=""http://i.stack.imgur.com/CLf7Y.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CLf7Y.png"" alt=""Expected results, cval = 3.0""></a></p>

<p>However, using <code>cval = 2.9</code>, as well as most other values for <code>cval</code> above or below 3.0, I get the following results, which is missing some key outliers:</p>

<p><a href=""http://i.stack.imgur.com/i6dIO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/i6dIO.png"" alt=""Missing outliers, cval = 2.9""></a></p>

<p>As I want to use this package without manual review for each timeseries, Ideally I'd be able to use a slightly lower <code>cval</code> value to ensure I am capturing most outliers, but the inconsistency that I am seeing is not allowing for this. Anything I am missing?</p>
"
"0.12427395320024","0.120385853085769","205970","<p>I'm trying to create an Arima model and forecast it ahead the next 20 hours using the code and data below.  </p>

<p>When I look at the median of df$tri for each hour and broken down by day of the week, each weekday seems to have a distinct 24 hour pattern.</p>

<p>So I thought I would try adding dummy variables for the day of the week to my model as xreg predictors.</p>

<p>When I plot Acast$mean, I'm just getting a flat line.  So I was wondering if there was something incorrect about the way I'm creating and using the dummy variables for day of the week.</p>

<p>Code:</p>

<pre><code>##BoxCox

Tlambda &lt;- BoxCox.lambda(df$Tri)

##Partitioning Time Series

EndTrain&lt;-336
ValStart&lt;-EndTrain+1
ValEnd&lt;-ValStart+20

tsTrain &lt;-df$Tri[1:EndTrain]
    tsValidation&lt;-df$Tri[ValStart:ValEnd]


##Weekday variables
Copydf&lt;-df
Copydf$Weekday&lt;-as.factor(weekdays(as.Date(Copydf$DateTime, ""%Y-%m-%d"")))
Weekdays &lt;- Copydf$Weekday[1:nrow(Copydf)]
xreg1 &lt;- model.matrix(~as.factor(Weekdays)+0)[, 1:7]
colnames(xreg1) &lt;- c(""Friday"", ""Monday"", ""Saturday"", ""Sunday"",""Thursday"",""Tuesday"",""Wednesday"")

xreg1Train&lt;-xreg1[1:EndTrain,]
xreg1Val&lt;-xreg1[ValStart:ValEnd,]

##Checking effect of Weekday
Arima.fit &lt;- auto.arima(tsTrain, lambda = Tlambda, xreg=xreg1Train, stepwise=FALSE, approximation = FALSE )

##Forecast

Acast&lt;-forecast(Arima.fit,xreg=xreg1Val, h=20)

Acast$mean
</code></pre>

<p>Data:</p>

<pre><code>dput(df$Tri[1:336])
</code></pre>

<p>c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 8.5, 4, 5, 9, 10, 11, 
7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 2, 15, 2.5, 17, 5, 5.5, 
7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 9.5, 3.5, 5, 4, 4, 9, 4.5, 
6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 12, 17.5, 19, 7, 14, 17, 3.5, 
6, 15, 11, 10.5, 11, 13, 9.5, 9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 
19, 6, 7, 7.5, 7.5, 7, 6.5, 9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 
5, 12, 6, NA, 4, 2, 5, 7.5, 11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 
7, 4.5, 9, 3, 4, 6, 17.5, 11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 
7, 7, 4, 7.5, 11, 6, 11, 7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 
6, 8.5, 7.5, 6, 5, 8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 
11.5, 3, 4, 16, 3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 
6.5, 9, 12, 17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 
6.5, 15, 8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 
16.5, 2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 
13, 10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 11.5, 
12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 10, 10, 
13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 5.5, 6, 14, 
16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 13, 6, 7, 3, 5.5, 
7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 13, NA, 12, 1.5, 7, 
7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 8, 6, 3, 7.5)</p>

<pre><code>dput(df$DateTime[1:336])
</code></pre>

<p>c(""2015-01-01 00:00"", ""2015-01-01 01:00"", ""2015-01-01 02:00"", 
""2015-01-01 03:00"", ""2015-01-01 04:00"", ""2015-01-01 05:00"", ""2015-01-01 06:00"", 
""2015-01-01 07:00"", ""2015-01-01 08:00"", ""2015-01-01 09:00"", ""2015-01-01 10:00"", 
""2015-01-01 11:00"", ""2015-01-01 12:00"", ""2015-01-01 13:00"", ""2015-01-01 14:00"", 
""2015-01-01 15:00"", ""2015-01-01 16:00"", ""2015-01-01 17:00"", ""2015-01-01 18:00"", 
""2015-01-01 19:00"", ""2015-01-01 20:00"", ""2015-01-01 21:00"", ""2015-01-01 22:00"", 
""2015-01-01 23:00"", ""2015-01-02 00:00"", ""2015-01-02 01:00"", ""2015-01-02 02:00"", 
""2015-01-02 03:00"", ""2015-01-02 04:00"", ""2015-01-02 05:00"", ""2015-01-02 06:00"", 
""2015-01-02 07:00"", ""2015-01-02 08:00"", ""2015-01-02 09:00"", ""2015-01-02 10:00"", 
""2015-01-02 11:00"", ""2015-01-02 12:00"", ""2015-01-02 13:00"", ""2015-01-02 14:00"", 
""2015-01-02 15:00"", ""2015-01-02 16:00"", ""2015-01-02 17:00"", ""2015-01-02 18:00"", 
""2015-01-02 19:00"", ""2015-01-02 20:00"", ""2015-01-02 21:00"", ""2015-01-02 22:00"", 
""2015-01-02 23:00"", ""2015-01-03 00:00"", ""2015-01-03 01:00"", ""2015-01-03 02:00"", 
""2015-01-03 03:00"", ""2015-01-03 04:00"", ""2015-01-03 05:00"", ""2015-01-03 06:00"", 
""2015-01-03 07:00"", ""2015-01-03 08:00"", ""2015-01-03 09:00"", ""2015-01-03 10:00"", 
""2015-01-03 11:00"", ""2015-01-03 12:00"", ""2015-01-03 13:00"", ""2015-01-03 14:00"", 
""2015-01-03 15:00"", ""2015-01-03 16:00"", ""2015-01-03 17:00"", ""2015-01-03 18:00"", 
""2015-01-03 19:00"", ""2015-01-03 20:00"", ""2015-01-03 21:00"", ""2015-01-03 22:00"", 
""2015-01-03 23:00"", ""2015-01-04 00:00"", ""2015-01-04 01:00"", ""2015-01-04 02:00"", 
""2015-01-04 03:00"", ""2015-01-04 04:00"", ""2015-01-04 05:00"", ""2015-01-04 06:00"", 
""2015-01-04 07:00"", ""2015-01-04 08:00"", ""2015-01-04 09:00"", ""2015-01-04 10:00"", 
""2015-01-04 11:00"", ""2015-01-04 12:00"", ""2015-01-04 13:00"", ""2015-01-04 14:00"", 
""2015-01-04 15:00"", ""2015-01-04 16:00"", ""2015-01-04 17:00"", ""2015-01-04 18:00"", 
""2015-01-04 19:00"", ""2015-01-04 20:00"", ""2015-01-04 21:00"", ""2015-01-04 22:00"", 
""2015-01-04 23:00"", ""2015-01-05 00:00"", ""2015-01-05 01:00"", ""2015-01-05 02:00"", 
""2015-01-05 03:00"", ""2015-01-05 04:00"", ""2015-01-05 05:00"", ""2015-01-05 06:00"", 
""2015-01-05 07:00"", ""2015-01-05 08:00"", ""2015-01-05 09:00"", ""2015-01-05 10:00"", 
""2015-01-05 11:00"", ""2015-01-05 12:00"", ""2015-01-05 13:00"", ""2015-01-05 14:00"", 
""2015-01-05 15:00"", ""2015-01-05 16:00"", ""2015-01-05 17:00"", ""2015-01-05 18:00"", 
""2015-01-05 19:00"", ""2015-01-05 20:00"", ""2015-01-05 21:00"", ""2015-01-05 22:00"", 
""2015-01-05 23:00"", ""2015-01-06 00:00"", ""2015-01-06 01:00"", ""2015-01-06 02:00"", 
""2015-01-06 03:00"", ""2015-01-06 04:00"", ""2015-01-06 05:00"", ""2015-01-06 06:00"", 
""2015-01-06 07:00"", ""2015-01-06 08:00"", ""2015-01-06 09:00"", ""2015-01-06 10:00"", 
""2015-01-06 11:00"", ""2015-01-06 12:00"", ""2015-01-06 13:00"", ""2015-01-06 14:00"", 
""2015-01-06 15:00"", ""2015-01-06 16:00"", ""2015-01-06 17:00"", ""2015-01-06 18:00"", 
""2015-01-06 19:00"", ""2015-01-06 20:00"", ""2015-01-06 21:00"", ""2015-01-06 22:00"", 
""2015-01-06 23:00"", ""2015-01-07 00:00"", ""2015-01-07 01:00"", ""2015-01-07 02:00"", 
""2015-01-07 03:00"", ""2015-01-07 04:00"", ""2015-01-07 05:00"", ""2015-01-07 06:00"", 
""2015-01-07 07:00"", ""2015-01-07 08:00"", ""2015-01-07 09:00"", ""2015-01-07 10:00"", 
""2015-01-07 11:00"", ""2015-01-07 12:00"", ""2015-01-07 13:00"", ""2015-01-07 14:00"", 
""2015-01-07 15:00"", ""2015-01-07 16:00"", ""2015-01-07 17:00"", ""2015-01-07 18:00"", 
""2015-01-07 19:00"", ""2015-01-07 20:00"", ""2015-01-07 21:00"", ""2015-01-07 22:00"", 
""2015-01-07 23:00"", ""2015-01-08 00:00"", ""2015-01-08 01:00"", ""2015-01-08 02:00"", 
""2015-01-08 03:00"", ""2015-01-08 04:00"", ""2015-01-08 05:00"", ""2015-01-08 06:00"", 
""2015-01-08 07:00"", ""2015-01-08 08:00"", ""2015-01-08 09:00"", ""2015-01-08 10:00"", 
""2015-01-08 11:00"", ""2015-01-08 12:00"", ""2015-01-08 13:00"", ""2015-01-08 14:00"", 
""2015-01-08 15:00"", ""2015-01-08 16:00"", ""2015-01-08 17:00"", ""2015-01-08 18:00"", 
""2015-01-08 19:00"", ""2015-01-08 20:00"", ""2015-01-08 21:00"", ""2015-01-08 22:00"", 
""2015-01-08 23:00"", ""2015-01-09 00:00"", ""2015-01-09 01:00"", ""2015-01-09 02:00"", 
""2015-01-09 03:00"", ""2015-01-09 04:00"", ""2015-01-09 05:00"", ""2015-01-09 06:00"", 
""2015-01-09 07:00"", ""2015-01-09 08:00"", ""2015-01-09 09:00"", ""2015-01-09 10:00"", 
""2015-01-09 11:00"", ""2015-01-09 12:00"", ""2015-01-09 13:00"", ""2015-01-09 14:00"", 
""2015-01-09 15:00"", ""2015-01-09 16:00"", ""2015-01-09 17:00"", ""2015-01-09 18:00"", 
""2015-01-09 19:00"", ""2015-01-09 20:00"", ""2015-01-09 21:00"", ""2015-01-09 22:00"", 
""2015-01-09 23:00"", ""2015-01-10 00:00"", ""2015-01-10 01:00"", ""2015-01-10 02:00"", 
""2015-01-10 03:00"", ""2015-01-10 04:00"", ""2015-01-10 05:00"", ""2015-01-10 06:00"", 
""2015-01-10 07:00"", ""2015-01-10 08:00"", ""2015-01-10 09:00"", ""2015-01-10 10:00"", 
""2015-01-10 11:00"", ""2015-01-10 12:00"", ""2015-01-10 13:00"", ""2015-01-10 14:00"", 
""2015-01-10 15:00"", ""2015-01-10 16:00"", ""2015-01-10 17:00"", ""2015-01-10 18:00"", 
""2015-01-10 19:00"", ""2015-01-10 20:00"", ""2015-01-10 21:00"", ""2015-01-10 22:00"", 
""2015-01-10 23:00"", ""2015-01-11 00:00"", ""2015-01-11 01:00"", ""2015-01-11 02:00"", 
""2015-01-11 03:00"", ""2015-01-11 04:00"", ""2015-01-11 05:00"", ""2015-01-11 06:00"", 
""2015-01-11 07:00"", ""2015-01-11 08:00"", ""2015-01-11 09:00"", ""2015-01-11 10:00"", 
""2015-01-11 11:00"", ""2015-01-11 12:00"", ""2015-01-11 13:00"", ""2015-01-11 14:00"", 
""2015-01-11 15:00"", ""2015-01-11 16:00"", ""2015-01-11 17:00"", ""2015-01-11 18:00"", 
""2015-01-11 19:00"", ""2015-01-11 20:00"", ""2015-01-11 21:00"", ""2015-01-11 22:00"", 
""2015-01-11 23:00"", ""2015-01-12 00:00"", ""2015-01-12 01:00"", ""2015-01-12 02:00"", 
""2015-01-12 03:00"", ""2015-01-12 04:00"", ""2015-01-12 05:00"", ""2015-01-12 06:00"", 
""2015-01-12 07:00"", ""2015-01-12 08:00"", ""2015-01-12 09:00"", ""2015-01-12 10:00"", 
""2015-01-12 11:00"", ""2015-01-12 12:00"", ""2015-01-12 13:00"", ""2015-01-12 14:00"", 
""2015-01-12 15:00"", ""2015-01-12 16:00"", ""2015-01-12 17:00"", ""2015-01-12 18:00"", 
""2015-01-12 19:00"", ""2015-01-12 20:00"", ""2015-01-12 21:00"", ""2015-01-12 22:00"", 
""2015-01-12 23:00"", ""2015-01-13 00:00"", ""2015-01-13 01:00"", ""2015-01-13 02:00"", 
""2015-01-13 03:00"", ""2015-01-13 04:00"", ""2015-01-13 05:00"", ""2015-01-13 06:00"", 
""2015-01-13 07:00"", ""2015-01-13 08:00"", ""2015-01-13 09:00"", ""2015-01-13 10:00"", 
""2015-01-13 11:00"", ""2015-01-13 12:00"", ""2015-01-13 13:00"", ""2015-01-13 14:00"", 
""2015-01-13 15:00"", ""2015-01-13 16:00"", ""2015-01-13 17:00"", ""2015-01-13 18:00"", 
""2015-01-13 19:00"", ""2015-01-13 20:00"", ""2015-01-13 21:00"", ""2015-01-13 22:00"", 
""2015-01-13 23:00"", ""2015-01-14 00:00"", ""2015-01-14 01:00"", ""2015-01-14 02:00"", 
""2015-01-14 03:00"", ""2015-01-14 04:00"", ""2015-01-14 05:00"", ""2015-01-14 06:00"", 
""2015-01-14 07:00"", ""2015-01-14 08:00"", ""2015-01-14 09:00"", ""2015-01-14 10:00"", 
""2015-01-14 11:00"", ""2015-01-14 12:00"", ""2015-01-14 13:00"", ""2015-01-14 14:00"", 
""2015-01-14 15:00"", ""2015-01-14 16:00"", ""2015-01-14 17:00"", ""2015-01-14 18:00"", 
""2015-01-14 19:00"", ""2015-01-14 20:00"", ""2015-01-14 21:00"", ""2015-01-14 22:00"", 
""2015-01-14 23:00"")</p>
"
"0.06213697660012","0.0601929265428846","74537","<p>With the below dataset, I have a series which needs transforming. Easy enough. However, how do you decide which of the SQRT or LOG transformations is better? And how do you draw that conclusion?</p>

<pre><code>x&lt;-c(75800,54700,85000,74600,103900,82000,77000,103600,62900,60700,58800,134800,81200,47700,76200,81900,95400,85400,84400,103400,63000,65500,59200,128000,74400,57100,75600,88300,111100,95000,91500,111400,73700,72800,64900,146300,83100,66200,101700,100100,120100,100200,97000,120600,88400,83500,73200,141800,87700,82700,106000,103900,121000,98800,96900,115400,87500,86500,81800,135300,88900,77100,109000,104000,113000,99000,104500,109400,92900,88700,90500,140200,91700,78800,114700,100700,113300,122800,117900,122200,102900,85300,92800,143800,88400,75400,111200,96300,114600,108300,113400,116600,103400,87300,88200,149800,90100,78800,108900,126300,122000,125100,119600,148800,114600,101600,108800,174100,101100,89900,126800,126400,141400,144700,132800,149000,124200,101500,106100,168100,104200,79900,126100,121600,139500,143100,144100,154500,129500,109800,116200,171100,106700,85500,132500,133700,135600,149400,157700,144500,165400,122700,113700,175000,113200,94400,138600,132400,129200,165700,153300,141900,170300,127800,124100,206700,131700,112700,170900,153000,146700,197800,173800,165400,201700,147000,144200,244900,146700,124400,168600,193400,167900,209800,198400,184300,214300,156200,154900,251200,127900,125100,171500,167000,163900,200900,188900,168000,203100,169800,171900,241300,141400,140600,172200,192900,178700,204600,222900,179900,229900,173100,174600,265400,147600,140800,171900,189900,185100,218400,207100,178800,228800,176900,170300,251500,149900,150300,192000,185100,184500,228800,219000,180000,241500,184300,174600,264500,166100,151900,194600,214600,201700,229400,233600,197500,254600,194000,201100,279500,175800,167200,235900,207400,215900,261800,236800,222400,281500,214100,218200,295000,194400,180200,250400,212700,251300,280200,249300,240000,304200,236900,232500,300700,207300,196900,246600,262500,272800,282300,271100,265600,313500,268000,256500,318100,232700,198500,268900,244300,262400,289200,286600,281100,330700,262000,244300,309300,246900,211800,263100,307700,284900,303800,296900,290400,356200,283700,274500,378300,263100,226900,283800,299900,296000,327600,313500,291700,333000,246500,227400,333200,239500,218600,283500,267900,294500,318600,318700,283400,351600,268400,251100,365100,249100,216400,245500,232100,236300,275600,296500,296900,354300,277900,287200,420200,299700,268200,329700,353600,356200,396500,379500,349100,437900,350600,338600,509100,342300,288800,378400,371200,395800,450000,414100,387600,486600,355300,358800,526800,346300,295600,361500,415300,402900,484100,412700,395800,491300,391000,374900,569200,369500,314900,422500,436400,439700,509200,461700,449500,560600,435000,429900,633400,417900,365700,459200,466500,488500,531500,483500,485400,575700,458000,433500,642600,409600,363100,430100,503900,500400,557400,565500,526700,628900,547700,520400,731200,494400,416800,558700,537100,556200,686700,616600,582600,725800,577700,552100,806700,554200,455000,532600,693000,619400,727100,684700)
y&lt;-ts(x,frequency=12, start=c(1976,1))
#Transforming the data to log or sqrt and plotting it
log.y&lt;-log(y)
plot(log.y)
sqrt.y&lt;-sqrt(y)
plot(sqrt.y)
</code></pre>
"
"0.0878749550327494","0.0851256530758749","220973","<p><a href=""http://i.stack.imgur.com/27CVA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/27CVA.png"" alt=""enter image description here""></a>I'm using the R function <code>auto.arima</code> to fit an arima model for a time series, 
the result is an ARIMA(2,1,1). After that I apply the <code>forecast</code> function to predict some futur values. My question is Should I do the transformation (""un-differentiate"" the predicted values) or is it done by <code>forecast</code> automatically ? 
edit : here is what i get when i execute the code : </p>

<pre><code>arimaf = auto.arima(timeseries)
pred = forecast(arimaf, h = 10)
plot(pred, main = ""PREDICTION USING ARIMA(2,1,1)"")
</code></pre>
"
"0.06213697660012","0.0601929265428846","32528","<p>I have fitted ARIMA(5,1,2) model using <code>auto.arima()</code> function in R and by looking order we can say this is not a best model to forecast. If outliers exist in the data series, what is the method to fit a model to such data?</p>
"
"0.304651592499683","0.306924907022426","188595","<p>I have already read</p>

<p><a href=""http://stats.stackexchange.com/questions/126525/time-series-forecast-convert-differenced-forecast-back-to-before-difference-lev"">Time Series Forecast: Convert differenced forecast back to before difference level</a></p>

<p>and</p>

<p><a href=""http://stats.stackexchange.com/questions/130448/how-to-undifference-a-time-series-variable"">How to &quot;undifference&quot; a time series variable</a></p>

<p>None of these unfortunately gives any clear answer how to convert forecast done in ARIMA using differenced method(diff()) to reach at stationary series.</p>

<p>code sample.</p>

<pre><code>## read data and start from 1 jan 2014
dat&lt;-read.csv(""rev forecast 2014-23 dec 2015.csv"")
val.ts &lt;- ts(dat$Actual,start=c(2014,1,1),freq=365)
##Check how we can get stationary series
plot((diff(val.ts)))
plot(diff(diff(val.ts)))
plot(log(val.ts))
plot(log(diff(val.ts)))
plot(sqrt(val.ts))
plot(sqrt(diff(val.ts)))
##I found that double differencing. i.e.diff(diff(val.ts)) gives stationary series.

#I ran below code to get value of 3 parameters for ARIMA from auto.arima
ARIMAfit &lt;- auto.arima(diff(diff(val.ts)), approximation=FALSE,trace=FALSE, xreg=diff(diff(xreg)))
#Finally ran ARIMA
fit &lt;- Arima(diff(diff(val.ts)),order=c(5,0,2),xreg = diff(diff(xreg)))

#plot original to see fit
plot(diff(diff(val.ts)),col=""orange"")
#plot fitted
lines(fitted(fit),col=""blue"")
</code></pre>

<p>This gives me a perfect fit time series. However, how do i reconvert fitted values into their original metric from the current form it is now in? i mean from double differencing into actual number? For log i know we can do 10^fitted(fit) for square root there is similar solution, however what to do for differencing, that too double differencing?</p>

<p>Any help on this please in R? After days of rigorous exercise, i am stuck at this point.</p>

<p>Edit: Let me paste images from 3 iterations i ran to test if differencing has any impact on model fit of auto.arima function and found that it does. so auto.arima can't handle non stationary series and it requires some effort on part of analyst to convert the series to stationary.</p>

<p>Firstly, auto.arima without any differencing. Orange color is actual value, blue is fitted.</p>

<pre><code>ARIMAfit &lt;- auto.arima(val.ts, approximation=FALSE,trace=FALSE, xreg=xreg)
plot(val.ts,col=""orange"")
lines(fitted(ARIMAfit),col=""blue"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/VWVHK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VWVHK.png"" alt=""enter image description here""></a></p>

<p>secondly, i tried differencing</p>

<pre><code>ARIMAfit &lt;- auto.arima(diff(val.ts), approximation=FALSE,trace=FALSE, xreg=diff(xreg))
plot(diff(val.ts),col=""orange"")
lines(fitted(ARIMAfit),col=""blue"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/sTnxQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sTnxQ.png"" alt=""enter image description here""></a> </p>

<p>thirdly, i did differencing 2 times.</p>

<pre><code>ARIMAfit &lt;- auto.arima(diff(diff(val.ts)), approximation=FALSE,trace=FALSE, 
xreg=diff(diff(xreg)))
plot(diff(diff(val.ts)),col=""orange"")
lines(fitted(ARIMAfit),col=""blue"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/1x8ex.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1x8ex.png"" alt=""enter image description here""></a></p>

<p>A visual inspection can suggest that 3rd graph is more accurate out of all. This i am aware of. The challenge is how to reconvert this fitted value which is in the form of double differenced form into the actual metric!</p>

<p>Edit2: Why it is not so simple. Let me explain by below example.</p>

<p>Actual data with single difference and double difference.
<a href=""http://i.stack.imgur.com/hJSOF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hJSOF.png"" alt=""enter image description here""></a></p>

<p>Lets go back to actual data by using differences and first value of prior series.</p>

<p><a href=""http://i.stack.imgur.com/IW6js.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IW6js.png"" alt=""enter image description here""></a></p>

<p>If i use diff(diff(val.ts)) in auto.arima as input data, i get below fitted values. However i do not have first value of first order difference of fitted value and neither i have first data point in fitted value in original metric format! This is where i am struck!</p>

<p><a href=""http://i.stack.imgur.com/llFtr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/llFtr.png"" alt=""enter image description here""></a></p>

<p>What if i use Richard Hardy's advice and use data from actual series as reference. This gives me negative numbers. Can you imagine negative sales? And to clarify my original numbers do not have ANY negative number and it does not have any returns or cancellation data!</p>

<p><a href=""http://i.stack.imgur.com/IEKrJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IEKrJ.png"" alt=""enter image description here""></a></p>
"
"0.12683657235796","0.147441956154897","32313","<p>I have a linear regression model that is used to forecast the 'afluent natural energy' (ANE) of some region.</p>

<p>The predictors for this model are:</p>

<ul>
<li>the previous month ANE (<code>ANE0</code>)</li>
<li>the previous month rain volume (<code>PREC0</code>)</li>
<li>the current month forecast for rain volume (<code>PREC1</code>)</li>
</ul>

<p>We have 7 years of historical data for all of these variables, for each month. The current model just runs a OLS linear regression. I feel there's a lot of improvements to be done, but i'm not a time series specialist.</p>

<p>The first thing I notice is that the predictors are highly correlated (multicollinearity).
I'm not certain of the impacts of multicollinearity on prediction confidence.</p>

<p>I decided to try a time series approach, so I ran a ACF and PACF on the historic data:
The ACF shows a sine wave pattern, and the PACF has a spike at 1 and 2. So I tried both <code>ARIMA (2, 0, 0)</code> and <code>ARIMA(2,0,1)</code> to predict 20 periods ahead.</p>

<p>The ARIMA(2,0,1) shows good results, but I'm not certain as to how to compare it to the linear regression model.</p>

<p>What's the best way to test the performance of these model?  I'm using R as analysis tool (together with the <code>forecast</code> package). </p>
"
"0.138942503594182","0.134595475514541","32694","<p>I'm using R together with the <code>forecast</code> package to set up a ARIMA model, that will be used to predict a energy related variable. I used <code>auto.arima()</code> to fit different models (according to geographic region), and I need to put the model coefficients in our database, so that the IT folks can automate things. That's exactly the problem: I simply don't know how set up the equations by looking at the model:</p>

<pre><code>ARIMA(1,0,1)(2,0,1)[12] with non-zero mean 

Coefficients:

       ar1     ma1    sar1    sar2     sma1   intercept    prec0    prec1
     0.3561  0.3290  0.6857  0.2855  -0.7079  11333.240   15.5291  28.0817

s.e. 0.2079  0.1845  0.2764  0.2251   0.3887   2211.302    6.2147   6.0906
</code></pre>

<p>I have 2 regressor variables (prec0 and prec1). Given the residuals, the ARIMA vector <code>ARIMA(1,0,1)(2,0,1)[12]</code>, the time series up to period $t$, the number $h$ of forecasting periods and the regressor matrix reg, how can I set a function to return the forecast values? I.e:</p>

<pre><code>do.forecast = function(residuals, ARIMA, timeSeries, h, regMatrix)
{
  p = ARIMA[1]
  q = ARIMA[3]

  ## arima equations here...
}
</code></pre>

<p>Thanks!  </p>

<p>PS: I know this is a possible duplicate of <a href=""http://stats.stackexchange.com/questions/23881/reproducing-arima-model-outside-r"">Reproducing ARIMA model outside R</a>, but my model seems very different, and I really don't know how to start with.</p>
"
"0.165698604266987","0.180578779628654","201755","<p>I would like to create a forecast that isn't just based off the historical patterns in the data, but would also take in to account if there was a spike in CustCount or TiTo just in the last few hours.  </p>

<p>I've been thinking a strategy like the auto.arima example below would be a good plan, except since I'll need to forecast the predictors as well as the time series I'm trying to predict, I'm starting to think that I would just be basing everything off the historical patterns in the data, like it would just be a forecast for tsTiTo with an xreg forecast for CustCount.  So say if CustCount was triple the average right in the last hour before the forecast, would that be reflected in the forecast for tsTiTo or would it be more influenced by the CustCount forecast historical pattern?</p>

<p>Am I interpreting auto.arima with predictors correctly?</p>

<p>Can I use auto.arima with predictors to create the type of forecast I'm describing?</p>

<p>Can anyone suggest a better approach?</p>

<p>I know the sample data below isn't very well correlated, the real dataset is much larger.  I just wanted to give an idea of what I've been trying.</p>

<p>Code:</p>

<pre><code>tsTrain &lt;-tsTiTo[1:60]
tsTest &lt;- tsTiTo[61:100]

LagXreg&lt;-c(NA,ds[1:99,]$CustCount)

##Lagged Predictors
xregTrain2 &lt;- LagXreg[1:60]
xregTest2 &lt;- LagXreg[61:100]


Arima.fit2 &lt;- auto.arima(tsTrain, xreg = xregTrain2,stepwise=FALSE,         approximation=FALSE)

Acast2&lt;-forecast(Arima.fit2, h=40, xreg = xregTest2)
</code></pre>

<p>Data:</p>

<pre><code>dput(ds$CustCount[1:100])
</code></pre>

<p>c(3, 3, 1, 4, 1, 3, 2, 3, 2, 4, 1, 1, 5, 6, 8, 5, 2, 7, 7, 3, 2, 2, 2, 1, 3, 2, 3, 1, 1, 2, 1, 1, 3, 2, 2, 2, 3, 7, 5, 6, 8, 7, 3, 5, 6, 6, 8, 4, 2, 1, 2, 1, NA, NA, 4, 2, 2, 4, 11, 2, 8, 1, 4, 7, 11, 5, 3, 10, 7, 1, 1, NA, 2, NA, NA, 2, NA, NA, 1, 2, 3, 5, 9, 5, 9, 6, 6, 1, 5, 3, 7, 5, 8, 3, 2, 6, 3, 2, 3, 1 )</p>

<pre><code>dput(tsTiTo[1:100])
</code></pre>

<p>c(45, 34, 11, 79, 102, 45, 21, 45, 104, 20, 2, 207, 45, 2, 3, 153, 8, 2, 173, 11, 207, 79, 45, 153, 192, 173, 130, 4, 173, 174, 173, 130, 79, 154, 4, 104, 192, 153, 192, 104, 28, 173, 52, 45, 11, 29, 22, 81, 7, 79, 193, 104, 1, 1, 46, 130, 45, 154, 153, 7, 174, 21, 193, 45, 79, 173, 45, 153, 45, 173, 2, 1, 2, 1, 1, 8, 1, 1, 79, 45, 79, 173, 45, 2, 173, 130, 104, 19, 4, 34, 2, 192, 42, 41, 31, 39, 11, 79, 4, 79)</p>
"
"0.12427395320024","0.120385853085769","16915","<p>Note that I do most of my analysis using R and Excel.</p>

<p>Let's take this data set for example. I modified it as the data itself is proprietary: the years are also different:</p>

<pre><code>1967    2,033,407
1968    2,162,275
1969    2,159,640
1970    2,312,352
1971    2,554,449
1972    2,548,425
1973    2,101,225
1974    1,951,944
1975    2,106,250
1976    1,687,625
1977    1,636,496
1978    1,494,525
1979    1,606,825
1980    1,460,937
1981    1,310,494
1982    1,319,750
1983    1,263,643
1984    1,171,656
1985    1,194,950
</code></pre>

<p>What I usually do:</p>

<ol>
<li>A linear regression</li>
<li>Some form of polynomial trending</li>
<li>Moving average and double moving average</li>
<li>Basic ARIMA using p = 1, q = 0.</li>
<li>I calculate the errors for all these as well</li>
<li>I average all the forecasts out and the error to have my final result.</li>
</ol>

<p>Note that I'm an engineer that wants to get into statistics and the ability to properly validate and calibrate my models.</p>

<h2>Question</h2>

<p>What is the correct way to forecast this to 5, 10, or even 15 future years?</p>

<p>In a way I'm looking to move beyond the plugging data into a model and believe the data. Yes, I'm aware I can look at the errors. I mainly use RMSE or MAE. But I still am not confident when it comes to just predicting data the right way.</p>

<h3>Note</h3>

<p>this is also related to <a href=""http://stats.stackexchange.com/questions/16545/how-can-i-be-confident-about-my-forecasts-and-improve-my-methodologies"">this question</a> I posted here before.</p>
"
"0.206085036989691","0.181488502160157","202319","<p>I have daily sales data for a department store for the past 850 days. I have indicators on the major holidays and the days leading up to the major holidays. The number of days before the holidays that are included was chosen by AIC. The issue I'm having is that there are outliers throughout the data that I'm not sure how to handle. Or, at least that's what I think is happening since I don't seem to get accurate forecasts. I'm using a CV to calculate the MAPE of forecasts two weeks out, using the first 450 days as the initial training set and the rest to see how well the model forecasts the data.</p>

<p>I've used tso() from the tsoutliers package and tsoutliers from the forecast package to find outliers. They both give different results.</p>

<pre><code>tsoutliers(data$Sales)

$index
[1] 230 270 271 328 635

$replacements
[1] 2222.160 2088.573 2231.577 1812.380 2138.655

train = 454
trainingdata = data$Sales[1:train]
trainingdata = ts(trainingdata,frequency = 7)
tso(trainingdata,types = c(""AO"", ""LS"", ""TC""))

Series: trainingdata 
ARIMA(2,1,1)(2,0,0)[7]                    

Coefficients:
     ar1     ar2      ma1    sar1    sar2      AO52      TC68       TC80      AO86
  0.2872  0.1331  -0.9717  0.3567  0.4607  885.2061  890.3690  -863.4296  836.8638
s.e.  0.0508  0.0480   0.0107  0.0436  0.0429  169.2521  163.4243   166.0282  169.8535
     AO111     AO121      TC229     AO259      TC270     AO328     AO416
  754.1791  691.0849  1236.8523  711.3954  1790.0292  764.9712  920.1783
s.e.  169.2042  167.7273   163.1458  167.9835   163.9663  170.0103  168.9235

sigma^2 estimated as 44080:  log likelihood=-3064.92
AIC=6152.24   AICc=6153.65   BIC=6222.21

Outliers:
type ind  time coefhat  tstat
1    AO  52  8:03   885.2  5.230
2    TC  68 10:05   890.4  5.448
3    TC  80 12:03  -863.4 -5.200
4    AO  86 13:02   836.9  4.927
5    AO 111 16:06   754.2  4.457
6    AO 121 18:02   691.1  4.120
7    TC 229 33:05  1236.9  7.581
8    AO 259 37:07   711.4  4.235
9    TC 270 39:04  1790.0 10.917
10   AO 328 47:06   765.0  4.500
11   AO 416 60:03   920.2  5.447
</code></pre>

<p>Running BoxCox on the data it recommends a transform of the data</p>

<pre><code>lambda &lt;- BoxCox.lambda(data$Sales)
trainingdata = BoxCox(trainingdata,lambda)
tso(trainingdata,types = c(""AO"", ""LS"", ""TC""))
Series: trainingdata 
ARIMA(3,1,1)(2,0,0)[7]                    

Coefficients:
     ar1     ar2      ar3      ma1    sar1    sar2      LS3    AO52     AO53    TC68
  0.3918  0.0993  -0.0587  -0.9856  0.3632  0.4144  13.5805  5.7218  -7.7957  6.3960
s.e.  0.0383  0.0418   0.0416   0.0142  0.0361  0.0341   1.3201  1.2980   1.3041  1.2763
      AO80   AO121   TC229   TC270   AO416     AO445   TC634   AO780
  -23.3707  5.5352  5.8088  7.0446  7.9304  -23.6372  5.5475  6.7194
s.e.    1.2376  1.2307  1.2594  1.2640  1.2476    1.2393  1.2598  1.2353

sigma^2 estimated as 2.332:  log likelihood=-1482.63
AIC=3003.26   AICc=3004.23   BIC=3092.34

Outliers:
type ind   time coefhat   tstat
1    LS   3   1:03  13.581  10.287
2    AO  52   8:03   5.722   4.408
3    AO  53   8:04  -7.796  -5.978
4    TC  68  10:05   6.396   5.012
5    AO  80  12:03 -23.371 -18.883
6    AO 121  18:02   5.535   4.498
7    TC 229  33:05   5.809   4.612
8    TC 270  39:04   7.045   5.573
9    AO 416  60:03   7.930   6.356
10   AO 445  64:04 -23.637 -19.073
11   TC 634  91:04   5.547   4.404
12   AO 780 112:03   6.719   5.439
</code></pre>

<p>Some of these outliers are already taken care of since they're the holidays. I'm not sure how to handle the rest of the outliers when fitting the model and in the CV.</p>

<p>What is the best way to go about taking care of the outliers? I can reset the values of the training data where it's predicted as an outliers to the recommended value if it's not a holiday for fitting the model and then still calculate the MAPE off of the original data. However, there's a LS at index 3 so I'm not sure that would make sense for that.</p>
"
"0.165698604266987","0.180578779628654","126196","<p>I'm developing an app in C# (WPF) that amongst other things, it makes a time-series based forecast of sales (4-5 months into the future). I'm an industrial engineer so I'm not pro in statistics nor in programming (basic knowledge of both).</p>

<p>What I'm doing right now is to aggregate my daily data into monthly data, then I test for monthly seasonality, and then either go for a <strong>Holt</strong>'s exponential smoothing or for a <strong>Holt-Winters</strong>'s one depending on the result. </p>

<p>For determining the <strong>smoothing parameters</strong> I'm using <strong>brute force</strong> (i.e. testing a lot of possible combinations) and keeping the one that would have predict the past year (backtesting) with minimum <a href=""http://en.wikipedia.org/wiki/Mean_absolute_error"" rel=""nofollow"">MAE</a>.</p>

<p>A <strong>problem</strong> arises: this method is SLOW (obviously, as always with brute force). It takes about 0,5s only trying the smoothing parameters in 0.05 intervals which doesn't give much accuracy. I need to do this with 1000+ items so it goes over 8 minutes (too much).</p>

<p>So I have a few <strong>questions</strong>:</p>

<ul>
<li>Is there any method to determine optimal smoothing parameters without testing all of them?</li>
<li>Using <em>R.NET</em> to use the forecast package of R will be faster?</li>
<li><p>If so, should I:</p>

<ul>
<li>Use daily or monthly data?</li>
<li>Make also an auto.arima? How to determine which model is better?</li>
</ul></li>
<li><p>Is my method of backtesting (make a model only with data previous to that point) valid to determine if a model is better than another?</p></li>
</ul>

<p><strong><em>EDIT:</em></strong> I have tried implementing R.NET. Time for <code>ets</code> is about 0,1s if I set which model to use and use only mae as <code>opt.crit</code> (if not, it goes up to 5s). </p>

<p>This is good enough <strong>IF</strong> I could get the same out-of-sample predictions I mention in the comment. If it's not possible then I would have to run it 12 times, adding up to 1,2s which is not fast enough.</p>

<ul>
<li>How can I do that (get predictions over the last 12 data without considering them in the model) in R?</li>
</ul>
"
"0.152203886829552","0.147441956154897","206701","<p>I have some very noisy data that seems like it might have a frequency to it.  I'm trying to build a model with the data, like the example code below.  So I've been experimenting with fourier series predictors.  So far I've just been guessing at the frequency, like frequency = 168 in the example.  I was wondering if anyone could suggest a better way to detect frequency in noisy data.  For example, it's entirely possible that my data actually has a frequency of 6 or 18.  </p>

<p>(Also I know I'm using frequency incorrectly here, but that's because in the ts function they use frequency like we would normally use period.  I hope that's not too confusing.)</p>

<p>Code:    </p>

<pre><code>##Partitioning Time Series
Train&lt;-336


TrainSeries &lt;-xData[1:Train]



##Fourier Terms

tsF&lt;-ts(xData, freq=168)


xregF&lt;-fourier(tsF,2)

xregFTrain&lt;-xregF[1:Train,]

##Model
Model1 &lt;- auto.arima(TrainSeries, xreg=xregFTrain, seasonal = FALSE )
</code></pre>

<p>Data:</p>

<pre><code>dput(xData)
</code></pre>

<p>c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 8.5, 4, 5, 9, 10, 11, 
7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 2, 15, 2.5, 17, 5, 5.5, 
7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 9.5, 3.5, 5, 4, 4, 9, 4.5, 
6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 12, 17.5, 19, 7, 14, 17, 3.5, 
6, 15, 11, 10.5, 11, 13, 9.5, 9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 
19, 6, 7, 7.5, 7.5, 7, 6.5, 9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 
5, 12, 6, NA, 4, 2, 5, 7.5, 11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 
7, 4.5, 9, 3, 4, 6, 17.5, 11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 
7, 7, 4, 7.5, 11, 6, 11, 7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 
6, 8.5, 7.5, 6, 5, 8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 
11.5, 3, 4, 16, 3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 
6.5, 9, 12, 17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 
6.5, 15, 8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 
16.5, 2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 
13, 10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 11.5, 
12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 10, 10, 
13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 5.5, 6, 14, 
16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 13, 6, 7, 3, 5.5, 
7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 13, NA, 12, 1.5, 7, 
7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 8, 6, 3, 7.5)    </p>
"
"0.215248801000253","0.208514414057075","219792","<p>My objective it to manually compute one-step ahead forecast using the estimated coefficientes given by the <code>arima</code> function in R.</p>

<p>I will consider the specific model ARIMA(0,0,0)(0,1,3) with weekly seasonality (<code>period = 7</code>). The equation for this model is:</p>

<p>$$ x_{t} = x_{t-7} + \Theta_{1}e_{t-7} + \Theta_{2}e_{t-14} + \Theta_{3}e_{t-21} + e_{t} $$</p>

<p>I will start by computing the one-step ahead forecast using the <code>predict</code> function and then compare it's value with the result given from the above equation. So first I will have to compute <code>theta</code> vector and the residuals vector <code>e_t</code>.</p>

<p>My data consists of daily observations for 35 days.</p>

<pre><code>data &lt;- c(2570,4530,3990,4480,5880,3380,1340,4180,4600,4170,1980,5170,2900,940,7430,6330,7310,9210,8460,3080,1020,4400,2980,5090,7230,3670,2440,1980,2090,3380,2410,3630,3930,2450,1590)
</code></pre>

<p>I start by fitting the model:</p>

<pre><code>fit &lt;- arima(data, order=c(0,0,0), seasonal=list(order=c(0,1,3), period=7), method=""ML"")
</code></pre>

<p>Then I recover the estimated <code>theta</code> coefficients and the last 3 observed residuals. Note that the seasonality period is 7, so the last 3 residuals regarding this seasonality are as stated:</p>

<ul>
<li>Last residual is given by position <code>35 - 7 + 1 = 29</code></li>
<li>Before last residual is given by position <code>35 - 14 + 1 = 22</code></li>
<li>Before before last residual is given by position <code>35 - 28 + 1 = 15</code></li>
</ul>

<p>So that's the reason I have the funny indexes in line two of the following code:</p>

<pre><code>theta &lt;- as.vector(fit$coef)
e_t &lt;- fit$residuals[c(29,22,15)]
</code></pre>

<p>Finnaly, I also fetch the last observation (given seasonality period 7)</p>

<pre><code>z_t &lt;- data[29]
</code></pre>

<p>And when I compute the above formula:</p>

<pre><code>sum(e_t * theta) + z_t)
</code></pre>

<p>I get the value of <code>4613.141</code> which is different from </p>

<pre><code>predict(fit)$pred[1]
</code></pre>

<p>which returns the value <code>4671.607</code>.</p>

<p>Can you please explain where is my error? I've tried this procedure with several different samples and sample sizes and I never get the same forecast as the R function.</p>
"
"0.18641092980036","0.180578779628654","59058","<p>I'm trying to forecast a seasonal time series based on its historical values, and also two more time series (that are seasonal themselves.)  </p>

<p>I'm trying to use an <strong>auto.arima</strong>, and I'm going to input the other two time series (the exogeneous regressors) as a contatenated list of dummy variables, in auto.arima's <strong>xreg</strong> parameter.</p>

<p>I am having difficulty how to use the forecast function after this point.  I've written up the following code, but I don't understand what I should put in the <strong>xreg</strong> and <strong>newxreg</strong> parameters of the forecast function.</p>

<pre><code>tempfit&lt;-auto.arima(dnew, xreg=dExt)
plot(forecast(tempfit, xreg=dnew1,newxreg=dExt1))
</code></pre>

<p>Also, my data points for these three series were all values per day that had a seven day seasonality. In order to let auto.arima calculate the (p,q,d) for seasonality, I converted them to time series with a frequency of 7. Now, after forecasting is done, the plot shows one unit for every seven days.  How can I covert this back to one unit per day?</p>

<p>Further, do you happen to know how we can input a set of external regressors to an ETS model?</p>

<p>I would greatly appreciate your inputs!</p>

<p>Thank you.</p>

<p><strong>EDIT</strong>:</p>

<p>I just saw the following page from Dr. Hyndman:
<a href=""http://stats.stackexchange.com/questions/34493/time-series-modeling-with-dynamic-regressors-in-sas-vs-in-r"">Time series modeling with dynamic regressors in SAS vs. in R</a></p>

<p>Is it safe to assume that I don't need to enter a newxreg parameter for my forecast?</p>

<p>Also, I really want to know if it's statistically correct to use the two external regressors in xreg, but then also use a number of dummy variables in xreg that will represent the seasonality of these two variables.  </p>
"
"NaN","NaN","166725","<p>I am trying to model an ARIMAX model on my time series.</p>

<pre><code>&gt; dput(tsOenb)
structure(c(1.0227039, -5.0683144, 0.6657713, 3.3161374, -2.1586704, 
-0.7833623, -0.2203209, 2.416144, -1.7625406, -0.1565037, -7.9803936, 
9.4594715, -4.8104584, 8.4827107, -6.1895262, 1.4288595, 1.4896459, 
-0.4198522, -5.1583964, 5.2502294, 1.0567102, -1.0923342, -1.5852298, 
0.6061936, -0.3752335, 2.5008664, -1.3999729, 2.2802166, -2.1468756, 
-1.4890328, -0.79254376, 3.21804705, -0.94407886, -0.27802316, 
-0.20753079, -1.12610048, 2.0883735, -0.7424854, 0.44203729, 
-1.48905938, 1.39644424, -3.8917377, 11.25665848, -9.22884035, 
3.26856762, -0.00179541, -2.39664325, 4.00455574, -5.60891295, 
4.6556348, -4.40536951, 6.64234497, -7.34787319, 7.56303006, 
-8.23083674, 4.43247855, 1.31090412, 1.0227039, -5.0683144), .Tsp = c(2000.25, 
2014.75, 4), class = ""ts"")
&gt; quaterlyDummies &lt;-  seq(ISOdate(2000,4,1), ISOdate(2014,12,31), by = ""quarter"") # or ""3 months""
&gt; month &lt;- month(quaterlyDummies)
&gt; xreg &lt;- model.matrix(~as.factor(month))[1:59]
&gt; fit &lt;- auto.arima(tsOenb, xreg=xreg) 
&gt; accuracy(fit)
                       ME     RMSE      MAE      MPE     MAPE      MASE       ACF1
Training set 7.215274e-17 4.216134 3.144967 45.39733 146.9616 0.8467543 -0.7332704
&gt; plot(tsOenb)
&gt; lines(fitted(fit),col=2)
&gt; 
&gt; fit1 &lt;- auto.arima(tsOenb) 
&gt; accuracy(fit1)
                    ME     RMSE      MAE       MPE     MAPE      MASE        ACF1
Training set 0.0707266 2.306156 1.739384 -3468.434 3627.246 0.4683134 0.003527999
&gt; plot(tsOenb)
&gt; lines(fitted(fit1),col=2)
</code></pre>

<p>Here is the output of my <code>arima model</code>:</p>

<p><a href=""http://i.stack.imgur.com/Aybe9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Aybe9.png"" alt=""enter image description here""></a></p>

<p>Here is a plot of my <code>ARIMAX model</code>:</p>

<p><a href=""http://i.stack.imgur.com/NLcAm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NLcAm.png"" alt=""enter image description here""></a></p>

<p>Any suggestions why my <code>Arimax model</code> is so bad? Were my dummy variables created wrongly?</p>

<p>I appreciate your replies!</p>
"
"0.157195498378372","0.152277397525376","130256","<p>I have a number of time series with strong seasonality and I am using auto.arima() from R's Forecast package along with Fourier and dummy/explanatory variables to address the seasonality to make forecasts for each time series.  In one part of the time series there are two peaks of activity.  I am looking at previous data to see how well my model predicts out-of-sample-error.  For most of my time series, my ARIMA models do a really good job at forecasting the peaks and troughs of activity.   The models will do a good job when estimating the peaks before they happen and also if I were to update the model with recent data during the middle of the first peak.  </p>

<p>My model, however, gets wonky if the first peak was higher than expected.  In this situation, if I estimate the future using only data before the first peak, my model underestimates the first peak but it accurately forecasts the following trough and does a reasonable job at forecasting the second peak.  (See below - Red is estimated activity; Black is observed activity) </p>

<p><img src=""http://i.stack.imgur.com/SXeSp.png"" alt=""Forecast before first peak""></p>

<p>If I try updating the model with recent data during the middle of the first peak, the forecast then substantially overestimates activity during the remainder of the time series. (See below - Red is estimated activity; Black is observed activity)</p>

<p><img src=""http://i.stack.imgur.com/YvP4N.png"" alt=""Forecast made during first peak""></p>

<p>Why does an updated model do this?  And is there a way to address this issue? I know from domain knowledge that even if the first peak is higher than expected the following trough should return back down, more or less, to the originally expected level.  I have tried playing with the Fourier parameter and manually testing out different ARIMA models.  </p>
"
"0.107624400500126","0.104257207028537","168055","<p>I have a line of code in R:</p>

<pre><code>garchFit(substitute(formula~arma(p,q)+garch(1,1),  
list(p=auto.arima(data[i:(i+T-1)])$arma[1],q=auto.arima(data[i:(i+T-1)])$arma[2])),  
data = data[i:(i+T-1)],trace=F)  
</code></pre>

<p>which is fitted repeatedly in a <code>for</code> loop for different <code>i</code> but unfortunately produces an error at when <code>i</code> reaches 3613.</p>

<p>The code is designed to fit ""best-fit"" ARIMA model to the data ANF GARCH(1,1) as there doesn't appear to be automatic ""best fit"" GARCH function in R unlike <code>auto.arima</code>.</p>

<p>The error is: </p>

<pre><code>Error in arima(.series$x, order = c(u, 0, v), include.mean = include.mean) : non-stationary AR part from CSS.
</code></pre>

<p>In addition warning is produced: </p>

<pre><code>In sqrt(diag(fit$cvar)) : NaNs produced.
</code></pre>

<p>Any help on how to solve this issue much appreciated.</p>
"
"0.107624400500126","0.104257207028537","115710","<p>I have been using the forecast package in R to make forecasts based on an ARIMA model and have noticed a difference in the output of the forecast and simulate functions when calculating confidence intervals.</p>

<p>For example the 95% quantile calculated by the forecast function is about 0.5% higher than that based on 10000 applications of the simulate() function.  Also the mean of the simulated values and the point forecasts provided by the forecast functions are slightly different.</p>

<p>Which one of the functions will do the job better?  Or are the differences too small to worry about?  (The only reason I decided to try simulate was so that a distribution could be fitted to the simulated data).</p>

<p>Edit 1: Example</p>

<pre><code>library(forecast)

#Fit arima model to data
dm1 = arima(DAP, order = c(1,1,0), method = ""ML"", seasonal = list(order = c(0,1,1)))   

#Simulate 10000 times
n.mnths = 7
    n.sim = 10000
    domesticsimulator = function(i){
      simulate(dm1, nsim = n.mnths)
    }

sim.d &lt;- sapply(1:n.sim, function(x)domesticsimulator(x))
distr.d.mat&lt;-t(sim.d); distr.d.mat
distr.d&lt;-data.frame(Jun = distr.d.mat[,1],Jul = distr.d.mat[,2], Aug = distr.d.mat[,3], Sep = distr.d.mat[,4], Oct = distr.d.mat[,5], Nov = distr.d.mat[,6], Dec = distr.d.mat[,7]); distr.d

#Compare to forecast
forecast(dm1)
</code></pre>

<p>Edit 2: Data</p>

<blockquote>
  <p>dput(DAP)
  structure(c(43032450L, 41166780L, 49992700L, 47033260L, 49152352L, 
  52209516L, 55810773L, 53920973L, 44213408L, 49944935L, 47059495L, 
  49757124L, 43815481L, 45306644L, 54147227L, 53253194L, 53030873L, 
  56959142L, 59614287L, 57380873L, 47671785L, 54167489L, 51782564L, 
  52640057L, 47977657L, 47074882L, 58838975L, 54908859L, 57323876L, 
  59724061L, 62396446L, 59110633L, 50600325L, 53738093L, 52766404L, 
  52801276L, 48886043L, 47348142L, 58286011L, 55828555L, 57145193L, 
  59297121L, 60838606L, 58303233L, 49949551L, 55088986L, 53852209L, 
  53538970L, 50022168L, 47766421L, 59244232L, 57398267L, 59285571L, 
  61493934L, 63457403L, 62660179L, 52310402L, 57208618L, 55047116L, 
  53291139L, 50245100L, 50118363L, 59213077L, 55611053L, 58047400L, 
  59559171L, 61401480L, 58966473L, 47680101L, 52956023L, 47658141L, 
  50253800L, 44825056L, 43680328L, 53534891L, 52247781L, 52951246L, 
  55898027L, 59468957L, 56568180L, 48235025L, 52279405L, 48584832L, 
  49793527L, 45501620L, 42440614L, 54424077L, 52498074L, 53842422L, 
  56689853L, 59142493L, 57370748L, 50304708L, 54826050L, 51420519L, 
  51076415L, 46305000L, 43657818L, 55649428L, 52858479L, 55982234L, 
  57778699L, 60310568L, 57403835L, 50982170L, 54124363L, 51660083L, 
  51534990L, 47080840L, 46405385L, 56200391L, 53691570L, 55749349L, 
  57903293L, 59688267L, 58646304L, 50134504L, 53779646L, 51844482L, 
  51165451L, 47814031L, 45736763L, 56564538L, 53226735L, 56557964L, 
  57986530L, 59306473L, 58110953L, 50761250L, 54682312L, 50538227L, 
  54329096L, 47941907L, 45486064L, 57729464L, 54821717L, 57145762L
  ), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>
"
"0.12427395320024","0.120385853085769","131041","<p>I have fitted a seasonal ARIMA model using R to a log transformed times series which I called lnseries. </p>

<p>I can forecast fine for the transformed time series (<code>lnseries</code>) storing the ARIMA model (which I called <code>fit</code>) then using the command:</p>

<p>$\texttt{plot(forecast(fit))},$</p>

<p>this shows me the forecast and 95% confidence interval. But I'm stuck on how to get the actual original time series forecast plot using this model.</p>

<p>Help anyone!?</p>
"
"0.165698604266987","0.180578779628654","33862","<p>I have some models built with the <code>auto.arima</code> function from the <code>forecast</code> package. I'm modeling a variable called 'natural efluent energy' (ena), which is how much energy you can extract from some Hydrography region. There are 2 regressor variables (rainfall precipitation from period $t$ and $t-1$.)</p>

<p>Each region has it's own model - some series show positive trend, some shows negative trend, and some seems stationary. The problem is that some forecasts 'from <code>auto.arima</code>' are giving values higher/lower than usual (some forecasts give me negative values, which are not possible).</p>

<p>My original call is below:</p>

<pre><code> m1 = auto.arima(serie, xreg = regvars)
</code></pre>

<p>For the data on the link, I changed it to</p>

<pre><code> m1 = auto.arima(serie, xreg = regvars, max.P = 0, max.Q = 0, stationary = TRUE)
</code></pre>

<p>Then I get good forecasts in this case. My question is, what these parameters(<code>max.P</code>, <code>max.Q</code>) actually control, and how they relate to the trend show by my model variable?</p>

<p>Here is a link for the historic data:
<a href=""http://www.datafilehost.com/download-7718b3fc.html"" rel=""nofollow"">http://www.datafilehost.com/download-7718b3fc.html</a></p>

<p>And here a link for the forecast regressors:
<a href=""http://www.datafilehost.com/download-ca44dfa4.html"" rel=""nofollow"">http://www.datafilehost.com/download-ca44dfa4.html</a></p>

<p>And here a link of mean historic values, the forecast must fall between these values:
<a href=""http://www.datafilehost.com/download-e1e265b7.html"" rel=""nofollow"">http://www.datafilehost.com/download-e1e265b7.html</a></p>

<p>My data starts at 2001/Jun, so the serie is:</p>

<pre><code>  y = ts(dframe$ena, freq = 12, start = c(2001, 6))
</code></pre>
"
"0.138942503594182","0.134595475514541","105367","<p>I have the weekly revenue data for an electronics company the decomposed plot of which is as follows:  </p>

<p><img src=""http://i.stack.imgur.com/9HWE6.png"" alt=""enter image description here""></p>

<p>I have decided to keep the seasonality and apply a suitable forecasting technique. I tried auto.arima:</p>

<pre><code>&gt; Elec &lt;- read.xlsx(""C:/Users/Himanshu.raunak/Revenue/Electronics.xlsx"", 1)
&gt; Elec$Date &lt;- as.Date(Elec$Date, format=""%Y-%m-%d"")
&gt; ElecTimeSeries &lt;- ts(Elec$Revenue, frequency=52)
&gt; ElecArima &lt;- auto.arima(ElecTimeSeries)
&gt; plot(forecast(ElecArima))
</code></pre>

<p>I get the following plot:</p>

<p><img src=""http://i.stack.imgur.com/fYKNN.png"" alt=""enter image description here""> </p>

<p>And the following warning messages:</p>

<p>1: In myarima(x, order = c(p, d, q), seasonal = c(P, D, Q),  ... :
  Unable to check for unit roots</p>

<p>2: In myarima(x, order = c(p, d, q), seasonal = c(P, D, Q),  ... :
  Unable to check for unit roots</p>

<p>3: In myarima(x, order = c(max.p > 0, d, 0), seasonal = c((m >  ... :
  Unable to check for unit roots</p>

<p>and so on.</p>

<p>The ARIMA parameters come out to be as follows:</p>

<p>ARIMA(2,1,2)(0,0,1)[52] with drift</p>

<p>Coefficients:</p>

<pre><code>       ar1      ar2      ma1     ma2    sma1     drift

      0.5282  -0.0316  -1.3125  0.3225  0.2283  2497.993

s.e.    NaN      NaN      NaN     NaN    0.0728    NaN

sigma^2 estimated as 3.563e+11:  log likelihood=-2931.26

AIC=5876.51   AICc=5877.1   BIC=5899.6
</code></pre>

<p>I realize that the AIC values are quite large.</p>

<p>Could you please point out at what I am doing incorrectly (warning messages and large ARIMA parameters) and provide a better solution. Also I need help understanding the ARIMA plot.</p>
"
"0.196494372972965","0.19034674690672","140163","<p>I am working on a small project where we are trying to predict the prices of commodities (Oil, Aluminium, Tin, etc.) for the next 6 months. I have 12 such variables to predict and I have data from Apr, 2008 - May, 2013.</p>

<p>How should I go about prediction? I have done the following:</p>

<ul>
<li>Imported data as a Timeseries dataset </li>
<li>All variable's seasonality tends to vary with Trend, so I am going to multiplicative model. </li>
<li>I took log of the variable to convert into additive model </li>
<li>For each variable decomposed the data using STL</li>
</ul>

<p>I am planning to use Holt Winters exponential smoothing, ARIMA and neural net to forecast. I split the data as training and testing (80, 20). Planning to choose the model with less MAE, MPE, MAPE and MASE.</p>

<p>Am I doing it right?</p>

<p>Also one question I had was, before passing to ARIMA or neural net should I smooth the data? If yes, using what? The data shows both Seasonality and trend.</p>

<p>EDIT:</p>

<p>Attaching the timeseries plot and data
<img src=""http://i.stack.imgur.com/V0wes.png"" alt=""enter image description here""></p>

<pre><code>Year  &lt;- c(2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009, 
           2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010, 
           2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 
           2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 
           2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 
           2012, 2012, 2013, 2013)
Month &lt;- c(4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 
           12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 
           8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2) 
Coil  &lt;- c(44000, 44500, 42000, 45000, 42500, 41000, 39000, 35000, 34000, 
           29700, 29700, 29000, 30000, 30000, 31000, 31000, 33500, 33500, 
           33000, 31500, 34000, 35000, 35000, 36000, 38500, 38500, 35500, 
           33500, 34500, 36000, 35500, 34500, 35500, 38500, 44500, 40700, 
           40500, 39100, 39100, 39100, 38600, 39500, 39500, 38500, 39500, 
           40000, 40000, 40500, 41000, 41000, 41000, 40500, 40000, 39300, 
           39300, 39300, 39300, 39300, 39800)
coil &lt;- data.frame(Year = Year, Month = Month, Coil = Coil)
</code></pre>

<p><strong>EDIT 2:</strong>
One question, can you please tell me if my data has any seasonality or trend? And also please give me some tips on how to identify them.
<img src=""http://i.stack.imgur.com/Hg1yp.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/PdNwJ.png"" alt=""enter image description here""></p>
"
"0.09320546490018","0.120385853085769","78681","<p>I have a SARIMA forecast from statewide Real Estate Sales data.. but I'm not happy with it. The SARIMA parameters are confusing to say the least.</p>

<p>I am finding that the current model is not forecasting high enough, although the month by month fluctuations look reasonable. Currently the projected year ahead is 4.9% above this year. but the year over year gain for the current year is about 10%+, so in short the model is not weighted heavily enough to the recent year over year change. Between the ARIMA and Seasonal components I can't see how to fix/specify this? The Parameter descriptions remain a mystery no matter how many versions I read. Looking at the chart it is clear that the growth rate over the last three years is higher than the current forecast, Surely there is a simple way to deal with this sort of specification in SARIMA, Bonus if it is understandable by mere mortals!?</p>

<p>R code follows (source data is public) I'm a real newb at R as well.</p>

<pre><code>require(RCurl)
myCsv &lt;- getURL(""https://docs.google.com/spreadsheet/pub?key=0Ak_wF7ZGeMmHdFZtQjI1a1hhUWR2UExCa2E4MFhiWWc&amp;single=true&amp;gid=1&amp;output=csv"")
fullmatrix &lt;- read.csv(textConnection(myCsv), stringsAsFactors = FALSE)
fullmatrix$date &lt;- as.Date(fullmatrix$date, format = ""%m/%d/%y"")
fullmatrix$Unit.Sales &lt;- as.numeric(gsub("","", """", fullmatrix$Unit.Sales))
fullmatrix$Average.Selling.Price &lt;- as.numeric(gsub("","", """",  fullmatrix$Average.Selling.Price))
fullmatrix$Median.Selling.Price  &lt;- as.numeric(gsub("","", """", fullmatrix$Median.Selling.Price))
fullmatrix$Total.For.Sale &lt;- as.numeric(gsub("","", """", fullmatrix$Total.For.Sale))
order.date &lt;- order(fullmatrix$date )
    fullmatrix &lt;-fullmatrix[order.date, ]
    library(xts)
    require(forecast)
    require(astsa)
    sarima(fullmatrix$Unit.Sales,12, 0, 1, 1, 0,1,0) # (slightly higher fore)
fore &lt;- sarima.for(fullmatrix$Unit.Sales,12, 12, 0, 1, 1,0,1,12)#specifies same model 
    fullmatrix.xts &lt;- as.xts(x=fullmatrix[,-1],order.by=fullmatrix$date)
unitsales.xts &lt;- as.xts(as.numeric(fullmatrix.xts$Unit.Sales),order.by=index(fullmatrix.xts))
    forecast.xts &lt;- as.xts(as.numeric(fore$pred), order.by=as.Date(c(""2013-11-01"",""2013-12-01"",""2014-01-01"",""2014-02-01"",""2014-03-01"",""2014-04-01"",""2014-05-01"",""2014-06-01"",""2014-07-01"",""2014-08-01"",""2014-09-01"",""2014-10-01"")))
orig_fore &lt;-rbind(unitsales.xts,forecast.xts)
colnames(orig_fore) &lt;- ""Unit.Sales""
plot(orig_fore)
</code></pre>

<p><img src=""http://i.stack.imgur.com/v26XT.png"" alt=""Actual with Forecast""></p>
"
"0.0878749550327494","0.0851256530758749","34226","<p>I'm using a STL decomposition to make forecasts in R (using the <code>forecast</code> package), but I'm not sure how to incorporate my regressors into the model.</p>

<p>I'm using the forecast function:</p>

<pre><code>f = forecast(stl(my.data, s.window = 'periodic'), h = 12, method = 'arima')
</code></pre>

<p>The function above accepts the <code>xreg</code> parameter, but how do I specify the
model regressors and the forecast regressors?</p>

<p>Is there a function that does this or do I have to do it 'by hand'?</p>
"
"0.152203886829552","0.147441956154897","34493","<p>I am using both R and SAS for the time series modeling. There is an option in SAS that I could not find so far in any packages developed in R for the time series modeling such as TSA or forecast package, at least to the best of my knowledge! To explain more, if we use the windowing environment in SAS to fit an ARIMA model with a regressor, we basically choose:  </p>

<p>Solution->Analysis->Time series Forecasting System->Develop Models<br>
Then Fit ARIMA model -> Predictors->Dynamic Regressors</p>

<p>If we ask to forecast this model, SAS says â€œThe following regressor(s) do not have any forecasting models. The system will automatically select forecasting models for these regressorsâ€. This means that we have not provided the values of the regressors over the forecasting period, and the system tries to find a model for that.</p>

<p>My questions:</p>

<ol>
<li>Is there any package in R with the same capability (explained above) as in SAS to forecast an ARIMA model?  </li>
<li>How can SAS automatically forecast the regressor(s) and based on what models?</li>
</ol>
"
"NaN","NaN","207987","<p>I have fit an ARIMA model to a time series with function <code>auto.arima</code> from ""forecast"" package in R. I wanted to check prediction intervals for robustness by changing the ARIMA terms. </p>

<p>Here is my R code:</p>

<pre><code>library(""forecast"", lib.loc=""~/R/win-library/3.2"")
library(""tseries"", lib.loc=""~/R/win-library/3.2"")

price = c(256, 223, 190, 170 ,140, 123, 133, 133, 125, 120, 125, 140, 166, 186, 206, 206, 206, 206, 206, 206,
       229, 263, 273, 273 ,273 ,273 ,258, 239, 233, 226, 226, 226, 249, 249, 249, 249, 249, 269, 279, 279,
       279, 279, 299, 316, 316, 316, 316, 316, 316, 316, 299, 299, 299 ,319, 319, 339 ,339, 356 ,356, 356,
       343, 343, 333 ,343 ,442 ,599, 599, 599, 599, 549, 516, 336, 336, 336, 309, 309 ,319, 565, 665, 832,
       832, 698, 632, 532, 499, 526, 526, 526, 526, 499, 466, 333 ,233, 233, 216, 200, 200, 200, 226, 239,
       279, 316, 333 ,366 ,366 ,366, 366 ,366 ,333 ,349 ,349, 349 ,359 ,359, 442 ,459 ,449 ,449, 449, 449,
       449, 449 ,449 ,459, 459 ,459, 459, 459, 446, 446, 446, 446, 459, 459, 439, 439, 439, 439, 482, 482,
       482, 482 ,516,516, 532, 532, 532 ,532 ,532 ,549, 599, 632 ,632 ,632, 632, 599 ,565 ,532, 482, 482,
       482, 482, 499 ,475 ,449, 416)

ts.plot(price)

auto.arima(price)

arima.fit&lt;-Arima(price, c(2,1,4), include.drift=TRUE)
plot(forecast.Arima(arima.fit, 60), ylim=c(-300,1300))

arima.fit&lt;-Arima(price, c(2,1,3), include.drift=TRUE)
plot(forecast.Arima(arima.fit, 60), ylim=c(-300,1300))
</code></pre>

<p>What I saw surprised me quite a bit:</p>

<p><a href=""http://i.stack.imgur.com/SHPAE.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SHPAE.jpg"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/9pNVK.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9pNVK.jpg"" alt=""enter image description here""></a></p>

<p>Why do the prediction intervals widen in the MA(3) case and hardly so in the MA(4) case? </p>
"
"0.107624400500126","0.104257207028537","147279","<p>I am trying to do time series analysis in R. 
I have data time series data set like this. </p>

<pre><code>    Month       Year    Value 
    December    2013    5300
    January     2014    289329.8
    February    2014    596518
    March       2014    328457
    April       2014    459600
    May         2014    391356
    June        2014    406288
    July        2014    644339
    August      2014    251238
    September   2014    386466.5
    October     2014    459792
    November    2014    641724
    December    2014    399831
    January     2015    210759
    February    2015    121690
    March       2015    280070
    April       2015    41336
</code></pre>

<p>Googling I found I can use auto.arima function to forecast the result. 
I managed to write R code to do forecast using auto.arima function </p>

<pre><code>    data &lt;- c(5300,289329.8,596518,328457,459600,391356,406288,644339,251238,386466.5,459792,641724,399831,210759,121690,280070,41336)
    data.ts &lt;- ts(data, start=c(2013, 12), end=c(2015, 4), frequency=12) 
    plot(data.ts)
    fit &lt;- auto.arima(data.ts)
    forec &lt;- forecast(fit)
    plot(forec)
</code></pre>

<p>Problem is my forecast result always remain same. </p>

<p><img src=""http://i.stack.imgur.com/SuJ6a.png"" alt=""enter image description here""></p>

<p>Could  any tell me what is going wrong. or help me to correct my forecast result. Thanks</p>
"
"0.0878749550327494","0.0851256530758749","208091","<p>I'm trying to understand how the rolling forecast example below from <a href=""http://robjhyndman.com/hyndsight/rolling-forecasts/"" rel=""nofollow"">Rob Hyndman's blog</a> works. In the final line of the <code>for</code> loop, is <code>fc</code> forecasting horizons into the future beyond the end of test?  Or is <code>fc</code> meant to be a forecasted version of test, that could be compared to check for accuracy? </p>

<p>My own goal is to create something similar that would train a model and forecast it several horizons in to the future.</p>

<p>Code:</p>

<pre><code>library(""fpp"")
library(""forecast"")

##Multi-step forecasts without re-estimation

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, model=fit)
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}
</code></pre>
"
"0.107624400500126","0.104257207028537","208080","<p>I compared the <code>auto.arima</code> forecast <code>checkts</code> below  to the rolling forecast <code>fc</code> and noticed that every of the error measures is lower for <code>fc</code>.  </p>

<p>Will rolling forecasts have lower errors than a forecasted <code>auto.arima</code> model in general?<br>
Why might that happen? </p>

<p>The data to run the code below is in the ""fpp"" package. Code:</p>

<pre><code>library(""fpp"")
library(""forecast"")

##Multi-step forecasts without re-estimation

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, model=fit)
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}


checkts&lt;-forecast(fit,h=71)

accuracy(checkts$mean,test)
	accuracy(fc,test) ##All Error measures are lower than Checkts$mean
</code></pre>
"
"0.12427395320024","0.120385853085769","208321","<p>I am trying to forecast the median wait time each hour for a customer to get served in a call center.  I know the median wait times each hour and the number of customers who called in each hour (CustCount) in the past, but I don't know how many operators were staffed each hour to answer calls.  I imagine the call center increases staff during the busy times of day, but I don't know.  My data is also very noisy and it's hard to see any clear patterns.</p>

<p>If anyone can suggest strategy or point to a similar example I would be grateful.  I've been experimenting with Arima models with predictors.  </p>

<p>I'm really wondering how much the staffing levels matter and how they could be identified or addressed.  I was thinking maybe looking for level shifts might be an approach.</p>

<p>I have some sample data below.</p>

<p>Data:</p>

<pre><code>dput(dfE86[1:525,c(""DateTime"",""WaitTime"",""CustCount"")])
</code></pre>

<p>structure(list(DateTime = c(""2015-01-01 00:00"", ""2015-01-01 01:00"", 
""2015-01-01 02:00"", ""2015-01-01 03:00"", ""2015-01-01 04:00"", ""2015-01-01 05:00"", 
""2015-01-01 06:00"", ""2015-01-01 07:00"", ""2015-01-01 08:00"", ""2015-01-01 09:00"", 
""2015-01-01 10:00"", ""2015-01-01 11:00"", ""2015-01-01 12:00"", ""2015-01-01 13:00"", 
""2015-01-01 14:00"", ""2015-01-01 15:00"", ""2015-01-01 16:00"", ""2015-01-01 17:00"", 
""2015-01-01 18:00"", ""2015-01-01 19:00"", ""2015-01-01 20:00"", ""2015-01-01 21:00"", 
""2015-01-01 22:00"", ""2015-01-01 23:00"", ""2015-01-02 00:00"", ""2015-01-02 01:00"", 
""2015-01-02 02:00"", ""2015-01-02 03:00"", ""2015-01-02 04:00"", ""2015-01-02 05:00"", 
""2015-01-02 06:00"", ""2015-01-02 07:00"", ""2015-01-02 08:00"", ""2015-01-02 09:00"", 
""2015-01-02 10:00"", ""2015-01-02 11:00"", ""2015-01-02 12:00"", ""2015-01-02 13:00"", 
""2015-01-02 14:00"", ""2015-01-02 15:00"", ""2015-01-02 16:00"", ""2015-01-02 17:00"", 
""2015-01-02 18:00"", ""2015-01-02 19:00"", ""2015-01-02 20:00"", ""2015-01-02 21:00"", 
""2015-01-02 22:00"", ""2015-01-02 23:00"", ""2015-01-03 00:00"", ""2015-01-03 01:00"", 
""2015-01-03 02:00"", ""2015-01-03 03:00"", ""2015-01-03 04:00"", ""2015-01-03 05:00"", 
""2015-01-03 06:00"", ""2015-01-03 07:00"", ""2015-01-03 08:00"", ""2015-01-03 09:00"", 
""2015-01-03 10:00"", ""2015-01-03 11:00"", ""2015-01-03 12:00"", ""2015-01-03 13:00"", 
""2015-01-03 14:00"", ""2015-01-03 15:00"", ""2015-01-03 16:00"", ""2015-01-03 17:00"", 
""2015-01-03 18:00"", ""2015-01-03 19:00"", ""2015-01-03 20:00"", ""2015-01-03 21:00"", 
""2015-01-03 22:00"", ""2015-01-03 23:00"", ""2015-01-04 00:00"", ""2015-01-04 01:00"", 
""2015-01-04 02:00"", ""2015-01-04 03:00"", ""2015-01-04 04:00"", ""2015-01-04 05:00"", 
""2015-01-04 06:00"", ""2015-01-04 07:00"", ""2015-01-04 08:00"", ""2015-01-04 09:00"", 
""2015-01-04 10:00"", ""2015-01-04 11:00"", ""2015-01-04 12:00"", ""2015-01-04 13:00"", 
""2015-01-04 14:00"", ""2015-01-04 15:00"", ""2015-01-04 16:00"", ""2015-01-04 17:00"", 
""2015-01-04 18:00"", ""2015-01-04 19:00"", ""2015-01-04 20:00"", ""2015-01-04 21:00"", 
""2015-01-04 22:00"", ""2015-01-04 23:00"", ""2015-01-05 00:00"", ""2015-01-05 01:00"", 
""2015-01-05 02:00"", ""2015-01-05 03:00"", ""2015-01-05 04:00"", ""2015-01-05 05:00"", 
""2015-01-05 06:00"", ""2015-01-05 07:00"", ""2015-01-05 08:00"", ""2015-01-05 09:00"", 
""2015-01-05 10:00"", ""2015-01-05 11:00"", ""2015-01-05 12:00"", ""2015-01-05 13:00"", 
""2015-01-05 14:00"", ""2015-01-05 15:00"", ""2015-01-05 16:00"", ""2015-01-05 17:00"", 
""2015-01-05 18:00"", ""2015-01-05 19:00"", ""2015-01-05 20:00"", ""2015-01-05 21:00"", 
""2015-01-05 22:00"", ""2015-01-05 23:00"", ""2015-01-06 00:00"", ""2015-01-06 01:00"", 
""2015-01-06 02:00"", ""2015-01-06 03:00"", ""2015-01-06 04:00"", ""2015-01-06 05:00"", 
""2015-01-06 06:00"", ""2015-01-06 07:00"", ""2015-01-06 08:00"", ""2015-01-06 09:00"", 
""2015-01-06 10:00"", ""2015-01-06 11:00"", ""2015-01-06 12:00"", ""2015-01-06 13:00"", 
""2015-01-06 14:00"", ""2015-01-06 15:00"", ""2015-01-06 16:00"", ""2015-01-06 17:00"", 
""2015-01-06 18:00"", ""2015-01-06 19:00"", ""2015-01-06 20:00"", ""2015-01-06 21:00"", 
""2015-01-06 22:00"", ""2015-01-06 23:00"", ""2015-01-07 00:00"", ""2015-01-07 01:00"", 
""2015-01-07 02:00"", ""2015-01-07 03:00"", ""2015-01-07 04:00"", ""2015-01-07 05:00"", 
""2015-01-07 06:00"", ""2015-01-07 07:00"", ""2015-01-07 08:00"", ""2015-01-07 09:00"", 
""2015-01-07 10:00"", ""2015-01-07 11:00"", ""2015-01-07 12:00"", ""2015-01-07 13:00"", 
""2015-01-07 14:00"", ""2015-01-07 15:00"", ""2015-01-07 16:00"", ""2015-01-07 17:00"", 
""2015-01-07 18:00"", ""2015-01-07 19:00"", ""2015-01-07 20:00"", ""2015-01-07 21:00"", 
""2015-01-07 22:00"", ""2015-01-07 23:00"", ""2015-01-08 00:00"", ""2015-01-08 01:00"", 
""2015-01-08 02:00"", ""2015-01-08 03:00"", ""2015-01-08 04:00"", ""2015-01-08 05:00"", 
""2015-01-08 06:00"", ""2015-01-08 07:00"", ""2015-01-08 08:00"", ""2015-01-08 09:00"", 
""2015-01-08 10:00"", ""2015-01-08 11:00"", ""2015-01-08 12:00"", ""2015-01-08 13:00"", 
""2015-01-08 14:00"", ""2015-01-08 15:00"", ""2015-01-08 16:00"", ""2015-01-08 17:00"", 
""2015-01-08 18:00"", ""2015-01-08 19:00"", ""2015-01-08 20:00"", ""2015-01-08 21:00"", 
""2015-01-08 22:00"", ""2015-01-08 23:00"", ""2015-01-09 00:00"", ""2015-01-09 01:00"", 
""2015-01-09 02:00"", ""2015-01-09 03:00"", ""2015-01-09 04:00"", ""2015-01-09 05:00"", 
""2015-01-09 06:00"", ""2015-01-09 07:00"", ""2015-01-09 08:00"", ""2015-01-09 09:00"", 
""2015-01-09 10:00"", ""2015-01-09 11:00"", ""2015-01-09 12:00"", ""2015-01-09 13:00"", 
""2015-01-09 14:00"", ""2015-01-09 15:00"", ""2015-01-09 16:00"", ""2015-01-09 17:00"", 
""2015-01-09 18:00"", ""2015-01-09 19:00"", ""2015-01-09 20:00"", ""2015-01-09 21:00"", 
""2015-01-09 22:00"", ""2015-01-09 23:00"", ""2015-01-10 00:00"", ""2015-01-10 01:00"", 
""2015-01-10 02:00"", ""2015-01-10 03:00"", ""2015-01-10 04:00"", ""2015-01-10 05:00"", 
""2015-01-10 06:00"", ""2015-01-10 07:00"", ""2015-01-10 08:00"", ""2015-01-10 09:00"", 
""2015-01-10 10:00"", ""2015-01-10 11:00"", ""2015-01-10 12:00"", ""2015-01-10 13:00"", 
""2015-01-10 14:00"", ""2015-01-10 15:00"", ""2015-01-10 16:00"", ""2015-01-10 17:00"", 
""2015-01-10 18:00"", ""2015-01-10 19:00"", ""2015-01-10 20:00"", ""2015-01-10 21:00"", 
""2015-01-10 22:00"", ""2015-01-10 23:00"", ""2015-01-11 00:00"", ""2015-01-11 01:00"", 
""2015-01-11 02:00"", ""2015-01-11 03:00"", ""2015-01-11 04:00"", ""2015-01-11 05:00"", 
""2015-01-11 06:00"", ""2015-01-11 07:00"", ""2015-01-11 08:00"", ""2015-01-11 09:00"", 
""2015-01-11 10:00"", ""2015-01-11 11:00"", ""2015-01-11 12:00"", ""2015-01-11 13:00"", 
""2015-01-11 14:00"", ""2015-01-11 15:00"", ""2015-01-11 16:00"", ""2015-01-11 17:00"", 
""2015-01-11 18:00"", ""2015-01-11 19:00"", ""2015-01-11 20:00"", ""2015-01-11 21:00"", 
""2015-01-11 22:00"", ""2015-01-11 23:00"", ""2015-01-12 00:00"", ""2015-01-12 01:00"", 
""2015-01-12 02:00"", ""2015-01-12 03:00"", ""2015-01-12 04:00"", ""2015-01-12 05:00"", 
""2015-01-12 06:00"", ""2015-01-12 07:00"", ""2015-01-12 08:00"", ""2015-01-12 09:00"", 
""2015-01-12 10:00"", ""2015-01-12 11:00"", ""2015-01-12 12:00"", ""2015-01-12 13:00"", 
""2015-01-12 14:00"", ""2015-01-12 15:00"", ""2015-01-12 16:00"", ""2015-01-12 17:00"", 
""2015-01-12 18:00"", ""2015-01-12 19:00"", ""2015-01-12 20:00"", ""2015-01-12 21:00"", 
""2015-01-12 22:00"", ""2015-01-12 23:00"", ""2015-01-13 00:00"", ""2015-01-13 01:00"", 
""2015-01-13 02:00"", ""2015-01-13 03:00"", ""2015-01-13 04:00"", ""2015-01-13 05:00"", 
""2015-01-13 06:00"", ""2015-01-13 07:00"", ""2015-01-13 08:00"", ""2015-01-13 09:00"", 
""2015-01-13 10:00"", ""2015-01-13 11:00"", ""2015-01-13 12:00"", ""2015-01-13 13:00"", 
""2015-01-13 14:00"", ""2015-01-13 15:00"", ""2015-01-13 16:00"", ""2015-01-13 17:00"", 
""2015-01-13 18:00"", ""2015-01-13 19:00"", ""2015-01-13 20:00"", ""2015-01-13 21:00"", 
""2015-01-13 22:00"", ""2015-01-13 23:00"", ""2015-01-14 00:00"", ""2015-01-14 01:00"", 
""2015-01-14 02:00"", ""2015-01-14 03:00"", ""2015-01-14 04:00"", ""2015-01-14 05:00"", 
""2015-01-14 06:00"", ""2015-01-14 07:00"", ""2015-01-14 08:00"", ""2015-01-14 09:00"", 
""2015-01-14 10:00"", ""2015-01-14 11:00"", ""2015-01-14 12:00"", ""2015-01-14 13:00"", 
""2015-01-14 14:00"", ""2015-01-14 15:00"", ""2015-01-14 16:00"", ""2015-01-14 17:00"", 
""2015-01-14 18:00"", ""2015-01-14 19:00"", ""2015-01-14 20:00"", ""2015-01-14 21:00"", 
""2015-01-14 22:00"", ""2015-01-14 23:00"", ""2015-01-15 00:00"", ""2015-01-15 01:00"", 
""2015-01-15 02:00"", ""2015-01-15 03:00"", ""2015-01-15 04:00"", ""2015-01-15 05:00"", 
""2015-01-15 06:00"", ""2015-01-15 07:00"", ""2015-01-15 08:00"", ""2015-01-15 09:00"", 
""2015-01-15 10:00"", ""2015-01-15 11:00"", ""2015-01-15 12:00"", ""2015-01-15 13:00"", 
""2015-01-15 14:00"", ""2015-01-15 15:00"", ""2015-01-15 16:00"", ""2015-01-15 17:00"", 
""2015-01-15 18:00"", ""2015-01-15 19:00"", ""2015-01-15 20:00"", ""2015-01-15 21:00"", 
""2015-01-15 22:00"", ""2015-01-15 23:00"", ""2015-01-16 00:00"", ""2015-01-16 01:00"", 
""2015-01-16 02:00"", ""2015-01-16 03:00"", ""2015-01-16 04:00"", ""2015-01-16 05:00"", 
""2015-01-16 06:00"", ""2015-01-16 07:00"", ""2015-01-16 08:00"", ""2015-01-16 09:00"", 
""2015-01-16 10:00"", ""2015-01-16 11:00"", ""2015-01-16 12:00"", ""2015-01-16 13:00"", 
""2015-01-16 14:00"", ""2015-01-16 15:00"", ""2015-01-16 16:00"", ""2015-01-16 17:00"", 
""2015-01-16 18:00"", ""2015-01-16 19:00"", ""2015-01-16 20:00"", ""2015-01-16 21:00"", 
""2015-01-16 22:00"", ""2015-01-16 23:00"", ""2015-01-17 00:00"", ""2015-01-17 01:00"", 
""2015-01-17 02:00"", ""2015-01-17 03:00"", ""2015-01-17 04:00"", ""2015-01-17 05:00"", 
""2015-01-17 06:00"", ""2015-01-17 07:00"", ""2015-01-17 08:00"", ""2015-01-17 09:00"", 
""2015-01-17 10:00"", ""2015-01-17 11:00"", ""2015-01-17 12:00"", ""2015-01-17 13:00"", 
""2015-01-17 14:00"", ""2015-01-17 15:00"", ""2015-01-17 16:00"", ""2015-01-17 17:00"", 
""2015-01-17 18:00"", ""2015-01-17 19:00"", ""2015-01-17 20:00"", ""2015-01-17 21:00"", 
""2015-01-17 22:00"", ""2015-01-17 23:00"", ""2015-01-18 00:00"", ""2015-01-18 01:00"", 
""2015-01-18 02:00"", ""2015-01-18 03:00"", ""2015-01-18 04:00"", ""2015-01-18 05:00"", 
""2015-01-18 06:00"", ""2015-01-18 07:00"", ""2015-01-18 08:00"", ""2015-01-18 09:00"", 
""2015-01-18 10:00"", ""2015-01-18 11:00"", ""2015-01-18 12:00"", ""2015-01-18 13:00"", 
""2015-01-18 14:00"", ""2015-01-18 15:00"", ""2015-01-18 16:00"", ""2015-01-18 17:00"", 
""2015-01-18 18:00"", ""2015-01-18 19:00"", ""2015-01-18 20:00"", ""2015-01-18 21:00"", 
""2015-01-18 22:00"", ""2015-01-18 23:00"", ""2015-01-19 00:00"", ""2015-01-19 01:00"", 
""2015-01-19 02:00"", ""2015-01-19 03:00"", ""2015-01-19 04:00"", ""2015-01-19 05:00"", 
""2015-01-19 06:00"", ""2015-01-19 07:00"", ""2015-01-19 08:00"", ""2015-01-19 09:00"", 
""2015-01-19 10:00"", ""2015-01-19 11:00"", ""2015-01-19 12:00"", ""2015-01-19 13:00"", 
""2015-01-19 14:00"", ""2015-01-19 15:00"", ""2015-01-19 16:00"", ""2015-01-19 17:00"", 
""2015-01-19 18:00"", ""2015-01-19 19:00"", ""2015-01-19 20:00"", ""2015-01-19 21:00"", 
""2015-01-19 22:00"", ""2015-01-19 23:00"", ""2015-01-20 00:00"", ""2015-01-20 01:00"", 
""2015-01-20 02:00"", ""2015-01-20 03:00"", ""2015-01-20 04:00"", ""2015-01-20 05:00"", 
""2015-01-20 06:00"", ""2015-01-20 07:00"", ""2015-01-20 08:00"", ""2015-01-20 09:00"", 
""2015-01-20 10:00"", ""2015-01-20 11:00"", ""2015-01-20 12:00"", ""2015-01-20 13:00"", 
""2015-01-20 14:00"", ""2015-01-20 15:00"", ""2015-01-20 16:00"", ""2015-01-20 17:00"", 
""2015-01-20 18:00"", ""2015-01-20 19:00"", ""2015-01-20 20:00"", ""2015-01-20 21:00"", 
""2015-01-20 22:00"", ""2015-01-20 23:00"", ""2015-01-21 00:00"", ""2015-01-21 01:00"", 
""2015-01-21 02:00"", ""2015-01-21 03:00"", ""2015-01-21 04:00"", ""2015-01-21 05:00"", 
""2015-01-21 06:00"", ""2015-01-21 07:00"", ""2015-01-21 08:00"", ""2015-01-21 09:00"", 
""2015-01-21 10:00"", ""2015-01-21 11:00"", ""2015-01-21 12:00"", ""2015-01-21 13:00"", 
""2015-01-21 14:00"", ""2015-01-21 15:00"", ""2015-01-21 16:00"", ""2015-01-21 17:00"", 
""2015-01-21 18:00"", ""2015-01-21 19:00"", ""2015-01-21 20:00"", ""2015-01-21 21:00"", 
""2015-01-21 22:00"", ""2015-01-21 23:00"", ""2015-01-22 00:00"", ""2015-01-22 01:00"", 
""2015-01-22 02:00"", ""2015-01-22 03:00"", ""2015-01-22 04:00"", ""2015-01-22 05:00"", 
""2015-01-22 06:00"", ""2015-01-22 07:00"", ""2015-01-22 08:00"", ""2015-01-22 09:00"", 
""2015-01-22 10:00"", ""2015-01-22 11:00"", ""2015-01-22 12:00"", ""2015-01-22 13:00"", 
""2015-01-22 14:00"", ""2015-01-22 15:00"", ""2015-01-22 16:00"", ""2015-01-22 17:00"", 
""2015-01-22 18:00"", ""2015-01-22 19:00"", ""2015-01-22 20:00""), 
    WaitTime = c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 
    8.5, 4, 5, 9, 10, 11, 7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 
    2, 15, 2.5, 17, 5, 5.5, 7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 
    9.5, 3.5, 5, 4, 4, 9, 4.5, 6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 
    12, 17.5, 19, 7, 14, 17, 3.5, 6, 15, 11, 10.5, 11, 13, 9.5, 
    9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 19, 6, 7, 7.5, 7.5, 7, 6.5, 
    9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 5, 12, 6, NA, 4, 2, 5, 7.5, 
    11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 7, 4.5, 9, 3, 4, 6, 17.5, 
    11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 7, 7, 4, 7.5, 11, 6, 11, 
    7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 6, 8.5, 7.5, 6, 5, 
    8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 11.5, 3, 4, 16, 
    3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 6.5, 9, 12, 
    17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 6.5, 15, 
    8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 16.5, 
    2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 13, 
    10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
    NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 
    11.5, 12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 
    10, 10, 13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 
    5.5, 6, 14, 16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 
    13, 6, 7, 3, 5.5, 7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 
    13, NA, 12, 1.5, 7, 7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 
    8, 6, 3, 7.5, 4, 7, 7.5, NA, NA, NA, NA, 6.5, 2, 16.5, 7.5, 
    8, 8, 5, 2, 7, 4, 6.5, 4.5, 10, 6, 4.5, 6.5, 9, 2, 6, 3.5, 
    NA, 5, 7, 3.5, 4, 4.5, 13, 19, 8.5, 10, 8, 13, 10, 10, 6, 
    13.5, 12, 11, 5.5, 6, 3.5, 9, 8, NA, 6, 5, 8.5, 3, 12, 10, 
    9.5, 7, 24, 7, 9, 11.5, 5, 7, 11, 6, 5.5, 3, 4.5, 4, 5, 5, 
    3, 4.5, 6, 10, 5, 4, 4, 9.5, 5, 7, 6, 3, 13, 5.5, 5, 7.5, 
    3, 5, 6.5, 5, 5.5, 6, 4, 3, 5, NA, 5, 5, 6, 7, 8, 5, 5.5, 
    9, 6, 8.5, 9.5, 8, 9, 6, 12, 5, 7, 5, 3.5, 4, 7.5, 7, 5, 
    4, 4, NA, 7, 5.5, 6, 8.5, 6.5, 9, 3, 2, 8, 15, 6, 4, 10, 
    7, 13, 14, 9.5, 9, 18, 6, 5, 4, 6, 4, 11.5, 17.5, 7, 8, 10, 
    4, 7, 5, 9, 6, 5, 4, 8, 4, 2, 1.5, 3.5, 6, 5.5, 5, 4, 8, 
    10.5, 4, 11, 9.5, 5, 6, 11, 21, 9.5, 11, 13.5, 7.5, 13, 10, 
    7, 9.5, 6, 10), CustCount = c(2, 6, 3, 5, 3, 2, 2, NA, 2, 6, 
    12, 11, 9, 10, 13, 9, 11, 7, 12, 8, 6, 4, 10, 6, 2, 7, 2, 
    1, 3, 2, 1, 3, 8, 7, 7, 8, 13, 13, 13, 11, 12, 4, 12, 18, 
    12, 7, 5, 4, 6, 4, 3, 3, NA, 4, 2, 8, 8, 8, 7, 3, 5, 3, 7, 
    8, 7, 7, 11, 8, 10, 3, 10, 6, 5, 5, 3, 1, 2, 1, 1, 3, 4, 
    8, 8, 5, 9, 12, 12, 11, 8, 5, 9, 10, 7, 8, 4, 6, 4, 1, 3, 
    1, 3, NA, 2, 1, 4, 10, 7, 13, 6, 9, 6, 16, 12, 11, 10, 12, 
    9, 7, 7, 7, 6, 2, 3, 1, 1, 2, 2, 3, 11, 10, 9, 8, 9, 13, 
    6, 6, 10, 9, 11, 10, 8, 7, 6, 4, 2, 3, 5, 3, 2, 4, 4, 4, 
    8, 5, 12, 8, 7, 12, 9, 12, 12, 12, 13, 12, 9, 8, 9, 10, 4, 
    7, 4, 2, 2, 4, 1, 7, 6, 6, 8, 11, 11, 5, 7, 6, 9, 12, 15, 
    9, 11, 5, 10, 5, 4, 4, 2, 3, 3, 2, 5, 4, 7, 8, 6, 6, 5, 12, 
    10, 8, 10, 10, 4, 13, 12, 6, 8, 6, 3, 1, 4, 2, NA, 4, 3, 
    2, 6, 5, 8, 10, 4, 13, 2, 13, 8, 11, 13, 8, 9, 10, 9, 5, 
    1, NA, 1, 1, 2, NA, 1, 7, 6, 10, 7, 8, 12, 12, 9, 5, 6, 8, 
    13, 13, 13, 8, 8, 1, 5, 7, 6, 2, NA, 2, 1, 2, 7, 9, 12, 12, 
    10, 10, 10, 6, 8, 2, 8, 3, 4, 5, 6, 2, 2, 1, 4, 1, NA, 3, 
    1, 3, 8, 8, 11, 11, 12, 5, 7, 14, 9, 10, 14, 11, 8, 6, 8, 
    7, 5, 4, 3, 4, 9, NA, 2, 4, 5, 8, 2, 12, 8, 15, 12, 8, 9, 
    12, 9, 9, 12, 7, 7, 8, 7, 5, 4, NA, 1, NA, NA, 4, 9, 8, 8, 
    8, 12, 13, 7, 11, 8, 14, 12, 13, 15, 8, 6, 4, 4, 5, 2, NA, 
    2, 5, 4, 5, 6, 15, 11, 10, 16, 10, 5, 5, 10, 13, 10, 9, 8, 
    7, 5, 4, 5, 6, NA, 2, 5, 4, 1, 6, 5, 8, 4, 3, 10, 11, 8, 
    12, 10, 10, 10, 12, 10, 10, 7, 5, 7, 3, 4, 3, 3, 3, 3, 8, 
    4, 8, 10, 5, 10, 10, 10, 11, 10, 11, 7, 10, 7, 6, 7, 7, 3, 
    3, NA, 3, 6, 5, 3, 3, 5, 6, 6, 13, 14, 14, 7, 13, 9, 10, 
    4, 9, 10, 8, 3, 6, 10, 5, 2, 1, NA, 3, 4, 4, 12, 12, 11, 
    12, 11, 13, 10, 9, 11, 11, 14, 10, 13, 10, 7, 11, 1, 3, 1, 
    4, 1, 2, 2, 3, 9, 6, 9, 9, 8, 9, 7, 12, 17, 13, 9, 10, 8, 
    8, 10, 2, 3, 3, 6, 2, 2, 1, 6, 8, 7, 9, 5, 11, 8, 8, 12, 
    13, 14, 10, 7, 5, 11)), .Names = c(""DateTime"", ""WaitTime"", 
""CustCount""), row.names = c(NA, 525L), class = ""data.frame"")</p>
"
"0.140913417690306","0.113753938798323","208515","<p>I'm trying to understand the steps in Rob Hyndman's Multi-step forecasts without re-estimation example below.  I'm wondering what the purpose is of </p>

<pre><code>refit &lt;- Arima(x, model=fit)
</code></pre>

<p>The model has already been determined and trained by auto.arima in the ""fit"" step.  So in the ""refit"" step are we re-training the model on a new data set?  If so, what is the point of retraining the same model on a new data set?</p>

<p>url:
<a href=""http://robjhyndman.com/hyndsight/rolling-forecasts/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/rolling-forecasts/</a></p>

<p>Code:</p>

<pre><code>library(fpp)

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, model=fit)
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}
</code></pre>

<p>Updated Code to re-estimate coefficients:</p>

<pre><code>h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
order &lt;- arimaorder(fit)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, order=order[1:3],seasonal=order[4:6])
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}
</code></pre>
"
"0.0717496003334175","0.104257207028537","214382","<p>If a predictor is negatively correlated with a variable you are trying to forecast in an Arima model, will Arima pick up the negative correlation when you add the predictor in the xreg argument?  Is there anything that needs to be done to the predictor when it is added in the xreg argument in order to indicate that it is negatively correlated with the variable you are trying to predict?</p>
"
"0.12427395320024","0.120385853085769","108925","<p>The following code shows a forecast of the next 24 hours of my electricity prices with two exogenous variables. 
My problem is, that I don't know how to build a forecast for the next 3 days or more because for example I have to take the first 24 hours into account when I want to predict the prices for the second day(25-48). And the time of my dummys and variables also have to grow in 24 hours steps. </p>

<p>I know that a loop is a solution but I  don't know how to create the loop. </p>

<p>I hope you understand my problem.  </p>

<p>My next problem is that I have to create a neural network with this data. Can someone give me a hint how to do this? </p>

<p>Thanks for your help =)</p>

<pre><code>tm1 &lt;- (25:6552)
arma.model =  auto.arima(price$Price[tm1],start.p=5,start.q=5,max.p=5,max.q=5, 
                      xreg=cbind(sol.prod$Production[tm1],wind.prod$Production[tm1],
                           price$Price[1:6528]),
                    trace=TRUE, stationary=TRUE)

arma.model 


PriceForecast = predict(object=arma.model,n.ahead=24, 
                    xreg=cbind(sol.prod$Production[tm1],wind.prod$Production[tm1],
                               price$Price[1:6528]),
                        newxreg=cbind(sol.prod$Production[6553:6576],wind.prod$Production[6553:6576],
                                  price$Price[6529:6552]))
</code></pre>
"
"0.23301366225045","0.240771706171538","58657","<p>I'm using a daily time series of sales data that contains about 2 years of daily data points. Based on some of the online-tutorials / examples I tried to identify the seasonality in the data. It seems that there is a weekly, monthly and probably a yearly periodicity / seasonality.</p>

<p>For example, there are paydays, particularly on 1st payday of the month effect that lasts for few days during the week. There are also some specific Holiday effects, clearly identifiable by noting the observations.</p>

<p>Equipped with some of these observations, I tried the following:</p>

<ol>
<li><p>ARIMA (with <code>Arima</code> and <code>auto.arima</code> from R-forecast package), using regressor (and other default values needed in the function).  The regressor I created is basically a matrix of 0/1 values:</p>

<ul>
<li>11 month (n-1) variables</li>
<li>12 holiday variables</li>
<li>Could not figure out the payday part...since it's little more complicated effect than I thought. The payday effect works differently, depending on the weekday of the 1st of month.</li>
</ul>

<p>I used 7 (i.e., weekly frequency) to model the time series. I tried the test - forecasting 7 days at a time. The results are reasonable: average accuracy for a forecast of 11 weeks comes to weekly avg RMSE to 5%.</p></li>
<li><p>TBATS model (from R-forecast package) - using multiple seasonality (7, 30.4375, 365.25) and obviously no regressor. The accuracy is surprisingly better than the ARIMA model at weekly avg RMSE 3.5% .</p>

<p>In this case, the model without ARMA errors perform slightly better. Now If I apply the coefficients for just the Holiday Effects from the ARIMA model described in #1, to the results of the TBATS model the weekly avg RMSE improves to 2.95%</p></li>
</ol>

<p>Now without having much background or knowledge on the underlying theories of these models, I'm in a dilemma whether this TBATS approach is even a valid one. Even though it's improving the RMSE significantly in the 11 weeks test, I'm wondering whether it can sustain this accuracy in the future. Or even if applying Holiday effects from ARIMA to the TBATS result is justifiable. Any thoughts from any / all the contributors will be highly appreciated. </p>

<p><a href=""https://s3.amazonaws.com/CKI-FILE-SHARE/TS+Test+Data.txt"">Link for Test Data</a></p>

<p>Note: Do ""Save Link As"", to download the file.</p>
"
"0.18641092980036","0.180578779628654","63681","<p>I have been adamantly searching the web to learn how to successfully implement a dynamic regression time series in the forecast package for R. The time series data that I am using is weekly data (frequency=52) of incoming call volume and prediction variables are mailers sent out every now and then. They are a significant predictor of the data for the week that they hit, the following week, and the week after that. I have created lagged variables and use these three as the predictors. </p>

<p>My main concern is that the arima model is not taking into account the time series frequency. When I tell it to recognize the ts with a frequency of 52 it has an error. 
I have looked at the <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">fortrain function</a> but do not understand it. I also have looked at the tbats suggested but found that those will not work with prediction variables. </p>

<p>The Zoo function recognizes 52 frequency but it is not advised to use with the <a href=""http://stackoverflow.com/questions/16050684/using-the-combination-forecastauto-arima"">forecast package</a>.</p>

<p>Here is the basic code. The problem is that the time series calwater[,5] is not recognized as such. It is imputed as a simple vector as an integer...</p>

<pre><code>#this works without taking into acount the ts
fit2 &lt;- auto.arima(calwater[6:96,5], xreg=calwater[6:96,6:8], d=0)
fccal &lt;- forecast(fit2, xreg=calwater[97:106,6:8], h=10)
fccal
plot(fccal, main=""Forecast Cal Water"", ylab=""Calls"")

#to form a ts object
calincall&lt;-ts(calwater[1:106,5],start=c(2011,23),frequency=52)

#once the ts is added to the model this dispalys
#Error in `[.default`(calincall, 2:100, 1) : incorrect number of dimensions
</code></pre>

<p>Maybe the error is because there is just a little over two years of data. </p>

<pre><code>#Time Series: Start = c(2011, 23), End = c(2013, 24),Frequency = 52 
</code></pre>

<p>I would be very grateful for any guidance in for this particular issue. I am using the forecast package and prefer to continue within the package but I am open to suggestions. </p>
"
"0.107624400500126","0.104257207028537","148820","<p>I am working on an alogorithm in R to automatize a monthly forecast calculation. I am using, among others, the forecast(method='arima') function from the forecast package to calculate forecast. It is working very well. But for some times series some forecast are quite strange.</p>

<p>Please find below the code i'm using:</p>

<p><code>train_ts&lt;- ts(values, frequency=12)
fit1 &lt;- stl(train_ts, s.window=""periodic"",t.window=24, )
arima &lt;- forecast(fit1,h=forecasthorizon,method ='arima')</code></p>

<p><code>values &lt;- c(27, 27, 7, 24, 39, 40, 24, 45, 36, 37, 31, 47, 16, 24, 6, 21, 35, 36, 21, 40, 32, 33, 27, 42, 14, 21, 5,   19, 31, 32, 19, 36, 29, 29, 24, 42, 15, 24, 21)</code></p>

<p>Here, on the graph, you will see the historical data (black), the fitted value (green) and the forecast(blue). The forecast is not in lines with the fitted value.</p>

<p><img src=""http://i.stack.imgur.com/5530d.png"" alt=""enter image description here"">
As you can see the Forecast is not in line with the history,
My question is ""does a setup for Arima to bound the forecast in line with the history exist"" ?</p>
"
"NaN","NaN","125909","<p>I am trying to predict values using arima(0,1,1).
After doing <code>predict(mod,n.ahead=5)</code> (in <code>R</code>) am getting the same value for all the predictions: </p>

<pre><code>5947.681 5947.681 5947.681 5947.681 5947.681 
</code></pre>

<p>Is it correct?</p>
"
"0.107624400500126","0.104257207028537","86248","<p>I've been attempting to forecast natural gas power demand and how it is affected by temperature and price. I'm not sure if I have done everything correctly (relatively new to R), but I do seem to get relevant data other than I can't seem to change my forecast period, nor am I sure this is an appropriate model for this data. Hopefully someone can provide me with some guidance.</p>

<p>Data: <a href=""https://www.dropbox.com/s/g9uytz3guyjrbq2/demand.csv"" rel=""nofollow"">demand.csv</a></p>

<pre><code>library(forecast)
data = read.csv(""demand.csv"")

# Create matrix of numeric predictors
xreg &lt;- cbind(weather=data$Weather,price=data$Price,m1=data$M1,
    m2=data$M2,m3=data$M3,m4=data$M4,m5=data$M5,m6=data$M6,
m7=data$M7,m8=data$M8,m9=data$M9,m10=data$M10,m11=data$M11)

# Rename columns
colnames(xreg) &lt;- c(""Weather"",""Price"",""Jan"",""Feb"",""Mar"",""Apr"",
""May"",""Jun"",""Jul"",""Aug"",""Sep"",""Oct"",""Nov"")

# Variable to be modelled
demandTS &lt;- ts(data$Demand, frequency=12)

# Find ARIMAX model
demandArima &lt;- auto.arima(demandTS, xreg=xreg)
demand.fcast &lt;- forecast(demandArima, xreg=xreg)
plot(demand.fcast)
</code></pre>

<p>Thank you for any help.</p>

<p>References:</p>

<p><a href=""http://stats.stackexchange.com/questions/41070/how-to-setup-xreg-argument-in-auto-arima-in-r"">How to setup xreg argument in auto ARIMA in R</a>
<a href=""http://stackoverflow.com/questions/10606295/from-auto-arima-to-forecast-in-r"">From auto ARIMA to forecast in R</a></p>
"
"0.187350033626992","0.199637352376173","187870","<p>I'm working on a sales forecasting package which should be easy to use for the end user. Given a time series with historical sales data I would like to automatically select one of the three forecasts: Auto.Arima, ETS and STLF. 
The idea is to split historical data into 80% train set and 20% test (holdout) set. Then run Auto.Arima, ETS, STLF and choose the one that has best MAPE on the test set. </p>

<p>Now comes the part that is not entirely clear to me. Once I figured out that e.g. ETS gives me the best result should I now </p>

<ol>
<li>Retrain ETS on the entire set of historical data and generate
forecast using this new model? My reservation here is that after I
run ETS again it may even change the class of the algorithm as well
as the fit parameters which will render the MAPE I got on the test
set irrelevant.  </li>
<li>Just generate the forecast using the model that was trained on the
80% train set? My problem with this approach is that we are ignoring
the last 20% of data which is probably the most important
information for the forecast.</li>
<li>The third idea is to use the same model fit parameters that we got
after training the model on the 80% train set. But then use the
entire set of data for        forecasting. This seems like a
reasonable approach but I cannot figure out how to do it for ETS and
STL (For Arima we can do it by supplying the original fit as the model
parameter of the arima function)</li>
</ol>

<p>Could you please let me know what is the right way to approach this problem?</p>
"
"0.107624400500126","0.104257207028537","173610","<p>I create an ARIMA model for my <code>ts</code>-object. My data is available in seconds or even miliseconds. I didn't find a way to specify the time information for the start- and end-parameters when creating the <code>ts</code> object?</p>

<p>I need the exact time, because I want to extract the time information when I do the forecast based on the ARIMA model to return the exact times for my forecasted values. It would be enough to store the end-time information somewhere in my ARIMA model, so that I can use it later when I do the forecast.</p>

<p>How is this done usually with ARIMA models?</p>

<p>Thanks!</p>
"
"0.144986278733613","0.140450161933397","173042","<p>I am new to R and forecasting. I have access to weekly data (104 weeks) for a certain SKU, its value and volume sales and a few promo variables.</p>

<p>Promo 1 and Promo 2 are continuous variables (unfortunately Promo 1 is 0 here for this SKU) while Promo 3 and Promo 4 are categorical variables.</p>

<p>I tried forecasting the volume sales for this SKU for the next 72 weeks. I included dummy variables using <code>seasonaldummy</code> function</p>

<pre><code>actual_vol = ts(data$Volume , frequency =52)
    dummy = seasonaldummy(actual_vol)
    xreg = cbind(data$Promo1 , data$Promo2 , data$Promo3 , data$Promo4 , dummy)

fit = auto.arima(actual_vol , xreg = xreg)
</code></pre>

<p>I am trying to forecast sales for the next 72 weeks by keeping my promo variables as 0 (basically baseline sales). I used <code>seasonaldummyf</code> and promo variables as 0 for forecast.</p>

<p>The plot looks something like this
<a href=""http://i.stack.imgur.com/tdsuO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tdsuO.png"" alt=""Forecast - Arima""></a></p>

<p>As you can see the forecast looks exactly the same as the previous data (same as using <code>snaive</code>) and it seems promo had no effect at all on volume sales.</p>

<p>Kindly let me know if the method is correct and if not how can I improve it.</p>

<p><a href=""https://drive.google.com/file/d/0B6sOv1da0JMeVHl1SlRMZmJDODQ/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B6sOv1da0JMeVHl1SlRMZmJDODQ/view?usp=sharing</a></p>
"
"0.175749910065499","0.17025130615175","188597","<p>I have daily data for 3 years. This sales data is of seasonal nature as business has spikes and downfall by month. Also, sales differ by each day of the week. for example, monday in general in a month tend to have similar pattern.</p>

<p>I have used ARIMA and created a matrix of month dummy variables and day of week dummy variables and have passed that in ARIMA. however i hit the bottom when i couldn't reconvert differenced stationary number forecasts into the actual sales metric. <a href=""http://stats.stackexchange.com/questions/188595/convert-double-differenced-forecast-into-actual-value"">Posted here already</a></p>

<p>I have also tried dummy regression using sales as dependent variable and 11 month dummy variables and 6 day of week dummy variables. i abandoned this as R square was low at 48% and MAPE from the forecasted results was more than 20%</p>

<p>Edit: I have tried auto.arima as well.
My question: What technique can i use for forecasting sales for next 365 days? that will consider this month of the year and day of the week seasonality?</p>
"
"0.12427395320024","0.120385853085769","43804","<p>I tried to fit <code>auto.arima()</code> with a <code>ts</code> data. But it is not giving the right forecast. For many it is coming as <code>arima(0,1,0)</code> model which is not good at all. Can I fit a GARCH model to the original series in this case? How do you get fitted and forecasted values of original data using <code>garch(1,1,)</code> or some other model? I tried to use code for GARCH but it is not giving the fitted and forecast of original values.</p>
"
"0.12683657235796","0.147441956154897","91675","<p>I have been looking for a function that can make recursive window out-of-sample forecasts, but seems there is none. So I'm thinking about about making a function that can be used for recursive window forecasting in an ARIMA model. However I know little about programming, so I'm seeking for help.</p>

<p>What I want to do is use the function <code>forecast.Arima</code> (<strong>forecast</strong> package) to predict future values in a expanding window. Suppose 20 years is the initial window, and I expand the window by 1 year on each iteration until it is of size  30 years. More specifically, use 20 years data to predict one value, use 21 years data to predict the next value, etc.</p>
"
"NaN","NaN","173505","<p>I have a basic question with the <code>auto.arima</code> function in the ""forecast"" package in R. I create a model using the following simple commands:</p>

<pre><code>data_ts&lt;-ts(data$Value, frequency=24)
fc&lt;-auto.arima(data_ts)
plot(forecast(fc, level=c(80), h=30*24)) 
</code></pre>

<p>The result looks always like this</p>

<p><a href=""http://i.stack.imgur.com/o5HzI.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/o5HzI.jpg"" alt=""enter image description here""></a></p>

<p>After some periods it converges to the mean value. Is this a ""normal"" behaviour with ARIMA models or is it a sign that some information or parameter is missing? </p>

<p>BTW: On weekends I have different patterns. Is this considered automatically by <code>auto.arima</code> or do I have to create a dummy variable and use <code>xreg</code>?</p>

<p>Thanks!</p>
"
"0.107624400500126","0.104257207028537","92935","<p>I'm making a project connected with identifying the dynamics of sales. My database concerns 26 weeks (so equally in 26 time-series observations) after launching the product.</p>

<p>This is what my database looks like: <a href=""https://imageshack.com/i/0yyh6ij"" rel=""nofollow"">https://imageshack.com/i/0yyh6ij</a> </p>

<p>I want to make forecast based on S-curve for clusters of time-series. The main aim was to compare two methods of forecasting:</p>

<ol>
<li>based on parameters of logistic curve</li>
<li>based on ARIMA</li>
</ol>

<p>However, I do not know how to compare these two methods = measure their performance.</p>

<p>That's a plot with prediction based on S-curve</p>

<p><a href=""http://imageshack.com/a/img850/6600/rzkp.jpg"" rel=""nofollow"">http://imageshack.com/a/img850/6600/rzkp.jpg</a></p>

<p>So my questions are:</p>

<ol>
<li>How to measure performance=forecast errors based on logistic curve?</li>
<li>How to compare forecasting based on logistic curve and ARIMA - what is the main difference between these two approaches if I base on one variable - units_sold_that_week?</li>
</ol>

<p>I would be grateful for any explanation.</p>
"
"0.206085036989691","0.199637352376173","189983","<p>I have daily data from last 2 years.</p>

<p>I want to do ARIMAX and the regressor component being autoregressive distributed lag of the same variable. Since it has impact, along with dummy variables to account for seasonality in the <code>xreg</code> paratemer in <code>auto.arima</code> function.</p>

<p>The challenge i am facing is predicting my predictor for future. For example, i used daily data for 2 year for model building. For forecasting into future, i also need values of lag variable, which i do not know. If i use 2 lags of daily data in the model, then in order to predict for future i will also need value of those lag variables as well. So to predict $Value$ at time $t$ i will need $Value$ at $t-1$ and $t-2$ which i have from past records. However, if i want to find value at $t+5$ then i will need to find $t+3$ and $t+4$. Not sure how to proceed in this direction. As stated earlier, i am using <code>auto.arima</code> function from <code>forecast</code> package in <code>R</code> . </p>

<p>My ultimate goal is to predict for next 365 days. What i assume to be a solution is that i predict for $t+1$ as it will require $t$ and $t-1$ as lag component which i already have. once done i can use this predicted $t+1$ component to predict for $t+2$ as i will know value of $t+1$ from previous iteration and $t$ from original values. Is it the right approach?</p>
"
"0.206085036989691","0.199637352376173","191120","<p>I am a beginner in time series analysis and I would like discuss a couple of numerical examples here implemented in R. I am reading some interesting books, but I also need some expert advice to get started.
The time series are</p>

<pre><code>ts1&lt;-structure(c(196, 196, 178, 165, 155, 138, 131, 132, 135, 146, 
160, 173, 180, 186, 180, 163, 132, 129, 134, 146, 159, 157, 161, 
179, 209, 225, 228, 196, 151, 144, 145, 157, 168, 161, 162, 176, 
205, 219, 219, 190, 147, 142, 146, 160, 175, 169, 171, 188, 220, 
235, 236, 202, 154, 146, 145, 155, 168, 158, 156, 168, 190, 202, 
204, 177, 135, 127, 125, 133, 145, 139, 143, 160, 190, 205, 200, 
160, 119, 113, 118, 129, 142, 135, 133, 142, 159, 171, 177, 164, 
135, 130, 130, 139, 152, 149, 152, 168, 195, 209, 211, 180, 138, 
134, 139, 152, 165, 158, 157, 168, 192, 207, 219, 206, 169, 164, 
161, 172, 182, 180, 182, 196, 218, 223, 229, 230, 196, 197, 200, 
209, 222, 219, 207, 210, 209, 221, 234, 224, 225, 221, 235, 216, 
224, 229, 229, 214, 230, 240, 243, 222, 189, 221, 217, 189, 197, 
194, 195, 202, 197, 224, 204, 218, 212, 191, 217, 215, 183, 186, 
191, 166, 177, 194, 180, 159, 158, 147, 166, 184, 159, 159, 187, 
194, 196, 204, 213, 236, 210, 218, 251, 227, 251, 214, 245, 209, 
215, 242, 196, 237, 212, 171, 206, 200, 204, 192, 185, 182, 194, 
242, 199, 200, 191, 172, 179, 165, 173, 198, 214, 197, 175, 227, 
197, 202, 205, 212, 216, 223, 222, 201, 217, 209, 239, 241, 251, 
225, 212, 210, 241, 223, 238, 226, 242, 228, 257, 248, 264, 229, 
223, 255, 251, 231, 254, 235, 246, 246, 243, 254, 256, 261, 254, 
247, 249, 243, 257, 228, 272), na.action = structure(c(1L, 2L, 
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 
30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 
43L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 55L, 
56L, 57L, 58L, 59L, 60L, 61L, 62L, 63L, 64L, 65L, 66L, 67L, 68L, 
69L, 70L, 71L, 72L, 73L, 74L, 75L, 76L, 77L, 78L, 79L, 80L, 81L, 
82L, 83L, 84L, 85L, 86L, 87L, 88L, 89L, 90L, 91L, 92L, 93L, 94L, 
95L, 96L, 97L, 98L, 99L, 100L, 101L, 102L, 103L, 104L, 105L, 
106L, 107L, 108L, 109L, 110L, 111L, 112L, 113L, 114L, 115L, 116L, 
117L, 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 
128L, 129L, 130L, 131L, 132L, 396L), class = ""omit""), .Tsp = c(1994, 
2015.83333333333, 12), class = ""ts"")

ts2&lt;-structure(c(3756, 3867, 3686, 3490, 3446, 3357, 3421, 3447,3321, 
3198, 3331, 3360, 3312, 3270, 3251, 3213, 2937, 3152, 3022, 2931, 
2697, 2626, 2775, 3030, 3067, 3349, 3225, 3175, 3061, 3089, 3166, 
3193, 3035, 2901, 2932, 2981, 3242, 3268, 3084, 2902, 2790, 2695, 
2756, 2649, 2627, 2643, 2554, 2638, 2783, 2660, 2618, 2383, 2319, 
2415, 2434, 2427, 2164, 2114, 2246, 2224, 2552, 2390, 2213, 2130, 
2274, 2140, 2317, 2191, 2086, 2112, 2134, 2153, 2401, 2450, 2273, 
2154, 2140, 2201, 2156, 2078, 2110, 2101, 2075, 2043, 2305, 2266, 
2227, 2134, 2002, 2008, 1945, 2110, 2045, 2017, 2106, 1913, 2068, 
2209, 2025, 2033, 1892, 1934, 1914, 1818, 1808, 
1851, 1939),na.action   = structure(c(1L, 
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 
29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 
42L, 43L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 
55L, 56L, 57L, 58L, 59L, 60L, 61L, 62L, 63L, 64L, 65L, 66L, 67L, 
68L, 69L, 70L, 71L, 72L, 73L, 74L, 75L, 76L, 77L, 78L, 79L, 80L, 
81L, 82L, 83L, 84L, 85L, 86L, 87L, 88L, 89L, 90L, 91L, 92L, 93L, 
94L, 95L, 96L, 97L, 98L, 99L, 100L, 101L, 102L, 103L, 104L, 105L, 
106L, 107L, 108L, 109L, 110L, 111L, 112L, 113L, 114L, 115L, 116L, 
117L, 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 
128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L, 
139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L, 
150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L, 
161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 
172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L, 
183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L, 
194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L, 
205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L, 
216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L, 
227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L, 
238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L, 
249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L, 
260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L, 
271L, 272L, 273L, 274L, 275L, 276L, 277L, 278L, 279L, 280L, 281L, 
282L, 283L, 284L, 285L, 286L, 287L, 288L, 396L),
class = ""omit""),.Tsp   = c(2007, 
2015.83333333333, 12), class = ""ts"")
</code></pre>

<p>I would prefer to avoid the use of auto.arima from the (excellent) forecast package, or at least not to use it as a black box.
I started looking at the plots of the first differences</p>

<pre><code>plot(diff(ts1))
</code></pre>

<p><a href=""http://i.stack.imgur.com/5EVBz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5EVBz.png"" alt=""enter image description here""></a></p>

<pre><code>plot(diff(ts2))
</code></pre>

<p><a href=""http://i.stack.imgur.com/ZWNea.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZWNea.png"" alt=""enter image description here""></a></p>

<p>which should remove any trend. I also looked at the decomposition: </p>

<pre><code>plot(decompose(ts1))
</code></pre>

<p><a href=""http://i.stack.imgur.com/VpDyN.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VpDyN.png"" alt=""enter image description here""></a></p>

<pre><code>plot(decompose(ts2))
</code></pre>

<p><a href=""http://i.stack.imgur.com/M3lkU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/M3lkU.png"" alt=""enter image description here""></a></p>

<p>I would tend to conclude that in both cases there is a seasonality in the data. 
However, diff(ts2) appears (to me, by eye) to yield a stationary process with constant variance, whereas diff(ts1) does not seem to have a constant variance. I tried diff(diff(ts1)) and diff(log(ts2)), but I am puzzled by what I see.
If I look at  </p>

<pre><code> acf(ts1)
</code></pre>

<p><a href=""http://i.stack.imgur.com/qPbtI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qPbtI.png"" alt=""enter image description here""></a></p>

<pre><code> acf(ts2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/TlNto.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TlNto.png"" alt=""enter image description here""></a></p>

<p>I see that in both cases the autocorrelation decays slowly and when I resort to</p>

<pre><code>acf(diff(ts1))
</code></pre>

<p><a href=""http://i.stack.imgur.com/Q62OT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Q62OT.png"" alt=""enter image description here""></a></p>

<pre><code>acf(diff(ts2))
</code></pre>

<p><a href=""http://i.stack.imgur.com/nxycs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nxycs.png"" alt=""enter image description here""></a></p>

<p>I see some spikes which I do not know how to interpret.
Essentially, I am at a loss about how to link these findings with a SARIMA model.
Any suggestion on either/both time series is very appreciated!</p>
"
"0.107624400500126","0.104257207028537","214379","<p>I'm working on an Arima model to forecast a given variable and so I'm looking in my data for variables with correlation to the variable I'm trying to predict, to add as predictors in the xreg argument.  I've found several that have correlation between 0.1 and 0.3.  I was wondering is there a way to combine predictors with lower correlation to a variable to create a predictor with higher correlation to a variable?</p>
"
"NaN","NaN","191559","<p>I have the following <a href=""https://drive.google.com/file/d/0B5qdN8N1gwhIWWVnWG9wZUFlMUU/view?usp=sharing"" rel=""nofollow"">dataset</a>.</p>

<p>I tried <code>arima</code> with <code>xreg</code></p>

<pre><code>Y = transactions$UNITS
regressors = transactions[,c(""FEATURE"",""DISPLAY"",""TPR_ONLY"",""REL_DISCOUNT"")]
model = arima(x = Y,xreg = regressors,order = c(2,0,0))
</code></pre>

<p>I tried different methods: ""CSS"" , ""CSS-ML"" , ""ML""
and get the same error :</p>

<pre><code>Error in optim(init[mask], armaCSS, method = optim.method, hessian = FALSE,  : 
  non-finite value supplied by optim
In addition: Warning message:
In trainingaccuracy(f, test, d, D) : test elements must be within sample
</code></pre>

<p>Any clue why it happens and how to fix it?</p>
"
"0.172336965564645","0.15025062674119","208985","<p>I'm wondering if a rolling forecast technique like the ones mentioned in Rob Hyndman's blogs, and the example below, could be used to select the order for an ARIMA model?</p>

<p>In the examples I've looked at, like the ones below, it seems like the order of the ARIMA model is already specified, or is determined once by auto.arima and then the single model is evaluated using the forloop in the rolling forecast. </p>

<p>I'm wondering how you could use the rolling forecast technique to select the order of the ARIMA model.  If anyone has a suggestion or example, that would be great.</p>

<p>Examples:
<a href=""http://robjhyndman.com/hyndsight/tscvexample/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/tscvexample/</a>
<a href=""http://robjhyndman.com/hyndsight/rolling-forecasts/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/rolling-forecasts/</a></p>

<p>Code:</p>

<pre><code>library(""fpp"")

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, model=fit)
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}
</code></pre>

<p><strong>Update:</strong></p>

<p>Pseudo code:</p>

<pre><code>library(""fpp"")

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1

##Create models for all combinations of p 10 to 0, d 2 to 0, q 10 to 0

fit1 &lt;- Arima(train, order=c(10,2,10)
fit2 &lt;- Arima(train, order=c(9,2,10)
fit3 &lt;- Arima(train, order=c(8,2,10)
.
.
.
fit10 &lt;- Arima(train, order=c(0,2,10)
fc1 &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
fc2 &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
fc3 &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
.
.
.
fc10 &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit1 &lt;- Arima(x, model=fit1)
  refit2 &lt;- Arima(x, model=fit2)
  refit3 &lt;- Arima(x, model=fit3)
  .
  .
  .
  refit10 &lt;- Arima(x, model=fit10)
  fc1[i] &lt;- forecast(refit1, h=h)$mean[h]
	  fc2[i] &lt;- forecast(refit2, h=h)$mean[h]
  fc3[i] &lt;- forecast(refit3, h=h)$mean[h]
	  .
	  .
	  .
	  fc10[i] &lt;- forecast(refit10, h=h)$mean[h]
}

##Calculating mape for forecasts

Accuracy(fc1$mean,test)[,5]
	Accuracy(fc2$mean,test)[,5]
Accuracy(fc3$mean,test)[,5]
	.
	.
	.
	Accuracy(fc10$mean,test)[,5]

##Return the order of the Arima model that has the lowest mape 
</code></pre>
"
"0.176844935675668","0.171312072216048","72244","<p>I must be doing something very wrong here, as auto.arima in R is completely dying, but I can't see what it is.  I have the latest version of forecast and R and I think this happens on both Windows and Unix.  It works for some/most time series I tried (equities) but fails for others.  It seems to fail more often when I diff the high/low with the previous day's close as opposed to just diffing the closes as below.  Is this a bug or am I somehow giving arima bad data? (and causing it to die with a horrible error message)  I tried searching for this error but didn't come up with much.  Thanks a lot.</p>

<pre><code>library(tseries)
library(forecast)
dwa &lt;- get.hist.quote(instrument=""DWA"", start=""2010-01-01"", end=""2013-10-31"")
logreturns &lt;- diff(log(dwa$Close))*100 
fit &lt;- auto.arima(logreturns, trace=TRUE)
</code></pre>

<p>Output:</p>

<pre><code>This is forecast 4.8 

trying URL 'http://chart.yahoo.com/table.csv?s=DWA&amp;a=0&amp;b=01&amp;c=2010&amp;d=9&amp;e=31&amp;f=2013&amp;g=d&amp;q=q&amp;y=0&amp;z=DWA&amp;x=.csv'
Content type 'text/csv' length unknown
opened URL
.......... .......... .......... .......... ....
downloaded 44 Kb

time series starts 2010-01-04
time series ends   2013-10-07

 ARIMA(2,1,2) with drift         : 1e+20 *
 ARIMA(0,1,0) with drift         : 1e+20 *
 ARIMA(1,1,0) with drift         : 1e+20 *
 ARIMA(0,1,1) with drift         : 1e+20 *
 ARIMA(1,1,2) with drift         : 1e+20 *
 ARIMA(3,1,2) with drift         : 1e+20 *
 ARIMA(2,1,1) with drift         : 1e+20 *
 ARIMA(2,1,3) with drift         : 1e+20 *
 ARIMA(1,1,1) with drift         : 1e+20 *
 ARIMA(3,1,3) with drift         : 1e+20 *
 ARIMA(2,1,2)          : 1e+20 *Error in if (diffs == 1 &amp; constant) { : argument is of length zero
Calls: auto.arima -&gt; myarima
In addition: Warning messages:
1: In if (is.constant(x)) { :
  the condition has length &gt; 1 and only the first element will be used
2: In if (is.constant(x)) return(d) :
  the condition has length &gt; 1 and only the first element will be used
3: In if (is.constant(dx)) { :
  the condition has length &gt; 1 and only the first element will be used
Execution halted
</code></pre>
"
"0.0878749550327494","0.0851256530758749","154770","<p>I'm trying to forecast a time series of a stock option using ARMA-GARCH modelling in R. First I determine the ARMA order using AIC and I found (0,1) to be the best one.</p>

<p>But when I run </p>

<pre><code>garchFit(formula=~arma(0,1)+garch(1,1),data=XX,trace=FALSE,include.mean=TRUE)
</code></pre>

<p>I get constant mean forecasts.</p>

<p>Whereas when I use ARMA(1,1):</p>

<pre><code>garchFit(formula=~arma(1,1)+garch(1,1),data=brentlog1,trace=FALSE,include.mean=TRUE)
</code></pre>

<p>I get variables mean forecasts. Does anyone know why is that?</p>

<p>How do I forecast the actual prices of the stock (not its variance)?</p>
"
"0.0439374775163747","0.0425628265379374","114185","<p>I have monthly usage data (spanning 3 years) for a customer base of around 200K, and I need to generate 1-month ahead forecasts for each of them. There are a couple of exogenous variables that would need to be included too.</p>

<p>One way to go about is to build an ""Arima with exogenous variables"" model for each customer. In which case, I would be looking to manage 200K models. I'm wondering if there is an alternate approach to solving this -- Assuming we are able to segment the customer base into groups that are ""homogenous"" in terms of usage, then would it make sense to create one model for each group? Would it be a good approximation to have the usage data time series for each group correspond to the mean values across the customers in that group? </p>

<p>Any pointers to literature related to this problem is appreciated. Additionally, I'm looking to use R, so pointers to specific packages will be of help as well.</p>

<p>Thanks!</p>
"
"0.176844935675668","0.171312072216048","115506","<p>Forecasting airline passengers seasonal time series using auto arima</p>

<p>Hi, I am trying to model some airline data in an attempt to provide an accurate monthly forecast for June-December this year using monthly data from January 2003 onwards.  The data is taken from: <a href=""http://www.transtats.bts.gov/Data_Elements.aspx?Data=1"" rel=""nofollow"">http://www.transtats.bts.gov/Data_Elements.aspx?Data=1</a></p>

<p>Here is the time series plot and ACF</p>

<p><a href=""http://imgur.com/EGh40pR"" rel=""nofollow""><img src=""http://i.imgur.com/EGh40pR.jpg"" title=""Hosted by imgur.com""/></a> </p>

<p><a href=""http://imgur.com/BJy78dn"" rel=""nofollow""><img src=""http://i.imgur.com/BJy78dn.jpg"" title=""Hosted by imgur.com""/></a></p>

<p>I have used auto.arima to develop two models and checked that they correspond to the autocorrelation functions.  Basically I am having trouble deciding whether to use:</p>

<ol>
<li>The following seasonal ARIMA model</li>
</ol>

<p><a href=""http://imgur.com/0k2Q8I4"" rel=""nofollow""><img src=""http://i.imgur.com/0k2Q8I4.jpg"" title=""Hosted by imgur.com""/></a></p>

<ol start=""2"">
<li><p>The following non-seasonal ARIMA model of $N_t$ after I first decomposed the model into a trend, seasonal component and random component $X_t = T_t +S_t +N_t $ using a 12-point moving average (basically did the same thing as the <code>decompose()</code> function manually)</p>

<p><a href=""http://imgur.com/r4TkpxX"" rel=""nofollow""><img src=""http://i.imgur.com/r4TkpxX.jpg"" title=""Hosted by imgur.com""/></a></p></li>
</ol>

<p>I have analysed the important properties of both models such as ensuring residuals are close to a white noise process and so on but am unsure which of the above 2 models is most suitable for forecasting purposes and why?</p>

<p>Also I am unsure how to compute forecast for the trend component vector if I use the classical decomposition model $X_t = T_t + S_t +N_t$.  Is it even possible to create forecasts using this type of model?</p>

<p>Edit:
Here is the output of <code>dput(IAP)</code> (the raw data without trend or seasonal component removed)</p>

<blockquote>
  <p>dput(IAP)
  structure(c(9726436L, 8283372L, 9538653L, 8309305L, 8801873L, 
  10347900L, 11705206L, 11799672L, 9454647L, 9608358L, 9481886L, 
  10512547L, 10252443L, 9310317L, 10976440L, 10802022L, 10971254L, 
  12159514L, 13502913L, 13203566L, 10570682L, 10772177L, 10174320L, 
  11244427L, 11387275L, 9945067L, 12479643L, 11521174L, 12164600L, 
  13140061L, 14421209L, 13703334L, 11325800L, 11107586L, 10580099L, 
  11812574L, 11724098L, 10167275L, 12707241L, 12619137L, 12610793L, 
  13690835L, 14912621L, 14171796L, 12010922L, 11517228L, 11222687L, 
  12385958L, 12072442L, 10590281L, 13246293L, 12795517L, 12978086L, 
  14170877L, 15470687L, 15120200L, 12321953L, 12381689L, 12004268L, 
  13098697L, 12767516L, 11648482L, 14194753L, 12961165L, 13602014L, 
  14413771L, 15449821L, 15327739L, 11731364L, 11921490L, 11256163L, 
  12463351L, 12075267L, 10412676L, 12508793L, 12629805L, 11806548L, 
  13199636L, 14953615L, 14844821L, 11659775L, 11905529L, 11093714L, 
  12659154L, 12393439L, 10694165L, 13279320L, 12398700L, 13380664L, 
  14406776L, 16026852L, 15317926L, 12599149L, 12874707L, 11651314L, 
  12915663L, 12668763L, 10944610L, 13473705L, 13537152L, 13935132L, 
  14814672L, 16623674L, 15753387L, 13220884L, 13185627L, 12144742L, 
  13546071L, 13206682L, 11732944L, 14387677L, 13995377L, 14291285L, 
  15582335L, 16969590L, 16621336L, 13791714L, 13397785L, 12762536L, 
  14096567L, 13766673L, 12023339L, 15177069L, 14278932L, 15306328L, 
  16232176L, 17645538L, 17517022L, 14239561L, 14209627L, 13133257L, 
  15083929L, 14589637L, 12385546L, 15486317L, 14857685L, 15615732L
  ), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>

<p>Here is the output of <code>dput(IAP.res)</code> (the random component from the decomposition)</p>

<blockquote>
  <p>dput(IAP.res)
  structure(c(NA, NA, NA, NA, NA, NA, -669127.347569446, -168943.285069446, 
  225871.456597222, 271337.106597223, 711896.11076389, 284583.435763889, 
  165401.360763887, 622993.194097221, -268299.21423611, -9406.73506944434, 
  -233904.910069446, -147124.755902779, -260973.055902776, -163628.243402778, 
  -43056.7100694457, 121365.814930555, 205106.485763889, -107464.272569445, 
  247575.569097221, 279399.444097225, 309270.160763888, -166333.068402778, 
  129823.798263889, 22571.1190972265, -113455.59756944, -384199.160069444, 
  62061.8315972222, -155858.226736111, 13600.0274305546, -87564.1475694429, 
  71845.7357638887, 8145.86076388881, 47627.494097226, 442212.72326389, 
  73639.5065972234, 60882.5774305568, -135204.389236112, -437744.576736112, 
  203832.581597222, -264145.435069444, 179945.61076389, 15812.1024305553, 
  -49648.0975694434, -61460.8059027772, 89656.3690972241, 118205.931597224, 
  -84196.4517361106, 4197.78576389072, -134118.722569442, -87234.4517361117, 
  -126555.418402776, -57714.9350694417, 293250.152430556, 59462.6857638892, 
  10340.8190972245, 416646.652430557, 526459.702430556, -135041.068402776, 
  239767.631597222, 67034.9940972247, -221066.180902774, 207611.839930556, 
  -424486.00173611, -94779.3517361115, 89796.4857638886, 130285.644097223, 
  104776.152430555, 16099.8607638888, -317097.047569448, 335867.264930556, 
  -796342.285069446, -446777.464236111, -93681.7225694442, 242962.798263888, 
  -143380.293402778, 135423.439930556, 28934.7357638923, 186390.185763891, 
  116969.777430558, -113617.264236109, -39733.9225694438, -471572.526736109, 
  130389.423263891, 80446.7857638926, 298895.444097222, 38486.7982638846, 
  143712.123263886, 419260.898263889, -113385.347569445, -181233.730902779, 
  -178686.680902779, -412733.597569445, -380106.797569444, 172783.973263888, 
  220863.173263891, 11443.2440972247, 392297.319097224, -62825.8267361117, 
  176278.664930556, 139372.439930556, -174159.88923611, -111755.439236109, 
  -206233.264236111, -197431.097569445, -55065.5892361099, 48314.3065972236, 
  -6745.32673610683, 193492.494097225, 155009.569097224, 241747.214930556, 
  209670.99826389, -173438.47673611, -101510.63923611, -128948.689236113, 
  -222773.597569443, -498474.472569441, 146856.619097224, -275463.026736109, 
  386273.214930557, 213400.994097223, 171865.11076389, 464391.381597217, 
  1489.99826388643, -9918.39340277936, -362009.847569447, NA, NA, 
  NA, NA, NA, NA), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>
"
"0.12683657235796","0.147441956154897","70866","<p>I have a modelling dilemma. I am creating a model that attempts to predict demand (leads not sales) based upon the correlation to advertising spend. We know that without advertising spend, demand is driven by seasonality. So our models include seasonal factors like month of the year and even day of the week. 
If I were building a regular linear regression model, I would fit a linear regression model to a training dataset, to get estimates of the coefficients of the seasonal factors and advertising spend to demand. In order to get an estimate of future baseline demand, I would forecast demand using all the coefficients from the model and then I would estimate a baseline by setting adspend equal to zero. 
For ARIMA models, there are additional factors such as AR and MA terms. Would I estimate my baseline the same way by just setting the coefficient on advertising spend equal to zero?
Thanks for any thoughts.</p>
"
"0.164398987305357","0.159255514317652","47419","<p>I am trying to fit a time series using the function auto.arima and I face some strange results.</p>

<p>As a first try, I use the command</p>

<pre><code>auto.arima(data,d=0,D=1,max.p=2,max.q=2,max.P=2,max.Q=2,max.order=8, xreg=xreg_past,trace=TRUE,ic=""aic"")
</code></pre>

<p>The model I get is an ARIMA(2,0,2)(0,1,1)[12] with an AIC equal to -300.14.</p>

<p>But since I know that this command will make use of the stepwise selection algorithm, I want to make a try with the tests of all possible models using the option stepwise=FALSE. </p>

<p>I thus try the command</p>

<pre><code>auto.arima(data,d=0,D=1,max.p=2,max.q=2,max.P=2,max.Q=2,max.order=8, xreg=xreg_past,stepwise=FALSE,trace=TRUE,ic=""aic"")
</code></pre>

<p>And now, the model I get is an ARIMA(0,0,2)(2,1,0)[12] with an AIC equal to -293.14. Since my second attempt takes all the models into account, this result is strange as the previous model had a lower AIC. Furthermore, If I take a look in the trace of the last function call, I see that the ARIMA(2,0,2)(0,1,1)[12] model has now an AIC of -245.13 which explains why it has been rejected. Why did the AIC value change ?</p>

<p>At least, if I use the simple command </p>

<pre><code>arima(data, order=c(2,0,2), seasonal= list(order=c(2,1,2), period=12), xreg=xreg_past)
</code></pre>

<p>I get an AIC value of -319.15, which is better that the two models provided before.</p>

<p>I think I am missing something important but I am not able to see what. Can somebody help me ?</p>

<p>Thanks in advance,</p>

<p>Regards,</p>

<p>Ludo</p>
"
"0.12427395320024","0.120385853085769","174514","<p>I used two different methods to forecast a time series data.</p>

<ol>
<li>The first one used is HoltWinters with Beta and Gamma as FALSE, since I don't see any trend or seasonality in the plot.</li>
</ol>

<p>Below is the result from Box.test</p>

<pre><code>Box.test(fore.holt.stat$residuals, type=""Ljung-Box"", lag=10)

Box-Ljung test
data:  fore.holt.stat$residuals
X-squared = 10.691, df = 10, p-value = 0.3821
</code></pre>

<p>The p-value is 0.3821</p>

<ol start=""2"">
<li><p>I used auto.arima on the data and below is the result</p>

<pre><code>Box.test(fore.arima$residuals, type=""Ljung-Box"", lag=10)

Box-Ljung test
data:  fore.arima$residuals
X-squared = 14.724, df = 10, p-value = 0.1425
</code></pre></li>
</ol>

<p>The p-value is 0.14</p>

<pre><code>Question 1 :
</code></pre>

<p>Can I say that the first model is better since I have a higher p-vale?</p>

<p>Below are few other observations :</p>

<p>Model 1:</p>

<pre><code>accuracy(fore.holt.stat)
               ME     RMSE      MAE       MPE     MAPE     MASE       ACF1
Training set 424.9864 10275.55 7930.602 0.8782302 9.251837 0.766108 0.02142331
</code></pre>

<p>Model 2:</p>

<pre><code>accuracy(fore.arima)
               ME     RMSE     MAE        MPE     MAPE      MASE      ACF1
Training set 284.5242 7243.413 5371.42 -0.1874984 6.036736 0.5183941 0.0100049


Question 2 :
</code></pre>

<p>Which of the model is correct based on the <code>accuracy</code> function output?</p>

<p>In both the models, the p-value is high, but the mean of errors is not close to zero.</p>
"
"NaN","NaN","215897","<p>I am very new to this field and I want to learn forecasting of stock price using R. Please let me know which are the step should I follow?</p>"
"NaN","NaN","<p>If someone know tutorial links for forecasting in R then it will be good.</p>",""
"NaN","NaN","<p>Thanks in advance.</p>",""
"NaN","NaN","","<r><forecasting><arima>"
"0.278200606389665","0.256663501169673","116145","<p>I have downloaded the daily stock Adjusted Close price of one stock from sep 2011 to till date. As per my study plan, I have plotted some basic plots to understand the daily stock Adjusted closing price.</p>

<p>Here is the xyplot of the stock closing price by date and the code used to plot(My x axis not visible).</p>

<pre><code>Stock_T=stocks[which(symbol=='Stock_T'),]
xyplot(Adj.Close~Date,type='l',data=Stock_T,main='Adj.Close Price of the Stock_T')
</code></pre>

<p><img src=""http://i.stack.imgur.com/Ivlmk.png"" alt=""Timeseries plot of the raw data- Adjusted Closing price of the Stock""></p>

<p>By seeing this plot, the closing price was stable for period but had sudden huge increase in the stock price, it might had some other indicator which caused this much change in the stock price. Now my objective is to learn some ARIMA modeling concepts using this stock prices and try to do some forecasting of the stock price for few weeks. </p>

<p>As I have basic knowledge in ARIMA modeling, and I learned in the books that we should have stationary series before applying the ARIMA Model.</p>

<p>So, now I have plotted the ACF and PACF of the above raw data timeseries.</p>

<pre><code>acf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/rpy8S.png"" alt=""Raw data ACF Plot""></p>

<pre><code>pacf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/QKQge.png"" alt=""Raw data PACF plot""></p>

<p>From the above ACF and PACF plot, the series is not stationary and have huge autocorrelation (please correct me if am wrong), by differencing the series we will have stationary series (please correct me if am wrong). Here is the below plot.</p>

<pre><code>Stock_T_d1=diff(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/sD9Tj.png"" alt=""First difference of the raw series""></p>

<p>Here the differencing series and its ACF AND PACF plots. ACF plot shows that there is no auto correlation and the series is stationary (please correct me if I am wrong) but I am unable to interpret the PACF plots, can someone explain it to me?  </p>

<p><img src=""http://i.stack.imgur.com/cAoVf.png"" alt=""ACF plot of Difference series""></p>

<p><img src=""http://i.stack.imgur.com/U10g8.png"" alt=""PACF plot of Difference series""></p>

<p>The above difference series shows some unequal variance in the series and so I am taking log transformation before differencing and its ACF and PACF.</p>

<pre><code>Stock_T_logd1=diff(log(Stock_T$Adj.Close))
</code></pre>

<p><img src=""http://i.stack.imgur.com/MWovq.png"" alt=""Difference Logged series ""></p>

<p><img src=""http://i.stack.imgur.com/oYd2d.png"" alt=""ACF of Difference logged Series""></p>

<p><img src=""http://i.stack.imgur.com/HQxZw.png"" alt=""PACF of Difference logged Series""></p>

<p>Now I will try to ask my questions.</p>

<ol>
<li>Should we have stationary series before we apply ARIMA?</li>
<li>Could you please explain me the ACF and PACF of the original series, and what we should do if we have this kind of series?</li>
<li>Could you please explain me the ACF and PACF of the difference series, and what will be the next step?</li>
<li>Could you please explain me the ACF and PACF of the difference logged series, and what will be the next step?</li>
<li>Should we use difference series or difference logged series?</li>
<li>What will be the ARIMA orders of this series?</li>
<li>Is there any R code to find the ARIMA order automatically of the original series?</li>
</ol>
"
"0.12427395320024","0.120385853085769","174687","<p>I have around 10000 time series and I want to train ARIMA model using 8000 of them.</p>

<p>I wanted to use auto.arima function <a href=""http://www.inside-r.org/packages/cran/forecast/docs/auto.arima"" rel=""nofollow"">http://www.inside-r.org/packages/cran/forecast/docs/auto.arima</a>
however I am unable to find best ARIMA model for many time series. </p>

<p>Here is the code, I can always use x as my time series.  but how to train it using more time series and find best model?</p>

<pre><code>y=auto.arima(x)
plot(forecast(y,h=30))
</code></pre>

<p>Sample 
time series 1</p>

<pre><code>0.0003748,0.0003929,0.0003653,0.0003557,0.0004463,0.000349,0.0003099,0.0003395,0.0003157,0.0002871,0.0002604,0.0002422,0.0001917,0.0002117,0.0002689
</code></pre>

<p>time series 2</p>

<pre><code>0.0003977,0.0003481,0.0002413,0.0002069,0.0002127,0.0002108,0.0002003,0.0002174,0.0002098,0.0002069,0.0001955,0.0001926,0.0002108,0.0002146,0.0002079
</code></pre>
"
"0.18641092980036","0.180578779628654","174692","<p>I have around 10000 time series showing one particular metric over 5 hours. </p>

<p>I used <a href=""http://www.inside-r.org/packages/cran/forecast/docs/auto.arima"" rel=""nofollow"">auto.arima function</a> </p>

<p>In my previous question, people suggested that I have to use auto.arima for each time series, hold off some of data points and test the prediction with my hold off points.</p>

<p>I am holding off 20% of data points (if you see sample out of 40 I will hold off 8) and then let auto.arima predict. Then I can compare generated 8 values with actual 8 values.
But is there a formal way to test accuracy in ARIMA model? Is my approach correcT?</p>

<p>Is there a prebuilt function to test the accuracy of Arima.</p>

<p>Here is the code, I can always use x as my time series.</p>

<pre><code>y=auto.arima(x)
plot(forecast(y,h=30))
</code></pre>

<p>Sample 
time series 1</p>

<pre><code>0.0003748,0.0003929,0.0003653,0.0003557,0.0004463,0.000349,0.0003099,0.0003395,0.0003157,0.0002871,0.0002604,0.0002422,0.0001917,0.0002117,0.0002689
</code></pre>

<p>time series 2</p>

<pre><code>0.0003977,0.0003481,0.0002413,0.0002069,0.0002127,0.0002108,0.0002003,0.0002174,0.0002098,0.0002069,0.0001955,0.0001926,0.0002108,0.0002146,0.0002079
</code></pre>

<p>Both have 40 points. I can hold off 20% of them (8) and compare after auto.arima predicts.   But is there a simpler way I can test accuracy?</p>
"
"0.0878749550327494","0.0851256530758749","143636","<p>I have daily visitors data for the last 10 years. I want to do some basic tests like which is the busiest day, which is the busiest month, busiest week etc. I used <code>auto.arima</code> function with argument <code>xreg</code> to find out the coefficients of all the days of the week, week of the month. This is the output I got:</p>

<pre><code>&gt; summary(arima1)
Series: dailysea 
ARIMA(1,1,2)                    

Coefficients:
          ar1      ma1      ma2         Sun        Mon         Tue        Wed         Thu
      -0.1250  -0.4506  -0.3712  -1466.6853  -3623.175  -3895.0555  -3722.146  -3327.4288
s.e.   0.1207   0.1117   0.0891    325.7253    386.738    379.8793    379.883    386.7512
            Fri
      -2146.910
s.e.    325.736

sigma^2 estimated as 7776468:  log likelihood=-6808.5
AIC=13637   AICc=13637.31   BIC=13682.92

Training set error measures:
                   ME     RMSE      MAE  MPE MAPE      MASE         ACF1
Training set 59.63838 2784.809 1952.625 -Inf  Inf 0.8353728 -0.001839015
</code></pre>

<p>Can I use these coefficients to conclude that Saturday is the busiest followed by Sunday, Friday etc.? Also I have infinite MAPE which is not making sense to me.</p>
"
"0.219687387581873","0.241189350381645","99488","<p>I am relatively new to statistics and not formally trained but have been given a complex problem to solve and need some guidance. I realise that I am out of my depth a bit here but would appreciate whatever help I can get bearing in mind that there is no budget for this and as a result it is not possible to purchase software or hire consultants.</p>

<p><strong>The Problem</strong></p>

<p>The business I work for has a large number of mobile representatives that can be dispatched to a variety of different jobs. There are ~100 different job types and each job can be broken up into 4 different final outcomes. Each of these 400 outcomes requires an allocation of man hours to complete. I have a count of how many times each one of these outcomes occurred in each hourband for the past 5 years.</p>

<p>I have been asked to forecast how many of each outcome will occur in each hourband for the 28days from the present. The resulting forecast will be used to anticipate staffing requirements on an hour-by-hour basis. As a result the forecasts for each hourband need to been fairly accurate.</p>

<p><strong>Factors</strong></p>

<p>In my data there are clearly some yearly, weekly, and daily seasonal effects. In general each outcome is more likely to occur at certain times of the day on certain days of the week and with some yearly trends.</p>

<p>Each different outcome is likely to be related to the frequency of a number of different outcomes. i.e. if <em>x</em> happens then <em>y</em> and/or <em>z</em> are likely but <em>a</em> and/or <em>b</em> are not.</p>

<p>There are a large number of environmental factors that contribute to the frequency of each outcome. These can include, but are not limited to weather, sociopolitical, financial trends, one off events.</p>

<p><strong>What I have tried</strong></p>

<p>So far I have tried using simple auto.arima, holtwinters and ets forecasts. holtwinters ended up producing a flat line (i.e. 5 and hour for the next 672 hours). ets doesnt work because the seasons are longer than 24 intervals. auto.arima produced the best results but they were still a long way off being accurate.</p>

<p>It was then suggested that I try tbats() and provide it with multiple seasonal lengths. I achieved best results by giving it seasonal lengths of 8760 (1yr) and 168 (1wk). Frustratingly, these results are within 1% when viewed as a sum of all hourbands in a 1 month block but are anything up to 300% (avg 20%) off when considering each individual hourband.</p>

<p>Both of these approaches have been applied over an individual outcome rather than considering all possible outcomes (and their correlation to each other).</p>

<p><strong>My thoughts so far</strong></p>

<p>At this stage I feel like my two options are to either to find a way to use something similar to tbats() that will look at the relationships between the multiple different outcomes as well as the seasonality and forecast based on that information.</p>

<p>or</p>

<p>Abandon that approach for a Neural Network model. My understanding (limited) is that using the Neural Network approach I may be able to 'factor' for the multitude of unknown environmental factors without having to actually identify them. I know this is lazy but my feeling for the data is that there are going to be a fair few unknown factors to identify and forecasting them may end up being a job in itself (i.e. weather conditions)</p>

<p><strong>The Question</strong> (finally)</p>

<p>What I am looking for is some guidance.</p>

<p>Considering the information above and the fact that I am pretty much limited to R, what is the best approach??</p>

<p>and</p>

<p>What are the basic steps I need to follow?</p>

<p>While I cant post my data online (due to my employers restrictions) I can send it an individual or two if someone was interested in giving us a hand to find a solution.</p>
"
"0.107624400500126","0.104257207028537","196703","<p>For all my forecast models (<code>arima</code> with Fourier, <code>tbats</code>, <code>ets</code> and <code>stlf</code> from the ""forecast"" package in R) I use the following:</p>

<pre><code>model &lt;- auto.arima(x, xreg=fourierf(x, K=y, h=52)) 
</code></pre>

<p>or </p>

<pre><code>model &lt;- tbats(x) 
</code></pre>

<p>or </p>

<pre><code>model &lt;- ets(x)
</code></pre>

<p>or </p>

<pre><code>model &lt;- stlf(x)
</code></pre>

<p>then</p>

<pre><code>forecast(model, h=52)
</code></pre>

<p><code>h=52</code> as that takes my data to the end of this quarter.</p>

<p>I also used multiple regression separately. </p>

<p>I then decided to use ARIMA with the dummy variables I'd used in my regression model:</p>

<pre><code>model &lt;- auto.arima(x, xreg=dummy)
</code></pre>

<p>then</p>

<pre><code>forecast(model, xreg=dummy, h=52)
</code></pre>

<p>However it doesnt matter if I use <code>h=52</code> or <code>h=1</code> or leave it blank, the forecast automatically seems to forecast the total length of my data set i.e. if I had 404 values it forecasted 404 values forward.</p>

<p>Just wondering why this happens and can I restrict the forecast to less?</p>
"
"0.0878749550327494","0.0851256530758749","180217","<p>I'm using time series data containing both trend and seasonality. I also have 2 endogenous predictor variables that I would like to include in my model.</p>

<p>In R I've used the forecast package to develop a dynamic regression model with use of <code>auto.arima()</code> and the <code>xreg</code> argument from the <code>forecast package</code>. I understand this procedure takes a regression and then attempts to fit the residuals with an ARMA Model.</p>

<p>I've also developed what seems to be an appropriate model using the forecasting Module in SPSS by specifying a Seasonal ARIMA model and including my covariates. However, one of the coefficients on one of my endogeneous predictors has a negative sign which makes no sense intuitively. </p>

<p>I've read Dr. Hyndman's article <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">The ARIMAX model muddle</a> and found it to be extremely insightful and useful. However, I have not been able to find any documentation on what type of statistical procedure SPSS uses to fit an ARIMA model with covariates, so I'm not sure how I should interpret the coefficients or how concerned I should be with a flipped sign. Any help clarifying the modelling procedure used by SPSS would be tremendously appreciated. </p>
"
"0.06213697660012","0.0601929265428846","198662","<p>I am using the code below:</p>

<pre><code>#Training seasonal ARIMAx model on input dataset
fit&lt;-arima(visits_ts, order=c(1,0,0),seasonal=c(1,0,0),xreg=reg,method=""CSS"")

#Forecasting for future
pred&lt;-predict(fit,n.ahead=13, newxreg=nreg)
</code></pre>

<p>The code only generates out of sample forecast. However, I would also like to see the in-sample forecast for the training data set. How can I get the in-sample forecast?</p>
"
"0.24854790640048","0.225723474535817","198844","<p>I'm trying to understand how <code>auto.arima</code> with covariates in the xreg parameter works. I'm familiar with regression and I'm starting to work on forecasting.</p>

<p>My understanding of forecasting is that you look for patterns in the past time series and then project those paterns onto the future.  </p>

<p>My uderstanding of regression is that you use predictors to try to generate an output value and minimize the difference between your created value and the real value.  </p>

<p>So how does forecasting <code>auto.arima</code> with <code>xreg</code> work? Do you create a forecast for a timeseries based on past data and regression model based on the input time series and input <code>xreg</code>, and then forecast each data point in the time series and for each forecasted data point use the regression model you built and future <code>xreg</code> values to adjust the forecasted values?</p>

<p>I'm a former physics grad student, so I'm not allergic to math but I'm just looking for a high level overview of the process here to understand how forecasting <code>auto.arima</code> works.  </p>

<p>For example like, </p>

<ul>
<li><p>step 1: build forecast model on input time series, and regression model on input time series and input <code>xreg</code> values</p></li>
<li><p>step 2: forecast model into future one step, and predict value with regression model and future <code>xreg</code> values</p></li>
<li><p>step 3: algorithm combines forecasted value and regression model prediction to get combined value</p></li>
</ul>

<p>This is just a guess at how it works, but it's an example of the kind of high level explanation I'm looking for.</p>

<p>I've included some code below that I've been working on trying to forecast time in to out <code>TiTo</code> for customers at a restaurant with predictor count of customers in the restaurant <code>CustCount</code>.</p>

<pre><code>OV&lt;-zoo(SampleData$TiTo, 
    order.by=SampleData$DateTime)


eDate &lt;- ts(OV, frequency = 24)

Train &lt;-eDate[1:15000]
Test &lt;- eDate[15001:22773]

xregTrain &lt;- SampleData[1:15000,]$CustCount
    xregTest &lt;- SampleData[15001:22773,]$CustCount

Arima.fit &lt;- auto.arima(Train, xreg = xregTrain)

Acast&lt;-forecast(Arima.fit, h=7772, xreg = xregTest)

accuracy(Acast$mean,Test)
</code></pre>
"
"0.152203886829552","0.147441956154897","225094","<p>In the <code>Arima()</code> method, in the <code>forecast</code> package in R, I can provide a vector of parameters to the <code>fixed</code> argument, and the model is estimated while ensuring the provided parameters are fixed to the supplied values.</p>

<p>However, when I do this, the model returns no standard errors for these coefficients. Why is this the case? Is it not possible to estimate standard errors of coefficients that are manually provided? Would love an explanation as to why this might be the case.</p>

<p>Moreover, the <code>forecast</code> method still calculates confidence intervals when forecasting from a model that has fixed parameters. Are these intervals still statistically valid? I would have thought such would rely on the standard errors of the estimated coefficients, which it seems we may not know in the case of manually-entered parameters?</p>
"
"0.152203886829552","0.147441956154897","223872","<p>Data consisting of 30 values is stored in a time series <code>time</code>.<br>
After applying ARIMA modelling on <code>time</code>, I used <code>forecast</code> function to predict future values:</p>

<pre><code>model = arima(time, order = c(3,2,1))
prediction = forecast.Arima(model,h=10)
prediction step is not working and showing error 
Error in ts(x) : object is not a matrix
</code></pre>

<p>As you see above, I am getting an error message. But if I do</p>

<pre><code>model = arima(time[1:25], order = c(3,2,1))
prediction = forecast.Arima(model,h=10)
</code></pre>

<p>it works. Why is it so?</p>

<p>When I used the <code>predict</code> function </p>

<pre><code>model = arima(time, order = c(3,2,1))
prediction=predict(model,n.ahead=10)
</code></pre>

<p>it also works.</p>

<p><strong>Which</strong> function would be better to use, <code>predict</code> or <code>forecast</code>, for ARIMA models in R, and <strong>why</strong>?</p>
"
"0.317649021564526","0.307710887584701","176129","<p>I've been working on some various time series forecasts and I've begun to notice a trend (pardon the pun) in my analyses. For about 5-7 datasets that I've worked with so far, it would be helpful to allow for multiple seasonal periods along with an option for holiday dummies. I've tried various methods and usually stick with <code>tbats</code> since <code>auto.arima()</code> with regressors has been giving me issues. By this point, it's probably obvious I'm working in R.</p>

<p>Before I get too far, let me give some sample data. Hopefully the following link works: <a href=""https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0"" rel=""nofollow"">https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0</a>.</p>

<p>This data yields the following time series plot:
<a href=""http://i.stack.imgur.com/FYS1x.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FYS1x.jpg"" alt=""Time Series Plot""></a>
The large dips are around Christmas and New Years, however there are also smaller dips around Thanksgiving. In the code below, I name this dataset <code>traindata</code>.</p>

<p>Now, <code>ets</code> and ""plain"" <code>auto.arima</code> don't look so hot in the long run since they are limited to only one seasonal period (I choose weekly). However for my test set that I held out they performed fairly well for the month's worth of data (with the exception of Labor Day weekend). This being said, forecasting out for a year would be ideal.</p>

<p>I next tried <code>tbats</code> with weekly and yearly seasonal periods. That results in the following forecast:
<a href=""http://i.stack.imgur.com/kcXmd.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kcXmd.jpg"" alt=""TBATS Forecast""></a></p>

<p>Now this looks pretty good. From the naked eye it looks great at taking into account the weekly and yearly seasonal periods as well as Christmas and New Years effects (since they obviously fall on the same dates each year). It would be best if I could include the holidays (and the days around them) as dummy variables. Hence my attempts at <code>auto.arima</code> with <code>xreg</code> regressors.</p>

<p>For ARIMA with regressors, I've followed Dr. Hyndman's suggestions for the fourier function (given here: <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) as well as his selection of the number of fourier terms (given here: <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/forecasting-weekly-data/</a>)</p>

<p>My code is as follows:</p>

<pre><code>fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep=""""),period,sep=""_"")
  return(X)
}

fcdaysout&lt;-365
m1&lt;-7
m2&lt;-30.4375
m3&lt;-365.25

hol&lt;-cbind(traindata$CPY_HOL, traindata$DAY_BEFORE_CPY_HOL, traindata$DAY_AFTER_CPY_HOL)
hol&lt;-as.matrix(hol)

n &lt;- nrow(traindata)
bestfit &lt;- list(aicc=Inf)
bestk &lt;- 0

for(i in 1:m1)
{
    fake_xreg = cbind(fourier(1:n,i,m1), fourier(1:n,i,m3), hol)
    fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = fake_xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
    	if(fit$aicc &lt; bestfit$aicc)
    {
        bestfit &lt;- fit
        bestk &lt;- i
    }
    else
    {
    }
}

k &lt;- bestk
k
##k&lt;-3

xreg&lt;-cbind(fourier(1:n,k,m1), fourier(1:n,k,m3), hol)
xreg&lt;-as.matrix(xreg)

aacov_fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aic"", allowdrift=TRUE)
summary(aacov_fit)
</code></pre>

<p>Where my issues come in is inside the for loop to determine the <code>k</code>, the number of fourier terms, that minimizes AIC. In all of my attempts at ARIMA with regressors, it always produces an error when <code>k&gt;3</code> (or <code>i&gt;3</code> if we're talking about inside my loop). The error being <code>Error in solve.default(res$hessian * n.used, A) : system is computationally singular: reciprocal condition number = 1.39139e-34</code>. Simply setting <code>k=3</code> gives some decent results for my test set but for the next year it doesn't appear to adequately catch the steep drops around the end of the year and is much smoother than imagined as evidenced in this forecast:<a href=""http://i.stack.imgur.com/rj30h.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rj30h.jpg"" alt=""AutoArima with Covariates (k=3)""></a></p>

<p>I assume this general smoothness is due to the small number of fourier pairs. Is there an oversight in my code in that I'm just royally screwing up the procedure provided by Dr. Hyndman? Or is there a theoretical issue that I'm unknowingly running into by trying to find more than 3 pairs of fourier terms for the multiple seasons I'm attempting to account for? Is there a better way to include the multiple seasonalities and dummy variables?</p>

<p>Any help in getting these covariates into the arima model with an appropriate number of fourier terms would be appreciated. If not, I'd at least like to know whether or not what I'm attempting is possible in general with larger number of fourier pairs.</p>
"
"0.0878749550327494","0.0851256530758749","181216","<p>I am new to R and forecasting .I have data for a certain product. It contains value sales and its promotions. The data is weekly and there are about 104 data points. </p>

<p>I converted the sales into a ts object and created seasonaldummy's to capture seasonality. </p>

<pre><code>actual_val = ts(sku1$Sal , frequency = 52)
dummy_val = seasonaldummy(actual_val) 
</code></pre>

<p>The dummy's were later combined with the promo variables to create external regressors xreg_val for the model. Those promotions which were not held for this sku were removed before combining the two.</p>

<pre><code>model_value &lt;- try(auto.arima(actual_val , xreg = xreg_val ) , silent = TRUE)
</code></pre>

<p>I have received the following error</p>

<pre><code>Error in optim(init[mask], armaCSS, method = optim.method, hessian = FALSE,    
non-finite value supplied by optim
</code></pre>

<p>I could not understand where exactly I have gone wrong in this.</p>

<p>Attaching a sample of the data. Kindly help me with this</p>

<p><a href=""https://drive.google.com/file/d/0B6sOv1da0JMeb01XYW92UzRSZ0U/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B6sOv1da0JMeb01XYW92UzRSZ0U/view?usp=sharing</a></p>
"
